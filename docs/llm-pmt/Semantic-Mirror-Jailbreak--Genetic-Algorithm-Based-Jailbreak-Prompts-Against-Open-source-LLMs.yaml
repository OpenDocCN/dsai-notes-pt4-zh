- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:16'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against
    Open-source LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.14872](https://ar5iv.labs.arxiv.org/html/2402.14872)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xiaoxia Li    Siyuan Liang    Jiyi Zhang    Han Fang    Aishan Liu    Ee-Chien
    Chang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs), used in creative writing, code generation, and
    translation, generate text based on input sequences but are vulnerable to jailbreak
    attacks, where crafted prompts induce harmful outputs. Most jailbreak prompt methods
    use a combination of jailbreak templates followed by questions to ask to create
    jailbreak prompts. However, existing jailbreak prompt designs generally suffer
    from excessive semantic differences, resulting in an inability to resist defenses
    that use simple semantic metrics as thresholds. Jailbreak prompts are semantically
    more varied than the original questions used for queries. In this paper, we introduce
    a Semantic Mirror Jailbreak (SMJ) approach that bypasses LLMs by generating jailbreak
    prompts that are semantically similar to the original question. We model the search
    for jailbreak prompts that satisfy both semantic similarity and jailbreak validity
    as a multi-objective optimization problem and employ a standardized set of genetic
    algorithms for generating eligible prompts. Compared to the baseline AutoDAN-GA,
    SMJ achieves attack success rates (ASR) that are at most 35.4% higher without
    ONION defense and 85.2% higher with ONION defense. SMJ’s better performance in
    all three semantic meaningfulness metrics of Jailbreak Prompt, Similarity, and
    Outlier, also means that SMJ is resistant to defenses that use those metrics as
    thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As Large Language Models (LLMs) have outstanding performance in various areas
    including translation, code generation, creative writing, and so on. Various safeguards
    are used to prevent LLMs from being misused, including reinforcement learning
    from human feedback (RLHF) (Ouyang et al., [2022](#bib.bib33)), OpenAI’s usage
    policies (OpenAI, [2024b](#bib.bib31)), and Meta Llama 2’s use policy (Meta, [2024](#bib.bib29)).
    However, recent studies have shown that deep models are vulnerable to security
    threats (Wei et al., [2018](#bib.bib44); Liang et al., [2020](#bib.bib15), [2022c](#bib.bib19);
    Liu et al., [2023a](#bib.bib24); Liang et al., [2022a](#bib.bib17); Liu et al.,
    [2023b](#bib.bib25); Liang et al., [2022b](#bib.bib18); He et al., [2023](#bib.bib6);
    Liang et al., [2021](#bib.bib16); Sun et al., [2023](#bib.bib40); Li et al., [2023b](#bib.bib13);
    Liu et al., [2023d](#bib.bib27); Liang et al., [2023a](#bib.bib14); Liu et al.,
    [2023c](#bib.bib26); Liang et al., [2023b](#bib.bib20); Wang et al., [2022](#bib.bib43);
    Liu et al., [2019](#bib.bib21), [2020b](#bib.bib23), [2020a](#bib.bib22); Wang
    et al., [2021](#bib.bib42); Liu et al., [2023a](#bib.bib24)). By carefully designing
    jailbreak prompts, which is to add a jailbreak template designed for a single
    LLM or various LLMs before the questions to ask, harmful responses and hate speech
    (Kang et al., [2023](#bib.bib10); Goldstein et al., [2023](#bib.bib4); Hazell,
    [2023](#bib.bib5)) can still be yielded from LLMs. Consequently, jailbreak attacks
    pose a threat to the widespread use of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: There are two main categories of existing jailbreak attacks, they are attacking
    using handcrafted jailbreak prompts and automatically generated jailbreak prompts.
    For handcrafted jailbreak prompts, those prompts designed for jailbreaking different
    LLMs spread through platforms such as Discord and Reddit. [Shen et al.](#bib.bib39)
    ([2023](#bib.bib39)) collected handcrafted jailbreak prompts and performed evaluations
    on LLMs’ vulnerability. For automatically generated jailbreak prompts, [Zou et al.](#bib.bib49)
    ([2023](#bib.bib49)) proposed GCG, which utilizes greedy and gradient-based search
    techniques to generate universal and transferable jailbreak templates by using
    multiple LLMs to perform optimization. [Liu et al.](#bib.bib28) ([2023e](#bib.bib28))
    also proposed AutoDAN, that method generates jailbreak prompts by optimizing with
    hierarchical genetic algorithm. Compared to GCG’s jailbreak templates, AutoDAN’s
    jailbreak templates have the advantage of stealthiness against naive jailbreak
    defenses such as perplexity-based detection (Jain et al., [2023](#bib.bib9)),
    and they are more semantically meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: However, since the existing jailbreak attacks employ a combination of jailbreak
    templates followed by questions to ask to create the jailbreak prompts, that jailbreak
    prompt design creates two limitations for the attacks. The first limitation is
    that by using jailbreak templates as a tool to perform attacks, the attacks would
    be more vulnerable to jailbreak defenses because the jailbreak prompts would be
    relatively less semantically meaningful as compared to the original question.
    The second limitation is without using jailbreak templates, it is not possible
    to elicit harmful questions’ responses from LLMs. Hence, jailbreak prompts’ semantic
    meaningfulness and the attack success rate (ASR) have a negative correlation,
    they cannot be optimized together.
  prefs: []
  type: TYPE_NORMAL
- en: This paper proposes Semantic Mirror Jailbreak (SMJ), it suggests a direct similarity
    between the original questions and the jailbreak prompts, much like a mirror reflecting
    an image. The method simultaneously optimizes the two goals of semantic similarity
    and attack validity to generate jailbreak prompts. SMJ aims to address the two
    limitations by utilizing the genetic algorithm. In the initialization stage, SMJ
    uses paraphrased questions generated by referring to the original question as
    the initial population to ensure jailbreak prompts’ semantic meaningfulness. By
    subsequently applying fitness evaluation, which takes both jailbreak prompts’
    semantic similarity and attack validity into consideration, before selection and
    crossover, this can guarantee both jailbreak prompts’ semantic meaningfulness
    and the attack success rate (ASR) optimized concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a new jailbreak attack that designs a jailbreak prompt that can satisfy
    both semantic similarity and attack availability. We model the above attack as
    a multi-objective optimization problem.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a genetic algorithm-based scheme for automatic prompt generation,
    i.e., semantic mirror jailbreak, which can achieve an effective attack while preserving
    the source semantic information by means of a well-designed population and optimization
    strategy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiments show that SMJ can resist simple defenses that use semantic meaningfulness
    metrics as thresholds and bypass more advanced defense, the ONION defense. Compared
    to the baseline AutoDAN-GA, SMJ achieves up to 35.4% improvement in ASR.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec1f34f02058892a33ef2605afea7f5b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An illustration of jailbreak prompt. If querying using a normal harmful
    question, LLMs will reject answering the question in red. However, if using the
    existing jailbreak prompt which combines a jailbreak template with the question,
    LLMs will generate a harmful response. Semantic Mirror Jailbreak (SMJ)’s jailbreak
    prompt can also reach the same outcome but the prompt would be more semantically
    meaningful.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background and Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) LLMs are transformer models that can generate human-like
    text, they are trained on massive datasets, and attention mechanisms are employed
    to predict the next word for response generation. There are open-sourced LLMs
    such as Llama-2 (Touvron et al., [2023](#bib.bib41)) and Vicuna (Zheng et al.,
    [2023](#bib.bib47)), and commercial APIs such as GPT-3.5 Turbo and GPT-4 (OpenAI,
    [2024a](#bib.bib30)). Since LLMs have outstanding performance in various areas,
    to prevent misuse, such as responding with hallucinations (Bang et al., [2023](#bib.bib1))
    and producing misinformation (Pegoraro et al., [2023](#bib.bib34); Zhou et al.,
    [2023](#bib.bib48)), safeguards were applied to provide ethical guidelines. For
    example, LLM developers using reinforcement learning from human feedback (RLHF)
    (Ouyang et al., [2022](#bib.bib33)) to align LLMs with human values and expectations,
    OpenAI refers to their usage policies (OpenAI, [2024b](#bib.bib31)), Vicuna filters
    inappropriate user inputs by using OpenAI moderation API (OpenAI, [2024c](#bib.bib32))
    in their online demo (Chiang et al., [2023](#bib.bib2)), Meta Llama 2 also refers
    to their use policy (Meta, [2024](#bib.bib29)).
  prefs: []
  type: TYPE_NORMAL
- en: Jailbreak Attacks against LLMs With the emergence of LLMs, although there are
    LLM’s safeguards set by model developers to prevent LLM from generating harmful
    responses and hate speech (Kang et al., [2023](#bib.bib10); Goldstein et al.,
    [2023](#bib.bib4); Hazell, [2023](#bib.bib5)), there are still ways to bypass
    the LLM’s safeguards by using jailbreak prompts, which are called jailbreak attacks.
    Starting from handcrafted jailbreak prompts, jailbreak templates with careful
    prompt engineering for jailbreaking different LLMs spread through platforms such
    as Discord and Reddit. [Shen et al.](#bib.bib39) ([2023](#bib.bib39)) collected
    handcrafted jailbreak templates from four platforms, by combining jailbreak templates
    and questions, they then performed attacks and discovered LLMs’ vulnerability
    against handcrafted jailbreak prompts. Other than handcrafted jailbreak prompts,
    another main jailbreak attack category is automatically generated jailbreak prompts.
    Extensive research has proposed automatic jailbreak attacks, [Zou et al.](#bib.bib49)
    ([2023](#bib.bib49)) produce automatic jailbreak prompts by generating adversarial
    suffixes, which are used to append to the end of the questions to ask, by a combination
    of greedy and gradient-based search techniques. [Liu et al.](#bib.bib28) ([2023e](#bib.bib28))
    proposed AutoDAN that generates jailbreak prompts by employing hierarchical genetic
    algorithm for optimization. Also, by applying the idea of chain of thought prompting,
    multi-step jailbreaking prompts can be used to jailbreak (Li et al., [2023a](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: Other than using jailbreak prompts to perform jailbreak attacks, there are still
    other ways, such as [Huang et al.](#bib.bib7) ([2023](#bib.bib7)), they jailbreak
    by exploiting different text generation configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Defense against Jailbreak Attacks With the occurrence of various jailbreak attacks
    against LLMs, [Jain et al.](#bib.bib9) ([2023](#bib.bib9)) evaluated three types
    of defenses, including perplexity-based detection, input preprocessing by paraphrase
    and retokenization, and adversarial training, on both white-box and gray-box settings.
    Moreover, [Laiyer.ai](#bib.bib11) ([2023](#bib.bib11)) proposes “deberta-v3-base-prompt-injection”,
    a model fine-tuned to identify jailbreak prompts and normal questions, which can
    used to effectively defense against existing jailbreak attacks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6c842369842aa39dda8c06cbbb87e77d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: This paper proposes Semantic Mirror Jailbreak (SMJ), a method that
    uses paraphrased questions generated by referring to the original question as
    the initial population to ensure jailbreak prompts’ semantic meaningfulness. By
    subsequently applying fitness evaluation, which takes both jailbreak prompts’
    semantic similarity and attack validity into consideration, before selection and
    crossover, this can guarantee both jailbreak prompts’ semantic meaningfulness
    and the attack success rate (ASR) optimized concurrently.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Preliminaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Threat model Given a set of harmful questions represented as $Q=\{\mathit{Q}_{1},\mathit{Q}_{2},\ldots,\mathit{Q}_{n}\}$,
    produce its related paraphrased questions set $$Q^{\prime}=\{Q^{\prime}_{i}=\{P_{i1},P_{i2},\ldots,P_{ij}\}\}\begin{subarray}{c}i=1,2,\ldots,n\\
  prefs: []
  type: TYPE_NORMAL
- en: j=1,2,\ldots,k\end{subarray}$$, using the paraphrased questions set $Q^{\prime}$,
    that $M(P_{ij})=1$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formulation Given a harmful question $Q_{i}$ can be chosen by maximizing:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\underset{P_{ij}\in Q^{\prime}_{i}}{{\arg\max}\,S(Q_{i},P_{ij}\mid M(P_{ij})=1)}.$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Semantic Mirror Jailbreak
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By considering the limitations of the existing jailbreak attack: 1) the jailbreak
    prompts would be relatively less semantically meaningful as compared to the original
    questions. The long jailbreak templates make the jailbreak prompts contain more
    confusing content, also leading the attack to be more vulnerable against jailbreak
    defenses. Moreover, 2) it seems that for existing methods, including jailbreak
    templates within jailbreak prompts is a must. Hence, bringing out the situation
    that without jailbreak prompts being less semantically meaningful, the jailbreak
    attacks are less likely to be successful, making it harder to optimize both the
    semantic meaningfulness and the attack success rate (ASR) metrics together. By
    considering those two limitations, this paper proposes SMJ, a method that addresses
    those two limitations using the genetic algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Overall Review
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since the existing jailbreak prompts are a combination of jailbreak templates
    followed by questions to ask, that combination produces relatively less semantically
    meaningful jailbreak prompts as compared to the original questions. Inspired by
    this idea, SMJ treats the questions to ask as jailbreak prompts, regardless of
    the jailbreak templates. In this way, the jailbreak prompts can be more normal
    and semantically meaningful. This method generates paraphrased questions according
    to the original questions in the initialization process. After that, by referring
    to the genetic algorithm, using fitness functions to perform selection, and then
    applying crossover to increase the population diversity, the resulting offspring
    would be used in the next generation. Finally, the best paraphrased questions
    would be chosen among all generations after iteratively employing the above operators
    until the termination criteria are met. By using this method, optimizing both
    the semantic meaningfulness and the attack success rate (ASR) metrics together
    becomes achievable. *The summary algorithm for SMJ can be found in [Algorithm 1](#alg1
    "In 3.3 Overall Review ‣ 3 Method ‣ Semantic Mirror Jailbreak: Genetic Algorithm
    Based Jailbreak Prompts Against Open-source LLMs"), the detailed algorithm can
    be found in [Algorithm 2](#alg2 "In Appendix B Detailed Algorithms. ‣ Appendix
    A Refusal keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier ‣ 4.2 Results
    ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak
    Prompts Against Open-source LLMs") in Supplementary Materials.*'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Semantic Mirror Jailbreak (SMJ)
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Harmful questions $Q$ following [Algorithm 3](#alg3 "In Appendix B Detailed
    Algorithms. ‣ Appendix A Refusal keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt)
    and Outlier ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic
    Algorithm Based Jailbreak Prompts Against Open-source LLMs")     Perform Fitness
    Evaluation following [Algorithm 4](#alg4 "In Appendix B Detailed Algorithms. ‣
    Appendix A Refusal keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier ‣
    4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based
    Jailbreak Prompts Against Open-source LLMs")     Perform Crossover following [Algorithm 5](#alg5
    "In Appendix B Detailed Algorithms. ‣ Appendix A Refusal keywords list. ‣ 4.2.2
    Jailbreak Prompt (JPt) and Outlier ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror
    Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs")     Perform
    Fitness Evaluation following [Algorithm 4](#alg4 "In Appendix B Detailed Algorithms.
    ‣ Appendix A Refusal keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier
    ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based
    Jailbreak Prompts Against Open-source LLMs")     repeat        Perform Selection
    following [Algorithm 6](#alg6 "In Appendix B Detailed Algorithms. ‣ Appendix A
    Refusal keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier ‣ 4.2 Results
    ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak
    Prompts Against Open-source LLMs")        Perform Crossover following [Algorithm 5](#alg5
    "In Appendix B Detailed Algorithms. ‣ Appendix A Refusal keywords list. ‣ 4.2.2
    Jailbreak Prompt (JPt) and Outlier ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror
    Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs")        Perform
    Fitness Evaluation following [Algorithm 4](#alg4 "In Appendix B Detailed Algorithms.
    ‣ Appendix A Refusal keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier
    ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based
    Jailbreak Prompts Against Open-source LLMs")     until Termination criteria is
    met  end for'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Population Initialization The first half of the initialization was conducted
    by generating $N$ jailbreak prompts are generated to be the initial population.
    *The detailed population initialization algorithm for SMJ can be found in [Algorithm 3](#alg3
    "In Appendix B Detailed Algorithms. ‣ Appendix A Refusal keywords list. ‣ 4.2.2
    Jailbreak Prompt (JPt) and Outlier ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror
    Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs")
    in Supplementary Materials.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fitness Evaluation The fitness evaluation contains two fitness functions, they
    are semantic similarity, which is evaluated using “all-mpnet-base-v2” model proposed
    by [Reimers & Gurevych](#bib.bib38) ([2022b](#bib.bib38)), and attack validity,
    which is judged by checking whether the jailbreak prompt’s LLM response contain
    any keywords from the refusal keywords list in [Appendix A](#A1 "Appendix A Refusal
    keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier ‣ 4.2 Results ‣ 4 Experiments
    ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against
    Open-source LLMs"). Once new jailbreak prompts are generated, fitness evaluations
    are immediately applied in both the initialization and the following generation
    processes. Only jailbreak prompts deemed jailbreak successful and with semantic
    similarity within $k\%$ is defaulted to be 10\. By optimizing those two fitness
    functions together, the second limitation can be solved. *The detailed fitness
    evaluation algorithm for SMJ can be found in [Algorithm 4](#alg4 "In Appendix
    B Detailed Algorithms. ‣ Appendix A Refusal keywords list. ‣ 4.2.2 Jailbreak Prompt
    (JPt) and Outlier ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic
    Algorithm Based Jailbreak Prompts Against Open-source LLMs") in Supplementary
    Materials.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Selection To select individuals for the crossover process, the roulette wheel
    selection is applied to either the filtered initial population or the filtered
    offspring. Given the semantic similarity of those jailbreak prompts for each generation,
    and assuming the current population is of size $Z$ being selected is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{ij}=\frac{S(Q_{i},P_{ij})}{\sum_{m=1}^{Z}S(Q_{i},P_{im})}$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Eventually, $O/10$, no selection is needed and all prompts in the current generation
    will be used for crossover. *The detailed selection algorithm for SMJ can be found
    in [Algorithm 6](#alg6 "In Appendix B Detailed Algorithms. ‣ Appendix A Refusal
    keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier ‣ 4.2 Results ‣ 4 Experiments
    ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against
    Open-source LLMs") in Supplementary Materials.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Crossover All individuals selected from the selection process would be used
    to crossover. Different from the traditional crossover in the genetic algorithm,
    to produce $O$ is defaulted to be 120\. *The detailed crossover algorithm for
    SMJ can be found in [Algorithm 5](#alg5 "In Appendix B Detailed Algorithms. ‣
    Appendix A Refusal keywords list. ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier ‣
    4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based
    Jailbreak Prompts Against Open-source LLMs") in Supplementary Materials.*'
  prefs: []
  type: TYPE_NORMAL
- en: Termination criteria There are three termination criteria for SMJ, the first
    criteria is the number of generations reaches the $MaxGenerations$ defaults to
    be 3\. The third criteria is no new individual is produced in the current generation,
    there are two situations, 1) all individuals in the offspring for any generation
    have already existed or were assessed before the current generation, or 2) there
    is no available initial population to start the generation after applying the
    fitness evaluation. If any of the above criteria are achieved, the algorithm stops
    and returns the best paraphrased question with the highest semantic similarity
    among all generations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Attack Success Rate (ASR) and Similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Llama-2-7b-chat-hf | Vicuna-7b | Guanaco-7b |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Methods | ASR% | Similarity% | ASR% | Similarity% | ASR% | Similarity% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original question | 1.40 | 100.00 | 32.00 | 100.00 | 47.00 | 100.00 |'
  prefs: []
  type: TYPE_TB
- en: '| AutoDAN-GA | 30.60 | 4.65 | 79.80 | 5.74 | 94.20 | 6.47 |'
  prefs: []
  type: TYPE_TB
- en: '| SMJ | 66.00 | 73.41 | 98.60 | 92.13 | 100.00 | 94.63 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Jailbreak Prompt (JPt) and Outlier.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Llama-2-7b-chat-hf | Vicuna-7b | Guanaco-7b |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Methods | JPt% | Outlier | JPt% | Outlier | JPt% | Outlier |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original question | 00.00 | 1.63 | 00.00 | 1.63 | 00.00 | 1.63 |'
  prefs: []
  type: TYPE_TB
- en: '| AutoDAN-GA | 100.00 | 22.24 | 99.00 | 19.65 | 99.60 | 14.44 |'
  prefs: []
  type: TYPE_TB
- en: '| SMJ | 00.00 | 2.47 | 00.00 | 2.11 | 00.00 | 1.99 |'
  prefs: []
  type: TYPE_TB
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental Setups
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataset The dataset used for evaluation was collected by [Yu et al.](#bib.bib46)
    ([2023](#bib.bib46)), it contains 100 questions related to the contents that are
    illegal, toxic, discrimination, immoral and so on. The dataset was closer to reality
    because the questions were either manually written or generated through crowdsourcing.
  prefs: []
  type: TYPE_NORMAL
- en: Baselines There are two baselines for the experiment. The first baseline is
    by directly attacking the victim model by using the original questions. The second
    baseline chosen is AutoDAN-GA (Liu et al., [2023e](#bib.bib28)). The attack makes
    use of handcrafted jailbreak prompts as the initial population, using those prompts
    to automatically generate jailbreak prompts by adopting genetic algorithm for
    optimization purpose. For evaluation, we follow the original setting of AutoDAN-GA
    except for the LLMs’ response generation, instead of generating one sequence at
    a time, we use batch inference.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics To assess the performance of the jailbreak methods, we use Attack Success
    Rate (ASR) as the primary metric. To assess whether the attack was deemed successful,
    we refer to all the refusal keywords list provided by [Zou et al.](#bib.bib49)
    ([2023](#bib.bib49)), [Liu et al.](#bib.bib28) ([2023e](#bib.bib28)), and [Yu
    et al.](#bib.bib46) ([2023](#bib.bib46)). The refusal keywords list includes keywords
    such as “I apologize” and “is illegal and unethical”, if the paraphrased question
    $P_{ij}$.
  prefs: []
  type: TYPE_NORMAL
- en: For semantic meaningfulness, the paper uses three metrics to evaluate. The first
    one is the percentage of jailbreak prompts (Jailbreak Prompt) identified by “deberta-v3-base-prompt-injection”
    model as prompt injections (Laiyer.ai, [2023](#bib.bib11)), the lower the Jailbreak
    Prompt, the better the method performs against the model used to classify inputs
    into jailbreak prompts or normal questions. The second is the mean of the Semantic
    Similarity (Similarity) (Reimers & Gurevych, [2022a](#bib.bib37)), which measures
    the semantic textual similarity between pairs of sentences (original questions
    and respective jailbreak prompts), this paper uses the SOTA “all-mpnet-base-v2”
    model (Reimers & Gurevych, [2022b](#bib.bib38)) for evaluation. Lastly, [Qi et al.](#bib.bib35)
    ([2020](#bib.bib35)) proposed ONION defense against textual backdoor attacks,
    the defense is based on outlier words detection in a sentence. Since that could
    be an effective metric to differentiate between jailbreak prompts produced by
    AutoDAN and SMJ, the mean of the Number of Outlier Words (Outlier) within one
    sentence becomes the third metric.
  prefs: []
  type: TYPE_NORMAL
- en: Models Three open-sourced large language models are used to evaluate against
    the generated jailbreak prompts, they are Llama-2-7b-chat-hf (Touvron et al.,
    [2023](#bib.bib41)), Vicuna-7b-v1.5 (Zheng et al., [2023](#bib.bib47)), and Guanaco-7b
    (Dettmers et al., [2023](#bib.bib3)). Llama 2 is a language model with transformer
    architecture, it is pretrained on data from publicly available sources. Specifically,
    both Vicuna and Guanaco are fine-tuned based on Llama 2 using conversations from
    ShareGPT and the multilingual dataset OASST1, respectively. All three models have
    different sizes including 7B, 13B, and more. Moreover, all three LLMs are subject
    to LLaMA license for intended use.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.2.1 Attack Success Rate (ASR) and Similarity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For ASR, only the best paraphrased question with a Similarity higher than 0.7000
    is deemed jailbreak successfully for method SMJ. [Table 1](#S3.T1 "In 3.4 Implementation
    Details ‣ 3 Method ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak
    Prompts Against Open-source LLMs") records the mean of Similarity for all baselines
    and SMJ’s generated jailbreak prompts regardless of whether the jailbreak is successful
    or not for all the victim models.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 1](#S3.T1 "In 3.4 Implementation Details ‣ 3 Method ‣ Semantic Mirror
    Jailbreak: Genetic Algorithm Based Jailbreak Prompts Against Open-source LLMs")
    shows that SMJ is able to achieve higher ASR performance than both of the baselines.
    Specifically, SMJ outperforms AutoDAN-GA’s ASR by 35.4%, 18.8%, and 5.8% while
    attacking LLMs of Llama-2, Vicuna-7b, and Guanaco-7b, respectively. For Similarity,
    as compared to AutoDAN-GA, SMJ has a much higher average Similarity, surpassing
    AutoDAN-GA by at most 88.16%, which indicates that SMJ can produce jailbreak prompts
    that are more semantically meaningful when compared to the original questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, SMJ can effectively improve ASR and produce more semantically meaningful
    jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Jailbreak Prompt (JPt) and Outlier
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For semantic meaningfulness purpose, JPt and Outlier are also evaluated. [Table 2](#S3.T2
    "In 3.4 Implementation Details ‣ 3 Method ‣ Semantic Mirror Jailbreak: Genetic
    Algorithm Based Jailbreak Prompts Against Open-source LLMs") records the mean
    of JPt and Outlier for all baselines and SMJ’s generated jailbreak prompts regardless
    of whether the jailbreak is successful or not for all the victim models. For both
    metrics, the lower the metrics, the better the method performs.'
  prefs: []
  type: TYPE_NORMAL
- en: To the performance of JPt, none of the jailbreak prompts from SMJ were classified
    as jailbreak prompts, which is the same as that of the original question method.
    Meanwhile, to AutoDAN-GA, 100%, 99%, and 99.6% of jailbreak prompts were classified
    as jailbreak prompts for Llama-2, Vicuna-7b, and Guanaco-7b, respectively. Regarding
    the performance of Outlier, to compare with the original question method, SMJ’s
    Outliers are at least 1.22 times and at most 1.52 times higher, while AutoDAN-GA’s
    Outliers are at least 8.86 times and at most 13.65 times higher.
  prefs: []
  type: TYPE_NORMAL
- en: Those results again indicate that SMJ’s jailbreak prompts are more semantically
    meaningful, and the method is more resistant to defenses that use those metrics
    as thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Cross-model transferability. The notation * denotes a white-box scenario,
    notation ^($\dagger$) denotes Similarity is measured on jailbreak prompts with
    no Similarity or jailbreak validity limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Source Models | Method | Llama-2-7b-chat-hf | Vicuna-7b | Guanaco-7b |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ASR% | Similarity% | ASR% | Similarity% | ASR% | Similarity% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7b-chat-hf | AutoDAN-GA | 30.60* | 5.45^($\dagger$) |'
  prefs: []
  type: TYPE_TB
- en: '| SMJ-0% | 100.00* | 73.41*^($\dagger$) |'
  prefs: []
  type: TYPE_TB
- en: '|  | SMJ-70% | 66.00* | 79.41^($\dotplus$) |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7b | AutoDAN-GA | 1.00 | 9.00^($\dagger$) |'
  prefs: []
  type: TYPE_TB
- en: '| SMJ-0% | 2.20 | 75.24^($\dagger$) |'
  prefs: []
  type: TYPE_TB
- en: '|  | SMJ-70% | 1.40 | 72.68^($\dotplus$) |'
  prefs: []
  type: TYPE_TB
- en: '| Guana co-7b | AutoDAN-GA | 0.20 | 0.06^($\dagger$) |'
  prefs: []
  type: TYPE_TB
- en: '| SMJ-0% | 1.20 | 72.99^($\dagger$) |'
  prefs: []
  type: TYPE_TB
- en: '|  | SMJ-70% | 1.20 | 72.99^($\dotplus$) |'
  prefs: []
  type: TYPE_TB
- en: 4.2.3 Transferability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The transferability for all three methods is conducted by taking all jailbreak
    prompts regardless of whether the jailbreak is successful or not in the source
    models, using them to attack the target models in the white and black box scenarios.
    As shown in the [Section 4.2.2](#S4.SS2.SSS2 "4.2.2 Jailbreak Prompt (JPt) and
    Outlier ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm
    Based Jailbreak Prompts Against Open-source LLMs"), it records the ASR for AutoDAN-GA
    (regardless of the Similarity level), SMJ with Similarity higher than 0%, and
    70% for different transfer experiments. It also records Similarity measured on
    jailbreak prompts with Similarity $\geq 0\%$ and jailbreak successfully, and the
    Similarity measured on jailbreak prompts with no Similarity or jailbreak validity
    limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Section 4.2.2](#S4.SS2.SSS2 "4.2.2 Jailbreak Prompt (JPt) and Outlier ‣ 4.2
    Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak
    Prompts Against Open-source LLMs") indicates that SMJ outperforms AutoDAN-GA’s
    ASR in all cases except for when the target model is Guanaco-7b in both Similarity
    higher than 0% and 70% settings, SMJ improves AutoDAN-GA’s ASR by at most 38.8%
    and 18.4% for Similarity higher than 0% and 70% settings in black box scenario.
    For Similarity, SMJ achieves higher performance than AutoDAN-GA in all cases.
    For Similarity evaluated on jailbreak prompts with Similarity $\geq 0\%$ meanwhile
    jailbreak successfully.'
  prefs: []
  type: TYPE_NORMAL
- en: The results indicate that SMJ has better transferability in the black box scenario
    than AutoDAN-GA in most cases. Its jailbreak prompts that jailbreak successfully
    has higher Similarity than the baseline in all cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Attack Success Rate (ASR) against ONION defense.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Llama-2-7b-chat-hf + ONION | Vicuna-7b + ONION | Guanaco-7b + ONION
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Methods | ASR% | ASR% | ASR% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original question | 1.40 | 32.00 | 47.00 |'
  prefs: []
  type: TYPE_TB
- en: '| AutoDAN-GA | 2.00 | 13.40 | 15.00 |'
  prefs: []
  type: TYPE_TB
- en: '| SMJ | 66.00 | 98.60 | 100.00 |'
  prefs: []
  type: TYPE_TB
- en: 4.2.4 Attack Success Rate (ASR) against ONION defense
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The defense chosen was the ONION defense, the defense measures the fluency of
    a sentence by the perplexity computed by GPT-2 (Wolf et al., [2020](#bib.bib45)).
    Then using the decrease in the perplexity of the sentence while each words in
    the sentence were removed one by one to compute the suspicion scores. If the suspicion
    score is larger than a hyper-parameter suspicion threshold, then the word removed
    is deemed an outlier word. For the defense purpose against jailbreak prompts,
    we further add an outlier threshold for the number of outliers within a sentence.
    If the number of outliers within a sentence is higher than the outlier threshold,
    then the sentence would be considered as a jailbreak prompt, otherwise, the sentence
    would be regarded as a normal question without any intention for any jailbreak
    attack. By using this defense, we can effectively defend against jailbreak prompts.
    The suspicion threshold used in this experiment is 0, and the outlier threshold
    for all three methods used is the maximum outlier contained in the sentence for
    all SMJ’s jailbreak prompts with respect to different victim LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '[Table 4](#S4.T4 "In 4.2.3 Transferability ‣ 4.2.2 Jailbreak Prompt (JPt) and
    Outlier ‣ 4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm
    Based Jailbreak Prompts Against Open-source LLMs") shows that using ONION defense
    can distinguish between AutoDAN-GA and SMJ’s jailbreak prompts. Consequently,
    AutoDAN-GA’s ASR for all LLMs decreased by 28.6%, 66.4%, and 79.2% for Llama-2,
    Vicuna-7b, and Guanaco-7b, respectively. While the original question and SMJ’s
    ASR for all LLMs remain the same as that in [Table 1](#S3.T1 "In 3.4 Implementation
    Details ‣ 3 Method ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based Jailbreak
    Prompts Against Open-source LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: This experiment shows that SMJ can bypass defense and appears to generate jailbreak
    prompts that are more similar to normal harmful questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Ablation Study.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Similarity level |  | $\geq 0\%$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Methods | Models | ASR% | Simi% | ASR% | Simi% | ASR% | Simi% | ASR% | Simi%
    | ASR% | Simi% |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original question | Llama-2 | 1.40 | 100.00 | 1.40 | 100.00 | 1.40 | 100.00
    | 1.40 | 100.00 | 1.40 | 100.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna | 32.00 | 100.00 | 32.00 | 100.00 | 32.00 | 100.00 | 32.00 | 100.00
    | 32.00 | 100.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco | 47.00 | 100.00 | 47.00 | 100.00 | 47.00 | 100.00 | 47.00 | 100.00
    | 47.00 | 100.00 |'
  prefs: []
  type: TYPE_TB
- en: '| + initialization | Llama-2 | 100.00 | 71.18 | 82.00 | 75.33 | 58.40 | 79.41
    | 26.80 | 85.36 | 4.60 | 93.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna | 100.00 | 91.61 | 99.00 | 92.06 | 97.60 | 92.44 | 91.00 | 93.61 |
    67.00 | 96.32 |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco | 100.00 | 94.52 | 100.00 | 94.52 | 100.00 | 94.52 | 99.20 | 94.66
    | 82.00 | 96.55 |'
  prefs: []
  type: TYPE_TB
- en: '| + initialization + SGA | Llama-2 | 100.00 | 73.41 | 89.00 | 75.92 | 66.00
    | 79.41 | 29.00 | 85.71 | 5.80 | 93.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna | 100.00 | 92.13 | 99.40 | 92.40 | 98.60 | 92.62 | 92.40 | 93.73 |
    69.40 | 96.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Guanaco | 100.00 | 94.63 | 100.00 | 94.63 | 100.00 | 94.63 | 99.20 | 94.77
    | 82.40 | 96.60 |'
  prefs: []
  type: TYPE_TB
- en: 4.2.5 Ablation Study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are three ablations conducted in this experiment. The ablation1 is to
    only use the original prompts to attack the victim LLMs. The ablation2 is to add
    the initialization process to ablation1\. Ablation3 (SMJ) is to add the standard
    genetic algorithm (SGA) to ablation2\. [Table 5](#S4.T5 "In 4.2.4 Attack Success
    Rate (ASR) against ONION defense ‣ 4.2.2 Jailbreak Prompt (JPt) and Outlier ‣
    4.2 Results ‣ 4 Experiments ‣ Semantic Mirror Jailbreak: Genetic Algorithm Based
    Jailbreak Prompts Against Open-source LLMs") contain the results of all three
    ablations for the three LLMs at different similarity levels. Specifically, “Llama-2”
    stands for “Llama-2-7b-chat-hf”, “Vicuna” stands for “Vicuna-7b”, “Guanaco” stands
    for “Guanaco-7b”, and “Simi” stands for “Similarity”. Moreover, both metrics of
    ASR and Similarity are based on jailbreak prompts that are within certain similarity
    levels meanwhile jailbreak successfully.'
  prefs: []
  type: TYPE_NORMAL
- en: For both ASR and Similarity, SMJ performs better than or equal to ablation2
    for all LLMs and all similarity levels except for the Similarity of Vicuna-7b
    at the similarity level greater than or equal to 90%. One possible explanation
    could be the ASR for SMJ is higher than ablation2, so the number of jailbreak
    prompts deemed jailbreak successfully from SMJ is higher, some of them might have
    lower Similarity, causing SMJ’s Similarity of Vicuna-7b at the similarity level
    greater than or equal to 90% lower than ablation2’s Similarity. Moreover, SMJ
    can improve robust LLM Llama-2’s ASR by at most 7.6%, improve Vicuna-7b’s ASR
    by at most 2.4%, and improve Guanaco-7b’s ASR by at most 0.4%. For Similarity,
    SMJ can improve robust model Llama-2 by at most 2.23%, improve Vicuna-7b by at
    most 0.52%, and improve Guanaco-7b by at most 0.11%. For LLMs Vicuna-7b and Guanaco-7b,
    as the similarity levels get higher, the more SMJ can improve ablation2 in ASR,
    the less SMJ can improve ablation2 in Similarity.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, SMJ can improve ablation2’s ASR and Similarity in most cases. SMJ can
    improve both Llama-2 and Vicuna-7b’s ASR the most at higher similarity levels
    while improving Similarity the most at lower similarity levels.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This paper proposes SMJ, a method that uses paraphrased questions as jailbreak
    prompts and optimizes fitness functions using the genetic algorithm to ensure
    jailbreak prompts’ semantic meaningfulness meanwhile optimizing both the jailbreak
    prompts’ semantic meaningfulness and the attack success rate (ASR) concurrently.
    This method can outperform AutoDAN-GA’s ASR. Moreover, by evaluating the three
    metrics: Jailbreak Prompt, Similarity, and Outlier, the results indicate that
    SMJ’s jailbreak prompts perform better than AutoDAN-GA in semantic meaningfulness.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Ethics Statement and Countermeasure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the research proposes a method to allow LLMs to generate harmful responses
    and hate speech, we believe that by referring to the current jailbreak attacks,
    more research on jailbreak attack defenses will emerge, which will help boost
    the safeguards of LLMs. Hence, we hope our work will not pose significant dangers
    to LLMs in the long term but rather clarify the potential vulnerabilities of LLMs.
    For countermeasure of SMJ, if an attacker uses lots of jailbreak prompts with
    similar Similarity to continuously query the LLMs, a possible defense can be detecting
    the Similarity of inputs, and refusing to answer those inputs with high Similarity.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bang et al. (2023) Bang, Y., Cahyawijaya, S., Lee, N., Dai, W., Su, D., Wilie,
    B., Lovenia, H., Ji, Z., Yu, T., Chung, W., et al. A multitask, multilingual,
    multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity.
    *arXiv preprint arXiv:2302.04023*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
    H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing,
    E. P. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality,
    March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,
    L. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goldstein et al. (2023) Goldstein, J. A., Sastry, G., Musser, M., DiResta,
    R., Gentzel, M., and Sedova, K. Generative language models and automated influence
    operations: Emerging threats and potential mitigations. *arXiv preprint arXiv:2301.04246*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hazell (2023) Hazell, J. Large language models can be used to effectively scale
    spear phishing campaigns. *arXiv preprint arXiv:2305.06972*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2023) He, B., Liu, J., Li, Y., Liang, S., Li, J., Jia, X., and Cao,
    X. Generating transferable 3d adversarial point cloud via random perturbation
    factorization. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023) Huang, Y., Gupta, S., Xia, M., Li, K., and Chen, D. Catastrophic
    jailbreak of open-source llms via exploiting generation. *arXiv preprint arXiv:2310.06987*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iyyer et al. (2018) Iyyer, M., Wieting, J., Gimpel, K., and Zettlemoyer, L.
    Adversarial example generation with syntactically controlled paraphrase networks.
    *arXiv preprint arXiv:1804.06059*, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain et al. (2023) Jain, N., Schwarzschild, A., Wen, Y., Somepalli, G., Kirchenbauer,
    J., Chiang, P.-y., Goldblum, M., Saha, A., Geiping, J., and Goldstein, T. Baseline
    defenses for adversarial attacks against aligned language models. *arXiv preprint
    arXiv:2309.00614*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kang et al. (2023) Kang, D., Li, X., Stoica, I., Guestrin, C., Zaharia, M.,
    and Hashimoto, T. Exploiting programmatic behavior of llms: Dual-use through standard
    security attacks. *arXiv preprint arXiv:2302.05733*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laiyer.ai (2023) Laiyer.ai. Fine-tuned deberta-v3 for prompt injection detection,
    2023. URL [https://huggingface.co/laiyer/deberta-v3-base-prompt-injection](https://huggingface.co/laiyer/deberta-v3-base-prompt-injection).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023a) Li, H., Guo, D., Fan, W., Xu, M., and Song, Y. Multi-step
    jailbreaking privacy attacks on chatgpt. *arXiv preprint arXiv:2304.05197*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) Li, J., Zhang, H., Liang, S., Dai, P., and Cao, X. Privacy-enhancing
    face obfuscation guided by semantic-aware attribution maps. *IEEE Transactions
    on Information Forensics and Security*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2023a) Liang, J., Liang, S., Liu, A., Ma, K., Li, J., and Cao,
    X. Exploring inconsistent knowledge distillation for object detection with data
    augmentation. In *Proceedings of the 31st ACM International Conference on Multimedia*,
    pp.  768–778, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2020) Liang, S., Wei, X., Yao, S., and Cao, X. Efficient adversarial
    attacks for visual object tracking. In *CEuropean Conference on Computer Vision*,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2021) Liang, S., Wei, X., and Cao, X. Generate more imperceptible
    adversarial examples for object detection. In *ICML 2021 Workshop on Adversarial
    Machine Learning*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022a) Liang, S., Li, L., Fan, Y., Jia, X., Li, J., Wu, B., and
    Cao, X. A large-scale multiple-objective method for black-box attack against object
    detection. In *European Conference on Computer Vision*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2022b) Liang, S., Liu, A., Liang, J., Li, L., Bai, Y., and Cao,
    X. Imitated detectors: Stealing knowledge of black-box object detectors. In *Proceedings
    of the 30th ACM International Conference on Multimedia*, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2022c) Liang, S., Wu, B., Fan, Y., Wei, X., and Cao, X. Parallel
    rectangle flip attack: A query-based black-box attack against object detection.
    *arXiv preprint arXiv:2201.08970*, 2022c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2023b) Liang, S., Zhu, M., Liu, A., Wu, B., Cao, X., and Chang,
    E.-C. Badclip: Dual-embedding guided backdoor attack on multimodal contrastive
    learning. *arXiv preprint arXiv:2311.12075*, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019) Liu, A., Liu, X., Fan, J., Ma, Y., Zhang, A., Xie, H., and
    Tao, D. Perceptual-sensitive gan for generating adversarial patches. In *AAAI*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020a) Liu, A., Huang, T., Liu, X., Xu, Y., Ma, Y., Chen, X., Maybank,
    S. J., and Tao, D. Spatiotemporal attacks for embodied agents. In *ECCV*, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2020b) Liu, A., Wang, J., Liu, X., Cao, B., Zhang, C., and Yu, H.
    Bias-based universal adversarial patch attack for automatic check-out. In *ECCV*,
    2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Liu, A., Guo, J., Wang, J., Liang, S., Tao, R., Zhou, W.,
    Liu, C., Liu, X., and Tao, D. X-adv: Physical adversarial object attacks against
    x-ray prohibited item detection. In *USENIX Security Symposium*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Liu, A., Tang, S., Liang, S., Gong, R., Wu, B., Liu, X.,
    and Tao, D. Exploring the relationship between architectural design and adversarially
    robust generalization. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp.  4096–4107, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023c) Liu, A., Zhang, X., Xiao, Y., Zhou, Y., Liang, S., Wang,
    J., Liu, X., Cao, X., and Tao, D. Pre-trained trojan attacks for visual recognition.
    *arXiv preprint arXiv:2312.15172*, 2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023d) Liu, J., Zhu, S., Liang, S., Zhang, J., Fang, H., Zhang,
    W., and Chang, E.-C. Improving adversarial transferability by stable diffusion.
    *arXiv preprint arXiv:2311.11017*, 2023d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023e) Liu, X., Xu, N., Chen, M., and Xiao, C. Autodan: Generating
    stealthy jailbreak prompts on aligned large language models. *arXiv preprint arXiv:2310.04451*,
    2023e.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meta (2024) Meta. Llama 2 - acceptable use policy - meta ai. [https://ai.meta.com/llama/use-policy/](https://ai.meta.com/llama/use-policy/),
    2024. Accessed: 2024-01-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2024a) OpenAI. Introducing chatgpt. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt),
    2024a. Accessed: 2024-01-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2024b) OpenAI. Usage policies. [https://openai.com/policies/usage-policies](https://openai.com/policies/usage-policies),
    2024b. Accessed: 2024-01-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2024c) OpenAI. Moderation. [https://platform.openai.com/docs/guides/moderation/overview](https://platform.openai.com/docs/guides/moderation/overview),
    2024c. Accessed: 2024-01-18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language
    models to follow instructions with human feedback. *Advances in Neural Information
    Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pegoraro et al. (2023) Pegoraro, A., Kumari, K., Fereidooni, H., and Sadeghi,
    A.-R. To chatgpt, or not to chatgpt: That is the question! *arXiv preprint arXiv:2304.01487*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2020) Qi, F., Chen, Y., Li, M., Yao, Y., Liu, Z., and Sun, M. Onion:
    A simple and effective defense against textual backdoor attacks. *arXiv preprint
    arXiv:2011.10369*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qi et al. (2021) Qi, F., Yao, Y., Xu, S., Liu, Z., and Sun, M. Turn the combination
    lock: Learnable textual backdoor attacks via word substitution. *arXiv preprint
    arXiv:2106.06361*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers & Gurevych (2022a) Reimers, N. and Gurevych, I. Semantic textual similarity.
    [https://www.sbert.net/docs/usage/semantic_textual_similarity.html#semantic-textual-similarity](https://www.sbert.net/docs/usage/semantic_textual_similarity.html#semantic-textual-similarity),
    2022a. Accessed: 2024-01-20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers & Gurevych (2022b) Reimers, N. and Gurevych, I. Pretrained models.
    [https://www.sbert.net/docs/pretrained_models.html](https://www.sbert.net/docs/pretrained_models.html),
    2022b. Accessed: 2024-01-20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Shen, X., Chen, Z., Backes, M., Shen, Y., and Zhang, Y.
    ” do anything now”: Characterizing and evaluating in-the-wild jailbreak prompts
    on large language models. *arXiv preprint arXiv:2308.03825*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Sun, C., Xu, C., Yao, C., Liang, S., Wu, Y., Liang, D., Liu,
    X., and Liu, A. Improving robust fariness via balance adversarial training. In
    *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 37, pp. 
    15161–15169, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models, 2023. *URL https://arxiv. org/abs/2307.09288*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Wang, J., Liu, A., Yin, Z., Liu, S., Tang, S., and Liu,
    X. Dual attention suppression attack: Generate adversarial camouflage in physical
    world. In *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Wang, Y., Shi, H., Min, R., Wu, R., Liang, S., Wu, Y., Liang,
    D., and Liu, A. Adaptive perturbation generation for multiple backdoors detection.
    *arXiv preprint arXiv:2209.05244*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2018) Wei, X., Liang, S., Chen, N., and Cao, X. Transferable adversarial
    attacks for image and video object detection. *arXiv preprint arXiv:1811.12641*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2020) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer,
    S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T. L., Gugger,
    S., Drame, M., Lhoest, Q., and Rush, A. M. Transformers: State-of-the-art natural
    language processing. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing: System Demonstrations*, pp.  38–45, Online, October
    2020\. Association for Computational Linguistics. URL [https://www.aclweb.org/anthology/2020.emnlp-demos.6](https://www.aclweb.org/anthology/2020.emnlp-demos.6).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Yu, J., Lin, X., and Xing, X. Gptfuzzer: Red teaming large
    language models with auto-generated jailbreak prompts. *arXiv preprint arXiv:2309.10253*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,
    Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging llm-as-a-judge with
    mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Zhou, J., Zhang, Y., Luo, Q., Parker, A. G., and De Choudhury,
    M. Synthetic lies: Understanding ai-generated misinformation and evaluating algorithmic
    and human solutions. In *Proceedings of the 2023 CHI Conference on Human Factors
    in Computing Systems*, pp.  1–20, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. Universal
    and transferable adversarial attacks on aligned language models. *arXiv preprint
    arXiv:2307.15043*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Refusal keywords list.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The refusal keywords list contains all lists provided by [Zou et al.](#bib.bib49)  ([2023](#bib.bib49)),
    [Liu et al.](#bib.bib28)  ([2023e](#bib.bib28)), and [Yu et al.](#bib.bib46)  ([2023](#bib.bib46)).
  prefs: []
  type: TYPE_NORMAL
- en: '| “I’m sorry” |'
  prefs: []
  type: TYPE_TB
- en: '| “Sorry” |'
  prefs: []
  type: TYPE_TB
- en: '| “I am sorry” |'
  prefs: []
  type: TYPE_TB
- en: '| “I apologize” |'
  prefs: []
  type: TYPE_TB
- en: '| “As an” |'
  prefs: []
  type: TYPE_TB
- en: '| “As an AI” |'
  prefs: []
  type: TYPE_TB
- en: '| “I’m an” |'
  prefs: []
  type: TYPE_TB
- en: '| “I’m just” |'
  prefs: []
  type: TYPE_TB
- en: '| “As a language model” |'
  prefs: []
  type: TYPE_TB
- en: '| “As an Assistant” |'
  prefs: []
  type: TYPE_TB
- en: '| “I cannot” |'
  prefs: []
  type: TYPE_TB
- en: '| “I can’t” |'
  prefs: []
  type: TYPE_TB
- en: '| “I do not” |'
  prefs: []
  type: TYPE_TB
- en: '| “Hello!” |'
  prefs: []
  type: TYPE_TB
- en: '| “is illegal and unethical” |'
  prefs: []
  type: TYPE_TB
- en: '| “I’m really sorry” |'
  prefs: []
  type: TYPE_TB
- en: '| “unethical” |'
  prefs: []
  type: TYPE_TB
- en: '| “not ethical” |'
  prefs: []
  type: TYPE_TB
- en: '| “illegal” |'
  prefs: []
  type: TYPE_TB
- en: '| “not legal” |'
  prefs: []
  type: TYPE_TB
- en: '| “My apologies” |'
  prefs: []
  type: TYPE_TB
- en: '| “but I cannot” |'
  prefs: []
  type: TYPE_TB
- en: '| “If you have any other non-malicious requests” |'
  prefs: []
  type: TYPE_TB
- en: '| “I’m not able to provide” |'
  prefs: []
  type: TYPE_TB
- en: '| “I am really sorry” |'
  prefs: []
  type: TYPE_TB
- en: '| “I cannot create” |'
  prefs: []
  type: TYPE_TB
- en: '| “I cannot support” |'
  prefs: []
  type: TYPE_TB
- en: '| “I can’t provide” |'
  prefs: []
  type: TYPE_TB
- en: '| “I can’t assist” |'
  prefs: []
  type: TYPE_TB
- en: '| “I must” |'
  prefs: []
  type: TYPE_TB
- en: '| “is not capable of” |'
  prefs: []
  type: TYPE_TB
- en: '| “never” |'
  prefs: []
  type: TYPE_TB
- en: '| “$<$” |'
  prefs: []
  type: TYPE_TB
- en: '| “Sorry, I am sorry” |'
  prefs: []
  type: TYPE_TB
- en: '| “I’m really sorry” |'
  prefs: []
  type: TYPE_TB
- en: '| “Hello” |'
  prefs: []
  type: TYPE_TB
- en: '| “OpenAI” |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Detailed Algorithms.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The detailed algorithms for SMJ, population initialization, fitness evaluation,
    crossover, and selection processes.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Detailed Semantic Mirror Jailbreak (SMJ)
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:  $$harmful\_question,substitute\_questions,max\_generations,population\_size,L_{refuse},seman\_simi,\newline'
  prefs: []
  type: TYPE_NORMAL
- en: 'static\_threshold$$  Output:  $best\_solution[0]$  $$init\_1,best\_solution,top,bottom,stop\leftarrow
    FITNESS\_EVALUATION(harmful\_question,substitute\_questions,\newline'
  prefs: []
  type: TYPE_NORMAL
- en: L_{refuse},seman\_simi,selection\_similarity\_region,top,stop,first=True)$$  $1$2  $$init\_2,best\_solution,top,bottom,stop\leftarrow
    FITNESS\_EVALUATION(harmful\_question,crossovers,L_{refuse},\newline
  prefs: []
  type: TYPE_NORMAL
- en: seman\_simi,selection\_similarity\_region,top,stop,first=True)$$  $current\_generation\leftarrow
    1$    $$o,best\_solution,top,bottom,stop\leftarrow FITNESS\_EVALUATION(harmful\_question,crossovers,L_{refuse},\newline
  prefs: []
  type: TYPE_NORMAL
- en: seman\_simi,selection\_similarity\_region,top,stop,first=False)$$    if $top$  end while
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Population Initialization
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:  $1$2, $$count\_down\_threhold,\newline'
  prefs: []
  type: TYPE_NORMAL
- en: 'similarity\_decrement,seman\_simi$$  Output:  $substitute\_questions$ for $harmful\_question$  end for'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 Fitness Evaluation
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:  $$harmful\_question,substitute\_questions,L_{refuse},seman\_simi,selection\_similarity\_region,top,stop,\newline'
  prefs: []
  type: TYPE_NORMAL
- en: 'first$$  Output: dictionary of population $f$ with $value$ then             $top\leftarrow
    f[seman\_simi][0]$       end if    end for  end if'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 5 Crossover
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:  $harmful\_question,substitute\_questions,stop,first$          if $crossover$          end if       end if    end for  end if'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 6 Selection
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:  $offspring,population\_size$]  end if'
  prefs: []
  type: TYPE_NORMAL
