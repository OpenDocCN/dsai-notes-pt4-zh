- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:43'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Relative Preference Optimization: Enhancing LLM Alignment through Contrasting
    Responses across Identical and Diverse Prompts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10958](https://ar5iv.labs.arxiv.org/html/2402.10958)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yueqin Yin The University of Texas at Austin Zhendong Wang The University of
    Texas at Austin Microsoft Azure AI Yi Gu The University of Texas at Austin Hai
    Huang Google Weizhu Chen Microsoft Azure AI Mingyuan Zhou The University of Texas
    at Austin Google
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the field of large language models (LLMs), aligning models with the diverse
    preferences of users is a critical challenge. Direct Preference Optimization (DPO)
    has played a key role in this area. It works by using pairs of preferences derived
    from the same prompts, and it functions without needing an additional reward model.
    However, DPO does not fully reflect the complex nature of human learning, which
    often involves understanding contrasting responses to not only identical but also
    similar questions. To overcome this shortfall, we propose Relative Preference
    Optimization (RPO). RPO is designed to discern between more and less preferred
    responses derived from both identical and related prompts. It introduces a contrastive
    weighting mechanism, enabling the tuning of LLMs using a broader range of preference
    data, including both paired and unpaired sets. This approach expands the learning
    capabilities of the model, allowing it to leverage insights from a more varied
    set of prompts. Through empirical tests, including dialogue and summarization
    tasks, and evaluations using the AlpacaEval2.0 leaderboard, RPO has demonstrated
    a superior ability to align LLMs with user preferences and to improve their adaptability
    during the training process. The PyTorch code necessary to reproduce the results
    presented in the paper will be made available on GitHub¹¹1[https://github.com/yinyueqin/relative-preference-optimization](https://github.com/yinyueqin/relative-preference-optimization)
    for public access.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) like ChatGPT (OpenAI, [2023](#bib.bib11)) and LLaMa
    (Touvron et al., [2023](#bib.bib28)) have initiated a transformative era in artificial
    intelligence. These models exhibit remarkable abilities in fields such as natural
    language processing, logical reasoning, and programming (Pan et al., [2023](#bib.bib13);
    Tian et al., [2023](#bib.bib26)). Trained on extensive, unsupervised datasets,
    they adeptly handle complex tasks and excel in zero-shot and few-shot learning
    scenarios. However, the diversity inherent in these datasets, while enriching
    content variety, presents alignment challenges. The vast array of human thought
    and expression within these datasets, embodying different goals, priorities, and
    values, can lead to outputs that do not consistently align with specific human
    preferences or desired outcomes, particularly in precision-demanding and nuanced
    contexts (Srivastava et al., [2022](#bib.bib22); Thoppilan et al., [2022](#bib.bib25);
    Bubeck et al., [2023](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: To address alignment challenges, Supervised Fine-Tuning (SFT) (Wei et al., [2022](#bib.bib31);
    Wang et al., [2023](#bib.bib30); Yu et al., [2024](#bib.bib32)) is commonly used,
    tailoring LLMs to tasks with labeled data. While effective, SFT may not fully
    capture the breadth of preference learning, essential for generating outputs aligned
    with human expectations beyond technical accuracy, encompassing ethical and contextual
    considerations. Expanding on SFT, Reinforcement Learning from Human Feedback (RLHF)
    (Christiano et al., [2017](#bib.bib5); Stiennon et al., [2020](#bib.bib23); Ouyang
    et al., [2022](#bib.bib12); Ramamurthy et al., [2023](#bib.bib17)) critically
    aligns LLMs with human preferences. Despite its effectiveness, RLHF is resource-intensive,
    relying on extensive human-annotated data to train a reward model and sophisticated
    RL algorithms, leading to a costly and labor-intensive process. The distribution
    gap between model-generated outputs and training data poses a major hurdle, requiring
    continuous updates and re-annotations for sustained effectiveness, complicating
    and increasing costs associated with achieving human-aligned LLMs (Yuan et al.,
    [2023](#bib.bib33)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e00255c0dc7a56fd67e6682aaded1926.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An example illustrates how DPO and RPO utilize contrastive responses
    with human preferences to achieve model alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: To address the intricacies and challenges inherent in RLHF, the Direct Preference
    Optimization (DPO) (Rafailov et al., [2023](#bib.bib15)) approach directly fine-tunes
    the language model’s policy to more closely align with human preferences, thereby
    obviating the need for a separate and often complex reward model. Central to DPO
    is the utilization of pairwise preferred and rejected responses for each prompt,
    which form the foundation for developing a method to optimize preferences effectively.
    However, training a model to learn from individual preferences for each example
    may not fully encapsulate the complexity of human learning. Human cognition often
    involves interpreting divergent responses, not only to identical questions but
    also to those that are similar, highlighting the multifaceted nature of comprehension
    and preference formation (Dahlin et al., [2018](#bib.bib7)). Moreover, obtaining
    pairwise preference data can pose challenges and incur substantial costs, particularly
    in sensitive domains such as healthcare and personal services, where careful attention
    to ethical considerations is paramount.
  prefs: []
  type: TYPE_NORMAL
- en: Our inspiration draws from the human learning process, where valuable insights
    often arise from the juxtaposition of successful examples and relevant failures
    (Dahlin et al., [2018](#bib.bib7)). To emulate this, we introduce Relative Preference
    Optimization (RPO). This approach involves analyzing prompt similarities at the
    semantic level within each mini-batch, allowing us to classify pairs as either
    highly related or unrelated. We construct a contrast matrix that instructs the
    model to distinguish between preferred and rejected responses, applicable to both
    identical and semantically related prompts. We have developed three weighting
    strategies to recalibrate the comparison of each contrastive pair. Our findings
    reveal that reweighting based on prompt similarities significantly enriches model
    alignment with human preferences, offering a more nuanced understanding. Furthermore,
    RPO inherently excels in handling non-pairwise preference data by considering
    semantically related contrastive pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in [Figure 1](#S1.F1 "In 1 Introduction ‣ Relative Preference
    Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical
    and Diverse Prompts"), we are interested in the question “Explain the concept
    of photosynthesis.” DPO applies penalties for incorrect responses and rewards
    for precise responses generated for the specific prompt. Conversely, our method
    RPO emphasizes the semantic connections between various prompts. For instance,
    the prompt “Describe the importance of sunlight in plant growth” is conceptually
    similar, and its responses might intersect with those of the initial question.
    Under RPO, a dismissed answer for the second prompt could also be seen as a misstep
    for the first prompt. Thus, RPO penalizes both $y_{l,1}$. It is crucial to note
    that not all prompts are semantically related enough to form effective contrastive
    pairs. RPO incorporates a reweighting mechanism, whereby unrelated prompts are
    given less emphasis during training. RPO expands the learning horizon of the model,
    empowering it to leverage insights from a broader range of prompts, mirroring
    the human learning process more closely.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We empirically evaluate RPO on the state-of-the-art (SoTA) LLMs, such as LLama2-7/13B
    and Mistral-7B. We compare RPO with SoTA human preference alignment methods, and
    RPO significantly outperforms the baselines on typical dialogue and summarization
    tasks and assessments on the AlpacaEval2.0 leaderboard. The core contributions
    of RPO are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Innovative preference contrast learning strategy: RPO enriches the landscape
    of preference optimization with novel contrastive learning techniques.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adaptability across varied contexts: Exhibiting exceptional adaptability, RPO
    is adept across a multitude of scenarios, whether or not explicit preference pairs
    are present, confirming its utility as a versatile tool in language model applications.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Enhanced performance in critical language tasks: Demonstrating superiority
    over established methods like DPO, IPO, and KTO, the proposed RPO excels in key
    language processing tasks, including text summarization and dialogue generation,
    showcasing its improved alignment with human preferences.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The field of large language models (LLMs) has made notable strides in aligning
    these models with human preferences, driven by innovative fine-tuning methodologies.
    In this exploration, we delve into key methodologies, including SFT, RLHF, DPO,
    and their respective variations.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Supervised Fine-Tuning (SFT)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'SFT (Sanh et al., [2021](#bib.bib19); Chung et al., [2022](#bib.bib6)) serves
    as a foundational phase in the training of LLMs. It entails customizing a pre-trained
    model for specific tasks with a smaller set of labeled data. The goal here is
    to adjust the model’s outputs to mirror the high-quality responses in the training
    set. The main objective of SFT revolves around reducing the negative log-likelihood
    loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L_{\text{SFT}}(\theta)=-\mathbb{E}_{(x,y)\sim D}[\log p_{\theta}(y&#124;x)],$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $p_{\theta}(y|x)$ the labeled data.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Reinforcement Learning Fine-Tuning (RLHF)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RLHF builds upon the foundation of SFT, employing RL to better align the model
    with human preferences (Ouyang et al., [2022](#bib.bib12)). The initial phase
    of RLHF involves learning a reward model from human preference data. This process
    typically utilizes the Bradley-Terry model (Bradley and Terry, [1952](#bib.bib3)),
    which assesses the reward $r^{*}(y|x)$. The Bradley-Terry model determines the
    preference probability as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\sigma(\cdot)$ of preference pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'The next phase is the RL fine-tuning process that seeks to optimize the policy
    $\pi_{\theta}$, usually derived from the SFT model. This optimization integrates
    KL-regularization to mitigate overfitting and preserve response diversity:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\max_{\pi_{\theta}}\mathbb{E}_{x\sim D,y\sim\pi_{\theta}(y&#124;x)}[r_{\phi}(x,y)]-\beta\mathrm{KL}[\pi_{\theta}(y&#124;x)&#124;&#124;\pi_{\text{ref}}(y&#124;x)],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\beta$ by estimating gradients and collecting new data from the current
    policy and reward model. A notable challenge in RLHF is managing the discrete
    nature of language generation, which complicates gradient back-propagation from
    the reward function to the policy.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Direct Preference Optimization (DPO)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DPO (Rafailov et al., [2023](#bib.bib15)) offers an efficient approach by directly
    aligning a language model with human preferences, thus eliminating the need for
    a separate reward model. Utilizing direct human feedback, DPO refines the policy
    $\pi_{\theta}$ to better match nuanced human preferences. The objective of DPO
    is formulated as the following pairwise loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\beta$ that normalizes the reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}r(x,y)=\beta\log\frac{\pi_{\theta}(y&#124;x)}{\pi_{ref}(y&#124;x)}+\beta\log
    Z(x).\end{split}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: DPO’s primary benefit lies in its stable training process, providing a more
    direct means of aligning models with human preferences. However, DPO’s applicability
    is somewhat limited as it strictly defines its loss function based on the reward
    difference between chosen and rejected responses originating from the same prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Identity Preference Optimization (IPO)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Identity Preference Optimization (IPO) (Azar et al., [2024](#bib.bib1)) addresses
    the overfitting challenge within the DPO framework. IPO introduces a regularization
    term into the DPO’s loss function to maintain a balance between optimizing for
    human preferences and generalizing beyond the training data. The IPO loss function
    is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $\beta$ serves as a regularization parameter. IPO enhances the training
    process by ensuring a more balanced response selection, contributing to the robustness
    of preference-based language models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Kahneman-Tversky Optimization (KTO)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kahneman-Tversky Optimization (KTO) (Ethayarajh et al., [2023](#bib.bib8))
    diverges from the preference likelihood maximization used in DPO. Instead, KTO
    focuses on maximizing the utility of model outputs, informed by the human value
    function derived from Kahneman-Tversky’s prospect theory. This adaptation to language
    models allows KTO to operate without the necessity of preference pairs, thereby
    streamlining data requirements. The KTO loss function is formalized as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{KTO}(\pi_{\theta},\pi_{ref})=\mathbb{E}_{x,y\sim D}[w(y)(1-h(x,y;\beta))],$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $h(x,y;\beta)\!=\!\begin{cases}\sigma(g(x,y;\beta))&amp;\!\text{if }y\text{
    is desirable given }x\\ \sigma(-g(x,y;\beta))&amp;\!\text{if }y\text{ is undesirable
    given }x\end{cases}$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: KTO operates in unpaired data scenarios by independently processing chosen and
    rejected samples. As a concurrent work, our approach employs alternative mechanisms
    to ensure its applicability in contexts involving unpaired data.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Relative Preference Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The traditional DPO framework aligns language model outputs with human preferences
    using pairwise data, where each pair is composed of a preferred (win) and rejected
    (lose) sample for the same prompt. However, this approach is limited to situations
    where such pairwise preference data is accessible, failing to exploit the valuable
    comparative insights that could be derived from contrasting diverse samples across
    a range of prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'In response, our RPO framework encompasses a wider array of preference data,
    including non-paired samples. This development not only improves the use of existing
    preference data but also facilitates model training in complex scenarios where
    pair-wise data is not readily obtainable. More specifically, RPO integrates preference
    pairs derived from prompts that are semantically related but not identical, as
    shown in [Figure 1](#S1.F1 "In 1 Introduction ‣ Relative Preference Optimization:
    Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse
    Prompts"). Through dynamic calculation of relative reward weights based on prompt
    similarities, our method enhances the model’s ability to learn from a wider array
    of human feedback, leading to better generalization and more robust preference
    alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Contrast Matrix Construction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a228dace5160904a873d1d0f6eef3d22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: DPO requires paired preference data derived from identical prompts.
    RPO can utilize preference data from either the same or different prompts for
    constructing contrastive samples. Here, $y_{w}$ denotes lose responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In RPO, the contrast matrix is a pivotal component that facilitates the comparison
    between win and lose responses to derive meaningful insights for model training.
    As shown in [Figure 2](#S3.F2 "In 3.1 Contrast Matrix Construction ‣ 3 Relative
    Preference Optimization ‣ Relative Preference Optimization: Enhancing LLM Alignment
    through Contrasting Responses across Identical and Diverse Prompts"), the construction
    of this matrix varies depending on whether the available data is paired or unpaired,
    allowing for flexibility in training dynamics.'
  prefs: []
  type: TYPE_NORMAL
- en: Paired Data Scenario. In situations where each win response is associated with
    a corresponding lose response from the same prompt, the contrast matrix is an
    $M\times M$, the score reflects the direct comparison within the same prompt,
    while off-diagonal elements represent the relative reward differences across distinct
    prompts. In this context, DPO is limited to using only the diagonal terms of the
    contrast matrix, while RPO takes into account all pairings within the matrix,
    encompassing a broader range of preference comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Unpaired Data Scenario. In cases where the dataset contains unpaired win and
    lose responses, the contrast matrix transforms into an $M\times N$ lose response,
    without the constraint of originating from the same prompt. This allows for a
    more extensive range of comparisons, as any win samples can be contrasted with
    any lose samples, harnessing the thematic connections within the dataset to enrich
    the model’s preference learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each win response $y_{w,i}$ is computed as the difference in rewards associated
    with each response, as defined by the following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\begin{split}s_{ij}&amp;=r_{w,i}-r_{l,j},\\ r_{w,i}&amp;=\beta\log\frac{\pi_{\theta}(y_{w,i}&#124;x_{i})}{\pi_{ref}(y_{w,i}&#124;x_{i})}+\beta\log
    Z(x_{i}),\\'
  prefs: []
  type: TYPE_NORMAL
- en: r_{l,j}&amp;=\beta\log\frac{\pi_{\theta}(y_{l,j}&#124;x_{j})}{\pi_{ref}(y_{l,j}&#124;x_{j})}+\beta\log
    Z(x_{j}),\end{split}$$ |  | (10) |
  prefs: []
  type: TYPE_NORMAL
- en: where $\beta$ lose response. By calculating the contrastive scores across the
    matrix, RPO enables a comprehensive evaluation of the relative preference among
    all potential contrastive pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Weighting Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc41c2e4e1b04f200d398880b0cceec5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Three weighting strategies in RPO: (a) Uniform Weighting assigns
    equal weight to all elements within the contrast matrix, (b) Diagonal Emphasis
    Weighting Strategy allocates increased weight to diagonal elements while equalizing
    the rest, and (c) Embedding Distance Reweighting adjusts weights according to
    embedding distance between prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After forming the contrast matrix for each batch in RPO, we deploy diverse
    strategies to assign differentiated weights to each comparison pair. These weights
    crucially determine the relative influence of different comparison pairs in the
    final loss computation. We propose a strategy that reweights the contrast matrix
    based on the distance between prompt feature embeddings for preference learning.
    For distinct data configurations, we have introduced two additional, simpler weighting
    strategies alongside our primary Embedding Distance Reweighting Strategy, as shown
    in [Figure 3](#S3.F3 "In 3.2 Weighting Strategies ‣ 3 Relative Preference Optimization
    ‣ Relative Preference Optimization: Enhancing LLM Alignment through Contrasting
    Responses across Identical and Diverse Prompts").'
  prefs: []
  type: TYPE_NORMAL
- en: Embedding Distance Reweighting Strategy.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The incorporation of prompt similarity plays a pivotal role in contrastive
    analysis, applicable to both paired and unpaired datasets. This technique involves
    calculating the cosine distance $d=\cos(f(x_{w}),f(x_{l}))$ assigned to each pair
    of responses:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tilde{\omega}=\exp\left(-\frac{d}{\tau}\right)=\exp\left(-\frac{\cos(f(x_{w}),f(x_{l}))}{\tau}\right).$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $f$ for a win and lose response pair is then calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $N$. In this configuration, the sum of the weights in each row of the
    matrix equals one. It is noteworthy that normalizing across the columns represents
    another valid approach. Moreover, more intricate reweighing methodologies, inspired
    by concepts such as optimal transport (Peyré et al., [2017](#bib.bib14)) or conditional
    transport (Zheng and Zhou, [2021](#bib.bib34)), might offer additional refinements
    when tuning LLMs (Tanwisuth et al., [2023](#bib.bib24)). These possibilities warrant
    further exploration in future research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following KTO (Ethayarajh et al., [2023](#bib.bib8)), we assume that $Z(x)$
    in [Equation 10](#S3.E10 "In 3.1 Contrast Matrix Construction ‣ 3 Relative Preference
    Optimization ‣ Relative Preference Optimization: Enhancing LLM Alignment through
    Contrasting Responses across Identical and Diverse Prompts") is the same for all
    inputs, and we obtain'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: This strategy ensures that contrast scores derived from thematically similar
    prompts are accentuated, enhancing context-sensitive preference learning. In RPO,
    while the Embedding Distance Reweighting Strategy serves as our primary method
    for analyzing the contrast matrix, we have also explored two additional strategies
    for specific data configurations, offering alternative ways to weight the contrast
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Uniform Weighting Strategy.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this strategy, each comparison pair within the $M\times N$. This method simplifies
    the analysis process and is applicable in both unpaired and paired data settings.
  prefs: []
  type: TYPE_NORMAL
- en: Diagonal Emphasis Weighting Strategy.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This strategy can only be applied to paired data scenarios with an $M\times
    M$) represent direct comparisons of win and lose responses for the same prompt,
    while non-diagonal terms account for comparisons across different prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'where $r_{w,i}-r_{l,j}$ follows the same formulation as described in [Equation 13](#S3.E13
    "In Embedding Distance Reweighting Strategy. ‣ 3.2 Weighting Strategies ‣ 3 Relative
    Preference Optimization ‣ Relative Preference Optimization: Enhancing LLM Alignment
    through Contrasting Responses across Identical and Diverse Prompts").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final RPO loss can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{\text{RPO}}=-\frac{1}{M*N}\sum_{i=1}^{M}\sum_{j=1}^{N}\log\sigma\left(s_{ij}\right)$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: where $s_{ij}$. RPO broadens the learning scope of the model, enabling it to
    utilize insights from a wider array of prompts, facilitating a more nuanced alignment
    with human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have undertaken a comprehensive series of experiments to address three primary
    questions of RPO: (a) Can conceptually related prompts be deemed effective contrastive
    pairs for human preference optimization? (b) What factors influence the performance
    of RPO? (c) How does the performance of RPO compare to current state-of-the-art
    preference alignment methods? In the following sections, we will begin by presenting
    the details of our experimental setup in [Section 4.1](#S4.SS1 "4.1 Experimental
    Setup ‣ 4 Experiments ‣ Relative Preference Optimization: Enhancing LLM Alignment
    through Contrasting Responses across Identical and Diverse Prompts"). We will
    then delve into an in-depth ablation study to address questions (a) and (b) in
    [Section 4.2](#S4.SS2 "4.2 Ablation Study ‣ 4 Experiments ‣ Relative Preference
    Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical
    and Diverse Prompts"), and finally, in [Section 4.3](#S4.SS3 "4.3 Benchmark Performance
    ‣ 4 Experiments ‣ Relative Preference Optimization: Enhancing LLM Alignment through
    Contrasting Responses across Identical and Diverse Prompts"), we will showcase
    the benchmark performance of our approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Training Tasks and Datasets.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following DPO (Rafailov et al., [2023](#bib.bib15)), our experiments were conducted
    on two pivotal datasets, each meticulously chosen to evaluate specific competencies
    in open-ended text generation tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Anthropic’s Helpful and Harmless (HH) Dataset (Bai et al., [2022](#bib.bib2)):
    This dataset was utilized for assessing single-turn dialogue performance of our
    models. With 170k dialogues, each comprising a human query and paired model responses
    rated for helpfulness and harmlessness. Following DPO (Rafailov et al., [2023](#bib.bib15)),
    the preferred responses from this dataset were utilized for the supervised Fine-Tuning
    (SFT) phase, aligning the initial model behavior with desirable conversational
    outcomes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI’s Summarization Dataset (Stiennon et al., [2020](#bib.bib23)): Targeted
    for the summarization task, each input $x$. Similar to the HH dataset, the SFT
    phase was informed by preferred responses from this dataset, which set a benchmark
    for the model’s summarization capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We assessed RPO against a range of alignment methods. These included SFT (Chung
    et al., [2022](#bib.bib6)) for initial model adaptation, PPO (Schulman et al.,
    [2017](#bib.bib20)) for reinforcement learning fine-tuning, DPO and IPO (Azar
    et al., [2024](#bib.bib1)) for preference-based model alignment, and KTO (Ethayarajh
    et al., [2023](#bib.bib8)) as an alternative approach incorporating human value
    functions. This varied set of baselines provided a comprehensive context for evaluating
    RPO’s performance in aligning language models with nuanced human preferences.
  prefs: []
  type: TYPE_NORMAL
- en: For these comparisons, we utilized a range of pre-trained large language models,
    including LLama-2-7/13B (Touvron et al., [2023](#bib.bib28)) and Mistral-7B (Jiang
    et al., [2023](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: We conducted the evaluations of RPO on the validation sets of Anthropic’s Helpful
    and Harmless (HH) Dataset for dialogue and the OpenAI Summarization Dataset for
    summarization. To further challenge RPO’s adaptability and general capability
    in instruction-following, we integrated the AlpacaEval2.0 leaderboard (Li et al.,
    [2023](#bib.bib10)) into our evaluation benchmark. This benchmark comprises a
    set of 805 diverse and carefully curated prompts, serving as an ideal platform
    for testing the model’s ability to follow general user instructions accurately
    and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Our primary evaluation metric was the win rate, calculated using the advanced
    capabilities of GPT-4 (OpenAI, [2023](#bib.bib11)) as the evaluative tool. This
    metric quantitatively assessed the preference rate of our model’s responses against
    those generated by baseline models, which refer to the chosen responses in the
    test set. By employing GPT-4 for evaluation, we leveraged its robust understanding
    and judgment abilities as a stand-in for human evaluators (Zheng et al., [2023](#bib.bib35);
    Li et al., [2023](#bib.bib10)). This method provided a scalable and consistent
    way to assess the alignment of the RPO-generated responses with nuanced human
    preferences, thereby offering a comprehensive measure of the model’s overall performance
    and effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: Training Details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following the experimental settings similar to KTO (Ethayarajh et al., [2023](#bib.bib8)),
    we maintained a consistent beta value ($\beta=0.1$) throughout all experiments.
    The training utilized 8 Nvidia A100 GPUs, with a batch size of 64, optimized with
    RMSProp optimizer (Tieleman and Hinton, [2017](#bib.bib27)). Initially, we trained
    the SFT, followed by training the subsequent alignment models based on the SFT.
    For more detailed information on training and evaluation, please refer to [Appendix A](#A1
    "Appendix A Training and Evaluation Details. ‣ Relative Preference Optimization:
    Enhancing LLM Alignment through Contrasting Responses across Identical and Diverse
    Prompts").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We initiated our investigation with an ablation study aimed at assessing the
    viability of using semantically related prompts as effective contrastive pairs
    for preference optimization. Initially, we utilized DPO as the baseline and began
    with the pairwise preference data, a setup similar to that of DPO. DPO primarily
    focuses on preference pairs in relation to each individual prompt, while RPO constructs
    a contrastive matrix encompassing all potential pairs within each mini-batch.
    In our experiments, we compared RPO with various weighting strategies against
    DPO, using the Mistral-7B model for dialogue tasks. We employed GPT-4 to determine
    the win rate compared to the suggested responses within the test dataset. As presented
    in [Table 1](#S4.T1 "In 4.2 Ablation Study ‣ 4 Experiments ‣ Relative Preference
    Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical
    and Diverse Prompts"), simple reweighting strategies, such as Uniform Weighting
    (where all contrastive pairs are considered equally important) and Diagonal Weighting
    (which places more emphasis on diagonal pairwise terms while treating the other
    non-diagonal terms evenly), yielded slightly worse results than the baseline DPO.
    This suggests that not all prompt pairs can be deemed effective contrastive pairs
    for preference optimization. This finding aligns with our intuition, indicating
    that only semantically related prompt responses can effectively serve as preferred
    and rejected pairs for each other. Motivated by these observations, we introduced
    a sentence embedding model to measure the semantic relatedness between prompts.
    We then utilized this similarity measure to emphasize the preference contrast
    for highly related pairs. The experiments demonstrated that with Embedding Reweighting,
    RPO significantly outperformed DPO in both paired and unpaired data scenarios.
    This success validates the notion that preference contrast between highly related
    prompts is extremely valuable for aligning human preferences, a process closely
    mirroring human learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our investigation progressed to examine the impact of different prompt embedding
    extraction models on the LLama2-7B model’s efficacy in dialogue tasks, as delineated
    in [Table 2](#S4.T2 "In 4.2 Ablation Study ‣ 4 Experiments ‣ Relative Preference
    Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical
    and Diverse Prompts"). In this comparative study, we scrutinized four distinct
    models: all-MiniLM-L6-v2 (Wang et al., [2020](#bib.bib29)), known for its efficiency
    and balance in handling context; sentence-t5-large (Raffel et al., [2020](#bib.bib16)),
    designed for generating semantically rich embeddings; paraphrase-mpnet-base-v2
    (Song et al., [2020](#bib.bib21)), optimized for capturing paraphrastic variations;
    and all-distilroberta-v1 (Sanh et al., [2019](#bib.bib18)), recognized for its
    distilled knowledge from larger models. Observing a consistent trend where moderate
    temperature settings enhance model performance, the all-MiniLM-L6 model, set at
    a temperature of 0.5, was selected as the benchmark setting for subsequent comparisons
    against other SoTA models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Ablation study on RPO weighting strategies. We use Mistral-7B as the
    base model and train RPO with the Anthropic-HH dataset using our proposed three
    weighting strategies. We employ GPT-4 to determine the win rate compared to the
    suggested responses within the test dataset. If applied sentence embedding model
    is set as all-MiniLM-L6-v2\.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Win Rate |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DPO (Rafailov et al., [2023](#bib.bib15)) | 72.26 |'
  prefs: []
  type: TYPE_TB
- en: '| Uniform Weighting | 68.36 |'
  prefs: []
  type: TYPE_TB
- en: '| Diagonal Weighting ($\alpha=0.8$) | 69.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding Reweighting (Unpaired, $\tau=0.75$) | 75.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding Reweighting (Paired, $\tau=0.5$) | 78.52 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Ablation study on prompt embedding extraction models across various
    temperature settings. We use LLama2-7B as the base model and train RPO on the
    Anthropic-HH dataset using multiple sentence embedding models and various temperature
    values. We employ GPT-4 to determine the win rate compared to the suggested responses
    within the test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Embedding Extraction Model |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| $\tau$ | all-MiniLM-L6-v2 | sentence-t5-large | paraphrase-mpnet-base-v2
    | all-distilroberta-v1 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.25 | 66.80 | 67.38 | 62.30 | 67.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 68.75 | 65.23 | 65.63 | 67.78 |'
  prefs: []
  type: TYPE_TB
- en: '| 0.75 | 67.97 | 65.43 | 64.45 | 65.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.0 | 64.06 | 65.63 | 66.41 | 63.67 |'
  prefs: []
  type: TYPE_TB
- en: 4.3 Benchmark Performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 3: win rate on Anthropic-HH and OpenAI Summarization datasets. We conduct
    a comparative analysis of RPO (unpaired/paired) against state-of-the-art human
    preference optimization baselines across various datasets, including Anthropic-HH
    and OpenAI Summarization. We evaluate their performance using metrics such as
    GPT-4 win rate and AlpacaEval2.0 leaderboard. The evaluation of AlpacaEval2.0
    leaderboard is carried out using Mistral-7B, which has been trained on the Anthropic-HH
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Anthropic-HH | OpenAI Summarization | AlpacaEval2.0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLama2-7B | LLama2-13B | Mistral-7B | Mistral-7B | Mistral-7B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT (Chung et al., [2022](#bib.bib6)) | 45.90 | 48.05 | 48.24 | 28.52 | 13.68
    |'
  prefs: []
  type: TYPE_TB
- en: '| PPO (Schulman et al., [2017](#bib.bib20)) | 50.39 | 51.95 | 58.98 | 39.84
    | 15.00 |'
  prefs: []
  type: TYPE_TB
- en: '| IPO (Azar et al., [2024](#bib.bib1)) | 53.91 | 46.48 | 63.48 | 33.98 | 21.62
    |'
  prefs: []
  type: TYPE_TB
- en: '| DPO (Rafailov et al., [2023](#bib.bib15)) | 63.67 | 63.28 | 72.26 | 48.83
    | 30.84 |'
  prefs: []
  type: TYPE_TB
- en: '| KTO (Ethayarajh et al., [2023](#bib.bib8)) | 67.78 | 71.48 | 61.13 | 39.45
    | 15.06 |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Unpaired ($\tau=0.75$) | 61.33 | 70.31 | 75.00 | 50.39 | 31.24 |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Paired ($\tau=0.5$) | 68.75 | 72.66 | 78.52 | 50.00 | 38.88 |'
  prefs: []
  type: TYPE_TB
- en: '[Table 3](#S4.T3 "In 4.3 Benchmark Performance ‣ 4 Experiments ‣ Relative Preference
    Optimization: Enhancing LLM Alignment through Contrasting Responses across Identical
    and Diverse Prompts") offers a detailed comparative analysis of win rates for
    diverse alignment methods applied to the LLama2-7B, LLama2-13B, and Mistral-7B
    models, addressing tasks across the Anthropic-HH, OpenAI Summarization datasets,
    and the AlpacaEval2.0 leaderboard. The array of methods evaluated includes SFT,
    PPO, IPO, DPO, KTO, and our RPO in both unpaired and paired modalities. Our findings
    indicate that while SFT establishes a fundamental layer of adaptation, it is surpassed
    by methods integrating human feedback such as PPO and IPO. DPO, with its strategy
    of leveraging direct human preferences, robustly outperforms SFT, PPO, and IPO,
    attesting to the efficacy of direct preference-based contrast learning. KTO, treating
    chosen and rejected samples separately, notches high win rates, especially with
    the LLama2-13B model on the Anthropic-HH dataset. Yet, it is the RPO approaches
    that command the highest win rates across the majority of datasets and models,
    with the RPO-Paired method particularly dominant, exemplified by a 78.52 win rate
    on the Anthropic-HH dataset with the Mistral-7B model, highlighting the significant
    benefits of constructing rich contrastive pairs from prompts with semantic similarities.'
  prefs: []
  type: TYPE_NORMAL
- en: In light of Mistral-7B’s exceptional performance on dialogue tasks, we solely
    advanced this model for additional scrutiny on the OpenAI Summarization dataset.
    The RPO-Unpaired method surprisingly outperformed the paired configuration with
    win rates of 50.39 and 50.00, respectively, suggesting that the unpaired setting
    can be remarkably effective, potentially due to the model’s inherent capability
    to leverage diverse training signals. Additionally, the Mistral-7B model, previously
    tuned on the Anthropic-HH dataset, was applied to the AlpacaEval2.0 leaderboard
    to assess its broad generalizability across a spectrum of instruction-following
    scenarios. Our results on both the Summarization and AlpacaEval benchmarks were
    highly promising, with the RPO-Unpaired model showcasing a level of performance
    that not only competes robustly with its paired counterpart but also exemplifies
    the method’s adaptability. This strong showing emphasizes RPO’s capacity to harness
    the information content in unpaired data effectively, further confirming the method’s
    robustness in diverse training scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In summary, Relative Preference Optimization (RPO) innovatively aligns Large
    Language Models (LLMs) with human preferences, adeptly handling both paired and
    non-paired data. Its contrastive weighting mechanism effectively processes similar
    and identical prompts, enriching the understanding of nuanced preferences. Empirical
    results on models like LLaMA2-7/13B and Mistral-7B show RPO outperforming the
    previous alignment methods in key tasks, particularly in dialogue and summarization.
    This adaptability and improved alignment highlight RPO’s potential in advancing
    LLM training, setting the stage for more user-centric and ethically aligned AI
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations & Future Work. RPO’s current limitation lies in its reliance on
    the quality of the embedding model. To advance, a more generalized approach is
    needed for constructing contrast pairs in both paired and unpaired preference
    data scenarios. A promising direction could involve developing algorithms that
    directly analyze linguistic patterns and contextual similarities within prompts,
    thus reducing dependence on any specific embedding model and enhancing RPO’s versatility
    and effectiveness across various data types.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azar et al. (2024) Mohammad Gheshlaghi Azar, Mark Rowland, Bilal Piot, Daniel
    Guo, Daniele Calandriello, Michal Valko, and Rémi Munos. A general theoretical
    paradigm to understand learning from human preferences. *AISTATS*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bradley and Terry (1952) Ralph Allan Bradley and Milton E Terry. Rank analysis
    of incomplete block designs: I. the method of paired comparisons. *Biometrika*,
    pages 324–345, 1952.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. (2017) Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,
    Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences.
    *NeurIPS*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dahlin et al. (2018) Kristina B Dahlin, You-Ta Chuang, and Thomas J Roulet.
    Opportunity, motivation, and ability to learn from failures and errors: Review,
    synthesis, and ways to move forward. *Academy of Management Annals*, pages 252–277,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethayarajh et al. (2023) Kawin Ethayarajh, Winnie Xu, Dan Jurafsky, and Douwe
    Kiela. Human-centered loss functions (halos). Technical report, Contextual AI,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. Alpacaeval:
    An automatic evaluator of instruction-following models. *GitHub repository*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *NeurIPS*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2023) Liangming Pan, Alon Albalak, Xinyi Wang, and William Yang
    Wang. Logic-lm: Empowering large language models with symbolic solvers for faithful
    logical reasoning. *EMNLP*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Peyré et al. (2017) Gabriel Peyré, Marco Cuturi, et al. Computational optimal
    transport. *Center for Research in Economics and Statistics Working Papers*, (2017-86),
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2023) Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano
    Ermon, Christopher D Manning, and Chelsea Finn. Direct preference optimization:
    Your language model is secretly a reward model. *NeurIPS*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *JMLR*,
    pages 1–67, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ramamurthy et al. (2023) Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kianté
    Brantley, Jack Hessel, Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and
    Yejin Choi. Is reinforcement learning (not) for natural language processing?:
    Benchmarks, baselines, and building blocks for natural language policy optimization.
    *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter.
    *arXiv preprint arXiv:1910.01108*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanh et al. (2021) Victor Sanh, Albert Webson, Colin Raffel, Stephen H Bach,
    Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,
    Arun Raja, et al. Multitask prompted training enables zero-shot task generalization.
    *arXiv preprint arXiv:2110.08207*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. (2017) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec
    Radford, and Oleg Klimov. Proximal policy optimization algorithms. *arXiv preprint
    arXiv:1707.06347*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2020) Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
    Mpnet: Masked and permuted pre-training for language understanding. *NeurIPS*,
    33:16857–16867, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and
    extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stiennon et al. (2020) Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,
    Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano. Learning
    to summarize with human feedback. *NeurIPS*, 33:3008–3021, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tanwisuth et al. (2023) Korawat Tanwisuth, Shujian Zhang, Huangjie Zheng, Pengcheng
    He, and Mingyuan Zhou. POUF: Prompt-oriented unsupervised fine-tuning for large
    pre-trained models. In *Proceedings of the 40th International Conference on Machine
    Learning*, ICML’23\. JMLR.org, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, et al. Lamda: Language models for dialog applications. *arXiv preprint
    arXiv:2201.08239*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tian et al. (2023) Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung,
    Jacques Klein, and Tegawendé F Bissyandé. Is chatgpt the ultimate programming
    assistant–how far is it? *arXiv preprint arXiv:2304.11938*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tieleman and Hinton (2017) Tijmen Tieleman and G Hinton. Divide the gradient
    by a running average of its recent magnitude. coursera: Neural networks for machine
    learning. *Technical report*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint*, arXiv:2307.09288, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and
    Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression
    of pre-trained transformers. *NeurIPS*, 33:5776–5788, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning
    language model with self generated instructions. *ACL*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *ICLR*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2024) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath:
    Bootstrap your own mathematical questions for large language models. *ICLR*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2023) Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang
    Huang, and Fei Huang. Rrhf: Rank responses to align language models with human
    feedback without tears. *arXiv preprint arXiv:2304.05302*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng and Zhou (2021) Huangjie Zheng and Mingyuan Zhou. Exploiting chain rule
    and Bayes’ theorem to compare probability distributions. In A. Beygelzimer, Y. Dauphin,
    P. Liang, and J. Wortman Vaughan, editors, *Advances in Neural Information Processing
    Systems*, 2021. URL [https://openreview.net/forum?id=f-ggKIDTu5D](https://openreview.net/forum?id=f-ggKIDTu5D).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *arXiv preprint arXiv:2306.05685*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Training and Evaluation Details.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The detailed hyperparameters are presented in [Table 4](#A1.T4 "In Appendix
    A Training and Evaluation Details. ‣ Relative Preference Optimization: Enhancing
    LLM Alignment through Contrasting Responses across Identical and Diverse Prompts");
    the majority of these parameters are in accordance with the DPO framework (Rafailov
    et al., [2023](#bib.bib15)). For instance, the maximum length for prompts is set
    to 256, and the combined cap for both prompt and response lengths is fixed at
    512\. Furthermore, the number of samples employed for calculating the win rate
    is established at 256.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Hyperparameters | Value |'
  prefs: []
  type: TYPE_TB
- en: '| Batch size | 64 |'
  prefs: []
  type: TYPE_TB
- en: '| GPUs | 8 |'
  prefs: []
  type: TYPE_TB
- en: '| Learning rate | 5e-7 |'
  prefs: []
  type: TYPE_TB
- en: '| Epochs | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Max prompt length | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| Max prompt length + Max response length | 512 |'
  prefs: []
  type: TYPE_TB
- en: '| Optimizer | RMSprop |'
  prefs: []
  type: TYPE_TB
- en: '| $\beta$ | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| $\tau$ for RPO-Unpaired | 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| $\tau$ for RPO-paired | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Sampling temperature | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt embedding extraction model | all-MiniLM-L6 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of comparisons to make | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT judge | gpt-4-0613 |'
  prefs: []
  type: TYPE_TB
- en: '| AlpacaEval judge | alpaca_eval_gpt4_turbo_fn |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Algorithm Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Algorithm 1 Relative Preference Optimization (RPO)
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Training dataset with the win and lose samples (paired or unpaired),
    Initial model parameters $\theta_{0}$           Update $C[i,j]$.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C The core Python implementation of RPO.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '1class  RPOTrainer(PairedPreferenceTrainer):2  """Trainer  class  for  Relative  Preference  Optimization  (RPO)  algorithm.3  This  class  extends  the  PairedPreferenceTrainer  class  and  implements  the  loss  function4  specific  to  RPO.5  Args:6  PairedPreferenceTrainer:  The  base  trainer  class.7  Attributes:8  None9  Methods:10  loss:  Compute  the  RPO  loss  for  a  batch  of  policy  and  reference  model  log  probabilities.11  """12  def  loss(self,13  policy_chosen_logps:  torch.FloatTensor,14  policy_rejected_logps:  torch.FloatTensor,15  reference_chosen_logps:  torch.FloatTensor,16  reference_rejected_logps:  torch.FloatTensor,17  prompts_emb:  Optional[torch.FloatTensor]  =  None)  ->  Tuple[torch.FloatTensor,  torch.FloatTensor,  torch.FloatTensor]:18  """Compute  the  RPO  loss  for  a  batch  of  policy  and  reference  model  log  probabilities.19  Args:20  policy_chosen_logps:  Log  probabilities  of  the  chosen  responses  by  the  policy  model.21  policy_rejected_logps:  Log  probabilities  of  the  rejected  responses  by  the  policy  model.22  reference_chosen_logps:  Log  probabilities  of  the  chosen  responses  by  the  reference  model.23  reference_rejected_logps:  Log  probabilities  of  the  rejected  responses  by  the  reference  model.24  prompts_emb:  Optional.  Embeddings  of  the  prompts.  Defaults  to  None.25  Returns:26  losses:  The  computed  losses.27  chosen_rewards:  The  computed  rewards  for  the  chosen  responses.28  rejected_rewards:  The  computed  rewards  for  the  rejected  responses.29  """30  chosen_logdiffs  =  policy_chosen_logps  -  reference_chosen_logps31  rejected_logdiffs  =  policy_rejected_logps  -  reference_rejected_logps32  logits  =  chosen_logdiffs.view(-1,  1)  -  rejected_logdiffs.view(1,  -1)33  #  Calculate  the  cosine  distance  between  each  chosen  and  rejected  prompts34  if  prompts_emb  is  not  None:35  prompts_emb  =  torch.tensor(prompts_emb,  dtype=policy_chosen_logps.dtype)36  norm_prompts_emb  =  F.normalize(prompts_emb,  p=2,  dim=1)37  cos_distance  =  1  -  torch.mm(norm_prompts_emb,  norm_prompts_emb.t())  #  Cosine  distance38  #  Rescale  the  cosine  distances  to  use  as  weights39  distance_weights  =  F.softmax(-cos_distance  /  self.config.loss.distance_temperature,  dim=1)40  else:41  distance_weights  =  torch.ones_like(logits)4243  weighted_logits  =  logits  *  distance_weights.to(logits.device)44  losses  =  -F.logsigmoid(self.config.loss.beta  *  weighted_logits).mean(-1)4546  chosen_rewards  =  self.config.loss.beta  *  (policy_chosen_logps  -  reference_chosen_logps).detach()47  rejected_rewards  =  self.config.loss.beta  *  (policy_rejected_logps  -  reference_rejected_logps).detach()4849  return  losses,  chosen_rewards,  rejected_rewards'
  prefs: []
  type: TYPE_NORMAL
- en: 'Listing 1: Python code for RPO Loss.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Evaluation Prompts.
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GPT-4 prompt for dialogue task.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is a conversation between <$|$>can now choose from different responses.
    Response 1:  END OF RESPONSE 1 Response 2:  END OF RESPONSE
    2 Between these two responses, which response is more helpful, harmless, and concise?
    Pick one.'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4 prompt for summarization task.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is a forum post presented by <$|$>can now choose from different summaries.
    Summary 1:  END OF SUMMARY 1 Summary 2:  END OF SUMMARY
    2 Between these two summaries, which summary does a better job of summarizing
    the most important points in the given forum post, without including unimportant
    or irrelevant details? A good summary is both precise and concise. Pick one.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Generation Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 5: Sample summaries to a post from the OpenAI Summarization test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | As the title says, we just don’t know what to talk about or really
    how to talk to each other. I really like her (a lot) and she feels the same towards
    me, and I just don’t know what to do. She was grounded for 2 months a couple days
    ago (her mom’s a little crazy when it comes to punishments), so seeing her in
    person is sadly out of the question. We’re both just as awkward in person, anyway…
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | I’ve seen on some other places that this happens in every relationship
    at some point and not to be worried about it, but I never see how to deal with
    it. Both of us agree we need to be more confident and open with each other, but
    it seems like there’s never anything new to talk about, and even if there was
    we wouldn’t know how to talk about it. |'
  prefs: []
  type: TYPE_TB
- en: '| SFT (Chung et al., [2022](#bib.bib6)) | How do I help me talk to my girlfriend
    more? |'
  prefs: []
  type: TYPE_TB
- en: '| PPO (Schulman et al., [2017](#bib.bib20)) | Don’t know what to talk to gf
    about. She and I need to be more open with each other, but have no idea what to
    talk about/how to talk about it. |'
  prefs: []
  type: TYPE_TB
- en: '| DPO (Rafailov et al., [2023](#bib.bib15)) | How do you deal with not knowing
    what to talk about with the one you love? When talking with her just seems stagnant
    and not much to talk about. Both her and I feel the same, and want to fix it,
    but don’t know how to do it. Relationship is new (about a month |'
  prefs: []
  type: TYPE_TB
- en: '| IPO (Azar et al., [2024](#bib.bib1)) | How do I help me and my gf get better
    at talking to each other, and how do we deal with the awkwardness that is in our
    relationship? |'
  prefs: []
  type: TYPE_TB
- en: '| KTO (Ethayarajh et al., [2023](#bib.bib8)) | How do you deal with not knowing
    what to talk about with someone you’re in a relationship with? |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Unpaired | With my girlfriend, we never know what to talk about or really
    how to talk to each other in general. She agrees something needs to change but
    what do we even say to each other? Nothing ever comes to mind. |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Paired | Girlfriend and I like each other, are into each other, but don’t
    know how to talk and start good conversations with one another. We agree we need
    to work on being more comfortable and open. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Sample summaries to a post from the OpenAI Summarization test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | I am a caregiver and one of the women I care for is openly racist.
    I began helping “Rose" and her husband “Dan" for 6 months. I started noticing
    that Rose was racist when we would chat, she would let on that she did not care
    for black, Asian, or gay people. These are the groups that I know of, anyway.
    Also, my sister is gay so it really hurt me when she said ‘Oh I HATE that woman,
    she is gay’ when referring to Ellen Degeneres. |'
  prefs: []
  type: TYPE_TB
- en: '|  | What’s worse, we were taking a walk today and we walked past a black woman.
    My client proceeded to express her dislike for this person based solely on her
    skin color. I was appalled. I did not know what to say, so I just said “Hmm."
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | How do I handle this? My client has had a stroke and she isn’t "all there"
    but she definitely knows what she is saying and does not have Alzheimer’s disease
    or anything like that. |'
  prefs: []
  type: TYPE_TB
- en: '| SFT (Chung et al., [2022](#bib.bib6)) | Woman is openly racist and it hurts
    me. How do I handle it? |'
  prefs: []
  type: TYPE_TB
- en: '| PPO (Schulman et al., [2017](#bib.bib20)) | My client is openly racist and
    it hurts me because we were taking a walk today and she made racist comments about
    a black woman we passed. What do I do? |'
  prefs: []
  type: TYPE_TB
- en: '| DPO (Rafailov et al., [2023](#bib.bib15)) | The woman I care for is openly
    racist. She expressed dislike for black, asian, and gay people. What do I do?
    Should I change clients? Or deal with it? It’s hurtful to me. |'
  prefs: []
  type: TYPE_TB
- en: '| IPO (Azar et al., [2024](#bib.bib1)) | The client I care for is racist and
    I am not sure how to handle it. Any advice? |'
  prefs: []
  type: TYPE_TB
- en: '| KTO (Ethayarajh et al., [2023](#bib.bib8)) | The woman I care for is racist
    and I am trying to figure out how to handle it. |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Unpaired | The woman I care for is openly racist. She isn’t senile or
    anything, she just holds these prejudices. How do I handle this? We live in a
    diverse area so I run into these instances a lot. |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Paired | The woman I care for is openly racist and expressed dislike
    for people of other races as we walked past them. She has had a stroke and is
    not senile, what do I do about her being racist while I’m her caregiver? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Sample responses to a prompt from the Anthropic-HH test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | My wife loves to cook. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SFT (Chung et al., [2022](#bib.bib6)) | Great, I can help out with the cooking!
    |'
  prefs: []
  type: TYPE_TB
- en: '| PPO (Schulman et al., [2017](#bib.bib20)) | I’m glad your wife loves to cook.
    I appreciate that quality too. Is there any specific way you’d like me to assist
    her, or support you in supporting her passion for cooking? |'
  prefs: []
  type: TYPE_TB
- en: '| DPO (Rafailov et al., [2023](#bib.bib15)) | That’s wonderful! I’m also a
    fan of cooking. What are her favorite dishes to prepare? Are you guys adventurous
    with new recipes? How often does she cook? What’s a dish of hers you can’t get
    enough of? What about her cooking style do you love the most? I’d love to exchange
    some ideas for recipes and cooking tips if you’re interested. |'
  prefs: []
  type: TYPE_TB
- en: '| IPO (Azar et al., [2024](#bib.bib1)) | That’s fantastic. Cooking is a shared
    passion! What types of cuisine do you and your wife enjoy preparing together?
    |'
  prefs: []
  type: TYPE_TB
- en: '| KTO (Ethayarajh et al., [2023](#bib.bib8)) | That’s excellent! Home-cooked
    meals are such a pleasure. It’s wonderful that you have that in your home. |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Unpaired | Great! Cooking is indeed a delightful hobby. If you have any
    cooking-related queries or need recipe advice, I’m all ears. How can I assist?
    What types of recipes is your wife looking to explore? I’m here to help with all
    things culinary, from ingredients to techniques. |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Paired | Lovely to hear that your wife enjoys cooking! It’s a beautiful
    way to unwind, express creativity, and bond with family. How can I assist in enriching
    this experience for both of you? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Sample responses to a prompt from the AlpacaEval2.0 test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Pretend to be a business professional. Act like you are giving your
    mentee professional advice on the most essential points to create a self-sustaining
    business within the next five years time without the help of external investors.
    |'
  prefs: []
  type: TYPE_TB
- en: '| SFT (Chung et al., [2022](#bib.bib6)) | Ah, that’s an interesting challenge,
    I’d be happy to take it on. Let’s start by brainstorming a few important things
    that a business needs to be successful. First and foremost, you’ll need to ensure
    that your product or service is both high quality and meets a market demand that
    is large enough to sustain the business. You will need to develop a business plan
    and marketing strategy to promote your product or service, and then hire a team
    of talented and motivated people to help implement your business plan. You’ll
    also need to ensure that you have the necessary capital and resources to get your
    business off the ground and running, and make sure you stay on top of changes
    and trends in the market to ensure that your business is sustainable over the
    long-term. |'
  prefs: []
  type: TYPE_TB
- en: '| PPO (Schulman et al., [2017](#bib.bib20)) | I can absolutely do that! Here
    are some key points that would be essential for starting and growing a successful
    business in the next five years: Have a clear and well-defined vision for the
    company, and work to make that vision a reality. Ensure that the company has a
    solid financial foundation, with a strong balance sheet and plenty of working
    capital. Develop a strong team of employees, and invest in training and development
    opportunities for them. Market the company effectively, both online and offline,
    and use digital marketing and social media to reach potential customers. Stay
    abreast of changes and developments in your industry, and be prepared to pivot
    and adapt your business model as needed. With these key points in mind, I am confident
    that you can create a self-sustaining business within five years! |'
  prefs: []
  type: TYPE_TB
- en: '| DPO (Rafailov et al., [2023](#bib.bib15)) | Sure! I think a really essential
    point for any business is to create a business plan with clear, actionable goals.
    When starting a business, it’s also helpful to identify the target customer and
    understand how your product or service fits into the market. Also, I would recommend
    setting up an efficient accounting system so you can track expenses and profits
    accurately. And it’s very important to establish a good financial foundation by
    obtaining loans, grants, or other financing to cover initial expenses. I could
    also give you advice on building a strong team and creating an efficient organizational
    structure to support your business. Does that make sense? |'
  prefs: []
  type: TYPE_TB
- en: '| IPO (Azar et al., [2024](#bib.bib1)) | Ah, that is a really difficult question,
    but I think I understand what you are asking. The essential points to creating
    a self-sustaining business within the next five years time would be to plan for
    sustainable growth, find ways to increase your customer base, and find a way to
    increase your profits. Firstly, you would need to establish a good foundation
    for growth. You will need to establish a good business plan, which should clearly
    lay out the goals and objectives of your business, the expected growth rate of
    your business, and an idea of how you will maintain your growth. Without a solid
    plan in place, you cannot hope to grow your business. Once you have a solid business
    plan in place, you will also need to invest in your business by investing in your
    product and your employees. You will also need to establish a solid customer base,
    and this will require marketing and advertising. You will need to market your
    business effectively in order to attract customers and to communicate your business
    value to potential customers. You will also need to keep your customers happy,
    and this will require investing in customer service, and providing an excellent
    customer experience. The last point you will need to consider is increasing your
    profits. You will need to establish and maintain profitable business practices,
    and also keep up with the latest trends and technology in your field in order
    to remain competitive. |'
  prefs: []
  type: TYPE_TB
- en: '| KTO (Ethayarajh et al., [2023](#bib.bib8)) | Good morning, this is Jill.
    I’m very glad to have the chance to talk to you today. I can offer some advice
    to help you get started. To start, you should know that you’ll need to build up
    a team of employees. Once you have employees working for you, you’ll need to invest
    time in building relationships, especially with customers. And you should know
    that you’ll need to make sure your business has good cash flow. The basic steps
    for starting a business are: Make a business plan: This is important to know what
    you want to do, how you’re going to do it, and what you’ll need to be successful.
    Set up a business name: Make sure it’s easy to remember and unique. And it should
    communicate your product or service. Obtain a business license: This varies from
    state to state, but usually it means you’ll need to register your business, pay
    fees, get insurance, etc. Set up a business bank account: This is important to
    protect your personal finances from your business finances. Find a location: This
    is important to make sure your location is convenient to your customers and employees,
    and also to make sure it’s a good investment. Hire employees. |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Unpaired | Sure! I think a really important first step would be to identify
    a clear business opportunity and develop a viable business model. Then you’d want
    to research the market thoroughly and identify your target customer and how to
    reach them. Once you’ve identified your target customer, you’d want to figure
    out how to deliver a compelling value proposition to them, including a unique
    selling point. Then you’d want to figure out your operational strategy and staffing
    requirements and develop a solid business plan. Once you’ve done all of that,
    you’d want to start working on building your brand and getting the word out about
    your business. You’d also want to put together a solid financial plan and make
    sure you have enough funding to sustain your business through its early stages.
    And finally, you’d want to set some milestones and performance targets for yourself,
    and work towards them over time. Does that make sense? |'
  prefs: []
  type: TYPE_TB
- en: '| RPO-Paired | Sure, here are some key points to consider when launching a
    self-sustaining business within five years: 1\. Define your business goals and
    mission. 2\. Create a detailed business plan, including market research and financial
    projections. 3\. Raise startup capital and secure funding sources. 4\. Identify
    your target market and customer base. 5\. Develop a strong marketing strategy
    to attract customers. 6\. Build a solid team of employees and partners. 7\. Establish
    strong financial controls and accounting systems. 8\. Focus on providing exceptional
    customer service and building a positive brand image. 9\. Monitor your business
    performance and make necessary adjustments to stay on track. |'
  prefs: []
  type: TYPE_TB
