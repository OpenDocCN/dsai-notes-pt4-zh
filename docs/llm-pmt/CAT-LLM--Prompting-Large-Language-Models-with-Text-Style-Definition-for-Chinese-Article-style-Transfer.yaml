- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:09'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:09
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese
    Article-style Transfer'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'CAT-LLM: 利用文本风格定义提示大型语言模型进行中文文章风格迁移'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.05707](https://ar5iv.labs.arxiv.org/html/2401.05707)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2401.05707](https://ar5iv.labs.arxiv.org/html/2401.05707)
- en: Zhen Tao¹    Dinghao Xi¹    Zhiyu Li²    Liumin Tang¹&Wei Xu¹ ¹School of Information,
    Renmin University of China
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Zhen Tao¹    Dinghao Xi¹    Zhiyu Li²    Liumin Tang¹&Wei Xu¹ ¹中国人民大学信息学院
- en: ²Institute for Advanced Algorithms Research, Shanghai {taozhen, xidinghao}@ruc.edu.cn,
    lizy@iaar.ac.cn, {tangliumin, weixu}@ruc.edu.cn Corresponding author
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ²高级算法研究所，上海 {taozhen, xidinghao}@ruc.edu.cn, lizy@iaar.ac.cn, {tangliumin, weixu}@ruc.edu.cn
    通讯作者
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Text style transfer is increasingly prominent in online entertainment and social
    media. However, existing research mainly concentrates on style transfer within
    individual English sentences, while ignoring the complexity of long Chinese texts,
    which limits the wider applicability of style transfer in digital media realm.
    To bridge this gap, we propose a Chinese Article-style Transfer framework (CAT-LLM),
    leveraging the capabilities of Large Language Models (LLMs). CAT-LLM incorporates
    a bespoke, pluggable Text Style Definition (TSD) module aimed at comprehensively
    analyzing text features in articles, prompting LLMs to efficiently transfer Chinese
    article-style. The TSD module integrates a series of machine learning algorithms
    to analyze article-style from both words and sentences levels, thereby aiding
    LLMs thoroughly grasp the target style without compromising the integrity of the
    original text. In addition, this module supports dynamic expansion of internal
    style trees, showcasing robust compatibility and allowing flexible optimization
    in subsequent research. Moreover, we select five Chinese articles with distinct
    styles and create five parallel datasets using ChatGPT, enhancing the models’
    performance evaluation accuracy and establishing a novel paradigm for evaluating
    subsequent research on article-style transfer. Extensive experimental results
    affirm that CAT-LLM outperforms current research in terms of transfer accuracy
    and content preservation, and has remarkable applicability to various types of
    LLMs. Source code is available at GitHub¹¹1https://github.com/TaoZhen1110/CAT-LLM.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 文本风格迁移在在线娱乐和社交媒体中越来越突出。然而，现有研究主要集中于单个英文句子的风格迁移，忽视了长篇中文文本的复杂性，这限制了风格迁移在数字媒体领域的更广泛应用。为填补这一空白，我们提出了一种中文文章风格迁移框架（CAT-LLM），利用大型语言模型（LLMs）的能力。CAT-LLM
    包含一个定制的、可插拔的文本风格定义（TSD）模块，旨在全面分析文章中的文本特征，促使 LLMs 高效地迁移中文文章风格。TSD 模块整合了一系列机器学习算法，从词汇和句子层面对文章风格进行分析，从而帮助
    LLMs 充分把握目标风格而不损害原始文本的完整性。此外，该模块支持内部风格树的动态扩展，展示出强大的兼容性，并允许在后续研究中进行灵活优化。此外，我们选择了五篇风格各异的中文文章，并使用
    ChatGPT 创建了五个平行数据集，以提高模型性能评估的准确性，并为后续研究提供了一种新的评估范式。大量实验结果证实，CAT-LLM 在迁移准确性和内容保留方面优于当前研究，并且在各种类型的
    LLMs 上具有显著的适用性。源代码可在 GitHub¹¹1https://github.com/TaoZhen1110/CAT-LLM 上获取。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Text style transfer plays a crucial role in the field of Natural Language Processing
    (NLP). Its core objective is to adjust the style of the text while retaining the
    information from the original content Toshevska and Gievska ([2021](#bib.bib22));
    Yi et al. ([2021](#bib.bib25)). Recent advancements have been particularly notable
    in areas like news detection Przybyla ([2020](#bib.bib15)) and social media Wang
    et al. ([2023](#bib.bib24)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 文本风格迁移在自然语言处理（NLP）领域中发挥着至关重要的作用。其核心目标是在保留原始内容信息的同时调整文本的风格 Toshevska 和 Gievska
    ([2021](#bib.bib22))；Yi 等 ([2021](#bib.bib25))。最近的进展在新闻检测 Przybyla ([2020](#bib.bib15))
    和社交媒体 Wang 等 ([2023](#bib.bib24)) 等领域尤为显著。
- en: '![Refer to caption](img/ad55d5094ca085cb7d670dfe52efc9c6.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad55d5094ca085cb7d670dfe52efc9c6.png)'
- en: 'Figure 1: An example that transfer stylishless text to “The Scream” style text.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：一个将无风格文本转换为《呐喊》风格文本的示例。
- en: 'Before the advent of LLMs, the frameworks for text style transfer predominantly
    rely on the small deep learning models. In accordance with the process of text
    transfer, these frameworks generally fall into two categories: end-to-end models
    and two-stage models. However, these models exhibit lower transfer accuracy and
    are mainly effective for simple style transfer tasks, such as emotion Yi et al.
    ([2021](#bib.bib25)), politeness Danescu-Niculescu-Mizil et al. ([2013](#bib.bib4)),
    and etiquette Sheikha and Inkpen ([2010](#bib.bib19)). Due to the lack of parallel
    corpora, previous researches could only use non-parallel datasets to train classifiers
    for evaluating the transfer accuracy of unsupervised models. It is worth noting
    that an author’s style is often highly associated with specific writing themes.
    For example, the term (Camel Xiangzi) essentially only appears in the works of
    Lao She. Therefore, classifiers trained on non-parallel datasets may result in
    significant evaluation errors. Furthermore, existing studies primarily focus on
    English sentences level style transfer, with limited research dedicated to Chinese
    long texts. Considering the distinct linguistic structures, expressions, and cultural
    nuances of Chinese, it is necessary to design models specifically tailored for
    style transfer in Chinese long texts. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese
    Article-style Transfer") shows an example of Chinese article-style transfer from
    ordinary vernacular to a fragment of the article “The Scream”.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '在大规模语言模型（LLMs）出现之前，文本风格转换的框架主要依赖于小型深度学习模型。根据文本转换的过程，这些框架通常分为两类：端到端模型和两阶段模型。然而，这些模型表现出较低的转换准确率，并且主要对简单的风格转换任务有效，如情感Yi等（[2021](#bib.bib25)）、礼貌Danescu-Niculescu-Mizil等（[2013](#bib.bib4)）和礼仪Sheikha和Inkpen（[2010](#bib.bib19)）。由于缺乏平行语料库，之前的研究只能使用非平行数据集来训练分类器以评估无监督模型的转换准确率。值得注意的是，作者的风格往往与特定的写作主题高度相关。例如，“（骆驼祥子）”这一术语基本上只出现在老舍的作品中。因此，基于非平行数据集训练的分类器可能会导致显著的评估误差。此外，现有研究主要集中在英文句子层级的风格转换，对于中文长文本的研究则相对有限。考虑到中文在语言结构、表达方式和文化细微差别上的独特性，有必要设计专门针对中文长文本风格转换的模型。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ CAT-LLM: Prompting Large Language Models with Text
    Style Definition for Chinese Article-style Transfer")展示了从普通白话文到《呐喊》一段文章风格转换的示例。'
- en: Recently, LLMs have demonstrated the ability to handle more complex NLP tasks
    such as summarization generation and role-playing through zero-shot transfer without
    the need for dedicated task-specific training Brown et al. ([2020](#bib.bib1));
    Song et al. ([2023](#bib.bib21)), attracting significant attention from both academia
    and industry. Given the strong capabilities of LLMs in semantic understanding
    and text generation, they can effectively address challenges faced by small models
    of style transfer, such as the lack of parallel data and low transfer accuracy.
    Recent studies have made significant headway in text style transfer leveraging
    LLMs. Lai et al. Lai et al. ([2023](#bib.bib10)) employ ChatGPT to evaluate the
    transfer accuracy and content preservation of various text style transfer models.
    Shanahan Shanahan et al. ([2023](#bib.bib17)) and Wang Wang et al. ([2023](#bib.bib24))
    explore role-play text generation in conversational agents using LLMs. However,
    designating unfamiliar authors may trigger hallucinatory issues in LLMs. In addition,
    considering the diversity and complexity of Chinese, including idioms, rhetorical
    devices, directly reading articles with LLMs not only consumes more tokens, affecting
    inference speed, but also results in insufficient understanding of the article’s
    style when only reading partial fragments. Furthermore, these approaches also
    lack guidance and interpretability.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，LLM展示了处理更复杂的自然语言处理任务的能力，例如通过零样本迁移进行摘要生成和角色扮演，无需专门的任务特定训练（Brown et al. ([2020](#bib.bib1))；Song
    et al. ([2023](#bib.bib21))），引起了学术界和工业界的广泛关注。鉴于LLM在语义理解和文本生成方面的强大能力，它们可以有效解决小型风格迁移模型面临的挑战，例如缺乏平行数据和低迁移准确性。最近的研究在利用LLM进行文本风格迁移方面取得了显著进展。Lai
    et al. ([2023](#bib.bib10))利用ChatGPT评估各种文本风格迁移模型的迁移准确性和内容保留。Shanahan et al. ([2023](#bib.bib17))和Wang
    et al. ([2023](#bib.bib24))探讨了在对话代理中使用LLM生成角色扮演文本。然而，指定不熟悉的作者可能会引发LLM的幻觉问题。此外，考虑到中文的多样性和复杂性，包括成语、修辞手法，直接用LLM阅读文章不仅消耗更多的tokens，影响推理速度，而且在仅阅读部分片段时，会导致对文章风格的理解不足。此外，这些方法也缺乏指导性和可解释性。
- en: 'To overcome the above challenges, we propose a Chinese Article-style Transfer
    framework (CAT-LLM) based on LLMs. Firstly, CAT-LLM combines various machine learning
    algorithms to comprehensively learn article styles, creating a detailed style
    definition. Our analysis focuses on words and sentences levels styles, which are
    not only academically reasonable but also easy to understand. Then, we design
    style-enhanced prompts for style transfer based on style definition. Coupled with
    the LLMs’ powerful capabilities in semantic understanding and long-text generation,
    we obtain the final transfer text. In addition, we select five classic Chinese
    literary works and use ChatGPT to generate stylishless text, creating large parallel
    datasets to support more accurate performance evaluation of style transfer models.
    Extensive experiments demonstrate that our framework surpasses state-of-the-art
    researches in both style accuracy and content preservation. The primary contributions
    of this paper are summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服上述挑战，我们提出了一种基于LLM的中文文章风格迁移框架（CAT-LLM）。首先，CAT-LLM结合了各种机器学习算法，全面学习文章风格，创建了详细的风格定义。我们的分析重点放在词汇和句子级别的风格，这些风格不仅在学术上合理，而且易于理解。然后，我们设计了基于风格定义的风格增强提示进行风格迁移。结合LLM在语义理解和长文本生成方面的强大能力，我们获得了最终的迁移文本。此外，我们选择了五部经典的中文文学作品，并使用ChatGPT生成风格化文本，创建了大规模的平行数据集，以支持对风格迁移模型更准确的性能评估。大量实验表明，我们的框架在风格准确性和内容保留方面超越了最新的研究成果。本文的主要贡献总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose CAT-LLM, a pioneering Chinese Article-style Transfer framework utilizing
    LLMs for the first time in chinese article-style transfer. Extensive experiments
    demonstrate that CAT-LLM surpasses state-of-the-art models in style transfer accuracy
    and content preservation.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了CAT-LLM，这是一种开创性的中文文章风格迁移框架，首次利用LLM进行中文文章风格迁移。大量实验表明，CAT-LLM在风格迁移准确性和内容保留方面超越了最先进的模型。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We engineer a Text Style Definition (TSD) module that integrates various small
    models to proficiently summarize text styles at the words and sentences levels.
    This module not only enhances the LLMs’ style transfer precision, but also supports
    dynamic expansion of internal style trees, allowing for flexible optimization
    in subsequent research.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一个文本风格定义（TSD）模块，集成了各种小型模型，以熟练地总结文本风格在词汇和句子层面上的特征。该模块不仅提高了 LLM 的风格转移精度，还支持内部风格树的动态扩展，从而在后续研究中实现灵活优化。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We create five parallel datasets of Chinese article-style transfer to assess
    the models’ performance more accurately, offering a novel evaluation paradigm
    for subsequent research.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们创建了五个平行的数据集，用于评估模型的性能，以便更准确地评估模型，提供了一种新的评估范式用于后续研究。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Text Style Transfer
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 文本风格转移
- en: Text style transfer has been a focus in NLP research, attracting widespread
    attention in computer science and literature. Hu et al. Hu et al. ([2017](#bib.bib6))
    pioneer the introduction of the concept of style transfer from image to text,
    laying a cornerstone for subsequent explorations in text style transfer. With
    the development of NLP, an increasing number of text style transfer methods have
    emerged. Based on the transfer processes, we can categorize previous works into
    two main families. The first family treats the content and style of a sentence
    as separate entities. It involves stripping the original style from the content
    and supplanting it with the desired style. Tian et al. Lee et al. ([2021](#bib.bib11))
    propose using reverse attention to implicitly remove style information from each
    token to preserve the original content, and introducing conditional layer normalization
    to construct content-dependent style representations. StoryTrans Zhu et al. ([2023](#bib.bib27))
    utilize discourse representations to capture source content information and transfer
    it into the target style through learnable style embeddings. However, in sentences
    and paragraphs, certain tokens mainly reflect distinct personal characteristics.
    Consequently, this family face challenges in guaranteeing the integrity of the
    original textual content. The second family directly designs an end-to-end model
    for style transfer. The Style Transformer Dai et al. ([2019](#bib.bib3)) leverages
    the Transformer as the backbone network, directly supplying the target style embedding
    to the decoder without explicit disentanglement. StyIns Yi et al. ([2021](#bib.bib25))
    utilizes generative flow for a unique style space and provides strong style signals
    to the attention-based decoder through multiple style instances. However, such
    family mainly focuses on the transfer of sentiment and tense in individual English
    sentences, with fewer studies conduct on Chinese long texts.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 文本风格转移一直是 NLP 研究的重点，吸引了计算机科学和文学领域的广泛关注。胡等人（[2017](#bib.bib6)）开创性地引入了从图像到文本的风格转移概念，为后续的文本风格转移探索奠定了基础。随着
    NLP 的发展，越来越多的文本风格转移方法应运而生。基于转移过程，我们可以将以前的研究分为两大类。第一类将句子的内容和风格视为独立的实体。它涉及去除原始风格并用所需风格替代。田等人和李等人（[2021](#bib.bib11)）提出使用反向注意力隐式去除每个词元的风格信息以保留原始内容，并引入条件层归一化来构建依赖于内容的风格表示。StoryTrans（朱等人（[2023](#bib.bib27)））利用话语表示捕捉源内容信息，并通过可学习的风格嵌入将其转移到目标风格中。然而，在句子和段落中，某些词元主要反映独特的个人特征。因此，这一类方法在保证原始文本内容的完整性方面面临挑战。第二类则直接设计端到端的风格转移模型。风格转换器（Dai
    等人（[2019](#bib.bib3)））利用 Transformer 作为主干网络，直接将目标风格嵌入提供给解码器，而无需明确的解耦。StyIns（Yi
    等人（[2021](#bib.bib25)））利用生成流为独特的风格空间提供强大的风格信号，通过多个风格实例提供给基于注意力的解码器。然而，这一类方法主要关注单个英语句子中情感和时态的转移，对中文长文本的研究较少。
- en: '![Refer to caption](img/f4b6d2b1a1ebcf68737ed4f73ae74e56.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/f4b6d2b1a1ebcf68737ed4f73ae74e56.png)'
- en: 'Figure 2: The framework of the proposed CAT-LLM. The English translation of
    the Chinese examples appearing in the framework can refer to subsequent Appendix
    Figure A1.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：提出的 CAT-LLM 的框架。框架中出现的中文示例的英文翻译可以参见后续的附录图 A1。
- en: 2.2 Large Language Models
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 大型语言模型
- en: 'The advent of LLMs has revolutionized NLP research, shifting the focus from
    traditional tasks like translation and classification to more complex, agent-level
    tasks such as role-playing and dialogue systems. This marks a significant advancement
    in the scope of NLP applications and the achievement of higher-level language
    comprehension and generation tasks. In terms of the openness of the LLMs source
    code, it can be categorized into two types: open-source models and closed-source
    models. Open-source LLMs mainly include LLaMA and Yi models, while closed-source
    models are represented by the highly anticipated GPT series. The latest ChatGPT
    can perform tasks that have never been encountered before when providing prompt
    instructions, demonstrating its flexibility and adaptability in facing complex
    and novel challenges. Therefore, designing appropriate prompts becomes particularly
    crucial Brown et al. ([2020](#bib.bib1)). Shao et al. Shao et al. ([2023](#bib.bib18))
    propose Prophet to prompt LLMs with answer heuristics for knowledge-based Visual
    Question Answering. Jang et al. Jang et al. ([2023](#bib.bib7)) investigate the
    effectiveness of negated prompts in directing LLMs, revealing limitations in their
    ability to follow such prompts accurately. Singh et al. Singh et al. ([2023](#bib.bib20))
    introduce a procedural LLMs prompt structure that enable LLMs to directly generate
    sequences of robot operations during task planning. Additionally, Li et al. Li
    et al. ([2023](#bib.bib13)) propose a directed stimulus prompt to guide LLMs in
    achieving specific desired outputs.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现彻底改变了自然语言处理（NLP）研究，将焦点从传统任务如翻译和分类转移到更复杂的代理级任务，如角色扮演和对话系统。这标志着NLP应用范围的显著进展，以及更高水平的语言理解和生成任务的实现。在LLMs源代码的开放性方面，可以分为两种类型：开源模型和闭源模型。开源LLMs主要包括LLaMA和Yi模型，而闭源模型则由备受期待的GPT系列代表。最新的ChatGPT能够在提供提示指令时执行前所未有的任务，展示了其在面对复杂和新颖挑战时的灵活性和适应性。因此，设计合适的提示变得尤为重要。Brown
    等人（[2020](#bib.bib1)）指出。Shao 等人（[2023](#bib.bib18)）提出了Prophet，用于通过答案启发式提示LLMs进行基于知识的视觉问答。Jang
    等人（[2023](#bib.bib7)）研究了否定提示在引导LLMs方面的有效性，揭示了它们在准确遵循这些提示方面的局限性。Singh 等人（[2023](#bib.bib20)）介绍了一种程序化LLMs提示结构，使LLMs在任务规划期间能够直接生成机器人操作序列。此外，Li
    等人（[2023](#bib.bib13)）提出了一种定向刺激提示，旨在引导LLMs实现特定的期望输出。
- en: 3 Methodology
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Task Definition and The Whole Framework
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 任务定义与整体框架
- en: Assuming there is a set of datasets $\{D_{i}\}_{i=1}^{K}$ to comprehensively
    evaluate the performance of the style transfer framework. Our research primarily
    centers on TSD module design and prompt construction, which will be elaborated
    upon in the subsequent text.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一组数据集 $\{D_{i}\}_{i=1}^{K}$ 来全面评估风格转换框架的性能。我们的研究主要集中在TSD模块设计和提示构建上，这将在后续文本中详细阐述。
- en: 3.2 Text Style Definition Module
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 文本风格定义模块
- en: 'As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Text Style Transfer ‣ 2 Related
    Work ‣ CAT-LLM: Prompting Large Language Models with Text Style Definition for
    Chinese Article-style Transfer"), we propose the text style definition (TSD) module,
    which computes the style of the style definition part $D_{i2}$ from both words
    and sentences levels and provides precise style definitions. The TSD module is
    designed to enhance the accurate identification of text styles, providing a clear
    and comprehensive prompt of styles for LLMs. In addition, this module can be seamlessly
    integrated into various LLMs to further optimize the model’s performance in style
    transfer. Simultaneously, it provides interpretability for researchers, enabling
    them to gain deeper insights into the underlying mechanisms of the models in terms
    of text style recognition and transfer.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [2](#S2.F2 "图 2 ‣ 2.1 文本风格转换 ‣ 2 相关工作 ‣ CAT-LLM: 通过文本风格定义提示大型语言模型进行中文文章风格转换")
    所示，我们提出了文本风格定义（TSD）模块，该模块从词汇和句子层面计算风格定义部分 $D_{i2}$ 的风格，并提供精确的风格定义。TSD模块旨在增强对文本风格的准确识别，为LLMs提供清晰而全面的风格提示。此外，该模块可以无缝集成到各种LLMs中，以进一步优化模型在风格转换中的表现。同时，它为研究人员提供了可解释性，使他们能够深入了解模型在文本风格识别和转换方面的潜在机制。'
- en: 3.2.1 Words Level
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 词汇层面
- en: Words form the cornerstone of constructing text and play a crucial role in shaping
    language features Serrano et al. ([2009](#bib.bib16)); Castillo and Tolchinsky
    ([2018](#bib.bib2)). Quantitative analysis of the use of vocabulary in texts can
    reveal language style differences between different texts. In this section, we
    conduct an in-depth analysis of the word style of the article from multiple perspectives
    such as part of speech, word length and syllabic words, modal particles and idioms.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 词语构成了文本的基础，并在塑造语言特征方面发挥了至关重要的作用 Serrano 等人 ([2009](#bib.bib16))；Castillo 和 Tolchinsky
    ([2018](#bib.bib2))。对文本中词汇使用的定量分析可以揭示不同文本之间的语言风格差异。在本节中，我们将从词性、词长和音节词、模态粒子和习语等多个角度深入分析文章的词汇风格。
- en: Part of Speech.We apply the $Cut$ corresponds to the sequence of part-of-speech
    tags. The part-of-speech of words is mainly divided into content words and function
    words. Content words include nouns, verbs, adjectives, etc., while function words
    include adverbs, prepositions, conjunctions, etc. By classifying and statistically
    analyzing the vocabulary in an article, the distribution of part-of-speech can
    be inferred.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 词性。我们应用 $Cut$ 对应词性标签序列。词的词性主要分为实词和虚词。实词包括名词、动词、形容词等，而虚词包括副词、介词、连词等。通过对文章中的词汇进行分类和统计分析，可以推断词性的分布。
- en: '|  | $f_{ps}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argmax}\{I_{ps}(w_{i},t_{i})\}$
    |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{ps}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argmax}\{I_{ps}(w_{i},t_{i})\}$
    |  | (1) |'
- en: where $I_{ps}(\cdot)$.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{ps}(\cdot)$。
- en: Words Length and Syllabic Words. When writers create articles, they have personal
    choices and unique preferences for using words of different lengths Lewis and
    Frank ([2016](#bib.bib12)). By conducting length statistics on the entire article’s
    words, we deduce the word length characteristics of the article.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 词长和音节词。当作者创作文章时，他们对不同长度的词有个人选择和独特偏好 Lewis 和 Frank ([2016](#bib.bib12))。通过对整篇文章的词汇进行长度统计，我们推测文章的词长特征。
- en: '|  | $f_{l}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{l}\{I_{l}(len(w_{i}))\}$
    |  | (2) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{l}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{l}\{I_{l}(len(w_{i}))\}$
    |  | (2) |'
- en: where $len(\cdot)$ most frequently occurring words in monosyllabic and polysyllabic
    words, respectively, as representatives of syllabic words.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $len(\cdot)$ 表示单音节和多音节词中最常出现的词，分别作为音节词的代表。
- en: '|  | $f_{mo},f_{po}={\rm argTop}K_{sy}\{freq(W_{mo},W_{po})\}$ |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{mo},f_{po}={\rm argTop}K_{sy}\{freq(W_{mo},W_{po})\}$ |  | (3) |'
- en: where $W_{mo}$ function is used to count the frequency of each word.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W_{mo}$ 函数用于统计每个词的频率。
- en: Modal Particles and idioms. For the statistics of modal particles and idioms,
    we use the existing modal library³³3https://baike.baidu.com/ and idiom library⁴⁴4https://github.com/crazywhalecc/idiom-database
    for regular matching, thereby extracting the top several modal particles and idioms
    with the highest frequency respectively as representatives.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模态粒子和习语。对于模态粒子和习语的统计，我们使用现有的模态库³³3https://baike.baidu.com/ 和习语库⁴⁴4https://github.com/crazywhalecc/idiom-database
    进行常规匹配，从中提取出频率最高的几个模态粒子和习语作为代表。
- en: '|  | $f_{modal}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{modal}\{freq(Re(w_{i},D_{modal}))\}$
    |  | (4) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{modal}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{modal}\{freq(Re(w_{i},D_{modal}))\}$
    |  | (4) |'
- en: '|  | $f_{idiom}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{idiom}\{freq(Re(w_{i},D_{idiom}))\}$
    |  | (5) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{idiom}=\underset{i\in\{1,2,...,n_{w}\}}{\rm argTop}K_{idiom}\{freq(Re(w_{i},D_{idiom}))\}$
    |  | (5) |'
- en: where $D_{modal}$ is the regularization operation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{modal}$ 是正则化操作。
- en: 'After obtaining various sub-features of words, we integrate these features
    to construct a comprehensive definition of words features:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在获得各种词汇子特征后，我们整合这些特征以构建词汇特征的综合定义：
- en: '|  | $f_{W}={\rm concat}(f_{ps},f_{l},f_{mo},f_{po},f_{modal},f_{idiom})$ |  |
    (6) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{W}={\rm concat}(f_{ps},f_{l},f_{mo},f_{po},f_{modal},f_{idiom})$ |  |
    (6) |'
- en: 3.2.2 Sentences Level
  id: totrans-51
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 句子级别
- en: Compared to the fine-grained analysis at the words level, a comprehensive examination
    of sentences style allows for a more holistic understanding of the author’s unique
    personality and characteristics in language expression Kakoma and Namagero ([2020](#bib.bib8));
    Gajda ([2022](#bib.bib5)). This contributes to a clearer perception of the overall
    style of the text, providing a more global perspective for a profound understanding
    of the article’s stylistic nuances. We mainly analyze the sentences style from
    the perspectives of sentences length, emotion, sentences structure and rhetoric.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 与逐词层面的细致分析相比，对句子风格的全面审查能够更全面地理解作者在语言表达中的独特个性和特征（Kakoma 和 Namagero [2020](#bib.bib8)；Gajda
    [2022](#bib.bib5)）。这有助于更清晰地感知文本的整体风格，为深入理解文章的风格细微差别提供了更全面的视角。我们主要从句子长度、情感、句子结构和修辞四个方面来分析句子风格。
- en: Sentences Length. To evaluate the average sentence length, we consider periods,
    question marks, and exclamation marks as sentences terminators, and use regular
    matching to segment the text into a set of sentences $S=(s_{1},s_{2},...,s_{n_{s}})$.
    By calculating the length of the sentences set and characters set, the average
    sentences length of the article can be obtained.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 句子长度。为了评估平均句子长度，我们将句点、问号和感叹号视为句子终止符，并使用正则匹配将文本分割成一组句子 $S=(s_{1},s_{2},...,s_{n_{s}})$。通过计算句子集合和字符集合的长度，可以得到文章的平均句子长度。
- en: '|  | $f_{len}=len(C)/len(S)$ |  | (7) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{len}=len(C)/len(S)$ |  | (7) |'
- en: 'In addition, when analyzing the tone of sentences length throughout the article,
    we evaluate the length of each sentence and classify it into short or long sentences
    based on a threshold of 20 words Karya and Mahardika ([2019](#bib.bib9)); Wallwork
    and Wallwork ([2016](#bib.bib23)):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在分析整篇文章的句子长度时，我们评估每个句子的长度，并根据 20 个单词的阈值将其分类为短句或长句（Karya 和 Mahardika [2019](#bib.bib9)；Wallwork
    和 Wallwork [2016](#bib.bib23)）：
- en: '|  | $f_{ls}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{ls}(s_{i})\}$
    |  | (8) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{ls}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{ls}(s_{i})\}$
    |  | (8) |'
- en: where $I_{ls}(\cdot)$ is a function for calculating the counts of long sentences
    and short sentences. If the article predominantly contains long sentences, it
    is classified as having more long sentences; conversely, it is categorized as
    having more short sentences. If the numbers are roughly equal, the article is
    considered to have a combination of both long and short sentences.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{ls}(\cdot)$ 是用于计算长句和短句数量的函数。如果文章中长句占主导，则归类为长句较多；相反，则归类为短句较多。如果数量大致相等，则文章被视为长句和短句的组合。
- en: Emotion. For the emotional tone expressed by the sentences, we employ the $Emotion$
    library⁵⁵5https://github.com/hidadeng/cnsenti to evaluate. This function outputs
    the score of each sentence on seven emotions (good, joy, sadness, anger, fear,
    disgust, surprise), and add up the scores of each sentence to determine the dominant
    emotion of the article.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 情感。对于句子表达的情感色调，我们使用 $Emotion$ 库⁵⁵5https://github.com/hidadeng/cnsenti 进行评估。该函数输出每个句子在七种情感（好、快乐、悲伤、愤怒、恐惧、厌恶、惊讶）上的分数，并将每个句子的分数相加，以确定文章的主要情感。
- en: '|  | $f_{em}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{em}(Emotion(s_{i}))\}$
    |  | (9) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{em}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{em}(Emotion(s_{i}))\}$
    |  | (9) |'
- en: where $I_{em}(\cdot)$ is used to calculate the total score of each emotion category.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{em}(\cdot)$ 用于计算每个情感类别的总分。
- en: Sentences Structure and Rhetoric. When evaluating the integrated and scattered
    sentences, we introduce the deep learning model to make classification decisions.
    Given the powerful text generation capabilities of ChatGPT, we design prompts
    to guide it to generate a classification dataset containing integrated and scattered
    sentences, and use this dataset to fine tune the MacBERT model⁶⁶6https://github.com/ymcui/MacBERT,
    developing a specialized Chinese sentences structure binary classification model
    $BERT_{st}$. Classify each sentence in the article to assess the distribution
    of integrated and scattered sentences. If the number of integrated sentences exceeds
    that of scattered sentences, it can be inferred that the entire article is predominantly
    composed of integrated sentences, otherwise more scattered sentences.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 句子结构与修辞。当评估整合句子和分散句子时，我们引入深度学习模型来做出分类决策。鉴于ChatGPT强大的文本生成能力，我们设计了提示来引导其生成一个包含整合句子和分散句子的分类数据集，并利用该数据集来微调MacBERT模型⁶⁶6https://github.com/ymcui/MacBERT，开发一个专门的中文句子结构二分类模型
    $BERT_{st}$。对文章中的每个句子进行分类，以评估整合句子和分散句子的分布。如果整合句子的数量超过分散句子，则可以推断整篇文章主要由整合句子组成，否则更多是分散句子。
- en: '|  | $f_{st}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{st}(BERT_{st}(s_{i}))\}$
    |  | (10) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{st}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argmax}\{I_{st}(BERT_{st}(s_{i}))\}$
    |  | (10) |'
- en: where $I_{st}(\cdot)$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{st}(\cdot)$。
- en: '|  | $f_{rhe}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argTop}K_{rhe}\{I_{rhe}(BERT_{rhe}(s_{i}))\}$
    |  | (11) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{rhe}=\underset{i\in\{1,2,...,n_{s}\}}{\rm argTop}K_{rhe}\{I_{rhe}(BERT_{rhe}(s_{i}))\}$
    |  | (11) |'
- en: 'where $I_{rhe}(\cdot)$ rhetorical devices with the highest number as the main
    rhetorical features of the article. Subsequently, we merge all sentences features
    to form a complete set of sentences features:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{rhe}(\cdot)$ 表示具有最高数量的修辞手段作为文章的主要修辞特征。随后，我们将所有句子特征合并形成完整的句子特征集：
- en: '|  | $f_{S}={\rm concat}(f_{len},f_{ls},f_{em},f_{st},f_{rhe})$ |  | (12) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $f_{S}={\rm concat}(f_{len},f_{ls},f_{em},f_{st},f_{rhe})$ |  | (12) |'
- en: 'Finally, we develop a comprehensive definition of article style by combining
    the features of both words and sentences levels:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过结合词汇和句子级别的特征来制定全面的文章风格定义：
- en: '|  | $F={\rm concat}(f_{W},f_{S})$ |  | (13) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $F={\rm concat}(f_{W},f_{S})$ |  | (13) |'
- en: 3.3 Style-enhanced Prompt
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 风格增强提示
- en: 'In this stage, leveraging the style definition $F$ generated by the TSD module,
    we design a style-enhanced prompt to enhance the zero-shot learning capability
    of LLMs Chinese article-style transfer. The style-enhanced prompt consists of
    a original text, task description, style definition and output indication. The
    complete format of the prompt is shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.3 Style-enhanced
    Prompt ‣ 3 Methodology ‣ CAT-LLM: Prompting Large Language Models with Text Style
    Definition for Chinese Article-style Transfer"). Given that our text feature calculations
    mainly involves statistical analysis of vocabulary and syntactic structures, mechanically
    enumerating these structures may lead to misunderstandings in LLMs, resulting
    in unnecessary hallucinations. Therefore, we appropriately add professional descriptions
    of feature structures for LLMs to better understand stylistic features. For example,
    this style has many long sentences, which make the article rich in connotation,
    specific in narrative, detailed in reasoning, and full of emotion. Furthermore,
    considering the challenges of understanding negative prompts for LLMs, we carefully
    design the task description to ensure that the generated text strikes an optimal
    balance between high transfer accuracy and content preservation, while avoiding
    the introduction of additional textual information.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个阶段，利用TSD模块生成的风格定义$F$，我们设计了一个风格增强的提示，以增强LLMs中文文章风格转换的零样本学习能力。风格增强提示由原始文本、任务描述、风格定义和输出指示组成。提示的完整格式如图
    [3](#S3.F3 "Figure 3 ‣ 3.3 Style-enhanced Prompt ‣ 3 Methodology ‣ CAT-LLM: Prompting
    Large Language Models with Text Style Definition for Chinese Article-style Transfer")所示。鉴于我们的文本特征计算主要涉及词汇和句法结构的统计分析，机械地列举这些结构可能会导致LLMs产生误解，导致不必要的幻觉。因此，我们适当地添加了特征结构的专业描述，以便LLMs更好地理解风格特征。例如，这种风格有许多长句，使文章充满内涵，叙述具体，推理详细，感情丰富。此外，考虑到LLMs对负面提示的理解挑战，我们精心设计任务描述，以确保生成的文本在高转移准确性和内容保留之间取得最佳平衡，同时避免引入额外的文本信息。'
- en: '![Refer to caption](img/a03b5fedc39ee17e8e88df83a4427f8a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a03b5fedc39ee17e8e88df83a4427f8a.png)'
- en: 'Figure 3: Style-enhanced article-style transfer prompt.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 风格增强的文章风格转换提示。'
- en: 4 Experiment
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Datasets
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集
- en: We mainly select five literary works with diverse styles covering modern and
    contemporary China to form our dataset. Fortress Besieged is authored by Qian
    Zhongshu and is classified as a modern novel characterized by humor and satire.
    The Scream is a novel written by Lu Xun and is considered one of his works in
    the genre of realism. The Fifteen Year of the Wanli Era is a historical work written
    by Huang Renyu. The Family Instructions of Zeng Guofan is an ancient family instruction
    authored by Zeng Guofan. The Three-Body Problem is a science fiction novel written
    by Liu Cixin.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要选择了五部风格多样的文学作品，包括现代和当代中国作品，来组成我们的数据集。《围城》由钱钟书创作，属于以幽默和讽刺为特点的现代小说。《呐喊》是鲁迅创作的小说，被认为是其现实主义作品之一。《万里长征》是黄仁宇创作的历史作品。《曾国藩家训》是曾国藩撰写的古代家训。《三体》是刘慈欣创作的科幻小说。
- en: 'We divide each book into two main parts: the style transfer part and the style
    definition part. We creat five parallel Chinese article-style transfer datasets
    by transfering the style definition part into stylishless text using ChatGPT.
    The number of test samples for each book is approximately 400, with each sample
    consisting of 120 to 140 words. Table [1](#S4.T1 "Table 1 ‣ 4.1 Datasets ‣ 4 Experiment
    ‣ CAT-LLM: Prompting Large Language Models with Text Style Definition for Chinese
    Article-style Transfer") presents detailed datasets statistics.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将每本书分为两个主要部分：风格转换部分和风格定义部分。我们通过使用ChatGPT将风格定义部分转换为无风格文本，创建了五个平行的中文文章风格转换数据集。每本书的测试样本数量约为400，每个样本由120到140个词组成。表[1](#S4.T1
    "表 1 ‣ 4.1 数据集 ‣ 4 实验 ‣ CAT-LLM: 使用文本风格定义提示大语言模型进行中文文章风格转换")展示了详细的数据集统计信息。'
- en: 'Table 1: Statistics of five datasets.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 五个数据集的统计数据。'
- en: '| Article | Numbers of style | Numbers of style \bigstrut[t] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 文章 | 风格数量 | 风格数量 \bigstrut[t] |'
- en: '| transfer examples | definition examples \bigstrut[b] |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 转换示例 | 定义示例 \bigstrut[b] |'
- en: '| Fortress Besieged | 623 | 946 \bigstrut[t] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 围城 | 623 | 946 \bigstrut[t] |'
- en: '| The Scream | 217 | 284 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 呐喊 | 217 | 284 |'
- en: '| The Fifteen Year of | 413 | 617 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 十五年 | 413 | 617 |'
- en: '| the Wanli Era |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 万里长征 |'
- en: '| The family instructions | 302 | 1026 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 家训 | 302 | 1026 |'
- en: '| of Zeng Guofan |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 曾国藩家训 |'
- en: '| The Three-Body | 513 | 1762 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 三体 | 513 | 1762 |'
- en: '| Problem | \bigstrut[b] |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 问题 | \bigstrut[b] |'
- en: 'Table 2: Automatic evaluation results on “Fortress Besieged” and “The Scream”.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: “围城”和“呐喊”的自动评估结果。'
- en: '| Article | Models | Transfer Accuracy(%) |  | Content Preservation(%)    \bigstrut
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 文章 | 模型 | 转换准确率(%) |  | 内容保留率(%)    \bigstrut |'
- en: '| BLEU-1 | BLEU-2 | Precision | Recall | F1 |  | BLEU-1 | BLEU-2 | Precision
    | Recall | F1 \bigstrut |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| BLEU-1 | BLEU-2 | 精确度 | 召回率 | F1 |  | BLEU-1 | BLEU-2 | 精确度 | 召回率 | F1 \bigstrut
    |'
- en: '| Fortress Besieged | Style Transformer | 17.80 | 11.03 | 55.45 | 58.79 | 57.08
    |  | 95.34 | 90.93 | 96.87 | 96.50 | 96.69 \bigstrut[t] |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 围城 | 风格转换器 | 17.80 | 11.03 | 55.45 | 58.79 | 57.08 |  | 95.34 | 90.93 | 96.87
    | 96.50 | 96.69 \bigstrut[t] |'
- en: '| RALoCN | 22.86 | 17.01 | 58.52 | 58.71 | 58.70 |  | 92.27 | 88.07 | 95.34
    | 95.84 | 95.59 \bigstrut[b] |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| RALoCN | 22.86 | 17.01 | 58.52 | 58.71 | 58.70 |  | 92.27 | 88.07 | 95.34
    | 95.84 | 95.59 \bigstrut[b] |'
- en: '| Role-play+ChatGLM3-6B-chat | 33.37 | 18.36 | 71.18 | 72.26 | 71.68 |  | 56.85
    | 50.51 | 81.58 | 82.70 | 82.10 \bigstrut[t] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演+ChatGLM3-6B-chat | 33.37 | 18.36 | 71.18 | 72.26 | 71.68 |  | 56.85
    | 50.51 | 81.58 | 82.70 | 82.10 \bigstrut[t] |'
- en: '| Read+ChatGLM3-6B-chat | 21.68 | 7.36 | 60.96 | 62.66 | 61.77 |  | 28.01 |
    14.39 | 63.72 | 64.69 | 64.15 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Read+ChatGLM3-6B-chat | 21.68 | 7.36 | 60.96 | 62.66 | 61.77 |  | 28.01 |
    14.39 | 63.72 | 64.69 | 64.15 |'
- en: '| CAT+ChatGLM3-6B-chat | 42.32 | 24.16 | 75.67 | 76.73 | 76.16 |  | 81.44 |
    79.44 | 90.30 | 90.99 | 90.59 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| CAT+ChatGLM3-6B-chat | 42.32 | 24.16 | 75.67 | 76.73 | 76.16 |  | 81.44 |
    79.44 | 90.30 | 90.99 | 90.59 |'
- en: '| Role-play+Baichuan-13B-chat | 35.13 | 16.54 | 72.18 | 73.88 | 73.01 |  |
    85.86 | 82.24 | 87.45 | 87.15 | 87.29 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演+Baichuan-13B-chat | 35.13 | 16.54 | 72.18 | 73.88 | 73.01 |  | 85.86
    | 82.24 | 87.45 | 87.15 | 87.29 |'
- en: '| Read+Baichuan-13B-chat | 17.71 | 6.91 | 60.23 | 63.91 | 62.00 |  | 29.91
    | 21.92 | 64.27 | 67.34 | 65.74 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Read+Baichuan-13B-chat | 17.71 | 6.91 | 60.23 | 63.91 | 62.00 |  | 29.91
    | 21.92 | 64.27 | 67.34 | 65.74 |'
- en: '| CAT+Baichuan-13B-chat | 43.76 | 26.00 | 76.78 | 79.36 | 77.98 |  | 86.15
    | 84.57 | 92.72 | 93.83 | 93.14 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| CAT+Baichuan-13B-chat | 43.76 | 26.00 | 76.78 | 79.36 | 77.98 |  | 86.15
    | 84.57 | 92.72 | 93.83 | 93.14 |'
- en: '| Role-play+GPT-3.5-turbo | 38.57 | 20.67 | 74.81 | 76.70 | 75.73 |  | 64.40
    | 53.79 | 88.32 | 88.63 | 88.46 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演+GPT-3.5-turbo | 38.57 | 20.67 | 74.81 | 76.70 | 75.73 |  | 64.40 | 53.79
    | 88.32 | 88.63 | 88.46 |'
- en: '| Read+GPT-3.5-turbo | 16.59 | 6.76 | 59.77 | 63.87 | 61.74 |  | 27.03 | 19.05
    | 64.21 | 68.09 | 66.07 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 阅读+GPT-3.5-turbo | 16.59 | 6.76 | 59.77 | 63.87 | 61.74 |  | 27.03 | 19.05
    | 64.21 | 68.09 | 66.07 |'
- en: '| CAT+GPT-3.5-turbo | 45.42 | 26.87 | 77.37 | 79.52 | 78.42 |  | 88.96 | 85.01
    | 96.07 | 96.49 | 96.27 \bigstrut[b] |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| CAT+GPT-3.5-turbo | 45.42 | 26.87 | 77.37 | 79.52 | 78.42 |  | 88.96 | 85.01
    | 96.07 | 96.49 | 96.27 \bigstrut[b] |'
- en: '| The Scream | Style Transformer | 16.89 | 10.22 | 59.58 | 60.71 | 60.13 |  |
    92.71 | 90.38 | 98.23 | 98.13 | 98.18 \bigstrut[t] |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| The Scream | Style Transformer | 16.89 | 10.22 | 59.58 | 60.71 | 60.13 |  |
    92.71 | 90.38 | 98.23 | 98.13 | 98.18 \bigstrut[t] |'
- en: '| RALoCN | 24.72 | 17.14 | 66.09 | 65.48 | 65.78 |  | 89.02 | 87.16 | 95.12
    | 95.85 | 95.49 \bigstrut[b] |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| RALoCN | 24.72 | 17.14 | 66.09 | 65.48 | 65.78 |  | 89.02 | 87.16 | 95.12
    | 95.85 | 95.49 \bigstrut[b] |'
- en: '| Role-play+ChatGLM3-6B-chat | 38.67 | 19.87 | 76.89 | 77.12 | 76.99 |  | 75.62
    | 68.16 | 88.64 | 89.56 | 89.06 \bigstrut[t] |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演+ChatGLM3-6B-chat | 38.67 | 19.87 | 76.89 | 77.12 | 76.99 |  | 75.62
    | 68.16 | 88.64 | 89.56 | 89.06 \bigstrut[t] |'
- en: '| Read+ChatGLM3-6B-chat | 13.72 | 4.17 | 57.88 | 62.96 | 60.30 |  | 13.89 |
    4.82 | 57.99 | 62.84 | 60.30 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 阅读+ChatGLM3-6B-chat | 13.72 | 4.17 | 57.88 | 62.96 | 60.30 |  | 13.89 | 4.82
    | 57.99 | 62.84 | 60.30 |'
- en: '| CAT+ChatGLM3-6B-chat | 43.70 | 24.96 | 77.40 | 77.73 | 77.53 |  | 77.75 |
    75.22 | 91.75 | 92.52 | 92.09 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| CAT+ChatGLM3-6B-chat | 43.70 | 24.96 | 77.40 | 77.73 | 77.53 |  | 77.75 |
    75.22 | 91.75 | 92.52 | 92.09 |'
- en: '| Role-play+Baichuan-13B-chat | 36.07 | 16.93 | 75.24 | 76.22 | 75.72 |  |
    80.90 | 78.51 | 88.37 | 88.17 | 88.27 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演+Baichuan-13B-chat | 36.07 | 16.93 | 75.24 | 76.22 | 75.72 |  | 80.90
    | 78.51 | 88.37 | 88.17 | 88.27 |'
- en: '| Read+Baichuan-13B-chat | 17.14 | 7.05 | 59.32 | 64.18 | 61.64 |  | 21.54
    | 12.78 | 61.24 | 65.85 | 63.44 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 阅读+Baichuan-13B-chat | 17.14 | 7.05 | 59.32 | 64.18 | 61.64 |  | 21.54 |
    12.78 | 61.24 | 65.85 | 63.44 |'
- en: '| CAT+Baichuan-13B-chat | 41.68 | 23.85 | 77.08 | 78.47 | 77.71 |  | 84.15
    | 82.23 | 90.41 | 91.13 | 90.63 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| CAT+Baichuan-13B-chat | 41.68 | 23.85 | 77.08 | 78.47 | 77.71 |  | 84.15
    | 82.23 | 90.41 | 91.13 | 90.63 |'
- en: '| Role-play+GPT-3.5-turbo | 41.13 | 22.91 | 77.60 | 78.45 | 78.01 |  | 73.82
    | 66.03 | 92.42 | 92.27 | 92.34 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演+GPT-3.5-turbo | 41.13 | 22.91 | 77.60 | 78.45 | 78.01 |  | 73.82 | 66.03
    | 92.42 | 92.27 | 92.34 |'
- en: '| Read+GPT-3.5-turbo | 19.76 | 8.94 | 60.91 | 64.75 | 62.74 |  | 29.70 | 22.23
    | 64.94 | 68.59 | 66.68 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 阅读+GPT-3.5-turbo | 19.76 | 8.94 | 60.91 | 64.75 | 62.74 |  | 29.70 | 22.23
    | 64.94 | 68.59 | 66.68 |'
- en: '| CAT+GPT-3.5-turbo | 45.42 | 26.39 | 78.54 | 79.79 | 79.15 |  | 88.89 | 84.69
    | 96.38 | 96.64 | 96.51 \bigstrut[b] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| CAT+GPT-3.5-turbo | 45.42 | 26.39 | 78.54 | 79.79 | 79.15 |  | 88.89 | 84.69
    | 96.38 | 96.64 | 96.51 \bigstrut[b] |'
- en: 4.2 Implementation Details
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实施细节
- en: For the algorithm parameters at the words level, we set $K_{l}$. For the configuration
    of LLMs, the temperature parameter is set to 0, and the seed parameter is uniformly
    set to 42.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于词级别的算法参数，我们设置了$K_{l}$。对于LLMs的配置，温度参数设置为0，种子参数统一设置为42。
- en: 4.3 Evaluation Metrics
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估指标
- en: Our evaluation primarily focus on style transfer accuracy and content preservation.
    A high-quality Chinese article-style transfer model should balance these two aspects.
    We compare the generated text with the original text of the article to calculate
    the transfer accuracy. Simultaneously, comparisons between the generated text
    and the stylishless text are conducted to assess the degree of content preservation.
    The evaluation metrics adopted for both perspectives are exactly the same. Specifically,
    we measure the lexical and semantic similarity between two texts using BLEU-n
    (n=1,2) Papineni et al. ([2002](#bib.bib14)) and BERTScore Zhang et al. ([2020](#bib.bib26)),
    where BERTScore mainly includes Precision, Recall, and F1 score. In addition,
    considering the randomness of LLMs generating text, we transfer each type of text
    ten times to calculate the final average metrics results.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的评估主要关注风格转移的准确性和内容保留。一个高质量的中文文章风格转移模型应平衡这两个方面。我们将生成的文本与原始文本进行比较，以计算转移准确性。同时，将生成的文本与无风格文本进行比较，以评估内容保留的程度。两方面采用的评估指标完全相同。具体而言，我们使用BLEU-n（n=1,2）Papineni等人（[2002](#bib.bib14)）和BERTScore
    Zhang等人（[2020](#bib.bib26)）来测量两个文本之间的词汇和语义相似性，其中BERTScore主要包括精准度、召回率和F1分数。此外，考虑到LLMs生成文本的随机性，我们将每种类型的文本转移十次，以计算最终的平均指标结果。
- en: '![Refer to caption](img/86d8b79a0343ca32fb8576cdf99dc06d.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86d8b79a0343ca32fb8576cdf99dc06d.png)'
- en: 'Figure 4: Radar charts of experimental results from other datasets.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：其他数据集实验结果的雷达图。
- en: 4.4 Baselines
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 基准
- en: 'We choose three currently popular LLMs as our baseline models: GPT-3.5-turbo,
    ChatGLM3-6B-chat, and Baichuan-13B-chat. The GPT-3.5-turbo model is the novel
    generation of LLMs from OpenAI, ChatGLM3-6B-chat is the latest open source model
    of Zhipu AI, and Baichuan-13B-chat represents the latest research on Chinese LLMs.
    These three models represent the current forefront of LLMs, capable of comprehensively
    assessing the robustness and applicability of our framework. Simultaneously, we
    conduct a comparative study on the transfer results of two classic models, Style
    Transformer Dai et al. ([2019](#bib.bib3)) and RACoLN Lee et al. ([2021](#bib.bib11)).
    These two models cover the two paradigms described in section 2.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了三种当前流行的LLM作为我们的基准模型：GPT-3.5-turbo、ChatGLM3-6B-chat和Baichuan-13B-chat。GPT-3.5-turbo模型是OpenAI的最新一代LLM，ChatGLM3-6B-chat是智谱AI的最新开源模型，而Baichuan-13B-chat代表了中国LLM的最新研究。这三种模型代表了当前LLM的前沿，能够全面评估我们框架的鲁棒性和适用性。同时，我们还对两个经典模型的转移结果进行了比较研究，即Style
    Transformer Dai et al. ([2019](#bib.bib3))和RACoLN Lee et al. ([2021](#bib.bib11))。这两个模型涵盖了第2节描述的两种范式。
- en: 'Table 3: Ablation study results on “The Scream”.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '表3: “呐喊”上的消融研究结果。'
- en: '| Style Arrangement | Transfer Accuracy(%) |  | Content Preservation(%)   
    \bigstrut |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 风格排列 | 转移准确率(%) |  | 内容保留(%)    \bigstrut |'
- en: '| --- | --- | --- | --- |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| BLEU-1 | BLEU-2 | Precision | Recall | F1 |  | BLEU-1 | BLEU-2 | Precision
    | Recall | F1 \bigstrut |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| BLEU-1 | BLEU-2 | Precision | Recall | F1 |  | BLEU-1 | BLEU-2 | Precision
    | Recall | F1 \bigstrut |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Sentences | 42.92 | 23.81 | 76.48 | 78.28 | 77.35 |  | 79.22 | 71.60 | 92.09
    | 93.33 | 92.69 \bigstrut[t] |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Sentences | 42.92 | 23.81 | 76.48 | 78.28 | 77.35 |  | 79.22 | 71.60 | 92.09
    | 93.33 | 92.69 \bigstrut[t] |'
- en: '| Words | 44.25 | 25.22 | 77.74 | 78.92 | 78.32 |  | 86.40 | 83.44 | 96.03
    | 96.19 | 96.11 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Words | 44.25 | 25.22 | 77.74 | 78.92 | 78.32 |  | 86.40 | 83.44 | 96.03
    | 96.19 | 96.11 |'
- en: '| Sentences+Words | 44.86 | 25.67 | 78.04 | 79.41 | 78.71 |  | 85.94 | 80.55
    | 95.29 | 95.73 | 95.50 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Sentences+Words | 44.86 | 25.67 | 78.04 | 79.41 | 78.71 |  | 85.94 | 80.55
    | 95.29 | 95.73 | 95.50 |'
- en: '| Words+Sentences | 45.42 | 26.39 | 78.54 | 79.79 | 79.15 |  | 88.89 | 84.69
    | 96.38 | 96.64 | 96.51 \bigstrut[b] |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Words+Sentences | 45.42 | 26.39 | 78.54 | 79.79 | 79.15 |  | 88.89 | 84.69
    | 96.38 | 96.64 | 96.51 \bigstrut[b] |'
- en: 4.5 Experimental Results
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 实验结果
- en: 'Our experimental article transfer models include the proposed CAT-LLM, direct
    reading of articles based on LLMs, role-playing under different LLMs, Style Transformer
    and RACoLN. It is worthy that, for experiments involving LLMs directly reading
    the articles, we ensure the tokens used for reading the articles are roughly consistent
    with the style definition tokens of CAT-LLM to ensure the experiment fairness.
    As shown in Table [2](#S4.T2 "Table 2 ‣ 4.1 Datasets ‣ 4 Experiment ‣ CAT-LLM:
    Prompting Large Language Models with Text Style Definition for Chinese Article-style
    Transfer"), we present the experimental results of the models on the first two
    datasets. The experimental results of the LLMs on other datasets are presented
    in radar charts shown in Figure [4](#S4.F4 "Figure 4 ‣ 4.3 Evaluation Metrics
    ‣ 4 Experiment ‣ CAT-LLM: Prompting Large Language Models with Text Style Definition
    for Chinese Article-style Transfer"). The experimental outcomes of all models
    are delineated in the subsequent Appendix(Table A1,A2,A3). CAT-LLM based on various
    LLMs achieves a better balance between style transfer and content preservation
    in each LLM transfer, providing more precise control over style transfer in generated
    text.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的实验文章转移模型包括所提出的CAT-LLM、基于LLM的直接阅读文章、在不同LLM下的角色扮演、Style Transformer和RACoLN。值得一提的是，对于涉及LLM直接阅读文章的实验，我们确保用于阅读文章的tokens与CAT-LLM的风格定义tokens大致一致，以确保实验的公平性。如表格[2](#S4.T2
    "Table 2 ‣ 4.1 Datasets ‣ 4 Experiment ‣ CAT-LLM: Prompting Large Language Models
    with Text Style Definition for Chinese Article-style Transfer")所示，我们展示了这些模型在前两个数据集上的实验结果。LLMs在其他数据集上的实验结果以图表形式呈现在图[4](#S4.F4
    "Figure 4 ‣ 4.3 Evaluation Metrics ‣ 4 Experiment ‣ CAT-LLM: Prompting Large Language
    Models with Text Style Definition for Chinese Article-style Transfer")中。所有模型的实验结果在后续的附录（表A1、A2、A3）中有所描述。基于各种LLM的CAT-LLM在每个LLM转移中实现了风格转移与内容保留之间的更好平衡，为生成文本中的风格转移提供了更精确的控制。'
- en: In terms of style transfer accuracy, the combined application of CAT+GPT-3.5-turbo
    achieve the best performance on all datasets. In terms of lexical similarity evaluation
    index BLEU-1, CAT+GPT-3.5-turbo has all reached above 45%, while maintaining a
    level of around 80% in semantic similarity. This success is not only attributed
    to the outstanding capabilities of GPT-3.5-turbo but also highlights the effectiveness
    of the TSD module’s style prompts, enabling CAT+GPT-3.5-turbo to excel in article-style
    transfer. However, the experimental results of LLMs directly reading article fragments
    for style transfer are not satisfactory. This is mainly that the LLMs only summarize
    the style of certain fragments of the article and lack understanding of the overall
    style, and illustrate the challenges LLMs face in handling more abstract and deeper
    semantic relationships, reasoning, and text comprehension. For role-playing, LLMs
    excel at imitating their familiar character writing style, while on the contrary,
    the transfer results are significantly weakened. For example, in the style transfer
    of Zeng Guofan’s family teachings, the effect of role-playing is not satisfactory.
    Furthermore, the transfer accuracy of both small models is at a low level. We
    examine the text generated by the small model and find that both the Style Transformer
    and RACoLN exhibit a tendency to replicate the input and selectively replace certain
    words, falling short in comprehending the semantic style features of the text.
    This is also why both exhibit lower transfer accuracy but higher content preservation
    results.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在风格转换准确性方面，CAT+GPT-3.5-turbo 的组合在所有数据集上都表现出最佳性能。在词汇相似度评价指标 BLEU-1 方面，CAT+GPT-3.5-turbo
    的得分均超过 45%，同时在语义相似度方面保持在约 80% 的水平。这一成功不仅归功于 GPT-3.5-turbo 的卓越能力，也突显了 TSD 模块的风格提示的有效性，使
    CAT+GPT-3.5-turbo 在文章风格转换中表现出色。然而，LLM 直接阅读文章片段进行风格转换的实验结果并不令人满意。这主要是因为 LLM 仅总结了文章某些片段的风格，而缺乏对整体风格的理解，说明
    LLM 在处理更抽象和深层次的语义关系、推理和文本理解方面面临挑战。对于角色扮演，LLM 擅长模仿其熟悉的角色写作风格，而相反，转换结果显著减弱。例如，在曾国藩家训的风格转换中，角色扮演的效果并不令人满意。此外，两个小模型的转换准确率都处于低水平。我们检查了小模型生成的文本，发现
    Style Transformer 和 RACoLN 都表现出复制输入和选择性替换某些词汇的倾向，未能理解文本的语义风格特征。这也是为什么它们都表现出较低的转换准确率但较高的内容保留结果。
- en: Regarding content preservation, CAT-LLM has also achieved competitive results
    in the article-style transfer of various LLMs. In the transfer experiment of directly
    reading articles with LLMs, we find the problem of introducing a large number
    of unrelated words, indicating that LLMs may be prone to hallucinations when dealing
    with complex problems. It is worth noting that the combination of our CAT-LLM
    also outperforms the Role-play+GPT-3.5-turbo, indicating that the proposed TSD
    module can develop the enormous potential of LLMs in article-style transfer.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 关于内容保留，CAT-LLM 在各种 LLM 的文章风格转换中也取得了具有竞争力的成果。在直接用 LLM 阅读文章的转换实验中，我们发现引入大量不相关词汇的问题，这表明
    LLM 在处理复杂问题时可能容易产生幻觉。值得注意的是，我们的 CAT-LLM 的组合也优于 Role-play+GPT-3.5-turbo，这表明所提出的
    TSD 模块能够开发 LLM 在文章风格转换中的巨大潜力。
- en: 4.6 Ablation Study
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 消融研究
- en: 'To further investigate the role and interaction of words level and sentences
    level style definitions in the TSD module, we conduct ablation experiments on
    the “The Scream” dataset based on GPT-3.5-turbo. As shown in Table [3](#S4.T3
    "Table 3 ‣ 4.4 Baselines ‣ 4 Experiment ‣ CAT-LLM: Prompting Large Language Models
    with Text Style Definition for Chinese Article-style Transfer"), words level prompts
    have a more significant positive impact on LLMs’ understanding of text style than
    sentences level prompts. This may be attributed to the fact that words level prompts
    provide more granular and localized style information, enabling LLMs to more precisely
    capture subtle semantic nuances and stylistic variations in the text. In contrast,
    sentences level prompts may be more macroscopic and struggle to convey the finer
    stylistic differences within the text. Furthermore, placing words level prompts
    before sentences level prompts yields better results in style transfer experiments
    compared to the reverse order. This may be because words level prompts can guide
    the LLMs to initially focus on fine-grained semantics and local stylistic information
    in the text. This arrangement can more effectively learn and retain the subtle
    semantic differences in the text, to more accurately convey the required style
    and enhance the sensitivity of the LLMs to the overall style of the text.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '为了进一步研究 TSD 模块中词级别和句级别风格定义的作用和互动，我们基于 GPT-3.5-turbo 在“The Scream”数据集上进行了消融实验。如表
    [3](#S4.T3 "Table 3 ‣ 4.4 Baselines ‣ 4 Experiment ‣ CAT-LLM: Prompting Large
    Language Models with Text Style Definition for Chinese Article-style Transfer")
    所示，词级别提示对 LLM 理解文本风格的正面影响比句级别提示更显著。这可能是因为词级别提示提供了更细致和局部的风格信息，使 LLM 能够更精确地捕捉文本中的细微语义差异和风格变化。相比之下，句级别提示可能更宏观，难以传达文本中的细腻风格差异。此外，将词级别提示置于句级别提示之前，在风格迁移实验中效果更佳。这可能是因为词级别提示可以引导
    LLM 初步关注文本中的细粒度语义和局部风格信息。这种安排能够更有效地学习和保留文本中的细微语义差异，从而更准确地传达所需风格，提升 LLM 对文本整体风格的敏感性。'
- en: 5 Conclusion and Future Work
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: In this paper, we propose the Chinese Article-style Transfer framework (CAT-LLM),
    a pioneering application of LLMs in the realm of Chinese long-text style transfer.
    The CAT-LLM framework, with its Text Style Definition (TSD) module, meticulously
    analyzes text at both words and sentences levels. By integrating various machine
    learning small models, our TSD module enhances LLMs’ directed generation, enriching
    literary analysis and computational linguistics. This approach effectively guides
    LLMs to grasp the stylistic nuances of texts more profoundly. Furthermore, leveraging
    ChatGPT, we create five parallel datasets of Chinese long texts, providing a new
    paradigm of the evaluation of transfer models for future researchers. Extensive
    experiments across these datasets confirm that CAT-LLM outperforms existing methods
    in balancing transfer accuracy and content preservation.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了中文文章风格迁移框架（CAT-LLM），这是 LLM 在中文长文本风格迁移领域的开创性应用。CAT-LLM 框架通过其文本风格定义（TSD）模块，精确分析了文本的词级别和句级别。通过整合各种机器学习小模型，我们的
    TSD 模块增强了 LLM 的定向生成，丰富了文学分析和计算语言学。这种方法有效引导 LLM 更深刻地把握文本的风格细微差别。此外，利用 ChatGPT，我们创建了五个中文长文本的并行数据集，为未来研究者提供了评估迁移模型的新范式。广泛的实验验证了
    CAT-LLM 在平衡迁移准确性和内容保留方面优于现有方法。
- en: Given the tremendous success of our framework, we plan to not directly use prompt
    in future work, but instead use the datasets and style data summarized in this
    study to construct supervised fine-tuning data and train LLMs to improve their
    applicability and efficiency in style transfer tasks. We highlight this task and
    leave it for future work.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们框架的巨大成功，我们计划在未来的工作中不直接使用提示，而是利用本研究总结的数据集和风格数据来构建监督微调数据，并训练 LLM 以提高其在风格迁移任务中的适用性和效率。我们突出这一任务并将其留待未来工作中完成。
- en: References
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. Advances in neural information
    processing systems, 33:1877–1901, 2020.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 语言模型是少样本学习者。神经信息处理系统的进展，33:1877–1901, 2020。
- en: Castillo and Tolchinsky [2018] Cristina Castillo and L. Tolchinsky. The contribution
    of vocabulary knowledge and semantic orthographic fluency to text quality through
    elementary school in catalan. Reading and Writing, 31:293–323, 2018.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Castillo 和 Tolchinsky [2018] Cristina Castillo 和 L. Tolchinsky。词汇知识和语义正字法流畅性对加泰罗尼亚语小学阶段文本质量的贡献。《阅读与写作》，31:293–323，2018年。
- en: 'Dai et al. [2019] Ning Dai, Jianze Liang, Xipeng Qiu, and Xuanjing Huang. Style
    transformer: Unpaired text style transfer without disentangled latent representation.
    In Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, pages 5997–6007, 2019.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等人 [2019] Ning Dai, Jianze Liang, Xipeng Qiu, 和 Xuanjing Huang。风格变换器：无缝文本风格迁移而不需要解耦的潜在表示。在第57届计算语言学协会年会的会议记录中，第5997–6007页，2019年。
- en: 'Danescu-Niculescu-Mizil et al. [2013] Cristian Danescu-Niculescu-Mizil, Moritz
    Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. A computational approach
    to politeness with application to social factors. In Proceedings of the 51st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    pages 250–259, 2013.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danescu-Niculescu-Mizil 等人 [2013] Cristian Danescu-Niculescu-Mizil, Moritz Sudhof,
    Dan Jurafsky, Jure Leskovec, 和 Christopher Potts。带有社交因素应用的礼貌性计算方法。在第51届计算语言学协会年会（第1卷：长篇论文）的会议记录中，第250–259页，2013年。
- en: Gajda [2022] S. Gajda. The faces of style and stylistics. Journal of Linguistics/Jazykovedný
    casopis, 73:7–26, 2022.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gajda [2022] S. Gajda。风格和风格学的面貌。《语言学期刊/Jazykovedný casopis》，73:7–26，2022年。
- en: Hu et al. [2017] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov,
    and Eric P. Xing. Toward controlled generation of text. In Proceedings of the
    34th International Conference on Machine Learning - Volume 70, page 1587–1596,
    2017.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2017] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, 和
    Eric P. Xing。朝向受控文本生成。在第34届国际机器学习大会 - 第70卷的会议记录中，第1587–1596页，2017年。
- en: Jang et al. [2023] Joel Jang, Seonghyeon Ye, and Minjoon Seo. Can large language
    models truly understand prompts? a case study with negated prompts. In Transfer
    Learning for Natural Language Processing Workshop, pages 52–62, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jang 等人 [2023] Joel Jang, Seonghyeon Ye, 和 Minjoon Seo。大型语言模型是否真正理解提示？一个关于否定提示的案例研究。在自然语言处理转移学习研讨会中，第52–62页，2023年。
- en: Kakoma and Namagero [2020] G. Kakoma and Dr. Shira Tendo Namagero. A stylistic
    analysis of ’o uganda, land of beauty’ by prof.george kakoma. International Journal
    on Studies in English Language and Literature, 2020.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kakoma 和 Namagero [2020] G. Kakoma 和 Dr. Shira Tendo Namagero。对 Prof. George
    Kakoma 所著《哦乌干达，美丽的土地》的风格分析。《国际英语语言与文学研究期刊》，2020年。
- en: 'Karya and Mahardika [2019] I Wayan Sidha Karya and Ida Bagus Adhika Mahardika.
    A study on how long and short sentences show the story’s pacing in anthony horowitz’s
    raven’s gate. SPHOTA: Jurnal Linguistik dan Sastra, 2019.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Karya 和 Mahardika [2019] I Wayan Sidha Karya 和 Ida Bagus Adhika Mahardika。研究长短句子如何展示安东尼·霍洛维茨《乌鸦之门》中故事的节奏。《SPHOTA:
    Jurnal Linguistik dan Sastra》，2019年。'
- en: Lai et al. [2023] Huiyuan Lai, Antonio Toral, and Malvina Nissim. Multidimensional
    evaluation for text style transfer using chatgpt. arXiv preprint arXiv:2304.13462,
    2023.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lai 等人 [2023] Huiyuan Lai, Antonio Toral, 和 Malvina Nissim。使用 ChatGPT 的多维度文本风格迁移评估。arXiv
    预印本 arXiv:2304.13462，2023年。
- en: 'Lee et al. [2021] Dongkyu Lee, Zhiliang Tian, Lanqing Xue, and Nevin L. Zhang.
    Enhancing content preservation in text style transfer using reverse attention
    and conditional layer normalization. In Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 93–102,
    2021.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2021] Dongkyu Lee, Zhiliang Tian, Lanqing Xue, 和 Nevin L. Zhang。利用逆向注意力和条件层归一化来增强文本风格迁移中的内容保留。在第59届计算语言学协会年会及第11届国际自然语言处理联合会议（第1卷：长篇论文）的会议记录中，第93–102页，2021年。
- en: Lewis and Frank [2016] M. Lewis and Michael C. Frank. The length of words reflects
    their conceptual complexity. Cognition, 153:182–195, 2016.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewis 和 Frank [2016] M. Lewis 和 Michael C. Frank。单词的长度反映了它们的概念复杂性。《认知》，153:182–195，2016年。
- en: Li et al. [2023] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng
    Gao, and Xifeng Yan. Guiding large language models via directional stimulus prompting.
    arXiv preprint arXiv:2302.11520, 2023.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023] Zekun Li, Baolin Peng, Pengcheng He, Michel Galley, Jianfeng Gao,
    和 Xifeng Yan。通过方向性刺激提示引导大型语言模型。arXiv 预印本 arXiv:2302.11520，2023年。
- en: 'Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings
    of the 40th annual meeting of the Association for Computational Linguistics, pages
    311–318, 2002.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Papineni et al. [2002] Kishore Papineni, Salim Roukos, Todd Ward, 和 Wei-Jing
    Zhu. Bleu：一种自动评估机器翻译的方法。见于《第40届计算语言学协会年会论文集》，第311–318页，2002年。
- en: Przybyla [2020] Piotr Przybyla. Capturing the style of fake news. In Proceedings
    of the AAAI conference on artificial intelligence, volume 34, pages 490–497, 2020.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Przybyla [2020] Piotr Przybyla. 捕捉虚假新闻的风格。见于《AAAI人工智能会议论文集》，第34卷，第490–497页，2020年。
- en: Serrano et al. [2009] M. Serrano, A. Flammini, and F. Menczer. Modeling statistical
    properties of written text. PLoS ONE, 4, 2009.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Serrano et al. [2009] M. Serrano, A. Flammini, 和 F. Menczer. 建模书面文本的统计特性。PLoS
    ONE, 4, 2009年。
- en: Shanahan et al. [2023] Murray Shanahan, Kyle McDonell, and Laria Reynolds. Role
    play with large language models. Nature, pages 1–6, 2023.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shanahan et al. [2023] Murray Shanahan, Kyle McDonell, 和 Laria Reynolds. 与大语言模型进行角色扮演。自然，
    第1–6页，2023年。
- en: Shao et al. [2023] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large
    language models with answer heuristics for knowledge-based visual question answering.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 14974–14983, 2023.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao et al. [2023] Zhenwei Shao, Zhou Yu, Meng Wang, 和 Jun Yu. 使用答案启发式提示大语言模型以进行基于知识的视觉问答。见于《IEEE/CVF计算机视觉与模式识别会议论文集》，第14974–14983页，2023年。
- en: Sheikha and Inkpen [2010] Fadi Abu Sheikha and Diana Inkpen. Automatic classification
    of documents by formality. In Proceedings of the 6th international conference
    on natural language processing and knowledge engineering (nlpke-2010), pages 1–5.
    IEEE, 2010.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheikha 和 Inkpen [2010] Fadi Abu Sheikha 和 Diana Inkpen. 按形式自动分类文档。见于《第6届国际自然语言处理与知识工程会议论文集（nlpke-2010）》，第1–5页。IEEE，2010年。
- en: 'Singh et al. [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. Progprompt:
    Generating situated robot task plans using large language models. In 2023 IEEE
    International Conference on Robotics and Automation (ICRA), pages 11523–11530\.
    IEEE, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh et al. [2023] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal,
    Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, 和 Animesh Garg. Progprompt：使用大语言模型生成具身机器人任务计划。见于2023
    IEEE国际机器人与自动化会议（ICRA），第11523–11530页。IEEE，2023年。
- en: 'Song et al. [2023] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied
    agents with large language models. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 2998–3009, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. [2023] Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler,
    Wei-Lun Chao, 和 Yu Su. Llm-planner：针对具身代理的少样本有根据规划。见于《IEEE/CVF国际计算机视觉会议论文集》，第2998–3009页，2023年。
- en: Toshevska and Gievska [2021] Martina Toshevska and Sonja Gievska. A review of
    text style transfer using deep learning. IEEE Transactions on Artificial Intelligence,
    2021.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Toshevska 和 Gievska [2021] Martina Toshevska 和 Sonja Gievska. 使用深度学习的文本风格迁移综述。IEEE人工智能学报，2021年。
- en: 'Wallwork and Wallwork [2016] Adrian Wallwork and Adrian Wallwork. Teaching
    students to recognize the pros and cons of short and long sentences. English for
    Academic Research: A Guide for Teachers, pages 69–77, 2016.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wallwork 和 Wallwork [2016] Adrian Wallwork 和 Adrian Wallwork. 教授学生识别短句和长句的优缺点。学术研究英语：教师指南，第69–77页，2016年。
- en: 'Wang et al. [2023] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu,
    Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al.
    Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large
    language models. arXiv preprint arXiv:2310.00746, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2023] Zekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu,
    Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, 等.
    Rolellm：基准测试、引出和增强大语言模型的角色扮演能力。arXiv预印本 arXiv:2310.00746，2023年。
- en: Yi et al. [2021] Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, and Maosong Sun. Text
    style transfer via learning style instance supported latent space. In Proceedings
    of the Twenty-Ninth International Conference on International Joint Conferences
    on Artificial Intelligence, pages 3801–3807, 2021.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi et al. [2021] Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, 和 Maosong Sun. 通过学习风格实例支持的潜在空间进行文本风格迁移。见于《第29届国际人工智能联合会议论文集》，第3801–3807页，2021年。
- en: 'Zhang et al. [2020] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,
    and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International
    Conference on Learning Representations, 2020.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2020] 张天怡、Varsha Kishore、Felix Wu、Kilian Q Weinberger 和 Yoav Artzi。Bertscore：用
    bert 评估文本生成。发表于国际学习表征会议，2020年。
- en: 'Zhu et al. [2023] Xuekai Zhu, Jian Guan, Minlie Huang, and Juan Liu. StoryTrans:
    Non-parallel story author-style transfer with discourse representations and content
    enhancing. In Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 14803–14819, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2023] 朱雪凯、关剑、黄敏磊 和 刘娟。StoryTrans：基于话语表示和内容增强的非平行故事作者风格转换。发表于第61届计算语言学协会年会论文集（第1卷：长篇论文），第14803–14819页，2023年。
