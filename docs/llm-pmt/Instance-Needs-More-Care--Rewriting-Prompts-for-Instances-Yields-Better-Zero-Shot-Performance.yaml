- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:15'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot
    Performance'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.02107](https://ar5iv.labs.arxiv.org/html/2310.02107)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Saurabh Srivastava
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: George Mason University
  prefs: []
  type: TYPE_NORMAL
- en: ssrivas6@gmu.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Chengyue Huang^∗'
  prefs: []
  type: TYPE_NORMAL
- en: Tippie College of Business
  prefs: []
  type: TYPE_NORMAL
- en: University of Iowa
  prefs: []
  type: TYPE_NORMAL
- en: chengyue-huang@uiowa.edu
  prefs: []
  type: TYPE_NORMAL
- en: \ANDWeiguo Fan
  prefs: []
  type: TYPE_NORMAL
- en: Tippie College of Business
  prefs: []
  type: TYPE_NORMAL
- en: University of Iowa
  prefs: []
  type: TYPE_NORMAL
- en: weiguo-fan@uiowa.edu &Ziyu Yao
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science
  prefs: []
  type: TYPE_NORMAL
- en: George Mason University
  prefs: []
  type: TYPE_NORMAL
- en: ziyuyao@gmu.edu Equal Contribution
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Enabling large language models (LLMs) to perform tasks in zero-shot has been
    an appealing goal owing to its labor-saving (i.e., requiring no task-specific
    annotations); as such, zero-shot prompting approaches also enjoy better task generalizability.
    To improve LLMs’ zero-shot performance, prior work has focused on devising more
    effective task instructions (e.g., “let’s think step by step” (Kojima et al.,
    [2022](#bib.bib12))). However, we argue that, in order for an LLM to solve them
    correctly in zero-shot, individual test instances need more carefully designed
    and customized instructions. To this end, we propose PRoMPTd, an approach that
    rewrites the task prompt for each individual test input to be more specific, unambiguous,
    and complete, so as to provide better guidance to the task LLM. We evaluated PRoMPTd
    on eight datasets covering tasks including arithmetics, logical reasoning, and
    code generation, using GPT-4 as the task LLM. Notably, PRoMPTd achieves an absolute
    improvement of around 10% on the complex MATH dataset and 5% on the code generation
    task on HumanEval, outperforming conventional zero-shot methods. In addition,
    we also showed that the rewritten prompt can provide better interpretability of
    how the LLM resolves each test instance, which can potentially be leveraged as
    a defense mechanism against adversarial prompting.¹¹1The source code and dataset
    can be obtained from [https://github.com/salokr/PRoMPTd](https://github.com/salokr/PRoMPTd).
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) have shown remarkable success in achieving comparable
    or even surpassing human annotation quality in various tasks. These models perform
    downstream tasks primarily via prompting: all relevant task specification and
    data to process is formatted as a textual context, and the models return a generated
    text completion. The success of these architectures has entailed explorations
    from many fields with a major focus on prompt engineering (Shin et al., [2020](#bib.bib22);
    Liu et al., [2023a](#bib.bib14)), including methods built upon human instinct
    or domain knowledge (Gilardi et al., [2023](#bib.bib7)), data-driven approaches (Zhou
    et al., [2022](#bib.bib33)), prompt optimization via in-context learning (Pryzant
    et al., [2023](#bib.bib20)), etc.'
  prefs: []
  type: TYPE_NORMAL
- en: The dominant prompt-based applications of LLMs can be categorized into two types,
    i.e., zero-shot and few-shot in-context learning. In zero-shot prompting (Kojima
    et al., [2022](#bib.bib12)), LLMs are provided with only a general instruction
    for the task at hand, while in few-shot learning (Brown et al., [2020](#bib.bib3))
    they are additionally supplied with a number of input-output pairs as task demonstrations,
    followed by the test input. Few-shot prompting offers more detailed information
    through the task demonstrations, but on the other hand is also sensitive to the
    demonstration selection and ordering (Pryzant et al., [2023](#bib.bib20); Lu et al.,
    [2021](#bib.bib17)). In contrast, zero-shot prompting gets rid of these issues
    due to its simplicity and further boasts better task generalizability by eschewing
    the need for task-specific annotations. This not only ensures efficiency in terms
    of time and resources but also capitalizes on LLMs’ extensive knowledge base,
    allowing versatility across multiple domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nevertheless, a significant concern remains: LLMs’ zero-shot performance, particularly
    in complex tasks like mathematical or logical reasoning, frequently trails behind
    the few-shot prompting (Zhang et al., [2023](#bib.bib32)). While well-curated
    instructions can evoke optimal responses, their efficiency may wane across different
    tasks. To improve zero-shot prompting, Kojima et al. ([2022](#bib.bib12)) have
    proposed the usage of single task instruction such as “let’s think step by step”
    to elicit reasoning steps and rationales from LLMs enhancing zero-shot capabilities
    of LLMs. Such single-task instruction, however, may lack the necessary specificity
    and clarity, since the task instruction can be vague or provide only a general
    description which is not easy for an LLM to interpret. In addition, such single-task
    instructions may not produce sensible responses for tasks outside reasoning, such
    as code or content generation (Section [3](#S3 "3 Experiments ‣ Instance Needs
    More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance")).
    To the best of our knowledge, prompt optimization for zero-shot prompting techniques
    remains a rather underexplored field of study.'
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the need to optimize prompts for zero-shot prompting techniques,
    in this paper, we propose PRoMPTd to enrich task instructions at the instance
    level by performing rewriting. Our motivation behind rewriting the prompt for
    each task instance in isolation is inspired by the fact that each test instance
    has its own set of requirements to be satisfied. For example, to solve a complex
    mathematical problem involving series summation, one could benefit from the mention
    or hint involving series names or tricks; in contrast, for creative tasks such
    as generating a poem, a more persona-driven instruction such as “Compose a poem
    in the voice of a 19th-century romantic poet” might be more pertinent. To achieve
    this goal, PRoMPTd uses an LLM to rewrite a prompt (i.e., a concatenation of the
    task instruction and the specific test input) to be more specific, complete, unambiguous,
    and optionally with a more easily parsable output format. Specifically, it formulates
    the prompt rewriting task itself as a few-shot in-context learning problem and
    leverages a set of 10 demonstrations distributed across 5 different task types
    to learn the skill. The rewritten prompts are then executed against the zero-shot
    LLM to elicit the response. In particular, our approach leverages the generalist
    capabilities of LLMs in two ways, i.e., instructing an LLM to rewrite prompts
    via in-context learning from contrastive pairs of good and bad prompts, and guiding
    the LLM to reason over a prompt’s shortcomings before rewriting it, so as to make
    the prompt rewriting process more grounded and interpretable.
  prefs: []
  type: TYPE_NORMAL
- en: On eight datasets in our experiments, PRoMPTd achieved impressive improvements
    and closed the gap qualitatively when compared to traditional zero-shot approaches.
    We also show that single prompts such as “Let’s solve this step by step.” or task-specific
    single prompts are not suitable for complex tasks such as mathematical reasoning
    tasks in the MATH dataset (Hendrycks et al., [2021](#bib.bib8)) and struggle in
    non-reasoning tasks such as code generation or content generation, echoing the
    need for prompt engineering at test-instance level. With ablation studies, we
    found that seeking guidance from similar task types and learning to reason over
    a prompt’s shortcomings, both play an important role in prompt rewriting. With
    human evaluation, we further confirmed that PRoMPTd indeed rewrites prompts to
    be more direct, unambiguous, and complete. Our additional analyses also revealed
    the potential for PRoMPTd to generalize and improve over task types that are not
    covered by its 10 demonstrations. The rewritten prompts with details also offer
    interpretability of how an LLM understands a test input, which as we showed could
    be leveraged as a defense mechanism against adversarial prompting (Maus et al.,
    [2023](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Improving Zero-Shot LLMs with Instance Prompt Rewriting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a98dbbeb42cb7bf0ecf32a1d01b6255c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of PRoMPTd compared with naive zero-shot and zero-shot
    Chain-of-Thought (Kojima et al., [2022](#bib.bib12), CoT). PRoMPTd rewrites the
    task instance to be less ambiguous and more specific, leading to a more accurate
    and better-formatted answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Background and Task Formulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this work, we focus on improving the prompts of LLMs in the zero-shot setting.
    Formally, we denote the LLM as $\mathcal{M}_{task}$.
  prefs: []
  type: TYPE_NORMAL
- en: Notably, we propose to perform this prompt rewriting at the *instance* level.
    That is, unlike prior work which targets improving a single task instruction $e$)
    for each individual test instance is necessary. Intuitively, this instance-level
    prompt rewriting will allow for better customized “hints” that can more effectively
    guide a zero-shot LLM to solve the specific test input.
  prefs: []
  type: TYPE_NORMAL
- en: '2.2 PRoMPTd: Prompt Rewriting for Instances'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Realizing the prompt rewriting function $\mathcal{F}:\rho\rightarrow\rho^{*}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we first delineate four criteria that define a good prompt: (1)
    Specificity, i.e., having a special application, bearing, or reference; otherwise,
    having a vague or non-specific prompt may result in hallucinated or incorrect
    responses (Pryzant et al., [2023](#bib.bib20)). (2) Non-Ambiguity, i.e., being
    clear and avoiding misinterpretation, as ambiguous prompts may lead to incorrect
    responses (Tamkin et al., [2023](#bib.bib25)). (3) Completeness, i.e., containing
    all the necessary steps to solve the specific test input; an incomplete prompt
    may lead to an incorrect or incomplete solution (Wu et al., [2023](#bib.bib30))
    and (4) Structuredness, i.e., specifying the output format with a certain structure,
    such that answers (e.g., numerical answers to mathematical reasoning questions)
    can be easily parsed from the LLM output.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To encourage the rewritten prompt $\rho^{*}$. An example of a rewritten prompt
    is shown in Figure [1](#S2.F1 "Figure 1 ‣ 2 Improving Zero-Shot LLMs with Instance
    Prompt Rewriting ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields
    Better Zero-Shot Performance").'
  prefs: []
  type: TYPE_NORMAL
- en: '2.3 PRoMT: Dataset of Demonstrations for Prompt Rewriting'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the challenges of realizing PRoMPTd lies in collecting the prompt rewriting
    demonstrations $\mathcal{D}$, which we will release to facilitate future research.
  prefs: []
  type: TYPE_NORMAL
- en: Based on the common applications of LLMs, we focus on task types including mathematical
    reasoning (Cobbe et al., [2021](#bib.bib5); Hendrycks et al., [2021](#bib.bib8)),
    logical reasoning (Srivastava et al., [2022](#bib.bib24)), code generation (Chen
    et al., [2021](#bib.bib4)), and content generation (Agossah et al., [2023](#bib.bib1)).
    We also add the task of instruction induction (Zhou et al., [2022](#bib.bib33))
    to encourage prompt rewriting. For each task type above, we prompt ChatGPT to
    generate a good prompt $\rho^{*}_{i}$, we manually execute both prompts in a zero-shot
    setting and examine their effects. We repeat the procedure to obtain a contrastive
    pair of prompts that satisfy the requirement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we stress that none of the benchmark datasets that we will use in
    experiments were exposed to the collection of PRoMT, although the same task types
    are covered. In Section [3.6](#S3.SS6 "3.6 Can PRoMPTd Handle New Task Types?
    ‣ 3 Experiments ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields
    Better Zero-Shot Performance"), we will demonstrate that our approach PRoMPTd,
    when prompted with contrastive pairs in PRoMT, can learn to rewrite prompts for
    unseen task types as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Experimental Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We conduct experiments on a diverse set of eight tasks involving (1) mathematical
    reasoning, i.e., GSM-8k (Cobbe et al., [2021](#bib.bib5)) and a subset of 350
    samples of MATH²²2Following Lightman et al. ([2023](#bib.bib13)), for cost-purposes,
    we randomly sampled ten instances from the five difficulty categories across 7
    sub-categories. (Hendrycks et al., [2021](#bib.bib8)); (2) a subset of five tasks
    from BigBench (Srivastava et al., [2022](#bib.bib24)) tailored for logical reasoning
    including Date Understanding, Sports Understanding, Analytical Entailment, Known-Unknowns,
    and Anachronisms; and (3) code generation based on the HumanEval dataset (Chen
    et al., [2021](#bib.bib4)). Each task is evaluated using its own, standard metric.
  prefs: []
  type: TYPE_NORMAL
- en: 'On each task dataset, we apply PRoMPTd to rewrite prompts for its test instances
    and perform zero-shot prompting using the rewritten prompts. We compared our approach
    with two baselines, the naive zero-shot prompting (denoted “Zero-Shot”), and zero-shot
    Chain-of-Thought (Kojima et al., [2022](#bib.bib12)) where a fixed instruction
    of “Let’s think step by step” is appended to each test input to trigger an LLM
    to reason (denoted “Zero-Shot CoT”). In addition, we also consider a variant of
    PRoMPTd, which optimizes prompts at the task level (denoted “PRoMPTd (Instruction-only)”)
    and can provide us insights on whether it is beneficial to rewrite prompts for
    individual instances. To implement this variant, we take the task instruction
    of each dataset (as provided by the source dataset or written by us manually)
    as the prompt $\rho$ and apply PRoMPTd to it. We note that for the baselines and
    the variant, because of the lack of output format specification, we follow Kojima
    et al. ([2022](#bib.bib12)) to perform a second prompt to extract their answers
    (Figure [1](#S2.F1 "Figure 1 ‣ 2 Improving Zero-Shot LLMs with Instance Prompt
    Rewriting ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better
    Zero-Shot Performance")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In all experiments, we use GPT-4 as both the zero-shot task LLM ($\mathcal{M}_{task}$).
    We set the sampling temperature to 1 and top_k to 1\. We include all reproducible
    details, e.g., the specific prompt scripts, in Appendix [A](#A1 "Appendix A Implementation
    Details ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better
    Zero-Shot Performance").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Main Experimental Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Dataset | Zero-Shot | Zero-Shot CoT | PRoMPTd (Instruction-only) | PRoMPTd
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MATH | 48.857 | 56.571 | 57.429 | 66.000 (+8.6)     |'
  prefs: []
  type: TYPE_TB
- en: '| GSM-8K | 90.144 | 92.494 | 92.576 | 94.685 (+2.1)     |'
  prefs: []
  type: TYPE_TB
- en: '| Code Generation | 67.000 | - | 66.868 | 72.561 (+5.7)     |'
  prefs: []
  type: TYPE_TB
- en: '| Analytical Entailment | 65.714 | 80.000 | 82.857 | 82.857  (+0.0)     |'
  prefs: []
  type: TYPE_TB
- en: '| Known-Unknowns | 86.957 | 86.957 | 56.522 | 89.130 (+2.2)     |'
  prefs: []
  type: TYPE_TB
- en: '| Date Understanding | 69.648 | 84.660 | 73.713 | 84.824 (+0.2)     |'
  prefs: []
  type: TYPE_TB
- en: '| Anachronisms | 82.173 | 66.087 | 80.000 | 83.674 (+1.5)     |'
  prefs: []
  type: TYPE_TB
- en: '| Sports Understanding | 80.025 | 80.996 | 81.110 | 82.900 (+1.8)     |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Main experimental results. We show improvement of PRoMPTd over the
    best baseline. We excluded Zero-Shot CoT from the Code Generation task because
    it generated natural language descriptions instead of valid Python code, leading
    to compilation errors. All the results are obtained using GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3.2 Main Experimental Results ‣ 3 Experiments ‣
    Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot
    Performance") illustrates the performance. Our observations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Rewritten prompts of PRoMPTd improves zero-shot LLM performance by a large margin.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We first compare the results of PRoMPTd with the naive zero-shot LLM. It is
    observed that PRoMPTd consistently outperforms the baseline on all datasets. In
    particular, it gains an absolute improvement of around 15-18% on MATH, Date Understanding,
    and Analytical Entailment, and 4-5% on the challenging GSM-8K and Code Generation.
    The results demonstrated the effectiveness of PRoMPTd in improving the prompts.
    In Section [3.4](#S3.SS4 "3.4 Human Evaluation of Rewritten Prompts ‣ 3 Experiments
    ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot
    Performance"), we will qualitatively show that PRoMPTd leads to strong improvements
    by making the prompts more specific, complete, and less ambiguous. This is particularly
    helpful for tasks involving complex reasoning (e.g., MATH, where GPT-4 obtained
    the worst zero-shot performance across all eight datasets).'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing prompts at the task level may not always help.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare the variant of PRoMPTd, when it is used to rewrite a prompt per
    task (i.e., “instruction-only”), with zero-shot and zero-shot CoT. First of all,
    including a customized task instruction, either manually (i.e., a hand-crafted
    “Let’s think step by step” in zero-shot CoT) or via LLM optimization (i.e., PRoMPTd
    (Instruction-only)), generally leads to better performance than the naive zero-shot
    LLM, which reveals the necessity of task-specific prompt improvement. However,
    we still observed two exceptions – on the Anachronisms dataset, zero-shot CoT
    underperforms the naive zero-shot by more than 10%, and on Known-Unknowns, PRoMPTd
    (Instruction-only) performs 30% worse than the naive zero-shot LLM. This is due
    to that task-level prompt optimization often overly simplifies the task instructions
    and cannot provide the most helpful hints that are differently needed by individual
    instances. We present all the rewritten task instructions of PRoMPTd (Instruction-only)
    in Appendix [B.1](#A2.SS1 "B.1 Task-specific Instructions Optimized By PRoMPTd
    ‣ Appendix B Additional Experimental Results ‣ Instance Needs More Care: Rewriting
    Prompts for Instances Yields Better Zero-Shot Performance").'
  prefs: []
  type: TYPE_NORMAL
- en: Rewriting prompts per instance yields stable improvements over task-level prompt
    optimization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Finally, we compare PRoMPTd, our approach which optimizes prompts for each
    instance, with the two baselines of per-task prompt optimization, i.e., zero-shot
    CoT and PRoMPTd (Instruction-only). In contrast to the unstable and limited improvement
    of the two task-level prompt optimization baselines, PRoMPTd is demonstrated to
    consistently yield the best performance on all datasets. On the complicated MATH
    and Code Generation datasets, it outperforms the baselines by around 6-9% absolute
    values. On the MATH dataset, we note a 10% absolute improvement in medium to very-hard
    categories against zero-shot baselines (detailed in Appendix [B.2](#A2.SS2 "B.2
    Results on MATH by categories ‣ Appendix B Additional Experimental Results ‣ Instance
    Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance")).
    Similarly, for the code generation task, PRoMPTd consistently removes ambiguity
    from task instructions and provides task demonstrations even if they are not provided
    in the original prompt leading to more specific and complete prompts. These results
    suggest the necessity and effectiveness of prompt optimization per instance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original Prompt: 
    def  sort_array(arr): ‘‘‘In  this  Kata,  you  have  to  sort  an  array  of  non-negative  integers  according  to  number  of  ones  in  their  binary  representation  in  ascending  order.  For  similar  number  of  ones,  sort  based  on  decimal  value.  It  must  be  implemented  like  this:
    >>>  sort_array([1,  5,  2,  3,  4])  ==  [1,  2,  3,  4,  5] >>>  sort_array([-2,  -3,  -4,  -5,  -6])  ==  [-6,  -5,  -4,  -3,  -2]
    >>>  sort_array([1,  0,  2,  3,  4])  [0,  1,  2,  3,  4]‘‘‘ |'
  prefs: []
  type: TYPE_TB
- en: '| Reason: The candidate prompt is inconsistent and contradictory. The function
    description states to sort the array based on the number of ones in the binary
    representation, but the provided examples are simply sorted in ascending order,
    which doesn’t align with the initial instructions. This discrepancy can confuse
    the model and result in incorrect outputs. The candidate prompt also does not
    provide a common output format like ”the answer is $\backslash\backslash$answer”.
    The better prompt removes this confusion by providing clear instructions and examples
    that align with those instructions. |'
  prefs: []
  type: TYPE_TB
- en: '| Rewritten Prompt: Given an array of non-negative integers, sort the array
    based on the number of ones in their binary representation in ascending order.
    If two or more numbers have the same number of ones in their binary representation,
    sort these numbers based on their decimal value in ascending order. Write a Python
    function with the following signature: 
    def  sort_array(arr:  List[int])  ->  List[int]: ‘‘‘Here  are  a  few  examples  to  illustrate  the  expected  behavior  of  your  function:
    >>>  sort_array([1,  5,  2,  3,  0])  should  return  [0,  1,  2,  3,  5] >>>  sort_array([0,  3,  2,  4])  should  return  [0,  2,  4,  3]
    >>>  sort_array([1,  0,  2,  3,  4])  should  return  [0,  1,  2,  3,  4]‘‘‘ |'
  prefs: []
  type: TYPE_TB
- en: '| Original Prompt: Identify whether a given statement contains an anachronism:
    William Shakespeare collaborated with Sappho to write epic love musicals. Your
    choices are: (1) Yes (2) No |'
  prefs: []
  type: TYPE_TB
- en: '| Reason: The candidate prompt is vague and lacks clear instructions on how
    to evaluate the statement for anachronisms. It does not provide any context or
    guidance on what an anachronism is or how to identify one. This could lead to
    an inaccurate or incomplete response. The candidate prompt also does not provide
    a common output format like ”the answer is $\backslash\backslash$answer” |'
  prefs: []
  type: TYPE_TB
- en: '| Rewritten Prompt: Your task is to evaluate the following statement for anachronisms:
    ”William Shakespeare collaborated with Sappho to write epic love musicals.” An
    anachronism is a chronological inconsistency in some arrangement, especially a
    juxtaposition of person(s), events, objects, or customs from different periods.
    In your evaluation, consider the historical timelines of the individuals mentioned,
    as well as the likelihood of the events or actions described. You should provide
    a clear and concise answer (1 for Yes and 2 for No), along with a brief explanation
    or evidence to support your conclusion. Print the output in the format ”the answer
    is $\backslash\backslash$answer” |'
  prefs: []
  type: TYPE_TB
- en: '| Original Prompt: A test of ‘hallucinations’ by asking questions whose answers
    are known to be unknown: When was Abraham Lincoln born? Your choices are: (1)
    February 12, 1809 (2) Unknown |'
  prefs: []
  type: TYPE_TB
- en: '| Reason: The candidate prompt is not clear about the task at hand. It states
    that it is a test of ’hallucinations’, but does not clearly explain what this
    means or how to conduct this test. Furthermore, it does not specify how to structure
    the response or what kind of reasoning or evidence to provide. The candidate prompt
    also does not provide a common output format like ”the answer is $\backslash\backslash$answer”
    |'
  prefs: []
  type: TYPE_TB
- en: '| Rewritten Prompt: Your task is to answer the following question: When was
    Abraham Lincoln born? Your choices are: (1) February 12, 1809 (2) Unknown. Provide
    a clear and concise answer, along with a brief explanation or evidence to support
    your answer. Ensure that your answer is contextually appropriate and maintains
    the same intent as the original question. Pay attention to providing a well-reasoned
    and evidence-based answer that is easy to understand and follow. Print the output
    in the format. ”the answer is $\backslash\backslash$answer” |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Examples of original and PRoMPTd-rewritten prompts drawn from the
    Code Generation, Anachronisms, and Known-Unknowns datasets. The three examples
    demonstrated how PRoMPTd can rewrite prompts to be less ambiguous, complete, and
    more specific, respectively. All rewritten prompts lead to better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b6b3e7ed56c589fe581b9ca7af942867.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Ablation performance of PRoMPTd with varying components.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We analyze the impact of two design choices of PRoMPTd, i.e., (1) prompting
    the LLM $\mathcal{M}_{rewrite}$ to provide a reason for the rewriting and (2)
    indicating the task type, by conducting an ablation study. For each ablation setting,
    we remove the corresponding component in the few-shot demonstrations of PRoMPTd
    and apply it to rewrite prompts. Like in the main experiments, the rewritten prompts
    are then used for zero-shot task performance. We conducted the experiments on
    Anachronisms and GSM-8K with a sample of 250 examples each. Our results in Figure [2](#S3.F2
    "Figure 2 ‣ Rewriting prompts per instance yields stable improvements over task-level
    prompt optimization. ‣ 3.2 Main Experimental Results ‣ 3 Experiments ‣ Instance
    Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance")
    show that the best performance of PRoMPTd can only be achieved when both components,
    the reason statement, and the task type, are included in the prompt rewriting
    process. In particular, dropping the reason statement leads to the worst performance,
    which implies the importance of prompting an LLM to elaborate on its thoughts,
    a phenomenon that has been consistently discovered in recent work (Wei et al.,
    [2022](#bib.bib29)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Human Evaluation of Rewritten Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We qualitatively analyze the rewritten prompts of PRoMPTd by conducting a human
    evaluation. In particular, we aim to investigate (1) if PRoMPTd can rewrite prompts
    to be specific, non-ambiguous, complete, and structured, as we discussed in Section [2.2](#S2.SS2
    "2.2 PRoMPTd: Prompt Rewriting for Instances ‣ 2 Improving Zero-Shot LLMs with
    Instance Prompt Rewriting ‣ Instance Needs More Care: Rewriting Prompts for Instances
    Yields Better Zero-Shot Performance"); and (2) if PRoMPTd introduces unwanted
    artifacts during the rewriting process, such as adding fabricated contents that
    are not stated in the original prompt and manipulates it directly. The human evaluation
    was performed by a group of three graduate students. The evaluators were asked
    to score 400 (25 from each dataset, original or rewritten) prompts following each
    dimension using the Likert scale ranging in $[1,5]$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da56d35886f6adbc1bcdcda0a4eda6a5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Human evaluation comparing the original and rewritten prompts of
    PRoMPTd in different dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S3.F3 "Figure 3 ‣ 3.4 Human Evaluation of Rewritten Prompts ‣ 3
    Experiments ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields
    Better Zero-Shot Performance") shows the results. It is observed that the rewritten
    prompts by PRoMPTd are greatly improved to be very specific (scored 5), less ambiguous,
    slightly more complete, and much more structured (scored 5). They are also shown
    to preserve the original semantic contexts with rare hallucinations (a lower score
    in “hallucinated” is better). We present examples of prompts rewritten by PRoMPTd
    in Table [2](#S3.T2 "Table 2 ‣ Rewriting prompts per instance yields stable improvements
    over task-level prompt optimization. ‣ 3.2 Main Experimental Results ‣ 3 Experiments
    ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot
    Performance"). In the first example (Code Generation), the function description
    is inconsistent with the provided test assertions in the candidate prompt. While
    the function description suggests sorting based on binary representation, the
    test cases simply sort the integers in ascending order. The good prompt removes
    the ambiguity by providing a clarified version of test cases. In the second example
    (Anachronism), the candidate prompt lacks guidance on how to evaluate the statement.
    The rewritten good prompt provides context and detailed instructions, making it
    more complete. In the third example (Known-Unknowns), the rewritten prompt is
    more specific as it clarifies the nature of the question and emphasizes the need
    for evidence-based and well-reasoned answers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the human study also reveals that for some of the rewritten instances,
    PRoMPTd omits necessary information from the candidate prompt. Hence it is unable
    to achieve a perfect score on all the examples (see Section [4](#S4 "4 Limitations
    and Discussions ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields
    Better Zero-Shot Performance") for discussions). Notably, PRoMPTd does not fabricate
    instructions or fictional facts during rewriting, avoiding hallucinations which
    is a potential issue with datasets such as Sports Understanding. Additionally,
    as discussed in Section [2](#S2 "2 Improving Zero-Shot LLMs with Instance Prompt
    Rewriting ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better
    Zero-Shot Performance"), PRoMPTd constrains prompts with an output structure that
    aids in answer extraction and hence avoids an extra API call over LLMs. As such,
    PRoMPTd receives a near-perfect score for Structredness. We present the rewritten
    prompts by PRoMPTd for each dataset in Table [8](#A3.T8 "Table 8 ‣ Appendix C
    Examples of Responses Generated by PRoMPTd ‣ Instance Needs More Care: Rewriting
    Prompts for Instances Yields Better Zero-Shot Performance")-[10](#A3.T10 "Table
    10 ‣ Appendix C Examples of Responses Generated by PRoMPTd ‣ Instance Needs More
    Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance") of
    Appendix [C](#A3 "Appendix C Examples of Responses Generated by PRoMPTd ‣ Instance
    Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Length Analysis of Rewritten Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/36f7e90ea17514557a9398b3ad65a593.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparison of prompt lengths for all the approaches. As expected,
    PRoMPTd results in detailed and longer prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conducted an analysis of token counts after applying PRoMPTd on various
    datasets to gain an insight into the performance of rewritten prompts (Figure [4](#S3.F4
    "Figure 4 ‣ 3.5 Length Analysis of Rewritten Prompts ‣ 3 Experiments ‣ Instance
    Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance")).
    Both PRoMPTd and its instruction-only variant result in longer prompts via rewriting.
    In particular, we have the following observations: (1) On Anachronisms and Date
    Understanding, PRoMPTd yields the longest rewritten prompts. By looking into the
    rewritten prompts, we found that for such logical reasoning tasks, PRoMPTd learns
    to elucidate the task definition further, such as outlining the formal definition
    for Anachronisms and introducing checks for leap years and referencing the Gregorian
    calendar for Date Understanding. (2) For mathematical reasoning tasks, i.e., MATH
    and GSM-8K, PRoMPTd offers hints regarding the problem and urges $\mathcal{M}_{\texttt{task}}$
    to provide a structured, step-by-step solution. This methodology of breaking down
    the prompt and seeking structured answers, akin to Kojima et al. ([2022](#bib.bib12)),
    has demonstrated enhanced performance. (3) For the code generation tasks, the
    token counts of PRoMPTd are similar to the original prompts’, but qualitatively,
    as shown in Table [2](#S3.T2 "Table 2 ‣ Rewriting prompts per instance yields
    stable improvements over task-level prompt optimization. ‣ 3.2 Main Experimental
    Results ‣ 3 Experiments ‣ Instance Needs More Care: Rewriting Prompts for Instances
    Yields Better Zero-Shot Performance"), it rewrites the task instruction in a more
    unambiguous way by removing any incorrect test assertions in the prompt. (4) Finally,
    we observed that PRoMPTd (Instruction-only) could rewrite per-task prompts to
    be longer than the per-instance prompts rewritten by PRoMPTd, although its longer
    prompts do not yield better zero-shot performance than PRoMPTd. Our examination
    shows that, when optimizing for each individual test instance, PRoMPTd learns
    to generate hints that are more concise and specific to the instance, which explains
    the shorter prompt but better task performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Can PRoMPTd Handle New Task Types?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Zero-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PRoMPTd |'
  prefs: []
  type: TYPE_TB
- en: '| BigBench (Sentiment Classification) | 69.100 | 73.200 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Performance of PRoMPTd on the unseen task type of sentiment classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While none of the task datasets are included in the few-shot demonstrations
    of PRoMPTd, their task types are still included in PRoMT (Section [2.3](#S2.SS3
    "2.3 PRoMT: Dataset of Demonstrations for Prompt Rewriting ‣ 2 Improving Zero-Shot
    LLMs with Instance Prompt Rewriting ‣ Instance Needs More Care: Rewriting Prompts
    for Instances Yields Better Zero-Shot Performance")). In this section, we further
    investigate if PRoMPTd can generalize to unseen task types. We study the efficacy
    of PRoMPTd on a completely unseen task type, i.e., sentiment classification from
    BigBench (Srivastava et al., [2022](#bib.bib24)). As shown in Table [3](#S3.T3
    "Table 3 ‣ 3.6 Can PRoMPTd Handle New Task Types? ‣ 3 Experiments ‣ Instance Needs
    More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance"),
    PRoMPTd gains $\sim 3\%$ absolute improvement over the zero-shot baseline. We
    observed that PRoMPTd rewrites the prompt by not only clarifying the sentiment
    class labels, but also supplying it with demonstrations, despite not being explicitly
    asked to. We include more details in Appendix [B.3](#A2.SS3 "B.3 Results for Task
    Type Generalization ‣ Appendix B Additional Experimental Results ‣ Instance Needs
    More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Use Case of PRoMPTd in Defending Adversarial Prompting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As one of the significant advantages, PRoMPTd makes the LLM problem-solving
    process more interpretable via prompt rewriting. In this section, we showcase
    the benefit of this interpretability in the application of defending adversarial
    prompting. At a high level, adversarial prompts (Maus et al., [2023](#bib.bib18))
    are carefully crafted inputs used to mislead or exploit the vulnerabilities of
    AI systems. In the context of LLMs, this has been realized in forms such as prompt
    injection, jailbreaking, etc. In Figure [5](#S3.F5 "Figure 5 ‣ 3.7 Use Case of
    PRoMPTd in Defending Adversarial Prompting ‣ 3 Experiments ‣ Instance Needs More
    Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance"), we
    show that an adversary can ask for unethical information about “car hotwiring”
    by misleading GPT-4 to consider the request as an ethical task of poem writing.
    PRoMPTd defends such adversarial behaviors by reasoning about its ethical concern
    and rewriting the original request to be innocuous and safe. It should be noted,
    however, that none of the ten demonstrations in PRoMT are about adversarial prompting,
    which further confirms the generalizability of PRoMPTd across various task types.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/68a61471b4745d87c9229baafbd13601.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Example showing the potential use of PRoMPTd for defending adversarial
    prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Limitations and Discussions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Error analysis of PRoMPTd.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While it is rare, we still observed hallucination errors made by PRoMPTd (Table [7](#A2.T7
    "Table 7 ‣ B.4 Error Analysis ‣ Appendix B Additional Experimental Results ‣ Instance
    Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance")
    in Appendix [B.4](#A2.SS4 "B.4 Error Analysis ‣ Appendix B Additional Experimental
    Results ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better
    Zero-Shot Performance")). The errors include changing the task instruction (e.g.,
    while the original prompt requires a function with signature “double_the_difference”,
    the rewritten prompt rephrases it to “sum_of_squares_of_odd_numbers”) or manipulating
    numbers in the original task description (e.g., incorrectly instructing the task
    LLM to use a “10% per month” condition in the second example). Future work should
    look into mechanisms that can better prevent hallucinations of LLMs in prompt
    rewriting.'
  prefs: []
  type: TYPE_NORMAL
- en: PRoMPTd works only when the LLM is strong enough.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We also analyze if PRoMPTd will work with relatively weaker LLM such as GPT-3.5-turbo.
    Our results are shown in Appendix [B.5](#A2.SS5 "B.5 Experiments of PRoMPTd using
    GPT-3.5 ‣ Appendix B Additional Experimental Results ‣ Instance Needs More Care:
    Rewriting Prompts for Instances Yields Better Zero-Shot Performance"). When using
    GPT-3.5 as both $\mathcal{M}_{task}$ (i.e., using GPT-3.5 to rewrite prompts for
    GPT-4), the resulting performance is worse than the naive GPT-4 zero-shot CoT,
    which suggests that using a weaker LLM cannot effectively rewrite prompts for
    a stronger one.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs in Zero-Shot.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To reduce the manual effort in devising task-specific demonstrations, recent
    works have been motivated to investigate zero-shot LLM prompting and shown its
    efficacy in reasoning (Wei et al., [2022](#bib.bib29); Kojima et al., [2022](#bib.bib12);
    Wang et al., [2022](#bib.bib27)), question-answering (Kannan et al., [2023](#bib.bib11)),
    generating goal-driven action plans (Wang et al., [2023](#bib.bib26)), natural
    language generation (Axelsson & Skantze, [2023](#bib.bib2)), etc. Zhang et al.
    ([2023](#bib.bib32)) demonstrated that LLMs such as GPT-3 (Brown et al., [2020](#bib.bib3)),
    despite being shown to perform few-shot learning remarkably well, are not very
    successful in zero-shot in-context learning. To this end, they finetuned more
    than 60 NLP datasets on natural language instructions. Despite its effectiveness,
    instruction tuning presents several challenges such as the need for crafting natural
    language instructions, and is ineffective against reasoning tasks. To improve
    the zero-shot reasoning capabilities of LLMs, Kojima et al. ([2022](#bib.bib12))
    proposed Zero-Shot Chain-of-Thought (Wei et al., [2022](#bib.bib29), CoT). They
    showed that by appending a single fixed trigger prompt “Let’s solve this step
    by step”, LLMs can be prompted with much stronger reasoning abilities. A similar
    strategy has been applied in (Reynolds & McDonell, [2021](#bib.bib21); Shwartz
    et al., [2020](#bib.bib23)) where a single prompt is used to handle arithmetic
    and question-answering tasks. However, usage of a single prompt results in subpar
    performance against complex reasoning tasks such as MATH and is not suitable for
    non-reasoning tasks such as content or code generation. Our approach PRoMPTd differs
    from instruction tuning as it does not require expensive fine-tuning and is generalizable
    to unseen task types. Unlike Zero-Shot CoT which uses a fixed hint for all instances
    and tasks, PRoMPTd employs optimized prompts for individual test instances.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Rewriting and Optimization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Providing natural language instructions in prompts is a useful new paradigm
    to improve the zero-shot task performance of LLMs. Several works have aimed to
    improve such prompts via manual rewrite (Reynolds & McDonell, [2021](#bib.bib21))
    or by gradient-based tuning (Liu et al., [2021](#bib.bib15)). Manual rewriting
    is time-consuming and requires subjective interpretations, while gradient-based
    tuning can be extremely computationally demanding for large models and may not
    be feasible for API-based models. Gao et al. ([2021](#bib.bib6)); Jiang et al.
    ([2020](#bib.bib10)); Yuan et al. ([2021](#bib.bib31)); Prasad et al. ([2022](#bib.bib19));
    Jiang et al. ([2020](#bib.bib10)) have attempted to study prompt optimization
    either in a few-shot settings or require training a model to optimize prompts.
    Moreover, Honovich et al. ([2022](#bib.bib9)) and Zhou et al. ([2022](#bib.bib33))
    proposed to automatically generate task instructions from several task demonstrations.
    The automatically induced task instructions are then executed in few-shot or zero-shot
    settings. However, these approaches all require task-specific, few-shot demonstrations
    and have shown subpar performance on complex tasks. Unlike prior works, our approach
    PRoMPTd optimizes prompts for individual test instances so as to provide instance-specific
    hints to aid the zero-shot LLM. The approach does not require task-specific annotations
    or supervision and is shown to improve complex reasoning tasks in particular.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we have proposed a new task of prompt rewriting at the instance
    level to improve the zero-shot abilities of LLMs. We show that optimizing at the
    instance level can aid in generating task-specific hints for complex tasks. We
    also conducted a series of evaluations, concluding that PRoMPTd (1) can rewrite
    original prompts into a more specific, complete, unambiguous, and structured prompt;
    (2) can generate human-interpretable reasons behind its decision to rewrite an
    original prompt; (3) can generate examples from task instructions; (4) generates
    prompts which are longer than the initial version but are rarely hallucinated;
    and (5) can aid in defending adversarial prompting. To motivate future research,
    we include all the reproducible prompts in the Appendix and will release the output
    of our approach publicly.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We do not anticipate any severe ethical issues from using the proposed approach.
    We use fully open-sourced datasets and will open-source our results and dataset
    as well. In addition, because of its explicit reasoning, PRoMPTd is shown with
    the potential to defend adversarial prompting, which indicates its unique positive
    societal impact.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The source code of PRoMPTd to reproduce all of the experiments as well as our
    dataset PRoMT will be made publicly available. The process to generate PRoMT has
    been described in Section [2.3](#S2.SS3 "2.3 PRoMT: Dataset of Demonstrations
    for Prompt Rewriting ‣ 2 Improving Zero-Shot LLMs with Instance Prompt Rewriting
    ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better Zero-Shot
    Performance"), and the exact prompt for PRoMPTd along with other details to run
    our experiment have been included in Appendix [A](#A1 "Appendix A Implementation
    Details ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better
    Zero-Shot Performance") to encourage reproducibility.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agossah et al. (2023) Alexandre Agossah, Frédérique Krupa, Matthieu Perreira
    Da Silva, and Patrick Le Callet. Llm-based interaction for content generation:
    A case study on the perception of employees in an it department. In *Proceedings
    of the 2023 ACM International Conference on Interactive Media Experiences*, pp. 
    237–241, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Axelsson & Skantze (2023) Agnes Axelsson and Gabriel Skantze. Using large language
    models for zero-shot natural language generation from knowledge graphs. *arXiv
    preprint arXiv:2307.07312*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2021) Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained
    language models better few-shot learners. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pp.  3816–3830,
    Online, August 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.295.
    URL [https://aclanthology.org/2021.acl-long.295](https://aclanthology.org/2021.acl-long.295).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilardi et al. (2023) Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt
    outperforms crowd-workers for text-annotation tasks. *arXiv preprint arXiv:2303.15056*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the math dataset. *arXiv preprint arXiv:2103.03874*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Honovich et al. (2022) Or Honovich, Uri Shaham, Samuel R Bowman, and Omer Levy.
    Instruction induction: From few examples to natural language task descriptions.
    *arXiv preprint arXiv:2205.10782*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2020) Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig.
    How can we know what language models know? *Transactions of the Association for
    Computational Linguistics*, 8:423–438, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kannan et al. (2023) Shyam Sundar Kannan, Vishnunandan LN Venkatesh, and Byung-Cheol
    Min. Smart-llm: Smart multi-agent robot task planning using large language models.
    *arXiv preprint arXiv:2309.10062*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in neural information processing systems*, 35:22199–22213, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards,
    Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.
    Let’s verify step by step. *arXiv preprint arXiv:2305.20050*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021) X Liu, Y Zheng, Z Du, M Ding, Y Qian, Z Yang, and J Tang.
    Gpt understands, too. arxiv. *arXiv preprint arXiv:2103.10385*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023b) Yiheng Liu, Tianle Han, Siyuan Ma, Jiayue Zhang, Yuanyuan
    Yang, Jiaming Tian, Hao He, Antong Li, Mengshen He, Zhengliang Liu, et al. Summary
    of chatgpt/gpt-4 research and perspective towards the future of large language
    models. *arXiv preprint arXiv:2304.01852*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2021) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming
    few-shot prompt order sensitivity. *arXiv preprint arXiv:2104.08786*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maus et al. (2023) Natalie Maus, Patrick Chao, Eric Wong, and Jacob Gardner.
    Adversarial prompting for black box foundation models. *arXiv preprint arXiv:2302.04237*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prasad et al. (2022) Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal.
    Grips: Gradient-free, edit-based instruction search for prompting large language
    models. *arXiv preprint arXiv:2203.07281*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pryzant et al. (2023) Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chenguang
    Zhu, and Michael Zeng. Automatic prompt optimization with” gradient descent” and
    beam search. *arXiv preprint arXiv:2305.03495*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reynolds & McDonell (2021) Laria Reynolds and Kyle McDonell. Prompt programming
    for large language models: Beyond the few-shot paradigm. In *Extended Abstracts
    of the 2021 CHI Conference on Human Factors in Computing Systems*, pp.  1–7, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. *arXiv preprint arXiv:2010.15980*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shwartz et al. (2020) Vered Shwartz, Peter West, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Unsupervised commonsense question answering with self-talk. *arXiv
    preprint arXiv:2004.05483*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. Beyond the imitation game: Quantifying and
    extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tamkin et al. (2023) Alex Tamkin, Kunal Handa, Avash Shrestha, and Noah Goodman.
    Task ambiguity in humans and language models. In *The Eleventh International Conference
    on Learning Representations*, 2023. URL [https://openreview.net/forum?id=QrnDe_9ZFd8](https://openreview.net/forum?id=QrnDe_9ZFd8).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models. *arXiv preprint arXiv:2305.04091*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Webson & Pavlick (2021) Albert Webson and Ellie Pavlick. Do prompt-based models
    really understand the meaning of their prompts? *arXiv preprint arXiv:2109.01247*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2023) Tongshuang Wu, Haiyi Zhu, Maya Albayrak, Alexis Axon, Amanda
    Bertsch, Wenxing Deng, Ziqi Ding, Bill Guo, Sireesh Gururaja, Tzu-Sheng Kuo, et al.
    Llms as workers in human-computational algorithms? replicating crowdsourcing pipelines
    with llms. *arXiv preprint arXiv:2307.10168*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yuan et al. (2021) Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore:
    Evaluating generated text as text generation. *Advances in Neural Information
    Processing Systems*, 34:27263–27277, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei
    Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, et al. Instruction
    tuning for large language models: A survey. *arXiv preprint arXiv:2308.10792*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level
    prompt engineers. *arXiv preprint arXiv:2211.01910*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Task Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate approaches on each dataset following their standard metrics. For
    accuracy, when the answer is a string or date, we either use exact matching (for
    Date Understanding ) or the accuracy measure (for reasoning tasks). When the answer
    is a number, we use the numerical representation to compare the answers. For the
    HumanEval (Chen et al., [2021](#bib.bib4)) dataset, we evaluate our approach using
    the Pass@1 metric (Chen et al., [2021](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Full Prompt Used For PRoMPTd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we present all the ten prompts used for all the experiments.
    We start with the task instruction and then present all ten demonstrations each
    with all four quadruples $(\rho_{i},r_{i},t_{i},\rho^{*}_{i})$. The test instance
    is appended at the end after the last “###Candidate Prompt###”.
  prefs: []
  type: TYPE_NORMAL
- en: '
    Your  task  is  to  rewrite  a  given  candidate  prompt  into  a  better  prompt  copying  all  the  provided  values  in  the  rewritten  better  prompt.  A  better  prompt  is  specific,  non-ambiguous,  complete,  and  specifies  the  format  structure  to  easily  extract  the  final  answer.  You  have  been  given  the  following  set  of  examples  where  each  example  consists  of  1)  the  candidate  prompt  (the  bad  prompt  which  needs  to  be  written  into  better  or  good  prompt).  2)  the  Reason  highlighting  the  shortcomings  of  the  candidate  prompt.  This  is  useful  so  first  identify  the  key  weakness  in  the  bad  prompt  and  then  rewrite  the  better  version.  3)  Better  Prompt  Type:  Helps  in  identifying  the  type  of  better  prompt.  This  can  help  reduce  the  search  space  for  each  prompt  type.  Remember  the  better  prompt  should  not  alter  or  skip  the  information  provided  in  the  candidate  prompt.  Once  you  have  understood  the  examples  below,  please  provide  a  reason  and  rewrite  the  candidate  prompt  into  a  better  one  which  is  given  at  the  end.  Remember  to  provide  a  common  output  format  like  "the  answer  is  \\answer{}"  whenever  required  for  example  for  questions  requiring  mathematical  or  logical  reasoning.
    ###Candidate  Prompt### Given  a  pandas  DataFrame  [INPUT],  write  a  Python  function  that  trains  a  classifier  from  the  sklearn  library.  The  DataFrame  has  a  ’target’  column  and  other  feature  columns.  The  function  signature  should  be:  ‘def  train_classifier(df:  pd.DataFrame)  ->  Classifier:‘
    ###Reason### The  candidate  prompt  is  vague  about  the  type  of  classifier  to  be  used  and  does  not  provide  any  parameters  for  splitting  the  data  or  the  classifier.  This  ambiguity  might  lead  the  GPT  model  to  guess  the  user’s  intentions,  which  may  not  align  with  their  actual  needs.  On  the  other  hand,  the  better  prompt  gives  specific,  clear,  and  complete  instructions.  It  specifies  the  classifier  type,  split  ratio,  random  states,  and  parameters  for  the  classifier,  eliminating  any  room  for  guesswork  and  ensuring  the  GPT-based  model  generates  the  desired  output.
    ###Better  Prompt  Type### [CODE  OUTPUT][CONSTRAINTED  OUTPUT] ###Better  Prompt###
    Given  a  pandas  DataFrame,  write  a  Python  function  that  trains  a  Random  Forest  Classifier  from  the  sklearn  library.  The  DataFrame  will  have  a  ’target’  column  for  the  labels,  and  the  remaining  columns  are  features.  Split  the  data  into  training  and  testing  sets  with  a  ratio  of  80:20  using  the  ‘train_test_split‘  function  from  sklearn  with  a  random  state  of  42.  Use  the  RandomForestClassifier  with  ‘n_estimators‘  set  to  100,  ‘max_depth‘  set  to  2,  ‘random_state‘  set  to  0.  The  function  signature  should  be:  ‘def  train_rf_classifier(df:  pd.DataFrame)  ->  RandomForestClassifier:‘
    ###Candidate  Prompt### Sammy  is  a  lawyer  who’s  working  on  two  lawsuits.  The  first  lawsuit  has  a  3%  chance  of  paying  out  3,000,000  USD  upon  a  win  and  $0  if  he  loses  it.  The  second  lawsuit  has  a  50%  chance  of  paying  out  $1,000,000  if  Sammy  loses  and  a  50%  chance  of  paying  out  1,000,000  USD  if  he  wins.  Expressed  as  a  percentage,  how  much  more  likely  is  it  that  Sammy  loses  both  lawsuits  compared  to  winning  both  of  them?
    ###Reason### The  candidate  prompt  asks  for  a  percentage  comparison  which  could  be  subject  to  multiple  interpretations  -  it  could  be  asking  for  a  relative  percent  increase,  percent  of  percent,  or  just  the  difference  in  probability  percentages.  Furthermore,  the  prompt  doesn’t  provide  a  common  output  format  for  the  answer  which  will  provide  a  consistent  and  easy  extraction  of  the  final  answer.  The  better  prompt  clarifies  the  required  calculations  and  provides  a  common  output  format  for  the  answer.  The  better  prompt  should  not  alter  or  skip  the  information  provided  in  the  candidate  prompt  which  could  lead  to  incorrect  calculation.
    ###Better  Prompt  Type### [MATHEMATICAL  REASONING] ###Better  Prompt### Sammy  is  a  lawyer  who’s  working  on  two  lawsuits.  The  first  lawsuit  has  a  3%  chance  of  paying  out  3,000,000  USD  upon  a  win  and  $0  if  he  loses  it.  The  second  lawsuit  has  a  50%  chance  of  paying  out  $1,000,000  if  Sammy  loses  and  a  50%  chance  of  paying  out  1,000,000  USD  if  he  wins.  Calculate  the  difference  in  the  probabilities  of  Sammy  losing  both  lawsuits  and  winning  both  lawsuits,  expressed  as  a  percentage.  Subtract  the  probability  of  winning  both  lawsuits  from  the  probability  of  losing  both  lawsuits.  Print  the  output  in  the  format  "The  answer  is  \\answer{}".
    ###Candidate  Prompt### Jammie  decides  to  open  up  a  cycle  shop.  The  most  common  repair  he  does  is  fixing  cycle  tires.  He  charges  USD  20  for  this  and  it  cost  him  $5  in  parts.  In  one  month  Jammie  does  600  of  these  repairs.  He  also  does  2  more  complex  repairs  for  $600  each  and  cost  $50  in  parts.  He  also  sells  $2000  profit  worth  of  things  from  his  retail  shop.  If  rent  and  other  fixed  expense  for  the  shop  is  $4000  a  month  how  much  profit  did  the  shop  make?
    ###Reason### The  candidate  prompt,  in  this  case,  involves  the  model  attempting  to  solve  the  entire  problem  at  once.  While  this  may  work  for  simpler  tasks,  in  complex  scenarios  it  might  lead  to  less  accurate  or  nonsensical  outputs.  Breaking  the  problem  down  would  make  the  reasoning  more  explicit  and  easier  for  the  model  to  handle.  The  ’Chain  of  Thought’  approach  is  better  in  this  context  because  it  makes  the  computation  more  manageable  by  breaking  it  down  into  simpler  steps.  This  approach  can  be  particularly  useful  for  complex  mathematical  problems  as  it  allows  the  model  to  focus  on  one  part  of  the  problem  at  a  time.  It  also  provides  a  clear  and  organized  structure,  making  it  easier  for  the  user  to  follow  the  model’s  reasoning  process.  The  candidate  prompt  also  does  not  provide  a  common  output  format  like  "the  answer  is  \\answer{}".  The  better  prompt  should  not  alter  or  skip  the  information  provided  in  the  candidate  prompt  which  could  lead  to  incorrect  calculation.
    ###Better  Prompt  Type### [MATHEMATICAL  REASONING] ###Better  Prompt### Solve  the  following  problem  step  by  step
    Step  1:  Calculate  the  profit  earned  by  Jammie  from  fixing  cycle  tires  given  that  he  charges  USD  20  and  it  cost  him  $5  in  parts  after  doing  600  of  these  repairs.  Step  2:  Compute  the  profit  from  the  two  more  complex  repairs,  which  cost  $50  in  parts  and  are  charged  at  $600  each.  Step  3:  Sum  up  the  profit  from  the  fixed  cycle  tires,  the  complex  repairs,  and  the  $2000  profit  from  the  retail  shop.  Step  4:  Subtract  the  rent  and  other  fixed  expenses  which  amounts  to  $4000  a  month,  from  the  total  profit  to  find  out  how  much  profit  Jammie’s  shop  made.  Print  the  output  in  the  format  "The  answer  is  \\answer{}"
    ###Candidate  Prompt### A  car  travels  at  a  speed  of  60  miles  per  hour.  How  far  will  it  travel  in  2.5  hours?
    ###Reason### By  using  chain  of  thoughts  prompting,  we  can  guide  the  model  to  break  down  the  word  problem  into  logical  steps  and  provide  a  step-by-step  solution.  The  better  prompt  should  not  alter  or  skip  the  information  provided  in  the  candidate  prompt  which  could  lead  to  incorrect  calculation.
    ###Better  Prompt  Type### [MATHEMATICAL  REASONING] ###Better  Prompt### Let’s  solve  the  following  word  problem  step  by  step:  A  car  travels  at  a  speed  of  60  miles  per  hour.  How  far  will  it  travel  in  2.5  hours?  Start  by  multiplying  the  speed  (60  mph)  by  the  time  (2.5  hours)  to  find  the  total  distance.  Then,  apply  the  formula  Distance  =  Speed  X  Time  to  calculate  the  answer.  Print  the  output  in  the  format  "The  answer  is  \\answer{}".
    ###Candidate  Prompt### You  have  to  generate  a  question  for  the  given  text.  The  question  should  be  relevant  and  contextually  appropriate,  capturing  the  essence  of  the  text.
    ###Reason### The  candidate  prompt  is  relatively  short  and  lacks  specific  instructions  and  requirements  for  generating  a  question  for  the  text.  Although  it  mentions  the  need  for  a  relevant  and  contextually  appropriate  question  that  captures  the  essence  of  the  text,  it  does  not  provide  clear  guidelines  on  how  to  determine  the  essence,  which  question  generation  model  to  use,  or  how  to  handle  complex  or  technical  text.  The  prompt  is  vague  and  leaves  many  crucial  details  to  interpretation,  making  it  difficult  for  the  model  to  generate  an  accurate  question.
    ###Better  Prompt  Type### [CONTENT_GENERATION][ANALYSIS] ###Better  Prompt###
    Your  task  is  to  generate  a  relevant  and  contextually  appropriate  question  for  the  given  text  that  captures  the  essence  of  the  text.  Use  a  well-defined  question  generation  model  or  library  to  generate  the  question.  Pay  attention  to  the  nuances  of  the  text,  such  as  the  main  ideas,  arguments,  or  themes,  that  may  affect  the  question  generation.  Ensure  that  the  question  is  contextually  appropriate  and  maintains  the  same  intent  as  the  original  text.  Provide  a  clear  and  concise  question  that  accurately  reflects  the  essence  of  the  text.  Aim  for  a  high  level  of  accuracy  and  coherence  in  your  question  generation.  If  necessary,  provide  a  brief  explanation  or  evidence  to  support  your  question.
    ###Candidate  Prompt### The  concert  was  scheduled  to  be  on  06/01/1943,  but  was  delayed  by  one  day  to  today.  What  is  the  date  10  days  ago  in  MM/DD/YYYY?
    ###Reason### The  candidate  prompt  is  relatively  short  and  lacks  specific  instructions  and  requirements  for  solving  the  problem.  Although  it  mentions  the  need  to  find  the  date  10  days  ago  in  MM/DD/YYYY  format,  it  does  not  provide  clear  guidelines  on  how  to  calculate  the  date,  which  calendar  system  to  use,  or  how  to  handle  leap  years.  The  prompt  is  vague  and  leaves  many  crucial  details  to  interpretation,  making  it  difficult  for  the  model  to  generate  an  accurate  answer.  The  candidate  prompt  also  does  not  provide  a  common  output  format  like  "the  answer  is  \\answer{}".
    ###Better  Prompt  Type### [MATHEMATICAL  REASONING][DATE  UNDERSTANDING] ###Better  Prompt###
    Your  task  is  to  calculate  the  date  10  days  ago  in  MM/DD/YYYY  format,  given  that  the  concert  was  scheduled  to  be  on  06/01/1943  but  was  delayed  by  one  day  to  today.  Use  the  Gregorian  calendar  system  to  calculate  the  date,  taking  into  account  leap  years  and  the  number  of  days  in  each  month.  Pay  attention  to  the  nuances  of  the  problem,  such  as  the  initial  date  and  the  number  of  days  to  subtract,  that  may  affect  the  calculation.  Ensure  that  the  date  is  contextually  appropriate  and  maintains  the  same  intent  as  the  original  problem.  Provide  a  clear  and  concise  date  that  accurately  reflects  the  solution  to  the  problem.  Aim  for  a  high  level  of  accuracy  and  consistency  in  your  calculations.  If  necessary,  provide  a  brief  explanation  or  evidence  to  support  your  answer.  Print  the  output  in  the  format  "The  answer  is  \\answer{}"
    ###Candidate  Prompt### Is  the  following  sentence  plausible?  "Joao  Moutinho  caught  the  screen  pass  in  the  NFC  championship."
    ###Reason### The  candidate  prompt  is  relatively  short  and  lacks  specific  instructions  and  requirements  for  evaluating  the  plausibility  of  the  sentence.  Although  it  provides  an  example  and  a  correct  answer,  it  does  not  provide  clear  guidelines  on  how  to  reason  through  the  sentence  or  how  to  handle  ambiguous  or  complex  sentences.  The  prompt  is  vague  and  leaves  many  crucial  details  to  interpretation,  making  it  difficult  for  the  model  to  generate  an  accurate  answer.  The  candidate  prompt  also  does  not  provide  a  common  output  format  like  "the  answer  is  \\answer{}".
    ###Better  Prompt  Type### [CONSTRAINED  OUTPUT][ANALYSIS] ###Better  Prompt###
    Your  task  is  to  determine  the  plausibility  of  the  following  sentence:  "Joao  Moutinho  caught  the  screen  pass  in  the  NFC  championship."  Provide  a  clear  and  concise  answer,  along  with  a  brief  explanation  or  evidence  to  support  your  answer.  Consider  the  context  of  the  sentence,  such  as  the  teams,  players,  and  events  mentioned,  as  well  as  the  rules  and  conventions  of  the  sport.  Ensure  that  your  answer  is  contextually  appropriate  and  maintains  the  same  intent  as  the  original  sentence.  Pay  attention  to  providing  a  well-reasoned  and  evidence-based  answer  that  is  easy  to  understand  and  follow.  Print  the  output  in  the  format  "The  answer  is  \\answer{}"
    ###Candidate  Prompt### Paraphrase  the  given  sentences.  Make  sure  the  paraphrased  versions  have  the  same  meaning  as  the  original  sentences.
    ###Reason### The  candidate  prompt  is  relatively  short  and  lacks  specific  instructions  and  requirements  for  paraphrasing.  It  only  mentions  the  need  for  paraphrasing  and  maintaining  the  same  meaning  as  the  original  sentences.  However,  it  does  not  provide  clear  guidelines  on  how  to  ensure  variations  in  wording,  sentence  structure,  or  phrasing.  The  prompt  is  vague  and  leaves  many  crucial  details  to  interpretation,  making  it  difficult  for  the  model  to  generate  accurate  and  diverse  paraphrases.
    ###Better  Prompt  Type### [PARAPHRASE][ROLE_PLAYING] ###Better  Prompt### Your  task  is  to  paraphrase  the  given  sentences  while  maintaining  the  original  meaning.  The  paraphrased  versions  should  exhibit  variations  in  wording,  sentence  structure,  and  phrasing.  Aim  for  paraphrases  that  are  linguistically  diverse,  capturing  different  ways  of  expressing  the  same  ideas.  Ensure  that  the  paraphrases  are  contextually  appropriate  and  maintain  the  same  intent  as  the  original  sentences.  Pay  attention  to  preserving  the  nuances,  tone,  and  style  of  the  original  sentences  during  the  paraphrasing  process.
    ###Candidate  Prompt### Apply  a  function  to  the  final  input  list  to  generate  the  output  list.  Use  any  preceding  inputs  and  outputs  as  examples.
    input  :  [1,  7,  9,  4,  6,  2,  0] output  :  [0] input  :  [8,  3,  4,  0,  5,  1,  6,  9,  2]
    output  :  [6] input  :  [8,  4,  5,  9,  0,  6,  1] output  :  [1] ###Reason###
    The  candidate  prompt  says  nothing  about  the  task  at  hand  which  makes  it  ambiguous.  Since  there  could  be  many  interpretations  from  a  few  input-output  pairs  the  prompt  will  lead  the  model  to  guess  the  task  at  hand  and  we  may  get  different  outputs.  To  tackle  this  problem,  the  better  prompt  should  specify  the  task  at  hand  by  clearly  specifying  the  instructions.  Specifying  the  objective  function  will  make  the  better  prompt  unambiguous  and  the  model  would  not  guess.  Also,  remember  we  need  to  consider  all  the  edge  cases  (such  as  zero  or  one  element)  before  designing  a  better  prompt.
    ###Better  Prompt  Type### [PATTERN  IDENTIFICATION][ANALYSIS] ###Better  Prompt###
    From  the  given  list  as  Input,  remove  all  the  elements  from  the  input  list  except  the  7th  element.  Print  the  output  in  the  format  "The  answer  is  \\answer{}".
    input  :  [1,  7,  9,  4,  6,  2,  0] output  :  [0] input  :  [8,  3,  4,  0,  5,  1,  6,  9,  2]
    output  :  [6] input  :  [8,  4,  5,  9,  0,  6,  1] output  :  [1] input  :  [3,  1,  7,  2,  9,  6,  8,  4,  5,  0]
    output  :  [8] ###Candidate  Prompt### Input:  Please  call  once  you  get  there.
    Output:  Please  call  upon  your  arrival. Input:  Thanks  for  your  help. Output:  Your  assistance  is  much  appreciated.
    Input:  This  song  is  fire. Output:  This  song  is  incredibly  good. ###Reason###
    The  candidate  prompt  is  vague,  ambiguous,  and  incomplete.  Merely  giving  input  and  output  pairs  makes  the  interpretation  of  the  task  ambiguous.  There  could  be  several  transformation  functions  that  could  be  applied  to  obtain  the  same  input  and  output  pairs.  The  better  prompt  should  analyze  the  examples,  identify  the  transformation  function,  and  make  the  task  clear  while  generating  the  better  prompt  type.  Specifying  the  objective  function  will  make  the  better  prompt  unambiguous  and  the  model  would  not  guess.  Also,  remember  we  need  to  consider  all  the  edge  cases  (such  as  zero  or  one  element)  before  designing  a  better  prompt.
    ###Better  Prompt  Type### [PATTERN  IDENTIFICATION][ANALYSIS] ###Better  Prompt###
    Write  a  paraphrase  of  the  input  sentence,  but  use  a  formal  style.  Print  the  output  in  the  format  "The  answer  is  \\answer{}"'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Task-specific Instructions Optimized By PRoMPTd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [4](#A2.T4 "Table 4 ‣ B.1 Task-specific Instructions Optimized By PRoMPTd
    ‣ Appendix B Additional Experimental Results ‣ Instance Needs More Care: Rewriting
    Prompts for Instances Yields Better Zero-Shot Performance") demonstrates all the
    optimized task instructions obtained using PRoMPTd. For reproducibility purposes,
    we present both the original and the rewritten task instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task Type | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K and MATH | Original Task Instruction: Solve the following word problem
    step by step. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Task Instruction: Consider solving a word problem step by step.
    Begin by reading the problem thoroughly to ensure a complete understanding. Identify
    all the variables and the relationships between them. Next, break the problem
    into smaller manageable parts and develop a strategy to solve each part. Lastly,
    review the solutions of all parts and check whether the final solution makes sense
    in the context of the original problem. |'
  prefs: []
  type: TYPE_TB
- en: '| Code Gen | Given an incomplete python template, generate the full code examining
    all the descriptions or test cases provided inside the template. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Task Instruction: You need to create a Python function based
    on the information provided in the enclosed comments. Ensure that the function
    adheres to the function signature and description provided. Include any necessary
    module imports, maintain the intent of the code, and consider possible edge cases
    to result in robust code execution. Please follow the specific instructions provided
    in the comments to develop the complete Python code. |'
  prefs: []
  type: TYPE_TB
- en: '| Anachronisms: | Original Task Instruction: Identify whether a given statement
    contains an anachronism. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Task Instruction: Review the given statement and identify whether
    it includes an anachronism - a chronological inconsistency in which a person,
    event, object, or language is mistakenly placed in a time period where it does
    not belong. Clearly state “Yes” if the statement contains an anachronism and “No”
    if it does not. Explain your reasoning briefly, providing details that led you
    to your conclusion. Print the output in the format “The answer is $\backslash\backslash$response”
    otherwise print “No anachronism found”. |'
  prefs: []
  type: TYPE_TB
- en: '| Date Understanding | Original Task Instruction: Infer the date from context
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Task Instruction: Given the context of the text, infer the date
    it is referring to. Consider any key phrases, words or events that could hint
    towards a specific date. If necessary, conduct a brief background check of any
    prominent figures or events that mention in the text. Try to narrow down the exact
    date in MM/DD/YYYY format. Print the output in the format “The answer is $\backslash\backslash$answer”.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Known Unknowns | Original Task Instruction: A test of ‘hallucinations’ by
    asking questions whose answers are known to be unknown |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Task Instruction: You will be provided with a series of questions
    that test for ’hallucinations,’ where the correct response is either a specific
    answer or unknown. Your job is to look at the question and determine if the answer
    can be found from the question or if it is undeterminable. If the answer cannot
    be concluded from the question alone, choose “(b) unknown.” If the answer can
    be determined from the question alone, provide the answer in the form “(a) Your
    Answer.” Print the output in the format “The answer is $\backslash\backslash$answer”
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sports Understanding | Original Task Instruction Determine whether the following
    statement or statements are plausible or implausible: |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Task Instruction: Your task is to evaluate the plausibility
    of the following statement(s): [input statement]. Consider the context, factual
    accuracy, logic, and consistency of the statement(s) in your evaluation. Then,
    provide a clear and substantiated determination of whether each statement is plausible
    (could likely be true) or implausible (unlikely to be true). Print the output
    in the format “The statement is $\backslash\backslash$answer with “plausible”
    or “implausible”). |'
  prefs: []
  type: TYPE_TB
- en: '| Analytical Entailment | Original Task Instruction: Identify whether one sentence
    entails the next |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Task Instruction: Your task is to analyze two sentences and
    determine if one logically entails the other. If the first sentence implies or
    logically leads to the truth of the second, state “Entailment”. If it is unclear
    or ambiguous, state “Uncertain”. Print the output in the format “The answer is
    $\backslash\backslash$answer” |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Rewritten Optimized task instructions by PRoMPTd for each of the tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Results on MATH by categories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '|  | Category - I (Very Easy and Easy) | Category - II (Medium, Hard, and Very
    Hard) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-Shot | 76.429 | 30.476 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-Shot-CoT | 80.714 | 40.476 |'
  prefs: []
  type: TYPE_TB
- en: '| PRoMPTd (Instruction-Only) | 76.429 | 44.762 |'
  prefs: []
  type: TYPE_TB
- en: '| PRoMPTd | 89.286 | 50.476 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Breakdown of performance of all the approaches across different difficulty
    categories of MATH dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#A2.T5 "Table 5 ‣ B.2 Results on MATH by categories ‣ Appendix B
    Additional Experimental Results ‣ Instance Needs More Care: Rewriting Prompts
    for Instances Yields Better Zero-Shot Performance") presents the breakdown of
    the numbers on the MATH dataset across task difficulties. for the medium to very
    hard categories in the MATH dataset PRoMPTd gains over 6-20% accuracy scores when
    compared with Zero-Shot baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Results for Task Type Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Bad Prompt | Classify the following sentences for sentiment. Examples are
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reason | The candidate prompt is vague and incomplete. It fails to clearly
    specify the task to be performed on the given sentences and does not provide any
    specific sentiment categories to assign to the sentences. Additionally, no examples
    are provided to guide the model in understanding the task. The better prompt should
    clearly specify the categories for sentiment analysis (e.g., positive, neutral,
    negative) and provide examples to illustrate the task. The prompt should also
    be structured in such a way as to instruct the model to utilize reasoning and
    understanding of language semantics to correctly classify the sentences’ sentiments.
    Remember the better prompt should not alter or skip the information provided in
    the candidate prompt. |'
  prefs: []
  type: TYPE_TB
- en: '| Task Type | [CLASSIFICATION][SENTIMENT ANALYSIS] |'
  prefs: []
  type: TYPE_TB
- en: '| Better Prompt | You have to classify the following sentences as either “Positive”,
    “Neutral”, or “Negative” based on sentiment analysis. Draw on your understanding
    of language semantics and reasoning skills to accurately assess the sentiment
    of each sentence. Print the output in the format “The sentence {input sentence}
    has a {positive/neutral/negative} sentiment”. For example: Input: This movie was
    fantastic. Output: The sentence This movie was fantastic has a Positive sentiment.
    Input: I can’t stand this weather. Output: The sentence I can’t stand this weather
    has a Negative sentiment |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Example prompt improvement by PRoMPTd for an unseen sentiment classification
    task. The original prompt lacks key details like sentiment categories and examples.
    PRoMPTd enhances the prompt by specifying positive, neutral, and negative sentiment
    classes, providing illustrative examples, and structuring the prompt to guide
    the model’s reasoning and output format. This results in a more complete, specific
    prompt that enables the successful completion of sentiment analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the experiment with PRoMPTd in Section [3.6](#S3.SS6 "3.6 Can
    PRoMPTd Handle New Task Types? ‣ 3 Experiments ‣ Instance Needs More Care: Rewriting
    Prompts for Instances Yields Better Zero-Shot Performance"), in Table [6](#A2.T6
    "Table 6 ‣ B.3 Results for Task Type Generalization ‣ Appendix B Additional Experimental
    Results ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better
    Zero-Shot Performance"), we present an example using PRoMPTd (Instruction-only)
    to rewrite the instruction for an unseen task type of sentiment classification.
    For the prompt “Classify the following sentences for sentiment. Examples are”,
    it correctly identifies the task type as a “Sentiment Analysis” and “Classification”
    task and generates a prompt that can be used for downstream sentiment classification
    tasks. Moreover, PRoMPTd can generate a possible set of classes (such as positive,
    neutral, or negative for sentiment classification task) and generate a set of
    examples that can be further used for in-context learning using few-shot sentiment
    classification demonstrations.'
  prefs: []
  type: TYPE_NORMAL
- en: B.4 Error Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Error Type | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Hallucinated instruction | Original Prompt: Given a list of numbers, return
    the sum of squares of the numbers[…]def double_the_difference(...) Rewritten Prompt:
    […]def sum_of_squares_of_odd_numbers (...) |'
  prefs: []
  type: TYPE_TB
- en: '| Manipulated information | Original Prompt: Tom buys a bedroom set for $3000\.
    He sells his old bedroom for $1000 and uses that to pay for part of the bedroom
    set.[…] Rewritten Prompt: […]Step 2: Compute the monthly payment Tom has to make
    for the bedroom set, considering that he has to pay 10% per month of the remaining
    amount[…] |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Error categories of PRoMPTd’s rewritten prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [7](#A2.T7 "Table 7 ‣ B.4 Error Analysis ‣ Appendix B Additional Experimental
    Results ‣ Instance Needs More Care: Rewriting Prompts for Instances Yields Better
    Zero-Shot Performance"), we summarize two common error categories of PRoMPTd.
    While such hallucination errors are rare, PRoMPTd is still found to make them
    during prompt rewriting.'
  prefs: []
  type: TYPE_NORMAL
- en: B.5 Experiments of PRoMPTd using GPT-3.5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f51ac8ad4547dd3cfc5411b255eaf18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Results of PRoMPTd using GPT-3.5-turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Figure [6](#A2.F6 "Figure 6 ‣ B.5 Experiments of PRoMPTd using GPT-3.5 ‣
    Appendix B Additional Experimental Results ‣ Instance Needs More Care: Rewriting
    Prompts for Instances Yields Better Zero-Shot Performance"), we present results
    of variants of PRoMPTd based on GPT-3.5-turbo, including: (1) zero-shot GPT-3.5;
    (2) zero-shot CoT of GPT-3.5; (3) PRoMPTd when using GPT-3.5 as both the task
    and the rewriting LLMs; (4) PRoMPTd when using GPT-3.5 to rewrite prompts while
    GPT-4 as the zero-shot task LLM; (4) PRoMPTd when using GPT-4 as both the task
    and the rewriting LLMs. Experiments were conducted on Anachronisms and GSM-8K.
    The results showed the difficulty of using GPT-3.5 to rewrite prompts. In particular,
    we found that the prompts rewritten by a relatively weaker LLM (GPT-3.5) would
    not assist the relatively stronger LLM (GPT-4) in task.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Examples of Responses Generated by PRoMPTd
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We present outputs of PRoMPTd on all eight datasets in Table [8](#A3.T8 "Table
    8 ‣ Appendix C Examples of Responses Generated by PRoMPTd ‣ Instance Needs More
    Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance")-[10](#A3.T10
    "Table 10 ‣ Appendix C Examples of Responses Generated by PRoMPTd ‣ Instance Needs
    More Care: Rewriting Prompts for Instances Yields Better Zero-Shot Performance").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task Type | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mathematical Reasoning | Original Prompt: Josh decides to try flipping a
    house. He buys a house for $80,000 and then puts in $50,000 in repairs. This increased
    the value of the house by 150%. How much profit did he make? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: The candidate prompt is unclear as ‘increased the value of the
    house by 150%’ could be interpreted in two ways: 1) the value of the house after
    the repair is 150% of the buying price, or 2) the value of the house after the
    repair is the buying price plus an additional 150% of the buying price. The prompt
    also does not specify whether the profit should include the cost of repairs or
    not, and does not provide a common output format for the answer. The better prompt
    clarifies these ambiguities by explicitly explaining the meaning of ‘increased
    the value by 150%’ and specifying that the profit should be calculated after taking
    into account the cost of repairs. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Prompt: Josh decides to flip a house. He bought a house for
    $80,000 then invested an additional $50,000 for its repairs. After repairs, the
    house’s value increased such that it’s now worth 150% more than the price at which
    he bought it. Calculate the profit he made after selling the house, considering
    both the buying price and cost of repairs. Print the output in the format “The
    answer is $\backslash\backslash$answer” |'
  prefs: []
  type: TYPE_TB
- en: '|  | Original Prompt: How many distinct arrangements are there of PAPA? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: The candidate prompt leaves room for interpretation, as it doesn’t
    ask for a specific output format or example to guide the reasoning process of
    the model. An improved prompt would provide clear instructions about the strategy,
    such as completing the task by using the formula for permutations of multi-set
    words, and would guide the desired output format to improve answer extraction.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Prompt: Compute the number of distinct arrangements that can
    be made from the word ’PAPA’. To solve this problem, use the formula to count
    the permutations of a multiset: n!/(n1!*n2!*…*nk!), where n is the total number
    of letters and n1, n2,… nk are the count of each distinct letter. In this case,
    ’P’ appears twice and ’A’ appears twice in ’PAPA’. Print the output in the format
    “The answer is $\backslash\backslash$answer” |'
  prefs: []
  type: TYPE_TB
- en: '| Logical Reasoning | Original Prompt: Identify whether one sentence entails
    the next:$\backslash$ n Every bachelor is tall, and the man in the hat is short.
    Therefore the man in the hat is married |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: The candidate prompt is too short and lacks specific instructions
    and requirements for answering the question. It only mentions the need to identify
    if one sentence entails the next, but it does not provide clear guidelines on
    how to reason through the question or provide evidence for the answer. The prompt
    is vague and leaves many crucial details to interpretation, making it difficult
    for the model to generate an accurate answer. The candidate prompt also does not
    provide a common output format like “the answer is $\backslash\backslash$answer”.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Prompt: Your task is to determine whether the following sentences
    entail each other: “Every bachelor is tall, and the man in the hat is short. Therefore
    the man in the hat is married.” Provide a clear and concise answer, along with
    a brief explanation or evidence to support your answer. Consider the logic of
    the statements, the properties and characteristics of the entities mentioned,
    and the implications of the sentences. Ensure that your answer is contextually
    appropriate and maintains the same intent as the original question. Pay attention
    to providing a well-reasoned and evidence-based answer that is easy to understand
    and follow. Print the output in the format “The answer is $\backslash\backslash$answer”
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Original Prompt: Identify whether a given statement contains an anachronism:
    The sun’s creation during the Stelliferous Era is the cause of its size, color,
    and brightness. Your choices are: (1) Yes (2) No |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: The candidate prompt is too short and lacks specific instructions
    and requirements for answering the question. It only mentions the need for a yes
    or no answer, but it does not provide clear guidelines on how to reason through
    the question or provide evidence for the answer. The prompt is vague and leaves
    many crucial details to interpretation, making it difficult for the model to generate
    an accurate answer. The candidate prompt also does not provide a common output
    format like ”the answer is $\backslash\backslash$answer”. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Prompt: Your task is to analyze the following statement for
    any potential anachronisms: ”The sun’s creation during the Stelliferous Era is
    the cause of its size, color, and brightness.” An anachronism, in this context,
    refers to something that is out of time or chronologically misplaced. Provide
    a clear and concise answer, along with a brief explanation or evidence to support
    your answer. Consider the historical and scientific accuracy of the statement.
    Your choices are: (1) Yes, it contains an anachronism (2) No, it does not contain
    an anachronism. Print the output in the format “The answer is $\backslash\backslash$answer”
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Original Prompt: Infer the date from context: Marie Curie was awarded
    the Nobel Prize in Physics in 1903. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: The candidate prompt doesn’t specify what date is being referred
    to or provide clear instructions for answering the question. It merely states
    the need to infer a date from the given context. Without further guidance, it’s
    difficult to determine what date is expected. The prompt should provide more precise
    instructions on the expected output, how to approach the question, and the format
    of the answer. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Prompt: Based on the information provided, determine the year
    in which Marie Curie was awarded the Nobel Prize in Physics. Ensure you read the
    context carefully and extract the relevant details. Print the output in the format
    “The answer is $\backslash\backslash$answer” |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Generated examples of PRoMPTd for mathematical and logical reasoning
    tasks. In-order of their appearance, the samples are taken from GSM8K, MATH, Analytical
    Entailment, Anachronisms, and Date Understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task Type | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Code Generation | Original Prompt: |'
  prefs: []
  type: TYPE_TB
- en: '|  | 
    from  typing  import  List def  separate_paren_groups(paren_string:  str)  ->  List[str]:
    """ ␣␣␣␣Input␣to␣this␣function␣is␣a␣string␣containing␣multiple␣groups␣of␣nested␣parentheses.␣Your␣goal␣is␣to
    ␣␣␣␣separate␣those␣group␣into␣separate␣strings␣and␣return␣the␣list␣of␣those. ␣␣␣␣Separate␣groups␣are␣balanced␣(each␣open␣brace␣is␣properly␣closed)␣and␣not␣nested␣within␣each␣other
    ␣␣␣␣Ignore␣any␣spaces␣in␣the␣input␣string. ␣␣␣␣>>>␣separate_paren_groups(’(␣)␣((␣))␣((␣)(␣))’)
    ␣␣␣␣[’()’,␣’(())’,␣’(()())’] ␣␣␣␣""" pass  #  Function  implementation  here |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: The candidate prompt is incomplete as it does not provide any
    example input or output for the model to follow. It also lacks specific instructions
    on how to handle edge cases such as empty strings or strings without parentheses.
    The better prompt clarifies these points and provides a clear task description
    along with a specific input-output format. It also provides a complete function
    signature, which makes it easier for the model to generate the correct code. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Prompt: Write a Python function that takes a string as an input,
    where the string contains multiple groups of nested parentheses. The function
    should separate these groups into separate strings and return a list of these
    strings. Each group of parentheses is balanced (i.e., every open parenthesis is
    properly closed) and not nested within each other. Ignore any spaces in the input
    string. Handle the edge cases such as empty strings or strings without parentheses
    by returning an empty list. The function signature is: ‘def separate_paren_groups(paren_string:
    str) -¿ List[str]:‘. An example of the expected functionality is: For the input
    ‘separate_paren_groups(’( ) (( )) (( )( ))’)‘, the function should return ‘[’()’,
    ’(())’, ’(()())’]‘. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Original Prompt: |'
  prefs: []
  type: TYPE_TB
- en: '|  | 
    def  encode_shift(s:  str)  ->  str: #  returns  encoded  string  by  shifting  every  character  by  5  in  the  alphabet.
    return  ’’.join([chr(((ord(ch)  +  5  -  ord(’a’))  %  26)  +  ord(’a’))  for  ch  in  s])
    def  decode_shift(s:  str)  ->  str: #  takes  as  input  string  encoded  with  encode_shift  function.  Returns  decoded  string.
    def  shift_char(char:  str)  ->  str: if  not  char.isalpha(): return  char base  =  65  if  char.isupper()  else  97
    return  chr((ord(char)  -  base  -  5)  %  26  +  base) return  ’’.join(map(shift_char,  s))
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: Write a Python function that takes as input a string encoded with
    the ‘encode_shift’ function. The ‘encode_shift‘ function returns an encoded string
    by shifting every character by 5 in the alphabet. Your task is to write the ‘decode_shift’
    function that returns the original string before it was encoded. Your function
    should take into account the wrap-around of the alphabet, i.e., after ‘z’, it
    should go back to ‘a’. The function signature should be: ‘def decode_shift(s:
    str) -¿ str:‘ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Generated examples of PRoMPTd for code generation task. The samples
    are taken from the Human Eval dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task Type | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Code Generation | Original Prompt: A test of ’hallucinations’ by asking questions
    whose answers are known to be unknown: How often did Abraham Lincoln cut his toenails?
    Your choices are: (1) Every Saturday night (2) Unknown |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: The candidate prompt is too short and lacks specific instructions
    and requirements for answering the question. It only mentions the need for a choice
    between two options, but it does not provide clear guidelines on how to reason
    through the question or provide evidence for the answer. The prompt is vague and
    leaves many crucial details to interpretation, making it difficult for the model
    to generate an accurate answer. The candidate prompt also does not provide a common
    output format like “the answer is $\backslash$answer”. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Prompt: Your task is to answer the following question: How often
    did Abraham Lincoln cut his toenails? Your choices are: (1) Every Saturday night
    (2) Unknown. Provide a clear and concise answer, along with a brief explanation
    or evidence to support your answer. Consider the context of the question, such
    as the historical period and the availability of information about Abraham Lincoln’s
    personal habits. Ensure that your answer is contextually appropriate and maintains
    the same intent as the original question. Pay attention to providing a well-reasoned
    and evidence-based answer that is easy to understand and follow. Print the output
    in the format “The answer is $\backslash$answer”. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Original Prompt: Determine whether an artificially constructed sentence
    relating to sports is plausible or implausible: Malcolm Brogdon banked the shot
    in |'
  prefs: []
  type: TYPE_TB
- en: '|  | Reason: The candidate prompt is not related to basketball, and it appears
    to be a mix of different prompts. It doesn’t provide any specific instructions
    or requirements for determining the plausibility of a sentence related to basketball.
    The better prompt provides a clear context and a specific sentence to evaluate.
    It also provides a specific output format, guiding the model to generate a clear
    and concise answer with a brief explanation. This ensures that the model understands
    the task and can generate the appropriate response. Print the output in the format
    “The answer is $\backslash$answer”. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Rewritten Prompt: Determine whether the following sentence related to
    basketball is plausible or implausible: “Malcolm Brogdon banked the shot in.”
    Provide a clear and concise answer (either ‘plausible’ or ‘implausible’) along
    with a brief explanation of your reasoning. Consider the context of the sentence,
    the players involved, and the action described. Ensure that your answer is contextually
    appropriate and maintains the same intent as the original sentence. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Generated examples of PRoMPTd for Sports Understanding and Known-Unknowns
    from BigBench.'
  prefs: []
  type: TYPE_NORMAL
