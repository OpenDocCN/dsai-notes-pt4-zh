- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:22'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated
    Images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13784](https://ar5iv.labs.arxiv.org/html/2404.13784)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ali Naseh, Katherine Thai, Mohit Iyyer & Amir Houmansadr
  prefs: []
  type: TYPE_NORMAL
- en: University of Massachusetts Amherst
  prefs: []
  type: TYPE_NORMAL
- en: '{anaseh,kbthai,miyyer,amir}@cs.umass.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the digital imagery landscape rapidly evolving, image stocks and AI-generated
    image marketplaces have become central to visual media. Traditional stock images
    now exist alongside innovative platforms that trade in prompts for AI-generated
    visuals, driven by sophisticated APIs like DALL-E 3 and Midjourney. This paper
    studies the possibility of employing multi-modal models with enhanced visual understanding
    to mimic the outputs of these platforms, introducing an original attack strategy.
    Our method leverages fine-tuned CLIP models, a multi-label classifier, and the
    descriptive capabilities of GPT-4V to create prompts that generate images similar
    to those available in marketplaces and from premium stock image providers, yet
    at a markedly lower expense. In presenting this strategy, we aim to spotlight
    a new class of economic and security considerations within the realm of digital
    imagery. Our findings, supported by both automated metrics and human assessment,
    reveal that comparable visual content can be produced for a fraction of the prevailing
    market prices ($0.23 - $0.27 per image), emphasizing the need for awareness and
    strategic discussions about the integrity of digital media in an increasingly
    AI-integrated landscape. Our work also contributes to the field by assembling
    a dataset consisting of approximately 19 million prompt-image pairs generated
    by the popular Midjourney platform, which we plan to release publicly.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In recent years, image stocks and marketplaces have become increasingly important
    in the commercial and business sectors. Alongside traditional stock images, known
    for their high quality and compositions by expert photographers, a new trend has
    emerged in the form of marketplaces for AI-generated images, such as PromptBase,¹¹1https://promptbase.com
    PromptSea,²²2https://www.promptsea.io and Neutron Field.³³3https://neutronfield.com
    Unlike traditional stocks where the images themselves are traded, these innovative
    platforms trade in the prompts that lead to the creation of AI-generated images.
    Advanced text-to-image APIs like DALL-E 3 (Betker et al., [2023](#bib.bib1)) and
    Midjourney,⁴⁴4https://www.midjourney.com with their extraordinary ability to generate
    stunning visuals, are at the forefront of this trend. However, identifying the
    right prompts to produce such images is not a straightforward task (Cao et al.,
    [2023](#bib.bib2); Oppenlaender, [2023](#bib.bib13)), leading to the development
    of marketplaces where users can exchange their crafted prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53e8b0a1889cad6f43f41289832baa8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Overview of our attack.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With the growing demand for purchasing prompts from AI-generated sources, and
    the continued interest in traditional stock images, a pivotal question emerges:
    Can an adversary find a prompt and utilize current state-of-the-art text-to-image
    models to generate similar images at a lower cost? This question becomes particularly
    significant when considering that some of the natural images in traditional stocks
    are very expensive.⁵⁵5https://tinyurl.com/28razsrn This paper investigates this
    query, demonstrating how the latest multi-modal models with visual understanding
    capabilities can be harnessed for such attacks against these marketplaces. Our
    study exposes potential vulnerabilities in the digital imagery landscape and emphasizes
    the necessity for strategic countermeasures in response to these evolving challenges.'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we first demonstrate that if an adversary is given images generated
    by one of the text-to-image APIs featured in AI-generated image marketplaces,
    they can, by accessing one of these APIs, find a prompt to generate similar images.
    Additionally, we show that the same attack methodology can be applied to generate
    images that closely resemble natural images offered in traditional stock markets.
    This dual capability of the attack strategy underscores its potential impact on
    both AI-generated and natural image domains.
  prefs: []
  type: TYPE_NORMAL
- en: Can we simply utilize off-the-shelf image captioning models to recover the prompts
    used to generate an input image? Current image captioning models (Li et al., [2023](#bib.bib10);
    [2022a](#bib.bib9); Wang et al., [2022](#bib.bib24); Chen et al., [2022](#bib.bib3))
    often produce general descriptions that capture the broad aspects of an image
    but typically lack the specificity required for text-to-image API prompts like
    those used in Midjourney. The textual input prompts for these APIs need to be
    more specific and follow a particular format, usually by incorporating keywords
    and key phrases that significantly influence the generation (Oppenlaender, [2023](#bib.bib13)).
    Directly using captions from standard image captioning models may not yield effective
    results since they often omit critical details and stylistic elements. While tools
    like CLIP Interrogator⁶⁶6https://huggingface.co/spaces/fffiloni/CLIP-Interrogator-2
    can suggest corresponding text and keywords, they may not provide enough detail
    and are especially limited when describing natural images. Similarly, although
    multimodal models like Gemini⁷⁷7https://deepmind.google/technologies/gemini/#introduction
    and GPT-4V can offer more elaborate descriptions, they might still miss out on
    essential keywords or named entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'To bridge the gaps presented by these tools, we introduce a three-component
    attack strategy: a fine-tuned CLIP model (Radford et al., [2021](#bib.bib14))
    on a large dataset of Midjourney generations, a multi-label classifier for related
    keywords and named entities, and GPT-4V for its ability to generate comprehensive
    prompts based on our instructions and information from the CLIP model and the
    classifier. We then implement a cyclic approach to refine these prompts, comparing
    the generated images with the original (ground-truth) image(s). The overview of
    our attack strategy is illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated
    Images").'
  prefs: []
  type: TYPE_NORMAL
- en: As a significant contribution of our work, we have collected a large-scale dataset
    consisting of 19,137,140 generations from Midjourney’s Discord server. This dataset,
    which pairs each prompt with its corresponding image or gridded image, aids in
    fine-tuning the CLIP model and training the multi-label classifier to identify
    related keywords and named entities. Our approach is validated through both automatic
    metrics and human evaluation, confirming that our attack outperforms existing
    baselines. Additionally, we provide a cost analysis to justify the feasibility
    of the attack. Our cost estimation indicates that an attacker can generate a reasonably
    similar image to the targeted one at a cost of only $0.23 to $0.27.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize our contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To the best of our knowledge, this work presents the first systematic investigation
    of potential attacks against images generated by two of the most popular text-to-image
    APIs, as well as against natural images from commercial stock collections.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We demonstrate that an attacker can generate images similar to those produced
    by AI, typically priced between $3 and $7, and natural stock images valued between
    $70 and $500 per image, all at a mere fraction of the cost — just a few cents.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A significant portion of our effort was dedicated to collecting and preprocessing
    a substantial dataset of Midjourney’s generations. This dataset is not only pivotal
    to our project but also stands as a valuable resource for future research in this
    field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Text-to-Image Generation Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The journey of text-to-image generation began with methods based on Generative
    Adversarial Networks (GANs) (Goodfellow et al., [2014](#bib.bib6)). These GAN-based
    models paved the way for the field, focusing on synthesizing visual content from
    textual descriptions (Reed et al., [2016](#bib.bib16); Mansimov et al., [2015](#bib.bib12);
    Zhang et al., [2017](#bib.bib27); Xu et al., [2018](#bib.bib25); Zhu et al., [2019](#bib.bib29)).
    In recent years, the emergence of diffusion models (Sohl-Dickstein et al., [2015](#bib.bib23))
    and large pre-trained autoregressive models has led to the introduction of many
    new text-to-image models (Ramesh et al., [2021](#bib.bib15); Ding et al., [2021](#bib.bib5);
    Cho et al., [2020](#bib.bib4)). These developments introduce multimodal transformer
    language models, which are proficient at learning the distribution of sequences
    of discrete image codes from given text inputs. Parti (Yu et al., [2022](#bib.bib26))
    introduces a sequence-to-sequence autoregressive model treating text-to-image
    synthesis as a translation task, converting text descriptions into visuals. Imagen (Saharia
    et al., [2022](#bib.bib19)) employs a large language model for quality text features
    and introduces an Efficient U-Net for diffusion models. Latent Diffusion Models
    (LDM) (Rombach et al., [2022](#bib.bib18)), such as Stable Diffusion (Rombach
    et al., [2021](#bib.bib17)), employ diffusion processes within the latent space.
    This approach facilitates efficient model training on systems with constrained
    computational resources, without compromising the fidelity and quality.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Prompt Stealing Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt stealing attacks represent a domain closely related to our work. In such
    attacks, the objective of the attacker is typically to reconstruct the original
    prompt used in generating an image. However, our goal is to replicate AI-generated
    and natural images, where the resulting prompt does not necessarily need to match
    the original one precisely, word-by-word. Although this area has seen limited
    exploration, there have been a few metods in the context of both Large Language
    Models (LLMs) (Zhang & Ippolito, [2023](#bib.bib28); Sha & Zhang, [2024](#bib.bib21))
    and Text-to-Image models (Shen et al., [2023](#bib.bib22)).
  prefs: []
  type: TYPE_NORMAL
- en: 3 Threat Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Attacker’s Capabilities.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our proposed attack, we consider an attack scenario that adheres to a realistic
    paradigm where the attacker is granted black-box access to all text-to-image APIs.
    Specifically, the attacker lacks access to the underlying model and its training
    data; their interaction is confined to providing input and receiving the corresponding
    output. A key assumption in our model is that the attacker possesses inference-time
    access to the API and also access to the generations of one of these APIs, namely
    Midjourney’s Discord server. This is a realistic assumption, as we have access
    to it. Furthermore, we extend our consideration to a more complicated scenario
    where the attacker gains access to an alternate API, distinct from the primary
    target, to facilitate the attack. Additionally, in the case of natural images,
    the attacker only has access to the targeted image(s) and nothing more.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Attacker’s Objective.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The attacker’s goal in this scenario is to identify a prompt that can generate
    images similar to a given image or set of images, whether produced by a text-to-image
    API or featured in one of the commercial stock image collections. This objective
    necessitates that the attacker has the capability to send queries to the API to
    execute the attack. As previously noted, our investigation also includes scenarios
    where the attacker utilizes an alternative API, distinct from the primary target,
    to carry out the attack.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Attacker’s Target.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our attack scenario, the main targets are two well-known text-to-image APIs:
    Midjourney and DALL-E 3\. We aim to target the images generated by these APIs
    in order to find prompts that produce similar images. Additionally, for natural
    images, we focus on images from the Getty Images website,⁸⁸8[https://www.gettyimages.com/](https://www.gettyimages.com/)
    which represents one of the popular image stock sources.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6599460a09bf430eaf378e8a06727584.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Examples illustrating scenarios where the baseline models, BLIP2
    and CLIP Interrogator, fail to accurately extract relevant keywords and modifiers
    from given prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'Our approach to executing the attack centers around three primary components:
    keyword extraction, modifier extraction, and prompt generation. Each of these
    components plays a pivotal role in the overall effectiveness of the attack strategy,
    as detailed in the subsequent subsections.'
  prefs: []
  type: TYPE_NORMAL
- en: Keyword Extraction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As previously noted, image captioning models often lack the specificity required
    for text-to-image API prompts, typically omitting crucial details and stylistic
    elements needed for APIs like Midjourney. Adding to this challenge, a key limitation
    of these models is their inconsistent accuracy in extracting specific keywords,
    special names, or named entities depicted in the images. Although they capture
    some details, they often lack the precision needed for completely accurate prompt
    replication. This combination of generalization and lack of precision in keyword
    and entity extraction significantly hinders their effectiveness in accurately
    generating prompts for text-to-image generation. An example of BLIP2’s failure
    to detect the keywords is shown in Figure [2](#S4.F2 "Figure 2 ‣ 4 Methodology
    ‣ Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated
    Images").
  prefs: []
  type: TYPE_NORMAL
- en: To address this challenge, we employ a substantial dataset obtained from Midjourney,
    consisting of a subset of 2 million pairs of prompts and corresponding upscaled
    images generated by Midjourney. We then fine-tune a CLIP model on this dataset.
    The CLIP model, initially pre-trained on 2 billion pairs of captions and images,
    many of which are not precisely aligned, requires fine-tuning on the Midjourney
    subset to better align with the nature of the prompts and images in our study.
  prefs: []
  type: TYPE_NORMAL
- en: The fine-tuned CLIP model is then utilized to generate a set of 5 million text
    and corresponding image embeddings. During the inference phase, we use the fine-tuned
    CLIP model to obtain image embeddings of the targeted image(s) and identify the
    closest text and image embeddings. From these, we extract a list of named entities
    and keywords from the most closely related prompts and the corresponding prompts
    of the nearest images. Finally, this information, along with other relevant data,
    is fed into GPT-4V.
  prefs: []
  type: TYPE_NORMAL
- en: Modifier Extraction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides keywords and named entities, incorporating adjectives, adverbs, and
    specific styles, referred to as modifiers, can significantly enhance the quality
    of generations (Oppenlaender, [2023](#bib.bib13)). However, image captioning models
    typically are unable to detect these modifiers, failing to recognize and extract
    them accurately. Additionally, the CLIP Interrogator also does not consistently
    perform well in detecting these modifiers. An example of the CLIP Interrogator’s
    failure to detect modifiers is illustrated in Figure [2](#S4.F2 "Figure 2 ‣ 4
    Methodology ‣ Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated
    Images"). To overcome this, we train a Multi-Layer Perceptron (MLP) as a multi-label
    classifier on image embeddings. After identifying 1800 frequent modifiers from
    our Midjourney dataset, we train the multi-label classifier on a selected subset
    of around $300,000$ samples. During inference, we determine the top modifiers
    with the highest probability scores. These modifiers, along with other pertinent
    information, are then provided to GPT-4V.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Generation.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The final step involves using a model with strong visual understanding capabilities
    for detailed image description. For this, we select GPT-4V, a multi-modal model
    known for its exceptional visual understanding. However, GPT-4V alone may not
    always suffice. To bridge this gap, we complement it with detailed instructions
    and information derived from the other two components. Our comprehensive instructions
    to GPT-4V, called initial instructions, include a detailed task description, relevant
    modifiers, general keywords, named entities, and an example prompt from Midjourney
    samples. While the fine-tuned CLIP model and the multi-label classifier provide
    relevant information, GPT-4V acts as an additional classifier, selecting the most
    appropriate modifiers, keywords, and named entities. This leads to a two-level
    extraction process. All the related information, along with the targeted images,
    is fed into GPT-4V to generate the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Refinement of the Initial Prompt.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The initial prompt generated by GPT-4V might not always capture every detail
    in the image. To enhance the prompt quality, we implement a cyclical refinement
    process. This involves devising a new set of instructions for GPT-4V, including
    elements like a detailed task description, relevant modifiers, keywords, named
    entities, an example from Midjourney, the targeted image(s), the prompt used in
    the previous round of the cycle, and the corresponding generated image(s). The
    task description also emphasizes comparing the generated image(s) with the targeted
    ones, refining the prompt based on differences in styles, themes, or elements.
    This iterative process continues over multiple rounds to progressively refine
    the prompt’s accuracy and relevance. The initial and refining prompts are displayed
    in Table [5](#A3.T5 "Table 5 ‣ Appendix C Failure Cases ‣ Iteratively Prompting
    Multimodal LLMs to Reproduce Natural and AI-Generated Images") and Table [6](#A3.T6
    "Table 6 ‣ Appendix C Failure Cases ‣ Iteratively Prompting Multimodal LLMs to
    Reproduce Natural and AI-Generated Images"), respectively. The overview of the
    attack is presented in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Iteratively
    Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images").
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experimental Settings and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Midjourney Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As previously mentioned, Midjourney stands as a frontier text-to-image API,
    known for generating high-quality and realistic images. Users interact with Midjourney’s
    bot on their official Discord server to input prompts and generate images, making
    these generations accessible to anyone who joins the server. Acknowledging the
    importance of this API, we collect millions of samples from Midjourney’s Discord
    channels. This substantial dataset is crucial for our attack, providing a wide
    array of prompts and corresponding images. Details about the dataset and its pre-processing
    can be found in the Appendix [A](#A1 "Appendix A Data Collection ‣ Iteratively
    Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images").
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Top-1 Accuracy | Top-5 Accuracy | Top-10 Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original CLIP model | 0.802 | 0.915 | 0.9395 |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned on 200K samples | 0.8065 | 0.9413 | 0.967 |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned on 500K samples | 0.9003 | 0.9653 | 0.9783 |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned on 2M samples | 0.9167 | 0.9762 | 0.9857 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparative metrics of the original CLIP and fine-tuned models on
    varying dataset sizes, highlighting improvements in Top-1, Top-5, and Top-10 accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Fine-Tuning the CLIP Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model and Hyperparameter Settings.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the task of fine-tuning, we select the CLIP ViT-G/14 model (Ilharco et al.,
    [2021](#bib.bib8)), which is pre-trained on a vast collection of 2 billion samples
    from the LAION dataset (Schuhmann et al., [2022](#bib.bib20)). We pick this model
    variant as it shows better performance in ImageNet zero-shot classification tasks.⁹⁹9[https://github.com/mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)
    Through iterative experimentation, we optimize the hyperparameters, setting a
    learning rate (lr) of $1\times 10^{-5}$, a batch size of 32, and gradient accumulation
    steps to 2\. The model is fine-tuned for 10 epochs on a cluster with 4 A100 GPUs,
    ensuring efficient computation and optimal convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Resource constraints guide our decision to fine-tune the CLIP model on a subset
    of the Midjourney dataset. After experimenting with various sample sizes, we settle
    on a subset comprising 2 million samples from Midjourney. The results of fine-tuning
    the model with different dataset sizes are detailed in Table [1](#S5.T1 "Table
    1 ‣ 5.1 Midjourney Dataset ‣ 5 Experimental Settings and Results ‣ Iteratively
    Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images").
  prefs: []
  type: TYPE_NORMAL
- en: Metric and Evaluation Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To evaluate the fine-tuned CLIP models, we use a test set of 10,000 samples,
    employing CLIP’s image and text encoders to generate embeddings. We calculate
    cosine similarity for each test image to identify the closest corresponding text,
    measuring the model’s accuracy by whether the ground-truth text is within the
    Top-1, Top-5, or Top-10 closest texts. For example, a Top-5 accuracy of 90% indicates
    that the ground-truth text ranks among the top-5 closest texts for 90% of the
    samples. Table [1](#S5.T1 "Table 1 ‣ 5.1 Midjourney Dataset ‣ 5 Experimental Settings
    and Results ‣ Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated
    Images") presents the evaluation metrics for both the original and the fine-tuned
    CLIP models across different dataset sizes. The fine-tuned model significantly
    outperforms the original in this specific data type. Particularly, the model fine-tuned
    on 2 million samples demonstrates the best performance, although the improvement
    is not substantially higher compared to other dataset sizes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bda82ca762fc422c84ede7a05ca9995e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of the multi-label classifier’s performance for different
    values of $k$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Training Multi-Label Classifier
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Model and Hyperparameter Settings.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We utilize a Multi-Layer Perceptron (MLP) architecture with three hidden layers
    for this task. The ReLU activation function is employed, along with a learning
    rate of $0.001$ epochs using an RTX 8000 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Upon analyzing the Midjourney dataset, we identify approximately $1800$ of this
    dataset as a validation set to monitor and adjust model performance. Detailed
    information about the process of extracting these modifiers can be found in the
    Appendix [A.5](#A1.SS5 "A.5 Collecting Modifiers ‣ Appendix A Data Collection
    ‣ Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated
    Images").
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics and Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For this task, we utilize precision and recall as the primary metrics. During
    inference, the top $k$ for our overall attack evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '| Setting | BLIP2 | CLIP Interrogator | Our Attack |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | CLIP-S | HE | CLIP-S | HE | CLIP-S | HE |'
  prefs: []
  type: TYPE_TB
- en: '| Midjourney (multiple images) | 0.8477 | 3.4 | 0.8874 | 3.88 | 0.8946 | 4.36
    |'
  prefs: []
  type: TYPE_TB
- en: '| Midjourney (single image) | 0.7483 | 3.6 | 0.8437 | 4.04 | 0.8844 | 4.52
    |'
  prefs: []
  type: TYPE_TB
- en: '| DALL-E 3 (multiple images) | 0.8534 | 4.16 | 0.8569 | 3.68 | 0.8883 | 4.56
    |'
  prefs: []
  type: TYPE_TB
- en: '| DALL-E 3 (single image) | 0.7932 | 2.8 | 0.8007 | 2.84 | 0.896 | 4.08 |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-setting (multiple images) | 0.798 | 3.8 | 0.8169 | 4.04 | 0.859 | 4.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-setting (single image) | 0.7911 | 3.04 | 0.8215 | 3.28 | 0.886 | 4.08
    |'
  prefs: []
  type: TYPE_TB
- en: '| Natural images (single image) | 0.7516 | 3.8 | 0.748 | 3.92 | 0.8031 | 4.36
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Comparison of different methods using CLIP-S (CLIP-score) and HE (Human
    Evaluation) metrics across various settings.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Overall Attack Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experimental Settings.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We evaluate two major text-to-image APIs, Midjourney and DALL-E 3, for AI-generated
    images and Getty Images website for natural images. Our scenarios include both
    multiple images from a single prompt and single image cases for Midjourney and
    DALL-E 3\. We also explore an adversary using a different API, with Midjourney
    as the target and DALL-E 3 as the alternate, across multiple and single image
    variations. This results in seven distinct settings, with $150$ samples for settings
    5-7.
  prefs: []
  type: TYPE_NORMAL
- en: Model and Hyperparameters.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In scenarios 1, 2, and 7, we utilize Midjourney as the text-to-image model for
    generating images. In scenarios 3-6, DALL-E 3 is employed for image generation,
    with the output size set to $1024\times 1024$ and standard quality. Across all
    scenarios, GPT-4V serves as the multimodal model for visual understanding and
    prompt generation.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For scenarios involving Midjourney-generated images, a small subset of the Midjourney
    dataset, not used in CLIP model fine-tuning or multi-label classifier training,
    is selected. In scenarios where DALL-E 3 generates the targeted images, we use
    a subset of Midjourney prompts to generate corresponding images, forming the evaluation
    dataset. For scenarios involving natural images, we select diverse samples from
    Getty Images website.
  prefs: []
  type: TYPE_NORMAL
- en: Setting Original Image Our Attack CLIP Interrogator BLIP2 Midjourney (Setting
    2) ![Refer to caption](img/67bcd43439bac9c6235b6068fa303883.png)   ![Refer to
    caption](img/eaa4341239bd483dbb1e322ee04db9c1.png)   ![Refer to caption](img/48df18a502115c4387c9417d553463e2.png)  ![Refer
    to caption](img/429735c53408c0e33576d2bd921e2889.png)    DALL-E 3 (Setting 4)   ![Refer
    to caption](img/05cda6d697336e288d72db4f7c4d9724.png)   ![Refer to caption](img/fbea3d8093c122b12735dac326528726.png)   ![Refer
    to caption](img/faa6291d36c6a655c0249aeab1a39e2f.png)  ![Refer to caption](img/0344968cbc1d564cf79132c541503fa5.png)    Cross-setting
    (Setting 6)   ![Refer to caption](img/c1d06f0d8f1787f4bf1c76670aed775b.png)   ![Refer
    to caption](img/09b8ea73a4bae8ce7f467348ce5be22e.png)   ![Refer to caption](img/5fca78c0c272a411ef0ef802f4f062b5.png)  ![Refer
    to caption](img/560dd85d475378484c1fc91475e42bca.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Comparison of the original image with those generated by our method,
    BLIP2, and CLIP Interrogator for three different settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Metric.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To assess the similarity between the images generated by our approach and the
    ground-truth images, we employ both automated metrics and human evaluation. The
    automated metric, termed Clip-score (Hessel et al., [2021](#bib.bib7)), involves
    calculating cosine similarity using the image embeddings from the original CLIP
    model provided by OpenAI. Additionally, for human evaluation, we select five random
    samples from each setting (35 samples in total) and ask five annotators to rate
    these images. They use a 5-point Likert scale to score the images based on perceived
    similarity. More details about the human evaluation process are presented in Appendix [B](#A2
    "Appendix B Human Evaluation ‣ Iteratively Prompting Multimodal LLMs to Reproduce
    Natural and AI-Generated Images").
  prefs: []
  type: TYPE_NORMAL
- en: Baselines.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our evaluation includes BLIP2, a recent image captioning model, and CLIP Interrogator
    2, as baselines. For scenarios involving a single image, we use the textual output
    provided by these baselines directly. In settings with multiple images, to ensure
    a fair comparison, we first process each image through the baselines and then
    select the one with the highest similarity score.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The results for all seven settings are presented in Table [2](#S5.T2 "Table
    2 ‣ Evaluation Metrics and Results. ‣ 5.3 Training Multi-Label Classifier ‣ 5
    Experimental Settings and Results ‣ Iteratively Prompting Multimodal LLMs to Reproduce
    Natural and AI-Generated Images"). Across these settings, our approach consistently
    outperforms the baselines. The margin of superiority is substantial in most settings,
    except for the first, where CLIP Interrogator shows close performance based on
    CLIP-score. It’s observed that the effectiveness of all methods diminishes in
    scenarios 5 and 6, likely due to the use of a different API by the attacker. Furthermore,
    the results show improved performance in scenarios involving multiple images compared
    to those with a single image, likely because multiple images provide more comprehensive
    information for analysis. Some examples of generated images by our approach and
    other baselines are presented in Figure [4](#S5.F4 "Figure 4 ‣ Dataset. ‣ 5.4
    Overall Attack Evaluation ‣ 5 Experimental Settings and Results ‣ Iteratively
    Prompting Multimodal LLMs to Reproduce Natural and AI-Generated Images").
  prefs: []
  type: TYPE_NORMAL
- en: '| Text-to-Image API | BLIP2 | CLIP Interrogator | Our Attack |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DALL-E 3 | 0.7509 | 0.7044 | 0.8284 |'
  prefs: []
  type: TYPE_TB
- en: '| Midjourney | 0.7516 | 0.7480 | 0.8132 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Image similarity scores for our attack on natural images using different
    text-to-image APIs, compared with baseline models.'
  prefs: []
  type: TYPE_NORMAL
- en: '5.5 Natural Images: DALL-E 3 vs Midjourney'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In scenarios where the target imagery comprises natural scenes from commercial
    stock providers, our investigation encompassed the use of both DALL-E 3 and Midjourney
    as the underlying text-to-image APIs. As delineated in Table [3](#S5.T3 "Table
    3 ‣ Evaluation Results. ‣ 5.4 Overall Attack Evaluation ‣ 5 Experimental Settings
    and Results ‣ Iteratively Prompting Multimodal LLMs to Reproduce Natural and AI-Generated
    Images"), DALL-E 3 and Midjourney exhibit comparable performance in terms of image
    similarity metrics. However, a qualitative assessment reveals that Midjourney’s
    outputs are often more lifelike and convincing. This observation is consistent
    with the platform’s design philosophy, which emphasizes artistic expression and
    photorealism. An illustrative example contrasting the outputs from both APIs,
    with respect to a natural image from a commercial stock collection, is provided
    in Figure [5](#S5.F5 "Figure 5 ‣ 5.5 Natural Images: DALL-E 3 vs Midjourney ‣
    5 Experimental Settings and Results ‣ Iteratively Prompting Multimodal LLMs to
    Reproduce Natural and AI-Generated Images").'
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-Image API Original Image BLIP2 CLIP Interrogator Our Attack DALL-E 3
    ![Refer to caption](img/3485e2ef86299317ec4cffdea0d12abe.png)   ![Refer to caption](img/b00b273e4b02ec1f37a52e09daa513c3.png)   ![Refer
    to caption](img/5714195baa29330f6d2d3d31bd53eeff.png)  ![Refer to caption](img/f54ba3e75cc59a7796ef3c2c4cab0721.png)  Midjourney    ![Refer
    to caption](img/4bcec1fd002ff90574b85ec4e541176c.png)   ![Refer to caption](img/a6704b9e2e2ebf315cdd960365506473.png)  ![Refer
    to caption](img/578da31c42b86c7ded9521c152d4530d.png)    DALL-E 3   ![Refer to
    caption](img/86e601d789841996e3da0e95f15462ef.png)   ![Refer to caption](img/8255cab92783f9cc941e9ce1e109adc1.png)   ![Refer
    to caption](img/067eb6fdd96f5f0a33bc258d16ad9985.png)  ![Refer to caption](img/02038d2cba7d02cc32911d40a3b4bdf4.png)  Midjourney    ![Refer
    to caption](img/8c5dd34a013e7646c4b1f6b30c226714.png)   ![Refer to caption](img/c1ca603d6a746e6d76c5f525cb0a1e7e.png)  ![Refer
    to caption](img/fbbc57ff3e78bcd11630264ba31eab6a.png)    DALL-E 3   ![Refer to
    caption](img/dd5f6f13a96469d88d4b63dd5e840440.png)   ![Refer to caption](img/b669753c78481524e5054379e7746644.png)   ![Refer
    to caption](img/7dc91f494495c91f5fb8a48ccb7e2e82.png)  ![Refer to caption](img/ef3d2887a899752ae0848b412ff9d92e.png)    Midjourney    ![Refer
    to caption](img/3f158b80264ee37ccc8149aa22885a67.png)   ![Refer to caption](img/97784dc0cad105e56188fc7d39d7a90b.png)  ![Refer
    to caption](img/e528c4c00e79b478a8a0c7abc3cf2639.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Comparison of natural images from two different Text-to-Image APIs
    with those generated by our method, BLIP2, and CLIP Interrogator.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like any other method, our approach may encounter failure cases due to the
    complexity of its multi-component pipeline. Potential reasons for these failures
    include:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fine-tuned CLIP model might not retrieve related keywords if the words are
    rare, or the model itself may fail to identify the closest texts to the images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The multi-label classifier might not select appropriate modifiers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-4V might not accurately extract related keywords, named entities, and modifiers,
    or it might fail to generate an appropriate prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The text-to-image models might not produce images that align well with the text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The inherent uncertainty in both multimodal LLMs and text-to-image models adds
    complexity to the task. We have included some examples from setting 1, which yielded
    the lowest image similarity scores, to analyze what contributed to the suboptimal
    results. These examples and their interpretations can be found in the Appendix [C](#A3
    "Appendix C Failure Cases ‣ Iteratively Prompting Multimodal LLMs to Reproduce
    Natural and AI-Generated Images"). The multi-component nature of our attack pipeline
    presents numerous opportunities for refinement. Each component, from the fine-tuning
    of the CLIP model and the training regimen of the multi-label classifier to the
    optimization of modifiers extracted from data, holds the potential to elevate
    the attack’s effectiveness. Moreover, the art of prompting GPT-4V is not monolithic;
    alternative prompt constructions could yield improved results within our cyclic
    approach. While this paper establishes a solid foundation, the practical enhancements
    of each component present as promising directions for future work, offering prospects
    for even more complex attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Cost Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Original Image Initial Prompt Round 1 Round 2 Round 3 ![Refer to caption](img/94b3beba2f2c1e4b04b2d337b6bc2af9.png)   ![Refer
    to caption](img/f0cd49187a00ec93a4eb747481fc89dc.png)   ![Refer to caption](img/c97ed41a0bc547fdf53f4e106e8ab3b9.png)   ![Refer
    to caption](img/bd6c375f038ed33b84aa511659d50e74.png)  ![Refer to caption](img/8cd159524230edebbe6891583f36ad77.png)    ![Refer
    to caption](img/8606fde4f8ed56af2f29dc2e02d193da.png)   ![Refer to caption](img/7421a1e172f8bd086d19b1118b5db1bc.png)   ![Refer
    to caption](img/f1d2673b209f002f9af67922b3573b7a.png)   ![Refer to caption](img/0d5b6d397aaac15066c143fec62eead2.png)  ![Refer
    to caption](img/5a9b6aae5e2d3ef8731c7f15300e6f1b.png)    ![Refer to caption](img/028083df6960fbd85d02f528220602a2.png)   ![Refer
    to caption](img/ee95436fa06505b437f3adb1cfab1cac.png)   ![Refer to caption](img/f4e9a4f8148876421d2ef7f7218c2a11.png)   ![Refer
    to caption](img/a8e22a0ceb404793f7b6e5ecd672059b.png)  ![Refer to caption](img/a1193a1e27266beba5996b774f9de298.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Examples of natural images where our attack method achieves an acceptable
    approximation of the original model with only 1-2 rounds of refinement.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To justify the practicality of our attack, we provide an estimation of its
    associated costs. These are mainly divided into two main components: the cost
    of using GPT-4V and the cost associated with the Text-to-Image API. We detail
    each as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: GPT-4V Cost.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: According to OpenAI’s pricing, there is a charge per million tokens for both
    input and output. To estimate the cost for using GPT-4V, we consider the number
    of input and output tokens required per round of our attack. Based on our analysis,
    the average number of input tokens is 900 for the initial prompt and 1165 for
    each subsequent round. The average number of rounds to achieve maximum similarity
    is approximately 2.7, leading to a total of 4395 input tokens per test sample.
    It’s noteworthy that the examples from natural images shown in Figure [6](#S7.F6
    "Figure 6 ‣ 7 Cost Estimation ‣ Iteratively Prompting Multimodal LLMs to Reproduce
    Natural and AI-Generated Images") demonstrate that even with 1-2 rounds, we can
    achieve an approximation of the targeted image. For output tokens, the average
    is 381 per round. Given the current rates on OpenAI’s website, the total cost
    for using GPT-4V, considering both input and output, is approximately $0.09\.
    Additionally, a small fee of approximately $0.03 applies for including images
    in queries, based on the average number of rounds.
  prefs: []
  type: TYPE_NORMAL
- en: Text-to-Image API Cost.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our experiments utilize either Midjourney or DALL-E 3, so we calculate the cost
    for each separately. Based on the rates from Midjourney’s and OpenAI’s websites,
    the cost per image generation is about $0.03 for Midjourney and $0.04 for DALL-E
    3 (for 1024*1024 resolution images). Consequently, the total generation cost using
    Midjourney is approximately $0.12, and for DALL-E 3, it’s around $0.16.
  prefs: []
  type: TYPE_NORMAL
- en: Overall Cost.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The total cost per sample depends on the text-to-image API used. If the attacker
    utilizes Midjourney, the cost is approximately $0.235 per sample, and for DALL-E
    3, it is around $0.275\. These costs are substantially lower than the typical
    prices in AI-generated image marketplaces (ranging from $3 to $7) and significantly
    below the cost of stocks of real images (which can be $50 to $500).
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research has unveiled a novel attack strategy in the realm of digital imagery,
    specifically targeting AI-generated image marketplaces and premium stock image
    providers. By effectively employing state-of-the-art multi-modal models, our method
    has demonstrated the ability to generate visually comparable content at a significantly
    reduced cost, challenging the current economic dynamics of the digital imagery
    landscape. Moreover, the compilation of a large-scale dataset from Midjourney
    is a pivotal contribution to this field. This dataset not only aids our research
    but also serves as a valuable resource for future studies, offering insights into
    the capabilities and challenges of text-to-image technologies. In conclusion,
    our study highlights new threats against digital imagery, stressing the need for
    urgent action in identifying and countering these risks. Future research should
    aim at developing protective measures to preserve digital image integrity amidst
    advancing AI technologies.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Betker et al. (2023) James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng
    Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving
    image generation with better captions. *Computer Science. https://cdn. openai.
    com/papers/dall-e-3\. pdf*, 2(3):8, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2023) Tingfeng Cao, Chengyu Wang, Bingyan Liu, Ziheng Wu, Jinhui
    Zhu, and Jun Huang. Beautifulprompt: Towards automatic prompt engineering for
    text-to-image synthesis. *arXiv preprint arXiv:2311.06752*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni,
    Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa,
    Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.
    *arXiv preprint arXiv:2209.06794*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2020) Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi,
    and Aniruddha Kembhavi. X-lxmert: Paint, caption and answer questions with multi-modal
    transformers. *arXiv preprint arXiv:2009.11278*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou,
    Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering
    text-to-image generation via transformers. *Advances in Neural Information Processing
    Systems*, 34:19822–19835, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *Advances in neural information processing systems*, 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning.
    *arXiv preprint arXiv:2104.08718*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilharco et al. (2021) Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
    Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok
    Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip,
    July 2021. URL [https://doi.org/10.5281/zenodo.5143773](https://doi.org/10.5281/zenodo.5143773).
    If you use this software, please cite it as below.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022a) Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
    Bootstrapping language-image pre-training for unified vision-language understanding
    and generation. In *International conference on machine learning*, pp.  12888–12900\.
    PMLR, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2:
    Bootstrapping language-image pre-training with frozen image encoders and large
    language models. In *International conference on machine learning*, pp.  19730–19742\.
    PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) Zhiheng Li, Martin Renqiang Min, Kai Li, and Chenliang Xu.
    Stylet2i: Toward compositional and high-fidelity text-to-image synthesis. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  18197–18207,
    2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mansimov et al. (2015) Elman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan
    Salakhutdinov. Generating images from captions with attention. *arXiv preprint
    arXiv:1511.02793*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oppenlaender (2023) Jonas Oppenlaender. A taxonomy of prompt modifiers for text-to-image
    generation. *Behaviour & Information Technology*, pp.  1–14, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pp.  8748–8763\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image
    generation. In *International conference on machine learning*, pp.  8821–8831\.
    Pmlr, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed et al. (2016) Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,
    Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis.
    In *International conference on machine learning*, pp.  1060–1069\. PMLR, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2021) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion
    models, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion
    models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, pp.  10684–10695, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,
    Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language
    understanding. *Advances in neural information processing systems*, 35:36479–36494,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schuhmann et al. (2022) Christoph Schuhmann, Romain Beaumont, Richard Vencu,
    Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton
    Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training
    next generation image-text models. *Advances in Neural Information Processing
    Systems*, 35:25278–25294, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sha & Zhang (2024) Zeyang Sha and Yang Zhang. Prompt stealing attacks against
    large language models. *arXiv preprint arXiv:2402.12959*, 2024.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2023) Xinyue Shen, Yiting Qu, Michael Backes, and Yang Zhang. Prompt
    stealing attacks against text-to-image generation models. *arXiv preprint arXiv:2302.09923*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,
    and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics.
    In *International conference on machine learning*, pp.  2256–2265\. PMLR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin
    Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A generative image-to-text
    transformer for vision and language. *arXiv preprint arXiv:2205.14100*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation
    with attentional generative adversarial networks. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pp.  1316–1324, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2022) Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid,
    Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al.
    Scaling autoregressive models for content-rich text-to-image generation. *arXiv
    preprint arXiv:2206.10789*, 2(3):5, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. In *Proceedings
    of the IEEE international conference on computer vision*, pp.  5907–5915, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang & Ippolito (2023) Yiming Zhang and Daphne Ippolito. Effective prompt extraction
    from language models. 2023. URL [https://api.semanticscholar.org/CorpusID:259847681](https://api.semanticscholar.org/CorpusID:259847681).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2019) Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. Dm-gan: Dynamic
    memory generative adversarial networks for text-to-image synthesis. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp.  5802–5810,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Data Collection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss our approach to gathering data from Midjourney.
    First, we provide a brief overview of Midjourney and its functionality. Next,
    we delve into the specific steps of our data collection process. Finally, we describe
    the characteristics of the data we collected from Midjourney. This offers readers
    a clear picture of the foundation of our research.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dff51224ca22cc6ded108c5959b5c3cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The distribution of the number of tokens in prompts from Midjourney
    samples.'
  prefs: []
  type: TYPE_NORMAL
- en: A.1 Introduction to Midjourney
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Midjourney is a powerful black-box text-to-image API that generates images from
    natural language descriptions, commonly referred to as “prompts.” It operates
    in a manner analogous to other AI tools like OpenAI’s DALL·E and Stability AI’s
    Stable Diffusion. Midjourney has consistently advanced its algorithms, launching
    new model versions periodically. The algorithm’s Version 2 was introduced in April
    2022, followed by Version 3 in July 2022\. Alpha iterations of Versions 4 and
    5 were released in November 2022 and March 2023, respectively. The enhancements
    in Version 5.1 leaned towards distinct image stylization, whereas its RAW variant
    optimized interpretations for clearer prompts. The subsequent Version 5.2 further
    refined the output image quality.
  prefs: []
  type: TYPE_NORMAL
- en: Midjourney operates exclusively via a Discord bot present on their official
    server. Users can engage with this bot directly or have it on a different server.
    To produce images, the /imagine command is employed along with a desired prompt,
    resulting in a set of four generated images. Users then have the option to upscale
    their preferred images.
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Data Crawling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Midjourney server offers a variety of channels designed for collaborative
    work, including dedicated rooms for beginners, themed generations, and general
    discussion areas. Both prompts and the corresponding generated images are posted
    within these channels. We target the general channels, which contain a broader
    range of generations compared to theme-specific channels. In our approach, we
    target 10 such general channels. It has to be noted that for our data collection,
    we utilize DiscordChatExporter,^(10)^(10)10https://github.com/Tyrrrz/DiscordChatExporter
    a tool adept at extracting chat histories from Discord channels. This tool facilitates
    the extraction of chat records from Discord, enabling us to handle direct and
    group messages, export from various channel types, and retain rich media and specific
    Discord formatting nuances.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Data Characteristics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our data collection process, we successfully crawled over 20 million samples.
    These encompass duplicated captions associated with distinct generated images.
    Among these, more than 12 million samples have unique captions. The collected
    samples can be categorized into two main groups. The first group consists of samples
    displaying a grid of 4 generated images. The second group presents samples with
    a single upscaled image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The collected raw data include several features: AuthorID, Author, Date, Content,
    Attachment, and Reactions. Both AuthorID and Author remain consistent for all
    samples, pointing to Midjourney’s BOT. The Date field contains timestamps in the
    Unix timestamp (or Epoch time) format, which counts the number of seconds that
    have elapsed since January 1, 1970 (UTC). An example of such a timestamp is 1681516818.
    Attachment denotes the URL of images corresponding to each caption. Lastly, Reactions
    pertains to the emoji reactions associated with the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the Content feature requires a more detailed description. The Content
    comprises the following items:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'URL (Optional): Users may embed the URL of an image in their prompt, particularly
    if they wish to modify or obtain different variations of an image.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Main Prompt: The primary textual prompt provided by the user.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'User’s ID: The identifier of the user who submitted the prompt.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Type: Specifies whether the image is a high-resolution upscaled version or
    a grid containing lower resolution variations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Parameters: Parameters are options added to a prompt that change how an image
    is generated. They can alter aspects such as the image’s aspect ratios, switch
    between Midjourney model versions, and much more. Parameters are always appended
    to the end of a prompt, following a double dash (‘–‘). Multiple parameters can
    be included in each prompt. Detailed documentation of each parameter is available
    on Midjourney’s website.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.4 Data Pre-processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We implement various data processing steps on this expansive dataset. Some
    key steps include:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extracting the original prompt from the Content feature.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing empty rows or rows with NaN values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Categorizing samples based on whether their prompts contain an image URL. For
    our method, we exclude these samples, ensuring none are included in our training
    data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Converting the Unix timestamp in the Date feature into a more standard date
    format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determining the version of the model using the Date feature and the known release
    dates of each version.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Excluding rows where prompts contain emojis or non-English words.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Lastly, from our processed dataset, we select approximately 4.8 million samples
    with upscaled images. We particularly choose prompts with 77 tokens or fewer,
    considering the constraint of the CLIP model which only accepts text inputs of
    up to 77 tokens. The distribution of token counts in these prompts is illustrated
    in Figure [7](#A1.F7 "Figure 7 ‣ Appendix A Data Collection ‣ Iteratively Prompting
    Multimodal LLMs to Reproduce Natural and AI-Generated Images").
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Collecting Modifiers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To extract modifiers from our large-scale dataset, we analyze the common format
    of Midjourney’s prompts. Upon reviewing numerous samples, we observe that prompts
    often contain a general description followed by modifiers, typically separated
    by commas. While this pattern is not always consistent, it is prevalent enough
    in a large dataset to yield a significant set of frequent modifiers. After examining
    millions of samples, we identify approximately 1800 modifiers, each occurring
    more than 1000 times in the dataset. Table [4](#A1.T4 "Table 4 ‣ A.5 Collecting
    Modifiers ‣ Appendix A Data Collection ‣ Iteratively Prompting Multimodal LLMs
    to Reproduce Natural and AI-Generated Images") presents some of the most frequent
    modifiers along with their corresponding frequencies.
  prefs: []
  type: TYPE_NORMAL
- en: '| Modifier | Frequency |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 8k | 0.0882 |'
  prefs: []
  type: TYPE_TB
- en: '| octane render | 0.0453 |'
  prefs: []
  type: TYPE_TB
- en: '| photorealistic | 0.0442 |'
  prefs: []
  type: TYPE_TB
- en: '| cinematic | 0.044 |'
  prefs: []
  type: TYPE_TB
- en: '| realistic | 0.0329 |'
  prefs: []
  type: TYPE_TB
- en: '| 4k | 0.0324 |'
  prefs: []
  type: TYPE_TB
- en: '| highly detailed | 0.0297 |'
  prefs: []
  type: TYPE_TB
- en: '| cinematic lighting | 0.0266 |'
  prefs: []
  type: TYPE_TB
- en: '| hyper realistic | 0.0257 |'
  prefs: []
  type: TYPE_TB
- en: '| unreal engine | 0.0249 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: The 10 most frequent modifiers extracted from the Midjourney dataset,
    with frequencies indicating the percentage of prompts containing each modifier
    relative to the total number of samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Human Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the human evaluation component of our study, we adopt a methodology similar
    to that done by Li et al. ([2022b](#bib.bib11)). As outlined in the experimental
    section, we randomly select five samples from each of the seven settings, amounting
    to a total of 35 samples. These are then presented to five annotators, who are
    instructed to score each image using a 5-point Likert scale. The user interface
    for this task is a Google Form consisting of 35 multiple-choice grid questions,
    one for each sample.
  prefs: []
  type: TYPE_NORMAL
- en: 'In each question, annotators are presented with four images: the original image
    at the top, and three other images below it, generated by our attack and the baselines,
    shuffled in no specific order. The task requires the annotators to assign a similarity
    score ranging from 1 (not similar at all) to 5 (very similar) for each of the
    three shuffled images, based on its resemblance to the original image. Detailed
    instructions and scoring guidelines are provided to the annotators to ensure a
    consistent and accurate evaluation process. Here are the instructions and scoring
    guide:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instructions to Annotators:'
  prefs: []
  type: TYPE_NORMAL
- en: ”In this task, you will be shown a set of images. For each set, the top image
    is the original, which may be a grid of four images showing different variants
    of a single generation. Below the original are three generated images. Your role
    is to assess how similar each of the three generated images is to the original
    image (or its variants) and give a similarity score from 1 to 5\. Score 1 for
    ’Completely Different’ if there are no discernible similarities, and score 5 for
    ’Very Similar’ if the generated image nearly identically matches the original
    in content, style, and details. Use the intermediate scores to indicate varying
    degrees of similarity.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Scoring Guide:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Completely Different: No discernible similarities exist between the generated
    image and the original, presenting entirely distinct content and style.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Barely Similar: The original and generated images may share some elements or
    themes, but they significantly differ in both content and style.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Somewhat Similar: The generated image bears a resemblance to the original in
    terms of content or subject matter, despite noticeable differences in style.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Closely Similar: Minor variations are present, but the generated image generally
    mirrors the original in content and subject matter.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Very Similar: The generated image closely resembles the original in content,
    style, and fine details, indicating a strong likeness.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix C Failure Cases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Figure [8](#A3.F8 "Figure 8 ‣ Appendix C Failure Cases ‣ Iteratively Prompting
    Multimodal LLMs to Reproduce Natural and AI-Generated Images"), we present three
    examples from setting 1 that receive the lowest image similarity scores, highlighting
    specific instances where our method does not perform as expected.
  prefs: []
  type: TYPE_NORMAL
- en: In the first failure case, our method successfully identifies several elements
    of the image, including the Asian woman, the expression, and the water pouring
    over her. However, the images are not deemed highly similar. A potential reason
    for this discrepancy is the word ”Awkwafina”. Although our named entity extractor
    feeds this keyword alongside other related keywords to GPT-4V, the model fails
    to select it as a related element of the prompt, leading to a less accurate representation
    in the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second failure case also exhibits a mismatch. While our method extracts
    many elements and features from the image, the degree of similarity is not substantial.
    The original prompt contains special names that our method does not extract. Furthermore,
    Midjourney fails to accurately render images of these characters, instead generating
    unrelated figures. This outcome underscores a dual failure: both our named entity
    extractor and the text-to-image model do not fulfill their intended functions
    effectively.'
  prefs: []
  type: TYPE_NORMAL
- en: In the final example, the simplicity of the original prompt grants Midjourney
    significant freedom in varying most aspects of the image, complicating our method’s
    ability to generate a singular prompt that accurately represents all images simultaneously.
    This issue arises because we use a completely random sample from Midjourney’s
    outputs to construct our test dataset, resulting in some instances with less effective
    prompts.
  prefs: []
  type: TYPE_NORMAL
- en: These cases underline the complex challenges in prompt-based image generation,
    where both the extraction of relevant information and the generative capabilities
    of the model play critical roles in achieving accurate results.
  prefs: []
  type: TYPE_NORMAL
- en: Original Prompt Original Image Generated Prompt Generated Image Gallons of water
    are pouring down on Awkwafina’s head, soaking wet, full body –v 5 –q 2 ![Refer
    to caption](img/50e7923704ce424b08c0c38519b31409.png)   An 8k high-definition,
    realistic, detailed photo-rendered portrait of Constance Wu from the waist up,
    caught off guard and visibly reacting to being suddenly soaked in a torrential
    downpour during daytime …  ![Refer to caption](img/e83fdd3ecb778eab08bd0d57c2b379ce.png)    A
    group photo of Alex Silva, Jesse Chevan, Justin Gonçalves, and Charlie Hack; 30
    years old, portrait, editorial photograph, 8k, weird and odd –s 750 –v 5   ![Refer
    to caption](img/2feae1f9f91ce42d7358bd32d382e76f.png)   Early 1970s Kodak Portrait
    style portrait of a group of nerdy young men with curly hair and vintage round
    glasses, dressed in muted, natural-colored academic attire, photographed against
    a simple backdrop with even, …  ![Refer to caption](img/79ac08b82d0c55c98bb72ffbacd8a72e.png)    adult
    kids with black hairs talking real image   ![Refer to caption](img/6a3b9431e130111ead06066ba05431c5.png)   Premium
    8k resolution portrait conveying a nuanced, refined family dynamic, with subtle
    punk fashion influences and minimal tattoos. The atmosphere should be moody yet
    sophisticated, with attire that is stylish and understated, …  ![Refer to caption](img/5962539ad1a357a43e8b29315bef8155.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Three examples of failed cases with the lowest image similarity from
    setting 1, showcasing the original and generated prompts alongside their corresponding
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt template to start the cycle |'
  prefs: []
  type: TYPE_TB
- en: '| Task: You are tasked with analyzing a grid of four images generated by a
    text-to-image model, keeping in mind that they represent varied interpretations
    from a single prompt, rather than a series of similar objects or scenes. The objective
    is to deduce the original prompt used to create these images. Your analysis should
    focus on identifying common themes, elements, or characteristics present across
    all four images. The challenge is to construct a descriptive prompt that encapsulates
    these observations. Here are some instructions to generate the prompt: • Be within
    15-50 tokens in length, as per the CLIP tokenizer. • Encompass all common features
    visible in the images. • Examine the provided list of modifiers. Select and include
    those you deem relevant and which meaningfully contribute to capturing the essence
    of the images: …list of modifiers here… • Prioritize the integration of names
    from the provided list into your analysis of the image grid, especially when they
    align with the context or themes depicted. When creating the prompt, if you recognize
    a famous individual in the image and their name is included in the special names
    list I’ve provided, please use that name directly in the prompt. Avoid using phrases
    like ’a character resembling someone’ and instead use the actual name from the
    list, provided it aligns with the person identified in the image: …list of named
    entities here… • Choose general words from the list that are relevant to the images,
    reflecting their themes and atmosphere. After ensuring relevance, use these words
    to enhance the prompt’s descriptiveness: …list of general words here… • Avoid
    phrases like ’Create a series of’ or using plural terms that might lead the image
    generation API to produce multiple versions in each variation. • Typically, follow
    a structure starting with a general description, followed by specific features
    and modifiers, separated by commas: ”Dark street Tokyo environment Traditional
    Japanese illustration of a Funky Musician. With bird mask traditional Japanese
    elements and neo electro string instrument big japanese graffiti, Crisp contemporary
    illustrations, bold lines, vibrant, metallic, moving, geometric, expressive” |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The initial prompt template provided to GPT-4V to begin the iterative
    refinement cycle. The sections highlighted in red represent the information obtained
    from the fine-tuned CLIP model and the multi-label classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt template for refining the prompts in the cycle |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Task: You are tasked with analyzing a grid of four images generated by a
    text-to-image model (The first grid). Keeping in mind that they represent varied
    interpretations from a single prompt, rather than a series of similar objects
    or scenes. The objective is to deduce the original prompt used to create these
    images. Your analysis should focus on identifying common themes, elements, or
    characteristics present across all four images. The challenge is to construct
    a descriptive prompt that encapsulates these observations. |'
  prefs: []
  type: TYPE_TB
- en: '| Instructions: Here are some instructions to generate the prompt: • Be within
    15-50 tokens in length, as per the CLIP tokenizer. • Encompass all common features
    visible in the images. • Examine the provided list of modifiers. Select and include
    those you deem relevant and which meaningfully contribute to capturing the essence
    of the images: …list of modifiers here… • Prioritize the integration of names
    from the provided list into your analysis of the image grid, especially when they
    align with the context or themes depicted. When creating the prompt, if you recognize
    a famous individual in the image and their name is included in the special names
    list I’ve provided, please use that name directly in the prompt. Avoid using phrases
    like ’a character resembling someone’ and instead use the actual name from the
    list, provided it aligns with the person identified in the image: …list of named
    entities here… • Choose general words from the list that are relevant to the images,
    reflecting their themes and atmosphere. After ensuring relevance, use these words
    to enhance the prompt’s descriptiveness: …list of general words here… • Avoid
    phrases like ’Create a series of’ or using plural terms that might lead the image
    generation API to produce multiple versions in each variation. • Typically, follow
    a structure starting with a general description, followed by specific features
    and modifiers, separated by commas: ”Dark street Tokyo environment Traditional
    Japanese illustration of a Funky Musician. With bird mask traditional Japanese
    elements and neo electro string instrument big japanese graffiti, Crisp contemporary
    illustrations, bold lines, vibrant, metallic, moving, geometric, expressive” Your
    generated prompt given the information above: …generated prompt from the previous
    round… Consider the second grid of images that were generated using the prompt
    I obtained from you above. Compare these images with the original image I first
    provided (the first grid of images). Your task now is to refine your prompt to
    achieve a closer resemblance to the original image. Identify areas where the generated
    images differ from the original, focusing on style, subject matter, themes, and
    other key elements. Utilize all provided information above which are related to
    the images, including different artistic styles, names of entities, or other relevant
    elements, to make the necessary adjustments. The objective is to modify your previous
    prompt in such a way that it results in images more accurately reflecting the
    original in appearance, atmosphere, and concept. Suggest specific changes, whether
    it involves a different artistic style, more accurate incorporation of certain
    entities or names, or thematic adjustments. The goal is for the improved prompt
    to bridge the gap between the generated images and the original image. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: The prompt provided to GPT-4V to refine the prompt at each round based
    on the generated image in the previous round.'
  prefs: []
  type: TYPE_NORMAL
