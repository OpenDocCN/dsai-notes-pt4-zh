- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:40:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:40:51'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GANPrompt: Enhancing Robustness in LLM-Based Recommendations with GAN-Enhanced
    Diversity Prompts'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GANPrompt：通过GAN增强的多样性提示提升基于LLM的推荐系统的鲁棒性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.09671](https://ar5iv.labs.arxiv.org/html/2408.09671)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.09671](https://ar5iv.labs.arxiv.org/html/2408.09671)
- en: Xinyu Li College of Management and Economics, Tianjin UniversityTianjinChina
    [lxyt7642@tju.edu.cn](mailto:lxyt7642@tju.edu.cn) ,  Chuang Zhao Department of
    Electronic and Computer Engineering, The Hong Kong University of Science and TechnologyHong
    KongChina [czhaobo@connect.ust.hk](mailto:czhaobo@connect.ust.hk) ,  Hongke Zhao
    College of Management and Economics, Tianjin UniversityTianjinChina [hongke@tju.edu.cn](mailto:hongke@tju.edu.cn)
    ,  Likang Wu College of Management and Economics, Tianjin UniversityTianjinChina
    [wulk@mail.ustc.edu.cn](mailto:wulk@mail.ustc.edu.cn)  and  Ming HE AI Lab at
    Lenovo ResearchBeijingChina [heming01@foxmail.com](mailto:heming01@foxmail.com)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xinyu Li 天津大学管理与经济学院 天津 中国 [lxyt7642@tju.edu.cn](mailto:lxyt7642@tju.edu.cn)
    ,  Chuang Zhao 香港科技大学电子与计算机工程系 香港 中国 [czhaobo@connect.ust.hk](mailto:czhaobo@connect.ust.hk)
    ,  Hongke Zhao 天津大学管理与经济学院 天津 中国 [hongke@tju.edu.cn](mailto:hongke@tju.edu.cn)
    ,  Likang Wu 天津大学管理与经济学院 天津 中国 [wulk@mail.ustc.edu.cn](mailto:wulk@mail.ustc.edu.cn)  和  Ming
    HE 联想研究院AI实验室 北京 中国 [heming01@foxmail.com](mailto:heming01@foxmail.com)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In recent years, LLM has demonstrated remarkable proficiency in comprehending
    and generating natural language, with a growing prevalence in the domain of recommender
    systems. However, LLM continues to face a significant challenge in that it is
    highly susceptible to the influence of prompt words. This inconsistency in response
    to minor alterations in prompt input may compromise the accuracy and resilience
    of recommendation models. To address this issue, this paper proposes GANPrompt,
    a multi-dimensional large language model prompt diversity framework based on Generative
    Adversarial Networks (GANs). The framework enhances the model’s adaptability and
    stability to diverse prompts by integrating GAN generation techniques with the
    deep semantic understanding capabilities of LLMs. GANPrompt first trains a generator
    capable of producing diverse prompts by analysing multidimensional user behavioural
    data. These diverse prompts are then used to train the LLM to improve its performance
    in the face of unseen prompts. Furthermore, to ensure a high degree of diversity
    and relevance of the prompts, this study introduces a mathematical theory-based
    diversity constraint mechanism that optimises the generated prompts to ensure
    that they are not only superficially distinct, but also semantically cover a wide
    range of user intentions. Through extensive experiments on multiple datasets,
    we demonstrate the effectiveness of the proposed framework, especially in improving
    the adaptability and robustness of recommender systems in complex and dynamic
    environments. The experimental results demonstrate that GANPrompt yields substantial
    enhancements in accuracy and robustness relative to existing state-of-the-art
    methodologies.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，LLM在理解和生成自然语言方面表现出显著的能力，并在推荐系统领域中越来越普及。然而，LLM仍然面临一个重大挑战，即对提示词的影响极为敏感。这种对提示输入微小变化的不一致反应可能会影响推荐模型的准确性和鲁棒性。为了解决这个问题，本文提出了GANPrompt，一种基于生成对抗网络（GAN）的多维大语言模型提示多样性框架。该框架通过将GAN生成技术与LLM的深层语义理解能力结合，增强了模型对多样提示的适应性和稳定性。GANPrompt首先训练一个生成器，该生成器能够通过分析多维用户行为数据生成多样的提示。这些多样的提示随后用于训练LLM，以提高其在面对未见提示时的表现。此外，为了确保提示的高度多样性和相关性，本研究引入了一种基于数学理论的多样性约束机制，该机制优化生成的提示，确保它们不仅在表面上有所不同，而且在语义上涵盖了广泛的用户意图。通过在多个数据集上进行广泛实验，我们展示了所提出框架的有效性，尤其是在复杂和动态环境中提高推荐系统的适应性和鲁棒性。实验结果表明，GANPrompt相较于现有的最先进方法，在准确性和鲁棒性方面有了显著提升。
- en: 'Recommendation Systems, Large Language Model, Generating Adversarial Networks,
    Prompt Learning^†^†ccs: Information systems Recommender systems'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '推荐系统、大语言模型、生成对抗网络、提示学习^†^†ccs: 信息系统 推荐系统'
- en: 1\. INTRODUCTION
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Since the advent of the Internet, recommender systems have become an indispensable
    component of the digital age. From their inception in the early days of the Internet,
    recommender systems have undergone a significant evolution, progressing from simple
    recommendations based on collaborative filtering to the complex systems of today,
    which incorporate machine learning, artificial intelligence and big data technologies.
    This development can be attributed to the mounting pressure on information overload
    and the relentless pursuit of a personalised user experience. From the initial
    deployment of recommender systems for product recommendations to the current integration
    of these systems into a multitude of domains, including music, video, social networks,
    and others, the impact of recommender systems on our lives has been profound.
    These systems have facilitated the delivery of more personalised and accurate
    services and experiences to users.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自互联网出现以来，推荐系统已成为数字时代不可或缺的组成部分。自互联网早期推荐系统的诞生起，它们经历了显著的演变，从基于协同过滤的简单推荐发展到今天复杂的系统，这些系统结合了机器学习、人工智能和大数据技术。这一发展归因于信息过载的压力增加以及对个性化用户体验的持续追求。从最初的产品推荐系统到如今这些系统集成到音乐、视频、社交网络等多个领域，推荐系统对我们生活的影响深远。这些系统促进了向用户提供更个性化和准确的服务与体验。
- en: The rapid development of artificial intelligence has led to the emergence of
    large language models, which have the potential to revolutionise the field of
    recommender systems. These models, such as OpenAI’s GPT series and Google’s BERT,
    are trained on vast quantities of text data using deep learning techniques, enabling
    them to understand and generate natural language text with exceptional language
    understanding and generation capabilities. The advent of such models offers a
    novel avenue for recommender systems, enabling recommendations to transcend the
    limitations of simple behavioural patterns and instead leverage richer, more nuanced
    semantic information.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能的快速发展导致了大型语言模型的出现，这些模型有可能彻底改变推荐系统领域。这些模型，如OpenAI的GPT系列和Google的BERT，通过深度学习技术在大量文本数据上进行训练，使它们能够以卓越的语言理解和生成能力来理解和生成自然语言文本。这些模型的出现为推荐系统提供了新颖的途径，使推荐能够超越简单行为模式的限制，利用更丰富、更细致的语义信息。
- en: 'The application of language models in recommender systems is focused on two
    paradigms: pre-training and prompt-based fine-tuning. The pre-training paradigm
    relies on training large-scale language models (LLMs) on large-scale textual data
    to enable them to capture linguistic nuances, patterns, and structures. However,
    this approach tends to require huge data samples and expensive computational resources,
    which is an unaffordable requirement for most recommender system application scenarios.
    In contrast, the prompt-based fine-tuning paradigm fine-tunes pre-trained LLMs
    by constructing prompt datasets related to specific recommendation tasks. During
    this process, deep semantic understanding and accurate matching of users and items
    are achieved by adjusting model parameters. This approach addresses the discrepancy
    between the training task and the downstream goal by redefining the downstream
    task to fit the pre-trained framework. In comparison to pre-training language
    models from scratch, the prompt-based fine-tuning approach significantly reduces
    the data requirements and computational resources, thus enabling the rapid and
    efficient integration of advanced LLM techniques into existing recommender system
    architectures. This approach has become a mainstream research direction in both
    academic and industrial contexts.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型在推荐系统中的应用集中于两种范式：预训练和基于提示的微调。预训练范式依赖于在大规模文本数据上训练大规模语言模型（LLMs），使其能够捕捉语言细微差别、模式和结构。然而，这种方法通常需要庞大的数据样本和昂贵的计算资源，对于大多数推荐系统应用场景而言，这是一项难以承担的要求。相比之下，基于提示的微调范式通过构建与特定推荐任务相关的提示数据集来微调预训练的LLMs。在此过程中，通过调整模型参数实现对用户和项目的深层语义理解和准确匹配。这种方法通过重新定义下游任务以适应预训练框架，解决了训练任务和下游目标之间的差异。与从零开始预训练语言模型相比，基于提示的微调方法显著减少了数据需求和计算资源，从而实现了先进LLM技术的快速高效集成。这种方法已成为学术界和工业界的主流研究方向。
- en: '![Refer to caption](img/d7ce4b20a47ee9e1df2f6955c29f5c6c.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d7ce4b20a47ee9e1df2f6955c29f5c6c.png)'
- en: Figure 1. The same user interaction history combined with different prompt templates
    gives different recommendation results in LLM. This is due to the fact that LLM
    is transiently sensitive to the diversity of prompt words, which can disrupt the
    model’s output results.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 相同的用户交互历史与不同的提示模板组合会在LLM中产生不同的推荐结果。这是因为LLM对提示词的多样性敏感，这可能会干扰模型的输出结果。
- en: 'The prompt-based approach directly adjusts the inputs of the LLM by constructing
    specific prompts (prompts) to align with specific recommendation tasks. The core
    of this approach lies in guiding the model to generate outputs that are closely
    related to the recommendation tasks through well-designed prompt statements. Although
    this approach facilitates the rapid deployment of recommender systems, it also
    suffers from over-sensitivity of the model to prompts, which may disturb the model
    outputs and thus affect the robustness of the model [1](#S1.F1 "Figure 1 ‣ 1\.
    INTRODUCTION ‣ GANPrompt: Enhancing Robustness in LLM-Based Recommendations with
    GAN-Enhanced Diversity Prompts"). The choice of prompt words has a direct impact
    on the recommendations generated by LLM, if the prompts are poorly designed or
    inconsistent with the context in which the model is trained, it may result in
    the model outputting recommendations that are highly biased, inaccurate, or even
    completely irrelevant. Even subtle differences in the prompts can significantly
    affect the model’s performance. Furthermore, an over-sensitivity to specific prompts
    may render the model susceptible to malicious manipulation. This can be achieved
    through the disruption of the recommendation results through adversarial prompts,
    which raises security and fairness issues.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示的方法通过构造特定的提示（prompts）直接调整LLM的输入，以使其与特定的推荐任务对齐。该方法的核心在于通过精心设计的提示语引导模型生成与推荐任务紧密相关的输出。虽然这种方法便于推荐系统的快速部署，但它也存在模型对提示过于敏感的问题，这可能会扰乱模型输出，从而影响模型的稳健性[1](#S1.F1
    "图1 ‣ 1\. 引言 ‣ GANPrompt：通过GAN增强多样性提示提升LLM推荐系统的鲁棒性")。提示词的选择对LLM生成的推荐结果有直接影响，如果提示设计不佳或与模型训练时的上下文不一致，可能会导致模型输出的推荐高度偏颇、不准确，甚至完全无关。即使是提示中的微小差异也会显著影响模型的表现。此外，对特定提示的过度敏感性可能使模型容易受到恶意操控。这可以通过对抗性提示来扰乱推荐结果，从而引发安全性和公平性问题。
- en: To address the above problems, we propose GANPrompt, a generative adversarial
    network (GAN)-based framework to generate diverse prompts for multidimensional
    LLMs, enhancing the stability and robustness of recommendation systems in complex
    environments. The framework combines the semantic understanding capability of
    large language models with the generative properties of generative adversarial
    networks to improve the adaptability and robustness of downstream recommendation
    tasks to diverse prompts. The framework initiates by training a diversity prompt
    generator, which collects user behaviour data from multidimensional data sources.
    The generator exploits the ability of GANs to create diverse and highly differentiated
    prompt samples, which helps to simulate complex and varied user queries in the
    real world. These generated diversity prompts are then used to train large language
    models, thereby improving the stability and accuracy of the models in the face
    of different, unseen prompts. Furthermore, to ensure that the generated prompts
    are both highly diverse and relevant, the framework introduces a diversity constraint
    mechanism based on mathematical theory. This mechanism adapts the GAN-generated
    prompts through an optimisation algorithm so that each prompt generated is not
    only superficially distinct, but also semantically encompasses a wide range of
    user intentions. This approach effectively extends and improves the model, ultimately
    leading to a more powerful and effective recommender system.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述问题，我们提出了GANPrompt，一个基于生成对抗网络（GAN）的框架，用于生成多维LLM的多样化提示，提高了推荐系统在复杂环境中的稳定性和鲁棒性。该框架将大型语言模型的语义理解能力与生成对抗网络的生成特性相结合，以提高下游推荐任务对多样化提示的适应性和鲁棒性。框架首先通过训练多样性提示生成器来启动，该生成器从多维数据源中收集用户行为数据。生成器利用GAN的能力创建多样化和高度差异化的提示样本，有助于模拟现实世界中的复杂和多变的用户查询。这些生成的多样性提示随后用于训练大型语言模型，从而提高模型面对不同、未见过的提示时的稳定性和准确性。此外，为了确保生成的提示既具有高度的多样性，又具有相关性，该框架引入了基于数学理论的多样性约束机制。该机制通过优化算法调整GAN生成的提示，使每个生成的提示不仅在表面上不同，而且在语义上涵盖广泛的用户意图。这种方法有效地扩展和改进了模型，*最终*形成了一个更强大和有效的推荐系统。
- en: 'Specifically, the contributions of this paper are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，本文的贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'This paper realises the deep integration of LLM and GAN: by using the encoder
    of LLM as a generator and constructing the corresponding discriminator to implement
    the generative adversarial network, we innovatively use the semantic parsing ability
    of LLM and the generative characteristics of GAN to construct a multi-dimensional
    prompt generator. This combination not only enhances the diversity and quality
    of generated prompts, but also improves the adaptability and accuracy of the model
    in complex recommendation scenarios.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文实现了LLM和GAN的深度融合：通过将LLM的编码器用作生成器，并构建相应的鉴别器来实现生成对抗网络，我们创新性地利用了LLM的语义解析能力和GAN的生成特性，构建了一个多维度提示生成器。这种组合不仅增强了生成提示的多样性和质量，还提高了模型在复杂推荐场景中的适应性和准确性。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Proposal of multidimensional diversity constraint: In order to ensure that
    the generated prompts are not only diverse but also semantically relevant, we
    propose a mathematical theory-based diversity constraint mechanism. This mechanism
    can effectively control the prompt generation process to ensure that each generated
    prompt covers a wide range of specific user intentions.'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多维度多样性约束的提出：为了确保生成的提示不仅具有多样性，还具有语义相关性，我们提出了一种基于数学理论的多样性约束机制。该机制可以有效地控制提示生成过程，确保每个生成的提示涵盖广泛的具体用户意图。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Extensive experimental validation: we validate the effectiveness of the proposed
    framework through experiments conducted on multiple datasets. The experimental
    results show that GANPrompt outperforms existing state-of-the-art techniques in
    several recommendation tasks, especially in demonstrating greater robustness and
    higher accuracy when dealing with dynamic and diverse inputs.'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 广泛的实验验证：我们通过在多个数据集上进行实验验证了所提框架的有效性。实验结果表明，GANPrompt在多个推荐任务中优于现有的最先进技术，尤其是在处理动态和多样化输入时表现出更强的鲁棒性和更高的准确性。
- en: 2\. RELATED WORK
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: In this paper, the related work involved includes the application of large language
    models in recommender systems, prompt learning based recommendation methods and
    generative adversarial networks.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 本文涉及的相关工作包括大型语言模型在推荐系统中的应用、基于提示学习的推荐方法以及生成对抗网络。
- en: 2.1\. Large Language Modelling as Recommendatiom System
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 大型语言模型作为推荐系统
- en: Large Language Models (LLMs) refer to Transformer language models containing
    hundreds of billions (or more) of parameters that are self-supervised trained
    on large amounts of textual data (Shanahan, [2024](#bib.bib21)), e.g., GPT-3(Brown
    et al., [2020](#bib.bib2)), PaLM(Chowdhery et al., [2023](#bib.bib3)), Galactica(Taylor
    et al., [2022](#bib.bib23)), and LLaMA(Touvron et al., [2023](#bib.bib24)). Due
    to its absorption of massive textual knowledge, LLM exhibits deep contextual semantic
    understanding and achieves impressive performance in solving complex task scenarios
    through text generation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是指包含数百亿（或更多）参数的Transformer语言模型，这些模型在大量文本数据上进行自监督训练（Shanahan，[2024](#bib.bib21)），例如，GPT-3（Brown
    et al., [2020](#bib.bib2)），PaLM（Chowdhery et al., [2023](#bib.bib3)），Galactica（Taylor
    et al., [2022](#bib.bib23)），和LLaMA（Touvron et al., [2023](#bib.bib24)）。由于其吸收了大量的文本知识，LLM展示了深刻的语境语义理解，并通过文本生成在解决复杂任务场景中取得了令人印象深刻的表现。
- en: With the significant achievements of Large Language Models (LLMs) in the field
    of Natural Language Processing (NLP), their use in recommender systems is becoming
    more widespread (Li et al., [2023c](#bib.bib16); Wu et al., [2023](#bib.bib28);
    Liu et al., [2023](#bib.bib18)). Among them, recommendation data enhancement using
    LLM has been widely studied, while it is also widely used in tasks such as sequence
    advancement, dialogue recommendation, and direct recommendation and so on.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）在自然语言处理（NLP）领域取得的显著成就，它们在推荐系统中的应用正变得越来越广泛（Li et al., [2023c](#bib.bib16)；Wu
    et al., [2023](#bib.bib28)；Liu et al., [2023](#bib.bib18)）。其中，使用LLM进行推荐数据增强已经被广泛研究，同时也广泛应用于序列推进、对话推荐和直接推荐等任务中。
- en: In recent studies, researchers have begun to explore the use of large-scale
    language models (LLMs) as a data enhancement tool to improve the performance of
    recommender systems (Dai et al., [2023](#bib.bib4); Li et al., [2022](#bib.bib12)).
    In particular, Liu et al. (Liu et al., [2024](#bib.bib17)) developed a method
    for generating multimodal language-image instruction tracking datasets using LLM.
    These data generated via LLM were used to optimise the model during training,
    significantly improving the model’s performance on visual and language comprehension
    tasks. Furthermore, some researchers have attempted to enhance the personalised
    inputs of recommender systems through LLM. For instance, Chen et al. (Yang et al.,
    [2023](#bib.bib31)) fed historical user behavioural data (e.g., clicks, purchases
    and ratings) into LLM to generate detailed user profiles. These profiles, in conjunction
    with historical interaction sequences and candidate item data, were employed to
    construct personalised recommendation prompts. These prompts were then utilised
    to predict potential interactions between users and items. In addition, Xi et
    al. (Xi et al., [2023](#bib.bib29)) explored an approach that integrates LLM’s
    ability to reason about user preferences with its understanding of item facts
    to generate informative input texts. These texts facilitate the capture of item
    features and details, thereby further enhancing the accuracy and relevance of
    the recommender system. The research of Lyu et al. (Lyu et al., [2023](#bib.bib19))
    is specifically focused on the utilisation of the knowledge and reasoning capabilities
    of the LLM to generate enhanced input text that more accurately captures the features
    and nuances of the items, thereby improving the performance of the recommendation
    performance.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究中，研究人员开始探讨使用大规模语言模型（LLMs）作为数据增强工具，以提高推荐系统的性能（Dai et al., [2023](#bib.bib4);
    Li et al., [2022](#bib.bib12)）。特别是，Liu et al.（Liu et al., [2024](#bib.bib17)）开发了一种利用LLM生成多模态语言-图像指令跟踪数据集的方法。这些通过LLM生成的数据用于优化训练过程中的模型，显著提升了模型在视觉和语言理解任务上的表现。此外，一些研究人员尝试通过LLM增强推荐系统的个性化输入。例如，Chen
    et al.（Yang et al., [2023](#bib.bib31)）将历史用户行为数据（如点击、购买和评分）输入LLM，生成详细的用户档案。这些档案与历史交互序列和候选项数据结合，用于构建个性化推荐提示。这些提示随后被用来预测用户和物品之间的潜在互动。此外，Xi
    et al.（Xi et al., [2023](#bib.bib29)）探索了一种将LLM对用户偏好的推理能力与其对物品事实的理解相结合，生成信息丰富的输入文本的方法。这些文本有助于捕捉物品特征和细节，从而进一步提升推荐系统的准确性和相关性。Lyu
    et al.（Lyu et al., [2023](#bib.bib19)）的研究特别关注于利用LLM的知识和推理能力生成增强输入文本，更准确地捕捉物品的特征和细微差别，从而提高推荐系统的表现。
- en: In sequential recommendation, researchers view the history of user interactions
    as a sequence of word tokens and make recommendation predictions. Morelra et al.
    (de Souza Pereira Moreira et al., [2021](#bib.bib5)) present an attempt to build
    a practical e-commerce and news recommendation framework based on the Transformers
    architecture. Sun et al. (Sun et al., [2019](#bib.bib22)) propose a methodwhereby
    som items in a sequence are masked auring training, thereby encouraging the model
    to predict the next recommended item based on the contextual environment through
    a completitive strategy. urthermore, by generating multiple training samples from
    each sequence, the simplification of the training process is avoided, and the
    model is made more robust. This approach avoids the task and makes the model more
    robust. Zhou et al. (Zhou et al., [2020](#bib.bib37)) exploits the mutual information
    between recommendation data and the correlation between sequences of different
    hierarchical levels to derive self-supervised signals and pre-train the way to
    enhance the data representation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列推荐中，研究人员将用户交互历史视为一系列词标记，并进行推荐预测。Moreira et al.（de Souza Pereira Moreira et
    al., [2021](#bib.bib5)）提出了一种基于Transformers架构构建实际电商和新闻推荐框架的尝试。Sun et al.（Sun et
    al., [2019](#bib.bib22)）提出了一种在训练期间对序列中的某些项进行掩蔽的方法，从而鼓励模型基于上下文环境通过竞争策略预测下一个推荐项。此外，通过从每个序列生成多个训练样本，避免了训练过程的简化，使模型更加稳健。这种方法避免了任务，使模型更具鲁棒性。Zhou
    et al.（Zhou et al., [2020](#bib.bib37)）利用推荐数据之间的互信息和不同层级序列之间的相关性，推导出自监督信号并预训练，以增强数据表示。
- en: Conversational recommender systems (CRS) are designed to proactively guide user
    preferences and recommend high-quality items through natural language dialogues.
    They provide recommendations in the form of interactive conversations (Jannach
    et al., [2021](#bib.bib10)). Wang et al. (Wang et al., [2022b](#bib.bib26)) propose
    a unified CRS model for knowledge-enhanced prompt learning, which constructs knowledge
    representations and combines task-specific sequential prompts. In addition to
    conversational context. Fu et al.(Fu et al., [2021](#bib.bib6)) consider the impact
    of popularity bias in CRS and achieve a balance between recommendation performance
    and item popularity by integrating semantic features derived from open-ended users’
    real-time discourse and history, as well as item popularity. Zhang(Zhang, [2023](#bib.bib35))
    combines LLM and graph neural networks to propose a graph-based dialogue Path
    Reasoning framework. This represents dialogue as interactive reasoning on knowledge
    graphs, and uses LLM for user modelling and response generation to improve the
    accuracy and user experience of CRS. Wang et al.(Wang et al., [2021](#bib.bib25))
    integrates the recommendation task into dialogue generation by introducing lexical
    pointers and introduces knowledge-aware biases learned from entity-oriented knowledge
    graphs to improve recommendation performance.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对话推荐系统（CRS）旨在通过自然语言对话主动引导用户偏好，并推荐高质量的项目。它们以互动对话的形式提供推荐（Jannach et al., [2021](#bib.bib10)）。Wang
    et al.（Wang et al., [2022b](#bib.bib26)）提出了一种统一的CRS模型用于知识增强的提示学习，该模型构建知识表示并结合任务特定的顺序提示。除此之外，Fu
    et al.（Fu et al., [2021](#bib.bib6)）考虑了CRS中的流行度偏差，通过整合来自开放式用户实时话语和历史的语义特征，以及项目流行度，实现了推荐性能与项目流行度之间的平衡。Zhang（Zhang,
    [2023](#bib.bib35)）结合LLM和图神经网络，提出了一种基于图的对话路径推理框架。该框架将对话表示为对知识图的互动推理，并使用LLM进行用户建模和响应生成，以提高CRS的准确性和用户体验。Wang
    et al.（Wang et al., [2021](#bib.bib25)）通过引入词汇指针，将推荐任务整合到对话生成中，并引入从面向实体的知识图中学习到的知识感知偏差，以提高推荐性能。
- en: All of the above studies involve prompt learning methods, which can be used
    not only to reconstruct the task form to align the goals of the recommendation
    task with the training goals of LLM; but also to introduce knowledge graphs and
    other external knowledge sources. Prompt-based recommendation methods show great
    potential for improving recommendation quality and enhancing user experience.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以上所有研究都涉及提示学习方法，这些方法不仅可以用于重构任务形式，以将推荐任务的目标与LLM的训练目标对齐；还可以引入知识图和其他外部知识来源。基于提示的推荐方法在提高推荐质量和增强用户体验方面展现了巨大的潜力。
- en: 2.2\. recommendations based on Prompt Learning
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 基于提示学习的推荐
- en: Prompt learning, an emerging technique, has been shown to be effective in bridging
    the gap between pre-training and downstream recommendation tasks for large language
    models (LLMs). The core idea of prompt learning is to redefine the downstream
    task into a form that matches the pre-training task by means of well-designed
    prompts, thus exploiting the powerful linguistic capabilities of the pre-trained
    model(Lester et al., [2021](#bib.bib11); Wang et al., [2022a](#bib.bib27)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 提示学习是一种新兴技术，已被证明在弥合大语言模型（LLM）预训练与下游推荐任务之间的差距方面非常有效。提示学习的核心思想是通过精心设计的提示将下游任务重新定义为与预训练任务匹配的形式，从而利用预训练模型的强大语言能力（Lester
    et al., [2021](#bib.bib11); Wang et al., [2022a](#bib.bib27)）。
- en: Discrete prompts are sequences of text or tokens embedded directly into the
    input of the model that motivate or direct the model to generate a specific output.
    These prompts are characterised by their clarity and interpretability, as they
    are explicitly expressed in natural language, such as specific questions, instructions
    or descriptions. LI et al.(Li et al., [2023a](#bib.bib14)) uses user and item
    IDs as prompts to overcome the semantic spatial differences between IDs and language
    models. In contrast, Zhang et al.(Zhang and Wang, [2023](#bib.bib36)) uses news
    titles instead of users and items in news recommendation scenarios, and designs
    specific discrete prompt templates from multi-dimensions to construct a complete-fill-in-the-blanks
    recommendation process. Geng et al.(Geng et al., [2022](#bib.bib8)) utilise personalised
    discrete prompt templates to convert multiple recommendation tasks into common
    natural language sequences, thereby unifying them into a shared framework for
    a ”Pretrain, Personalised Prompt, and Predict Paradigm” text-to-text recommendation
    paradigm. Wang et al.(Wang et al., [2022b](#bib.bib26))employs a prompt design
    that integrates knowledge representation and dialogue context in a session recommendation
    system, thereby providing more adequate contextual information for the task of
    session recommendation. Zhai et al.(Zhai et al., [2023](#bib.bib34)) propose the
    use of discrete prompts as a means of bridging the semantic divide, integrating
    structured knowledge graphs into language model-based sequential recommendation
    tasks in order to enhance the knowledge-driven capabilities of recommendation
    systems.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 离散提示是直接嵌入模型输入的文本或标记序列，用于激励或指导模型生成特定输出。这些提示的特点是其**清晰性**和**可解释性**，因为它们以自然语言明确表达，例如具体的问题、指令或描述。LI
    等（Li et al., [2023a](#bib.bib14)）使用用户和项目 ID 作为提示，以克服 ID 与语言模型之间的语义空间差异。相比之下，Zhang
    等（Zhang and Wang, [2023](#bib.bib36)）在新闻推荐场景中使用新闻标题，而不是用户和项目，并从多维度设计具体的离散提示模板，以构建一个完整的填空推荐过程。Geng
    等（Geng et al., [2022](#bib.bib8)）利用个性化的离散提示模板，将多个推荐任务转化为通用的自然语言序列，从而将它们统一为一个“预训练、个性化提示和预测范式”的文本到文本推荐范式。Wang
    等（Wang et al., [2022b](#bib.bib26)）采用一种将知识表示和对话上下文整合到会话推荐系统中的提示设计，从而为会话推荐任务提供更充分的上下文信息。Zhai
    等（Zhai et al., [2023](#bib.bib34)）提出了使用离散提示作为弥合语义鸿沟的一种手段，将结构化知识图谱集成到基于语言模型的序列推荐任务中，以增强推荐系统的知识驱动能力。
- en: Continuous Prompts, also known as Soft Prompts, is a relatively novel technique
    in which the prompts are incorporated into the model input in the form of learnable
    parameters. These parameters do not necessarily correspond to actual words in
    natural language; rather, they are optimised through the training process to most
    effectively guide the model through a specific task. The advantage of continuous
    cues is that they can be optimised directly through backpropagation, allowing
    for greater flexibility in adapting to different task requirements. Continuous
    cues are typically integrated with the model’s embedding or input layer, enabling
    the model to utilise these learned signals more effectively during the task. Yi
    et al.(Li et al., [2023b](#bib.bib15)) combined a graph neural network with a
    prompt fine-tuning paradigm to construct continuous prompt templates using personalised
    graph structural representations, thereby achieving feature enhancement in sequence
    recommendation. The generation of prompt vectors is achieved through the controlled
    length of the prompt vectors, thus improving the efficiency of LLM fine-tuning.
    Yi et al. (Yi et al., [2023](#bib.bib32)) combine a graph neural network with
    a prompt fine-tuning paradigm to construct continuous prompt templates using personalised
    graph structural representations, thus enhancing the features in sequence recommendation.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 连续提示，也称为软提示，是一种相对新颖的技术，其中提示以可学习参数的形式被纳入模型输入。这些参数不一定对应于自然语言中的实际词汇；相反，它们通过训练过程进行优化，以最有效地引导模型完成特定任务。连续提示的优势在于可以通过反向传播直接优化，从而在适应不同任务需求时提供更大的灵活性。连续提示通常与模型的嵌入层或输入层集成，使模型在任务执行过程中能更有效地利用这些学习到的信号。Yi等（Li等，[2023b](#bib.bib15)）将图神经网络与提示微调范式相结合，利用个性化的图结构表示构建了连续提示模板，从而在序列推荐中实现了特征增强。提示向量的生成通过控制提示向量的长度来实现，从而提高了大型语言模型（LLM）微调的效率。Yi等（Yi等，[2023](#bib.bib32)）将图神经网络与提示微调范式相结合，利用个性化的图结构表示构建了连续提示模板，从而增强了序列推荐中的特征。
- en: Discrete prompts are typically expressed in natural language and are designed
    to provide models with clear and unambiguous task goals, conferring a significant
    advantage in terms of interpretability. However, they tend to be more rigid, not
    easily adaptable to subtle changes in the task, and require a great deal of expert
    knowledge, manual design and adaptation. Continuous prompting offers greater flexibility
    in the form of parameters, which can be directly adjusted by optimisation algorithms
    such as shave descent, allowing the model to be more finely tuned to the specific
    task. However, their interpretability is poor and the concepts of many parameters
    cannot be aligned with human understanding. Despite the advantages and disadvantages
    of each of these approaches, they all generally face a key problem, namely a high
    sensitivity to prompts. The performance of a model is often highly contingent
    on the design and quality of the prompts, and inappropriate prompts may result
    in a significant decline in model performance or divergence from the desired objective.
    Furthermore, models may be excessively tailored to a specific prompt structure
    and may perform poorly in the absence of precise prompts or when the prompt format
    changes. In this paper, we propose a novel approach to prompt learning that combines
    it with generative adversarial networks. This approach enables the construction
    of a diverse prompt generator, which reduces the model’s dependence on precise
    prompts and improves its adaptability and robustness in changing environments
    by enhancing the diversity of training data.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 离散提示通常以自然语言形式表达，并旨在为模型提供清晰明确的任务目标，从而在可解释性方面具有显著优势。然而，它们往往较为僵化，不易适应任务中的细微变化，并且需要大量的专业知识、手动设计和调整。连续提示则提供了更大的灵活性，其形式为参数，可以通过诸如梯度下降等优化算法直接调整，从而使模型能够更精细地调整到特定任务。然而，它们的可解释性较差，许多参数的概念无法与人类理解对齐。尽管这些方法各有优缺点，但它们通常面临一个关键问题，即对提示的高度敏感。模型的性能往往高度依赖于提示的设计和质量，不恰当的提示可能导致模型性能显著下降或偏离预期目标。此外，模型可能过度依赖特定的提示结构，在缺乏精确提示或提示格式变化时表现不佳。在本文中，我们提出了一种将提示学习与生成对抗网络相结合的新方法。这种方法能够构建一个多样化的提示生成器，通过增强训练数据的多样性，减少模型对精确提示的依赖，并提高其在变化环境中的适应性和鲁棒性。
- en: 2.3\. Generating Adversarial Networks
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 生成对抗网络
- en: Generative adversarial network is a deep learning model based on zero-sum game
    theory (Goodfellow et al., [2014](#bib.bib9)), which combines generators and discriminators
    in an adversarial learning manner, and its optimal learning process is a minimum
    game problem (Li et al., [2017](#bib.bib13)), through the minimum-maximum game
    between generators and discriminators to (Goodfellow et al., [2014](#bib.bib9))
    to find the Nash equilibrium of both parties (Nash, [1953](#bib.bib20)), which
    improves the model performance and allows the generator to estimate the distribution
    of data samples.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络是一种基于零和博弈理论的深度学习模型（Goodfellow et al., [2014](#bib.bib9)），它将生成器和判别器以对抗学习的方式结合在一起，其最优学习过程是一个最小游戏问题（Li
    et al., [2017](#bib.bib13)），通过生成器和判别器之间的最小-最大博弈（Goodfellow et al., [2014](#bib.bib9)）找到双方的纳什均衡（Nash,
    [1953](#bib.bib20)），这提高了模型性能，并使生成器能够估计数据样本的分布。
- en: 3\. METHOD
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法
- en: In this section, we first formalise the problem and then elaborate on the details
    of the overall framework and sub-modules.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先对问题进行形式化，然后详细阐述整体框架及其子模块的细节。
- en: 3.1\. Definition of the problem
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 问题定义
- en: Suppose we have a set of users denoted by $U$ denotes the text of the user’s
    comment on this item.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组用户，$U$ 表示用户对该项的评论文本。
- en: For each user $u$ means no interaction record.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个用户 $u$ 表示没有交互记录。
- en: 'Prompt Template Definition. In this paper, we forcus on sequence recommendation
    task. Referring to the previous work (Wang et al., [2022b](#bib.bib26)), different
    original recommendation templates are constructed for different recommendation
    tasks. For different template construction and task definition are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 提示模板定义。本文主要集中于序列推荐任务。参考之前的工作（Wang et al., [2022b](#bib.bib26)），为不同的推荐任务构建了不同的原始推荐模板。不同模板的构建和任务定义如下：
- en: 'Sequence Recommendation. In sequence recommendation, the model will be based
    on the user’s interaction sequence $[i_{1},i_{2},i_{3},...,i_{n-1}]$. Specifically,
    two task forms and corresponding prompt templates are used in this paper:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 序列推荐。在序列推荐中，模型将基于用户的交互序列 $[i_{1},i_{2},i_{3},...,i_{n-1}]$。具体而言，本文使用了两种任务形式和相应的提示模板：
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Template:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模板：
- en: 'Input template: {user_entity} has the following purchase history:'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入模板：{user_entity} 具有以下购买历史：
- en: '{purchase_history}'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{purchase_history}'
- en: does the user likely to buy {target_item} next ?
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户是否可能在下一次购买 {target_item}？
- en: 'Target template: {answer_choices[label]} (yes/no)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标模板：{answer_choices[label]} (是/否)
- en: In this task, discriminative prompts are employed, whereby the model determines
    whether to interact with the next candidate based on the user interaction history.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，使用了判别性提示，模型根据用户的交互历史来决定是否与下一个候选项互动。
- en: 3.2\. Our Method
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 我们的方法
- en: 'This subsection presents the modelling framework and module details proposed
    in this paper in detail. Firstly, an overview of the framework proposed in this
    paper is given. The framework proposed in this paper contains two major parts:
    the Diversity encoder construction and the recommendation task access. The Diversity
    encoder construction part contains three small modules: the attribute generation
    module, the encoder diversity module based on GANs and the diversity constraint
    module. In the recommendation task access process, the trained Diversity encoder
    will be employed to generate the representation of text data and subsequently
    access the downstream LLM in order to achieve the recommendation task. A detailed
    diagram of the specific framework is shown in [2](#S3.F2 "Figure 2 ‣ 3.2\. Our
    Method ‣ 3\. METHOD ‣ GANPrompt: Enhancing Robustness in LLM-Based Recommendations
    with GAN-Enhanced Diversity Prompts").'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '本小节详细介绍了本文提出的建模框架和模块细节。首先，给出了本文提出的框架的概述。本文提出的框架包含两个主要部分：多样性编码器构建和推荐任务访问。多样性编码器构建部分包含三个小模块：属性生成模块、基于GAN的编码器多样性模块和多样性约束模块。在推荐任务访问过程中，训练好的多样性编码器将用于生成文本数据的表示，然后访问下游LLM，以实现推荐任务。具体框架的详细图示见
    [2](#S3.F2 "图 2 ‣ 3.2\. 我们的方法 ‣ 3\. 方法 ‣ GANPrompt: 利用GAN增强的多样性提示提升LLM基础推荐的鲁棒性")。'
- en: '![Refer to caption](img/ddb212fcaf6dd4c7c97ceeaaaa54c755.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ddb212fcaf6dd4c7c97ceeaaaa54c755.png)'
- en: Figure 2. Overall framework diagram of the model.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 模型的总体框架图。
- en: 3.2.1\. attribute generation
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 属性生成
- en: The utilisation of LLM as a data generator can effectively address the necessity
    for task-specific data(Gao et al., [2022](#bib.bib7)). The utilisation of intricate
    attribute prompts to generate disparate attribute-generated data has been demonstrated
    to be an efficacious and expedient addition to the original dataset, thereby enhancing
    the performance and robustness of downstream tasks(Yu et al., [2024](#bib.bib33)).
    Consequently, the objective of this paper is to design complex attribute prompts
    for different tasks and input them into LLM, with the intention of generating
    the corresponding diversity attributes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 将LLM作为数据生成器可以有效解决任务特定数据的需求（Gao et al., [2022](#bib.bib7)）。使用复杂的属性提示生成不同属性数据已经被证明是对原始数据集的有效且便捷的补充，从而提高了下游任务的性能和鲁棒性（Yu
    et al., [2024](#bib.bib33)）。因此，本文的目标是为不同任务设计复杂的属性提示，并将其输入LLM，旨在生成相应的多样性属性。
- en: In the context of sequence recommendation, the corresponding item description
    sequence $[d_{1},d_{2},d_{3},.....d_{n}]$ is constructed from the original training
    data and input into the following attribute generation template.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在序列推荐的背景下，相应的项目描述序列 $[d_{1},d_{2},d_{3},.....d_{n}]$ 是从原始训练数据构建的，并输入到以下属性生成模板中。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Template:'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模板：
- en: 'You are a professional shopper, please generate the exact attributes based
    on the user’s historical browsing sequence below:'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你是一个专业的购物者，请根据用户的历史浏览序列生成准确的属性：
- en: '{title_seq}'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '{title_seq}'
- en: If you recommend {dataset_item} to this user, you should focus on {attribute}.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你向该用户推荐 {dataset_item}，你应该关注 {attribute}。
- en: Where {title_seq} corresponds to the title sequence of the user’s interaction
    history; {dataset_item} represents the product category contained in the corresponding
    recommendation dataset; and {attribute} is the attribute generated by the model
    for the data sample.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 {title_seq} 对应于用户交互历史的标题序列；{dataset_item} 表示在相应推荐数据集中包含的产品类别；{attribute}
    是模型为数据样本生成的属性。
- en: It can be formalized as follows.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以被形式化为如下。
- en: '|  | $1$2 |  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $T_{Attr}^{seq}$ denotes the corresponding attribute generated for the
    sample of sequence recommendation data.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $T_{Attr}^{seq}$ 表示为序列推荐数据样本生成的相应属性。
- en: 3.2.2\. encoder diversity based on GANs
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 基于GAN的编码器多样性
- en: Although the method of attribute generation helps to increase the diversity
    of the data, there are still limitations. In this paper, inspired by GANs, we
    use the encoder of LLM as a generator $G_{LLM_{e}}(dian)$ to distinguish the generated
    text from the real text, in order to realise the zero-sum game process of GANs.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管属性生成方法有助于增加数据的多样性，但仍然存在局限性。本文受到GAN的启发，使用LLM的编码器作为生成器 $G_{LLM_{e}}(dian)$ 来区分生成的文本和真实文本，以实现GAN的零和博弈过程。
- en: Generation process.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 生成过程。
- en: 'Through the attribute generation process, we can obtain $k+1$ attribute datasets.
    In the generation process, we take the data samples with added attributes as random
    noise and input them into the LLM encoder to generate the latent features of the
    data, which are in the following form for the sequence recommendation task, for
    example:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 通过属性生成过程，我们可以获得 $k+1$ 个属性数据集。在生成过程中，我们将添加属性的数据样本视为随机噪声，并将其输入LLM编码器以生成数据的潜在特征，以下是序列推荐任务的形式，例如：
- en: '|  | $1$2 |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $1$2 |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: In this context, $a_{n}^{seq}$ represents the potential feature generated for
    the nth attribute data of the user.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下， $a_{n}^{seq}$ 表示为用户的第n个属性数据生成的潜在特征。
- en: Discriminative Process.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 判别过程。
- en: 'Following the generation of potential feature vectors for the corresponding
    data, a simple discriminator was constructed to classify them for the adversarial
    learning gaming process. The discriminator comprises three layers of multilayer
    perceptrons (MLPs):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成对应数据的潜在特征向量后，构建了一个简单的判别器以对其进行分类，用于对抗学习博弈过程。判别器由三层多层感知器（MLPs）组成：
- en: '|  | $1$2 |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: The input vector, $x$ represents the layer number, which ranges from 1 to 3\.
    Subsequently, the output vector of the generator is fed into the discriminator,
    which then outputs the predicted category.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量 $x$ 表示层数，范围从1到3。随后，生成器的输出向量被输入到判别器中，判别器然后输出预测的类别。
- en: '|  | $y_{GAN}=D_{MLP}(emb^{seq}),$ |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $y_{GAN}=D_{MLP}(emb^{seq}),$ |  |'
- en: The input of the data feature vector to the discriminator, denoted by $emb^{seq}$.
    For real data and data of the k attribute, the discriminator is expected to recognize
    the input into the corresponding seven categories.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据特征向量输入到鉴别器中，记作 $emb^{seq}$。对于真实数据和具有 k 属性的数据，期望鉴别器能够将输入识别为对应的七个类别。
- en: 3.2.3\. diversity constraint
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 多样性约束
- en: Diversity is a fundamental problem in classification and clustering tasks, and
    its concept is strongly related to distance; as the distance between two samples
    increases, the similarity between them decreases, thus increasing the diversity
    between the samples(Xia et al., [2015](#bib.bib30)). In order to make the diversity
    encoder more effective in expanding the differences between different samples,
    this paper introduces the cosine similarity distance and the JS dispersion from
    the perspective of mathematical theory to calculate the angle and information
    differences between different samples, in order to measure the diversity between
    samples. And it is used as the diversity constraint index in the encoder optimization
    process, so that the optimized diversity encoder can distinguish samples more
    effectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性是分类和聚类任务中的一个基本问题，其概念与距离密切相关；随着两个样本之间距离的增加，它们之间的相似性减少，从而增加样本之间的多样性（Xia 等，
    [2015](#bib.bib30)）。为了使多样性编码器在扩展不同样本之间的差异时更加有效，本文从数学理论的角度引入了余弦相似度距离和 JS 散度，以计算不同样本之间的角度和信息差异，从而测量样本之间的多样性。它被用作编码器优化过程中的多样性约束指标，以便优化后的多样性编码器能够更有效地区分样本。
- en: Cosine similarity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度。
- en: 'Cosine similarity uses the cosine of the angle between two vectors to measure
    similarity. It is expected that there will be a small angle between two similar
    vectors $x_{i}^{\prime}$, and the cosine similarity is defined as follows:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度使用两个向量之间角度的余弦值来度量相似性。期望两个相似向量 $x_{i}^{\prime}$ 之间会有一个小角度，余弦相似度定义如下：
- en: '|  | $1$2 |  |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: Where $d$ decreases, the angle between the two vectors increases, which simultaneously
    implies an increase in the distance between the vectors and a weakening of the
    similarity, which in this paper will be viewed as an increase in the diversity
    between the vector samples.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $d$ 减少时，两向量之间的角度增加，这同时意味着向量之间的距离增加，相似性减弱，这在本文中被视为向量样本之间多样性的增加。
- en: However, basic cosine similarity has the serious problem of focusing only on
    the directions between patterns, which leads to the computation of the diversity
    metric between vectors being isolated. For this reason, this paper introduces
    JS scattering to complement it.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，基本的余弦相似度存在一个严重的问题，即只关注模式之间的方向，这导致向量之间的多样性度量计算被孤立。为此，本文引入了 JS 散度来进行补充。
- en: Jensen-Shannon Divergence.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Jensen-Shannon 散度。
- en: 'The JS scatter is a symmetric version of the KL (Kullback-Leibler Divergence)
    scatter, which is used to compute the loss of information between two probability
    distributions and thus measure the difference between them. For two similar vectors
    $x_{i}^{\prime}$. The specific formula is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度是 KL（Kullback-Leibler 散度）散度的对称版本，用于计算两个概率分布之间的信息损失，从而测量它们之间的差异。对于两个相似的向量
    $x_{i}^{\prime}$，具体公式如下：
- en: '|  | $D_{KL}(x_{i}\parallel x_{i}^{\prime})=\sum x_{i}\log\frac{x_{i}}{x_{i}^{\prime}},$
    |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $D_{KL}(x_{i}\parallel x_{i}^{\prime})=\sum x_{i}\log\frac{x_{i}}{x_{i}^{\prime}},$
    |  |'
- en: 'The JS scatter is based on the KL scatter and symmetrizes and normalizes the
    KL scatter by introducing an intermediate distribution. The specific formula is
    as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: JS 散度基于 KL 散度，通过引入中间分布对 KL 散度进行对称化和归一化。具体公式如下：
- en: '|  | $1$2 |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: '|  | $m=\frac{1}{2}(x_{i}+x_{i}^{\prime}),$ |  |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $m=\frac{1}{2}(x_{i}+x_{i}^{\prime}),$ |  |'
- en: where $m$ is the mean distribution of the sample.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $m$ 是样本的均值分布。
- en: 'The distance metric used in this paper combines cosine similarity and JS scatter
    in the following form:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 本文使用的距离度量结合了余弦相似度和 JS 散度，形式如下：
- en: '|  | $1$2 |  |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $\alpha$ are the weights in the distance-metric synthesis process. By
    combining the cosine similarity and JS scatter, this metric can synthesize the
    angular distance differences and information differences between sample vectors.
    This diversity metric is involved in the diversity encoder optimisation process
    for the construction of sample diversity in both the distance and the information
    dimensions.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是距离度量合成过程中的权重。通过结合余弦相似度和 JS 散点，这个度量可以合成样本向量之间的角距离差异和信息差异。这个多样性度量参与多样性编码器优化过程，以在距离和信息维度上构建样本多样性。
- en: 3.3\. Recommendation Task
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 推荐任务
- en: 'Most of the existing fine-tuned large language model recommendation methods
    based on textual cues follow the paradigm of natural language processing: first,
    the prompt text is transformed into a sequence of tokens and encoder, and the
    feature embeddings of tokens and sentences are computed, and after that fine-tuning
    is performed on the structure of the large language model either directly or by
    employing, e.g., lora, and it is the large language model that is adapted to downstream
    recommendation tasks.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现有的基于文本提示的微调大型语言模型推荐方法遵循自然语言处理的范式：首先，将提示文本转换为一系列的标记并进行编码，计算标记和句子的特征嵌入，然后对大型语言模型的结构进行微调，直接进行或通过使用例如
    lora 这样的工具进行微调，而大型语言模型则被调整以适应下游推荐任务。
- en: 'With the previous work, we implement an attribute-based that generates embedding
    representations for items combining attributes with a diversity of distance and
    information dimensions. In order to enhance the cue-based big language model to
    be more robust to downstream recommendation tasks, we plug-in this diversity encoder
    as a plug-in module into an existing big language model-based recommender system
    for recommendation task fine-tuning. Taking the sequence recommendation task as
    an example, the recommendation task cue template, user interaction history sequence,
    and corresponding attributes are inputted into the diversity encoder to compute
    the embedded features in the following specific form:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 利用之前的工作，我们实现了一种基于属性的嵌入表示生成方法，将属性与距离和信息维度的多样性结合起来。为了增强基于提示的大语言模型在下游推荐任务中的鲁棒性，我们将这个多样性编码器作为插件模块插入现有的大语言模型推荐系统中进行推荐任务的微调。以序列推荐任务为例，将推荐任务提示模板、用户交互历史序列和相应的属性输入到多样性编码器中，以计算嵌入特征，具体形式如下：
- en: '|  | $1$2 |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $T_{rec}^{seq}$ is the data embedding vector generated using the diversity
    encoder.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $T_{rec}^{seq}$ 是使用多样性编码器生成的数据嵌入向量。
- en: 'Specifically, we will use a pre-trained diversity encoder to replace the original
    encoder structure of the large language model, and for LLMs whose hidden layer
    dimensions do not match the encoder, we access the projection layer for dimension
    mapping. The specific form is as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将使用预训练的多样性编码器来替代大型语言模型的原始编码器结构，对于隐藏层维度与编码器不匹配的 LLM，我们访问投影层进行维度映射。具体形式如下：
- en: '|  | $P_{ui}=LLM_{Rec}(\sigma_{i}(W_{i}E_{ui}+b_{i}))),$ |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $P_{ui}=LLM_{Rec}(\sigma_{i}(W_{i}E_{ui}+b_{i}))),$ |  |'
- en: where $w_{i}$ is the large language model used for the recommendation, and in
    this paper we use the LLama2 base model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $w_{i}$ 是用于推荐的大语言模型，本文中我们使用 LLama2 基础模型。
- en: 3.4\. Two-stage optimization
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 两阶段优化
- en: 'Now, we consider how to optimize the model parameters. First, to obtain diverse
    item representations, we construct a diversity encoder optimization module based
    on adversarial generative networks. This module generates diverse item feature
    vectors, which are then used to fine-tune large language models (LLMs), enhancing
    their robustness. Second, to achieve the recommendation task, we utilize the Lora
    module to assist the LLM in learning the recommendation task. By freezing the
    parameters of the LLM and focusing on optimizing the Lora module, we accelerate
    the tuning process of the large language model. A simple way is to train them
    simultaneously. However, due to the continuous transformation by the diversity
    encoder module, the large language model’s understanding of items might change,
    potentially negatively affecting the consistency of the large model’s understanding
    of the recommendation data text. To address this issue, we adopt a two-step tuning
    approach: diversity encoder fine-tuning and recommendation robustness enhancement.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们考虑如何优化模型参数。首先，为了获得多样化的项表示，我们基于对抗生成网络构建了一个多样性编码器优化模块。该模块生成多样化的项特征向量，然后用于微调大型语言模型（LLMs），以增强其鲁棒性。其次，为了实现推荐任务，我们利用Lora模块帮助LLM学习推荐任务。通过冻结LLM的参数并专注于优化Lora模块，我们加速了大型语言模型的调优过程。一种简单的方法是同时训练它们。然而，由于多样性编码器模块的持续变换，大型语言模型对项目的理解可能会发生变化，这可能会对大型模型对推荐数据文本的理解一致性产生负面影响。为了解决这个问题，我们采用了两步调优方法：多样性编码器微调和推荐鲁棒性增强。
- en: '3.4.1\. Stage 1: Optimization of the diversity generator'
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.1\. 阶段 1：多样性生成器优化
- en: In the diversity encoder enhancement phase, we construct the generative adversarial
    network structure by categorizing the data samples with added attributes into
    multiple classes and constructing an MLP-based discriminator to classify the embeddings
    of the data samples. In each training iteration, the generator and discriminator
    are alternately optimized until convergence.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在多样性编码器增强阶段，我们通过将数据样本与附加属性分类为多个类别，并构建基于MLP的判别器来对数据样本的嵌入进行分类，从而构建生成对抗网络结构。在每次训练迭代中，生成器和判别器交替优化直到收敛。
- en: Generator Optimization.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器优化。
- en: 'First, for the generator, which is also known as the diversity encoder, the
    loss function employed is the cross-entropy loss function. This loss function
    measures the difference between the target ranked sequence and the sequence that
    has been predicted by the model.:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对于生成器，也称为多样性编码器，使用的损失函数是交叉熵损失函数。这个损失函数衡量了目标排名序列与模型预测序列之间的差异。
- en: '|  | $\mathcal{L}_{G_{LLM_{e}}}=-\sum_{t=1}^{T}\log P(y_{t}\mid y_{<t},\mathbf{x}),$
    |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{G_{LLM_{e}}}=-\sum_{t=1}^{T}\log P(y_{t}\mid y_{<t},\mathbf{x}),$
    |  |'
- en: 'where $y_{t}$ given the input sequence and all previous target words. The model
    uses this probability distribution to determine the most likely next token in
    the sequence. To enhance the model’s performance, we introduce a loss function
    that adds a diversity constraint, aiming to encourage the model to generate more
    diverse and less repetitive sequences. The total loss function that incorporates
    the diversity constraint on this basis is as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{t}$ 给定输入序列和所有先前的目标词。模型使用这个概率分布来确定序列中最可能的下一个标记。为了提升模型的性能，我们引入了一个添加多样性约束的损失函数，旨在鼓励模型生成更具多样性和较少重复的序列。在此基础上，包含多样性约束的总损失函数如下：
- en: '|  | $\mathcal{L}_{G}=\mathcal{L}_{G_{LLM_{e}}}+\gamma D_{total}\\ =-\sum_{t=1}^{T}\log
    P(y_{t}\mid y_{<t},\mathbf{x})+\gamma(\alpha D_{cos}(x_{i}\parallel x_{i}^{\prime})+\beta
    D_{JS}(x_{i}\parallel x_{i}^{\prime}))),$ |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{G}=\mathcal{L}_{G_{LLM_{e}}}+\gamma D_{total}\\ =-\sum_{t=1}^{T}\log
    P(y_{t}\mid y_{<t},\mathbf{x})+\gamma(\alpha D_{cos}(x_{i}\parallel x_{i}^{\prime})+\beta
    D_{JS}(x_{i}\parallel x_{i}^{\prime}))),$ |  |'
- en: where $\mathcal{L}_{G_{LLM_{e}}}$ is used to control the strength of this diversity
    constraint, essentially balancing between the accuracy of the generated samples
    and their diversity.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}_{G_{LLM_{e}}}$ 用于控制这种多样性约束的强度，实质上是在生成样本的准确性和它们的多样性之间进行平衡。
- en: 'The objective of the optimization process for the generator is to minimize
    the loss function $\mathcal{L}_{G}$. Formally, the optimization objective of the
    generator can be expressed as:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器优化过程的目标是最小化损失函数 $\mathcal{L}_{G}$。正式地，生成器的优化目标可以表示为：
- en: '|  | $\theta_{G}\leftarrow\arg\min_{\theta_{G}}\mathcal{L}_{G},$ |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{G}\leftarrow\arg\min_{\theta_{G}}\mathcal{L}_{G},$ |  |'
- en: $\theta_{G}$ is the optimized parameter in the generator, representing the parameter
    that has been fine-tuned during the training process to ensure the best possible
    performance of the LLM’s encoder.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: $\theta_{G}$ 是生成器中优化的参数，表示在训练过程中经过微调以确保 LLM 编码器最佳性能的参数。
- en: Discriminator Optimization.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器优化。
- en: 'For the discriminator $D_{MLP}$, which refers to the multilayer perceptron
    (MLP) classification model, we employ a loss function of multiclassification.
    This loss function is crucial for measuring the performance of the model in distinguishing
    between multiple classes. The computation of this loss function is done in the
    following manner:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于判别器 $D_{MLP}$，即多层感知机（MLP）分类模型，我们采用了多分类的损失函数。该损失函数对于测量模型在区分多个类别时的表现至关重要。此损失函数的计算方法如下：
- en: '|  | $\mathcal{L}_{D_{MLP}}=-\sum_{i=1}^{C}y_{i}\log(\hat{y}_{i}),$ |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{D_{MLP}}=-\sum_{i=1}^{C}y_{i}\log(\hat{y}_{i}),$ |  |'
- en: In this context, where $y_{i}$ denotes the predicted category of the data attribute,
    the objective of the discriminator is to optimize its performance by minimizing
    the objective function $$\\
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，其中 $y_{i}$ 表示数据属性的预测类别，判别器的目标是通过最小化目标函数 $$\\
- en: 'mathcal{L}_{D_{MLP}}$$. This optimization process ensures that the discriminator
    learns to better distinguish between different categories, enhancing its ability
    to make accurate predictions. The mathematical representation of this objective
    is given as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: mathcal{L}_{D_{MLP}}$$。这一优化过程确保判别器学会更好地区分不同类别，提高其做出准确预测的能力。该目标的数学表示如下：
- en: '|  | $\theta_{D}\leftarrow\arg\min_{\theta_{D}}\mathcal{L}_{D_{MLP}}$ |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{D}\leftarrow\arg\min_{\theta_{D}}\mathcal{L}_{D_{MLP}}$ |  |'
- en: $\theta_{D}$ is the parameter to be optimized in the discriminant. It represents
    the set of weights and biases used by the model to make classifications based
    on the input features.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: $\theta_{D}$ 是判别器中需要优化的参数。它表示模型用于根据输入特征进行分类的权重和偏差集合。
- en: '3.4.2\. Stage 2: Recommendation Robustness Fine-tuning'
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.4.2\. 阶段 2：推荐鲁棒性微调
- en: 'In the stage of enhancing the robustness of the recommendation system, we replace
    the original encoding layer of the large model with a diversity encoder and the
    corresponding feature mapping layer. Subsequently, we focus on optimizing the
    Lora module specifically fine-tuned for the recommendation task. Specifically,
    BCELoss is used as the loss function for the recommendation task:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在增强推荐系统鲁棒性的阶段，我们用多样性编码器及其对应的特征映射层替代了大模型的原始编码层。随后，我们专注于优化针对推荐任务特别微调的 Lora 模块。具体来说，BCELoss
    被用作推荐任务的损失函数：
- en: '|  | $1$2 |  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'In this context, $y_{i}$-th sample. The predicted outcome for each sample is
    expressed through the log-probabilities of the corresponding token in the large
    model’s vocabulary. At this stage, the model parameters that need to be optimized
    include the mapping layer of the diversity encoder and the parameters of the Lora
    module. Formally, this can be expressed as:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，$y_{i}$-th 样本。每个样本的预测结果通过大模型词汇表中相应标记的对数概率来表示。在这一阶段，需要优化的模型参数包括多样性编码器的映射层和
    Lora 模块的参数。形式上，可以表示为：
- en: '|  | $\theta_{Rec}\leftarrow\arg\min_{\theta_{D}}\mathcal{L}_{Rec},$ |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{Rec}\leftarrow\arg\min_{\theta_{D}}\mathcal{L}_{Rec},$ |  |'
- en: The parameter $\theta_{Rec}$ represents the model parameters that need to be
    optimized in the recommendation task. This includes the mapping layer of the diversity
    encoder and the learning parameters of the Lora module.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 $\theta_{Rec}$ 表示在推荐任务中需要优化的模型参数。这包括多样性编码器的映射层和 Lora 模块的学习参数。
- en: 4\. EXPERIMENTS
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验
- en: In this section, we first introduce the research questions to be answered in
    the experiment, then present the specific details of the experiment, and finally
    provide a detailed analysis of the following research questions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍实验中需要回答的研究问题，然后呈现实验的具体细节，最后提供对以下研究问题的详细分析。
- en: 'RQ1:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RQ1:'
- en: Whether GANPrompt can improve the recommended performance in the case of cue
    diversity?
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GANPrompt 是否能在提示多样性情况下提高推荐性能？
- en: 'RQ2:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'RQ2:'
- en: Whether data diversity is significantly enhanced or not?
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据多样性是否显著增强？
- en: 4.1\. Experiment Setting
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 实验设置
- en: This section details the datasets used in the experiments, the baseline methods
    and evaluation metrics for the comparisons, and the deployment details of the
    models in this paper.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了实验中使用的数据集、基线方法和比较的评估指标，以及本文模型的部署细节。
- en: 'Datesets.We conducted experiments on the Amazon dataset to evaluate GANPrompt.
    Amazon is a product review dataset that records user review behaviour across multiple
    domains. The recorded information includes user-side IDs, product IDs, ratings,
    and timestamps, as well as product-side descriptions, images, and up-level associations.
    To evaluate GANPrompt on this dataset, we selected three evaluation datasets,
    Amazon-Beauty, Amazon-Toys, and Amazon-Sports. Following common data preprocessing
    methods, we only consider user ratings of $5$ ) are used for model training. Table
    [1](#S4.T1 "Table 1 ‣ 4.1\. Experiment Setting ‣ 4\. EXPERIMENTS ‣ GANPrompt:
    Enhancing Robustness in LLM-Based Recommendations with GAN-Enhanced Diversity
    Prompts") gives the statistics of these datasets.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '对于数据集，我们在Amazon数据集上进行了实验，以评估GANPrompt。Amazon是一个记录用户评价行为的产品评论数据集，涵盖多个领域。记录的信息包括用户侧ID、产品ID、评分和时间戳，以及产品侧描述、图片和上级关联。为了评估GANPrompt，我们选择了三个评估数据集：Amazon-Beauty、Amazon-Toys和Amazon-Sports。按照常见的数据预处理方法，我们仅考虑用户评分为$5$的记录用于模型训练。表格[1](#S4.T1
    "Table 1 ‣ 4.1\. Experiment Setting ‣ 4\. EXPERIMENTS ‣ GANPrompt: Enhancing Robustness
    in LLM-Based Recommendations with GAN-Enhanced Diversity Prompts")提供了这些数据集的统计信息。'
- en: '| Dtaset | #User | #Item | #Iteraction | #Seq num | #Sparsity |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 用户数 | 项目数 | 交互数 | 序列数量 | 稀疏度 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Beauty | 2267 | 1557 | 19119 | 2414 | 99.46% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 美容 | 2267 | 1557 | 19119 | 2414 | 99.46% |'
- en: '| Toys | 1774 | 1390 | 13106 | 1818 | 99.47% |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 玩具 | 1774 | 1390 | 13106 | 1818 | 99.47% |'
- en: '| Sports | 6667 | 4021 | 51550 | 6846 | 99.80% |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 体育 | 6667 | 4021 | 51550 | 6846 | 99.80% |'
- en: Table 1. Statistics on preprocessed data sets
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表1. 预处理数据集统计信息
- en: Baselines. For different recommendation tasks, we evaluated the method proposed
    in this paper using a variety of representative recommendation methods, including
    traditional sequential recommendation algorithms, direct recommendation algorithms,
    and LLM-based recommendation methods.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 基线方法。针对不同的推荐任务，我们使用多种具有代表性的推荐方法来评估本文提出的方法，包括传统的序列推荐算法、直接推荐算法和基于LLM的推荐方法。
- en: •
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Sequence recommendation tasks
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 序列推荐任务
- en: –
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Caser: Applying horizontal and vertical convolution on a user’s historical
    behavioural sequence to capture sequential patterns and make recommendations.'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Caser: 对用户的历史行为序列进行水平和垂直卷积，以捕捉序列模式并进行推荐。'
- en: –
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'DIN: Using an attention mechanism to dynamically capture user interests and
    match them with target items to improve recommendation results.'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'DIN: 使用注意力机制动态捕捉用户兴趣，并将其与目标项目匹配，以改善推荐结果。'
- en: –
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'HGN: Dynamically selecting and aggregating different levels of historical user
    behaviour information through hierarchical gating mechanisms to enhance recommendation
    effectiveness.'
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'HGN: 通过分层门控机制动态选择和聚合不同级别的历史用户行为信息，以增强推荐效果。'
- en: –
  id: totrans-159
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'GRU4Rec: Modelling user sequential behaviours and making recommendations using
    GRUs'
  id: totrans-160
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'GRU4Rec: 使用GRU对用户序列行为进行建模并进行推荐'
- en: –
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'BERT4Rec: A Transformer-based sequential recommendation model that bi-directionally
    encodes a user’s sequence of historical behaviours by autoregression to capture
    complex contextual dependencies to enhance recommendation effectiveness.'
  id: totrans-162
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BERT4Rec: 一种基于Transformer的序列推荐模型，通过自回归双向编码用户的历史行为序列，以捕捉复杂的上下文依赖关系，提升推荐效果。'
- en: –
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'SASRec: It is a sequence recommendation model based on the self-attention mechanism,
    which achieves efficient personalised recommendation by capturing long-range dependencies
    in user behaviour sequences.'
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SASRec: 这是一种基于自注意力机制的序列推荐模型，通过捕捉用户行为序列中的长距离依赖关系来实现高效的个性化推荐。'
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Direct referral tasks
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接推荐任务
- en: –
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'MF: It is a collaborative filtering model that predicts user preferences for
    unrated items by decomposing the user-item interaction matrix into implicit feature
    vectors of the user and the item.'
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'MF: 这是一种协同过滤模型，通过将用户-项目交互矩阵分解为用户和项目的隐含特征向量，来预测用户对未评分项目的偏好。'
- en: –
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'LightGCN: A graph convolutional network model that significantly improves the
    efficiency and effectiveness of collaborative filtering recommendation through
    a simplified neighbour aggregation and jump connection mechanism.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'LightGCN: 一种图卷积网络模型，通过简化的邻居聚合和跳跃连接机制显著提高了协同过滤推荐的效率和效果。'
- en: •
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: LLM-based recommendation model
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 LLM 的推荐模型
- en: –
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'P5: Transforms multiple data into natural language sequences, thus unifying
    multiple recommendation tasks in a shared framework.'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'P5: 将多种数据转换为自然语言序列，从而将多个推荐任务统一到一个共享框架中。'
- en: –
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'TallRec: By structuring recommendation data into instructions and using a lightweight
    tuning approach to align LLMs for recommendation tasks, we improve the performance
    of LLMs in the recommendation domain and show strong generalisation capabilities
    across domains.'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'TallRec: 通过将推荐数据结构化为指令，并使用轻量级调整方法对齐大规模语言模型（LLMs）以用于推荐任务，我们提高了 LLMs 在推荐领域的性能，并展示了跨领域的强大泛化能力。'
- en: –
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'CoLLM: Effective integration of collaborative information in recommendation
    tasks is achieved by combining collaborative information with a large-scale language
    model, utilising collaborative embeddings generated by external conventional collaborative
    models and mapping them to the input embedding space of the language model.'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CoLLM: 通过将协同信息与大规模语言模型结合，有效整合了推荐任务中的协同信息，利用外部传统协同模型生成的协同嵌入，并将其映射到语言模型的输入嵌入空间。'
- en: –
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: CTRL:Combining collaborative models and pre-trained language models for click-through
    rate prediction tasks through cross-modal knowledge alignment and supervised fine-tuning
    to improve the performance and inference efficiency of recommender systems.
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CTRL: 通过跨模态知识对齐和监督微调，结合协同模型和预训练语言模型，用于点击率预测任务，以提高推荐系统的性能和推理效率。'
- en: '| model | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| model | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
- en: '| caser | 0.604967 | 0.059991 | 0.173798 | 0.272607 | 0.059991 | 0.095685 |
    0.121545 | 0.106353 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| caser | 0.604967 | 0.059991 | 0.173798 | 0.272607 | 0.059991 | 0.095685 |
    0.121545 | 0.106353 |'
- en: '| din | 0.811243* | 0.162770 | 0.479047* | 0.671372* | 0.162770 | 0.259455
    | 0.307251 | 0.242778 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| din | 0.811243* | 0.162770 | 0.479047* | 0.671372* | 0.162770 | 0.259455
    | 0.307251 | 0.242778 |'
- en: '| gru4rec | 0.793985 | 0.119982 | 0.411116 | 0.587120 | 0.119982 | 0.218118
    | 0.262077 | 0.209023 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| gru4rec | 0.793985 | 0.119982 | 0.411116 | 0.587120 | 0.119982 | 0.218118
    | 0.262077 | 0.209023 |'
- en: '| HGN | 0.792072 | 0.139832 | 0.438465 | 0.634318 | 0.139832 | 0.241389 | 0.291402
    | 0.229860 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| HGN | 0.792072 | 0.139832 | 0.438465 | 0.634318 | 0.139832 | 0.241389 | 0.291402
    | 0.229860 |'
- en: '| SASrec | 0.804019 | 0.123952 | 0.423467 | 0.614027 | 0.123952 | 0.227056
    | 0.276191 | 0.217349 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| SASrec | 0.804019 | 0.123952 | 0.423467 | 0.614027 | 0.123952 | 0.227056
    | 0.276191 | 0.217349 |'
- en: '| S3rec | 0.653376 | 0.048081 | 0.208205 | 0.307896 | 0.048081 | 0.101100 |
    0.127164 | 0.108495 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| S3rec | 0.653376 | 0.048081 | 0.208205 | 0.307896 | 0.048081 | 0.101100 |
    0.127164 | 0.108495 |'
- en: '| P5 | 0.634582 | 0.035846 | 0.050945 | 0.069852 | 0.035846 | 0.039751 | 0.043081
    | 0.057462 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| P5 | 0.634582 | 0.035846 | 0.050945 | 0.069852 | 0.035846 | 0.039751 | 0.043081
    | 0.057462 |'
- en: '| TALLRec | 0.702796 | 0.265642* | 0.425841 | 0.471253 | 0.265642* | 0.438516*
    | 0.481524* | 0.256842* |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| TALLRec | 0.702796 | 0.265642* | 0.425841 | 0.471253 | 0.265642* | 0.438516*
    | 0.481524* | 0.256842* |'
- en: '| GANPrompt | 0.705952 | 0.282794 | 0.578534 | 0.696252 | 0.282794 | 0.552869
    | 0.598784 | 0.291348 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| GANPrompt | 0.705952 | 0.282794 | 0.578534 | 0.696252 | 0.282794 | 0.552869
    | 0.598784 | 0.291348 |'
- en: '| Improvement (%) | - | +6.46% | +20.77% | +3.71% | +6.46% | +26.08% | +24.35%
    | +13.43% |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 改进 (%) | - | +6.46% | +20.77% | +3.71% | +6.46% | +26.08% | +24.35% | +13.43%
    |'
- en: Table 2. Performance metrics on the Beauty dataset.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. Beauty 数据集上的性能指标。
- en: '| model | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| model | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
- en: '| caser | 0.575176 | 0.046348 | 0.128844 | 0.201890 | 0.046348 | 0.078403 |
    0.099363 | 0.093628 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| caser | 0.575176 | 0.046348 | 0.128844 | 0.201890 | 0.046348 | 0.078403 |
    0.099363 | 0.093628 |'
- en: '| din | 0.717959* | 0.087896* | 0.260687* | 0.386081* | 0.087896* | 0.156796*
    | 0.191938* | 0.163307* |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| din | 0.717959* | 0.087896* | 0.260687* | 0.386081* | 0.087896* | 0.156796*
    | 0.191938* | 0.163307* |'
- en: '| gru4rec | 0.711225 | 0.086396 | 0.247338 | 0.368532 | 0.086396 | 0.148791
    | 0.182685 | 0.156953 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| gru4rec | 0.711225 | 0.086396 | 0.247338 | 0.368532 | 0.086396 | 0.148791
    | 0.182685 | 0.156953 |'
- en: '| HGN | 0.696624 | 0.083396 | 0.243588 | 0.357432 | 0.083396 | 0.146091 | 0.178133
    | 0.153098 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| HGN | 0.696624 | 0.083396 | 0.243588 | 0.357432 | 0.083396 | 0.146091 | 0.178133
    | 0.153098 |'
- en: '| SASrec | 0.705065 | 0.074696 | 0.233838 | 0.353532 | 0.074696 | 0.137811
    | 0.171229 | 0.146856 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| SASrec | 0.705065 | 0.074696 | 0.233838 | 0.353532 | 0.074696 | 0.137811
    | 0.171229 | 0.146856 |'
- en: '| S3rec | 0.541157 | 0.016799 | 0.077096 | 0.137993 | 0.016799 | 0.042931 |
    0.060194 | 0.061554 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| S3rec | 0.541157 | 0.016799 | 0.077096 | 0.137993 | 0.016799 | 0.042931 |
    0.060194 | 0.061554 |'
- en: '| P5 | 0.622521 | 0.028462 | 0.039574 | 0.046895 | 0.028462 | 0.032165 | 0.035846
    | 0.045813 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| P5 | 0.622521 | 0.028462 | 0.039574 | 0.046895 | 0.028462 | 0.032165 | 0.035846
    | 0.045813 |'
- en: '| TALLRec | 0.606144 | 0.037584 | 0.143585 | 0.261345 | 0.037584 | 0.083193
    | 0.102213 | 0.09754 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| TALLRec | 0.606144 | 0.037584 | 0.143585 | 0.261345 | 0.037584 | 0.083193
    | 0.102213 | 0.09754 |'
- en: '| GANPrompt | 0.618993 | 0.088610 | 0.198172 | 0.338204 | 0.088610 | 0.194582
    | 0.245934 | 0.193426 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| GANPrompt | 0.618993 | 0.088610 | 0.198172 | 0.338204 | 0.088610 | 0.194582
    | 0.245934 | 0.193426 |'
- en: '| Improvement (%) | - | +0.81% | - | - | +0.81% | +24.10% | +28.13% | +18.44%
    |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 改进 (%) | - | +0.81% | - | - | +0.81% | +24.10% | +28.13% | +18.44% |'
- en: Table 3. Performance metrics on the Sports dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3. 体育数据集上的性能指标。
- en: '| model | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | auc | hit@1 | hit@5 | hit@10 | ndcg@1 | ndcg@5 | ndcg@10 | mrr |'
- en: '| caser | 0.563149 | 0.034386 | 0.098083 | 0.161218 | 0.034386 | 0.061459 |
    0.080471 | 0.080474 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| caser | 0.563149 | 0.034386 | 0.098083 | 0.161218 | 0.034386 | 0.061459 |
    0.080471 | 0.080474 |'
- en: '| din | 0.753086 | 0.110485 | 0.304961 | 0.444194 | 0.110485 | 0.191770 | 0.231055
    | 0.195825 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| din | 0.753086 | 0.110485 | 0.304961 | 0.444194 | 0.110485 | 0.191770 | 0.231055
    | 0.195825 |'
- en: '| gru4rec | 0.739768 | 0.088501 | 0.277903 | 0.424464 | 0.088501 | 0.168506
    | 0.209935 | 0.175074 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| gru4rec | 0.739768 | 0.088501 | 0.277903 | 0.424464 | 0.088501 | 0.168506
    | 0.209935 | 0.175074 |'
- en: '| HGN | 0.758495* | 0.127959* | 0.316798* | 0.473506* | 0.127959* | 0.208445*
    | 0.252562* | 0.213780* |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| HGN | 0.758495* | 0.127959* | 0.316798* | 0.473506* | 0.127959* | 0.208445*
    | 0.252562* | 0.213780* |'
- en: '| SASrec | 0.757952 | 0.122886 | 0.310598 | 0.467869 | 0.122886 | 0.199035
    | 0.243883 | 0.206036 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| SASrec | 0.757952 | 0.122886 | 0.310598 | 0.467869 | 0.122886 | 0.199035
    | 0.243883 | 0.206036 |'
- en: '| S3rec | 0.621746 | 0.043968 | 0.135287 | 0.225479 | 0.043968 | 0.084166 |
    0.110406 | 0.102184 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| S3rec | 0.621746 | 0.043968 | 0.135287 | 0.225479 | 0.043968 | 0.084166 |
    0.110406 | 0.102184 |'
- en: '| P5 | 0.668452 | 0.048653 | 0.068452 | 0.079520 | 0.048653 | 0.057892 | 0.059183
    | 0.069521 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| P5 | 0.668452 | 0.048653 | 0.068452 | 0.079520 | 0.048653 | 0.057892 | 0.059183
    | 0.069521 |'
- en: '| TALLRec | 0.691331 | 0.055784 | 0.175421 | 0.324157 | 0.055784 | 0.071487
    | 0.086995 | 0.074075 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| TALLRec | 0.691331 | 0.055784 | 0.175421 | 0.324157 | 0.055784 | 0.071487
    | 0.086995 | 0.074075 |'
- en: '| GANPrompt | 0.712376 | 0.213842 | 0.389260 | 0.540811 | 0.213842 | 0.256983
    | 0.274613 | 0.232908 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| GANPrompt | 0.712376 | 0.213842 | 0.389260 | 0.540811 | 0.213842 | 0.256983
    | 0.274613 | 0.232908 |'
- en: '| Improvement (%) | - | +67.12% | +22.87% | +14.21% | +67.12% | +23.28% | +8.73%
    | +8.95% |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 改进 (%) | - | +67.12% | +22.87% | +14.21% | +67.12% | +23.28% | +8.73% | +8.95%
    |'
- en: Table 4. Performance metrics on the Toys dataset.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4. 玩具数据集上的性能指标。
- en: Evaluation Metrics. To fairly evaluate the recommendation performance of GANPrompt
    and the baseline model, we utilized widely accepted evaluation metrics previously
    employed in recommendation methods. These metrics include the Area Under the Curve
    (AUC) for accuracy assessments, as well as the Normalized Discounted Cumulative
    Gain (NDCG), Hit Rate (HR) for specific cutoff values $k=1,5,10$ and MRR are more
    focused on the quality of the desired recommendations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。为了公平评估GANPrompt和基线模型的推荐性能，我们采用了之前在推荐方法中广泛接受的评估指标。这些指标包括用于准确性评估的曲线下面积（AUC），以及针对特定截止值
    $k=1,5,10$ 的归一化折扣累积增益（NDCG）、命中率（HR）和更注重推荐质量的均值倒数排名（MRR）。
- en: Implementation Details.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节。
- en: All experiments were conducted in a Python 3.9 environment using the PyTorch
    framework and CUDA version 12.3, on NVIDIA’s A100 80G GPUs. Specifically, for
    the attribute generation segment, we utilized BERT as the attribute generator
    in a masked prediction task, selecting the top five attributes with the highest
    prediction frequency from each dataset as additional features. During the diversity
    encoder training phase, GANPrompt employed the T5 model’s encoder as the generator
    in a Generative Adversarial Network (GAN), with a custom-designed three-layer
    Multilayer Perceptron (MLP) serving as the discriminator; the batch size was set
    to 256, and both the encoder and discriminator were trained with a learning rate
    of 5e-5 using the Ada optimizer. For the sequential recommendation task, we used
    the pre-trained T5 language model as the foundational LLM for recommendations,
    setting the minimum length of historical interaction sequences at 3 and the maximum
    at 10, based on the P5\. The training batch size was 128 with a learning rate
    of 1e-5, using the AdamW optimizer. Differently from previous approaches, we fine-tuned
    only the T5’s decoder to adapt to the recommendation tasks while preserving the
    encoder’s capability to generate diverse data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验在 Python 3.9 环境中使用 PyTorch 框架和 CUDA 版本 12.3 进行，硬件为 NVIDIA 的 A100 80G GPU。具体而言，在属性生成部分，我们使用了
    BERT 作为属性生成器进行掩码预测任务，从每个数据集中选择预测频率最高的前五个属性作为附加特征。在多样性编码器训练阶段，GANPrompt 采用 T5 模型的编码器作为生成器，搭配自定义设计的三层多层感知机（MLP）作为判别器；批量大小设置为
    256，编码器和判别器的学习率均为 5e-5，使用 Ada 优化器。在序列推荐任务中，我们使用预训练的 T5 语言模型作为推荐的基础 LLM，设置历史交互序列的最小长度为
    3，最大长度为 10，基于 P5\. 训练批量大小为 128，学习率为 1e-5，使用 AdamW 优化器。与之前的方法不同，我们仅对 T5 的解码器进行了微调，以适应推荐任务，同时保留了编码器生成多样化数据的能力。
- en: 4.2\. Main Experiment Results
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 主要实验结果
- en: '![Refer to caption](img/c9e5c5215ebad806f25474605fc7fdc1.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c9e5c5215ebad806f25474605fc7fdc1.png)'
- en: (a) Distribution of data before encoder adjustment on the Beauty dataset.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Beauty 数据集上编码器调整前的数据分布。
- en: '![Refer to caption](img/1e3f7fc0266457f8ae0b7041a11a63d4.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1e3f7fc0266457f8ae0b7041a11a63d4.png)'
- en: (b) Distribution of encoder-adjusted data on Beauty dataset.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Beauty 数据集上经过编码器调整的数据分布。
- en: '![Refer to caption](img/5819a5a05ceff6a71f37bdcfcd7dcdea.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5819a5a05ceff6a71f37bdcfcd7dcdea.png)'
- en: (c) Distribution of data before encoder adjustment on the Sports dataset.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Sports 数据集上编码器调整前的数据分布。
- en: '![Refer to caption](img/395594ebf61c028474940ae83dbd21cf.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/395594ebf61c028474940ae83dbd21cf.png)'
- en: (d) Distribution of encoder-adjusted data on Sports dataset.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Sports 数据集上经过编码器调整的数据分布。
- en: '![Refer to caption](img/a8b5ece95922b801c7418d26381e7770.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/a8b5ece95922b801c7418d26381e7770.png)'
- en: (e) Distribution of data before encoder adjustment on the Toys dataset.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Toys 数据集上编码器调整前的数据分布。
- en: '![Refer to caption](img/34cfdb61bc62a6c2a52d6b5df52f4b96.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/34cfdb61bc62a6c2a52d6b5df52f4b96.png)'
- en: (f) Distribution of encoder-adjusted data on Toys dataset.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Toys 数据集上经过编码器调整的数据分布。
- en: Figure 3. Visualisation of data samples before and after Diversity encoder tuning,
    including the original sequence data of the three datasets and the attribute data
    after adding five attribute features.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. 多样性编码器调整前后数据样本的可视化，包括三个数据集的原始序列数据和添加了五个属性特征后的属性数据。
- en: In order to answer RQ1.The performance of GANPrompt and all baseline models
    on the three real-world datasets is shown in Tables LABEL:tab3,tab4,tab5, with
    the optimal results shown in bold and the best baseline results marked with an
    asterisk.The performance improvement of GANPrompt over the best baseline is shown
    as a percentage in the ‘Improvement’ row. The performance improvement of GANPrompt
    over the best baseline is shown in percentage in the ‘Improvement’ row.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答 RQ1，GANPrompt 和所有基线模型在三个真实数据集上的表现如表 LABEL:tab3、tab4、tab5 所示，最佳结果用粗体显示，最佳基线结果用星号标记。GANPrompt
    相对于最佳基线的性能提升以“提升”行中的百分比显示。
- en: The experimental results show that our proposed GANPrompt achieves leading recommendation
    performance on most evaluation metrics compared to the state-of-the-art baseline
    model. In particular, GANPrompt improves on the hit@1 metric in all datasets,
    especially on the Toys dataset, hit@1 metric improved by 67.12%. In addition,
    GANPrompt improves more than 20% on the hit@5 metric on both Beauty and Toys datasets,
    and more than 20% and 10% on the ndcg@5 and mrr metrics on all three datasets.
    While traditional sequential recommendation methods such as DIN excel in capturing
    users’ changing interests across advertisements or recommendation scenarios, and
    thus have superior performance in personalised recommendation accuracy, they perform
    poorly when dealing with diverse item features.GANPrompt learns how to deal with
    commonalities and differences between diverse data by training recommendation
    tasks on a diversity encoder. In contrast, LLM-based methods such as TALLRec,
    while taking advantage of LLM’s large number of parameters and pre-training knowledge,
    have good generalisation ability to diverse data, but are too sensitive to small
    differences between prompts. Compared to Llama2, the base model T5 used by GANPrompt,
    although having a smaller structure and parameters, exhibits more robust recommendation
    performance by adequately fine-tuning the diversity encoder for the recommendation
    task.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，我们提出的GANPrompt在大多数评估指标上相较于最先进的基线模型表现出色。特别是，GANPrompt在所有数据集上均提高了hit@1指标，尤其是在Toys数据集上，hit@1指标提高了67.12%。此外，GANPrompt在Beauty和Toys数据集上的hit@5指标提高了20%以上，在所有三个数据集上的ndcg@5和mrr指标分别提高了20%和10%以上。虽然传统的序列推荐方法如DIN在捕捉用户在广告或推荐场景中兴趣变化方面表现出色，因此在个性化推荐准确性上具有优势，但在处理多样化的项目特征时表现较差。GANPrompt通过在多样性编码器上训练推荐任务来学习如何处理多样数据的共性和差异。相比之下，尽管LLM方法如TALLRec利用了LLM的大量参数和预训练知识，对多样数据具有良好的泛化能力，但对提示中的细微差别过于敏感。与Llama2相比，GANPrompt使用的基础模型T5虽然结构和参数较小，但通过充分微调多样性编码器来适应推荐任务，从而展现了更稳健的推荐性能。
- en: However, although GANPrompt performs better on the auc metric when comparing
    it to the LLM-based approach, overall the traditional model performs better overall
    on the auc metric, obtaining optimal results. This is because the auc metric focuses
    more on the overall model ranking ability, i.e., the model’s ability to distinguish
    between positive and negative samples. In contrast, hit@k, NDCG@k and MRR are
    more concerned with the specific position of the recommendation list, focusing
    on measuring the ranking ability of the front end of the model. This suggests
    that while LLM-based approaches are able to leverage semantic information to accurately
    place relevant items at the top of the list, achieving very superior performance
    locally; for the model’s overall sorting of positive and negative samples, traditional
    mini-models, on the other hand, perform better on the global recommendation task
    through a more streamlined structure and learning task.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管GANPrompt在auc指标上表现优于基于LLM的方法，但总体而言，传统模型在auc指标上的表现更佳，获得了最佳结果。这是因为auc指标更关注整体模型排名能力，即模型区分正负样本的能力。相比之下，hit@k、NDCG@k和MRR更关注推荐列表的具体位置，重点测量模型前端的排名能力。这表明，虽然基于LLM的方法能够利用语义信息准确地将相关项目放在列表顶部，实现局部性能上的卓越表现；但在模型整体排序正负样本方面，传统的微型模型通过更精简的结构和学习任务在全局推荐任务上表现更好。
- en: 4.3\. Diversity encoder visualisation
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 多样性编码器可视化
- en: In order to verify whether the diversity encoder, which has been adapted by
    applying the GAN framework, can effectively differentiate data samples and enhance
    the diversity among data samples and answer the RQ2, we conducted detailed experimental
    analyses on three different datasets. These datasets all involve sequence recommendation
    tasks with rich and diverse feature data. In our experiments, we first considered
    the sequence recommendation data in each dataset as the original input, and then
    constructed a composite dataset with six attribute dimensions by combining the
    top 5 attribute data of each dataset for further visual presentation and analysis.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证应用了GAN框架的多样性编码器是否能有效区分数据样本并增强样本之间的多样性，并回答RQ2，我们对三个不同的数据集进行了详细的实验分析。这些数据集都涉及具有丰富和多样特征数据的序列推荐任务。在我们的实验中，我们首先将每个数据集中的序列推荐数据视为原始输入，然后通过结合每个数据集的前5个属性数据，构建一个具有六个属性维度的复合数据集，以便进一步进行可视化呈现和分析。
- en: 'The visualisation results are shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.2\.
    Main Experiment Results ‣ 4\. EXPERIMENTS ‣ GANPrompt: Enhancing Robustness in
    LLM-Based Recommendations with GAN-Enhanced Diversity Prompts"). From the figure,
    it can be observed that the encoder of the pre-trained language model T5 has difficulty
    in distinguishing datasets with different attributes before training, and all
    the sample points are almost clustered in the same data distribution region. However,
    after the diversity constraint adjustment by introducing the GAN framework and
    diversity encoder, the situation changes significantly. The datasets with different
    attributes are effectively differentiated in the feature space, and the data points
    of each collection start to scatter over a wider area.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '可视化结果如图[3](#S4.F3 "Figure 3 ‣ 4.2\. Main Experiment Results ‣ 4\. EXPERIMENTS
    ‣ GANPrompt: Enhancing Robustness in LLM-Based Recommendations with GAN-Enhanced
    Diversity Prompts")所示。从图中可以观察到，预训练语言模型T5的编码器在训练前很难区分具有不同属性的数据集，所有样本点几乎都聚集在相同的数据分布区域。然而，通过引入GAN框架和多样性编码器进行多样性约束调整后，情况发生了显著变化。具有不同属性的数据集在特征空间中得到了有效区分，每个集合的数据点开始在更广泛的区域内分散。'
- en: This obvious differentiation effect indicates that the diversity-adjusted encoder
    can effectively expand the distance between data with different attributes and
    change their distribution in the feature space. This not only illustrates the
    effectiveness of the diversity encoder under the GAN framework in improving data
    diversity, but also provides richer and more discriminative feature representations
    for the subsequent sequential recommendation task, enhancing the advantage of
    the subsequent model in dealing with complex and multi-attribute recommender system
    data.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这种明显的区分效果表明，经过多样性调整的编码器可以有效地扩展具有不同属性的数据之间的距离，并改变它们在特征空间中的分布。这不仅说明了在GAN框架下多样性编码器在提高数据多样性方面的有效性，还为后续的序列推荐任务提供了更丰富和更具辨别性的特征表示，增强了后续模型处理复杂和多属性推荐系统数据的优势。
- en: 5\. CONCLUSIONS
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 结论
- en: In this paper, we present an innovative framework called GANPrompt, which enhances
    the robustness of Large Language Models (LLMs) in recommender systems using Generative
    Adversarial Networks (GANs). Our research focuses on enhancing the model’s adaptability
    and stability to different cues through diverse cue generation. Specifically,
    GANPrompt first trains a multidimensional cue generator through GAN generation
    techniques, which is capable of generating highly diverse cues based on user behavioural
    data. These diverse cues are then used to train the LLM to improve its performance
    when faced with unseen cues. Furthermore, to ensure that the generated cues are
    both highly diverse and relevant, we introduce a diversity constraint mechanism
    based on mathematical theory to optimise cue generation and ensure that they semantically
    cover a wide range of user intentions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一个名为GANPrompt的创新框架，该框架利用生成对抗网络（GANs）增强了大语言模型（LLMs）在推荐系统中的鲁棒性。我们的研究重点是通过多样化的线索生成来提高模型对不同线索的适应性和稳定性。具体而言，GANPrompt首先通过GAN生成技术训练一个多维线索生成器，该生成器能够基于用户行为数据生成高度多样化的线索。这些多样化的线索随后用于训练LLM，以改善其在面对未见过的线索时的表现。此外，为了确保生成的线索既高度多样又相关，我们引入了一个基于数学理论的多样性约束机制，以优化线索生成，并确保它们在语义上覆盖广泛的用户意图。
- en: Through extensive experiments on multiple datasets, we demonstrate the effectiveness
    of the proposed framework, especially in improving the adaptability and robustness
    of recommender systems in complex and dynamic environments. Experimental results
    show that GANPrompt provides significant improvements in accuracy and robustness
    compared to existing state-of-the-art approaches. These results not only demonstrate
    the effectiveness of diversity encoders under the GAN framework in enhancing the
    diversity of data, but also provide new technical paths and strong experimental
    evidence for processing complex and multi-attribute recommender system data. Advantages
    when processing complex and multi-attribute recommender system data.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对多个数据集的广泛实验，我们展示了所提出框架的有效性，特别是在改善推荐系统在复杂和动态环境中的适应性和鲁棒性方面。实验结果表明，与现有的最先进方法相比，GANPrompt
    在准确性和鲁棒性方面提供了显著的改进。这些结果不仅展示了 GAN 框架下多样性编码器在增强数据多样性方面的有效性，还为处理复杂和多属性推荐系统数据提供了新的技术路径和有力的实验证据。
- en: References
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, 等. 2020. 语言模型是少量样本学习者。*神经信息处理系统进展* 33 (2020), 1877–1901。
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research* 24, 240 (2023), 1–113.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, 等. 2023. Palm: 通过路径扩展语言建模。*机器学习研究杂志* 24, 240 (2023), 1–113。'
- en: 'Dai et al. (2023) Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang,
    Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, et al. 2023.
    Auggpt: Leveraging chatgpt for text data augmentation. *arXiv preprint arXiv:2302.13007*
    (2023).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dai et al. (2023) Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang,
    Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, 等. 2023. Auggpt:
    利用 ChatGPT 进行文本数据增强。*arXiv 预印本 arXiv:2302.13007* (2023)。'
- en: 'de Souza Pereira Moreira et al. (2021) Gabriel de Souza Pereira Moreira, Sara
    Rabhi, Jeong Min Lee, Ronay Ak, and Even Oldridge. 2021. Transformers4rec: Bridging
    the gap between nlp and sequential/session-based recommendation. In *Proceedings
    of the 15th ACM conference on recommender systems*. 143–153.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'de Souza Pereira Moreira et al. (2021) Gabriel de Souza Pereira Moreira, Sara
    Rabhi, Jeong Min Lee, Ronay Ak, 和 Even Oldridge. 2021. Transformers4rec: 弥合自然语言处理与顺序/会话推荐之间的差距。在*第15届
    ACM 推荐系统大会论文集*中，143–153。'
- en: 'Fu et al. (2021) Zuohui Fu, Yikun Xian, Shijie Geng, Gerard De Melo, and Yongfeng
    Zhang. 2021. Popcorn: Human-in-the-loop popularity debiasing in conversational
    recommender systems. In *Proceedings of the 30th ACM International Conference
    on Information & Knowledge Management*. 494–503.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Fu et al. (2021) Zuohui Fu, Yikun Xian, Shijie Geng, Gerard De Melo, 和 Yongfeng
    Zhang. 2021. Popcorn: 人工智能介入的对话推荐系统中的流行度去偏差。在*第30届 ACM 国际信息与知识管理大会论文集*中，494–503。'
- en: Gao et al. (2022) Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, Zhiyong
    Wu, Weizhong Zhang, Xiaodan Liang, Zhenguo Li, and Lingpeng Kong. 2022. Self-guided
    noise-free data generation for efficient zero-shot learning. *arXiv preprint arXiv:2205.12679*
    (2022).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2022) Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng Ye, Zhiyong
    Wu, Weizhong Zhang, Xiaodan Liang, Zhenguo Li, 和 Lingpeng Kong. 2022. 自指导无噪声数据生成用于高效零样本学习。*arXiv
    预印本 arXiv:2205.12679* (2022)。
- en: 'Geng et al. (2022) Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and
    Yongfeng Zhang. 2022. Recommendation as language processing (rlp): A unified pretrain,
    personalized prompt & predict paradigm (p5). In *Proceedings of the 16th ACM Conference
    on Recommender Systems*. 299–315.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Geng et al. (2022) Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, 和 Yongfeng
    Zhang. 2022. 推荐作为语言处理 (RLP): 统一的预训练、个性化提示和预测范式 (P5)。在*第16届 ACM 推荐系统大会论文集*中，299–315。'
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. *Advances in neural information processing systems*
    27 (2014).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, 和 Yoshua Bengio. 2014.
    生成对抗网络。*神经信息处理系统进展* 27 (2014)。
- en: Jannach et al. (2021) Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, and Li
    Chen. 2021. A survey on conversational recommender systems. *ACM Computing Surveys
    (CSUR)* 54, 5 (2021), 1–36.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jannach et al. (2021) Dietmar Jannach, Ahtsham Manzoor, Wanling Cai, 和 Li Chen.
    2021. 对话推荐系统调查. *ACM计算机调查（CSUR）* 54, 5 (2021), 1–36.
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*
    (2021).
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. (2021) Brian Lester, Rami Al-Rfou, 和 Noah Constant. 2021. 参数高效提示调优的规模效应.
    *arXiv 预印本 arXiv:2104.08691* (2021).
- en: 'Li et al. (2022) Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti
    Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. 2022.
    Elevater: A benchmark and toolkit for evaluating language-augmented visual models.
    *Advances in Neural Information Processing Systems* 35 (2022), 9287–9301.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022) Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti
    Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee 等. 2022.
    Elevater: 评估语言增强视觉模型的基准和工具包. *神经信息处理系统进展* 35 (2022), 9287–9301.'
- en: Li et al. (2017) Li Li, Yi-Lun Lin, Dong-Pu Cao, Nan-Ning Zheng, and Fei-Yue
    Wang. 2017. Parallel learning-a new framework for machine learning. *Acta Automatica
    Sinica* 43, 1 (2017), 1–8.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2017) Li Li, Yi-Lun Lin, Dong-Pu Cao, Nan-Ning Zheng, 和 Fei-Yue Wang.
    2017. 并行学习——机器学习的新框架. *自动化学报* 43, 1 (2017), 1–8.
- en: Li et al. (2023a) Lei Li, Yongfeng Zhang, and Li Chen. 2023a. Personalized prompt
    learning for explainable recommendation. *ACM Transactions on Information Systems*
    41, 4 (2023), 1–26.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023a) Lei Li, Yongfeng Zhang, 和 Li Chen. 2023a. 可解释推荐的个性化提示学习. *ACM信息系统交易*
    41, 4 (2023), 1–26.
- en: Li et al. (2023b) Lei Li, Yongfeng Zhang, and Li Chen. 2023b. Prompt distillation
    for efficient llm-based recommendation. In *Proceedings of the 32nd ACM International
    Conference on Information and Knowledge Management*. 1348–1357.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023b) Lei Li, Yongfeng Zhang, 和 Li Chen. 2023b. 高效 LLM 基于提示的推荐.
    见于 *第32届ACM国际信息与知识管理会议论文集*. 1348–1357.
- en: 'Li et al. (2023c) Lei Li, Yongfeng Zhang, Dugang Liu, and Li Chen. 2023c. Large
    language models for generative recommendation: A survey and visionary discussions.
    *arXiv preprint arXiv:2309.01157* (2023).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023c) Lei Li, Yongfeng Zhang, Dugang Liu, 和 Li Chen. 2023c. 用于生成推荐的大型语言模型:
    一项调查与前瞻性讨论. *arXiv 预印本 arXiv:2309.01157* (2023).'
- en: Liu et al. (2024) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024.
    Visual instruction tuning. *Advances in neural information processing systems*
    36 (2024).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024) Haotian Liu, Chunyuan Li, Qingyang Wu, 和 Yong Jae Lee. 2024.
    视觉指令调优. *神经信息处理系统进展* 36 (2024).
- en: 'Liu et al. (2023) Peng Liu, Lemei Zhang, and Jon Atle Gulla. 2023. Pre-train,
    Prompt, and Recommendation: A Comprehensive Survey of Language Modeling Paradigm
    Adaptations in Recommender Systems. *Transactions of the Association for Computational
    Linguistics* 11 (2023), 1553–1571.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023) Peng Liu, Lemei Zhang, 和 Jon Atle Gulla. 2023. 预训练、提示与推荐:
    语言建模范式在推荐系统中的综合调查. *计算语言学协会会刊* 11 (2023), 1553–1571.'
- en: 'Lyu et al. (2023) Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, and Jiebo
    Luo. 2023. Llm-rec: Personalized recommendation via prompting large language models.
    *arXiv preprint arXiv:2307.15780* (2023).'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lyu et al. (2023) Hanjia Lyu, Song Jiang, Hanqing Zeng, Yinglong Xia, 和 Jiebo
    Luo. 2023. Llm-rec: 通过提示大型语言模型进行个性化推荐. *arXiv 预印本 arXiv:2307.15780* (2023).'
- en: 'Nash (1953) John Nash. 1953. Two-person cooperative games. *Econometrica: Journal
    of the Econometric Society* (1953), 128–140.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nash (1953) John Nash. 1953. 两人合作博弈. *经济学计量学杂志* (1953), 128–140.
- en: Shanahan (2024) Murray Shanahan. 2024. Talking about large language models.
    *Commun. ACM* 67, 2 (2024), 68–79.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shanahan (2024) Murray Shanahan. 2024. 论大型语言模型. *Commun. ACM* 67, 2 (2024),
    68–79.
- en: 'Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu
    Ou, and Peng Jiang. 2019. BERT4Rec: Sequential recommendation with bidirectional
    encoder representations from transformer. In *Proceedings of the 28th ACM international
    conference on information and knowledge management*. 1441–1450.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sun et al. (2019) Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu
    Ou, 和 Peng Jiang. 2019. BERT4Rec: 使用来自 Transformer 的双向编码器表示进行序列推荐. 见于 *第28届ACM国际信息与知识管理会议论文集*.
    1441–1450.'
- en: 'Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom,
    Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic.
    2022. Galactica: A large language model for science. *arXiv preprint arXiv:2211.09085*
    (2022).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taylor et al. (2022) Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom,
    Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, 和 Robert Stojnic.
    2022. Galactica: 一个用于科学的大型语言模型. *arXiv 预印本 arXiv:2211.09085* (2022).'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971* (2023).'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, 等. 2023. Llama：开放而高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971* (2023)。
- en: 'Wang et al. (2021) Lingzhi Wang, Huang Hu, Lei Sha, Can Xu, Kam-Fai Wong, and
    Daxin Jiang. 2021. RecInDial: A unified framework for conversational recommendation
    with pretrained language models. *arXiv preprint arXiv:2110.07477* (2021).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2021) Lingzhi Wang, Huang Hu, Lei Sha, Can Xu, Kam-Fai Wong, 和 Daxin
    Jiang. 2021. RecInDial：一个统一的对话推荐框架，基于预训练语言模型。*arXiv 预印本 arXiv:2110.07477* (2021)。
- en: Wang et al. (2022b) Xiaolei Wang, Kun Zhou, Ji-Rong Wen, and Wayne Xin Zhao.
    2022b. Towards unified conversational recommender systems via knowledge-enhanced
    prompt learning. In *Proceedings of the 28th ACM SIGKDD Conference on Knowledge
    Discovery and Data Mining*. 1929–1937.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2022b) Xiaolei Wang, Kun Zhou, Ji-Rong Wen, 和 Wayne Xin Zhao. 2022b.
    通过知识增强提示学习实现统一的对话推荐系统。载于 *第28届ACM SIGKDD知识发现与数据挖掘会议论文集*。1929–1937。
- en: Wang et al. (2022a) Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi
    Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. 2022a.
    Learning to prompt for continual learning. In *Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*. 139–149.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2022a) Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun,
    Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, 和 Tomas Pfister. 2022a. 学习提示以实现持续学习。载于
    *IEEE/CVF计算机视觉与模式识别会议论文集*。139–149。
- en: Wu et al. (2023) Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu,
    Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, et al. 2023. A survey
    on large language models for recommendation. *arXiv preprint arXiv:2305.19860*
    (2023).
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2023) Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia
    Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, 等. 2023. 大型语言模型在推荐中的综述。*arXiv
    预印本 arXiv:2305.19860* (2023)。
- en: Xi et al. (2023) Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu,
    Jieming Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, et al. 2023. Towards
    open-world recommendation with knowledge augmentation from large language models.
    *arXiv preprint arXiv:2306.10933* (2023).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xi 等 (2023) Yunjia Xi, Weiwen Liu, Jianghao Lin, Xiaoling Cai, Hong Zhu, Jieming
    Zhu, Bo Chen, Ruiming Tang, Weinan Zhang, Rui Zhang, 等. 2023. 通过大型语言模型的知识增强实现开放世界推荐。*arXiv
    预印本 arXiv:2306.10933* (2023)。
- en: Xia et al. (2015) Peipei Xia, Li Zhang, and Fanzhang Li. 2015. Learning similarity
    with cosine similarity ensemble. *Information sciences* 307 (2015), 39–52.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等 (2015) Peipei Xia, Li Zhang, 和 Fanzhang Li. 2015. 使用余弦相似度集成学习相似性。*信息科学*
    307 (2015), 39–52。
- en: 'Yang et al. (2023) Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang
    Huang, and Yanbin Lu. 2023. Palr: Personalization aware llms for recommendation.
    *arXiv preprint arXiv:2305.07622* (2023).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 (2023) Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang,
    和 Yanbin Lu. 2023. Palr：关注个性化的推荐大型语言模型。*arXiv 预印本 arXiv:2305.07622* (2023)。
- en: Yi et al. (2023) Zixuan Yi, Iadh Ounis, and Craig Macdonald. 2023. Contrastive
    graph prompt-tuning for cross-domain recommendation. *ACM Transactions on Information
    Systems* 42, 2 (2023), 1–28.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yi 等 (2023) Zixuan Yi, Iadh Ounis, 和 Craig Macdonald. 2023. 用于跨域推荐的对比图提示调优。*ACM信息系统交易*
    42, 2 (2023), 1–28。
- en: 'Yu et al. (2024) Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J Ratner,
    Ranjay Krishna, Jiaming Shen, and Chao Zhang. 2024. Large language model as attributed
    training data generator: A tale of diversity and bias. *Advances in Neural Information
    Processing Systems* 36 (2024).'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu 等 (2024) Yue Yu, Yuchen Zhuang, Jieyu Zhang, Yu Meng, Alexander J Ratner,
    Ranjay Krishna, Jiaming Shen, 和 Chao Zhang. 2024. 大型语言模型作为归因训练数据生成器：多样性与偏见的故事。*神经信息处理系统进展*
    36 (2024)。
- en: Zhai et al. (2023) Jianyang Zhai, Xiawu Zheng, Chang-Dong Wang, Hui Li, and
    Yonghong Tian. 2023. Knowledge prompt-tuning for sequential recommendation. In
    *Proceedings of the 31st ACM International Conference on Multimedia*. 6451–6461.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhai 等 (2023) Jianyang Zhai, Xiawu Zheng, Chang-Dong Wang, Hui Li, 和 Yonghong
    Tian. 2023. 知识提示调优用于序列推荐。载于 *第31届ACM国际多媒体会议论文集*。6451–6461。
- en: 'Zhang (2023) Gangyi Zhang. 2023. User-centric conversational recommendation:
    Adapting the need of user with large language models. In *Proceedings of the 17th
    ACM Conference on Recommender Systems*. 1349–1354.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang (2023) Gangyi Zhang. 2023. 用户中心的对话推荐：用大型语言模型适应用户需求。载于 *第17届ACM推荐系统会议论文集*。1349–1354。
- en: Zhang and Wang (2023) Zizhuo Zhang and Bang Wang. 2023. Prompt learning for
    news recommendation. In *Proceedings of the 46th International ACM SIGIR Conference
    on Research and Development in Information Retrieval*. 227–237.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张自卓和王邦. 2023. 新闻推荐的提示学习. 收录于*第46届国际ACM SIGIR信息检索研究与发展会议论文集*. 227–237.
- en: 'Zhou et al. (2020) Kun Zhou, Hui Wang, Wayne Xin Zhao, Yutao Zhu, Sirui Wang,
    Fuzheng Zhang, Zhongyuan Wang, and Ji-Rong Wen. 2020. S3-rec: Self-supervised
    learning for sequential recommendation with mutual information maximization. In
    *Proceedings of the 29th ACM international conference on information & knowledge
    management*. 1893–1902.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '周昆, 王辉, 韦恩·辛·赵, 朱雨涛, 王思瑞, 张福征, 王中原, 和温基荣. 2020. S3-rec: 通过互信息最大化进行序列推荐的自监督学习.
    收录于*第29届ACM国际信息与知识管理会议论文集*. 1893–1902.'
