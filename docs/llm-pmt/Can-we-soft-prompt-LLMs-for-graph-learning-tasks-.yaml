- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:46:46'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:46:46
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can we soft prompt LLMs for graph learning tasks?
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们能否对图学习任务进行软提示 LLM？
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10359](https://ar5iv.labs.arxiv.org/html/2402.10359)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10359](https://ar5iv.labs.arxiv.org/html/2402.10359)
- en: Zheyuan Liu University of Notre Dame  [zliu29@nd.edu](mailto:zliu29@nd.edu)
    ,  Xiaoxin He National University of Singapore  [he.xiaoxin@u.nus.edu](mailto:he.xiaoxin@u.nus.edu)
    ,  Yijun Tian University of Notre Dame  [yijun.tian@nd.edu](mailto:yijun.tian@nd.edu)
     and  Nitesh V. Chawla University of Notre Dame  [nchawla@nd.edu](mailto:nchawla@nd.edu)(2023)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘哲远 圣母大学 [zliu29@nd.edu](mailto:zliu29@nd.edu) ，  贺晓欣 新加坡国立大学 [he.xiaoxin@u.nus.edu](mailto:he.xiaoxin@u.nus.edu)
    ，  田一军 圣母大学 [yijun.tian@nd.edu](mailto:yijun.tian@nd.edu)  和  内特什·V·查瓦拉 圣母大学 [nchawla@nd.edu](mailto:nchawla@nd.edu)(2023)
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Graph plays an important role in representing complex relationships in real-world
    applications such as social networks, biological data and citation networks. In
    recent years, Large Language Models (LLMs) have achieved tremendous success in
    various domains, which makes applying LLMs to graphs particularly appealing. However,
    directly applying LLMs to graph modalities presents unique challenges due to the
    discrepancy and mismatch between the graph and text modalities. Hence, to further
    investigate LLMs’ potential for comprehending graph information, we introduce
    GraphPrompter, a novel framework designed to align graph information with LLMs
    via soft prompts. Specifically, GraphPrompter consists of two main components:
    a graph neural network to encode complex graph information and an LLM that effectively
    processes textual information. Comprehensive experiments on various benchmark
    datasets under node classification and link prediction tasks demonstrate the effectiveness
    of our proposed method. The GraphPrompter framework unveils the substantial capabilities
    of LLMs as predictors in graph-related tasks, enabling researchers to utilize
    LLMs across a spectrum of real-world graph scenarios more effectively.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图在表示现实世界应用中的复杂关系中扮演着重要角色，例如社交网络、生物数据和引文网络。近年来，大型语言模型（LLMs）在各个领域取得了巨大的成功，这使得将
    LLMs 应用于图形特别具有吸引力。然而，由于图和文本模态之间存在差异和不匹配，直接将 LLMs 应用于图模态会带来独特的挑战。因此，为了进一步探讨 LLMs
    理解图信息的潜力，我们引入了 GraphPrompter，这是一个新颖的框架，旨在通过软提示将图信息与 LLMs 对齐。具体而言，GraphPrompter
    由两个主要组件组成：一个图神经网络，用于编码复杂的图信息，以及一个有效处理文本信息的 LLM。对各种基准数据集在节点分类和链路预测任务上的全面实验展示了我们提出的方法的有效性。GraphPrompter
    框架揭示了 LLMs 作为图相关任务预测器的巨大能力，使研究人员能够更有效地利用 LLMs 处理各种现实世界的图形场景。
- en: 'Large Language Models, Graph Neural Networks, Natural Language Processing,
    Graph Representation Learning^†^†copyright: acmcopyright^†^†journalyear: 2023^†^†copyright:
    none^†^†conference: The Web Conference 2024; May 13 - 17, 2024; Singapore'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型、图神经网络、自然语言处理、图表示学习^†^†版权所有：acmcopyright^†^†期刊年份：2023^†^†版权：无^†^†会议：The
    Web Conference 2024；2024年5月13日至17日；新加坡
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Large Language Models (LLMs)  (Brown et al., [2020](#bib.bib3); Chowdhery et al.,
    [2023](#bib.bib6); Touvron et al., [2023](#bib.bib15); Radford et al., [2019](#bib.bib14);
    Ouyang et al., [2022](#bib.bib13); Lewkowycz et al., [2022](#bib.bib9)) has demonstrated
    significant success across various domains, largely attributed to their extensive
    knowledge memorized during the pretraining phase and their exceptional ability
    to generalize during the fine-tuning process on diverse textual datasets  (Hoffmann
    et al., [2022](#bib.bib8); Webson and Pavlick, [2021](#bib.bib17); Liang et al.,
    [2022](#bib.bib10); Carlini et al., [2022](#bib.bib4)). This success has spurred
    interest in combining graph neural networks (GNNs) with LLMs to enhance their
    capabilities in understanding and modeling graphs (Yang et al., [2021](#bib.bib19);
    Zhang et al., [2022a](#bib.bib21), [b](#bib.bib23); Ostendorff et al., [2022](#bib.bib12)),
    including implementing LLMs as encoders to process features within GNNs (Chai
    et al., [2023](#bib.bib5); Mavromatis et al., [2023](#bib.bib11); He et al., [2023](#bib.bib7);
    Yu et al., [2023](#bib.bib20)), and employing LLMs as aligners with GNNs to enhance
    performance (Zhang et al., [2021](#bib.bib22); Zhao et al., [2022](#bib.bib24);
    Wen and Fang, [2023](#bib.bib18); Brannon et al., [2023](#bib.bib2)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）(Brown et al., [2020](#bib.bib3); Chowdhery et al., [2023](#bib.bib6);
    Touvron et al., [2023](#bib.bib15); Radford et al., [2019](#bib.bib14); Ouyang
    et al., [2022](#bib.bib13); Lewkowycz et al., [2022](#bib.bib9)) 已在多个领域取得了显著成功，这主要归功于它们在预训练阶段记忆的广泛知识以及在多样化文本数据集上的微调过程中出色的泛化能力
    (Hoffmann et al., [2022](#bib.bib8); Webson and Pavlick, [2021](#bib.bib17); Liang
    et al., [2022](#bib.bib10); Carlini et al., [2022](#bib.bib4))。这一成功激发了将图神经网络（GNNs）与LLMs结合的兴趣，以增强它们在理解和建模图方面的能力
    (Yang et al., [2021](#bib.bib19); Zhang et al., [2022a](#bib.bib21), [b](#bib.bib23);
    Ostendorff et al., [2022](#bib.bib12))，包括将LLMs作为编码器处理GNN中的特征 (Chai et al., [2023](#bib.bib5);
    Mavromatis et al., [2023](#bib.bib11); He et al., [2023](#bib.bib7); Yu et al.,
    [2023](#bib.bib20))，以及将LLMs作为对齐器与GNNs配合以提高性能 (Zhang et al., [2021](#bib.bib22);
    Zhao et al., [2022](#bib.bib24); Wen and Fang, [2023](#bib.bib18); Brannon et
    al., [2023](#bib.bib2))。
- en: However, directly applying LLMs to graph modalities presents unique challenges
    due to the discrepancy and mismatch between the graph and text modalities. For
    example, existing works primarily map the graph into text, ignoring the irrelevant
    information and noises pertinent to the graphs. This oversight results in an inadequate
    understanding of crucial structural knowledge within the graphs. Furthermore,
    the challenge is amplified when dealing with large graphs containing thousands
    or millions of nodes and edges, as this complexity hinders LLMs’ ability to grasp
    intricate structural information.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，直接将LLMs应用于图模态面临独特的挑战，因为图模态和文本模态之间存在差异和不匹配。例如，现有工作主要将图映射到文本中，忽略了与图相关的无关信息和噪声。这种忽视导致对图中的关键结构知识理解不够。此外，当处理包含成千上万或百万个节点和边的大型图时，这一挑战更加突出，因为这种复杂性阻碍了LLMs
    grasping 复杂的结构信息。
- en: 'These limitations motivate our investigation into using a graph as a soft prompt
    encoded by GNNs for LLMs in graph learning tasks. The intuition behind this approach
    is that GNN are more proficient in aggregating and transforming information from
    the neighbourhood information, which could provide topological information to
    LLMs. Additionally, it can guide the LLM in selecting relevant information from
    textual input and control the generation process for token generation. Specifically,
    in this work, we aim to examine the efficacy of a soft graph prompt in informing
    the LLM’s predictions. Hence, a natural yet pivotal research question arise: Can
    we soft prompt LLMs for graph leanring tasks?'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制促使我们研究将图作为由GNN编码的软提示用于LLM在图学习任务中的应用。这个方法的直觉是，GNN在聚合和转换邻域信息方面更为熟练，这可以向LLM提供拓扑信息。此外，它可以指导LLM从文本输入中选择相关信息，并控制生成过程中的标记生成。具体来说，在这项工作中，我们旨在检验软图提示在指导LLM预测中的有效性。因此，一个自然且关键的研究问题出现了：我们能否使用软提示来指导LLM在图学习任务中的应用？
- en: To answer this question, we introduce GraphPrompter, a novel framework that
    combines the strength of GNNs and LLMs to process and comprehend graph-structured
    data. In particular, GraphPrompter capitalizes on the frozen LLM as a robust feature
    extractor that taps into its vast pretraining knowledge, thus enabling us to avoid
    extensive task-specific fine-tuning. In parallel, the GNN works on the graph to
    produce node embeddings, which are later concatenated with a prompt instruction
    to guide LLMs for graph learning tasks. The LLM, due to its powerful autoregressive
    nature, generates a language response based on the fused graph and text information,
    effectively turning the LLM into a powerful tool for graph understanding tasks.
    See Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Can we soft prompt LLMs for
    graph learning tasks?") for an illustration.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们引入了 GraphPrompter，一个结合 GNN 和 LLM 优势的框架，用于处理和理解图结构数据。特别地，GraphPrompter
    利用冻结的 LLM 作为强大的特征提取器，借助其广泛的预训练知识，从而避免了大量的任务特定微调。同时，GNN 在图上工作以生成节点嵌入，这些嵌入随后与提示指令连接，以指导
    LLM 进行图学习任务。由于 LLM 的强大自回归特性，它基于融合的图和文本信息生成语言响应，从而有效地将 LLM 转变为强大的图理解工具。请参见图 [1](#S1.F1
    "图 1 ‣ 1. 引言 ‣ 我们能否通过软提示来进行 LLM 的图学习任务？") 以获取示意图。
- en: 'This hybrid approach is particularly suitable for graphs with textual attributes
    (i.e., textual graphs), which requires an understanding of both the textual content
    and the graph structure, such as identifying the subcategory of an academic paper
    based on its citation network and abstract. We experimentally demonstrate that
    our framework is capable of prompting LLMs for graph learning tasks on five benchmark
    datasets under node classification and link prediction tasks. It showcases the
    potential for significant advancements in the use of LLMs for complex data structures
    beyond traditional text, opening up new avenues for research and application in
    the realm of AI assistants capable of intricate graph comprehension. Our main
    contributions are as follows ¹¹1The code is avilable at [https://github.com/franciscoliu/graphprompter](https://github.com/franciscoliu/graphprompter).:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种混合方法特别适用于具有文本属性的图（即，文本图），这些图需要理解文本内容和图结构，例如，根据学术论文的引用网络和摘要识别其子类别。我们通过实验展示了我们的框架能够在节点分类和链接预测任务中，使用五个基准数据集对
    LLM 进行图学习任务的提示。它展示了在超越传统文本的数据结构中使用 LLM 的巨大潜力，为能够进行复杂图理解的 AI 助手开辟了新的研究和应用领域。我们的主要贡献如下¹¹1代码可在
    [https://github.com/franciscoliu/graphprompter](https://github.com/franciscoliu/graphprompter)
    找到。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To the best of our knowledge, this is the very first work investigating whether
    LLMs can understand graph learning tasks via soft prompting.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，这项工作是首个研究 LLM 是否可以通过软提示理解图学习任务的研究。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose GraphPrompter, a novel plug-and-play framework that first employ
    GNN to get node representations from the textual graph. Then the obtained embeddings
    are concatenated with a prompt instruction to guide LLMs for graph learning tasks.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 GraphPrompter，这是一个新颖的即插即用框架，首先使用 GNN 从文本图中获取节点表示。然后，将获得的嵌入与提示指令连接，以指导
    LLM 进行图学习任务。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Extensive experiments demonstrate the effectiveness of our proposed framework
    under both node classification and link prediction tasks across various graph
    benchmarks.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 广泛的实验证明了我们提出的框架在各种图基准测试中的节点分类和链接预测任务中的有效性。
- en: '![Refer to caption](img/9c822a050e85632b9064b633963e6390.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/9c822a050e85632b9064b633963e6390.png)'
- en: Figure 1. Illustration of the proposed GraphPrompter for node classification
    task. The process involves extracting a k-hop subgraph for each node, feeding
    it into a GNN followed by a projection layer. Simultaneously, the textual attributes
    associated with each node are processed by the text embedder. The resulting node
    embedding is then concatenated with text embeddings, serving as a soft promt to
    guide the LLM for graph learning tasks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. 提出了用于节点分类任务的 GraphPrompter 的示意图。该过程涉及为每个节点提取一个 k-hop 子图，将其输入到 GNN 中，然后通过一个投影层。同时，与每个节点相关的文本属性由文本嵌入器处理。生成的节点嵌入随后与文本嵌入连接，作为软提示来指导
    LLM 进行图学习任务。
- en: 2\. Method
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 方法
- en: To enhance the alignment of graph knowledge with LLM, we introduce GraphPrompter,
    a plug-and-play pipeline that fuses post-processed node embeddings with LLMs.
    This integration is designed to leverage the rich semantic context LLMs provide,
    enhancing the interpretability and utility of graph representations, as illustrated
    in Figure  [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Can we soft prompt LLMs for
    graph learning tasks?").
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增强图知识与LLM的对齐，我们引入了GraphPrompter，一个即插即用的管道，它将后处理的节点嵌入与LLM融合。这种集成旨在利用LLM提供的丰富语境，提升图表示的可解释性和实用性，如图
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Can we soft prompt LLMs for graph learning
    tasks?") 所示。
- en: 2.1\. Graph Section
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 图部分
- en: 'A graph can be represented as $G=(V,E)$, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图可以表示为 $G=(V,E)$，如下所示：
- en: '| (1) |  | $X_{i}=\text{GNN}(G_{s_{i}})\in\mathbb{R}^{d_{g}},$ |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $X_{i}=\text{GNN}(G_{s_{i}})\in\mathbb{R}^{d_{g}},$ |  |'
- en: where $d_{g}$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{g}$。
- en: 'We employ a projection layer, specifically, a multilayer perceptron (MLP),
    to align the node embedding with the vector space of the LLM:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用一个投影层，特别是一个多层感知器（MLP），将节点嵌入与LLM的向量空间对齐：
- en: '| (2) |  | $\hat{X}_{i}=\text{MLP}(X_{i})\in\mathbb{R}^{d_{l}},$ |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\hat{X}_{i}=\text{MLP}(X_{i})\in\mathbb{R}^{d_{l}},$ |  |'
- en: where $d_{l}$ is the hidden dimension of the LLM.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{l}$ 是LLM的隐藏维度。
- en: 'Both the GNN and the projection layer are trained in an end-to-end manner,
    ensuring that the embeddings $X_{i}$ are discriminative for the downstream task.
    The rationale behind this approach is twofold: first, to leverage the GNN’s inherent
    capacity to aggregate and transform local neighborhood information into meaningful
    embeddings; and second, to enable the subsequent integration with the LLM, which
    interprets these embeddings as a soft prompt in conjunction with textual data.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: GNN和投影层都以端到端的方式进行训练，确保嵌入 $X_{i}$ 对下游任务具有区分性。这种方法的理由有两个：首先，利用GNN固有的能力，将局部邻域信息聚合和转换为有意义的嵌入；其次，便于随后与LLM的集成，LLM将这些嵌入视为与文本数据结合的软提示。
- en: 2.2\. LLM Section
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. LLM部分
- en: 'After encoding the graph structure, the GraphPrompter proceeds to process the
    textual information associated with each node. This is where the power of the
    LLM comes into play, as it is kept frozen for efficient fine-tuning while preserving
    the LLM’s pre-trained knowledge. For each node $v_{i}$ using the frozen tokenizer
    of the LLM as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码图结构后，GraphPrompter 继续处理与每个节点相关的文本信息。在这里，LLM的力量发挥作用，因为它保持冻结状态以进行高效的微调，同时保留LLM的预训练知识。对于每个节点
    $v_{i}$ 使用LLM的冻结分词器，如下所示：
- en: '| (3) |  | $\displaystyle T_{\text{tokens}}=\text{Tokenizer}(T_{i}).$ |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\displaystyle T_{\text{tokens}}=\text{Tokenizer}(T_{i}).$ |  |'
- en: 'The tokenizer converts the text into a sequence of discrete tokens $T_{\text{tokens}}$
    in the LLM’s vocabulary. Subsequently, these tokens are then embedded into a continuous
    space:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器将文本转换为LLM词汇中的离散符号序列 $T_{\text{tokens}}$。随后，这些符号被嵌入到一个连续空间中：
- en: '| (4) |  | $\displaystyle T_{\text{emb}}=\text{Embed}(T_{\text{tokens}})\in\mathbb{R}^{M\times
    d_{l}},$ |  |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle T_{\text{emb}}=\text{Embed}(T_{\text{tokens}})\in\mathbb{R}^{M\times
    d_{l}},$ |  |'
- en: where $M$ is the hidden dimension of the LLM. These embeddings are designed
    to capture the semantic meaning of the text, complementing the structural information
    provided by the GNN.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $M$ 是LLM的隐藏维度。这些嵌入旨在捕捉文本的语义意义，补充GNN提供的结构信息。
- en: Next, we concatenate the node embeddings and the text embeddings, denoted as
    $[\hat{X}_{i},T_{\text{emb}}]$, which will go through the self-attention layers
    in the LLM as usual.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将节点嵌入和文本嵌入进行连接，记作 $[\hat{X}_{i},T_{\text{emb}}]$，这些嵌入将像往常一样经过LLM中的自注意力层。
- en: 'The motivation for this step is to ensure that the LLM can process the rich
    semantic content of the text alongside the graph embeddings. By encoding the text
    attributes into a compatible format, we aim to capitalize on the LLM’s advanced
    understanding of natural language, which is crucial for tasks that require a combination
    of structural and textual data interpretation, such as node classification in
    citation networks. Here, we point out the key distinction between node classification
    and link prediction: node classification evaluates information from a single node,
    whereas link prediction considers the attributes of two nodes, specifically their
    origin and destination.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步骤的动机是确保 LLM 能够处理文本的丰富语义内容以及图嵌入。通过将文本属性编码成兼容格式，我们旨在利用 LLM 对自然语言的高级理解，这对于需要结构和文本数据解释相结合的任务（例如引用网络中的节点分类）至关重要。在这里，我们指出节点分类与链接预测之间的主要区别：节点分类评估单个节点的信息，而链接预测则考虑两个节点的属性，具体包括它们的起点和终点。
- en: 3\. Experiments
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 实验
- en: '![Refer to caption](img/537960850a2d241a4ff0dce73e1e3497.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/537960850a2d241a4ff0dce73e1e3497.png)'
- en: (a) Dense (Title + Abstract)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 密集（标题 + 摘要）
- en: '![Refer to caption](img/205553c280e6c6de7fa71849e109774f.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/205553c280e6c6de7fa71849e109774f.png)'
- en: (b) Sparse (Title Only)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 稀疏（仅标题）
- en: Figure 2. Node classification experiments comparing the proposed GraphPrompter
    with baseline methods (i.e., soft prompt tuning and fine-tuning). The $x$ axis
    displays accuracy scores. Figure [2(a)](#S3.F2.sf1 "In Figure 2 ‣ 3\. Experiments
    ‣ Can we soft prompt LLMs for graph learning tasks?") illustrates a dense semantic
    setting including both the title and abstract of a paper within the node embeddings,
    while Figure [2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3\. Experiments ‣ Can we soft prompt
    LLMs for graph learning tasks?") illustrates a sparse semantic setting with title
    only.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2. 节点分类实验，比较了提出的 GraphPrompter 与基线方法（即软提示调整和微调）。$x$ 轴显示了准确率得分。图 [2(a)](#S3.F2.sf1
    "在图 2 ‣ 3\. 实验 ‣ 我们能否对图学习任务进行软提示 LLM？") 说明了一个密集的语义设置，包括节点嵌入中的论文标题和摘要，而图 [2(b)](#S3.F2.sf2
    "在图 2 ‣ 3\. 实验 ‣ 我们能否对图学习任务进行软提示 LLM？") 说明了一个仅包含标题的稀疏语义设置。
- en: 'In this section, we conduct extensive experiments to validate the effectiveness
    of the GraphPrompter. Specifically, our experiments aim to address the central
    question: Can we soft prompt LLMs for graph learning tasks?'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行广泛的实验以验证 GraphPrompter 的有效性。具体来说，我们的实验旨在解决核心问题：我们能否对图学习任务进行软提示 LLM？
- en: 3.1\. Experiment setups
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 实验设置
- en: 3.1.1\. Datasets and baseline models.
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 数据集和基线模型。
- en: 'Our experiments mainly focus on node classification and link prediction using
    various graph benchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv, and Ogbn-products.
    For Ogbn-products, given its substantial scale of 2 million nodes and 61 million
    edges, we have employed a node sampling strategy to obtain a subgraph containing
    54k nodes and 74k edges. We compare our approach with three kinds of baseline
    models: 1) Pure GNN; 2) Frozen LLM, which includes zero shot and soft prompt tuning;
    and 3) Tuned LLM with LoRA. We employ the LLAMA2-7B (Touvron et al., [2023](#bib.bib15))
    as the LLM backbone. For the pure GNN method, we use GAT (Veličković et al., [2017](#bib.bib16)).
    In the LLM-frozen setting, for zero-shot, we use a frozen LLM for direct node
    classification/edge prediction using textual attributes of the nodes. In soft
    prompt tuning, we keep the parameters of the LLM frozen and adjust only the prompt.
    In LLM-tuned, the original LLM parameters are updated for downstream tasks by
    utilizing LoRA.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验主要集中在使用各种图基准数据集进行节点分类和链接预测：Cora、Citeseer、Pubmed、Ogbn-arxiv 和 Ogbn-products。对于
    Ogbn-products，由于其包含 200 万节点和 6100 万边的规模庞大，我们采用了节点采样策略以获取一个包含 54k 节点和 74k 边的子图。我们将我们的方法与三种基线模型进行比较：1)
    纯 GNN；2) 冻结的 LLM，包括零样本和软提示调整；3) 使用 LoRA 的调优 LLM。我们使用 LLAMA2-7B（Touvron et al.,
    [2023](#bib.bib15)）作为 LLM 骨干。对于纯 GNN 方法，我们使用 GAT（Veličković et al., [2017](#bib.bib16)）。在
    LLM 冻结设置中，对于零样本，我们使用冻结的 LLM 直接进行节点分类/边预测，利用节点的文本属性。在软提示调整中，我们保持 LLM 的参数冻结，仅调整提示。在
    LLM 调优中，通过利用 LoRA 更新原始 LLM 参数以适应下游任务。
- en: 3.1.2\. Implementation Details.
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 实现细节。
- en: We report the mean of five independent runs with different seeds. The experiments
    are conducted on 2 A100 GPUs (80GB).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们报告了五次独立运行的均值，每次运行使用不同的种子。实验在 2 个 A100 GPU（80GB）上进行。
- en: 4\. Experiments Result
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验结果
- en: To answer the central question, we conduct experiments across various graph
    benchmark to comprehensively evaluate the effectiveness of GraphPrompter. The
    performance is reported in Table  [1](#S4.T1 "Table 1 ‣ 4.1\. Node Classification
    Task. ‣ 4\. Experiments Result ‣ Can we soft prompt LLMs for graph learning tasks?")
    and Table  [2](#S4.T2 "Table 2 ‣ 4.1\. Node Classification Task. ‣ 4\. Experiments
    Result ‣ Can we soft prompt LLMs for graph learning tasks?").
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答核心问题，我们在各种图基准上进行实验，以全面评估 GraphPrompter 的有效性。性能结果在表格 [1](#S4.T1 "Table 1
    ‣ 4.1\. Node Classification Task. ‣ 4\. Experiments Result ‣ Can we soft prompt
    LLMs for graph learning tasks?") 和表格 [2](#S4.T2 "Table 2 ‣ 4.1\. Node Classification
    Task. ‣ 4\. Experiments Result ‣ Can we soft prompt LLMs for graph learning tasks?")
    中报告。
- en: 4.1\. Node Classification Task.
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 节点分类任务。
- en: In Table [1](#S4.T1 "Table 1 ‣ 4.1\. Node Classification Task. ‣ 4\. Experiments
    Result ‣ Can we soft prompt LLMs for graph learning tasks?"), we present the results
    of node classification experiments conducted on a variety of benchmark datasets,
    where both title and abstract information are provided to LLM alongside detailed
    instructions. As it can be seen from the table, the integration of the proposed
    method (GraphPrompter) with LoRA consistently surpasses the performance of other
    baseline methods across diverse benchmarks. In particular, GraphPrompter with
    LoRA in the PubMed and Citeseer datasets with top accuracies of 94.80 % and 73.61
    %, respectively. The naive fine-tuning with LoRA approach typically appears as
    the runner-up and has small performance gap with GraphPrompter. Though subgraph
    prompt tuning approach has relatively close accuracy score with naive fine-tuning,
    it still not as effective as fine-tuning on node classification task. Furthermore,
    we observe that zero-shot generally ranks lowest in performance across all benchmark
    datasets, with the exception of PubMed dataset. This reflects the limitation of
    LLMs in capturing graph knowledge without the auxiliary processing provided by
    GNNs. Additionally, two distinct experimental setups are illustrated in Figure
    [2](#S3.F2 "Figure 2 ‣ 3\. Experiments ‣ Can we soft prompt LLMs for graph learning
    tasks?"), where Figure [2(a)](#S3.F2.sf1 "In Figure 2 ‣ 3\. Experiments ‣ Can
    we soft prompt LLMs for graph learning tasks?") encompasses both title and abstract
    within the node embeddings, and Figure [2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3\. Experiments
    ‣ Can we soft prompt LLMs for graph learning tasks?") restricts the node embeddings
    to only the title. The comparative analysis illustrated in the figures further
    illustrates the superior average performance of the proposed method (GraphPrompter),
    which combines subgraph prompt tuning and GraphPrompter, in comparison to other
    baseline approaches, specifically soft prompt tuning and fine-tuning methodologies.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在表格 [1](#S4.T1 "Table 1 ‣ 4.1\. Node Classification Task. ‣ 4\. Experiments
    Result ‣ Can we soft prompt LLMs for graph learning tasks?") 中，我们展示了在多种基准数据集上进行的节点分类实验结果，其中标题和摘要信息与详细说明一同提供给
    LLM。从表中可以看出，所提出的方法（GraphPrompter）与 LoRA 的结合在各种基准测试中 consistently 超过了其他基线方法的性能。特别是，GraphPrompter
    与 LoRA 在 PubMed 和 Citeseer 数据集上的准确率分别达到了 94.80% 和 73.61%。采用 LoRA 的简单微调方法通常作为亚军出现，并且与
    GraphPrompter 的性能差距很小。尽管子图提示调优方法的准确率与简单微调相对接近，但在节点分类任务上的效果仍不如微调。此外，我们观察到，零-shot
    在所有基准数据集中通常表现最差，PubMed 数据集除外。这反映了 LLM 在没有 GNN 提供的辅助处理的情况下捕捉图知识的局限性。此外，图 [2](#S3.F2
    "Figure 2 ‣ 3\. Experiments ‣ Can we soft prompt LLMs for graph learning tasks?")
    中展示了两种不同的实验设置，其中图 [2(a)](#S3.F2.sf1 "In Figure 2 ‣ 3\. Experiments ‣ Can we soft
    prompt LLMs for graph learning tasks?") 包括了节点嵌入中的标题和摘要，而图 [2(b)](#S3.F2.sf2 "In
    Figure 2 ‣ 3\. Experiments ‣ Can we soft prompt LLMs for graph learning tasks?")
    将节点嵌入限制为仅标题。图中的比较分析进一步说明了所提出的方法（GraphPrompter）结合子图提示调优的平均性能优于其他基线方法，特别是软提示调优和微调方法。
- en: '| Method | Cora | Citeseer | Pubmed | Ogbn-arxiv | Ogbn-products |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Cora | Citeseer | Pubmed | Ogbn-arxiv | Ogbn-products |'
- en: '| Node Classification Accuracy (%) ${\color[rgb]{0,0,1}\uparrow}$ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 节点分类准确率 (%) ${\color[rgb]{0,0,1}\uparrow}$ |'
- en: '| GAT | 84.69 | 70.78 | 84.09 | 71.82 | 70.52 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| GAT | 84.69 | 70.78 | 84.09 | 71.82 | 70.52 |'
- en: '| Zero-Shot | 43.31 | 29.22 | 91.39 | 44.23 | 15.05 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot | 43.31 | 29.22 | 91.39 | 44.23 | 15.05 |'
- en: '| Soft Prompt Tuning | 70.31 | 70.97 | 91.45 | 71.99 | 75.14 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 软提示调优 | 70.31 | 70.97 | 91.45 | 71.99 | 75.14 |'
- en: '| Fine-tuning + LoRA | 75.97 | 73.45 | 94.68 | 74.58 | 78.99 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 微调 + LoRA | 75.97 | 73.45 | 94.68 | 74.58 | 78.99 |'
- en: '| Subgraph Prompt Tuning | 80.17 | 72.29 | 93.84 | 75.04 | 75.30 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 子图提示调优 | 80.17 | 72.29 | 93.84 | 75.04 | 75.30 |'
- en: '| GraphPrompter + LoRA | 80.26 | 73.61 | 94.80 | 75.61 | 79.54 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GraphPrompter + LoRA | 80.26 | 73.61 | 94.80 | 75.61 | 79.54 |'
- en: 'Table 1. Node classification result of our proposed GraphPrompter, with a number
    of baselines under various graph benchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv
    and Ogbn-products. For each model, we present the mean accuracy from five independent
    runs, with both the title and abstract of papers provided as input along with
    specific instructions. Bold indicates the best performance and underline indicates
    the runner-up.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. 我们提出的 GraphPrompter 的节点分类结果，展示了在各种图基准下的若干基线模型：Cora、Citeseer、Pubmed、Ogbn-arxiv
    和 Ogbn-products。对于每个模型，我们展示了五次独立运行的平均准确率，这些运行提供了论文的标题和摘要作为输入，并附有具体的说明。粗体表示最佳性能，下划线表示亚军。
- en: '| Method | Cora | Citeseer | Pubmed | Ogbn-arxiv | Ogbn-products |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Cora | Citeseer | Pubmed | Ogbn-arxiv | Ogbn-products |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Link Prediction Accuracy (%) ${\color[rgb]{0,0,1}\uparrow}$ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 链接预测准确率 (%) ${\color[rgb]{0,0,1}\uparrow}$ |'
- en: '| --- |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| GAT | 90.71 | 87.40 | 86.18 | 72.93 | 65.67 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| GAT | 90.71 | 87.40 | 86.18 | 72.93 | 65.67 |'
- en: '| Zero-Shot | 21.94 | 7.69 | 10.23 | 34.41 | 7.47 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 | 21.94 | 7.69 | 10.23 | 34.41 | 7.47 |'
- en: '| Soft Prompt Tuning | 86.77 | 88.87 | 83.98 | 69.13 | 62.78 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 软提示调优 | 86.77 | 88.87 | 83.98 | 69.13 | 62.78 |'
- en: '| Fine-tuning + LoRA | 87.58 | 87.91 | 81.33 | 70.10 | 67.31 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 微调 + LoRA | 87.58 | 87.91 | 81.33 | 70.10 | 67.31 |'
- en: '| Subgraph Prompt Tuning | 89.15 | 93.49 | 87.20 | 75.28 | 68.18 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 子图提示调优 | 89.15 | 93.49 | 87.20 | 75.28 | 68.18 |'
- en: '| GraphPrompter + LoRA | 90.10 | 91.67 | 86.49 | 73.21 | 69.55 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| GraphPrompter + LoRA | 90.10 | 91.67 | 86.49 | 73.21 | 69.55 |'
- en: 'Table 2. Link prediction result of our proposed GraphPrompter, with a number
    of baselines under various graph benchmarks: Cora, Citeseer, Pubmed, Ogbn-arxiv
    and Ogbn-products. For each model, we present the mean accuracy derived from five
    independent runs conducted on the link prediction task, with only the abstract
    of papers are provided as input. Bold indicates the best performance and underline
    indicates the runner-up.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2. 我们提出的 GraphPrompter 的链接预测结果，展示了在各种图基准下的若干基线模型：Cora、Citeseer、Pubmed、Ogbn-arxiv
    和 Ogbn-products。对于每个模型，我们展示了从五次独立运行中得出的平均准确率，这些运行仅提供了论文的摘要作为输入。粗体表示最佳性能，下划线表示亚军。
- en: 4.2\. Link Prediction Task.
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 链接预测任务。
- en: In Table [2](#S4.T2 "Table 2 ‣ 4.1\. Node Classification Task. ‣ 4\. Experiments
    Result ‣ Can we soft prompt LLMs for graph learning tasks?"), we present the results
    of link prediction experiments conducted on a variety of benchmark datasets, where
    only the title is provided to LLM. As seen in Table  [2](#S4.T2 "Table 2 ‣ 4.1\.
    Node Classification Task. ‣ 4\. Experiments Result ‣ Can we soft prompt LLMs for
    graph learning tasks?"), subgraph prompt tuning approach consistently surpasses
    baseline models in link prediction tasks on various benchmarks. It shows remarkable
    performance especially on the Citeseer dataset, achieving a 93.49 % accuracy rate.
    GraphPrompter + LoRA is usually the runner-up model for the majority case. Notably,
    similar to node classification task, the Zero-Shot approach generally scores lowest,
    underscoring the necessity of GNN integration for LLMs to effectively process
    and understand graph data. It is notable that under both node classification and
    link prediction, GraphPrompter can continue outperforming traditional GNN and
    soft prompting techniques. This observation indicates that GraphPrompter, is particularly
    adept at capturing and utilizing the complex intersection between graph components
    and textual information, leading to a better performance on various graph related
    tasks.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 [2](#S4.T2 "表 2 ‣ 4.1\. 节点分类任务 ‣ 4\. 实验结果 ‣ 我们能否通过软提示让 LLM 进行图学习任务？") 中，我们展示了在各种基准数据集上进行的链接预测实验结果，其中仅提供了标题作为
    LLM 的输入。如表 [2](#S4.T2 "表 2 ‣ 4.1\. 节点分类任务 ‣ 4\. 实验结果 ‣ 我们能否通过软提示让 LLM 进行图学习任务？")
    所示，子图提示调优方法在各种基准上的链接预测任务中始终优于基线模型。它在 Citeseer 数据集上表现尤为出色，达到了 93.49% 的准确率。GraphPrompter
    + LoRA 通常是大多数情况下的亚军模型。值得注意的是，与节点分类任务类似，零样本方法通常得分最低，突显了 GNN 集成对于 LLM 有效处理和理解图数据的必要性。值得一提的是，无论是在节点分类还是链接预测任务中，GraphPrompter
    都能持续超越传统 GNN 和软提示技术。这一观察表明，GraphPrompter 特别擅长捕捉和利用图组件与文本信息之间的复杂交集，从而在各种图相关任务中表现更好。
- en: 5\. Analysis
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 分析
- en: 'The experiments on benchmark datasets for node classification and link prediction
    tasks demonstrate LLMs’ adaptability and efficiency in graph learning, enabled
    by our proposed soft graph prompting strategies. Here, we summarize the key findings:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在节点分类和链路预测任务的基准数据集上的实验展示了 LLMs 在图学习中的适应性和效率，这是通过我们提出的软图提示策略实现的。在这里，我们总结了关键发现：
- en: •
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Performance Benchmark: Overall, our proposed method ranks either as the best
    or runner-up across all benchmark datasets for both tasks. This consistently high
    performance affirms a positive response to our central question: we can effectively
    soft prompt for LLMs in graph learning tasks.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能基准：总体而言，我们提出的方法在所有基准数据集中对两个任务的排名要么是最佳，要么是亚军。这种 consistently 高的表现确认了我们核心问题的积极回应：我们可以有效地对
    LLMs 进行软提示，以支持图学习任务。
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Methodological Superiority: The fine-tuning approach, while competitive, displayed
    a marginal performance gap when compared to GraphPrompter. This indicates that
    while traditional fine-tuning remains effective, the specialized soft graph prompt
    tuning strategy designed for graph data offers a more potent solution for enhancing
    LLM’s graph learning capabilities.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法论优势：尽管微调方法具有竞争力，但与 GraphPrompter 相比表现出轻微的性能差距。这表明，虽然传统的微调仍然有效，但为图数据设计的专门软图提示微调策略提供了增强
    LLM 图学习能力的更有效解决方案。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Challenges with Zero-Shot Learning: The zero-shot approach consistently ranked
    the lowest across all benchmarks for both tasks. This reflects the challenges
    LLMs face in comprehending graph structure and semantics, highlighting the necessity
    for tailored approaches like GraphPrompter in graph learning tasks.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零样本学习的挑战：零样本方法在所有任务的基准测试中 consistently 排名最低。这反映了大型语言模型在理解图结构和语义方面面临的挑战，突显了在图学习任务中需要像
    GraphPrompter 这样的量身定制方法。
- en: 6\. Conclusion
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: 'In conclusion, our study reveals the significant potential of leveraging LLMs
    for interpreting graph structures through a prompt tuning methodology. To facilitate
    this study, we presents GraphPrompter, a novel plug-and-play framework that integrates
    the capabilities of Large Language Models with the structural insights of Graph
    Neural Networks for the task of node classification and link prediction in graph
    data. Specifically, GraphPrompter consists of two sections: the graph section
    utilizes GNN for structural insights, while the text section applies LLM to interpret
    node-related textual data. Extensive experiments and in-depth studies demonstrate
    the superiority of GraphPrompter across multiple benchmark datasets. Future work
    could focus on extending our pipeline to other more complex graph level tasks.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的研究揭示了通过提示调优方法利用 LLMs 解释图结构的显著潜力。为促进这项研究，我们提出了 GraphPrompter，这是一种新颖的即插即用框架，将大型语言模型的能力与图神经网络的结构洞察力结合起来，用于图数据中的节点分类和链路预测任务。具体而言，GraphPrompter
    包含两个部分：图部分利用 GNN 提供结构洞察，而文本部分应用 LLM 来解释节点相关的文本数据。广泛的实验和深入研究证明了 GraphPrompter 在多个基准数据集上的优越性。未来的工作可以集中在将我们的管道扩展到其他更复杂的图级任务。
- en: References
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Brannon et al. (2023) William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang,
    Brandon Roy, Jad Kabbara, and Deb Roy. 2023. ConGraT: Self-Supervised Contrastive
    Pretraining for Joint Graph and Text Embeddings. *arXiv preprint arXiv:2305.14321*
    (2023).'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brannon 等人 (2023) William Brannon, Suyash Fulay, Hang Jiang, Wonjune Kang, Brandon
    Roy, Jad Kabbara, 和 Deb Roy 2023。ConGraT：用于联合图和文本嵌入的自监督对比预训练。*arXiv 预印本 arXiv:2305.14321*
    (2023)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems* 33 (2020), 1877–1901.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等人 2020。语言模型是少样本学习者。*神经信息处理系统进展* 33 (2020), 1877–1901。
- en: Carlini et al. (2022) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization
    across neural language models. *arXiv preprint arXiv:2202.07646* (2022).
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等人 (2022) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine
    Lee, Florian Tramer, 和 Chiyuan Zhang 2022。量化神经语言模型的记忆。*arXiv 预印本 arXiv:2202.07646*
    (2022)。
- en: 'Chai et al. (2023) Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai
    Hu, Xuanwen Huang, and Yang Yang. 2023. Graphllm: Boosting graph reasoning ability
    of large language model. *arXiv preprint arXiv:2310.05845* (2023).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chai et al. (2023) Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai
    Hu, Xuanwen Huang, 和 Yang Yang。2023年。《Graphllm: 提升大型语言模型的图推理能力》。*arXiv 预印本 arXiv:2310.05845*（2023）。'
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research* 24, 240 (2023), 1–113.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, 等人。2023年。《Palm: 通过路径扩展语言建模》。*机器学习研究期刊* 24, 240（2023），1–113。'
- en: 'He et al. (2023) Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi.
    2023. Explanations as Features: LLM-Based Features for Text-Attributed Graphs.
    *arXiv preprint arXiv:2305.19523* (2023).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2023) Xiaoxin He, Xavier Bresson, Thomas Laurent, 和 Bryan Hooi。2023年。《解释作为特征：基于LLM的文本属性图特征》。*arXiv
    预印本 arXiv:2305.19523*（2023）。
- en: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, et al. 2022. Training compute-optimal large language
    models. *arXiv preprint arXiv:2203.15556* (2022).
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffmann et al. (2022) Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena
    Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks,
    Johannes Welbl, Aidan Clark, 等人。2022年。《训练计算最优的大型语言模型》。*arXiv 预印本 arXiv:2203.15556*（2022）。
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language
    models. *Advances in Neural Information Processing Systems* 35 (2022), 3843–3857.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, 等人。2022年。《利用语言模型解决定量推理问题》。*神经信息处理系统进展* 35（2022），3843–3857。
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*
    (2022).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, 等人。2022年。《语言模型的整体评估》。*arXiv 预印本 arXiv:2211.09110*（2022）。
- en: 'Mavromatis et al. (2023) Costas Mavromatis, Vassilis N Ioannidis, Shen Wang,
    Da Zheng, Soji Adeshina, Jun Ma, Han Zhao, Christos Faloutsos, and George Karypis.
    2023. Train Your Own GNN Teacher: Graph-Aware Distillation on Textual Graphs.
    *arXiv preprint arXiv:2304.10668* (2023).'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mavromatis et al. (2023) Costas Mavromatis, Vassilis N Ioannidis, Shen Wang,
    Da Zheng, Soji Adeshina, Jun Ma, Han Zhao, Christos Faloutsos, 和 George Karypis。2023年。《训练你自己的GNN教师：在文本图上的图感知蒸馏》。*arXiv
    预印本 arXiv:2304.10668*（2023）。
- en: Ostendorff et al. (2022) Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein,
    Bela Gipp, and Georg Rehm. 2022. Neighborhood contrastive learning for scientific
    document representations with citation embeddings. *arXiv preprint arXiv:2202.06671*
    (2022).
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ostendorff et al. (2022) Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein,
    Bela Gipp, 和 Georg Rehm。2022年。《带有引用嵌入的科学文档表示的邻域对比学习》。*arXiv 预印本 arXiv:2202.06671*（2022）。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems* 35 (2022), 27730–27744.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, 等人。2022年。《训练语言模型以遵循人类反馈的指令》。*神经信息处理系统进展* 35（2022），27730–27744。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog* 1, 8 (2019), 9.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, 等人。2019年。《语言模型是无监督的多任务学习者》。*OpenAI 博客* 1, 8（2019），9。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288* (2023).'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等人。2023年。《Llama 2: 开放基础和微调聊天模型》。*arXiv 预印本 arXiv:2307.09288*（2023）。'
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. 2017. Graph attention networks.
    *arXiv preprint arXiv:1710.10903* (2017).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veličković et al. (2017) 佩塔尔·维利克诺维奇、吉列姆·库库鲁、阿兰查·卡萨诺瓦、阿德里安娜·罗梅罗、皮特罗·利奥和约书亚·本吉奥。2017。图注意力网络。*arXiv
    预印本 arXiv:1710.10903* (2017)。
- en: Webson and Pavlick (2021) Albert Webson and Ellie Pavlick. 2021. Do prompt-based
    models really understand the meaning of their prompts? *arXiv preprint arXiv:2109.01247*
    (2021).
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Webson and Pavlick (2021) 阿尔伯特·韦布森和艾莉·帕夫里克。2021。基于提示的模型真的理解他们提示的意义吗？*arXiv 预印本
    arXiv:2109.01247* (2021)。
- en: Wen and Fang (2023) Zhihao Wen and Yuan Fang. 2023. Augmenting Low-Resource
    Text Classification with Graph-Grounded Pre-training and Prompting. *arXiv preprint
    arXiv:2305.03324* (2023).
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen and Fang (2023) 志浩·温和袁芳。2023。通过图基础预训练和提示增强低资源文本分类。*arXiv 预印本 arXiv:2305.03324*
    (2023)。
- en: 'Yang et al. (2021) Junhan Yang, Zheng Liu, Shitao Xiao, Chaozhuo Li, Defu Lian,
    Sanjay Agrawal, Amit Singh, Guangzhong Sun, and Xing Xie. 2021. GraphFormers:
    GNN-nested transformers for representation learning on textual graph. *Advances
    in Neural Information Processing Systems* 34 (2021), 28798–28810.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2021) 君涵·杨、郑柳、士涛·肖、超卓·李、德富·连、桑贾伊·阿格拉瓦尔、阿密特·辛格、广中·孙和邢谢。2021。GraphFormers：用于文本图表示学习的GNN嵌套变换器。*神经信息处理系统进展*
    34 (2021)，28798–28810。
- en: Yu et al. (2023) Jianxiang Yu, Yuxiang Ren, Chenghua Gong, Jiaqi Tan, Xiang
    Li, and Xuecang Zhang. 2023. Empower text-attributed graphs learning with large
    language models (llms). *arXiv preprint arXiv:2310.09872* (2023).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2023) 建祥·余、玉翔·任、程华·龚、佳琪·谭、翔·李和雪苍·张。2023。通过大型语言模型（LLMs）增强文本属性图学习。*arXiv
    预印本 arXiv:2310.09872* (2023)。
- en: 'Zhang et al. (2022a) Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu
    Ren, Percy Liang, Christopher D Manning, and Jure Leskovec. 2022a. Greaselm: Graph
    reasoning enhanced language models for question answering. *arXiv preprint arXiv:2201.08860*
    (2022).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022a) 习坤·张、安托万·博塞鲁、道一郎·安、洪宇·任、珀西·梁、克里斯托弗·D·曼宁和朱雷·莱斯科维奇。2022a。Greaselm：用于问答的图推理增强语言模型。*arXiv
    预印本 arXiv:2201.08860* (2022)。
- en: Zhang et al. (2021) Xinyang Zhang, Chenwei Zhang, Xin Luna Dong, Jingbo Shang,
    and Jiawei Han. 2021. Minimally-supervised structure-rich text categorization
    via learning on text-rich networks. In *Proceedings of the Web Conference 2021*.
    3258–3268.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021) 辛阳·张、陈伟·张、辛·露娜·董、静波·尚和佳伟·韩。2021。通过学习文本丰富网络进行最小监督结构丰富文本分类。在*Web会议
    2021 论文集*中，3258–3268。
- en: Zhang et al. (2022b) Yu Zhang, Zhihong Shen, Chieh-Han Wu, Boya Xie, Junheng
    Hao, Ye-Yi Wang, Kuansan Wang, and Jiawei Han. 2022b. Metadata-induced contrastive
    learning for zero-shot multi-label text classification. In *Proceedings of the
    ACM Web Conference 2022*. 3162–3173.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022b) 余张、志宏·沈、纪寒·吴、博雅·谢、俊衡·郝、叶一·王、宽三·王和佳伟·韩。2022b。元数据诱导的对比学习用于零样本多标签文本分类。在*ACM
    Web会议 2022 论文集*中，3162–3173。
- en: Zhao et al. (2022) Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui
    Li, Xing Xie, and Jian Tang. 2022. Learning on large-scale text-attributed graphs
    via variational inference. *arXiv preprint arXiv:2210.14709* (2022).
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2022) 贾南·赵、孟阔、超卓·李、郝艳、钱刘、瑞·李、邢谢和建堂。2022。通过变分推断在大规模文本属性图上进行学习。*arXiv
    预印本 arXiv:2210.14709* (2022)。
