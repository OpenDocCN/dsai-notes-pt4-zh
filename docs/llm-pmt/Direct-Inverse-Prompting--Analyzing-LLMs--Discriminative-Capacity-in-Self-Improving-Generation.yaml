- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:41:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:41:33
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Direct-Inverse Prompting: Analyzing LLMs’ Discriminative Capacity in Self-Improving
    Generation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 直接-反向提示：分析 LLM 的自我改进生成中的判别能力
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.11017](https://ar5iv.labs.arxiv.org/html/2407.11017)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.11017](https://ar5iv.labs.arxiv.org/html/2407.11017)
- en: Jihyun Janice Ahn^($\spadesuit$)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jihyun Janice Ahn^($\spadesuit$)
- en: Rui Zhang^($\spadesuit$)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Rui Zhang^($\spadesuit$)
- en: ^($\spadesuit$) University of Illinois at Chicago
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\spadesuit$) 伊利诺伊大学芝加哥分校
- en: '{jfa5672, wenpeng}@psu.edu;   lucheng@uic.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{jfa5672, wenpeng}@psu.edu;   lucheng@uic.edu'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Mainstream LLM research has primarily focused on enhancing their generative
    capabilities. However, even the most advanced LLMs experience uncertainty in their
    outputs, often producing varied results on different runs or when faced with minor
    changes in input, despite no substantial change in content. Given multiple responses
    from the same LLM to the same input, we advocate leveraging the LLMs’ discriminative
    capability to reduce this generative uncertainty, aiding in identifying the correct
    answers. Specifically, we propose and analyze three discriminative prompts: Direct
    Prompt, Inverse Prompt, and Combination, to explore the potential of both closed-source
    and open-source LLMs in self-improving their generative performance on two benchmark
    datasets. Our insights reveal which discriminative prompt is most promising and
    when to use it. To our knowledge, this is the first work to systematically analyze
    LLMs’ discriminative capacity to address generative uncertainty.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 主流的 LLM 研究主要集中在增强其生成能力上。然而，即使是最先进的 LLM 也会在其输出中经历不确定性，通常在不同的运行中或在面对输入的微小变化时，产生不同的结果，尽管内容没有实质性变化。鉴于同一
    LLM 对相同输入产生的多个回应，我们倡导利用 LLM 的判别能力来减少这种生成不确定性，帮助识别正确答案。具体而言，我们提出并分析了三种判别提示：直接提示、反向提示和组合提示，以探索封闭源和开放源
    LLM 在两个基准数据集上自我改进生成性能的潜力。我们的见解揭示了哪种判别提示最有前途以及何时使用它。据我们所知，这是首次系统地分析 LLM 的判别能力以解决生成不确定性的工作。
- en: 'Direct-Inverse Prompting: Analyzing LLMs’ Discriminative Capacity'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 直接-反向提示：分析 LLM 的判别能力
- en: in Self-Improving Generation
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在自我改进生成中
- en: Jihyun Janice Ahn^($\spadesuit$) University of Illinois at Chicago {jfa5672,
    wenpeng}@psu.edu;   lucheng@uic.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Jihyun Janice Ahn^($\spadesuit$) 伊利诺伊大学芝加哥分校 {jfa5672, wenpeng}@psu.edu;   lucheng@uic.edu
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Generative AI is revolutionizing various fields by utilizing large language
    models (LLMs) trained to generate human-like responses based on given instructions.
    Despite the increasing strength of existing LLMs in terms of generation capability,
    a widely recognized issue is their uncertainty in responses to inputs—the same
    model may produce significantly different responses on different runs or to equivalently
    varied inputs.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 生成性 AI 正在通过利用大语言模型（LLMs）来彻底改变各个领域，这些模型经过训练可以根据给定的指令生成类似人类的回应。尽管现有 LLM 在生成能力方面的强度不断提高，但一个广泛认可的问题是它们对输入的回应的不确定性——同一个模型在不同的运行中或对等效变化的输入可能产生显著不同的回应。
- en: Previous studies have explored LLMs’ self-improving capability that either relied
    on external human/tool supervision Wang et al. ([2023a](#bib.bib16)); Paul et al.
    ([2024](#bib.bib15)); Gou et al. ([2023](#bib.bib6)); Chen et al. ([2023b](#bib.bib4));
    Olausson et al. ([2023](#bib.bib12)); Gao et al. ([2023](#bib.bib5)) or have not
    successfully explored the inner capabilities of LLMs, such as their own discriminative
    capability, to reduce uncertainty Jiang et al. ([2024](#bib.bib8)). We argue that
    LLMs should focus on both their generative and discriminative capabilities. In
    this work, we explore various discriminative capabilities of LLMs to reduce the
    uncertainty of their self-improving generations.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究已经探讨了 LLM 的自我改进能力，这些研究要么依赖于外部人类/工具监督 Wang et al. ([2023a](#bib.bib16));
    Paul et al. ([2024](#bib.bib15)); Gou et al. ([2023](#bib.bib6)); Chen et al.
    ([2023b](#bib.bib4)); Olausson et al. ([2023](#bib.bib12)); Gao et al. ([2023](#bib.bib5))，要么没有成功探索
    LLM 的内部能力，例如其自身的判别能力，以减少不确定性 Jiang et al. ([2024](#bib.bib8))。我们认为 LLM 应该关注其生成能力和判别能力。在这项工作中，我们探索了
    LLM 的各种判别能力，以减少其自我改进生成的不确定性。
- en: 'Specifically, we propose and analyze three types of discriminative prompts
    to identify the most promising answer from a group of generated responses: Direct
    Prompt: directly asking the LLM which responses are correct; Inverse Prompt: contrasting
    Direct Promptby asking which responses are incorrect; Combination: combining Direct
    Promptand Inverse Prompt, since intuitively they perform the same reasoning process
    from complementary perspectives.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们提出并分析了三种类型的区分提示，以从一组生成的响应中识别最有前景的答案：直接提示：直接询问LLM哪些响应是正确的；逆向提示：通过询问哪些响应是错误的来对比直接提示；组合：将直接提示和逆向提示结合，因为从直观上看，它们从互补的角度执行相同的推理过程。
- en: 'We conduct analyses with two closed-source LLMs (GPT-4 OpenAI ([2023](#bib.bib13))
    and GPT-4o OpenAI ([2024](#bib.bib14))) and two open-source LLMs (Llama-3-8B-Instruct
    Meta ([2024](#bib.bib11)) and MetaMath-7B-V1.0 Yu et al. ([2023](#bib.bib20)))
    on two math-related datasets, MATH Hendrycks et al. ([2021](#bib.bib7)) and MathQA
    Amini et al. ([2019](#bib.bib1)). We observe: i) For closed-source LLMs, using
    discriminative capability, either Direct Prompt or Inverse Prompt, is highly effective
    for reducing uncertainty in self-improving generations. ii) For open-source LLMs,
    if not instruction-tuned, using discriminative capability is not recommended.
    Even if instruction-tuned, only Direct Prompt is recommended due to likely issues
    with understanding negation in Inverse Prompt.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对两个封闭源LLM（GPT-4 OpenAI ([2023](#bib.bib13)) 和 GPT-4o OpenAI ([2024](#bib.bib14))）和两个开源LLM（Llama-3-8B-Instruct
    Meta ([2024](#bib.bib11)) 和 MetaMath-7B-V1.0 Yu et al. ([2023](#bib.bib20))）在两个与数学相关的数据集（MATH
    Hendrycks et al. ([2021](#bib.bib7)) 和 MathQA Amini et al. ([2019](#bib.bib1))）上进行了分析。我们观察到：i)
    对于封闭源LLM，使用区分能力，无论是直接提示还是逆向提示，都对减少自我提升生成中的不确定性非常有效。ii) 对于开源LLM，如果没有进行指令调优，建议不使用区分能力。即使进行了指令调优，由于逆向提示可能存在理解否定的难题，建议仅使用直接提示。
- en: 'Our contributions are threefold:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献有三点：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Proposing Direct-Inverse Discriminative Prompting, a multi-angle complementary
    method, to assess LLMs’ discriminative capability in self-improving generation;
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出直接-逆向区分提示，这是一种多角度互补的方法，用于评估LLM在自我提升生成中的区分能力；
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The first systematic analysis of the potential of LLMs’ discriminative capability
    to reduce generative uncertainty;
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对LLM区分能力在减少生成不确定性方面潜力的首次系统性分析；
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Providing insights and suggestions for future users on how to effectively utilize
    LLMs’ discriminative capability in practice.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供对未来用户如何有效利用LLM的区分能力的见解和建议。
- en: 2 Related Work
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM self-improves generation.
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM自我提升生成。
- en: Various methods are being devised to increase the certainty of LLM-generated
    answers. Chain-of-Thought Wei et al. ([2023](#bib.bib18)) tries to add a detailed
    reasoning path from the input to the output answer so that the answer is more
    explainable and certain. Self-Consistency Wang et al. ([2023b](#bib.bib17)) has
    the LLM solve the same problem multiple times to obtain several results. A majority
    vote is then conducted to choose the most consistent result as the final answer.
    This approach guarantees a higher success rate than Chain-of-Thought. Based on
    this, diverse variants of Self-Consistency exist; for example, Universal Self-Consistency
    Chen et al. ([2023a](#bib.bib3)), which includes reasoning to select the most
    consistent value as the final answer, or Early Stop Self-Consistency Li et al.
    ([2024](#bib.bib9)), which reduces the number of answer sets used in the majority
    vote to save cost and time. It is worth mentioning that the above approaches are
    fully unsupervised, namely no human or external signals are needed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 各种方法正在被设计用以提高LLM生成答案的确定性。链式思维 Wei et al. ([2023](#bib.bib18)) 尝试从输入到输出答案添加详细的推理路径，使得答案更加可解释和确定。自我一致性
    Wang et al. ([2023b](#bib.bib17)) 让LLM多次解决相同的问题以获得多个结果。然后进行多数投票，选择最一致的结果作为最终答案。这种方法比链式思维保证了更高的成功率。基于此，存在多种自我一致性的变体；例如，通用自我一致性
    Chen et al. ([2023a](#bib.bib3))，包括推理以选择最一致的值作为最终答案，或早停自我一致性 Li et al. ([2024](#bib.bib9))，减少用于多数投票的答案集数量，以节省成本和时间。值得一提的是，上述方法完全是无监督的，即不需要人工或外部信号。
- en: Exploring LLM discriminative capability to enhance generation.
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 探索LLM的区分能力以增强生成效果。
- en: 'To assess the generative and discriminative capabilities of LLMs, Liu et al.
    ([2023](#bib.bib10)) and Arora and Kambhampati ([2023](#bib.bib2)) carried out
    experiments on summarization and planning problem, respectively. The most related
    work, Jiang et al. ([2024](#bib.bib8)), concluded that LLMs struggle to enhance
    their generation performance through discriminative capability because their discriminative
    capability is not stronger than their generative capability. Our work differs
    from this study in two key ways: i) Jiang et al. ([2024](#bib.bib8)) only considered
    a simplified discriminative prompt similar to our Direct Prompt. They provided
    the discriminative prompt with all the generated final answers without the reasoning
    paths. In contrast, our Direct Prompt includes reasoning-path equipped answers,
    which we believe can help LLMs better determine the correct answer. ii) We further
    analyze another complementary discriminative capability expressed by Inverse Prompt.
    While Inverse Prompt should theoretically yield the same conclusions if applied
    to humans, the inconsistency between Direct Prompt and Inverse Prompt in LLMs
    allows us to better understand their discriminative potential in reducing generative
    uncertainty. iii) Our findings suggest a different conclusion: LLMs’ discriminative
    capabilities can indeed enhance their generation if used skillfully.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估LLM的生成能力和判别能力，Liu 等（[2023](#bib.bib10)）和 Arora 与 Kambhampati（[2023](#bib.bib2)）分别在总结和规划问题上进行了实验。最相关的研究，Jiang
    等（[2024](#bib.bib8)），得出结论认为LLM在通过判别能力提高其生成表现方面存在困难，因为它们的判别能力不如生成能力强。我们的研究与该研究在两个关键方面有所不同：i)
    Jiang 等（[2024](#bib.bib8)）仅考虑了类似于我们直接提示的简化判别提示。他们提供了所有生成的最终答案而没有推理路径。相比之下，我们的直接提示包括配备了推理路径的答案，我们相信这可以帮助LLM更好地确定正确答案。ii)
    我们进一步分析了逆向提示表达的另一种补充判别能力。虽然如果将逆向提示应用于人类，理论上应该得出相同的结论，但在LLM中，直接提示和逆向提示之间的不一致使我们能够更好地理解它们在减少生成不确定性方面的判别潜力。iii)
    我们的发现提出了不同的结论：LLM的判别能力如果使用得当，确实可以增强其生成能力。
- en: 3 Direct-Inverse Discriminative Prompting
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 直接-逆向判别提示
- en: Given multiple answer options by LLMs’ generative process (here uses five for
    example), this section introduces our discriminative approach Direct-Inverse Discriminative
    Prompting, that asks LLMs with Direct Prompt, Inverse Prompt, and finally combines
    their lens to find the most certain answer in self-improving generation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 给定LLM生成过程中的多个答案选项（此处以五个为例），本节介绍了我们的判别方法 Direct-Inverse Discriminative Prompting，该方法使用直接提示、逆向提示，并最终结合它们的视角，以找到自我改进生成中的最确定答案。
- en: Direct Prompt.
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 直接提示。
- en: 'Here, we directly ask LLMs which options are correct with the following prompt:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们直接询问LLM哪些选项是正确的，使用以下提示：
- en: 'This problem [*problem description*] has the following reasoning paths you
    generated: “ A: [$path_{1}$]”. Please output the correct ones.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '这个问题[*问题描述*]具有以下你生成的推理路径：“A: [$path_{1}$]”。请输出正确的路径。'
- en: Inverse Prompt.
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 逆向提示。
- en: 'Here, we ask LLMs which options are incorrect with the following prompt:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们询问LLM哪些选项是不正确的，使用以下提示：
- en: 'This problem [*problem description*] has the following reasoning paths you
    generated: “ A: [$path_{1}$]”. Please output the incorrect ones.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '这个问题[*问题描述*]具有以下你生成的推理路径：“A: [$path_{1}$]”。请输出不正确的路径。'
- en: Combination.
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 组合。
- en: 'As humans, when asked using both Direct Prompt and Inverse Prompt prompts,
    their answers should be consistent. However, this is not the case with LLMs, as
    our analysis in Section [5.2](#S5.SS2 "5.2 Analysis ‣ 5 Results ‣ Direct-Inverse
    Prompting: Analyzing LLMs’ Discriminative Capacity in Self-Improving Generation")
    shows. For instance, using Direct Prompt, an LLM may believe “A and B” are correct,
    but when asked using Inverse Prompt, it might believe “B and C” are incorrect,
    implying that “A, D, and E” are correct. Direct Prompt and Inverse Prompt reflect
    LLMs’ discriminative analysis of the problem from different perspectives, and
    we combine their results to improve accuracy. Specifically, we run Direct Prompt and
    Inverse Prompt separately multiple times and select the final answer by identifying
    the most consensus among the responses.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于人类来说，当使用直接提示和反向提示提问时，他们的回答应该是一致的。然而，对于 LLMs 来说情况并非如此，正如我们在第 [5.2](#S5.SS2
    "5.2 分析 ‣ 5 结果 ‣ 直接-反向提示：分析 LLMs 在自我改进生成中的辨别能力") 节中所展示的那样。例如，使用直接提示时，LLM 可能认为“A
    和 B”是正确的，但当使用反向提示时，它可能认为“B 和 C”是错误的，暗示“A、D 和 E”是正确的。直接提示和反向提示从不同的角度反映了 LLMs 对问题的辨别分析，我们结合它们的结果以提高准确性。具体来说，我们分别多次运行直接提示和反向提示，并通过识别响应中最一致的答案来选择最终答案。
- en: '|  | MATH | MathQA |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | MATH | MathQA |'
- en: '| --- | --- | --- |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | GPT4 | GPT-4o | Llama3 | MetaMath | GPT4 | GPT-4o | Llama3 | MetaMath
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | GPT4 | GPT-4o | Llama3 | MetaMath | GPT4 | GPT-4o | Llama3 | MetaMath
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Chain-of-Thought | 47.58 | 50.67 | 21.55 | 10.83 | 72.57 | 82.73 | 39.03
    | 11.96 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 思维链 | 47.58 | 50.67 | 21.55 | 10.83 | 72.57 | 82.73 | 39.03 | 11.96 |'
- en: '| Uni. Self-Consist. | 55.14 | 54.72 | 26.72 | 12.04 | 79.50 | 85.33 | 42.58
    | 11.79 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| Uni. Self-Consist. | 55.14 | 54.72 | 26.72 | 12.04 | 79.50 | 85.33 | 42.58
    | 11.79 |'
- en: '| Direct Prompt | 54.18 | 57.44 | 27.54 | 0.18 | 81.64 | 86.73 | 46.40 | 0.00
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 直接提示 | 54.18 | 57.44 | 27.54 | 0.18 | 81.64 | 86.73 | 46.40 | 0.00 |'
- en: '| Inverse Prompt | 54.62 | 55.48 | 18.08 | 0.06 | 82.34 | 86.40 | 37.45 | 0.00
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 反向提示 | 54.62 | 55.48 | 18.08 | 0.06 | 82.34 | 86.40 | 37.45 | 0.00 |'
- en: '| Combination | 56.44 | 56.82 | 25.98 | 0.24 | 82.04 | 86.63 | 42.98 | 0.00
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 组合 | 56.44 | 56.82 | 25.98 | 0.24 | 82.04 | 86.63 | 42.98 | 0.00 |'
- en: 'Table 1: Comparing discriminative prompts Direct Prompt, Inverse Prompt, and
    Combination on LLMs. Bold: top score. Underline: surpass the Universal Self-Consistency.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：比较 LLMs 上的辨别提示直接提示、反向提示和组合。粗体：最高分。下划线：超越通用自我一致性。
- en: 4 Experiments
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: Datasets.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集。
- en: 'Two datasets. An example of each dataset is given in appendix [A](#A1 "Appendix
    A Example Appendix ‣ Direct-Inverse Prompting: Analyzing LLMs’ Discriminative
    Capacity in Self-Improving Generation").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 两个数据集。每个数据集的示例见附录 [A](#A1 "附录 A 示例 附录 ‣ 直接-反向提示：分析 LLMs 在自我改进生成中的辨别能力")。
- en: '• MATH Hendrycks et al. ([2021](#bib.bib7)): This dataset contains 7 types
    of open-ended math problems, including algebra and geometry, with average high
    school difficulty. For this project, we selected the entire test dataset of 5,000
    problems. Each problem includes a “problem” label, representing the math word
    problem, and a “solution” label, which provides the explanation of how to solve
    the problem, including an answer formatted as $\boxed{$A$ is the answer. To maintain
    consistency, all models were instructed to return the final answer in the same
    format as the dataset.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: • MATH Hendrycks 等人 ([2021](#bib.bib7))：该数据集包含 7 种开放性数学问题，包括代数和几何，难度为高中水平。对于本项目，我们选择了全部
    5,000 个问题的测试数据集。每个问题包含一个“问题”标签，表示数学文字问题，以及一个“解决方案”标签，提供解决问题的解释，包括格式为 $\boxed{$A$
    是答案。为了保持一致性，所有模型被指示以与数据集相同的格式返回最终答案。
- en: '• MathQA Amini et al. ([2019](#bib.bib1)): This dataset includes 6 types of
    math problems, with college-level difficulty. We selected all 2,985 problems from
    the test dataset. Each entry in MathQA contains a “problem,” a “rationale” explaining
    how to solve it, “options” that list possible answers, and “correct,” indicating
    the correct answer from the options. LLMs were instructed to return the correct
    option’s alphabet from the given choices.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: • MathQA Amini 等人 ([2019](#bib.bib1))：该数据集包含 6 种数学问题，难度为大学水平。我们从测试数据集中选择了全部
    2,985 个问题。MathQA 中的每个条目包括一个“问题”，一个解释如何解决问题的“理由”，列出可能答案的“选项”，以及表示正确答案的“正确”标签。LLMs
    被指示从给定的选项中返回正确选项的字母。
- en: LLMs.
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs。
- en: 'i) Two closed-source LLMs: GPT-4 OpenAI ([2023](#bib.bib13)) and GPT-4o OpenAI
    ([2024](#bib.bib14)). Both by OpenAI APIs. We do not consider more closed-source
    LLMs due to budget limits, and GPT-4 and GPT-4o are already widely recognized
    as the strongest LLMs. ii) Open-source LLMs: Llama-3 Meta ([2024](#bib.bib11))
    and MetaMath Yu et al. ([2023](#bib.bib20))–a LLM specifically optimized for math
    problem solving. In our experiments, five A100 GPUs were used for running Llama-3
    and MetaMath inference.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: i) 两个闭源 LLM：GPT-4 OpenAI ([2023](#bib.bib13)) 和 GPT-4o OpenAI ([2024](#bib.bib14))。均由
    OpenAI API 提供。由于预算限制，我们未考虑更多的闭源 LLM，GPT-4 和 GPT-4o 已被广泛认可为最强的 LLM。 ii) 开源 LLM：Llama-3
    Meta ([2024](#bib.bib11)) 和 MetaMath Yu 等 ([2023](#bib.bib20))——一个专门优化用于数学问题解决的
    LLM。在我们的实验中，使用了五个 A100 GPU 运行 Llama-3 和 MetaMath 推理。
- en: Baselines.
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基准线。
- en: i) Chain-of-Thought (CoT) Wei et al. ([2022](#bib.bib19)). We run it three times
    and report the average performance. ii) Universal Self-Consistency Chen et al.
    ([2023a](#bib.bib3)), the state-of-the-art approach CoT reasoning process five
    times, and finally choosing the answer with majority voting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: i) 思维链（CoT）Wei 等 ([2022](#bib.bib19))。我们运行了三次并报告了平均表现。 ii) 通用自一致性 Chen 等 ([2023a](#bib.bib3))，最先进的方法
    CoT 推理过程五次，最后选择多数票的答案。
- en: Setting.
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 设置。
- en: To prevent the LLMs’ responses to options like “A, B, C, etc.” from being biased
    due to their pretraining, we will shuffle these options and re-index them for
    each run. The final performance will be the average of three runs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止 LLM 对选项如“A、B、C 等”的响应因其预训练而产生偏差，我们将对这些选项进行洗牌并为每次运行重新索引。最终性能将是三次运行的平均值。
- en: 5 Results
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果
- en: 5.1 Main Results
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 主要结果
- en: 'Table [1](#S3.T1 "Table 1 ‣ Combination. ‣ 3 Direct-Inverse Discriminative
    Prompting ‣ Direct-Inverse Prompting: Analyzing LLMs’ Discriminative Capacity
    in Self-Improving Generation") presents the main results comparing different discriminative
    prompts (Direct Prompt, Inverse Prompt, and Combination) of LLMs on the MATH and
    MathQA datasets. Here are some key observations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S3.T1 "表 1 ‣ 组合。 ‣ 3 直接-逆向区分提示 ‣ 直接-逆向提示：分析 LLM 在自我改进生成中的区分能力") 展示了比较不同区分提示（直接提示、逆向提示和组合）在
    MATH 和 MathQA 数据集上的主要结果。以下是一些关键观察：
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discriminative prompts (Direct Prompt, Inverse Prompt, and Combination) do not
    work for MetaMath. This is because MetaMath was specifically optimized for solving
    math problems rather than following instructions. In our experiments, MetaMath
    responded to our discriminative prompts with noise and unstructured outputs, making
    answer parsing impossible.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 区分提示（直接提示、逆向提示和组合）对 MetaMath 无效。这是因为 MetaMath 被专门优化用于解决数学问题，而不是执行指令。在我们的实验中，MetaMath
    对我们的区分提示响应了噪声和无结构的输出，使得答案解析不可能。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Excluding MetaMath, Inverse Prompt outperforms Direct Prompt in 2 out of 6 cases,
    performs equally in one case (GPT-4o on MathQA), and underperforms in the remaining
    three cases. This is expected because negation is often more challenging for AI
    models to understand.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了 MetaMath 外，逆向提示在 6 个案例中有 2 个表现优于直接提示，在一个案例中表现相同（GPT-4o 在 MathQA 上），在剩下的三个案例中表现较差。这是预期中的，因为否定通常更难为
    AI 模型理解。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In most cases (except for MetaMath), both Direct Prompt and Combination outperform
    Universal Self-Consistency (and even Inverse Prompt generally surpasses it on
    closed-source LLMs), indicating the effectiveness of using LLMs’ discriminative
    capabilities to find the most certain answer.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在大多数情况下（MetaMath 除外），直接提示和组合都优于通用自一致性（甚至逆向提示通常在闭源 LLM 上超越它），这表明使用 LLM 的区分能力来找到最确定的答案的有效性。
- en: '|  | MATH | MathQA |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | MATH | MathQA |'
- en: '| --- | --- | --- |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-4 | 36.88 | 23.75 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 36.88 | 23.75 |'
- en: '| GPT-4o | 46.00 | 23.85 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 46.00 | 23.85 |'
- en: '| Llama-3 | 97.34 | 97.96 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3 | 97.34 | 97.96 |'
- en: '| MetaMath | 100.00 | 100.00 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| MetaMath | 100.00 | 100.00 |'
- en: 'Table 2: Conflicting percentage per dataset.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 每个数据集的冲突百分比。'
- en: '|  | MATH | MathQA |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | MATH | MathQA |'
- en: '| --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| GPT-4 | 71.86 / 25.49 | 89.02 / 58.81 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 | 71.86 / 25.49 | 89.02 / 58.81 |'
- en: '| GPT-4o | 77.93 / 30.30 | 93.36 / 62.64 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4o | 77.93 / 30.30 | 93.36 / 62.64 |'
- en: '| Llama-3 | 76.69 / 23.07 | 73.77 / 42.23 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Llama-3 | 76.69 / 23.07 | 73.77 / 42.23 |'
- en: '| MetaMath | 0.00 / 0.12 | 0.00 / 0.00 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| MetaMath | 0.00 / 0.12 | 0.00 / 0.00 |'
- en: 'Table 3: Fine-grained Combination performance on agreed/disagreed responses
    of Direct Prompt and Inverse Prompt .'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 直接提示和逆向提示在一致/不一致响应上的细粒度组合表现。'
- en: 5.2 Analysis
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 分析
- en: '$\mathcal{Q}_{1}$: How frequently do LLMs experience uncertainty in their decisions,
    indicated by conflicts between Direct Promptand Inverse Prompt?'
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\mathcal{Q}_{1}$：LLM 在决策中经历不确定性的频率如何，这种不确定性通过直接提示和反向提示之间的冲突来表示？
- en: When Inverse Prompt outputs, for instance, “B, C” as incorrect answers, we consider
    the remaining options, i.e., “A, D, E” as the correct answer inferred by Inverse
    Prompt. Conflicts arise when Direct Prompt and Inverse Prompt reach different
    conclusions. The conflict degree is calculated as the number of conflicts divided
    by the total number of problems for each dataset.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当反向提示输出，例如，“B, C”作为错误答案时，我们将其余选项，即“ A, D, E”视为反向提示推断出的正确答案。当直接提示和反向提示得出不同结论时，会产生冲突。冲突程度计算为每个数据集中冲突的数量除以问题的总数。
- en: 'Table [2](#S5.T2 "Table 2 ‣ 5.1 Main Results ‣ 5 Results ‣ Direct-Inverse Prompting:
    Analyzing LLMs’ Discriminative Capacity in Self-Improving Generation") provides
    a summary of the severity of self-conflict within each LLM. GPT-4 demonstrates
    the highest consistency and self-confidence, with the lowest conflict percentages
    across both datasets. GPT-4o shows moderate consistency, performing better on
    the MathQA dataset than on MATH. Llama-3 exhibits the weakest performance in terms
    of consistency on the MathQA dataset, with the second-highest conflict rates in
    the MATH dataset, indicating its unreliability in this analysis. Lastly, MetaMath
    shows the highest conflict rates in both datasets having 100% of conflict rates.
    These results underscore the enhanced reliability of advanced models like GPT-4\.
    They also emphasize the interestingness of our work, which leverages the inconsistency
    in discriminative capability to enhance the certainty in generative'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S5.T2 "表 2 ‣ 5.1 主要结果 ‣ 5 结果 ‣ 直接-反向提示：分析 LLM 的自我改进生成中的区分能力")提供了每个 LLM
    内部自我冲突严重性的总结。GPT-4 展现了最高的一致性和自信，在两个数据集上都有最低的冲突百分比。GPT-4o 显示出适中的一致性，在 MathQA 数据集上的表现优于
    MATH。Llama-3 在 MathQA 数据集上的一致性表现最弱，在 MATH 数据集上冲突率第二高，表明其在此分析中的不可靠性。最后，MetaMath
    在两个数据集中都显示了最高的冲突率，冲突率为 100%。这些结果强调了像 GPT-4 这样的先进模型的增强可靠性，也突出了我们工作的趣味性，它利用区分能力的不一致来增强生成过程中的确定性。
- en: '$\mathcal{Q}_{2}$: How are LLMs performing when their choice is agreed or disagreed
    by Direct Prompt and Inverse Prompt?'
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\mathcal{Q}_{2}$：当 LLM 的选择在直接提示和反向提示中达成一致或不一致时，它们的表现如何？
- en: To answer this question, we check the fine-grained Combination performance for
    the agreed and disagreed subsets between Direct Prompt and Inverse Prompt.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为回答这个问题，我们检查了直接提示和反向提示之间一致和不一致子集的细粒度组合性能。
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.1 Main Results ‣ 5 Results ‣ Direct-Inverse Prompting:
    Analyzing LLMs’ Discriminative Capacity in Self-Improving Generation") presents
    the performance of LLMs when they are certain (both Direct Prompt and Inverse
    Prompt agree) or uncertain (they conflict). It is clear that when Direct Prompt and
    Inverse Prompt agree, the answers are more likely to be correct, demonstrating
    significantly higher performance than both their disagreed subset and the overall
    dataset in Table [1](#S3.T1 "Table 1 ‣ Combination. ‣ 3 Direct-Inverse Discriminative
    Prompting ‣ Direct-Inverse Prompting: Analyzing LLMs’ Discriminative Capacity
    in Self-Improving Generation"). This further suggests that combining Direct Prompt and
    Inverse Prompt is an effective method for reducing uncertainty. If Direct Prompt and
    Inverse Prompt disagree, a comparison between Table [1](#S3.T1 "Table 1 ‣ Combination.
    ‣ 3 Direct-Inverse Discriminative Prompting ‣ Direct-Inverse Prompting: Analyzing
    LLMs’ Discriminative Capacity in Self-Improving Generation") and Table [3](#S5.T3
    "Table 3 ‣ 5.1 Main Results ‣ 5 Results ‣ Direct-Inverse Prompting: Analyzing
    LLMs’ Discriminative Capacity in Self-Improving Generation") indicates that Direct
    Prompt is the preferred approach. These conclusions generally apply to most LLMs,
    except for MetaMath, which is non-functional due to its pretraining limitations.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '表[3](#S5.T3 "Table 3 ‣ 5.1 Main Results ‣ 5 Results ‣ Direct-Inverse Prompting:
    Analyzing LLMs’ Discriminative Capacity in Self-Improving Generation")展示了LLMs在确定（Direct
    Prompt和Inverse Prompt一致）或不确定（它们冲突）情况下的表现。显然，当Direct Prompt和Inverse Prompt一致时，答案更可能正确，表现显著优于它们不一致的子集和表[1](#S3.T1
    "Table 1 ‣ Combination. ‣ 3 Direct-Inverse Discriminative Prompting ‣ Direct-Inverse
    Prompting: Analyzing LLMs’ Discriminative Capacity in Self-Improving Generation")中的整体数据集。这进一步表明，将Direct
    Prompt和Inverse Prompt结合是一种有效减少不确定性的方法。如果Direct Prompt和Inverse Prompt不一致，那么表[1](#S3.T1
    "Table 1 ‣ Combination. ‣ 3 Direct-Inverse Discriminative Prompting ‣ Direct-Inverse
    Prompting: Analyzing LLMs’ Discriminative Capacity in Self-Improving Generation")和表[3](#S5.T3
    "Table 3 ‣ 5.1 Main Results ‣ 5 Results ‣ Direct-Inverse Prompting: Analyzing
    LLMs’ Discriminative Capacity in Self-Improving Generation")的对比表明，Direct Prompt是首选方法。这些结论一般适用于大多数LLMs，但MetaMath由于其预训练限制而无法使用。'
- en: $\mathcal{Q}_{3}:$ When to suggest using Direct Prompt and Inverse Prompt to
    self-improve generation?
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $\mathcal{Q}_{3}:$ 什么时候建议使用Direct Prompt和Inverse Prompt来自我改进生成？
- en: 'Based on Table [1](#S3.T1 "Table 1 ‣ Combination. ‣ 3 Direct-Inverse Discriminative
    Prompting ‣ Direct-Inverse Prompting: Analyzing LLMs’ Discriminative Capacity
    in Self-Improving Generation"), we can summarize two criteria: i) For top-performing
    closed-source LLMs like GPT-4 and GPT-4o, using either Direct Prompt or Inverse
    Prompt, or their combination Combination, shows promise. These top LLMs perform
    similarly when Direct Prompt and Inverse Prompt are used separately. Combining
    them can result in robust performance, but the additional time and budget required
    for Combination may not be appealing. Therefore, the concise conclusion for the
    top-performing closed-source models is that either Direct Prompt or Inverse Prompt is
    sufficient. ii) For open-source LLMs, the decision to try discriminative prompts
    depends on two factors: a) If the LLMs are not optimized to follow instructions,
    such as MetaMath, neither Direct Prompt nor Inverse Prompt is recommended. b)
    Even if the model is instruction-tuned, open-source LLMs are more likely to struggle
    with understanding negation, so only Direct Prompt is strongly and exclusively
    recommended.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '基于表[1](#S3.T1 "Table 1 ‣ Combination. ‣ 3 Direct-Inverse Discriminative Prompting
    ‣ Direct-Inverse Prompting: Analyzing LLMs’ Discriminative Capacity in Self-Improving
    Generation")，我们可以总结出两个标准：i) 对于像GPT-4和GPT-4o这样的高性能封闭源LLMs，使用Direct Prompt或Inverse
    Prompt，或者它们的组合显示出前景。这些顶级LLMs在分别使用Direct Prompt和Inverse Prompt时表现相似。将它们结合可以获得强大的性能，但Combination所需的额外时间和预算可能不具吸引力。因此，对于高性能封闭源模型，Direct
    Prompt或Inverse Prompt中的任意一个都足够。ii) 对于开源LLMs，尝试区分提示的决策取决于两个因素：a) 如果LLMs未优化以遵循指令，例如MetaMath，建议不要使用Direct
    Prompt或Inverse Prompt。b) 即使模型经过指令调优，开源LLMs更可能在理解否定方面遇到困难，因此强烈且专门推荐使用Direct Prompt。'
- en: 6 Conclusion
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This study analyzed the development of LLM’s discriminative capability to enhance
    self-improving generation performance. Specifically, we introduce Direct-Inverse
    Discriminative Prompting, a multi-faceted complementary approach to evaluating
    LLMs’ discriminative potential. Our findings indicate that both Direct Prompt and
    Inverse Prompt are effective for closed-source LLMs, while for open-source LLMs,
    using Direct Prompt is highly and solely recommended.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究分析了 LLM 的辨别能力发展，以提升自我改进的生成性能。具体而言，我们引入了 Direct-Inverse Discriminative Prompting，这是一种多面向的补充方法，用于评估
    LLM 的辨别潜力。我们的发现表明，Direct Prompt 和 Inverse Prompt 对于封闭源 LLM 都是有效的，而对于开源 LLM，则高度且唯一推荐使用
    Direct Prompt。
- en: Limitations
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Our study is limited by the fact that experiments were conducted using only
    two datasets. In addition, if budget permits, exploring more closed-source LLMs
    is preferred.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究受到实验仅使用两个数据集的限制。此外，如果预算允许，建议探索更多的封闭源 LLM。
- en: Ethics Statement
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This study uses publicly and automatically accessed datasets, and no ethical
    issues are present.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究使用公开且自动访问的数据集，没有伦理问题存在。
- en: References
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Amini et al. (2019) Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski,
    Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math
    word problem solving with operation-based formalisms. In *Proceedings of NAACL-HLT*,
    pages 2357–2367.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amini 等 (2019) Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski,
    Yejin Choi, 和 Hannaneh Hajishirzi. 2019. Mathqa：基于操作的形式主义进行可解释的数学词题求解。在 *NAACL-HLT
    会议论文集中*，第 2357–2367 页。
- en: Arora and Kambhampati (2023) Daman Arora and Subbarao Kambhampati. 2023. [Learning
    and leveraging verifiers to improve planning capabilities of pre-trained language
    models](https://doi.org/10.48550/ARXIV.2305.17077). *CoRR*, abs/2305.17077.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 和 Kambhampati (2023) Daman Arora 和 Subbarao Kambhampati. 2023. [学习和利用验证器以提升预训练语言模型的规划能力](https://doi.org/10.48550/ARXIV.2305.17077)。*CoRR*，abs/2305.17077。
- en: Chen et al. (2023a) Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao,
    Pengcheng Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou. 2023a.
    [Universal self-consistency for large language model generation](https://arxiv.org/abs/2311.17311).
    *Preprint*, arXiv:2311.17311.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023a) Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng
    Yin, Sushant Prakash, Charles Sutton, Xuezhi Wang, 和 Denny Zhou. 2023a. [大语言模型生成的普遍自洽性](https://arxiv.org/abs/2311.17311)。*预印本*，arXiv:2311.17311。
- en: Chen et al. (2023b) Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    2023b. [Teaching large language models to self-debug](https://doi.org/10.48550/ARXIV.2304.05128).
    *CoRR*, abs/2304.05128.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 (2023b) Xinyun Chen, Maxwell Lin, Nathanael Schärli, 和 Denny Zhou. 2023b.
    [教会大型语言模型自我调试](https://doi.org/10.48550/ARXIV.2304.05128)。*CoRR*，abs/2304.05128。
- en: 'Gao et al. (2023) Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi
    Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and
    Kelvin Guu. 2023. [RARR: researching and revising what language models say, using
    language models](https://doi.org/10.18653/V1/2023.ACL-LONG.910). In *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, pages 16477–16508\.
    Association for Computational Linguistics.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等 (2023) Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi
    Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, 和
    Kelvin Guu. 2023. [RARR：研究和修订语言模型的输出，使用语言模型](https://doi.org/10.18653/V1/2023.ACL-LONG.910)。在
    *第 61 届计算语言学协会年会（第 1 卷：长篇论文），ACL 2023，多伦多，加拿大，2023 年 7 月 9-14 日*，第 16477–16508
    页。计算语言学协会。
- en: 'Gou et al. (2023) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu
    Yang, Nan Duan, and Weizhu Chen. 2023. [CRITIC: large language models can self-correct
    with tool-interactive critiquing](https://doi.org/10.48550/ARXIV.2305.11738).
    *CoRR*, abs/2305.11738.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou 等 (2023) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang,
    Nan Duan, 和 Weizhu Chen. 2023. [CRITIC：大型语言模型可以通过工具互动评估自我纠正](https://doi.org/10.48550/ARXIV.2305.11738)。*CoRR*，abs/2305.11738。
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical
    problem solving with the MATH dataset. In *Proceedings of NeurIPS*.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, 和 Jacob Steinhardt. 2021. 使用 MATH 数据集测量数学问题解决能力。在
    *NeurIPS 会议论文集中*。
- en: 'Jiang et al. (2024) Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir,
    Benjamin Van Durme, and Daniel Khashabi. 2024. [Self-[in]correct: Llms struggle
    with refining self-generated responses](https://arxiv.org/abs/2404.04298). *Preprint*,
    arXiv:2404.04298.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024) Dongwei Jiang, Jingyu Zhang, Orion Weller, Nathaniel Weir,
    Benjamin Van Durme, 和 Daniel Khashabi. 2024. [自-[不]正确：LLMs 在修正自生成回应时的困难](https://arxiv.org/abs/2404.04298)。*预印本*，arXiv:2404.04298。
- en: 'Li et al. (2024) Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin
    Wang, Bin Sun, Heda Wang, and Kan Li. 2024. [Escape sky-high cost: Early-stopping
    self-consistency for multi-step reasoning](https://arxiv.org/abs/2401.10480).
    *Preprint*, arXiv:2401.10480.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024) Yiwei Li, Peiwen Yuan, Shaoxiong Feng, Boyuan Pan, Xinglin
    Wang, Bin Sun, Heda Wang, 和 Kan Li. 2024. [逃避高昂成本：多步骤推理的早停自一致性](https://arxiv.org/abs/2401.10480)。*预印本*，arXiv:2401.10480。
- en: Liu et al. (2023) Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng
    Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan.
    2023. [Benchmarking generation and evaluation capabilities of large language models
    for instruction controllable summarization](https://doi.org/10.48550/ARXIV.2311.09184).
    *CoRR*, abs/2311.09184.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023) Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng
    Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, 和 Arman Cohan.
    2023. [大语言模型在指令可控总结方面的生成和评估能力基准测试](https://doi.org/10.48550/ARXIV.2311.09184)。*CoRR*，abs/2311.09184。
- en: 'Meta (2024) Meta. 2024. [Build the future of ai with meta llama 3](https://llama.meta.com/llama3/).
    Accessed: 2024-06-07.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meta (2024) Meta. 2024. [与 Meta Llama 3 一起构建 AI 的未来](https://llama.meta.com/llama3/)。访问日期：2024-06-07。
- en: Olausson et al. (2023) Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang,
    Jianfeng Gao, and Armando Solar-Lezama. 2023. [Demystifying GPT self-repair for
    code generation](https://doi.org/10.48550/ARXIV.2306.09896). *CoRR*, abs/2306.09896.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olausson et al. (2023) Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang,
    Jianfeng Gao, 和 Armando Solar-Lezama. 2023. [揭示 GPT 自我修复在代码生成中的神秘性](https://doi.org/10.48550/ARXIV.2306.09896)。*CoRR*，abs/2306.09896。
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://arxiv.org/abs/2303.08774).
    *Preprint*, arXiv:2303.08774.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. [GPT-4 技术报告](https://arxiv.org/abs/2303.08774)。*预印本*，arXiv:2303.08774。
- en: 'OpenAI (2024) OpenAI. 2024. [Hello gpt-4o](https://openai.com/index/hello-gpt-4o/).
    Accessed: 2024-06-07.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2024) OpenAI. 2024. [你好 GPT-4o](https://openai.com/index/hello-gpt-4o/)。访问日期：2024-06-07。
- en: 'Paul et al. (2024) Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, and Boi Faltings. 2024. [REFINER: reasoning feedback
    on intermediate representations](https://aclanthology.org/2024.eacl-long.67).
    In *Proceedings of the 18th Conference of the European Chapter of the Association
    for Computational Linguistics, EACL 2024 - Volume 1: Long Papers, St. Julian’s,
    Malta, March 17-22, 2024*, pages 1100–1126\. Association for Computational Linguistics.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paul et al. (2024) Debjit Paul, Mete Ismayilzada, Maxime Peyrard, Beatriz Borges,
    Antoine Bosselut, Robert West, 和 Boi Faltings. 2024. [REFINER：中间表示的推理反馈](https://aclanthology.org/2024.eacl-long.67)。发表于
    *第18届欧洲计算语言学协会会议论文集，EACL 2024 - 第1卷：长篇论文，马耳他圣朱利安，2024年3月17-22日*，第1100-1126页。计算语言学协会。
- en: 'Wang et al. (2023a) Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien,
    Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam
    Fazel-Zarandi, and Asli Celikyilmaz. 2023a. [Shepherd: A critic for language model
    generation](https://doi.org/10.48550/ARXIV.2308.04592). *CoRR*, abs/2308.04592.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean O’Brien,
    Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam
    Fazel-Zarandi, 和 Asli Celikyilmaz. 2023a. [Shepherd：语言模型生成的批评者](https://doi.org/10.48550/ARXIV.2308.04592)。*CoRR*，abs/2308.04592。
- en: Wang et al. (2023b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. [Self-consistency improves
    chain of thought reasoning in language models](https://arxiv.org/abs/2203.11171).
    *Preprint*, arXiv:2203.11171.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, 和 Denny Zhou. 2023b. [自一致性改善语言模型中的思维链推理](https://arxiv.org/abs/2203.11171)。*预印本*，arXiv:2203.11171。
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. [Chain-of-thought prompting
    elicits reasoning in large language models](https://arxiv.org/abs/2201.11903).
    *Preprint*, arXiv:2201.11903.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, 和 Denny Zhou. 2023. [思维链提示在大型语言模型中引发推理](https://arxiv.org/abs/2201.11903)。*预印本*，arXiv:2201.11903。
- en: 'Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2022. [Chain-of-thought
    prompting elicits reasoning in large language models](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).
    In *Advances in Neural Information Processing Systems 35: Annual Conference on
    Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA,
    November 28 - December 9, 2022*.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等（2022）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Brian Ichter、Fei
    Xia、Ed H. Chi、Quoc V. Le 和 Denny Zhou。2022年。[Chain-of-thought prompting elicits
    reasoning in large language models](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html)。在*Advances
    in Neural Information Processing Systems 35: Annual Conference on Neural Information
    Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December
    9, 2022*。'
- en: 'Yu et al. (2023) Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying
    Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023.
    Metamath: Bootstrap your own mathematical questions for large language models.
    *arXiv preprint arXiv:2309.12284*.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2023）Longhui Yu、Weisen Jiang、Han Shi、Jincheng Yu、Zhengying Liu、Yu Zhang、James
    T Kwok、Zhenguo Li、Adrian Weller 和 Weiyang Liu。2023年。Metamath: Bootstrap your own
    mathematical questions for large language models。*arXiv preprint arXiv:2309.12284*。'
- en: Appendix A Example Appendix
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 示例附录
- en: A.1 MATH
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 数学
- en: $\mathcal{Q}$$.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{Q}$。
- en: where “$\mathcal{Q}$” includes the answer in a specific format which is boxed{A},
    where A is the answer for the problem.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中“$\mathcal{Q}$”包括以特定格式框起来的答案{A}，其中 A 是问题的答案。
- en: A.2 MathQA
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 MathQA
- en: '$\mathcal{Q}$: a) 129 , b) 130 , c) 124 , d) 133 , e) 145'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{Q}$：a) 129 , b) 130 , c) 124 , d) 133 , e) 145
- en: '$\mathcal{A}$: a'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{A}$：a
- en: where "$\mathcal{Q}$" for answers.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 其中“$\mathcal{Q}$”用于答案。
