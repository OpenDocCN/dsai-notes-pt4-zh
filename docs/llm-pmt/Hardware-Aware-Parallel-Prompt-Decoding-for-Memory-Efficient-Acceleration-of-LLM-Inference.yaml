- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:43:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:43:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 硬件感知的并行提示解码用于内存高效加速LLM推理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18628](https://ar5iv.labs.arxiv.org/html/2405.18628)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.18628](https://ar5iv.labs.arxiv.org/html/2405.18628)
- en: \newfloatcommand
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newfloatcommand
- en: capbtabboxtable[][\FBwidth]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: capbtabboxtable[][\FBwidth]
- en: Hao (Mark) Chen¹  Wayne Luk¹  Ka Fai Cedric Yiu²
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Hao (Mark) Chen¹  Wayne Luk¹  Ka Fai Cedric Yiu²
- en: Rui Li³  Konstantin Mishchenko³  Stylianos I. Venieris³  Hongxiang Fan^(1,3)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Rui Li³  Konstantin Mishchenko³  Stylianos I. Venieris³  Hongxiang Fan^(1,3)
- en: ¹Imperial College London, UK
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ¹帝国理工学院，英国
- en: ²Hong Kong Polytechnic University, Hong Kong
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ²香港理工大学，香港
- en: ³Samsung AI Center, Cambridge, UK
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ³三星AI中心，剑桥，英国
- en: '{hc1620,w.luk}@ic.ac.uk  {rui.li,s.venieris}@samsung.com'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '{hc1620,w.luk}@ic.ac.uk  {rui.li,s.venieris}@samsung.com'
- en: konsta.mish@gmail.com  cedric.yiu@polyu.edu.hk  hongxiangfan@ieee.org
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: konsta.mish@gmail.com  cedric.yiu@polyu.edu.hk  hongxiangfan@ieee.org
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The auto-regressive decoding of Large Language Models (LLMs) results in significant
    overheads in their hardware performance. While recent research has investigated
    various speculative decoding techniques for multi-token generation, these efforts
    have primarily focused on improving processing speed such as throughput. Crucially,
    they often neglect other metrics essential for real-life deployments, such as
    memory consumption and training cost. To overcome these limitations, we propose
    a novel parallel prompt decoding that requires only $0.0002$ further speed improvement.
    Our code is available at [https://github.com/hmarkc/parallel-prompt-decoding](https://github.com/hmarkc/parallel-prompt-decoding).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的自回归解码会导致其硬件性能显著开销。虽然近期研究探讨了多标记生成的各种推测解码技术，这些努力主要集中在提高处理速度如吞吐量上。然而，它们往往忽视了现实部署中至关重要的其他指标，如内存消耗和训练成本。为克服这些限制，我们提出了一种新颖的并行提示解码方法，仅需$0.0002$的进一步速度提升。我们的代码可在[https://github.com/hmarkc/parallel-prompt-decoding](https://github.com/hmarkc/parallel-prompt-decoding)找到。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/fb1c33fd97409e332a49f9513a4a43fb.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fb1c33fd97409e332a49f9513a4a43fb.png)'
- en: 'Figure 1: Comparison of memory, speedup, and training cost on MT-Bench with
    Vicuna-7B. Circle diameter shows training GPU hours.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：与Vicuna-7B在MT-Bench上的内存、加速和训练成本比较。圆圈直径表示训练GPU小时数。
- en: The recent advances in large language models (LLMs) are increasingly gaining
    influence across various AI applications. However, autoregressive generation,
    the de facto approach employed in LLM inference, suffers from inadequate hardware
    performance due to its inherent sequential nature [[23](#bib.bib23)]. Speculative
    decoding [[13](#bib.bib13), [2](#bib.bib2), [11](#bib.bib11)], an emerging acceleration
    technique, employs a guess-and-verify framework for LLM inference, where a smaller
    draft model first predicts multiple tokens sequentially and then the original
    LLM verifies them in parallel. Despite its potential, the effectiveness of speculative
    decoding is limited by the complexity and cost of training a draft model capable
    of consistently achieving high acceptance rates across diverse base models and
    datasets. Additionally, the extra runtime memory overhead for executing draft
    models poses a significant barrier to the broader adoption of speculative decoding,
    particularly in edge and mobile environments where memory capacity is limited.
    Considering the growing need for user privacy and personalization, deploying LLMs
    on devices urges a more memory- and cost-efficient solution for accelerating LLM
    inference. Recent efforts have explored the possibility of generating multiple
    tokens in parallel without relying on a separate transformer draft model [[20](#bib.bib20)].
    Approaches such as inserting additional decoding heads [[1](#bib.bib1)] and retrieving
    frequently used tokens [[9](#bib.bib9)] are employed to enhance performance. However,
    these methods either aggressively assume conditional independence among the tokens
    generated in a single step [[1](#bib.bib1), [9](#bib.bib9)], or use placeholder
    tokens (e.g., [PAD] token) that do not convey enough contextual information [[20](#bib.bib20)].
    Therefore, they often suffer from low acceptance rates or degradation in output
    quality due to the lack of sufficient conditional information during inference.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的大型语言模型（LLMs）取得的进展在各种人工智能应用中越来越有影响力。然而，由于其固有的顺序性质，实际使用的自回归生成方法在LLM推理中受到硬件性能不足的限制[[23](#bib.bib23)]。一种新兴的加速技术——猜测解码[[13](#bib.bib13),
    [2](#bib.bib2), [11](#bib.bib11)]，采用了一种猜测和验证的框架进行LLM推理，其中一个较小的草稿模型首先顺序预测多个标记，然后由原始LLM并行验证这些标记。尽管具有潜力，但猜测解码的有效性受限于训练一个能够在各种基础模型和数据集上
    consistently achieving high acceptance rates 的草稿模型的复杂性和成本。此外，执行草稿模型所需的额外运行时内存开销是猜测解码更广泛采用的一个重要障碍，特别是在内存容量有限的边缘和移动环境中。考虑到对用户隐私和个性化的日益增长的需求，将LLMs
    部署到设备上需要一种更具内存和成本效率的解决方案来加速LLM推理。最近的努力探讨了在不依赖单独的变压器草稿模型的情况下并行生成多个标记的可能性[[20](#bib.bib20)]。例如，插入额外的解码头[[1](#bib.bib1)]和检索经常使用的标记[[9](#bib.bib9)]等方法被用来提升性能。然而，这些方法要么过度假设在单一步骤中生成的标记之间的条件独立性[[1](#bib.bib1),
    [9](#bib.bib9)]，要么使用的占位符标记（例如，[PAD] 标记）无法传达足够的上下文信息[[20](#bib.bib20)]。因此，它们往往由于在推理过程中缺乏足够的条件信息而遭遇低接受率或输出质量下降的问题。
- en: To alleviate the complexity and overhead associated with the use of draft models
    while maintaining a high acceptance rate, we propose Parallel Prompt Decoding
    (PPD), a novel architecture-agnostic and memory-efficient framework that adopts
    prompt tuning for non-autoregressive LLM inference. Inspired by the human natural
    language generation process where continuous words like common expressions and
    phrases are produced simultaneously, PPD introduces the use of prompt tokens,
    the meticulously trained embeddings, for multi-token prediction. Specifically,
    these trained prompt tokens are appended to the original input sequence in parallel,
    enabling the concurrent generation of multiple output tokens in a single forward
    pass. The key intuition of PPD lies in the observation that if trained properly,
    prompt tokens appended to the input can approximate tokens generated at future
    timesteps, thereby partially recovering the missing conditional dependency information
    for multi-token generation. By strategically positioning trained prompt tokens,
    PPD achieves up to a 28% higher acceptance rate when predicting long-range tokens.
    To further increase the token acceptance rate, we generate multiple candidate
    continuations with each prompt token and use them in combination with a customized
    tree attention mask to minimize the computation and memory overhead. The capability
    of PPD to use low-cost prompt tokens for accurate multi-token prediction forms
    the foundation for accelerating LLM inference. As shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference"), PPD achieves a comparable speedup to the state-of-the-art
    speculative decoding approaches with negligible memory overhead and reduced training
    cost. Moreover, to facilitate the optimized implementation of PPD across different
    hardware platforms, we propose a hardware-aware dynamic sparse tree technique
    that adaptively refines the prompt structure during runtime based on the computational
    resources available on the specific hardware.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻使用草稿模型所带来的复杂性和开销，同时保持高接受率，我们提出了并行提示解码（PPD），这是一种新颖的架构无关且内存高效的框架，采用提示调优来进行非自回归的
    LLM 推理。受到人类自然语言生成过程的启发，在这种过程中，像常见表达和短语这样的连续词汇会同时生成，PPD 引入了提示标记，这些精心训练的嵌入用于多标记预测。具体而言，这些训练过的提示标记会并行地附加到原始输入序列中，从而在一次前向传递中同时生成多个输出标记。PPD
    的关键直觉在于，如果训练得当，附加到输入中的提示标记可以近似未来时间步生成的标记，从而部分恢复多标记生成所缺失的条件依赖信息。通过战略性地定位训练过的提示标记，PPD
    在预测长距离标记时可实现高达 28% 的接受率提升。为了进一步提高标记接受率，我们生成多个候选续写，并将它们与自定义的树形注意力掩码结合使用，以最小化计算和内存开销。PPD
    利用低成本提示标记进行准确的多标记预测的能力，构成了加速 LLM 推理的基础。如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference") 所示，PPD 实现了与最先进的推测解码方法相当的加速，且内存开销微乎其微，训练成本降低。此外，为了促进 PPD 在不同硬件平台上的优化实施，我们提出了一种硬件感知的动态稀疏树技术，该技术在运行时根据特定硬件上的计算资源自适应地优化提示结构。
- en: '![Refer to caption](img/83cc2e570f9e9491521f830ba311fb6b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/83cc2e570f9e9491521f830ba311fb6b.png)'
- en: 'Figure 2: Overview of PPD. The left section shows the location of trainable
    parameters and the middle section displays the combined guess-and-verify process
    during inference. The "prompt token" denotes the special token with separately
    trained embeddings to perform parallel prediction.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：PPD 概述。左侧部分展示了可训练参数的位置，中间部分展示了推理过程中的联合猜测与验证过程。 "prompt token" 指的是具有独立训练嵌入的特殊标记，用于执行并行预测。
- en: To demonstrate the effectiveness of our approach, we evaluate PPD on MobileLLaMA
     [[6](#bib.bib6)], Vicuna-7b and Vicuna-13b [[5](#bib.bib5)]. Running on a single
    GPU using the A100-40GB and RTX 4090, our method achieves a speedup ratio for
    inference from 2.12$\times$ across a diverse range of popular datasets including
    MT-Bench, HumanEval, and GSM8K. Our experiments demonstrate that PPD not only
    achieves comparable throughput to the state-of-the-art speculative decoding method,
    but it also manages this with significantly fewer trainable parameters—specifically,
    0.0002% of trainable parameters—and incurs only a minimal memory overhead (0.0004%),
    showcasing that PPD is remarkably cost- and memory-efficient. The training of
    prompt tokens can be completed in 16 hours using one A100 GPU, 8 hours using four
    GeForce RTX 3090 GPUs, compared to the 1-2 days on four A100 GPUs required for
    Eagle [[16](#bib.bib16)]. Furthermore, since PPD does not require the modification
    of the original LLM or the addition of extra networks, it is highly adaptable
    and orthogonal to other decoding techniques. For instance, it can be effectively
    combined with a draft model to further reduce inference latency.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示我们方法的有效性，我们在MobileLLaMA [[6](#bib.bib6)]、Vicuna-7b和Vicuna-13b [[5](#bib.bib5)]上评估了PPD。在单个A100-40GB和RTX
    4090 GPU上运行，我们的方法在包括MT-Bench、HumanEval和GSM8K在内的各种流行数据集上实现了2.12$\times$的推理加速比。我们的实验表明，PPD不仅在吞吐量上与最先进的推测解码方法相当，而且用显著更少的可训练参数——具体为0.0002%的可训练参数——并且仅带来最小的内存开销（0.0004%），展示了PPD在成本和内存上的高效性。提示标记的训练可以在使用一个A100
    GPU的情况下完成16小时，使用四个GeForce RTX 3090 GPU的情况下完成8小时，相较于Eagle [[16](#bib.bib16)]在四个A100
    GPU上所需的1-2天。此外，由于PPD不需要修改原始LLM或添加额外的网络，因此它具有很高的适应性，并且与其他解码技术是正交的。例如，它可以有效地与草稿模型结合，以进一步减少推理延迟。
- en: 'Our contributions are summarized as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献总结如下：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A novel Parallel Prompt Decoding (PPD) that adopts cost-effective prompt tokens
    for non-autoregressive LLM inference, achieving a high acceptance rate for long-distance
    token prediction with preserved output quality.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种新颖的并行提示解码（PPD），采用具有成本效益的提示标记进行非自回归LLM推理，实现了对长距离标记预测的高接受率，同时保持了输出质量。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A hardware-aware dynamic sparse tree technique that adaptively optimizes the
    prompt structure of PPD at runtime based on the available compute and memory resources,
    facilitating its efficient deployment on various hardware platforms.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种硬件感知的动态稀疏树技术，根据可用的计算和内存资源在运行时自适应优化PPD的提示结构，促进其在各种硬件平台上的高效部署。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An open-source implementation of PPD, accompanied by comprehensive evaluations
    on various models and benchmarks. Our experiments demonstrate that PPD achieves
    significant speed improvements with negligible memory overhead and reduced training
    cost.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个开源的PPD实现，伴随着对各种模型和基准的全面评估。我们的实验表明，PPD在内存开销微不足道的情况下，实现了显著的速度提升，并且降低了训练成本。
- en: 2 Background and Related Work
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: To enhance the inference speed of LLM, various approaches adopt an iterative
    guess-and-verify strategy to enable multi-token generation. In the guessing phase,
    potential future tokens are proposed at a faster speed than in traditional autoregressive
    implementations. Subsequently, a parallelized verification process assesses which
    guessed tokens should be accepted. Depending on how tokens are generated during
    the guess stage, these approaches can generally be categorized as i) speculative
    decoding and ii) parallel decoding.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提升LLM的推理速度，各种方法采用了迭代猜测与验证策略，以实现多标记生成。在猜测阶段，提出潜在的未来标记的速度比传统的自回归实现更快。随后，通过并行化验证过程评估哪些猜测的标记应该被接受。根据标记在猜测阶段的生成方式，这些方法一般可以被归类为i) 推测解码和ii) 并行解码。
- en: 2.1 Speculative Decoding
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1 推测解码
- en: The guessing phase of speculative decoding adopts a lightweight draft model
    to generate multiple tokens at an increased speed [[11](#bib.bib11)]. During the
    verification stage, the original LLM subsequently determines the acceptance of
    the guessed tokens. It is worth noting that both draft and original models still
    follow the auto-regressive inference scheme. The speedup comes from two factors: i) the
    draft model runs much faster than the original model and more tokens can be generated
    within the same time unit; and ii) token verification is executed concurrently,
    either by batching or by incorporating multiple candidates into a single input
    using customized sparse attention masks [[18](#bib.bib18)]. Therefore, the overall
    speedup depends on the acceptance rate and the inference latency of draft models.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 投机解码的猜测阶段采用了轻量级的草稿模型，以更高的速度生成多个标记 [[11](#bib.bib11)]。在验证阶段，原始LLM随后确定猜测的标记是否被接受。值得注意的是，草稿模型和原始模型仍然遵循自回归推断方案。加速来自两个因素：i)
    草稿模型比原始模型运行得更快，能够在相同的时间单位内生成更多标记；ii) 标记验证并行执行，或通过批处理，或通过使用定制的稀疏注意力掩码 [[18](#bib.bib18)]
    将多个候选项合并到单个输入中。因此，总体加速取决于接受率和草稿模型的推断延迟。
- en: Building on the speculative decoding scheme, various studies have been conducted
    to further optimize its inference speed. To improve the accuracy of the draft
    model and its token acceptance rate, Eagle [[16](#bib.bib16)] incorporates the
    hidden features into the draft model’s forward pass. SpecInfer [[18](#bib.bib18)]
    adopts a tree-based speculative inference and verification scheme, improving the
    diversity of speculation candidates. Sequoia [[4](#bib.bib4)] optimizes the sparse
    tree structure by considering the capability of the underlying hardware platforms.
    However, most of these methods require the storage and maintenance of a separate
    draft model. Moreover, there is extra complexity in designing an efficient draft
    model.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在投机解码方案的基础上，已经进行了一些研究，以进一步优化其推断速度。为了提高草稿模型的准确性及其标记接受率，**Eagle** [[16](#bib.bib16)]
    将隐藏特征纳入草稿模型的前向传递中。**SpecInfer** [[18](#bib.bib18)] 采用基于树的投机推断和验证方案，提高了猜测候选项的多样性。**Sequoia**
    [[4](#bib.bib4)] 通过考虑底层硬件平台的能力优化了稀疏树结构。然而，这些方法大多数需要存储和维护单独的草稿模型。此外，设计高效草稿模型还有额外的复杂性。
- en: 2.2 Parallel Decoding
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2 并行解码
- en: To overcome the inherent limitations of autoregressive inference and the memory
    overhead associated with using a separate draft model, several attempts have been
    made to integrate both guessing and verification using one unified model. Medusa¹¹1We
    categorize Medusa as parallel decoding because it only adopts LM heads instead
    of separate models. [[1](#bib.bib1)] introduces language model (LM) heads at the
    final layer of the original LLM, facilitating the generation of multiple tokens
    in a single forward pass. It also utilizes tree attention masks in its verification
    process to increase speed even further. To enhance token drafting with retrieval-augmented
    generation [[10](#bib.bib10)], Rest [[9](#bib.bib9)] introduce retrieval-based
    decoding tailored for specific scenarios. Inspired by Jacobi decoding [[20](#bib.bib20)]
    that adopts multiple special tokens to accelerate machine translation, Lookahead
    Decoding [[8](#bib.bib8)] improves upon this method by generating parallel n-grams
    and employing a caching memory pool. To capture more information while using multiple
    special tokens at distinct positions, PaSS [[19](#bib.bib19)] trains additional
    tokens with embedding layers for parallel decoding. Hierarchical parallel decoding [[17](#bib.bib17)]
    introduces the use of $[Fork]$ tokens, enabling parallel execution of multiple
    structural subroutines.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服自回归推断的固有局限性和使用单独草稿模型所带来的内存开销，已经做出了几次尝试，通过一个统一的模型集成猜测和验证。**Medusa¹¹1** 被归类为并行解码，因为它只采用LM头而不是单独的模型。[[1](#bib.bib1)]
    在原始LLM的最终层引入语言模型（LM）头，便于在单次前向传递中生成多个标记。它还在验证过程中利用树状注意力掩码，进一步提高速度。为了通过检索增强生成 [[10](#bib.bib10)]
    来增强标记草拟，**Rest** [[9](#bib.bib9)] 介绍了针对特定场景量身定制的基于检索的解码。受到采用多个特殊标记以加速机器翻译的**Jacobi解码**
    [[20](#bib.bib20)] 的启发，**Lookahead Decoding** [[8](#bib.bib8)] 通过生成并行n-grams并使用缓存内存池改进了这种方法。为了在不同位置使用多个特殊标记时捕获更多信息，**PaSS**
    [[19](#bib.bib19)] 通过并行解码训练附加标记与嵌入层。**Hierarchical parallel decoding** [[17](#bib.bib17)]
    引入了 $[Fork]$ 标记，使得多个结构性子例程可以并行执行。
- en: Our approach can be categorized as parallel decoding, with three novel features
    to distinguish it from other approaches: 1) PPD trains the embeddings of parameterized
    ensemble prompt tokens, 2) it utilizes a dynamic sparse tree, adapting its structure
    at every inference step, and 3) we propose a hardware-aware algorithm for designing
    a dynamic sparse tree tailored to each hardware platform.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法可以被归类为并行解码，其具有三个新颖的特征，使其与其他方法区分开来：1) PPD 训练参数化的集合提示词的嵌入，2) 它利用动态稀疏树，在每一步推理中适应其结构，以及
    3) 我们提出了一种硬件感知算法，用于为每个硬件平台设计动态稀疏树。
- en: 3 Parallel Prompt Decoding (PPD)
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3 并行提示解码（PPD）
- en: 'PPD trains embeddings for prompt tokens rather than developing a separate model.
    Our method integrates three substeps into a single decoding step, following the
    guess-and-verify strategy: (1) candidate generation, where multiple candidate
    continuations²²2A candidate token, also referred to as a ”guess token”, is a draft
    token generated from a prompt token. are predicted by strategically inserting
    the prompt tokens into the input sequence. Tree attention [[18](#bib.bib18)] merges
    the processing of multiple candidates into a single forward pass; (2) candidate
    verification, where two verification schemes, exact matching [[8](#bib.bib8)]
    and typical acceptance [[1](#bib.bib1)], are implemented; (3) candidate acceptance,
    where validated candidates are integrated into the input and KV cache is updated
    accordingly. The inference scheme in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference") illustrates the generation and verification combined in a single
    forward pass.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: PPD 训练提示词的嵌入，而不是开发一个单独的模型。我们的方法将三个子步骤整合到一个解码步骤中，遵循猜测和验证策略：（1）候选生成，在此过程中，通过战略性地将提示词插入输入序列中来预测多个候选续写²²2候选词，也称为“猜测词”，是从提示词生成的草稿词。；（2）候选验证，其中实施了两种验证方案，精确匹配
    [[8](#bib.bib8)] 和典型接受 [[1](#bib.bib1)]；（3）候选接受，将验证过的候选词整合到输入中，并相应更新 KV 缓存。图 [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") 中的推理方案展示了在单个前向传播中结合生成和验证。
- en: 3.1 Prompt Tokens
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1 提示词
- en: The prompt tokens are the key component of PPD to realize multi-token generation.
    Initially introduced in [[12](#bib.bib12)] to adapt LLMs for specific tasks, prompt
    tokens are typically prepended to the input, with outputs generated in an autoregressive
    manner. In this work, we propose a novel approach of utilizing prompt tokens by
    strategically positioning them at locations where tokens are anticipated to be
    generated in parallel. For conventional parallel decoding techniques [[23](#bib.bib23),
    [1](#bib.bib1)] that presume complete conditional independence among tokens decoded
    in a single step, the exact conditional probability $p(y_{i+k+1}|x,y_{1:i+k})$.
    Through this forward pass in the decoder layers, these causally linked prompt
    tokens facilitate the flow of information along the sequence of speculative tokens,
    thus restoring the conditional probability.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 提示词是 PPD 实现多词生成的关键组件。最初在 [[12](#bib.bib12)] 中引入，以适应 LLM 特定任务，提示词通常被预置在输入之前，输出以自回归方式生成。在这项工作中，我们提出了一种利用提示词的新方法，通过将其战略性地放置在预测将并行生成的词的位置来实现。对于传统的并行解码技术
    [[23](#bib.bib23), [1](#bib.bib1)]，假设在单一步骤中解码的词完全条件独立，条件概率为 $p(y_{i+k+1}|x,y_{1:i+k})$。通过在解码器层中的前向传播，这些因果关联的提示词促进了信息沿着猜测词序列的流动，从而恢复了条件概率。
- en: 3.2 Ensemble Prompt Tokens
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2 集合提示词
- en: Inspired by prompt ensembling [[12](#bib.bib12)], which uses multiple prompts
    to generate diverse responses and aggregates these to derive a single answer,
    we introduce the concept of ensemble prompt token (EPT). This additional abstraction
    allows us to decouple each prompt token from the fixed embedding dimension. For
    every prompt token, there exist multiple corresponding EPTs, each with its distinct
    embedding. We modify the attention mask to ensure that each $n^{\text{th}}$. ⁴⁴4Further
    details about EPTs can be found in Appendix [B](#A2 "Appendix B Extended Ablation
    Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration
    of LLM Inference").
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 受到提示集合[[12](#bib.bib12)]的启发，该方法使用多个提示生成多样化的响应，并将这些响应聚合以得出单一答案，我们引入了集合提示标记 (EPT)
    的概念。这一额外的抽象允许我们将每个提示标记与固定的嵌入维度解耦。对于每个提示标记，都存在多个对应的 EPT，每个 EPT 都具有其独特的嵌入。我们修改了注意力掩码，以确保每个
    $n^{\text{th}}$。⁴⁴4有关 EPT 的进一步细节请参见附录 [B](#A2 "附录 B 扩展消融研究 ‣ 硬件感知并行提示解码以实现 LLM
    推理的内存高效加速")。
- en: 3.3 Training
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3 训练
- en: 'During training, only the embeddings of prompt tokens are changed, with the
    parameters of the original LLM remaining frozen. We adopt the following two training
    techniques:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，仅更改提示标记的嵌入，原始 LLM 的参数保持不变。我们采用以下两种训练技术：
- en: 'Random Insertion of Prompt Tokens: Randomly inserting prompt tokens throughout
    the input sequence reduces contextual bias from appending them only at the end.
    This approach broadens the predictive capacity of prompt tokens beyond a limited
    vocabulary such as  and punctuation.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机插入提示标记：在输入序列中随机插入提示标记，减少了仅在末尾附加它们所带来的上下文偏差。这种方法扩展了提示标记的预测能力，超越了有限的词汇表，如 
    和标点符号。
- en: 'Knowledge Distillation: To align the predictive behavior of prompt tokens with
    the original LLM, we employ knowledge distillation. Instead of using hard labels,
    prompt tokens are trained against the logits produced by the original LLM. Following
    Medusa [[1](#bib.bib1)], The loss function is formulated as:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏：为了使提示标记的预测行为与原始 LLM 对齐，我们采用知识蒸馏。我们并非使用硬标签，而是针对原始 LLM 产生的 logits 训练提示标记。遵循
    Medusa [[1](#bib.bib1)]，损失函数被公式化为：
- en: '|  | $L_{PD}=\frac{1}{N}\sum_{i=1}^{N}D_{KL}(P_{i}\parallel Q_{i})\cdot\alpha^{i-1},$
    |  | (1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{PD}=\frac{1}{N}\sum_{i=1}^{N}D_{KL}(P_{i}\parallel Q_{i})\cdot\alpha^{i-1},$
    |  | (1) |'
- en: where $D_{KL}$ is the decay ratio.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{KL}$ 是衰减比率。
- en: 4 Dynamic Sparse Tree
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4 动态稀疏树
- en: 4.1 Motivation
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1 动机
- en: To achieve higher speedup, PPD utilizes a specialized tree attention [[1](#bib.bib1),
    [18](#bib.bib18)] to process multiple candidates within a single decoding step
    without expanding the batch size. Notably, PPD employs a sparse tree [[1](#bib.bib1),
    [4](#bib.bib4)], designed to prioritize candidates with higher prediction accuracy.
    One key distinction from the sparse tree used in previous works is the appending
    of a sequence of prompt tokens to each tree node as shown in Figure [3](#S4.F3
    "Figure 3 ‣ 4.1 Motivation ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference"). To optimize the
    amortized acceptance length across decoding steps, it is crucial to carefully
    balance the number of candidate tokens and prompt tokens. Instead of appending
    a uniform number of prompt tokens to every candidate token, we allocate them based
    on each candidate’s probability, causing the tree’s maximum depth and structure
    to vary at each decoding step.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更高的加速，PPD 利用专门的树形注意力[[1](#bib.bib1), [18](#bib.bib18)] 在单个解码步骤中处理多个候选项，而无需扩展批处理大小。值得注意的是，PPD
    使用了稀疏树[[1](#bib.bib1), [4](#bib.bib4)]，旨在优先考虑预测准确性更高的候选项。与以前工作的稀疏树的一个关键区别是向每个树节点附加一系列提示标记，如图
    [3](#S4.F3 "图 3 ‣ 4.1 动机 ‣ 4 动态稀疏树 ‣ 硬件感知并行提示解码以实现 LLM 推理的内存高效加速") 所示。为了优化跨解码步骤的摊销接受长度，必须仔细平衡候选标记和提示标记的数量。我们根据每个候选项的概率分配提示标记，而不是向每个候选项附加统一数量的提示标记，这会导致树的最大深度和结构在每个解码步骤中有所不同。
- en: '![Refer to caption](img/d7c00009a37d210065004fb074abdba1.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d7c00009a37d210065004fb074abdba1.png)'
- en: 'Figure 3: Dynamic sparse tree.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：动态稀疏树。
- en: 4.2 Construction Algorithm
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2 构建算法
- en: We aim to construct a dynamic sparse tree that maximizes the amortized number
    of tokens generated with limited candidate tokens and prompt tokens. We first
    define the tree construction algorithm as a constrained optimization problem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是构建一个动态稀疏树，以在有限的候选标记和提示标记下最大化生成的标记数。我们首先将树构建算法定义为一个约束优化问题。
- en: Definition 4.1.
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 4.1。
- en: Let $m$.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 设$m$。
- en: Proposition 4.1.
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.1。
- en: For a dynamic sparse tree state $T_{k}$ to the expected number of tokens.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动态稀疏树状态$T_{k}$，预期的令牌数量。
- en: We then propose an approximation of the amortized number of tokens generated,
    by considering the tokens generated at the current and the next decoding step.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后提出了一种摊销令牌数量的近似方法，考虑了当前和下一个解码步骤生成的令牌。
- en: Proposition 4.2.
  id: totrans-64
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.2。
- en: The expected total number of tokens $F(T_{k})$.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 预期的总令牌数量$F(T_{k})$。
- en: We are now ready to introduce Proposition [4.3](#S4.Thmproposition3 "Proposition
    4.3\. ‣ 4.2 Construction Algorithm ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel
    Prompt Decoding for Memory-Efficient Acceleration of LLM Inference"), which we
    use in the pruning algorithm.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备介绍命题 [4.3](#S4.Thmproposition3 "命题 4.3. ‣ 4.2 构造算法 ‣ 4 动态稀疏树 ‣ 硬件感知的并行提示解码用于内存高效加速LLM推理")，这是我们在修剪算法中使用的。
- en: Proposition 4.3.
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.3。
- en: For a dynamic sparse tree state $T_{k}$.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于动态稀疏树状态$T_{k}$。
- en: 'To construct an approximately optimal dynamic sparse tree with specified numbers
    of candidate and prompt tokens, the process includes: (1) Optimal Candidate Trees:
    Constructing trees using only candidate tokens at varying depths, employing the
    algorithm from Medusa [[1](#bib.bib1)] and Sequoia [[4](#bib.bib4)] to maximize
    $f(T_{k})$ (Proposition [4.3](#S4.Thmproposition3 "Proposition 4.3\. ‣ 4.2 Construction
    Algorithm ‣ 4 Dynamic Sparse Tree ‣ Hardware-Aware Parallel Prompt Decoding for
    Memory-Efficient Acceleration of LLM Inference")), continuing until the desired
    prompt token count is reached.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个具有指定候选和提示令牌数量的近似最优动态稀疏树，过程包括：（1）最优候选树：使用候选令牌在不同深度上构建树，运用来自Medusa [[1](#bib.bib1)]和Sequoia [[4](#bib.bib4)]的算法，以最大化$f(T_{k})$（命题
    [4.3](#S4.Thmproposition3 "命题 4.3. ‣ 4.2 构造算法 ‣ 4 动态稀疏树 ‣ 硬件感知的并行提示解码用于内存高效加速LLM推理")），并继续直到达到所需的提示令牌数量。
- en: We now introduce the formulation of the real amortized number of tokens generated.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在介绍实际摊销生成令牌数量的公式。
- en: Proposition 4.4.
  id: totrans-71
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 命题 4.4。
- en: The amortized number of tokens $R(T_{k})$ is the function defined in Proposition
    [4.1](#S4.Thmproposition1 "Proposition 4.1\. ‣ 4.2 Construction Algorithm ‣ 4
    Dynamic Sparse Tree ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference").
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 摊销令牌数量$R(T_{k})$是命题 [4.1](#S4.Thmproposition1 "命题 4.1. ‣ 4.2 构造算法 ‣ 4 动态稀疏树
    ‣ 硬件感知的并行提示解码用于内存高效加速LLM推理")中定义的函数。
- en: 'Hardware-awareness. All the probabilities used above can be approximated on
    a validation dataset. The dynamic sparse tree construction algorithm can now be
    formulated as finding the dynamic sparse tree $T$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知。上述所有概率可以在验证数据集上进行近似。动态稀疏树构造算法现在可以表述为寻找动态稀疏树$T$：
- en: '|  | $c(n_{c},n_{p})=\max_{T,&#124;C(T)&#124;=n_{c},&#124;T&#124;=n_{c}+n_{p}}R(T).$
    |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $c(n_{c},n_{p})=\max_{T,|C(T)|=n_{c},|T|=n_{c}+n_{p}}R(T).$ |  |'
- en: For a fixed tree size $n$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于固定的树大小$n$。
- en: 5 Experiments
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5 实验
- en: Models and testbeds. We conducted all the experiments using MobileLLaMA-1.4B [[6](#bib.bib6)],
    Vicuna-7B and Vicuna-13B [[5](#bib.bib5)]. We used 3 prompt tokens and 1 EPT per
    prompt token for all inference experiments. The inference throughputs of the models
    are evaluated on a single NVIDIA A100 GPU with 40GB of memory and a GeForce RTX
    4090 using a batch size of 1 and FP16 precision. Further details about the experimental
    setup can be found in Appendix [C](#A3 "Appendix C Experiment Details ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference").
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和测试平台。我们使用MobileLLaMA-1.4B [[6](#bib.bib6)]、Vicuna-7B和Vicuna-13B [[5](#bib.bib5)]进行了所有实验。所有推理实验中使用了3个提示令牌和每个提示令牌1个EPT。模型的推理吞吐量在单个40GB内存的NVIDIA
    A100 GPU和GeForce RTX 4090上进行评估，使用批量大小为1和FP16精度。有关实验设置的更多细节可以在附录 [C](#A3 "附录 C
    实验细节 ‣ 硬件感知的并行提示解码用于内存高效加速LLM推理")中找到。
- en: Training. We froze all trainable parameters of the original LLM. Prompt token
    embeddings were trained using distillation logits generated from the ShareGPT
    dataset [[22](#bib.bib22)], with a maximum context length of 1024, a cosine learning
    rate scheduler starting at 0.01, and no warmup. Prompt token embeddings are initialized
    with normal text token embeddings.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 训练。我们冻结了原始LLM的所有可训练参数。提示令牌嵌入使用从ShareGPT数据集 [[22](#bib.bib22)] 生成的蒸馏logits进行训练，最大上下文长度为1024，余弦学习率调度器起始值为0.01，且没有预热。提示令牌嵌入使用正常的文本令牌嵌入初始化。
- en: Datasets. We assess the throughput performance of PPD across various tasks and
    datasets. Specifically, we evaluated PPD using the MT-Bench dataset [[25](#bib.bib25)],
    which contains multi-turn questions with a range of topics, in both non-greedy
    (temperature follows the default configuration) and greedy settings (temperature=0).
    We used the GSM8K [[7](#bib.bib7)] and HumanEval [[3](#bib.bib3)] datasets only
    in the greedy setting. The GSM8K dataset consists of grade school math problems
    and we used the first 500 questions of the test split for our evaluations. HumanEval
    includes coding tasks, for which we set a maximum new token limit of 512 to control
    the length of the generated sequences. We used the Alpaca [[15](#bib.bib15)] dataset
    as the validation dataset to produce the latencies and acceptance lengths used
    for dynamic sparse tree construction.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们评估了PPD在各种任务和数据集上的吞吐量性能。具体而言，我们使用了MT-Bench数据集[[25](#bib.bib25)]，该数据集包含了各种主题的多轮问题，包括非贪婪设置（温度遵循默认配置）和贪婪设置（温度=0）。我们仅在贪婪设置中使用了GSM8K[[7](#bib.bib7)]和HumanEval[[3](#bib.bib3)]数据集。GSM8K数据集包括了学级数学问题，我们使用了测试集的前500个问题进行评估。HumanEval包含编码任务，我们设置了最大新令牌限制为512，以控制生成序列的长度。我们使用了Alpaca[[15](#bib.bib15)]数据集作为验证数据集，以生成用于动态稀疏树构建的延迟和接受长度。
- en: 5.1 Speedup Comparison with Parallel Decoding Methods
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1 与并行解码方法的加速比较
- en: '![Refer to caption](img/ac2920a015e6501aa01dc2597efcf1d1.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ac2920a015e6501aa01dc2597efcf1d1.png)'
- en: 'Figure 4: Comparative evaluation of latency speedup between PPD and other parallel
    decoding methods. The experiments were conducted using the MT-Bench dataset, with
    the temperature set to MT-Bench’s default configuration for Medusa and PPD.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：PPD与其他并行解码方法在延迟加速上的比较评估。实验使用了MT-Bench数据集，温度设置为MT-Bench对Medusa和PPD的默认配置。
- en: We compare the speedup ratios of PPD with state-of-the-art parallel decoding
    methods on MT-Bench in non-greedy settings in Figure [4](#S5.F4 "Figure 4 ‣ 5.1
    Speedup Comparison with Parallel Decoding Methods ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference").
    PPD achieves speedups up to 13.8% higher than Medusa and between 2 times and 3
    times higher than other parallel decoding methods. We examine the factors contributing
    to the enhanced speedup ratios and other performance metrics, as presented in
    Table [1](#S5.T1 "Table 1 ‣ 5.1 Speedup Comparison with Parallel Decoding Methods
    ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference"). The reasons for the increase in speedup ratios
    are two-fold. Firstly, PPD produces candidate tokens with a higher acceptance
    rate than Medusa when utilizing a sparse tree of the same size. Notably, PPD continues
    to achieve a comparable or slightly better acceptance rate even when employing
    a much smaller sparse tree – ranging from one-third to half the size. Secondly,
    PPD benefits from lower forward pass latency due to its ability to use smaller
    sparse tree sizes and hence shorter input lengths. PPD also eliminates the computational
    overhead associated with separate decoding heads. PPD maintains the same output
    quality, achieving about the same score on MT-Bench while using significantly
    fewer trainable parameters.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了PPD与最先进的并行解码方法在MT-Bench上非贪婪设置下的加速比，如图[4](#S5.F4 "图 4 ‣ 5.1 与并行解码方法的加速比较
    ‣ 5 实验 ‣ 硬件感知的并行提示解码用于内存高效加速LLM推理")所示。PPD的加速比比Medusa高出最高13.8%，比其他并行解码方法高出2倍到3倍。我们检查了导致加速比提升的因素以及其他性能指标，如表[1](#S5.T1
    "表 1 ‣ 5.1 与并行解码方法的加速比较 ‣ 5 实验 ‣ 硬件感知的并行提示解码用于内存高效加速LLM推理")所示。加速比增加的原因有两个。首先，当利用相同大小的稀疏树时，PPD生成的候选令牌的接受率高于Medusa。值得注意的是，即使使用的稀疏树大大减小（从原大小的三分之一到一半），PPD仍能保持相当或略高的接受率。其次，PPD受益于较低的前向传播延迟，因为它能够使用更小的稀疏树尺寸，从而缩短输入长度。PPD还消除了与独立解码头相关的计算开销。PPD保持了相同的输出质量，在MT-Bench上获得了大致相同的分数，同时使用了显著更少的可训练参数。
- en: '| Model | Method | $T$ |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | $T$ |'
- en: '| M | Vanilla | 50.2 | 1.00 | 0.020 | - | NA | NA | 1 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| M | Vanilla | 50.2 | 1.00 | 0.020 | - | NA | NA | 1 |'
- en: '| PPD | 108.7 | 2.43 | 0.022 | Same | 4.50$e^{-4}$ | (10,84,89) | (40,285,285)
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| PPD | 108.7 | 2.43 | 0.022 | 相同 | 4.50$e^{-4}$ | (10,84,89) | (40,285,285)
    |'
- en: '| V-7B | Vanilla | 39.2 | 1.00 | 0.026 | 5.99 | NA | NA | 1 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| V-7B | Vanilla | 39.2 | 1.00 | 0.026 | 5.99 | NA | NA | 1 |'
- en: '| Medusa | 82.0 | 2.51 | 0.0307 | 5.98 | 8.07 | 63 | 63 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| Medusa | 82.0 | 2.51 | 0.0307 | 5.98 | 8.07 | 63 | 63 |'
- en: '| PPD | 88.0 | 2.54 | 0.029 | 5.93 | 1.82$e^{-4}$ | (10,33,34) | (40,105,105)
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| PPD | 88.0 | 2.54 | 0.029 | 5.93 | 1.82$e^{-4}$ | (10,33,34) | (40,105,105)
    |'
- en: '| V-13B | Vanilla | 30.4 | 1.00 | 0.0330 | 6.38 | NA | NA | 1 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| V-13B | 原始 | 30.4 | 1.00 | 0.0330 | 6.38 | NA | NA | 1 |'
- en: '| Medusa | 63.4 | 2.59 | 0.0408 | - | 5.52 | 63 | 63 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| Medusa | 63.4 | 2.59 | 0.0408 | - | 5.52 | 63 | 63 |'
- en: '| PPD | 66.1 | 2.44 | 0.0379 | 6.32 | 7.87$e^{-5}$ | (10,20,20) | (40,60,60)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| PPD | 66.1 | 2.44 | 0.0379 | 6.32 | 7.87$e^{-5}$ | (10,20,20) | (40,60,60)
    |'
- en: 'Table 1: Comparative performance metrics of MobileLLaMA (M) for greedy setting,
    Vicuna-7B (V-7B) and Vicuna-13B (V-13B) for non-greedy setting using different
    decoding methods. The table details throughput ($T$), represented as tuples. Same
    means the output matches with that of the original LLM.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：MobileLLaMA（M）在贪婪设置下，Vicuna-7B（V-7B）和Vicuna-13B（V-13B）在非贪婪设置下使用不同解码方法的比较性能指标。表格详细说明了吞吐量（$T$），以元组形式表示。相同表示输出与原始LLM匹配。
- en: Figure [5](#S5.F5 "Figure 5 ‣ 5.1 Speedup Comparison with Parallel Decoding
    Methods ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") displays the throughput of PPD on MT-Bench, HumanEval,
    and GSM8K with temperature equal to 0\. PPD achieves consistent walltime speedup
    ratios from 2.12$\times$. This can be attributed to the fact that both code and
    math equations often contain fixed patterns and repetitive symbols, which narrows
    the range of plausible candidates and simplifies the prediction. We also found
    that with typical acceptance, the speedup increases with temperature. Another
    notable trend is that smaller models, such as Vicuna-7B, generally achieve more
    significant speedup ratios as compared to larger models, like Vicuna-13B. PPD
    aims to generate more tokens per step, which comes with increased computational
    demands. For larger models that already require substantial computational resources,
    it is necessary to limit the size of the sparse tree to avoid exceeding the GPU’s
    utilization cap and causing increased latency. As a result, the number of tokens
    accepted per step is reduced, leading to lower speedups. However, this can be
    amortized when using more powerful GPUs than the NVIDIA A100 and the RTX 4090,
    such as NVIDIA H100.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S5.F5 "图 5 ‣ 5.1 并行解码方法的加速比较 ‣ 5 实验 ‣ 硬件感知的并行提示解码用于内存高效加速LLM推理")显示了PPD在MT-Bench、HumanEval和GSM8K上的吞吐量，温度设置为0。PPD实现了一致的墙时加速比，从2.12$\times$起。这可以归因于代码和数学公式通常包含固定模式和重复符号，这缩小了可能候选的范围并简化了预测。我们还发现，典型接受下，加速比随着温度的增加而增加。另一个显著的趋势是，相较于大型模型，如Vicuna-13B，较小的模型，如Vicuna-7B，通常实现了更显著的加速比。PPD旨在每步生成更多的tokens，这带来了更高的计算需求。对于已经需要大量计算资源的大型模型，必须限制稀疏树的大小，以避免超出GPU的利用上限并导致延迟增加。因此，每步接受的tokens数量减少，从而导致加速比降低。然而，当使用比NVIDIA
    A100和RTX 4090更强大的GPU，如NVIDIA H100时，这种情况可以得到弥补。
- en: '![Refer to caption](img/2fafdcc4d5855bef9bf01c005cc32ac4.png)![Refer to caption](img/16dc144812532ff123d93bd87c250800.png)![Refer
    to caption](img/769e8600c769e6529b01321a7cf4911c.png)![Refer to caption](img/1b7a6b7e739841607e07ae652a257925.png)![Refer
    to caption](img/37f4802fa5bb2fd715de8b6320a69e13.png)![Refer to caption](img/f40587693ba432d81d6aea41fde4dfa8.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2fafdcc4d5855bef9bf01c005cc32ac4.png)![参见说明](img/16dc144812532ff123d93bd87c250800.png)![参见说明](img/769e8600c769e6529b01321a7cf4911c.png)![参见说明](img/1b7a6b7e739841607e07ae652a257925.png)![参见说明](img/37f4802fa5bb2fd715de8b6320a69e13.png)![参见说明](img/f40587693ba432d81d6aea41fde4dfa8.png)'
- en: 'Figure 5: Throughput of PPD and vanilla models across different tasks. The
    temperature for experiments are set to 0 and the generated output of PPD exactly
    matches that of the original LLM. We do not show results of Vicuna-13B on RTX
    4090 as it does not fit into the GPU memory.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：PPD和原始模型在不同任务中的吞吐量。实验的温度设置为0，PPD生成的输出与原始LLM完全一致。我们没有展示RTX 4090上Vicuna-13B的结果，因为它无法装入GPU内存。
- en: 5.2 Long-range Token Prediction
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2 长程Token预测
- en: '![Refer to caption](img/cffd636015b3728d435d80793d4912eb.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cffd636015b3728d435d80793d4912eb.png)'
- en: (a) PD vs. Medusa
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PD vs. Medusa
- en: '![Refer to caption](img/af587a71942c3786939aacd9cb4d87c1.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/af587a71942c3786939aacd9cb4d87c1.png)'
- en: (b) 100 EPT vs. 1 EPT
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 100 EPT vs. 1 EPT
- en: '![Refer to caption](img/a4e13b56f0ff223a691b5a3fdde293b6.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a4e13b56f0ff223a691b5a3fdde293b6.png)'
- en: (c) 13b vs. 7b
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 13b vs. 7b
- en: 'Figure 6: Accumulative accuracy comparisons across different model configurations
    and prediction distances. ‘V7’ for Vicuna-7B, and ‘V13’ for Vicuna-13B. The notation
    ‘@$i$. ‘100 EPT’ represents 100 EPTs per prompt token. Accumulative accuracy is
    defined as top-k accuracy (e.g., a prediction is correct if the top-k candidates
    contain the ground truth). These measurements were obtained from the Alpaca Eval
    dataset with a maximum of 20 steps.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：不同模型配置和预测距离的累积准确率比较。“V7”代表 Vicuna-7B，“V13”代表 Vicuna-13B。符号‘@$i$. ‘100 EPT’表示每个提示标记100个EPT。累积准确率定义为前k准确率（例如，如果前k个候选项包含真实值，则预测正确）。这些测量值来自
    Alpaca Eval 数据集，最大步数为20。
- en: For a specific sparse tree, the accumulative accuracy provides a theoretical
    upper bound for the number of generated tokens per step and the maximum possible
    speedup ratio. Hence, maximizing accumulative accuracy is crucial for the effectiveness
    of PPD. Figure [6](#S5.F6 "Figure 6 ‣ 5.2 Long-range Token Prediction ‣ 5 Experiments
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference") demonstrates the accumulative accuracy of the tokens predicted
    at various positions. We summarize the following three key insights from the results.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的稀疏树，累积准确率提供了每步生成标记数量的理论上限和可能的最大加速比。因此，最大化累积准确率对 PPD 的有效性至关重要。图 [6](#S5.F6
    "Figure 6 ‣ 5.2 Long-range Token Prediction ‣ 5 Experiments ‣ Hardware-Aware Parallel
    Prompt Decoding for Memory-Efficient Acceleration of LLM Inference") 展示了在不同位置预测标记的累积准确率。我们从结果中总结出以下三个关键见解。
- en: PPD excels at predicting more distant tokens. As depicted in Figure [6(a)](#S5.F6.sf1
    "In Figure 6 ‣ 5.2 Long-range Token Prediction ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference"),
    PPD consistently outperforms Medusa in accuracy across all token positions. The
    accuracy gap between PPD and Medusa widens with the increased token distance (e.g.,
    the top-10 accuracy difference is 0.03 for the ‘next next’ word versus 0.12 for
    the ‘next next next next’ word). This improvement can be attributed to PPD’s ability
    to partially recover conditional dependency information through causally connected
    prompt tokens.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: PPD 在预测更远的标记方面表现优异。如图 [6(a)](#S5.F6.sf1 "In Figure 6 ‣ 5.2 Long-range Token
    Prediction ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") 所示，PPD 在所有标记位置上的准确率均优于 Medusa。PPD 和 Medusa 之间的准确率差距随着标记距离的增加而扩大（例如，“next
    next”词的前10名准确率差异为0.03，而“next next next next”词为0.12）。这种改进可归因于 PPD 能够通过因果连接的提示标记部分恢复条件依赖信息。
- en: PPD performs well at generating a broader array of plausible token candidates.
    For example, in predicting the token at a token distance of 3, the top-10 candidates
    exhibit an accuracy improvement of 0.1 over Medusa, compared to only 0.02 for
    the top-1 candidate. This demonstrates the value of using tree attention and the
    largest viable tree size during inference, as multiple candidate continuations
    further boost accuracy improvement.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: PPD 在生成更广泛的可能性标记候选项方面表现出色。例如，在预测一个距离为3的标记时，前10个候选项的准确率比 Medusa 提高了0.1，而前1个候选项仅提高了0.02。这表明，在推理过程中使用树形注意力和最大可行树大小的价值，因为多个候选项的延续进一步提升了准确率。
- en: Multiple EPTs per prompt token and larger model sizes yield modest improvements
    in prediction accuracy. Figure [6(b)](#S5.F6.sf2 "In Figure 6 ‣ 5.2 Long-range
    Token Prediction ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for
    Memory-Efficient Acceleration of LLM Inference") shows that using 100 EPTs per
    prompt token leads to accuracy improvement, ranging from 0.018 to 0.045. Figure [6(c)](#S5.F6.sf3
    "In Figure 6 ‣ 5.2 Long-range Token Prediction ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")
    displays that PPD with Vicuna-13B outperforms Vicuna-7B with an accuracy gain
    of 0.011$\thicksim$0.038\. This increase is due to Vicuna-13B’s greater embedding
    dimensions and deeper layers, which enhance the expressive power of prompt tokens.
    However, these gains are modest and can be offset by the increased computational
    burden of larger models.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 每个提示令牌的多个 EPT 和更大的模型尺寸带来了适度的预测准确性提升。图 [6(b)](#S5.F6.sf2 "在图 6 ‣ 5.2 长距离令牌预测
    ‣ 5 实验 ‣ 硬件感知并行提示解码以实现内存高效加速 LLM 推理") 显示，使用每个提示令牌 100 个 EPT 导致准确性提升，范围从 0.018
    到 0.045。图 [6(c)](#S5.F6.sf3 "在图 6 ‣ 5.2 长距离令牌预测 ‣ 5 实验 ‣ 硬件感知并行提示解码以实现内存高效加速 LLM
    推理") 显示，PPD 与 Vicuna-13B 的表现优于 Vicuna-7B，准确性提升了 0.011$\thicksim$0.038。这种提升归因于
    Vicuna-13B 的更大嵌入维度和更深层次，增强了提示令牌的表达能力。然而，这些增益较为适中，并可能被更大模型增加的计算负担所抵消。
- en: 5.3 Memory Efficiency and Synergistic Integrations with Speculative Decoding
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3 记忆效率与推测解码的协同集成
- en: '![Refer to caption](img/695694b18033060589255ac4174d4456.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/695694b18033060589255ac4174d4456.png)'
- en: 'Figure 7: Model memory usage.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：模型内存使用情况。
- en: Memory efficiency. As shown in Figure [7](#S5.F7 "Figure 7 ‣ 5.3 Memory Efficiency
    and Synergistic Integrations with Speculative Decoding ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference"),
    we compare the memory overhead of PPD with the leading parallel decoding (Medusa)
    and speculative decoding approaches (Eagle). The memory overhead of PPD is just
    0.004% of Medusa’s and 0.007% of Eagle’s. This efficiency stems from the efficient
    use of embeddings in PPD, which are significantly smaller than decoding heads
    and draft models, both of which scale with vocabulary size.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 内存效率。如图 [7](#S5.F7 "图 7 ‣ 5.3 记忆效率与推测解码的协同集成 ‣ 5 实验 ‣ 硬件感知并行提示解码以实现内存高效加速 LLM
    推理") 所示，我们比较了 PPD 与领先的并行解码（Medusa）和推测解码方法（Eagle）的内存开销。PPD 的内存开销仅为 Medusa 的 0.004%
    和 Eagle 的 0.007%。这种效率源于 PPD 中对嵌入的高效利用，这些嵌入明显小于解码头和草稿模型，这两者的规模随词汇量的增加而增加。
- en: PPD + Speculative Decoding. As an orthogonal optimization in accelerating LLMs,
    PPD can be easily integrated with speculative decoding [[11](#bib.bib11)]. To
    demonstrate this, we applied PPD to Vicuna-68M [[24](#bib.bib24)] and used it
    as the draft model for Vicuna-7B. This combination resulted in a speedup of up
    to 1.22$\times$ for speculative decoding on Vicuna-7B compared to using speculative
    decoding alone.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PPD + 推测解码。作为加速 LLM 的一个正交优化，PPD 可以很容易地与推测解码集成[[11](#bib.bib11)]。为了演示这一点，我们将
    PPD 应用于 Vicuna-68M [[24](#bib.bib24)]，并将其作为 Vicuna-7B 的草稿模型。这一组合使得与单独使用推测解码相比，Vicuna-7B
    的推测解码速度提高了最多 1.22$\times$。
- en: 5.4 Ablation Study
  id: totrans-114
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4 消融研究
- en: Dynamic Sparse Tree. Figure [8(a)](#S5.F8.sf1 "In Figure 8 ‣ 5.4 Ablation Study
    ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") shows that dynamic sparse trees consistently achieve
    longer acceptance lengths compared to static and random ones across varying sizes.
    The acceptance length for dynamic sparse trees shows a steady increase as the
    tree size extends, suggesting its good scalability. The convergence of dynamic
    and static sparse trees at larger sizes suggests a structural similarity emerging
    from constraints in tree depth and tree node count.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 动态稀疏树。图 [8(a)](#S5.F8.sf1 "在图 8 ‣ 5.4 消融研究 ‣ 5 实验 ‣ 硬件感知并行提示解码以实现内存高效加速 LLM
    推理") 显示，与静态和随机树相比，动态稀疏树在不同大小下始终实现了更长的接受长度。动态稀疏树的接受长度随着树的大小增加而稳定增长，表明其良好的可扩展性。动态和静态稀疏树在较大尺寸下的收敛表明在树深度和树节点数的限制下，结构上出现了相似性。
- en: Hardware-aware Tree Size. Figure [8(b)](#S5.F8.sf2 "In Figure 8 ‣ 5.4 Ablation
    Study ‣ 5 Experiments ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") presents the theoretical speedup across different
    GPUs. Figure [8(c)](#S5.F8.sf3 "In Figure 8 ‣ 5.4 Ablation Study ‣ 5 Experiments
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference") validates that the optimal sparse tree size, derived from theoretical
    speedup models, indeed results in the greatest actual speedup observed.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知树大小。图[8(b)](#S5.F8.sf2 "图8 ‣ 5.4 消融研究 ‣ 5 实验 ‣ 硬件感知的并行提示解码，用于内存高效加速LLM推理")展示了不同GPU上的理论加速比。图[8(c)](#S5.F8.sf3
    "图8 ‣ 5.4 消融研究 ‣ 5 实验 ‣ 硬件感知的并行提示解码，用于内存高效加速LLM推理")验证了从理论加速模型中获得的最优稀疏树大小确实带来了观察到的最大实际加速。
- en: '![Refer to caption](img/7f2200db0837d718ac9529d52e25c608.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7f2200db0837d718ac9529d52e25c608.png)'
- en: (a)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/577f6b437e78d028817d5df937652e63.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/577f6b437e78d028817d5df937652e63.png)'
- en: (b)
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/2f15873121e7c5b01e7190046c2bfed6.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2f15873121e7c5b01e7190046c2bfed6.png)'
- en: (c)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 8: Evaluation of Dynamic Sparse Tree Performance. The static sparse
    trees in (a) always use the largest possible prompt tokens for each candidate.
    The theoretical speedup in (b) is calculated as the ratio of acceptance lengths
    (hardware-independent) to latency overhead (hardware-dependent). The optimal tree
    size is obtained from the peak value of the theoretical speedup. The latencies
    in (b) are obtained from inference on the same prompt for 512 forward passes.
    (c) shows the actual speedup obtained by running inference on different GPUs with
    different tree lengths on Alpaca Eval dataset.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：动态稀疏树性能评估。(a)中的静态稀疏树总是使用每个候选项的最大可能提示符。理论加速比(b)计算为接受长度（硬件无关）与延迟开销（硬件相关）的比率。理论加速比的峰值对应的最优树大小是从理论模型中获得的。(b)中的延迟是对相同提示符进行512次前向传递的推理结果。(c)显示了在不同GPU上对Alpaca
    Eval数据集进行推理时，不同树长度所获得的实际加速比。
- en: 6 Conclusion
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6 结论
- en: We introduced PPD, a memory-efficient, cost-effective, and powerful parallel
    decoding method that incorporates a hardware-aware dynamic sparse tree. Utilizing
    specially trained prompt tokens to predict long-range tokens accurately, PPD achieves
    a speedup of up to 2.49$\times$ in inference while employing only 0.0002% additional
    trainable parameters and without incorporating new models or architectural components.
    We believe that PPD offers a novel perspective on the capabilities of parallel
    decoding. In future work, it could be synergized with other speculative or parallel
    decoding techniques to expedite inference even further.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了PPD，一种内存高效、成本有效且强大的并行解码方法，结合了硬件感知的动态稀疏树。通过利用特别训练的提示符来准确预测长距离令牌，PPD实现了高达2.49$\times$的推理加速，同时只增加了0.0002%的可训练参数，并且不引入新的模型或架构组件。我们认为PPD为并行解码的能力提供了一个新的视角。在未来的工作中，它可以与其他推测或并行解码技术协同作用，以进一步加速推理。
- en: References
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Cai et al. [2024] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D.
    Lee, Deming Chen, and Tri Dao. Medusa: Simple LLM Inference Acceleration Framework
    with Multiple Decoding Heads. *arXiv preprint arXiv:2401.10774*, 2024.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '蔡等人 [2024] 田乐·蔡、余红·李、郑阳·耿、洪武·彭、杰森·D·李、德明·陈和三道。Medusa: 简单的LLM推理加速框架，具有多个解码头。*arXiv
    预印本 arXiv:2401.10774*，2024年。'
- en: Chen et al. [2023] Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste
    Lespiau, Laurent Sifre, and John Jumper. Accelerating Large Language Model Decoding
    with Speculative Sampling. *arXiv preprint arXiv:2302.01318*, 2023.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2023] 查理·陈、塞巴斯蒂安·博尔戈德、杰弗里·欧文、让-巴蒂斯特·莱斯皮亚、洛朗·西弗和约翰·跳默。通过推测采样加速大语言模型解码。*arXiv
    预印本 arXiv:2302.01318*，2023年。
- en: Chen et al. [2021] Mark Chen et al. Evaluating Large Language Models Trained
    on Code. *arXiv preprint arXiv:2107.03374*, 2021.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2021] 马克·陈等人。评估训练于代码上的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021年。
- en: 'Chen et al. [2024] Zhuoming Chen, Avner May, Ruslan Svirschevski, Yuhsun Huang,
    Max Ryabinin, Zhihao Jia, and Beidi Chen. Sequoia: Scalable, Robust, and Hardware-aware
    Speculative Decoding. *arXiv preprint arXiv:2402.12374*, 2024.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2024] 朱铭陈、阿夫纳·梅、鲁斯兰·斯维尔谢夫斯基、黄玉顺、马克斯·里亚宾宁、智浩·贾和贝迪·陈。Sequoia: 可扩展、鲁棒且硬件感知的推测解码。*arXiv
    预印本 arXiv:2402.12374*，2024年。'
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An Open-Source Chatbot Impressing GPT-4
    with 90%* ChatGPT Quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang et al. [2023] 魏林·姜，卓焕·李，子林，英胜，张浩·吴，张浩，廉敏·郑，思源·庄，永浩·庄，约瑟夫·E·冈萨雷斯，伊昂·斯托伊卡，埃里克·P·星。Vicuna：一款开放源代码的聊天机器人，以90%*
    ChatGPT质量令GPT-4印象深刻，2023年3月。网址 [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: 'Chu et al. [2023] Xiangxiang Chu, Limeng Qiao, Xinyang Lin, Shuang Xu, Yang
    Yang, Yiming Hu, Fei Wei, Xinyu Zhang, Bo Zhang, Xiaolin Wei, and Chunhua Shen.
    MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices.
    *arXiv preprint arXiv:2312.16886*, 2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chu et al. [2023] 朱向向，乔利萌，林欣阳，徐爽，杨洋，胡义名，魏飞，张鑫瑜，张博，魏小林，沈春华。MobileVLM：一款快速、强大且开放的移动设备视觉语言助手。*arXiv
    预印本 arXiv:2312.16886*，2023年。
- en: Cobbe et al. [2021] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe et al. [2021] 卡尔·科贝，维尼特·科萨拉朱，穆罕默德·巴瓦里安，马克·陈，许宇俊，卢卡斯·凯瑟，马蒂亚斯·普拉普特，杰瑞·特沃雷克，雅各布·希尔顿，中野礼一郎，克里斯托弗·赫斯，约翰·舒尔曼。训练验证器以解决数学文字题。*arXiv
    预印本 arXiv:2110.14168*，2021年。
- en: Fu et al. [2023] Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Breaking
    the Sequential Dependency of LLM Inference Using Lookahead Decoding, November
    2023. URL [https://lmsys.org/blog/2023-11-21-lookahead-decoding/](https://lmsys.org/blog/2023-11-21-lookahead-decoding/).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu et al. [2023] 余超，彼得·贝利斯，伊昂·斯托伊卡，张浩。利用前瞻解码打破 LLM 推理的顺序依赖，2023年11月。网址 [https://lmsys.org/blog/2023-11-21-lookahead-decoding/](https://lmsys.org/blog/2023-11-21-lookahead-decoding/)。
- en: 'He et al. [2023] Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He.
    Rest: Retrieval-based Speculative Decoding. *arXiv preprint arXiv:2311.08252*,
    2023.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. [2023] 赫振宇，钟泽轩，蔡天乐，杰森·D·李，和 赫迪。Rest：基于检索的推测解码。*arXiv 预印本 arXiv:2311.08252*，2023年。
- en: Karpukhin et al. [2020] Vladimir Karpukhin, Barlas Oğuz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense Passage Retrieval
    for Open-Domain Question Answering. In *Proceedings of the 2020 Conference on
    Empirical Methods in Natural Language Processing (EMNLP)*, pages 6769–6781, 2020.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpukhin et al. [2020] 弗拉基米尔·卡普库欣，巴拉斯·奥古兹，斯翁·敏，帕特里克·刘易斯，莱德尔·吴，谢尔盖·埃杜诺夫，陈丹琪，温涛·伊。开放域问答的密集段落检索。在
    *2020年自然语言处理实证方法会议（EMNLP）* 上，第6769–6781页，2020年。
- en: Kim et al. [2024] Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik,
    Michael W. Mahoney, Amir Gholami, and Kurt Keutzer. Speculative Decoding with
    Big Little Decoder. *Advances in Neural Information Processing Systems (NeurIPS)*,
    36, 2024.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim et al. [2024] 金世勋，卡特基亚·曼加拉姆，苏宏·穆恩，吉滕德拉·马利克，迈克尔·W·马霍尼，阿米尔·戈拉米，库尔特·凯特泽。利用大小解码器的推测解码。*神经信息处理系统进展（NeurIPS）*，第36卷，2024年。
- en: Lester et al. [2021] Brian Lester, Rami Al-Rfou, and Noah Constant. The Power
    of Scale for Parameter-Efficient Prompt Tuning. In *Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, 2021.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. [2021] 布赖恩·莱斯特，拉米·阿尔-拉福，诺亚·康斯坦特。规模化对参数高效提示调优的影响。在 *自然语言处理实证方法会议（EMNLP）*
    上，2021年。
- en: Leviathan et al. [2023] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    Inference from Transformers via Speculative Decoding. In *International Conference
    on Machine Learning (ICML)*, 2023.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan et al. [2023] 亚尼夫·利维坦，马坦·卡尔曼，约西·马蒂亚斯。通过推测解码实现变换器的快速推理。在 *国际机器学习会议（ICML）*
    上，2023年。
- en: 'Li and Liang [2021] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 4582–4597\.
    Association for Computational Linguistics, 2021.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Liang [2021] 李翔（Lisa Li）和 Percy Liang。前缀调优：优化连续提示以进行生成。在 *第59届计算语言学协会年会及第11届国际自然语言处理联合会议（第1卷：长篇论文）*
    中，第4582–4597页。计算语言学协会，2021年。
- en: 'Li et al. [2023] Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. AlpacaEval:
    An Automatic Evaluator of Instruction-following Models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval),
    2023.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023] 李雪辰、张天逸、扬·迪布瓦、罗汉·塔奥里、伊莎安·古尔拉贾尼、卡洛斯·古斯特林、珀西·梁和辰修·B·哈希莫托。AlpacaEval:
    一种自动评估指令跟随模型的工具。 [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval)，2023。'
- en: 'Li et al. [2024] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE:
    Speculative Sampling Requires Rethinking Feature Uncertainty. *arXiv preprint
    arXiv:2401.15077*, 2024.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2024] 李宇辉、魏方云、张超和张宏阳。EAGLE: 推测采样需要重新思考特征不确定性。*arXiv 预印本 arXiv:2401.15077*，2024。'
- en: 'Liu et al. [2024] Mingdao Liu, Aohan Zeng, Bowen Wang, Peng Zhang, Jie Tang,
    and Yuxiao Dong. APAR: LLMs can do auto-parallel auto-regressive decoding. *arXiv
    preprint arXiv:2401.06761*, 2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2024] 刘明道、曾敖寒、王博文、张鹏、唐杰和董宇霄。APAR: LLMs 可以进行自动并行自回归解码。*arXiv 预印本
    arXiv:2401.06761*，2024。'
- en: 'Miao et al. [2024] Xupeng Miao et al. SpecInfer: Accelerating Large Language
    Model Serving with Tree-based Speculative Inference and Verification. In *ACM
    International Conference on Architectural Support for Programming Languages and
    Operating Systems (ASPLOS)*, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Miao et al. [2024] 矛旭鹏等。SpecInfer: 使用基于树的推测推理和验证加速大语言模型服务。发表于*ACM 国际程序设计语言和操作系统架构支持会议
    (ASPLOS)*，2024。'
- en: 'Monea et al. [2023] Giovanni Monea, Armand Joulin, and Edouard Grave. PaSS:
    Parallel Speculative Sampling. *arXiv preprint arXiv:2311.13581*, 2023.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Monea et al. [2023] 乔瓦尼·莫尼亚、阿尔芒·朱林和爱德华·格雷夫。PaSS: 并行推测采样。*arXiv 预印本 arXiv:2311.13581*，2023。'
- en: Santilli et al. [2023] Andrea Santilli, Silvio Severino, Emilian Postolache,
    Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. Accelerating
    Transformer Inference for Translation via Parallel Decoding. In *Annual Meeting
    of the Association for Computational Linguistics (ACL)*, 2023.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santilli et al. [2023] 安德烈亚·桑蒂利、席尔维奥·塞维里诺、埃米利安·波斯托拉切、瓦伦蒂诺·马约卡、米歇尔·曼库西、里卡尔多·马林和埃马努埃尔·罗多拉。通过并行解码加速变换器推理以进行翻译。发表于*计算语言学协会年会
    (ACL)*，2023。
- en: Saxena [2023] Apoorv Saxena. Prompt Lookup Decoding, November 2023. URL [https://github.com/apoorvumang/prompt-lookup-decoding/](https://github.com/apoorvumang/prompt-lookup-decoding/).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saxena [2023] 阿普尔沃·萨克塞纳。提示查找解码，2023年11月。网址 [https://github.com/apoorvumang/prompt-lookup-decoding/](https://github.com/apoorvumang/prompt-lookup-decoding/)。
- en: ShareGPT [2023] ShareGPT. ShareGPT, 2023. URL [https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered).
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ShareGPT [2023] ShareGPT。ShareGPT，2023年。网址 [https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered](https://huggingface.co/datasets/Aeala/ShareGPT_Vicuna_unfiltered)。
- en: Stern et al. [2018] Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise
    Parallel Decoding for Deep Autoregressive Models. In *Advances in Neural Information
    Processing Systems (NeurIPS)*, 2018.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stern et al. [2018] 米切尔·斯特恩、诺姆·沙泽尔和雅各布·乌斯科雷特。深度自回归模型的块状并行解码。发表于*神经信息处理系统进展 (NeurIPS)*，2018。
- en: Yang et al. [2024] Sen Yang, Shujian Huang, Xinyu Dai, and Jiajun Chen. Multi-Candidate
    Speculative Decoding. *arXiv preprint arXiv:2401.06706*, 2024.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2024] 杨森、黄书剑、戴欣瑜和陈佳军。多候选推测解码。*arXiv 预印本 arXiv:2401.06706*，2024。
- en: Zheng et al. [2023] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging LLM-as-a-Judge with MT-Bench
    and Chatbot Arena. In *Advances in Neural Information Processing Systems (NeurIPS)*,
    2023.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. [2023] 郑连敏、姜伟麟、盛颖、庄思远、吴章浩、庄永浩、林紫、李卓涵、李大成、埃里克·P·邢、张浩、约瑟夫·E·冈萨雷斯和伊昂·斯托伊卡。通过
    MT-Bench 和聊天机器人竞技场评估 LLM 作为裁判的能力。发表于*神经信息处理系统进展 (NeurIPS)*，2023。
- en: Supplementary Material
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 补充材料
- en: Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件感知的并行提示解码，用于内存高效加速 LLM 推理
- en: \doparttoc\faketableofcontents\parttoc
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: \doparttoc\faketableofcontents\parttoc
- en: Appendix A Training Loss
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 A 训练损失
- en: '![Refer to caption](img/fb93c1feb73074a26b3a56fd516e6e9b.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb93c1feb73074a26b3a56fd516e6e9b.png)'
- en: (a) 3 prompt tokens, 1 EPTs
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 3 提示令牌，1 EPTs
- en: '![Refer to caption](img/5bcaa9f654594d66db214b02006803cd.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5bcaa9f654594d66db214b02006803cd.png)'
- en: (b) 3 prompt tokens, 100 EPTs
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 3 提示令牌，100 EPTs
- en: 'Figure 9: Training Loss'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 训练损失'
- en: We study the training loss of PPD with different EPTs. Figure [9(a)](#A1.F9.sf1
    "In Figure 9 ‣ Appendix A Training Loss ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference") shows that, with 3 prompt
    tokens and 1 EPT, the initial loss is quite high, starting above 5\. There is
    a sharp decrease in loss within the first epoch, dropping below 2\. After this
    initial drop, the loss stabilizes and oscillates around a value slightly below
    2 for the remainder of the training epochs (up to epoch 12). The loss oscillations
    remain within a narrow range, indicating consistent performance. The fluctuation
    can be attributed to the insertion of prompt tokens at random positions. On the
    other hand, Figure [9(b)](#A1.F9.sf2 "In Figure 9 ‣ Appendix A Training Loss ‣
    Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM
    Inference"), with 3 prompt tokens and 100 EPTs, shows the initial loss starting
    below 3, significantly lower than PPD with 1 EPT. Similarly, there is a sharp
    decrease within the first epoch, with the loss dropping to around 2.5\. However,
    unlike PPD with 1 EPT, the loss continues to decrease gradually over the epochs,
    showing a downward trend. This suggests that increasing the number of EPTs improves
    the model’s learning capacity and reduce training loss more effectively over time.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了不同 EPT 下 PPD 的训练损失。图[9(a)](#A1.F9.sf1 "图 9 ‣ 附录 A 训练损失 ‣ 硬件感知的并行提示解码用于内存高效加速
    LLM 推断") 显示，使用 3 个提示 token 和 1 个 EPT 时，初始损失相当高，超过 5。第一轮训练中损失急剧下降，降至 2 以下。初始下降后，损失稳定并在接下来的训练轮次（最多到第
    12 轮）中波动在略低于 2 的值附近。损失波动保持在一个狭窄的范围内，表明性能一致。波动可以归因于提示 token 随机插入的位置。另一方面，图[9(b)](#A1.F9.sf2
    "图 9 ‣ 附录 A 训练损失 ‣ 硬件感知的并行提示解码用于内存高效加速 LLM 推断") 显示，使用 3 个提示 token 和 100 个 EPT
    时，初始损失低于 3，明显低于 1 个 EPT 的 PPD。类似地，在第一轮中损失急剧下降，降至约 2.5。然而，与 1 个 EPT 的 PPD 不同，损失在接下来的轮次中逐渐减少，呈现出下降趋势。这表明增加
    EPT 数量可以更有效地提高模型的学习能力，并随时间减少训练损失。
- en: Appendix B Extended Ablation Study
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 B 扩展消融研究
- en: B.1 Effect of EPTs on Prediction Accuracy
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.1 EPT 对预测准确率的影响
- en: 'Table 2: Prediction Accuracy of PPD with different EPTs. ’@i’ denotes a token
    distance of i. ’Top-k’ denotes the top-k prediction accuracy. The results are
    obtained on Alpaca dataset with 20 steps.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 不同 EPT 下 PPD 的预测准确率。‘@i’ 表示 token 距离为 i。‘Top-k’ 表示前 k 的预测准确率。结果基于 Alpaca
    数据集的 20 轮训练。'
- en: '| EPT | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| EPT | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 100 | 0.506 | 0.794 | 0.276 | 0.602 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 0.506 | 0.794 | 0.276 | 0.602 |'
- en: '| 50 | 0.502 | 0.791 | 0.281 | 0.604 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 0.502 | 0.791 | 0.281 | 0.604 |'
- en: '| 20 | 0.501 | 0.791 | 0.276 | 0.607 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.501 | 0.791 | 0.276 | 0.607 |'
- en: '| 10 | 0.494 | 0.786 | 0.273 | 0.600 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.494 | 0.786 | 0.273 | 0.600 |'
- en: '| 5 | 0.499 | 0.787 | 0.265 | 0.596 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.499 | 0.787 | 0.265 | 0.596 |'
- en: '| 2 | 0.486 | 0.777 | 0.259 | 0.583 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.486 | 0.777 | 0.259 | 0.583 |'
- en: '| 1 | 0.472 | 0.771 | 0.248 | 0.576 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.472 | 0.771 | 0.248 | 0.576 |'
- en: Table [2](#A2.T2 "Table 2 ‣ B.1 Effect of EPTs on Prediction Accuracy ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") presents the prediction accuracy of PPD using
    different EPTs. The results indicate that increasing the number of EPTs generally
    enhances the prediction accuracy of PPD, particularly for long-range token predictions.
    Higher EPT numbers (e.g., 100 and 50) consistently produce better prediction accuracy
    compared to lower EPT numbers.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#A2.T2 "表 2 ‣ B.1 EPT 对预测准确率的影响 ‣ 附录 B 扩展消融研究 ‣ 硬件感知的并行提示解码用于内存高效加速 LLM
    推断") 展示了使用不同 EPT 的 PPD 预测准确率。结果表明，增加 EPT 数量通常能提高 PPD 的预测准确率，特别是对于长距离 token 预测。较高的
    EPT 数量（例如 100 和 50）相比于较低的 EPT 数量，一致地表现出更好的预测准确率。
- en: B.2 Impact of Knowledge Distillation (KD), Epochs, and Batch Size on Prediction
    Accuracy
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.2 知识蒸馏 (KD)、轮次和批量大小对预测准确率的影响
- en: 'Table 3: Prediction Accuracy for PPD with and without knowledge distillation
    (KD) for different EPTs, epochs, and batch sizes.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 对比不同 EPT、轮次和批量大小下的 PPD 预测准确率与知识蒸馏 (KD)。'
- en: '| EPT | KD | Epoch | Batch | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| EPT | KD | 轮次 | 批量 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 100 | Yes | 1 | 4 | 0.504 | 0.793 | 0.273 | 0.598 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 1 | 4 | 0.504 | 0.793 | 0.273 | 0.598 |'
- en: '| 100 | Yes | 2 | 4 | 0.512 | 0.797 | 0.288 | 0.611 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 2 | 4 | 0.512 | 0.797 | 0.288 | 0.611 |'
- en: '| 100 | Yes | 6 | 4 | 0.520 | 0.802 | 0.302 | 0.620 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 6 | 4 | 0.520 | 0.802 | 0.302 | 0.620 |'
- en: '| 100 | Yes | 8 | 4 | 0.524 | 0.804 | 0.307 | 0.619 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 8 | 4 | 0.524 | 0.804 | 0.307 | 0.619 |'
- en: '| 100 | Yes | 10 | 4 | 0.523 | 0.804 | 0.305 | 0.623 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 10 | 4 | 0.523 | 0.804 | 0.305 | 0.623 |'
- en: '| 100 | Yes | 12 | 4 | 0.525 | 0.805 | 0.308 | 0.625 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 12 | 4 | 0.525 | 0.805 | 0.308 | 0.625 |'
- en: '| 100 | No | 12 | 4 | 0.506 | 0.794 | 0.276 | 0.602 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 否 | 12 | 4 | 0.506 | 0.794 | 0.276 | 0.602 |'
- en: '| 100 | Yes | 12 | 1 | 0.530 | 0.809 | 0.309 | 0.626 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 是 | 12 | 1 | 0.530 | 0.809 | 0.309 | 0.626 |'
- en: '| 1 | Yes | 12 | 1 | 0.484 | 0.775 | 0.259 | 0.581 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 12 | 1 | 0.484 | 0.775 | 0.259 | 0.581 |'
- en: '| 1 | Yes | 2 | 4 | 0.474 | 0.773 | 0.247 | 0.574 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 2 | 4 | 0.474 | 0.773 | 0.247 | 0.574 |'
- en: '| 1 | Yes | 6 | 4 | 0.480 | 0.773 | 0.250 | 0.580 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 6 | 4 | 0.480 | 0.773 | 0.250 | 0.580 |'
- en: '| 1 | Yes | 8 | 4 | 0.484 | 0.778 | 0.257 | 0.583 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 8 | 4 | 0.484 | 0.778 | 0.257 | 0.583 |'
- en: '| 1 | Yes | 10 | 4 | 0.482 | 0.777 | 0.257 | 0.584 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 10 | 4 | 0.482 | 0.777 | 0.257 | 0.584 |'
- en: '| 1 | Yes | 12 | 4 | 0.485 | 0.779 | 0.261 | 0.586 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 是 | 12 | 4 | 0.485 | 0.779 | 0.261 | 0.586 |'
- en: '| 1 | No | 12 | 4 | 0.472 | 0.771 | 0.248 | 0.576 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 否 | 12 | 4 | 0.472 | 0.771 | 0.248 | 0.576 |'
- en: Table [3](#A2.T3 "Table 3 ‣ B.2 Impact of Knowledge Distillation (KD), Epochs,
    and Batch Size on Prediction Accuracy ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")
    summarizes our results with different settings. We analyze the effect of each
    factor on the prediction accuracy in the following discussion.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [3](#A2.T3 "表 3 ‣ B.2 知识蒸馏 (KD)、周期和批次大小对预测准确性的影响 ‣ 附录 B 扩展消融研究 ‣ 面向内存高效加速的大型语言模型推理的硬件感知并行提示解码")
    总结了我们在不同设置下的结果。我们将在以下讨论中分析每个因素对预测准确性的影响。
- en: B.2.1 Training Epochs
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.2.1 训练周期
- en: We first investigate the effect of the number of training epochs on prediction
    accuracy. For models using 100 EPTs with KD enabled and a batch size of 4, we
    observe a steady improvement in prediction accuracy as the number of epochs increases.
    Specifically, the Top-1 accuracy at a 1-token distance increases from 0.504 at
    1 epoch to 0.525 at 12 epochs, while the Top-5 accuracy at a 1-token distance
    improves from 0.793 to 0.805\. Similarly, Top-1 accuracy at a 2-token distance
    increases from 0.273 to 0.308, and Top-5 accuracy at a 2-token distance improves
    from 0.598 to 0.625 over the same range of epochs. This trend demonstrates the
    positive impact of prolonged training on the performance of PPD when KD is applied.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调查了训练周期数对预测准确性的影响。对于使用100个EPT并启用KD、批次大小为4的模型，我们观察到随着周期数的增加，预测准确性稳步提高。具体而言，1-token距离下的Top-1准确率从1个周期的0.504增加到12个周期的0.525，而1-token距离下的Top-5准确率从0.793提高到0.805。同样，2-token距离下的Top-1准确率从0.273增加到0.308，2-token距离下的Top-5准确率从0.598提高到0.625。这一趋势表明，延长训练时间对PPD的性能有正面影响，尤其是在应用KD时。
- en: B.2.2 Knowledge Distillation
  id: totrans-197
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.2.2 知识蒸馏
- en: When KD is not applied, as shown for 100 EPTs at 12 epochs with a batch size
    of 4, the performance metrics are generally lower. The improvement in prediction
    accuracy with KD is up to 12%. This suggests that KD contributes significantly
    to prediction accuracy for PPD.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当未应用KD时，如100 EPT在12个周期、批次大小为4的情况下所示，性能指标普遍较低。KD的预测准确率提升高达12%。这表明KD对PPD的预测准确性有显著贡献。
- en: B.2.3 Effect of Batch Size
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.2.3 批次大小的影响
- en: We also examine the impact of batch size on the prediction accuracy. For the
    model trained with 100 EPTs, KD enabled, and 12 epochs, reducing the batch size
    from 4 to 1 results in a slight improvement in prediction accuracy up to 1%.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还考察了批次大小对预测准确性的影响。对于使用100个EPT、启用KD并训练12个周期的模型，将批次大小从4减少到1，预测准确性略微提高了1%。
- en: B.3 Prefix Tuning + Prompt Token
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.3 前缀调优 + 提示令牌
- en: Prefix tuning [[14](#bib.bib14)], similar to prompt tuning, provides a parameter-efficient
    approach to fine-tune a pre-trained model. Unlike prompt tuning, it modifies the
    KV cache of every attention layer by prepending trained vectors. We hypothesize
    that the combination of prefix tuning and prompt tokens can lead to greater learning
    capacity and higher prediction accuracy. This hypothesis is based on the intuition
    that prompt tokens should see a different context than the input tokens when predicting
    long-range tokens. For example, if the input sequence is "Once upon a time", then
    enhancing the input with a prompt template might provide more suitable semantic
    context for long-range prediction. An enhanced input like "Predict the next-next
    token. Once upon a time" might empower the prompt token to predict the correct
    next-next token. Prefix tuning serves as the prompt template to enhance the hidden
    states visible to the prompt tokens.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 前缀调优[[14](#bib.bib14)]，类似于提示调优，提供了一种参数高效的方法来微调预训练模型。不同于提示调优，它通过在每个注意力层前添加训练向量来修改KV缓存。我们假设前缀调优和提示标记的组合可以带来更大的学习能力和更高的预测准确率。这一假设基于这样一种直觉，即提示标记在预测长距离标记时应看到不同于输入标记的上下文。例如，如果输入序列是“从前有一个”，那么使用提示模板增强输入可能会为长距离预测提供更合适的语义上下文。像“预测下下一个标记。从前有一个”这样的增强输入可能会使提示标记能够预测正确的下下一个标记。前缀调优作为提示模板，用于增强提示标记可见的隐藏状态。
- en: '![Refer to caption](img/0a56fc4b6e742fdfda0b368716c150a5.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0a56fc4b6e742fdfda0b368716c150a5.png)'
- en: 'Figure 10: ’P1’ is the prefix token for the prompt token ’S1’ and ’P2’ for
    ’S2’. ’C’ is the input token. The green tick means visibility during attention
    calculation. For instance, ’S1’ can see ’P1’ but cannot see ’P2’. ’C’ does not
    see any prefix tokens so the generated output corresponding to ’C’ is not altered
    by the use of prefix tuning.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：’P1’是提示词’ S1’的前缀标记，’P2’是’S2’的前缀标记。’C’是输入标记。绿色勾号表示在注意力计算过程中可见。例如，’S1’可以看到’P1’，但不能看到’P2’。’C’看不到任何前缀标记，因此对应于’C’的生成输出不会因为使用前缀调优而被改变。
- en: To retain the original model’s distribution, we modify the attention mask so
    that prefix tokens are only visible to prompt tokens. This ensures that we can
    generate outputs that preserve the original model’s distribution. We posit that
    prompt tokens at different positions should see different contexts so we allow
    a prompt token at a specific position to see a distinct set of prefix tokens,
    as shown in Figure [10](#A2.F10 "Figure 10 ‣ B.3 Prefix Tuning + Prompt Token
    ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference").
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持原始模型的分布，我们修改了注意力掩码，使得前缀标记仅对提示标记可见。这确保我们可以生成保持原始模型分布的输出。我们假设在不同位置的提示标记应该看到不同的上下文，因此我们允许特定位置的提示标记看到一组不同的前缀标记，如图[10](#A2.F10
    "图10 ‣ B.3 前缀调优 + 提示标记 ‣ 附录B 扩展消融研究 ‣ 面向内存高效加速的硬件感知并行提示解码")所示。
- en: 'Table 4: Prediction Accuracy of PPD with and without prefix tuning. 1 EPT is
    used for all models and 1 prefix token is used for prefix tuning.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：带有和不带有前缀调优的PPD预测准确率。所有模型均使用1 EPT，前缀调优使用1个前缀标记。
- en: '| Prefix Tuning | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 前缀调优 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| No | 0.485 | 0.779 | 0.261 | 0.586 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 否 | 0.485 | 0.779 | 0.261 | 0.586 |'
- en: '| Yes | 0.412 | 0.738 | 0.204 | 0.541 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 0.412 | 0.738 | 0.204 | 0.541 |'
- en: Table [4](#A2.T4 "Table 4 ‣ B.3 Prefix Tuning + Prompt Token ‣ Appendix B Extended
    Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") compares the prediction accuracy of PPD with and
    without the use of prefix tuning. The results show that the models without prefix
    tuning outperform those with prefix tuning up to 28%, which suggests that, in
    this setup, prefix tuning does not enhance the prediction accuracy of PPD. Instead,
    it appears to degrade performance, potentially due to the complexity introduced
    by modifying the KV cache of attention layers with the prefix token. Unlike prompt
    tokens, prefix tokens do not interact with input tokens, meaning they do not change
    dynamically through the transformer layers based on the input context. This lack
    of interaction and dynamic adjustment could be a factor contributing to the decreased
    prediction accuracy observed with prefix tuning.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#A2.T4 "表 4 ‣ B.3 前缀调优 + Prompt Token ‣ 附录 B 扩展消融研究 ‣ 硬件感知并行 Prompt 解码以实现
    LLM 推理的内存高效加速") 比较了使用和不使用前缀调优的 PPD 预测准确度。结果显示，没有前缀调优的模型在性能上比有前缀调优的模型高出最多 28%，这表明在这种设置下，前缀调优并未提升
    PPD 的预测准确度。相反，它似乎降低了性能，这可能是由于通过前缀 token 修改注意力层的 KV 缓存引入的复杂性所致。与 prompt tokens
    不同，前缀 tokens 不与输入 tokens 交互，这意味着它们不会根据输入上下文在 transformer 层中动态变化。这种缺乏交互和动态调整可能是导致前缀调优时预测准确度降低的一个因素。
- en: B.4 Custom Decoding Heads + Prompt Token
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.4 自定义解码头 + Prompt Token
- en: It has been demonstrated that a fine-tuned decoding head alone can effectively
    predict long-range tokens [[23](#bib.bib23), [1](#bib.bib1)]. Thus, we hypothesize
    that combining a separately fine-tuned decoding head with prompt tokens might
    further enhance the potential of PPD. As shown in Figure [11](#A2.F11 "Figure
    11 ‣ B.4 Custom Decoding Heads + Prompt Token ‣ Appendix B Extended Ablation Study
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference"), we trained a separate decoding head to transform only the hidden
    states of prompt tokens into logits. A key distinction from Medusa is that this
    decoding head is responsible for generating tokens at multiple positions, rather
    than just one.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 已证明，仅通过微调解码头就能有效预测长距离的 tokens [[23](#bib.bib23), [1](#bib.bib1)]。因此，我们假设将单独微调的解码头与
    prompt tokens 结合，可能会进一步增强 PPD 的潜力。如图 [11](#A2.F11 "图 11 ‣ B.4 自定义解码头 + Prompt
    Token ‣ 附录 B 扩展消融研究 ‣ 硬件感知并行 Prompt 解码以实现 LLM 推理的内存高效加速") 所示，我们训练了一个单独的解码头，将 prompt
    tokens 的隐藏状态转换为 logits。与 Medusa 的关键区别在于，这个解码头负责在多个位置生成 tokens，而不仅仅是一个位置。
- en: '![Refer to caption](img/9678fc8ddba45f8071566016b9982ee2.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9678fc8ddba45f8071566016b9982ee2.png)'
- en: 'Figure 11: Custom decoding head with PPD. The feature extractor refers to the
    LLMs without the decoding heads. ’H1’ is the generated hidden state for the input
    token ’C’. ’H2’ is the hidden state for the prompt token ’S1’ and ’H3’ for ’S2’.
    ’LM1’ is the original LLM’s decoding head and it takes in the hidden states of
    input tokens. ’LM2’ is the custom decoding heads for PPD and only takes in the
    hidden states of prompt tokens.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：PPD 的自定义解码头。特征提取器指的是没有解码头的 LLMs。’H1’ 是输入 token ’C’ 的生成隐藏状态。’H2’ 是 prompt
    token ’S1’ 的隐藏状态，’H3’ 是 ’S2’ 的隐藏状态。’LM1’ 是原始 LLM 的解码头，它接收输入 tokens 的隐藏状态。’LM2’
    是 PPD 的自定义解码头，仅接收 prompt tokens 的隐藏状态。
- en: We propose two training methods. In the first method, the custom decoding head
    and prompt tokens are trained together from scratch in a single stage. In the
    second method, the prompt tokens are initially trained for 2 epochs, followed
    by training both the prompt tokens and the decoding head with a smaller learning
    rate in a two-stage process.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了两种训练方法。第一种方法是在单阶段中从头开始同时训练自定义解码头和 prompt tokens。第二种方法是在前 2 个 epoch 中先训练
    prompt tokens，然后在两阶段过程中使用较小的学习率训练 prompt tokens 和解码头。
- en: 'Table 5: Prediction Accuracy of PPD with and without custom decoding head.
    1 EPT is used for all models. 1-stage and 2-stage refer to the training strategies
    of custom decoding head.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：PPD 使用和不使用自定义解码头的预测准确度。所有模型均使用 1 EPT。1-stage 和 2-stage 指的是自定义解码头的训练策略。
- en: '| Method Name | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PPD without custom decoding head | 0.485 | 0.779 | 0.261 | 0.586 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 不使用自定义解码头的 PPD | 0.485 | 0.779 | 0.261 | 0.586 |'
- en: '| PPD with custom decoding head (1-stage) | 0.385 | 0.614 | 0.229 | 0.482 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 使用自定义解码头的 PPD（1-stage） | 0.385 | 0.614 | 0.229 | 0.482 |'
- en: '| PPD with custom decoding head (2-stage) | 0.506 | 0.795 | 0.276 | 0.602 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 具有自定义解码头的PPD（2阶段） | 0.506 | 0.795 | 0.276 | 0.602 |'
- en: Table [5](#A2.T5 "Table 5 ‣ B.4 Custom Decoding Heads + Prompt Token ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") presents the prediction accuracy of PPD with and
    without a custom decoding head. When trained using the single-stage method, PPD
    with the custom decoding head shows a 12%-21% decrease in prediction accuracy
    compared to the baseline PPD without the custom decoding head. This suggests that
    the single-stage approach does not result in stable or effective training.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [5](#A2.T5 "Table 5 ‣ B.4 Custom Decoding Heads + Prompt Token ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") 展示了使用和不使用自定义解码头的PPD预测准确性。当使用单阶段方法训练时，具有自定义解码头的PPD相比于没有自定义解码头的基线PPD，预测准确性下降了12%-21%。这表明单阶段方法未能实现稳定或有效的训练。
- en: In contrast, the two-stage training method results in a limited improvement
    of 2.1%-4.3% in prediction accuracy compared to the baseline. This suggests that
    adding a custom decoding head may not be necessary, given the additional trainable
    parameters and the limited improvement in prediction accuracy.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，二阶段训练方法相比于基线，预测准确性仅有2.1%-4.3%的有限提升。这表明，考虑到额外的可训练参数和预测准确性的有限提升，添加自定义解码头可能不是必要的。
- en: B.5 Attention Masking for EPTs
  id: totrans-225
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.5 EPTs的注意力遮罩
- en: In this paper, we proposed a specialized attention mask for EPTs to achieve
    the effect of prompt ensemble. However, there are alternative masking strategies
    available. Here, we describe and compare three types of attention masks that we
    implemented and experimented with.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种专门的注意力遮罩用于EPTs，以实现提示集成的效果。然而，还有其他可用的遮罩策略。这里，我们描述并比较了我们实现和实验的三种类型的注意力遮罩。
- en: '![Refer to caption](img/93523f27ca451486a148386453552cee.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/93523f27ca451486a148386453552cee.png)'
- en: (a) Ensemble Attention Mask
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 集成注意力遮罩
- en: '![Refer to caption](img/25af6ca50580371f4c35376fa7d93be0.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25af6ca50580371f4c35376fa7d93be0.png)'
- en: (b) Decoder-like Attention Mask
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 类似解码器的注意力遮罩
- en: '![Refer to caption](img/367dba5a22a6a6901e63c805b6948cf4.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/367dba5a22a6a6901e63c805b6948cf4.png)'
- en: (c) Encoder-like Attention Mask
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 类似编码器的注意力遮罩
- en: 'Figure 12: Different Mask Strategies for EPTs. ’C’ is an input token. ’V1’
    and ’V2’ are the EPTs for prompt tokens ’S1’ and ’V3’ and ’V4’ for ’S2’.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：不同的EPT遮罩策略。’C’ 是输入令牌。’V1’ 和 ’V2’ 是提示令牌 ’S1’ 的 EPTs，’V3’ 和 ’V4’ 是 ’S2’ 的
    EPTs。
- en: B.5.1 Ensemble Attention Masking
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.5.1 集成注意力遮罩
- en: The ensemble attention masking is the masking strategy we previously described.
    In this approach, EPTs are divided into $n$. Since this masking strategy effectively
    averages the results of disjoint groups of EPTs, we refer to it as the "ensemble
    attention masking". Figure [12(a)](#A2.F12.sf1 "In Figure 12 ‣ B.5 Attention Masking
    for EPTs ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference") provides an example
    of the ensemble attention masking.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 集成注意力遮罩是我们之前描述的遮罩策略。在这种方法中，EPTs被分为$n$组。由于这种遮罩策略有效地对不相交的EPT组的结果进行平均，因此我们称之为“集成注意力遮罩”。图
    [12(a)](#A2.F12.sf1 "In Figure 12 ‣ B.5 Attention Masking for EPTs ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") 提供了集成注意力遮罩的一个示例。
- en: B.5.2 Decoder-like Attention Masking
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.5.2 类似解码器的注意力遮罩
- en: Decoder-like attention masking is a simple strategy where EPTs can only attend
    to EPTs with smaller position indices. This results in a triangular-shaped attention
    mask, similar to the one used in decoder layers, hence the name "decoder-like
    attention masking". Figure [12(b)](#A2.F12.sf2 "In Figure 12 ‣ B.5 Attention Masking
    for EPTs ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference") provides an example
    of this masking strategy.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 类似解码器的注意力遮罩是一种简单的策略，其中EPTs只能关注位置索引较小的EPTs。这导致了一个三角形的注意力遮罩，类似于解码器层中使用的遮罩，因此得名“类似解码器的注意力遮罩”。图
    [12(b)](#A2.F12.sf2 "In Figure 12 ‣ B.5 Attention Masking for EPTs ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") 提供了这种遮罩策略的一个示例。
- en: B.5.3 Encoder-like Attention Masking
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.5.3 类似编码器的注意力遮罩
- en: In encoder-like attention masking, an EPT corresponding to a prompt token $P$.
    This allows EPTs to see both preceding and succeeding EPTs, similar to the token
    visibility in an encoder layer, hence the name "encoder-like attention masking".
    Figure [12(c)](#A2.F12.sf3 "In Figure 12 ‣ B.5 Attention Masking for EPTs ‣ Appendix
    B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient
    Acceleration of LLM Inference") illustrates this masking strategy.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器类似的注意力掩码中，一个 EPT 对应于提示令牌 $P$。这允许 EPTs 同时看到前面和后面的 EPTs，类似于编码器层中的令牌可见性，因此称之为“编码器类似的注意力掩码”。图
    [12(c)](#A2.F12.sf3 "在图 12 ‣ B.5 EPTs 的注意力掩码 ‣ 附录 B 扩展消融研究 ‣ 硬件感知并行提示解码用于内存高效加速
    LLM 推理") 说明了这种掩码策略。
- en: B.5.4 Results
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: B.5.4 结果
- en: 'Table 6: Prediction Accuracy of PPD with different attention masking strategies
    for EPTs. 100 EPT is used for all models.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：不同注意力掩码策略下 PPD 的预测准确性。所有模型均使用 100 个 EPT。
- en: '| Method Name | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PPD with ensemble attention mask | 0.506 | 0.794 | 0.276 | 0.602 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 使用集成注意力掩码的 PPD | 0.506 | 0.794 | 0.276 | 0.602 |'
- en: '| PPD with decoder attention mask | 0.465 | 0.755 | 0.262 | 0.572 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 使用解码器注意力掩码的 PPD | 0.465 | 0.755 | 0.262 | 0.572 |'
- en: '| PPD with encoder attention mask | 0.473 | 0.765 | 0.256 | 0.573 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 使用编码器注意力掩码的 PPD | 0.473 | 0.765 | 0.256 | 0.573 |'
- en: The results in Table [6](#A2.T6 "Table 6 ‣ B.5.4 Results ‣ B.5 Attention Masking
    for EPTs ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference") indicate that the
    ensemble attention mask outperforms the other masking strategies. In comparison,
    the PPD with decoder attention mask shows 4.9%-8.0% lower prediction accuracy.
    The PPD with encoder attention mask also underperforms in prediction accuracy
    relative to the ensemble attention mask by 3.7%-7.2%.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#A2.T6 "表 6 ‣ B.5.4 结果 ‣ B.5 EPTs 的注意力掩码 ‣ 附录 B 扩展消融研究 ‣ 硬件感知并行提示解码用于内存高效加速
    LLM 推理") 中的结果表明，集成注意力掩码优于其他掩码策略。相比之下，使用解码器注意力掩码的 PPD 显示出 4.9%-8.0% 的预测准确率下降。使用编码器注意力掩码的
    PPD 相对于集成注意力掩码也在预测准确性上低 3.7%-7.2%。
- en: These results suggest that the ensemble attention mask is the most effective
    strategy among the three, likely due to its ability to effectively average the
    votes of disjoint groups of EPTs, thereby improving prediction accuracy. The decoder-like
    and encoder-like attention masks, while simpler, do not provide the same level
    of performance, indicating that the structure and specificity of the ensemble
    attention mask better facilitate accurate long-range token prediction. Additionally,
    ensemble attention masking is more sparse, which offers greater potential for
    optimization.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，集成注意力掩码在三种策略中最为有效，这可能是由于它能够有效地对不重叠的 EPT 组的投票进行平均，从而提高预测准确性。虽然解码器类似的和编码器类似的注意力掩码更简单，但它们并未提供相同水平的性能，这表明集成注意力掩码的结构和特性更有助于准确的远程令牌预测。此外，集成注意力掩码更加稀疏，这提供了更大的优化潜力。
- en: B.6 Aggregation Method for EPTs
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.6 EPTs 的聚合方法
- en: 'In addition to simply averaging the logits from EPTs, we explored more advanced
    aggregation methods. For instance, we applied learned weights to aggregate the
    logits. The final logit $p$ can be expressed as:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 除了简单地对 EPTs 的 logits 进行平均外，我们还探索了更高级的聚合方法。例如，我们应用了学习到的权重来聚合 logits。最终的 logit
    $p$ 可以表示为：
- en: '|  | $p=\sum_{i=1}^{n}w_{i}\cdot p_{i},$ |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $p=\sum_{i=1}^{n}w_{i}\cdot p_{i},$ |  |'
- en: where $n$ EPT.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n$ 为 EPT。
- en: 'Table 7: Prediction Accuracy of PPD with different aggregation methods for
    EPTs. 100 EPT is used for all models.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同聚合方法下 PPD 的预测准确性。所有模型均使用 100 个 EPT。
- en: '| Aggregation Method | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 聚合方法 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Average | 0.506 | 0.794 | 0.276 | 0.602 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 0.506 | 0.794 | 0.276 | 0.602 |'
- en: '| Learned Weight | 0.503 | 0.779 | 0.250 | 0.576 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 学习权重 | 0.503 | 0.779 | 0.250 | 0.576 |'
- en: 'The results in Table [7](#A2.T7 "Table 7 ‣ B.6 Aggregation Method for EPTs
    ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference") show the prediction accuracy
    of PPD with two different aggregation methods for EPTs: simple averaging and learned
    weights. When using learned weights to aggregate logits, the model shows a slight
    decrease of 0.6%-9.4% in prediction accuracy.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [7](#A2.T7 "表 7 ‣ B.6 EPT 的聚合方法 ‣ 附录 B 扩展消融研究 ‣ 面向内存高效加速 LLM 推理的硬件感知并行提示解码")
    中的结果显示了 PPD 在两种不同聚合方法下的预测准确性：简单平均和学习的权重。当使用学习的权重来聚合 logits 时，模型的预测准确性略微下降了 0.6%-9.4%。
- en: These results suggest that while learned weights provide a more flexible aggregation
    method, they do not necessarily lead to improved prediction accuracy in this context.
    The simplicity and stability of the averaging method appear to offer better performance,
    possibly due to the additional complexity and potential overfitting introduced
    by learning the weights.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，虽然学习的权重提供了更灵活的聚合方法，但在这种情况下并不一定能提高预测准确性。简单和稳定的平均方法似乎提供了更好的性能，这可能是由于学习权重引入的额外复杂性和潜在的过拟合。
- en: B.7 Multi-exit Ensemble
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: B.7 多出口集成
- en: While using EPTs for prompt ensemble improves prediction accuracy, it also increases
    input length, resulting in higher computational overhead and forward pass latency.
    To address this, we propose the use of a multi-exit ensemble method. In multi-exit
    ensemble, the hidden states of a prompt token from the last $k$ decoder layers
    are extracted and averaged to produce the final hidden state, which is then decoded
    by the decoding head into a guess token, as illustrated in Figure [13](#A2.F13
    "Figure 13 ‣ B.7 Multi-exit Ensemble ‣ Appendix B Extended Ablation Study ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference").
    This approach achieves prompt ensemble without the associated computational costs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用 EPT 进行提示集成提高了预测准确性，但也增加了输入长度，从而导致更高的计算开销和前向传播延迟。为了解决这个问题，我们提出了多出口集成方法。在多出口集成中，从最后
    $k$ 个解码器层提取和平均提示符号的隐藏状态，生成最终的隐藏状态，然后由解码头解码成一个猜测符号，如图 [13](#A2.F13 "图 13 ‣ B.7
    多出口集成 ‣ 附录 B 扩展消融研究 ‣ 面向内存高效加速 LLM 推理的硬件感知并行提示解码") 所示。这种方法实现了提示集成，而不会增加相关的计算成本。
- en: '![Refer to caption](img/d5a14b7eaa1e26a1cacb1d6c442402bf.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d5a14b7eaa1e26a1cacb1d6c442402bf.png)'
- en: 'Figure 13: Mult-exit ensemble. ’D1’, ’D10’, ’D11’, and ’D12’ are the decoder
    layers in order. ’S1’ is a prompt token and ’H1’, ’H2’, ’H3’ are the corresponding
    hidden states from the last 3 decoder layers. ’H4’ is obtained from averaging
    these 3 hidden states. The decoding head ’LM’ translates ’H4’ into a token ’E’.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：多出口集成。’D1’，’D10’，’D11’，和 ’D12’ 是按顺序排列的解码器层。’S1’ 是提示符号，’H1’，’H2’，’H3’ 是来自最后
    3 个解码器层的相应隐藏状态。’H4’ 是通过平均这 3 个隐藏状态得到的。解码头 ’LM’ 将 ’H4’ 转换为一个符号 ’E’。
- en: The hypothesis is that taking the hidden states from the last few decoder layers
    for ensemble might work because these layers capture increasingly abstract and
    high-level representations of the input sequence. By averaging the hidden states
    from multiple layers, we can combine diverse but complementary information, leading
    to a more robust and accurate final hidden state. Additionally, since the final
    layers are closest to the output, they are more likely to contain refined and
    contextually relevant information, making the ensemble more effective.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 假设是，取最后几个解码器层的隐藏状态进行集成可能有效，因为这些层捕捉了输入序列中越来越抽象和高级的表示。通过平均来自多个层的隐藏状态，我们可以结合多样但互补的信息，从而得到更强健和准确的最终隐藏状态。此外，由于最终层最接近输出，它们更可能包含精炼和上下文相关的信息，使得集成更加有效。
- en: 'Table 8: Prediction Accuracy of PPD with and without multi-exit ensemble. 1
    EPT is used for all models. $k$ exits refer to the number of exits used.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：PPD 在有无多出口集成下的预测准确度。所有模型均使用 1 EPT。$k$ 出口指的是使用的出口数量。
- en: '| Method Name | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 方法名称 | @1 Top-1 | @1 Top-5 | @2 Top-1 | @2 Top-5 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| PPD without multi-exit | 0.485 | 0.779 | 0.261 | 0.586 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| PPD 无多出口 | 0.485 | 0.779 | 0.261 | 0.586 |'
- en: '| PPD with 3 exits | 0.422 | 0.723 | 0.214 | 0.517 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| PPD 使用 3 个出口 | 0.422 | 0.723 | 0.214 | 0.517 |'
- en: '| PPD with 2 exits | 0.420 | 0.723 | 0.213 | 0.518 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| PPD 使用 2 个出口 | 0.420 | 0.723 | 0.213 | 0.518 |'
- en: Table [8](#A2.T8 "Table 8 ‣ B.7 Multi-exit Ensemble ‣ Appendix B Extended Ablation
    Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration
    of LLM Inference") shows the comparison of prediction accuracy of PPD with and
    without mult-exit ensemble. The results indicate that the introduction of multi-exit
    ensemble with both 2 and 3 exits results in a 7%-18% decrease in prediction accuracy
    compared to the baseline model without multi-exit.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [8](#A2.T8 "Table 8 ‣ B.7 Multi-exit Ensemble ‣ Appendix B Extended Ablation
    Study ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration
    of LLM Inference") 显示了 PPD 在有无多出口集成下预测准确率的比较。结果表明，采用 2 个和 3 个出口的多出口集成相对于基线模型（无多出口）导致预测准确率下降了
    7%-18%。
- en: These findings suggest that the multi-exit ensemble approach, as implemented,
    does not enhance prediction accuracy and instead leads to a notable decrease in
    performance. This may be due to the averaging of hidden states from multiple layers
    introducing noise or reducing the specificity of the representations needed for
    accurate prediction. Further refinement of the multi-exit ensemble may be necessary
    to achieve the desired improvements in accuracy.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这些发现表明，所实现的多出口集成方法并未提高预测准确率，反而导致了性能显著下降。这可能是因为多个层的隐藏状态的平均引入了噪声或降低了准确预测所需表示的特异性。可能需要进一步优化多出口集成，以实现所期望的准确性提升。
- en: Appendix C Experiment Details
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 C 实验细节
- en: For the throughput experiments, each result is obtained by averaging three separate
    runs. The standard deviations of these runs are reported as error bars in the
    bar charts. To ensure a fair comparison in our comparative experiments, we maintained
    consistent hardware settings and software versions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于吞吐量实验，每个结果是通过平均三次独立运行获得的。这些运行的标准差在条形图中以误差条的形式报告。为了确保我们比较实验的公平性，我们保持了硬件设置和软件版本的一致性。
- en: We selected 3 prompt tokens because adding more would not further increase the
    expected acceptance length due to the tree size limit. The number of EPTs per
    prompt token was optimized to maximize throughput.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了 3 个提示令牌，因为添加更多的提示令牌不会进一步增加预期的接受长度，这是由于树的大小限制。每个提示令牌的 EPT 数量经过优化，以最大化吞吐量。
- en: In Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Hardware-Aware Parallel Prompt
    Decoding for Memory-Efficient Acceleration of LLM Inference"), the temperature
    settings for PPD, Eagle [[16](#bib.bib16)], and Medusa [[1](#bib.bib1)] follow
    the default configuration, while the other models use a greedy setting (temperature=0).
    This choice is based on findings that retrieval-based methods perform significantly
    worse in non-greedy settings. Similarly, LOOKAHEAD DECODING [[8](#bib.bib8)],
    REST [[9](#bib.bib9)], and PLD [[21](#bib.bib21)] in Fig. [4](#S5.F4 "Figure 4
    ‣ 5.1 Speedup Comparison with Parallel Decoding Methods ‣ 5 Experiments ‣ Hardware-Aware
    Parallel Prompt Decoding for Memory-Efficient Acceleration of LLM Inference")
    also use a temperature setting of 0 for the same reasons.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Hardware-Aware Parallel Prompt Decoding
    for Memory-Efficient Acceleration of LLM Inference") 中，PPD、Eagle [[16](#bib.bib16)]
    和 Medusa [[1](#bib.bib1)] 的温度设置遵循默认配置，而其他模型使用贪婪设置（温度=0）。这一选择是基于发现检索方法在非贪婪设置下表现显著较差。类似地，图 [4](#S5.F4
    "Figure 4 ‣ 5.1 Speedup Comparison with Parallel Decoding Methods ‣ 5 Experiments
    ‣ Hardware-Aware Parallel Prompt Decoding for Memory-Efficient Acceleration of
    LLM Inference") 中的 LOOKAHEAD DECODING [[8](#bib.bib8)]、REST [[9](#bib.bib9)] 和
    PLD [[21](#bib.bib21)] 也因相同原因使用了 0 的温度设置。
- en: Appendix D Limitations
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录 D 局限性
- en: 'Despite its efficiency, we have identified the following limitations of PPD:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其效率较高，我们已经确定了 PPD 的以下局限性：
- en: '1.'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Low prediction accuracy for very small models. We found that for very small
    models like Vicuna-68M [[24](#bib.bib24)], which only has 2 decoder layers and
    an embedding dimension of less than 1000, PPD suffers from low prediction accuracy.
    This is because the embedding dimension determines the expressive power of a prompt
    token, and the transformer architecture’s depth is crucial for efficient information
    flow to the prompt tokens.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于非常小的模型预测准确率较低。我们发现，对于像 Vicuna-68M [[24](#bib.bib24)] 这样只有 2 个解码层且嵌入维度小于 1000
    的非常小的模型，PPD 的预测准确率较低。这是因为嵌入维度决定了提示令牌的表达能力，而变换器架构的深度对信息高效流向提示令牌至关重要。
- en: '2.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: GPU compute resource constraint. Since PPD trades additional compute resources
    for increased throughput, its effectiveness depends on the availability of idle
    GPU compute resources. On a GPU with limited compute resources, the speedup ratios
    achieved by PPD are expected to decrease.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPU计算资源限制。由于PPD通过增加吞吐量来交换额外的计算资源，其效果取决于空闲GPU计算资源的可用性。在计算资源有限的GPU上，PPD实现的加速比预期会下降。
- en: '3.'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Extended input length. The improvement in acceptance length with PPD is not
    as significant as the gain in prediction accuracy compared to Medusa. This is
    because PPD must reserve a substantial portion of the input for prompt tokens,
    which limits the size of the sparse tree that can be used.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 扩展输入长度。与Medusa相比，PPD在接受长度方面的改进不如预测准确度的提高显著。这是因为PPD必须为提示令牌保留大量输入，这限制了可以使用的稀疏树的大小。
- en: Appendix E Societal Impact
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附录E 社会影响
- en: In this paper, we proposed PPD to accelerate LLMs easily and cheaply. Since
    PPD reduces the time required for handling a single inference request, it could
    bring down the cost of deploying LLMs for both the companies and the public. This
    might lead to increased accessibility of LLM services. Moreover, latency-sensitive
    applications like chatbots will benefit greatly from the usage of PPD as it reduces
    the inference latency greatly, thereby enhancing the user experience.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了PPD以便轻松且廉价地加速LLM。由于PPD减少了处理单个推理请求所需的时间，因此可能降低公司和公众部署LLM的成本。这可能会增加LLM服务的可访问性。此外，像聊天机器人这样的延迟敏感应用将从使用PPD中获益，因为它大大减少了推理延迟，从而提升了用户体验。
- en: While PPD aims to make AI more accessible, there may still be a digital divide
    where certain communities lack the necessary infrastructure, such as stable internet
    connections or modern hardware, to fully benefit from these advancements. This
    could further widen the gap between technology-privileged and underserved populations.
    On the other hand, PPD might be misused by malicious parties to manipulate the
    output of the original LLM, resulting in the generation of unreliable information
    and fake data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管PPD旨在使人工智能更易于访问，但仍可能存在数字鸿沟，某些社区缺乏必要的基础设施，例如稳定的互联网连接或现代硬件，无法充分受益于这些进步。这可能会进一步扩大技术特权与服务不足群体之间的差距。另一方面，PPD可能被恶意方滥用，操控原始LLM的输出，从而生成不可靠的信息和虚假数据。
