- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:51'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation
    through LLM Affordance-Grounding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.11000](https://ar5iv.labs.arxiv.org/html/2404.11000)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Edmond Tong¹, Anthony Opipari¹, Stanley Lewis¹, Zhen Zeng², and Odest Chadwicke
    Jenkins¹ ¹E. Tong, A. Opipari, S. Lewis, and O.C. Jenkins are with the Department
    of Robotics, University of Michigan, Ann Arbor, MI, USA, 48109.²Z. Zeng is with
    J.P. Morgan AI Research.^∗E. Tong is the corresponding author: ekjt@umich.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In order for robots to interact with objects effectively, they must understand
    the form and function of each object they encounter. Essentially, robots need
    to understand which actions each object affords, and where those affordances can
    be acted on. Robots are ultimately expected to operate in unstructured human environments,
    where the set of objects and affordances is not known to the robot before deployment
    (i.e. the open-vocabulary setting). In this work, we introduce OVAL-Prompt, a
    prompt-based approach for open-vocabulary affordance localization in RGB-D images.
    By leveraging a Vision Language Model (VLM) for open-vocabulary object part segmentation
    and a Large Language Model (LLM) to ground each part-segment-affordance, OVAL-Prompt
    demonstrates generalizability to novel object instances, categories, and affordances
    without domain-specific finetuning. Quantitative experiments demonstrate that
    without any finetuning, OVAL-Prompt achieves localization accuracy that is competitive
    with supervised baseline models. Moreover, qualitative experiments show that OVAL-Prompt
    enables affordance-based robot manipulation of open-vocabulary object instances
    and categories.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For robots to function effectively in unstructured settings like homes and offices,
    they must be adept at identifying objects in their surroundings and utilizing
    them appropriately. The potential uses for each object can be understood in terms
    of the object’s “affordances.” Originally termed by psychologist James J. Gibson [[2](#bib.bib2)],
    an object’s affordance describes what actions are possible for a robot to perform
    using the given object – thereby enabling the robot to reason about effective
    object use. For example, knives afford cutting solids and spoons afford scooping
    liquids. Once the robot has detected which object affordances are necessary for
    task completion, it must then be able to localize the relevant object geometry
    that enables said affordances. For this work, we refer to the combined detection
    and localization task as ‘affordance grounding.’ Traditional approaches to affordance
    grounding have relied heavily on extensive labeled datasets and task-specific
    training, limiting their flexibility and scalability for robotic applications.
    Roboticists have increasingly sought to address this limitation by developing
    approaches for open-vocabulary settings, where the set of objects [[3](#bib.bib3)]
    and affordances [[4](#bib.bib4)] is not constrained to the same fixed set that
    was used during training. In particular, Large Language Models (LLMs) and Vision
    Language Models (VLMs) have been proposed as sources of knowledge to address the
    open-vocabulary setting [[5](#bib.bib5), [4](#bib.bib4), [6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: There is a growing interest in using LLMs and VLMs for robotic applications
     [[7](#bib.bib7), [5](#bib.bib5), [8](#bib.bib8), [9](#bib.bib9)], with notable
    examples including CLIP [[10](#bib.bib10), [11](#bib.bib11)] and Say-Can [[5](#bib.bib5)].
    Specifically, LLMs and VLMs have been proposed as knowledge bases that can be
    tapped into for solving classical robotics challenges. For example, within planning,
    Valmeekam et al. and Kambhampati et al. proposed using LLMs as a source of knowledge
    within ‘LLM-Modulo’ planning frameworks but find that LLMs struggle to plan in
    isolation [[12](#bib.bib12), [9](#bib.bib9)]. For affordance grounding and localization
    specifically, Qian et al. propose finetuning a LLM to generate affordance mask-tokens,
    which can be decoded into localized affordance segments [[6](#bib.bib6)]. Nguyen
    et al. propose finetuning a text-encoder to be used for open-vocabulary feature
    correlations within an affordance localization pipeline [[4](#bib.bib4)]. In contrast,
    we set out to understand the potential for pre-trained LLMs and VLMs to be used
    without domain-specific finetuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our work is motivated by two simple questions: Can pre-trained LLMs and VLMs
    be used to ground object-part affordances without domain-specific finetuning and
    do they enable robots to manipulate open-vocabulary affordances? To answer these
    questions, we develop an LLM and VLM-based affordance-grounding pipeline to detect
    and localize part-based object affordances in images. By using pre-trained foundation
    models, the developed pipeline is applicable to the open-vocabulary setting where
    object instances, categories, and affordances are evaluated without having been
    trained for in a supervised fashion. The proposed pipeline is quantitatively evaluated
    through multiple experiments to measure its accuracy relative to state-of-the-art
    supervised methods. Furthermore, real robot experiments are performed using the
    affordance-grounding pipeline to establish whether the pipeline is applicable
    to real-world robotic scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, this paper sets out to advance affordance-based robotic manipulation
    with the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, the OVAL-Prompt algorithm is introduced to perform open-vocabulary affordance
    localization. OVAL-Prompt uses a LLM to ground part-segments generated by a VLM
    with corresponding action-affordances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Second, we explore the importance of prompt structure in affordance-prompting
    to ground visual-affordances with natural language tokens. Results from these
    experiments suggest using a LLM to translate affordance to object part and prompt
    it for similar names.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Third, we demonstrate that OVAL-Prompt, which uses no domain finetuning, is
    competitive with multiple finetuned state-of-the-art baseline models on a real-world
    affordance localization dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fourth, robot experiments are performed showing OVAL-Prompt can be successfully
    used on real robot platforms for successful affordance-based object and tool grasping.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Closed-Vocabulary Affordance Localization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Early approaches to affordance grounding relied on hand-crafted features such
    as geometric shapes [[13](#bib.bib13)]. However, recent research has shifted towards
    deep learning techniques [[14](#bib.bib14)]. Traditional deep learning approaches
    use a convolutional backbone (e.g. VGG16 [[15](#bib.bib15)]) for feature extraction
    and use the resulting latent features to generate a final prediction [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)].
    These approaches rely on supervised training with a fixed set of object affordances
    that do not change during inference, thereby limiting their applicability to robotic
    applications. In contrast, the present paper sets out to address affordance localization
    in the open-vocabulary setting where the object categories and affordances are
    not fixed after training.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Open-Vocabulary Affordance Localization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ‘Open-vocabulary’ refers to machine learning tasks in which models are trained
    using a fixed set of category labels and evaluated using a non-fixed set of category
    labels [[22](#bib.bib22)]. In the context of affordance localization, the open-vocabulary
    setting has received increasing interest [[3](#bib.bib3), [23](#bib.bib23), [4](#bib.bib4),
    [24](#bib.bib24)]. Luo et al. introduce a large-scale image dataset for open-vocabulary
    affordance localization and develop a feature co-relation strategy to achieve
    open-vocabulary inference [[3](#bib.bib3)]. Li et al. propose a vision transformer
    and exocentric human demonstrations to identify affordances in egocentric images
    with weakly-supervised training [[23](#bib.bib23)]. Similarly, Li et al. propose
    a vision transformer trained with a one-shot approach in which only one labeled
    example per object category is used for training [[24](#bib.bib24)]. Nguyen et
    al. focus on affordance localization within 3D point clouds with a contributed
    dataset and use feature correlations along with a pre-trained text-encoder for
    the open-vocabulary setting [[4](#bib.bib4)]. In contrast, the present paper sets
    out to avoid expensive finetuning strategies and instead to explore the potential
    for pre-trained foundation models to be used for open-vocabulary affordance localization.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Foundation Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several Large Language Models (LLMs), such as GPT-4[[25](#bib.bib25)] and Gemini[[26](#bib.bib26)],
    have demonstrated exceptional capabilities in tasks like test-taking, gaining
    significant traction in both research and practical applications. These advancements
    in LLMs, have paved the way for their widespread adoption across various domains.
    In the field of robotics, LLMs are increasingly being utilized for a broad spectrum
    of functions. These include enhancing human-robot interaction [[5](#bib.bib5)],
    planning[[27](#bib.bib27)], and navigation[[28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: 'This work aims to answer the questions: Do LLMs understand affordances? While
    LLMs are known for their remarkable recall and comprehension abilities, they have
    also been criticized for concocting infeasible plans[[29](#bib.bib29)], and demonstrating
    flawed reasoning[[30](#bib.bib30)].'
  prefs: []
  type: TYPE_NORMAL
- en: III Affordance Localization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: III-A Problem Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We tackle zero-shot open-vocabulary affordance localization, by identifying
    actionable object parts from images and descriptions without prior specific training.
    We focus on the task of open-vocabulary affordance localization in images. Given
    a RGB image, $I\in\mathcal{R}^{H\times W\times 3}$, present in the image. Crucially,
    for the open-vocabulary setting, the set of test labels can differ from the one
    used during model training.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Evaluation Metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To evaluate zero-shot affordance localization, we use the weighted F1-score
    defined by [1](#S3.E1 "In III-B Evaluation Metric ‣ III Affordance Localization
    ‣ OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation
    through LLM Affordance-Grounding") that has been used in previous affordance localization
    papers [[13](#bib.bib13)].'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F_{\beta}^{w}=\frac{(1+\beta^{2})\cdot P_{r}^{w}\cdot R_{c}^{w}}{\beta^{2}\cdot
    P_{r}^{w}+R_{c}^{w}},\text{ with }\beta=1$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Where $P_{r}^{w}$measures. Where TP = true positive, FP = false positive, FN
    = false negative. Precision indicates how many of the items identified as relevant
    are actually relevant, while recall measures how many relevant items were identified
    out of all relevant items. For more information see [[13](#bib.bib13)]. In zero-shot
    evaluation, an affordance that is not predicted but exist in the ground truth
    sample is assigned an F-score of 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'IV OVAL-Prompt: Affordance-Prompting'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our method processes a RGB image and task description, using a VLM to detect
    and list objects within the image. This list informs a natural language prompt
    for a LLM, which specifies the object part related to the task. The VLM then segments
    this part, creating a mask for manipulation tasks. If segmentation fails, the
    LLM is reprompted for alternative part names, and the VLM tries again.
  prefs: []
  type: TYPE_NORMAL
- en: This approach is highly adaptable, using off-the-shelf components. The LLM and
    VLM are interchangeable, allowing for easy updates in the future, promising significant
    enhancements as LLMs and VLMs progress.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A VLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We employ the Vision-Language Model (VLM) for object detection and part segmentation.
    Initially, we supply the VLM with an image and a list of potential objects to
    identify, discarding detections below 50% confidence. This filtered list is then
    provided to the LLM where it is processed and returns an object part. Upon receiving
    an object part from the LLM, the VLM segments it and generates a binary mask.
  prefs: []
  type: TYPE_NORMAL
- en: We chose VLpart[[31](#bib.bib31)] for its part segmentation capability, specifically
    the “swinbase_cascade_lvis_paco _pascalpart_partimagenet_in” model, because of
    its high mAP score in their evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Current research highlights the significance of crafting precise prompts for
    Large Language Models (LLMs). Our method uses straightforward natural language
    prompts with organized outputs, engaging LLMs at three key stages: identifying
    relevant objects, pinpointing the object part related to the task, and seeking
    alternative part names.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we provide the LLM with a list of items and the task, from which it selects
    objects suited to the task.
  prefs: []
  type: TYPE_NORMAL
- en: Object Prompt
    Your task is to $<$
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer like the following: Objects: Reason:'
  prefs: []
  type: TYPE_NORMAL
- en: Our prompts for the LLM include “Object” and “Reason” sections to specify the
    chosen object and its justification, enhancing response validity through a logical
    “chain of thought.” Following object identification, the LLM is further queried
    for the task-related object part, using a similar prompt structure. If the part
    segmentation by the VLM fails, we seek alternative part names from the LLM, like
    “cup side” for “cup body.” We use GPT-4[[25](#bib.bib25)] API with a zero temperature
    setting for consistent outputs.
  prefs: []
  type: TYPE_NORMAL
- en: V Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V-A Affordance Localization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our experiments, we aim to determine the LLM’s understanding of affordances
    by testing our pipeline on the UMD[[13](#bib.bib13)] dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We selected the UMD[[13](#bib.bib13)] dataset for its extensive usage and the
    availability of numerous benchmarks for comparison. This dataset features RGB-D
    images with ground truth pixel-wise affordance labels for 105 items encompassing
    different viewpoints, 7 affordances, and 17 object classes. We use the weighted
    F-score as used in the original publication.
  prefs: []
  type: TYPE_NORMAL
- en: 'The UMD[[13](#bib.bib13)] dataset, includes the following objects [knife, saw,
    scissors, shears, scoop, spoon, trowel, bowl, cup, ladle, mug, pot, shovel, turner,
    hammer, mallet, tenderizer] and affordances [grasp, cut, scoop, contain, pound,
    support, wrap-grasp]. We did not finetune or train on this list, but we use the
    list of objects and affordances as inputs to the system. If a given object was
    not detected or an object part was not detected, a f-score of 0 was assigned to
    that case. We ran this method on the test set images and the results are in table
    [I](#S5.T1 "TABLE I ‣ V-A Affordance Localization ‣ V Experiments ‣ OVAL-Prompt:
    Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Performance on UMD Dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | HMP[[13](#bib.bib13)] | DeepLab[[32](#bib.bib32)] | AffordenceNet[[18](#bib.bib18)]
    | Attention-CNN[[33](#bib.bib33)] | RelaNet[[34](#bib.bib34)] | GSE[[35](#bib.bib35)]
    | OVAL-Prompt(Ours) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Grasp | 0.367 | 0.620 | 0.731 | 0.784 | 0.737 | 0.779 | 0.650 |'
  prefs: []
  type: TYPE_TB
- en: '| W-Grasp | 0.373 | 0.730 | 0.814 | 0.822 | 0.824 | 0.840 | 0.718 |'
  prefs: []
  type: TYPE_TB
- en: '| Cut | 0.415 | 0.600 | 0.762 | 0.761 | 0.755 | 0.776 | 0.823 |'
  prefs: []
  type: TYPE_TB
- en: '| Contain | 0.810 | 0.900 | 0.833 | 0.840 | 0.941 | 0.924 | 0.688 |'
  prefs: []
  type: TYPE_TB
- en: '| Support | 0.643 | 0.600 | 0.821 | 0.844 | 0.866 | 0.893 | 0.538 |'
  prefs: []
  type: TYPE_TB
- en: '| Scoop | 0.524 | 0.800 | 0.793 | 0.862 | 0.802 | 0.856 | 0.753 |'
  prefs: []
  type: TYPE_TB
- en: '| Pound | 0.767 | 0.880 | 0.836 | 0.847 | 0.879 | 0.918 | 0.809 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 0.557 | 0.733 | 0.799 | 0.823 | 0.829 | 0.855 | 0.711 |'
  prefs: []
  type: TYPE_TB
- en: OVAL-Prompt achieves a F-score +0.154 higher than HMP on average but -0.144
    lower than the highest-performing GSE model. Despite its sub state-of-the-art
    performance it achieves a level of performance that is functional for real-world
    application.
  prefs: []
  type: TYPE_NORMAL
- en: V-A1 Failure mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The segmentation from the VLM resulted in lower F-scores primarily due to misaligned
    ground truth and imprecise part isolation, as depicted in Figure [1](#S5.F1 "Figure
    1 ‣ V-A1 Failure mode ‣ V-A Affordance Localization ‣ V Experiments ‣ OVAL-Prompt:
    Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding").
    Ground truth often mismatched the actual object parts, and VLM occasionally segmented
    entire objects instead of a part. The system exhibited weaknesses at multiple
    stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/624f7b0e031f24fb850d93c0df4914be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Segmentation Failures'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class names: VLM struggled with class names like “turner” and “pot,” causing
    confusion. Using more common names such as “spatula” instead of “turner” and “plant
    pot” for “pot” might improve its performance, as VLM better recognizes common
    names.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spatial relations: The LLM produced part descriptions involving spatial relationships
    (e.g. cup top), which were difficult for the VLM to process. To overcome this,
    the LLM was prompted once more for alternative names when the VLM failed to recognize
    the initially described part. This approach improved the VLM’s detection accuracy
    when the reprompted part was a physical part (e.g. cup rim).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: V-B Robot Demonstration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate the concept with an actual robot, we utilized a Fetch and Freight
    Research Edition robot to grasp various random objects that were placed on a table
    directly in front of it. We captured both RGB and depth images using the onboard
    cameras. The RGB image was then processed through the previously described pipeline,
    where we segmented the image area that affords grasping, resulting in a binary
    mask. This mask, along with the depth information, was fed into DexNet[[1](#bib.bib1)]
    to suggest a potential grasp. To simplify the grasping process, we modified the
    grasp suggested by DexNet[[1](#bib.bib1)] and converted it into a top-down pick-up.
    This was achieved by using the x, y, z coordinates, setting the gripper orientation
    to face downwards along the negative z-axis, and then calculating the z-axis rotation
    for the wrist rotation. The finalized grasp configuration was then executed by
    the robot using the MoveIt[[36](#bib.bib36)]. A image of the setup of shown in
    figure [2](#S5.F2 "Figure 2 ‣ V-B Robot Demonstration ‣ V Experiments ‣ OVAL-Prompt:
    Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c6df4e77a31bec7760237fdcaae4aea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Robot Experimental Setup'
  prefs: []
  type: TYPE_NORMAL
- en: 'Video: https://youtu.be/fXvYo-0AJek'
  prefs: []
  type: TYPE_NORMAL
- en: 'The experiment involved a spatula, walkie-talkie, toy lightsaber, mug, clamp,
    and brush, repeated ten times each. Table [II](#S5.T2 "TABLE II ‣ V-B Robot Demonstration
    ‣ V Experiments ‣ OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot
    Manipulation through LLM Affordance-Grounding") shows successful segmentations
    where segments matched the correct object parts, the rate of feasible grasp proposals
    at suggested parts, and successful object pickups by the robot at these parts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Grasping Performance Metrics'
  prefs: []
  type: TYPE_NORMAL
- en: '| Object | Sugge- | Segmentation | Grasp | Pick |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | sted Part | Success | Success | Success |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Brush | Handle | 10/10 | 10/10 | 10/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Clamp | Handle | 6/10 | 6/6 | 5/6 |'
  prefs: []
  type: TYPE_TB
- en: '| Mug | Handle | 10/10 | 10/10 | 7/10 |'
  prefs: []
  type: TYPE_TB
- en: '| Lightsaber | Handle | 5/10 | 5/5 | 5/5 |'
  prefs: []
  type: TYPE_TB
- en: '| Spatula | Handle | 7/10 | 7/7 | 6/7 |'
  prefs: []
  type: TYPE_TB
- en: '| Walkie Talkie | Body | 4/10 | 4/4 | 4/4 |'
  prefs: []
  type: TYPE_TB
- en: Failures are mostly caused by imprecise segmentation that include whole objects
    instead of specific parts. However, with accurate segmentation, grasp and pickup
    success rates approach 100%. Grasping failures occurred for the mug because its
    shapes was not considered in grasp planning.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Ablation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To validate our method, we removed components from our network to evaluate
    their impact on performance. We tested two variations: 1) using only the VLM with
    a list of affordances and 2) excluding the reprompting step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Ablation on UMD dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | VLM only | No Reprompting | Full Network |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| grasp | 0.002 | 0.498 | 0.650 |'
  prefs: []
  type: TYPE_TB
- en: '| cut | 0.013 | 0.401 | 0.823 |'
  prefs: []
  type: TYPE_TB
- en: '| scoop | 0.062 | 0.345 | 0.753 |'
  prefs: []
  type: TYPE_TB
- en: '| contain | 0 | 0.437 | 0.688 |'
  prefs: []
  type: TYPE_TB
- en: '| pound | 0 | 0.317 | 0.809 |'
  prefs: []
  type: TYPE_TB
- en: '| support | 0 | 0.436 | 0.538 |'
  prefs: []
  type: TYPE_TB
- en: '| wrap-grasp | 0 | 0.312 | 0.718 |'
  prefs: []
  type: TYPE_TB
- en: '| Average | 0.011 | 0.392 | 0.711 |'
  prefs: []
  type: TYPE_TB
- en: 'Table [III](#S5.T3 "TABLE III ‣ V-C Ablation ‣ V Experiments ‣ OVAL-Prompt:
    Open-Vocabulary Affordance Localization for Robot Manipulation through LLM Affordance-Grounding")
    shows that a standalone VLM struggles with affordances, likely because VLMs are
    not typically trained on such tasks. The necessity of reprompting for alternative
    part names is also highlighted, emphasizing how the VLM’s open vocabulary can
    respond variably to different words, showing that using synonyms can improve results.'
  prefs: []
  type: TYPE_NORMAL
- en: VI Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our experiments demonstrate OVAL-Prompt’s potential for open-vocabulary affordance
    localization, achieving results in the UMD dataset that is comparable to supervised
    models without fine-tuning. Furthermore, OVAL-Prompt’s efficacy in real-world
    robotic tasks highlight its practicality.
  prefs: []
  type: TYPE_NORMAL
- en: Our method efficiently performs zero-shot affordance localization but struggles
    with scalability for numerous items due to individual LLM queries. Future improvements
    could enhance simultaneous multi-item and affordance identification. Testing in
    cluttered environments and eliminating the VLM’s need for predefined object lists
    are also potential development areas.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported in part by a Qualcomm Innovation Fellowship, J.P. Morgan
    AI Research, Amazon, and Ford Motor Company.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J. Mahler, J. Liang, S. Niyaz, M. Laskey, R. Doan, X. Liu, J. A. Ojea,
    and K. Goldberg, “Dex-net 2.0: Deep learning to plan robust grasps with synthetic
    point clouds and analytic grasp metrics,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. J. Gibson, *The ecological approach to visual perception: classic edition*,
    ser. Psychology Press classic editions.   Psychology Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] H. Luo, W. Zhai, J. Zhang, Y. Cao, and D. Tao, “Learning affordance grounding
    from exocentric images,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] T. Nguyen, M. N. Vu, A. Vuong, D. Nguyen, T. Vo, N. Le, and A. Nguyen,
    “Open-vocabulary affordance detection in 3d point clouds,” in *2023 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2023, pp. 5692–5698.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn,
    C. Fu, K. Gopalakrishnan, K. Hausman, A. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter,
    A. Irpan, E. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi, R. Julian,
    D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine, Y. Lu, L. Luu, C. Parada, P. Pastor,
    J. Quiambao, K. Rao, J. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan,
    A. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu, M. Yan, and A. Zeng, “Do
    as i can and not as i say: Grounding language in robotic affordances,” in *arXiv
    preprint arXiv:2204.01691*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] S. Qian, W. Chen, M. Bai, X. Zhou, Z. Tu, and L. E. Li, “Affordancellm:
    Grounding affordance from vision language models,” *arXiv preprint arXiv:2401.06341*,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,
    and A. Zeng, “Code as policies: Language model programs for embodied control,”
    in *arXiv preprint arXiv:2209.07753*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] K. Kawaharazuka, T. Matsushima, A. Gambardella, J. Guo, C. Paxton, and
    A. Zeng, “Real-world robot applications of foundation models: A review,” *arXiv
    preprint arXiv:2402.05741*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] S. Kambhampati, K. Valmeekam, L. Guan, K. Stechly, M. Verma, S. Bhambri,
    L. Saldyt, and A. Murthy, “Llms can’t plan, but can help planning in llm-modulo
    frameworks,” *arXiv preprint arXiv:2402.01817*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, “Learning transferable
    visual models from natural language supervision,” 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. Rashid, S. Sharma, C. M. Kim, J. Kerr, L. Y. Chen, A. Kanazawa, and
    K. Goldberg, “Language embedded radiance fields for zero-shot task-oriented grasping,”
    in *7th Annual Conference on Robot Learning*, 2023\. [Online]. Available: [https://openreview.net/forum?id=k-Fg8JDQmc](https://openreview.net/forum?id=k-Fg8JDQmc)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] K. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati, “On the planning
    abilities of large language models-a critical investigation,” *Advances in Neural
    Information Processing Systems*, vol. 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. Myers, C. L. Teo, C. Fermüller, and Y. Aloimonos, “Affordance detection
    of tool parts from geometric features,” in *2015 IEEE International Conference
    on Robotics and Automation (ICRA)*, 2015, pp. 1374–1381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. Chen, D. Kong, J. Li, S. Wang, and B. Yin, “A survey of visual affordance
    recognition based on deep learning,” *IEEE Transactions on Big Data*, vol. 9,
    no. 6, pp. 1458–1476, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” *arXiv preprint arXiv:1409.1556*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] A. Nguyen, D. Kanoulas, D. G. Caldwell, and N. G. Tsagarakis, “Detecting
    object affordances with convolutional neural networks,” in *2016 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*, 2016, pp. 2765–2770.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] ——, “Object-based affordances detection with convolutional neural networks
    and dense conditional random fields,” in *2017 IEEE/RSJ International Conference
    on Intelligent Robots and Systems (IROS)*, 2017, pp. 5908–5915.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T.-T. Do, A. Nguyen, and I. Reid, “Affordancenet: An end-to-end deep learning
    approach for object affordance detection,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] T. Nagarajan, C. Feichtenhofer, and K. Grauman, “Grounded human-object
    interaction hotspots from video,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2019, pp. 8688–8697.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] K. Fang, Y. Zhu, A. Garg, A. Kurenkov, V. Mehta, L. Fei-Fei, and S. Savarese,
    “Learning task-oriented grasping for tool manipulation from simulated self-supervision,”
    *The International Journal of Robotics Research*, vol. 39, no. 2-3, pp. 202–216,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. Deng, X. Xu, C. Wu, K. Chen, and K. Jia, “3d affordancenet: A benchmark
    for visual object affordance understanding,” in *proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition*, 2021, pp. 1778–1787.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Wu, X. Li, S. Xu, H. Yuan, H. Ding, Y. Yang, X. Li, J. Zhang, Y. Tong,
    X. Jiang, B. Ghanem, and D. Tao, “Towards open vocabulary learning: A survey,”
    *T-PAMI*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] G. Li, V. Jampani, D. Sun, and L. Sevilla-Lara, “Locate: Localize and
    transfer object parts for weakly supervised affordance grounding,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 10 922–10 931.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] G. Li, D. Sun, L. Sevilla-Lara, and V. Jampani, “One-shot open affordance
    learning with foundation models,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] OpenAI, “Gpt-4,” [https://openai.com/research/gpt-4](https://openai.com/research/gpt-4),
    2024, accessed: 2024-03-04.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] “Gemini: Google deepmind’s multimodal language model,” [https://deepmind.google](https://deepmind.google),
    2024, accessed: 2024-03-04.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su,
    “Llm-planner: Few-shot grounded planning for embodied agents with large language
    models,” in *Proceedings of the IEEE/CVF International Conference on Computer
    Vision (ICCV)*, October 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] D. Shah, B. Osinski, B. Ichter, and S. Levine, “Lm-nav: Robotic navigation
    with large pre-trained models of language, vision, and action,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] K. Valmeekam, M. Marquez, S. Sreedharan, and S. Kambhampati, “On the planning
    abilities of large language models : A critical investigation,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] S. Kambhampati, “Can llms really reason and plan?” *Communications of
    the ACM*, Sep 2023\. [Online]. Available: [https://cacm.acm.org/blogcacm/can-llms-really-reason-and-plan/](https://cacm.acm.org/blogcacm/can-llms-really-reason-and-plan/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] P. Sun, S. Chen, C. Zhu, F. Xiao, P. Luo, S. Xie, and Z. Yan, “Going denser
    with open-vocabulary part segmentation,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille, “Deeplab:
    Semantic image segmentation with deep convolutional nets, atrous convolution,
    and fully connected crfs,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Q. Gu, J. Su, and L. Yuan, “Visual affordance detection using an efficient
    attention convolutional neural network,” vol. 440, pp. 36–44\. [Online]. Available:
    [https://linkinghub.elsevier.com/retrieve/pii/S0925231221000278](https://linkinghub.elsevier.com/retrieve/pii/S0925231221000278)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] X. Zhao, Y. Cao, and Y. Kang, “Object affordance detection with relationship-aware
    network,” vol. 32, no. 18, pp. 14 321–14 333\. [Online]. Available: [http://link.springer.com/10.1007/s00521-019-04336-0](http://link.springer.com/10.1007/s00521-019-04336-0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Y. Zhang, H. Li, T. Ren, Y. Dou, and Q. Li, “Multi-scale fusion and global
    semantic encoding for affordance detection,” in *2022 International Joint Conference
    on Neural Networks (IJCNN)*, 2022, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] D. T. Coleman, I. A. Sucan, S. Chitta, and N. Correll, “Reducing the barrier
    to entry of complex robotic software: a MoveIt! case study,” publisher: [object
    Object]. [Online]. Available: [https://aisberg.unibg.it//handle/10446/87657](https://aisberg.unibg.it//handle/10446/87657)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Mai, M. Yang, and W. Luo, “Erasing integrated learning: A simple yet
    effective approach for weakly supervised object localization,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*,
    June 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] X. Pan, Y. Gao, Z. Lin, F. Tang, W. Dong, H. Yuan, F. Huang, and C. Xu,
    “Unveiling the potential of structure preserving for weakly supervised object
    localization,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*, June 2021, pp. 11 642–11 651.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Yao, F. Wan, W. Gao, X. Pan, Z. Peng, Q. Tian, and Q. Ye, “Ts-cam:
    Token semantic coupled attention map for weakly supervised object localization,”
    *IEEE Transactions on Neural Networks and Learning Systems*, pp. 1–13, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] H. Luo, W. Zhai, J. Zhang, Y. Cao, and D. Tao, “Grounded affordance from
    exocentric view,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] D. Hadjivelichkov, S. Zwane, M. P. Deisenroth, L. Agapito, and D. Kanoulas,
    “One-shot transfer of affordance regions? affcorrs!” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: VII Supplementary Material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to the UMD dataset we evaluated our method on the AGD20K dataset.
  prefs: []
  type: TYPE_NORMAL
- en: VII-1 AGD20K
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The AGD20K dataset is intended for affordance grounding tasks and includes
    over 20,000 images across 36 affordance categories. It utilizes three evaluation
    metrics: Kullback-Leibler Divergence (KLD), similarity metric (SIM), and Normalized
    Scanpath Saliency (NSS). Detailed information on their design and implementation
    can be found in the supplementary material of the Luo et al. paper [[3](#bib.bib3)].
    For the performance of other models, we referenced the data from the paper by
    Li et al. [[23](#bib.bib23)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our method (see table [IV](#S7.T4 "TABLE IV ‣ VII-1 AGD20K ‣ VII Supplementary
    Material ‣ OVAL-Prompt: Open-Vocabulary Affordance Localization for Robot Manipulation
    through LLM Affordance-Grounding")) has higher KLD than the best seen model by
    9.429 points and the unseen by 7.427\. Our SIM and NSS scores were competitive
    with other models beating several models and only being surpassed by LOCATE[[23](#bib.bib23)]
    for both seen and unseen categories, suggesting our segmentation closely aligns
    with the ground truth. The high KLD stems from our method producing a binary mask
    for the affordance area, in contrast to the ground truth comparison that uses
    a value distribution, the peak of which indicates the affordance region. Consequently,
    our non-distributional output incurs a significant penalty in KLD calculation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: AGD20K Dataset Evaulation'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Seen | Unseen |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | KLD↓ | SIM↑ | NSS↑ | KLD↓ | SIM↑ | NSS↑ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EIL[[37](#bib.bib37)] | 1.931 | 0.285 | 0.522 | 2.167 | 0.227 | 0.330 |'
  prefs: []
  type: TYPE_TB
- en: '| SPA[[38](#bib.bib38)] | 5.528 | 0.221 | 0.357 | 7.425 | 0.169 | 0.262 |'
  prefs: []
  type: TYPE_TB
- en: '| TS-CAM [[39](#bib.bib39)] | 1.842 | 0.260 | 0.336 | 2.104 | 0.201 | 0.151
    |'
  prefs: []
  type: TYPE_TB
- en: '| Hotspots [[19](#bib.bib19)] | 1.773 | 0.278 | 0.615 | 1.994 | 0.237 | 0.577
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-view-AG [[3](#bib.bib3)] | 1.538 | 0.334 | 0.927 | 1.787 | 0.285 |
    0.829 |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-view-AG+ [[40](#bib.bib40)] | 1.489 | 0.342 | 0.981 | 1.765 | 0.279
    | 0.882 |'
  prefs: []
  type: TYPE_TB
- en: '| AffCorrs†[[41](#bib.bib41)] | 1.407 | 0.359 | 1.026 | 1.618 | 0.348 | 1.021
    |'
  prefs: []
  type: TYPE_TB
- en: '| LOCATE[[23](#bib.bib23)] | 1.226 | 0.401 | 1.177 | 1.405 | 0.372 | 1.157
    |'
  prefs: []
  type: TYPE_TB
- en: '| OVAL-Prompt(Ours) | 10.649 | 0.339 | 1.044 | 8.832 | 0.365 | 0.925 |'
  prefs: []
  type: TYPE_TB
