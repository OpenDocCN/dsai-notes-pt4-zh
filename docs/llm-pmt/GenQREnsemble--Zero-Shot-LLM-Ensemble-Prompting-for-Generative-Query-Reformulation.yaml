- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:45:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GenQREnsemble: 基于零-shot LLM 的集成提示生成查询重构'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.03746](https://ar5iv.labs.arxiv.org/html/2404.03746)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.03746](https://ar5iv.labs.arxiv.org/html/2404.03746)
- en: '¹¹institutetext: Department of Computer Science'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹institutetext: 计算机科学系'
- en: Emory University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 埃默里大学
- en: Atlanta, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 美国亚特兰大
- en: '¹¹email: {kaustubh.dhole, eugene.agichtein}@emory.eduKaustubh D. Dhole    Eugene
    Agichtein'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '¹¹email: {kaustubh.dhole, eugene.agichtein}@emory.eduKaustubh D. Dhole    Eugene
    Agichtein'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Query Reformulation(QR) is a set of techniques used to transform a user’s original
    search query to a text that better aligns with the user’s intent and improves
    their search experience. Recently, zero-shot QR has been shown to be a promising
    approach due to its ability to exploit knowledge inherent in large language models.
    By taking inspiration from the success of ensemble prompting strategies which
    have benefited many tasks, we investigate if they can help improve query reformulation.
    In this context, we propose an ensemble based prompting technique, GenQREnsemble
    which leverages paraphrases of a zero-shot instruction to generate multiple sets
    of keywords ultimately improving retrieval performance. We further introduce its
    post-retrieval variant, GenQREnsembleRF to incorporate pseudo relevant feedback.
    On evaluations over four IR benchmarks, we find that GenQREnsemble generates better
    reformulations with relative nDCG@10 improvements up to 18% and MAP improvements
    upto 24% over the previous zero-shot state-of-art. On the MSMarco Passage Ranking
    task, GenQREnsembleRF shows relative gains of 5% MRR using pseudo-relevance feedback,
    and 9% nDCG@10 using relevant feedback documents.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重构（QR）是一套用于将用户原始搜索查询转化为更符合用户意图并提升搜索体验的文本的技术。最近，零-shot QR由于能够利用大型语言模型固有的知识，显示出成为一种有前途的方法。通过借鉴成功的集成提示策略，我们研究了它们是否能帮助改善查询重构。在这种背景下，我们提出了一种基于集成的提示技术，GenQREnsemble，该技术利用零-shot
    指令的同义表达生成多个关键词集，*最终提高检索性能*。我们进一步介绍了其后检索变体 GenQREnsembleRF，以纳入伪相关反馈。在对四个信息检索基准的评估中，我们发现
    GenQREnsemble 生成的重构相较于以前的零-shot 最佳状态，nDCG@10 的相对提升高达 18%，MAP 的提升高达 24%。在 MSMarco
    Passage Ranking 任务中，GenQREnsembleRF 使用伪相关反馈显示出 5% 的 MRR 相对增益，使用相关反馈文档显示出 9% 的
    nDCG@10 增益。
- en: 'Keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Query Reformulation Zero-Shot Prompting Relevance Feedback
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重构 零-shot 提示 相关反馈
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Users searching for relevant documents might not always be able to accurately
    express their information needs in their initial queries. This could result in
    queries being vague or ambiguous or lacking the necessary domain vocabulary. Query
    Reformulation (QR) is a set of techniques used to transform a user’s original
    search query to a text that better aligns with the user’s intent and improves
    their search experience. Such reformulation alleviates the vocabulary mismatch
    problem by expanding the query with related terms or paraphrasing it into a suitable
    form by incorporating additional context.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 用户在搜索相关文档时，可能无法准确表达其信息需求。这可能导致查询模糊或含糊不清，或缺乏必要的领域词汇。查询重构（QR）是一套用于将用户原始搜索查询转化为更符合用户意图并提升搜索体验的文本的技术。这种重构通过扩展查询或将其改写为适当的形式来缓解词汇不匹配问题，方法是通过引入额外的上下文。
- en: Recently, with the success of large language models (LLMs) [[5](#bib.bib5),
    [23](#bib.bib23)], a plethora of QR approaches have been developed. The generative
    capabilities of LLMs have been exploited to produce novel queries [[22](#bib.bib22)],
    as well as useful keywords to be appended to the users’ original queries [[29](#bib.bib29)].
    By gaining exposure to enormous amounts of text during pre-training, prompting
    has become a promising avenue for utilizing knowledge inherent in an LLM for the
    benefit of subsequent downstream tasks [[27](#bib.bib27)] especially QR [[14](#bib.bib14),
    [32](#bib.bib32)].
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，随着大型语言模型（LLMs）的成功[[5](#bib.bib5), [23](#bib.bib23)]，已经开发出了大量的 QR 方法。LLMs
    的生成能力已被利用来产生新颖的查询[[22](#bib.bib22)]，以及有用的关键词以附加到用户的原始查询中[[29](#bib.bib29)]。通过在预训练过程中接触大量文本，提示已成为利用
    LLM 固有知识为后续下游任务提供益处的一个有前途的途径[[27](#bib.bib27)]，尤其是 QR[[14](#bib.bib14), [32](#bib.bib32)]。
- en: Unlike training or few-shot learning, zero-shot prompting does not rely on any
    labeled examples. The advantage of a zero-shot approach is the ease with which
    a standalone generative model can be used to reformulate queries by prompting
    a templated piece of instruction along with the original query. Particularly,
    zero-shot QR can be used to generate keywords by prompting the user’s original
    query along with an instruction that defines the task of query reformulation in
    natural language like Generate useful search terms for the given query:‘List all
    the breweries in Austin’.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 与训练或少样本学习不同，零样本提示不依赖任何标记示例。零样本方法的优势在于，独立的生成模型可以通过提示一个模板化的指令与原始查询一起使用，以重新构建查询。特别是，零样本
    QR 可以通过提示用户的原始查询以及定义查询改写任务的自然语言指令（如“生成对给定查询有用的搜索词：‘列出奥斯汀所有的酿酒厂’”）来生成关键词。
- en: '| Instruction | Expansions Generated |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 指令 | 生成的扩展 |'
- en: '| --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Increase the search efficacy by offering | age goldfish grow outsmart outlive
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 通过提供 | 年龄 金鱼 长大 智胜 长寿 |'
- en: '| beneficial expansion keywords for the query | ageing species… |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 对查询有益的扩展关键词 | 老化 物种… |'
- en: '| Enhance search outcomes by recommending beneficial | Goldfish breed sizes
    What kind of |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 通过推荐有益的 | 金鱼 品种 大小 什么样的 |'
- en: '| expansion terms to supplement the query | goldfish grows the fastest… |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 扩展术语以补充查询 | 金鱼生长最快… |'
- en: 'Figure 1: Keywords generated for the query (“do goldfish grow”) differ drastically
    when generated from two paraphrastic instructions prompted to flan-t5-xxl [[7](#bib.bib7)].'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：针对查询（“金鱼会长大吗”）生成的关键词在使用两个不同的改写指令生成时有很大差异[[7](#bib.bib7)]。
- en: 'However, such a zero-shot prompting approach is still contingent on the exact
    instruction appearing in the prompt providing plenty of avenues of improvement.
    While LLMs have been known to vary significantly in performance across different
    prompts [[36](#bib.bib36), [8](#bib.bib8)] and generation settings [[33](#bib.bib33)],
    many natural language tasks have benefited by exploiting such variation via ensembling
    multiple prompts or generating diverse reasoning paths [[16](#bib.bib16), [3](#bib.bib3),
    [31](#bib.bib31)]. Whether such improvements also transfer to tasks like QR is
    yet to be determined. In Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ GenQREnsemble:
    Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation"), a vast
    difference is noticed in the keywords generated when the input instruction is
    altered to a semantically similar variant. We hypothesize that QR might naturally
    benefit from such variation – An ensemble of zero-shot reformulators with paraphrastic
    instructions can be tasked to look at the input query in diverse ways so as to
    elicit different expansions. This work proposes the following contributions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种零样本提示方法仍然依赖于提示中出现的确切指令，提供了许多改进的途径。虽然 LLM 已知在不同的提示[[36](#bib.bib36), [8](#bib.bib8)]和生成设置[[33](#bib.bib33)]下表现差异很大，但许多自然语言任务通过利用这种差异，如集成多个提示或生成不同的推理路径[[16](#bib.bib16),
    [3](#bib.bib3), [31](#bib.bib31)]，已经获益。是否这种改进也会转移到像 QR 这样的任务上尚待确定。在图 [1](#S1.F1
    "图 1 ‣ 1 介绍 ‣ GenQREnsemble：用于生成性查询改写的零样本 LLM 集成提示") 中，当输入指令更改为语义上类似的变体时，生成的关键词差异巨大。我们假设
    QR 可能自然受益于这种变异——具有改写指令的零样本重写器的集成可以从不同的角度查看输入查询，以引发不同的扩展。本研究提出了以下贡献：
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel method,GenQREnsemble – a zero-shot Ensemble based Generative
    Query Reformulator which exploits multiple zero-shot instructions for QR to generate
    a more effective query reformulation than possible with an individual instruction.
    (Section 3)
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新方法 GenQREnsemble——一种基于零样本的生成查询改写器，它利用多个零样本指令进行 QR，以生成比单个指令更有效的查询改写。（第
    3 节）
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We further introduce an extension GenQREnsembleRF to incorporate Relevance Feedback
    into the process. (Section 3)
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进一步引入了扩展 GenQREnsembleRF 来将相关反馈融入流程中。（第 3 节）
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We evaluate the proposed methods over four standard IR benchmarks, demonstrating
    significant relative improvements vs. recent state of the art, of up to 18% on
    nDCG@10 in pre-retrieval settings, and of up to 9% nDCG@10 on post-retrieval (feedback)
    settings, demonstrating increased generalizability of our approach.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在四个标准 IR 基准上评估了提出的方法，显示出相对于近期最先进技术的显著相对改进，在预检索设置下 nDCG@10 的改进高达 18%，在后检索（反馈）设置下
    nDCG@10 的改进高达 9%，显示了我们方法的更强泛化能力。
- en: Next, we summarize the prior work to place our contributions in context.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们总结了先前的工作，以便将我们的贡献置于上下文中。
- en: 2 Related Work
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Query reformulation has been shown to be effective in many settings [[6](#bib.bib6)].
    It can be done pre-retrieval, or post-retrieval, via incorporating evidence from
    feedback, obtained either from a user or from top-ranked results in the sparse
    retrieval setting [[15](#bib.bib15)], and in the dense retrieval setting [[30](#bib.bib30),
    [35](#bib.bib35)].
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 查询重构在许多设置中已被证明是有效的 [[6](#bib.bib6)]。它可以在检索前或检索后通过结合来自反馈的证据来完成，这些反馈可以来自用户或来自稀疏检索设置中的顶级结果
    [[15](#bib.bib15)]，以及在密集检索设置中 [[30](#bib.bib30), [35](#bib.bib35)]。
- en: Recently, zero-shot approaches to query reformulation have received considerable
    attention. Wang et al. [[29](#bib.bib29)] design a query reformulator by fine-tuning
    a sequence-to-sequence transformer, T5 [[25](#bib.bib25)] on pairs of raw and
    transformed queries. Their zero-shot prompting approach uses an instruction-tuned
    model, FlanT5 [[7](#bib.bib7)] to generate keywords for query expansion and incorporating
    PRF. Jagerman et al. [[14](#bib.bib14)] demonstrate LLMs can be more powerful
    than traditional methods for query expansion. Mo et al. [[19](#bib.bib19)] propose
    a framework to reformulate conversational search queries using LLMs. Gao et al. [[11](#bib.bib11)]’s
    framework performs retrieval through fake documents generated by prompting LLMs
    with user queries. Alaofi et al. [[2](#bib.bib2)] prompt LLMs with information
    descriptions to generate query variants.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，零样本方法在查询重构方面受到相当大的关注。Wang 等人 [[29](#bib.bib29)] 通过微调序列到序列的变换器 T5 [[25](#bib.bib25)]
    设计了一种查询重构器，该变换器使用原始和转化后的查询对进行训练。他们的零样本提示方法使用了一个经过指令调优的模型 FlanT5 [[7](#bib.bib7)]，用于生成查询扩展的关键词并结合
    PRF。Jagerman 等人 [[14](#bib.bib14)] 证明了 LLMs 在查询扩展方面可以比传统方法更强大。Mo 等人 [[19](#bib.bib19)]
    提出了一个使用 LLMs 进行对话搜索查询重构的框架。Gao 等人 [[11](#bib.bib11)] 的框架通过用用户查询提示 LLMs 生成的虚假文档进行检索。Alaofi
    等人 [[2](#bib.bib2)] 通过信息描述提示 LLMs 以生成查询变体。
- en: However, using a single query reformulation can sometimes degrade performance
    compared to the original query. To address this drawback, prior efforts have incorporated
    ensemble strategies via keywords from numerous sources or fusing documents from
    different queries. Gao et al. [[10](#bib.bib10)], combine features derived from
    various translation models to generate better query rewrites. Si et al. [[26](#bib.bib26)]
    perform QR by utilizing multiple external biomedical resources. Hsu and Taksa [[13](#bib.bib13)]
    present a data fusion framework suggesting that diverse query formulations represent
    distinct evidence sources for inferring document relevance. Later, Mohankumar
    et al. [[20](#bib.bib20)] generated diverse queries by introducing a diversity-driven
    RL algorithm. For other tasks, recent works demonstrated the benefits of ensemble
    strategies for prompting LLMs, including self-consistency [[31](#bib.bib31)] for
    arithmetic and common sense tasks, Chain of Verification [[9](#bib.bib9)] for
    improving factuality, and Diverse [[16](#bib.bib16)] for question answering. However,
    zero-shot based ensemble methods for LLM have not been explored for the Query
    Reformulation task, as we propose in this paper.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用单一的查询重构有时会降低相对于原始查询的性能。为了解决这一缺陷，先前的工作已经通过来自多个来源的关键词或融合来自不同查询的文档来结合集成策略。Gao
    等人 [[10](#bib.bib10)] 结合了来自各种翻译模型的特征，以生成更好的查询重写。Si 等人 [[26](#bib.bib26)] 通过利用多个外部生物医学资源进行
    QR。Hsu 和 Taksa [[13](#bib.bib13)] 提出了一个数据融合框架，表明多样的查询公式代表了推断文档相关性的不同证据来源。随后，Mohankumar
    等人 [[20](#bib.bib20)] 通过引入一个以多样性驱动的 RL 算法生成了多样的查询。对于其他任务，最近的工作展示了集成策略对提示 LLMs
    的好处，包括自一致性 [[31](#bib.bib31)] 用于算术和常识任务、验证链 [[9](#bib.bib9)] 用于提高真实性、以及多样性 [[16](#bib.bib16)]
    用于问答。然而，零样本基于 LLM 的集成方法在查询重构任务中尚未被探索，如我们在本文中提出的。
- en: '3 Proposed Approach: GenQREnsemble'
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提议的方法：GenQREnsemble
- en: In this section, we describe two variations of our proposed approach, for the
    pre- and post-retrieval settings. In the pre-retrieval setting, a Query Reformulation
    $R$ to improve retrieval effectiveness for a given search task (e.g., passage
    or document retrieval). We also consider the post-retrieval setting, wherein the
    reformulator can incorporate additional contextual information like document or
    passage-level feedback.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了我们提议的方法的两种变体，分别用于检索前和检索后设置。在检索前设置中，查询重新表述$R$用于提高给定搜索任务（例如，段落或文档检索）的检索效果。我们还考虑了检索后设置，在该设置中，重新表述器可以结合额外的上下文信息，如文档或段落级反馈。
- en: '![Refer to caption](img/2acda55b53552929f513ca6803ca5f0f.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2acda55b53552929f513ca6803ca5f0f.png)'
- en: 'Figure 2: The complete flow and algorithm shown on the top right.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：右上角显示的完整流程和算法。
- en: 'Pre-retrieval: We propose GenQREnsemble – an ensemble prompting based query
    reformulator which uses $N$. The complete process and algorithm are shown in Figure [2](#S3.F2
    "Figure 2 ‣ 3 Proposed Approach: GenQREnsemble ‣ GenQREnsemble: Zero-Shot LLM
    Ensemble Prompting for Generative Query Reformulation").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 检索前：我们提出了GenQREnsemble——一种基于集成提示的查询重新表述器，它使用了$N$。完整的过程和算法见图[2](#S3.F2 "图 2 ‣
    3 提议的方法：GenQREnsemble ‣ GenQREnsemble：用于生成查询重新表述的零样本LLM集成提示")。
- en: 'Post-retrieval: To assess how well our method can incorporate additional context
    like document feedback, we introduce GenQREnsembleRF. Here, we prepend the $N$,
    obtained either as pseudo-relevance feedback from initial retrieval or manually
    chosen by the user.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 检索后：为了评估我们的方法如何结合额外的上下文信息，如文档反馈，我们引入了GenQREnsembleRF。在这里，我们将$N$，它可以是从初始检索获得的伪相关反馈或由用户手动选择，作为前缀。
- en: 4 Experiments
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: '| # | Instruction |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| # | 指令 |'
- en: '| --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | Improve the search effectiveness by suggesting expansion terms for the
    query |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 通过建议扩展术语来提高搜索效果 |'
- en: '| 2 | Recommend expansion terms for the query to improve search results |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 推荐查询的扩展术语以改善搜索结果 |'
- en: '| 3 | Improve the search effectiveness by suggesting useful expansion terms
    for the query |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 通过建议有用的扩展术语来提高搜索效果 |'
- en: '| 4 | Maximize search utility by suggesting relevant expansion phrases for
    the query |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 通过为查询建议相关的扩展短语来最大化搜索效用 |'
- en: '| 5 | Enhance search efficiency by proposing valuable terms to expand the query
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 通过提出有价值的术语来提高搜索效率 |'
- en: '| 6 | Elevate search performance by recommending relevant expansion phrases
    for the query |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 通过推荐相关的扩展短语来提升搜索性能 |'
- en: '| 7 | Boost the search accuracy by providing helpful expansion terms to enrich
    the query |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 通过提供有帮助的扩展术语来提升搜索准确性 |'
- en: '| 8 | Increase the search efficacy by offering beneficial expansion keywords
    for the query |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 通过提供有益的扩展关键词来提高搜索效果 |'
- en: '| 9 | Optimize search results by suggesting meaningful expansion terms to enhance
    the query |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 通过建议有意义的扩展术语来优化搜索结果 |'
- en: '| 10 | Enhance search outcomes by recommending beneficial expansion terms to
    supplement the query |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 通过推荐有益的扩展术语来提高搜索结果 |'
- en: 'Figure 3: Reformulation instructions generated ($N$=10).'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：生成的重新表述指令（$N$=10）。
- en: We now describe the experiments and analysis performed for different retrieval
    settings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们描述在不同检索设置下进行的实验和分析。
- en: To instruct the LLM to generate query reformulations, we start with the instruction
    empirically chosen by Wang et al. [[29](#bib.bib29)] – as our base QR instruction
    $I_{1}$. These paraphrases serve as our instruction set for subsequent experiments.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了指导LLM生成查询重新表述，我们从Wang等人实证选择的指令[[29](#bib.bib29)]开始——作为我们的基础QR指令$I_{1}$。这些同义句作为我们后续实验的指令集。
- en: For generating the actual query reformulations, we employ flan-t5-xxl [[7](#bib.bib7)],
    an instruction-tuned model. The FlanT5 set of models is created by fine-tuning
    the text-to-text transformer model, T5 [[25](#bib.bib25)] on instruction data
    of a variety of NL tasks. We use the checkpoint²²2[https://huggingface.co/google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl)
    provided through HuggingFace’s Transformers library [[34](#bib.bib34)]. Nucleus
    sampling is performed with a cutoff probability of 0.92 keeping the top 200 tokens
    (top_k) and a repetition penalty of 1.2.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为生成实际的查询改写，我们使用了 flan-t5-xxl [[7](#bib.bib7)]，这是一个经过指令调优的模型。FlanT5 模型集是通过对各种自然语言任务的指令数据进行微调的文本到文本的转换器模型
    T5 [[25](#bib.bib25)] 创建的。我们使用了 HuggingFace 的 Transformers 库 [[34](#bib.bib34)]
    提供的 checkpoint²²2[https://huggingface.co/google/flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl)。采用了带有
    0.92 截止概率的核采样，保留前 200 个标记（top_k），重复惩罚为 1.2。
- en: 'For evaluation, we use four popular benchmarks through IRDataset [[17](#bib.bib17)]’s
    interface: 1)TP19: TREC 19 Passage Ranking which uses the MSMarco dataset [[21](#bib.bib21),
    [14](#bib.bib14)] consisting of search engine queries. 2)TR04: TREC Robust 2004
    Track, a task intended for testing poorly performing topics. In our experiments,
    we use the Title as our choice of query. And two tasks from the BEIR [[28](#bib.bib28)]
    benchmark 3)WT: Webis Touche [[4](#bib.bib4)] for argument retrieval 4)DE: DBPedia
    Entity Retrieval [[12](#bib.bib12)].'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估，我们通过 IRDataset [[17](#bib.bib17)] 的接口使用四个流行的基准：1)TP19：TREC 19 Passage Ranking，使用包含搜索引擎查询的
    MSMarco 数据集 [[21](#bib.bib21)，[14](#bib.bib14)]。2)TR04：TREC Robust 2004 Track，一个用于测试表现不佳话题的任务。在我们的实验中，我们使用标题作为查询的选择。以及
    BEIR [[28](#bib.bib28)] 基准中的两个任务 3)WT：Webis Touche [[4](#bib.bib4)] 用于论证检索 4)DE：DBPedia
    实体检索 [[12](#bib.bib12)]。
- en: '4.1 Baselines:'
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基线：
- en: We compare our work against the following using the Pyterrier [[18](#bib.bib18)]
    framework. For all the post-retrieval experiments, we use 5 documents as feedback.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Pyterrier [[18](#bib.bib18)] 框架下将我们的工作与以下内容进行比较。对于所有的检索后实验，我们使用 5 篇文档作为反馈。
- en: 'With BM25 Retriever:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 BM25 检索器：
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BM25: Here, we retrieve using raw queries without any reformulation'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BM25：在这里，我们使用未经任何改写的原始查询进行检索。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FlanQR [[29](#bib.bib29)]: We implement Wang et al’s single-instruction zero-shot
    QR [[29](#bib.bib29)] which is also a specific case of GenQREnsemble when N=1'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FlanQR [[29](#bib.bib29)]：我们实现了 Wang 等人的单指令零样本 QR [[29](#bib.bib29)]，这也是 GenQREnsemble
    当 N=1 时的一个特定情况。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BM25+RM3 [[1](#bib.bib1)]: BM25 retrieval with RM3 expanded queries (#feedback
    terms=10)'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BM25+RM3 [[1](#bib.bib1)]：BM25 检索，使用 RM3 扩展查询（#反馈项=10）
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BM25+FlanPRF [[29](#bib.bib29)]: BM25 retrieval with FlanPRF expanded queries'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BM25+FlanPRF [[29](#bib.bib29)]：BM25 检索，使用 FlanPRF 扩展查询。
- en: 'With Neural Reranking: Here, we re-evaluate the above settings in conjunction
    with a MonoT5 neural reranker [[24](#bib.bib24)] with all other parameters constant.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经重新排序：在这里，我们结合 MonoT5 神经重新排序器 [[24](#bib.bib24)] 重新评估上述设置，其余参数保持不变。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BM25+MonoT5: BM25 retrieval using raw queries, re-ranked with MonoT5 model [[24](#bib.bib24)]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BM25+MonoT5：使用原始查询进行 BM25 检索，重新排序使用 MonoT5 模型 [[24](#bib.bib24)]。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'FlanQR+MonoT5: BM25 retrieval with FlanQR reformulations, re-ranked with MonoT5
    model'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FlanQR+MonoT5：BM25 检索，使用 FlanQR 改写，重新排序使用 MonoT5 模型。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BM25+RM3+MonoT5: BM25 retrieval with RM3 expanded queries, re-ranked with MonoT5
    model'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BM25+RM3+MonoT5：BM25 检索，使用 RM3 扩展查询，重新排序使用 MonoT5 模型。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BM25+FlanPRF+MonoT5: BM25 retrieval with FlanPRF expanded queries, re-ranked
    with MonoT5 model'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BM25+FlanPRF+MonoT5：BM25 检索，使用 FlanPRF 扩展查询，重新排序使用 MonoT5 模型。
- en: 5 Results and Analysis
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果与分析
- en: We now report the results of query reformulation for pre- and post-retrieval
    settings.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在报告了查询改写在检索前后设置的结果。
- en: 5.1 Pre-Retrieval Performance
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 检索前表现
- en: '![Refer to caption](img/c672ba76b441fa26bd0f391ad8073a94.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/c672ba76b441fa26bd0f391ad8073a94.png)'
- en: 'Figure 4: nDCG@10 Scores of GenQREnsemble and FlanQR relative to BM25'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：GenQREnsemble 和 FlanQR 相对于 BM25 的 nDCG@10 分数。
- en: 'We first compare the retrieval performances of raw queries and reformulations
    from FlanQR, and GenQREnsemble in Table [1](#S5.T1 "Table 1 ‣ 5.1 Pre-Retrieval
    Performance ‣ 5 Results and Analysis ‣ GenQREnsemble: Zero-Shot LLM Ensemble Prompting
    for Generative Query Reformulation").GenQREnsemble outperforms the raw queries
    as well as generates better reformulations than FlanQR’s reformulated queries
    across all the four benchmarks over a BM25 retriever, indicating the usefulness
    of paraphrasing initial instructions. On TP19, nDCG@10 and MAP improve significantly
    with relative improvements of 18% and 24% respectively. This is further validated
    through the querywise analysis shown in Figure [4](#S5.F4 "Figure 4 ‣ 5.1 Pre-Retrieval
    Performance ‣ 5 Results and Analysis ‣ GenQREnsemble: Zero-Shot LLM Ensemble Prompting
    for Generative Query Reformulation") – Relative to BM25, nDCG@10 scores of GenQREnsemble
    (shown in green) are overall better than FlanQR (shown in blue).GenQREnsemble
    seems more robust too as it avoids drastic degradation in at least 6 queries on
    which FlanQR fails.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先比较了原始查询和 FlanQR 以及 GenQREnsemble 重新表述的检索表现，结果见表 [1](#S5.T1 "表 1 ‣ 5.1 预检索性能
    ‣ 5 结果与分析 ‣ GenQREnsemble: 零样本 LLM 集成提示生成查询重述")。GenQREnsemble 超越了原始查询，并且在四个基准测试中生成的重述比
    FlanQR 的重述更好，表明初始指令的释义是有用的。在 TP19 上，nDCG@10 和 MAP 分别有 18% 和 24% 的显著提升。通过图 [4](#S5.F4
    "图 4 ‣ 5.1 预检索性能 ‣ 5 结果与分析 ‣ GenQREnsemble: 零样本 LLM 集成提示生成查询重述") 中的逐查询分析进一步验证了这一点——相对于
    BM25，GenQREnsemble 的 nDCG@10 分数（以绿色显示）总体上优于 FlanQR（以蓝色显示）。GenQREnsemble 似乎也更具鲁棒性，因为它避免了
    FlanQR 在至少 6 个查询上出现的严重退化。'
- en: 'Table 1: Performance of GenQREnsemble on the four benchmarks. $\alpha$) over
    FlanQR. +% indicates % improvements relative to FlanQR (as whole numbers).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: GenQREnsemble 在四个基准测试中的表现。$\alpha$) 相对于 FlanQR。+% 表示相对于 FlanQR 的百分比提升（以整数表示）。'
- en: '| Evaluation Set | TREC Passage 19 | TREC Robust 04 | Webis Touche | DBpedia
    Entity |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 评估集 | TREC Passage 19 | TREC Robust 04 | Webis Touche | DBpedia Entity |'
- en: '| Setting | nDCG@10 | MAP | MRR | P@10 | nDCG@10 | MRR | nDCG@10 | MAP | MRR
    | nDCG@10 | MAP | MRR |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | nDCG@10 | MAP | MRR | P@10 | nDCG@10 | MRR | nDCG@10 | MAP | MRR | nDCG@10
    | MAP | MRR |'
- en: '| BM25 | .480 | .286 | .642 | .426 | .434 | .154 | .260 | .206 | .454 | .321
    | .168 | .297 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | .480 | .286 | .642 | .426 | .434 | .154 | .260 | .206 | .454 | .321
    | .168 | .297 |'
- en: '| FlanQR | .477 | .302 | .593 | .473 | .483 | .151 | .315 | .241 | .511 | .342
    | .196 | .345 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| FlanQR | .477 | .302 | .593 | .473 | .483 | .151 | .315 | .241 | .511 | .342
    | .196 | .345 |'
- en: '| FlanQR[β=.05] | .511 | .323 | .621 | .469 | .477 | .150 | .276 | .221 | .476
    | .353 | .188 | .339 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| FlanQR[β=.05] | .511 | .323 | .621 | .469 | .477 | .150 | .276 | .221 | .476
    | .353 | .188 | .339 |'
- en: '| GenQREnsemble | .564^α+18% | .375^α+24% | .706+19% | .500^α+6% | .513^α+6%
    | .159+6% | .317+1% | .257+6% | .555+9% | .374^α+9% | .212^α+8% | .376^α+9% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| GenQREnsemble | .564^α+18% | .375^α+24% | .706+19% | .500^α+6% | .513^α+6%
    | .159+6% | .317+1% | .257+6% | .555+9% | .374^α+9% | .212^α+8% | .376^α+9% |'
- en: '| GenQREnsemble[β=.05] | .575^α | .377^α | .714 | .502^α | .512^α | .159 |
    .292 | .242 | .489 | .377^α | .212^α | .380^α |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GenQREnsemble[β=.05] | .575^α | .377^α | .714 | .502^α | .512^α | .159 |
    .292 | .242 | .489 | .377^α | .212^α | .380^α |'
- en: '| BM25+MonoT5 | .718 | .477 | .881 | .492 | .513 | .173 | .299 | .216 | .525
    | .414 | .249 | .444 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| BM25+MonoT5 | .718 | .477 | .881 | .492 | .513 | .173 | .299 | .216 | .525
    | .414 | .249 | .444 |'
- en: '| FlanQR+MonoT5 | .707 | .486 | .847 | .490 | .510 | .170 | .292 | .215 | .530
    | .415 | .255 | .446 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| FlanQR+MonoT5 | .707 | .486 | .847 | .490 | .510 | .170 | .292 | .215 | .530
    | .415 | .255 | .446 |'
- en: '| GenQREnsemble+MonoT5 | .722+2% | .503+3% | .862+2% | .484-1% | .506-1% |
    .170 | .298+3% | .219+2% | .548+3% | .420+1% | .258+1% | .450+1% |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| GenQREnsemble+MonoT5 | .722+2% | .503+3% | .862+2% | .484-1% | .506-1% |
    .170 | .298+3% | .219+2% | .548+3% | .420+1% | .258+1% | .450+1% |'
- en: 'We further look at GenQREnsemble under the neural reranker setting shown at
    the bottom half of Table [1](#S5.T1 "Table 1 ‣ 5.1 Pre-Retrieval Performance ‣
    5 Results and Analysis ‣ GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative
    Query Reformulation"). In three of the four settings, viz., TP19, WT, and DE,
    GenQREnsemble is preferable to its zero-shot variant, FlanQR. Evidently, the gains
    of both the zero-shot approaches in the traditional setting are stronger vis-à-vis
    the neural setting. We hypothesize this could be due to GenQREnsemble and FlanQR
    both expanding the query via incorporating semantically similar but lexically
    different keywords. Comparatively, neural models are adept at capturing notions
    of semantic similarity and might benefit less with query expansion. This also
    is in line with Weller et al.’s [[32](#bib.bib32)] recent analysis on the non-ensemble
    variant.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步考察了表 [1](#S5.T1 "Table 1 ‣ 5.1 Pre-Retrieval Performance ‣ 5 Results and
    Analysis ‣ GenQREnsemble: Zero-Shot LLM Ensemble Prompting for Generative Query
    Reformulation") 下半部分所示的神经重新排序器设置中的 GenQREnsemble。在四种设置中的三种，即 TP19、WT 和 DE，GenQREnsemble
    优于其零-shot 变体 FlanQR。显然，在传统设置下，两种零-shot 方法的增益强于神经设置。我们推测这可能是因为 GenQREnsemble 和
    FlanQR 都通过引入语义相似但词汇不同的关键词来扩展查询。相比之下，神经模型擅长捕捉语义相似性，因此可能在查询扩展方面受益较少。这也与 Weller 等人的[[32](#bib.bib32)]
    最近对非集成变体的分析一致。'
- en: 5.2 Post-Retrieval Performance
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 检索后性能
- en: 'Table 2: Comparison of PRF performance on the TREC 19 Passage Ranking Task'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: TREC 19 Passage Ranking 任务中 PRF 性能的比较'
- en: '|  | With BM25 Retriever | With Neural Reranking |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 使用 BM25 检索器 | 使用神经重新排序 |'
- en: '| Setting | nDCG@10 | nDCG@20 | MAP | MRR | nDCG@10 | nDCG@20 | MAP | MRR |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | nDCG@10 | nDCG@20 | MAP | MRR | nDCG@10 | nDCG@20 | MAP | MRR |'
- en: '| BM25 | .480 | .473 | .286 | .642 | .718 | .696 | .477 | .881 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| BM25 | .480 | .473 | .286 | .642 | .718 | .696 | .477 | .881 |'
- en: '| RM3 | .504 | .496 | .311 | .595 | .716 | .699 | .480 | .858 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| RM3 | .504 | .496 | .311 | .595 | .716 | .699 | .480 | .858 |'
- en: '| FlanPRF | .576 | .553 | .363 | .715 | .722 | .703 | .486 | .874 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| FlanPRF | .576 | .553 | .363 | .715 | .722 | .703 | .486 | .874 |'
- en: '| GenQREnsembleRF | .585+2% | .560+1% | .373+3% | .753+5% | .729+1% | .706+1%
    | .501+3% | .894+2% |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| GenQREnsembleRF | .585+2% | .560+1% | .373+3% | .753+5% | .729+1% | .706+1%
    | .501+3% | .894+2% |'
- en: '| FlanPRF (Oracle) | .753 | .728 | .501 | .936 | .742 | .734 | .545 | .881
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| FlanPRF (Oracle) | .753 | .728 | .501 | .936 | .742 | .734 | .545 | .881
    |'
- en: '| GenQREnsembleRF (Oracle) | .820^α+9% | .773+6% | .545+9% | .977+4% | .756+2%
    | .751+2% | .545 | .897+2% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GenQREnsembleRF (Oracle) | .820^α+9% | .773+6% | .545+9% | .977+4% | .756+2%
    | .751+2% | .545 | .897+2% |'
- en: 'We now investigate if GenQREnsembleRF can effectively incorporate PRF in Table [2](#S5.T2
    "Table 2 ‣ 5.2 Post-Retrieval Performance ‣ 5 Results and Analysis ‣ GenQREnsemble:
    Zero-Shot LLM Ensemble Prompting for Generative Query Reformulation"). We find
    that GenQREnsembleRF improves retrieval performance as compared to other PRF approaches
    and is able to incorporate feedback from a BM25 retriever better than RM3 as well
    as its zero-shot counterpart. To assess if GenQREnsembleRF and FlanPRF can at
    all benefit from incorporating relevant documents, we perform oracle testing by
    providing the highest relevant gold documents as context. We find that GenQREnsembleRF
    is able to improve over GenQREnsemble (without feedback) showing that it is able
    to capture context effectively as well as benefit from it. Further, it can incorporate
    relevant feedback better than its single-instruction counterpart FlanPRF. We notice
    improvements even under the neural reranker setting as GenQREnsembleRF outperforms
    RM3 and FlanPRF. Besides, the oracle improvements are higher with only a BM25
    retriever as compared to when a neural reranker is introduced.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '我们现在研究 GenQREnsembleRF 是否可以有效地结合表 [2](#S5.T2 "Table 2 ‣ 5.2 Post-Retrieval
    Performance ‣ 5 Results and Analysis ‣ GenQREnsemble: Zero-Shot LLM Ensemble Prompting
    for Generative Query Reformulation") 中的 PRF。我们发现，与其他 PRF 方法相比，GenQREnsembleRF
    改善了检索性能，并且能够比 RM3 及其零-shot 对应物更好地结合来自 BM25 检索器的反馈。为了评估 GenQREnsembleRF 和 FlanPRF
    是否能从相关文档中获益，我们通过提供最相关的黄金文档作为上下文来进行 oracle 测试。我们发现，GenQREnsembleRF 能够比 GenQREnsemble（没有反馈）有所提升，表明它能够有效捕捉上下文并从中受益。此外，它比其单指令对手
    FlanPRF 更好地结合了相关反馈。我们注意到，即使在神经重新排序器设置下，GenQREnsembleRF 也表现优于 RM3 和 FlanPRF。此外，仅使用
    BM25 检索器时的 oracle 改进比引入神经重新排序器时更高。'
- en: '![Refer to caption](img/2cb66fc4b5e1c5635de275211f665847.png)![Refer to caption](img/c7b952e91fed0614daf1507c00f03228.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2cb66fc4b5e1c5635de275211f665847.png)![参见说明](img/c7b952e91fed0614daf1507c00f03228.png)'
- en: 'Figure 5: Effect of feedback documents under sparse (BM25) and neural (MonoT5)
    rankers'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 在稀疏（BM25）和神经（MonoT5）排名器下反馈文档的影响'
- en: We further evaluate the effect of varying the number of feedback documents from
    0 to 5\. We notice that resorting to an ensemble approach is highly beneficial.
    In the BM25 setting, the ensemble approach seems always preferable. Under the
    neural reranker setting too,GenQREnsembleRF almost always outperforms FlanPRF.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步评估了反馈文档数量从0到5的变化效果。我们注意到，采用集成方法非常有益。在BM25设置下，集成方法似乎总是更可取的。在神经重排序设置下，GenQREnsembleRF几乎总是优于FlanPRF。
- en: 6 Conclusions
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Zero-shot QR is advantageous since it does not rely on any labeled relevance
    judgements and allows eliciting pre-trained knowledge in the form of keywords
    by prompting the model with the original query and appropriate instruction. By
    introducing GenQREnsemble, we show that zero-shot performance can be further enhanced
    by using multiple views of the initial instruction. We also show that the extension
    GenQREnsembleRF is able to effectively incorporate relevance feedback, either
    automated or from users. While generative QR greatly benefits from our ensemble
    approach, the proposed methods come at a cost of potentially increased latency,
    but this is becoming less problematic with the increased availability of batch
    inference for LLMs. The proposed ensemble approach could also be applied to other
    settings, for example, to address different aspects of queries or metrics to optimize,
    or to better control the generated reformulations.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot QR具有优势，因为它不依赖于任何标记的相关性判断，并允许通过用原始查询和适当的指令提示模型来引出预训练的知识。通过引入GenQREnsemble，我们展示了通过使用初始指令的多个视角，零-shot性能可以进一步提升。我们还展示了扩展的GenQREnsembleRF能够有效地结合相关反馈，无论是自动化的还是来自用户的。尽管生成式QR大大受益于我们的集成方法，但所提出的方法可能会导致潜在的延迟增加，但随着LLMs批量推理的可用性增加，这种问题变得越来越不明显。所提出的集成方法还可以应用于其他设置，例如，解决查询的不同方面或优化指标，或更好地控制生成的重组。
- en: References
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Abdul-Jaleel, N., Allan, J., Croft, W.B., Diaz, F., Larkey, L., Li, X.,
    Smucker, M.D., Wade, C.: Umass at trec 2004: Novelty and hard. Computer Science
    Department Faculty Publication Series p. 189 (2004)'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Abdul-Jaleel, N., Allan, J., Croft, W.B., Diaz, F., Larkey, L., Li, X.,
    Smucker, M.D., Wade, C.: Umass在TREC 2004：新颖性与难度。计算机科学系教员出版系列第189页（2004）'
- en: '[2] Alaofi, M., Gallagher, L., Sanderson, M., Scholer, F., Thomas, P.: Can
    generative llms create query variants for test collections? an exploratory study.
    In: Proceedings of the 46th International ACM SIGIR Conference on Research and
    Development in Information Retrieval (SIGIR ’23). pp. 1869–1873\. Association
    for Computing Machinery, New York, NY, USA (2023). https://doi.org/10.1145/3539618.3591960'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Alaofi, M., Gallagher, L., Sanderson, M., Scholer, F., Thomas, P.: 生成式语言模型能否为测试集合创建查询变体？一项探索性研究。见：第46届国际ACM
    SIGIR信息检索研究与开发会议（SIGIR ’23）论文集。第1869–1873页。计算机机械学会，美国纽约（2023）。 https://doi.org/10.1145/3539618.3591960'
- en: '[3] Arora, S., Narayan, A., Chen, M.F., Orr, L., Guha, N., Bhatia, K., Chami,
    I., Re, C.: Ask me anything: A simple strategy for prompting language models.
    In: The Eleventh International Conference on Learning Representations (2022)'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Arora, S., Narayan, A., Chen, M.F., Orr, L., Guha, N., Bhatia, K., Chami,
    I., Re, C.: 问我任何事：一种简单的语言模型提示策略。见：第十一届国际学习表征会议（2022）'
- en: '[4] Bondarenko, A., Fröbe, M., Beloucif, M., Gienapp, L., Ajjour, Y., Panchenko,
    A., Biemann, C., Stein, B., Wachsmuth, H., Potthast, M., et al.: Overview of touché
    2020: argument retrieval. In: Experimental IR Meets Multilinguality, Multimodality,
    and Interaction: 11th International Conference of the CLEF Association, CLEF 2020,
    Thessaloniki, Greece, September 22–25, 2020, Proceedings 11\. pp. 384–395\. Springer
    (2020)'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Bondarenko, A., Fröbe, M., Beloucif, M., Gienapp, L., Ajjour, Y., Panchenko,
    A., Biemann, C., Stein, B., Wachsmuth, H., Potthast, M., 等：Touché 2020概述：论据检索。见：实验信息检索遇上多语言、多模态和交互：CLEF协会第11届国际会议，CLEF
    2020，希腊塞萨洛尼基，2020年9月22–25日，论文集第11卷。第384–395页。Springer（2020）'
- en: '[5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are
    few-shot learners. Advances in neural information processing systems 33, 1877–1901
    (2020)'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., 等：语言模型是少样本学习者。神经信息处理系统进展 33,
    1877–1901 (2020)'
- en: '[6] Carpineto, C., Romano, G.: A survey of automatic query expansion in information
    retrieval. Acm Computing Surveys (CSUR) 44(1), 1–50 (2012)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Carpineto, C., Romano, G.: 自动查询扩展在信息检索中的调查。Acm Computing Surveys (CSUR)
    44(1), 1–50 (2012)'
- en: '[7] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y.,
    Wang, X., Dehghani, M., Brahma, S., et al.: Scaling instruction-finetuned language
    models. arXiv preprint arXiv:2210.11416 (2022)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Chung, H.W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y.,
    Wang, X., Dehghani, M., Brahma, S., 等：扩展指令微调语言模型。arXiv 预印本 arXiv:2210.11416（2022年）'
- en: '[8] Dhole, K., Gangal, V., Gehrmann, S., Gupta, A., Li, Z., Mahamood, S., Mahadiran,
    A., Mille, S., Shrivastava, A., Tan, S., et al.: Nl-augmenter: A framework for
    task-sensitive natural language augmentation. Northern European Journal of Language
    Technology 9(1) (2023)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Dhole, K., Gangal, V., Gehrmann, S., Gupta, A., Li, Z., Mahamood, S., Mahadiran,
    A., Mille, S., Shrivastava, A., Tan, S., 等：Nl-augmenter：一个任务敏感的自然语言增强框架。《北欧语言技术期刊》9(1)（2023年）'
- en: '[9] Dhuliawala, S.Z., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz,
    A., Weston, J.E.: Chain-of-verification reduces hallucination in large language
    models (2023)'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Dhuliawala, S.Z., Komeili, M., Xu, J., Raileanu, R., Li, X., Celikyilmaz,
    A., Weston, J.E.：链式验证减少了大型语言模型中的幻觉（2023年）'
- en: '[10] Gao, J., Xie, S., He, X., Ali, A.: Learning lexicon models from search
    logs for query expansion. In: Tsujii, J., Henderson, J., Paşca, M. (eds.) Proceedings
    of the 2012 Joint Conference on Empirical Methods in Natural Language Processing
    and Computational Natural Language Learning. pp. 666–676\. Association for Computational
    Linguistics, Jeju Island, Korea (Jul 2012), [https://aclanthology.org/D12-1061](https://aclanthology.org/D12-1061)'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Gao, J., Xie, S., He, X., Ali, A.：从搜索日志中学习词汇模型以进行查询扩展。载于：Tsujii, J., Henderson,
    J., Paşca, M.（编辑）《2012年自然语言处理与计算自然语言学习联合会议论文集》。第 666–676 页。计算语言学协会，韩国济州岛（2012年7月），
    [https://aclanthology.org/D12-1061](https://aclanthology.org/D12-1061)'
- en: '[11] Gao, L., Ma, X., Lin, J., Callan, J.: Precise zero-shot dense retrieval
    without relevance labels. In: Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers). pp. 1762–1777 (2023)'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Gao, L., Ma, X., Lin, J., Callan, J.：没有相关标签的精确零-shot 密集检索。载于：第61届计算语言学协会年会（第一卷：长篇论文）论文集。第
    1762–1777 页（2023年）'
- en: '[12] Hasibi, F., Nikolaev, F., Xiong, C., Balog, K., Bratsberg, S.E., Kotov,
    A., Callan, J.: Dbpedia-entity v2: a test collection for entity search. In: Proceedings
    of the 40th International ACM SIGIR Conference on Research and Development in
    Information Retrieval. pp. 1265–1268 (2017)'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Hasibi, F., Nikolaev, F., Xiong, C., Balog, K., Bratsberg, S.E., Kotov,
    A., Callan, J.：《Dbpedia-entity v2：一个实体检索测试集合》。载于：第40届国际 ACM SIGIR 信息检索研究与发展会议论文集。第
    1265–1268 页（2017年）'
- en: '[13] Hsu, F.D., Taksa, I.: Comparing rank and score combination methods for
    data fusion in information retrieval. Information Retrieval 8(3), 449–480 (2005)'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Hsu, F.D., Taksa, I.：比较信息检索中的排名和评分组合方法。信息检索 8(3)，449–480（2005年）'
- en: '[14] Jagerman, R., Zhuang, H., Qin, Z., Wang, X., Bendersky, M.: Query expansion
    by prompting large language models. arXiv preprint arXiv:2305.03653 (2023)'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Jagerman, R., Zhuang, H., Qin, Z., Wang, X., Bendersky, M.：通过提示大型语言模型进行查询扩展。arXiv
    预印本 arXiv:2305.03653（2023年）'
- en: '[15] Li, H., Mourad, A., Zhuang, S., Koopman, B., Zuccon, G.: Pseudo relevance
    feedback with deep language models and dense retrievers: Successes and pitfalls.
    ACM Transactions on Information Systems 41(3), 1–40 (2023)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Li, H., Mourad, A., Zhuang, S., Koopman, B., Zuccon, G.：利用深度语言模型和密集检索器的伪相关反馈：成功与陷阱。《ACM
    信息系统期刊》41(3)，1–40（2023年）'
- en: '[16] Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.G., Chen, W.: Making
    language models better reasoners with step-aware verifier. In: Proceedings of
    the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers). pp. 5315–5333 (2023)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.G., Chen, W.：通过步骤感知验证器使语言模型成为更好的推理者。载于：第61届计算语言学协会年会（第一卷：长篇论文）论文集。第
    5315–5333 页（2023年）'
- en: '[17] MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., Goharian,
    N.: Simplified data wrangling with ir_datasets. In: Proceedings of the 44th International
    ACM SIGIR Conference on Research and Development in Information Retrieval. pp.
    2429–2436 (2021)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] MacAvaney, S., Yates, A., Feldman, S., Downey, D., Cohan, A., Goharian,
    N.：《使用 ir_datasets 简化数据整理》。载于：第44届国际 ACM SIGIR 信息检索研究与发展会议论文集。第 2429–2436 页（2021年）'
- en: '[18] Macdonald, C., Tonellotto, N., MacAvaney, S., Ounis, I.: Pyterrier: Declarative
    experimentation in python from bm25 to dense retrieval. In: Proceedings of the
    30th acm international conference on information & knowledge management. pp. 4526–4533
    (2021)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Macdonald, C., Tonellotto, N., MacAvaney, S., Ounis, I.：《Pyterrier：从 bm25
    到密集检索的 Python 声明性实验》。载于：第30届 ACM 国际信息与知识管理会议论文集。第 4526–4533 页（2021年）'
- en: '[19] Mo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., Nie, J.Y.: Convgqr: Generative
    query reformulation for conversational search. In: Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
    pp. 4998–5012 (2023)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Mo, F., Mao, K., Zhu, Y., Wu, Y., Huang, K., Nie, J.Y.：《Convgqr：用于对话搜索的生成式查询改写》。发表于：第61届计算语言学协会年会论文集（第1卷：长篇论文）。页码：4998–5012（2023）'
- en: '[20] Mohankumar, A.K., Begwani, N., Singh, A.: Diversity driven query rewriting
    in search advertising. In: Proceedings of the 27th ACM SIGKDD Conference on Knowledge
    Discovery & Data Mining. pp. 3423–3431 (2021)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Mohankumar, A.K., Begwani, N., Singh, A.：《以多样性驱动的搜索广告查询重写》。发表于：第27届ACM
    SIGKDD知识发现与数据挖掘会议论文集。页码：3423–3431（2021）'
- en: '[21] Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R.,
    Deng, L.: Ms marco: A human-generated machine reading comprehension dataset (2016)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Nguyen, T., Rosenberg, M., Song, X., Gao, J., Tiwary, S., Majumder, R.,
    Deng, L.：《Ms Marco：一个人工生成的机器阅读理解数据集》（2016）'
- en: '[22] Nogueira, R., Lin, J., Epistemic, A.: From doc2query to doctttttquery.
    Online preprint 6(2) (2019)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Nogueira, R., Lin, J., Epistemic, A.：《从doc2query到doctttttquery》。在线预印本
    6(2)（2019）'
- en: '[23] Peng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction tuning with
    gpt-4\. arXiv preprint arXiv:2304.03277 (2023)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Peng, B., Li, C., He, P., Galley, M., Gao, J.：《使用GPT-4进行指令微调》。arXiv预印本
    arXiv:2304.03277（2023）'
- en: '[24] Pradeep, R., Nogueira, R., Lin, J.: The expando-mono-duo design pattern
    for text ranking with pretrained sequence-to-sequence models. arXiv preprint arXiv:2101.05667
    (2021)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Pradeep, R., Nogueira, R., Lin, J.：《使用预训练序列到序列模型的文本排名的扩展-单一-双重设计模式》。arXiv预印本
    arXiv:2101.05667（2021）'
- en: '[25] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,
    Zhou, Y., Li, W., Liu, P.J.: Exploring the limits of transfer learning with a
    unified text-to-text transformer. Journal of machine learning research 21(140),
    1–67 (2020)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,
    Zhou, Y., Li, W., Liu, P.J.：《利用统一的文本到文本变换器探索迁移学习的极限》。 《机器学习研究杂志》21(140)，1–67（2020）'
- en: '[26] Si, L., Lu, J., Callan, J.: Combining multiple resources, evidences and
    criteria for genomic information retrieval. In: TREC (November 2006)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Si, L., Lu, J., Callan, J.：《结合多种资源、证据和标准进行基因组信息检索》。发表于：TREC（2006年11月）'
- en: '[27] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch,
    A., Brown, A.R., Santoro, A., Gupta, A., Garriga-Alonso, A., et al.: Beyond the
    imitation game: Quantifying and extrapolating the capabilities of language models.
    Transactions on Machine Learning Research (2023)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A.A.M., Abid, A., Fisch,
    A., Brown, A.R., Santoro, A., Gupta, A., Garriga-Alonso, A., 等：《超越模仿游戏：量化和推断语言模型的能力》。《机器学习研究交易》（2023）'
- en: '[28] Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., Gurevych, I.: Beir:
    A heterogeneous benchmark for zero-shot evaluation of information retrieval models.
    In: Thirty-fifth Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track (Round 2) (2021)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Thakur, N., Reimers, N., Rücklé, A., Srivastava, A., Gurevych, I.：《Beir：用于零样本评估信息检索模型的异构基准》。发表于：第三十五届神经信息处理系统会议数据集与基准追踪（第2轮）（2021）'
- en: '[29] Wang, X., MacAvaney, S., Macdonald, C., Ounis, I.: Generative query reformulation
    for effective adhoc search. arXiv preprint arXiv:2308.00415 (2023)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Wang, X., MacAvaney, S., Macdonald, C., Ounis, I.：《生成式查询改写以实现有效的即席搜索》。arXiv预印本
    arXiv:2308.00415（2023）'
- en: '[30] Wang, X., Macdonald, C., Tonellotto, N., Ounis, I.: Colbert-prf: Semantic
    pseudo-relevance feedback for dense passage and document retrieval. ACM Transactions
    on the Web 17(1), 1–39 (2023)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Wang, X., Macdonald, C., Tonellotto, N., Ounis, I.：《Colbert-prf：用于密集段落和文档检索的语义伪相关反馈》。
    《ACM网络事务》17(1)，1–39（2023）'
- en: '[31] Wang, X., Wei, J., Schuurmans, D., Le, Q.V., Chi, E.H., Narang, S., Chowdhery,
    A., Zhou, D.: Self-consistency improves chain of thought reasoning in language
    models. In: The Eleventh International Conference on Learning Representations
    (2022)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Wang, X., Wei, J., Schuurmans, D., Le, Q.V., Chi, E.H., Narang, S., Chowdhery,
    A., Zhou, D.：《自一致性提高了语言模型中的思维链推理》。发表于：第十一届国际学习表征会议（2022）'
- en: '[32] Weller, O., Lo, K., Wadden, D., Lawrie, D., Van Durme, B., Cohan, A.,
    Soldaini, L.: When do generative query and document expansions fail? a comprehensive
    study across methods, retrievers, and datasets. In: Graham, Y., Purver, M. (eds.)
    Findings of the Association for Computational Linguistics: EACL 2024\. pp. 1987–2003\.
    Association for Computational Linguistics, St. Julian’s, Malta (Mar 2024), [https://aclanthology.org/2024.findings-eacl.134](https://aclanthology.org/2024.findings-eacl.134)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Weller, O., Lo, K., Wadden, D., Lawrie, D., Van Durme, B., Cohan, A.,
    Soldaini, L.：生成查询和文档扩展何时失败？方法、检索器和数据集的全面研究。在：Graham, Y., Purver, M.（编）《计算语言学学会发现：EACL
    2024》。第1987–2003页。计算语言学学会，马耳他圣朱利安（2024年3月），[https://aclanthology.org/2024.findings-eacl.134](https://aclanthology.org/2024.findings-eacl.134)'
- en: '[33] Wiher, G., Meister, C., Cotterell, R.: On decoding strategies for neural
    text generators. Transactions of the Association for Computational Linguistics
    10, 997–1012 (2022)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Wiher, G., Meister, C., Cotterell, R.：关于神经文本生成器的解码策略。《计算语言学学会会刊》10, 997–1012（2022）'
- en: '[34] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac,
    P., Rault, T., Louf, R., Funtowicz, M., et al.: Transformers: State-of-the-art
    natural language processing. In: Proceedings of the 2020 conference on empirical
    methods in natural language processing: system demonstrations. pp. 38–45 (2020)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac,
    P., Rault, T., Louf, R., Funtowicz, M., 等：Transformers：最先进的自然语言处理。在：2020年自然语言处理经验方法会议论文集：系统演示。第38–45页（2020）'
- en: '[35] Yu, H., Xiong, C., Callan, J.: Improving query representations for dense
    retrieval with pseudo relevance feedback. In: Proceedings of the 30th ACM International
    Conference on Information & Knowledge Management. pp. 3592–3596 (2021)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Yu, H., Xiong, C., Callan, J.：通过伪相关反馈提高密集检索的查询表示。在：第30届ACM国际信息与知识管理大会论文集。第3592–3596页（2021）'
- en: '[36] Zhao, Z., Wallace, E., Feng, S., Klein, D., Singh, S.: Calibrate before
    use: Improving few-shot performance of language models. In: International conference
    on machine learning. pp. 12697–12706\. PMLR (2021)'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Zhao, Z., Wallace, E., Feng, S., Klein, D., Singh, S.：使用前校准：提高语言模型的少样本性能。在：国际机器学习会议。第12697–12706页。PMLR（2021）'
