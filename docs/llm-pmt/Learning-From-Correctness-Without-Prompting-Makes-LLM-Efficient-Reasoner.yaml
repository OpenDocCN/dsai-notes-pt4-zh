- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Learning From Correctness Without Prompting Makes LLM Efficient Reasoner
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.19094](https://ar5iv.labs.arxiv.org/html/2403.19094)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yuxuan Yao^(1,∗) Han Wu^(2,∗) Zhijiang Guo${}^{2,^{\dagger}}$
  prefs: []
  type: TYPE_NORMAL
- en: ¹Department of Computer Science City University of Hong Kong
  prefs: []
  type: TYPE_NORMAL
- en: ²Huawei Noah’s Ark Lab
  prefs: []
  type: TYPE_NORMAL
- en: yuxuanyao3-c@my.cityu.edu.hk
  prefs: []
  type: TYPE_NORMAL
- en: wu.han1, guozhijiang@huawei.com
  prefs: []
  type: TYPE_NORMAL
- en: linqi.song@cityu.edu.hk
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) have demonstrated outstanding performance across
    various tasks, yet they still exhibit limitations such as hallucination, unfaithful
    reasoning, and toxic content. One potential approach to mitigate these issues
    is learning from human or external feedback (e.g. tools). In this paper, we introduce
    an intrinsic self-correct reasoning framework for LLMs that eliminates the need
    for human feedback, external tools, and handcraft prompts. The proposed framework,
    based on a multi-step reasoning paradigm Learning from Correctness (LeCo), improves
    reasoning performance without needing to learn from errors. This paradigm prioritizes
    learning from correct reasoning steps, and a unique method to measure confidence
    for each reasoning step based on generation logits. Experimental results across
    various multi-step reasoning tasks demonstrate the effectiveness of the framework
    in improving reasoning performance with reduced token consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '^†^†footnotetext: ^∗Equal Contribution.'
  prefs: []
  type: TYPE_NORMAL
- en: ^†Corresponding Authors.
  prefs: []
  type: TYPE_NORMAL
- en: Large language models (LLMs) (OpenAI, [2023](#bib.bib34); Touvron et al., [2023](#bib.bib45))
    have exhibited remarkable performance on a diverse range of natural language processing
    benchmarks (Hendrycks et al., [2021a](#bib.bib17); Srivastava et al., [2022](#bib.bib43))
    and also showcased promising results on real-world applications (Wu et al., [2023](#bib.bib50);
    Thirunavukarasu et al., [2023](#bib.bib44)). However, it is imperative to acknowledge
    that LLMs still possess certain limitations. For instance, the occurrence of undesirable
    behaviors like hallucinations (Rawte et al., [2023](#bib.bib39)), generating harmful
    content (Bai et al., [2022](#bib.bib3)), and non-adherence to established rules
    and constraints (Ouyang et al., [2022](#bib.bib35); Peng et al., [2023](#bib.bib38))
    remains largely unexplored.
  prefs: []
  type: TYPE_NORMAL
- en: One extensively employed approach to address these problems is learning from
    feedback (Pan et al., [2023](#bib.bib36)). It involves guiding LLMs to improve
    their responses through a cycle of trial, examination, and correction. During
    the examination phase, feedback is provided to identify the shortcomings in the
    trial answer and guide the necessary corrections. Huang et al. ([2023a](#bib.bib19));
    Gou et al. ([2023a](#bib.bib13)) have confirmed high-quality feedback can offer
    valuable insights into further corrections. Although human feedback (Ouyang et al.,
    [2022](#bib.bib35); Fernandes et al., [2023](#bib.bib9)) and external tools feedback
    (Gou et al., [2023a](#bib.bib13); [b](#bib.bib14)) are generally valuable, they
    are either expensive to collect or heavily dependent on the abilities of the selected
    tools. To eliminate external intervention, another popular line of research is
    self-correction, where the model progressively learns from the feedback it generates
    internally, without relying on external sources (An et al., [2023](#bib.bib2)).
    However, Huang et al. ([2023b](#bib.bib20)) recently suggests that LLMs do not
    possess the inherent capabilities to find the errors and rectify their responses
    just by designing the prompts. More frustratingly, these methods often require
    creating extensive and elaborate handcraft prompts to guide the model in acquiring
    and understanding the feedback, which is a time-consuming and labor-intensive
    process, finally tuning our researchers into “prompt engineers”.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we present a novel intrinsic self-correct reasoning framework
    that eliminates the need for human feedback, external tools, and handcraft prompts.
    Different from the existing self-correction methods, which are predominantly based
    on learning from errors (An et al., [2023](#bib.bib2); Gou et al., [2023a](#bib.bib13)),
    we propose a new multi-step reasoning paradigm known as Learning from Correctness
    (LeCo). As illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Learning
    From Correctness Without Prompting Makes LLM Efficient Reasoner"), we begin by
    assigning a confidence score to each reasoning step in the first-round reasoning
    path. The step with the lowest confidence score will be identified as the earliest
    potential error step, and the steps before this point are considered to be “correct”.
    Then, the correct steps, considered as “correctness”, are appended to the input,
    and repeat the reasoning process. While the insight of learning from errors comes
    from the learning process of human students, the motivation behind our method
    is derived from progressive learning (Wu et al., [2019](#bib.bib51); Fayek et al.,
    [2020](#bib.bib8)), where correct reasoning steps are gradually accumulated to
    ultimately approach the correct answer. Furthermore, we also introduce an efficient
    method to measure the confidence for each reasoning step based on the generation
    logits, without the need for additional tokens or external tools. Specifically,
    we jointly consider the average confidence of each token within a step, the confidence
    divergence of a step, and the probability of step transmission to calculate the
    overall step confidence. We surprisingly find our method can identify almost 65%
    incorrect steps. We conduct experiments with both closed-source models (e.g. GPT-3.5
    and GPT-4) and open-source models (e.g. DeepSeek; Shao et al. [2024](#bib.bib41))
    on various multi-step reasoning tasks, including arithmetic reasoning, commonsense
    reasoning, and logical reasoning, show that our framework can significantly improve
    reasoning performance with less token consumption.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ed752b080def0202e6bfda8d474ec4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The framework of LeCo. LeCo first obtains an initial solution for
    the input problem. Then, we progressively collect the correct steps from the latest
    solution until the final answer is obtained.'
  prefs: []
  type: TYPE_NORMAL
- en: Our primary contributions include 1) we propose a novel multi-step reasoning
    paradigm learning from correctness, dubbed as LeCo, which progressively accumulates
    the correct steps and approaches the final answer; 2) we challenge the conventional
    belief that high-quality feedback can only come from external sources and propose
    a unique intrinsic method to measure the confidence for each reasoning step, and
    3) Both the off-the-shelf and open-source models can benefit from LeCo on various
    multi-step reasoning tasks with reduced token consumption. More excitingly, LeCo
    completely eliminates the need for prompt engineering.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Learning from Feedback
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Improving LLMs through learning from feedback has become a prevalent strategy,
    notably through reinforcement learning from human feedback, which seeks to align
    LLMs with human values by refining their outputs based on feedback (Ouyang et al.,
    [2022](#bib.bib35); Bai et al., [2022](#bib.bib3); Touvron et al., [2023](#bib.bib45)).
    However, this method faces challenges such as high costs due to manual labor and
    a lack of real-time feedback capabilities (Pan et al., [2023](#bib.bib36); Fernandes
    et al., [2023](#bib.bib9)). An alternative strategy involves using self-correcting
    LLMs, which rely on automated feedback to iteratively adapt and understand the
    consequences of their actions without heavy reliance on human intervention. This
    feedback can be derived from outside sources such as other models (Yang et al.,
    [2022](#bib.bib55); Lightman et al., [2023](#bib.bib26); Xiong et al., [2023](#bib.bib53)),
    tools (Gou et al., [2023a](#bib.bib13); Charalambous et al., [2023](#bib.bib5)),
    knowledge bases (Gao et al., [2023](#bib.bib11); Yu et al., [2023](#bib.bib60)),
    or evaluation metrics (Jung et al., [2022](#bib.bib21); Welleck et al., [2023](#bib.bib49)).
  prefs: []
  type: TYPE_NORMAL
- en: External feedback leverages external perspectives to identify errors and verify
    factual accuracy, offering insights that may not be recognized by the LLM alone.
    Conversely, feedback can also be internally generated, where the LLM evaluates
    and refines its output iteratively until a desired quality is achieved (Madaan
    et al., [2023](#bib.bib31); Shinn et al., [2023](#bib.bib42); Helbling et al.,
    [2023](#bib.bib16); Xie et al., [2023](#bib.bib52)). This self-improvement mechanism
    is particularly valuable in scenarios where external feedback is scarce or restricted (Yan
    et al., [2023](#bib.bib54); Ye et al., [2023](#bib.bib58)). However, recent effort (Huang
    et al., [2023b](#bib.bib20)) suggests that LLMs struggle to independently identify
    and correct errors through self-generated prompts. Unlike existing efforts, LeCo
    focuses on learning from one’s correct reasoning steps, without the need for feedback
    mechanisms including human intervention, external tools, or tailored prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Reasoning without Prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent studies have been focusing on improving the reasoning abilities of LLMs
    through various methodologies, primarily centered around the enhancement of prompting
    techniques. These works include few-shot prompting with intermediate steps augmented
    demonstrations (Wei et al., [2022](#bib.bib48); Fu et al., [2023](#bib.bib10);
    Yao et al., [2023](#bib.bib56); Wang et al., [2023](#bib.bib47)) or zero-shot
    prompting with specific instructions (Kojima et al., [2022](#bib.bib23); Yasunaga
    et al., [2023](#bib.bib57)). Although these methods have shown promising results,
    their effectiveness is often constrained by their task-specific nature and the
    labor-intensive process of designing prompts, leading to inconsistent outcomes
    across different tasks (Ye & Durrett, [2022](#bib.bib59); Zhou et al., [2023](#bib.bib61)).
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy to facilitate reasoning involves instruction tuning, which
    leverages a significant volume of chain-of-thought (CoT) data (Chung et al., [2022](#bib.bib6);
    Mukherjee et al., [2023](#bib.bib32); Gunasekar et al., [2023](#bib.bib15); Luo
    et al., [2023](#bib.bib29)). Recently, Liu et al. ([2024](#bib.bib28)) proposed
    to tune LLMs by comparing the logits differences between a pair of tuned and untuned
    smaller models, showcasing improvements in reasoning without CoT distillation.
    In contrast to these methods, our LeCo introduces an intrinsic self-correct reasoning
    mechanism that does not depend on fine-tuning or auxiliary models.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there has been an interest in refining decoding algorithms specifically
    for reasoning. Notably, contrastive decoding (Li et al., [2023](#bib.bib25)) has
    been developed to enhance a model’s generation quality by adjusting the logits
    from smaller models, with recent research indicating its potential to boost reasoning
    performance (O’Brien & Lewis, [2023](#bib.bib33)). Wang & Zhou ([2024](#bib.bib46))
    discovered that CoT reasoning patterns naturally occur within the decoding trajectories
    of LLMs, leading to the development of CoT-decoding, which aims to identify more
    reliable decoding paths. Such advancements present a promising avenue to augment
    the efficacy of LeCo. Future work could explore the integration of these decoding
    algorithms to extend beyond the current use of greedy decoding.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduce LeCo, a learning from correctness framework, designed to enhance
    multi-step reasoning capabilities. Our core insight is that providing the model
    with more correct reasoning steps helps it narrow down the search space for the
    solution. This facilitates the process of reaching the final answer. To achieve
    this, LeCo utilizes a prompt-free method to calculate the confidence score of
    each reasoning step. By identifying the most reliable steps, the model can then
    leverage these insights to guide its reasoning process.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Step Confidence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Preliminary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In generation tasks, logits represent the log probabilities of candidate tokens
    being chosen as the next word. Confidence, on the other hand, refers to a model’s
    certainty in its prediction. Within reasoning tasks, step confidence specifically
    measures the model’s belief in the correctness or factual basis of each reasoning
    step. Inspired by Li et al. ([2023](#bib.bib25)), we propose leveraging logits
    to estimate step confidence. We further design three logit-based scores that comprehensively
    evaluate confidence from both intra- and inter-step perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Confidence-based Reasoning Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:input $x_{0}$
  prefs: []
  type: TYPE_NORMAL
- en: Formally, we denote the entire reasoning path as $S=\left(s_{1},s_{2},\ldots,s_{n}\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: Average Token Score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A straightforward approach to measure step confidence is by averaging the token
    probabilities within a given step. This average reflects the model’s certainty
    in its reasoning during that step. Therefore, we define single-step confidence
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $avg\_score_{i}=\frac{1}{&#124;s_{i}&#124;}\sum_{j=1}^{&#124;s_{i}&#124;}p_{i,j}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Step Divergence Score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While average token probability seems intuitive, it can be misleading. Within
    a step, most tokens tend to be common words with high confidence scores but carry
    little information. Conversely, tokens crucial for reasoning, e.g. mathematical
    calculations, often have lower confidence. This paradox leads to a high average
    token confidence for the entire step, which contradicts our goal.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this issue, we propose the step divergence score. This metric measures
    the distribution uniformity of token probabilities within a step. Ideally, we
    want the token probabilities to be both high and evenly distributed across all
    tokens. To achieve this, we formulate the step divergence score based on the Kullback-Leibler
    Divergence (KLD; Kullback & Leibler [1951](#bib.bib24)) between the normalized
    distribution $P_{i}=\text{norm}(p_{i,1},p_{i,2},...,p_{i,|s_{i}|})$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $diver\_score_{i}=\text{ln}{(\text{KLD}^{\tau}(P_{i},U)+1)},$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$ is set to 0.3.
  prefs: []
  type: TYPE_NORMAL
- en: Inter-step Transition Score
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following the intra-step measurements, we sought to quantify the transition
    between consecutive steps. Our preliminary experiments yielded two key insights:
    1) steps with lower overall confidence tend to have lower confidence levels specifically
    in the initial heading tokens (typically the first three). 2) These initial heading
    tokens were also the most likely to change across different program runs. Based
    on these observations, we propose using the probabilities of the heading tokens
    in a step to represent the inter-step transition score between that step and the
    subsequent one. In other words, the transition score is determined by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $trans\_score_{i}=\frac{1}{K}\sum_{j=1}^{K}p_{i,j}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $K$ here.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the confidence score $s_{i}\_score$ is denoted as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '3.2 LeCo: Learning From Correctness'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While leveraging step confidence scores, previous approaches (Gou et al., [2023a](#bib.bib13);
    Huang et al., [2023a](#bib.bib19)) heavily rely on prompting LLMs to pinpoint
    and rectify erroneous steps. This dependence on prompts makes them rather sensitive.
    Our LeCo framework tackles this issue by iteratively gathering correct steps and
    consequently refining the search space for potential reasoning steps. As depicted
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Learning From Correctness Without
    Prompting Makes LLM Efficient Reasoner"), LeCo operates in a two-stage process.
  prefs: []
  type: TYPE_NORMAL
- en: Initial Stage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Given an input $x_{0}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $y_{0}={\mathcal{M}}\left(x_{0},Demo_{x}\right),$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $y_{0}(s_{0},s_{1},...,s_{|y_{0}|})$ consists of multiple reasoning steps.
  prefs: []
  type: TYPE_NORMAL
- en: Rethink Stage
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this stage, we first calculate the confidence score for each step within
    the initial solution $y_{0}$-th iteration, the workflow can be formulated as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{t}\leftarrow x_{t-1}+y_{t-1}(s<e),\quad y_{t}={\mathcal{M}}\left(x_{t},Demo_{x}\right).$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: LeCo alternates between input updating and rethink response generation until
    the stopping condition is met. The process either stops at a maximum iteration
    number $T$ or identifies the two consecutive same answers. The algorithm can be
    found in Algorithm [12](#alg1.l12 "In Algorithm 1 ‣ Preliminary ‣ 3.1 Step Confidence
    ‣ 3 Methodology ‣ Learning From Correctness Without Prompting Makes LLM Efficient
    Reasoner").
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Model | Method | Date |  | Commonsense |  | Arithmetic | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | CSQA | StrategyQA |  | AQuA | SVAMP | GSM8K |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | CoT | 80.80 |  | 79.69 | 73.25 |  | 51.57 | 84.00 | 77.86 | 74.53
    |'
  prefs: []
  type: TYPE_TB
- en: '| Complex | 84.20 |  | 77.33 | 69.84 |  | 54.49 | 81.25 | 80.89 | 74.67 |'
  prefs: []
  type: TYPE_TB
- en: '| ADPSC | 83.60 |  | 75.92 | 68.99 |  | 51.97 | 78.89 | 79.00 | 73.06 |'
  prefs: []
  type: TYPE_TB
- en: '| SC | 84.48 |  | 77.47 | 70.37 |  | 55.51 | 81.6 | 81.03 | 75.08 |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 74.97 |  | 68.34 | 51.94 |  | 35.50 | 79.95 | 75.25 | 64.33 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-11 | LeCo+CoT | 82.8 |  | 79.77 | 71.13 |  | 52.72 | 85.00 |
    78.24 | 74.93 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+2.00) |  | (+0.08) | (-2.12) |  | (+1.15) | (+1.00) | (+0.38) | (+0.40)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeCo+Complex | 84.92 |  | 77.68 | 71.05 |  | 56.77 | 82.35 | 82.33 | 75.85
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+0.72) |  | (+0.35) | (+1.21) |  | (+2.28) | (+1.10) | (+1.44) | (+1.18)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | CoT | 92.80 |  | 87.46 | 83.63 |  | 71.60 | 93.05 | 94.84 | 87.23
    |'
  prefs: []
  type: TYPE_TB
- en: '| Complex | 90.40 |  | 86.40 | 82.75 |  | 71.94 | 90.90 | 95.42 | 86.30 |'
  prefs: []
  type: TYPE_TB
- en: '| ADPSC | 89.20 |  | 85.67 | 83.87 |  | 70.08 | 88.99 | 94.09 | 85.32 |'
  prefs: []
  type: TYPE_TB
- en: '| SC | 90.72 |  | 86.81 | 83.75 |  | 72.19 | 93.49 | 95.51 | 86.67 |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 89.88 |  | 86.16 | 74.62 |  | 47.59 | 90.59 | 86.23 | 79.18 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-11 | LeCo+CoT | 93.60 |  | 87.63 | 83.25 |  | 71.99 | 93.55 |
    95.14 | 87.53 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+0.80) |  | (+0.17) | (-0.38) |  | (+0.39) | (+0.50) | (+0.30) | (+0.30)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeCo+Complex | 90.80 |  | 86.90 | 83.97 |  | 72.33 | 91.40 | 95.68 | 86.85
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+0.40) |  | (+0.50) | (+1.22) |  | (+0.39) | (+0.50) | (+0.26) | (+0.55)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Performance of GPT models on logical reasoning, commonsense reasoning,
    and arithmetic reasoning tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | Subset | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | Count | Geometry | Iter | Num | Prealgebra | Precaculus |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | Complex | 58.55 | 30.80 | 29.83 | 17.46 | 31.93 | 61.11 | 15.39
    | 35.01 |'
  prefs: []
  type: TYPE_TB
- en: '| ADPSC | 54.22 | 28.18 | 26.89 | 13.69 | 28.93 | 59.70 | 14.34 | 32.28 |'
  prefs: []
  type: TYPE_TB
- en: '| SC | 56.20 | 30.87 | 29.98 | 17.65 | 32.25 | 61.80 | 18.13 | 35.27 |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 49.79 | 24.25 | 18.76 | 10.16 | 25.09 | 53.71 | 13.08 | 27.83 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-10 | LeCo+Complex | 58.72 | 34.70 | 31.89 | 18.80 | 33.37 | 62.21
    | 18.53 | 36.89 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+0.17) | (+3.90) | (+2.06) | (+1.34) | (+1.44) | (+1.10) | (+3.14) |
    (+1.88) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | Complex | 69.06 | 50.32 | 38.62 | 25.33 | 46.39 | 76.98 | 28.23 |
    47.85 |'
  prefs: []
  type: TYPE_TB
- en: '| ADPSC | 60.13 | 40.13 | 30.55 | 15.84 | 37.39 | 69.46 | 21.10 | 39.23 |'
  prefs: []
  type: TYPE_TB
- en: '| SC | 71.04 | 52.23 | 40.48 | 25.89 | 50.37 | 77.84 | 30.51 | 49.77 |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 65.49 | 46.93 | 29.71 | 16.56 | 43.68 | 73.99 | 27.07 | 43.35 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-10 | LeCo+Complex | 71.92 | 53.27 | 41.13 | 27.49 | 49.14 | 78.29
    | 32.02 | 50.47 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+2.86) | (+3.05) | (+2.51) | (+2.16) | (+2.75) | (+1.31) | (+3.79) |
    (+2.62) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Performance of GPT models on the MATH dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset and Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the performance of LeCo using a variety of datasets and baselines.
    The datasets are categorized into three reasoning types: arithmetic reasoning,
    commonsense reasoning, and logical reasoning. The arithmetic reasoning datasets
    include GSM8K (Cobbe et al., [2021](#bib.bib7)), MATH (Hendrycks et al., [2021b](#bib.bib18)),
    AQuA (Ling et al., [2017](#bib.bib27)), and SVAMP (Patel et al., [2021](#bib.bib37)).
    For commonsense reasoning, we use CSQA (Saha et al., [2018](#bib.bib40)) and StrategyQA (Geva
    et al., [2021](#bib.bib12)). The logical reasoning dataset is represented by Date
    Understanding (Srivastava et al., [2022](#bib.bib43)).'
  prefs: []
  type: TYPE_NORMAL
- en: Our evaluation utilizes both off-the-shelf models, such as GPT-3.5-Turbo and
    GPT-4, and open-source models like DeepSeekMath-RL-7B (Shao et al., [2024](#bib.bib41)).
    The open-source models are chosen for their superior performance on well-known
    mathematical datasets. We also incorporate two suites of public demonstrations,
    namely exemplars from vanilla CoT (Wei et al., [2022](#bib.bib48)) and exemplars
    from complex-CoT (Complex; Fu et al. [2023](#bib.bib10)), which are prompts with
    higher reasoning complexity to improve language models multi-step reasoning ability.
  prefs: []
  type: TYPE_NORMAL
- en: We compare LeCo with several baselines, including self-consistency (SC; Wang
    et al. [2023](#bib.bib47)), adaptive self-consistency (ADPSC; Aggarwal et al.
    [2023](#bib.bib1)), and RCI (Kim et al., [2023](#bib.bib22)). SC polls the LLM
    multiple times and outputs the most frequent solution. ADPSC follows SC manner
    while conserving iterations via dynamically adjusting the number of samples per
    question using a lightweight stopping criterion. RCI is a representative work
    of learning from errors, which identifies errors and then self-corrects using
    designed prompts. In most runs, we use greedy decoding with a temperature of 0,
    except for the adaptive self-consistency and self-consistency settings, where
    a temperature of 0.7 is applied. The iteration number of self-consistency is set
    to 10\. All experiments are run 10 times with different seeds, and the average
    scores are reported.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Methods | GSM8K | Math | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | Count | Geometry | Iter | Num | Prealgebra | Precaculus |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSeek | Complex | 79.76 | 69.96 | 40.08 | 38.41 | 21,59 | 40.56 | 68.35
    | 24.18 | 47.87 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-11 | LeCo+Complex | 80.14 | 70.51 | 40.30 | 38.62 | 22.15 | 42.69
    | 68.52 | 23.99 | 48.37 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+0.38) | (+0.55) | (+0.22) | (+0.21) | (+0.56) | (+2.13) | (+0.17) |
    (-0.19) | (+0.50) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Performance of DeepSeekMath-7B on GSM8K and MATH, where Count represents
    counting and probability subset; Iter refers to intermediate algebra subset; Num
    means number theory subset.'
  prefs: []
  type: TYPE_NORMAL
- en: Main Results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in Table [1](#S4.T1 "Table 1 ‣ 4 Experiments ‣ Learning From Correctness
    Without Prompting Makes LLM Efficient Reasoner"), [2](#S4.T2 "Table 2 ‣ 4 Experiments
    ‣ Learning From Correctness Without Prompting Makes LLM Efficient Reasoner") and
    [3](#S4.T3 "Table 3 ‣ Dataset and Baselines ‣ 4 Experiments ‣ Learning From Correctness
    Without Prompting Makes LLM Efficient Reasoner"), LeCo consistently improves the
    reasoning performance across the board. Particularly noteworthy is its outstanding
    performance in arithmetic reasoning, especially evident in the MATH dataset. The
    MATH dataset is renowned for its challenging nature, like more intricate problems
    and the need for more reasoning steps, with common CoT approaches demonstrating
    limited effectiveness on this benchmark. However, LeCo effectively addresses this
    complexity by progressively collecting correct steps, thereby reducing reasoning
    perplexity and achieving substantial improvements. We also find that high-quality
    demonstrations are preferred when using LeCo as larger improvements are consistently
    observed with LeCo+Complex.
  prefs: []
  type: TYPE_NORMAL
- en: For commonsense reasoning tasks, LeCo obtains slight improvements or comparable
    performance against baselines. Except for the StrategyQA dataset, some performance
    drops are spotted. We think this is because commonsense reasoning necessitates
    incorporating knowledge concerning events and their relationships. However, LeCo
    primarily focuses on augmenting intrinsic reasoning ability through correctness,
    hence a moderate enhancement is deemed reasonable. This finding is also aligned
    with observations in Lyu et al. ([2023](#bib.bib30)). Conversely, remarkable improvements
    are obtained in the date understanding dataset since this task is more similar
    to mathematical reasoning. It is worth noting that the difficulty of the task
    correlates positively with the impact of LeCo, as evidenced by the substantial
    improvements achieved on the AQuA and MATH datasets. The primary reason for this
    is that the LLM tends to remain their initial reasoning path on the easy problems,
    offering fewer improvement rooms for LeCo. For a comprehensive evaluation, we
    also apply LeCo on the open-source model. We chose DeepSeekMath-RL-7B, as it demonstrates
    competitive performance in mathematical reasoning tasks. As shown in Table [3](#S4.T3
    "Table 3 ‣ Dataset and Baselines ‣ 4 Experiments ‣ Learning From Correctness Without
    Prompting Makes LLM Efficient Reasoner"), LeCo can consistently improve the reasoning
    performance on GSM8K and MATH datasets, indicating its effectiveness on open-source
    models.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, LeCo also exhibits its superiority in reducing token consumption.
    As shown in Section [A.2](#A1.SS2 "A.2 Average Iterations Numbers by Different
    Methods and Models ‣ Appendix A Efficiency of Different Models ‣ Learning From
    Correctness Without Prompting Makes LLM Efficient Reasoner"), although adaptive
    self-consistency has tried to reduce the iterations and token consumption by settings
    the early stop criterion, it still needs almost 4.46 rounds to determine the final
    answer while RCI needs 2.74 rounds. However, using the similar stop criterion
    of RCI, LeCo can reach the final answer just with 2.15 rounds. This phenomenon
    suggests that learning from correctness is more effective than learning from errors,
    as it does not necessitate the model’s understanding of the error cues. Additionally,
    during each iteration, LeCo reduces API consumption by alleviating prompting the
    model to identify and understand the errors and shortening the output length.
    Therefore, as shown in Section [A.1](#A1.SS1 "A.1 Token Consumption ‣ Appendix
    A Efficiency of Different Models ‣ Learning From Correctness Without Prompting
    Makes LLM Efficient Reasoner"), LeCo reduces the token consumption by 80%/20%
    compared to SC/RCI.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Further Analyses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Models | Methods | Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | StrategyQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | Complex | 82.47 | 70.17 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-4 | Random | 82.09 | 69.96 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (-0.38) | (-0.21) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | Complex | 95.34 | 82.69 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-4 | Random | 95.22 | 83.37 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (-0.12) | (+0.68) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Coarse-grained level ablation study on GSM8K and StrategyQA datasets
    with GPT-3.5.'
  prefs: []
  type: TYPE_NORMAL
- en: '| GSM8K | Exact Correct | Partial Correct | Wrong |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Only Avg | 38 | 9 | 53 |'
  prefs: []
  type: TYPE_TB
- en: '| Only Div | 35 | 16 | 49 |'
  prefs: []
  type: TYPE_TB
- en: '| Only Trans | 42 | 24 | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| Avg+Div | 36 | 14 | 50 |'
  prefs: []
  type: TYPE_TB
- en: '| Avg+Trans | 50 | 16 | 34 |'
  prefs: []
  type: TYPE_TB
- en: '| Div+Trans | 47 | 16 | 37 |'
  prefs: []
  type: TYPE_TB
- en: '| LeCo | 53 | 10 | 37 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Fine-grained level ablation study of the three factors for calculating
    the step confidence. Avg denotes the average token confidence; Div denotes the
    step divergence score; and Trans denotes the inter-step transition score.'
  prefs: []
  type: TYPE_NORMAL
- en: Ablation Study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conduct ablation studies at two levels of granularity. At the coarse-grained
    level, we explore the effectiveness of the learning-from-correctness framework
    by replacing the selection of correct steps with random choices. Specifically,
    in the rethink stage, we randomly choose a reasoning step as the earliest error
    step and consider the preceding steps as the “correctness”. From Table [5](#S5.T5
    "Table 5 ‣ 5 Further Analyses ‣ Learning From Correctness Without Prompting Makes
    LLM Efficient Reasoner"), we can see that the random selection of correct steps
    generally hurt the reasoning performance, suggesting the importance of identifying
    the true correctness.
  prefs: []
  type: TYPE_NORMAL
- en: At the fine-grained level, we deeply investigate the design of step confidence,
    which involves calculating the sum of the average token confidence, step divergence
    score, and inter-step transition score. To minimize the time and token consumption,
    we employ the accuracy of identifying the earliest error step as our metric. This
    measurement has proven to be crucial for enhancing reasoning performance in subsequent
    rounds, as evidenced by the results in Table [5](#S5.T5 "Table 5 ‣ 5 Further Analyses
    ‣ Learning From Correctness Without Prompting Makes LLM Efficient Reasoner").
    To this end, we randomly sampled 100 incorrect solutions on the GSM8K dataset
    and manually annotated the earliest error step for these solutions. Then, we divide
    the predicted step into three categories, including exact_correct, partial_correct
    and wrong, wherein exact_correct means the predicted step is exactly the labeled
    earliest step; partial_correct means the predicted step is actually an error step
    but located after the earliest step, and wrong means the predicted step is before
    the target location. As presented in Table [5](#S5.T5 "Table 5 ‣ 5 Further Analyses
    ‣ Learning From Correctness Without Prompting Makes LLM Efficient Reasoner"),
    LeCo performs best in finding the earliest error step, with accuracy over 50%.
    We also observe the significant performance drops when separately adopting one
    of these factors. More interestingly, among the three factors, we find the inter-step
    transition score affects the final performance most. This finding is also well-aligned
    with the observations in our preliminary experiments, as stated in Section [3.1](#S3.SS1.SSS0.Px4
    "Inter-step Transition Score ‣ 3.1 Step Confidence ‣ 3 Methodology ‣ Learning
    From Correctness Without Prompting Makes LLM Efficient Reasoner"), which suggests
    that the heading tokens of a step warrant more attention.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80a5a0d2cf4aefb29918e4fa1a07540d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Evaluation of the changes after rethink stage. We compare our LeCO
    and RCI on GSM8K and StrategyQA datasets with GPT-3.5\. W2R: the wrong answer
    is changed to right. R2W:the right answer is altered to wrong. W2W: a wrong answer
    is changed to another wrong answer. No change: The answer remains unchanged.'
  prefs: []
  type: TYPE_NORMAL
- en: Rethink Analysis
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As LeCo and RCI are both the self-refinement framework, distinguished by their
    learning mechanisms from correctness or errors, we then compare them regarding
    the changes in answers after the rethinking stage. As illustrated in Figure [2](#S5.F2
    "Figure 2 ‣ Ablation Study ‣ 5 Further Analyses ‣ Learning From Correctness Without
    Prompting Makes LLM Efficient Reasoner"), on the GSM8K dataset, over 85% of the
    time, both LeCo and RCI retain the original answer. Among the remaining instances,
    LeCo can modify more incorrect answers to correct ones than RCI (3.7% vs. 1.5%).
    On the StrategyQA dataset, the performance gap between LeCo and RCI is more significant,
    where RCI revises 24.8% correct answers to incorrect. This phenomenon is in line
    with the recent findings(Huang et al., [2023b](#bib.bib20)) that LLMs are currently
    incapable of self-correction based on their own feedback. Superior to RCI, LeCo
    cleverly uses the accumulated correct information and avoids meticulous self-evaluation
    prompts to achieve better reasoning performance.
  prefs: []
  type: TYPE_NORMAL
- en: Oracle Test
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We also conduct the oracle test to explore the upper bound of learning-from-correctness
    by directly providing the correct steps to LLMs during the rethink stage. To this
    end, we sampled 100 incorrect solutions generated by GPT-3.5-Turbo on the StrategyQA
    and GSM8K datasets, respectively. Subsequently, we manually annotate the earliest
    error step for these solutions. After collecting the preceding correct steps and
    appending them to the input, we generate an updated solution. As shown in Table
    [7](#S5.T7 "Table 7 ‣ Oracle Test ‣ 5 Further Analyses ‣ Learning From Correctness
    Without Prompting Makes LLM Efficient Reasoner"), promising results are obtained
    that 36% and 22% wrong solutions can be amended with the help of correctness.
    It is important to note that these figures do not represent the absolute upper
    limit of the potential to learn from correctness since the refinement process
    is iterative but we can only label the first round. More interestingly, LeCo achieves
    a comparable performance (33 vs. 36; 21 vs. 22) with Oracle and significantly
    outperforms the random choices, suggesting the effectiveness of LeCo in identifying
    the true correctness.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Methods | StrategyQA | GSM8K |'
  prefs: []
  type: TYPE_TB
- en: '| Complex | 31 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| Random | 25 | 13 |'
  prefs: []
  type: TYPE_TB
- en: '| Oracle | 36 | 22 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline1-3 LeCo | 33 | 21 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Oracle test on StrategyQA and GSM8K by GPT-3.5-Turbo. Random denotes
    randomly selecting the earliest error step. Oracle denotes human annotated earliest
    error step.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Methods | Datasets |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | StrategyQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | Complex | 81.58 | 70.94 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-4 | Early stop | 82.03 | 69.31 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+0.45) | (-1.63) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | Complex | 95.11 | 81.25 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-4 | Early stop | 95.41 | 81.87 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (+0.30) | (+0.62) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Early Stop of LeCo on the GSM8K and StrategyQA using GPT-3.5-Turbo
    and GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: Early Stop of LeCo
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As discussed above, the majority of initial solutions would not be modified
    after the rethink stage, which additionally escalates token consumption and ratio
    of “correct $\Rightarrow$ incorrect”. To alleviate these problems, we present
    an early stop strategy of LeCo, which dynamically determines whether the initial
    solution requires the refinement based on the overall solution score.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the step confidence, we calculate the overall solution confidence
    score $sln\_score$ by jointly considering the average score of step confidence
    and the inter-step divergence, formulated as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $s_{i}\_score$ and an equal-length uniform discrete distribution, analogy
    to the Equation [2](#S3.E2 "In Step Divergence Score ‣ 3.1 Step Confidence ‣ 3
    Methodology ‣ Learning From Correctness Without Prompting Makes LLM Efficient
    Reasoner").
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, we conducted test on the GSM8K dataset using GPT-3.5-Turbo and recorded
    the solution confidence scores following Equation [7](#S5.E7 "In Early Stop of
    LeCo ‣ 5 Further Analyses ‣ Learning From Correctness Without Prompting Makes
    LLM Efficient Reasoner"). As shown in Figure [3](#S5.F3 "Figure 3 ‣ Early Stop
    of LeCo ‣ 5 Further Analyses ‣ Learning From Correctness Without Prompting Makes
    LLM Efficient Reasoner")(a), we observed that the distributions of scores for
    both correct and incorrect solutions consistently tend to follow the norm distribution,
    with the average point of correct answers notably surpassing that of incorrect
    ones. We aim to employ this discrepancy to early stop the rethink stage. Specifically,
    we first randomly sample a subset from the testing data to obtain the distribution
    of solution scores, approximately 1/6 of the data of the entire test set used.
    Figure [3](#S5.F3 "Figure 3 ‣ Early Stop of LeCo ‣ 5 Further Analyses ‣ Learning
    From Correctness Without Prompting Makes LLM Efficient Reasoner")(b) illustrates
    the distribution on the GSM8K sample set, which also follows the norm distribution.
    Then, based on the 3-$\sigma$) as our threshold, which covers 84% incorrect samples
    while only including around 50% correct instances.
  prefs: []
  type: TYPE_NORMAL
- en: As demonstrated in Table [7](#S5.T7 "Table 7 ‣ Oracle Test ‣ 5 Further Analyses
    ‣ Learning From Correctness Without Prompting Makes LLM Efficient Reasoner"),
    consistent improvements can be obtained with early-stop LeCo over the vanilla
    CoT-based method. Compared to the standard LeCo, there are slight performance
    drops since more incorrect instances are filtered and not modified. However, early-stop
    LeCo can still maintain the performance levels intermediate to those of SC and
    LeCo while using fewer iteration rounds and tokens, approximately further reducing
    10% tokens against the standard LeCo (More details in Appendix [B](#A2 "Appendix
    B Details of Early Stop LeCo ‣ Learning From Correctness Without Prompting Makes
    LLM Efficient Reasoner")). We note that early-stop LeCo is an alternative choice
    for the users to achieve a better trade-off between the token consumption and
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/207731a3f090b88fd7b7df9d53ed3995.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The distribution of correct and incorrect solutions of GSM8K by GPT-3.5-Turbo.
    The curve in pink represents incorrect answers, and the curve in blue represents
    correct answers.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work introduces LeCo, an intrinsic self-correct reasoning framework designed
    to enhance LLM reasoning performance without relying on human feedback, external
    tools, or handcrafted prompts. LeCo leverages a multi-step reasoning paradigm,
    prioritizing learning from successful reasoning steps. It incorporates a novel
    method for measuring confidence in each step based on generation logits. Our experiments
    across diverse multi-step reasoning tasks demonstrate LeCo’s effectiveness in
    improving reasoning accuracy while minimizing token consumption. This approach
    represents a distinct pathway for augmenting LLM capabilities, offering a promising
    avenue for advancing their aptitude in reasoning tasks. For future work, a worthy
    noting point is that LeCo, especially its step confidence algorithm, would stand
    as an excellent candidate for pruning the complex reasoning structures, such as
    Tree-of-Thoughts (Yao et al., [2023](#bib.bib56)) and Graph-of-Thoughts (Besta
    et al., [2023](#bib.bib4)).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Aggarwal et al. (2023) Aman Madaan Pranjal Aggarwal, Yiming Yang, and Mausam.
    Let’s sample step by step: Adaptive-consistency for efficient reasoning and coding
    with llms. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), *Proceedings of
    the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2023, Singapore, December 6-10, 2023*, pp.  12375–12396\. Association for Computational
    Linguistics, 2023. URL [https://aclanthology.org/2023.emnlp-main.761](https://aclanthology.org/2023.emnlp-main.761).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An et al. (2023) Shengnan An, Zexiong Ma, Zeqi Lin, Nanning Zheng, Jian-Guang
    Lou, and Weizhu Chen. Learning from mistakes makes LLM better reasoner. *CoRR*,
    abs/2310.20689, 2023. doi: 10.48550/ARXIV.2310.20689. URL [https://doi.org/10.48550/arXiv.2310.20689](https://doi.org/10.48550/arXiv.2310.20689).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bai et al. (2022) Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell,
    Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron
    McKinnon, Carol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn
    Drain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr, Jared
    Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosiute, Liane
    Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemí Mercado, Nova
    DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston, Shauna Kravec,
    Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-Lawton, Tom Conerly,
    Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario
    Amodei, Nicholas Joseph, Sam McCandlish, Tom Brown, and Jared Kaplan. Constitutional
    AI: harmlessness from AI feedback. *CoRR*, abs/2212.08073, 2022. doi: 10.48550/ARXIV.2212.08073.
    URL [https://doi.org/10.48550/arXiv.2212.08073](https://doi.org/10.48550/arXiv.2212.08073).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, and Torsten Hoefler. Graph of thoughts: Solving elaborate problems
    with large language models. *CoRR*, abs/2308.09687, 2023. doi: 10.48550/ARXIV.2308.09687.
    URL [https://doi.org/10.48550/arXiv.2308.09687](https://doi.org/10.48550/arXiv.2308.09687).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Charalambous et al. (2023) Yiannis Charalambous, Norbert Tihanyi, Ridhi Jain,
    Youcheng Sun, Mohamed Amine Ferrag, and Lucas C. Cordeiro. A new era in software
    security: Towards self-healing software via large language models and formal verification.
    *CoRR*, abs/2305.14752, 2023. doi: 10.48550/ARXIV.2305.14752. URL [https://doi.org/10.48550/arXiv.2305.14752](https://doi.org/10.48550/arXiv.2305.14752).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yanping Huang, Andrew M.
    Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts,
    Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language
    models. *CoRR*, abs/2210.11416, 2022. doi: 10.48550/ARXIV.2210.11416. URL [https://doi.org/10.48550/arXiv.2210.11416](https://doi.org/10.48550/arXiv.2210.11416).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems. *CoRR*, abs/2110.14168, 2021. URL [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fayek et al. (2020) Haytham M. Fayek, Lawrence Cavedon, and Hong Ren Wu. Progressive
    learning: A deep learning framework for continual learning. *Neural Networks*,
    128:345–357, 2020. doi: 10.1016/J.NEUNET.2020.05.011. URL [https://doi.org/10.1016/j.neunet.2020.05.011](https://doi.org/10.1016/j.neunet.2020.05.011).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fernandes et al. (2023) Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas,
    Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang
    Wu, Graham Neubig, and André F. T. Martins. Bridging the gap: A survey on integrating
    (human) feedback for natural language generation. *CoRR*, abs/2305.00955, 2023.
    doi: 10.48550/ARXIV.2305.00955. URL [https://doi.org/10.48550/arXiv.2305.00955](https://doi.org/10.48550/arXiv.2305.00955).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fu et al. (2023) Yao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar
    Khot. Complexity-based prompting for multi-step reasoning. In *The Eleventh International
    Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*.
    OpenReview.net, 2023. URL [https://openreview.net/pdf?id=yf1icZHC-l9](https://openreview.net/pdf?id=yf1icZHC-l9).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2023) Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony Chen, Arun Tejasvi
    Chaganty, Yicheng Fan, Vincent Y. Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and
    Kelvin Guu. RARR: researching and revising what language models say, using language
    models. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), *Proceedings
    of the 61st Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, pp.  16477–16508\.
    Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.910.
    URL [https://doi.org/10.18653/v1/2023.acl-long.910](https://doi.org/10.18653/v1/2023.acl-long.910).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan
    Roth, and Jonathan Berant. Did aristotle use a laptop? A question answering benchmark
    with implicit reasoning strategies. *Trans. Assoc. Comput. Linguistics*, 9:346–361,
    2021. doi: 10.1162/TACL“˙A“˙00370. URL [https://doi.org/10.1162/tacl_a_00370](https://doi.org/10.1162/tacl_a_00370).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2023a) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu
    Yang, Nan Duan, and Weizhu Chen. CRITIC: large language models can self-correct
    with tool-interactive critiquing. *CoRR*, abs/2305.11738, 2023a. doi: 10.48550/ARXIV.2305.11738.
    URL [https://doi.org/10.48550/arXiv.2305.11738](https://doi.org/10.48550/arXiv.2305.11738).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2023b) Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu
    Yang, Minlie Huang, Nan Duan, and Weizhu Chen. Tora: A tool-integrated reasoning
    agent for mathematical problem solving. *CoRR*, abs/2309.17452, 2023b. doi: 10.48550/ARXIV.2309.17452.
    URL [https://doi.org/10.48550/arXiv.2309.17452](https://doi.org/10.48550/arXiv.2309.17452).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gunasekar et al. (2023) Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro
    Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo
    de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang,
    Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li.
    Textbooks are all you need. *CoRR*, abs/2306.11644, 2023. doi: 10.48550/ARXIV.2306.11644.
    URL [https://doi.org/10.48550/arXiv.2306.11644](https://doi.org/10.48550/arXiv.2306.11644).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Helbling et al. (2023) Alec Helbling, Mansi Phute, Matthew Hull, and Duen Horng
    Chau. LLM self defense: By self examination, llms know they are being tricked.
    *CoRR*, abs/2308.07308, 2023. doi: 10.48550/ARXIV.2308.07308. URL [https://doi.org/10.48550/arXiv.2308.07308](https://doi.org/10.48550/arXiv.2308.07308).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. In *9th International Conference on Learning Representations, ICLR
    2021, Virtual Event, Austria, May 3-7, 2021*. OpenReview.net, 2021a. URL [https://openreview.net/forum?id=d7KBjmI3GmQ](https://openreview.net/forum?id=d7KBjmI3GmQ).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul
    Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung
    (eds.), *Proceedings of the Neural Information Processing Systems Track on Datasets
    and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual*,
    2021b. URL [https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2023a) Jiaxin Huang, Shixiang Gu, Le Hou, Yuexin Wu, Xuezhi Wang,
    Hongkun Yu, and Jiawei Han. Large language models can self-improve. In Houda Bouamor,
    Juan Pino, and Kalika Bali (eds.), *Proceedings of the 2023 Conference on Empirical
    Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10,
    2023*, pp.  1051–1068\. Association for Computational Linguistics, 2023a. URL
    [https://aclanthology.org/2023.emnlp-main.67](https://aclanthology.org/2023.emnlp-main.67).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023b) Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven
    Zheng, Adams Wei Yu, Xinying Song, and Denny Zhou. Large language models cannot
    self-correct reasoning yet. *CoRR*, abs/2310.01798, 2023b. doi: 10.48550/ARXIV.2310.01798.
    URL [https://doi.org/10.48550/arXiv.2310.01798](https://doi.org/10.48550/arXiv.2310.01798).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jung et al. (2022) Jaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brahman, Chandra
    Bhagavatula, Ronan Le Bras, and Yejin Choi. Maieutic prompting: Logically consistent
    reasoning with recursive explanations. In Yoav Goldberg, Zornitsa Kozareva, and
    Yue Zhang (eds.), *Proceedings of the 2022 Conference on Empirical Methods in
    Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December
    7-11, 2022*, pp.  1266–1279\. Association for Computational Linguistics, 2022.
    doi: 10.18653/V1/2022.EMNLP-MAIN.82. URL [https://doi.org/10.18653/v1/2022.emnlp-main.82](https://doi.org/10.18653/v1/2022.emnlp-main.82).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2023) Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Language
    models can solve computer tasks. In Alice Oh, Tristan Naumann, Amir Globerson,
    Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), *Advances in Neural Information
    Processing Systems 36: Annual Conference on Neural Information Processing Systems
    2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/7cc1005ec73cfbaac9fa21192b622507-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/7cc1005ec73cfbaac9fa21192b622507-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In
    Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.),
    *Advances in Neural Information Processing Systems 35: Annual Conference on Neural
    Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November
    28 - December 9, 2022*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kullback & Leibler (1951) Solomon Kullback and Richard A Leibler. On information
    and sufficiency. *The annals of mathematical statistics*, 22(1):79–86, 1951.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason
    Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, and Mike Lewis. Contrastive decoding:
    Open-ended text generation as optimization. In Anna Rogers, Jordan L. Boyd-Graber,
    and Naoaki Okazaki (eds.), *Proceedings of the 61st Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada,
    July 9-14, 2023*, pp.  12286–12312\. Association for Computational Linguistics,
    2023. doi: 10.18653/V1/2023.ACL-LONG.687. URL [https://doi.org/10.18653/v1/2023.acl-long.687](https://doi.org/10.18653/v1/2023.acl-long.687).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lightman et al. (2023) Hunter Lightman, Vineet Kosaraju, Yura Burda, Harrison
    Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and
    Karl Cobbe. Let’s verify step by step. *CoRR*, abs/2305.20050, 2023. doi: 10.48550/ARXIV.2305.20050.
    URL [https://doi.org/10.48550/arXiv.2305.20050](https://doi.org/10.48550/arXiv.2305.20050).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    Program induction by rationale generation: Learning to solve and explain algebraic
    word problems. In Regina Barzilay and Min-Yen Kan (eds.), *Proceedings of the
    55th Annual Meeting of the Association for Computational Linguistics, ACL 2017,
    Vancouver, Canada, July 30 - August 4, Volume 1: Long Papers*, pp.  158–167\.
    Association for Computational Linguistics, 2017. doi: 10.18653/V1/P17-1015. URL
    [https://doi.org/10.18653/v1/P17-1015](https://doi.org/10.18653/v1/P17-1015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024) Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov,
    Yejin Choi, and Noah A. Smith. Tuning language models by proxy. *CoRR*, abs/2401.08565,
    2024. doi: 10.48550/ARXIV.2401.08565. URL [https://doi.org/10.48550/arXiv.2401.08565](https://doi.org/10.48550/arXiv.2401.08565).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2023) Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou,
    Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. Wizardmath:
    Empowering mathematical reasoning for large language models via reinforced evol-instruct.
    *CoRR*, abs/2308.09583, 2023. doi: 10.48550/ARXIV.2308.09583. URL [https://doi.org/10.48550/arXiv.2308.09583](https://doi.org/10.48550/arXiv.2308.09583).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lyu et al. (2023) Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought
    reasoning. *CoRR*, abs/2301.13379, 2023. doi: 10.48550/ARXIV.2301.13379. URL [https://doi.org/10.48550/arXiv.2301.13379](https://doi.org/10.48550/arXiv.2301.13379).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2023) Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan,
    Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
    Sean Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, Amir Yazdanbakhsh,
    and Peter Clark. Self-refine: Iterative refinement with self-feedback. *CoRR*,
    abs/2303.17651, 2023. doi: 10.48550/ARXIV.2303.17651. URL [https://doi.org/10.48550/arXiv.2303.17651](https://doi.org/10.48550/arXiv.2303.17651).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukherjee et al. (2023) Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar,
    Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. Orca: Progressive learning
    from complex explanation traces of GPT-4. *CoRR*, abs/2306.02707, 2023. doi: 10.48550/ARXIV.2306.02707.
    URL [https://doi.org/10.48550/arXiv.2306.02707](https://doi.org/10.48550/arXiv.2306.02707).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'O’Brien & Lewis (2023) Sean O’Brien and Mike Lewis. Contrastive decoding improves
    reasoning in large language models. *CoRR*, abs/2309.09117, 2023. doi: 10.48550/ARXIV.2309.09117.
    URL [https://doi.org/10.48550/arXiv.2309.09117](https://doi.org/10.48550/arXiv.2309.09117).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2023) OpenAI. GPT-4 technical report. *CoRR*, abs/2303.08774, 2023.
    doi: 10.48550/ARXIV.2303.08774. URL [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F. Christiano, Jan Leike, and Ryan Lowe. Training
    language models to follow instructions with human feedback. In Sanmi Koyejo, S. Mohamed,
    A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), *Advances in Neural Information
    Processing Systems 35: Annual Conference on Neural Information Processing Systems
    2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022*, 2022.
    URL [http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pan et al. (2023) Liangming Pan, Michael Saxon, Wenda Xu, Deepak Nathani, Xinyi
    Wang, and William Yang Wang. Automatically correcting large language models: Surveying
    the landscape of diverse self-correction strategies. *CoRR*, abs/2308.03188, 2023.
    doi: 10.48550/ARXIV.2308.03188. URL [https://doi.org/10.48550/arXiv.2308.03188](https://doi.org/10.48550/arXiv.2308.03188).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patel et al. (2021) Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are
    NLP models really able to solve simple math word problems? In Kristina Toutanova,
    Anna Rumshisky, Luke Zettlemoyer, Dilek Hakkani-Tür, Iz Beltagy, Steven Bethard,
    Ryan Cotterell, Tanmoy Chakraborty, and Yichao Zhou (eds.), *Proceedings of the
    2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*,
    pp.  2080–2094\. Association for Computational Linguistics, 2021. doi: 10.18653/V1/2021.NAACL-MAIN.168.
    URL [https://doi.org/10.18653/v1/2021.naacl-main.168](https://doi.org/10.18653/v1/2021.naacl-main.168).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. Instruction tuning with GPT-4. *CoRR*, abs/2304.03277, 2023. doi:
    10.48550/ARXIV.2304.03277. URL [https://doi.org/10.48550/arXiv.2304.03277](https://doi.org/10.48550/arXiv.2304.03277).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rawte et al. (2023) Vipula Rawte, Amit P. Sheth, and Amitava Das. A survey
    of hallucination in large foundation models. *CoRR*, abs/2309.05922, 2023. doi:
    10.48550/ARXIV.2309.05922. URL [https://doi.org/10.48550/arXiv.2309.05922](https://doi.org/10.48550/arXiv.2309.05922).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saha et al. (2018) Amrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan,
    and Sarath Chandar. Complex sequential question answering: Towards learning to
    converse over linked question answer pairs with a knowledge graph. In Sheila A.
    McIlraith and Kilian Q. Weinberger (eds.), *Proceedings of the Thirty-Second AAAI
    Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications
    of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational
    Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
    2-7, 2018*, pp.  705–713\. AAAI Press, 2018. doi: 10.1609/AAAI.V32I1.11332. URL
    [https://doi.org/10.1609/aaai.v32i1.11332](https://doi.org/10.1609/aaai.v32i1.11332).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shao et al. (2024) Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao
    Song, Mingchuan Zhang, Y.K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the
    limits of mathematical reasoning in open language models, 2024. URL [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al. (2023) Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik
    Narasimhan, and Shunyu Yao. Reflexion: language agents with verbal reinforcement
    learning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,
    and Sergey Levine (eds.), *Advances in Neural Information Processing Systems 36:
    Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023,
    New Orleans, LA, USA, December 10 - 16, 2023*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/1b44b878bb782e6954cd888628510e90-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal,
    Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali
    Tazarv, Alice Xiang, Alicia Parrish, Allen Nie, Aman Hussain, Amanda Askell, Amanda
    Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli,
    Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela
    Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Antonio Norelli,
    Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan,
    Asher Mullokandov, Ashish Sabharwal, Austin Herrick, Avia Efrat, Aykut Erdem,
    Ayla Karakas, and et al. Beyond the imitation game: Quantifying and extrapolating
    the capabilities of language models. *CoRR*, abs/2206.04615, 2022. doi: 10.48550/ARXIV.2206.04615.
    URL [https://doi.org/10.48550/arXiv.2206.04615](https://doi.org/10.48550/arXiv.2206.04615).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thirunavukarasu et al. (2023) Arun James Thirunavukarasu, Darren Shu Jeng Ting,
    Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and Daniel Shu Wei Ting. Large
    language models in medicine. *Nature medicine*, 29(8):1930–1940, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton-Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurélien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models. *CoRR*, abs/2307.09288, 2023. doi:
    10.48550/ARXIV.2307.09288. URL [https://doi.org/10.48550/arXiv.2307.09288](https://doi.org/10.48550/arXiv.2307.09288).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang & Zhou (2024) Xuezhi Wang and Denny Zhou. Chain-of-thought reasoning without
    prompting. *CoRR*, abs/2402.10200, 2024. doi: 10.48550/ARXIV.2402.10200. URL [https://doi.org/10.48550/arXiv.2402.10200](https://doi.org/10.48550/arXiv.2402.10200).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H.
    Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. In *The Eleventh International
    Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023*.
    OpenReview.net, 2023. URL [https://openreview.net/pdf?id=1PL1NIMMrw](https://openreview.net/pdf?id=1PL1NIMMrw).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting
    elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
    Danielle Belgrave, K. Cho, and A. Oh (eds.), *Advances in Neural Information Processing
    Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS
    2022, New Orleans, LA, USA, November 28 - December 9, 2022*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Welleck et al. (2023) Sean Welleck, Ximing Lu, Peter West, Faeze Brahman, Tianxiao
    Shen, Daniel Khashabi, and Yejin Choi. Generating sequences by learning to self-correct.
    In *The Eleventh International Conference on Learning Representations, ICLR 2023,
    Kigali, Rwanda, May 1-5, 2023*. OpenReview.net, 2023. URL [https://openreview.net/pdf?id=hH36JeQZDaO](https://openreview.net/pdf?id=hH36JeQZDaO).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2023) Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang,
    Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling
    next-gen LLM applications via multi-agent conversation framework. *CoRR*, abs/2308.08155,
    2023. doi: 10.48550/ARXIV.2308.08155. URL [https://doi.org/10.48550/arXiv.2308.08155](https://doi.org/10.48550/arXiv.2308.08155).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) Yu Wu, Yutian Lin, Xuanyi Dong, Yan Yan, Wei Bian, and Yi Yang.
    Progressive learning for person re-identification with one example. *IEEE Trans.
    Image Process.*, 28(6):2872–2881, 2019. doi: 10.1109/TIP.2019.2891895. URL [https://doi.org/10.1109/TIP.2019.2891895](https://doi.org/10.1109/TIP.2019.2891895).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2023) Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, James Xu Zhao, Min-Yen
    Kan, Junxian He, and Michael Qizhe Xie. Self-evaluation guided beam search for
    reasoning. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt,
    and Sergey Levine (eds.), *Advances in Neural Information Processing Systems 36:
    Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023,
    New Orleans, LA, USA, December 10 - 16, 2023*, 2023. URL [http://papers.nips.cc/paper_files/paper/2023/hash/81fde95c4dc79188a69ce5b24d63010b-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2023/hash/81fde95c4dc79188a69ce5b24d63010b-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiong et al. (2023) Jing Xiong, Zixuan Li, Chuanyang Zheng, Zhijiang Guo, Yichun
    Yin, Enze Xie, Zhicheng Yang, Qingxing Cao, Haiming Wang, Xiongwei Han, Jing Tang,
    Chengming Li, and Xiaodan Liang. Dq-lore: Dual queries with low rank approximation
    re-ranking for in-context learning. *CoRR*, abs/2310.02954, 2023. doi: 10.48550/ARXIV.2310.02954.
    URL [https://doi.org/10.48550/arXiv.2310.02954](https://doi.org/10.48550/arXiv.2310.02954).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2023) Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang, Wen-tau
    Yih, and Ziyu Yao. Learning to simulate natural language feedback for interactive
    semantic parsing. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.),
    *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023*, pp.  3149–3170\.
    Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.ACL-LONG.177.
    URL [https://doi.org/10.18653/v1/2023.acl-long.177](https://doi.org/10.18653/v1/2023.acl-long.177).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022) Kevin Yang, Yuandong Tian, Nanyun Peng, and Dan Klein. Re3:
    Generating longer stories with recursive reprompting and revision. *CoRR*, abs/2210.06774,
    2022. doi: 10.48550/ARXIV.2210.06774. URL [https://doi.org/10.48550/arXiv.2210.06774](https://doi.org/10.48550/arXiv.2210.06774).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L.
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. *CoRR*, abs/2305.10601, 2023. doi: 10.48550/ARXIV.2305.10601.
    URL [https://doi.org/10.48550/arXiv.2305.10601](https://doi.org/10.48550/arXiv.2305.10601).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yasunaga et al. (2023) Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong
    Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. Large language
    models as analogical reasoners. *CoRR*, abs/2310.01714, 2023. doi: 10.48550/ARXIV.2310.01714.
    URL [https://doi.org/10.48550/arXiv.2310.01714](https://doi.org/10.48550/arXiv.2310.01714).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2023) Seonghyeon Ye, Yongrae Jo, Doyoung Kim, Sungdong Kim, Hyeonbin
    Hwang, and Minjoon Seo. Selfee: Iterative self-revising llm empowered by self-feedback
    generation. Blog post, May 2023. URL [https://kaistai.github.io/SelFee/](https://kaistai.github.io/SelFee/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye & Durrett (2022) Xi Ye and Greg Durrett. The unreliability of explanations
    in few-shot prompting for textual reasoning. In Sanmi Koyejo, S. Mohamed, A. Agarwal,
    Danielle Belgrave, K. Cho, and A. Oh (eds.), *Advances in Neural Information Processing
    Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS
    2022, New Orleans, LA, USA, November 28 - December 9, 2022*, 2022. URL [http://papers.nips.cc/paper_files/paper/2022/hash/c402501846f9fe03e2cac015b3f0e6b1-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/c402501846f9fe03e2cac015b3f0e6b1-Abstract-Conference.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Wenhao Yu, Zhihan Zhang, Zhenwen Liang, Meng Jiang, and Ashish
    Sabharwal. Improving language models via plug-and-play retrieval feedback. *CoRR*,
    abs/2305.14002, 2023. doi: 10.48550/ARXIV.2305.14002. URL [https://doi.org/10.48550/arXiv.2305.14002](https://doi.org/10.48550/arXiv.2305.14002).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level
    prompt engineers. In *The Eleventh International Conference on Learning Representations,
    ICLR 2023, Kigali, Rwanda, May 1-5, 2023*. OpenReview.net, 2023. URL [https://openreview.net/pdf?id=92gvk82DE-](https://openreview.net/pdf?id=92gvk82DE-).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Appendix A Efficiency of Different Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Token Consumption
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Model | Method | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Date | CSQA | StrategyQA | AuQA | SVAMP | GSM8K |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | CoT | 174K/19K | 959K/77K | 476K/67K | 178K/45K | 945K/76K | 1.3M/169K
    |'
  prefs: []
  type: TYPE_TB
- en: '| Complex | 169K/20K | 1.4M/81K | 833K/103K | 523K/46K | 2.5M/96K | 3.6M/195K
    |'
  prefs: []
  type: TYPE_TB
- en: '| ADPSC | 727K/86K | 6.1M/351K | 3.6M/490K | 2.7M/247K | 8.8M/261K | 14.3M/716K
    |'
  prefs: []
  type: TYPE_TB
- en: '| SC | 1.7M/194K | 14.4M/8.3M | 8.3M/1.1M | 5.2M/452K | 25.5M/703K | 36.3M/1.6M
    |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 501K/64K | 4.5M/263K | 2.4M/214K | 1.4M/122K | 6.6M/211K | 10.2M/469K
    |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | LeCo+CoT | 386K/35K | 2.0M/125K | 1.1M/127K | 406K/81K |
    1.9M/136K | 2.5M/337K |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeCo+Complex | 363K/35K | 3.0M/151K | 1.9M/182K | 1.2M/104K | 5.1M/170K
    | 8.2M/394K |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | CoT | 174K/19K | 959K/76K | 476K/58K | 178K/33K | 945K/72K | 1.3M/163K
    |'
  prefs: []
  type: TYPE_TB
- en: '| Complex | 169K/20K | 1.4M/77K | 833K/94K | 523K/40K | 2.5M/92K | 3.6M/177K
    |'
  prefs: []
  type: TYPE_TB
- en: '| ADPSC | 721K/92K | 6.2M/350K | 3.7M/466K | 3.0M/244K | 10.8M/318K | 14.1M/684K
    |'
  prefs: []
  type: TYPE_TB
- en: '| SC | 1.7M/209K | 14.4M/791K | 8.3M/1.0M | 5.2M/405K | 25.5M/701K | 36.3M/1.4M
    |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 393K/42K | 3.5M/186K | 2.3M/226K | 1.7M/134K | 9.1M/261K | 9.8M/475K
    |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-8 | LeCo+CoT | 357K/30K | 2.0M/110K | 999K/99K | 388K/58K | 1.9M/126K
    | 2.5M/326K |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeCo+Complex | 341K/34K | 3.0M/149K | 1.8M/167K | 1.2M/85K | 5.5M/168K
    | 7.4M/334K |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Average consumed in/out tokens with OpenAI models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | Count | Geometry | Iter | Num | Prealgebra | Precaculus |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | Complex | 2.9M/254K | 1.2M/96K | 1.2M/113K | 2.2M/295K | 1.3M/117K
    | 2.1M/146213 | 1.3M/165K |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 8.5M/701K | 3.7M/305K | 4.1M/321K | 7.7M/658K | 4.1M/392K | 6.9M/426K
    | 4.4M/491K |'
  prefs: []
  type: TYPE_TB
- en: '| ADPSC | 15.5M/1.5M | 6.2M/608K | 6.7M/744K | 15.0M/1.9M | 7.7M/721K | 14.7M/1.1M
    | 11.6M/1.5M |'
  prefs: []
  type: TYPE_TB
- en: '| SC | 28.9M/2.6M | 11.6M/934K | 12.0M/10.8M | 22.2M/2.7M | 13.1M/1.2M | 21.3M/1.5M
    | 13.5M/1.9M |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-9 | LeCo+Complex | 7.4M/627K | 3.3M/273K | 3.4M/309K | 6.9M/860K
    | 4.2M/349K | 5.5M/361K | 4.1M/483K |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | Complex | 2.9M/216K | 1.2M/86K | 1.2M/96K | 2.2M/241K | 13.1M/104K
    | 2.1M/124K | 1.3M/144K |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 10.4M/613K | 4.3M/267K | 4.6M/283K | 8.5M/626K | 4.9M/323K | 7.4M/325K
    | 5.0M/446K |'
  prefs: []
  type: TYPE_TB
- en: '| ADPSC | 16.7M/1.4M | 8.4M/692K | 8.3M/719K | 19.3M/2.1M | 10.1M/880K | 12.0M/786K
    | 11.4M/1.3M |'
  prefs: []
  type: TYPE_TB
- en: '| SC | 29.0M/1.9M | 11.6M/895K | 12.0M/1.1M | 22.2M/2.3M | 13.1M/1.1M | 21.4M/1.3M
    | 13.5M/1.5M |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-9 | LeCo+Complex | 7.4M/515K | 3.2M/227K | 3.5M/270K | 7.2M/720K
    | 3.6M/273K | 5.0M/274K | 4.2M/432K |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Average consumed in/out tokens on MATH dataset with OpenAI models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Methods | GSM8K | Math |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | Count | Geometry | Iter | Num | Prealgebra | Precaculus |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSeek | Complex | 3.8M/275K | 2.8M/376K | 1.1M/144K | 1.1M/159K | 2.1M/425K
    | 1.2M/189K | 2.0M/195K | 1.3M/272K |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-10 | LeCo+Complex | 8.7M/589K | 6.2M/878K | 2.7M/353K | 2.8M/410K
    | 5.4M/1.1M | 3.1M/458K | 4.6M/457k | 3.4M/708K |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Average consumed in/out tokens on MATH and GSM8K datasets with DeepSeek
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Average Iterations Numbers by Different Methods and Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [11](#A1.T11 "Table 11 ‣ A.2 Average Iterations Numbers by Different Methods
    and Models ‣ Appendix A Efficiency of Different Models ‣ Learning From Correctness
    Without Prompting Makes LLM Efficient Reasoner") and [12](#A1.T12 "Table 12 ‣
    A.2 Average Iterations Numbers by Different Methods and Models ‣ Appendix A Efficiency
    of Different Models ‣ Learning From Correctness Without Prompting Makes LLM Efficient
    Reasoner") presents the average iteration numbers on the arithmetic reasoning,
    commonsense reasoning, logical reasoning and complex mathematical reasoning using
    OpenAI models. Table [13](#A1.T13 "Table 13 ‣ A.2 Average Iterations Numbers by
    Different Methods and Models ‣ Appendix A Efficiency of Different Models ‣ Learning
    From Correctness Without Prompting Makes LLM Efficient Reasoner") illustrate the
    average iteration numbers on the GSM8K and MATH datasets using the DeepSeek model.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | Dataset | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Date | CSQA | StrategyQA | AuQA | SVAMP | GSM8K |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | ADPSC | 4.31 | 4.21 | 4.43 | 5.13 | 4.27 | 4.42 | 4.46 |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 2.39 | 2.90 | 2.57 | 3.67 | 2.56 | 2.35 | 2.74 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-9 | LeCo+CoT | 2.16 | 2.08 | 2.18 | 2.16 | 2.14 | 2.20 | 2.15
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeCo+Complex | 2.11 | 2.08 | 2.17 | 2.43 | 2.24 | 2.29 | 2.22 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | ADPSC | 4.28 | 4.32 | 4.56 | 5.44 | 4.39 | 4.21 | 4.53 |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 2.08 | 2.31 | 2.47 | 2.9 | 3.21 | 2.25 | 2.54 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-9 | LeCo+CoT | 2.00 | 2.02 | 2.05 | 2.08 | 2.05 | 2.05 | 2.04
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | LeCo+Complex | 2.01 | 2.05 | 2.08 | 2.24 | 2.13 | 2.08 | 2.10 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Average iterations on diverse datasets with OpenAI models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Method | Dataset | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | Count | Geometry | Iter | Num | Prealgebra | Precaculus |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | ADPSC | 5.36 | 5.92 | 6.21 | 5.84 | 6.76 | 5.59 | 6.36 | 6.01 |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 2.59 | 2.83 | 3.00 | 2.75 | 2.97 | 2.58 | 2.78 | 2.79 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-10 | LeCo+Complex | 2.52 | 2.83 | 2.81 | 2.91 | 2.78 | 2.42 |
    2.94 | 2.74 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | ADPSC | 6.44 | 7.22 | 5.91 | 7.70 | 8.63 | 5.03 | 8.38 | 7.04 |'
  prefs: []
  type: TYPE_TB
- en: '| RCI | 3.31 | 3.41 | 3.51 | 3.41 | 3.43 | 3.27 | 3.29 | 3.38 |'
  prefs: []
  type: TYPE_TB
- en: '| \cdashline2-10 | LeCo+Complex | 2.47 | 2.75 | 2.9 | 2.79 | 2.63 | 2.31 |
    2.81 | 2.66 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Average iterations on MATH dataset with OpenAI models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Methods | GSM8K | MATH | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| Algebra | Count | Geometry | Iter | Num | Prealgebra | Precaculus |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSeek | LeCo+Complex | 2.25 | 2.22 | 2.44 | 2.46 | 2.52 | 2.45 | 2.25
    | 2.59 | 2.40 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Average iterations on MATH and GSM8K datasets with DeepSeek model.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Details of Early Stop LeCo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Algorithm of Early stop LeCo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As presented in Algorithm [21](#alg2.l21 "In Algorithm 2 ‣ B.1 Algorithm of
    Early stop LeCo ‣ Appendix B Details of Early Stop LeCo ‣ Learning From Correctness
    Without Prompting Makes LLM Efficient Reasoner"), firstly, we sample the entire
    dataset according to a certain proportion, obtaining distributions of correct
    and incorrect solutions. Leveraging the normal distribution traits of incorrect
    responses, we utilize the positive 1-$\sigma$ value as the threshold. For the
    remaining data, if its solution score surpasses the threshold, we accept this
    answer outright; otherwise, we resort to the standard LeCo method for reconsideration.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Early Stop of LeCo
  prefs: []
  type: TYPE_NORMAL
- en: 1:input questions $x$ is correct then6:         $C\leftarrow C\cup sln\_score(y_{t_{s}})$
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Token Consumption and Iteration Number of Early Stop LeCo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [14](#A2.T14 "Table 14 ‣ B.2 Token Consumption and Iteration Number of
    Early Stop LeCo ‣ Appendix B Details of Early Stop LeCo ‣ Learning From Correctness
    Without Prompting Makes LLM Efficient Reasoner") and [15](#A2.T15 "Table 15 ‣
    B.2 Token Consumption and Iteration Number of Early Stop LeCo ‣ Appendix B Details
    of Early Stop LeCo ‣ Learning From Correctness Without Prompting Makes LLM Efficient
    Reasoner") presents the average token consumptions and average iteration numbers
    on the GSM8K and StrategyQA datasets using OpenAI models via early-stop LeCo.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Methods | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | StrategyQA |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo-0613 | Early Stop | 8.0M/367.6K | 1.7M/132.7K |'
  prefs: []
  type: TYPE_TB
- en: '| LeCo | 8.2M/393.8K | 1.9M/181.9K |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4 | Early Stop | 7.0M/315.7K | 1.7M/162.3K |'
  prefs: []
  type: TYPE_TB
- en: '| LeCo | 7.4M/334.2K | 1.8M/167.3K |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Average Token Consumption on GSM8K and StrategyQA of Early-stop LeCo'
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Methods | Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| GSM8K | StrategyQA |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-3.5-turbo-0613 | Early Stop | 2.16 | 2.11 |'
  prefs: []
  type: TYPE_TB
- en: '| LeCo | 2.39 | 2.17 |'
  prefs: []
  type: TYPE_TB
- en: '| gpt-4 | Early Stop | 2.03 | 2.06 |'
  prefs: []
  type: TYPE_TB
- en: '| LeCo | 2.08 | 2.08 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: Average Iterations on GSM8K and StrategyQA of Early-stop LeCo'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Case Study of LeCo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table [16](#A3.T16 "Table 16 ‣ Appendix C Case Study of LeCo ‣ Learning From
    Correctness Without Prompting Makes LLM Efficient Reasoner"), [17](#A3.T17 "Table
    17 ‣ Appendix C Case Study of LeCo ‣ Learning From Correctness Without Prompting
    Makes LLM Efficient Reasoner") and [18](#A3.T18 "Table 18 ‣ Appendix C Case Study
    of LeCo ‣ Learning From Correctness Without Prompting Makes LLM Efficient Reasoner")
    lists the specific cases of reasoning results via different methods on GSM8K,
    StrategyQA and MATH datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '| Question |'
  prefs: []
  type: TYPE_TB
- en: '&#124; A shoe store was having a weekend sale on a brand of popular tennis
    shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; On Friday the store sold 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The next day they sold double that number of shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; On the last day of the sale they sold one-half the amount that they
    did the day before, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; but six people returned their pairs because they didn’t fit. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; How many tennis shoes were sold by the end of the sale? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gold Answer | 50.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: On Friday, the store sold 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: The next day, they sold double that number, which is 14 x 2 =
    28 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: On the last day, they sold one-half the amount of the day before,
    which is 28 / 2 = 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: But 6 pairs were returned, so the total number of tennis shoes
    sold is 14 + 14 - 6 = 22 pairs. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: The answer is \boxed{22}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Complex |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: On Friday, the store sold 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: The next day, they sold double that number, so they sold 14 x
    2 = 28 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: On the last day, they sold one-half the amount they did the day
    before, so they sold 28 / 2 = 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: However, six people returned their pairs, so the total number
    of pairs sold is 14 + 14 - 6 = 22 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: Since each pair of tennis shoes represents one unit, the store
    sold 22 tennis shoes by the end of the sale. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: The answer is \boxed{22}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| RCI |'
  prefs: []
  type: TYPE_TB
- en: '&#124; The previous answer is incorrect because it only calculates the number
    of pairs of tennis shoes sold, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; not the total number of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; To find the total number of tennis shoes, we need to multiply the number
    of pairs by 2. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Let’s correct the answer: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: On Friday, the store sold 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: The next day, they sold double that number, so they sold 14 x
    2 = 28 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: On the last day, they sold one-half the amount they did the day
    before, so they sold 28 / 2 = 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: However, six people returned their pairs, so the total number
    of pairs sold is 14 + 14 - 6 = 22 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: Since each pair of tennis shoes represents two units (one for
    the left foot and one for the right foot), &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; the store sold 22 x 2 = 44 tennis shoes by the end of the sale. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The answer is \boxed{44}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ADPSC |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: On Friday, the store sold 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: On the next day, they sold double that number, so they sold 14
    * 2 = 28 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: On the last day, they sold half the amount they did the day before,
    so they sold 28 / 2 = 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: However, six people returned their pairs, so the total number
    of pairs sold is 14 + 14 - 6 = 22 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: Since each pair consists of two shoes, the total number of tennis
    shoes sold is 22 * 2 = 44 tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: The answer is 44. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SC |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: On Friday, the store sold 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: The next day, they sold double that number, so they sold 14 x
    2 = 28 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: On the last day, they sold one-half the amount as the day before,
    so they sold 28 / 2 = 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: However, 6 people returned their pairs, so the total number of
    pairs sold is 14 + 14 - 6 = 22 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: Since each pair of tennis shoes consists of 2 shoes, the total
    number of tennis shoes sold is 22 x 2 = 44 tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: The answer is 44. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| COT+LeCo |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: On Friday, the store sold 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: On Saturday, they sold double that number, so 14 x 2 = 28 pairs
    of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: On the last day, they sold one-half the amount of the day before,
    so 28 / 2 = 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: But 6 pairs were returned, so 14 - 6 = 8 pairs of tennis shoes
    were sold on the last day. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: In total, they sold 14 + 28 + 8 = 50 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: The answer is \boxed{50}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Complex+LeCo |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: On Friday, the store sold 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: The next day, they sold double that number, so they sold 14 x
    2 = 28 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: On the last day, they sold one-half the amount they did the day
    before, so they sold 28 / 2 = 14 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: However, six people returned their pairs, so the total number
    of tennis shoes sold is 14 + 28 + 14 - 6 = 50 pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: The answer is \boxed{50} pairs of tennis shoes. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 16: Case Study of LeCo on GSM8K by GPT-3.5-Turbo'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Could JPMorgan Chase give every American $10? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gold Answer | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| CoT |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 1: JPMorgan Chase is a bank. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 2: Banks have a lot of money. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 3: There are approximately 330 million Americans. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 4: $10multipliedby330millionis$3.3 billion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 5: JPMorgan Chase is a very large bank, but it may not have enough
    money to give $10 to every American. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 6: Thus, JPMorgan Chase may not be able to give every American
    $10\. Step 7: So the answer is \boxed{no}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Complex |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step. Step 1: JPMorgan Chase is a multinational
    investment bank and financial services company. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 2: JPMorgan Chase has a market capitalization of over $400 billion.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 3: The population of the United States is over 330 million. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 4: To give every American $10, JPMorgan Chase would need to distribute
    over $3.3 billion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 5: JPMorgan Chase’s market capitalization is significantly smaller
    than the amount needed to give every American $10. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 6: Thus, JPMorgan Chase could not give every American $10. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 7: So the answer is \boxed{no} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| RCI |'
  prefs: []
  type: TYPE_TB
- en: '&#124; I apologize for the mistake in my previous answer. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Upon reviewing the information, it is clear that JPMorgan Chase would
    not be able to give every American $10. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; The amount required to give $10 to each of the approximately 331 million
    Americans would be $3.31 billion, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; which is a significant amount even for a large financial institution
    like JPMorgan Chase. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Therefore, the correct answer is \boxed{no}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ADPSC |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 1: JPMorgan Chase is a multinational investment bank and financial
    services company. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 2: The current population of the United States is approximately
    331 million people. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 3: If JPMorgan Chase were to give every American $10, they would
    need to distribute a total of $3.31 billion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 4: JPMorgan Chase’s total assets as of 2020 were $3.42 trillion.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 5: $3.31 billion is a very small fraction of JPMorgan Chase’s total
    assets. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 6: Thus, JPMorgan Chase could give every American $10. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 7: So the answer is \boxed{yes}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SC |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 1: JPMorgan Chase is a multinational investment bank and financial
    services company. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 2: The current population of the United States is approximately
    331 million people. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 3: If JPMorgan Chase were to give every American $10, they would
    need to distribute a total of $3.31 billion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 4: JPMorgan Chase’s total assets as of 2020 were $3.42 trillion.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 5: $3.31 billion is a very small fraction of JPMorgan Chase’s total
    assets. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 6: Thus, JPMorgan Chase could give every American $10. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 7: So the answer is \boxed{yes} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| COT+LeCo |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 1: JPMorgan Chase is a bank. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 2: Banks have a lot of money. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: Thus, JPMorgan Chase could potentially give every American $10.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: So the answer is \boxed{yes} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Complex+LeCo |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step 1: JPMorgan Chase is a multinational investment bank and financial
    services company. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: JPMorgan Chase is one of the largest banks in the United States.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: The population of the United States is over 300 million people.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: To give every American $10, JPMorgan Chase would need to distribute
    over $3 billion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: JPMorgan Chase’s total assets are over $3 trillion. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: Thus, JPMorgan Chase could give every American $10. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step7: So the answer is \boxed{yes} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 17: Case Study of LeCo on StrategyQA by GPT-3.5-Turbo'
  prefs: []
  type: TYPE_NORMAL
- en: '| Question |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Five balls are numbered with the integers 1 through 5 and placed in
    a jar. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Three are drawn without replacement. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; What is the probability that the sum of the three integers on the balls
    is odd? &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Express your answer as a common fraction. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gold Answer | $\frac{2}{5}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Complex |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: There are two cases to consider: either all three integers are
    odd, or exactly two of the integers are odd. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: Case 1: If all three integers are odd, there are $\binom{3}{3}=1$
    ways to choose the odd integers. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: There are $\binom{5}{3}=10$ total ways to choose three integers
    from the five. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: Case 2: If exactly two of the integers are odd, there are $\binom{3}{2}=3$
    ways to choose the even integer. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: There are $\binom{5}{3}=10$ total ways to choose three integers
    from the five. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: So, the probability that the sum of the three integers is odd
    is $\frac{1+3\cdot 2}{10}=\frac{1+6}{10}=boxed\{\frac{7}{10}\}$. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step7: The answer is \boxed{$\frac{7}{10}$}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| RCI |'
  prefs: []
  type: TYPE_TB
- en: '&#124; A: Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: There are a total of $\binom{5}{3}=10$ ways to choose 3 balls
    out of 5. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: We need to count the number of ways to choose 3 balls such that
    the sum of their integers is odd. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: The sum of three integers will be odd if and only if there are
    an odd number of odd integers among the three chosen balls. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: There are $\binom{3}{1}=3$ ways to choose 1 odd integer and 2
    even integers. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: There are $\binom{2}{3}=0$ ways to choose 3 odd integers. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: There are $\binom{3}{2}=3$ ways to choose 2 odd integers and
    1 even integer. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step7: So, the total number of ways to choose 3 balls such that the
    sum of their integers is odd is $3+0+3=6$. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step8: Therefore, the probability is $\frac{6}{10}=boxed\{\frac{3}{5}\}$.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step9: The answer is \boxed{$\frac{3}{5}$}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ADPSC |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: There are two cases to consider: either all three integers are
    odd, or two are odd and one is even. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: Case 1: If all three integers are odd, there are $\binom{3}{3}=1$
    ways to choose the odd integers. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: There are $5$. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: Case 2: If two integers are odd and one is even, there are $\binom{3}{2}\cdot\binom{2}{1}=3\cdot
    2=6$ ways to choose the integers. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: There are $3$. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: The total probability is $\frac{1}{10}+\frac{3}{5}=\frac{1}{10}+\frac{6}{10}=\frac{7}{10}$.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step7: The answer is \boxed{$\frac{7}{10}$} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| SC |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: There are two cases to consider: either all three integers are
    odd, or two are odd and one is even. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: Case 1: If all three integers are odd, there are $\binom{3}{3}=1$
    ways to choose the odd integers. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: There are $5$. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: Case 2: If two integers are odd and one is even, there are $\binom{3}{2}\cdot\binom{2}{1}=3\cdot
    2=6$ ways to choose the integers. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: There are $3$. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: The total probability is $\frac{1}{10}+\frac{3}{5}=\frac{1}{10}+\frac{6}{10}=\frac{7}{10}$.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step7: The answer is \boxed{$\frac{7}{10}$} &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Complex+LeCo |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Let’s think step by step &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step1: There are two cases to consider: the sum is odd if either all
    three numbers are odd or two numbers are odd and one number is even. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step2: Case 1: All three numbers are odd. There are 3 odd numbers (1,
    3, 5) and 2 even numbers (2, 4) to choose from. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step3: The probability of choosing 3 odd numbers is $\frac{3}{5}\cdot\frac{2}{4}\cdot\frac{1}{3}=\frac{1}{10}$.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step4: Case 2: Two numbers are odd and one number is even. There are
    3 odd numbers and 2 even numbers to choose from. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step5: The probability of choosing 2 odd numbers and 1 even number is
    $\frac{3}{5}\cdot\frac{2}{4}\cdot\frac{2}{3}+\frac{3}{5}\cdot\frac{2}{4}\cdot\frac{2}{3}+\frac{3}{5}\cdot\frac{2}{4}\cdot\frac{2}{3}=\frac{3}{10}$.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step6: The total probability is $\frac{1}{10}+\frac{3}{10}=\/boxed\{\frac{2}{5}\}$.
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Step7: The answer is \boxed{$\frac{2}{5}$}. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 18: Case Study of LeCo on the MATH dataset using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
