- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:42:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: What Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt
    Engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.12334](https://ar5iv.labs.arxiv.org/html/2406.12334)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Federico Errica    Giuseppe Siracusano    Davide Sanvito    Roberto Bifulco
  prefs: []
  type: TYPE_NORMAL
- en: NEC Laboratories Europe
  prefs: []
  type: TYPE_NORMAL
- en: name.surname@neclab.eu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) changed the way we design and interact with software
    systems. Their ability to process and extract information from text has drastically
    improved productivity in a number of routine tasks. Developers that want to include
    these models in their software stack, however, face a dreadful challenge: debugging
    their inconsistent behavior across minor variations of the prompt. We therefore
    introduce two metrics for classification tasks, namely sensitivity and consistency,
    which are complementary to task performance. First, sensitivity measures changes
    of predictions across rephrasings of the prompt, and does not require access to
    ground truth labels. Instead, consistency measures how predictions vary across
    rephrasings for elements of the same class. We perform an empirical comparison
    of these metrics on text classification tasks, using them as guideline for understanding
    failure modes of the LLM. Our hope is that sensitivity and consistency will be
    powerful allies in automatic prompt engineering frameworks to obtain LLMs that
    balance robustness with performance.'
  prefs: []
  type: TYPE_NORMAL
- en: \mdtheorem
  prefs: []
  type: TYPE_NORMAL
- en: '[style=Takeaway]takeawayTakeaway'
  prefs: []
  type: TYPE_NORMAL
- en: What Did I Do Wrong?
  prefs: []
  type: TYPE_NORMAL
- en: Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering
  prefs: []
  type: TYPE_NORMAL
- en: Federico Errica  and Giuseppe Siracusano  and Davide Sanvito  and Roberto Bifulco
    NEC Laboratories Europe name.surname@neclab.eu
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are only two hard things in Computer Science: cache invalidation and
    naming things. - Phil Karlton'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This famous quote refers to the innate tendency of computer scientists to choose
    poor names for a program’s variables despite the existence of good coding practices
    (McConnell, [2004](#bib.bib20)). From a purely practical perspective, this is
    not a problem as long as the program does its job, but can we still argue the
    same in the era of Large Language Models?
  prefs: []
  type: TYPE_NORMAL
- en: LLMs (Brown et al., [2020](#bib.bib1)) have significantly changed how we process
    text by providing a straightforward interface, i.e., natural language, to define
    the problem to be solved (Devlin et al., [2019](#bib.bib6)). They provide software
    engineers with useful coding tips and can be used as part of larger and more complex
    software systems (Dakhel et al., [2023](#bib.bib4)). It is common to set up an
    LLM using a set of instructions called “prompt”, and it soon became clear to both
    researchers and developers that the prompt itself can greatly influence an LLM’s
    performance (Zhao et al., [2021](#bib.bib33); Sclar et al., [2024](#bib.bib25)).
    The process of writing a good prompt for the current task is called prompt engineering,
    and a great deal of different techniques have been proposed in this direction
    (Nori et al., [2023](#bib.bib22); Sahoo et al., [2024](#bib.bib24)), ranging from
    a simple description of the problem to providing few-shot examples.
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer to caption](img/46dac5a64bd7a6193ab91c062cb86728.png)![Refer
    to caption](img/1f1da7842735e5ba360beefabce162ae.png)![Refer to
    caption](img/1aa4b1bf707447c919fec010f0c69017.png)![Refer to
    caption](img/1325cde1daca610ed93f304f672957ca.png) Labels.schema()  "What is Australia’s
  prefs: []
  type: TYPE_NORMAL
- en: national flower?" Function Calling"Description" "Entity" Parser LLM replace ENTY = "An Entity"class Labels(str, enum.Enum):"""Enumeration for single-labeltext classification."""NUM = "Number"DESC = "Description"ENTY = "Entity"
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Real example of the behavior of GPT3.5 when classifying a question
    in terms of what it is referring to. A slight change in the definition of the
    class “ENTY” causes a minor prompt variation that disrupts the LLM’s prediction.
    This happens under the hood, making it very hard for a developer to debug the
    program. Note that the same might happen, for instance, if the ordering of the
    labels or the fields’ names is changed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From a practical standpoint, integrating LLMs in a software infrastructure
    introduces additional complexities, from choosing good prompt engineering strategies
    to parsing and controlling the output format of responses. In an effort to simplify
    the LLM into a straightforward function call, new software libraries like Instructor
    (Liu, [2024](#bib.bib16)) have emerged, reducing even further the entry barrier
    of LLMs for the ordinary programmer. Instructor requires to define target labels
    as static fields of a Python class, which is then automatically converted into
    a standardized JSON schema. The schema and the input sentence are then used to
    produce an LLM response via a mechanism known as Function Calling (Kang et al.,
    [2023](#bib.bib12)). Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ What Did I
    Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering")
    provides a real example of text classification using Instructor. Here, the assumption
    of the programmer should be that small changes to the labels’ descriptions, as
    well as the labels’ ordering in the code, should not affect the final result.
    Unfortunately, today’s reality is different: a change in the label definition,
    such as adding an article as shown in the figure, can lead to minor prompt variations
    with drastic changes in the final prediction. As a result, developers remain unaware
    of the malfunction’s cause and might later abandon the tool due to frustration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This simple yet troubling example might be generalized to other problems, for
    instance, code generation (Liu et al., [2023](#bib.bib17)); at the end of the
    day, LLM engineers would like to know whether it makes sense to spend their time
    modifying prompts to obtain a better-behaving LLM; perhaps the LLM does not change
    its predictions much no matter how the prompt is re-written. Similarly, an LLM
    whose predictions greatly vary depending on how the prompt is written might generally
    be regarded as unreliable in a production environment. Therefore, the question
    we want to address in this paper is the following: "how can we quantify the sensitivity
    of an LLM to variations of the prompt?". Existing works have answered this question
    by considering accuracy as the sole metric of interest (McCoy et al., [2023](#bib.bib21)),
    but this has a limited impact on the everyday life of developers and requires
    enough ground truth labels for the estimate to be reliable. As a matter of fact,
    with the recent progress in LLM agents (Gioacchini et al., [2024](#bib.bib7))
    and chain of thoughts (Wei et al., [2022](#bib.bib29)) techniques, the existence
    of multiple intermediate steps and/or user inputs, each handled somehow by an
    LLM, implies an exponential amount of potential failure paths. LLM engineers need
    a computationally feasible way to analyze each step individually, possibly irrespective
    of the final task they need to solve, to reduce the chances that something goes
    wrong along the way.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We set out to address these problems by proposing two diagnostic metrics for
    LLM classifiers that are complementary to accuracy: the sensitivity to the input,
    which does not depend on the ground truth labels, and the consistency of predictions
    across examples of the same class. Intuitively, it is desirable to have LLMs that
    are robust to semantically equivalent variations of the initial prompt, and their
    predicted labels’ distributions should not vary much across samples of the same
    class. Striving to improve these two metrics might significantly reduce the unpredictability
    of LLMs’ behavior in complex software systems running in production; low sensitivity
    and high consistency might mean that LLMs hallucinate less and have a better understanding
    of the tasks. Consider the example of Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ What Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt
    Engineering"), taken from our experiments, where the predicted labels’ distribution
    over samples of a given class completely changes for two semantically equivalent
    but syntactically different rephrasings of the prompt. This behavior is highly
    undesirable, and the first step to address it is to define suitable metrics to
    measure it.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/caec3083e6a002b1c6c6c365d8cb0cad.png)![Refer to caption](img/02d3af52d8179fddba296e6e855ecb59.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Comparison between predicted class distributions of Llama3 across
    samples of the same class (TREC dataset, Section [4](#S4 "4 Experiments ‣ What
    Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering")).
    A merely syntactic rephrasing of the prompt leads to an abrupt change in the distribution,
    suggesting low consistency.'
  prefs: []
  type: TYPE_NORMAL
- en: We evaluate these metrics on five different datasets, two closed and two open-source
    LLMs, and three prompting strategies, and we find that they indeed convey different
    information about the LLMs’ behavior. We use this information to analyze more
    qualitatively some of the datasets, mimicking the kind of study that a user or
    an LLM engineer would do when debugging their systems. We argue that these metrics
    should be included in all automatic prompt engineering/optimization frameworks,
    and perhaps in the training of LLMs themselves, to progress the development of
    trustworthy AI systems.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section positions our work in the context of three different but related
    research directions: influence of spurious features, uncertainty quantification,
    and prompt optimization. All these works fall under the broader umbrella of prompt
    engineering.'
  prefs: []
  type: TYPE_NORMAL
- en: Spurious Features.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is well-known that variations of the prompt affect stability of the LLM’s
    accuracy. In a broad study across diverse tasks, McCoy et al. ([2023](#bib.bib21))
    showed that the performance of the LLM depends on the likelihood of the input
    prompt and the correct output answer. Such result is consistent with the fact
    that LLMs are, in essence, autoregressive models trained to maximize a likelihood
    objective. In light of these considerations, it is unsurprising that simply changing
    the ordering of the examples in a few-shot prompting strategy can lead to almost
    random accuracy on sentiment analysis tasks (Zhao et al., [2021](#bib.bib33)).
    Similar considerations motivated frameworks like FormatSpread, which tries to
    predict the expected performance under prompt’s variations without accessing the
    LLM’s weights (Sclar et al., [2024](#bib.bib25)). Also, spurious features in the
    prompt had severe repercussions on the detection of security vulnerabilities,
    where LLMs are inconsistent and unfaithful (Ullah et al., [2024](#bib.bib27)).
    Finally, Yang et al. ([2024](#bib.bib31)) study the effect of prompt rephrasings
    and LLMs’ temperature on classification and uncertainty metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the solution to the spurious feature problem might also reside in
    a more structured approach: Retrieval Augmented Generation systems (Lewis et al.,
    [2020](#bib.bib15)) or Knowledge Graphs-enhanced LLMs (Luo et al., [2024](#bib.bib18))
    reduce hallucinations and dependencies on specific prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: Uncertainty Quantification.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Complementary to the discussion in this paper is the estimation of the LLM’s
    uncertainty. Typically, uncertainty is defined over the different answers of the
    LLM given the same prompt (Press et al., [2024](#bib.bib23)); being generative
    models of text, LLMs might produce contradicting predictions due to stochasticity
    in the output response. Several works have already investigated uncertainty: Kadavath
    et al. ([2022](#bib.bib11)) asked the LLM to provide a numerical score of confidence,
    whereas Chen et al. ([2024](#bib.bib2)) argued that aggregating an LLM’s responses
    via a majority voting mechanism reveals a non-monotonic relationship between the
    number of LLM calls and system performance. Motivated by the fact that higher
    uncertainty should imply lower performances, Huang et al. ([2024](#bib.bib8))
    define a rank calibration error to quantify deviations from the ideal relationship
    between the two quantities. In a completely different study with about 400 human
    participants, Kim et al. ([2024](#bib.bib13)) provide evidence that allowing an
    LLM to express uncertainty in the form of natural language reduces the users’
    trust in the system. The authors argue that reducing output uncertainty is crucial
    for LLMs’ successful adoption. Very recently, Yadkori et al. ([2024](#bib.bib30))
    proposed an information-theoretic metric to distinuish between epistemic uncertainty,
    which arises from lack of knowledge about the ground truth, and aleatoric uncertainty,
    which comes from irreducible randomness. They enable the identification of unreliable
    model outputs and hallucinations without modifying the training process.'
  prefs: []
  type: TYPE_NORMAL
- en: While related to our approach, it does not fall within the scope of this paper
    to further analyze output uncertainty. Because LLMs can also behave deterministically
    (or close to it) (Yang et al., [2024](#bib.bib31)), which limits the impact of
    such uncertainty quantification metrics, this work solely focuses on variations
    of the input prompts. For a comprehensive study of uncertainty quantification
    techniques on LLMs, we refer the reader to Huang et al. ([2023](#bib.bib9)).
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Optimization.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we discuss techniques to optimize prompts so that the LLMs improve
    their performance on the task. One cause for bad performances is the bias of the
    LLM towards samples of a class that appears frequently in the training data; for
    this reason, a model calibration technique has been proposed to make the predictions
    more uniform across answers (Zhao et al., [2021](#bib.bib33)). The approach requires
    access to the LLM’s inner workings, which might not be practical for developers.
    Automated Prompt Engineer (Zhou et al., [2023](#bib.bib35)) optimizes prompt instructions
    with the help of an LLM. This approach has also been applied to train the LLM
    itself, to reduce its susceptibility to adversarial attacks and jailbreaks that
    overcome the safeguards for ethical use of these systems (Zhou et al., [2024](#bib.bib34)).
    At the same time, there is still much work to do: LLM-based automatic prompt optimizers
    struggle to identify the true causes of errors, and we should rather focus on
    an automated behavior optimization paradigm (Ma et al., [2024](#bib.bib19)).'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt optimization is already a step ahead compared to the quantification of
    sensitivity, and we hope that the definition of suitable metrics will inspire
    new prompting strategies that do not focus on the sole evaluation of task accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We formally introduce the metrics of interest in a bottom-up fashion, namely
    sensitivity and consistency. We include uncertainty in the discussion to stress
    the differences between all quantities.
  prefs: []
  type: TYPE_NORMAL
- en: Let us consider a classification task $\tau$ under different variants of the
    same prompt as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{\tau}(y&#124;\bm{x})=\mathbb{E}_{\rho\sim p(\cdot)}[p(y&#124;\bm{x},\rho)].$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'In practice, we can approximate Equation [1](#S3.E1 "In 3 Methodology ‣ What
    Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering")
    by Monte Carlo integration, using $Q$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{\tau}(y&#124;\bm{x})\approx\frac{1}{Q}\sum_{i=1}^{Q}p(y&#124;\bm{x},\rho_{i}).$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: We then define the sensitivity of an LLM to the prompt to reflect how much the
    LLM prediction varies under the rephrasings of the original prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1  (Sensitivity).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given a categorical distribution $p_{\tau}(y|\bm{x})$ is its entropy
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle S_{\tau}(\bm{x})=-\mathbb{E}_{y\sim p_{\tau}(\cdot&#124;\bm{x})}[\ln
    p_{\tau}(y&#124;\bm{x})],$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: whereas the expected sensitivity is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle S_{\tau}=\mathbb{E}_{\bm{x}}[S_{\tau}(\bm{x})]\approx\frac{1}{N}\sum_{i=1}^{N}S_{\tau}(\bm{x}_{i}).$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'It is important to note that the sensitivity does not require access to ground
    truth labels, which are often hard to acquire, and it does not necessarily correlate
    with the task’s performance. The sensitivity should be used, for instance, as
    a guide to compare the “robustness” of different LLMs to variations of the prompt.
    A highly sensitive LLM may require significant prompt optimization efforts, whereas
    a less sensitive LLM tells us there might be no further room for improvement.
    The scope of the prompt variation is also important: one can measure sensitivity
    w.r.t. minor variations of the original prompt, as well as completely different
    prompting strategies as long as they convey a semantically equivalent instruction
    to the LLM. The interpretation we attribute to sensitivity ultimately depends
    on the use case, but it is universal.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second metric is called consistency. It measures how much the distribution
    of Equation [1](#S3.E1 "In 3 Methodology ‣ What Did I Do Wrong? Quantifying LLMs’
    Sensitivity and Consistency to Prompt Engineering") differs for two samples $\bm{x},\bm{x}^{\prime}$
    using the Total Variation Distance (TVD):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\text{TVD}(p,q)=\frac{1}{2}\sum_{c=1}^{C}&#124;p(c)-q(c)&#124;,$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: whose values range between $0$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2  (Consistency).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Given a categorical distribution $p_{\tau}(y|\bm{x})$, the pair-wise consistency
    of a classifier is measured as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle C_{y}(\bm{x},\bm{x}^{\prime})=1-\textnormal{TVD}(p_{\tau}(\cdot&#124;\bm{x}),p_{\tau}(\cdot&#124;\bm{x}^{\prime})],$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: whereas the expected consistency is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Intuitively, a consistent LLM produces similar distributions $p_{\tau}(\cdot|\bm{x})$
    .
  prefs: []
  type: TYPE_NORMAL
- en: We argue that, in order to avoid bad surprises in production environments where
    new LLMs have to be tested and replaced quickly, it might be desirable to select
    LLMs with low sensitivity and high consistency, which means $S_{\tau}\rightarrow
    0$. Jointly optimizing the prompt (or the LLM!) against accuracy, sensitivity,
    and consistency, might result in less hallucinations and a better understanding
    of the tasks’s context.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the difference of sensitivity and consistency with respect to the
    output uncertainty, defined by Huang et al. ([2024](#bib.bib8)) as the entropy
    of the LLM predictions for fixed input $\bm{x}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle U(\bm{x},\rho)=-\mathbb{E}_{y\sim p(\cdot&#124;\bm{x},\rho)}[\ln
    p(\cdot&#124;\bm{x},\rho)].$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: It is clear that the output uncertainty is orthogonal to sensitivity, and it
    treats the LLM as a stochastic predictor. However, in classification tasks it
    is reasonable to force a deterministic behavior of the LLM, effectively achieving
    $U(\bm{x},\rho)=0\ \forall\bm{x},\rho$. In contrast, as of today, there is no
    solution for achieving zero sensitivity with an LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of the experiments is to assess sensitivity and consistency of LLMs
    on different English-based text classification datasets, analyzing the impact
    of prompt variations and prompting strategies from a different angle compared
    to previous works, and showing developers how to use these metrics¹¹1[https://github.com/nec-research/sensitivity-consistency-LLM](https://github.com/nec-research/sensitivity-consistency-LLM)..
    We ran the LLMs on a 4 Tesla V100 server with 32 GBs of memory and 252 GBs of
    RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider Llama-3-70B-Instruct (Touvron et al., [2023](#bib.bib26)) and Mixtral-8x7B-Instruct-v0.1
    (Jiang et al., [2024](#bib.bib10)) as the two open-source LLMs available on our
    servers²²2Refer to [https://huggingface.co/models](https://huggingface.co/models).,
    as well as GPT3.5-turbo-0125 and GPT-4o-2024-05-13 (Brown et al., [2020](#bib.bib1))
    as closed-source models. The temperature is set to zero and the seed is fixed
    to 42 to obtain quasi-deterministic behavior. Following Zhao et al. ([2021](#bib.bib33)),
    five multiclass classification datasets are used for the comparison: TREC, a 6-class
    (and 50 subclasses) question-answering task (Voorhees and Tice, [2000](#bib.bib28))
    with 500 test samples, CommittmentBank (CB) as a 3-way classification problem
    (De Marneffe et al., [2019](#bib.bib5)) with 250 test samples, a binary textual
    entailment problem (RTE, Dagan et al. ([2005](#bib.bib3))) with 2490 test samples,
    and the 14-class ontology extraction dataset DBPedia (Zhang et al., [2015](#bib.bib32))
    with 2000 balanced test samples. In addition, we also consider the 7-class Web
    of Science 46985 (WoS) dataset with 2000 balanced test samples (Kowsari et al.,
    [2018](#bib.bib14)). In the event the LLM cannot produce a valid class, we add
    an extra class label N/A.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We consider three different prompting strategies: a simple strategy, where
    the prompt consists of the task description and the list of classes; a detail
    strategy, where we provide a detailed description of each class; and a 1-shot
    strategy in which, compared to simple, we also provide one example for each class
    taken from samples that do not belong to the test set. To build different rephrasings
    of the task description only, we use the aforementioned LLMs. In particular, the
    prompt asks to rephrase the task description by changing the length or adding
    unnecessary words as long as the meaning remains the same. As done in Yang et al.
    ([2024](#bib.bib31)), we choose $Q=10$ for completeness. Since the distributions
    of values are far from being Gaussian, we do not report standard deviations and
    refer to more qualitative analyses in later sections that show deviations from
    the mean values.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we want to answer two questions:'
  prefs: []
  type: TYPE_NORMAL
- en: i) are sensitivity and consistency complementary to accuracy metrics?; ii) how
    can we use them to fix prompts, LLMs, and choose the most suitable LLMs for a
    specific use case?
  prefs: []
  type: TYPE_NORMAL
- en: '|  | LLama3 | Mixtral |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Simple &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detail &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Simple &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detail &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| TREC | .250/.636/.846 | .285/.626/.816 | .148/.654/.836 | .388/.599/.730
    | .157/.674/.766 | .129/.666/.756 |'
  prefs: []
  type: TYPE_TB
- en: '| CB | .025/.639/.932 | .020/.637/.924 | .008/.644/.916 | .245/.563/.704 |
    .005/.640/.932 | .176/.652/.944 |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | .149/.590/.721 | .214/.591/.764 | .047/.588/.818 | .182/.581/.780 |
    .165/.584/.739 | .041/.587/.827 |'
  prefs: []
  type: TYPE_TB
- en: '| DBPedia | .031/.880/.947 | .097/.869/.868 | .047/.879/.936 | .156/.748/.875
    | .067/.872/.890 | .168/.759/.814 |'
  prefs: []
  type: TYPE_TB
- en: '| WoS | .076/.514/.642 | .059/.512/.641 | .062/.487/600 | .180/.482/.604 |
    .225/.481/.570 | .207/.466/.539 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT3.5 | GPT-4o |'
  prefs: []
  type: TYPE_TB
- en: '|  |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Simple &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detail &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Simple &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Detail &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $S_{\tau}$/F1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| TREC | .220/.580/.662 | .245/.553/.646 | .150/.611/.718 | .080/.676/.848
    | .115/.653/.794 | .086/.691/.854 |'
  prefs: []
  type: TYPE_TB
- en: '| CB | .349/.572/.688 | .404/.576/.680 | .346/.564/.740 | .115/.642/.920 |
    .118/.647/.900 | .098/.651/.920 |'
  prefs: []
  type: TYPE_TB
- en: '| RTE | .284/.570/.795 | .343/.571/.773 | .240/.566/.735 | .353/.614/.528 |
    .085/.612/.901 | .083/.619/.920 |'
  prefs: []
  type: TYPE_TB
- en: '| DBPedia | .040/.873/.955 | .074/.864/.906 | .047/.870/.949 | .061/.886/.914
    | .063/.879/.925 | .053/.878/.954 |'
  prefs: []
  type: TYPE_TB
- en: '| WoS | .200/.515/.632 | .190/.517/.619 | .196/.511/.653 | .097/.515/.658 |
    .093/.515/.658 | .090/.511/.660 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Sensitivity $S_{\tau}$ (higher is better) are shown for different
    datasets, models, and prompting strategies. Best values across open and closed-source
    models are shown in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To answer question i), Table [1](#S5.T1 "Table 1 ‣ 5 Results ‣ What Did I Do
    Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering") reports,
    for each model, dataset, and prompting strategy tried, the values of sensitivity,
    consistency, and micro F1 score. The first and most important observation is that
    there seems to be no consistent agreement between the proposed metrics across
    LLMs and prompting strategies. It is possible that when the F1 score is very high,
    as in the case of Llama3 with a Simple prompt, the sensitivity is almost double
    the one achieved with a 1-shot prompt. Similarly, although certainly more related
    to the task’s performance, the best consistency is rarely associated with the
    best sensitivity. This result shows that the empirical results agree with our
    intuitions about the utility of these metrics: a random predictor $p_{\tau}(y|\bm{x})$.
    All combinations are possible, and the sensitivity does not depend on the task.
    As a result, they provide different views about the behavior of the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, the Simple and Detail strategies seem to be more effective across
    all metrics when using Llama3 and GPT3.5, whereas the Detail and 1-shot ones work
    better for Mixtral and GPT-4o. This reminds us that developers should always pay
    attention when switching from one LLM to another in their application scenarios:
    a prompt that worked well with an LLM might cause instability (see the sensitivity
    gaps on CB) and significantly worse performance (up to 13% less on DBPedia) on
    another. For instance, on CB it might be preferable to choose the Detail strategy
    with Mixtral, since sensitivity is extremely low and the other metrics are close
    to the best values.'
  prefs: []
  type: TYPE_NORMAL
- en: '{takeaway}'
  prefs: []
  type: TYPE_NORMAL
- en: We observe no consistent pattern of best sensitivity, consistency, and F1; these
    metrics convey distinct information.
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/10fef68a82ce5bcf50ef55605098d258.png)![Refer to caption](img/e9fa04aded79e787e18122e1f5dc3d1e.png)![Refer
    to caption](img/2f602a4863d08e049a6fce93a9902314.png)![Refer to caption](img/2078b91c4627984049c6d132affa2640.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) TREC
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad508ea1c9ebeeaaedf5f6371d6bf65b.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) CB
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f0fa1e2436d62989be9ae5573523f62.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) DBPedia
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Top: we plot the sensitivity $S_{\tau}$ (Section [3](#S3 "3 Methodology
    ‣ What Did I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt
    Engineering")).'
  prefs: []
  type: TYPE_NORMAL
- en: We move on to answer question ii), by inspecting the sensitivity values against
    each class and prompting strategy. When some ground truth information is available,
    Figure [3](#S5.F3 "Figure 3 ‣ Sensitivity Analysis ‣ 5 Results ‣ What Did I Do
    Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering") (top)
    can give additional insights into the problematic classes of the task. Consistently
    with the example of Section [1](#S1 "1 Introduction ‣ What Did I Do Wrong? Quantifying
    LLMs’ Sensitivity and Consistency to Prompt Engineering"), we see that on TREC
    the classes Description, Entity, as well as Number are the ones with the highest
    sensitivity. This makes sense because the semantic meaning of these two classes
    can be ambiguous depending on the context. A similar result holds for CB, where
    the neutral statements are the hardest to classify; here Llama3 is more sensitive
    to rephrasings of the task description when it comes to neutral statements. This
    suggests that it might be worth increasing the number of few-shot examples for
    that class, for instance, or providing a better definition of a neutral statement
    in the prompt. On DBPedia, instead, it seems that Llama3 has sensitivity issues
    with the samples of class Artists Building, Animal, and Written Work; as future
    work, it would be interesting to understand if Llama3 has been trained on less
    text coming from these domains and relate it to sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: In Figure [3](#S5.F3 "Figure 3 ‣ Sensitivity Analysis ‣ 5 Results ‣ What Did
    I Do Wrong? Quantifying LLMs’ Sensitivity and Consistency to Prompt Engineering")
    (bottom), we visualize the distributions of sensitivity across samples and dataset,
    divided by prompting strategy. These distributions give us a better intuition
    of the behavior of the LLM compared to simply checking their mean values. For
    instance, Llama3 has an average TREC sensitivity of 0.148 with the 1-shot strategy,
    but there are still a non-negligible number of samples for which sensitivity is
    still very high. If a developer does not have access to ground truth labels at
    all, these samples can be manually inspected or given to labelers to identify
    classes that need more care. As a result, the labeling cost is reduced and the
    developer knows how to improve the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '{takeaway}'
  prefs: []
  type: TYPE_NORMAL
- en: Sensitivity can be used with or without ground truth labels to find “problematic”
    samples, revealing LLMs’ weak spots.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae9d557ce38839af56432ea537f71b45.png)![Refer to caption](img/3c8320c30de87325e2ab7366b755057d.png)![Refer
    to caption](img/3ffb555c78b57699f08670d03735ed02.png)![Refer to caption](img/0c2f3abbb5ac874e7fa43fdaeb20b27c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Person
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5908f7b84bfe11163926de531db1135f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Entity
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d3ab307924c022b1ca872e1a52f8637.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Description
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Top: we visualize the matrix of pairwise $C_{y}(\bm{x},\bm{x}^{\prime})$
    for three different TREC classes, using Llama3 as a classifier. Bottom: we build
    a histogram for each of the above matrices, to show the distribution of consistency
    across samples of a given class.'
  prefs: []
  type: TYPE_NORMAL
- en: As a reminder, the pair-wise consistency tells us how much the distribution
    $p_{\tau}(\cdot|\bm{x}),\bm{x}\in\mathcal{D}_{y}$ for three TREC classes, namely
    Person, Entity, and Description. Regarding the first class, we consistently observe
    high consistency except a few cases. By direct observation of the troublesome
    samples, we can devise prompting strategies targeted to them. In the case of the
    Entity class, the number of inconsistent pairs is higher than the number of consistent
    ones, whereby a batch of samples with IDs 46-50 are very consistent with each
    other but wholly inconsistent against all other samples, e.g., those with IDs
    51-58\. The former belong to different subclassees (color and other) than the
    latter (mostly animal), hence defining subclasses in the prompt might provide
    a better semantic definition of the the class itself. Finally, the matrix of class
    Description has mixed values, with most of the probability mass being assigned
    to consistency values smaller than 0.75\. Figure [4](#S5.F4 "Figure 4 ‣ Consistency
    Analysis ‣ 5 Results ‣ What Did I Do Wrong? Quantifying LLMs’ Sensitivity and
    Consistency to Prompt Engineering") (bottom) provides the histogram of these values
    that convey an aggregated view of these matrices. Compared to Figure [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ What Did I Do Wrong? Quantifying LLMs’ Sensitivity
    and Consistency to Prompt Engineering"), it is likely that effort spent improving
    consistency of the Entity class will also resolve the inconsistencies for the
    Description class, as these two are often confused by the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we analyze the distribution of consistency values across the different
    prompting strategies, which is shown in Figure [5](#S5.F5 "Figure 5 ‣ Consistency
    Analysis ‣ 5 Results ‣ What Did I Do Wrong? Quantifying LLMs’ Sensitivity and
    Consistency to Prompt Engineering"). There is an interesting pattern on TREC and
    WoS: the 1-shot strategy reduces the medium-level consistencies, but that does
    not necessarily imply an increase in all samples’ consistency. There are more
    pairs of values with consistency 0 compared to not using a Simple or Detail strategy.
    This is another reason why qualitative results are more helpful than mean values:
    the behavior of the LLM is counter-intuitive compared to what one would expect,
    e.g., that a prompting strategy as the few-shot can only increase the consistency.
    Care should be put when analyzing such behaviors, and the prompt should be adapted
    accordingly. As regards the CB dataset, the distributions look very similar except
    for Detail: apparently, providing a class clarification makes the model more sensitive
    about its predictions for some classes, and we have shown earlier how to debug
    such faulty behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/971c48248316be28aba0e644fe5789ce.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) TREC
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b5efe740036ffcd3951f2086be1fc56.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) DBPedia
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c434e8391f5c9ec408b2533206398d5b.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) WoS
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: We show the violin plot of the Llama3 consistency over samples of
    the same classes, arranged by prompting technique, on different datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '{takeaway}'
  prefs: []
  type: TYPE_NORMAL
- en: Consistency finds sample groups misclassified similarly. Tuning prompts to large
    groups offers cost-benefit trade-offs.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we have expanded the LLM developer’s toolbox with two diagnostic
    metrics, namely sensitivity and consistency, that complement performance metrics
    such as accuracy. In our experiment, we showed how different prompting strategies
    influence these metrics and how we can guide prompt engineering decisions based
    on new criteria that value intrinsic characteristics of LLMs’ predictions. Indeed,
    an LLM that is very sensitive to variation of the prompt and has high test set
    accuracy might not be a good choice in a production environment, where multiple
    intermediate steps exist and each could lead to minor alterations in the prompt
    of the LLM predictor. Notably, sensitivity does not require access to the ground
    truth labels, and it would be interesting in future work to extend it to tasks
    different from classification, for instance, code generation. Also, while this
    work mentioned prompt optimization, it is still an open question how to integrate
    these metrics in an automatic prompt engineering framework, leading to LLMs insensitive
    to nonsensical prompt variations and consistent in their (good) performances.
    As LLM will continue to be neural autoregressive models, and more generally pure
    neural networks, it is likely that the sensitivity to the input will remain a
    pain point of these architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first clear limitation of the proposed metrics is that they work for classification
    problems only; despite classification being a common task in information extraction,
    extending at least sensitivity to more general problems is an important future
    work. Another inherent limitation is the trade-off between the quality of the
    approximation used to compute sensitivity and consistency, based on the number
    $Q$ of different prompt rephrasings, and the cost to run the LLMs. In addition,
    while the sensitivity can be computed in the absence of ground-truth labels, our
    analyses revealed that expected values give limited information, and more insights
    can be gained when we have access to such ground truth, which allows us to understand
    class-wise sensitivity. Future work should investigate if higher moments, such
    as the variance, of the metrics we have proposed provide more information without
    having access to class labels.
  prefs: []
  type: TYPE_NORMAL
- en: 8 Ethical Considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our work proposes to evaluate LLMs according to metrics that gauge how well
    they perform when varying the input prompt and identify failure modes that need
    solving. For instance, we could use these metrics to discover that an LLM is more
    sensitive to a minority class than another, allowing us to solve the problem.
    Malicious attackers can use these metrics to understand if one LLM is more subject
    to jailbreaks than another, but they give no indication of how to do so. At the
    same time, making LLMs robust and optimized for these metrics will allow for more
    trustworthy AI systems that are harder to attack.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. In *Proceedings of
    the 34th Conference on Neural Information Processing Systems (NeurIPS)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2024) Lingjiao Chen, Jared Quincy Davis, Boris Hanin, Peter Bailis,
    Ion Stoica, Matei Zaharia, and James Zou. 2024. Are more llm calls all you need?
    towards scaling laws of compound inference systems. *arXiv preprint arXiv:2403.02419*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dagan et al. (2005) Ido Dagan, Oren Glickman, and Bernardo Magnini. 2005. The
    pascal recognising textual entailment challenge. In *Machine learning challenges
    workshop*. Springer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dakhel et al. (2023) Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam,
    Foutse Khomh, Michel C Desmarais, and Zhen Ming Jack Jiang. 2023. Github copilot
    ai pair programmer: Asset or liability? *Journal of Systems and Software*, 203.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'De Marneffe et al. (2019) Marie-Catherine De Marneffe, Mandy Simons, and Judith
    Tonhauser. 2019. The commitmentbank: Investigating projection in naturally occurring
    discourse. In *proceedings of Sinn und Bedeutung*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Annual Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies
    (NAACL)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gioacchini et al. (2024) Luca Gioacchini, Giuseppe Siracusano, Davide Sanvito,
    Kiril Gashteovski, David Friede, Roberto Bifulco, and Carolin Lawrence. 2024.
    Agentquest: A modular benchmark framework to measure progress and improve llm
    agents. In *Proceedings of the 2024 Annual Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies
    (NAACL)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2024) Xinmeng Huang, Shuo Li, Mengxin Yu, Matteo Sesia, Hamed
    Hassani, Insup Lee, Osbert Bastani, and Edgar Dobriban. 2024. Uncertainty in language
    models: Assessment through rank-calibration. *arXiv preprint arXiv:2404.03163*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2023) Yuheng Huang, Jiayang Song, Zhijie Wang, Huaming Chen,
    and Lei Ma. 2023. Look before you leap: An exploratory study of uncertainty measurement
    for large language models. *arXiv preprint arXiv:2307.10236*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. *arXiv preprint
    arXiv:2401.04088*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kadavath et al. (2022) Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan,
    Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma,
    Eli Tran-Johnson, et al. 2022. Language models (mostly) know what they know. *arXiv
    preprint arXiv:2207.05221*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2023) Sungmin Kang, Gabin An, and Shin Yoo. 2023. A preliminary
    evaluation of llm-based fault localization. *arXiv preprint arXiv:2308.05487*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2024) Sunnie SY Kim, Q Vera Liao, Mihaela Vorvoreanu, Stephanie
    Ballard, and Jennifer Wortman Vaughan. 2024. "i’m not sure, but…": Examining the
    impact of large language models’ uncertainty expression on user reliance and trust.
    *arXiv preprint arXiv:2405.00623*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kowsari et al. (2018) Kamran Kowsari, Donald Brown, Mojtaba Heidarysafa, Kiana
    Jafari Meimandi, Matthew Gerber, and Laura Barnes. 2018. Web of science dataset
    v6. *Mendeley Data*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive
    nlp tasks. In *Proceesings of the 34th Conference on Neural Information Processing
    Systems (NeurIPS)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu (2024) Jason Liu. 2024. Instructor. [https://github.com/jxnl/instructor](https://github.com/jxnl/instructor).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang.
    2023. Is your code generated by chatgpt really correct? rigorous evaluation of
    large language models for code generation. In *Proceedings of the 37th Conference
    on Neural Information Processing Systems (NeurIPS)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Luo et al. (2024) Linhao Luo, Yuan-Fang Li, Reza Haf, and Shirui Pan. 2024.
    Reasoning on graphs: Faithful and interpretable large language model reasoning.
    In *The 12th International Conference on Learning Representations (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ma et al. (2024) Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui,
    Qi Zhang, and Xuanjing Huang. 2024. Are large language models good prompt optimizers?
    *arXiv preprint arXiv:2402.02101*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McConnell (2004) Steve McConnell. 2004. *Code complete*. Pearson Education.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McCoy et al. (2023) R Thomas McCoy, Shunyu Yao, Dan Friedman, Matthew Hardy,
    and Thomas L Griffiths. 2023. Embers of autoregression: Understanding large language
    models through the problem they are trained to solve. *arXiv preprint arXiv:2309.13638*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nori et al. (2023) Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard
    Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu,
    et al. 2023. Can generalist foundation models outcompete special-purpose tuning?
    case study in medicine. *Medicine*, 84.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Press et al. (2024) Ori Press, Ravid Shwartz-Ziv, Yann LeCun, and Matthias
    Bethge. 2024. The entropy enigma: Success and failure of entropy minimization.
    *arXiv preprint arXiv:2405.05012*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sahoo et al. (2024) Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija
    Jain, Samrat Mondal, and Aman Chadha. 2024. A systematic survey of prompt engineering
    in large language models: Techniques and applications. *arXiv preprint arXiv:2402.07927*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sclar et al. (2024) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr.
    2024. Quantifying language models’ sensitivity to spurious features in prompt
    design or: How i learned to start worrying about prompt formatting. In *The 12th
    International Conference on Learning Representations (ICLR)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ullah et al. (2024) Saad Ullah, Mingji Han, Saurabh Pujar, Hammond Pearce,
    Ayse Coskun, and Gianluca Stringhini. 2024. Llms cannot reliably identify and
    reason about security vulnerabilities (yet?): A comprehensive evaluation, framework,
    and benchmarks. In *IEEE Symposium on Security and Privacy (SP)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voorhees and Tice (2000) Ellen M Voorhees and Dawn M Tice. 2000. Building a
    question answering test collection. In *Proceedings of the 23rd annual international
    ACM SIGIR conference on Research and development in information retrieval (SIGIR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. In *Proceesings of the 36th Conference on
    Neural Information Processing Systems (NeurIPS)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yadkori et al. (2024) Yasin Abbasi Yadkori, Ilja Kuzborskij, András György,
    and Csaba Szepesvári. 2024. To believe or not to believe your llm. *arXiv preprint
    arXiv:2406.02543*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2024) Adam Yang, Chen Chen, and Konstantinos Pitas. 2024. Just
    rephrase it! uncertainty estimation in closed-source language models via multiple
    rephrased queries. *arXiv preprint arXiv:2405.13907*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level
    convolutional networks for text classification. In *Proceedings of the 29th Conference
    on Neural Information Processing Systems (NeurIPS)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. 2021. Calibrate before use: Improving few-shot performance of language
    models. In *Proceedings of the 38th International Conference on Machine Learning
    (ICML)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2024) Andy Zhou, Bo Li, and Haohan Wang. 2024. Robust prompt optimization
    for defending language models against jailbreaking attacks. *arXiv preprint arXiv:2401.17263*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large language models are human-level
    prompt engineers. In *The 11th International Conference on Learning Representations
    (ICLR)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
