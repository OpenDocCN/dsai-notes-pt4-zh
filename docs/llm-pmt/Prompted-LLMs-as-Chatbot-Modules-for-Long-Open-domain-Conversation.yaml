- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:31'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Prompted LLMs as Chatbot Modules for Long Open-domain Conversation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.04533](https://ar5iv.labs.arxiv.org/html/2305.04533)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Gibbeum Lee ^(1*)   Volker Hartmann ^(1*)   Jongho Park^(1*)
  prefs: []
  type: TYPE_NORMAL
- en: Dimitris Papailiopoulos ^(1,2)   Kangwook Lee ^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹ KRAFTON   ² University of Wisconsin-Madison
  prefs: []
  type: TYPE_NORMAL
- en: '{pirensisco, volker, jongho.park}@krafton.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In this paper, we propose $\mathrm{MPC}$ is on par with fine-tuned chatbot models
    in open-domain conversations, making it an effective solution for creating consistent
    and engaging chatbots.
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: Equal Contributions'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Language models with billions of parameters, such as GPT-3 Brown et al. ([2020](#bib.bib1))
    and PaLM Chowdhery et al. ([2022](#bib.bib3)), have achieved state-of-the-art
    performance on many NLP tasks. To fine-tune these large language models (LLMs)
    for open-domain chatbot tasks, one could use a dataset of conversational data
    that is representative of the target domain. However, fine-tuning LLMs for open-domain
    chatbots can be challenging due to the computational burden of updating models
    with billions of parameters and the scarcity of data in the dialogue domain. Furthermore,
    fine-tuning can limit the model’s versatility by restricting it to a specific
    domain, and result in the loss of domain-agnostic knowledge acquired during pre-training,
    as reported by Yang and Ma ([2022](#bib.bib32)). Multi-task training on different
    datasets, as proposed by Roller et al. ([2021](#bib.bib19)), can address the versatility
    issue but has limitations, such as the need for data to train each skill and the
    difficulty determining the necessary skills for an open-domain chatbot. In fact,
    the growing number of modules for chatbots, as in Blenderbot3 (BB3) Shuster et al.
    ([2022](#bib.bib21)), points towards the increasing burden of data and computation
    when fine-tuning for each new chatbot model.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, some LLMs have the ability to perform in-context learning (ICL) Nye
    et al. ([2022](#bib.bib17)); Wei et al. ([2022b](#bib.bib29)); Lewkowycz et al.
    ([2022](#bib.bib13)); Wei et al. ([2022a](#bib.bib28)); Zhou et al. ([2022](#bib.bib36));
    Dasgupta et al. ([2022](#bib.bib5)); Chung et al. ([2022](#bib.bib4)). This capability
    enables the model to rapidly adapt to and execute a specific task based on a brief
    instruction and a few examples, without requiring additional fine-tuning. This
    can be utilized to create an open-domain chatbot, where a prompt describing a
    task required for open-domain dialogue and a few examples of solving such task
    can be provided to the LLM, allowing it to generate information that is pertinent
    to the current conversation.
  prefs: []
  type: TYPE_NORMAL
- en: Our Contributions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We present a novel approach for creating high-quality conversational agents
    without the need for fine-tuning. Our proposed chatbot, $\mathrm{MPC}$ is on par
    with or even preferred over fine-tuned LLMs, such as Blenderbot, in an open-domain
    conversational setting. This approach highlights the potential of pre-trained
    LLMs to adapt to new tasks without fine-tuning, providing an efficient solution
    for creating open-domain conversational agents. ¹¹1Our code is available in [https://github.com/krafton-ai/MPC](https://github.com/krafton-ai/MPC).
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Modular Prompting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Well-crafted elicitive prompts can enhance reasoning abilities, resulting in
    improved performance across various benchmarks Kojima et al. ([2022](#bib.bib11));
    Wei et al. ([2022b](#bib.bib29)); Suzgun et al. ([2022](#bib.bib23)). For complex
    problems, Press et al. ([2022](#bib.bib18)) identified the compositionality gap
    which arises when an LM can solve sub-problems but not the overall solution and
    further showed that CoT narrows this gap. Since then, there has been a flurry
    of work that solves tasks by decomposing them into smaller tasks solved by different
    “prompt modules” Zhou et al. ([2022](#bib.bib36)); Wang et al. ([2022](#bib.bib27));
    Khot et al. ([2022](#bib.bib9)); Khattab et al. ([2022](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: Modular prompting has found use beyond benchmarks and in conversation generation.
    Kim et al. ([2022](#bib.bib10)) used an LLM to generate a socially diverse dialogue
    dataset that is more natural and detailed than existing crowdsourced datasets.
    Moreover, hierarchical prompt modules prove to help long-range coherence for generating
    narratives and plays Yang et al. ([2022](#bib.bib33)); Mirowski et al. ([2022](#bib.bib16)).
    We refer to Mialon et al. ([2023](#bib.bib15)) for a detailed overview on such
    augmented uses of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Open-domain Chatbots
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many recent dialogue agents rely on dialogue-finetuned LLMs. In Thoppilan et al.
    ([2022](#bib.bib24)), LaMDa has been trained on large amounts of crawled conversational
    data and has used a fine-tuned classifier for model safety. More recently, similar
    to our modularization approach, BB3 fine-tunes Open Pre-trained Transformers (OPT)
     Zhang et al. ([2022](#bib.bib35)); Shuster et al. ([2022](#bib.bib21)) on QA
    and dialogue datasets and uses one shared model weight as multiple modules.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand,  Madotto et al. ([2021](#bib.bib14)) eliminate the need for
    fine-tuning on dialogue data by feeding retrieved dialogue samples as few-shot
    for GPT-J Wang and Komatsuzaki ([2021](#bib.bib26)). We find this work to be complementary
    to our work, as the few-shot dialogue can be seen as an approach to enhance the
    utterance generator module.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Multi-Session Chat dataset Xu et al. ([2022](#bib.bib31)) allows for measuring
    how well conversational agents maintain a long-term memory of facts about the
    user and bot. Information is retrieved using Dense Passage Retriever (DPR) Karpukhin
    et al. ([2020](#bib.bib7)), while BART compresses memories before storing them.
    In Shuster et al. ([2022](#bib.bib21)), a modular approach is used to incorporate
    long-term memory and factual grounding through internet search with an LLM. This
    work is closest to our work since it includes an ablation study in which prompt-based
    modules are compared with fine-tuned modules. However, in our work, we argue that
    more reasoning-based prompting, as demonstrated in Wei et al. ([2022b](#bib.bib29)),
    is beneficial for better contextual understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Modular Prompted Chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present a modular chatbot system (Fig. [1](#S3.F1 "Figure 1 ‣ 3 Modular Prompted
    Chatbot ‣ Prompted LLMs as Chatbot Modules for Long Open-domain Conversation"))
    that uses prompt-based LLMs to maintain persona and engagement throughout long-term
    conversations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb0ede5bd07f0119af1c54adcd74b522.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Our modular design for improving long-term consistency in open-domain
    conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: At the start of a conversation, a pre-defined persona is stored in the memory
    pool. When a user sends a message, the clarifier rephrases it to resolve any ambiguities
    and passes it to the DPR model which retrieves relevant memories from the memory
    pool. The retrieved memories and clarifier output are fed into the memory processor
    to get a single context-relevant memory, which is then passed to an utterance
    generator for producing a response from the chatbot. Every few turns, we call
    upon a summarizer module to extract important information from dialogue and store
    it in the memory pool for future use (see Appendices [C](#A3 "Appendix C Few-shot
    Prompts ‣ Prompted LLMs as Chatbot Modules for Long Open-domain Conversation")
    and [D](#A4 "Appendix D Utterance Generator Prompt Examples ‣ Prompted LLMs as
    Chatbot Modules for Long Open-domain Conversation")).
  prefs: []
  type: TYPE_NORMAL
- en: Utterance Clarifier
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As conversations are often muddled with vague coreferences and contextual cues,
    our clarifier module is an LM prompted with the recent dialogue to resolve any
    ambiguities. For instance, depending on prior context, the user input “Do you
    like working there?” would output “Does Sarah like working at ZYX company?”. By
    resolving contextual ambiguity, the clarifier assists the DPR model and memory
    processor module by providing an information-dense query to fetch and process
    relevant memories.
  prefs: []
  type: TYPE_NORMAL
- en: Memory Processor
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As demonstrated in Fig. [1](#S3.F1 "Figure 1 ‣ 3 Modular Prompted Chatbot ‣
    Prompted LLMs as Chatbot Modules for Long Open-domain Conversation"), we formulate
    memory processing as an LLM reasoning task of finding the most relevant information
    given the dialogue. Following the footsteps in solving hard reasoning tasks Suzgun
    et al. ([2022](#bib.bib23)), we provide CoT examples to show reasons for ignoring
    certain memories and synthesizing others. For models incapable of CoT, we simply
    provide the few-shot examples without the reasoning portion.
  prefs: []
  type: TYPE_NORMAL
- en: Since the memory pool accumulates as the conversation progresses, we use a pre-trained
    DPR with the output of the clarifier as the query to retrieve the top-$k$ most
    relevant memories from the memory pool. The memory processor then condenses the
    top memories into one refined memory.
  prefs: []
  type: TYPE_NORMAL
- en: Utterance Generator
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The utterance generator module generates the final response of the chatbot given
    the recent dialogue history and memory provided by the memory processor. The prompt
    consists of the dialogue history, condensed memory, and the generation instruction
    (e.g., “Give a friendly response to the user.”). For some models, we find that
    inserting the generation instruction at the end was helpful as placing it before
    the dialogue minimizes the effect of the instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Dialogue Summarizer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We provide a few-shot prompt to ensure we record specific details of the conversation
    and the user.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We evaluate our chatbot’s performance by assessing core skills necessary for
    long-term conversations. We assess consistency by assigning one of five personas,
    each with 12 facts from PersonaChat  Zhang et al. ([2018](#bib.bib34)), and presenting
    these facts to evaluators. For each experiment, we collect 20 turns from each
    evaluator and at least 500 turns in total from two subgroups: Amazon Mechanical
    Turk and university students. See Appendix [E](#A5 "Appendix E Data Collection
    Details ‣ Prompted LLMs as Chatbot Modules for Long Open-domain Conversation")
    for a detailed explanation of our data collection.'
  prefs: []
  type: TYPE_NORMAL
- en: In our setup, there are four groups of models.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuned chatbot models such as BB3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Vanilla is an utterance generator that either prepends full persona or no persona
    to the dialogue history in the prompt. This represents the naive approach of using
    an LM as a chatbot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $\mathrm{MPC}$ is as described in Section [3](#S3 "3 Modular Prompted Chatbot
    ‣ Prompted LLMs as Chatbot Modules for Long Open-domain Conversation"). Specifically,
    we only form one memory from the memory processor. Full persona is not explicitly
    prepended.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: $\mathrm{MPC}$ that prepends the full persona. See examples in Appendix [D](#A4
    "Appendix D Utterance Generator Prompt Examples ‣ Prompted LLMs as Chatbot Modules
    for Long Open-domain Conversation").
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.1 Single Model Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluate each model separately using Sensibleness, Consistency, and Engagingness
    metrics and collect a final rating (out of 5.0). The exact questions and evaluation
    forms are in Appendix  [A](#A1 "Appendix A Evaluation Details ‣ Prompted LLMs
    as Chatbot Modules for Long Open-domain Conversation"). We also report two types
    of combined score SCE (Sensible Consistent and Engaging): a "perfect" score SCE-p,
    where all metrics must be positive for a positive response, and the weighted score
    SCE-w, which is similar to SSI  Thoppilan et al. ([2022](#bib.bib24)) and reported
    in Appendix  [B](#A2 "Appendix B Experimental Results ‣ Prompted LLMs as Chatbot
    Modules for Long Open-domain Conversation").'
  prefs: []
  type: TYPE_NORMAL
- en: We use OpenAI GPT-3 text-davinci-002 (td2) and davinci, OPT 30B, 66B, GPT-JT-6B Together
    ([2022](#bib.bib25)), and BLOOM-176B  Scao et al. ([2022](#bib.bib20)) as base
    LMs for $\mathrm{MPC}$. For fine-tuned group, we use BB3-30B with the same persona
    settings. For BB3-175B, we request crowdworkers to evaluate the online demo for
    20 turns.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Pairwise Models Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We A/B test two chatbot models by providing the user with two randomized responses
    A and B. The user then evaluates them based on Sensibleness, Consistency, Interestingness,
    and Preference. The conversation then continues with the response chosen for Preference.
    This lets us to control for dialogue history when comparing two models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we conduct two main experiments: (1) $\mathrm{MPC}_{\mathrm{OPT\text{-}30B}}$
    vs. Vanilla td2 (full persona).'
  prefs: []
  type: TYPE_NORMAL
- en: Implicit Persona
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In reality, we implicitly learn about someone through dialogue. In contrast,
    our previous experiments show explicit persona to both evaluators and models.
    As such, we devise an experiment by providing a 10-turn pre-defined dialogue to
    the crowdworker and pairwise models, $\mathrm{MPC}_{\mathrm{td2}}$ and Vanilla
    td2 (no persona). We then ask workers to ask about the previous dialogue for 6
    new turns. Here, we set a shorter maximum context length than the 10-turn dialogue,
    so that the setup represents long conversations where necessary information is
    beyond the LM context length.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pre-trained vs. Fine-tuned
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '| $\mathrm{MPC}_{\mathrm{OPT\text{-}30B}}$ | Tie | BB3-30B |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sensibleness | 45.0 | 32.0 | 23.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency | 31.3 | 34.1 | 34.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Interestingness | 40.9 | 21.0 | 38.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Preference | 50.0 | 9.7 | 40.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Pairwise evaluation of $\mathrm{MPC}_{\mathrm{OPT\text{-}30B}}$.)'
  prefs: []
  type: TYPE_NORMAL
- en: Our human evaluations show that $\mathrm{MPC}$ models in Table. [2](#S5.T2 "Table
    2 ‣ Pre-trained vs. Fine-tuned ‣ 5 Results ‣ Prompted LLMs as Chatbot Modules
    for Long Open-domain Conversation") demonstrates superior performance to BB3-30B.
  prefs: []
  type: TYPE_NORMAL
- en: For BB3-30B, we have observed issues of consecutive utterance repetition. We
    report the evaluation results of only including dialogues without repetition in
    Table [5](#A1.T5 "Table 5 ‣ A.3 Dense Passage Retriever ‣ Appendix A Evaluation
    Details ‣ Prompted LLMs as Chatbot Modules for Long Open-domain Conversation").
    Even without repetition, $\mathrm{MPC}_{\mathrm{OPT\text{-}30B}}$ in Table [1](#S5.T1
    "Table 1 ‣ Pre-trained vs. Fine-tuned ‣ 5 Results ‣ Prompted LLMs as Chatbot Modules
    for Long Open-domain Conversation") shows higher sensibleness and preference than
    BB3-30B, while scoring similarly in consistency and interestingness.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Sens. | Cons. | Eng. | SCE-p | Rating |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| BB3-30B | 71.3 | 77.8 | 73.7 | 54.3 | 2.9 |'
  prefs: []
  type: TYPE_TB
- en: '| BB3-175B | 85.9 | (88.7) | 84.8 | 73.1 | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Full persona |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| td2 | 94.0 | 94.7 | 84.3 | 79.7 | 4.1 |'
  prefs: []
  type: TYPE_TB
- en: '| davinci | 91.8 | 89.2 | 78.8 | 70.8 | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathrm{MPC}$ |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| td2 | 93.6 | 87.8 | 85.5 | 75.0 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: '| davinci | 80.2 | 72.0 | 69.1 | 53.3 | 3.1 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-66B | 90.5 | 84.8 | 88.1 | 73.9 | 4.1 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-30B | 86.1 | 79.1 | 80.7 | 63.4 | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-JT | 91.1 | 83.2 | 65.3 | 53.5 | 3.1 |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM | 65.2 | 65.5 | 61.4 | 40.5 | 2.8 |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathrm{MPC}$+Full |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| td2 | 94.4 | 92.2 | 92.8 | 83.0 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-30B | 85.6 | 87.2 | 89.0 | 72.6 | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Single model evaluations of baselines and $\mathrm{MPC}$s. The top-3
    scores for each metric are highlighted, while the highest score is bolded. See
    Appendix [B](#A2 "Appendix B Experimental Results ‣ Prompted LLMs as Chatbot Modules
    for Long Open-domain Conversation") Table [5](#A1.T5 "Table 5 ‣ A.3 Dense Passage
    Retriever ‣ Appendix A Evaluation Details ‣ Prompted LLMs as Chatbot Modules for
    Long Open-domain Conversation") for more detailed results.'
  prefs: []
  type: TYPE_NORMAL
- en: Modular vs. Non-modular
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: $\mathrm{MPC}$, it shows lower consistency than td2 (full persona), albeit the
    high rates of ties in sensibleness and consistency (Table [4](#S5.T4 "Table 4
    ‣ Modular vs. Non-modular ‣ 5 Results ‣ Prompted LLMs as Chatbot Modules for Long
    Open-domain Conversation")). In general, we find that users would ask primarily
    about the bot’s persona rather than having a two-sided conversation, leading to
    td2 (full persona) performing better in consistency.
  prefs: []
  type: TYPE_NORMAL
- en: '| $\mathrm{MPC}_{\mathrm{td2}}$ | Tie | td2 (no persona) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sensibleness | 40.6 | 46.1 | 13.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency | 57.2 | 28.9 | 13.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Interestingness | 47.2 | 31.1 | 21.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Preference | 67.2 | 10.6 | 22.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Implicit persona experiment for $\mathrm{MPC}_{\mathrm{td2}}$)'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\mathrm{MPC}_{\mathrm{td2}}$ | Tie | td2 (full persona) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Sensibleness | 27.5 | 42.6 | 29.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Consistency | 24.4 | 44.7 | 30.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Interestingness | 40.7 | 26.2 | 33.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Preference | 42.8 | 15.9 | 41.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: $\mathrm{MPC}_{\mathrm{td2}}$)'
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Size
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When other variables are held the same, we observe model size is positively
    correlated with positive evaluations. The most compelling evidence can be seen
    in the superiority of $\mathrm{MPC}_{\mathrm{OPT\text{-}66B}}$, one of the largest
    models, scores the lowest in our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Effect of Instruction-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instruction-tuning helps the creation of a modular dialogue system by enabling
    adaptation to various tasks. Not only does $\mathrm{MPC}_{\mathrm{td2}}$.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We demonstrated that a modular approach using LLMs, namely $\mathrm{MPC}$ to
    fine-tuned and vanilla LM baselines and found that our approach achieved superior
    performance by human evaluation. Additionally, our modular system incorporated
    persona and information from dialogue history more effectively than the non-modular
    ones according to our consistency evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we investigate the use of pre-trained language models for long-term
    English conversations. While we expect a modular approach may be effective for
    other languages when given a capable language model, it should also be noted that
    further research is needed to confirm the applicability of our findings to other
    languages. For instance, though BLOOM is trained as a multilingual language model,
    we only implement $\mathrm{MPC}_{\mathrm{BLOOM}}$ in English and evaluate its
    English capability as a open-domain dialogue agent.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, a modular system can create additional inference overhead or error
    accumulation. The system performance would become much better if we optimally
    choose the LM for each module. For example, we could use GPT-3 td2 for the memory
    processor, while we employ OPT-175B for the utterance generator. We would need
    to evaluate every module to find the best model for each, which we leave to future
    work.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of evaluation methodology, our human evaluations of $\mathrm{MPC}$
    and its analysis face the same challenges as previous studies on evaluating interactive
    conversational tasks. As demonstrated by  Smith et al. ([2022](#bib.bib22)), there
    is currently no definitive evaluation method for determining the best chatbot
    model. Additionally, there are several factors that must be taken into account
    during data collection and interpretation, such as annotator subjectivity, instruction
    bias, and crowdworker working conditions. For a more in-depth discussion of human-LM
    interaction, we refer the reader to Lee et al. ([2022](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: 'As described in Appendix [E](#A5 "Appendix E Data Collection Details ‣ Prompted
    LLMs as Chatbot Modules for Long Open-domain Conversation"), to gather a diverse
    range of evaluations, we have collected qualitative data from two groups: English-speaking
    annotators on Amazon Mechanical Turk (MTurk), and qualified university students
    who were capable of speaking English. To some extent, this evaluation setup reduces
    cultural bias and platform homogeneity compared to using MTurk workers alone.
    However, the limitations of this approach should be acknowledged and this may
    further complicate the analysis when controlling for $\mathrm{MPC}$’s performance
    on different subgroups.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we note that running $\mathrm{MPC}$ infeasible to even load on a single
    node for heavy models such as BLOOM-176B.
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: $\mathrm{MPC}$ utilizes publicly available pre-trained LMs for chatbot utterance
    generation. Language generation from these LMs is known to have concerns about
    toxicity and bias Xu et al. ([2020](#bib.bib30)). Thus, ensuring safe deployment
    and interaction is a necessity.
  prefs: []
  type: TYPE_NORMAL
- en: Accordingly, we outline our data collection procedure in Appendix [E](#A5 "Appendix
    E Data Collection Details ‣ Prompted LLMs as Chatbot Modules for Long Open-domain
    Conversation"). We allow crowdworkers to directly provide us with feedback and
    also manually check for any offensive or controversial outputs. To ensure the
    protection of personal information, all crowdworkers were instructed not to share
    any personally identifiable or private information. Additionally, they were asked
    to give their consent for the collection of anonymous information for research
    purposes. Prior to participating, all workers were informed of the purpose of
    data collection and, after evaluation, were compensated with a competitive hourly
    rate, approximately $12-16 per hour.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Yulong Chen, Yang Liu, Liang Chen, and Yue Zhang. 2021.
    Dialogsum: A real-life scenario dialogue summarization dataset. In *Findings of
    the Association for Computational Linguistics: ACL-IJCNLP 2021*, pages 5062–5074.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dasgupta et al. (2022) Ishita Dasgupta, Andrew K Lampinen, Stephanie CY Chan,
    Antonia Creswell, Dharshan Kumaran, James L McClelland, and Felix Hill. 2022.
    Language models show human-like content effects on reasoning. *arXiv e-prints*,
    pages arXiv–2207.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2021) Song Feng, Siva Sankalp Patel, Hui Wan, and Sachindra Joshi.
    2021. Multidoc2dial: Modeling dialogues grounded in multiple documents. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    6162–6176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karpukhin et al. (2020) Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick
    Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage
    retrieval for open-domain question answering. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 6769–6781.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khattab et al. (2022) Omar Khattab, Keshav Santhanam, Xiang Lisa Li, David
    Hall, Percy Liang, Christopher Potts, and Matei Zaharia. 2022. Demonstrate-search-predict:
    Composing retrieval and language models for knowledge-intensive nlp. *arXiv preprint
    arXiv:2212.14024*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khot et al. (2022) Tushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle
    Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular
    approach for solving complex tasks. *arXiv preprint arXiv:2210.02406*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2022) Hyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu, Youngjae
    Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee Kim, Maarten Sap, et al.
    2022. Soda: Million-scale dialogue distillation with social commonsense contextualization.
    *arXiv preprint arXiv:2212.10465*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *arXiv preprint arXiv:2205.11916*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2022) Mina Lee, Megha Srivastava, Amelia Hardy, John Thickstun,
    Esin Durmus, Ashwin Paranjape, Ines Gerard-Ursin, Xiang Lisa Li, Faisal Ladhak,
    Frieda Rong, et al. 2022. Evaluating human-language model interaction. *arXiv
    preprint arXiv:2212.09746*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Johan Andreassen, David Dohan,
    Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil,
    Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems
    with language models. In *Advances in Neural Information Processing Systems*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madotto et al. (2021) Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, and
    Pascale Fung. 2021. Few-shot bot: Prompt-based learning for dialogue systems.
    *arXiv preprint arXiv:2110.08118*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mialon et al. (2023) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos
    Nalmpantis, Ram Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane
    Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented language models: a survey.
    *arXiv preprint arXiv:2302.07842*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mirowski et al. (2022) Piotr Mirowski, Kory W Mathewson, Jaylen Pittman, and
    Richard Evans. 2022. Co-writing screenplays and theatre scripts with language
    models: An evaluation by industry professionals. *arXiv preprint arXiv:2209.14958*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nye et al. (2022) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, et al. 2022. Show your work: Scratchpads for intermediate computation
    with language models. In *Deep Learning for Code Workshop*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Press et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A
    Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap
    in language models. *arXiv preprint arXiv:2210.03350*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roller et al. (2021) Stephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary
    Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau,
    et al. 2021. Recipes for building an open-domain chatbot. In *Proceedings of the
    16th Conference of the European Chapter of the Association for Computational Linguistics:
    Main Volume*, pages 300–325.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuster et al. (2022) Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael
    Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al.
    2022. Blenderbot 3: a deployed conversational agent that continually learns to
    responsibly engage. *arXiv preprint arXiv:2208.03188*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith et al. (2022) Eric Smith, Orion Hsu, Rebecca Qian, Stephen Roller, Y-Lan
    Boureau, and Jason Weston. 2022. Human evaluation of conversations is an open
    problem: comparing the sensitivity of various methods for evaluating dialogue
    agents. In *Proceedings of the 4th Workshop on NLP for Conversational AI*, pages
    77–97.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
    Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can
    solve them. *arXiv preprint arXiv:2210.09261*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, et al. 2022. Lamda: Language models for dialog applications. *arXiv preprint
    arXiv:2201.08239*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Together (2022) Together. 2022. GPT-JT-6B-v1. [https://huggingface.co/togethercomputer/GPT-JT-6B-v1](https://huggingface.co/togethercomputer/GPT-JT-6B-v1).
    Accessed: 2022-12-15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. 2021. GPT-J-6B:
    A 6 Billion Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Boshi Wang, Xiang Deng, and Huan Sun. 2022. Iteratively prompt
    pre-trained language models for chain of thought. In *Proceedings of the 2022
    Conference on Empirical Methods in Natural Language Processing*, pages 2714–2730.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. 2022a. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. 2022b. Chain of thought prompting elicits reasoning in
    large language models. *arXiv preprint arXiv:2201.11903*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and
    Emily Dinan. 2020. Recipes for safety in open-domain chatbots. *arXiv preprint
    arXiv:2010.07079*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2022) Jing Xu, Arthur Szlam, and Jason Weston. 2022. Beyond goldfish
    memory: Long-term open-domain conversation. In *Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*,
    pages 5180–5197.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Ma (2022) Chenghao Yang and Xuezhe Ma. 2022. Improving stability of
    fine-tuning pretrained language models via component-wise gradient norm clipping.
    *arXiv preprint arXiv:2210.10325*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022) Kevin Yang, Nanyun Peng, Yuandong Tian, and Dan Klein. 2022.
    Re3: Generating longer stories with recursive reprompting and revision. *arXiv
    preprint arXiv:2210.06774*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018) Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam,
    Douwe Kiela, and Jason Weston. 2018. Personalizing dialogue agents: I have a dog,
    do you have pets too? In *Proceedings of the 56th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pages 2204–2213.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Hattie Zhou, Azade Nova, Hugo Larochelle, Aaron Courville,
    Behnam Neyshabur, and Hanie Sedghi. 2022. Teaching algorithmic reasoning via in-context
    learning. *arXiv preprint arXiv:2211.09066*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Evaluation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In our work, we present two modes of experiments: single and pairwise model
    evaluation. Our single model evaluation is similar to a hybrid of SM-Turn and
    SM-Dialogue evaluations and a pairwise model to PW-Turn from Smith et al. ([2022](#bib.bib22)).
    For each turn, we ask crowdworkers to evaluate the quality of the chatbot response
    based on the following metrics. We attach the exact wording of the question.'
  prefs: []
  type: TYPE_NORMAL
- en: Single Model Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensibleness Whether the response makes sense.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: “Does the response make sense?”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency Whether the response does not contradict the contextual information
    or the persona of the chatbot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: “Is the response consistent with the information based on the persona list and
    context of the conversation?”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engagingness Whether the user is engaged and would want to continue the conversation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: “Are you engaged by the response? Do you want to continue the conversation?”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final Rating
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: “How was your chat? From a scale of 1 (very bad) to 5 (very good), rate the
    quality of the overall conversation.”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Pairwise Model Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensibleness Which response makes more sense.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: “Which response makes more sense?”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consistency Which response is more true to and consistent with the persona.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: “If you had to say one of these speakers is more true to and consistent with
    the listed persona and one is not, who would you say is more consistent?”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Interestingness Which response is more interesting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: “If you had to say one of these responses is interesting and one is boring,
    which would you say is more interesting?”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preference Which response is preferred for a long conversation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: “Based on the current response, who would you prefer to talk to for a long conversation?
    Your conversation will continue with the selected response.”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.2 Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For single model evaluation of $\mathrm{MPC}$, We use OPT (30B, 66B), OpenAI
    GPT-3 (davinci, text-davinci-002) GPT-JT, and BLOOM-176B as the base LM for the
    open-sourced pretrained LLM group. On the other hand, we evaluate Blenderbot3
    30B (BB3-30B), the best publicly available open-domain chatbot model, for the
    fine-tuned model group. We also evaluate BB3-175B which is only available through
    the web interface [https://blenderbot.ai/](https://blenderbot.ai/). We evaluate
    BB3-175B for comparison purposes, though the comparison is not fair as we cannot
    instill a persona into BB3 and cannot control for further differences, such as
    internet search and user interface.
  prefs: []
  type: TYPE_NORMAL
- en: For pairwise model evaluation of $\mathrm{MPC}$ and BB3-30B to compare human
    evaluations of a pre-trained model and a fine-tuned model. This experiment controls
    for many variables as BB3-30B was initialized with OPT-30B before fine-tuning.
    For the module experiment, we tested with OPT-30B, and we compared the one with
    the whole pipeline, and the other only with an utterance generator with a fixed
    persona.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Dense Passage Retriever
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the DPR model of the memory module, we use the model weights from the custom
    DPR model finetuned for MultiDoc2Dial Feng et al. ([2021](#bib.bib6)), as we have
    observed that this model performs slightly better than the original DPR model
    from  Karpukhin et al. ([2020](#bib.bib7)).
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Sens. | Cons. | Eng. | SCE-w | SCE-p | Length | Latency | Rating
    | number |'
  prefs: []
  type: TYPE_TB
- en: '|  | (%) | (%) | (%) | (%) | (%) | (tokens) | (s) | (/5.0) | (people&#124;utterances)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuned |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| BB3-30B | 71.3 | 77.8 | 73.7 | 62.0 | 54.3 | 24.9 | 3.7 | 2.9 | 27&#124;540
    |'
  prefs: []
  type: TYPE_TB
- en: '| BB3-30B (non-repeated) | 84.4 | 80.3 | 90.3 | 73.2 | 65.6 | 25.4 | 3.7 |
    3.5 | 16&#124;320 |'
  prefs: []
  type: TYPE_TB
- en: '| BB3-175B* | 85.9 | 88.7 | 84.8 | 80.0 | 73.3 | 25.4 | - | 3.8 | 27&#124;540
    |'
  prefs: []
  type: TYPE_TB
- en: '| Full persona |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-002 | 94.0 | 94.7 | 84.3 | 88.4 | 79.7 | 15.4 | 0.8 | 4.1 |
    35&#124;700 |'
  prefs: []
  type: TYPE_TB
- en: '| davinci* | 91.8 | 89.2 | 78.8 | 82.5 | 70.8 | 13.4 | 1.5 | 3.8 | 25&#124;500
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-30B* | 92.0 | 87.2 | 86.0 | 82.3 | 72.8 | 13.7 | 1.0 | 3.7 | 25&#124;500
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathrm{MPC}$ |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-002 | 93.6 | 87.8 | 85.5 | 84.4 | 75.0 | 23.1 | 4.7 | 4.2 |
    39&#124;780 |'
  prefs: []
  type: TYPE_TB
- en: '| davinci | 80.2 | 72.0 | 69.1 | 66.4 | 53.3 | 19.9 | 8.4 | 3.1 | 27&#124;540
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-66B | 90.5 | 84.8 | 88.1 | 81.4 | 73.9 | 14.2 | 4.1 | 4.1 | 33&#124;660
    |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-30B | 86.1 | 79.1 | 80.7 | 73.9 | 63.4 | 15.4 | 3.3 | 3.6 | 37&#124;740
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-JT | 91.1 | 83.2 | 65.3 | 74.4 | 53.5 | 8.6 | 2.0 | 3.1 | 33&#124;660
    |'
  prefs: []
  type: TYPE_TB
- en: '| BLOOM-176B | 65.2 | 65.5 | 61.4 | 51.8 | 40.5 | 15.6 | 12.4 | 2.8 | 28&#124;500
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathrm{MPC}$+Full |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| text-davinci-002* | 94.4 | 92.2 | 92.8 | 88.7 | 83.0 | 31.4 | 16.3 | 4.2
    | 25&#124;500 |'
  prefs: []
  type: TYPE_TB
- en: '| OPT-30B* | 85.6 | 87.2 | 89.0 | 78.8 | 72.6 | 15.2 | 2.4 | 3.7 | 25&#124;500
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Full experimental evaluation results of single model evaluation. BB3-30B
    (non-repeated) is the same as BB3-30B but excluding any conversations that had
    repetition of previous bot utterances. * denotes model experiments that were only
    run on MTurk.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Experimental Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we report all experimental results and miscellaneous analysis.
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Full Single Model Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this subsection, we report all our model evaluations with additional details,
    such as average latency (the amount of time it took to compute the next utterance
    and show the user), and average utterance length, which is the number of tokens
    measured by the OPT tokenizer. We also additionally report SCE-w, which was not
    reported in the main body due to space restrictions in Table [5](#A1.T5 "Table
    5 ‣ A.3 Dense Passage Retriever ‣ Appendix A Evaluation Details ‣ Prompted LLMs
    as Chatbot Modules for Long Open-domain Conversation").
  prefs: []
  type: TYPE_NORMAL
- en: We compute a weighted score SCE-w similar to SSI Thoppilan et al. ([2022](#bib.bib24)),
    in which a response can only be consistent if it is sensible and only engaging
    if it is sensible and consistent. The final score is the average over all SCE-w-adjusted
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: We also note that OpenAI API calls have highly variable latency since December
    2022 due to ChatGPT, so a measure of latency due to modularization cannot properly
    be quantified.
  prefs: []
  type: TYPE_NORMAL
- en: BB3-175B Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our results for BB3-17B are not directly comparable to other models due to a
    different evaluation procedure. Since the model weights for BB3-175B are not publicly
    available, we referred evaluators to interact with the Blenderbot version available
    at https://blenderbot.ai/chat. Evaluators were then asked to copy and paste all
    user inputs and chatbot responses while annotating the chatbot responses similarly
    to other models. We also note, that consistency can only be evaluated for persona
    consistency within the dialog context as we are unable to provide a specific persona
    for the online version of Blenderbot.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Model Configurations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We include our code for running each model we evaluate in our anonymous repository
    at [https://anonymous.4open.science/r/modular-chatbot-9BB7](https://anonymous.4open.science/r/modular-chatbot-9BB7).
    Each model configuration can be found in the repository. The configuration includes
    decoding parameters (e.g., sampling method, temperature for each module) and slight
    variations of the prompts for the utterance generator module.
  prefs: []
  type: TYPE_NORMAL
- en: B.3 Subgroup Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd6fb2caf70d89f8c31425b11d8463c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The average score of MTurk workers group minus the average score
    of university students group. We find the two subgroups are very similar on average
    across metrics, though sensibleness seems to show the greatest difference. Students,
    in general, score chatbot models slightly more harshly. BB3-30B is an outlier
    which students score significantly lower than MTurk workers.'
  prefs: []
  type: TYPE_NORMAL
- en: We measure the difference in ratings, for each metric and our combined metrics,
    between MTurk and student evaluators and find no significant difference for most
    experiments and metrics (Fig.[2](#A2.F2 "Figure 2 ‣ B.3 Subgroup Analysis ‣ Appendix
    B Experimental Results ‣ Prompted LLMs as Chatbot Modules for Long Open-domain
    Conversation"). In general, we observe the greatest rating difference for sensibleness.
  prefs: []
  type: TYPE_NORMAL
- en: BB3-30B is an outlier with a significant difference across all metrics, especially
    for sensibleness with a difference >20%. BB3-30B dialogs contain an increased
    ratio of repetition issues for students but even when excluding those dialogs,
    we still find a significant rating difference. Looking at the dialog histories,
    we observe that BB3-30B student evaluators were less patient than MTurk evaluators
    when BB3-30B would repeat the same questions or suddenly change the subject. It
    should be noted that this is a qualitative observation and should be taken with
    a grain of salt.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Few-shot Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we show our full few-shot prompts, including the CoT prompt,
    for each module. The utterance generator prompts vary slightly between model experiments.
    These prompts can be seen in the model configurations in our repository. The second
    example for dialogue summary is modified from a sample from the dataset DialogSum Chen
    et al. ([2021](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Utterance
    Clarifier Rephrase User’s question in
    third-person. Sarah: I’ve been working at the coffee shop for about six months.
    User: I see. what did you do before that? # Specifically, What did Sarah do before
    working at a coffee shop for six months?# Ashley: Do you know who Ronaldo is?
    User: I don’t know, who is he? # Specifically, Who is Ronaldo?# Robert: Hey, how
    are you doing? User: Good. What’s your name? Do you know my name? # Specifically,
    What is Robert’s name? Does Robert know User’s name?# Jennifer: I like reading
    about history and science. User: What kind of history? I like history too! # Specifically,
    What kind of history does Jennifer read about?# John: Hi! How are you doing today?
    User: good. how about you # Specifically, How is John doing today?#Memory Processor w/o CoT
    # This is the list of John’s knowledge. John’s full name is John
    Parker. User is a teacher at a local middle school. User teaches biology. John
    likes to go for a run. User enjoys watching movies, but User doesn’t like superhero
    movies. Q: What does User do for a living? A: John thinks User is a biology teacher
    at a local middle school. # This is the list of Ashley’s knowledge. Ashley likes
    history documentaries. Ashley does not like Korean food. Ashley is a teacher at
    a local middle school. User likes biology and especially anatomy. Ashley likes
    French cuisine. Q: What is Ashley’s favorite dish? A: Ashley thinks Ashley likes
    French cuisine but does not like Korean food.Memory Processor w/ CoT
    # This is the list of Ashley’s knowledge. (1) Ashley likes history
    documentaries. (2) Ashley does not like Korean food. (3) Ashley is a teacher at
    a local middle school. (4) User likes biology and especially anatomy. (5) Ashley
    likes French cuisine. Q: What is Ashley’s favorite dish? A: Let’s think step by
    step. (1) History documentaries are not related to Ashley’s favorite dish. (2)
    Ashley’s favorite dish would not be Korean because she does not like Korean food.
    (3) Ashley being a teacher does not tell us anything about her favorite dish.
    (4) This fact is about User, not Ashley. (5) Ashley’s favorite dish may be French
    since she likes French cuisine. Therefore, (2) and (5) can help answer the question.
    Answer: Ashley thinks Ashley likes French cuisine but does not like Korean food.Dialogue Summarizer
    #Dialogue User: Tell me about yourself Sally: I’m 26 years old
    and graduated from a college in Wisconsin. User: Were you a leader when you were
    in college? Sally: Yes. I was the head TA for a computer science course at our
    university. User: Were you involved in any club activities at your university?
    Sally: Yes. I was a member of the basketball Society. I like playing basketball.
    #Summary - Sally is 26 years old and graduated college in Wisconsin. - Sally was
    the head TA for a computer science course. - Sally played basketball in college.#
    #Dialogue User: Out of every movie that you’ve seen, which one is your favorite?
    John: I’m going to have to say that Superbad is the best movie ever. User: You
    think so, how come? John: Well, Superbad is super funny. User: You’re not lying,
    I found that movie absolutely hilarious. John: I didn’t know that you saw Superbad
    before. User: I made sure to be in line to see it the first day it came out. #Summary
    - John’s favorite movie is Superbad. - John and User think Superbad is funny.
    - User watched Superbad the first day it came out.#'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Utterance Generator Prompt Examples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide example prompts for the utterance generator of $\mathrm{MPC}$
    and Vanilla models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Utterance
    Generator of MPC Sarah is polite, fun, and engaging.
    The following is a conversation between Sarah and Person1. Person1: My Favorite
    part of the concert is making sure I get great seats and then I can catch guitar
    picks Sarah: That sounds like a lot of fun! Person1: It is a lot of fun, so what
    do you like to do for fun? Sarah: I love going to concerts, car shows, music festivals,
    and houses with fireplaces. Person1: I love going to car shows and looking at
    older cars, what is your favorite car? Sarah: I think my favorite car is a vintage
    mustang. Person1: OH man vintage Mustangs are great, I had a friend who owned
    a 69 mach 1, that was a dream car Sarah: Well, I hope you get to experience your
    dream car one day! Person1: Maybe someday I will, do you own a Mustang? The following
    statements are true about Sarah. Sarah thinks Sarah owns a Mustang. Give a friendly
    sensible response that is interesting and polite to Person1\. End with a question.
    Sarah: Well , I hope you get to experience your dream car one day! Person1: Maybe
    someday I will, do you own a Mustang? Sarah:Response from previous prompt
    "Yes, I do own a Mustang. It’s a great car to drive. Do you like
    driving?"Utterance
    Generator of MPC with full persona The
    following are persona facts about Sarah. Sarah is 40 years old. Sarah currently
    lives in small town in Georgia. Sarah reads twenty books a year. Sarah is a stunt
    double as Sarah’s second job. Sarah only eats kosher. Sarah was poor growing up.
    Sarah was raised in a single parent household. Sarah has two dogs. Sarah likes
    to work on vintage cars. Sarah owns two vintage mustangs. Sarah’s favorite music
    is country. Sarah volunteers at a soup kitchen. Sarah thinks Sarah grew up in
    a small town and didn’t have a car. Sarah is polite, fun, and engaging. The following
    is a conversation between Sarah and Person1. Person1: We at least had a playground
    in tow and my grandparents lived in right in front of it, so we went to both places
    a good bit. Sarah: Maybe it was difficult to find things to do in your town, but
    it sounds like you had a lot of family support, which is really important. It’s
    good that you had a place to go where you felt safe and loved. Person1: You ended
    up growing up poor too. Sarah: Yes, I did. I think it’s made me a lot more sympathetic
    to other people’s experiences. Person1: We were poor too and my family didn’t
    have a car. Sarah: I can imagine that it would be tough to not have a car. My
    family had a car, but we were poor too. Person1: It was. Almost every month we
    had barely any food and my parents had to deal with what we had. Give a friendly
    sensible response that is interesting and polite to Person1\. End with a question.
    Sarah:Response
    from previous prompt "It sounds like it was really
    tough for you and your family. I’m sorry that you had to go through that. Do you
    think that growing up in a small town made it harder for you?"Utterance Generator of Vanilla
    (full persona) The following statements are
    true about Sarah. Sarah is 24 years old. Sarah currently lives in Canada. Sarah
    is a swim coach at Sarah’s local pool. Sarah is studying to be a computer programmer.
    Sarah is also a graduate student. Sarah is now looking for a new job. Sarah’s
    mother is very traditional while Sarah prefers to be more free spirited. Sarah’s
    family and Sarah are from India. Sarah’s favorite music genre is death metal.
    Sarah is a famous twitch streamer. Sarah likes watching war documentaries. Sarah’s
    favorite food is mexican food. Sarah is polite, fun, and engaging. The following
    is a conversation between Sarah and Person1. Person1: Have you thought about finding
    a job that allows you to travel? Sarah: I have, but I’m not sure what type of
    job would allow me to do that. Person1: What types of jobs have you looked into
    already? Sarah: I’ve looked into jobs in the travel industry, but I’m not sure
    if that’s what I really want to do. Person1: Since your a twitch streamer, could
    you maybe make that your career? Sarah:Response from previous prompt "I
    don’t know, I’ve never thought about that before. I’ll have to look into it."'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Data Collection Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We attach screenshots of the evaluation forms, used for data collection, for
    single (Fig. [3](#A5.F3 "Figure 3 ‣ E.3 Crowdworker Instruction ‣ Appendix E Data
    Collection Details ‣ Prompted LLMs as Chatbot Modules for Long Open-domain Conversation"))
    and pairwise models (Fig. [4](#A5.F4 "Figure 4 ‣ E.3 Crowdworker Instruction ‣
    Appendix E Data Collection Details ‣ Prompted LLMs as Chatbot Modules for Long
    Open-domain Conversation")) respectively. We also attach the screenshot of the
    instruction, alert, and consent format that we gave to participants (Fig. [5](#A5.F5
    "Figure 5 ‣ E.3 Crowdworker Instruction ‣ Appendix E Data Collection Details ‣
    Prompted LLMs as Chatbot Modules for Long Open-domain Conversation") and  [6](#A5.F6
    "Figure 6 ‣ E.3 Crowdworker Instruction ‣ Appendix E Data Collection Details ‣
    Prompted LLMs as Chatbot Modules for Long Open-domain Conversation")).
  prefs: []
  type: TYPE_NORMAL
- en: E.1 MTurk Setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We gathered at least 15 MTurk workers for each task. We require them to have
    an MTurk Master’s qualification, a HIT approval rate of more than 99%, and a number
    of HITs approved more than 100 times, and to live in the US, UK, Canada, Australia,
    and New Zealand.
  prefs: []
  type: TYPE_NORMAL
- en: We found that data quality improves significantly when using MTurk Master workers.
    These workers tend to produce more fluent messages than non-Master workers and
    are less likely to provide random or generic responses. The difference was more
    pronounced as non-Master crowdworkers would often provide a score of 99% on average
    despite the clear problems of repetition and contradiction from the dialogue agent.
  prefs: []
  type: TYPE_NORMAL
- en: E.2 Student Participants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To cross-validate the result of MTurk, we gathered 49 English-proficient students
    from a several universities. They are either undergraduate or graduate students.
    The minimum requirement they should meet is English proficiency: IELTS >= 7.0;
    TOEFL IBT >= 95; and TOEIC >= 900. This criterion is similar or equivalent to
    the minimum required level of English proficiency for admission to the graduate
    schools of MIT. The average English scores of students we gathered are the following:
    IBT 108.4 out of 120; TOEIC 960.9 out of 990; and IELTS 7.83 out of 9.0.'
  prefs: []
  type: TYPE_NORMAL
- en: E.3 Crowdworker Instruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The crowd workers are asked to continue the chat for 20 turns and evaluate each
    response by the metrics described in the section below. We instruct them to type
    more than 3 words on average, and not to repeat meaningless or generic messages.
    Also, we request them not to randomly choose between yes or no and to provide
    evaluations honestly. Lastly, we strongly emphasize the importance of maintaining
    confidentiality and request the crowd workers refrain from disclosing any private
    information about themselves or others during the evaluation process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c20aaf44a3564d664a1107d8d4ae157e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Evaluation form for a single model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed8f06173e29f1fe6608670c3088bd7c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Evaluation form for pairwise model comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/25ec24b7074bf8d3f44d0b193456c15d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: We display this page before the evaluators start the evaluation process
    to inform them about the task and gather their consent for data usage.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9782252fed041de58169eeac4064a87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Additional Alert in Mturk HIT page.'
  prefs: []
  type: TYPE_NORMAL
