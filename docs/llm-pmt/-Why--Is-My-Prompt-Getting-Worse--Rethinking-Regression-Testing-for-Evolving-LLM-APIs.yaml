- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:48:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:03
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: (Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving
    LLM APIs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: （为何）我的提示变得更糟？重新思考针对不断演变的 LLM API 的回归测试
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2311.11123](https://ar5iv.labs.arxiv.org/html/2311.11123)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2311.11123](https://ar5iv.labs.arxiv.org/html/2311.11123)
- en: Wanqin Ma [wmaag@connect.ust.hk](mailto:wmaag@connect.ust.hk) The Hong Kong
    University of Science and Technology ,  Chenyang Yang [cyang3@cs.cmu.edu](mailto:cyang3@cs.cmu.edu)
    Carnegie Mellon University  and  Christian Kästner Carnegie Mellon University(2024)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Wanqin Ma [wmaag@connect.ust.hk](mailto:wmaag@connect.ust.hk) 香港科技大学，Chenyang
    Yang [cyang3@cs.cmu.edu](mailto:cyang3@cs.cmu.edu) 卡内基梅隆大学 和 Christian Kästner
    卡内基梅隆大学（2024）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Large Language Models (LLMs) are increasingly integrated into software applications.
    Downstream application developers often access LLMs through APIs provided as a
    service. However, LLM APIs are often updated silently and scheduled to be deprecated,
    forcing users to continuously adapt to evolving models. This can cause performance
    regression and affect prompt design choices, as evidenced by our case study on
    toxicity detection. Based on our case study, we emphasize the need for and re-examine
    the concept of regression testing for evolving LLM APIs. We argue that regression
    testing LLMs requires fundamental changes to traditional testing approaches, due
    to different correctness notions, prompting brittleness, and non-determinism in
    LLM APIs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）正越来越多地集成到软件应用程序中。下游应用程序开发人员通常通过作为服务提供的 API 访问 LLM。然而，LLM API 经常在没有任何通知的情况下更新，并计划被弃用，迫使用户不断适应不断演变的模型。这可能会导致性能回归并影响提示设计选择，正如我们对毒性检测的案例研究所证明的那样。基于我们的案例研究，我们强调了回归测试演变中的
    LLM API 的必要性，并重新审视了这一概念。我们认为，回归测试 LLM 需要对传统测试方法进行根本性的变化，因为 LLM API 存在不同的正确性概念、提示的脆弱性和非确定性。
- en: 'Large Language Models (LLM), regression testing^†^†journalyear: 2024^†^†copyright:
    rightsretained^†^†conference: Conference on AI Engineering Software Engineering
    for AI; April 14–15, 2024; Lisbon, Portugal^†^†booktitle: Conference on AI Engineering
    Software Engineering for AI (CAIN 2024), April 14–15, 2024, Lisbon, Portugal^†^†doi:
    10.1145/3644815.3644950^†^†isbn: 979-8-4007-0591-5/24/04^†^†ccs: Software and
    its engineering Software testing and debugging'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLM），回归测试^†^†journalyear: 2024^†^†copyright: rightsretained^†^†conference:
    AI 工程软件工程大会；2024年4月14–15日；葡萄牙里斯本^†^†booktitle: AI 工程软件工程大会（CAIN 2024），2024年4月14–15日，葡萄牙里斯本^†^†doi:
    10.1145/3644815.3644950^†^†isbn: 979-8-4007-0591-5/24/04^†^†ccs: 软件及其工程 软件测试和调试'
- en: 1\. Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: '![Refer to caption](img/72b6fa20756174ca8541116898e1bbf8.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/72b6fa20756174ca8541116898e1bbf8.png)'
- en: 'Figure 1\. An LLM API update from text-davinci-003 to gpt-3.5-turbo-instruct
    causes a major performance downgrade on classifying toxic comments. The API update
    also changes the prompt choice: Prompt A (left) now outperforms Prompt B (right)
    by 8.7% accuracy.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 从 text-davinci-003 到 gpt-3.5-turbo-instruct 的 LLM API 更新导致了对毒性评论分类的重大性能下降。API
    更新还改变了提示选择：提示 A（左）现在比提示 B（右）提高了 8.7% 的准确性。
- en: 'Large Language Models (LLMs) are increasingly integrated into software applications (Kaddour
    et al., [2023](#bib.bib17)). Due to the high cost of developing and maintaining
    in-house LLMs, many applications rely on LLM APIs provided by companies like OpenAI,
    Anthropic, and Google (OpenAI, [2023a](#bib.bib27); Google, [2023a](#bib.bib14);
    Anthropic, [2023](#bib.bib4)). Although LLM APIs provide easy access to state-of-the-art
    models, they also bring in uncertainties for their downstream applications: It
    is not uncommon for application developers to find their carefully engineered
    prompts that worked yesterday work less well after updates from the LLM provider’s
    side (radiator57, [2023](#bib.bib31); Chen et al., [2023](#bib.bib8)). In Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ (Why) Is My Prompt Getting Worse? Rethinking Regression
    Testing for Evolving LLM APIs"), we highlight such an example from our case study
    on a toxicity detection task, where the LLM API update from text-davinci-003 to
    gpt-3.5-turbo-instruct causes a major performance downgrade and changes the good
    choices for prompt selection.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）越来越多地被集成到软件应用中（Kaddour et al., [2023](#bib.bib17)）。由于开发和维护内部 LLMs
    的高成本，许多应用依赖于像 OpenAI、Anthropic 和 Google 提供的 LLM API（OpenAI, [2023a](#bib.bib27);
    Google, [2023a](#bib.bib14); Anthropic, [2023](#bib.bib4)）。尽管 LLM API 提供了对最先进模型的便捷访问，但它们也为下游应用带来了不确定性：应用开发者经常发现他们精心设计的提示在
    LLM 提供方更新后效果变差（radiator57, [2023](#bib.bib31); Chen et al., [2023](#bib.bib8)）。在图
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ (Why) Is My Prompt Getting Worse? Rethinking
    Regression Testing for Evolving LLM APIs") 中，我们展示了我们对一个毒性检测任务的案例研究，其中从 text-davinci-003
    到 gpt-3.5-turbo-instruct 的 LLM API 更新导致了性能的大幅下降，并改变了提示选择的最佳方案。
- en: 'Similar to traditional web service API (Li et al., [2013](#bib.bib19)) and
    more conventional ML APIs (Cummaudo et al., [2020](#bib.bib10)), updates to server-side
    LLM API controlled by a different party are hard to deal with. First, LLM APIs
    can be updated silently: OpenAI’s gpt-3.5-turbo model has been updated twice (by
    Nov 2023) but the updates are not visible to the downstream developers. Such silent
    API updates change only the underlying LLM but not the API signature¹¹1For our
    purpose, an LLM API describes both the signature of the service and its behavior.
    In this work, we primarily focus on behavior changes that are difficult to document
    and detect., causing unexpected behavioral changes (e.g., formatting of generated
    code) (Chen et al., [2023](#bib.bib8)) to the application developers. Second,
    LLM APIs are scheduled to be deprecated and discontinued (OpenAI, [2023b](#bib.bib28)),
    effectively forcing application developers to adopt newer API versions. For example,
    the text-davinci-003 model will be deprecated on Jan 2024\. The forced transition
    to gpt-3.5-turbo-instruct can cause unexpected prompt performance changes, including
    the introduced performance downgrade as illustrated in our example in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ (Why) Is My Prompt Getting Worse? Rethinking Regression
    Testing for Evolving LLM APIs").'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于传统的 Web 服务 API（Li et al., [2013](#bib.bib19)）和更传统的 ML API（Cummaudo et al.,
    [2020](#bib.bib10)），由不同方控制的服务器端 LLM API 更新很难处理。首先，LLM API 可能会静默更新：OpenAI 的 gpt-3.5-turbo
    模型已经更新了两次（截至 2023 年 11 月），但这些更新对下游开发者不可见。这种静默 API 更新仅更改了底层 LLM，而不改变 API 签名¹¹1对于我们的目的，LLM
    API 描述了服务的签名及其行为。在这项工作中，我们主要关注难以记录和检测的行为变化。），导致应用开发者意外的行为变化（例如生成代码的格式）（Chen et
    al., [2023](#bib.bib8)）。其次，LLM API 定期被弃用和终止（OpenAI, [2023b](#bib.bib28)），有效地迫使应用开发者采用更新的
    API 版本。例如，text-davinci-003 模型将于 2024 年 1 月被弃用。强制过渡到 gpt-3.5-turbo-instruct 可能会导致意外的提示性能变化，包括我们在图
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ (Why) Is My Prompt Getting Worse? Rethinking
    Regression Testing for Evolving LLM APIs") 中展示的性能下降。
- en: 'To cope with evolving LLM APIs, application developers need support for monitoring
    and analyzing how their prompts perform differently when the LLM API changes.
    Existing software engineering practices suggest that regression testing is essential
    for identifying changes between software versions, often particularly to ensure
    that fixed bugs are not reintroduced (Sommerville, [2015](#bib.bib35)). We argue
    that LLM application developers should take a similar approach. However, existing
    regression testing practices can not directly translate in the LLM context, as
    we will illustrate. Based on our observations in the case study, we highlight
    three fundamental changes for regression testing LLM APIs:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对不断发展的 LLM API，应用开发者需要支持监控和分析提示在 LLM API 更改时表现的不同。现有的软件工程实践表明，回归测试对于识别软件版本之间的变化至关重要，通常特别是为了确保修复的错误不会重新出现（Sommerville，[2015](#bib.bib35)）。我们认为
    LLM 应用开发者也应该采取类似的方法。然而，现有的回归测试实践不能直接应用于 LLM 上下文，正如我们将说明的那样。基于我们在案例研究中的观察，我们强调了回归测试
    LLM API 的三个基本变化：
- en: First, LLM regression tests should be defined at a different granularity. In
    traditional software engineering, a single breaking regression test would indicate
    a bug in the software implementation. In contrast, it is common for ML models
    to change predictions for individual data points after updates. The common practice
    is to examine overall model accuracy, which has been criticized for being coarse-grained (Ribeiro
    et al., [2020](#bib.bib34)). To gain a more nuanced understanding than overall
    model accuracy, LLM regression tests should be defined over data slices rather
    than on single predictions or the entire dataset. This calls for a different correctness
    notion, as “regression” is defined over slice-level aggregated metrics and the
    slice-level test only fails when the metrics change beyond a threshold.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，LLM 回归测试应该在不同的粒度上定义。在传统的软件工程中，单个失败的回归测试会指示软件实现中的错误。相比之下，ML 模型在更新后对单个数据点的预测发生变化是很常见的。常见的做法是检查整体模型准确性，但这种方法被批评为粗粒度的（Ribeiro
    等人，[2020](#bib.bib34)）。为了获得比整体模型准确性更细致的理解，LLM 回归测试应该在数据切片上定义，而不是单个预测或整个数据集。这要求采用不同的正确性概念，因为“回归”是在切片级别聚合指标上定义的，切片级测试仅在指标变化超出阈值时才会失败。
- en: Second, LLM regression tests need to monitor both model and prompt updates.
    It is well-known that prompt engineering can greatly influence LLMs’ performance (Liu
    et al., [2023a](#bib.bib20)). As we will show, we observed that different prompt
    designs regress or improve differently on the same API update, making the optimal
    prompt design change from API version to version. We argue that tracking both
    LLM and prompt versions is essential for LLM regression tests.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，LLM 回归测试需要监控模型和提示的更新。众所周知，提示工程可以大大影响 LLM 的性能（Liu 等人，[2023a](#bib.bib20)）。正如我们将展示的，我们观察到不同的提示设计在相同的
    API 更新中会有不同的回归或改进，使得最佳提示设计在不同 API 版本中有所变化。我们认为，跟踪 LLM 和提示版本对于 LLM 回归测试至关重要。
- en: 'Third, LLM regression tests need to deal with non-determinism of LLM APIs.
    LLMs are known to produce non-deterministic outputs: Non-determinism is often
    introduced intentionally for generating high-quality outputs with a non-zero temperature (e.g.,
    Fried et al., [2023](#bib.bib13)), but can even be observed with a zero temperature
    setting (Ouyang et al., [2023](#bib.bib30)), where the LLM should deterministically
    predict the next most likely token. It is necessary to deal with flakiness in
    LLM regression tests by considering their inherent non-determinism.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，LLM 回归测试需要处理 LLM API 的非确定性。众所周知，LLM 会产生非确定性的输出：非确定性通常是为了生成高质量的输出而故意引入的，例如非零温度（例如，Fried
    等人，[2023](#bib.bib13)），但即使在零温度设置下也能观察到（Ouyang 等人，[2023](#bib.bib30)），在这种情况下，LLM
    应该确定性地预测下一个最可能的令牌。必须通过考虑其固有的非确定性来处理 LLM 回归测试中的波动性。
- en: 'In summary, our paper has the following contributions:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的论文有以下贡献：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An exploratory case study on toxicity detection with the GPT-3.5 model family,
    showing API upgrades can cause significant performance deterioration, and that
    prompt is an important factor in behavioral changes.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对 GPT-3.5 模型家族的毒性检测进行的探索性案例研究表明，API 升级可能导致显著的性能下降，而提示是行为变化中的一个重要因素。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A re-examination of the concept of regression testing for LLM APIs and its required
    fundamental changes, due to different correctness notions, prompting brittleness,
    and non-determinism in LLMs.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对 LLM API 的回归测试概念及其所需的根本性变化的重新审视，因不同的正确性概念、提示脆弱性和 LLM 的非确定性。
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A vision on research opportunities in supporting systematic regression testing
    for prompting LLM APIs.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 支持系统回归测试的提示 LLM API 的研究机会展望。
- en: '| Model | Endpoint Type | Training Method | Release Date |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 端点类型 | 训练方法 | 发布日期 |'
- en: '| gpt-3.5-turbo-instruct | Text completion | RLHF | Sep 2023 |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-instruct | 文本补全 | RLHF | 2023年9月 |'
- en: '| gpt-3.5-turbo-0613 | Chat | RLHF | June 2023 |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-0613 | Chat | RLHF | 2023年6月 |'
- en: '| gpt-3.5-turbo-0301 | Chat | RLHF | Mar 2023 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-0301 | Chat | RLHF | 2023年3月 |'
- en: '| text-davinci-003 | Text completion | RLHF | Nov 2022 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003 | 文本补全 | RLHF | 2022年11月 |'
- en: '| text-davinci-002 | Text completion | fine-tuning | Mar 2022 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002 | 文本补全 | 微调 | 2022年3月 |'
- en: Table 1\. Representative models from OpenAI’s GPT-3.5 family (OpenAI, [2023c](#bib.bib29)),
    sorted by release date.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. OpenAI GPT-3.5 系列的代表性模型 (OpenAI, [2023c](#bib.bib29))，按发布日期排序。
- en: 2\. Background and Related Work
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景和相关工作
- en: 2.1\. Evolving AI APIs
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 演变的 AI API
- en: As ML models are increasingly provided as a cloud service through APIs (e.g.,
    Perspective API (Google, [2023b](#bib.bib15)), ChatGPT (OpenAI, [2023a](#bib.bib27)),
    Amazon Rekognition (Mishra, [2019](#bib.bib24))), it has been noticed that these
    models evolve over time without clear documentation (Cummaudo et al., [2019](#bib.bib11);
    Tu et al., [2023](#bib.bib38)), similar to traditional web service API (Li et al.,
    [2013](#bib.bib19)). This can pose risks to downstream application developers,
    who do not have control over model updates and can potentially suffer from performance
    regression (Chen et al., [2023](#bib.bib8); Cummaudo et al., [2020](#bib.bib10)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 ML 模型越来越多地通过 API（例如，Perspective API (Google, [2023b](#bib.bib15))，ChatGPT
    (OpenAI, [2023a](#bib.bib27))，Amazon Rekognition (Mishra, [2019](#bib.bib24)))
    提供为云服务，已注意到这些模型随着时间的推移而演变，没有明确的文档 (Cummaudo et al., [2019](#bib.bib11); Tu et
    al., [2023](#bib.bib38))，类似于传统的 web 服务 API (Li et al., [2013](#bib.bib19))。这对下游应用开发者可能构成风险，因为他们无法控制模型更新，可能会遭受性能回退
    (Chen et al., [2023](#bib.bib8); Cummaudo et al., [2020](#bib.bib10))。
- en: Beyond demonstrating the problem, there has only been limited work on actually
    supporting developers facing evolving APIs not under their control. The most prominent
    example for ML models is done by Cummaudo et al. ([2020](#bib.bib10)), where they
    focus on detecting changes in the label space and prediction confidence for vision
    APIs. Our work extends the existing literature by explicitly adapting the concept
    of regression testing in the LLM contexts and highlighting the need for more nuanced
    regression test suites.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了展示问题之外，针对实际支持面对不受控制的演变 API 的开发者的工作非常有限。ML 模型中最突出的例子是 Cummaudo et al. ([2020](#bib.bib10))，他们专注于检测视觉
    API 中标签空间和预测置信度的变化。我们的工作通过在 LLM 环境中明确适应回归测试的概念，扩展了现有文献，并强调了对更细致的回归测试套件的需求。
- en: 2.2\. The Rise of Prompting LLMs
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 提示 LLM 的崛起
- en: 'LLMs present a fundamental shift in NLP applications through the prompting
    interface, which allows rapid prototyping and iterations (Liu et al., [2023a](#bib.bib20)):
    Application developers can easily tweak prompts and validate prompts on a few
    examples without the need to curate data and build models. In a sense, the LLM
    together with a specific prompt can be considered equivalent to a traditional
    specifically-trained ML model for a specific task, such as toxicity detection.
    However, the prompting paradigm also brings in the risk of prompt brittleness,
    as prompts can be sensitive to small changes (Lu et al., [2022](#bib.bib22)) and
    the good choices for prompts change when the LLM changes. Our work highlights
    prompts as an additional factor to consider for regression testing LLMs.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 通过提示接口在 NLP 应用中呈现出根本性的变化，这使得快速原型设计和迭代成为可能 (Liu et al., [2023a](#bib.bib20))：应用开发者可以轻松调整提示，并在少量示例上验证提示，而无需策划数据和构建模型。在某种意义上，LLM
    与特定提示可以被视为等同于为特定任务（如毒性检测）特别训练的传统 ML 模型。然而，提示范式也带来了提示脆弱性的风险，因为提示对小变化很敏感 (Lu et
    al., [2022](#bib.bib22))，并且随着 LLM 的变化，良好的提示选择也会变化。我们的工作强调了提示作为回归测试 LLM 时需要考虑的额外因素。
- en: 2.3\. ML Model Testing
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. ML 模型测试
- en: ML models are usually evaluated by model fit using aggregated metrics like accuracy,
    as models are expected to make occasional mistakes (Kästner, [2022](#bib.bib18)).
    However, traditional model evaluation has been criticized for being coarse-grained (Ribeiro
    et al., [2020](#bib.bib34)) and suffering from issues like spurious correlations (Adebayo
    et al., [2020](#bib.bib2)). Therefore, recent work has proposed nuanced behavioral
    model testing as an alternative (Naik et al., [2018](#bib.bib26); Ribeiro et al.,
    [2020](#bib.bib34)), where the testers explore nuanced model behaviors beyond
    a single score.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ML 模型通常通过使用诸如准确率等汇总指标来评估模型拟合，因为模型预计会犯偶尔的错误（Kästner，[2022](#bib.bib18)）。然而，传统的模型评估被批评为过于粗略（Ribeiro
    等，[2020](#bib.bib34)），并且存在虚假相关等问题（Adebayo 等，[2020](#bib.bib2)）。因此，最近的工作提出了细致的行为模型测试作为替代方法（Naik
    等，[2018](#bib.bib26)；Ribeiro 等，[2020](#bib.bib34)），其中测试人员探索超出单一分数的细致模型行为。
- en: 'Prior work has explored different methods to explore and test model behaviors (e.g.,
    Ribeiro et al., [2020](#bib.bib34); Ribeiro and Lundberg, [2022](#bib.bib33);
    Yang et al., [2023b](#bib.bib42)), as well as different ways to automate testing
    specific model behaviors (e.g., Sun et al., [2020](#bib.bib36), [2022](#bib.bib37))
    (see Yang et al. ([2023a](#bib.bib41)) for a detailed survey). Another line of
    work on data slicing (e.g., Cabrera et al., [2023](#bib.bib7); Eyuboglu et al.,
    [2022](#bib.bib12); Barash et al., [2019](#bib.bib5)) focuses on identifying data
    regions where a model under-performs. Our work introduces a new scenario for ML
    model testing: regression testing over evolving LLM APIs.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究探讨了不同的方法来探索和测试模型行为（例如，Ribeiro 等，[2020](#bib.bib34)；Ribeiro 和 Lundberg，[2022](#bib.bib33)；Yang
    等，[2023b](#bib.bib42)），以及自动化测试特定模型行为的不同方式（例如，Sun 等，[2020](#bib.bib36)，[2022](#bib.bib37)）（详见
    Yang 等（[2023a](#bib.bib41)）的详细综述）。另一类研究集中于数据切片（例如，Cabrera 等，[2023](#bib.bib7)；Eyuboglu
    等，[2022](#bib.bib12)；Barash 等，[2019](#bib.bib5)），其重点是识别模型表现不佳的数据区域。我们的工作引入了一个新的
    ML 模型测试场景：对不断演变的 LLM API 进行回归测试。
- en: '3\. Case Study: Toxicity Detection'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 案例研究：毒性检测
- en: Since regression of evolving LLM APIs is an emerging problem of which we have
    little understanding, we first explored the problem with an exploratory case study.
    We picked a paradigmatic case (Yin, [2009](#bib.bib43)) of toxicity detection,
    a task widely used for online content moderation and long performed by models
    specifically trained for that task (Hosseini et al., [2017](#bib.bib16)), but
    recently LLMs with a suitable prompt have shown similar or better performance (Wang
    and Chang, [2022](#bib.bib39)). Our case study aims to explore (a) how prompt
    behaviors change (regress) over LLM updates and (b) where regressions can be detected.²²2Code
    available at [https://github.com/MAWanqin2002/LLM_Regression_Testing](https://github.com/MAWanqin2002/LLM_Regression_Testing).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 由于演变中的 LLM API 的回归是一个新兴问题，我们对其了解甚少，因此首先通过探索性案例研究来探讨这个问题。我们选择了一个典型案例（Yin，[2009](#bib.bib43)）作为毒性检测的任务，这个任务广泛用于在线内容管理，并且长期以来由专门为该任务训练的模型执行（Hosseini
    等，[2017](#bib.bib16)），但最近具有合适提示的 LLM 显示出相似或更好的表现（Wang 和 Chang，[2022](#bib.bib39)）。我们的案例研究旨在探索（a）提示行为如何在
    LLM 更新中发生变化（回归），以及（b）回归可以在哪里被检测到。²²2代码可在 [https://github.com/MAWanqin2002/LLM_Regression_Testing](https://github.com/MAWanqin2002/LLM_Regression_Testing)
    获得。
- en: 3.1\. Experiment Setup
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. 实验设置
- en: 3.1.1\. Datasets.
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1. 数据集。
- en: 'We selected two toxicity detection datasets for our case study: Civil Comments (cjadams
    et al., [2019](#bib.bib9)) and GitHub Discussion (Miller et al., [2022](#bib.bib23)),
    covering different contents (generic vs. specialized) and text lengths (short
    vs. long) for toxicity detection.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为案例研究选择了两个毒性检测数据集：Civil Comments（cjadams 等，[2019](#bib.bib9)）和 GitHub Discussion（Miller
    等，[2022](#bib.bib23)），涵盖了不同的内容（通用 vs. 专业）和文本长度（短 vs. 长）的毒性检测。
- en: The Civil Comments dataset is collected from Civil Comments platform, representing
    a wide range of comments on the Internet. We sampled 1000 comments from the dataset,
    among which 41 are toxic and 959 are non-toxic.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: Civil Comments 数据集来自 Civil Comments 平台，代表了互联网上广泛的评论。我们从数据集中抽取了 1000 条评论，其中 41
    条是有毒的，959 条是无毒的。
- en: The GitHub Discussion Dataset contains 174 discussions, among which 74 are toxic
    and 100 are non-toxic. The 74 toxic discussions are collected using the links
    provided by an existing study (Miller et al., [2022](#bib.bib23)), and we randomly
    sample another 100 non-toxic discussions from GitHub.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub Discussion 数据集包含 174 条讨论，其中 74 条是有毒的，100 条是无毒的。这 74 条有毒讨论是使用现有研究提供的链接（Miller
    等，[2022](#bib.bib23)）收集的，我们从 GitHub 随机抽取了另外 100 条无毒讨论。
- en: 3.1.2\. Models.
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2. 模型。
- en: 'We selected five widely used models from OpenAI’s GPT-3.5 family (OpenAI, [2023c](#bib.bib29)):
    gpt-3.5-turbo-instruct, gpt-3.5-turbo-0613, gpt-3.5-turbo-0301, text-davinci-003,
    and text-davinci-002 (shown in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣
    (Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving LLM
    APIs")). These models were released over a span of only 18 months, from March
    2022 to September 2023, covering different endpoint types (chat vs. completion)
    and training methods (fine-tuning vs. RLHF). We treat each model pair as a potential
    update and study 10 model update pairs in our experiment.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从OpenAI的GPT-3.5家族中选择了五个广泛使用的模型 (OpenAI, [2023c](#bib.bib29))：gpt-3.5-turbo-instruct、gpt-3.5-turbo-0613、gpt-3.5-turbo-0301、text-davinci-003和text-davinci-002（见表 [1](#S1.T1
    "Table 1 ‣ 1\. Introduction ‣ (Why) Is My Prompt Getting Worse? Rethinking Regression
    Testing for Evolving LLM APIs")）。这些模型在18个月内发布，从2022年3月到2023年9月，涵盖了不同的端点类型（聊天与完成）和训练方法（微调与强化学习）。我们将每对模型视为一个潜在的更新，并在实验中研究了10对模型更新。
- en: 'Noticeably, four out of these five models are already scheduled to be deprecated
    in 2024, effectively forcing application developers to switch to one of the newer
    models. The models are also updated silently: gpt-3.5-turbo-0301 and gpt-3.5-turbo-0613
    are snapshots of the gpt-3.5-turbo model, which will soon point to gpt-3.5-turbo-1106.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，这五个模型中的四个已经计划于2024年被弃用，这实际上迫使应用开发者切换到其中一个较新的模型。这些模型也会静默更新：gpt-3.5-turbo-0301和gpt-3.5-turbo-0613是gpt-3.5-turbo模型的快照，后者将很快指向gpt-3.5-turbo-1106。
- en: 3.1.3\. Prompts.
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. 提示。
- en: 'We employed four prompting strategies to explore how they behave differently
    on model updates:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了四种提示策略来探索它们在模型更新中的不同表现：
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Simple instruction (P1): The prompt instructs the model to classify the text
    as “toxic” or “non-toxic”, followed by the text to classify. This serves as a
    simple baseline a developer might first try.'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单指令（P1）：提示指示模型将文本分类为“有毒”或“无毒”，然后是需要分类的文本。这作为开发者可能首先尝试的简单基线。
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Simple instruction, placed last (P2): The same as above but put instructions
    after the text. This design follows the insight that LLMs have recency bias (Zhao
    et al., [2021](#bib.bib46)) and stating instructions last makes LLMs less likely
    to ramble (Liu et al., [2023b](#bib.bib21)).'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单指令，放在最后（P2）：与上述相同，但将指令放在文本之后。这种设计遵循了LLMs有近期偏差的洞察 (Zhao et al., [2021](#bib.bib46))，并且最后陈述指令使LLMs不容易闲聊 (Liu
    et al., [2023b](#bib.bib21))。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Detailed instruction (P3): The prompt first describes the classification goal
    in detail, explaining what the developer deems toxic. The description is followed
    by the classification instruction and the text to classify.'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 详细指令（P3）：提示首先详细描述分类目标，解释开发者认为有毒的内容。描述之后是分类指令和需要分类的文本。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Simple instruction + Few-shot examples (P4): After simple instructions, the
    prompt shows two examples, one toxic and the other non-toxic, followed by the
    text to classify. This follows the popular in-context learning paradigm (Brown
    et al., [2020](#bib.bib6)).'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单指令 + 少量示例（P4）：在简单指令之后，提示展示了两个示例，一个是有毒的，另一个是无毒的，然后是需要分类的文本。这符合流行的上下文学习范式 (Brown
    et al., [2020](#bib.bib6))。
- en: 'We share the prompt templates in Figure [2](#S3.F2 "Figure 2 ‣ 3.1.3\. Prompts.
    ‣ 3.1\. Experiment Setup ‣ 3\. Case Study: Toxicity Detection ‣ (Why) Is My Prompt
    Getting Worse? Rethinking Regression Testing for Evolving LLM APIs").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '我们分享了图 [2](#S3.F2 "Figure 2 ‣ 3.1.3\. Prompts. ‣ 3.1\. Experiment Setup ‣ 3\.
    Case Study: Toxicity Detection ‣ (Why) Is My Prompt Getting Worse? Rethinking
    Regression Testing for Evolving LLM APIs") 中的提示模板。'
- en: '# Simple  instruction  (P1)Classify  the  GitHub  discussion  as  "toxic"  or  "non-toxic".  Only  reply  with  the  label.Document:  {text}#
    Simple  instruction,  placed  last  (P2)Document:  {text}Classify  the  GitHub  discussion  as  "toxic"  or  "non-toxic".  Only  reply  with  the  label.#
    Detailed  instruction  (P3)Below  is  a  GitHub  discussion.  Sometimes  the  discussion  can  get  heated  and  have  toxic  comments.  Toxic  comments  can  contain  curse  words,  can  sound  condescending,  can  be  mean  to  others,  or  can  make  people  feel  angry  without  using  offensive  words.Classify  the  GitHub  discussion  as  "toxic"  or  "non-toxic".  Only  reply  with  the  label.Document:  {text}#
    Simple  instruction  +  Few-shot  examples  (P4)Classify  the  GitHub  discussion  as  "toxic"  or  "non-toxic".  Only  reply  with  the  label.Document:  [text  omitted]Label:  toxicDocument:  [text  omitted]Label:  non-toxicDocument:  {text}Label:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '# 简单指令 (P1) 将 GitHub 讨论分类为 "toxic" 或 "non-toxic"。仅回复标签。文档: {text} # 简单指令，放在最后
    (P2) 文档: {text} 将 GitHub 讨论分类为 "toxic" 或 "non-toxic"。仅回复标签。# 详细指令 (P3) 以下是 GitHub
    讨论。讨论有时可能会激烈并包含毒性评论。毒性评论可能包含脏话，可能显得居高临下，对他人刻薄，或者在没有使用冒犯性语言的情况下让人感到愤怒。将 GitHub
    讨论分类为 "toxic" 或 "non-toxic"。仅回复标签。文档: {text} # 简单指令 + 少量示例 (P4) 将 GitHub 讨论分类为
    "toxic" 或 "non-toxic"。仅回复标签。文档: [文本省略] 标签: toxic 文档: [文本省略] 标签: non-toxic 文档:
    {text} 标签:'
- en: Figure 2\. Prompt templates for our experiments on the GitHub discussion dataset.
    Templates for the Civil Comments dataset are similar with some adaptations.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 我们在 GitHub 讨论数据集上的 prompt 模板。Civil Comments 数据集的模板类似，略有调整。
- en: 3.1.4\. Metrics.
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. 评估指标。
- en: To evaluate the accuracy of each model + prompt combination and monitor their
    changes, we use the standard performance metrics accuracy and F1, and set model
    temperature to 0 to obtain the most likely predictions.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估每个模型 + prompt 组合的准确性并监测其变化，我们使用标准的性能指标准确率和 F1，并将模型温度设置为 0，以获得最可能的预测。
- en: '| Model | Civil Comments | GitHub Discussion |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Civil Comments | GitHub Discussion |'
- en: '| P1 | P2 | P3 | P4 | P1 | P2 | P3 | P4 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| P1 | P2 | P3 | P4 | P1 | P2 | P3 | P4 |'
- en: '| gpt-3.5-turbo-instruct | 0.688 | 0.518 | 0.733 | 0.820 | 0.638 | 0.793 |
    0.770 | 0.793 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-instruct | 0.688 | 0.518 | 0.733 | 0.820 | 0.638 | 0.793 |
    0.770 | 0.793 |'
- en: '| gpt-3.5-turbo-0613 | 0.671 | 0.745 | 0.928 | 0.774 | 0.822 | 0.862 | 0.856
    | 0.776 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-0613 | 0.671 | 0.745 | 0.928 | 0.774 | 0.822 | 0.862 | 0.856
    | 0.776 |'
- en: '| gpt-3.5-turbo-0301 | 0.767 | 0.743 | 0.915 | 0.733 | 0.810 | 0.799 | 0.868
    | 0.816 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-0301 | 0.767 | 0.743 | 0.915 | 0.733 | 0.810 | 0.799 | 0.868
    | 0.816 |'
- en: '| text-davinci-003 | 0.862 | 0.814 | 0.938 | 0.933 | 0.655 | 0.672 | 0.644
    | 0.655 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003 | 0.862 | 0.814 | 0.938 | 0.933 | 0.655 | 0.672 | 0.644
    | 0.655 |'
- en: '| text-davinci-002 | 0.803 | 0.587 | 0.861 | 0.822 | 0.839 | 0.874 | 0.810
    | 0.770 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002 | 0.803 | 0.587 | 0.861 | 0.822 | 0.839 | 0.874 | 0.810
    | 0.770 |'
- en: Table 2\. Accuracy for prompt (Pn) and model combinations on the Civil Comments
    and GitHub Discussion datasets. The best-performing prompt(s) for each LLM API
    are highlighted in bold. We observed similar results for F1 scores.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 在 Civil Comments 和 GitHub Discussion 数据集上，prompt（Pn）和模型组合的准确性。每个 LLM API
    的最佳表现 prompt 被加粗标出。我们观察到 F1 分数的结果也类似。
- en: 3.2\. Observations
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 观察
- en: 3.2.1\. Prompt performance can regress over API updates.
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. Prompt 性能可能会随着 API 更新而回退。
- en: 'First of all, we found that regression does exist over API updates: 58.8% of
    prompt + model combinations drop accuracy over API updates (Table [2](#S3.T2 "Table
    2 ‣ 3.1.4\. Metrics. ‣ 3.1\. Experiment Setup ‣ 3\. Case Study: Toxicity Detection
    ‣ (Why) Is My Prompt Getting Worse? Rethinking Regression Testing for Evolving
    LLM APIs")). Among them, 70.2% drop accuracy greater than 5%. Noticeably, across
    all different prompts, the model update from text-davinci-002 to text-davinci-003
    causes a consistent performance drop (16.8% on average) on the GitHub Discussion
    Dataset but a consistent performance increase (11.8% on average) on the Civil
    Comments dataset. We hypothesize that the huge performance differences are due
    to the new training method text-davinci-003 used, which causes major inconsistency
    across the two versions.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们发现 API 更新确实存在回归现象：58.8%的提示 + 模型组合在 API 更新后准确性下降（见表 [2](#S3.T2 "表 2 ‣ 3.1.4\.
    指标 ‣ 3.1\. 实验设置 ‣ 3\. 案例研究：毒性检测 ‣（为什么）我的提示变得更差？重新思考演变中的 LLM API 的回归测试")）。其中，70.2%
    的准确性下降超过 5%。值得注意的是，在所有不同提示中，从 text-davinci-002 到 text-davinci-003 的模型更新在 GitHub
    Discussion 数据集上导致了一致的性能下降（平均下降 16.8%），但在 Civil Comments 数据集上导致了一致的性能提升（平均提升 11.8%）。我们假设这些巨大的性能差异是由于
    text-davinci-003 使用了新的训练方法，导致两个版本之间存在重大不一致。
- en: 3.2.2\. Model updates affect different prompting strategies differently.
  id: totrans-78
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 模型更新对不同的提示策略有不同的影响。
- en: We observed that among all model updates, 55% do not cause a consistent performance
    drop or increase across prompts, i.e., the same model update helps some prompts
    but hurts others for the same task. Specifically, we found that the simplest prompt,
    P1, drops accuracy in 75% of the model updates, while the few-shot prompt, P4,
    only drops accuracy 45% of all times. Zooming in, we can see that the update from
    gpt-3.5-turbo-0301 to gpt-3.5-turbo-0613 caused a 9.6% accuracy drop for P1, but
    a 5.1% increase for P4 on the Civil Comments dataset. This is particularly concerning,
    as the update is silent when a developer uses the main API gpt-3.5-turbo, which
    updates the underlying model from time to time.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，在所有模型更新中，55% 的更新不会在提示间造成一致的性能下降或提升，即相同的模型更新对某些提示有帮助，但对其他提示则有害。具体来说，我们发现最简单的提示
    P1 在 75% 的模型更新中准确性下降，而少量示例提示 P4 仅在 45% 的情况下下降。进一步分析，我们可以看到，从 gpt-3.5-turbo-0301
    更新到 gpt-3.5-turbo-0613 导致 P1 在 Civil Comments 数据集上的准确性下降 9.6%，但 P4 的准确性提高了 5.1%。这尤其令人担忧，因为当开发者使用主
    API gpt-3.5-turbo 时，该更新是静默的，而主 API 会不时更新底层模型。
- en: 'Such non-uniform performance changes cause a major problem for prompt engineering:
    The developer may find that their carefully engineered prompt is no longer the
    best choice after a silent API update. For example, the detailed instruction prompt
    (P3) has been the best-performing prompt up to the last model update, but falls
    behind the few-shot prompt (P4) by 8.7% on the latest model (gpt-3.5-turbo-instruct).
    This indicates that prompt engineering is not a one-time effort, and calls for
    prompt versioning and prompt monitoring (see detailed discussion in Section [4.2](#S4.SS2
    "4.2\. Tracking Prompts for Regression Testing ‣ 4\. Towards Regression Testing
    for Prompting LLMs ‣ (Why) Is My Prompt Getting Worse? Rethinking Regression Testing
    for Evolving LLM APIs")).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不均匀的性能变化给提示工程带来了一个主要问题：开发者可能会发现，他们精心设计的提示在静默的 API 更新后不再是最佳选择。例如，详细指令提示（P3）在上一次模型更新之前一直是表现最佳的提示，但在最新模型（gpt-3.5-turbo-instruct）中落后于少量示例提示（P4）8.7%。这表明，提示工程不是一次性工作的，需要进行提示版本控制和提示监控（详见第 [4.2](#S4.SS2
    "4.2\. 追踪提示以进行回归测试 ‣ 4\. 面向回归测试的提示 LLM ‣（为什么）我的提示变得更差？重新思考演变中的 LLM API 的回归测试")节的详细讨论）。
- en: 3.2.3\. Regressions happen even when prompt performance improves.
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 即使提示性能有所提高，也会发生回归。
- en: We also found that overall 10.9% individual predictions regress (from correct
    to wrong) over API updates. Almost always (87.9%) when overall accuracy improves
    in an update, at least one previously correct prediction regresses. For example,
    the model update from text-davinci-002 to text-davinci-003 improves P3’s accuracy
    on the Civil Comments dataset by 7.7%, but 1.8% of the previously correct predictions
    now fail.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还发现，整体上 10.9% 的单独预测在 API 更新中发生了回归（从正确变为错误）。几乎总是（87.9%）当整体准确性在更新中提升时，至少有一个以前正确的预测会回归。例如，从
    text-davinci-002 到 text-davinci-003 的模型更新使 P3 在 Civil Comments 数据集上的准确性提高了 7.7%，但
    1.8% 以前正确的预测现在失败了。
- en: As such regressions are invisible in the aggregated accuracy scores, it would
    be particularly concerning if the improvements and regressions are not uniform–the
    prompt may work better on some data slices but worse on others, causing fairness
    implications even when overall accuracy stays stable or improves.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这种回归在汇总的准确性得分中是不可见的，因此如果改进和回归不均匀，将特别令人担忧——提示可能在某些数据片段上表现更好，但在其他数据片段上表现更差，即使整体准确性保持稳定或改善，也会造成公平性问题。
- en: '| Model | Civil Comments | GitHub Discussion |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Civil Comments | GitHub 讨论 |'
- en: '| Regression | Improvement | Unflipped | Regression | Improvement | Unflipped
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 回归 | 改进 | 未翻转 | 回归 | 改进 | 未翻转 |'
- en: '| gpt-3.5-turbo-instruct | 0.319 | 0.289 | 0.078 | 0.186 | 0.213 | 0.258 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-instruct | 0.319 | 0.289 | 0.078 | 0.186 | 0.213 | 0.258 |'
- en: '| gpt-3.5-turbo-0613 | 0.190 | 0.138 | 0.025 | 0.267 | 0.139 | 0.063 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-0613 | 0.190 | 0.138 | 0.025 | 0.267 | 0.139 | 0.063 |'
- en: '| gpt-3.5-turbo-0301 | 0.075 | 0.096 | 0.006 | 0.015 | 0.010 | 0.026 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| gpt-3.5-turbo-0301 | 0.075 | 0.096 | 0.006 | 0.015 | 0.010 | 0.026 |'
- en: '| text-davinci-003 | 0.022 | 0.028 | 0.010 | 0.005 | 0.018 | 0.018 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-003 | 0.022 | 0.028 | 0.010 | 0.005 | 0.018 | 0.018 |'
- en: '| text-davinci-002 | 0.251 | 0.296 | 0.137 | 0.467 | 0.302 | 0.227 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| text-davinci-002 | 0.251 | 0.296 | 0.137 | 0.467 | 0.302 | 0.227 |'
- en: '| average | 0.171 | 0.169 | 0.051 | 0.188 | 0.136 | 0.118 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | 0.171 | 0.169 | 0.051 | 0.188 | 0.136 | 0.118 |'
- en: Table 3\. Model entropy on the Civil Comments and GitHub Discussion datasets,
    averaged across all prompts.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '表3\. 在Civil Comments和GitHub Discussion数据集上的模型熵，平均跨所有提示。 '
- en: 3.2.4\. Regressions happen beyond the decision boundary.
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 回归发生在决策边界之外。
- en: 'A natural hypothesis is that regressions happen on data points that models
    are less confident with (i.e. near the decision boundary). To explore this hypothesis,
    following existing work (Zhang et al., [2019](#bib.bib45)), we use information
    entropy to measure the model’s confidence on a data point:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一个自然的假设是回归发生在模型对数据点不太自信的地方（即接近决策边界）。为了探究这个假设，参考已有的研究（Zhang等，[2019](#bib.bib45)），我们使用信息熵来衡量模型对数据点的自信程度：
- en: '|  | $E_{j}=\sum_{i}-p_{ij}\cdot\log{p_{ij}}$ |  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $E_{j}=\sum_{i}-p_{ij}\cdot\log{p_{ij}}$ |  |'
- en: where $E_{j}$. Intuitively, when the model’s prediction probabilities are more
    evenly distributed across different labels, the entropy is higher and the model
    is more uncertain on the input. Since many LLM APIs do not expose the actual prediction
    probabilities, we approximate a model’s prediction probabilities by running it
    on the same input multiple (n=20) times with a non-zero temperature (t=0.7).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E_{j}$。直观上，当模型的预测概率在不同标签间分布更均匀时，熵值较高，模型对输入的确定性较低。由于许多LLM API并未公开实际的预测概率，我们通过在相同输入上多次（n=20）运行模型，并使用非零温度（t=0.7），来近似模型的预测概率。
- en: 'Overall, we found that models are indeed more uncertain about flipping data
    points on average (Table [3](#S3.T3 "Table 3 ‣ 3.2.3\. Regressions happen even
    when prompt performance improves. ‣ 3.2\. Observations ‣ 3\. Case Study: Toxicity
    Detection ‣ (Why) Is My Prompt Getting Worse? Rethinking Regression Testing for
    Evolving LLM APIs")). However, we also found that 63.8% of regressions happen
    when models are very confident about their results (i.e. entropy $=0$). This implies
    that model updates can drastically change predictions on data points far away
    from the decision boundary.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，我们发现模型对翻转数据点的平均不确定性确实较高（表[3](#S3.T3 "表 3 ‣ 3.2.3\. 即使提示性能提高也会发生回归。 ‣ 3.2\.
    观察 ‣ 3\. 案例研究：毒性检测 ‣ （为什么）我的提示变得更差了？重新考虑演变中的回归测试")）。然而，我们还发现63.8%的回归发生在模型对其结果非常自信时（即熵
    $=0$）。这意味着模型更新可能会大幅改变远离决策边界的数据点上的预测。
- en: 'Across the models, we also found that different models show different levels
    of self-consistency: gpt-3.5-turbo-0301 seems to be the most self-consistent one,
    while the update to gpt-3.5-turbo-0613 makes it much less self-consistent. This
    indicates another form of *regression*: While the two models’ accuracy is comparable,
    the update can affect model calibration (Zhao et al., [2021](#bib.bib46)) and
    make the model less self-consistent (or over-confident).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在各个模型之间，我们还发现不同模型表现出不同程度的自洽性：gpt-3.5-turbo-0301似乎是最自洽的，而更新到gpt-3.5-turbo-0613后，它的自洽性显著降低。这表明另一种形式的*回归*：尽管这两个模型的准确率相当，但更新可能会影响模型的标定（Zhao等，[2021](#bib.bib46)），使模型的自洽性降低（或过于自信）。
- en: '![Refer to caption](img/4a8fe8302b317b2715becae0dd04b4d5.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4a8fe8302b317b2715becae0dd04b4d5.png)'
- en: Figure 3\. Regressions disproportionally happen when the toxicity relates to
    politics (25.7% vs. 33.3%), targets code (21.6% vs. 33.3%), or is severe (54.1%
    vs. 66.7%).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 当毒性与政治相关（25.7% vs. 33.3%）、针对代码（21.6% vs. 33.3%）或严重（54.1% vs. 66.7%）时，回归发生的比例不成比例。
- en: 3.2.5\. Regressions are not uniform across data slices.
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5\. 回归在数据切片之间不一致。
- en: We next explore where regressions happen systematically for specific data slices,
    with the metadata provided by the authors of the GitHub Discussion dataset (Miller
    et al., [2022](#bib.bib23)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来探索了特定数据切片的回归系统性发生的位置，使用了GitHub讨论数据集作者提供的元数据（Miller等，[2022](#bib.bib23)）。
- en: 'We found that 90% of regressions happen on toxic discussions, despite only
    42.5% of discussions being toxic in the dataset. Breaking down the regression
    on toxic discussions by the provided metadata (Figure [3](#S3.F3 "Figure 3 ‣ 3.2.4\.
    Regressions happen beyond the decision boundary. ‣ 3.2\. Observations ‣ 3\. Case
    Study: Toxicity Detection ‣ (Why) Is My Prompt Getting Worse? Rethinking Regression
    Testing for Evolving LLM APIs")), we found that regressions are disproportionally
    common when the toxicity is triggered by politics (25.7% overall vs. 33.3% among
    regressions), targets code (21.6% vs. 33.3%), or is severe (54.1% vs. 66.7%),
    suggesting that model updates can cause systematic worse performance for these
    specific data slices.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现90%的回归发生在毒性讨论中，尽管数据集中只有42.5%的讨论是有毒的。根据提供的元数据对毒性讨论的回归进行拆解（图[3](#S3.F3 "图3
    ‣ 3.2.4\. 回归发生在决策边界之外。 ‣ 3.2\. 观察 ‣ 3\. 案例研究：毒性检测 ‣ （为什么）我的提示变得更差了？重新考虑进化的LLM
    API的回归测试")），我们发现当毒性由政治触发（25.7%总体 vs. 33.3%回归中）、针对代码（21.6% vs. 33.3%）或严重（54.1%
    vs. 66.7%）时，回归的不成比例更为普遍，这表明模型更新可能导致这些特定数据切片的系统性表现变差。
- en: 3.2.6\. Limitations.
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.6\. 限制。
- en: 'Readers should be careful when generalizing the results beyond the current
    experiment settings: We used specific prompt formats and sent the prompts as a
    single user request. The optimal prompt design may change when the LLM API varies.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 读者在将结果推广到当前实验设置之外时应谨慎：我们使用了特定的提示格式，并将提示作为单个用户请求发送。当LLM API发生变化时，最佳提示设计可能会有所不同。
- en: 4\. Towards Regression Testing for Prompting LLMs
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 针对提示LLM的回归测试
- en: Our exploratory case study highlights that model regression is a real problem
    that is deeply affected by prompting and LLM non-determinism. Based on our observations,
    we conclude with a discussion on how researchers can support regression testing
    for LLMs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的探索性案例研究突出了模型回归是一个真实问题，受到提示和LLM非确定性的深刻影响。基于我们的观察，我们讨论了研究人员如何支持LLM的回归测试。
- en: 4.1\. Identifying Data Slices as Regression Test Suites
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 将数据切片识别为回归测试套件
- en: In our case study, we found that individual predictions regress frequently (10.9%).
    Therefore, treating each data point as a regression test will simply be intractable.
    An alternative would be to look at aggregated metrics over the entire dataset.
    However, this level of monitoring is too coarse-grained and cannot inform developers
    on how to debug and adjust their prompts.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例研究中，我们发现个别预测频繁回归（10.9%）。因此，将每个数据点作为回归测试将是不切实际的。另一种方法是查看整个数据集的聚合指标。然而，这种监控级别过于粗糙，无法为开发人员提供如何调试和调整提示的信息。
- en: We argue that LLM regression tests should be at the level of slices. Our preliminary
    results show that it is possible to look at semantic slices and localize where
    regressions happen (e.g., toxicity targeting code for GitHub toxicity). However,
    our slicing relies on extra metadata, which may not be available for many datasets.
    Future research should further scaffold developers to identify data slices as
    regression test suites, possibly by transferring existing approaches like slice
    discovery (Eyuboglu et al., [2022](#bib.bib12)) and error analysis (Wu et al.,
    [2019](#bib.bib40); Cabrera et al., [2023](#bib.bib7)) on a single model to regression
    testing.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为LLM回归测试应该在切片级别进行。我们的初步结果显示，查看语义切片并定位回归发生的位置是可能的（例如，针对代码的毒性检测）。然而，我们的切片依赖于额外的元数据，这可能在许多数据集中不可用。未来的研究应进一步支持开发人员将数据切片识别为回归测试套件，可能通过将现有方法如切片发现（Eyuboglu等，[2022](#bib.bib12)）和错误分析（Wu等，[2019](#bib.bib40)；Cabrera等，[2023](#bib.bib7)）转移到回归测试上。
- en: 4.2\. Tracking Prompts for Regression Testing
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 追踪提示以进行回归测试
- en: Our case study points out that prompt performance can be unstable across different
    APIs and each API has different best-performing prompts. Therefore, developers
    need to track and update their prompt (possibly from a history version), to maintain
    or improve prompt+LLM performance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的案例研究指出，提示性能在不同 API 之间可能不稳定，每个 API 具有不同的最佳提示。因此，开发者需要跟踪和更新他们的提示（可能是从历史版本中更新），以维持或提高提示+LLM
    的性能。
- en: However, existing prompt engineering practices provide insufficient support
    for prompt versioning and monitoring (Zamfirescu-Pereira et al., [2023](#bib.bib44))–
    Information and knowledge are often lost in the iterative prompt engineering process.
    Future research can design systems for prompt+LLM tracking (e.g., Amershi et al.,
    [2015](#bib.bib3); Mishra et al., [2023](#bib.bib25)) to help developers explore
    behavioral changes, debug regressions, and update their prompts.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现有的提示工程实践对于提示版本控制和监控提供的支持不足 (Zamfirescu-Pereira 等，[2023](#bib.bib44)) ——
    信息和知识在迭代的提示工程过程中常常丢失。未来的研究可以设计用于提示+LLM 跟踪的系统（例如，Amershi 等，[2015](#bib.bib3)；Mishra
    等，[2023](#bib.bib25)），以帮助开发者探索行为变化、调试回归问题并更新他们的提示。
- en: 4.3\. Tackling Non-determinism in LLM Regression Testing
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 处理 LLM 回归测试中的非确定性
- en: Our case study shows that LLM predictions can flip a lot with a non-zero temperature.
    This can cause lots of flakiness when we perform regression testing for LLMs.
    Future research on LLM regression testing should explicitly consider such non-determinism
    in their research design. For example, to avoid a large sample size for each regression
    test, researchers can develop suitable statistical tests and test minimization
    strategies.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的案例研究表明，LLM 预测在非零温度下可能会有很大波动。这在我们进行 LLM 回归测试时可能会导致大量的不稳定性。未来的 LLM 回归测试研究应在其研究设计中明确考虑这种非确定性。例如，为了避免每次回归测试的样本量过大，研究人员可以开发适当的统计测试和测试最小化策略。
- en: While our work focused on classification tasks, regressions can also happen
    for generative tasks, where non-determinism is even more common for generating
    high-quality outputs. To support regression testing LLMs on generative tasks,
    future research should consider incorporating multi-dimensional metrics (Zhong
    et al., [2022](#bib.bib47)) and supporting developers in testing output properties
    specific to their requirements (Ribeiro, [2023](#bib.bib32)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们的工作集中在分类任务上，但生成任务也可能出现回归问题，其中非确定性在生成高质量输出时更为普遍。为了支持对生成任务中的 LLM 进行回归测试，未来的研究应考虑结合多维度指标
    (Zhong 等，[2022](#bib.bib47)) 并支持开发者测试符合其需求的输出属性 (Ribeiro, [2023](#bib.bib32))。
- en: Acknowledgements.
  id: totrans-117
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: We thank Sherry Tongshuang Wu and Rohan Padhye for their discussion and feedback
    on this work.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感谢 Sherry Tongshuang Wu 和 Rohan Padhye 对这项工作的讨论和反馈。
- en: References
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Adebayo et al. (2020) Julius Adebayo, Michael Muelly, Ilaria Liccardi, and Been
    Kim. 2020. Debugging Tests for Model Explanations. In *Proceedings of the 34th
    International Conference on Neural Information Processing Systems* (Vancouver,
    BC, Canada) *(NIPS’20)*. Curran Associates Inc., Article 60, 13 pages.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adebayo 等 (2020) Julius Adebayo, Michael Muelly, Ilaria Liccardi, 和 Been Kim.
    2020. 用于模型解释的调试测试。载于 *第34届国际神经信息处理系统大会论文集*（加拿大温哥华） *(NIPS’20)*。Curran Associates
    Inc., 文章 60, 13 页。
- en: 'Amershi et al. (2015) Saleema Amershi, Max Chickering, Steven M. Drucker, Bongshin
    Lee, Patrice Simard, and Jina Suh. 2015. ModelTracker: Redesigning Performance
    Analysis Tools for Machine Learning. In *Proceedings of the 33rd Annual ACM Conference
    on Human Factors in Computing Systems* (Seoul, Republic of Korea) *(CHI ’15)*.
    Association for Computing Machinery, 337–346.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Amershi 等 (2015) Saleema Amershi, Max Chickering, Steven M. Drucker, Bongshin
    Lee, Patrice Simard, 和 Jina Suh. 2015. ModelTracker: 重新设计机器学习的性能分析工具。载于 *第33届年度
    ACM 人机交互系统会议论文集*（韩国首尔） *(CHI ’15)*。计算机协会，337–346。'
- en: Anthropic (2023) Anthropic. 2023. Claude. [https://claude.ai/](https://claude.ai/)
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthropic (2023) Anthropic. 2023. Claude. [https://claude.ai/](https://claude.ai/)
- en: Barash et al. (2019) Guy Barash, Eitan Farchi, Ilan Jayaraman, Orna Raz, Rachel
    Tzoref-Brill, and Marcel Zalmanovici. 2019. Bridging the Gap between ML Solutions
    and Their Business Requirements Using Feature Interactions. In *Proceedings of
    the 2019 27th ACM Joint Meeting on European Software Engineering Conference and
    Symposium on the Foundations of Software Engineering* (Tallinn, Estonia) *(ESEC/FSE
    2019)*. Association for Computing Machinery, 1048–1058.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barash et al. (2019) Guy Barash、Eitan Farchi、Ilan Jayaraman、Orna Raz、Rachel
    Tzoref-Brill 和 Marcel Zalmanovici。2019年。《通过特征交互弥合ML解决方案与业务需求之间的差距》。发表于 *第27届ACM欧洲软件工程会议与软件工程基础研讨会联合会议*（爱沙尼亚塔林）
    *(ESEC/FSE 2019)*。计算机协会，1048–1058。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, et al. 2020. Language Models are
    Few-Shot Learners. In *Advances in Neural Information Processing Systems*, H. Larochelle,
    M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33\. Curran Associates,
    Inc., 1877–1901.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom Brown、Benjamin Mann 等。2020年。《语言模型是少样本学习者》。发表于 *神经信息处理系统进展*，H.
    Larochelle、M. Ranzato、R. Hadsell、M.F. Balcan 和 H. Lin（编辑），第33卷。Curran Associates,
    Inc.，1877–1901。
- en: 'Cabrera et al. (2023) Ángel Alexander Cabrera, Erica Fu, Donald Bertucci, Kenneth
    Holstein, Ameet Talwalkar, Jason I. Hong, and Adam Perer. 2023. Zeno: An Interactive
    Framework for Behavioral Evaluation of Machine Learning. In *Proceedings of the
    2023 CHI Conference on Human Factors in Computing Systems* (Hamburg, Germany)
    *(CHI ’23)*. Association for Computing Machinery, Article 419, 14 pages.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cabrera et al. (2023) Ángel Alexander Cabrera、Erica Fu、Donald Bertucci、Kenneth
    Holstein、Ameet Talwalkar、Jason I. Hong 和 Adam Perer。2023年。《Zeno：用于机器学习行为评估的互动框架》。发表于
    *2023 CHI计算机系统人因会议论文集*（德国汉堡） *(CHI ’23)*。计算机协会，第419篇，14页。
- en: Chen et al. (2023) Lingjiao Chen, Matei Zaharia, and James Zou. 2023. How is
    ChatGPT’s behavior changing over time? arXiv:2307.09009 [cs.CL]
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Lingjiao Chen、Matei Zaharia 和 James Zou。2023年。《ChatGPT的行为如何随时间变化？》arXiv:2307.09009
    [cs.CL]
- en: cjadams et al. (2019) cjadams, Daniel Borkan, inversion, Jeffrey Sorensen, Lucas
    Dixon, Lucy Vasserman, and nithum. 2019. Jigsaw Unintended Bias in Toxicity Classification.
    [https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification](https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification)
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cjadams et al. (2019) cjadams、Daniel Borkan、inversion、Jeffrey Sorensen、Lucas
    Dixon、Lucy Vasserman 和 nithum。2019年。《Jigsaw 未经意的偏见在毒性分类中的应用》。 [https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification](https://kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification)
- en: Cummaudo et al. (2020) Alex Cummaudo, Scott Barnett, Rajesh Vasa, John Grundy,
    and Mohamed Abdelrazek. 2020. Beware the Evolving ‘intelligent’ Web Service! An
    Integration Architecture Tactic to Guard AI-First Components. In *Proceedings
    of the 28th ACM Joint Meeting on European Software Engineering Conference and
    Symposium on the Foundations of Software Engineering* (Virtual Event, USA) *(ESEC/FSE
    2020)*. Association for Computing Machinery, 269–280.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cummaudo et al. (2020) Alex Cummaudo、Scott Barnett、Rajesh Vasa、John Grundy 和
    Mohamed Abdelrazek。2020年。《当心演变中的‘智能’网络服务！一种集成架构策略以保护AI优先组件》。发表于 *第28届ACM欧洲软件工程会议与软件工程基础研讨会联合会议*（虚拟会议，美国）
    *(ESEC/FSE 2020)*。计算机协会，269–280。
- en: 'Cummaudo et al. (2019) Alex Cummaudo, Rajesh Vasa, John Grundy, Mohamed Abdelrazek,
    and Andrew Cain. 2019. Losing Confidence in Quality: Unspoken Evolution of Computer
    Vision Services. In *2019 IEEE International Conference on Software Maintenance
    and Evolution (ICSME)*. 333–342. [https://doi.org/10.1109/ICSME.2019.00051](https://doi.org/10.1109/ICSME.2019.00051)'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cummaudo et al. (2019) Alex Cummaudo、Rajesh Vasa、John Grundy、Mohamed Abdelrazek
    和 Andrew Cain。2019年。《对质量的信心丧失：计算机视觉服务的无声演变》。发表于 *2019 IEEE国际软件维护与演化会议（ICSME）*。333–342。
    [https://doi.org/10.1109/ICSME.2019.00051](https://doi.org/10.1109/ICSME.2019.00051)
- en: 'Eyuboglu et al. (2022) Sabri Eyuboglu, Maya Varma, Khaled Saab, Jean-Benoit
    Delbrouck, Christopher Lee-Messer, Jared Dunnmon, James Zou, and Christopher Ré.
    2022. Domino: Discovering systematic errors with cross-modal embeddings. *arXiv
    preprint arXiv:2203.14960* (2022).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Eyuboglu et al. (2022) Sabri Eyuboglu、Maya Varma、Khaled Saab、Jean-Benoit Delbrouck、Christopher
    Lee-Messer、Jared Dunnmon、James Zou 和 Christopher Ré。2022年。《Domino：通过跨模态嵌入发现系统误差》。*arXiv预印本
    arXiv:2203.14960*（2022年）。
- en: 'Fried et al. (2023) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke Zettlemoyer, and Mike Lewis.
    2023. InCoder: A Generative Model for Code Infilling and Synthesis. In *The Eleventh
    International Conference on Learning Representations*.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fried et al. (2023) Daniel Fried、Armen Aghajanyan、Jessy Lin、Sida Wang、Eric Wallace、Freda
    Shi、Ruiqi Zhong、Scott Yih、Luke Zettlemoyer 和 Mike Lewis。2023年。《InCoder：代码填充与合成的生成模型》。发表于
    *第十一届国际学习表征会议*。
- en: Google (2023a) Google. 2023a. Bard. [https://bard.google.com/chat](https://bard.google.com/chat)
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2023a) Google. 2023a. Bard. [https://bard.google.com/chat](https://bard.google.com/chat)
- en: Google (2023b) Google. 2023b. Using machine learning to reduce toxicity online.
    [https://www.perspectiveapi.com/](https://www.perspectiveapi.com/)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2023b) Google. 2023b. 使用机器学习减少在线毒性。 [https://www.perspectiveapi.com/](https://www.perspectiveapi.com/)
- en: Hosseini et al. (2017) Hossein Hosseini, Sreeram Kannan, Baosen Zhang, and Radha
    Poovendran. 2017. Deceiving google’s perspective api built for detecting toxic
    comments. *arXiv preprint arXiv:1702.08138* (2017).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hosseini et al. (2017) Hossein Hosseini, Sreeram Kannan, Baosen Zhang, 和 Radha
    Poovendran. 2017. 欺骗Google的视角API，以检测有毒评论。*arXiv 预印本 arXiv:1702.08138* (2017)。
- en: Kaddour et al. (2023) Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie
    Bradley, Roberta Raileanu, and Robert McHardy. 2023. Challenges and applications
    of large language models. *arXiv preprint arXiv:2307.10169* (2023).
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaddour et al. (2023) Jean Kaddour, Joshua Harris, Maximilian Mozes, Herbie
    Bradley, Roberta Raileanu, 和 Robert McHardy. 2023. 大型语言模型的挑战与应用。*arXiv 预印本 arXiv:2307.10169*
    (2023)。
- en: 'Kästner (2022) Christian Kästner. 2022. *Machine Learning in Production: From
    Models to Products*.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kästner (2022) Christian Kästner. 2022. *生产中的机器学习：从模型到产品*。
- en: Li et al. (2013) Jun Li, Yingfei Xiong, Xuanzhe Liu, and Lu Zhang. 2013. How
    does web service API evolution affect clients?. In *2013 IEEE 20th International
    Conference on Web Services*. IEEE, 300–307.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2013) Jun Li, Yingfei Xiong, Xuanzhe Liu, 和 Lu Zhang. 2013. Web服务API演变如何影响客户端？发表于
    *2013 IEEE 第20届国际Web服务会议*。IEEE，300–307。
- en: 'Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023a. Pre-Train, Prompt, and Predict: A Systematic
    Survey of Prompting Methods in Natural Language Processing. *ACM Comput. Surv.*
    55, 9, Article 195 (jan 2023), 35 pages.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, 和 Graham Neubig. 2023a. 预训练、提示和预测：自然语言处理中的提示方法系统综述。*ACM Comput. Surv.*
    55, 9, 文章195 (2023年1月)，35页。
- en: Liu et al. (2023b) Yanjun Liu, Xianfeng Zeng, Fandong Meng, and Jie Zhou. 2023b.
    Instruction Position Matters in Sequence Generation with Large Language Models.
    *ArXiv* abs/2308.12097 (2023). [https://api.semanticscholar.org/CorpusID:261076308](https://api.semanticscholar.org/CorpusID:261076308)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) Yanjun Liu, Xianfeng Zeng, Fandong Meng, 和 Jie Zhou. 2023b.
    指令位置在大型语言模型序列生成中的重要性。*ArXiv* abs/2308.12097 (2023)。 [https://api.semanticscholar.org/CorpusID:261076308](https://api.semanticscholar.org/CorpusID:261076308)
- en: 'Lu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and
    Pontus Stenetorp. 2022. Fantastically Ordered Prompts and Where to Find Them:
    Overcoming Few-Shot Prompt Order Sensitivity. In *Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*.
    Association for Computational Linguistics, 8086–8098.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu et al. (2022) Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, 和 Pontus
    Stenetorp. 2022. 奇妙排列的提示及其发现方式：克服少样本提示顺序敏感性。发表于 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*。计算语言学协会，8086–8098。
- en: Miller et al. (2022) Courtney Miller, Sophie Cohen, Daniel Klug, Bogdan Vasilescu,
    and Christian Kästner. 2022. “Did You Miss My Comment or What?” Understanding
    Toxicity in Open Source Discussions. In *2022 IEEE/ACM 44th International Conference
    on Software Engineering (ICSE)*. 710–722. [https://doi.org/10.1145/3510003.3510111](https://doi.org/10.1145/3510003.3510111)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller et al. (2022) Courtney Miller, Sophie Cohen, Daniel Klug, Bogdan Vasilescu,
    和 Christian Kästner. 2022. “你是不是忽视了我的评论？” 理解开源讨论中的毒性。发表于 *2022 IEEE/ACM 第44届国际软件工程会议
    (ICSE)*. 710–722. [https://doi.org/10.1145/3510003.3510111](https://doi.org/10.1145/3510003.3510111)
- en: 'Mishra (2019) Abhishek Mishra. 2019. Machine learning in the AWS cloud: Add
    intelligence to applications with Amazon Sagemaker and Amazon Rekognition. [https://aws.amazon.com/rekognition/](https://aws.amazon.com/rekognition/)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra (2019) Abhishek Mishra. 2019. AWS云中的机器学习：通过Amazon Sagemaker和Amazon Rekognition为应用程序添加智能。
    [https://aws.amazon.com/rekognition/](https://aws.amazon.com/rekognition/)
- en: 'Mishra et al. (2023) Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang,
    Bum Chul Kwon, and Chris Bryan. 2023. PromptAid: Prompt Exploration, Perturbation,
    Testing and Iteration using Visual Analytics for Large Language Models. *arXiv
    preprint arXiv:2304.01964* (2023).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mishra et al. (2023) Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang,
    Bum Chul Kwon, 和 Chris Bryan. 2023. PromptAid: 使用视觉分析探索、扰动、测试和迭代大型语言模型的提示。*arXiv
    预印本 arXiv:2304.01964* (2023)。'
- en: Naik et al. (2018) Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn
    Rose, and Graham Neubig. 2018. Stress Test Evaluation for Natural Language Inference.
    In *Proceedings of the 27th International Conference on Computational Linguistics*.
    Association for Computational Linguistics, 2340–2353.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naik et al. (2018) Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn
    Rose, 和 Graham Neubig. 2018. 自然语言推理的压力测试评估。见于 *第27届计算语言学国际会议论文集*。计算语言学协会，2340–2353。
- en: OpenAI (2023a) OpenAI. 2023a. ChatGPT. [https://chat.openai.com/](https://chat.openai.com/)
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023a) OpenAI. 2023a. ChatGPT。 [https://chat.openai.com/](https://chat.openai.com/)
- en: OpenAI (2023b) OpenAI. 2023b. Deprecations - OpenAI API. [https://platform.openai.com/docs/deprecations](https://platform.openai.com/docs/deprecations)
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023b) OpenAI. 2023b. 废弃功能 - OpenAI API。 [https://platform.openai.com/docs/deprecations](https://platform.openai.com/docs/deprecations)
- en: OpenAI (2023c) OpenAI. 2023c. GPT-3.5 Documentation. Retrieved from. [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023c) OpenAI. 2023c. GPT-3.5 文档。检索自 [https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)
- en: 'Ouyang et al. (2023) Shuyin Ouyang, Jie M Zhang, Mark Harman, and Meng Wang.
    2023. LLM is Like a Box of Chocolates: the Non-determinism of ChatGPT in Code
    Generation. *arXiv preprint arXiv:2308.02828* (2023).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2023) Shuyin Ouyang, Jie M Zhang, Mark Harman, 和 Meng Wang. 2023.
    LLM 就像一盒巧克力：ChatGPT 在代码生成中的非确定性。*arXiv 预印本 arXiv:2308.02828*（2023年）。
- en: radiator57 (2023) radiator57\. 2023. Experiencing Decreased Performance with
    ChatGPT-4. [https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269](https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269)
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: radiator57 (2023) radiator57。2023. 使用 ChatGPT-4 性能下降的体验。 [https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269](https://community.openai.com/t/experiencing-decreased-performance-with-chatgpt-4/234269)
- en: Ribeiro (2023) Marco Tulio Ribeiro. 2023. Testing language models (and prompts)
    like we test software. *Medium* (May 2023). [https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359](https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359)
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro (2023) Marco Tulio Ribeiro. 2023. 像测试软件一样测试语言模型（和提示）。*Medium*（2023年5月）。[https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359](https://towardsdatascience.com/testing-large-language-models-like-we-test-software-92745d28a359)
- en: 'Ribeiro and Lundberg (2022) Marco Tulio Ribeiro and Scott Lundberg. 2022. Adaptive
    Testing and Debugging of NLP Models. In *Proceedings of the 60th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, Smaranda
    Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational
    Linguistics, 3253–3267.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro and Lundberg (2022) Marco Tulio Ribeiro 和 Scott Lundberg. 2022. NLP
    模型的自适应测试和调试。见于 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*，Smaranda Muresan, Preslav Nakov, 和
    Aline Villavicencio（编）。计算语言学协会，3253–3267。
- en: 'Ribeiro et al. (2020) Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,
    and Sameer Singh. 2020. Beyond Accuracy: Behavioral Testing of NLP Models with
    CheckList. In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.).
    Association for Computational Linguistics, 4902–4912.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro et al. (2020) Marco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin, 和
    Sameer Singh. 2020. 超越准确性：使用 CheckList 对 NLP 模型进行行为测试。见于 *第58届计算语言学协会年会论文集*，Dan
    Jurafsky, Joyce Chai, Natalie Schluter, 和 Joel Tetreault（编）。计算语言学协会，4902–4912。
- en: Sommerville (2015) Ian Sommerville. 2015. *Software Engineering* (10th ed.).
    Pearson.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sommerville (2015) Ian Sommerville. 2015. *软件工程*（第10版）。Pearson。
- en: Sun et al. (2020) Zeyu Sun, Jie M. Zhang, Mark Harman, Mike Papadakis, and Lu
    Zhang. 2020. Automatic Testing and Improvement of Machine Translation. In *Proceedings
    of the ACM/IEEE 42nd International Conference on Software Engineering* (Seoul,
    South Korea) *(ICSE ’20)*. Association for Computing Machinery, 974–985.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2020) Zeyu Sun, Jie M. Zhang, Mark Harman, Mike Papadakis, 和 Lu
    Zhang. 2020. 自动化测试和改进机器翻译。见于 *ACM/IEEE第42届国际软件工程大会论文集*（首尔，韩国）*(ICSE ’20)*。计算机协会，974–985。
- en: Sun et al. (2022) Zeyu Sun, Jie M. Zhang, Yingfei Xiong, Mark Harman, Mike Papadakis,
    and Lu Zhang. 2022. Improving Machine Translation Systems via Isotopic Replacement.
    In *Proceedings of the 44th International Conference on Software Engineering*
    (Pittsburgh, Pennsylvania) *(ICSE ’22)*. Association for Computing Machinery,
    1181–1192.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2022) Zeyu Sun, Jie M. Zhang, Yingfei Xiong, Mark Harman, Mike Papadakis,
    和 Lu Zhang. 2022. 通过同位素替换改进机器翻译系统。见于 *第44届国际软件工程大会论文集*（匹兹堡，宾夕法尼亚州）*(ICSE ’22)*。计算机协会，1181–1192。
- en: 'Tu et al. (2023) Shangqing Tu, Chunyang Li, Jifan Yu, Xiaozhi Wang, Lei Hou,
    and Juanzi Li. 2023. ChatLog: Recording and Analyzing ChatGPT Across Time. *arXiv
    preprint arXiv:2304.14106* (2023).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu 等 (2023) Shangqing Tu, Chunyang Li, Jifan Yu, Xiaozhi Wang, Lei Hou, 和 Juanzi
    Li. 2023. ChatLog：记录和分析 ChatGPT 随时间的变化。*arXiv 预印本 arXiv:2304.14106* (2023)。
- en: Wang and Chang (2022) Yau-Shian Wang and Yingshan Chang. 2022. Toxicity detection
    with generative prompt-based inference. *arXiv preprint arXiv:2205.12390* (2022).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Chang (2022) Yau-Shian Wang 和 Yingshan Chang. 2022. 基于生成提示推理的毒性检测。*arXiv
    预印本 arXiv:2205.12390* (2022)。
- en: 'Wu et al. (2019) Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel
    Weld. 2019. Errudite: Scalable, Reproducible, and Testable Error Analysis. In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*,
    Anna Korhonen, David Traum, and Lluís Màrquez (Eds.). Association for Computational
    Linguistics, 747–763.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2019) Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, 和 Daniel Weld.
    2019. Errudite：可扩展、可复现和可测试的错误分析。在 *第57届计算语言学协会年会论文集* 中，Anna Korhonen, David Traum,
    和 Lluís Màrquez (编)。计算语言学协会，747–763。
- en: Yang et al. (2023a) Chenyang Yang, Rachel A Brower-Sinning, Grace Lewis, Christian
    Kästner, and Tongshuang Wu. 2023a. Capabilities for Better ML Engineering. In
    *Proceedings of the AAAI-23 Workshop on Artificial Intelligence Safety (SafeAI)*
    (Washington, DC).
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 (2023a) Chenyang Yang, Rachel A Brower-Sinning, Grace Lewis, Christian
    Kästner, 和 Tongshuang Wu. 2023a. 提升机器学习工程能力。在 *2023 AAAI 人工智能安全研讨会 (SafeAI) 论文集*
    中 (华盛顿特区)。
- en: 'Yang et al. (2023b) Chenyang Yang, Rishabh Rustogi, Rachel Brower-Sinning,
    Grace A Lewis, Christian Kästner, and Tongshuang Wu. 2023b. Beyond Testers’ Biases:
    Guiding Model Testing with Knowledge Bases using LLMs. (12 2023). [http://arxiv.org/abs/2310.09668](http://arxiv.org/abs/2310.09668)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 (2023b) Chenyang Yang, Rishabh Rustogi, Rachel Brower-Sinning, Grace
    A Lewis, Christian Kästner, 和 Tongshuang Wu. 2023b. 超越测试者偏见：利用知识库和 LLM 指导模型测试。
    (2023年12月)。 [http://arxiv.org/abs/2310.09668](http://arxiv.org/abs/2310.09668)
- en: 'Yin (2009) Robert K Yin. 2009. *Case study research: Design and methods*. Vol. 5.
    sage.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin (2009) Robert K Yin. 2009. *案例研究：设计与方法*。第5卷。sage。
- en: 'Zamfirescu-Pereira et al. (2023) J.D. Zamfirescu-Pereira, Richmond Y. Wong,
    Bjoern Hartmann, and Qian Yang. 2023. Why Johnny Can’t Prompt: How Non-AI Experts
    Try (and Fail) to Design LLM Prompts. In *Proceedings of the 2023 CHI Conference
    on Human Factors in Computing Systems* (Hamburg, Germany) *(CHI ’23)*. Association
    for Computing Machinery, Article 437, 21 pages.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zamfirescu-Pereira 等 (2023) J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern
    Hartmann, 和 Qian Yang. 2023. 为什么 Johnny 不能提示：非人工智能专家如何尝试（以及失败）设计 LLM 提示。在 *2023
    CHI 人机交互会议论文集* 中 (汉堡，德国) *(CHI ’23)*。计算机协会，文章 437，21 页。
- en: 'Zhang et al. (2019) Xuchao Zhang, Fanglan Chen, Chang-Tien Lu, and Naren Ramakrishnan.
    2019. Mitigating Uncertainty in Document Classification. In *Proceedings of the
    2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, Jill
    Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Computational
    Linguistics, 3126–3136.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 (2019) Xuchao Zhang, Fanglan Chen, Chang-Tien Lu, 和 Naren Ramakrishnan.
    2019. 减少文档分类中的不确定性。在 *2019 年北美计算语言学协会会议：人类语言技术，第1卷（长文和短文）* 中，Jill Burstein, Christy
    Doran, 和 Thamar Solorio (编)。计算语言学协会，3126–3136。
- en: 'Zhao et al. (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. 2021. Calibrate Before Use: Improving Few-shot Performance of Language
    Models. In *Proceedings of the 38th International Conference on Machine Learning*
    *(Proceedings of Machine Learning Research, Vol. 139)*, Marina Meila and Tong
    Zhang (Eds.). PMLR, 12697–12706.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等 (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, 和 Sameer Singh.
    2021. 使用前校准：提升语言模型的少样本性能。在 *第38届国际机器学习会议论文集* *(机器学习研究论文集，第139卷)* 中，Marina Meila
    和 Tong Zhang (编)。PMLR，12697–12706。
- en: Zhong et al. (2022) Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei
    Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a Unified Multi-Dimensional
    Evaluator for Text Generation. In *Proceedings of the 2022 Conference on Empirical
    Methods in Natural Language Processing*, Yoav Goldberg, Zornitsa Kozareva, and
    Yue Zhang (Eds.). Association for Computational Linguistics, 2023–2038.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhong 等 (2022) Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei
    Liu, Chenguang Zhu, Heng Ji, 和 Jiawei Han. 2022. 面向统一的多维文本生成评估器。在 *2022 年自然语言处理实证方法会议论文集*
    中，Yoav Goldberg, Zornitsa Kozareva, 和 Yue Zhang (编)。计算语言学协会，2023–2038。
