- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:56'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Learning Interpretable Style Embeddings via Prompting LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.12696](https://ar5iv.labs.arxiv.org/html/2305.12696)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ajay Patel
  prefs: []
  type: TYPE_NORMAL
- en: University of Pennsylvania
  prefs: []
  type: TYPE_NORMAL
- en: ajayp@upenn.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Delip Rao'
  prefs: []
  type: TYPE_NORMAL
- en: University of Pennsylvania
  prefs: []
  type: TYPE_NORMAL
- en: deliprao@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Ansh Kothary'
  prefs: []
  type: TYPE_NORMAL
- en: Columbia University
  prefs: []
  type: TYPE_NORMAL
- en: ank2145@columbia.edu
  prefs: []
  type: TYPE_NORMAL
- en: \ANDKathleen McKeown
  prefs: []
  type: TYPE_NORMAL
- en: Columbia University
  prefs: []
  type: TYPE_NORMAL
- en: kathy@cs.columbia.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Chris Callison-Burch'
  prefs: []
  type: TYPE_NORMAL
- en: University of Pennsylvania
  prefs: []
  type: TYPE_NORMAL
- en: ccb@upenn.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Style representation learning builds content-independent representations of
    author style in text. To date, no large dataset of texts with stylometric annotations
    on a wide range of style dimensions has been compiled, perhaps because the linguistic
    expertise to perform such annotation would be prohibitively expensive. Therefore,
    current style representation approaches make use of unsupervised neural methods
    to disentangle style from content to create style vectors. These approaches, however,
    result in uninterpretable representations, complicating their usage in downstream
    applications like authorship attribution where auditing and explainability is
    critical. In this work, we use prompting to perform stylometry on a large number
    of texts to generate a synthetic stylometry dataset. We use this synthetic data
    to then train human-interpretable style representations we call Lisa embeddings.
    We release our synthetic dataset (StyleGenome) and our interpretable style embedding
    model (Lisa) as resources.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Interpretable Style Embeddings via Prompting LLMs
  prefs: []
  type: TYPE_NORMAL
- en: Ajay Patel University of Pennsylvania ajayp@upenn.edu                       
    Delip Rao University of Pennsylvania deliprao@gmail.com                       
    Ansh Kothary Columbia University ank2145@columbia.edu
  prefs: []
  type: TYPE_NORMAL
- en: Kathleen McKeown Columbia University kathy@cs.columbia.edu                       
    Chris Callison-Burch University of Pennsylvania ccb@upenn.edu
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Style representation learning aims to represent the stylistic attributes of
    an authored text. Prior work has treated the style of a text as separable from
    the content. Stylistic attributes have included, but are not limited to, linguistic
    choices in syntax, grammar, spelling, vocabulary, and punctuation (Jafaritazehjani
    et al., [2020](#bib.bib24)). Style representations should represent two texts
    with similar stylistic attributes more closely than texts with different attributes
    independent of what content is present in the texts.
  prefs: []
  type: TYPE_NORMAL
- en: Stylometry, the analysis of style, applies forensic linguistics to tasks like
    authorship attribution. Stylometry often relies on semi-manual analysis by forensic
    linguistic experts (Mosteller and Wallace, [1963](#bib.bib34); Holmes, [1994](#bib.bib21);
    Rosso et al., [2016](#bib.bib49)). Computational stylometry often uses rule-based
    approaches utilizing count-based features like the frequencies of function words
    (Stamatatos, [2009](#bib.bib54); Koppel et al., [2009](#bib.bib29); Tausczik and
    Pennebaker, [2010](#bib.bib55)). More modern, neural approaches attempt to learn
    style representations in an unsupervised fashion through a proxy task like style
    transfer (Shen et al., [2017](#bib.bib53); Fu et al., [2018](#bib.bib15); John
    et al., [2019](#bib.bib25); Dai et al., [2019](#bib.bib11); Li et al., [2019](#bib.bib30);
    Yi et al., [2021](#bib.bib64); Zhu et al., [2022](#bib.bib67)) or authorship verification
    (Boenninghoff et al., [2019](#bib.bib7); Hay et al., [2020](#bib.bib19); Zhu and
    Jurgens, [2021](#bib.bib66); Wegmann et al., [2022](#bib.bib61)). These stronger
    neural approaches, unlike simpler frequency-based techniques, are uninterpretable.
    This makes it difficult to effectively analyze their representations and their
    failure modes, and precludes their usage in real-world authorship attribution
    scenarios because interpretability and verification is critical for legal admissibility
    (Tiersma and Solan, [2002](#bib.bib56)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fc92609b86b8f749977271a244e8a085.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An example of a 768-dimensional interpretable style vector produced
    by Lisa, trained using a GPT-3 annotated synthetic stylometery dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this motivation, we propose a human-interpretable style representation
    model $\mathcal{M}$) of stylistic attributes would likely require annotators with
    linguistic expertise and be prohibitively expensive. Given this, we use GPT-3
    (Brown et al., [2020](#bib.bib8)), a large language model (LLM), and zero-shot
    prompts to generate a synthetic dataset we call StyleGenome of human-interpretable
    stylometric annotations for various texts. Our approach is motivated by recent
    works showing models trained on synthetic datasets annotated by prompting LLMs
    can match and sometimes even outperform models trained on human-labeled datasets
    (Wang et al., [2022](#bib.bib57); Gilardi et al., [2023](#bib.bib16); Huang et al.,
    [2022](#bib.bib23); Honovich et al., [2022](#bib.bib22)). Training on StyleGenome,
    we develop the Linguistically-Interpretable Style Attribute (Lisa) embedding model.
    We summarize our primary contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We outline an unsupervised method for producing interpretable style embeddings
    using zero-shot prompting and distillation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We generate and release StyleGenome, a synthetic stylometry dataset with ~5.5M
    examples, the first large-scale dataset with texts paired with wide range of stylometric
    annotations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We train, evaluate, and release Lisa, the first-ever interpretable style embedding
    model with a wide variety of linguistic dimensions ($D=768$). We find Lisa matches
    the performance of existing style representations, while allowing for explainability
    and transparency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Generating StyleGenome
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To create StyleGenome for training Lisa, we select Reddit data from the Million
    User Dataset (MUD) (Khan et al., [2021](#bib.bib27); Andrews and Bishop, [2019](#bib.bib3))
    to stylometrically annotate following prior work that uses Reddit to source a
    diversity of styles from different authors (Wegmann et al., [2022](#bib.bib61)).
    We sample 10 random posts per author for 1,000 random authors, resulting in 10,000
    total posts selected for annotation. We display some of the diversity of styles
    captured in the following examples from our Reddit authors. They vary in formality,
    punctuation, emoji usage, etc.:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reddit User #47: forreal though sell that **** its worth like 650$ Reddit User
    #205: This was amazing :D Please, make more! Reddit User #216: I thought that
    was Israel…or Saudi Arabia…or Egypt…Venezuela? Reddit User #356: I wouldn’t watch
    a football game like that either. It’s novel, but I think its appeal is limited
    and again, not the same thing as actual VR where things feel tangible and to scale.'
  prefs: []
  type: TYPE_NORMAL
- en: We use GPT-3¹¹1We use text-davinci-003 with a temperature setting of 0.0 and
    a top_p setting of 1.0. (Brown et al., [2020](#bib.bib8); Ouyang et al., [2022](#bib.bib37))
    to annotate each post in two stages.
  prefs: []
  type: TYPE_NORMAL
- en: Stage 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We elicit the model with a zero-shot prompt to generate a description of the
    style of a given Reddit post:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt: Write a long paragraph describing the unique grammar style of the following
    passage without referring to specifics about the topic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Passage: Wow that’s really cool. I’ll have to watch for that next time I watch
    it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Description:'
  prefs: []
  type: TYPE_NORMAL
- en: $\downarrow$
  prefs: []
  type: TYPE_NORMAL
- en: 'Generation: The passage reflects a conversational style in its grammar, with
    short, simple sentences that are easy to understand. The language is informal
    and direct, with the speaker expressing their enthusiasm for the topic in a straightforward
    manner. The speaker also uses contractions, such as “I’ll," to emphasize their
    excitement. The speaker also uses a casual tone, with the phrase "really cool"
    to emphasize their interest in the topic. The speaker also uses the present tense
    to express their anticipation for the future, with the phrase “next time I watch
    it." This style of grammar is effective in conveying the speaker’s enthusiasm
    and interest in the topic.'
  prefs: []
  type: TYPE_NORMAL
- en: Stage 2
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We use another zero-shot prompt to standardize the generations from Stage 1
    into short, declarative sentences with a uniform structure:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt: Here’s a description of an author’s writing style for a passage: The
    passage utilizes a conversational style of grammar, with short…'
  prefs: []
  type: TYPE_NORMAL
- en: Rewrite this description as a long list of short sentences describing the author’s
    writing style where each sentence is in the format of "The author is X." or "The
    author uses X.".
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: $\downarrow$
  prefs: []
  type: TYPE_NORMAL
- en: 'Generation: The author is using a conversational style of grammar.'
  prefs: []
  type: TYPE_NORMAL
- en: The author is using short, simple sentences.
  prefs: []
  type: TYPE_NORMAL
- en: The author is using language that is informal and direct.
  prefs: []
  type: TYPE_NORMAL
- en: The author is expressing enthusiasm for the topic in a straightforward manner.
  prefs: []
  type: TYPE_NORMAL
- en: The author is using contractions, such as "I’ll".
  prefs: []
  type: TYPE_NORMAL
- en: The author is using a casual tone.
  prefs: []
  type: TYPE_NORMAL
- en: The author is emphasizing their interest in the topic with the phrase "really
    cool".
  prefs: []
  type: TYPE_NORMAL
- en: The author is using the present tense to express anticipation for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'We run both stages with 93 different Stage 1 prompts for all 10,000 posts²²2We
    preprocess posts to the first 25 sentences and use the emoji Python package to
    convert emojis to textual representations for better tokenization. . 6 of the
    93 total prompts are open-ended prompts that elicit descriptions of a passage
    on a broad dimension of style:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a long paragraph describing the unique
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: grammar style of the following passage…
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a long paragraph describing the unique vocabulary style of the following
    passage…
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a long paragraph describing the unique punctuation style of the following
    passage…
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: … see all 6 open-ended prompts in Appendix [A.1](#A1.SS1 "A.1 Open-ended Prompt
    Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remaining 87 prompts target narrow and specific dimensions of style:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a description of whether the author of the following passage has any figurative
    language …
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a description of whether the author of the following passage has any swear
    words …
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a description of whether the author of the following passage has any repeated
    words …
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: … see all 87 targeted prompts in Appendix [A.2](#A1.SS2 "A.2 Targeted Prompt
    Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs")
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The 87 targeted prompts are derived from surveys of stylometry literature, and
    they cover all of  (Tausczik and Pennebaker, [2010](#bib.bib55))’s linguistic
    and psychological categories. See Appendix [A.2](#A1.SS2 "A.2 Targeted Prompt
    Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs") for more details. We report the results of an ablation experiment
    between the two Stage 1 prompt categories in Appendix [C](#A3 "Appendix C Annotation
    Prompts Ablation ‣ Learning Interpretable Style Embeddings via Prompting LLMs").
    Appendix [D](#A4 "Appendix D StyleGenome Annotation Cost ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") details dataset annotation costs.
  prefs: []
  type: TYPE_NORMAL
- en: StyleGenome
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The output of Stage 2 is sentence tokenized³³3We use the sentence-splitter Python
    package. and filtered to keep only sentences beginning with “The author”. We refer
    to these sentences as human-interpretable style attributes. Our method annotates
    the texts with nearly 1.3M style attributes. These style attributes are represented
    in natural language so “The author creates a conversational tone” and “The author
    has a conversational tone” are counted separately in the raw dataset. Our training
    procedure in Section [3.1](#S3.SS1 "3.1 Sfam ‣ 3 Method ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") is able to train directly on these natural
    language style attributes, obviating a normalization step. Some annotations may
    be hallucinated resulting in a noisy dataset, but we choose to train on the full
    synthetic dataset, without manual intervention, to maintain an unsupervised procedure
    following prior work (Wang et al., [2022](#bib.bib57)). We hypothesize our model
    will find signal in the noise, which we evaluate in Section [4](#S4 "4 Results
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs"). The final dataset
    statistics can be found in Table [1](#S2.T1 "Table 1 ‣ StyleGenome ‣ 2 Generating
    StyleGenome ‣ Learning Interpretable Style Embeddings via Prompting LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '| # of Reddit Authors | 1,000 |'
  prefs: []
  type: TYPE_TB
- en: '| # of Reddit Posts | 10,000 |'
  prefs: []
  type: TYPE_TB
- en: '| # of Interpretable Style Attributes | 1,255,874 |'
  prefs: []
  type: TYPE_TB
- en: '| # of (Text, Style Attribute) labeled pairs | 5,490,847 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Statistics for the StyleGenome dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first distill stylometric annotation knowledge from GPT-3 into a Style Feature
    Agreement Model (Sfam). Given a text $t$-dimensional vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{M}_{\textsc{Lisa}}(t)=\bigl{(}{\textsc{Sfam}}(t,a_{0}),{\textsc{Sfam}}(t,a_{1}),\ldots,{\textsc{Sfam}}(t,a_{D})\bigr{)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The Euclidean distance between style vectors for two texts ${\lVert\mathcal{M}_{\textsc{Lisa}}(t_{2})-\mathcal{M}_{\textsc{Lisa}}(t_{1})\rVert}_{2}$
    a Lisa style embedding. We discuss training in detail next, leaving hyperparameter
    and implementation specifics in Appendix [E](#A5 "Appendix E Training Details
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: '| Type | Dataset | Style Attribute | Spearman Correlation ($\rho$) |'
  prefs: []
  type: TYPE_TB
- en: '| Formality | Formality in Online Communication | The author uses informal
    language. | 0.599 |'
  prefs: []
  type: TYPE_TB
- en: '| Grammarly’s Yahoo Answers Formality Corpus | 0.200 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment | Yelp Reviews | The author uses a negative tone. | 0.788 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB Large Movie Review Dataset | 0.665 |'
  prefs: []
  type: TYPE_TB
- en: '| …abbreviated for space, see Appendix [F](#A6 "Appendix F Full Sfam Evaluation
    Results ‣ Learning Interpretable Style Embeddings via Prompting LLMs") for full
    results |  |'
  prefs: []
  type: TYPE_TB
- en: '| Emotion | DAIR.AI Emotion (Love vs. Anger) | The author is expressing {{emotion}}.
    | 0.542 |'
  prefs: []
  type: TYPE_TB
- en: '| DAIR.AI Emotion (Joy vs. Sad) | 0.531 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Love vs. Anger) | 0.769 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Joy vs. Sadness) | 0.639 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Disgust vs. Desire) | 0.630 |'
  prefs: []
  type: TYPE_TB
- en: '| …abbreviated for space, see Appendix [F](#A6 "Appendix F Full Sfam Evaluation
    Results ‣ Learning Interpretable Style Embeddings via Prompting LLMs") for full
    results |  |'
  prefs: []
  type: TYPE_TB
- en: '| Author Profiling | Political Slant | The author is a Democrat. | 0.005 |'
  prefs: []
  type: TYPE_TB
- en: '| Twitter User Gender Classification | The author is female. | 0.166 |'
  prefs: []
  type: TYPE_TB
- en: '| African-American Vernacular English | The author uses African-American Vernacular
    English. | 0.238 |'
  prefs: []
  type: TYPE_TB
- en: '| Shakespeare (Early Modern English) | The author uses Early Modern English.
    | 0.108 |'
  prefs: []
  type: TYPE_TB
- en: '| Wikipedia Bias | The author has a biased point of view. | 0.014 |'
  prefs: []
  type: TYPE_TB
- en: '| Harmful Speech | HateSpeech18 | The author’s writing contains hate speech.
    | 0.229 |'
  prefs: []
  type: TYPE_TB
- en: '| Offensive Social Media | The author uses offensive language. | 0.401 |'
  prefs: []
  type: TYPE_TB
- en: '| Text Simplification | Simple Wikipedia | The author uses simple language.
    | 0.043 |'
  prefs: []
  type: TYPE_TB
- en: '| ASSET | 0.050 |'
  prefs: []
  type: TYPE_TB
- en: '| Linguistic Acceptability | CoLA | The author uses incorrect grammar. | 0.078
    |'
  prefs: []
  type: TYPE_TB
- en: '| BLiMP | 0.020 |'
  prefs: []
  type: TYPE_TB
- en: '| Average |  |  | 0.342 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Correlation of agreement scores produced by Sfam against human judgments
    on texts over a wide variety linguistic and authorship dimensions. The natural
    language style attributes used as input to Sfam when producing the agreement scores
    for each dataset are also provided.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Sfam
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use distillation (Ba and Caruana, [2014](#bib.bib5)) to teach the stylometric
    annotation capabilities of GPT-3 to EncT5⁴⁴4We use the t5-base model and truncate
    at 512 tokens. (Liu et al., [2021](#bib.bib31); Raffel et al., [2020](#bib.bib44)),
    a smaller, more efficient student model.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling Batches
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We train EncT5 with a binary classifier head on randomly sampled batches of
    examples $(x_{i},y_{i})$ “The author is using a positive tone.|||You got this
    ;)”. Labeled pairs from StyleGenome are sampled as positive examples such that
    each style attribute is sampled with equal probability. For each positive example,
    we perform negative sampling and retrieve a negative example text where the positive
    example’s style attribute is likely not present. To do this, we find the 10,000
    most dissimilar style attributes to the positive example’s style attribute with
    SBERT⁵⁵5We use the nli-distilroberta-base-v2 model (Sanh et al., [2019](#bib.bib50);
    Liu et al., [2019](#bib.bib32); Devlin et al., [2019](#bib.bib14); Reimers and
    Gurevych, [2019](#bib.bib46)) for all SBERT usage in this paper. similarity. We
    select a text that is positively labeled with a randomly selected dissimilar style
    attribute as the negative example text.
  prefs: []
  type: TYPE_NORMAL
- en: '| Text | Top 5 Lisa Vector Dimensions |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Lol right on | 1\. 1.00 – The author is being polite. 2\. 1.00 – The author
    is writing in a cheerful manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. 1.00 – The author is using a lighthearted tone.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. 1.00 – The author is laughing.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. 1.00 – The author is complimentary. |
  prefs: []
  type: TYPE_NORMAL
- en: '| There is the Toyota GT86 R3 ;) http://www.toyota-motorsport.com/motorsport/downloads/com_droppics/59/
    DSF4311.jpg | 1\. 0.99 – The author is simply describing a product. $\ast$ 2\.
    0.99 – The author is providing a visual cue to the reader.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. 0.98 – The author simply provides information.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. 0.97 – The author is using an emoji.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. 0.97 – The author provides information. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Every time i watched this episode as a kid i was always like "WTF, JAMES,
    A POKEBALL ISNT EVEN A POKEMON?! GET YOUR ACT TOGETHER, SON!!" | 1\. 1.00 – The
    author is discussing a television show. $\ast$ 2\. 0.99 – The author has a hint
    of nostalgia.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. 0.99 – The author is making a humorous comment.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. 0.99 – The author is animated in their writing.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. 0.99 – The author is laughing. |
  prefs: []
  type: TYPE_NORMAL
- en: '| 13 POT MORDE BABY WOOOOOOOOOOOOOOOO | 1\. 1.00 – The author is using an elongated
    word. 2\. 1.00 – The author is using only English words.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. 1.00 – The author is using a single word.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. 1.00 – The author is using swear words.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. 1.00 – The author uses two exclamation marks. |
  prefs: []
  type: TYPE_NORMAL
- en: '| No wonder everyone resorts to performing a murder spree eventually. | 1\.
    1.00 – The author is scornful. 2\. 1.00 – The author is ungenerous.'
  prefs: []
  type: TYPE_NORMAL
- en: 3\. 0.99 – The author is expressing antisocial behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. 0.99 – The author is uncaring.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. 0.99 – The author is dramatic. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Podcast originally refers to an iPod, and before that there was definitely
    TWiT, which still calls itself a Netcast | 1\. 0.97 – The author uses a variety
    of words to describe              the same concept. 2\. 0.97 – The author is simply
    describing a product. $\ast$'
  prefs: []
  type: TYPE_NORMAL
- en: 3. 0.95 – The author uses specific terms related to the              topic.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. 0.94 – The author has a deep understanding of the topic.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. 0.94 – The author is using words focusing on the past. |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: The five highest scoring dimensions from the 768-dimensional Lisa
    vector produced on various Reddit texts. The interpretable style attribute corresponding
    to each dimension is displayed along with the score. We manually inspect the top
    style attributes and annotate them as reasonable, plausible, or incorrect. Attributes
    annotated with $\ast$ blur the line between style and content. Error analysis
    can be found in Section [4.1](#S4.SS1 "4.1 Error Analysis ‣ 4 Results ‣ Learning
    Interpretable Style Embeddings via Prompting LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: Training and Inference
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Training over the ~1.3M unique style attributes in StyleGenome, our training
    dataset for Sfam is effectively a multitask mixture. Style attributes presented
    in natural language to the model allows the pre-trained T5 encoder to jointly
    learn between style attributes and generalize to unseen style attributes using
    the semantic information in those natural language descriptions. This is especially
    desirable since some style attributes only have a handful of text examples, while
    others may have thousands. This setup resembles the multitask mixture trained
    on in Raffel et al. ([2020](#bib.bib44)). To validate training, we hold-out 50
    random style attributes that have between 30-50 examples each as a validation
    set. We validate learning during training by measuring the ability of Sfam to
    generalize and produce accurate agreement scores for the unseen style attributes.
    At inference, we softmax the binary class logits to interpret them as probabilities
    and we take the probability of $y_{i}=1$ as the agreement score. We also study
    the effect of the size of StyleGenome on performance and find that as the synthetic
    dataset grows, validation performance improves and Sfam generalizes to better
    predict agreement scores for unseen style attribute and text pairs (see Appendix
    [B](#A2 "Appendix B Effect of StyleGenome Dataset Size ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs")).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Lisa Style Vectors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed earlier, Sfam is directly used to produce the Lisa interpretable
    style vectors. We arbitrarily choose $D=768$ characters, are not pure ASCII, contain
    “not” or “avoids” (negative statements), or contain quotes or the word “mentions”
    (these attributes tend to be more relevant to content than style). Examples of
    Lisa can be found in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") and Table [3](#S3.T3 "Table 3 ‣ Sampling
    Batches ‣ 3.1 Sfam ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting
    LLMs"). With 768 dimensions, producing a single Lisa vector would require 768
    inferences of Sfam, a computationally expensive operation. To address this, we
    produce the Lisa representations for 1,000,000 random Reddit posts from MUD. We
    then distill into a new EncT5 model with 768 regression labels. We hold-out 10,000
    examples as a validation set. After distillation to the dedicated model, the 768-dimensional
    style vector can be produced in a single forward pass with minimal degradation
    (validation MSE = 0.005).
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Formal | Complex | Numb3r | C’tion | Avg | Interpretable |'
  prefs: []
  type: TYPE_TB
- en: '| Random Baseline | 0.50/0.50 | 0.50/0.50 | 0.50/0.50 | 0.50/0.50 | 0.50/0.50
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Content-Aware Representations |'
  prefs: []
  type: TYPE_TB
- en: '| SBERT | 0.78/0.00 | 0.54/0.01 | 0.81/0.04 | 0.86/0.00 | 0.75/0.01 | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| LUAR | 0.80/0.14 | 0.67/0.00 | 0.74/0.03 | 0.77/0.00 | 0.75/0.04 | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Content-Independent Style Representations |'
  prefs: []
  type: TYPE_TB
- en: '| LIWC | 0.52/  - | 0.52/  - | 0.50/  - | 0.99/  - | 0.63/  - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Wegmann et al. ([2022](#bib.bib61)) | 0.84/0.69 | 0.59/0.26 | 0.56/0.03 |
    0.96/0.02 | 0.74/0.25 | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Lisa | 0.69/0.07 | 0.57/0.01 | 0.80/0.03 | 0.77/0.00 | 0.71/0.03 | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Lisa (Wegmann + $w$) | 0.72/0.07 | 0.61/0.03 | 0.81/0.08 | 0.68/0.00 | 0.71/0.05
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Lisa (Wegmann + $W$) | 0.66/0.03 | 0.56/0.01 | 0.70/0.01 | 0.87/0.00 | 0.70/0.01
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Lisa (LUAR + $w$) | 0.73/0.05 | 0.65/0.00 | 0.85/0.03 | 0.92/0.00 | 0.79/0.02
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Lisa (LUAR + $W$) | 0.81/0.07 | 0.56/0.01 | 0.74/0.03 | 0.82/0.00 | 0.73/0.03
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Accuracy scores on STEL/STEL-or-Content, an evaluation framework for
    style measures proposed by Wegmann and Nguyen ([2021](#bib.bib60)) and Wegmann
    et al. ([2022](#bib.bib61)). “LIWC” results are from Wegmann and Nguyen ([2021](#bib.bib60)).
    “Lisa” is the 768-dimensional style vector. “Lisa (…)” uses Lisa embeddings with
    the training dataset and embedding layer type denoted in (…). Gray indicates worse
    than random baseline performance on the adversarially challenging STEL-or-Content
    task. All approaches underperform on STEL-or-Content, but Lisa approaches outperform
    or closely match existing style representation choices on STEL, while providing
    interpretability.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Lisa Style Embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We experiment with two different simple and interpretable embedding layers,
    a weight vector ($w_{768}$). We attach these on top of the Lisa model and train
    just the layer using a contrastive learning objective and triplet loss (Khosla
    et al., [2020](#bib.bib28); Schroff et al., [2015](#bib.bib52)). We also experiment
    with two different authorship datasets from prior works to train the embedding
    layer; we refer to these datasets as the Wegmann dataset (Wegmann et al., [2022](#bib.bib61))
    and the LUAR dataset (Rivera-Soto et al., [2021](#bib.bib48)). Like the prior
    work, we assume an author has consistent style between their different texts.
    Given some anchor text by an author, we use another text by the same author as
    a positive example, and text by a different author as a negative example for our
    triplets. This objective minimizes the distance between two texts by the same
    author and maximizes the distance between texts by different authors, learning
    a meaningful metric.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first evaluate to what degree Sfam, which is ultimately used to build Lisa
    representations, learns useful stylometric annotation capabilities that align
    with human reviewers. We then evaluate Lisa itself on STEL, a framework purpose-built
    for evaluating the quality of style measures (Wegmann and Nguyen, [2021](#bib.bib60)).
    All evaluations were completed after the collection of the StyleGenome dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Correlation to Human Judgments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We conduct a broad set of 55 studies across 21 datasets in 7 distinct categories
    of linguistic style and authorship dimensions in Table [2](#S3.T2 "Table 2 ‣ 3
    Method ‣ Learning Interpretable Style Embeddings via Prompting LLMs"). We measure
    the correlation of Sfam’s agreement scores to human judgments. Sfam performs stronger
    on dimensions like formality, sentiment, and emotion than dimensions like linguistic
    acceptability. This is likely an artifact of the effectiveness of GPT-3 in annotating
    these categories, an expected result given prior work has shown language models
    struggle with identifying these features (Warstadt et al., [2020](#bib.bib58)).
    Interestingly, Sfam demonstrates some limited ability to perform authorship profiling,
    a task adjacent to stylometry. The ability to probe Sfam in an interpretable manner
    helps identify which categories of features it can reliably represent, whereas
    prior approaches were more opaque. Overall, the Table [2](#S3.T2 "Table 2 ‣ 3
    Method ‣ Learning Interpretable Style Embeddings via Prompting LLMs") results
    demonstrate Sfam’s annotations do correlate with human judgments on some important
    dimensions of style. We hypothesize future research with larger datasets (> 10,000
    posts), more diverse sources of texts, and larger and more performant LLMs may
    further broaden and improve learned stylometric annotation capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: '| Text | Style Attribute |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Subscribed. Interesting idea. I would like to see some advanced stats on
    hitting percentages to different locations on the court. For example, having the
    court broken up into maybe 12 zones and then hitting percentages from each position
    to those zones. I remember seeing an article that did this years ago and I have
    never been able to find anything online. I said recently on this sub that the
    deep angle shot from the left side or right side was the highest percentage shot
    in volleyball, but I was not able to back up my claim with any sources or anything.
    Anyways, I am a VB nerd, no doubt. Interested to see what direction you take this.
    Cheers! | The author is being polite. |'
  prefs: []
  type: TYPE_TB
- en: '| Yeah I also work in QA, and seeing this kind of stuff get released is maddening.
    About a year ago working on a new platform we were seeing bugs in the hundreds
    each week, we pushed back the release of the product 3 months because basically
    it didn’t work. If it was up to the devs, they’d have released it on time, because
    the stuff they’d written code for worked. Thorough doesn’t even cover the work
    we go through every 3 months, and Niantic’s approach seems completely amateur
    from this side. They’re putting bandaids on problems and hiding things like the
    3 step problem behind curtains without seemingly fixing anything, although I do
    have to say their balance tweaks to battling have been a big step in the right
    direction. | The author is using a personal anecdote to illustrate their point.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Thank you. I’d be interested in reading more about your experiences, in addition
    to the "American Wedding" story. Are you watching the stream? I wish there was
    a way to find out how many people in the world are watching it. The music is lovely,
    huh? God damn. He’s got his bunny Fair Isle sweater on, drinking Dunkin’ Donuts
    coffee. I would have thought him a Starbucks man. :-) | The author is using an
    emoji. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Sentence-level Lisa vectors over each sentence from a longer passage
    of text can help identify and quantify which sentences contribute to overall style
    attributes scored on the longer passage providing granular interpretability.'
  prefs: []
  type: TYPE_NORMAL
- en: STEL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Table [4](#S3.T4 "Table 4 ‣ 3.2 Lisa Style Vectors ‣ 3 Method ‣ Learning
    Interpretable Style Embeddings via Prompting LLMs"), we provide the results of
    evaluating Lisa using STEL. The STEL task evaluates whether two texts with similar
    styles can be matched using the distance/similarity metric defined by a style
    representation. We compare with other content-independent style representations,
    or methods that explicitly limit representation of content in favor of style.
    Lisa explicitly limits the representation of content through the 768 style-focused
    attributes that act as a bottleneck. Content-aware representations like SBERT,
    on the other hand, have direct access to the text and may be able to represent
    the content in the text to an extreme degree, representing the usage of a specific
    rare word or discussion of a specific concept. We provide the results of content-aware
    representations simply for reference. We find Lisa embeddings are able to closely
    match (and on average slightly outperform) prior style representations on STEL
    while providing interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: '| Texts | Top 3 Common/Distinct Style Attributes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Anchor: Devices that use two pronged instead of three pronged plugs are required
    to meet certain safe design requirements. Among other things, if a device has
    a switch, the switched line MUST BE hot, not neutral. The polarized plugs make
    sure that the right prong/wire is hot. This is why devices that have no switches
    (primarily wall warts) need not have polarized plugs. |  |'
  prefs: []
  type: TYPE_TB
- en: '| Same Author: Your diaphragm would be trying to contract against the air pressure
    in your lungs. That’s why deep sea diving requires regulators, to match the pressure
    of the air supply to the pressure surrounding your rib cage. You can breathe against
    a maximum of about 1/2 PSI, which is not enough pressure to adequately oxygenate
    your blood. | (0.89,  1.00)  –  The author is using a scientific approach. (0.96,
    0.98) – The author is using a combination of                     technical terms
    and everyday language.'
  prefs: []
  type: TYPE_NORMAL
- en: (0.91, 0.84)   –  The author is using formal and
  prefs: []
  type: TYPE_NORMAL
- en: professional language. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Different Author: That’s great! I’m glad it seems to be finding its’ niche.
    Now if they could just make a Star Wars version of this game, I’d happily swallow
    that fat learning curve and overcome my frustrations with the combat system. ;)
    | (0.06, 0.99)  – The author is using words related to the                     game
    they are discussing. $\ast$ (0.00, 0.88)   –  The author is using an emoji.'
  prefs: []
  type: TYPE_NORMAL
- en: (0.02, 0.87)   –  The author uses an emoticon at the end. |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Example interpretable explanations on the Contrastive Authorship Verification
    task. The top style attributes in common between the Anchor text and a text by
    the Same Author are shown. The top distinct style attributes between the Anchor
    text and a text by a Different Author are also shown. The scores of each style
    attribute against the texts is shown in (•,  •/•). Attributes annotated with $\ast$
    blur the line between style and content. Error analysis can be found in Section
    [4.1](#S4.SS1 "4.1 Error Analysis ‣ 4 Results ‣ Learning Interpretable Style Embeddings
    via Prompting LLMs"). Further examples and details on style attribute ranking
    can be found in Appendix [G](#A7 "Appendix G Interpretable Authorship Verification
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence-Level Interpretability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Table [5](#S4.T5 "Table 5 ‣ Correlation to Human Judgments ‣ 4 Results ‣
    Learning Interpretable Style Embeddings via Prompting LLMs"), we demonstrate how
    visualizing a dimension of sentence-level Lisa vectors can help explain which
    sentences contribute to a dimension activated on a passage-level Lisa vector.
  prefs: []
  type: TYPE_NORMAL
- en: Forensic Interpretability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Typically for authorship attribution tasks, content-aware representations that
    capture both content and style are used to make a determination. Author style,
    however, is still an important component in determining attribution (Rivera-Soto
    et al., [2021](#bib.bib48)). Offering a clear explanation and presenting supporting
    evidence is crucial, particularly in the context of forensic analysis, such as
    when presenting evidence in a court trial. Explainability has often been overlooked
    in neural approaches to authorship attribution tasks. To motivate this as a future
    research direction using our interpretable stylometric representations and our
    general approach, we provide an example of explanations on the Contrastive Authorship
    Verification task from Wegmann et al. ([2022](#bib.bib61)) in Table [6](#S4.T6
    "Table 6 ‣ STEL ‣ 4 Results ‣ Learning Interpretable Style Embeddings via Prompting
    LLMs") with Lisa (LUAR + $W$). Further examples and discussion on how the top
    common and distinct style attributes are ranked can be found in Appendix [G](#A7
    "Appendix G Interpretable Authorship Verification ‣ Learning Interpretable Style
    Embeddings via Prompting LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Error Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We highlight insights and observations around common failure modes of our technique
    in this section. We annotate the common failure modes with their percentage rate
    of occurrence⁷⁷7We manually inspect a small sample set of 2,000 style attribute
    annotations (the top 20 style attributes for 100 random texts) by Lisa..
  prefs: []
  type: TYPE_NORMAL
- en: Content vs. Style Attributes (3%)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It is unclear whether style and content can truly be separated as some content
    features are important for style or profiling an author (Jafaritazehjani et al.,
    [2020](#bib.bib24); Bischoff et al., [2020](#bib.bib6); Patel et al., [2022](#bib.bib39)).
    Even after filtering, 3% of dimensions of Lisa still represent content. For example,
    “The author is using words related to the game they are discussing”. However,
    while Lisa may have the ability to represent that two texts are both discussing
    the topic of video games, it does not have the direct ability a content-aware
    approach would of representing which specific video game is being discussed, due
    to the limited set of 768 features that act as a bottleneck. Our approach also
    allows visibility into understanding how much of the representation derives from
    content-related features, while other neural representations are opaque and may
    use content-related features in a way that cannot be easily assessed.
  prefs: []
  type: TYPE_NORMAL
- en: Conflating Style Attributes with Content (2%)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For some style attributes, Lisa conflates the content of text with the presence
    of the style attribute. For example, “The author is cautious”, may have a high
    agreement score on any text containing the word “caution” even if the author is
    not actually expressing caution in the text.
  prefs: []
  type: TYPE_NORMAL
- en: Spurious Correlations (6%)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For other style attributes, Lisa has learned spurious correlations. For example,
    “The author uses two exclamation marks”, often has a high agreement score on any
    text that is exclamatory in nature, but does not actually use exclamation marks.
    An example can be found in Table [3](#S3.T3 "Table 3 ‣ Sampling Batches ‣ 3.1
    Sfam ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: Fundamental Errors (10%)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lisa sometimes produces a high agreement score for text displaying the polar
    opposite of a style attribute or produces a high agreement score for an attribute
    that simply is not present in the text. Table [3](#S3.T3 "Table 3 ‣ Sampling Batches
    ‣ 3.1 Sfam ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting
    LLMs") demonstrates some of these incorrect examples. Inspecting our dataset,
    this error happens both due to EncT5’s internal representations likely aligning
    on relatedness instead of similarity (Hill et al., [2015](#bib.bib20)) and due
    to hallucination and annotation errors by GPT-3\. Hallucinated generations is
    a common issue with any LLM-guided approach and we discuss it further in [Limitations](#Sx1
    "Limitations and Broader Impacts ‣ Learning Interpretable Style Embeddings via
    Prompting LLMs") along with potential future mitigations.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose a promising novel approach to learning interpretable
    style representations. To overcome a lack of stylometrically annotated training
    data, we use a LLM to generate StyleGenome, a synthetic stylometry dataset. Our
    approach distills the stylometric knowledge from StyleGenome into two models,
    Sfam and Lisa. We find that these models learn style representations that match
    the performance of recent direct neural approaches and introduce interpretability
    grounded in explanations that correlate with human judgments. Our approach builds
    towards a research direction focused on making style representations more useful
    for downstream applications where such properties are desirable such as in a forensic
    analysis context. Future directions that introduce human-in-the-loop supervised
    annotations or newer, larger, and better aligned LLMs for annotation have the
    potential to yield further gains in both performance and interpretability.
  prefs: []
  type: TYPE_NORMAL
- en: Model and Data Release
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We release our dataset (StyleGenome) and our two models (Sfam and Lisa) to further
    research in author style.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and Broader Impacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Limitations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Handcrafted features by forensic linguists typically rely on frequency counts
    of word usage, usage of unique words or phrases, etc. (Mosteller and Wallace,
    [1963](#bib.bib34)). The space of these kinds of features is non-enumerable and
    would not be well-represented with our technique that scores a fixed set of 768
    interpretable features. Pure neural approaches may capture these kinds of features,
    but are non-interpretable and may capture undesirable content-related features.
    We explicitly trade-off the use of these kinds of features in this work to achieve
    interpretability. While we demonstrate our synthetic annotations are enough for
    a model to learn to identify stylistic properties in text in Table [2](#S3.T2
    "Table 2 ‣ 3 Method ‣ Learning Interpretable Style Embeddings via Prompting LLMs"),
    they cannot be fully relied on yet for the reasons we discuss in Section [4.1](#S4.SS1
    "4.1 Error Analysis ‣ 4 Results ‣ Learning Interpretable Style Embeddings via
    Prompting LLMs"). As large language models scale and improve, however, we believe
    this work could benefit from increasing coherency and decreasing hallucination
    in the annotations (Kaplan et al., [2020](#bib.bib26)). StyleGenome is collected
    only on 10,000 English Reddit posts, however, larger datasets may improve performance
    as we show in Figure [2](#A2.F2 "Figure 2 ‣ Appendix B Effect of StyleGenome Dataset
    Size ‣ Learning Interpretable Style Embeddings via Prompting LLMs") and future
    research in multilingual LLMs may make it feasible to replicate this procedure
    for other languages.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical considerations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Style representations are useful for text style transfer (Riley et al., [2021](#bib.bib47))
    and in manipulating the output of machine generated text to match a user’s style,
    for example, in machine translation (Niu et al., [2017](#bib.bib36); Rabinovich
    et al., [2017](#bib.bib43)). While style transfer can be a useful benign commercial
    application of this work, superior style representations may aid the impersonation
    of authors. We demonstrate how style representations may aid legitimate cases
    of authorship attribution, a task that is typically done by forensic linguist
    experts. Our work introduces an interpretable approach, an important step in legitimizing
    the use of computational models for authorship attribution by providing explanations
    for predictions that can be audited and verified.
  prefs: []
  type: TYPE_NORMAL
- en: Diversity and inclusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We believe style representations that capture wider dimensions of style can
    help aid in analyzing and representing minority writing styles in downstream applications
    like style transfer.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research is based upon work supported in part by the DARPA KAIROS Program
    (contract FA8750-19-2-1004), the DARPA LwLL Program (contract FA8750-19-2-0201),
    the IARPA HIATUS Program (contract 2022-22072200005), and the NSF (Award 1928631).
    Approved for Public Release, Distribution Unlimited. The views and conclusions
    contained herein are those of the authors and should not be interpreted as necessarily
    representing the official policies, either expressed or implied, of DARPA, IARPA,
    NSF, or the U.S. Government.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Alva-Manchego et al. (2020) Fernando Alva-Manchego, Louis Martin, Antoine Bordes,
    Carolina Scarton, Benoît Sagot, and Lucia Specia. 2020. [ASSET: A dataset for
    tuning and evaluation of sentence simplification models with multiple rewriting
    transformations](https://doi.org/10.18653/v1/2020.acl-main.424). In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 4668–4679, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon.com (2018) Inc. Amazon.com. 2018. Amazon Customer Reviews Dataset — s3.amazonaws.com.
    [https://s3.amazonaws.com/amazon-reviews-pds/readme.html](https://s3.amazonaws.com/amazon-reviews-pds/readme.html).
    [Accessed 17-May-2023].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Andrews and Bishop (2019) Nicholas Andrews and Marcus Bishop. 2019. Learning
    invariant representations of social media users. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 1684–1695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Atwell et al. (2022) Katherine Atwell, Sabit Hassan, and Malihe Alikhani. 2022.
    [APPDIA: A discourse-aware transformer-based style transfer model for offensive
    social media conversations](https://aclanthology.org/2022.coling-1.530). In *Proceedings
    of the 29th International Conference on Computational Linguistics*, pages 6063–6074,
    Gyeongju, Republic of Korea. International Committee on Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ba and Caruana (2014) Jimmy Ba and Rich Caruana. 2014. Do deep nets really need
    to be deep? *Advances in neural information processing systems*, 27.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bischoff et al. (2020) Sebastian Bischoff, Niklas Deckers, Marcel Schliebs,
    Ben Thies, Matthias Hagen, Efstathios Stamatatos, Benno Stein, and Martin Potthast.
    2020. The importance of suppressing domain style in authorship analysis. *arXiv
    preprint arXiv:2005.14714*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boenninghoff et al. (2019) Benedikt Boenninghoff, Robert M Nickel, Steffen Zeiler,
    and Dorothea Kolossa. 2019. Similarity learning for authorship verification in
    social media. In *ICASSP 2019-2019 IEEE International Conference on Acoustics,
    Speech and Signal Processing (ICASSP)*, pages 2457–2461\. IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 1877–1901\.
    Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Coster and Kauchak (2011) William Coster and David Kauchak. 2011. [Simple English
    Wikipedia: A new text simplification task](https://aclanthology.org/P11-2117).
    In *Proceedings of the 49th Annual Meeting of the Association for Computational
    Linguistics: Human Language Technologies*, pages 665–669, Portland, Oregon, USA.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CrowdFlower (2017) CrowdFlower. 2017. Twitter User Gender Classification — kaggle.com.
    [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification).
    [Accessed 17-May-2023].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2019) Ning Dai, Jianze Liang, Xipeng Qiu, and Xuan-Jing Huang.
    2019. Style transformer: Unpaired text style transfer without disentangled latent
    representation. In *Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics*, pages 5997–6007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: de Gibert et al. (2018) Ona de Gibert, Naiara Perez, Aitor García-Pablos, and
    Montse Cuadros. 2018. [Hate Speech Dataset from a White Supremacy Forum](https://doi.org/10.18653/v1/W18-5102).
    In *Proceedings of the 2nd Workshop on Abusive Language Online (ALW2)*, pages
    11–20, Brussels, Belgium. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Demszky et al. (2020) Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko,
    Alan Cowen, Gaurav Nemade, and Sujith Ravi. 2020. [GoEmotions: A dataset of fine-grained
    emotions](https://doi.org/10.18653/v1/2020.acl-main.372). In *Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics*, pages 4040–4054,
    Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*, pages 4171–4186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. (2018) Zhenxin Fu, Xiaoye Tan, Nanyun Peng, Dongyan Zhao, and Rui
    Yan. 2018. Style transfer in text: Exploration and evaluation. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gilardi et al. (2023) Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023.
    Chatgpt outperforms crowd-workers for text-annotation tasks. *arXiv preprint arXiv:2303.15056*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grano et al. (2017) Giovanni Grano, Andrea Di Sorbo, Francesco Mercaldo, Corrado A.
    Visaggio, Gerardo Canfora, and Sebastiano Panichella. 2017. [Android apps and
    user feedback: A dataset for software evolution and quality improvement](https://doi.org/10.1145/3121264.3121266).
    In *Proceedings of the 2nd ACM SIGSOFT International Workshop on App Market Analytics*,
    WAMA 2017, page 8–11, New York, NY, USA. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Groenwold et al. (2020) Sophie Groenwold, Lily Ou, Aesha Parekh, Samhita Honnavalli,
    Sharon Levy, Diba Mirza, and William Yang Wang. 2020. [Investigating African-American
    Vernacular English in transformer-based text generation](https://doi.org/10.18653/v1/2020.emnlp-main.473).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 5877–5883, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hay et al. (2020) Julien Hay, Bich-Lien Doan, Fabrice Popineau, and Ouassim Ait
    Elhara. 2020. Representation learning of writing style. In *Proceedings of the
    Sixth Workshop on Noisy User-generated Text (W-NUT 2020)*, pages 232–243.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hill et al. (2015) Felix Hill, Roi Reichart, and Anna Korhonen. 2015. [SimLex-999:
    Evaluating semantic models with (genuine) similarity estimation](https://doi.org/10.1162/COLI_a_00237).
    *Computational Linguistics*, 41(4):665–695.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Holmes (1994) David I. Holmes. 1994. [Authorship attribution](http://www.jstor.org/stable/30200315).
    *Computers and the Humanities*, 28(2):87–106.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Honovich et al. (2022) Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
    2022. Unnatural instructions: Tuning language models with (almost) no human labor.
    *arXiv preprint arXiv:2212.09689*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve.
    *arXiv preprint arXiv:2210.11610*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jafaritazehjani et al. (2020) Somayeh Jafaritazehjani, Gwénolé Lecorvé, Damien
    Lolive, and John Kelleher. 2020. Style versus content: A distinction without a
    (learnable) difference? In *International Conference on Computational Linguistics*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: John et al. (2019) Vineet John, Lili Mou, Hareesh Bahuleyan, and Olga Vechtomova.
    2019. Disentangled representation learning for non-parallel text style transfer.
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 424–434.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan et al. (2020) Jared Kaplan, Sam McCandlish, T. J. Henighan, Tom B. Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeff Wu, and Dario Amodei.
    2020. Scaling laws for neural language models. *ArXiv*, abs/2001.08361.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khan et al. (2021) Aleem Khan, Elizabeth Fleming, Noah Schofield, Marcus Bishop,
    and Nicholas Andrews. 2021. A deep metric learning approach to account linking.
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 5275–5287.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khosla et al. (2020) Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,
    Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. 2020.
    Supervised contrastive learning. *Advances in neural information processing systems*,
    33:18661–18673.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koppel et al. (2009) Moshe Koppel, Jonathan Schler, and Shlomo Argamon. 2009.
    [Computational methods in authorship attribution](https://doi.org/https://doi.org/10.1002/asi.20961).
    *Journal of the American Society for Information Science and Technology*, 60(1):9–26.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Dianqi Li, Yizhe Zhang, Zhe Gan, Yu Cheng, Chris Brockett,
    William B Dolan, and Ming-Ting Sun. 2019. Domain adaptive text style transfer.
    In *Proceedings of the 2019 Conference on Empirical Methods in Natural Language
    Processing and the 9th International Joint Conference on Natural Language Processing
    (EMNLP-IJCNLP)*, pages 3304–3313.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021) Frederick Liu, Siamak Shakeri, Hongkun Yu, and Jing Li. 2021.
    Enct5: Fine-tuning t5 encoder for non-autoregressive tasks. *arXiv preprint arXiv:2110.08426*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019.
    Roberta: A robustly optimized bert pretraining approach. *arXiv preprint arXiv:1907.11692*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Maas et al. (2011) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang,
    Andrew Y. Ng, and Christopher Potts. 2011. [Learning word vectors for sentiment
    analysis](http://www.aclweb.org/anthology/P11-1015). In *Proceedings of the 49th
    Annual Meeting of the Association for Computational Linguistics: Human Language
    Technologies*, pages 142–150, Portland, Oregon, USA. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mosteller and Wallace (1963) Frederick Mosteller and David L. Wallace. 1963.
    [Inference in an authorship problem](http://www.jstor.org/stable/2283270). *Journal
    of the American Statistical Association*, 58(302):275–309.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Naji (2012) Ibrahim Naji. 2012. TSATC: Twitter Sentiment Analysis Training
    Corpus. In *thinknook*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niu et al. (2017) Xing Niu, Marianna Martindale, and Marine Carpuat. 2017.
    A study of style in machine translation: Controlling the formality of machine
    translation output. In *Proceedings of the 2017 Conference on Empirical Methods
    in Natural Language Processing*, pages 2814–2819.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *arXiv preprint arXiv:2203.02155*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pang and Lee (2005) Bo Pang and Lillian Lee. 2005. [Seeing stars: Exploiting
    class relationships for sentiment categorization with respect to rating scales](https://doi.org/10.3115/1219840.1219855).
    In *Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics*,
    ACL ’05, page 115–124, USA. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patel et al. (2022) Ajay Patel, Nicholas Andrews, and Chris Callison-Burch.
    2022. Low-resource authorship style transfer with in-context learning. *arXiv
    preprint arXiv:2212.08986*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pavlick and Tetreault (2016) Ellie Pavlick and Joel Tetreault. 2016. [An empirical
    analysis of formality in online communication](https://doi.org/10.1162/tacl_a_00083).
    *Transactions of the Association for Computational Linguistics*, 4:61–74.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prabhumoye et al. (2018) Shrimai Prabhumoye, Yulia Tsvetkov, Ruslan Salakhutdinov,
    and Alan W Black. 2018. [Style transfer through back-translation](https://doi.org/10.18653/v1/P18-1080).
    In *Proceedings of the 56th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 866–876, Melbourne, Australia. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pryzant et al. (2020) Reid Pryzant, Richard Diehl Martinez, Nathan Dass, Sadao
    Kurohashi, Dan Jurafsky, and Diyi Yang. 2020. [Automatically neutralizing subjective
    bias in text](https://doi.org/10.1609/aaai.v34i01.5385). *Proceedings of the AAAI
    Conference on Artificial Intelligence*, 34(01):480–489.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rabinovich et al. (2017) Ella Rabinovich, Raj Nath Patel, Shachar Mirkin, Lucia
    Specia, and Shuly Wintner. 2017. Personalized machine translation: Preserving
    original author traits. In *Proceedings of the 15th Conference of the European
    Chapter of the Association for Computational Linguistics: Volume 1, Long Papers*,
    pages 1074–1084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring
    the limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21:1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rao and Tetreault (2018) Sudha Rao and Joel Tetreault. 2018. [Dear sir or madam,
    may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality
    style transfer](https://doi.org/10.18653/v1/N18-1012). In *Proceedings of the
    2018 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 129–140,
    New Orleans, Louisiana. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reimers and Gurevych (2019) Nils Reimers and Iryna Gurevych. 2019. Sentence-bert:
    Sentence embeddings using siamese bert-networks. In *Proceedings of the 2019 Conference
    on Empirical Methods in Natural Language Processing and the 9th International
    Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*, pages 3982–3992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Riley et al. (2021) Parker Riley, Noah Constant, Mandy Guo, Girish Kumar, David C
    Uthus, and Zarana Parekh. 2021. Textsettr: Few-shot text style extraction and
    tunable targeted restyling. In *Proceedings of the 59th Annual Meeting of the
    Association for Computational Linguistics and the 11th International Joint Conference
    on Natural Language Processing (Volume 1: Long Papers)*, pages 3786–3800.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rivera-Soto et al. (2021) Rafael A Rivera-Soto, Olivia Elizabeth Miano, Juanita
    Ordonez, Barry Y Chen, Aleem Khan, Marcus Bishop, and Nicholas Andrews. 2021.
    Learning universal authorship representations. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pages 913–919.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosso et al. (2016) Paolo Rosso, Francisco Rangel, Martin Potthast, Efstathios
    Stamatatos, Michael Tschuggnall, and Benno Stein. 2016. Overview of pan’16: new
    challenges for authorship analysis: cross-genre profiling, clustering, diarization,
    and obfuscation. In *Experimental IR Meets Multilinguality, Multimodality, and
    Interaction: 7th International Conference of the CLEF Association, CLEF 2016,
    Évora, Portugal, September 5-8, 2016, Proceedings 7*, pages 332–350\. Springer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. 2019. Distilbert, a distilled version of bert: smaller, faster, cheaper
    and lighter. *ArXiv*, abs/1910.01108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saravia et al. (2018) Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin
    Wu, and Yi-Shin Chen. 2018. [CARER: Contextualized affect representations for
    emotion recognition](https://doi.org/10.18653/v1/D18-1404). In *Proceedings of
    the 2018 Conference on Empirical Methods in Natural Language Processing*, pages
    3687–3697, Brussels, Belgium. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schroff et al. (2015) Florian Schroff, Dmitry Kalenichenko, and James Philbin.
    2015. Facenet: A unified embedding for face recognition and clustering. In *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, pages 815–823.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shen et al. (2017) Tianxiao Shen, Tao Lei, Regina Barzilay, and Tommi Jaakkola.
    2017. Style transfer from non-parallel text by cross-alignment. *Advances in neural
    information processing systems*, 30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stamatatos (2009) Efstathios Stamatatos. 2009. [A survey of modern authorship
    attribution methods](https://doi.org/https://doi.org/10.1002/asi.21001). *Journal
    of the American Society for Information Science and Technology*, 60(3):538–556.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tausczik and Pennebaker (2010) Yla R Tausczik and James W Pennebaker. 2010.
    The psychological meaning of words: Liwc and computerized text analysis methods.
    *Journal of language and social psychology*, 29(1):24–54.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tiersma and Solan (2002) Peter Tiersma and Lawrence M. Solan. 2002. [The linguist
    on the witness stand: Forensic linguistics in american courts](http://www.jstor.org/stable/3086556).
    *Language*, 78(2):221–239.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning
    language model with self generated instructions. *arXiv preprint arXiv:2212.10560*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Warstadt et al. (2020) Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey,
    Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. [BLiMP: The benchmark of
    linguistic minimal pairs for English](https://doi.org/10.1162/tacl_a_00321). *Transactions
    of the Association for Computational Linguistics*, 8:377–392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warstadt et al. (2019) Alex Warstadt, Amanpreet Singh, and Samuel R. Bowman.
    2019. [Neural network acceptability judgments](https://doi.org/10.1162/tacl_a_00290).
    *Transactions of the Association for Computational Linguistics*, 7:625–641.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wegmann and Nguyen (2021) Anna Wegmann and Dong Nguyen. 2021. Does it capture
    stel? a modular, similarity-based linguistic style evaluation framework. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    7109–7130.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wegmann et al. (2022) Anna Wegmann, Marijn Schraagen, Dong Nguyen, et al. 2022.
    Same author or just same topic? towards content-independent style representations.
    In *Proceedings of the 7th Workshop on Representation Learning for NLP*, page
    249\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    and Jamie Brew. 2019. [Huggingface’s transformers: State-of-the-art natural language
    processing](http://arxiv.org/abs/1910.03771). *CoRR*, abs/1910.03771.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu (2017) Wei Xu. 2017. [From shakespeare to Twitter: What are language styles
    all about?](https://doi.org/10.18653/v1/W17-4901) In *Proceedings of the Workshop
    on Stylistic Variation*, pages 1–9, Copenhagen, Denmark. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2021) Xiaoyuan Yi, Zhenghao Liu, Wenhao Li, and Maosong Sun. 2021.
    Text style transfer via learning style instance supported latent space. In *Proceedings
    of the Twenty-Ninth International Conference on International Joint Conferences
    on Artificial Intelligence*, pages 3801–3807.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2015) Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. [Character-level
    convolutional networks for text classification](https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 28\. Curran Associates,
    Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu and Jurgens (2021) Jian Zhu and David Jurgens. 2021. Idiosyncratic but
    not arbitrary: Learning idiolects in online registers reveals distinctive yet
    consistent individual styles. In *Proceedings of the 2021 Conference on Empirical
    Methods in Natural Language Processing*, pages 279–297.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2022) Kangchen Zhu, Zhiliang Tian, Ruifeng Luo, and Xiaoguang Mao.
    2022. Styleflow: Disentangle latent representations via normalizing flow for unsupervised
    text style transfer. *arXiv preprint arXiv:2212.09670*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Prompt Templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Open-ended Prompt Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grammar Style: Write a long paragraph describing the unique grammar style of
    the following passage without referring to specifics about the topic.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Passage: {{passage}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Description:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vocabulary Style: Write a long paragraph describing the unique vocabulary style
    of the following passage without referring to specifics about the topic.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Passage: {{passage}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Description:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Punctuation Style: Write a long paragraph describing the unique punctuation
    style of the following passage without referring to specifics about the topic.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Passage: {{passage}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Description:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grammar Errors: Write a long paragraph describing the grammar errors (if any)
    of the following passage without referring to specifics about the topic.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Passage: {{passage}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Description:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spelling Errors: Write a long paragraph describing the spelling errors (if
    any) of the following passage without referring to specifics about the topic.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Passage: {{passage}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Description:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Forensic Linguist: Write a long paragraph describing the unique stylometric
    features of the following passage without referring to specifics about the topic
    from the perspective of a forensic linguist psychoanalyzing the writer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Passage: {{passage}}'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Description:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A.2 Targeted Prompt Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use the following template:'
  prefs: []
  type: TYPE_NORMAL
- en: '{{target_feature_definition}}'
  prefs: []
  type: TYPE_NORMAL
- en: Write a description of whether the author of the following passage has any {{target_feature}}?
  prefs: []
  type: TYPE_NORMAL
- en: 'Passage: {{passage}}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Description:'
  prefs: []
  type: TYPE_NORMAL
- en: 'for targeted prompts, substituting {{target_feature}} with each of the following
    targeted features:'
  prefs: []
  type: TYPE_NORMAL
- en: '| • figurative language • sarcasm • sentence fragment • run on sentences •
    an active voice • a passive voice • agreement errors • male pronouns • female
    pronouns • prosocial behaviors • antisocial behaviors • being polite • showing
    interpersonal conflict • moralizing • communication words • indicators of power
    • talk of achievement • indication of certitude • being tentative • insight •
    all or none thinking • words related to memory • positive emotion • negative emotion
    • anxiety • anger • sadness • swear words • positive tone | • negative tone •
    neutral tone • words related to auditory perception • words related to visual
    perception • words related to space perception • words related to motion perception
    • words related to attention • words related to allure • words related to curiosity
    • words related to risk • words related to reward • words expressing needs • words
    expressing wants • words expressing acquisition • words expressing lack • words
    expressing fulfillment • words expressing fatigue • words expressing illness •
    words expressing wellness • words related to mental health • words related to
    food or eating • words related to death • words related to self-harm • sexual
    content • words related to leisure • words related to home • words related to
    work • words related to money • words related to religion | • words related to
    politics • words related to culture • swear words • foreign words • scholarly
    words • slang words • social media slang words • filler words • words focusing
    on the past • words focusing on the present • words focusing on the future • words
    related to time • misspelled words • repeated words • words expressing quantity
    • words indicating family • words indicating friends • words indicating men •
    words indicating women • words indicating pets • words indicating social status
    • words indicating poverty • words indicating wealth • punctuation symbols • hyphenated
    words • oxford comma • parentheticals • numbers • elongated words |'
  prefs: []
  type: TYPE_TB
- en: To give GPT-3 more context, we also substitute {{target_feature_definition}}
    with a definition of the target feature, also generated by GPT-3\. The full set
    of targeted prompts can be found in the released source package for this paper.
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Standardization Prompt Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The descriptions of style generated from the prompts in Appendix [A.1](#A1.SS1
    "A.1 Open-ended Prompt Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs") and Appendix [A.2](#A1.SS2 "A.2 Targeted
    Prompt Templates ‣ Appendix A Prompt Templates ‣ Learning Interpretable Style
    Embeddings via Prompting LLMs") are substituted into the following standardization
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a description of an author’s writing style for a passage: {{description}}'
  prefs: []
  type: TYPE_NORMAL
- en: Rewrite this description as a long list of short sentences describing the author’s
    writing style where each sentence is in the format of "The author is X." or "The
    author uses X.".
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: which transforms the verbose descriptions into short, declarative, uniform sentences
    beginning with “The author…,” which are the final style attributes used in building
    the StyleGenome dataset that Sfam is trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Effect of StyleGenome Dataset Size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training Sfam, we experiment with artificially limiting the size of the
    synthetic dataset, by limiting the number of authors in the dataset, to determine
    the effect of dataset size on the validation performance. In Figure [2](#A2.F2
    "Figure 2 ‣ Appendix B Effect of StyleGenome Dataset Size ‣ Learning Interpretable
    Style Embeddings via Prompting LLMs"), we find that as the synthetic dataset grows,
    validation performance improves and Sfam generalizes to better predict agreement
    scores for unseen style attribute and text pairs.
  prefs: []
  type: TYPE_NORMAL
- en: $10^{1}$Number
    of AuthorsValidation F1
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Best F1 achieved by Sfam on a held-out validation set of examples
    at various dataset sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Annotation Prompts Ablation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Prompts Used to Generate StyleGenome | Validation F1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Open-ended Prompts | 0.865 |'
  prefs: []
  type: TYPE_TB
- en: '| Targeted Prompts | 0.898 |'
  prefs: []
  type: TYPE_TB
- en: '| Open-ended Prompts & Targeted Prompts | 0.920 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Best F1 achieved by Sfam on a held-out validation set of examples
    with different sets of Stage 1 prompts used to annotate Reddit posts and generate
    the synthetic training data used during distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D StyleGenome Annotation Cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our inference cost with the OpenAI API was priced at $0.02 / 1K tokens, with
    a cost of ~$8 to annotate 10 Reddit posts by a single author with all of our prompts.
    Our full dataset of 1,000 authors cost ~$8,000 to annotate.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Training Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: E.1 Sfam
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the EncT5 architecture (Liu et al., [2021](#bib.bib31)) with a binary
    classifier in Hugging Face (Wolf et al., [2019](#bib.bib62)). We randomly sample
    training batches of size 1,440 and use the AdamW optimizer with a learning rate
    of 0.001\. We employ early stopping with a threshold of 0.01 on the validation
    set F1 metric and a patience of 50 batches.
  prefs: []
  type: TYPE_NORMAL
- en: E.2 Lisa
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use the EncT5 architecture (Liu et al., [2021](#bib.bib31)) with 768 regression
    labels in Hugging Face (Wolf et al., [2019](#bib.bib62)) and use MSELoss. We randomly
    sample training batches of size 1,440 and use the AdamW optimizer with a learning
    rate of 0.001\. We employ early stopping with a threshold of 1e-6 on the validation
    set MSE metric and a patience of 20 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: E.3 Lisa Embedding Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We experiment with two types of embedding layers $w$. We also experiment with
    two training datasets, the Wegmann dataset (Wegmann et al., [2022](#bib.bib61))
    and the LUAR dataset (Rivera-Soto et al., [2021](#bib.bib48)). For LUAR, we use
    the train split with 5% held out as validation and we sample random authors as
    negative examples. For Wegmann, we use the Conversation dataset train split and
    the dev split as validation. We use a margin of 1.0, a batch size of 32, an AdamW
    optimizer with a learning rate of 0.001, and employ early stopping with a threshold
    of 0.001 on the validation set loss.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Full Sfam Evaluation Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Type | Dataset | Style Attribute | Spearman Correlation ($\rho$) |'
  prefs: []
  type: TYPE_TB
- en: '| Formality | Formality in Online Communication | The author uses informal
    language. | 0.599 |'
  prefs: []
  type: TYPE_TB
- en: '| Grammarly’s Yahoo Answers Formality Corpus | 0.200 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment | Yelp Reviews | The author uses a negative tone. | 0.788 |'
  prefs: []
  type: TYPE_TB
- en: '| IMDB Large Movie Review Dataset | 0.665 |'
  prefs: []
  type: TYPE_TB
- en: '| Amazon Customer Reviews Dataset | 0.432 |'
  prefs: []
  type: TYPE_TB
- en: '| Rotten Tomatoes Movie Review Data | 0.463 |'
  prefs: []
  type: TYPE_TB
- en: '| App Reviews | 0.350 |'
  prefs: []
  type: TYPE_TB
- en: '| Twitter Sentiment Analysis Training Corpus | 0.299 |'
  prefs: []
  type: TYPE_TB
- en: '| Emotion | DAIR.AI Emotion (Love vs. Anger) | The author is expressing {{emotion}}.
    | 0.542 |'
  prefs: []
  type: TYPE_TB
- en: '| DAIR.AI Emotion (Joy vs. Sad) | 0.531 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Love vs. Anger) | 0.769 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Joy vs. Sadness) | 0.639 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Disgust vs. Desire) | 0.630 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Disappointment vs. Admiration) | 0.571 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Pride vs. Embarassment) | 0.419 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Nervousness vs. Optimism) | 0.447 |'
  prefs: []
  type: TYPE_TB
- en: '| GoEmotions (Disapproval vs. Approval) | 0.432 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Admiration vs. Neutral) | 0.578 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Amusement vs. Neutral) | 0.508 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Anger vs. Neutral) | 0.372 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Annoyance vs. Neutral) | 0.355 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Approval vs. Neutral) | 0.257 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Caring vs. Neutral) | 0.303 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Confusion vs. Neutral) | 0.343 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Curiosity vs. Neutral) | 0.464 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Desire vs. Neutral) | 0.315 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Disappointment vs. Neutral) | 0.317 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Disapproval vs. Neutral) | 0.284 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Disgust vs. Neutral) | 0.307 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Embarrassment vs. Neutral) | 0.173 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Excitement vs. Neutral) | 0.295 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Fear vs. Neutral) | 0.309 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Gratitude vs. Neutral) | 0.626 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Grief vs. Neutral) | 0.093 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Joy vs. Neutral) | 0.390 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Love vs. Neutral) | 0.497 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Nervousness vs. Neutral) | 0.171 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Optimism vs. Neutral) | 0.407 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Pride vs. Neutral) | 0.097 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Realization vs. Neutral) | 0.121 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Relief vs. Neutral) | 0.123 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Remorse vs. Neutral) | 0.276 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Sadness vs. Neutral) | 0.413 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GoEmotions (Surprise vs. Neutral) | 0.317 |'
  prefs: []
  type: TYPE_TB
- en: '| Author Profiling | Political Slant | The author is a Democrat. | 0.005 |'
  prefs: []
  type: TYPE_TB
- en: '| Twitter User Gender Classification | The author is female. | 0.166 |'
  prefs: []
  type: TYPE_TB
- en: '| African-American Vernacular English | The author uses African-American Vernacular
    English. | 0.238 |'
  prefs: []
  type: TYPE_TB
- en: '| Shakespeare (Early Modern English) | The author uses Early Modern English.
    | 0.108 |'
  prefs: []
  type: TYPE_TB
- en: '| Wikipedia Bias | The author has a biased point of view. | 0.014 |'
  prefs: []
  type: TYPE_TB
- en: '| Harmful Speech | HateSpeech18 | The author’s writing contains hate speech.
    | 0.229 |'
  prefs: []
  type: TYPE_TB
- en: '| Offensive Social Media | The author uses offensive language. | 0.401 |'
  prefs: []
  type: TYPE_TB
- en: '| Text Simplification | Simple Wikipedia | The author uses simple language.
    | 0.043 |'
  prefs: []
  type: TYPE_TB
- en: '| ASSET | 0.050 |'
  prefs: []
  type: TYPE_TB
- en: '| Linguistic Acceptability | CoLA | The author uses incorrect grammar. | 0.078
    |'
  prefs: []
  type: TYPE_TB
- en: '| BLiMP | 0.020 |'
  prefs: []
  type: TYPE_TB
- en: '| Average |  |  | 0.342 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Correlation of agreement scores produced by Sfam against human judgments
    on texts over a wide variety linguistic and authorship dimensions. The natural
    language style attributes used as input to Sfam when producing the agreement scores
    for each dataset are also provided.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Interpretable Authorship Verification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Texts | Top 3 Common/Distinct Style Attributes |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Anchor: Devices that use two pronged instead of three pronged plugs are required
    to meet certain safe design requirements. Among other things, if a device has
    a switch, the switched line MUST BE hot, not neutral. The polarized plugs make
    sure that the right prong/wire is hot. This is why devices that have no switches
    (primarily wall warts) need not have polarized plugs. |  |'
  prefs: []
  type: TYPE_TB
- en: '| Same Author: Your diaphragm would be trying to contract against the air pressure
    in your lungs. That’s why deep sea diving requires regulators, to match the pressure
    of the air supply to the pressure surrounding your rib cage. You can breathe against
    a maximum of about 1/2 PSI, which is not enough pressure to adequately oxygenate
    your blood. | (0.89,  1.00)  –  The author is using a scientific approach. (0.96,
    0.98) – The author is using a combination of                     technical terms
    and everyday language.'
  prefs: []
  type: TYPE_NORMAL
- en: (0.91, 0.84)   –  The author is using formal and
  prefs: []
  type: TYPE_NORMAL
- en: professional language. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Different Author: That’s great! I’m glad it seems to be finding its’ niche.
    Now if they could just make a Star Wars version of this game, I’d happily swallow
    that fat learning curve and overcome my frustrations with the combat system. ;)
    | (0.06, 0.99)  – The author is using words related to the                     game
    they are discussing. $\ast$ (0.00, 0.88)   –  The author is using an emoji.'
  prefs: []
  type: TYPE_NORMAL
- en: (0.02, 0.87)   –  The author uses an emoticon at the end. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Anchor: Not sure what the income tax is in Germany, but in the Netherlands
    the income can be up 50% for the higher income classes. |  |'
  prefs: []
  type: TYPE_TB
- en: '| Same Author: The salaries in the US alway blow my mind. A software developer
    in Amsterdam gets like €40.000/year, maybe €50.000/year if your good, and maybe
    €60.000/year if you’re some kind of manager. Anything position over €100.000/year
    is basically running the entire company. | (0.84, 0.90)  –  The author is using
    words indicating                    poverty.'
  prefs: []
  type: TYPE_NORMAL
- en: (0.87, 1.00)  –  The author is using words indicating
  prefs: []
  type: TYPE_NORMAL
- en: wealth.
  prefs: []
  type: TYPE_NORMAL
- en: (0.80, 0.93)  –  The author is using words related to money. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Different Author: How would you even test this software? The setup would
    be just insane. | (0.00, 1.00) – The author is comfortable with technology. (0.00,
    0.85) – The author is discussing a product. $\ast$'
  prefs: []
  type: TYPE_NORMAL
- en: (0.13, 0.92) – The author is using formal and
  prefs: []
  type: TYPE_NORMAL
- en: professional language. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Anchor: If only there was something he could have done to avoid this backlash.
    Like maybe not acting like a complete d**khead. |  |'
  prefs: []
  type: TYPE_TB
- en: '| Same Author: I take issue with a faster landing being marked as less skilled.
    By that logic the slowest, smoothest possible landing would be the most skilled
    and that is plain wrong. Maybe war machine intentionally does faster and harder
    landings. | (1.00, 0.96) – The author is emphasizing the contrast                   between
    the two ideas.'
  prefs: []
  type: TYPE_NORMAL
- en: (0.78, 0.89) – The author is able to draw conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: (0.97, 0.86) – The author is using an all-or-none thinking
  prefs: []
  type: TYPE_NORMAL
- en: style. |
  prefs: []
  type: TYPE_NORMAL
- en: '| Different Author: She was the Ronald Reagan of the UK in the same time period.
    | (0.83, 0.05) – The author is describing sexual content. $\ast$ (0.38, 0.94)
     – The author is using words related to'
  prefs: []
  type: TYPE_NORMAL
- en: politics. $\ast$
  prefs: []
  type: TYPE_NORMAL
- en: (0.74, 0.38) –  The author is using parentheticals. |
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Example interpretable explanations on the Contrastive Authorship Verification
    task. The top style attributes in common between the Anchor text and a text by
    the Same Author are shown. The top distinct style attributes between the Anchor
    text and a text by a Different Author are also shown. The scores of each style
    attribute against the texts is shown in (•,  •/•). We manually inspect the style
    attributes and annotate them as reasonable, plausible, or incorrect explanations.
    Attributes annotated with $\ast$ blur the line between style and content. Error
    analysis can be found in Section [4.1](#S4.SS1 "4.1 Error Analysis ‣ 4 Results
    ‣ Learning Interpretable Style Embeddings via Prompting LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We perform this task with Lisa (LUAR + $W$ to the Euclidean distance as a measure
    of the general importance of each dimension. The importance score is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: 'To retrieve the top common style attributes, we rank the dimensions, and the
    corresponding style attributes, in descending order by the following score function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textsc{Score}_{\text{common}}(d)=\frac{\mathcal{I}(d)}{\sum\limits_{k=0}^{D}{\mathcal{I}(k)}}*\vec{v_{1}}_{d}*\vec{v_{2}}_{d}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'To retrieve the top distinct style attributes, we rank the dimensions, and
    the corresponding style attributes, in descending order by the following score
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix H Resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We provide links and citations to resources used in this paper which provide
    license information, documentation, and their intended use. Our usage follows
    the intended usage of all resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'We utilize the following models:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\text{GPT-3}_{\text{175B}}$ (text-davinci-003) (Brown et al., [2020](#bib.bib8);
    Ouyang et al., [2022](#bib.bib37))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EncT5 (t5-base) (Devlin et al., [2019](#bib.bib14); Liu et al., [2019](#bib.bib32))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DistilRoBERTa (nli-distilroberta-base-v2) (Sanh et al., [2019](#bib.bib50);
    Liu et al., [2019](#bib.bib32); Devlin et al., [2019](#bib.bib14); Reimers and
    Gurevych, [2019](#bib.bib46))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning Universal Authorship Representations (LUAR) Embedding model (Rivera-Soto
    et al., [2021](#bib.bib48))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Style embedding model from Wegmann et al. ([2022](#bib.bib61))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We utilize the following datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reddit Million User Dataset (Khan et al., [2021](#bib.bib27); Andrews and Bishop,
    [2019](#bib.bib3))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STEL dataset (Wegmann and Nguyen, [2021](#bib.bib60))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrastive Authorship Verification dataset (Wegmann et al., [2022](#bib.bib61))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formality in Online Communication (Pavlick and Tetreault, [2016](#bib.bib40))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grammarly’s Yahoo Answers Formality Corpus (Rao and Tetreault, [2018](#bib.bib45))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yelp Reviews Dataset (Zhang et al., [2015](#bib.bib65))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IMDB Large Movie Review Dataset (Maas et al., [2011](#bib.bib33))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon Customer Reviews Dataset (Amazon.com, [2018](#bib.bib2)) –  [https://s3.amazonaws.com/amazon-reviews-pds/readme.html](https://s3.amazonaws.com/amazon-reviews-pds/readme.html)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rotten Tomatoes Movie Review Data (Pang and Lee, [2005](#bib.bib38))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: App Reviews Dataset (Grano et al., [2017](#bib.bib17))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter Sentiment Analysis Training Corpus (Naji, [2012](#bib.bib35))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DAIR.AI Emotion Dataset (Saravia et al., [2018](#bib.bib51))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GoEmotions Dataset (Demszky et al., [2020](#bib.bib13))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Political Slant Dataset (Prabhumoye et al., [2018](#bib.bib41))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Twitter User Gender Classification Dataset (CrowdFlower, [2017](#bib.bib10))
    –  [https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification](https://www.kaggle.com/datasets/crowdflower/twitter-user-gender-classification)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: African-American Vernacular English Dataset (Groenwold et al., [2020](#bib.bib18))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shakespeare Dataset (Xu, [2017](#bib.bib63))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wikipedia Bias Dataset (Pryzant et al., [2020](#bib.bib42))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HateSpeech18 (de Gibert et al., [2018](#bib.bib12))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offensive Social Media Dataset (Atwell et al., [2022](#bib.bib4))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple Wikipedia Dataset (Coster and Kauchak, [2011](#bib.bib9))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ASSET (Alva-Manchego et al., [2020](#bib.bib1))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CoLA (Warstadt et al., [2019](#bib.bib59))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BLiMP (Warstadt et al., [2019](#bib.bib59))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We utilize the following software:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers (Wolf et al., [2019](#bib.bib62))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence-Transformers (Reimers and Gurevych, [2019](#bib.bib46))
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: emoji –  [https://pypi.org/project/sentence-splitter/](https://pypi.org/project/sentence-splitter/)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: sentence-splitter –  [https://pypi.org/project/sentence-splitter/](https://pypi.org/project/sentence-splitter/)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We estimate the total compute budget and detail computing infrastructure used
    to run the computational experiments found in this paper below:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1x NVIDIA RTX A6000 / 30GB RAM / 4x CPU – 230 hours
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
