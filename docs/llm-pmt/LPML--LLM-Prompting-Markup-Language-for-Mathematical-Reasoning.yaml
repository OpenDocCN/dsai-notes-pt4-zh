- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:30'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LPML: LLM-Prompting Markup Language for Mathematical Reasoning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.13078](https://ar5iv.labs.arxiv.org/html/2309.13078)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ryutaro Yamauchi
  prefs: []
  type: TYPE_NORMAL
- en: The University of Tokyo
  prefs: []
  type: TYPE_NORMAL
- en: ryutaro_yamauchi@weblab.t.u-tokyo.ac.jp \AndSho Sonoda
  prefs: []
  type: TYPE_NORMAL
- en: Center for Advanced Intelligence Project, RIKEN
  prefs: []
  type: TYPE_NORMAL
- en: sho.sonoda@riken.jp \ANDAkiyoshi Sannai
  prefs: []
  type: TYPE_NORMAL
- en: Kyoto University
  prefs: []
  type: TYPE_NORMAL
- en: sannai.akiyoshi.7z@kyoto-u.ac.jp \AndWataru Kumagai
  prefs: []
  type: TYPE_NORMAL
- en: The University of Tokyo
  prefs: []
  type: TYPE_NORMAL
- en: kumagai@weblab.t.u-tokyo.ac.jp This work was done when the author was at Albert
    Inc.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In utilizing large language models (LLMs) for mathematical reasoning, addressing
    the errors in the reasoning and calculation present in the generated text by LLMs
    is a crucial challenge. In this paper, we propose a novel framework that integrates
    the Chain-of-Thought (CoT) method with an external tool (Python REPL). We discovered
    that by prompting LLMs to generate structured text in XML-like markup language,
    we could seamlessly integrate CoT and the external tool and control the undesired
    behaviors of LLMs. With our approach, LLMs can utilize Python computation to rectify
    errors within CoT. We applied our method to ChatGPT (GPT-3.5) to solve challenging
    mathematical problems and demonstrated that combining CoT and Python REPL through
    the markup language enhances the reasoning capability of LLMs. Our approach enables
    LLMs to write the markup language and perform advanced mathematical reasoning
    using only zero-shot prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have demonstrated remarkable success in a wide
    range of natural language processing (NLP) tasks, despite being trained on the
    simplistic task of predicting subsequent words in sentences. LLMs possess the
    capability to extract rules and principles from the provided text (called In-Context
    Learning). This enables LLMs to solve even novel tasks by being prompted with
    a few-shot example (Brown et al., [2020](#bib.bib1)). Furthermore, it has been
    reported that effective prompts can improve the performance of LLMs. For instance,
    we can improve the ability of LLMs to perform arithmetic reasoning by prompting
    them to output not only the final answer but also intermediate steps involved
    in problem-solving. This technique, reffered to as Chain-of-Thought (CoT), is
    widely used  (Wei et al., [2022](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: One of the most essential challenges in employing LLMs for mathematical reasoning
    is the management of calculation and reasoning errors in LLMs’ outputs (Hendrycks
    et al., [2021](#bib.bib3)). One approach is to improve the performance of LLMs
    by increasing the model size (OpenAI, [2023](#bib.bib4)) or by fine-tuning with
    mathematical and technical texts (Lewkowycz et al., [2022](#bib.bib5); Chung et al.,
    [2022](#bib.bib6)), but training LLMs are often expensive. As a result, alternative
    strategies that improve performance without modifying the LLMs themselves have
    been sought. Several studies have LLMs generate explicit reasoning step (Wei et al.,
    [2022](#bib.bib2); Zhou et al., [2023](#bib.bib7); Nye et al., [2021](#bib.bib8)).
    (Wang et al., [2023](#bib.bib9)) proposes a self-consistency strategy to have
    LLMs generate multiple reasoning paths and merge them. Furthermore, methods for
    integrating LLMs with external tools have been proposed (Gao et al., [2022](#bib.bib10);
    Yao et al., [2022](#bib.bib11); Schick et al., [2023](#bib.bib12); Chen et al.,
    [2022](#bib.bib13); Mishra et al., [2022](#bib.bib14)). Given LLMs’ limitations
    with large-number calculations and complex reasoning, outsourcing these tasks
    to external tools can potentially reduce reasoning errors. For instance, PAL (Gao
    et al., [2022](#bib.bib10)) outsources calculations that would be difficult for
    LLMs alone to a computer by few-shot prompting with pairs of a problem and a Python
    code to solve it. However, these methods treat the use of external tools as a
    module independent of other reasoning processes, so the accuracy limitations of
    each module constrain overall performance. To fundamentally improve the accuracy
    of reasoning, the consistency of the external tool’s behavior should be monitored
    by LLMs themselves.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/01004b0bc4189941254cec0f4b590696.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An example of the reasoning process: assistant (LLM) outputs CoT
    and Python code, while the system (computer) provides feedback on the code’s execution
    results. Since assistant may output invalid elements, system removes them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we propose a novel framework for integrating the Python REPL
    (read–eval–print loop) with LLMs. Our approach differs from previous works in
    that we use Python code (and its execution results) as part of the CoT rather
    than having LLMs directly generate the Python code to solve the problem. Specifically,
    we construct an interaction loop between LLMs and the Python REPL by having LLMs
    generate both the CoT and the Python code and then feeding the results of the
    code execution back to LLMs to induce further CoTs (Fig. [2](#S2.F2 "Figure 2
    ‣ 2.1 Interaction loop between ChatGPT and Python REPL ‣ 2 Methods ‣ LPML: LLM-Prompting
    Markup Language for Mathematical Reasoning")). This allows LLMs to have two reasoning
    paths, CoT and Python code execution and is expected to improve the reasoning
    performance by self-consistency. However, we found that simply feeding back Python
    code execution results does not lead LLMs to behave as we expected: when the code
    is written in the CoT, LLMs do not wait for the execution results but output dummy
    results as well. Also, when LLMs make mistakes in CoT reasoning, they tend to
    think that the code is incorrect, even if they write correct Python code and get
    the result of that execution. To avoid these problems, we define an XML-like markup
    language (LLM-prompting markup language, LPML) and feed its syntax to LLMs to
    prompt them to output text structured in the markup language (Fig. [3](#S2.F3
    "Figure 3 ‣ 2.2 LLM-prompting markup language ‣ 2 Methods ‣ LPML: LLM-Prompting
    Markup Language for Mathematical Reasoning")). LPML includes `THINK` tags for
    CoT, `PYTHON` tags for Python code, and `OUTPUT` tags for the results of Python
    code execution, etc. By using these tags, the LLMs’ output text is structured.
    The advantages of this approach are as follows: 1\. we can easily parse the text
    generated by LLMs and segment it by function and meaning; 2\. we can mechanically
    remove invalid text generated by LLMs, such as dummy execution results because
    the tags correspond to their contents; and 3\. we can define relationships between
    tags, which enables us to make LLMs trust the results of Python calculations over
    those in the CoT, which are prone to errors. As a result, we achieve the fall-back
    in reasoning that LLMs often have difficulty with. We applied our method to ChatGPT (OpenAI,
    [2022](#bib.bib15)) to solve challenging mathematical problems and demonstrated
    that the combination of CoT and Python computation improves LLMs’ reasoning ability.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Interaction loop between ChatGPT and Python REPL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this study, we used OpenAI ChatGPT (GPT-3.5-Turbo) (OpenAI, [2022](#bib.bib15))
    via API, which is an LLM that takes a sequence of messages $[m_{0},\cdots,m_{k}]$.
    The API specification allows us to set three roles as the speaker of a message:
    assistant, user, and system, where assistant is an AI, user is a human, and system
    is a conversation context manager.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f9278febe72554f199f9153b59d6525.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An overview of the mathematical reasoning process. system and assistant
    continue to interact until an answer is obtained. All messages are structured
    in the markup language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown below, we designed the mathematical reasoning process as an interaction
    between system and assistant. First, as system, we present the problem to be solved
    and define the syntax of the markup language and rules of reasoning ($m^{s}_{0}$).
    We also remove the answer if assistant outputs both the Python code and the answer
    in a single message, and then feed back the Python code execution results. We
    made system and assistant repeat this process until assistant outputs the answer
    (Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Interaction loop between ChatGPT and Python REPL
    ‣ 2 Methods ‣ LPML: LLM-Prompting Markup Language for Mathematical Reasoning")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 LLM-prompting markup language
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30d795afb8344940fe1490be39c7a7ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The prompt: the syntax definition of LPML and instructions on how
    to solve the problem written in LPML.'
  prefs: []
  type: TYPE_NORMAL
- en: The LPML, the markup language we have defined, is based on XML syntax and is
    a set of elements consisting of content enclosed in a start tag `` and end
    tag ``. Since the dataset used to train LLMs is assumed to include text
    written in XML and HTML, LLMs can write LPML that is similar to them. The LPML
    includes the tags `DEFINE`, `PROBLEM`, `ANSWER`, `THINK`, `PYTHON`, and `OUTPUT`.
    `DEFINE` tag is for defining tags and rules. The syntax of the language and the
    tags are defined by `DEFINE` tag. `PROBLEM` and `ANSWER` tags are for describing
    the problem and answer, and the solving process ends when assistant outputs `ANSWER`
    tag. `THINK` tag is for describing thoughts. By defining `THINK` tag as “a tag
    for describing thinking step-by-step”, we induce Zero-shot-CoT (Kojima et al.,
    [2022](#bib.bib16)). `PYTHON` tag is for describing Python code. When assistant
    uses `PYTHON` tag, system returns the execution result using `OUTPUT` tag. By
    using these tags, all messages, including the initial system prompt, are written
    in the LPML, thus strongly conditioning assistant to output in the LPML.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Instructions to ChatGPT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the initial prompt ($m_{0}^{s}$), we instruct ChatGPT to solve the math problem
    in the PROBLEM tag using the two tags `PYTHON` and `THINK` and to write the answer
    using the `ANSWER` tag. We do not give a few-shot example of how to solve the
    problem but just give the LPML definition, instructions on which tags to use,
    and some notes.
  prefs: []
  type: TYPE_NORMAL
- en: We also instructed ChatGPT to trust the contents of `OUTPUT` tags rather than
    the contents of `THINK` tags. This enables LLMs to ignore errors in CoT by referring
    to the results of Python code execution. Without this instruction, ChatGPT would
    either assume that the Python code was incorrect and fall into a loop of repeated
    debugging or ignore the contents of the `OUTPUT` tag.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experiments ans Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Datasets and Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We evaluated our method on the GSM8K dataset (Cobbe et al., [2021](#bib.bib17))
    and the MATH dataset (Hendrycks et al., [2021](#bib.bib3)). Both datasets are
    available under MIT License. The GSM8K is a dataset of 8.5K grade-school level
    math word problems, and we used the 1319 test problems in it for our evaluation.
    The MATH dataset consists of 12,500 challenging competitive math problems, and
    the dataset contains seven subjects: Prealgebra, Algebra, Number Theory, Counting
    and Probability, Geometry, Intermediate Algebra, and Precalculus. We sampled 20
    problems from each of the six subjects except Geometry and solved these 120 problems
    using our method.'
  prefs: []
  type: TYPE_NORMAL
- en: We scored only the content of the `ANSWER` tags in evaluating our method. And
    we manually scored the MATH dataset because of the arbitrariness of how to write
    the answers.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We used Chain-of-Thought (CoT) (Wei et al., [2022](#bib.bib2)) and Program-aided
    Language models (PAL) (Gao et al., [2022](#bib.bib10)) for comparison. PAL is
    a method of solving mathematical problems by having LLMs write Python code. Since
    PAL uses Python code and does not use CoT, we can evaluate the effectiveness of
    our method using both Python and CoT by comparing our method with PAL. We used
    ChatGPT (GPT-3.5-Turbo) as the backbone LLM for both methods. Since both the GSM8K
    and MATH datasets contain manually written reasoning paths, we randomly selected
    five problems from each dataset and used them as few-shot examples for CoT. For
    PAL, we used the author’s implementation. The author’s implementation contains
    Python code examples for few-shot prompting. Since this example is not tuned for
    the MATH dataset, we recreated the prompt using problems from the MATH dataset,
    but since there was no significant difference in performance, we present results
    from the author’s prompt in subsequent reports.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Implementation details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our implementation of system behaves as follows. First, system parses the output
    of assistant $\hat{m}_{k}^{a}$ to assisntant.
  prefs: []
  type: TYPE_NORMAL
- en: We set the number of system and assistant interactions to 8 turns, excluding
    the initial prompt. If the normalized assistant output $m_{k}^{a}$ contains an
    `ANSWER` tag, the interaction is terminated at that point. If no `ANSWER` tag
    was output once throughout the interaction, the interaction is redone from the
    beginning. We set the ChatGPT temperature to 0\. However, if the ANSWER tag was
    not output after 8 interactions, we increased the temperature in 0.1 increments
    for up to 5 attempts.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [4](#S3.F4 "Figure 4 ‣ 3.4 Results ‣ 3 Experiments ans Results ‣ LPML:
    LLM-Prompting Markup Language for Mathematical Reasoning") shows the comparison
    results between the proposed method and the previous method on the GSM8K and MATH
    datasets. And Figure [4](#S3.F4 "Figure 4 ‣ 3.4 Results ‣ 3 Experiments ans Results
    ‣ LPML: LLM-Prompting Markup Language for Mathematical Reasoning") shows the success
    rate of each method in each subject of the MATH data set. In the GSM8K dataset,
    PAL showed a higher accuracy rate than our method. However, our method showed
    a higher accuracy rate in the MATH dataset. The problems included in the MATH
    dataset are relatively difficult, and it is not always possible to implement Python
    code to solve them directly. Our method can combine the strengths of Python calculations
    and CoT reasonings, enabling LLMs to use CoT to solve reasoning that cannot be
    achieved with Python code while reducing CoT reasoning errors with Python calculations.
    On the other hand, the problems in the GSM8K dataset can be easily implemented
    with Python code, which may have resulted in decreased performance due to CoT-derived
    reasoning errors.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | GSM8K | MATH |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 57.1 | 31.7 |'
  prefs: []
  type: TYPE_TB
- en: '| PAL | 79.8 | 47.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 76.6 | 60.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Comparison of the success rate of each method on the GSM8K and MATH.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e95234da4eed936f118c4b2c7b847171.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The number of correct answers per subject in the MATH dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Discussion and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We demonstrate that the reasoning ability of LLMs can be enhanced by having
    LLMs generate structured text in a markup language. Structuring the text generated
    by LLMs allows us to easily parse it, allowing us to control the behavior of LLMs
    and integrate LLMs with the Python REPL flexibly. Also, the tags qualify and divide
    texts into sections, allowing us to define relationships between texts and inducing
    LLMs behavior that prioritizes certain sentences over others.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to speech, humans have a variety of language output ports, including
    internal speech and writing. The use of markup languages to qualify and divide
    generated text into semantic and functional units is the equivalent to LLMs having
    such a variety of language output ports. In this study, two tags, `PYTHON` and
    `THINK`, were given to LLMs, but theoretically, any variety of tags can be defined,
    such as tags that output queries for web search or database operations or tags
    that output internal speech. However, the use of such a markup language is not
    a natural behavior for existing LLMs, and it is unclear whether LLMs can handle
    more complex syntax. Therefore, one intriguing future research direction is the
    fine-tuning of LLMs based on the use of markup languages.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we have successfully corrected some CoT errors with Python execution result
    feedback, this approach does not guarantee consistent success. Consequently, the
    number of errors caused by the limitations of LLMs will be higher compared to
    scenarios where LLMs generate Python code without CoT. As a result, in situations
    where there is no synergy between CoT and external tools, the performance will
    be reduced. Additionally, while it is possible to make LLMs acknowledge their
    errors, understanding the specific details of these errors often remains challenging.
    We are currently exploring potential solutions to address these limitations in
    principle.
  prefs: []
  type: TYPE_NORMAL
- en: Because LLMs are trained on a large corpus of text collected from the web, they
    have the potential to amplify biases in the training data. Thus, our method, which
    utilizes the statistical patterns that LLMs acquire via prompting, has the same
    problem. However, we believe that our method enables the seamless integration
    of LLMs with external resources and has the potential to correct the biases of
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
    M.F. Balcan, and H. Lin, editors, *Advances in Neural Information Processing Systems*,
    volume 33, pages 1877–1901\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian
    ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny Zhou. Chain-of-thought prompting
    elicits reasoning in large language models. In S. Koyejo, S. Mohamed, A. Agarwal,
    D. Belgrave, K. Cho, and A. Oh, editors, *Advances in Neural Information Processing
    Systems*, volume 35, pages 24824–24837\. Curran Associates, Inc., 2022. URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2021) Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,
    Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical
    problem solving with the math dataset. *NeurIPS*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewkowycz et al. (2022) Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan
    Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag,
    Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra.
    Solving quantitative reasoning problems with language models. In S. Koyejo, S. Mohamed,
    A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, *Advances in Neural Information
    Processing Systems*, volume 35, pages 3843–3857\. Curran Associates, Inc., 2022.
    URL [https://proceedings.neurips.cc/paper_files/paper/2022/file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf](https://proceedings.neurips.cc/paper_files/paper/2022/file/18abbeef8cfe9203fdf9053c9c4fe191-Paper-Conference.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert
    Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery,
    Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav
    Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov,
    Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and
    Jason Wei. Scaling instruction-finetuned language models, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and
    Ed Chi. Least-to-most prompting enables complex reasoning in large language models,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nye et al. (2021) Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk
    Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten
    Bosma, David Luan, Charles Sutton, and Augustus Odena. Show your work: Scratchpads
    for intermediate computation with language models, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2022) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. Pal: Program-aided language models.
    *arXiv preprint arXiv:2211.10435*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    Program of thoughts prompting: Disentangling computation from reasoning for numerical
    reasoning tasks. *arXiv preprint arXiv:2211.12588*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mishra et al. (2022) Swaroop Mishra, Matthew Finlayson, Pan Lu, Leonard Tang,
    Sean Welleck, Chitta Baral, Tanmay Rajpurohit, Oyvind Tafjord, Ashish Sabharwal,
    Peter Clark, and Ashwin Kalyan. LILA: A unified benchmark for mathematical reasoning.
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 5807–5832, Abu Dhabi, United Arab Emirates, December 2022\.
    Association for Computational Linguistics. URL [https://aclanthology.org/2022.emnlp-main.392](https://aclanthology.org/2022.emnlp-main.392).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2022) OpenAI. OpenAI: Introducing ChatGPT. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang (Shane) Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In
    *Advances in Neural Information Processing Systems*, volume 35, pages 22199–22213,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math
    word problems, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Example Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure [5](#A2.F5 "Figure 5 ‣ Appendix B Examples of Reasoning Processes ‣
    LPML: LLM-Prompting Markup Language for Mathematical Reasoning") shows an example
    of the prompts we used. By writing the prompt itself in LPML format, we can strongly
    prompt LLMs to generate text in LPML format.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Examples of Reasoning Processes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figures [6](#A2.F6 "Figure 6 ‣ Appendix B Examples of Reasoning Processes ‣
    LPML: LLM-Prompting Markup Language for Mathematical Reasoning"), [7](#A2.F7 "Figure
    7 ‣ Appendix B Examples of Reasoning Processes ‣ LPML: LLM-Prompting Markup Language
    for Mathematical Reasoning"), and [8](#A2.F8 "Figure 8 ‣ Appendix B Examples of
    Reasoning Processes ‣ LPML: LLM-Prompting Markup Language for Mathematical Reasoning")
    show examples of the inference process. LPML definitions and instructions to LLMs
    are omitted. In the example in Figure [6](#A2.F6 "Figure 6 ‣ Appendix B Examples
    of Reasoning Processes ‣ LPML: LLM-Prompting Markup Language for Mathematical
    Reasoning"), the LLM makes a computational error within the CoT, but corrects
    the error using the Python results and reaches the correct answer. On the other
    hand, in the example in Figure [7](#A2.F7 "Figure 7 ‣ Appendix B Examples of Reasoning
    Processes ‣ LPML: LLM-Prompting Markup Language for Mathematical Reasoning"),
    the LLM writes the correct Python code but ignores the results and outputs a different
    answer. In the example in Figure [8](#A2.F8 "Figure 8 ‣ Appendix B Examples of
    Reasoning Processes ‣ LPML: LLM-Prompting Markup Language for Mathematical Reasoning"),
    there is a problem with the Python code, but the CoT has arrived at the correct
    answer. As these examples show, LLM’s way to use external tools is inconsistent,
    and the success or failure of the reasoning still depends on chance.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 5: Example prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 6: The LLM makes a computational error within the CoT, but corrects
    the error using the Python results and reaches the correct answer'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 7: The LLM writes the correct Python code but ignores the results and
    outputs a different answer.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Figure 8: There is a problem with the Python code, but the CoT has arrived
    at the correct answer.'
  prefs: []
  type: TYPE_NORMAL
