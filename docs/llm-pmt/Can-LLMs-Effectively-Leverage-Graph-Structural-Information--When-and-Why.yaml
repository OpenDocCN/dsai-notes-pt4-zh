- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Can LLMs Effectively Leverage Graph Structural Information: When and Why'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs是否能有效利用图结构信息：何时及为何
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16595](https://ar5iv.labs.arxiv.org/html/2309.16595)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16595](https://ar5iv.labs.arxiv.org/html/2309.16595)
- en: Jin Huang¹ Xingjian Zhang¹ Qiaozhu Mei¹ Jiaqi Ma²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jin Huang¹ Xingjian Zhang¹ Qiaozhu Mei¹ Jiaqi Ma²
- en: ¹University of Michigan  ²University of Illinois Urbana-Champaign
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹密歇根大学  ²伊利诺伊大学厄尔巴纳-香槟分校
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This paper studies Large Language Models (LLMs) augmented with structured data–particularly
    graphs–a crucial data modality that remains underexplored in the LLM literature.
    We aim to understand when and why the incorporation of structural information
    inherent in graph data can improve the prediction performance of LLMs on node
    classification tasks with textual features. To address the “when” question, we
    examine a variety of prompting methods for encoding structural information, in
    settings where textual node features are either rich or scarce. For the “why”
    questions, we probe into two potential contributing factors to the LLM performance:
    data leakage and homophily. Our exploration of these questions reveals that (i)
    LLMs can benefit from structural information, especially when textual node features
    are scarce; (ii) there is no substantial evidence indicating that the performance
    of LLMs is significantly attributed to data leakage; and (iii) the performance
    of LLMs on a target node is strongly positively related to the local homophily
    ratio of the node¹¹1Codes and datasets are at: [https://github.com/TRAIS-Lab/LLM-Structured-Data](https://github.com/TRAIS-Lab/LLM-Structured-Data).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本文研究了结合结构化数据（特别是图形）的大型语言模型（LLMs），这是LLM文献中仍然未充分探讨的关键数据模态。我们旨在了解何时以及为何图形数据中的结构信息的融入可以提高LLM在节点分类任务中的预测性能。为了回答“何时”这个问题，我们考察了多种编码结构信息的提示方法，研究文本节点特征丰富或稀少的情况。对于“为何”问题，我们探讨了两个可能影响LLM性能的因素：数据泄漏和同质性。对这些问题的探索揭示了（i）LLM可以从结构信息中获益，特别是当文本节点特征稀少时；（ii）没有实质证据表明LLM的性能显著受数据泄漏影响；以及（iii）LLM在目标节点上的性能与该节点的局部同质性比率呈强正相关¹¹1代码和数据集请见：[https://github.com/TRAIS-Lab/LLM-Structured-Data](https://github.com/TRAIS-Lab/LLM-Structured-Data)。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: 'Large Language Models (LLMs) have gained great popularity for a broad range
    of applications (Brown et al., [2020](#bib.bib4); OpenAI, [2023](#bib.bib35)).
    One important reason for their widespread adoption is the ability of an LLM to
    act as a versatile model, capable of solving a variety of tasks in a zero- or
    few-shot fashion. Recently, there is an increasing interest in enhancing the versatility
    of LLMs through multi-modal capabilities (Yin et al., [2023](#bib.bib50); Yang
    et al., [2023](#bib.bib47)). Several modalities, including images (Radford et al.,
    [2021](#bib.bib38)), videos (Li et al., [2023](#bib.bib23)), and even robotics (Brohan
    et al., [2023](#bib.bib3)), have been intensively explored; yet structured data,
    particularly in the form of graphs, remains largely underexplored. This leads
    us to an intriguing question: could the incorporation of structural information
    (such as graphs), when available, improve the predictive accuracy of LLMs?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）因其在广泛应用中的巨大受欢迎程度而备受关注（Brown et al., [2020](#bib.bib4); OpenAI, [2023](#bib.bib35)）。它们被广泛采用的一个重要原因是LLM具备作为多功能模型的能力，能够以零样本或少样本的方式解决各种任务。最近，增强LLM多模态能力的兴趣不断增加（Yin
    et al., [2023](#bib.bib50); Yang et al., [2023](#bib.bib47)）。多个模态，包括图像（Radford
    et al., [2021](#bib.bib38)）、视频（Li et al., [2023](#bib.bib23)）甚至机器人（Brohan et al.,
    [2023](#bib.bib3)）已被深入探索；然而，结构化数据，特别是图形形式的数据，仍然较少被探讨。这引出了一个有趣的问题：当有结构信息（如图形）时，是否可以提高LLM的预测准确性？
- en: Directly answering this question turns to be tricky. Consider citation networks
    as an example, where each node represents a research paper, and each edge indicates
    a citation relationship between papers. While LLMs can make predictions based
    on node-level information alone, such as a paper’s title and abstract, there has
    not been a systematic understanding on whether LLMs can benefit from the neighborhood
    surrounding the target node. A few studies have touched on incorporating structured
    data with LLMs (Wang et al., [2023](#bib.bib44); He et al., [2023](#bib.bib15);
    Chen et al., [2023](#bib.bib7)). A recent work concurrent to this study, Chen
    et al. ([2023](#bib.bib7)), suggests that LLMs can, in some cases, benefit from
    neighborhood information, although the extent of this benefit can be dataset-dependent
    and the underlying mechanisms are not fully understood. Indeed, a notable concern
    arises as most node classification benchmarks have a data cut-off that predates
    the training data cut-off of LLMs like ChatGPT. This discrepancy raises concerns
    about data leakage–LLMs may have seen and memorized at least part of the test
    data of the common benchmark datasets–which could undermine the reliability of
    studies using earlier benchmark datasets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 直接回答这个问题变得棘手。以引文网络为例，其中每个节点代表一篇研究论文，每条边表示论文之间的引用关系。虽然LLMs可以仅根据节点级信息（例如论文的标题和摘要）进行预测，但尚未系统了解LLMs是否可以从目标节点周围的邻域中受益。一些研究已经涉及将结构化数据与LLMs结合（Wang
    et al., [2023](#bib.bib44); He et al., [2023](#bib.bib15); Chen et al., [2023](#bib.bib7)）。与本研究同时进行的一项工作，Chen
    et al. ([2023](#bib.bib7))，表明LLMs在某些情况下可以从邻域信息中受益，尽管这种益处的程度可能依赖于数据集，且其潜在机制尚未完全理解。确实，一个显著的担忧是大多数节点分类基准数据集的截止日期早于LLMs如ChatGPT的训练数据截止日期。这种不一致引发了关于数据泄露的担忧——LLMs可能已经看过并记住了至少部分公共基准数据集的测试数据——这可能会削弱使用早期基准数据集的研究的可靠性。
- en: To this end, this paper focuses on two concrete questions relevant to the incorporation
    of structural information into LLMs. Firstly, we seek to understand the conditions
    under which incorporating structural information improves the prediction accuracy
    of LLMs. Secondly, we examine potential factors contributing to the performance
    of LLMs (either desirable or not), particularly *data leakage* and *homophily* (McPherson
    et al., [2001](#bib.bib30)), the latter being the tendency of nodes with similar
    characteristics to connect. As an early attempt towards these questions, we focus
    on prompting methods for encoding structural information throughout this study,
    and leave the investigation of more advanced methods to future work.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了达到这个目的，本文聚焦于将结构信息融入大型语言模型（LLMs）的两个具体问题。首先，我们希望了解在什么条件下融入结构信息能提高LLMs的预测准确性。其次，我们考察影响LLMs性能的潜在因素（无论是积极的还是消极的），特别是*数据泄露*和*同质性*（McPherson
    et al., [2001](#bib.bib30)），后者是指具有相似特征的节点倾向于连接。作为对这些问题的初步尝试，本文重点研究了在整个研究过程中编码结构信息的提示方法，并将对更先进方法的探讨留待未来的工作。
- en: Addressing the first question, we examine various methods to encode structural
    information into prompts, and using ChatGPT API (OpenAI, [2022](#bib.bib34)),
    we test them on node classification datasets with textual features. In particular,
    we transform the textual content of a target node and its neighboring nodes into
    natural language and instruct LLM to make predictions. By varying the richness
    of node-level textual information and the information incorporated from neighboring
    nodes, we reveal the conditions under which LLMs would benefit more from structural
    information.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 针对第一个问题，我们考察了将结构信息编码到提示中的各种方法，并使用ChatGPT API（OpenAI, [2022](#bib.bib34)）在具有文本特征的节点分类数据集上进行了测试。特别地，我们将目标节点及其邻近节点的文本内容转化为自然语言，并指示LLM进行预测。通过改变节点级文本信息的丰富程度和来自邻近节点的信息，我们揭示了LLMs在何种条件下能够更好地从结构信息中受益。
- en: For the second question, we first investigate the extent to which data leakage
    might artificially inflate the performance of LLMs. To rigorously measure the
    data leakage effect, we collect a new dataset, ensuring that the test nodes are
    sampled from time periods post the data cut-off of ChatGPT. Additionally, we examine
    the impact of homophily on the classification performance of LLMs. Through controlled
    experiments and correlation analyses, we establish a relationship between the
    local homophily ratio and the prediction accuracy of LLMs.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个问题，我们首先调查了数据泄漏可能在多大程度上人为地提高了LLMs的性能。为了严格测量数据泄漏效应，我们收集了一个新数据集，确保测试节点来自于ChatGPT数据截止后的时间段。此外，我们还检验了同质性对LLMs分类性能的影响。通过控制实验和相关性分析，我们建立了局部同质性比率与LLMs预测准确性之间的关系。
- en: Our key findings are summarized as follows. (i) LLMs benefit more from structural
    information when textual information of the target node is scarce. (ii) There
    is no strong evidence that data leakage is a major factor contributing to the
    performance of LLMs on node classification benchmark datasets. (iii) Homophily
    in the graph-structured data is a significant contributor to the improved accuracy
    observed in LLMs after incorporating structural information.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要发现总结如下：（i）当目标节点的文本信息稀缺时，LLMs从结构信息中获益更多。（ii）没有强有力的证据表明数据泄漏是导致LLMs在节点分类基准数据集上表现的主要因素。（iii）图结构数据中的同质性是LLMs在融入结构信息后准确性提高的一个重要因素。
- en: Overall, this study marks an early attempt for the ambitious goal of enabling
    LLMs to be effectively augmented with structured data, an important data modality.
    By adapting node classification datasets with textual features, we establish a
    proper testbed for this goal. We have also examined various prompting methods
    for encoding the structural information with deeper understandings of their performance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，本研究标志着在实现LLMs与结构化数据有效增强这一雄心勃勃目标方面的早期尝试，这是一个重要的数据模式。通过调整节点分类数据集中的文本特征，我们为这一目标建立了一个合适的测试平台。我们还检验了各种提示方法，用于编码结构信息，并深入了解它们的表现。
- en: 2 Related Literature
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关文献
- en: LLMs for graph learning.
  id: totrans-19
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 用于图学习的LLMs。
- en: 'We make a distinction between two lines of research: Using LLMs to solve graph
    learning tasks, and augmenting LLMs with structured data.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究分为两个方向：使用LLMs解决图学习任务，以及通过结构化数据增强LLMs。
- en: 'The first line has been examined by a few studies recently. He et al. ([2023](#bib.bib15))
    propose a method where LLMs perform zero-shot predictions along with generating
    explanations for their decisions, which are then used to enhance node features
    for training Message Passing Neural Networks (MPNNs) (Gilmer et al., [2017](#bib.bib11))
    to predict node categories. Chen et al. ([2023](#bib.bib7)) extend the work of
     He et al. ([2023](#bib.bib15)) by using LLMs both as feature enhancers and as
    predictors for node classification. They offer several observations such as Chain-of-thoughts
    is not contributing to performance gains. Wang et al. ([2023](#bib.bib44)) introduce
    NLGraph to benchmark LLMs on traditional graph tasks, while Guo et al. ([2023](#bib.bib12))
    perform an empirical study on using LLMs to solve structure and semantic understanding
    tasks. More recently, Ye et al. ([2023](#bib.bib49)) propose InstructGLM for the
    instruction tuning of LLMs, like LLaMA (Touvron et al., [2023](#bib.bib41)), for
    node classification tasks. One commonality for many of these methods is that they
    use LLMs as a sub-component (e.g., as a feature extractor) of conventional graph
    learning framework. Our study differs with this line of research in terms of the
    motivation: while we are using node classification datasets as a testbed, our
    primary goal is to understand LLMs’ capability of processing the graph modality,
    instead of leveraging LLMs to better solve node classification tasks.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行最近已通过几项研究进行了检查。He等人（[2023](#bib.bib15)）提出了一种方法，其中LLMs执行零-shot预测并生成决策解释，然后利用这些解释来增强节点特征，以训练消息传递神经网络（MPNNs）（Gilmer等人，[2017](#bib.bib11)）以预测节点类别。Chen等人（[2023](#bib.bib7)）通过将LLMs既用作特征增强器，又用作节点分类预测器，扩展了He等人（[2023](#bib.bib15)）的工作。他们提供了几项观察结果，如链式思维未能带来性能提升。Wang等人（[2023](#bib.bib44)）引入了NLGraph，用于在传统图任务中基准测试LLMs，而Guo等人（[2023](#bib.bib12)）对使用LLMs解决结构和语义理解任务进行了实证研究。最近，Ye等人（[2023](#bib.bib49)）提出了InstructGLM，用于LLMs的指令调优，如LLaMA（Touvron等人，[2023](#bib.bib41)），用于节点分类任务。这些方法的共同点之一是它们将LLMs用作传统图学习框架的子组件（例如，作为特征提取器）。我们的研究在动机上与这一研究方向有所不同：尽管我们使用节点分类数据集作为测试平台，但我们的主要目标是了解LLMs处理图模式的能力，而不是利用LLMs更好地解决节点分类任务。
- en: On the other side, the line of research for augmenting LLMs with structured
    data, which our work belongs to, has also been explored in literature. Works by
    Zhang ([2023](#bib.bib51)) and Jiang et al. ([2023](#bib.bib18)) start to explore
    this space by interfacing LLMs with external tools and enhancing reasoning over
    structured data like knowledge graphs (KGs) or tables. Pan et al. ([2023](#bib.bib36))
    further investigate this by outlining a roadmap for integrating LLMs with KGs.
    However, structured data other than KGs and tables are still underexplored. Despite
    these initial efforts, a comprehensive understanding of the circumstances under
    which LLMs can efficiently leverage structural information in a zero-shot setting
    remains elusive. Our work contributes to this emerging field, seeking to provide
    more insights into the effective integration of LLMs with structured data.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，我们的工作属于增强LLMs与结构化数据结合的研究方向，这一方向在文献中也有所探讨。Zhang（[2023](#bib.bib51)）和Jiang等人（[2023](#bib.bib18)）的工作开始通过将LLMs与外部工具接口和增强对结构化数据（如知识图谱（KGs）或表格）的推理来探索这一领域。Pan等人（[2023](#bib.bib36)）进一步研究了这一领域，概述了将LLMs与KGs集成的路线图。然而，除了KGs和表格之外的结构化数据仍然未被充分探索。尽管有这些初步努力，但在零-shot设置下，LLMs如何有效利用结构信息的全面理解仍然难以捉摸。我们的工作为这一新兴领域做出了贡献，旨在提供更多关于LLMs与结构化数据有效集成的见解。
- en: Data leakage in LLMs.
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLMs中的数据泄漏。
- en: Data leakage in LLMs has become a focal point of discussion due to the models’
    intrinsic ability to memorize training data. As demonstrated by Carlini et al.
    ([2022](#bib.bib6)), LLMs can emit memorized portions of their training data when
    appropriately prompted, a phenomenon that intensifies with increased model capacity
    and training data duplication. While memorization is inherent to their function,
    it raises serious security and privacy concerns. A study by Carlini et al. ([2021](#bib.bib5))
    shows that extraction attacks can recover sensitive information such as personally
    identifiable information (PII) from GPT-2 (Radford et al., [2019](#bib.bib37)).
    This capability to store and potentially leak personal data is further explored
    by Huang et al. ([2022](#bib.bib17)), confirming that although the risk is relatively
    low, there is a tangible potential for information leakage. Specifically,  Carlini
    et al. ([2022](#bib.bib6)) show that the 6 billion parameter GPT-J model (Wang
    & Komatsuzaki, [2021](#bib.bib43)) memorizes at least 1% of its training dataset.
    Furthermore, the issue of data leakage complicates the evaluation of these models.
    As highlighted by Aiyappa et al. ([2023](#bib.bib1)), the closed nature and continuous
    updates of models like ChatGPT make it challenging to prevent data contamination,
    affecting the reliability of evaluation on LLMs in various applications. In node
    classification tasks, a concurrent work by Chen et al. ([2023](#bib.bib7)) observe
    that a specific prompt alteration significantly improved performance on ogbn-arxiv,
    raising concerns about potential test data leakage. In this work, we take a rigorours
    approach by curating a new dataset for node classification tasks, which is explicitly
    designed to address the data leakage issues in existing benchmarks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型固有的记忆训练数据的能力，LLMs 中的数据泄漏成为讨论的焦点。正如 Carlini 等人（[2022](#bib.bib6)）所示，LLMs
    可以在适当提示时释放其记忆中的训练数据部分，这种现象随着模型容量和训练数据重复的增加而加剧。虽然记忆是其功能的固有特征，但它引发了严重的安全和隐私问题。Carlini
    等人（[2021](#bib.bib5)）的研究显示，提取攻击可以从 GPT-2（Radford 等，[2019](#bib.bib37)）中恢复敏感信息，如个人身份信息（PII）。Huang
    等人（[2022](#bib.bib17)）进一步探讨了存储和潜在泄露个人数据的能力，确认尽管风险相对较低，但确实存在信息泄露的潜在可能性。具体来说，Carlini
    等人（[2022](#bib.bib6)）展示了 60 亿参数的 GPT-J 模型（Wang & Komatsuzaki，[2021](#bib.bib43)）记忆了至少
    1% 的训练数据集。此外，数据泄漏问题使得这些模型的评估变得复杂。正如 Aiyappa 等人（[2023](#bib.bib1)）所强调的，像 ChatGPT
    这样的模型由于其封闭性和持续更新，使得防止数据污染变得具有挑战性，影响了对 LLMs 在各种应用中的评估可靠性。在节点分类任务中，Chen 等人（[2023](#bib.bib7)）的研究发现，特定提示的调整显著提高了
    ogbn-arxiv 上的性能，引发了对潜在测试数据泄露的担忧。在这项工作中，我们采取了严格的方法，通过策划一个新的节点分类数据集，专门设计用于解决现有基准中的数据泄漏问题。
- en: Homophily in graph learning.
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 图学习中的同质性。
- en: 'The concept of homophily (McPherson et al., [2001](#bib.bib30)), which describes
    the tendency of nodes to form connections with similar nodes, plays an important
    role in the effectiveness of various graph learning methods (Zhu et al., [2020](#bib.bib52);
    Halcrow et al., [2020](#bib.bib13); Maurya et al., [2021](#bib.bib28); Lim et al.,
    [2021](#bib.bib24)). The principle of homophily enables MPNNs to smooth node representations
    by aggregating features from their likely similarly-labeled neighboring nodes.
    This aggregation process is particularly effective in various types of real-world
    graphs, such as political networks (Knoke, [1990](#bib.bib20)), and citation networks (Ciotti
    et al., [2016](#bib.bib8)). Despite its benefits, the reliance on homophily presents
    a challenge: MPNNs tend to underperform in graphs characterized by heterophily,
    where connected nodes are likely to differ in properties or labels (Zhu et al.,
    [2020](#bib.bib52)). Notably, the impact of homophily on the integration of structured
    data into LLMs remains an open area for exploration.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同质性（McPherson 等，[2001](#bib.bib30)）这一概念描述了节点倾向于与相似节点建立连接的趋势，在各种图学习方法的有效性中扮演着重要角色（Zhu
    等，[2020](#bib.bib52)；Halcrow 等，[2020](#bib.bib13)；Maurya 等，[2021](#bib.bib28)；Lim
    等，[2021](#bib.bib24)）。同质性的原则使得 MPNNs 通过聚合来自可能具有相似标签的邻近节点的特征来平滑节点表示。这一聚合过程在各种类型的真实世界图中尤其有效，如政治网络（Knoke，[1990](#bib.bib20)）和引文网络（Ciotti
    等，[2016](#bib.bib8)）。尽管有其好处，但依赖同质性也带来了挑战：MPNNs 在以异质性为特征的图中表现往往不佳，其中连接的节点可能在属性或标签上存在差异（Zhu
    等，[2020](#bib.bib52)）。值得注意的是，同质性对结构化数据整合到 LLMs 中的影响仍然是一个待探索的领域。
- en: 3 When and Why Can LLMs Benefit from Structural Information?
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 何时以及为什么 LLMs 可以从结构信息中受益？
- en: 3.1 Research Questions
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 研究问题
- en: In this section, we aim to gain a deeper understanding of two central questions.
    Firstly, under what circumstances can LLMs benefit from structural information
    inherent in the data (the “when” question)? Furthermore, what factors can be attributed
    to LLMs’s performance (the “why” question)? To ground our study, we experiment
    with the ChatGPT API on node classification datasets that have textual node features.
    We also decompose the questions into hypotheses of finer granularity, as described
    below.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们旨在深入理解两个核心问题。首先，在什么情况下LLM可以从数据中固有的结构信息中受益（即“何时”问题）？此外，哪些因素可以归因于LLM的性能（即“为什么”问题）？为了奠定我们的研究基础，我们在具有文本节点特征的节点分类数据集上使用ChatGPT
    API进行实验。我们还将这些问题细化为更细粒度的假设，如下所述。
- en: The when question.
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: “何时”问题。
- en: 'We hypothesize that the usefulness of structural information for LLMs on a
    node classification task depends on 1) the prompting methods used to encode the
    structural information; and 2) the richness of the textual information of each
    target node. To this end, we explore a variety of prompting methods under two
    distinct settings, one with *rich textual context* and another with *scarce textual
    context*. The detailed experimental design and results are discussed in Section [3.2](#S3.SS2
    "3.2 Influence of Structural Information on LLMs Under Varying Textual Contexts
    ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设，LLM在节点分类任务中结构信息的有效性取决于 1) 用于编码结构信息的提示方法；以及 2) 每个目标节点的文本信息的丰富程度。为此，我们在两种不同的设置下探索了各种提示方法，一种具有*丰富的文本上下文*，另一种具有*稀少的文本上下文*。详细的实验设计和结果讨论见第[3.2节](#S3.SS2
    "3.2 结构信息对LLM在不同文本上下文中的影响 ‣ 3 何时以及为什么LLM可以从结构信息中受益？ ‣ LLM能否有效利用图结构信息：何时以及为什么")。
- en: The why question.
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: “为什么”问题。
- en: 'Motivated by existing literature in LLM evaluation and graph learning, we hypothesize
    that *data leakage* and *homophily* are two potential contributing factors to
    the LLM performance on node classification tasks. While the latter is acceptable
    and even desirable, the former is not. We investigate the potential impact of
    data leakage in Section [3.3](#S3.SS3 "3.3 Data Leakage as a Potential Contributor
    of Performance ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why").
    In Section [3.4](#S3.SS4 "3.4 Impact of Homophily on LLMs Classification Accuracy
    ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why"), we examine the role of
    homophily in the performance of LLMs augmented with structural information.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 受到现有LLM评估和图学习文献的启发，我们假设*数据泄露*和*同质性*是影响LLM在节点分类任务上表现的两个潜在因素。虽然后者是可接受的，甚至是期望的，但前者则不是。我们在第[3.3节](#S3.SS3
    "3.3 数据泄露作为性能潜在贡献因素 ‣ 3 何时以及为什么LLM可以从结构信息中受益？ ‣ LLM能否有效利用图结构信息：何时以及为什么")中探讨了数据泄露的潜在影响。在第[3.4节](#S3.SS4
    "3.4 同质性对LLM分类准确度的影响 ‣ 3 何时以及为什么LLM可以从结构信息中受益？ ‣ LLM能否有效利用图结构信息：何时以及为什么")中，我们考察了同质性在增强了结构信息的LLM性能中的作用。
- en: 3.2 Influence of Structural Information on LLMs Under Varying Textual Contexts
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 结构信息对LLM在不同文本上下文中的影响
- en: 'We study the impact of structural information on LLM predictions across four
    node classification benchmark datasets with textual node features: cora (McCallum
    et al., [2000](#bib.bib29); Lu & Getoor, [2003](#bib.bib26); Sen et al., [2008](#bib.bib39);
    Yang et al., [2016](#bib.bib48)), pubmed (Namata et al., [2012](#bib.bib33); Yang
    et al., [2016](#bib.bib48)), ogbn-arxiv (Hu et al., [2020](#bib.bib16)) and ogbn-product (Hu
    et al., [2020](#bib.bib16))²²2Please see Appendix [B.1](#A2.SS1 "B.1 Datasets
    Statistics and Splits ‣ Appendix B Datasets Information ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why") for the details of the datasets..
    We create prompts that encode both the textual features and the local graph structure
    of a target node in natural language, and then request ChatGPT API³³3We have used
    gpt-3.5-turbo for throughout the experiments. to make predictions for the target
    node. The prompt for each node is formulated in one of several styles, as we introduce
    in details below. Additionally, a fixed dataset-level instruction is attached
    to the prompt when the prompt is sent to the ChatGPT API. The dataset-level instructions
    are listed in Table [6](#A1.T6 "Table 6 ‣ Appendix A Details about Prompting Format
    and Settings ‣ Can LLMs Effectively Leverage Graph Structural Information: When
    and Why") in Appendix [A](#A1 "Appendix A Details about Prompting Format and Settings
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了结构信息对LLM预测的影响，使用了四个节点分类基准数据集，这些数据集具有文本节点特征：cora (McCallum et al., [2000](#bib.bib29);
    Lu & Getoor, [2003](#bib.bib26); Sen et al., [2008](#bib.bib39); Yang et al.,
    [2016](#bib.bib48))，pubmed (Namata et al., [2012](#bib.bib33); Yang et al., [2016](#bib.bib48))，ogbn-arxiv
    (Hu et al., [2020](#bib.bib16)) 和 ogbn-product (Hu et al., [2020](#bib.bib16))²²2详细数据集信息请见附录
    [B.1](#A2.SS1 "B.1 数据集统计和拆分 ‣ 附录 B 数据集信息 ‣ LLMs 能有效利用图结构信息吗：何时及为何")。我们创建了编码目标节点文本特征和局部图结构的自然语言提示，并请求
    ChatGPT API³³3我们在整个实验中使用了 gpt-3.5-turbo。对目标节点进行预测。每个节点的提示以多种风格之一进行制定，具体如下所述。此外，当将提示发送给
    ChatGPT API 时，附加了固定的数据集级指令。这些数据集级指令列在附录 [A](#A1 "附录 A 提示格式和设置的详细信息 ‣ LLMs 能有效利用图结构信息吗：何时及为何")
    的表 [6](#A1.T6 "表 6 ‣ 附录 A 提示格式和设置的详细信息 ‣ LLMs 能有效利用图结构信息吗：何时及为何") 中。
- en: Prompt styles.
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 提示风格。
- en: 'Here we introduce the design of prompt styles in our experiments. The exact
    prompt templates can be found in Table [1](#S3.T1 "Table 1 ‣ Prompt styles. ‣
    3.2 Influence of Structural Information on LLMs Under Varying Textual Contexts
    ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why").'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们介绍了实验中的提示风格设计。具体的提示模板可以在表 [1](#S3.T1 "表 1 ‣ 提示风格 ‣ 3.2 结构信息在不同文本上下文中的对LLMs的影响
    ‣ 3 何时及为何LLMs可以受益于结构信息？ ‣ LLMs 能有效利用图结构信息吗：何时及为何") 中找到。
- en: We first have a few prompt styles that do not encode structural information.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有几种不编码结构信息的提示风格。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Zero-shot: LLMs make zero-shot predictions based on the target node’s textual
    features only.'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零-shot：LLMs 仅基于目标节点的文本特征进行零-shot 预测。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Few-shot: LLMs make predictions on nodes’ textual features only but with few-shot
    examples from the training set.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Few-shot：LLMs 仅基于节点的文本特征进行预测，但提供了来自训练集的 few-shot 示例。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Zero-shot Chain-of-Thought (CoT): Adding “Let’s think step by step” to the
    end of the zero-shot prompt (Kojima et al., [2022](#bib.bib21)). This simple change
    has been shown to boost LLMs’ performance on various tasks comparable to CoT prompts (Wei
    et al., [2022](#bib.bib46)).'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 零-shot 连锁思维 (CoT)：在零-shot 提示的末尾添加“让我们一步一步思考”（Kojima et al., [2022](#bib.bib21)）。这一简单的改变已经被证明能提高
    LLMs 在各种任务上的表现，与 CoT 提示相当（Wei et al., [2022](#bib.bib46)）。
- en: 'Then we have two strategies for prompt design conceptually inspired by MPNNs,
    where information from neighboring nodes is aggregated to enhance the representation
    of the target node:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们有两个概念上受 MPNNs 启发的提示设计策略，其中从邻近节点汇总的信息用于增强目标节点的表示：
- en: 'The first strategy incorporates randomly selected neighbors into the prompt.
    The idea behind this strategy is to aggregate information from neighboring nodes,
    following the paradigms of GCN (Kipf & Welling, [2016](#bib.bib19)) and GraphSAGE
    (Hamilton et al., [2017](#bib.bib14)). The inclusion of 1-hop neighborhood information
    in the prompt can be seen as an analogous operation to a single-layer aggregation
    in GCN, where messages from direct neighbors are aggregated. Specifically, we
    have two styles:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个策略将随机选择的邻居纳入提示中。该策略的想法是从邻近节点聚合信息，遵循 GCN (Kipf & Welling, [2016](#bib.bib19))
    和 GraphSAGE (Hamilton et al., [2017](#bib.bib14)) 的范式。在提示中包含 1-跳邻域信息可以视为 GCN 中单层聚合的类似操作，其中来自直接邻居的消息被聚合。具体来说，我们有两种风格：
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '$k$-hop title: LLMs make predictions based on the target node’s textual features
    as well as titles of neighbors up to k-hop.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $k$-跳标题：LLMs 基于目标节点的文本特征以及邻居节点（最多 $k$-跳）的标题进行预测。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $k$-hop title, we include the labels for neighbors in training set or validation
    set .
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $k$-跳标题，我们包括训练集或验证集中的邻居标签。
- en: 'The second strategy is designed to weigh the influence of neighboring nodes
    during the prediction process. This strategy is inspired by Graph Attention Networks
    (GAT) (Veličković et al., [2017](#bib.bib42)), which employ attention mechanisms
    to dynamically allocate weights to neighboring nodes based on their task-specific
    importance. The strategy consists of two steps. a) Attention extraction: the LLM
    ranks neighbors based on their relevance to the target node. b) Attention prediction:
    the LLM makes predictions based on the target node and top-ranked neighbors. We
    name the whole strategy as $k$-hop attention in our experiment results.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个策略旨在在预测过程中加权邻近节点的影响。该策略受到图注意力网络 (GAT) (Veličković et al., [2017](#bib.bib42))
    的启发，GAT 使用注意力机制动态地根据邻近节点在任务中的重要性分配权重。该策略包括两个步骤。a) 注意力提取：LLM 根据邻居与目标节点的相关性对邻居进行排名。b)
    注意力预测：LLM 根据目标节点和排名靠前的邻居进行预测。在我们的实验结果中，我们将整个策略称为 $k$-跳注意力。
- en: 'Table 1: Prompt styles and their corresponding templates. For the style “$k$-hop
    attention strategy.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：提示风格及其对应的模板。对于“$k$-跳注意力策略”风格。
- en: '|  |  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '| --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Prompt Style | Prompt Template |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 提示风格 | 提示模板 |'
- en: '| Zero-shot | Abstract: \nTitle: \nDo not give any reasoning
    or logic for your answer. \nAnswer: \n\n |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 | 摘要：\n标题：\n请不要给出任何推理或逻辑。 \n答案：\n\n |'
- en: '| Zero-shot CoT | Abstract: \nTitle: \nAnswer: \n\nLet’s think
    step by step. \n |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 零样本 CoT | 摘要：\n标题：\n答案：\n\n让我们一步一步思考。 \n |'
- en: '| Few-shot | Abstract: \n… \nAnswer: \n\n\n…
    (more few-shot examples)\nAbstract: … \nAnswer: \n\n |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 少样本 | 摘要：\n… \n答案：\n\n\n…（更多少样本示例）\n摘要：… \n答案：\n\n |'
- en: '| $k$-hop title+label | Abstract: \nTitle: \nIt has following
    neighbor papers at hop 1:\nPaper 1 title: \nLabel: \n…
    (more 1-hop neighbors)\nIt has following neighbor papers at hop 2:\n… (more 2-hop
    neighbors)\nDo not give any reasoning or logic for your answer. \nAnswer: \n\n
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| $k$-跳标题+标签 | 摘要：\n标题：\n它有以下 1-跳邻居论文：\n论文 1 标题：\n标签：\n…（更多 1-跳邻居）\n它有以下 2-跳邻居论文：\n…（更多 2-跳邻居）\n请不要给出任何推理或逻辑。 \n答案：\n\n |'
- en: '| Attention extraction | The paper of interest is . Please return a
    Python list of at most  indices of the most related papers among the following
    neighbors, ordered from most related to least related. If there are fewer than
     neighbors, just rank the neighbors by relevance. The list should look like
    this: [1, 2, 3, …]\n1: \n… (more 1-hop neighbors) \n |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 注意力提取 | 相关论文是。请返回一个最多包含  个索引的 Python 列表，表示在以下邻居中与之最相关的论文，从最相关到最不相关进行排序。如果邻居少于
     个，请仅按相关性对邻居进行排名。列表应如下所示：[1, 2, 3, …]\n1: \n…（更多 1-跳邻居）\n |'
- en: '| Attention prediction | Abstract: \nTitle: \nIt has following
    important neighbors, from most related to least related:\n(more neighbors chosen
    by attention)\nDo not give any reasoning or logic for your answer. \nAnswer: \n\n
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 注意力预测 | 摘要：\n标题：\n它有以下重要邻居，从最相关到最不相关：\n（更多通过注意力选择的邻居）\n请不要给出任何推理或逻辑。
    \n答案：\n\n |'
- en: Richness of textual node features.
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 文本节点特征的丰富性。
- en: 'To examine how the richness of the textual node features affects node classification,
    we compare two different settings:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考察文本节点特征的丰富程度如何影响节点分类，我们比较了两种不同的设置：
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rich textual context. In this setting, the nodes are associated with abundant
    textual features. Specifically, in citation networks (cora, pubmed and ogbn-arxiv),
    both the paper title and abstract are associated with each node as textual features.
    In the co-purchasing network (ogbn-product), both the product title and product
    content are associated with each node as textual features. This setting is adopted
    by several prior studies (Chen et al., [2023](#bib.bib7); Ye et al., [2023](#bib.bib49);
    Guo et al., [2023](#bib.bib12); Wang et al., [2023](#bib.bib44); He et al., [2023](#bib.bib15)).
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 丰富的文本背景。在这种设置下，节点与丰富的文本特征相关联。具体而言，在引文网络（cora、pubmed和ogbn-arxiv）中，每个节点都与论文标题和摘要作为文本特征相关联。在共同购买网络（ogbn-product）中，每个节点都与产品标题和产品内容作为文本特征相关联。这种设置已被若干先前研究采用（Chen
    et al., [2023](#bib.bib7); Ye et al., [2023](#bib.bib49); Guo et al., [2023](#bib.bib12);
    Wang et al., [2023](#bib.bib44); He et al., [2023](#bib.bib15)）。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Scarce textual context. In this setting, the nodes are associated with limited
    textual features. In citation networks (cora, pubmed and ogbn-arxiv), only the
    paper title is used as textual features. In product networks (ogbn-product), only
    the product name is associated with each node as textual features. While this
    setting is less explored in the literature, it is of great practical importance
    due to the prevalence of short texts in social networks (Alsmadi & Gan, [2019](#bib.bib2)).
    Such limited textual features present challenges like feature sparseness and non-standardization,
    reducing the effectiveness of traditional methods (Song et al., [2014](#bib.bib40)).
    In such scenarios, we expect the structural information becomes more useful for
    the predictions.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 稀缺的文本背景。在这种设置下，节点与有限的文本特征相关联。在引文网络（cora、pubmed和ogbn-arxiv）中，仅使用论文标题作为文本特征。在产品网络（ogbn-product）中，每个节点仅与产品名称作为文本特征相关联。虽然这种设置在文献中较少被探索，但由于社交网络中短文本的普遍存在（Alsmadi
    & Gan, [2019](#bib.bib2)），它具有重要的实际意义。这种有限的文本特征带来了特征稀疏和非标准化等挑战，降低了传统方法的有效性（Song
    et al., [2014](#bib.bib40)）。在这种情况下，我们预期结构信息对预测变得更加有用。
- en: Experimental results.
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验结果。
- en: 'The experimental results of different prompting methods under the two settings
    with different richness of textual context are shown in Table [2](#S3.T2 "Table
    2 ‣ Experimental results. ‣ 3.2 Influence of Structural Information on LLMs Under
    Varying Textual Contexts ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why").
    We have the following observations:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '不同提示方法在两种不同文本背景丰富程度下的实验结果如表[2](#S3.T2 "Table 2 ‣ Experimental results. ‣ 3.2
    Influence of Structural Information on LLMs Under Varying Textual Contexts ‣ 3
    When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why")所示。我们有以下观察结果：'
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Incorporating structural information in prompts brings more gain when textual
    information about the target node is limited. In rich textual context, zero-shot
    predictions are very strong baselines because prompts with structural information
    yield marginal gains on ogbn-arxiv, pubmed, and ogbn-product (1.6% average increase).
    This suggests that abundant textual features often suffice for LLMs to make predictions
    even without structural information. However, in scarce textual contexts, LLMs
    gain significantly more improvement in accuracy by incorporating structural information
    compared to rich textual contexts, suggesting that structural information is more
    important when textual information is limited.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在目标节点的文本信息有限时，将结构信息纳入提示能带来更多收益。在文本背景丰富的情况下，零-shot预测是非常强的基准，因为带有结构信息的提示在ogbn-arxiv、pubmed和ogbn-product上带来的收益很小（平均增加1.6%）。这表明，丰富的文本特征通常足以让LLMs进行预测，即使没有结构信息。然而，在文本背景稀缺的情况下，LLMs通过结合结构信息在准确性上获得显著的提升，相比于文本信息丰富的背景，这表明在文本信息有限时，结构信息更加重要。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Few-shot and zero-shot CoT prompts do not yield significant performance gains.
    Sometimes, they even underperform zero-shot prompts.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 少-shot和零-shot CoT提示没有带来显著的性能提升。有时，它们甚至表现得不如零-shot提示。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In both rich and scarce textual contexts, the difference of performance between
    prompting styles that encode structural information ($k$-hop attention) is minimal.
    This underlines that the availability of textual information is a more critical
    factor of performance than the specific prompting style used.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在丰富和稀缺文本背景下，编码结构信息的提示风格（$k$-hop attention）之间的性能差异很小。这强调了文本信息的可用性是比使用的具体提示风格更为关键的性能因素。
- en: In conclusion, structural information offers more benefits for node classification
    in scarce textual contexts than in rich textual contexts. Next, we further delve
    into potential factors contributing to the performance of LLMs on node classification
    tasks.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，结构信息在稀缺文本背景下对节点分类的好处超过了丰富文本背景下。接下来，我们将进一步深入探讨影响LLMs在节点分类任务上表现的潜在因素。
- en: 'Table 2: Node classification accuracy for the ogbn-arxiv, cora, pubmed, and
    ogbn-product datasets. $\uparrow$ denotes the improvements of best prompt style
    that leverages structural information over zero-shot method. Best results are
    in bold.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：ogbn-arxiv、cora、pubmed和ogbn-product数据集的节点分类准确率。$\uparrow$ 表示利用结构信息的最佳提示风格相较于零样本方法的改进。最佳结果以**粗体**显示。
- en: '| Textual context | Prompt style | ogbn-arxiv | cora | pubmed | ogbn-product
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| Textual context | Prompt style | ogbn-arxiv | cora | pubmed | ogbn-product
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Rich | Zero-shot | 74.0 | 66.1 | 88.6 | 83.7 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| Rich | Zero-shot | 74.0 | 66.1 | 88.6 | 83.7 |'
- en: '| Few-shot | 72.9 | 65.1 | 85.0 | 83.8 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| Few-shot | 72.9 | 65.1 | 85.0 | 83.8 |'
- en: '| Zero-shot CoT | 71.8 | 56.6 | 81.9 | 80.5 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot CoT | 71.8 | 56.6 | 81.9 | 80.5 |'
- en: '| 1-hop title+label | 75.1 | 72.5 | 89.1 | 85.2 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 1-hop title+label | 75.1 | 72.5 | 89.1 | 85.2 |'
- en: '| 2-hop title+label | 74.5 | 74.7 | 89.7 | 86.2 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 2-hop title+label | 74.5 | 74.7 | 89.7 | 86.2 |'
- en: '| 1-hop attention | 74.7 | 72.5 | 88.8 | 86.2 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 1-hop attention | 74.7 | 72.5 | 88.8 | 86.2 |'
- en: '| $\uparrow$ | 1.1 | 8.6 | 1.1 | 2.5 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| $\uparrow$ | 1.1 | 8.6 | 1.1 | 2.5 |'
- en: '| Scarce | Zero-shot | 69.8 | 61.8 | 85.7 | 78.5 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Scarce | Zero-shot | 69.8 | 61.8 | 85.7 | 78.5 |'
- en: '| 1-hop title | 72.3 | 69.6 | 84.8 | 80.5 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 1-hop title | 72.3 | 69.6 | 84.8 | 80.5 |'
- en: '| 1-hop title+label | 74.3 | 73.9 | 86.4 | 85.3 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 1-hop title+label | 74.3 | 73.9 | 86.4 | 85.3 |'
- en: '| 2-hop title | 71.3 | 69.9 | 86.2 | 80.6 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 2-hop title | 71.3 | 69.9 | 86.2 | 80.6 |'
- en: '| 2-hop title+label | 74.2 | 74.5 | 86.9 | 85.4 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 2-hop title+label | 74.2 | 74.5 | 86.9 | 85.4 |'
- en: '| 1-hop attention | 71.3 | 74.7 | 85.1 | 83.9 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 1-hop attention | 71.3 | 74.7 | 85.1 | 83.9 |'
- en: '|  | $\uparrow$ | 4.5 | 12.9 | 1.2 | 6.9 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\uparrow$ | 4.5 | 12.9 | 1.2 | 6.9 |'
- en: 3.3 Data Leakage as a Potential Contributor of Performance
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据泄露作为性能的潜在因素
- en: 'While LLMs have achieved decent performance on the node classification tasks,
    there is a risk that the performance of LLMs is artificially inflated by data
    leakage. Note that most node classification benchmark datasets have a data cut-off
    at 2019 (see Table [7](#A2.T7 "Table 7 ‣ B.1 Datasets Statistics and Splits ‣
    Appendix B Datasets Information ‣ Can LLMs Effectively Leverage Graph Structural
    Information: When and Why") in Appendix [B.1](#A2.SS1 "B.1 Datasets Statistics
    and Splits ‣ Appendix B Datasets Information ‣ Can LLMs Effectively Leverage Graph
    Structural Information: When and Why")), and ChatGPT was trained on data up to
    September 2021 (OpenAI, [2023](#bib.bib35)). While the training dataest of ChatGPT
    is not publicly available, given the widespread of these datasets on the internet
    and the enormous training corpus of ChatGPT, it is reasonable to worry about the
    data leakage issue on these datasets.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管LLMs在节点分类任务上表现不错，但存在LLMs的性能可能因数据泄露而被人为夸大的风险。请注意，大多数节点分类基准数据集的数据截止时间为2019年（见附录[B.1](#A2.SS1
    "B.1 Datasets Statistics and Splits ‣ Appendix B Datasets Information ‣ Can LLMs
    Effectively Leverage Graph Structural Information: When and Why")中的表[7](#A2.T7
    "Table 7 ‣ B.1 Datasets Statistics and Splits ‣ Appendix B Datasets Information
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why")），而ChatGPT的训练数据截至到2021年9月（OpenAI，[2023](#bib.bib35)）。尽管ChatGPT的训练数据集没有公开，但考虑到这些数据集在互联网上的广泛传播和ChatGPT庞大的训练语料库，担心这些数据集上的数据泄露问题是合理的。'
- en: To this end, we curate a new node classification dataset, arxiv-2023, which
    is designed to resemble ogbn-arxiv as much as possible except that the test nodes
    are chosen as arXiv Computer Science (CS) papers published in 2023\. With the
    new dataset, we can rigorously investigate the influence of data leakage by comparing
    the LLM performance between arxiv-2023 and ogbn-arxiv.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们策划了一个新的节点分类数据集，arxiv-2023，它旨在尽可能模拟ogbn-arxiv，只不过测试节点选择的是2023年发表的arXiv计算机科学（CS）论文。通过这个新数据集，我们可以通过比较arxiv-2023和ogbn-arxiv上的LLM表现，深入研究数据泄露的影响。
- en: Dataset collection.
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集收集。
- en: 'While, ideally, we should curate the new dataset by simply extending ogbn-arxiv by
    including new papers, this is practically challenging for a couple of reasons.
    In particular, ogbn-arxiv  represents arXiv CS papers in the Microsoft Academic
    Graph (MAG) until 2019 (Hu et al., [2020](#bib.bib16)), where MAG is a heterogeneous
    graph representing scholarly communications (Wang et al., [2020](#bib.bib45)).
    Unfortunately, MAG and its APIs were retired in 2021 and no subsequent data is
    available⁴⁴4[https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/](https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/).
    Furthermore, the pipeline to collect and construct MAG is not publicly released.
    Consequently, we develop our own data collection pipeline to create arxiv-2023.
    Specifically, we first sample test nodes from arXiv CS papers published in 2023,
    and then gather papers within a 2-hop of these test nodes to create a citation
    network. More details about collection can be found in Appendix [B.2](#A2.SS2
    "B.2 Collection of arxiv-2023 ‣ Appendix B Datasets Information ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '理想情况下，我们应该通过包括新论文来简单地扩展ogbn-arxiv，以策划新的数据集，但实际操作中面临一些挑战。特别是，ogbn-arxiv代表了2019年之前在Microsoft
    Academic Graph (MAG)中的arXiv CS论文（Hu et al., [2020](#bib.bib16)），MAG是一个表示学术交流的异质图（Wang
    et al., [2020](#bib.bib45)）。不幸的是，MAG及其API在2021年已经退役，且没有后续的数据可用⁴⁴4[https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/](https://www.microsoft.com/en-us/research/project/microsoft-academic-graph/)。此外，收集和构建MAG的管道没有公开发布。因此，我们开发了自己的数据收集管道来创建arxiv-2023。具体而言，我们首先从2023年发布的arXiv
    CS论文中采样测试节点，然后在这些测试节点的2跳范围内收集论文，以创建一个引文网络。有关收集的更多细节可以在附录[B.2](#A2.SS2 "B.2 Collection
    of arxiv-2023 ‣ Appendix B Datasets Information ‣ Can LLMs Effectively Leverage
    Graph Structural Information: When and Why")中找到。'
- en: Comparison between arxiv-2023 and ogbn-arxiv.
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: arxiv-2023与ogbn-arxiv的比较。
- en: 'As can be seen in Table [3](#S3.T3 "Table 3 ‣ Comparison between arxiv-2023
    and ogbn-arxiv. ‣ 3.3 Data Leakage as a Potential Contributor of Performance ‣
    3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why"), arxiv-2023 and ogbn-arxiv share
    great similarities in their network characteristics, with consistent in-degree/out-degree
    pointing to analogous citation behaviors. arxiv-2023 shows a lower average in-degree
    in the test set, which is likely because the test papers in arxiv-2023 are new
    and have not had much time to accumulate citations. Additionally, Figure [1](#S3.F1
    "Figure 1 ‣ Comparison between arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as
    a Potential Contributor of Performance ‣ 3 When and Why Can LLMs Benefit from
    Structural Information? ‣ Can LLMs Effectively Leverage Graph Structural Information:
    When and Why") illustrates that the label distributions of the two datasets are
    comparable. A notable trend from arxiv-2023, in alignment with arXiv statistics⁵⁵5[https://info.arxiv.org/help/stats/2021_by_area/index.html](https://info.arxiv.org/help/stats/2021_by_area/index.html),
    indicates a rise in AI-related categories like ML, LG, CL, reflecting the current
    academic focus.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[3](#S3.T3 "Table 3 ‣ Comparison between arxiv-2023 and ogbn-arxiv. ‣ 3.3
    Data Leakage as a Potential Contributor of Performance ‣ 3 When and Why Can LLMs
    Benefit from Structural Information? ‣ Can LLMs Effectively Leverage Graph Structural
    Information: When and Why")所示，arxiv-2023与ogbn-arxiv在网络特征上有很大的相似性，入度/出度一致，指向类似的引文行为。arxiv-2023在测试集中的平均入度较低，这可能是因为arxiv-2023中的测试论文较新，尚未积累很多引文。此外，图[1](#S3.F1
    "Figure 1 ‣ Comparison between arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as
    a Potential Contributor of Performance ‣ 3 When and Why Can LLMs Benefit from
    Structural Information? ‣ Can LLMs Effectively Leverage Graph Structural Information:
    When and Why")显示了两个数据集的标签分布是可比的。从arxiv-2023中可以看出一个显著的趋势，与arXiv统计数据⁵⁵5[https://info.arxiv.org/help/stats/2021_by_area/index.html](https://info.arxiv.org/help/stats/2021_by_area/index.html)一致，表明AI相关类别如ML、LG、CL有所上升，反映了当前的学术关注点。'
- en: 'Furthermore, we compare the performance of MPNNs on the two datasets. As can
    be seen from the two bottom rows in Table [4](#S3.T4 "Table 4 ‣ LLM performance
    on arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as a Potential Contributor of
    Performance ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can
    LLMs Effectively Leverage Graph Structural Information: When and Why"), we observe
    that the performance metrics for MPNNs (GCN and SAGE) across both datasets are
    closely matched, suggesting that both datasets present comparable challenges for
    classification. For a more comprehensive setting of MPNNs, one can refer to Appendix [C](#A3
    "Appendix C MPNNs as Baselines ‣ Can LLMs Effectively Leverage Graph Structural
    Information: When and Why").'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还比较了 MPNNs 在两个数据集上的表现。从表 [4](#S3.T4 "表 4 ‣ LLM 在 arxiv-2023 和 ogbn-arxiv
    上的表现 ‣ 3.3 数据泄漏作为性能的潜在因素 ‣ 3 LLM 何时以及为何可以从结构信息中受益？ ‣ LLM 能否有效利用图结构信息：何时以及为何")
    的两行底部可以看出，我们观察到 MPNNs（GCN 和 SAGE）在两个数据集上的表现指标非常接近，这表明两个数据集在分类方面提供了类似的挑战。有关 MPNNs
    更全面的设置，可以参考附录 [C](#A3 "附录 C MPNNs 作为基线 ‣ LLM 能否有效利用图结构信息：何时以及为何")。
- en: 'Table 3: Statistics of ogbn-arxiv and arxiv-2023 datasets. Both represent directed
    citation networks where each node corresponds to a paper published on arXiv and
    each edge indicates one paper citing another. The metrics In-Degree/Out-Degree,
    Average Degree, and Published Year are presented for test nodes.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：ogbn-arxiv 和 arxiv-2023 数据集的统计信息。两者均表示有向引用网络，其中每个节点对应一篇在 arXiv 上发表的论文，每条边表示一篇论文引用了另一篇。提供了测试节点的入度/出度、平均度和发表年份等指标。
- en: '|  | Full Dataset | Test Set |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整数据集 | 测试集 |'
- en: '| Dataset | #Nodes | #Edges | In-Degree/Out-Degree | Average Degree | Published
    Year |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 节点数量 | 边数量 | 入度/出度 | 平均度 | 发表年份 |'
- en: '| ogbn-arxiv | 169343 | 1166243 | 1.33/11.1 | 12.43 | 2019 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | 169343 | 1166243 | 1.33/11.1 | 12.43 | 2019 |'
- en: '| arxiv-2023 | 33868 | 305672 | 0.16/10.6 | 10.76 | 2023 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| arxiv-2023 | 33868 | 305672 | 0.16/10.6 | 10.76 | 2023 |'
- en: '![Refer to caption](img/38be075a75b79c7e66e888f3a343439b.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/38be075a75b79c7e66e888f3a343439b.png)'
- en: 'Figure 1: Proportional distribution of labels in ogbn-arxiv and arxiv-2023 datasets.
    Each label represents an arXiv Computer Science Category.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：ogbn-arxiv 和 arxiv-2023 数据集中标签的按比例分布。每个标签代表一个 arXiv 计算机科学类别。
- en: LLM performance on arxiv-2023 and ogbn-arxiv.
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 在 arxiv-2023 和 ogbn-arxiv 上的表现。
- en: 'If data leakage is a major contributor of performance on ogbn-arxiv, we would
    expect prompting methods based on LLMs to perform worse than MPNNs on arxiv-2023.
    This is because LLMs may benefit from their memory on ogbn-arxiv, but this advantage
    is not likely on arxiv-2023. However, as shown in Table [4](#S3.T4 "Table 4 ‣
    LLM performance on arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as a Potential
    Contributor of Performance ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why"),
    our findings indicate that prompting methods actually exceed MPNNs by 3% in rich
    textual contexts for arxiv-2023, and their accuracy in scarce textual contexts
    is nearly the same. This observation disproves our initial hypothesis that data
    leakage is a significant contributing factor to the LLM performance.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据泄漏是 ogbn-arxiv 上性能的主要因素，我们会期望基于 LLM 的提示方法在 arxiv-2023 上表现不如 MPNNs。这是因为 LLM
    可能会从其在 ogbn-arxiv 上的记忆中受益，但这种优势在 arxiv-2023 上不太可能存在。然而，正如表 [4](#S3.T4 "表 4 ‣ LLM
    在 arxiv-2023 和 ogbn-arxiv 上的表现 ‣ 3.3 数据泄漏作为性能的潜在因素 ‣ 3 LLM 何时以及为何可以从结构信息中受益？ ‣
    LLM 能否有效利用图结构信息：何时以及为何") 所示，我们的发现表明，提示方法在 arxiv-2023 上的丰富文本环境中实际超越了 MPNNs 3%，而在稀缺文本环境中的准确性几乎相同。这一观察结果推翻了我们最初的假设，即数据泄漏是
    LLM 性能的重要因素。
- en: To conclude, the observed results neither offer clear evidence in favor of data
    leakage nor does it advocate that data leakage predominantly improves LLM’s performance.
    Instead, LLM’s consistent performance across both datasets stresses its resilience
    and ability to generalize across varying distribution domains.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，观察到的结果既没有明确证据支持数据泄漏，也没有表明数据泄漏主要提高了 LLM 的性能。相反，LLM 在两个数据集上的一致表现强调了其在不同分布领域中的弹性和泛化能力。
- en: 'Table 4: Comparison between LLM’s performance on ogbn-arxiv and arxiv-2023.
    Best results in prompting methods are in bold. 1-hop attention means attention
    extraction and prediction over 1-hop neighbors'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：LLM 在 ogbn-arxiv 和 arxiv-2023 上的表现比较。提示方法中的最佳结果以粗体显示。1-hop attention 意味着对
    1-hop 邻居的注意力提取和预测。
- en: '| Rich context |  |  | Scarce context |  |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 丰富的上下文 |  |  | 稀缺的上下文 |  |  |'
- en: '| Prompt style | ogbn-arxiv | arxiv-2023 | Prompt style | ogbn-arxiv | arxiv-2023
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 提示样式 | ogbn-arxiv | arxiv-2023 | 提示样式 | ogbn-arxiv | arxiv-2023 |'
- en: '| Zero-shot | 74.0 | 73.5 | Zero-shot | 69.8 | 66.6 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot | 74.0 | 73.5 | Zero-shot | 69.8 | 66.6 |'
- en: '| Few-shot | 72.9 | 73.6 | 1-hop title | 72.3 | 70.7 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 少量样本 | 72.9 | 73.6 | 1-hop 标题 | 72.3 | 70.7 |'
- en: '| Zero-shot CoT | 71.8 | 73.7 | 1-hop title+label | 74.3 | 70.4 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot CoT | 71.8 | 73.7 | 1-hop 标题+标签 | 74.3 | 70.4 |'
- en: '| 1-hop title+label | 75.1 | 73.8 | 2-hop title | 71.3 | 68.9 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 1-hop 标题+标签 | 75.1 | 73.8 | 2-hop 标题 | 71.3 | 68.9 |'
- en: '| 2-hop title+label | 74.5 | 73.2 | 2-hop title+label | 74.2 | 68.5 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 2-hop 标题+标签 | 74.5 | 73.2 | 2-hop 标题+标签 | 74.2 | 68.5 |'
- en: '| 1-hop attention | 74.7 | 73.7 | 1-hop attention | 71.3 | 69.6 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 1-hop 注意力 | 74.7 | 73.7 | 1-hop 注意力 | 71.3 | 69.6 |'
- en: '| GCN | 75.4 | 70.3 | GCN | 74.8 | 70.3 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GCN | 75.4 | 70.3 | GCN | 74.8 | 70.3 |'
- en: '| SAGE | 75.0 | 70.9 | SAGE | 74.4 | 69.1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| SAGE | 75.0 | 70.9 | SAGE | 74.4 | 69.1 |'
- en: 3.4 Impact of Homophily on LLMs Classification Accuracy
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 同质性对 LLMs 分类准确度的影响
- en: '![Refer to caption](img/dc26abebc85aacb116a97925b30aab2e.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dc26abebc85aacb116a97925b30aab2e.png)'
- en: 'Figure 2: Performance comparison of dropping neighbors using different strategies
    across arxiv-2023, cora, and ogbn-product datasets. Three dropping strategies
    are evaluated: “drop same” removes neighbors with the same label as the target
    node, “drop different” removes neighbors with different labels as the target node,
    and “drop random” randomly selects neighbors for removal. When percentage is $1$,
    “drop same” strategy drops all same-label neighbors but preserves all different-label
    neighbors, and “drop different” strategy drops all different-label neighbors but
    preserves all same-label neighbors. Details about the strategies are in Appendix [D](#A4.SS0.SSS0.Px1
    "Details about dropping experiments. ‣ Appendix D Additional Analysis for Data
    Leakage ‣ Can LLMs Effectively Leverage Graph Structural Information: When and
    Why")'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：在 arxiv-2023、cora 和 ogbn-product 数据集上使用不同策略删除邻居的性能比较。评估了三种删除策略：“删除相同”移除与目标节点具有相同标签的邻居，“删除不同”移除与目标节点具有不同标签的邻居，而“随机删除”则随机选择邻居进行移除。当百分比为
    $1$ 时，“删除相同”策略删除所有相同标签的邻居，但保留所有不同标签的邻居，而“删除不同”策略删除所有不同标签的邻居，但保留所有相同标签的邻居。有关策略的详细信息见附录
    [D](#A4.SS0.SSS0.Px1 "删除实验的详细信息。 ‣ 附录 D 数据泄漏的附加分析 ‣ LLMs 是否能有效利用图结构信息：何时以及为什么")
- en: Homophily, the tendency of nodes with similar characteristics to connect, is
    foundational for many MPNNs. In fact, the degree of homophily in a dataset often
    correlates with the efficacy of MPNNs in classification tasks (Zhu et al., [2020](#bib.bib52);
    [2021](#bib.bib53); Lim et al., [2021](#bib.bib24); Maurya et al., [2021](#bib.bib28)).
    Given this significance, it becomes imperative to explore if and how homophily
    impacts the efficacy of LLMs in similar classification contexts, drawing potential
    parallels or contrasts with MPNN behaviors.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 同质性，即具有相似特征的节点连接的倾向，是许多 MPNNs 的基础。实际上，数据集中的同质性程度通常与 MPNNs 在分类任务中的有效性相关（Zhu 等，[2020](#bib.bib52)；[2021](#bib.bib53)；Lim
    等，[2021](#bib.bib24)；Maurya 等，[2021](#bib.bib28)）。鉴于此重要性，有必要探讨同质性是否以及如何影响 LLMs
    在类似分类背景下的有效性，并与 MPNN 行为进行潜在的类比或对比。
- en: Since LLM performs node-wise prediction over the neighborhood surrounding the
    target node, we use local homophily ratio (Loveland et al., [2023](#bib.bib25))
    to measure the degree of homophily with respect to the target node. For a prompt
    to predict the category of a target node, the local homophily ratio is defined
    as the fraction of neighbors sharing the same groundtruth label as the target
    node over the total number of neighbors included in the prompt. Intuitively, a
    higher local homophily ratio signals scenarios where a node is surrounded by a
    greater proportion of neighbors from the same category.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 LLM 在目标节点周围的邻域上执行节点级预测，我们使用局部同质性比率（Loveland 等，[2023](#bib.bib25)）来衡量相对于目标节点的同质性程度。为了让提示预测目标节点的类别，局部同质性比率被定义为与目标节点共享相同真实标签的邻居占提示中包含的所有邻居的比例。从直观上看，更高的局部同质性比率表示一个节点被相同类别的邻居所包围的比例更高。
- en: The neighbor dropping experiment.
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 邻居删除实验。
- en: 'We design a controlled experiment to demonstrate the effect of local homophily
    ratio on prediction accuracy. We gradually drop neighbors in three different ways:
    a) drop the neighbors with same label as the target node; b) drop the neighbors
    with different label as the target node; and c) drop neighbors randomly. We include
    details about the neighbor dropping strategies in Appendix [D](#A4.SS0.SSS0.Px1
    "Details about dropping experiments. ‣ Appendix D Additional Analysis for Data
    Leakage ‣ Can LLMs Effectively Leverage Graph Structural Information: When and
    Why"). The experimental results are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.4
    Impact of Homophily on LLMs Classification Accuracy ‣ 3 When and Why Can LLMs
    Benefit from Structural Information? ‣ Can LLMs Effectively Leverage Graph Structural
    Information: When and Why"), where we observe an evident trend: as we selectively
    remove neighbors sharing the same labels, there’s a decrease in prediction accuracy.
    Conversely, discarding neighbors with different labels leads to an increase in
    accuracy. This selective dropping inherently modifies the local homophily ratio
    within the prompts. The results show that accuracy of predictions made by LLMs
    is positively related to local homophily ratio.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '我们设计了一个受控实验以展示本地同质性比例对预测准确性的影响。我们以三种不同方式逐步丢弃邻居：a) 丢弃与目标节点标签相同的邻居；b) 丢弃与目标节点标签不同的邻居；c)
    随机丢弃邻居。邻居丢弃策略的详细信息见附录[D](#A4.SS0.SSS0.Px1 "Details about dropping experiments.
    ‣ Appendix D Additional Analysis for Data Leakage ‣ Can LLMs Effectively Leverage
    Graph Structural Information: When and Why")。实验结果如图[2](#S3.F2 "Figure 2 ‣ 3.4
    Impact of Homophily on LLMs Classification Accuracy ‣ 3 When and Why Can LLMs
    Benefit from Structural Information? ‣ Can LLMs Effectively Leverage Graph Structural
    Information: When and Why")所示，我们观察到一个明显的趋势：当我们选择性地移除具有相同标签的邻居时，预测准确性降低。相反，丢弃不同标签的邻居会导致准确性提高。这种选择性丢弃本质上改变了提示中的本地同质性比例。结果表明，LLMs的预测准确性与本地同质性比例正相关。'
- en: Correlation study.
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 相关性研究。
- en: 'Table 5: Point biserial correlation between local homophily ratio and prediction
    correctness across five datasets (p-values in brackets). Point biserial correlation
    ranges between $[-1,1]$, where a value of 1 indicates a perfect positive relationship.
    A higher correlation value indicates that the local homophily ratio and prediction
    correctness are more positively related.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：五个数据集间本地同质性比例与预测正确性的点二列相关性（括号中为p值）。点二列相关性的范围为$[-1,1]$，其中值为1表示完全正相关。较高的相关性值表明本地同质性比例与预测正确性之间的关系更为正向。
- en: '| Prompt Style | ogbn-arxiv | cora | pubmed | arxiv-2023 | ogbn-product |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 提示风格 | ogbn-arxiv | cora | pubmed | arxiv-2023 | ogbn-product |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Zero-shot | 0.440 (0.000) | 0.070 (0.106) | 0.278 (0.000) | 0.367 (0.000)
    | 0.387 (0.000) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Zero-shot | 0.440 (0.000) | 0.070 (0.106) | 0.278 (0.000) | 0.367 (0.000)
    | 0.387 (0.000) |'
- en: '| 1-hop title+label | 0.518 (0.000) | 0.222 (0.000) | 0.443 (0.000) | 0.481
    (0.000) | 0.560 (0.000) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 1-hop title+label | 0.518 (0.000) | 0.222 (0.000) | 0.443 (0.000) | 0.481
    (0.000) | 0.560 (0.000) |'
- en: 'Building on the insights from the dropping neighbors experiment, we further
    investigate the relationship between local homophily ratio and the prediction
    correctness across different datasets. Each node possesses two key attributes:
    a) its local homophily ratio, which is a continuous random variable in $[0,1]$,
    and b) its prediction correctness, which is a binary random variable (0 indicating
    an incorrect prediction and 1 indicating a correct prediction). To quantify the
    correlation between these two attributes, we employ the point biserial correlation
    method (Kornbrot, [2014](#bib.bib22)). This correlation coefficient ranges between
    -1 and 1, where a value of 1 signifies a perfect positive relationship. The results
    of our analysis across five datasets are detailed in Table [5](#S3.T5 "Table 5
    ‣ Correlation study. ‣ 3.4 Impact of Homophily on LLMs Classification Accuracy
    ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can LLMs Effectively
    Leverage Graph Structural Information: When and Why").'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '基于邻居丢弃实验的见解，我们进一步探讨了不同数据集中本地同质性比例与预测正确性之间的关系。每个节点具有两个关键属性：a) 本地同质性比例，这是一个$[0,1]$范围内的连续随机变量；b)
    预测正确性，这是一个二元随机变量（0表示预测错误，1表示预测正确）。为了量化这两个属性之间的相关性，我们采用点二列相关性方法 (Kornbrot, [2014](#bib.bib22))。该相关系数的范围为-1到1，其中值为1表示完全正相关。我们在五个数据集上的分析结果详细列在表[5](#S3.T5
    "Table 5 ‣ Correlation study. ‣ 3.4 Impact of Homophily on LLMs Classification
    Accuracy ‣ 3 When and Why Can LLMs Benefit from Structural Information? ‣ Can
    LLMs Effectively Leverage Graph Structural Information: When and Why")中。'
- en: For the cora dataset, we observe no significant correlation when only the title
    is used in prompts. However, a positive correlation emerges when neighbors are
    included alongside the title. This suggests that the more homophily is incorporated
    into the prompt, the more accurate the prediction becomes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 cora 数据集，当仅使用标题作为提示时，我们没有观察到显著的相关性。然而，当邻居信息与标题一起包含时，出现了正相关。这表明，当提示中融入更多的同质性时，预测的准确性就会提高。
- en: For the other datasets, a positive correlation is evident in both the zero-shot
    and 1-hop title+label settings. However, the strength of this positive correlation
    is amplified when neighborhood information is incorporated, highlighting the significance
    of homophily in prediction correctness. A plausible explanation for the observed
    positive relationship between local homophily ratio and prediction correctness
    in the zero-shot setting, could be the inherent difficulty in predicting nodes
    with lower homophily.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他数据集，在零样本和 1-hop 标题+标签设置下，都可以明显看到正相关。然而，当融入邻域信息时，这种正相关的强度会增强，突显了同质性在预测准确性中的重要性。在零样本设置中观察到的局部同质性比例与预测准确性之间的正关系的合理解释可能是预测具有较低同质性的节点本身具有的难度。
- en: In summary, our findings underline the critical role of homophily in influencing
    LLM’s node classification performance. The experiments and analyses consistently
    point to a positive relationship between local homophily ratio and prediction
    correctness, emphasizing the importance of understanding network structures and
    node relationships in enhancing classification outcomes.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的发现强调了同质性在影响 LLM 节点分类性能中的关键作用。实验和分析一致指出了局部同质性比例与预测准确性之间的正关系，强调了理解网络结构和节点关系在提高分类结果中的重要性。
- en: 4 Conclusions and Future Work
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论与未来工作
- en: 'This study marks an early step towards a broader research aim: enabling LLMs
    to process structured data, a crucial data modality commonly seen in practice.
    In this study, we have adapted node classfication datasets with textual features
    from graph learning benchmarks to establish a testbed for LLMs augmented with
    structured data. Our preliminary examination on prompting methods for encoding
    the structural information shows that LLMs benefit more from structural information
    when the textual features of the target node is scarce. We also delve into the
    impact of data leakage and homophily, which provides deeper insights about the
    LLM performance when augmented with graph-structured data.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究标志着向更广泛研究目标迈出的早期一步：使 LLM 能够处理结构化数据，这是一种在实际中常见的重要数据形式。在本研究中，我们将带有文本特征的节点分类数据集从图学习基准适配到用于
    LLM 的结构化数据测试平台。我们对编码结构信息的提示方法的初步检查表明，当目标节点的文本特征稀缺时，LLM 更能从结构化信息中获益。我们还*深入探讨*了数据泄露和同质性的影响，这为增强图结构数据的
    LLM 性能提供了更深层次的见解。
- en: This study also opens several avenues for future research. Firstly, the findings
    of this study, as well as the new dataset curated by this work, establish a proper
    benchmark setup for more advanced methods to encode structural information for
    LLMs, such as finetuning or adapter training. Secondly, while we find that data
    leakage is not a major concern for the prompting methods examined in this paper,
    it is still possible that more advanced methods can elicit the memory of the LLMs
    from training corpus. We may need further investigation on the data leakage issue
    when proceeding with evaluating other methods. Finally, the fact that homophily
    plays a crucial role in the performance gain of LLMs with structured data suggests
    that LLMs may be utilizing superficial correlational information to aid the prediction
    tasks. It would be interesting to further investigate whether we can make LLMs
    grasp the deeper relational structure of the graph data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究也为未来的研究打开了几条新的途径。首先，本研究的发现以及本工作整理的新数据集，为更先进的方法编码结构信息，如微调或适配器训练，建立了一个适当的基准设置。其次，尽管我们发现数据泄露在本文中检查的提示方法中不是主要问题，但更先进的方法可能会引发
    LLM 从训练语料库中提取记忆。我们可能需要进一步调查数据泄露问题，以便在评估其他方法时进行处理。最后，*同质性*在结构化数据的 LLM 性能提升中发挥了关键作用，这表明
    LLM 可能利用表面的相关信息来辅助预测任务。进一步调查是否可以使 LLM 理解图数据的更深层关系结构将是很有趣的。
- en: References
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aiyappa et al. (2023) Rachith Aiyappa, Jisun An, Haewoon Kwak, and Yong-yeol
    Ahn. Can we trust the evaluation on ChatGPT? In *Proceedings of the 3rd Workshop
    on Trustworthy Natural Language Processing (TrustNLP 2023)*, pp.  47–54, Toronto,
    Canada, July 2023\. Association for Computational Linguistics. doi: 10.18653/v1/2023.trustnlp-1.5.
    URL [https://aclanthology.org/2023.trustnlp-1.5](https://aclanthology.org/2023.trustnlp-1.5).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aiyappa 等人 (2023) Rachith Aiyappa、Jisun An、Haewoon Kwak 和 Yong-yeol Ahn。我们可以相信对
    ChatGPT 的评估吗？在 *第 3 届可信自然语言处理研讨会 (TrustNLP 2023)* 中，页码 47–54，加拿大多伦多，2023年7月。计算语言学协会。doi:
    10.18653/v1/2023.trustnlp-1.5。网址 [https://aclanthology.org/2023.trustnlp-1.5](https://aclanthology.org/2023.trustnlp-1.5)。'
- en: Alsmadi & Gan (2019) Issa Alsmadi and Keng Hoon Gan. Review of short-text classification.
    *International Journal of Web Information Systems*, 15(2):155–182, 2019.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alsmadi & Gan (2019) Issa Alsmadi 和 Keng Hoon Gan。短文本分类综述。*国际网络信息系统期刊*，15(2):155–182，2019年。
- en: 'Brohan et al. (2023) Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman,
    Alexander Herzog, Daniel Ho, Julian Ibarz, Alex Irpan, Eric Jang, Ryan Julian,
    et al. Do as i can, not as i say: Grounding language in robotic affordances. In
    *Conference on Robot Learning*, pp.  287–318\. PMLR, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brohan 等人 (2023) Anthony Brohan、Yevgen Chebotar、Chelsea Finn、Karol Hausman、Alexander
    Herzog、Daniel Ho、Julian Ibarz、Alex Irpan、Eric Jang、Ryan Julian 等人。按我所能，而非我所说：将语言基础于机器人可实现性。在
    *机器人学习会议* 中，页码 287–318。PMLR，2023年。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared
    Kaplan、Prafulla Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda
    Askell、Sandhini Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon
    Child、Aditya Ramesh、Daniel M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark
    Chen、Eric Sigler、Mateusz Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher
    Berner、Sam McCandlish、Alec Radford、Ilya Sutskever 和 Dario Amodei。语言模型是少样本学习者，2020年。
- en: Carlini et al. (2021) Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew
    Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song,
    Ulfar Erlingsson, et al. Extracting training data from large language models.
    In *30th USENIX Security Symposium (USENIX Security 21)*, pp.  2633–2650, 2021.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等人 (2021) Nicholas Carlini、Florian Tramer、Eric Wallace、Matthew Jagielski、Ariel
    Herbert-Voss、Katherine Lee、Adam Roberts、Tom Brown、Dawn Song、Ulfar Erlingsson 等人。从大型语言模型中提取训练数据。在
    *第 30 届 USENIX 安全研讨会 (USENIX Security 21)* 中，页码 2633–2650，2021年。
- en: Carlini et al. (2022) Nicholas Carlini, Daphne Ippolito, Matthew Jagielski,
    Katherine Lee, Florian Tramer, and Chiyuan Zhang. Quantifying memorization across
    neural language models. *arXiv preprint arXiv:2202.07646*, 2022.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carlini 等人 (2022) Nicholas Carlini、Daphne Ippolito、Matthew Jagielski、Katherine
    Lee、Florian Tramer 和 Chiyuan Zhang。量化神经语言模型中的记忆化。*arXiv 预印本 arXiv:2202.07646*，2022年。
- en: Chen et al. (2023) Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi
    Wei, Shuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. Exploring the potential
    of large language models (llms) in learning on graphs. *arXiv preprint arXiv:2307.03393*,
    2023.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 (2023) Zhikai Chen、Haitao Mao、Hang Li、Wei Jin、Hongzhi Wen、Xiaochi Wei、Shuaiqiang
    Wang、Dawei Yin、Wenqi Fan、Hui Liu 等人。探索大型语言模型 (llms) 在图上学习的潜力。*arXiv 预印本 arXiv:2307.03393*，2023年。
- en: Ciotti et al. (2016) Valerio Ciotti, Moreno Bonaventura, Vincenzo Nicosia, Pietro
    Panzarasa, and Vito Latora. Homophily and missing links in citation networks.
    *EPJ Data Science*, 5:1–14, 2016.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciotti 等人 (2016) Valerio Ciotti、Moreno Bonaventura、Vincenzo Nicosia、Pietro Panzarasa
    和 Vito Latora。引用网络中的同质性与缺失链接。*EPJ 数据科学*，5:1–14，2016年。
- en: Clement et al. (2019) Colin B. Clement, Matthew Bierbaum, Kevin P. O’Keeffe,
    and Alexander A. Alemi. On the use of arxiv as a dataset, 2019.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clement 等人 (2019) Colin B. Clement、Matthew Bierbaum、Kevin P. O’Keeffe 和 Alexander
    A. Alemi。关于将 arxiv 作为数据集的使用，2019年。
- en: Fey & Lenssen (2019) Matthias Fey and Jan Eric Lenssen. Fast graph representation
    learning with pytorch geometric. *arXiv preprint arXiv:1903.02428*, 2019.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fey & Lenssen (2019) Matthias Fey 和 Jan Eric Lenssen。使用 pytorch geometric 进行快速图表示学习。*arXiv
    预印本 arXiv:1903.02428*，2019年。
- en: Gilmer et al. (2017) Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley,
    Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry.
    In Doina Precup and Yee Whye Teh (eds.), *Proceedings of the 34th International
    Conference on Machine Learning*, volume 70 of *Proceedings of Machine Learning
    Research*, pp.  1263–1272\. PMLR, 06–11 Aug 2017. URL [https://proceedings.mlr.press/v70/gilmer17a.html](https://proceedings.mlr.press/v70/gilmer17a.html).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gilmer et al. (2017) 贾斯汀·吉尔默、Samuel S. Schoenholz、Patrick F. Riley、Oriol Vinyals和George
    E. Dahl. 量子化学的神经消息传递。见Doina Precup和Yee Whye Teh（编），*第34届国际机器学习大会论文集*，*机器学习研究论文集*第70卷，页1263–1272，PMLR，2017年8月6–11日。网址
    [https://proceedings.mlr.press/v70/gilmer17a.html](https://proceedings.mlr.press/v70/gilmer17a.html)。
- en: 'Guo et al. (2023) Jiayan Guo, Lun Du, and Hengyu Liu. Gpt4graph: Can large
    language models understand graph structured data? an empirical evaluation and
    benchmarking. *arXiv preprint arXiv:2305.15066*, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo et al. (2023) 郭佳彦、杜伦和刘恒宇. Gpt4graph: 大型语言模型能否理解图结构数据？实证评估与基准测试。*arXiv预印本
    arXiv:2305.15066*，2023年。'
- en: 'Halcrow et al. (2020) Jonathan Halcrow, Alexandru Mosoi, Sam Ruth, and Bryan
    Perozzi. Grale: Designing networks for graph learning. In *Proceedings of the
    26th ACM SIGKDD international conference on knowledge discovery & data mining*,
    pp.  2523–2532, 2020.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Halcrow et al. (2020) Jonathan Halcrow、Alexandru Mosoi、Sam Ruth和Bryan Perozzi.
    Grale: 设计用于图学习的网络。见*第26届ACM SIGKDD国际知识发现与数据挖掘大会论文集*，页2523–2532，2020年。'
- en: Hamilton et al. (2017) Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive
    representation learning on large graphs. *Advances in neural information processing
    systems*, 30, 2017.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton et al. (2017) Will Hamilton、Ying Zhitao和Jure Leskovec. 大图的归纳表示学习。*神经信息处理系统进展*，30，2017年。
- en: 'He et al. (2023) Xiaoxin He, Xavier Bresson, Thomas Laurent, and Bryan Hooi.
    Explanations as features: Llm-based features for text-attributed graphs. *arXiv
    preprint arXiv:2305.19523*, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2023) 贺晓欣、Xavier Bresson、Thomas Laurent和Bryan Hooi. 解释作为特征：基于LLM的文本属性图特征。*arXiv预印本
    arXiv:2305.19523*，2023年。
- en: 'Hu et al. (2020) Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu
    Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets
    for machine learning on graphs. *Advances in neural information processing systems*,
    33:22118–22133, 2020.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2020) 胡伟华、Matthias Fey、Marinka Zitnik、董玉霄、任红雨、刘博文、Michele Catasta和Jure
    Leskovec. 开放图基准：图机器学习的数据集。*神经信息处理系统进展*，33:22118–22133，2020年。
- en: Huang et al. (2022) Jie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang. Are
    large pre-trained language models leaking your personal information? *arXiv preprint
    arXiv:2205.12628*, 2022.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2022) 黄杰、邵汉银和Kevin Chen-Chuan Chang. 大型预训练语言模型是否泄露了你的个人信息？*arXiv预印本
    arXiv:2205.12628*，2022年。
- en: 'Jiang et al. (2023) Jinhao Jiang, Kun Zhou, Zican Dong, Keming Ye, Wayne Xin
    Zhao, and Ji-Rong Wen. Structgpt: A general framework for large language model
    to reason over structured data. *arXiv preprint arXiv:2305.09645*, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang et al. (2023) 姜鑫浩、周昆、董子灿、叶克铭、赵伟欣和温基荣. Structgpt: 用于结构化数据推理的通用框架。*arXiv预印本
    arXiv:2305.09645*，2023年。'
- en: Kipf & Welling (2016) Thomas N Kipf and Max Welling. Semi-supervised classification
    with graph convolutional networks. *arXiv preprint arXiv:1609.02907*, 2016.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf & Welling (2016) Thomas N Kipf和Max Welling. 图卷积网络的半监督分类。*arXiv预印本 arXiv:1609.02907*，2016年。
- en: 'Knoke (1990) David Knoke. *Political networks: the structural perspective*,
    volume 4. Cambridge University Press, 1990.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knoke (1990) David Knoke. *政治网络：结构视角*，第4卷。剑桥大学出版社，1990年。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in neural information processing systems*, 35:22199–22213, 2022.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2022) 小岛刚、Shixiang Shane Gu、Machel Reid、松尾丰和岩泽优介. 大型语言模型是零-shot推理者。*神经信息处理系统进展*，35:22199–22213，2022年。
- en: 'Kornbrot (2014) Diana Kornbrot. Point biserial correlation. *Wiley StatsRef:
    Statistics Reference Online*, 2014.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kornbrot (2014) Diana Kornbrot. 点二序列相关。*Wiley StatsRef: 统计参考在线*，2014年。'
- en: 'Li et al. (2023) KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping
    Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding.
    *arXiv preprint arXiv:2305.06355*, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) 李坤昌、何一男、王毅、李亦卓、王文海、罗平、王亚莉、王黎敏和乔宇. Videochat: 以聊天为中心的视频理解。*arXiv预印本
    arXiv:2305.06355*，2023年。'
- en: 'Lim et al. (2021) Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi
    Gupta, Omkar Bhalerao, and Ser Nam Lim. Large scale learning on non-homophilous
    graphs: New benchmarks and strong simple methods. *Advances in Neural Information
    Processing Systems*, 34:20887–20902, 2021.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lim et al. (2021) Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi
    Gupta, Omkar Bhalerao 和 Ser Nam Lim。非同质图上的大规模学习：新的基准和强大的简单方法。*神经信息处理系统进展*，34:20887–20902，2021年。
- en: Loveland et al. (2023) Donald Loveland, Jiong Zhu, Mark Heimann, Benjamin Fish,
    Michael T Shaub, and Danai Koutra. On performance discrepancies across local homophily
    levels in graph neural networks. *arXiv preprint arXiv:2306.05557*, 2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Loveland et al. (2023) Donald Loveland, Jiong Zhu, Mark Heimann, Benjamin Fish,
    Michael T Shaub 和 Danai Koutra。图神经网络中本地同质性水平的性能差异。*arXiv 预印本 arXiv:2306.05557*，2023年。
- en: Lu & Getoor (2003) Qing Lu and Lise Getoor. Link-based classification. In *International
    Conference on Machine Learning (ICML)*, Washington, DC, USA, 2003.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu & Getoor (2003) Qing Lu 和 Lise Getoor。基于链接的分类。在 *国际机器学习会议 (ICML)*，美国华盛顿特区，2003年。
- en: 'Ma et al. (2022) Jiaqi Ma, Xingjian Zhang, Hezheng Fan, Jin Huang, Tianyue
    Li, Ting Wei Li, Yiwen Tu, Chenshu Zhu, and Qiaozhu Mei. Graph learning indexer:
    A contributor-friendly and metadata-rich platform for graph learning benchmarks.
    In *Learning on Graphs Conference*, pp.  7–1\. PMLR, 2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma et al. (2022) Jiaqi Ma, Xingjian Zhang, Hezheng Fan, Jin Huang, Tianyue Li,
    Ting Wei Li, Yiwen Tu, Chenshu Zhu 和 Qiaozhu Mei。图学习索引器：一个对贡献者友好且富含元数据的图学习基准平台。在
    *Learning on Graphs Conference*，第 7–1 页。PMLR，2022年。
- en: Maurya et al. (2021) Sunil Kumar Maurya, Xin Liu, and Tsuyoshi Murata. Improving
    graph neural networks with simple architecture design. *arXiv preprint arXiv:2105.07634*,
    2021.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maurya et al. (2021) Sunil Kumar Maurya, Xin Liu 和 Tsuyoshi Murata。通过简单的架构设计改进图神经网络。*arXiv
    预印本 arXiv:2105.07634*，2021年。
- en: McCallum et al. (2000) Andrew Kachites McCallum, Kamal Nigam, Jason Rennie,
    and Kristie Seymore. Automating the construction of internet portals with machine
    learning. *Information Retrieval*, 3(2):127–163, 2000.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCallum et al. (2000) Andrew Kachites McCallum, Kamal Nigam, Jason Rennie 和
    Kristie Seymore。利用机器学习自动构建互联网门户。*信息检索*，3(2):127–163，2000年。
- en: 'McPherson et al. (2001) Miller McPherson, Lynn Smith-Lovin, and James M Cook.
    Birds of a feather: Homophily in social networks. *Annual review of sociology*,
    27(1):415–444, 2001.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McPherson et al. (2001) Miller McPherson, Lynn Smith-Lovin 和 James M Cook。物以类聚：社交网络中的同质性。*社会学年鉴*，27(1):415–444，2001年。
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. Distributed representations of words and phrases and their compositionality.
    *Advances in neural information processing systems*, 26, 2013.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado
    和 Jeff Dean。词语和短语的分布式表示及其组合性。*神经信息处理系统进展*，26，2013年。
- en: 'Miller et al. (2009) Frederic P. Miller, Agnes F. Vandome, and John McBrewster.
    *Levenshtein Distance: Information Theory, Computer Science, String (Computer
    Science), String Metric, Damerau?Levenshtein Distance, Spell Checker, Hamming
    Distance*. Alpha Press, 2009. ISBN 6130216904.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miller et al. (2009) Frederic P. Miller, Agnes F. Vandome 和 John McBrewster。*Levenshtein
    距离：信息理论、计算机科学、字符串（计算机科学）、字符串度量、Damerau-Levenshtein 距离、拼写检查器、汉明距离*。Alpha Press，2009年。ISBN
    6130216904。
- en: Namata et al. (2012) Galileo Mark Namata, Ben London, Lise Getoor, and Bert
    Huang. Query-driven active surveying for collective classification. In *International
    Workshop on Mining and Learning with Graphs (MLG)*, Edinburgh, Scotland, 2012.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Namata et al. (2012) Galileo Mark Namata, Ben London, Lise Getoor 和 Bert Huang。基于查询的主动调查以进行集体分类。在
    *图挖掘与学习国际研讨会 (MLG)*，英国爱丁堡，2012年。
- en: OpenAI (2022) OpenAI. Introducing chatgpt, 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI。介绍 ChatGPT，2022年。网址 [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。GPT-4 技术报告，2023年。
- en: 'Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang,
    and Xindong Wu. Unifying large language models and knowledge graphs: A roadmap.
    *arXiv preprint arXiv:2306.08302*, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan et al. (2023) Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang
    和 Xindong Wu。统一大型语言模型和知识图谱：路线图。*arXiv 预印本 arXiv:2306.08302*，2023年。
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, 等人。语言模型是无监督的多任务学习者。*OpenAI博客*，1(8):9，2019年。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pp.  8748–8763\. PMLR, 2021.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2021) **Alec Radford**、**Jong Wook Kim**、**Chris Hallacy**、**Aditya
    Ramesh**、**Gabriel Goh**、**Sandhini Agarwal**、**Girish Sastry**、**Amanda Askell**、**Pamela
    Mishkin**、**Jack Clark** 等. 从自然语言监督中学习可转移的视觉模型。 在 *国际机器学习会议*，第8748–8763页。PMLR，2021年。
- en: Sen et al. (2008) Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor,
    Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data.
    *AI magazine*, 29(3):93–93, 2008.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sen et al. (2008) **Prithviraj Sen**、**Galileo Namata**、**Mustafa Bilgic**、**Lise
    Getoor**、**Brian Galligher** 和 **Tina Eliassi-Rad**. 网络数据中的集体分类。 *人工智能杂志*，29(3):93–93，2008年。
- en: 'Song et al. (2014) Ge Song, Yunming Ye, Xiaolin Du, Xiaohui Huang, and Shifu
    Bie. Short text classification: a survey. *Journal of multimedia*, 9(5), 2014.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. (2014) **Ge Song**、**Yunming Ye**、**Xiaolin Du**、**Xiaohui Huang**
    和 **Shifu Bie**. 短文本分类：综述。 *多媒体期刊*，9(5)，2014年。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) **Hugo Touvron**、**Thibaut Lavril**、**Gautier Izacard**、**Xavier
    Martinet**、**Marie-Anne Lachaux**、**Timothée Lacroix**、**Baptiste Rozière**、**Naman
    Goyal**、**Eric Hambro**、**Faisal Azhar** 等. Llama：开放且高效的基础语言模型。 *arXiv 预印本 arXiv:2302.13971*，2023年。
- en: Veličković et al. (2017) Petar Veličković, Guillem Cucurull, Arantxa Casanova,
    Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. *arXiv
    preprint arXiv:1710.10903*, 2017.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Veličković et al. (2017) **Petar Veličković**、**Guillem Cucurull**、**Arantxa
    Casanova**、**Adriana Romero**、**Pietro Lio** 和 **Yoshua Bengio**. 图注意力网络。 *arXiv
    预印本 arXiv:1710.10903*，2017年。
- en: 'Wang & Komatsuzaki (2021) Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion
    Parameter Autoregressive Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax),
    May 2021.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang & Komatsuzaki (2021) **Ben Wang** 和 **Aran Komatsuzaki**. GPT-J-6B：一个60亿参数的自回归语言模型。
    [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)，2021年5月。
- en: Wang et al. (2023) Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang
    Han, and Yulia Tsvetkov. Can language models solve graph problems in natural language?
    *arXiv preprint arXiv:2305.10037*, 2023.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) **Heng Wang**、**Shangbin Feng**、**Tianxing He**、**Zhaoxuan
    Tan**、**Xiaochuang Han** 和 **Yulia Tsvetkov**. 语言模型能否解决自然语言中的图问题？ *arXiv 预印本 arXiv:2305.10037*，2023年。
- en: 'Wang et al. (2020) Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu,
    Yuxiao Dong, and Anshul Kanakia. Microsoft academic graph: When experts are not
    enough. *Quantitative Science Studies*, 1(1):396–413, 2020.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) **Kuansan Wang**、**Zhihong Shen**、**Chiyuan Huang**、**Chieh-Han
    Wu**、**Yuxiao Dong** 和 **Anshul Kanakia**. 微软学术图谱：当专家不够时。 *定量科学研究*，1(1):396–413，2020年。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) **Jason Wei**、**Xuezhi Wang**、**Dale Schuurmans**、**Maarten
    Bosma**、**Fei Xia**、**Ed Chi**、**Quoc V Le**、**Denny Zhou** 等. 思维链提示在大型语言模型中引发推理。
    *神经信息处理系统进展*，35:24824–24837，2022年。
- en: 'Yang et al. (2023) Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan
    Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react:
    Prompting chatgpt for multimodal reasoning and action. *arXiv preprint arXiv:2303.11381*,
    2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) **Zhengyuan Yang**、**Linjie Li**、**Jianfeng Wang**、**Kevin
    Lin**、**Ehsan Azarnasab**、**Faisal Ahmed**、**Zicheng Liu**、**Ce Liu**、**Michael
    Zeng** 和 **Lijuan Wang**. Mm-react：提示 ChatGPT 进行多模态推理和行动。 *arXiv 预印本 arXiv:2303.11381*，2023年。
- en: Yang et al. (2016) Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting
    semi-supervised learning with graph embeddings. In *International conference on
    machine learning*, pp.  40–48\. PMLR, 2016.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2016) **Zhilin Yang**、**William Cohen** 和 **Ruslan Salakhudinov**.
    重新审视带有图嵌入的半监督学习。 在 *国际机器学习会议*，第40–48页。PMLR，2016年。
- en: Ye et al. (2023) Ruosong Ye, Caiqi Zhang, Runhui Wang, Shuyuan Xu, and Yongfeng
    Zhang. Natural language is all a graph needs. *arXiv preprint arXiv:2308.07134*,
    2023.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. (2023) **Ruosong Ye**、**Caiqi Zhang**、**Runhui Wang**、**Shuyuan Xu**
    和 **Yongfeng Zhang**. 自然语言就是图所需的一切。 *arXiv 预印本 arXiv:2308.07134*，2023年。
- en: Yin et al. (2023) Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. A survey on multimodal large language models. *arXiv preprint
    arXiv:2306.13549*, 2023.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin et al. (2023) **Shukang Yin**、**Chaoyou Fu**、**Sirui Zhao**、**Ke Li**、**Xing
    Sun**、**Tong Xu** 和 **Enhong Chen**. 关于多模态大型语言模型的调查。 *arXiv 预印本 arXiv:2306.13549*，2023年。
- en: 'Zhang (2023) Jiawei Zhang. Graph-toolformer: To empower llms with graph reasoning
    ability via prompt augmented by chatgpt. *arXiv preprint arXiv:2304.11116*, 2023.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang（2023）Jiawei Zhang. 图工具转换器：通过 chatgpt 增强的提示赋能 llms 以进行图推理能力。*arXiv 预印本
    arXiv:2304.11116*，2023。
- en: 'Zhu et al. (2020) Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman
    Akoglu, and Danai Koutra. Beyond homophily in graph neural networks: Current limitations
    and effective designs. *Advances in neural information processing systems*, 33:7793–7804,
    2020.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2020）Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu,
    和 Danai Koutra. 图神经网络中的超越同质性：当前限制与有效设计。*神经信息处理系统进展*，33:7793–7804，2020。
- en: Zhu et al. (2021) Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka,
    Nesreen K Ahmed, and Danai Koutra. Graph neural networks with heterophily. In
    *Proceedings of the AAAI conference on artificial intelligence*, volume 35, pp. 
    11168–11176, 2021.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2021）Jiong Zhu, Ryan A Rossi, Anup Rao, Tung Mai, Nedim Lipka, Nesreen
    K Ahmed, 和 Danai Koutra. 具有异质性的图神经网络。见 *AAAI 人工智能会议论文集*，第 35 卷，页码 11168–11176，2021。
- en: Appendix A Details about Prompting Format and Settings
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 关于提示格式和设置的详细信息
- en: 'Our API call to ChatGPT utilize a two-part prompt structure, in line with the
    ChatGPT Chat Completions API⁶⁶6[https://platform.openai.com/docs/guides/gpt/chat-completions-api](https://platform.openai.com/docs/guides/gpt/chat-completions-api).
    Each API call involves a system prompt and a user prompt. The system prompt, detailed
    in Table [6](#A1.T6 "Table 6 ‣ Appendix A Details about Prompting Format and Settings
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why"),
    sets ChatGPT’s objective and return format. The user prompt, outlined in Table [1](#S3.T1
    "Table 1 ‣ Prompt styles. ‣ 3.2 Influence of Structural Information on LLMs Under
    Varying Textual Contexts ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why"),
    provides information on the target node and its neighborhood for prediction. To
    standardize ChatGPT’s output format, we append “Do not give any reasoning or logic
    for your answer” to the end of all prompts, except zero-shot CoT prompts.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对 ChatGPT 的 API 调用使用了双部分提示结构，与 ChatGPT 聊天完成 API⁶⁶6[https://platform.openai.com/docs/guides/gpt/chat-completions-api](https://platform.openai.com/docs/guides/gpt/chat-completions-api)
    保持一致。每次 API 调用涉及系统提示和用户提示。系统提示，详见表 [6](#A1.T6 "表 6 ‣ 附录 A 关于提示格式和设置的详细信息 ‣ LLM
    能否有效利用图结构信息：何时以及为什么")，设定了 ChatGPT 的目标和返回格式。用户提示，详见表 [1](#S3.T1 "表 1 ‣ 提示风格。 ‣
    3.2 结构信息对 LLM 的影响 ‣ 3 何时以及为什么 LLM 能从结构信息中受益？ ‣ LLM 能否有效利用图结构信息：何时以及为什么")，提供了关于目标节点及其邻域的信息以供预测。为了标准化
    ChatGPT 的输出格式，我们在所有提示的末尾附加了“不要对你的回答给出任何推理或逻辑”，零-shot CoT 提示除外。
- en: 'Table 6: System prompts for each dataset.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：每个数据集的系统提示。
- en: '|  |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '| --- | --- |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Dataset | System Prompt |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 系统提示 |'
- en: '| ogbn-arxiv, arxiv-2023 | Please predict the most appropriate arXiv Computer
    Science (CS) sub-category for the paper. The predicted sub-category should be
    in the format ’cs.XX’. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv, arxiv-2023 | 请预测论文最合适的 arXiv 计算机科学（CS）子类别。预测的子类别应采用’cs.XX’格式。
    |'
- en: '| cora | Please predict the most appropriate category for the paper. Choose
    from the following categories:\nRule Learning\nNeural Networks\nCase Based\nGenetic
    Algorithms\nTheory\nReinforcement Learning\nProbabilistic Methods\n |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| cora | 请预测论文最合适的类别。从以下类别中选择：\n规则学习\n神经网络\n案例学习\n遗传算法\n理论\n强化学习\n概率方法\n |'
- en: '| pubmed | Please predict the most likely type of the paper. Your answer should
    be chosen from:\nType 1 diabetes\nType 2 diabetes\nExperimentally induced diabetes.\n
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| pubmed | 请预测论文最可能的类型。你的答案应从以下选项中选择：\n1 型糖尿病\n2 型糖尿病\n实验性诱导糖尿病。\n |'
- en: '| ogbn-product | Please predict the most likely category of this product from
    Amazon. Your answer should be chosen from the list:\nHome & Kitchen\nHealth &
    Personal Care\n… |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-product | 请预测这个产品最可能的类别。你的答案应从以下列表中选择：\n家居 & 厨房\n健康 & 个人护理\n… |'
- en: 'We outline the settings for each prompting method as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们概述了每种提示方法的设置如下：
- en: '1.'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Few-shot: Two correct example predictions from ChatGPT are added before the
    target node information.'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Few-shot: 从 ChatGPT 添加的两个正确示例预测被插入到目标节点信息之前。'
- en: '2.'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Target node with neighbors: For datasets ogbn-arxiv, cora, pubmed and arxiv-2023,
    prompts include up to 20 one-hop and 5 two-hop neighbors. For ogbn-product, up
    to 40 one-hop and 10 two-hop neighbors are included.'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标节点及其邻居：对于数据集 ogbn-arxiv、cora、pubmed 和 arxiv-2023，提示包括最多 20 个一跳和 5 个二跳邻居。对于
    ogbn-product，则包括最多 40 个一跳和 10 个二跳邻居。
- en: '3.'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Attention extraction: The maximum number of neighbors is the same as Target
    node with neighbors. We only consider one-hop attention in this study, setting
    the attention number $k$ to 5.'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意力提取：最大邻居数量与目标节点及其邻居相同。我们在本研究中仅考虑一跳注意力，将注意力数量 $k$ 设置为 5。
- en: Common settings for all methods include a temperature of 0 and a maximum output
    token limit of 500\. If a neighbor belongs to the training or validation set,
    its label is included in the prompt.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 所有方法的共同设置包括温度为 0 和最大输出令牌限制为 500。如果一个邻居属于训练集或验证集，则其标签会包含在提示中。
- en: Appendix B Datasets Information
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 数据集信息
- en: In this section we detail the information about benchmark datasets and the collection
    pipeline of arxiv-2023.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了基准数据集和 arxiv-2023 的收集流程信息。
- en: B.1 Datasets Statistics and Splits
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 数据集统计与划分
- en: 'Table [7](#A2.T7 "Table 7 ‣ B.1 Datasets Statistics and Splits ‣ Appendix B
    Datasets Information ‣ Can LLMs Effectively Leverage Graph Structural Information:
    When and Why") presents basic statistics for each dataset. For detailed information
    on datasets and methods to obtain raw text attributes, please see Appendix A in
     Chen et al. ([2023](#bib.bib7)).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [7](#A2.T7 "表 7 ‣ B.1 数据集统计与划分 ‣ 附录 B 数据集信息 ‣ LLMs 能否有效利用图结构信息：何时及为何") 显示了每个数据集的基本统计信息。有关数据集的详细信息以及获取原始文本属性的方法，请参见
    Chen et al. ([2023](#bib.bib7)) 附录 A。
- en: 'Table 7: Statistics of datasets. Data cut-off indicates the latest data coverage
    of the dataset.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：数据集统计。数据截止指示数据集的最新数据覆盖情况。
- en: '| Dataset | #Nodes | #Edges | #Task | Metric | #Test Nodes | Data Cut-Off |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 节点数 | 边数 | 任务数 | 评价指标 | 测试节点数 | 数据截止 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| cora | 2,708 | 5,429 | 7 | Accuracy | 542 | 2000 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| cora | 2,708 | 5,429 | 7 | 准确率 | 542 | 2000 |'
- en: '| pubmed | 19,717 | 44,338 | 3 | Accuracy | 1,000 | 2000 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| pubmed | 19,717 | 44,338 | 3 | 准确率 | 1,000 | 2000 |'
- en: '| ogbn-arxiv | 169,343 | 1,166,243 | 40 | Accuracy | 1,000 | 2019 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | 169,343 | 1,166,243 | 40 | 准确率 | 1,000 | 2019 |'
- en: '| ogbn-product | 2,449,029 | 61,859,140 | 1 | Accuracy | 1,000 | 2019 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-product | 2,449,029 | 61,859,140 | 1 | 准确率 | 1,000 | 2019 |'
- en: '| arxiv-2023 | 33,868 | 305,672 | 40 | Accuracy | 668 | 2023 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| arxiv-2023 | 33,868 | 305,672 | 40 | 准确率 | 668 | 2023 |'
- en: 'The dataset splits are as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集划分如下：
- en: '1.'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'cora: Training/Validation/Testing ratios are 0.1/0.2/0.2.'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: cora：训练/验证/测试比率为 0.1/0.2/0.2。
- en: '2.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'pubmed: Training/Validation/Testing ratios are 0.6/0.2/0.2, following  He et al.
    ([2023](#bib.bib15)).'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: pubmed：训练/验证/测试比率为 0.6/0.2/0.2，参考 He et al. ([2023](#bib.bib15))。
- en: '3.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'ogbn-arxiv: Original OGB (Hu et al., [2020](#bib.bib16)) splits are used, categorizing
    papers by their publication year: training (pre-2017), validation (2018), and
    testing (2019).'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ogbn-arxiv：使用原始 OGB (Hu et al., [2020](#bib.bib16)) 划分，根据论文出版年份进行分类：训练（2017
    年前）、验证（2018 年）和测试（2019 年）。
- en: '4.'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'ogbn-product: Original OGB splits are used based on sales ranking: top 8% for
    training, next 2% for validation, and the remainder for testing.'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ogbn-product：基于销售排名使用原始 OGB 划分：前 8% 用于训练，接下来的 2% 用于验证，其余用于测试。
- en: '5.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'arxiv-2023: Year-based splits similar to ogbn-arxivis adopted: training (pre-2019),
    validation (2020), and testing (2023).'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: arxiv-2023：采用类似于 ogbn-arxiv 的按年份划分：训练（2019 年前）、验证（2020 年）和测试（2023 年）。
- en: Due to API cost and rate limits, we test on a random sample of 1,000 nodes for
    pubmed, ogbn-arxiv, and ogbn-product, using a fixed seed for reproducibility.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 API 成本和速率限制，我们对 pubmed、ogbn-arxiv 和 ogbn-product 的随机样本 1,000 个节点进行测试，使用固定种子以确保可重复性。
- en: B.2 Collection of arxiv-2023
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 arxiv-2023 收集
- en: 'The detailed pipeline is as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 详细流程如下：
- en: '1.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Sample 668 test nodes from around 46,000 arXiv CS papers published from January
    1 to August 22, 2023.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从 2023 年 1 月 1 日到 8 月 22 日的约 46,000 篇 arXiv CS 论文中抽样 668 个测试节点。
- en: '2.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Extract references to identify one-hop and two-hop neighbors. References were
    obtained by two steps. First, we search for valid arXiv IDs within each paper,
    following a method similar to  (Clement et al., [2019](#bib.bib9)). Second, we
    use AnyStyle⁷⁷7https://github.com/inukshuk/anystyle to extract the titles of the
    references, which we then search for via the arXiv API⁸⁸8https://info.arxiv.org/help/api/basics.html.
    Titles found on arXiv are considered valid citations if they have a small levenshtein
    distance (Miller et al., [2009](#bib.bib32)) from the searched title. To prevent
    duplicate searches, we skip any references that already have a matched arXiv ID.
    To comply with the arXiv API’s rate limit, each paper is restricted to a maximum
    of 30 searches. For papers published before 2019, we attempt to match them to
    nodes in the ogbn-arxiv based on titles. Unmatched pre-2019 nodes are excluded
    from our dataset.
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提取参考文献以识别一跳和二跳邻居。参考文献通过两个步骤获得。首先，我们在每篇论文中搜索有效的 arXiv ID，方法类似于 (Clement 等人，[2019](#bib.bib9))。其次，我们使用
    AnyStyle⁷⁷7https://github.com/inukshuk/anystyle 提取参考文献的标题，然后通过 arXiv API⁸⁸8https://info.arxiv.org/help/api/basics.html
    进行搜索。在 arXiv 上找到的标题，如果与搜索标题的 levenshtein 距离较小，则视为有效引用。为防止重复搜索，我们跳过任何已经匹配的 arXiv
    ID 的参考文献。为了遵守 arXiv API 的速率限制，每篇论文最多限制 30 次搜索。对于 2019 年之前发布的论文，我们尝试根据标题将其匹配到 ogbn-arxiv
    中的节点。未匹配的 2019 年之前的节点将被排除在数据集之外。
- en: '3.'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Construct a citation network using nodes from step 2\. Basically for each node
    we need a list of paper it cites. While references for test nodes and one-hop
    nodes are obtained through both arXiv ID matching and title searching, the references
    for two-hop nodes are solely determined by arXiv ID matching, due to rate limit
    constraints. Dataset statistics are in Table [3](#S3.T3 "Table 3 ‣ Comparison
    between arxiv-2023 and ogbn-arxiv. ‣ 3.3 Data Leakage as a Potential Contributor
    of Performance ‣ 3 When and Why Can LLMs Benefit from Structural Information?
    ‣ Can LLMs Effectively Leverage Graph Structural Information: When and Why").
    We have similar test node degrees between ogbn-arxiv and arxiv-2023.'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用第 2 步中的节点构建引文网络。基本上，对于每个节点，我们需要一个它引用的论文列表。虽然测试节点和一跳节点的参考文献是通过 arXiv ID 匹配和标题搜索获得的，但由于速率限制，二跳节点的参考文献仅通过
    arXiv ID 匹配确定。数据集统计信息见表 [3](#S3.T3 "表 3 ‣ arxiv-2023 和 ogbn-arxiv 的比较。 ‣ 3.3 数据泄露作为潜在的性能影响因素
    ‣ 3 LLMs 在何时何地能从结构信息中受益？ ‣ LLMs 能有效利用图结构信息吗：何时及为何"). 我们在 ogbn-arxiv 和 arxiv-2023
    之间有类似的测试节点度。
- en: Appendix C MPNNs as Baselines
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C MPNNs 作为基线模型
- en: Embedding generation.
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌入生成。
- en: We adapt the embedding generation pipeline from Hu et al. ([2020](#bib.bib16))
    to train a skip-gram model (Mikolov et al., [2013](#bib.bib31)) on corpus comprising
    titles and abstracts from both ogbn-arxiv and arxiv-2023. Each paper’s 128-dimensional
    feature vector is then obtained by averaging the word embeddings in its title.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 Hu 等人 ([2020](#bib.bib16)) 适配了嵌入生成管道，以训练一个跳字模型 (Mikolov 等人，[2013](#bib.bib31))，该模型在包含
    ogbn-arxiv 和 arxiv-2023 标题及摘要的语料库上进行训练。每篇论文的 128 维特征向量通过对其标题中的词嵌入进行平均获得。
- en: Hyperparameter tunning.
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 超参数调优。
- en: 'Baseline models GCN and SAGE are implemented with PyG (Fey & Lenssen, [2019](#bib.bib10)).
    For hyperparameter tunning, we perform a random search on the following hyperparameter
    tuning range for every model following Ma et al. ([2022](#bib.bib27)):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 基线模型 GCN 和 SAGE 使用 PyG (Fey & Lenssen，[2019](#bib.bib10)) 实现。对于超参数调优，我们按照 Ma
    等人 ([2022](#bib.bib27)) 的方法，对每个模型进行以下超参数调优范围的随机搜索：
- en: •
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hidden size: $\{32,64\}$.'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐藏层大小：$\{32,64\}$。
- en: •
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Learning rate: $\{.001,.005,.01,.1\}$.'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 学习率：$\{.001,.005,.01,.1\}$。
- en: •
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dropout rate: $\{.2,.4,.6,.8\}$.'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 丢弃率：$\{.2,.4,.6,.8\}$。
- en: •
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Weight decay: $\{.0001,.001,.01,.1\}$.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重衰减：$\{.0001,.001,.01,.1\}$。
- en: Each model is run on 100 random configurations and each random configuration
    is run for 3 times on ogbn-arxiv and arxiv-2023. The max training epoch number
    is 2000\. When training is finished, we use the model with highest average validation
    accuracy on the dataset for testing.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型在 100 个随机配置上运行，每个随机配置在 ogbn-arxiv 和 arxiv-2023 上运行 3 次。最大训练轮次为 2000。当训练完成后，我们使用数据集中具有最高平均验证准确度的模型进行测试。
- en: Appendix D Additional Analysis for Data Leakage
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 数据泄露的额外分析
- en: Details about dropping experiments.
  id: totrans-262
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 关于丢弃实验的详细信息。
- en: 'We have three different strategies: a) drop the neighbors with same label (drop
    same), b) drop the neighbors with different label (drop different), c) drop neighbors
    randomly (drop random). Let’s define $x$, we elaborate on the three strategies:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有三种不同策略：a) 丢弃具有相同标签的邻居（drop same），b) 丢弃具有不同标签的邻居（drop different），c) 随机丢弃邻居（drop
    random）。让我们定义$x$，详细说明这三种策略：
- en: '1.'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'drop random: We randomly drop $(x+y)p$ neighbors.'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'drop random: 我们随机丢弃$(x+y)p$个邻居。'
- en: '2.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'drop same: We retain $\max(x-(x+y)p,0)$ neighbors with different labels.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'drop same: 我们保留$\max(x-(x+y)p,0)$个具有不同标签的邻居。'
- en: '3.'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'drop different: We retain $\max(y-(x+y)p,0)$ neighbors with same labels.'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'drop different: 我们保留$\max(y-(x+y)p,0)$个具有相同标签的邻居。'
- en: We further explain this by an example. Assume node $A$ neighbors with same label.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一个例子进一步解释这一点。假设节点$A$的邻居具有相同的标签。
- en: Investigating data leakage through prompt variability.
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过提示变量性研究数据泄露。
- en: 'Chen et al. ([2023](#bib.bib7)) reveal considerable fluctuations in Language
    Model (LLM) performance on ogbn-arxivwhen using three distinct prompt words: ”arXiv
    cs subcategory,” ”arXiv identifier,” and natural language. These variations have
    been interpreted as potential indicators of data leakage.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人 ([2023](#bib.bib7)) 揭示了在使用三种不同提示词时，语言模型（LLM）在ogbn-arxiv上的表现有显著波动： ”arXiv
    cs子类别”, ”arXiv标识符”, 和自然语言。这些变化被解释为潜在的数据泄露指标。
- en: 'To delve deeper into this issue, we expand upon their experiments by testing
    additional prompt words. We also introduce two experimental settings: one with
    label options provided and another without. As displayed in Table [8](#A4.T8 "Table
    8 ‣ Investigating data leakage through prompt variability. ‣ Appendix D Additional
    Analysis for Data Leakage ‣ Can LLMs Effectively Leverage Graph Structural Information:
    When and Why"), the relative efficacy of various prompts on ogbn-arxiv mirrors
    their performance on arxiv-2023. Importantly, prompts with options underperform
    on both datasets, underscoring a consistent trend.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了深入探讨这个问题，我们通过测试额外的提示词来扩展他们的实验。我们还引入了两种实验设置：一种是提供标签选项的，另一种是不提供。正如表[8](#A4.T8
    "表8 ‣ 通过提示变量性研究数据泄露 ‣ 附录D 数据泄露附加分析 ‣ LLM是否能有效利用图结构信息：何时以及为什么")所示，不同提示在ogbn-arxiv上的相对效能反映了它们在arxiv-2023上的表现。重要的是，带有选项的提示在这两个数据集上表现不佳，突显出一致的趋势。
- en: Also, utilizing structural information in the prompts can somewhat mitigate
    the performance drop from less effective prompts. Indicate that LLMs can leverage
    structural information to improve predictions. This further supports that there
    is no conclusive evidence for data leakage.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，利用提示中的结构信息可以在一定程度上缓解低效提示带来的性能下降。这表明LLMs可以利用结构信息来改善预测。这进一步支持了没有数据泄露的确凿证据。
- en: 'Table 8: Performance across different prompt types between ogbn-arxiv and arxiv-2023.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：ogbn-arxiv和arxiv-2023之间不同提示类型的性能。
- en: '| System Prompt | Zero-shot | 1-hop title+label |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 系统提示 | 零样本 | 1-hop标题+标签 |'
- en: '| ogbn-arxiv | arxiv-2023 | ogbn-arxiv | arxiv-2023 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| ogbn-arxiv | arxiv-2023 | ogbn-arxiv | arxiv-2023 |'
- en: '| Please predict the most appropriate arXiv Computer Science (CS) sub-category
    for the paper. The predicted sub-category should be in the format ’cs.XX’. | 74.0
    | 73.7 | 74.3 | 70.4 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 请预测论文最合适的arXiv计算机科学（CS）子类别。预测的子类别应采用’cs.XX’格式。 | 74.0 | 73.7 | 74.3 | 70.4
    |'
- en: '| Please predict the most appropriate arXiv Computer Science (CS) sub-category
    for the paper. Your answer should be chosen from cs.AI, ..cs.SY. The predicted
    sub-category should be in the format ’cs.XX’. | 66.0 | 68.1 | 70.7 | 67.9 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 请预测论文最合适的arXiv计算机科学（CS）子类别。你的答案应从cs.AI, ..cs.SY中选择。预测的子类别应采用’cs.XX’格式。 |
    66.0 | 68.1 | 70.7 | 67.9 |'
- en: '| Please predict the most appropriate original arXiv identifier for the paper.
    The predicted arxiv identifier should be in the format ’arxiv cs.xx’. | 71.3 |
    70.8 | 73.7 | 67.5 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 请预测论文最合适的原始arXiv标识符。预测的arxiv标识符应采用’arxiv cs.xx’格式。 | 71.3 | 70.8 | 73.7 |
    67.5 |'
- en: '| Please predict the most appropriate original arXiv identifier for the paper.
    Your answer should be chosen from cs.ai,.. cs.sy. The predicted arxiv identifier
    should be in the format ’arxiv cs.xx’. | 58.4 | 57.2 | 71.7 | 64.2 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 请预测论文最合适的原始arXiv标识符。你的答案应从cs.ai, .. cs.sy中选择。预测的arxiv标识符应采用’arxiv cs.xx’格式。
    | 58.4 | 57.2 | 71.7 | 64.2 |'
- en: '| Please predict the most appropriate category for the paper. Your answer should
    be chosen from ”Artificial Intelligence”,.. ”Systems and Control”. | 54.6 | 53.4
    | 74.1 | 67.8 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 请预测论文最合适的类别。你的答案应从”人工智能”, .. ”系统与控制”中选择。 | 54.6 | 53.4 | 74.1 | 67.8 |'
