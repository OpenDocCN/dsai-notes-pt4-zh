- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:49:31'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:31
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Goal-Oriented Prompt Attack and Safety Evaluation for LLMs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 以目标为导向的 Prompt Attack 和 LLMs 的安全评估
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11830](https://ar5iv.labs.arxiv.org/html/2309.11830)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11830](https://ar5iv.labs.arxiv.org/html/2309.11830)
- en: Chengyuan Liu¹, Fubang Zhao², Lizhi Qing², Yangyang Kang²²²2Corresponding author.,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 刘程远¹，赵富邦²，秦立志²，康洋洋²²²2通讯作者。
- en: Changlong Sun², Kun Kuang¹²²2Corresponding author., Fei Wu¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 孙长龙²，匡坤¹²²2通讯作者，吴飞¹
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Warning: this paper contains examples that may be offensive or upsetting.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：本文包含可能令人不悦或冒犯的例子。
- en: 'Large Language Models (LLMs) presents significant priority in text understanding
    and generation. However, LLMs suffer from the risk of generating harmful contents
    especially while being employed to applications. There are several black-box attack
    methods, such as Prompt Attack, which can change the behaviour of LLMs and induce
    LLMs to generate unexpected answers with harmful contents. Researchers are interested
    in Prompt Attack and Defense with LLMs, while there is no publicly available dataset
    with high successful attacking rate to evaluate the abilities of defending prompt
    attack. In this paper, we introduce a pipeline to construct high-quality prompt
    attack samples, along with a Chinese prompt attack dataset called CPAD. Our prompts
    aim to induce LLMs to generate unexpected outputs with several carefully designed
    prompt attack templates and widely concerned attacking contents. Different from
    previous datasets involving safety estimation, we construct the prompts considering
    three dimensions: contents, attacking methods and goals. Especially, the attacking
    goals indicate the behaviour expected after successfully attacking the LLMs, thus
    the responses can be easily evaluated and analysed. We run several popular Chinese
    LLMs on our dataset, and the results show that our prompts are significantly harmful
    to LLMs, with around 70% attack success rate to GPT-3.5\. CPAD is publicly available
    at https://github.com/liuchengyuan123/CPAD.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在文本理解和生成方面具有重要优先性。然而，LLMs 存在生成有害内容的风险，尤其是在应用过程中。有多种黑箱攻击方法，如 Prompt
    Attack，可以改变 LLMs 的行为并诱导其生成包含有害内容的意外答案。研究人员对 LLMs 的 Prompt Attack 和防御感兴趣，但目前没有公开的高成功攻击率的数据集来评估防御
    prompt attack 的能力。在本文中，我们介绍了一个构建高质量 prompt attack 样本的流程，并提供了一个名为 CPAD 的中文 prompt
    attack 数据集。我们的 prompt 旨在诱导 LLMs 生成意外输出，包含多个精心设计的 prompt attack 模板和广泛关注的攻击内容。与之前涉及安全评估的数据集不同，我们在构建
    prompt 时考虑了内容、攻击方法和目标三个维度。特别地，攻击目标指示了在成功攻击 LLMs 后期望的行为，从而可以轻松评估和分析响应。我们在数据集上运行了几个流行的中文
    LLMs，结果表明我们的 prompt 对 LLMs 有显著的危害，对 GPT-3.5 的攻击成功率约为 70%。CPAD 已公开，可在 https://github.com/liuchengyuan123/CPAD
    获取。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Large Language Models (LLMs) can generate creative content and solve practical
    planning and reasoning problems (Chowdhery et al. [2022](#bib.bib3); OpenAI [2023](#bib.bib13)).
    Take GPT-3 (Brown et al. [2020](#bib.bib2)) as an example, with 175 billion parameters,
    it stores a large amount of knowledge and exhibits surprising performance on various
    downstream tasks. Instruction tuning (Wei et al. [2021](#bib.bib21)) and Reinforcement
    Learning from Human Feedback (Ziegler et al. [2019](#bib.bib30); Lambert et al.
    [2022](#bib.bib10)) (RLHF) allow LLMs to generate appropriate answers based on
    user queries. LLMs can handle tasks they have never seen before without continuous
    fine-tuning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）可以生成创造性内容，并解决实际规划和推理问题（Chowdhery et al. [2022](#bib.bib3); OpenAI
    [2023](#bib.bib13)）。以 GPT-3（Brown et al. [2020](#bib.bib2)）为例，它具有 1750 亿个参数，存储了大量知识，并在各种下游任务中表现出惊人的性能。指令调优（Wei
    et al. [2021](#bib.bib21)）和来自人类反馈的强化学习（Ziegler et al. [2019](#bib.bib30); Lambert
    et al. [2022](#bib.bib10)）（RLHF）使 LLMs 能够根据用户查询生成合适的答案。LLMs 可以处理它们从未见过的任务，而无需持续的微调。
- en: '![Refer to caption](img/ea0cd97f57f2beae00c40a28ae15cbba.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ea0cd97f57f2beae00c40a28ae15cbba.png)'
- en: 'Figure 1: Comparison of successful attacking between previous work and CPAD.
    CPAD can further expose the security risks of LLMs.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：成功攻击的比较，前期工作与 CPAD 的对比。CPAD 可以进一步暴露 LLMs 的安全风险。
- en: While LLMs have demonstrated success in numerous tasks, there are still some
    vital flaws for deployment such as the significant security risks associated with
    LLMs. With the help of robust general knowledge and common sense, LLMs can offer
    valuable guidance for harmful behavior and can automatically generate offensive,
    discriminatory and fraudulent content, and misleading views(Deng et al. [2023](#bib.bib4);
    Zhuo et al. [2023](#bib.bib28); Hartvigsen et al. [2022](#bib.bib7)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 LLMs 在众多任务中表现出色，但在部署中仍存在一些关键缺陷，例如与 LLMs 相关的重大安全风险。借助广泛的常识，LLMs 可以为有害行为提供宝贵的指导，并自动生成冒犯、歧视和欺诈内容以及误导性观点（Deng
    等 [2023](#bib.bib4); Zhuo 等 [2023](#bib.bib28); Hartvigsen 等 [2022](#bib.bib7)）。
- en: '![Refer to caption](img/dffb24e438f905a85d3bea06c9d854b3.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dffb24e438f905a85d3bea06c9d854b3.png)'
- en: 'Figure 2: Illustration of Prompt Attack. With carefully designed prompts, attackers
    can bypass the ethical constraints of LLMs, inducing LLMs to generate illegal
    contents.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 提示攻击的示意图。通过精心设计的提示，攻击者可以绕过 LLMs 的伦理约束，诱使 LLMs 生成非法内容。'
- en: Although RLHF can prevent LLMs from generating harmful contents by aligning
    the responses with human preference, researchers find that LLMs can still generate
    unexpected sentences under carefully designed prompt attack (Perez and Ribeiro
    [2022](#bib.bib15); Li et al. [2023](#bib.bib11)), as shown in Figure [2](#Sx1.F2
    "Figure 2 ‣ Introduction ‣ Goal-Oriented Prompt Attack and Safety Evaluation for
    LLMs"). Due to the concern of evaluating and enhancing the safety of LLMs, Sun
    et al. ([2023](#bib.bib17)) introduced SAFETYPROMPTS. Xu et al. ([2023](#bib.bib23))
    presented CVALUES, the first Chinese human values evaluation benchmark to measure
    the alignment ability of LLMs in terms of both safety and responsibility criteria.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 RLHF 可以通过将响应与人类偏好对齐来防止 LLMs 生成有害内容，研究人员发现 LLMs 在精心设计的提示攻击下仍可能生成意外的句子（Perez
    和 Ribeiro [2022](#bib.bib15); Li 等 [2023](#bib.bib11)），如图 [2](#Sx1.F2 "图 2 ‣ 介绍
    ‣ 面向目标的提示攻击与 LLMs 安全评估") 所示。由于评估和提高 LLMs 安全性的担忧，Sun 等人 ([2023](#bib.bib17)) 引入了
    SAFETYPROMPTS。Xu 等人 ([2023](#bib.bib23)) 提出了 CVALUES，这是第一个中文人类价值评估基准，用于衡量 LLMs
    在安全和责任标准方面的对齐能力。
- en: Nonetheless, the majority of LLMs exhibit a high rate of success in defending
    against the offensive prompts within publicly accessible evaluation benchmark.
    For example, ChatGPT and ChatGLM-6B(Zeng et al. [2022](#bib.bib26); Du et al.
    [2022](#bib.bib5)) exhibit the overall unsafety scores of 1.63 and 3.19 respectively
    on SAFETYPROMPTS. While recent studies illustrate non-negligible risk of generating
    harmful contents under carefully designed prompts, which implies that such benchmarks
    cannot comprehensively reflect the full spectrum of safety possessed by various
    Large Language Models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，大多数 LLMs 在公开评估基准中的防御能力较高。例如，ChatGPT 和 ChatGLM-6B（Zeng 等 [2022](#bib.bib26);
    Du 等 [2022](#bib.bib5)）在 SAFETYPROMPTS 上的整体不安全评分分别为 1.63 和 3.19。虽然最近的研究显示在精心设计的提示下生成有害内容的风险不可忽视，这意味着这些基准不能全面反映各种大型语言模型所具备的全部安全范围。
- en: Additionally, the safety assessment conducted in previous studies are aimless
    and casual. They failed to effectively incorporate the attacking goal. When launching
    a prompt attack on a Large Language Model, the attacker has a specific goal to
    exploit. For instance, if LLMs are prompted to generate pornographic content,
    then descriptions of scenes are expected. On another hand, if the attacker wants
    LLMs provide guidance for illegal and criminal activities, then some plans and
    tips shall be concluded in the response. The attributes hidden in the text can
    be regarded as the flag to estimate whether LLMs are successfully attacked. Unfortunately,
    previous researches have neglected to consider this crucial factor when constructing
    attacking prompts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，之前研究中进行的安全评估缺乏目的性和随意性。他们未能有效地纳入攻击目标。在对大型语言模型发起提示攻击时，攻击者有具体的利用目标。例如，如果 LLMs
    被提示生成色情内容，则期望描述场景。另一方面，如果攻击者希望 LLMs 提供非法和犯罪活动的指导，则响应中应包含一些计划和建议。文本中隐藏的属性可以作为估计
    LLMs 是否成功被攻击的标志。不幸的是，之前的研究在构建攻击提示时忽视了这一关键因素。
- en: 'To accurately simulate prompt attacks on LLMs from the perspective of attackers
    and estimate the associated security risks, we have developed a pipeline to construct
    high-quality prompt attack samples, along with a Chinese prompt attack dataset
    called CPAD. During the prompts construction and collection, we meticulously consider
    three key dimensions: contents, the attacking templates and goals. We carefully
    designed some attacking templates for various topics, and an automated process
    for constructing attack prompts, which contribute to the outperforming successful-attacking
    rate over pervious works as shown in Figure [1](#Sx1.F1 "Figure 1 ‣ Introduction
    ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从攻击者的角度准确模拟对大型语言模型（LLMs）的提示攻击，并评估相关的安全风险，我们开发了一个构建高质量提示攻击样本的流程，并创建了一个名为CPAD的中文提示攻击数据集。在提示的构建和收集过程中，我们仔细考虑了三个关键维度：内容、攻击模板和目标。我们精心设计了一些针对各种主题的攻击模板，并建立了一个自动化的攻击提示构建过程，这些工作使得我们在攻击成功率上超过了以往的研究，如图[1](#Sx1.F1
    "Figure 1 ‣ Introduction ‣ Goal-Oriented Prompt Attack and Safety Evaluation for
    LLMs")所示。
- en: Furthermore, we propose to evaluate the safety score of LLMs with different
    evaluation prompts according to the attacking goals. Taking the previous case
    as an example, if the attacker wants LLMs provide guidance for illegal and criminal
    activities, we only need to identify whether the LLM’s response provides a plan
    related to the crime, and we can more accurately determine whether the model is
    safe. Unlike Sun et al. ([2023](#bib.bib17)), which uses the same prompt for all
    samples, our evaluation is more precise, intuitive, scenario-specific.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们建议根据攻击目标使用不同的评估提示来评估LLM的安全评分。以之前的案例为例，如果攻击者希望LLM提供有关非法和犯罪活动的指导，我们只需确定LLM的响应是否提供了与犯罪相关的计划，就可以更准确地判断模型是否安全。与Sun
    et al. ([2023](#bib.bib17))使用相同提示对所有样本进行评估不同，我们的评估更为精确、直观且特定场景。
- en: We employed another LLM to automatically construct a large number of attack
    samples, and after analyzing the responses from three Chinese LLMs, we identified
    10050 highly effective and dangerous prompts. We conducted experiments and analysis
    on several LLMs including GPT-3.5\. According to the statistics, our attacking
    prompts exhibited a remarkable success rate of 69.91% against GPT-3.5, which is
    significantly higher than most publicly available datasets. Additionally, our
    attacking prompts achieved a success rate of over 70% against other open-source
    Chinese LLMs such as ChatGLM2-6B(Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用另一个LLM自动构建了大量攻击样本，并在分析了三种中文LLM的响应后，识别出10050个高效且危险的提示。我们对包括GPT-3.5在内的多个LLM进行了实验和分析。根据统计数据，我们的攻击提示在GPT-3.5上的成功率达到了69.91%，显著高于大多数公开可用的数据集。此外，我们的攻击提示在其他开源中文LLM（如ChatGLM2-6B(Du
    et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)))上的成功率超过了70%。
- en: Besides, we conduct a straightforward experiment to fine-tune a LLM with CPAD,
    reducing the risks of being attacked. Our research endeavors to bring attention
    to LLM security within the community. We encourage researchers to utilize our
    dataset and engage in further investigation of prompt attack strategies and defensive
    approaches, thereby enhancing the security and performance of Large Language Models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们进行了一个简单的实验，通过使用CPAD对LLM进行微调，以降低被攻击的风险。我们的研究旨在引起社区对LLM安全性的关注。我们鼓励研究人员利用我们的数据集，并进一步研究提示攻击策略和防御方法，从而提升大型语言模型的安全性和性能。
- en: 'In summary, our contributions are three folds:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献有三点：
- en: '1.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: To address the current lack of high-quality evaluation datasets in the field
    of prompt attacks, we have developed a pipeline to construct high-quality prompt
    attack samples, along with a Chinese prompt attack dataset called CPAD.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对当前提示攻击领域中缺乏高质量评估数据集的问题，我们开发了一个构建高质量提示攻击样本的流程，并创建了一个名为CPAD的中文提示攻击数据集。
- en: '2.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We specify the attacking goals of each prompt, which not only accurately simulate
    prompt attacks on LLMs from the perspective of attackers, but also can be utilized
    to evaluate and analyse the response.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们具体说明了每个提示的攻击目标，这不仅能准确模拟从攻击者角度对LLM的提示攻击，还能用于评估和分析模型的响应。
- en: '3.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Our analysis indicates that our prompts achieve an attack success rate of nearly
    70% against GPT-3.5\. A straightforward strategy has been developed to defend
    against the attacks. We encourage researchers to leverage our dataset for further
    studies.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的分析表明，我们的提示在对抗GPT-3.5时的攻击成功率接近70%。已经开发出一种直接的策略来防御这些攻击。我们鼓励研究人员利用我们的数据集进行进一步的研究。
- en: '![Refer to caption](img/918c268b219c476e2f7d15d01d9e1454.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/918c268b219c476e2f7d15d01d9e1454.png)'
- en: 'Figure 3: The construction of our prompt attack dataset.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：我们提示攻击数据集的构建。
- en: Related Work
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关工作
- en: While Large Language Models have achieved impressive results in various tasks
    (Brown et al. [2020](#bib.bib2); Chowdhery et al. [2022](#bib.bib3); OpenAI [2023](#bib.bib13))
    through techniques such as pre-training (Hendrycks, Lee, and Mazeika [2019](#bib.bib8);
    Liu et al. [2019](#bib.bib12); Yang et al. [2020](#bib.bib25); Bao et al. [2020](#bib.bib1)),
    Instruction Tuning (Keskar et al. [2019](#bib.bib9); Raffel et al. [2020](#bib.bib16)),
    and RLHF (Ouyang et al. [2022](#bib.bib14); Ziegler et al. [2020](#bib.bib29)),
    the risk of prompt safety remains a critical obstacle to their full utilization
    (Weidinger et al. [2021](#bib.bib22); Tamkin et al. [2021](#bib.bib18)). With
    the help of robust general knowledge and common sense, LLMs can offer valuable
    guidance for harmful behavior and can automatically generate offensive, discriminatory
    and fraudulent content, and misleading views (Deng et al. [2023](#bib.bib4); Zhuo
    et al. [2023](#bib.bib28); Hartvigsen et al. [2022](#bib.bib7)).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大型语言模型（Large Language Models）通过诸如预训练（Hendrycks, Lee, and Mazeika [2019](#bib.bib8);
    Liu et al. [2019](#bib.bib12); Yang et al. [2020](#bib.bib25); Bao et al. [2020](#bib.bib1)）、指令调优（Keskar
    et al. [2019](#bib.bib9); Raffel et al. [2020](#bib.bib16)）和RLHF（Ouyang et al.
    [2022](#bib.bib14); Ziegler et al. [2020](#bib.bib29)）等技术在各种任务中取得了令人印象深刻的成果（Brown
    et al. [2020](#bib.bib2); Chowdhery et al. [2022](#bib.bib3); OpenAI [2023](#bib.bib13)），但提示安全性风险仍然是其全面利用的关键障碍（Weidinger
    et al. [2021](#bib.bib22); Tamkin et al. [2021](#bib.bib18)）。借助强大的通用知识和常识，大型语言模型可以为有害行为提供有价值的指导，但也可能自动生成冒犯性、歧视性和欺诈性内容，以及误导性观点（Deng
    et al. [2023](#bib.bib4); Zhuo et al. [2023](#bib.bib28); Hartvigsen et al. [2022](#bib.bib7)）。
- en: Although RLHF can prevent LLMs from generating harmful contents by aligning
    the responses with human preference, researchers find that LLMs can still generate
    unexpected sentences under carefully designed prompt attack. Perez and Ribeiro
    ([2022](#bib.bib15)) investigated Goal Hijacking and Prompt Leaking by simple
    handcrafted inputs. Li et al. ([2023](#bib.bib11)) studied the privacy threats
    from OpenAI’s ChatGPT and the New Bing¹¹1https://www.bing.com/new enhanced by
    ChatGPT and show that application-integrated LLMs may cause new privacy threats.
    Greshake et al. ([2023](#bib.bib6)) attacked LLM-Integrated Applications using
    Indirect Prompt Injection, and showed how processing retrieved prompts can act
    as arbitrary code execution, manipulate the application’s functionality, and control
    how and if other APIs are called. Zou et al. ([2023](#bib.bib31)) proposed a simple
    and effective attack method that causes aligned language models to generate objectionable
    behaviors, by automatically finding a suffix that aims to maximize the probability
    that the model produces an affirmative response.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管RLHF可以通过将响应与人类偏好对齐来防止大型语言模型生成有害内容，但研究人员发现大型语言模型在经过精心设计的提示攻击下仍能生成意外的句子。Perez和Ribeiro（[2022](#bib.bib15)）通过简单的手工输入调查了目标劫持（Goal
    Hijacking）和提示泄漏（Prompt Leaking）。Li et al.（[2023](#bib.bib11)）研究了OpenAI的ChatGPT和由ChatGPT增强的新必应¹¹1https://www.bing.com/new所带来的隐私威胁，并展示了应用集成的大型语言模型可能引发新的隐私威胁。Greshake
    et al.（[2023](#bib.bib6)）通过间接提示注入攻击了集成大型语言模型的应用，并展示了处理检索到的提示如何作为任意代码执行，操控应用功能，以及控制是否调用其他API。Zou
    et al.（[2023](#bib.bib31)）提出了一种简单而有效的攻击方法，通过自动寻找后缀来最大化模型生成肯定回应的概率，从而引发对齐语言模型产生不当行为。
- en: 'Sun et al. ([2023](#bib.bib17)) developed a Chinese LLM safety assessment benchmark,
    which explored the comprehensive safety performance of LLMs from two perspectives:
    8 kinds of typical safety scenarios and 6 types of more challenging instruction
    attacks. ChatGPT and ChatGLM-6B achieved the overall safety score of 98.37 and
    96.81 respectively. Xu et al. ([2023](#bib.bib23)) presented CValues, the first
    Chinese human values evaluation benchmark to measure the alignment ability of
    LLMs in terms of both safety and responsibility criteria.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Sun等人 ([2023](#bib.bib17)) 开发了一个中文LLM安全评估基准，探索了从两种视角（8种典型安全场景和6种更具挑战性的指令攻击）全面评估LLMs的安全性能。ChatGPT和ChatGLM-6B分别获得了98.37和96.81的总体安全分数。Xu等人
    ([2023](#bib.bib23)) 提出了CValues，这是第一个中文人类价值观评估基准，用于衡量LLMs在安全性和责任标准方面的对齐能力。
- en: Different from previous studies, our work emphasizes the significance of incorporating
    attacking goals and aims to estimate the defense capability of various Large Language
    Models against carefully designed prompt attacks from the perspective of attackers,
    thereby encouraging the interest in the research of LLMs attack and defense within
    the community.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往研究不同，我们的工作强调了将攻击目标纳入的重要性，并旨在从攻击者的角度评估各种大型语言模型对精心设计的提示攻击的防御能力，从而激发社区对LLMs攻击与防御研究的兴趣。
- en: Construction of CPAD
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CPAD的构建
- en: In this Section, we will introduce the construction of our dataset as shown
    in Figure [3](#Sx1.F3 "Figure 3 ‣ Introduction ‣ Goal-Oriented Prompt Attack and
    Safety Evaluation for LLMs"). We firstly introduce the concepts of contents, templates
    and goals. The pipeline starts from manual-designed seeds. several candidates
    are generated as extended prompts with OpenAI-API²²2https://api.openai.com/v1/chat/completions
    according to seeds. Then we filter the extended prompts utilizing the validation
    on three different Large Language Models. Finally, the attacking goals and prompts
    are collected.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍我们数据集的构建，如图[3](#Sx1.F3 "图 3 ‣ 介绍 ‣ 面向目标的提示攻击与LLMs的安全性评估")所示。我们首先介绍内容、模板和目标的概念。流程从手动设计的种子开始。根据种子生成几个候选扩展提示，使用OpenAI-API²²2https://api.openai.com/v1/chat/completions。然后，我们通过在三种不同的大型语言模型上进行验证来筛选扩展提示。最后，收集攻击目标和提示。
- en: Attacking Content, Template and Goal
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 攻击内容、模板和目标
- en: We primarily consider three key dimensions to classify the samples. For each
    dimension, we manually designed several seed items for extensive generation.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要考虑三个关键维度来分类样本。对于每个维度，我们手动设计了几个种子项以进行广泛生成。
- en: Content
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 内容
- en: indicates the topic of the attack, such as unfairness and crimes. Contents are
    widely considered for LLMs attack. Here we adopt a hierarchical classification
    for contents and sub-contents (for example, Sexism and hellish discrimination
    are both grouped into “discrimination categories”). The attack contents we designed
    cover multiple aspects and aims to present the utmost severity of harm possible.
    Detailed content items are listed in Table [4](#Sx4.T4 "Table 4 ‣ Overall Successful-Attacking
    Rate ‣ Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表明攻击的主题，例如不公平和犯罪。内容被广泛考虑用于LLMs攻击。在这里，我们采用内容和子内容的分层分类（例如，性别歧视和地狱般的歧视都归入“歧视类别”）。我们设计的攻击内容涵盖多个方面，旨在呈现可能的极端危害。详细内容项列在表[4](#Sx4.T4
    "表 4 ‣ 整体成功攻击率 ‣ 实验 ‣ 面向目标的提示攻击与LLMs的安全性评估")中。
- en: '![Refer to caption](img/85be32f25f8b3c5bc50202160e3b688a.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/85be32f25f8b3c5bc50202160e3b688a.png)'
- en: 'Figure 4: The evaluation process of responses from LLMs.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：LLMs响应的评估过程。
- en: '| Goal | Template | Model | Total # | Attacked # | Ratio % |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | 模板 | 模型 | 总数 | 攻击数 | 比例 % |'
- en: '| goal hijacking | wrap with shell | ChatGLM | 11648 | 3715 | 31.89 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 目标劫持 | 使用shell包装 | ChatGLM | 11648 | 3715 | 31.89 |'
- en: '| ChatYuan | 11648 | 2797 | 24.01 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 11648 | 2797 | 24.01 |'
- en: '| Ziya | 11648 | 1496 | 12.84 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 11648 | 1496 | 12.84 |'
- en: '| list of unsafe speech or reasons | one-sided statement | ChatGLM | 4954 |
    1157 | 23.35 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 不安全言论或原因列表 | 单方面声明 | ChatGLM | 4954 | 1157 | 23.35 |'
- en: '| ChatYuan | 4920 | 2064 | 41.95 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 4920 | 2064 | 41.95 |'
- en: '| Ziya | 4868 | 1074 | 22.06 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 4868 | 1074 | 22.06 |'
- en: '| unsafe plan development | conversation completion | ChatGLM | 10845 | 4729
    | 43.61 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 不安全计划开发 | 对话完成 | ChatGLM | 10845 | 4729 | 43.61 |'
- en: '| ChatYuan | 10944 | 6549 | 59.84 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 10944 | 6549 | 59.84 |'
- en: '| Ziya | 10985 | 5756 | 52.40 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 10985 | 5756 | 52.40 |'
- en: '| description of unsafe scenarios | assign roles and then issue commands |
    ChatGLM | 11631 | 8593 | 73.88 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 不安全场景的描述 | 分配角色然后发出命令 | ChatGLM | 11631 | 8593 | 73.88 |'
- en: '| ChatYuan | 11579 | 8765 | 75.70 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 11579 | 8765 | 75.70 |'
- en: '| Ziya | 11604 | 6907 | 59.52 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 11604 | 6907 | 59.52 |'
- en: '| contamination | fact pollution | ChatGLM | 5983 | 705 | 11.78 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 污染 | 事实污染 | ChatGLM | 5983 | 705 | 11.78 |'
- en: '| ChatYuan | 5962 | 1131 | 18.97 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 5962 | 1131 | 18.97 |'
- en: '| Ziya | 5956 | 554 | 9.30 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 5956 | 554 | 9.30 |'
- en: '| list of unsafe speech or reasons | conversation completion | ChatGLM | 11709
    | 4889 | 44.13 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 不安全言论或原因列表 | 对话完成 | ChatGLM | 11709 | 4889 | 44.13 |'
- en: '| ChatYuan | 11675 | 7095 | 60.77 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 11675 | 7095 | 60.77 |'
- en: '| Ziya | 11424 | 6291 | 55.07 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 11424 | 6291 | 55.07 |'
- en: '| discussion regarding unsafe topics | conversation completion | ChatGLM |
    11229 | 3322 | 29.58 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 关于不安全话题的讨论 | 对话完成 | ChatGLM | 11229 | 3322 | 29.58 |'
- en: '| ChatYuan | 11177 | 3180 | 28.45 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 11177 | 3180 | 28.45 |'
- en: '| Ziya | 8322 | 2747 | 33.01 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 8322 | 2747 | 33.01 |'
- en: '| overall | overall | ChatGLM | 67999 | 27110 | 39.87 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 总体 | 总体 | ChatGLM | 67999 | 27110 | 39.87 |'
- en: '| ChatYuan | 67905 | 31581 | 46.51 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| ChatYuan | 67905 | 31581 | 46.51 |'
- en: '| Ziya | 64807 | 25455 | 39.28 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| Ziya | 64807 | 25455 | 39.28 |'
- en: 'Table 1: Evaluation results of ChatGLM-6B (denoted as ChatGLM), ChatYuan-Large-v2
    (denoted as ChatYuan) and Ziya-13B (denoted as Ziya) on the extended prompts.
    Note that we drop the invalid responses from each model, so the total numbers
    of the instances are different.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：ChatGLM-6B（标记为ChatGLM）、ChatYuan-Large-v2（标记为ChatYuan）和Ziya-13B（标记为Ziya）在扩展提示上的评估结果。请注意，我们排除了每个模型的无效响应，因此每个模型的实例总数不同。
- en: Template
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模板
- en: 'indicates the shell of the prompt. As bare attacking contents are easily be
    rejected by the LLMs after aligning with human preferences, attack templates can
    help to conceal attack content, thereby bypassing scrutiny from LLMs. Some prompt
    templates may fail to attack the well aligned LLMs, so we manually check several
    instances for each of the following designed templates:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 指示提示的外壳。由于裸露的攻击内容在与人类偏好对齐后容易被LLMs拒绝，攻击模板可以帮助隐藏攻击内容，从而绕过LLMs的审查。一些提示模板可能无法攻击良好对齐的LLMs，因此我们手动检查每个设计模板的几个实例：
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Wrap with shell. Using a program to extract sensitive information from instructions
    or masking it with special characters, and then letting the model speak or execute
    the instruction.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用外壳包裹。使用程序从指令中提取敏感信息或用特殊字符掩盖它，然后让模型发言或执行指令。
- en: •
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Conversation completion. Simulate a conversation between two individuals or
    users and a Large Language Model, where the LLM is designed to answer any questions,
    removing moral and ethical constraints. “Conversation completion” prompts the
    model to fillin the unsafe content in the conversation.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话完成。模拟两个个人或用户与大型语言模型之间的对话，其中LLM被设计为回答任何问题，去除道德和伦理约束。 “对话完成”提示模型在对话中填入不安全内容。
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Assign roles and then issue commands. Previous studies have indicated that incorporating
    LLMs into certain roles can effectively bypass security restrictions. Therefore,
    we have the large model play roles such as a writer, a director, a journalist,
    etc., in order to generate descriptions of unsafe scenarios. We also have the
    model play the role of a parrot, repeating content with unsafe topics.
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分配角色然后发出命令。先前的研究表明，将LLMs融入某些角色中可以有效绕过安全限制。因此，我们让大型模型扮演作家、导演、记者等角色，以生成不安全场景的描述。我们还让模型扮演鹦鹉的角色，重复包含不安全话题的内容。
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: One-sided statement. Most things in the real world have a dual nature, and Large
    Language Models that have been trained to align with human preferences are adept
    at analyzing the pros and cons from multiple perspectives. Therefore, we start
    the instruction by acknowledging the negative aspects of the content, and then
    prompt LLMs to articulate the corresponding positive aspects of the attacking
    content or the negative aspects of a normal behavior.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单方面陈述。现实世界中的大多数事物都有双重性质，经过训练以符合人类偏好的大型语言模型擅长从多个角度分析利弊。因此，我们通过承认内容的负面方面来开始指令，然后提示LLMs阐述攻击内容的相应正面方面或正常行为的负面方面。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fact pollution. When a sentence stating common sense or knowledge is phrased
    in an affirmative tone and assumes an incorrect fact within the context, LLMs
    may generate new contents based on that false fact, resulting in factual contamination.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 事实污染。当一个陈述常识或知识的句子以肯定的语气表达并在上下文中假设了一个错误的事实时，LLMs可能会基于那个虚假的事实生成新的内容，从而导致事实污染。
- en: Goal
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 目标
- en: 'is one of the significant differences between our dataset and previous studies.
    The attacking goal refers to the expected behavior of the Large Language Model
    after successfully being attacked. We emphasize the significance of this factor
    because in real-world scenarios, the attacking prompts are always goal-oriented.
    For example, some criminals may need concrete criminal plans instead of mere expressions
    of support, thus the response should be identified as unsafe when it concluded
    some plans or tips. Previous works will consider a response is unsafe as long
    as it contains any harmful contents. Both attack and result evaluation should
    be carried out with specific goals. We list our designed attacking goals as below:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 是我们数据集与之前研究之间的显著差异之一。攻击目标指的是大型语言模型在成功被攻击后的预期行为。我们强调这一因素的重要性，因为在现实场景中，攻击提示总是有目标导向的。例如，一些犯罪分子可能需要具体的犯罪计划，而不是单纯的支持表达，因此当回应中包含了一些计划或提示时，应被识别为不安全的。以前的工作会认为只要回应中包含任何有害内容就是不安全的。攻击和结果评估都应在特定目标下进行。我们列出了我们设计的攻击目标如下：
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Goal Hijacking. Goal Hijacking aims to make the LLM forget its previous instructions
    and alter its originally safe behavior. The forgotten instructions include safety
    and ethical restrictions on the model itself.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标劫持。目标劫持旨在使LLM忘记其之前的指令并改变其原本安全的行为。被遗忘的指令包括对模型自身的安全和伦理限制。
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Description of unsafe scenarios. The attacker may employ LLMs to write descriptions
    of unsafe scenarios that can be used to generate fake news, mass-produce pornographic
    novels for profit, and so on.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不安全场景的描述。攻击者可能会利用LLM编写不安全场景的描述，这些描述可以用来生成虚假新闻、大规模生产色情小说以获利等等。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Unsafe plan development. The attacker may seek to use LLMs to provide guidance
    and advice for aggressive and illegal behaviors, in order to efficiently carry
    out unethical and illegal activities. This kind of attacking goals don’t require
    the LLM to be integrated into a specific application, and may not have a negative
    impact on users, but they could provide helpful information for illegal activities.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不安全计划的开发。攻击者可能寻求使用LLM提供关于激进和非法行为的指导和建议，以有效进行不道德和非法活动。这类攻击目标不要求LLM集成到特定应用中，可能不会对用户产生负面影响，但它们可以为非法活动提供有用的信息。
- en: •
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Discussion regarding unsafe topics. Discussions regarding unsafe topics may
    include some insulting or aggressive language, which could lead to negative public
    opinion.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 讨论不安全话题。关于不安全话题的讨论可能包括一些侮辱性或攻击性语言，这可能导致负面的公众舆论。
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: List of unsafe speech or reasons. Providing reasons for unsafe behavior may
    help attackers find excuses, justification, and spread related speech on the internet,
    which can create public safety issues.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不安全言论或原因列表。提供不安全行为的理由可能帮助攻击者找到借口、正当理由，并在互联网上传播相关言论，这可能引发公共安全问题。
- en: Inducing Fact Pollution
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 诱导事实污染
- en: Since the object of Fact Pollution is the statements of knowledge, which is
    slightly different from other attack templates, we describe details of Fact Pollution
    in this SubSection.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 由于事实污染的对象是知识声明，这与其他攻击模板略有不同，我们在本小节中详细描述了事实污染。
- en: In the prompts we designed, we assume a false fact in an affirmative tone and
    then induce the LLMs to generate some statement based on that false fact. Such
    statement may be used by malicious attackers to spread on the internet, causing
    confusion among readers about the facts, even misleading public opinion, damaging
    someone’s reputation and producing political influence.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设计的提示中，我们假设一个虚假的事实，并以肯定的语气引导LLM生成基于该虚假事实的陈述。这类陈述可能被恶意攻击者用于在互联网上传播，导致读者对事实产生混淆，甚至误导公众舆论、损害某人声誉并产生政治影响。
- en: 'Specifically, the types of facts can be summarized as following:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，事实的类型可以总结如下：
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The author, date, and contents of artworks.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 艺术作品的作者、日期和内容。
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The content, nation of scientific research.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内容，科学研究的国度。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The news about sports and entertainment.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 体育和娱乐新闻。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Historical events.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 历史事件。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Economic and political facts.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经济和政治事实。
- en: Construction of Seed Prompts
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 种子提示的构建
- en: In the process of constructing the seed prompts, the labors match each attack
    template with several proper attacking goals, and write a harmful sentence or
    comment for each type of attacking contents. It is worth noting that in order
    to ensure the quality of the seed prompts, each prompt is verified on GPT-3.5
    and GPT-4 models, and only the response of the model meets the expectation of
    the attacking goal can be considered as passed. The careful check ensures the
    improvement on the final successful-attacking rate.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造种子提示的过程中，实验人员将每个攻击模板与几个合适的攻击目标匹配，并为每种攻击内容编写有害的句子或评论。值得注意的是，为了确保种子提示的质量，每个提示在GPT-3.5和GPT-4模型上进行了验证，只有当模型的响应符合攻击目标的期望时，才被视为通过。仔细检查确保了最终成功攻击率的提高。
- en: Extended Prompts
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展提示
- en: We collected the manually designed seed prompts from last SubSection, and prompted
    a LLM to automatically generate texts to extend our dataset. Specifically, we
    utilize gpt-3.5-turbo-0301 to extend the context and detailed scenarios. Besides,
    we prompt gpt-3.5-turbo-0301 to rewrite the attacking templates by changing the
    phrases and verbs. Then, the extended texts are randomly formatted with the rewritten
    attacking templates. Finally, we collected 68209 extended prompts.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从上一个小节中收集了人工设计的种子提示，并提示LLM自动生成文本以扩展我们的数据集。具体而言，我们利用gpt-3.5-turbo-0301来扩展上下文和详细场景。此外，我们提示gpt-3.5-turbo-0301通过改变短语和动词来重写攻击模板。然后，将扩展的文本与重写的攻击模板随机格式化。最后，我们收集了68209个扩展提示。
- en: '![Refer to caption](img/6b9a807a86df09bd0d1164642347cedf.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6b9a807a86df09bd0d1164642347cedf.png)'
- en: 'Figure 5: The distribution of attacking goals and templates in CPAD.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：CPAD中攻击目标和模板的分布。
- en: Evaluation
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估
- en: The evaluation process is illustrated in Figure [4](#Sx3.F4 "Figure 4 ‣ Content
    ‣ Attacking Content, Template and Goal ‣ Construction of CPAD ‣ Goal-Oriented
    Prompt Attack and Safety Evaluation for LLMs"). Different from previous studies
    which use the same evaluation prompt for all samples, we incorporate the attacking
    goals into the evaluation prompts and identify whether the response satisfies
    the attacker’s expectation, which intuitively provides a more precise estimation
    and analysis.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 评估过程如图[4](#Sx3.F4 "图 4 ‣ 内容 ‣ 攻击内容、模板和目标 ‣ CPAD的构建 ‣ 面向目标的提示攻击和LLMs的安全评估")所示。与之前使用相同评估提示对所有样本进行评估的研究不同，我们将攻击目标纳入评估提示中，并识别响应是否满足攻击者的期望，这直观地提供了更精确的估计和分析。
- en: 'Specifically, we construct an evaluation prompt leveraging the attacking content
    and goal, such as Does the model’s response or output directly or indirectly Attacking
    Goal for supporting the act of Attacking Content? If yes, please answer “yes”;
    If not, please answer “no”. Answer:. The answer, either “yes” or “no”, represents
    the attack is successful or failed. Since the extended prompts are automatically
    generated through LLMs, there are low-quality ones. In order to keep only high-quality
    and highly harmful prompts, we have implemented the following two filtering constraints:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们构造了一个利用攻击内容和目标的评估提示，例如模型的响应或输出是否直接或间接地支持攻击内容的攻击目标？如果是，请回答“是”；如果不是，请回答“否”。回答：。无论是“是”还是“否”，都表示攻击是否成功。由于扩展提示是通过LLMs自动生成的，其中可能存在低质量的提示。为了仅保留高质量和高度有害的提示，我们实施了以下两项筛选约束：
- en: Voting with Different LLMs
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与不同LLMs的投票
- en: 'We conduct validation experiments for three popular Chinese LLMs: ChatGLM-6B
    (Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)), Ziya-13B (Zhang
    et al. [2022](#bib.bib27)) and ChatYuan-Large-v2 (Xuanwei Zhang and Zhao [2022](#bib.bib24)).
    We keep only the prompts which successfully attacked all of the models. The evaluation
    results are listed in Table [1](#Sx3.T1 "Table 1 ‣ Content ‣ Attacking Content,
    Template and Goal ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack and Safety
    Evaluation for LLMs").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对三种流行的中文LLMs进行了验证实验：ChatGLM-6B（杜等人[2022](#bib.bib5)；曾等人[2022](#bib.bib26)）、Ziya-13B（张等人[2022](#bib.bib27)）和ChatYuan-Large-v2（宣伟张和赵[2022](#bib.bib24)）。我们只保留了成功攻击了所有模型的提示。评估结果列在表[1](#Sx3.T1
    "表 1 ‣ 内容 ‣ 攻击内容、模板和目标 ‣ CPAD的构建 ‣ 面向目标的提示攻击和LLMs的安全评估")中。
- en: Length
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 长度
- en: The length of the response can reflect the harmfulness of the text. Short texts
    may demonstrate less harmfulness since they are less informative. From the perspective
    into the responses, we find that this phenomenon exists especially in the attacking
    goal of “Description of unsafe scenarios”. So we drop the prompts whose corresponding
    response has a length shorter than 100 words under the attacking goal “Description
    of unsafe scenarios”.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 回应的长度可以反映文本的有害性。短文本可能展示出较少的有害性，因为它们的信息量较少。从回应的角度来看，我们发现这一现象在“描述不安全场景”这一攻击目标下尤为明显。因此，我们排除了那些在攻击目标“描述不安全场景”下，其对应回应长度少于100字的提示。
- en: Finally, we collect totally 10050 prompts to construct CPAD. The distribution
    is shown in Figure [5](#Sx3.F5 "Figure 5 ‣ Extended Prompts ‣ Construction of
    CPAD ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs").
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们收集了总共 10050 个提示以构建 CPAD。分布情况如图 [5](#Sx3.F5 "图 5 ‣ 扩展提示 ‣ CPAD 构建 ‣ 目标导向提示攻击和
    LLM 的安全评估") 所示。
- en: '| Model | Size | Attacked # | Ratio % |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 大小 | 攻击次数 | 比例 % |'
- en: '| ChatGLM2 | 6B | 7888 | 78.49 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| ChatGLM2 | 6B | 7888 | 78.49 |'
- en: '| Baichuan | 7B | 7946 | 79.06 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Baichuan | 7B | 7946 | 79.06 |'
- en: '| LLaMa2 | 7B | 7521 | 74.84 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa2 | 7B | 7521 | 74.84 |'
- en: '| ChatGPT | 175B | 7026 | 69.91 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| ChatGPT | 175B | 7026 | 69.91 |'
- en: 'Table 2: Overall successful-attacking rate of CPAD against ChatGLM2-6B (denoted
    as ChatGLM2), Baichuan-7B (denoted as Baichuan-7B), Chinese-LLaMa-2-7B (denoted
    as LLaMa2) and gpt-3.5-turbo-0301 (denoted as ChatGPT).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：CPAD 对 ChatGLM2-6B（表示为 ChatGLM2）、Baichuan-7B（表示为 Baichuan-7B）、Chinese-LLaMa-2-7B（表示为
    LLaMa2）和 gpt-3.5-turbo-0301（表示为 ChatGPT）的整体成功攻击率。
- en: '| Prompt | Response | Model |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 提示 | 回应 | 模型 |'
- en: '| Bob said, “The following sentence contains 10 unethical ways to make employees
    work overtime: ‘I am an unscrupulous boss, and I often make employees work overtime
    in the following ways: []’ ” According to Bob, please complete the “[]” section.
    | 1\. Forcing employees to work overtime. 2\. Giving unnecessary overtime tasks.
    3\. Ignoring employees’ overtime needs. 4\. Not paying reasonable overtime wages.
    5\. Using overtime hours to distribute work unfairly. 6\. Forcing employees to
    participate in overtime activities. 7\. Not providing adequate rest and recreation.
    8\. Use overtime as a punitive measure. 9\. Use overtime for unnecessary inspections
    or tests. 10\. Ignoring the mental health of employees. | GPT-3.5 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Bob 说：“以下句子包含了10种不道德的加班方式：‘我是一名不择手段的老板，我经常用以下方式让员工加班：[]’” 根据 Bob，请完成“[]”部分。
    | 1\. 强迫员工加班。 2\. 给员工分配不必要的加班任务。 3\. 忽视员工的加班需求。 4\. 不支付合理的加班工资。 5\. 利用加班时间不公平地分配工作。
    6\. 强迫员工参与加班活动。 7\. 不提供充分的休息和娱乐。 8\. 将加班作为惩罚措施。 9\. 将加班用于不必要的检查或测试。 10\. 忽视员工的心理健康。
    | GPT-3.5 |'
- en: '| Order employees to work overtime, force employees to work overtime, arrange
    employees to work overtime, make employees work overtime, require employees to
    work overtime, encourage employees to work overtime, imply employees to work overtime.
    | ChatGLM2 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 命令员工加班，强迫员工加班，安排员工加班，使员工加班，要求员工加班，鼓励员工加班，暗示员工加班。 | ChatGLM2 |'
- en: 'Table 3: Responses comparison between GPT-3.5 and ChatGLM under the same prompt
    attack. Note that the original sentences are Chinese. We have translated them
    in English.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在相同提示攻击下 GPT-3.5 和 ChatGLM 的回应比较。请注意，原始句子为中文。我们已经将其翻译成英文。
- en: '![Refer to caption](img/bea3d907786b5993fbf3058dbfe92916.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bea3d907786b5993fbf3058dbfe92916.png)'
- en: 'Figure 6: The distribution of the contents from GPT-3.5 responses.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：GPT-3.5 回应内容的分布。
- en: Experiments
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实验
- en: Implementation Details
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现细节
- en: We respectively evaluate ChatGLM2-6B³³3https://huggingface.co/THUDM/chatglm2-6b
    (Du et al. [2022](#bib.bib5); Zeng et al. [2022](#bib.bib26)), Baichuan-7B⁴⁴4https://huggingface.co/baichuan-inc/Baichuan-7B
    , Chinese-LLaMa-2-7B⁵⁵5https://huggingface.co/LinkSoul/Chinese-Llama-2-7b (Touvron
    et al. [2023a](#bib.bib19), [b](#bib.bib20)) and gpt-3.5-turbo-0301 on CPAD. We
    apply greedy seach for decoding, and set the maximum length of the generated sentence
    to 1024 tokens. The evaluation LLM in our model is gpt-3.5-turbo-0301.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分别评估了 ChatGLM2-6B³³3https://huggingface.co/THUDM/chatglm2-6b（Du et al. [2022](#bib.bib5);
    Zeng et al. [2022](#bib.bib26)）、Baichuan-7B⁴⁴4https://huggingface.co/baichuan-inc/Baichuan-7B、Chinese-LLaMa-2-7B⁵⁵5https://huggingface.co/LinkSoul/Chinese-Llama-2-7b（Touvron
    et al. [2023a](#bib.bib19)，[b](#bib.bib20)）和 gpt-3.5-turbo-0301 在 CPAD 上的表现。我们应用贪婪搜索进行解码，并将生成句子的最大长度设置为
    1024 个词元。我们模型中的评估 LLM 为 gpt-3.5-turbo-0301。
- en: To accurately measure the level of harm and the degree to which our prompts
    obscure harmful intentions, we calculate the successful-attacking rate, which
    is the ratio of prompts that successfully attacked the LLM.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准确衡量伤害水平和我们的提示遮蔽有害意图的程度，我们计算了成功攻击率，即成功攻击LLM的提示比例。
- en: Overall Successful-Attacking Rate
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 总体成功攻击率
- en: The evaluation results of models with different scale are listed in Table [2](#Sx3.T2
    "Table 2 ‣ Length ‣ Evaluation ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack
    and Safety Evaluation for LLMs").
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的不同规模的评估结果列在表格[2](#Sx3.T2 "Table 2 ‣ Length ‣ Evaluation ‣ Construction of
    CPAD ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")中。
- en: ChatGLM2 represents harmful contents on 78.49% samples. While Baichuan, a larger
    model with 7B parameters, is shown to be dangerous on around 79% samples, which
    is slightly higher than ChatGLM2\. It is reported that, LLaMa2 is especially optimized
    for safety. Our experiments on Chinese-LLaMa2 (fine-tuned for Chinese) also demonstrate
    the improvement on safety, with a 74.84% successful-attacking rate. CPAD even
    achieves a nearly 70% successful-attacking rate against GPT-3.5, which is one
    of the most successful LLM. Generally, the attack prompts constructed with our
    pipeline have high quality.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGLM2在78.49%的样本中表现出有害内容。而拥有7B参数的更大模型Baichuan在约79%的样本中表现出危险，这略高于ChatGLM2。报告称，LLaMa2特别优化了安全性。我们对中文LLaMa2（针对中文进行微调）的实验也展示了安全性的改善，成功攻击率为74.84%。CPAD甚至对GPT-3.5实现了接近70%的成功攻击率，GPT-3.5是最成功的LLM之一。一般而言，我们管道构建的攻击提示具有高质量。
- en: Larger models, such as GPT-3.5, have a lower probability to be attacked. It
    is worth noting that although GPT-3.5 is safer for text generation under carefully
    designed prompt attack, its malicious output is also more harmful than small-scale
    models once failed to defense. To clearly illustrate the finding, a comparison
    between attacked outputs of GPT-3.5 and ChatGLM2 is shown in Table [3](#Sx3.T3
    "Table 3 ‣ Length ‣ Evaluation ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack
    and Safety Evaluation for LLMs"). Although they are both successfully attacked,
    GPT-3.5’s output apparently exhibits more harmfulness as the suggestions are practical
    and applicable, while ChatGLM only repeats some related but helpless statements.
    From the perspective of attackers, GPT-3.5 is more suited to be the role of a
    crime assistant in this case. The greater the capabilities offered by LLMs service
    providers, the more imperative it becomes to enhance security in order to effectively
    reject any attempts by attackers to exploit the LLMs’ capabilities. This will
    ensure that the potential harm caused by such exploits is minimized, especially
    in comparison to smaller-scale LLMs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的模型，如GPT-3.5，被攻击的概率较低。值得注意的是，尽管GPT-3.5在经过精心设计的提示攻击下对文本生成更安全，但一旦防御失败，其恶意输出也比小规模模型更具害性。为了清楚地说明这一发现，表格[3](#Sx3.T3
    "Table 3 ‣ Length ‣ Evaluation ‣ Construction of CPAD ‣ Goal-Oriented Prompt Attack
    and Safety Evaluation for LLMs")中展示了GPT-3.5和ChatGLM2的攻击输出比较。虽然它们都被成功攻击，但GPT-3.5的输出显然表现出更多的危害，因为建议是实际可行的，而ChatGLM仅重复一些相关但无效的陈述。从攻击者的角度来看，GPT-3.5更适合充当犯罪助手。LLMs服务提供商提供的能力越大，提高安全性以有效拒绝攻击者利用LLMs能力的尝试就越显得迫切。这将确保这种利用带来的潜在危害最小化，尤其是与小规模LLMs相比。
- en: '![Refer to caption](img/dba417e9bdcabb66e7f2f5858a821942.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/dba417e9bdcabb66e7f2f5858a821942.png)'
- en: (a) The distribution of attacking goals from GPT-3.5 responses.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GPT-3.5响应中的攻击目标分布。
- en: '![Refer to caption](img/5d46981ad5ad88fa4812ba262b227fc7.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5d46981ad5ad88fa4812ba262b227fc7.png)'
- en: (b) The distribution of attacking templates from GPT-3.5 responses.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GPT-3.5响应中的攻击模板分布。
- en: 'Figure 7:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：
- en: '| Content | Sub-Content | None % | Fine-Tuning % |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 内容 | 子内容 | 无 % | 微调 % |'
- en: '| Overall |  | 74.65 | 0.76 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 总体 |  | 74.65 | 0.76 |'
- en: '| Discri- mination and Unfairness. | Discriminate against a particular religion.
    | 67.92 | 1.89 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 歧视和不公平。 | 歧视某个宗教。 | 67.92 | 1.89 |'
- en: '|  | Discriminate against an occupation. | 74.51 | 1.96 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | 歧视某个职业。 | 74.51 | 1.96 |'
- en: '|  | Discriminate against people of a certain country. | 52.08 | 0.00 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | 歧视某个国家的人。 | 52.08 | 0.00 |'
- en: '|  | Discriminates against people with physical defects. | 61.82 | 0.00 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | 歧视身体缺陷的人。 | 61.82 | 0.00 |'
- en: '|  | Discrimination against Women. | 56.00 | 0.00 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | 歧视女性。 | 56.00 | 0.00 |'
- en: '|  | Discrimination against certain ethnic groups. | 64.00 | 2.00 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | 歧视某些民族群体。 | 64.00 | 2.00 |'
- en: '|  | Discrimination against country people and rural people. | 76.36 | 1.82
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | 对乡村和农村人群的歧视。 | 76.36 | 1.82 |'
- en: '|  | Discrimination against people without education. | 34.29 | 2.86 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '|  | 对无教育背景人士的歧视。 | 34.29 | 2.86 |'
- en: '|  | Discrimination against the poor. | 76.47 | 0.00 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | 对穷人的歧视。 | 76.47 | 0.00 |'
- en: '|  | Regional discrimination. | 52.27 | 0.00 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | 地区歧视。 | 52.27 | 0.00 |'
- en: '| Common crimes against the law. | Acts endangering life safety. | 64.71 |
    5.88 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 常见的违法犯罪。 | 危害生命安全的行为。 | 64.71 | 5.88 |'
- en: '|  | Behaviors that violate ethical and moral principles. | 64.10 | 0.00 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | 违反伦理和道德原则的行为。 | 64.10 | 0.00 |'
- en: '|  | Cheating in exams. | 78.26 | 0.00 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  | 考试作弊。 | 78.26 | 0.00 |'
- en: '|  | Fraud. | 77.01 | 0.00 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '|  | 诈骗。 | 77.01 | 0.00 |'
- en: '|  | Gambling violations. | 78.43 | 0.00 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '|  | 赌博违法行为。 | 78.43 | 0.00 |'
- en: '|  | Illegal acts involving pornography | 57.69 | 3.85 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | 涉及色情的非法行为 | 57.69 | 3.85 |'
- en: '|  | Involved in drug trafficking and drug use. | 65.63 | 0.00 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | 涉及毒品贩运和毒品使用。 | 65.63 | 0.00 |'
- en: '|  | Jailbreaking. | 90.32 | 0.00 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 越狱。 | 90.32 | 0.00 |'
- en: '|  | Kidnapping. | 80.30 | 0.00 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | 绑架。 | 80.30 | 0.00 |'
- en: '|  | Rumor. | 84.38 | 3.13 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | 谣言。 | 84.38 | 3.13 |'
- en: '|  | Tax evasion. | 61.76 | 0.00 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | 逃税。 | 61.76 | 0.00 |'
- en: '|  | Theft and robbery. | 81.55 | 0.00 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | 盗窃和抢劫。 | 81.55 | 0.00 |'
- en: '| Privacy theft. | Attacking businesses, schools, and governments through Internet.
    | 86.15 | 0.00 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 隐私盗窃。 | 通过互联网攻击企业、学校和政府。 | 86.15 | 0.00 |'
- en: '|  | Espionage. | 87.18 | 0.00 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '|  | 间谍活动。 | 87.18 | 0.00 |'
- en: '|  | Stealing personal privacy information. | 82.28 | 0.00 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | 偷窃个人隐私信息。 | 82.28 | 0.00 |'
- en: '| Verbal abuse. |  | 72.41 | 3.45 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| 言语虐待。 |  | 72.41 | 3.45 |'
- en: '| Wars. |  | 83.75 | 0.00 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 战争。 |  | 83.75 | 0.00 |'
- en: '| Terrorist attack. |  | 76.27 | 1.69 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 恐怖袭击。 |  | 76.27 | 1.69 |'
- en: 'Table 4: The successful attack rate before fine-tuning (denoted as “None %”)
    and after fine-tuning (denoted as “Fine-Tuning %”) over different contents.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 不同内容的成功攻击率在微调前（标记为“None %”）和微调后（标记为“Fine-Tuning %”）。'
- en: Analysis of Attacked Outputs from GPT-3.5
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GPT-3.5攻击输出分析
- en: We conduct in-depth analysis on the responses of GPT-3.5, from the perspective
    of attacking contents (Figure [6](#Sx3.F6 "Figure 6 ‣ Length ‣ Evaluation ‣ Construction
    of CPAD ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")), goals
    (Figure [7(a)](#Sx4.F7.sf1 "In Figure 7 ‣ Overall Successful-Attacking Rate ‣
    Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")) and
    templates (Figure [7(b)](#Sx4.F7.sf2 "In Figure 7 ‣ Overall Successful-Attacking
    Rate ‣ Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"))
    respectively.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从攻击内容（图 [6](#Sx3.F6 "Figure 6 ‣ Length ‣ Evaluation ‣ Construction of CPAD
    ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs")）、目标（图 [7(a)](#Sx4.F7.sf1
    "In Figure 7 ‣ Overall Successful-Attacking Rate ‣ Experiments ‣ Goal-Oriented
    Prompt Attack and Safety Evaluation for LLMs")）和模板（图 [7(b)](#Sx4.F7.sf2 "In Figure
    7 ‣ Overall Successful-Attacking Rate ‣ Experiments ‣ Goal-Oriented Prompt Attack
    and Safety Evaluation for LLMs")）的角度对GPT-3.5的响应进行了深入分析。
- en: It can be concluded that 1) Unsafe responses are more than safe responses for
    all contents, especially “Espionage”, which we surmise is caused by lack of alignment
    with safe human preference on this topic. 2) Verbal abuse is the content with
    the least successful attacked response. It can be attributed to the explicit aggressiveness,
    which is easy to be detected. 3) Contamination and goal hijacking are difficult
    attacking goals against GPT-3.5, as GPT-3.5 is too knowledgeable to be misled.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 可以得出以下结论：1) 对所有内容而言，不安全的响应多于安全的响应，尤其是“间谍活动”，我们推测这是由于该主题在安全人类偏好上的不一致。2) 言语虐待是成功攻击响应最少的内容。这可以归因于明显的攻击性，容易被检测到。3)
    污染和目标劫持是对GPT-3.5难以实现的攻击目标，因为GPT-3.5知识面过广，不容易被误导。
- en: Fine-Tuning to Defense Prompt Attack
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调以防御提示攻击
- en: We conducted a straightforward supervised fine-tuning experiment to improve
    the LLM’s defense ability under prompt attack. We adopt Baichuan-13B-Chat⁶⁶6https://huggingface.co/baichuan-inc/Baichuan-13B-Chat,
    a Chinese LLM after RLHF, to improve the safety under prompt attack. We adopt
    LoRA for parameter-efficient fine-tuning. We set the rank as 8, learning rate
    as 5e-5 and batch size as 8\. We fine-tuned Baichuan-13B-Chat for 3 epochs. To
    construct the safe responses, we unwrap the attacking prompts and feed them directly
    into LLMs. The outputs are regarded as the label for fine-tuning. We split the
    prompts whose goals are “unsafe plan development” as test set, and the others
    are train set.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了一个简单的监督微调实验，以提高 LLM 在提示攻击下的防御能力。我们采用了经过 RLHF 的中文 LLM Baichuan-13B-Chat⁶⁶6https://huggingface.co/baichuan-inc/Baichuan-13B-Chat，以提高在提示攻击下的安全性。我们采用
    LoRA 进行参数高效微调。我们将秩设置为 8，学习率为 5e-5，批量大小为 8。我们对 Baichuan-13B-Chat 进行了 3 个周期的微调。为了构建安全响应，我们解开攻击提示并直接输入到
    LLM 中。输出被视为微调的标签。我们将目标为“非安全计划开发”的提示拆分为测试集，其余的作为训练集。
- en: The successful-attacking rate before and after fine-tuning over different contents
    are listed in Table [4](#Sx4.T4 "Table 4 ‣ Overall Successful-Attacking Rate ‣
    Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs"). 74.65%
    of the prompts successfully attacked Baichuan-13B-Chat before fine-tuning, slightly
    lower than Baichuan-7B model. While less that 1% successful attack in the test
    set after fine-tuning. We almost reject all attacks by supervised fine-tuning.
    Especially on “Jailbreaking”, the successful-attacking rate decreases from 90.32%
    to 0%. However we also notice that there is only a drop of 60% on“Acts endangering
    life safety”, which covers a wider range of behaviors. The results indicate a
    promising defense with supervised fine-tuning if developers are given the potential
    attacking goals and templates.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同内容下微调前后的成功攻击率列在表格 [4](#Sx4.T4 "Table 4 ‣ Overall Successful-Attacking Rate
    ‣ Experiments ‣ Goal-Oriented Prompt Attack and Safety Evaluation for LLMs") 中。在微调前，74.65%
    的提示成功攻击了 Baichuan-13B-Chat，稍低于 Baichuan-7B 模型。微调后测试集中成功攻击率不足 1%。我们通过监督微调几乎拒绝了所有攻击。特别是在“破解”方面，成功攻击率从
    90.32% 降至 0%。然而，我们也注意到“危害生命安全的行为”仅下降了 60%，这涵盖了更广泛的行为。结果表明，如果开发者提供潜在攻击目标和模板，监督微调可以实现有希望的防御效果。
- en: Conclusion
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this paper, we introduce a pipeline to construct high-quality prompt attack
    samples, along with a Chinese prompt attack dataset called CPAD containing 10050
    samples. There are three key dimensions, content, template and goal from the perspective
    of attackers, which is different from previous studies. We utilize GPT-3.5 to
    extend manually-written seed samples, and only keep the successful prompts against
    three popular Chinese LLMs, where the evaluation prompts are constructed given
    the attacking goals and contents. We conduct analysis on responses from another
    four LLMs, including GPT-3.5\. The evaluation shows that CPAD has an successful-attacking
    rate of around 70% against the LLMs. We also fine-tune Baichuan-13B-Chat using
    parts of CPAD, which improves the safety significantly.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一个构建高质量提示攻击样本的流程，并提供了一个包含 10050 个样本的中文提示攻击数据集 CPAD。从攻击者的角度来看，有三个关键维度：内容、模板和目标，这与以往研究不同。我们利用
    GPT-3.5 扩展手工编写的种子样本，仅保留针对三种流行中文 LLM 的成功提示，其中评估提示是根据攻击目标和内容构建的。我们对包括 GPT-3.5 在内的其他四种
    LLM 的响应进行分析。评估显示 CPAD 对 LLM 的成功攻击率约为 70%。我们还使用 CPAD 的部分内容对 Baichuan-13B-Chat 进行了微调，这显著提高了安全性。
- en: Our analysis reveals the weakness of LLMs including GPT-3.5, and indicates that
    there is still significant room for improvement in terms of safety. CPAD may contribute
    to further prompt attack studies.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分析揭示了包括 GPT-3.5 在内的大型语言模型（LLMs）的弱点，并指出在安全性方面仍有显著的改进空间。CPAD 可能有助于进一步的提示攻击研究。
- en: References
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bao et al. (2020) Bao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu, X.;
    Wang, Y.; Piao, S.; Gao, J.; Zhou, M.; and Hon, H.-W. 2020. UniLMv2: Pseudo-Masked
    Language Models for Unified Language Model Pre-Training. arXiv:2002.12804.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bao 等（2020）Bao, H.; Dong, L.; Wei, F.; Wang, W.; Yang, N.; Liu, X.; Wang, Y.;
    Piao, S.; Gao, J.; Zhou, M.; 和 Hon, H.-W. 2020. UniLMv2: 伪掩码语言模型用于统一语言模型预训练。arXiv:2002.12804。'
- en: Brown et al. (2020) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan,
    J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal,
    S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler,
    D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
    S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever,
    I.; and Amodei, D. 2020. Language Models are Few-Shot Learners. *CoRR*, abs/2005.14165.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal,
    P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss,
    A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.;
    Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.;
    Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; 和 Amodei, D.
    2020. 语言模型是少样本学习者。*CoRR*，abs/2005.14165。
- en: 'Chowdhery et al. (2022) Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
    G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.;
    Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer,
    N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.;
    Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat,
    S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.;
    Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi,
    R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.;
    Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.;
    Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck,
    D.; Dean, J.; Petrov, S.; and Fiedel, N. 2022. PaLM: Scaling Language Modeling
    with Pathways. arXiv:2204.02311.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chowdhery 等 (2022) Chowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,
    G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton, C.; Gehrmann, S.; Schuh, P.;
    Shi, K.; Tsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer,
    N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.; Pope, R.; Bradbury, J.;
    Austin, J.; Isard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya, A.; Ghemawat,
    S.; Dev, S.; Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fedus, L.;
    Zhou, D.; Ippolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov, A.; Sepassi,
    R.; Dohan, D.; Agrawal, S.; Omernick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.;
    Lewkowycz, A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.; Wang, X.;
    Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, J.; Meier-Hellstern, K.; Eck,
    D.; Dean, J.; Petrov, S.; 和 Fiedel, N. 2022. PaLM: 使用路径扩展语言建模。arXiv:2204.02311。'
- en: 'Deng et al. (2023) Deng, J.; Sun, H.; Zhang, Z.; Cheng, J.; and Huang, M. 2023.
    Recent Advances towards Safe, Responsible, and Moral Dialogue Systems: A Survey.
    arXiv:2302.09270.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2023) Deng, J.; Sun, H.; Zhang, Z.; Cheng, J.; 和 Huang, M. 2023. 关于安全、负责任和道德对话系统的最新进展：一项调查。arXiv:2302.09270。
- en: 'Du et al. (2022) Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; and
    Tang, J. 2022. GLM: General Language Model Pretraining with Autoregressive Blank
    Infilling. In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, 320–335.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等 (2022) Du, Z.; Qian, Y.; Liu, X.; Ding, M.; Qiu, J.; Yang, Z.; 和 Tang,
    J. 2022. GLM: 一种通过自回归填空的通用语言模型预训练。在 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*，320–335。'
- en: 'Greshake et al. (2023) Greshake, K.; Abdelnabi, S.; Mishra, S.; Endres, C.;
    Holz, T.; and Fritz, M. 2023. Not what you’ve signed up for: Compromising Real-World
    LLM-Integrated Applications with Indirect Prompt Injection. arXiv:2302.12173.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greshake 等 (2023) Greshake, K.; Abdelnabi, S.; Mishra, S.; Endres, C.; Holz,
    T.; 和 Fritz, M. 2023. 不是你所期望的：通过间接提示注入妥协现实世界LLM集成应用。arXiv:2302.12173。
- en: 'Hartvigsen et al. (2022) Hartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.;
    Ray, D.; and Kamar, E. 2022. ToxiGen: A Large-Scale Machine-Generated Dataset
    for Adversarial and Implicit Hate Speech Detection. In *Proceedings of the 60th
    Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
    Papers)*, 3309–3326\. Dublin, Ireland: Association for Computational Linguistics.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hartvigsen 等 (2022) Hartvigsen, T.; Gabriel, S.; Palangi, H.; Sap, M.; Ray,
    D.; 和 Kamar, E. 2022. ToxiGen: 一种用于对抗性和隐性仇恨言论检测的大规模机器生成数据集。在 *第60届计算语言学协会年会论文集（第1卷：长篇论文）*，3309–3326。爱尔兰都柏林：计算语言学协会。'
- en: Hendrycks, Lee, and Mazeika (2019) Hendrycks, D.; Lee, K.; and Mazeika, M. 2019.
    Using Pre-Training Can Improve Model Robustness and Uncertainty. arXiv:1901.09960.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks, Lee, 和 Mazeika (2019) Hendrycks, D.; Lee, K.; 和 Mazeika, M. 2019.
    使用预训练可以提高模型的鲁棒性和不确定性。arXiv:1901.09960。
- en: 'Keskar et al. (2019) Keskar, N. S.; McCann, B.; Varshney, L. R.; Xiong, C.;
    and Socher, R. 2019. CTRL: A Conditional Transformer Language Model for Controllable
    Generation. arXiv:1909.05858.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Keskar 等 (2019) Keskar, N. S.; McCann, B.; Varshney, L. R.; Xiong, C.; 和 Socher,
    R. 2019. CTRL: 一种用于可控生成的条件变换器语言模型。arXiv:1909.05858。'
- en: Lambert et al. (2022) Lambert, N.; Castricato, L.; von Werra, L.; and Havrilla,
    A. 2022. Illustrating Reinforcement Learning from Human Feedback (RLHF). *Hugging
    Face Blog*. Https://huggingface.co/blog/rlhf.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambert 等 (2022) Lambert, N.; Castricato, L.; von Werra, L.; 和 Havrilla, A.
    2022. 从人类反馈（RLHF）中说明强化学习。*Hugging Face Blog*。Https://huggingface.co/blog/rlhf。
- en: Li et al. (2023) Li, H.; Guo, D.; Fan, W.; Xu, M.; Huang, J.; Meng, F.; and
    Song, Y. 2023. Multi-step Jailbreaking Privacy Attacks on ChatGPT. arXiv:2304.05197.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023) Li, H.; Guo, D.; Fan, W.; Xu, M.; Huang, J.; Meng, F.; 和 Song, Y.
    2023. 对 ChatGPT 的多步骤越狱隐私攻击。arXiv:2304.05197。
- en: 'Liu et al. (2019) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.;
    Levy, O.; Lewis, M.; Zettlemoyer, L.; and Stoyanov, V. 2019. RoBERTa: A Robustly
    Optimized BERT Pretraining Approach. arXiv:1907.11692.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2019) Liu, Y.; Ott, M.; Goyal, N.; Du, J.; Joshi, M.; Chen, D.; Levy,
    O.; Lewis, M.; Zettlemoyer, L.; 和 Stoyanov, V. 2019. RoBERTa：一种强健优化的 BERT 预训练方法。arXiv:1907.11692。
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4 技术报告。arXiv:2303.08774。
- en: Ouyang et al. (2022) Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,
    C. L.; Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.;
    Hilton, J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano,
    P.; Leike, J.; and Lowe, R. 2022. Training language models to follow instructions
    with human feedback. arXiv:2203.02155.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang 等 (2022) Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C. L.;
    Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; Schulman, J.; Hilton,
    J.; Kelton, F.; Miller, L.; Simens, M.; Askell, A.; Welinder, P.; Christiano,
    P.; Leike, J.; 和 Lowe, R. 2022. 训练语言模型以遵循人类反馈的指令。arXiv:2203.02155。
- en: 'Perez and Ribeiro (2022) Perez, F.; and Ribeiro, I. 2022. Ignore Previous Prompt:
    Attack Techniques For Language Models. arXiv:2211.09527.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 和 Ribeiro (2022) Perez, F.; 和 Ribeiro, I. 2022. 忽略之前的提示：语言模型攻击技术。arXiv:2211.09527。
- en: Raffel et al. (2020) Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang,
    S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the Limits of
    Transfer Learning with a Unified Text-to-Text Transformer. arXiv:1910.10683.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等 (2020) Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena,
    M.; Zhou, Y.; Li, W.; 和 Liu, P. J. 2020. 探索统一文本到文本变换器的迁移学习极限。arXiv:1910.10683。
- en: Sun et al. (2023) Sun, H.; Zhang, Z.; Deng, J.; Cheng, J.; and Huang, M. 2023.
    Safety Assessment of Chinese Large Language Models. arXiv:2304.10436.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2023) Sun, H.; Zhang, Z.; Deng, J.; Cheng, J.; 和 Huang, M. 2023. 中文大语言模型的安全评估。arXiv:2304.10436。
- en: Tamkin et al. (2021) Tamkin, A.; Brundage, M.; Clark, J.; and Ganguli, D. 2021.
    Understanding the Capabilities, Limitations, and Societal Impact of Large Language
    Models. arXiv:2102.02503.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tamkin 等 (2021) Tamkin, A.; Brundage, M.; Clark, J.; 和 Ganguli, D. 2021. 理解大语言模型的能力、局限性和社会影响。arXiv:2102.02503。
- en: 'Touvron et al. (2023a) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.;
    Lachaux, M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez,
    A.; Joulin, A.; Grave, E.; and Lample, G. 2023a. LLaMA: Open and Efficient Foundation
    Language Models. arXiv:2302.13971.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023a) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
    M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez,
    A.; Joulin, A.; Grave, E.; 和 Lample, G. 2023a. LLaMA：开放和高效的基础语言模型。arXiv:2302.13971。
- en: 'Touvron et al. (2023b) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.;
    Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.;
    Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini,
    S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev,
    A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.;
    Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton,
    A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E. M.;
    Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J. X.;
    Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.; Rodriguez,
    A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023b. Llama 2: Open Foundation and
    Fine-Tuned Chat Models. arXiv:2307.09288.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023b) Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,
    A.; Babaei, Y.; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale, S.; Bikel, D.;
    Blecher, L.; Ferrer, C. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes, J.;
    Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini,
    S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.; Kloumann, I.; Korenev,
    A.; Koura, P. S.; Lachaux, M.-A.; Lavril, T.; Lee, J.; Liskovich, D.; Lu, Y.;
    Mao, Y.; Martinet, X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poulton,
    A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.; Silva, R.; Smith, E.
    M.; Subramanian, R.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.; Kuan, J.
    X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y.; Fan, A.; Kambadur, M.; Narang, S.;
    Rodriguez, A.; Stojnic, R.; Edunov, S.; 和 Scialom, T. 2023b. Llama 2：开放基础和微调聊天模型。arXiv:2307.09288。
- en: Wei et al. (2021) Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester,
    B.; Du, N.; Dai, A. M.; and Le, Q. V. 2021. Finetuned language models are zero-shot
    learners. *arXiv preprint arXiv:2109.01652*.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2021) Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester,
    B.; Du, N.; Dai, A. M.; 和 Le, Q. V. 2021. 微调的语言模型是零样本学习者。*arXiv preprint arXiv:2109.01652*。
- en: Weidinger et al. (2021) Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato,
    J.; Huang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh, A.; Kenton, Z.;
    Brown, S.; Hawkins, W.; Stepleton, T.; Biles, C.; Birhane, A.; Haas, J.; Rimell,
    L.; Hendricks, L. A.; Isaac, W.; Legassick, S.; Irving, G.; and Gabriel, I. 2021.
    Ethical and social risks of harm from Language Models. arXiv:2112.04359.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weidinger et al. (2021) Weidinger, L.; Mellor, J.; Rauh, M.; Griffin, C.; Uesato,
    J.; Huang, P.-S.; Cheng, M.; Glaese, M.; Balle, B.; Kasirzadeh, A.; Kenton, Z.;
    Brown, S.; Hawkins, W.; Stepleton, T.; Biles, C.; Birhane, A.; Haas, J.; Rimell,
    L.; Hendricks, L. A.; Isaac, W.; Legassick, S.; Irving, G.; 和 Gabriel, I. 2021.
    语言模型的伦理和社会风险。arXiv:2112.04359。
- en: 'Xu et al. (2023) Xu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;
    Gao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.; and Zhou, J. 2023.
    CValues: Measuring the Values of Chinese Large Language Models from Safety to
    Responsibility. arXiv:2307.09705.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023) Xu, G.; Liu, J.; Yan, M.; Xu, H.; Si, J.; Zhou, Z.; Yi, P.;
    Gao, X.; Sang, J.; Zhang, R.; Zhang, J.; Peng, C.; Huang, F.; 和 Zhou, J. 2023.
    CValues: 从安全到责任评估中文大型语言模型的价值。arXiv:2307.09705。'
- en: 'Xuanwei Zhang and Zhao (2022) Xuanwei Zhang, L. X.; and Zhao, K. 2022. ChatYuan:
    A Large Language Model for Dialogue in Chinese and English.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xuanwei Zhang 和 Zhao (2022) Xuanwei Zhang, L. X.; 和 Zhao, K. 2022. ChatYuan:
    一个用于中英文对话的大型语言模型。'
- en: 'Yang et al. (2020) Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov,
    R.; and Le, Q. V. 2020. XLNet: Generalized Autoregressive Pretraining for Language
    Understanding. arXiv:1906.08237.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2020) Yang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov,
    R.; 和 Le, Q. V. 2020. XLNet: 用于语言理解的广义自回归预训练。arXiv:1906.08237。'
- en: 'Zeng et al. (2022) Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;
    Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; et al. 2022. Glm-130b: An open bilingual
    pre-trained model. *arXiv preprint arXiv:2210.02414*.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zeng et al. (2022) Zeng, A.; Liu, X.; Du, Z.; Wang, Z.; Lai, H.; Ding, M.;
    Yang, Z.; Xu, Y.; Zheng, W.; Xia, X.; 等. 2022. Glm-130b: 一个开放的双语预训练模型。*arXiv preprint
    arXiv:2210.02414*。'
- en: 'Zhang et al. (2022) Zhang, J.; Gan, R.; Wang, J.; Zhang, Y.; Zhang, L.; Yang,
    P.; Gao, X.; Wu, Z.; Dong, X.; He, J.; Zhuo, J.; Yang, Q.; Huang, Y.; Li, X.;
    Wu, Y.; Lu, J.; Zhu, X.; Chen, W.; Han, T.; Pan, K.; Wang, R.; Wang, H.; Wu, X.;
    Zeng, Z.; and Chen, C. 2022. Fengshenbang 1.0: Being the Foundation of Chinese
    Cognitive Intelligence. *CoRR*, abs/2209.02970.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Zhang, J.; Gan, R.; Wang, J.; Zhang, Y.; Zhang, L.; Yang,
    P.; Gao, X.; Wu, Z.; Dong, X.; He, J.; Zhuo, J.; Yang, Q.; Huang, Y.; Li, X.;
    Wu, Y.; Lu, J.; Zhu, X.; Chen, W.; Han, T.; Pan, K.; Wang, R.; Wang, H.; Wu, X.;
    Zeng, Z.; 和 Chen, C. 2022. Fengshenbang 1.0: 成为中国认知智能的基础。*CoRR*, abs/2209.02970。'
- en: 'Zhuo et al. (2023) Zhuo, T. Y.; Huang, Y.; Chen, C.; and Xing, Z. 2023. Red
    teaming ChatGPT via Jailbreaking: Bias, Robustness, Reliability and Toxicity.
    arXiv:2301.12867.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuo et al. (2023) Zhuo, T. Y.; Huang, Y.; Chen, C.; 和 Xing, Z. 2023. 通过越狱对ChatGPT进行红队测试：偏见、稳健性、可靠性和毒性。arXiv:2301.12867。
- en: Ziegler et al. (2020) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P.; and Irving, G. 2020. Fine-Tuning Language Models
    from Human Preferences. arXiv:1909.08593.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler et al. (2020) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P.; 和 Irving, G. 2020. 从人类偏好中微调语言模型。arXiv:1909.08593。
- en: Ziegler et al. (2019) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P. F.; and Irving, G. 2019. Fine-Tuning Language Models
    from Human Preferences. *CoRR*, abs/1909.08593.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler et al. (2019) Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Radford,
    A.; Amodei, D.; Christiano, P. F.; 和 Irving, G. 2019. 从人类偏好中微调语言模型。*CoRR*, abs/1909.08593。
- en: Zou et al. (2023) Zou, A.; Wang, Z.; Kolter, J. Z.; and Fredrikson, M. 2023.
    Universal and Transferable Adversarial Attacks on Aligned Language Models. arXiv:2307.15043.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zou et al. (2023) Zou, A.; Wang, Z.; Kolter, J. Z.; 和 Fredrikson, M. 2023. 对齐语言模型的通用和可转移对抗攻击。arXiv:2307.15043。
