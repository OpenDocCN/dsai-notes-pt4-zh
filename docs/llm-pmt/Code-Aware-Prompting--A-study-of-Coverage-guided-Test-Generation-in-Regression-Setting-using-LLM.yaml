- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码感知提示：基于LLM的回归设置中覆盖指导测试生成的研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.00097](https://ar5iv.labs.arxiv.org/html/2402.00097)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.00097](https://ar5iv.labs.arxiv.org/html/2402.00097)
- en: 'Gabriel Ryan [gabe@cs.columbia.edu](mailto:gabe@cs.columbia.edu) Columbia University
    ,  Siddhartha Jain [siddjin@amazon.com](mailto:siddjin@amazon.com) ,  Mingyue
    Shang [myshang@amazon.com](mailto:myshang@amazon.com) ,  Shiqi Wang [wshiqi@amazon.com](mailto:wshiqi@amazon.com)
    ,  Xiaofei Ma [xiaofeim@amazon.com](mailto:xiaofeim@amazon.com) ,  Murali Krishna
    Ramanathan [mkraman@amazon.com](mailto:mkraman@amazon.com)  and  Baishakhi Ray
    [rabaisha@amazon.com](mailto:rabaisha@amazon.com) AWS AI Labs(2023; 20 February
    2007; 12 March 2009; 5 June 2009; Date: August 2023)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Gabriel Ryan [gabe@cs.columbia.edu](mailto:gabe@cs.columbia.edu) 哥伦比亚大学，Siddhartha
    Jain [siddjin@amazon.com](mailto:siddjin@amazon.com)，Mingyue Shang [myshang@amazon.com](mailto:myshang@amazon.com)，Shiqi
    Wang [wshiqi@amazon.com](mailto:wshiqi@amazon.com)，Xiaofei Ma [xiaofeim@amazon.com](mailto:xiaofeim@amazon.com)，Murali
    Krishna Ramanathan [mkraman@amazon.com](mailto:mkraman@amazon.com) 和 Baishakhi
    Ray [rabaisha@amazon.com](mailto:rabaisha@amazon.com) AWS AI Labs（2023；2007年2月20日；2009年3月12日；2009年6月5日；日期：2023年8月）
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Testing plays a pivotal role in ensuring software quality, yet conventional
    Search Based Software Testing (SBST) methods often struggle with complex software
    units, achieving suboptimal test coverage. Recent work using large language models
    (LLMs) for test generation have focused on improving generation quality through
    optimizing the test generation context and correcting errors in model outputs,
    but use fixed prompting strategies that prompt the model to generate tests without
    additional guidance. As a result LLM-generated testsuites still suffer from low
    coverage.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 测试在确保软件质量方面发挥着关键作用，但传统的基于搜索的软件测试（SBST）方法在处理复杂软件单元时常常难以实现理想的测试覆盖率。最近，利用大型语言模型（LLMs）进行测试生成的研究着重于通过优化测试生成上下文和纠正模型输出中的错误来提高生成质量，但使用固定的提示策略，促使模型生成测试而没有额外的指导。因此，LLM生成的测试套件仍然存在低覆盖率的问题。
- en: In this paper, we present SymPrompt, a code-aware prompting strategy for LLMs
    in test generation. SymPrompt’s approach is based on recent work that demonstrates
    LLMs can solve more complex logical problems when prompted to reason about the
    problem in a multi-step fashion. We apply this methodology to test generation
    by deconstructing the testsuite generation process into a multi-stage sequence,
    each of which is driven by a specific prompt aligned with the execution paths
    of the method under test, and exposing relevant type and dependency focal context
    to the model. Our approach enables pretrained LLMs to generate more complete test
    cases without any additional training. We implement SymPrompt using the TreeSitter
    parsing framework and evaluate on a benchmark challenging methods from open source
    Python projects. SymPrompt enhances correct test generations by a factor of 5
    and bolsters relative coverage by 26% for CodeGen2\. Notably, when applied to
    GPT-4, symbolic path prompts improve coverage by over $2\times$ compared to baseline
    prompting strategies.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了SymPrompt，一种用于测试生成的代码感知提示策略。SymPrompt的方法基于近期的研究，证明当LLMs被提示以多步骤方式思考问题时，可以解决更复杂的逻辑问题。我们将这种方法应用于测试生成，通过将测试套件生成过程分解为多阶段序列，每个阶段由特定的提示驱动，与待测试方法的执行路径对齐，并向模型暴露相关的类型和依赖上下文。我们的方法使预训练的LLMs能够生成更完整的测试用例，而无需额外的训练。我们使用TreeSitter解析框架实现了SymPrompt，并在开源Python项目中的具有挑战性的方法上进行评估。SymPrompt将正确测试生成的提高了5倍，并将CodeGen2的相对覆盖率提高了26%。值得注意的是，当应用于GPT-4时，符号路径提示相比基线提示策略将覆盖率提高了超过$2\times$。
- en: 'Test Generation, Large Language Models^†^†copyright: acmcopyright^†^†journalyear:
    2023^†^†doi: XXXXXXX.XXXXXXX^†^†conference: The ACM International Conference on
    the Foundations of Software Engineering; July 15–19, 2024; Porto de Galinha, Brazil^†^†booktitle:
    FSE ’24: The ACM International Conference on the Foundations of Software Engineering,
    July 15–19, 2024, Porto de Galinha, Brazil^†^†ccs: Software and its engineering^†^†ccs:
    Computing methodologies Artificial intelligence'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 测试生成，大型语言模型^†^†版权：acmcopyright^†^†期刊年：2023^†^†doi：XXXXXXX.XXXXXXX^†^†会议：ACM国际软件工程基础会议；2024年7月15–19日；巴西波尔图德加林哈^†^†图书标题：FSE
    ’24：ACM国际软件工程基础会议，2024年7月15–19日，巴西波尔图德加林哈^†^†ccs：软件及其工程^†^†ccs：计算方法论 人工智能
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Testing is an essential component of software development that allows developers
    to catch bugs early in the development lifecycle and prevents costly releases
    of buggy software (Planning, [2002](#bib.bib32)). However, manual test writing
    can be time-consuming, taking up more than 15% of development time on average (Daka
    and Fraser, [2014](#bib.bib11)). Extensive research has therefore been devoted
    to developing automated test generation approaches, which can automate the process
    of writing testsuites for software units under development. Automated test generation
    for the purposes of generating a testsuite that becomes part of development codebase
    is typically performed in a *regression setting*, which assumes that the code
    currently under test is implemented correctly, and the objective is to generate
    a suite of tests that will effectively detect *future* bugs that may be introduced
    in to the codebase during later development, causing a regression (Yoo and Harman,
    [2012](#bib.bib43)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 测试是软件开发中的一个重要组成部分，它允许开发人员在开发生命周期早期捕获错误，并防止有缺陷的软件进行昂贵的发布（Planning，[2002](#bib.bib32)）。然而，手动编写测试可能耗时较长，平均占用超过15%的开发时间（Daka和Fraser，[2014](#bib.bib11)）。因此，广泛的研究致力于开发自动化测试生成方法，这些方法可以自动化编写测试套件的过程，以适用于正在开发的软件单元。为了生成成为开发代码库一部分的测试套件，自动化测试生成通常在*回归设置*中进行，这种设置假定当前测试的代码已正确实现，目标是生成一个有效检测*未来*可能在后续开发中引入的错误的测试套件，从而导致回归（Yoo和Harman，[2012](#bib.bib43)）。
- en: Limitation of Existing Approaches.
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 现有方法的局限性。
- en: Widely used test-suite generation tools such as Evosuite (Fraser and Arcuri,
    [2011](#bib.bib14)) use a Search Based Software Testing (SBST) approach in which
    test inputs are randomly generated and mutated to maximize coverage of the software
    unit under test. However, SBST approaches struggle to generate high coverage test
    inputs in many cases, such as when branch conditions depend on specific values
    or states that are difficult to resolve with randomized inputs and heuristics.
    In a large scale study of SBST on 110 widely used open source projects, Fraser
    et al. observed that more than 25% of the tested software classes had less than
    20% coverage (Fraser and Arcuri, [2014](#bib.bib16)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 广泛使用的测试套件生成工具，如Evosuite（Fraser和Arcuri，[2011](#bib.bib14)），采用了基于搜索的软件测试（SBST）方法，其中测试输入是随机生成和变异的，以最大化被测试软件单元的覆盖率。然而，SBST方法在许多情况下难以生成高覆盖率的测试输入，例如当分支条件依赖于特定值或状态，而这些值或状态难以通过随机输入和启发式方法解决时。在对110个广泛使用的开源项目进行的大规模SBST研究中，Fraser等人观察到，超过25%的被测试软件类的覆盖率不到20%（Fraser和Arcuri，[2014](#bib.bib16)）。
- en: The limitations of SBST have motivated recent work using large language models
    (LLMs) for automated testsuite generation (Tufano et al., [2020](#bib.bib37)).
    In this setting, LLMs are typically instructed to generate test cases by supplying
    the source code of the focal method and optionally some additional code context.
    The instruction with which as user interacts with an LLM is commonly referred
    to as prompt.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: SBST的局限性促使了最近利用大型语言模型（LLMs）进行自动化测试套件生成的研究（Tufano等人，[2020](#bib.bib37)）。在这种设置中，LLMs通常通过提供焦点方法的源代码和可选的额外代码上下文来生成测试用例。用户与LLM互动的指令通常称为提示。
- en: Unlike SBST approaches that reason about the execution behavior of a method
    under test (commonly called focal method), LLMs approximate the overall functionalities
    of the focal method based on the natural naming convention (Allamanis et al.,
    [2015](#bib.bib5); Ahmad et al., [2020](#bib.bib3)) of its implementation (e.g.,
    meaningful method name, variable names, etc.), API usage, and calling context.
    This capability of LLMs can be harnessed to create test cases, specifically generating
    inputs targeting branch conditions that necessitate particular input values or
    states. However, when tasked with generating test inputs for methods with challenging
    or complex branch conditions, LLMs usually succeed in producing inputs for the
    easy branches, leaving behind the branches with more complex conditions. Consequently,
    for hard real-world use cases, LLMs usually struggle to generate high coverage
    testsuites (Dinella et al., [2022](#bib.bib13); Alagarsamy et al., [2023](#bib.bib4)).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与 SBST 方法根据测试中的方法执行行为（通常称为焦点方法）进行推理不同，LLM 根据其实现的自然命名约定（Allamanis et al., [2015](#bib.bib5);
    Ahmad et al., [2020](#bib.bib3)）（例如有意义的方法名称、变量名称等）、API 使用和调用上下文，来近似焦点方法的整体功能。LLM
    的这一能力可以被用来创建测试用例，特别是生成针对需要特定输入值或状态的分支条件的输入。然而，当面临生成具有挑战性或复杂分支条件的方法的测试输入时，LLM 通常只会成功生成易于处理的分支的输入，而对更复杂条件的分支则无能为力。因此，对于难度较大的实际用例，LLM
    通常难以生成高覆盖率的测试套件（Dinella et al., [2022](#bib.bib13); Alagarsamy et al., [2023](#bib.bib4)）。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM") illustrates
    how a focal method can pose challenges for both SBST and LLM-based test generation
    approaches. The method takes a string representing a filesystem object as input
    and categorizes the type of filesystem object based on an external method call.
    Generating high coverage test inputs for this method is very difficult for a SBST
    approach because generating strings that represent different filesystem devices
    is extremely unlikely with randomized input generation, and in practice it will
    only test paths for which it has preprogrammed heuristics to generate string inputs
    such as directories (see Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣ Results. ‣ 1\.
    Introduction ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM")). Conversely, an LLM trained on a large code
    corpus should in theory have the knowledge to generate strings representing filesystem
    objects like block devices and sockets, but in practice, when given an open ended
    prompt to implement a set of tests for the method, will only generate tests for
    relatively simple and common inputs for a filesystem utility function, such as
    temporary files and directories (see Figure [1(c)](#S1.F1.sf3 "In Figure 1 ‣ Results.
    ‣ 1\. Introduction ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM")).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "Figure 1 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM") 说明了一个焦点方法如何对
    SBST 和基于 LLM 的测试生成方法提出挑战。该方法以表示文件系统对象的字符串作为输入，并根据外部方法调用对文件系统对象的类型进行分类。对于 SBST
    方法来说，为该方法生成高覆盖率的测试输入是非常困难的，因为随机输入生成下生成表示不同文件系统设备的字符串的可能性极低，实际上，它只能测试那些已经编程好的启发式方法可以生成字符串输入的路径，例如目录（见图
    [1(b)](#S1.F1.sf2 "In Figure 1 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM")）。相反，理论上，经过大规模代码语料库训练的
    LLM 应该具备生成表示文件系统对象（如块设备和套接字）的字符串的知识，但在实践中，当给出一个开放性提示以为该方法实现一组测试时，LLM 通常只会生成针对相对简单和常见的文件系统实用功能的测试，例如临时文件和目录（见图
    [1(c)](#S1.F1.sf3 "In Figure 1 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM")）。'
- en: Our Solution.
  id: totrans-18
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 我们的解决方案。
- en: In this work we introduce a novel approach, SymPrompt, to constructing prompts
    leveraging different code properties, which enables LLMs to generate test inputs
    for more complex focal methods. Recent research employing LLMs for logical problem-solving
    demonstrates that when LLMs are prompted to decompose the problem into multiple
    stages of reasoning first instead of directly trying to generate the answer, they
    exhibit greatly enhanced capability in solving more complex problems (Wei et al.,
    [2022b](#bib.bib39), [a](#bib.bib38); Yao et al., [2023](#bib.bib42)). We build
    on this concept devising a unit test specific decomposition framework and integrating
    it into a novel multi-stage prompting strategy. Our key insight is that the process
    of generating a test-suite to fully cover a focal method can be broken down into
    a sequence of logical problems that are posed to the LLM prompts. For each prompt,
    the LLM will generate an appropriate test case, and thus, will eventually generate
    a series of test cases with higher code coverage to test the diverse behavior
    of the focal method.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了一种新颖的方法——SymPrompt，用于利用不同的代码属性构建提示，这使得 LLM 能够为更复杂的核心方法生成测试输入。最近的研究表明，当
    LLM 被提示先将问题分解为多个推理阶段，而不是直接尝试生成答案时，它们在解决更复杂问题方面表现出显著增强的能力 (Wei et al., [2022b](#bib.bib39),
    [a](#bib.bib38); Yao et al., [2023](#bib.bib42))。我们基于这一概念，设计了一个特定于单元测试的分解框架，并将其集成到一种新颖的多阶段提示策略中。我们关键的见解是，生成一个完整覆盖核心方法的测试套件的过程可以被分解为一系列逻辑问题，这些问题被提出给
    LLM 提示。对于每个提示，LLM 将生成一个合适的测试用例，从而最终生成一系列具有更高代码覆盖率的测试用例，以测试核心方法的多样行为。
- en: 'At a high level, SymPrompt works in three stages: (i) Given a focal method,
    SymPrompt tries to statically capture the execution behavior of the method by
    collecting approximate path constraints and return values for each execution path.
    (ii) SymPrompt collects some properties of the focal method including argument
    types, external library dependencies, code context, etc. (iii) For each execution
    path, SymPrompt constructs a prompt using the collected information and solicits
    the LLM to generate a test input that will execute the corresponding path. This
    generation is carried out iteratively, and the test cases generated in previous
    iterations are appended to subsequent prompts to provide further guidance to the
    LLM in generating a thorough testsuite.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在高级层面，SymPrompt 分为三个阶段： (i) 给定一个核心方法，SymPrompt 通过收集每个执行路径的近似路径约束和返回值，静态捕捉该方法的执行行为。
    (ii) SymPrompt 收集核心方法的一些属性，包括参数类型、外部库依赖、代码上下文等。 (iii) 对于每个执行路径，SymPrompt 使用收集到的信息构造一个提示，并请求
    LLM 生成一个能够执行相应路径的测试输入。这一生成过程是迭代进行的，之前迭代生成的测试用例会被附加到后续的提示中，为 LLM 生成全面的测试套件提供进一步的指导。
- en: Conceptually, our approach resembles symbolic analysis based test generation
    techniques. However, traditional symbolic analysis is known to suffer in real-world
    code with complex data types, external dependencies, and complex branching behavior.
    The proposed work addresses these limitations in three ways. First, SymPrompt
    collects approximate constraints based on static code rather than attempting to
    resolve all data types and unresolved dependencies while collecting path constraints.
    For instance, instead of attempting to symbolically reason about a call like os.is_dir(),
    we keep it as it is and rely on the LLM to reason about the underlying functionality
    of the method call based on its name and usage. Second, to mitigate computational
    overhead arising from numerous branching conditions, we focus only on the paths
    with unique branching conditions. Finally, instead of relying on a solver to resolve
    underlying constraints, we employ an LLM to generate test cases based on the prompt.
    The insight here is that, although the LLM may not precisely reason about all
    possible constraints, it typically extracts sufficient information from the code
    context and approximate path constraints to generate meaningful test inputs.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们的方法类似于基于符号分析的测试生成技术。然而，传统的符号分析在处理具有复杂数据类型、外部依赖项和复杂分支行为的实际代码时往往存在问题。提出的工作从三个方面解决了这些限制。首先，SymPrompt
    基于静态代码收集近似约束，而不是在收集路径约束时尝试解析所有数据类型和未解决的依赖项。例如，我们不会尝试对像 os.is_dir() 这样的调用进行符号推理，而是保持其原样，并依靠
    LLM 根据方法调用的名称和使用情况来推理其底层功能。其次，为了减少由于众多分支条件引起的计算开销，我们只关注具有独特分支条件的路径。最后，我们没有依赖求解器来解析底层约束，而是利用
    LLM 根据提示生成测试用例。这里的见解是，虽然 LLM 可能无法精确推理所有可能的约束，但它通常可以从代码上下文和近似路径约束中提取足够的信息，以生成有意义的测试输入。
- en: Our step-by-step prompting approach further facilitates the test generation
    process. Without this type of prompting, the LLM would need to (1) infer what
    branch the previous test it generated covered, (2) search for a new branch to
    address, and (3) create a test for it. In our approach, we delegate steps 1 and
    2 to traditional static analysis, allowing the LLM to focus solely on step 3.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的逐步提示方法进一步促进了测试生成过程。如果没有这种提示，LLM 需要（1）推断之前生成的测试覆盖了哪个分支，（2）搜索新的分支来解决，以及（3）为其创建一个测试。在我们的方法中，我们将步骤
    1 和 2 委托给传统的静态分析，使 LLM 专注于步骤 3。
- en: Results.
  id: totrans-23
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结果。
- en: 'We implement SymPrompt for Python using the TreeSitter parsing framework and
    evaluate on a benchmark of 897 focal methods that are challenging for existing
    SBST test generation. We prototype our technique on open source CodeGen2 16B LLM:
    SymPrompt improves the ratio of correct test generations by a factor of 5 and
    improve relative coverage by 26%. To check generalizability of our technique,
    we further evaluate SymPrompt with a state-of-the-art LLM, GPT-4: SymPrompt improves
    relative coverage by 105% over tests generated with a baseline prompting strategy.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 TreeSitter 解析框架为 Python 实现了 SymPrompt，并在 897 个对现有 SBST 测试生成具有挑战性的焦点方法上进行评估。我们在开源
    CodeGen2 16B LLM 上对我们的技术进行了原型测试：SymPrompt 将正确测试生成的比例提高了 5 倍，相对覆盖率提高了 26%。为了检查我们技术的通用性，我们进一步在最先进的
    LLM GPT-4 上评估了 SymPrompt：与基准提示策略生成的测试相比，SymPrompt 的相对覆盖率提高了 105%。
- en: 'In summary, this paper makes the following contributions:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文做出了以下贡献：
- en: (1)
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: We introduce *Path Constraint Prompting*, a novel, code-aware prompting strategy
    for LLM test generation that breaks the process of generating a testsuite into
    a multi-stage procedure of generating test inputs for each execution path in the
    method under test.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了*路径约束提示*，这是一种新颖的代码感知提示策略，用于 LLM 测试生成，它将生成测试套件的过程拆分为对被测试方法中每个执行路径生成测试输入的多阶段过程。
- en: (2)
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: We implement our approach in SymPrompt using the TreeSitter parsing framework
    with integrations for both open-source transformers and GPT models. SymPrompt
    is currently a proprietary research prototype and we are working with our legal
    team to make the code publicly available.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在 SymPrompt 中实现了我们的方法，使用 TreeSitter 解析框架，并与开源变压器和 GPT 模型进行了集成。SymPrompt 目前是一个专有的研究原型，我们正在与法律团队合作，将代码公开。
- en: (3)
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: We evaluate SymPrompt on a benchmark of 897 methods that are challenging for
    existing SBST approaches from widely used Python projects and show it improves
    relative coverage by 26% for CodeGen2 and by a factor of more than $2\times$ (105%
    relative improvement) when used with GPT-4.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在一个包含897种方法的基准测试上评估了SymPrompt，这些方法对现有的SBST方法构成了挑战，数据来源于广泛使用的Python项目。结果显示，与GPT-4一起使用时，SymPrompt相对于CodeGen2提高了26%的覆盖率，并且提高了超过$2\times$（105%的相对改进）。
- en: '![Refer to caption](img/739ba2a39fba7a5b0638f45bbb79b277.png) (a) Focal method.  ![Refer
    to caption](img/0aef08339f221d2780b594655fc0677e.png) (b) SBST test generations.
       ![Refer to caption](img/3ddc45577d8da1ef5f355f629f388d27.png) (c) LLM test
    generations.  ![Refer to caption](img/e59399637dd7f3854adf36aba3067a4f.png) (d)
    SymPrompt test generations.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明文字](img/739ba2a39fba7a5b0638f45bbb79b277.png) (a) 焦点方法。 ![参见说明文字](img/0aef08339f221d2780b594655fc0677e.png)
    (b) SBST测试生成。    ![参见说明文字](img/3ddc45577d8da1ef5f355f629f388d27.png) (c) LLM测试生成。
    ![参见说明文字](img/e59399637dd7f3854adf36aba3067a4f.png) (d) SymPrompt测试生成。'
- en: Figure 1. Example test generations from an SBST tool (Pynguin), zero shot LLM
    (CodeGen2), and SymPrompt prompts for focal method exists_as in the flutils open
    source Python project. An SBST approach is unable to generate full coverage tests
    for this method without special configuration because it is unable to generate
    strings that represent specific types of filesystem objects (e.g., block devices).
    An LLM conversely is able to generate input strings associated with filesystem
    objects such as block devices, but in practice will only test a small subset of
    use cases based on the most likely usage scenarios such as paths to files and
    directories. SymPrompt constructs path specific prompts to guide the model to
    generate high coverage testsuites.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图1. 来自SBST工具（Pynguin）、零样本LLM（CodeGen2）和SymPrompt针对flutils开源Python项目中的焦点方法exists_as的提示的示例测试生成。SBST方法由于无法生成表示特定类型文件系统对象（例如块设备）的字符串，不能生成该方法的全面测试用例，除非进行特殊配置。相反，LLM能够生成与文件系统对象（如块设备）相关的输入字符串，但在实际应用中，LLM通常只会基于最可能的使用场景（例如文件和目录路径）测试一个小的用例子集。SymPrompt构建了路径特定的提示，以指导模型生成高覆盖率的测试套件。
- en: '![Refer to caption](img/d55487a94d80283cc4c4dc30549250de.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d55487a94d80283cc4c4dc30549250de.png)'
- en: 'Figure 2. Workflow for generating path constraint prompts. The focal method
    shown in Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ Results. ‣ 1\. Introduction ‣
    Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM") is first parsed and its abstract syntax tree is traversed
    in preorder. In step \raisebox{-.9pt} {1}⃝, the traversal first visits the first
    method statement, normalize_path(path), but does not record any information since
    it is not a branch constraint. In step \raisebox{-.9pt} {2}⃝, it then traverses
    to the first if statement, and records that there is a constraint path.is_dir()
    that must be satisfied to execute the current path on the AST. It then reaches
    the return ’directory’ under the first if check, and records that there is an
    execution path where ’directory’ is returned when path.is_dir() is true. The preorder
    traversal next visits the if path.is_file(), return ’file’ branch of the AST in
    \raisebox{-.9pt} {3}⃝ and records a second path where path.is_dir() is false and
    path.is_file() is true, and the return behavior is ’file’. This traveral continues
    until in step \raisebox{-.9pt} {4}⃝, the final return statement is reached, based
    on an execution path where none of the branch constraints are true. Each collected
    execution path and return value is then used to construct prompts for test generations
    that specifies both the path constraints and return behavior for the target test
    case.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2. 生成路径约束提示的工作流程。图[1(a)](#S1.F1.sf1 "图1 ‣ 结果 ‣ 1\. 介绍 ‣ 基于代码的提示：使用LLM进行回归设置下的覆盖率引导测试生成研究")中的焦点方法首先被解析，其抽象语法树按照前序遍历。在步骤\raisebox{-.9pt}
    {1}⃝中，遍历首先访问第一个方法语句normalize_path(path)，但由于它不是一个分支约束，因此不记录任何信息。在步骤\raisebox{-.9pt}
    {2}⃝中，遍历接着访问第一个if语句，并记录有一个必须满足的约束path.is_dir()，以便在AST上执行当前路径。然后，它到达第一个if检查下的return
    ’directory’，并记录在path.is_dir()为真时会返回’directory’的执行路径。前序遍历接着访问if path.is_file()，在\raisebox{-.9pt}
    {3}⃝的AST分支中返回’file’，记录第二个路径，其中path.is_dir()为假而path.is_file()为真，返回行为为’file’。这个遍历继续进行，直到在步骤\raisebox{-.9pt}
    {4}⃝中，最终的return语句被访问到，基于一个没有任何分支约束为真的执行路径。然后，将每个收集到的执行路径和返回值用于构建测试生成提示，这些提示指定了目标测试用例的路径约束和返回行为。
- en: 2\. Working Example
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 工作示例
- en: 'In this section we provide a working example of how path constraint prompts
    are constructed and used to guide an LLM generate high coverage tests in a regression
    setting. Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ Results. ‣ 1\. Introduction ‣
    Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM") illustrates a focal method, named exists_as, extracted from
    our evaluation within the flutils.path_utils module. Testing this method poses
    a significant challenge for both SBST (Search-Based Software Testing) and LLM-based
    approaches due to its extensive branching structure, which requires tailored input
    data to achieve full coverage of test cases. Each branch in this method demands
    specific input values that satisfy precise constraints—particular string inputs
    that reference filesystem objects like directories and block devices.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一个工作示例，说明路径约束提示是如何构建和使用以指导LLM生成高覆盖率测试的。图[1(a)](#S1.F1.sf1 "在图1中 ‣
    结果 ‣ 1\. 引言 ‣ 代码感知提示：基于LLM的回归设置中的覆盖引导测试生成研究")展示了一种焦点方法，名为exists_as，提取自我们在flutils.path_utils模块中的评估。由于其广泛的分支结构，该方法的测试对SBST（基于搜索的软件测试）和基于LLM的方法都构成了重大挑战，需要量身定制的输入数据才能全面覆盖测试用例。该方法中的每个分支都需要满足特定约束的输入值——即引用文件系统对象（如目录和块设备）的特定字符串输入。
- en: 'SBST-based Test Generation. Figure [1(b)](#S1.F1.sf2 "In Figure 1 ‣ Results.
    ‣ 1\. Introduction ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM") shows a suite of regression tests generated
    with the Python SBST tool Pynguin. Since SBST approaches generate inputs randomly
    according pre-programmed heuristics, they are unlikely to generate inputs that
    represent specific filesystem objects such as block devices and sockets unless
    the tool was specifically configured to generate strings representing these objects
    as inputs. In the case of Pynguin, the input strings it generated in Figure [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware Prompting: A study of
    Coverage guided Test Generation in Regression Setting using LLM") represent a
    nonexistent device (‘#O01E’) and a directory that is defined in the Pynguin test
    environment (‘/pynguin’) that cover two of return behaviors in the focal method,
    but do not test the other use cases that require other specific input values.
    Pynguin is not capable of generating input strings that test these other use cases
    without special configuration. In addition, it generates tests that do not follow
    common usage patterns in developer written tests, which makes the tests more difficult
    to maintain (Tufano et al., [2020](#bib.bib37)).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于SBST的测试生成。图[1(b)](#S1.F1.sf2 "在图1中 ‣ 结果 ‣ 1\. 引言 ‣ 代码感知提示：基于LLM的回归设置中的覆盖引导测试生成研究")展示了一组使用Python
    SBST工具Pynguin生成的回归测试。由于SBST方法依据预编程启发式算法随机生成输入，因此它们不太可能生成代表特定文件系统对象（如块设备和套接字）的输入，除非工具被专门配置为生成表示这些对象的字符串作为输入。在Pynguin的情况下，它在图[1(b)](#S1.F1.sf2
    "在图1中 ‣ 结果 ‣ 1\. 引言 ‣ 代码感知提示：基于LLM的回归设置中的覆盖引导测试生成研究")中生成的输入字符串表示一个不存在的设备（‘#O01E’）和一个在Pynguin测试环境中定义的目录（‘/pynguin’），这些字符串覆盖了焦点方法中的两个返回行为，但未测试其他需要特定输入值的用例。没有特殊配置，Pynguin无法生成测试这些其他用例的输入字符串。此外，它生成的测试不符合开发者编写测试中的常见使用模式，这使得测试更难维护（Tufano等，[2020](#bib.bib37)）。
- en: 'LLM-based Test Generation. Figure [1(c)](#S1.F1.sf3 "In Figure 1 ‣ Results.
    ‣ 1\. Introduction ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM") shows a set of regression test calls generated
    using a standard code-completion testing prompt used by Lemieux et al. (Lemieux
    et al., [2023](#bib.bib23)). Unlike an SBST tool such as Pynguin, an LLM has an
    approximate domain understanding to reason that a branch constraint like path.is_file()
    will likely be tested by an input like ‘./tmp’ and can generate associated input
    strings to test these use cases, even without observing the definitions of normalize_path
    and is_file on the first and second lines of code in exists_as in Figure [1(a)](#S1.F1.sf1
    "In Figure 1 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware Prompting: A study of
    Coverage guided Test Generation in Regression Setting using LLM"). However, in
    cases where focal methods have many different paths, and some paths represent
    less common use cases (e.g., if the input string is a block device), LLMs will
    only generate test cases for the most common use cases of a focal method, even
    if many test generations are sampled. In this case, the LLM (CodeGen2) only generates
    tests for the two most common uses, where the input is either a directory or a
    file.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 LLM 的测试生成。图 [1(c)](#S1.F1.sf3 "图 1 ‣ 结果 ‣ 1\. 引言 ‣ 代码感知提示：基于覆盖率指导的回归设置中使用
    LLM 的测试生成研究") 展示了一组使用 Lemieux 等人（Lemieux et al.，[2023](#bib.bib23)）所用的标准代码补全测试提示生成的回归测试调用。与
    SBST 工具如 Pynguin 不同，LLM 具有大致的领域理解，能够推断像 path.is_file() 这样的分支约束可能会通过类似 ‘./tmp’
    的输入来测试，并生成相关的输入字符串来测试这些用例，即使在图 [1(a)](#S1.F1.sf1 "图 1 ‣ 结果 ‣ 1\. 引言 ‣ 代码感知提示：基于覆盖率指导的回归设置中使用
    LLM 的测试生成研究") 的 exists_as 中没有观察到 normalize_path 和 is_file 的定义。然而，在重点方法具有许多不同路径且某些路径代表较少见的用例（例如，如果输入字符串是块设备）时，即使样本量较大，LLM
    也只会为重点方法的最常见用例生成测试用例。在这种情况下，LLM（CodeGen2）仅生成针对两种最常见使用情况的测试，其中输入要么是目录，要么是文件。
- en: 'SymPrompt. Our approach works in three steps: (i) Collecting Approximate Path
    Constraints. This step us the core to our approach. Figure [2](#S1.F2 "Figure
    2 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware Prompting: A study of Coverage guided
    Test Generation in Regression Setting using LLM") shows how path constraints are
    collected and used to prompt an LLM to generate high coverage tests. Possible
    execution paths in the focal method are collected by traversing the method’s abstract
    syntax tree (AST) in preorder and recording branch conditions on each possible
    execution path, where if a branch is not taken its branch condition is inverted
    on a given path. When a return statement is reached, the set of branch constraints
    that need to be satisfied to reach that return statement is recorded, along with
    the returned value. Each recorded path constraint and return value is then used
    to generate a prompt, which instructs the LLM to generate a test case targeting
    the corresponding path.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: SymPrompt。我们的方法分为三个步骤：(i) 收集近似路径约束。这个步骤是我们方法的核心。图 [2](#S1.F2 "图 2 ‣ 结果 ‣ 1\.
    引言 ‣ 代码感知提示：基于覆盖率指导的回归设置中使用 LLM 的测试生成研究") 展示了如何收集路径约束并利用它们提示 LLM 生成高覆盖率的测试。在重点方法中，通过前序遍历方法的抽象语法树（AST）来收集可能的执行路径，并记录每条可能执行路径上的分支条件，如果某个分支没有被执行，其分支条件会在给定路径上取反。当到达一个返回语句时，记录到达该返回语句所需满足的分支约束集合及返回值。每个记录的路径约束和返回值都会用来生成一个提示，指示
    LLM 生成一个针对相应路径的测试用例。
- en: '(ii) Context Construction. Besides the path constraint, each prompt includes
    includes the focal method signature, which guides the LLM to generate a correct
    focal method call. We further include additional focal context for generation
    based on the types and methods that appear in the focal method that exposes relevant
    data structure and external library dependencies the model may need to reference
    for effective test generation, as shown in the figure below. For the focal method
    exists_as, we include the definition of the input parameter _PATH and the external
    method call normalize_path. One such prompt for our working example is shown in
    Figure [3](#S2.F3 "Figure 3 ‣ 2\. Working Example ‣ Code-Aware Prompting: A study
    of Coverage guided Test Generation in Regression Setting using LLM").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (ii) 上下文构建。除了路径约束，每个提示包括焦点方法签名，这指导LLM生成正确的焦点方法调用。我们进一步根据出现在焦点方法中的类型和方法，包含额外的焦点上下文，以便生成相关数据结构和外部库依赖，模型可能需要参考这些信息以有效生成测试，如下图所示。对于焦点方法`exists_as`，我们包含了输入参数_PATH的定义和外部方法调用`normalize_path`。我们工作示例的一个提示如图[3](#S2.F3
    "图 3 ‣ 2\. 工作示例 ‣ 代码感知提示：基于LLM的回归设置中的覆盖指导测试生成研究")所示。
- en: '![Refer to caption](img/d5a4f3e569c533a45c9cd77029d101e4.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d5a4f3e569c533a45c9cd77029d101e4.png)'
- en: Figure 3. Example of generation used by SymPrompt. The prompt exposes both the
    type and dependency context of the focal method to the model in addition to the
    path constraint prompt.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3. SymPrompt使用的生成示例。该提示不仅展示了焦点方法的类型和依赖上下文，还包括路径约束提示。
- en: '(iii) Test Generation. For the focal method exists_as, path constraint prompting
    guides the LLM to generate tests for use cases it does not normally cover without
    specific prompting. In particular, path constraint prompts guide the model to
    generate test strings that satisfy the constraints for is_block_device(), is_char_device(),
    is_fifo(), and is_socket(). When specifically prompted, CodeGen2 is able to generate
    correct test inputs in three of these four cases in our evaluation, more than
    doubling the number of tested branches covered in the generated testsuite (see
    Figure [1(d)](#S1.F1.sf4 "In Figure 1 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM")).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: (iii) 测试生成。对于焦点方法`exists_as`，路径约束提示指导LLM生成通常不覆盖的用例的测试，特别是路径约束提示引导模型生成满足`is_block_device()`、`is_char_device()`、`is_fifo()`和`is_socket()`约束的测试字符串。在具体提示下，CodeGen2能够在我们的评估中正确生成这四个案例中的三个的测试输入，测试分支的覆盖数量增加了两倍多（见图[1(d)](#S1.F1.sf4
    "图 1 ‣ 结果 ‣ 1\. 介绍 ‣ 代码感知提示：基于LLM的回归设置中的覆盖指导测试生成研究")）。
- en: Path constraint prompting, in conjunction with type-aware focal contexts, allows
    us to leverage advantages of LLMs in deriving meaningful test inputs from context,
    understanding how to correctly initialize complex input types, utilizing relevant
    external API calls in testing, and resolving difficult-to-cover branch constraints,
    while deriving the advantages of a coverage-aware testing strategy similar to
    SBST by prompting for a set of high coverage tests.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 路径约束提示与类型感知焦点上下文结合，使我们能够利用LLM在从上下文中推导有意义的测试输入方面的优势，理解如何正确初始化复杂输入类型，利用相关外部API调用进行测试，并解决难以覆盖的分支约束，同时通过提示一组高覆盖率测试来获得类似SBST的覆盖感知测试策略的优势。
- en: 3\. Methodology
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法论
- en: In this section, we elaborate on our approach, SymPrompt, to generating test
    cases for a given focal method in a regression setting. At the core of our approach
    is crafting tailored prompts that break the problem of test generation into multiple
    stages of reasoning based on the possible execution paths in focal method. These
    prompts are designed to instruct the model to generate test cases for a specific
    set of execution paths within the focal method that will ensure comprehensive
    branch and line coverage.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细阐述了我们的方法SymPrompt，在回归设置中为给定的焦点方法生成测试用例。我们方法的核心是制定定制提示，将测试生成的问题分解为多个基于焦点方法可能执行路径的推理阶段。这些提示旨在指导模型为焦点方法中的特定执行路径生成测试用例，以确保全面的分支和行覆盖。
- en: 3.1\. Overview
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 概述
- en: Our approach to constructing multi-stage reasoning prompts is based on static
    symbolic analysis techniques (Godefroid, [2007](#bib.bib18); King, [1976](#bib.bib21);
    Baldoni et al., [2018](#bib.bib7)) to reason about underlying program behavior.
    At a high level, for each control path ($\rho$.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建多阶段推理提示的方法基于静态符号分析技术 (Godefroid, [2007](#bib.bib18); King, [1976](#bib.bib21);
    Baldoni et al., [2018](#bib.bib7))，以推理潜在的程序行为。在高层次上，对于每个控制路径（$\rho$）。
- en: Once the path constraints are collected, in the conventional symbolic analysis-based
    testing approach, a systematic search algorithm is employed to enumerate all the
    path constraints (Godefroid et al., [2005](#bib.bib19)). Feasible paths are those
    for which the corresponding $\Phi_{\rho}$ in a regression setting.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦路径约束被收集，在传统的基于符号分析的测试方法中，系统搜索算法被用来枚举所有路径约束 (Godefroid et al., [2005](#bib.bib19))。可行的路径是那些在回归设置中对应的$\Phi_{\rho}$。
- en: However, when dealing with complex real-world code (for instance, when input
    parameters of the focal method are of complex data types, or when focal methods
    rely on external dependencies and API calls), the traditional symbolic analysis-based
    techniques often encounters difficulty in finding a solution. Furthermore, in
    cases where focal methods feature numerous nested branches and conditions, symbolic
    execution frequently experiences significant computational overhead.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在处理复杂的现实世界代码时（例如，当核心方法的输入参数为复杂数据类型时，或者核心方法依赖于外部依赖和API调用时），传统的基于符号分析的技术通常在找到解决方案时遇到困难。此外，当核心方法具有大量嵌套分支和条件时，符号执行通常会遇到显著的计算开销。
- en: 'In this work, we overcome the above mentioned shortcomings in three ways. First,
    instead of trying to resolve all the data types and unresolved dependencies while
    collecting path constraints, we allow approximations. For example, if the method
    path.is_dir() in Figure [2](#S1.F2 "Figure 2 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM") cannot be reasoned about symbolically, we leave the condition as it is while
    collecting $\phi_{\rho}$. Second, to address computational overhead with many
    branching conditions, we only focus on the paths having unique branching conditions.
    This design decision significantly helps to reduce the number of paths. Finally,
    instead of using a solver to solve the underlying constraints we leverage an LLM
    to generate test cases based on the collected path constraints. Our insight is
    that, although the LLM may not reason precisely about all possible constraints,
    it usually derives enough information from code context and the approximate path
    constraints to generate meaningful test inputs.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项工作中，我们通过三种方式克服了上述缺点。首先，在收集路径约束时，我们不试图解决所有数据类型和未解决的依赖，而是允许近似。例如，如果图[2](#S1.F2
    "Figure 2 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware Prompting: A study of Coverage
    guided Test Generation in Regression Setting using LLM")中的方法path.is_dir()无法用符号方式推理，我们在收集$\phi_{\rho}$时保留条件不变。其次，为了解决许多分支条件带来的计算开销，我们只关注具有唯一分支条件的路径。这一设计决策显著有助于减少路径数量。最后，我们不是使用求解器来解决潜在的约束，而是利用LLM基于收集的路径约束生成测试用例。我们的见解是，尽管LLM可能不能准确推理所有可能的约束，但它通常能从代码上下文和近似路径约束中获得足够的信息来生成有意义的测试输入。'
- en: 'To this end, our test generation contains three steps: (Step-I) collecting
    approximate path constraints and return expressions for program paths, (Step-II)
    capturing relevant code context, and (Step-III) generating prompts amenable to
    a LLM by leveraging the path constrains and context identified in the above step.
    This entire process is done statically without executing the focal method. Figure
    [4](#S3.F4 "Figure 4 ‣ 3.1\. Overview ‣ 3\. Methodology ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM") gives
    a high level overview of how we use SymPrompt to generate tests.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '为此，我们的测试生成包含三个步骤：（步骤-I）收集程序路径的近似路径约束和返回表达式，（步骤-II）捕获相关的代码上下文，以及（步骤-III）利用上述步骤中识别的路径约束和上下文生成适用于LLM的提示。整个过程是静态完成的，无需执行核心方法。图[4](#S3.F4
    "Figure 4 ‣ 3.1\. Overview ‣ 3\. Methodology ‣ Code-Aware Prompting: A study of
    Coverage guided Test Generation in Regression Setting using LLM")概述了我们如何使用SymPrompt生成测试。'
- en: '![Refer to caption](img/988391db92b19511dd4221f44da5107b.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/988391db92b19511dd4221f44da5107b.png)'
- en: Figure 4. Overview of SymPrompt’s framework for test generation. In Step-I,
    path constraint collection is performed on the focal method. In Step-II, the type
    and dependency context from the focal method are parsed from the focal file along
    with the focal method itself. Finally, in Step-III, prompts for each set of path
    constraints are then constructed and iteratively passed to the model to generate
    test cases.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. SymPrompt 测试生成框架概述。在步骤 I 中，对焦点方法执行路径约束收集。在步骤 II 中，从焦点文件中解析焦点方法的类型和依赖上下文以及焦点方法本身。最后，在步骤
    III 中，为每组路径约束构建提示，并将其迭代地传递给模型以生成测试用例。
- en: '3.2\. Step-I: Collecting Approximate Path Constraints'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 步骤 I：收集近似路径约束
- en: The objective of SymPrompt is to generate test inputs that will follow specific
    execution paths in the focal method. To facilitate this, we construct prompts
    to expose the model to relevant path constraints. This step describes in details
    how we collect approximate path constraints by statically traversing the abstract
    parse tree (AST) of the focal method.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SymPrompt 的目标是生成将遵循焦点方法中特定执行路径的测试输入。为此，我们构建提示以将模型暴露于相关路径约束。此步骤详细描述了我们如何通过静态遍历焦点方法的抽象语法树（AST）来收集近似路径约束。
- en: 3.2.1\. Path Constraint Collection
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 路径约束收集
- en: 'Our approach to collecting path constraints is similar to static symbolic execution.
    We perform a preorder traversal of the focal method abstract parse tree and collect
    constraints for each branch and loop condition while maintaining a set of all
    constraints that appear on each possible execution path. When the traversal encounters
    a return statement, we record the path constraints for the paths that terminate
    at that return, along with the return value expression. To prevent an exponential
    explosion of paths when collecting path constraints, we minimize the path set
    on each traversal step to only include paths that increase overall branch coverage
    (see Section [3.2.2](#S3.SS2.SSS2 "3.2.2\. Path Minimization ‣ 3.2\. Step-I: Collecting
    Approximate Path Constraints ‣ 3\. Methodology ‣ Code-Aware Prompting: A study
    of Coverage guided Test Generation in Regression Setting using LLM")).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集路径约束的方法类似于静态符号执行。我们对焦点方法的抽象语法树进行前序遍历，并为每个分支和循环条件收集约束，同时维护一个包含每个可能执行路径上的所有约束的集合。当遍历遇到返回语句时，我们记录终止于该返回的路径的路径约束以及返回值表达式。为了防止在收集路径约束时路径数量呈指数爆炸，我们在每一步遍历中将路径集合最小化，仅包括那些增加整体分支覆盖率的路径（参见第
    [3.2.2](#S3.SS2.SSS2 "3.2.2\. 路径最小化 ‣ 3.2\. 步骤 I：收集近似路径约束 ‣ 3\. 方法论 ‣ 代码感知提示：基于
    LLM 的回归设置覆盖引导测试生成研究") 节）。
- en: We provide a detailed description of the path constraint collection procedure
    in Appendix A in our supplemental materials.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在附录 A 的补充材料中提供了路径约束收集程序的详细描述。
- en: 3.2.2\. Path Minimization
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 路径最小化
- en: 1:procedure minimizePaths(paths)2:     minconstraints = {}3:     minpaths =
    {}4:     for path in paths do5:         path_constraints = splitConstraints(path)6:         if any
    of path_constraints is not in minconstraints then7:              minconstraints
    = minconstraints $\cap$ path               9:     return minpaths
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '1:procedure minimizePaths(paths)2:     minconstraints = {}3:     minpaths =
    {}4:     for path in paths do5:         path_constraints = splitConstraints(path)6:         if
    any of path_constraints is not in minconstraints then7:              minconstraints
    = minconstraints $\cap$ path               9:     return minpaths'
- en: (a) Path Minimization Algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 路径最小化算法。
- en: '![Refer to caption](img/79321ee979e271121faac8928ef37891.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/79321ee979e271121faac8928ef37891.png)'
- en: (b) Example on three branches.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 三个分支的示例。
- en: Figure 5. Path minimization algorithmic definition and illustration of how path
    minimization prevents the number of paths from growing exponentially in the number
    of branches. A method with $n=3$, each of which covers a unique branch condition.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5. 路径最小化算法定义及其如何防止路径数量在分支数目增加时呈指数增长的示例。一个方法有 $n=3$，每个方法覆盖一个独特的分支条件。
- en: One challenge in enumerating execution paths in a method is that the number
    of possible execution paths grows exponentially in the number of branches. Therefore
    collecting path constraints can result in a very large number of paths to test.
    This is undesirable because most of the paths will usually not add additional
    line or branch coverage and therefore are redundant from the perspective of a
    developer. Therefore, when collecting path constraints to construct path constraint
    prompts we only collect a *linearly independent subset* of basis path constraints
    for use in prompting.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在枚举方法中的执行路径时，一个挑战是可能的执行路径数量在分支数量的指数级增长。因此，收集路径约束可能会导致测试路径数量非常庞大。这是不可取的，因为大多数路径通常不会增加额外的行或分支覆盖，因此从开发者的角度来看是冗余的。因此，在收集路径约束以构建路径约束提示时，我们仅收集基础路径约束的
    *线性无关子集* 以用于提示。
- en: Basis paths were first proposed as a measure of method complexity (McCabe, [1976](#bib.bib27))
    and are referred to a basis paths because they form a linear basis for all paths
    when expressed as a set of columns in the adjacency matrix of the method control-flow-graph.
    For testing purposes, basis paths are convenient because a set of tests that execute
    a set of basis paths in a method will achieve full branch coverage on that method.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 基础路径最早被提出作为方法复杂性的度量（McCabe，[1976](#bib.bib27)），之所以称之为基础路径，是因为它们在方法控制流图的邻接矩阵中以列的形式表达时，形成了所有路径的线性基。为了测试的方便，基础路径很实用，因为执行一组基础路径的测试集会实现该方法的全面分支覆盖。
- en: 'Algorithm [5(a)](#S3.F5.sf1 "In Figure 5 ‣ 3.2.2\. Path Minimization ‣ 3.2\.
    Step-I: Collecting Approximate Path Constraints ‣ 3\. Methodology ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM") describes how we compute basis paths from a set of path constraints. We
    first split each path into its constituent branch constraints, and check if any
    of the path’s constraints are not in the set of linearized path constraints. If
    a path includes a branch constraint that is not in the linearized constraint set,
    we add it to the set of linearized paths and add its branch constraints to the
    set of linearized path constraints.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [5(a)](#S3.F5.sf1 "在图 5 ‣ 3.2.2\. 路径最小化 ‣ 3.2\. 第一步：收集近似路径约束 ‣ 3\. 方法论 ‣
    代码感知提示：基于 LLM 的回归设置中的覆盖指导测试生成研究") 描述了我们如何从路径约束集计算基础路径。我们首先将每个路径拆分为其组成的分支约束，并检查路径的约束是否不在线性化路径约束集中。如果路径包括不在线性化约束集中的分支约束，我们将其添加到线性化路径集，并将其分支约束添加到线性化路径约束集中。
- en: 'Figure [5](#S3.F5 "Figure 5 ‣ 3.2.2\. Path Minimization ‣ 3.2\. Step-I: Collecting
    Approximate Path Constraints ‣ 3\. Methodology ‣ Code-Aware Prompting: A study
    of Coverage guided Test Generation in Regression Setting using LLM") illustrates
    how path minimization reduces the number of collected paths for a simple example
    method with three sequential if-else branches. The total number of paths in the
    method is $2\times 2\times 2=8$ for each branch). We apply path minimization in
    Algorithm LABEL:alg:sympath_generation after visitASTNode is called on any if
    or while statement that introduces new path constraints.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S3.F5 "图 5 ‣ 3.2.2\. 路径最小化 ‣ 3.2\. 第一步：收集近似路径约束 ‣ 3\. 方法论 ‣ 代码感知提示：基于
    LLM 的回归设置中的覆盖指导测试生成研究") 说明了路径最小化如何减少一个简单示例方法中三个顺序 if-else 分支的收集路径数量。该方法中路径的总数为
    $2\times 2\times 2=8$（每个分支）。我们在 Algorithm LABEL:alg:sympath_generation 中应用路径最小化，在对任何
    if 或 while 语句调用 visitASTNode 后，这些语句引入了新的路径约束。
- en: 3.3\. Step-II. Context Construction
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 第二步：上下文构建
- en: '![Refer to caption](img/39866e85330e7e824bdda2ec219ef3fb.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/39866e85330e7e824bdda2ec219ef3fb.png)'
- en: 'Figure 6. Illustration of context construction on a focal method is_schema_valid.
    The context is composed of 5 components: 1\. *Imports and Globals.* Imported modules
    and classes that appear in the focal file, along with global variables defined
    in focal file. 2\. *Type Context.* Definitions of types that are used in the focal
    method and defined in the focal file. 3\. *Focal Class Type Context.* The definition
    of the focal class type signature and initialization. 4\. *Focal Class Method
    Context.* Definitions of any focal class methods that are called in the focal
    method. 5\. *Focal Method.* The definition of the focal method to be tested. Constructing
    the generation context to expose relevant types and dependencies helps the model
    to attend to relevant definitions when generating. All objects that are included
    in the context are included in the test execution context and override any import
    statements generated by the model. This is particularly beneficial for test generations
    with GPT models, which we found are prone to generating hallucinated import statements
    (see Section [4.4](#S4.SS4 "4.4\. RQ4: Large Model Performance Impact ‣ 4\. Evaluation
    ‣ Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM")).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '图6. 说明了在焦点方法`is_schema_valid`上的上下文构建。上下文由5个组件组成：1. *导入和全局变量。* 出现在焦点文件中的导入模块和类，以及在焦点文件中定义的全局变量。2.
    *类型上下文。* 在焦点方法中使用且在焦点文件中定义的类型定义。3. *焦点类类型上下文。* 焦点类类型签名和初始化的定义。4. *焦点类方法上下文。* 在焦点方法中调用的任何焦点类方法的定义。5.
    *焦点方法。* 要测试的焦点方法的定义。构建生成上下文以暴露相关类型和依赖关系有助于模型在生成时关注相关定义。所有包含在上下文中的对象都会被包括在测试执行上下文中，并覆盖模型生成的任何导入语句。这对于使用GPT模型的测试生成尤其有利，我们发现这些模型容易生成虚构的导入语句（参见第[4.4节](#S4.SS4
    "4.4. RQ4: 大型模型性能影响 ‣ 4. 评估 ‣ 代码感知提示：使用LLM进行回归设置中的覆盖引导测试生成研究")）。'
- en: 'Prior work in test generation with LLMs have demonstrated that including additional
    context to the focal method definition is beneficial to the quality of test generations (Tufano
    et al., [2020](#bib.bib37)). For SymPrompt, we focus on exposing two types of
    context to the model that are beneficial to generating correct test cases: (i)
    *Type context.* Context that includes type definitions for focal method parameters
    guides the model to generate correct test inputs, particularly when the input
    involves complex data structures. (ii) *Dependency Context.* We construct the
    generation focal context selectively to include only type context and dependency
    context, but also evaluate as a baseline test generation with a local focal context
    that is constructed greedily by adding lines from the focal file until the model’s
    input is fully utilized (see Sections [4.1](#S4.SS1 "4.1\. RQ1: Performance Improvement
    ‣ 4\. Evaluation ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM"), [4.3](#S4.SS3 "4.3\. RQ3: Design Choices ‣
    4\. Evaluation ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM")). In addition, we construct the test execution
    context based on the focal context to override any model-generated imports and
    type definitions. We found that this prevents many common errors, particularly
    when generating tests with chat-tuned models such as GPT-4 (See Section [4.4](#S4.SS4
    "4.4\. RQ4: Large Model Performance Impact ‣ 4\. Evaluation ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM")).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '先前在LLMs测试生成中的研究表明，将额外的上下文包含到焦点方法定义中对测试生成的质量是有益的（Tufano等人，[2020](#bib.bib37)）。对于SymPrompt，我们专注于向模型暴露对生成正确测试用例有益的两种上下文：（i）*类型上下文。*
    包含焦点方法参数类型定义的上下文引导模型生成正确的测试输入，特别是当输入涉及复杂数据结构时。（ii）*依赖上下文。* 我们选择性地构建生成焦点上下文，只包含类型上下文和依赖上下文，同时也作为基线测试生成评估本地焦点上下文，该上下文通过从焦点文件中逐行添加直到模型的输入被完全利用（参见第[4.1节](#S4.SS1
    "4.1. RQ1: 性能改进 ‣ 4. 评估 ‣ 代码感知提示：使用LLM进行回归设置中的覆盖引导测试生成研究")，[4.3节](#S4.SS3 "4.3.
    RQ3: 设计选择 ‣ 4. 评估 ‣ 代码感知提示：使用LLM进行回归设置中的覆盖引导测试生成研究")）。此外，我们基于焦点上下文构建测试执行上下文，以覆盖模型生成的任何导入和类型定义。我们发现，这可以防止许多常见错误，特别是在使用像GPT-4这样的聊天调优模型生成测试时（见第[4.4节](#S4.SS4
    "4.4. RQ4: 大型模型性能影响 ‣ 4. 评估 ‣ 代码感知提示：使用LLM进行回归设置中的覆盖引导测试生成研究")）。'
- en: 'Figure [6](#S3.F6 "Figure 6 ‣ 3.3\. Step-II. Context Construction ‣ 3\. Methodology
    ‣ Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM") illustrates how we construct the generation context. We first
    parse the focal file and extract all import statements, global definitions, class
    definitions, and method definitions. We first add all import statements and global
    definitions to the context. We then check which classes and functions are used
    in the focal method and add their definitions to the context window if they are
    defined in the focal file. If the focal method is defined in a class, we extract
    the class’s type signature and constructor definition, and then include the definitions
    of any methods in the focal class that called in the focal method. Finally, we
    append the full focal method definition at the end of the context window immediately
    before the test context and test generation prompt.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图[6](#S3.F6 "图 6 ‣ 3.3\. 第二步：上下文构建 ‣ 3\. 方法论 ‣ 代码感知提示：基于LLM的回归设置覆盖引导测试生成研究")展示了我们如何构建生成上下文。我们首先解析焦点文件，提取所有的导入语句、全局定义、类定义和方法定义。我们首先将所有的导入语句和全局定义添加到上下文中。然后检查焦点方法中使用了哪些类和函数，并在它们在焦点文件中定义时将其定义添加到上下文窗口中。如果焦点方法在一个类中定义，我们提取类的类型签名和构造函数定义，然后包括在焦点方法中调用的焦点类中的任何方法的定义。最后，在测试上下文和测试生成提示之前，我们将完整的焦点方法定义附加到上下文窗口的末尾。
- en: 3.4\. Step-III. Test Generation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 第三步：测试生成
- en: '![Refer to caption](img/d6fbb341d48840f703d56a549aecf5ee.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d6fbb341d48840f703d56a549aecf5ee.png)'
- en: 'Figure 7. Example of path constraint prompt construction on a simplified focal
    method in the tornado project. Each path prompt has three components: \raisebox{-.9pt}
    {1}⃝ Method Signature. The prompt specifies the test case is for the focal method,
    including its full signature. \raisebox{-.9pt} {2}⃝ Path Constraints. Next, the
    prompt specifies what path constraints should be satisfied by the inputs in the
    given testcase in order to follow the desired execution path. \raisebox{-.9pt}
    {3}⃝ Return Behavior. The prompt specifies what return behavior, if any, is expected
    on the specified execution path. This can guide correct generation of assertions
    for specific return cases.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7. 在Tornado项目中对简化焦点方法的路径约束提示构建示例。每个路径提示有三个组件：\raisebox{-.9pt} {1}⃝ 方法签名。提示指定测试用例针对焦点方法，包括其完整签名。
    \raisebox{-.9pt} {2}⃝ 路径约束。接下来，提示指定输入在给定测试用例中需要满足的路径约束，以遵循所需的执行路径。 \raisebox{-.9pt}
    {3}⃝ 返回行为。提示指定在指定执行路径上预期的返回行为（如果有的话）。这可以指导针对特定返回情况的断言正确生成。
- en: 'Once a set of path constraints have been collected for a given focal method,
    we construct prompts to generate tests for each path based on the focal method
    signature, path constraints, and return behavior on each collected execution path.
    Figure [7](#S3.F7 "Figure 7 ‣ 3.4\. Step-III. Test Generation ‣ 3\. Methodology
    ‣ Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM") shows how a prompt is constructed for a single path in a simplified
    focal method for serializing a timestamp value. Each prompt first specifies the
    focal method the testcase is for along with the focal method signature, which
    serves to guide the model to correctly generate the focal method call in the test.
    The path constraints are combined to indicate that all constraints should be applied
    when generating the given test case. If the path used to generate the prompt terminates
    in a return statement, a returns:  specifier is added to the prompt
    to guide assertion generation on the return value.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦收集了给定焦点方法的路径约束，我们构建提示以根据焦点方法签名、路径约束和每个收集的执行路径上的返回行为生成每个路径的测试。图[7](#S3.F7
    "图 7 ‣ 3.4\. 第三步：测试生成 ‣ 3\. 方法论 ‣ 代码感知提示：基于LLM的回归设置覆盖引导测试生成研究")展示了如何为一个简化的焦点方法构建一个路径的提示，以序列化时间戳值。每个提示首先指定测试用例所针对的焦点方法及其焦点方法签名，这有助于指导模型正确生成测试中的焦点方法调用。路径约束被组合以表明在生成给定测试用例时应应用所有约束。如果生成提示的路径以返回语句终止，则会在提示中添加`returns:
    `说明符，以指导返回值的断言生成。'
- en: 'After constructing a set of path constraint prompts, we use these to iteratively
    prompt the model to generate tests for each path, and include the generated tests
    in the prompt for the next generation. This iterative prompting procedure, illustrated
    in Figure [4](#S3.F4 "Figure 4 ‣ 3.1\. Overview ‣ 3\. Methodology ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM"), reduces the challenging problem of generating a high coverage testsuite
    into a multistage reasoning procedure based on individual execution paths and
    test inputs. Following common practice in code generation, for each generation,
    we check if the output code can be parsed, and if there are parse errors delete
    lines from the end of the generated code until it parses without errors. This
    eliminates errors caused by truncated model outputs.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '在构建一组路径约束提示后，我们使用这些提示迭代性地提示模型为每条路径生成测试，并将生成的测试包含在下一次生成的提示中。这个迭代提示过程，如图 [4](#S3.F4
    "Figure 4 ‣ 3.1\. Overview ‣ 3\. Methodology ‣ Code-Aware Prompting: A study of
    Coverage guided Test Generation in Regression Setting using LLM") 所示，将生成高覆盖率测试套件的挑战性问题简化为基于单独执行路径和测试输入的多阶段推理过程。按照代码生成中的常见做法，对于每次生成，我们检查输出代码是否可以解析，如果存在解析错误，则从生成代码的末尾删除行，直到代码无错误地解析。这消除了由于模型输出被截断而导致的错误。'
- en: Once a full set of tests have been generated, we construct the test execution
    context by importing all defined classes and variables that appear in the generation
    context. In addition, we copy all of the import statements that appear in the
    generation context and include them in the execution context. If the model generated
    any import statements for objects that are imported in the execution context,
    we remove the model-generated import.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦生成了一整套测试，我们通过导入所有在生成上下文中出现的定义类和变量来构建测试执行上下文。此外，我们复制所有在生成上下文中出现的导入语句，并将其包含在执行上下文中。如果模型生成了在执行上下文中已导入的对象的导入语句，我们会删除模型生成的导入。
- en: 4\. Evaluation
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 评估
- en: 'We address the following research questions in our evaluation:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在评估中解决以下研究问题：
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RQ1. Performance Impact. How does SymPrompt effect testing performance over
    simple test generation prompting methods in a regression setting?
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ1. 性能影响。SymPrompt 如何在回归设置中影响简单测试生成提示方法的测试性能？
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RQ2. Training Data Memorization. How does SymPrompt perform on projects that
    do not appear in the model training data?
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ2. 训练数据记忆。SymPrompt 在模型训练数据中未出现的项目上表现如何？
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RQ3. Design Choices. How do path constraints and calling contexts each contribute
    to the performance gains achieved by SymPrompt?
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ3. 设计选择。路径约束和调用上下文各自如何贡献于 SymPrompt 实现的性能提升？
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RQ4. Performance Impact on Large Models. Does SymPrompt still improve performance
    on very large models?
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RQ4. 性能对大型模型的影响。SymPrompt 是否仍然能提高非常大型模型的性能？
- en: Experiment Setting. We perform all evaluations on an AWS p4d.24xl instance with
    8 Nvidia A100 GPUs and 96 vCPUs. We use Python 10 and Pytorch 1.13 with the Huggingface
    transformers framework to run local models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 实验设置。我们在一个 AWS p4d.24xl 实例上进行所有评估，该实例配备了 8 个 Nvidia A100 GPU 和 96 个 vCPU。我们使用
    Python 10 和 Pytorch 1.13 与 Huggingface transformers 框架运行本地模型。
- en: 'Evaluation Metrics. Based on prior work on test generation (Tufano et al.,
    [2020](#bib.bib37); Schäfer et al., [2023a](#bib.bib33)), we use the following
    metrics in evaluation:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。基于以前关于测试生成的工作（Tufano 等，[2020](#bib.bib37)；Schäfer 等，[2023a](#bib.bib33)），我们在评估中使用以下指标：
- en: (1)
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Pass@1: The average number of tests that run without errors and pass in each
    generated testsuite for each focal method when executed.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pass@1：每个焦点方法在每个生成的测试套件中运行无错误且通过的测试的平均数量。
- en: (2)
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'FM Call@1: The average number of tests that correctly call the focal method
    in each generated testsuite for each focal method.'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: FM Call@1：每个生成的测试套件中正确调用焦点方法的测试的平均数量。
- en: (3)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Correct@1: The average number of tests that both pass and correctly call the
    focal method for each generated testsuite.'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Correct@1：每个生成的测试套件中既通过又正确调用焦点方法的测试的平均数量。
- en: (4)
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Line & Branch Coverage: Average line and branch coverage on the focal method
    for each testsuite generation.'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 行 & 分支覆盖率：每个测试套件生成中焦点方法的平均行和分支覆盖率。
- en: These metrics are computed similarly to the standard Pass@1 metric used in program
    generation benchmarks such as HumanEval (Chen et al., [2021](#bib.bib10)) and
    MBPP (Austin et al., [2021](#bib.bib6)), but may contain partially passing rates
    on each generation, since a single generated testsuite may contain both passing
    and failing tests.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标的计算方法类似于用于程序生成基准测试（如 HumanEval (Chen et al., [2021](#bib.bib10)) 和 MBPP
    (Austin et al., [2021](#bib.bib6))）的标准 Pass@1 指标，但可能包含每次生成的部分通过率，因为单个生成的测试套件可能包含通过和失败的测试。
- en: Benchmark Programs. We evaluate on 897 focal methods drawn from 26 open source
    projects used in benchmarks in prior work (Widyasari et al., [2020](#bib.bib40);
    Lemieux et al., [2023](#bib.bib23)). We select focal methods from these projects
    where Pynguin (Lukasczyk and Fraser, [2022](#bib.bib25)) (an SBST tool for Python)
    was unable to achieve full coverage on the focal method during 10 runs, indicating
    the focal method poses a challenge for existing automated test generation tools.
    In this dataset, Pynguin had an average line coverage of 72.4% with std. deviation
    of 12.7%.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基准程序。我们在 26 个开源项目中选取了 897 个焦点方法，这些项目在之前的工作中用于基准测试 (Widyasari et al., [2020](#bib.bib40);
    Lemieux et al., [2023](#bib.bib23))。我们从这些项目中选择了 Pynguin (Lukasczyk and Fraser,
    [2022](#bib.bib25)) (一个用于 Python 的 SBST 工具) 在 10 次运行中未能对焦点方法实现完全覆盖的焦点方法，这表明该焦点方法对现有自动化测试生成工具构成挑战。在此数据集中，Pynguin
    的平均行覆盖率为 72.4%，标准差为 12.7%。
- en: Evaluated Models. Following the literature on test generation with language
    models (Tufano et al., [2020](#bib.bib37); Alagarsamy et al., [2023](#bib.bib4)),
    we evaluate SymPrompt with a single recent open source model, CodeGen2 (Nijkamp
    et al., [2023](#bib.bib28)). To measure SymPrompt’s ability to generalize to larger
    closed source models, we additionally perform an evaluation with GPT-4 (OpenAI,
    [2023](#bib.bib29)).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 评估模型。根据测试生成领域的文献 (Tufano et al., [2020](#bib.bib37); Alagarsamy et al., [2023](#bib.bib4))，我们使用了一个最新的开源模型
    CodeGen2 (Nijkamp et al., [2023](#bib.bib28)) 来评估 SymPrompt。为了衡量 SymPrompt 泛化到更大封闭源模型的能力，我们还进行了与
    GPT-4 (OpenAI, [2023](#bib.bib29)) 的评估。
- en: '4.1\. RQ1: Performance Improvement'
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.1 RQ1: 性能提升'
- en: We first evaluate how SymPrompt effects testing performance over simple test
    generation prompting methods in a regression setting.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先评估 SymPrompt 在回归设置中对简单测试生成提示方法的测试性能影响。
- en: 'Evaluation. We evaluate RQ1 on CodeGen2 (Nijkamp et al., [2023](#bib.bib28)),
    a recently released open source code model with 16 Billion parameters and a mixed
    causal training objective with span corruption and infilling. We use two baselines:
    the pynguin-generated test cases, and and a test completion prompt based on prior
    work using Codex for test generation (Lemieux et al., [2023](#bib.bib23)). For
    each focal method and prompting strategy, we performed 10 generations with CodeGen2
    and averaged the results for each focal method over the 10 generations.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 评估。我们在 CodeGen2 (Nijkamp et al., [2023](#bib.bib28)) 上评估 RQ1，这是一个最近发布的开源代码模型，具有
    160 亿参数，并且具有跨度破坏和填充的混合因果训练目标。我们使用两个基线：pynguin 生成的测试用例，以及基于 Codex 的测试生成的测试完成提示
    (Lemieux et al., [2023](#bib.bib23))。对于每种焦点方法和提示策略，我们使用 CodeGen2 进行了 10 次生成，并对每种焦点方法在
    10 次生成中的结果进行了平均。
- en: 'We also include as a baseline No-Op tests which simply load the target module
    in our execution framework and then return without explicitly running any test
    cases. In many cases loading the module containing the focal method will cause
    a significant proportion of the focal method to be covered, since both the method
    signature and docstring are counted as covered on load. The No-Op test format
    is shown in Figure [8(a)](#S4.F8.sf1 "In Figure 8 ‣ 4.1\. RQ1: Performance Improvement
    ‣ 4\. Evaluation ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM"). The test completion prompt is formatted as
    a partial test function that the model fills in. First a comment specifies the
    method is a test of the focal method, followed by the function signature for the
    a test of the focal method, as shown in Figure [8(b)](#S4.F8.sf2 "In Figure 8
    ‣ 4.1\. RQ1: Performance Improvement ‣ 4\. Evaluation ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还包括作为基准的 No-Op 测试，它仅在我们的执行框架中加载目标模块，然后返回，而没有明确运行任何测试用例。在许多情况下，加载包含焦点方法的模块会导致焦点方法的大部分被覆盖，因为方法签名和文档字符串在加载时都被计为已覆盖。No-Op
    测试格式如图 [8(a)](#S4.F8.sf1 "在图 8 ‣ 4.1\. RQ1: 性能提升 ‣ 4\. 评估 ‣ 代码感知提示：使用 LLM 的回归设置中基于覆盖率的测试生成研究")
    所示。测试完成提示格式为一个部分测试函数，由模型填写。首先，一个注释指定该方法是焦点方法的测试，接着是焦点方法测试的函数签名，如图 [8(b)](#S4.F8.sf2
    "在图 8 ‣ 4.1\. RQ1: 性能提升 ‣ 4\. 评估 ‣ 代码感知提示：使用 LLM 的回归设置中基于覆盖率的测试生成研究") 所示。'
- en: '![Refer to caption](img/723ea463dbcc9d2dd3af7ef7cfb36d3b.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/723ea463dbcc9d2dd3af7ef7cfb36d3b.png)'
- en: (a) No-Op test.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: (a) No-Op 测试。
- en: '![Refer to caption](img/6c2ecd6860605579b6afbb92045e041d.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/6c2ecd6860605579b6afbb92045e041d.png)'
- en: (b) Baseline test prompt.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 基准测试提示。
- en: '![Refer to caption](img/149929732d766de67cdb6ea883dbfda7.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/149929732d766de67cdb6ea883dbfda7.png)'
- en: (c) GPT Describe-Generate prompt.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (c) GPT 描述-生成提示。
- en: Figure 8. Baseline prompts used in evaluation.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8. 评估中使用的基准提示。
- en: 'Observations. Table [1](#S4.T1 "Table 1 ‣ 4.1\. RQ1: Performance Improvement
    ‣ 4\. Evaluation ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM") summarizes our results for the evaluation of
    RQ1\. Overall, we found that SymPrompt significantly improves performance both
    in generating tests that execute with/without errors and call the focal method,
    and also significantly improve coverage over the model tests generated with the
    baseline prompt. We found the SymPrompt is beneficial in two ways: first, the
    structure of path constraint prompts guides the model to generate tests by placing
    relevant information about how to call the focal method correctly based on its
    signature and how to generate correct assertions based on the expected method
    return behavior. This contributes to tests generated with SymPrompt improving
    pass rate by a factor of nearly $4\times$. We found that while the model could
    often generate focal method calls 34% of tests on average with the baseline prompt,
    most of these generations had incorrect focal method usage or other errors that
    prevented them passing, resulting in an overall correct generation of only 3%
    on average.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '观察结果。表 [1](#S4.T1 "表 1 ‣ 4.1\. RQ1: 性能提升 ‣ 4\. 评估 ‣ 代码感知提示：使用 LLM 的回归设置中基于覆盖率的测试生成研究")
    总结了我们对 RQ1 的评估结果。总体而言，我们发现 SymPrompt 在生成测试方面显著提高了性能，无论是执行有无错误的测试还是调用焦点方法的测试，并且在覆盖率上也显著优于基准提示生成的模型测试。我们发现
    SymPrompt 有两个方面的好处：首先，路径约束提示的结构通过提供有关如何正确调用焦点方法的相关信息以及如何根据预期的返回行为生成正确断言，指导模型生成测试。这使得使用
    SymPrompt 生成的测试通过率提高了近 $4\times$。我们发现，虽然模型在基准提示下平均可以生成 34% 的焦点方法调用，但这些生成大多数有错误的焦点方法使用或其他错误，导致它们未能通过，整体正确生成率仅为平均
    3%。'
- en: The tests generated by SymPrompt also achieve a 10% improvement in line coverage
    and 4% in branch coverage over the baseline prompts, indicating that path constraint
    prompts are also effective at guiding the model to generate higher coverage tests
    that exercise more distinct use cases for the target method. Moreover, if the
    line coverage that occurs on module load is taken into account by measuring improvement
    over No-Op tests, then the baseline prompts only improve absolute coverage by
    5% while SymPrompt improves coverage by 15%, a $3\times$ improvement.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: SymPrompt生成的测试还在行覆盖率上比基线提示提高了10%，在分支覆盖率上提高了4%，表明路径约束提示在引导模型生成覆盖率更高的测试，以测试目标方法的更多不同用例方面也有效。此外，如果考虑到模块加载时发生的行覆盖率，通过测量与No-Op测试相比的改进，那么基线提示仅将绝对覆盖率提高了5%，而SymPrompt将覆盖率提高了15%，即$3\times$的改进。
- en: On average, Pynguin achieves 72% line coverage and 64% branch coverage this
    benchmark, which is still significantly higher than the coverage of the tests
    generated by CodeGen2 with SymPrompt. As has been observed in prior work, test
    generation model results are biased by errors in model generations that cause
    many test suites to fail before they can execute the focal method (Schäfer et al.,
    [2023b](#bib.bib34)). In contrast, during Pynguin’s mutation-based testing process,
    it randomly generates and executes many different test candidates, and mutations
    that fail to improve coverage (e.g., to errors) are discarded. Therefore, we also
    run a version of SymPrompt (Symprompt Filtered) where test suites with no working
    tests (i.e., that do not execute and call the focal method) are discarded. Under
    this setting, CodeGen2 SymPrompt compares favorably with Pynguin (77% line coverage,
    66% branch coverage on average).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，Pynguin在此基准测试中实现了72%的行覆盖率和64%的分支覆盖率，这仍然显著高于CodeGen2使用SymPrompt生成的测试的覆盖率。正如先前的工作所观察到的，测试生成模型的结果受到模型生成错误的偏差影响，这些错误导致许多测试套件在执行焦点方法之前失败（Schäfer
    et al., [2023b](#bib.bib34)）。相比之下，在Pynguin的基于突变的测试过程中，它随机生成并执行许多不同的测试候选者，无法提高覆盖率（例如，导致错误）的突变被丢弃。因此，我们还运行了SymPrompt的一个版本（Symprompt
    Filtered），在这个版本中，没有有效测试（即不执行和调用焦点方法）的测试套件被丢弃。在这种设置下，CodeGen2 SymPrompt的表现与Pynguin相当（平均77%的行覆盖率，66%的分支覆盖率）。
- en: 'Case Studies. We found that SymPrompt improved test generation in two ways:
    it reduced errors in calling the focal method by giving more precise guidance,
    and it helped to generate higher coverage test cases by testing more paths. The
    case study shown in Figure [9](#S4.F9 "Figure 9 ‣ 4.1\. RQ1: Performance Improvement
    ‣ 4\. Evaluation ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM") illustrates both of these cases for a focal
    method burp from the pytutils project. The test shown in Figure [9(a)](#S4.F9.sf1
    "In Figure 9 ‣ 4.1\. RQ1: Performance Improvement ‣ 4\. Evaluation ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM") only tests one of the two execution paths in the burp method, where the
    input parameter filename is set to a normal value, and has an error where the
    focal method is called with an incorrect parameter. In contrast, the SymPrompt
    generated tests call burp correctly and use test inputs to cover both the regular
    and filename=‘-’ paths, even though ‘-’ is not a natural name for a file.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '案例研究。我们发现SymPrompt在两方面改善了测试生成：一是通过提供更精确的指导减少了调用焦点方法时的错误，二是通过测试更多路径帮助生成了更高覆盖率的测试用例。图[9](#S4.F9
    "Figure 9 ‣ 4.1\. RQ1: Performance Improvement ‣ 4\. Evaluation ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM")展示的案例说明了pytutils项目中burp焦点方法的这两种情况。图[9(a)](#S4.F9.sf1
    "In Figure 9 ‣ 4.1\. RQ1: Performance Improvement ‣ 4\. Evaluation ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM")中显示的测试仅测试了burp方法的两个执行路径中的一个，其中输入参数filename设置为正常值，并且存在一个错误，即焦点方法使用不正确的参数进行调用。相比之下，SymPrompt生成的测试正确调用burp并使用测试输入覆盖了常规路径和filename=‘-’路径，尽管‘-’不是文件的自然名称。'
- en: 'Figure [10](#S4.F10 "Figure 10 ‣ 4.1\. RQ1: Performance Improvement ‣ 4\. Evaluation
    ‣ Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM") illustrates how SymPrompt can benefit testing a method that
    is challenging for an SBST approach due to specific type requirements for its
    input parameters. The input types are specified in a comment that an SBST tool
    cannot leverage, and as a result it generates test inputs that cause an exception
    on the first line of the method (see Figure [10(b)](#S4.F10.sf2 "In Figure 10
    ‣ 4.1\. RQ1: Performance Improvement ‣ 4\. Evaluation ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM")).
    A language model however can use the comments for type hints along with the method
    signature to generate a correct method call that executes without errors (see
    Figure [10(b)](#S4.F10.sf2 "In Figure 10 ‣ 4.1\. RQ1: Performance Improvement
    ‣ 4\. Evaluation ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM")).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [10](#S4.F10 "图 10 ‣ 4.1. RQ1: 性能提升 ‣ 4. 评估 ‣ 代码感知提示：一种使用 LLM 的回归设置中覆盖引导测试生成的研究")
    说明了 SymPrompt 如何有助于测试由于输入参数的特定类型要求而对 SBST 方法具有挑战性的方法。输入类型在注释中指定，而 SBST 工具无法利用这些注释，结果生成的测试输入会在方法的第一行引发异常（参见图
    [10(b)](#S4.F10.sf2 "图 10 ‣ 4.1. RQ1: 性能提升 ‣ 4. 评估 ‣ 代码感知提示：一种使用 LLM 的回归设置中覆盖引导测试生成的研究")）。然而，语言模型可以利用注释中的类型提示和方法签名来生成正确的方法调用，并且不出现错误（参见图
    [10(b)](#S4.F10.sf2 "图 10 ‣ 4.1. RQ1: 性能提升 ‣ 4. 评估 ‣ 代码感知提示：一种使用 LLM 的回归设置中覆盖引导测试生成的研究")）。'
- en: 'Table 1. RQ 1\. Results (LHS): Results for evaluation of CodeGen2 test generation
    on 897 hard-to-test focal methods are shown in the left 4 columns. SymPrompt is
    effective at guiding the model to call the focal method correctly and generate
    passing tests, as well as covering a wider range of use cases in its generated
    tests. These result in relative improvements of $5\times$ improvement) and coverage
    over the baseline prompt generated tests.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1. RQ 1. 结果（左侧）：对于 897 个难以测试的焦点方法，CodeGen2 测试生成的评估结果显示在前四列中。SymPrompt 在引导模型正确调用焦点方法和生成通过的测试方面非常有效，并且覆盖了更广泛的用例。这些结果相较于基准提示生成的测试有相对
    $5\times$ 的改进和覆盖。
- en: '|  | Full Benchmark | Unseen Projects |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 完整基准 | 未见过的项目 |'
- en: '| Method | Pass@1 | FM Call@1 | Correct@1 | Line Cov. | Branch Cov. | Pass@1
    | FM Call@1 | Correct@1 | Line Cov. | Branch Cov. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Pass@1 | FM Call@1 | Correct@1 | 行覆盖率 | 分支覆盖率 | Pass@1 | FM Call@1 |
    Correct@1 | 行覆盖率 | 分支覆盖率 |'
- en: '| No-Op Tests | 1.00 | 0.00 | 0.00 | 0.33 | 0.33 | 1.00 | 0.00 | 0.00 | 0.26
    | 0.36 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 无操作测试 | 1.00 | 0.00 | 0.00 | 0.33 | 0.33 | 1.00 | 0.00 | 0.00 | 0.26 | 0.36
    |'
- en: '| Baseline Prompt | 0.12 | 0.34 | 0.03 | 0.38 | 0.40 | 0.12 | 0.32 | 0.03 |
    0.32 | 0.45 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 基准提示 | 0.12 | 0.34 | 0.03 | 0.38 | 0.40 | 0.12 | 0.32 | 0.03 | 0.32 | 0.45
    |'
- en: '| SymPrompt | 0.41 | 0.49 | 0.15 | 0.48 | 0.44 | 0.41 | 0.35 | 0.12 | 0.36
    | 0.35 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| SymPrompt | 0.41 | 0.49 | 0.15 | 0.48 | 0.44 | 0.41 | 0.35 | 0.12 | 0.36
    | 0.35 |'
- en: '| Pynguin | - | - | - | 0.72 | 0.64 | - | - | - | 0.68 | 0.57 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Pynguin | - | - | - | 0.72 | 0.64 | - | - | - | 0.68 | 0.57 |'
- en: '| SymPrompt Filtered | 0.81 | 1.00 | 0.81 | 0.77 | 0.66 | 0.83 | 1.00 | 0.83
    | 0.71 | 0.57 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SymPrompt 过滤后的 | 0.81 | 1.00 | 0.81 | 0.77 | 0.66 | 0.83 | 1.00 | 0.83 |
    0.71 | 0.57 |'
- en: '![Refer to caption](img/ae17fcf14ded10bfc036835d064e4d0b.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ae17fcf14ded10bfc036835d064e4d0b.png)'
- en: (a) Baseline test generation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 基准测试生成。
- en: '![Refer to caption](img/7e0c932e0bac36b1e5a0f347b2dcef32.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7e0c932e0bac36b1e5a0f347b2dcef32.png)'
- en: (b) SymPrompt test generation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: (b) SymPrompt 测试生成。
- en: Figure 9. Case study of a simple focal method burp in the pytutils project with
    two main execution paths, based on whether the input filename is - or not. A test
    generation using the baseline prompt executes the method twice, but does not check
    for filename=’-’ as a test input. Moreover, the send focal method call uses a
    nonexistent parameter, iter_by, preventing the test from fully executing. In contrast,
    SymPrompt tests both paths and uses method correctly. Note that in this case the
    method does not return a value, therefore SymPrompt does not prompt, correctly,
    for assertions on the return statement.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9. 在 pytutils 项目中，基于输入文件名是否为 - 的情况，对简单焦点方法的回溯进行的案例研究。使用基准提示的测试生成方法执行了两次，但没有检查文件名为’-’的测试输入。此外，send
    焦点方法调用使用了一个不存在的参数 iter_by，导致测试无法完全执行。相比之下，SymPrompt 测试了两个路径，并正确地使用了方法。注意，在这种情况下，该方法不返回值，因此
    SymPrompt 不会正确地提示关于返回语句的断言。
- en: '![Refer to caption](img/b49d46c5b05ad3097cfd01459b02d02f.png) (a) Focal method.
       ![Refer to caption](img/1b6f399d7d0ecf06c653a9b53dc63ffa.png) (b) SymPrompt
    test generation.  ![Refer to caption](img/3522417883d70d0643c87a7115127bbc.png)
    (c) SBST test generation.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![请参见标题](img/b49d46c5b05ad3097cfd01459b02d02f.png) (a) 焦点方法。    ![请参见标题](img/1b6f399d7d0ecf06c653a9b53dc63ffa.png)
    (b) SymPrompt测试生成。 ![请参见标题](img/3522417883d70d0643c87a7115127bbc.png) (c) SBST测试生成。'
- en: Figure 10. Example test generations from an SBST tool (Pynguin) and LLM (CodeGen2)
    for focal method aes_cbc_decrypt. The method operates on arrays of ints, but an
    SBST approach is unable to infer types (even if they are specified in the comments,
    and generates test inputs that immediately raise an exception on the key_expansion(key)
    call. An LLM can infer input types from context and comments and therefore generate
    a test case with correctly typed inputs that execute the entire method without
    errors.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图10. 来自SBST工具（Pynguin）和LLM（CodeGen2）对焦方法aes_cbc_decrypt的示例测试生成。该方法在整数数组上操作，但SBST方法无法推断类型（即使在注释中指定了），并生成会立即在key_expansion(key)调用时引发异常的测试输入。LLM可以从上下文和注释中推断输入类型，因此能够生成具有正确类型输入的测试用例，这些测试用例能够在没有错误的情况下执行整个方法。
- en: '4.2\. RQ2: Training Data Memorization'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.2\. RQ2: 训练数据记忆'
- en: Since training data memorization can bias results with language model evaluations,
    we evaluate performance separately on a subset of projects that are not included
    in CodeGen2’s training data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练数据记忆可能会影响语言模型评估的结果，我们在CodeGen2训练数据中未包含的项目子集上单独评估性能。
- en: Evaluation. To evaluate the potential impact of training data memorization on
    the results shown in RQ1, we conduct a seperate performance evaluation exclusively
    on projects that were excluded from CodeGen2’s training data. Since CodeGen2 is
    trained on a subset of the Stack (Kocetkov et al., [2022](#bib.bib22)), we use
    the AmIInTheStack tool (int, [[n. d.]](#bib.bib2)) to identify three projects
    in the evaluation set that not included in the stack and evaluate on the focal
    methods drawn from these projects.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 评估。为了评估训练数据记忆对RQ1中显示的结果的潜在影响，我们对从CodeGen2训练数据中排除的项目进行单独的性能评估。由于CodeGen2是在Stack的一个子集上训练的（Kocetkov等，[2022](#bib.bib22)），我们使用AmIInTheStack工具（int，[[n. d.]](#bib.bib2)）识别评估集中未包含在栈中的三个项目，并对从这些项目中抽取的焦点方法进行评估。
- en: 'Observations. Table [1](#S4.T1 "Table 1 ‣ 4.1\. RQ1: Performance Improvement
    ‣ 4\. Evaluation ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM") shows results of this evaluation. Compared to
    the large scale evaluation results, CodeGen2’s test generations with both baseline
    prompts and SymPrompt have lower rates of correct test generations and coverage.
    However, the tests generated with SymPrompt still have significantly higher rates
    of correct test generations ($\tfrac{.12}{.3}=4\times$) and has 12.5% higher line
    coverage relative to the LLM baseline. These results indicate that SymPrompt is
    beneficial for generating more correct tests with higher coverage when focal methods
    are different from those seen in the training data.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '观察。表[1](#S4.T1 "表 1 ‣ 4.1\. RQ1: 性能提升 ‣ 4\. 评估 ‣ 代码感知提示：使用LLM在回归设置中进行覆盖引导的测试生成研究")显示了这次评估的结果。与大规模评估结果相比，CodeGen2使用基准提示和SymPrompt生成的测试具有更低的正确测试生成率和覆盖率。然而，使用SymPrompt生成的测试仍然具有显著更高的正确测试生成率（$\tfrac{.12}{.3}=4\times$）和相对于LLM基线高出12.5%的行覆盖率。这些结果表明，SymPrompt在焦点方法与训练数据中看到的方法不同的情况下，有助于生成更多正确的测试和更高的覆盖率。'
- en: '4.3\. RQ3: Design Choices'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.3\. RQ3: 设计选择'
- en: 'Table 2. RQ 3\. Results (LHS): Ablation results evaluated on 100 randomly sampled
    focal methods from the benchmark used in RQ1\. The ablation results demonstrate
    that both type and dependency in calling context and path constraint prompts improve
    correct generations and coverage relative to baseline prompts, but path constraint
    prompts contribute significantly more to the overall improvement in correct generations
    and coverage demonstrated by SymPrompt for generations with CodeGen2\. RQ 4\.
    Results (RHS): Results with GPT. When evaluating GPT we use as a baseline a two
    stage describe then generate prompt based on prior work. Although performance
    is similar for both prompting approaches without calling context, when SymPrompt
    is used without ablation it gives a relative improvement of 178% in Correct@1
    rate and 1.05% relative improvement in coverage over baseline prompts.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. RQ 3\. 结果（左侧）：在从RQ1中使用的基准中随机抽取的100个焦点方法上评估的消融结果。消融结果表明，无论是调用上下文还是路径约束提示中的类型和依赖性，都相较于基线提示改善了正确生成和覆盖率，但路径约束提示对正确生成和覆盖率的整体改进贡献更大，尤其是在使用CodeGen2进行生成时，SymPrompt展示了这一点。RQ
    4\. 结果（右侧）：GPT的结果。在评估GPT时，我们使用基线作为描述然后生成的两阶段提示，基于之前的工作。虽然在没有调用上下文的情况下，两种提示方法的表现相似，但当SymPrompt在没有消融的情况下使用时，相比于基线提示，在Correct@1率上有178%的相对提升，在覆盖率上有1.05%的相对提升。
- en: '|  | CodeGen2 | GPT-4 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | CodeGen2 | GPT-4 |'
- en: '| --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Method | Pass@1 | FM Call@1 | Correct@1 | Line Cov. | Branch Cov. | Pass@1
    | FM Call@1 | Correct@1 | Line Cov. | Branch Cov. |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | Pass@1 | FM Call@1 | Correct@1 | 行覆盖率 | 分支覆盖率 | Pass@1 | FM Call@1 |
    Correct@1 | 行覆盖率 | 分支覆盖率 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Baseline Prompt | 0.09 | 0.37 | 0.04 | 0.30 | 0.29 | 0.14 | 0.12 | 0.09 |
    0.36 | 0.40 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 基线提示 | 0.09 | 0.37 | 0.04 | 0.30 | 0.29 | 0.14 | 0.12 | 0.09 | 0.36 | 0.40
    |'
- en: '| Constraints Only | 0.27 | 0.49 | 0.17 | 0.49 | 0.38 | 0.15 | 0.15 | 0.10
    | 0.39 | 0.43 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 约束条件仅 | 0.27 | 0.49 | 0.17 | 0.49 | 0.38 | 0.15 | 0.15 | 0.10 | 0.39 | 0.43
    |'
- en: '| Context Only | 0.16 | 0.40 | 0.06 | 0.42 | 0.35 | 0.38 | 0.28 | 0.18 | 0.43
    | 0.47 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 仅上下文 | 0.16 | 0.40 | 0.06 | 0.42 | 0.35 | 0.38 | 0.28 | 0.18 | 0.43 | 0.47
    |'
- en: '| SymPrompt | 0.50 | 0.65 | 0.26 | 0.53 | 0.42 | 0.46 | 0.39 | 0.25 | 0.74
    | 0.74 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| SymPrompt | 0.50 | 0.65 | 0.26 | 0.53 | 0.42 | 0.46 | 0.39 | 0.25 | 0.74
    | 0.74 |'
- en: SymPrompt uses both selective type and dependency focal context and path constraint
    prompts to improve performance over baseline test completion prompts. We evaluate
    the impact of each of these components on SymPrompt’s performance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: SymPrompt使用选择性类型和依赖焦点上下文及路径约束提示来改善性能，超越基线测试完成提示。我们评估了这些组件对SymPrompt性能的影响。
- en: Evaluation. We conduct an ablation to evaluate how each of these methods contributes
    to performance improvements in isolation. When ablating focal context, we use
    the local context around the focal method based on prior work (Lemieux et al.,
    [2023](#bib.bib23)). We evaluate on 100 randomly sampled focal methods from the
    benchmark used in RQ1.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 评估。我们进行了一次消融实验，以评估这些方法中每一种对性能提升的贡献。在消融焦点上下文时，我们使用基于之前工作的焦点方法周围的局部上下文（Lemieux
    et al., [2023](#bib.bib23)）。我们在从RQ1中使用的基准中随机抽取的100个焦点方法上进行评估。
- en: 'Observations. Table [2](#S4.T2 "Table 2 ‣ 4.3\. RQ3: Design Choices ‣ 4\. Evaluation
    ‣ Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM") (LHS) shows results of the ablations. SymPrompt with no ablations
    achieves an overall 26% correct generation rate and 53% coverage on average. Ablating
    path constraint prompts but retaining calling context reduces the FM call rate
    and substantially reduces pass rate, indicating that the additional guidance from
    path constraint prompting is very beneficial for generating tests with correct
    focal method calls. The path constraint ablation also has significantly lower
    coverage, indicating that the path constraints serve to generate more thorough
    test cases.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '观察。表[2](#S4.T2 "表2 ‣ 4.3\. RQ3: 设计选择 ‣ 4\. 评估 ‣ Code-Aware Prompting: 基于LLM的回归设置中的覆盖指导测试生成研究")（左侧）显示了消融的结果。没有消融的SymPrompt整体达到26%的正确生成率和53%的平均覆盖率。消融路径约束提示但保留调用上下文减少了FM调用率，并显著降低了通过率，表明路径约束提示提供的额外指导对生成正确的焦点方法调用测试非常有益。路径约束消融也有显著较低的覆盖率，这表明路径约束有助于生成更全面的测试用例。'
- en: The ablation of calling context also reduces the pass rate and FM call rate
    of generated tests, although much less than ablating path constraint prompting,
    and still has significantly better performance than a full ablation of both methods.
    These results indicate that while test generations with path constraint prompts
    benefit from using calling context, the additional guidance and structure provided
    to the model in the symbolic prompts are crucial to the performance improvements
    in correct generations and coverage by SymPrompt over baseline test generation
    prompts.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 调用上下文的消融也减少了生成测试的通过率和FM调用率，尽管远不如消融路径约束提示那么显著，但仍然比完全消融两种方法的性能要好。这些结果表明，虽然带有路径约束提示的测试生成在使用调用上下文时有所受益，但提供给模型的符号提示中的额外指导和结构对于SymPrompt在正确生成和覆盖率方面的性能提升是至关重要的。
- en: '4.4\. RQ4: Large Model Performance Impact'
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '4.4\. RQ4: 大模型性能影响'
- en: In addition to evaluating on an open source 16B parameter model, we also evaluate
    if the prompting strategy used by SymPrompt can benefit test generations with
    a larger and better trained model. Recent work has shown that increasingly large
    scale language models exhibit *emergent abilities* that are completely absent
    in smaller scale models (Wei et al., [2022a](#bib.bib38)). In this evaluation
    we show that a significantly larger language model, GPT-4, exhibits the ability
    to reason precisely about path constraints in a focal method and generate its
    own path constraint prompts in a zero-shot setting. We find that prompting the
    model to approach test generation in this way is very beneficial for generating
    high coverage testsuites.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对开源的16B参数模型进行评估外，我们还评估了SymPrompt使用的提示策略是否能在更大、更好训练的模型中带来测试生成的好处。近期研究表明，越来越大规模的语言模型展现出了*突现能力*，这些能力在较小规模的模型中完全不存在（Wei
    et al., [2022a](#bib.bib38)）。在本次评估中，我们展示了一个显著更大的语言模型GPT-4在零样本设置下，能够准确推理路径约束并生成自己的路径约束提示。我们发现，提示模型以这种方式进行测试生成对生成高覆盖率的测试集非常有利。
- en: 'Evaluation. We evaluate with GPT-4 (OpenAI, [2023](#bib.bib29)), a significantly
    larger model than CodeGen2 that benefits from much more extensive training. We
    found that GPT-4 was capable of generating precise descriptions of the execution
    paths and constraints in focal methods given a 0-shot prompt, so instead of generating
    path prompts with static analysis, we prompt the model to describe execution paths
    and then embed each path description in a test docstring (See Figure [8(c)](#S4.F8.sf3
    "In Figure 8 ‣ 4.1\. RQ1: Performance Improvement ‣ 4\. Evaluation ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM")).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '评估。我们使用GPT-4（OpenAI, [2023](#bib.bib29)）进行评估，该模型比CodeGen2大得多，并且经过了更广泛的训练。我们发现，GPT-4能够在零样本提示下生成精确的执行路径和焦点方法中的约束描述，因此我们没有使用静态分析生成路径提示，而是提示模型描述执行路径，然后将每个路径描述嵌入到测试文档字符串中（见图
    [8(c)](#S4.F8.sf3 "在图8中 ‣ 4.1\. RQ1: 性能提升 ‣ 4\. 评估 ‣ 代码感知提示：使用LLM进行回归设置中的覆盖指导测试生成研究")）。'
- en: Recent works have demonstrated that test generation with GPT models benefit
    from multi-stage prompts that incorporate description and planning (Yuan et al.,
    [2023](#bib.bib44); Li et al., [2023](#bib.bib24)), therefore as a baseline we
    use a 2 stage prompt that first asks the model to describe the intent of the method
    under test and then to generate a testsuite based on both the description and
    the focal context based on (Yuan et al., [2023](#bib.bib44)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，使用GPT模型的测试生成从包含描述和规划的多阶段提示中受益（Yuan et al., [2023](#bib.bib44); Li et
    al., [2023](#bib.bib24)），因此作为基线我们使用一个2阶段提示，首先要求模型描述被测方法的意图，然后基于描述和焦点上下文生成测试集（基于Yuan
    et al., [2023](#bib.bib44)）。
- en: 'Observations. Table [2](#S4.T2 "Table 2 ‣ 4.3\. RQ3: Design Choices ‣ 4\. Evaluation
    ‣ Code-Aware Prompting: A study of Coverage guided Test Generation in Regression
    Setting using LLM") (RHS) summarizes the results of our evaluation with GPT-4\.
    We found that calling contexts in the generated tests were especially important
    for GPT-4s generations, since the model was prone to hullucinating incorrect import
    statements or using undefined classes and objects that were defined in the focal
    context. Overwriting the model-generated imports with the classes and objects
    identified in the calling context greatly reduced errors when the tests were executed.
    Using the path constraint prompts generated by GPT-4 in isolation did not lead
    to significant performance improvements over the baseline describe and generate
    prompts, but when used in conjunction with calling context the path constraint
    prompts improved the average coverage of generated tests over the baseline describe-generate
    prompt by a factor of more than 2, from 36% to 74%. We hypothesize that the significant
    performance difference occurs because path constraint prompts are effective for
    generating more high coverage testsuites, but most of those tests fail due to
    errors with imports and undefined variables when calling contexts are not used.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '观察。表格[2](#S4.T2 "表格 2 ‣ 4.3\. RQ3: 设计选择 ‣ 4\. 评估 ‣ 代码感知提示：基于 LLM 的回归设置中的覆盖引导测试生成研究")
    (右侧) 总结了我们使用 GPT-4 进行评估的结果。我们发现，生成测试中的调用上下文对于 GPT-4 的生成尤为重要，因为该模型容易产生错误的导入语句或使用在焦点上下文中定义的未定义类和对象。用调用上下文中识别的类和对象覆盖模型生成的导入，显著减少了测试执行时的错误。单独使用
    GPT-4 生成的路径约束提示并未显著提升相较于基线描述和生成提示的性能，但与调用上下文一起使用时，路径约束提示将生成测试的平均覆盖率提高了2倍以上，从36%提升到74%。我们推测，显著的性能差异发生在于路径约束提示在生成更高覆盖率的测试套件时效果显著，但大多数测试由于导入和未定义变量的错误而失败，当未使用调用上下文时尤为明显。'
- en: 'GPT vs. Analysis-Generated Path Prompts. Figure [11](#S4.F11 "Figure 11 ‣ 4.4\.
    RQ4: Large Model Performance Impact ‣ 4\. Evaluation ‣ Code-Aware Prompting: A
    study of Coverage guided Test Generation in Regression Setting using LLM") shows
    a comparison of the path descriptions generated by GPT-4 and SymPrompt’s static
    analysis on the method _serialize shown in Figure [12(a)](#S4.F12.sf1 "In Figure
    12 ‣ 4.4\. RQ4: Large Model Performance Impact ‣ 4\. Evaluation ‣ Code-Aware Prompting:
    A study of Coverage guided Test Generation in Regression Setting using LLM").
    We found that, compared to the static analysis, GPT-4 was able to generate more
    natural path constraints while still giving precise descriptions. Figure [12](#S4.F12
    "Figure 12 ‣ 4.4\. RQ4: Large Model Performance Impact ‣ 4\. Evaluation ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM") shows the test cases generated for two GPT path prompts on the method _serialize.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPT 与分析生成的路径提示。图[11](#S4.F11 "图 11 ‣ 4.4\. RQ4: 大模型性能影响 ‣ 4\. 评估 ‣ 代码感知提示：基于
    LLM 的回归设置中的覆盖引导测试生成研究") 展示了 GPT-4 生成的路径描述与 SymPrompt 的静态分析在方法 _serialize 上的比较，如图[12(a)](#S4.F12.sf1
    "在图 12 ‣ 4.4\. RQ4: 大模型性能影响 ‣ 4\. 评估 ‣ 代码感知提示：基于 LLM 的回归设置中的覆盖引导测试生成研究") 所示。我们发现，相比静态分析，GPT-4
    能够生成更自然的路径约束，同时仍然给出精确的描述。图[12](#S4.F12 "图 12 ‣ 4.4\. RQ4: 大模型性能影响 ‣ 4\. 评估 ‣ 代码感知提示：基于
    LLM 的回归设置中的覆盖引导测试生成研究") 展示了两种 GPT 路径提示生成的测试用例，方法为 _serialize。'
- en: 'Path-Following Generation Accuracy. In addition to measuring the impact of
    SymPrompt on overall coverage, we conducted a small study of how effective the
    models are at generating tests that follow the specific paths specified in the
    prompts by manually examining 10 generated tests in the GPT-generated set. Of
    these, 6 out of 10 followed their specified paths. The missing four cases are
    either due to deeply nested branching or exception handling–the model either did
    not generate correct preconditions for deeply nested branches or error-handling
    paths. We also observed qualitatively that CodeGen2 usually generates test inputs
    that follow specified paths when the constraint involves an input parameter, as
    shown in Figure [1](#S1.F1 "Figure 1 ‣ Results. ‣ 1\. Introduction ‣ Code-Aware
    Prompting: A study of Coverage guided Test Generation in Regression Setting using
    LLM"), while GPT-4 is also able to generate test inputs for more complex path
    constraints involving class variables and external function calls, as shown in
    Figure [12](#S4.F12 "Figure 12 ‣ 4.4\. RQ4: Large Model Performance Impact ‣ 4\.
    Evaluation ‣ Code-Aware Prompting: A study of Coverage guided Test Generation
    in Regression Setting using LLM").'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '路径跟踪生成准确性。除了测量 SymPrompt 对整体覆盖率的影响外，我们还进行了一项小型研究，检查模型在生成遵循提示中指定的特定路径的测试时的有效性，通过手动检查
    GPT 生成集中的 10 个生成测试。其中特定路径的 6 个测试中有 6 个遵循了指定路径。其余四个案例要么由于深度嵌套的分支要么由于异常处理——模型要么未生成正确的深度嵌套分支或错误处理路径的前提条件。我们还定性观察到，当约束涉及输入参数时，CodeGen2
    通常生成遵循指定路径的测试输入，如图 [1](#S1.F1 "图 1 ‣ 结果 ‣ 1\. 引言 ‣ 代码感知提示：使用 LLM 的回归设置中的覆盖率引导测试生成研究")
    所示，而 GPT-4 也能生成更复杂路径约束的测试输入，包括类变量和外部函数调用，如图 [12](#S4.F12 "图 12 ‣ 4.4\. RQ4: 大型模型性能影响
    ‣ 4\. 评估 ‣ 代码感知提示：使用 LLM 的回归设置中的覆盖率引导测试生成研究") 所示。'
- en: '![Refer to caption](img/12dca7bf2a8d790f2cd192ea9d2b5c37.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/12dca7bf2a8d790f2cd192ea9d2b5c37.png)'
- en: (a) SymPrompt path prompt.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: (a) SymPrompt 路径提示。
- en: '![Refer to caption](img/5458ceb04c7403184507ae53eade043c.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5458ceb04c7403184507ae53eade043c.png)'
- en: (b) GPT-4 path prompt.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GPT-4 路径提示。
- en: Figure 11. Comparison of SymPrompt-generated path prompt to GPT-4 generated
    path prompt. GPT-4 is capable of generating precise execution path descriptions
    with more natural language. We construct path prompts by using a markdown parser
    to extract each path description and embed them as docstrings for each test generation.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11. SymPrompt 生成的路径提示与 GPT-4 生成的路径提示的比较。GPT-4 能够生成更自然语言的精确执行路径描述。我们通过使用 markdown
    解析器提取每个路径描述，并将其作为 docstrings 嵌入每个测试生成中。
- en: '![Refer to caption](img/2f67cd4e6e29a9e4153c22013ce021a2.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f67cd4e6e29a9e4153c22013ce021a2.png)'
- en: (a) Focal method.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 焦点方法。
- en: '![Refer to caption](img/fd1ecd0453f5978d93ef0c5174a34258.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fd1ecd0453f5978d93ef0c5174a34258.png)'
- en: (b) GPT-4 generated tests.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GPT-4 生成的测试。
- en: Figure 12. Case study showing GPT generations with path constraint prompts.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12. 案例研究展示了具有路径约束提示的 GPT 生成。
- en: 5\. Threats to Validity & Discussion
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 有效性威胁与讨论
- en: 'Model and Benchmark Validity: Our evaluations focus on open source Python projects
    and utilize specific language models (CodeGen2 and GPT-4). This restricts the
    generalizability of our findings to other languages or models. However, these
    are two large state-of-the-art models. Also, none of the methods are specific
    to Python. So we expect the findings will hold consistently in other settings.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和基准有效性：我们的评估重点关注开源 Python 项目，并利用特定的语言模型（CodeGen2 和 GPT-4）。这限制了我们发现的对其他语言或模型的普遍适用性。然而，这两个模型都是最新的前沿模型。此外，所有方法都不是
    Python 特有的。因此，我们期望这些发现会在其他设置中保持一致。
- en: 'Memorization Validity: Although we use the AmIInTheStack tool to prevent training
    data memorization from biasing our RQ 2 results, the possibility remains that
    CodeGen2 could have seen some of the focal methods or similar code in its training
    data. However, since these models are not explicitly trained for test generation
    tasks, we think this threat is minimal.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆有效性：虽然我们使用 AmIInTheStack 工具来防止训练数据记忆对我们的 RQ 2 结果产生偏差，但 CodeGen2 可能仍然会看到一些焦点方法或类似代码。尽管如此，由于这些模型并未明确针对测试生成任务进行训练，我们认为这一威胁是最小的。
- en: 'Metric Validity: Our evaluations are based on the metrics Pass@1, FM Call@1,
    Correct@1, and Line Coverage. However, these metrics might not capture the full
    complexity or usefulness of a generated test case.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 指标有效性：我们的评估基于 Pass@1、FM Call@1、Correct@1 和 Line Coverage 这些指标。然而，这些指标可能无法捕捉生成测试用例的全部复杂性或实用性。
- en: 'Test Generation in a Regression Setting: In this paper, we operated under the
    assumption that the tests are generated within a regression setting, assuming
    the correctness of the underlying focal method implementation. As a result, our
    generated tests may not uncover any implementation bugs. This approach to testing
    also has limitations, particularly when the implementation of the focal method
    is not yet finalized, a scenario commonly encountered in continuous development
    environments.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 回归设置中的测试生成：在本文中，我们假设测试是在回归设置中生成的，假设底层焦点方法实现的正确性。因此，我们生成的测试可能不会发现任何实现上的 bug。这种测试方法也有局限性，特别是在焦点方法的实现尚未最终确定时，这在持续开发环境中是常见的情况。
- en: 6\. Related Work
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 相关工作
- en: 'Our work relates to the following areas: Search Based Software Testing, Symbolic
    Test Generation, Test Generation with LLMs, and Hybrid LLM-SBST test generation.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作涉及以下领域：基于搜索的软件测试、符号测试生成、使用大语言模型的测试生成以及混合的大语言模型-基于搜索的软件测试（LLM-SBST）测试生成。
- en: Search Based Software Testing (SBST) and Symbolic Approaches. Evosuite is an
    SBST regression testing framework for java that generates regression tests based
    mutation testing and coverage guided randomized test generation (Fraser and Arcuri,
    [2011](#bib.bib14), [2013](#bib.bib15), [2015](#bib.bib17)). Randoop uses coverage
    guided randomized test generation for Java in conjunction with sanity checking
    oracles to check for common classes of bugs like NullException errors (Pacheco
    and Ernst, [2007](#bib.bib30); Pacheco et al., [2007](#bib.bib31)). Pynguin applies
    SBST to generate regression tests for Python (Lukasczyk et al., [2020](#bib.bib26);
    Lukasczyk and Fraser, [2022](#bib.bib25)). PeX is a whitebox test generation with
    concolic execution and constraint solver (Tillmann and De Halleux, [2008](#bib.bib36)).
    Korat tests based on a formal specification (Boyapati et al., [2002](#bib.bib9)).
    Dart performs concolic test generation (Godefroid et al., [2005](#bib.bib19)).
    Cute performs concolic testing for c (Sen et al., [2005](#bib.bib35)). Our approach
    is conceptually related to coverage driven SBST approaches and Concolic Execution
    because it formulates test generation as a constraint solving problem for the
    LLM, where the LLM is guided to generate a test that will follow a specific execution
    path. However, instead of performing symbolic constraint solving to follow specific
    execution paths we give the LLM access to the focal method source code.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 基于搜索的软件测试（SBST）和符号方法。Evosuite 是一个用于 Java 的 SBST 回归测试框架，通过变异测试和覆盖率驱动的随机测试生成来生成回归测试
    (Fraser 和 Arcuri, [2011](#bib.bib14), [2013](#bib.bib15), [2015](#bib.bib17))。Randoop
    使用覆盖率驱动的随机测试生成与健全性检查 oracle 结合来检查常见的 bug 类别，如 NullException 错误 (Pacheco 和 Ernst,
    [2007](#bib.bib30); Pacheco 等, [2007](#bib.bib31))。Pynguin 将 SBST 应用于生成 Python
    的回归测试 (Lukasczyk 等, [2020](#bib.bib26); Lukasczyk 和 Fraser, [2022](#bib.bib25))。PeX
    是一个带有合成执行和约束求解器的白盒测试生成工具 (Tillmann 和 De Halleux, [2008](#bib.bib36))。Korat 基于形式规范进行测试
    (Boyapati 等, [2002](#bib.bib9))。Dart 执行合成测试生成 (Godefroid 等, [2005](#bib.bib19))。Cute
    对 C 语言进行合成测试 (Sen 等, [2005](#bib.bib35))。我们的方法在概念上与覆盖驱动的 SBST 方法和合成执行相关，因为它将测试生成表述为
    LLM 的约束求解问题，其中 LLM 被引导生成一个将遵循特定执行路径的测试。然而，与执行符号约束求解以遵循特定执行路径不同，我们将 LLM 访问焦点方法的源代码。
- en: LLM Regression Test generation. Athenatest finetune pretrained transformers
    on paired method-test data and show including more focal context in prompt leads
    to higher coverage test generations (Tufano et al., [2020](#bib.bib37)). Bariess
    et. al. compare Codex-generated tests to randoop based on a one-shot test example
    and show the codex-generated tests achieve better coverage(arxiv) (Bareiß et al.,
    [2022](#bib.bib8)). A3Test apply postprocessing to correct test naming errors
    in model generations using PLBart(arxiv) (Alagarsamy et al., [2023](#bib.bib4)).
    Hashtroudi et al. show that models finetuned on existing project testsuites output
    higher quality test generations(arxiv) (Hashtroudi et al., [2023](#bib.bib20)).
    MuTAP uses mutation testing to guide LLM test generations towards tests that are
    more likely to detect bugs and show improved bug detection with Codex and Llama2
    on Defects4j(arxiv) (Dakhel et al., [2023](#bib.bib12)). These approaches use
    fixed prompting strategies, our work focuses on developing code-aware prompts
    that guide the model to generate a high coverage set of tests.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 回归测试生成。Athenatest 对预训练的变换器进行微调，使用配对的方法-测试数据，显示在提示中包含更多的焦点上下文会导致更高的覆盖率测试生成
    (Tufano et al., [2020](#bib.bib37))。Bariess et al. 将 Codex 生成的测试与 randoop 进行比较，基于单次测试示例，结果显示
    Codex 生成的测试实现了更好的覆盖率 (arxiv) (Bareiß et al., [2022](#bib.bib8))。A3Test 使用后处理来修正模型生成中的测试命名错误，使用
    PLBart (arxiv) (Alagarsamy et al., [2023](#bib.bib4))。Hashtroudi et al. 表明，在现有项目测试套件上进行微调的模型生成的测试质量更高
    (arxiv) (Hashtroudi et al., [2023](#bib.bib20))。MuTAP 使用突变测试来指导 LLM 测试生成，生成更有可能检测到错误的测试，并显示在
    Defects4j 上使用 Codex 和 Llama2 进行的 bug 检测有所改进 (arxiv) (Dakhel et al., [2023](#bib.bib12))。这些方法使用固定的提示策略，我们的工作专注于开发代码感知提示，引导模型生成高覆盖率的测试集。
- en: ChatGPT test generation. Testpilot generates javascript tests with ChatGPT 3.5
    using a 0-shot test prompt and then iteratively adds additional code and documentation
    context if the generated tests fail(arxiv) (Schäfer et al., [2023a](#bib.bib33)).
    ChatUnitTest generates tests using adaptive focal context based on the maximum
    focal length and then applies both rules-based repair and self-debugging based
    on error messages when generated tests fail (Xie et al., [2023](#bib.bib41)).
    ChatTester similarly generates tests based on the focal context and then prompts
    ChatGPT with error messages when generated tests fail(arxiv) (Yuan et al., [2023](#bib.bib44)).
    These approaches all focus on constructing the focal context and then fixing ChatGPT’s
    generations. In contrast our work focuses on developing prompts to guide the model
    to test each execution path in the focal method, improving testsuite coverage.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 测试生成。Testpilot 使用 ChatGPT 3.5 生成 JavaScript 测试，采用零-shot 测试提示，然后在生成的测试失败时迭代性地添加额外的代码和文档上下文
    (arxiv) (Schäfer et al., [2023a](#bib.bib33))。ChatUnitTest 使用基于最大焦点长度的自适应焦点上下文生成测试，然后在生成的测试失败时应用基于规则的修复和自我调试
    (Xie et al., [2023](#bib.bib41))。ChatTester 类似地基于焦点上下文生成测试，并在生成的测试失败时用错误消息提示 ChatGPT
    (arxiv) (Yuan et al., [2023](#bib.bib44))。这些方法都专注于构建焦点上下文，然后修复 ChatGPT 的生成。相比之下，我们的工作专注于开发提示，引导模型测试焦点方法中的每条执行路径，从而提高测试套件的覆盖率。
- en: Hybrid SBST-LLM regression test generation. Codamosa runs Pynguin, an SBST tool
    for python, and iteratively calls Codex to generate additional testcases for methods
    with low coverage (Lemieux et al., [2023](#bib.bib23)). Our work focuses on improving
    LLM testsuite generations instead.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 混合 SBST-LLM 回归测试生成。Codamosa 运行 Pynguin，这是一个用于 Python 的 SBST 工具，并迭代调用 Codex 为覆盖率低的方法生成额外的测试用例
    (Lemieux et al., [2023](#bib.bib23))。我们的工作则专注于改进 LLM 测试套件生成。
- en: 7\. Conclusion
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 结论
- en: This paper introduces SymPrompt, a novel approach to test generation with LLMs
    by decomposing the test suite generation process into a structured, code-aware
    sequence of prompts. SymPrompt significantly enhances the generation of comprehensive
    test suites with a recent open source Code LLM, CodeGen2, and achieves substantial
    improvements in the ratio of correct test generations and coverage. Moreover,
    we show that when given a specific instruction prompt to analyze execution path
    constraints, GPT-4 is capable of generating its own path constraint prompts, which
    improves the coverage of its generating testsuites by a factor of $2\times$ over
    prompting strategies from recent prior work.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了SymPrompt，一种通过将测试套件生成过程分解为结构化的、代码感知的提示序列来生成测试的新方法。SymPrompt显著提高了与最近开源的代码LLM
    CodeGen2的综合测试套件生成，并在正确测试生成率和覆盖率上取得了显著的改进。此外，我们展示了当给予一个特定的指令提示来分析执行路径约束时，GPT-4能够生成其自己的路径约束提示，这使其生成的测试套件的覆盖率比最近工作的提示策略提高了$2\times$。
- en: References
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: int ([n. d.]) [n. d.]. Am I in the stack? [https://huggingface.co/spaces/bigcode/in-the-stack](https://huggingface.co/spaces/bigcode/in-the-stack).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: int ([n. d.]) [n. d.]。我在栈中吗？[https://huggingface.co/spaces/bigcode/in-the-stack](https://huggingface.co/spaces/bigcode/in-the-stack)。
- en: Ahmad et al. (2020) Wasi Ahmad, Saikat Chakraborty, Baishakhi Ray, and Kai-Wei
    Chang. 2020. A Transformer-based Approach for Source Code Summarization. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*.
    Association for Computational Linguistics, Online, 4998–5007. [https://doi.org/10.18653/v1/2020.acl-main.449](https://doi.org/10.18653/v1/2020.acl-main.449)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ahmad等（2020）Wasi Ahmad、Saikat Chakraborty、Baishakhi Ray和Kai-Wei Chang。2020年。一种基于Transformer的源代码摘要方法。在*第58届计算语言学协会年会论文集*中。计算语言学协会，在线，4998–5007。[https://doi.org/10.18653/v1/2020.acl-main.449](https://doi.org/10.18653/v1/2020.acl-main.449)
- en: 'Alagarsamy et al. (2023) Saranya Alagarsamy, Chakkrit Tantithamthavorn, and
    Aldeida Aleti. 2023. A3Test: Assertion-Augmented Automated Test Case Generation.
    *arXiv preprint arXiv:2302.10352* (2023).'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alagarsamy等（2023）Saranya Alagarsamy、Chakkrit Tantithamthavorn和Aldeida Aleti。2023年。A3Test：断言增强的自动化测试用例生成。*arXiv预印本
    arXiv:2302.10352*（2023年）。
- en: Allamanis et al. (2015) Miltiadis Allamanis, Earl T Barr, Christian Bird, and
    Charles Sutton. 2015. Suggesting accurate method and class names. In *Proceedings
    of the 2015 10th joint meeting on foundations of software engineering*. 38–49.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Allamanis等（2015）Miltiadis Allamanis、Earl T Barr、Christian Bird和Charles Sutton。2015年。建议准确的方法和类名。在*2015年第10届软件工程基础联合会议论文集*中，38–49。
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program synthesis with large language models. *arXiv preprint
    arXiv:2108.07732* (2021).
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin等（2021）Jacob Austin、Augustus Odena、Maxwell Nye、Maarten Bosma、Henryk Michalewski、David
    Dohan、Ellen Jiang、Carrie Cai、Michael Terry、Quoc Le等。2021年。使用大型语言模型的程序合成。*arXiv预印本
    arXiv:2108.07732*（2021年）。
- en: Baldoni et al. (2018) Roberto Baldoni, Emilio Coppa, Daniele Cono D’elia, Camil
    Demetrescu, and Irene Finocchi. 2018. A survey of symbolic execution techniques.
    *ACM Computing Surveys (CSUR)* 51, 3 (2018), 1–39.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baldoni等（2018）Roberto Baldoni、Emilio Coppa、Daniele Cono D’elia、Camil Demetrescu和Irene
    Finocchi。2018年。符号执行技术综述。*ACM计算机调查（CSUR）* 51, 3（2018），1–39。
- en: Bareiß et al. (2022) Patrick Bareiß, Beatriz Souza, Marcelo d’Amorim, and Michael
    Pradel. 2022. Code generation tools (almost) for free? a study of few-shot, pre-trained
    language models on code. *arXiv preprint arXiv:2206.01335* (2022).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bareiß等（2022）Patrick Bareiß、Beatriz Souza、Marcelo d’Amorim和Michael Pradel。2022年。代码生成工具（几乎）免费？对少量样本预训练语言模型在代码上的研究。*arXiv预印本
    arXiv:2206.01335*（2022年）。
- en: 'Boyapati et al. (2002) Chandrasekhar Boyapati, Sarfraz Khurshid, and Darko
    Marinov. 2002. Korat: Automated testing based on Java predicates. *ACM SIGSOFT
    Software Engineering Notes* 27, 4 (2002), 123–133.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boyapati等（2002）Chandrasekhar Boyapati、Sarfraz Khurshid和Darko Marinov。2002年。Korat：基于Java谓词的自动化测试。*ACM
    SIGSOFT软件工程笔记* 27, 4（2002），123–133。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374* (2021).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen等（2021）Mark Chen、Jerry Tworek、Heewoo Jun、Qiming Yuan、Henrique Ponde de Oliveira
    Pinto、Jared Kaplan、Harri Edwards、Yuri Burda、Nicholas Joseph、Greg Brockman等。2021年。评估在代码上训练的大型语言模型。*arXiv预印本
    arXiv:2107.03374*（2021年）。
- en: Daka and Fraser (2014) Ermira Daka and Gordon Fraser. 2014. A survey on unit
    testing practices and problems. In *2014 IEEE 25th International Symposium on
    Software Reliability Engineering*. IEEE, 201–211.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daka 和 Fraser（2014）Ermira Daka 和 Gordon Fraser。2014年。单元测试实践和问题的调查。在 *2014年 IEEE
    第25届国际软件可靠性工程研讨会*。IEEE，201–211页。
- en: Dakhel et al. (2023) Arghavan Moradi Dakhel, Amin Nikanjam, Vahid Majdinasab,
    Foutse Khomh, and Michel C Desmarais. 2023. Effective Test Generation Using Pre-trained
    Large Language Models and Mutation Testing. *arXiv preprint arXiv:2308.16557*
    (2023).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dakhel 等（2023）Arghavan Moradi Dakhel, Amin Nikanjam, Vahid Majdinasab, Foutse
    Khomh, 和 Michel C Desmarais。2023年。利用预训练的大型语言模型和突变测试的有效测试生成。*arXiv 预印本 arXiv:2308.16557*（2023年）。
- en: 'Dinella et al. (2022) Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, and
    Shuvendu K Lahiri. 2022. Toga: A neural method for test oracle generation. In
    *Proceedings of the 44th International Conference on Software Engineering*. 2130–2141.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dinella 等（2022）Elizabeth Dinella, Gabriel Ryan, Todd Mytkowicz, 和 Shuvendu K
    Lahiri。2022年。Toga：一种用于测试神谕生成的神经方法。在 *第44届国际软件工程会议论文集*。2130–2141页。
- en: 'Fraser and Arcuri (2011) Gordon Fraser and Andrea Arcuri. 2011. Evosuite: automatic
    test suite generation for object-oriented software. In *Proceedings of the 19th
    ACM SIGSOFT symposium and the 13th European conference on Foundations of software
    engineering*. 416–419.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fraser 和 Arcuri（2011）Gordon Fraser 和 Andrea Arcuri。2011年。Evosuite：面向面向对象软件的自动测试套件生成。在
    *第19届 ACM SIGSOFT 研讨会和第13届欧洲软件工程基础会议论文集*。416–419页。
- en: 'Fraser and Arcuri (2013) Gordon Fraser and Andrea Arcuri. 2013. Evosuite: On
    the challenges of test case generation in the real world. In *2013 IEEE sixth
    international conference on software testing, verification and validation*. IEEE,
    362–369.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fraser 和 Arcuri（2013）Gordon Fraser 和 Andrea Arcuri。2013年。Evosuite：现实世界中测试用例生成的挑战。在
    *2013年 IEEE 第六届国际软件测试、验证与验证会议*。IEEE，362–369页。
- en: Fraser and Arcuri (2014) Gordon Fraser and Andrea Arcuri. 2014. A large-scale
    evaluation of automated unit test generation using evosuite. *ACM Transactions
    on Software Engineering and Methodology (TOSEM)* 24, 2 (2014), 1–42.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fraser 和 Arcuri（2014）Gordon Fraser 和 Andrea Arcuri。2014年。使用Evosuite的大规模自动单元测试生成评估。*ACM
    软件工程与方法学杂志（TOSEM）* 24, 2（2014年），1–42页。
- en: 'Fraser and Arcuri (2015) Gordon Fraser and Andrea Arcuri. 2015. 1600 faults
    in 100 projects: automatically finding faults while achieving high coverage with
    evosuite. *Empirical software engineering* 20 (2015), 611–639.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fraser 和 Arcuri（2015）Gordon Fraser 和 Andrea Arcuri。2015年。100个项目中的1600个缺陷：在实现高覆盖率的同时自动发现缺陷。*经验软件工程*
    20（2015年），611–639页。
- en: Godefroid (2007) Patrice Godefroid. 2007. Compositional dynamic test generation.
    In *Proceedings of the 34th annual ACM SIGPLAN-SIGACT symposium on Principles
    of programming languages*. 47–54.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Godefroid（2007）Patrice Godefroid。2007年。组合动态测试生成。在 *第34届 ACM SIGPLAN-SIGACT 程序设计语言原理年会论文集*。47–54页。
- en: 'Godefroid et al. (2005) Patrice Godefroid, Nils Klarlund, and Koushik Sen.
    2005. DART: Directed automated random testing. In *Proceedings of the 2005 ACM
    SIGPLAN conference on Programming language design and implementation*. 213–223.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Godefroid 等（2005）Patrice Godefroid, Nils Klarlund, 和 Koushik Sen。2005年。DART：定向自动随机测试。在
    *2005年 ACM SIGPLAN 编程语言设计与实现会议论文集*。213–223页。
- en: Hashtroudi et al. (2023) Sepehr Hashtroudi, Jiho Shin, Hadi Hemmati, and Song
    Wang. 2023. Automated Test Case Generation Using Code Models and Domain Adaptation.
    *arXiv preprint arXiv:2308.08033* (2023).
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hashtroudi 等（2023）Sepehr Hashtroudi, Jiho Shin, Hadi Hemmati, 和 Song Wang。2023年。使用代码模型和领域适应的自动测试用例生成。*arXiv
    预印本 arXiv:2308.08033*（2023年）。
- en: King (1976) James C King. 1976. Symbolic execution and program testing. *Commun.
    ACM* 19, 7 (1976), 385–394.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: King（1976）James C King。1976年。符号执行与程序测试。*Commun. ACM* 19, 7（1976年），385–394页。
- en: 'Kocetkov et al. (2022) Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
    Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean
    Hughes, Thomas Wolf, et al. 2022. The stack: 3 tb of permissively licensed source
    code. *arXiv preprint arXiv:2211.15533* (2022).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kocetkov 等（2022）Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao
    Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas
    Wolf 等。2022年。The stack：3TB 的宽松许可源代码。*arXiv 预印本 arXiv:2211.15533*（2022年）。
- en: 'Lemieux et al. (2023) Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri,
    and Siddhartha Sen. 2023. CODAMOSA: Escaping coverage plateaus in test generation
    with pre-trained large language models. In *International conference on software
    engineering (ICSE)*.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lemieux 等（2023）Caroline Lemieux, Jeevana Priya Inala, Shuvendu K Lahiri, 和 Siddhartha
    Sen。2023年。CODAMOSA：利用预训练的大型语言模型突破测试生成中的覆盖率平台。在 *国际软件工程会议（ICSE）*。
- en: Li et al. (2023) Tsz-On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, and
    Shing-Chi Cheung. 2023. Finding Failure-Inducing Test Cases with ChatGPT. *arXiv
    preprint arXiv:2304.11686* (2023).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 (2023) Tsz-On Li、Wenxi Zong、Yibo Wang、Haoye Tian、Ying Wang 和 Shing-Chi
    Cheung. 2023. 使用 ChatGPT 查找失败诱发测试用例. *arXiv 预印本 arXiv:2304.11686* (2023).
- en: 'Lukasczyk and Fraser (2022) Stephan Lukasczyk and Gordon Fraser. 2022. Pynguin:
    Automated unit test generation for python. In *Proceedings of the ACM/IEEE 44th
    International Conference on Software Engineering: Companion Proceedings*. 168–172.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lukasczyk 和 Fraser (2022) Stephan Lukasczyk 和 Gordon Fraser. 2022. Pynguin:
    针对 Python 的自动化单元测试生成. 载于 *第44届ACM/IEEE国际软件工程会议: 附录会议论文集*. 168–172.'
- en: 'Lukasczyk et al. (2020) Stephan Lukasczyk, Florian Kroiß, and Gordon Fraser.
    2020. Automated unit test generation for python. In *Search-Based Software Engineering:
    12th International Symposium, SSBSE 2020, Bari, Italy, October 7–8, 2020, Proceedings
    12*. Springer, 9–24.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lukasczyk 等 (2020) Stephan Lukasczyk、Florian Kroiß 和 Gordon Fraser. 2020. 针对
    Python 的自动化单元测试生成. 载于 *基于搜索的软件工程: 第12届国际研讨会, SSBSE 2020, 意大利巴里, 2020年10月7-8日,
    会议论文集 12*. Springer, 9–24.'
- en: McCabe (1976) Thomas J. McCabe. 1976. A Complexity Measure. *IEEE Transactions
    on Software Engineering* SE-2 (1976), 308–320. [https://api.semanticscholar.org/CorpusID:9116234](https://api.semanticscholar.org/CorpusID:9116234)
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McCabe (1976) Thomas J. McCabe. 1976. 一种复杂度度量. *IEEE软件工程学报* SE-2 (1976), 308–320.
    [https://api.semanticscholar.org/CorpusID:9116234](https://api.semanticscholar.org/CorpusID:9116234)
- en: 'Nijkamp et al. (2023) Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio
    Savarese, and Yingbo Zhou. 2023. Codegen2: Lessons for training llms on programming
    and natural languages. *arXiv preprint arXiv:2305.02309* (2023).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nijkamp 等 (2023) Erik Nijkamp、Hiroaki Hayashi、Caiming Xiong、Silvio Savarese
    和 Yingbo Zhou. 2023. Codegen2: 训练大型语言模型进行编程和自然语言处理的经验教训. *arXiv 预印本 arXiv:2305.02309*
    (2023).'
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774 [cs.CL]
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. GPT-4技术报告. arXiv:2303.08774 [cs.CL]
- en: 'Pacheco and Ernst (2007) Carlos Pacheco and Michael D Ernst. 2007. Randoop:
    feedback-directed random testing for Java. In *Companion to the 22nd ACM SIGPLAN
    conference on Object-oriented programming systems and applications companion*.
    815–816.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pacheco 和 Ernst (2007) Carlos Pacheco 和 Michael D Ernst. 2007. Randoop: 针对
    Java 的反馈驱动随机测试. 载于 *第22届ACM SIGPLAN面向对象编程系统与应用会议附录*. 815–816.'
- en: Pacheco et al. (2007) Carlos Pacheco, Shuvendu K Lahiri, Michael D Ernst, and
    Thomas Ball. 2007. Feedback-directed random test generation. In *29th International
    Conference on Software Engineering (ICSE’07)*. IEEE, 75–84.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pacheco 等 (2007) Carlos Pacheco、Shuvendu K Lahiri、Michael D Ernst 和 Thomas Ball.
    2007. 反馈驱动的随机测试生成. 载于 *第29届国际软件工程会议 (ICSE’07)*. IEEE, 75–84.
- en: Planning (2002) Strategic Planning. 2002. The economic impacts of inadequate
    infrastructure for software testing. *National Institute of Standards and Technology*
    1 (2002).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Planning (2002) Strategic Planning. 2002. 不足的基础设施对软件测试的经济影响. *国家标准与技术研究院* 1
    (2002).
- en: Schäfer et al. (2023a) Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip.
    2023a. Adaptive test generation using a large language model. *arXiv preprint
    arXiv:2302.06527* (2023).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schäfer 等 (2023a) Max Schäfer、Sarah Nadi、Aryaz Eghbali 和 Frank Tip. 2023a. 使用大型语言模型的自适应测试生成.
    *arXiv 预印本 arXiv:2302.06527* (2023).
- en: Schäfer et al. (2023b) Max Schäfer, Sarah Nadi, Aryaz Eghbali, and Frank Tip.
    2023b. An empirical evaluation of using large language models for automated unit
    test generation. *IEEE Transactions on Software Engineering* (2023).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schäfer 等 (2023b) Max Schäfer、Sarah Nadi、Aryaz Eghbali 和 Frank Tip. 2023b. 使用大型语言模型进行自动化单元测试生成的实证评估.
    *IEEE软件工程学报* (2023).
- en: 'Sen et al. (2005) Koushik Sen, Darko Marinov, and Gul Agha. 2005. CUTE: A concolic
    unit testing engine for C. *ACM SIGSOFT Software Engineering Notes* 30, 5 (2005),
    263–272.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sen 等 (2005) Koushik Sen、Darko Marinov 和 Gul Agha. 2005. CUTE: 用于 C 的协同符号单元测试引擎.
    *ACM SIGSOFT 软件工程笔记* 30, 5 (2005), 263–272.'
- en: Tillmann and De Halleux (2008) Nikolai Tillmann and Jonathan De Halleux. 2008.
    Pex–white box test generation for. net. In *International conference on tests
    and proofs*. Springer, 134–153.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tillmann 和 De Halleux (2008) Nikolai Tillmann 和 Jonathan De Halleux. 2008. Pex–.NET
    的白盒测试生成. 载于 *国际测试与证明会议*. Springer, 134–153.
- en: Tufano et al. (2020) Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun
    Deng, and Neel Sundaresan. 2020. Unit test case generation with transformers and
    focal context. *arXiv preprint arXiv:2009.05617* (2020).
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tufano 等 (2020) Michele Tufano、Dawn Drain、Alexey Svyatkovskiy、Shao Kun Deng
    和 Neel Sundaresan. 2020. 使用变换器和焦点上下文生成单元测试用例. *arXiv 预印本 arXiv:2009.05617* (2020).
- en: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    et al. 2022a. Emergent abilities of large language models. *arXiv preprint arXiv:2206.07682*
    (2022).
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022a) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret
    Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    等人. 2022a. 大型语言模型的突现能力. *arXiv 预印本 arXiv:2206.07682* (2022).
- en: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems* 35 (2022), 24824–24837.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022b) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等人. 2022b. Chain-of-thought prompting 在大型语言模型中引发推理.
    *神经信息处理系统进展* 35 (2022), 24824–24837.
- en: 'Widyasari et al. (2020) Ratnadira Widyasari, Sheng Qin Sim, Camellia Lok, Haodi
    Qi, Jack Phan, Qijin Tay, Constance Tan, Fiona Wee, Jodie Ethelda Tan, Yuheng
    Yieh, Brian Goh, Ferdian Thung, Hong Jin Kang, Thong Hoang, David Lo, and Eng Lieh
    Ouh. 2020. BugsInPy: A Database of Existing Bugs in Python Programs to Enable
    Controlled Testing and Debugging Studies. In *Proceedings of the 28th ACM Joint
    Meeting on European Software Engineering Conference and Symposium on the Foundations
    of Software Engineering* (Virtual Event, USA) *(ESEC/FSE 2020)*. Association for
    Computing Machinery, New York, NY, USA, 1556–1560. [https://doi.org/10.1145/3368089.3417943](https://doi.org/10.1145/3368089.3417943)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Widyasari et al. (2020) Ratnadira Widyasari, Sheng Qin Sim, Camellia Lok, Haodi
    Qi, Jack Phan, Qijin Tay, Constance Tan, Fiona Wee, Jodie Ethelda Tan, Yuheng
    Yieh, Brian Goh, Ferdian Thung, Hong Jin Kang, Thong Hoang, David Lo, 和 Eng Lieh
    Ouh. 2020. BugsInPy: 一个现有 Python 程序中的 bug 数据库，用于支持受控测试和调试研究. 在 *第28届 ACM 欧洲软件工程会议与软件工程基础研讨会联合会议论文集*
    (虚拟会议，美国) *(ESEC/FSE 2020)*. 计算机协会, 纽约, NY, 美国, 1556–1560. [https://doi.org/10.1145/3368089.3417943](https://doi.org/10.1145/3368089.3417943)'
- en: 'Xie et al. (2023) Zhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, and
    Jianwei Yin. 2023. ChatUniTest: a ChatGPT-based automated unit test generation
    tool. *arXiv preprint arXiv:2305.04764* (2023).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie et al. (2023) Zhuokui Xie, Yinghao Chen, Chen Zhi, Shuiguang Deng, 和 Jianwei
    Yin. 2023. ChatUniTest：一个基于 ChatGPT 的自动化单元测试生成工具. *arXiv 预印本 arXiv:2305.04764*
    (2023).
- en: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of thoughts: Deliberate
    problem solving with large language models. *arXiv preprint arXiv:2305.10601*
    (2023).'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, 和 Karthik Narasimhan. 2023. 思维树：利用大型语言模型进行深思熟虑的问题解决. *arXiv
    预印本 arXiv:2305.10601* (2023).
- en: 'Yoo and Harman (2012) Shin Yoo and Mark Harman. 2012. Regression testing minimization,
    selection and prioritization: a survey. *Software testing, verification and reliability*
    22, 2 (2012), 67–120.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yoo and Harman (2012) Shin Yoo 和 Mark Harman. 2012. 回归测试的最小化、选择和优先级排序：一项调查.
    *软件测试、验证与可靠性* 22, 2 (2012), 67–120.
- en: Yuan et al. (2023) Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin
    Wang, Yixuan Chen, and Xin Peng. 2023. No More Manual Tests? Evaluating and Improving
    ChatGPT for Unit Test Generation. *arXiv preprint arXiv:2305.04207* (2023).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2023) Zhiqiang Yuan, Yiling Lou, Mingwei Liu, Shiji Ding, Kaixin
    Wang, Yixuan Chen, 和 Xin Peng. 2023. 不再需要手动测试？评估和改进 ChatGPT 以生成单元测试. *arXiv 预印本
    arXiv:2305.04207* (2023).
