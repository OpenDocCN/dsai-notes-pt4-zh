- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:40:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:40:47'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation
    in LLMs via Zero-Shot Cross-Lingual Transfer'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弥合语言差距：通过零样本跨语言迁移增强LLMs中的多语言基于提示的代码生成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.09701](https://ar5iv.labs.arxiv.org/html/2408.09701)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.09701](https://ar5iv.labs.arxiv.org/html/2408.09701)
- en: Mingda Li, Abhijit Mishra, Utkarsh Mujumdar
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李名达, 阿比吉特·米什拉, 乌特卡什·穆久姆达
- en: School of Information, University of Texas at Austin
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学奥斯汀分校信息学院
- en: '{mingdali, abhijitmishra, utkarsh.mujumdar@utexas.edu}@utexas.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{mingdali, abhijitmishra, utkarsh.mujumdar@utexas.edu}@utexas.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The use of Large Language Models (LLMs) for program code generation has gained
    substantial attention, but their biases and limitations with non-English prompts
    challenge global inclusivity. This paper investigates the complexities of multilingual
    prompt-based code generation. Our evaluations of LLMs, including CodeLLaMa and
    CodeGemma, reveal significant disparities in code quality for non-English prompts;
    we also demonstrate the inadequacy of simple approaches like prompt translation,
    bootstrapped data augmentation, and fine-tuning. To address this, we propose a
    zero-shot cross-lingual approach using a neural projection technique, integrating
    a cross-lingual encoder like LASER Artetxe and Schwenk ([2019](#bib.bib3)) to
    map multilingual embeddings from it into the LLM’s token space. This method requires
    training only on English data and scales effectively to other languages. Results
    on a translated and quality-checked *MBPP* dataset show substantial improvements
    in code quality. This research promotes a more inclusive code generation landscape
    by empowering LLMs with multilingual capabilities to support the diverse linguistic
    spectrum in programming.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 使用大型语言模型（LLMs）进行程序代码生成已引起广泛关注，但它们在非英语提示下的偏差和局限性挑战了全球包容性。本文探讨了多语言基于提示的代码生成的复杂性。我们对包括CodeLLaMa和CodeGemma在内的LLMs进行的评估揭示了非英语提示下代码质量的显著差异；我们还展示了诸如提示翻译、引导式数据增强和微调等简单方法的不足。为了解决这个问题，我们提出了一种使用神经投影技术的零样本跨语言方法，集成了类似LASER
    Artetxe和Schwenk（[2019](#bib.bib3)）的跨语言编码器，将多语言嵌入映射到LLM的标记空间。这种方法仅需对英语数据进行训练，并能有效扩展到其他语言。对一个翻译和质量检查过的*MBPP*数据集的结果显示，代码质量有了显著改善。这项研究通过赋予LLMs多语言能力来支持编程中的多样语言谱系，从而促进了一个更具包容性的代码生成环境。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'The use of Large Language Models (LLMs) for code generation, such as generating
    Python programs from problem specifications, has gained substantial interest due
    to their effectiveness in handling complex language tasks (Zhao et al., [2023](#bib.bib38);
    Gao et al., [2023](#bib.bib11); Austin et al., [2021](#bib.bib4)). This capability
    has led to the development of innovative applications like GitHub Copilot Yetistiren
    et al. ([2022](#bib.bib35)) and specialized LLMs such as CodeLLaMa Roziere et al.
    ([2023](#bib.bib25)), underscoring the growing importance of this field. Although
    LLMs are globally prevalent and proficient in processing multilingual inputs,
    they often exhibit biases against non-English prompts Talat et al. ([2022](#bib.bib30));
    Choudhury and Deshpande ([2021](#bib.bib9)), which is particularly evident in
    code generation. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Bridging the Language
    Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot
    Cross-Lingual Transfer") under Baseline Output illustrates how the quality of
    generated code diminishes as prompts shift from English to other languages, a
    disparity linked to the availability of data used in LLM training and fine-tuning.
    Ensuring LLMs deliver consistent quality across languages is crucial for fostering
    fair and inclusive code generation, especially as the global programming community
    is increasingly composed of non-English speakers. Data from coding platforms like
    The Competitive Programming Hall of Fame¹¹1https://cphof.org/countries highlight
    the skew towards non-English-speaking regions. As the global population of coders
    grows, addressing these biases in LLMs is essential to promoting an equitable
    and accessible environment for developers worldwide.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '大型语言模型（LLMs）在代码生成中的应用，比如从问题规格生成 Python 程序，因其在处理复杂语言任务中的有效性而获得了大量关注（Zhao et
    al., [2023](#bib.bib38); Gao et al., [2023](#bib.bib11); Austin et al., [2021](#bib.bib4)）。这一能力促成了诸如
    GitHub Copilot Yetistiren et al. ([2022](#bib.bib35)) 和 CodeLLaMa Roziere et al.
    ([2023](#bib.bib25)) 这样的创新应用的发展，突显了这一领域的日益重要性。尽管 LLMs 在全球范围内广泛存在且擅长处理多语言输入，但它们对非英语提示常常表现出偏见
    Talat et al. ([2022](#bib.bib30)); Choudhury and Deshpande ([2021](#bib.bib9))，这种偏见在代码生成中尤为明显。图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer") 在基线输出下展示了当提示从英语转变为其他语言时生成的代码质量如何下降，这种差异与
    LLM 训练和微调中使用的数据可用性有关。确保 LLMs 在不同语言间提供一致的质量对促进公平和包容的代码生成至关重要，尤其是随着全球编程社区越来越多地由非英语使用者组成。来自编码平台的数据如
    The Competitive Programming Hall of Fame¹¹1https://cphof.org/countries 突显了对非英语国家的偏向。随着全球程序员数量的增加，解决
    LLMs 中的这些偏见对于促进全球开发者的公平和可及的环境至关重要。'
- en: '![Refer to caption](img/646d59a03c363f7214159c0dfad5336f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/646d59a03c363f7214159c0dfad5336f.png)'
- en: 'Figure 1: Disparity in output code generated by *CodeLLaMa-Instruct* modelRoziere
    et al. ([2023](#bib.bib25)) with 7B parameters for the same problem statement
    given in multiple languages'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: *CodeLLaMa-Instruct* 模型Roziere et al. ([2023](#bib.bib25)) 以 7B 参数为相同问题陈述生成的多语言输出代码的差异'
- en: This paper tries to bridge the language gap in multilingual prompt-based code
    generation by enhancing LLMs through self-supervised fine-tuning. We first evaluate
    the performance of LLMs like CodeLLaMa and GPT-4 on English and five non-English
    languages, using a translated and quality-checked version of the MBPP dataset
    Austin et al. ([2021](#bib.bib4)). Significant disparities in code quality across
    languages were observed, even when using the same problem statements. Inspired
    by Shi et al. ([2022a](#bib.bib28)) and Qin et al. ([2023](#bib.bib24)), who improved
    LLM performance on multilingual tasks with Chain-of-Thought (CoT), and Awasthi
    et al. ([2022](#bib.bib5)), who used bootstrapping for multilingual data generation,
    we form strong baselines for code generation with CoT and fine-tuning on bootstrapped
    multilingual data. However, these approaches showed only marginal and inconsistent
    improvements.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文试图通过自监督微调来弥合多语言提示基础代码生成中的语言差距。我们首先评估了像 CodeLLaMa 和 GPT-4 这样的 LLMs 在英语和五种非英语语言上的表现，使用了经翻译和质量检查的
    MBPP 数据集 Austin et al. ([2021](#bib.bib4)) 的版本。即使在使用相同问题陈述的情况下，也观察到了语言之间代码质量的显著差异。受到
    Shi et al. ([2022a](#bib.bib28)) 和 Qin et al. ([2023](#bib.bib24)) 的启发，他们通过 Chain-of-Thought（CoT）提高了
    LLM 在多语言任务上的表现，以及 Awasthi et al. ([2022](#bib.bib5)) 使用引导法生成多语言数据，我们通过 CoT 和对引导多语言数据的微调形成了强有力的代码生成基线。然而，这些方法仅显示了边际和不一致的改进。
- en: 'To address data sparsity and limited multilingual exposure, we propose a novel
    approach: (a) using a pre-trained multilingual encoder like LASER Artetxe and
    Schwenk ([2019](#bib.bib3)) to encode multilingual inputs into a joint vector
    space; (b) projecting these embeddings into the LLM’s input space and aligning
    them through training solely on English data; and (c) employing this LASER$\rightarrow$LLM
    pipeline at inference for zero-shot cross-lingual processing of non-English inputs.
    This method familiarizes models with multiple languages without needing additional
    external training data. Our evaluation shows improved code quality and reduced
    syntax and logical errors, as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation
    in LLMs via Zero-Shot Cross-Lingual Transfer") under Zero-shot Cross-Lingual Inference.
    Our approach integrates seamlessly as a minor training step that does not require
    any expensive pretraining or fine tuning, thus offering a promising way to enhance
    LLMs’ multilingual capabilities in code generation.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决数据稀疏和有限的多语言曝光问题，我们提出了一种新颖的方法：（a）使用像 LASER Artetxe 和 Schwenk（[2019](#bib.bib3)）这样的预训练多语言编码器，将多语言输入编码到一个联合向量空间中；（b）将这些嵌入投影到
    LLM 的输入空间中，并通过仅使用英文数据的训练来对齐它们；（c）在推理过程中采用 LASER$\rightarrow$LLM 流程，以实现对非英文输入的零样本跨语言处理。该方法使模型熟悉多种语言，而无需额外的外部训练数据。我们的评估显示代码质量有所提升，语法和逻辑错误减少，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer") 所示，属于零样本跨语言推理。我们的方法无缝集成作为一个小的训练步骤，不需要任何昂贵的预训练或微调，因此提供了一种有前景的方式来增强
    LLM 在代码生成中的多语言能力。'
- en: 'The contributions of the paper are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献如下：
- en: '1.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We create a novel multilingual test dataset with quality-checked translations
    and new evaluation metrics.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们创建了一个新颖的多语言测试数据集，具有质量检查过的翻译和新的评估指标。
- en: '2.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We introduce a scalable projection technique by integrating the LASER multilingual
    encoder with popular open-source LLMs like CodeLLaMa Roziere et al. ([2023](#bib.bib25)),
    CodeGemma Team ([2024](#bib.bib32)), and Mistral Jiang et al. ([2023](#bib.bib16))
    for zero-shot cross-lingual code generation.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们通过将 LASER 多语言编码器与流行的开源 LLM，如 CodeLLaMa Roziere et al.（[2023](#bib.bib25)）、CodeGemma
    Team（[2024](#bib.bib32)）和 Mistral Jiang et al.（[2023](#bib.bib16)）集成，介绍了一种可扩展的投影技术，用于零样本跨语言代码生成。
- en: '3.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Our evaluation of the approach against Chain-of-Thought (CoT) and fine-tuning
    with multilingual bootstrapped data, highlights the strengths and limitations
    of each method.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对这种方法与链式思维（CoT）和使用多语言自举数据进行微调的评估，突出了每种方法的优点和局限性。
- en: We will make our code²²2https://github.com/lmd0420/Multilingual_Code_Gen and
    multilingual evaluation data³³3https://huggingface.co/datasets/Mingda/MBPP-Translated
    publicly available for academic use.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的代码²²2https://github.com/lmd0420/Multilingual_Code_Gen 和多语言评估数据³³3https://huggingface.co/datasets/Mingda/MBPP-Translated
    公开提供用于学术用途。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: As AI technology advances, transformer-based LLMs like GPT (OpenAI, [2023](#bib.bib21)),
    LLaMA (Touvron et al., [2023](#bib.bib34)), Mistral Jiang et al. ([2023](#bib.bib16)),
    and Gemma Team et al. ([2024](#bib.bib33)) have become prominent in research and
    applications. While pre-trained models such as LLaMA2 and Gemma are fine-tuned
    for code generation, their English-centric training data limits multilingual proficiency
    (Lai et al., [2023](#bib.bib17); Akiki et al., [2022](#bib.bib2)). Studies show
    these models face performance issues with non-English tasks (Shi et al., [2022b](#bib.bib29);
    Becker et al., [2023](#bib.bib6)), and human supervision remains crucial for quality
    (Sarsa et al., [2022](#bib.bib26)).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 AI 技术的发展，基于变换器的 LLM，如 GPT（OpenAI，[2023](#bib.bib21)）、LLaMA（Touvron et al.，[2023](#bib.bib34)）、Mistral
    Jiang et al.（[2023](#bib.bib16)）和 Gemma Team et al.（[2024](#bib.bib33)），在研究和应用中变得尤为突出。尽管像
    LLaMA2 和 Gemma 这样的预训练模型已针对代码生成进行了微调，但其以英语为中心的训练数据限制了其多语言能力（Lai et al.，[2023](#bib.bib17)；Akiki
    et al.，[2022](#bib.bib2)）。研究表明，这些模型在处理非英语任务时面临性能问题（Shi et al.，[2022b](#bib.bib29)；Becker
    et al.，[2023](#bib.bib6)），而人工监督对于质量仍然至关重要（Sarsa et al.，[2022](#bib.bib26)）。
- en: To address these gaps, Ahuja et al. ([2023](#bib.bib1)) developed a multilingual
    benchmark for evaluating LLMs, revealing performance drops across languages. Tan
    and Golovneva ([2020](#bib.bib31)) and Huang et al. ([2023](#bib.bib14)) suggest
    leveraging transfer learning and cross-lingual prompting to improve multilingual
    capabilities. Additional methods include language-specific pre-training (Pfeiffer
    et al., [2022](#bib.bib23)) and consistency regularization for fine-tuning (Zheng
    et al., [2021](#bib.bib39)).
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这些不足，Ahuja 等人 ([2023](#bib.bib1)) 开发了一个多语言基准测试来评估 LLMs，揭示了各语言间的性能下降。Tan
    和 Golovneva ([2020](#bib.bib31)) 以及 Huang 等人 ([2023](#bib.bib14)) 建议利用迁移学习和跨语言提示来提升多语言能力。其他方法包括语言特定的预训练（Pfeiffer
    等人，[2022](#bib.bib23)）和用于微调的一致性正则化（Zheng 等人，[2021](#bib.bib39)）。
- en: Optimizing prompts enhances multilingual LLM accuracy (Zhao and Schütze, [2021](#bib.bib37);
    Huang et al., [2022](#bib.bib15)), and CoT techniques improve code generation
    (Ma et al., [2023](#bib.bib20)). Fine-tuning with multilingual synthetic data,
    including translation and back-translation (Sennrich et al., [2015](#bib.bib27);
    Hoang et al., [2018](#bib.bib12)), further refines LLMs, as demonstrated by Li
    et al. ([2023](#bib.bib18)) and Zhang et al. ([2024](#bib.bib36)). Contrary to
    these popular approaches, we take an orthogonal route by using specialized multimodal
    encoders and lightweight projectors to bridge language gaps in popular LLMs. Our
    work is inspired by multimodal AI literature, integrating projection techniques
    for different modalities such as language, vision and speech (Liu et al., [2024](#bib.bib19);
    Fathullah et al., [2024](#bib.bib10); Beyer et al., [2024](#bib.bib7)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 优化提示可以提高多语言 LLM 的准确性（Zhao 和 Schütze，[2021](#bib.bib37)；Huang 等人，[2022](#bib.bib15)），而
    CoT 技术改善了代码生成（Ma 等人，[2023](#bib.bib20)）。通过多语言合成数据进行微调，包括翻译和反向翻译（Sennrich 等人，[2015](#bib.bib27)；Hoang
    等人，[2018](#bib.bib12)），进一步优化了 LLMs，正如 Li 等人 ([2023](#bib.bib18)) 和 Zhang 等人 ([2024](#bib.bib36))
    所展示的。与这些流行的方法相反，我们采取了不同的途径，通过使用专门的多模态编码器和轻量级投影器来弥合流行 LLMs 中的语言差距。我们的工作受到了多模态 AI
    文献的启发，结合了语言、视觉和语音等不同模态的投影技术（Liu 等人，[2024](#bib.bib19)；Fathullah 等人，[2024](#bib.bib10)；Beyer
    等人，[2024](#bib.bib7)）。
- en: 3 Experimental Setup
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验设置
- en: This section details our experimental setup, including the creation of a multilingual
    benchmark dataset, the models evaluated, and the metrics used. Our focus is on
    Python code generation from multilingual prompts, though the methods and insights
    are applicable to other languages and contexts.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细介绍了我们的实验设置，包括多语言基准数据集的创建、评估的模型以及使用的指标。我们的重点是从多语言提示中生成 Python 代码，尽管这些方法和见解适用于其他语言和上下文。
- en: 3.1 Evaluation Dataset
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 评估数据集
- en: To the best of our knowledge, datasets with multilingual prompts for program
    generation code are elusive. To address this, we adapted the Mostly Basic Programming
    Problems (MBPP) dataset Austin et al. ([2021](#bib.bib4)), specifically the sanitized
    version with its "test" split, containing $257$ problems with solutions and three
    test cases each. We translated these prompts into five languages—Chinese-Simplified
    (zh-cn), Spanish (es), Japanese (ja), Russian (ru), and Hindi (hi)—using the Google
    Translate API⁴⁴4https://cloud.google.com/translate, chosen for their diverse linguistic
    representation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，包含多语言提示的程序生成代码数据集十分稀缺。为此，我们调整了 Mostly Basic Programming Problems (MBPP)
    数据集 Austin 等人 ([2021](#bib.bib4))，特别是其 "test" 切分的清洗版，其中包含 $257$ 道问题及每道问题的三个测试用例。我们将这些提示翻译成五种语言——简体中文
    (zh-cn)、西班牙语 (es)、日语 (ja)、俄语 (ru) 和印地语 (hi)——使用了 Google Translate API⁴⁴4https://cloud.google.com/translate，因为它们具有多样的语言代表性。
- en: 'Translation quality was assessed by (a) expert bilingual speakers via Amazon
    Mechanical Turk, who rated translations as acceptable or not (Note: guidelines
    were provided for binary rating and consent was obtained to report the statistics
    in the paper), with results showing superior quality (see Table [1](#S3.T1 "Table
    1 ‣ 3.1 Evaluation Dataset ‣ 3 Experimental Setup ‣ Bridging the Language Gap:
    Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual
    Transfer")), and (b) GPT-4, which rated translations on a scale of 1 to 5\. Table
    [2](#S3.T2 "Table 2 ‣ 3.1 Evaluation Dataset ‣ 3 Experimental Setup ‣ Bridging
    the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs
    via Zero-Shot Cross-Lingual Transfer") presents GPT-4’s ratings, again indicating
    that translations are of high-quality with high mean scores and low standard deviations.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译质量通过 (a) 亚马逊机械土耳其的双语专家进行评估，他们将翻译评为可接受或不可接受（注意：提供了二元评分的指南，并获得了在论文中报告统计数据的同意），结果显示质量优越（见表
    [1](#S3.T1 "表 1 ‣ 3.1 评估数据集 ‣ 3 实验设置 ‣ 弥合语言差距：通过零样本跨语言转移增强多语言提示基础的代码生成")），以及 (b)
    GPT-4，该系统对翻译进行 1 到 5 的评分。表 [2](#S3.T2 "表 2 ‣ 3.1 评估数据集 ‣ 3 实验设置 ‣ 弥合语言差距：通过零样本跨语言转移增强多语言提示基础的代码生成")
    显示了 GPT-4 的评分，再次表明翻译质量高，平均分高且标准差低。
- en: '| Translation | A1 | A2 | Agreement (%) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 翻译 | A1 | A2 | 一致性 (%) |'
- en: '| en_es | 0.94 | 0.96 | 89.69 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| en_es | 0.94 | 0.96 | 89.69 |'
- en: '| en-hi | 0.93 | 0.96 | 89.11 |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| en-hi | 0.93 | 0.96 | 89.11 |'
- en: '| en_ja | 0.93 | 0.96 | 89.88 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| en_ja | 0.93 | 0.96 | 89.88 |'
- en: '| en_ru | 0.93 | 0.96 | 90.43 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| en_ru | 0.93 | 0.96 | 90.43 |'
- en: '| en_zh | 0.94 | 0.96 | 90.79 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| en_zh | 0.94 | 0.96 | 90.79 |'
- en: 'Table 1: Human Evaluation of Translated Prompts. Two distinct bilingual speakers
    from MTurk rated translations with 1 (acceptable) or 0 (not acceptable) for each
    translation. A1 and A2 represent their average scores.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：翻译提示的人类评估。来自 MTurk 的两个不同的双语讲者对每个翻译进行了 1（可接受）或 0（不可接受）的评分。A1 和 A2 代表他们的平均分数。
- en: '| Lang. Pair | Average Rating | St.Dev |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 语言对 | 平均评分 | 标准差 |'
- en: '| en-hi | 4.88 | 0.40 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| en-hi | 4.88 | 0.40 |'
- en: '| en-es | 4.90 | 0.48 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| en-es | 4.90 | 0.48 |'
- en: '| en-ru | 4.95 | 0.30 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| en-ru | 4.95 | 0.30 |'
- en: '| en-zh_cn | 4.93 | 0.39 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| en-zh_cn | 4.93 | 0.39 |'
- en: '| en-ja | 4.87 | 0.55 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| en-ja | 4.87 | 0.55 |'
- en: 'Table 2: Average Rating and Standard Deviation for Translation from English
    to Other Languages'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：从英语到其他语言的平均评分和标准差
- en: '![Refer to caption](img/145c967b21bd0aded13954808efa6d8b.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/145c967b21bd0aded13954808efa6d8b.png)'
- en: (a) Baselines with direct prompting, Chain of Thought (CoT) and fine-tuning
    with bootstrapped data
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 直接提示、思维链（CoT）和使用引导数据的微调基准
- en: '![Refer to caption](img/dcefa74bcdb9f0cde6773e8756a52b28.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/dcefa74bcdb9f0cde6773e8756a52b28.png)'
- en: (b) Our proposed approach based on cross lingual encoder and projector training
    and zero shot inference
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 我们提出的基于跨语言编码器和投影器训练及零样本推理的方法
- en: 'Figure 2: Explored approaches'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：探讨的方法
- en: 3.2 Models Used for Evaluation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 评估中使用的模型
- en: We consider three open source variants of instruction tuned models for evaluation,
    namely CodeLLaMa7B⁵⁵5codellama/CodeLlama-7b-Instruct-hf, CodeGemma7B⁶⁶6google/codegemma-7b-it
    and Mistral-7B-v0.3⁷⁷7mistralai/Mistral-7B-Instruct-v0.3. These models are specialized
    versions of their base models to programming-related tasks. They have demonstrated
    greater efficacy at code generation, infilling, and debugging capabilities compared
    to the standard versions. We access the models are accessed from HuggingFace ([http://huggingface.co](http://huggingface.co))
    hub. For benchmarking, we use GPT-4 as the reference system (a.k.a Skyline) due
    to its proven effectiveness in various tasks, including code generation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们考虑了三种开源的指令调优模型进行评估，即 CodeLLaMa7B⁵⁵5codellama/CodeLlama-7b-Instruct-hf、CodeGemma7B⁶⁶6google/codegemma-7b-it
    和 Mistral-7B-v0.3⁷⁷7mistralai/Mistral-7B-Instruct-v0.3。这些模型是其基础模型在编程相关任务上的专业化版本。与标准版本相比，它们在代码生成、补全和调试能力上表现出了更高的效率。我们从
    HuggingFace ([http://huggingface.co](http://huggingface.co)) hub 获取这些模型。为了基准测试，我们使用
    GPT-4 作为参考系统（即 Skyline），因为它在包括代码生成在内的各种任务中证明了其有效性。
- en: 3.3 Inference Pipeline and Evaluation Metrics
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 推理管道和评估指标
- en: 'We developed a pipeline to process task descriptions in various languages.
    The pipeline feeds these prompts into the models and variants described in Sections
    [4](#S4 "4 Issues with Trivial Baselines ‣ Bridging the Language Gap: Enhancing
    Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual
    Transfer") and [5](#S5 "5 Our Approach: Projection-Based Zero-Shot Transfer ‣
    Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation
    in LLMs via Zero-Shot Cross-Lingual Transfer") and stores the results. Python
    code from the outputs is extracted using regular expressions. We then identify
    function names in the code, replacing the function names in the MBPP test assertions
    with those from the model outputs. Finally, we generate bash scripts from the
    extracted code and assertions and measure the following metrics:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '我们开发了一个管道来处理各种语言的任务描述。该管道将这些提示输入到[4](#S4 "4 Issues with Trivial Baselines ‣
    Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation
    in LLMs via Zero-Shot Cross-Lingual Transfer")和[5](#S5 "5 Our Approach: Projection-Based
    Zero-Shot Transfer ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based
    Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer")节中描述的模型和变体中，并存储结果。通过正则表达式提取输出中的Python代码。然后，我们识别代码中的函数名称，用模型输出中的函数名称替换MBPP测试断言中的函数名称。最后，我们从提取的代码和断言中生成bash脚本，并测量以下指标：'
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Logical Error Rate (LER): The ratio of code samples that execute successfully
    but produce incorrect results, to the total number of samples. Lower is better.'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 逻辑错误率（LER）：成功执行但产生不正确结果的代码样本比例，与样本总数之比。越低越好。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Syntax Error Rate (SER): The ratio of code samples containing syntax errors,
    to the total number of samples. Lower is better.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语法错误率（SER）：包含语法错误的代码样本比例，与样本总数之比。越低越好。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Total Error Rate (TotalER): The ratio of code samples that fail at least one
    test case, to the total number of samples. Lower is better.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总体错误率（TotalER）：至少一个测试用例失败的代码样本比例，与样本总数之比。越低越好。
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'All Tests Passed Rate (ATPR): The ratio of code samples that pass all given
    test cases, to the total number of samples. Higher is better.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全部测试通过率（ATPR）：通过所有给定测试用例的代码样本比例，与样本总数之比。越高越好。
- en: Additionally, we also observe the Code Completion Rate as a supplementary metric,
    which indicates the proportion of complete codes in model responses. A higher
    value represents a better result.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还观察了代码完成率作为补充指标，表示模型响应中完整代码的比例。更高的值表示更好的结果。
- en: 'With this setup, we can now evaluate LLM code generation quality and propose
    mitigation strategies. Our approach and baselines are summarized in Figure [2](#S3.F2
    "Figure 2 ‣ 3.1 Evaluation Dataset ‣ 3 Experimental Setup ‣ Bridging the Language
    Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot
    Cross-Lingual Transfer"), detailed in the following section.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '有了这个设置，我们现在可以评估LLM代码生成质量并提出缓解策略。我们的方案和基准总结在图[2](#S3.F2 "Figure 2 ‣ 3.1 Evaluation
    Dataset ‣ 3 Experimental Setup ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer")中，详细内容见下一节。'
- en: 4 Issues with Trivial Baselines
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 关于简单基准的挑战
- en: 'Given that language models exhibit emergent capabilities and scale effectively
    across tasks and languages, efficient prompting and prompt tuning are generally
    preferred over costly training or fine-tuning that demands extensive data curation.
    Based on our experimental setup, we highlight the challenges LLMs face with multilingual
    code generation in conventional settings, providing a detailed analysis of existing
    models’ performance and their limitations. Throughout this section, we will reference
    Table [3](#S6.T3 "Table 3 ‣ 6 Results and Discussions ‣ Bridging the Language
    Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot
    Cross-Lingual Transfer") for a comprehensive discussion of the results.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '鉴于语言模型展示出突出的能力，并且在任务和语言间有效扩展，高效的提示和提示调整通常优于需要大量数据整理的昂贵训练或微调。基于我们的实验设置，我们突出展示了LLMs在传统设置下面临的多语言代码生成挑战，提供了现有模型性能及其局限性的详细分析。在本节中，我们将参考表[3](#S6.T3
    "Table 3 ‣ 6 Results and Discussions ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer")以全面讨论结果。'
- en: 4.1 Baseline 1\. Original Prompt
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 基准 1\. 原始提示
- en: 'Here, each query in the dataset is passed through the pipeline, where the model
    generates response code, filtered from extraneous information such as code explanations,
    and executed using an automatically constructed bash script. The results are presented
    in first column of each section of Table [3](#S6.T3 "Table 3 ‣ 6 Results and Discussions
    ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation
    in LLMs via Zero-Shot Cross-Lingual Transfer"), with the following key observations:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '在这里，数据集中的每个查询都经过管道处理，模型生成响应代码，从多余的信息如代码解释中过滤出来，并通过自动构建的bash脚本执行。结果呈现在表[3](#S6.T3
    "Table 3 ‣ 6 Results and Discussions ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer")每节的第一列中，主要观察如下：'
- en: GPT-4, recognized for its robustness and extensive engineering, reliably generates
    code across all language prompts, though with slightly varying error profiles—except
    for Hindi and Chinese. In contrast, open-source models like CodeLLaMa show more
    pronounced disparities between languages, with higher error rates and lower all-tests-passed
    rates compared to English. Notably, some models, such as CodeLLaMa-Instruct-7B,
    perform better in non-English languages like Spanish. This may seem unusual but
    aligns with findings from Chen et al. ([2024](#bib.bib8)), which show that LLaMa
    7B, when instruction-tuned for multilingual tasks, performs better in Spanish
    than English. Since CodeLLaMa is based on this instruction-tuned model, this could
    explain the atypical performance in Spanish. Overall, these results highlight
    a lack of consistency in code output quality as the language changes. We use the
    abbreviation Orig. to refer to this baseline henceforth.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-4以其强大的性能和广泛的工程能力而闻名，能够可靠地生成所有语言提示的代码，尽管错误特征略有不同——除印地语和中文外。相比之下，开源模型如CodeLLaMa在语言间的差异更为明显，与英语相比，错误率更高，通过所有测试的比率更低。值得注意的是，一些模型，如CodeLLaMa-Instruct-7B，在西班牙语等非英语语言中表现更佳。这可能看起来不寻常，但与Chen等人（[2024](#bib.bib8)）的研究结果一致，该研究显示，当LLaMa
    7B针对多语言任务进行指令调整时，在西班牙语中的表现优于英语。由于CodeLLaMa基于这一指令调整模型，这可能解释了其在西班牙语中的非典型表现。总体而言，这些结果突显了随着语言变化，代码输出质量的一致性不足。我们将缩写Orig.用来指代这一基线。
- en: 4.2 Chain-of-Thought with Back-translation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 回译的链式思维
- en: 'Due to uneven language representation in LLM training datasets, achieving consistent
    results with direct prompting is challenging. A potential solution is to use back
    translation: translate non-English prompts into English and use the English version
    as the query. This Chain-of-Thought (CoT) approach involves translating the problem
    statement with the prompt: Translate the sentence $PROBLEM from $TARGET-LANG to
    English, then generating code outputs from the translated prompt. Our experiments,
    detailed in the second column of Table [3](#S6.T3 "Table 3 ‣ 6 Results and Discussions
    ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation
    in LLMs via Zero-Shot Cross-Lingual Transfer"), show that back translation did
    not significantly improve results. In some cases, it even reduced performance,
    as indicated by lower ATPR scores. Qualitative analysis suggests that models struggle
    with non-canonical language representations and topic drift, despite the translations
    not being of poor quality. We use the abbreviation CoT to refer to this baseline
    henceforth.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '由于在LLM训练数据集中语言表示的不均匀，直接提示获得一致结果是具有挑战性的。一种可能的解决方案是使用回译：将非英语提示翻译成英语，并使用英语版本作为查询。这种链式思维（CoT）方法包括使用提示翻译问题陈述：将句子$PROBLEM
    从$TARGET-LANG翻译成英语，然后从翻译后的提示生成代码输出。我们的实验，详见表[3](#S6.T3 "Table 3 ‣ 6 Results and
    Discussions ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code
    Generation in LLMs via Zero-Shot Cross-Lingual Transfer")的第二列，表明回译并未显著改善结果。在某些情况下，它甚至降低了性能，ATP评分较低。定性分析表明，尽管翻译质量并不差，但模型在处理非标准语言表示和主题漂移时仍然存在困难。我们将缩写CoT用来指代这一基线。'
- en: 4.3 Bootstrapping Multilingual Data and Fine Tuning
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 引导多语言数据和微调
- en: 'Fine-tuning pre-trained models is effective for many NLP tasks but is often
    resource-intensive, requiring costly and time-consuming task-specific labeled
    data. Instead of manually creating such data for multiple languages—designing
    prompts, validating answers, and translating while preserving semantic meaning—we
    use a bootstrapping approach. In this method, we utilize a powerful LLM like ChatGPT
    to generate English programming problems and their answers. These problems are
    then translated into target languages and back-translated into English. We assess
    the similarity of translations using the BLEU score Papineni et al. ([2002](#bib.bib22)),
    retaining translations that meet a quality threshold (e.g., 0.8) to create new
    training data. This method preserves text quality in target languages and allows
    the model to validate its translations, as detailed in Algorithm [1](#alg1 "Algorithm
    1 ‣ Appendix A Appendix ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based
    Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer") under Appendix
    [A](#A1 "Appendix A Appendix ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '微调预训练模型对许多 NLP 任务有效，但通常需要大量资源，需耗费高成本且耗时的任务特定标注数据。我们没有为多种语言手动创建这些数据——设计提示、验证答案和翻译，同时保持语义意义——而是使用了引导方法。在这种方法中，我们利用像
    ChatGPT 这样的强大 LLM 生成英文编程问题及其答案。这些问题随后被翻译成目标语言，再翻译回英文。我们使用 BLEU 分数 Papineni 等人（[2002](#bib.bib22)）评估翻译的相似性，保留满足质量阈值（例如
    0.8）的翻译，以创建新的训练数据。这种方法保持了目标语言中文本的质量，并允许模型验证其翻译，具体细节见附录 [A](#A1 "Appendix A Appendix
    ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation
    in LLMs via Zero-Shot Cross-Lingual Transfer") 下的算法 [1](#alg1 "Algorithm 1 ‣ Appendix
    A Appendix ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code
    Generation in LLMs via Zero-Shot Cross-Lingual Transfer")。'
- en: 'After bootstrapping data for all target languages, we shuffle and use it to
    fine-tune the LLMs with a single A100 GPU. Models are quantized to FP16 and fine-tuned
    using parameter-efficient techniques, including low-rank adaptation Hu et al.
    ([2021](#bib.bib13)). We set the temperature to 0.8 for consistency and use two
    epochs. As shown in the third column of Table [3](#S6.T3 "Table 3 ‣ 6 Results
    and Discussions ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based
    Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer"), while bootstrapping
    with ChatGPT reduces syntax errors. It also increases hallucinations, leading
    to lower test pass rates and higher total errors. This suggests that the model,
    although producing more complete code, struggles with accuracy and reliability.
    We use the abbreviation BFT to refer to this baseline henceforth.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '在为所有目标语言引导数据后，我们对数据进行洗牌，并利用单个 A100 GPU 对 LLMs 进行微调。模型被量化为 FP16，并使用参数高效的技术进行微调，包括低秩适应
    Hu 等人（[2021](#bib.bib13)）。我们将温度设置为 0.8 以保持一致性，并使用两个训练轮次。如表 [3](#S6.T3 "Table 3
    ‣ 6 Results and Discussions ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer") 的第三列所示，虽然用
    ChatGPT 进行引导可以减少语法错误，但也会增加幻觉现象，从而导致测试通过率降低和总错误率增加。这表明，尽管模型生成的代码更完整，但在准确性和可靠性方面仍存在困难。今后我们将使用缩写
    BFT 来指代这一基线。'
- en: '5 Our Approach: Projection-Based Zero-Shot Transfer'
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 我们的方法：基于投影的零样本迁移
- en: 'Our approach focuses on avoiding the use of in-language training data, which
    can be costly and impractical. Instead, we utilize an intermediate, lightweight
    method that relies on abundant English data and the LASER multilingual encoder
    Artetxe and Schwenk ([2019](#bib.bib3)), which provides joint embeddings for over
    200 languages. In this setup, the LASER encoder preprocesses and embeds input
    tokens before passing them to the LLM, which then operates on these embeddings
    rather than raw input IDs. This method enables efficient language scaling, as
    similar meanings are represented consistently across languages (e.g., the English
    token "add" and its Hindi counterpart "JoDaNe" are embedded similarly, as shown
    in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Evaluation Dataset ‣ 3 Experimental Setup
    ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation
    in LLMs via Zero-Shot Cross-Lingual Transfer") part (b)).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法专注于避免使用成本高昂且不切实际的语言内训练数据。相反，我们利用一种中间的轻量级方法，该方法依赖于丰富的英语数据和 LASER 多语言编码器
    Artetxe 和 Schwenk ([2019](#bib.bib3))，该编码器为 200 多种语言提供联合嵌入。在这种设置中，LASER 编码器在将输入令牌传递给
    LLM 之前进行预处理和嵌入，LLM 然后在这些嵌入上操作，而不是原始输入 ID。这种方法支持高效的语言扩展，因为相似的含义在不同语言中表现一致（例如，英语令牌“add”和其印地语对应词“JoDaNe”被类似地嵌入，如图
    [2](#S3.F2 "图 2 ‣ 3.1 评估数据集 ‣ 3 实验设置 ‣ 弥合语言差距：通过零样本跨语言转移提升 LLM 的多语言提示生成") 部分 (b)
    所示）。
- en: 'Two key challenges arise with this approach: (A) differing tokenization between
    the multilingual encoder and the LLM, and (B) the LLM’s unfamiliarity with the
    multilingual embeddings. To address (A), we use word tokens and extract mean-pooled
    embeddings from subwords using tokenizers such as NLTK ⁸⁸8https://www.nltk.org
    for space sparated lanaguge inputs, Jieba ⁹⁹9https://github.com/fxsjy/jieba for
    Chinese, and Janome^(10)^(10)10https://mocobeta.github.io/janome/en/ for Japanese.
    We then train a projector to align these embeddings. For a given word token, we
    compute the LLM’s subword embeddings ($\hat{H}_{llm}$, is defined as:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法面临两个主要挑战：（A）多语言编码器和 LLM 之间的令牌化差异，（B）LLM 对多语言嵌入的不熟悉。为了解决 (A)，我们使用词令牌并通过 NLTK
    ⁸⁸8https://www.nltk.org 进行空间分隔语言输入，从 Jieba ⁹⁹9https://github.com/fxsjy/jieba 获取中文子词嵌入，从
    Janome^(10)^(10)10https://mocobeta.github.io/janome/en/ 获取日语子词嵌入。然后我们训练一个投影器来对齐这些嵌入。对于给定的词令牌，我们计算
    LLM 的子词嵌入（$\hat{H}_{llm}$）：
- en: '|  | $\mathbf{H}_{llm}=\mathbf{W}_{llm}\cdot\mathbf{H}_{laser}+\mathbf{b}_{llm}$
    |  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{H}_{llm}=\mathbf{W}_{llm}\cdot\mathbf{H}_{laser}+\mathbf{b}_{llm}$
    |  |'
- en: 'The model is trained by minimizing the Mean Squared Error (MSE) between $\hat{H}_{llm}$:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通过最小化 $\hat{H}_{llm}$ 的均方误差 (MSE) 进行训练。
- en: '|  | $\text{MSE}=\frac{1}{N}\sum_{i=1}^{N}\left\&#124;\hat{H}_{llm}^{i}-\mathbf{H}_{llm}^{i}\right\&#124;^{2}$
    |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{MSE}=\frac{1}{N}\sum_{i=1}^{N}\left\&#124;\hat{H}_{llm}^{i}-\mathbf{H}_{llm}^{i}\right\&#124;^{2}$
    |  |'
- en: where $N$ is the number of word tokens. Training utilizes English tokens from
    the MBPP dataset, which includes 127 examples. We do projector training on a single
    consumer grade NVIDIA 4060 GPU and training the projector happens in 200 epochs
    in less than one hour. During inference, tokens are first word-tokenized and embedded
    using LASER, then projected, and finally input to the LLM for multilingual processing
    without requiring in-language data. To enhance performance and align with baselines,
    we also concatenate system prompt embeddings with the original programming prompt
    embeddings. Notably, LASER embeddings are of size 1024, while LLM embeddings are
    typically 4096 or larger, necessitating a 4-fold upsampling. We achieve this using
    two linear projection layers as outlined in the above equations. We use the abbreviation
    LP to explain this system henceforth.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 是词令牌的数量。训练使用来自 MBPP 数据集的英语令牌，该数据集包含 127 个示例。我们在一台消费级 NVIDIA 4060 GPU
    上进行投影器训练，并在不到一小时的 200 轮训练中完成。推理期间，令牌首先通过 LASER 进行词令牌化和嵌入，然后进行投影，最后输入到 LLM 进行多语言处理，而无需语言内数据。为了提高性能并与基准对齐，我们还将系统提示嵌入与原始编程提示嵌入拼接。值得注意的是，LASER
    嵌入的大小为 1024，而 LLM 嵌入通常为 4096 或更大，因此需要 4 倍的上采样。我们使用上述方程中的两个线性投影层来实现这一点。从此之后，我们用缩写
    LP 来解释该系统。
- en: 6 Results and Discussions
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结果与讨论
- en: '| LLM | Lang | TotalER$\downarrow$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LLM | Lang | TotalER$\downarrow$ |'
- en: '| Orig. | CoT | BFT | LP | Orig. | CoT | BFT | LP | Orig. | CoT | BFT | LP
    | Orig. | CoT | BFT | LP |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | CoT | BFT | LP | 原始 | CoT | BFT | LP | 原始 | CoT | BFT | LP | 原始 | CoT
    | BFT | LP |'
- en: '| GPT-4 (Skyline) | en | 58.37 | - | - | - | 10.9 | - | - | - | 47.47 | - |
    - | - | 41.63 | - | - | - |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 (Skyline) | en | 58.37 | - | - | - | 10.9 | - | - | - | 47.47 | - |
    - | - | 41.63 | - | - | - |'
- en: '| es | 62.65 | - | - | - | 12.85 | - | - | - | 49.8 | - | - | - | 37.35 | -
    | - | - |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| es | 62.65 | - | - | - | 12.85 | - | - | - | 49.8 | - | - | - | 37.35 | -
    | - | - |'
- en: '| hi | 67.7 | - | - | - | 17.9 | - | - | - | 49.8 | - | - | - | 32.3 | - |
    - | - |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| hi | 67.7 | - | - | - | 17.9 | - | - | - | 49.8 | - | - | - | 32.3 | - |
    - | - |'
- en: '| ja | 64.2 | - | - | - | 13.62 | - | - | - | 50.58 | - | - | - | 35.8 | -
    | - | - |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ja | 64.2 | - | - | - | 13.62 | - | - | - | 50.58 | - | - | - | 35.8 | -
    | - | - |'
- en: '| ru | 65.37 | - | - | - | 17.12 | - | - | - | 48.25 | - | - | - | 34.63 |
    - | - | - |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| ru | 65.37 | - | - | - | 17.12 | - | - | - | 48.25 | - | - | - | 34.63 |
    - | - | - |'
- en: '| zh | 67.7 | - | - | - | 16.73 | - | - | - | 50.97 | - | - | - | 32.3 | -
    | - | - |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| zh | 67.7 | - | - | - | 16.73 | - | - | - | 50.97 | - | - | - | 32.3 | -
    | - | - |'
- en: '| Code LLaMa-7B | en | 87.16 | - | 82.1 | 75.49 | 63.04 | - | 28.79 | 22.57
    | 24.12^∗ | - | 53.31 | 52.92 | 12.84 | - | 17.9 | 24.51 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Code LLaMa-7B | en | 87.16 | - | 82.1 | 75.49 | 63.04 | - | 28.79 | 22.57
    | 24.12^∗ | - | 53.31 | 52.92 | 12.84 | - | 17.9 | 24.51 |'
- en: '| es | 79.77 | 91.83 | 81.71 | 81.71 | 28.8 | 56.81 | 26.07 | 24.9 | 50.97
    | 35.02^∗ | 55.64 | 56.81 | 20.23 | 8.17 | 18.29 | 18.29 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| es | 79.77 | 91.83 | 81.71 | 81.71 | 28.8 | 56.81 | 26.07 | 24.9 | 50.97
    | 35.02^∗ | 55.64 | 56.81 | 20.23 | 8.17 | 18.29 | 18.29 |'
- en: '| hi | 96.5 | 97.66 | 96.5 | 95.72 | 65.37 | 61.08 | 61.87 | 25.29 | 31.13
    | 36.58 | 34.63 | 70.43 | 3.5 | 2.34 | 3.5 | 4.28 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| hi | 96.5 | 97.66 | 96.5 | 95.72 | 65.37 | 61.08 | 61.87 | 25.29 | 31.13
    | 36.58 | 34.63 | 70.43 | 3.5 | 2.34 | 3.5 | 4.28 |'
- en: '| ja | 89.49 | 84.82 | 84.82 | 84.44 | 50.58 | 52.91 | 34.24 | 22.96 | 38.91
    | 31.91 | 50.58 | 61.48 | 10.51 | 15.18 | 15.18 | 15.56 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| ja | 89.49 | 84.82 | 84.82 | 84.44 | 50.58 | 52.91 | 34.24 | 22.96 | 38.91
    | 31.91 | 50.58 | 61.48 | 10.51 | 15.18 | 15.18 | 15.56 |'
- en: '| ru | 82.1 | 86.38 | 85.21 | 82.88 | 39.69 | 61.87 | 31.51 | 23.35 | 42.41
    | 24.51 | 53.7 | 59.53 | 17.9 | 13.62 | 14.79 | 17.12 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ru | 82.1 | 86.38 | 85.21 | 82.88 | 39.69 | 61.87 | 31.51 | 23.35 | 42.41
    | 24.51 | 53.7 | 59.53 | 17.9 | 13.62 | 14.79 | 17.12 |'
- en: '| zh | 93.77 | 96.5 | 88.72 | 82.1 | 77.43 | 73.15 | 35.41 | 26.46 | 16.34
    | 23.35 | 53.31 | 55.64 | 6.23 | 3.5 | 11.28 | 17.9 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| zh | 93.77 | 96.5 | 88.72 | 82.1 | 77.43 | 73.15 | 35.41 | 26.46 | 16.34
    | 23.35 | 53.31 | 55.64 | 6.23 | 3.5 | 11.28 | 17.9 |'
- en: '| Code Gemma-7B | en | 82.1 | - | 92.22 | 77.04 | 41.63 | - | 63.04 | 25.68
    | 40.47 | - | 29.18 | 51.36 | 17.9 | - | 7.78 | 22.96 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Code Gemma-7B | en | 82.1 | - | 92.22 | 77.04 | 41.63 | - | 63.04 | 25.68
    | 40.47 | - | 29.18 | 51.36 | 17.9 | - | 7.78 | 22.96 |'
- en: '| es | 86.38 | 89.1 | 91.05 | 77.82 | 47.86 | 42.02 | 57.59 | 24.51 | 38.52
    | 47.08 | 33.46 | 53.31 | 13.62 | 10.9 | 8.95 | 22.18 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| es | 86.38 | 89.1 | 91.05 | 77.82 | 47.86 | 42.02 | 57.59 | 24.51 | 38.52
    | 47.08 | 33.46 | 53.31 | 13.62 | 10.9 | 8.95 | 22.18 |'
- en: '| hi | 89.49 | 91.05 | 94.16 | 81.71 | 49.41 | 50.58 | 74.71 | 29.18 | 40.08
    | 40.47 | 19.45 | 52.53 | 10.51 | 8.95 | 5.84 | 18.29 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| hi | 89.49 | 91.05 | 94.16 | 81.71 | 49.41 | 50.58 | 74.71 | 29.18 | 40.08
    | 40.47 | 19.45 | 52.53 | 10.51 | 8.95 | 5.84 | 18.29 |'
- en: '| ja | 83.66 | 90.27 | 91.05 | 79.77 | 38.91 | 44.75 | 50.58 | 24.13 | 44.75
    | 45.52 | 40.47 | 55.64 | 16.34 | 9.73 | 8.95 | 20.23 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ja | 83.66 | 90.27 | 91.05 | 79.77 | 38.91 | 44.75 | 50.58 | 24.13 | 44.75
    | 45.52 | 40.47 | 55.64 | 16.34 | 9.73 | 8.95 | 20.23 |'
- en: '| ru | 85.99 | 88.72 | 89.1 | 77.04 | 42.41 | 48.25 | 59.53 | 25.68 | 43.58
    | 40.47 | 29.57 | 51.36 | 14.01 | 11.28 | 10.9 | 22.96 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| ru | 85.99 | 88.72 | 89.1 | 77.04 | 42.41 | 48.25 | 59.53 | 25.68 | 43.58
    | 40.47 | 29.57 | 51.36 | 14.01 | 11.28 | 10.9 | 22.96 |'
- en: '| zh | 84.82 | 86.38 | 93.0 | 79.38 | 39.68 | 48.64 | 62.26 | 28.02 | 45.14
    | 37.74 | 30.74 | 51.36 | 15.18 | 13.62 | 7.0 | 20.62 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| zh | 84.82 | 86.38 | 93.0 | 79.38 | 39.68 | 48.64 | 62.26 | 28.02 | 45.14
    | 37.74 | 30.74 | 51.36 | 15.18 | 13.62 | 7.0 | 20.62 |'
- en: '| Mistral -7B-v0.3 | en | 85.21 | - | 92.61 | 83.27 | 35.41 | - | 28.41 | 27.24
    | 49.8 | - | 64.2 | 56.03 | 14.79 | - | 7.39 | 16.73 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Mistral -7B-v0.3 | en | 85.21 | - | 92.61 | 83.27 | 35.41 | - | 28.41 | 27.24
    | 49.8 | - | 64.2 | 56.03 | 14.79 | - | 7.39 | 16.73 |'
- en: '| es | 87.55 | 86.38 | 94.94 | 84.82 | 39.69 | 29.18 | 26.46 | 26.06 | 47.86
    | 57.2 | 68.48 | 58.76 | 12.45 | 13.62 | 5.06 | 15.18 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| es | 87.55 | 86.38 | 94.94 | 84.82 | 39.69 | 29.18 | 26.46 | 26.06 | 47.86
    | 57.2 | 68.48 | 58.76 | 12.45 | 13.62 | 5.06 | 15.18 |'
- en: '| hi | 91.44 | 91.05 | 98.83 | 92.22 | 35.41 | 35.41 | 24.12 | 30.74 | 56.03
    | 55.64 | 74.71 | 61.48 | 8.56 | 8.95 | 1.17 | 7.78 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| hi | 91.44 | 91.05 | 98.83 | 92.22 | 35.41 | 35.41 | 24.12 | 30.74 | 56.03
    | 55.64 | 74.71 | 61.48 | 8.56 | 8.95 | 1.17 | 7.78 |'
- en: '| ja | 88.72 | 86.77 | 96.11 | 87.55 | 35.8 | 31.91 | 28.02 | 22.57 | 52.92
    | 54.86 | 68.09 | 64.98 | 11.28 | 13.23 | 3.89 | 12.45 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ja | 88.72 | 86.77 | 96.11 | 87.55 | 35.8 | 31.91 | 28.02 | 22.57 | 52.92
    | 54.86 | 68.09 | 64.98 | 11.28 | 13.23 | 3.89 | 12.45 |'
- en: '| ru | 85.6 | 84.05 | 95.33 | 84.05 | 33.85 | 30.74 | 26.85 | 24.13 | 51.75
    | 53.31 | 68.48 | 59.92 | 14.4 | 15.95 | 4.67 | 15.95 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ru | 85.6 | 84.05 | 95.33 | 84.05 | 33.85 | 30.74 | 26.85 | 24.13 | 51.75
    | 53.31 | 68.48 | 59.92 | 14.4 | 15.95 | 4.67 | 15.95 |'
- en: '| zh | 88.72 | 87.16 | 94.55 | 84.05 | 39.3 | 30.74 | 26.07 | 26.85 | 49.42
    | 56.42 | 68.48 | 57.2 | 11.28 | 12.84 | 5.45 | 15.95 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| zh | 88.72 | 87.16 | 94.55 | 84.05 | 39.3 | 30.74 | 26.07 | 26.85 | 49.42
    | 56.42 | 68.48 | 57.2 | 11.28 | 12.84 | 5.45 | 15.95 |'
- en: 'Table 3: Comprehensive comparison of different models across multiple languages
    and configurations. TotalER: Total Error Rate, LER: Logical Error Rate, SER: Syntax
    Error Rate, ATPR: All Test Passed Rate. Orig: Directly Querying LLMs, CoT: Chain
    of Thought with Translation, BFT: Fine tuning on Bootstrapped Multilingual Data,
    LP (Our approach): Fine tuning on Multilingual Projection with LASER Encoders'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同模型在多种语言和配置下的综合比较。TotalER：总错误率，LER：逻辑错误率，SER：语法错误率，ATPR：所有测试通过率。Orig：直接查询LLMs，CoT：带翻译的思维链，BFT：在引导的多语言数据上进行微调，LP（我们的方法）：在多语言投影上进行微调，使用LASER编码器
- en: '![Refer to caption](img/33c542037473870ff5e950e5d9cd9216.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/33c542037473870ff5e950e5d9cd9216.png)'
- en: 'Figure 3: Code Completion Rate (CCR) for Models and Languages, with LP represented
    by perfect polygons, thus demonstrating between all languages and with highest
    surface area demonstrating higher CCR, often more than 90%'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：模型和语言的代码完成率（CCR），LP用完美的多边形表示，从而展示了所有语言之间的差异，且具有最大表面积的表示更高的CCR，通常超过90%
- en: 'Table [3](#S6.T3 "Table 3 ‣ 6 Results and Discussions ‣ Bridging the Language
    Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot
    Cross-Lingual Transfer") presents the overall performance models and variants
    discussed in sections [4](#S4 "4 Issues with Trivial Baselines ‣ Bridging the
    Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via
    Zero-Shot Cross-Lingual Transfer") and [5](#S5 "5 Our Approach: Projection-Based
    Zero-Shot Transfer ‣ Bridging the Language Gap: Enhancing Multilingual Prompt-Based
    Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer"). Our observations
    indicate that across all metrics, our proposed model consistently reduces the
    performance gap between English and non-English languages, as reflected in the
    differences and deviations. This improvement is particularly evident when comparing
    the direct querying setup (Orig.) with our multilingual projector-based variant
    (LP), where deviations from English are generally smaller. We explore the details
    of each metric below.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表[3](#S6.T3 "表 3 ‣ 6 结果与讨论 ‣ 弥合语言差距：通过零样本跨语言迁移提升多语言提示生成在LLMs中的表现")展示了在[4](#S4
    "4 对简单基线的问题 ‣ 弥合语言差距：通过零样本跨语言迁移提升多语言提示生成在LLMs中的表现")和[5](#S5 "5 我们的方法：基于投影的零样本迁移
    ‣ 弥合语言差距：通过零样本跨语言迁移提升多语言提示生成在LLMs中的表现")部分讨论的模型和变体的整体表现。我们的观察表明，在所有指标中，我们提出的模型始终缩小了英语与非英语语言之间的性能差距，如差异和偏差所示。这一改进在比较直接查询设置（Orig.）与我们的多语言投影变体（LP）时尤为明显，其中英语的偏差通常较小。我们将在下面详细探讨每个指标。
- en: 6.1 Total Error Rate (TotalER)
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 总错误率（TotalER）
- en: The Total Error Rate (TotalER) is an important metric that quantifies the overall
    error rate of the generated code. Our proposed method, LP, consistently achieves
    the lowest TotalER across nearly all languages and models, demonstrating its effectiveness.
    For example, with the CodeLLaMa-7B model, LP significantly reduces the TotalER
    to $75.49$ for Chinese (zh), outperforming the original model (Orig) and other
    methods. This improvement is especially pronounced in languages with complex syntax
    and morphology, such as Hindi (hi) and Russian (ru), where LP reduces the TotalER
    by over 10% in some cases compared to the original model. Even in cases where
    LP is the second-best, its performance is very close to the top-performing method,
    highlighting its reliability. In contrast, finetuning on multilingual bootstrapped
    data (BFT), a strong trivial baseline, tends to increase the TotalER due to hallucinations,
    as observed in our data analysis, despite slightly improving the all test cases
    passed metric.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总错误率（TotalER）是一个重要的指标，用于量化生成代码的整体错误率。我们提出的方法LP在几乎所有语言和模型中始终实现了最低的TotalER，显示了其有效性。例如，对于CodeLLaMa-7B模型，LP将TotalER显著降低到$75.49$，针对中文（zh），超越了原始模型（Orig）和其他方法。这一改进在语法和形态复杂的语言中尤为明显，例如印地语（hi）和俄语（ru），在一些情况下LP相较于原始模型将TotalER降低了超过10%。即使在LP是第二名的情况下，其表现也非常接近于最佳表现方法，突显了其可靠性。相比之下，在多语言引导数据（BFT）上进行微调，这一强大的简单基线，因幻觉现象往往增加了TotalER，尽管在所有测试通过率上有轻微提升。
- en: 6.2 Logical Error Rate (LER)
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 逻辑错误率（LER）
- en: The Logical Error Rate (LER) is a critical component of the total error, measuring
    the proportion of code samples that execute without errors but produce incorrect
    results. A lower LER indicates a model’s ability to generate logically sound code,
    making it a key metric for evaluating performance. It’s important to note that
    we classify a logical error not only when no valid code is generated but also
    when any of the test cases fail.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑错误率（LER）是总错误的一个关键组成部分，它衡量执行无错误但产生不正确结果的代码样本的比例。较低的 LER 表明模型生成逻辑上合理的代码的能力，使其成为评估性能的关键指标。需要注意的是，我们不仅在未生成有效代码时才将其归类为逻辑错误，还包括当任何测试用例失败时。
- en: Our approach, LP, consistently outperforms other methods in terms of LER, with
    only a few exceptions where the difference is marginal and still better than other
    candidates. For instance, with the CodeGemma-7B model, LP achieved an LER of 25.68
    for English, significantly lower than the 41.63 in Orig and 63.04 in bootstrapped
    multilingual fine tuning (BFT). This trend is also evident in other languages,
    such as Spanish (es) and Japanese (ja), where LP substantially reduces LER, underscoring
    its effectiveness in ensuring logical correctness across multilingual scenarios.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法 LP 在 LER 方面始终优于其他方法，仅在少数情况下差异很小但仍优于其他候选方法。例如，在 CodeGemma-7B 模型中，LP 实现了
    25.68 的英语 LER，显著低于 Orig 的 41.63 和经过引导的多语言微调（BFT）的 63.04。这种趋势在西班牙语（es）和日语（ja）等其他语言中也很明显，LP
    大幅降低了 LER，突显了其在确保多语言场景下逻辑正确性方面的有效性。
- en: 6.3 Syntax Error Rate (SER)
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 语法错误率（SER）
- en: The Syntax Error Rate (SER) is a component of total error and indicates the
    proportion of code samples that contain syntax errors. A lower SER reflects the
    model’s ability to generate syntactically correct code. Our overall observations
    with respect to this metric is that models like ours that often produce code than
    omitting it (which is indicated by the lower logical error) are more prone to
    syntax error due to the high recall. While syntax error solving is a crucial step
    in program debugging, we believe such a form of error is slightly easier to solve
    than logical errors. Thus, given that LP consistently achieves the lowest LER
    across all languages and models, we believe this demonstrates the LP’s proficiency
    in helping with generating error-free code, particularly in linguistically diverse
    contexts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 语法错误率（SER）是总错误的一个组成部分，表示包含语法错误的代码样本的比例。较低的 SER 反映了模型生成语法正确代码的能力。我们对这一指标的总体观察是，像我们这样的模型经常生成代码而不是遗漏代码（这表明较低的逻辑错误）由于高召回率，更容易出现语法错误。尽管解决语法错误是程序调试中的一个关键步骤，但我们认为这种错误比逻辑错误稍微容易解决。因此，考虑到
    LP 在所有语言和模型中始终实现最低的 LER，我们认为这表明 LP 在帮助生成无错误代码方面的能力，特别是在语言多样化的环境中。
- en: 6.4 All Test Passed Rate (ATPR)
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 所有测试通过率（ATPR）
- en: The All Tests Passed Rate (ATPR) measures the proportion of code samples that
    successfully pass all given test cases. A higher ATPR signifies greater reliability
    of the generated code, making it a crucial metric. Our observations show that
    LP consistently outperforms other methods in terms of ATPR across most cases.
    However, there are exceptions with the Mistral-7B-v0.3 model in a few languages.
    This model, being more recent, benefits from enhanced multilingual capabilities
    due to its diverse pretraining datasets and extended vocabulary. Overall, ATPR
    improvements are consistent across other languages, highlighting LP’s superior
    performance in generating reliable and functional code.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所有测试通过率（ATPR）衡量成功通过所有给定测试用例的代码样本的比例。较高的 ATPR 表明生成的代码更可靠，使其成为一个重要的指标。我们的观察显示，LP
    在大多数情况下在 ATPR 上始终优于其他方法。然而，Mistral-7B-v0.3 模型在一些语言中存在例外。该模型较新，由于其多样的预训练数据集和扩展词汇表，具有更强的多语言能力。总体而言，其他语言中的
    ATPR 改进是持续的，突显了 LP 在生成可靠和功能性代码方面的卓越表现。
- en: 'Our observations using Multilingual Projections with LASER Encoders reveal
    that LP not only reduces errors but also enhances the logical correctness and
    reliability of the generated code, establishing it as the leading approach for
    multilingual Python code generation. Additionally, we analyze the Code Completion
    Rate (CCR) to assess the robustness of these models in generating meaningful code
    rather than nonsensical explanations across languages. LP consistently outperforms
    other variants in this regard, as shown in the spider graph in Figure [3](#S6.F3
    "Figure 3 ‣ 6 Results and Discussions ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer"). This
    graph illustrates LP’s strong performance in producing complete code across all
    languages. Notably, the shapes representing LP in the graph are perfect polygons,
    reflecting its consistent behavior and reliability across different languages.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用带有LASER编码器的多语言投影的观察结果显示，LP不仅减少了错误，还提高了生成代码的逻辑正确性和可靠性，使其成为多语言Python代码生成的领先方法。此外，我们分析了代码完成率（CCR），以评估这些模型在生成有意义代码而非无意义解释方面的鲁棒性。LP在这方面始终优于其他变体，如图[3](#S6.F3
    "Figure 3 ‣ 6 Results and Discussions ‣ Bridging the Language Gap: Enhancing Multilingual
    Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer")的蜘蛛图所示。该图展示了LP在生成所有语言的完整代码方面的强劲表现。值得注意的是，图中表示LP的形状是完美的多边形，反映了其在不同语言中的一致行为和可靠性。'
- en: 7 Conclusions and Future Work
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来工作
- en: In this paper, we demonstrated the significant potential of Large Language Models
    to bridge language gaps and promote inclusivity in multilingual prompt-based code
    generation. While LLMs exhibit promising capabilities across various languages,
    their performance can be inconsistent, particularly with non-English prompts.
    Our comprehensive analysis and evaluation using a benchmark dataset revealed both
    strengths and limitations in multilingual code generation, highlighting areas
    needing improvement.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们展示了大型语言模型在弥合语言差距和促进多语言提示基础代码生成中的重大潜力。虽然LLMs在各种语言中展示了有希望的能力，但它们的表现可能不一致，特别是在非英语提示下。我们使用基准数据集的全面分析和评估揭示了多语言代码生成中的优势和局限性，突出了需要改进的领域。
- en: We showcased the effectiveness of bootstrapping multilingual training data and
    fine-tuning LLMs to enhance code generation quality across multiple languages.
    Our zero-shot cross-lingual transfer approach, utilizing projected embeddings,
    proved effective, as evidenced by improved ATPR and reduced TotalER values. This
    method eliminates the need for extensive external multilingual data, maximizing
    the model’s potential internally. Future work will expand this approach to include
    more languages, diverse prompt patterns, and programming languages beyond Python.
    Our findings underscore the importance of advancing these techniques to enhance
    LLM adaptability and utility for a global audience, stressing the need for ongoing
    efforts to improve their effectiveness and versatility in diverse linguistic contexts.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了引导多语言训练数据和微调LLMs以提高多语言代码生成质量的有效性。我们的零样本跨语言迁移方法利用了投影嵌入，证明了其有效性，表现在改进的ATPR和减少的TotalER值上。这种方法消除了对大量外部多语言数据的需求，最大化了模型在内部的潜力。未来的工作将扩展此方法以包括更多语言、多样化的提示模式和Python以外的编程语言。我们的发现强调了推进这些技术以增强LLM的适应性和全球受众的实用性的重要性，强调了在不同语言背景下提高其有效性和多样性的持续努力的必要性。
- en: 8 Limitations
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 限制
- en: A major limitation of this work lies in the reliance on word tokenization and
    pooled token embeddings, which introduces external dependencies and may not scale
    effectively to extremely low-resource languages where tokenizers are not readily
    available. Furthermore, the sequence of projected embeddings from the target language
    can significantly differ from the canonical English order, potentially hindering
    the model’s ability to fully leverage these embeddings. This misalignment could
    contribute to the generation of hallucinatory and erroneous outputs. To address
    this issue, some degree of fine-tuning of LLMs with denoising objectives may be
    necessary.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的一个主要限制在于对单词分词和池化的词嵌入的依赖，这引入了外部依赖，并且可能无法有效扩展到分词器不易获得的极低资源语言。此外，目标语言的投影嵌入的顺序可能与标准的英语顺序显著不同，这可能妨碍模型充分利用这些嵌入。这种不匹配可能导致生成幻觉和错误的输出。为了解决这个问题，可能需要对LLMs进行一定程度的带有去噪目标的微调。
- en: Moreover, our exploration is limited to only five non-English languages, which,
    while a promising start, is not comprehensive enough to establish the approach
    as a fully robust multilingual solution. Additionally, our study focuses solely
    on generating code from scratch and does not cover code-filling scenarios, which
    is another important aspect that warrants future exploration. Due to resource
    constraints, the scope of this study has been limited to Python, but expanding
    the approach to encompass other general-purpose and special-purpose programming
    languages is essential for broader applicability.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的探索仅限于五种非英语语言，这虽然是一个有希望的开始，但尚不足以将这种方法确立为一个完全稳健的多语言解决方案。此外，我们的研究仅关注从头生成代码，而不涵盖代码填充场景，这是另一个值得未来探索的重要方面。由于资源限制，本研究的范围被限制在
    Python，但将这种方法扩展到涵盖其他通用编程语言和特殊用途编程语言对于更广泛的适用性至关重要。
- en: 9 Ethical considerations
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理考虑
- en: The models we utilized in our study are widely used ones from OpenAI, Google,
    MistralAI and Meta, and we employed Google Cloud Translator and the MBPP dataset
    on Hugging Face. All of these resources are publicly accessible; we did not introduce
    any additional real-world data, thus avoiding the creation of new ethical and
    privacy issues.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在研究中使用的模型是 OpenAI、Google、MistralAI 和 Meta 的广泛使用的模型，并且我们使用了 Google Cloud Translator
    和 Hugging Face 上的 MBPP 数据集。所有这些资源都是公开可用的；我们没有引入任何额外的现实世界数据，从而避免了新伦理和隐私问题的产生。
- en: Given we are dealing with black-box Large Language Models as part of this study,
    there needs to be careful consideration of any potential biases that can be harmful
    in nature. Although we are focusing on a objective task with little to no opinion
    sourcing from the models, cultural and racial biases can occur given we are exposing
    the models to multi-lingual prompts. Since the applications we are focusing on
    are essentially user-centric in nature, a proper communication protocol should
    be established that can help clarify potential erratic behaviour of models, especially
    for low-resource languages. We would also like to share that we employed OpenAI’s
    ChatGPT-4 system to enhance writing efficiency by generating LaTeX code, ensuring
    concise sentences, and aiding in error debugging.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们在本研究中处理的是黑箱大型语言模型，需要仔细考虑可能存在的有害偏见。尽管我们专注于一个客观的任务，模型几乎没有意见来源，但由于我们将模型暴露于多语言提示中，文化和种族偏见可能会发生。由于我们关注的应用本质上是以用户为中心的，应该建立适当的沟通协议，以帮助澄清模型的潜在异常行为，特别是对于资源匮乏的语言。我们还希望分享，我们使用了
    OpenAI 的 ChatGPT-4 系统来提高写作效率，通过生成 LaTeX 代码、确保简洁的句子和辅助错误调试。
- en: References
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ahuja et al. (2023) Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng,
    Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed
    Axmed, Kalika Bali, and Sunayana Sitaram. 2023. [Mega: Multilingual evaluation
    of generative ai](http://arxiv.org/abs/2303.12528).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ahuja 等人（2023）Kabir Ahuja, Harshita Diddee, Rishav Hada, Millicent Ochieng,
    Krithika Ramesh, Prachi Jain, Akshay Nambi, Tanuja Ganu, Sameer Segal, Maxamed
    Axmed, Kalika Bali, 和 Sunayana Sitaram。2023。[Mega: 生成性 AI 的多语言评估](http://arxiv.org/abs/2303.12528)。'
- en: 'Akiki et al. (2022) Christopher Akiki, Giada Pistilli, Margot Mieskes, Matthias
    Gallé, Thomas Wolf, Suzana Ilić, and Yacine Jernite. 2022. [Bigscience: A case
    study in the social construction of a multilingual large language model](http://arxiv.org/abs/2212.04960).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Akiki 等人（2022）Christopher Akiki, Giada Pistilli, Margot Mieskes, Matthias Gallé,
    Thomas Wolf, Suzana Ilić, 和 Yacine Jernite。2022。[Bigscience: 多语言大型语言模型的社会构建案例研究](http://arxiv.org/abs/2212.04960)。'
- en: Artetxe and Schwenk (2019) Mikel Artetxe and Holger Schwenk. 2019. Massively
    multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond.
    *Transactions of the association for computational linguistics*, 7:597–610.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Artetxe 和 Schwenk（2019）Mikel Artetxe 和 Holger Schwenk。2019。用于零样本跨语言迁移的大规模多语言句子嵌入。*计算语言学协会会刊*，7:597–610。
- en: Austin et al. (2021) Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. 2021. Program synthesis with large language models. *arXiv preprint
    arXiv:2108.07732*.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin 等人（2021）Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
    Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le 等人。2021。大型语言模型的程序合成。*arXiv
    预印本 arXiv:2108.07732*。
- en: Awasthi et al. (2022) Abhijeet Awasthi, Nitish Gupta, Bidisha Samanta, Shachi
    Dave, Sunita Sarawagi, and Partha Talukdar. 2022. Bootstrapping multilingual semantic
    parsers using large language models. *arXiv preprint arXiv:2210.07313*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Awasthi 等（2022）阿比杰特·阿瓦斯提、尼提什·古普塔、比迪莎·萨曼塔、沙奇·戴夫、苏尼塔·萨拉瓦吉 和帕尔塔·塔卢克达尔。2022。利用大型语言模型引导多语言语义解析器。*arXiv
    预印本 arXiv:2210.07313*。
- en: 'Becker et al. (2023) Brett A Becker, Paul Denny, James Finnie-Ansley, Andrew
    Luxton-Reilly, James Prather, and Eddie Antonio Santos. 2023. Programming is hard-or
    at least it used to be: Educational opportunities and challenges of ai code generation.
    In *Proceedings of the 54th ACM Technical Symposium on Computer Science Education
    V. 1*, pages 500–506.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Becker 等（2023）布雷特·A·贝克、保罗·丹尼、詹姆斯·芬尼-安斯利、安德鲁·卢克斯顿-雷利、詹姆斯·普拉瑟 和埃迪·安东尼奥·桑托斯。2023。编程很难——或者说以前很难：AI
    代码生成的教育机会和挑战。见于 *第 54 届 ACM 计算机科学教育技术研讨会 V. 1*，页码 500–506。
- en: 'Beyer et al. (2024) Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander
    Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael
    Tschannen, Emanuele Bugliarello, et al. 2024. Paligemma: A versatile 3b vlm for
    transfer. *arXiv preprint arXiv:2407.07726*.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beyer 等（2024）卢卡斯·贝耶尔、安德烈亚斯·斯坦纳、安德烈·苏萨诺·平托、亚历山大·科列斯尼科夫、肖旺、丹尼尔·萨尔茨、马克西姆·诺伊曼、易卜拉欣·阿拉卜杜勒莫辛、迈克尔·察南、埃曼纽尔·布利亚列洛等。2024。Paligemma：一种多功能的
    3b vlm 用于迁移。*arXiv 预印本 arXiv:2407.07726*。
- en: 'Chen et al. (2024) Pinzhen Chen, Shaoxiong Ji, Nikolay Bogoychev, Andrey Kutuzov,
    Barry Haddow, and Kenneth Heafield. 2024. [Monolingual or multilingual instruction
    tuning: Which makes a better alpaca](http://arxiv.org/abs/2309.08958).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2024）陈品臻、贾少雄、尼古拉·博戈耶夫、安德烈·库图佐夫、巴里·哈多 和肯尼斯·希菲尔德。2024。[单语还是多语指令调整：哪种更适合
    alpaca](http://arxiv.org/abs/2309.08958)。
- en: Choudhury and Deshpande (2021) Monojit Choudhury and Amit Deshpande. 2021. How
    linguistically fair are multilingual pre-trained language models? In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 35, pages 12710–12718.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choudhury 和 Deshpande（2021）莫诺吉特·周德里和阿米特·德斯潘德。2021。多语言预训练语言模型的语言公平性如何？见于 *AAAI
    人工智能会议论文集*，第 35 卷，页码 12710–12718。
- en: Fathullah et al. (2024) Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng
    Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli,
    et al. 2024. Prompting large language models with speech recognition abilities.
    In *ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal
    Processing (ICASSP)*, pages 13351–13355\. IEEE.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fathullah 等（2024）亚瑟·法图拉、春阳·吴、埃戈尔·拉科姆金、俊腾·贾、袁·尚冠、柯·李、晋熙·郭、文瀚·熊、杰·马哈德卡、厄兹莱姆·卡林利
    等。2024。通过语音识别能力对大型语言模型进行提示。见于 *ICASSP 2024-2024 IEEE 国际声学、语音和信号处理会议（ICASSP）*，页码
    13351–13355。IEEE。
- en: 'Gao et al. (2023) Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu,
    Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language
    models. In *International Conference on Machine Learning*, pages 10764–10799\.
    PMLR.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等（2023）刘雨雨、阿曼·马丹、舒燕·周、乌里·阿隆、彭飞·刘、易铭·杨、杰米·卡兰 和格雷厄姆·纽比格。2023。Pal：程序辅助语言模型。见于
    *国际机器学习会议*，页码 10764–10799。PMLR。
- en: Hoang et al. (2018) Cong Duy Vu Hoang, Philipp Koehn, Gholamreza Haffari, and
    Trevor Cohn. 2018. Iterative back-translation for neural machine translation.
    In *2nd Workshop on Neural Machine Translation and Generation*, pages 18–24\.
    Association for Computational Linguistics.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoang 等（2018）孔杜伊·吴·黄、菲利普·科恩、戈拉姆雷扎·哈法里 和特雷弗·科恩。2018。神经机器翻译的迭代回译。见于 *第 2 届神经机器翻译与生成研讨会*，页码
    18–24。计算语言学协会。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation
    of large language models. *arXiv preprint arXiv:2106.09685*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等（2021）爱德华·J·胡、叶龙·申、菲利普·沃利斯、泽远·艾伦-朱、袁智·李、申·王、陆·王 和魏铸·陈。2021。Lora：大语言模型的低秩适应。*arXiv
    预印本 arXiv:2106.09685*。
- en: 'Huang et al. (2023) Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao,
    Ting Song, Yan Xia, and Furu Wei. 2023. [Not all languages are created equal in
    llms: Improving multilingual capability by cross-lingual-thought prompting](http://arxiv.org/abs/2305.07004).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2023）郝阳·黄、田毅·唐、董东·张、韦恩·辛·赵、丁松、严霞 和付如·魏。2023。[不是所有语言在 llms 中都一样：通过跨语言思维提示提升多语言能力](http://arxiv.org/abs/2305.07004)。
- en: Huang et al. (2022) Lianzhe Huang, Shuming Ma, Dongdong Zhang, Furu Wei, and
    Houfeng Wang. 2022. Zero-shot cross-lingual transfer of prompt-based tuning with
    a unified multilingual prompt. *arXiv preprint arXiv:2202.11451*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等（2022）连哲·黄、舒敏·马、董东·张、付如·魏 和侯锋·王。2022。通过统一的多语言提示实现零样本跨语言迁移。*arXiv 预印本
    arXiv:2202.11451*。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
- en: 'Lai et al. (2023) Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu
    Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. [Chatgpt beyond
    english: Towards a comprehensive evaluation of large language models in multilingual
    learning](http://arxiv.org/abs/2304.05613).'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lai et al. (2023) Viet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Veyseh, Hieu
    Man, Franck Dernoncourt, Trung Bui, and Thien Huu Nguyen. 2023. [Chatgpt beyond
    english: Towards a comprehensive evaluation of large language models in multilingual
    learning](http://arxiv.org/abs/2304.05613).'
- en: Li et al. (2023) Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer,
    Omer Levy, Jason Weston, and Mike Lewis. 2023. Self-alignment with instruction
    backtranslation. *arXiv preprint arXiv:2308.06259*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer,
    Omer Levy, Jason Weston, and Mike Lewis. 2023. Self-alignment with instruction
    backtranslation. *arXiv preprint arXiv:2308.06259*.
- en: Liu et al. (2024) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024.
    Visual instruction tuning. *Advances in neural information processing systems*,
    36.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2024) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024.
    Visual instruction tuning. *Advances in neural information processing systems*,
    36.
- en: 'Ma et al. (2023) Yingwei Ma, Yue Yu, Shanshan Li, Yu Jiang, Yong Guo, Yuanliang
    Zhang, Yutao Xie, and Xiangke Liao. 2023. Bridging code semantic and llms: Semantic
    chain-of-thought prompting for code generation. *arXiv preprint arXiv:2310.10698*.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. (2023) Yingwei Ma, Yue Yu, Shanshan Li, Yu Jiang, Yong Guo, Yuanliang
    Zhang, Yutao Xie, and Xiangke Liao. 2023. Bridging code semantic and llms: Semantic
    chain-of-thought prompting for code generation. *arXiv preprint arXiv:2310.10698*.'
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In
    *Proceedings of the 40th annual meeting of the Association for Computational Linguistics*,
    pages 311–318.'
- en: 'Pfeiffer et al. (2022) Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James
    Cross, Sebastian Riedel, and Mikel Artetxe. 2022. [Lifting the curse of multilinguality
    by pre-training modular transformers](https://doi.org/10.18653/v1/2022.naacl-main.255).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3479–3495,
    Seattle, United States. Association for Computational Linguistics.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pfeiffer et al. (2022) Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James
    Cross, Sebastian Riedel, and Mikel Artetxe. 2022. [Lifting the curse of multilinguality
    by pre-training modular transformers](https://doi.org/10.18653/v1/2022.naacl-main.255).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 3479–3495,
    Seattle, United States. Association for Computational Linguistics.'
- en: 'Qin et al. (2023) Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang
    Che. 2023. [Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning
    across languages](http://arxiv.org/abs/2310.14799).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin et al. (2023) Libo Qin, Qiguang Chen, Fuxuan Wei, Shijue Huang, and Wanxiang
    Che. 2023. [Cross-lingual prompting: Improving zero-shot chain-of-thought reasoning
    across languages](http://arxiv.org/abs/2310.14799).'
- en: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950*.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere et al. (2023) Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code llama: Open foundation models for code. *arXiv preprint
    arXiv:2308.12950*.'
- en: Sarsa et al. (2022) Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen.
    2022. Automatic generation of programming exercises and code explanations using
    large language models. In *Proceedings of the 2022 ACM Conference on International
    Computing Education Research-Volume 1*, pages 27–43.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarsa et al. (2022) Sami Sarsa, Paul Denny, Arto Hellas, and Juho Leinonen.
    2022. Automatic generation of programming exercises and code explanations using
    large language models. In *Proceedings of the 2022 ACM Conference on International
    Computing Education Research-Volume 1*, pages 27–43.
- en: Sennrich et al. (2015) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015.
    Improving neural machine translation models with monolingual data. *arXiv preprint
    arXiv:1511.06709*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich et al. (2015) Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015.
    Improving neural machine translation models with monolingual data. *arXiv preprint
    arXiv:1511.06709*.
- en: Shi et al. (2022a) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou,
    Dipanjan Das, and Jason Wei. 2022a. [Language models are multilingual chain-of-thought
    reasoners](http://arxiv.org/abs/2210.03057).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2022a) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou,
    Dipanjan Das 和 Jason Wei。2022a年。[语言模型是多语言链式思维推理者](http://arxiv.org/abs/2210.03057)。
- en: Shi et al. (2022b) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou,
    et al. 2022b. Language models are multilingual chain-of-thought reasoners. *arXiv
    preprint arXiv:2210.03057*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2022b) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou
    等。2022b年。《语言模型是多语言链式思维推理者》。*arXiv 预印本 arXiv:2210.03057*。
- en: 'Talat et al. (2022) Zeerak Talat, Aurélie Névéol, Stella Biderman, Miruna Clinciu,
    Manan Dey, Shayne Longpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell, Dragomir
    Radev, Shanya Sharma, Arjun Subramonian, Jaesung Tae, Samson Tan, Deepak Tunuguntla,
    and Oskar Van Der Wal. 2022. [You reap what you sow: On the challenges of bias
    evaluation under multilingual settings](https://doi.org/10.18653/v1/2022.bigscience-1.3).
    In *Proceedings of BigScience Episode #5 – Workshop on Challenges & Perspectives
    in Creating Large Language Models*, pages 26–41, virtual+Dublin. Association for
    Computational Linguistics.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Talat et al. (2022) Zeerak Talat, Aurélie Névéol, Stella Biderman, Miruna Clinciu,
    Manan Dey, Shayne Longpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell, Dragomir
    Radev, Shanya Sharma, Arjun Subramonian, Jaesung Tae, Samson Tan, Deepak Tunuguntla
    和 Oskar Van Der Wal。2022年。[你种下什么，收获什么: 多语言环境下的偏差评估挑战](https://doi.org/10.18653/v1/2022.bigscience-1.3)。在*BigScience
    Episode #5 – 创建大型语言模型的挑战与展望研讨会*，第26–41页，虚拟+都柏林。计算语言学协会。'
- en: Tan and Golovneva (2020) Lizhen Tan and Olga Golovneva. 2020. Evaluating cross-lingual
    transfer learning approaches in multilingual conversational agent models. *arXiv
    preprint arXiv:2012.03864*.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan and Golovneva (2020) Lizhen Tan 和 Olga Golovneva。2020年。《评估多语言对话代理模型中的跨语言迁移学习方法》。*arXiv
    预印本 arXiv:2012.03864*。
- en: 'Team (2024) CodeGemma Team. 2024. Codegemma: Open code models based on gemma.
    *arXiv preprint arXiv:2406.11409*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team (2024) CodeGemma Team。2024年。《Codegemma: 基于 gemma 的开放代码模型》。*arXiv 预印本 arXiv:2406.11409*。'
- en: 'Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research
    and technology. *arXiv preprint arXiv:2403.08295*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. (2024) Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi,
    Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay
    Kale, Juliette Love 等。2024年。《Gemma: 基于 gemini 研究和技术的开放模型》。*arXiv 预印本 arXiv:2403.08295*。'
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等。2023年。《Llama 2: 开放基础和微调聊天模型》。*arXiv 预印本 arXiv:2307.09288*。'
- en: Yetistiren et al. (2022) Burak Yetistiren, Isik Ozsoy, and Eray Tuzun. 2022.
    Assessing the quality of github copilot’s code generation. In *Proceedings of
    the 18th International Conference on Predictive Models and Data Analytics in Software
    Engineering*, pages 62–71.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yetistiren et al. (2022) Burak Yetistiren, Isik Ozsoy 和 Eray Tuzun。2022年。《评估
    github copilot 的代码生成质量》。在*第18届国际预测模型与数据分析在软件工程中的应用大会论文集*，第62–71页。
- en: 'Zhang et al. (2024) Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen,
    Xin Huang, Xue Han, Junlan Feng, Chao Deng, and Shujian Huang. 2024. [Getting
    more from less: Large language models are good spontaneous multilingual learners](http://arxiv.org/abs/2405.13816).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2024) Shimao Zhang, Changjiang Gao, Wenhao Zhu, Jiajun Chen,
    Xin Huang, Xue Han, Junlan Feng, Chao Deng 和 Shujian Huang。2024年。[从更少中获得更多: 大型语言模型是出色的自发多语言学习者](http://arxiv.org/abs/2405.13816)。'
- en: Zhao and Schütze (2021) Mengjie Zhao and Hinrich Schütze. 2021. Discrete and
    soft prompting for multilingual models. *arXiv preprint arXiv:2109.03630*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao and Schütze (2021) Mengjie Zhao 和 Hinrich Schütze。2021年。《多语言模型的离散和软提示》。*arXiv
    预印本 arXiv:2109.03630*。
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong 等。2023年。《大型语言模型调查》。*arXiv
    预印本 arXiv:2303.18223*。
- en: 'Zheng et al. (2021) Bo Zheng, Li Dong, Shaohan Huang, Wenhui Wang, Zewen Chi,
    Saksham Singhal, Wanxiang Che, Ting Liu, Xia Song, and Furu Wei. 2021. [Consistency
    regularization for cross-lingual fine-tuning](https://doi.org/10.18653/v1/2021.acl-long.264).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing
    (Volume 1: Long Papers)*, pages 3403–3417, Online. Association for Computational
    Linguistics.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人（2021）博·郑、李东、邵汉·黄、温辉·王、泽文·池、萨克什姆·辛格哈尔、万象·车、婷·刘、夏·宋、傅如·魏。2021年。[跨语言微调的一致性正则化](https://doi.org/10.18653/v1/2021.acl-long.264)。发表于*第59届计算语言学协会年会暨第11届国际自然语言处理联合会议（第1卷：长篇论文）*，第3403–3417页，在线。计算语言学协会。
- en: Appendix A Appendix
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: Algorithm 1 Bootstrap Training Data
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 自举训练数据
- en: 1:function BootstrapData($LLM,$12:     end for13:     for $q$)21:         $score\leftarrow$27:end function
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '1:function BootstrapData($LLM,$12: end for13: for $q$)21: $score\leftarrow$27:end
    function'
