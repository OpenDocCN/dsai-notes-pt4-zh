- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:50'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03624](https://ar5iv.labs.arxiv.org/html/2407.03624)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Dharunish Yugeswardeenoo           Kevin Zhu           Sean O’Brien
  prefs: []
  type: TYPE_NORMAL
- en: Algoverse AI Research
  prefs: []
  type: TYPE_NORMAL
- en: dharyugi@gmail.com, kevin@algoverseacademy.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Although LLMs have the potential to transform many fields, they still underperform
    humans in reasoning tasks. Existing methods induce the model to produce step-by-step
    calculations, but this research explores the question: Does making the LLM analyze
    the question improve its performance? We propose a novel prompting strategy called
    Question Analysis Prompting (QAP), in which the model is prompted to explain the
    question in n words before solving. The value of n influences the length of response
    generated by the model. QAP is evaluated on GPT 3.5 Turbo and GPT 4 Turbo on arithmetic
    datasets GSM8K, AQuA, and SAT and commonsense dataset StrategyQA. QAP is compared
    with other state-of- the-art prompts including Chain-of-Thought (CoT), Plan and
    Solve Prompting (PS+) and Take A Deep Breath (TADB). QAP outperforms all state-of-the-art
    prompts on AQuA and SAT datasets on both GPT3.5 and GPT4\. QAP consistently ranks
    among the top-2 prompts on 75% of the tests. A key factor of QAP performance can
    be attributed to response length, where detailed responses are beneficial when
    answering harder questions, but can negatively affect easy questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks
  prefs: []
  type: TYPE_NORMAL
- en: Dharunish Yugeswardeenoo           Kevin Zhu           Sean O’Brien Algoverse
    AI Research dharyugi@gmail.com, kevin@algoverseacademy.com
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) have recently shown rapid improvement across a
    host of standard natural language processing (NLP) tasks, including arithmetic,
    commonsense and symbolic reasoning. (Brown et al., [2020](#bib.bib1)) Although
    these models show improved ability to understand and generate text (OpenAI, [2023](#bib.bib8)),
    their performance can still be further improved. One solution is to encourage
    the model to think step-by-step. Using chain-of-thought prompting (Wei et al.,
    [2022](#bib.bib12)), LLMs are given Q&A exemplars which are designed to elicit
    a structured step-by-step response from the model. Many newly developed strategies
    meant to improve LLM performance have been focused on sophisticating the model’s
    step-by-step calculation (Gu et al., [2023](#bib.bib4)). Despite SoTA prompts’
    remarkable success across various tasks, their accuracies can still be further
    improved. In this work, we explore ways to improve the model reasoning not only
    in the answer steps, but also how the model interprets the question itself. By
    making the model to explicitly interpret the question, we maximize its understanding
    of the question and minimize missed key information. This paper introduces Question-Analysis
    Prompting (QAP), a simple zero-shot prompting strategy that induces the model
    to first explain the question before solving. This method is adaptable to various
    problem difficulties and shows promising results in math and commonsense reasoning
    across different model sizes.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Prompt Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The key principle behind QAP is that the model should reiterate the problem
    before solving. Another principle is that we should be able to control how much
    the model explains so that we can adapt the prompt to different model sizes and
    problem complexities. The specific prompt used is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Explain this problem to me in at least $n$ words. Then solve for the answer."'
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we experiment with n = 25, 50, 100, 150, 200. The versions of
    these prompts are named QAP$n$ in the Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Prompt Impact
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Figure [1](#S3.F1 "Figure 1 ‣ 3 Prompt Impact ‣ Question-Analysis Prompting
    Improves LLM Performance in Reasoning Tasks"), we highlight the structure of a
    standard QAP output. First, the model breaks down the question in its own words
    and provides in-depth analysis on each event. We notice a direct relationship
    between the explanation and the answer steps. Each calculation is previously mentioned
    in the explanation portion, and this proves that the explanation has allowed the
    model to plan its approach even before solving. As a result, there is a significant
    increase in step-by-step calculation and a decreased chance of missed steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc44ffd78aa361b1f41054907a31fedb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Example of QAP prompting - shows how the prompt triggers explanation
    of the question followed by an approach to solve the problem, detailed steps,
    finally leading to correct answer'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Benchmarks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate the effectiveness of QAP on three arithmetic reasoning datasets.
    These include grade-school math questions from GSM8K (Cobbe et al., [2021](#bib.bib2)),
    algebraic word problems from AQuA (Ling et al., [2017](#bib.bib6)), and SAT math
    problems from AGIEval (Zhong et al., [2023](#bib.bib14)). For commonsense reasoning,
    we evaluate on open-domain questions that require implicit reasoning, from StrategyQA
    (Geva et al., [2021](#bib.bib3)). We evaluate on the test sets of all benchmarks,
    as some proprietary models are partially trained on the training set of such tasks.
    (OpenAI, [2023](#bib.bib8))
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We specifically choose our models to observe the prompts’ impacts across differences
    in model size. The smaller model is GPT3.5 Turbo with version `gpt-3.5-turbo-0613`.
    Our larger model is GPT4 Turbo with version `gpt-4-1106-preview` (OpenAI, [2023](#bib.bib8)).
    For both of the models we used the OpenAI API ¹¹1[https://platform.openai.com/docs/api-reference/chat](https://platform.openai.com/docs/api-reference/chat)
    for running our experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For all datasets and models, we experiment with different variations of QAP.
    We utilize QAP25, QAP50, QAP100, QAP150, and QAP200. We compare the performance
    of QAP with the baseline (no prompt). Additionally we compare QAP with two different
    zero-shot prompts, TADB - "Take a deep breath and work on this problem step-by-step"
    (Yang et al., [2023](#bib.bib13)) and PS+ (Plan and Solve Plus) (Wang et al.,
    [2023](#bib.bib10)). Finally we also compare QAP with 8-shot chain-of-thought
    prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The results for GPT-3.5 and GPT-4 Turbo are shown in Table  [Table 1](#S4.T1
    "Table 1 ‣ 4.4 Results ‣ 4 Experimental Setup ‣ Question-Analysis Prompting Improves
    LLM Performance in Reasoning Tasks") and Table  [Table 2](#S4.T2 "Table 2 ‣ 4.4
    Results ‣ 4 Experimental Setup ‣ Question-Analysis Prompting Improves LLM Performance
    in Reasoning Tasks") respectively. General word counts are shown in Figure [7](#A1.F7
    "Figure 7 ‣ A.6 Word Counts for all datasets with GPT 3.5 and GPT 4 ‣ Appendix
    A Appendix ‣ Question-Analysis Prompting Improves LLM Performance in Reasoning
    Tasks").
  prefs: []
  type: TYPE_NORMAL
- en: 'Arithmetic Reasoning: On GPT 3.5 Turbo, a variant of QAP is the top performer
    in 2 out of 3 arithmetic tasks. QAP shows significant gains on AQuA and SAT. With
    GPT-4 Turbo, QAP performs the best in the same 2 out of 3 arithmetic tasks. This
    suggests that QAP may be more beneficial on questions involving algebraic and
    higher-level problem solving; additionally, GPT-4 is trained at least in part
    on GSM8K (OpenAI, [2023](#bib.bib8)) and thus its performance may be less sensitive
    to prompting changes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | GSM8K | AQuA | SAT | StratQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 78.7 | 52.8 | 70.9 | 65.1 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP25 | 67.1 | 39.4 | 35.0 | 63.1 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP50 | 77.8 | 50.0 | 52.7 | 61.4 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP100 | 77.4 | 53.9 | 75.0 | 57.1 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP150 | 78.5 | 59.4 | 78.6 | 53.2 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP200 | 76.8 | 52.4 | 75.0 | 51.8 |'
  prefs: []
  type: TYPE_TB
- en: '| TADB | 78.5 | 57.1 | 74.5 | 62.9 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 79.0 | 53.1 | 65.9 | 59.2 |'
  prefs: []
  type: TYPE_TB
- en: '| PS+ | 74.7 | 35.0 | 70.9 | 35.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Results for GPT-3.5 Turbo (highest scores bolded)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | GSM8K | AQuA | SAT | StratQA |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Baseline | 95.3 | 78.7 | 96.8 | 76.3 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP25 | 94.8 | 77.6 | 94.5 | 77.6 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP50 | 93.4 | 79.1 | 95.9 | 76.9 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP100 | 94.6 | 75.6 | 96.8 | 77.2 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP150 | 94.7 | 78.0 | 97.3 | 77.6 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP200 | 95.0 | 76.4 | 98.2 | 75.9 |'
  prefs: []
  type: TYPE_TB
- en: '| TADB | 95.1 | 78.7 | 96.8 | 78.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 95.6 | 74.4 | 95.0 | 75.1 |'
  prefs: []
  type: TYPE_TB
- en: '| PS+ | 94.8 | 52.8 | 97.3 | 77.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Results for GPT 4 Turbo. (highest scores bolded)'
  prefs: []
  type: TYPE_NORMAL
- en: Commonsense Reasoning:. On StrategyQA, QAP consistently performs second-best
    when compared to other prompts. On both models, QAP25 is the highest QAP performer.
    This suggests that fewer-word explanations benefit commonsense reasoning. This
    is because too much explanation can cause the model to confuse a simple answer
    [6](#A1.F6 "Figure 6 ‣ A.5 Large value of n for simple problems hurts the performance
    ‣ Appendix A Appendix ‣ Question-Analysis Prompting Improves LLM Performance in
    Reasoning Tasks") While there is a decline in performance as $n$ increases on
    the 3.5 model, the larger GPT-4 Turbo model yields similar performances across
    all QAP variants.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Question Difficulties Based On Baseline Performance: Within a given dataset,
    the difficulty of the individual question may vary. We propose a method to measure
    question difficulty based on performance with the baseline prompt. If the model
    can answer the problem correctly with the baseline prompt, then we consider the
    question to be easy; otherwise the question is hard. We analyze the performance
    of different prompts across “easy” and “hard” questions. [Table 3](#A1.T3 "Table
    3 ‣ A.1 Analysis of Accuracy Based On Question Difficulty ‣ Appendix A Appendix
    ‣ Question-Analysis Prompting Improves LLM Performance in Reasoning Tasks") and
    [Table 4](#A1.T4 "Table 4 ‣ A.1 Analysis of Accuracy Based On Question Difficulty
    ‣ Appendix A Appendix ‣ Question-Analysis Prompting Improves LLM Performance in
    Reasoning Tasks") shows that QAP consistently outperforms other prompts in the
    “hard” category.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Impact Of Word Counts On Question Difficulties: QAP generates higher word counts
    for both “easy" and “hard" questions ( [Table 5](#A1.T5 "Table 5 ‣ A.2 Analysis
    of Word Count based on Question Difficulty ‣ Appendix A Appendix ‣ Question-Analysis
    Prompting Improves LLM Performance in Reasoning Tasks") and  [Table 6](#A1.T6
    "Table 6 ‣ A.2 Analysis of Word Count based on Question Difficulty ‣ Appendix
    A Appendix ‣ Question-Analysis Prompting Improves LLM Performance in Reasoning
    Tasks") ), despite performing lower on “easy” questions. Although more step-by-step
    thought processes are encouraged to avoid mistakes during reasoning, this suggests
    that over-explanation can negatively impact the mode (also shown in Figure  [Figure 5](#A1.F5
    "Figure 5 ‣ A.5 Large value of n for simple problems hurts the performance ‣ Appendix
    A Appendix ‣ Question-Analysis Prompting Improves LLM Performance in Reasoning
    Tasks")). Thus, the most suitable word count to solve a problem will vary from
    task to task; longer explanations are best suited to more complicated questions
    for which baseline prompting fails.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Downsides Of Smaller QAPs: Despite high performance on StrategyQA, QAP25 performs
    poorly on arithmetic datasets (mostly SAT and AQuA) using GPT-3.5 Turbo. Due to
    a small value of n, the model outputs are unfinished responses (i.e. the model
    stops midway through its reasoning steps) (shown in Figure [8](#A1.F8 "Figure
    8 ‣ A.7 QAP25 Unfinished Response ‣ Appendix A Appendix ‣ Question-Analysis Prompting
    Improves LLM Performance in Reasoning Tasks")) On SAT math, 51% of responses were
    incomplete for QAP25. On AQuA, 19% of responses were incomplete for QAP25.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b072caec7d73cf22e863d281e7c6b1fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: We consider difficulty of the problem based on baseline’s results.
    E.g., an incorrect answer is “hard” and a correct answer is “easy”. Left chart
    shows accuracy within each difficulty. Right chart shows mean (average) word count
    for within each difficulty. All results for each prompt are shown in Table: [6](#A1.T6
    "Table 6 ‣ A.2 Analysis of Word Count based on Question Difficulty ‣ Appendix
    A Appendix ‣ Question-Analysis Prompting Improves LLM Performance in Reasoning
    Tasks") and Table:[4](#A1.T4 "Table 4 ‣ A.1 Analysis of Accuracy Based On Question
    Difficulty ‣ Appendix A Appendix ‣ Question-Analysis Prompting Improves LLM Performance
    in Reasoning Tasks")'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Additional Studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Placement of the prompt: In this evaluation, we studied the impact of prompt
    placement on performance using GSM8K dataset. Two options for prompt placement
    were considered, Q_Begin - adding the prompt before the question and Q_End - adding
    the prompt after the question. Both placements provided similar results on GPT-3.5
    and GPT-4\. Results shown in the rest of the paper are based on Q_End.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Two-stage QAP: In this approach, we performed the prompting in two stages.
    In the first stage the model is prompted with, “Explain this problem to me in
    at least 50 words WITHOUT SOLVING.” In the second stage, the model is prompted
    again with the question and the explanation from the first stage. On GSM8K and
    AQuA, the model not only explained the problem, but also outlined steps needed
    to solve it. However, the accuracy was almost 50% worse than single stage prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In one-shot and few-shot prompting, the model is given one or more input/output
    examples which will serve as a demonstration for it to solve the problem using
    in-context learning (Mahabadi et al., [2022](#bib.bib7)). QAP is a zero-shot prompt.
    In zero-shot prompting the model does not receive exemplars, but is given a specially
    crafted instruction on how to approach the task (Kojima et al., [2022](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Chain of Thought: Chain-of-thought reasoning is a notable few-shot (zero-shot
    also exists (Yang et al., [2023](#bib.bib13)) example in which the model is shown
    how to express its reasoning steps (Wei et al., [2022](#bib.bib12)). This approach
    was highly effective as the model would replicate these exemplars, and their accuracies
    improved drastically. CoT encouraged the model to think step-by-step, and this
    concept would be repeating theme among other zero-shot counterparts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TADB: Among different variants of Zero-Shot CoT, the TADB prompt (Yang et al.,
    [2023](#bib.bib13)) was derived using an optimization objective to find instructions
    that would maximize task accuracy. The eventual prompt was "Take a deep breath,
    and work on this problem step by step". TADB is an example of how the wording
    of a prompt can drastically impact responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plan and Solve Prompting Plus: Another zero-shot prompt is Plan-and-Solve Prompting
    (Wang et al., [2022](#bib.bib11)). There were two versions to this prompt. The
    first simply asked the model devise a plan and solve step-by-step. The second
    version (PS+) extended the prompt by specifically asking the prompt to extract
    relevant variables and their corresponding numerals and to calculate intermediate
    results. We used PS+ on our experiments. One difference between PS+ and QAP is
    that PS+ prompt is more specific to math datasets - as it instructs to extract
    variables, intermediate results etc, whereas QAP is more general. Also, PS+ prompts
    the model to understand the problem, but it is not clear if model should output
    anything specific to the question itself, but QAP explicitly instructs the model
    to explain the problem in n words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question Decomposition: Question Decomposition (Radhakrishnan et al., [2023](#bib.bib9))
    strategy causes the model to break down the question by creating sub-questions.
    The model answers each of these sub-questions and it ties together all the sub-answers
    into a final answer. It considers two methods for decomposition, Factored Decomposition
    and CoT Decomposition. In factored decomposition each sub-question is answered
    in a separate context. CoT decomposition is an intermediate between factored decomposition
    and CoT. It enforces one context for sub-question, sub-answer and the answer to
    the original question. The analysis of question decomposition shows reduced bias
    and ignored reasoning, improves the faithfulness of a model-generated reasoning
    over CoT while retaining the performance gains of CoT.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we explored the approach of question analysis prompting to improve
    LLM accuracy across math and commonsense reasoning. The ability of this prompting
    method to perform well in diverse model types and tasks difficulty and type of
    tasks seems promising. To our best understanding, QAP is the first zero-shot prompt
    to introduce adaptability with a configurable parameter. We plan to extend this
    work further by combining QAP with other prompt strategies,  applying decoding
    strategies and evaluating multi-modal tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a few limitations of QAP. First, LLMs are sensitive to the prompt’s
    word choice, particularly for zero-shot prompts. As a result so small changes
    to the prompt wording can impact the model’s performance. For example, the current
    QAP prompt asks the model to "solve" for the answer. While this works well for
    math tasks, it may not be optimal for commonsense tasks. Secondly, the results
    in this paper are based on four datasets and a single class of aligned models;
    further results should evaluate on more diverse and multi-modal datasets, as well
    as a greater variety of models. Finally, more robust methods (e.g., based on a
    classifier) to determine the choice of the parameter $n$ should be investigated
    to go beyond manual selection.
  prefs: []
  type: TYPE_NORMAL
- en: 10 Ethics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We experimented on three arithmetic datasets: GSM8K (Cobbe et al., [2021](#bib.bib2)),
    AQuA (Ling et al., [2017](#bib.bib6)), and AGIEval SAT Math (Zhong et al., [2023](#bib.bib14)).
    For commonsense reasoning, used StrategyQA (Geva et al., [2021](#bib.bib3)). GSM8K
    use the MIT License code, while AQUA and StrategyQA use the Apache-2.0 code. QAP
    and the prompts used in this work do not jeopardize the safety of others. They
    do not include any wording which may deem offensive to any individual or group.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, et al. 2021. Training verifiers to solve math word problems. *arXiv preprint
    arXiv:2110.14168*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geva et al. (2021) Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth,
    and Jonathan Berant. 2021. Did aristotle use a laptop? a question answering benchmark
    with implicit reasoning strategies. *Transactions of the Association for Computational
    Linguistics*, 9:346–361.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2023) Jindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He,
    Gengyuan Zhang, Ruotong Liao, Yao Qin, Volker Tresp, and Philip Torr. 2023. A
    systematic survey of prompt engineering on vision-language foundation models.
    *arXiv preprint arXiv:2307.12980*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners.
    *Advances in neural information processing systems*, 35:22199–22213.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. (2017) Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom.
    2017. Program induction by rationale generation: Learning to solve and explain
    algebraic word problems. *arXiv preprint arXiv:1705.04146*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mahabadi et al. (2022) Rabeeh Karimi Mahabadi, Luke Zettlemoyer, James Henderson,
    Marzieh Saeidi, Lambert Mathias, Veselin Stoyanov, and Majid Yazdani. 2022. Perfect:
    Prompt-free and efficient few-shot learning with language models. *arXiv preprint
    arXiv:2204.01172*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://api.semanticscholar.org/CorpusID:257532815).
    *ArXiv*, abs/2303.08774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radhakrishnan et al. (2023) Ansh Radhakrishnan, Karina Nguyen, Anna Chen, Carol
    Chen, Carson E. Denison, Danny Hernandez, Esin Durmus, Evan Hubinger, John Kernion,
    Kamil.e Lukovsiut.e, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver
    Rausch, Sam McCandlish, Sheer El Showk, Tamera Lanham, Tim Maxwell, Venkat Chandrasekaran,
    Zac Hatfield-Dodds, Jared Kaplan, Janina Brauner, Sam Bowman, and Ethan Perez.
    2023. [Question decomposition improves the faithfulness of model-generated reasoning](https://api.semanticscholar.org/CorpusID:259980634).
    *ArXiv*, abs/2307.11768.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Lei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi Lan,
    Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-and-solve prompting: Improving zero-shot
    chain-of-thought reasoning by large language models. *arXiv preprint arXiv:2305.04091*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V
    Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. *arXiv
    preprint arXiv:2309.03409*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhong et al. (2023) Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai
    Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric
    benchmark for evaluating foundation models. *arXiv preprint arXiv:2304.06364*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Analysis of Accuracy Based On Question Difficulty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Performance of prompts on problems categorized into easy and hard - where easy
    problems are those where baseline prompt leads to a correct answer and hard problems
    are those where baseline prompt leads to a wrong answer. For each category the
    % of correct answers are calculated by number of correct answers(per prompt) over
    the total number of problems in that category (easy or hard)
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Easy | Hard |'
  prefs: []
  type: TYPE_TB
- en: '| QAP25 | 84.7 | 30.1 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP50 | 90.0 | 36.7 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP100 | 91.5 | 39.5 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP150 | 92.3 | 43.2 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP200 | 91.1 | 41.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TADB | 93.6 | 34.9 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 92.6 | 35.0 |'
  prefs: []
  type: TYPE_TB
- en: '| PS+ | 88.2 | 31.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Accuracy for Arithmetic Reasoning'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Easy | Hard |'
  prefs: []
  type: TYPE_TB
- en: '| QAP25 | 89.5 | 24.3 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP50 | 87.7 | 24.6 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP100 | 83.8 | 26.9 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP150 | 81.4 | 27.0 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP200 | 80.0 | 25.0 |'
  prefs: []
  type: TYPE_TB
- en: '| TADB | 91.3 | 20.3 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 85.8 | 27.3 |'
  prefs: []
  type: TYPE_TB
- en: '| PS+ | 70.6 | 21.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Accuracy for Commonsense Reasoning'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Analysis of Word Count based on Question Difficulty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Median word count generated by various prompts on all datasets and models categorized
    into easy and hard - where easy problems are those where baseline prompt leads
    to a correct answer and hard problems are those where baseline prompt leads to
    a wrong answer.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Easy | Hard |'
  prefs: []
  type: TYPE_TB
- en: '| QAP25 | 94.6 | 126.7 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP50 | 123.6 | 158.5 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP100 | 200.4 | 229.6 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP150 | 224.4 | 257.9 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP200 | 270.0 | 301.0 |'
  prefs: []
  type: TYPE_TB
- en: '| TADB | 146.3 | 214.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 99.4 | 128.3 |'
  prefs: []
  type: TYPE_TB
- en: '| PS+ | 197.8 | 216.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Mean word count for Arithmetic Reasoning'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Easy | Hard |'
  prefs: []
  type: TYPE_TB
- en: '| QAP25 | 36.9 | 38.7 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP50 | 71.5 | 73.8 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP100 | 183.8 | 192.3 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP150 | 215.8 | 220.4 |'
  prefs: []
  type: TYPE_TB
- en: '| QAP200 | 268.8 | 274.6 |'
  prefs: []
  type: TYPE_TB
- en: '| TADB | 37.5 | 58.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 29.1 | 30.9 |'
  prefs: []
  type: TYPE_TB
- en: '| PS+ | 162.4 | 179.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Mean word count for Commonsense Reasoning'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Example Explanations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e721b679fdde5379a2fcdf271201769.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Examples of QAP inducing explanations of the question on GSM8K, AQuA,
    and StrategyQA. The prompts include QAP50, QAP150, QAP50 respectively. Pink highlights
    key phrases (math reasoning) and orange highloghts represents useful background
    information (commonsense reasoning).'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Impact of Changing n
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d5246800fa42cb8b35f91b0f589b368.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: This comparison shows how responses vary when changing n. This is
    only the answer portion. This was experimented on QAP50 and QAP20 on GSM8K on
    AQuA. Blue represents a QAP200 section which provides more detail than QAP100’s
    (Red) response on the same step. Green represents a section that QAP200 had that
    QAP100 did not have at all.'
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Large value of n for simple problems hurts the performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e4aa52a25bdfaad2e5093ecb6bb4b6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Example in which over-explanation can negatively impact a response.
    QAP50 acquires the correct answer (34), but QAP200 does not. In fact, QAP200 reaches
    the correct answer, but additional explanation leads to a wrong answer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/35d822d628f82831078a76433a44fd06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Example in which over-explanation negatively impacts a commonsense
    reasoning response. The comparison shows that more words can confuse the model.'
  prefs: []
  type: TYPE_NORMAL
- en: A.6 Word Counts for all datasets with GPT 3.5 and GPT 4
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f49cc8e30c0a08569cf7a598e46eb8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Median word counts in response for all datasets using GPT 3.5 Turbo
    and GPT 4 Turbo'
  prefs: []
  type: TYPE_NORMAL
- en: A.7 QAP25 Unfinished Response
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1c8df51f7b800ef514cb411059e4cdbb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Example in which QAP25 outputs an unfinished response on the SAT
    dataset.'
  prefs: []
  type: TYPE_NORMAL
