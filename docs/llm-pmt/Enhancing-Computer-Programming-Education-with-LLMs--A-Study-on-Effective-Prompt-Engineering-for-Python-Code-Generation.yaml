- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:41:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:41:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Enhancing Computer Programming Education with LLMs: A Study on Effective Prompt
    Engineering for Python Code Generation'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用大型语言模型提升计算机编程教育：关于 Python 代码生成的有效提示工程研究
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.05437](https://ar5iv.labs.arxiv.org/html/2407.05437)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.05437](https://ar5iv.labs.arxiv.org/html/2407.05437)
- en: '[![[Uncaptioned image]](img/5c894e0c749a495fb3e0a26edcb02627.png) Tianyu Wang](https://orcid.org/0000-0002-9244-8798)'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[![[无标题图像]](img/5c894e0c749a495fb3e0a26edcb02627.png) 王天宇](https://orcid.org/0000-0002-9244-8798)'
- en: Mercy University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 梅西大学
- en: Math & Computer Science Department
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数学与计算机科学系
- en: 555 Broadway,
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 555 百老汇，
- en: Dobbs Ferry, NY 10522, USA
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 多布斯费里，纽约 10522，美国
- en: 'twang4@mercy.edu These authors contributed equally to this work.    [![[Uncaptioned
    image]](img/5c894e0c749a495fb3e0a26edcb02627.png) Nianjun Zhou](https://orcid.org/0000-0002-3473-6097)¹¹footnotemark:
    1'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: twang4@mercy.edu 这些作者对本工作贡献相同。    [![[无标题图像]](img/5c894e0c749a495fb3e0a26edcb02627.png) 周念军](https://orcid.org/0000-0002-3473-6097)¹¹脚注标记：1
- en: IBM Research
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: IBM 研究
- en: 1101 Kitchawan Road, Route 134
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 1101 基查湾路，134号公路
- en: Yorktown Heights, NY 10598, USA
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 约克城高地，纽约 10598，美国
- en: jzhou@us.ibm.com    [![[Uncaptioned image]](img/5c894e0c749a495fb3e0a26edcb02627.png) Zhixiong
    Chen](https://orcid.org/0000-0002-9874-6972)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: jzhou@us.ibm.com    [![[无标题图像]](img/5c894e0c749a495fb3e0a26edcb02627.png) 陈志雄](https://orcid.org/0000-0002-9874-6972)
- en: Mercy University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 梅西大学
- en: Math & Computer Science Department
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 数学与计算机科学系
- en: 555 Broadway,
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 555 百老汇，
- en: Dobbs Ferry, NY 10522, USA
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 多布斯费里，纽约 10522，美国
- en: zchen@mercy.edu
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: zchen@mercy.edu
- en: Abstract
  id: totrans-21
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) and prompt engineering hold significant potential
    for advancing computer programming education through personalized instruction.
    This paper explores this potential by investigating three critical research questions:
    the systematic categorization of prompt engineering strategies tailored to diverse
    educational needs, the empowerment of LLMs to solve complex problems beyond their
    inherent capabilities, and the establishment of a robust framework for evaluating
    and implementing these strategies. Our methodology involves categorizing programming
    questions based on educational requirements, applying various prompt engineering
    strategies, and assessing the effectiveness of LLM-generated responses. Experiments
    with GPT-4, GPT-4o, Llama3-8b, and Mixtral-8x7b models on datasets such as LeetCode
    and USACO reveal that GPT-4o consistently outperforms others, particularly with
    the "multi-step" prompt strategy. The results show that tailored prompt strategies
    significantly enhance LLM performance, with specific strategies recommended for
    foundational learning, competition preparation, and advanced problem-solving.
    This study underscores the crucial role of prompt engineering in maximizing the
    educational benefits of LLMs. By systematically categorizing and testing these
    strategies, we provide a comprehensive framework for both educators and students
    to optimize LLM-based learning experiences. Future research should focus on refining
    these strategies and addressing current LLM limitations to further enhance educational
    outcomes in computer programming instruction.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）和提示工程在通过个性化指导提升计算机编程教育方面具有重要潜力。本文通过研究三个关键的研究问题来探索这种潜力：针对不同教育需求的提示工程策略的系统分类、赋能LLMs以解决超出其固有能力的复杂问题，以及建立评估和实施这些策略的稳健框架。我们的方法论包括根据教育需求对编程问题进行分类，应用各种提示工程策略，并评估LLM生成的响应的有效性。对
    GPT-4、GPT-4o、Llama3-8b 和 Mixtral-8x7b 模型在 LeetCode 和 USACO 数据集上的实验表明，GPT-4o 一贯优于其他模型，特别是在“多步骤”提示策略下。结果显示，量身定制的提示策略显著提高了LLM的表现，针对基础学习、竞赛准备和高级问题解决推荐了具体策略。本研究强调了提示工程在最大化LLM教育效益中的关键作用。通过系统分类和测试这些策略，我们为教育者和学生提供了一个全面的框架，以优化基于LLM的学习体验。未来的研究应集中在完善这些策略和解决当前LLM的局限性上，以进一步提高计算机编程教育的成果。
- en: '*Keywords* Prompt Engineering, Large Language Models (LLMs), Computer Programming,
    Code Quality Evaluation, Education'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键词* 提示工程、大型语言模型（LLMs）、计算机编程、代码质量评估、教育'
- en: 1 Introduction
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/cb35ed36d812a81fd8ea96cdb185d911.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题说明](img/cb35ed36d812a81fd8ea96cdb185d911.png)'
- en: 'Figure 1: Flowchart illustrating the users in code generation and evaluation
    process'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示代码生成和评估过程中的用户流程图
- en: 'In the rapidly evolving field of Artificial Intelligence (AI), Large Language
    Models (LLMs) (Bommasani et al., [2021](#bib.bib1); Touvron et al., [2023](#bib.bib2)),
    a breakthrough in AI, have shown remarkable potential in a variety of applications,
    including natural language processing, content creation, and more recently, in
    code generation. Numerous LLM models, such as ChatGPT and LLaM, have shown their
    capability to cater to a variety of task domains, as Fig [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Enhancing Computer Programming Education with LLMs: A Study
    on Effective Prompt Engineering for Python Code Generation") shown, ranging from
    question answering to the generation of code snippets, (Radford et al., [2019](#bib.bib3);
    Bommasani et al., [2021](#bib.bib1); Ouyang et al., [2022](#bib.bib4); Liu et al.,
    [2023a](#bib.bib5); Touvron et al., [2023](#bib.bib2)). Furthermore, in the evolving
    landscape of computer science education, integrating advanced technologies has
    become increasingly pivotal. Many Integrated Development Environments (IDEs) have
    equipped with LLMs to generate code in software development tools, such as VS
    Code¹¹1https://code.visualstudio.com/. As the demand for proficient programmers
    grows, so does the necessity for innovative and effective teaching methods. These
    usages of tools draw attention to developing software vulnerabilities and security
    concerns (Asare et al., [2023](#bib.bib6); Pearce et al., [2022a](#bib.bib7);
    Dakhel et al., [2023a](#bib.bib8)).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '在快速发展的人工智能（AI）领域中，大型语言模型（LLMs）（Bommasani et al., [2021](#bib.bib1); Touvron
    et al., [2023](#bib.bib2)）作为人工智能的一项突破，展示了在各种应用中的显著潜力，包括自然语言处理、内容创作，以及最近的代码生成。许多LLM模型，如ChatGPT和LLaM，已显示出它们在各种任务领域的能力，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Enhancing Computer Programming Education
    with LLMs: A Study on Effective Prompt Engineering for Python Code Generation")
    所示，从问答到代码片段生成（Radford et al., [2019](#bib.bib3); Bommasani et al., [2021](#bib.bib1);
    Ouyang et al., [2022](#bib.bib4); Liu et al., [2023a](#bib.bib5); Touvron et al.,
    [2023](#bib.bib2)）。此外，在计算机科学教育不断发展的背景下，整合先进技术变得越来越重要。许多集成开发环境（IDEs）已配备LLMs，用于在软件开发工具中生成代码，例如VS
    Code¹¹1https://code.visualstudio.com/。随着对熟练程序员的需求增加，创新和有效的教学方法的必要性也在增加。这些工具的使用引起了对软件漏洞和安全问题的关注（Asare
    et al., [2023](#bib.bib6); Pearce et al., [2022a](#bib.bib7); Dakhel et al., [2023a](#bib.bib8)）。'
- en: This paper presents significant contributions to the field of AI-assisted programming
    education by focusing on the optimization of Python code generation with LLMs
    for educational applications. Our research addresses critical aspects of how LLMs
    can be effectively utilized to create personalized and adaptive learning environments
    that cater to diverse educational needs. We systematically investigate and categorize
    prompt engineering strategies, tailoring them to specific educational objectives
    and problem types. By doing so, we enable educators and students to leverage LLMs
    in a way that maximizes their problem-solving potential and instructional effectiveness.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过关注优化Python代码生成的LLMs在教育应用中的使用，对AI辅助编程教育领域做出了重要贡献。我们的研究解决了LLMs如何有效用于创建个性化和适应性学习环境，以满足多样化教育需求的关键方面。我们系统地调查和分类了提示工程策略，将其量身定制以适应特定教育目标和问题类型。通过这样做，我们使教育工作者和学生能够以最大化问题解决潜力和教学效果的方式利用LLMs。
- en: 'Our contributions are threefold:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的贡献有三个方面：
- en: '1.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Categorization of prompt engineering strategies. We develop a comprehensive
    categorization of prompt engineering strategies that align with various educational
    requirements, ranging from foundational knowledge and skills to competition-level
    challenges and advanced problem-solving tasks. This categorization provides a
    structured approach for educators to customize prompts and optimize learning pathways
    for their students.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示工程策略的分类。我们制定了一种全面的提示工程策略分类，符合各种教育需求，从基础知识和技能到竞赛级挑战和高级问题解决任务。这一分类为教育者提供了一种结构化的方法，以定制提示和优化学生的学习路径。
- en: '2.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Explore prompt engineering impact on LLM performance in code generation. We
    propose a robust framework designed to explore and validate various prompt engineering
    strategies, assessing their impact on LLM performance in generating Python code.
    Our findings underscore the effectiveness of different prompt strategies in guiding
    students through complex problem-solving processes, thereby enhancing the role
    of LLMs as facilitators in the educational journey.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 探索提示工程对 LLM 代码生成性能的影响。我们提出了一个强大的框架，用于探索和验证各种提示工程策略，评估它们对 LLM 在生成 Python 代码方面的影响。我们的研究结果强调了不同提示策略在引导学生解决复杂问题过程中的有效性，从而增强了
    LLM 作为教育过程促进者的作用。
- en: '3.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Provide a general prompt guidelines for different educational purpose. Following
    the evaluation of prompt engineering strategies using various datasets, we present
    comprehensive guidelines for educators. These guidelines facilitate the systematic
    use of LLMs to generate superior code and optimize LLM-based learning experiences.
    By ensuring the efficient application of diverse prompt strategies, these guidelines
    aim to enhance educational outcomes in computer programming instruction across
    different educational requirements.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为不同教育目的提供一般提示指南。在使用各种数据集评估提示工程策略后，我们为教育工作者提供了全面的指南。这些指南有助于系统地使用 LLM 生成优质代码，并优化基于
    LLM 的学习体验。通过确保多样化提示策略的有效应用，这些指南旨在提升计算机编程教学中的教育成果，以满足不同教育需求。
- en: By advancing the application of LLMs in Python code generation for educational
    purposes, our research contributes to the broader goal of integrating AI technologies
    into educational practices, paving the way for more dynamic, personalized, and
    effective learning experiences in the field of computer programming.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过推进 LLM 在 Python 代码生成中的应用，我们的研究致力于将 AI 技术融入教育实践，推动计算机编程领域更加动态、个性化和有效的学习体验。
- en: 2 Related Work
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Recent research in the educational sector has prominently featured the application
    of Large Language Models (LLMs) to enhance learning outcomes, particularly in
    the programming domain Denny et al. ([2023a](#bib.bib9)). This review synthesizes
    findings from key studies that illustrate the diverse roles LLMs play in education,
    from interactive assistance in computer science courses to the evaluation of programming
    skills. Murr et al. ([2023](#bib.bib10)) focused on the effectiveness of LLMs
    in generating code, emphasizing the critical role of prompt specificity. Many
    studies have demonstrated the efficacy of integrating AI code generators in introductory
    programming courses, as evidenced by research conducted by Finnie-Ansley et al.
    ([2022](#bib.bib11)), Hellas et al. ([2023](#bib.bib12)), and Kazemitabaar et al.
    ([2023](#bib.bib13)). In addition, Kiesler and Schiffner ([2023](#bib.bib14))
    assessed the capabilities of ChatGPT-3.5 and GPT-4 in solving introductory Python
    programming tasks sourced from CodingBat. Pearce et al. ([2022b](#bib.bib15))
    explored the application of LLMs in reverse engineering tasks and exhibited promising
    results. Supporting discussions on LLMs’ application in programming education,
    particularly with development assistant, additional references such as Asare et al.
    ([2023](#bib.bib6)), Pearce et al. ([2022a](#bib.bib7)), Dakhel et al. ([2023a](#bib.bib8))
    and Denny et al. ([2023b](#bib.bib16)) provide insights into the integration of
    AI tools within software development environments. These studies collectively
    underscore the transformative impact of LLMs on programming education, suggesting
    avenues for future research in optimizing their use for educational enhancement
    and addressing broader software development challenges Chen et al. ([2021](#bib.bib17)).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 近期在教育领域的研究突出地展示了大语言模型（LLMs）在提升学习成果方面的应用，尤其是在编程领域 Denny 等人 ([2023a](#bib.bib9))。本综述综合了关键研究的发现，说明了
    LLMs 在教育中扮演的多种角色，从计算机科学课程中的互动辅助到编程技能的评估。Murr 等人 ([2023](#bib.bib10)) 关注了 LLMs
    在生成代码方面的有效性，强调了提示具体性的重要性。许多研究已经证明，将 AI 代码生成器集成到入门编程课程中的有效性，例如 Finnie-Ansley 等人
    ([2022](#bib.bib11))、Hellas 等人 ([2023](#bib.bib12)) 和 Kazemitabaar 等人 ([2023](#bib.bib13))
    进行的研究。另有 Kiesler 和 Schiffner ([2023](#bib.bib14)) 评估了 ChatGPT-3.5 和 GPT-4 在解决
    CodingBat 源自的入门 Python 编程任务中的能力。Pearce 等人 ([2022b](#bib.bib15)) 探索了 LLMs 在逆向工程任务中的应用，并展示了令人鼓舞的结果。支持
    LLMs 在编程教育中应用，特别是在开发助手方面的讨论，额外参考了 Asare 等人 ([2023](#bib.bib6))、Pearce 等人 ([2022a](#bib.bib7))、Dakhel
    等人 ([2023a](#bib.bib8)) 和 Denny 等人 ([2023b](#bib.bib16))，提供了关于在软件开发环境中集成 AI 工具的见解。这些研究共同强调了
    LLMs 对编程教育的变革性影响，建议了未来优化其用于教育提升的研究方向，并解决更广泛的软件开发挑战 Chen 等人 ([2021](#bib.bib17))。
- en: While LLMs were impressively successful in generating code for different purpose
    in education and production, they stumbled when confronting real-world security
    and risks concerns. Bommasani et al. ([2021](#bib.bib1)) examine the potential
    and risks associated with LLMs and highlights their content creation capabilities
    and warns about potential issues like bias, misinformation, and homogenization.
     Dakhel et al. ([2023b](#bib.bib18)) leveraged LLMs for generating unit tests
    in software development. To overcome the inaccurate response from LLM, a new discipline
    or guideline called ’Prompt Engineering’, which includes specific strategies for
    maximizing the capability of LLM, applies to us with modification (Reynolds and
    McDonell, [2021](#bib.bib19); Liu et al., [2023a](#bib.bib5)). delve into methodologies
    for leveraging the inherent capabilities of narratives and cultural anchors to
    intricately encode intentions and strategies, thereby facilitating a structured
    breakdown of problems into their constituent elements prior to reaching conclusions
    Reynolds and McDonell ([2021](#bib.bib19)). Expanding upon this notion, introduce
    a novel approach termed least-to-most prompting, which systematically deconstructs
    complex issues into manageable sub-problems, addressing them sequentially to enhance
    problem-solving efficiency in LLMs Zhou et al. ([2022](#bib.bib20)). Complementing
    these insights, demonstrate the natural emergence of reasoning capabilities within
    sizable LLMs through what is known as chain-of-thought prompting. This technique
    involves the use of select demonstrations that guide the model through a thought
    process, thereby facilitating the comprehension and solving of tasks Wei et al.
    ([2022](#bib.bib21)). In a similar vein, propose an innovative "Ask Me Anything"
    (AMA) prompting strategy that iteratively employs the LLM itself to reformulate
    task inputs into a more effective question-and-answer format, thereby significantly
    augmenting the performance of LLMs Arora et al. ([2022](#bib.bib22)). Additionally,
    explore the potential of refining language models’ task execution and instruction-following
    capabilities through the integration of human feedback, marking a significant
    step towards more interactive and adaptive LLMs Ouyang et al. ([2022](#bib.bib4)).
    The study by White et al. ([2023a](#bib.bib23)) on prompt pattern catalog to enhance
    prompt engineering with ChatGPT provides a comprehensive overview of best practices
    and patterns in prompt engineering, highlighting its importance in optimizing
    LLM outputs for specific tasks.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大型语言模型（LLMs）在教育和生产中生成代码方面取得了显著成功，但在面对现实世界的安全性和风险问题时却出现了问题。Bommasani等人（[2021](#bib.bib1)）研究了LLMs的潜力和风险，强调了其内容创建能力，并警告了可能出现的偏见、虚假信息和同质化等问题。Dakhel等人（[2023b](#bib.bib18)）利用LLMs在软件开发中生成单元测试。为了克服LLM的不准确响应，一种被称为“Prompt
    Engineering”的新学科或指南出现了，其中包括最大化LLM能力的具体策略（Reynolds和McDonell，[2021](#bib.bib19)；Liu等人，[2023a](#bib.bib5)）。*深入探讨*了利用叙事和文化锚点的固有能力来精细地编码意图和策略的方法，从而在得出结论之前促进问题的结构性拆解（Reynolds和McDonell，[2021](#bib.bib19)）。在此基础上，引入了一种称为“从少到多提示”的新方法，该方法将复杂问题系统地拆解成可管理的子问题，依次解决以提高LLM的解决问题效率（Zhou等人，[2022](#bib.bib20)）。补充这些见解，展示了通过所谓的链式思维提示，LLMs在处理推理能力方面的自然出现。这种技术涉及使用选定的示例来引导模型的思维过程，从而促进任务的理解和解决（Wei等人，[2022](#bib.bib21)）。类似地，提出了一种创新的“问我任何事”（AMA）提示策略，该策略通过迭代使用LLM自身将任务输入重新格式化为更有效的问题和答案格式，从而显著提高LLM的性能（Arora等人，[2022](#bib.bib22)）。此外，探索通过整合人类反馈来改进语言模型的任务执行和指令跟随能力，标志着朝着更具互动性和适应性的LLMs迈出了重要一步（Ouyang等人，[2022](#bib.bib4)）。White等人（[2023a](#bib.bib23)）对提升ChatGPT提示工程的提示模式目录的研究提供了有关提示工程的最佳实践和模式的全面概述，突显了其在优化LLM输出以完成特定任务中的重要性。
- en: The code quality generated by LLMs holds paramount importance in applications
    spanning educational contexts and real-world production environments and many
    code evaluation studies have been introduced. Hendrycks et al. ([2021](#bib.bib24))
    unveiled APPS, a benchmark specifically designed for code generation tasks. This
    benchmark assesses the capability of models to interpret arbitrary natural language
    specifications and produce Python code that meets the specified requirements.
    Furthermore, Chen et al. ([2021](#bib.bib17)) introduced HumanEval, an innovative
    evaluation set aimed at measuring the functional correctness of programs synthesized
    from docstrings. Xu et al. ([2022](#bib.bib25)) conducted a comprehensive assessment
    of the largest code-generating models available, spanning multiple programming
    languages. They introduced a novel model, PolyCoder, which demonstrated superior
    performance in generating C programming code, outperforming its counterparts.
    Liu et al. ([2023b](#bib.bib26)) developed EvalPlus, a comprehensive framework
    for the evaluation of code synthesis. This framework is meticulously designed
    to benchmark the functional correctness of code generated by LLMs with a high
    degree of rigor. White et al. ([2023b](#bib.bib27)) presented a more systematic
    methodology for the cataloging of software engineering patterns. This study classifies
    various patterns and delves into numerous prompt strategies that have been employed
    to enhance code quality and system design. In a comparative study, Murr et al.
    ([2023](#bib.bib10)) analyzed the efficacy of code produced by different LLMs
    across 104 customized Python challenges, utilizing widely recognized metrics such
    as the pass rate for assessment.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由 LLM 生成的代码质量在教育背景和现实生产环境中的应用具有至关重要的意义，许多代码评估研究已被提出。Hendrycks 等人（[2021](#bib.bib24)）推出了
    APPS，这是一个专门为代码生成任务设计的基准。该基准评估模型解释任意自然语言规范并生成符合要求的 Python 代码的能力。此外，Chen 等人（[2021](#bib.bib17)）引入了
    HumanEval，这是一个创新的评估集，旨在测量从文档字符串合成的程序的功能正确性。Xu 等人（[2022](#bib.bib25)）对现有的最大代码生成模型进行了全面评估，涉及多种编程语言。他们推出了一种新模型
    PolyCoder，展示了在生成 C 编程代码方面的优越性能，超越了其他模型。Liu 等人（[2023b](#bib.bib26)）开发了 EvalPlus，这是一个全面的代码合成评估框架。该框架被精心设计以高严格度基准化
    LLM 生成代码的功能正确性。White 等人（[2023b](#bib.bib27)）提出了一种更系统的方法来编目软件工程模式。该研究对各种模式进行了分类，并*深入探讨*了许多用于提升代码质量和系统设计的提示策略。在一项比较研究中，Murr
    等人（[2023](#bib.bib10)）分析了不同 LLM 在 104 个定制 Python 挑战中的代码有效性，利用了诸如通过率等广泛认可的评估指标。
- en: Beyond conventional methodologies, deep learning techniques have increasingly
    been applied to the evaluation of code. Kanade et al. ([2020](#bib.bib28)) explored
    the capabilities of a finely tuned CuBERT model, revealing that it surpasses traditional
    methods in source code evaluation. This advantage was observed even with limited
    training and a smaller number of labeled examples. Ciniselli et al. ([2021](#bib.bib29))
    presented an empirical study employing a RoBERTa model to assess its efficiency
    in code completion tasks from various angles. The findings indicate that BERT-based
    models are a promising avenue for enhancing code completion capabilities. Wang
    and Chen ([2023](#bib.bib30)) proposed an approach for the automatic assessment
    of code quality using a BERT model that has been meticulously fine-tuned with
    specific datasets, offering a groundbreaking perspective on the evaluation of
    code quality.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 超越传统方法，深度学习技术越来越多地应用于代码评估。Kanade 等人（[2020](#bib.bib28)）探索了精细调校的 CuBERT 模型的能力，揭示其在源代码评估中超越了传统方法。即使在有限的训练和较少的标记示例下，这一优势也得到了观察。Ciniselli
    等人（[2021](#bib.bib29)）进行了实证研究，采用 RoBERTa 模型从多个角度评估其在代码补全任务中的效率。研究结果表明，基于 BERT
    的模型是提升代码补全能力的有前景的途径。Wang 和 Chen（[2023](#bib.bib30)）提出了一种自动评估代码质量的方法，该方法使用经过特定数据集精细调校的
    BERT 模型，提供了对代码质量评估的突破性视角。
- en: 3 Problem Statement
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题陈述
- en: 'In computer programming education, LLMs and prompt engineering hold promise
    for personalized instruction. By harnessing LLMs’ capability to comprehend and
    respond to natural language prompts, we can create adaptive learning environments
    that dynamically adjust to a education requirement and problem-solving approach.
    However, a pivotal question persists: can we fully exploit this potential to enhance
    educational outcomes?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机编程教育中，LLMs 和提示工程为个性化教学带来了希望。通过利用 LLMs 理解和回应自然语言提示的能力，我们可以创建动态调整的自适应学习环境，以适应教育需求和问题解决方法。然而，一个关键问题依然存在：我们是否能充分利用这种潜力来提升教育成果？
- en: 'This paper investigates this very issue by focusing on three key research questions:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过关注三个关键研究问题来研究这一问题：
- en: '1.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Can we systematically categorize prompt engineering strategies tailored to various
    educational requirements and question types? This categorization would enable
    educators and students to customize prompts according to specific problem types
    and educational objectives, thereby maximizing the effectiveness of computer programming
    instruction. Understanding these categories is crucial for developing targeted
    and efficient learning pathways.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们能否系统地分类针对各种教育需求和问题类型的提示工程策略？这种分类将使教育者和学生能够根据具体问题类型和教育目标定制提示，从而最大化计算机编程教学的效果。理解这些类别对于开发针对性和高效的学习路径至关重要。
- en: '2.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Do different strategies empower LLMs to address problems beyond their immediate
    solution capabilities? This exploration aims to unlock the potential for LLMs
    to guide students through complex or unfamiliar problem-solving processes. By
    identifying strategies that extend LLM capabilities, we can enhance their role
    as facilitators in the learning journey, providing support even in challenging
    scenarios.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同策略是否能使 LLMs 解决超出其直接解决能力的问题？这项探索旨在解锁 LLMs 引导学生解决复杂或陌生问题的潜力。通过识别能够扩展 LLM 能力的策略，我们可以增强它们在学习旅程中的促进作用，即使在具有挑战性的情境下也能提供支持。
- en: '3.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Can we establish a robust framework for testing the effectiveness of various
    prompt engineering strategies and provide comprehensive guidelines for their implementation?
    This framework would allow educators to systematically evaluate and optimize LLM-based
    learning experiences, ensuring maximum benefit for students. By developing and
    validating these guidelines, we can offer a structured approach to enhance educational
    practices and outcomes through LLMs.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们能否建立一个可靠的框架来测试各种提示工程策略的有效性，并提供实施的综合指南？这个框架将使教育者能够系统地评估和优化基于 LLM 的学习体验，确保学生获得最大利益。通过制定和验证这些指南，我们可以提供一种结构化的方法，通过
    LLMs 提升教育实践和成果。
- en: By addressing these questions, this research aims to pave the way for a future
    of personalized and adaptive coding education, empowering students to approach
    problems with confidence and a deeper understanding of core programming concepts.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过解决这些问题，本研究旨在为个性化和自适应编码教育的未来铺平道路，使学生能够以自信和对核心编程概念的更深入理解来应对问题。
- en: 4 Methodology
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法论
- en: '![Refer to caption](img/80575974757da054396d89addbae434c.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/80575974757da054396d89addbae434c.png)'
- en: 'Figure 2: Conceptual Diagram Highlighting the Interaction Between LLMs and
    Prompt Engineering'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：突出显示 LLMs 与提示工程之间互动的概念图
- en: 'To address our research problem, we designed a comprehensive model, as illustrated
    in Fig [2](#S4.F2 "Figure 2 ‣ 4 Methodology ‣ Enhancing Computer Programming Education
    with LLMs: A Study on Effective Prompt Engineering for Python Code Generation").
    This model outlines the process of categorizing questions, applying various prompt
    engineering strategies, and evaluating the effectiveness of LLM-generated responses.
    The methodology is structured into three primary steps:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决我们的研究问题，我们设计了一个综合模型，如图 [2](#S4.F2 "图 2 ‣ 4 方法论 ‣ 利用 LLMs 提升计算机编程教育：关于 Python
    代码生成的有效提示工程研究") 所示。该模型概述了分类问题、应用各种提示工程策略和评估 LLM 生成的响应有效性的过程。该方法论分为三个主要步骤：
- en: '1.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Categorization of Questions Based on Educational Requirements. The first step
    involves categorizing computer programming questions according to different educational
    requirements. This categorization helps in tailoring prompt engineering strategies
    to the specific needs of learners. We classify the questions into three distinct
    levels:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于教育要求的问题分类。第一步涉及根据不同的教育要求对计算机编程问题进行分类。这种分类有助于根据学习者的具体需求定制提示工程策略。我们将问题分为三个不同的层级：
- en: •
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Knowledge and Skills: This category focuses on fundamental algorithms and data
    structures, preparing students for coding interviews. It aims to build a solid
    foundation in programming by familiarizing students with essential concepts and
    techniques.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识与技能：这一类别侧重于基础算法和数据结构，为学生准备编码面试。它旨在通过使学生熟悉基本概念和技术，建立坚实的编程基础。
- en: •
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Competitions: This category is designed to enhance programming skills for higher-level
    competitions. It includes problems that challenge students to think critically
    and innovatively, helping them gain recognition that can benefit their academic
    and professional careers.'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比赛：这一类别旨在提升编程技能，以便更好地参与高水平的比赛。它包括挑战学生批判性和创新性思维的问题，帮助他们获得对学术和职业生涯有益的认可。
- en: •
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Advanced Complex Problems: This category addresses advanced topics such as
    algorithm design, optimization, and number theory. It is intended for students
    who have a strong grasp of basic concepts and are looking to tackle more sophisticated
    and intricate problems.'
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高级复杂问题：这一类别涉及高级主题，如算法设计、优化和数论。它面向那些对基本概念有扎实掌握、希望解决更复杂和精细问题的学生。
- en: '2.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Exploration of Prompt Engineering Strategies. In the next step, we explore
    different prompt engineering strategies on the categorized questions. There are
    three major prompt engineering strategies that we employ:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示工程策略的探索。在下一步中，我们将在分类问题上探索不同的提示工程策略。我们使用三种主要的提示工程策略：
- en: •
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Without Prompt Engineering: In this approach, we present the original question
    to the LLM without any additional guidance. This strategy tests the LLM’s inherent
    ability to comprehend and solve problems without external aid.'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无提示工程：在这种方法中，我们将原始问题直接呈现给LLM，而不提供额外指导。这一策略测试LLM在没有外部帮助的情况下理解和解决问题的内在能力。
- en: •
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'General Prompt Engineering: This method involves providing some general prompt
    templates to help the LLM understand the problem better. For example, we include
    the requirements of question constraints, test cases, and guidelines to assist
    the LLM in breaking down the problem into logical sub-problems. This approach
    serves as a framework to improve the LLM’s problem-solving capabilities.'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通用提示工程：这一方法包括提供一些通用提示模板，以帮助大语言模型（LLM）更好地理解问题。例如，我们会包含问题约束的要求、测试用例和指导方针，帮助LLM将问题分解为逻辑子问题。这种方法作为框架来提升LLM的解决问题能力。
- en: •
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Specific Prompt Engineering: This strategy provides detailed instructions on
    how to address the problem, rather than using a generic approach. For instance,
    we may instruct the LLM to consider extreme scenarios or treat the problem as
    an optimization task. This method is particularly useful for challenging and complex
    problems that require specialized solutions.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具体提示工程：这一策略提供了详细的说明，指导如何解决问题，而不是使用通用方法。例如，我们可能会指导LLM考虑极端情况或将问题视为优化任务。这种方法特别适用于需要专门解决方案的具有挑战性和复杂的问题。
- en: '3.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Evaluation of LLM Responses. After applying the appropriate prompt engineering
    strategies, we evaluate the LLM-generated responses using different metrics. The
    evaluation process involves:'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLM响应的评估。在应用适当的提示工程策略后，我们使用不同的指标来评估LLM生成的响应。评估过程包括：
- en: •
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Correctness: Assessing the correctness of the generated code. We verify if
    the code meets the problem requirements and constraints.'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正确性：评估生成代码的正确性。我们验证代码是否满足问题的要求和约束。
- en: •
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Effectiveness: Measuring the overall effectiveness of the LLM’s code. This
    includes testing the code against efficiency in terms of time and memory complexity.
    More details about evaluation can be found at Section [5.3](#S5.SS3 "5.3 Evaluation
    Methods ‣ 5 Experiments ‣ Enhancing Computer Programming Education with LLMs:
    A Study on Effective Prompt Engineering for Python Code Generation").'
  id: totrans-77
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 效率：衡量 LLM 代码的整体有效性。这包括测试代码在时间和内存复杂性方面的效率。有关评估的更多细节，请参见第 [5.3](#S5.SS3 "5.3 评估方法
    ‣ 5 实验 ‣ 使用 LLM 增强计算机编程教育：针对 Python 代码生成的有效提示工程研究") 节。
- en: By following this structured methodology, we aim to systematically investigate
    the potential of LLMs and prompt engineering in enhancing computer programming
    education. The insights gained from this research will provide valuable guidelines
    for educators to optimize LLM-based learning experiences for maximum student benefit.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循这一结构化方法论，我们旨在系统地探讨 LLM 和提示工程在增强计算机编程教育方面的潜力。从这项研究中获得的见解将为教育工作者提供有价值的指导，以优化基于
    LLM 的学习体验，最大化学生收益。
- en: 4.1 General Prompt Engineering
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 一般提示工程
- en: 'Table 1: Prompt Configurations for Model'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 模型提示配置'
- en: '| Model | System Prompt | User Prompts and Chains |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 系统提示 | 用户提示和链 |'
- en: '| --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| base | See Appendix [A](#A1 "Appendix A Prompts in This Research ‣ Enhancing
    Computer Programming Education with LLMs: A Study on Effective Prompt Engineering
    for Python Code Generation") | Question &#124; llm -> Code |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 基础 | 见附录 [A](#A1 "附录 A 本研究中的提示 ‣ 使用 LLM 增强计算机编程教育：针对 Python 代码生成的有效提示工程研究")
    | 问题 &#124; llm -> 代码 |'
- en: '| example (1-shot) | Question &#124; llm -> Code &#124; Example -> Code |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 示例（1-shot） | 问题 &#124; llm -> 代码 &#124; 示例 -> 代码 |'
- en: '| dynamic example | Question &#124; llm -> Dynamic Related Example -> Code
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 动态示例 | 问题 &#124; llm -> 动态相关示例 -> 代码 |'
- en: '| guide |  | Question &#124; llm -> Code &#124; General Guide²²2See appendix [B](#A2
    "Appendix B General Guidelines for High-Quality Python Code ‣ Enhancing Computer
    Programming Education with LLMs: A Study on Effective Prompt Engineering for Python
    Code Generation") -> Code |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 指南 |  | 问题 &#124; llm -> 代码 &#124; 一般指南²²2见附录 [B](#A2 "附录 B 高质量 Python 代码的一般指南
    ‣ 使用 LLM 增强计算机编程教育：针对 Python 代码生成的有效提示工程研究") -> 代码 |'
- en: '| multi |  | Question &#124; llm -> multi-step chat guide &#124; llm -> Code
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 多步 |  | 问题 &#124; llm -> 多步骤聊天指南 &#124; llm -> 代码 |'
- en: '| all-in-one |  | Question &#124; All-in-one &#124; llm -> Code |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 一体化 |  | 问题 &#124; 一体化 &#124; llm -> 代码 |'
- en: 'Prompt engineering is crucial in the development and optimization of LLMs due
    to its significant impact on the performance and accuracy of these models. The
    choice of prompts directly influences the output generated by LLMs, as they determine
    how the model interprets and responds to the given input. By experimenting with
    and testing different prompts for the same question, researchers can identify
    the most effective phrasing and structure, thereby enhancing the model’s ability
    to generate relevant, coherent, and contextually appropriate responses. This iterative
    process of prompt refinement ensures that the LLMs are not only robust but also
    adaptable to a wide range of applications, thereby maximizing their utility in
    various domains. As Table [1](#S4.T1 "Table 1 ‣ 4.1 General Prompt Engineering
    ‣ 4 Methodology ‣ Enhancing Computer Programming Education with LLMs: A Study
    on Effective Prompt Engineering for Python Code Generation") shown, to explore
    and compare different prompts in LLMs, we use the following prompt settings in
    our study.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程在 LLM 的开发和优化中至关重要，因为它对这些模型的性能和准确性有着显著影响。提示的选择直接影响 LLM 生成的输出，因为它们决定了模型如何解释和回应给定的输入。通过对同一问题进行不同提示的实验和测试，研究人员可以确定最有效的措辞和结构，从而提高模型生成相关、连贯和上下文适当的响应的能力。这一提示精炼的迭代过程确保
    LLM 不仅坚固，还能适应广泛的应用，从而最大化其在各个领域的效用。如表 [1](#S4.T1 "表 1 ‣ 4.1 一般提示工程 ‣ 4 方法论 ‣ 使用
    LLM 增强计算机编程教育：针对 Python 代码生成的有效提示工程研究") 所示，为了探索和比较 LLM 中的不同提示，我们在研究中使用了以下提示设置。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt 1 (base): This configuration serves as our baseline. In this setup,
    the model is presented with the fundamental structure of a problem statement.
    The objective is to assess the model’s inherent capability to understand and generate
    responses with minimal guidance.'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示 1（基础）：此配置作为我们的基准。在此设置中，模型提供了问题陈述的基本结构。目标是评估模型在最小指导下理解和生成响应的固有能力。
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt 2 (example (1-shot)): In this prompt, the model is initially provided
    with the basic structure of a problem statement. Subsequently, we introduce a
    single of high-quality code example to the LLM. This configuration aims to evaluate
    the impact of enhanced prompt structures on the model’s output, particularly focusing
    on how the inclusion of an example influences the generated responses.'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Prompt 2 (example (1-shot))：在这个提示中，模型最初提供了问题陈述的基本结构。随后，我们向LLM引入一个高质量代码示例。这个配置旨在评估增强提示结构对模型输出的影响，特别是示例的包含如何影响生成的响应。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt 3 (dynamic example): Similar to prompt 2, this prompt initially presents
    the problem statement. However, instead of using a static, high-quality code example,
    the LLM is tasked with generating a related but distinct example. The goal here
    is to assess the model’s adaptability and ability to produce a code solution based
    on dynamically generated examples.'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Prompt 3 (dynamic example)：类似于提示2，此提示最初呈现问题陈述。然而，与使用静态的高质量代码示例不同，LLM被要求生成一个相关但不同的示例。这里的目标是评估模型的适应能力以及基于动态生成的示例生成代码解决方案的能力。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt 4 (guide): This prompt incorporates general coding guidelines alongside
    the problem statement. The intention is to guide the LLM towards generating code
    that adheres to broad quality standards. Details regarding the content of these
    general guidelines are provided in Appendix [B](#A2 "Appendix B General Guidelines
    for High-Quality Python Code ‣ Enhancing Computer Programming Education with LLMs:
    A Study on Effective Prompt Engineering for Python Code Generation").'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Prompt 4 (guide)：此提示结合了通用编码指南和问题陈述。目的是指导LLM生成符合广泛质量标准的代码。关于这些通用指南内容的详细信息在附录[B](#A2
    "附录 B 高质量 Python 代码的一般指南 ‣ 利用 LLMs 提升计算机编程教育：关于有效提示工程的研究")中提供。
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt 5 (multi): This configuration employs a multi-step conversational prompt
    strategy with LLMs. Initially, it provides a multiple turns of interaction, where
    the model responds to ongoing inputs that build on previous context. Instructors
    or students evaluate each suggestion, providing feedback to guide the LLM towards
    optimal solutions. This approach usually suits problems requiring sustained engagement
    like tutoring or solving complex problems. Specifically,'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Prompt 5 (multi)：此配置采用了多步骤对话提示策略与LLMs。最初，它提供了多轮互动，其中模型回应建立在之前上下文基础上的持续输入。教师或学生评估每个建议，提供反馈以指导LLM朝向最佳解决方案。该方法通常适用于需要持续参与的问题，如辅导或解决复杂问题。具体而言，
- en: '1.'
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Generate Pseudo Code: The initial step involves generating pseudo code based
    on the question. This pseudo code acts as an intermediate representation of the
    solution, outlining the logical steps necessary to solve the problem without delving
    into specific syntax. The generated pseudo code serves as the foundation for subsequent
    verification and refinement processes.'
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成伪代码：初始步骤涉及根据问题生成伪代码。这个伪代码作为解决方案的中间表示，概述了解决问题所需的逻辑步骤，而无需涉及具体的语法。生成的伪代码作为后续验证和改进过程的基础。
- en: '2.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Verify Pseudo Code Logic: In the second step, the generated pseudo code is
    subjected to a logic verification process. This step ensures that the pseudo code
    accurately reflects a viable solution to the problem by identifying and correcting
    any logical errors. The verified pseudo code provides a reliable blueprint for
    generating sample inputs and outputs.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证伪代码逻辑：在第二步中，生成的伪代码会经过逻辑验证过程。此步骤确保伪代码准确地反映了问题的可行解决方案，通过识别和修正任何逻辑错误来实现。经过验证的伪代码提供了生成样本输入和输出的可靠蓝图。
- en: '3.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Input and Output Sample: Following the verification of pseudo code logic, the
    LLM takes sample input and output pairs from original question. These samples
    are used by LLM to better understand the question and play the important role
    for testing the correctness of the logical flow in pseudo code.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入和输出示例：在验证伪代码逻辑之后，LLM从原始问题中获取样本输入和输出对。这些样本用于帮助LLM更好地理解问题，并在测试伪代码逻辑流的正确性方面发挥重要作用。
- en: '4.'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Verify Code Logic: The focus here is on ensuring that the code from previous
    step behaves as expected when provided with the sample inputs, thereby producing
    the correct outputs. This verification process is essential for validating the
    logical coherence of the solution before final translating into real programming
    code.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证代码逻辑：这里的重点是确保前一步生成的代码在提供样本输入时按预期运行，从而产生正确的输出。此验证过程对于在最终转化为真实编程代码之前验证解决方案的逻辑一致性至关重要。
- en: '5.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Convert into Code: After checked the logic of pseudo code, this steps focus
    on the implementation of translating pseudo code into real programming code.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 转换为代码：在检查伪代码的逻辑之后，此步骤专注于将伪代码转换为真实编程代码的实现。
- en: '6.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Verify Code Logic: The focus here is on ensuring that the translated code behaves
    as expected when provided with the pseudo code logic, sample inputs, thereby producing
    the correct outputs.'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 验证代码逻辑：这里的重点是确保转换后的代码在提供伪代码逻辑和样本输入时按预期运行，从而产生正确的输出。
- en: '7.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Input and Output Format: The final step involves refining the code to adhere
    to the specified input and output format requirements. This step ensures that
    the code meets the problem’s specifications in terms of structure and presentation.
    The output from this step is the finalized code, which is expected to solve the
    given problem accurately.'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入和输出格式：最后一步涉及将代码调整为符合指定的输入和输出格式要求。这一步确保代码在结构和展示方面符合问题的规范。此步骤的输出是最终的代码，预计能够准确解决给定的问题。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prompt 6 (all-in-one): This model includes all the prompt configurations from
    Prompt 5 (multi). However, instead of using a multi-step conversational process,
    all prompt settings are consolidated into one single prompt. The objective is
    to compare the efficacy of multi-step prompting versus an all-in-one approach.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示 6（全合一）：该模型包含提示 5（多步骤）中的所有提示配置。然而，所有提示设置都合并为一个单一的提示，而不是使用多步骤的对话过程。其目的是比较多步骤提示与全合一方法的效果。
- en: By employing these diverse prompts, our study systematically investigates the
    influence of different prompt structures on the performance of LLMs, thereby providing
    insights into the optimal strategies for prompt engineering in the context of
    LLM-driven code assessment and guidance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这些多样的提示，我们的研究系统地调查了不同提示结构对 LLM 性能的影响，从而提供了有关在 LLM 驱动的代码评估和指导中优化提示工程策略的见解。
- en: 4.2 Problem-Specific Prompts
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 问题特定的提示
- en: 'These are often unusual coding problems, commonly seen in coding competitions,
    complex optimization problem, or even number theory problems, where a universal
    guide prompt is ineffective. Detailed, problem-specific prompts are required to
    navigate the intricacies of such problems. For example, advanced dynamic programming
    challenges or problems involving intricate mathematical concepts typically require
    bespoke prompts that provide in-depth, step-by-step guidance tailored to the specific
    problem at hand. We will discuss this category in details in section [6.2](#S6.SS2
    "6.2 USACO Results ‣ 6 Result Analysis ‣ Enhancing Computer Programming Education
    with LLMs: A Study on Effective Prompt Engineering for Python Code Generation").'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这些通常是非常规的编码问题，常见于编码竞赛、复杂的优化问题，甚至是数论问题，其中通用的指导提示无效。需要详细的问题特定提示来应对这些问题的复杂性。例如，涉及高级动态规划挑战或复杂数学概念的问题通常需要定制的提示，提供针对特定问题的深入、逐步的指导。我们将在第[6.2](#S6.SS2
    "6.2 USACO 结果 ‣ 6 结果分析 ‣ 使用 LLM 提升计算机编程教育：针对 Python 代码生成的有效提示工程研究")节中详细讨论这一类别。
- en: 5 Experiments
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Dataset
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集
- en: 'Numerous sources and coding question banks (benchmarks) are available to assess
    the capabilities of AI code generation. The majority of these code questions are
    publicly accessible, having been extensively analyzed and discussed, thus serving
    as training data for most large language models (LLMs). Identifying a unique,
    diverse, and challenging coding dataset that has not been extensively used in
    training LLMs remains a significant challenge. In this study we used two data
    source as our dataset: 1) LeetCode 2) USACO.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多来源和编码问题库（基准）可用于评估 AI 代码生成的能力。这些代码问题大多是公开的，已经经过广泛分析和讨论，因此成为大多数大型语言模型（LLMs）的训练数据。找到一个独特、多样且具有挑战性的编码数据集，而该数据集没有被广泛用于训练
    LLMs，仍然是一个重大挑战。在本研究中，我们使用了两个数据源作为我们的数据集：1）LeetCode 2）USACO。
- en: 5.1.1 LeetCode Dataset
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1 LeetCode 数据集
- en: We start with LeetCode ³³3https://leetcode.com/ as our experinment dataset,
    due to its comprehensive range of well-defined and real-world-relevant programming
    problems. The structured nature of these problems enhances the AI’s ability to
    process and generate solutions, providing a consistent benchmark for performance
    analysis. Furthermore, LeetCode holds significant educational value; it offers
    students practical applications of theoretical concepts, enhancing learning through
    exposure to diverse problem-solving techniques. For our experiments, we selected
    and compiled the first 100 questions from LeetCode (lc100), including both questions
    and test cases.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 LeetCode ³³3https://leetcode.com/ 开始作为我们的实验数据集，因为它涵盖了全面且与现实世界相关的编程问题。这些问题的结构化性质增强了
    AI 处理和生成解决方案的能力，为性能分析提供了一致的基准。此外，LeetCode 具有重要的教育价值；它为学生提供了理论概念的实际应用，通过接触不同的解决问题技巧来增强学习。对于我们的实验，我们选择并编译了
    LeetCode 的前 100 个问题（lc100），包括问题和测试用例。
- en: 'A Sample of LeetCode Question Problem Content: Given an array of integers nums
    and an integer target, return indices of the two numbers such that they add up
    to target. You may assume that each input would have exactly one solution, and
    you may not use the same element twice. You can return the answer in any order.
    Test Input: nums = [2, 7, 11, 15], target = 9 Test Output: [0,1]'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 LeetCode 问题内容示例：给定一个整数数组 nums 和一个整数 target，返回两个数字的索引，使它们的和等于 target。你可以假设每个输入都有唯一一个解，并且你不能使用相同的元素两次。你可以按任何顺序返回答案。测试输入：nums
    = [2, 7, 11, 15]，target = 9 测试输出：[0,1]
- en: 5.1.2 USACO Dataset
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2 USACO 数据集
- en: 'Given that the LeetCode dataset has been widely published and discussed and
    extensively utilized for training by many current LLMs, Brown et al. ([2020](#bib.bib31)),
    we aimed to identify a less commonly used dataset for our analysis. In this context,
    we have identified the United States of America Computing Olympiad (USACO) ⁴⁴4https://usaco.org/,
    an online computer programming competition that serves as a qualifier for the
    International Olympiad in Informatics in the USA, as a valuable data source. USACO
    provides a diverse set of coding problems, categorized by difficulty levels: Bronze,
    Silver, Gold, and Platinum. This dataset is particularly valuable because it is
    less commonly used as a training dataset by major LLMs, thereby offering a fresh
    and underutilized resource for evaluation. For our study, we selected relatively
    easy questions from the Bronze category, randomly picking 20 questions from different
    years spanning 2016 to 2023\. This dataset is subsequently referred to as usaco20.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到 LeetCode 数据集已广泛发布、讨论并被许多当前的 LLMs 用于训练（Brown 等，[2020](#bib.bib31)），我们旨在确定一个较少使用的数据集进行分析。在这种情况下，我们确定了美国计算机奥林匹克（USACO）⁴⁴4https://usaco.org/，这是一个在线计算机编程竞赛，作为美国信息学国际奥林匹克的预选赛，作为一个有价值的数据来源。USACO
    提供了各种编码问题，按难度级别分类：铜、银、金和铂金。该数据集特别有价值，因为它作为训练数据集被主要 LLMs 使用的频率较低，从而提供了一个新鲜且未充分利用的评估资源。对于我们的研究，我们从
    2016 年到 2023 年的不同年份中随机挑选了 20 个相对简单的铜级问题。该数据集随后被称为 usaco20。
- en: 'A Sample of USACO Question Problem Content: Farmer John has lost his prize
    cow Bessie, and he needs to find her! Fortunately, there is only one long path
    running across the farm, and Farmer John knows that Bessie has to be at some location
    on this path. If we think of the path as a number line, then Farmer John is currently
    at position x and Bessie is currently at position y (unknown to Farmer John).
    If Farmer John only knew where Bessie was located, he could walk directly to her,
    traveling a distance of $|x-y|$ between himself and Bessie before he finds her
    (this is also true, and the factor of 9 is actually the smallest such worst case
    guarantee any strategy can achieve). Farmer John is curious to verify this result.
    Given x and y, please compute the total distance he will travel according to the
    zig-zag search strategy above until he finds Bessie. Testing Format: INPUT FORMAT
    (file lostcow.in): The single line of input contains two distinct space-separated
    integers x and y. Both are in the range 0…1,000. OUTPUT FORMAT (file lostcow.out):
    Print one line of output, containing the distance Farmer John will travel to reach
    Bessie. Testing Case: SAMPLE INPUT:3 6 SAMPLE OUTPUT:9'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: USACO问题内容示例：农夫约翰丢失了他的奖牛贝西，他需要找到她！幸运的是，农夫约翰知道贝西必须在这条长路径上的某个位置。如果我们把这条路径看作一个数轴，那么农夫约翰现在在位置x，而贝西在位置y（农夫约翰未知）。如果农夫约翰知道贝西的位置，他可以直接走到她那里，旅行的距离是$|x-y|$（这是正确的，且9这个因子实际上是任何策略能达到的最小最坏情况保证）。农夫约翰想验证这个结果。给定x和y，请计算他根据上述之字形搜索策略直到找到贝西所行走的总距离。测试格式：输入格式（文件lostcow.in）：输入行包含两个不同的空格分隔整数x和y。范围是0…1,000。输出格式（文件lostcow.out）：输出一行，包含农夫约翰到达贝西的旅行距离。测试案例：示例输入：3
    6 示例输出：9
- en: 5.2 LLM Models
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 大型语言模型
- en: 'In this study, we recognize the vast and rapidly evolving landscape of LLMs,
    which includes numerous models of significance. Given the impracticality of listing
    all of them, we focus on the most popular and practically usable models, both
    open-source and closed-source. Our comparative analysis includes the following
    prominent LLMs: ChatGPT (gpt-4-turbo) ⁵⁵5https://openai.com/research/gpt-4, ChatGPT
    (gpt-4o) ⁶⁶6https://openai.com/index/hello-gpt-4o/, LLAMA (llama3-8b) ⁷⁷7https://llama.meta.com/llama3/,
    and Mistral (mixtral-8x7b) ⁸⁸8https://mistral.ai. Detailed configurations of these
    models are delineated in the Appendix under section [A](#A1 "Appendix A Prompts
    in This Research ‣ Enhancing Computer Programming Education with LLMs: A Study
    on Effective Prompt Engineering for Python Code Generation").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们认识到大型语言模型（LLMs）广泛而快速发展的领域，其中包含了众多重要模型。由于列出所有模型不切实际，我们专注于最受欢迎和实用的模型，包括开源和闭源的。我们的比较分析包括以下显著的LLMs：ChatGPT
    (gpt-4-turbo) ⁵⁵5https://openai.com/research/gpt-4, ChatGPT (gpt-4o) ⁶⁶6https://openai.com/index/hello-gpt-4o/,
    LLAMA (llama3-8b) ⁷⁷7https://llama.meta.com/llama3/, 和 Mistral (mixtral-8x7b)
    ⁸⁸8https://mistral.ai。这些模型的详细配置在附录中的 [A](#A1 "附录 A 本研究中的提示 ‣ 通过LLMs增强计算机编程教育：关于Python代码生成的有效提示工程研究")
    部分中进行了说明。
- en: 5.3 Evaluation Methods
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 评估方法
- en: 'For the LeetCode dataset, the website ⁹⁹9https://leetcode.com/problemset/ provides
    all test cases and expected results for each question. Given that LeetCode questions
    are widely used in coding interviews, it is crucial to employ a comprehensive
    set of evaluation criteria that capture both functional correctness and code efficiency.
    Therefore, we utilize three primary indicators: pass rate (correctness), time
    spent, and Pylint score, each selected for their distinct contributions to a holistic
    evaluation.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LeetCode数据集，网站 ⁹⁹9https://leetcode.com/problemset/ 提供了每个问题的所有测试用例和预期结果。鉴于LeetCode问题在编码面试中广泛使用，采用全面的评估标准以捕捉功能正确性和代码效率至关重要。因此，我们利用三个主要指标：通过率（正确性）、时间花费和Pylint评分，每个指标都对全面评估贡献独特。
- en: '1.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Pass Rate: The pass rate serves as a fundamental measure of correctness and
    reliability. It quantifies the proportion of test cases that a solution correctly
    handles, directly reflecting its ability to meet the problem’s specifications
    under various scenarios. This metric is crucial as it directly correlates with
    the primary goal of any coding solution—its accuracy and functionality.'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过率：通过率作为正确性和可靠性的基本度量。它量化了一个解决方案正确处理测试用例的比例，直接反映了其在各种场景下满足问题规格的能力。这个指标至关重要，因为它直接与任何编码解决方案的主要目标——准确性和功能性相关。
- en: '2.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Time Spent: This metric evaluates the efficiency of the problem-solving process.
    It encompasses the duration from initiating the coding of the solution to its
    successful execution and debugging. Monitoring time spent is essential for understanding
    the complexity and efficiency of the solution from a practical standpoint. In
    competitive programming, where time efficiency is as critical as correctness,
    this metric provides insights into the algorithm’s performance under time constraints.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间消耗：这一指标评估了解决问题的效率。它涵盖了从开始编码解决方案到成功执行和调试的时间。监控时间消耗对于从实际角度理解解决方案的复杂性和效率至关重要。在时间效率与正确性同样关键的竞赛编程中，这一指标提供了算法在时间限制下的表现洞察。
- en: '3.'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Pylint Score: As a widely recognized tool for code analysis in Python, Pylint
    assesses code quality based on a set of coding standards and heuristics. The Pylint
    score is indicative of the maintainability, readability, and structural quality
    of code. High scores suggest adherence to Python coding conventions and best practices,
    which are pivotal for long-term code maintenance and clarity. By including this
    metric, the evaluation encompasses not only the functional aspects but also the
    quality of the coding practices employed.'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Pylint 评分：作为一种广泛认可的 Python 代码分析工具，Pylint 根据一系列编码标准和启发式方法来评估代码质量。Pylint 评分反映了代码的可维护性、可读性和结构质量。高分数表明遵循了
    Python 编码规范和最佳实践，这对长期代码维护和清晰度至关重要。通过包括这一指标，评估不仅涵盖了功能方面，还包括了所使用的编码实践的质量。
- en: 'For USACO dataset, their website provides a user submission evaluation system
    ^(10)^(10)10https://usaco.guide/general/usaco-faq?lang=py. All submitted code
    solutions are evaluated and scored against a set of predetermined test cases,
    considering not only correctness but also time and memory usage (Fig. [3](#S5.F3
    "Figure 3 ‣ 5.3 Evaluation Methods ‣ 5 Experiments ‣ Enhancing Computer Programming
    Education with LLMs: A Study on Effective Prompt Engineering for Python Code Generation")).
    In this study, we use the USACO website to submit code generated by large language
    models (LLMs). A passing example is defined as code that successfully passes all
    test cases on their website.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 USACO 数据集，他们的网站提供了用户提交评估系统 ^(10)^(10)10https://usaco.guide/general/usaco-faq?lang=py。所有提交的代码解决方案都根据一系列预定的测试用例进行评估和评分，不仅考虑正确性，还考虑时间和内存使用情况（图
    [3](#S5.F3 "图 3 ‣ 5.3 评估方法 ‣ 5 实验 ‣ 使用 LLMs 提升计算机编程教育：关于 Python 代码生成的有效提示工程研究")）。在本研究中，我们使用
    USACO 网站提交由大型语言模型（LLMs）生成的代码。通过的示例定义为在他们的网站上成功通过所有测试用例的代码。
- en: '![Refer to caption](img/b9c3e4c839ad85682c3f394ed14ccff9.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b9c3e4c839ad85682c3f394ed14ccff9.png)'
- en: 'Figure 3: A screenshot of the USACO evaluation system displaying user submission
    results (all pass).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: USACO 评估系统截图，显示用户提交结果（全部通过）。'
- en: At last, our experiment was conducted within a meticulously controlled and isolated
    virtual setting. By adopting this method, we guarantee the results’ reliability
    and uniformity, offering a transparent and impartial evaluation of each LLM’s
    effectiveness in addressing coding challenges.^(11)^(11)11We repeatedly tested
    using different high-quality code examples, and the results remained stable with
    no significant performance variations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们的实验在一个精心控制和隔离的虚拟环境中进行。通过采用这种方法，我们保证了结果的可靠性和一致性，提供了对每个 LLM 在解决编码挑战中的有效性的透明和公正的评估。^(11)^(11)11我们反复测试了不同的高质量代码示例，结果保持稳定，没有显著的性能波动。
- en: 6 Result Analysis
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结果分析
- en: 6.1 LeetCode Results
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 LeetCode 结果
- en: 'Table 2: Model Comparisons (Accuracy (%), the higher, the better)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 模型比较（准确率（%），数值越高越好）'
- en: '|  | base | example (1-shot) | dynamic example | guide | multi | all-in-one
    |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | base | 示例（1-shot） | 动态示例 | 指导 | 多重 | 一体化 |'
- en: '| gpt-4 | 98% | 99% | 99% | 99% | 99% | 97% |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | 98% | 99% | 99% | 99% | 99% | 97% |'
- en: '| gpt-4o | 97% | 98% | 88% | 97% | 100% | 96% |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4o | 97% | 98% | 88% | 97% | 100% | 96% |'
- en: '| llama3-8b | 94% | 93% | 85% | 79% | 74% | 74% |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| llama3-8b | 94% | 93% | 85% | 79% | 74% | 74% |'
- en: '| mixtral-8x7b | 75% | 66% | 66% | 75% | 67% | 74% |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| mixtral-8x7b | 75% | 66% | 66% | 75% | 67% | 74% |'
- en: 'Table 3: Model Comparison (Time Spent (ms), the lower, the better)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 模型比较（时间消耗（毫秒），数值越低越好）'
- en: '|  | base | example (1-shot) | dynamic example | guide | multi | all-in-one
    |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | base | 示例（1-shot） | 动态示例 | 指导 | 多重 | 一体化 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| gpt-4 | 4095 | 3988 | 3999 | 3994 | 3959 | 3984 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | 4095 | 3988 | 3999 | 3994 | 3959 | 3984 |'
- en: '| gpt-4o | 4604 | 4431 | 4589 | 4430 | 4371 | 4129 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4o | 4604 | 4431 | 4589 | 4430 | 4371 | 4129 |'
- en: '| llama3-8b | 4325 | 4173 | 4238 | 4216 | 4142 | 4196 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| llama3-8b | 4325 | 4173 | 4238 | 4216 | 4142 | 4196 |'
- en: '| mixtral-8x7b | 4180 | 4097 | 4017 | 3954 | 3939 | 4032 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| mixtral-8x7b | 4180 | 4097 | 4017 | 3954 | 3939 | 4032 |'
- en: 'Table 4: Model Comparisons (Pylint Score, the higher, the better)'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：模型比较（Pylint得分，分数越高越好）
- en: '|  | base | example (1-shot) | dynamic example | guide | multi | all-in-one
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | base | example (1-shot) | dynamic example | guide | multi | all-in-one
    |'
- en: '| gpt-4 | 9.59 | 9.58 | 9.63 | 9.56 | 9.66 | 9.38 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4 | 9.59 | 9.58 | 9.63 | 9.56 | 9.66 | 9.38 |'
- en: '| gpt-4o | 9.34 | 9.49 | 9.25 | 9.00 | 9.62 | 9.52 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| gpt-4o | 9.34 | 9.49 | 9.25 | 9.00 | 9.62 | 9.52 |'
- en: '| llama3-8b | 9.18 | 9.14 | 10.00 | 8.64 | 8.41 | 7.24 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| llama3-8b | 9.18 | 9.14 | 10.00 | 8.64 | 8.41 | 7.24 |'
- en: '| mixtral-8x7b | 9.05 | 9.17 | 8.27 | 8.04 | 8.27 | 7.74 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| mixtral-8x7b | 9.05 | 9.17 | 8.27 | 8.04 | 8.27 | 7.74 |'
- en: 'In our experiments, we evaluated the performance of various models using different
    prompting strategies on easily solvable coding problems. The results, summarized
    in Table [2](#S6.T2 "Table 2 ‣ 6.1 LeetCode Results ‣ 6 Result Analysis ‣ Enhancing
    Computer Programming Education with LLMs: A Study on Effective Prompt Engineering
    for Python Code Generation"), indicate that GPT-4 and GPT-4o consistently outperform
    Llama3-8b and Mixtral-8x7b in terms of pass rate. Notably, GPT-4o achieved a perfect
    pass rate of 100% with the "multi" prompt strategy, highlighting its adaptability
    and efficiency. GPT-4 also demonstrated high reliability with pass rates predominantly
    at 99%, except for minor variations in the base scenario (98%) and all-in-one
    prompt (97%). Llama3-8b and Mixtral-8x7b showed lower adaptability, with Llama3-8b
    performing best in the base scenario (94%) and declining in more complex prompts.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的实验中，我们使用不同的提示策略评估了各种模型在易于解决的编码问题上的性能。结果总结在表[2](#S6.T2 "Table 2 ‣ 6.1 LeetCode
    Results ‣ 6 Result Analysis ‣ Enhancing Computer Programming Education with LLMs:
    A Study on Effective Prompt Engineering for Python Code Generation")中，表明GPT-4和GPT-4o在通过率方面始终优于Llama3-8b和Mixtral-8x7b。特别是，GPT-4o在“multi”提示策略下达到了100%的完美通过率，突显了其适应性和效率。GPT-4也显示出高度可靠，通过率通常在99%以内，除了基础场景（98%）和全合一提示（97%）有些微变化。Llama3-8b和Mixtral-8x7b适应性较差，Llama3-8b在基础场景（94%）表现最佳，但在更复杂的提示中表现下降。'
- en: 'Regarding time efficiency, as shown in Table [3](#S6.T3 "Table 3 ‣ 6.1 LeetCode
    Results ‣ 6 Result Analysis ‣ Enhancing Computer Programming Education with LLMs:
    A Study on Effective Prompt Engineering for Python Code Generation"), GPT-4 required
    the least time to execute across all prompts, with the shortest time recorded
    at 3959 milliseconds for the "multi" prompt. This suggests that GPT-4 is both
    effective and efficient. GPT-4o, while achieving the highest pass rate, exhibited
    slightly longer execution times, ranging from 4371 milliseconds ("multi") to 4604
    milliseconds ("base"). Llama3-8b and Mixtral-8x7b generally required more time
    than GPT-4 but less than GPT-4o.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '关于时间效率，如表[3](#S6.T3 "Table 3 ‣ 6.1 LeetCode Results ‣ 6 Result Analysis ‣ Enhancing
    Computer Programming Education with LLMs: A Study on Effective Prompt Engineering
    for Python Code Generation")所示，GPT-4在所有提示中执行所需时间最短，“multi”提示的记录时间为3959毫秒。这表明GPT-4既有效又高效。虽然GPT-4o的通过率最高，但执行时间略长，从4371毫秒（“multi”）到4604毫秒（“base”）。Llama3-8b和Mixtral-8x7b通常比GPT-4花费更多时间，但少于GPT-4o。'
- en: 'Pylint scores, reported in Table [4](#S6.T4 "Table 4 ‣ 6.1 LeetCode Results
    ‣ 6 Result Analysis ‣ Enhancing Computer Programming Education with LLMs: A Study
    on Effective Prompt Engineering for Python Code Generation"), reveal that all
    models achieved high scores, indicating good adherence to coding standards. GPT-4
    generally exhibited the highest Pylint scores, especially under the "multi" prompt,
    scoring 9.66\. GPT-4o displayed variability in scores, with the highest being
    9.62 ("multi") and the lowest 9.00 ("guide"). Llama3-8b and Mixtral-8x7b showed
    lower and more variable scores, with particularly low scores in the "all-in-one"
    and "guide" prompts, respectively. Notably, the "dynamic example" prompt resulted
    in a perfect score of 10.00 for Llama3-8b, though this was an outlier.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 'Pylint得分，如表[4](#S6.T4 "Table 4 ‣ 6.1 LeetCode Results ‣ 6 Result Analysis ‣
    Enhancing Computer Programming Education with LLMs: A Study on Effective Prompt
    Engineering for Python Code Generation")所示，所有模型都取得了高分，表明良好的编码标准遵守情况。GPT-4通常显示出最高的Pylint得分，特别是在“multi”提示下，得分为9.66。GPT-4o得分变动较大，最高为9.62（“multi”），最低为9.00（“guide”）。Llama3-8b和Mixtral-8x7b的得分较低且变动较大，特别是在“all-in-one”和“guide”提示下得分较低。值得注意的是，“dynamic
    example”提示使Llama3-8b取得了10.00的完美得分，但这是一个异常值。'
- en: In summary, our findings indicate that while the base prompt and GPT-4 family
    models (GPT-4, GPT-4o) already exhibit exceptional performance, the multi-step
    prompt strategy offers limited improvement. This is primarily due to the widespread
    public discussion of questions like those on LeetCode, and the fact that the data
    used in training most LLMs, such as Common Crawl, already includes these questions
    and their solutions Brown et al. ([2020](#bib.bib31)). Consequently, for simpler
    problems, more complex prompting strategies provide only incremental benefits,
    and using LLMs without additional prompts is sufficient.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，我们的发现表明，虽然基础提示和GPT-4系列模型（GPT-4, GPT-4o）已经表现出色，但多步骤提示策略提供的改进有限。这主要是由于LeetCode等问题的广泛公开讨论，以及大多数LLMs（如Common
    Crawl）用于训练的数据已经包含这些问题及其解决方案（Brown等人（[2020](#bib.bib31)））。因此，对于简单问题，更复杂的提示策略仅提供了增量收益，而无需额外提示的LLMs已足够。
- en: 6.2 USACO Results
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 USACO结果
- en: 'Table 5: Results of USACO Dataset (Pass Example & Accuracy)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：USACO数据集的结果（通过示例及准确性）
- en: '|  | Sovable (base prompt) | Solvable (multi prompt) | Solvable^(12)^(12)12The
    results for ‘Solvable (multi+spec prompt)’ include additional specific prompts.
    (multi+spec prompt) | Currently Not Solvable |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | 可解（基础提示） | 可解（多步骤提示） | 可解^(12)^(12)12“可解（多+特定提示）”结果包括额外的特定提示。 (多+特定提示)
    | 目前无法解决 |'
- en: '| Question ID (cpid) | 639, 737, 761, 766, 807, 939 | 639, 641, 735, 737, 739,
    760, 761, 766, 807, 939, 1228 | 639, 641, 735, 737, 739, 760, 761, 766, 783, 785,
    787, 807, 939, 1228, 1323 | 644, 738, 808, 1035, 1131 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 问题ID (cpid) | 639, 737, 761, 766, 807, 939 | 639, 641, 735, 737, 739, 760,
    761, 766, 807, 939, 1228 | 639, 641, 735, 737, 739, 760, 761, 766, 783, 785, 787,
    807, 939, 1228, 1323 | 644, 738, 808, 1035, 1131 |'
- en: '| Count # | 6 | 11 | 15 | 5 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Count # | 6 | 11 | 15 | 5 |'
- en: '| % (out of 20) | 30% | 55% | 75% | 25% |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| % (out of 20) | 30% | 55% | 75% | 25% |'
- en: '^(12)^(12)footnotetext: Including solvable multi prompt and partial solvable
    spec prompts.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ^(12)^(12)脚注文本：包括可解的多步骤提示和部分可解的特定提示。
- en: 'We summarize all the pass examples in Table [5](#S6.T5 "Table 5 ‣ 6.2 USACO
    Results ‣ 6 Result Analysis ‣ Enhancing Computer Programming Education with LLMs:
    A Study on Effective Prompt Engineering for Python Code Generation"). Given multi-step
    prompt (multi) and GPT-4o model demonstrated superior performance in our previous
    experiments on the LeetCode dataset, we compare three prompts (base, multi, and
    spec) with the GPT-4o model on the USACO dataset.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[5](#S6.T5 "Table 5 ‣ 6.2 USACO Results ‣ 6 Result Analysis ‣ Enhancing
    Computer Programming Education with LLMs: A Study on Effective Prompt Engineering
    for Python Code Generation")中总结了所有通过的示例。鉴于多步骤提示（multi）和GPT-4o模型在我们之前对LeetCode数据集的实验中表现出色，我们将三种提示（base,
    multi, 和 spec）与GPT-4o模型在USACO数据集上进行比较。'
- en: The base prompt configuration solved only 6 out of 20 problems (30%). This indicates
    that LLMs struggle to generate valid code solutions without any form of prompt
    engineering. The low success rate suggests that minimal guidance in problem statements
    does not sufficiently leverage the capabilities of LLMs for effective problem-solving
    in computer programming.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 基础提示配置仅解决了20个问题中的6个（30%）。这表明LLMs在没有任何形式的提示工程时难以生成有效的代码解决方案。较低的成功率表明问题陈述中的最低指导不足以有效发挥LLMs在计算机编程中的解决问题能力。
- en: 'In contrast, the multi prompt configuration showed significant improvement,
    solving 11 out of 20 problems (55%). The Question IDs exclusive to the multi prompt
    (questions: 641, 735, 739, 760, 1228) are highlighted in bold in the table. This
    improvement underscores the advantages of multi-step conversational prompts. These
    prompts enhance the LLMs’ contextual understanding, allow for iterative refinement,
    ensure logical coherence, and encourage user engagement and feedback. This method
    is particularly well-suited for complex problem solving as it incorporates thorough
    verification and validation processes, making it superior to the without-prompt
    (base) approach.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，多步骤提示配置显示了显著的改进，解决了20个问题中的11个（55%）。表中用粗体标出的是仅适用于多步骤提示的问题ID（问题：641, 735,
    739, 760, 1228）。这一改进突显了多步骤对话提示的优势。这些提示增强了LLMs的上下文理解，允许进行迭代改进，确保逻辑连贯性，并鼓励用户参与和反馈。这种方法特别适用于复杂问题的解决，因为它包含了彻底的验证和确认过程，使其优于不使用提示（base）的方法。
- en: The multi+spec prompt configuration further extends the LLMs’ capabilities,
    solving 15 out of 20 problems (75%). This approach integrates specific question-related
    prompts (highlight in bold), such as analyzing the original code’s time complexity
    and suggesting more efficient approaches (question 785), or listing all possible
    corner cases in a problem (question 783). Although these specific prompts cannot
    be generalized into a single prompt applicable to all questions, tailored prompts
    provide more detailed scenarios and considerations, thereby augmenting the LLMs’
    problem-solving abilities for complex tasks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: multi+spec提示配置进一步扩展了LLMs的能力，解决了20个问题中的15个（75%）。这种方法整合了特定问题相关的提示（加粗显示），例如分析原始代码的时间复杂度和建议更高效的方法（问题785），或列出问题中的所有可能边界情况（问题783）。尽管这些特定提示不能被泛化为适用于所有问题的单一提示，但量身定制的提示提供了更详细的场景和考虑因素，从而增强了LLMs处理复杂任务的能力。
- en: 'Despite these advancements, some problems remain unsolved by the LLMs, even
    with enhanced prompts (questions: 644, 738, 808, 1035, 1131). These challenging
    problems typically require complex logical reasoning, sequential decision-making,
    and optimization under constraints. They involve maintaining context across multiple
    stages, understanding intricate relationships between variables, and handling
    combinatorial tasks. For example, ensuring a farm remains fully connected after
    each barn closure or optimizing cow milking time based on pairings involves high-order
    planning and sophisticated problem-solving skills. These tasks often surpass the
    capabilities of current LLMs due to limitations in context retention, logical
    reasoning, and handling numerical and combinatorial complexities.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了这些进展，仍然有一些问题在LLMs的帮助下未能解决，即使是使用增强提示（问题：644, 738, 808, 1035, 1131）。这些具有挑战性的问题通常需要复杂的逻辑推理、顺序决策以及在约束条件下的优化。它们涉及在多个阶段中保持上下文，理解变量之间的复杂关系，以及处理组合任务。例如，确保在每个仓库关闭后农场保持完全连接，或根据配对优化挤奶时间，都涉及高级规划和复杂的问题解决技能。这些任务通常超出了当前LLMs的能力，因为在上下文保持、逻辑推理以及处理数值和组合复杂性方面存在限制。
- en: The results of our study indicate that prompt engineering significantly enhances
    the problem-solving capabilities of LLMs in the context of computer programming
    education. The multi-step prompts, in particular, demonstrate the potential to
    guide LLMs through complex problem-solving processes by providing a structured
    and iterative approach. We will discuss the usages of different strategies in
    prompt engineering in the next section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究结果表明，提示工程显著增强了大型语言模型（LLMs）在计算机编程教育中的问题解决能力。特别是多步骤提示显示了通过提供结构化和迭代的方法来引导LLMs完成复杂问题解决过程的潜力。我们将在下一节讨论不同策略在提示工程中的应用。
- en: 7 Discussion
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 讨论
- en: 7.1 LLM Model in Code Generation
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 LLM模型在代码生成中的应用
- en: The results from our experiments indicate a clear recommendation for the use
    of the GPT-4 and its optimized variant GPT-4o as the preferred LLMs for educational
    purposes in computer programming instruction. The consistently high performance
    of these models across various prompting strategies highlights their adaptability,
    efficiency, and robustness. GPT-4o, in particular, demonstrated a perfect pass
    rate in the "multi" prompt strategy, suggesting its superior capability in understanding
    and generating accurate responses in complex problem-solving scenarios. The slightly
    higher execution times of GPT-4o compared to GPT-4 are a reasonable trade-off
    for its enhanced performance and adaptability. Therefore, we recommend GPT-4o
    as the primary model for educational settings, especially where complex problem-solving
    and iterative refinement processes are critical.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实验的结果明确建议使用GPT-4及其优化变体GPT-4o作为计算机编程教学中首选的LLMs。这些模型在各种提示策略下表现 consistently
    高，突显了它们的适应性、效率和鲁棒性。特别是GPT-4o在“multi”提示策略中表现出完美的通过率，表明它在理解和生成准确回应方面具有出色的能力，尤其是在复杂问题解决情境中。相比之下，GPT-4o的执行时间略高于GPT-4，但这对于其增强的性能和适应性而言是合理的权衡。因此，我们建议在教育环境中，尤其是那些复杂问题解决和迭代优化过程至关重要的场景中，优先使用GPT-4o作为主要模型。
- en: 7.2 Prompt Strategies Based on Educational Requirements
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 基于教育需求的提示策略
- en: 'Furthermore, to maximize the effectiveness of LLMs in different educational
    contexts, we propose tailored prompt strategies for three distinct educational
    requirements: regular knowledge and skills, competition preparation, and advanced
    problem-solving.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了最大化 LLM 在不同教育背景下的有效性，我们提出了针对三种不同教育需求的定制提示策略：常规知识与技能、比赛准备和高级问题解决。
- en: '1.'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Knowledge and Skills: For foundational learning and skill-building in computer
    programming, no-prompt engineering approaches suffice. This strategy involves
    presenting the LLM with questions that include problem content, constraints, and
    test cases. Since most fundamental computer science knowledge and exercises are
    already well-represented in LLM training datasets, this structured prompting method
    effectively aids in developing essential skills necessary for class exercise,
    basic algorithmic problem-solving and coding interviews. The method’s systematic
    nature supports the learners in acquiring a solid foundation in programming concepts
    and techniques.'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识与技能：对于计算机编程中的基础学习和技能培养，无需提示工程方法。这种策略包括向 LLM 提出包含问题内容、约束条件和测试用例的问题。由于大多数基础计算机科学知识和练习已经在
    LLM 的训练数据集中充分体现，这种结构化的提示方法有效地帮助开发必要的技能，以应对课堂练习、基本算法问题解决和编码面试。该方法的系统性支持学习者在编程概念和技术方面打下坚实的基础。
- en: '2.'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Competition Preparation (e.g., USACO): In the context of preparing for programming
    competitions, such as USACO, the "Multi-Step Conversational Prompt" strategy is
    most effective. This approach allows for a dynamic interaction between the LLM
    and the student, where multiple turns of feedback and refinement are possible.
    The steps involved in generating pseudo code, verifying logic, and iteratively
    refining the solution are particularly beneficial for complex and competitive
    problems that require critical and innovative thinking. This strategy enhances
    the LLM’s contextual understanding and ensures logical coherence, making it ideal
    for high-stakes competitive scenarios.'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 比赛准备（例如 USACO）：在准备编程竞赛（如 USACO）的背景下，“多步骤对话提示”策略最为有效。这种方法允许 LLM 与学生之间进行动态互动，多个反馈和改进回合成为可能。生成伪代码、验证逻辑和迭代改进解决方案的步骤对于需要批判性和创新性思维的复杂和竞争性问题尤其有益。这一策略提升了
    LLM 的上下文理解，并确保逻辑连贯，使其在高风险的竞争场景中理想无比。
- en: '3.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Advanced Problem-Solving: For tackling advanced and complex problems, the "Specific
    Prompt Engineering" strategy is recommended. This method provides detailed instructions
    and considerations specific to the problem at hand, such as treating the problem
    as an optimization task or considering extreme scenarios. The focused guidance
    helps in addressing sophisticated topics like algorithm design, optimization,
    and number theory. This strategy is essential for students who already have a
    strong grasp of basic concepts and are looking to deepen their understanding and
    tackle more intricate problems. The integration of tailored prompts, as seen in
    the "multi+spec" prompt configuration, further extends the LLMs’ capabilities,
    making it suitable for advanced educational objectives.'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高级问题解决：对于解决高级和复杂问题，推荐使用“特定提示工程”策略。此方法提供了针对当前问题的详细说明和考虑事项，例如将问题视为优化任务或考虑极端场景。集中的指导有助于处理算法设计、优化和数论等复杂主题。这一策略对于已经掌握基本概念并希望深化理解和解决更复杂问题的学生至关重要。通过“multi+spec”提示配置所见的定制提示的整合，进一步扩展了
    LLM 的能力，使其适用于高级教育目标。
- en: 7.3 Future work
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 未来工作
- en: As LLMs continue to advance, the use of multi-round prompts with various strategies
    remains vital in addressing complex problems. Future research will target several
    pivotal areas to augment LLM capabilities through prompt engineering. Initially,
    we will extend our testing of prompt engineering strategies on more challenging
    datasets, including more competition and number theory problems, to evaluate and
    refine our approaches across a broader spectrum of difficult issues. Furthermore,
    our research will emphasize improving context retention, enhancing logical reasoning,
    and better managing numerical and combinatorial complexities. Developing sophisticated
    prompt engineering techniques tailored to these challenging problem types is essential
    for optimizing the educational potential of LLMs. By systematically categorizing
    and testing various prompt engineering strategies, we aim to establish a robust
    framework incorporating retrieval-augmented generation (RAG), multi-agents system,
    and Plan-and-Execute tools for educational implementation. This framework will
    provide educators with comprehensive guidelines to optimize LLM-based learning
    experiences, ultimately enhancing educational outcomes in computer programming
    instruction.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 随着LLMs的不断进步，使用多轮提示与各种策略仍然是解决复杂问题的关键。未来的研究将集中在几个关键领域，以通过提示工程增强LLMs的能力。首先，我们将扩展对提示工程策略在更具挑战性数据集上的测试，包括更多竞赛和数论问题，以评估和改进我们在更广泛难题上的方法。此外，我们的研究将重点改善上下文保留，提升逻辑推理能力，并更好地管理数值和组合复杂性。开发针对这些挑战性问题类型的复杂提示工程技术对优化LLMs的教育潜力至关重要。通过系统地分类和测试各种提示工程策略，我们旨在建立一个稳健的框架，结合检索增强生成（RAG）、多代理系统和计划执行工具用于教育实施。该框架将为教育工作者提供全面的指南，以优化基于LLMs的学习体验，最终提升计算机编程教育成果。
- en: 8 Conclusion
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: 'This paper explores the potential of prompt engineering in large language models
    (LLMs) to enhance educational outcomes in computer programming instruction. Our
    research is driven by three key questions: the systematic categorization of prompt
    engineering strategies tailored to educational requirements, the empowerment of
    LLMs to solve complex problems, and the establishment of a robust framework for
    testing and implementing these strategies.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了提示工程在大语言模型（LLMs）中提高计算机编程教育成果的潜力。我们的研究由三个关键问题驱动：针对教育需求量身定制的提示工程策略的系统分类，赋能LLMs解决复杂问题，以及建立一个稳健的框架以测试和实施这些策略。
- en: Our findings indicate that the GPT-4 and GPT-4o models outperform other LLMs
    such as Llama3-8b and Mixtral-8x7b in terms of pass rates, execution times, and
    adherence to coding standards. The GPT-4o model, in particular, demonstrated a
    successfully pass rate with the "multi" prompt strategy, highlighting its superior
    adaptability and efficiency. These results lead us to recommend GPT-4o as the
    preferred model for educational purposes in computer programming.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现表明，GPT-4和GPT-4o模型在通过率、执行时间和编码标准遵守方面优于其他LLMs，如Llama3-8b和Mixtral-8x7b。特别是GPT-4o模型在“多步”提示策略下表现出成功的通过率，突显了其卓越的适应性和效率。这些结果使我们推荐GPT-4o作为计算机编程教育的首选模型。
- en: We propose tailored prompt strategies based on educational requirements. For
    foundational learning and skill-building, such as LeetCode, ask question directly
    without prompt engineering is suffice, providing structured guidance that helps
    students grasp essential concepts and techniques. For competition preparation,
    such as USACO, the "Multi-Step Conversational Prompt" strategy proves beneficial,
    facilitating dynamic interaction and iterative refinement that enhance contextual
    understanding and problem-solving skills. For advanced problem-solving, the "Specific
    Prompt Engineering" strategy is ideal, offering detailed instructions that address
    complex topics like algorithm design and optimization.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据教育需求提出量身定制的提示策略。对于基础学习和技能培养，如 LeetCode，直接提问而无需提示工程是足够的，提供结构化指导帮助学生掌握基本概念和技术。对于竞赛准备，如
    USACO，“多步对话提示”策略证明是有效的，促进动态互动和迭代优化，增强情境理解和解决问题的能力。对于高级问题解决，“具体提示工程”策略是理想的，提供详细的指示来处理如算法设计和优化等复杂主题。
- en: Our study also highlights the significant role of prompt engineering in maximizing
    the potential of LLMs in educational contexts. By categorizing and testing various
    strategies, we have established a robust framework for their implementation, providing
    educators with comprehensive guidelines to optimize LLM-based learning experiences.
    Despite the advancements, certain complex problems remain challenging for current
    LLMs, suggesting the need for further research to enhance context retention, logical
    reasoning, and handling of numerical and combinatorial complexities.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究还强调了提示工程在最大化 LLMs 在教育环境中的潜力方面的重大作用。通过对各种策略进行分类和测试，我们建立了一个强健的实施框架，为教育工作者提供了全面的指导方针，以优化基于
    LLM 的学习体验。尽管取得了进展，但某些复杂问题仍然对当前的 LLMs 造成挑战，这表明需要进一步研究以增强上下文保留、逻辑推理能力以及处理数值和组合复杂性的能力。
- en: In conclusion, prompt engineering significantly enhances the capabilities of
    LLMs in computer programming education. The tailored strategies we propose align
    with specific educational objectives, from foundational learning to advanced problem-solving.
    The superior performance of GPT-4 and GPT-4o confirms their suitability for a
    wide range of educational applications. By adopting and refining these strategies,
    educators can significantly improve educational outcomes, providing students with
    a more effective and personalized learning experience in computer programming.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，提示工程显著增强了大语言模型（LLMs）在计算机编程教育中的能力。我们提出的量身定制策略与特定的教育目标相一致，从基础学习到高级问题解决。GPT-4
    和 GPT-4o 的卓越表现确认了它们在广泛教育应用中的适用性。通过采用和改进这些策略，教育工作者可以显著提高教育成果，为学生提供更有效和个性化的计算机编程学习体验。
- en: References
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bommasani et al. [2021] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman,
    Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut,
    Emma Brunskill, et al. On the opportunities and risks of foundation models. *arXiv
    preprint arXiv:2108.07258*, 2021.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 博马萨尼等 [2021] 瑞希·博马萨尼、德鲁·A·哈德森、伊赫桑·阿德利、拉斯·阿尔特曼、辛曼·阿罗拉、悉尼·冯·阿克斯、迈克尔·S·伯恩斯坦、珍妮特·博赫、安托万·博塞卢、艾玛·布伦斯基等。关于基础模型的机遇与风险。*arXiv
    预印本 arXiv:2108.07258*，2021。
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图弗龙等 [2023] 雨果·图弗龙、蒂博·拉夫里尔、戈蒂埃·伊扎卡德、泽维尔·马尔蒂内、玛丽-安·拉肖、提莫特·拉克鲁瓦、巴普蒂斯特·罗齐埃、纳曼·戈亚尔、埃里克·汉布罗、法伊萨尔·阿扎尔等。Llama：开放且高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023。
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拉德福德等 [2019] 亚历克·拉德福德、杰弗里·吴、瑞温·柴尔德、大卫·阮、达里奥·阿莫德伊、伊利亚·苏茨克维尔等。语言模型是无监督的多任务学习者。*OpenAI
    博客*，1(8):9，2019。
- en: Ouyang et al. [2022] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欧阳等 [2022] 长欧阳、杰弗里·吴、徐江、迪奥戈·阿尔梅达、卡罗尔·温赖特、帕梅拉·米什金、钟张、桑迪尼·阿加瓦尔、卡塔里娜·斯拉马、亚历克斯·雷等。利用人类反馈训练语言模型以遵循指令。*神经信息处理系统进展*，35:27730–27744，2022。
- en: 'Liu et al. [2023a] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35, 2023a.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等 [2023a] 刘鹏飞、袁伟哲、付金兰、姜正宝、林博辉和格雷厄姆·纽比格。预训练、提示和预测：自然语言处理中的提示方法系统综述。*ACM 计算机调查*，55(9):1–35，2023a。
- en: Asare et al. [2023] Owura Asare, Meiyappan Nagappan, and N Asokan. Is github’s
    copilot as bad as humans at introducing vulnerabilities in code? *Empirical Software
    Engineering*, 28(6):129, 2023.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阿萨雷等 [2023] 奥乌拉·阿萨雷、梅伊亚潘·纳加潘和 N·阿索坎。GitHub 的 Copilot 在引入代码漏洞方面是否与人类一样糟糕？*实证软件工程*，28(6):129，2023。
- en: Pearce et al. [2022a] Hammond Pearce, Baleegh Ahmad, Benjamin Tan, Brendan Dolan-Gavitt,
    and Ramesh Karri. Asleep at the keyboard? assessing the security of github copilot’s
    code contributions. In *2022 IEEE Symposium on Security and Privacy (SP)*, pages
    754–768\. IEEE, 2022a.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 皮尔斯等 [2022a] 汉蒙德·皮尔斯、巴利赫·艾哈迈德、本杰明·谭、布伦丹·多兰-加维特、拉梅什·卡里。键盘上打盹？评估 GitHub Copilot
    的代码贡献的安全性。见 *2022 IEEE 安全与隐私研讨会 (SP)*，第754–768页。IEEE，2022a。
- en: 'Dakhel et al. [2023a] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam,
    Foutse Khomh, Michel C Desmarais, and Zhen Ming Jack Jiang. Github copilot ai
    pair programmer: Asset or liability? *Journal of Systems and Software*, 203:111734,
    2023a.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dakhel 等 [2023a] Arghavan Moradi Dakhel, Vahid Majdinasab, Amin Nikanjam, Foutse
    Khomh, Michel C Desmarais 和 Zhen Ming Jack Jiang. Github Copilot AI 编程助手：资产还是负担？*系统与软件期刊*，203:111734，2023a。
- en: Denny et al. [2023a] Paul Denny, James Prather, Brett A Becker, James Finnie-Ansley,
    Arto Hellas, Juho Leinonen, Andrew Luxton-Reilly, Brent N Reeves, Eddie Antonio
    Santos, and Sami Sarsa. Computing education in the era of generative ai. *arXiv
    preprint arXiv:2306.02608*, 2023a.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denny 等 [2023a] Paul Denny, James Prather, Brett A Becker, James Finnie-Ansley,
    Arto Hellas, Juho Leinonen, Andrew Luxton-Reilly, Brent N Reeves, Eddie Antonio
    Santos 和 Sami Sarsa. 生成式 AI 时代的计算机教育。*arXiv 预印本 arXiv:2306.02608*，2023a。
- en: Murr et al. [2023] Lincoln Murr, Morgan Grainger, and David Gao. Testing llms
    on code generation with varying levels of prompt specificity. *arXiv preprint
    arXiv:2311.07599*, 2023.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murr 等 [2023] Lincoln Murr, Morgan Grainger 和 David Gao. 测试大型语言模型在不同水平提示特异性下的代码生成。*arXiv
    预印本 arXiv:2311.07599*，2023。
- en: 'Finnie-Ansley et al. [2022] James Finnie-Ansley, Paul Denny, Brett A Becker,
    Andrew Luxton-Reilly, and James Prather. The robots are coming: Exploring the
    implications of openai codex on introductory programming. In *Proceedings of the
    24th Australasian Computing Education Conference*, pages 10–19, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finnie-Ansley 等 [2022] James Finnie-Ansley, Paul Denny, Brett A Becker, Andrew
    Luxton-Reilly 和 James Prather. 机器人来了：探索 OpenAI Codex 对入门编程的影响。见 *第 24 届澳大利亚计算教育会议论文集*，第
    10–19 页，2022。
- en: Hellas et al. [2023] Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme,
    Lilja Kujanpää, and Juha Sorva. Exploring the responses of large language models
    to beginner programmers’ help requests. *arXiv preprint arXiv:2306.05715*, 2023.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hellas 等 [2023] Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja
    Kujanpää 和 Juha Sorva. 探索大型语言模型对初学程序员求助请求的响应。*arXiv 预印本 arXiv:2306.05715*，2023。
- en: Kazemitabaar et al. [2023] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma,
    Barbara J Ericson, David Weintrop, and Tovi Grossman. Studying the effect of ai
    code generators on supporting novice learners in introductory programming. In
    *Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems*,
    pages 1–23, 2023.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kazemitabaar 等 [2023] Majeed Kazemitabaar, Justin Chow, Carl Ka To Ma, Barbara
    J Ericson, David Weintrop 和 Tovi Grossman. 研究 AI 代码生成器对初学者学习入门编程的支持效果。见 *2023
    年 CHI 会议论文集：计算机系统中的人因*，第 1–23 页，2023。
- en: 'Kiesler and Schiffner [2023] Natalie Kiesler and Daniel Schiffner. Large language
    models in introductory programming education: Chatgpt’s performance and implications
    for assessments, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kiesler 和 Schiffner [2023] Natalie Kiesler 和 Daniel Schiffner. 大型语言模型在入门编程教育中的应用：ChatGPT
    的表现及其对评估的影响，2023。
- en: Pearce et al. [2022b] Hammond Pearce, Benjamin Tan, Prashanth Krishnamurthy,
    Farshad Khorrami, Ramesh Karri, and Brendan Dolan-Gavitt. Pop quiz! can a large
    language model help with reverse engineering?, 2022b.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearce 等 [2022b] Hammond Pearce, Benjamin Tan, Prashanth Krishnamurthy, Farshad
    Khorrami, Ramesh Karri 和 Brendan Dolan-Gavitt. 突击测验！大型语言模型能帮助逆向工程吗？2022b。
- en: 'Denny et al. [2023b] Paul Denny, Brett A Becker, Juho Leinonen, and James Prather.
    Chat overflow: Artificially intelligent models for computing education-renaissance
    or apocaiypse? In *Proceedings of the 2023 Conference on Innovation and Technology
    in Computer Science Education V. 1*, pages 3–4, 2023b.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Denny 等 [2023b] Paul Denny, Brett A Becker, Juho Leinonen 和 James Prather. 聊天溢出：计算教育中的人工智能模型——复兴还是末日？见
    *2023 年计算机科学教育创新与技术会议论文集 V. 1*，第 3–4 页，2023b。
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等。评估训练于代码上的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021。
- en: Dakhel et al. [2023b] Arghavan Moradi Dakhel, Amin Nikanjam, Vahid Majdinasab,
    Foutse Khomh, and Michel C Desmarais. Effective test generation using pre-trained
    large language models and mutation testing. *arXiv preprint arXiv:2308.16557*,
    2023b.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dakhel 等 [2023b] Arghavan Moradi Dakhel, Amin Nikanjam, Vahid Majdinasab, Foutse
    Khomh 和 Michel C Desmarais. 使用预训练的大型语言模型和突变测试的有效测试生成。*arXiv 预印本 arXiv:2308.16557*，2023b。
- en: 'Reynolds and McDonell [2021] Laria Reynolds and Kyle McDonell. Prompt programming
    for large language models: Beyond the few-shot paradigm. In *Extended Abstracts
    of the 2021 CHI Conference on Human Factors in Computing Systems*, pages 1–7,
    2021.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reynolds and McDonell [2021] Laria Reynolds 和 Kyle McDonell. 大型语言模型的提示编程：超越少样本范式。在
    *2021 年 CHI 人机交互系统会议扩展摘要*，第 1–7 页，2021。
- en: Zhou et al. [2022] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, et al.
    Least-to-most prompting enables complex reasoning in large language models. *arXiv
    preprint arXiv:2205.10625*, 2022.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2022] Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, 等.
    最少到最多提示使大型语言模型能够进行复杂推理。*arXiv 预印本 arXiv:2205.10625*，2022。
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, 等. 思维链提示引发大型语言模型的推理。*神经信息处理系统进展*，35:24824–24837，2022。
- en: 'Arora et al. [2022] Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr,
    Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, and Christopher Ré. Ask me
    anything: A simple strategy for prompting language models. *arXiv preprint arXiv:2210.02441*,
    2022.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora et al. [2022] Simran Arora, Avanika Narayan, Mayee F Chen, Laurel Orr,
    Neel Guha, Kush Bhatia, Ines Chami, Frederic Sala, 和 Christopher Ré. 随便问我什么：一种简单的语言模型提示策略。*arXiv
    预印本 arXiv:2210.02441*，2022。
- en: White et al. [2023a] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos
    Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt.
    A prompt pattern catalog to enhance prompt engineering with chatgpt. *arXiv preprint
    arXiv:2302.11382*, 2023a.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White et al. [2023a] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos
    Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, 和 Douglas C Schmidt.
    提示模式目录以增强与 ChatGPT 的提示工程。*arXiv 预印本 arXiv:2302.11382*，2023a。
- en: Hendrycks et al. [2021] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    et al. Measuring coding challenge competence with apps. *arXiv preprint arXiv:2105.09938*,
    2021.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. [2021] Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    等. 通过应用程序测量编码挑战能力。*arXiv 预印本 arXiv:2105.09938*，2021。
- en: Xu et al. [2022] Frank F Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn.
    A systematic evaluation of large language models of code. In *Proceedings of the
    6th ACM SIGPLAN International Symposium on Machine Programming*, pages 1–10, 2022.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2022] Frank F Xu, Uri Alon, Graham Neubig, 和 Vincent Josua Hellendoorn.
    大型语言模型的系统评估。在 *第 6 届 ACM SIGPLAN 国际机器编程研讨会*，第 1–10 页，2022。
- en: Liu et al. [2023b] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of
    large language models for code generation. *arXiv preprint arXiv:2305.01210*,
    2023b.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023b] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, 和 Lingming Zhang.
    你的代码是由 ChatGPT 生成的真的正确吗？对代码生成的大型语言模型的严格评估。*arXiv 预印本 arXiv:2305.01210*，2023b。
- en: White et al. [2023b] Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith,
    and Douglas C Schmidt. Chatgpt prompt patterns for improving code quality, refactoring,
    requirements elicitation, and software design. *arXiv preprint arXiv:2303.07839*,
    2023b.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White et al. [2023b] Jules White, Sam Hays, Quchen Fu, Jesse Spencer-Smith,
    和 Douglas C Schmidt. ChatGPT 提示模式用于改进代码质量、重构、需求引导和软件设计。*arXiv 预印本 arXiv:2303.07839*，2023b。
- en: Kanade et al. [2020] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. Learning and evaluating contextual embedding of source code. In *International
    conference on machine learning*, pages 5110–5121\. PMLR, 2020.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanade et al. [2020] Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, 和 Kensen
    Shi. 学习和评估源代码的上下文嵌入。在 *国际机器学习会议*，第 5110–5121 页。PMLR，2020。
- en: Ciniselli et al. [2021] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys
    Poshyvanyk, Massimiliano Di Penta, and Gabriele Bavota. An empirical study on
    the usage of bert models for code completion. In *2021 IEEE/ACM 18th International
    Conference on Mining Software Repositories (MSR)*, pages 108–119\. IEEE, 2021.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ciniselli et al. [2021] Matteo Ciniselli, Nathan Cooper, Luca Pascarella, Denys
    Poshyvanyk, Massimiliano Di Penta, 和 Gabriele Bavota. 关于使用 BERT 模型进行代码补全的实证研究。在
    *2021 IEEE/ACM 第 18 届国际软件存储库挖掘会议 (MSR)*，第 108–119 页。IEEE，2021。
- en: Wang and Chen [2023] Tianyu Wang and Zhixiong Chen. Analyzing code text strings
    for code evaluation. In *2023 IEEE International Conference on Big Data (BigData)*,
    pages 5619–5628\. IEEE, 2023.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 和 Chen [2023] Tianyu Wang 和 Zhixiong Chen。分析代码文本字符串以进行代码评估。在 *2023 IEEE
    国际大数据会议（BigData）*，第 5619–5628 页。IEEE，2023。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。语言模型是少量学习者。*神经信息处理系统的进展*，33:1877–1901，2020。
- en: Appendix A Prompts in This Research
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 本研究中的提示
- en: Within the domain of LLMs, such as the entity engaged in the current interaction,
    the terminologies system prompts and user prompts delineate distinct categories
    of input stimuli instrumental in directing the model’s output generation.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LLM 的领域中，如当前交互中的实体，术语系统提示和用户提示划分了两类不同的输入刺激，这些输入刺激对指导模型的输出生成至关重要。
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'System Prompt: The system prompt is used to define the persona that LLM will
    play. In our application, it is part of the system’s design to help guide the
    LLM in generating responses or performing code generation to ensure it plays as
    a sophisticated programmer professional or mentor.'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统提示：系统提示用于定义 LLM 将扮演的角色。在我们的应用中，它是系统设计的一部分，用于帮助引导 LLM 生成响应或执行代码生成，以确保其扮演一个成熟的编程专家或导师。
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'User prompts: They are the prompts specifying the questions, commands, or statements
    that users input into the system to seek a response from the LLM (played as a
    programming tutor). In essence, this encompasses the textual or verbal input furnished
    by the user to the model.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 用户提示：这些是指定用户输入系统以寻求 LLM（扮演编程导师）的响应的问题、命令或陈述。实质上，这包括用户向模型提供的文本或语言输入。
- en: The following figures presents illustrative examples of prompts utilized within
    the scope of our manuscript.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了在我们手稿范围内使用的提示的说明性示例。
- en: '![Refer to caption](img/786679b9038182b3c85d2d07568b5e55.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/786679b9038182b3c85d2d07568b5e55.png)'
- en: 'Figure 4: System Prompt'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 系统提示'
- en: '![Refer to caption](img/6e2c8967f99dcb85551b7a4524e1a87e.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/6e2c8967f99dcb85551b7a4524e1a87e.png)'
- en: 'Figure 5: User Prompt (Base and Example)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: 用户提示（基础和示例）'
- en: '![Refer to caption](img/b0e24b674adc6bd073f1abb2499126f8.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b0e24b674adc6bd073f1abb2499126f8.png)'
- en: 'Figure 6: User Prompt (Dynamic Example, General Guide)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 用户提示（动态示例，一般指南）'
- en: '![Refer to caption](img/d9e2d1256d72e7f5be597de9e56334d5.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d9e2d1256d72e7f5be597de9e56334d5.png)'
- en: 'Figure 7: User Prompt (Category Question)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 用户提示（类别问题）'
- en: '![Refer to caption](img/eabd4bffaa41db69c692145fe3ff5fb1.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/eabd4bffaa41db69c692145fe3ff5fb1.png)'
- en: 'Figure 8: User Prompt (Category and Guide)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 用户提示（类别和指南）'
- en: '![Refer to caption](img/2e7df951f8e5c4304f5432369c44883e.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2e7df951f8e5c4304f5432369c44883e.png)'
- en: 'Figure 9: User Prompt (Multi-round Guide)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 用户提示（多轮指南）'
- en: Appendix B General Guidelines for High-Quality Python Code
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 高质量 Python 代码的一般指南
- en: '1.'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Use of Standard Libraries: The code effectively utilizes Python’s standard
    libraries such as re (for regular expressions), heapq (for heap queue algorithms),
    bisect (for array bisection algorithms), and math.'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用标准库：代码有效地利用了 Python 的标准库，如 re（用于正则表达式）、heapq（用于堆队列算法）、bisect（用于数组二分算法）和 math。
- en: 'Best Practice: Always prefer standard libraries for common algorithms. Do not
    import unnecessary libraries.'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳实践：始终优先使用标准库中的常见算法。不导入不必要的库。
- en: '2.'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Use Proper Data Structures: Fundamental building blocks for linked lists and
    binary trees. These structures are essential for representing hierarchical or
    sequential data. Stack: Implements a basic data structure, like Node, ListNode,
    TreeNode: a stack, a queue, a linked list, or a binary tree.'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用适当的数据结构：链表和二叉树的基本构建块。这些结构对于表示层次结构或顺序数据至关重要。堆栈：实现一个基本数据结构，如节点、列表节点、树节点：堆栈、队列、链表或二叉树。
- en: 'Best Practice: Create and choose the right data structure based on the problem
    requirements for efficiency in both time and space complexity.'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳实践：根据问题需求创建和选择合适的数据结构，以提高时间和空间复杂度的效率。
- en: '3.'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Itertools Usage: The use of itertools for permutations, combinations, and cartesian
    product is a sign of advanced Python knowledge. These functions are crucial for
    solving combinatorial problems.'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: itertools 用法：使用 itertools 进行排列、组合和笛卡尔积，显示了高级 Python 知识。这些函数对于解决组合问题至关重要。
- en: 'Best Practice: Leverage itertools to write cleaner and more efficient looping
    constructs.'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳实践：利用 itertools 编写更简洁和高效的循环构造。
- en: '4.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Heap Queue (heapq): The heapq module is used for implementing priority queues.
    This is vital in algorithms where you need to repeatedly access the smallest or
    largest element.'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 堆队列（heapq）：heapq 模块用于实现优先队列。这在需要重复访问最小或最大元素的算法中至关重要。
- en: 'Best Practice: Use heapq for priority queue implementation if necessary instead
    of a sorted list for better performance.'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳实践：如有必要，使用 heapq 实现优先队列，而不是使用排序列表，以提高性能。
- en: '5.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Functional Programming: Usage of functools like reduce, cache, and lru_cache
    indicates a functional approach to problem-solving. This is efficient for operations
    that benefit from memoization or reducing a list.'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 函数式编程：使用 functools 中的 reduce、cache 和 lru_cache 表明了问题解决的函数式方法。这对于受益于记忆化或对列表进行减少的操作非常高效。
- en: 'Best Practice: Employ functional programming concepts where applicable to make
    the code more concise and readable.'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳实践：在适用时使用函数式编程概念，使代码更简洁和可读。
- en: '6.'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Math and Random Modules: The inclusion of math operations and random for random
    number generation is suitable for problems involving math computations and stochastic
    processes.'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数学与随机模块：包括数学运算和随机数生成的随机模块，适用于涉及数学计算和随机过程的问题。
- en: 'Best Practice: Understand and utilize the vast array of functions provided
    by these modules to simplify complex calculations.'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳实践：理解并利用这些模块提供的广泛函数，以简化复杂计算。
- en: '7.'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Efficiency and Optimization: The use of bisect for binary searches and heapq
    for efficient element access in priority queues shows an understanding of algorithmic
    efficiency.'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 效率与优化：使用 bisect 进行二分查找和 heapq 进行优先队列中的高效元素访问，显示了对算法效率的理解。
- en: 'Best Practice: Always consider time and space complexity; optimize code where
    necessary but avoid premature optimization.'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳实践：始终考虑时间和空间复杂度；必要时优化代码，但避免过早优化。
- en: '8.'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Number Theory: When solving problems related to number theory, it is essential
    to leverage mathematical properties and efficient algorithms. Utilize functions
    from the math module for operations like finding greatest common divisors (gcd),
    least common multiples (lcm), and performing modular arithmetic.'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数论：在解决与数论相关的问题时，利用数学属性和高效算法至关重要。使用数学模块中的函数进行操作，如查找最大公约数（gcd）、最小公倍数（lcm）以及执行模运算。
- en: 'Best Practice: Use built-in functions and algorithms for common number theory
    operations. Optimize prime-related computations using the Sieve of Eratosthenes
    for prime generation and employ efficient algorithms for primality testing and
    factorization to handle large numbers effectively.'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最佳实践：对常见的数论操作使用内置函数和算法。使用埃拉托斯特尼筛法优化与素数相关的计算，并采用高效算法进行素性测试和因式分解，以有效处理大数。
- en: Appendix C Hyper-parameters in LLMs
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C LLMs 中的超参数
- en: 'Table 6: Hyper-Parameter of LLMs'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：LLMs 的超参数
- en: '| Parameter | Models |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 模型 |'
- en: '| --- | --- |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Engine |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 引擎 |'
- en: '&#124; gpt-4-turbo, gpt-4o, &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; gpt-4-turbo, gpt-4o, &#124;'
- en: '&#124; meta-llama-3-8b-instruct, &#124;'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; meta-llama-3-8b-instruct, &#124;'
- en: '&#124; open-mixtral-8x7b &#124;'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; open-mixtral-8x7b &#124;'
- en: '|'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Max Token | 4096 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 最大令牌 | 4096 |'
- en: '| n (responses) | 1 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| n（响应） | 1 |'
- en: '| Temperature | 0.7 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 温度 | 0.7 |'
- en: '| Top P | 1 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Top P | 1 |'
- en: '| Frequency Penalty | 0 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 频率惩罚 | 0 |'
- en: '| Presentation Penalty | 0.6 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 展示惩罚 | 0.6 |'
- en: 'In this research paper, we choose and compare three popular LLMs models: gpt-4-turbo,
    gpt-4o, meta-llama-3-8b-instruct and open-mixtral-8x7b. The details configurations
    can be found at Table [6](#A3.T6 "Table 6 ‣ Appendix C Hyper-parameters in LLMs
    ‣ Enhancing Computer Programming Education with LLMs: A Study on Effective Prompt
    Engineering for Python Code Generation"). We set the maximum token limit to 4096,
    allowing for extensive and detailed responses. Our experiments were conducted
    with a single response output ($n=1$), ensuring focused and specific replies and
    each LeetCode question will only generate one code solution. To balance creativity
    with relevance, we use the default temperature 0.7, a moderate setting that encourages
    a mix of predictable and innovative responses. The top_p parameter was set to
    1, enabling the model to consider the full range of possible next words. We applied
    a frequency penalty of 0.0 to prevent repetition and a presence penalty of 0.6,
    encouraging the model to introduce new concepts and ideas throughout the conversation.
    This configuration was pivotal in achieving the desired balance between coherence,
    relevance, and novelty in the model’s responses.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
