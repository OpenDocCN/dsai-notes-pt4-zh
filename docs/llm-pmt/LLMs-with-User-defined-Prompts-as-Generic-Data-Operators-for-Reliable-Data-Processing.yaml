- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:23'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.16351](https://ar5iv.labs.arxiv.org/html/2312.16351)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Luyi Ma^∗, Nikhil Thakurdesai^∗, Jiao Chen^∗,
  prefs: []
  type: TYPE_NORMAL
- en: Jianpeng Xu, Evren Korpeoglu, Sushant Kumar, Kannan Achan Personalization Team
  prefs: []
  type: TYPE_NORMAL
- en: Walmart Global Tech Sunnyvale, CA, USA
  prefs: []
  type: TYPE_NORMAL
- en: '{luyi.ma, nikhil.thakurdesai, jiao.chen0,'
  prefs: []
  type: TYPE_NORMAL
- en: jianpeng.xu, ekorpeoglu, sushant.kumar, kannan.achan}@walmart.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Data processing is one of the fundamental steps in machine learning pipelines
    to ensure data quality. Majority of the applications consider the user-defined
    function (UDF) design pattern for data processing in databases. Although the UDF
    design pattern introduces flexibility, reusability and scalability, the increasing
    demand on machine learning pipelines brings three new challenges to this design
    pattern – not low-code, not dependency-free and not knowledge-aware. To address
    these challenges, we propose a new design pattern that large language models (LLMs)
    could work as a generic data operator (LLM-GDO) for reliable data cleansing, transformation
    and modeling with their human-compatible performance. In the LLM-GDO design pattern,
    user-defined prompts (UDPs) are used to represent the data processing logic rather
    than implementations with a specific programming language. LLMs can be centrally
    maintained so users don’t have to manage the dependencies at the run-time. Fine-tuning
    LLMs with domain-specific data could enhance the performance on the domain-specific
    tasks which makes data processing knowledge-aware. We illustrate these advantages
    with examples in different data processing tasks. Furthermore, we summarize the
    challenges and opportunities introduced by LLMs to provide a complete view of
    this design pattern for more discussions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Models, Data Modeling, Data Cleansing, Data Transformations,
    Design Pattern^*^*footnotetext: Equal Contribution'
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning (ML) powers numerous data-driven applications for varied types
    of use cases. A typical machine learning pipeline consists of data processing,
    feature engineering, model selection, model training, hyper-parameters tuning,
    evaluation, testing, and serving [[1](#bib.bib1)]. Many of these steps not only
    require high-quality data to ensure the machine learning applications will perform
    as expected, but also prefer huge volume of data to support the training [[2](#bib.bib2)].
  prefs: []
  type: TYPE_NORMAL
- en: However, most of the reliable datasets were generated by human annotations.
    This process couldn’t scale up well as it is both expensive and time-consuming,
    limiting its further applications. Moreover, with the increasing parameters and
    complexity of machine learning models, huge volume of high-quality data are required
    for model training. The data processing tasks need to support the growing demand
    of effective data cleansing, transformation and modeling. Following the definition
    of a typical ETL (Extract, Transform, Load) process in data warehousing, our main
    focus is on the transform process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To support this growing demand of data transformation, user-defined functions
    (UDFs) are commonly used to clean, transform and model data in a data warehouse
    or a data lake [[3](#bib.bib3)]. A typical UDF template is shown in Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ LLMs with User-defined Prompts as Generic Data Operators
    for Reliable Data Processing")-(a) with a pythonic way of coding. Within the UDF,
    a user could import the run-time dependencies, implement the logic and process
    the input data. When applying the UDF on a database, following the classic narrow
    transformation setting in Spark, the UDF will be applied to each row of the data
    and the processed row will be stored ¹¹1https://www.databricks.com/glossary/what-are-transformations.
    The UDF design pattern introduces three advantages in large-scale data processing.
    First, it provides the flexibility for users to implement their own data processing
    logic that are not supported by built-in functions. Second, it abstracts the functionalities
    for better understanding, debugging and reusability (modular programming). Third,
    it can be easily scaled up by big data processing engine like Spark [[4](#bib.bib4)].
    With the above advantages, a user could implement an UDF with a programming language
    supported by the system (e.g., Python in a PySpark cluster built on a Hadoop file
    system), and apply the logic in parallel over all the records. However, this design
    pattern also meets increasing challenges. (1) Not low-code or zero-code: it requires
    the users to have substantial programming skills and experiences. (2) Not dependency-free:
    it could require a complicated run-time environment for different UDFs. Managing
    the dependencies is difficult in both development and deployment. For example,
    if the sets of run-time dependencies for two UDFs have no overlap, we need two
    separated pipeline to manage the dependencies. (3) Not knowledge-aware: it is
    difficult to natively incorporate prior knowledge for the data processing logics
    into current implementations of UDFs. Prior knowledge is usually task-specific.
    For instance, e-commerce item category classification requires a strong domain
    knowledge to identify the item attributes. It is difficult to implant this knowledge
    into UDFs deterministically for item classification due to the enormous combinations
    of item attributes.'
  prefs: []
  type: TYPE_NORMAL
- en: Recently, artificial intelligence (AI) development has been gaining a promising
    progress in the past years with the emergence of Large Language Models (LLMs).
    LLMs, such as Llama2 [[5](#bib.bib5)] and GPT-4 [[6](#bib.bib6)], show their effectiveness
    in solving a wide range of downstream tasks (e.g., question answering, multi-step
    reasoning etc.) due to the emergent abilities [[7](#bib.bib7)], which reduces
    the gap between natural language and programming. With well-designed prompts,
    a user without sufficient experience in data processing can easily employ an LLM
    to extract the aspects of products, which usually requires the knowledge of e-commerce
    domain experts [[8](#bib.bib8)]. This learning ability, which only relies on natural
    language instructions and input-output examples, and without optimizing any parameters,
    is called in-context learning [[9](#bib.bib9)]. This in-context learning ability
    advances LLMs to understand few-shots even zero-shot learning tasks, for instance,
    classifying the tabular[[10](#bib.bib10)] and anomaly detection in system logs
    [[11](#bib.bib11)]. Similar to other pre-trained models, LLMs’ performance could
    be further improved by fine-tuning with different techniques, for example, LoRA
    [[12](#bib.bib12)] and QLora [[13](#bib.bib13)] can efficiently fine-tune LLMs
    by optimizing the rank decomposition matrices of the dense layers in a neural
    network. The fine-tuned LLMs could achieve human-compatible results in many tasks
    [[6](#bib.bib6)][[14](#bib.bib14)], greatly reducing the human effort in labeling
    and annotation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/563d129faf454a84d16b5538d7c587df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: (a) UDF design pattern and (b) LLM-GDO design pattern.'
  prefs: []
  type: TYPE_NORMAL
- en: In our paper, we address the limitations in the current UDF-based data processing
    practice and summarize a new design pattern for data processing involving LLMs
    and user-defined prompts (UDPs) to balance flexibility and human-level accuracy.
    We propose a design pattern that LLMs with UDPs could work as Generic Data Operators
    (LLM-GDOs) for data cleansing, transformation and modeling. To visualize this
    design pattern, we show an example of LLM-GDO in Figure [1](#S1.F1 "Figure 1 ‣
    I Introduction ‣ LLMs with User-defined Prompts as Generic Data Operators for
    Reliable Data Processing")-(b). In LLM-GDO design, we simplify the UDF with two
    changes. First, instead of defining a programming-language-based UDF, a user could
    define a prompt (or a prompt template for better representation) ‘user_defined_prompt’
    to describe the data processing logic (low-code and zero-code). When the data
    distribution is changed, instead of updating the processing code, UDPs can update
    the data processing logic easily with modifying the instructions and examples
    in prompts. Unlike an UDF which requires run-time dependencies to support the
    execution, an LLM (pre-trained or fine-tuned) could work as a compiler for the
    prompt and execute the request independently (dependency free). As we use the
    same LLM with a proper version control, we can align the offline development and
    online serving. When processing data, the database talks with the remote LLM resource
    via LLM gateways (e.g., APIs or Agents) behind which LLMs are maintained to process
    the requests. We abstract this function as ‘llm_call’ and the implementation details
    could be done by platforms and encapsulated well from users. By fine-tuning LLMs,
    we can seamlessly introduce the domain-specific knowledge into LLMs with a small
    dataset and enhance their performance on these tasks (knowledge-aware). Although
    LLMs are versatile, they also have limitations which we should keep improving.
    To provide a complete view of this design pattern, We foresee the challenges in
    this design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we introduce the design pattern LLM-GDO in the big data setting for ML pipelines.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we summarize the potential applications with LLM-GDOs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: we discuss the challenges and opportunities in LLM-GDO.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The paper is structured as follows. We will introduce the key concepts in Section
    [II](#S2 "II Preliminaries ‣ LLMs with User-defined Prompts as Generic Data Operators
    for Reliable Data Processing") and present a comprehensive comparison between
    UDFs and LLM-GDOs in Section [III](#S3 "III Methodology ‣ LLMs with User-defined
    Prompts as Generic Data Operators for Reliable Data Processing"), followed with
    challenges and opportunities in Section [IV](#S4 "IV Discussion ‣ LLMs with User-defined
    Prompts as Generic Data Operators for Reliable Data Processing"). Finally we conclude
    our paper in Section [V](#S5 "V Conclusion ‣ LLMs with User-defined Prompts as
    Generic Data Operators for Reliable Data Processing").
  prefs: []
  type: TYPE_NORMAL
- en: II Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-1 Narrow Transformations and Wide Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data transformations are instructions of modifying the rows of database. In
    Spark, depending on the dependencies between data points, data transformations
    could be grouped into narrow transformations (Figure [2](#S2.F2 "Figure 2 ‣ II-1
    Narrow Transformations and Wide Transformations ‣ II Preliminaries ‣ LLMs with
    User-defined Prompts as Generic Data Operators for Reliable Data Processing")-(a))
    and wide transformations (Figure [2](#S2.F2 "Figure 2 ‣ II-1 Narrow Transformations
    and Wide Transformations ‣ II Preliminaries ‣ LLMs with User-defined Prompts as
    Generic Data Operators for Reliable Data Processing")-(b)). Typically, the output
    of a narrow transformation operation depends on only one input data (no data shuffling),
    while output of a wide transformation depends on multiple input rows (with data
    shuffling). In our paper, we focus on the narrow transformations as they are dominant
    in many early-stage data processing steps to improve the data quality.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ce35db91bb3044ff708e4ba6e20a199.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: (a) Narrow Transformations and (b) Wide Transformations'
  prefs: []
  type: TYPE_NORMAL
- en: II-2 LLMs Function calling and Fine-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are many ways of accessing LLMs, e.g. OpenAI API ²²2https://platform.openai.com/docs/api-reference
    provides one of the popular solutions. In our paper, we assume that the LLM’s
    gateways are predominantly maintained by the platforms (remote or local). In Figure
    [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ LLMs with User-defined Prompts as Generic
    Data Operators for Reliable Data Processing")-(b), we abstract the LLM function
    calling by the ‘llm_call’ function. This function takes the formatted UDP and
    returns the processed output. LLMs could be fine-tuned by a small sample of high-quality
    data. As new data flows into the database, the platform could seamlessly conduct
    fine-tuning of LLMs by extracting a small sample of high-quality data. Although
    there are still many challenges with LLM fine-tuning, it is beyond the scope of
    this discussion.
  prefs: []
  type: TYPE_NORMAL
- en: III Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present a list of tasks where LLM-GDOs could improve UDFs.
    Note that LLMs are still evolving so we mainly focus on the design pattern in
    this section to address the connection between UDFs and LLM-GDOs. We will provide
    comprehensive discussion about the challenges of current LLM-GDO design in Section
    [IV](#S4 "IV Discussion ‣ LLMs with User-defined Prompts as Generic Data Operators
    for Reliable Data Processing"). For brevity, we reuse the definition of ‘user_defined_function’
    in Figure [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ LLMs with User-defined Prompts
    as Generic Data Operators for Reliable Data Processing")-(b) in the following
    case studies. All the example LLM outputs in this paper have been generated using
    Chat-GPT 3.5 [[15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8144c4cfe28920898fe3fa379cc06139.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: A sample table ‘item_rating’ with columns ‘item_id’, ‘user_id’, ‘user_rating’
    and ‘date’.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Data Cleansing and Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data cleansing and transformation are crucial steps to ensure the performance
    of a ML pipeline. To compare UDFs and LLM-GDOs in data cleansing and transformation,
    we consider a sample table ‘item_rating’ defined in Figure [3](#S3.F3 "Figure
    3 ‣ III Methodology ‣ LLMs with User-defined Prompts as Generic Data Operators
    for Reliable Data Processing") with the following three tasks. They illustrate
    the low-code feature and dependency-free feature of LLM-GDO.
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 Data Structural Consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Data structural consistency is usually one of the initial steps. It helps to
    structuralize the data to improve the downstream transformations. In the ‘item_rating’
    table (Figure [3](#S3.F3 "Figure 3 ‣ III Methodology ‣ LLMs with User-defined
    Prompts as Generic Data Operators for Reliable Data Processing")), the ‘date’
    column contains date strings in different formats. Figure [4](#S3.F4 "Figure 4
    ‣ III-A1 Data Structural Consistency ‣ III-A Data Cleansing and Transformation
    ‣ III Methodology ‣ LLMs with User-defined Prompts as Generic Data Operators for
    Reliable Data Processing") shows an example to structuralization the date data.
    With LLM-GDO in UDF, we can define the output format (YYYYMMDD) in the prompt
    and let LLMs handle the data processing (Figure [4](#S3.F4 "Figure 4 ‣ III-A1
    Data Structural Consistency ‣ III-A Data Cleansing and Transformation ‣ III Methodology
    ‣ LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing")-(b)).
    Traditional UDFs can also complete this structuralization but it usually requires
    either enumeration of the date format or leverage different packages.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76f8ab3fe6be87eaf25df1e62df64e6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: LLM-GDO-based data transformation for data structural consistency.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 Data Type Conversion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After cleaning the data structure with better structural consistency, we can
    convert the data from one type to another. Figure [5](#S3.F5 "Figure 5 ‣ III-A2
    Data Type Conversion ‣ III-A Data Cleansing and Transformation ‣ III Methodology
    ‣ LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing")
    presents an example of UNIX epoch time conversion from the given date data. Again,
    the LLM-GDO completes this data transformation with the instruction in the prompt,
    while the traditional UDF users need to understand the definition of UNIX epoch
    time for implementation or know the right packages to call.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2706ad26a84042def60469c799e4b09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: LLM-GDO-baesd data transformation for data type conversion.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A3 Data Standardization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another important task is normalizing the numeric data. In many machine learning
    pipelines, normalization of numeric values increases the stability of the model
    training. In Figure [6](#S3.F6 "Figure 6 ‣ III-A3 Data Standardization ‣ III-A
    Data Cleansing and Transformation ‣ III Methodology ‣ LLMs with User-defined Prompts
    as Generic Data Operators for Reliable Data Processing"), we present an example
    wehre LLM-GDO is employed to normalize the ‘user_rating’ column by providing the
    rating range .
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a2e634e3f803f21f55e8e4e46f24a17b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: LLM-GDO-baesd data transformation for standardization.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Data Modeling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to data cleansing and transformation, many feature engineering steps
    require machine learning models to conduct the basic reasoning and generate high-quality
    features. For example, in many NLP tasks, word annotations and tagging are important
    features for modeling [[16](#bib.bib16)] [[17](#bib.bib17)]. These features usually
    require a dedicated machine learning pipeline to extract the features, which are
    inconvenient to maintain without a proper support from ML operations. A user needs
    to understand the machine learning pipeline in detail to run an UDF to employ
    these complicated models, managing the dependencies and another layer of data
    processing. LLM-GDOs, however, can keep the same convenience with a uniform practice.
    We consider a new sample table ‘item_information’ defined in Figure [7](#S3.F7
    "Figure 7 ‣ III-B Data Modeling ‣ III Methodology ‣ LLMs with User-defined Prompts
    as Generic Data Operators for Reliable Data Processing") for item information.
    We highlight this advantage with the following two tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9f623737f32b0fc0e9cc6018bbd8ee05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: A sample table ‘item_information’ with columns ’item_id’, ‘item_types’
    and ‘item_name’.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Reasoning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this task, we need to parse each row of data and conduct reasoning (e.g.,
    classification) to get the output. One of the common use cases is to detect the
    anomaly value in the database. For example, in many system log databases, the
    system error messages are grouped under several categories by their severity levels
    or the types of events. Another example is that items are grouped into product
    types in e-commerce for item display. The classification could be invalid but
    the anomaly is hard to find due to the huge data volume. Anomaly detection could
    be done by separate machine learning pipelines [[18](#bib.bib18)] but integrating
    them into database is challenging. First, these machine learning pipelines have
    different dependencies and require domain knowledge. Second, as the underlying
    data changes, the models might not be updated without a proper orchestration system
    for model retraining. LLM-GDO can conduct reasoning to detect the anomaly in this
    case. Figure [8](#S3.F8 "Figure 8 ‣ III-B1 Reasoning ‣ III-B Data Modeling ‣ III
    Methodology ‣ LLMs with User-defined Prompts as Generic Data Operators for Reliable
    Data Processing") shows an example of LLM-GDO for anomaly detection in the ‘item_information’
    table. We can see that LLM-GDO can easily detect the wrong item type for the ‘103’
    item in Figure [7](#S3.F7 "Figure 7 ‣ III-B Data Modeling ‣ III Methodology ‣
    LLMs with User-defined Prompts as Generic Data Operators for Reliable Data Processing")
    and correct it in Figure [8](#S3.F8 "Figure 8 ‣ III-B1 Reasoning ‣ III-B Data
    Modeling ‣ III Methodology ‣ LLMs with User-defined Prompts as Generic Data Operators
    for Reliable Data Processing")-(b). In this prompt, we can leverage the in-context
    learning of LLM by designing a prompt [[10](#bib.bib10)]. Compared with the traditional
    UDF, users have to define the complex heuristic or different versions of deep
    learning models to complete the task. The LLM-GDO simplify the development of
    the logic and maintenance because LLMs at the back-end could be managed in a centralized
    way which powers the tasks without disclosing the details to users.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6994b3412fac4b52694a70c02a44910.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An LLM-GDO application for data reasoning and anomaly detection on
    item type classification.'
  prefs: []
  type: TYPE_NORMAL
- en: III-B2 Embedding Database
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Embedding is another important use case for data processing. With LLM-GDO, we
    can also generate high-quality embedding based on the ‘item_name’ column ³³3https://platform.openai.com/docs/guides/embeddings.
    Due to the limited space, we skip the prompt example and the output embeddings.
    However, with this approach, we can timely update the embedding for downstream
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: IV Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the remarkable human-compatible and programmable performance of LLMs
    in data transformation, it is crucial to highlight the numerous opportunities
    and challenges present in the LLM-GDO design pattern. This will help to shed light
    on potential future research directions and enhance our understanding of this
    evolving field.
  prefs: []
  type: TYPE_NORMAL
- en: LLM inference and Scalability
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Although LLMs demonstrate greater scalability compared to traditional UDFs,
    they demand significantly more computational resources. This can be attributed
    to the massive number of parameters in LLMs, which require extensive computational
    resources for model inference. For instance, a GPT-3-sized LLM requires the use
    of eight 80G A100 GPUs for inference⁴⁴4https://blogs.oracle.com/research/post/oracle-first-to-finetune-gpt3-sized-ai-models-with-nvidia-a100-gpu.
    Moreover, current commercial LLMs, such as GPT-3.5 and PaLM2, have limited speed
    in response. Nonetheless, research on LLM knowledge distillation[[19](#bib.bib19)]
    and compression[[20](#bib.bib20)] are promising avenues for reducing LLM size
    and computational demands, thereby making them more cost-effective, efficient,
    and scalable.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Hallucination
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Hallucination means a LLM makes up contents that do not exist in real world
    or do not satisfy the requirements in the prompt. While hallucination can be a
    feature for many creative use cases, it is usually an obstacle for data processing
    [[21](#bib.bib21)] . For instance, when the desired output data format is tab-separated
    values (TSV), it is crucial to avoid inconsistencies, such as the inclusion of
    alternative separators, particularly when employing an LLM within a production
    data engineering pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Various methods have been proposed to mitigate and quantify hallucinations in
    LLMs, including reducing the model’s temperature, providing more context in the
    prompt, employing a chain-of-thought approach [[22](#bib.bib22)], ensuring self-consistency
    [[23](#bib.bib23)], or specifying more precise formatting requirements within
    the prompt. Despite these efforts, the effective detection and prevention of hallucinations
    in LLMs during data processing remains a formidable challenge.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Unit Testing and Evaluation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Traditional UDFs are typically deterministic, allowing for the development of
    unit tests to assess their correctness. In contrast, LLMs employ a generative
    approach to produce results based on probability [[24](#bib.bib24)], making it
    more challenging to conduct universal testing. Recently, some teams have leveraged
    the larger LLMs to generate the unit test cases (e.g., expected outputs) for the
    prospective LLMs ⁵⁵5https://www.anyscale.com/blog/fine-tuning-llama-2-a-comprehensive-case-study-for-tailoring-models-to-unique-applications.
    We believe LLM unit testing will attract more attentions in LLM development and
    application.
  prefs: []
  type: TYPE_NORMAL
- en: LLM Privacy
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Fine-tuning an industry-level LLM for data processing could depend on data from
    different departments, involving data transition and sharing. For example, when
    processing data from a social app, customers’ profile and preference are highly
    sensitive. Data sharing will increase the probability of data leaks and violate
    the data privacy policy [[25](#bib.bib25)]. Recent works in federated learning
    [[26](#bib.bib26)] address the privacy issues in LLMs. We expect more research
    interests on LLM privacy issues.
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this research paper, we propose a novel design pattern, LLM-GDO, aimed at
    improving the efficiency and reliability of data transformation. LLM-GDO offers
    the benefits of utilizing low-code and dependency-free implementations for knowledge-aware
    data processing. However, it also encounters challenges stemming from LLMs. We
    examine these challenges and opportunities, and provide an in-depth perspective
    on the LLM-GDO design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Zaharia, A. Chen, A. Davidson, A. Ghodsi, S. A. Hong, A. Konwinski,
    S. Murching, T. Nykodym, P. Ogilvie, M. Parkhe *et al.*, “Accelerating the machine
    learning lifecycle with mlflow.” *IEEE Data Eng. Bull.*, vol. 41, no. 4, pp. 39–45,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE conference on computer
    vision and pattern recognition*.   Ieee, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Nambiar and D. Mundra, “An overview of data warehouse and data lake
    in modern enterprise data management,” *Big Data and Cognitive Computing*, vol. 6,
    no. 4, p. 132, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. Zaharia, R. S. Xin, P. Wendell, T. Das, M. Armbrust, A. Dave, X. Meng,
    J. Rosen, S. Venkataraman, M. J. Franklin *et al.*, “Apache spark: a unified engine
    for big data processing,” *Communications of the ACM*, vol. 59, no. 11, pp. 56–65,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale *et al.*, “Llama 2: Open foundation and fine-tuned
    chat models,” *arXiv preprint arXiv:2307.09288*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] OpenAI, “Gpt-4 technical report,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] J. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama,
    M. Bosma, D. Zhou, D. Metzler *et al.*, “Emergent abilities of large language
    models,” *arXiv preprint arXiv:2206.07682*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] R. Y. Maragheh, L. Morishetti, R. Giahi, K. Nag, J. Xu, J. Cho, E. Korpeoglu,
    S. Kumar, and K. Achan, “Llm-based aspect augmentations for recommendation systems,”
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell *et al.*, “Language models are few-shot learners,”
    *Advances in neural information processing systems*, vol. 33, pp. 1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. Hegselmann, A. Buendia, H. Lang, M. Agrawal, X. Jiang, and D. Sontag,
    “Tabllm: Few-shot classification of tabular data with large language models,”
    in *International Conference on Artificial Intelligence and Statistics*.   PMLR,
    2023, pp. 5549–5581.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Qi, S. Huang, Z. Luan, C. Fung, H. Yang, and D. Qian, “Loggpt: Exploring
    chatgpt for log-based anomaly detection,” *arXiv preprint arXiv:2309.01189*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and
    W. Chen, “Lora: Low-rank adaptation of large language models,” *arXiv preprint
    arXiv:2106.09685*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient
    finetuning of quantized llms,” *arXiv preprint arXiv:2305.14314*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. Chen, L. Ma, X. Li, N. Thakurdesai, J. Xu, J. H. Cho, K. Nag, E. Korpeoglu,
    S. Kumar, and K. Achan, “Knowledge graph completion models are few-shot learners:
    An empirical study of relation labeling in e-commerce with llms,” *arXiv preprint
    arXiv:2305.09858*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] OpenAI, “Chatgpt 3.5 turbo,” OpenAI API, 2022, accessed: [Oct 25, 2023].
    [Online]. Available: https://openai.com'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] O. Owoputi, B. O’Connor, C. Dyer, K. Gimpel, N. Schneider, and N. A. Smith,
    “Improved part-of-speech tagging for online conversational text with word clusters,”
    in *Proceedings of the 2013 conference of the North American chapter of the association
    for computational linguistics: human language technologies*, 2013, pp. 380–390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] H. Hu, C. Zhang, Y. Luo, Y. Wang, J. Han, and E. Ding, “Wordsup: Exploiting
    word annotations for character based text detection,” in *Proceedings of the IEEE
    international conference on computer vision*, 2017, pp. 4940–4949.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] G. Pang, C. Shen, L. Cao, and A. V. D. Hengel, “Deep learning for anomaly
    detection: A review,” *ACM computing surveys (CSUR)*, vol. 54, no. 2, pp. 1–38,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] C.-Y. Hsieh, C.-L. Li, C.-K. Yeh, H. Nakhost, Y. Fujii, A. Ratner, R. Krishna,
    C.-Y. Lee, and T. Pfister, “Distilling step-by-step! outperforming larger language
    models with less training data and smaller model sizes,” *arXiv preprint arXiv:2305.02301*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] X. Zhu, J. Li, Y. Liu, C. Ma, and W. Wang, “A survey on model compression
    for large language models,” *arXiv preprint arXiv:2308.07633*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. Zhang, O. Press, W. Merrill, A. Liu, and N. A. Smith, “How language
    model hallucinations can snowball,” *arXiv preprint arXiv:2305.13534*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Z. Zhang, A. Zhang, M. Li, and A. Smola, “Automatic chain of thought prompting
    in large language models,” *arXiv preprint arXiv:2210.03493*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han, “Large
    language models can self-improve,” *arXiv preprint arXiv:2210.11610*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang, X. Yi, C. Wang,
    Y. Wang *et al.*, “A survey on evaluation of large language models,” *arXiv preprint
    arXiv:2307.03109*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Ghayyur, J. Averitt, E. Lin, E. Wallace, A. Deshpande, and H. Luthi,
    “Panel: Privacy challenges and opportunities in LLM-Based chatbot applications.”   Santa
    Clara, CA: USENIX Association, Sep. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] T. Fan, Y. Kang, G. Ma, W. Chen, W. Wei, L. Fan, and Q. Yang, “Fate-llm:
    A industrial grade federated learning framework for large language models,” *arXiv
    preprint arXiv:2310.10049*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
