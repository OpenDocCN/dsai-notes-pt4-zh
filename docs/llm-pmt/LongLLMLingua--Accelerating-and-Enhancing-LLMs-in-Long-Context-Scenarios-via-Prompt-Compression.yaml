- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:52'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.06839](https://ar5iv.labs.arxiv.org/html/2310.06839)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Huiqiang Jiang, Qianhui Wu, Xufang Luo,
  prefs: []
  type: TYPE_NORMAL
- en: Dongsheng Li, Chin-Yew Lin, Yuqing Yang, Lili Qiu
  prefs: []
  type: TYPE_NORMAL
- en: Microsoft Corporation {hjiang,qianhuiwu,xufang.luo}@microsoft.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In long context scenarios, large language models (LLMs) face three main challenges:
    higher computational/financial cost, longer latency, and inferior performance.
    Some studies reveal that the performance of LLMs depends on both the density and
    the position of the key information (question relevant) in the input prompt. Inspired
    by these findings, we propose LongLLMLingua for prompt compression towards improving
    LLMs’ perception of the key information to simultaneously address the three challenges.
    We conduct evaluation on a wide range of long context scenarios including single-/multi-document
    QA, few-shot learning, summarization, synthetic tasks, and code completion. The
    experimental results show that LongLLMLingua compressed prompt can derive higher
    performance with much less cost. The latency of the end-to-end system is also
    reduced. For example, on NaturalQuestions benchmark, LongLLMLingua gains a performance
    boost of up to 17.1% over the original prompt with $\sim$10k tokens at a compression
    rate of 2x-10x, LongLLMLingua can speed up the end-to-end latency by 1.4x-3.8x.
    ¹¹1Our code is available at [https://aka.ms/LLMLingua](https://aka.ms/LLMLingua).'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: ChatGPT and other large language models (LLMs) have revolutionized user-oriented
    language technologies and are serving as crucial components in more and more applications.
    Carefully designing prompts is necessary to achieve better performance in specific
    downstream tasks. The commonly used technologies such as In-Context Learning (ICL) (Dong
    et al., [2023](#bib.bib8)), Retrieval Augment Generation (RAG) (Lewis et al.,
    [2020](#bib.bib17)), and Agent (Park et al., [2023](#bib.bib24)) are driving prompts
    to be increasingly longer, even reaching thousands of tokens. Scenarios such as
    multi-document question answering, code completion, and document summarization
    also necessitate the processing of long contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main challenges when LLMs are used in long context scenarios:
    (1) The higher computational and financial cost required to run these models or
    to call APIs from companies providing LLM services. This can be a significant
    barrier for individuals or smaller organizations with limited resources. (2) The
    longer latency associated with LLMs, which can cause delays in generating responses
    or predictions and is particularly problematic in real-time scenarios where users
    expect quick and accurate responses. (3) The inferior performance caused by the
    extended window size of LLMs (Xiong et al., [2023](#bib.bib33)), and the low density
    as well as the less sensitive position of the question-relevant key information
    in the prompt. Figure [1a](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    shows that LLMs’ performance in downstream tasks may decrease as the noisy information
    in the prompt increases (Shi et al., [2023](#bib.bib29)). Moreover, the purple
    curve in Figure [1b](#S1.F1.sf2 "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua:
    Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression")
    indicates that LLMs’ ability to capture the relevant information depends on their
    positions in the prompt (Liu et al., [2023](#bib.bib20)): they achieve the highest
    performance when relevant information occurs at the beginning or end of the input
    context, and significantly degrades if relevant information is located in the
    middle of long contexts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/67837a1d6c05d9a22f6dc39a8f5d5273.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Performance v.s. Document Number
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/808a8cc98f1873ee2c344faffaffa60d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Performance v.s. Key Information Position
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: (a) LLMs’ performance in downstream tasks may decrease as the noisy
    information in the prompt increases. In this case, we keep $k$ implies more noise
    introduced into the prompt. To improve the key information density in the prompt,
    we present question-aware coarse-to-fine compression. (b) LLMs’ ability to capture
    the relevant information depends on their positions in the prompt. To reduce information
    loss in the middle, we introduce a document reordering mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspired by these observations, we propose LongLLMLingua to address the three
    challenges. Specifically, we use the advanced while efficient LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) as our backbone framework for prompt compression
    to address the first two challenges, i.e., reduce cost and latency. However, in
    the case of long contexts, the distribution of question-relevant key information
    in the prompt is generally sparse. Existing prompt compression methods like LLMLingua (Jiang
    et al., [2023a](#bib.bib13)) and Selective-Context (Li, [2023](#bib.bib19)) that
    do not consider the content of the question during compression may retain too
    much noisy information in the compressed results, leading to inferior performance.
    In this paper, LongLLMLingua is designed to enhance LLM’s perception of key information
    (relevant to the question) in the prompt, so that the third challenge of inferior
    performance in long context scenarios could be addressed. Figure [1b](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression") is an example. The underlying
    principle of LongLLMLingua is that small language models are inherently capable
    of capturing the distribution of key information relevant to a given question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions are five-fold: (1) We propose a question-aware coarse-to-fine
    compression method to improve the key information density in the prompt (Sec.
    [4.1](#S4.SS1 "4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression")); (2) We introduce a document reordering mechanism to reduce
    information loss in the middle. (Sec. [4.2](#S4.SS2 "4.2 How to reduce information
    loss in the middle? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing
    LLMs in Long Context Scenarios via Prompt Compression")); (3) We present dynamic
    compression ratios to bridge the coarse-grained compression and fine-grained compression
    for adaptive granular control (Sec. [4.3](#S4.SS3 "4.3 How to achieve adaptive
    granular control during compression? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression")); (4) We
    propose a post-compression subsequence recovery strategy to improve the integrity
    of the key information ([4.4](#S4.SS4 "4.4 How to improve the integrity of key
    information? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating and Enhancing LLMs
    in Long Context Scenarios via Prompt Compression")). (5) We evaluate LongLLMLingua
    on three benchmarks, i.e., NaturalQuestions (Liu et al., [2023](#bib.bib20)),
    LongBench (Bai et al., [2023](#bib.bib1)), and ZeroSCROLLS (Shaham et al., [2023](#bib.bib28)).
    Experimental results demonstrate that compared with original prompts, LongLLMLingua
    compressed prompts can achieve higher performance with much lower costs. The latency
    of the end-to-end system is also reduced.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem Formulation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Following LLMLingua (Jiang et al., [2023a](#bib.bib13)), we use $\mathbf{x}=(\mathbf{x}^{\text{ins}},\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K},\mathbf{x}^{\text{que}})$.
    The objective of a prompt compression system can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\widetilde{\mathbf{x}}}D\left(\mathbf{y},\widetilde{\mathbf{y}}\right)+\lambda\lVert\widetilde{\mathbf{x}}\rVert_{0},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\widetilde{\mathbf{x}}$ for joint optimization.
  prefs: []
  type: TYPE_NORMAL
- en: '3 Preliminary: LLMLingua'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMLingua (Jiang et al., [2023a](#bib.bib13)) uses a small language model $\mathcal{M}_{S}$
    used for prompt compression.
  prefs: []
  type: TYPE_NORMAL
- en: 4 LongLLMLingua
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3ed1fa5e5389e25042c3fe0e36321ebe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Framework of LongLLMLingua. Gray Italic content: As in LLMLingua.'
  prefs: []
  type: TYPE_NORMAL
- en: LongLLMLingua is developed upon the framework of LLMLingua towards prompt compression
    in long context scenarios. The primary challenge in long context scenarios is
    how to enhance LLM’s perception of key information relevant to the question in
    the prompt. LongLLMLingua addresses this challenge from three perspectives, and
    further applies a subsequence recovery strategy to improve the accuracy and reliability
    of the information provided to users. We elaborate on each component in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 How to improve key information density in the prompt?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Question-Aware Coarse-Grained Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In coarse-grained compression, we aim to figure out a metric $r_{k}$ as the
    intermediate compressed results.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLMLingua uses document-level perplexity to represent the importance of documents:
    $r_{k}=1/N_{k}\sum_{i}^{N_{k}}p(x_{k,i}^{\text{doc}})\log p(x_{k,i}^{\text{doc}}),k\in\{1,2,\cdots,K\}$
    and instead become noise, reducing key information density in the compressed results
    and bringing difficulties for LLM to output correct answers. As shown in Figure [3a](#S4.F3.sf1
    "In Figure 3 ‣ Question-Aware Coarse-Grained Compression ‣ 4.1 How to improve
    key information density in the prompt? ‣ 4 LongLLMLingua ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression"), the recall@16
    of LLMLingua only reaches 50%, indicating its incompetence in retaining key information
    during compression.'
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-based methods are also feasible here. We can use $\mathbf{x}^{\text{que}}$75%
    accuracy in recall@5, which implies that the final accuracy upper bound of LLMs
    with 4x compression is only 75%.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e6955b6f2f232c285024fef939bddac.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Recall Distribution
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c40ed1ba8bcaf2e7730050901753b27c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Perplexity Distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: (a) Comparison of recall on NaturalQuestions Multi-documemnt QA dataset.
    (b) Comparison between perplexities and contrastive perplexities of tokens in
    the prompt from Multi-documemnt QA dataset. The document with the ground truth
    is located on the left side of the dashed line.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One approach to improve key information density in the compressed results is
    to calculate document-level perplexity conditioned on the question $\mathbf{x}^{\text{que}}$.
    It can be regarded as a regularization term that mitigates the impact of hallucinations.
    This can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $x^{\text{que},\text{restrict}}_{i}$ in the number of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3a](#S4.F3.sf1 "In Figure 3 ‣ Question-Aware Coarse-Grained Compression
    ‣ 4.1 How to improve key information density in the prompt? ‣ 4 LongLLMLingua
    ‣ LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via
    Prompt Compression") demonstrates that our coarse-level compression approach achieves
    the highest recall with different numbers of retained documents, suggesting that
    it preserves the most key information from the documents $(\mathbf{x}^{\text{doc}}_{1},\cdots,\mathbf{x}^{\text{doc}}_{K})$
    in the compressed results.'
  prefs: []
  type: TYPE_NORMAL
- en: Question-Aware Fine-Grained Compression
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In fine-grained compression, we assess the importance of each token in the instruction
    $\mathbf{x}^{\text{ins}}$, so that the compressed results could contain more question-relevant
    key information.
  prefs: []
  type: TYPE_NORMAL
- en: 'A straightforward solution for the awareness of $\mathbf{x}^{\text{que}}$ can
    be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{i}=\text{perplexity}(x_{i}&#124;x_{Ours
    w/o Token-level Question-aware: Compressed Prompt: Write a high-quality answer
    for the given question using only the provided search results (some of which might
    be irrelevant). Document [1](: Physics)gen,, who received2K, which is ,73,0 in0\.
    Johnen only to twice6\. Mariaie won, for.g was, until1estate he. Two:Mayer (1963).
    As of 2017, the prize has been awarded Question: who got the first nobel prize
    in physics Answer: LLMs’ Response: No answer found in the given search results.
    Ours w/ Token-level Question-aware: Compressed Prompt: Write a
    high-quality answer for the given question using only the provided search results
    (some of which might be irrelevant). 1Title: List of Nobelates in The first Nobel
    Prize was1 to , of who received 1582
    which,70 in0 en the prize. Skska also won two Nobeles for physics3g01, theate
    he women prize:ertMayer (1963). As of 2017, the prize has been awarded Question:
    who got the first nobel prize in physics Answer: LLMs’ Response: Wilhelmrad LLMs’
    Response after Subsquence Recovery: Wilhelm Conrad Röntgen Ground Truth: Wilhelm
    Conrad Röntgen'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Comparing the compressed prompt and LLMs’ response before and after
    using Question-aware Fine-grained Compression and Subsequence Recovery($1/\tau$=30x,
    high compression ratio setting) from NaturalQuestions Multi-document QA (Liu et al.,
    [2023](#bib.bib20)) using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Economic Cost
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '|  | Multi-document QA | LongBench | ZeroScolls |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original | 4.6 | 31.5 | 30.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Ours | 1.3 | 3.0 | 3.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 7: The inference costs(per 1,000 samples $) for various datasets using
    GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [7](#A4.F7 "Figure 7 ‣ Appendix D Economic Cost ‣ LongLLMLingua: Accelerating
    and Enhancing LLMs in Long Context Scenarios via Prompt Compression") presents
    the estimated per 1,000 samples inference costs for various datasets, encompassing
    input prompts and generated output text, based on GPT-3.5-Turbo pricing^(14)^(14)14https://openai.com/pricing.
    Our approach demonstrates substantial savings in computational resources and monetary
    expenses, particularly in long context situations. Cost reductions of $3.3, $28.5,
    and $27.4 per 1,000 samples are observed for Multi-document QA, LongBench, and
    ZeroScrolls, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Cases Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Original
    Prompt: … Document [1](Title: Dancing on Ice) It was confirmed on 25 January 2018,
    that Dancing on Ice had been recommissioned for an eleventh series to air in 2019.
    … Compressed Prompt: Write a high-quality answer for the given question using
    only the provided search results (some of which might be irrelevant). 1Title:
    Dancing on was confirmed on 2 January 2018 that Dancing on had been recommissioned
    for an eleventh series air in . Document
    [2Title: Dan on) Dan on Ice Dancing on British presented by Phillip Schof alongside
    Holly Willough from 26 to 2011, and Christine Bleakley from 2012 to 204 The show
    consists of celebrit and professional partners figure skating in front of a panel
    of judges The, broadcast on ITV, started on January 2006 and ended on 9 March
    2014 after showćontract not renewed by ITV On 4 September 2017, it was announced
    that rev series would on I 7 January 201 Sch and Willby returning as a 5(: on
    ( on () The third series of a from January to168TV. The from Saturdays, with Holby
    present Kar,y Sliner Robin Cins returned to Panel”, with Ruth H joining the panel
    as replacement for Natalia Bestova. The commission of the was confirmed by at
    the07 announcedova depart the series Robinen Bar,ater and Jasoniner announced
    7( on ( )) Dan 2 second of Dan on a from January to1207 ITV It presented Phillip
    Sch Holly Willough, and judged the ”I P consisting Nicky Slater, Nataliaian Karenres
    Jason Gardiner Karen Barber and Robin Cousins Jaynevill and Christopher Dean co
    and trained the contestants In this series, cele to ten in first series. The series
    was won former Kyran Bracken, with Mel Lambert the winner. It announced thatenresge
    Document []( on Ice on 08 on TV edition started 8 TV2 The Russian version ”анду)
    being on channel0, and renamed in8 to ” Ice” (). Its counterpart called ”Ice Age
    (, ”Stars on Ice on Channel Oneak IceHviezdyľJ. The Turkish version” is called
    Dans” (”ance on Document1 on Ice its, all,é () and Sje Chris de In series.2 edition
    ](: on Ice world) Dan Ice is a made competition world format, and been subsequently
    Italy Chile where titled after series There have a, the show was broadcast on
    Channel 13 as a Document [17](Title: Dancing on Ice) the insight to the training
    of the celebrities over the last week. It was presented by television presenter
    Ben Shephard and former contestant and ”Loose Women” star Coleen Nolan. The show
    was broadcast from 8 pm to 8.30 pm on Friday evenings on ITV throughout the duration
    of the main shows season. STV who broadcast the main show did not broadcast this
    on the Friday evening but after repeating the previous weekś main show on the
    following Saturday afternoon. Due to poor ratings, ”Dancing on Ice Friday” was
    axed prior to the 2011 series. The show was based in the Question: when is dancing
    on ice on the tv Answer: LLMs’ Response: 209 LLMs’ Response after Subsquence Recovery:
    2019 Ground Truth: 2019'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Cases study on NaturalQuestions Multi-document QA dataset (Liu et al.,
    [2023](#bib.bib20)) in 4x constraint using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compressed
    Prompt: Please complete the code given below.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next line of code: LLMs’ Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Ground Truth:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Zero-shot LLMs’ Response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Cases study on lcc code completion task in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compressed
    Prompt: Please the of the question. questions are sometimes your cold but the
    of you isnt:ason: What food hasges:: Who the first coach the Clevelandns What
    arch the Placede: Other: Who created Harryime What Carbean cult didvey:: did Iraqi
    troops::ose cover is of an of Universal Import What the of Betty theest thectic::
    Wh the founder and of The National Review:: was T Tims What the historicalals
    following the of Agra is whiteolate: of What the the: is a of everything:ase and:ose
    old London come- was : “y my sweet:: The major team in is called: Group or organization
    of: How dorow: M of: the name to ofese ?: Animal: is gymnia: of the between k
    and ch: of: the lawyer for Randy C:: the Francisco What year the in whereci became
    What country most is g the Who the to P What are the states the the name , Elino:
    What manmade waterways is1.76: Other of Z:ivalent of: of What was the:: How do
    ants have: of: the Dow first the high sound that hear in ear every then , but
    then it away ,:: didist control in:: How can I ofies ’ What did theramid-ers of
    Egypt eat:: How does Belle her inast: M of: When reading classs does EENTY ::
    Expression abbre: When was Florida:: manyelies were killed the: Whative on Punchl
    Hill and has1 What the Filenes the cookies in Internet: What word contains: Word
    with a special is Larry: a person: a Frenchist: of What American wrote : “ Goodors::
    Where theiestk rail stations:: many people ofosis: the worsticane Whatbean is
    of was Jean: What the2 What caused Harryini What buildingately enough the the1d
    bill: Other location: many logmic there a rule:: the the word , JJ the average
    hours per months byOL:: How a cop of: many are of is Ch:: is Whatation does: the
    the Whatte is “ a whole new: Other: the Chyl nuclear: the first the: Invention,
    book and otherative What does “ Philebus-:: didoco painting: the between: is Po
    What. the lowest highestation 6:: How the inpy: an the “ What was General Douglasthur
    in was by Presidentuman: How isaster: an the forini:: was Dick:: Where can find
    on religion and health the and: Other Whatian the TV51 theBC show for How the
    is of What Englishrighted “ thee , so What song put James:ative piece What new
    school in Philadelphia: Whatwestern isbed is B: is What Asian was as The Little
    Brown theans What of thean meeting: is: much the91 ?:: On which isbor: Who first::
    the:: How you a paint: an What then-der theterset ,:ivalent What is to hold the
    lens the the star: Why toason a for behavior , or that the accepted of:ivalent
    of Perg What religion What country you the What does V:: Where I a goodboard for::
    buyies on the the the: areter cookiespped with cres: theoe thated ofasticitations
    , as ‘ the rules to “: the three What do for an:: CNN in:: is a:ose special bears
    was on17 the Who used Au an electionan: what book: is to the various ways can
    measure IT:chni and method is software What British minister and wereins: aic
    the to overcome fear What drink would the biggest:: the States do people longest::
    which the the rare disease as : , andentizations , , and is of a is What Russian
    mastery What a perfect a: What c was Thomas in: Other: did the of What did What
    can feature the different:ques the-O the ons lips at What anetic did Victoria
    used her child: D What do: many from to of ofors , body: and is What causes get
    in: the G What is Other Who the1 century-stone who gained of Florence but endedake:
    of c: the oldest relationship sister with The the world of a to detectchni Whaty
    make:: Stuart is first: is w What a character by Rs … Question: What is a fuel
    cell ? Type: LLMs’ Response: Atlas’ mountain LLMs’ Response after Subsquence Recovery:
    Definition of something Ground Truth: Definition of something'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Cases study on trec few-show learning in LongBench benchmark (Bai
    et al., [2023](#bib.bib1)) in 2,000 constraint using GPT-3.5-Turbo.'
  prefs: []
  type: TYPE_NORMAL
