- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:42'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08056](https://ar5iv.labs.arxiv.org/html/2312.08056)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shengguang Wu Peking UniversityBeijingChina [wushengguang@stu.pku.edu.cn](mailto:wushengguang@stu.pku.edu.cn)
    ,  Zhenglun Chen Peking UniversityBeijingChina [danielchenbj@gmail.com](mailto:danielchenbj@gmail.com)
     and  Qi Su Peking UniversityBeijingChina [sukia@pku.edu.cn](mailto:sukia@pku.edu.cn)(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Ancient artifacts are an important medium for cultural preservation and restoration.
    However, many physical copies of artifacts are either damaged or lost, leaving
    a blank space in archaeological and historical studies that calls for artifact
    image generation techniques. Despite the significant advancements in open-domain
    text-to-image synthesis, existing approaches fail to capture the important domain
    knowledge presented in the textual description, resulting in errors in recreated
    images such as incorrect shapes and patterns. In this paper, we propose a novel
    knowledge-aware artifact image synthesis approach that brings lost historical
    objects accurately into their visual forms. We use a pretrained diffusion model
    as backbone and introduce three key techniques to enhance the text-to-image generation
    framework: 1) we construct prompts with explicit archaeological knowledge elicited
    from large language models (LLMs); 2) we incorporate additional textual guidance
    to correlated historical expertise in a contrastive manner; 3) we introduce further
    visual-semantic constraints on edge and perceptual features that enable our model
    to learn more intricate visual details of the artifacts. Compared to existing
    approaches, our proposed model produces higher-quality artifact images that align
    better with the implicit details and historical knowledge contained within written
    documents, thus achieving significant improvements across automatic metrics and
    in human evaluation. Our code and data are available at [https://github.com/danielwusg/artifact_diffusion](https://github.com/danielwusg/artifact_diffusion).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ancient Artifact Visualization, Text-to-Image Synthesis, Diffusion Models,
    Multi-Source Supervision, Large Language Models^†^†copyright: acmcopyright^†^†journalyear:
    2023^†^†doi: XXXXXXX.XXXXXXX^†^†conference: arXiv; Preprint; ^†^†submissionid:
    xxxx'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fe22936b3707e77f5724369a5c5bc81e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Images of artifacts generated by a vanilla diffusion model, the shape,
    color, pattern, and material differ greatly from the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: Ancient artifacts are crucial for cultural preservation, as they represent tangible
    evidence of the past, offering insights into history. In recent years, innovative
    artifact-related projects have emerged, including the restoration of degraded
    character images (Shi et al., [2022](#bib.bib31)), the generation of captions
    for ancient artwork (Sheng and Moens, [2019](#bib.bib30)), and the deciphering
    of oracle bone inscriptions (Chang et al., [2022](#bib.bib4)). These works have
    opened up new avenues for researchers to study artifacts and gain insights into
    the past. Despite these advancements, there is still much to be explored in artifact-related
    tasks, one of which is to recreate visual images of artifacts from text descriptions
    as many physical copies of artifacts are often damaged or lost, leaving only textual
    records behind. This task could prove immensely invaluable to historical studies
    and cultural preservation, as it provides historians with new visual angles to
    study the past and enables people to connect with their cultural heritage.
  prefs: []
  type: TYPE_NORMAL
- en: One area that has shown potential to aid in the recreation of visual images
    of ancient artifacts is text-to-image synthesis. This task has been a popular
    area of research, especially in recent years with the introduction of diffusion
    models (Yang et al., [2022](#bib.bib45); Ho et al., [2020](#bib.bib12); Rombach
    et al., [2021](#bib.bib25); Song et al., [2021](#bib.bib36); Nichol and Dhariwal,
    [2021](#bib.bib20); Song et al., [2022](#bib.bib33)) that have demonstrated significant
    capabilities in generating photorealistic images based on a given text prompt
    in open-domain problems (Nichol et al., [2022](#bib.bib21); Rombach et al., [2021](#bib.bib25);
    Saharia et al., [2022](#bib.bib28); Gu et al., [2022](#bib.bib8); Ramesh et al.,
    [2022](#bib.bib24)). However, in the specialized area of archaeological studies,
    where data is often limited and domain knowledge is required, vanilla diffusion
    models struggle to produce promising results even with finetuning, as shown in
    Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Knowledge-Aware Artifact Image
    Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"). The generated
    images often display errors in shape, patterns, and details that fail to match
    the implicit knowledge in the textual information and the underlying historical
    context of the target artifact.
  prefs: []
  type: TYPE_NORMAL
- en: We have identified a key cause for this problem to be the lack of knowledge
    supervision during the generating process, which can be attributed to two main
    aspects. 1). Current text prompts may not be infused with domain-specific knowledge
    from the archaeological and historical fields, leading to noisiness and lack of
    well-presented knowledge information in the text prompt. 2). The text and visual
    modules in the vanilla diffusion models (Rombach et al., [2021](#bib.bib25); Sohl-Dickstein
    et al., [2015](#bib.bib32); Song et al., [2020](#bib.bib35); Ho et al., [2020](#bib.bib12);
    Song and Ermon, [2020](#bib.bib34)) may be unable to capture domain-specific knowledge
    under the standard training pipeline, resulting in the lack of detailed textual
    and visual signals of ancient artifacts in the generation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these challenges, we propose our knowledge-aware artifact image
    synthesis approach with a pretrained Chinese Stable Diffusion model (Rombach et al.,
    [2021](#bib.bib25); Zhang et al., [2022a](#bib.bib47)) as our backbone. Our method
    can generate visualizations of lost artifacts that well align with the underlying
    domain-knowledge presented in their textual records. Specifically: 1). To address
    the issue of noisiness and lack of well-presented knowledge information in the
    text prompt, we propose to use Large Language Models (LLMs) to enhance our text
    prompts in two ways: for one, we use LLMs to extract the core and meaningful information
    in the given text prompt and reorganize them in a more structured way to explicitly
    present the current knowledge information; for another, we use LLMs as an external
    knowledge base to retrieve relevant archaeological knowledge information and augment
    them in the restructured text prompt. 2). To address the lack of both textual
    and visual knowledge supervision in the generation process, we introduce additional
    supervisions in both modalities. Firstly, we introduce a contrastive training
    paradigm that enables the text encoder to make the textual representation of the
    artifact more in line with their archaeological knowledge. Secondly, we apply
    stricter visual constraints using edge loss (Seif and Androutsos, [2018](#bib.bib29))
    and perceptual loss (Johnson et al., [2016](#bib.bib13)) to make the final visual
    output align with the visual domain knowledge of ancient artifacts.'
  prefs: []
  type: TYPE_NORMAL
- en: Both quantitative experiments and a user study demonstrate that our knowledge-aware
    artifact image synthesis approach significantly outperforms existing text-to-image
    models and greatly improves the generation quality of historical artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, our main contributions can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose to use LLMs as both information extractor and external knowledge
    base to aid better prompt construction in a specialized domain, which in our case
    is archaeological studies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce additional multimodal supervisions to enable our model to learn
    textual representations and visual features that better align with archaeological
    knowledge and historical context, thus improving the current finetuning paradigm
    of diffusion models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To our best knowledge, we are the first to explore text-to-image synthesis etask
    in the archaeological domain as an attempt to recreate lost artifacts, thus aiding
    archaeologists to gain deeper insights into history.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-image Synthesis.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Text-to-image synthesis tasks have long been a vital task at the intersection
    between computer vision and natural language processing, of which models are given
    a plain text description to generate the corresponding image. One major architecture
    in this area is GAN (Goodfellow et al., [2014](#bib.bib7)), whose variations (Zhang
    et al., [2016](#bib.bib46); Xu et al., [2017](#bib.bib44); Kang et al., [2023](#bib.bib14))
    have resulted in the state-of-art performance of text-to-image synthesis tasks.
    Recently, diffusion models (Rombach et al., [2021](#bib.bib25); Sohl-Dickstein
    et al., [2015](#bib.bib32); Song et al., [2020](#bib.bib35); Ho et al., [2020](#bib.bib12);
    Song and Ermon, [2020](#bib.bib34)) also have demonstrated their ability to achieve
    new state-of-the-art results (Dhariwal and Nichol, [2021](#bib.bib5)). Diffusion
    models make use of two Markov chains: forward and reverse. The forward chain gradually
    adds noise to the data with Gaussian prior. The reverse chain aims to denoise
    the data gradually. The transition probability at each timestep is learned by
    a deep neural network, which in the case of text-to-image synthesis is usually
    a U-Net (Ronneberger et al., [2015](#bib.bib27)) model.'
  prefs: []
  type: TYPE_NORMAL
- en: Large Language Models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Language models are a family of probabilistic models that predict the probability
    of the next word, given a sequence of previous words within a context. The introduction
    of GPT-3 (Brown et al., [2020](#bib.bib2)), which contains 175B parameters, has
    led to the emergence of Large Language Models (LLMs), referring to language models
    with a large number of parameters. These LLMs have demonstrated never-seen-before
    abilities, expanding the frontiers of what is possible with language models. One
    emerging ability of LLMs is in-context learning (Brown et al., [2020](#bib.bib2)),
    where LLMs are able to perform downstream tasks after being prompted with just
    a few examples without further parameter updates. Thus, by providing carefully
    designed examples, we can make use of LLMs as an information extractor given a
    noisy and unstructured text. LLMs have also shown their ability to acquire world
    knowledge from the massive training corpus (Petroni et al., [2019](#bib.bib22);
    Wang et al., [2021](#bib.bib40); Liu et al., [2022](#bib.bib19)). An efficient
    way to extract the implicit knowledge from LLMs is to ask questions with proper
    prompt engineering as LLMs are highly sensitive to the prompt input (Liang et al.,
    [2022](#bib.bib18)).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. Problem Statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned in Section [1](#S1 "1\. Introduction ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"), for
    many artifacts, text documents are the only available source of information. Hence
    our task is to recreate a visual image $I_{i}^{\prime}$ is the corresponding artifacts
    image. The raw text information available for an artifact - as often cataloged
    in museums - contains roughly four parts: the name or title of the artifact; the
    time period of origin; a raw description of the artifact (often presented in a
    messy way); the physical size of the artifact. Formulated from accessible resources
    of such kind, the task of our work is then to generate accurate artifact images
    based on these textual descriptions of historical objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Diffusion Preliminaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To solve the task defined in the above section [3.1](#S3.SS1 "3.1\. Problem
    Statement ‣ 3\. Background ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision"), we propose to build our model upon the
    text-conditioned Stable Diffusion pipeline (Rombach et al., [2021](#bib.bib25)).
    Before diving deeper into our approach, we briefly introduce diffusion models
    and the Stable Diffusion architecture which serves as the backbone of our method.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '(Rombach et al., [2021](#bib.bib25); Sohl-Dickstein et al., [2015](#bib.bib32);
    Song et al., [2020](#bib.bib35); Ho et al., [2020](#bib.bib12); Song and Ermon,
    [2020](#bib.bib34)) are a family of probabilistic models which involves two processes:
    forward process and reverse process. Let $p(x_{0})$, where'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $q(x_{t}&#124;x_{0})=\prod^{T}_{t=1}q(x_{t}&#124;x_{t-1})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (2) |  | $q(x_{t}&#124;x_{t-1})=\mathcal{N}(x_{t}&#124;\sqrt{1-\beta_{t}}x_{t-1};\beta_{t}\boldsymbol{I})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The reverse process $p_{\theta}$, where
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $p_{\theta}(x_{0})=p(x_{T})\prod^{T}_{t=1}p_{\theta}(x_{t-1}&#124;x_{t})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (4) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: The original diffusion model (Ho et al., [2020](#bib.bib12)) set $\boldsymbol{\Sigma_{\theta}}(x_{t},t)=\sigma^{2}_{t}\boldsymbol{I}$,
    leading to the loss function
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $L(\theta):=\mathbb{E}_{t,x_{0},\epsilon}&#124;&#124;\epsilon-\epsilon_{\theta}(\alpha_{t}x_{0}+\sigma_{t}\epsilon,t)&#124;&#124;^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{t}=\sqrt{1-\sigma^{2}_{t}}$ is a neural model using U-Net (Ronneberger
    et al., [2015](#bib.bib27)) as the backbone.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Diffusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '(SD) (Rombach et al., [2021](#bib.bib25)) introduces perceptual image compression
    which first maps the image to a latent space, then applies diffusion process and
    reverse process on the latent space rather than the pixel space which was used
    in earlier diffusion models. Overall, a Stable Diffusion model is comprised of
    three parts: a VAE (Kingma and Welling, [2013](#bib.bib16)), a text encoder and
    a U-Net (Ronneberger et al., [2015](#bib.bib27)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'VAE (Kingma and Welling, [2013](#bib.bib16)) contains two parts: an encoder
    $\mathcal{E}$ in the forward process during training. The decoder part of VAE
    (Kingma and Welling, [2013](#bib.bib16)) is used to decode the denoised latent
    representation back into an image at inference time.'
  prefs: []
  type: TYPE_NORMAL
- en: Text Encoder $\mathcal{E}_{text}$. Stable diffusion (Rombach et al., [2021](#bib.bib25))
    uses a pre-trained CLIP (Radford et al., [2021](#bib.bib23)) as the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'U-Net (Ronneberger et al., [2015](#bib.bib27)) contains two parts: an encoder
    and a decoder, with ResNet (He et al., [2015](#bib.bib10)) as the block structure.
    The encoder part projects the image into a low-resolution image presentation and
    the decoder part aims to restore the original image. Similar to the original diffusion
    model, in Stable Diffusion, the U-Net structure serves as $\epsilon_{\theta}$
    during the reverse process, conditioned on the text embedding using cross-attention
    (Vaswani et al., [2017](#bib.bib39)).'
  prefs: []
  type: TYPE_NORMAL
- en: During finetuning, the training objective for Stable Diffusion can be written
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $L_{SD}(\theta):=\mathbb{E}_{t,\mathcal{E}(x_{0}),\epsilon}&#124;&#124;\epsilon-\epsilon_{\theta}(z_{t},t,w)&#124;&#124;^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\epsilon$ is the latent space encoder.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3d393f70123ecaae1f28e5d4a71a755a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Raw artifact descriptions fail to depict the artifact with sufficient
    archaeological information, such as their artifact-“type” which determines important
    aspects of their visual appearance.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Our Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/48bbfd2da627bb2e1aadc8998f4a0a3f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3\. Our proposed knowledge-aware approach is illustrated in a). It features
    a Chinese Stable Diffusion model as backbone and our proposed three key techniques
    labeled as follows: b) illustrates the way of performing textual contrastive learning,
    which is discussed in Section [4.2](#S4.SS2 "4.2\. Alignment with Domain-Expertise
    via Contrastive Learning ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision"); c) is edge loss, and
    d) is perceptual loss, both of which are part of the additional visual-semantic
    supervision, as discussed in Section [4.3](#S4.SS3 "4.3\. Restoring Visual Details
    via Additional Visual-Semantic Supervision ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision").'
  prefs: []
  type: TYPE_NORMAL
- en: Our proposed approach for knowledge-aware artifact image synthesis is built
    upon a pretrained Stable Diffusion model, which retains its powerful generative
    capability of common domains and is further finetuned to align with the specific
    characteristics of ancient artifacts. The generic Stable Diffusion model, even
    with finetuning, however, struggles to generate visually and historically accurate
    artifact images and shows multifaceted errors, as is demonstrated in Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision").
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these issues, we propose specific modifications at three steps in
    the Stable Diffusion system: 1). Given source text information $T_{i}$ and apply
    additional visual-semantic supervision with edge loss (Seif and Androutsos, [2018](#bib.bib29))
    and perceptual loss (Johnson et al., [2016](#bib.bib13)) to steer the generation
    of our model closer towards the ground-truth appearance of artifacts.'
  prefs: []
  type: TYPE_NORMAL
- en: The overall framework for our approach is illustrated in Figure [3](#S4.F3 "Figure
    3 ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision") and explained in detail in the following
    subsections.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Prompt-Construction Enhanced by LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have noticed that the raw description of an artifact accessible in museum
    resources (as mentioned in Section [3.1](#S3.SS1 "3.1\. Problem Statement ‣ 3\.
    Background ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision")) is far from ideal for prompting a text-to-image
    model. It is often incomplete and filled with noisy messages and fails to sufficiently
    depict a historical object. Other than the messiness problem, these off-the-shelf
    descriptions may well lack specific information about an artifact that is essential
    to its visual form, such as its fundamental classification (or: artifact-“type”).
    An example¹¹1To maintain a consistent language usage throughout the paper, we
    translate all Chinese text (e.g., the textual descriptions of artifacts) into
    English via ChatGPT. of this is given in Figure [2](#S3.F2 "Figure 2 ‣ Stable
    Diffusion ‣ 3.2\. Diffusion Preliminaries ‣ 3\. Background ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"). The
    fact that the artifact on the left side is classified as a “Round Ding” rather
    than a “Square Ding” confines its shape to a round body having only three legs
    as opposed to four legs. However, key archaeological information of this kind
    is often missing in the raw description of the artifact, prohibiting a text-to-image
    model from sufficiently understanding the association between the visual appearance
    and the textual prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: To alleviate the problems of noisiness and knowledge deficiency in the original
    text information, we propose to utilize an LLM as both an information extractor
    to retrieve the most useful information, and as an external knowledge base to
    complete any missing important attributes of the artifact.
  prefs: []
  type: TYPE_NORMAL
- en: Based on archaeological expertise, we have compiled a list of key attributes
    that are vital for effectively describing artifacts and defining their physical
    forms, see Table [1](#S4.T1 "Table 1 ‣ 4.1\. Prompt-Construction Enhanced by LLM
    ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision"). Examples of these attributes are given
    in Table [5](#A0.T5 "Table 5 ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision") in Appendix [A](#A1 "Appendix A Example
    of the Enhanced Artifact Attributes ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision"). While the “name”,
    “time period” and “size” of an artifact are usually available in museum resources,
    the specific “material”, “shape” and “pattern” need to be extracted or derived
    from the raw description of the object. Further, as explained above, the classified
    “type” of an artifact determines certain fundamental aspects of its looks, which
    are specified by the generic definition of this artifact-type (i.e., “type definition”).
    It requires a general knowledge of archaeology to be able to categorize an ancient
    object into a certain artifact-type and to define the basic appearance of this
    type.
  prefs: []
  type: TYPE_NORMAL
- en: '| Expert Attribute | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Name | name or title of an artifact |'
  prefs: []
  type: TYPE_TB
- en: '| Material | the material an artifact is made of |'
  prefs: []
  type: TYPE_TB
- en: '| Time Period | time period of origin |'
  prefs: []
  type: TYPE_TB
- en: '| Type | classified type of an artifact |'
  prefs: []
  type: TYPE_TB
- en: '| Type Definition | general definition of artifact type |'
  prefs: []
  type: TYPE_TB
- en: '| Shape | shape and structure of an artifact |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern | patterns/motifs on an artifact |'
  prefs: []
  type: TYPE_TB
- en: '| Size | physical dimensions of an artifact |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. Expert attributes of artifacts that are vital to their visual appearance
    according to archaeological expertise. See Table [5](#A0.T5 "Table 5 ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")
    in Appendix [A](#A1 "Appendix A Example of the Enhanced Artifact Attributes ‣
    Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision") for examples of these attributes.
  prefs: []
  type: TYPE_NORMAL
- en: 'An LLM is well-suited for fulfilling these two tasks with its ability to obtain
    a certain extent of world knowledge from the massive pretraining corpus (Liang
    et al., [2022](#bib.bib18)) and to learn to perform specialized downstream tasks
    using the in-context learning paradigm (Brown et al., [2020](#bib.bib2)). Specifically,
    we use GPT-3.5-TURBO as our knowledge-base LLM, and the prompt for querying GPT-3.5
    is designed with a similar format following self-instruct (Wang et al., [2022](#bib.bib41)).
    Our prompt template consists of three parts: 1). A task statement that describes
    to GPT-3.5 the task to be done; 2). Two in-context examples of high quality sampled
    from our labeled pool of 54 artifacts written by archaeology experts; 3). The
    target artifacts whose “material”, “shape”, “pattern”, “type” and “type definition”
    are left blank and need to be answered by GPT-3.5\. The former 3 attributes can
    be retrieved from the given “description” and the latter 2 artifact-type related
    features need to be fulfilled via the world knowledge of GPT-3.5 and its in-context
    learning from the given human-labeled examples. An example of our prompt for querying
    artifact information is illustrated in Figure [6](#A4.F6 "Figure 6 ‣ Learned Perceptual
    Image Patch Similarity (LPIPS). ‣ Appendix D More Details on Evaluation Metrics
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision") in Appendix [B](#A2 "Appendix B Example of the Prompt Template for
    Querying GPT-3.5 ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision").'
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the power of LLM as both an information extractor and external
    knowledge provider, we are able to collect all the key attributes of a given artifact,
    which are then rearranged into the prompt to our diffusion model with a $[SEP]$
    (implemented as a Chinese comma in our work) splitting each key feature, as shown
    by the example in Table [5](#A0.T5 "Table 5 ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision") in Appendix [A](#A1
    "Appendix A Example of the Enhanced Artifact Attributes ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"). Such
    input prompt thus contains enriched text information that provides well-defined
    archaeology-knowledge guidance. It assists the text-to-image diffusion model in
    synthesizing a more realistic result that better corresponds to the ground-truth
    artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Alignment with Domain-Expertise via Contrastive Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another issue we identify is that the text encoder might not encode the text
    into a representation that reflects the underlying archaeological knowledge and
    thus needs further finetuning. We observe that the names of ancient artifacts
    are often accurate and concise summarizations of the artifact’s key attributes,
    while the descriptions provide an extended version of the artifact’s features.
    Given that both the names and descriptions are provided by domain experts - as
    written in museum sources, they reflect a high level of expertise in the field.
    Thus, we believe that closely aligning the names and descriptions is essential
    to reflect this domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this goal, we propose the use of contrastive learning, which aims
    to minimize the distance between positive pairs, consisting of matching $([description]_{i},[name]_{i})$,
    and to maximize the distance between negative ones with mismatching names. However,
    we have also observed that artifacts with similar attributes (i.e., similar description
    contents) and origins share similar names, making it unintuitive to finetune the
    text encoder to differentiate between similar pairs. We believe that such pairs
    should be close to each other in the semantic space. Therefore, we readjust our
    sampling strategy for negative pairs. From the perspective of historical studies,
    “time period” is one of the most determining factors in the style and appearance
    of an artifact, where different artifacts from different eras can be vastly different.
    Therefore, aiming to separate hard negatives rather than slightly different ones,
    we sample our negative samples from artifact names in different eras.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our approach, we use InfoNCE (van den Oord et al., [2018](#bib.bib38)) to
    penalize the misalignment in the representation encoded by the text encoder. The
    formula for text contrastive learning can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $L_{\text{text}}:=-\mathbb{E}_{X}\log\frac{EXP(x_{i})}{\sum_{x_{j}\in
    X}EXP(x_{j})}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $x_{i}=\mathcal{E}([description]_{i})\cdot\mathcal{E}([name]_{i})$.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Restoring Visual Details via Additional Visual-Semantic Supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artifact images generated by the vanilla Stable Diffusion model suffer from
    blurry edges and false color and patterns under the current setting (see Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision")), implying that stricter visual constraints
    need to be enforced to address these issues. Therefore, we propose to use edge
    loss (Seif and Androutsos, [2018](#bib.bib29)) and perceptual loss (Johnson et al.,
    [2016](#bib.bib13)) that apply additional visual-semantic supervision on images
    generated by our Stable Diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: Edge Loss.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Building upon the insights from (Seif and Androutsos, [2018](#bib.bib29)),
    we penalize the differences in contours between two images, by aiming to minimize
    the $L_{2}$ distance between their edge maps, as shown in part c) of Figure [3](#S4.F3
    "Figure 3 ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision"). Since the vanilla Stable Diffusion model
    often produces images that suffer from the problem of incorrect and blurry shape
    compared to the ground-truth artifact, it is necessary to penalize such errors
    as defined here in the edge loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $L_{\text{edge}}:=&#124;&#124;EDGE(I_{i})-EDGE(I^{\prime}_{i})&#124;&#124;^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $EDGE(\cdot)$ distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/68812f7f3ac5389bdab2f12bf3a98164.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4\. Canny edge maps of artifacts
  prefs: []
  type: TYPE_NORMAL
- en: Perceptual Loss.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similar to (Johnson et al., [2016](#bib.bib13)), we also penalize the problem
    of mismatching high-level details between the generated image and the real one.
    As we have also observed on the images generated by vanilla Stable Diffusion model,
    the high-level details (such as colors and patterns) are often misaligned with
    the original ones. Therefore, we incorporate perceptual loss into our training
    process to tackle such issue, as perceptual loss works by mapping the images into
    a semantic space using a pretrained network, and then minimizing the difference
    between the high-level features of the generated image and the original image.
    The formula for perceptual loss is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $L_{\text{perceptual}}:=&#124;&#124;\phi(I_{i})-\phi(I^{\prime}_{i})&#124;&#124;^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\phi$ denotes a pretrained image encoder to extract the high-level features
    of an image. This is applied to impose a stricter supervision on color, texture,
    and other high-level features. In our method, we use a CLIP-ViT-L/14 (Radford
    et al., [2021](#bib.bib23)) image encoder to act as our pretrained image encoder
    for perceptual loss.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Objective Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Combining all the extra multi-source multi-modal supervisions above, the overall
    training objective of our system is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $1$2 |  |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{1}$ are the ground-truth and the restored image from our model’s
    prediction.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Prompt | $\lambda_{1}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Taiyi-SD-finetuned-description | raw description | - | - | - | 0.772 | 0.536
    | 0.608 |'
  prefs: []
  type: TYPE_TB
- en: '| Taiyi-SD-finetuned-attributes | LLM enhanced attribute | - | - | - | 0.792
    | 0.554 | 0.598 |'
  prefs: []
  type: TYPE_TB
- en: '| OURS-attributes +text | LLM enhanced attributes | 0.5 | - | - | 0.801 | 0.580
    | 0.552 |'
  prefs: []
  type: TYPE_TB
- en: '| OURS-attributes +edge+perceptual | LLM enhanced attributes | - | 0.3 | 0.1
    | 0.815 | 0.636 | 0.497 |'
  prefs: []
  type: TYPE_TB
- en: '| OURS-attributes +text+edge+perceptual | LLM enhanced attributes | 0.3 | 0.3
    | 0.1 | 0.831 | 0.594 | 0.536 |'
  prefs: []
  type: TYPE_TB
- en: Table 2\. Quantitative comparison of our models against the finetuned Taiyi-SD
    baselines over CLIP Visual Similarity (CLIP-VS), SSIM and LPIPS.
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | CLIP-VS $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Raw description | 0.748 | 0.383 | 0.748 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM enhanced attributes-sequence | 0.765 | 0.413 | 0.730 |'
  prefs: []
  type: TYPE_TB
- en: Table 3\. Quantitative comparison between zero-shot Taiyi-SD models using different
    textual prompts over CLIP Visual Similarity (CLIP-VS), SSIM and LPIPS.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Material $\uparrow$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Taiyi-SD-finetuned | 2.66 | 1.50 | 1.44 | 1.79 | 2.12 | 1.90 |'
  prefs: []
  type: TYPE_TB
- en: '| OURS | 3.94 | 3.38 | 3.25 | 3.30 | 3.20 | 3.41 |'
  prefs: []
  type: TYPE_TB
- en: Table 4\. Human evaluation of the quality of artifact images generated by the
    finetuned baseline and our model. The images are rated from 5 different aspects
    on a scale of 0 to 5 by 20 archaeology experts from top institutions.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1\. Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the sparsity of paired text-image data in the ancient artifact domain,
    we build our own text-to-image dataset by collecting artifact information from
    National Palace Museum Open Data Platform (Taipei National Palace Museum, [2019](#bib.bib37)).
    After careful cleansing of available entries, we are left with 16,092 unique artifact
    samples with their descriptions and ground-truth images. We split the data by
    80%/10%/10% for training, validation, and testing.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation Details.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For our backbone model, we use a pretrained Chinese Stable Diffusion Taiyi-Stable-Diffusion-1B-Chinese-v0.1
    (Zhang et al., [2022a](#bib.bib47)) (dubbed Taiyi-SD) which was trained on 20M
    filtered Chinese image-text pairs. Taiyi-SD inherits the same VAE and U-Net from
    stable-diffusion-v1-4 (Rombach et al., [2022](#bib.bib26)) and trains a Chinese
    text encoder from Taiyi-CLIP-RoBERTa-102M-ViT-L-Chinese (Zhang et al., [2022b](#bib.bib48))
    to align Chinese prompts with the images. Further training details are left in
    Appendix [C](#A3 "Appendix C More on Implementation Details ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision").
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation Metrics.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To comprehensively evaluate our method for text-to-image synthesis quantitatively,
    we employ three commonly used metrics that measure image generation quality: CLIP
    Visual Similarity, Structural Similarity Index (SSIM) (Wang et al., [2004](#bib.bib42))
    and Learned Perceptual Image Patch Similarity (LPIPS) (Zhang et al., [2018](#bib.bib49)).
    Each of them highlights different aspects of the generated image. Together, they
    provide a thorough judgment of a synthesized artifact image in terms of its overall
    resemblance to the ground truth, the accuracy of its shape and pattern, and its
    perceptual affinity to the target image. We leave an extensive explanation of
    these metrics in Appendix [D](#A4 "Appendix D More Details on Evaluation Metrics
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f7ba795988979112df9fa50e6271125a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Comparison between the finetuned Taiyi-SD baseline model and OUR
    approach trained with additional edge loss and perceptual loss against the ground
    truth. Clearly, objects generated by OUR model display more accurate shape, colors,
    and patterns when compared to the ground truth, whereas these delicate visual
    details are easily neglected by the baseline.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Main Results and Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Table [2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\. Our Method ‣
    Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision"), we compare the quantitative results of our approach with the baselines
    on our test set. The first column denotes the models we experimented with.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the baselines, we use Taiyi-SD via two versions: Taiyi-SD-finetuned-description:
    the finetuned Taiyi-SD with the raw description (directly available from museum
    archives) as input prompt; Taiyi-SD-finetuned-attributes: the finetuned Taiyi-SD
    using LLM-enhanced prompt (a sequence of artifact attributes) as designed in Section [4.1](#S4.SS1
    "4.1\. Prompt-Construction Enhanced by LLM ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision").'
  prefs: []
  type: TYPE_NORMAL
- en: 'For our approach, we apply the LLM-enhanced prompt by default and also explore
    three different versions of extra supervisions in addition to training the Taiyi-SD
    backbone: OURS-attributes +text: finetuning with additional text contrastive loss
    (see Section [4.2](#S4.SS2 "4.2\. Alignment with Domain-Expertise via Contrastive
    Learning ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision")) to align the text representation of
    our model with domain expertise; OURS-attributes +edge+perceptual: finetuning
    with edge loss and perceptual loss (see Section [4.3](#S4.SS3 "4.3\. Restoring
    Visual Details via Additional Visual-Semantic Supervision ‣ 4\. Our Method ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"))
    as additional supervision to enforce more visual-semantic constraints on the image
    generation process; OURS-attributes +text+edge+perceptual: finetuning with both
    text contrastive loss and the edge and perceptual loss as multi-source supervisions.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, our proposed artifact image synthesis approach significantly outperforms
    the finetuned Taiyi-SD-baselines across all metrics. The improvement on SSIM indicates
    that images generated by our model better preserve the shapes and boundaries of
    the original artifacts. An increase in CLIP Visual Similarity also indicates that
    our approach produces images that are more closely aligned to the ground truths.
  prefs: []
  type: TYPE_NORMAL
- en: Additional visual-semantic constraints in the form of edge loss and perceptual
    loss contribute greatly to boosting the SSIM and LPIPS scores. This can be attributed
    to the fact that edge loss and perceptual loss put a stricter condition on both
    structural details like edge and contour (captured by SSIM) and perceptual-level
    image features like color and texture (captured by LPIPS). These visual details
    are exactly much desired in our case of artifact image synthesis, as the shape,
    pattern, and texture of artifacts are of vital importance for determining their
    historical position and status.
  prefs: []
  type: TYPE_NORMAL
- en: By further incorporating the text contrastive loss into the overall training
    objective, we observe a slight increase in CLIP Visual Similarity, yet a decrease
    in SSIM and LPIPS scores. We believe there are two reasons behind this phenomenon.
    For one, by aligning the text knowledge (descriptions with names) (see Section [4.2](#S4.SS2
    "4.2\. Alignment with Domain-Expertise via Contrastive Learning ‣ 4\. Our Method
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision")), the textual guidance for generating the image is better represented
    and closer to the general visual content of the artifact, thus leading to a higher
    CLIP Visual Similarity. For another, the relative weight of edge and perceptual
    loss is reduced with the additional text contrastive loss, which might compromise
    the strict supervision on structural coherence and perceptual similarity and limit
    the model’s performance on SSIM and LPIPS.
  prefs: []
  type: TYPE_NORMAL
- en: As is also evidently shown by just comparing the baselines finetuned with different
    prompt formats in Table [2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\.
    Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision"), using LLM to enhance the prompt construction as
    a sequence of important artifact attributes effectively improves the performance
    of the finetuned baseline model across all three metrics. More about the effects
    of our LLM-enhanced prompting method will be discussed in the following subsection [5.3](#S5.SS3
    "5.3\. Ablation Studies ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision").
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Ablation Studies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To investigate the contribution of components proposed in our approach and for
    further studies, we conduct extensive ablation studies on two key designs of our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness of LLM enhanced prompt.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As is shown in Table [2](#S4.T2 "Table 2 ‣ 4.4\. Objective Functions ‣ 4\. Our
    Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision"), the finetuned model benefits from LLM-enhanced
    prompting, achieving better scores on all three quantitative metrics. To further
    illustrate the effectiveness of our proposed prompting method, we explore the
    zero-shot setting, where the baseline Taiyi-SD is directly prompted to generate
    artifact images without any training on our artifact dataset. We use either the
    raw description from the museum archives or the sequence of artifact attributes
    enhanced by LLM as prompt. The results, as shown in Table [3](#S4.T3 "Table 3
    ‣ 4.4\. Objective Functions ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image
    Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"), again demonstrate
    the superiority of LLM-enhanced prompting, which excels across all metrics. This
    can be credited to the organized information format in the attribute sequence
    and the additional knowledge provided by LLM (see Section [4.1](#S4.SS1 "4.1\.
    Prompt-Construction Enhanced by LLM ‣ 4\. Our Method ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")).
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness of Edge Loss and Perceptual Loss.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Figure [5](#S5.F5 "Figure 5 ‣ Evaluation Metrics. ‣ 5.1\. Experimental Setup
    ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision"), we compare the artifact images generated
    from our model that uses edge loss and perceptual loss against the finetuned Taiyi-SD
    baseline that does not involve these visual semantic constraints. Evidently, the
    shape, colors, and patterns of the artifacts are more accurate and close to the
    ground truth if the model is additionally supervised by edge and perceptual loss.
    On the other hand, the vanilla finetuning paradigm may easily lead to output objects
    either lacking proper shapes and forms or manifesting incorrect motifs and patterns.
    For example, in the second column of Figure [5](#S5.F5 "Figure 5 ‣ Evaluation
    Metrics. ‣ 5.1\. Experimental Setup ‣ 5\. Experiment ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision"), the
    “tall-footed” aspect of the target bowl is clearly neglected without edge and
    perceptual constraints. Also, the intricate cracking lines on the “begonia-blossom
    shaped vase” shown in the third column are better simulated with our model.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4\. User Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition to quantitative evaluation, we conducted a user study involving
    archaeology experts to evaluate the generated images. This study is designed to
    assess various aspects of the generated artifacts, as outlined in our prompt design:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Material: How accurately does the generated artifact resemble the actual manufacturing
    material?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shape: How closely does the generated artifact match the described shape?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pattern/Color: How faithful is the representation of patterns and colors on
    the generated artifact?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Size/Ratio: How accurately does the generated artifact maintain the ratio of
    height and width?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dynasty: How well does the generated artifact reflect the characteristics of
    its era?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Each aspect is rated on a scale of 0 to 5, with higher ratings indicating better
    quality. We randomly select 30 samples from the test set and provide the model-generated
    images to 20 graduate students of archaeology major from top institutions for
    assessment. The average ratings of images generated by our proposed method (OURS)
    are compared with those generated by the baseline Chinese SD model also finetuned
    on our data. The results are presented in Table [4](#S4.T4 "Table 4 ‣ 4.4\. Objective
    Functions ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision").
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, according to human experts, the artifact images generated by our method
    are much better in quality across all five important rating aspects, especially
    in terms of shape, pattern, and color. These results of our user study resonate
    with the findings from the automatic evaluation metrics and further highlight
    the superior performance of our model in generating artifact images that accurately
    align with history.
  prefs: []
  type: TYPE_NORMAL
- en: To offer a richer qualitative demonstration of our model’s capabilities, we
    present a diverse collection of artifact images generated by our model, showcasing
    its remarkable fidelity across a broad spectrum of historical artifacts. Refer
    to Figure [7](#A4.F7 "Figure 7 ‣ Learned Perceptual Image Patch Similarity (LPIPS).
    ‣ Appendix D More Details on Evaluation Metrics ‣ Knowledge-Aware Artifact Image
    Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision") in Appendix [E](#A5
    "Appendix E More Examples of Artifact Images Generated by Our Model ‣ Knowledge-Aware
    Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision")
    for a comprehensive visual display.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we present a novel approach to tackle the challenge of artifact
    image synthesis. Our method features three key techniques: 1). Leveraging an LLM
    to infuse textual prompts with archaeological knowledge, 2). Aligning textual
    representations with domain expertise via contrasting learning, and 3). Employing
    stricter visual-semantic constraints (edge and perceptual) to generate images
    with higher fidelity to visual details of historical artifacts. Quantitative experiments
    and our user study confirm the superior performance of our approach compared to
    existing models, significantly advancing the quality of generated artifact images.
    Beyond technological contributions, our work introduces a profound societal impact.
    As the first attempt to restore lost artifacts from the remaining descriptions,
    our work empowers archaeologists and historians with a tool to resurrect lost
    artifacts visually, offering new perspectives on cultural heritage and enriching
    our understanding of history. We also hope that this work will open new avenues
    for further exploration, fostering deeper insights into our past and cultural
    legacy with the help of technical advances.'
  prefs: []
  type: TYPE_NORMAL
- en: Ethics Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this project, we have used training data sourced from National Palace Museum
    (Taipei National Palace Museum, [2019](#bib.bib37)). This ensures that the data
    we’ve worked with has already been scrutinized by authorities and is open to the
    public. However, we recognize possible inaccuracies in our model’s generation
    despite our extensive efforts to improve its fidelity. Therefore, anyone using
    our model should be warned of possible mistakes in the generated artifact images
    and we strongly advise all users to verify important content.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Canny (1986) John Canny. 1986. A Computational Approach to Edge Detection. *IEEE
    Transactions on Pattern Analysis and Machine Intelligence* PAMI-8, 6 (1986), 679–698.
    [https://doi.org/10.1109/TPAMI.1986.4767851](https://doi.org/10.1109/TPAMI.1986.4767851)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chang et al. (2022) Xiang Chang, Fei Chao, Changjing Shang, and Qiang Shen.
    2022. Sundial-GAN: A Cascade Generative Adversarial Networks Framework for Deciphering
    Oracle Bone Inscriptions. In *Proceedings of the 30th ACM International Conference
    on Multimedia*. ACM. [https://doi.org/10.1145/3503161.3547925](https://doi.org/10.1145/3503161.3547925)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhariwal and Nichol (2021) Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion
    Models Beat GANs on Image Synthesis. In *Advances in Neural Information Processing
    Systems*, M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan
    (Eds.), Vol. 34\. Curran Associates, Inc., 8780–8794. [https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/49ad23d1ec9fa4bd8d77d02681df5cfa-Paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gal et al. (2022) Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H
    Bermano, Gal Chechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing
    text-to-image generation using textual inversion. *arXiv preprint arXiv:2208.01618*
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative Adversarial Nets. In *Advances in Neural Information Processing Systems*,
    Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger (Eds.),
    Vol. 27\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2022) Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong
    Chen, Lu Yuan, and Baining Guo. 2022. Vector Quantized Diffusion Model for Text-to-Image
    Synthesis. arXiv:2111.14822 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hang et al. (2023) Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen,
    Han Hu, Xin Geng, and Baining Guo. 2023. Efficient Diffusion Training via Min-SNR
    Weighting Strategy. (March 2023). arXiv:2303.09556 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2015) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015.
    Deep residual learning for image recognition. (Dec. 2015). arXiv:1512.03385 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. 2021. CLIPScore: A Reference-free Evaluation Metric for Image
    Captioning. In *Proceedings of the 2021 Conference on Empirical Methods in Natural
    Language Processing*. Association for Computational Linguistics, Online and Punta
    Cana, Dominican Republic, 7514–7528. [https://doi.org/10.18653/v1/2021.emnlp-main.595](https://doi.org/10.18653/v1/2021.emnlp-main.595)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising
    Diffusion Probabilistic Models. (June 2020). arXiv:2006.11239 [cs.LG] [https://arxiv.org/pdf/2006.11239.pdf](https://arxiv.org/pdf/2006.11239.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Johnson et al. (2016) Justin Johnson, Alexandre Alahi, and Li Fei-Fei. 2016.
    Perceptual losses for real-time style transfer and super-resolution. (March 2016).
    arXiv:1603.08155 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2023) Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli
    Shechtman, Sylvain Paris, and Taesung Park. 2023. Scaling up GANs for text-to-image
    synthesis. (March 2023). arXiv:2303.05511 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma and Ba (2014) Diederik P. Kingma and Jimmy Ba. 2014. Adam: A Method
    for Stochastic Optimization. arXiv:arXiv:1412.6980'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling (2013) Diederik P Kingma and Max Welling. 2013. Auto-Encoding
    Variational Bayes. (Dec. 2013). arXiv:1312.6114 [stat.ML]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumari et al. (2022) Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman,
    and Jun-Yan Zhu. 2022. Multi-Concept Customization of Text-to-Image Diffusion.
    *arXiv preprint arXiv:2212.04488* (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,
    Christopher D Manning, Christopher Ré, Diana Acosta-Navas, Drew A Hudson, Eric
    Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue
    Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun,
    Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter Henderson, Qian
    Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori
    Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen
    Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2022. Holistic Evaluation of Language
    Models. (Nov. 2022). arXiv:2211.09110 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Jiacheng Liu, Alisa Liu, Ximing Lu, Sean Welleck, Peter West,
    Ronan Le Bras, Yejin Choi, and Hannaneh Hajishirzi. 2022. Generated Knowledge
    Prompting for Commonsense Reasoning. In *Proceedings of the 60th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*. Association
    for Computational Linguistics, Dublin, Ireland, 3154–3169. [https://doi.org/10.18653/v1/2022.acl-long.225](https://doi.org/10.18653/v1/2022.acl-long.225)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nichol and Dhariwal (2021) Alex Nichol and Prafulla Dhariwal. 2021. Improved
    Denoising Diffusion Probabilistic Models. arXiv:2102.09672 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nichol et al. (2022) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
    Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2022. GLIDE:
    Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion
    Models. arXiv:2112.10741 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick
    Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language Models
    as Knowledge Bases?. In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP)*. Association for Computational Linguistics,
    Hong Kong, China, 2463–2473. [https://doi.org/10.18653/v1/D19-1250](https://doi.org/10.18653/v1/D19-1250)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual
    models from natural language supervision. (Feb. 2021). arXiv:2103.00020 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. 2022. Hierarchical Text-Conditional Image Generation with CLIP
    Latents. arXiv:2204.06125 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2021) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2021. High-resolution image synthesis with latent diffusion
    models. (Dec. 2021). arXiv:2112.10752 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion
    Models. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR)*. 10684–10695.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ronneberger et al. (2015) Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
    2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. (May 2015).
    arXiv:1505.04597 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara
    Mahdavi, Rapha Gontijo Lopes, Tim Salimans, Jonathan Ho, David J Fleet, and Mohammad
    Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language
    Understanding. arXiv:2205.11487 [cs.CV]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seif and Androutsos (2018) George Seif and Dimitrios Androutsos. 2018. Edge-Based
    Loss Function for Single Image Super-Resolution. In *2018 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*. 1468–1472. [https://doi.org/10.1109/ICASSP.2018.8461664](https://doi.org/10.1109/ICASSP.2018.8461664)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng and Moens (2019) Shurong Sheng and Marie-Francine Moens. 2019. Generating
    Captions for Images of Ancient Artworks. In *Proceedings of the 27th ACM International
    Conference on Multimedia*. ACM. [https://doi.org/10.1145/3343031.3350972](https://doi.org/10.1145/3343031.3350972)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2022) Daqian Shi, Xiaolei Diao, Lida Shi, Hao Tang, Yang Chi, Chuntao
    Li, and Hao Xu. 2022. CharFormer: A Glyph Fusion based Attentive Framework for
    High-precision Character Image Denoising. (2022). [https://doi.org/10.1145/3503161.3548208](https://doi.org/10.1145/3503161.3548208)
    arXiv:arXiv:2207.07798'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sohl-Dickstein et al. (2015) Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan,
    and Surya Ganguli. 2015. Deep Unsupervised Learning using Nonequilibrium Thermodynamics.
    arXiv:arXiv:1503.03585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2022) Jiaming Song, Chenlin Meng, and Stefano Ermon. 2022. Denoising
    Diffusion Implicit Models. arXiv:2010.02502 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song and Ermon (2020) Yang Song and Stefano Ermon. 2020. Generative Modeling
    by Estimating Gradients of the Data Distribution. arXiv:1907.05600 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020) Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. 2020. Score-Based Generative Modeling through
    Stochastic Differential Equations. arXiv:arXiv:2011.13456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2021) Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. 2021. Score-Based Generative Modeling through
    Stochastic Differential Equations. arXiv:2011.13456 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taipei National Palace Museum (2019) Taipei National Palace Museum. 2019. National
    Palace Museum Open Data. data retrieved from National Palace Museum Open Data,
    [https://theme.npm.edu.tw/opendata/index.aspx?lang=2](https://theme.npm.edu.tw/opendata/index.aspx?lang=2).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van den Oord et al. (2018) Aaron van den Oord, Yazhe Li, and Oriol Vinyals.
    2018. Representation learning with Contrastive Predictive Coding. (July 2018).
    arXiv:1807.03748 [cs.LG]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. (June 2017). arXiv:1706.03762 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Cunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can Generative
    Pre-trained Language Models Serve As Knowledge Bases for Closed-book QA?. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing (Volume
    1: Long Papers)*. Association for Computational Linguistics, Online, 3241–3251.
    [https://doi.org/10.18653/v1/2021.acl-long.251](https://doi.org/10.18653/v1/2021.acl-long.251)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022) Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-Instruct:
    Aligning Language Model with Self Generated Instructions. arXiv:2212.10560 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2004) Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli.
    2004. Image quality assessment: from error visibility to structural similarity.
    *IEEE Transactions on Image Processing* 13, 4 (2004), 600–612. [https://doi.org/10.1109/TIP.2003.819861](https://doi.org/10.1109/TIP.2003.819861)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wei et al. (2023) Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang,
    and Wangmeng Zuo. 2023. Elite: Encoding visual concepts into textual embeddings
    for customized text-to-image generation. *arXiv preprint arXiv:2302.13848* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2017) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, and Xiaodong He. 2017. AttnGAN: Fine-grained text to image generation
    with attentional generative adversarial networks. (Nov. 2017). arXiv:1711.10485 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2022) Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng
    Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. 2022. Diffusion models:
    A comprehensive survey of methods and applications. (Sept. 2022). arXiv:2209.00796 [cs.LG]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2016) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris Metaxas. 2016. StackGAN: Text to photo-realistic
    image synthesis with Stacked Generative Adversarial Networks. (Dec. 2016). arXiv:1612.03242 [cs.CV]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022a) Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin
    Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo,
    Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen,
    Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, and Chongpei
    Chen. 2022a. Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence.
    arXiv:arXiv:2209.02970'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022b) Jiaxing Zhang, Ruyi Gan, Junjie Wang, Yuxiang Zhang, Lin
    Zhang, Ping Yang, Xinyu Gao, Ziwei Wu, Xiaoqun Dong, Junqing He, Jianheng Zhuo,
    Qi Yang, Yongfeng Huang, Xiayu Li, Yanghan Wu, Junyu Lu, Xinyu Zhu, Weifeng Chen,
    Ting Han, Kunhao Pan, Rui Wang, Hao Wang, Xiaojun Wu, Zhongshen Zeng, and Chongpei
    Chen. 2022b. Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence.
    *CoRR* abs/2209.02970 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,
    and Oliver Wang. 2018. The Unreasonable Effectiveness of Deep Features as a Perceptual
    Metric. arXiv:arXiv:1801.03924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Expert Attribute | Example |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Name | Yuhuchun vase in cobalt blue glaze |'
  prefs: []
  type: TYPE_TB
- en: '| Material | Porcelain |'
  prefs: []
  type: TYPE_TB
- en: '| Time Period | Qing Dynasty, Yongzheng reign, 1723-1735 AD |'
  prefs: []
  type: TYPE_TB
- en: '| Type | Yuhuchun vase |'
  prefs: []
  type: TYPE_TB
- en: '| Type Definition | Also known as ”narrow-necked vase,” yuhuchun vase is a
    practical commemorative ceramic widely popular in the northern regions. The vase
    consists of five parts: neck, shoulders, body, foot, and mouth. The neck is long
    and slender, the body is plump, and the foot can be a short circular footring
    or a horseshoe-shaped foot. Yuhuchun vases are created using various clay recipes
    and glaze techniques, resulting in distinct colors and surface effects for each
    piece |'
  prefs: []
  type: TYPE_TB
- en: '| Shape | Flared mouth, slender neck, sloping shoulders, pear-shaped ample
    body, and a circular footring |'
  prefs: []
  type: TYPE_TB
- en: '| Pattern | The body of the vase is adorned with a cobalt blue glaze, which
    shines with a bright indigo color. The interior and the base of the vessel are
    covered in white glaze. The footring reveals the white body of the vase |'
  prefs: []
  type: TYPE_TB
- en: '| Size | Height of 30.3 cm, mouth diameter of 8.5 cm, base diameter of 12.0
    cm |'
  prefs: []
  type: TYPE_TB
- en: '| Enhanced Prompt Example | Yuhuchun vase in cobalt blue glaze [SEP] Porcelain
    [SEP] Qing Dynasty, Yongzheng reign, 1723-1735 AD [SEP] Yuhuchun vase [SEP] Also
    known as ”narrow-necked vase,” yuhuchun vase is a practical commemorative ceramic
    widely popular in the northern regions. The vase consists of five parts: neck,
    shoulders, body, foot, and mouth. The neck is long and slender, the body is plump,
    and the foot can be a short circular footring or a horseshoe-shaped foot. Yuhuchun
    vases are created using various clay recipes and glaze techniques, resulting in
    distinct colors and surface effects for each piece [SEP] Flared mouth, slender
    neck, sloping shoulders, pear-shaped ample body, and a circular footring [SEP]
    The body of the vase is adorned with a cobalt blue glaze, which shines with a
    bright indigo color. The interior and the base of the vessel are covered in white
    glaze. The footring reveals the white body of the vase [SEP] Height of 30.3 cm,
    mouth diameter of 8.5 cm, base diameter of 12.0 cm |'
  prefs: []
  type: TYPE_TB
- en: Table 5\. An example of the proposed expert attributes of artifacts. The last
    row forms the LLM-enhanced prompt input to our text-to-image model. The special
    delimiter [SEP] connecting attributes is implemented as a Chinese comma.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Example of the Enhanced Artifact Attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a supplement to Section [4.1](#S4.SS1 "4.1\. Prompt-Construction Enhanced
    by LLM ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision") and Table [1](#S4.T1 "Table 1 ‣ 4.1\.
    Prompt-Construction Enhanced by LLM ‣ 4\. Our Method ‣ Knowledge-Aware Artifact
    Image Synthesis with LLM-Enhanced Prompting and Multi-Source Supervision") in
    the main text, we provide a concrete example of the proposed expert attributes
    of the historical artifact “yuhuchun vase in cobalt blue glaze” in Table [5](#A0.T5
    "Table 5 ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting
    and Multi-Source Supervision") of this appendix²²2The numbering of tables, figures
    and equations in this appendix follows that of the main text. So the number does
    not start fresh from 1 in this appendix.. The last row is the arranged sequence
    of the artifact attributes separated by [SEP] (implemented as a Chinese comma),
    which forms the LLM-enhanced prompt input to our text-to-image models.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Example of the Prompt Template for Querying GPT-3.5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As specified in Section [4.1](#S4.SS1 "4.1\. Prompt-Construction Enhanced by
    LLM ‣ 4\. Our Method ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced
    Prompting and Multi-Source Supervision") in the main text, we use GPT-3.5-TURBO
    as our knowledge-base LLM. The prompt template for querying GPT-3.5 is designed
    with a similar format following self-instruct (Wang et al., [2022](#bib.bib41)).
    It consists of three parts: 1). A task statement that describes to GPT-3.5 the
    task to be done; 2). Two in-context examples sampled from our labeled pool of
    54 artifacts written by archaeology experts; 3). The target artifacts whose “material”,
    “shape”, “pattern”, “type” and “type definition” are left blank and need to be
    answered by GPT-3.5\. In between the parts and the different artifact samples,
    we use “###” as a separator. Figure [6](#A4.F6 "Figure 6 ‣ Learned Perceptual
    Image Patch Similarity (LPIPS). ‣ Appendix D More Details on Evaluation Metrics
    ‣ Knowledge-Aware Artifact Image Synthesis with LLM-Enhanced Prompting and Multi-Source
    Supervision") shows an example of our prompt for querying information about the
    artifact “snuff bottle with intertwined floral decoration in fencai polychrome
    enamels on a yellow ground”.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C More on Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here we provide additional details with regard to the implementation of our
    method as a supplement to the model details specified in Section [5.1](#S5.SS1
    "5.1\. Experimental Setup ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision") of the main text: To
    get the canny edge map of images for the edge loss computation, we implement a
    canny filter (Canny, [1986](#bib.bib3)) with a Gaussian kernel of size 3, Sobel
    filter kernel size of 3, a low threshold on pixel intensity of 0.15. We compute
    perceptual loss (Johnson et al., [2016](#bib.bib13)) using visual features extracted
    by the image encoder of CLIP-ViT-L/14 (Radford et al., [2021](#bib.bib23)). The
    weights of additional losses used in our model training objective defined in (5)
    of the main text are $\lambda_{1}=$ for all our experiments, which run on dual
    NVIDIA A100 GPUs.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D More Details on Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we offer a more extensive explanation of the automatic metrics
    (CLIP Visual Similarity, Structural Similarity Index (SSIM) (Wang et al., [2004](#bib.bib42))
    and Learned Perceptual Image Patch Similarity (LPIPS)) employed to quantitatively
    evaluate the performance of artifact image generation models. This serves to provide
    additional technical details to the evaluation metrics stated in Section [5.1](#S5.SS1
    "5.1\. Experimental Setup ‣ 5\. Experiment ‣ Knowledge-Aware Artifact Image Synthesis
    with LLM-Enhanced Prompting and Multi-Source Supervision") in the main text.
  prefs: []
  type: TYPE_NORMAL
- en: CLIP Visual Similarity.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by the CLIP Score (Hessel et al., [2021](#bib.bib11)), which is widely
    employed to assess the similarity between a text-image pair, we compute the visual
    similarity of two images (i.e., the ground-truth image and the generated image)
    with the visual module of a pre-trained CLIP model (specifically, CLIP-ViT-L/14)
    (Radford et al., [2021](#bib.bib23)). We refer to this metric as Clip Visual Similarity,
    which is also utilized in (Gal et al., [2022](#bib.bib6); Kumari et al., [2022](#bib.bib17);
    Wei et al., [2023](#bib.bib43)). Since the CLIP model has shown a strong ability
    in capturing the overall image contents and mapping those into a feature space,
    a higher similarity of the encoded visual features suggests a generally closer
    resemblance of the generated image to the ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: Structural Similarity Index (SSIM).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SSIM (Wang et al., [2004](#bib.bib42)) is designed to primarily measure the
    similarity in terms of structural components between two images. It evaluates
    how well the synthesized output preserves the structural details present in the
    ground truth images. This includes important edges, boundaries, and overall structural
    coherence. By quantifying the degree of structural resemblance, SSIM provides
    valuable insights into the accuracy of artifact image synthesis in terms of preserving
    crucial details related to the formative appearance of artifacts, e.g., their
    shape, and patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Learned Perceptual Image Patch Similarity (LPIPS).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LPIPS (Zhang et al., [2018](#bib.bib49)) judges the perceptual similarities
    between two images. In artifact image synthesis tasks, it is indispensable to
    assess not only the structural similarity but also the perceptual quality of the
    synthesized images. LPIPS is designed to align with human perception of image
    quality which also corresponds to the goal of artifact image synthesis tasks of
    generating historical objects that are visually convincing to human experts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20de2713d1b4055d5796e71b4941e6b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6\. An example of our GPT-3.5 querying prompt. We use Chinese by default
    because of the origin language of our data. The right-hand side is an English
    translation also done by the same GPT-3.5-TURBO engine.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2723d590b7d77e0e80ae45bc843a8f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7\. High-fidelity images of a wide range of artifacts generated by our
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E More Examples of Artifact Images Generated by Our Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our model is capable of synthesizing images of a wide range of artifacts with
    high historical accuracy based on simple textual descriptions, as shown in Figure [7](#A4.F7
    "Figure 7 ‣ Learned Perceptual Image Patch Similarity (LPIPS). ‣ Appendix D More
    Details on Evaluation Metrics ‣ Knowledge-Aware Artifact Image Synthesis with
    LLM-Enhanced Prompting and Multi-Source Supervision") in this appendix.
  prefs: []
  type: TYPE_NORMAL
