- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ChainForge: 一个用于提示工程和 LLM 假设测试的视觉工具包'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.09128](https://ar5iv.labs.arxiv.org/html/2309.09128)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.09128](https://ar5iv.labs.arxiv.org/html/2309.09128)
- en: Ian Arawjo [ianarawjo@g.harvard.edu](mailto:ianarawjo@g.harvard.edu) Harvard
    UniversityCambridgeMassachusettsUSA02134 ,  Chelse Swoopes [cswoopes@g.harvard.edu](mailto:cswoopes@g.harvard.edu)
    Harvard UniversityCambridgeMassachusettsUSA02134 ,  Priyan Vaithilingam [pvaithilingam@g.harvard.edu](mailto:pvaithilingam@g.harvard.edu)
    Harvard UniversityCambridgeMassachusettsUSA02134 ,  Martin Wattenberg [wattenberg@seas.harvard.edu](mailto:wattenberg@seas.harvard.edu)
    Harvard UniversityCambridgeMassachusettsUSA02134  and  Elena Glassman [glassman@seas.harvard.edu](mailto:glassman@seas.harvard.edu)
    Harvard UniversityCambridgeMassachusettsUSA02134
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ian Arawjo [ianarawjo@g.harvard.edu](mailto:ianarawjo@g.harvard.edu) 哈佛大学 剑桥
    马萨诸塞州 美国 02134 ， Chelse Swoopes [cswoopes@g.harvard.edu](mailto:cswoopes@g.harvard.edu)
    哈佛大学 剑桥 马萨诸塞州 美国 02134 ， Priyan Vaithilingam [pvaithilingam@g.harvard.edu](mailto:pvaithilingam@g.harvard.edu)
    哈佛大学 剑桥 马萨诸塞州 美国 02134 ， Martin Wattenberg [wattenberg@seas.harvard.edu](mailto:wattenberg@seas.harvard.edu)
    哈佛大学 剑桥 马萨诸塞州 美国 02134 和 Elena Glassman [glassman@seas.harvard.edu](mailto:glassman@seas.harvard.edu)
    哈佛大学 剑桥 马萨诸塞州 美国 02134
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Evaluating outputs of large language models (LLMs) is challenging, requiring
    making—and making sense of—many responses. Yet tools that go beyond basic prompting
    tend to require knowledge of programming APIs, focus on narrow domains, or are
    closed-source. We present ChainForge, an open-source visual toolkit for prompt
    engineering and on-demand hypothesis testing of text generation LLMs. ChainForge
    provides a graphical interface for comparison of responses across models and prompt
    variations. Our system was designed to support three tasks: model selection, prompt
    template design, and hypothesis testing (e.g., auditing). We released ChainForge
    early in its development and iterated on its design with academics and online
    users. Through in-lab and interview studies, we find that a range of people could
    use ChainForge to investigate hypotheses that matter to them, including in real-world
    settings. We identify three modes of prompt engineering and LLM hypothesis testing:
    opportunistic exploration, limited evaluation, and iterative refinement.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 评估大型语言模型（LLMs）的输出是一项挑战，需要对大量响应进行理解和处理。然而，超越基本提示的工具往往需要编程 API 知识，集中于狭窄领域，或是闭源的。我们推出了
    ChainForge，这是一个开源的视觉工具包，用于提示工程和按需假设测试文本生成 LLMs。ChainForge 提供了一个图形界面，用于比较模型和提示变化之间的响应。我们的系统旨在支持三项任务：模型选择、提示模板设计和假设测试（例如，审计）。我们在开发早期发布了
    ChainForge，并与学术界和在线用户一起迭代其设计。通过实验室研究和访谈研究，我们发现各种人群能够使用 ChainForge 来调查对他们重要的假设，包括在现实世界环境中。我们识别了三种提示工程和
    LLM 假设测试的模式：机会探索、有限评估和迭代改进。
- en: 'language models, toolkits, visual programming environments, prompt engineering,
    auditing^†^†copyright: none^†^†ccs: Human-centered computing Interactive systems
    and tools^†^†ccs: Human-centered computing Empirical studies in interaction design^†^†ccs:
    Computing methodologies Natural language processing![Refer to caption](img/25bf782f3ddb52816baff75d7b108dc7.png)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '语言模型、工具包、视觉编程环境、提示工程、审计^†^†版权：无^†^†ccs: 以人为本的计算 互动系统与工具^†^†ccs: 以人为本的计算 互动设计中的实证研究^†^†ccs:
    计算方法 自然语言处理![参见标题](img/25bf782f3ddb52816baff75d7b108dc7.png)'
- en: 'Figure 1\. The ChainForge interface, depicting a limited evaluation that tests
    a model’s robustness to prompt injection attacks. The entire experiment was developed
    in 15 minutes, and illustrates a key benefit of ChainForge’s design: the evaluation
    logic can be followed from start to finish in a single screenshot. Users can share
    this flow as a file or web link.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. ChainForge 界面，展示了一个有限的评估，测试模型对提示注入攻击的鲁棒性。整个实验在15分钟内完成，说明了ChainForge设计的一个关键优势：评估逻辑可以从头到尾通过一个截图跟踪。用户可以将这一流程作为文件或网页链接分享。
- en: \Description
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: 1\. Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: Large language models (LLMs) have captured imaginations, and concerns, across
    the world. Both imagination and concern derives, in part, from ambiguity around
    model capabilities—the difficulty of characterizing LLM behavior. Everyone from
    developers to model auditors encounters this same challenge. Developers struggle
    with “prompt engineering,” or finding a prompt that leads to consistent, quality
    outputs (Beurer-Kellner et al., [2023](#bib.bib5); Liffiton et al., [2023](#bib.bib22)).
    Auditors of models, to check for bias, must learn programming APIs to test hypotheses
    systematically. To help demystify LLMs, we need powerful, accessible tools that
    help people gain more comprehensive understandings of LLM behavior, beyond a single
    prompt or chat.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在全球范围内激发了人们的想象力和担忧。这些想象力和担忧部分源于对模型能力的模糊性——即描述LLM行为的困难。从开发人员到模型审计员，每个人都面临着这个挑战。开发人员在“提示工程”中苦苦挣扎，或寻找能产生一致、高质量输出的提示（Beurer-Kellner等，[2023](#bib.bib5);
    Liffiton等，[2023](#bib.bib22)）。模型审计员为了检查偏见，必须学习编程API，以系统地测试假设。为了帮助揭开LLMs的神秘面纱，我们需要强大且易于访问的工具，帮助人们获得对LLM行为更全面的理解，而不仅仅是一个提示或对话。
- en: 'In this paper, we introduce a visual toolkit, ChainForge, that supports on-demand
    hypothesis testing of the behavior of text generating LLMs on open-domain tasks,
    with minimal to no coding required. We describe the design of ChainForge, including
    how it was motivated from real use cases at our university, and how our design
    evolved with feedback from fellow academics and online users. Since early summer
    2023, ChainForge has been publicly available at [chainforge.ai](https://chainforge.ai)
    as web and local software, is free and open-source, and allows users to share
    their experiments with others as files or links. Unlike other systems work in
    HCI, we developed ChainForge in the open, seeking an alternative to closed-off
    or ‘prototype-and-move-on’ patterns of work. Since its launch, our tool has been
    used by many people, including in other HCI research projects submitted to this
    very conference. We report a qualitative user study engaging a range of participants,
    including people with non-computing backgrounds. Our goal was to examine how users
    applied ChainForge to tasks that mattered to them, position the tools’ strengths
    and limitations, and pose implications for future interfaces. We show that users
    were able to apply ChainForge to a variety of investigations, from plotting LLMs’
    understanding of material properties, to discovering subtle biases in model outputs
    across languages. Through a small interview study, we found that actual users
    find ChainForge useful for real-world tasks, including by extending its source
    code, and remark on differences between their usage and in-lab users.’ Consistent
    with HCI ‘toolkit’ or constructive research (Ledo et al., [2018](#bib.bib20)),
    our contributions are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一个可视化工具包ChainForge，它支持对文本生成LLMs在开放领域任务中的行为进行按需假设测试，无需或仅需最少的编码。我们描述了ChainForge的设计，包括其如何受到我们大学实际使用案例的启发，以及我们的设计如何随着学术界和在线用户的反馈而演变。自2023年初夏以来，ChainForge已作为网页和本地软件公开提供于[chainforge.ai](https://chainforge.ai)，是免费的开源软件，允许用户将他们的实验以文件或链接的形式与他人共享。与其他HCI系统的工作不同，我们在公开的环境中开发了ChainForge，寻求一种替代封闭式或‘原型并转移’的工作模式。自发布以来，我们的工具已被许多人使用，包括在提交到本次会议的其他HCI研究项目中。我们报告了一项定性用户研究，涵盖了包括非计算背景人员在内的一系列参与者。我们的目标是考察用户如何将ChainForge应用于对他们重要的任务，定位工具的优势和局限性，并提出对未来界面的启示。我们展示了用户能够将ChainForge应用于各种调查，从绘制LLMs对材料属性的理解，到发现模型输出中的细微偏见。通过一项小规模的访谈研究，我们发现实际用户发现ChainForge对现实任务有用，包括通过扩展其源代码，并指出他们的使用与实验室用户的差异。与HCI‘工具包’或建设性研究（Ledo等，[2018](#bib.bib20)）一致，我们的贡献包括：
- en: •
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the *artifact* of ChainForge, which is publicly available, open-source, and
    iteratively developed with users
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ChainForge的*工具*，它是公开的、开源的，并与用户迭代开发
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*in-lab usability* and *interview studies* of a system for open-ended, on-demand
    hypothesis testing of LLM behavior'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*实验室可用性*和*访谈研究*，用于对LLM行为进行开放式、按需假设测试的系统'
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*implications* for future tools which target prompt engineering and hypothesis
    testing of LLM outputs'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对未来旨在进行提示工程和LLM输出假设测试的工具的*启示*
- en: 'Synthesizing across studies, we identify three modes of prompt engineering
    and LLM hypothesis testing more broadly: *opportunistic exploration*, *limited
    evaluation*, and *iterative refinement*. These modes highlight different stages
    and user mindsets when prompt engineering and testing hypotheses. As design contributions,
    we also present one of the first prompt engineering tools that supports *cross-LLM*
    comparison in the HCI literature, and introduce the notion of *prompt template
    chaining*, an extension of AI chains (Wu et al., [2022b](#bib.bib45)), where prompt
    templates may be recursively nested.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 综合各项研究，我们识别出提示工程和 LLM 假设测试的三种模式：*机会探索*、*有限评估*和*迭代优化*。这些模式突出了提示工程和假设测试中的不同阶段和用户心态。作为设计贡献，我们还介绍了
    HCI 文献中首个支持*跨 LLM* 比较的提示工程工具，并引入了*提示模板链*的概念，这是 AI 链的扩展（Wu et al., [2022b](#bib.bib45)），其中提示模板可以递归嵌套。
- en: 'Our studies demonstrate that many users found ChainForge effective for the
    very tasks and behaviors targeted by our design goals—model selection, prompt
    iteration, hypothesis testing—with some perceiving it to be more efficient than
    tools like Jupyter notebooks. Our findings on a structured task also suggest decisions
    around prompts and models are highly subjective: even given the same criteria
    and scenario, user interpretations and ranking of criteria can vary widely. Finally,
    we found that many real-world users were using ChainForge for a need we had not
    anticipated: *prototyping data processing pipelines*. Although prior research
    focuses on AI chaining or prompt engineering (Wu et al., [2022b](#bib.bib45);
    Mishra et al., [2023](#bib.bib26); Brade et al., [2023](#bib.bib7); Zamfirescu-Pereira
    et al., [2023](#bib.bib46)), they provide little to no context on *why* real people
    would prompt engineer or program an AI chain. We find that while users’ *sub*tasks
    matched our design goals (e.g., prompt template iteration, choosing a model),
    these subtasks were usually in service of one of two overarching goals—*prototyping
    data processing pipelines*, or *testing model behavior* (i.e., auditing). When
    prompt engineering is placed into a larger context of data processing, unique
    needs and pain-points of our real-world users—getting data out, sharing with others—seem
    obvious in retrospect. We recommend that future systems for prompt engineering
    or AI chains consider users’ broader context and goals beyond prompt/chain iteration
    itself—and, especially, that they draw inspiration from past frameworks for data
    processing.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究表明，许多用户发现 ChainForge 在我们的设计目标所针对的任务和行为中非常有效——模型选择、提示迭代、假设测试——有些用户认为它比 Jupyter
    notebooks 等工具更高效。我们在结构化任务中的发现还表明，关于提示和模型的决策高度主观：即使在相同的标准和情境下，用户的解释和标准排序也可能大相径庭。最后，我们发现许多现实世界的用户使用
    ChainForge 来满足我们未曾预料的需求：*原型数据处理管道*。虽然以前的研究专注于 AI 链接或提示工程（Wu et al., [2022b](#bib.bib45);
    Mishra et al., [2023](#bib.bib26); Brade et al., [2023](#bib.bib7); Zamfirescu-Pereira
    et al., [2023](#bib.bib46)），但它们几乎没有提供关于*为什么*真实用户会进行提示工程或编程 AI 链的背景。我们发现，尽管用户的*子*任务与我们的设计目标相符（例如，提示模板迭代、选择模型），这些子任务通常服务于两个总体目标中的一个——*原型数据处理管道*或*测试模型行为*（即审计）。当将提示工程放在数据处理的更大背景下时，我们现实世界用户的独特需求和痛点——获取数据、与他人共享——在事后看来似乎显而易见。我们建议未来的提示工程或
    AI 链系统考虑用户更广泛的背景和目标，而不仅仅是提示/链的迭代——尤其是，从过去的数据处理框架中汲取灵感。
- en: 2\. Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: Over the past decade, rising interest in machine learning (ML) has produced
    an industry of software for ML operations (“MLOps”). Tools generally target ML
    experts and cover tasks across the ML pipeline (Huyen, [2022](#bib.bib13)) from
    dataset curation, to training, to evaluating performance (e.g. Google Vertex AI).
    LLMs have brought their own unique challenges and users. LLMs are too big to fully
    evaluate across all possible use cases; are frequently black-boxed or virtually
    impossible to ‘explain’ (Sun et al., [2022](#bib.bib39); Binder et al., [2022](#bib.bib6))
    ; and finding the right prompt or model has become an industry unto itself. Compounding
    these issues, users of LLMs are frequently not ML experts at all—such as auditors
    checking for bias, or non-ML software developers. LLMs are thus spurring their
    own infrastructure and tooling ecosystem (“LLMOps”).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，对机器学习（ML）的兴趣日益增加，催生了一个ML操作（“MLOps”）软件行业。这些工具通常面向ML专家，并涵盖ML管道中的任务（Huyen，[2022](#bib.bib13)），从数据集策划到训练，再到性能评估（例如，Google
    Vertex AI）。LLM带来了其独特的挑战和用户。LLM太大，无法在所有可能的使用案例中进行全面评估；常常是黑箱或几乎不可能“解释”（Sun等， [2022](#bib.bib39)；Binder等，
    [2022](#bib.bib6)）；找到合适的提示或模型已成为一个独立的行业。更复杂的是，LLM的用户通常根本不是ML专家——例如，检查偏差的审计员或非ML软件开发人员。因此，LLM正在推动其自身的基础设施和工具生态系统（“LLMOps”）。
- en: '![Refer to caption](img/ce6b9d50989e2b48cdbb8a87d470b133.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce6b9d50989e2b48cdbb8a87d470b133.png)'
- en: Figure 2\. The emerging space of tools for LLM operations (“LLMOps”).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. LLM 操作工具的新兴领域（“LLMOps”）。
- en: 'The LLMOps space is rapidly evolving. We represent the emerging ecosystem as
    a graph (Figure [2](#S2.F2 "Figure 2 ‣ 2\. Related Work ‣ ChainForge: A Visual
    Toolkit for Prompt Engineering and LLM Hypothesis Testing")), with *exploration*
    and *discovery* on one end (e.g., playgrounds, ChatGPT), and *systematic evaluation*
    and *testing* of LLM outputs on the other. This horizontal axis represents two
    related, but distinct parts of prompt engineering: *discovering* a prompt that
    works robustly according to user criteria, involving improvisation and experimentation
    both on the prompt and the criteria; and *evaluating* prompt(s) once chosen, usually
    in production contexts to ensure a change of prompt will not alter user experience.
    (These stages generalize beyond prompts to “chains” or AI agents (Wu et al., [2022b](#bib.bib45)).)
    The two aspects are analogous to software engineering, where environments like
    Jupyter Notebooks support messy exploration and fast prototyping, while automated
    pipelines ensure quality control. A vertical axis characterizes the style of interaction—from
    textual APIs to tools with a graphical user interface (GUI). In what follows,
    we zoom in to specific parts of this landscape.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLMOps 领域正在迅速发展。我们将新兴的生态系统表示为图（图 [2](#S2.F2 "Figure 2 ‣ 2\. Related Work ‣
    ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")），一端为*探索*和*发现*（例如，游乐场，ChatGPT），另一端为LLM输出的*系统评估*和*测试*。这一水平轴表示了提示工程中的两个相关但不同的部分：*发现*一个根据用户标准稳健工作的提示，涉及对提示和标准的即兴创作和实验；以及*评估*选择后的提示，通常在生产环境中，以确保提示的更改不会影响用户体验。（这些阶段也可以推广到“链”或AI代理（Wu等，
    [2022b](#bib.bib45)）。）这两个方面类似于软件工程，其中像Jupyter Notebooks这样的环境支持混乱的探索和快速原型制作，而自动化管道则确保质量控制。垂直轴描述了交互风格——从文本API到具有图形用户界面（GUI）的工具。在接下来的部分中，我们将详细探讨这个领域的具体部分。'
- en: LLMOps for Prompt Engineering. There are a growing number of academic projects
    designed for prompting LLMs (Brade et al., [2023](#bib.bib7); Jiang et al., [2022](#bib.bib15);
    Mishra et al., [2023](#bib.bib26); Wu et al., [2022a](#bib.bib44)), but few support
    systematic, as opposed to manual, evaluation of textual responses (Zamfirescu-Pereira
    et al., [2023](#bib.bib46)). For example, PromptMaker helps users create prompts
    with few-shot examples; authors concluded that users “found it difficult to systematically
    evaluate” their prompts, wished they could score responses, and that such scoring
    “tended to be highly specific to their use case… rather than a metric that could
    be universally applied” (Jiang et al., [2022](#bib.bib15)). One rare system addressing
    prompt evaluation for text generation is PromptAid (Mishra et al., [2023](#bib.bib26)),
    which uses a NLP paraphrasing model to perturb input prompts with semantically-similar
    rephrasings, resends the queries to a single LLM and plots evaluation scores.
    Powerful in concept, it was tested on only one sentiment analysis task, where
    all the test prompts, model, and evaluation metric were pre-defined for users.
    BotDesigner (Zamfirescu-Pereira et al., [2023](#bib.bib46)) supports prompt-based
    design of chat models, yet its evaluation was also highly structured around a
    specific task (creating an AI professional chef). It remains unclear how to support
    users in *open-ended* tasks that matter to them—especially comparing across multiple
    LLMs and setting up their own metrics—so that they test hypotheses about LLM behavior
    in an improvisational, yet systematic manner.¹¹1It also bears mentioning that
    many papers published about LLM-prompting are closed-source or unreleased, including
    PromptAid, PromptMaker, PromptChainer, and BotDesigner (Jiang et al., [2022](#bib.bib15);
    Mishra et al., [2023](#bib.bib26); Wu et al., [2022a](#bib.bib44); Zamfirescu-Pereira
    et al., [2023](#bib.bib46)).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**LLMOps** 用于提示工程。虽然有越来越多的学术项目旨在提示 LLMs（Brade et al., [2023](#bib.bib7); Jiang
    et al., [2022](#bib.bib15); Mishra et al., [2023](#bib.bib26); Wu et al., [2022a](#bib.bib44)），但很少有支持系统化评估文本响应的工具（Zamfirescu-Pereira
    et al., [2023](#bib.bib46)）。例如，**PromptMaker** 帮助用户通过少量示例创建提示；作者得出结论认为用户“发现系统化评估”他们的提示“很困难”，希望能够对响应进行评分，而且这种评分“往往高度特定于他们的使用案例……而不是一种可以普遍应用的度量标准”（Jiang
    et al., [2022](#bib.bib15)）。一个少数的系统解决了文本生成提示的评估问题，**PromptAid**（Mishra et al.,
    [2023](#bib.bib26)），它使用 NLP 改写模型对输入提示进行语义上类似的改写，将查询发送到单一 LLM 并绘制评估分数。虽然概念强大，但它只在一个情感分析任务上进行了测试，其中所有测试提示、模型和评估度量都预先定义给用户。**BotDesigner**（Zamfirescu-Pereira
    et al., [2023](#bib.bib46)）支持基于提示的聊天模型设计，但其评估也高度围绕特定任务（创建 AI 专业厨师）进行。如何支持用户在对他们重要的*开放式*任务中，尤其是跨多个
    LLM 进行比较并设置自己的度量标准，仍然不明确——以便他们以即兴而系统的方式测试有关 LLM 行为的假设。¹¹1此外，值得一提的是，许多关于 LLM 提示的论文是闭源或未发布的，包括
    **PromptAid**、**PromptMaker**、**PromptChainer** 和 **BotDesigner**（Jiang et al.,
    [2022](#bib.bib15); Mishra et al., [2023](#bib.bib26); Wu et al., [2022a](#bib.bib44);
    Zamfirescu-Pereira et al., [2023](#bib.bib46)）。'
- en: Since we launched ChainForge, a number of commercial prompt engineering and
    LLMOps tools have emerged, and more emerge everyday.²²2For instance, as we write
    this, we see the launch of *baserun.ai*, a Y-Combinator backed startup for LLM
    output evaluation. Like many such startups, the website is glossy and promises
    much, but the tool itself is inaccessible and limited to a few screenshots. Examples
    are Weights and Biases Prompts, nat.dev, Vellum.ai, Vercel, Zeno Build, and promptfoo (Weights
    and Biases, [2023](#bib.bib43); Friedman et al., [2023](#bib.bib10); Vellum, [2023](#bib.bib41);
    Vercel, [2023](#bib.bib42); Neubig and He, [2023](#bib.bib27); promptfoo, [2023](#bib.bib34)).
    These systems range from prompting sandboxes (OpenAI, [2023a](#bib.bib30)) to
    prompt verification and versioning inside production applications, and usually
    rely upon integration with code, command-line scripts, or config files (TruLens,
    [2023](#bib.bib40); promptfoo, [2023](#bib.bib34); OpenAI, [2023b](#bib.bib31)).
    For instance, promptfoo (promptfoo, [2023](#bib.bib34)) is an evaluation harness
    akin to testing frameworks like jest (Jest, [2023](#bib.bib14)), where users write
    config files that specify prompts and expected outputs. Tests are run from the
    command line. Although most systems support prompt templating, few support sending
    each prompt to multiple models at once; the few that support cross-model comparison,
    like Vellum.ai, are playgrounds that test single prompts, making it cumbersome
    to compare systematically.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 自从我们推出 ChainForge 以来，许多商业化的提示工程和 LLMOps 工具已经出现，而且每天都有新工具问世。例如，在我们撰写这篇文章时，我们看到了
    *baserun.ai* 的发布，这是一个由 Y-Combinator 支持的初创企业，专注于 LLM 输出评估。像许多这样的初创企业一样，网站看起来光鲜亮丽，承诺很多，但工具本身却无法访问，仅限于几个截图。示例包括
    Weights and Biases Prompts、nat.dev、Vellum.ai、Vercel、Zeno Build 和 promptfoo（Weights
    and Biases，[2023](#bib.bib43)；Friedman 等，[2023](#bib.bib10)；Vellum，[2023](#bib.bib41)；Vercel，[2023](#bib.bib42)；Neubig
    和 He，[2023](#bib.bib27)；promptfoo，[2023](#bib.bib34)）。这些系统涵盖了从提示沙箱（OpenAI，[2023a](#bib.bib30)）到生产应用中的提示验证和版本控制，通常依赖于与代码、命令行脚本或配置文件的集成（TruLens，[2023](#bib.bib40)；promptfoo，[2023](#bib.bib34)；OpenAI，[2023b](#bib.bib31)）。例如，promptfoo（promptfoo，[2023](#bib.bib34)）是一个评估工具，类似于测试框架如
    jest（Jest，[2023](#bib.bib14)），用户编写配置文件来指定提示和预期输出。测试从命令行运行。虽然大多数系统支持提示模板化，但很少有系统支持同时将每个提示发送到多个模型；那些支持跨模型比较的系统，如
    Vellum.ai，都是测试单个提示的游乐场，使得系统性比较变得繁琐。
- en: 'Visual Data Flow Environments for LLMOps. Related visually, but distinct from
    our design concern of evaluation, are visual data flow environments built around
    LLM responses. These have two flavors: sensemaking interfaces for information
    foraging, and tools for designing LLM applications. Graphologue and Sensecape,
    instances of the former, are focused on helping users interact non-linearly with
    a chat LLM and provide features to, for example, elaborate on its answers (Suh
    et al., [2023](#bib.bib38); Jiang et al., [2023](#bib.bib16)). Second are systems
    for designing LLM-based applications, usually integrating with the LangChain Python
    package (Lan, [2023](#bib.bib2)): Langflow, Flowise, and Microsoft PromptFlow
    on Azure services (Logspace, [2023](#bib.bib23); FlowiseAI, [2023](#bib.bib9);
    Microsoft, [2023](#bib.bib25)). All three tools were predated by PromptChainer,
    a closed-source visual programming environment for LLM app development by Wu et
    al. (Wu et al., [2022a](#bib.bib44)). Such environments focus on constructing
    “AI chains” (Wu et al., [2022b](#bib.bib45)), or data flows between LLMs and other
    tools or scripts. Here, we leverage design concepts from visual flow-based tools,
    while focusing our design on supporting exploration and evaluation of LLM response
    quality. One key difference is the need for hypothesis testing tools to support
    combinatorial power, i.e., querying multiple models with multiple prompts at once,
    whereas both LLM app building and sensemaking tools focus on single responses
    and models.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 面向LLMOps的可视化数据流环境。与我们的评估设计关注点相关，但有所不同的是，围绕LLM响应构建的可视化数据流环境。这些环境有两种类型：信息搜索的意义构建界面和LLM应用设计工具。Graphologue和Sensecape是前者的实例，专注于帮助用户与聊天LLM进行非线性互动，并提供例如详细阐述其回答的功能（Suh
    et al., [2023](#bib.bib38); Jiang et al., [2023](#bib.bib16)）。其次是设计LLM应用的系统，通常与LangChain
    Python包集成（Lan, [2023](#bib.bib2)）：Langflow、Flowise和Microsoft PromptFlow在Azure服务中（Logspace,
    [2023](#bib.bib23); FlowiseAI, [2023](#bib.bib9); Microsoft, [2023](#bib.bib25)）。这三种工具都被Wu
    et al.开发的PromptChainer所先行，它是一个封闭源代码的LLM应用开发可视化编程环境（Wu et al., [2022a](#bib.bib44)）。这些环境专注于构建“AI链”（Wu
    et al., [2022b](#bib.bib45)），即LLM与其他工具或脚本之间的数据流。在这里，我们利用了可视化流基工具的设计概念，同时将设计重点放在支持LLM响应质量的探索和评估上。一个关键的区别是需要假设测试工具以支持组合能力，即一次查询多个模型与多个提示，而LLM应用构建和意义构建工具则专注于单个响应和模型。
- en: 'Overall, then, the evolving LLMOps landscape may be summarized as follows.
    Tools for prompt discovery appear largely limited to simple playgrounds or chats,
    where users send off single prompts at a time through trial and error. Tools for
    systematic testing, on the other hand, tend to require idiosyncratic config files,
    command-line calls, ML engineering knowledge, or integration with a programming
    API—making them difficult to use for discovery and improvisation (not to mention
    non-programmers). We wanted to design a system to bridge the gap between *exploration*
    and *evaluation* aspects of LLMOps: a graphical interface that facilitates rapid
    discovery and iteration, but also inspection of many responses and systematic
    evaluation, without requiring extensive knowledge of a programming API. By blending
    the usability of visual programming tools with power features like sending the
    same prompts to multiple LLMs at once, we sought to make it easier for people
    to experiment with and characterize LLM behavior.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，正在发展的LLMOps领域可以总结如下。提示发现工具主要局限于简单的游乐场或聊天中，用户一次通过试错发送单一提示。另一方面，系统测试工具通常需要特有的配置文件、命令行调用、机器学习工程知识或与编程API的集成——使其难以用于发现和即兴创作（更不用说非程序员了）。我们希望设计一个系统，弥合LLMOps的*探索*和*评估*方面的差距：一个图形界面，既能促进快速发现和迭代，又能检查大量响应和系统评估，而无需广泛的编程API知识。通过将可视化编程工具的可用性与将相同提示同时发送给多个LLM等强大功能结合起来，我们旨在简化人们对LLM行为的实验和特征描述。
- en: 3\. Design Goals and Motivation
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 设计目标与动机
- en: The impetus for ChainForge came from our own experience testing prompts while
    developing LLM-powered software for other research projects. Across our research
    lab, we needed a way to systematically test prompts to reach one that satisfied
    certain criteria. This criteria was project-specific and evolved improvisationally
    over development. We also noticed other researchers and industry developers facing
    similar problems when trying to evaluate LLM behavior.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ChainForge的推动力来源于我们在为其他研究项目开发LLM驱动的软件时测试提示的经验。在我们的研究实验室中，我们需要一种系统的方法来测试提示，以达到符合特定标准的提示。这个标准是项目特定的，并在开发过程中不断演变。我们还注意到其他研究人员和行业开发者在尝试评估LLM行为时面临类似问题。
- en: 'We designed ChainForge for a broad range of tasks that fall into the category
    of *hypothesis testing* about LLM behavior. Hypothesis testing includes prompt
    engineering (finding a prompt involves coming up with hypotheses about prompts
    and testing them), but also encompasses auditing of models for security, bias
    and fairness, etc. Specifically, we intended our interface to support four concrete
    user goals and behaviors:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设计了ChainForge，适用于广泛的任务，这些任务属于关于LLM行为的*假设测试*。假设测试包括提示工程（找到一个提示涉及提出关于提示的假设并进行测试），但也包括对模型进行安全性、偏见和公平性等方面的审计。具体来说，我们希望我们的界面支持四个具体的用户目标和行为：
- en: D1.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D1。
- en: Model selection. Easy comparison of LLM behavior across models. We were motivated
    by fine-tuning LLMs, and how to ‘evaluate’ what changed in the fine-tuned versus
    the base model. Users should be able to gain quick insights into what model to
    use, or which performs the ‘best’ for their use case.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型选择。轻松比较不同模型下的LLM行为。我们的动机来源于对LLM进行微调，以及如何‘评估’微调后的模型与基础模型之间的变化。用户应该能够快速了解应该使用哪个模型，或者哪个模型在他们的使用案例中表现‘最佳’。
- en: D2.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D2。
- en: Prompt template design. Prompt engineers typically need to find not a good prompt,
    but a good prompt *template* (a prompt with variables in {braces}) that performs
    consistently across many possible inputs. Existing tools make it difficult to
    compare, side-by-side, differences between templates, and thus hinder quick iteration.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示模板设计。提示工程师通常需要寻找的不仅仅是一个好的提示，而是一个能够在多种可能输入下保持一致表现的良好**提示模板**（一个包含变量的提示）。现有工具使得很难并排比较模板之间的差异，从而阻碍了快速迭代。
- en: D3.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D3。
- en: Systematic evaluation. To verify hypotheses about LLM behavior beyond anecdotal
    evidence, one needs a mass of responses (and ideally more than a single response
    per prompt). However, manual inspection (scoring) of responses becomes time-consuming
    and unwieldy quickly. To rectify this, the system must support sending a ton of
    parametrized queries, help users navigate them and score them according to their
    own idiosyncratic critera (Jiang et al., [2022](#bib.bib15)), and facilitate quick
    skimming of results (e.g., via plots).
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统评估。为了验证关于大语言模型（LLM）行为的假设，超越轶事证据的需要，需要大量的响应（理想情况下，每个提示超过一个响应）。然而，手动检查（评分）响应很快变得费时且不便。为了解决这个问题，系统必须支持发送大量参数化查询，帮助用户浏览这些查询并根据他们自己的特定标准进行评分（Jiang
    et al., [2022](#bib.bib15)），并促进结果的快速浏览（例如，通过图表）。
- en: D4.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D4。
- en: Improvisation (Kang et al., [2018](#bib.bib17)). We imagined a system that supported
    quick-and-messy iteration and likened its role to Jupyter Notebooks in software
    engineering. If in the course of exploration a user develops another hypothesis
    they wish to test, the system should support on-demand testing of that hypothesis—whether
    amending prompts, swapping models, or changing evaluations. This design goal is
    in tension with D3, even sometimes embracing imprecision in measuring response
    quality—although we imagined the system could conduct detailed evaluations, our
    primary goal was to support on-demand (as opposed to paper-quality) evaluations.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即兴创作（Kang et al., [2018](#bib.bib17)）。我们设想了一个支持快速且混乱迭代的系统，并将其角色比作软件工程中的Jupyter
    Notebooks。如果在探索过程中，用户提出了另一个希望测试的假设，系统应该支持按需测试该假设——无论是修改提示、切换模型还是更改评估。这个设计目标与D3相矛盾，有时甚至接受对响应质量测量的不精确——尽管我们设想该系统可以进行详细评估，但我们的主要目标是支持按需（而非纸面质量的）评估。
- en: We also had two high-level goals. We wanted the system to take care of ‘the
    basics’—such as prompting multiple models at once, plotting graphs, or inspecting
    responses—such that researchers could extend or leverage our project to enable
    more nuanced research questions (for instance, designing their own visualization
    widget). Second, we wanted to explore open-source iteration, where, unlike typical
    HCI system research, online users themselves can give feedback on the project
    via GitHub. In part, we were motivated by disillusionment with close-source or
    ‘prototype-and-move-on’ patterns of work in HCI, which risk ecological validity
    and tend to privilege academic notoriety over public benefit (Greenberg and Buxton,
    [2008](#bib.bib12)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有两个高层次的目标。我们希望系统能够处理“基础工作”——例如同时提示多个模型、绘制图表或检查响应——以便研究人员可以扩展或利用我们的项目来实现更微妙的研究问题（例如，设计自己的可视化小部件）。其次，我们希望探索开源迭代，与典型的
    HCI 系统研究不同，在线用户可以通过 GitHub 对项目提供反馈。部分原因是我们对封闭源或“原型并继续”工作模式感到失望，这些模式存在生态有效性风险，并倾向于优先考虑学术声誉而非公众利益（Greenberg
    和 Buxton，[2008](#bib.bib12)）。
- en: Finally, we were guided by differentiation and enrichment theories of human
    learning, Variation Theory (Marton, [2014](#bib.bib24)) and Analogical Learning
    Theory (Gentner and Markman, [1997](#bib.bib11)), which are complementary perspectives
    on the value of variation within (structurally) aligned, diverse data. Both theories
    hold that experiencing variation within and across objects of learning (in this
    case, models, prompts and/or prompt variables) helps humans develop more accurate
    mental models that more robustly generalize to novel scenarios. ChainForge provides
    infrastructure that helps users set up these juxtapositions across analogous differences
    across dimensions of variation that, given what they want to learn, users construct,
    i.e., by choosing multiple models, prompts, and/or values for prompt variables.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们受到人类学习的区分和丰富理论、变异理论（Marton，[2014](#bib.bib24)）以及类比学习理论（Gentner 和 Markman，[1997](#bib.bib11)）的指导，这些理论是对结构上对齐、多样化数据中变异价值的互补视角。这两种理论都认为，经历学习对象中的变异（在这种情况下，即模型、提示和/或提示变量）有助于人们发展出更准确的心理模型，这些模型能更稳健地推广到新的情境。ChainForge
    提供了帮助用户设置这些对比的基础设施，通过选择多个模型、提示和/或提示变量，用户可以构建他们所需学习的内容。
- en: 4\. ChainForge
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. ChainForge
- en: Before describing our system in detail, we walk readers through one example
    usage scenario. The scenario relates to the real-world need to make LLMs robust
    against prompt injection attacks (Perez and Ribeiro, [2022](#bib.bib33)), and
    derives from an interaction the first author had with Google Doc’s AI writing
    assistant, where the tool, supposed to suggest rewriting of highlighted text,
    took the text as a command instead. More case studies of usage will be presented
    in our findings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在详细描述我们的系统之前，我们通过一个示例使用场景引导读者。这个场景涉及到使 LLMs 对抗提示注入攻击（Perez 和 Ribeiro，[2022](#bib.bib33)）的实际需求，并源于第一作者与
    Google Doc 的 AI 写作助手的互动，其中该工具本应建议重写高亮文本，却将文本视为命令。我们的发现中将展示更多使用案例研究。
- en: Farah is developing an AI writing assistant where users can highlight text in
    their document and click buttons to expand, shorten, or rewrite the text. In code,
    she uses a prompt template and feeds the users’ input as a variable below her
    commands. However, she is worried about whether the model is robust to prompt
    injection attacks, or, users purposefully trying to divert the model to behave
    against her instructions. She decides to compare a few models and choose whichever
    is most robust. Importantly, she wants to reach a conclusion quickly and avoid
    writing a custom program.
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: Farah 正在开发一个 AI 写作助手，用户可以在文档中高亮文本，然后点击按钮来扩展、缩短或重写文本。在代码中，她使用一个提示模板，并将用户输入作为她命令下方的变量。然而，她担心模型是否对提示注入攻击具有鲁棒性，或者用户是否故意试图让模型偏离她的指令。她决定比较几个模型，并选择最具鲁棒性的那个。重要的是，她希望快速得出结论，避免编写自定义程序。
- en: ''
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Loading ChainForge, Farah adds a Prompt Node and pastes in her prompt template
    (Figure [1](#S0.F1 "Figure 1 ‣ ChainForge: A Visual Toolkit for Prompt Engineering
    and LLM Hypothesis Testing")). She puts her three command prompts in a TextFields
    Node—representing the three buttons to expand, shorten, and rewrite text—and enters
    some injection attacks in a second TextFields, attempting to get the model to
    ignore its instructions and just output “LOL”.³³3ChainForge can actually be used
    to compare across system instructions for OpenAI models as well, but for simplicity
    here, we put the instruction in the prompt. Farah could also templatize the “LOL”
    to test on a variety of different injection values, and use that variable’s value
    in evaluators. She connects the TextFields to her template variables {command}
    and {input}, respectively. Adding four models to the Prompt node, she sets “Num
    responses” to three for some variation and runs it, collecting responses from
    all models for all permutations of inputs. Adding a JavaScript Evaluator, she
    checks whether the response starts with LOL, indicating the attack succeeded;
    and connects a Vis Node to plot success rate.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '加载 ChainForge 后，Farah 添加了一个 Prompt Node 并粘贴了她的提示模板（图 [1](#S0.F1 "Figure 1 ‣
    ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")）。她将三个命令提示放在一个
    TextFields Node 中——代表扩展、缩短和重写文本的三个按钮——并在第二个 TextFields 中输入一些注入攻击，试图让模型忽略其指令并仅输出“LOL”。³³3
    实际上，ChainForge 也可以用于比较 OpenAI 模型的系统指令，但为了简单起见，我们将指令放在提示中。Farah 还可以将“LOL”模板化，以测试不同的注入值，并在评估器中使用该变量的值。她将
    TextFields 分别连接到她的模板变量 {command} 和 {input}。添加四个模型到 Prompt node，她将“Num responses”设置为三以获取一些变化，然后运行它，收集所有模型对所有输入组合的响应。添加
    JavaScript Evaluator 后，她检查响应是否以 LOL 开头，以指示攻击是否成功；并连接一个 Vis Node 来绘制成功率。'
- en: ''
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: In fifteen minutes, Farah can already see that model GPT-4 appears the most
    robust; however, GPT-3.5 is not far behind.⁴⁴4Actual scores depicted; uses March
    2023 versions of OpenAI models *gpt-3.5-turbo* and *gpt-4*, Anthropic’s *claude-2*,
    and Google’s *chat-bison-001*. She sends the flow to her colleagues and chats
    with them about which model to choose, given that GPT-4 is more expensive. The
    team agrees to go with GPT-3.5, but a colleague suggests they remove all but the
    GPT models and try different variations of their command prompts, including statements
    not to listen to injection-style attacks…
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在十五分钟内，Farah 已经可以看到模型 GPT-4 显得最为强大；然而，GPT-3.5 也不甘示弱。⁴⁴4实际评分如图所示；使用了 2023 年 3
    月版本的 OpenAI 模型 *gpt-3.5-turbo* 和 *gpt-4*、Anthropic 的 *claude-2* 以及 Google 的 *chat-bison-001*。她将流程发送给同事，与他们讨论选择哪个模型，因为
    GPT-4 更昂贵。团队决定使用 GPT-3.5，但一位同事建议他们去掉所有除了 GPT 模型的选项，并尝试不同的命令提示变体，包括不听从注入式攻击的语句…
- en: Farah and her colleagues might continue to use ChainForge to iterate on their
    prompts, testing criteria, etc., or just decide on a model and move on. The expected
    usage is that the team uses ChainForge to reach conclusions quickly, then proceeds
    elsewhere with their implementation. Note that while Farah’s task might fall under
    the rubric of “prompt engineering,” there is also an auditing component, and we
    designed the system to support a variety of scenarios beyond this example.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Farah 和她的同事们可能会继续使用 ChainForge 对他们的提示进行迭代，测试标准等，或者直接决定一个模型然后继续前进。预期的使用方式是团队使用
    ChainForge 快速得出结论，然后在其他地方进行实现。请注意，虽然 Farah 的任务可能属于“提示工程”范畴，但系统也包括审计组件，我们设计的系统支持多种场景，超出了这个例子。
- en: 4.1\. Design Overview
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 设计概述
- en: 'The main ChainForge interface is depicted in Figure [1](#S0.F1 "Figure 1 ‣
    ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing").
    Common to data flow programming environments (Wu et al., [2022a](#bib.bib44)),
    users can add nodes and connect them by edges. ChainForge has four types of nodes—inputs,
    generators, evaluators, and visualizers—as well as miscellany like comment nodes
    (available nodes listed in Appendix A, Table [3](#A1.T3 "Table 3 ‣ Appendix A
    List of Nodes ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis
    Testing")). This typology roughly aligns with the “cells, generators, lenses”
    writing tool LLM framework of Kim et al. (Kim et al., [2023](#bib.bib18)), but
    for a broader class of problems and node types. Like PromptChainer (Wu et al.,
    [2022a](#bib.bib44)), data flowing between nodes are typically LLM responses with
    metadata attached (with the exception of input nodes, which export text). Table [1](#S4.T1
    "Table 1 ‣ 4.3\. Implementation ‣ 4\. ChainForge ‣ ChainForge: A Visual Toolkit
    for Prompt Engineering and LLM Hypothesis Testing") describes how aspects of our
    implementation relate to design goals in Section 3\. For comprehensive information
    on nodes and features, we point readers to our documentation, provided in Supplementary
    Material and at [chainforge.ai/docs](https://chainforge.ai/docs). Hereafter, we
    focus on describing high-level design challenges unique to our tool and relevant
    for hypothesis testing.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 'ChainForge的主要界面如图[1](#S0.F1 "图 1 ‣ ChainForge: 一个用于提示工程和LLM假设测试的可视化工具包")所示。与数据流编程环境（Wu
    et al., [2022a](#bib.bib44)）相同，用户可以添加节点并通过边连接它们。ChainForge有四种类型的节点——输入、生成器、评估器和可视化器，以及诸如注释节点等其他杂项（可用节点列在附录A，第[3](#A1.T3
    "表 3 ‣ 附录 A 节点列表 ‣ ChainForge: 一个用于提示工程和LLM假设测试的可视化工具包")表）。这种类型大致与Kim等人的“单元格、生成器、透镜”写作工具LLM框架（Kim
    et al., [2023](#bib.bib18)）对齐，但适用于更广泛的问题和节点类型。像PromptChainer（Wu et al., [2022a](#bib.bib44)）一样，节点之间流动的数据通常是带有元数据的LLM响应（输入节点除外，它们导出文本）。表[1](#S4.T1
    "表 1 ‣ 4.3\. 实现 ‣ 4\. ChainForge ‣ ChainForge: 一个用于提示工程和LLM假设测试的可视化工具包")描述了我们实现的各个方面如何与第3节中的设计目标相关。有关节点和功能的全面信息，请参阅我们的文档，提供在补充材料中以及[chainforge.ai/docs](https://chainforge.ai/docs)。之后，我们将重点描述我们工具的独特高级设计挑战及其在假设测试中的相关性。'
- en: '![Refer to caption](img/81f8b959332aa786653a4b4f6db347ba.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/81f8b959332aa786653a4b4f6db347ba.png)'
- en: Figure 3\. An example of chaining prompt templates, one of ChainForge’s unique
    features. Users can test different templates at once by using the same input variable
    (in {} brackets). Templates can be chained at arbitrary depth using TextFields
    nodes. The user is hovering over the Run button of the Prompt Node, displaying
    a reactive tooltip of how many queries will be sent off.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 这是链式提示模板的一个示例，这是ChainForge的独特功能之一。用户可以通过使用相同的输入变量（在{}括号中）一次测试不同的模板。模板可以通过TextFields节点以任意深度进行链式连接。用户正悬停在提示节点的运行按钮上，显示了将发送多少个查询的动态提示。
- en: \Description
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: 'The key design difference between ChainForge and other flow-based LLMOps tools
    is combinatorial power—users can send off not only multiple prompts at once, but
    query multiple models, with multiple prompt variables that might be hierarchically
    organized (through chained templates) or carry additional metadata. This leads
    to what two users called the “multiverse problem.” Unique to this design is our
    Prompt Node, which allows users to query multiple models at once (Figure [3](#S4.F3
    "Figure 3 ‣ 4.1\. Design Overview ‣ 4\. ChainForge ‣ ChainForge: A Visual Toolkit
    for Prompt Engineering and LLM Hypothesis Testing")). Many features aim to help
    users navigate this multiverse of outputs and reduce complexity to reach conclusions
    across them, such as the response inspector, evaluators and visual plots. The
    combinatorial complexity of generating LLM queries in ChainForge may be summarized
    in an equation, roughly:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 'ChainForge与其他基于流的LLMOps工具之间的主要设计区别在于组合能力——用户不仅可以一次发送多个提示，还可以查询多个模型，使用可能层次组织的多个提示变量（通过链式模板）或携带额外元数据。这导致了两个用户所称的“多重宇宙问题”。我们独特的设计是我们的提示节点，它允许用户同时查询多个模型（图[3](#S4.F3
    "图 3 ‣ 4.1\. 设计概述 ‣ 4\. ChainForge ‣ ChainForge: 一个用于提示工程和LLM假设测试的可视化工具包")）。许多功能旨在帮助用户导航这些输出的多重宇宙，并减少复杂性以达成结论，如响应检查器、评估器和可视化图。生成LLM查询的组合复杂性在ChainForge中可以总结为一个方程，大致如下：'
- en: '|  | $(\textit{P prompts})\times(\textit{M models})\times(\textit{N responses
    per prompt})\times\texttt{max}(1,(\textit{C Chat histories}))$ |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\textit{P prompts})\times(\textit{M models})\times(\textit{N responses
    per prompt})\times\texttt{max}(1,(\textit{C Chat histories}))$ |  |'
- en: where P is produced through a combination of prompt variables, M may be generalized
    to response providers (model variations, AI agents, etc), and ${C}{=}{0}$ responses.
    All responses are cached; users can change upstream fields then re-prompt, and
    ChainForge will only send off queries it needs.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 P 是通过提示变量的组合生成的，M 可以推广到响应提供者（模型变体、AI 代理等），并且 ${C}{=}{0}$ 响应。所有响应都被缓存；用户可以更改上游字段然后重新提示，ChainForge
    仅发送所需的查询。
- en: '![Refer to caption](img/59feb33b647c97595a62dfe439c137a6.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/59feb33b647c97595a62dfe439c137a6.png)'
- en: Figure 4\. (A) The Response Inspector in Grouped List layout, showing four LLMs’
    responses side-by-side to the same prompt. Each color represents a different LLM,
    named in each box’s top-right corner. Here the user requested has $n=2$ responses
    per prompt, and has grouped responses by prompt variables *command* and then *input*.
    (B) Users can click on groupings (blue headers) to expand/collapse them. (C) An
    alternative Table Layout offers a grid for interactive comparison across prompt
    variables and models, where users can change the main column-plotted variable.
    Users can also export data to a spreadsheet (not shown). Interactive version at
    [chainforge.ai/play](https://chainforge.ai/play).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4. （A）分组列表布局中的响应检查器，显示四个 LLM 对相同提示的响应并排展示。每种颜色代表一个不同的 LLM，在每个框的右上角标注其名称。这里用户请求每个提示有
    $n=2$ 个响应，并按照提示变量 *command* 和 *input* 对响应进行分组。（B）用户可以点击分组（蓝色标题）来展开/折叠它们。（C）替代的表格布局提供了一个网格，以便在提示变量和模型之间进行互动比较，用户可以更改主列绘制的变量。用户还可以将数据导出到电子表格（未显示）。互动版本见
    [chainforge.ai/play](https://chainforge.ai/play)。
- en: 'To inspect responses, users open a pop-up Response Inspector (Figure [4](#S4.F4
    "Figure 4 ‣ 4.1\. Design Overview ‣ 4\. ChainForge ‣ ChainForge: A Visual Toolkit
    for Prompt Engineering and LLM Hypothesis Testing")). The inspector has two layouts:
    *Grouped List,* where users see LLM responses side-by-side for the same prompt
    and can organize responses by hierarchically grouping on input variables; and
    *Table,* with columns plotting input variables and/or LLMs by user choice. Both
    layouts present responses in colored boxes, representing an LLM’s response(s)
    to a single prompt (each color maps to a specific LLM and is consistent across
    the application). Grouped List has collapse-able response groups, with one opened
    by default; users can expand/collapse groups by clicking their headers. In Table
    layout, all rows appear at once. We observed in pilots that, depending on the
    user and the task, users preferred one view or the other.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '要检查响应，用户可以打开弹出式响应检查器（图 [4](#S4.F4 "图 4 ‣ 4.1\. 设计概述 ‣ 4\. ChainForge ‣ ChainForge:
    用于提示工程和 LLM 假设测试的可视化工具包")）。检查器有两种布局：*分组列表*，用户可以看到 LLM 对相同提示的响应并排展示，并按输入变量的层级分组组织响应；以及*表格*，根据用户选择以列的方式绘制输入变量和/或
    LLM。两种布局都以彩色框呈现响应，表示一个 LLM 对单个提示的响应（每种颜色映射到一个特定的 LLM，并在应用程序中保持一致）。分组列表有可折叠的响应组，默认情况下有一个展开；用户可以通过点击标题来展开/折叠组。在表格布局中，所有行会一次性出现。我们在试点中观察到，根据用户和任务的不同，用户对视图的偏好有所不同。'
- en: 'There are many more features, more than we can cover in limited space; but,
    to provide readers a greater sense of ChainForge, we present a more complex example,
    utilizing Tabular Data and Simple Evaluator nodes to conduct a ground truth evaluation
    on an OpenAI evals (OpenAI, [2023b](#bib.bib31)) benchmark (Figure [5](#S4.F5
    "Figure 5 ‣ 4.2\. Iterative Development with Online and Pilot Users ‣ 4\. ChainForge
    ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")).
    At each step, metadata (a prompt template’s “fill history”) annotates outputs,
    and may be referenced downstream in a chain. Here, the “Ideal” column of the Tabular
    Data (A) is used as a metavariable in a Simple Evaluator (C), checking if the
    LLM response contains the expected value. Note that “Ideal” *is not the input
    to a template*, but instead is associated, by virtue of the table, with the output
    to *Prompt*. The user has plotted by *command* (D) to compare differences in performance
    across two prompt variables. Spot-checking the stacked bar chart, they see Claude
    and Falcon.7B perform slightly better on one command than the other.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多的功能，超出了我们在有限空间内能够覆盖的范围；但是，为了让读者对 ChainForge 有更深入的了解，我们展示了一个更复杂的示例，利用表格数据和简单评估器节点在
    OpenAI evals（OpenAI，[2023b](#bib.bib31)）基准上进行真实情况评估（图 [5](#S4.F5 "图 5 ‣ 4.2\.
    与在线和试点用户的迭代开发 ‣ 4\. ChainForge ‣ ChainForge：一个用于提示工程和 LLM 假设测试的视觉工具包")）。在每一步，元数据（提示模板的“填充历史”）注释输出，并可能在链的下游进行引用。在这里，表格数据（A）的“理想”列被用作简单评估器（C）中的元变量，检查
    LLM 响应是否包含预期值。请注意，“理想”*不是模板的输入*，而是通过表格的关联，与*提示*的输出相关。用户通过*命令*（D）绘制图表，以比较两个提示变量的性能差异。通过检查堆叠条形图，他们发现
    Claude 和 Falcon.7B 在一个命令上的表现比另一个稍好。
- en: 4.2\. Iterative Development with Online and Pilot Users
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 与在线和试点用户的迭代开发
- en: We iterated ChainForge with pilot users (academics in computing) and online
    users (through public GitHub Issues and comments). We summarize the substantial
    changes and additions which resulted.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与试点用户（计算领域的学术人员）和在线用户（通过公共 GitHub 问题和评论）进行了 ChainForge 的迭代开发。我们总结了导致的实质性变化和新增功能。
- en: 'Early in ChainForge’s development, we tested it on ongoing research projects
    in our lab. The most important outcome was the development of *prompt template
    chaining*, where templates may be recursively nested, enabling comparing across
    prompt templates themselves (Fig. 3). Early use cases of ChainForge included:
    shortening text with minimal rewordings, checking what programming APIs were imported
    for what prompts, and evaluating how well responses conformed to a domain-specific
    language. For instance, we discovered that a ChatGPT prompt we were using performed
    worst for an ‘only delete words’ task, tending to reword the most compared to
    other prompts.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ChainForge 的早期开发中，我们在实验室的正在进行的研究项目上对其进行了测试。最重要的成果是开发了*提示模板链*，在这种方式下，模板可以递归嵌套，从而能够对提示模板本身进行比较（图
    3）。ChainForge 的早期使用案例包括：在最小改写的情况下缩短文本，检查编程 API 为哪些提示所导入，以及评估响应是否符合特定领域的语言。例如，我们发现我们使用的
    ChatGPT 提示在“仅删除单词”任务中的表现最差，相比其他提示更倾向于改写。
- en: 'We also ran five pilot studies. Pilot users requested two features: an easier
    way to score responses without code, and a way to carry chat context. These features
    became *LLM Scorer* and *Chat Turn* nodes. Finally, some potential users were
    wary of the need to install on their own machine. Thus, we rewrote the backend
    from Python into TypeScript (2000+ lines of code) and hosted ChainForge on the
    web, so that anyone can try the interface simply by visiting the site. Moreover,
    we added a “Share” button, so that users can share their experiments with others
    as links.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还进行了五个试点研究。试点用户提出了两个功能需求：一种无需代码即可评分响应的更简单的方法，以及一种携带聊天上下文的方法。这些功能成为了*LLM 评分器*和*聊天轮次*节点。最后，一些潜在用户对需要在自己的机器上安装感到担忧。因此，我们将后台从
    Python 重写为 TypeScript（2000 多行代码），并将 ChainForge 托管在网络上，以便任何人都可以通过访问网站来尝试接口。此外，我们添加了一个“分享”按钮，以便用户可以将他们的实验作为链接与他人分享。
- en: '![Refer to caption](img/d5c71df10d2a219a23fa71ee612cee95.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d5c71df10d2a219a23fa71ee612cee95.png)'
- en: Figure 5\. A more complex example, depicting a ground truth evaluation using
    Tabular Data (A), TextFields (B), and Simple Evaluator (C) nodes. User has plotted
    scores (D) by a prompt variable *command* to compare prompts, finding that Claude
    and Falcon.7B do slightly better on their second prompt. User can then go back
    to (B) or (A), iterating on prompts or input data, and re-run prompt and evaluator
    nodes; ChainForge only sends off queries it has not already collected.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图5\. 更复杂的示例，展示了使用Tabular Data（A）、TextFields（B）和Simple Evaluator（C）节点进行的地面真值评估。用户通过提示变量*command*绘制了评分（D）以比较提示，发现Claude和Falcon.7B在第二个提示中表现略好。用户可以返回到（B）或（A），对提示或输入数据进行迭代，并重新运行提示和评估节点；ChainForge仅发送尚未收集的查询。
- en: 'Since its launch in late May 2023, online users also provided feedback on our
    system by raising GitHub Issues. According to PyPI statistics, the local version
    of ChainForge has been installed around 5000 times, and the public GitHub has
    attained over 1300 stars. In August 2023, over 3000 unique users accessed the
    web app from countries across the world, averaging about 100 daily (top countries:
    U.S., South Korea, Germany, and India). Online comments include:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自2023年5月底上线以来，在线用户还通过提交GitHub Issues对我们的系统提供了反馈。根据PyPI统计，ChainForge的本地版本已安装约5000次，公开的GitHub获得了超过1300个星标。到2023年8月，来自世界各国的3000多名独立用户访问了该网络应用，每日平均约100人（主要国家：美国、韩国、德国和印度）。在线评论包括：
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Software developer at a Big-5 Tech Company, via GitHub Issue: *“I showed this
    to my colleagues, they were all amazed by the power and flexibility of the tool.
    Brilliant work!”*'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大型科技公司软件开发者通过GitHub Issue表示：*“我把这个展示给了我的同事，他们都对工具的强大和灵活性感到惊讶。干得好！”*
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Startup developer, on a prominent programmer news site: *“We just used this
    on a project and it was very helpful! Cool to see it here”*'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 初创公司开发者在知名程序员新闻网站上表示：*“我们刚刚在一个项目中使用了这个，它非常有帮助！看到它在这里真是太酷了”*
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Head of product design at a top ML company, on a social media site: *“Just
    played a bit with [ChainForge] to compare LLMs and the UX is satisfying”*'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顶级ML公司产品设计负责人在社交媒体上表示：*“刚刚玩了一下[ChainForge]，以比较LLMs，用户体验令人满意”*
- en: 'Beyond identifying bugs, online feedback resulted in: adding support for Microsoft’s
    Azure OpenAI service; a way to preview prompts before they are sent off; toggling
    fields on TextFields nodes ’on’ or ’off’; running on different hosts and ports;
    and implicit template variables.⁶⁶6The last is discussed in our docs; one uses
    a hashtag before a template variable, e.g. {#country}, to reference metadata associated
    with each input value. For instance, one might set up a table with an *Expected*
    column, then use {#Expected} in an LLM Scorer to compare with the expected value.
    Since its launch, the code of ChainForge has also been adapted by two other research
    teams: one team related to the last author, and one unrelated team at a U.S. research
    university whose authors are adapting our code for HCI research into prototyping
    with LLM image models (whom we interviewed in our evaluation).'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 除了识别错误，在线反馈还导致了：增加对微软Azure OpenAI服务的支持；提供发送前预览提示的功能；在TextFields节点上切换字段的“开”或“关”状态；在不同主机和端口上运行；以及隐式模板变量。⁶⁶6最后一个在我们的文档中讨论；一个使用模板变量前加上井号，例如{#country}，以引用与每个输入值相关的元数据。例如，可以设置一个带有*Expected*列的表格，然后在LLM评分器中使用{#Expected}与预期值进行比较。自上线以来，ChainForge的代码还被另外两个研究团队采纳：一个与最后一位作者相关，另一个则是美国一所研究型大学的团队，正在将我们的代码适配用于HCI研究中的LLM图像模型原型（我们在评估中采访了他们）。
- en: 4.3\. Implementation
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 实施
- en: ChainForge was programmed by the first author in React, TypeScript, and Python.
    It uses ReactFlow for the front-end UI and Mantine for UI elements. The local
    version uses Flask to serve the app and load API keys from environment variables.
    The app logic for prompt permutations and sending API requests is custom designed
    and uses asynchronous generator functions to improve performance; it is capable
    of sending off hundreds of requests simultaneously to multiple LLMs, streams progress
    back in real-time, rate limits the requests appropriately based on the model provider,
    and collects API request errors without disrupting other requests. The source
    code to ChainForge is released publicly under the MIT License.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ChainForge 是由第一作者使用 React、TypeScript 和 Python 编程的。它使用 ReactFlow 作为前端 UI，Mantine
    作为 UI 元素。本地版本使用 Flask 提供应用服务，并从环境变量中加载 API 密钥。应用逻辑涉及提示的排列组合和发送 API 请求，采用自定义设计，使用异步生成器函数提高性能；能够同时向多个
    LLM 发送数百个请求，实时流回进度，根据模型提供商适当限制请求速率，并在不干扰其他请求的情况下收集 API 请求错误。ChainForge 的源代码已公开发布，遵循
    MIT 许可证。
- en: '| Design Goal | Implementation Features |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 设计目标 | 实施特性 |'
- en: '| --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Model selection (D1) | Query multiple models at once in Prompt and Chat Turn
    nodes. Query same model multiple times at different settings. Compare LLM responses
    side-by-side in inspector. Vis Node groups-by-LLM by default on box-and-whisker
    and accuracy bar plots. Extend ChainForge with custom response providers via Python.
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 模型选择 (D1) | 在 Prompt 和 Chat Turn 节点中一次查询多个模型。对同一模型进行多次查询，使用不同的设置。在检查器中并排比较
    LLM 响应。默认情况下，Vis Node 在箱线图和准确度条形图中按 LLM 分组。通过 Python 扩展 ChainForge，以使用自定义响应提供者。
    |'
- en: '| Prompt template design (D2) | Template prompts with variables in Prompt,
    Chat Turn, and TextFields nodes. Recursively nest templates by chaining TextFields
    nodes. Plot columns by prompt variables in Response Inspector’s Table view. Plot
    by prompt variables on y-axis of Vis Node to slice data by variable. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 提示模板设计 (D2) | 在 Prompt、Chat Turn 和 TextFields 节点中使用变量的模板提示。通过连接 TextFields
    节点递归嵌套模板。在 Response Inspector 的表格视图中按提示变量绘制列。在 Vis Node 的 y 轴上按提示变量绘制，以按变量切片数据。
    |'
- en: '| Systematic evaluation (D3) | Easily increase number of generations per prompt
    to  test robustness
    (Jiang et al., [2022](#bib.bib15)). Set up no-code, code (Python and JavaScript),
    and LLM-based evaluation functions. Refer to variables and metavariables in evaluators.
    Set up groud truth evaluations via Tabular Data. Visualize scores in Vis Node.
    Let users navigate and plot data by different prompt variables. Highlight “false”
    (failed) scores in red in response inspectors, for easy skimming. Import libraries
    and custom scripts in Python. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 系统评估 (D3) | 轻松增加每个提示的生成数量以测试鲁棒性 (Jiang 等，[2022](#bib.bib15))。设置无代码、代码（Python
    和 JavaScript）以及基于 LLM 的评估函数。在评估器中引用变量和元变量。通过 Tabular Data 设置真实值评估。在 Vis Node 中可视化评分。允许用户按不同的提示变量导航和绘制数据。在响应检查器中用红色突出显示“错误”（失败）评分，以便快速浏览。在
    Python 中导入库和自定义脚本。 |'
- en: '| Improvisation (D4) | Downstream nodes react to edits and additions to upstream
    input data (e.g., adding field on TextFields, changing a row of Tabular Data,
    etc). Cache LLM responses and calculate only which prompts require responses,
    to reduce cost and time. Swap out models or change settings at will. Chain Prompt
    nodes together, or continue chats via Chat Turn nodes. Allow users to branch experiments
    non-linearly (flow UI). |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 即兴创作 (D4) | 下游节点对上游输入数据的编辑和添加做出反应（例如，在 TextFields 上添加字段，修改 Tabular Data 的一行等）。缓存
    LLM 响应，仅计算需要响应的提示，以降低成本和时间。随意更换模型或更改设置。将 Chain Prompt 节点连接在一起，或通过 Chat Turn 节点继续聊天。允许用户以非线性方式分支实验（流式
    UI）。 |'
- en: Table 1\. Some relationships between our Design Goals and Implementation Features
    of our toolkit (not comprehensive).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 我们工具包设计目标与实施特性之间的一些关系（并不全面）。
- en: 5\. Evaluation Rationale, Design, and Context
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 评估原理、设计和背景
- en: Toolkits are notoriously difficult to evaluate in HCI (Ledo et al., [2018](#bib.bib20);
    Greenberg and Buxton, [2008](#bib.bib12); Olsen Jr, [2007](#bib.bib29)). The predominant
    method of evaluation, the controlled usability study, is a poor match for toolkits,
    as usability studies tend to focus on a narrow subset of a toolkit’s capabilities
    (Ledo et al., [2018](#bib.bib20); Olsen Jr, [2007](#bib.bib29)), rarely aligning
    with “how [the system] would be adopted and used in everyday practice” (Greenberg
    and Buxton, [2008](#bib.bib12)). To standardize evaluation expectations for toolkit
    papers, Ledo et al. found that successful toolkit publications tended to adopt
    two of four methods, the most popular among them being demonstrations of usage
    (example scenarios) and user studies that try to capture the breadth of the tool
    (“which tasks or activities can a target user group perform and which ones still
    remain challenging?” (Ledo et al., [2018](#bib.bib20), p. 5)). These insights
    informed how we approached an evaluation of ChainForge.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 工具包在 HCI 中 notoriously 难以评估（Ledo 等，[2018](#bib.bib20)；Greenberg 和 Buxton，[2008](#bib.bib12)；Olsen
    Jr，[2007](#bib.bib29)）。主要的评估方法，即受控的可用性研究，对于工具包来说并不匹配，因为可用性研究往往集中在工具包能力的狭窄子集上（Ledo
    等，[2018](#bib.bib20)；Olsen Jr，[2007](#bib.bib29)），很少与“[系统] 如何在日常实践中被采纳和使用”对接（Greenberg
    和 Buxton，[2008](#bib.bib12)）。为了标准化工具包论文的评估期望，Ledo 等发现成功的工具包出版物通常采用四种方法中的两种，其中最受欢迎的是使用演示（示例场景）和用户研究，后者试图捕捉工具的广度（“目标用户组可以执行哪些任务或活动，还有哪些任务仍然具有挑战性？”（Ledo
    等，[2018](#bib.bib20)，第 5 页））。这些见解指导了我们对 ChainForge 的评估方法。
- en: 'Our goal for a study was investigate how ChainForge might help people investigate
    hypotheses about LLM behavior *that personally matters to them*, while acknowledging
    the limitations of prior knowledge, of who would find such a toolkit useful, and
    of the impossibility of learning all capabilities in a short time-frame (Ledo
    et al., [2018](#bib.bib20); Greenberg and Buxton, [2008](#bib.bib12)). ChainForge
    is designed for open-ended hypothesis testing on a broad range of tasks; therefore,
    it was important that our evaluation was similarly open-ended, capturing (as much
    as possible in limited time) some actual tasks that users wanted to perform. As
    such, we took a primarily qualitative approach, conducting both an in-lab usability
    study with new users, and a small interview study (8) with actual users—people
    who had found our system online and already applied it, or its source code, to
    real-world tasks. We hoped these studies would give us a rounded sense of our
    toolkit’s strengths and weaknesses, as well as identify potential mismatches between
    in-lab and real-world usage. Overall, we wanted to discover:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对研究的目标是调查 ChainForge 如何帮助人们调查对他们个人重要的 LLM 行为假设，同时承认先前知识的局限性、谁会发现这样的工具包有用以及在短时间内学习所有功能的不可行性（Ledo
    等，[2018](#bib.bib20)；Greenberg 和 Buxton，[2008](#bib.bib12)）。ChainForge 旨在对广泛的任务进行开放式假设测试；因此，我们的评估也必须是开放式的，尽可能多地捕捉用户希望执行的一些实际任务。为此，我们采取了主要的定性方法，既进行了与新用户的实验室可用性研究，又进行了与实际用户的小型访谈研究（8）——这些实际用户在网上发现了我们的系统，并已将其或其源代码应用于现实世界的任务。我们希望这些研究能给我们提供工具包优缺点的全面认识，以及识别实验室使用与现实世界使用之间的潜在不匹配。总体而言，我们想要发现：
- en: (1)
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （1）
- en: Are there any general patterns in how people use ChainForge?
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人们使用 ChainForge 有哪些普遍模式？
- en: (2)
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （2）
- en: What pain-points (usability and conceptual issues) do people encounter?
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人们遇到了哪些痛点（可用性和概念问题）？
- en: (3)
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （3）
- en: What kinds of tasks do people find ChainForge useful for already?
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人们已经发现 ChainForge 对哪些任务有用？
- en: (4)
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （4）
- en: Which kinds of tasks did people want to accomplish, but find difficult or outside
    the scope of current features?
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 人们希望完成哪些任务，但发现这些任务困难或超出了当前功能的范围？
- en: 'For the in-lab study, the majority of study time was taken up by free exploration.
    We separated it into two sections: a structured section that served as a tutorial
    and mock prompt engineering task; followed by an unstructured exploration of a
    participants’ idea, where the participant could ask the researcher for help and
    guidance. Before the study, we asked for informed consent. Participants filled
    in a pre-study survey, with demographic info, prior experience with AI text generation
    models, past programming knowledge (Likert scores 1-5; 5 highest), and whether
    they had ever worked on a project involving evaluating LLMs. Participants then
    watched a five-minute video introducing the interface.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实验室内的研究，大部分时间都用于自由探索。我们将其分为两个部分：一个结构化部分，作为教程和模拟提示工程任务；随后是对参与者想法的非结构化探索，参与者可以向研究人员寻求帮助和指导。在研究之前，我们要求获得知情同意。参与者填写了一份预研究调查，包括人口统计信息、对AI文本生成模型的先前经验、过去的编程知识（Likert评分1-5；5为最高），以及是否曾参与过涉及评估LLMs的项目。参与者随后观看了一段介绍界面的五分钟视频。
- en: 'In the structured task, participants navigated a mock prompt engineering scenario
    in two parts, where a developer first chooses a model, then iterates on a prompt
    to improve performance according to some criteria. We asked participants to choose
    a model and prompt to “professionalize an email” (translate a prospective email
    message to sound more professional).⁷⁷7Although we could have contrived a task
    with an objective ‘best’ answer—best model or prompt template—that wouldn’t reflect
    the kind of ambiguities present in many real world decisions around LLM usage.
    In part one, participants were given a preloaded flow, briefed on the scenario
    (*“Imagine you are a developer…”*), and presented with two criteria on a slip
    of paper: (1) *The response should just be the translated email*, and (2) *The
    email should sound very professional*. Participants were tasked with choosing
    the ‘best’ model given the criteria, and to justify their choice. All participants
    saw the exact same cached responses from GPT-4, Claude-2, and PaLM2, in the exact
    same order, for the prompt *“Convert the following email to have a more professional
    and polite tone”* with four example emails (e.g., *“Why didn’t you reply to my
    last email???”*). After they spent some time inspecting responses, we asked them
    to add one more example to translate and to increase *Num of responses per prompt*,
    to show them how the same LLMs can vary on the same prompt.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在结构化任务中，参与者在两个部分中导航一个模拟提示工程场景，其中开发者首先选择一个模型，然后根据一些标准迭代提示以提高性能。我们要求参与者选择一个模型和提示，以“专业化一封邮件”（将潜在的电子邮件消息翻译成更专业的语气）。⁷⁷虽然我们本可以设计一个具有客观“最佳”答案的任务——最佳模型或提示模板——但那不会反映许多现实世界中关于LLM使用的模糊性。在第一部分中，参与者获得了一个预加载的流程，简要介绍了场景（*“假设你是一个开发者……”*），并在纸条上呈现了两个标准：(1)
    *回应应该仅仅是翻译后的邮件*，以及(2) *邮件应该听起来非常专业*。参与者的任务是根据标准选择“最佳”模型，并解释他们的选择。所有参与者看到的GPT-4、Claude-2和PaLM2的缓存回应都是完全相同的，顺序也完全相同，针对提示*“将以下邮件转换成更专业和礼貌的语气”*，并提供了四个示例邮件（例如，*“你为什么没有回复我上封邮件？？？”*）。在他们花了一些时间检查回应后，我们要求他们添加一个更多的示例进行翻译，并增加*每个提示的响应数量*，以展示相同的LLMs在相同提示下的变化。
- en: 'Once participants chose a model, we asked them to remove all but their selected
    model. We then guided them to abstract the pre-given “command prompt” into a TextFields,
    and add at least two more command prompts of their own choosing. On a slip, we
    gave them a third criteria: *“the email should be concise.”* After participants
    inspected responses and started to decide on a ‘best’ prompt, we asked them to
    add one code Evaluator and Vis Node, plotting lengths of responses by their *command*
    variable. After spending some time with the plot, participants were asked to decide.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦参与者选择了一个模型，我们要求他们删除除所选模型外的所有模型。然后我们指导他们将预给定的“命令提示”抽象为一个TextFields，并添加至少两个自己选择的命令提示。在一个纸条上，我们给出了第三个标准：*“邮件应该简洁。”*
    在参与者检查回应并开始决定“最佳”提示后，我们要求他们添加一个代码评估器和可视化节点，通过*命令*变量绘制回应长度。参与者花了一些时间查看图表后被要求做出决定。
- en: The remaining study time was taken up by an unstructured, exploratory section
    meant to emulate how users—provided enough support and documentation—might use
    ChainForge to investigate a hypothesis about LLM behavior that mattered to them.
    We asked participants a day before their study to think up an idea, question,
    or hypothesis they had about AI text generation models, and gave a list of six
    possible investigation areas (e.g., checking models for bias, conducting adversarial
    attacks), but did not provide any concrete examples. During the study, participants
    then explored their idea through the interface with the help of the researcher.
    Importantly, researchers were instructed to only support participants in pursuit
    of their investigations, not to guide them towards particular domains of interest.
    The one exception is where a participant only queried a single model; in this
    case, the researcher could suggest that the user try querying multiple models
    at once. Participants used the exact same interface as the public version of our
    tool, and had access to OpenAI’s *gpt-3.5* and *gpt-4*, Anthropic’s *claude-2*,
    Google’s *chat-bison-001*, and HuggingFace models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的研究时间用于一个非结构化的探索性部分，旨在模拟用户在得到足够支持和文档的情况下，如何使用ChainForge来调查对他们重要的LLM行为假设。我们在研究前一天要求参与者想出一个关于AI文本生成模型的想法、问题或假设，并提供了六个可能的调查领域（例如，检查模型的偏见，进行对抗攻击），但没有提供具体示例。在研究期间，参与者通过界面探索他们的想法，并在研究人员的帮助下进行。重要的是，研究人员被指示仅支持参与者追求他们的调查，而不是引导他们进入特定的兴趣领域。唯一的例外是当参与者只查询了单一模型时；在这种情况下，研究人员可以建议用户尝试同时查询多个模型。参与者使用的界面与我们工具的公众版本完全相同，并且可以访问OpenAI的
    *gpt-3.5* 和 *gpt-4*，Anthropic的 *claude-2*，Google的 *chat-bison-001* 和 HuggingFace模型。
- en: After the tasks, we held a brief post-interview (5-10 min), asking participants
    to rate the interface (1-5) and explain their reasoning, what difficulties they
    encountered, suggestions for improvements, whether they felt their understanding
    of AI was affected or not, and whether they would use the interface again and
    why.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 任务结束后，我们进行了简短的后访谈（5-10分钟），询问参与者对界面的评分（1-5）及其理由，遇到的困难，改进建议，是否感到对AI的理解受到影响，以及是否会再次使用该界面及其原因。
- en: 5.1\. Recruitment, Participant Demographics, and Data Analysis
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 招募、参与者人口统计和数据分析
- en: We recruited in-lab participants around our U.S.-based university through listservs,
    Slack channels, and flyers. We tried to expand our reach beyond people experienced
    in CS and ML, specifically targeting participants in humanities and education.
    Participants were generally in their twenties to early thirties (nine 23-27; eight
    28-34; three 18-22; one 55-64), predominantly self-reported as male (14 men, 7
    women), and largely had backgrounds in computing, engineering, or natural sciences
    (ten from CS, data science, or tech; seven from bioengineering, physics, material
    science, or robotics; two from education; one from medicine and one from design).
    They had a moderate amount of past experience with AI text generation models (mean=3.3,
    stdev=1.0); one had none. Past Python programming experience varied (mean=3.1,
    stdev=1.3), with less experience in JavaScript (mean=2.0, stdev=1.3); two had
    no programming experience. Eight had “worked on an academic study, paper, or project
    that involved evaluating large language models.” All participants came in to the
    lab, with studies divided equally among the first three coauthors. Each study
    took 75 minutes, and participants were given $30 in compensation (USD). Due to
    ethical concerns surrounding the overuse of Amazon gift cards in human subject
    studies (Pater et al., [2021](#bib.bib32); Ng et al., [2022](#bib.bib28)), we
    paid all participants in cash.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过邮件列表、Slack频道和传单在我们位于美国的大学中招募了实验室参与者。我们尝试扩大招募范围，超越计算机科学和机器学习领域的经验丰富人士，特别是针对人文学科和教育领域的参与者。参与者一般在二十多岁到三十出头（九人23-27岁；八人28-34岁；三人18-22岁；一人55-64岁），主要自报为男性（14名男性，7名女性），且大多具有计算机、工程或自然科学背景（十人来自计算机科学、数据科学或技术；七人来自生物工程、物理学、材料科学或机器人学；两人来自教育；一人来自医学，一人来自设计）。他们对AI文本生成模型有一定的过往经验（均值=3.3，标准差=1.0）；其中一人没有经验。Python编程经验有所不同（均值=3.1，标准差=1.3），JavaScript经验较少（均值=2.0，标准差=1.3）；有两人没有编程经验。八人曾“参与过涉及评估大型语言模型的学术研究、论文或项目。”所有参与者都到实验室进行研究，研究任务在前三位共同作者之间均分。每项研究花费75分钟，参与者获得30美元的报酬。由于涉及到在人体研究中过度使用亚马逊礼品卡的伦理问题（Pater
    et al., [2021](#bib.bib32); Ng et al., [2022](#bib.bib28)），我们以现金支付所有参与者。
- en: For our interview study, we sought participants who had already used ChainForge
    for real-world tasks, reaching out via social media, GitHub, and academic networks.
    The first author held six semi-structured, 60 min. interviews with eight participants
    (in two interviews, two people had worked together). Interviews took place via
    videoconferencing. Interviewees were asked to share their screen and walk through
    something they had created with ChainForge. Unlike our in-lab study, we kept interviewees’
    screen recordings private unless they allowed us to take a screenshot, since real-world
    users are often working with sensitive information. Interviewees generously volunteered
    their time.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的访谈研究中，我们寻求那些已经在实际任务中使用过 ChainForge 的参与者，通过社交媒体、GitHub 和学术网络联系他们。第一作者与八名参与者进行了六次半结构化的、各
    60 分钟的访谈（其中两次访谈中，两个人曾一起合作过）。访谈通过视频会议进行。受访者被要求共享他们的屏幕并演示他们用 ChainForge 创建的内容。与我们的实验室研究不同，我们将受访者的屏幕录制保持私密，除非他们允许我们截图，因为现实世界的用户通常处理敏感信息。受访者慷慨地提供了他们的时间。
- en: We transcribed all 32 hours of screen recordings and interviews, adding notes
    to clarify participant actions and references (e.g., *“[Opens inspector; scrolls
    to top]. It seems like it went fast enough… [Reading from first email group] ‘Hi…”’*).
    We noted conceptual or usability problems and the content of participant references.
    We analyzed the transcripts through a combination of inductive thematic analysis
    through affinity diagramming, augmented with a spreadsheet to list participants’
    ideas, behaviors (nodes added, process of their exploration, whether they imported
    data, etc), and answers to post-interview questions. For our in-lab study, three
    coauthors separately affinity diagrammed three transcripts each, then met and
    joined the clusters through mutual discussion. The merged cluster was iteratively
    expanded with more participant data until clusters reached saturation. For interviews,
    the first author affinity diagrammed all transcripts to determine emergent themes.
    In what follows, in-lab participants are P1, P2, etc.; interviewees are Q1, Q2,
    etc.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对所有 32 小时的屏幕录制和访谈进行了文字记录，并添加了注释以澄清参与者的行为和引用（例如，*“[打开检查器；滚动到顶部]。似乎速度还不错……[阅读来自第一个邮件组]
    ‘嗨……’”*）。我们记录了概念性或可用性问题以及参与者的引用内容。我们通过归纳的主题分析结合亲和图，并使用电子表格列出参与者的想法、行为（添加的节点、探索过程、是否导入数据等）以及对访谈后问题的回答。对于我们的实验室研究，三位共同作者分别对三份转录文本进行亲和图分析，然后通过讨论会面并整合这些簇。合并后的簇通过不断添加更多参与者的数据进行迭代扩展，直到簇达到饱和。对于访谈，第一作者对所有转录文本进行了亲和图分析，以确定新兴主题。接下来，实验室参与者标记为
    P1、P2 等；受访者标记为 Q1、Q2 等。
- en: 6\. Modes of Prompt Engineering and LLM Hypothesis Testing
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 提示工程模式和 LLM 假设测试
- en: 'What process do people follow when prompt engineering and testing hypotheses
    about LLM behavior more generally? Before we break down findings per study, we
    provide a birds-eye view of how participants in general used ChainForge. Synthesizing
    across studies, we find that people tend to move from an *opportunistic exploration*
    mode, to a *limited evaluation* mode, to an *iterative refinement* mode. About
    half of our in-lab users, especially end-users with limited prior experience,
    never left exploration mode; while programmers or auditors of LLMs quickly moved
    into limited evaluation mode. Some interviewees had disconnected parts of their
    flows that corresponded to exploration mode, then would scroll down to reveal
    extensive evaluation pipeline(s), explaining they had transferred prompts from
    the exploratory part into their evaluation. In Section [7.2](#S7.SS2 "7.2\. Case
    Studies for Modes of Usage ‣ 7\. In-lab Study Findings ‣ ChainForge: A Visual
    Toolkit for Prompt Engineering and LLM Hypothesis Testing"), we provide one Case
    Study for each mode. Notice how these modes correspond to users moving from the
    left side of Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Related Work ‣ ChainForge: A Visual
    Toolkit for Prompt Engineering and LLM Hypothesis Testing") towards the right.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '人们在进行提示工程和测试大型语言模型（LLM）行为的假设时，通常遵循什么过程？在我们详细解析每项研究的发现之前，我们先提供一个关于参与者如何使用 ChainForge
    的概览。综合各项研究，我们发现人们通常会从*机会探索*模式，转到*有限评估*模式，再到*迭代优化*模式。我们发现，大约一半的实验室用户，特别是经验有限的终端用户，从未离开探索模式；而程序员或
    LLM 审核员则很快转入有限评估模式。一些受访者在他们的流程中有与探索模式相关的断裂部分，然后会向下滚动以显示广泛的评估管道，解释说他们已将提示从探索部分转移到了评估部分。在第
    [7.2](#S7.SS2 "7.2\. Case Studies for Modes of Usage ‣ 7\. In-lab Study Findings
    ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")
    节中，我们为每种模式提供了一个案例研究。注意这些模式如何对应到用户从图 [2](#S2.F2 "Figure 2 ‣ 2\. Related Work ‣
    ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")
    的左侧移动到右侧。'
- en: 'Opportunistic exploration mode is characterized by rapid iteration on prompts,
    input data, and hypotheses; a limited number of prompts and input data; and multi-model
    comparison. Users *prompt / inspect / revise*: send off a few prompts, inspect
    results, revise prompts, inputs, hypotheses, and ideas. In this mode, users are
    sending off quick experiments to probe and poke at model behavior (*“throw things
    on the wall to see what’s gonna stick”*, Q3). For instance, participants who conducted
    adversarial attacks like jailbreaking (Deng et al., [2023](#bib.bib8)) would opportunistically
    try different styles of jailbreak prompts, and were especially interested in checking
    which model(s) they could bypass.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 机会探索模式的特点是对提示、输入数据和假设进行快速迭代；使用有限数量的提示和输入数据；以及进行多模型比较。用户*提示/检查/修订*：发送几个提示，检查结果，修订提示、输入、假设和想法。在这种模式下，用户进行快速实验以探测和挑衅模型行为（*“把东西扔到墙上看看能粘上什么”*，Q3）。例如，进行对抗性攻击（如越狱）研究的参与者（Deng
    等， [2023](#bib.bib8)）会机会性地尝试不同风格的越狱提示，并特别关注哪些模型可以被绕过。
- en: 'Limited evaluation mode is characterized by moving from ad-hoc prompting to
    *prototyping an evaluation*. Users have reached the limits of manual inspection
    and now want a more efficient, “at-a-glance” test of LLM behavior, achieved by
    encoding criteria into automated evaluator(s) to score responses. Users *prompt
    / evaluate / visualize / revise*: prompt model(s), score responses downstream
    in their chain, visualize results, and revise their prompts, input data, models,
    and/or hypotheses accordingly. Hallmarks of this mode are users setting up an
    analysis pipeline, iterating on their evaluation itself, and “scaling up” input
    data. The evaluation is “limited” as evaluation criteria at this stage is often
    “coarse”—for example, rather than checking factuality, check if the output is
    formatted correctly at all.⁸⁸8Crucially, this mode does *not* imply multiple prompts:
    a few in-lab participants set up an evaluation pipeline that *only sent off a
    single prompt*. Though these participants did add a TextFields or Tabular Data
    node, it only had a single value/field. Indications were that, with more time,
    they would have “scaled up” how many inputs or parameters they were sending to
    test more specific hypotheses. However, some users might have also had conceptual
    trouble imagining how to scale up their testing; we discuss this more later.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 有限评估模式的特点是从临时提示转向*原型评估*。用户已经达到了手动检查的极限，现在希望更高效的、能够“一目了然”的测试LLM行为，通过将标准编码到自动评估器中来打分。用户*提示/评估/可视化/修订*：提示模型，评估链下游的响应，查看结果，并相应地修订提示、输入数据、模型和/或假设。这种模式的标志是用户建立分析流程，对评估本身进行迭代，并“扩大”输入数据。在这个阶段，评估是“有限的”，因为评估标准通常是“粗略的”——例如，与检查事实性相比，检查输出是否格式正确。关键是，这种模式*不*意味着多个提示：一些实验室参与者建立了一个*仅发送单个提示*的评估流程。尽管这些参与者确实添加了一个文本字段或表格数据节点，但它只有一个值/字段。有迹象表明，若有更多时间，他们会“扩大”发送更多输入或参数以测试更具体的假设。然而，一些用户可能在想象如何扩大测试时也遇到概念上的困难；我们稍后会讨论更多。
- en: 'Iterative refinement mode is characterized by having an already-established
    evaluation pipeline and criteria and *tweaking* prompt templates and input data
    through further parametrization or direct edits, setting up one-off evaluations
    to check effects of tweaks, increasing input data complexity, and removing or
    swapping out models. Users *tweak / test / refine*: modify or parametrize some
    aspect of their pipeline, test how tweaks affect outputs compared to their “control”,
    and refine the pipeline accordingly. The key difference between limited evaluation
    and iterative refinement is in the solidity of the chain: here, users’ prompts,
    input data, and evaluation criteria have largely stabilized, and they are looking
    to *optimize* (e.g., through tweaks to their prompt, or extending input data to
    identify failure modes). Some interview participants had reached this mode, and
    were refining prompt templates or scaling up input data. The few in-lab participants
    that had brought in “prompt engineering” problems by importing prompt templates
    or spreadsheets would immediately set up evaluation pipelines, moving towards
    this mode.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代改进模式的特点是已经建立了评估流程和标准，通过进一步的参数化或直接编辑来*调整*提示模板和输入数据，设置一次性评估来检查调整的效果，增加输入数据的复杂性，并移除或更换模型。用户*调整/测试/改进*：修改或参数化流程的某些方面，测试调整如何影响输出与“控制”相比，并相应地改进流程。有限评估和迭代改进之间的关键区别在于链的稳固性：在这里，用户的提示、输入数据和评估标准已经基本稳定，他们在寻求*优化*（例如，通过调整提示，或扩展输入数据以识别失败模式）。一些访谈参与者已经进入了这种模式，正在改进提示模板或扩大输入数据。少数实验室参与者通过导入提示模板或电子表格带来了“提示工程”问题，他们会立即建立评估流程，向这种模式发展。
- en: 'These modes are suggestive and not rigidly linear; e.g., users may scrap their
    limited evaluation and return to opportunistic exploration. In Sections 7 and 8
    below, we delve into specific findings for each study. For our in-lab study, we
    describe how people selected prompts and models, present Case Studies of each
    mode ([7.2](#S7.SS2 "7.2\. Case Studies for Modes of Usage ‣ 7\. In-lab Study
    Findings ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis
    Testing")), and note conceptual and usability issues. For our interview study,
    we focus on what differed from in-lab users.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '这些模式具有指示性，而非严格的线性；例如，用户可能会放弃有限的评估，回到机会性探索。在下面的第7节和第8节中，我们深入探讨每项研究的具体发现。对于我们的实验室研究，我们描述了人们如何选择提示和模型，展示了每种模式的案例研究（[7.2](#S7.SS2
    "7.2\. Case Studies for Modes of Usage ‣ 7\. In-lab Study Findings ‣ ChainForge:
    A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")），并指出了概念和可用性问题。对于我们的访谈研究，我们重点关注了与实验室用户的不同之处。'
- en: 7\. In-lab Study Findings
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 实验室研究发现
- en: On average, participants rated the interface a 4.19/5.0 (stdev=0.66). No participant
    rated it lower than a three. When asked for a reason for their score, participants
    generally cited minor usability issues (e.g., finicky when connecting nodes, color
    palette, font choice, more plotting options). Eighteen participants wanted to
    use the interface again; five before being explicitly asked. Some just wanted
    to play around, citing model comparison and multi-response generation. Participants
    who had prior experience testing LLM behavior in academia or industry cited speed
    and efficiency of iteration as the primary value of the tool (*“If I had started
    with using this, I’d have gotten much further with my prompt engineering… This
    is much faster than a Jupyter Notebook”*, P4; *“this would save me half a day
    for sure… You could do a lot of stuff with it”*, P21). Participants mentioned
    prior behavior as having multiple tabs open to chat with different models, manually
    copying responses into spreadsheets, or writing programs. Three wanted to use
    ChainForge for academic research.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 平均而言，参与者对界面的评分为4.19/5.0（标准差=0.66）。没有参与者给出低于三分的评分。当被询问评分原因时，参与者通常提到了一些小的可用性问题（例如，连接节点时不够顺畅、颜色调色板、字体选择、更多的绘图选项）。有十八名参与者希望再次使用该界面；其中五人是在明确询问之前就表达了这一意愿。有些人只是想尝试一下，提到模型比较和多响应生成。具有学术或工业界LLM行为测试经验的参与者将工具的主要价值归结为迭代的速度和效率（*“如果我一开始就使用这个工具，我的提示工程会进展得更快……这比Jupyter
    Notebook快多了”*，P4；*“这肯定能让我节省半天时间……你可以用它做很多事情”*，P21）。参与者提到以前的行为包括打开多个标签页与不同模型聊天、手动将响应复制到电子表格中或编写程序。有三人希望将ChainForge用于学术研究。
- en: We recount participants’ behavior in the structured task to choose a model and
    prompt template, overview how ChainForge supported participants’ explorations
    and understanding, and reflect on pain points.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了参与者在结构化任务中的行为，以选择模型和提示模板，概述了ChainForge如何支持参与者的探索和理解，并反映了痛点。
- en: 7.1\. How People Decide on Models and Prompts
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 人们如何决定模型和提示
- en: How do people choose a text generation model or prompt, when presented with
    side-by-side responses? People appear to weigh *trade-offs* in response quality
    for different criteria and contexts of usage. Participants would perceive one
    prompt or model to excel in one criteria or context, but do poorly in another;
    for another prompt or model, it was vice-versa. Here, we use “criteria” liberally
    to mean both our explicit criteria and also participants’ tacit preferences. Participants
    would also implicitly *rank* criteria, assigning more weight to some over others,
    and refer to friction between criterias (e.g., P2 *“prefer[red] professional over
    concise, because it [email] can be concise, but misconstrued”*). Moreover, seeing
    *multiple representations* of prompt performance, each of which better surfaced
    aspects of responses that corresponded to different criteria, could affect participants’
    theorizing and decision-making. We unpack these findings here.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对并排响应时，人们如何选择文本生成模型或提示？人们似乎会权衡不同标准和使用背景下的响应质量的*权衡*。参与者可能会认为某个提示或模型在一个标准或背景下表现出色，但在另一个标准或背景下表现不佳；而另一个提示或模型则相反。在这里，我们宽泛地使用“标准”一词，既包括我们明确的标准，也包括参与者的隐性偏好。参与者还会隐性地*排序*标准，对某些标准赋予更多权重，并提到标准之间的摩擦（例如，P2
    *“更喜欢专业而非简洁，因为[邮件]可以简洁，但容易被误解”*）。此外，看到提示性能的*多重表现*，每种表现更好地显现出响应的不同方面，可能会影响参与者的理论化和决策过程。我们在这里详细解读这些发现。
- en: 'For the first part of our structured task, participants reached no consensus
    on which model performed “better”: eight chose PaLM2, seven GPT-4, and six Claude-2\.
    There was no pattern in reasoning. Participants *did* notice similar features
    of each models’ response style, but *how* they valued that style differed. Some
    participants liked some models for the same reason others disliked them; for instance,
    P1 praised PaLM2 for its lengthy emails; while P17 chose GPT-4 because *“PaLM2
    is too lengthy.”* Although we had deliberately designed our first criteria against
    the outputs of Claude (for its explanatory information around the email), some
    participants still preferred Claude, perceived its explanations as useful to their
    imagined users, or preferring its writing style. In the unstructured task, participants
    developing apps also mentioned exogenous factors such as pricing, access, and
    response time when comparing models.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结构化任务的第一部分，参与者在“哪个模型表现更好”上没有达成共识：八人选择 PaLM2，七人选择 GPT-4，六人选择 Claude-2\. 理由中没有明显的模式。参与者*确实*注意到每个模型响应风格的相似特征，但*如何*评价这种风格则各不相同。一些参与者喜欢某些模型的原因正是其他人不喜欢它们的原因；例如，P1
    赞扬 PaLM2 的长篇邮件，而 P17 选择 GPT-4，因为*“PaLM2 太长了。”* 尽管我们故意将第一个标准设计为针对 Claude 的输出（以获取其邮件中的解释信息），但一些参与者仍然偏好
    Claude，认为其解释对他们设想的用户有用，或偏爱其写作风格。在无结构任务中，开发应用的参与者在比较模型时也提到了一些外部因素，如定价、访问权限和响应时间。
- en: '![Refer to caption](img/22b2b59d32f89a2208cff8348b0f8c12.png) \Description'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/22b2b59d32f89a2208cff8348b0f8c12.png) \描述'
- en: Figure 6\. P17 plots the response lengths of three command prompts, augmenting
    her theories about prompts’ performance.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. P17 绘制了三种命令提示的响应长度图，增强了她对提示性能的理论。
- en: 'How did people choose one prompt among multiple? Like when choosing models,
    participants appeared to weigh trade-offs between different criteria and contexts.
    Having multiple representations (e.g., plots of prompt performance) could especially
    give users a different “view” that augmented understanding and theorizing. P1
    describes tensions between his and his users’ needs, referencing both manual inspection
    and a plot of response lengths by prompt:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 人们如何在多个提示中选择一个？就像选择模型一样，参与者似乎在不同标准和背景之间权衡取舍。拥有多种表示（例如，提示性能图）可能特别能给用户不同的“视角”，从而增强理解和理论化。P1
    描述了他和他的用户需求之间的紧张关系，参考了手动检查和按提示绘制的响应长度图：
- en: '*“If I am a developer, I like this one [third prompt] because it will help
    me better to pass the output… But if they [users] have a chance to see this graph
    [Vis node], they would probably choose this one [second prompt] because it fits
    their needs and it’s more concise [box-and-whiskers plot has smallest median and
    lowest variability]… So I think it depends on the view.”*'
  id: totrans-123
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*“如果我是开发者，我喜欢这个 [第三个提示]，因为它能更好地帮助我传递输出……但如果他们 [用户] 有机会看到这张图 [Vis node]，他们可能会选择这个
    [第二个提示]，因为它符合他们的需求，而且更简洁 [箱线图的中位数最小且变异性最低]……所以我认为这要看视角。”*'
- en: 'Multiple representations could also augment users’ theorizing about prompting
    strategy. For instance, P17 had three command prompts, each iteration just tacking
    more formatting instructions onto the end of the default prompt (Figure [6](#S7.F6
    "Figure 6 ‣ 7.1\. How People Decide on Models and Prompts ‣ 7\. In-lab Study Findings
    ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")).
    Comparing between her plot and Table Layout, she theorizes: *“After adding ‘generate
    response in an email format’ it made it lengthier… But if I don’t say ‘with concise
    wording’… sometimes it generates responses that are three paragraphs, for a really
    simple request. So I would [go with] the second instruction… [and its] the length
    difference [variance] is less.”* Seeing that one prompt resulted in shorter or
    *less variable* responses could cause a participant to revise an earlier opinion.
    After noticing via the plot that his first command *“seem[s] more consistent”*,
    P4 wanted to mix features from it into his chosen prompt to improve the latter’s
    concision, as he still preferred the latter’s textual quality.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '多种表示方式还可以增强用户对提示策略的理论化。例如，P17 有三个命令提示，每次迭代只是将更多格式化指令附加到默认提示的末尾（图[6](#S7.F6
    "Figure 6 ‣ 7.1\. How People Decide on Models and Prompts ‣ 7\. In-lab Study Findings
    ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")）。比较她的图表和表格布局时，她推测：*“添加了‘以电子邮件格式生成回应’后，它变得更长了……但如果我不说‘用简洁的措辞’……有时它会生成三段的回应，尽管请求非常简单。所以我会[选择]第二个指令……[它的]长度差异[变化]较小。”*
    看到一个提示生成了较短或*变化较小*的回应，可能会导致参与者修正早期的观点。在通过图表注意到他的第一个命令*“似乎更一致”*之后，P4 希望将其特征混合到他选择的提示中，以提高后者的简洁性，因为他仍然更喜欢后者的文本质量。'
- en: These observations suggest that systematic evaluations can contest fixation
    (Zamfirescu-Pereira et al., [2023](#bib.bib46)) caused by manual inspection. However,
    it also reveals users may need *multiple* representations, or they will make decisions
    biased by features that are easiest to spot in only one. With multiple, they can
    make decisions more confidently, mixing and matching parts of each prompt to progress
    towards an imagined ideal. The benefit of prompt comparison also underscores the
    importance of starting from a *variety* of prompts—similar to past work (Zamfirescu-Pereira
    et al., [2023](#bib.bib46)), many of our participants struggled to come up with
    a variety of prompts, with thirteen just perturbing our initial command prompt.
    We reflect on this more in our Discussion.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察结果表明，系统评估可以挑战由人工检查造成的固定思维（Zamfirescu-Pereira et al., [2023](#bib.bib46)）。然而，这也揭示了用户可能需要*多种*表示方式，否则他们会基于仅在一种方式中最容易发现的特征做出偏见决策。通过多种表示，他们可以更自信地做出决策，将每个提示的部分混合搭配，朝着想象中的理想前进。提示比较的好处也强调了从*多样*提示开始的重要性——类似于以往的研究（Zamfirescu-Pereira
    et al., [2023](#bib.bib46)），我们的许多参与者在提出多样提示时遇到了困难，十三个只是扰动了我们初始的命令提示。我们在讨论中会进一步反思这一点。
- en: 7.2\. Case Studies for Modes of Usage
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 使用模式的案例研究
- en: 'Participants brought in a variety of ideas to the unstructured task, ranging
    from auditing of LLM behavior to refining an established prompt used in production.⁹⁹9A
    more detailed accounting here: Seven participants evaluated model behavior given
    concrete criteria; ideas ranged from testing model’s ability to understand program
    patch files, to classifying user attitudes in messaging logs. Nine audited models
    in opportunistic exploration mode, looking for biases or testing limits (e.g.,
    asking undecidable questions like *“Does God exist?”*, P20). Of these users, four
    conducted adversarial attacks (Deng et al., [2023](#bib.bib8)), seemingly influenced
    by popular culture about jailbreaking. P9 and P15, both with no programming experience,
    used the tool to audit behavior, the former comparing models’ ability to generate
    culturally-appropriate stories about Native Alaskans. Others were interested in
    generating text for creative writing tasks like travel itineraries. Participants
    often searched the internet, such as cross-checking factual data, copying prompts
    from Reddit, or evaluating code in an online interpreter. Nine participants imported
    data to use in their flow (with six importing spreadsheets), often sending us
    data during the study.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 参与者在这个非结构化任务中带来了各种各样的想法，从审计 LLM 行为到改进生产中使用的已建立提示。⁹⁹9 更详细的情况是：七位参与者根据具体标准评估模型行为；想法包括测试模型理解程序补丁文件的能力，到在消息记录中分类用户态度。九位参与者在机会探索模式下审计模型，寻找偏差或测试极限（例如，提出无法决定的问题，如
    *“上帝存在吗？”*，P20）。其中四位用户进行了对抗性攻击（Deng et al., [2023](#bib.bib8)），显然受到了关于越狱的流行文化影响。P9
    和 P15，两者均无编程经验，使用该工具来审计行为，前者比较了模型生成关于阿拉斯加土著人的文化适当故事的能力。其他人则对生成用于创意写作任务的文本如旅行行程感兴趣。参与者经常搜索互联网，例如交叉核对事实数据，从
    Reddit 复制提示，或在在线解释器中评估代码。九位参与者导入数据以用于他们的工作流程（其中六位导入了电子表格），经常在研究过程中向我们发送数据。
- en: 'To help readers understand how people used ChainForge and how their interactions
    varied, we walk through three participants’ experiences. Each Case Study corresponds
    to one mode from Section [6](#S6 "6\. Modes of Prompt Engineering and LLM Hypothesis
    Testing ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis
    Testing").'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '为帮助读者理解人们如何使用 ChainForge 以及他们的互动如何变化，我们将介绍三位参与者的经历。每个案例研究对应于第 [6](#S6 "6\.
    Modes of Prompt Engineering and LLM Hypothesis Testing ‣ ChainForge: A Visual
    Toolkit for Prompt Engineering and LLM Hypothesis Testing") 节中的一种模式。'
- en: '7.2.1\. Opportunistic exploration mode: Iterating on hypotheses through rapid
    discovery of model behavior.'
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1. 机会探索模式：通过快速发现模型行为来迭代假设。
- en: A graduate student from Indonesia, P15 wanted to test how well AI models knew
    the Indonesian education participation rate and could give advice on *“what the
    future of us as educators need to do.”* She opens a browser tab with official
    data from Badan Pusat Statistik (BDS), Indonesia’s Central Agency for Statistics.
    She wants to know *“what is the difference, if I use a different language?”* She
    adds a TextFields with two fields, one prompt in English, *“Tell me the participation
    rate of Indonesian students going to university”*; the second its Indonesian translation.
    *“Let’s just try two. I just want to see where it goes.”* Collecting responses,
    she looks over side-by-side responses of three models to her English prompt. All
    models provide different years and percentages. Scrolling down and expanding the
    response group for her Indonesian prompt, she finds that Falcon.7B only repeats
    her prompt and the PaLM2 model has triggered a safety filter.^(10)^(10)10This
    is a real problem with PaLM2 that we have communicated with the Google AI team
    about; they identified the issue and are fixing it. The last model, GPT-3.5, gives
    a different statistic than its English response.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一位来自印度尼西亚的研究生 P15 想测试 AI 模型对印度尼西亚教育参与率的了解程度，并能提供有关 *“作为教育者我们未来需要做什么”* 的建议。她打开了一个浏览器标签，查看来自印度尼西亚中央统计局
    (Badan Pusat Statistik, BDS) 的官方数据。她想知道 *“如果我使用不同的语言，会有什么区别？”* 她添加了一个包含两个字段的文本框，一个是英语提示，*“告诉我印度尼西亚学生上大学的参与率”*；第二个是其印尼语翻译。*“我们就试试这两个吧。我只是想看看会发生什么。”*
    收集到回答后，她查看了三个模型对她的英语提示的并排回答。所有模型提供了不同的年份和百分比。向下滚动并展开她的印尼语提示的回答组时，她发现 Falcon.7B
    仅重复了她的提示，而 PaLM2 模型触发了安全过滤器。^(10)^(10)10 这是 PaLM2 的一个实际问题，我们已经与 Google AI 团队沟通过，他们已识别出问题并正在修复。最后一个模型
    GPT-3.5 给出了与其英语回答不同的统计数据。
- en: 'Looking over these responses in less than a minute, P9 has discovered three
    aspects of AI model behavior: first, that models differ in their “facts”; second,
    that some models can refuse to answer when queried in a non-English language;
    third, that the *same* models can differ in facts when queried in a different
    language. She compares each number to the BDS statistics, finding them inaccurate.
    *“Oh my god, I’m curious. Why do they have like different answers across [models]?”*
    She then adds models to the Prompt Node. *“Can I try all [models]? I want to see
    if it’s in the table.”*'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在不到一分钟的时间里，P9发现了AI模型行为的三个方面：首先，模型在“事实”上存在差异；其次，有些模型在用非英语语言提问时会拒绝回答；第三，*相同*的模型在用不同语言提问时，事实也可能不同。她将每个数字与BDS统计数据进行比较，发现这些数据不准确。*“天哪，我很奇怪。他们为什么在[模型]之间有不同的回答？”*
    她接着将模型添加到Prompt Node中。*“我可以试试所有[模型]吗？我想看看是否在表格中。”*
- en: 'She queries the new models. A new hypothesis brews: *“In our prompt, [do] we
    need to say our source of data? Would that be like, more accurate?”* She wonders
    if different models are pulling data from different sources. Inspecting responses,
    she finds some models have cited sources of data: Claude cites UNESCO and GPT-4
    cites the World Bank, UNESCO, and the Indonesian Ministry of Education and Culture.
    For her Indonesian prompt, she discovers that the same models only cite BPS in
    their responses. *“BPS is only mentioned when I use Indonesian… For the English
    [prompt]… [it’s] more like, global… Wow, it’s very interesting how, the different
    language you use, there’s also a different source of data.”*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 她查询了新的模型。一个新的假设正在酝酿中：*“在我们的提示中，[是否]需要说明数据来源？这样会更准确吗？”* 她想知道不同的模型是否从不同的来源提取数据。检查回应时，她发现一些模型引用了数据来源：Claude引用了联合国教科文组织，而GPT-4引用了世界银行、联合国教科文组织和印尼教育文化部。对于她的印尼语提示，她发现相同的模型在回应中只引用了BPS。*“只有在我使用印尼语时，BPS才会被提及……对于英语[提示]……更像是，全球……哇，真的很有趣，不同的语言使用下，数据来源也不同。”*
- en: She adds a second prompt variable, *{organization}*, to her prompt template.
    She attaches values World Bank, UNESCO, and Badan Pusat Statistik to it.^(11)^(11)11Note
    that this part is in English now for both queries. Re-sending queries and inspecting
    responses, she expands the subgroups for BPS under both her Indonesian and English
    response groups, such that the two subgroups are on the same screen. When asking
    for BPS data in English, both GPT-3.5 and Claude refuse to answer, whereas the
    same models provide BPS numbers when asked in Indonesian. Moreover, Claude’s English
    response *suggests the reader look at World Bank and UNESCO data instead,* citing
    those sources. *“That’s really interesting. Wow.”*
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 她向提示模板中添加了第二个提示变量，*{organization}*。她将世界银行、联合国教科文组织和巴丹普斯统计局的值附加到这个变量上。^(11)^(11)11注意，这部分现在对于两个查询都是用英文的。重新发送查询并检查回应时，她扩展了印尼语和英语回应组下的BPS子组，使这两个子组在同一屏幕上。当用英语询问BPS数据时，GPT-3.5和Claude都拒绝回答，而相同的模型在用印尼语提问时提供了BPS数据。此外，Claude的英语回应*建议读者查看世界银行和联合国教科文组织的数据，*并引用了这些来源。*“这真的很有趣。哇。”*
- en: Although the study ended here, this case illustrates hypothesis iteration, limited
    prompts, and eagerness for cross-model comparisons, key aspects of opportunistic
    exploration mode. With more time, the user might have set up an evaluation to
    check how models cite “global” sources of information when queried in English,
    compared to Indonesian.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究到此结束，但这个案例展示了假设迭代、有限提示和对模型间比较的热切，都是机会探索模式的关键方面。如果有更多时间，用户可能会设置一个评估来检查模型在用英语提问时如何引用“全球”信息来源，相比之下，印尼语则不同。
- en: '7.2.2\. Limited evaluation mode: Setting up an evaluation pipeline to spot-check
    factual accuracy.'
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2\. 有限评估模式：设置评估管道以检查事实准确性。
- en: How do users transition from exploratory to limited evaluation mode? We illustrate
    prototyping an evaluation and “scaling up” with P18, a material design student
    who used ChainForge to check an LLM’s understanding conductivity values of additives
    to polymers. The example also depicts a usability issue as the user tried to scale
    up.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 用户如何从探索模式过渡到有限评估模式？我们通过P18的例子来说明评估原型和“扩展”的过程。P18是一名材料设计学生，她使用ChainForge检查LLM对添加剂与聚合物的导电值的理解。这个例子还展示了用户尝试扩展时遇到的可用性问题。
- en: 'Like Case #1, P18 begins in Opportunistic Exploration mode. They *prompt /
    inspect / refine*—send off queries, inspect responses, revise input data or prompts.
    They create a prompt template with two variables: *Base* and *additives* (Fig. [7](#S7.F7
    "Figure 7 ‣ 7.2.2\. Limited evaluation mode: Setting up an evaluation pipeline
    to spot-check factual accuracy. ‣ 7.2\. Case Studies for Modes of Usage ‣ 7\.
    In-lab Study Findings ‣ ChainForge: A Visual Toolkit for Prompt Engineering and
    LLM Hypothesis Testing")). Initially they start with only one Base, and four additives.
    Inspecting responses, P18 is impressed with GPT-4’s ability to suggest and explain
    *specific* additives under P18’s broad categories (e.g., EMIMBF4 for *Ionic Liquid*).
    They refine their questioning: *“I want to estimate the approximate conductivity
    value.”* They amend their prompt template, adding *“and estimate the conductivity
    value”*. Reviewing responses, they find the numeric ranges roughly correct.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与案例#1类似，P18以机会探索模式开始。他们*提示/检查/完善*——发送查询，检查响应，修订输入数据或提示。他们创建了一个包含两个变量的提示模板：*Base*和*additives*（见图[7](#S7.F7
    "图 7 ‣ 7.2.2. 有限评估模式：设置评估管道以检查事实准确性。 ‣ 7.2. 使用模式案例 ‣ 7. 实验室研究发现 ‣ ChainForge：一个用于提示工程和LLM假设测试的可视化工具包")）。最初他们只使用一个Base和四种additives。检查响应时，P18对GPT-4在P18的广泛类别下建议和解释*具体*additives（例如EMIMBF4用于*离子液体*）的能力印象深刻。他们完善了提问：*“我想估计大致的导电值。”*
    他们修改了提示模板，添加了*“并估计导电值”*。审查响应时，他们发现数值范围大致正确。
- en: 'They then wish to inspect the numbers in a more systematic fashion than manual
    inspection, and move into Limited Evaluation mode. The researcher helps P18 with
    how to extract the numbers, using an evaluator node, LLM Scorer, which they only
    saw once in the intro video. With this node, users can enter a natural language
    prompt to score responses. P18 iterates on the scorer prompt through a prompt/inspect/refine
    loop: first asking just for the number, then adding “without units” after they
    find it sometimes outputs units.^(12)^(12)12Here is where a framework like LMQL
    (Beurer-Kellner et al., [2023](#bib.bib5)) may come in handy to improve usability
    of this node. *“This is good. So we add some Vis Node.”* They plot by *additive*
    on the *y-axis* (Fig. [7](#S7.F7 "Figure 7 ‣ 7.2.2\. Limited evaluation mode:
    Setting up an evaluation pipeline to spot-check factual accuracy. ‣ 7.2\. Case
    Studies for Modes of Usage ‣ 7\. In-lab Study Findings ‣ ChainForge: A Visual
    Toolkit for Prompt Engineering and LLM Hypothesis Testing")). *“Very good. [Researcher:
    Is this true?] Roughly, yes. Roughly.”*'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 他们接着希望以比人工检查更系统的方式来检查这些数字，并进入有限评估模式。研究员帮助P18提取这些数字，使用一个评估节点LLM Scorer，这是他们在介绍视频中只见过一次。利用这个节点，用户可以输入自然语言提示来评分响应。P18通过提示/检查/完善循环迭代评分提示：首先只是询问数字，然后在发现有时输出单位后添加“无单位”。^(12)^(12)12在这里，像LMQL（Beurer-Kellner等，[2023](#bib.bib5)）这样的框架可能有助于提高该节点的可用性。*“这很好。所以我们添加一些Vis节点。”*
    他们在*y轴*上以*加法*方式进行绘图（见图[7](#S7.F7 "图 7 ‣ 7.2.2. 有限评估模式：设置评估管道以检查事实准确性。 ‣ 7.2. 使用模式案例
    ‣ 7. 实验室研究发现 ‣ ChainForge：一个用于提示工程和LLM假设测试的可视化工具包")）。*“非常好。[研究员：这是真的吗？] 大致上，是的。大致上。”*
- en: 'P18 then wants to “scale up” by adding a second polymer to their Base variable.
    They search Google for the abbreviation of a conducting polymer, Polyaniline (PANI).
    They paste it as a second field and re-query the prompt and scorer nodes. Skimming
    scores in Table Layout in two seconds: *“Oh, wow… It’s really good. Because PEDOT
    is most [conductive].”* Inspecting the Vis Node, they encounter a usability limitation:
    they want to *group by* *Base*, when *additive* is plotted in y-axis, but cannot.
    Plotting by *Base* on the y-axis, they see via box-and-whiskers plot that PANI
    is collectively lower than PEDOT. They ask the researcher to export the evaluation
    scores.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然后P18希望通过在其基础变量中添加第二种聚合物来“扩展规模”。他们在Google上搜索导电聚合物聚苯胺（PANI）的缩写。他们将其粘贴为第二个字段，并重新查询提示和评分节点。用两秒钟的时间浏览表格布局中的分数：*“哦，哇……真的很好。因为PEDOT是最[导电]的。”*
    检查Vis节点时，他们遇到了一种可用性限制：他们希望在*y轴*上按*Base*分组，但不能。通过在y轴上按*Base*绘图，他们通过箱线图看到PANI总体上低于PEDOT。他们要求研究员导出评估分数。
- en: '![Refer to caption](img/44d26df87e021ea76ff5bfcd0f6904e8.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/44d26df87e021ea76ff5bfcd0f6904e8.png)'
- en: Figure 7\. P18’s final flow, with one value toggled off to display their initial
    plot. The user asked GPT-4 to estimate how much the conductivity to a base polymer,
    PEDOT:PSS, may increase given one of four additives. They use an LLM Scorer to
    extract the mentioned conductivity value; after spot-checking it using Inspect,
    they plot in a Vis Node, finding it *“roughly”* correct.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图7\. P18的最终流程，其中一个值被切换关闭以显示其初始图。用户要求GPT-4估算给定四种添加剂之一后，基础聚合物PEDOT:PSS的导电性可能会增加多少。他们使用LLM评分器提取提到的导电性值；在使用Inspect进行抽查后，他们在可视化节点中绘图，发现结果*“大致”*正确。
- en: \Description
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: \描述
- en: This example illustrates limited evaluation mode, such as iterating on an evaluation
    pipeline (refining a scoring prompt), and beginning to “scale up” by extending
    the input data after the pipeline is set up. The user also encountered friction
    with usability when scaling up, wanting more options for visualization as input
    data complexity increased.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了有限评估模式，例如在评估管道上进行迭代（完善评分提示），以及在管道设置完成后通过扩展输入数据来“扩大规模”。用户在扩大规模时也遇到了可用性方面的摩擦，希望随着输入数据复杂性的增加，能够有更多的可视化选项。
- en: '7.2.3\. Iterative Refinement mode: Tweaking an established prompt and model
    to attempt an optimization.'
  id: totrans-144
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3\. 迭代优化模式：调整已有提示和模型以尝试优化。
- en: P8 works with a German startup, and brought in a prompt engineering problem,
    importing a dataset and prompt template from LangChain (Lan, [2023](#bib.bib2))
    (*“we’re building a custom LLM app for an e-commerce company, a virtual shop assistant”*).
    This template had already underwent substantial revisions; thus, the participant
    immediately moved into iterative refinement mode, allowing us to observe interactions
    we could only glimpse retroactively during our interview study.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: P8与一家德国初创公司合作，带来了一个提示工程问题，从LangChain（Lan, [2023](#bib.bib2)）导入了数据集和提示模板（*“我们正在为一家电子商务公司构建一个定制的LLM应用程序，一个虚拟商店助理”*）。这个模板已经经历了大量修订；因此，参与者立即进入了迭代优化模式，使我们能够观察到在我们的访谈研究中只能追溯性地窥见的互动。
- en: P8’s startup was using GPT-4 (because *“GPT-3.5 in German is really not that
    good”*), but was curious about whether other models could perform better. He knew
    of Claude and PaLM2, but had been put off by needing to code up custom API calls.
    He also had a hypothesis that using English in parts of his German prompt would
    yield better results. Upon entering the unstructured task, he imported a spreadsheet
    with a Tabular Data Node and pasted his three-variable prompt template in a Prompt
    Node, connecting them up. He then added a Python Evaluator Node to check whether
    the LLM stuck to a length constraint he had put in his template. Using Grouped
    List layout, he compared responses between Claude and GPT-4 across ten input values
    for variable *product_information*. *“GPT4 is going over [too long]… Claude seems
    to be fairly good at sticking—[opens another response group], actually, you know,
    we have an outlier here.”*
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: P8的初创公司使用GPT-4（因为*“GPT-3.5在德语中真的不好”*），但对其他模型是否表现更好感到好奇。他知道Claude和PaLM2，但因为需要编写自定义API调用而感到不便。他还假设在德语提示的部分使用英语会产生更好的结果。在输入无结构任务后，他导入了一个包含表格数据节点的电子表格，并将他的三变量提示模板粘贴到提示节点中，进行连接。然后，他添加了一个Python评估器节点，以检查LLM是否遵循了他在模板中设置的长度限制。使用分组列表布局，他比较了Claude和GPT-4在十个输入值下对变量*product_information*的回复。*“GPT4超出了[太长]…Claude似乎相当好地遵循—[打开另一个回复组]，实际上，你知道，我们这里有一个异常值。”*
- en: 'Looking over responses manually, he implies that he had been manually evaluating
    each response (prior to the study) across his ten criteria. *“I gave it… almost
    10 instructions… Formal language, length, and so on. And for each… I now need
    to review it.”* He notices that one of Claude’s responses includes the word *Begleiter*,
    a word he had explicitly instructed it to exclude: *“Because that was a pattern
    I noticed with GPT-4 that it kept using this word… So I’m going to try now… how
    is Claude behaving if I give this instruction in English, rather than [German]?”*'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 手动查看回复时，他暗示他在研究之前已经在十个标准下手动评估了每个回复。*“我给了它…几乎10条指令…正式语言、长度等等。对于每一个…我现在需要重新审视。”*
    他注意到Claude的一个回复中包含了他明确指示要排除的词*Begleiter*：*“因为我发现GPT-4有一个模式，就是不断使用这个词…所以我现在要尝试…如果我用英语给出这个指令，Claude会有什么表现，而不是[德语]？”*
- en: 'To test this, he *abstracts* the “avoid the following words” part of his prompt
    template into a new variable, *{avoid_words_ instruction}*. He pastes the previous
    command into a TextFields, and add a second one—the same command but in English.
    He adds a Simple Evaluator node, checking if the response contains Begleiter.
    In Grouped List layout, he groups responses by *avoid_words_instruction* and click
    “Only show scores” to only see true/false values ( false in red). Glancing: *“So
    it’s not very statistically significant. But… GPT-4 never made the mistake, and
    Claude made the mistake with both English and German… So it doesn’t matter which
    language… [Claude] will still violate the instructions.”* He attaches another
    Simple Evaluator to test another term, remarking that in practice he would write
    a Python script to test all cases at once, but the study is running out of time.
    *“So Claude again violates it in both cases… [But for] English, it only violates
    it once—again—and in German it violates it twice. So maybe it’s slowly becoming
    statistically significant.”* As the study ends, he declares that his investigation
    justified his original choice: *“I should probably keep using GPT-4.”*'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这一点，他*抽象*了提示模板中“避免使用以下词语”的部分，设为一个新变量，*{avoid_words_instruction}*。他将之前的命令粘贴到TextFields中，并添加了第二个——相同的命令但用英语。他添加了一个简单的评估器节点，检查响应是否包含Begleiter。在分组列表布局中，他按*avoid_words_instruction*对响应进行分组，并点击“仅显示分数”以仅查看真/假值（假为红色）。瞥一眼：*“所以这不是非常具有统计学意义。但是……GPT-4从未犯过这个错误，而Claude在英语和德语中都犯了这个错误……所以语言无所谓……[Claude]仍然会违反指令。”*
    他附加了另一个简单评估器来测试另一个术语，并评论说，实际上他会编写一个Python脚本来一次测试所有情况，但研究时间紧迫。*“所以Claude在这两种情况下再次违反了……[但对于]英语，它只违反了一次——再次——而在德语中它违反了两次。所以也许它慢慢地变得具有统计学意义。”*
    随着研究的结束，他宣称他的调查证实了他最初的选择：*“我应该继续使用GPT-4。”*
- en: Here we see aspects of iterative refinement mode—the participant has already
    optimized their prompt (pipeline) and is trying to tweak the prompt and model
    to see if they can improve the outputs even further, *according to specific criteria*.
    As we found in our structured task, in making decisions, users weigh trade-offs
    between how different models and/or prompts fulfill specific criteria, and also
    rank criteria importance. For P8, his “avoid-words” criteria seemed mission-critical,
    whereas word count—which he perceived Claude better at sticking to—was evidently
    less important.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到迭代优化模式的几个方面——参与者已经优化了他们的提示（管道），并尝试调整提示和模型，以查看是否可以进一步改善输出，*根据特定标准*。正如我们在结构化任务中发现的那样，在做决策时，用户权衡不同模型和/或提示如何满足特定标准的权衡，并且还对标准的重要性进行排序。对于P8，他的“避免使用的词”标准似乎至关重要，而字数——他认为Claude在这方面表现更好——显然不那么重要。
- en: 7.3\. ChainForge Affected Participants’ Understanding of AI Behavior or Practice
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. ChainForge 影响参与者对AI行为或实践的理解
- en: 'In the post-interview, fifteen participants said their understanding of AI
    was affected by their experience. Six were surprised by the performance of Claude-2
    or PaLM2, feeling that, when confronted with direct comparisons to OpenAI models,
    they matched or exceeded the latter’s performance. Five said that their *strategy*
    of prompting or prompt engineering had changed (*“[Before], I wasn’t doing these
    things efficiently… I [would] make minor modifications and rerun, and that would
    take hours… Here, since everything is laid out for me, I don’t want to give up”*,
    P4). Others less experienced with AI models learned about general behavior. P16,
    who had never prompted an AI model before, *“realized that different models have
    completely different ways of understanding my prompts and hence responding, they
    also have a completely different style of response.”* P15, covered in Case Study [7.2.1](#S7.SS2.SSS1
    "7.2.1\. Opportunistic exploration mode: Iterating on hypotheses through rapid
    discovery of model behavior. ‣ 7.2\. Case Studies for Modes of Usage ‣ 7\. In-lab
    Study Findings ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis
    Testing"), said she had lost *“trust”* in AI.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '在后续访谈中，十五位参与者表示他们对 AI 的理解受到了他们的体验影响。六人对 Claude-2 或 PaLM2 的表现感到惊讶，觉得在与 OpenAI
    模型直接比较时，它们的表现相匹配或超越了后者。五人表示他们的*策略*发生了变化（*“[之前]，我没有高效地做这些事情……我[会]做小的修改并重新运行，这会花费几个小时……在这里，由于一切都已为我布局，我不想放弃”*，P4）。其他对
    AI 模型不太熟悉的人了解了常规行为。P16 从未提示过 AI 模型，她*“意识到不同的模型对我的提示有完全不同的理解方式，从而产生完全不同的回应风格。”*
    P15 在案例研究 [7.2.1](#S7.SS2.SSS1 "7.2.1\. Opportunistic exploration mode: Iterating
    on hypotheses through rapid discovery of model behavior. ‣ 7.2\. Case Studies
    for Modes of Usage ‣ 7\. In-lab Study Findings ‣ ChainForge: A Visual Toolkit
    for Prompt Engineering and LLM Hypothesis Testing")中提到她已经失去了对 AI 的*“信任”*。'
- en: 7.4\. Challenges and Pain-points
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 挑战和痛点
- en: 'Though many participants derived value from ChainForge, that is not to say
    their experience was frictionless. The majority of usability issues revolved around
    the flow UI, such as needing to move nodes around to make space, connecting nodes
    and deleting edges; others related to inconsistencies in the ordering of plotted
    variables, and wanting more control over colors and visualizations. Some participants
    also encountered conceptual issues, which sometimes indicate users getting used
    to the interface. The most common conceptual issue was learning how prompt templating
    worked, and especially, forgetting to declare input variables in Prompt Nodes.
    Once users learned how to template, however, the issue often disappeared (*“prompt
    variables… there’s a bit of a learning curve, but I think it makes sense, the
    design choice”*, P13). Learning template variables seemed related to past programming
    expertise and not AI, suggesting users without any prior programming experience
    will need extra resources.^(13)^(13)13For instance, P16, who had never prompted
    an AI model before, attributed her adeptness with templating to prior programming
    experience. By contrast, P9 had no programming experience and struggled; after
    the intro video he was *“a little overwhelmed”*, asking: *“what language am I
    in?”*'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多参与者从 ChainForge 中获得了价值，但这并不意味着他们的体验毫无摩擦。大多数可用性问题围绕着流动 UI，比如需要移动节点以腾出空间、连接节点和删除边缘；其他问题涉及到绘制变量顺序的不一致，以及希望对颜色和可视化有更多控制。一些参与者还遇到了概念性问题，这有时表明用户正在适应界面。最常见的概念性问题是学习提示模板的工作原理，特别是忘记在提示节点中声明输入变量。然而，一旦用户学会了如何使用模板，这个问题通常会消失（*“提示变量…有一点学习曲线，但我认为设计选择是有意义的”*，P13）。学习模板变量似乎与过去的编程经验有关，而不是与
    AI 相关，这表明没有任何编程经验的用户将需要额外的资源。^(13)^(13)13例如，P16 从未提示过 AI 模型，她将自己在模板方面的熟练度归因于之前的编程经验。相比之下，P9
    没有编程经验，感到困难；在介绍视频之后，他*“有点不知所措”*，问道：*“我在使用什么语言？”*
- en: Import to reflect on is that, in the lab, researchers were on-hand to guide
    users. Although users were the ones suggesting ideas—often highly domain-specific
    ones—researchers could help users with ways to implement them and overcome conceptual
    hurdles. Some end-users *and even a few users with substantial prior experience
    with AI models or programming with LLM APIs* appeared to have trouble “scaling
    up,” or systematizing, their evaluations. For example, P10 rated themselves as
    an expert in Python (5) and had conducted prior research on LLM image models.
    They set up an impressive evaluation, complete with a prompt template, Prompt
    Node, Chat Turn, Simple Evaluator and Vis nodes, but ultimately only sent off
    a single prompt to multiple models. We remark more on this behavior in Discussion.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 需要反思的是，在实验室中，研究人员随时指导用户。尽管用户是提出想法的人——通常是高度特定领域的想法——研究人员可以帮助用户实现这些想法并克服概念障碍。一些最终用户*甚至还有一些具有
    AI 模型或 LLM API 编程的丰富经验的用户*似乎在“扩展”或系统化他们的评估时遇到了困难。例如，P10 将自己评为 Python 专家（5），并且进行了关于
    LLM 图像模型的先前研究。他们设置了一个令人印象深刻的评估，配备了提示模板、Prompt 节点、Chat Turn、Simple Evaluator 和
    Vis 节点，但最终仅向多个模型发送了一个提示。我们将在讨论中进一步说明这种行为。
- en: 8\. Interviews with Real-World Users
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8. 现实世界用户访谈
- en: Our interview findings complement, but in important places diverge, from our
    in-lab studies. Like many in-lab participants, real-world users praised ChainForge’s
    features and used it for goals we had designed for—like selecting models or prompt
    testing—however, some things real users cared about were hardly, if ever mentioned
    by in-lab participants. As we analyzed the data and compared it with our in-lab
    study, we realized that many user needs and pain-points revolve around the fact
    that they were using ChainForge to *prototype data processing pipelines*, a wider
    context that re-frames the tasks we had designed ChainForge to support as subtasks
    of a larger goal. Interviewees remarked most about *easing the export and sharing
    of data* from ChainForge, adding *processor nodes*, the *importance of the Inspect
    Node* for sharing and rapid iteration, and the *open-source nature* of the project
    for their ability to adapt the code to their use case. We discuss these insights
    more below; but first, we provide an overall picture, reviewing similarities,
    use cases, and concrete value that real-world users derived from ChainForge.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的访谈发现与我们的实验室研究互补，但在重要地方有所不同。与许多实验室参与者一样，现实世界的用户称赞了 ChainForge 的功能，并将其用于我们设计的目标——例如选择模型或提示测试——然而，现实用户关心的一些问题在实验室参与者中几乎没有提到。当我们分析数据并将其与实验室研究进行比较时，我们意识到许多用户需求和痛点围绕着他们使用
    ChainForge 来*原型数据处理管道*的事实，这一更广泛的背景重新定义了我们设计 ChainForge 支持的任务为更大目标的子任务。受访者最多提到的是*简化数据的导出和共享*，增加*处理器节点*，*Inspect
    节点*对共享和快速迭代的重要性，以及项目的*开源性质*对他们调整代码以适应其使用案例的能力。我们在下面进一步讨论这些见解；但首先，我们提供一个总体概况，回顾现实世界用户从
    ChainForge 中获得的相似性、使用案例和具体价值。
- en: '| ID(s) | Location | Work | How discovered | Use Case(s) | Major nodes and
    features used | Outcome |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| ID(s) | 位置 | 工作 | 如何发现 | 使用案例 | 主要使用的节点和功能 | 结果 |'
- en: '| Q1-2 | U.S. (west) | Acad. | Medium post | Adapting code for LLM image model
    prototyping | *Adapted source code (esp. nodes, model querying, cacheing)* | Submitting
    HCI paper to major conference in time for deadline |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| Q1-2 | 美国（西部） | 学术 | Medium 文章 | 为 LLM 图像模型原型调整代码 | *调整后的源代码（特别是节点、模型查询、缓存）*
    | 在截止日期前向主要会议提交 HCI 论文 |'
- en: '| Q3 | U.K. | Ind. | GitHub | Supporting requirements analysis in software
    testing | Prompt, Tabular Data, TextFields, Vis, JS Eval; export to excel, metavariables
    | *“I’ve had good conversations with [other] developers as a result”* |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Q3 | 英国 | 工业 | GitHub | 支持软件测试中的需求分析 | Prompt, 表格数据, TextFields, Vis, JS
    Eval；导出到 excel, metavariables | *“因此，我与[其他]开发者进行了良好的对话”* |'
- en: '| Q4 | U.S. (east) | Acad. | Hacker-News | Auditing models for gender bias
    | Prompt, TextFields, Chat Turn, Inspect node; prompt chaining, LLM scoring |
    Improved prompt templates for a pipeline they are building in Python |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Q4 | 美国（东部） | 学术 | Hacker-News | 对模型进行性别偏见审计 | Prompt, TextFields, Chat Turn,
    Inspect 节点；prompt 链接，LLM 评分 | 改进了他们在 Python 中构建的管道的提示模板 |'
- en: '| Q5-6 | Germany | Ind. | LearnPro-mpting.org | Information synthesis and extraction
    from documents | Prompt, Tabular Data, JS Eval, Vis, Comment, Inspect node; comparing
    models | *“Convinced client [to] continue with the next phase of [a] project”*
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Q5-6 | 德国 | 工业 | LearnPro-mpting.org | 从文档中合成和提取信息 | Prompt, Tabular Data,
    JS Eval, Vis, Comment, Inspect node; 比较模型 | *“说服客户继续项目的下一阶段”* |'
- en: '| Q7 | U.S., Austria | Acad. | Word of mouth | Cleaning a column of tabular
    data | Prompt, Tabular Data, TextFields, JS Eval, Vis; variables in code eval
    | Decided that LLMs were not reliable enough for their task |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| Q7 | 美国，奥地利 | 学术 | 口碑 | 清理表格数据的一列 | Prompt, Tabular Data, TextFields, JS
    Eval, Vis; code eval 中的变量 | 决定 LLM 对他们的任务不够可靠 |'
- en: '| Q8 | U.S. (central) | Acad. | Twitter | Extracting and formatting info from
    podcast episode metadata | Prompt, TextFields, CSV, JS Eval, Vis, Comment; comparing
    models, prompt chaining, template chaining | Found that neither OpenAI model produced
    good enough outputs; now looking into local models |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Q8 | 美国（中部） | 学术 | Twitter | 从播客剧集元数据中提取和格式化信息 | Prompt, TextFields, CSV,
    JS Eval, Vis, Comment; 比较模型，prompt 链接，模板链 | 发现 OpenAI 的模型都无法生成足够好的输出；现在在研究本地模型
    |'
- en: Table 2\. Our interviewees. All interviewees imported data into ChainForge,
    with the exception of Q1-2 (who just adapted source code). Interviewees also commonly
    had multiple evaluation branches in their flows, as well as multiple flows in
    the same document.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 我们的受访者。所有受访者都将数据导入了 ChainForge，除了 Q1-2（他们只是调整了源代码）。受访者通常在他们的流程中有多个评估分支，以及同一文档中的多个流程。
- en: 'We list interview participants in Table [2](#S8.T2 "Table 2 ‣ 8\. Interviews
    with Real-World Users ‣ ChainForge: A Visual Toolkit for Prompt Engineering and
    LLM Hypothesis Testing"), with use cases and nodes used. The Outcome column suggests
    the actionable value that ChainForge provided. Note that Q1 and Q2’s primary use
    case was building on the source code to enable their HCI research project. All
    six users of the *interface* found it especially useful for prototyping and iterating
    on prompts and pipelines (e.g., Q5: *“I see the use case for ChainForge as a very
    good prompt prototyping environment”*). Usage reflected modes of *limited evaluation*
    and *iterative refinement*, with multiple participants describing a prompt/evaluate/visualize/revise
    loop: query LLM(s), evaluate responses and view the plot, then refine prompts
    or change models, until one reaches the desired results. For instance, Q3 described
    tweaking a prompt template until the LLM output in a consistent format, facilitated
    by maximizing 100% bars in a Vis Node across all input data. Some participants
    saw ChainForge as a rapid prototyping tool missing from the wider LLMOps ecosystem,
    a tool they used *“until I get to the point where I can actually write it into
    hard code”* (Q4). Three appreciated how *few* nodes there were in ChainForge given
    its relative power, compared to other node-based interfaces (e.g., Q8: *“It’s
    impressive. What you’re able to accomplish with so few”*). They worried that adding
    too many new nodes would make the interface more daunting for new users. Q4 and
    Q7 found it more effective than Jupyter notebooks (Q7: *“I enjoyed ChainForge…
    because I could run the whole workflow over and over again, and… in Jupyter, that
    was not easy”*). In the rest of this section, we expand upon differences from
    our in-lab study.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表 [2](#S8.T2 "Table 2 ‣ 8\. Interviews with Real-World Users ‣ ChainForge:
    A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing") 中列出了受访者及其用例和使用的节点。结果列建议了
    ChainForge 提供的可操作价值。请注意 Q1 和 Q2 的主要用例是基于源代码来支持他们的 HCI 研究项目。所有六位使用*界面*的用户都发现它对原型设计和迭代
    prompts 和管道特别有用（例如，Q5: *“我认为 ChainForge 是一个非常好的 prompt 原型环境”*）。使用反映了*有限评估*和*迭代改进*的模式，多位参与者描述了一个
    prompt/评估/可视化/修订循环：查询 LLM，评估响应并查看图表，然后细化 prompts 或更换模型，直到达到期望结果。例如，Q3 描述了调整 prompt
    模板，直到 LLM 输出以一致的格式呈现，通过在所有输入数据的 Vis Node 中最大化 100% 的条形图来实现。一些参与者认为 ChainForge
    是 LLMOps 生态系统中缺少的快速原型工具，是他们使用*“直到我能实际编写成硬代码”*的工具（Q4）。三位参与者欣赏 ChainForge 在相对强大的情况下节点的*少量*（例如，Q8:
    *“这很令人印象深刻。你能用这么少的节点完成这么多”*）。他们担心增加过多的新节点会使界面对新用户更具挑战性。Q4 和 Q7 发现它比 Jupyter notebooks
    更有效（Q7: *“我喜欢 ChainForge……因为我可以反复运行整个工作流程，而……在 Jupyter 中，这并不容易”*）。在本节的其余部分，我们扩展了与我们实验室内研究的差异。'
- en: 8.1\. Prototyping data processing pipelines
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 原型设计数据处理管道
- en: Five interviewees were using ChainForge not (only) for prompt engineering or
    model selection, but for *on-demand prototyping of data processing pipelines involving
    LLMs*. All imported data from spreadsheets, then would send off many parametrized
    prompts, iterate on their prompt templates and pipelines, and ultimately export
    data to share with others. Such users also used ChainForge for at least one of
    its intended design goals, but always in service of their larger data processing
    goal. For Q7, *“the idea was to write a pipeline that… helps you with this whole
    process of data cleaning.”* For him, ChainForge was ideal for *“whenever you have
    a variety of prompts you want to use on something particular, like a data set.
    And you want to explore or investigate something.”* Another user, Q3, would open
    his refined flow, edit one value, re-run it and then export the responses to a
    spreadsheet. Like other participants, he remarked on ChainForge’s combinatorial
    power as its chief benefit, compared to other tools (*“This tool is strong at
    prompt refining. With [Flowise]…Let’s say I wanted to try multiple [input fields].
    I don’t think I could do that”*). Participants also mentioned iterating on the
    *input data* as part of the prototyping process. Finally, related to data processing,
    three users wished for processor nodes, like “join” nodes to concatenate LLM responses,
    and in one case were manually copying LLM outputs into a separate flow to emulate
    concatenation. Note that many needs and pain-points below are related to data
    processing.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 五位受访者使用 ChainForge 不仅仅是为了提示工程或模型选择，还为了*按需原型设计涉及 LLM 的数据处理管道*。所有人都从电子表格导入数据，然后发送许多参数化的提示，迭代他们的提示模板和管道，并最终导出数据以便与他人分享。这些用户也使用
    ChainForge 实现了其至少一个设计目标，但总是为了更大的数据处理目标服务。对于 Q7，*“这个想法是写一个管道…帮助你完成整个数据清理过程。”* 对他来说，ChainForge
    是*“每当你有各种提示要在某个特定数据集上使用时，或者你想探索或调查某些内容时”的理想选择。* 另一位用户 Q3 会打开他的优化流程，编辑一个值，重新运行，然后将响应导出到电子表格。与其他参与者一样，他指出
    ChainForge 的组合能力是其主要优点，相比其他工具（*“这个工具在提示优化方面很强。使用 [Flowise]…假设我想尝试多个 [输入字段]。 我不认为我能做到这一点。”*）。参与者还提到在原型设计过程中迭代*输入数据*。最后，关于数据处理，三位用户希望有处理器节点，例如“连接”节点来连接
    LLM 响应，其中一位用户手动将 LLM 输出复制到一个单独的流程中以模拟连接。请注意，下面提到的许多需求和痛点与数据处理相关。
- en: 8.2\. Getting data out and sharing with others
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2. 获取数据并与他人共享
- en: 'Many participants wanted to export data out of ChainForge. This was also the
    most common pain point, especially when transitioning from a prototyping stage—which
    they perceived as ChainForge’s strong point—to a production stage (e.g., *“it
    would be helpful when we are out of this prototyping stage, that the burden or
    the gap—changing the environment… gets tightened”*, Q5). Needs broke down into
    two categories: exporting for integration into another application, and exporting
    for sharing results with others. For the former, developer users would use ChainForge
    to battle-test prompts, model behavior, and/or prompt chains, but then wished
    for an easier way to export their flows to text files or app building environments.^(14)^(14)14This
    does not, however, mean they perceived ChainForge as an “app building” tool—some
    even expressed worry about it trying to do too much. When asked about how he would
    feel if ChainForge supported app development, Q3 remarked, *“I just worry about
    it [ChainForge] becoming too complicated. Like, are you building the app side
    of it?””* He said even if ChainForge supported app-building, it would need different
    “modes”: *“app building mode or prompt refining mode.”* For the latter, five interviewees
    shared results with others, whether through files, screenshots of their flows,
    exported Excel spreadsheets of responses, or copied responses. Q5 and Q6 stressed
    the importance of the Inspect Node—a node that no in-lab participant used or mentioned
    (*“[Once] the result is worth documenting, you create an Inspect node.”*). They
    took screenshots of flows and sent them to clients, in one case convincing a client
    to move forward with a project. The anticipation of sharing with others also could
    change behavior. Q3 had several TextFields nodes with only a single value, *“because
    I knew that it was something that essentially other teams might want to change.”*.
    Sharing could also be a pain-point, with two wanting easier shareable “reports”
    of their analysis results.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 许多参与者希望将数据导出到 ChainForge 之外。这也是最常见的痛点，特别是在从他们认为是 ChainForge 强项的原型阶段过渡到生产阶段时（例如，*“当我们脱离原型阶段时，能够解决环境变更的负担或差距...会很有帮助”*，Q5）。需求分为两类：一类是为了与其他应用程序集成而导出，另一类是为了与他人共享结果。对于前者，开发者用户会使用
    ChainForge 进行提示、模型行为和/或提示链的测试，但希望能有更简单的方式将他们的流程导出为文本文件或应用程序构建环境。^(14)^(14)14这并不意味着他们将
    ChainForge 视为一个“应用程序构建”工具——有些人甚至担心它会做得过多。当被问及如果 ChainForge 支持应用开发时会有何感受时，Q3 说，*“我只是担心它
    [ChainForge] 会变得太复杂。你们是在构建应用程序的一部分吗？”* 他表示即使 ChainForge 支持应用构建，也需要不同的“模式”：*“应用构建模式或提示优化模式。”*
    对于后者，五位受访者与他人共享了结果，无论是通过文件、流程的截图、导出的 Excel 响应表，还是复制的响应。Q5 和 Q6 强调了 Inspect Node
    的重要性——一个实验室内没有参与者使用或提到的节点（*“[一旦] 结果值得记录，你就创建一个 Inspect 节点。”*）。他们截取了流程的截图并发送给客户，在一个案例中说服客户继续推进项目。与他人分享的期待也可能改变行为。Q3
    有几个 TextFields 节点只有一个值，*“因为我知道这可能是其他团队想要更改的东西。”* 分享也可能是一个痛点，有两个人希望能够更轻松地分享他们分析结果的“报告”。
- en: '8.3\. Pain points: Hidden affordances and friction during opportunistic exploration
    mode'
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 痛点：在机会探索模式中的隐性使用特性和摩擦
- en: 'Like in-lab participants, interviewees also encountered usability and conceptual
    issues. A common theme was individual users expressing a need for features that
    already exist but are relatively hidden, surfaced only through examples or documentation.
    These hidden affordances included *implicit template variables*, *metavariables*,
    and *template chaining*. The former two features address users’ need to reference
    upstream metadata—metadata associated with input data or responses—further downstream
    in a chain.^(15)^(15)15For example, in ChainForge one can define a column of a
    table and then refer to it downstream via a metavariable (Fig. [5](#S4.F5 "Figure
    5 ‣ 4.2\. Iterative Development with Online and Pilot Users ‣ 4\. ChainForge ‣
    ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing")).
    However, Q5 and Q6 seemed unaware of this feature and had implemented a workaround.
    For implicit template variables, Q4 needed to reference an upstream value {gender}
    later downstream in a prompt chain, and was unaware that an implicit template
    variable could accomplish this (e.g. {#gender}). Another pain point was friction
    during the opportunistic exploration phase. In Section 6, we mentioned some interviewees
    had disconnected regions of their flows, with one region we termed opportunistic
    exploration mode (rapid, early-stage iteration through input data, prompts, models,
    and hypotheses; usually, a chain of three nodes, TextField-Prompt-Inspect). In
    this mode, some interviewees preferred to inspect responses directly on the flow
    with an Inspect Node (instead of the pop-up window), as it facilitated rapid iteration.
    They wanted an even more immediate, in-context way to read LLM responses that
    would not require them to attach another node.^(16)^(16)16This need was again
    reflected in a later GitHub Issue and has since been addressed with a pull-out
    inspector drawer.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 与实验室参与者一样，受访者也遇到了可用性和概念性问题。一个共同的主题是，个别用户表达了对已经存在但相对隐藏的功能的需求，这些功能仅通过示例或文档显现出来。这些隐藏的功能包括*隐式模板变量*、*元变量*和*模板链*。前两种功能满足用户需要在链的更下游引用上游元数据——与输入数据或响应相关的元数据。^(15)^(15)15例如，在ChainForge中，可以定义一个表格的列，然后通过元变量在下游引用它（图[5](#S4.F5
    "图 5 ‣ 4.2\. 与在线和试点用户的迭代开发 ‣ 4\. ChainForge ‣ ChainForge：一个用于提示工程和LLM假设测试的可视化工具包")）。然而，Q5和Q6似乎没有意识到这个功能，并实施了变通方案。对于隐式模板变量，Q4需要在提示链中稍后引用一个上游值{gender}，并且没有意识到隐式模板变量可以完成这一任务（例如{#gender}）。另一个痛点是在机会性探索阶段的摩擦。在第6节中，我们提到了一些受访者的流程中存在断开的区域，其中一个区域被称为机会性探索模式（通过输入数据、提示、模型和假设的快速、早期阶段迭代；通常是一个由三个节点组成的链：TextField-Prompt-Inspect）。在这种模式下，一些受访者更喜欢直接在流程中使用Inspect
    Node检查响应（而不是弹出窗口），因为这有利于快速迭代。他们希望有一种更直接、更上下文的方式来读取LLM响应，而不需要再附加另一个节点。^(16)^(16)16这一需求在后来的GitHub问题中再次反映出来，并已通过一个可拉出的检查器抽屉解决。
- en: 8.4\. Open-source flexibility
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4\. 开源灵活性
- en: 'Multiple interviewees mentioned looking at our source code, and two projects
    extended it. Q5 and Q6, employees of a consulting firm that works with the German
    government, extended the code to support a German-based LLM provider, AlephAlpha
    (Aleph-Alpha, [2023](#bib.bib3)), complete with a settings screen. They cited
    the value of supporting European businesses and GDPR data protection laws: *“the
    government [of Germany] wants to support it. It’s a local player… [and] There’s
    a strong need to to hide and to to protect your data. I mean, GDPR, it’s very
    strict in this.”* Their goal was to use ChainForge to determine “if it makes sense
    to switch to” the German model for their use cases, over OpenAI models. HCI researchers
    Q1 and Q2’s chief interaction with the tool was its source code, finding it helpful
    for jumpstarting a project on a flow-based tool for LLM image model prototyping.
    Q2 appreciated the *“thought put into”* caching, Prompt Node progress bar, and
    multi-model querying, adding: *“It was very easy for me to set up ChainForge…
    [and it was] surprisingly easy to [extend]… a lot easier than I had expected.”*
    They said that the jump-start ChainForge provided was a chief reason they were
    able to complete their project in time to submit a paper to the annual CHI conference.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 多名受访者提到查看了我们的源代码，其中两个项目对其进行了扩展。Q5 和 Q6，作为一家与德国政府合作的咨询公司员工，将代码扩展以支持基于德国的 LLM
    提供商 AlephAlpha (Aleph-Alpha, [2023](#bib.bib3))，并添加了设置界面。他们提到了支持欧洲企业和 GDPR 数据保护法的重要性：*“德国政府希望支持它。这是一个本地参与者……
    [而且] 有很强的需求去隐藏和保护你的数据。我的意思是，GDPR，对此非常严格。”* 他们的目标是使用 ChainForge 来确定“是否有意义切换到”适用于他们使用场景的德国模型，而不是
    OpenAI 模型。HCI 研究员 Q1 和 Q2 对该工具的主要互动是其源代码，他们发现它对于启动一个基于流的工具进行 LLM 图像模型原型设计的项目非常有帮助。Q2
    赞赏了 *“在”* 缓存、Prompt Node 进度条和多模型查询中投入的 *“思考”*，并补充道：*“我设置 ChainForge 非常容易…… [而且]
    扩展起来非常轻松…… 比我预期的要容易得多。”* 他们表示，ChainForge 提供的启动帮助是他们能够按时完成项目并提交论文到年度 CHI 会议的主要原因。
- en: 9\. Discussion and Conclusion
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9. 讨论与结论
- en: Our observations suggest that ChainForge is useful both in itself, but also
    as an ‘enabling’ contribution, an open-source project which others can extend
    (and are extending) to investigate their own ideas and topics, including other
    research publications to this very conference. Given that ChainForge was released
    only a few months ago, we believe the stories presented here provide evidence
    for its real-world usefulness. In the rest of this paper, we review our key findings.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的观察表明，ChainForge 本身有用，同时也作为一种‘赋能’贡献，一个开源项目，其他人可以扩展（并且正在扩展）以调查自己的想法和主题，包括到本次会议上的其他研究出版物。鉴于
    ChainForge 仅在几个月前发布，我们认为这里呈现的故事为其现实世界的实用性提供了证据。在本文的其余部分，我们回顾了我们的关键发现。
- en: 'Our work represents one of the only “prompt engineering” system contributions
    with data about real-world usage, as opposed to in-lab studies on structured tasks.
    Some of what real users cared about, like features for exporting data and sharing,
    were absent from our in-lab study—and are, in fact, also absent from similar LLM-prompting-system
    research with in-lab studies (Wu et al., [2022a](#bib.bib44), [b](#bib.bib45);
    Brade et al., [2023](#bib.bib7); Mishra et al., [2023](#bib.bib26); Zamfirescu-Pereira
    et al., [2023](#bib.bib46)). Most surprising (to us) was that some knowledge workers
    were using ChainForge for a task we had never anticipated—*data processing*. Although
    we only had six interface users in our interview study, the only two in-lab participants
    in startups, P8 and P4, were both testing LLMs’ ability to process and reformat
    data. Most prior LLM tools target sensemaking (Jiang et al., [2023](#bib.bib16);
    Suh et al., [2023](#bib.bib38)), prompt engineering (Mishra et al., [2023](#bib.bib26);
    Jiang et al., [2022](#bib.bib15)), or app building (Wu et al., [2022a](#bib.bib44)),
    but do not specifically target, or even mention, data processing. Our findings
    suggest a need for systems to support *on-demand creation of data processing pipelines
    involving LLMs,* where the purpose is not (always) to make apps, but simply process
    data and share the results. ChainForge’s combinatorial power—the ability to send
    off many queries at once, parametrized by imported data—appeared key to supporting
    this need. Future systems should go further by providing users more accessible
    ways to reference upstream metadata further downstream in their chain (see [8.3](#S8.SS3
    "8.3\. Pain points: Hidden affordances and friction during opportunistic exploration
    mode ‣ 8\. Interviews with Real-World Users ‣ ChainForge: A Visual Toolkit for
    Prompt Engineering and LLM Hypothesis Testing")).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作代表了少数几项“提示工程”系统贡献之一，提供了关于实际使用的数据，而不是在结构化任务上的实验室研究。实际用户关心的一些方面，比如数据导出和共享功能，在我们的实验室研究中缺失了——事实上，这些也缺失于类似的LLM提示系统研究中（Wu等，[2022a](#bib.bib44)，[b](#bib.bib45)；Brade等，[2023](#bib.bib7)；Mishra等，[2023](#bib.bib26)；Zamfirescu-Pereira等，[2023](#bib.bib46)）。让我们感到最意外的是，一些知识工作者在进行我们未曾预见的任务——*数据处理*。尽管我们在访谈研究中只有六个接口用户，但唯一两个实验室参与者P8和P4都在测试LLMs处理和重新格式化数据的能力。大多数先前的LLM工具集中在意义构建（Jiang等，[2023](#bib.bib16)；Suh等，[2023](#bib.bib38)）、提示工程（Mishra等，[2023](#bib.bib26)；Jiang等，[2022](#bib.bib15)）或应用构建（Wu等，[2022a](#bib.bib44)），但并不专门针对，甚至提及数据处理。我们的发现表明，系统需要支持*按需创建涉及LLMs的数据处理管道*，其目的不总是创建应用程序，而只是处理数据并分享结果。ChainForge的组合能力——能够一次发送多个查询，并通过导入的数据进行参数化——似乎是支持这一需求的关键。未来的系统应进一步提供更便捷的方式，让用户能够在链条的下游引用上游元数据（见[8.3](#S8.SS3
    "8.3\. 痛点：机会探索模式中的隐性功能和摩擦 ‣ 8\. 与真实世界用户的访谈 ‣ ChainForge：提示工程和LLM假设测试的可视化工具包")）。
- en: 'Second, we identified three modes of prompt engineering and LLM hypothesis
    testing: *opportunistic exploration*, *limited evaluation*, and *iterative refinement*.
    The first mode is similar to Barke et al.’s exploration mode for GitHub CoPilot
    (Barke et al., [2023](#bib.bib4)). Future systems should explicitly consider these
    modes when designing and framing the work. For instance, users often too quickly
    enter iterative refinement mode—refining on the first prompt they try—rather than
    exploring a variety before settling on one (Zamfirescu-Pereira et al., [2023](#bib.bib46)).
    If a prompt engineering tool only targets iterative refinement, then the opportunistic
    exploration stage—finding a good prompt to begin with—may be too quickly skirted
    over, trapping users in potentially suboptimal prompting strategies. These modes
    also suggest design opportunities. For instance, we believe that ChainForge’s
    design could have better supported opportunistic exploration mode, with some users
    wanting a simpler way to inspect LLM responses in-context ([8.3](#S8.SS3 "8.3\.
    Pain points: Hidden affordances and friction during opportunistic exploration
    mode ‣ 8\. Interviews with Real-World Users ‣ ChainForge: A Visual Toolkit for
    Prompt Engineering and LLM Hypothesis Testing")). One design solution may be to
    concretize each mode into separate, related interfaces or layouts—e.g., a more
    chat-like interface for exploration mode, that then facilitates the transition
    to later modes, each with dedicated interfaces. Prior LLM-prompting systems seem
    to target opportunistic exploration (Jiang et al., [2023](#bib.bib16); Suh et al.,
    [2023](#bib.bib38)) or iterative refinement (Mishra et al., [2023](#bib.bib26);
    Strobelt et al., [2022](#bib.bib37)), but overlook *limited evaluation*: an important
    mid-way point characterized by prototyping small-scale, quick-and-messy evaluations
    on the way to greater understanding. Future work might target the prototyping
    of on-demand LLM evaluation pipelines themselves (see “model sketching” for inspiration
    (Lam et al., [2023](#bib.bib19))).'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我们识别了三种提示工程和大语言模型（LLM）假设测试的模式：*机会性探索*、*有限评估*和*迭代优化*。第一种模式类似于Barke等人对GitHub
    CoPilot的探索模式（Barke et al., [2023](#bib.bib4)）。未来的系统在设计和框定工作时应明确考虑这些模式。例如，用户通常会过快地进入迭代优化模式——在尝试第一个提示后就开始优化——而不是在确定一个之前探索多种提示（Zamfirescu-Pereira
    et al., [2023](#bib.bib46)）。如果一个提示工程工具仅针对迭代优化，那么机会性探索阶段——找到一个好的提示——可能会被过快地忽略，从而使用户陷入潜在的次优提示策略中。这些模式也暗示了设计机会。例如，我们认为ChainForge的设计本可以更好地支持机会性探索模式，有些用户希望有一种更简单的方式来检查上下文中的LLM响应（[8.3](#S8.SS3
    "8.3\. 痛点：机会性探索模式中的隐藏赋能与摩擦 ‣ 8\. 真实世界用户访谈 ‣ ChainForge：提示工程和LLM假设测试的可视化工具包")）。一种设计解决方案可能是将每种模式具体化为分开但相关的界面或布局——例如，探索模式使用更像聊天的界面，然后过渡到后续模式，每种模式都有专用界面。先前的LLM提示系统似乎针对机会性探索（Jiang
    et al., [2023](#bib.bib16)；Suh et al., [2023](#bib.bib38)）或迭代优化（Mishra et al.,
    [2023](#bib.bib26)；Strobelt et al., [2022](#bib.bib37)），但忽视了*有限评估*：这是一个重要的中间点，其特征是对小规模、快速而杂乱的评估进行原型设计，以达到更深的理解。未来的工作可能会针对按需LLM评估管道的原型设计（参见“模型草图”以获取灵感（Lam
    et al., [2023](#bib.bib19)））。
- en: 'Third, we found that when people choose different prompts and models, they
    weigh *trade-offs* in performance for different criteria and contexts, and bring
    their own perspectives, values, preferences, and contexts to bear on decision-making.
    Having multiple representations of responses seemed to help participants weigh
    trade-offs, rank prompts and models, develop better mental models, and make revisions
    to their prompts or hypotheses more confidently. Connecting to theories of human
    learning (Gentner and Markman, [1997](#bib.bib11); Marton, [2014](#bib.bib24)),
    the case study in [7.2](#S7.SS2 "7.2\. Case Studies for Modes of Usage ‣ 7\. In-lab
    Study Findings ‣ ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis
    Testing").1 suggests that cross-model comparison might also help novices improve
    mental models of AI by forcing them to encounter differences in factual information,
    jarring AI over-reliance (Liao and Sundar, [2022](#bib.bib21)). The subjectivity
    of choosing a model and prompt implies that, while LLMs can certainly *help* users
    generate or evaluate prompts (Brade et al., [2023](#bib.bib7); Zhou et al., [2022](#bib.bib47)),
    there will never be such a thing as *fully* automated prompt engineering. Rather
    than framing prompt engineering (purely) as an optimization problem, projects
    looking to support prompt engineering should instead look for ways to give users
    greater *control* over their search process (e.g., “steering” (Brade et al., [2023](#bib.bib7);
    Zhou et al., [2022](#bib.bib47))).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '第三，我们发现，当人们选择不同的提示和模型时，他们会在不同的标准和背景下权衡*权衡*的表现，并在决策过程中带入自己的视角、价值观、偏好和背景。拥有多个响应的表现似乎帮助参与者权衡权衡，排名提示和模型，发展更好的心理模型，并更自信地修订他们的提示或假设。联系到人类学习理论（Gentner和Markman，[1997](#bib.bib11)；Marton，[2014](#bib.bib24)），在[7.2](#S7.SS2
    "7.2\. Case Studies for Modes of Usage ‣ 7\. In-lab Study Findings ‣ ChainForge:
    A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing").1中的案例研究表明，跨模型比较也可能帮助新手通过强制他们遇到事实信息的差异、打破对AI的过度依赖（Liao和Sundar，[2022](#bib.bib21)）来改善AI的心理模型。选择模型和提示的主观性意味着，尽管LLM可以*帮助*用户生成或评估提示（Brade等，[2023](#bib.bib7)；Zhou等，[2022](#bib.bib47)），但永远不会有*完全*自动化的提示工程。与其将提示工程（纯粹）框架为一个优化问题，寻求支持提示工程的项目应当寻找方法，让用户对他们的搜索过程拥有更大的*控制*（例如，“引导”（Brade等，[2023](#bib.bib7)；Zhou等，[2022](#bib.bib47)））。'
- en: 'A final point and caveat: while users found ChainForge useful for *implementation*
    and *iteration*, including on real-world tasks, more work needs to be done on
    *conceptualization* and *planning* aspects, to help users move out of opportunistic
    exploration into more systematic evaluations. In-lab users seemed limited in their
    ability to imagine systematizing their tests, *even a few with prior expertise
    in AI or programming with LLM APIs*. This extends prior work studying how “non-AI-experts”
    prompt LLMs (Zamfirescu-Pereira et al., [2023](#bib.bib46)), suggesting even people
    who otherwise perceive themselves to be AI experts may have trouble systematizing
    their evaluations. Since LLMs are nondeterministic (at least, often queried at
    non-zero temperatures) and prone to unexpected jumps in behavior from small perturbations,
    it is important that future systems and resources help reduce fixation and guide
    users from early exploration into systematic evaluations. We might leverage concepts
    from tools designed for more targeted use cases; e.g., the auditing tool AdaTest++
    provides users “prompt templates that translate experts’ auditing strategies into
    reusable prompts” (Rastogi et al., [2023](#bib.bib35), p. 15-6). Other work supports
    creation of prompts or searching of a “prompt space” (Shi et al., [2023](#bib.bib36);
    Mishra et al., [2023](#bib.bib26); Strobelt et al., [2022](#bib.bib37)). To support
    systematization/scaling up, we might also employ an interaction whereby a user
    chats with an AI that sketches out an evaluation strategy.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点和警告：虽然用户发现 ChainForge 对于*实施*和*迭代*，包括在实际任务中很有用，但在*概念化*和*规划*方面仍需更多工作，以帮助用户从机会性探索转向更系统的评估。实验室用户在想象系统化测试的能力方面似乎受限，*即使是那些在
    AI 或编程与 LLM API 方面有一定经验的用户*。这扩展了先前关于“非 AI 专家”如何提示 LLM 的研究（Zamfirescu-Pereira 等，[2023](#bib.bib46)），表明即使是那些自认为是
    AI 专家的用户，也可能在系统化评估方面遇到困难。由于 LLM 是非确定性的（至少，通常在非零温度下查询）并且容易因小的扰动而行为异常，因此未来的系统和资源应帮助减少固守并引导用户从早期探索转向系统化评估。我们可能会借鉴为更具针对性的用例设计的工具的概念；例如，审计工具
    AdaTest++ 为用户提供了“将专家审计策略转化为可重复使用提示的提示模板”（Rastogi 等，[2023](#bib.bib35)，第 15-6 页）。其他研究支持创建提示或搜索“提示空间”（Shi
    等，[2023](#bib.bib36)；Mishra 等，[2023](#bib.bib26)；Strobelt 等，[2022](#bib.bib37)）。为了支持系统化/规模化，我们也可以采用一种交互方式，即用户与
    AI 进行对话，AI 绘制出评估策略。
- en: 9.1\. Limitations
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 限制
- en: Our choice to use a qualitative evaluation methodology derived from well-known
    difficulties around toolkit research (Ledo et al., [2018](#bib.bib20); Olsen Jr,
    [2007](#bib.bib29)), concerns about ecological validity, and, most importantly,
    from the fact that we could not find a prior, well-established interface that
    matched the entire featureset of ChainForge. Our goal was thus to establish a
    baseline system that future work might improve upon. While we believe our qualitative
    evaluation yielded some important findings, more quantitative, controlled approaches
    should be performed on parts of the ChainForge interface to answer targeted scientific
    questions. Our in-lab study was also of a relatively short duration (75 min);
    future work might observe changes in user behavior over longer timeframes, for
    instance with a multi-week workshop. Finally, for our interview study, we acknowledge
    a self-selection bias, where participating interviewees may already have found
    ChainForge useful, missing users who did not. Our in-lab study provided some insights—we
    speculate that users’ prior exposure to programming was important to the quality
    of their experience.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择使用定性评估方法，源于工具包研究中的一些已知困难（Ledo 等， [2018](#bib.bib20)；Olsen Jr，[2007](#bib.bib29)），对生态有效性的关注，以及最重要的一点，即我们未能找到一个先前存在的、匹配
    ChainForge 所有功能特性的成熟界面。因此，我们的目标是建立一个基准系统，未来的工作可以在此基础上进行改进。虽然我们认为我们的定性评估得出了一些重要发现，但应对
    ChainForge 界面的部分进行更多定量、控制的方法，以回答特定的科学问题。我们的实验室研究时间也相对较短（75 分钟）；未来的工作可能会观察用户行为在更长时间范围内的变化，例如通过多周的研讨会。最后，对于我们的访谈研究，我们承认存在自我选择偏差，即参与访谈的受访者可能已经发现
    ChainForge 有用，而未能涵盖那些没有发现其用处的用户。我们的实验室研究提供了一些见解——我们推测用户的编程背景对他们的体验质量很重要。
- en: Acknowledgements.
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: This work was partially funded by the NSF grants IIS-2107391, IIS-2040880, and
    IIS-1955699\. Any opinions, findings, and conclusions or recommendations expressed
    in this material are those of the author(s) and do not necessarily reflect the
    views of the National Science Foundation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究部分由 NSF 资助，资助编号为 IIS-2107391, IIS-2040880 和 IIS-1955699。本文中表达的任何意见、发现、结论或建议均为作者（们）的观点，不一定反映国家科学基金会的观点。
- en: References
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: Lan (2023) Accessed 2023. langchain 0.0.288. [https://pypi.org/project/langchain/](https://pypi.org/project/langchain/).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan (2023) 访问于2023年。langchain 0.0.288. [https://pypi.org/project/langchain/](https://pypi.org/project/langchain/)。
- en: 'Aleph-Alpha (2023) Aleph-Alpha. 2023. Aleph-Alpha. [https://www.aleph-alpha.com](https://www.aleph-alpha.com)
    Accessed: Sep 2 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Aleph-Alpha (2023) Aleph-Alpha. 2023. Aleph-Alpha. [https://www.aleph-alpha.com](https://www.aleph-alpha.com)
    访问日期: 2023年9月2日。'
- en: 'Barke et al. (2023) Shraddha Barke, Michael B James, and Nadia Polikarpova.
    2023. Grounded copilot: How programmers interact with code-generating models.
    *Proceedings of the ACM on Programming Languages* 7, OOPSLA1 (2023), 85–111.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Barke 等人 (2023) Shraddha Barke, Michael B James 和 Nadia Polikarpova. 2023.
    Grounded copilot: 程序员如何与代码生成模型互动. *ACM 编程语言会议论文集* 7, OOPSLA1 (2023), 85–111。'
- en: 'Beurer-Kellner et al. (2023) Luca Beurer-Kellner, Marc Fischer, and Martin
    Vechev. 2023. Prompting is programming: A query language for large language models.
    *Proceedings of the ACM on Programming Languages* 7, PLDI (2023), 1946–1969.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beurer-Kellner 等人 (2023) Luca Beurer-Kellner, Marc Fischer 和 Martin Vechev.
    2023. 提示即编程：大型语言模型的查询语言. *ACM 编程语言会议论文集* 7, PLDI (2023), 1946–1969。
- en: Binder et al. (2022) Markus Binder, Bernd Heinrich, Marcus Hopf, and Alexander
    Schiller. 2022. Global reconstruction of language models with linguistic rules–Explainable
    AI for online consumer reviews. *Electronic Markets* 32, 4 (2022), 2123–2138.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Binder 等人 (2022) Markus Binder, Bernd Heinrich, Marcus Hopf 和 Alexander Schiller.
    2022. 使用语言规则对语言模型进行全球重建–在线消费者评论的可解释AI. *电子市场* 32, 4 (2022), 2123–2138。
- en: 'Brade et al. (2023) Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore,
    and Tovi Grossman. 2023. Promptify: Text-to-Image Generation through Interactive
    Prompt Exploration with Large Language Models. *arXiv preprint arXiv:2304.09337*
    (2023).'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brade 等人 (2023) Stephen Brade, Bryan Wang, Mauricio Sousa, Sageev Oore 和 Tovi
    Grossman. 2023. Promptify: 通过与大型语言模型的交互提示探索进行文本到图像生成. *arXiv 预印本 arXiv:2304.09337*
    (2023)。'
- en: 'Deng et al. (2023) Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang,
    Zefeng Li, Haoyu Wang, Tianwei Zhang, and Yang Liu. 2023. Jailbreaker: Automated
    Jailbreak Across Multiple Large Language Model Chatbots. *arXiv preprint arXiv:2307.08715*
    (2023).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Deng 等人 (2023) Gelei Deng, Yi Liu, Yuekang Li, Kailong Wang, Ying Zhang, Zefeng
    Li, Haoyu Wang, Tianwei Zhang 和 Yang Liu. 2023. Jailbreaker: 跨多个大型语言模型聊天机器人进行自动越狱.
    *arXiv 预印本 arXiv:2307.08715* (2023)。'
- en: FlowiseAI (2023) Inc FlowiseAI. Accessed 2023. FlowiseAI Build LLMs Apps Easily.
    [flowiseai.com](flowiseai.com).
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FlowiseAI (2023) Inc FlowiseAI. 访问于2023年。FlowiseAI 轻松构建LLM应用程序。 [flowiseai.com](flowiseai.com)。
- en: Friedman et al. (2023) Nat Friedman, Zain Huda, and Alex Lourenco. Accessed
    2023. Nat.Dev. [https://nat.dev/](https://nat.dev/).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Friedman 等人 (2023) Nat Friedman, Zain Huda 和 Alex Lourenco. 访问于2023年。Nat.Dev.
    [https://nat.dev/](https://nat.dev/)。
- en: Gentner and Markman (1997) Dedre Gentner and Arthur B Markman. 1997. Structure
    mapping in analogy and similarity. *American psychologist* 52, 1 (1997), 45.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gentner 和 Markman (1997) Dedre Gentner 和 Arthur B Markman. 1997. 类比和相似性的结构映射.
    *美国心理学家* 52, 1 (1997), 45。
- en: Greenberg and Buxton (2008) Saul Greenberg and Bill Buxton. 2008. Usability
    evaluation considered harmful (some of the time). In *Proceedings of the SIGCHI
    conference on Human factors in computing systems*. 111–120.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greenberg 和 Buxton (2008) Saul Greenberg 和 Bill Buxton. 2008. 可用性评估有时被认为有害。在
    *SIGCHI 人机计算系统会议论文集* 中. 111–120。
- en: Huyen (2022) Chip Huyen. 2022. *Designing machine learning systems*. ” O’Reilly
    Media, Inc.”.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huyen (2022) Chip Huyen. 2022. *设计机器学习系统*. ” O’Reilly Media, Inc.”。
- en: Jest (2023) Jest. Accessed 2023. Jest. [https://jestjs.io/](https://jestjs.io/).
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jest (2023) Jest. 访问于2023年。Jest. [https://jestjs.io/](https://jestjs.io/)。
- en: 'Jiang et al. (2022) Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina,
    Aaron Donsbach, Michael Terry, and Carrie J Cai. 2022. Promptmaker: Prompt-based
    prototyping with large language models. In *CHI Conference on Human Factors in
    Computing Systems Extended Abstracts*. 1–8.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jiang 等人 (2022) Ellen Jiang, Kristen Olson, Edwin Toh, Alejandra Molina, Aaron
    Donsbach, Michael Terry 和 Carrie J Cai. 2022. Promptmaker: 基于提示的原型设计与大型语言模型。在
    *CHI 人机计算系统扩展摘要* 中. 1–8。'
- en: 'Jiang et al. (2023) Peiling Jiang, Jude Rayan, Steven P Dow, and Haijun Xia.
    2023. Graphologue: Exploring Large Language Model Responses with Interactive Diagrams.
    *arXiv preprint arXiv:2305.11473* (2023).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Peiling Jiang、Jude Rayan、Steven P Dow 和 Haijun Xia。2023。Graphologue：使用交互式图表探索大型语言模型的响应。*arXiv
    预印本 arXiv:2305.11473*（2023）。
- en: 'Kang et al. (2018) Laewoo Kang, Steven J Jackson, and Phoebe Sengers. 2018.
    Intermodulation: improvisation and collaborative art practice for hci. In *Proceedings
    of the 2018 CHI conference on human factors in computing systems*. 1–13.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang 等（2018）Laewoo Kang、Steven J Jackson 和 Phoebe Sengers。2018。互调：HCI 的即兴和协作艺术实践。收录于
    *2018 年 CHI 人机计算系统会议论文集*。1–13。
- en: 'Kim et al. (2023) Tae Soo Kim, Yoonjoo Lee, Minsuk Chang, and Juho Kim. 2023.
    Cells, Generators, and Lenses: Design Framework for Object-Oriented Interaction
    with Large Language Models. In *Proceedings of the 36th Annual ACM Symposium on
    User Interface Software and Technology*. 1–18. To appear..'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2023）Tae Soo Kim、Yoonjoo Lee、Minsuk Chang 和 Juho Kim。2023。单元、生成器和透镜：面向大型语言模型的面向对象交互设计框架。收录于
    *第 36 届 ACM 用户界面软件与技术年会论文集*。1–18。即将出版。
- en: 'Lam et al. (2023) Michelle S Lam, Zixian Ma, Anne Li, Izequiel Freitas, Dakuo
    Wang, James A Landay, and Michael S Bernstein. 2023. Model Sketching: Centering
    Concepts in Early-Stage Machine Learning Model Design. In *Proceedings of the
    2023 CHI Conference on Human Factors in Computing Systems*. 1–24.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lam 等（2023）Michelle S Lam、Zixian Ma、Anne Li、Izequiel Freitas、Dakuo Wang、James
    A Landay 和 Michael S Bernstein。2023。模型草图：在早期阶段的机器学习模型设计中聚焦概念。收录于 *2023 年 CHI 人机计算系统会议论文集*。1–24。
- en: Ledo et al. (2018) David Ledo, Steven Houben, Jo Vermeulen, Nicolai Marquardt,
    Lora Oehlberg, and Saul Greenberg. 2018. Evaluation strategies for HCI toolkit
    research. In *Proceedings of the 2018 CHI Conference on Human Factors in Computing
    Systems*. 1–17.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ledo 等（2018）David Ledo、Steven Houben、Jo Vermeulen、Nicolai Marquardt、Lora Oehlberg
    和 Saul Greenberg。2018。HCI 工具包研究的评估策略。收录于 *2018 年 CHI 人机计算系统会议论文集*。1–17。
- en: 'Liao and Sundar (2022) Q Vera Liao and S Shyam Sundar. 2022. Designing for
    responsible trust in AI systems: A communication perspective. In *Proceedings
    of the 2022 ACM Conference on Fairness, Accountability, and Transparency*. 1257–1268.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao 和 Sundar（2022）Q Vera Liao 和 S Shyam Sundar。2022。为 AI 系统设计负责任的信任：一种沟通视角。收录于
    *2022 年 ACM 公平性、问责制和透明度会议论文集*。1257–1268。
- en: 'Liffiton et al. (2023) Mark Liffiton, Brad Sheese, Jaromir Savelka, and Paul
    Denny. 2023. CodeHelp: Using Large Language Models with Guardrails for Scalable
    Support in Programming Classes. *arXiv preprint arXiv:2308.06921* (2023).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liffiton 等（2023）Mark Liffiton、Brad Sheese、Jaromir Savelka 和 Paul Denny。2023。CodeHelp：使用带保护措施的大型语言模型为编程课程提供可扩展支持。*arXiv
    预印本 arXiv:2308.06921*（2023）。
- en: Logspace (2023) Logspace. Accessed 2023. LangFlow. [https://www.langflow.org/](https://www.langflow.org/).
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Logspace（2023）Logspace。访问于 2023。LangFlow。 [https://www.langflow.org/](https://www.langflow.org/)。
- en: Marton (2014) Ference Marton. 2014. *Necessary conditions of learning*. Routledge.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marton（2014）Ference Marton。2014。*学习的必要条件*。Routledge。
- en: Microsoft (2023) Microsoft. Accessed 2023. Prompt flow. [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft（2023）Microsoft。访问于 2023。Prompt flow。 [https://microsoft.github.io/promptflow/](https://microsoft.github.io/promptflow/)。
- en: 'Mishra et al. (2023) Aditi Mishra, Utkarsh Soni, Anjana Arunkumar, Jinbin Huang,
    Bum Chul Kwon, and Chris Bryan. 2023. PromptAid: Prompt Exploration, Perturbation,
    Testing and Iteration using Visual Analytics for Large Language Models. *arXiv
    preprint arXiv:2304.01964* (2023).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mishra 等（2023）Aditi Mishra、Utkarsh Soni、Anjana Arunkumar、Jinbin Huang、Bum Chul
    Kwon 和 Chris Bryan。2023。PromptAid：利用视觉分析进行大型语言模型的提示探索、扰动、测试和迭代。*arXiv 预印本 arXiv:2304.01964*（2023）。
- en: Neubig and He (2023) Graham Neubig and Zhiwei He. 2023. Zeno GPT Machine Translation
    Report.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neubig 和 He（2023）Graham Neubig 和 Zhiwei He。2023。Zeno GPT 机器翻译报告。
- en: 'Ng et al. (2022) Wing Ng, Ava Anjom, and Joanna M Drinane. 2022. Beyond Amazon:
    Social Justice and Ethical Considerations for Research Compensation. *Psychotherapy
    Bulletin* (2022), 17.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ng 等（2022）Wing Ng、Ava Anjom 和 Joanna M Drinane。2022。超越亚马逊：研究补偿中的社会正义和伦理考虑。*心理治疗公报*（2022），17。
- en: Olsen Jr (2007) Dan R Olsen Jr. 2007. Evaluating user interface systems research.
    In *Proceedings of the 20th annual ACM symposium on User interface software and
    technology*. 251–258.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olsen Jr（2007）Dan R Olsen Jr。2007。评估用户界面系统研究。收录于 *第 20 届 ACM 用户界面软件与技术年会论文集*。251–258。
- en: OpenAI (2023a) OpenAI. Accessed 2023a. OpenAI Playground. [https://platform.openai.com/playground](https://platform.openai.com/playground).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI（2023a）OpenAI。访问于 2023a。OpenAI Playground。 [https://platform.openai.com/playground](https://platform.openai.com/playground)。
- en: OpenAI (2023b) OpenAI. Accessed 2023b. openai/evals. [https://github.com/openai/evals](https://github.com/openai/evals).
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023b) OpenAI. 访问于 2023b。openai/evals。 [https://github.com/openai/evals](https://github.com/openai/evals)。
- en: 'Pater et al. (2021) Jessica Pater, Amanda Coupe, Rachel Pfafman, Chanda Phelan,
    Tammy Toscos, and Maia Jacobs. 2021. Standardizing reporting of participant compensation
    in HCI: A systematic literature review and recommendations for the field. In *Proceedings
    of the 2021 CHI conference on human factors in computing systems*. 1–16.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pater et al. (2021) Jessica Pater, Amanda Coupe, Rachel Pfafman, Chanda Phelan,
    Tammy Toscos, 和 Maia Jacobs. 2021. 在 HCI 中标准化参与者补偿的报告：系统文献综述及领域建议。发表于*2021 CHI
    人因计算系统会议论文集*。1–16。
- en: 'Perez and Ribeiro (2022) Fábio Perez and Ian Ribeiro. 2022. Ignore Previous
    Prompt: Attack Techniques For Language Models. In *NeurIPS ML Safety Workshop*.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perez 和 Ribeiro (2022) Fábio Perez 和 Ian Ribeiro. 2022. 忽略前一个提示：语言模型攻击技术。发表于*NeurIPS
    ML 安全研讨会*。
- en: promptfoo (2023) promptfoo. Accessed 2023. promptfoo Test your prompts. [https://www.promptfoo.dev/](https://www.promptfoo.dev/).
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: promptfoo (2023) promptfoo. 访问于 2023。promptfoo 测试你的提示。 [https://www.promptfoo.dev/](https://www.promptfoo.dev/)。
- en: Rastogi et al. (2023) Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, and
    Saleema Amershi. 2023. Supporting Human-AI Collaboration in Auditing LLMs with
    LLMs. *arXiv preprint arXiv:2304.09991* (2023).
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rastogi et al. (2023) Charvi Rastogi, Marco Tulio Ribeiro, Nicholas King, 和
    Saleema Amershi. 2023. 通过 LLM 支持人机协作进行 LLM 审计。*arXiv 预印本 arXiv:2304.09991* (2023)。
- en: Shi et al. (2023) Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan
    Lu, and Xiaodong Lin. 2023. Prompt Space Optimizing Few-shot Reasoning Success
    with Large Language Models. *arXiv preprint arXiv:2306.03799* (2023).
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2023) Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan
    Lu, 和 Xiaodong Lin. 2023. 提示空间优化：大型语言模型的少样本推理成功。*arXiv 预印本 arXiv:2306.03799* (2023)。
- en: Strobelt et al. (2022) Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin
    Hoover, Johanna Beyer, Hanspeter Pfister, and Alexander M Rush. 2022. Interactive
    and visual prompt engineering for ad-hoc task adaptation with large language models.
    *IEEE transactions on visualization and computer graphics* 29, 1 (2022), 1146–1156.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strobelt et al. (2022) Hendrik Strobelt, Albert Webson, Victor Sanh, Benjamin
    Hoover, Johanna Beyer, Hanspeter Pfister, 和 Alexander M Rush. 2022. 交互式和可视化的提示工程用于大语言模型的即席任务适配。*IEEE
    视觉化与计算机图形学期刊* 29, 1 (2022), 1146–1156。
- en: 'Suh et al. (2023) Sangho Suh, Bryan Min, Srishti Palani, and Haijun Xia. 2023.
    Sensecape: Enabling Multilevel Exploration and Sensemaking with Large Language
    Models. *arXiv preprint arXiv:2305.11483* (2023).'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suh et al. (2023) Sangho Suh, Bryan Min, Srishti Palani, 和 Haijun Xia. 2023.
    Sensecape：利用大型语言模型实现多层次探索和理解。*arXiv 预印本 arXiv:2305.11483* (2023)。
- en: Sun et al. (2022) Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and
    Xipeng Qiu. 2022. Black-box tuning for language-model-as-a-service. In *International
    Conference on Machine Learning*. PMLR, 20841–20855.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2022) Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, 和 Xipeng
    Qiu. 2022. 语言模型即服务的黑箱调优。发表于*国际机器学习大会*。PMLR, 20841–20855。
- en: TruLens (2023) TruLens. Accessed 2023. trulens Evaluate and Track LLM Applications.
    [https://www.trulens.org/](https://www.trulens.org/).
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TruLens (2023) TruLens. 访问于 2023。trulens 评估和跟踪 LLM 应用。 [https://www.trulens.org/](https://www.trulens.org/)。
- en: Vellum (2023) Vellum. Accessed 2023. Vellum The dev platform for production
    LLM apps. [https://www.vellum.ai/](https://www.vellum.ai/).
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vellum (2023) Vellum. 访问于 2023。Vellum 生产 LLM 应用的开发平台。 [https://www.vellum.ai/](https://www.vellum.ai/)。
- en: Vercel (2023) Vercel. Accessed 2023. Vercel Deveop.Preview.Ship. [https://vercel.com/](https://vercel.com/).
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vercel (2023) Vercel. 访问于 2023。Vercel 开发。预览。发布。 [https://vercel.com/](https://vercel.com/)。
- en: 'Weights and Biases (2023) Weights and Biases. Accessed 2023. Weights and Biases
    Docs: Prompts for LLMs. [https://docs.wandb.ai/guides/prompts](https://docs.wandb.ai/guides/prompts).'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weights and Biases (2023) Weights and Biases. 访问于 2023。Weights and Biases 文档：LLM
    提示。 [https://docs.wandb.ai/guides/prompts](https://docs.wandb.ai/guides/prompts)。
- en: 'Wu et al. (2022a) Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra
    Molina, Michael Terry, and Carrie J Cai. 2022a. Promptchainer: Chaining large
    language model prompts through visual programming. In *CHI Conference on Human
    Factors in Computing Systems Extended Abstracts*. 1–10.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2022a) Tongshuang Wu, Ellen Jiang, Aaron Donsbach, Jeff Gray, Alejandra
    Molina, Michael Terry, 和 Carrie J Cai. 2022a. Promptchainer：通过视觉编程链式处理大型语言模型提示。发表于*CHI
    计算机系统人因会议扩展摘要*。1–10。
- en: 'Wu et al. (2022b) Tongshuang Wu, Michael Terry, and Carrie Jun Cai. 2022b.
    Ai chains: Transparent and controllable human-ai interaction by chaining large
    language model prompts. In *Proceedings of the 2022 CHI conference on human factors
    in computing systems*. 1–22.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 (2022b) Tongshuang Wu, Michael Terry, 和 Carrie Jun Cai. 2022b. AI 链：通过链式大语言模型提示实现透明和可控的人机交互。在
    *2022 年 CHI 人机交互会议论文集*。1–22。
- en: 'Zamfirescu-Pereira et al. (2023) J.D. Zamfirescu-Pereira, Richmond Y. Wong,
    Bjoern Hartmann, and Qian Yang. 2023. Why Johnny Can’t Prompt: How Non-AI Experts
    Try (and Fail) to Design LLM Prompts. In *Proceedings of the 2023 CHI Conference
    on Human Factors in Computing Systems* (Hamburg, Germany) *(CHI ’23)*. Association
    for Computing Machinery, New York, NY, USA, Article 437, 21 pages. [https://doi.org/10.1145/3544548.3581388](https://doi.org/10.1145/3544548.3581388)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zamfirescu-Pereira 等 (2023) J.D. Zamfirescu-Pereira, Richmond Y. Wong, Bjoern
    Hartmann, 和 Qian Yang. 2023. 为什么 Johnny 无法提示：非 AI 专家如何（且失败）设计 LLM 提示。在 *2023 年
    CHI 人机交互会议论文集*（汉堡，德国） *(CHI ’23)*. 计算机协会，纽约，NY，美国，文章 437，21 页。 [https://doi.org/10.1145/3544548.3581388](https://doi.org/10.1145/3544548.3581388)
- en: Zhou et al. (2022) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. 2022. Large language models are human-level
    prompt engineers. *arXiv preprint arXiv:2211.01910* (2022).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等 (2022) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, 和 Jimmy Ba. 2022. 大语言模型是人类级别的提示工程师。 *arXiv 预印本 arXiv:2211.01910*
    (2022).
- en: Appendix A List of Nodes
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 节点列表
- en: '| Node Name | Usage | Special Features |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 节点名称 | 用途 | 特殊功能 |'
- en: '| Inputs |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 输入 |'
- en: '| TextFields Node | Specify input values to a template variables in prompt
    or chat nodes. | Supports templating; can declare variables in brackets {} to
    chain inputs together. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 文本字段节点 | 指定输入值到提示或聊天节点的模板变量中。 | 支持模板化；可以在括号 {} 中声明变量以将输入串联在一起。 |'
- en: '| CSV Node | Specify input data as comma-separated values. Good for specifying
    many short values. | Brackets {} in data are escaped by default. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| CSV 节点 | 指定输入数据为以逗号分隔的值。适合指定许多短值。 | 数据中的括号 {} 默认转义。 |'
- en: '| Tabular Data Node | Import or create a spreadsheet of data to use as input
    to prompt or chat nodes. | Output values “carry together” when filling in multiple
    variables in a prompt template. Brackets {} in data are escaped by default. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 表格数据节点 | 导入或创建数据电子表格作为提示或聊天节点的输入。 | 当在提示模板中填写多个变量时，输出值“一起携带”。数据中的括号 {} 默认转义。
    |'
- en: '| Generators |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 生成器 |'
- en: '| Prompt Node | Prompt one or multiple LLMs. Declare template variables in
    {}s to attach input data. | Can chain together. Can set number of generations
    per prompt to greater than one. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 提示节点 | 提示一个或多个 LLM。声明 {} 中的模板变量以附加输入数据。 | 可以串联在一起。可以设置每个提示的生成次数大于一。 |'
- en: '| Chat Turn Node | Continue a turn of conversation with one or multiple chat
    LLMs. Supports templating of follow-up message. | Attach past prompt or chat output
    as context. Can also change what LLM(s) to use to continue conversation. |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 聊天轮次节点 | 与一个或多个聊天 LLM 继续对话轮次。支持后续消息的模板化。 | 附加过去的提示或聊天输出作为上下文。也可以更改使用的 LLM(s)
    以继续对话。 |'
- en: '| Evaluators |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 评估者 |'
- en: '| JavaScript Evaluator | Write a JavaScript function to ‘score’ a single response.
    Scores annotate responses. | Boolean ‘false’ values display in red in response
    inspector. *console.log()* prints to node. |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| JavaScript 评估器 | 编写 JavaScript 函数来‘评分’单个响应。评分注释响应。 | 布尔值‘false’在响应检查器中显示为红色。
    *console.log()* 打印到节点。 |'
- en: '| Python Evaluator | Same as JavaScript Evaluator but for Python. | Can import
    packages and use *print()*. |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Python 评估器 | 与 JavaScript 评估器相同，但用于 Python。 | 可以导入包并使用 *print()*。 |'
- en: '| LLM Scorer | Prompt an LLM to score responses. (GPT-4 at zero temperature
    is default.) | Unlike prompt chaining, this attaches the scores as annotations
    on existing responses. |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| LLM 评分器 | 提示 LLM 评分响应。（默认使用 GPT-4 的零温度设置。） | 与提示链不同，这会将评分附加为现有响应上的注释。 |'
- en: '| Simple Evaluator | Specify simple criteria to score responses as true if
    they meet the criteria. | Can test whether response *contains*, *starts with*,
    etc. a certain value. Can also compare against prompt variables or metavariables.
    |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 简单评估器 | 指定简单标准以评分响应，如果符合标准则为真。 | 可以测试响应是否 *包含*、*以某值开始* 等。也可以与提示变量或元变量进行比较。
    |'
- en: '| Visualizers |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 可视化工具 |'
- en: '| Vis Node | Plot evaluation scores. Currently only supports boolean and numeric
    scores. | Plots by LLM by default. Change y-axis to plot by different prompt variables.
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 视图节点 | 绘制评估评分。目前仅支持布尔和数值评分。 | 默认按 LLM 绘制。更改 y 轴以按不同的提示变量绘制。'
- en: '| Inspect Node | Inspect LLM responses like the pop-up inspector, only inside
    a flow. | Only supports Grouped List layout. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 检查节点 | 像弹出检查器一样检查LLM响应，只是在流程内部。 | 仅支持分组列表布局。 |'
- en: Table 3\. The nodes in ChainForge, grouped by type. A final node, the “Comment
    Node”, allows users to write comments.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. ChainForge 中的节点，按类型分组。一个最终节点，“评论节点”，允许用户写评论。
