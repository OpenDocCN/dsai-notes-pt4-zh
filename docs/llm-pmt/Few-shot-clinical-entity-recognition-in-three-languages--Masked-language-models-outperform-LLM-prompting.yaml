- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:46:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:46:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Few shot clinical entity recognition in three languages: Masked language models
    outperform LLM prompting'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 三种语言中的少样本临床实体识别：掩蔽语言模型优于LLM提示
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12801](https://ar5iv.labs.arxiv.org/html/2402.12801)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.12801](https://ar5iv.labs.arxiv.org/html/2402.12801)
- en: Marco Naguib Xavier Tannier Aurélie Névéol
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Marco Naguib Xavier Tannier Aurélie Névéol
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models are becoming the go-to solution for many natural language
    processing tasks, including in specialized domains where their few-shot capacities
    are expected to yield high performance in low-resource settings. Herein, we aim
    to assess the performance of Large Language Models for few shot clinical entity
    recognition in multiple languages. We evaluate named entity recognition in English,
    French and Spanish using 8 in-domain (clinical) and 6 out-domain gold standard
    corpora. We assess the performance of 10 auto-regressive language models using
    prompting and 16 masked language models used for text encoding in a biLSTM-CRF
    supervised tagger. We create a few-shot set-up by limiting the amount of annotated
    data available to 100 sentences. Our experiments show that although larger prompt-based
    models tend to achieve competitive F-measure for named entity recognition outside
    the clinical domain, this level of performance does not carry over to the clinical
    domain where lighter supervised taggers relying on masked language models perform
    better, even with the performance drop incurred from the few-shot set-up. In all
    experiments, the CO2 impact of masked language models is inferior to that of auto-regressive
    models. Results are consistent over the three languages and suggest that few-shot
    learning using Large language models is not production ready for named entity
    recognition in the clinical domain. Instead, models could be used for speeding-up
    the production of gold standard annotated data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型正成为许多自然语言处理任务的首选解决方案，包括在其少样本能力预期能在低资源环境中取得高性能的专业领域。本文旨在评估大型语言模型在多语言少样本临床实体识别中的表现。我们使用8个领域内（临床）和6个领域外的黄金标准语料库评估英语、法语和西班牙语中的命名实体识别。我们评估了10个自回归语言模型在提示下的表现和16个用于文本编码的掩蔽语言模型在biLSTM-CRF监督标注器中的表现。通过将可用的标注数据限制为100个句子，我们创建了一个少样本设置。实验结果表明，尽管较大的基于提示的模型在临床领域外的命名实体识别中往往能够取得竞争性的F-measure，这种性能水平在临床领域却没有延续到实际应用中，在少样本设置下，依赖于掩蔽语言模型的轻量级监督标注器表现更好。在所有实验中，掩蔽语言模型的CO2影响低于自回归模型。结果在三种语言中一致，表明使用大型语言模型的少样本学习尚未为临床领域的命名实体识别做好生产准备。相反，这些模型可以用于加速黄金标准标注数据的生产。
- en: 'keywords:'
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Deep Learning , Natural Language Processing , Named Entity Recognition , Few-shot
    Learning , Large Language Models^†^†journal: Journal of Artificial Intelligence
    in Medicine\affiliation'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，自然语言处理，命名实体识别，少样本学习，大型语言模型^†^†期刊：医学人工智能期刊\affiliation
- en: '[LISN]organization=LISN, CNRS, Université Paris-Saclay, addressline=1 Rue du
    Belvédère, postcode=91400, city=Orsay, country=France \affiliation[LIMICS]organization=LIMICS,
    Sorbonne Université, Inserm, Université Sorbonne Paris-Nord, addressline=15 rue
    de l’école de médecine, postcode=75005, city=Paris, country=France'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[LISN]组织=LISN, CNRS, 巴黎-萨克雷大学，地址：1 Rue du Belvédère，邮政编码：91400，城市：Orsay，国家：法国
    \affiliation[LIMICS]组织=LIMICS, 索邦大学, Inserm, 巴黎-索邦大学，地址：15 rue de l’école de médecine，邮政编码：75005，城市：巴黎，国家：法国'
- en: 1 INTRODUCTION
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Electronic Health Records (EHR) are rich sources of clinical information [[1](#bib.bib1)],
    which often appear in unstructured text only [[2](#bib.bib2)]. Efficiently extracting
    information from EHRs into a more structured form can help advance clinical research,
    public health surveillance and automatic clinical decision support [[3](#bib.bib3)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 电子健康记录（EHR）是丰富的临床信息来源[[1](#bib.bib1)]，这些信息通常仅以非结构化文本形式出现[[2](#bib.bib2)]。将EHR中的信息有效提取成更结构化的形式可以帮助推进临床研究、公共健康监测和自动临床决策支持[[3](#bib.bib3)]。
- en: 'Named Entity Recognition (NER) is a critical primary step in information extraction.
    It consists in identifying mentions of relevant entities in text. In the context
    of clinical information extraction from EHRs, these can be mentions of clinical
    entities such as disorders or drugs. Extracting these entities can particularly
    benefit concept normalization [[4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6)]
    as well as facilitate interpreting patient profiling and phenotyping [[7](#bib.bib7)].
    While general-domain NER (identifying entities such as persons and locations)
    has received much attention in the Natural Language Processing (NLP) community,
    clinical NER is widely considered as a harder problem : clinical entities are
    often jargon or ambiguous, and clinical texts have a nonstandard phrasal structure
    [[8](#bib.bib8), [9](#bib.bib9)].'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）是信息提取中的一个关键步骤。它包括在文本中识别相关实体的提及。在从EHR中提取临床信息的背景下，这些可以是临床实体如疾病或药物的提及。提取这些实体可以特别有助于概念规范化[[4](#bib.bib4),
    [5](#bib.bib5), [6](#bib.bib6)]，以及有助于解读患者档案和表型分析[[7](#bib.bib7)]。尽管通用领域NER（识别如人物和地点等实体）在自然语言处理（NLP）社区中受到广泛关注，但临床NER被普遍认为是一个更难的问题：临床实体通常是行话或模棱两可的，临床文本具有非标准的短语结构[[8](#bib.bib8),
    [9](#bib.bib9)]。
- en: Language Models have progressively become the main approach for tackling NER
    [[10](#bib.bib10), [11](#bib.bib11)]. Prior work has focused on general-domain
    NER [[12](#bib.bib12)] as well as clinical NER [[7](#bib.bib7), [13](#bib.bib13)].
    This work can be mainly divided into two approaches, depending on the type of
    language models used.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型逐渐成为解决NER的主要方法[[10](#bib.bib10), [11](#bib.bib11)]。此前的工作主要集中在通用领域NER[[12](#bib.bib12)]以及临床NER[[7](#bib.bib7),
    [13](#bib.bib13)]。这些工作可以主要分为两种方法，取决于使用的语言模型类型。
- en: The first approach is to use pre-trained Masked Language Models (MLM). This
    type of models is first pre-trained to predict randomly-selected masked words
    in large text corpora using a dense vector representation of every token (e.g.,
    word) in the text [[12](#bib.bib12), [14](#bib.bib14)]. Leveraging these models
    for NER usually involves training a linear projection to map vector representations
    into an NER tagging of the sentence, while jointly fine-tuning the parameters
    of the language model itself for the downstream task of NER. This approach has
    gained a lot of attention in the community since the release of the BERT architecture
    [[12](#bib.bib12)], and became the go-to solution for building robust NER systems.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是使用预训练的掩码语言模型（MLM）。这种模型首先在大规模文本语料库中预训练，以预测随机选择的掩码词，使用每个词（例如单词）的密集向量表示[[12](#bib.bib12),
    [14](#bib.bib14)]。利用这些模型进行NER通常涉及训练一个线性投影，将向量表示映射到句子的NER标注，同时联合微调语言模型本身的参数以适应NER的下游任务。自从BERT架构的发布以来，这种方法在社区中获得了大量关注[[12](#bib.bib12)]，并成为构建鲁棒NER系统的首选解决方案。
- en: However, when dealing with clinical NER, this approach encounters two problems.
    First, due to the sensitive nature of EHRs, public corpora are rare, restrictively
    licensed, with limited availability in languages other than English. This leads
    the community to solutions built on MLMs pre-trained mainly on general-domain
    corpora, and thus suffering from the domain shift problem. Second, for fine-tuning
    to be efficient, large corpora of in-domain annotated text is required [[15](#bib.bib15),
    [16](#bib.bib16)]. But clinical NER annotation campaigns are highly expensive
    and time-consuming due to the level of domain expertise needed to perform it [[8](#bib.bib8),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)]. Additionally, due to the
    diversity of clinical cases, data annotated for one biomedical application might
    not necessarily be helpful for another. Hence the need for data-efficient clinical
    NER, also known as few-shot NER.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在处理临床命名实体识别（NER）时，这种方法会遇到两个问题。首先，由于电子健康记录（EHR）的敏感性质，公共语料库很少，许可证限制严格，且除英语外其他语言的可用性有限。这导致社区转向基于主要在通用领域语料库上预训练的掩码语言模型（MLM）的解决方案，从而遭遇领域偏移问题。其次，为了使微调高效，需要大量的领域内标注文本语料库[[15](#bib.bib15),
    [16](#bib.bib16)]。但临床NER标注活动成本高昂且耗时，因为执行这些任务需要高度的领域专业知识[[8](#bib.bib8), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19)]。此外，由于临床病例的多样性，为一种生物医学应用标注的数据可能不一定对另一种应用有用。因此，需要数据高效的临床NER，也就是所谓的少样本NER。
- en: The second, novel, approach is to use pre-trained Causal Language Models (CLM).
    These substantially larger models are pre-trained on (often larger) corpora as
    generative, auto-regressive models. That is, the model is given a series of tokens
    or prompt as input and estimates the most probable following series of tokens.
    To leverage these language models for downstream tasks such as NER, one can formulate
    the task in natural language in a prompt. The prompt is formulated in such a way
    that the continuity of the text involves resolving the task. The language model
    is then used to predict this continuity. This process is often called "In-context
    learning" (ICL) [[20](#bib.bib20)]. Optionally, one can build a prompt featuring
    a few demonstrations of the task resolved for other instances (in this case, task-specific
    NER-labeled instances), along with the new test input [[21](#bib.bib21)]. The
    model thus outputs an estimation of the most probable NER tagging for the test
    input instance. While MLMs have been studied for few-shot NER [[22](#bib.bib22)],
    CLMs seem more naturally adapted to it. ICL learning has in fact proven particular
    success with LLMs in few-shot learning, showing state-of-the-art results in a
    wide set of NLP tasks [[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种新颖的方法是使用预训练的因果语言模型（CLM）。这些显著更大的模型作为生成型、自回归模型在（通常更大的）语料库上进行预训练。也就是说，模型接收一系列令牌或提示作为输入，并估计最可能的后续令牌序列。为了利用这些语言模型进行下游任务如
    NER，可以将任务用自然语言在提示中进行表述。提示的制定方式使得文本的连续性涉及到任务的解决。然后使用语言模型来预测这种连续性。这个过程通常被称为“上下文学习”（ICL）[[20](#bib.bib20)]。可以选择性地，构建一个提示，其中包含一些对其他实例（在这种情况下，为特定任务标记的
    NER 实例）解决的任务的演示，以及新的测试输入[[21](#bib.bib21)]。模型因此输出对测试输入实例的最可能的 NER 标记估计。虽然 MLMs
    已经被研究用于少样本 NER [[22](#bib.bib22)]，但 CLMs 似乎更自然地适应于此。ICL 学习实际上在少样本学习中对 LLMs 取得了特别成功，显示出一系列
    NLP 任务中的最先进结果[[23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)]。
- en: 'However, the superiority of CLMs over MLMs for NER is questionable. First,
    many efforts towards "few-shot learning" with CLMs design prompts based on their
    performance on large held-out validation datasets [[20](#bib.bib20), [26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)]. This is problematic because ICL is shown
    [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)] to depend greatly on the
    prompt structure : a small change in task phrasing, the examples presented, the
    order of examples, or the tagging format can affect the performance. Therefore,
    making these choices assuming large annotated validation dataset leads to results
    that are shown [[32](#bib.bib32)] to be over-optimistic and impossible to find
    in a real few-shot setting. Second, most of these studies have been mainly concentrated
    on the English language, and on GPT-based models [[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)]. This can lead to over-fitting the prompts
    for this language and this language model. Hence, we identify a need for a systematic,
    model-independent, study of the prompt building in the clinical context, and for
    languages other than English. The contributions of this work are as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，CLMs 在 NER 中相对于 MLMs 的优越性是值得怀疑的。首先，许多面向“少样本学习”的 CLMs 研究基于它们在大规模保留验证数据集上的表现来设计提示[[20](#bib.bib20),
    [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28)]。这有问题，因为 ICL 被证明[[29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31)] 极大地依赖于提示结构：任务措辞、小的变化、展示的示例、示例的顺序或标记格式都可能影响性能。因此，基于假设大规模标注验证数据集进行这些选择，结果往往被显示[[32](#bib.bib32)]
    为过于乐观，并且在实际的少样本设置中难以找到。其次，这些研究大多集中在英语语言和基于 GPT 的模型[[33](#bib.bib33), [34](#bib.bib34),
    [35](#bib.bib35), [36](#bib.bib36)]。这可能导致对这种语言和语言模型的提示过拟合。因此，我们识别出在临床背景下以及非英语语言中需要进行系统、模型无关的提示构建研究。本文的贡献如下：
- en: '1.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'We present and compare the most recent NER prompting techniques when applied
    to clinical NER in three languages : English, French and Spanish. To the best
    of our knowledge, this is the first work focused on NER prompts for languages
    other than English, and the first work comparing prompts for clinical NER.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示并比较了最新的 NER 提示技术在临床 NER 中应用于三种语言：英语、法语和西班牙语。我们认为，这是首次关注非英语语言的 NER 提示的研究，也是首次比较临床
    NER 提示的工作。
- en: '2.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We bring particular attention to tagging prompts, a novel NER prompting fashion,
    and measure the improvements brought by it.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们特别关注标记提示，这是一种新颖的 NER 提示方法，并测量其带来的改进。
- en: '3.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We offer a fair comparison with the best-performing MLMs in a few-shot setting
    across languages, models, and prompt-structures when applied to few-shot clinical
    NER.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在少样本临床命名实体识别的设置中，提供了对不同语言、模型和提示结构下表现最佳的MLMs的公平比较。
- en: '4.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We conduct easily reproducible experiments, using easy-to-implement methods,
    exclusively on publicly available datasets and publicly available language models.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行易于重复的实验，使用易于实现的方法，仅在公开可用的数据集和公开可用的语言模型上进行。
- en: 2 BACKGROUND AND SIGNIFICANCE
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与重要性
- en: 2.1 Few-shot NER with pre-trained Masked Language Models
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 使用预训练掩码语言模型的少样本命名实体识别
- en: The standard fashion of using MLMs for NER is as encoders. Usually, an NER tagging
    layer is trained from scratch to map the encoding of text into NER tagging of
    its tokens [[12](#bib.bib12)]. Other approaches have been proposed to leverage
    MLMs for few-shot learning. Namely, metric learning [[37](#bib.bib37), [38](#bib.bib38),
    [39](#bib.bib39)] proposes to train systems to instead learn a metric over the
    output space. New instances can then be classified based on the distance separating
    them from other labeled instances. Label encoding [[40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)] suggests, instead, to leverage label names or textual label
    descriptions and encode them along with the instances in order to better tag them.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MLMs进行NER的标准方式是作为编码器。通常，NER标注层从头开始训练，以将文本的编码映射到其标记的NER标注 [[12](#bib.bib12)]。还提出了其他方法来利用MLMs进行少样本学习。即，度量学习
    [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)] 提出训练系统来学习输出空间上的度量。新的实例可以基于它们与其他标记实例之间的距离来进行分类。标签编码
    [[40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42)] 则建议利用标签名称或文本标签描述，并将其与实例一起编码，以便更好地标记它们。
- en: 2.2 Few-shot NER with Causal Language Models
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 使用因果语言模型的少样本命名实体识别
- en: Recently, prompt construction has gained interest in the community [[20](#bib.bib20),
    [43](#bib.bib43)]. While most related work focused on studying prompt formulation
    and exploring better-performing prompt structures [[24](#bib.bib24), [34](#bib.bib34),
    [44](#bib.bib44), [33](#bib.bib33)] also known as "prompt engineering", other
    work proposed continuous optimization of the prompt through prompt tuning [[45](#bib.bib45),
    [46](#bib.bib46), [47](#bib.bib47)], usually reporting marginal improvements over
    baselines.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，提示构建在社区中引起了兴趣 [[20](#bib.bib20), [43](#bib.bib43)]。虽然大多数相关工作集中在研究提示的制定和探索更好的提示结构
    [[24](#bib.bib24), [34](#bib.bib34), [44](#bib.bib44), [33](#bib.bib33)]，也称为“提示工程”，但其他工作则提出了通过提示调整对提示进行连续优化
    [[45](#bib.bib45), [46](#bib.bib46), [47](#bib.bib47)]，通常报告与基线相比的边际改进。
- en: There is no standard, widely adopted manner of building NER prompts [[43](#bib.bib43)].
    In fact, NER associates to each instance a set of spans, each of which having
    a type. This structured nature of the prediction make it hard to find an intuitive
    but efficient manner to prompt a language model for NER, that adapts well to all
    contexts.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 目前没有标准的、广泛采用的NER提示构建方式 [[43](#bib.bib43)]。事实上，NER为每个实例关联一组跨度，每个跨度都有一个类型。这种预测的结构化性质使得很难找到一种直观但高效的方式来提示语言模型进行NER，这种方式能够很好地适应所有上下文。
- en: For instance, the main practice is to use separate prompts for different entity
    types [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)]. This choice seems
    well-suited when the task is interested in a handful of types of entities (typically
    5-10). When interested in less entity types, a single prompt can be used for detecting
    all entities [[34](#bib.bib34)]. On the other hand, if there is more entity types,
    it could be interesting to enumerate every possible span in the input sentence
    and let the model predict the entity type of the span, if any [[51](#bib.bib51)].
    This method, on the inverse, is impractical for long inputs.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，主要的做法是为不同的实体类型使用单独的提示 [[48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50)]。当任务只关注少数几种实体类型（通常为5-10种）时，这种选择似乎比较合适。当关注的实体类型较少时，可以使用一个提示来检测所有实体
    [[34](#bib.bib34)]。另一方面，如果实体类型较多，可以枚举输入句子中的每个可能跨度，并让模型预测该跨度的实体类型（如果有的话） [[51](#bib.bib51)]。这种方法在长输入中反而不切实际。
- en: 'We identify three families of manners to prompt LLMs :'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别出三种提示LLMs的方式：
- en: '1.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Constrained prompting attempts to better formulate the NER task by constraining
    the generation to fill in specific hand-crafted templates, usually adapted to
    MLMs [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53), [54](#bib.bib54)].
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 约束提示尝试通过约束生成以填充特定的手工制作模板，通常适应于MLMs [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53),
    [54](#bib.bib54)]，以更好地制定NER任务。
- en: '2.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: Listing prompts consist in simply making the language model predict the entities
    in a list [[34](#bib.bib34)].
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表提示仅仅是让语言模型预测列表中的实体 [[34](#bib.bib34)]。
- en: '3.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: Tagging prompts were studied more recently by [[33](#bib.bib33)]. They make
    the language model surround entity mentions with special tags.
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标注提示最近由 [[33](#bib.bib33)] 进行了研究。他们让语言模型将实体提及包围在特殊标签中。
- en: 2.3 Few-shot clinical NER
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 少量样本临床 NER
- en: MLM-based few-shot NER has also been explored in the biomedical domain [[55](#bib.bib55)].
    Metric leaning [[38](#bib.bib38)] and label encoding [[40](#bib.bib40), [41](#bib.bib41)]
    have been explored, as well as other approaches such as active learning [[56](#bib.bib56)],
    supervised pretraining [[57](#bib.bib57)] and MLM-based in-context learning [[21](#bib.bib21)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 MLM 的少量样本 NER 在生物医学领域也有探讨 [[55](#bib.bib55)]。探讨了度量学习 [[38](#bib.bib38)] 和标签编码
    [[40](#bib.bib40), [41](#bib.bib41)]，以及其他方法如主动学习 [[56](#bib.bib56)]、监督预训练 [[57](#bib.bib57)]
    和基于 MLM 的上下文学习 [[21](#bib.bib21)]。
- en: Few studies have focused on CLM-based few-shot clinical NER. In [[35](#bib.bib35)],
    GPT-3 and ChatGPT are evaluated on the 2010 i2b2/VA task [[58](#bib.bib58)] in
    a zero-shot context. In [[36](#bib.bib36)], GPT-3 is evaluated on a set of biomedical
    information extractions tasks including the NCBI-Disease [[18](#bib.bib18)]. Another
    interesting direction is partly fine-tuning [[59](#bib.bib59)] a general-domain
    CLM on clinical text [[60](#bib.bib60), [61](#bib.bib61)], and prompting the resulting
    CLM.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 少数研究专注于基于 CLM 的少量样本临床 NER。在 [[35](#bib.bib35)] 中，GPT-3 和 ChatGPT 在 2010 年 i2b2/VA
    任务 [[58](#bib.bib58)] 中以零样本上下文进行评估。在 [[36](#bib.bib36)] 中，GPT-3 在一组生物医学信息提取任务中进行评估，包括
    NCBI-Disease [[18](#bib.bib18)]。另一个有趣的方向是对通用领域 CLM 进行部分微调 [[59](#bib.bib59)] 以适应临床文本
    [[60](#bib.bib60), [61](#bib.bib61)]，并对结果 CLM 进行提示。
- en: 3 MATERIALS AND METHODS
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 材料与方法
- en: 3.1 Evaluation tasks
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 评估任务
- en: In order to evaluate the models. We use 14 publicly-available NER datasets,
    described as follows. For each study language, we selected two out-domain datasets
    and two or three in-domain datasets, aiming to use comparable resources (same
    genre, tagset, annotation guidelines) across languages whenever possible.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估模型，我们使用了 14 个公开可用的 NER 数据集，描述如下。对于每种研究语言，我们选择了两个领域外的数据集和两个或三个领域内的数据集，尽可能使用可比的资源（相同的类型、标签集、注释指南）。
- en: 3.1.1 General-domain evaluation datasets
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 通用领域评估数据集
- en: WikiNER [[62](#bib.bib62)] is a multilingual silver-standard annotated NER dataset.
    It consists in a late-2010 snapshot of Wikipedia in nine languages. Hyperlinks
    referring to persons, locations or organizations were automatically annotated.
    We use the English, French and Spanish versions of this dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: WikiNER [[62](#bib.bib62)] 是一个多语言银标注 NER 数据集。它由 2010 年底的九种语言的维基百科快照组成。自动注释了指向人物、地点或组织的超链接。我们使用了这个数据集的英语、法语和西班牙语版本。
- en: CoNLL-2002 [[63](#bib.bib63)] and CoNLL-2003 [[64](#bib.bib64)] are two manually-annotated
    multilingual NER dataset released as a part of CoNLL shared tasks. Mentions of
    persons, locations, organizations and miscellaneous entities are annotated. We
    use the Spanish data of the 2002 version, which is a collection of news wire articles
    made available by the Spanish EFE News Agency, released in May 2000\. We use the
    English data of the 2003 version, which consists of Reuters news stories between
    1996 and 1997.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: CoNLL-2002 [[63](#bib.bib63)] 和 CoNLL-2003 [[64](#bib.bib64)] 是两个手动注释的多语言 NER
    数据集，作为 CoNLL 共享任务的一部分发布。对人物、地点、组织和其他实体的提及进行了注释。我们使用 2002 版的西班牙语数据，这是西班牙 EFE 新闻社提供的一系列新闻线稿，发布于
    2000 年 5 月。我们使用 2003 版的英语数据，其中包含 1996 年至 1997 年间的路透社新闻故事。
- en: 'Quaero French Press [[65](#bib.bib65)] is a manually annotated corpus of about
    100 hours of speech transcribed from French speaking radio broadcast. This corpus
    was used in the 2011 Quaero named entity evaluation campaign. It comprises annotations
    for 5 entity types further divided into 32 subtypes. Our experiments relied on
    the five entity types: persons, locations, organizations, functions, and facilities.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Quaero French Press [[65](#bib.bib65)] 是一个手动注释的语料库，包含约 100 小时的法语广播转录语音。该语料库用于
    2011 年 Quaero 实体评估活动。它包括 5 种实体类型的注释，并进一步分为 32 个子类型。我们的实验依赖于这五种实体类型：**人物**、**地点**、**组织**、**职能**和**设施**。
- en: 3.1.2 Clinical evaluation datasets
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 临床评估数据集
- en: 'E3C [[66](#bib.bib66)] is a European multilingual corpus (Italian, English,
    French, Spanish, and Basque) of semantically annotated clinical narratives. The
    texts are collected from multiple publicly-available sources such as abstracts
    extracted from CC-licensed journals. We use the gold standard material available
    from the English, French and Spanish versions of this dataset. The clinical narratives
    are annotated with 6 entity types : actors, body parts, events, RMLs (measurements
    and test results) and clinical entities.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: E3C [[66](#bib.bib66)] 是一个包含意大利语、英语、法语、西班牙语和巴斯克语的欧洲多语种语料库，包含语义标注的临床叙述。这些文本收集自多个公开可用的来源，如从
    CC 许可期刊中提取的摘要。我们使用来自该数据集的英语、法语和西班牙语版本的黄金标准材料。临床叙述标注了 6 种实体类型：演员、身体部位、事件、RMLs（测量和测试结果）和临床实体。
- en: The n2c2-2019 [[8](#bib.bib8)] shared task focuses on medical concept normalization.
    It uses the MCN corpus developed by [[67](#bib.bib67)], often referred to as the
    n2c2-2019 dataset. It includes discharge summaries from the Partners HealthCare
    and Beth Israel Deaconess Medical Center. In order to convert the medical concept
    normalization task into an NER task, we use the annotated Concept Unique Identifiers
    (CUIs) to map each mention to the corresponding UMLS semantic group [[68](#bib.bib68),
    [69](#bib.bib69)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: n2c2-2019 [[8](#bib.bib8)] 共享任务专注于医学概念标准化。它使用由 [[67](#bib.bib67)] 开发的 MCN 语料库，通常称为
    n2c2-2019 数据集。该数据集包括来自 Partners HealthCare 和 Beth Israel Deaconess Medical Center
    的出院总结。为了将医学概念标准化任务转换为 NER 任务，我们使用带注释的概念唯一标识符 (CUIs) 将每个提及映射到相应的 UMLS 语义组 [[68](#bib.bib68),
    [69](#bib.bib69)]。
- en: 'The NCBI-Disease [[18](#bib.bib18)] corpus gathers 793 PubMed abstracts where
    mentions of diseases are annotated in four types depending on their syntax : Specific
    Diseases (e.g. diastrophic dysplasia), Disease Classes (e.g. an autosomal recessive
    disease), Composite Mentions (e.g. colorectal, endometrial, and ovarian cancers),
    and Modifiers (e.g. C7-deficient) .'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: NCBI-Disease [[18](#bib.bib18)] 语料库收集了 793 条 PubMed 摘要，其中疾病的提及根据其语法被标注为四种类型：特定疾病（例如，二萜畸形）、疾病类别（例如，常染色体隐性疾病）、复合提及（例如，结直肠、子宫内膜和卵巢癌）和修饰词（例如，C7
    缺乏）。
- en: QuaeroFrenchMed [[17](#bib.bib17)] consists of two text sources that we treat
    separately. The first part, EMEA is a collection of 13 patient information leaflets
    on marketed drugs from the European Medicines Agency (EMEA). The second part,
    MEDLINE, consist of 2,500 titles of research articles indexed in the MEDLINE database¹¹1[http://pubmed.ncbi.nlm.nih.gov/](http://pubmed.ncbi.nlm.nih.gov/).
    The two parts are annotated with 10 entity types corresponding to UMLS semantic
    groups.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: QuaeroFrenchMed [[17](#bib.bib17)] 包含两个我们分别处理的文本来源。第一部分，EMEA 是来自欧洲药品管理局 (EMEA)
    的 13 份药物市场信息手册的集合。第二部分，MEDLINE，包含了 2,500 条在 MEDLINE 数据库中索引的研究文章标题¹¹1[http://pubmed.ncbi.nlm.nih.gov/](http://pubmed.ncbi.nlm.nih.gov/)。这两个部分均标注了
    10 种对应于 UMLS 语义组的实体类型。
- en: 'The Chilean Waiting List [[19](#bib.bib19)] corpus consists of 900 de-identified
    referrals for several specialty consultations in Spanish from the waiting list
    in Chilean public hospitals, manually annotated with 10 entity types : abbreviations,
    body parts, clinical findings, diagnostic procedure, diseases, family members,
    laboratory or test results, laboratory procedures, medications, procedures, signs
    or symptoms and therapeutic procedures. It can be noted that these types can be
    redundant (e.g. all diagnostic procedures are also annotated as procedures).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 智利等待名单 [[19](#bib.bib19)] 语料库包含 900 条来自智利公立医院的西班牙语去标识化转诊，手动标注了 10 种实体类型：缩写、身体部位、临床发现、诊断程序、疾病、家庭成员、实验室或测试结果、实验室程序、药物、程序、体征或症状以及治疗程序。可以注意到，这些类型可能会有冗余（例如，所有诊断程序也被标注为程序）。
- en: 3.1.3 Few-shot learning set-up
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 少样本学习设置
- en: In order to study our models in a few-shot context, we simulate the few-shot
    context by only providing the models with a few annotated examples. These are
    all the annotated examples models are allowed to use in training, in prompting
    and in validation. In this study, we choose to mainly focus on $k=100$.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在少样本环境中研究我们的模型，我们通过仅提供少量带注释的示例来模拟少样本环境。这些是模型在训练、提示和验证中允许使用的所有注释示例。在这项研究中，我们选择主要关注
    $k=100$。
- en: Additionally, we test the best-performing models with a full train dataset for
    a skyline comparison.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们使用完整的训练数据集对表现最佳的模型进行全面比较。
- en: 3.2 Language Models
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 语言模型
- en: Table LABEL:tab:LM_features presents an overview of the language models used
    in our study. While French and Spanish are covered in many of the causal models,
    we can observe that English is ubiquitous. Except for mBERT and XLM-RoBERTa, masked
    language models cover only one of our study languages.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:tab:LM_features 展示了我们研究中使用的语言模型的概述。虽然许多因果模型涵盖了法语和西班牙语，但我们可以观察到英语的使用无处不在。除了
    mBERT 和 XLM-RoBERTa，掩码语言模型仅覆盖了我们研究中的一种语言。
- en: 3.3 NER with Masked Language Models
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 使用掩码语言模型进行的命名实体识别（NER）
- en: 'As mentioned in section [2.1](#S2.SS1 "2.1 Few-shot NER with pre-trained Masked
    Language Models ‣ 2 BACKGROUND AND SIGNIFICANCE ‣ Few shot clinical entity recognition
    in three languages: Masked language models outperform LLM prompting"), Masked
    Language Models have been adapted to few-shot learning in architectures suited
    for low-ressource contexts [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42)] However, in this work, we
    are interested in comparing the novel CLM approaches to the widespread, standard
    MLMs usage in without any further adaptation for few-shot learning. We use NLStruct
    [[95](#bib.bib95)], an open-source Python library that implements the standard
    approach described in section [2.1](#S2.SS1 "2.1 Few-shot NER with pre-trained
    Masked Language Models ‣ 2 BACKGROUND AND SIGNIFICANCE ‣ Few shot clinical entity
    recognition in three languages: Masked language models outperform LLM prompting").
    In addition, it processes nested entities, which are present in some or the study
    corpora. Instead of classifying the representation of every token separately into
    a BIO scheme, NLStruct classifies every representation span directly into entity
    types using a biLSTM-CRF tagger. We train the model for 20 epochs on 80% of the
    data and use the remaining held-out 20% for early stopping.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '正如 [2.1](#S2.SS1 "2.1 Few-shot NER with pre-trained Masked Language Models
    ‣ 2 BACKGROUND AND SIGNIFICANCE ‣ Few shot clinical entity recognition in three
    languages: Masked language models outperform LLM prompting") 节中提到的，掩码语言模型已被调整为适合低资源背景的少样本学习架构
    [[37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42)]。然而，在这项工作中，我们感兴趣的是将新颖的 CLM 方法与广泛使用的标准 MLM 用法进行比较，而不对少样本学习进行任何进一步的调整。我们使用
    NLStruct [[95](#bib.bib95)]，这是一个开源 Python 库，实施了 [2.1](#S2.SS1 "2.1 Few-shot NER
    with pre-trained Masked Language Models ‣ 2 BACKGROUND AND SIGNIFICANCE ‣ Few
    shot clinical entity recognition in three languages: Masked language models outperform
    LLM prompting") 节中描述的标准方法。此外，它处理嵌套实体，这些实体出现在某些或所有研究语料库中。NLStruct 不是将每个标记的表示单独分类为
    BIO 方案，而是使用 biLSTM-CRF 标注器将每个表示跨度直接分类为实体类型。我们在 80% 的数据上训练模型 20 个周期，并使用剩余的 20%
    数据进行早期停止。'
- en: 3.4 NER with Causal Language Models
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 使用因果语言模型进行的命名实体识别（NER）
- en: 'Our experiments prompt models to tag entities in the input sentence, instead
    of listing them. We discuss this choice in further detail in section [5.4.1](#S5.SS4.SSS1
    "5.4.1 Listing prompts ‣ 5.4 Ablation ‣ 5 DISCUSSION ‣ Few shot clinical entity
    recognition in three languages: Masked language models outperform LLM prompting").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的实验提示模型在输入句子中标记实体，而不是列出它们。我们将在 [5.4.1](#S5.SS4.SSS1 "5.4.1 Listing prompts
    ‣ 5.4 Ablation ‣ 5 DISCUSSION ‣ Few shot clinical entity recognition in three
    languages: Masked language models outperform LLM prompting") 节中进一步讨论这一选择。'
- en: 'The upper part of figure [1](#S3.F1 "Figure 1 ‣ 3.4 NER with Causal Language
    Models ‣ 3 MATERIALS AND METHODS ‣ Few shot clinical entity recognition in three
    languages: Masked language models outperform LLM prompting") shows a sample tagging
    prompt, highlighting sections in the prompt that guided use to design features
    for prompt phrasing. The 9 prompt phrasing features we considered are described
    below.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S3.F1 "Figure 1 ‣ 3.4 NER with Causal Language Models ‣ 3 MATERIALS
    AND METHODS ‣ Few shot clinical entity recognition in three languages: Masked
    language models outperform LLM prompting") 的上半部分展示了一个标记提示示例，突出显示了在提示中指导使用以设计提示短语特征的部分。我们考虑的
    9 个提示短语特征在下文中进行了描述。'
- en: '![Refer to caption](img/c0b49f474f4d3dc870175fca6ae9b622.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c0b49f474f4d3dc870175fca6ae9b622.png)'
- en: 'Figure 1: Example of a tagging prompt, used in the main experiment (top) and
    a self-verification prompt (bottom) for detecting DISO mentions in n2c2-2019'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：标记提示的示例，用于主要实验（上）和自我验证提示（下）以检测 n2c2-2019 中的 DISO 提及
- en: '1.'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Prompt language: By default, we prompt all language models in English, as it
    is the most ubiquitous language in all of their training corpora. This feature
    allows the model to be prompted in French or Spanish, to align the prompt language
    with that of the test sentence.'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示语言：默认情况下，我们用英语提示所有语言模型，因为它是所有训练语料库中最常见的语言。该特性允许模型以法语或西班牙语提示，以将提示语言与测试句子的语言对齐。
- en: '2.'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Additional sentences: By default, we present 5 annotated sentences in the prompts.
    This feature presents 5 additional sentences (i.e., 10 sentences in total).'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 附加句子：默认情况下，我们在提示中呈现5个注释句子。此功能呈现5个附加句子（即，总共10个句子）。
- en: '3.'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Self verification: By default, we follow [[33](#bib.bib33)] by selecting the
    5 closest sentences to the test sentence in terms of TF-IDF distance. The mentions
    tagged by the model are then considered to be the model’s final predictions. This
    feature selects instead the 5 sentences featuring the most entities of the target
    type and features them in an initial prompt. Intuitively, this prompt results
    in higher recall and lower precision. A second "self-verification" prompt is then
    used over the model’s initial predictions in order to filter out the false positives.
    A sample self-verification prompt is shown in the bottom part of figure [1](#S3.F1
    "Figure 1 ‣ 3.4 NER with Causal Language Models ‣ 3 MATERIALS AND METHODS ‣ Few
    shot clinical entity recognition in three languages: Masked language models outperform
    LLM prompting"). The number of demonstrations follows that of the main prompt.'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '自我验证：默认情况下，我们遵循[[33](#bib.bib33)]，通过选择与测试句子在TF-IDF距离上最接近的5个句子。然后将模型标记的提及视为模型的最终预测。此功能则选择包含目标类型最多实体的5个句子，并在初始提示中展示它们。直观地，这种提示会导致更高的召回率和较低的准确率。然后使用第二个“自我验证”提示来过滤假阳性。样本自我验证提示见图[1](#S3.F1
    "Figure 1 ‣ 3.4 NER with Causal Language Models ‣ 3 MATERIALS AND METHODS ‣ Few
    shot clinical entity recognition in three languages: Masked language models outperform
    LLM prompting")的下部。演示的数量与主提示相同。'
- en: '4.'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Taggers: By default, we follow [[33](#bib.bib33)] prompting the model to surround
    mentions with @@ and ##. This feature prompts it to surround mentions with quotes
    << and >> instead.'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标记器：默认情况下，我们遵循[[33](#bib.bib33)]，提示模型用@@和##围绕提及。此功能则提示模型用引号<>围绕提及。
- en: '5.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'Address a specialist in the prompt: By default, the first sentence is the task
    description shown in figure [1](#S3.F1 "Figure 1 ‣ 3.4 NER with Causal Language
    Models ‣ 3 MATERIALS AND METHODS ‣ Few shot clinical entity recognition in three
    languages: Masked language models outperform LLM prompting"). This feature starts
    the prompt with You are an excellent . You can identify all the mentions
    of  in a sentence, by putting them in a specific format. Here are
    some examples you can handle: instead. The  is a linguist or a clinician,
    following the task domain.'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在提示中称呼专家：默认情况下，第一句是任务描述，见图[1](#S3.F1 "Figure 1 ‣ 3.4 NER with Causal Language
    Models ‣ 3 MATERIALS AND METHODS ‣ Few shot clinical entity recognition in three
    languages: Masked language models outperform LLM prompting")。此功能以“你是一个优秀的。你可以识别句子中所有的提及，并以特定格式标出它们。以下是你可以处理的一些例子：”开始提示。
    是语言学家或临床专家，依据任务领域。'
- en: '6.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'Include label definitions in the prompt: This feature adds a one-sentence description
    for each entity type. Full entity descriptions used can be found in appendix 1.'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在提示中包含标签定义：此功能为每个实体类型添加一个句子的描述。完整的实体描述见附录1。
- en: '7.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '7.'
- en: 'Introductory sentence for the test instance: By default, the demonstrations
    are immediately followed by the test instance. This feature separates them with
    Identify all the mentions of  in the following sentence, by putting
     in front and a  behind each of them.'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 测试实例的引言句子：默认情况下，演示后面会紧跟测试实例。此功能通过在每个实体的前面加上，后面加上，来分隔它们。
- en: '8.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '8.'
- en: 'Require a long answer for the self-verification: By default, the self-verification
    prompt demonstrates Yes (respectively No) as answers. This feature demonstrates
     is a(n) , yes. (respectively  is not a(n) ,
    no.) instead.'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '对自我验证要求长回答：默认情况下，自我验证提示演示“是”（分别“否”）作为答案。此功能则演示是一个，是的。（分别不是一个，不是的。） '
- en: '9.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '9.'
- en: 'Dialogue template: This feature replaces the Input: and Output: in the prompt
    by dashes to imitate a dialog template.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对话模板：此功能将提示中的Input:和Output:替换为破折号，以模拟对话模板。
- en: ICL learning performance is shown to vary greatly depending on the exact phrasing
    of the prompt [[30](#bib.bib30), [31](#bib.bib31)]. In addition, the optimal choice
    for each of these features can vary depending on the model used. For instance,
    intuitively, models that are heavily pretrained on the English language tend to
    perform better with an English template than one in the language of the corpus.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ICL 学习性能被证明在提示的确切措辞下变化很大 [[30](#bib.bib30), [31](#bib.bib31)]。此外，每个特性的最佳选择可能会根据所使用的模型而有所不同。例如，直观上，那些在英语语言上经过大量预训练的模型，往往在使用英语模板时表现更佳，而不是在语料库语言中的模板。
- en: 'While our system aims to search for the best combination of parameters for
    each model, a grid search over them would require $2^{9}=512$ experiments for
    each model, for each dataset. In order to build a lighter system, we choose to
    perform a greedy search. We iterate over the features in this order, testing the
    non-default value, and keeping it if it performs better than the default. In section
    [5.4.3](#S5.SS4.SSS3 "5.4.3 Hyperparameter grid search ‣ 5.4 Ablation ‣ 5 DISCUSSION
    ‣ Few shot clinical entity recognition in three languages: Masked language models
    outperform LLM prompting"), we compare this approach to a grid search for one
    model over one dataset.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的系统旨在为每个模型寻找最佳参数组合，但对其进行网格搜索将需要 $2^{9}=512$ 次实验，每个模型，每个数据集。为了构建一个更轻量的系统，我们选择执行贪婪搜索。我们按照此顺序迭代特性，测试非默认值，如果其性能优于默认值，则保留。在第
    [5.4.3](#S5.SS4.SSS3 "5.4.3 超参数网格搜索 ‣ 5.4 消融 ‣ 5 讨论 ‣ 三种语言的少样本临床实体识别：掩码语言模型优于
    LLM 提示") 节中，我们将这种方法与一个模型在一个数据集上的网格搜索进行比较。
- en: Many efforts towards "few-shot learning" with CLMs optimize prompts on large
    held-out validation datasets [[20](#bib.bib20), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)]. This leads to results that are shown [[32](#bib.bib32)] to
    be over-optimistic. A fair comparison between MLMs and CLMs should compare them
    with access to the same (small) number of annotated instances, which corresponds
    to our $k=100$. In this no-training context, we follow [[32](#bib.bib32)] optimizing
    these features through a leave-one-out cross-validation (LOOCV).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 许多“少样本学习”努力通过 CLMs 在大型保留验证数据集上优化提示 [[20](#bib.bib20), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28)]。这导致的结果 [[32](#bib.bib32)] 被证明过于乐观。MLMs 和 CLMs 之间的公平比较应在相同（小）数量的注释实例下进行，这对应于我们的
    $k=100$。在这种无训练的背景下，我们遵循 [[32](#bib.bib32)] 通过留一交叉验证 (LOOCV) 来优化这些特性。
- en: 3.5 Measures
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 测量
- en: We assess the performance of models using F-measure and grams of CO2 emissions.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 F-measure 和 CO2 排放量来评估模型的性能。
- en: '1.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Micro-F1 for simplicity, we evaluate models over one global performance score.
    It is computed as the micro-average of F1-measures of the retrieval of each entity
    type.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为简便起见，我们通过一个全球性能得分来评估模型。它是每种实体类型检索的 F1-measure 的微平均值。
- en: '2.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: Carbon footprint we use GreenAlgorithms v2.2 [[96](#bib.bib96)] ²²2http://calculator.green-algorithms.org/
    to estimate the carbon footprint of each experiment, based on factors such as
    runtime, computing hardware and location where electricity used by our computer
    facility was produced.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 碳足迹我们使用 GreenAlgorithms v2.2 [[96](#bib.bib96)] ²²2http://calculator.green-algorithms.org/
    来估算每次实验的碳足迹，基于运行时间、计算硬件和我们计算机设施使用的电力生产地点等因素。
- en: 4 RESULTS
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: 4.1 Performance
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 性能
- en: 'Table LABEL:tab:results and figure [2](#S4.F2 "Figure 2 ‣ 4.2 Environmental
    Impact ‣ 4 RESULTS ‣ Few shot clinical entity recognition in three languages:
    Masked language models outperform LLM prompting") describe the performance of
    the tested models.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 LABEL:tab:results 和图 [2](#S4.F2 "图 2 ‣ 4.2 环境影响 ‣ 4 结果 ‣ 三种语言的少样本临床实体识别：掩码语言模型优于
    LLM 提示") 描述了测试模型的性能。
- en: 4.2 Environmental Impact
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 环境影响
- en: Appendix 2 details the carbon emission estimations for all of our experiments.
    In particular, we estimate the experiment using Mistral-7B over ConLL-2003 to
    have generated 41g of CO2 equivalent. (6g for prompt optimization and 35g for
    inference on the test set). LLaMA-2-70B, around 10 times larger, is estimated
    to have generated 191g of CO2 equivalent. (44g for prompt optimization and 147g
    for inference on the test set).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 附录 2 详细说明了我们所有实验的碳排放估算。特别是，我们估算 Mistral-7B 在 ConLL-2003 上的实验产生了 41 克 CO2 当量。（提示优化
    6 克，测试集推理 35 克）。LLaMA-2-70B，约大 10 倍，估算产生了 191 克 CO2 当量。（提示优化 44 克，测试集推理 147 克）。
- en: On the other hand, the experiment on a the BERT-large MLM is estimated to have
    generated 6g of CO2 equivalent. (2g for fine-tuning and training and 4g for inference
    on the test set).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对BERT-large MLM的实验估计生成了6克CO2当量（2克用于微调和训练，4克用于测试集上的推断）。
- en: In total, the experiments described in this paper are estimated to have generated
    around 27kg of CO2 equivalent (25kg for the main experiments, and 2kg for ablation).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，本文描述的实验估计产生了约27公斤的CO2当量（主要实验25公斤，消融实验2公斤）。
- en: '![Refer to caption](img/ce42b6332a8521a47f5a713037135559.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce42b6332a8521a47f5a713037135559.png)'
- en: (a) English
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 英语
- en: '![Refer to caption](img/8b1d246aeb6fe02d3d9cc0307d639484.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8b1d246aeb6fe02d3d9cc0307d639484.png)'
- en: (b) French
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 法语
- en: '![Refer to caption](img/be9c312c16d2f11db1d9e66b14b12030.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/be9c312c16d2f11db1d9e66b14b12030.png)'
- en: (c) Spanish
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 西班牙语
- en: 'Figure 2: General vs. Clinical performance of studied models'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：研究模型的通用与临床表现
- en: 5 DISCUSSION
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: 5.1 Comparison of model performance
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 模型性能比较
- en: Our experiments offer a comparative analysis of various masked and causal language
    models for named entity recognition. We further focus the scope to low resource
    settings commonly found in real-life biomedical applications. Our results show
    that, despite being smaller and theoretically requiring a larger amount of training
    data, masked, "BERT-like" models consistently outperform CLMs in this context.
    In addition, this performance comes at a much lower environmental impact (CO2
    emissions are 10-50 times lower for MLMs vs. CLMs).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验提供了针对命名实体识别的各种掩码和因果语言模型的比较分析。我们进一步将范围集中到现实生物医学应用中常见的低资源设置。我们的结果显示，尽管体积较小并且理论上需要更多的训练数据，掩码的“BERT-like”模型在这一背景下始终优于CLM。此外，这种性能的环境影响也要低得多（MLM的CO2排放量比CLM低10到50倍）。
- en: Another important finding is that, in addition to their higher scores, the different
    MLMs achieve results that are relatively close to each other. For example, on
    the WikiNER generalist task in English, the 4 general-domain models tested achieved
    F1-scores of between 0.768 and 0.79.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要发现是，除了得分较高之外，不同的MLM在结果上相对接近。例如，在英文的WikiNER通用任务中，测试的4个通用领域模型的F1得分在0.768到0.79之间。
- en: 'Besides, we show that the MLMs specialized in the biomedical field (ClinicalBERT,
    CamemBERT-bio, etc.), on the one hand, suffer a sharp drop in general domain tasks,
    illustrating the classical issue of "catastrophic forgetting"; and on the other
    hand, do not bring any significant improvement in specialized tasks, with the
    exception of Spanish tasks. This comment must, however, be balanced by the difference
    in size between the models : all the specialized models only have 110 million
    parameters.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们展示了在生物医学领域专门化的MLM（如ClinicalBERT、CamemBERT-bio等）在一方面，在通用领域任务中会出现显著下降，体现了经典的“灾难性遗忘”问题；另一方面，在专门任务中没有带来任何显著的改进，西班牙语任务除外。然而，这一评论必须考虑到模型大小的差异：所有专门化模型只有1.1亿个参数。
- en: Named entity recognition based on BERT-type representations has received a great
    deal of attention in recent years, and is undoubtedly more mature than the use
    of CLMs for this task. We have implemented the CLM-based NER techniques recently
    published in the literature, to the best of our knowledge. It is, of course, possible
    that new approaches will make it possible to increase performance in the future.
    However, this is arguably a difficult task for a generative model, as it is highly
    constrained in its syntax and its evaluation. These results are no indication
    of performance on other tasks such as classification.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于BERT类型表示的命名实体识别近年来受到广泛关注，无疑比使用CLM的成熟度更高。我们实现了在文献中最近发布的基于CLM的NER技术，尽我们所知。当然，未来可能会有新的方法提高性能。然而，这对于生成模型来说无疑是一个困难的任务，因为它在语法和评估方面受到高度限制。这些结果并不能说明在其他任务如分类上的表现。
- en: 5.2 Practical use of language models for low-resource NER
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 低资源NER的语言模型实际应用
- en: Overall, our experiments suggest that the performance of language models for
    clinical named entity recognition is currently sub-optimal. In particular, even
    MLM-based models fail to approach the performance of fully supervised models.
    The three large models trained with the entirety of each training dataset (*skylines*
    Table LABEL:tab:results) systematically outperform the best few-shot results,
    by between 5% and 16% for the general domain, and between 8% and 48% for the biomedical
    domain. However, performance can be judged satisfactory enough for pre-annotation
    use, to complement or accelerate manual annotation, for example in an online or
    active learning context.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的实验表明，语言模型在临床命名实体识别中的表现目前仍不尽如人意。特别是，即便是基于MLM的模型也未能接近完全监督模型的表现。用完整的每个训练数据集训练的三大模型（*skylines*
    表LABEL:tab:results）系统地超越了最佳的少样本结果，在通用领域的提升幅度为5%到16%，在生物医学领域的提升幅度为8%到48%。然而，表现可以被认为足够令人满意，用于预注释，以补充或加速人工注释，例如在在线或主动学习的背景下。
- en: 5.3 Limitations of our study
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 我们研究的局限性
- en: Random Noise
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随机噪声
- en: In MLM experiments, the parameters of the NER tagging layer added on top of
    the pretrained language model are initialized randomly. Likewise, in CLM experiments,
    the demonstrations in the prompts are shuffled randomly, and the negative examples
    in the self verification prompts are selected randomly. These random decisions
    can introduce noise in our performance measurements. Replicating all the experiments
    would allow us to draw more solid conclusions [[97](#bib.bib97)], but would also
    come at a considerable cost (25kg of CO2 equivalent, and around 56 hours of computation
    for each replication). The large number of models tested and tasks addressed can
    however comfort the main observations of this article. For instance, we use Almost
    Stochastic Order (ASO) ³³3Given the performance scores of two algorithms A and
    B, each of which run several times with different settings, ASO computes a test-specific
    value ($\epsilon_{\_}{min}$=0) for all clinical datasets.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLM实验中，添加到预训练语言模型顶部的NER标记层的参数是随机初始化的。同样，在CLM实验中，提示中的演示被随机打乱，自我验证提示中的负面示例也是随机选择的。这些随机决定可能在我们的性能测量中引入噪声。复制所有实验将使我们能够得出更有力的结论[[97](#bib.bib97)]，但也将付出相当大的成本（25公斤CO2当量，约56小时的计算时间用于每次复制）。尽管如此，测试的大量模型和涉及的任务可以使我们对本文的主要观察结果感到安慰。例如，我们使用几乎随机顺序（ASO）³³给定两个算法A和B的性能得分，每个算法在不同设置下运行几次，ASO计算一个特定于测试的值（$\epsilon_{\_}{min}$=0）用于所有临床数据集。
- en: Data contamination.
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据污染。
- en: The size of the training corpora used for creating LLMs makes it increasingly
    difficult to control for data contamination, i.e. the presence of test corpora.
    The community is calling for efforts towards better documentation of training
    datasets [[99](#bib.bib99)]. While some datasets are by construction incompatible
    with some models (e.g., there is no Spanish training corpus in GPT-J or LLAMA-2)
    we are unable to affirm full exclusion of all datasets from all models studied.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建LLMs的训练语料库的规模使得控制数据污染（即测试语料库的存在）变得越来越困难。社区呼吁对训练数据集进行更好的文档记录[[99](#bib.bib99)]。虽然某些数据集由于构造上的原因与某些模型不兼容（例如，GPT-J或LLAMA-2中没有西班牙语训练语料库），但我们无法确认所有数据集完全排除在所有研究的模型之外。
- en: 5.4 Ablation
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融
- en: To better understand the contribution of each step of our approach, we carried
    out a series of complementary experiments.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们方法中每一步的贡献，我们进行了系列的补充实验。
- en: 5.4.1 Listing prompts
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1 列表提示
- en: 'In this section, we compare the adopted tagging prompts to listing prompts.
    In listing prompts, demonstrations simply list the tagged mentions. The list separator
    is optimized (in the same way as the taggers) between a comma and a newline character.
    Eventually, the introductory sentences asks to list entities. The results shown
    in table [1](#S5.T1 "Table 1 ‣ 5.4.1 Listing prompts ‣ 5.4 Ablation ‣ 5 DISCUSSION
    ‣ Few shot clinical entity recognition in three languages: Masked language models
    outperform LLM prompting") further corroborate our choice of only focusing on
    tagging prompts.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将采用的标记提示与列表提示进行比较。在列表提示中，演示只是列出标记的提及。列表分隔符在逗号和换行符之间进行优化（与标记器相同）。最终，介绍句要求列出实体。表[1](#S5.T1
    "表 1 ‣ 5.4.1 列表提示 ‣ 5.4 消融 ‣ 5 讨论 ‣ 三种语言中的少样本临床实体识别：掩蔽语言模型优于LLM提示")中显示的结果进一步证实了我们只专注于标记提示的选择。
- en: English French Spanish Model WikiNER CoNLL2003 E3C n2c2 NCBI WikiNER QFP E3C
    EMEA MEDLINE WikiNER CoNLL2002 E3C CWL Listing prompts Mistral-7B 0.659 0.533
    0.417 0.281 0.340 0.676 0.083 0.451 0.169 0.403 0.697 0.620 0.211 0.273 Tagging
    prompts Mistral-7B 0.754 0.646 0.488 0.291 0.395 0.727 0.428 0.590 0.229 0.333
    0.720 0.707 0.083 0.374
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: English French Spanish Model WikiNER CoNLL2003 E3C n2c2 NCBI WikiNER QFP E3C
    EMEA MEDLINE WikiNER CoNLL2002 E3C CWL Listing prompts Mistral-7B 0.659 0.533
    0.417 0.281 0.340 0.676 0.083 0.451 0.169 0.403 0.697 0.620 0.211 0.273 Tagging
    prompts Mistral-7B 0.754 0.646 0.488 0.291 0.395 0.727 0.428 0.590 0.229 0.333
    0.720 0.707 0.083 0.374
- en: 'Table 1: F1 scores obtained with the listing and tagging prompts.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：使用列出和标记提示获得的F1分数。
- en: 5.4.2 Sample and sample size
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2 样本和样本大小
- en: 'We tested our approach with different samples and different sample sizes for
    one MLM : XLM-RoBERTa-large, and one CLM : Mistral-7B. The results are reported
    in table [2](#S5.T2 "Table 2 ‣ 5.4.2 Sample and sample size ‣ 5.4 Ablation ‣ 5
    DISCUSSION ‣ Few shot clinical entity recognition in three languages: Masked language
    models outperform LLM prompting"). It can be noted that, whereas the standard
    deviation with respect to $p$ is rather high, a significant difference can still
    be consistently observed between the two models across samples of the same size.
    We also observe that, as the number of annotated instances decreases, the performance
    of the MLM drops faster than that of the CLM.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对一种MLM（XLM-RoBERTa-large）和一种CLM（Mistral-7B）进行了不同样本和样本大小的测试。结果见表[2](#S5.T2
    "Table 2 ‣ 5.4.2 Sample and sample size ‣ 5.4 Ablation ‣ 5 DISCUSSION ‣ Few shot
    clinical entity recognition in three languages: Masked language models outperform
    LLM prompting")。可以注意到，尽管与$p$相关的标准差相对较高，但在相同大小的样本之间，两个模型之间的显著差异仍然可以一致地观察到。我们还观察到，随着标注实例数量的减少，MLM的性能下降比CLM更快。'
- en: '|  | CoNLL2003 | n2c2 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | CoNLL2003 | n2c2 |'
- en: '| 100 annotated instances |  |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 100 annotated instances |  |  |'
- en: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
- en: '| Mistral-7B | 0.646 | 0.626 | 0.714 | 0.291 | 0.178 | 0.215 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 0.646 | 0.626 | 0.714 | 0.291 | 0.178 | 0.215 |'
- en: '| XLM-R-large | 0.826 | 0.814 | 0.786 | 0.462 | 0.478 | 0.526 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| XLM-R-large | 0.826 | 0.814 | 0.786 | 0.462 | 0.478 | 0.526 |'
- en: '| 50 annotated instances |  |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 50 annotated instances |  |  |'
- en: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
- en: '| Mistral-7B | 0.615 | 0.648 | 0.637 | 0.278 | 0.176 | 0.106 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 0.615 | 0.648 | 0.637 | 0.278 | 0.176 | 0.106 |'
- en: '| XLM-R-large | 0.697 | 0.77 | 0.714 | 0.431 | 0.476 | 0.35 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| XLM-R-large | 0.697 | 0.77 | 0.714 | 0.431 | 0.476 | 0.35 |'
- en: '| 25 annotated instances |  |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 25 annotated instances |  |  |'
- en: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | p=1 | p=2 | p=3 | p=1 | p=2 | p=3 |'
- en: '| Mistral-7B | 0.509 | 0.599 | 0.52 | 0.152 | 0.252 | 0.116 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | 0.509 | 0.599 | 0.52 | 0.152 | 0.252 | 0.116 |'
- en: '| XLM-R-large | 0.487 | 0.588 | 0.637 | 0.393 | 0.361 | 0.283 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| XLM-R-large | 0.487 | 0.588 | 0.637 | 0.393 | 0.361 | 0.283 |'
- en: 'Table 2: F1 scores obtained over experiments with different training samples
    and different training sample sizes.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：在不同训练样本和不同训练样本大小下获得的F1分数。
- en: 5.4.3 Hyperparameter grid search
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3 超参数网格搜索
- en: 'In order to asses the quality of our adopted search method used to find the
    best feature combination to incorporate in the prompt, we compare this method
    to a naïve grid search over these features. We test all 512 combinations of our
    identified 9 features, for Mistral-7B over ConLL2003\. The scores found through
    LOOCV vary between 0.0 and 0.656 with a mean value of 0.387 and a median of 0.46\.
    The best-preforming combination is : Additional sentences, Self-verification,
    Introductory sentence for the test instance and Require a long answer for the
    self-verification, which is exactly the same combination we found initially through
    a greedy, tree search, that is around 20 times faster and less consuming.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们采用的搜索方法在找到最佳特征组合以纳入提示中的质量，我们将该方法与对这些特征进行的天真的网格搜索进行了比较。我们测试了所有512种由9个特征组成的组合，针对Mistral-7B在ConLL2003上的表现。通过LOOCV得到的分数在0.0到0.656之间变化，平均值为0.387，中位数为0.46。表现最好的组合是：附加句子、自我验证、测试实例的引言句子和要求长回答的自我验证，这正是我们最初通过贪婪树搜索找到的组合，该方法速度快约20倍，消耗更少。
- en: 6 CONCLUSION
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This study assessed the performance of two types of large languages models,
    for few-shot entity recognition in three languages. Our experiments show that
    few-shot learning performance is significantly lower in the clinical vs. general
    domain. While masked language models perform better than causal language models
    (higher F1, lower CO2 emissions), few-shot use should be limited to assisting
    gold standard annotation rather than effective information extraction.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究评估了两种大型语言模型在三种语言中的少量样本实体识别性能。我们的实验显示，临床领域的少量样本学习性能显著低于一般领域。尽管掩码语言模型的表现优于因果语言模型（具有更高的F1值和更低的CO2排放），但少量样本的使用应限于辅助金标准注释，而非有效的信息提取。
- en: ACKNOWLEDGEMENT
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was performed using HPC resources from GENCI-IDRIS (Grant 2023-AD011014533).
    The authors thank Dr. Juan Manual Coria for his help phrasing prompts in Spanish.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作使用了GENCI-IDRIS的HPC资源（资助编号2023-AD011014533）。作者感谢Juan Manual Coria博士在西班牙语提示词编写方面的帮助。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] D. Demner-Fushman, W. W. Chapman, C. J. McDonald, [What can natural language
    processing do for clinical decision support?](https://www.sciencedirect.com/science/article/pii/S1532046409001087),
    Journal of Biomedical Informatics 42 (5) (2009) 760–772, biomedical Natural Language
    Processing. [doi:https://doi.org/10.1016/j.jbi.2009.08.007](https://doi.org/https://doi.org/10.1016/j.jbi.2009.08.007).'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] D. Demner-Fushman, W. W. Chapman, C. J. McDonald, [自然语言处理对临床决策支持的作用是什么？](https://www.sciencedirect.com/science/article/pii/S1532046409001087)，《生物医学信息学杂志》42
    (5) (2009) 760–772，生物医学自然语言处理。 [doi:https://doi.org/10.1016/j.jbi.2009.08.007](https://doi.org/https://doi.org/10.1016/j.jbi.2009.08.007)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046409001087](https://www.sciencedirect.com/science/article/pii/S1532046409001087)
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S1532046409001087](https://www.sciencedirect.com/science/article/pii/S1532046409001087)
- en: '[2] J.-B. Escudié, B. Rance, G. Malamut, S. Khater, A. Burgun, C. Cellier,
    A.-S. Jannot, A novel data-driven workflow combining literature and electronic
    health records to estimate comorbidities burden for a specific disease: a case
    study on autoimmune comorbidities in patients with celiac disease, BMC medical
    informatics and decision making 17 (1) (2017) 1–10.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] J.-B. Escudié, B. Rance, G. Malamut, S. Khater, A. Burgun, C. Cellier,
    A.-S. Jannot, 一种结合文献和电子健康记录的数据驱动工作流程，用于估计特定疾病的共病负担：以乳糜泻患者的自身免疫共病为例，BMC医学信息学与决策制定
    17 (1) (2017) 1–10。'
- en: '[3] Y. Wang, L. Wang, M. Rastegar-Mojarad, S. Moon, F. Shen, N. Afzal, S. Liu,
    Y. Zeng, S. Mehrabi, S. Sohn, H. Liu, [Clinical information extraction applications:
    A literature review](https://www.sciencedirect.com/science/article/pii/S1532046417302563),
    Journal of Biomedical Informatics 77 (2018) 34–49. [doi:https://doi.org/10.1016/j.jbi.2017.11.011](https://doi.org/https://doi.org/10.1016/j.jbi.2017.11.011).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Y. Wang, L. Wang, M. Rastegar-Mojarad, S. Moon, F. Shen, N. Afzal, S. Liu,
    Y. Zeng, S. Mehrabi, S. Sohn, H. Liu, [临床信息提取应用：文献综述](https://www.sciencedirect.com/science/article/pii/S1532046417302563)，《生物医学信息学杂志》77
    (2018) 34–49. [doi:https://doi.org/10.1016/j.jbi.2017.11.011](https://doi.org/https://doi.org/10.1016/j.jbi.2017.11.011)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046417302563](https://www.sciencedirect.com/science/article/pii/S1532046417302563)
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S1532046417302563](https://www.sciencedirect.com/science/article/pii/S1532046417302563)
- en: '[4] H. Cho, W. Choi, H. Lee, A method for named entity normalization in biomedical
    articles: Application to diseases and plants, BMC Bioinformatics 18 (10 2017).
    [doi:10.1186/s12859-017-1857-8](https://doi.org/10.1186/s12859-017-1857-8).'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] H. Cho, W. Choi, H. Lee, 一种用于生物医学文章中命名实体规范化的方法：应用于疾病和植物，BMC生物信息学 18 (10
    2017)。 [doi:10.1186/s12859-017-1857-8](https://doi.org/10.1186/s12859-017-1857-8)。'
- en: '[5] P. Wajsbürt, A. Sarfati, X. Tannier, [Medical concept normalization in
    french using multilingual terminologies and contextual embeddings](https://www.sciencedirect.com/science/article/pii/S1532046421000137),
    Journal of Biomedical Informatics 114 (2021) 103684. [doi:https://doi.org/10.1016/j.jbi.2021.103684](https://doi.org/https://doi.org/10.1016/j.jbi.2021.103684).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] P. Wajsbürt, A. Sarfati, X. Tannier, [使用多语言术语和上下文嵌入进行法语医学概念规范化](https://www.sciencedirect.com/science/article/pii/S1532046421000137)，《生物医学信息学杂志》114
    (2021) 103684. [doi:https://doi.org/10.1016/j.jbi.2021.103684](https://doi.org/https://doi.org/10.1016/j.jbi.2021.103684)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046421000137](https://www.sciencedirect.com/science/article/pii/S1532046421000137)
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S1532046421000137](https://www.sciencedirect.com/science/article/pii/S1532046421000137)
- en: '[6] M. Sung, M. Jeong, Y. Choi, D. Kim, J. Lee, J. Kang, [BERN2: an advanced
    neural biomedical named entity recognition and normalization tool](https://doi.org/10.1093/bioinformatics/btac598),
    Bioinformatics 38 (20) (2022) 4837–4839. [arXiv:https://academic.oup.com/bioinformatics/article-pdf/38/20/4837/46535173/btac598.pdf](http://arxiv.org/abs/https://academic.oup.com/bioinformatics/article-pdf/38/20/4837/46535173/btac598.pdf),
    [doi:10.1093/bioinformatics/btac598](https://doi.org/10.1093/bioinformatics/btac598).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Sung, M. Jeong, Y. Choi, D. Kim, J. Lee, J. Kang, [BERN2：一种先进的神经生物医学命名实体识别与标准化工具](https://doi.org/10.1093/bioinformatics/btac598)，《生物信息学》38
    (20) (2022) 4837–4839。 [arXiv:https://academic.oup.com/bioinformatics/article-pdf/38/20/4837/46535173/btac598.pdf](http://arxiv.org/abs/https://academic.oup.com/bioinformatics/article-pdf/38/20/4837/46535173/btac598.pdf)，[doi:10.1093/bioinformatics/btac598](https://doi.org/10.1093/bioinformatics/btac598)。'
- en: URL [https://doi.org/10.1093/bioinformatics/btac598](https://doi.org/10.1093/bioinformatics/btac598)
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://doi.org/10.1093/bioinformatics/btac598](https://doi.org/10.1093/bioinformatics/btac598)
- en: '[7] C. Gérardin, P. Wajsbürt, P. Vaillant, A. Bellamine, F. Carrat, X. Tannier,
    [Multilabel classification of medical concepts for patient clinical profile identification](https://www.sciencedirect.com/science/article/pii/S0933365722000768),
    Artificial Intelligence in Medicine 128 (2022) 102311. [doi:https://doi.org/10.1016/j.artmed.2022.102311](https://doi.org/https://doi.org/10.1016/j.artmed.2022.102311).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Gérardin, P. Wajsbürt, P. Vaillant, A. Bellamine, F. Carrat, X. Tannier,
    [用于患者临床档案识别的多标签医学概念分类](https://www.sciencedirect.com/science/article/pii/S0933365722000768)，《医学人工智能》128
    (2022) 102311。 [doi:https://doi.org/10.1016/j.artmed.2022.102311](https://doi.org/https://doi.org/10.1016/j.artmed.2022.102311)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S0933365722000768](https://www.sciencedirect.com/science/article/pii/S0933365722000768)
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S0933365722000768](https://www.sciencedirect.com/science/article/pii/S0933365722000768)
- en: '[8] Y.-F. Luo, S. Henry, Y. Wang, F. Shen, O. Uzuner, A. Rumshisky, [The 2019
    n2c2/UMass Lowell shared task on clinical concept normalization](https://doi.org/10.1093/jamia/ocaa106),
    Journal of the American Medical Informatics Association 27 (10) (2020) 1529–e1.
    [arXiv:https://academic.oup.com/jamia/article-pdf/27/10/1529/39739985/ocaa106.pdf](http://arxiv.org/abs/https://academic.oup.com/jamia/article-pdf/27/10/1529/39739985/ocaa106.pdf),
    [doi:10.1093/jamia/ocaa106](https://doi.org/10.1093/jamia/ocaa106).'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y.-F. Luo, S. Henry, Y. Wang, F. Shen, O. Uzuner, A. Rumshisky, [2019年n2c2/UMass
    Lowell共享任务：临床概念标准化](https://doi.org/10.1093/jamia/ocaa106)，《美国医学信息学协会杂志》27 (10)
    (2020) 1529–e1。 [arXiv:https://academic.oup.com/jamia/article-pdf/27/10/1529/39739985/ocaa106.pdf](http://arxiv.org/abs/https://academic.oup.com/jamia/article-pdf/27/10/1529/39739985/ocaa106.pdf)，[doi:10.1093/jamia/ocaa106](https://doi.org/10.1093/jamia/ocaa106)。'
- en: URL [https://doi.org/10.1093/jamia/ocaa106](https://doi.org/10.1093/jamia/ocaa106)
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://doi.org/10.1093/jamia/ocaa106](https://doi.org/10.1093/jamia/ocaa106)
- en: '[9] R. Leaman, R. Khare, Z. Lu, [Challenges in clinical natural language processing
    for automated disorder normalization](https://www.sciencedirect.com/science/article/pii/S1532046415001501),
    Journal of Biomedical Informatics 57 (2015) 28–37. [doi:https://doi.org/10.1016/j.jbi.2015.07.010](https://doi.org/https://doi.org/10.1016/j.jbi.2015.07.010).'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] R. Leaman, R. Khare, Z. Lu, [临床自然语言处理中的自动化疾病标准化挑战](https://www.sciencedirect.com/science/article/pii/S1532046415001501)，《生物医学信息学杂志》57
    (2015) 28–37。 [doi:https://doi.org/10.1016/j.jbi.2015.07.010](https://doi.org/https://doi.org/10.1016/j.jbi.2015.07.010)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046415001501](https://www.sciencedirect.com/science/article/pii/S1532046415001501)
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S1532046415001501](https://www.sciencedirect.com/science/article/pii/S1532046415001501)
- en: '[10] J. Li, A. Sun, J. Han, C. Li, [A survey on deep learning for named entity
    recognition](https://doi.org/10.1109/TKDE.2020.2981314), IEEE Trans. on Knowl.
    and Data Eng. 34 (1) (2022) 50–70. [doi:10.1109/TKDE.2020.2981314](https://doi.org/10.1109/TKDE.2020.2981314).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Li, A. Sun, J. Han, C. Li, [用于命名实体识别的深度学习调查](https://doi.org/10.1109/TKDE.2020.2981314)，《IEEE
    知识与数据工程学报》34 (1) (2022) 50–70。 [doi:10.1109/TKDE.2020.2981314](https://doi.org/10.1109/TKDE.2020.2981314)。'
- en: URL [https://doi.org/10.1109/TKDE.2020.2981314](https://doi.org/10.1109/TKDE.2020.2981314)
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://doi.org/10.1109/TKDE.2020.2981314](https://doi.org/10.1109/TKDE.2020.2981314)
- en: '[11] Y. Wang, H. Tong, Z. Zhu, Y. Li, [Nested named entity recognition: A survey](https://doi.org/10.1145/3522593),
    ACM Trans. Knowl. Discov. Data 16 (6) (jul 2022). [doi:10.1145/3522593](https://doi.org/10.1145/3522593).'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y. Wang, H. Tong, Z. Zhu, Y. Li, [嵌套命名实体识别：一项调查](https://doi.org/10.1145/3522593)，《ACM
    知识发现与数据学报》16 (6) (2022年7月)。 [doi:10.1145/3522593](https://doi.org/10.1145/3522593)。'
- en: URL [https://doi.org/10.1145/3522593](https://doi.org/10.1145/3522593)
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://doi.org/10.1145/3522593](https://doi.org/10.1145/3522593)
- en: '[12] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, [Bert: Pre-training of deep
    bidirectional transformers for language understanding](https://api.semanticscholar.org/CorpusID:52967399),
    in: North American Chapter of the Association for Computational Linguistics, 2019.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, [Bert: 深度双向变换器的预训练用于语言理解](https://api.semanticscholar.org/CorpusID:52967399)，收录于：北美计算语言学协会年会，2019年。'
- en: URL [https://api.semanticscholar.org/CorpusID:52967399](https://api.semanticscholar.org/CorpusID:52967399)
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网址 [https://api.semanticscholar.org/CorpusID:52967399](https://api.semanticscholar.org/CorpusID:52967399)
- en: '[13] C. Sun, Z. Yang, L. Wang, Y. Zhang, H. Lin, J. Wang, [Biomedical named
    entity recognition using bert in the machine reading comprehension framework](https://www.sciencedirect.com/science/article/pii/S1532046421001283),
    Journal of Biomedical Informatics 118 (2021) 103799. [doi:https://doi.org/10.1016/j.jbi.2021.103799](https://doi.org/https://doi.org/10.1016/j.jbi.2021.103799).'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] C. Sun, Z. Yang, L. Wang, Y. Zhang, H. Lin, J. Wang, [在机器阅读理解框架中使用bert进行生物医学命名实体识别](https://www.sciencedirect.com/science/article/pii/S1532046421001283)，《生物医学信息学杂志》118
    (2021) 103799。[doi:https://doi.org/10.1016/j.jbi.2021.103799](https://doi.org/https://doi.org/10.1016/j.jbi.2021.103799)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046421001283](https://www.sciencedirect.com/science/article/pii/S1532046421001283)
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网址 [https://www.sciencedirect.com/science/article/pii/S1532046421001283](https://www.sciencedirect.com/science/article/pii/S1532046421001283)
- en: '[14] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettlemoyer,
    [Deep contextualized word representations](https://aclanthology.org/N18-1202),
    in: M. Walker, H. Ji, A. Stent (Eds.), Proceedings of the 2018 Conference of the
    North American Chapter of the Association for Computational Linguistics: Human
    Language Technologies, Volume 1 (Long Papers), Association for Computational Linguistics,
    New Orleans, Louisiana, 2018, pp. 2227–2237. [doi:10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettlemoyer,
    [深度上下文化词表示](https://aclanthology.org/N18-1202)，收录于：M. Walker, H. Ji, A. Stent（编），《2018年北美计算语言学协会年会论文集：人类语言技术，第1卷（长篇论文）》，计算语言学协会，新奥尔良，路易斯安那州，2018年，页码2227–2237。[doi:10.18653/v1/N18-1202](https://doi.org/10.18653/v1/N18-1202)。'
- en: URL [https://aclanthology.org/N18-1202](https://aclanthology.org/N18-1202)
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网址 [https://aclanthology.org/N18-1202](https://aclanthology.org/N18-1202)
- en: '[15] C. Jia, X. Liang, Y. Zhang, [Cross-domain NER using cross-domain language
    modeling](https://aclanthology.org/P19-1236), in: A. Korhonen, D. Traum, L. Màrquez
    (Eds.), Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics, Association for Computational Linguistics, Florence, Italy, 2019,
    pp. 2464–2474. [doi:10.18653/v1/P19-1236](https://doi.org/10.18653/v1/P19-1236).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. Jia, X. Liang, Y. Zhang, [使用跨领域语言建模的跨领域NER](https://aclanthology.org/P19-1236)，收录于：A.
    Korhonen, D. Traum, L. Màrquez（编），《第57届计算语言学协会年会论文集》，计算语言学协会，佛罗伦萨，意大利，2019年，页码2464–2474。[doi:10.18653/v1/P19-1236](https://doi.org/10.18653/v1/P19-1236)。'
- en: URL [https://aclanthology.org/P19-1236](https://aclanthology.org/P19-1236)
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网址 [https://aclanthology.org/P19-1236](https://aclanthology.org/P19-1236)
- en: '[16] Z. Liu, Y. Xu, T. Yu, W. Dai, Z. Ji, S. Cahyawijaya, A. Madotto, P. Fung,
    [Crossner: Evaluating cross-domain named entity recognition](https://ojs.aaai.org/index.php/AAAI/article/view/17587),
    Proceedings of the AAAI Conference on Artificial Intelligence 35 (15) (2021) 13452–13460.
    [doi:10.1609/aaai.v35i15.17587](https://doi.org/10.1609/aaai.v35i15.17587).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Z. Liu, Y. Xu, T. Yu, W. Dai, Z. Ji, S. Cahyawijaya, A. Madotto, P. Fung,
    [Crossner: 跨领域命名实体识别评估](https://ojs.aaai.org/index.php/AAAI/article/view/17587)，《AAAI人工智能会议论文集》35卷（15）(2021)
    13452–13460。[doi:10.1609/aaai.v35i15.17587](https://doi.org/10.1609/aaai.v35i15.17587)。'
- en: URL [https://ojs.aaai.org/index.php/AAAI/article/view/17587](https://ojs.aaai.org/index.php/AAAI/article/view/17587)
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 网址 [https://ojs.aaai.org/index.php/AAAI/article/view/17587](https://ojs.aaai.org/index.php/AAAI/article/view/17587)
- en: '[17] A. Névéol, C. Grouin, J. Leixa, S. Rosset, P. Zweigenbaum, The quaero
    french medical corpus: A ressource for medical entity recognition and normalization,
    Proc of BioTextMining Work (2014) 24–30.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. Névéol, C. Grouin, J. Leixa, S. Rosset, P. Zweigenbaum, The quaero
    法语医学语料库：用于医学实体识别和规范化的资源，收录于《BioTextMining工作会议》 (2014) 24–30。'
- en: '[18] R. I. Doğan, R. Leaman, Z. Lu, [Ncbi disease corpus: A resource for disease
    name recognition and concept normalization](https://www.sciencedirect.com/science/article/pii/S1532046413001974),
    Journal of Biomedical Informatics 47 (2014) 1–10. [doi:https://doi.org/10.1016/j.jbi.2013.12.006](https://doi.org/https://doi.org/10.1016/j.jbi.2013.12.006).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] R. I. Doğan, R. Leaman, Z. Lu，[Ncbi 疾病语料库：一种用于疾病名称识别和概念规范化的资源](https://www.sciencedirect.com/science/article/pii/S1532046413001974)，《生物医学信息学期刊》47
    (2014) 1–10。[doi:https://doi.org/10.1016/j.jbi.2013.12.006](https://doi.org/https://doi.org/10.1016/j.jbi.2013.12.006)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046413001974](https://www.sciencedirect.com/science/article/pii/S1532046413001974)
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S1532046413001974](https://www.sciencedirect.com/science/article/pii/S1532046413001974)
- en: '[19] P. Báez, F. Villena, M. Rojas, M. Durán, J. Dunstan, [The Chilean waiting
    list corpus: a new resource for clinical named entity recognition in Spanish](https://aclanthology.org/2020.clinicalnlp-1.32),
    in: Proceedings of the 3rd Clinical Natural Language Processing Workshop, Association
    for Computational Linguistics, Online, 2020, pp. 291–300. [doi:10.18653/v1/2020.clinicalnlp-1.32](https://doi.org/10.18653/v1/2020.clinicalnlp-1.32).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] P. Báez, F. Villena, M. Rojas, M. Durán, J. Dunstan，[智利等待名单语料库：一种新的西班牙语临床命名实体识别资源](https://aclanthology.org/2020.clinicalnlp-1.32)，收录于：《第3届临床自然语言处理研讨会论文集》，计算语言学协会，在线，2020年，第291–300页。[doi:10.18653/v1/2020.clinicalnlp-1.32](https://doi.org/10.18653/v1/2020.clinicalnlp-1.32)。'
- en: URL [https://aclanthology.org/2020.clinicalnlp-1.32](https://aclanthology.org/2020.clinicalnlp-1.32)
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2020.clinicalnlp-1.32](https://aclanthology.org/2020.clinicalnlp-1.32)
- en: '[20] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al., Language models are few-shot learners,
    Advances in neural information processing systems 33 (2020) 1877–1901.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A.
    Neelakantan, P. Shyam, G. Sastry, A. Askell, 等，[语言模型是少样本学习者](https://aclanthology.org/2020.clinicalnlp-1.32)，《神经信息处理系统进展》33
    (2020) 1877–1901。'
- en: '[21] D.-H. Lee, A. Kadakia, K. Tan, M. Agarwal, X. Feng, T. Shibuya, R. Mitani,
    T. Sekiya, J. Pujara, X. Ren, [Good examples make a faster learner: Simple demonstration-based
    learning for low-resource NER](https://aclanthology.org/2022.acl-long.192), in:
    S. Muresan, P. Nakov, A. Villavicencio (Eds.), Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Dublin, Ireland, 2022, pp. 2687–2700.
    [doi:10.18653/v1/2022.acl-long.192](https://doi.org/10.18653/v1/2022.acl-long.192).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] D.-H. Lee, A. Kadakia, K. Tan, M. Agarwal, X. Feng, T. Shibuya, R. Mitani,
    T. Sekiya, J. Pujara, X. Ren，[良好的例子使学习者更快：低资源NER的简单示范学习](https://aclanthology.org/2022.acl-long.192)，收录于：S.
    Muresan, P. Nakov, A. Villavicencio (编)，《第60届计算语言学协会年会（第一卷：长篇论文）》论文集，计算语言学协会，都柏林，爱尔兰，2022年，第2687–2700页。[doi:10.18653/v1/2022.acl-long.192](https://doi.org/10.18653/v1/2022.acl-long.192)。'
- en: URL [https://aclanthology.org/2022.acl-long.192](https://aclanthology.org/2022.acl-long.192)
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2022.acl-long.192](https://aclanthology.org/2022.acl-long.192)
- en: '[22] S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, Q. Lei, Few-shot learning via
    learning the representation, provably, 2021.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] S. S. Du, W. Hu, S. M. Kakade, J. D. Lee, Q. Lei, 通过学习表示的少样本学习，2021。'
- en: '[23] S. Shin, S.-W. Lee, H. Ahn, S. Kim, H. Kim, B. Kim, K. Cho, G. Lee, W. Park,
    J.-W. Ha, N. Sung, [On the effect of pretraining corpora on in-context learning
    by a large-scale language model](https://aclanthology.org/2022.naacl-main.380),
    in: M. Carpuat, M.-C. de Marneffe, I. V. Meza Ruiz (Eds.), Proceedings of the
    2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Association for Computational Linguistics,
    Seattle, United States, 2022, pp. 5168–5186. [doi:10.18653/v1/2022.naacl-main.380](https://doi.org/10.18653/v1/2022.naacl-main.380).'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] S. Shin, S.-W. Lee, H. Ahn, S. Kim, H. Kim, B. Kim, K. Cho, G. Lee, W.
    Park, J.-W. Ha, N. Sung, [关于大规模语言模型在上下文学习中的预训练语料库影响的研究](https://aclanthology.org/2022.naacl-main.380)，收录于：M.
    Carpuat, M.-C. de Marneffe, I. V. Meza Ruiz (编)，《2022年北美计算语言学协会会议：人类语言技术论文集》，计算语言学协会，西雅图，美国，2022年，第5168–5186页。[doi:10.18653/v1/2022.naacl-main.380](https://doi.org/10.18653/v1/2022.naacl-main.380)。'
- en: URL [https://aclanthology.org/2022.naacl-main.380](https://aclanthology.org/2022.naacl-main.380)
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2022.naacl-main.380](https://aclanthology.org/2022.naacl-main.380)
- en: '[24] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou,
    et al., Chain-of-thought prompting elicits reasoning in large language models,
    Advances in Neural Information Processing Systems 35 (2022) 24824–24837.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D.
    Zhou 等人，《链式思维提示在大语言模型中的推理》，《神经信息处理系统进展》第35卷（2022年）24824–24837。'
- en: '[25] A. Srivastava, A. Rastogi, A. Rao, A. A. M. Shoeb, A. Abid, A. Fisch,
    A. R. Brown, A. Santoro, A. Gupta, A. Garriga-Alonso, A. Kluska, A. Lewkowycz,
    A. Agarwal, A. Power, A. Ray, A. Warstadt, A. W. Kocurek, A. Safaya, A. Tazarv,
    A. Xiang, A. Parrish, A. Nie, A. Hussain, A. Askell, A. Dsouza, A. Slone, A. Rahane,
    A. S. Iyer, A. J. Andreassen, A. Madotto, A. Santilli, A. Stuhlmüller, A. M. Dai,
    A. La, A. Lampinen, A. Zou, A. Jiang, A. Chen, A. Vuong, A. Gupta, A. Gottardi,
    A. Norelli, A. Venkatesh, A. Gholamidavoodi, A. Tabassum, A. Menezes, A. Kirubarajan,
    A. Mullokandov, A. Sabharwal, A. Herrick, A. Efrat, A. Erdem, A. Karakaş, B. R.
    Roberts, B. S. Loe, B. Zoph, B. Bojanowski, B. Özyurt, B. Hedayatnia, B. Neyshabur,
    B. Inden, B. Stein, B. Ekmekci, B. Y. Lin, B. Howald, B. Orinion, C. Diao, C. Dour,
    C. Stinson, C. Argueta, C. Ferri, C. Singh, C. Rathkopf, C. Meng, C. Baral, C. Wu,
    C. Callison-Burch, C. Waites, C. Voigt, C. D. Manning, C. Potts, C. Ramirez, C. E.
    Rivera, C. Siro, C. Raffel, C. Ashcraft, C. Garbacea, D. Sileo, D. Garrette, D. Hendrycks,
    D. Kilman, D. Roth, C. D. Freeman, D. Khashabi, D. Levy, D. M. González, D. Perszyk,
    D. Hernandez, D. Chen, D. Ippolito, D. Gilboa, D. Dohan, D. Drakard, D. Jurgens,
    D. Datta, D. Ganguli, D. Emelin, D. Kleyko, D. Yuret, D. Chen, D. Tam, D. Hupkes,
    D. Misra, D. Buzan, D. C. Mollo, D. Yang, D.-H. Lee, D. Schrader, E. Shutova,
    E. D. Cubuk, E. Segal, E. Hagerman, E. Barnes, E. Donoway, E. Pavlick, E. Rodolà,
    E. Lam, E. Chu, E. Tang, E. Erdem, E. Chang, E. A. Chi, E. Dyer, E. Jerzak, E. Kim,
    E. E. Manyasi, E. Zheltonozhskii, F. Xia, F. Siar, F. Martínez-Plumed, F. Happé,
    F. Chollet, F. Rong, G. Mishra, G. I. Winata, G. de Melo, G. Kruszewski, G. Parascandolo,
    G. Mariani, G. X. Wang, G. Jaimovitch-Lopez, G. Betz, G. Gur-Ari, H. Galijasevic,
    H. Kim, H. Rashkin, H. Hajishirzi, H. Mehta, H. Bogar, H. F. A. Shevlin, H. Schuetze,
    H. Yakura, H. Zhang, H. M. Wong, I. Ng, I. Noble, J. Jumelet, J. Geissinger, J. Kernion,
    J. Hilton, J. Lee, J. F. Fisac, J. B. Simon, J. Koppel, J. Zheng, J. Zou, J. Kocon,
    J. Thompson, J. Wingfield, J. Kaplan, J. Radom, J. Sohl-Dickstein, J. Phang, J. Wei,
    J. Yosinski, J. Novikova, J. Bosscher, J. Marsh, J. Kim, J. Taal, J. Engel, J. Alabi,
    J. Xu, J. Song, J. Tang, J. Waweru, J. Burden, J. Miller, J. U. Balis, J. Batchelder,
    J. Berant, J. Frohberg, J. Rozen, J. Hernandez-Orallo, J. Boudeman, J. Guerr,
    J. Jones, J. B. Tenenbaum, J. S. Rule, J. Chua, K. Kanclerz, K. Livescu, K. Krauth,
    K. Gopalakrishnan, K. Ignatyeva, K. Markert, K. Dhole, K. Gimpel, K. Omondi, K. W.
    Mathewson, K. Chiafullo, K. Shkaruta, K. Shridhar, K. McDonell, K. Richardson,
    L. Reynolds, L. Gao, L. Zhang, L. Dugan, L. Qin, L. Contreras-Ochando, L.-P. Morency,
    L. Moschella, L. Lam, L. Noble, L. Schmidt, L. He, L. Oliveros-Colón, L. Metz,
    L. K. Senel, M. Bosma, M. Sap, M. T. Hoeve, M. Farooqi, M. Faruqui, M. Mazeika,
    M. Baturan, M. Marelli, M. Maru, M. J. Ramirez-Quintana, M. Tolkiehn, M. Giulianelli,
    M. Lewis, M. Potthast, M. L. Leavitt, M. Hagen, M. Schubert, M. O. Baitemirova,
    M. Arnaud, M. McElrath, M. A. Yee, M. Cohen, M. Gu, M. Ivanitskiy, M. Starritt,
    M. Strube, M. Swędrowski, M. Bevilacqua, M. Yasunaga, M. Kale, M. Cain, M. Xu,
    M. Suzgun, M. Walker, M. Tiwari, M. Bansal, M. Aminnaseri, M. Geva, M. Gheini,
    M. V. T, N. Peng, N. A. Chi, N. Lee, N. G.-A. Krakover, N. Cameron, N. Roberts,
    N. Doiron, N. Martinez, N. Nangia, N. Deckers, N. Muennighoff, N. S. Keskar, N. S.
    Iyer, N. Constant, N. Fiedel, N. Wen, O. Zhang, O. Agha, O. Elbaghdadi, O. Levy,
    O. Evans, P. A. M. Casares, P. Doshi, P. Fung, P. P. Liang, P. Vicol, P. Alipoormolabashi,
    P. Liao, P. Liang, P. W. Chang, P. Eckersley, P. M. Htut, P. Hwang, P. Miłkowski,
    P. Patil, P. Pezeshkpour, P. Oli, Q. Mei, Q. Lyu, Q. Chen, R. Banjade, R. E. Rudolph,
    R. Gabriel, R. Habacker, R. Risco, R. Millière, R. Garg, R. Barnes, R. A. Saurous,
    R. Arakawa, R. Raymaekers, R. Frank, R. Sikand, R. Novak, R. Sitelew, R. L. Bras,
    R. Liu, R. Jacobs, R. Zhang, R. Salakhutdinov, R. A. Chi, S. R. Lee, R. Stovall,
    R. Teehan, R. Yang, S. Singh, S. M. Mohammad, S. Anand, S. Dillavou, S. Shleifer,
    S. Wiseman, S. Gruetter, S. R. Bowman, S. S. Schoenholz, S. Han, S. Kwatra, S. A.
    Rous, S. Ghazarian, S. Ghosh, S. Casey, S. Bischoff, S. Gehrmann, S. Schuster,
    S. Sadeghi, S. Hamdan, S. Zhou, S. Srivastava, S. Shi, S. Singh, S. Asaadi, S. S.
    Gu, S. Pachchigar, S. Toshniwal, S. Upadhyay, S. S. Debnath, S. Shakeri, S. Thormeyer,
    S. Melzi, S. Reddy, S. P. Makini, S.-H. Lee, S. Torene, S. Hatwar, S. Dehaene,
    S. Divic, S. Ermon, S. Biderman, S. Lin, S. Prasad, S. Piantadosi, S. Shieber,
    S. Misherghi, S. Kiritchenko, S. Mishra, T. Linzen, T. Schuster, T. Li, T. Yu,
    T. Ali, T. Hashimoto, T.-L. Wu, T. Desbordes, T. Rothschild, T. Phan, T. Wang,
    T. Nkinyili, T. Schick, T. Kornev, T. Tunduny, T. Gerstenberg, T. Chang, T. Neeraj,
    T. Khot, T. Shultz, U. Shaham, V. Misra, V. Demberg, V. Nyamai, V. Raunak, V. V.
    Ramasesh, vinay uday prabhu, V. Padmakumar, V. Srikumar, W. Fedus, W. Saunders,
    W. Zhang, W. Vossen, X. Ren, X. Tong, X. Zhao, X. Wu, X. Shen, Y. Yaghoobzadeh,
    Y. Lakretz, Y. Song, Y. Bahri, Y. Choi, Y. Yang, Y. Hao, Y. Chen, Y. Belinkov,
    Y. Hou, Y. Hou, Y. Bai, Z. Seid, Z. Zhao, Z. Wang, Z. J. Wang, Z. Wang, Z. Wu,
    [Beyond the imitation game: Quantifying and extrapolating the capabilities of
    language models](https://openreview.net/forum?id=uyTL5Bvosj), Transactions on
    Machine Learning Research (2023).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[超越模仿游戏：量化和推测语言模型的能力](https://openreview.net/forum?id=uyTL5Bvosj)，《机器学习研究交易》
    (2023)。'
- en: URL [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj)
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://openreview.net/forum?id=uyTL5Bvosj](https://openreview.net/forum?id=uyTL5Bvosj)
- en: '[26] D. Tam, R. R. Menon, M. Bansal, S. Srivastava, C. Raffel, [Improving and
    simplifying pattern exploiting training](https://aclanthology.org/2021.emnlp-main.407),
    in: M.-F. Moens, X. Huang, L. Specia, S. W.-t. Yih (Eds.), Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021,
    pp. 4980–4991. [doi:10.18653/v1/2021.emnlp-main.407](https://doi.org/10.18653/v1/2021.emnlp-main.407).'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] D. Tam, R. R. Menon, M. Bansal, S. Srivastava, C. Raffel, [改进和简化模式利用训练](https://aclanthology.org/2021.emnlp-main.407)，在：M.-F.
    Moens, X. Huang, L. Specia, S. W.-t. Yih（编），《2021年自然语言处理经验方法会议论文集》，计算语言学协会，线上与多米尼加共和国蓬塔卡纳，2021年，第4980–4991页。[doi:10.18653/v1/2021.emnlp-main.407](https://doi.org/10.18653/v1/2021.emnlp-main.407)。'
- en: URL [https://aclanthology.org/2021.emnlp-main.407](https://aclanthology.org/2021.emnlp-main.407)
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2021.emnlp-main.407](https://aclanthology.org/2021.emnlp-main.407)
- en: '[27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, I. Sutskever, [Learning transferable
    visual models from natural language supervision](https://proceedings.mlr.press/v139/radford21a.html),
    in: M. Meila, T. Zhang (Eds.), Proceedings of the 38th International Conference
    on Machine Learning, Vol. 139 of Proceedings of Machine Learning Research, PMLR,
    2021, pp. 8748–8763.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,
    A. Askell, P. Mishkin, J. Clark, G. Krueger, I. Sutskever, [从自然语言监督中学习可迁移的视觉模型](https://proceedings.mlr.press/v139/radford21a.html)，在：M.
    Meila, T. Zhang（编），《第38届国际机器学习会议论文集》，机器学习研究第139卷，PMLR，2021年，第8748–8763页。'
- en: URL [https://proceedings.mlr.press/v139/radford21a.html](https://proceedings.mlr.press/v139/radford21a.html)
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://proceedings.mlr.press/v139/radford21a.html](https://proceedings.mlr.press/v139/radford21a.html)
- en: '[28] G. Qin, J. Eisner, [Learning how to ask: Querying LMs with mixtures of
    soft prompts](https://aclanthology.org/2021.naacl-main.410), in: K. Toutanova,
    A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard, R. Cotterell,
    T. Chakraborty, Y. Zhou (Eds.), Proceedings of the 2021 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Association for Computational Linguistics, Online, 2021, pp. 5203–5212.
    [doi:10.18653/v1/2021.naacl-main.410](https://doi.org/10.18653/v1/2021.naacl-main.410).'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] G. Qin, J. Eisner, [学习如何提问：用软提示的混合查询语言模型](https://aclanthology.org/2021.naacl-main.410)，在：K.
    Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard,
    R. Cotterell, T. Chakraborty, Y. Zhou（编），《2021年北美计算语言学协会会议论文集：人类语言技术》，计算语言学协会，线上，2021年，第5203–5212页。[doi:10.18653/v1/2021.naacl-main.410](https://doi.org/10.18653/v1/2021.naacl-main.410)。'
- en: URL [https://aclanthology.org/2021.naacl-main.410](https://aclanthology.org/2021.naacl-main.410)
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2021.naacl-main.410](https://aclanthology.org/2021.naacl-main.410)
- en: '[29] Z. Zhao, E. Wallace, S. Feng, D. Klein, S. Singh, Calibrate before use:
    Improving few-shot performance of language models, in: International Conference
    on Machine Learning, PMLR, 2021, pp. 12697–12706.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Z. Zhao, E. Wallace, S. Feng, D. Klein, S. Singh, 在使用前进行校准：提高语言模型的少样本性能，发表于《国际机器学习会议》，PMLR，2021年，第12697–12706页。'
- en: '[30] Y. Lu, M. Bartolo, A. Moore, S. Riedel, P. Stenetorp, [Fantastically ordered
    prompts and where to find them: Overcoming few-shot prompt order sensitivity](https://aclanthology.org/2022.acl-long.556),
    in: S. Muresan, P. Nakov, A. Villavicencio (Eds.), Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Dublin, Ireland, 2022, pp. 8086–8098.
    [doi:10.18653/v1/2022.acl-long.556](https://doi.org/10.18653/v1/2022.acl-long.556).'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Y. Lu, M. Bartolo, A. Moore, S. Riedel, P. Stenetorp, [奇妙排序的提示及其发现：克服少量提示顺序敏感性](https://aclanthology.org/2022.acl-long.556)，在：S.
    Muresan, P. Nakov, A. Villavicencio（编），《计算语言学协会第60届年会论文集（第1卷：长篇论文）》，计算语言学协会，爱尔兰都柏林，2022年，第8086–8098页。[doi:10.18653/v1/2022.acl-long.556](https://doi.org/10.18653/v1/2022.acl-long.556)。'
- en: URL [https://aclanthology.org/2022.acl-long.556](https://aclanthology.org/2022.acl-long.556)
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2022.acl-long.556](https://aclanthology.org/2022.acl-long.556)
- en: '[31] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, L. Zettlemoyer,
    [Rethinking the role of demonstrations: What makes in-context learning work?](https://aclanthology.org/2022.emnlp-main.759),
    in: Y. Goldberg, Z. Kozareva, Y. Zhang (Eds.), Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing, Association for Computational
    Linguistics, Abu Dhabi, United Arab Emirates, 2022, pp. 11048–11064. [doi:10.18653/v1/2022.emnlp-main.759](https://doi.org/10.18653/v1/2022.emnlp-main.759).'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, L. Zettlemoyer,
    [重新思考演示的作用：是什么使得上下文学习有效？](https://aclanthology.org/2022.emnlp-main.759)，在：Y. Goldberg,
    Z. Kozareva, Y. Zhang (编)，2022年自然语言处理实证方法会议论文集，计算语言学协会，阿布扎比，阿联酋，2022年，第11048–11064页。
    [doi:10.18653/v1/2022.emnlp-main.759](https://doi.org/10.18653/v1/2022.emnlp-main.759)。'
- en: URL [https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759)
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2022.emnlp-main.759](https://aclanthology.org/2022.emnlp-main.759)
- en: '[32] E. Perez, D. Kiela, K. Cho, [True few-shot learning with language models](https://openreview.net/forum?id=ShnM-rRh4T),
    in: A. Beygelzimer, Y. Dauphin, P. Liang, J. W. Vaughan (Eds.), Advances in Neural
    Information Processing Systems, 2021.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] E. Perez, D. Kiela, K. Cho, [真正的少样本学习与语言模型](https://openreview.net/forum?id=ShnM-rRh4T)，在：A.
    Beygelzimer, Y. Dauphin, P. Liang, J. W. Vaughan (编)，神经信息处理系统进展，2021年。'
- en: URL [https://openreview.net/forum?id=ShnM-rRh4T](https://openreview.net/forum?id=ShnM-rRh4T)
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://openreview.net/forum?id=ShnM-rRh4T](https://openreview.net/forum?id=ShnM-rRh4T)
- en: '[33] S. Wang, X. Sun, X. Li, R. Ouyang, F. Wu, T. Zhang, J. Li, G. Wang, Gpt-ner:
    Named entity recognition via large language models (2023). [arXiv:2304.10428](http://arxiv.org/abs/2304.10428).'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. Wang, X. Sun, X. Li, R. Ouyang, F. Wu, T. Zhang, J. Li, G. Wang, Gpt-ner:
    通过大型语言模型进行命名实体识别（2023年）。 [arXiv:2304.10428](http://arxiv.org/abs/2304.10428)。'
- en: '[34] D. Ashok, Z. Lipton, Promptner: Prompting for named entity recognition
    (May 2023).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] D. Ashok, Z. Lipton, Promptner: 为命名实体识别提供提示（2023年5月）。'
- en: '[35] Y. Hu, I. Ameer, X. Zuo, X. Peng, Y. Zhou, Z. Li, Y. Li, J. Li, X. Jiang,
    H. Xu, Zero-shot clinical entity recognition using chatgpt, arXiv preprint arXiv:2303.16416
    (2023).'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Y. Hu, I. Ameer, X. Zuo, X. Peng, Y. Zhou, Z. Li, Y. Li, J. Li, X. Jiang,
    H. Xu, 使用 chatgpt 的零样本临床实体识别，arXiv 预印本 arXiv:2303.16416 (2023)。'
- en: '[36] B. Jimenez Gutierrez, N. McNeal, C. Washington, Y. Chen, L. Li, H. Sun,
    Y. Su, [Thinking about GPT-3 in-context learning for biomedical IE? think again](https://aclanthology.org/2022.findings-emnlp.329),
    in: Y. Goldberg, Z. Kozareva, Y. Zhang (Eds.), Findings of the Association for
    Computational Linguistics: EMNLP 2022, Association for Computational Linguistics,
    Abu Dhabi, United Arab Emirates, 2022, pp. 4497–4512. [doi:10.18653/v1/2022.findings-emnlp.329](https://doi.org/10.18653/v1/2022.findings-emnlp.329).'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] B. Jimenez Gutierrez, N. McNeal, C. Washington, Y. Chen, L. Li, H. Sun,
    Y. Su, [思考GPT-3的上下文学习在生物医学信息提取中的应用？再想想](https://aclanthology.org/2022.findings-emnlp.329)，在：Y.
    Goldberg, Z. Kozareva, Y. Zhang (编)，计算语言学协会发现：EMNLP 2022，计算语言学协会，阿布扎比，阿联酋，2022年，第4497–4512页。
    [doi:10.18653/v1/2022.findings-emnlp.329](https://doi.org/10.18653/v1/2022.findings-emnlp.329)。'
- en: URL [https://aclanthology.org/2022.findings-emnlp.329](https://aclanthology.org/2022.findings-emnlp.329)
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2022.findings-emnlp.329](https://aclanthology.org/2022.findings-emnlp.329)
- en: '[37] A. Fritzler, V. Logacheva, M. Kretov, [Few-shot classification in named
    entity recognition task](https://doi.org/10.1145/3297280.3297378), in: Proceedings
    of the 34th ACM/SIGAPP Symposium on Applied Computing, SAC ’19, Association for
    Computing Machinery, New York, NY, USA, 2019, p. 993–1000. [doi:10.1145/3297280.3297378](https://doi.org/10.1145/3297280.3297378).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Fritzler, V. Logacheva, M. Kretov, [命名实体识别任务中的少样本分类](https://doi.org/10.1145/3297280.3297378)，在：第34届ACM/SIGAPP应用计算研讨会论文集，SAC
    ’19，计算机协会，纽约，NY，美国，2019年，第993–1000页。 [doi:10.1145/3297280.3297378](https://doi.org/10.1145/3297280.3297378)。'
- en: URL [https://doi.org/10.1145/3297280.3297378](https://doi.org/10.1145/3297280.3297378)
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://doi.org/10.1145/3297280.3297378](https://doi.org/10.1145/3297280.3297378)
- en: '[38] Y. Yang, A. Katiyar, [Simple and effective few-shot named entity recognition
    with structured nearest neighbor learning](https://aclanthology.org/2020.emnlp-main.516),
    in: B. Webber, T. Cohn, Y. He, Y. Liu (Eds.), Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP), Association for Computational
    Linguistics, Online, 2020, pp. 6365–6375. [doi:10.18653/v1/2020.emnlp-main.516](https://doi.org/10.18653/v1/2020.emnlp-main.516).'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Y. Yang, A. Katiyar, [简单而有效的少量样本命名实体识别与结构化最近邻学习](https://aclanthology.org/2020.emnlp-main.516)，在：B.
    Webber, T. Cohn, Y. He, Y. Liu (编)，2020年自然语言处理实证方法会议（EMNLP）论文集，计算语言学协会，在线，2020年，第6365–6375页。
    [doi:10.18653/v1/2020.emnlp-main.516](https://doi.org/10.18653/v1/2020.emnlp-main.516)。'
- en: URL [https://aclanthology.org/2020.emnlp-main.516](https://aclanthology.org/2020.emnlp-main.516)
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2020.emnlp-main.516](https://aclanthology.org/2020.emnlp-main.516)
- en: '[39] J. Huang, C. Li, K. Subudhi, D. Jose, S. Balakrishnan, W. Chen, B. Peng,
    J. Gao, J. Han, [Few-shot named entity recognition: An empirical baseline study](https://aclanthology.org/2021.emnlp-main.813),
    in: M.-F. Moens, X. Huang, L. Specia, S. W.-t. Yih (Eds.), Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021,
    pp. 10408–10423. [doi:10.18653/v1/2021.emnlp-main.813](https://doi.org/10.18653/v1/2021.emnlp-main.813).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] J. Huang, C. Li, K. Subudhi, D. Jose, S. Balakrishnan, W. Chen, B. Peng,
    J. Gao, J. Han, [少样本命名实体识别：一种经验基准研究](https://aclanthology.org/2021.emnlp-main.813)，编者：M.-F.
    Moens, X. Huang, L. Specia, S. W.-t. Yih，第2021年自然语言处理实证方法会议论文集，计算语言学协会，在线和多米尼加共和国蓬塔卡纳，2021年，第10408–10423页。[doi:10.18653/v1/2021.emnlp-main.813](https://doi.org/10.18653/v1/2021.emnlp-main.813)。'
- en: URL [https://aclanthology.org/2021.emnlp-main.813](https://aclanthology.org/2021.emnlp-main.813)
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2021.emnlp-main.813](https://aclanthology.org/2021.emnlp-main.813)
- en: '[40] R. Aly, A. Vlachos, R. McDonald, [Leveraging type descriptions for zero-shot
    named entity recognition and classification](https://aclanthology.org/2021.acl-long.120),
    in: C. Zong, F. Xia, W. Li, R. Navigli (Eds.), Proceedings of the 59th Annual
    Meeting of the Association for Computational Linguistics and the 11th International
    Joint Conference on Natural Language Processing (Volume 1: Long Papers), Association
    for Computational Linguistics, Online, 2021, pp. 1516–1528. [doi:10.18653/v1/2021.acl-long.120](https://doi.org/10.18653/v1/2021.acl-long.120).'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] R. Aly, A. Vlachos, R. McDonald, [利用类型描述进行零样本命名实体识别和分类](https://aclanthology.org/2021.acl-long.120)，编者：C.
    Zong, F. Xia, W. Li, R. Navigli，第59届计算语言学协会年会暨第11届国际自然语言处理联合会议论文集（第1卷：长篇论文），计算语言学协会，在线，2021年，第1516–1528页。[doi:10.18653/v1/2021.acl-long.120](https://doi.org/10.18653/v1/2021.acl-long.120)。'
- en: URL [https://aclanthology.org/2021.acl-long.120](https://aclanthology.org/2021.acl-long.120)
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2021.acl-long.120](https://aclanthology.org/2021.acl-long.120)
- en: '[41] J. Ma, M. Ballesteros, S. Doss, R. Anubhai, S. Mallya, Y. Al-Onaizan,
    D. Roth, [Label semantics for few shot named entity recognition](https://aclanthology.org/2022.findings-acl.155),
    in: S. Muresan, P. Nakov, A. Villavicencio (Eds.), Findings of the Association
    for Computational Linguistics: ACL 2022, Association for Computational Linguistics,
    Dublin, Ireland, 2022, pp. 1956–1971. [doi:10.18653/v1/2022.findings-acl.155](https://doi.org/10.18653/v1/2022.findings-acl.155).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Ma, M. Ballesteros, S. Doss, R. Anubhai, S. Mallya, Y. Al-Onaizan,
    D. Roth, [少样本命名实体识别的标签语义](https://aclanthology.org/2022.findings-acl.155)，编者：S.
    Muresan, P. Nakov, A. Villavicencio，ACL 2022 计算语言学协会会议论文集，计算语言学协会，爱尔兰都柏林，2022年，第1956–1971页。[doi:10.18653/v1/2022.findings-acl.155](https://doi.org/10.18653/v1/2022.findings-acl.155)。'
- en: URL [https://aclanthology.org/2022.findings-acl.155](https://aclanthology.org/2022.findings-acl.155)
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2022.findings-acl.155](https://aclanthology.org/2022.findings-acl.155)
- en: '[42] Y. Hou, W. Che, Y. Lai, Z. Zhou, Y. Liu, H. Liu, T. Liu, [Few-shot slot
    tagging with collapsed dependency transfer and label-enhanced task-adaptive projection
    network](https://aclanthology.org/2020.acl-main.128), in: D. Jurafsky, J. Chai,
    N. Schluter, J. Tetreault (Eds.), Proceedings of the 58th Annual Meeting of the
    Association for Computational Linguistics, Association for Computational Linguistics,
    Online, 2020, pp. 1381–1393. [doi:10.18653/v1/2020.acl-main.128](https://doi.org/10.18653/v1/2020.acl-main.128).'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Y. Hou, W. Che, Y. Lai, Z. Zhou, Y. Liu, H. Liu, T. Liu, [利用压缩依赖转移和标签增强的任务自适应投影网络进行少样本槽标注](https://aclanthology.org/2020.acl-main.128)，编者：D.
    Jurafsky, J. Chai, N. Schluter, J. Tetreault，第58届计算语言学协会年会论文集，计算语言学协会，在线，2020年，第1381–1393页。[doi:10.18653/v1/2020.acl-main.128](https://doi.org/10.18653/v1/2020.acl-main.128)。'
- en: URL [https://aclanthology.org/2020.acl-main.128](https://aclanthology.org/2020.acl-main.128)
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2020.acl-main.128](https://aclanthology.org/2020.acl-main.128)
- en: '[43] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, G. Neubig, Pre-train, prompt,
    and predict: A systematic survey of prompting methods in natural language processing,
    ACM Computing Surveys 55 (9) (2023) 1–35.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, G. Neubig, 预训练、提示和预测：自然语言处理中的提示方法系统综述，ACM计算机调查
    55 (9) (2023) 1–35。'
- en: '[44] D. Vilar, M. Freitag, C. Cherry, J. Luo, V. Ratnakar, G. Foster, [Prompting
    PaLM for translation: Assessing strategies and performance](https://aclanthology.org/2023.acl-long.859),
    in: Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), Association for Computational Linguistics,
    Toronto, Canada, 2023, pp. 15406–15427. [doi:10.18653/v1/2023.acl-long.859](https://doi.org/10.18653/v1/2023.acl-long.859).'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] D. Vilar, M. Freitag, C. Cherry, J. Luo, V. Ratnakar, G. Foster, [为翻译提示PaLM：评估策略与表现](https://aclanthology.org/2023.acl-long.859)，见：《第61届计算语言学协会年会论文集（卷1：长篇论文）》，计算语言学协会，多伦多，加拿大，2023年，第15406–15427页。[doi:10.18653/v1/2023.acl-long.859](https://doi.org/10.18653/v1/2023.acl-long.859)。'
- en: URL [https://aclanthology.org/2023.acl-long.859](https://aclanthology.org/2023.acl-long.859)
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2023.acl-long.859](https://aclanthology.org/2023.acl-long.859)
- en: '[45] R. Ma, X. Zhou, T. Gui, Y. Tan, L. Li, Q. Zhang, X. Huang, [Template-free
    prompt tuning for few-shot NER](https://aclanthology.org/2022.naacl-main.420),
    in: M. Carpuat, M.-C. de Marneffe, I. V. Meza Ruiz (Eds.), Proceedings of the
    2022 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Association for Computational Linguistics,
    Seattle, United States, 2022, pp. 5721–5732. [doi:10.18653/v1/2022.naacl-main.420](https://doi.org/10.18653/v1/2022.naacl-main.420).'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] R. Ma, X. Zhou, T. Gui, Y. Tan, L. Li, Q. Zhang, X. Huang, [无需模板的提示调优用于少样本命名实体识别](https://aclanthology.org/2022.naacl-main.420)，见：M.
    Carpuat, M.-C. de Marneffe, I. V. Meza Ruiz (主编)，《2022年北美计算语言学协会年会论文集：人类语言技术》，计算语言学协会，西雅图，美国，2022年，第5721–5732页。[doi:10.18653/v1/2022.naacl-main.420](https://doi.org/10.18653/v1/2022.naacl-main.420)。'
- en: URL [https://aclanthology.org/2022.naacl-main.420](https://aclanthology.org/2022.naacl-main.420)
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2022.naacl-main.420](https://aclanthology.org/2022.naacl-main.420)
- en: '[46] A. Layegh, A. H. Payberah, A. Soylu, D. Roman, M. Matskin, Contrastner:
    Contrastive-based prompt tuning for few-shot ner, arXiv preprint arXiv:2305.17951
    (2023).'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Layegh, A. H. Payberah, A. Soylu, D. Roman, M. Matskin, Contrastner:
    基于对比的提示调优用于少样本命名实体识别，arXiv预印本 arXiv:2305.17951（2023）。'
- en: '[47] N. Hu, X. Zhou, B. Xu, H. Liu, X. Xie, H.-T. Zheng, [Vpn: Variation on
    prompt tuning for named-entity recognition](https://www.mdpi.com/2076-3417/13/14/8359),
    Applied Sciences 13 (14) (2023). [doi:10.3390/app13148359](https://doi.org/10.3390/app13148359).'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] N. Hu, X. Zhou, B. Xu, H. Liu, X. Xie, H.-T. Zheng, [Vpn：命名实体识别的提示调优变体](https://www.mdpi.com/2076-3417/13/14/8359)，《应用科学》13(14)（2023）。[doi:10.3390/app13148359](https://doi.org/10.3390/app13148359)。'
- en: URL [https://www.mdpi.com/2076-3417/13/14/8359](https://www.mdpi.com/2076-3417/13/14/8359)
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.mdpi.com/2076-3417/13/14/8359](https://www.mdpi.com/2076-3417/13/14/8359)
- en: '[48] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, J. Li, [A unified MRC framework
    for named entity recognition](https://aclanthology.org/2020.acl-main.519), in:
    D. Jurafsky, J. Chai, N. Schluter, J. Tetreault (Eds.), Proceedings of the 58th
    Annual Meeting of the Association for Computational Linguistics, Association for
    Computational Linguistics, Online, 2020, pp. 5849–5859. [doi:10.18653/v1/2020.acl-main.519](https://doi.org/10.18653/v1/2020.acl-main.519).'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] X. Li, J. Feng, Y. Meng, Q. Han, F. Wu, J. Li, [统一的MRC框架用于命名实体识别](https://aclanthology.org/2020.acl-main.519)，见：D.
    Jurafsky, J. Chai, N. Schluter, J. Tetreault (主编)，《第58届计算语言学协会年会论文集》，计算语言学协会，在线，2020年，第5849–5859页。[doi:10.18653/v1/2020.acl-main.519](https://doi.org/10.18653/v1/2020.acl-main.519)。'
- en: URL [https://aclanthology.org/2020.acl-main.519](https://aclanthology.org/2020.acl-main.519)
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2020.acl-main.519](https://aclanthology.org/2020.acl-main.519)
- en: '[49] A. T. Liu, W. Xiao, H. Zhu, D. Zhang, S.-W. Li, A. O. Arnold, [Qaner:
    Prompting question answering models for few-shot named entity recognition](https://api.semanticscholar.org/CorpusID:247222693),
    ArXiv abs/2203.01543 (2022).'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] A. T. Liu, W. Xiao, H. Zhu, D. Zhang, S.-W. Li, A. O. Arnold, [Qaner：为少样本命名实体识别提示问答模型](https://api.semanticscholar.org/CorpusID:247222693)，ArXiv
    abs/2203.01543（2022）。'
- en: URL [https://api.semanticscholar.org/CorpusID:247222693](https://api.semanticscholar.org/CorpusID:247222693)
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://api.semanticscholar.org/CorpusID:247222693](https://api.semanticscholar.org/CorpusID:247222693)
- en: '[50] J. Chen, Y. Lu, H. Lin, J. Lou, W. Jia, D. Dai, H. Wu, B. Cao, X. Han,
    L. Sun, [Learning in-context learning for named entity recognition](https://aclanthology.org/2023.acl-long.764),
    in: A. Rogers, J. Boyd-Graber, N. Okazaki (Eds.), Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Toronto, Canada, 2023, pp. 13661–13675.
    [doi:10.18653/v1/2023.acl-long.764](https://doi.org/10.18653/v1/2023.acl-long.764).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. Chen, Y. Lu, H. Lin, J. Lou, W. Jia, D. Dai, H. Wu, B. Cao, X. Han,
    L. Sun, [上下文学习在命名实体识别中的应用](https://aclanthology.org/2023.acl-long.764)，见：A. Rogers,
    J. Boyd-Graber, N. Okazaki (编辑)，《第61届计算语言学协会年会论文集（第一卷：长篇论文）》，计算语言学协会，多伦多，加拿大，2023年，第13661–13675页。
    [doi:10.18653/v1/2023.acl-long.764](https://doi.org/10.18653/v1/2023.acl-long.764)。'
- en: URL [https://aclanthology.org/2023.acl-long.764](https://aclanthology.org/2023.acl-long.764)
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2023.acl-long.764](https://aclanthology.org/2023.acl-long.764)
- en: '[51] L. Cui, Y. Wu, J. Liu, S. Yang, Y. Zhang, [Template-based named entity
    recognition using BART](https://aclanthology.org/2021.findings-acl.161), in: C. Zong,
    F. Xia, W. Li, R. Navigli (Eds.), Findings of the Association for Computational
    Linguistics: ACL-IJCNLP 2021, Association for Computational Linguistics, Online,
    2021, pp. 1835–1845. [doi:10.18653/v1/2021.findings-acl.161](https://doi.org/10.18653/v1/2021.findings-acl.161).'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] L. Cui, Y. Wu, J. Liu, S. Yang, Y. Zhang, [基于模板的命名实体识别使用 BART](https://aclanthology.org/2021.findings-acl.161)，见：C.
    Zong, F. Xia, W. Li, R. Navigli (编辑)，《计算语言学协会发现：ACL-IJCNLP 2021》，计算语言学协会，在线，2021年，第1835–1845页。
    [doi:10.18653/v1/2021.findings-acl.161](https://doi.org/10.18653/v1/2021.findings-acl.161)。'
- en: URL [https://aclanthology.org/2021.findings-acl.161](https://aclanthology.org/2021.findings-acl.161)
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2021.findings-acl.161](https://aclanthology.org/2021.findings-acl.161)
- en: '[52] Y. Shen, Z. Tan, S. Wu, W. Zhang, R. Zhang, Y. Xi, W. Lu, Y. Zhuang, [PromptNER:
    Prompt locating and typing for named entity recognition](https://aclanthology.org/2023.acl-long.698),
    in: A. Rogers, J. Boyd-Graber, N. Okazaki (Eds.), Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),
    Association for Computational Linguistics, Toronto, Canada, 2023, pp. 12492–12507.
    [doi:10.18653/v1/2023.acl-long.698](https://doi.org/10.18653/v1/2023.acl-long.698).'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Y. Shen, Z. Tan, S. Wu, W. Zhang, R. Zhang, Y. Xi, W. Lu, Y. Zhuang, [PromptNER:
    提示定位与命名实体识别](https://aclanthology.org/2023.acl-long.698), 见：A. Rogers, J. Boyd-Graber,
    N. Okazaki (编辑)，《第61届计算语言学协会年会论文集（第一卷：长篇论文）》，计算语言学协会，多伦多，加拿大，2023年，第12492–12507页。
    [doi:10.18653/v1/2023.acl-long.698](https://doi.org/10.18653/v1/2023.acl-long.698)。'
- en: URL [https://aclanthology.org/2023.acl-long.698](https://aclanthology.org/2023.acl-long.698)
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2023.acl-long.698](https://aclanthology.org/2023.acl-long.698)
- en: '[53] F. Ye, L. Huang, S. Liang, K. Chi, [Decomposed two-stage prompt learning
    for few-shot named entity recognition](https://www.mdpi.com/2078-2489/14/5/262),
    Information 14 (5) (2023). [doi:10.3390/info14050262](https://doi.org/10.3390/info14050262).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] F. Ye, L. Huang, S. Liang, K. Chi, [分解的两阶段提示学习用于少样本命名实体识别](https://www.mdpi.com/2078-2489/14/5/262)，《信息》14
    (5) (2023)。 [doi:10.3390/info14050262](https://doi.org/10.3390/info14050262)。'
- en: URL [https://www.mdpi.com/2078-2489/14/5/262](https://www.mdpi.com/2078-2489/14/5/262)
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.mdpi.com/2078-2489/14/5/262](https://www.mdpi.com/2078-2489/14/5/262)
- en: '[54] T. Schick, H. Schütze, [It’s not just size that matters: Small language
    models are also few-shot learners](https://aclanthology.org/2021.naacl-main.185),
    in: K. Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard,
    R. Cotterell, T. Chakraborty, Y. Zhou (Eds.), Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, Association for Computational Linguistics, Online,
    2021, pp. 2339–2352. [doi:10.18653/v1/2021.naacl-main.185](https://doi.org/10.18653/v1/2021.naacl-main.185).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] T. Schick, H. Schütze, [不仅仅是规模的重要性：小型语言模型也是少样本学习者](https://aclanthology.org/2021.naacl-main.185)，见：K.
    Toutanova, A. Rumshisky, L. Zettlemoyer, D. Hakkani-Tur, I. Beltagy, S. Bethard,
    R. Cotterell, T. Chakraborty, Y. Zhou (编辑)，《2021年北美计算语言学协会年会：人类语言技术会议论文集》，计算语言学协会，在线，2021年，第2339–2352页。
    [doi:10.18653/v1/2021.naacl-main.185](https://doi.org/10.18653/v1/2021.naacl-main.185)。'
- en: URL [https://aclanthology.org/2021.naacl-main.185](https://aclanthology.org/2021.naacl-main.185)
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2021.naacl-main.185](https://aclanthology.org/2021.naacl-main.185)
- en: '[55] Y. Ge, Y. Guo, S. Das, M. A. Al-Garadi, A. Sarker, [Few-shot learning
    for medical text: A review of advances, trends, and opportunities](https://www.sciencedirect.com/science/article/pii/S153204642300179X),
    Journal of Biomedical Informatics 144 (2023) 104458. [doi:https://doi.org/10.1016/j.jbi.2023.104458](https://doi.org/https://doi.org/10.1016/j.jbi.2023.104458).'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Ge, Y. Guo, S. Das, M. A. Al-Garadi, A. Sarker, [医学文本的少样本学习：进展、趋势和机会综述](https://www.sciencedirect.com/science/article/pii/S153204642300179X)，《生物医学信息学杂志》144（2023）104458。[doi:https://doi.org/10.1016/j.jbi.2023.104458](https://doi.org/https://doi.org/10.1016/j.jbi.2023.104458)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S153204642300179X](https://www.sciencedirect.com/science/article/pii/S153204642300179X)
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S153204642300179X](https://www.sciencedirect.com/science/article/pii/S153204642300179X)
- en: '[56] A. Kormilitzin, N. Vaci, Q. Liu, A. Nevado-Holgado, [Med7: A transferable
    clinical natural language processing model for electronic health records](https://www.sciencedirect.com/science/article/pii/S0933365721000798),
    Artificial Intelligence in Medicine 118 (2021) 102086. [doi:https://doi.org/10.1016/j.artmed.2021.102086](https://doi.org/https://doi.org/10.1016/j.artmed.2021.102086).'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] A. Kormilitzin, N. Vaci, Q. Liu, A. Nevado-Holgado, [Med7：一个可转移的临床自然语言处理模型，用于电子健康记录](https://www.sciencedirect.com/science/article/pii/S0933365721000798)，《医学中的人工智能》118（2021）102086。[doi:https://doi.org/10.1016/j.artmed.2021.102086](https://doi.org/https://doi.org/10.1016/j.artmed.2021.102086)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S0933365721000798](https://www.sciencedirect.com/science/article/pii/S0933365721000798)
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S0933365721000798](https://www.sciencedirect.com/science/article/pii/S0933365721000798)
- en: '[57] J. Huang, C. Li, K. Subudhi, D. Jose, S. Balakrishnan, W. Chen, B. Peng,
    J. Gao, J. Han, [Few-shot named entity recognition: An empirical baseline study](https://aclanthology.org/2021.emnlp-main.813),
    in: M.-F. Moens, X. Huang, L. Specia, S. W.-t. Yih (Eds.), Proceedings of the
    2021 Conference on Empirical Methods in Natural Language Processing, Association
    for Computational Linguistics, Online and Punta Cana, Dominican Republic, 2021,
    pp. 10408–10423. [doi:10.18653/v1/2021.emnlp-main.813](https://doi.org/10.18653/v1/2021.emnlp-main.813).'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. Huang, C. Li, K. Subudhi, D. Jose, S. Balakrishnan, W. Chen, B. Peng,
    J. Gao, J. Han, [少样本命名实体识别：一个实证基线研究](https://aclanthology.org/2021.emnlp-main.813)，在：M.-F.
    Moens, X. Huang, L. Specia, S. W.-t. Yih (编辑)，《2021年自然语言处理实证方法会议论文集》，计算语言学协会，在线和多米尼加共和国蓬塔卡纳，2021年，第10408–10423页。[doi:10.18653/v1/2021.emnlp-main.813](https://doi.org/10.18653/v1/2021.emnlp-main.813)。'
- en: URL [https://aclanthology.org/2021.emnlp-main.813](https://aclanthology.org/2021.emnlp-main.813)
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2021.emnlp-main.813](https://aclanthology.org/2021.emnlp-main.813)
- en: '[58] Ö. Uzuner, B. R. South, S. Shen, S. L. DuVall, 2010 i2b2/va challenge
    on concepts, assertions, and relations in clinical text, Journal of the American
    Medical Informatics Association 18 (5) (2011) 552–556.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Ö. Uzuner, B. R. South, S. Shen, S. L. DuVall, 2010年i2b2/va挑战赛：临床文本中的概念、断言和关系，《美国医学信息学协会杂志》18（5）（2011）552–556。'
- en: '[59] B. Liao, Y. Meng, C. Monz, [Parameter-efficient fine-tuning without introducing
    new latency](https://aclanthology.org/2023.acl-long.233), in: A. Rogers, J. Boyd-Graber,
    N. Okazaki (Eds.), Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers), Association for Computational
    Linguistics, Toronto, Canada, 2023, pp. 4242–4260. [doi:10.18653/v1/2023.acl-long.233](https://doi.org/10.18653/v1/2023.acl-long.233).'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] B. Liao, Y. Meng, C. Monz, [高效的参数微调而不引入新的延迟](https://aclanthology.org/2023.acl-long.233)，在：A.
    Rogers, J. Boyd-Graber, N. Okazaki（编辑），《计算语言学协会第61届年会论文集（第1卷：长篇论文）》计算语言学协会，多伦多，加拿大，2023年，第4242–4260页。[doi:10.18653/v1/2023.acl-long.233](https://doi.org/10.18653/v1/2023.acl-long.233)。'
- en: URL [https://aclanthology.org/2023.acl-long.233](https://aclanthology.org/2023.acl-long.233)
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2023.acl-long.233](https://aclanthology.org/2023.acl-long.233)
- en: '[60] T. Han, L. C. Adams, J.-M. Papaioannou, P. Grundmann, T. Oberhauser, A. Löser,
    D. Truhn, K. K. Bressem, Medalpaca–an open-source collection of medical conversational
    ai models and training data, arXiv preprint arXiv:2304.08247 (2023).'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] T. Han, L. C. Adams, J.-M. Papaioannou, P. Grundmann, T. Oberhauser, A.
    Löser, D. Truhn, K. K. Bressem, Medalpaca：一个开源的医疗对话AI模型和训练数据集合，arXiv预印本 arXiv:2304.08247（2023）。'
- en: '[61] A. Toma, P. R. Lawler, J. Ba, R. G. Krishnan, B. B. Rubin, B. Wang, Clinical
    camel: An open-source expert-level medical language model with dialogue-based
    knowledge encoding, arXiv preprint arXiv:2305.12031 (2023).'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Toma, P. R. Lawler, J. Ba, R. G. Krishnan, B. B. Rubin, B. Wang, Clinical
    camel: 一个开源的专家级医疗语言模型，具有基于对话的知识编码，arXiv预印本 arXiv:2305.12031（2023）。'
- en: '[62] J. Nothman, N. Ringland, W. Radford, T. Murphy, J. R. Curran, [Learning
    multilingual named entity recognition from wikipedia](https://www.sciencedirect.com/science/article/pii/S0004370212000276),
    Artificial Intelligence 194 (2013) 151–175, artificial Intelligence, Wikipedia
    and Semi-Structured Resources. [doi:https://doi.org/10.1016/j.artint.2012.03.006](https://doi.org/https://doi.org/10.1016/j.artint.2012.03.006).'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] J. Nothman, N. Ringland, W. Radford, T. Murphy, J. R. Curran, [从维基百科学习多语言命名实体识别](https://www.sciencedirect.com/science/article/pii/S0004370212000276)，人工智能
    194 (2013) 151–175，人工智能，维基百科和半结构化资源。 [doi:https://doi.org/10.1016/j.artint.2012.03.006](https://doi.org/https://doi.org/10.1016/j.artint.2012.03.006)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S0004370212000276](https://www.sciencedirect.com/science/article/pii/S0004370212000276)
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S0004370212000276](https://www.sciencedirect.com/science/article/pii/S0004370212000276)
- en: '[63] E. F. Tjong Kim Sang, [Introduction to the CoNLL-2002 shared task: Language-independent
    named entity recognition](https://aclanthology.org/W02-2024), in: COLING-02: The
    6th Conference on Natural Language Learning 2002 (CoNLL-2002), 2002.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] E. F. Tjong Kim Sang, [CoNLL-2002 共享任务简介：语言无关的命名实体识别](https://aclanthology.org/W02-2024)，载于：COLING-02：第六届自然语言学习会议
    2002 (CoNLL-2002)，2002年。'
- en: URL [https://aclanthology.org/W02-2024](https://aclanthology.org/W02-2024)
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/W02-2024](https://aclanthology.org/W02-2024)
- en: '[64] E. F. Tjong Kim Sang, F. De Meulder, [Introduction to the CoNLL-2003 shared
    task: Language-independent named entity recognition](https://aclanthology.org/W03-0419),
    in: Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL
    2003, 2003, pp. 142–147.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] E. F. Tjong Kim Sang, F. De Meulder, [CoNLL-2003 共享任务简介：语言无关的命名实体识别](https://aclanthology.org/W03-0419)，载于：第七届自然语言学习会议论文集
    HLT-NAACL 2003，2003年，页码 142–147。'
- en: URL [https://aclanthology.org/W03-0419](https://aclanthology.org/W03-0419)
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/W03-0419](https://aclanthology.org/W03-0419)
- en: '[65] C. Grouin, S. Rosset, P. Zweigenbaum, K. Fort, O. Galibert, L. Quintard,
    [Proposal for an extension of traditional named entities: From guidelines to evaluation,
    an overview](https://aclanthology.org/W11-0411), in: N. Ide, A. Meyers, S. Pradhan,
    K. Tomanek (Eds.), Proceedings of the 5th Linguistic Annotation Workshop, Association
    for Computational Linguistics, Portland, Oregon, USA, 2011, pp. 92–100.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] C. Grouin, S. Rosset, P. Zweigenbaum, K. Fort, O. Galibert, L. Quintard,
    [对传统命名实体扩展的提案：从指南到评估的概述](https://aclanthology.org/W11-0411)，载于：N. Ide, A. Meyers,
    S. Pradhan, K. Tomanek (编)，第五届语言学标注研讨会论文集，计算语言学协会，美国俄勒冈州波特兰，2011年，页码 92–100。'
- en: URL [https://aclanthology.org/W11-0411](https://aclanthology.org/W11-0411)
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/W11-0411](https://aclanthology.org/W11-0411)
- en: '[66] B. Magnini, B. Altuna, A. Lavelli, M. Speranza, R. Zanoli, The e3c project:
    European clinical case corpus, Language 1 (L2) (2021) L3.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] B. Magnini, B. Altuna, A. Lavelli, M. Speranza, R. Zanoli, E3C 项目：欧洲临床案例语料库，语言
    1 (L2) (2021) L3。'
- en: '[67] Y.-F. Luo, W. Sun, A. Rumshisky, [Mcn: A comprehensive corpus for medical
    concept normalization](https://www.sciencedirect.com/science/article/pii/S1532046419300504),
    Journal of Biomedical Informatics 92 (2019) 103132. [doi:https://doi.org/10.1016/j.jbi.2019.103132](https://doi.org/https://doi.org/10.1016/j.jbi.2019.103132).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Y.-F. Luo, W. Sun, A. Rumshisky, [Mcn：医疗概念规范化的综合语料库](https://www.sciencedirect.com/science/article/pii/S1532046419300504)，生物医学信息学杂志
    92 (2019) 103132。 [doi:https://doi.org/10.1016/j.jbi.2019.103132](https://doi.org/https://doi.org/10.1016/j.jbi.2019.103132)。'
- en: URL [https://www.sciencedirect.com/science/article/pii/S1532046419300504](https://www.sciencedirect.com/science/article/pii/S1532046419300504)
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://www.sciencedirect.com/science/article/pii/S1532046419300504](https://www.sciencedirect.com/science/article/pii/S1532046419300504)
- en: '[68] D. A. Lindberg, B. L. Humphreys, A. T. McCray, The unified medical language
    system, Yearbook of medical informatics 2 (01) (1993) 41–51.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] D. A. Lindberg, B. L. Humphreys, A. T. McCray, 统一医学语言系统，医学信息年鉴 2 (01)
    (1993) 41–51。'
- en: '[69] A. McCray, A. Burgun, O. Bodenreider, Aggregating umls semantic types
    for reducing conceptual complexity, Studies in health technology and informatics
    84 (2001) 216–20. [doi:10.3233/978-1-60750-928-8-216](https://doi.org/10.3233/978-1-60750-928-8-216).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. McCray, A. Burgun, O. Bodenreider, 汇总 UMLS 语义类型以减少概念复杂性，健康技术与信息学研究
    84 (2001) 216–20。 [doi:10.3233/978-1-60750-928-8-216](https://doi.org/10.3233/978-1-60750-928-8-216)。'
- en: '[70] L. Campillos, L. Deléger, C. Grouin, T. Hamon, A.-L. Ligozat, A. Névéol,
    A french clinical corpus with comprehensive semantic annotations: development
    of the medical entity and relation limsi annotated text corpus (merlot), Language
    Resources and Evaluation 52 (2018) 571–601.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] L. Campillos, L. Deléger, C. Grouin, T. Hamon, A.-L. Ligozat, A. Névéol，《一个具有全面语义注释的法语临床语料库：医疗实体和关系的
    LIMSI 注释文本语料库（MERLOT）的开发》，《语言资源与评估》52 (2018) 571–601。'
- en: '[71] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov,
    S. Batra, P. Bhargava, S. Bhosale, et al., Llama 2: Open foundation and fine-tuned
    chat models, arXiv preprint arXiv:2307.09288 (2023).'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N.
    Bashlykov, S. Batra, P. Bhargava, S. Bhosale, 等，《Llama 2: 开放基础和微调聊天模型》，arXiv 预印本
    arXiv:2307.09288 (2023)。'
- en: '[72] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D. d. l.
    Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, et al., Mistral 7b, arXiv
    preprint arXiv:2310.06825 (2023).'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Q. Jiang, A. Sablayrolles, A. Mensch, C. Bamford, D. S. Chaplot, D.
    d. l. Casas, F. Bressand, G. Lengyel, G. Lample, L. Saulnier, 等，《Mistral 7b》，arXiv
    预印本 arXiv:2310.06825 (2023)。'
- en: '[73] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow,
    R. Castagné, A. S. Luccioni, F. Yvon, et al., Bloom: A 176b-parameter open-access
    multilingual language model, arXiv preprint arXiv:2211.05100 (2022).'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić, D. Hesslow,
    R. Castagné, A. S. Luccioni, F. Yvon, 等，《Bloom: 一个1760亿参数的开放访问多语言模型》，arXiv 预印本
    arXiv:2211.05100 (2022)。'
- en: '[74] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,
    T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen, et al., The
    bigscience roots corpus: A 1.6 tb composite multilingual dataset, Advances in
    Neural Information Processing Systems 35 (2022) 31809–31826.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del Moral,
    T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada, H. Nguyen, 等，《The bigscience
    roots 数据集：一个1.6 TB的复合多语言数据集》，《神经信息处理系统进展》35 (2022) 31809–31826。'
- en: '[75] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli,
    B. Pannier, E. Almazrouei, J. Launay, The refinedweb dataset for falcon llm: outperforming
    curated corpora with web data, and web data only, arXiv preprint arXiv:2306.01116
    (2023).'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli, H. Alobeidli,
    B. Pannier, E. Almazrouei, J. Launay，《The refinedweb 数据集用于 Falcon LLM: 超越精心策划的语料库与仅使用网页数据》，arXiv
    预印本 arXiv:2306.01116 (2023)。'
- en: '[76] B. Wang, A. Komatsuzaki, GPT-J-6B: A 6 Billion Parameter Autoregressive
    Language Model, [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
    (May 2021).'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] B. Wang, A. Komatsuzaki，《GPT-J-6B: 一种60亿参数的自回归语言模型》，[https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)
    (2021年5月)。'
- en: '[77] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, et al., The pile: An 800gb dataset of diverse text
    for language modeling, arXiv preprint arXiv:2101.00027 (2020).'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] L. Gao, S. Biderman, S. Black, L. Golding, T. Hoppe, C. Foster, J. Phang,
    H. He, A. Thite, N. Nabeshima, 等，《The pile: 一个800GB的多样文本数据集用于语言建模》，arXiv 预印本 arXiv:2101.00027
    (2020)。'
- en: '[78] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, et al., Opt: Open pre-trained transformer language
    models, arXiv preprint arXiv:2205.01068 (2022).'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan,
    M. Diab, X. Li, X. V. Lin, 等，《Opt: 开放预训练的变换器语言模型》，arXiv 预印本 arXiv:2205.01068 (2022)。'
- en: '[79] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing, et al., Judging llm-as-a-judge with mt-bench and chatbot
    arena, arXiv preprint arXiv:2306.05685 (2023).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing, 等，《利用 MT-Bench 和 Chatbot Arena 判断 LLM 作为评判者的表现》，arXiv 预印本
    arXiv:2306.05685 (2023)。'
- en: '[80] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán,
    E. Grave, M. Ott, L. Zettlemoyer, V. Stoyanov, [Unsupervised cross-lingual representation
    learning at scale](https://aclanthology.org/2020.acl-main.747), in: D. Jurafsky,
    J. Chai, N. Schluter, J. Tetreault (Eds.), Proceedings of the 58th Annual Meeting
    of the Association for Computational Linguistics, Association for Computational
    Linguistics, Online, 2020, pp. 8440–8451. [doi:10.18653/v1/2020.acl-main.747](https://doi.org/10.18653/v1/2020.acl-main.747).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzmán,
    E. Grave, M. Ott, L. Zettlemoyer, V. Stoyanov，[无监督跨语言表示学习的大规模研究](https://aclanthology.org/2020.acl-main.747)，收录于：D.
    Jurafsky, J. Chai, N. Schluter, J. Tetreault（编），《第58届计算语言学协会年会论文集》，计算语言学协会，在线，2020年，第8440–8451页。
    [doi:10.18653/v1/2020.acl-main.747](https://doi.org/10.18653/v1/2020.acl-main.747)。'
- en: URL [https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747)
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747)
- en: '[81] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,
    S. Fidler, Aligning books and movies: Towards story-like visual explanations by
    watching movies and reading books, in: Proceedings of the IEEE international conference
    on computer vision, 2015, pp. 19–27.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba,
    S. Fidler, 对齐书籍与电影：通过观看电影和阅读书籍来实现类似故事的视觉解释，见：IEEE国际计算机视觉会议论文集，2015，页码19–27。'
- en: '[82] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, V. Stoyanov, Roberta: A robustly optimized bert pretraining approach,
    arXiv preprint arXiv:1907.11692 (2019).'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,
    L. Zettlemoyer, V. Stoyanov, Roberta：一种稳健优化的BERT预训练方法，arXiv预印本 arXiv:1907.11692
    (2019)。'
- en: '[83] E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Naumann, M. McDermott,
    [Publicly available clinical BERT embeddings](https://aclanthology.org/W19-1909),
    in: A. Rumshisky, K. Roberts, S. Bethard, T. Naumann (Eds.), Proceedings of the
    2nd Clinical Natural Language Processing Workshop, Association for Computational
    Linguistics, Minneapolis, Minnesota, USA, 2019, pp. 72–78. [doi:10.18653/v1/W19-1909](https://doi.org/10.18653/v1/W19-1909).'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] E. Alsentzer, J. Murphy, W. Boag, W.-H. Weng, D. Jindi, T. Naumann, M.
    McDermott，[公开可用的临床BERT嵌入](https://aclanthology.org/W19-1909)，见：A. Rumshisky, K.
    Roberts, S. Bethard, T. Naumann（编），第2届临床自然语言处理研讨会论文集，计算语言学协会，美国明尼阿波利斯，2019，页码72–78。
    [doi:10.18653/v1/W19-1909](https://doi.org/10.18653/v1/W19-1909)。'
- en: URL [https://aclanthology.org/W19-1909](https://aclanthology.org/W19-1909)
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/W19-1909](https://aclanthology.org/W19-1909)
- en: '[84] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi,
    B. Moody, P. Szolovits, L. Anthony Celi, R. G. Mark, Mimic-iii, a freely accessible
    critical care database, Scientific data 3 (1) (2016) 1–9.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi,
    B. Moody, P. Szolovits, L. Anthony Celi, R. G. Mark, Mimic-iii，一个开放访问的重症监护数据库，《科学数据》3
    (1) (2016) 1–9。'
- en: '[85] G. Wang, X. Liu, Z. Ying, G. Yang, Z. Chen, Z. Liu, M. Zhang, H. Yan,
    Y. Lu, Y. Gao, et al., Optimized glycemic control of type 2 diabetes with reinforcement
    learning: a proof-of-concept trial, Nature Medicine (2023) 1–10.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] G. Wang, X. Liu, Z. Ying, G. Yang, Z. Chen, Z. Liu, M. Zhang, H. Yan,
    Y. Lu, Y. Gao, 等，利用强化学习优化2型糖尿病的血糖控制：一个概念验证试验，《自然医学》 (2023) 1–10。'
- en: '[86] C. Vasantharajan, K. Z. Tun, H. Thi-Nga, S. Jain, T. Rong, C. E. Siong,
    Medbert: A pre-trained language model for biomedical named entity recognition,
    in: 2022 Asia-Pacific Signal and Information Processing Association Annual Summit
    and Conference (APSIPA ASC), 2022, pp. 1482–1488. [doi:10.23919/APSIPAASC55919.2022.9980157](https://doi.org/10.23919/APSIPAASC55919.2022.9980157).'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] C. Vasantharajan, K. Z. Tun, H. Thi-Nga, S. Jain, T. Rong, C. E. Siong,
    Medbert：一种用于生物医学命名实体识别的预训练语言模型，见：2022年亚太信号与信息处理协会年会及会议（APSIPA ASC），2022，页码1482–1488。
    [doi:10.23919/APSIPAASC55919.2022.9980157](https://doi.org/10.23919/APSIPAASC55919.2022.9980157)。'
- en: '[87] L. Martin, B. Muller, P. J. Ortiz Suárez, Y. Dupont, L. Romary, É. de la
    Clergerie, D. Seddah, B. Sagot, [CamemBERT: a tasty French language model](https://aclanthology.org/2020.acl-main.645),
    in: D. Jurafsky, J. Chai, N. Schluter, J. Tetreault (Eds.), Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, Association
    for Computational Linguistics, Online, 2020, pp. 7203–7219. [doi:10.18653/v1/2020.acl-main.645](https://doi.org/10.18653/v1/2020.acl-main.645).'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] L. Martin, B. Muller, P. J. Ortiz Suárez, Y. Dupont, L. Romary, É. de
    la Clergerie, D. Seddah, B. Sagot，[CamemBERT：一种美味的法语语言模型](https://aclanthology.org/2020.acl-main.645)，见：D.
    Jurafsky, J. Chai, N. Schluter, J. Tetreault（编），第58届计算语言学协会年会论文集，计算语言学协会，在线，2020，页码7203–7219。
    [doi:10.18653/v1/2020.acl-main.645](https://doi.org/10.18653/v1/2020.acl-main.645)。'
- en: URL [https://aclanthology.org/2020.acl-main.645](https://aclanthology.org/2020.acl-main.645)
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2020.acl-main.645](https://aclanthology.org/2020.acl-main.645)
- en: '[88] P. J. O. Suárez, L. Romary, B. Sagot, A monolingual approach to contextualized
    word embeddings for mid-resource languages, arXiv preprint arXiv:2006.06202 (2020).'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] P. J. O. Suárez, L. Romary, B. Sagot, 一种针对中等资源语言的单语上下文化词嵌入方法，arXiv预印本
    arXiv:2006.06202 (2020)。'
- en: '[89] H. Le, L. Vial, J. Frej, V. Segonne, M. Coavoux, B. Lecouteux, A. Allauzen,
    B. Crabbé, L. Besacier, D. Schwab, [FlauBERT: Unsupervised language model pre-training
    for French](https://aclanthology.org/2020.lrec-1.302), in: N. Calzolari, F. Béchet,
    P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard,
    J. Mariani, H. Mazo, A. Moreno, J. Odijk, S. Piperidis (Eds.), Proceedings of
    the Twelfth Language Resources and Evaluation Conference, European Language Resources
    Association, Marseille, France, 2020, pp. 2479–2490.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] H. Le, L. Vial, J. Frej, V. Segonne, M. Coavoux, B. Lecouteux, A. Allauzen,
    B. Crabbé, L. Besacier, D. Schwab, [FlauBERT: 无监督法语语言模型预训练](https://aclanthology.org/2020.lrec-1.302)，收录于：N.
    Calzolari, F. Béchet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi,
    H. Isahara, B. Maegaard, J. Mariani, H. Mazo, A. Moreno, J. Odijk, S. Piperidis（编），第十二届语言资源与评估会议论文集，欧洲语言资源协会，法国马赛，2020年，第2479–2490页。'
- en: URL [https://aclanthology.org/2020.lrec-1.302](https://aclanthology.org/2020.lrec-1.302)
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2020.lrec-1.302](https://aclanthology.org/2020.lrec-1.302)
- en: '[90] Y. Labrak, A. Bazoge, R. Dufour, M. Rouvier, E. Morin, B. Daille, P.-A.
    Gourraud, [DrBERT: A robust pre-trained model in French for biomedical and clinical
    domains](https://aclanthology.org/2023.acl-long.896), in: A. Rogers, J. Boyd-Graber,
    N. Okazaki (Eds.), Proceedings of the 61st Annual Meeting of the Association for
    Computational Linguistics (Volume 1: Long Papers), Association for Computational
    Linguistics, Toronto, Canada, 2023, pp. 16207–16221. [doi:10.18653/v1/2023.acl-long.896](https://doi.org/10.18653/v1/2023.acl-long.896).'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Y. Labrak, A. Bazoge, R. Dufour, M. Rouvier, E. Morin, B. Daille, P.-A.
    Gourraud, [DrBERT: 一个在生物医学和临床领域的强健预训练法语模型](https://aclanthology.org/2023.acl-long.896)，收录于：A.
    Rogers, J. Boyd-Graber, N. Okazaki（编），第61届计算语言学协会年会论文集（第一卷：长篇论文），计算语言学协会，加拿大多伦多，2023年，第16207–16221页。
    [doi:10.18653/v1/2023.acl-long.896](https://doi.org/10.18653/v1/2023.acl-long.896)。'
- en: URL [https://aclanthology.org/2023.acl-long.896](https://aclanthology.org/2023.acl-long.896)
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2023.acl-long.896](https://aclanthology.org/2023.acl-long.896)
- en: '[91] R. Touchent, L. Romary, E. De La Clergerie, [CamemBERT-bio : Un modèle
    de langue français savoureux et meilleur pour la santé](https://hal.science/hal-04130187),
    in: C. Servan, A. Vilnat (Eds.), 18e Conférence en Recherche d’Information et
    Applications'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] R. Touchent, L. Romary, E. De La Clergerie, [CamemBERT-bio : 一种更佳、更健康的法语语言模型](https://hal.science/hal-04130187)，收录于：C.
    Servan, A. Vilnat（编），第18届信息检索与应用会议'
- en: 16e Rencontres Jeunes Chercheurs en RI
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 16e Rencontres Jeunes Chercheurs en RI
- en: 30e Conférence sur le Traitement Automatique des Langues Naturelles
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 30e Conférence sur le Traitement Automatique des Langues Naturelles
- en: 25e Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique
    des Langues, ATALA, Paris, France, 2023, pp. 323–334.
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 25e Rencontre des Étudiants Chercheurs en Informatique pour le Traitement Automatique
    des Langues, ATALA, 法国巴黎，2023年，第323–334页。
- en: URL [https://hal.science/hal-04130187](https://hal.science/hal-04130187)
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://hal.science/hal-04130187](https://hal.science/hal-04130187)
- en: '[92] J. Cañete, G. Chaperon, R. Fuentes, J.-H. Ho, H. Kang, J. Pérez, Spanish
    pre-trained bert model and evaluation data, in: PML4DC at ICLR 2020, 2020.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Cañete, G. Chaperon, R. Fuentes, J.-H. Ho, H. Kang, J. Pérez, 西班牙语预训练BERT模型及评估数据，收录于：PML4DC
    at ICLR 2020，2020年。'
- en: '[93] J. Tiedemann, [Parallel data, tools and interfaces in OPUS](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf),
    in: N. Calzolari, K. Choukri, T. Declerck, M. U. Doğan, B. Maegaard, J. Mariani,
    A. Moreno, J. Odijk, S. Piperidis (Eds.), Proceedings of the Eighth International
    Conference on Language Resources and Evaluation (LREC’12), European Language Resources
    Association (ELRA), Istanbul, Turkey, 2012, pp. 2214–2218.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Tiedemann, [OPUS中的平行数据、工具和接口](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf)，收录于：N.
    Calzolari, K. Choukri, T. Declerck, M. U. Doğan, B. Maegaard, J. Mariani, A. Moreno,
    J. Odijk, S. Piperidis（编），第八届国际语言资源与评估会议论文集（LREC’12），欧洲语言资源协会（ELRA），土耳其伊斯坦布尔，2012年，第2214–2218页。'
- en: URL [http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf)
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf](http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf)
- en: '[94] C. P. Carrino, J. Llop, M. Pàmies, A. Gutiérrez-Fandiño, J. Armengol-Estapé,
    J. Silveira-Ocampo, A. Valencia, A. Gonzalez-Agirre, M. Villegas, [Pretrained
    biomedical language models for clinical NLP in Spanish](https://aclanthology.org/2022.bionlp-1.19),
    in: Proceedings of the 21st Workshop on Biomedical Language Processing, Association
    for Computational Linguistics, Dublin, Ireland, 2022, pp. 193–199. [doi:10.18653/v1/2022.bionlp-1.19](https://doi.org/10.18653/v1/2022.bionlp-1.19).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] C. P. Carrino, J. Llop, M. Pàmies, A. Gutiérrez-Fandiño, J. Armengol-Estapé,
    J. Silveira-Ocampo, A. Valencia, A. Gonzalez-Agirre, M. Villegas, [西班牙语临床NLP的预训练生物医学语言模型](https://aclanthology.org/2022.bionlp-1.19)，见：第21届生物医学语言处理研讨会论文集，计算语言学学会，爱尔兰都柏林，2022年，第193–199页。[doi:10.18653/v1/2022.bionlp-1.19](https://doi.org/10.18653/v1/2022.bionlp-1.19)。'
- en: URL [https://aclanthology.org/2022.bionlp-1.19](https://aclanthology.org/2022.bionlp-1.19)
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/2022.bionlp-1.19](https://aclanthology.org/2022.bionlp-1.19)
- en: '[95] P. Wajsbürt, [Extraction and normalization of simple and structured entities
    in medical documents](https://hal.archives-ouvertes.fr/tel-03624928), Theses,
    Sorbonne Université (Dec. 2021).'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] P. Wajsbürt, [医学文档中的简单和结构化实体的提取与标准化](https://hal.archives-ouvertes.fr/tel-03624928)，论文，索邦大学（2021年12月）。'
- en: URL [https://hal.archives-ouvertes.fr/tel-03624928](https://hal.archives-ouvertes.fr/tel-03624928)
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://hal.archives-ouvertes.fr/tel-03624928](https://hal.archives-ouvertes.fr/tel-03624928)
- en: '[96] L. Lannelongue, J. Grealey, M. Inouye, Green algorithms: quantifying the
    carbon footprint of computation, Advanced science 8 (12) (2021) 2100707.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] L. Lannelongue, J. Grealey, M. Inouye, 绿色算法：量化计算的碳足迹，先进科学 8 (12) (2021)
    2100707。'
- en: '[97] N. Reimers, I. Gurevych, [Reporting score distributions makes a difference:
    Performance study of LSTM-networks for sequence tagging](https://aclanthology.org/D17-1035),
    in: M. Palmer, R. Hwa, S. Riedel (Eds.), Proceedings of the 2017 Conference on
    Empirical Methods in Natural Language Processing, Association for Computational
    Linguistics, Copenhagen, Denmark, 2017, pp. 338–348. [doi:10.18653/v1/D17-1035](https://doi.org/10.18653/v1/D17-1035).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] N. Reimers, I. Gurevych, [报告分数分布有影响：LSTM网络在序列标注中的性能研究](https://aclanthology.org/D17-1035)，见：M.
    Palmer, R. Hwa, S. Riedel（编），第2017年自然语言处理实证方法会议论文集，计算语言学学会，丹麦哥本哈根，2017年，第338–348页。[doi:10.18653/v1/D17-1035](https://doi.org/10.18653/v1/D17-1035)。'
- en: URL [https://aclanthology.org/D17-1035](https://aclanthology.org/D17-1035)
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/D17-1035](https://aclanthology.org/D17-1035)
- en: '[98] R. Dror, S. Shlomov, R. Reichart, [Deep dominance - how to properly compare
    deep neural models](https://doi.org/10.18653/v1/p19-1266), in: A. Korhonen, D. R.
    Traum, L. Màrquez (Eds.), Proceedings of the 57th Conference of the Association
    for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019,
    Volume 1: Long Papers, Association for Computational Linguistics, 2019, pp. 2773–2785.
    [doi:10.18653/v1/p19-1266](https://doi.org/10.18653/v1/p19-1266).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] R. Dror, S. Shlomov, R. Reichart, [深度优势 - 如何正确比较深度神经模型](https://doi.org/10.18653/v1/p19-1266)，见：A.
    Korhonen, D. R. Traum, L. Màrquez（编），第57届计算语言学协会年会论文集，ACL 2019，意大利佛罗伦萨，2019年7月28日-8月2日，第1卷：长篇论文，计算语言学学会，2019年，第2773–2785页。[doi:10.18653/v1/p19-1266](https://doi.org/10.18653/v1/p19-1266)。'
- en: URL [https://doi.org/10.18653/v1/p19-1266](https://doi.org/10.18653/v1/p19-1266)
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://doi.org/10.18653/v1/p19-1266](https://doi.org/10.18653/v1/p19-1266)
- en: '[99] E. M. Bender, B. Friedman, [Data statements for natural language processing:
    Toward mitigating system bias and enabling better science](https://aclanthology.org/Q18-1041),
    Transactions of the Association for Computational Linguistics 6 (2018) 587–604.
    [doi:10.1162/tacl_a_00041](https://doi.org/10.1162/tacl_a_00041).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] E. M. Bender, B. Friedman, [自然语言处理中的数据声明：旨在缓解系统偏差并促进更好的科学](https://aclanthology.org/Q18-1041)，计算语言学学会会刊
    6 (2018) 587–604。[doi:10.1162/tacl_a_00041](https://doi.org/10.1162/tacl_a_00041)。'
- en: URL [https://aclanthology.org/Q18-1041](https://aclanthology.org/Q18-1041)
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: URL [https://aclanthology.org/Q18-1041](https://aclanthology.org/Q18-1041)
- en: Appendix A NER labels descriptions
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A NER标签描述
- en: Appendix B Carbon footprint
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 碳足迹
