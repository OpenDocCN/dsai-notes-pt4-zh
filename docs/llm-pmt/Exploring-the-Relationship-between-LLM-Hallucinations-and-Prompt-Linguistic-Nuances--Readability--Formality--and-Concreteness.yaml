- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Exploring the Relationship between LLM Hallucinations and Prompt Linguistic
    Nuances: Readability, Formality, and Concreteness'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索LLM幻觉与提示语言细微差别之间的关系：可读性、正式性和具体性
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11064](https://ar5iv.labs.arxiv.org/html/2309.11064)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.11064](https://ar5iv.labs.arxiv.org/html/2309.11064)
- en: Vipula Rawte¹, Prachi Priya², S.M Towhidul Islam Tonmoy³, S M Mehedi Zaman³,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Vipula Rawte¹, Prachi Priya², S.M Towhidul Islam Tonmoy³, S M Mehedi Zaman³，
- en: Amit Sheth¹, Amitava Das¹
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Amit Sheth¹, Amitava Das¹
- en: ¹AI Institute, University of South Carolina, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹AI研究所，南卡罗来纳大学，美国
- en: ²Indian Institute of Technology, Kharagpur
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²印度理工学院，卡拉戈尔
- en: ³Islamic University of Technology {vrawte}@mailbox.sc.edu corresponding author
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ³伊斯兰科技大学 {vrawte}@mailbox.sc.edu 通讯作者
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As Large Language Models (LLMs) have advanced, they have brought forth new challenges,
    with one of the prominent issues being LLM hallucination. While various mitigation
    techniques are emerging to address hallucination, it is equally crucial to delve
    into its underlying causes. Consequently, in this preliminary exploratory investigation,
    we examine how linguistic factors in prompts, specifically readability, formality,
    and concreteness, influence the occurrence of hallucinations. Our experimental
    results suggest that prompts characterized by greater formality and concreteness
    tend to result in reduced hallucination. However, the outcomes pertaining to readability
    are somewhat inconclusive, showing a mixed pattern.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLM）的进步，它们带来了新的挑战，其中一个显著的问题是LLM幻觉。虽然各种减轻幻觉的技术正在出现，但深入探讨其根本原因同样重要。因此，在这项初步的探索性研究中，我们考察了提示中的语言因素，特别是可读性、正式性和具体性，如何影响幻觉的发生。我们的实验结果表明，具有较高正式性和具体性的提示往往会导致幻觉减少。然而，关于可读性的结果则有些不确定，显示出混合模式。
- en: '1 Hallucination in LLMs: An introduction'
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 LLM中的幻觉：简介
- en: The remarkable advantages offered by extensive generative AI models like GPT-4
    (Brown et al., [2020](#bib.bib2); OpenAI, [2023](#bib.bib18)), Stable Diffusion
    Rombach et al. ([2022](#bib.bib26)), DALL-E Ramesh et al. ([2021](#bib.bib25),
    [2022](#bib.bib24)), and Midjourney Midjourney ([2022](#bib.bib14)) are accompanied
    by a significant potential for misuse.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型生成AI模型如GPT-4（Brown et al., [2020](#bib.bib2); OpenAI, [2023](#bib.bib18)）、Stable
    Diffusion（Rombach et al., [2022](#bib.bib26)）、DALL-E（Ramesh et al., [2021](#bib.bib25),
    [2022](#bib.bib24)）和Midjourney（Midjourney, [2022](#bib.bib14)）所带来的显著优势也伴随着滥用的重大潜力。
- en: The recent and rapid developments in the field of LLMs have been gaining significant
    attention and use in various applications. This included natural language understanding
    and generation for chatbots, content generation, translation, summarization, and
    more. They were also being applied in fields like healthcare, finance, and education.
    Nevertheless, these LLMs encounter significant hurdles, with one prominent issue
    being termed hallucination. This term describes a situation in which the LLM generates
    responses that contain factual inaccuracies or fabrications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近和迅速发展的LLM领域正获得显著关注，并在各种应用中得到使用。这包括用于聊天机器人、内容生成、翻译、总结等的自然语言理解和生成。它们还被应用于医疗保健、金融和教育等领域。然而，这些LLM面临着显著的障碍，其中一个突出的问题被称为幻觉。这个术语描述了LLM生成包含事实不准确或虚构的响应的情况。
- en: Original
    Prompt Who owned the ships in the
    Boston Tea Party?The ships involved in the Boston
    Tea Party were British-owned. The three ships—Dartmouth, Eleanor, and Beaver—that
    carried the tea cargo were merchant vessels operated by British companies. They
    were docked in Boston Harbor and were targeted by American colonists who protested
    against the British-imposed Tea Act by boarding the ships and dumping the tea
    into the harbor.Reformulated Prompt Which American
    merchant owned the ship Dartmouth in the Boston Tea Party?The ship Dartmouth, one of the vessels involved in the Boston
    Tea Party, was owned by a partnership of American merchants. The principal owner
    of the Dartmouth was Francis Rotch, a wealthy American merchant and prominent
    figure in the New England whaling industry. Joseph Rotch, his brother, and William
    Rotch, their cousin, were also involved in the ownership of the Dartmouth. These
    merchants were associated with the tea trade and had interests in shipping and
    commerce. Their ship, the Dartmouth, carried a shipment of British tea to Boston
    Harbor, where it was targeted by American colonists during the Boston Tea Party
    protest against British taxation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 原始提示
    谁拥有波士顿倾茶事件中的船只？参与波士顿倾茶事件的船只是由英国人拥有的。这三艘船——达特茅斯号、埃莉诺号和比弗号——都是由英国公司运营的商船。它们停靠在波士顿港，并被抗议英国施加的茶叶法案的美国殖民者袭击，这些殖民者登上船只，将茶叶倾倒到港口。重新表述的提示
    哪位美国商人拥有波士顿倾茶事件中的达特茅斯号船只？在波士顿倾茶事件中涉及的达特茅斯号船只由一组美国商人共同拥有。达特茅斯号的主要拥有者是富有的美国商人弗朗西斯·罗奇，他是新英格兰捕鲸业的杰出人物。罗奇的兄弟约瑟夫·罗奇和他们的堂兄威廉·罗奇也参与了达特茅斯号的拥有。这些商人与茶叶贸易有关，并且对航运和商业有兴趣。他们的船只达特茅斯号运送了一批英国茶叶到达波士顿港，并在波士顿倾茶事件中成为美国殖民者反对英国税收的目标。
- en: 'Figure 1: An illustration of how a “reformulated prompt” can aid in addressing
    the hallucination issue by providing pertinent context. Here, the hallucinated
    text is highlighted in red. By introducing additional context highlighted in blue,
    such as “who” and “what”, we modify the prompt to be more formal and concrete.
    Thus, the newly generated response now incorporates the factually correct (dehallucinated)
    text, highlighted in green.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示了“重构提示”如何通过提供相关上下文来帮助解决幻觉问题。这里，幻觉文本用红色标出。通过引入额外的蓝色高亮上下文，例如“谁”和“什么”，我们将提示修改得更为正式和具体。因此，生成的新响应现在包含了事实正确的（去幻觉的）文本，标记为绿色。
- en: 'Several mitigation techniques have emerged to address and reduce the occurrence
    of hallucinations. These techniques can be broadly categorized into two groups:
    i) Black-box Mündler et al. ([2023](#bib.bib15)), which operates without depending
    on external grounded knowledge, and ii) Gray-box Zhang et al. ([2023](#bib.bib33));
    Peng et al. ([2023](#bib.bib21)); Li et al. ([2023](#bib.bib11)), which incorporates
    external knowledge to a certain extent.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对和减少幻觉的发生，已经出现了几种缓解技术。这些技术大致可以分为两类：i）黑箱 Mündler 等人（[2023](#bib.bib15)），不依赖于外部基础知识；ii）灰箱
    Zhang 等人（[2023](#bib.bib33)）；Peng 等人（[2023](#bib.bib21)）；Li 等人（[2023](#bib.bib11)），在一定程度上融入外部知识。
- en: Prompt engineering can play a crucial role in mitigating hallucinations in generative
    AI models. By providing clear and specific prompts, users can steer the AI model
    toward generating content that aligns with their intended context or requirements.
    This can reduce the chances of the model producing hallucinated or inaccurate
    information. Prompts can include contextual cues that help the AI model understand
    the context of the request. This additional context can guide the model in generating
    responses that are more contextually accurate and less prone to hallucination.
    Complex prompts can be used to guide the model through a series of steps, ensuring
    that it follows a logical sequence of thought and produces coherent responses.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程在缓解生成式 AI 模型中的幻觉方面可以发挥关键作用。通过提供清晰且具体的提示，用户可以引导 AI 模型生成符合其预期上下文或要求的内容。这可以减少模型生成幻觉或不准确的信息的可能性。提示可以包括上下文线索，帮助
    AI 模型理解请求的背景。这些额外的上下文可以引导模型生成更具上下文准确性的响应，减少幻觉的发生。复杂的提示可以用来指导模型经过一系列步骤，确保其遵循逻辑思路并产生连贯的响应。
- en: 'The state-of-the-art LLMs have the capability to process lengthy prompts as
    input. However, findings in Liu et al. ([2023b](#bib.bib13)) indicate (see [Fig. 2](#S1.F2
    "In 1 Hallucination in LLMs: An introduction ‣ Exploring the Relationship between
    LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and
    Concreteness")) that these models tend to perform best when pertinent information
    is located at the beginning or end of the input context. their performance significantly
    diminishes when they need to access relevant information in the middle of lengthy
    contexts. Moreover, as the input context becomes more extended, even models explicitly
    designed for longer contexts experience a substantial decrease in performance.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的 LLMs 能够处理长提示作为输入。然而，Liu 等人（[2023b](#bib.bib13)）的研究发现（见 [图 2](#S1.F2 "在
    1 中的 LLM 幻觉：介绍 ‣ 探索 LLM 幻觉与提示语言学细微差别的关系：可读性、正式性和具体性")），这些模型通常在相关信息位于输入上下文的开头或结尾时表现最佳。当需要访问长上下文中间的相关信息时，其性能显著下降。此外，随着输入上下文的延长，即使是专为长上下文设计的模型，也会经历显著的性能下降。
- en: '![Refer to caption](img/ec05b018c314a23eb18d3e85d3dce758.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ec05b018c314a23eb18d3e85d3dce758.png)'
- en: 'Figure 2: Empirical results in Liu et al. ([2023b](#bib.bib13)) show that the
    models tend to excel at utilizing pertinent information found at the very start
    or end of their input context, but their performance notably declines when they
    need to access and utilize information situated in the middle of their input context.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：Liu 等人（[2023b](#bib.bib13)）的实证结果显示，模型通常在利用输入上下文开头或结尾的相关信息时表现出色，但当需要访问和利用输入上下文中间的信息时，其性能显著下降。
- en: 'In this paper, our primary objective is to explore the impact of the key linguistic
    attributes of prompts on hallucinations generated in LLMs. The contributions are
    as follows: 1) We delineate the broad categories of hallucinations observed in
    LLMs, as discussed in [Section 2](#S2 "2 Types of Hallucination ‣ Exploring the
    Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability,
    Formality, and Concreteness"). 2) We construct and provide annotations for our
    dataset, which is derived from tweets related to New York Times events, as detailed
    in [Section 3](#S3 "3 Dataset and Annotation ‣ Exploring the Relationship between
    LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and
    Concreteness"). 3) We analyze the relationship between the primary linguistic
    aspects of prompts, such as their readability, formality, and concreteness, and
    the occurrence of hallucinations in LLMs, as discussed in [Section 4](#S4 "4 Linguistic
    Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们的主要目标是探索提示的关键语言属性对LLM生成的幻觉的影响。贡献如下：1）我们阐明了LLM中观察到的广泛幻觉类别，如[第2节](#S2 "2
    种幻觉 ‣ 探索LLM幻觉与提示语言细微差别之间的关系：可读性、正式性和具体性")所讨论。2）我们构建并提供了我们的数据集的注释，该数据集来源于与《纽约时报》事件相关的推文，如[第3节](#S3
    "3 数据集和注释 ‣ 探索LLM幻觉与提示语言细微差别之间的关系：可读性、正式性和具体性")中详细描述。3）我们分析了提示的主要语言方面，如其可读性、正式性和具体性，与LLM中幻觉的发生之间的关系，如[第4节](#S4
    "4 提示的语言属性 ‣ 探索LLM幻觉与提示语言细微差别之间的关系：可读性、正式性和具体性")中所讨论。
- en: 2 Types of Hallucination
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 种幻觉
- en: In this study, we explore the following four different categories of hallucination.
    Additionally, we offer examples for each case in which the hallucinated text is
    marked in red.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们探索了以下四种不同类别的幻觉。此外，我们提供了每种情况的示例，其中幻觉文本以红色标记。
- en: '1\. Person (P):'
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 1\. 人物 (P)：
- en: 'The issue of generating fictional characters is discussed in Ladhak et al.
    ([2023](#bib.bib10)) and [Table 1](#S2.T1 "In 2\. Location (L): ‣ 2 Types of Hallucination
    ‣ Exploring the Relationship between LLM Hallucinations and Prompt Linguistic
    Nuances: Readability, Formality, and Concreteness").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 生成虚构角色的问题在Ladhak等人（[2023](#bib.bib10)）和[表1](#S2.T1 "在 2\. 位置 (L)： ‣ 2 种幻觉 ‣
    探索LLM幻觉与提示语言细微差别之间的关系：可读性、正式性和具体性")中进行了讨论。
- en: '2\. Location (L):'
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2\. 位置 (L)：
- en: 'The case of generating fictional places is addressed in Ladhak et al. ([2023](#bib.bib10))
    and [Table 1](#S2.T1 "In 2\. Location (L): ‣ 2 Types of Hallucination ‣ Exploring
    the Relationship between LLM Hallucinations and Prompt Linguistic Nuances: Readability,
    Formality, and Concreteness").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 生成虚构地点的情况在Ladhak等人（[2023](#bib.bib10)）和[表1](#S2.T1 "在 2\. 位置 (L)： ‣ 2 种幻觉 ‣
    探索LLM幻觉与提示语言细微差别之间的关系：可读性、正式性和具体性")中进行了讨论。
- en: '| Original | Antoine Richard is a former athlete from France who mainly competed
    in the 100 metres. He was French 100 metre champion on 5 occasions, and also 200
    metre winner in 1985\. He also won the French 60 metres title 5 times as well.
    |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | Antoine Richard是一位来自法国的前运动员，主要比赛项目为100米。他曾5次获得法国100米冠军，并在1985年赢得了200米冠军。他还5次获得法国60米冠军。
    |'
- en: '| AI-generated | Athlete Naoki Tsukahara was born in Tokyo, Japan to a Japanese
    father and French mother. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| AI生成 | 运动员Naoki Tsukahara出生在日本东京，父亲是日本人，母亲是法国人。 |'
- en: 'Table 1: An example showing how imaginary places such as Tokyo and persons
    such as father and mother are hallucinated Ladhak et al. ([2023](#bib.bib10)).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：一个示例展示了虚构地点（如东京）和人物（如父亲和母亲）如何被Ladhak等人（[2023](#bib.bib10)）幻觉化。
- en: '3\. Number (N):'
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3\. 数字 (N)：
- en: 'Similarly, Varshney et al. ([2023](#bib.bib30)) delves into the generation
    of imaginary numbers, as shown in [Table 2](#S2.T2 "In 3\. Number (N): ‣ 2 Types
    of Hallucination ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，Varshney等人（[2023](#bib.bib30)）深入探讨了虚构数字的生成，如[表2](#S2.T2 "在 3\. 数字 (N)： ‣
    2 种幻觉 ‣ 探索LLM幻觉与提示语言细微差别之间的关系：可读性、正式性和具体性")中所示。
- en: '| Original | Freddie Frith. |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | Freddie Frith. |'
- en: '| --- | --- |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| AI-generated | He was born in London in 1929 and began his racing career
    in 1951. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| AI生成 | 他于1929年出生在伦敦，并于1951年开始了他的赛车生涯。 |'
- en: '| Fact | He was born in Grimsby in 1909 and began his career in 1930. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 事实 | 他于1909年出生在Grimsby，并于1930年开始他的职业生涯。 |'
- en: 'Table 2: Both years 1929 and 1951 are hallucinated.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：1929 年和 1951 年这两个年份都是虚构的。
- en: '4\. Acronym (A):'
  id: totrans-40
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '4\. 缩略词 (A):'
- en: 'Additionally, we investigate the potential role of acronyms in prompting the
    generation of inaccurate responses as illustrated in [Table 3](#S2.T3 "In 4\.
    Acronym (A): ‣ 2 Types of Hallucination ‣ Exploring the Relationship between LLM
    Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还调查了缩略词在促使生成不准确回应中的潜在作用，如 [表 3](#S2.T3 "在 4\. 缩略词 (A)：‣ 2 种虚构 ‣ 探索 LLM
    虚构与提示语言细微差别的关系：可读性、正式性和具体性") 中所示。
- en: '| Original | Robert used PDF for his math homework. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 原始 | 罗伯特使用 PDF 完成他的数学作业。 |'
- en: '| AI-generated | In the context you provided, “PDF” most likely refers to “Portable
    Document Format,” which is a common file format used for documents, including
    math homework assignments. PDF files are widely used for sharing documents because
    they preserve the formatting and layout across different devices and operating
    systems. Robert likely used a PDF file for his math homework to ensure that it
    could be easily shared and viewed by others without compatibility issues. |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| AI 生成的 | 在您提供的背景下，“PDF”很可能指的是“便携式文档格式”，这是一种用于文档的常见文件格式，包括数学作业。PDF 文件广泛用于共享文档，因为它们能够在不同的设备和操作系统之间保留格式和布局。罗伯特可能使用了
    PDF 文件来完成数学作业，以确保其他人能够轻松共享和查看，而不会出现兼容性问题。 |'
- en: 'Table 3: In the mathematical context, “PDF” stands for “Probability Density
    Function.”.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在数学上下文中，“PDF”代表“概率密度函数”。
- en: 3 Dataset and Annotation
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集和注释
- en: To conduct our empirical analysis, where we examine how linguistic properties
    affect hallucination, we create and annotate a hallucination dataset using the
    NYT tweets detailed in the following sections.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行我们的实证分析，我们创建并注释了一个虚构数据集，使用了以下部分详细描述的 NYT 推文。
- en: 3.1 New York Times News Tweets
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 《纽约时报》新闻推文
- en: 'We utilize a news dataset, specifically the New York Times (NYT) news events
    tweets [NYT](#bib.bib16) . We selected a total of 2,500 tweets. These news tweets
    serve as our source of factually accurate prompts, which are then presented to
    the fifteen Large Language Models (LLMs) described in [Section 3.2](#S3.SS2 "3.2
    Selection of LLMs ‣ 3 Dataset and Annotation ‣ Exploring the Relationship between
    LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and
    Concreteness").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了一个新闻数据集，特别是《纽约时报》（NYT）新闻事件推文 [NYT](#bib.bib16)。我们选择了 2,500 条推文。这些新闻推文作为我们事实准确提示的来源，然后呈现给
    [第 3.2 节](#S3.SS2 "3.2 LLMs 选择 ‣ 3 数据集和注释 ‣ 探索 LLM 虚构与提示语言细微差别的关系：可读性、正式性和具体性")
    中描述的十五种大型语言模型（LLMs）。
- en: 3.2 Selection of LLMs
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLMs 选择
- en: 'We have selected 15 contemporary LLMs that have consistently demonstrated outstanding
    performance across a wide spectrum of NLP tasks. These models include: (i) GPT-4
    OpenAI ([2023](#bib.bib18)) (ii) GPT-3.5 OpenAI ([2022](#bib.bib17)) (iii) GPT-3
    Brown et al. ([2020](#bib.bib2)) (iv) GPT-2 Radford et al. ([2019](#bib.bib22))
    (v) MPT Wang et al. ([2023](#bib.bib31)) (vi) OPT Zhang et al. ([2022](#bib.bib34))
    (vii) LLaMA Touvron et al. ([2023](#bib.bib29)) (viii) BLOOM Scao et al. ([2022](#bib.bib27))
    (ix) Alpaca Taori et al. ([2023](#bib.bib28)) (x) Vicuna Chiang et al. ([2023](#bib.bib4))
    (xi) Dolly databricks ([2023](#bib.bib5)) (xii) StableLM Liu et al. ([2023a](#bib.bib12))
    (xiii) XLNet Yang et al. ([2019](#bib.bib32)) (xiv) T5 Raffel et al. ([2020](#bib.bib23))
    (xv) T0 Deleu et al. ([2022](#bib.bib6)).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了 15 种在广泛的 NLP 任务中始终表现出色的现代 LLMs。这些模型包括：（i）GPT-4 OpenAI ([2023](#bib.bib18))（ii）GPT-3.5
    OpenAI ([2022](#bib.bib17))（iii）GPT-3 Brown 等 ([2020](#bib.bib2))（iv）GPT-2 Radford
    等 ([2019](#bib.bib22))（v）MPT Wang 等 ([2023](#bib.bib31))（vi）OPT Zhang 等 ([2022](#bib.bib34))（vii）LLaMA
    Touvron 等 ([2023](#bib.bib29))（viii）BLOOM Scao 等 ([2022](#bib.bib27))（ix）Alpaca
    Taori 等 ([2023](#bib.bib28))（x）Vicuna Chiang 等 ([2023](#bib.bib4))（xi）Dolly databricks
    ([2023](#bib.bib5))（xii）StableLM Liu 等 ([2023a](#bib.bib12))（xiii）XLNet Yang 等
    ([2019](#bib.bib32))（xiv）T5 Raffel 等 ([2020](#bib.bib23))（xv）T0 Deleu 等 ([2022](#bib.bib6))。
- en: 3.3 Annotation guidelines
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 注释指南
- en: For the purpose of annotating the 2,500 text snippets, we leveraged the services
    of Amazon Mechanical Turk (AMT) [Amazon](#bib.bib1) . Through this platform, we
    obtained annotations at the sentence level to identify the different four categories
    of hallucination.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了注释 2,500 个文本片段，我们利用了 Amazon Mechanical Turk (AMT) [Amazon](#bib.bib1) 的服务。通过这个平台，我们获得了句子级别的注释，以识别出四种不同的虚构类别。
- en: 3.4 Dataset statistics
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 数据集统计
- en: 'Following the annotation process, our dataset statistics for the hallucination
    categories are presented in [Table 4](#S3.T4 "In 3.4 Dataset statistics ‣ 3 Dataset
    and Annotation ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在注释过程之后，我们的数据集统计信息显示了幻觉类别的[表4](#S3.T4 "在3.4节 数据集统计 ‣ 3 数据集和注释 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")。
- en: '| Category | Hallucinated sentences |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 幻觉句子 |'
- en: '| --- | --- |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Person | 14850 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 人物 | 14850 |'
- en: '| Location | 13050 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 位置 | 13050 |'
- en: '| Number | 7275 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 数量 | 7275 |'
- en: '| Acronym | 1225 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 缩写 | 1225 |'
- en: '| Total | 36910 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 36910 |'
- en: 'Table 4: Hallucination dataset statistics'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：幻觉数据集统计
- en: 4 Linguistic Properties of the prompt
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提示的4个语言特性
- en: 'Linguistic properties refer to the various characteristics and attributes of
    language and its components. These properties encompass a wide range of aspects
    that help define and understand a language. Some fundamental linguistic properties
    include: syntactic, semantic, pragmatic, and lexical. Considering these characteristics,
    we will delve more deeply into the three primary linguistic subtleties in the
    forthcoming [Sections 4.1](#S4.SS1 "4.1 Readability ‣ 4 Linguistic Properties
    of the prompt ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness"), [4.2](#S4.SS2
    "4.2 Formality ‣ 4 Linguistic Properties of the prompt ‣ Exploring the Relationship
    between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality,
    and Concreteness") and [4.3](#S4.SS3 "4.3 Concreteness ‣ 4 Linguistic Properties
    of the prompt ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 语言特性指的是语言及其组成部分的各种特征和属性。这些特性涵盖了帮助定义和理解语言的广泛方面。一些基本的语言特性包括：句法、语义、语用和词汇。考虑到这些特性，我们将在接下来的[4.1节](#S4.SS1
    "4.1 可读性 ‣ 4 提示的语言特性 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")、[4.2节](#S4.SS2 "4.2 正式性
    ‣ 4 提示的语言特性 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")和[4.3节](#S4.SS3 "4.3 具体性 ‣ 4 提示的语言特性
    ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")中深入探讨三种主要的语言细微差别。
- en: 4.1 Readability
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 可读性
- en: Readability quantifies the ease with which a text can be comprehended. Several
    factors, including the text’s complexity, familiarity, legibility, and typography,
    collectively contribute to its readability.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 可读性量化了文本被理解的难易程度。多个因素，包括文本的复杂性、熟悉度、可读性和排版，共同影响其可读性。
- en: 'The Flesch Reading Ease Score (FRES) Flesch ([1948](#bib.bib7)) (see [Eq. 1](#S4.E1
    "In 4.1 Readability ‣ 4 Linguistic Properties of the prompt ‣ Exploring the Relationship
    between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality,
    and Concreteness")) is a measure of the readability of a text. It was developed
    to assess how easy or difficult a piece of text is to read and understand. The
    score is calculated based on two factors: (a) Sentence Length and (b) Word Complexity.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Flesch阅读容易得分（FRES）Flesch ([1948](#bib.bib7))（见[公式1](#S4.E1 "在4.1节 可读性 ‣ 4 提示的语言特性
    ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")）是一种测量文本可读性的指标。它旨在评估文本的阅读和理解难易程度。得分是基于两个因素计算的：（a）句子长度和（b）词汇复杂性。
- en: As shown in the following example, in the first sentence, the language is straightforward,
    and the sentence is easy to understand, resulting in a high readability score.
    In contrast, the second sentence contains complex vocabulary and lengthy phrasing,
    making it more challenging to comprehend, resulting in a lower readability score.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如下例所示，在第一个句子中，语言简单明了，句子易于理解，因此得分较高。相反，第二个句子包含复杂的词汇和冗长的表述，使其更难理解，因此得分较低。
- en: Easy Readability (High Flesch Reading Ease Score)
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 易读性（高Flesch阅读容易得分）
- en: 'Sentence: The sun rises in the east every morning.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 句子：太阳每天早上从东方升起。
- en: Difficult Readability (Low Flesch Reading Ease Score)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 难读性（低Flesch阅读容易得分）
- en: 'Sentence: The intricacies of quantum mechanics, as expounded upon by renowned
    physicists, continue to baffle even the most astute scholars.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 句子：著名物理学家阐述的量子力学的复杂性，至今仍令最敏锐的学者感到困惑。
- en: '|  |  $\text{FRES}=206.835-1.015\left(\frac{\text{ total words }}{\text{ total
    sentences }}\right)-84.6\left(\frac{\text{ total syllables }}{\text{ total words
    }}\right)$  |  | (1) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  |  $\text{FRES}=206.835-1.015\left(\frac{\text{ 总词数 }}{\text{ 总句数 }}\right)-84.6\left(\frac{\text{
    总音节数 }}{\text{ 总词数 }}\right)$  |  | (1) |'
- en: 'To investigate the impact of the readability of the prompt, we pose the following
    research questions:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了调查提示的可读性影响，我们提出了以下研究问题：
- en: RQ 1
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ 1
- en: How does the complexity of a prompt’s language or vocabulary affect the likelihood
    of hallucination in LLM-generated responses?
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示语言或词汇的复杂性如何影响LLM生成响应中的幻觉可能性？
- en: RQ 2
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ 2
- en: Does the length of a prompt impact the potential for hallucination, and how
    does the readability of a long versus a short prompt affect LLM behavior?
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示的长度是否影响幻觉的可能性，且长短提示的可读性如何影响LLM行为？
- en: RQ 3
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ 3
- en: How do different LLM architectures (e.g., GPT-3, GPT-4, etc.) respond to prompts
    of varying linguistic readability, and do they exhibit differences in hallucination
    tendencies?
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不同的LLM架构（例如GPT-3、GPT-4等）如何对不同语言可读性的提示作出反应，它们是否表现出幻觉倾向的差异？
- en: 4.2 Formality
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 正式性
- en: The formality of language refers to the degree of sophistication, decorum, or
    politeness conveyed by the choice of words, sentence structure, and overall tone
    in communication. It is a way to indicate the level of etiquette, respect, or
    professionalism in a given context.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 语言的正式性指的是通过选择的词汇、句子结构和整体语调传达的复杂性、礼仪或礼貌程度。这是一种指示给定情境中礼节、尊重或专业水平的方法。
- en: In the example given below, both sentences convey an identical message, yet
    the initial one carries significantly more formality. Such stylistic distinctions
    frequently exert a more significant influence on the reader’s comprehension of
    the sentence than the literal meaning itself Hovy ([1987](#bib.bib9)).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在下述示例中，尽管两个句子传达了相同的信息，但第一个句子的正式程度显著更高。这种风格上的差异往往对读者对句子的理解产生比字面意义更重要的影响（Hovy（[1987](#bib.bib9)））。
- en: Example of formality in sentences Pavlick and Tetreault ([2016](#bib.bib20))
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Pavlick和Tetreault（[2016](#bib.bib20)）中正式性的句子示例
- en: •
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Those recommendations were unsolicited and undesirable.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那些建议是不请自来的，并且是不受欢迎的。
- en: •
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: that’s the stupidest suggestion EVER.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 那是最愚蠢的建议。
- en: 'Formality (defined in Heylighen and Dewaele ([1999](#bib.bib8))) is calculated
    as given in [Eq. 2](#S4.E2 "In 4.2 Formality ‣ 4 Linguistic Properties of the
    prompt ‣ Exploring the Relationship between LLM Hallucinations and Prompt Linguistic
    Nuances: Readability, Formality, and Concreteness"):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '正式性（定义见Heylighen和Dewaele（[1999](#bib.bib8)））的计算方法如[Eq. 2](#S4.E2 "In 4.2 Formality
    ‣ 4 Linguistic Properties of the prompt ‣ Exploring the Relationship between LLM
    Hallucinations and Prompt Linguistic Nuances: Readability, Formality, and Concreteness")所示：'
- en: '|  | $\displaystyle\text{F}=(\text{noun frequency}+\text{adjective freq.}$
    |  | (2) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{F}=(\text{名词频率}+\text{形容词频率}$ |  | (2) |'
- en: '|  | $\displaystyle+\text{preposition freq.}+\text{article freq.}$ |  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle+\text{介词频率}+\text{冠词频率}$ |  |'
- en: '|  | - pronoun freq. - verb freq. |  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | - 代词频率 - 动词频率 |  |'
- en: '|  | $\displaystyle\text{- adverb freq.}\text{ - interjection freq.}+100)/2$
    |  |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{- 副词频率}\text{ - 感叹词频率}+100)/2$ |  |'
- en: To examine how the formality of the prompt influences the outcome, we ask the
    following research inquiries.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究提示的正式程度如何影响结果，我们提出了以下研究问题。
- en: RQ 1
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ 1
- en: How does the level of formality in prompts influence the likelihood of hallucination
    in responses generated by LLMs?
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示中的正式性水平如何影响LLM生成响应中的幻觉可能性？
- en: RQ 2
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ 2
- en: Are there specific categories of hallucination that are more prevalent in responses
    prompted with formal versus informal language?
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用正式与非正式语言提示的响应中是否存在更为普遍的幻觉类别？
- en: 4.3 Concreteness
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 具体性
- en: Concreteness assesses the extent to which a word represents a tangible or perceptible
    concept. As per the theory in Paivio ([2013](#bib.bib19)), it is suggested that
    concrete words are easier to process compared to abstract words. The degree of
    concreteness associated with each word is expressed using a 5-point rating scale
    that ranges from abstract to concrete.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 具体性评估一个词代表有形或可感知概念的程度。根据Paivio（[2013](#bib.bib19)）的理论，具体词汇比抽象词汇更容易处理。每个词的具体性程度使用从抽象到具体的5级评分量表进行表示。
- en: A concrete word receives a higher rating and pertains to something that physically
    exists in reality, i.e. one can directly experience it through senses (smell,
    taste, touch, hear, see) and actions. An abstract word receives a lower rating
    and refers to something that isn’t directly accessible through your senses or
    actions. Its meaning is dependent on language and is usually elucidated by employing
    other words since there’s no straightforward method for direct demonstration.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 具体词汇的评级较高，与现实中物理存在的事物相关，即可以通过感官（嗅觉、味觉、触觉、听觉、视觉）和动作直接体验。抽象词汇的评级较低，指的是那些不能通过感官或动作直接接触的事物。它的含义依赖于语言，通常通过使用其他词汇来阐明，因为没有直接展示的简单方法。
- en: Examples of concrete words
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 具体词汇示例
- en: Apple, Dog, Chair, Book, Water, Mountain, Car
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 苹果、狗、椅子、书、水、山、车
- en: Examples of abstract words
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 抽象词汇示例
- en: Justice, Love, Happiness, Courage, Friendship, Wisdom, Equality, Democracy
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 公正、爱、幸福、勇气、友谊、智慧、平等、民主
- en: 'Concreteness ratings for 37,058 individual English words and 2,896 two-word
    expressions (i.e., a total of 39,954) are provided in Brysbaert et al. ([2014](#bib.bib3)).
    Since these ratings are at the word level, we compute the concreteness of a sentence
    by taking an average as described in [Eq. 3](#S4.E3 "In 4.3 Concreteness ‣ 4 Linguistic
    Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 37,058个单独英语词汇和2,896个双词表达（即总共39,954个）的具体性评级在Brysbaert等（[2014](#bib.bib3)）中提供。由于这些评级是按词汇级别进行的，我们通过计算平均值来得出句子的具体性，如[公式
    3](#S4.E3 "在4.3具体性 ‣ 4 提示的语言学属性 ‣ 探索大型语言模型幻觉与提示语言学细微差别的关系：阅读性、正式性和具体性")所述。
- en: '|  | $\displaystyle\text{concreteness of a sentence containing $n$ tokens}=$
    |  | (3) |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{包含 $n$ 个标记的句子的具体性}=$ |  | (3) |'
- en: '|  | $\displaystyle\frac{\sum_{i=1}^{n}\text{concreteness rating}_{i}}{n}$
    |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\sum_{i=1}^{n}\text{concreteness rating}_{i}}{n}$
    |  |'
- en: In order to explore the influence of the prompt’s concreteness on the study,
    we present the following research questions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探讨提示的具体性对研究的影响，我们提出以下研究问题。
- en: RQ 1
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ 1
- en: How does the level of linguistic concreteness in a prompt impact the probability
    of hallucination in LLMs?
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示中的语言学具体性水平如何影响大型语言模型产生幻觉的概率？
- en: RQ 2
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ 2
- en: Do LLMs tend to hallucinate less when provided with prompts that include specific
    details and constraints?
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当提供包含具体细节和限制条件的提示时，大型语言模型是否更少产生幻觉？
- en: RQ 3
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RQ 3
- en: Are LLMs more prone to hallucination when given abstract or vague prompts compared
    to concrete and specific prompts?
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与具体和特定的提示相比，大型语言模型在面对抽象或模糊的提示时是否更容易产生幻觉？
- en: 5 Our findings
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 我们的发现
- en: 'To investigate how the linguistic characteristics of prompts affect the generation
    of hallucinations in LLMs, we initially define the ranges for three specific scores,
    as outlined in [Table 5](#S5.T5 "In 5 Our findings ‣ Exploring the Relationship
    between LLM Hallucinations and Prompt Linguistic Nuances: Readability, Formality,
    and Concreteness"). A comprehensive analysis of these findings is presented in
    the following sections.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究提示的语言学特征如何影响大型语言模型中幻觉的生成，我们首先定义了三个具体评分的范围，如[表 5](#S5.T5 "在5 我们的发现 ‣ 探索大型语言模型幻觉与提示语言学细微差别的关系：阅读性、正式性和具体性")中所述。以下部分将详细分析这些发现。
- en: '| Range $\rightarrow$ |  |  |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 范围 $\rightarrow$ |  |  |  |'
- en: '| Linguistic Aspect $\downarrow$ | Low | Mid | High |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 语言学方面 $\downarrow$ | 低 | 中 | 高 |'
- en: '| Readability | 0-30 | 31-70 | 71-100 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 阅读性 | 0-30 | 31-70 | 71-100 |'
- en: '| Formality | 0-30 | 31-70 | 71-100 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 正式性 | 0-30 | 31-70 | 71-100 |'
- en: '| Concreteness | 1-2.5 | 2.5-3.5 | 3.5-5 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 具体性 | 1-2.5 | 2.5-3.5 | 3.5-5 |'
- en: 'Table 5: Range(s) for three linguistic aspects of the prompt.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：提示语三种语言学方面的范围。
- en: 5.1 Effects of readability on hallucination in LLMs
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 阅读性对大型语言模型中的幻觉的影响
- en: 'The figure (see [Fig. 3](#S5.F3 "In 5.3 Effects of concreteness on hallucination
    in LLMs ‣ 5 Our findings ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness")) illustrates
    our empirical findings and the following are the main insights that address the
    research questions posed earlier in [Section 4.1](#S4.SS1 "4.1 Readability ‣ 4
    Linguistic Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 该图（见[图 3](#S5.F3 "在5.3 具体性对LLM幻觉的影响 ‣ 5 我们的发现 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")）说明了我们的实证发现，以下是回应[第4.1节](#S4.SS1
    "4.1 可读性 ‣ 4 提示的语言属性 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")中提出的研究问题的主要见解。
- en: Effects of readability on LLM’s
    hallucination: ➠ Prompts that are easier to
    read tend to have fewer instances of hallucinations. ➠ Some difficult-to-read
    prompts, but more formal also hallucinate less. ➠ Hence, the results regarding
    readability are somewhat uncertain, displaying a combination of findings.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Effects of readability on LLM’s
    hallucination: ➠ Prompts that are easier to
    read tend to have fewer instances of hallucinations. ➠ Some difficult-to-read
    prompts, but more formal also hallucinate less. ➠ Hence, the results regarding
    readability are somewhat uncertain, displaying a combination of findings.
- en: 5.2 Effects of formality on hallucination in LLMs
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 正式性对LLM幻觉的影响
- en: '[Fig. 4](#S5.F4 "In 5.3 Effects of concreteness on hallucination in LLMs ‣
    5 Our findings ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness") represents our
    empirical findings.The following points outline the primary insights that respond
    to the research queries introduced in [Section 4.2](#S4.SS2 "4.2 Formality ‣ 4
    Linguistic Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4](#S5.F4 "在5.3 具体性对LLM幻觉的影响 ‣ 5 我们的发现 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")
    代表了我们的实证发现。以下几点概述了回应[第4.2节](#S4.SS2 "4.2 正式性 ‣ 4 提示的语言属性 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")中提出的研究问题的主要见解。'
- en: Effects of formality on LLM’s
    hallucination: ➠ Formal language prompts typically
    exhibit a lower propensity for generating hallucinatory content. ➠ Our findings
    demonstrate how utilizing more formal prompts can address hallucinations in the
    Name and Location categories. ➠ The linguistic impacts of the prompts become more
    evident in LLMs such as GPT-4, OPT, and subsequent versions.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Effects of formality on LLM’s
    hallucination: ➠ Formal language prompts typically
    exhibit a lower propensity for generating hallucinatory content. ➠ Our findings
    demonstrate how utilizing more formal prompts can address hallucinations in the
    Name and Location categories. ➠ The linguistic impacts of the prompts become more
    evident in LLMs such as GPT-4, OPT, and subsequent versions.
- en: 5.3 Effects of concreteness on hallucination in LLMs
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 具体性对LLM幻觉的影响
- en: '[Fig. 5](#S5.F5 "In 5.3 Effects of concreteness on hallucination in LLMs ‣
    5 Our findings ‣ Exploring the Relationship between LLM Hallucinations and Prompt
    Linguistic Nuances: Readability, Formality, and Concreteness") shows our experimental
    results. The following section highlights the core insights that address the research
    inquiries introduced in [Section 4.3](#S4.SS3 "4.3 Concreteness ‣ 4 Linguistic
    Properties of the prompt ‣ Exploring the Relationship between LLM Hallucinations
    and Prompt Linguistic Nuances: Readability, Formality, and Concreteness").'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 5](#S5.F5 "在5.3 具体性对LLM幻觉的影响 ‣ 5 我们的发现 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")
    显示了我们的实验结果。以下部分突出介绍了回应[第4.3节](#S4.SS3 "4.3 具体性 ‣ 4 提示的语言属性 ‣ 探索LLM幻觉与提示语言细微差别的关系：可读性、正式性和具体性")中提出的研究问题的核心见解。'
- en: Effects
    of concreteness on LLM’s hallucination: ➠
    Prompts that use clearer and more specific language tend to generate fewer hallucinations.
    ➠ Our results show that incorporating more specific and concrete terms into the
    prompts effectively reduces hallucinations in the Number and Acronym categories.
    ➠ Just as we observed with our formality findings, the impact of concrete prompts
    becomes increasingly apparent in advanced LLMs like GPT-4, OPT, and their later
    iterations.![Refer to caption](img/91bba7c0cbd65e2f9ee7c86f6364a4ae.png)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 具体性对LLM的幻觉的影响：
    ➠ 使用更清晰和具体的语言的提示往往会产生更少的幻觉。 ➠ 我们的结果显示，将更具体和明确的术语纳入提示中有效减少了数字和缩写类别的幻觉。
    ➠ 正如我们在形式性的发现中观察到的那样，具体提示的影响在像GPT-4、OPT及其后续版本这样的高级LLM中变得越来越明显。![参见说明](img/91bba7c0cbd65e2f9ee7c86f6364a4ae.png)
- en: 'Figure 3: Hallucination vs Readability'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：幻觉与可读性
- en: '![Refer to caption](img/c902cdba6c42545024182f93dd339ed9.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c902cdba6c42545024182f93dd339ed9.png)'
- en: 'Figure 4: Hallucination vs Formality'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：幻觉与形式性
- en: '![Refer to caption](img/8ac4d2052f67912d7e8d2b47c0c96549.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8ac4d2052f67912d7e8d2b47c0c96549.png)'
- en: 'Figure 5: Hallucination vs Concreteness'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：幻觉与具体性
- en: 6 Conclusion
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: 'In this preliminary research study, we begin by categorizing the primary types
    of hallucinations present in LLMs. Subsequently, we compile our dataset by utilizing
    New York Times news tweets, aligning with these established categories. Language
    intricacies assume a crucial role in the comprehension of language. Therefore,
    we delve into the examination of three significant linguistic dimensions: readability,
    formality, and concreteness, and their potential influence on the occurrence of
    hallucinations in LLMs.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项初步研究中，我们首先将LLM中存在的主要幻觉类型进行分类。随后，我们通过利用《纽约时报》的新闻推文来编制数据集，以符合这些既定类别。语言的复杂性在语言理解中起着关键作用。因此，我们*深入探讨*了三个重要的语言维度：可读性、形式性和具体性，以及它们对LLM中幻觉发生的潜在影响。
- en: References
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Amazon. [Amazon mechanical turk](https://www.mturk.com/).
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) 亚马逊。 [Amazon mechanical turk](https://www.mturk.com/)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. [Language models are few-shot learners](https://arxiv.org/abs/2005.14165).
    *Advances in neural information processing systems*, 33:1877–1901.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等 (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等. 2020. [语言模型是少样本学习者](https://arxiv.org/abs/2005.14165)。*神经信息处理系统进展*，33:1877–1901。
- en: Brysbaert et al. (2014) Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman.
    2014. Concreteness ratings for 40 thousand generally known english word lemmas.
    *Behavior research methods*, 46:904–911.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brysbaert 等 (2014) Marc Brysbaert, Amy Beth Warriner, 和 Victor Kuperman. 2014.
    40,000 个普遍已知的英语词元的具体性评分。*行为研究方法*，46:904–911。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://vicuna.lmsys.org).'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang 等 (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
    Stoica, 和 Eric P. Xing. 2023. [Vicuna：一个开源聊天机器人以90%* chatgpt质量打动gpt-4](https://vicuna.lmsys.org)。
- en: databricks (2023) databricks. 2023. [Dolly](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: databricks (2023) databricks. 2023. [Dolly](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)。
- en: Deleu et al. (2022) Tristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg, Yoshua
    Bengio, Guillaume Lajoie, and Pierre-Luc Bacon. 2022. [Continuous-time meta-learning
    with forward mode differentiation](https://openreview.net/forum?id=57PipS27Km).
    In *The Tenth International Conference on Learning Representations, ICLR 2022,
    Virtual Event, April 25-29, 2022*. OpenReview.net.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deleu 等 (2022) Tristan Deleu, David Kanaa, Leo Feng, Giancarlo Kerg, Yoshua
    Bengio, Guillaume Lajoie, 和 Pierre-Luc Bacon. 2022. [连续时间元学习与前向模式微分](https://openreview.net/forum?id=57PipS27Km)。在*第十届国际学习表征会议，ICLR
    2022，虚拟事件，2022年4月25-29日*。OpenReview.net。
- en: 'Flesch (1948) R Flesch. 1948. A new readability yardstick journal of applied
    psychology 32: 221–233.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flesch (1948) R Flesch. 1948. 一种新的可读性标准。*应用心理学杂志*，32:221–233。
- en: 'Heylighen and Dewaele (1999) Francis Heylighen and Jean-Marc Dewaele. 1999.
    Formality of language: definition, measurement and behavioral determinants. *Interner
    Bericht, Center “Leo Apostel”, Vrije Universiteit Brüssel*, 4(1).'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heylighen 和 Dewaele (1999) Francis Heylighen 和 Jean-Marc Dewaele. 1999. 语言的形式性：定义、测量和行为决定因素。*内部报告，“Leo
    Apostel”中心，布鲁塞尔自由大学*，4(1)。
- en: Hovy (1987) Eduard Hovy. 1987. Generating natural language under pragmatic constraints.
    *Journal of Pragmatics*, 11(6):689–719.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hovy (1987) Eduard Hovy. 1987. 在语用约束下生成自然语言。*语用学期刊*，11(6):689–719。
- en: Ladhak et al. (2023) Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang,
    Dan Jurafsky, Kathleen Mckeown, and Tatsunori B Hashimoto. 2023. [When do pre-training
    biases propagate to downstream tasks? a case study in text summarization](https://aclanthology.org/2023.eacl-main.234.pdf).
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pages 3198–3211.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ladhak et al. (2023) Faisal Ladhak, Esin Durmus, Mirac Suzgun, Tianyi Zhang,
    Dan Jurafsky, Kathleen Mckeown, and Tatsunori B Hashimoto. 2023. [预训练偏见何时会传递到下游任务？文本摘要的案例研究](https://aclanthology.org/2023.eacl-main.234.pdf)。在
    *第17届欧洲计算语言学协会年会论文集*，第3198–3211页。
- en: 'Li et al. (2023) Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong
    Bing, Shafiq Joty, and Soujanya Poria. 2023. Chain of knowledge: A framework for
    grounding large language models with structured knowledge bases. *arXiv preprint
    arXiv:2305.13269*.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023) Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Lidong
    Bing, Shafiq Joty, and Soujanya Poria. 2023. Chain of knowledge: A framework for
    grounding large language models with structured knowledge bases. *arXiv 预印本 arXiv:2305.13269*。'
- en: Liu et al. (2023a) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. 2023a. [Is your code generated by chatgpt really correct? rigorous evaluation
    of large language models for code generation](https://arxiv.org/abs/2305.01210).
    *arXiv preprint arXiv:2305.01210*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. 2023a. [你的代码是由 ChatGPT 生成的真的正确吗？对大语言模型代码生成的严格评估](https://arxiv.org/abs/2305.01210)。*arXiv
    预印本 arXiv:2305.01210*。
- en: 'Liu et al. (2023b) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle:
    How language models use long contexts. *arXiv preprint arXiv:2307.03172*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2023b) Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape,
    Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle:
    How language models use long contexts. *arXiv 预印本 arXiv:2307.03172*。'
- en: Midjourney (2022) Midjourney. 2022. [https://www.midjourney.com](https://www.midjourney.com).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Midjourney (2022) Midjourney. 2022. [https://www.midjourney.com](https://www.midjourney.com)。
- en: 'Mündler et al. (2023) Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin
    Vechev. 2023. [Self-contradictory hallucinations of large language models: Evaluation,
    detection and mitigation](http://arxiv.org/abs/2305.15852).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mündler et al. (2023) Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin
    Vechev. 2023. [大语言模型的自相矛盾的幻觉：评估、检测和缓解](http://arxiv.org/abs/2305.15852)。
- en: (16) NYT. [https://www.nytimes.com/topic/company/twitter](https://www.nytimes.com/topic/company/twitter).
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) NYT. [https://www.nytimes.com/topic/company/twitter](https://www.nytimes.com/topic/company/twitter)。
- en: OpenAI (2022) OpenAI. 2022. [Introducing chatgpt](https://openai.com/blog/chatgpt).
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2022) OpenAI. 2022. [介绍 ChatGPT](https://openai.com/blog/chatgpt)。
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. [Gpt-4 技术报告](http://arxiv.org/abs/2303.08774)。
- en: 'Paivio (2013) Allan Paivio. 2013. Dual coding theory, word abstractness, and
    emotion: a critical review of kousta et al.(2011).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paivio (2013) Allan Paivio. 2013. 双编码理论、词汇抽象性与情感：对Kousta等人（2011）的批判性回顾。
- en: Pavlick and Tetreault (2016) Ellie Pavlick and Joel Tetreault. 2016. [An empirical
    analysis of formality in online communication](https://doi.org/10.1162/tacl_a_00083).
    *Transactions of the Association for Computational Linguistics*, 4:61–74.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pavlick and Tetreault (2016) Ellie Pavlick and Joel Tetreault. 2016. [在线沟通中的形式性实证分析](https://doi.org/10.1162/tacl_a_00083)。*计算语言学协会学报*,
    4:61–74。
- en: 'Peng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia
    Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check
    your facts and try again: Improving large language models with external knowledge
    and automated feedback. *arXiv preprint arXiv:2302.12813*.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Peng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia
    Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check
    your facts and try again: Improving large language models with external knowledge
    and automated feedback. *arXiv 预印本 arXiv:2302.12813*。'
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. [Language models are unsupervised multitask
    learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf).
    *OpenAI blog*, 1(8):9.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. [语言模型是无监督的多任务学习者](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)。*OpenAI
    博客*, 1(8):9。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf).
    *The Journal of Machine Learning Research*, 21(1):5485–5551.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter J Liu. 2020. [探索统一文本到文本变换器的迁移学习极限](https://www.jmlr.org/papers/volume21/20-074/20-074.pdf).
    *机器学习研究杂志*, 21(1):5485–5551.
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. 2022. [Hierarchical text-conditional image generation with clip
    latents](https://arxiv.org/abs/2204.06125). *arXiv preprint arXiv:2204.06125*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    和 Mark Chen. 2022. [基于 CLIP 潜在变量的分层文本条件图像生成](https://arxiv.org/abs/2204.06125).
    *arXiv 预印本 arXiv:2204.06125*.
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. [Zero-shot text-to-image
    generation](https://arxiv.org/abs/2102.12092). In *International Conference on
    Machine Learning*, pages 8821–8831\. PMLR.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, 和 Ilya Sutskever. 2021. [零-shot 文本到图像生成](https://arxiv.org/abs/2102.12092).
    见 *国际机器学习会议*, 页码 8821–8831\. PMLR.
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. 2022. [High-resolution image synthesis with latent diffusion
    models](https://arxiv.org/abs/2112.10752). In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 10684–10695.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, 和 Björn Ommer. 2022. [高分辨率图像合成与潜在扩散模型](https://arxiv.org/abs/2112.10752).
    见 *IEEE/CVF 计算机视觉与模式识别会议论文集*, 页码 10684–10695.
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. [Bloom: A 176b-parameter open-access multilingual
    language model](https://arxiv.org/abs/2211.05100). *arXiv preprint arXiv:2211.05100*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, 等. 2022. [Bloom: 一个 176b-参数的开放访问多语言模型](https://arxiv.org/abs/2211.05100).
    *arXiv 预印本 arXiv:2211.05100*.'
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. [Stanford
    alpaca: An instruction-following llama model](https://crfm.stanford.edu/2023/03/13/alpaca.html).
    [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023. [斯坦福阿帕卡：一个跟随指令的
    Llama 模型](https://crfm.stanford.edu/2023/03/13/alpaca.html). [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023. [Llama: Open and efficient foundation language models](https://arxiv.org/abs/2302.13971).
    *arXiv preprint arXiv:2302.13971*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, 和
    Guillaume Lample. 2023. [Llama：开放且高效的基础语言模型](https://arxiv.org/abs/2302.13971).
    *arXiv 预印本 arXiv:2302.13971*.
- en: 'Varshney et al. (2023) Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu
    Chen, and Dong Yu. 2023. A stitch in time saves nine: Detecting and mitigating
    hallucinations of llms by validating low-confidence generation. *arXiv preprint
    arXiv:2307.03987*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Varshney et al. (2023) Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu
    Chen, 和 Dong Yu. 2023. 一针见血：通过验证低置信度生成检测和减轻 LLM 的幻觉。 *arXiv 预印本 arXiv:2307.03987*.
- en: Wang et al. (2023) Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris,
    Huan Sun, and Yoon Kim. 2023. [Multitask prompt tuning enables parameter-efficient
    transfer learning](https://openreview.net/forum?id=Nk2pDtuhTq). In *The Eleventh
    International Conference on Learning Representations*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Zhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio Feris,
    Huan Sun, 和 Yoon Kim. 2023. [多任务提示调优实现参数高效的迁移学习](https://openreview.net/forum?id=Nk2pDtuhTq).
    见 *第十一届国际学习表征会议*.
- en: 'Yang et al. (2019) Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R
    Salakhutdinov, and Quoc V Le. 2019. [Xlnet: Generalized autoregressive pretraining
    for language understanding](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf).
    *Advances in neural information processing systems*, 32.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等（2019）Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov,
    和 Quoc V Le。2019年。[Xlnet: Generalized autoregressive pretraining for language
    understanding](https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf)。*神经信息处理系统的进展*，32。'
- en: Zhang et al. (2023) Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang
    Wang. 2023. [Mitigating language model hallucination with interactive question-knowledge
    alignment](http://arxiv.org/abs/2305.13669).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Shuo Zhang, Liangming Pan, Junzhou Zhao, 和 William Yang Wang。2023年。[Mitigating
    language model hallucination with interactive question-knowledge alignment](http://arxiv.org/abs/2305.13669)。
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. [Opt: Open pre-trained
    transformer language models](http://arxiv.org/abs/2205.01068).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等（2022）Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor
    Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura,
    Anjali Sridhar, Tianlu Wang, 和 Luke Zettlemoyer。2022年。[Opt: Open pre-trained transformer
    language models](http://arxiv.org/abs/2205.01068)。'
