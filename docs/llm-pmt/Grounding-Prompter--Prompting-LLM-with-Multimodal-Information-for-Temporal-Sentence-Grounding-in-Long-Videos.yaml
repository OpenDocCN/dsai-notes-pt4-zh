- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:47:20'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:47:20'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal
    Sentence Grounding in Long Videos'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Grounding-Prompter: 通过多模态信息提示LLM进行长视频中的时间句子定位'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17117](https://ar5iv.labs.arxiv.org/html/2312.17117)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.17117](https://ar5iv.labs.arxiv.org/html/2312.17117)
- en: Houlun Chen
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 侯伦陈
- en: Tsinghua University    Xin Wang
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 清华大学    王鑫
- en: Tsinghua University Corresponding author.    Hong Chen
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 清华大学 相关作者    陈红
- en: Tsinghua University    Zihan Song
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 清华大学    宋子涵
- en: Tsinghua University    Jia Jia^∗
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 清华大学    贾佳^∗
- en: Tsinghua University    Wenwu Zhu^∗
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 清华大学    朱文武^∗
- en: Tsinghua University
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 清华大学
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Temporal Sentence Grounding (TSG), which aims to localize moments from videos
    based on the given natural language queries, has attracted widespread attention.
    Existing works are mainly designed for short videos, failing to handle TSG in
    long videos, which poses two challenges: i) complicated contexts in long videos
    require temporal reasoning over longer moment sequences, and ii) multiple modalities
    including textual speech with rich information require special designs for content
    understanding in long videos. To tackle these challenges, in this work we propose
    a Grounding-Prompter method, which is capable of conducting TSG in long videos
    through prompting LLM with multimodal information. In detail, we first transform
    the TSG task and its multimodal inputs including speech and visual, into compressed
    task textualization. Furthermore, to enhance temporal reasoning under complicated
    contexts, a Boundary-Perceptive Prompting strategy is proposed, which contains
    three folds: i) we design a novel multiscale denoising Chain-of-Thought (CoT)
    to combine global and local semantics with noise filtering step by step, ii) we
    set up validity principles capable of constraining LLM to generate reasonable
    predictions following specific formats, and iii) we introduce one-shot In-Context-Learning
    (ICL) to boost reasoning through imitation, enhancing LLM in TSG task understanding.
    Experiments demonstrate the state-of-the-art performance of our Grounding-Prompter
    method, revealing the benefits of prompting LLM with multimodal information for
    TSG in long videos.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 时间句子定位（TSG），旨在根据给定的自然语言查询从视频中定位时刻，已引起广泛关注。现有的工作主要针对短视频，无法处理长视频中的TSG，这带来了两个挑战：i)
    长视频中的复杂背景需要对更长的时刻序列进行时间推理，以及 ii) 包含丰富信息的文本语音等多种模态需要特殊设计以理解长视频中的内容。为了解决这些挑战，在本工作中，我们提出了一种Grounding-Prompter方法，能够通过提示LLM多模态信息来进行长视频中的TSG。具体而言，我们首先将TSG任务及其包括语音和视觉在内的多模态输入转换为压缩的任务文本化。此外，为了增强在复杂背景下的时间推理，我们提出了一种边界感知提示策略，包含三个方面：i)
    我们设计了一种新型的多尺度去噪思维链（CoT），逐步结合全局和局部语义并进行噪声过滤，ii) 我们建立了有效性原则，能够约束LLM生成遵循特定格式的合理预测，iii)
    我们引入了一次性上下文学习（ICL），通过模仿来增强推理能力，提高LLM在TSG任务中的理解。实验表明，我们的Grounding-Prompter方法具有最先进的性能，揭示了通过多模态信息提示LLM在长视频中的TSG的好处。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/b25078f8969407b7d84ae72e71ce5514.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b25078f8969407b7d84ae72e71ce5514.png)'
- en: 'Figure 1: Comparison on technical roadmaps on TSG task.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: TSG任务的技术路线图比较。'
- en: Temporal Sentence Grounding (TSG) [[1](#bib.bib1), [16](#bib.bib16)] aims to
    localize a moment from an untrimmed video to match the given query, requiring
    methods to understand the temporal boundaries and contexts across videos and texts.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 时间句子定位（TSG）[[1](#bib.bib1), [16](#bib.bib16)]旨在从未剪辑的视频中定位一个时刻以匹配给定的查询，需要方法理解视频和文本中的时间边界和背景。
- en: 'Nevertheless, existing literature [[4](#bib.bib4), [16](#bib.bib16), [50](#bib.bib50),
    [14](#bib.bib14), [28](#bib.bib28), [31](#bib.bib31), [27](#bib.bib27), [23](#bib.bib23)]
    are mainly designed for short videos, failing to handle TSG in long videos which
    is very prevalent in practical scenarios covering movies, news, and courses etc.
    However, exploring TSG in long videos encounters the following challenges:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有文献[[4](#bib.bib4), [16](#bib.bib16), [50](#bib.bib50), [14](#bib.bib14),
    [28](#bib.bib28), [31](#bib.bib31), [27](#bib.bib27), [23](#bib.bib23)]主要针对短视频，无法处理长视频中的TSG，而长视频在实际场景中如电影、新闻和课程等非常普遍。然而，探索长视频中的TSG面临以下挑战：
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Long videos normally contain complicated contexts, which require temporal reasoning
    over longer moment sequences.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 长视频通常包含复杂的背景，这需要对较长的时刻序列进行时间推理。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Long videos such as movies may contain multiple modalities including textual
    speech with rich information, requiring special designs for content understanding.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 像电影这样的长视频可能包含多种模态，包括包含丰富信息的文本语音，这需要特别的设计来理解内容。
- en: 'On the one hand, it’s difficult to extend traditional TSG methods [[4](#bib.bib4),
    [16](#bib.bib16), [50](#bib.bib50), [14](#bib.bib14), [28](#bib.bib28), [31](#bib.bib31),
    [27](#bib.bib27), [23](#bib.bib23)] to long videos, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")(a). With huge amounts
    of parameters required, they involve high computational costs when fully trained
    on long-video datasets. Besides, traditional TSG methods suffer from fitting bias
    of specific datasets [[22](#bib.bib22), [52](#bib.bib52)], thereby exhibiting
    poor generalization to long moment sequences. Additionally, the incapability of
    capturing rich semantics from textual speeches prevent them from conducting TSG
    on long speech-intensive videos. On the other hand, recent trials on employing
    multimodal large language models for videos (MLLM-V) [[5](#bib.bib5), [55](#bib.bib55),
    [25](#bib.bib25), [32](#bib.bib32)] have sprung up with remarkable performance
    gain across several video tasks. However, these LLM based approaches fail to align
    the TSG task well with LLMs and thus show poor temporal reasoning ability, especially
    in long videos, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(b).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '一方面，将传统TSG方法[[4](#bib.bib4), [16](#bib.bib16), [50](#bib.bib50), [14](#bib.bib14),
    [28](#bib.bib28), [31](#bib.bib31), [27](#bib.bib27), [23](#bib.bib23)]扩展到长视频中是困难的，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")(a)所示。由于需要大量的参数，当在长视频数据集上完全训练时，它们涉及高计算成本。此外，传统TSG方法存在特定数据集的拟合偏差[[22](#bib.bib22),
    [52](#bib.bib52)]，因此对长时间序列的泛化能力较差。此外，无法从文本语音中捕获丰富语义也阻碍了它们在长语音密集视频中进行TSG。另一方面，最近对多模态大型语言模型用于视频（MLLM-V）[[5](#bib.bib5),
    [55](#bib.bib55), [25](#bib.bib25), [32](#bib.bib32)]的尝试取得了显著的性能提升。然而，这些基于LLM的方法未能将TSG任务与LLMs对齐，因此表现出较差的时间推理能力，尤其是在长视频中，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")(b)所示。'
- en: 'To handle the challenges above, we reformulate TSG into a long-textual task
    and propose to empower large language models (LLMs) with the ability to conduct
    temporal reasoning under complicated context in long videos. Concretely, we propose
    a novel Grounding-Prompter method via prompting LLMs with speech and visual information
    to solve the TSG task, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")(c). First, to align LLMs with the TSG task, we transcribe
    speeches and caption the sparsely sampled frames that align speeches and scenes
    with temporal information in order to obtain compressed task textualization. Additionally,
    to enhance temporal reasoning, we propose a Boundary-Perceptive Prompting strategy,
    which consists of i) a multiscale denoising Chain-of-Thought (CoT) that combines
    global and local semantics with noise filtering step by step, ii) validity principles
    that constrain LLMs to generate reasonable predictions following specific formats,
    and iii) one-shot In-Context-Learning (ICL) that enhances LLMs in TSG understanding
    and temporal reasoning through imitation.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '为了应对上述挑战，我们将TSG重新定义为一个长文本任务，并提出赋予大型语言模型（LLMs）在复杂背景下进行时间推理的能力，特别是在长视频中。具体而言，我们提出了一种新颖的Grounding-Prompter方法，通过提供语音和视觉信息来提示LLMs以解决TSG任务，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")(c)所示。首先，为了使LLMs与TSG任务对齐，我们将语音转录并为稀疏采样的帧加上字幕，以对齐语音和场景的时间信息，从而获得压缩的任务文本化。此外，为了增强时间推理能力，我们提出了一种Boundary-Perceptive
    Prompting策略，其中包括i)一种多尺度去噪Chain-of-Thought (CoT)方法，它逐步结合全局和局部语义，并进行噪声过滤，ii)约束LLMs生成合理预测的有效性原则，iii)通过模仿来增强LLMs在TSG理解和时间推理方面的能力的单次In-Context-Learning
    (ICL)。'
- en: To verify the superiority of the proposed Grounding-Prompter, we establish a
    VidChapters-mini dataset for experiments from VidChapters-7M [[48](#bib.bib48)].
    Empirical results show that our Grounding-Prompter strategy achieves the state-of-the-art
    performance with great margins compared to other baseline methods. Ablation studies
    further validate the effectiveness of our designs, demonstrating that our Grounding-Prompter
    benefits from both textual speech and visual modalities when handling complicated
    and noisy long contexts around 10k tokens.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证所提出的 Grounding-Prompter 的优越性，我们从 VidChapters-7M 中建立了一个 VidChapters-mini
    数据集进行实验[[48](#bib.bib48)]。实证结果表明，我们的 Grounding-Prompter 策略相比其他基线方法在性能上取得了显著的领先。消融研究进一步验证了我们设计的有效性，证明在处理复杂且嘈杂的长上下文（约
    10k 个 token）时，我们的 Grounding-Prompter 从文本语音和视觉模态中都受益。
- en: 'To conclude, our contributions lie in the following folds:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献包括以下几个方面：
- en: '1.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We propose Grounding-Prompter, the first trial to address TSG in long videos
    through LLM, to the best of our knowledge.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了 Grounding-Prompter，这是我们所知的首个尝试通过 LLM 解决长视频中的 TSG 问题。
- en: '2.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We integrate textual speech and visual modalities to LLMs with compressed task
    textualization to handle TSG in long videos, where each modality significantly
    benefits the predictions.
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将文本语音和视觉模态整合到 LLM 中，并通过压缩的任务文本化来处理长视频中的 TSG，其中每种模态都显著有利于预测。
- en: '3.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We propose a novel Boundary-Perceptive Prompting strategy which enables LLMs
    to conduct temporal reason over time boundaries correctly under complicated and
    noisy long contexts around 10k tokens.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的边界感知提示策略，该策略使 LLM 能够在复杂且嘈杂的长上下文（约 10k 个 token）中正确地进行时间边界推理。
- en: '4.'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We establish a VidChapters-mini dataset and conduct extensive experiments to
    demonstrate the advantages of Grounding-Prompter over existing baseline methods.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建立了一个 VidChapters-mini 数据集，并进行了广泛的实验，以展示 Grounding-Prompter 相对于现有基线方法的优势。
- en: '![Refer to caption](img/f88e6b504a6f806695f0f518ac593b09.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f88e6b504a6f806695f0f518ac593b09.png)'
- en: 'Figure 2: Framework of our Grounding-Prompter. The task inputs are transformed
    into a compressed textualized representation to feed LLM. To enhance the temporal
    perception capability, a boundary-perceptive prompting strategy is proposed. Then,
    the task inputs are rephrased into a fluent prompt under this prompting strategy.
    LLM is activated to regulate its answer into JSON format and the predictions can
    be parsed automatically.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们的 Grounding-Prompter 框架。任务输入被转换为压缩的文本表示，以供 LLM 处理。为了增强时间感知能力，提出了一种边界感知的提示策略。然后，任务输入在该提示策略下被重新表述为流畅的提示。LLM
    被激活以将其答案调整为 JSON 格式，预测结果可以自动解析。
- en: 2 Related Works
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Temporal Sentence Grounding (TSG). Temporal Sentence Grounding [[1](#bib.bib1),
    [16](#bib.bib16)] aims to localize a moment from an untrimmed video that matches
    the given query. Most early TSG literature [[4](#bib.bib4), [16](#bib.bib16),
    [50](#bib.bib50), [54](#bib.bib54), [56](#bib.bib56), [59](#bib.bib59), [26](#bib.bib26),
    [34](#bib.bib34), [51](#bib.bib51), [53](#bib.bib53)] solves it in a supervised
    manner. Since they heavily rely on labor-intensive manual annotations, a few works [[10](#bib.bib10),
    [17](#bib.bib17), [33](#bib.bib33), [42](#bib.bib42), [7](#bib.bib7), [14](#bib.bib14),
    [28](#bib.bib28), [41](#bib.bib41), [31](#bib.bib31)] apply weak-supervised techniques
    on TSG, where locations of ground truth moments are unavailable during the training
    stage [[22](#bib.bib22)]. However, these specialized methods trained on specific
    datasets consume high computational resources and suffer from fitting bias [[22](#bib.bib22),
    [52](#bib.bib52)]. To alleviate these problems, many works [[27](#bib.bib27),
    [30](#bib.bib30), [47](#bib.bib47), [23](#bib.bib23)] resort to pretraining with
    diverse video-language tasks in a more unified framework to boost TSG. However,
    existing TSG methods mainly focus on short videos, leaving TSG in long videos
    largely unexplored, which usually involve temporal reasoning under much more complicated
    contexts.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 时间句子定位（TSG）。时间句子定位[[1](#bib.bib1)、[16](#bib.bib16)]旨在从未剪辑的视频中定位与给定查询匹配的时刻。大多数早期的TSG文献[[4](#bib.bib4)、[16](#bib.bib16)、[50](#bib.bib50)、[54](#bib.bib54)、[56](#bib.bib56)、[59](#bib.bib59)、[26](#bib.bib26)、[34](#bib.bib34)、[51](#bib.bib51)、[53](#bib.bib53)]以监督方式解决这个问题。由于它们严重依赖劳动密集型的人工标注，一些研究[[10](#bib.bib10)、[17](#bib.bib17)、[33](#bib.bib33)、[42](#bib.bib42)、[7](#bib.bib7)、[14](#bib.bib14)、[28](#bib.bib28)、[41](#bib.bib41)、[31](#bib.bib31)]在TSG上应用了弱监督技术，在训练阶段无法获得真实时刻的位置[[22](#bib.bib22)]。然而，这些专门的方法在特定数据集上训练，消耗了大量计算资源，并且存在拟合偏差[[22](#bib.bib22)、[52](#bib.bib52)]。为了缓解这些问题，许多研究[[27](#bib.bib27)、[30](#bib.bib30)、[47](#bib.bib47)、[23](#bib.bib23)]依赖于在更统一的框架中通过多样的视频-语言任务进行预训练来提升TSG。然而，现有的TSG方法主要集中在短视频上，长视频中的TSG尚未得到深入探索，这通常涉及到在更加复杂的背景下进行时间推理。
- en: Meanwhile, recent TSG works introduce more modalities from videos for better
    localization. A few literatures incorporate modalities, such as optical flows [[1](#bib.bib1),
    [9](#bib.bib9), [29](#bib.bib29)] and audio [[8](#bib.bib8), [30](#bib.bib30),
    [6](#bib.bib6)] besides the RGB frames. However, the speech modality has not been
    comprehensively explored, which sometimes contributes the most to localization,
    especially in long videos like news, courses, *etc*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，近期的TSG研究引入了更多来自视频的模态以实现更好的定位。一些文献结合了模态，如光流[[1](#bib.bib1)、[9](#bib.bib9)、[29](#bib.bib29)]和音频[[8](#bib.bib8)、[30](#bib.bib30)、[6](#bib.bib6)]，除了RGB帧。然而，语音模态尚未被全面探索，而语音模态有时对定位贡献最大，特别是在新闻、课程等长视频中。
- en: Therefore, in this paper, we solve TSG in long videos via LLM with both visual
    and speech modalities integrated.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，本文通过集成视觉和语音模态，利用LLM解决长视频中的TSG问题。
- en: Large Language Models (LLM). Recent advancement in Large Language Models (LLM) [[35](#bib.bib35),
    [3](#bib.bib3), [43](#bib.bib43), [11](#bib.bib11), [13](#bib.bib13)] has made
    a great difference for its remarkable abilities in Chain-of-Thought (CoT) [[44](#bib.bib44)],
    In-Context Learning (ICL) [[3](#bib.bib3)], *etc*. CoT decomposes complex tasks
    into a series of intermediate reasoning steps [[49](#bib.bib49)], while ICL encourages
    LLM to learn from analogy [[12](#bib.bib12)], empowering LLM with the capability
    of few-shot learning. For the purpose of handling multimodal tasks with its powerful
    reasoning ability, Multimodal Large Language Models (MLLM) [[49](#bib.bib49),
    [18](#bib.bib18), [58](#bib.bib58)] emerge, which take both multimodal features
    and texts as inputs. Some MLLM for videos (MLLM-V) literature [[5](#bib.bib5),
    [55](#bib.bib55), [25](#bib.bib25), [32](#bib.bib32)] project video feature sequences
    into the token embedding space of LLM to have LLM to understand videos. However,
    they fail to understand the TSG task well and are deficient in temporal perception.
    In contrast, we take an alternative approach in that we textualize video inputs
    with temporal marks and activate LLM to conduct TSG with our Boundary-Perceptive
    Prompting strategy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型 (LLM)。大型语言模型 (LLM)的最新进展 [[35](#bib.bib35), [3](#bib.bib3), [43](#bib.bib43),
    [11](#bib.bib11), [13](#bib.bib13)] 在连锁思维 (CoT) [[44](#bib.bib44)]、上下文学习 (ICL) [[3](#bib.bib3)]、*等*方面展现出了显著的能力。CoT
    将复杂任务分解为一系列中间推理步骤 [[49](#bib.bib49)]，而ICL则鼓励LLM通过类比学习 [[12](#bib.bib12)]，赋予LLM少样本学习的能力。为了处理多模态任务并利用其强大的推理能力，出现了多模态大型语言模型 (MLLM) [[49](#bib.bib49),
    [18](#bib.bib18), [58](#bib.bib58)]，它们同时接受多模态特征和文本作为输入。一些针对视频的MLLM (MLLM-V) 文献 [[5](#bib.bib5),
    [55](#bib.bib55), [25](#bib.bib25), [32](#bib.bib32)] 将视频特征序列投影到LLM的令牌嵌入空间中，以便LLM理解视频。然而，它们未能很好地理解TSG任务，并且在时间感知上存在不足。相反，我们采取了另一种方法，即通过时间标记对视频输入进行文本化，并激活LLM使用我们的Boundary-Perceptive
    Prompting策略进行TSG。
- en: Long Video Understanding. Handling long videos imposes substantial demands on
    computational resources and memories. Efficient sparse sampling is often a valid
    mechanism to better cover long video inputs with controllable memory usage. Such
    sampling strategies are mainly based on saliency [[20](#bib.bib20), [57](#bib.bib57)],
    adaptability [[46](#bib.bib46), [15](#bib.bib15)], or stochastics [[38](#bib.bib38)].
    Since there is much more redundancy in long videos, [[2](#bib.bib2)] removes a
    great number of irrelevant video segments via multimodal guidance. Since sampling
    inevitably loses some information, some works [[39](#bib.bib39), [45](#bib.bib45)]
    process several video slices and integrate them for global perception. Besides,
    information compression improves the perception coverage of videos with condensed
    representations. [[40](#bib.bib40)] designs a memory mechanism to combine short
    and long memories for long video question answering. We argue that textualization
    is also an efficient compression of videos in TSG, especially for the visual modality,
    thus making it possible to localize moments by LLM.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 长视频理解。处理长视频对计算资源和内存的需求极高。高效的稀疏采样通常是一种有效的机制，可以在可控的内存使用下更好地覆盖长视频输入。这些采样策略主要基于显著性 [[20](#bib.bib20),
    [57](#bib.bib57)]，适应性 [[46](#bib.bib46), [15](#bib.bib15)]，或随机性 [[38](#bib.bib38)]。由于长视频中存在大量冗余，[[2](#bib.bib2)]
    通过多模态指导去除了大量无关的视频片段。由于采样不可避免地会丢失一些信息，一些研究 [[39](#bib.bib39), [45](#bib.bib45)]
    处理多个视频片段并将它们整合以进行全局感知。此外，信息压缩通过浓缩表示提高了视频的感知覆盖范围。[[40](#bib.bib40)] 设计了一种记忆机制，将短期记忆和长期记忆结合用于长视频问答。我们认为，文本化也是TSG中视频的一种有效压缩方式，特别是对于视觉模态，从而使得通过LLM定位时刻成为可能。
- en: '3 Proposed Method: Grounding-Prompter'
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 提议的方法：Grounding-Prompter
- en: 'We elaborate on the details of our proposed Grounding-Prompter method in this
    section (Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Grounding-Prompter: Prompting
    LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos")).
    After formulating this problem (Section [3.1](#S3.SS1 "3.1 Problem Formulation
    ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos")), we first
    transform the TSG task and its multimodal inputs into compressed representations
    to feed LLM (Section [3.2](#S3.SS2 "3.2 Compressed Task Textualization ‣ 3 Proposed
    Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")). Then, to further
    activate LLM in temporal reasoning, the Boundary-Perceptive Prompting strategy
    is designed (Section [3.3](#S3.SS3 "3.3 Boundary-Perceptive Prompting ‣ 3 Proposed
    Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")), where we feed LLM
    with additional Multiscale Denoising Chain-of-Thought (CoT) (Section [3.3.1](#S3.SS3.SSS1
    "3.3.1 Multiscale Denoising Chain-of-Thought ‣ 3.3 Boundary-Perceptive Prompting
    ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos")), Validity
    Principles (Section [3.3.2](#S3.SS3.SSS2 "3.3.2 Validity Principles ‣ 3.3 Boundary-Perceptive
    Prompting ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting
    LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos")),
    and One-Shot In-Context-Learning (ICL) (Section [3.3.3](#S3.SS3.SSS3 "3.3.3 One-Shot
    In-Context-Learning ‣ 3.3 Boundary-Perceptive Prompting ‣ 3 Proposed Method: Grounding-Prompter
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")). With the above, the LLM gives the prediction via
    step-by-step reasoning.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '本节详细阐述了我们提出的Grounding-Prompter方法的细节（图[2](#S1.F2 "图 2 ‣ 1 引言 ‣ Grounding-Prompter:
    使用多模态信息提示LLM进行长视频中的时间句子定位")）。在表述这个问题（第[3.1](#S3.SS1 "3.1 问题表述 ‣ 3 提议的方法：Grounding-Prompter
    ‣ Grounding-Prompter: 使用多模态信息提示LLM进行长视频中的时间句子定位")）后，我们首先将TSG任务及其多模态输入转换为压缩表示，以便输入LLM（第[3.2](#S3.SS2
    "3.2 压缩任务文本化 ‣ 3 提议的方法：Grounding-Prompter ‣ Grounding-Prompter: 使用多模态信息提示LLM进行长视频中的时间句子定位")）。接下来，为了进一步激活LLM在时间推理中的能力，我们设计了Boundary-Perceptive
    Prompting策略（第[3.3](#S3.SS3 "3.3 边界感知提示 ‣ 3 提议的方法：Grounding-Prompter ‣ Grounding-Prompter:
    使用多模态信息提示LLM进行长视频中的时间句子定位")），在该策略中，我们向LLM提供额外的Multiscale Denoising Chain-of-Thought（CoT）（第[3.3.1](#S3.SS3.SSS1
    "3.3.1 多尺度去噪思维链 ‣ 3.3 边界感知提示 ‣ 3 提议的方法：Grounding-Prompter ‣ Grounding-Prompter:
    使用多模态信息提示LLM进行长视频中的时间句子定位")）、Validity Principles（第[3.3.2](#S3.SS3.SSS2 "3.3.2
    有效性原则 ‣ 3.3 边界感知提示 ‣ 3 提议的方法：Grounding-Prompter ‣ Grounding-Prompter: 使用多模态信息提示LLM进行长视频中的时间句子定位")）和One-Shot
    In-Context-Learning（ICL）（第[3.3.3](#S3.SS3.SSS3 "3.3.3 一次性上下文学习 ‣ 3.3 边界感知提示 ‣
    3 提议的方法：Grounding-Prompter ‣ Grounding-Prompter: 使用多模态信息提示LLM进行长视频中的时间句子定位")）。以上方法使LLM能够通过逐步推理给出预测。'
- en: 3.1 Problem Formulation
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: Given a video $V$. Usually, $N_{s},N_{c}$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个视频 $V$。通常，$N_{s},N_{c}$。
- en: 3.2 Compressed Task Textualization
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 压缩任务文本化
- en: To transform the TSG task and its multimodal inputs into texts that LLM can
    take in, we design the compressed task textualization pipeline.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将TSG任务及其多模态输入转换为LLM可以处理的文本，我们设计了压缩任务文本化管道。
- en: 'To align the behaviors of LLM with the TSG task, we explain the meaning of
    TSG and formulate the format of its inputs and outputs to LLM, shown in Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(1). Then, to have LLM understand multimodal inputs, shown in Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(2-4), we textualize speeches and visual modalities into transcriptions
    and captions with temporal marks. We argue that this textualization preserves
    enough semantics for localization, thereby an efficiently compressed representation
    of long videos, which is verified in our experiments (Section [4](#S4 "4 Experiments
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '为了使LLM的行为与TSG任务对齐，我们解释了TSG的含义，并制定了其输入和输出的格式，如第[3.4](#S3.SS4 "3.4 Prompt Example
    ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos")(1)节所示。然后，为了让LLM理解多模态输入，如第[3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(2-4)节所示，我们将演讲和视觉模态转化为带有时间标记的文本和字幕。我们认为这种文本化保留了足够的语义以进行定位，因此是一种高效压缩长视频的表示，这在我们的实验中得到了验证（第[4](#S4
    "4 Experiments ‣ Grounding-Prompter: Prompting LLM with Multimodal Information
    for Temporal Sentence Grounding in Long Videos")节）。'
- en: Speeches are transcribed into non-overlapped sentences that are temporally partitioned
    via Automatic Speech Recognition (ASR). Since huge amounts of frames in long videos
    contain much redundancy, a straightforward but effective sampling strategy is
    adopted, where we pre-detect scene transformations and only sample frames in alignment
    with scenes and speeches. The intermediate frame for each piece of transcriptions
    and each scene is selected, *i.e*. the frame at time ${(t_{s(s)}^{(i)}+t_{s(e)}^{(i)})}/{2}$.
    Captions are only generated on these sampled frames. After that, we feed transcriptions
    and captions with the query text to LLM as the task inputs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 演讲内容被转录为不重叠的句子，这些句子通过自动语音识别（ASR）进行时间分区。由于长视频中的大量帧包含许多冗余信息，因此采用了一种直接但有效的采样策略，我们预先检测场景变换，只采样与场景和演讲对齐的帧。每段转录和每个场景的中间帧被选定，即时间
    ${(t_{s(s)}^{(i)}+t_{s(e)}^{(i)})}/{2}$ 的帧。仅在这些采样帧上生成字幕。然后，我们将转录和字幕与查询文本一起输入到LLM中，作为任务输入。
- en: We note that it’s possible that the visual captions could be quite noisy, however,
    the information of the captions is still able to benefit moment localization,
    which is proved in our experiments.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到视觉字幕可能会有很大的噪声，但这些字幕的信息仍然能够有助于时刻定位，这在我们的实验中得到了证明。
- en: 3.3 Boundary-Perceptive Prompting
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 边界感知提示
- en: Since mainstream generation tasks in LLM hardly consider temporal boundary perception
    under complicated long contexts, it’s difficult for LLM to accurately follow the
    TSG task. Additionally, LLM outputs free-form responses, making it possible to
    generate unreasonable predictions or even fail to provide predictions. To handle
    them, we propose the Boundary-Perceptive Prompting strategy. First, we design
    the Multiscale Denoising Chain-of-Thought to enhance temporal boundary perception
    by reasoning step-by-step under long and noisy contexts. Moreover, several validity
    principles are introduced to regularize the answer of LLM. Besides, we integrate
    one-shot ICL as LLMs are few-shot learners [[3](#bib.bib3)].
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于主流的大型语言模型（LLM）在处理复杂的长上下文时几乎不会考虑时间边界感知，因此LLM很难准确地执行TSG任务。此外，LLM输出自由格式的响应，可能会生成不合理的预测，甚至无法提供预测。为了解决这些问题，我们提出了边界感知提示策略。首先，我们设计了多尺度去噪链式思维（Multiscale
    Denoising Chain-of-Thought），通过在长且嘈杂的上下文中逐步推理来增强时间边界感知。此外，引入了几个有效性原则来规范LLM的答案。此外，我们将一-shot
    ICL（即“一次性学习”）集成到LLM中，因为LLM是少量学习者[[3](#bib.bib3)]。
- en: 3.3.1 Multiscale Denoising Chain-of-Thought
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 多尺度去噪链式思维
- en: We decompose our Multiscale Denoising CoT design into the following steps to
    combine global and local semantics with noise resistance when generating predictions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将多尺度去噪链式思维（Multiscale Denoising CoT）设计分解为以下步骤，以结合全局和局部语义以及噪声抗性，从而生成预测。
- en: 'Step 1: Global Understanding. We have LLM to summarize the whole video, just
    shown in Section [3.4](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")(5), to filter detailed redundancy and unnecessary repetition
    with global high-level semantics preserved.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤1：整体理解。我们让LLM总结整个视频，如第[3.4节](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed Method:
    Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with Multimodal Information
    for Temporal Sentence Grounding in Long Videos")(5)所示，以筛选详细的冗余和不必要的重复，同时保留全球高层语义。'
- en: 'Step 2: Noise Evaluation. Since visual captions usually contain larger amounts
    of noise compared to speech transcriptions, thus, as illustrated in Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(6), we prompt LLM this observation and instruct LLM to evaluate how the
    captions assist in moment localization and balance visual-speech information gaps
    adaptively.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤2：噪声评估。由于视觉字幕通常包含比语音转录更多的噪声，因此，如第[3.4节](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed
    Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")(6)所示，我们提示LLM这一观察结果，并指导LLM评估字幕如何辅助时刻定位，并自适应地平衡视觉-语音信息的差距。'
- en: 'Step 3: Partition Understanding. To encourage LLM to understand local details
    in both matched and mismatched moments, as shown in Section [3.4](#S3.SS4 "3.4
    Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting
    LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos")(7),
    we have LLM to partition the video by the yet-to-be-predicted start-and-end timestamps
    and summarize each partition with the given query as conditions, to inspire LLM
    to capture differences relevant to query among several parts.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '步骤3：分段理解。为了鼓励LLM理解匹配和不匹配时刻的局部细节，如第[3.4节](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed
    Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with Multimodal
    Information for Temporal Sentence Grounding in Long Videos")(7)所示，我们让LLM根据尚未预测的开始和结束时间戳对视频进行分段，并以给定查询作为条件总结每个分段，以激励LLM捕捉在多个部分中与查询相关的差异。'
- en: 'Step 4: Prediction. Finally, we have LLM to predict the timestamps $(\hat{t}_{s},\hat{t}_{e})$
    according to the reasoning steps above.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤4：预测。最后，我们让LLM根据上述推理步骤预测时间戳$(\hat{t}_{s},\hat{t}_{e})$。
- en: 3.3.2 Validity Principles
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 有效性原则
- en: We set up the following three validity principles to constrain the behaviors
    of LLM and ensure that a reasonable prediction can be obtained.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们制定了以下三个有效性原则来约束LLM的行为，并确保能够获得合理的预测。
- en: 'Format Compliance. To better improve the LLM’s compliance with our multiscale
    denoising CoT, we devise an answering template in JSON, whose keys cover the reasoning
    steps in Section [3.3.1](#S3.SS3.SSS1 "3.3.1 Multiscale Denoising Chain-of-Thought
    ‣ 3.3 Boundary-Perceptive Prompting ‣ 3 Proposed Method: Grounding-Prompter ‣
    Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos"), demonstrated in Section [3.4](#S3.SS4 "3.4 Prompt
    Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting
    LLM with Multimodal Information for Temporal Sentence Grounding in Long Videos")(8).
    With format compliance principles, the answers of LLM would be parsed automatically.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '格式合规性。为了更好地提高LLM对我们多尺度去噪链式思维（CoT）的符合性，我们设计了一个JSON格式的回答模板，其键涵盖了第[3.3.1节](#S3.SS3.SSS1
    "3.3.1 Multiscale Denoising Chain-of-Thought ‣ 3.3 Boundary-Perceptive Prompting
    ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos")中的推理步骤，在第[3.4节](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(8)中进行了演示。通过格式合规性原则，LLM的回答将会被自动解析。'
- en: 'Answer Regularization. To avoid improper predictions, as Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(9) shows, we inform LLM to regularize its answer. First, there exists
    only one appropriate moment given a query. Furthermore, $\hat{t}_{s}<\hat{t}_{e}$
    must be ensured for each prediction.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '答案规范化。为了避免不正确的预测，如第[3.4节](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed Method:
    Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with Multimodal Information
    for Temporal Sentence Grounding in Long Videos")(9)所示，我们通知LLM对其答案进行规范化。首先，给定一个查询，只有一个合适的时刻。进一步地，必须确保每个预测满足$\hat{t}_{s}<\hat{t}_{e}$。'
- en: 'Plagiarism Prohibition. To prevent LLM from copying the prediction of the given
    example, therefore, we emphasize that LLM should imitate the format and reasoning
    steps from the example, rather than just copying the prediction, referring to
    Section [3.4](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")(10).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '禁止抄袭。为了防止LLM复制给定示例的预测，因此，我们强调LLM应该模仿示例中的格式和推理步骤，而不是仅仅复制预测，参见第[3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")节(10)。'
- en: 3.3.3 One-Shot In-Context-Learning
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 单例上下文学习
- en: 'With just a one-shot example introduced, illustrated in Section [3.4](#S3.SS4
    "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(11), it is proven to significantly enhance temporal reasoning and format
    compliance. It’s fixed during inference for convenience.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '通过引入一个单例示例，如第[3.4](#S3.SS4 "3.4 Prompt Example ‣ 3 Proposed Method: Grounding-Prompter
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos")节(11)所述，证明显著提升了时间推理和格式合规性。为了方便起见，在推理过程中是固定的。'
- en: '| Dataset | DiDeMo [[1](#bib.bib1)] | Charades-STA [[16](#bib.bib16)] | ActivityNet
    Captions [[21](#bib.bib21)] | TACoS [[16](#bib.bib16)] | VidChapters-mini |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | DiDeMo [[1](#bib.bib1)] | Charades-STA [[16](#bib.bib16)] | ActivityNet
    Captions [[21](#bib.bib21)] | TACoS [[16](#bib.bib16)] | VidChapters-mini |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Ave Duration (min) | 0.49 | 0.51 | 1.96 | 4.78 | 13.96 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 平均时长（分钟） | 0.49 | 0.51 | 1.96 | 4.78 | 13.96 |'
- en: '| Modalities | V+A | V+A | V+A | V | V+A+S |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 模态 | V+A | V+A | V+A | V | V+A+S |'
- en: 'Table 1: Statistical Comparison between VidChapters-mini and other TSG benchmark
    datasets, where V, A, and S denote visual, audio, and speech modalities.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：VidChapters-mini与其他TSG基准数据集的统计比较，其中V、A和S分别表示视觉、音频和语音模态。
- en: 3.4 Prompt Example
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 提示示例
- en: We present a prompt example in this section to better explain the details of
    our prompt design.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中展示了一个提示示例，以更好地解释我们的提示设计细节。
- en: '(1) Task Description & Formulation: You can analyze the correlations between
    a video and query, and locate the video segment that matches the query. You are
    given: (1) Video title (2) Query (3) Speech transcription, with temporal information
    in the format of: [START-TIMESTAMP]-[END-TIMESTAMP]:[TRANSCRIPTION] (4) Visual
    caption, with temporal information in the format of: [TIMESTAMP]:[CAPTION]. You
    should give the answer in [X, Y] format where X, Y are the start and end timestamps
    of the matching segment.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 任务描述与公式化：你可以分析视频和查询之间的相关性，并定位匹配查询的视频片段。你将获得：（1）视频标题（2）查询（3）演讲记录，时间格式为：[START-TIMESTAMP]-[END-TIMESTAMP]:[TRANSCRIPTION]（4）视觉字幕，时间格式为：[TIMESTAMP]:[CAPTION]。你应该以[X,
    Y]格式给出答案，其中X和Y是匹配片段的开始和结束时间戳。
- en: '(2) Query: Habit 2: Build other people up'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 查询：习惯2：提升他人
- en: '(3) Speech Transcriptions: 0-7: While watching clips from my last Game of Thrones
    video…'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: (3) 演讲记录：0-7：在观看我上一个《权力的游戏》视频片段时…
- en: '(4) Visual Captions: 5: A woman with long blonde hair…'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: (4) 视觉字幕：5：一位拥有长金发的女性…
- en: '(5) Global Understanding: You summarize the video.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: (5) 全球理解：你总结视频内容。
- en: '(6) Noise Evaluation: We note that the visual caption might be quite NOISY.
    Now you comment if the visual captions are helpful enough for localization. You
    can give up information from captions if you think some of them are wrong.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: (6) 噪声评估：我们注意到视觉字幕可能会非常“嘈杂”。现在你可以评论视觉字幕是否足够有助于定位。如果你认为其中一些字幕是错误的，你可以忽略这些信息。
- en: '(7) Partition Understanding: You analyze the video content before X, between
    X and Y, and after Y, respectively. After that, you give the answer [X, Y].'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: (7) 分段理解：你分别分析X之前、X和Y之间、以及Y之后的视频内容。然后，你给出答案[X, Y]。
- en: '(8) Format Compliance: Please use JSON format of {“summary”:“…”(you summarize
    the whole video),“comment”: “…”(you evaluate effectiveness of visual captions),
    “query”:“…”(the query input), “before X”: “…”(you summarize video before X), “between
    X and Y”: “…”(you summarize video between X and Y), “after Y”: “…”(you summarize
    video after Y), “answer”: [X, Y]}.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '(8) 格式合规：请使用JSON格式的 {“summary”:“…”（你总结整个视频），“comment”: “…”（你评价视觉字幕的有效性），“query”:“…”（查询输入），“before
    X”: “…”（你总结视频中X之前的内容），“between X and Y”: “…”（你总结视频中X和Y之间的内容），“after Y”: “…”（你总结视频中Y之后的内容），“answer”:
    [X, Y]}。'
- en: '(9) Answer Regularization: We ensure there does exist ONE moment matching the
    query and X is no more than Y.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: (9) 答案规范化：我们确保存在一个时刻匹配查询，并且 X 不超过 Y。
- en: '(10) Plagiarism Prohibition: You MUST NOT just copy the answer given by the
    example! X and Y should be replaced by the real start and end timestamps of the
    moment you find in videos.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: (10) 剽窃禁令：你绝对不能仅仅复制示例给出的答案！X 和 Y 应该替换为你在视频中找到的实际开始和结束时间戳。
- en: '(11) One-Shot In-Context-Learning: $<$'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: (11) 单次上下文学习：$<$
- en: 4 Experiments
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Datasets and Settings
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集和设置
- en: 'Datasets. We adopt the VidChapters-7M [[48](#bib.bib48)] dataset for TSG in
    long videos, which contains 817K open-domain YouTube videos with 7M user-annotated
    chapters, featuring longer duration and richer modalities with speeches compared
    with other TSG benchmark datasets. Considering the huge amount of videos with
    various durations in VidChapters-7M, we randomly select 3 chapter annotations
    for each video whose duration is 13-15 minutes in its test split, resulting in
    1830 query-moment pairs in the final test, called VidChapters-mini. Statistical
    comparison is shown in Table [1](#S3.T1 "Table 1 ‣ 3.3.3 One-Shot In-Context-Learning
    ‣ 3.3 Boundary-Perceptive Prompting ‣ 3 Proposed Method: Grounding-Prompter ‣
    Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos").'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们采用 VidChapters-7M [[48](#bib.bib48)] 数据集用于长视频中的 TSG，它包含 817K 开放域的 YouTube
    视频和 7M 用户注释的章节，相比其他 TSG 基准数据集，它具有更长的时长和更丰富的演讲模态。考虑到 VidChapters-7M 中的大量各种时长的视频，我们在测试集中的每个视频中随机选择
    3 个章节注释，结果在最终测试中产生了 1830 对查询时刻，称为 VidChapters-mini。统计比较见表 [1](#S3.T1 "表 1 ‣ 3.3.3
    单次上下文学习 ‣ 3.3 边界感知提示 ‣ 3 提议的方法：基础提示器 ‣ 基础提示器：使用多模态信息进行长视频的时间句子定位")。
- en: Evaluation Metrics. We follow the metrics “r@{$m$ seconds from the corresponding
    ground truth start-time. To measure the instruction-following ability of LLM in
    TSG, a new metric, collapse rate “cr”(%), is introduced, which is defined as the
    proportion of predictions failing to generate answers adhering to the prescribed
    format. If a collapse happens, the model is regarded as giving an invalid answer,
    with its IoU being 0.0 and the distance to the ground truth start-time being +inf.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 评估指标。我们遵循指标“r@{$m$ 秒，从相应的真实起始时间”。为了衡量 LLM 在 TSG 中的指令跟随能力，引入了一个新指标，崩溃率“cr”（%），定义为未生成符合规定格式的答案的预测比例。如果发生崩溃，则模型被视为给出了无效答案，其
    IoU 为 0.0，距离真实起始时间为 +inf。
- en: Implementation Details. We apply GPT-3.5-turbo-16k ¹¹1[https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)
    as the LLM model. The video scenes are detected by PySceneDetect ²²2[https://www.scenedetect.com/](https://www.scenedetect.com/)
    tool based on content transition, and the sampled frames are captioned by BLIP [[24](#bib.bib24)]
    model. For speech transcriptions, we take the Whisper-based [[37](#bib.bib37)]
    tools to finish ASR following [[48](#bib.bib48)]. The temperature of LLM is set
    to 0.0 in all experiments for reproducibility and we set the minimum number of
    sampled frames $\overline{N_{c}}$ to 100.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节。我们使用 GPT-3.5-turbo-16k¹¹1[https://platform.openai.com/docs/models/gpt-3-5](https://platform.openai.com/docs/models/gpt-3-5)
    作为 LLM 模型。视频场景由 PySceneDetect²²2[https://www.scenedetect.com/](https://www.scenedetect.com/)
    工具根据内容过渡检测，采样帧由 BLIP [[24](#bib.bib24)] 模型进行标注。对于语音转录，我们采用基于 Whisper 的 [[37](#bib.bib37)]
    工具进行 ASR，遵循 [[48](#bib.bib48)]。为了确保实验的可重复性，我们将 LLM 的温度设置为 0.0，并将采样帧的最小数量 $\overline{N_{c}}$
    设置为 100。
- en: '| Methods | Modalities | r@0.3 | r@0.5 | r@0.7 | r@0.9 | mIoU | r@1s | r@3s
    | r@5s | r@10s | cr |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 模态 | r@0.3 | r@0.5 | r@0.7 | r@0.9 | mIoU | r@1s | r@3s | r@5s | r@10s
    | cr |'
- en: '| Random | - | 13.10 | 5.36 | 1.67 | 0.19 | 10.31 | 0.37 | 0.83 | 1.38 | 2.52
    | - |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 随机 | - | 13.10 | 5.36 | 1.67 | 0.19 | 10.31 | 0.37 | 0.83 | 1.38 | 2.52 |
    - |'
- en: '| Complete | - | 12.62 | 3.77 | 1.04 | 0.16 | 15.94 | 10.11 | 10.16 | 10.27
    | 11.04 | - |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 完整 | - | 12.62 | 3.77 | 1.04 | 0.16 | 15.94 | 10.11 | 10.16 | 10.27 | 11.04
    | - |'
- en: '| CLIP Zero-shot | V | 12.08 | 6.01 | 2.13 | 0.60 | 10.15 | 1.09 | 3.17 | 5.08
    | 7.65 | - |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| CLIP Zero-shot | V | 12.08 | 6.01 | 2.13 | 0.60 | 10.15 | 1.09 | 3.17 | 5.08
    | 7.65 | - |'
- en: '| BERT Zero-shot | S | 15.25 | 6.94 | 2.95 | 0.49 | 12.80 | 0.77 | 2.13 | 3.17
    | 4.59 | - |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| BERT Zero-shot | S | 15.25 | 6.94 | 2.95 | 0.49 | 12.80 | 0.77 | 2.13 | 3.17
    | 4.59 | - |'
- en: '| VideoChat | V+S | 1.20 | 0.38 | 0.05 | 0.00 | 1.48 | 3.61 | 4.27 | 4.76 |
    6.24 | 43.84 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| VideoChat | V+S | 1.20 | 0.38 | 0.05 | 0.00 | 1.48 | 3.61 | 4.27 | 4.76 |
    6.24 | 43.84 |'
- en: '| Video-ChatGPT | V+S | 1.20 | 0.60 | 0.05 | 0.05 | 1.21 | 2.57 | 3.01 | 3.55
    | 4.32 | 46.72 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Video-ChatGPT | V+S | 1.20 | 0.60 | 0.05 | 0.05 | 1.21 | 2.57 | 3.01 | 3.55
    | 4.32 | 46.72 |'
- en: '| Video-LLaMA | A+V+S | 0.60 | 0.24 | 0.12 | 0.06 | 1.16 | 2.60 | 3.50 | 3.98
    | 5.25 | 52.81 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Video-LLaMA | A+V+S | 0.60 | 0.24 | 0.12 | 0.06 | 1.16 | 2.60 | 3.50 | 3.98
    | 5.25 | 52.81 |'
- en: '| M-DETR Zero-shot | V | 14.59 | 6.34 | 2.30 | 0.33 | 11.68 | 1.15 | 2.51 |
    3.28 | 5.08 | - |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| M-DETR Zero-shot | V | 14.59 | 6.34 | 2.30 | 0.33 | 11.68 | 1.15 | 2.51 |
    3.28 | 5.08 | - |'
- en: '| M-DETR | V | 49.48 | 36.85 | 26.41 | 10.93 | 25.41 | 10.33 | 15.53 | 18.97
    | 27.99 | - |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| M-DETR | V | 49.48 | 36.85 | 26.41 | 10.93 | 25.41 | 10.33 | 15.53 | 18.97
    | 27.99 | - |'
- en: '| Ours | V+S | 34.81 | 22.95 | 14.92 | 6.28 | 26.81 | 17.60 | 25.41 | 32.02
    | 39.73 | 10.27 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | V+S | 34.81 | 22.95 | 14.92 | 6.28 | 26.81 | 17.60 | 25.41 | 32.02
    | 39.73 | 10.27 |'
- en: 'Table 2: Comparison with other technical roadmaps on VidChapters-mini, where
    Moment-DETR (M-DETR) Zero-shot is pretrained with visuals. A, V, and S are short
    for audio, visual, and speech modalities, which are the input modalities from
    videos. The best and second are highlighted by bold and underline.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：与VidChapters-mini上的其他技术路线图的比较，其中Moment-DETR (M-DETR) Zero-shot在视觉上进行预训练。A、V
    和 S 分别代表音频、视觉和语音模态，这些是视频中的输入模态。最佳和次佳结果通过粗体和下划线进行标记。
- en: '![Refer to caption](img/df65e980c0cbef8cb4c908ce87e96e18.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df65e980c0cbef8cb4c908ce87e96e18.png)'
- en: (a)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/e86a7e432c72eba1bbcf23c9e59202ca.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e86a7e432c72eba1bbcf23c9e59202ca.png)'
- en: (b)
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/f67b678a7f98fb8819fd568c427659bc.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f67b678a7f98fb8819fd568c427659bc.png)'
- en: (c)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 3: Comparisons between moment-DETR and its variants, where the green
    and orange bars are overlapped in the bottom to clearly illustrate the performance
    gaps. Moment-DETR still demonstrates comparable performance even some inputs are
    removed, showing that it largely suffers from fitting bias.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：moment-DETR及其变体之间的比较，其中绿色和橙色条形图在底部重叠，以清晰地说明性能差距。即使去除一些输入，moment-DETR仍展示出可比的性能，表明它在很大程度上受到了拟合偏差的影响。
- en: 4.2 Technical Roadmap Baselines
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 技术路线图基准
- en: 'We compare our method with three types of technical baselines: the rule-based,
    Multimodal Large Language Models for Videos (MLLM-V), and state-of-the-art TSG
    models.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法与三种技术基准进行比较：基于规则的方法、多模态大语言模型（MLLM-V），以及最先进的TSG模型。
- en: 'The Rule-Based. We implement four ones. (i) “Random”: We randomly select a
    pair $(\hat{t}_{s},\hat{t}_{e})$ to 0.05 in accordance with [[48](#bib.bib48)].'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的方法。我们实现了四种方法。(i) “随机”：我们随机选择一对 $(\hat{t}_{s},\hat{t}_{e})$ 以0.05为依据，详见
    [[48](#bib.bib48)]。
- en: MLLM-V. We compare with three methods, *i.e*. VideoChat [[25](#bib.bib25)],
    Video-ChatGPT [[32](#bib.bib32)], and Video-LLaMA [[55](#bib.bib55)] in this category.
    For a fair comparison, these MLLM-Vs take the whole video and its speech transcriptions
    as input. We sample as many frames as possible from videos to integrate more visual
    information and also feed them with a similar one-shot example if supported by
    the MLLM-V. For automatic purposes, format control is adopted as well in order
    to obtain regularized answers.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: MLLM-V。我们在这一类别中与三种方法进行比较，即 VideoChat [[25](#bib.bib25)]、Video-ChatGPT [[32](#bib.bib32)]
    和 Video-LLaMA [[55](#bib.bib55)]。为了公平比较，这些MLLM-V将整个视频及其语音转录作为输入。我们从视频中采样尽可能多的帧以整合更多的视觉信息，并在支持的情况下，还提供类似的一次性示例。为了自动化，我们还采用了格式控制，以获得规范化的答案。
- en: TSG Models. Furthermore, we also compare our method with the state-of-the-art
    TSG method, moment-DETR [[23](#bib.bib23)]. We test it in both training-free and
    training-based fashions. In the training-free fashion, we apply the checkpoint
    pretrained on QVHighlights [[23](#bib.bib23)] dataset and transfer it to VidChapters-mini.
    Additionally, in the training-based fashion, a moment-DETR is fully trained on
    the training split of VidChapters-7M. Additionally, we report three other results
    of this trained model, by clearing the query, setting visual features to zero,
    and implementing both of them to check the bias.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: TSG模型。此外，我们还将我们的方法与最先进的TSG方法moment-DETR [[23](#bib.bib23)]进行比较。我们在无训练和基于训练的情况下进行测试。在无训练情况下，我们应用在QVHighlights
    [[23](#bib.bib23)] 数据集上预训练的检查点，并将其迁移到VidChapters-mini。此外，在基于训练的情况下，我们在VidChapters-7M的训练分割上对moment-DETR进行了完全训练。此外，我们报告了该训练模型的其他三种结果，包括清除查询、将视觉特征设为零，以及同时实现这两者以检查偏差。
- en: '| Methods |  |  | r@0.3 | r@0.5 | r@0.7 | r@0.9 | mIoU | r@1s | r@3s | r@5s
    | r@10s | cr |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 |  |  | r@0.3 | r@0.5 | r@0.7 | r@0.9 | mIoU | r@1s | r@3s | r@5s | r@10s
    | cr |'
- en: '| Random |  |  | 13.10 | 5.36 | 1.67 | 0.19 | 10.31 | 0.37 | 0.83 | 1.38 |
    2.52 | - |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 随机 |  |  | 13.10 | 5.36 | 1.67 | 0.19 | 10.31 | 0.37 | 0.83 | 1.38 | 2.52
    | - |'
- en: '| Complete |  |  | 12.62 | 3.77 | 1.04 | 0.16 | 15.94 | 10.11 | 10.16 | 10.27
    | 11.04 | - |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 完成 |  |  | 12.62 | 3.77 | 1.04 | 0.16 | 15.94 | 10.11 | 10.16 | 10.27 | 11.04
    | - |'
- en: '| Ours | CoT | ICL |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | CoT | ICL |  |  |  |  |  |  |  |  |  |  |'
- en: '| ✘ | ✘ | 2.40 | 1.26 | 0.38 | 0.05 | 4.37 | 12.84 | 18.85 | 23.50 | 29.89
    | 0.00 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ✘ | ✘ | 2.40 | 1.26 | 0.38 | 0.05 | 4.37 | 12.84 | 18.85 | 23.50 | 29.89
    | 0.00 |'
- en: '| ✓ | ✘ | 12.73 | 6.99 | 3.33 | 0.98 | 11.56 | 16.39 | 23.61 | 29.62 | 37.27
    | 3.66 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✘ | 12.73 | 6.99 | 3.33 | 0.98 | 11.56 | 16.39 | 23.61 | 29.62 | 37.27
    | 3.66 |'
- en: '| ✘ | ✓ | 21.69 | 13.06 | 7.32 | 2.24 | 17.24 | 16.39 | 23.44 | 27.6 | 34.32
    | 0.00 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| ✘ | ✓ | 21.69 | 13.06 | 7.32 | 2.24 | 17.24 | 16.39 | 23.44 | 27.6 | 34.32
    | 0.00 |'
- en: '| Speech | Visual |  |  |  |  |  |  |  |  |  |  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 语音 | 视觉 |  |  |  |  |  |  |  |  |  |  |'
- en: '| ✓ | ✘ | 28.25 | 18.42 | 11.37 | 4.70 | 21.84 | 16.78 | 23.93 | 29.34 | 36.12
    | 18.03 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✘ | 28.25 | 18.42 | 11.37 | 4.70 | 21.84 | 16.78 | 23.93 | 29.34 | 36.12
    | 18.03 |'
- en: '| ✘ | ✓ | 20.22 | 10.49 | 5.14 | 1.15 | 15.17 | 5.90 | 10.27 | 12.40 | 15.68
    | 5.08 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ✘ | ✓ | 20.22 | 10.49 | 5.14 | 1.15 | 15.17 | 5.90 | 10.27 | 12.40 | 15.68
    | 5.08 |'
- en: '| ✓ | ✓ | 34.81 | 22.95 | 14.92 | 6.28 | 26.81 | 17.60 | 25.41 | 32.02 | 39.73
    | 10.27 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | 34.81 | 22.95 | 14.92 | 6.28 | 26.81 | 17.60 | 25.41 | 32.02 | 39.73
    | 10.27 |'
- en: 'Table 3: Ablation Studies on VidChapters-mini. The best and the second are
    highlighted by bold and underline, respectively.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：对 VidChapters-mini 的消融研究。最佳和第二好的结果分别用**粗体**和*下划线*标出。
- en: 4.3 Empirical Results
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实证结果
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.1 Datasets and Settings ‣ 4 Experiments ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos") presents the performance on VidChapters-mini, which indicates that our
    method achieves the state-of-the-art in training-free settings. We could even
    beat the fully-trained moment-DETR on more than half of the metrics, especially
    on r@{$n$}s.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [2](#S4.T2 "表 2 ‣ 4.1 数据集和设置 ‣ 4 实验 ‣ Grounding-Prompter: 使用多模态信息对长视频中的时间句子进行定位")展示了
    VidChapters-mini 上的性能，这表明我们的方法在无训练设置中达到了最先进水平。我们甚至在超过一半的指标上超越了完全训练的 moment-DETR，特别是在
    r@{$n$}s 上。'
- en: Rows 5-7 show the MLLM-Vs completely collapse when handling TSG. They fail to
    provide answers that follow specific formats on around half of the samples and
    are even beaten by “random” and “complete” on IoU-based metrics, indicating the
    drawbacks of these MLLM-Vs to understand the TSG task and perceive temporal boundaries
    across multiple modalities.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 第 5 行至第 7 行显示，MLLM-Vs 在处理 TSG 时完全崩溃。它们在大约一半的样本上未能提供符合特定格式的答案，甚至在 IoU 基准上被“随机”和“完整”击败，这表明这些
    MLLM-Vs 在理解 TSG 任务和跨多模态感知时间边界方面存在缺陷。
- en: 'Observing rows 8-12, though moment-DETR achieves the highest performance on
    most IoU-based metrics, however, we observe that the “state-of-the-art” performance
    largely derives from fitting distribution bias of both queries and video moments,
    indicating that it mainly learns spurious correlations other than real semantics,
    as shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.1 Datasets and Settings ‣ 4 Experiments
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos"). This may explain the reason why moment-DETR only shows
    marginal improvements compared to CLIP and BERT in zero-shot settings. In contrast,
    without training, our method shows a high level of generalization capability.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '观察第 8 行至第 12 行，尽管 moment-DETR 在大多数 IoU 基准上表现最佳，但我们观察到“最先进”的性能主要源于对查询和视频时刻分布偏差的拟合，这表明它主要学习的是虚假的相关性而非真实语义，如图 [3](#S4.F3
    "图 3 ‣ 4.1 数据集和设置 ‣ 4 实验 ‣ Grounding-Prompter: 使用多模态信息对长视频中的时间句子进行定位")所示。这可能解释了为什么
    moment-DETR 在零-shot 设置中相对于 CLIP 和 BERT 仅显示出微小的改进。相比之下，我们的方法在没有训练的情况下表现出较高的泛化能力。'
- en: 4.4 Ablation Analysis
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融分析
- en: 'We conduct the following ablation studies in Table [3](#S4.T3 "Table 3 ‣ 4.2
    Technical Roadmap Baselines ‣ 4 Experiments ‣ Grounding-Prompter: Prompting LLM
    with Multimodal Information for Temporal Sentence Grounding in Long Videos") to
    evaluate the crucial components in our method. For rows 3-5, we remove the three
    steps of CoT in Section [3.3.1](#S3.SS3.SSS1 "3.3.1 Multiscale Denoising Chain-of-Thought
    ‣ 3.3 Boundary-Perceptive Prompting ‣ 3 Proposed Method: Grounding-Prompter ‣
    Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos") and prompt LLM to merely generate a pair of timestamp
    predictions in column w/o CoT, and instruct LLM to give answers without one-shot
    example in column w/o ICL. For rows 6-7, we remove the speech transcriptions or
    visual captions in both the one-shot example and the test input, to evaluate the
    effectiveness of multimodal information.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格 [3](#S4.T3 "Table 3 ‣ 4.2 Technical Roadmap Baselines ‣ 4 Experiments
    ‣ Grounding-Prompter: Prompting LLM with Multimodal Information for Temporal Sentence
    Grounding in Long Videos") 中进行了以下消融研究，以评估我们方法中的关键组件。对于第3-5行，我们移除了第 [3.3.1](#S3.SS3.SSS1
    "3.3.1 Multiscale Denoising Chain-of-Thought ‣ 3.3 Boundary-Perceptive Prompting
    ‣ 3 Proposed Method: Grounding-Prompter ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos") 节中的 CoT
    三个步骤，并提示 LLM 仅生成一对时间戳预测（列 w/o CoT），并指示 LLM 在没有一-shot 示例的情况下给出答案（列 w/o ICL）。对于第6-7行，我们移除了无论是一-shot
    示例还是测试输入中的语音转录或视觉字幕，以评估多模态信息的有效性。'
- en: 4.4.1 Effect of Boundary-Perceptive Prompting
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 边界感知提示的效果
- en: For rows 3-5, our findings indicate the effectiveness of our Boundary-Perception
    Prompting strategy, with around 40- and 125-fold improvements on tighter r@0.7
    and r@0.9 metrics when both CoT and ICL are implemented. In addition, there are
    more in-depth observations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第3-5行，我们的发现表明我们的边界感知提示策略的有效性，当同时实施 CoT 和 ICL 时，r@0.7 和 r@0.9 指标的提升分别约为 40
    倍和 125 倍。此外，还有更深入的观察。
- en: •
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CoT and ICL mainly boost performance by improving temporal coverage of predictions,
    *i.e*. better predicting the start and end timestamps jointly other than improving
    the accuracy of point localization. LLM is capable of predicting a single timestamp
    without careful prompt designs on r@{$n$} and mIoU without CoT and ICL (row 3),
    indicating both CoT and ICL are indispensable for LLM to truly understand the
    TSG task.
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CoT 和 ICL 主要通过提高预测的时间覆盖率来提升性能，即更好地预测开始和结束时间戳，而不是提高点定位的准确性。LLM 能够在没有精心设计的提示的情况下预测单个时间戳
    `r@{$n$}` 和 mIoU（第3行），这表明 CoT 和 ICL 对于 LLM 真实理解 TSG 任务是不可或缺的。
- en: •
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Compared to learning from CoT, LLM is better at imitating examples to understand
    this novel and complicated temporal task. The implementation of ICL yields performance
    on r@{$m$}s metrics (rows 4-5).
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相比于从 CoT 中学习，LLM 更擅长通过模仿示例来理解这一新的复杂时间任务。ICL 的实施在 r@{$m$}s 指标上取得了性能提升（第4-5行）。
- en: •
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: More reasoning steps activate LLM to understand the novel TSG task but hinder
    format compliance. One-shot ICL activates imitating ability well with significant
    performance advancement, but it seems anti-intuitive that there are more format
    collapses when it’s implemented. Interestingly, it has been observed that LLM
    usually conveys meanings such as “The answer cannot be determined due to the absence
    of any existing matches” when format collapses occur, showing interpretation for
    failure cases to some extent.
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更多的推理步骤激活 LLM 理解新的 TSG 任务，但却阻碍了格式的符合性。一-shot ICL 很好地激活了模仿能力，显著提升了性能，但似乎有些反直觉的是，当实施时，格式崩溃现象更多。有趣的是，当格式崩溃发生时，观察到
    LLM 通常传达诸如“由于没有现有匹配，无法确定答案”这样的意思，在一定程度上显示出对失败情况的解释。
- en: 4.4.2 Effect of Multimodal Information
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 多模态信息的效果
- en: Rows 6-8 show our method comprehensively combines speech and visual modalities,
    outperforming itself with great margins when any of them is removed. (i) Speeches
    make more contributions to localization with a 5.13% improvement on r@0.9, and
    performance also improves significantly when captions are added, with a 1.58%
    update on r@0.9\. (ii) The start point prediction accuracy is close to the optimal
    only with ASR, but captions work well in improving the IoU accuracy. (iii) Although
    some key information is lost and much noise is introduced when translating videos
    into sparse captions, LLM still benefits from visual information, showing its
    strong ability to resist noise. (iv) Speeches contribute to more performance improvement
    but also cause a higher likelihood of collapse occurrences. The collapses are
    mitigated when visual captions are integrated, indicating that integrating multimodal
    information is advantageous for the robustness of our method.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第6-8行显示，我们的方法全面结合了语音和视觉模态，当其中任何一个被移除时，表现差距显著地提高。 (i) 语音对定位的贡献更大，r@0.9 提高了5.13%，且添加字幕时性能也显著提升，r@0.9
    更新了1.58%。 (ii) 仅使用ASR时，起始点预测准确度接近最佳，但字幕在提高IoU准确度方面表现良好。 (iii) 尽管将视频翻译为稀疏字幕时一些关键信息丢失，并且引入了大量噪声，LLM仍从视觉信息中受益，显示出其强大的抗噪声能力。
    (iv) 语音有助于提升更多性能，但也会导致更高的崩溃发生率。当整合视觉字幕时，这些崩溃现象得到缓解，表明整合多模态信息有助于提高我们方法的鲁棒性。
- en: 4.5 Qualitative Analysis
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 定性分析
- en: Compared to traditional TSG methods, our method not only provides structural
    predictions, but more explanatory remarks as well. In this section, we conduct
    qualitative analysis on GPT-3.5-turbo-16k.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的TSG方法相比，我们的方法不仅提供了结构化预测，还提供了更多的解释性说明。在本节中，我们对GPT-3.5-turbo-16k进行了定性分析。
- en: '![Refer to caption](img/90046560fa64bdb8b3f9e3c239a3c92b.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/90046560fa64bdb8b3f9e3c239a3c92b.png)'
- en: (a)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/6546b7726c7c20f4859a28c11b3f63d0.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6546b7726c7c20f4859a28c11b3f63d0.png)'
- en: (b)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/45c7487660dd9094b009be5252cbfee9.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/45c7487660dd9094b009be5252cbfee9.png)'
- en: (c)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 4: Case demonstration on the multimodal settings. For clarity and brevity,
    we present them in multi-round dialogs to show how we vary the input we feed the
    LLM, but in fact, our method only requires one round interaction with LLM. we
    only demonstrate relevant words with multimodal from LLM’s responses. Orange inputs,
    green outputs.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：多模态设置下的案例演示。为清晰和简洁起见，我们以多轮对话的形式展示输入如何变化，但实际上，我们的方法只需与LLM进行一轮交互。我们仅演示了LLM响应中的相关多模态词汇。橙色输入，绿色输出。
- en: '![Refer to caption](img/8a3e144e6853160499a0b64fb9b6894b.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a3e144e6853160499a0b64fb9b6894b.png)'
- en: (a)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/b4ad6cbad8b5cf755665303fba6f04f8.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b4ad6cbad8b5cf755665303fba6f04f8.png)'
- en: (b)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: '![Refer to caption](img/7a4a2f3eea4bcf6e39b7ee65b1898eb9.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7a4a2f3eea4bcf6e39b7ee65b1898eb9.png)'
- en: (c)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: 'Figure 5: Case demonstration on the prompting strategy, where LLM correctly
    captures the boundaries. Orange inputs, green outputs.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：提示策略下的案例演示，其中LLM正确捕捉了边界。橙色输入，绿色输出。
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.5 Qualitative Analysis ‣ 4 Experiments ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos") demonstrates how our method benefits from multimodal information. In
    Figure [4](#S4.F4 "Figure 4 ‣ 4.5 Qualitative Analysis ‣ 4 Experiments ‣ Grounding-Prompter:
    Prompting LLM with Multimodal Information for Temporal Sentence Grounding in Long
    Videos")(a), we find our method able to resist noise. LLM properly evaluates the
    noise in visual captions and filters noisy information in captions adaptively,
    thereby giving a more precise prediction. In Figure [4](#S4.F4 "Figure 4 ‣ 4.5
    Qualitative Analysis ‣ 4 Experiments ‣ Grounding-Prompter: Prompting LLM with
    Multimodal Information for Temporal Sentence Grounding in Long Videos")(b), it’s
    observed that our method can refine predictions through valid visual captions.
    LLM concludes that visual captions are beneficial to localization and it refines
    the end-time prediction. In Figure [4](#S4.F4 "Figure 4 ‣ 4.5 Qualitative Analysis
    ‣ 4 Experiments ‣ Grounding-Prompter: Prompting LLM with Multimodal Information
    for Temporal Sentence Grounding in Long Videos")(c), the LLM fails to provide
    answers solely based on speeches. However, a reasonable prediction is obtained
    when incorporating visual captions.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S4.F4 "图 4 ‣ 4.5 定性分析 ‣ 4 实验 ‣ Grounding-Prompter: 使用多模态信息对LLM进行时间句子定位")展示了我们的方法如何受益于多模态信息。在图[4](#S4.F4
    "图 4 ‣ 4.5 定性分析 ‣ 4 实验 ‣ Grounding-Prompter: 使用多模态信息对LLM进行时间句子定位")（a）中，我们发现我们的方法能够抵抗噪声。LLM适当地评估视觉字幕中的噪声，并自适应地过滤噪声信息，从而提供更精确的预测。在图[4](#S4.F4
    "图 4 ‣ 4.5 定性分析 ‣ 4 实验 ‣ Grounding-Prompter: 使用多模态信息对LLM进行时间句子定位")（b）中，可以观察到我们的方法通过有效的视觉字幕精炼预测。LLM认为视觉字幕对定位有益，并且精炼了结束时间的预测。在图[4](#S4.F4
    "图 4 ‣ 4.5 定性分析 ‣ 4 实验 ‣ Grounding-Prompter: 使用多模态信息对LLM进行时间句子定位")（c）中，LLM仅根据演讲无法提供答案。然而，当结合视觉字幕时，得到了一种合理的预测。'
- en: 'To demonstrate the temporal reasoning ability of our method with the Boundary-Perceptive
    Prompting strategy, a few examples are exhibited in Figure [5](#S4.F5 "Figure
    5 ‣ 4.5 Qualitative Analysis ‣ 4 Experiments ‣ Grounding-Prompter: Prompting LLM
    with Multimodal Information for Temporal Sentence Grounding in Long Videos"),
    showing that our method provides appropriate temporal partitions and correct moment
    summaries correspondingly.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示我们的方法通过**Boundary-Perceptive Prompting**策略的时间推理能力，图[5](#S4.F5 "图 5 ‣ 4.5
    定性分析 ‣ 4 实验 ‣ Grounding-Prompter: 使用多模态信息对LLM进行时间句子定位")中展示了一些例子，显示我们的方法提供了适当的时间分段和正确的时刻总结。'
- en: 5 Conclusion
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, we propose a Grounding-Prompter method to solve the Temporal
    Sentence Grounding (TSG) task in long videos. With the compressed task textualization,
    we effectively activate LLM to understand the TSG task and its multimodal inputs,
    speeches and visual content. After that, we propose the Boundary-Perceptive Prompting
    strategy, which is proven to enhance temporal reasoning and boundary perception
    under complicated and noisy long contexts. Experiments prove that our proposed
    Grounding-Prompter can effectively generate answers with consistent format compliance
    and achieve state-of-the-art performance with great margins compared to other
    baseline methods.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种**Grounding-Prompter**方法，用于解决长视频中的**时间句子定位**（TSG）任务。通过压缩任务文本化，我们有效激活了LLM来理解TSG任务及其多模态输入，包括演讲和视觉内容。随后，我们提出了**Boundary-Perceptive
    Prompting**策略，证明它能够在复杂和嘈杂的长文本环境下提升时间推理和边界感知。实验证明，我们提出的**Grounding-Prompter**可以有效地生成格式一致的答案，并且与其他基线方法相比，取得了显著的**最先进**表现。
- en: To the best of our knowledge, we are the first to explore LLMs for TSG in long
    videos by reformulating TSG into a long-textual task. With novel prompt designs,
    the competitive performance of our method indicates the potential of LLM to conduct
    temporal video tasks in a novel training-free manner. It will be interesting to
    investigate tuning tailored LLMs for TSG related tasks in future research.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，我们是第一个通过将TSG重新表述为长文本任务来探索LLM在长视频中的应用的团队。通过新颖的提示设计，我们的方法的竞争性能表明LLM在无训练的情况下执行时间视频任务的潜力。未来研究中探索为TSG相关任务量身定制LLM的调优将是很有趣的。
- en: References
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Anne Hendricks et al. [2017] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman,
    Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with
    natural language. In *Proceedings of the IEEE international conference on computer
    vision*, pages 5803–5812, 2017.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anne Hendricks et al. [2017] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman,
    Josef Sivic, Trevor Darrell, 和 Bryan Russell。使用自然语言定位视频中的时刻。在*IEEE国际计算机视觉会议论文集*，第5803–5812页，2017。
- en: Barrios et al. [2023] Wayner Barrios, Mattia Soldan, Alberto Mario Ceballos-Arroyo,
    Fabian Caba Heilbron, and Bernard Ghanem. Localizing moments in long video via
    multimodal guidance. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 13667–13678, 2023.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barrios et al. [2023] Wayner Barrios, Mattia Soldan, Alberto Mario Ceballos-Arroyo,
    Fabian Caba Heilbron, 和 Bernard Ghanem。通过多模态引导在长视频中定位时刻。在*IEEE/CVF国际计算机视觉会议论文集*，第13667–13678页，2023。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: Cao et al. [2021] Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, and Yuexian
    Zou. On pursuit of designing multi-modal transformer for video grounding. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    9810–9823, 2021.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao et al. [2021] Meng Cao, Long Chen, Mike Zheng Shou, Can Zhang, 和 Yuexian
    Zou。追求为视频定位设计多模态变换器。在*2021年自然语言处理方法会议论文集*，第9810–9823页，2021。
- en: 'Chen et al. [2023a] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei
    Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu, et al. Videollm: Modeling
    video sequence with large language models. *arXiv preprint arXiv:2305.13292*,
    2023a.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2023a] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei
    Huang, Junting Pan, Yi Wang, Yali Wang, Yu Qiao, Tong Lu 等。Videollm: 用大型语言模型建模视频序列。*arXiv预印本
    arXiv:2305.13292*，2023a。'
- en: 'Chen et al. [2023b] Houlun Chen, Xin Wang, Xiaohan Lan, Hong Chen, Xuguang
    Duan, Jia Jia, and Wenwu Zhu. Curriculum-listener: Consistency-and complementarity-aware
    audio-enhanced temporal sentence grounding. In *Proceedings of the 31st ACM International
    Conference on Multimedia*, pages 3117–3128, 2023b.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2023b] Houlun Chen, Xin Wang, Xiaohan Lan, Hong Chen, Xuguang Duan,
    Jia Jia, 和 Wenwu Zhu。课程-听众：一致性和互补性意识的音频增强时间句子定位。在*第31届ACM国际多媒体会议论文集*，第3117–3128页，2023b。
- en: Chen and Jiang [2021] Shaoxiang Chen and Yu-Gang Jiang. Towards bridging event
    captioner and sentence localizer for weakly supervised dense event captioning.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 8425–8435, 2021.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen and Jiang [2021] Shaoxiang Chen 和 Yu-Gang Jiang。致力于桥接事件字幕生成器和句子定位器，以进行弱监督密集事件字幕生成。在*IEEE/CVF计算机视觉与模式识别会议论文集*，第8425–8435页，2021。
- en: 'Chen et al. [2020a] Shaoxiang Chen, Wenhao Jiang, Wei Liu, and Yu-Gang Jiang.
    Learning modality interaction for temporal sentence localization and event captioning
    in videos. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
    August 23–28, 2020, Proceedings, Part IV 16*, pages 333–351\. Springer, 2020a.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen et al. [2020a] Shaoxiang Chen, Wenhao Jiang, Wei Liu, 和 Yu-Gang Jiang。学习模态交互以进行视频中的时间句子定位和事件字幕。在*计算机视觉–ECCV
    2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第IV部分16*，第333–351页。Springer，2020a。'
- en: Chen et al. [2021] Yi-Wen Chen, Yi-Hsuan Tsai, and Ming-Hsuan Yang. End-to-end
    multi-modal video temporal grounding. *Advances in Neural Information Processing
    Systems*, 34:28442–28453, 2021.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2021] Yi-Wen Chen, Yi-Hsuan Tsai, 和 Ming-Hsuan Yang。端到端多模态视频时间定位。*神经信息处理系统进展*，34:28442–28453，2021。
- en: 'Chen et al. [2020b] Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, and Kwan-Yee K
    Wong. Look closer to ground better: Weakly-supervised temporal grounding of sentence
    in video. *arXiv preprint arXiv:2001.09308*, 2020b.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2020b] Zhenfang Chen, Lin Ma, Wenhan Luo, Peng Tang, 和 Kwan-Yee
    K Wong。更仔细地看以更好地定位：视频中句子的弱监督时间定位。*arXiv预印本 arXiv:2001.09308*，2020b。
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez
    等。Vicuna: 一个开源聊天机器人，在90%*chatgpt质量下给gpt-4留下深刻印象。*见 https://vicuna.lmsys.org (访问日期
    2023年4月14日)*，2023。'
- en: Dong et al. [2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, and Zhifang Sui. A survey for in-context learning.
    *arXiv preprint arXiv:2301.00234*, 2022.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等 [2022] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu 和 Zhifang Sui. 语境学习综述。*arXiv 预印本 arXiv:2301.00234*，2022年。
- en: 'Du et al. [2021] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. *arXiv preprint arXiv:2103.10360*, 2021.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Du 等 [2021] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin
    Yang 和 Jie Tang. Glm: 使用自回归空白填充的通用语言模型预训练。*arXiv 预印本 arXiv:2103.10360*，2021年。'
- en: Duan et al. [2018] Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu
    Zhu, and Junzhou Huang. Weakly supervised dense event captioning in videos. *Advances
    in Neural Information Processing Systems*, 31, 2018.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duan 等 [2018] Xuguang Duan, Wenbing Huang, Chuang Gan, Jingdong Wang, Wenwu
    Zhu 和 Junzhou Huang. 视频中的弱监督密集事件标注。*神经信息处理系统进展*，31，2018年。
- en: Fayyaz et al. [2022] Mohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei
    Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash,
    and Jürgen Gall. Adaptive token sampling for efficient vision transformers. In
    *European Conference on Computer Vision*, pages 396–414\. Springer, 2022.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fayyaz 等 [2022] Mohsen Fayyaz, Soroush Abbasi Koohpayegani, Farnoush Rezaei
    Jafari, Sunando Sengupta, Hamid Reza Vaezi Joze, Eric Sommerlade, Hamed Pirsiavash
    和 Jürgen Gall. 高效视觉变换器的自适应令牌采样。见于*欧洲计算机视觉会议*，第396–414页，Springer，2022年。
- en: 'Gao et al. [2017] Jiyang Gao, Chen Sun, Zhenheng Yang, and Ram Nevatia. Tall:
    Temporal activity localization via language query. In *Proceedings of the IEEE
    international conference on computer vision*, pages 5267–5275, 2017.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等 [2017] Jiyang Gao, Chen Sun, Zhenheng Yang 和 Ram Nevatia. Tall: 通过语言查询进行时间活动定位。见于*IEEE
    国际计算机视觉会议论文集*，第5267–5275页，2017年。'
- en: 'Gao et al. [2019] Mingfei Gao, Larry Davis, Richard Socher, and Caiming Xiong.
    Wslln: Weakly supervised natural language localization networks. In *Proceedings
    of the 2019 Conference on Empirical Methods in Natural Language Processing and
    the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)*,
    pages 1481–1487, 2019.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等 [2019] Mingfei Gao, Larry Davis, Richard Socher 和 Caiming Xiong. Wslln:
    弱监督自然语言定位网络。见于*2019 年自然语言处理经验方法会议与第九届国际联合自然语言处理会议 (EMNLP-IJCNLP) 论文集*，第1481–1487页，2019年。'
- en: 'Gao et al. [2023] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng,
    Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et al. Llama-adapter v2:
    Parameter-efficient visual instruction model. *arXiv preprint arXiv:2304.15010*,
    2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao 等 [2023] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun
    Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue 等。Llama-adapter v2: 参数高效的视觉指令模型。*arXiv
    预印本 arXiv:2304.15010*，2023年。'
- en: 'Kenton and Toutanova [2019] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of naacL-HLT*, page 2, 2019.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kenton 和 Toutanova [2019] Jacob Devlin Ming-Wei Chang Kenton 和 Lee Kristina
    Toutanova. Bert: 深度双向变换器的预训练用于语言理解。见于*naacL-HLT 会议论文集*，第2页，2019年。'
- en: 'Korbar et al. [2019] Bruno Korbar, Du Tran, and Lorenzo Torresani. Scsampler:
    Sampling salient clips from video for efficient action recognition. In *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, pages 6232–6242,
    2019.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Korbar 等 [2019] Bruno Korbar, Du Tran 和 Lorenzo Torresani. Scsampler: 从视频中采样显著片段以提高动作识别效率。见于*IEEE/CVF
    国际计算机视觉会议论文集*，第6232–6242页，2019年。'
- en: Krishna et al. [2017] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,
    and Juan Carlos Niebles. Dense-captioning events in videos. In *Proceedings of
    the IEEE international conference on computer vision*, pages 706–715, 2017.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等 [2017] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei 和 Juan
    Carlos Niebles. 视频中的密集事件标注。见于*IEEE 国际计算机视觉会议论文集*，第706–715页，2017年。
- en: Lan et al. [2023] Xiaohan Lan, Yitian Yuan, Xin Wang, Zhi Wang, and Wenwu Zhu.
    A survey on temporal sentence grounding in videos. *ACM Transactions on Multimedia
    Computing, Communications and Applications*, 19(2):1–33, 2023.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 等 [2023] Xiaohan Lan, Yitian Yuan, Xin Wang, Zhi Wang 和 Wenwu Zhu. 视频中的时间句子定位综述。*ACM
    多媒体计算、通信与应用期刊*，19(2):1–33，2023年。
- en: Lei et al. [2021] Jie Lei, Tamara L Berg, and Mohit Bansal. Detecting moments
    and highlights in videos via natural language queries. *Advances in Neural Information
    Processing Systems*, 34:11846–11858, 2021.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lei 等 [2021] Jie Lei, Tamara L Berg 和 Mohit Bansal. 通过自然语言查询检测视频中的时刻和亮点。*神经信息处理系统进展*，34:11846–11858，2021年。
- en: 'Li et al. [2022a] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:
    Bootstrapping language-image pre-training for unified vision-language understanding
    and generation. In *International Conference on Machine Learning*, pages 12888–12900\.
    PMLR, 2022a.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2022a] Junnan Li, Dongxu Li, Caiming Xiong, 和 Steven Hoi. Blip：用于统一视觉-语言理解与生成的语言-图像预训练引导。见于*国际机器学习会议*，页码12888–12900。PMLR，2022a年。
- en: 'Li et al. [2023] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping
    Luo, Yali Wang, Limin Wang, and Yu Qiao. Videochat: Chat-centric video understanding.
    *arXiv preprint arXiv:2305.06355*, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023] KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping
    Luo, Yali Wang, Limin Wang, 和 Yu Qiao. Videochat：以聊天为中心的视频理解。*arXiv预印本 arXiv:2305.06355*，2023年。
- en: 'Li et al. [2022b] Mengze Li, Tianbao Wang, Haoyu Zhang, Shengyu Zhang, Zhou
    Zhao, Jiaxu Miao, Wenqiao Zhang, Wenming Tan, Jin Wang, Peng Wang, et al. End-to-end
    modeling via information tree for one-shot natural language spatial video grounding.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 8707–8717, 2022b.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2022b] Mengze Li, Tianbao Wang, Haoyu Zhang, Shengyu Zhang, Zhou
    Zhao, Jiaxu Miao, Wenqiao Zhang, Wenming Tan, Jin Wang, Peng Wang, 等等。通过信息树进行端到端建模用于一次性自然语言空间视频定位。见于*第60届计算语言学协会年会论文集（第1卷：长篇论文）*，页码8707–8717，2022b年。
- en: 'Lin et al. [2023] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick,
    Difei Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified
    video-language temporal grounding. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, pages 2794–2804, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2023] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick,
    Difei Gao, Alex Jinpeng Wang, Rui Yan, 和 Mike Zheng Shou. Univtg：迈向统一的视频-语言时间定位。见于*IEEE/CVF国际计算机视觉会议论文集*，页码2794–2804，2023年。
- en: Lin et al. [2020] Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, and Huasheng Liu.
    Weakly-supervised video moment retrieval via semantic completion network. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, pages 11539–11546, 2020.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. [2020] Zhijie Lin, Zhou Zhao, Zhu Zhang, Qi Wang, 和 Huasheng Liu.
    通过语义完成网络进行弱监督视频时刻检索。见于*AAAI人工智能会议论文集*，页码11539–11546，2020年。
- en: Liu et al. [2023] Daizong Liu, Xiang Fang, Wei Hu, and Pan Zhou. Exploring optical-flow-guided
    motion and detection-based appearance for temporal sentence grounding. *IEEE Transactions
    on Multimedia*, 2023.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023] Daizong Liu, Xiang Fang, Wei Hu, 和 Pan Zhou. 探索光流引导的运动和基于检测的外观用于时间句子定位。*IEEE多媒体学报*，2023年。
- en: 'Liu et al. [2022] Ye Liu, Siyuan Li, Yang Wu, Chang-Wen Chen, Ying Shan, and
    Xiaohu Qie. Umt: Unified multi-modal transformers for joint video moment retrieval
    and highlight detection. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 3042–3051, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2022] Ye Liu, Siyuan Li, Yang Wu, Chang-Wen Chen, Ying Shan, 和 Xiaohu
    Qie. Umt：统一的多模态变换器用于视频时刻检索和亮点检测。见于*IEEE/CVF计算机视觉与模式识别会议论文集*，页码3042–3051，2022年。
- en: Lv et al. [2023] Zezhong Lv, Bing Su, and Ji-Rong Wen. Counterfactual cross-modality
    reasoning for weakly supervised video moment localization. In *Proceedings of
    the 31st ACM International Conference on Multimedia*, pages 6539–6547, 2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lv et al. [2023] Zezhong Lv, Bing Su, 和 Ji-Rong Wen. 针对弱监督视频时刻定位的反事实跨模态推理。见于*第31届ACM国际多媒体会议论文集*，页码6539–6547，2023年。
- en: 'Maaz et al. [2023] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz
    Khan. Video-chatgpt: Towards detailed video understanding via large vision and
    language models. *arXiv preprint arXiv:2306.05424*, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maaz et al. [2023] Muhammad Maaz, Hanoona Rasheed, Salman Khan, 和 Fahad Shahbaz
    Khan. Video-chatgpt：通过大型视觉和语言模型实现详细的视频理解。*arXiv预印本 arXiv:2306.05424*，2023年。
- en: Mithun et al. [2019] Niluthpol Chowdhury Mithun, Sujoy Paul, and Amit K Roy-Chowdhury.
    Weakly supervised video moment retrieval from text queries. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11592–11601,
    2019.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mithun et al. [2019] Niluthpol Chowdhury Mithun, Sujoy Paul, 和 Amit K Roy-Chowdhury.
    从文本查询中进行弱监督视频时刻检索。见于*IEEE/CVF计算机视觉与模式识别会议论文集*，页码11592–11601，2019年。
- en: Mun et al. [2020] Jonghwan Mun, Minsu Cho, and Bohyung Han. Local-global video-text
    interactions for temporal grounding. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 10810–10819, 2020.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mun et al. [2020] Jonghwan Mun, Minsu Cho, 和 Bohyung Han. 局部-全球视频-文本交互用于时间定位。见于*IEEE/CVF计算机视觉与模式识别会议论文集*，页码10810–10819，2020年。
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. GPT-4技术报告，2023年。
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2021] Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel
    Goh、Sandhini Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark 等。从自然语言监督中学习可转移的视觉模型。载于
    *国际机器学习会议*，第8748–8763页。PMLR，2021年。
- en: Radford et al. [2023] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine
    McLeavey, and Ilya Sutskever. Robust speech recognition via large-scale weak supervision.
    In *International Conference on Machine Learning*, pages 28492–28518\. PMLR, 2023.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2023] Alec Radford、Jong Wook Kim、Tao Xu、Greg Brockman、Christine
    McLeavey 和 Ilya Sutskever。通过大规模弱监督进行鲁棒语音识别。载于 *国际机器学习会议*，第28492–28518页。PMLR，2023年。
- en: Rodriguez et al. [2023] Cristian Rodriguez, Edison Marrese-Taylor, Basura Fernando,
    Hiroya Takamura, and Qi Wu. Memory-efficient temporal moment localization in long
    videos. In *Proceedings of the 17th Conference of the European Chapter of the
    Association for Computational Linguistics*, pages 1901–1916, 2023.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rodriguez et al. [2023] Cristian Rodriguez、Edison Marrese-Taylor、Basura Fernando、Hiroya
    Takamura 和 Qi Wu。在长视频中进行内存高效的时间性时刻定位。载于 *第17届欧洲计算语言学协会会议论文集*，第1901–1916页，2023年。
- en: 'Soldan et al. [2022] Mattia Soldan, Alejandro Pardo, Juan León Alcázar, Fabian
    Caba, Chen Zhao, Silvio Giancola, and Bernard Ghanem. Mad: A scalable dataset
    for language grounding in videos from movie audio descriptions. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    5026–5035, 2022.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Soldan et al. [2022] Mattia Soldan、Alejandro Pardo、Juan León Alcázar、Fabian
    Caba、Chen Zhao、Silvio Giancola 和 Bernard Ghanem。Mad: 用于从电影音频描述中进行语言引导的可扩展数据集。载于
    *IEEE/CVF计算机视觉与模式识别会议论文集*，第5026–5035页，2022年。'
- en: 'Song et al. [2023] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang, Haoyang
    Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat:
    From dense token to sparse memory for long video understanding. *arXiv preprint
    arXiv:2307.16449*, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Song et al. [2023] Enxin Song、Wenhao Chai、Guanhong Wang、Yucheng Zhang、Haoyang
    Zhou、Feiyang Wu、Xun Guo、Tian Ye、Yan Lu、Jenq-Neng Hwang 等。Moviechat: 从密集标记到稀疏记忆以理解长视频。*arXiv
    预印本 arXiv:2307.16449*，2023年。'
- en: Song et al. [2020] Yijun Song, Jingwen Wang, Lin Ma, Zhou Yu, and Jun Yu. Weakly-supervised
    multi-level attentional reconstruction network for grounding textual queries in
    videos. *arXiv preprint arXiv:2003.07048*, 2020.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song et al. [2020] Yijun Song、Jingwen Wang、Lin Ma、Zhou Yu 和 Jun Yu。用于在视频中定位文本查询的弱监督多层次注意重建网络。*arXiv
    预印本 arXiv:2003.07048*，2020年。
- en: 'Tan et al. [2021] Reuben Tan, Huijuan Xu, Kate Saenko, and Bryan A Plummer.
    Logan: Latent graph co-attention network for weakly-supervised video moment retrieval.
    In *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision*, pages 2083–2092, 2021.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan et al. [2021] Reuben Tan、Huijuan Xu、Kate Saenko 和 Bryan A Plummer。Logan:
    用于弱监督视频时刻检索的潜在图共注意网络。载于 *IEEE/CVF冬季计算机视觉应用会议论文集*，第2083–2092页，2021年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: Wei et al. [2022] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Fei Xia、Ed
    Chi、Quoc V Le、Denny Zhou 等。链式思维提示在大型语言模型中引发推理。*神经信息处理系统进展*，35:24824–24837，2022年。
- en: Wu et al. [2019a] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming
    He, Philipp Krahenbuhl, and Ross Girshick. Long-term feature banks for detailed
    video understanding. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pages 284–293, 2019a.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. [2019a] Chao-Yuan Wu、Christoph Feichtenhofer、Haoqi Fan、Kaiming He、Philipp
    Krahenbuhl 和 Ross Girshick。用于详细视频理解的长期特征库。载于 *IEEE/CVF计算机视觉与模式识别会议论文集*，第284–293页，2019年。
- en: 'Wu et al. [2019b] Zuxuan Wu, Caiming Xiong, Chih-Yao Ma, Richard Socher, and
    Larry S Davis. Adaframe: Adaptive frame selection for fast video recognition.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 1278–1287, 2019b.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2019b] 吴祖轩、熊采铭、马志尧、理查德·索切和拉里·S·戴维斯。Adaframe：用于快速视频识别的自适应帧选择。发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，第 1278–1287 页，2019b 年。
- en: 'Yan et al. [2023] Shen Yan, Xuehan Xiong, Arsha Nagrani, Anurag Arnab, Zhonghao
    Wang, Weina Ge, David Ross, and Cordelia Schmid. Unloc: A unified framework for
    video localization tasks. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 13623–13633, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yan 等人 [2023] 闫申、熊学涵、阿尔沙·纳格拉尼、阿努拉格·阿纳布、汪中浩、葛伟娜、大卫·罗斯和科尔德利亚·施密德。Unloc：视频定位任务的统一框架。发表于
    *IEEE/CVF 国际计算机视觉会议论文集*，第 13623–13633 页，2023 年。
- en: 'Yang et al. [2023] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and
    Cordelia Schmid. Vidchapters-7m: Video chapters at scale. *arXiv preprint arXiv:2309.13952*,
    2023.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2023] 安托万·杨、阿尔沙·纳格拉尼、伊万·拉普捷夫、约瑟夫·西维克和科尔德利亚·施密德。Vidchapters-7m：大规模视频章节。*arXiv
    预印本 arXiv:2309.13952*，2023 年。
- en: Yin et al. [2023] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong
    Xu, and Enhong Chen. A survey on multimodal large language models. *arXiv preprint
    arXiv:2306.13549*, 2023.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等人 [2023] 尹书康、傅朝友、赵思睿、李珂、孙星、许通和陈恩洪。关于多模态大型语言模型的综述。*arXiv 预印本 arXiv:2306.13549*，2023
    年。
- en: Yuan et al. [2019a] Yitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, and Wenwu Zhu.
    Semantic conditioned dynamic modulation for temporal sentence grounding in videos.
    *Advances in Neural Information Processing Systems*, 32, 2019a.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 [2019a] 袁义天、马林、王晶雯、刘伟和朱文武。用于视频中时序句子定位的语义条件动态调制。*神经信息处理系统进展*，第 32 卷，2019a
    年。
- en: 'Yuan et al. [2019b] Yitian Yuan, Tao Mei, and Wenwu Zhu. To find where you
    talk: Temporal sentence localization in video with attention based location regression.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 9159–9166,
    2019b.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 [2019b] 袁义天、梅涛和朱文武。找到你说话的地方：基于注意力的时序句子定位。发表于 *AAAI 人工智能会议论文集*，第 9159–9166
    页，2019b 年。
- en: 'Yuan et al. [2021] Yitian Yuan, Xiaohan Lan, Xin Wang, Long Chen, Zhi Wang,
    and Wenwu Zhu. A closer look at temporal sentence grounding in videos: Dataset
    and metric. In *Proceedings of the 2nd international workshop on human-centric
    multimedia analysis*, pages 13–21, 2021.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan 等人 [2021] 袁义天、蓝晓涵、王鑫、陈龙、王志和朱文武。对视频中的时序句子定位的深入探讨：数据集与度量。发表于 *第二届以人为本的多媒体分析国际研讨会论文集*，第
    13–21 页，2021 年。
- en: Zeng et al. [2020] Runhao Zeng, Haoming Xu, Wenbing Huang, Peihao Chen, Mingkui
    Tan, and Chuang Gan. Dense regression network for video grounding. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    10287–10296, 2020.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeng 等人 [2020] 曾润浩、徐浩铭、黄文兵、陈佩豪、谭明奎和甘创。用于视频定位的密集回归网络。发表于 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第
    10287–10296 页，2020 年。
- en: 'Zhang et al. [2019] Da Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang, and Larry S
    Davis. Man: Moment alignment network for natural language moment retrieval via
    iterative graph adjustment. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 1247–1257, 2019.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2019] 张达、戴希阳、王鑫、王元芳和拉里·S·戴维斯。Man：自然语言时刻检索的时刻对齐网络，通过迭代图调整。发表于 *IEEE/CVF
    计算机视觉与模式识别会议论文集*，第 1247–1257 页，2019 年。
- en: 'Zhang et al. [2023] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned
    audio-visual language model for video understanding. *arXiv preprint arXiv:2306.02858*,
    2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2023] 张航、李鑫和邴立东。Video-llama：一种用于视频理解的指令调整音视频语言模型。*arXiv 预印本 arXiv:2306.02858*，2023
    年。
- en: Zhang et al. [2020] Songyang Zhang, Houwen Peng, Jianlong Fu, and Jiebo Luo.
    Learning 2d temporal adjacent networks for moment localization with natural language.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 12870–12877,
    2020.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2020] 张颂洋、彭厚文、傅建龙和罗杰博。学习二维时序邻接网络用于自然语言的时刻定位。发表于 *AAAI 人工智能会议论文集*，第
    12870–12877 页，2020 年。
- en: 'Zhi et al. [2021] Yuan Zhi, Zhan Tong, Limin Wang, and Gangshan Wu. Mgsampler:
    An explainable sampling strategy for video action recognition. In *Proceedings
    of the IEEE/CVF International conference on Computer Vision*, pages 1513–1522,
    2021.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhi 等人 [2021] 袁志、詹通、王利民和吴刚山。Mgsampler：一种可解释的视频动作识别采样策略。发表于 *IEEE/CVF 国际计算机视觉会议论文集*，第
    1513–1522 页，2021 年。
- en: 'Zhu et al. [2023a] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023a.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2023a] 德耀·朱、君·陈、晓倩·沈、翔·李和穆罕默德·埃尔霍塞尼。Minigpt-4：通过先进的大型语言模型提升视觉-语言理解。*arXiv
    预印本 arXiv:2304.10592*，2023a。
- en: Zhu et al. [2023b] Jiahao Zhu, Daizong Liu, Pan Zhou, Xing Di, Yu Cheng, Song
    Yang, Wenzheng Xu, Zichuan Xu, Yao Wan, Lichao Sun, et al. Rethinking the video
    sampling and reasoning strategies for temporal sentence grounding. *arXiv preprint
    arXiv:2301.00514*, 2023b.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2023b] 佳浩·朱、代宗·刘、潘·周、邢·迪、余·程、宋·杨、文政·徐、子川·徐、姚·万、李超·孙等。重新思考时间句子定位的视频采样和推理策略。*arXiv
    预印本 arXiv:2301.00514*，2023b。
