- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:29'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.13082](https://ar5iv.labs.arxiv.org/html/2404.13082)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xuechen Zhang    Zijian Huang    Ege Onur Taga    Carlee Joe-Wong    Samet Oymak
       Jiasi Chen
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent successes in natural language processing have led to the proliferation
    of large language models (LLMs) by multiple providers. Each LLM offering has different
    inference accuracy, monetary cost, and latency, and their accuracy further depends
    on the exact wording of the question (i.e., the specific prompt). At the same
    time, users often have a limit on monetary budget and latency to answer all their
    questions, and they do not know which LLMs to choose for each question to meet
    their accuracy and long term budget requirements. To navigate this rich design
    space, we propose TREACLE (Thrifty Reasoning via Context-Aware LLM and Prompt
    Selection), a reinforcement learning policy that jointly selects the model and
    prompting scheme while respecting the user’s monetary cost and latency constraints.
    TREACLE uses the problem context, including question text embeddings (reflecting
    the type or difficulty of a query) and the response history (reflecting the consistency
    of previous responses) to make smart decisions. Our evaluations on standard reasoning
    datasets (GSM8K, CSQA, and LLC ) with various LLMs and prompts show that TREACLE
    enables cost savings of up to 85% compared to baselines, while maintaining high
    accuracy. Importantly, it provides the user with the ability to gracefully trade
    off accuracy for cost.
  prefs: []
  type: TYPE_NORMAL
- en: Machine Learning, ICML
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The success of large language models (LLMs) in recent years has led to a explosion
    of heterogeneous models and providers, including as Meta’s Llama, OpenAI’s ChatGPT,
    and Google’s Gemini. As LLMs continue to proliferate in the near future, we envisage
    a generative AI marketplace with a large variety of providers, LLMs, and deployments.
    Notably, LLMs have widely varying capabilities and costs: capabilities in terms
    of accuracy in responding to different types of queries, and cost in terms of
    monetary price and query latency. As an illustration, the accuracy versus cost
    tradeoffs of various Llama and GPT LLMs are shown in Fig. [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt
    Selection") on grade school math word problems (Cobbe et al., [2021](#bib.bib4)).
    As can be seen, GPT-3.5 tends to have lower accuracy than GPT-4 (79% vs 92% respectively),
    but costs about 20 times less. This heterogeneous array of LLMs can bewilder users
    who must choose between them.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/724cd40b47a3d5a1c5e1789274605224.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: ${\sf\small TREACLE}$85% cost reduction compared to individual LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another challenge is that the *specific prompt* included in the question plays
    a critical role in eliciting accurate responses. This is especially true for reasoning
    problems where prompting a model to explain its reasoning can produce more accurate,
    but often more costly, answers. Chain-of-thought (CoT) (Wei et al., [2022](#bib.bib17))
    is an example of such a prompting scheme, in which the question includes a few
    examples of worked out problems, which cost more (due to the additional words
    included in the question) but also produce more accurate responses. For example,
    in [Figure 1](#S1.F1 "In 1 Introduction ‣ TREACLE: Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection"), GPT-4 with CoT (pink triangle) achieves a 92% accuracy,
    compared to GPT-4 with a domain expert prompt (brown dot, reminding the LLM that
    it is a “math solver”) that achieves 83%. However, using the CoT prompt costs
    3.9$\times$ more due to the extra words included in the query. A final challenge
    is that the optimal choice of LLM and prompt depends on the *specific question*
    being asked; the accuracy of a particular LLM and prompt combination for a particular
    question is unknown in advance, requiring learning or prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the heterogeneity of the LLM landscape and the tradeoffs between accuracy
    and cost make it challenging to determine the optimal strategy of: *Which LLM
    to select and how to prompt it, in order to answer all questions while respecting
    cost constraints?* To address this, we propose a Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection (${\sf\small TREACLE}$ achieves the Pareto front of individual
    LLMs by combining them intelligently.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Several recent works utilize multiple LLMs during inference with a focus on
    cascade design and accuracy-cost tradeoffs. Most aim to maximize accuracy and
    lack an explicit way to control long-term costs, as TREACLE has. More broadly,
    by posing the problem of LLM and prompt selection as a budget-constrained policy
    optimization, TREACLE provides a unified approach to efficient LLM cascades (see
    Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ TREACLE: Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection")). TREACLE’s policy can make informed decisions based
    on the full context of the LLM cascade, including the query embedding, answer
    statistics, and remaining budget.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison to related works. TREACLE poses the problem of LLM and
    prompt selection as a policy optimization. This policy seamlessly admits query
    embedding, long term budget, output statistics/consistency, and more as its input.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Query embedding | Consistency | Prompt *and* LLM selection | Long term
    budget | Robust to new models |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| FrugalGPT  (Chen et al., [2023](#bib.bib3)) | ✓ | ✗ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AutoMix  (Madaan et al., [2023](#bib.bib7)) | ✓ | ✓ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Mixture of Thought  (Yue et al., [2023](#bib.bib20)) | ✗ | ✓ | ✗ | ✗ | ✗
    |'
  prefs: []
  type: TYPE_TB
- en: '| TREACLE (ours) | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Overall, this paper makes the following contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Characterization of the accuracy, monetary cost, and latency of LLMs. To understand
    the trade-offs between the LLMs, we quantify the accuracy and cost of 5 different
    LLMs (Llama and GPT variants) with 3 different prompt strategies (standard, domain
    expert, and CoT) on 3 datasets (GSM8K, CSQA, and LLC).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An adaptive LLM and prompt selection policy based on reinforcement learning.
    ${\sf\small TREACLE}$ dynamically chooses the right LLM and prompt for each question.
    It does this by leveraging context about the current question, re-querying the
    models if needed to verify the consistency of the responses, and thinking ahead
    about the remaining budget.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extensive evaluations. We show that TREACLE substantially saves on cost while
    maintaining high accuracy on challenging mathematical and commonsense reasoning
    tasks. We demonstrate its robustness to different budgets, time-varying costs,
    question difficulty, price changes, new LLMs, and new unseen task types.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The paper is organized as follows. We describe related work (§[2](#S2 "2 Related
    Work ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection")),
    the problem statement (§[3](#S3 "3 Problem Statement ‣ TREACLE: Thrifty Reasoning
    via Context-Aware LLM and Prompt Selection")), and our framework (§[4](#S4 "4
    Proposed Framework: TREACLE ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM
    and Prompt Selection")). We then describe our experiments (§[5](#S5 "5 Experiments
    ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection")) and
    conclusions (§[6](#S6 "6 Conclusions ‣ TREACLE: Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'FrugalGPT (Chen et al., [2023](#bib.bib3)) is perhaps the closest to this work,
    as they considered a similar cost-constrained LLM selection problem with a threshold-based
    policy to select from a sorted list of LLMs. Our approach differs in several key
    aspects: we utilize a reinforcement learning policy that chooses both LLMs and
    prompts, rather than a threshold-based scheme; we utilize the full context of
    the current question to make decisions, including the text embedding of the current
    question and the history of past responses; and our method can *re-query* the
    same LLM and aggregate the responses of previously queried LLMs to estimate the
    correctness of the current response.'
  prefs: []
  type: TYPE_NORMAL
- en: Mixture of Thought (Yue et al., [2023](#bib.bib20)) explored the idea of response
    consistency in order to choose the right LLMs. The intuition is that higher consistency
    in the re-queries implies higher confidence in the correctness of the response.
    TREACLE employs response consistency as an input feature, along with other features,
    for LLM selection. AutoMix (Madaan et al., [2023](#bib.bib7)) introduces a “meta-verifier”
    to estimate whether a response is correct or a more powerful LLM is needed. Both
    works measure cost as a by-product of combining multiple LLMs rather than as a
    core long-term constraint across an entire set of questions, as we do.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other lines of work include uncertainty estimation or prompt engineering to
    improve accuracy (Lin et al., [2022](#bib.bib6); Xiong et al., [2023](#bib.bib18);
    Yue et al., [2023](#bib.bib20); Si et al., [2023](#bib.bib13); Cai et al., [2023](#bib.bib1);
    Naik et al., [2023](#bib.bib9)), which is complementary to our work. The related
    work is summarized in [Table 1](#S1.T1 "In 1 Introduction ‣ TREACLE: Thrifty Reasoning
    via Context-Aware LLM and Prompt Selection").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Problem Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We study the standard natural language query problem of providing correct responses
    to a series of questions. We focus on reasoning problems (e.g., grade school math
    problems) because they are challenging with multiple logical steps required to
    reach a final correct response. The problem involves answering a sequence of $n$.
    These models and prompts have different costs (in terms of latency and monetary
    price) and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The primary goal is to ensure that as many responses as possible are correct,
    while simultaneously minimizing the associated costs. Thus, we seek to maximize
    the overall performance $\mathbb{E}_{(Q,Y)}\texttt{reward}(Y,\hat{Y})$ is the
    response returned by TREACLE.
  prefs: []
  type: TYPE_NORMAL
- en: Cost functions. We consider two types of costs in this work, monetary price
    and latency, resulting in two types of cost functions. *(1) Pure monetary price.*
    LLMs can run remotely, where the monetary price per token is set by the provider
    (e.g., OpenAI). LLMs can also run locally, where the monetary price depends on
    a number of factors such as capital expenditures, server cooling and maintenance,
    electricity, etc. In our setup, the GPT models run remotely and the Llama models,
    which are free and open-source, run locally. *(2) Monetary price-latency combination.*
    Monetary price is important for some users (e.g., small companies) while latency
    plays a more crucial role in other settings (e.g., real-time voice assistants).
    Users who are latency-sensitive may be willing to pay more for lower latency,
    whereas others might be more patient and prefer lower prices. TREACLE allows users
    to choose the trade-off between monetary cost and latency by adjusting a trade-off
    coefficient $\beta$.
  prefs: []
  type: TYPE_NORMAL
- en: '4 Proposed Framework: TREACLE'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To solve this problem, we propose the TREACLE framework, depicted in [Figure 2](#S4.F2
    "In 4 Proposed Framework: TREACLE ‣ TREACLE: Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection"). Let the possible combinations of language models and
    prompts be denoted by $\{(M+P)_{1},(M+P)_{2},\ldots,(M+P)_{K}\}$ (whose choice
    may be informed by the result of all previously chosen models, prompts, and their
    responses) and re-query. This iterative process continues until TREACLE returns
    a final response (based on its learned policy). TREACLE then proceeds to the next
    question with the remaining budget and repeats the process, until all questions
    have been answered or there is no remaining budget. We use the terminology that
    one “question” may have multiple “re-queries” to different LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fbe52714b649d3025ba72d471e7817cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overview of TREACLE framework. TREACLE decides on the next (LLM,
    prompt) pair to query in a context-aware fashion which is summarized in the State
    variable.'
  prefs: []
  type: TYPE_NORMAL
- en: We model the problem as a Markov decision process, consisting of a set of states
    $\mathcal{S}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'States. The state vector contains the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Response consistency: Records all previous responses and the normalized frequency
    of their occurrences. The intuition is that the consistency of the previous responses
    can be used as a measure of confidence in the response correctness (Wang et al.,
    [2022](#bib.bib16); Madaan et al., [2023](#bib.bib7)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Input and output length: The number of tokens in the current query and any
    preceding responses to the same query. This helps TREACLE understand the monetary
    price of each query and response, which can differ for each query. It also helps
    capture the difficulty, as question with longer input or output tend to be harder.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Current question’s text embedding: Intuitively, we want to capture the question
    type or difficulty, which can impact the model and prompt selection decision.
    TREACLE does this using a text embedding of the query (Greene et al., [2022](#bib.bib5)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of re-queries: The number of re-queries for each model-prompt pair helps
    TREACLE decide whether to re-query again or move to the next question.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normalized remaining budget: Based on the remaining budget, we compute the
    estimated number of queries for each model prompt pair as follows: $\mathcal{B}_{k}=\frac{\text{total
    remaining budget}}{(\text{\# questions remaining})(\text{avg cost per query of
    $(M+P)_{k}$})}$. The average cost per query is estimated based on the questions
    seen so far. If there is a large remaining budget, TREACLE may consider re-querying
    with large models, but when there is less remaining budget, the policy may wish
    to use a cheaper model or a terse prompt.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Actions. The action space $\mathcal{A}$ consists of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Action $a_{1}$. If no models have been queried yet and this action is chosen,
    it is equivalent to skipping the question.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Action $a_{2}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Action $a_{3}$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By allowing re-querying (action $a_{2}$ combinations (§[5.1.2](#S5.SS1.SSS2
    "5.1.2 Data collection and training ‣ 5.1 Experiment Setup ‣ 5 Experiments ‣ TREACLE:
    Thrifty Reasoning via Context-Aware LLM and Prompt Selection")) with theoretical
    justification in [Appendix B](#A2 "Appendix B Understanding Optimal Cascade Strategy
    ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection").'
  prefs: []
  type: TYPE_NORMAL
- en: Rewards. The reward function assigns a positive reward to correct responses.
    Specifically, $R_{a}\left(s,s^{\prime}\right)=\mathbb{P}\left[\hat{Y}=Y|a=a_{1}\right]+\lambda\mathbb{P}\left[\hat{O}=Y|a\in\{a_{1},a_{2},a_{3}\}\right]$
    is known only when training TREACLE; during test, the policy executes using the
    expected reward calculated by the trained policy.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We first describe the experiment setup (§[5.1](#S5.SS1 "5.1 Experiment Setup
    ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt
    Selection")) and then the main results (§[5.2](#S5.SS2 "5.2 Results ‣ 5 Experiments
    ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection")). Specifically,
    we examine robustness to new LLMs and changing API prices (§[5.2.1](#S5.SS2.SSS1
    "5.2.1 Addition of new LLMs ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning
    via Context-Aware LLM and Prompt Selection")), time-varying API query latency
    (§[5.2.2](#S5.SS2.SSS2 "5.2.2 Time-Varying API Query Latency ‣ 5.2 Results ‣ 5
    Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection")),
    shifts in question difficulty (§[5.2.3](#S5.SS2.SSS3 "5.2.3 Shifts in Question
    Difficulty ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection")), and different reasoning datasets (§[5.2.4](#S5.SS2.SSS4
    "5.2.4 Different types of reasoning tasks ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE:
    Thrifty Reasoning via Context-Aware LLM and Prompt Selection")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Experiment Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 5.1.1 Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use three representative datasets for the experiments.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GSM8K (Cobbe et al., [2021](#bib.bib4)): The Grade School Math 8K dataset contains
    8.5K high quality grade school math problems created by human writers, in which
    7.5K are in the training data and 1K are in the testing data. We further split
    the 7.5K training data into 6K training data and 1.5K validation data for calibration.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CSQA (Saha et al., [2018](#bib.bib11)): The Complex Sequential Question Answering
    dataset consists of 12102 multiple choice commonsense reasoning questions encountered
    in daily life. The training set, validation set, and testing set contain 9741,
    1221 and 1140 samples respectively.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLC (Wei et al., [2022](#bib.bib17)) The Last Letter Concatenation task is to
    concatenate the last letters of words in a name (e.g., “Amy Brown” $\rightarrow$
    “yn”).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df09350caf6e6f7b13d7c9b1ddc34473.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Characterizing accuracy, cost, latency of different model-prompt
    pairs $(M+P)$ on the GSM8K test dataset. Higher accuracy corresponds to higher
    price or and lower latency.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 Data collection and training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To evaluate our methods, we perform two steps: (1) Collect query-response pairs
    for different combinations of LLMs and prompt, then (2) train TREACLE with these
    pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: (1) Collecting query-response pairs.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We collect query-response pairs from each dataset for different combinations
    of LLM, prompt, and LLM temperature. The accuracy, latency, and monetary price
    of the best combinations are shown in [Figure 3](#S5.F3 "In 5.1.1 Datasets ‣ 5.1
    Experiment Setup ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection"), with full results in [Table 4](#A3.T4 "In C.1 Model
    and Prompt Characterization ‣ Appendix C Additional Results ‣ TREACLE: Thrifty
    Reasoning via Context-Aware LLM and Prompt Selection") in the Appendix. We selected
    those combinations at the Pareto frontier of accuracy and cost. Generally, larger
    models and more sophisticated prompts significantly improve accuracy, but with
    a higher financial cost.'
  prefs: []
  type: TYPE_NORMAL
- en: '*LLMs.* We used 5 different LLMs: Llama-2-7b-chat, Llama-2-13b-chat (Touvron
    et al., [2023](#bib.bib15)), GPT-3.5-turbo, GPT-4, and GPT-4-turbo (OpenAI, [2023](#bib.bib10)).
    These models are of varying sizes (7b, 13b, 154b and 1.76t respectively). The
    Llama models are open-source and run locally on our servers (one A40 GPU for Llama-2-7b
    and two A40 for Llama-2-13b), while the GPT models rely on commercial APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prompt types.* We employ several prompting schemes to elicit the reasoning
    abilities of LLMs. The full prompts are shown in [Appendix E](#A5 "Appendix E
    Full Prompts ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection").
    A prompt generally consists of two parts: the “content message” containing the
    question, and the “system message” with additional context. The full prompts are
    given in [Appendix E](#A5 "Appendix E Full Prompts ‣ TREACLE: Thrifty Reasoning
    via Context-Aware LLM and Prompt Selection").'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The plain text prompt submits the questions to the LLM as the content message
    (no system message).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The domain expert prompt feeds information about the question’s domain as a
    system message (e.g., “math solver”), and keeping the user’s content message as
    plain text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standard few-shot prompt includes a system message (“Follow the given examples
    and answer the question” (Wei et al., [2022](#bib.bib17))) and the content message,
    which consists of few-shot examples together with the plain text prompt. It tends
    to improve response accuracy compared to the plain text prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Chain-of-Thought (CoT) few-shot prompt (Wei et al., [2022](#bib.bib17))
    adds some intermediate explanations to the few-shot examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Temperature.* The LLM temperature is a configurable parameter that influences
    the variety of the responses it generates. With a higher temperature, the model
    may output more diverse but possibly inaccurate responses. We set the temperature
    to 0 for a new query, and to 0.8 or 1.0 for a re-query for Llama and GPT, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: (2) Training TREACLE.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We used Deep Q-Network (DQN) (Mnih et al., [2015](#bib.bib8)) to train the reinforcement
    learning (RL) policy in TREACLE, consisting of a two-layer neural network. To
    generate diverse trajectories consisting of $(s_{t},a_{t},r_{t},s_{t+1})$.
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer
    to caption](img/5a17b56f628f6cb418733aeaca89dca2.png)GSM8K, $\alpha=\frac{1}{50}$50k![Refer to
    caption](img/08ceca62e38b05c363db78c9e3378d3c.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: The performance of various methods for different cost functions and
    budget constraints. The dashed lines are methods that have ground knowledge, which
    is impractical but illustrates the best achievable performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.3 Baselines
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We evaluated the following baseline methods, reproducing the methods as faithfully
    as possible with a common set of LLMs and prompt options.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FrugalGPT (Chen et al., [2023](#bib.bib3)). We reproduce FrugalGPT, which uses
    a DistilBERT model (Sanh et al., [2019](#bib.bib12)) to estimate the response
    accuracy. If this estimate is below a threshold, the next LLM in the cascade is
    queried. This baseline shows how TREACLE compares to the state-of-the-art that
    lacks re-querying.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calibrated cascade. We build on FrugalGPT’s response accuracy estimation and
    develop a 2-layer neural network, whose input is a state vector to TREACLE and
    whose output is the estimated response accuracy. If this estimate is below a threshold
    (tuned on the validation set), the next LLM in the cascade is queried. This baseline
    shows how TREACLE compares to an improved version of FrugalGPT.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Majority Voting. For each query, we output the final response based on the majority
    vote from $N$ based on the best empirical results. The (LLM, prompt) combinations
    are progressively queried until their per-question budget runs out. This baseline
    allows comparison with TREACLE’s response consistency feature in the state vector.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Offline and online knapsack. Given the cost of LLM responses and their accuracy,
    we formulate a multiple choice knapsack problem where the items are the $(M+P)$
    combinations, the values are the correctness probabilities, and the costs are
    the latency and monetary price functions. Solving this offline knapsack problem
    gives the optimal solution when re-queries are not allowed. We also implement
    an online approximation algorithm (Chakrabarty et al., [2008](#bib.bib2)). These
    baselines show how TREACLE compares to methods with perfect knowledge of question
    costs and accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single model. The (LLM, prompt) combinations are sorted by increasing cost and
    accuracy, then the most capable option that fits within the allocated budget is
    selected for all questions. This baseline shows how TREACLE compares to a fixed
    single LLM and prompt.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To evaluate the performance of ${\sf\small TREACLE}$, and total budget, TREACLE
    consistently outperforms the baselines and is close to the Offline Knapsack– an
    approach not feasible in practical deployments. We note that the relatively good
    performance of the Calibrated Cascade is due to it using the same state vector
    we designed for TREACLE. To delve deeper into the behavior of TREACLE, we perform
    additional experiments and make the following observations.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ parameters were produced after training TREACLE only once (with different
    parameter settings during the training). This highlights TREACLE’s adaptability
    to different cost function variations.
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$ and insufficient for all queries, 52.7% of the questions TREACLE chooses
    to answer are correct. To provide context, the cheapest model (Llama-2-7b) can
    only answer 23.65% of questions correctly. Thus TREACLE can evaluate question
    difficulty and opt not to respond to those that are infeasible within the budget.
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ intelligently chooses more powerful model-prompt combinations.* This
    is shown in [Figure 5(a)](#S5.F5.sf1 "In Figure 5 ‣ 5.2 Results ‣ 5 Experiments
    ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection"), which
    shows the average number of times each model-prompt combination is re-queried.
    As the budget increases, the more powerful models (right side of x-axis) are increasingly
    selected). Interestingly, we observe that for budgets $0.3 to $10, the Llama-2-13b
    model is queried approximately once per question, despite its suboptimal performance.
    Even with these larger budgets, it’s still beneficial to check Llama before moving
    onto more powerful models, to see whether its responses are consistent.'
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer to
    caption](img/39b9e5be35bb67581e81dcb24ad704b3.png)
  prefs: []
  type: TYPE_NORMAL
- en: (a) Varying budget with $\alpha=\frac{1}{20}$
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer to
    caption](img/81ae6f2b0c7d06604efc961efe4e96db.png)
  prefs: []
  type: TYPE_NORMAL
- en: (b) Varying $\alpha$ with $1.5 budget.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Number of times each model is re-queried.'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ *Observation 4: Re-querying helps.* We conducted an ablation study
    where we trained both TREACLE and the Calibrated Cascade Algorithm baseline without
    the ability to re-query. The results are shown in [Figure 6](#S5.F6 "In 5.2 Results
    ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt
    Selection"), where the dashed line represents method variants that permits re-querying.
    We observed a notable decrease in accuracy when re-querying was not allowed. Methods
    without re-querying eventually achieved comparable accuracy with those with re-querying
    capability, but with significantly larger budgets.Additional ablation experiments
    showing that re-querying or prompt selection help are shown in [Section C.3](#A3.SS3
    "C.3 Ablation experiments ‣ Appendix C Additional Results ‣ TREACLE: Thrifty Reasoning
    via Context-Aware LLM and Prompt Selection").'
  prefs: []
  type: TYPE_NORMAL
- en: '$\bullet$ decreases), TREACLE increasingly utilizes Llama to answer queries,
    allowing for cost savings, as shown in [Figure 5(b)](#S5.F5.sf2 "In Figure 5 ‣
    5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM
    and Prompt Selection"). This shift enables use of more expensive models like GPT-4
    when tackling complex problems, thereby enhancing overall accuracy. When Llama
    becomes more expensive, TREACLE no longer chooses it. This aligns with our intuition
    that using Llama to verify response consistency becomes less economical.'
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer to
    caption](img/2a1e6973fd99a649125198f8bb619986.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Performance of ${\sf\small TREACLE}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Addition of new LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLM development is rapid, with better models continuously emerging, and the
    API prices set by providers can change at any time. TREACLE’s ability to react
    to such changes is thus an important practical consideration. We show that TREACLE
    can adapt by fine-tuning itself using few samples.
  prefs: []
  type: TYPE_NORMAL
- en: We study two types of changes to the LLMs and their prices. (1) *API price adjustment:*
    In November 2023, OpenAI released GPT-4-turbo, offering performance on par with
    GPT-4 but at a more affordable price. Concurrently, the price for GPT-3.5-turbo
    was lowered. (2) *Fine-tuned open-source LLMs:* Several domain-specific fine-tuned
    models with higher accuracy have been released. Specifically, we exchanged Llama-2
    for MetaMath (Yu et al., [2023](#bib.bib19)), which is fine-tuned specifically
    for GSM8K. For both scenarios, we partitioned the GSM8K test data into 80% validation
    and 20% test samples, generated new state-action trajectories from the validation
    set, then fine-tuned TREACLE on these new trajectories. To create a comparable
    baseline, we similarly fine-tuned FrugalGPT’s DistilBERT.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d4d993d00d992a34e804fa54db51f539.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Performance with new LLMs and lowered prices. Lines and dots in light
    (dark) colors are results with old (new) prices and LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Firstly, we show the performance of TREACLE with both the API price adjustments
    and improved LLMs in [Figure 7](#S5.F7 "In 5.2.1 Addition of new LLMs ‣ 5.2 Results
    ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt
    Selection"). The individual points on the plot illustrate the changes in the API
    prices for gpt-3.5-turbo. The lines show the performance of the new TREACLE with
    new models and prices and the old TREACLE (i.e., from previous subsections). The
    results shows that the new TREACLE can achieve the peak accuracy with only a $1
    budget, clearly benefiting from the new models and lowered prices. Benefits are
    also significant for lower budgets, where the improved TREACLE has significantly
    higher accuracy, because the lowest performing Llama-2 models were replaced by
    fine-tuned Metamaths. Finally, for a FrugalGPT that relies on a fine-tuned DistilBERT
    accuracy estimator, performance didn’t improve and can even degrade due to distribution
    shifts and overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f352b14ad8536db65a8f79234393180c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) With new GPT models and prices (API price adjustment)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f67e7182a1092af303f2c486e5d88efd.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) With new Llama models (fine-tuned open-source LLMs)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Sample complexity for different budgets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, we investigate the sample efficiency of fine-tuning compared to training
    TREACLE from scratch. The sample efficiency is important it can be expensive to
    collect query-response pairs from new LLMs to further train TREACLE. The results
    are presented in [Figure 8](#S5.F8 "In 5.2.1 Addition of new LLMs ‣ 5.2 Results
    ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt
    Selection") and indicate that when there are minor changes to the available LLMs,
    deploying the previously trained TREACLE can be sufficient. For instance, in [Figure 8(a)](#S5.F8.sf1
    "In Figure 8 ‣ 5.2.1 Addition of new LLMs ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE:
    Thrifty Reasoning via Context-Aware LLM and Prompt Selection") when there is limited
    budget ($0.15) and upgrades to the expensive models, deploying the previously
    trained TREACLE (# samples = 0) achieves comparable performance to the fine-tuning
    TREACLE (# samples = 800). On the other hand, when upgrades are introduced to
    cheaper models ([Figure 8(b)](#S5.F8.sf2 "In Figure 8 ‣ 5.2.1 Addition of new
    LLMs ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection")), deploying the old TREACLE may initially result in
    poor accuracy performance, but TREACLE can quickly adapt to the new LLM options
    by fine-tuning with a few number of samples (around 300). All experiments with
    $\alpha=\frac{1}{10}$. The performance of finetuned model with all types of changes
    are shown in [Section C.4](#A3.SS4 "C.4 Additional new LLM experiments ‣ Appendix
    C Additional Results ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt
    Selection").'
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer
    to caption](img/92a394cdc17cd53d30be717c61a7e437.png)o’clockLatency
    (s)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: End-to-end latency of querying LLM models over a 24-hour period.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Performance with time-varying API query latency.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Accuracy with Time-varying Latency | Accuracy assuming Constant
    Latency | Update time (s) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TREACLE | 86.4 | 76.1 | 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Calibrated | 80.1 | 75.7 | 617.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Cascade |'
  prefs: []
  type: TYPE_TB
- en: 5.2.2 Time-Varying API Query Latency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The latency of querying LLM APIs (such as OpenAI’s GPT models) may vary over
    the short term time based on network congestion or LLM inference time. To showcase
    this, we recorded traces of the API latency (including communication and communication
    latency) over a 24-hour period. The measurements are shown in [Figure 9](#S5.F9
    "In 5.2.1 Addition of new LLMs ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty
    Reasoning via Context-Aware LLM and Prompt Selection"). We also modified TREACLE
    and the experimental setup slightly. Each hour, TREACLE attempts to answer the
    entire GSM8K test set with a budget of $0.6, using the historical average latency
    from the previous hour to update the per-query cost in the denominator of $\mathcal{B}_{k}$
    each hour is also minimal, at 20 ms.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c1231a5f639cf78c2751b35246c4067.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Hard
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49156a21ac46da7f9ae08d4fe58c357c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Easy
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Performance of various methods on “easy” and “hard” partitions of
    the test set. The models are trained using the original training data, leading
    to a distribution shift in difficulty on the test set. Experiments with $\alpha=\frac{1}{20}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Performance on “Easy” and “Hard” partitions of the test set. The total
    budget is $$1$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | # of questions unanswered | Budget spent ($) |'
  prefs: []
  type: TYPE_TB
- en: '| All | Easy | Hard | All | Easy | Hard |'
  prefs: []
  type: TYPE_TB
- en: '| TREACLE | 1/1319 | 0/500 | 2/500 | 0.972 | 0.841 | 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| Single Model | 2/1319 | 0/500 | 0/500 | 0.689 | 0.688 | 0.703 |'
  prefs: []
  type: TYPE_TB
- en: '| Cal. Cascade | 2/1319 | 0/500 | 26/500 | 0.931 | 0.702 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: '| FrugalGPT | 3/1319 | 0/500 | 31/500 | 0.955 | 0.713 | 0.998 |'
  prefs: []
  type: TYPE_TB
- en: 5.2.3 Shifts in Question Difficulty
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Thus far in the evaluations, easier and harder questions were evenly mixed
    throughout the training and test sets. In practice, easier and harder questions
    may not be uniformly spaced, and we would like TREACLE to be robust to that. In
    this subsection, we examine two types of non-uniform distributions of query difficulty:
    harder/easier problems in the training and test sets, and towards the end of the
    test set.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Difficulty shifts between training and test. We divided the GSM8K test set
    into “hard” and “easy” subsets based on the question difficulty. The difficulty
    is defined by the number of LLM models correctly answering the question. Intuitively,
    more models answering a question correctly means it is easier. Basic performance
    on the easy and hard questions is shown in [Table 3](#S5.T3 "In 5.2.2 Time-Varying
    API Query Latency ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via
    Context-Aware LLM and Prompt Selection"). When the questions are hard, each question
    ends up consuming too much budget, leaving insufficient budget for subsequent
    questions that then go unanswered. The single model baseline does well in terms
    of cost and unanswered questions, but has low accuracy. We plot the performance
    for variable budgets in [Figure 10](#S5.F10 "In 5.2.2 Time-Varying API Query Latency
    ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM
    and Prompt Selection"), and find that TREACLE’s accuracy remains stable, no matter
    whether the test distribution shifts to an easier level or a harder level. This
    is because TREACLE can dynamically adjust based on the remaining budget in online
    fashion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Difficulty shifts within the test set. To further evaluate the robustness to
    question difficulty shifts, we test TREACLE with the full test set sorted from
    easy-to-hard queries or hard-to-easy queries. The hope is that with the help of
    query text embedding in the state vector (which should capture some estimate of
    difficulty), TREACLE can remain relatively stable in terms of accuracy even if
    the ordering of the questions changes. This hypothesis is borne out in [Figure 11](#S5.F11
    "In 5.2.3 Shifts in Question Difficulty ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE:
    Thrifty Reasoning via Context-Aware LLM and Prompt Selection"), while Online Knapsack
    performs significantly worse than TREACLE if the questions are sorted from hard
    to easy. This is because much of the budget is wasted on the difficult queries
    that arrive at the beginning.'
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer to
    caption](img/89c274f069fcb23ec4cdbeb0bc11a933.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: TREACLE is robust to re-ordered question difficulty in the test
    set. Experiments with $\alpha=\frac{1}{20}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.4 Different types of reasoning tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We conducted experiments on various types of reasoning datasets to understand
    TREACLE’s performance, with full results given in the Appendix. To visualize the
    differences between the three datasets, in [Figure 12](#S5.F12 "In 5.2.4 Different
    types of reasoning tasks ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning
    via Context-Aware LLM and Prompt Selection") we plotted the fraction of questions
    where the most powerful (LLM, prompt) combination in the sorted list correctly
    answered the question (the “in order” pie slice), versus those questions where
    a less powerful combination succeeded and a more powerful combination failed (all
    other slices of the pie). Interestingly for all datasets, there are minority cases
    where less powerful LLMs (the smaller pieces of the pie) can answer the question
    correctly. Such cases are most prevalent in the GSM8K dataset and least prevalent
    in LLC, possibly because the math questions of GSM8K are more difficult. Despite
    these dataset differences, TREACLE still chooses the right (LLM, prompt) combination
    to achieve higher accuracy in all datasets than the baselines.'
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer
    to caption](img/fc2d534627d00dfa7893fd3e7dae24c1.png)GSM8K![Refer to
    caption](img/d5ce2b3c27f4414b69b181909e33fe46.png)LLC
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: Fraction of questions that are solved by (LLM, prompt) combinations
    ordered from least to most powerful (“in order”). Minority slices are questions
    where less powerful combinations correctly answered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixture of tasks. We also seek to examine whether one model can handle multiple
    types of tasks under one common budget. This is in contrast to the previous experiments
    where each task required a specialized model. Specifically, we trained a single
    model with all 3 datasets and recorded the test accuracy on the same 3 datasets.
    The results shown in [Figure 13](#S5.F13 "In 5.2.4 Different types of reasoning
    tasks ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware
    LLM and Prompt Selection") for “TREACLE (all tasks)”, offline knapsack, and online
    knapsack are the test accuracy from an equal mix of CSQA, GSM8K and LLC queries.
    “TREACLE (individual tasks)” is the test accuracy on the same mix of queries,
    using the models from previous subsections, where each model (corresponding to
    a task) is assigned to 1/3 of the common budget. The results show that “TREACLE
    (all tasks)” can handle a mixture of tasks under a common budget (e.g., outperforming
    online knapsack), and can significantly outperform the individual tasks baseline
    (“TREACLE (individual tasks)”) by effectively allocating its common budget across
    queries of different types.'
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer to
    caption](img/47ebfccff70d5eca4231cbfae3477a92.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 13: TREACLE can handle a mixture of tasks under a common budget.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Addition of an unseen new task type. We now consider the scenario where the
    model has not been trained on certain tasks. To show that TREACLE can adapt to
    new tasks easily, we performed additional experiments. The base model is trained
    using the CSQA dataset, and the unseen new tasks are queries from GSM8K. Interestingly,
    in our design, we decouple “decision making” from the “task embedding”: To transfer
    from CSQA to GSM8K, we freeze the base RL policy of CSQA (the decision making
    part), and fine-tune the “current question’s text embedding” feature in the state
    vector (the task embedding part) that feeds to this RL policy. The question is
    how many samples are needed to fine-tune the text embedding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Figure 14](#S5.F14 "In 5.2.4 Different types of reasoning tasks
    ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM
    and Prompt Selection"), with a budget of 0.6, the original model fully-trained
    on GSM8K (“train on GSM8K”) achieves a test accuracy of 0.848, compared to 0.78
    when trained on CSQA and fine-tuned with only 200 additional samples from GSM8K
    (“fine-tune on 200 GSM8K”). This highlights a relatively small accuracy loss when
    transferring to new types of unseen tasks. The results suggest that our method
    can seamlessly adapt to new tasks with only a small amount of additional training,
    which is important for overall cost savings.'
  prefs: []
  type: TYPE_NORMAL
- en: ![Refer to
    caption](img/ed1fc3c83cd8fe2403540bdd1d78048e.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14: Test accuracy on GSM8K'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We proposed TREACLE, an learning-based LLM querying framework that intelligently
    chose between LLM and prompt combinations based on questoin context and past response
    history. Our experiments showed that TREACLE outperforms other baselines and is
    robust to different budgets, LLM availability and prices, query latency, etc.
    For future work, we plan to explore other non-reasoning tasks and incorporate
    other features such as privacy into the cost function. We hope our framework can
    help spur research on cost-efficient and robust LLM systems.
  prefs: []
  type: TYPE_NORMAL
- en: Broader impact. The potential broader impact of this work is to make LLMs cheaper
    to use and more accessible to cost-sensitive users.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cai et al. (2023) Cai, Z., Chang, B., and Han, W. Human-in-the-loop through
    chain-of-thought, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chakrabarty et al. (2008) Chakrabarty, D., Zhou, Y., and Lukose, R. Online knapsack
    problems. In *Workshop on internet and network economics (WINE)*, 2008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) Chen, L., Zaharia, M., and Zou, J. Frugalgpt: How to use
    large language models while reducing cost and improving performance. *arXiv preprint
    arXiv:2305.05176*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
    Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training
    verifiers to solve math word problems. *arXiv preprint arXiv:2110.14168*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Greene et al. (2022) Greene, R., Sanders, T., Weng, L., and Neelakantan, A.
    New and improved embedding model, 2022. URL [https://openai.com/blog/new-and-improved-embedding-model](https://openai.com/blog/new-and-improved-embedding-model).
    Accessed: 2023-10-27.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2022) Lin, S., Hilton, J., and Evans, O. Teaching models to express
    their uncertainty in words, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al. (2023) Madaan, A., Aggarwal, P., Anand, A., Potharaju, S. P.,
    Mishra, S., Zhou, P., Gupta, A., Rajagopal, D., Kappaganthu, K., Yang, Y., Upadhyay,
    S., Mausam, and Faruqui, M. Automix: Automatically mixing language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mnih et al. (2015) Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
    J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidjeland, A. K., Ostrovski,
    G., et al. Human-level control through deep reinforcement learning. *nature*,
    518(7540):529–533, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Naik et al. (2023) Naik, R., Chandrasekaran, V., Yuksekgonul, M., Palangi, H.,
    and Nushi, B. Diversity of thought improves reasoning abilities of large language
    models. *arXiv preprint arXiv:2310.07088*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
    URL [https://api.semanticscholar.org/CorpusID:257532815](https://api.semanticscholar.org/CorpusID:257532815).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Saha et al. (2018) Saha, A., Pahuja, V., Khapra, M., Sankaranarayanan, K.,
    and Chandar, S. Complex sequential question answering: Towards learning to converse
    over linked question answer pairs with a knowledge graph. In *Proceedings of the
    AAAI conference on artificial intelligence*, volume 32, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al. (2019) Sanh, V., Debut, L., Chaumond, J., and Wolf, T. Distilbert,
    a distilled version of bert: smaller, faster, cheaper and lighter. *arXiv preprint
    arXiv:1910.01108*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Si et al. (2023) Si, C., Gan, Z., Yang, Z., Wang, S., Wang, J., Boyd-Graber,
    J., and Wang, L. Prompting gpt-3 to be reliable, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (14) together.ai. together pricing. [https://www.together.ai/pricing](https://www.together.ai/pricing).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang,
    S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning
    in language models. *arXiv preprint arXiv:2203.11171*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi,
    E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in
    large language models. *Advances in Neural Information Processing Systems*, 35:24824–24837,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiong et al. (2023) Xiong, M., Hu, Z., Lu, X., Li, Y., Fu, J., He, J., and Hooi,
    B. Can llms express their uncertainty? an empirical evaluation of confidence elicitation
    in llms, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2023) Yu, L., Jiang, W., Shi, H., Yu, J., Liu, Z., Zhang, Y., Kwok,
    J. T., Li, Z., Weller, A., and Liu, W. Metamath: Bootstrap your own mathematical
    questions for large language models. *arXiv preprint arXiv:2309.12284*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yue et al. (2023) Yue, M., Zhao, J., Zhang, M., Liang, D., and Yao, Z. Large
    language model cascades with mix-ture of thought representations for cost-efficient
    reasoning. *arXiv preprint arXiv:2310.03094*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: During training, we used the Adam optimizer with a learning rate $1\times 10^{-}4$.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Understanding Optimal Cascade Strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Setup: Suppose there are $M$. During the cascade, we assume access to an oracle
    that tells when the answer of a model is incorrect so that we can move to the
    next model. The procedure continues until a correct answer is obtained. The goal
    is to minimize the expected cost of inference.'
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suppose there are two models with probability of correct answers $p_{1}$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Cascade terminates when the first correct answer is obtained. The expected cost
    of inference if we query model 1 first is $c_{1}p_{1}+(c_{1}+c_{2})(1-p_{1})$
    is larger. ∎
  prefs: []
  type: TYPE_NORMAL
- en: Lemma 2.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suppose there are $M$.
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Suppose the models are not ordered in terms of $p_{i}/c_{i}$. Let us compute
    the expected change in inference cost when we flip their order.
  prefs: []
  type: TYPE_NORMAL
- en: The change in cost of inference arises from the scenarios model $k$ is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ec_{k}=q_{k}c_{k}+q_{k}(1-p_{k})c_{k+1}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: This is because if first $k-1$ failed).
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, excess cost associated to $k+1$)
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ec_{k+1}=q_{k}c_{k+1}+q_{k}(1-p_{k+1})c_{k}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: To proceed, observe that if $p_{k}/c_{k}![Refer
    to caption](img/e58e2d5eb629d0bc586e42c9f1598ace.png)CSQA, $\alpha=\frac{1}{50}$3M![Refer to caption](img/6be422a2069086cef961cafd283de9fa.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 16: The performance of various methods for different cost functions
    and budget constraints. The dashed lines are methods that have ground knowledge,
    which is impractical but illustrates the best achievable performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Overview of average cost ($) per query for different models and prompting
    strategies’ combinations in different pricing strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) GSM8K dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Pricing Strategy | Llama-2-7b | Llama-2-13b | GPT-3.5-turbo (old) | GPT-3.5-turbo
    (old) | GPT-4 | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | CoT | Domain Expert | CoT | Domain Expert | CoT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=10$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=20$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=50$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=50K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=500K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=1000K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pricing Strategy | MetaMath-7b | MetaMath-13b | GPT-3.5-turbo (new) | GPT-3.5-turbo
    (new) | GPT-4-turbo | GPT-4-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| Domain Expert | Domain Expert | Domain Expert | CoT | Domain Expert | CoT
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=10$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=20$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=50$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=50K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=500K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=1000K$ |'
  prefs: []
  type: TYPE_TB
- en: (b) CSQA dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Pricing Strategy | Llama-2-7b | Llama-2-13b | GPT-3.5-turbo | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | CoT | Standard | Standard |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=10$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=20$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=50$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=50K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=500K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=1000K$ |'
  prefs: []
  type: TYPE_TB
- en: (c) LLC dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Pricing Strategy | Llama-2-7b | GPT-3.5-turbo | GPT-3.5-turbo | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | plaintext | CoT | CoT |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=10$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=20$ |'
  prefs: []
  type: TYPE_TB
- en: '| Pure monetary, $\alpha=50$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=150K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=1500K$ |'
  prefs: []
  type: TYPE_TB
- en: '| Price-latency combo, $\beta=3000K$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Overview of average accuracy of different models and different prompting
    strategies’ combinations. In GSM8K table, simple CoT few-shot and complex CoT
    few-shot mean CoT few-shot prompts with easy and hard examples.'
  prefs: []
  type: TYPE_NORMAL
- en: (a) GSM8K dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | System Prompt | Content Prompt | Training Set Accuracy | Testing
    Set Accuracy | avg input length | avg output length |'
  prefs: []
  type: TYPE_TB
- en: '| (%) | (%) | (Training/Testing) | (Training/Testing) |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7b | “Follow example” | simple CoT few-shot | 23.36 | 23.65 | 909.81/911.43
    | 120.49/119.14 |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-13b | NA | simple CoT few-shot | 35.65 | 33.81 | 827.81/829.43 |
    218.42/214.38 |'
  prefs: []
  type: TYPE_TB
- en: '| domain Eexpert | plain text | 4.47 | 25.70 | 90.15/83.43 | 28.83/130.51 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | simple CoT few-shot | 37.90 | 37.91 | 909.81/911.43 |
    128.41/128.29 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | complex CoT few-shot | 42.77 | 44.05 | 2943.81/2945.43
    | 328.99/326.11 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | domain expert | plain text | 76.60 | 73.62 | 88.31/90.98
    | 125.07/114.58 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | simple CoT few-shot | 82.00 | 79.15 | 772.05/773.70 |
    107.23/108.31 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | complex CoT few-shot | 83.30 | 82.94 | 2419.00/2416.00
    | 82.00/49.00 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-turbo | domain expert | plain text | 88.18 | 88.48 | 87.31/88.98 |
    166.97/167.41 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | simple CoT few-shot | 92.61 | 92.34 | 770.05/771.70 |
    146.83/149.67 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | domain expert | plain text | 84.33 | 83.17 | 87.31/88.98 | 78.00/81.73
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | simple CoT few-shot | 93.95 | 92.95 | 770.05/771.70 |
    101.51/103.62 |'
  prefs: []
  type: TYPE_TB
- en: '| MetaMath-7b | domain expert | plain text | 92.48 | 66.19 | 109.15/110.80
    | 48.03/54.68 |'
  prefs: []
  type: TYPE_TB
- en: '| MetaMath-13b | domain expert | plain text | 92.81 | 70.43 | 109.15/110.80
    | 47.26/56.36 |'
  prefs: []
  type: TYPE_TB
- en: '| PaLM | “Follow example” | simple CoT few-shot | 63.05 | 62.17 | 860.04/861.70
    | 115.15/115.51 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | complex CoT few-shot | 66.95 | 64.14 | 1918.04/1919.70
    | 110.68/110.69 |'
  prefs: []
  type: TYPE_TB
- en: (b) CSQA dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | System Prompting | Content Prompting | Training Set Accuracy | Testing
    Set Accuracy | avg input length | avg output length |'
  prefs: []
  type: TYPE_TB
- en: '| (%) | (%) | (Training/Testing) | (Training/Testing) |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7b | NA | plain text | 33.76 | 34.23 | 52.36/52.06 | 86.44/81.03
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | standard few-shot | 58.86 | 63.06 | 446.36/446.06 | 512.00/512.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | CoT few-shot | 64.72 | 67.65 | 640.36/640.06 | 512.00/512.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-13b | NA | plain text | 29.45 | 32.10 | 52.36/52.06 | 287.60/287.34
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | standard few-shot | 65.29 | 66.83 | 446.36/446.06 | 512.00/511.80
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | CoT few-shot | 68.19 | 71.17 | 640.30/640.06 | 512.00/512.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | NA | plain text | 71.37 | 73.96 | 56.74/56.44 | 4.48/4.49
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | standard few-shot | 74.09 | 76.82 | 396.74/396.44 | 3.50/3.55
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | CoT few-shot | 68.31 | 68.55 | 575.74/575.44 | 16.51/16.74
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | NA | plain text | 79.86 | 83.46 | 56.74/56.44 | 4.71/4.69 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | standard few-shot | 84.29 | 87.14 | 396.74/396.44 | 2.00/2.00
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | CoT few-shot | 82.12 | 85.83 | 575.74/575.44 | 5.05/5.20
    |'
  prefs: []
  type: TYPE_TB
- en: (c) LLC dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | System Prompting | Content Prompting | Training Set Accuracy | Testing
    Set Accuracy | avg input length | avg output length |'
  prefs: []
  type: TYPE_TB
- en: '| (%) | (%) | (Training/Testing) | (Training/Testing) |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-7b | NA | plain text | 0.06 | 0.13 | 26.25/26.17 | 51.74/51.61 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | standard few-shot | 0.94 | 1.4 | 155.25/358.24 | 155.17/352.73
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | CoT few-shot | 44.23 | 44.60 | 344.25/344.17 | 71.77/71.11
    |'
  prefs: []
  type: TYPE_TB
- en: '| Llama-2-13b | NA | plain text | 9.01 | 9.73 | 26.25/26.17 | 55.01/54.55 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | standard few-shot | 2.41 | 2.93 | 155.25/155.17 | 239.28/243.78
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | CoT few-shot | 48.63 | 48.87 | 344.25/491.84 | 71.77/493.51
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | NA | plain text | 62.71 | 63.20 | 30.51/30.45 | 36.93/34.08
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | standard few-shot | 8.16 | 9.47 | 138.51/138.45 | 5.54/5.51
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | CoT few-shot | 87.13 | 86.53 | 304.51/304.45 | 63.01/62.86
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | NA | plain text | 80.54 | 81.73 | 30.51/29.92 | 36.93/30.24 |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | standard few-shot | 23.74 | 24.27 | 138.51/138.45 | 5.72/5.72
    |'
  prefs: []
  type: TYPE_TB
- en: '| “Follow example” | CoT few-shot | 92.68 | 93.2 | 304.51/304.45 | 63.00/62.86
    |'
  prefs: []
  type: TYPE_TB
- en: C.3 Ablation experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also run ablation experiments showing that prompt selection is useful, compared
    to using a fixed prompt (e.g., CoT). The results are shown in [Figure 17](#A3.F17
    "In C.3 Ablation experiments ‣ Appendix C Additional Results ‣ TREACLE: Thrifty
    Reasoning via Context-Aware LLM and Prompt Selection"), where TREACLE outperforms
    “TREACLE (CoT only)”, indicating that the ability to choose the prompt helps.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/470bb84acd5ee522390beb3d5635831f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Ablation study. Without TREACLE’s re-query and prompt selection,
    the performance decreases dramatically.'
  prefs: []
  type: TYPE_NORMAL
- en: C.4 Additional new LLM experiments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this subsection, we report additional results relating to [Section 5.2.1](#S5.SS2.SSS1
    "5.2.1 Addition of new LLMs ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty Reasoning
    via Context-Aware LLM and Prompt Selection"). The performance of the fine-tuned
    models with the API price adjustments or the improved open-source LLM is shown
    in [Figure 18](#A3.F18 "In C.4 Additional new LLM experiments ‣ Appendix C Additional
    Results ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection")
    (“Finetuned:GPT” and “Finetuned:Llama”, respectively). The results show that the
    fine-tuned model with both improvements (“Finetuned:all”, same as [Figure 7](#S5.F7
    "In 5.2.1 Addition of new LLMs ‣ 5.2 Results ‣ 5 Experiments ‣ TREACLE: Thrifty
    Reasoning via Context-Aware LLM and Prompt Selection")) performs the best. The
    sample efficiency results for fine-tuning these models with both types of changes
    (corresponding to “Finetuned:all”) are shown in [Figure 18](#A3.F18 "In C.4 Additional
    new LLM experiments ‣ Appendix C Additional Results ‣ TREACLE: Thrifty Reasoning
    via Context-Aware LLM and Prompt Selection").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/230856c3d993e926b13221141aa56e29.png)![Refer to caption](img/4aaae04bee6a6e871992f2a5e97855bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Additional new LLM results. Left: Zoomed in view of the accuracy
    with new GPT models, new Llama models, or both. Right: Sample efficiency with
    both new GPT models and new LLama models.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D API prices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[Table 5](#A3.T5 "In C.2 Different cost function parameters and datasets ‣
    Appendix C Additional Results ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM
    and Prompt Selection") shows further details on the parameters used in the cost
    functions described in [Section 3](#S3 "3 Problem Statement ‣ TREACLE: Thrifty
    Reasoning via Context-Aware LLM and Prompt Selection"). [Table 7](#A4.T7 "In Appendix
    D API prices ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection")
    shows the change of the GPT API’s monetary price in different API versions, relating
    to [Section 5.2.1](#S5.SS2.SSS1 "5.2.1 Addition of new LLMs ‣ 5.2 Results ‣ 5
    Experiments ‣ TREACLE: Thrifty Reasoning via Context-Aware LLM and Prompt Selection").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: API price GPT API price of different versions, where NA means no corresponding
    model at that version. The price unit is $/1K tokens. In our experiment setting,
    old version refers to the version of 0613 and the new version refers to the version
    1106.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | 0613 | 1106 | 0125 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| input | output | input | output | input | output |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5-turbo | 0.0015 | 0.002 | 0.001 | 0.002 | 0.0005 | 0.002 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4-turbo | NA | 0.01 | 0.03 | 0.01 | 0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 0.06 | 0.12 | 0.06 | 0.12 | 0.06 | 0.12 |'
  prefs: []
  type: TYPE_TB
- en: Appendix E Full Prompts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we show our full prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Domain expert prompting strategy (“Math solver” and “Math assistant”)
    in GSM8K dataset, where {Question} means that original question text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| System Prompt | User Content Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| You are a math solver. Give the answer to the following question. | {Question}
    |'
  prefs: []
  type: TYPE_TB
- en: '| ### Instruction: |  |'
  prefs: []
  type: TYPE_TB
- en: '| You are a math assistant. Solve the following problem. |  |'
  prefs: []
  type: TYPE_TB
- en: '| ### Problem: | {Question} |'
  prefs: []
  type: TYPE_TB
- en: '| {User Content Prompt} |  |'
  prefs: []
  type: TYPE_TB
- en: '| ### Answer: |  |'
  prefs: []
  type: TYPE_TB
- en: '| Let’s think step by step. |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Chain-of-Thought (CoT) few-shot prompting strategy in GSM8K dataset,
    where {Question} means that original question text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| System Prompt | User Content Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Follow the given examples and answer the question. | Question: There are
    15 trees in the grove. Grove workers will plant trees in the grove today. After
    they are done, there will be 21 trees. How many trees did the grove workers plant
    today? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Let’s think step by step |'
  prefs: []
  type: TYPE_TB
- en: '|  | There are 15 trees originally. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Then there were 21 trees after some more were planted. |'
  prefs: []
  type: TYPE_TB
- en: '|  | So there must have been 21 - 15 = 6. |'
  prefs: []
  type: TYPE_TB
- en: '|  | The answer is 6. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: If there are 3 cars in the parking lot and 2 more cars arrive,
    how many cars are in the parking lot? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Let’s think step by step |'
  prefs: []
  type: TYPE_TB
- en: '|  | There are originally 3 cars. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 2 more cars arrive. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3 + 2 = 5. |'
  prefs: []
  type: TYPE_TB
- en: '|  | The answer is 5. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: Leah had 32 chocolates and her sister had 42\. If they ate 35,
    how many pieces do they have left in total? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Let’s think step by step |'
  prefs: []
  type: TYPE_TB
- en: '|  | Originally, Leah had 32 chocolates. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Her sister had 42. |'
  prefs: []
  type: TYPE_TB
- en: '|  | So in total they had 32 + 42 = 74. |'
  prefs: []
  type: TYPE_TB
- en: '|  | After eating 35, they had 74 - 35 = 39. |'
  prefs: []
  type: TYPE_TB
- en: '|  | The answer is 39. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason
    has 12 lollipops. How many lollipops did Jason give to Denny? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Let’s think step by step |'
  prefs: []
  type: TYPE_TB
- en: '|  | Jason started with 20 lollipops. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Then he had 12 after giving some to Denny. |'
  prefs: []
  type: TYPE_TB
- en: '|  | So he gave Denny 20 - 12 = 8. |'
  prefs: []
  type: TYPE_TB
- en: '|  | The answer is 8. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: Shawn has five toys. For Christmas, he got two toys each from
    his mom and dad. How many toys does he have now? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Let’s think step by step |'
  prefs: []
  type: TYPE_TB
- en: '|  | Shawn started with 5 toys. |'
  prefs: []
  type: TYPE_TB
- en: '|  | If he got 2 toys each from his mom and dad, then that is 4 more toys.
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | 5 + 4 = 9. |'
  prefs: []
  type: TYPE_TB
- en: '|  | The answer is 9. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: There were nine computers in the server room. Five more computers
    were installed each day, from monday to thursday. How many computers are now in
    the server room? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Let’s think step by step |'
  prefs: []
  type: TYPE_TB
- en: '|  | There were originally 9 computers. |'
  prefs: []
  type: TYPE_TB
- en: '|  | For each of 4 days, 5 more computers were added. |'
  prefs: []
  type: TYPE_TB
- en: '|  | So 5 * 4 = 20 computers were added. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 9 + 20 is 29. |'
  prefs: []
  type: TYPE_TB
- en: '|  | The answer is 29. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: Michael had 58 golf balls. On tuesday, he lost 23 golf balls.
    On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Let’s think step by step |'
  prefs: []
  type: TYPE_TB
- en: '|  | Michael started with 58 golf balls. |'
  prefs: []
  type: TYPE_TB
- en: '|  | After losing 23 on tues- day, he had 58 - 23 = 35. |'
  prefs: []
  type: TYPE_TB
- en: '|  | After losing 2 more, he had 35 - 2 = 33 golf balls. |'
  prefs: []
  type: TYPE_TB
- en: '|  | The answer is 33. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: Olivia has $23\. She bought five bagels for $3 each. How much
    money does she have left? |'
  prefs: []
  type: TYPE_TB
- en: '|  | Let’s think step by step |'
  prefs: []
  type: TYPE_TB
- en: '|  | Olivia had 23 dollars. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. |'
  prefs: []
  type: TYPE_TB
- en: '|  | So she has 23 - 15 dollars left. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 23 - 15 is 8. |'
  prefs: []
  type: TYPE_TB
- en: '|  | The answer is 8. |'
  prefs: []
  type: TYPE_TB
- en: '|  | Question: {Question} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Standard few-shot prompting strategy in CSQA dataset, where {Question}
    means that original question text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| User Content Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Q: What do people use to absorb extra ink from a fountain pen? Answer Choices:
    (A) shirt pocket (B) calligrapher’s hand (C) inkwell (D) desk drawer (E) blotter
    |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer is E. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: What home entertainment equipment requires cable? Answer Choices: (A)
    radio shack (B) substation (C) television (D) cabinet |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer is C. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: The fox walked from the city into the forest, what was it looking for?
    Answer Choices: (A) pretty flowers (B) hen house (C) natural habitat (D) storybook
    |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer is B. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Sammy wanted to go to where the people were. Where might he go? Answer
    Choices: (A) populated areas (B) race track (C) desert (D) apartment (E) roadblock
    |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer is A. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Where do you put your grapes just before checking out? Answer Choices:
    (A) mouth (B) grocery cart (C)supermarket (D) fruit basket (E) fruit market |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer is B. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Google Maps and other highway and street GPS services have replaced what?
    Answer Choices: (A) united states (B) mexico (C) countryside (D) atlas |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer is D. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Before getting a divorce, what did the wife feel who was doing all the
    work? Answer Choices: (A) harder (B) anguish (C) bitterness (D) tears (E) sadness
    |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer is C. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: {Question } |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer is |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Chain-of-Thought (CoT) few-shot prompting strategy in CSQA dataset,
    where {Question} means that original question text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| User Content Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Q: What do people use to absorb extra ink from a fountain pen? Answer Choices:
    (A) shirt pocket (B) calligrapher’s hand (C) inkwell (D) desk drawer (E) blotter
    |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer must be an item that can absorb ink. Of the above choices,
    only blotters are used to absorb ink. The answer is E. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: What home entertainment equipment requires cable? Answer Choices: (A)
    radio shack (B) substation (C) television (D) cabinet |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer must require cable. Of the above choices, only television requires
    cable. The answer is C. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: The fox walked from the city into the forest, what was it looking for?
    Answer Choices: (A) pretty flowers (B) hen house (C) natural habitat (D) storybook
    |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer must be something in the forest. Of the above choices, only
    natural habitat is in the forest. The answer is B. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Sammy wanted to go to where the people were. Where might he go? Answer
    Choices: (A) populated areas (B) race track (C) desert (D) apartment (E) roadblock
    |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer must be a place with a lot of people. Of the above choices,
    only populated areas have a lot of people. The answer is A. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Where do you put your grapes just before checking out? Answer Choices:
    (A) mouth (B) grocery cart (C)supermarket (D) fruit basket (E) fruit market |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer should be the place where grocery items are placed before checking
    out. Of the above choices, grocery cart makes the most sense for holding grocery
    items. The answer is B. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Google Maps and other highway and street GPS services have replaced what?
    Answer Choices: (A) united states (B) mexico (C) countryside (D) atlas |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer must be something that used to do what Google Maps and GPS
    services do, which is to give directions. Of the above choices, only atlases are
    used to give directions. The answer is D. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: Before getting a divorce, what did the wife feel who was doing all the
    work? Answer Choices: (A) harder (B) anguish (C) bitterness (D) tears (E) sadness
    |'
  prefs: []
  type: TYPE_TB
- en: '| A: The answer should be the feeling of someone getting divorced who was doing
    all the work. Of the above choices, the closest feeling is bitterness. The answer
    is C. |'
  prefs: []
  type: TYPE_TB
- en: '| Q: {Question } |'
  prefs: []
  type: TYPE_TB
- en: '| A: |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Standard few-shot prompting strategy in LLC dataset, where {Question}
    means that original question text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| User Content Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Take the last letters of the words in “Elon Musk” and concatenate
    them. |'
  prefs: []
  type: TYPE_TB
- en: '| The answer is nk. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Take the last letters of the words in “Larry Page” and concatenate
    them. |'
  prefs: []
  type: TYPE_TB
- en: '| The answer is ye. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Take the last letters of the words in “Sergey Brin” and concatenate
    them. |'
  prefs: []
  type: TYPE_TB
- en: '| The answer is yn. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Take the last letters of the words in ”Bill Gates” and concatenate
    them. |'
  prefs: []
  type: TYPE_TB
- en: '| The answer is ls. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: {Question} |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Chain-of-Thought (CoT) few-shot prompting strategy in LLC dataset,
    where {Question} means that original question text.'
  prefs: []
  type: TYPE_NORMAL
- en: '| User Content Prompt |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Take the last letters of the words in “Elon Musk” and concatenate
    them. |'
  prefs: []
  type: TYPE_TB
- en: '| Let’s think step by step. |'
  prefs: []
  type: TYPE_TB
- en: '| The last letter of “Elon” is “n”. |'
  prefs: []
  type: TYPE_TB
- en: '| The last letter of “Musk” is “k”. |'
  prefs: []
  type: TYPE_TB
- en: '| Concatenating them is “nk”. |'
  prefs: []
  type: TYPE_TB
- en: '| The answer is nk. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Take the last letters of the words in “Larry Page” and concatenate
    them. |'
  prefs: []
  type: TYPE_TB
- en: '| Let’s think step by step. |'
  prefs: []
  type: TYPE_TB
- en: '| The last letter of ”Larry” is “y”. |'
  prefs: []
  type: TYPE_TB
- en: '| The last letter of ”Page” is “e”. |'
  prefs: []
  type: TYPE_TB
- en: '| Concatenating them is “ye”. |'
  prefs: []
  type: TYPE_TB
- en: '| The answer is ye. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Take the last letters of the words in “Sergey Brin” and concatenate
    them. |'
  prefs: []
  type: TYPE_TB
- en: '| Let’s think step by step. |'
  prefs: []
  type: TYPE_TB
- en: '| The last letter of “Sergey” is “y”. |'
  prefs: []
  type: TYPE_TB
- en: '| The last letter of ”Brin” is “n”. |'
  prefs: []
  type: TYPE_TB
- en: '| Concatenating them is “yn”. |'
  prefs: []
  type: TYPE_TB
- en: '| The answer is yn. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: Take the last letters of the words in ”Bill Gates” and concatenate
    them. |'
  prefs: []
  type: TYPE_TB
- en: '| Let’s think step by step. |'
  prefs: []
  type: TYPE_TB
- en: '| The last letter of “Bill” is “l”. |'
  prefs: []
  type: TYPE_TB
- en: '| The last letter of “Gates” is “s”. |'
  prefs: []
  type: TYPE_TB
- en: '| Concatenating them is “ls”. |'
  prefs: []
  type: TYPE_TB
- en: '| The answer is ls. |'
  prefs: []
  type: TYPE_TB
- en: '| Question: {Question} |'
  prefs: []
  type: TYPE_TB
- en: '| Let’s think step by step. |'
  prefs: []
  type: TYPE_TB
