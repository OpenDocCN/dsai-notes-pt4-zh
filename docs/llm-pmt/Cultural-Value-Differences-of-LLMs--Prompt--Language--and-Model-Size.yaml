- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:41:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:41:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Cultural Value Differences of LLMs: Prompt, Language, and Model Size'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLMs的文化价值差异：提示、语言和模型规模
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16891](https://ar5iv.labs.arxiv.org/html/2407.16891)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.16891](https://ar5iv.labs.arxiv.org/html/2407.16891)
- en: Qishuai Zhong Yike Yun  Aixin Sun
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 钟启帅 云逸克  孙爱欣
- en: Nanyang Technological University, Singapore(May 2024)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 南洋理工大学，新加坡（2024年5月）
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Our study aims to identify behavior patterns in cultural values exhibited by
    large language models (LLMs). The studied variants include question ordering,
    prompting language, and model size. Our experiments reveal that each tested LLM
    can efficiently behave with different cultural values. More interestingly: (i)
    LLMs exhibit relatively consistent cultural values when presented with prompts
    in a single language. (ii) The prompting language e.g., Chinese or English, can
    influence the expression of cultural values. The same question can elicit divergent
    cultural values when the same LLM is queried in a different language. (iii) Differences
    in sizes of the same model (e.g., Llama2-7B vs 13B vs 70B) have a more significant
    impact on their demonstrated cultural values than model differences (e.g., Llama2
    vs Mixtral). Our experiments reveal that query language and model size of LLM
    are the main factors resulting in cultural value differences.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究旨在识别大型语言模型（LLMs）在文化价值上的行为模式。研究的变量包括问题排序、提示语言和模型规模。我们的实验显示，每个测试过的LLM都能有效地展示不同的文化价值。更有趣的是：（i）当使用单一语言提示时，LLMs展现出相对一致的文化价值。（ii）提示语言（如中文或英文）可以影响文化价值的表达。同样的问题在不同语言中询问同一LLM时，可能引发不同的文化价值。（iii）相同模型的不同规模（例如，Llama2-7B
    vs 13B vs 70B）对其展示的文化价值的影响，比模型间的差异（例如，Llama2 vs Mixtral）更为显著。我们的实验揭示了查询语言和LLM模型规模是导致文化价值差异的主要因素。
- en: 'Cultural Value Differences of LLMs: Prompt, Language, and Model Size'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs的文化价值差异：提示、语言和模型规模
- en: Qishuai Zhong Yike Yun  Aixin Sun Nanyang Technological University, Singapore
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 钟启帅 云逸克  孙爱欣 南洋理工大学，新加坡
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Since GPT-3 Brown et al. ([2020](#bib.bib4)), Large Language Models (LLMs),
    capable of generating human-like text based on instructions, have garnered significant
    attention from both academia and industry. Numerous benchmarks and datasets have
    been created and employed to assess LLMs’ capability in generating human-like
    text across various tasks like question-answering, chatbot, and summarization Clark
    et al. ([2018](#bib.bib6)); Zellers et al. ([2019](#bib.bib43)); Hendrycks et al.
    ([2021a](#bib.bib14)). The open-source leaderboard Park ([2023](#bib.bib32)) allows
    researchers and engineers to directly compare language models across various dimensions,
    spanning from commonsense reasoning to advanced question answering, showcasing
    their respective abilities. However, focusing on information content while ignoring
    language’s social factors is currently a limitation of natural language processing
    (NLP) Hovy and Yang ([2021](#bib.bib17)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自GPT-3 Brown et al. ([2020](#bib.bib4))以来，大型语言模型（LLMs）能够基于指令生成类似人类的文本，已引起学术界和工业界的广泛关注。众多基准测试和数据集已被创建和使用，以评估LLMs在各类任务（如问答、聊天机器人和摘要生成）中生成类似人类文本的能力 Clark
    et al. ([2018](#bib.bib6)); Zellers et al. ([2019](#bib.bib43)); Hendrycks et al.
    ([2021a](#bib.bib14))。开放源码的排行榜 Park ([2023](#bib.bib32)) 允许研究人员和工程师在各个维度上直接比较语言模型，从常识推理到高级问答，展示其各自的能力。然而，当前自然语言处理（NLP）的一个限制是专注于信息内容而忽视了语言的社会因素 Hovy
    and Yang ([2021](#bib.bib17))。
- en: Given their capacity to generate human-like text, it is imperative to investigate
    whether LLMs demonstrate human-like behaviors stemming from the internalized values
    and cultural insights acquired from large-scale training corpora. As model-generated
    text gains wider adoption, ethical concerns arise due to the potential influence
    of cultural biases embedded in the generated text on its users Kumar et al. ([2023](#bib.bib25)).
    Hence, an emerging research trend involves quantifying the cultural biases within
    language models and understanding their impact on the models’ performance across
    various tasks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于大语言模型（LLMs）能够生成类似人类的文本，调查这些模型是否展示出源于从大规模训练语料中内化的价值观和文化洞察的类似人类行为显得至关重要。随着模型生成的文本被广泛采用，伦理问题逐渐浮现，因为生成文本中嵌入的文化偏见可能对用户产生影响 Kumar
    et al. ([2023](#bib.bib25))。因此，一种新兴的研究趋势是量化语言模型中的文化偏见，并理解这些偏见对模型在各类任务中的表现的影响。
- en: The primary methods for assessing values in LLMs typically involve using social
    science and psychological instruments originally designed for humans Feng et al.
    ([2023](#bib.bib9)); Arora et al. ([2023](#bib.bib1)) to assess various cultural
    aspects quantitatively, or by developing specialized datasets to examine model
    biases Parrish et al. ([2022](#bib.bib34)); Huang and Xiong ([2023](#bib.bib18)).
    Many studies on social science instruments primarily evaluate text generated by
    models in English. However, historical linguistic research, such as the Whorfian
    hypothesis proposed by Sapir-Whorf, suggests that language structure significantly
    influences individual perceptions and worldviews Kay and Kempton ([1984](#bib.bib21)).
    Research has shown that cultural accommodation occurs when individuals engage
    in multilingual contexts, as evidenced by experiments with human subjects Harzing
    and Maznevski ([2002](#bib.bib13)). Similarly, multilingual language models, pre-trained
    on text from various languages, can inherit biases and inconsistencies from their
    training data Garrido-Muñoz  et al. ([2021](#bib.bib11)). Therefore, assessments
    using only English-based instruments may not fully capture the breadth of knowledge
    in multilingual models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLMs价值观的主要方法通常涉及使用最初为人类设计的社会科学和心理学工具Feng等（[2023](#bib.bib9)）；Arora等（[2023](#bib.bib1)）以定量方式评估各种文化方面，或通过开发专门的数据集来检验模型偏差Parrish等（[2022](#bib.bib34)）；Huang和Xiong（[2023](#bib.bib18)）。许多社会科学工具的研究主要评估模型生成的英文文本。然而，历史语言学研究，例如Sapir-Whorf提出的Whorfian假说，表明语言结构显著影响个人的感知和世界观Kay和Kempton（[1984](#bib.bib21)）。研究表明，当个体参与多语言环境时，会发生文化适应，如人类受试者的实验所示Harzing和Maznevski（[2002](#bib.bib13)）。同样，预先训练于各种语言文本的多语言模型可以继承其训练数据中的偏见和不一致性Garrido-Muñoz等（[2021](#bib.bib11)）。因此，仅使用基于英文的工具进行评估可能无法完全捕捉多语言模型的知识广度。
- en: 'To provide a more comprehensive understanding of LLMs’ cultural values, this
    study investigated patterns of cultural values expressed by different models using
    three distinct approaches: (i) experimenting with varied prompts in a single language,
    (ii) using prompts in different languages, and (iii) conducting experiments across
    different models. The pipeline is visualized in Figure [3](#A1.F3 "Figure 3 ‣
    Appendix A Investigation Pipeline and Prompt Format ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size") in Appendix [A](#A1 "Appendix A Investigation
    Pipeline and Prompt Format ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size"). All sets of experiments were designed and implemented using
    Hofstede’s latest Value Survey Module (VSM) Hofstede and Hofstede ([2016](#bib.bib16)),
    a data collection instrument that quantifies cultural values across six dimensions Taras
    et al. ([2023](#bib.bib39)).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供对LLMs文化价值观的更全面理解，本研究使用三种不同的方法调查了不同模型表达的文化价值模式：（i）在单一语言中试验不同的提示，（ii）使用不同语言的提示，以及（iii）在不同模型间进行实验。该流程在附录[A](#A1
    "附录 A 调查流程和提示格式 ‣ LLM的文化价值差异：提示、语言和模型大小")的图[3](#A1.F3 "图 3 ‣ 附录 A 调查流程和提示格式 ‣
    LLM的文化价值差异：提示、语言和模型大小")中进行了可视化。所有实验集均使用Hofstede最新的价值调查模块（VSM）Hofstede和Hofstede（[2016](#bib.bib16)）设计和实施，这是一种量化六个维度文化价值的数据收集工具Taras等（[2023](#bib.bib39)）。
- en: 'A total of 6 LLMs were involved in our experiments, with each model being provided
    54 simulated identities to contextualize its response to the VSM questionnaire.
    Through our investigation, we found that: (i) LLMs consistently demonstrate similar
    cultural values within a single language, despite variations in prompt content.
    However, their responses are affected by alterations in the positioning of options.
    (ii) LLMs show notably different cultural values across different languages; and
    (iii) Differences in the cultural values expressed by models correlate with variations
    in text generation proficiency. For the last finding, although we considered the
    models’ text generation proficiency in our study to conduct our analysis and support
    our findings, further assessment of the models’ generation capability is beyond
    the scope of our research.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验涉及了总共6个大语言模型，每个模型被提供了54个模拟身份，以便对其在VSM问卷中的响应进行背景设置。通过我们的调查，我们发现： (i) 尽管提示内容有所不同，语言模型在单一语言中始终展示出类似的文化价值观。然而，它们的回应受到选项位置变化的影响。
    (ii) 语言模型在不同语言中显示出明显不同的文化价值观； (iii) 模型表达的文化价值观的差异与文本生成能力的差异相关。关于最后一点，尽管我们在研究中考虑了模型的文本生成能力以进行分析并支持我们的发现，但进一步评估模型的生成能力超出了我们研究的范围。
- en: 2 Related Work
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Several studies have contributed to detecting social and cultural biases displayed
    by models, as values can be inferred from the expression of biases. Another approach
    is to incorporate social science models for a direct evaluation of the values
    inherent in the models. We review both approaches.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究对模型表现出的社会和文化偏见进行了探讨，因为这些价值观可以从偏见的表现中推断出来。另一种方法是将社会科学模型纳入其中，直接评估模型固有的价值观。我们回顾了这两种方法。
- en: 2.1 Bias Study of Language Model
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 语言模型的偏见研究
- en: Assessing social and cultural biases in language models is crucial to mitigate
    associated risks and reveal the values embodied by the models. Liang et al. ([2021](#bib.bib27))
    provided a formal comprehension of social biases in language models. The work
    identified fine-grained local biases and high-level global biases as sources of
    representational biases and proposed the evaluation metrics for measurement. Subsequently,
    it introduced the mitigation method. Sheng et al. ([2021](#bib.bib37)) presented
    the first comprehensive survey on societal biases in language generation in 2021,
    identifying their negative impact and exploring methods for evaluation and mitigation.
    The study highlighted the challenge of bias assessment due to the open-domain
    nature of NLG and the diverse conceptualizations of bias across cultures. Recently,
    more studies have focused on evaluating bias and values in large language models,
    with innovative methodologies employed. Cheng et al. ([2023](#bib.bib5)) utilized
    the concept of markedness, initially linguistic but now a part of social science,
    to evaluate models’ stereotypes unsupervisedly. Meanwhile, Kotek et al. ([2023](#bib.bib22))
    employed a direct method to assess gender bias in LLMs, revealing models’ tendency
    to reflect imbalances over gender due to training on skewed datasets. In Ferrara
    ([2023](#bib.bib10)), bias in generative language models was defined and its sources,
    such as training data and model specifications, were investigated. However, the
    study also acknowledged that some biases may persist inevitably due to the inherent
    nature of language and cultural norms.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 评估语言模型中的社会和文化偏见对于减轻相关风险和揭示模型所体现的价值观至关重要。Liang等人（[2021](#bib.bib27)）对语言模型中的社会偏见进行了正式的理解。这项工作识别了细粒度的局部偏见和高层次的全球偏见作为表征偏见的来源，并提出了测量的评估指标。随后，介绍了缓解方法。Sheng等人（[2021](#bib.bib37)）在2021年首次全面调查了语言生成中的社会偏见，识别了其负面影响，并探索了评估和缓解的方法。研究突出了由于NLG的开放域特性和各文化对偏见的多样化概念，偏见评估的挑战。最近，更多研究集中于评估大型语言模型中的偏见和价值观，并采用了创新的方法。Cheng等人（[2023](#bib.bib5)）利用了标记性这一概念，最初是语言学的，但现在已成为社会科学的一部分，来无监督地评估模型的刻板印象。与此同时，Kotek等人（[2023](#bib.bib22)）采用了直接方法评估大语言模型中的性别偏见，揭示了模型因训练于不平衡的数据集而反映性别不平衡的倾向。在Ferrara（[2023](#bib.bib10)）中，生成语言模型中的偏见被定义，并调查了其来源，如训练数据和模型规格。然而，该研究也承认，由于语言和文化规范的固有性质，一些偏见可能不可避免地存在。
- en: Previous studies have demonstrated diverse techniques for accurately and efficiently
    identifying biases. However, they have also underscored the challenges in mitigating
    biases in generated text, as biases can be inherited from human language and culture
    in training data. This indicates that the exhibited values of models are shaped
    by the training data, making it impossible to dissociate the influence of training
    data when trying to understand the patterns of values expressed by models.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究已经展示了多种准确高效识别偏差的技术。然而，它们也突显了在生成文本中减轻偏差的挑战，因为偏差可能会从训练数据中的人类语言和文化中继承。这表明模型展示的价值观受到训练数据的影响，使得在试图理解模型表达的价值模式时，无法剥离训练数据的影响。
- en: 2.2 Social Science Models
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 社会科学模型
- en: While studies have investigated language models’ social and cultural biases,
    there’s still relatively less systematic exploration of how these models exhibit
    values under varying circumstances. Quantifying results in this domain is challenging.
    Consequently, research instruments initially focused on humans have been integrated
    into understanding language models’ values. Feng et al. ([2023](#bib.bib9)) utilized
    the political compass test to map the political leaning of language models in
    a two-dimensional space. Through the experiments conducted, the study demonstrated
    that pretrained language models are influenced by the political leaning inherent
    in the training data. Regarding culture measurement, Hofstede’s Value Survey Module
    (VSM) Hofstede and Hofstede ([2016](#bib.bib16)) and the World Values Survey Inglehart
    et al. ([2014](#bib.bib19)) were employed by Arora et al. ([2023](#bib.bib1))
    to explore cross-cultural values embedded in multilingual masked language models.
    The evaluation covered 13 languages to probe the models’ cultural values across
    13 cultures. The findings indicated that pretrained language models captured noticeable
    differences in values between cultures, albeit with weak correlations to values
    surveys. Kovač et al. ([2023](#bib.bib23)) utilized three human psychology questionnaires
    to assess how models’ expression of values changes with varying contexts, such
    as varying paragraphs and textual formats. They introduced the metaphor “LLM as
    a superposition of perspectives" to highlight the context-dependent nature of
    LLM behavior. Shu et al. ([2023](#bib.bib38)) created a dataset covering various
    persona measurement instruments to evaluate the consistency of LLMs’ "personality"
    across different prompts with minor variations. Their experiments revealed that
    even minor perturbations notably impacted the models’ question-answering performance.
    Therefore, they argued the current practice of prompting is insufficient to accurately
    capture model perceptions. The aforementioned articles challenged the practice
    of using psychological models to reveal personalities by regarding language models
    as individuals Bodroza et al. ([2023](#bib.bib3)); Pan and Zeng ([2023](#bib.bib31)).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究已经探讨了语言模型的社会和文化偏差，但关于这些模型在不同情况下如何展示价值观的系统性探索仍然相对较少。在这一领域量化结果是具有挑战性的。因此，最初专注于人类的研究工具被整合进了对语言模型价值观的理解中。Feng
    等人（[2023](#bib.bib9)）利用政治罗盘测试将语言模型的政治倾向映射到二维空间。通过进行实验，研究表明预训练语言模型受到训练数据中固有的政治倾向的影响。在文化测量方面，Hofstede
    的价值观调查模块（VSM）Hofstede 和 Hofstede（[2016](#bib.bib16)）以及世界价值观调查 Inglehart 等人（[2014](#bib.bib19)）被
    Arora 等人（[2023](#bib.bib1)）用来探索多语言掩码语言模型中的跨文化价值观。评估覆盖了13种语言，以探讨模型在13种文化中的文化价值观。结果表明，预训练语言模型捕捉到了文化间价值观的显著差异，尽管与价值观调查的相关性较弱。Kovač
    等人（[2023](#bib.bib23)）利用三份人类心理学问卷来评估模型在不同背景下（如不同段落和文本格式）表达价值观的变化。他们引入了“LLM 作为视角的叠加”这一隐喻，以突出
    LLM 行为的背景依赖性。Shu 等人（[2023](#bib.bib38)）创建了一个涵盖各种人格测量工具的数据集，以评估 LLM 在不同提示下“人格”的一致性。他们的实验揭示了即使是微小的扰动也会显著影响模型的问答表现。因此，他们认为目前的提示实践不足以准确捕捉模型的感知。上述文章挑战了通过将语言模型视为个体来使用心理模型揭示人格的做法
    Bodroza 等人（[2023](#bib.bib3)）；Pan 和 Zeng（[2023](#bib.bib31)）。
- en: Summarized from previous research, it is clear that prompt engineering and training
    data significantly impact how models express values. However, there is a need
    for a systematic study to evaluate these factors comprehensively. In our study,
    we systematically explore the expression of cultural values by models under varying
    circumstances, including the effects of prompt engineering, language differences,
    and model capabilities.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从以往研究总结来看，提示工程和训练数据显著影响模型表达价值的方式。然而，需要进行系统研究以全面评估这些因素。在我们的研究中，我们系统地探讨了模型在不同情况下表达文化价值的情况，包括提示工程、语言差异和模型能力的影响。
- en: 3 Measures by VSM
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 VSM 测量
- en: Similarly to previous studies, we utilize a value survey and additional measurement
    metrics to evaluate the alignment of cultural values in the LLMs. Value Survey
    Module (VSM) Hofstede and Hofstede ([2016](#bib.bib16)) is for measuring cultural
    values as outlined in Hofstede’s Cultural Dimensions Theory Gerlach and Eriksson
    ([2021](#bib.bib12)). Despite facing criticism for its psychometric deficiencies Taras
    et al. ([2023](#bib.bib39)) and simplicity Ercan et al. ([1991](#bib.bib8)), its
    value representation has become a cornerstone for a substantial body of research
    on cross-cultural differences in values Arora et al. ([2023](#bib.bib1)). In this
    study, we utilize the latest version of the survey (VSM 2013) as the foundational
    assessment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与以往的研究类似，我们利用价值调查和额外的测量指标来评估LLMs中文化价值的对齐情况。Hofstede和Hofstede（[2016](#bib.bib16)）的价值调查模块（VSM）用于测量文化价值，如Hofstede的文化维度理论中所述Gerlach和Eriksson（[2021](#bib.bib12)）。尽管面临心理测量缺陷Taras等（[2023](#bib.bib39)）和简化Ercan等（[1991](#bib.bib8)）的批评，其价值表示已成为大量跨文化价值差异研究的基石Arora等（[2023](#bib.bib1)）。在本研究中，我们使用调查的最新版本（VSM
    2013）作为基础评估。
- en: 'The value test is structured as a questionnaire with 24 questions to evaluate
    the interviewees’ cultural values. Another six questions intended to gather background
    information about the interviewees are excluded from our study. The complete questionnaire
    is in Appendix [J](#A10 "Appendix J VSM Questionnaire ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size"). Each question offers respondents
    five options, labeled with option IDs from 1 to 5\. Option IDs also serve as raw
    scores for each question. The authors of the VSM further developed a scoring system
    based on each question’s raw score, comprising six dimensions for measuring cultural
    values: Power Distance (PDI), Individualism (IDV), Uncertainty Avoidance (UAI),
    Masculinity (MAS), Long-term Orientation (LTO), and Indulgence (IVR). Each dimension
    is calculated using a formula with the raw scores from four survey questions.
    The complete list of formulas is in Appendix [B](#A2 "Appendix B VSM Dimension
    Formula ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size").'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '价值测试被设计为一个包含24个问题的问卷，以评估受访者的文化价值观。另有六个问题用于收集受访者的背景信息，但这些问题不包括在我们的研究中。完整的问卷见附录
    [J](#A10 "Appendix J VSM Questionnaire ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size")。每个问题提供五个选项，选项ID从1到5。选项ID也作为每个问题的原始分数。VSM的作者进一步开发了一个评分系统，基于每个问题的原始分数，包含六个测量文化价值的维度：权力距离（PDI）、个人主义（IDV）、不确定性规避（UAI）、男性气质（MAS）、长期导向（LTO）和享乐主义（IVR）。每个维度使用一个公式计算，公式包含四个调查问题的原始分数。完整的公式列表见附录
    [B](#A2 "Appendix B VSM Dimension Formula ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size")。'
- en: 'All experiments are conducted using prompts derived from the questionnaire.
    The prompt is delivered in a zero-shot manner, and the LLM is expected to respond
    in JSON format, specifying the chosen option ID and the rationale behind the selection.
    We require models to respond with option IDs to mitigate the performance degradation
    outlined by Zheng et al. ([2024](#bib.bib44)). Prompt samples are depicted in
    Figure [4](#A1.F4 "Figure 4 ‣ Appendix A Investigation Pipeline and Prompt Format
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size") in Appendix [A](#A1
    "Appendix A Investigation Pipeline and Prompt Format ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size"). In each prompt, we give instructions
    on the reply format, provide a survey question, and supply a simulated background
    identity. The simulation provided a target for the model to contextualize the
    response. Contextual simulation or targeting specific groups of people is a common
    methodology used by previous studies to guide the generation Kovač et al. ([2023](#bib.bib23));
    Narayanan Venkit et al. ([2023](#bib.bib29)); Ramezani and Xu ([2023](#bib.bib35));
    Cheng et al. ([2023](#bib.bib5)).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验都使用从问卷中衍生出的提示进行。提示以零样本的方式传递，期望LLM以JSON格式响应，指定所选择的选项ID以及选择理由。我们要求模型以选项ID进行回应，以减轻郑等人（[2024](#bib.bib44)）所指出的性能退化。提示样本在附录[A](#A1
    "附录 A 调查流程和提示格式 ‣ LLM的文化价值差异：提示、语言和模型规模")的图[4](#A1.F4 "图 4 ‣ 附录 A 调查流程和提示格式 ‣
    LLM的文化价值差异：提示、语言和模型规模")中展示。在每个提示中，我们提供了回复格式的说明、一个调查问题，并提供了一个模拟背景身份。模拟为模型提供了一个背景，以便上下文化地回应。上下文化模拟或针对特定群体是先前研究中常用的方法论，用于引导生成，如Kovač等人（[2023](#bib.bib23)）；Narayanan
    Venkit等人（[2023](#bib.bib29)）；Ramezani和Xu（[2023](#bib.bib35)）；Cheng等人（[2023](#bib.bib5)）。
- en: 3.1 Experiment Set
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验组
- en: 'The experiment conducted in this study consists of multiple experiment sets.
    Each set is defined by a unique combination of three hyper-parameters: (i) the
    tested LLM, (ii) the prompt language, and (iii) whether options are shuffled.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中进行的实验由多个实验组组成。每组由三个超参数的独特组合定义：（i）测试的LLM，（ii）提示语言，以及（iii）选项是否被打乱。
- en: 'Within each experiment set, the language model was presented with a curated
    collection of simulated identities, each comprising three variables: (i) nationality,
    (ii) age, and (iii) gender to furnish context for the model’s responses to questions.
    The study encompasses nine nationalities (refer to the full list in Appendix [C](#A3
    "Appendix C Nationalities for Experiment ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size")), two genders, and three age groups (25, 35,
    45), resulting in a total of 54 identities. These variables align with the VSM
    survey, encompassing interviewees from various countries, genders, and ages. The
    chosen nations are globally diverse, representing a range of cultures. To prevent
    coincidence, each question was queried ten times with different seeds. Consequently,
    we could collect $10\times 24\times 54=12960$ responses for each experiment set.
    During the analysis, we calculate the average of the ten outputs as the final
    output for a simulated identity, which is used as a single data point (a 24-d
    vector) in the experiment set.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个实验组中，语言模型呈现了一个经过精心挑选的模拟身份集合，每个身份包括三个变量：（i）国籍，（ii）年龄，以及（iii）性别，以为模型回答问题提供背景。研究涵盖了九种国籍（参见附录[C](#A3
    "附录 C 实验的国籍 ‣ LLM的文化价值差异：提示、语言和模型规模")中的完整列表）、两种性别和三个年龄组（25、35、45），共计54个身份。这些变量与VSM调查相一致，包括来自不同国家、性别和年龄的受访者。选择的国家具有全球多样性，代表了各种文化。为了防止巧合，每个问题用不同的种子查询了十次。因此，我们可以为每个实验组收集到$10\times
    24\times 54=12960$个响应。在分析过程中，我们计算十次输出的平均值作为模拟身份的最终输出，这作为实验组中的一个单一数据点（一个24维向量）。
- en: 3.2 Measures by VSM Raw Scores
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 VSM原始分数的测量
- en: Each set of responses from a simulated identity is represented as a 24-dimensional
    vector, essential for comparisons within and between experiment sets. To evaluate
    the strength of relationships between these groups, we calculate the Pearson correlation
    coefficients ($\rho$, then we reject the null hypothesis and conclude that there
    is a significant relationship between the vectors.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每组来自模拟身份的响应表示为一个24维向量，这对于在实验组内和组间进行比较至关重要。为了评估这些组之间关系的强度，我们计算皮尔逊相关系数（$\rho$），然后我们拒绝原假设，得出向量之间存在显著关系的结论。
- en: 3.3 Measures by VSM Scores
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 VSM分数的测量
- en: 'Using VSM formulas in Appendix [B](#A2 "Appendix B VSM Dimension Formula ‣
    Cultural Value Differences of LLMs: Prompt, Language, and Model Size"), we can
    generate 6-dimensional score vectors (i.e., PDI, IDV, UAI, MAS, LTO, and IVR)
    from the 24-d vectors.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用附录 [B](#A2 "附录 B VSM 维度公式 ‣ LLM 的文化价值差异：提示、语言和模型大小") 中的 VSM 公式，我们可以从 24-d
    向量生成 6 维度的分数向量（即 PDI、IDV、UAI、MAS、LTO 和 IVR）。
- en: Intra-set Disparity Measurement
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验集内差异测量
- en: 'In Hofstede’s research, VSM scores are analyzed nationally to explore cultural
    value differences between countries. The scores for the nine nations involved
    in this study are displayed in Appendix [D](#A4 "Appendix D Human Results of VSM
    Scores ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size"),
    where each nation’s score represents the average of all responses from its interviewees.
    Similarly, we calculate the national average for model responses of each experiment
    set. We then use the standard deviation, denoted as $\sigma_{m}(v_{i})$ for the
    human results.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在霍夫斯泰德的研究中，VSM 分数按国家进行分析，以探索不同国家之间的文化价值差异。本研究中涉及的九个国家的分数显示在附录 [D](#A4 "附录 D
    VSM 分数的人类结果 ‣ LLM 的文化价值差异：提示、语言和模型大小") 中，每个国家的分数代表其受访者所有回答的平均值。类似地，我们计算每个实验集的模型回应的国家平均值。然后我们使用标准差，用
    $\sigma_{m}(v_{i})$ 表示人类结果。
- en: 'The mean values for each dimension, across all experiment sets and human results,
    range from $-60$, indicating that comparing the disparity between models and human
    results is reasonable. Appendix [E](#A5 "Appendix E Mean Values of VSM Scores
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size") provides
    the complete list of mean values.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 各维度的均值，在所有实验集和人类结果中，范围从 $-60$，这表明比较模型与人类结果之间的差异是合理的。附录 [E](#A5 "附录 E VSM 分数均值
    ‣ LLM 的文化价值差异：提示、语言和模型大小") 提供了均值的完整列表。
- en: 'We then define the distance among nations observed for humans as $D_{h}$ as
    the “Model Cultural Disparity (MCD).", shown in Eq. [3](#S3.E3 "Equation 3 ‣ Intra-set
    Disparity Measurement ‣ 3.3 Measures by VSM Scores ‣ 3 Measures by VSM ‣ Cultural
    Value Differences of LLMs: Prompt, Language, and Model Size"):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将观察到的人类国家间的距离定义为 $D_{h}$，即“模型文化差异 (MCD)”，如公式 [3](#S3.E3 "方程 3 ‣ 实验集内差异测量 ‣
    3.3 VSM 分数的度量 ‣ 3 通过 VSM 的度量 ‣ LLM 的文化价值差异：提示、语言和模型大小") 所示。
- en: '|  | $\displaystyle D_{h}$ |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle D_{h}$ |  | (1) |'
- en: '|  | $\displaystyle D_{m}$ |  | (2) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle D_{m}$ |  | (2) |'
- en: '|  | $\displaystyle MCD$ |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle MCD$ |  | (3) |'
- en: MCD compares the dispersion of cultural values exhibited by models based on
    simulated nations to that observed among humans in Hofstede’s study.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: MCD 比较了基于模拟国家的模型所展示的文化价值的离散程度与霍夫斯泰德研究中观察到的人类之间的离散程度。
- en: Inter-set Disparity Measurement
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验集间差异测量
- en: The intra-set disparity underscores the impact of contextual information on
    the models’ expression of cultural values. Furthermore, our pipeline uses inter-set
    disparity to explore how changes in any of the three hyper-parameters—shuffling
    of options, language, and the tested model—affect the expression of cultural values.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 实验集内的差异强调了上下文信息对模型表达文化价值的影响。此外，我们的流程使用实验集间的差异来探索三种超参数——选项的打乱、语言和测试模型——的变化如何影响文化价值的表达。
- en: 'We employ clustering methodologies, Davies-Bouldin Index (DBI) Davies and Bouldin
    ([1979](#bib.bib7)) and the Silhouette Score ($SS$) Rousseeuw ([1987](#bib.bib36)),
    to assess the effectiveness of separation between each pair of experiment sets.
    Detailed descriptions of the two metrics can be found in Appendix [F](#A6 "Appendix
    F Clustering Measurement Methods ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"). In our study, we pre-define the model responses from
    each experiment set as a cluster, comprising 54 data points, as detailed in Section [3.1](#S3.SS1
    "3.1 Experiment Set ‣ 3 Measures by VSM ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size").'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用聚类方法，包括 Davies-Bouldin 指数 (DBI) Davies 和 Bouldin ([1979](#bib.bib7)) 和轮廓系数
    ($SS$) Rousseeuw ([1987](#bib.bib36))，来评估每对实验集之间的分离效果。两种度量的详细描述可以在附录 [F](#A6 "附录
    F 聚类测量方法 ‣ LLM 的文化价值差异：提示、语言和模型大小") 中找到。在我们的研究中，我们预定义每个实验集的模型回应为一个集群，包含 54 个数据点，详细信息见第 [3.1](#S3.SS1
    "3.1 实验集 ‣ 3 通过 VSM 的度量 ‣ LLM 的文化价值差异：提示、语言和模型大小") 节。
- en: Additionally, we have introduced a new measurement method, the Silhouette Score
    with Human Reference ($SS_{h}$), to measure the absolute disparity between pairs
    of sets, taking human results as the reference point.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们引入了一种新的测量方法——带有人类参考的轮廓系数（$SS_{h}$），以测量成对集合之间的绝对差异，以人为结果为参考点。
- en: '$SS_{h}$ is designed based on the Silhouette Score, utilizing nationally aggregated
    average VSM scores:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: $SS_{h}$ 是基于轮廓系数设计的，利用了全国汇总的平均 VSM 分数：
- en: '|  | $\displaystyle a_{h}(n_{i})$ |  | (4) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle a_{h}(n_{i})$ |  | (4) |'
- en: '|  | $\displaystyle SS_{h}$ |  | (5) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle SS_{h}$ |  | (5) |'
- en: where $a_{h}(n_{i})$ value exceeding one suggests that the separation between
    the two sets is more pronounced than the disparity observed among humans from
    various nations.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$a_{h}(n_{i})$ 值超过一，表明两个集合之间的分离比不同国家的人的差异更为明显。
- en: 4 Experiment Setting and RQs
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验设置和 RQs
- en: 'Given that the value survey is structured as a questionnaire, we have specifically
    chosen and employed models fine-tuned for chat purposes for this study. A total
    of six models are evaluated in this study, including members of the Llama2 family Touvron
    et al. ([2023](#bib.bib40)): Llama2-7b-chat-hf, Llama2-13b-chat-hf, and Llama2-70b-chat-hf;
    members of the Qwen family Bai et al. ([2023](#bib.bib2)): Qwen-14b-chat and Qwen-72b-chat;
    and Mixtral-8x7B-Instruct-v0.1 Jiang et al. ([2024](#bib.bib20)), which features
    a different architecture from the other models.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于价值观调查被结构化为问卷，我们特意选择并使用了经过聊天目的微调的模型进行本研究。本研究评估了总共六个模型，包括 Llama2 家族成员 Touvron
    等人（[2023](#bib.bib40)）：Llama2-7b-chat-hf、Llama2-13b-chat-hf 和 Llama2-70b-chat-hf；Qwen
    家族成员 Bai 等人（[2023](#bib.bib2)）：Qwen-14b-chat 和 Qwen-72b-chat；以及 Mixtral-8x7B-Instruct-v0.1
    Jiang 等人（[2024](#bib.bib20)），该模型具有与其他模型不同的架构。
- en: All experiments were conducted using Vllm Kwon et al. ([2023](#bib.bib26)) with
    Transformers Wolf et al. ([2020](#bib.bib42)) to achieve faster inference. We
    utilized four Nvidia A6000 cards with CUDA 12.2\. We used the default config.json
    and framework parameters for the models’ text generation.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验都使用了 Vllm Kwon 等人（[2023](#bib.bib26)）和 Transformers Wolf 等人（[2020](#bib.bib42)）以实现更快的推理。我们使用了四张
    Nvidia A6000 显卡和 CUDA 12.2。我们使用了模型文本生成的默认 config.json 和框架参数。
- en: Through experiments, we aim to gain insights into three Research Questions (RQs).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实验，我们旨在深入了解三个研究问题（RQs）。
- en: 'RQ1: Can large language models consistently express cultural values when presented
    with perturbed questions in a single language? We focus on how responses of the
    same model vary with changes in contextual information and option order shuffling.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: RQ1：大型语言模型在遇到同一语言中扰动的问题时，是否能一致地表达文化价值观？我们关注同一模型的响应如何随着上下文信息的变化和选项顺序的打乱而有所不同。
- en: 'RQ2: How does language affect the expression of cultural values in models?
    We examine the consistency of cultural values expressed by models when identical
    questions are posed in different languages.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: RQ2：语言如何影响模型中文化价值观的表达？我们检查了当相同的问题用不同语言提出时，模型表达的文化价值观的一致性。
- en: 'RQ3: What can we infer about models’ expression of cultural values when comparing
    them? We evaluate whether models from the same family show more consistent cultural
    values and investigate how differences in text generation capabilities relate
    to variations in cultural values.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: RQ3：在比较模型时，我们可以推断出关于模型表达文化价值观的什么信息？我们评估了来自同一家庭的模型是否展示出更一致的文化价值观，并调查了文本生成能力的差异如何与文化价值观的变化相关。
- en: 5 Experiment Results
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验结果
- en: Among the 24 questions in the VSM 2013 survey, questions 15 and 18 pertain to
    the interviewee’s recent mental and physical health. Consequently, we assign the
    most neutral option (option 3) to these two questions. We similarly assign option
    3 for any unrecognizable responses from the models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VSM 2013 调查的 24 个问题中，第 15 和第 18 问涉及被访者近期的心理和身体健康。因此，我们将最中立的选项（选项 3）分配给这两个问题。对于模型中的任何无法识别的回答，我们也同样分配选项
    3。
- en: Initially, we requested responses in Chinese when querying models with Chinese
    prompts. However, about 7% of Llama2-7b-chat-hf’s responses and 24% of Llama2-13b-chat-hf’s
    responses were unrecognizable, in contrast to other models which had at least
    99% recognizable responses. As a result, these two models are required to respond
    in English to Chinese prompts.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们在用中文提示查询模型时请求中文回答。然而，大约 7% 的 Llama2-7b-chat-hf 的回答和 24% 的 Llama2-13b-chat-hf
    的回答无法识别，相比之下，其他模型的可识别回答率至少为 99%。因此，这两个模型需要对中文提示做出英文回应。
- en: 5.1 Prompt Variants (RQ1)
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 提示变体（RQ1）
- en: 'To evaluate a single model’s consistency in expressing cultural values within
    a single language, we developed prompt variations focusing on two aspects: simulated
    identity and options order. The former modifies only the context presented to
    the model, whereas the latter entails further prompt engineering. The impact of
    simulated identity is assessed within the experiment set, while the effectiveness
    of options order is evaluated through inter-set methods.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估单一模型在单一语言中表达文化价值的一致性，我们开发了关注两个方面的提示变体：模拟身份和选项顺序。前者仅修改呈现给模型的上下文，而后者则涉及进一步的提示工程。模拟身份的影响在实验组内评估，而选项顺序的有效性通过组间方法进行评估。
- en: Simulated Identity
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模拟身份
- en: Within each experiment set, the model is queried with 54 simulated identities.
    VSM raw scores and intra-set measurements are used to examine the impact of simulated
    identities.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在每组实验中，模型会被查询54个模拟身份。VSM原始分数和组内测量用于检验模拟身份的影响。
- en: 'Based on raw scores, each tested model consistently produces results with a
    similar distribution, irrespective of changes in the context. The correlation
    coefficients for the average score vectors, grouped by context variables in Table [5](#A6.T5
    "Table 5 ‣ Appendix F Clustering Measurement Methods ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size") in the Appendix, indicate that responses
    across different identities are highly correlated.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '基于原始分数，每个测试模型在上下文变化的情况下持续产生具有相似分布的结果。附录中表[5](#A6.T5 "Table 5 ‣ Appendix F Clustering
    Measurement Methods ‣ Cultural Value Differences of LLMs: Prompt, Language, and
    Model Size")中按上下文变量分组的平均分数向量的相关系数表明，不同身份之间的响应高度相关。'
- en: 'Similarly, the intra-set measurements based on VSM scores presented in Table [6](#A6.T6
    "Table 6 ‣ Appendix F Clustering Measurement Methods ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size"), show that the simulated nations assigned
    to the LLMs exhibit significantly less cultural value diversity compared to the
    differences observed among human interviewees from those nations.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '同样，基于VSM分数的组内测量（见表[6](#A6.T6 "Table 6 ‣ Appendix F Clustering Measurement Methods
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size")）显示，分配给LLMs的模拟国家的文化价值多样性显著低于在这些国家的人类受访者之间观察到的差异。'
- en: In summary, the evaluated models produce responses with relatively consistent
    cultural values and show limited sensitivity to changes in the context of the
    prompts. The cultural values learned from the training corpus help mitigate the
    effects of variations in the simulated identities provided in the context.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，评估的模型产生的响应具有相对一致的文化价值，并且对提示的上下文变化表现出有限的敏感性。从训练语料库中学习到的文化价值有助于减轻上下文中模拟身份变化的影响。
- en: '| Models | $DBI$ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | $DBI$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama2-7b-chat-hf | 1.837 | 0.169 | 0.430 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf | 1.837 | 0.169 | 0.430 |'
- en: '| Llama2-13b-chat-hf | 1.694 | 0.205 | 0.228 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf | 1.694 | 0.205 | 0.228 |'
- en: '| Llama2-70b-chat-hf | 0.658 | 0.572 | 0.574 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf | 0.658 | 0.572 | 0.574 |'
- en: '| Qwen-14b-chat | 0.981 | 0.409 | 1.033 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat | 0.981 | 0.409 | 1.033 |'
- en: '| Qwen-72b-chat | 0.825 | 0.478 | 0.483 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat | 0.825 | 0.478 | 0.483 |'
- en: '| Mixtral-8x7B | 0.542 | 0.641 | 0.680 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 0.542 | 0.641 | 0.680 |'
- en: 'Table 1: Results of three measurements are listed in the table to quantify
    the disparity between model responses for the two sets, “Eng w/o shuffled options"
    and “Eng w. shuffled options". Figures showing the greatest distinctness are highlighted
    in bold in each column.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：表格列出了三种测量结果，以量化模型响应在两个组，“Eng w/o shuffled options”和“Eng w. shuffled options”之间的差异。每列中显示出最大区别的数字用粗体标出。
- en: Shuffled Options
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 打乱选项
- en: As noted by Zheng et al. ([2024](#bib.bib44)), LLMs are susceptible to selection
    bias, primarily due to token bias and, to a lesser extent, position bias, both
    of which originate from the training data. Accordingly, our experiment maintains
    the original option IDs and their corresponding text, only altering their positions
    to minimize token bias.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如郑等人（[2024](#bib.bib44)）所指出的，LLMs易受选择偏差的影响，主要是由于令牌偏差，较少的是位置偏差，这些都源于训练数据。因此，我们的实验保持了原始选项ID及其对应的文本，仅改变其位置以最小化令牌偏差。
- en: We evaluate the consistency of the model’s cultural values despite selection
    bias by analyzing changes in the distribution of raw scores and measuring the
    inter-set disparity between the "Eng" and "Eng w. Shuffle" experiment sets for
    each model.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过分析原始分数分布的变化，并测量每个模型“Eng”和“Eng w. Shuffle”实验集之间的跨集差异，以评估模型文化价值的一致性，尽管存在选择偏差。
- en: 'The Centroid vector of each experiment set represents the distribution of the
    set. The correlation coefficient and $p$-value are computed between the centroids,
    with comprehensive results presented in Table [7](#A7.T7 "Table 7 ‣ G.2 Shuffled
    Options ‣ Appendix G Experiments Results for RQ1 ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size") in Appendix. These results indicate
    that most models maintain highly correlated score distributions after option shuffling.
    However, the overall correlation scores are noticeably lower than those calculated
    for simulated identities.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '每个实验集的质心向量表示该集的分布。计算质心之间的相关系数和 $p$-值，综合结果见附录中的表 [7](#A7.T7 "Table 7 ‣ G.2 Shuffled
    Options ‣ Appendix G Experiments Results for RQ1 ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size")。这些结果表明，大多数模型在选项随机化后维持了高度相关的分数分布。然而，与模拟身份计算的相关性分数相比，整体相关性分数明显较低。'
- en: 'The inter-set disparity measurement results, as shown in Table [1](#S5.T1 "Table
    1 ‣ Simulated Identity ‣ 5.1 Prompt Variants (RQ1) ‣ 5 Experiment Results ‣ Cultural
    Value Differences of LLMs: Prompt, Language, and Model Size"), display the effect
    of shuffling to models from the aspect of VSM score. The results of $DBI$, we
    find that most models exhibit a noticeable absolute shift in cultural values between
    the sets, which does not correspond to the significant differences observed among
    humans from diverse nations.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '跨集差异测量结果，如表 [1](#S5.T1 "Table 1 ‣ Simulated Identity ‣ 5.1 Prompt Variants
    (RQ1) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size") 所示，从 VSM 分数的角度展示了随机化对模型的影响。$DBI$ 的结果表明，大多数模型在各集合之间的文化价值存在明显的绝对偏移，这与来自不同国家的人类之间观察到的显著差异不相符。'
- en: The experiment results show that models remain vulnerable to selection bias,
    consistent with the findings reported in Zheng et al. ([2024](#bib.bib44)). Unlike
    human behavior, models fail to maintain consistent cultural values in the face
    of textual ambiguities.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果表明，模型仍然易受选择偏差影响，这与郑等人（[2024](#bib.bib44)）报告的发现一致。与人类行为不同，模型在面对文本模糊性时未能保持一致的文化价值。
- en: '|  |  |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '|  |  |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |'
- en: '| ![Refer to caption](img/bf2e752f8fa2fc051cfdcc6ae358f79f.png) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ![参见说明](img/bf2e752f8fa2fc051cfdcc6ae358f79f.png) |'
- en: '![Refer to caption](img/21f21afb5a576ec3f5b741198466c04f.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/21f21afb5a576ec3f5b741198466c04f.png)'
- en: (a) Llama2-7b-chat-hf
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: (a) Llama2-7b-chat-hf
- en: '![Refer to caption](img/b817ea9da0f5966f140f88bb0377d012.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b817ea9da0f5966f140f88bb0377d012.png)'
- en: (b) Llama2-13b-chat-hf
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Llama2-13b-chat-hf
- en: '![Refer to caption](img/106622dd854500212597f3d6d0423d31.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/106622dd854500212597f3d6d0423d31.png)'
- en: (c) Llama2-70b-chat-hf
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Llama2-70b-chat-hf
- en: '![Refer to caption](img/5d6e501e1914ce5a31a160c5311698e4.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5d6e501e1914ce5a31a160c5311698e4.png)'
- en: (d) Qwen-14b-chat
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: (d) Qwen-14b-chat
- en: '![Refer to caption](img/283f5f90826c162d533374a9b242e04e.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/283f5f90826c162d533374a9b242e04e.png)'
- en: (e) Qwen-72b-chat
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: (e) Qwen-72b-chat
- en: '![Refer to caption](img/d4b32f0fb3b97bff741cd03ee2045d59.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4b32f0fb3b97bff741cd03ee2045d59.png)'
- en: (f) Mixtral-8x7B
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: (f) Mixtral-8x7B
- en: 'Figure 1: The 6-d VSM scores for different experiment sets for each model are
    visualized using the t-SNE technique van der Maaten and Hinton ([2008](#bib.bib41))
    to facilitate direct comparisons. Results from English queries (denoted as "Eng")
    are displayed with black circles; results from English with Shuffled Options (denoted
    as "Eng w. Shuffle") are shown with pink stars; and results from Chinese (denoted
    as "Chn") are represented by green squares.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用 t-SNE 技术（van der Maaten 和 Hinton ([2008](#bib.bib41)）对每个模型的不同实验集的 6-d
    VSM 分数进行可视化，以便进行直接比较。来自英语查询（标记为“Eng”）的结果以黑色圆圈显示；来自英语和随机选项（标记为“Eng w. Shuffle”）的结果以粉色星星表示；来自中文（标记为“Chn”）的结果以绿色方块表示。
- en: 'Model response distributions across different experiment sets are visualized
    using t-SNE in Figure [1](#S5.F1 "Figure 1 ‣ Shuffled Options ‣ 5.1 Prompt Variants
    (RQ1) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size") van der Maaten and Hinton ([2008](#bib.bib41)). The visualization
    also indicates that most models demonstrate less or comparable separation effectiveness
    between sets divided by “Shuffling of options" compared to those split by “Language".'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 各种实验集中的模型响应分布通过t-SNE在图 [1](#S5.F1 "图 1 ‣ 洗牌选项 ‣ 5.1 提示变体 (RQ1) ‣ 5 实验结果 ‣ LLMs的文化价值差异：提示、语言和模型大小") van der
    Maaten 和 Hinton ([2008](#bib.bib41))中进行了可视化。可视化结果还表明，大多数模型在“选项洗牌”与“语言”划分的实验集之间的分离效果较少或相当。
- en: 5.2 Language Variants (RQ2)
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 语言变体 (RQ2)
- en: '| Models | $DBI$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | $DBI$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Llama2-7b-chat-hf | 0.962 | 0.423 | 1.357 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf | 0.962 | 0.423 | 1.357 |'
- en: '| Llama2-13b-chat-hf | 0.720 | 0.533 | 0.581 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf | 0.720 | 0.533 | 0.581 |'
- en: '| Llama2-70b-chat-hf | 0.799 | 0.499 | 0.707 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf | 0.799 | 0.499 | 0.707 |'
- en: '| Qwen-14b-chat | 1.846 | 0.215 | 0.622 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat | 1.846 | 0.215 | 0.622 |'
- en: '| Qwen-72b-chat | 0.529 | 0.646 | 0.961 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat | 0.529 | 0.646 | 0.961 |'
- en: '| Mixtral-8x7B | 0.651 | 0.581 | 0.660 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 0.651 | 0.581 | 0.660 |'
- en: 'Table 2: Results of three measurements are listed in the table to quantify
    the disparity between model responses for the two sets, “Eng w/o shuffled options"
    and “Chn w/o shuffled options". Figures showing the greatest distinctness are
    highlighted in bold in each column.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：列出了三个测量结果，以量化模型响应在两个集合“Eng w/o shuffled options”和“Chn w/o shuffled options”之间的差异。每列中显示出最大区别的数字以粗体标出。
- en: In addition to varying prompts within the same language, we conduct experiments
    to evaluate each model’s behavior when prompted in English and Chinese. For the
    Chinese queries, we carefully crafted prompts using the Chinese version of the
    VSM 2013 questionnaires. Contextual information of the simulated identities is
    manually translated.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在相同语言中变化提示外，我们还进行实验来评估每个模型在用英语和中文提示时的表现。对于中文查询，我们精心编写了使用VSM 2013问卷的中文版本的提示。模拟身份的背景信息由人工翻译。
- en: Correlation coefficients and $p$-values for other models remain below the threshold,
    the overall correlation coefficient is lower than that observed with prompt variants.
    This suggests that language impacts the models’ choice of options more significantly
    than the shuffling of option order.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 其他模型的相关系数和 $p$-值保持在阈值以下，整体相关系数低于提示变体的观察值。这表明语言对模型选择选项的影响比选项顺序洗牌更为显著。
- en: 'In addition to the raw scores, the inter-set disparity measurement results
    based on VSM scores are detailed in Table [2](#S5.T2 "Table 2 ‣ 5.2 Language Variants
    (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size"), with a comprehensive analysis of values provided in Appendix [H](#A8
    "Appendix H Experiment Results for RQ2 ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"). Based on the results of $DBI$ results suggest that
    when queried with the same questions in a different language, the model is expected
    to exhibit cultural values with a variability of at least 50%, akin to that of
    an individual from another country. Language differences can result in a more
    distinctive separation in expressing cultural values. The t-SNE figures in Figure [1](#S5.F1
    "Figure 1 ‣ Shuffled Options ‣ 5.1 Prompt Variants (RQ1) ‣ 5 Experiment Results
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size") also
    clearly illustrate that most models express cultural values more variably when
    queried in different languages.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 除了原始分数外，基于VSM分数的集间差异测量结果详细列在表 [2](#S5.T2 "表 2 ‣ 5.2 语言变体 (RQ2) ‣ 5 实验结果 ‣ LLMs的文化价值差异：提示、语言和模型大小")中，附录 [H](#A8
    "附录 H RQ2实验结果 ‣ LLMs的文化价值差异：提示、语言和模型大小")提供了详细的值分析。根据 $DBI$ 的结果，当用不同语言查询相同的问题时，模型预计会表现出至少50%的文化价值变异性，类似于来自其他国家的个体。语言差异可能导致在表达文化价值时更明显的分离。图 [1](#S5.F1
    "图 1 ‣ 洗牌选项 ‣ 5.1 提示变体 (RQ1) ‣ 5 实验结果 ‣ LLMs的文化价值差异：提示、语言和模型大小")中的t-SNE图也清楚地表明，大多数模型在用不同语言查询时更具文化价值的变异性。
- en: The discrepancies between the initial measurements and $SS_{h}$ formula considers
    inter-set distance with human disparity (a constant), providing an absolute measure
    of inter-set disparity.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 初始测量值与$SS_{h}$公式之间的差异考虑了人类差异（一个常量）的集合间距离，提供了集合间差异的绝对度量。
- en: Summarizing the findings, we observe that language significantly influences
    the models’ responses and the cultural values expressed by those responses. This
    observation aligns with research findings Norton ([1997](#bib.bib30)) that suggest
    values are commonly conveyed through language. We argue that the diverse cultural
    values expressed by the model in various languages are acquired from the distinct
    training corpora of those languages, similar to other types of knowledge transferred
    from training corpora to the language model Lin et al. ([2019](#bib.bib28)); Krishna
    et al. ([2023](#bib.bib24)).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 总结发现，我们观察到语言显著影响模型的回应及这些回应所表达的文化价值。这一观察与Norton（[1997](#bib.bib30)）的研究结果一致，研究表明价值观通常通过语言传达。我们认为，模型在不同语言中表达的多样化文化价值是从这些语言的独特训练语料库中获得的，类似于其他类型的知识从训练语料库转移到语言模型（Lin等人，[2019](#bib.bib28)；Krishna等人，[2023](#bib.bib24)）。
- en: '|  |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '|  |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  |  |'
- en: '![Refer to caption](img/58b7a59867609f49493ff385c2df35dc.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/58b7a59867609f49493ff385c2df35dc.png)'
- en: (a) $SS_{h}$ among Models with English Questions
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 具有英语问题的模型间$SS_{h}$
- en: '![Refer to caption](img/9b0aeec0930ead0f74192b516661e45d.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9b0aeec0930ead0f74192b516661e45d.png)'
- en: (b) MMLU Distance among Models
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 模型间的MMLU距离
- en: '![Refer to caption](img/054c89e3db41056edb46f5486bf51df8.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/054c89e3db41056edb46f5486bf51df8.png)'
- en: (c) $SS_{h}$ among Models with Chinese Questions
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 具有中文问题的模型间$SS_{h}$
- en: '![Refer to caption](img/87ea1ae07cc2da7c63e603d6241b9a7c.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/87ea1ae07cc2da7c63e603d6241b9a7c.png)'
- en: (d) $SS_{h}$ among Models cross Languages
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 跨语言模型间的$SS_{h}$
- en: 'Figure 2: The three red heatmaps display the $SS_{h}$ values among models,
    with darker colors highlighting greater disparities. The green heatmap displays
    the differences in MMLU scores among models, corresponding to the disparities
    observed in the adjacent red heatmap.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：三张红色热图显示了模型间的$SS_{h}$值，颜色越深表示差异越大。绿色热图显示了模型间MMLU分数的差异，对应于邻近红色热图中观察到的差异。
- en: 5.3 Models Comparison (RQ3)
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 模型比较 (RQ3)
- en: 'We now analyze the patterns of cultural values expressed by different models
    based on their inter-set disparity. This analysis encompasses three types of comparisons:
    (i) among models queried solely in English (without “shuffling"), (ii) among models
    queried solely in Chinese, and (iii) cross-language comparisons. All comparisons
    utilize $SS_{h}$ values. We represent all three comparison subsets with heatmap
    charts, as shown in Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language Variants (RQ2)
    ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size").'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在根据模型之间的集合差异分析不同模型所表达的文化价值模式。这项分析包括三种类型的比较：（i）仅用英语查询的模型之间的比较（没有“洗牌”），（ii）仅用中文查询的模型之间的比较，以及（iii）跨语言比较。所有比较都使用$SS_{h}$值。我们用热图展示所有三种比较子集，如图[2](#S5.F2
    "图 2 ‣ 5.2 语言变体 (RQ2) ‣ 5 实验结果 ‣ LLM的文化价值差异：提示、语言和模型大小")所示。
- en: 'Observations from Heatmaps (a) and (c) in Figure [2](#S5.F2 "Figure 2 ‣ 5.2
    Language Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of
    LLMs: Prompt, Language, and Model Size") reveal that models from the same family
    do not necessarily exhibit closer cultural value alignment. Additionally, all
    Llama2 models, irrespective of size, are trained using the same datasets for the
    same duration Touvron et al. ([2023](#bib.bib40)). The Qwen technical report Bai
    et al. ([2023](#bib.bib2)) also indicates that identical datasets and hyperparameters
    are applied across various model sizes during pretraining and fine-tuning stages
    (SRF and RLHF).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从图[2](#S5.F2 "图 2 ‣ 5.2 语言变体 (RQ2) ‣ 5 实验结果 ‣ LLM的文化价值差异：提示、语言和模型大小")中的热图 (a)
    和 (c) 的观察结果显示，来自同一家族的模型不一定表现出更接近的文化价值对齐。此外，所有Llama2模型，无论大小如何，都是使用相同的数据集在相同的时长内进行训练的（Touvron等人，[2023](#bib.bib40)）。Qwen技术报告（Bai等人，[2023](#bib.bib2)）也表明，在预训练和微调阶段（SRF和RLHF）中，不同模型大小应用了相同的数据集和超参数。
- en: 'Based on the findings: (i) models from the same family do not guarantee consistency
    in expressing cultural values; (ii) models with the same background receive uniform
    training; and (iii) larger models within the same family demonstrate better text-generation
    performance. We can deduce that variations in cultural values among models of
    the same family are linked to differences in their text-generation capabilities
    instead of training data. Larger models in the same family are guaranteed to handle
    complex patterns, understand context more effectively, and generalize better to
    unseen data. As a result, they are more adept at comprehending questions posed
    in value tests and generating more appropriate responses compared to smaller models.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些发现：（i）同一家族的模型在表达文化价值观方面并不保证一致；（ii）具有相同背景的模型接受了统一的训练；（iii）同一家族中的较大模型在文本生成性能上表现更好。我们可以推断，同一家族模型之间的文化价值观变化与其文本生成能力的差异有关，而不是训练数据。同一家族中的较大模型能够处理复杂模式，更有效地理解上下文，并更好地泛化到未见数据。因此，与较小模型相比，它们在理解价值测试中的问题和生成更合适的回答方面更为熟练。
- en: 'We further link our findings with the evaluation results of generation. A common
    evaluation all six models have undergone is the MMLU (Massive Multitask Language
    Understanding) test Hendrycks et al. ([2021b](#bib.bib15)). Differences in MMLU
    scores among models are displayed in Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language
    Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"), Heatmap (b). A large $SS_{h}$ value between models.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步将我们的发现与生成评估结果联系起来。所有六个模型都经历了MMLU（大规模多任务语言理解）测试Hendrycks等（[2021b](#bib.bib15)）。模型间的MMLU分数差异显示在图[2](#S5.F2
    "Figure 2 ‣ 5.2 Language Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value
    Differences of LLMs: Prompt, Language, and Model Size")，热图（b）。模型之间的$SS_{h}$值很大。'
- en: 'Additionally, in the heatmap (d) of Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language
    Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"), the overall disparities between models across languages
    are significantly larger than those observed within a single language. The marked
    inter-set disparities noted in cross-language comparisons indicate that language
    variations can cause substantial differences in cultural values among models.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，在图[2](#S5.F2 "Figure 2 ‣ 5.2 Language Variants (RQ2) ‣ 5 Experiment Results
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size")的热图（d）中，跨语言模型之间的整体差异显著大于单一语言内部的差异。跨语言比较中显著的组间差异表明，语言变体可以导致模型之间的文化价值观发生显著差异。'
- en: Our hypothesis that differences in cultural values correlate with variations
    in model capabilities is based on observations. Developing a testing mechanism
    that simultaneously evaluates text quality, the expression of cultural values,
    and their alignment is part of future work. This approach will enhance our understanding
    of how language model performance impacts the expression of cultural values.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关于文化价值观差异与模型能力变化相关的假设是基于观察结果的。开发一个同时评估文本质量、文化价值观表达及其对齐情况的测试机制是未来的工作之一。这种方法将有助于我们更好地理解语言模型性能如何影响文化价值观的表达。
- en: 6 Conclusion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this study, we developed an investigative pipeline to assess the behavior
    of large language models concerning expressions of cultural values. Our results
    show that (i) Cultural values tend to remain relatively consistent across variations
    in prompts, especially when changes are limited to content alone. (ii) LLMs exhibit
    significantly divergent cultural values across different languages, and (iii)
    The difference in cultural values among models is relevant to variations in the
    models’ overall proficiency in text generation. Furthermore, upon comparing the
    results illustrating the second and third findings, we find that language variants
    can lead to greater disparities in cultural values. Language emerges as the most
    significant factor influencing the cultural values exhibited by the models.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们开发了一个调查流程来评估大型语言模型在表达文化价值观方面的行为。我们的结果表明：（i）文化价值观在提示变化时通常保持相对一致，特别是当变化仅限于内容时。（ii）不同语言之间的大型语言模型表现出显著不同的文化价值观，（iii）模型之间的文化价值观差异与模型在文本生成方面的整体能力差异有关。此外，在比较第二和第三个发现的结果时，我们发现语言变体可以导致文化价值观的更大差异。语言成为影响模型所展现的文化价值观的最重要因素。
- en: 7 Limitations
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 限制
- en: This study has a few limitations that require further investigation in future
    research. (i) We limited our exploration of cultural values expressed by models
    to the 24 questions of the VSM 2013 survey, which has been criticized for its
    simplicity. Therefore, future research should consider incorporating additional
    cultural value surveys to investigate the models’ behavior further. (ii) This
    study evaluated and assessed only six models. To further validate the findings
    regarding the models’ expression of cultural values and their performance differences,
    additional models should be explored and included in future studies. (iii) In
    our experiments, models are prompted within a narrowly defined context to generate
    responses in a zero-shot manner, conditioned solely on the provided context. Future
    studies should extend beyond direct prompts, exploring how models express cultural
    values when supplied with extensive past experiences and acting as believable
    agents Park et al. ([2023](#bib.bib33)). (iv) A new evaluation pipeline or mechanism
    needs to be designed to assess and quantify the relationship between specific
    cultural value patterns and the generated text’s quality. This would build upon
    the current finding that variations in text quality result in different cultural
    values. (v) Although we have observed variations in the cultural values of large
    language models when the same questions are asked in different languages, we have
    not thoroughly analyzed user preferences concerning these differences. Future
    research should develop a systematic approach to assess how language-induced disparities
    in cultural values impact users and to formulate strategies to mitigate any negative
    effects.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究存在一些局限性，未来研究需要进一步调查。（i）我们将对模型所表达的文化价值的探索限制在VSM 2013调查的24个问题上，该调查因其简单性而受到批评。因此，未来研究应考虑纳入更多的文化价值调查，以进一步探讨模型的行为。（ii）本研究仅评估了六个模型。为了进一步验证有关模型文化价值表达及其表现差异的发现，应在未来的研究中探索并纳入更多的模型。（iii）在我们的实验中，模型在狭义的上下文中被提示以零样本方式生成响应，仅依据提供的上下文。未来的研究应超越直接提示，探索模型在提供了大量过去经验并作为可信代理时如何表达文化价值
    [Park et al. (2023)](#bib.bib33)。（iv）需要设计一种新的评估流程或机制，以评估和量化特定文化价值模式与生成文本质量之间的关系。这将建立在当前发现的基础上，即文本质量的变化导致不同的文化价值。（v）尽管我们观察到大语言模型在用不同语言询问相同问题时文化价值的变化，但我们尚未深入分析用户对这些差异的偏好。未来的研究应开发系统的方法来评估语言引发的文化价值差异如何影响用户，并制定减轻任何负面影响的策略。
- en: 8 Ethical Consideration
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 伦理考量
- en: All experiments described in this study rely on data from the widely recognized
    Value Survey Module (VSM) 2013 Hofstede and Hofstede ([2016](#bib.bib16)) and
    utilize open-source language models. While our analysis includes human subject
    data, it is important to note that this data is derived from the well-established
    findings of the VSM 2013 study. Additionally, although our research examines the
    responses of various large language models to assess cultural values, we explicitly
    avoid ranking these models to maintain objectivity and ethical integrity.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究中描述的所有实验依赖于广泛认可的VSM 2013 Hofstede和Hofstede ([2016](#bib.bib16))的数据，并使用开源语言模型。虽然我们的分析包括了人类主体数据，但重要的是要指出，这些数据来源于VSM
    2013研究的成熟发现。此外，尽管我们的研究考察了各种大语言模型的响应以评估文化价值，我们明确避免对这些模型进行排名，以保持客观性和伦理完整性。
- en: References
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Arora et al. (2023) Arnav Arora, Lucie-Aimée Kaffee, and Isabelle Augenstein.
    2023. [Probing pre-trained language models for cross-cultural differences in values](https://arxiv.org/abs/2203.13722).
    *Preprint*, arXiv:2203.13722.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora等（2023）Arnav Arora、Lucie-Aimée Kaffee和Isabelle Augenstein。2023年。[探究预训练语言模型的跨文化价值差异](https://arxiv.org/abs/2203.13722)。*预印本*，arXiv:2203.13722。
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. [Qwen technical report](https://arxiv.org/abs/2309.16609).
    *Preprint*, arXiv:2309.16609.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. [Qwen技术报告](https://arxiv.org/abs/2309.16609)。*预印本*，arXiv:2309.16609。
- en: 'Bodroza et al. (2023) Bojana Bodroza, Bojana M. Dinic, and Ljubisa Bojic. 2023.
    [Personality testing of gpt-3: Limited temporal reliability, but highlighted social
    desirability of gpt-3’s personality instruments results](https://arxiv.org/abs/2306.04308).
    *Preprint*, arXiv:2306.04308.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bodroza et al. (2023) Bojana Bodroza, Bojana M. Dinic, and Ljubisa Bojic. 2023.
    [GPT-3的个性测试：有限的时间可靠性，但突显了GPT-3个性工具结果的社会期望](https://arxiv.org/abs/2306.04308)。*预印本*，arXiv:2306.04308。
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://arxiv.org/abs/2005.14165).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [语言模型是少样本学习者](https://arxiv.org/abs/2005.14165)。
- en: 'Cheng et al. (2023) Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023. [Marked
    personas: Using natural language prompts to measure stereotypes in language models](https://arxiv.org/abs/2305.18189).
    *Preprint*, arXiv:2305.18189.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cheng et al. (2023) Myra Cheng, Esin Durmus, and Dan Jurafsky. 2023. [标记化角色：使用自然语言提示来衡量语言模型中的刻板印象](https://arxiv.org/abs/2305.18189)。*预印本*，arXiv:2305.18189。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. [Think you have solved
    question answering? try arc, the ai2 reasoning challenge](https://api.semanticscholar.org/CorpusID:3922816).
    *ArXiv*, abs/1803.05457.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. [认为你已经解决了问答问题？试试ARC，AI2推理挑战](https://api.semanticscholar.org/CorpusID:3922816)。*ArXiv*，abs/1803.05457。
- en: Davies and Bouldin (1979) David L. Davies and Donald W. Bouldin. 1979. [A cluster
    separation measure](https://doi.org/10.1109/TPAMI.1979.4766909). *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, PAMI-1(2):224–227.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Davies and Bouldin (1979) David L. Davies and Donald W. Bouldin. 1979. [一种聚类分离度度量](https://doi.org/10.1109/TPAMI.1979.4766909)。*IEEE模式分析与机器智能学报*，PAMI-1(2):224–227。
- en: 'Ercan et al. (1991) G Ercan, Hamad Nasif, Bahman Al-Daeaj, and Mary S Ebrahimi.
    1991. Methodological problems in cross-cultural research: An updated review. *Management
    International Review*, pages 79–91.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ercan et al. (1991) G Ercan, Hamad Nasif, Bahman Al-Daeaj, and Mary S Ebrahimi.
    1991. 跨文化研究中的方法学问题：更新的综述。*国际管理评论*，页79–91。
- en: 'Feng et al. (2023) Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov.
    2023. [From pretraining data to language models to downstream tasks: Tracking
    the trails of political biases leading to unfair NLP models](https://doi.org/10.18653/v1/2023.acl-long.656).
    In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pages 11737–11762, Toronto, Canada. Association
    for Computational Linguistics.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng et al. (2023) Shangbin Feng, Chan Young Park, Yuhan Liu, and Yulia Tsvetkov.
    2023. [从预训练数据到语言模型再到下游任务：追踪政治偏见如何导致不公平的NLP模型](https://doi.org/10.18653/v1/2023.acl-long.656)。在*第61届计算语言学协会年会论文集（第1卷：长篇论文）*中，页11737–11762，多伦多，加拿大。计算语言学协会。
- en: Ferrara (2023) Emilio Ferrara. 2023. [Should chatgpt be biased? challenges and
    risks of bias in large language models](https://doi.org/10.5210/fm.v28i11.13346).
    *First Monday*.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ferrara（2023）埃米利奥·费拉拉。2023。 [ChatGPT 应该有偏见吗？大规模语言模型中的偏见挑战和风险](https://doi.org/10.5210/fm.v28i11.13346)。*第一周刊*。
- en: Garrido-Muñoz  et al. (2021) Ismael Garrido-Muñoz , Arturo Montejo-Ráez , Fernando
    Martínez-Santiago , and L. Alfonso Ureña-López . 2021. [A survey on bias in deep
    nlp](https://doi.org/10.3390/app11073184). *Applied Sciences*, 11(7).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Garrido-Muñoz 等（2021）伊斯梅尔·加里多-穆尼奥斯、阿图罗·蒙特霍-拉埃斯、费尔南多·马丁内斯-圣地亚哥和L·阿方索·乌雷尼亚-洛佩斯。2021。
    [深度自然语言处理中的偏见调查](https://doi.org/10.3390/app11073184)。*应用科学*，11(7)。
- en: 'Gerlach and Eriksson (2021) Philipp Gerlach and Kimmo Eriksson. 2021. Measuring
    cultural dimensions: External validity and internal consistency of hofstede’s
    VSM 2013 scales. *Front. Psychol.*, 12:662604.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gerlach 和 Eriksson（2021）菲利普·格拉赫和金莫·埃里克森。2021。测量文化维度：霍夫斯泰德 VSM 2013 标度的外部效度和内部一致性。*心理学前沿*，12:662604。
- en: 'Harzing and Maznevski (2002) Anne-Wil Harzing and Martha Maznevski. 2002. [The
    interaction between language and culture: A test of the cultural accommodation
    hypothesis in seven countries](https://doi.org/10.1080/14708470208668081). *Language
    and Intercultural Communication*, 2(2):120–139.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harzing 和 Maznevski（2002）安妮-威尔·哈辛和玛莎·马兹涅夫斯基。2002。 [语言与文化之间的互动：在七个国家对文化适应假设的检验](https://doi.org/10.1080/14708470208668081)。*语言与跨文化交流*，2(2)：120–139。
- en: Hendrycks et al. (2021a) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021a. Measuring massive multitask
    language understanding. *Proceedings of the International Conference on Learning
    Representations (ICLR)*.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2021a）丹·亨德里克斯、科林·伯恩斯、斯蒂文·巴萨特、安迪·邹、曼塔斯·马泽卡、道恩·宋和雅各布·斯坦赫特。2021a。测量大规模多任务语言理解。*国际学习表征会议论文集（ICLR）*。
- en: Hendrycks et al. (2021b) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021b. [Measuring massive multitask
    language understanding](https://arxiv.org/abs/2009.03300). *Preprint*, arXiv:2009.03300.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等（2021b）丹·亨德里克斯、科林·伯恩斯、斯蒂文·巴萨特、安迪·邹、曼塔斯·马泽卡、道恩·宋和雅各布·斯坦赫特。2021b。 [测量大规模多任务语言理解](https://arxiv.org/abs/2009.03300)。*预印本*，arXiv:2009.03300。
- en: 'Hofstede and Hofstede (2016) G Hofstede and G. J. Hofstede. 2016. VSM 2013.
    [https://geerthofstede.com/research-and-vsm/vsm-2013/](https://geerthofstede.com/research-and-vsm/vsm-2013/).
    Accessed: 2024-1-11.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hofstede 和 Hofstede（2016）G·霍夫斯泰德和G·J·霍夫斯泰德。2016。VSM 2013。 [https://geerthofstede.com/research-and-vsm/vsm-2013/](https://geerthofstede.com/research-and-vsm/vsm-2013/)。访问时间：2024-1-11。
- en: 'Hovy and Yang (2021) Dirk Hovy and Diyi Yang. 2021. [The importance of modeling
    social factors of language: Theory and practice](https://doi.org/10.18653/v1/2021.naacl-main.49).
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 588–602, Online.
    Association for Computational Linguistics.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hovy 和 Yang（2021）迪尔克·霍维和迪伊·杨。2021。 [建模语言社会因素的重要性：理论与实践](https://doi.org/10.18653/v1/2021.naacl-main.49)。在*2021年北美计算语言学协会年会：人类语言技术会议论文集*，第588–602页，在线。计算语言学协会。
- en: 'Huang and Xiong (2023) Yufei Huang and Deyi Xiong. 2023. [Cbbq: A chinese bias
    benchmark dataset curated with human-ai collaboration for large language models](https://arxiv.org/abs/2306.16244).
    *Preprint*, arXiv:2306.16244.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Huang 和 Xiong（2023）黄玉飞和熊德毅。2023。 [Cbbq: 一个通过人类-人工智能协作构建的中文偏差基准数据集，用于大规模语言模型](https://arxiv.org/abs/2306.16244)。*预印本*，arXiv:2306.16244。'
- en: 'Inglehart et al. (2014) R. Inglehart, C. Haerpfer, A. Moreno, C. Welzel, K. Kizilova,
    J. Diez-Medrano, M. Lagos, P. Norris, E. Ponarin, B. Puranen, and et al. 2014.
    [World values survey: Round six - country-pooled datafile version](/brokenurl#www.worldvaluessurvey.org/WVSDocumentationWV6.jsp).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inglehart 等（2014）R. 英格尔哈特、C. 哈尔普费尔、A. 莫雷诺、C. 韦泽尔、K. 基齐洛娃、J. 迪埃斯-梅德拉诺、M. 拉戈斯、P.
    诺里斯、E. 波纳林、B. 普拉嫩等。2014。 [世界价值观调查：第六轮 - 国家汇总数据文件版本](/brokenurl#www.worldvaluessurvey.org/WVSDocumentationWV6.jsp)。
- en: Jiang et al. (2024) Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample,
    Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep
    Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut
    Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024. [Mixtral of
    experts](https://arxiv.org/abs/2401.04088). *Preprint*, arXiv:2401.04088.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2024）Albert Q. Jiang、Alexandre Sablayrolles、Antoine Roux、Arthur Mensch、Blanche
    Savary、Chris Bamford、Devendra Singh Chaplot、Diego de las Casas、Emma Bou Hanna、Florian
    Bressand、Gianna Lengyel、Guillaume Bour、Guillaume Lample、Lélio Renard Lavaud、Lucile
    Saulnier、Marie-Anne Lachaux、Pierre Stock、Sandeep Subramanian、Sophia Yang、Szymon
    Antoniak、Teven Le Scao、Théophile Gervet、Thibaut Lavril、Thomas Wang、Timothée Lacroix
    和 William El Sayed。2024年。[专家的混合模型](https://arxiv.org/abs/2401.04088)。*预印本*，arXiv:2401.04088。
- en: Kay and Kempton (1984) Paul Kay and Willett Kempton. 1984. What is the sapir-whorf
    hypothesis? *Am. Anthropol.*, 86(1):65–79.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kay 和 Kempton（1984）Paul Kay 和 Willett Kempton。1984年。什么是萨丕尔-沃尔夫假设？*美洲人类学家*，86(1):65–79。
- en: Kotek et al. (2023) Hadas Kotek, Rikker Dockum, and David Sun. 2023. [Gender
    bias and stereotypes in large language models](https://doi.org/10.1145/3582269.3615599).
    In *Proceedings of The ACM Collective Intelligence Conference*, CI ’23, page 12–24,
    New York, NY, USA. Association for Computing Machinery.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotek 等（2023）Hadas Kotek、Rikker Dockum 和 David Sun。2023年。[大型语言模型中的性别偏见和刻板印象](https://doi.org/10.1145/3582269.3615599)。收录于
    *ACM集体智能会议论文集*，CI ’23，第12–24页，美国纽约。计算机协会。
- en: Kovač et al. (2023) Grgur Kovač, Masataka Sawayama, Rémy Portelas, Cédric Colas,
    Peter Ford Dominey, and Pierre-Yves Oudeyer. 2023. [Large language models as superpositions
    of cultural perspectives](https://arxiv.org/abs/2307.07870). *Preprint*, arXiv:2307.07870.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovač 等（2023）Grgur Kovač、Masataka Sawayama、Rémy Portelas、Cédric Colas、Peter
    Ford Dominey 和 Pierre-Yves Oudeyer。2023年。[大型语言模型作为文化视角的叠加](https://arxiv.org/abs/2307.07870)。*预印本*，arXiv:2307.07870。
- en: Krishna et al. (2023) Kundan Krishna, Saurabh Garg, Jeffrey P. Bigham, and Zachary C.
    Lipton. 2023. [Downstream datasets make surprisingly good pretraining corpora](https://arxiv.org/abs/2209.14389).
    *Preprint*, arXiv:2209.14389.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishna 等（2023）Kundan Krishna、Saurabh Garg、Jeffrey P. Bigham 和 Zachary C. Lipton。2023年。[下游数据集意外地成为了优秀的预训练语料库](https://arxiv.org/abs/2209.14389)。*预印本*，arXiv:2209.14389。
- en: 'Kumar et al. (2023) Sachin Kumar, Vidhisha Balachandran, Lucille Njoo, Antonios
    Anastasopoulos, and Yulia Tsvetkov. 2023. [Language generation models can cause
    harm: So what can we do about it? an actionable survey](https://doi.org/10.18653/v1/2023.eacl-main.241).
    In *Proceedings of the 17th Conference of the European Chapter of the Association
    for Computational Linguistics*, pages 3299–3321, Dubrovnik, Croatia. Association
    for Computational Linguistics.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等（2023）Sachin Kumar、Vidhisha Balachandran、Lucille Njoo、Antonios Anastasopoulos
    和 Yulia Tsvetkov。2023年。[语言生成模型可能造成的伤害：我们可以怎么办？一项可操作的调查](https://doi.org/10.18653/v1/2023.eacl-main.241)。收录于
    *第17届欧洲计算语言学协会会议论文集*，第3299–3321页，杜布罗夫尼克，克罗地亚。计算语言学协会。
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. [Efficient
    memory management for large language model serving with pagedattention](https://arxiv.org/abs/2309.06180).
    *Preprint*, arXiv:2309.06180.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等（2023）Woosuk Kwon、Zhuohan Li、Siyuan Zhuang、Ying Sheng、Lianmin Zheng、Cody
    Hao Yu、Joseph E. Gonzalez、Hao Zhang 和 Ion Stoica。2023年。[大语言模型服务的高效内存管理与分页注意力](https://arxiv.org/abs/2309.06180)。*预印本*，arXiv:2309.06180。
- en: Liang et al. (2021) Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan
    Salakhutdinov. 2021. [Towards understanding and mitigating social biases in language
    models](https://proceedings.mlr.press/v139/liang21a.html). In *Proceedings of
    the 38th International Conference on Machine Learning*, volume 139 of *Proceedings
    of Machine Learning Research*, pages 6565–6576\. PMLR.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2021）Paul Pu Liang、Chiyu Wu、Louis-Philippe Morency 和 Ruslan Salakhutdinov。2021年。[理解和缓解语言模型中的社会偏见](https://proceedings.mlr.press/v139/liang21a.html)。收录于
    *第38届国际机器学习会议论文集*，第139卷，*机器学习研究论文集*，第6565–6576页。PMLR。
- en: 'Lin et al. (2019) Yongjie Lin, Yi Chern Tan, and Robert Frank. 2019. [Open
    sesame: Getting inside bert’s linguistic knowledge](https://arxiv.org/abs/1906.01698).
    *Preprint*, arXiv:1906.01698.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2019）Yongjie Lin、Yi Chern Tan 和 Robert Frank。2019年。[开门见山：深入了解BERT的语言知识](https://arxiv.org/abs/1906.01698)。*预印本*，arXiv:1906.01698。
- en: Narayanan Venkit et al. (2023) Pranav Narayanan Venkit, Sanjana Gautam, Ruchi
    Panchanadikar, Ting-Hao Huang, and Shomir Wilson. 2023. [Nationality bias in text
    generation](https://doi.org/10.18653/v1/2023.eacl-main.9). In *Proceedings of
    the 17th Conference of the European Chapter of the Association for Computational
    Linguistics*, pages 116–122, Dubrovnik, Croatia. Association for Computational
    Linguistics.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayanan Venkit 等人（2023）Pranav Narayanan Venkit、Sanjana Gautam、Ruchi Panchanadikar、Ting-Hao
    Huang 和 Shomir Wilson。2023年。[文本生成中的国籍偏见](https://doi.org/10.18653/v1/2023.eacl-main.9)。收录于*第17届欧洲计算语言学协会会议论文集*，第116–122页，克罗地亚杜布罗夫尼克。计算语言学协会。
- en: Norton (1997) Bonny Norton. 1997. [Language, identity, and the ownership of
    english](http://www.jstor.org/stable/3587831). *TESOL Quarterly*, 31(3):409–429.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Norton（1997）Bonny Norton。1997年。[语言、身份与英语的所有权](http://www.jstor.org/stable/3587831)。*TESOL季刊*，31(3):409–429。
- en: Pan and Zeng (2023) Keyu Pan and Yawen Zeng. 2023. [Do llms possess a personality?
    making the mbti test an amazing evaluation for large language models](https://arxiv.org/abs/2307.16180).
    *Preprint*, arXiv:2307.16180.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 和 Zeng（2023）Keyu Pan 和 Yawen Zeng。2023年。[大型语言模型是否拥有个性？将MBTI测试作为大型语言模型的惊人评估](https://arxiv.org/abs/2307.16180)。*预印本*，arXiv:2307.16180。
- en: Park (2023) Daniel Park. 2023. [Open-llm-leaderboard-report](https://github.com/dsdanielpark/Open-LLM-Leaderboard-Report).
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park（2023）Daniel Park。2023年。[Open-llm-leaderboard-report](https://github.com/dsdanielpark/Open-LLM-Leaderboard-Report)。
- en: 'Park et al. (2023) Joon Sung Park, Joseph C. O’Brien, Carrie J. Cai, Meredith Ringel
    Morris, Percy Liang, and Michael S. Bernstein. 2023. [Generative agents: Interactive
    simulacra of human behavior](https://arxiv.org/abs/2304.03442). *Preprint*, arXiv:2304.03442.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）Joon Sung Park、Joseph C. O’Brien、Carrie J. Cai、Meredith Ringel
    Morris、Percy Liang 和 Michael S. Bernstein。2023年。[生成代理：人类行为的互动仿真](https://arxiv.org/abs/2304.03442)。*预印本*，arXiv:2304.03442。
- en: 'Parrish et al. (2022) Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh
    Padmakumar, Jason Phang, Jana Thompson, Phu Mon Htut, and Samuel R. Bowman. 2022.
    [Bbq: A hand-built bias benchmark for question answering](https://arxiv.org/abs/2110.08193).
    *Preprint*, arXiv:2110.08193.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parrish 等人（2022）Alicia Parrish、Angelica Chen、Nikita Nangia、Vishakh Padmakumar、Jason
    Phang、Jana Thompson、Phu Mon Htut 和 Samuel R. Bowman。2022年。[Bbq：一个手工制作的问答偏见基准](https://arxiv.org/abs/2110.08193)。*预印本*，arXiv:2110.08193。
- en: Ramezani and Xu (2023) Aida Ramezani and Yang Xu. 2023. [Knowledge of cultural
    moral norms in large language models](https://arxiv.org/abs/2306.01857). *Preprint*,
    arXiv:2306.01857.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ramezani 和 Xu（2023）Aida Ramezani 和 Yang Xu。2023年。[大型语言模型中的文化道德规范知识](https://arxiv.org/abs/2306.01857)。*预印本*，arXiv:2306.01857。
- en: 'Rousseeuw (1987) Peter J. Rousseeuw. 1987. [Silhouettes: A graphical aid to
    the interpretation and validation of cluster analysis](https://doi.org/10.1016/0377-0427(87)90125-7).
    *Journal of Computational and Applied Mathematics*, 20:53–65.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rousseeuw（1987）Peter J. Rousseeuw。1987年。[轮廓图：集群分析解释和验证的图形辅助工具](https://doi.org/10.1016/0377-0427(87)90125-7)。*计算与应用数学杂志*，20:53–65。
- en: 'Sheng et al. (2021) Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun
    Peng. 2021. [Societal biases in language generation: Progress and challenges](https://arxiv.org/abs/2105.04054).
    *Preprint*, arXiv:2105.04054.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sheng 等人（2021）Emily Sheng、Kai-Wei Chang、Premkumar Natarajan 和 Nanyun Peng。2021年。[语言生成中的社会偏见：进展与挑战](https://arxiv.org/abs/2105.04054)。*预印本*，arXiv:2105.04054。
- en: 'Shu et al. (2023) Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia Dunagan,
    Dallas Card, and David Jurgens. 2023. [You don’t need a personality test to know
    these models are unreliable: Assessing the reliability of large language models
    on psychometric instruments](https://arxiv.org/abs/2311.09718). *Preprint*, arXiv:2311.09718.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu 等人（2023）Bangzhao Shu、Lechen Zhang、Minje Choi、Lavinia Dunagan、Dallas Card
    和 David Jurgens。2023年。[你无需个性测试就能知道这些模型不可靠：评估大型语言模型在心理测量工具上的可靠性](https://arxiv.org/abs/2311.09718)。*预印本*，arXiv:2311.09718。
- en: Taras et al. (2023) Vas Taras, Piers Steel, and Madelynn Stackhouse. 2023. [A
    comparative evaluation of seven instruments for measuring values comprising hofstede’s
    model of culture](https://doi.org/10.1016/j.jwb.2022.101386). *Journal of World
    Business*, 58(1):101386.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taras 等人（2023）Vas Taras、Piers Steel 和 Madelynn Stackhouse。2023年。[霍夫斯特德文化模型的七种测量工具的比较评估](https://doi.org/10.1016/j.jwb.2022.101386)。*国际商业杂志*，58(1):101386。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. [Llama
    2: Open foundation and fine-tuned chat models](https://arxiv.org/abs/2307.09288).
    *Preprint*, arXiv:2307.09288.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing
    Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
    Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov 和 Thomas Scialom. 2023. [Llama
    2：开放基础和微调的聊天模型](https://arxiv.org/abs/2307.09288)。*预印本*，arXiv:2307.09288。
- en: van der Maaten and Hinton (2008) Laurens van der Maaten and Geoffrey Hinton.
    2008. [Visualizing data using t-sne](http://jmlr.org/papers/v9/vandermaaten08a.html).
    *Journal of Machine Learning Research*, 9(86):2579–2605.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van der Maaten and Hinton (2008) Laurens van der Maaten 和 Geoffrey Hinton. 2008.
    [使用 t-sne 可视化数据](http://jmlr.org/papers/v9/vandermaaten08a.html)。*机器学习研究期刊*，9(86)：2579–2605。
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Huggingface’s transformers: State-of-the-art natural
    language processing](https://arxiv.org/abs/1910.03771). *Preprint*, arXiv:1910.03771.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander M. Rush. 2020. [Huggingface的变换器：最先进的自然语言处理](https://arxiv.org/abs/1910.03771)。*预印本*，arXiv:1910.03771。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi
    和 Yejin Choi. 2019. Hellaswag：机器真的能完成你的句子吗？在 *第57届计算语言学协会年会论文集* 中。
- en: Zheng et al. (2024) Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie
    Huang. 2024. [Large language models are not robust multiple choice selectors](https://arxiv.org/abs/2309.03882).
    *Preprint*, arXiv:2309.03882.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng et al. (2024) Chujie Zheng, Hao Zhou, Fandong Meng, Jie Zhou 和 Minlie
    Huang. 2024. [大型语言模型并不是稳健的多项选择选择器](https://arxiv.org/abs/2309.03882)。*预印本*，arXiv:2309.03882。
- en: Appendix A Investigation Pipeline and Prompt Format
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 调查流程和提示格式
- en: '![Refer to caption](img/5d45bfac1136af890cb4a9b77f9aa9bb.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5d45bfac1136af890cb4a9b77f9aa9bb.png)'
- en: 'Figure 3: Pipeline of investigations, exploring cultural values alignment in
    LLMs in three steps. (i) Evaluating cultural values exhibited by an LLM queried
    by a single language but with variants of prompts. (ii) Assessing cultural values
    in the context of different languages. (iii) Examining cultural values exhibited
    by different LLMs, within and across model families and in different model sizes.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：调查流程，分三步探索 LLM 中的文化价值对齐。 (i) 评估由单一语言查询的 LLM 展现的文化价值，但使用不同的提示变体。 (ii) 在不同语言的背景下评估文化价值。
    (iii) 检查不同 LLM 展现的文化价值，包括模型家族内和跨模型家族的不同模型大小。
- en: '![Refer to caption](img/40ea6c1d0c75282a1d7c05b4e9997f15.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/40ea6c1d0c75282a1d7c05b4e9997f15.png)'
- en: 'Figure 4: Prompt samples for the two languages used in the experiment. In both
    samples, the syntax highlighted in red is copied from the original question in
    the questionnaire. During the VSM 2013 testing, there are approximately nine types
    of questions. All customized components are embedded with the respective values
    when querying the model.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 实验中使用的两种语言的提示样本。在两个样本中，用红色高亮的语法是从问卷中的原始问题复制的。在 VSM 2013 测试中，大约有九种问题类型。所有自定义组件在查询模型时都嵌入了相应的值。'
- en: Appendix B VSM Dimension Formula
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B VSM 维度公式
- en: '|  | $\displaystyle PDI$ |  | (6) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle PDI$ |  | (6) |'
- en: '|  | $\displaystyle IDV$ |  | (7) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle IDV$ |  | (7) |'
- en: '|  | $\displaystyle MAS$ |  | (8) |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle MAS$ |  | (8) |'
- en: '|  | $\displaystyle UAI$ |  | (9) |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle UAI$ |  | (9) |'
- en: '|  | $\displaystyle LTO$ |  | (10) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle LTO$ |  | (10) |'
- en: '|  | $\displaystyle IVR$ |  | (11) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle IVR$ |  | (11) |'
- en: Appendix C Nationalities for Experiment
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 实验国籍
- en: The full list of nationalities used in experiments for simulated identities
    includes U.S.A, China, France, Germany, Brazil, India, Singapore, Japan, and South
    Africa.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 进行实验的模拟身份的完整国籍列表包括美国、中国、法国、德国、巴西、印度、新加坡、日本和南非。
- en: Appendix D Human Results of VSM Scores
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D VSM 评分的人类结果
- en: 'Human results, grouped by nations, are presented in Table [3](#A4.T3 "Table
    3 ‣ Appendix D Human Results of VSM Scores ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size").'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 按国家分组的人类结果见表[3](#A4.T3 "表 3 ‣ 附录 D VSM 评分的人类结果 ‣ LLMs 的文化价值差异：提示、语言和模型大小")。
- en: '| Nations | Dimensional Mean |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 国家 | 维度均值 |'
- en: '| PDI | IDV | MAS | UAI | LTO | IVR |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| PDI | IDV | MAS | UAI | LTO | IVR |'
- en: '| U.S.A. | 40 | 91 | 62 | 46 | 26 | 68 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 美国 | 40 | 91 | 62 | 46 | 26 | 68 |'
- en: '| China | 80 | 20 | 66 | 30 | 87 | 24 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 中国 | 80 | 20 | 66 | 30 | 87 | 24 |'
- en: '| France | 68 | 71 | 43 | 86 | 63 | 48 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 法国 | 68 | 71 | 43 | 86 | 63 | 48 |'
- en: '| Germany | 35 | 67 | 66 | 65 | 83 | 40 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 德国 | 35 | 67 | 66 | 65 | 83 | 40 |'
- en: '| Brazil | 69 | 38 | 49 | 76 | 44 | 59 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 巴西 | 69 | 38 | 49 | 76 | 44 | 59 |'
- en: '| India | 77 | 48 | 56 | 40 | 51 | 26 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 印度 | 77 | 48 | 56 | 40 | 51 | 26 |'
- en: '| Singapore | 74 | 20 | 48 | 8 | 72 | 46 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 新加坡 | 74 | 20 | 48 | 8 | 72 | 46 |'
- en: '| Japan | 54 | 46 | 95 | 92 | 88 | 42 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 日本 | 54 | 46 | 95 | 92 | 88 | 42 |'
- en: '| South Africa | 49 | 65 | 63 | 49 | 34 | 63 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 南非 | 49 | 65 | 63 | 49 | 34 | 63 |'
- en: 'Table 3: Human results for the nine nations involved in the experiments.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 九个参与实验的国家的人类结果。'
- en: Appendix E Mean Values of VSM Scores
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E VSM 评分的均值
- en: 'The mean values for each VSM dimension for all experiment sets and human results
    are outlined in Table [4](#A5.T4 "Table 4 ‣ Appendix E Mean Values of VSM Scores
    ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size")'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 所有实验组和人类结果的每个 VSM 维度的均值在表[4](#A5.T4 "表 4 ‣ 附录 E VSM 评分的均值 ‣ LLMs 的文化价值差异：提示、语言和模型大小")中列出。
- en: '| Models | Dimensional Mean |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 维度均值 |'
- en: '| PDI | IDV | MAS | UAI | LTO | IVR |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| PDI | IDV | MAS | UAI | LTO | IVR |'
- en: '| Llama2-7b-chat-hf (Eng) | 18 | 20 | 15 | 13 | -12 | 82 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf (中文) | 18 | 20 | 15 | 13 | -12 | 82 |'
- en: '| Llama2-7b-chat-hf (Eng w. Shuffle) | 22 | 8 | 4 | 17 | -9 | 33 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf (英语，打乱) | 22 | 8 | 4 | 17 | -9 | 33 |'
- en: '| Llama2-7b-chat-hf (Chn) | -18 | 94 | -58 | -52 | 3 | 74 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf (中文) | -18 | 94 | -58 | -52 | 3 | 74 |'
- en: '| Llama2-13b-chat-hf (Eng) | 22 | 45 | -5 | -4 | 20 | 22 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf (英语) | 22 | 45 | -5 | -4 | 20 | 22 |'
- en: '| Llama2-13b-chat-hf (Eng w. Shuffle) | 40 | 29 | -8 | -6 | 24 | 14 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf (英语，打乱) | 40 | 29 | -8 | -6 | 24 | 14 |'
- en: '| Llama2-13b-chat-hf (Chn) | 17 | 1 | -2 | 0 | -3 | 18 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf (中文) | 17 | 1 | -2 | 0 | -3 | 18 |'
- en: '| Llama2-70b-chat-hf (Eng) | -16 | 67 | -33 | -38 | 4 | 49 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf (英语) | -16 | 67 | -33 | -38 | 4 | 49 |'
- en: '| Llama2-70b-chat-hf (Eng w. Shuffle) | -12 | 28 | -4 | -23 | 0 | 40 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf (英语，打乱) | -12 | 28 | -4 | -23 | 0 | 40 |'
- en: '| Llama2-70b-chat-hf (Chn) | -32 | 30 | -39 | -33 | -47 | 57 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf (中文) | -32 | 30 | -39 | -33 | -47 | 57 |'
- en: '| Qwen-14b-chat (Eng) | 28 | 83 | -20 | -17 | -5 | 13 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat (英语) | 28 | 83 | -20 | -17 | -5 | 13 |'
- en: '| Qwen-14b-chat (Eng w. Shuffle) | -11 | 7 | 2 | -1 | 1 | -10 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat (英语，打乱) | -11 | 7 | 2 | -1 | 1 | -10 |'
- en: '| Qwen-14b-chat (Chn) | -7 | 72 | -55 | 1 | -1 | 56 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat (中文) | -7 | 72 | -55 | 1 | -1 | 56 |'
- en: '| Qwen-72b-chat (Eng) | -13 | 74 | -40 | -2 | -26 | 26 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat (英语) | -13 | 74 | -40 | -2 | -26 | 26 |'
- en: '| Qwen-72b-chat (Eng w. Shuffle) | 14 | 47 | -27 | -1 | 2 | 22 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat (英语，打乱) | 14 | 47 | -27 | -1 | 2 | 22 |'
- en: '| Qwen-72b-chat (Chn) | 7 | 11 | -8 | -33 | 12 | 24 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat (中文) | 7 | 11 | -8 | -33 | 12 | 24 |'
- en: '| Mixtral-8x7B (Eng) | -33 | 70 | 34 | -31 | 2 | 47 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B (英语) | -33 | 70 | 34 | -31 | 2 | 47 |'
- en: '| Mixtral-8x7B (Eng w. Shuffle) | 4 | 30 | 9 | -34 | 15 | 44 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B (英语，打乱) | 4 | 30 | 9 | -34 | 15 | 44 |'
- en: '| Mixtral-8x7B (Chn) | -56 | 48 | 0 | 1 | 29 | 38 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B (中文) | -56 | 48 | 0 | 1 | 29 | 38 |'
- en: '| Hofstede’s Research | 61 | 52 | 61 | 55 | 61 | 46 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 霍夫斯泰德研究 | 61 | 52 | 61 | 55 | 61 | 46 |'
- en: 'Table 4: The mean values for each VSM dimension for all experiment sets and
    human results are calculated. These mean values are presented in integer format
    to maintain consistency with the human results listed in Table [3](#A4.T3 "Table
    3 ‣ Appendix D Human Results of VSM Scores ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size").'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：计算了所有实验集和人工结果的每个 VSM 维度的均值。这些均值以整数格式呈现，以保持与表 [3](#A4.T3 "表 3 ‣ 附录 D VSM
    分数的人工结果 ‣ LLM 文化价值差异：提示、语言和模型大小")中列出的人工结果的一致性。
- en: Appendix F Clustering Measurement Methods
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 聚类测量方法
- en: •
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Davies-Bouldin Index (DBI) Davies and Bouldin ([1979](#bib.bib7)): The metric
    quantifies the average similarity between each cluster. In our case, it offers
    an overview of the disparity in models’ cultural values at the experiment set
    level. We calculate the DBI value for each pair of sets. The formula is given
    by:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Davies-Bouldin Index (DBI) Davies 和 Bouldin ([1979](#bib.bib7))：该指标量化了每个集群之间的平均相似性。在我们的案例中，它提供了实验集层面模型文化价值差异的概述。我们计算每对集合的
    DBI 值。公式如下：
- en: '|  | $DBI(e_{i},e_{j})=\left(\frac{S(e_{i})+S(e_{j})}{M(e_{i},e_{j})}\right)$
    |  | (12) |'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $DBI(e_{i},e_{j})=\left(\frac{S(e_{i})+S(e_{j})}{M(e_{i},e_{j})}\right)$
    |  | (12) |'
- en: where $S(e_{i})$. The lower the DBI value, the better the separation between
    the two sets. If the DBI value is larger than one, it suggests that the separation
    between clusters is not very distinct.
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $S(e_{i})$。DBI 值越低，两个集合之间的分离越好。如果 DBI 值大于 1，则表明集群之间的分离不是很明显。
- en: •
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Silhouette Score ($SS$ is given by:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Silhouette Score（$SS$ 由以下公式给出：
- en: '|  | $\displaystyle a(p_{i})$ |  | (13) |'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle a(p_{i})$ |  | (13) |'
- en: '|  | $\displaystyle b(p_{i})$ |  | (14) |'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle b(p_{i})$ |  | (14) |'
- en: '|  | $\displaystyle SS$ |  | (15) |'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\displaystyle SS$ |  | (15) |'
- en: where $a(p_{i})$. Our study computes the average score across all points from
    two sets to determine the disparity score between them. The silhouette score ranges
    from -1 to 1, where a higher value indicates more effective separation between
    clusters.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $a(p_{i})$。我们的研究计算两个集合中所有点的平均分，以确定它们之间的差异分数。轮廓系数的范围从 -1 到 1，值越高表示集群之间的分离越有效。
- en: '| Models | Identity Context |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 身份上下文 |'
- en: '| Nation | Age | Gender |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 国家 | 年龄 | 性别 |'
- en: '| PCC ($\rho$) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| PCC ($\rho$) |'
- en: '| Llama2-7b-chat-hf (Eng) | 0.969 | 0.987 | 0.925 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf (英文) | 0.969 | 0.987 | 0.925 |'
- en: '| Llama2-7b-chat-hf (Eng w. Shuffle) | 0.942 | 0.994 | 0.949 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf (英文带洗牌) | 0.942 | 0.994 | 0.949 |'
- en: '| Llama2-7b-chat-hf (Chn) | 0.842 | 0.971 | 0.969 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf (中文) | 0.842 | 0.971 | 0.969 |'
- en: '| Llama2-13b-chat-hf (Eng) | 0.978 | 0.993 | 0.996 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf (英文) | 0.978 | 0.993 | 0.996 |'
- en: '| Llama2-13b-chat-hf (Eng w. Shuffle) | 0.969 | 0.993 | 0.987 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf (英文带洗牌) | 0.969 | 0.993 | 0.987 |'
- en: '| Llama2-13b-chat-hf (Chn) | 0.993 | 0.997 | 0.998 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf (中文) | 0.993 | 0.997 | 0.998 |'
- en: '| Llama2-70b-chat-hf (Eng) | 0.991 | 1.000 | 0.995 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf (英文) | 0.991 | 1.000 | 0.995 |'
- en: '| Llama2-70b-chat-hf (Eng w. Shuffle) | 0.987 | 0.999 | 0.996 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf (英文带洗牌) | 0.987 | 0.999 | 0.996 |'
- en: '| Llama2-70b-chat-hf (Chn) | 0.969 | 0.996 | 0.995 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf (中文) | 0.969 | 0.996 | 0.995 |'
- en: '| Qwen-14b-chat (Eng) | 0.934 | 0.992 | 0.995 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat (英文) | 0.934 | 0.992 | 0.995 |'
- en: '| Qwen-14b-chat (Eng w. Shuffle) | 0.752 | 0.905 | 0.837 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat (英文带洗牌) | 0.752 | 0.905 | 0.837 |'
- en: '| Qwen-14b-chat (Chn) | 0.807 | 0.939 | 0.858 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat (中文) | 0.807 | 0.939 | 0.858 |'
- en: '| Qwen-72b-chat (Eng) | 0.934 | 0.992 | 0.995 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat (英文) | 0.934 | 0.992 | 0.995 |'
- en: '| Qwen-72b-chat (Eng w. Shuffle) | 0.943 | 0.986 | 0.994 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat (英文带洗牌) | 0.943 | 0.986 | 0.994 |'
- en: '| Qwen-72b-chat (Chn) | 0.915 | 0.988 | 0.987 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat (中文) | 0.915 | 0.988 | 0.987 |'
- en: '| Mixtral-8x7B (Eng) | 0.992 | 0.997 | 0.998 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B (英文) | 0.992 | 0.997 | 0.998 |'
- en: '| Mixtral-8x7B (Eng w. Shuffle) | 0.995 | 0.999 | 0.998 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B (英文带洗牌) | 0.995 | 0.999 | 0.998 |'
- en: '| Mixtral-8x7B (Chn) | 0.947 | 0.989 | 0.953 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B (中文) | 0.947 | 0.989 | 0.953 |'
- en: 'Table 5: The Pearson Correlation Coefficient $\rho$. Correlation coefficients
    below 0.9 are in boldface.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：皮尔逊相关系数 $\rho$。相关系数低于 0.9 的用粗体表示。
- en: '| Models | Dimensional Standard Deviation | Distance | MCD |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 维度标准差 | 距离 | MCD |'
- en: '| PDI | IDV | MAS | UAI | LTO | IVR |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| PDI | IDV | MAS | UAI | LTO | IVR |'
- en: '| Llama2-7b-chat-hf (Eng) | 7.587 | 4.648 | 6.960 | 7.014 | 11.402 | 14.096
    | 8.618 | 0.424 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf (英文) | 7.587 | 4.648 | 6.960 | 7.014 | 11.402 | 14.096
    | 8.618 | 0.424 |'
- en: '| Llama2-7b-chat-hf (Eng w. Shuffle) | 6.831 | 7.047 | 3.903 | 6.549 | 7.712
    | 9.916 | 6.993 | 0.344 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf (英文带洗牌) | 6.831 | 7.047 | 3.903 | 6.549 | 7.712 | 9.916
    | 6.993 | 0.344 |'
- en: '| Llama2-7b-chat-hf (Chn) | 13.629 | 21.835 | 14.901 | 5.429 | 14.681 | 10.993
    | 13.578 | 0.668 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf（中文） | 13.629 | 21.835 | 14.901 | 5.429 | 14.681 | 10.993
    | 13.578 | 0.668 |'
- en: '| Llama2-13b-chat-hf (Eng) | 5.833 | 5.952 | 3.608 | 2.850 | 4.187 | 3.004
    | 4.239 | 0.209 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf（英语） | 5.833 | 5.952 | 3.608 | 2.850 | 4.187 | 3.004 |
    4.239 | 0.209 |'
- en: '| Llama2-13b-chat-hf (Eng w. Shuffle) | 3.109 | 4.301 | 3.134 | 2.530 | 6.586
    | 4.148 | 3.888 | 0.191 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf（英语，带洗牌） | 3.109 | 4.301 | 3.134 | 2.530 | 6.586 | 4.148
    | 3.888 | 0.191 |'
- en: '| Llama2-13b-chat-hf (Chn) | 4.131 | 2.758 | 3.394 | 0.919 | 3.153 | 4.085
    | 3.074 | 0.151 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf（中文） | 4.131 | 2.758 | 3.394 | 0.919 | 3.153 | 4.085 |
    3.074 | 0.151 |'
- en: '| Llama2-70b-chat-hf (Eng) | 4.160 | 1.096 | 2.789 | 4.866 | 3.197 | 5.113
    | 3.537 | 0.174 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf（英语） | 4.160 | 1.096 | 2.789 | 4.866 | 3.197 | 5.113 |
    3.537 | 0.174 |'
- en: '| Llama2-70b-chat-hf (Eng w. Shuffle) | 2.746 | 2.844 | 2.829 | 1.680 | 9.016
    | 5.674 | 4.132 | 0.203 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf（英语，带洗牌） | 2.746 | 2.844 | 2.829 | 1.680 | 9.016 | 5.674
    | 4.132 | 0.203 |'
- en: '| Llama2-70b-chat-hf (Chn) | 8.183 | 3.965 | 9.947 | 3.942 | 16.616 | 7.673
    | 8.388 | 0.413 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf（中文） | 8.183 | 3.965 | 9.947 | 3.942 | 16.616 | 7.673 |
    8.388 | 0.413 |'
- en: '| Qwen-14b-chat (Eng) | 7.376 | 6.512 | 6.596 | 5.405 | 4.026 | 6.388 | 6.051
    | 0.298 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat（英语） | 7.376 | 6.512 | 6.596 | 5.405 | 4.026 | 6.388 | 6.051
    | 0.298 |'
- en: '| Qwen-14b-chat (Eng w. Shuffle) | 5.965 | 9.709 | 5.916 | 3.485 | 16.145 |
    5.451 | 7.778 | 0.383 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat（英语，带洗牌） | 5.965 | 9.709 | 5.916 | 3.485 | 16.145 | 5.451 |
    7.778 | 0.383 |'
- en: '| Qwen-14b-chat (Chn) | 10.607 | 22.082 | 13.947 | 6.285 | 11.354 | 7.974 |
    12.042 | 0.592 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat（中文） | 10.607 | 22.082 | 13.947 | 6.285 | 11.354 | 7.974 | 12.042
    | 0.592 |'
- en: '| Qwen-72b-chat (Eng) | 3.947 | 4.767 | 3.660 | 1.952 | 13.470 | 6.036 | 5.638
    | 0.277 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat（英语） | 3.947 | 4.767 | 3.660 | 1.952 | 13.470 | 6.036 | 5.638
    | 0.277 |'
- en: '| Qwen-72b-chat (Eng w. Shuffle) | 4.250 | 4.854 | 3.767 | 3.409 | 6.386 |
    4.267 | 4.489 | 0.221 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat（英语，带洗牌） | 4.250 | 4.854 | 3.767 | 3.409 | 6.386 | 4.267 | 4.489
    | 0.221 |'
- en: '| Qwen-72b-chat (Chn) | 14.556 | 3.968 | 2.458 | 9.098 | 12.066 | 9.795 | 8.657
    | 0.426 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat（中文） | 14.556 | 3.968 | 2.458 | 9.098 | 12.066 | 9.795 | 8.657
    | 0.426 |'
- en: '| Mixtral-8x7B (Eng) | 7.078 | 0.591 | 0.583 | 7.785 | 7.947 | 10.799 | 5.797
    | 0.285 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B（英语） | 7.078 | 0.591 | 0.583 | 7.785 | 7.947 | 10.799 | 5.797
    | 0.285 |'
- en: '| Mixtral-8x7B (Eng w. Shuffle) | 2.983 | 1.650 | 5.904 | 1.693 | 3.251 | 3.465
    | 3.158 | 0.155 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B（英语，带洗牌） | 2.983 | 1.650 | 5.904 | 1.693 | 3.251 | 3.465 | 3.158
    | 0.155 |'
- en: '| Mixtral-8x7B (Chn) | 7.319 | 5.035 | 0.412 | 1.523 | 5.332 | 11.495 | 5.186
    | 0.255 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B（中文） | 7.319 | 5.035 | 0.412 | 1.523 | 5.332 | 11.495 | 5.186
    | 0.255 |'
- en: '| Human Results | 16.613 | 23.904 | 15.301 | 27.491 | 23.337 | 15.336 | 20.330
    | 1.0 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 人类结果 | 16.613 | 23.904 | 15.301 | 27.491 | 23.337 | 15.336 | 20.330 | 1.0
    |'
- en: 'Table 6: The standard deviation for each VSM dimension is calculated across
    nations. For the models, these deviations are derived from responses grouped by
    simulated nations, while for human results, they are based on Hofstede’s research
    findings. Distances and MCDs are calculated as outlined in [3.3](#S3.SS3.SSS0.Px1
    "Intra-set Disparity Measurement ‣ 3.3 Measures by VSM Scores ‣ 3 Measures by
    VSM ‣ Cultural Value Differences of LLMs: Prompt, Language, and Model Size").
    The highest MCD among models is emphasized in bold, indicating that a larger MCD
    suggests a greater influence of simulated nations on the models’ expression of
    cultural values.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：每个 VSM 维度的标准差是根据各国计算的。对于模型，这些偏差来自模拟国家分组的响应，而对于人类结果，它们基于 Hofstede 的研究结果。距离和
    MCD 按照 [3.3](#S3.SS3.SSS0.Px1 "内部集差异测量 ‣ 3.3 按 VSM 得分测量 ‣ 3 测量 VSM ‣ LLM 的文化价值差异：提示、语言和模型大小")
    进行计算。模型中最高的 MCD 以粗体字突出显示，表示较大的 MCD 表示模拟国家对模型文化价值表达的影响更大。
- en: Appendix G Experiments Results for RQ1
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 实验结果（RQ1）
- en: G.1 Variant Context
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.1 变体上下文
- en: •
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Results based on the raw scores of 24 questions are listed in Table [5](#A6.T5
    "Table 5 ‣ Appendix F Clustering Measurement Methods ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size").'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 24 道问题的原始得分的结果列在表 [5](#A6.T5 "表 5 ‣ 附录 F 聚类测量方法 ‣ LLM 的文化价值差异：提示、语言和模型大小")
    中。
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Results for intra-set comparison based on VSM scores are listed in Table [6](#A6.T6
    "Table 6 ‣ Appendix F Clustering Measurement Methods ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size"). The largest MCD among all experiment
    sets is less than 0.7, and only two out of eighteen groups have scores greater
    than 0.5.'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 VSM 得分的内部集比较结果列在表 [6](#A6.T6 "表 6 ‣ 附录 F 聚类测量方法 ‣ LLM 的文化价值差异：提示、语言和模型大小")
    中。所有实验集中的最大 MCD 小于 0.7，并且只有 18 个组中的两个组得分超过 0.5。
- en: G.2 Shuffled Options
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: G.2 洗牌选项
- en: •
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Results based on the raw scores of 24 questions for each pair of experiment
    sets are listed in Table [7](#A7.T7 "Table 7 ‣ G.2 Shuffled Options ‣ Appendix
    G Experiments Results for RQ1 ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size").'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于每对实验集的24个问题的原始分数的结果列在表格[7](#A7.T7 "表 7 ‣ G.2 随机选项 ‣ 附录 G 实验结果 ‣ RQ1 ‣ LLMs的文化价值差异：提示、语言和模型大小")中。
- en: •
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Results for inter-set comparison based on VSM scores for each pair of experiment
    sets are listed in Table [1](#S5.T1 "Table 1 ‣ Simulated Identity ‣ 5.1 Prompt
    Variants (RQ1) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size"). As shown in the table, the smallest Davies-Bouldin
    Index (DBI) value among all models exceeds 0.5, with values closer to 0 indicating
    better clustering quality. Additionally, the highest Silhouette Score (SS) is
    below 0.7, where values closer to 1 signify more effective clustering. These statistics
    again underscore that the change in context within prompts does not significantly
    alter the cultural values in models’ responses.'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于VSM分数的组间比较结果列在表格[1](#S5.T1 "表 1 ‣ 模拟身份 ‣ 5.1 提示变体 (RQ1) ‣ 5 实验结果 ‣ LLMs的文化价值差异：提示、语言和模型大小")中。如表中所示，所有模型中最小的Davies-Bouldin指数（DBI）值超过0.5，接近0的值表示更好的聚类质量。此外，最高的Silhouette分数（SS）低于0.7，接近1的值表示更有效的聚类。这些统计数据再次强调了提示中的上下文变化不会显著改变模型响应中的文化价值。
- en: 'From the perspective of $SS_{h}$ value exceeds one, indicating a greater disparity
    than human results. This model also has the lowest Pearson correlation coefficient
    between the two sets as shown in Table [7](#A7.T7 "Table 7 ‣ G.2 Shuffled Options
    ‣ Appendix G Experiments Results for RQ1 ‣ Cultural Value Differences of LLMs:
    Prompt, Language, and Model Size").'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从$SS_{h}$值超过1的角度来看，表明与人工结果相比存在更大的差异。该模型在两个数据集之间的Pearson相关系数最低，如表格[7](#A7.T7
    "表 7 ‣ G.2 随机选项 ‣ 附录 G 实验结果 ‣ RQ1 ‣ LLMs的文化价值差异：提示、语言和模型大小")所示。
- en: '| Models | PCC ($\rho$) | P-value |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | PCC ($\rho$) | P值 |'
- en: '| --- | --- | --- |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Llama2-7b-chat-hf | 0.894 | $\ll 0.05$ |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf | 0.894 | $\ll 0.05$ |'
- en: '| Llama2-13b-chat-hf | 0.861 | $\ll 0.05$ |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf | 0.861 | $\ll 0.05$ |'
- en: '| Llama2-70b-chat-hf | 0.938 | $\ll 0.05$ |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf | 0.938 | $\ll 0.05$ |'
- en: '| Qwen-14b-chat | 0.718 | $\ll 0.05$ |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat | 0.718 | $\ll 0.05$ |'
- en: '| Qwen-72b-chat | 0.922 | $\ll 0.05$ |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat | 0.922 | $\ll 0.05$ |'
- en: '| Mixtral-8x7B | 0.876 | $\ll 0.05$ |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 0.876 | $\ll 0.05$ |'
- en: 'Table 7: The table presents Pearson correlation coefficients ($\rho$) and p-values
    comparing centroids of models’ responses between “w. Shuffle" and “w/o Shuffle"
    options (all prompts are in English), assessing the consistency of responses from
    the aspect of the original scores.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：该表展示了Pearson相关系数（$\rho$）和p值，用于比较模型响应的质心在“w. Shuffle”和“w/o Shuffle”选项之间（所有提示均为英文），评估响应一致性的原始分数方面。
- en: Appendix H Experiment Results for RQ2
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 实验结果用于 RQ2
- en: •
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Results based on the raw scores of 24 questions are listed in Table [8](#A8.T8
    "Table 8 ‣ Appendix H Experiment Results for RQ2 ‣ Cultural Value Differences
    of LLMs: Prompt, Language, and Model Size").'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于24个问题的原始分数的结果列在表格[8](#A8.T8 "表 8 ‣ 附录 H 实验结果用于 RQ2 ‣ LLMs的文化价值差异：提示、语言和模型大小")中。
- en: •
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Results for intra-set comparison based on VSM scores are listed in Table [2](#S5.T2
    "Table 2 ‣ 5.2 Language Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value
    Differences of LLMs: Prompt, Language, and Model Size"). From the $DBI$ is 0.07
    higher. Nevertheless, using standard clustering metrics, we find no significant
    differences between the results in Table [2](#S5.T2 "Table 2 ‣ 5.2 Language Variants
    (RQ2) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size") and Table [1](#S5.T1 "Table 1 ‣ Simulated Identity ‣ 5.1 Prompt
    Variants (RQ1) ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt,
    Language, and Model Size").'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于VSM分数的组内比较结果列在表格[2](#S5.T2 "表 2 ‣ 5.2 语言变体 (RQ2) ‣ 5 实验结果 ‣ LLMs的文化价值差异：提示、语言和模型大小")中。$DBI$值高出0.07。然而，使用标准聚类指标，我们发现表格[2](#S5.T2
    "表 2 ‣ 5.2 语言变体 (RQ2) ‣ 5 实验结果 ‣ LLMs的文化价值差异：提示、语言和模型大小")和表格[1](#S5.T1 "表 1 ‣
    模拟身份 ‣ 5.1 提示变体 (RQ1) ‣ 5 实验结果 ‣ LLMs的文化价值差异：提示、语言和模型大小")中的结果没有显著差异。
- en: 'However, the results of $SS_{h}$ value for language comparison is 42.7% higher
    than that for "shuffling". The observations suggest that language differences
    can more readily "induce" the model to select a different option than selection
    bias. Consequently, this results in a more distinctive separation in the expression
    of cultural values by the same model. The t-SNE figures in Figure [1](#S5.F1 "Figure
    1 ‣ Shuffled Options ‣ 5.1 Prompt Variants (RQ1) ‣ 5 Experiment Results ‣ Cultural
    Value Differences of LLMs: Prompt, Language, and Model Size") also illustrate
    the differences in intra-set disparity, clearly showing that most models express
    cultural values more variably when queried in different languages.'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，语言比较的 $SS_{h}$ 值比“洗牌”时的结果高出 42.7%。观察结果表明，语言差异可以更容易地“诱导”模型选择不同的选项，而不是选择偏差。因此，这导致了相同模型在文化价值表达上的更显著分离。图
    [1](#S5.F1 "图 1 ‣ 洗牌选项 ‣ 5.1 提示变体 (RQ1) ‣ 5 实验结果 ‣ LLMs 的文化价值差异：提示、语言和模型大小") 中的
    t-SNE 图也展示了内部集差异的不同，清楚地显示出大多数模型在不同语言的查询下更为多样地表达文化价值。
- en: '| Models | PCC($\rho$) | P-value |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | PCC($\rho$) | P值 |'
- en: '| Llama2-7b-chat-hf | 0.315 | 0.134 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-7b-chat-hf | 0.315 | 0.134 |'
- en: '| Llama2-13b-chat-hf | 0.704 | $\ll 0.05$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-13b-chat-hf | 0.704 | $\ll 0.05$ |'
- en: '| Llama2-70b-chat-hf | 0.841 | $\ll 0.05$ |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| Llama2-70b-chat-hf | 0.841 | $\ll 0.05$ |'
- en: '| Qwen-14b-chat | 0.531 | 0.008 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-14b-chat | 0.531 | 0.008 |'
- en: '| Qwen-72b-chat | 0.643 | $\ll 0.05$ |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| Qwen-72b-chat | 0.643 | $\ll 0.05$ |'
- en: '| Mixtral-8x7B | 0.535 | 0.007 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | 0.535 | 0.007 |'
- en: 'Table 8: The table presents Pearson correlation coefficients ($\rho$) and p-values
    comparing centroids of models’ responses between “English" and “Chinese" prompts,
    assessing the consistency of responses from the aspect of the original scores.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 表格展示了 Pearson 相关系数 ($\rho$) 和 p 值，用于比较“英语”和“中文”提示下模型响应的质心，从原始分数的角度评估响应的一致性。'
- en: Appendix I Experiment Results for RQ3
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I RQ3 实验结果
- en: •
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Heatmap (a) in Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language Variants (RQ2) ‣
    5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language, and
    Model Size") shows that the 13b and 70b models from the Llama2 family are closest
    to the 14b and 72b models from the Qwen family. Similarly, the Qwen-14b-chat model
    has the smallest $SS_{h}$ values to the model outside their own family.'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 [2](#S5.F2 "图 2 ‣ 5.2 语言变体 (RQ2) ‣ 5 实验结果 ‣ LLMs 的文化价值差异：提示、语言和模型大小") 中的热图
    (a) 显示，来自 Llama2 系列的 13b 和 70b 模型与来自 Qwen 系列的 14b 和 72b 模型最为接近。类似地，Qwen-14b-chat
    模型的 $SS_{h}$ 值对其他模型的距离最小。
- en: •
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'In heatmap (d) of Figure [2](#S5.F2 "Figure 2 ‣ 5.2 Language Variants (RQ2)
    ‣ 5 Experiment Results ‣ Cultural Value Differences of LLMs: Prompt, Language,
    and Model Size"), we present the $SS_{h}$ values in the heatmap (d) of Figure [2](#S5.F2
    "Figure 2 ‣ 5.2 Language Variants (RQ2) ‣ 5 Experiment Results ‣ Cultural Value
    Differences of LLMs: Prompt, Language, and Model Size") is notably sparse, with
    38.9% of values exceeding 1.0 and 10.5% falling below 0.5\. However, all values
    below 0.5 correspond to comparisons between one model and others tested in a different
    language. This suggests that the dimensional space utilized in the VSM testing
    might be too constrained, causing overlap in results from various experiment sets.
    Despite the overlap, the pronounced inter-set disparities observed in cross-language
    comparisons suggest that variations in language can lead to more significant differences
    in cultural values among models.'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在图 [2](#S5.F2 "图 2 ‣ 5.2 语言变体 (RQ2) ‣ 5 实验结果 ‣ LLMs 的文化价值差异：提示、语言和模型大小") 中的热图
    (d) 中，我们展示了 $SS_{h}$ 值。热图 (d) 中的 $SS_{h}$ 值显著稀疏，有 38.9% 的值超过 1.0，10.5% 的值低于 0.5。然而，所有低于
    0.5 的值都对应于一个模型与在不同语言中测试的其他模型之间的比较。这表明，VSM 测试中使用的维度空间可能过于狭窄，导致各种实验集结果的重叠。尽管存在重叠，但在跨语言比较中观察到的明显集间差异表明，语言变化可能导致模型之间的文化价值差异更为显著。
- en: Appendix J VSM Questionnaire
  id: totrans-333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 J VSM 问卷
- en: '![Refer to caption](img/014caf904b2bff5451e6331d6bc71afc.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/014caf904b2bff5451e6331d6bc71afc.png)'
- en: 'Figure 5: VSM Questionnaire Page 1'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: VSM 问卷 第 1 页'
- en: '![Refer to caption](img/72453d20a237b5575dbf41b496d782c8.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/72453d20a237b5575dbf41b496d782c8.png)'
- en: 'Figure 6: VSM Questionnaire Page 2'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: VSM 问卷 第 2 页'
- en: '![Refer to caption](img/2ba2e52736c05d654e0327c3d786cb18.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2ba2e52736c05d654e0327c3d786cb18.png)'
- en: 'Figure 7: VSM Questionnaire Page 3'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: VSM 问卷 第 3 页'
