- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:44:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction
    Tuning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.12353](https://ar5iv.labs.arxiv.org/html/2404.12353)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hang Hua^*, Yunlong Tang^*, Chenliang Xu, Jiebo Luo^†
  prefs: []
  type: TYPE_NORMAL
- en: University of Rochester
  prefs: []
  type: TYPE_NORMAL
- en: '{hhua2,jluo}@cs.rochester.edu'
  prefs: []
  type: TYPE_NORMAL
- en: '{yunlong.tang,chenliang.xu}@rochester.edu'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://hanghuacs.github.io/v2xum/](https://hanghuacs.github.io/v2xum/)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Video summarization aims to create short, accurate, and cohesive summaries
    of longer videos. Despite the existence of various video summarization datasets,
    a notable limitation is their limited amount of source videos, which hampers the
    effective fine-tuning of advanced large vision-language models (VLMs). Additionally,
    most existing datasets are created for video-to-video summarization, overlooking
    the contemporary need for multimodal video content summarization. Recent efforts
    have been made to expand from unimodal to multimodal video summarization, categorizing
    the task into three sub-tasks based on the summary’s modality: video-to-video
    (V2V), video-to-text (V2T), and a combination of video and text summarization
    (V2VT). However, the textual summaries in previous multimodal datasets are inadequate.
    To address these issues, we introduce Instruct-V2Xum, a cross-modal video summarization
    dataset featuring 30,000 diverse videos sourced from YouTube, with lengths ranging
    from 40 to 940 seconds and an average summarization ratio of 16.39%. Each video
    summary in Instruct-V2Xum is paired with a textual summary that references specific
    frame indexes, facilitating the generation of aligned video and textual summaries.
    In addition, we propose a new video summarization framework named V2Xum-LLM. V2Xum-LLM,
    specifically V2Xum-LLaMA in this study, is the first framework that unifies different
    video summarization tasks into one large language model’s (LLM) text decoder and
    achieves task-controllable video summarization with temporal prompts and task
    instructions. Experiments show that V2Xum-LLaMA outperforms strong baseline models
    on multiple video summarization tasks. Furthermore, we propose an enhanced evaluation
    metric for V2V and V2VT summarization tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹footnotetext: Equal Contribution.²²footnotetext: Corresponding author.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0851c46a48d58d921d191743f287abfa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of cross-modal video summarization.'
  prefs: []
  type: TYPE_NORMAL
- en: The interest in sharing life experiences has surged in recent years, making
    video the most informative and diverse visual medium on social media platforms.
    This trend has led to significant demands for a variety of video and language
    understanding tasks, such as video captioning [[79](#bib.bib79)], video question
    answering [[82](#bib.bib82), [78](#bib.bib78)], moment retrieval [[32](#bib.bib32)],
    and video summarization [[14](#bib.bib14), [61](#bib.bib61)]. Video summarization
    (V2V) provides an efficient way for humans to obtain key information from a long
    video. This process entails selecting the most significant information from a
    video and condensing it into a shorter form, all while maintaining the essence
    of the original content. Beyond extensive research in V2V summarization, there
    are a few recent explorations of V2T and V2VT summarization. Most notably, VideoXum
    [[37](#bib.bib37)] seeks to broaden the modality of video summaries to include
    text summaries. It utilizes the dense captions from ActivityNetCap [[31](#bib.bib31)]
    videos as text summaries and annotates the corresponding video segments as video
    summaries. Other datasets for video summarization include TVSum [[61](#bib.bib61)],
    SumMe [[14](#bib.bib14)], QFVS [[57](#bib.bib57)], MED Summaries [[53](#bib.bib53)],
    and so on. While it is expected that powerful LLMs can help improve video summarization,
    the insufficient number of source videos may not be able to support the robust
    fine-tuning of LLMs to perform this task since finetuning a large model with a
    limited number of training examples is prone to overfitting [[20](#bib.bib20),
    [21](#bib.bib21)]. In addition, the data compiled in VideoXum cannot be considered
    genuine summaries because the dense captions of ActivityNetCap contain a significant
    amount of redundant information. More recently, Shot2Story20K [[15](#bib.bib15)]
    collects 20k video-text data that enables the robust finetuning of LLMs, but it
    only supports video-to-text summarization. To address these issues, we propose
    Instruct-V2Xum, a new large-scale cross-model video summarization dataset that
    contains 30k open domain videos, partitioned as 25,000 in the training set, 1,000
    in the validation set, and 4,000 in the test set. In Instruct-V2Xum, we obtain
    the source videos from YouTube using the video list provided by InternVid [[74](#bib.bib74)].
    The methodology of video summarization parallels that of extractive text summarization,
    where the objective is to isolate the pivotal frames or sentences from the source
    videos or documents, respectively. Extractive text summarization is a foundational
    task in the NLP field, with numerous LLMs setting the benchmark in this domain
    [[85](#bib.bib85), [26](#bib.bib26), [17](#bib.bib17)]. Inspired by this, we first
    extract frames from the source videos and employ LLaVA-1.5-7B [[39](#bib.bib39)]
    to generate detailed captions for each frame. Then we take all the frame captions
    as a document to perform extractive document summarization using GPT-4 [[1](#bib.bib1)].
    This approach enables us to obtain both video summaries and their corresponding
    textual summaries. Finally, the extracted text summaries are further refined into
    a grammar-fluent short paragraph by GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Numerous visual instruction tuning-based methods have been proposed for video-language
    understanding [[22](#bib.bib22), [70](#bib.bib70), [36](#bib.bib36)]. These models
    can process the long videos for general video-language understanding and reasoning
    tasks such as video question answering, video captioning, and so on. Recently,
    there have been some attempts for fine-grained video moments understanding [[22](#bib.bib22)]
    or video-language temporal grounding [[38](#bib.bib38)] using large VLMs. However,
    these approaches, which typically process video frames as sequential images for
    a frozen visual encoder and train an LLM decoder for identifying video moment
    boundaries, are not well-suited for dense temporal prediction in video summarization
    tasks, particularly in the V2VT tasks. Furthermore, most existing models require
    large-scale data to train new parameters added to pre-trained VLMs [[38](#bib.bib38),
    [36](#bib.bib36), [44](#bib.bib44)]. This requirement significantly limits their
    practicality in scenarios where only a limited number of training examples are
    available. To address these problems, we design a new temporal prompt instruction
    tuning framework – V2Xum-LLaMA. In V2Xum-LLaMA, we unify different modalities
    of video summary generation into one LLM decoder. This framework stands out by
    removing the dependence on task-specific layers that were required for video summarization
    in earlier VLM-based approaches. The main advantages of this method are that it
    enables the effective adaptation of the learned knowledge and the powerful capabilities
    of the pretrained language models to dense video temporal and content understanding.
    All the pretrained parameters of the VLMs are reused, and the model takes interleaved
    video frames and natural language temporal prompts as input to facilitate end-to-end
    model training. As video temporal prediction is performed by using language models,
    there is a challenge for calculating the correlation for V2V evaluation, since
    the language model-predicted video summaries are the discrete frame indexes. To
    overcome this challenge, we propose a solution in section [5](#S5 "5 Experiments
    ‣ V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction
    Tuning") to calculate the scores for language model-predicted video summaries.
    Furthermore, we also provide an analysis of the existing video summarization tasks
    and propose $F_{CLIP}$, the enhanced evaluation metrics for V2V and V2VT summarization
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our main contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose a novel and flexible cross-modal video summarization framework –
    V2Xum-LLaMA, where we unify different video summarization tasks into one pre-trained
    language decoder without introducing the randomly initialized task-specific heads.
    The proposed framework takes interleaved video frames and natural language temporal
    prompts as input, which facilitates the pre-trained VLMs to process long sequences
    in an end-to-end manner. Our proposed method outperforms all the selected strong
    baseline models on existing mainstream V2V, V2T, and V2VT benchmarks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address the challenge of insufficient video-language data for effectively
    fine-tuning large VLMs for various video summarization tasks, we build a new instruction
    following dataset named Instruct-V2Xum for cross-model video summarization. This
    dataset comprises 30k diverse videos sourced from YouTube, with durations spanning
    from 40 to 940 seconds. The proposed dataset enables VLMs to generate modality-controllable
    video summaries with the task prompt. The experiments validate the rationality
    of our proposed dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a comprehensive analysis of various existing video summarization
    tasks from the perspectives of data, method, and evaluation. Building on this
    analysis, we propose $F_{CLIP}$, an enhanced evaluation metric for V2V and V2VT
    summarization tasks. Experimental results indicate that our proposed evaluation
    metrics are highly consistent with the traditional evaluation metrics including
    F1, Spearman correlation, and Kendall correlation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Video Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Video summarization is the process of distilling salient information from video
    content by analyzing its structure and temporal redundancies. Traditional video
    summarization, also known as video-to-video summarization, typically generates
    a condensed version of the original video, comprising selected frames [[41](#bib.bib41),
    [13](#bib.bib13), [71](#bib.bib71), [8](#bib.bib8)], shots [[25](#bib.bib25),
    [9](#bib.bib9), [88](#bib.bib88)], or segments [[66](#bib.bib66), [30](#bib.bib30)].
    These models are commonly trained using supervised learning approaches, with reinforcement
    learning methods like policy gradient [[77](#bib.bib77)], optimizing for diversity
    and representativeness in the summarized output [[91](#bib.bib91)], gaining popularity.
    The standard datasets for video summarization tasks include SumMe [[14](#bib.bib14)]
    and TVSum [[61](#bib.bib61)], which are widely used for benchmarking purposes.
    In recent years, cross-modal video summarization [[11](#bib.bib11), [16](#bib.bib16),
    [24](#bib.bib24)] has emerged as an area of interest, incorporating additional
    modalities such as audio, speech, subtitles, and captions. This approach leverages
    multimodal models to create more comprehensive summaries. Video-to-text summarization [[50](#bib.bib50),
    [4](#bib.bib4)] is an evolving field that aims to generate descriptive paragraphs
    in natural language that encapsulate video content. VideoXum [[37](#bib.bib37)]
    makes progress in this direction by repurposing the ActivityNetCap [[31](#bib.bib31)]
    dataset and employing the vision-language model BLIP-2 [[33](#bib.bib33)] for
    both video-to-video and video-to-text summarization. However, it is important
    to note that the data collected in [[37](#bib.bib37)] may not constitute genuine
    summaries, as the dense captions from the ActivityNetCap dataset often contain
    significant redundant information.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Video Temporal Understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Beyond video summarization, there has been an increasing interest in temporal
    understanding tasks for video analysis. Temporal localization plays a pivotal
    role in comprehending long-form or untrimmed videos by associating specific segments
    with their corresponding semantics. Research has primarily concentrated on tasks
    such as temporal video grounding [[42](#bib.bib42), [75](#bib.bib75)], dense video
    captioning [[72](#bib.bib72), [81](#bib.bib81)], video highlight detection [[32](#bib.bib32),
    [28](#bib.bib28)], step localization [[58](#bib.bib58)], and procedure planning [[2](#bib.bib2)].
    Despite significant progress, models designed for action temporal localization
    and highlight detection often rely on predefined labels [[83](#bib.bib83)], which
    hampers a comprehensive understanding. To address these limitations, research
    has focused on generic event boundary detection [[59](#bib.bib59), [73](#bib.bib73),
    [67](#bib.bib67)] and dense video captioning [[31](#bib.bib31)], utilizing datasets
    that provide event descriptions [[37](#bib.bib37)]. Nevertheless, Most existing
    models still rely on regression for temporal predictions, necessitating separate
    heads for captioning and regression tasks [[72](#bib.bib72), [86](#bib.bib86),
    [66](#bib.bib66)]. Recent advancements in Large Language Models (LLMs) have introduced
    a paradigm shift by enabling the direct use of natural language to specify temporal
    locations in videos, thus offering a more intuitive approach to video understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, Large Language Models (LLMs) have witnessed rapid advancements [[1](#bib.bib1),
    [68](#bib.bib68), [69](#bib.bib69), [89](#bib.bib89)]. With pretraining on extensive
    corpora from the Internet, LLMs acquire substantial knowledge, enabling powerful
    zero-shot and in-context learning capabilities [[1](#bib.bib1), [76](#bib.bib76),
    [29](#bib.bib29), [19](#bib.bib19)]. Efforts have been increasingly directed toward
    leveraging LLMs for multimodal tasks [[43](#bib.bib43), [60](#bib.bib60), [63](#bib.bib63)].
    Techniques such as vision-language alignment and adapter fine-tuning are employed
    to integrate LLMs into the multimodal domain. These methods align the visual features
    extracted by visual encoders with the input token space of LLMs [[39](#bib.bib39)].
    Typically, the parameters of LLMs are frozen to retain their existing capabilities
    although LoRA [[18](#bib.bib18)] fine-tuning is occasionally applied to adapt
    to data with varying distributions or to acquire new behaviors. Utilizing this
    approach, several studies [[70](#bib.bib70), [44](#bib.bib44), [36](#bib.bib36),
    [84](#bib.bib84)] have successfully employed LLMs for video understanding tasks,
    referred to as Vid-LLMs [[64](#bib.bib64)]. However, current research primarily
    concentrates on general video understanding tasks, such as video question-answering
    (QA) and video captioning, with less emphasis on temporal information. Recent
    works [[22](#bib.bib22), [55](#bib.bib55), [65](#bib.bib65)] have started exploring
    LLMs’ potential in temporal grounding and localization, highlighting the untapped
    potential of LLMs in video temporal understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Comparison with existing video-to-video summarization and video-to-text
    summarization datasets. “V2V”, “V2T”, and “V2VT” denote whether the datasets support
    video-to-video summarization, video-to-text summarization, or both tasks, respectively.
    “Instruction” denotes whether the dataset supports video-text instruction tuning.
    M and S stand for manual and model synthesized, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Domain | # Videos | Annotation | V2V-Sum | V2T-Sum | V2VT-Sum |
    Instruction |'
  prefs: []
  type: TYPE_TB
- en: '| MSVD [[3](#bib.bib3)] | Open | 1,970 | M | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| YouCook [[5](#bib.bib5)] | Cooking | 88 | M | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| MSR-VTT [[79](#bib.bib79)] | Open | 7,180 | M | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| UCF101 [[62](#bib.bib62)] | Open | 13,320 | M | ✗ | ✓ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ActivityNetCap [[31](#bib.bib31)] | Activities | 20,000 | M | ✗ | ✓ | ✗ |
    ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Shot2Story20k [[15](#bib.bib15)] | Open | 20,000 | M+S | ✗ | ✓ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SumMe [[14](#bib.bib14)] | Events, holidays, sports | 25 | M | ✓ | ✗ | ✗
    | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TVSum [[61](#bib.bib61)] | News, documentaries, vlogs | 50 | M | ✓ | ✗ |
    ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| OVP [[6](#bib.bib6)] | Documentaries, lectures | 50 | M | ✓ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| VSUMM [[7](#bib.bib7)] | Cartoons, news, commercials, shows | 50 | M | ✓
    | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| EDUVSUM [[12](#bib.bib12)] | Letures | 98 | M | ✓ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| LoL [[10](#bib.bib10)] | Matches of League of Legends | 218 | M | ✓ | ✗ |
    ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Ads-1K [[66](#bib.bib66)] | Commercials | 1,041 | M+S | ✓ | ✗ | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| VideoXum [[37](#bib.bib37)] | Activities | 14,001 | M | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Instruct-V2Xum | Open | 30,000 | M+S | ✓ | ✓ | ✓ | ✓ | ![Refer to caption](img/2f26e1a6b242a032f053abbcc46ea6ff.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Distribution of the duration of source videos (in seconds) the summaries
    length for both video and text in Instruct-V2Xum.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 The Instruct-V2Xum Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we introduce the details of data collection and provide the
    quantity and quality analysis for Instruct-V2Xum, which is an instruction-tuning
    dataset for cross-modal Video summarization.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Data Curation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.1.1 Frame Captioning and Extractive Summarization.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The source videos are sampled from InternVid [[74](#bib.bib74)]. We first filter
    the raw videos according to the duration of the videos and their aesthetic scores.
    We obtain 30k for video and text summaries generation. Initially, we extract frames
    at a rate of 1 FPS, followed by the transformation of these sampled video frames
    into their detailed textual representations using LLaVA-1.5-7B [[39](#bib.bib39)].
    Upon obtaining the text representations of the video frames, we utilize GPT-4
    to perform extractive document summarization. Finally, the frame captions from
    the text summaries are converted into both video and text summaries.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Text Summarization Refinement.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To further reduce the redundancy of the text summaries, we utilize BERT score
    [[90](#bib.bib90)] to filter out the frame text representations that are similar
    to other summary frames. Here, we set the threshold to 0.93\. The filtered video
    frame captions’ indexes serve as the video summaries for the source videos. Then,
    we employ GPT-4 to further compress and rewrite the video summaries to be shorter
    and more grammar-fluent.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Quantity Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here we provide the statistical results for our collected data. Figure [2](#S2.F2
    "Figure 2 ‣ 2.3 Large Language Models ‣ 2 Related Work ‣ V2Xum-LLM: Cross-Modal
    Video Summarization with Temporal Prompt Instruction Tuning") illustrates the
    distribution of the duration of source videos and the summaries length for both
    video and text in Instruct-V2Xum. We also provide more statistical results in
    the appendix. The average duration of the source videos is 183 seconds. The average
    length of the text summaries is 239 tokens. The average length of the video summaries
    is 30 frames. The average compression ratio is 16.39%.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Quality Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the text summaries are synthesized by LLMs, we analyze the quality of
    the generated summaries from the perspective of grammar fluency and commonsense
    plausibility. For the grammar-fluency evaluation, we leverage a grammar-check
    model [[46](#bib.bib46)] that assigns high scores to correct texts grammatically.
    For the commonsense plausibility evaluation, we utilize Vera [[40](#bib.bib40)],
    a plausibility estimation model, to identify the nonsensical bias. The results
    of these evaluations are presented in the appendix. Overall, our data demonstrates
    reasonable scores in both grammar and Vera evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0f3b677d974271577399e714277ed7ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The architecture of our proposed V2Xum-LLaMA.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section presents our proposed unified cross-modal video summarization
    framework V2Xum-LLaMA. Our method is distinguished by its utilization of temporal
    prompt encoding and temporal-aware decoding to condense and summarize video content
    effectively. As depicted in Fig [3](#S3.F3 "Figure 3 ‣ 3.3 Quality Analysis ‣
    3 The Instruct-V2Xum Dataset ‣ V2Xum-LLM: Cross-Modal Video Summarization with
    Temporal Prompt Instruction Tuning"), our model architecture comprises three key
    components: video frames are encoded through a vision encoder, subsequently combined
    with temporal prompts, and finally processed by a decoder-only LLM. Utilizing
    LLaMA as the decoder, our system is capable of generating both video-to-video
    and video-to-text summarizations. Temporal Prompt Encoding associates video frames
    with corresponding timestamps, embedding positional information into each frame.
    These encodings are then merged with specific summarization instructions, and
    the summaries are produced through temporal-aware decoding via LLaMA. Our methodology
    allows for modality control over the generated summaries, based on the provided
    instructions, producing either video or text summarizations, or a combination
    thereof. This innovative encoding-decoding mechanism enhances the model’s understanding
    of the video timeline and improves the summarization output’s relevance and accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Interleaved Video and Temporal Prompt Encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The temporal prompt mechanism is designed to bind the visual tokens with the
    corresponding timestamps at the frame level to inject positional information for
    each frame. To this end, we first encode each video frame $f_{i}$ is the number
    of sampled frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then bind temporal prompts with each visual token. The temporal prompts
    are tokenized zero-padded numbers in natural language format like “00”, “06”,
    “12”, “99”, etc., indicated by $T=\{t_{1},t_{2},...,t_{L}\}$ to form a new sequence
    with interleaved visual tokens and temporal prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S=\{t_{1},v_{1},t_{2},v_{2},...,t_{L},v_{L}\}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Compared to the original visual token sequence, the temporal prompted sequence
    can better capture the relations between the timestamps and the visual semantics.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Temporal-Aware Decoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use decoder-only LLM, LLaMA [[69](#bib.bib69)] as the decoder to generate
    video-to-video summarization $A^{v}$, the output is given by the temporal-aware
    decoding:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A^{x}=LLM(S,I^{x}),~{}x\in\{v,t,b\}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The video-to-video summarization $A^{v}$. The video-to-text summarization is
    a summarized caption in a natural language format. In our implementation shown
    as [Fig. 3](#S3.F3 "In 3.3 Quality Analysis ‣ 3 The Instruct-V2Xum Dataset ‣ V2Xum-LLM:
    Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning"), the
    video-to-text summarization also contains frame referring temporal tokens, i.e.:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $A^{t}=\{...,w_{i-1},t_{j},w_{i+1},w_{i+2}...\},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where the temporal token $t_{j}$. This is called temporal-aware decoding. An
    example is shown in [Fig. 3](#S3.F3 "In 3.3 Quality Analysis ‣ 3 The Instruct-V2Xum
    Dataset ‣ V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction
    Tuning"). When decoding, the temporal tokens can be decoded together with the
    text summaries and represent the temporal position where the described visual
    content occurred in the input video.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Task-Controllable Video Summarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned before, the types of summarization can be controlled by the instructions
    $I^{x}$, and we will further introduce how to utilize the instruction following
    the capability of LLM to realize the modality controllable video summarization.
  prefs: []
  type: TYPE_NORMAL
- en: We use instructions like “Please generate a/BOTH VIDEO/TEXT summarization for
    this video.” as the instructions and tokenize them with the same text tokenizer
    that tokenizes the temporal prompts.
  prefs: []
  type: TYPE_NORMAL
- en: During the instruction tuning, we freeze the parameters of the vision encoder
    and update the vision adapter, an MLP that aligns the visual semantic with the
    input token space of the LLM, and the language decoder to force it to generate
    temporal indexes.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table 2: Comparison Results on the VideoXum dataset. “TSH-Free” means the model
    is task-specific-head-free; “B-4” denotes the BLEU-4; “M” denotes the METEOR metric;
    “R-L” refers to the ROUGE-L metric; “C” represents the CIDEr metric for consensus-based
    evaluation. For all evaluation metrics, higher scores indicate better performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Cross- Modal | LLM- Based | TSH- Free | V2T | V2V | V2VT |'
  prefs: []
  type: TYPE_TB
- en: '| B-4 | M | R-L | C | F1 | Spearman | Kendall | F[CLIP] | Cross-F[CLIP] |'
  prefs: []
  type: TYPE_TB
- en: '| DENSE [[31](#bib.bib31)] | ✗ | ✗ | ✓ | 1.6 | 8.9 | - | - | - | - | - | -
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| DVC-D-A [[34](#bib.bib34)] | ✗ | ✗ | ✓ | 1.7 | 9.3 | - | - | - | - | - |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Bi-LSTM+TempoAttn [[93](#bib.bib93)] | ✗ | ✗ | ✓ | 2.1 | 10.0 | - | - | -
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Masked Transformer [[93](#bib.bib93)] | ✗ | ✗ | ✓ | 2.8 | 11.1 | - | - |
    - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Support-Set [[52](#bib.bib52)] | ✗ | ✗ | ✓ | 1.5 | 6.9 | 17.8 | 3.2 | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Frozen-BLIP [[33](#bib.bib33)] | ✓ | ✓ | ✗ | 0.0 | 0.4 | 1.4 | 0.0 | 16.1
    | 0.011 | 0.008 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Vid2Seq-HCY [[81](#bib.bib81)] | ✓ | ✓ | ✓ | 2.3 | 8.2 | 19.0 | 7.6 | 24.2
    | - | - | 0.888 | 0.214 |'
  prefs: []
  type: TYPE_TB
- en: '| Vid2Seq-HC [[81](#bib.bib81)] | ✓ | ✓ | ✓ | 2.7 | 8.5 | 19.8 | 8.4 | 24.5
    | - | - | 0.892 | 0.217 |'
  prefs: []
  type: TYPE_TB
- en: '| Vid2Seq-HCV [[81](#bib.bib81)] | ✓ | ✓ | ✓ | 2.7 | 8.4 | 19.8 | 8.3 | 25.1
    | - | - | 0.899 | 0.200 |'
  prefs: []
  type: TYPE_TB
- en: '| VSUM-BLIP [[37](#bib.bib37)] | ✗ | ✓ | ✗ | - | - | - | - | 21.7 | 0.207 |
    0.131 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TSUM-BLIP [[37](#bib.bib37)] | ✗ | ✓ | ✗ | 5.6 | 11.8 | 24.9 | 20.9 | - |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| VTSUM-BLIP [[37](#bib.bib37)] | ✓ | ✓ | ✗ | 5.8 | 12.2 | 25.1 | 23.1 | 23.5
    | 0.258 | 0.196 | 0.894 | 0.247 |'
  prefs: []
  type: TYPE_TB
- en: '| V2Xum-LLaMA-7B (ours) | ✓ | ✓ | ✓ | 5.8 | 12.3 | 26.3 | 26.9 | 29.0 | 0.298
    | 0.204 | 0.931 | 0.253 |'
  prefs: []
  type: TYPE_TB
- en: '| V2Xum-LLaMA-13B (ours) | ✓ | ✓ | ✓ | 5.7 | 12.3 | 26.2 | 25.3 | 31.6 | 0.276
    | 0.200 | 0.957 | 0.251 |'
  prefs: []
  type: TYPE_TB
- en: '| Human | ✓ | - | - | 5.2 | 14.7 | 25.7 | 24.2 | 33.8 | 0.305 | 0.336 | 0.944
    | 0.256 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison results on the TVSum and SumMe datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | TVSum | SumMe |'
  prefs: []
  type: TYPE_TB
- en: '| Spearman | Kendall | Spearman | Kendall |'
  prefs: []
  type: TYPE_TB
- en: '| dppLSTM [[87](#bib.bib87)] | 0.055 | 0.042 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSN [[91](#bib.bib91)] | 0.020 | 0.026 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Sumgraph [[51](#bib.bib51)] | 0.138 | 0.094 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP-it [[48](#bib.bib48)] | 0.147 | 0.108 | 0.120 | 0.109 |'
  prefs: []
  type: TYPE_TB
- en: '| TL;DW [[47](#bib.bib47)] | 0.167 | 0.143 | 0.128 | 0.111 |'
  prefs: []
  type: TYPE_TB
- en: '| iPTNet [[27](#bib.bib27)] | 0.174 | 0.148 | 0.131 | 0.114 |'
  prefs: []
  type: TYPE_TB
- en: '| A2Summ [[17](#bib.bib17)] | 0.178 | 0.150 | 0.143 | 0.121 |'
  prefs: []
  type: TYPE_TB
- en: '| Standard ranker [[56](#bib.bib56)] | 0.230 | 0.176 | 0.014 | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: '| VSUM-BLIP [[37](#bib.bib37)] | 0.261 | 0.200 | 0.365 | 0.268 |'
  prefs: []
  type: TYPE_TB
- en: '| V2Xum-LLaMA | 0.293 | 0.222 | 0.378 | 0.296 |'
  prefs: []
  type: TYPE_TB
- en: 5.1 Baseline Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our V2Xum-LLaMA model, including both 7B and 13B versions, is benchmarked against
    various models adept at video-to-video (V2V) summarization, video-to-text (V2T)
    summarization, or both tasks on the VideoXum dataset. Baseline models include
    LLM-based approaches such as Frozen-BLIP [[33](#bib.bib33)], VSUM-BLIP [[37](#bib.bib37)],
    TSUM-BLIP [[37](#bib.bib37)], and VTSUM-BLIP [[37](#bib.bib37)]. We compare with
    task-specific-head-free (TSH-Free) models like DENSE [[31](#bib.bib31)], DVC-D-A [[34](#bib.bib34)],
    Bi-LSTM+TempoAttn [[93](#bib.bib93)], Masked Transformer [[93](#bib.bib93)], and
    Support-Set [[52](#bib.bib52)], which do not rely on regression-based timestamp
    prediction with extra task-specific heads. Additionally, on the classical TVSum [[61](#bib.bib61)]
    and SumMe [[14](#bib.bib14)] datasets, we compare our 7B version V2Xum-LLaMA with
    the following V2V summarization methods: dppLSTM [[87](#bib.bib87)], DSN [[91](#bib.bib91)],
    Sumgraph [[51](#bib.bib51)], CLIP-it [[48](#bib.bib48)], TL;DW [[47](#bib.bib47)],
    iPTNet [[27](#bib.bib27)], A2Summ [[17](#bib.bib17)], Standard ranker [[56](#bib.bib56)],
    and VSUM-BLIP [[37](#bib.bib37)]. We also evaluated Vid2Seq [[81](#bib.bib81)]
    models, initially pre-trained on the HowTo100M [[45](#bib.bib45)], VidChapter-7M [[80](#bib.bib80)],
    YouCook2 [[92](#bib.bib92)], and ViTT [[23](#bib.bib23)] datasets, and subsequently
    fine-tuned on the VideoXum dataset to establish baseline models for our experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We introduce new CLIP-based evaluation metrics for V2V and V2VT summarization
    evaluation. While the F1 score is a common metric for V2V summarization tasks,
    the process of video summarization annotation is highly subjective, leading to
    considerable variance among human annotators [[49](#bib.bib49)]. The traditional
    F1 score, which compares predicted video frames directly with the ground truth,
    fails to account for semantically similar frames that are close in time but not
    exactly matching, thus potentially undervaluing accurate summaries. To mitigate
    this, we introduce the $F_{CLIP}$, the recall, precision, and F1 scores are:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle R_{CLIP}(v,\hat{v})=\frac{1}{&#124;v&#124;}\sum_{v_{i}\in
    v}\max_{\hat{v}_{j}\in\hat{v}{}}\mathbf{v}_{i}^{\top}\mathbf{\hat{v}}_{j}$ |  |
    (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'For the $Cross-F_{CLIP}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Given that the cosine similarity values range from -1 to 1, we adjust the similarity
    scores by applying the operation $\max(\cos(\mathbf{v},\mathbf{\hat{v}}),0)$.
    This ensures that only non-negative similarity scores are considered in our analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use CLIP ViT-L/14@336 as the vision encoder and Vicuna-v1.5-7B/13B as the
    large language model. The vision adapter is pre-trained by vision-text alignment
    with image-text pairs from LCS-558K [[39](#bib.bib39)] dataset. After being processed
    by the Vicuna-v1.5 tokenizer, each temporal prompt takes four tokens. To facilitate
    model training, we normalize the video length into 100 via downsampling. For each
    setting, we train the V2Xum-LLaMA for 5 epochs with a learning rate of 1e-4 on
    8$\times$NVIDIA A100 GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Quantitative Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To verify the effectiveness of our proposed method, we conduct experiments on
    multiple video summarization benchmarks, including cross-modal video summarization
    and video summarization tasks. For cross-modal video summarization, we evaluate
    our method using the VideoXum dataset [[37](#bib.bib37)] along with our newly
    proposed V2Xum dataset. In the realm of V2V summarization, we utilized TVSum [[61](#bib.bib61)]
    and SumMe [[14](#bib.bib14)] as benchmarks for our evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4.1 Cross-Modal Video Summarization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The experimental results of VideoXum are shown in Table [2](#S5.T2 "Table 2
    ‣ 5 Experiments ‣ V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt
    Instruction Tuning"). It can be summarized from the table that our proposed V2Xum-LLaMA
    model outperforms all the baseline models. Specifically, in the task of V2V summarization,
    V2Xum-LLaMA achieves a 8.1% higher F1-Score than VTSUM-BLIP, alongside significant
    improvements in both Spearman and Kendall correlation metrics. In addition, V2Xum-LLaMA-13B
    achieves higher evaluation scores on the V2V summarization task than V2Xum-LLaMA-7B.
    On the contrary, V2Xum-LLaMA-7B performs better on V2T summarization. We attribute
    this result to the increased complexity of the V2T summarization compared to V2V
    summarization. Additionally, the VideoXum training set comprises only 8,000 examples,
    a quantity insufficient for effectively training models with large language decoders.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We present the performance of various models on our newly proposed Instruct-V2Xum
    dataset, with the results detailed in Table [4](#S5.T4 "Table 4 ‣ 5.4.1 Cross-Modal
    Video Summarization ‣ 5.4 Quantitative Results ‣ 5 Experiments ‣ V2Xum-LLM: Cross-Modal
    Video Summarization with Temporal Prompt Instruction Tuning"). The results indicate
    that the models are well-adapted to the dataset, exhibiting sound performance.
    Moreover, V2Xum-LLaMA-13B outshines V2Xum-LLaMA-7B in V2T summarization, which
    we believe is due to the larger language models benefiting from the increased
    volume of training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison results on the Instruct-V2Xum test set.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | V2T | V2V | V2TV |'
  prefs: []
  type: TYPE_TB
- en: '| B-4 | M | R-L | C | F1 | F[CLIP] | Cross-F[CLIP] |'
  prefs: []
  type: TYPE_TB
- en: '| Vid2Seq-HC [[81](#bib.bib81)] | 3.8 | 6.1 | 22.6 | 0.4 | 23.0 | 80.5 | 16.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vid2Seq-HCY [[81](#bib.bib81)] | 3.7 | 6.2 | 22.4 | 0.5 | 24.7 | 81.3 | 16.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vid2Seq-HCV [[81](#bib.bib81)] | 3.6 | 6.2 | 22.5 | 0.4 | 25.1 | 81.5 | 16.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| V2Xum-LLaMA-7B | 6.8 | 15.8 | 26.9 | 0.9 | 31.7 | 95.5 | 23.1 |'
  prefs: []
  type: TYPE_TB
- en: '| V2Xum-LLaMA-13B | 6.7 | 15.8 | 27.0 | 0.8 | 31.3 | 95.3 | 23.0 |'
  prefs: []
  type: TYPE_TB
- en: 5.4.2 Video-to-Video Summarization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the capability of V2Xum-LLaMA on VideoXum, TVSum, SumMe, and Instruct-V2Xum
    datasets. The results are shown in Table [2](#S5.T2 "Table 2 ‣ 5 Experiments ‣
    V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning")
    and Table [3](#S5.T3 "Table 3 ‣ 5 Experiments ‣ V2Xum-LLM: Cross-Modal Video Summarization
    with Temporal Prompt Instruction Tuning"). It can be concluded from the empirical
    results that the unified video summarization using the language decoders in V2Xum-LLaMA
    can effectively perform traditional V2V summarization tasks. V2Xum-LLaMA outperforms
    all the previous methods that relied on task-specific regression heads for generating
    video summaries. This result indicates that LLMs with temporal prompts are capable
    of performing fine-grained video temporal understanding. The results presented
    in Table [4](#S5.T4 "Table 4 ‣ 5.4.1 Cross-Modal Video Summarization ‣ 5.4 Quantitative
    Results ‣ 5 Experiments ‣ V2Xum-LLM: Cross-Modal Video Summarization with Temporal
    Prompt Instruction Tuning") affirm the validity of our proposed V2Xum dataset.
    It demonstrates that the model can properly fit the data.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further evaluate the efficacy of our proposed V2Xum-LLaMA framework and
    to underscore the importance of augmenting the training dataset, we conduct an
    ablation study on V2V and V2VT summarization tasks. First, we explore the performance
    enhancements attributed to the introduction of the temporal prompt mechanism,
    with findings detailed in Table [5](#S5.T5 "Table 5 ‣ 5.5 Ablation Study ‣ 5 Experiments
    ‣ V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction
    Tuning"). The data reveal that integrating the temporal prompt into the V2Xum-LLaMA
    framework yields superior results in both V2V and V2T summarization tasks compared
    to models lacking this feature, which indicates the temporal prompt benefits language
    models for fine-grained dense video temporal understanding. Second, to analyze
    the impact of concurrently generating video and text summaries on the performance
    of V2V and V2T summarization tasks, we conduct ablation studies. The results,
    as illustrated in Table [5](#S5.T5 "Table 5 ‣ 5.5 Ablation Study ‣ 5 Experiments
    ‣ V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction
    Tuning"), suggest that the simultaneous generation of video and text summaries
    enhances the performance in the V2V summarization task. Third, we also compare
    the performance of models training with more data (Instruct-V2Xum + VideoXum)
    and the models finetuned without more data. The results indicate that instruction
    data can effectively improve models’ performance, especially for the V2T summarization
    task. Furthermore, we conduct a comparison between models equipped with a pre-trained
    vision adapter and those without to highlight the critical importance of pre-training
    the vision adapter in enhancing performance. The results indicate that a well-pre-trained
    visual prompt adapter enhances the models’ performance for V2V and V2T tasks.
    Finally, we provide a comparison of parameter-efficient finetuning (PEFT) and
    full parameter fine-tuning. The results are presented in Table [5](#S5.T5 "Table
    5 ‣ 5.5 Ablation Study ‣ 5 Experiments ‣ V2Xum-LLM: Cross-Modal Video Summarization
    with Temporal Prompt Instruction Tuning"). The results demonstrate that full parameter
    fine-tuning outperforms PEFT in performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Ablation Study of our V2Xum-LLaMA (7B) on the VideoXum dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | V2T | V2V | V2VT |'
  prefs: []
  type: TYPE_TB
- en: '| BLEU-4 | METEOR | ROUGE-L | CIDEr | F1-Score | Spearman | Kendall | F[CLIP]
    | Cross-F[CLIP] |'
  prefs: []
  type: TYPE_TB
- en: '| V2Xum-LLaMA | 5.8 | 12.3 | 26.3 | 26.9 | 29.0 | 0.298 | 0.204 | 0.931 | 0.253
    |'
  prefs: []
  type: TYPE_TB
- en: '| w/o simultaneous VT-Sum | 5.6 | 12.2 | 25.6 | 25.9 | 25.1 | 0.249 | 0.203
    | 0.926 | 0.251 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Instruct-V2Xum | 4.9 | 12.0 | 24.3 | 21.6 | 23.1 | 0.260 | 0.191 | 0.921
    | 0.252 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o fully fine-tuning | 4.5 | 11.7 | 24.7 | 22.8 | 23.4 | 0.222 | 0.175 |
    0.915 | 0.250 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o temporal prompts | 4.4 | 11.7 | 24.4 | 21.2 | 23.9 | 0.258 | 0.192 |
    0.910 | 0.249 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o pretrained adapter | 3.1 | 11.1 | 21.9 | 9.5 | 3.7 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '6 Discussions on Video Summarization: Data, Method, and Evaluation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the existing vision summarization tasks from the
    perspective of data, method, and evaluation. As mentioned before, current existing
    video summarization datasets, including V2V and V2VT video summarization datasets,
    contain few training examples and cannot support training large-scale deep neural
    networks. The TVSum dataset comprises merely 50 YouTube videos; the SumMe dataset
    includes only 25 personal videos sourced from YouTube, while the QFVS dataset
    provides 135 video-query training samples. Additionally, VideoXum, the first cross-modal
    video summarization dataset, provides only 8k training examples. Our experimental
    results reveal that it is insufficient for training 13B models for the V2T summarization
    task. This issue is one of the significant drawbacks of current existing datasets.
    To address this problem, we collect more videos from YouTube and generate the
    corresponding cross-modal summaries for the videos using GPT-4 and propose a large-scale
    cross-modal video summarization dataset.
  prefs: []
  type: TYPE_NORMAL
- en: A conventional approach to V2V summarization involves training a regression
    head to assign an importance score to each frame, ranking frames based on these
    scores, and selecting the top K% frames to evaluate performance using metrics
    like the F1 score or Kendall/Spearman correlation against the ground truth. However,
    as demand for cross-modal video summarization grows, this method is inadequate
    for multimodal video summarization. Recent studies have begun to explore the use
    of language models for generating temporal and spatial references in videos [[22](#bib.bib22),
    [35](#bib.bib35), [55](#bib.bib55)], demonstrating the viability of using language
    models to generate the video interval indexes. In addition, leveraging large language
    models’ text decoders for both V2V and V2T summarization tasks is a logical step.
    Therefore, in this study, we investigate how to prompt large VLMs to understand
    the fine-grained video content along with temporal information, how to maximally
    leverage the powerful capability of content understanding and reasoning of large
    language models, and how to achieve task controllability via natural language
    instructions. To that end, we first propose the temporal prompt mechanism, and
    then design the visual encoder that takes the interleaved video frames, temporal,
    and language as input. Instead of using the regression head to perform V2V summarization,
    we unify different video summarization tasks into one language decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating video summaries poses a significant challenge, primarily because
    the criteria for quality are inherently subjective, vary across different viewers
    and even fluctuate over time. This subjectivity, coupled with the limited availability
    of evaluation videos and annotations, further exacerbates the ambiguity in assessing
    video summary quality [[49](#bib.bib49)]. The traditional F1 score, designed to
    directly compare predicted video frames with ground truth, ignores the nuances
    of semantically similar frames that, while temporally proximate, do not exactly
    match. This oversight can lead to the undervaluation of otherwise accurate summaries.
    To address this, we design new CLIP-based F scores for evaluating V2V and V2VT
    summarization tasks. Unlike the traditional F1 score, which relies on exact matches
    for its precision and recall calculations, $F_{CLIP}$ adopts a greedy matching
    strategy to optimize the similarity score between individual video frames and
    corresponding sentences, ensuring a more granular and accurate alignment.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we address the deficiencies in current video summarization datasets
    including the insufficient number of training examples and the insufficient evaluation
    of video summarization by building a new large-scale cross-model video summarization
    dataset Instruct-V2Xum and designing the improved video summarization evaluation
    metrics. We also propose a novel temporal prompt instruction tuning method – V2Xum-LLM
    that unifies the generation of different modalities of video summaries into the
    text decoder of VLMs. This method does not rely on task-specific heads to perform
    video summarization tasks and enables the models to accept the interleaved long
    video and language input sequences. In addition, our proposed method also supports
    modality controllable summary generation through language instructions. The experimental
    results show the effectiveness of our proposed method.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Jing Bi, Jiebo Luo, and Chenliang Xu. Procedure planning in instructional
    videos via contextual modeling and model-based policy learning. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 15611–15620,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] David L. Chen and William B. Dolan. Collecting highly parallel data for
    paraphrase evaluation. In Annual Meeting of the Association for Computational
    Linguistics, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Jinsoo Choi, Tae-Hyun Oh, and In So Kweon. Contextually customized video
    summaries via natural language. In 2018 IEEE Winter Conference on Applications
    of Computer Vision (WACV), pages 1718–1726\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Pradipto Das, Chenliang Xu, Richard F Doell, and Jason J Corso. A thousand
    frames in just a few words: Lingual description of videos through latent topics
    and sparse object stitching. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2634–2641, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Sandra Eliza Fontes de Avila, Ana Paula Brandão Lopes, Antonio da Luz,
    and Arnaldo de Albuquerque Araújo. Vsumm: A mechanism designed to produce static
    video summaries and a novel evaluation method. Pattern Recognition Letters, 32(1):56–68,
    2011. Image Processing, Computer Vision and Pattern Recognition in Latin America.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Sandra Eliza Fontes De Avila, Ana Paula Brandao Lopes, Antonio da Luz Jr,
    and Arnaldo de Albuquerque Araújo. Vsumm: A mechanism designed to produce static
    video summaries and a novel evaluation method. Pattern recognition letters, 32(1):56–68,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Jiri Fajtl, Hajar Sadeghi Sokeh, Vasileios Argyriou, Dorothy Monekosso,
    and Paolo Remagnino. Summarizing videos with attention. In Computer Vision–ACCV
    2018 Workshops: 14th Asian Conference on Computer Vision, Perth, Australia, December
    2–6, 2018, Revised Selected Papers 14, pages 39–54\. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Litong Feng, Ziyin Li, Zhanghui Kuang, and Wei Zhang. Extractive video
    summarizer with memory augmented neural networks. In Proceedings of the 26th ACM
    international conference on Multimedia, pages 976–983, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Tsu-Jui Fu, Shao-Heng Tai, and Hwann-Tzong Chen. Attentive and adversarial
    learning for video summarization. In 2019 IEEE Winter Conference on Applications
    of Computer Vision (WACV), pages 1579–1587, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Xiyan Fu, Jun Wang, and Zhenglu Yang. Multi-modal summarization for video-containing
    documents. arXiv preprint arXiv:2009.08018, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Junaid Ahmed Ghauri, Sherzod Hakimov, and Ralph Ewerth. Classification
    of important segments in educational videos using multimodal features. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Junaid Ahmed Ghauri, Sherzod Hakimov, and Ralph Ewerth. Supervised video
    summarization via multiple feature sets with parallel attention. In 2021 IEEE
    International Conference on Multimedia and Expo (ICME), pages 1–6s. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Michael Gygli, Helmut Grabner, Hayko Riemenschneider, and Luc Van Gool.
    Creating summaries from user videos. In Computer Vision–ECCV 2014: 13th European
    Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part VII 13,
    pages 505–520\. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Mingfei Han, Xiaojun Chang, Heng Wang, and Linjie Yang. Shot2story20k:
    A new benchmark for comprehensive understanding of multi-shot videos. arXiv preprint
    arXiv:2312.10300, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Li Haopeng, Ke Qiuhong, Gong Mingming, and Tom Drummond. Progressive video
    summarization via multimodal self-supervised learning. arXiv preprint arXiv:2201.02494,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Bo He, Jun Wang, Jielin Qiu, Trung Bui, Abhinav Shrivastava, and Zhaowen
    Wang. Align and attend: Multimodal summarization with dual contrastive losses.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 14867–14878, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language
    models. In International Conference on Learning Representations, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A Smith, and Jiebo
    Luo. Promptcap: Prompt-guided task-aware image captioning. arXiv preprint arXiv:2211.09699,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, and Jiebo Luo. Noise
    stability regularization for improving bert fine-tuning. arXiv preprint arXiv:2107.04835,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Hang Hua, Xingjian Li, Dejing Dou, Cheng-Zhong Xu, and Jiebo Luo. Improving
    pretrained language model fine-tuning with noise stability regularization. IEEE
    Transactions on Neural Networks and Learning Systems, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Bin Huang, Xin Wang, Hong Chen, Zihan Song, and Wenwu Zhu. Vtimellm: Empower
    llm to grasp video moments. arXiv preprint arXiv:2311.18445, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu Soricut. Multimodal
    pretraining for dense video captioning. arXiv preprint arXiv:2011.11760, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Jia-Hong Huang, Luka Murn, Marta Mrak, and Marcel Worring. Gpt2mvs: Generative
    pre-trained transformer-2 for multi-modal video summarization. In Proceedings
    of the 2021 International Conference on Multimedia Retrieval, pages 580–589, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Zhong Ji, Kailin Xiong, Yanwei Pang, and Xuelong Li. Video summarization
    with attention-based encoder–decoder networks. IEEE Transactions on Circuits and
    Systems for Video Technology, 30(6):1709–1717, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Ruipeng Jia, Yanan Cao, Hengzhu Tang, Fang Fang, Cong Cao, and Shi Wang.
    Neural extractive summarization with hierarchical attentive heterogeneous graph
    network. In Proceedings of the 2020 conference on empirical methods in natural
    language processing (EMNLP), pages 3622–3631, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Hao Jiang and Yadong Mu. Joint video summarization and moment localization
    by cross-task sample transfer. In 2022 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), pages 16367–16377, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Hanwen Jiang, Santhosh Kumar Ramakrishnan, and Kristen Grauman. Single-stage
    visual query localization in egocentric videos. Advances in Neural Information
    Processing Systems, 36, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
    Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws
    for neural language models. arXiv preprint arXiv:2001.08361, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Petros Koutras and Petros Maragos. Susinet: See, understand and summarize
    it. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops, pages 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles.
    Dense-captioning events in videos. In Proceedings of the IEEE international conference
    on computer vision, pages 706–715, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Jie Lei, Tamara L Berg, and Mohit Bansal. Detecting moments and highlights
    in videos via natural language queries. Advances in Neural Information Processing
    Systems, 34:11846–11858, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping
    language-image pre-training with frozen image encoders and large language models.
    In International conference on machine learning, pages 19730–19742\. PMLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao Mei. Jointly localizing
    and describing events for dense video captioning. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 7492–7500, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Zhaowei Li, Qi Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi Qi, Ran Zhou,
    Junting Pan, Zefeng Li, Van Tu Vu, et al. Lego: Language enhanced multi-modal
    grounding model. arXiv preprint arXiv:2401.06071, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava:
    Learning united visual representation by alignment before projection. arXiv preprint
    arXiv:2311.10122, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Jingyang Lin, Hang Hua, Ming Chen, Yikang Li, Jenhao Hsiao, Chiuman Ho,
    and Jiebo Luo. Videoxum: Cross-modal visual and textural summarization of videos.
    IEEE Transactions on Multimedia, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Kevin Qinghong Lin, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei
    Gao, Alex Jinpeng Wang, Rui Yan, and Mike Zheng Shou. Univtg: Towards unified
    video-language temporal grounding. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 2794–2804, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. Advances in neural information processing systems, 36, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Jiacheng Liu, Wenya Wang, Dianzhuo Wang, Noah A Smith, Yejin Choi, and
    Hannaneh Hajishirzi. Vera: A general-purpose plausibility estimation model for
    commonsense statements. arXiv preprint arXiv:2305.03695, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Yen-Ting Liu, Yu-Jhe Li, and Yu-Chiang Frank Wang. Transforming multi-concept
    attention into video summarization. In Proceedings of the Asian Conference on
    Computer Vision, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Dezhao Luo, Jiabo Huang, Shaogang Gong, Hailin Jin, and Yang Liu. Towards
    generalisable video moment retrieval: Visual-dynamic injection to image-text pre-training.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 23045–23055, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu,
    Zefeng Du, Shuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling
    with image, audio, video, and text integration. arXiv preprint arXiv:2306.09093,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt:
    Towards detailed video understanding via large vision and language models. arXiv
    preprint arXiv:2306.05424, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi,
    Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching
    hundred million narrated video clips. In Proceedings of the IEEE/CVF international
    conference on computer vision, pages 2630–2640, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] John X Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun
    Qi. Textattack: A framework for adversarial attacks, data augmentation, and adversarial
    training in nlp. arXiv preprint arXiv:2005.05909, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Medhini Narasimhan, Arsha Nagrani, Chen Sun, Michael Rubinstein, Trevor
    Darrell, Anna Rohrbach, and Cordelia Schmid. Tl; dw? summarizing instructional
    videos with task relevance and cross-modal saliency. In European Conference on
    Computer Vision, pages 540–557\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Medhini Narasimhan, Anna Rohrbach, and Trevor Darrell. Clip-it! language-guided
    video summarization. Advances in neural information processing systems, 34:13988–14000,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Mayu Otani, Yuta Nakashima, Esa Rahtu, and Janne Heikkila. Rethinking
    the evaluation of video summaries. In Proceedings of the IEEE/CVF conference on
    computer vision and pattern recognition, pages 7596–7604, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Shruti Palaskar, Jindrich Libovickỳ, Spandana Gella, and Florian Metze.
    Multimodal abstractive summarization for how2 videos. arXiv preprint arXiv:1906.07901,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Jungin Park, Jiyoung Lee, Ig-Jae Kim, and Kwanghoon Sohn. Sumgraph: Video
    summarization via recursive graph modeling. In Computer Vision–ECCV 2020: 16th
    European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16,
    pages 647–663\. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann,
    Joao Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation
    learning. arXiv preprint arXiv:2010.02824, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Danila Potapov, Matthijs Douze, Zaid Harchaoui, and Cordelia Schmid. Category-specific
    video summarization. In Computer Vision–ECCV 2014: 13th European Conference, Zurich,
    Switzerland, September 6-12, 2014, Proceedings, Part VI 13, pages 540–555\. Springer,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
    Learning transferable visual models from natural language supervision. In International
    conference on machine learning, pages 8748–8763\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Shuhuai Ren, Linli Yao, Shicheng Li, Xu Sun, and Lu Hou. Timechat: A time-sensitive
    multimodal large language model for long video understanding. arXiv preprint arXiv:2312.02051,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Yassir Saquil, Da Chen, Yuan He, Chuan Li, and Yong-Liang Yang. Multiple
    pairwise ranking networks for personalized video summarization. In 2021 IEEE/CVF
    International Conference on Computer Vision (ICCV), pages 1698–1707, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Aidean Sharghi, Jacob S Laurel, and Boqing Gong. Query-focused video summarization:
    Dataset, evaluation, and a memory network based approach. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 4788–4797, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Yuhan Shen, Lu Wang, and Ehsan Elhamifar. Learning to segment actions
    from visual and language instructions via differentiable weak sequence alignment.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 10156–10165, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Mike Zheng Shou, Stan Weixian Lei, Weiyao Wang, Deepti Ghadiyaram, and
    Matt Feiszli. Generic event boundary detection: A benchmark for event segmentation.
    In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    8075–8084, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Fangxun Shu, Lei Zhang, Hao Jiang, and Cihang Xie. Audio-visual llm for
    video understanding. arXiv preprint arXiv:2312.06720, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Yale Song, Jordi Vallmitjana, Amanda Stent, and Alejandro Jaimes. Tvsum:
    Summarizing web videos using titles. In Proceedings of the IEEE conference on
    computer vision and pattern recognition, pages 5179–5187, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset
    of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan Wang, and Deng Cai. Pandagpt:
    One model to instruction-follow them all. arXiv preprint arXiv:2305.16355, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang,
    Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, et al. Video understanding with
    large language models: A survey. arXiv preprint arXiv:2312.17432, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Yunlong Tang, Daiki Shimada, Jing Bi, and Chenliang Xu. Avicuna: Audio-visual
    llm with interleaver and context-boundary alignment for temporal referential dialogue.
    arXiv preprint arXiv:2403.16276, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Yunlong Tang, Siting Xu, Teng Wang, Qin Lin, Qinglin Lu, and Feng Zheng.
    Multi-modal segment assemblage network for ad video editing with importance-coherence
    reward. In Proceedings of the Asian Conference on Computer Vision, pages 3519–3535,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Yunlong Tang, Jinrui Zhang, Xiangchen Wang, Teng Wang, and Feng Zheng.
    Llmva-gebc: Large language model with video adapter for generic event boundary
    captioning. arXiv preprint arXiv:2306.10354, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Junke Wang, Dongdong Chen, Chong Luo, Xiyang Dai, Lu Yuan, Zuxuan Wu,
    and Yu-Gang Jiang. Chatvideo: A tracklet-centric multimodal and versatile video
    understanding system. arXiv preprint arXiv:2304.14407, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Junbo Wang, Wei Wang, Zhiyong Wang, Liang Wang, Dagan Feng, and Tieniu
    Tan. Stacked memory network for video summarization. In Proceedings of the 27th
    ACM international conference on multimedia, pages 836–844, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo.
    End-to-end dense video captioning with parallel decoding. In Proceedings of the
    IEEE/CVF International Conference on Computer Vision, pages 6847–6857, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Yuxuan Wang, Difei Gao, Licheng Yu, Weixian Lei, Matt Feiszli, and Mike Zheng
    Shou. Geb+: A benchmark for generic event boundary captioning, grounding and retrieval.
    In European Conference on Computer Vision, pages 709–725\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao
    Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al. Internvid: A large-scale video-text
    dataset for multimodal understanding and generation. arXiv preprint arXiv:2307.06942,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Zhenzhi Wang, Limin Wang, Tao Wu, Tianhao Li, and Gangshan Wu. Negative
    sample matters: A renaissance of metric learning for temporal grounding. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 36, pages 2613–2623,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian
    Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. Emergent
    abilities of large language models. arXiv preprint arXiv:2206.07682, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Ronald J Williams. Simple statistical gradient-following algorithms for
    connectionist reinforcement learning. Machine learning, 8:229–256, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Junbin Xiao, Xindi Shang, Angela Yao, and Tat-Seng Chua. Next-qa: Next
    phase of question-answering to explaining temporal actions. In Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition, pages 9777–9786,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description
    dataset for bridging video and language. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 5288–5296, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Antoine Yang, Arsha Nagrani, Ivan Laptev, Josef Sivic, and Cordelia Schmid.
    Vidchapters-7m: Video chapters at scale. Advances in Neural Information Processing
    Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine Miech, Jordi Pont-Tuset,
    Ivan Laptev, Josef Sivic, and Cordelia Schmid. Vid2seq: Large-scale pretraining
    of a visual language model for dense video captioning. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 10714–10726, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng
    Tao. Activitynet-qa: A dataset for understanding complex web videos via question
    answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,
    pages 9127–9134, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Chenlin Zhang, Jianxin Wu, and Yin Li. Actionformer: Localizing moments
    of actions with transformers. In European Conference on Computer Vision, pages
    492–510\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An instruction-tuned
    audio-visual language model for video understanding. arXiv preprint arXiv:2306.02858,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Haopeng Zhang, Xiao Liu, and Jiawei Zhang. Diffusum: Generation enhanced
    extractive summarization with diffusion. arXiv preprint arXiv:2305.01735, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Jinrui Zhang, Teng Wang, Feng Zheng, Ran Cheng, and Ping Luo. Exploiting
    context information for generic event boundary captioning. arXiv preprint arXiv:2207.01050,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Ke Zhang, Wei-Lun Chao, Fei Sha, and Kristen Grauman. Video summarization
    with long short-term memory. In Computer Vision–ECCV 2016: 14th European Conference,
    Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part VII 14, pages
    766–782\. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Ke Zhang, Kristen Grauman, and Fei Sha. Retrospective encoders for video
    summarization. In Proceedings of the European conference on computer vision (ECCV),
    pages 383–399, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu,
    Shilin Yan, Pan Lu, Hongsheng Li, and Yu Qiao. Llama-adapter: Efficient fine-tuning
    of language models with zero-init attention. arXiv preprint arXiv:2303.16199,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav
    Artzi. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Kaiyang Zhou, Yu Qiao, and Tao Xiang. Deep reinforcement learning for
    unsupervised video summarization with diversity-representativeness reward. In
    Proceedings of the AAAI conference on artificial intelligence, volume 32, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning
    of procedures from web instructional videos. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 32, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and Caiming Xiong.
    End-to-end dense video captioning with masked transformer. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 8739–8748, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Importance Score Prediction with LLM Logits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since our V2Xum-LLaMA directly utilizes natural language to represent the temporal
    positions, it is hard to obtain the corresponding importance scores that are usually
    adopted to calculate some conventional video summarization evaluation metrics,
    such as Spearman’s $\rho$. Therefore, we use the logits from LLM that decode the
    temporal positions (numbers) as the importance scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we first find the indices of the positions that are numbers in
    the output of V2Xum-LLaMA. Then, the corresponding logits are processed by the
    Softmax function to obtain the probabilities of the tens place and ones place:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{i}(tens)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle p_{i}(ones)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $t$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{i}(tens,ones)\approx p_{i}(tens)\times p_{i}(ones).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, we compute the average of all the $p(tens,ones)$ generated from the
    prediction in natural language format and treat it as important scores for calculating
    evaluation metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $imp\_score=\frac{1}{M}\sum_{i=i}^{M}p_{i}(tens,ones).$ |  |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Quality Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We analyze the quality of GPT-Synthesised data from the perspective of grammar
    fluency and commonsense plausibility. For the grammar-fluency evaluation, we leverage
    a grammar-check model [[46](#bib.bib46)] that assigns high scores to correct texts
    grammatically. For the commonsense plausibility evaluation, we utilize Vera [[40](#bib.bib40)],
    a plausibility estimation model, to identify the nonsensical bias. The results
    are presented in Figure [4](#A2.F4 "Figure 4 ‣ Appendix B Quality Analysis ‣ V2Xum-LLM:
    Cross-Modal Video Summarization with Temporal Prompt Instruction Tuning") and
    Figure [5](#A2.F5 "Figure 5 ‣ Appendix B Quality Analysis ‣ V2Xum-LLM: Cross-Modal
    Video Summarization with Temporal Prompt Instruction Tuning"). We can conclude
    that our dataset with the refinement process achieves a satisfactory score.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/48d10c3f2b9914cec7f951ee559ff085.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comparison of the Vera scores for the dataset before (left) and after
    (right) refinement, with higher scores indicating better results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7d644cd60b9ea44051a0d3c15a99836c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Comparison of the Grammar scores for the dataset before (left) and
    after (right) refinement, with higher scores indicating better results.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Example Data from Instruct-V2Xum
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we show some data points sampled from the Instruct-V2Xum training
    set in Figure [6](#A3.F6 "Figure 6 ‣ Appendix C Example Data from Instruct-V2Xum
    ‣ V2Xum-LLM: Cross-Modal Video Summarization with Temporal Prompt Instruction
    Tuning").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afa27f644c9c610f1acd9077081d8f2a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Some videos with video summaries and text summaries annotations from
    our Instruct-V2Xum dataset.'
  prefs: []
  type: TYPE_NORMAL
