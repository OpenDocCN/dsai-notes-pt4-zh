- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:32'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted
    Prompt Interpretation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.12579](https://ar5iv.labs.arxiv.org/html/2407.12579)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '¹¹institutetext: National Yang Ming Chiao Tung University, Taiwan,'
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹email: {leo81005.ee10, cfhsu311510211.ee11, hhshuai}@nycu.edu.tw ²²institutetext:
    Jilin University, China, ³³institutetext: National Taiwan University, Taiwan'
  prefs: []
  type: TYPE_NORMAL
- en: '³³email: {wenhuang@ntu.edu.tw} ^*^*footnotetext: These authors contributed
    equally to this workYi Yao 1*1* [0000-0001-8227-5662](https://orcid.org/0000-0001-8227-5662
    "ORCID identifier")    Chan-Feng Hsu 1*1*    Jhe-Hao Lin 11    Hongxia Xie 22
    [0000-0002-5652-4327](https://orcid.org/0000-0002-5652-4327 "ORCID identifier")
       Terence Lin 11    Yi-Ning Huang 11    Hong-Han Shuai 11 [0000-0003-2216-077X](https://orcid.org/0000-0003-2216-077X
    "ORCID identifier")    Wen-Huang Cheng 33 [0000-0002-4662-7875](https://orcid.org/0000-0002-4662-7875
    "ORCID identifier")'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In spite of recent advancements in text-to-image generation, limitations persist
    in handling complex and imaginative prompts due to the restricted diversity and
    complexity of training data. This work explores how diffusion models can generate
    images from prompts requiring artistic creativity or specialized knowledge. We
    introduce the Realistic-Fantasy Benchmark (RFBench), a novel evaluation framework
    blending realistic and fantastical scenarios. To address these challenges, we
    propose the Realistic-Fantasy Network (RFNet), a training-free approach integrating
    diffusion models with LLMs. Extensive human evaluations and GPT-based compositional
    assessments demonstrate our approach’s superiority over state-of-the-art methods.
    Our code and dataset is available at [https://leo81005.github.io/Reality-and-Fantasy/](https://leo81005.github.io/Reality-and-Fantasy/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Text-to-image Generation Realistic-Fantasy Benchmark Diffusion Model Large Language
    Models (LLMs)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/00740728149d9e7554a397734c1c227a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Text-to-image diffusion models such as Stable Diffusion [[31](#bib.bib31)]
    often struggle to accurately follow prompts that involve scientific and empirical
    reasoning, metaphorical thinking, role conflicting, or imaginative scenarios.
    Our method achieves enhanced prompt understanding capabilities and accurately
    follows these types of prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Considerable advancements have been made in the field of text-to-image generation,
    especially with the introduction of diffusion models, e.g., Stable Diffusion [[31](#bib.bib31)],
    GLIDE [[22](#bib.bib22)], DALLE2 [[30](#bib.bib30)] and Imagen [[32](#bib.bib32)].
    These models exhibit remarkable proficiency in generating diverse and high-fidelity
    images based on natural language prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, despite their impressive capabilities, diffusion models occasionally
    face challenges in accurately interpreting complex prompts that demand a deep
    understanding or specialized knowledge [[43](#bib.bib43), [39](#bib.bib39), [41](#bib.bib41)].
    This limitation becomes particularly apparent with creative and abstract prompts,
    which require a nuanced grasp of context and subtleties. For example, in scenarios
    where the prompt involves unconventional scenarios like “a rat is hunting a lion”,
    traditional diffusion models might not accurately represent the intended dynamics
    or relationships between entities (as shown in Fig.[1](#S0.F1 "Figure 1 ‣ The
    Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation")).'
  prefs: []
  type: TYPE_NORMAL
- en: A significant obstacle for traditional diffusion models in creating abstract
    images is the bias present within their training datasets [[24](#bib.bib24), [6](#bib.bib6)].
    These datasets often do not include images of scenarios that defy conventional
    reality, such as a mouse hunting a lion. Traditionally, mitigating these challenges
    has required costly data collection and complex filtering, as well as model retraining
    or fine-tuning [[8](#bib.bib8), [12](#bib.bib12), [42](#bib.bib42), [40](#bib.bib40)].
    Research costs significantly increase due to these long and labor-intensive processes.
    Moreover, fine-tuning neural networks and model editing can lead to catastrophic
    forgetting and overall performance degradation [[14](#bib.bib14), [33](#bib.bib33)].
    Recently, it has been demonstrated that utilizing Large Language Models (LLMs)
    to aid in the generation process ensures the production of accurate details [[36](#bib.bib36),
    [16](#bib.bib16), [15](#bib.bib15), [40](#bib.bib40), [5](#bib.bib5), [38](#bib.bib38),
    [27](#bib.bib27)]. Directly integrating these models during the generation phase
    signifies a more efficient strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we want to address the question: how can generative models be
    improved to better capture imaginative and abstract concepts in images? In response
    to the existing gap in benchmarks for abstract and creative text-to-image synthesis,
    our work introduces a novel benchmark, Realistic-Fantasy Benchmark (RFBench).
    This benchmark is designed to evaluate both Realistic & Analytical and Creativity
    & Imagination interpretations in generated images. The Realistic & Analytical
    category includes four sub-categories, focusing on the models’ ability to adhere
    to realism and analytical depth. Images are generated in response to prompts that
    require not only precision in science but also cultural sensitivity and nuanced
    expression of symbolic meaning. On the other hand, Creativity & Imagination, is
    segmented into five specific sub-categories based on attribute distinctions, challenges
    models to navigate the complexities of generating images from prompts that necessitate
    a high degree of creativity and abstract reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: To empower diffusion models with the capability to generate imaginative and
    abstract images, we introduce an innovative training-free approach Realistic-Fantasy
    Network (RFNet) that integrates diffusion models with LLMs. Given a prompt describing
    the desired image, the LLM generates an image layout, which includes bounding
    boxes for main subjects and background elements, along with textual details to
    support logic or interpret scientific data. To refine image generation, we further
    propose the Semantic Alignment Assessment (SAA), ensuring consistency with the
    scene’s objects. This crucial step improves the final image quality. The enhanced
    details direct the diffusion model, enabling precise object placement through
    guidance constraints. Our method, leveraging pre-trained models, is compatible
    with independently trained LLMs and diffusion models, eliminating the need for
    parameter adjustments.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our key contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have collected a novel Realistic-Fantasy Benchmark (RFBench), which is a
    meticulously curated benchmark that stands out for its rich diversity of scenarios.
    It challenges and extends the boundaries of generative model creativity and inference
    capabilities, establishing a new standard for assessing imaginative data processing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To empower diffusion models with the capability to generate imaginative and
    abstract images, we introduce an innovative training-free approach, Realistic-Fantasy
    Network (RFNet), that integrates diffusion models with LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Through our proposed RFBench, extensive human evaluations coupled with GPT-based
    compositional assessments have demonstrated our approach’s superiority over other
    state-of-the-art methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Text-guided diffusion models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Diffusion models, utilizing stochastic differential equations, have emerged
    as effective tools for generating realistic images [[31](#bib.bib31), [1](#bib.bib1),
    [9](#bib.bib9), [4](#bib.bib4), [21](#bib.bib21), [30](#bib.bib30), [22](#bib.bib22),
    [32](#bib.bib32)]. DALL-E 2 [[30](#bib.bib30)] pioneered the approach of converting
    textual descriptions into joint image-text embeddings with the aid of CLIP [[29](#bib.bib29)].
    GLIDE [[22](#bib.bib22)] demonstrated that classifier-free guidance [[11](#bib.bib11)]
    is favored by human evaluators over CLIP guidance for generating images based
    on text descriptions. Imagen [[32](#bib.bib32)] follows GLIDE but uses pretrained
    text encoder instead, further reducing negligible computation burden to the online
    training of the text-to-image diffusion prior, and can improve sample quality
    significantly by simply scaling the text encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Although text-to-image capabilities have seen significant development, there
    has been limited focus on generating images involving high levels of creativity,
    scientific principles, cultural references, and symbolic meanings. The primary
    reason is the data bias in the training dataset [[20](#bib.bib20), [34](#bib.bib34),
    [23](#bib.bib23)]. Several studies [[24](#bib.bib24), [18](#bib.bib18), [20](#bib.bib20)]
    have investigated the impact of data bias on diffusion models, particularly in
    the context of Text-to-Image generation. Perera et al.[[24](#bib.bib24)] investigates
    the bias exhibited by diffusion models across various attributes in face generation.
    Luccioni et al.[[18](#bib.bib18)] evaluates bias levels in text-to-image systems
    regarding gender and ethnicity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we introduce a new task: reality and fantasy scene generation.
    Recognizing the absence of a dedicated evaluation framework for such tasks, we
    introduce a new benchmark, the Realistic-Fantasy Benchmark (RFBench), which blends
    scenarios from both realistic and fantastical realms.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 LLMs for image generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, researchers have explored using LLMs to provide guidance or auxiliary
    information for text-to-image generation systems [[28](#bib.bib28), [19](#bib.bib19),
    [15](#bib.bib15), [7](#bib.bib7), [38](#bib.bib38), [36](#bib.bib36), [27](#bib.bib27)].
    In LMD [[15](#bib.bib15)], foreground objects are identified using LLMs, and then
    images are generated based on the layout determined by the diffusion model. Phung
    et al. [[25](#bib.bib25)] proposes attention-refocusing losses to constrain the
    generated objects on their assigned boxes generated by LLMs. LVD [[16](#bib.bib16)]
    requires LLMs to generate continuous spatial constraints to accomplish video generation.
    Besides using LLMs to generate spatial layout from user prompts, some studies
    [[36](#bib.bib36), [38](#bib.bib38), [27](#bib.bib27)] investigate integrating
    LLMs directly into the image generation pipeline. SLD [[36](#bib.bib36)] integrates
    open-vocabulary object detection with LLMs to enhance image editing. RPG [[38](#bib.bib38)]
    integrates LLMs in a closed-loop manner, allowing generated images to continuously
    improve through LLMs feedback, and uses Chain-of-Thought [[35](#bib.bib35)] to
    further improve generation quality.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of these developments, LLMs can be incorporated into pipelines for
    the generation of images. In this work, we use LLMs to uncover and elaborate upon
    the complexities embedded within complex and abstract prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Our Proposed Realistic-Fantasy Benchmark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this study, we explore how diffusion models can effectively process and generate
    imagery from prompts that pose significant challenges due to their reliance on
    creative thinking or specialized knowledge. Recognizing the absence of a dedicated
    evaluation framework for such tasks, we introduce a new benchmark, the Realistic-Fantasy
    Benchmark (RFBench), which blends scenarios from both realistic and fantastical
    realms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark Collection. We focus on two main categories, each with distinct subcategories,
    Realistic & Analytical and Creativity & Imagination, totaling nine subcategories.
    Each sub-category is meticulously crafted with around 25 text prompts, leading
    to an aggregate of 229 unique compositional text prompts designed to test the
    models against both conventional and unprecedented creative challenges. The collection
    process, outlined in [Fig. 2](#S3.F2 "In 3 Our Proposed Realistic-Fantasy Benchmark
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), employs a hybrid method combining in-context learning and predefined
    rules, leveraging powerful language models such as ChatGPT and Bard for diverse
    text prompts creation. By alternating between these models, we achieve a diverse
    set of responses, capitalizing on the distinct advantages of each LLM. It boosts
    the variety and complexity of prompts while reducing the reliance on manual labeling.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a560b5aaf755b857f869790347e0f29c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The collection pipeline of our proposed RFBench.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Categories of our proposed Realistic-Fantasy Benchmark (RFBench).
    The full dataset are show in our supplementary material.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Categories | Definition | Example |'
  prefs: []
  type: TYPE_TB
- en: '| Realistic & Analytical |'
  prefs: []
  type: TYPE_TB
- en: '| To evaluate models’ comprehensive understanding of complex ideas, factual
    accuracy, and cultural insights |'
  prefs: []
  type: TYPE_TB
- en: '| Scientific and Empirical Reasoning | Relates to hypothesis testing, deduction,
    and scientific methodology | “A drop of water on the International Space Station.”
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cultural and Temporal Awareness | Necessitates understanding and knowledge
    of particular cultural or historical events | “Children in costumes going door-to-door
    on October 31st.” |'
  prefs: []
  type: TYPE_TB
- en: '| Factual or Literal Descriptions | Evaluates the models’ capacity to generate
    images that adhere closely to factual accuracy and realistic depiction | “A tank
    that’s been sitting on the beach for 50 years.” |'
  prefs: []
  type: TYPE_TB
- en: '| Conceptual and Metaphorical Thinking | Focuses on the models’ ability to
    comprehend and depict the underlying symbolic messages within the prompts | “A
    man is as brave as a lion.” |'
  prefs: []
  type: TYPE_TB
- en: '| Creativity & Imagination |'
  prefs: []
  type: TYPE_TB
- en: '| To evaluate model’s capability in employing creativity and abstract thinking
    |'
  prefs: []
  type: TYPE_TB
- en: '| Common Objects in Unusual Contexts | Challenges models’ capacity to maintain
    object integrity while adapting to surreal environments | “A rubber duck sailing
    across a field of hot lava.” |'
  prefs: []
  type: TYPE_TB
- en: '| Imaginative Scenarios | Evaluate the models’ abilities to craft scenes involving
    animals or humans in fantastical or unlikely scenarios | “An octopus playing chess
    with a seahorse.” |'
  prefs: []
  type: TYPE_TB
- en: '| Counterfactual Scenarios | Focuses on scenarios that defy conventional expectations
    of reality | “Fish swimming in the clouds.” |'
  prefs: []
  type: TYPE_TB
- en: '| Role Reversal or Conflicting | Stimulates consideration of perspectives and
    scenarios where typical roles or expectations are inverted | “A cat is chased
    by a mouse.” |'
  prefs: []
  type: TYPE_TB
- en: '| Anthropomorphic Scenarios | Examines the model’s ability to imbue inanimate
    objects or phenomena with human-like characteristics | “A snowman building a friend
    in the blizzard.” |'
  prefs: []
  type: TYPE_TB
- en: 'Realistic & Analytical Category. There are four sub-categories: Scientific
    and Empirical Reasoning, Cultural and Temporal Awareness, Factual or Literal Descriptions,
    and Conceptual and Metaphorical Thinking (details are shown in the upper part
    of [Tab. 1](#S3.T1 "In 3 Our Proposed Realistic-Fantasy Benchmark ‣ The Fabrication
    of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation")).
    These sub-categories are anchored in real-world contexts, emphasizing logical
    reasoning, accurate data, and an understanding of cultural or historical contexts.
    They contain scientific exploration, realistic descriptions, and culturally symbolic
    narratives. This demands that the models not only draw from an extensive knowledge
    pool but also demonstrate an ability to grasp and articulate underlying concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Creativity & Imagination Category. It consists of five sub-categories: Common
    Objects in Unusual Contexts, Imaginative Scenarios, Counterfactual Scenarios,
    Role Reversal or Conflicting, and Anthropomorphic Scenarios (details are shown
    in the lower part of [Tab. 1](#S3.T1 "In 3 Our Proposed Realistic-Fantasy Benchmark
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation")). This evaluation focuses on the model’s capacity to innovatively
    repurpose familiar objects, attribute human-like characteristics to inanimate
    objects, and generate novel environments for everyday items. This category tests
    the model’s out-of-the-box thinking and imaginative capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Our Proposed Realistic-Fantasy Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad2ad9fb288469cca2706c443510a1bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Overview of our proposed Realistic-Fantasy Network (RFNet). In stage
    1, the user’s input prompt is first processed by a LLM to extract the layout and
    descriptions. The descriptions then go through a text encoder, which is the text-processing
    component of the CLIP model, and are refined by the SAA to form a better prompt.
    In stage 2, the refined prompts are fed into the diffusion model for in-depth
    object generation, which creates each target object with precision. The resulting
    cross-attention map and mask latent are then utilized for seamless background
    integration, merging objects into one single image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we propose a Realistic-Fantasy Network (RFNet) for the benchmark
    scenario we proposed in the previous section. To thoroughly interpret the details
    from the input prompt, we divide our approach into two stages, as shown in [Fig. 3](#S4.F3
    "In 4 Our Proposed Realistic-Fantasy Network ‣ The Fabrication of Reality and
    Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation"). In the first
    stage, we transform the initial input prompt into a refined version specifically
    tailored for image generation by LLMs. In the second stage, we utilize a diffusion
    model through a two-step process to generate outputs with extraordinary details.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 LLM-Driven Detail Synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the first stage of our methodology, we concentrate on utilizing LLMs to
    uncover and elaborate on the intricacies embedded within the user’s input prompt.
    This process involves specifying task requirements to more accurately define the
    task and incorporating in-context learning to enhance understanding for LLMs.
    The enriched response from the LLM encompasses additional information, such as
    layout, detailed descriptions, background scenes, and negative prompts ¹¹1One
    detailed sample can be found in our supplementary material.. This step is crucial
    as it aims to mitigate the primary challenge we seek to overcome: the training
    data bias inherent in current diffusion models. By leveraging the pre-trained
    LLM for logical reasoning and conjecture, we aim to compensate for the gaps left
    by these biases, ensuring a more accurate and coherent image generation process.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Semantic Alignment Assessment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we proceed with generating images using the diffusion model using the details
    generated by the previous step, there is a critical challenge: the description
    lists generated by LLMs for one object usually overlook the relationships among
    them. For example, interpretations of “a lion” could range from being “unaware
    and asleep” to “frightened and trying to escape.” Although both depictions are
    valid, descriptions such as “unaware” and “trying to escape” can lead to conflicting
    interpretations, thus complicating the image generation process.'
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this challenge, we introduce the Semantic Alignment Assessment (SAA)
    module. This module calculates the relevance between different object vectors,
    thereby selecting the candidate description that best fits the current scenario.
    By conducting the cosine similarity among different descriptions, we can navigate
    the complexities introduced by the LLM’s output, selecting the most compatible
    details for the diffusion model. This step is crucial for maintaining the coherence
    and accuracy of the generated images, highlighting our novel approach to mitigating
    the risk of conflicting descriptions. Through this module, we ensure textual precision
    and compatibility, and provide clear, consistent instructions for the subsequent
    diffusion model to generate visually coherent representations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d457cfeedc263c00d942c960235c0c85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Comprehensive Image Synthesis. In step 1, utilizing the prompt refined
    by the SAA module, the frozen stable diffusion model generates each foreground
    object independently. During the denoising phase, the cross-attention map is extracted
    and saved for Guidance Constraint in the next step. In addition, a Suppression
    Constraint is also added in step 2 to minimize influence between different objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Comprehensive Image Synthesis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the second stage of our proposed RFNet, following LMD [[15](#bib.bib15)],
    we propose a two-step generation process for imaginative and abstract concepts.
    As shown in [Fig. 4](#S4.F4 "In 4.2 Semantic Alignment Assessment ‣ 4 Our Proposed
    Realistic-Fantasy Network ‣ The Fabrication of Reality and Fantasy: Scene Generation
    with LLM-Assisted Prompt Interpretation"), in the first step, we focus on generating
    each foreground object with comprehensive details. In the second step, we integrate
    the objects generated in the first step into corresponding background derived
    from the initial prompt. This structured approach ensures a cohesive integration
    of detailed foreground objects into a contextually relevant background, enhancing
    the overall effectiveness of our framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 1: In-Depth Object Generation.'
  prefs: []
  type: TYPE_NORMAL
- en: We re-organize the SAA description lists to consider both the layout of specific
    objects and their descriptions. By concatenating the background prompt with the
    target object and its relevant descriptions, we set up the input prompt as “[background
    prompt] with [target object], [descriptions]” (e.g., “A grassland scene with a
    rat, roaring, with big mouth and sharp teeth, leap out at…”). Following LMD [[15](#bib.bib15)],
    the initial latent representation for each target object is fixed to facilitate
    the fusion of various objects into a cohesive background scene.
  prefs: []
  type: TYPE_NORMAL
- en: 'During generation, the diffusion model uses cross-attention layers to manage
    the influence of textual information on the visual output, allowing precise control
    over image details. The cross-attention map’s constraint function integrates objects
    within the bounding box by enhancing cross-attention inside the box for accurate
    object representation while minimizing it outside the box. This function guides
    the update of the noised latent vector during denoising to ensure spatial conditions
    match predefined specifications. The constraint function is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\textbf{m}_{i}$. Prior to each denoising step, the latent is refined
    by minimizing the constraint function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z_{t}^{{}^{\prime}}\leftarrow z_{t}-\alpha\cdot\nabla_{z_{t}}\sum_{v\in
    V}\mathcal{L}(\textbf{A},i,v),$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $z_{t-1}\leftarrow DiffusionStep(z_{t}^{{}^{\prime}},\mathcal{P}^{(i)}),$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha$, are served as the inputs to the diffusion model.
  prefs: []
  type: TYPE_NORMAL
- en: After the generation, the cross-attention map derived from each target object
    is then converted into a saliency mask. This mask is applied to the latent representation
    of the target object through element-wise multiplication at each step of the denoising
    process. Both the cross-attention map and the masked latent representation of
    the target object between each denoising step are transmitted to the next step
    for background integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Step 2: Seamless Background Integration. This step involves fusing the generated
    results with the background while preserving the high-quality generation achieved
    in the first step. To accomplish this, we first replace the generated latent $z_{t}^{{}^{\prime}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Following the approach established in Step 1, the initial latent representation
    $z_{T}^{{}^{\prime}}$, reserving principles from LMD [[15](#bib.bib15)]. According
    to LMD, the diffusion model determines the position of the objects in the early
    denoising steps, while adjusting details in the later steps. This helps us to
    preserve exceptional control over the layout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we incorporate a specialized constraint function designed to enhance
    the integration of generated objects with their background, distinguished by two
    key components: guidance constraint and suppression constraint. As shown in [Fig. 4](#S4.F4
    "In 4.2 Semantic Alignment Assessment ‣ 4 Our Proposed Realistic-Fantasy Network
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), the guidance constraint is engineered to reduce the cross-attention
    within each bounding box relative to the original object’s attention. With the
    purpose of seamlessly integrating with the detailed object generated in the first
    step. Conversely, the suppression constraint works to minimize cross-attention
    outside the bounding box, thereby mitigating interference among multiple objects
    when processed together, as illustrated in [Eq. 5](#S4.E5 "In 4.3 Comprehensive
    Image Synthesis ‣ 4 Our Proposed Realistic-Fantasy Network ‣ The Fabrication of
    Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation").
    These constraint functions mark a departure from conventional methods that predominantly
    use loss to fix the layout.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\textbf{A}^{{}^{\prime}}$ is fed into the decoder to produce the final
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Our strategy focuses on maintaining the integrity and coherence of the foreground
    objects generated, emphasizing the preservation of their quality and interaction
    with the background. By doing so, the generated visual elements are fidelity-aware
    and contextually appropriate.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Implementation Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experimental Setup. In this work, we choose versions 1.4 and 2.1 of Stable Diffusion [[31](#bib.bib31)]
    as the text-to-image baseline model. The number of denoising steps is set as 50
    with a fixed guidance scale of 7.5, and the synthetic images are in a resolution
    of 512 × 512\. All experiments are conducted on the NVIDIA RTX 3090 GPU with 24
    GB memory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Evaluation Metrics. We generate 32 images for each text prompt in RFBench for
    automatic evaluation. We selected the following two metrics: (1) GPT4-CLIP ²²2We
    adopt GPT4-CLIP due to BLIP-CLIP’s [[3](#bib.bib3)] limitations in accurately
    capturing image meanings through generated captions.. By utilizing GPT4 for captioning
    and calculating CLIP text-text cosine similarity, GPT4-CLIP ensures a more precise
    reflection of the intended meanings between images and prompts. (2) GPT4Score.
    Inspired by  [[13](#bib.bib13), [17](#bib.bib17)], we adopt GPT4Score to evaluate
    image alignment with text prompts, where GPT4 rates images on a 0-100 scale based
    on their fidelity to the prompts, enabling precise assessment of model-generated
    visuals against specified criteria³³3The widely recognized metric, CLIPScore [[29](#bib.bib29),
    [10](#bib.bib10)] exhibits limitations in evaluating our task. For detailed examples,
    please see the supplementary materials..'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Benchmarking on GPT4-CLIP and GPT4Score. R & A, C & I, Avg represent
    Realistic & Analytical category, Creativity & Imagination category, and average
    of both categories, respectively. The red text indicates the improvement ratio
    of our method compared to Stable Diffusion.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | GPT4-CLIP | GPT4Score |'
  prefs: []
  type: TYPE_TB
- en: '| R & A | C & I | Avg | R & A | C & I | Avg |'
  prefs: []
  type: TYPE_TB
- en: '| Stable Diffusion [[31](#bib.bib31)] | 0.573 | 0.552 | 0.561 | 0.667 | 0.440
    | 0.541 |'
  prefs: []
  type: TYPE_TB
- en: '| MultiDiffusion [[2](#bib.bib2)] | 0.510 | 0.510 | 0.510 | 0.517 | 0.493 |
    0.504 |'
  prefs: []
  type: TYPE_TB
- en: '| Attend and Excite [[3](#bib.bib3)] | 0.523 | 0.560 | 0.546 | 0.633 | 0.520
    | 0.570 |'
  prefs: []
  type: TYPE_TB
- en: '| LLM-groundedDiffusion [[15](#bib.bib15)] | 0.457 | 0.536 | 0.501 | 0.550
    | 0.600 | 0.578 |'
  prefs: []
  type: TYPE_TB
- en: '| BoxDiff [[37](#bib.bib37)] | 0.532 | 0.553 | 0.543 | 0.583 | 0.520 | 0.548
    |'
  prefs: []
  type: TYPE_TB
- en: '| SDXL [[26](#bib.bib26)] | 0.536 | 0.619 | 0.582 | 0.567 | 0.587 | 0.578 |'
  prefs: []
  type: TYPE_TB
- en: '| RFNet(ours) | 0.587 (2%$\uparrow$) |'
  prefs: []
  type: TYPE_TB
- en: Comparison with Existing Methods. We benchmark our proposed RFNet against various
    open-source scene generation methods, including Stable Diffusion [[31](#bib.bib31)],
    Attend and Excite [[3](#bib.bib3)], LMD [[15](#bib.bib15)], BoxDiff [[37](#bib.bib37)],
    MultiDiffusion [[2](#bib.bib2)], and SDXL [[26](#bib.bib26)]. Notably, all methods,
    including ours, utilize Stable Diffusion 2.1 as the foundational model, ensuring
    a fair comparison.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Quantitative Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Evaluation on RFBench. As evidenced in [Tab. 5](#S4.T5 "In D Human Evaluation
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), our approach significantly outperforms other methods for both
    Realistic & Analytical and Creativity & Imagination tasks. For Realistic & Analytical
    task, our method seamlessly integrates LLM-based insights, achieving a remarkable
    accuracy improvement. Unlike Attend-and-excite, which focuses on semantic guidance,
    our method ensures precise adherence to detailed and complex prompt requirements.
    For the Creativity & Imagination, which demands high degrees of creativity and
    abstract conceptualization, our method outperforms others by not only adhering
    to the imaginative aspects of prompts but also maintaining coherent structure
    and contexts. For instance, SDXL, while adept at high-resolution image synthesis,
    occasionally lacks in capturing the nuanced creativity intended in prompts; our
    method fills this gap effectively. Similarly, LMD, though enhancing prompt understanding
    through LLMs, sometimes struggles with the scientific reasoning required for Realistic&
    Analytical tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Notably, for Realistic & Analytical category, our approach shows a 61% performance
    increase over MultiDiffusion on GPT4Score. Meanwhile, in Creativity & Imagination
    task, we observe a substantial enhancement, outperforming Stable Diffusion by
    over 43%. In light of the above, our method is unique in its ability to bridge
    the gap between realistic reasoning and imagination, creating a new benchmark
    for text-to-image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: GPT4Score comparison with Imagen on DrawBench subset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Prompt | Imagen | Ours |'
  prefs: []
  type: TYPE_TB
- en: '| A bird scaring a scarecrow | 0.069 | 0.275 |'
  prefs: []
  type: TYPE_TB
- en: '| A blue coloured pizza | 0.425 | 0.125 |'
  prefs: []
  type: TYPE_TB
- en: '| A fish eating a pelican | 0.000 | 0.000 |'
  prefs: []
  type: TYPE_TB
- en: '| A horse riding an astronaut | 0.000 | 0.000 |'
  prefs: []
  type: TYPE_TB
- en: '| A panda making latte art | 0.050 | 0.250 |'
  prefs: []
  type: TYPE_TB
- en: '| A pizza cooking an oven | 0.700 | 0.831 |'
  prefs: []
  type: TYPE_TB
- en: '| A shark in the desert | 0.194 | 0.713 |'
  prefs: []
  type: TYPE_TB
- en: '| An elephant under the sea | 0.300 | 0.900 |'
  prefs: []
  type: TYPE_TB
- en: '| Hovering cow abducting aliens | 0.025 | 0.144 |'
  prefs: []
  type: TYPE_TB
- en: '| Rainbow coloured penguin | 0.394 | 0.519 |'
  prefs: []
  type: TYPE_TB
- en: 'Evaluation on DrawBench. We also evaluate our method on DrawBench [[32](#bib.bib32)],
    a comprehensive and challenging benchmark for text-to-image models. Similar to
    us, DrawBench also includes some Creativity & Imagination prompts, and we evaluate
    our method with Imagen [[32](#bib.bib32)] on these prompts. As shown in [Tab. 3](#S5.T3
    "In 5.2 Quantitative Evaluation ‣ 5 Experiments ‣ The Fabrication of Reality and
    Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation"), our approach
    significantly outperforms Imagen on most prompt settings, demonstrating the generalization
    ability of our model.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Qualitative Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99241f26add22f86eece1409d8d0d51d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Qualitative comparison on RFBench. The compared models include (a)
    Stable Diffusion, (b) MultiDiffusion, (c) Attend and Excite, (d) LMD, (e) BoxDiff,
    (f) SDXL, (g) Ours (Best viewed in color and zoom in. More samples can be found
    in our supplementary material.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the qualitative comparison of text-guided image generation, we select some
    advanced baseline methods, including Attend-and-excite [[3](#bib.bib3)], BoxDiff [[37](#bib.bib37)],
    LMD [[15](#bib.bib15)], MultiDiffusion [[2](#bib.bib2)], and SDXL [[26](#bib.bib26)].
    Attend-and-excite focuses on enhancing the semantic understanding of prompts through
    attention mechanisms, while BoxDiff introduces a novel approach to text-to-image
    synthesis with box-constrained diffusion without the need for explicit training.
    MultiDiffusion proposes a method for fusing multiple diffusion paths to achieve
    greater control over the image generation process, and SDXL aims at improving
    the capabilities of latent diffusion models for synthesizing high-resolution images.
    As shown in Fig. [5](#S5.F5 "Figure 5 ‣ 5.3 Qualitative Evaluation ‣ 5 Experiments
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), our method, produces more precise editing results than the aforementioned
    methods. This is attributed to our In-Depth Object Generation and Seamless Background
    Integration strategy. It ensures outstanding fidelity in outcomes and flawlessly
    retains the semantic structure of the source image, highlighting our approach’s
    superior capability in complex editing tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 User study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Through an extensive user study, we benchmarked our model against other methods
    to assess real human preferences for the generated images. Utilizing our newly
    proposed benchmark, the RFBench, we selected a diverse set of 27 prompts and generated
    six images per prompt to ensure a broad representation of the model’s capabilities.
    Detailed feedbacks were collected from 120 participants, evaluating each image
    for visual quality and text prompt fidelity ⁴⁴4Details of survey samples can be
    found in our supplementary material.. These criteria are critical, which measure
    the image’s quality and correctness of semantics in the synthesized image. Participants
    rated images on a scale from {1, 2, 3, 4, 5}, with scores normalized by dividing
    by 5\. We calculated the average score across all images and participants.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [6](#S5.F6 "Figure 6 ‣ 5.4 User study ‣ 5 Experiments
    ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt
    Interpretation"), participants uniformly favored our model’s output, recognizing
    it as superior in both quality and alignment with the textual descriptions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/de8597446ecfa7e34dd10faf2c12003c.png)![Refer to caption](img/3cd4f402d2c27847e82b211cd7e424b0.png)![Refer
    to caption](img/ab51159eb21167a64daf738074a4d1c7.png)![Refer to caption](img/09010d7494e269e019cac3c1794bcb93.png)![Refer
    to caption](img/ffee3ec22d2c1a0e560f418eee2d8414.png)![Refer to caption](img/6f38e5244f00cd4f0d9aa98aee00418d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparison between our method and other advanced methods on RFBench.
    The image-text Alignment and Fidelity of our method are highly preferred by users.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Ablation study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table 4: Ablation studies on various components on RFBench.'
  prefs: []
  type: TYPE_NORMAL
- en: '| SAA | guidance | suppression | GPT4Score |'
  prefs: []
  type: TYPE_TB
- en: '|  |  |  | 0.295 |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\checkmark$ | 0.407 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\checkmark$ |  | 0.554 |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\checkmark$ | 0.572 |'
  prefs: []
  type: TYPE_TB
- en: '| $\checkmark$ | 0.719 |'
  prefs: []
  type: TYPE_TB
- en: 'Impact of Various Constraints. To validate the impact of guidance constraint
    and suppression constraint, we perform ablation studies on different combinations
    of constraints, and the results are listed in [Tab. 4](#S5.T4 "In 5.5 Ablation
    study ‣ 5 Experiments ‣ The Fabrication of Reality and Fantasy: Scene Generation
    with LLM-Assisted Prompt Interpretation"). As shown, the baseline model (Stable
    Diffusion) achieves a 0.295 in terms of GPT4Score without any constraints. As
    guidance constraint and suppression constraint work complementary to restrict
    the cross-attention of objects inside the conditional boxes, a higher GPT4Score
    of 0.572 is achieved on the generated images. Both proposed constraints are effective
    in controlling image quality and layout of synthesized foreground objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Impact of Semantic Alignment Assessment (SAA) Module. As aforementioned, using
    conflict descriptions in the denoise step may potentially affect image synthesis.
    The quantitative evaluation is presented in [Tab. 4](#S5.T4 "In 5.5 Ablation study
    ‣ 5 Experiments ‣ The Fabrication of Reality and Fantasy: Scene Generation with
    LLM-Assisted Prompt Interpretation"). In the absence of SAA, the model attains
    a GPT4Score of 0.572\. Similarly, with SAA, the model reaches a GPT4Score of 0.719\.
    This indicates a lack of consistency between the semantics generated in the images
    and the provided text prompts, leading to a reduction in image quality. It is
    important to note that the inclusion of SAA significantly enhances the clarity
    of the images obtained. One visual illustration can be found in  [Fig. 7](#S5.F7
    "In 5.5 Ablation study ‣ 5 Experiments ‣ The Fabrication of Reality and Fantasy:
    Scene Generation with LLM-Assisted Prompt Interpretation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ff1b7552f48a4f17512a2a3cfa699183.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The illustration of final generated results of minimum (min) and
    maximum (max) similarity descriptions during the SAA. It can be observed that
    prompts with higher similarity yield images of higher quality, which is significantly
    better than the one with the lowest similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this research, we present a novel challenge: generating scenes that blend
    reality and fantasy. We investigate the capacity of diffusion models to create
    visuals from prompts that demand high levels of creativity or specific knowledge.
    Noting the lack of a specific evaluation mechanism for such tasks, we establish
    the Realistic-Fantasy Benchmark (RFBench), combining elements of both realistic
    and imaginary scenarios. To address the task of generating realistic and fantastical
    scenes, we introduce a unique, training-free, two-tiered method, Realistic-Fantasy
    Network (RFNet), that combines diffusion models with large language models (LLMs).
    Our approach, evaluated through the RFBench using thorough human assessments and
    GPT-based compositional evaluations, has proven to be superior to existing cutting-edge
    techniques. Given the novelty of our task, future research could develop additional
    evaluation metrics beyond those used in this study, enhancing the assessment of
    generated scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Anciukevičius, T., Xu, Z., Fisher, M., Henderson, P., Bilen, H., Mitra,
    N.J., Guerrero, P.: Renderdiffusion: Image diffusion for 3d reconstruction, inpainting
    and generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition. pp. 12608–12618 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Bar-Tal, O., Yariv, L., Lipman, Y., Dekel, T.: Multidiffusion: Fusing diffusion
    paths for controlled image generation. In: International Conference on Machine
    Learning (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., Cohen-Or, D.: Attend-and-excite:
    Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions
    on Graphics (TOG) 42(4), 1–10 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Feng, W., He, X., Fu, T.J., Jampani, V., Akula, A.R., Narayana, P., Basu,
    S., Wang, X.E., Wang, W.Y.: Training-free structured diffusion guidance for compositional
    text-to-image synthesis. In: International Conference on Learning Representations
    (2023), [https://openreview.net/forum?id=PUIqjT4rzq7](https://openreview.net/forum?id=PUIqjT4rzq7)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Feng, W., Zhu, W., Fu, T.j., Jampani, V., Akula, A., He, X., Basu, S.,
    Wang, X.E., Wang, W.Y.: Layoutgpt: Compositional visual planning and generation
    with large language models. In: Advances in Neural Information Processing Systems.
    vol. 36 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Friedrich, F., Brack, M., Struppek, L., Hintersdorf, D., Schramowski, P.,
    Luccioni, S., Kersting, K.: Fair diffusion: Instructing text-to-image generation
    models on fairness. arXiv preprint arXiv:2302.10893 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Gani, H., Bhat, S.F., Naseer, M., Khan, S., Wonka, P.: Llm blueprint: Enabling
    text-to-image generation with complex and detailed prompts. In: International
    Conference on Learning Representations (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Golnari, P.A.: Lora-enhanced distillation on guided diffusion models. arXiv
    preprint arXiv:2312.06899 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Gong, J., Foo, L.G., Fan, Z., Ke, Q., Rahmani, H., Liu, J.: Diffpose: Toward
    more reliable 3d pose estimation. In: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition. pp. 13041–13051 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Hessel, J., Holtzman, A., Forbes, M., Bras, R.L., Choi, Y.: Clipscore:
    A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718
    (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Ho, J., Salimans, T.: Classifier-free diffusion guidance. In: Advances
    in Neural Information Processing Systems Workshop (2021), [https://openreview.net/forum?id=qw8AKxfYbI](https://openreview.net/forum?id=qw8AKxfYbI)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,
    L., Chen, W.: Lora: Low-rank adaptation of large language models. In: International
    Conference on Learning Representations (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Huang, K., Sun, K., Xie, E., Li, Z., Liu, X.: T2i-compbench: A comprehensive
    benchmark for open-world compositional text-to-image generation. In: Advances
    in Neural Information Processing Systems. vol. 36 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Kemker, R., McClure, M., Abitino, A., Hayes, T., Kanan, C.: Measuring
    catastrophic forgetting in neural networks. In: Proceedings of the AAAI Conference
    on Artificial Intelligence. vol. 32 (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Lian, L., Li, B., Yala, A., Darrell, T.: Llm-grounded diffusion: Enhancing
    prompt understanding of text-to-image diffusion models with large language models.
    arXiv preprint arXiv:2305.13655 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Lian, L., Shi, B., Yala, A., Darrell, T., Li, B.: Llm-grounded video diffusion
    models. arXiv preprint arXiv:2309.17444 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Advances
    in Neural Information Processing Systems (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Luccioni, S., Akiki, C., Mitchell, M., Jernite, Y.: Stable bias: Evaluating
    societal representations in diffusion models. In: Advances in Neural Information
    Processing Systems. vol. 36 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Mantri, K.S.I., Sasikumar, N.: Interactive fashion content generation
    using llms and latent diffusion models. In: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition Workshop (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Naik, R., Nushi, B.: Social biases through the text-to-image generation
    lens. In: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society. p.
    786–808\. AIES ’23, Association for Computing Machinery, New York, NY, USA (2023).
    https://doi.org/10.1145/3600211.3604711'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Nair, N.G., Cherian, A., Lohit, S., Wang, Y., Koike-Akino, T., Patel,
    V.M., Marks, T.K.: Steered diffusion: A generalized framework for plug-and-play
    conditional image synthesis. In: Proceedings of the IEEE/CVF International Conference
    on Computer Vision. pp. 20850–20860 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew,
    B., Sutskever, I., Chen, M.: Glide: Towards photorealistic image generation and
    editing with text-guided diffusion models. In: Proceedings of Machine Learning
    Research. pp. 16784–16804 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Orgad, H., Kawar, B., Belinkov, Y.: Editing implicit assumptions in text-to-image
    diffusion models. In: 2023 IEEE/CVF International Conference on Computer Vision.
    pp. 7030–7038\. IEEE Computer Society, Los Alamitos, CA, USA (oct 2023). https://doi.org/10.1109/ICCV51070.2023.00649'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Perera, M.V., Patel, V.M.: Analyzing bias in diffusion-based face generation
    models. arXiv preprint arXiv:2305.06402 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Phung, Q., Ge, S., Huang, J.B.: Grounded text-to-image synthesis with
    attention refocusing. arXiv preprint arXiv:2306.05427 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., Müller,
    J., Penna, J., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution
    image synthesis. In: International Conference on Learning Representations (2024),
    [https://openreview.net/forum?id=di52zR8xgf](https://openreview.net/forum?id=di52zR8xgf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Qin, J., Wu, J., Chen, W., Ren, Y., Li, H., Wu, H., Xiao, X., Wang, R.,
    Wen, S.: Diffusiongpt: Llm-driven text-to-image generation system. arXiv preprint
    arXiv:2401.10061 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Qu, L., Wu, S., Fei, H., Nie, L., Chua, T.S.: Layoutllm-t2i: Eliciting
    layout guidance from llm for text-to-image generation. In: Proceedings of the
    ACM International Conference on Multimedia (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable
    visual models from natural language supervision. In: International Conference
    on Machine Learning. pp. 8748–8763\. PMLR (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical
    text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125
    1(2),  3 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
    image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition. pp. 10684–10695 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,
    K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic
    text-to-image diffusion models with deep language understanding. In: Advances
    in Neural Information Processing Systems. vol. 35, pp. 36479–36494 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Smith, J.S., Hsu, Y.C., Zhang, L., Hua, T., Kira, Z., Shen, Y., Jin, H.:
    Continual diffusion: Continual customization of text-to-image diffusion with c-lora.
    arXiv preprint arXiv:2304.06027 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Su, X., Ren, Y., Qiang, W., Song, Z., Gao, H., Wu, F., Zheng, C.: Unbiased
    image synthesis via manifold-driven sampling in diffusion models. arXiv preprint
    arXiv:2307.08199 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., et al.: Chain-of-thought prompting elicits reasoning in large language
    models. In: Advances in Neural Information Processing Systems. vol. 35, pp. 24824–24837
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Wu, T.H., Lian, L., Gonzalez, J.E., Li, B., Darrell, T.: Self-correcting
    llm-controlled diffusion models. arXiv preprint arXiv:2311.16090 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Xie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y., Shou, M.Z.:
    Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion.
    In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp.
    7452–7461 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Yang, L., Yu, Z., Meng, C., Xu, M., Ermon, S., Cui, B.: Mastering text-to-image
    diffusion: Recaptioning, planning, and generating with multimodal llms. arXiv
    preprint arXiv:2401.11708 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W.,
    Cui, B., Yang, M.H.: Diffusion models: A comprehensive survey of methods and applications.
    ACM Comput. Surv. 56(4) (2023). https://doi.org/10.1145/3626235'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Yang, Z., Wang, J., Gan, Z., Li, L., Lin, K., Wu, C., Duan, N., Liu, Z.,
    Liu, C., Zeng, M., Wang, L.: Reco: Region-controlled text-to-image generation.
    In: Proceedings of the IEEE/CVF International Conference on Computer Vision and
    Pattern Recognition. pp. 14246–14255 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Zhang, C., Zhang, C., Zhang, M., Kweon, I.S.: Text-to-image diffusion
    model in generative ai: A survey. arXiv preprint arXiv:2303.07909 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
    diffusion models. In: Proceedings of the IEEE/CVF International Conference on
    Computer Vision. pp. 3836–3847 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Zhang, T., Wang, Z., Huang, J., Tasnim, M.M., Shi, W.: A survey of diffusion
    based image generation models: Issues and their solutions. arXiv preprint arXiv:2308.13142
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A LLM-Driven Detail Synthesis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this work, as described in the Sec. 4.1 of the main paper, we emphasized
    that by leveraging LLMs, we have significantly enriched responses to encompass
    additional information, such as layout, detailed descriptions, background scenes,
    and negative prompts. To achieve this, we facilitated an interaction with a LLM
    as shown in [Fig. 8](#S1.F8 "In A LLM-Driven Detail Synthesis ‣ The Fabrication
    of Reality and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation").
    The input given to the LLM, depicted on the left side of the figure, includes
    detailed task specifications and in-context learning examples to enhance the LLM’s
    comprehension. The response from the LLM, shown on the right, is rich with details
    extracted from the prompt. Notably, the descriptions are particularly crucial
    for our work, serving as indispensable information for the later image generation
    stage.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3fa4b7bd8c21299ce5ab8c6e012e48eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Detail Synthesis. The illustration of the interaction with a LLM
    in our work.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d22883f60838316c001b1a45648043a7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: More results on Realistic and Analytical. The compared models include
    (a) Stable Diffusion, (b) MultiDiffusion, (c) AttendandExcite, (d) LMD, (e) BoxDiff,
    (f) SDXL, (g) Ours'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2be88a6f76470e3cd89a744ddc24c3ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: More results on Creativity and Imagination. The compared models
    include (a) Stable Diffusion, (b) MultiDiffusion, (c) AttendandExcite, (d) LMD,
    (e) BoxDiff, (f) SDXL, (g) Ours'
  prefs: []
  type: TYPE_NORMAL
- en: B Qualitative Comparison on RFBench
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [Fig. 9](#S1.F9 "In A LLM-Driven Detail Synthesis ‣ The Fabrication of Reality
    and Fantasy: Scene Generation with LLM-Assisted Prompt Interpretation") and [Fig. 10](#S1.F10
    "In A LLM-Driven Detail Synthesis ‣ The Fabrication of Reality and Fantasy: Scene
    Generation with LLM-Assisted Prompt Interpretation"), we present additional qualitative
    examples to showcase the exceptional outcomes of our work. [Fig. 9](#S1.F9 "In
    A LLM-Driven Detail Synthesis ‣ The Fabrication of Reality and Fantasy: Scene
    Generation with LLM-Assisted Prompt Interpretation") shows the results under the
    category Realistic and Analytical, while [Fig. 10](#S1.F10 "In A LLM-Driven Detail
    Synthesis ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted
    Prompt Interpretation") shows the category Creativity and Imagination. Both figures
    demonstrate that our method achieves more accurate editing results compared to
    other approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fca95df7d04f51287981f62a5996b4d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Survey on Image-Text Alignment and Image Fidelity'
  prefs: []
  type: TYPE_NORMAL
- en: C GPT4Score
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We follow the approach of T2I-Compbench, using Multimodal LLM (MLLM) to measure
    the similarity between generated images and input prompts. The key deviation lies
    in our observation that MiniGPT4, employed in T2I-Compbench, struggles to comprehend
    the surreal aspects of the images effectively. Therefore, we employ GPT4, a more
    powerful MLLM, as our new benchmarking model for evaluation, as mentioned in the
    Sec. 5.1 of the main paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, given a generated image and its prompt, we input both the image
    and prompt into GPT4\. Subsequently, we pose two questions to the model: “*Describe
    the image*” and “*Predict the image-text alignment score*”, the generated image
    is then assigned the final output score predicted by GPT4\. For detailed prompts,
    please refer to the appendix of T2I-Compbench.'
  prefs: []
  type: TYPE_NORMAL
- en: D Human Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the human evaluation process, as introduced in the Sec. 5.4 of the main
    paper, we request annotators to assess the correspondence between a produced image
    and the textual prompt employed to create the image.  [Fig. 11](#S2.F11 "In B
    Qualitative Comparison on RFBench ‣ The Fabrication of Reality and Fantasy: Scene
    Generation with LLM-Assisted Prompt Interpretation") show the interfaces for human
    evaluation. The participants can choose a score from {1, 2, 3, 4, 5} and we normalize
    the scores by dividing them by 5\. We then compute the average score across all
    images and all participants.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: The correlation between automatic evaluation metrics and human evaluation'
  prefs: []
  type: TYPE_NORMAL
- en: '|   Metrics | CLIPScore | GPT4Score |'
  prefs: []
  type: TYPE_TB
- en: '| $\tau$) |'
  prefs: []
  type: TYPE_TB
- en: '| Realistic & Analytical |'
  prefs: []
  type: TYPE_TB
- en: '|   Scientific and Empirical Reasoning | -0.4880 | -0.5946 | 0.6351 | 0.7157
    |'
  prefs: []
  type: TYPE_TB
- en: '|   Cultural and Temporal Awareness | -0.0476 | -0.1429 | 0.3273 | 0.3780 |'
  prefs: []
  type: TYPE_TB
- en: '|   Factual or Literal Descriptions | 0.2333 | 0.3656 | 0.7620 | 0.8909 |'
  prefs: []
  type: TYPE_TB
- en: '|   Conceptual and Metaphorical Thinking | -0.1952 | -0.1982 | 0.9234 | 0.9633
    |'
  prefs: []
  type: TYPE_TB
- en: '| Creativity & Imagination |'
  prefs: []
  type: TYPE_TB
- en: '|   Common Objects in Unusual Contexts | -0.2381 | -0.2857 | -0.5345 | -0.6124
    |'
  prefs: []
  type: TYPE_TB
- en: '|   Imaginative Scenarios | 0.3752 | 0.6335 | 0.7265 | 0.8432 |'
  prefs: []
  type: TYPE_TB
- en: '|   Role Reversal or Conflicting | 0.0476 | 0.1429 | 0.5040 | 0.5774 |'
  prefs: []
  type: TYPE_TB
- en: '|   Anthropomorphic Scenarios | -0.1429 | -0.1429 | -0.5345 | -0.6124 |'
  prefs: []
  type: TYPE_TB
- en: E Human Correlation of the Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We adopt the methodology from T2I-Compbench, calculating Kendall’s tau ($\tau$)
    to evaluate the ranking correlation between CLIPScore, GPT4Score, and human evaluation.
    For better comparison, the scores predicted by each evaluation metric are normalized
    to a 0-1 scale. The human correlation results are presented in  [Tab. 5](#S4.T5
    "In D Human Evaluation ‣ The Fabrication of Reality and Fantasy: Scene Generation
    with LLM-Assisted Prompt Interpretation"). These results indicate that CLIP underperforms
    in both categories, as discussed in Section 5.1 of the main paper. This underperformance
    may be due to CLIP’s approach to image understanding, which is often too simplistic.
    Nevertheless, both metrics encounter challenges with Creativity and Imagination,
    highlighting that although GPT4Score offers a broader understanding of images,
    accurately assessing creativity remains a difficult task for both.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc065e19094f950681e6d4d558cef4ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Ablation study on various components in our work.'
  prefs: []
  type: TYPE_NORMAL
- en: F Visualization of Ablation Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the quantitative results presented in our ablation study, we
    have also included visual examples to showcase the impact of different components
    in our work. As shown in [Fig. 12](#S5.F12 "In E Human Correlation of the Evaluation
    Metrics ‣ The Fabrication of Reality and Fantasy: Scene Generation with LLM-Assisted
    Prompt Interpretation"), the removal of guidance constraint and suppression constraint
    both causes the diffusion model to become muddled when dealing with multiple objects.
    Besides, eliminating the SAA module leads to unclear outcomes with the generated
    objects.'
  prefs: []
  type: TYPE_NORMAL
- en: F.1 Effect of the hyperparameter $\beta$ of guidance constraint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our paper, we emphasize the critical role of the guidance constraint in integrating
    multiple objects into the background. To underscore its significance, we performed
    an additional ablation study focusing on the hyperparameter $\beta$ values, such
    as 0.1 or 30, disrupt the layout and diminish the overall quality of the generated
    images.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/30c422688cded664c2fcd3d6108330b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Effect of the hyperparameter $\beta$ of guidance constraint.'
  prefs: []
  type: TYPE_NORMAL
