- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:42:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:42:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: An Investigation of Prompt Variations for Zero-shot LLM-based Rankers
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对零-shot LLM 基础排名器的提示变体的调查
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14117](https://ar5iv.labs.arxiv.org/html/2406.14117)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.14117](https://ar5iv.labs.arxiv.org/html/2406.14117)
- en: Shuoqi Sun¹, Shengyao Zhuang^(1,2), Shuai Wang¹, Guido Zuccon¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shuoqi Sun¹, Shengyao Zhuang^(1,2), Shuai Wang¹, Guido Zuccon¹
- en: ¹The University of Queensland,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹昆士兰大学，
- en: ²CSIRO
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²CSIRO
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We provide a systematic understanding of the impact of specific components and
    wordings used in prompts on the effectiveness of rankers based on zero-shot Large
    Language Models (LLMs). Several zero-shot ranking methods based on LLMs have recently
    been proposed. Among many aspects, methods differ across (1) the ranking algorithm
    they implement, e.g., pointwise vs. listwise, (2) the backbone LLMs used, e.g.,
    GPT3.5 vs. FLAN-T5, (3) the components and wording used in prompts, e.g., the
    use or not of role-definition (role-playing) and the actual words used to express
    this. It is currently unclear whether performance differences are due to the underlying
    ranking algorithm, or because of spurious factors such as better choice of words
    used in prompts. This confusion risks to undermine future research. Through our
    large-scale experimentation and analysis, we find that ranking algorithms do contribute
    to differences between methods for zero-shot LLM ranking. However, so do the LLM
    backbones – but even more importantly, the choice of prompt components and wordings
    affect the ranking. In fact, in our experiments, we find that, at times, these
    latter elements have more impact on the ranker’s effectiveness than the actual
    ranking algorithms, and that differences among ranking methods become more blurred
    when prompt variations are considered.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们系统地了解了在零-shot 大型语言模型（LLMs）基础上的排名器的效果中，特定组件和提示用词的影响。最近提出了几种基于 LLMs 的零-shot
    排名方法。在许多方面，这些方法在（1）实现的排名算法，例如点对点 vs. 列表对比，（2）使用的基础 LLMs，例如 GPT3.5 vs. FLAN-T5，（3）提示中使用的组件和用词，例如是否使用角色定义（角色扮演）及表达这些的实际词语等方面有所不同。目前尚不清楚性能差异是由于基础排名算法，还是由于如提示中用词选择等虚假的因素。这种困惑可能会影响未来的研究。通过我们的大规模实验和分析，我们发现排名算法确实对零-shot
    LLM 排名方法之间的差异有所贡献。然而，LLM 的基础模型也同样重要——更重要的是，提示组件和用词的选择影响排名。事实上，在我们的实验中，我们发现这些后者元素在排名器的有效性上有时比实际的排名算法更有影响，当考虑提示变体时，排名方法之间的差异变得更加模糊。
- en: An Investigation of Prompt Variations for Zero-shot LLM-based Rankers
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对零-shot LLM 基础排名器的提示变体的调查
- en: Shuoqi Sun¹, Shengyao Zhuang^(1,2), Shuai Wang¹, Guido Zuccon¹ ¹The University
    of Queensland, ²CSIRO
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Shuoqi Sun¹, Shengyao Zhuang^(1,2), Shuai Wang¹, Guido Zuccon¹ ¹昆士兰大学，²CSIRO
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) are massively parametrised transformer models that
    have been undergone training on a large extent of text Zhao et al. ([2023](#bib.bib32)).
    Generative LLMs are capable of generating text in response to some textual input,
    called prompt Brown et al. ([2020](#bib.bib3)), or context. This prompting facility
    has been used to instruct LLMs to perform specific tasks Brown et al. ([2020](#bib.bib3));
    Wang et al. ([2023](#bib.bib29)); White et al. ([2023](#bib.bib30)); Zhuang et al.
    ([2023b](#bib.bib36)); Fan et al. ([2023](#bib.bib5)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）是大规模参数化的变换器模型，这些模型已经在大量文本上进行了训练 Zhao et al. ([2023](#bib.bib32))。生成式
    LLMs 能够根据某些文本输入生成文本，这些输入被称为提示 Brown et al. ([2020](#bib.bib3))，或上下文。这种提示功能已被用来指导
    LLMs 执行特定任务 Brown et al. ([2020](#bib.bib3))；Wang et al. ([2023](#bib.bib29))；White
    et al. ([2023](#bib.bib30))；Zhuang et al. ([2023b](#bib.bib36))；Fan et al. ([2023](#bib.bib5))。
- en: In this paper, we investigate the use of LLMs to create zero-shot rankers¹¹1We
    specifically focus on re-rankers, where an initial set of documents is retrieved
    from the index using a first-stage retriever, and a subset is provided to the
    re-ranker for producing the final search engine results. For ease of reading,
    we use rankers, in place of re-rankers throughout the paper. Ma et al. ([2023](#bib.bib14));
    Sun et al. ([2023](#bib.bib22)); Tang et al. ([2023](#bib.bib23)); Zhuang et al.
    ([2023a](#bib.bib33)); Qin et al. ([2023](#bib.bib18)); Zhuang et al. ([2023b](#bib.bib36)).
    With zero-shot, we mean ranking methods that do not require to be specifically
    trained for ranking tasks (beyond the pretraining executed to create the backbone
    LLM). These rankers operate by following the instructions provided in the prompt,
    which include the query for which the ranking should be produced, and the $k$
    documents that should be considered for ranking. Using the LLM, rankers then generate
    an answer to comply with the ranking instruction. Finally, either the generated
    answer contains the ranking provided by the method, or the logits of the answer
    are examined to infer a ranking.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了使用LLM创建零-shot排序器¹¹1我们特别关注再排序器，其中初始文档集由第一阶段检索器从索引中检索出来，并将子集提供给再排序器以生成最终的搜索引擎结果。为便于阅读，我们在整个论文中使用排序器代替再排序器。Ma
    等人 ([2023](#bib.bib14)); Sun 等人 ([2023](#bib.bib22)); Tang 等人 ([2023](#bib.bib23));
    Zhuang 等人 ([2023a](#bib.bib33)); Qin 等人 ([2023](#bib.bib18)); Zhuang 等人 ([2023b](#bib.bib36))。这里的零-shot指的是不需要为排序任务专门训练的排序方法（超出创建基础LLM时执行的预训练）。这些排序器通过遵循提示中提供的指令来操作，其中包括生成排序的查询和应考虑的$k$个文档。使用LLM，排序器然后生成一个符合排序指令的答案。最后，生成的答案要么包含方法提供的排序，要么检查答案的logits以推断排序。
- en: 'Four families of zero-shot LLM rankers have been proposed: pointwise Zhuang
    et al. ([2023a](#bib.bib33)), pairwise Qin et al. ([2023](#bib.bib18)), listwise Ma
    et al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Tang et al. ([2023](#bib.bib23)),
    and setwise Zhuang et al. ([2023b](#bib.bib36)). They differ because of the ranking
    algorithm (or mechanism) implemented in the instructions described in the prompt.
    For example, in pointwise the LLM is instructed to determine the relevance of
    a document, while in pairwise the LLM is instructed to determine which of two
    documents is more relevant. Within each family, one or more methods have been
    proposed. They typically differ in the backbone LLM, the wording of the prompts,
    and in how the ranking is formed once the answer is generated by the LLM.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出四类零-shot LLM排序器：pointwise Zhuang 等人 ([2023a](#bib.bib33))，pairwise Qin 等人
    ([2023](#bib.bib18))，listwise Ma 等人 ([2023](#bib.bib14)); Sun 等人 ([2023](#bib.bib22));
    Tang 等人 ([2023](#bib.bib23))，以及setwise Zhuang 等人 ([2023b](#bib.bib36))。它们的不同在于提示中描述的排序算法（或机制）。例如，在pointwise中，LLM被指示确定文档的相关性，而在pairwise中，LLM被指示确定两个文档中哪个更相关。在每个类别中，已经提出了一种或多种方法。它们通常在基础LLM、提示的措辞以及生成LLM答案后的排序形成方式上有所不同。
- en: '| Method | Prompt |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 提示 |'
- en: '| PRP, [Qin et al.](#bib.bib18) | Passage: {text} Query: {query} Does the passage
    answer the query? |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| PRP, [Qin 等人](#bib.bib18) | Passage: {text} Query: {query} 该段落是否回答了查询？ |'
- en: '| RankGPT, [Sun et al.](#bib.bib22) | You are RankGPT, an intelligent assistant
    that can rank passages based on their relevancy to the query. I will provide you
    with num passages, each indicated by number identifier []. Rank the passages based
    on their relevance to query: {query}. {PASSAGES} Search Query: {query}. Rank the
    num passages above based on their relevance to the search query. The passages
    should be listed in descending order using identifiers. The most relevant passages
    should be listed first. The output format should be [] > [], e.g., [1] > [2].
    Only response the ranking results, do not say any word or explain. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| RankGPT, [Sun 等人](#bib.bib22) | 你是RankGPT，一种可以根据查询的相关性对段落进行排序的智能助手。我将提供num个段落，每个段落由编号标识符[]指示。根据它们与查询的相关性对段落进行排序：{query}。{PASSAGES}
    搜索查询：{query}。根据搜索查询的相关性对上述num个段落进行排序。段落应按编号的降序列出。最相关的段落应首先列出。输出格式应为[] > []，例如，[1]
    > [2]。仅响应排序结果，不要说任何话或解释。 |'
- en: 'Table 1: Comparison between the prompts used by PRP Qin et al. ([2023](#bib.bib18))
    and RankGPT Sun et al. ([2023](#bib.bib22)): they do not simply differ in the
    ranking approach used (PRP: pointwise; RankGPT: listwise), but also in the accessory
    wording present in the prompt, e.g., the role-playing present in RankGPT (first
    line of the prompt).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：PRP Qin 等人 ([2023](#bib.bib18)) 和 RankGPT Sun 等人 ([2023](#bib.bib22)) 使用的提示的比较：它们不仅在所用的排序方法（PRP：逐点；RankGPT：逐列表）上有所不同，还在提示中的附加措辞上有所不同，例如
    RankGPT 中的角色扮演（提示的第一行）。
- en: 'Recent work by Zhuang et al. ([2023b](#bib.bib36)) has shown that, once these
    zero-shot LLM rankers are compared using the same backbone LLM and fixing how
    the ranking is derived (i.e. generation vs. logits), setwise methods are the most
    effective. Depending on the dataset, pairwise and listwise methods are similarly
    effective, with pointwise methods providing lower effectiveness overall. While
    they did recognise that the use of different backbone LLMs in previous work biased
    the comparison between methods, they did not identify that the actual prompts
    used by the different rankers differed not just in terms of the words used to
    describe the ranking algorithm, i.e. the instruction associated with how scoring
    should be performed, but also in terms of “accessory wording” used. For example,
    consider the original prompt used by [Qin et al.](#bib.bib18) for their PRP pairwise
    approach Qin et al. ([2023](#bib.bib18)) and that used by [Sun et al.](#bib.bib22)
    for their RankGPT listwise approach Sun et al. ([2023](#bib.bib22)) – see Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ An Investigation of Prompt Variations for Zero-shot
    LLM-based Rankers"). One can notice that RankGPT’s prompt includes also a “role-playing”
    component (in the system part of the prompt: “You are RankGPT […]”), absent in
    PRP.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Zhuang 等人 ([2023b](#bib.bib36)) 最近的研究表明，一旦使用相同的基础 LLM 比较这些零-shot LLM 排名器，并且固定排序的生成方式（即生成
    vs. logits），集合方法是最有效的。根据数据集的不同，成对和逐列表方法的效果相似，而逐点方法的整体效果较低。他们认识到，之前工作中使用不同的基础 LLM
    使得方法间的比较产生了偏差，但并没有识别到不同排名器实际使用的提示不仅在描述排序算法的词语上有所不同，即与评分执行方式相关的指令，还在“附加措辞”上有所不同。例如，考虑
    [Qin et al.](#bib.bib18) 为他们的 PRP 成对方法使用的原始提示 ([2023](#bib.bib18)) 和 [Sun et al.](#bib.bib22)
    为他们的 RankGPT 逐列表方法使用的提示 ([2023](#bib.bib22)) – 见表 [1](#S1.T1 "表 1 ‣ 1 引言 ‣ 对零-shot
    LLM 排名器提示变异的调查")。可以注意到，RankGPT 的提示还包括一个“角色扮演”组件（在提示的系统部分：“你是 RankGPT [...]”），而
    PRP 中则没有。
- en: 'With this respect, then, we ask ourself: What is the effect of such differences
    in wording used in the prompts? And, more broadly: Are differences in effectiveness
    due to the actual ranking algorithm, or they are due to the choice of words used
    in the prompts? Are differences due to LLM characteristics such as backbone and
    size?'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 就此而言，我们自问：这些提示中措辞差异的效果是什么？更广泛地说：效果的差异是由于实际的排序算法，还是由于提示中使用的词语选择？差异是否由于 LLM 的特性，如模型结构和规模？
- en: These are important questions because answering them will give us an understanding
    of what impacts the effectiveness of LLM rankers, and will influence how methods
    should be compared in future. We explore these directions of enquiry through a
    wide array of experiments in which we fix the backbone LLMs of the rankers, and
    we vary prompts in a controlled manner, categorising different prompt components
    and investigating their effects.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题很重要，因为回答这些问题将帮助我们理解哪些因素影响 LLM 排名器的效果，并影响未来方法的比较方式。我们通过一系列实验探索这些问题，其中我们固定了排名器的基础
    LLM，并以受控的方式变换提示，分类不同的提示组件并研究其效果。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Sensibility of LLMs to Prompt
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 对提示的敏感性
- en: 'Previous work has shown that LLMs are sensitive to the prompt formulation.
    For example, Kim et al. ([2023](#bib.bib10)) explored the effect of various prompts
    and prompting technology on deploying LLMs across Natural Language Generation
    tasks, showing dependency between prompt and effectiveness. Similar findings were
    reported by Thomas et al. ([2023](#bib.bib25)), who explored different prompt
    wordings in the context of relevance labelling. Kamruzzaman and Kim ([2024](#bib.bib9))
    explored how different prompting strategies could influence the presence of social
    biases in LLM outputs: prompts designed to engage higher cognitive processes could
    reduce biases compared to simpler prompts. These previous work, among others,
    have demonstrated that the structure and subtle nuances of prompt phrasing can
    dramatically impact the performance of the LLMs, across a wide range of tasks
    and contexts.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以往的研究表明，LLM对提示的制定非常敏感。例如，Kim等人（[2023](#bib.bib10)）探讨了不同提示和提示技术在自然语言生成任务中的应用效果，展示了提示与效果之间的依赖关系。Thomas等人（[2023](#bib.bib25)）在相关性标注的背景下探讨了不同的提示措辞，得出了类似的发现。Kamruzzaman和Kim（[2024](#bib.bib9)）探讨了不同提示策略如何影响LLM输出中的社会偏见：设计以激发更高认知过程的提示相比简单提示能够减少偏见。这些及其他的先前研究已经证明，提示措辞的结构和微妙差异可以显著影响LLM的表现，涵盖广泛的任务和背景。
- en: 2.2 Prompt Optimisation and Self-Optimisers
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 提示优化和自我优化器
- en: 'Strategic prompt design is not only beneficial but necessary to harness the
    full capabilities of LLMs. Recent studies like that by Yang et al. ([2023](#bib.bib31))
    have taken this further by investigating LLMs as self-optimisers. These models
    utilize their generative capabilities to iteratively refine prompts, thereby enhancing
    their performance on downstream tasks. Guo et al. ([2023](#bib.bib7)) integrated
    evolutionary algorithms with LLMs so that prompts can be dynamically adapted without
    needing gradient information. Sabbatella et al. ([2024](#bib.bib19))’s method
    involves a continuous relaxation of the search space, allowing for efficient optimization
    of prompts; this is suitable for scenarios where only black-box access to LLMs
    is provided. In our work we do not explore the adaptation of self-optimisers to
    prompts for zero-shot LLM rankers: but our study motivates pursuing this as a
    direction for future work.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 战略性提示设计不仅是有益的，而且是必需的，以充分发挥LLM的能力。最近的研究如Yang等人（[2023](#bib.bib31)）进一步探讨了LLM作为自我优化器的功能。这些模型利用其生成能力迭代地优化提示，从而提升在下游任务中的表现。Guo等人（[2023](#bib.bib7)）将进化算法与LLM集成，使得提示能够在不需要梯度信息的情况下动态调整。Sabbatella等人（[2024](#bib.bib19)）的方法涉及搜索空间的连续松弛，从而实现对提示的高效优化；这适用于仅提供黑箱访问LLM的场景。在我们的研究中，我们并未探讨自我优化器在零-shot
    LLM排名器中的适应性，但我们的研究激励了未来朝着这个方向进行探索。
- en: 2.3 Zero-Shot LLM Rankers
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 零-Shot LLM 排名器
- en: 'We next review generative LLM-based rankers; four ranker families are discussed:
    pointwise Zhuang et al. ([2023a](#bib.bib33)), pairwise Qin et al. ([2023](#bib.bib18)),
    listwise Ma et al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Tang
    et al. ([2023](#bib.bib23)), and setwise Zhuang et al. ([2023b](#bib.bib36)).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们回顾基于生成式LLM的排名器；讨论了四种排名器类型：pointwise Zhuang等人（[2023a](#bib.bib33)）、pairwise
    Qin等人（[2023](#bib.bib18)）、listwise Ma等人（[2023](#bib.bib14)）；Sun等人（[2023](#bib.bib22)）；Tang等人（[2023](#bib.bib23)），以及setwise
    Zhuang等人（[2023b](#bib.bib36)）。
- en: 'In pointwise, two approaches can be followed: generation Liang et al. ([2022](#bib.bib12));
    Nogueira et al. ([2020](#bib.bib16)) and likelihood Zhuang et al. ([2021](#bib.bib34));
    Zhuang and Zuccon ([2021](#bib.bib37)). In generation, LLMs are prompted with
    a query-document pair and asked to generate a binary answer (“yes"/“no") to the
    question of whether the document is relevant to the query, and the likelihood
    of generating “yes" (extracted from the associated token logits) decides the ranking.
    In likelihood, a query likelihood model is involved in ranking. The LLM is prompted
    with the document and asked to produce a relevant query. Then, the raking of documents
    is based on the likelihood of generating the provided query Sachan et al. ([2022](#bib.bib20)),
    which is obtained from the associated token logits. In our experiments with pointwise
    we implemented the generation approach, which is more commonly used in previous
    work.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在点对点（pointwise）中，可以采用两种方法：生成方法 Liang et al. ([2022](#bib.bib12)); Nogueira et
    al. ([2020](#bib.bib16)) 和可能性方法 Zhuang et al. ([2021](#bib.bib34)); Zhuang and
    Zuccon ([2021](#bib.bib37))。在生成方法中，LLM 接受一个查询-文档对的提示，并被要求生成一个二元答案（“yes”/“no”），以回答文档是否与查询相关，生成“yes”的可能性（从相关的
    token logits 提取）决定了排名。在可能性方法中，涉及一个查询可能性模型进行排名。LLM 接受文档的提示，并被要求生成一个相关的查询。然后，文档的排名基于生成提供的查询
    Sachan et al. ([2022](#bib.bib20)) 的可能性，这从相关的 token logits 中获得。在我们的点对点实验中，我们实现了生成方法，这在以前的工作中更常使用。
- en: Pairwise ranking instead compares the relevance of two documents to a single
    query; these are passed onto the model via the prompt. LLMs are prompted to answer
    which document is more relevant to the query. The ranking of documents is based
    on relative relevance Qin et al. ([2023](#bib.bib18)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 配对方法（pairwise）排名则比较两个文档对单个查询的相关性；这些文档通过提示传递给模型。LLM 被提示回答哪个文档对查询更相关。文档的排名基于相对相关性
    Qin et al. ([2023](#bib.bib18))。
- en: For listwise ranking a list of documents and a query is passed to the LLM via
    the prompt. LLMs are driven to generate the document labels of relevant documents
    in a certain order; the ranking then depends on this Ma et al. ([2023](#bib.bib14));
    Sun et al. ([2023](#bib.bib22)); Pradeep et al. ([2023](#bib.bib17)).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于列表方法（listwise）排名，一组文档和一个查询通过提示传递给 LLM。LLM 被驱动以某种顺序生成相关文档的标签；排名则依赖于此 Ma et
    al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Pradeep et al. ([2023](#bib.bib17))。
- en: In setwise, a set of documents is provided with one query. LLMs are prompted
    to select the most relevant document, in an iterative way. This allows sorting
    algorithms to give the ranking results based on the preference to documents, and
    to stop ranking after $k$ were given as input) Zhuang et al. ([2023b](#bib.bib36)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在集合方法（setwise）中，提供了一组文档和一个查询。LLM 被提示以迭代的方式选择最相关的文档。这允许排序算法根据对文档的偏好给出排名结果，并在给出
    $k$ 个输入后停止排名 Zhuang et al. ([2023b](#bib.bib36))。
- en: In common across all these rankers is their use of generative LLM backbones
    in a zero-shot manner, i.e. without the need for further training the LLM for
    the specific ranking approach.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这些排名方法共同的特点是它们在零-shot 模式下使用生成性 LLM 骨干，即无需对 LLM 进行特定排名方法的进一步训练。
- en: 'We finally note a recent, different trend in information retrieval where generative
    LLMs are used to obtain dense representations of documents and queries (separately,
    i.e. in a bi-encoder manner), which are then in turn used for dense retrieval:
    while most require fine-tuning Lee et al. ([2024](#bib.bib11)); Wang et al. ([2024a](#bib.bib27),
    [b](#bib.bib28)); BehnamGhader et al. ([2024](#bib.bib2)), zero-shot methods are
    also emerging Zhuang et al. ([2024](#bib.bib35)). In this paper we focus on zero-shot
    LLM-based re-rankers, and thus do not consider these dense retrievers; however
    we note they are likely affected in the same way from the issues we investigate Zhuang
    et al. ([2024](#bib.bib35)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后注意到信息检索中一个最近的不同趋势，其中生成性 LLM 被用来获得文档和查询的密集表示（分别，即以双编码器的方式），这些表示随后用于密集检索：虽然大多数方法需要微调
    Lee et al. ([2024](#bib.bib11)); Wang et al. ([2024a](#bib.bib27), [b](#bib.bib28));
    BehnamGhader et al. ([2024](#bib.bib2))，但零-shot 方法也在出现 Zhuang et al. ([2024](#bib.bib35))。在本文中，我们专注于零-shot
    LLM 基于的再排名器，因此不考虑这些密集检索器；不过我们注意到它们可能会受到我们调查的问题的类似影响 Zhuang et al. ([2024](#bib.bib35))。
- en: 3 Methodology
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: Different strategies for using LLMs as rankers have been proposed; they not
    only differ in the ranking algorithm and backbone used, but also in the specific
    wording provided to the LLMs to perform the task. Next, we aim to collect the
    original prompts used in the literature, which will serve as the foundational
    prompt setting for our experiments. We then analyse the individual characteristics
    of each prompt, building a taxonomy of the components used in the prompts along
    with a list of instantiations used for each component across the different prompts
    (Section [3.1](#S3.SS1 "3.1 Prompts Collection and Taxonomy of Ranking Prompt
    Components ‣ 3 Methodology ‣ An Investigation of Prompt Variations for Zero-shot
    LLM-based Rankers")). With this information, we aim then to design experiments
    where we can systematically analyse the impact of components and specific wordings
    associated with components across different ranking algorithms and backbones.
    We do this by assembling prompt variations (Section [3.2](#S3.SS2 "3.2 Building
    Prompt Variations ‣ 3 Methodology ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers")). Experiment configurations are then outlined in
    Section [3.3](#S3.SS3 "3.3 Experiments Settings ‣ 3 Methodology ‣ An Investigation
    of Prompt Variations for Zero-shot LLM-based Rankers").
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出了不同的策略来使用 LLM 作为排名器；它们不仅在排名算法和基础架构上有所不同，还在提供给 LLM 执行任务的具体措辞上有所不同。接下来，我们旨在收集文献中使用的原始提示，这些提示将作为我们实验的基础提示设置。然后，我们分析每个提示的个体特征，建立一个提示组件的分类法，并列出不同提示中每个组件的实例
    (第 [3.1](#S3.SS1 "3.1 提示收集和排名提示组件分类 ‣ 3 方法 ‣ 零-shot LLM 基础排名器的提示变体调查") 节)。有了这些信息，我们的目标是设计实验，以系统地分析不同排名算法和基础架构中组件及其相关措辞的影响。我们通过组装提示变体来实现这一目标
    (第 [3.2](#S3.SS2 "3.2 组建提示变体 ‣ 3 方法 ‣ 零-shot LLM 基础排名器的提示变体调查") 节)。实验配置随后在第 [3.3](#S3.SS3
    "3.3 实验设置 ‣ 3 方法 ‣ 零-shot LLM 基础排名器的提示变体调查") 节中进行了概述。
- en: 3.1 Prompts Collection and Taxonomy of Ranking Prompt Components
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 提示收集和排名提示组件分类
- en: We started by collecting the original prompts used in current zero-shot LLM-based
    rankers Zhuang et al. ([2023a](#bib.bib33)); Qin et al. ([2023](#bib.bib18));
    Ma et al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Tang et al. ([2023](#bib.bib23));
    Zhuang et al. ([2023b](#bib.bib36)); these are reported in Appendix [A](#A1 "Appendix
    A Original Prompts of LLM rankers ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers").
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先收集了当前零-shot LLM 基础排名器中使用的原始提示，这些排名器包括 Zhuang 等 ([2023a](#bib.bib33))；Qin
    等 ([2023](#bib.bib18))；Ma 等 ([2023](#bib.bib14))；Sun 等 ([2023](#bib.bib22))；Tang
    等 ([2023](#bib.bib23))；Zhuang 等 ([2023b](#bib.bib36))；这些内容在附录 [A](#A1 "附录 A LLM
    排名器的原始提示 ‣ 零-shot LLM 基础排名器的提示变体调查") 中有报告。
- en: 'Table 2: Type and wording alternatives of prompt components. None (Wording
    Alternative 0) refers to a prompt that does not use that component. A dash ($-$)
    means all prompts require that component and so the Wording Alternative 0 cannot
    be used (i.e. that component cannot be left empty). A checkmark means the prompt
    can be created without that component.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：提示组件的类型和措辞备选项。无 (措辞备选项 0) 指的是不使用该组件的提示。破折号（$-$）表示所有提示都需要该组件，因此措辞备选项 0 不能使用（即该组件不能留空）。勾号表示提示可以在没有该组件的情况下创建。
- en: '|  | Wording Alternatives |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | 措辞备选项 |'
- en: '| Component | Ranker | None (0) | 1 | 2 | 3 | 4 | 5 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 组件 | 排名器 | 无 (0) | 1 | 2 | 3 | 4 | 5 |'
- en: '| Task Instruction (TI) | pointwise | - | Does the passage answer the query?
    | Is this passage relevant to the query? | For the following query and document,
    judge whether they are relevant. | Judge the relevance between the query and the
    document. | - |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| 任务说明 (TI) | 点对点 | - | 段落是否回答了查询？ | 这个段落与查询相关吗？ | 对于以下查询和文档，判断它们是否相关。 | 判断查询与文档之间的相关性。
    | - |'
- en: '| pairwise | - | Given a query, which of the following two passages is more
    relevant to the query? | - | - | - |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 成对比较 | - | 给定一个查询，以下两个段落中哪个与查询更相关？ | - | - | - |  |'
- en: '| listwise | - | Rank the {num} passages based on their relevance to the search
    query. | Sort the Passages by their relevance to the Query. | I will provide you
    with {num} passages, each indicated by number identifier []. Rank the passages
    based on their relevance to query. | - | - |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 列表 | - | 根据它们与搜索查询的相关性对 {num} 个段落进行排名。 | 按照它们与查询的相关性对段落进行排序。 | 我将提供给你 {num}
    个段落，每个段落用编号 [] 标识。根据段落与查询的相关性对段落进行排名。 | - | - |'
- en: '| setwise | - | Which one is the most relevant to the query. | - | - | - |
    - |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| setwise | - | 哪一个与查询最相关。 | - | - | - | - |'
- en: '| Output Type (OT) | pointwise |  | Judge whether they are "Highly Relevant",
    "Somewhat Relevant", or "Not Relevant”. | From a scale of 0 to 4, judge the relevance.
    | Answer ’Yes’ or ’No’. | Answer True/False. | - |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 输出类型 (OT) | 逐点 |  | 判断它们是“高度相关”、“某种相关”还是“无关”。 | 从 0 到 4 的范围内判断相关性。 | 回答“是”或“否”。
    | 回答真/假。 | - |'
- en: '| pairwise | - | Output Passage A or Passage B. | - | - | - | - |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 逐对 | - | 输出段落 A 或段落 B。 | - | - | - | - |'
- en: '| listwise | - | Sorted Passages = [ | The passages should be listed in descending
    order using identifiers. The most relevant passages should be listed first. The
    output format should be [] >[], e.g., [1] >[2]. | - | - | - |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 列表 | - | 排序段落 = [ | 段落应该按降序列出，使用标识符。最相关的段落应排在前面。输出格式应为 [] >[]，例如 [1] >[2]。
    | - | - | - |'
- en: '| setwise | - | Output the passage label of the most relevant passage. | Generate
    the passage label. | Generate the passage label that is the most relevant to the
    query, then explain why you think this passage is the most relevant. | - | - |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| setwise | - | 输出最相关段落的标签。 | 生成段落标签。 | 生成最相关于查询的段落标签，然后解释你为什么认为这个段落是最相关的。
    | - | - |'
- en: '| Tone Words (TW) | All |  | You better get this right or you will be punished.
    | Only output the ranking results, do not say any word or explanation. | Please
    | Only | Must |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 语气词 (TW) | 全部 |  | 你最好把这个搞对，否则你会受到惩罚。 | 仅输出排名结果，不要说任何话或解释。 | 请 | 仅 | 必须 |'
- en: '| Role Playing (RP) | All |  | You are RankGPT, an intelligent assistant that
    can rank passages based on their relevancy to the query. | - | - | - | - |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 角色扮演 (RP) | 全部 |  | 你是 RankGPT，一个智能助手，可以根据与查询的相关性对段落进行排序。 | - | - | - | -
    |'
- en: After the original prompts were collected, we analysed the prompts to identify
    high level components that are present in at least one original prompt, along
    with the associated variants. We also augmented the list of components and variants
    with other wordings we devised to explore specific categories further, e.g., tone
    words (see below).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了原始提示后，我们分析了这些提示，以识别至少在一个原始提示中存在的高级组件及其相关变体。我们还通过我们设计的其他表述来扩展组件和变体的列表，以进一步探索特定类别，例如语气词（见下文）。
- en: 'The analysis revealed five components:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 分析揭示了五个组成部分：
- en: •
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evidence (EV): these are the query and the associated passages to rank.'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证据 (EV)：这些是查询和与之相关的需要排序的段落。
- en: •
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Task Instruction (TI): the instructions associated to the specific ranking
    strategy: these outline to the LLM the algorithmic steps to follow to produce
    a ranking. Example wordings include “which passage is more relevant” (pairwise)
    and “is this passage relevant to the query” (pointwise).'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 任务指令 (TI)：与特定排名策略相关的指令：这些指令概述了 LLM 需要遵循的算法步骤，以生成排名。示例表述包括“哪个段落更相关”（逐对）和“这个段落是否与查询相关”（逐点）。
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Output Type (OT): the instructions that specify the format of the output the
    LLM needs to generate. For example, for pointwise ranking the LLM could be instructed
    to generated a Yes/No or a True/False answer.'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出类型 (OT)：指定 LLM 需要生成的输出格式的指令。例如，对于逐点排名，LLM 可以被指示生成“是/否”或“真/假”答案。
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Tone Words (TW): words that express a positive, negative, or neutral connotation
    and that help express the attitude of the prompt author towards the ranking instruction,
    e.g., “please” or “you better get this right or you will be punished”.'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 语气词 (TW)：表达积极、消极或中性含义的词，帮助表达提示作者对排名指令的态度，例如“请”或“你最好把这个搞对，否则你会受到惩罚”。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Role Playing (RP): a description of the tool implemented by the LLM, used to
    make the LLM “impersonate” that role.'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 角色扮演 (RP)：描述由 LLM 实施的工具，用于使 LLM “扮演”该角色。
- en: Note that TI and OT largely determine the ranker family that is implemented,
    while TW and RP can be generally applied to any ranker family and EV are always
    present. Table [2](#S3.T2 "Table 2 ‣ 3.1 Prompts Collection and Taxonomy of Ranking
    Prompt Components ‣ 3 Methodology ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers") lists the options we consider in our experiments
    for each of these components (but EV, since it is always the same). Some options
    refer to variations found in an original ranking prompt formulation; we augmented
    these with wordings we devised to explore additional alternatives for some of
    the components.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，TI和OT在很大程度上决定了所实现的排名器家族，而TW和RP可以一般适用于任何排名器家族，EV始终存在。表[2](#S3.T2 "Table 2
    ‣ 3.1 Prompts Collection and Taxonomy of Ranking Prompt Components ‣ 3 Methodology
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers")列出了我们在实验中考虑的每个组件的选项（但EV，因为它始终相同）。一些选项涉及原始排名提示公式中发现的变体；我们用我们设计的措辞进行了扩展，以探索一些组件的额外替代方案。
- en: 'In addition to these components, our analysis of existing ranking prompts identified
    different approaches in the ordering of some of the components within the prompts;
    in particular:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些组件，我们对现有排名提示的分析发现了在提示中某些组件排序的不同方法；特别是：
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Evidence Ordering (EO): the relative ordering of the query and passage(s) provided
    to the LLM – whether the query is given first, followed by the passage(s), which
    we label as QF, or vice versa, passage(s) followed by the query (labelled PF).'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证据排序（EO）：提供给LLM的查询和段落的相对排序 - 查询是否首先给出，然后是段落，我们标记为QF，或者相反，段落在查询之前（标记为PF）。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Position of Evidence (PE): instruction to specify the position of the evidence
    in the prompt – at the beginning (B) or at the end of the prompt (E).'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 证据位置（PE）：指示在提示中证据的位置 - 在提示的开头（B）或在提示的末尾（E）。
- en: 3.2 Building Prompt Variations
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 构建提示变体
- en: To explore the role of the identified components, their interactions, and the
    effect of specific wordings used to instantiate each of the components, we setup
    a large scale experiment where prompts with unique combinations of these aspects
    are built.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索识别出的组件的作用、它们的相互作用以及用于实例化每个组件的特定措辞的效果，我们设立了一个大规模实验，其中构建了具有这些方面唯一组合的提示。
- en: 'To build prompts, we first consider the ordering options we identified, and
    build a prompt template for each combination: this gives rise to four prompt templates,
    shown in Table [3](#S3.T3 "Table 3 ‣ 3.2 Building Prompt Variations ‣ 3 Methodology
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建提示，我们首先考虑我们识别出的排序选项，并为每种组合构建一个提示模板：这产生了四个提示模板，如表[3](#S3.T3 "Table 3 ‣ 3.2
    Building Prompt Variations ‣ 3 Methodology ‣ An Investigation of Prompt Variations
    for Zero-shot LLM-based Rankers")所示。
- en: 'Table 3: Prompt templates that combine the five components with the four ordering
    options available. Q and P denote query text passage(s) text, respectively.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：结合五个组件与四种可用排序选项的提示模板。Q和P分别表示查询文本和段落文本。
- en: '| EO/PE | B | E |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| EO/PE | B | E |'
- en: '| QF | RP+ TI (Q) + P + TW+ TO | RP+ TW+ TO + TI (Q) + P |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| QF | RP+ TI (Q) + P + TW+ TO | RP+ TW+ TO + TI (Q) + P |'
- en: '| PF | RP+ P + TI (Q) + TW+ TO | RP+ TW+ TO + P + TI (Q) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| PF | RP+ P + TI (Q) + TW+ TO | RP+ TW+ TO + P + TI (Q) |'
- en: Then, for each template, we consider all possible instantiations. The number
    of variations of wordings for the task instruction (TI) and output type (OT) components
    differ across families of rankers – thus giving rise to different number of prompt
    instantiations across each family. In particular, for pointwise we consider 768
    unique prompts; for pairwise 48; for listwise 288; and for setwise 144.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，对于每个模板，我们考虑所有可能的实例化。任务指令（TI）和输出类型（OT）组件的措辞变体数量在排名器家族之间有所不同 - 从而导致每个家族中不同数量的提示实例化。特别是，对于点对点我们考虑768个独特的提示；对于对对我们考虑48个；对于列表我们考虑288个；对于集合我们考虑144个。
- en: 'Finally note that we did minor modifications to the original prompts from previous
    works to improve their consistency: for example, some prompts enclosed the query
    in quotes, while others appended the query after a colon; other differences included
    the presence of multiple line breaks. We settled on adapting a unique format for
    these aspects. These differences in prompt formatting resulted in non statistically
    significant differences in effectiveness (e.g., [Qin et al.](#bib.bib18)’s original
    prompt on COVID and Llama 3-8B backbone obtained an nDCG@10 of 0.8014, while our
    adjusted prompt lead to an nDCG@10 of 0.7966).'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意我们对之前工作的原始提示进行了微小的修改，以提高它们的一致性：例如，一些提示将查询用引号括起来，而其他提示则在冒号后附加查询；其他差异包括多个换行符的存在。我们决定对这些方面采用独特的格式。这些提示格式的差异导致了效果上的统计学上不显著的差异（例如，[Qin
    等人](#bib.bib18) 的 COVID 和 Llama 3-8B 主干网络的原始提示获得了 nDCG@10 为 0.8014，而我们调整后的提示则导致
    nDCG@10 为 0.7966）。
- en: 3.3 Experiments Settings
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 实验设置
- en: To experiment with the LLM rankers and the prompt variations, we setup a two
    stage ranking pipeline in line with previous work. For the first stage, we use
    the BM25 implementation from Pyserini Lin et al. ([2021](#bib.bib13)) to retrieve
    the top 100 documents for a query. For the second stage, we use the LLM ranker
    to re-rank these 100 documents.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实验 LLM 排序器和提示变体，我们建立了一个与之前工作一致的两阶段排序管道。在第一阶段，我们使用 Pyserini Lin 等人 ([2021](#bib.bib13))
    的 BM25 实现来检索查询的前 100 个文档。在第二阶段，我们使用 LLM 排序器对这 100 个文档进行重新排序。
- en: As LLMs backbones, we selected instruction-tuned checkpoints from the Flan-T5
    family Chung et al. ([2024](#bib.bib4)), Mistral 7B Jiang et al. ([2023](#bib.bib8)),
    and Llama3 8B AI@Meta ([2024](#bib.bib1)). These are popular and highly-performant
    open LLMs; we excluded close-source LLMs like GPT4 in our analysis because of
    the high costs associated with running our experiments on commercial APIs.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 LLM 主干网络，我们选择了 Flan-T5 家族 Chung 等人 ([2024](#bib.bib4))、Mistral 7B Jiang 等人
    ([2023](#bib.bib8)) 和 Llama3 8B AI@Meta ([2024](#bib.bib1)) 的指令调优检查点。这些是流行且高性能的开源
    LLM；由于在商业 API 上运行我们的实验的高成本，我们在分析中排除了像 GPT4 这样的闭源 LLM。
- en: 'For Flan-T5 we considered checkpoints of different sizes: Large (783M), XL
    (2.85B) and XXL (11.3B). This allowed us to explore the role of backbone size
    in instruction following ability and ranking. We did not considered extending
    the experiments to larger sizes of the other backbones, e.g., Llama3 70B, because
    of the high computational costs associated with doing this systematically.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Flan-T5，我们考虑了不同大小的检查点：Large (783M)、XL (2.85B) 和 XXL (11.3B)。这使我们能够探讨主干网络大小在指令跟随能力和排序中的作用。由于系统化处理这些问题的计算成本过高，我们没有考虑将实验扩展到其他主干网络的更大尺寸，例如
    Llama3 70B。
- en: Flan-T5 models have a maximum input length of 512 tokens. Upon analysis of queries
    and passages lengths used in our evaluation, methods such as setwise and listwise
    will exceed the length limits as they have multiple passages in the prompt, and
    amount all methods we investigated, listwise requires longest input length. Hence,
    to make comparisons between the different experimental conditions fair,we truncated
    inputs for all methods and LLM backbones to fit the 512 tokens limit. Based on
    our calculation of the longest non-query-and-passage prompt instructions, we set
    the query and passage lengths at 20 and 80 words, respectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Flan-T5 模型的最大输入长度为 512 个标记。在分析了我们评估中使用的查询和段落长度后，像 setwise 和 listwise 这样的办法将超出长度限制，因为它们在提示中包含多个段落，而所有我们调查的方法中，listwise
    需要最长的输入长度。因此，为了公平地比较不同的实验条件，我们将所有方法和 LLM 主干网络的输入截断到 512 个标记以内。根据我们对最长非查询和段落提示指令的计算，我们将查询和段落的长度分别设置为
    20 和 80 个词。
- en: 'We used three datasets from the ir_datasets MacAvaney et al. ([2021](#bib.bib15))
    python library: TREC Deep Learning (DL) 2019 and 2020 (43 and 48 queries respectively),
    and BeIR TREC COVID (50 queries) Voorhees et al. ([2021](#bib.bib26)); Thakur
    et al. ([2021](#bib.bib24)). We performed our analysis using nDCG@10, the primary
    metric across these datasets.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了来自 ir_datasets MacAvaney 等人 ([2021](#bib.bib15)) python 库的三个数据集：TREC 深度学习
    (DL) 2019 和 2020（分别为 43 和 48 个查询），以及 BeIR TREC COVID（50 个查询）Voorhees 等人 ([2021](#bib.bib26));
    Thakur 等人 ([2021](#bib.bib24))。我们使用 nDCG@10 进行分析，这是这些数据集的主要指标。
- en: 4 Results Analysis
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果分析
- en: We base our empirical analysis along six main lines of enquiry that help us
    investigate the impact prompts have on LLM rankers, including what makes ranking
    prompts effective, how rankers respond to different prompts, how ranking methods
    truly compare at the net of variations in prompt wording, and the impact of LLM
    backbones.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于六个主要调查方向进行实证分析，这些方向帮助我们探究提示对LLM排名器的影响，包括什么使排名提示有效、排名器如何响应不同的提示、排名方法在提示措辞变动中的真实比较，以及LLM主干网络的影响。
- en: 4.1 Are there better prompts?
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 是否有更好的提示？
- en: 'For each family of rankers, we compare the effectiveness of the original prompts
    with all other prompt variations. Results are displayed in Figure [1](#S4.F1 "Figure
    1 ‣ 4.1 Are there better prompts? ‣ 4 Results Analysis ‣ An Investigation of Prompt
    Variations for Zero-shot LLM-based Rankers"): for each family of rankers, the
    star symbols identify the original prompts, while the boxplot shows the distribution
    of effectiveness across all prompt variations. In all cases we find better prompts
    that can achieve statistically higher effectiveness than the original prompts,
    with the exception of listwise and pairwise on the COVID dataset when the Llama
    3 backbone is used. We also identify cases where the original prompt was the worst
    among those considered for the specific ranking family: this is the case for example
    for the listwise prompt evaluated on DL19 and DL 20 with the FlanT5-Large backbone.
    We present the actual nDCG@10 scores of original prompts and the best prompts
    in Appendix [C](#A3 "Appendix C Effectiveness of Original and Best Prompts ‣ An
    Investigation of Prompt Variations for Zero-shot LLM-based Rankers").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个排名器家族，我们将原始提示的效果与所有其他提示变体进行比较。结果显示在图[1](#S4.F1 "Figure 1 ‣ 4.1 Are there
    better prompts? ‣ 4 Results Analysis ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers")中：对于每个排名器家族，星号符号标识原始提示，而箱线图显示了所有提示变体的效果分布。在所有情况下，我们发现比原始提示更有效的提示，除了在使用Llama
    3主干网络的COVID数据集上的listwise和pairwise外。我们还识别出在特定排名家族中，原始提示在所考虑的提示中是最差的情况：例如，FlanT5-Large主干网络上评估的DL19和DL
    20的listwise提示就是这种情况。我们在附录[C](#A3 "Appendix C Effectiveness of Original and Best
    Prompts ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers")中展示了原始提示和最佳提示的实际nDCG@10分数。
- en: 'Figure 1: Effectiveness (nDCG@10) of zero-shot LLM-based rankers across ranking
    methods, prompt variations, LLM backbones and datasets. Effectiveness achieved
    by original prompts for each method is marked with a star, and annotated with
    value.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：零-shot LLM-based排名器在不同排名方法、提示变体、LLM主干网络和数据集上的效果（nDCG@10）。每种方法中原始提示的效果以星号标记，并注有数值。
- en: (a) DL 19
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (a) DL 19
- en: '![Refer to caption](img/bed431941901b3899adc3b38dd1dde8b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/bed431941901b3899adc3b38dd1dde8b.png)'
- en: (b) DL 20
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (b) DL 20
- en: '![Refer to caption](img/ba1d692fd0ce141d01d6800b255a0e90.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ba1d692fd0ce141d01d6800b255a0e90.png)'
- en: (c) COVID
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (c) COVID
- en: '![Refer to caption](img/c7818e9fdb9fdbef49391788cacfa07b.png)![Refer to caption](img/aa073e9a999b471ff28f427809fdeb3c.png)![Refer
    to caption](img/3851d873819c1b64e739352149927efe.png)![Refer to caption](img/bc0cc7a884231c1ff4ec5009f652f436.png)![Refer
    to caption](img/ff326f77291ea6d0b9b6116ac7f364f8.png)![Refer to caption](img/8462b330ce5697d584c6f3aebab43c9a.png)![Refer
    to caption](img/25b43892fdf762883422e09c3aa35f1e.png)![Refer to caption](img/4908776a2387cd9a279170ae8d62a497.png)![Refer
    to caption](img/fdba0576755a84e2b6f0ec3cb9e3bd0b.png)![Refer to caption](img/1674e1643cf2e5cc0bf6d49766d118f3.png)![Refer
    to caption](img/677b2500e8eea4fbf519f4f06ffa66ea.png)![Refer to caption](img/096235fdf56a2330319b030d08282620.png)![Refer
    to caption](img/d8aaeb1a4e3827cc9d66f4ad5a641ed1.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c7818e9fdb9fdbef49391788cacfa07b.png)![参考标题](img/aa073e9a999b471ff28f427809fdeb3c.png)![参考标题](img/3851d873819c1b64e739352149927efe.png)![参考标题](img/bc0cc7a884231c1ff4ec5009f652f436.png)![参考标题](img/ff326f77291ea6d0b9b6116ac7f364f8.png)![参考标题](img/8462b330ce5697d584c6f3aebab43c9a.png)![参考标题](img/25b43892fdf762883422e09c3aa35f1e.png)![参考标题](img/4908776a2387cd9a279170ae8d62a497.png)![参考标题](img/fdba0576755a84e2b6f0ec3cb9e3bd0b.png)![参考标题](img/1674e1643cf2e5cc0bf6d49766d118f3.png)![参考标题](img/677b2500e8eea4fbf519f4f06ffa66ea.png)![参考标题](img/096235fdf56a2330319b030d08282620.png)![参考标题](img/d8aaeb1a4e3827cc9d66f4ad5a641ed1.png)'
- en: 4.2 What are the characteristic of the best prompts?
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 最佳提示的特点是什么？
- en: Table [5](#A2.T5 "Table 5 ‣ Appendix B Best Prompt Variations ‣ An Investigation
    of Prompt Variations for Zero-shot LLM-based Rankers") in Appendix [B](#A2 "Appendix
    B Best Prompt Variations ‣ An Investigation of Prompt Variations for Zero-shot
    LLM-based Rankers") reports the optimal prompt components for various ranking
    methods and datasets. We identify notable patterns as below.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#A2.T5 "Table 5 ‣ Appendix B Best Prompt Variations ‣ An Investigation
    of Prompt Variations for Zero-shot LLM-based Rankers") 在附录 [B](#A2 "Appendix B
    Best Prompt Variations ‣ An Investigation of Prompt Variations for Zero-shot LLM-based
    Rankers") 中报告了各种排名方法和数据集的最佳提示组件。我们确定了以下显著模式。
- en: 'Task Instruction and Output Type: We analyse task instructions and output types
    together due to their inherent interrelation; the type of output largely depends
    on the ranking algorithm the task instruction implements. We analyse these separately
    for each ranking family because instructions and output types vary across ranking
    families. We do not analyse pairwise here as for this method there are no alternative
    choices for these wordings.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 任务指令与输出类型：由于任务指令与输出类型之间存在固有的相互关系，我们将它们一起分析；输出类型在很大程度上取决于任务指令所实施的排名算法。我们对每个排名系列分别分析这些内容，因为不同排名系列中的指令和输出类型有所不同。我们这里不分析成对比较方法，因为该方法对于这些措辞没有替代选择。
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pointwise ranking does not show a consistent optimal choice; however, Task
    Instruction #3 is most prevalent, while Output Type #2 (judging relevance on a
    numerical scale) is seldom optimal.'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '点对点排名并未显示出一致的最佳选择；然而，任务指令 #3 最为普遍，而输出类型 #2（基于数值尺度判断相关性）很少是最佳选择。'
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Listwise ranking shows that the best task instruction varies by dataset and
    LLM backbone. Output Type #2 appears in 80% of the most effective configurations.'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '列表排名显示，最佳任务指令因数据集和 LLM 主干而异。输出类型 #2 出现在 80% 的最有效配置中。'
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Setwise ranking studied with a single task instruction, finds Output Type #3
    (label and explain the most relevant passage) as the most common among top-performing
    prompts, appearing in 53% of cases.'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '使用单一任务指令的集合排名研究发现，输出类型 #3（标注并解释最相关的段落）在表现最好的提示中最为常见，出现在 53% 的情况下。'
- en: 'Tone Words: The inclusion of tone words does not present a clear trend, with
    the percentage of prompts achieving a higher effectiveness with no tone words
    or each of the five tone words distributed as follows: 18% (no tone words), 17%,
    18%, 20%, 15%, and 12%. This indicates a relatively uniform influence of tone
    words choice on prompt effectiveness. Furthermore, no consistent patterns were
    observed regarding the influence of LLM backbones or datasets on the effectiveness
    related to tone words. Notably, including a tone word in the prompt led to increased
    effectiveness in 82% of the cases, underscoring the potential benefit of tone
    words in enhancing prompt performance.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 语气词：语气词的使用并未呈现出明确的趋势，提示在没有语气词或每种语气词的效果百分比分布如下：18%（没有语气词）、17%、18%、20%、15% 和 12%。这表明语气词选择对提示效果的影响相对均匀。此外，未观察到关于
    LLM 主干或数据集对语气词相关效果的影响的稳定模式。值得注意的是，包含语气词的提示在 82% 的情况下效果有所提升，突显了语气词在提升提示性能方面的潜在好处。
- en: 'Role Playing: Role playing often leads to the best effectiveness for pointwise
    and pairwise prompts (80% and 66% of the cases, respectively), it has mixed effects
    for setwise (ipresent in the best prompt about half of the times), while it is
    not associated with the best effectiveness for listwise (13%). Overall, 55% of
    the prompts with highest effectiveness include role playing wording. Role playing
    was originally only used by one ranking method, RankGPT Sun et al. ([2023](#bib.bib22)),
    highlighting the difficulty in comparing different ranking methods if their prompt
    variations are not fairly explored.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 角色扮演：角色扮演通常会导致点对点和成对提示的最佳效果（分别为 80% 和 66% 的情况），但对集合排名效果混合（大约在最佳提示中出现一半的时间），而对于列表排名（13%）则不具备最佳效果。总体而言，55%
    的高效提示中包含了角色扮演措辞。角色扮演最初只被一个排名方法使用，即 RankGPT Sun et al. ([2023](#bib.bib22))，这突显了如果不同排名方法的提示变化未被公平探索，比较不同排名方法的难度。
- en: 'Evidence Ordering: For pointwise ranking, presenting passage text before the
    query text is preferred in 86% of top-performing prompts. The preference is less
    clear in other ranking types: 40% for pairwise; 40% for listwise; 46% for setwise
    where the best prompts have the passage text before the query text. Considering
    model backbones, Flan-T5 tends to perform best when presented with passage text
    before query text (66% of cases); while results are mixed for the other backbone
    models.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 证据排序：对于点对点排序，在86%的表现最佳的提示中，展示段落文本在查询文本之前更受偏好。在其他排序类型中，偏好不那么明确：对比排序为40%；列表排序为40%；集合排序为46%，其中最佳提示中段落文本在查询文本之前。考虑到模型骨干，Flan-T5在段落文本在查询文本之前时表现最佳（66%的情况）；而其他骨干模型的结果则混合不一。
- en: 'Position of Evidence: Among the best prompts, there tend to be an overall preference
    for prompts that provide the evidence at the beginning (before any other instruction):
    this is the case in 63% of the best prompts, with pointwise and listwise prompts
    exhibiting more often this pattern (73% and 67% respectively). Across all datasets,
    most best prompts for the FlanT5-XXL backbone have evidence at the beginning.
    This is also the case for Llama3-8B for DL19 and COVID and for Mistral-7B for
    COVID.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 证据的位置：在最佳提示中，通常倾向于将证据放在开始部分（在其他指示之前）：在最佳提示中，这种情况出现了63%，其中点对点和列表提示更常表现出这种模式（分别为73%和67%）。在所有数据集中，FlanT5-XXL骨干模型的大多数最佳提示都将证据放在开始部分。Llama3-8B在DL19和COVID以及Mistral-7B在COVID的情况也是如此。
- en: Summary. The analysis of the prompt templates associated with the prompts leading
    to the best effectiveness across all ranking families did not highlight any specific
    prompt wording combination that is more conducive of best effectiveness than others
    across all datasets and backbones – though we found that tone words and role playing
    are frequent among most of the best prompts. This analysis is further extended
    in Section [4.4](#S4.SS4 "4.4 Are ranking methods stable? ‣ 4 Results Analysis
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers") where
    the stability of rankers across prompts variations is considered.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。与所有排序家族中效果最佳的提示相关的提示模板分析未突出任何特定的提示措辞组合，它们在所有数据集和骨干模型中比其他组合更具有效性——尽管我们发现语气词和角色扮演在大多数最佳提示中较为频繁。这一分析在第[4.4](#S4.SS4
    "4.4 排序方法是否稳定？ ‣ 4 结果分析 ‣ 零样本LLM基础排名器提示变体调查")节中进一步扩展，考虑了提示变体下排序器的稳定性。
- en: 4.3 Which ranking method is most effective?
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 哪种排序方法最有效？
- en: Before our analysis of prompt variations, the only comparison of ranking methods
    across the four families within a consistent setting of datasets and backbone
    was provided by Zhuang et al. ([2023b](#bib.bib36)). According to their results,
    the best performing rankers were setwise and pairwise (depending on dataset and
    backbone), followed by listwise and then pointwise, which were distinguishably
    worse.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对提示变体的分析之前，Zhuang等人（[2023b](#bib.bib36)）提供了在一致的数据集和骨干模型设置下，四种排序方法的唯一比较。根据他们的结果，表现最佳的排序器是基于集合和对比的（取决于数据集和骨干模型），其次是基于列表的，然后是基于点的，这些方法的表现明显较差。
- en: 'Our analysis of prompt variations reveals a somewhat different picture. Consider
    the top results for each ranking family across datasets and LLM backbone reported
    in Figure [1](#S4.F1 "Figure 1 ‣ 4.1 Are there better prompts? ‣ 4 Results Analysis
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers"). Similarly
    to previous findings, we also observe that overall pairwise and setwise methods
    deliver the best results. However, we find that pointwise can be as competitive
    as these previous methods if instructed with specific prompts. This is the case
    throughout all datasets and LLM backbones, with the exception of Llama 3\. For
    Llama 3 on DL datasets, in fact, we observe that pointwise significantly underperforms
    other methods. At the same time, we also observe that there are instances in which
    pointwise ranking can far exceed other methods: this is the case on DL19 when
    using the FLanT5-Large and XL backbones (best pointwise nDCG@10 respectively 0.6918
    and 0.7010, statistically significantly outperforming the other methods in most
    cases), and on DL20 when using Mistral-7B (best pointwise 0.6486).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对提示变化的分析揭示了略有不同的情况。考虑每个排名系列在数据集和LLM基础模型上的最佳结果，如图[1](#S4.F1 "Figure 1 ‣ 4.1
    Are there better prompts? ‣ 4 Results Analysis ‣ An Investigation of Prompt Variations
    for Zero-shot LLM-based Rankers")所示。与之前的发现类似，我们也观察到总体上，成对和集合方法提供了最佳结果。然而，我们发现如果用特定提示指导，逐点方法可以与这些之前的方法一样具竞争力。这种情况在所有数据集和LLM基础模型中都存在，除了Llama
    3。对于DL数据集上的Llama 3，我们实际上观察到逐点方法显著逊色于其他方法。同时，我们还观察到逐点排名有时可以远远超过其他方法：例如，在使用FLanT5-Large和XL基础模型时，在DL19上的最佳逐点nDCG@10分别为0.6918和0.7010，在大多数情况下显著优于其他方法，以及在DL20使用Mistral-7B时（最佳逐点为0.6486）。
- en: 4.4 Are ranking methods stable?
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 排名方法稳定吗？
- en: 'We consider the four ranking families independently, and study the variance
    of their effectiveness across all prompt variations for that family. Results are
    displayed in Figure [1](#S4.F1 "Figure 1 ‣ 4.1 Are there better prompts? ‣ 4 Results
    Analysis ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers").
    We observe that pointwise methods display the largest variability in effectiveness
    due to prompt variations, with some prompt variations delivering poor effectiveness.
    Setwise and pairwise instead do better, displaying lower variability: setwise
    achieves this across all backbones and datasets investigated, while pairwise displays
    larger variability in specific conditions, e.g., when using Mistral, and for the
    COVID dataset when using Llama3.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们独立考虑四个排名系列，并研究这些系列在所有提示变化下的有效性差异。结果如图[1](#S4.F1 "Figure 1 ‣ 4.1 Are there
    better prompts? ‣ 4 Results Analysis ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers")所示。我们观察到，由于提示变化，逐点方法的有效性波动最大，有些提示变化导致效果较差。相反，集合和成对方法表现更好，波动较小：集合方法在所有基础模型和数据集中都表现出这种特性，而成对方法在特定条件下表现出更大的波动，例如，使用Mistral时，以及在使用Llama3时的COVID数据集。
- en: 'Two key insights arise: 1) ranking algorithms are susceptible to prompt variations
    and may exhibit wide-ranging performance variations, and 2) different ranking
    algorithms have varying sensitivity to prompts, indicating the potential for some
    to mitigate the impact of these variations. This suggests more comprehensive prompting
    optimisation, e.g., via self-prompting, may further improve effectiveness across
    all rankers.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个关键见解：1) 排名算法对提示的变化敏感，可能表现出广泛的性能波动，2) 不同的排名算法对提示的敏感性各不相同，这表明某些算法可能有减轻这些变化影响的潜力。这表明更全面的提示优化，例如通过自我提示，可能会进一步提高所有排名算法的有效性。
- en: 4.5 Does the LLM size matter?
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 LLM的规模重要吗？
- en: We answer this question by focusing on the FlanT5 models only, which come in
    three different sizes (first three rows of plots in Figure [1](#S4.F1 "Figure
    1 ‣ 4.1 Are there better prompts? ‣ 4 Results Analysis ‣ An Investigation of Prompt
    Variations for Zero-shot LLM-based Rankers")). We observe that in general larger
    models deliver higher effectiveness and reduced variations across the board. However,
    the pointwise family represents an exception to this. Improvements are observed
    when passing from FlanT5-large to FlanT5-XL. However when using FlanT5-XXL we
    observe both decreased effectiveness and large variance in effectiveness across
    the prompt variations.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过专注于 FlanT5 模型来回答这个问题，这些模型有三种不同的尺寸（图 [1](#S4.F1 "Figure 1 ‣ 4.1 Are there
    better prompts? ‣ 4 Results Analysis ‣ An Investigation of Prompt Variations for
    Zero-shot LLM-based Rankers") 的前三行图）。我们观察到，一般来说，较大的模型提供了更高的效果和更少的变化。然而，pointwise
    家族是一个例外。从 FlanT5-large 过渡到 FlanT5-XL 观察到有改进。然而，当使用 FlanT5-XXL 时，我们观察到效果降低且在提示变体之间的效果差异较大。
- en: 4.6 Does the LLM backbone matter?
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 LLM 主干重要吗？
- en: 'We answer this question by comparing three backbones across the results from
    Figure [1](#S4.F1 "Figure 1 ‣ 4.1 Are there better prompts? ‣ 4 Results Analysis
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers")): FlanT5-XXL
    (11.3B parameters), Mistral-7B and Llama3-8B: the last two are of comparable size,
    while the first has approximately 40-60% more parameters. We observe that generally
    Llama3 and FlanT5 outperform Mistral-based rankers. Llama3 and FlanT5 have overall
    similar effectiveness, though on the COVID dataset Llama3-based rankers consistently
    outperform those with FlanT5\. We also observe that Mistral-based rankers exhibit
    larger variance in effectiveness due to prompt variations than rankers based on
    the other backbones.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过比较三个主干在图 [1](#S4.F1 "Figure 1 ‣ 4.1 Are there better prompts? ‣ 4 Results
    Analysis ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers")
    的结果来回答这个问题：FlanT5-XXL（11.3B 参数）、Mistral-7B 和 Llama3-8B：后两个主干的大小相当，而第一个主干的参数大约多出
    40-60%。我们观察到，通常 Llama3 和 FlanT5 的表现优于基于 Mistral 的排序器。Llama3 和 FlanT5 在整体效果上相似，尽管在
    COVID 数据集上，Llama3 基于的排序器始终优于 FlanT5。我们还观察到，基于 Mistral 的排序器由于提示变体表现出比基于其他主干的排序器更大的效果差异。
- en: 5 Conclusions
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'Recent works have shown the promise of using generative LLMs to implement effective
    zero-shot re-rankers. Although distinctions among various methods primarily arise
    from the underlying ranking algorithms, further differences emerge based on the
    characteristics of the prompts employed to implement these algorithms. These characteristics
    are not associated with the actual ranking algorithm: e.g., they may be wordings
    related to a role-playing strategy, or tone words. In this paper, we analysed
    these prompts and mapped prompt wordings into a set of components to form prompt
    templates that allowed us to better understand the content of these prompts. We
    then performed a systematic analysis of prompt variations across different types
    of zero-shot LLM rankers. Our analysis revealed that ranking effectiveness varies
    considerably across different implementations of prompt components. Optimal prompt
    wording showed variability depending on the ranking method, dataset, and LLM backbone
    employed, suggesting that automatic prompt optimization, tailored to specific
    ranking methods and datasets, may be more effective than manual prompt engineering
    for optimizing ranking performance.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究显示了使用生成型 LLM 实现有效零-shot 重新排序器的潜力。虽然各种方法之间的区别主要来自于基础的排序算法，但进一步的差异基于用于实现这些算法的提示的特征。这些特征与实际的排序算法无关：例如，它们可能是与角色扮演策略相关的措辞，或语气词。在本文中，我们分析了这些提示，并将提示措辞映射到一组组件中，形成了提示模板，这使我们能够更好地理解这些提示的内容。随后，我们对不同类型的零-shot
    LLM 排序器的提示变体进行了系统分析。我们的分析揭示了不同提示组件的实现方式在排序效果上存在显著差异。最佳的提示措辞在排序方法、数据集和 LLM 主干的不同下表现出变化，这表明自动化提示优化，针对特定的排序方法和数据集，可能比手动提示工程更有效于优化排序性能。
- en: We make code, runs and analysis available at [https://github.com/ielab/zeroshot-rankers-prompt-variations](https://github.com/ielab/zeroshot-rankers-prompt-variations).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [https://github.com/ielab/zeroshot-rankers-prompt-variations](https://github.com/ielab/zeroshot-rankers-prompt-variations)
    提供了代码、运行结果和分析。
- en: 6 Limitations
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 限制
- en: Latency is an important factor affective the deployment of rankers in production.
    In this paper, we did not consider query latency as one of the dimensions of analysis
    and comparison because our focus was on varying the prompts to measure ranking
    effectiveness. [Zhuang et al.](#bib.bib36) provided a first insight into comparing
    latency among the zero-shot LLM rankers we considered. Latency of the method and
    prompts we considered in our analysis would be similar to that performed there;
    however we note that some of the prompts we consider are longer than others (e.g.
    if the role playing component is added to prompts) – longer prompts result in
    increased query latency.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 延迟是影响排名器生产环境部署的重要因素。在本文中，我们没有将查询延迟考虑为分析和比较的维度，因为我们的重点是变化提示以测量排名效果。[Zhuang et
    al.](#bib.bib36)提供了对我们考虑的零样本LLM排名器之间延迟比较的初步见解。我们分析中考虑的方法和提示的延迟将与那里执行的类似；然而，我们注意到我们考虑的某些提示比其他提示更长（例如，如果角色扮演组件添加到提示中）——较长的提示会导致查询延迟增加。
- en: Overall, our experiments considered 1,248 prompt variations. While this is a
    large number, many more prompt variations could have been designed and investigated.
    However, sensibly increasing the number of prompt variations in our experiments
    would have been infeasible as it would have exceeded our compute budget. The current
    set of experiments took in excess of 12,400 GPU-hours to execute. In particular,
    varying prompt for the pairwise ranking methods results in large computational
    requirements compared to other methods because of the extensive number of pairwise
    computations (and thus LLM inferences) required by this method to answer a query.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们的实验考虑了1248种提示变体。虽然这是一个大数字，但还可以设计和研究更多的提示变体。然而，合理增加实验中的提示变体数量将不可行，因为这将超过我们的计算预算。目前的实验集执行了超过12400个GPU小时。特别地，与其他方法相比，为配对排名方法变化提示会导致较大的计算需求，因为这种方法需要进行大量的配对计算（因此需要大量的LLM推理）来回答一个查询。
- en: 'In our analysis we considered instruction-tuned checkpoints from three LLM
    backbones: FlanT5 (in three different sizes), Mistral and Llama 3\. A wider range
    of LLMs could have been considered, importantly including OpenAI’s GPT models
    such as GPT3.5 and GPT4\. GPT3.5 for example was found more effective than Llama
    2 and Vicuna based LLM rankers in a limited set of experiments in previous work Zhuang
    et al. ([2023b](#bib.bib36)). We were however restricted to using non-commercial
    models because of the high costs involved in performing the large number of experiments
    we considered with commercial APIs. For example, using the cost estimates from
    [Zhuang et al.](#bib.bib36), executing our experiments for pairwise, listwise
    and setwise across all considered prompts using GPT3.5 alone would have costed
    approximately USD $5,000.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的分析中，我们考虑了来自三种LLM主干模型的指令调优检查点：FlanT5（有三种不同的尺寸）、Mistral 和 Llama 3。可以考虑更广泛的LLM，尤其是包括OpenAI的GPT模型，如GPT3.5和GPT4。例如，GPT3.5在之前的工作Zhuang
    et al. ([2023b](#bib.bib36))中的有限实验中被发现比Llama 2和Vicuna基础的LLM排名器更有效。然而，由于使用商业API进行大量实验的高成本，我们被限制使用非商业模型。例如，使用[Zhuang
    et al.](#bib.bib36)的成本估算，单独使用GPT3.5进行我们所有考虑的对比、列表和集合实验的费用大约为5000美元。
- en: 7 Ethical considerations
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 伦理考虑
- en: 'In our experiments we considered a large number of prompt variations: for pointwise
    we consider 768 unique prompts; for pairwise 48; for listwise 288; and for setwise
    144 – for a total of 1,248 prompts. We ran these prompts across three datasets,
    and using five different LLMs with up to 11B parameters. The execution of these
    experiments required a large amount of compute power; we ran experiments across
    three clusters: one with Nvidia A100 GPUs, another with Nvidia H100 GPUs, and
    a smaller one with Nvidia A6000 GPUs. Although we only considered zero-shot LLM
    ranking approaches and thus did not conduct any LLM training, we recognize that
    our experiments may have still consumed substantial energy, thereby contributing
    to CO2 emissions Scells et al. ([2022](#bib.bib21)) and water consumption Zuccon
    et al. ([2023](#bib.bib38)).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们考虑了大量的提示变体：点对点我们考虑了768个独特的提示；对对对我们考虑了48个；对列表我们考虑了288个；对集合我们考虑了144个——总共1,248个提示。我们在三个数据集上运行了这些提示，并使用了五种不同的LLM，参数多达11B。这些实验的执行需要大量计算能力；我们在三个集群中进行了实验：一个使用Nvidia
    A100 GPUs，另一个使用Nvidia H100 GPUs，还有一个较小的集群使用Nvidia A6000 GPUs。虽然我们只考虑了零-shot LLM
    排名方法，因此没有进行任何LLM训练，但我们认识到我们的实验可能仍然消耗了大量能源，从而导致CO2 排放 [Scells et al. (2022)](#bib.bib21)
    和水资源消耗 [Zuccon et al. (2023)](#bib.bib38)。
- en: The publicly available LLMs we used may have been trained with content that
    contains several types of societal biases Gallegos et al. ([2024](#bib.bib6))
    – and these biases may permeate in the rankings our zero-shot LLM rankers produced.
    Future research could explore ways to mitigate these biases prompt engineering.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的公开可用LLMs可能已经用包含多种社会偏见的内容进行了训练 [Gallegos et al. (2024)](#bib.bib6)——这些偏见可能会渗透到我们零-shot
    LLM 排名器生成的排名中。未来的研究可以探索缓解这些偏见的方法，提示工程。
- en: References
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: AI@Meta (2024) AI@Meta. 2024. [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta (2024) AI@Meta。2024年。[Llama 3 模型卡](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'BehnamGhader et al. (2024) Parishad BehnamGhader, Vaibhav Adlakha, Marius Mosbach,
    Dzmitry Bahdanau, Nicolas Chapados, and Siva Reddy. 2024. [Llm2vec: Large language
    models are secretly powerful text encoders](https://arxiv.org/abs/2404.05961).
    *Preprint*, arXiv:2404.05961.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BehnamGhader 等人 (2024) 帕里沙德·贝赫南·加德, 维布哈夫·阿德拉卡, 马里乌斯·莫斯巴赫, 德兹米特里·巴赫达瑙, 尼古拉斯·查帕多斯,
    和 希瓦·雷迪。2024年。[Llm2vec: 大型语言模型秘密强大的文本编码器](https://arxiv.org/abs/2404.05961)。*预印本*，arXiv:2404.05961。'
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. 2020. Language models are few-shot learners. *Advances in neural
    information processing systems*, 33:1877–1901.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020) 汤姆·布朗, 本杰明·曼, 尼克·赖德, 梅拉尼·苏比亚, 贾瑞德·D·卡普兰, 普拉弗拉·达里瓦尔, 阿尔文·尼拉坎坦,
    普拉纳夫·夏姆, 吉里什·萨斯特里, 阿曼达·阿斯克尔 等。2020年。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901。
- en: Chung et al. (2024) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2024. Scaling instruction-finetuned language models. *Journal of Machine Learning
    Research*, 25(70):1–53.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等人 (2024) 衡温·钟, 乐·侯, 沙恩·朗普雷, 巴雷特·佐普, 易·泰, 威廉·费德斯, 云轩·李, 薛志·王, 穆斯塔法·德赫加尼,
    西达尔塔·布拉赫玛 等。2024年。规模化指令微调语言模型。*机器学习研究杂志*, 25(70):1–53。
- en: Fan et al. (2023) Wenqi Fan, Zihuai Zhao, Jiatong Li, Yunqing Liu, Xiaowei Mei,
    Yiqi Wang, Jiliang Tang, and Qing Li. 2023. Recommender systems in the era of
    large language models (llms). *arXiv preprint arXiv:2307.02046*.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等人 (2023) 文琪·范, 梓怀·赵, 嘉彤·李, 云青·刘, 小伟·梅, 一琪·王, 继良·唐, 和 青·李。2023年。大语言模型时代的推荐系统
    (LLMs)。*arXiv 预印本 arXiv:2307.02046*。
- en: 'Gallegos et al. (2024) Isabel O Gallegos, Ryan A Rossi, Joe Barrow, Md Mehrab
    Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K
    Ahmed. 2024. Bias and fairness in large language models: A survey. *Computational
    Linguistics*, pages 1–79.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gallegos 等人 (2024) 伊莎贝尔·O·加列戈斯, 瑞安·A·罗西, 乔·巴罗, Md·梅赫拉布·坦金, 成柱·金, 弗兰克·德尔农库尔,
    仝·余, 瑞伊·张, 和 内斯林·K·艾哈迈德。2024年。大型语言模型中的偏见与公平性：一项综述。*计算语言学*，第1–79页。
- en: Guo et al. (2023) Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song,
    Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu Yang. 2023. Connecting large language
    models with evolutionary algorithms yields powerful prompt optimizers. *arXiv
    preprint arXiv:2309.08532*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等人 (2023) 祁岩·郭, 瑞·王, 俊亮·郭, 贝·李, 凯涛·宋, 许·谭, 国庆·刘, 江·边, 和 宇久·杨。2023年。将大型语言模型与进化算法结合产生强大的提示优化器。*arXiv
    预印本 arXiv:2309.08532*。
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等（2023）Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
    Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,
    Guillaume Lample, Lucile Saulnier 等。2023。Mistral 7b。*arXiv 预印本 arXiv:2310.06825*。
- en: Kamruzzaman and Kim (2024) Mahammed Kamruzzaman and Gene Louis Kim. 2024. Prompting
    techniques for reducing social bias in llms through system 1 and system 2 cognitive
    processes. *arXiv preprint arXiv:2404.17218*.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kamruzzaman 和 Kim（2024）Mahammed Kamruzzaman 和 Gene Louis Kim。2024。通过系统 1 和系统
    2 认知过程减少大型语言模型中的社会偏见的提示技术。*arXiv 预印本 arXiv:2404.17218*。
- en: Kim et al. (2023) Joonghoon Kim, Saeran Park, Kiyoon Jeong, Sangmin Lee, Seung Hun
    Han, Jiyoon Lee, and Pilsung Kang. 2023. Which is better? exploring prompting
    strategy for llm-based metrics. *arXiv preprint arXiv:2311.03754*.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2023）Joonghoon Kim, Saeran Park, Kiyoon Jeong, Sangmin Lee, Seung Hun
    Han, Jiyoon Lee 和 Pilsung Kang。2023。哪个更好？探索基于 LLM 的度量的提示策略。*arXiv 预印本 arXiv:2311.03754*。
- en: 'Lee et al. (2024) Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer,
    Jeremy R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai
    Meher Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya
    Kusupati, Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang, and Iftekhar
    Naim. 2024. [Gecko: Versatile text embeddings distilled from large language models](https://arxiv.org/abs/2403.20327).
    *Preprint*, arXiv:2403.20327.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等（2024）Jinhyuk Lee, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Jeremy
    R. Cole, Kai Hui, Michael Boratko, Rajvi Kapadia, Wen Ding, Yi Luan, Sai Meher
    Karthik Duddu, Gustavo Hernandez Abrego, Weiqiang Shi, Nithi Gupta, Aditya Kusupati,
    Prateek Jain, Siddhartha Reddy Jonnalagadda, Ming-Wei Chang 和 Iftekhar Naim。2024。[Gecko:
    从大型语言模型中提炼的多功能文本嵌入](https://arxiv.org/abs/2403.20327)。*预印本*，arXiv:2403.20327。'
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2022）Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara
    Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar
    等。2022。语言模型的整体评估。*arXiv 预印本 arXiv:2211.09110*。
- en: 'Lin et al. (2021) Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang,
    Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A python toolkit for reproducible
    information retrieval research with sparse and dense representations. In *Proceedings
    of the 44th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*, pages 2356–2362.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2021）Jimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep
    和 Rodrigo Nogueira。2021。Pyserini：一个用于可重复信息检索研究的 Python 工具包，支持稀疏和密集表示。在 *第44届国际
    ACM SIGIR 信息检索研究与发展会议* 上发表，页码 2356–2362。
- en: Ma et al. (2023) Xueguang Ma, Xinyu Zhang, Ronak Pradeep, and Jimmy Lin. 2023.
    Zero-shot listwise document reranking with a large language model. *arXiv preprint
    arXiv:2305.02156*.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ma 等（2023）Xueguang Ma, Xinyu Zhang, Ronak Pradeep 和 Jimmy Lin。2023。利用大语言模型进行零样本列表文档重新排序。*arXiv
    预印本 arXiv:2305.02156*。
- en: MacAvaney et al. (2021) Sean MacAvaney, Andrew Yates, Sergey Feldman, Doug Downey,
    Arman Cohan, and Nazli Goharian. 2021. Simplified data wrangling with ir_datasets.
    In *SIGIR*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacAvaney 等（2021）Sean MacAvaney, Andrew Yates, Sergey Feldman, Doug Downey,
    Arman Cohan 和 Nazli Goharian。2021。使用 ir_datasets 简化数据处理。在 *SIGIR* 上发表。
- en: Nogueira et al. (2020) Rodrigo Nogueira, Zhiying Jiang, and Jimmy Lin. 2020.
    Document ranking with a pretrained sequence-to-sequence model. *arXiv preprint
    arXiv:2003.06713*.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nogueira 等（2020）Rodrigo Nogueira, Zhiying Jiang 和 Jimmy Lin。2020。使用预训练序列到序列模型进行文档排名。*arXiv
    预印本 arXiv:2003.06713*。
- en: 'Pradeep et al. (2023) Ronak Pradeep, Sahel Sharifymoghaddam, and Jimmy Lin.
    2023. Rankvicuna: Zero-shot listwise document reranking with open-source large
    language models. *arXiv preprint arXiv:2309.15088*.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pradeep 等（2023）Ronak Pradeep, Sahel Sharifymoghaddam 和 Jimmy Lin。2023。Rankvicuna：使用开源大型语言模型进行零样本列表文档重新排序。*arXiv
    预印本 arXiv:2309.15088*。
- en: Qin et al. (2023) Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu,
    Jiaming Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang, et al. 2023.
    Large language models are effective text rankers with pairwise ranking prompting.
    *arXiv preprint arXiv:2306.17563*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等（2023）Zhen Qin, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming
    Shen, Tianqi Liu, Jialu Liu, Donald Metzler, Xuanhui Wang 等。2023。大型语言模型是有效的文本排序器，采用成对排序提示。*arXiv
    预印本 arXiv:2306.17563*。
- en: Sabbatella et al. (2024) Antonio Sabbatella, Andrea Ponti, Ilaria Giordani,
    Antonio Candelieri, and Francesco Archetti. 2024. Prompt optimization in large
    language models. *Mathematics*, 12(6):929.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabbatella等人（2024）Antonio Sabbatella, Andrea Ponti, Ilaria Giordani, Antonio
    Candelieri, 和 Francesco Archetti。2024年。大型语言模型中的提示优化。*数学*，12(6):929。
- en: Sachan et al. (2022) Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen
    Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving
    passage retrieval with zero-shot question generation. *arXiv preprint arXiv:2204.07496*.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sachan等人（2022）Devendra Singh Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan,
    Wen-tau Yih, Joelle Pineau, 和 Luke Zettlemoyer。2022年。通过零样本问题生成改进段落检索。*arXiv预印本
    arXiv:2204.07496*。
- en: 'Scells et al. (2022) Harrisen Scells, Shengyao Zhuang, and Guido Zuccon. 2022.
    Reduce, reuse, recycle: Green information retrieval research. In *Proceedings
    of the 45th International ACM SIGIR Conference on Research and Development in
    Information Retrieval*, pages 2825–2837.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scells等人（2022）Harrisen Scells, Shengyao Zhuang, 和 Guido Zuccon。2022年。减少、重用、回收：绿色信息检索研究。在*第45届国际ACM
    SIGIR信息检索研究与发展会议论文集*，第2825–2837页。
- en: Sun et al. (2023) Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin,
    and Zhaochun Ren. 2023. Is chatgpt good at search? investigating large language
    models as re-ranking agent. *arXiv preprint arXiv:2304.09542*.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun等人（2023）Weiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie Ren, Dawei Yin, 和 Zhaochun
    Ren。2023年。ChatGPT在搜索中表现如何？调查大型语言模型作为重新排序代理的表现。*arXiv预印本 arXiv:2304.09542*。
- en: 'Tang et al. (2023) Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan
    Ture. 2023. Found in the middle: Permutation self-consistency improves listwise
    ranking in large language models. *arXiv preprint arXiv:2310.07712*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2023）Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, 和 Ferhan Ture。2023年。发现于中间：排列自一致性改善大型语言模型中的列表排名。*arXiv预印本
    arXiv:2310.07712*。
- en: 'Thakur et al. (2021) Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek
    Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot
    evaluation of information retrieval models. *arXiv preprint arXiv:2104.08663*.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thakur等人（2021）Nandan Thakur, Nils Reimers, Andreas Rücklé, Abhishek Srivastava,
    和 Iryna Gurevych。2021年。Beir：一个用于零样本评估信息检索模型的异构基准。*arXiv预印本 arXiv:2104.08663*。
- en: Thomas et al. (2023) Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar
    Mitra. 2023. Large language models can accurately predict searcher preferences.
    *arXiv preprint arXiv:2309.10621*.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thomas等人（2023）Paul Thomas, Seth Spielman, Nick Craswell, 和 Bhaskar Mitra。2023年。大型语言模型可以准确预测搜索者偏好。*arXiv预印本
    arXiv:2309.10621*。
- en: 'Voorhees et al. (2021) Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman,
    William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, and Lucy Lu Wang. 2021.
    Trec-covid: constructing a pandemic information retrieval test collection. In
    *ACM SIGIR Forum*, volume 54, pages 1–12\. ACM New York, NY, USA.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voorhees等人（2021）Ellen Voorhees, Tasmeer Alam, Steven Bedrick, Dina Demner-Fushman,
    William R Hersh, Kyle Lo, Kirk Roberts, Ian Soboroff, 和 Lucy Lu Wang。2021年。Trec-covid：构建一个流行病信息检索测试集。在*ACM
    SIGIR论坛*，第54卷，第1–12页。ACM纽约，NY，美国。
- en: Wang et al. (2024a) Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun
    Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. 2024a. [Text embeddings by weakly-supervised
    contrastive pre-training](https://arxiv.org/abs/2212.03533). *Preprint*, arXiv:2212.03533.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2024a）Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang,
    Daxin Jiang, Rangan Majumder, 和 Furu Wei。2024a年。[弱监督对比预训练的文本嵌入](https://arxiv.org/abs/2212.03533)。*预印本*，arXiv:2212.03533。
- en: Wang et al. (2024b) Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan
    Majumder, and Furu Wei. 2024b. [Improving text embeddings with large language
    models](https://arxiv.org/abs/2401.00368). *Preprint*, arXiv:2401.00368.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2024b）Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder,
    和 Furu Wei。2024b年。[通过大型语言模型改进文本嵌入](https://arxiv.org/abs/2401.00368)。*预印本*，arXiv:2401.00368。
- en: Wang et al. (2023) Shuai Wang, Harrisen Scells, Bevan Koopman, and Guido Zuccon.
    2023. Can chatgpt write a good boolean query for systematic review literature
    search? In *Proceedings of the 46th International ACM SIGIR Conference on Research
    and Development in Information Retrieval*, pages 1426–1436.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2023）Shuai Wang, Harrisen Scells, Bevan Koopman, 和 Guido Zuccon。2023年。ChatGPT能否为系统评价文献检索编写有效的布尔查询？在*第46届国际ACM
    SIGIR信息检索研究与发展会议论文集*，第1426–1436页。
- en: White et al. (2023) Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos
    Olea, Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt.
    2023. A prompt pattern catalog to enhance prompt engineering with chatgpt. *arXiv
    preprint arXiv:2302.11382*.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White等人（2023）Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea,
    Henry Gilbert, Ashraf Elnashar, Jesse Spencer-Smith, 和 Douglas C Schmidt。2023年。增强ChatGPT的提示工程的提示模式目录。*arXiv预印本
    arXiv:2302.11382*。
- en: Yang et al. (2023) Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V
    Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. *arXiv
    preprint arXiv:2309.03409*.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2023) Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc
    V Le, Denny Zhou, and Xinyun Chen. 2023. 大语言模型作为优化器。*arXiv 预印本 arXiv:2309.03409*。
- en: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al.
    2023. A survey of large language models. *arXiv preprint arXiv:2303.18223*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei
    Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, 等. 2023.
    大语言模型调查。*arXiv 预印本 arXiv:2303.18223*。
- en: 'Zhuang et al. (2023a) Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan,
    Xuanhui Wang, and Michael Berdersky. 2023a. Beyond yes and no: Improving zero-shot
    llm rankers via scoring fine-grained relevance labels. *arXiv preprint arXiv:2310.14122*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2023a) Honglei Zhuang, Zhen Qin, Kai Hui, Junru Wu, Le Yan, Xuanhui
    Wang, and Michael Berdersky. 2023a. 超越是与否：通过评分细粒度相关标签提升零样本 LLM 排名器。*arXiv 预印本
    arXiv:2310.14122*。
- en: 'Zhuang et al. (2021) Shengyao Zhuang, Hang Li, and Guido Zuccon. 2021. Deep
    query likelihood model for information retrieval. In *Advances in Information
    Retrieval: 43rd European Conference on IR Research, ECIR 2021, Virtual Event,
    March 28–April 1, 2021, Proceedings, Part II 43*, pages 463–470\. Springer.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2021) Shengyao Zhuang, Hang Li, and Guido Zuccon. 2021. 深度查询可能性模型用于信息检索。*《信息检索进展：第43届欧洲信息检索研究会议，ECIR
    2021，虚拟活动，2021年3月28日至4月1日，会议论文集，第 II 卷 43》*，第463–470页。Springer。
- en: 'Zhuang et al. (2024) Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin,
    and Guido Zuccon. 2024. Promptreps: Prompting large language models to generate
    dense and sparse representations for zero-shot document retrieval. *arXiv preprint
    arXiv:2404.18424*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2024) Shengyao Zhuang, Xueguang Ma, Bevan Koopman, Jimmy Lin,
    and Guido Zuccon. 2024. Promptreps：提示大语言模型生成密集和稀疏表示用于零样本文档检索。*arXiv 预印本 arXiv:2404.18424*。
- en: Zhuang et al. (2023b) Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido
    Zuccon. 2023b. A setwise approach for effective and highly efficient zero-shot
    ranking with large language models. *arXiv preprint arXiv:2310.09497*.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang et al. (2023b) Shengyao Zhuang, Honglei Zhuang, Bevan Koopman, and Guido
    Zuccon. 2023b. 一种针对大型语言模型的有效且高效的零样本排名集方法。*arXiv 预印本 arXiv:2310.09497*。
- en: 'Zhuang and Zuccon (2021) Shengyao Zhuang and Guido Zuccon. 2021. Tilde: Term
    independent likelihood model for passage re-ranking. In *Proceedings of the 44th
    International ACM SIGIR Conference on Research and Development in Information
    Retrieval*, pages 1483–1492.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhuang and Zuccon (2021) Shengyao Zhuang 和 Guido Zuccon. 2021. Tilde: 术语独立可能性模型用于段落重排名。*《第44届国际ACM
    SIGIR信息检索研究与开发会议论文集》*，第1483–1492页。'
- en: 'Zuccon et al. (2023) Guido Zuccon, Harrisen Scells, and Shengyao Zhuang. 2023.
    Beyond co2 emissions: The overlooked impact of water consumption of information
    retrieval models. In *Proceedings of the 2023 ACM SIGIR International Conference
    on Theory of Information Retrieval*, pages 283–289.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zuccon et al. (2023) Guido Zuccon, Harrisen Scells, and Shengyao Zhuang. 2023.
    超越二氧化碳排放：信息检索模型水消耗的被忽视影响。*《2023年ACM SIGIR国际信息检索理论会议论文集》*，第283–289页。
- en: Appendix A Original Prompts of LLM rankers
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A LLM 排名器的原始提示
- en: 'Table 4: The content of each original prompt is structured with specific placeholders
    enclosed in braces “{}”. For instance, “{query}” represents the query text.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 每个原始提示的内容都以特定的占位符结构化，括在大括号“{}”中。例如，“{query}”代表查询文本。'
- en: '| Rankers | Original Prompt 1 | Original Prompt 2 | Original Prompt 3 | Original
    Prompt 4 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 排名 | 原始提示 1 | 原始提示 2 | 原始提示 3 | 原始提示 4 |'
- en: '| Pointwise |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 点对点 |'
- en: '&#124; Query: {query} &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 查询: {query} &#124;'
- en: '&#124; Passage: {text} &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 片段: {text} &#124;'
- en: '&#124; Does the passage answer the query? &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 该片段是否回答了查询？ &#124;'
- en: '&#124; Answer ’Yes’ or ’No’ &#124;'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请回答‘是’或‘否’ &#124;'
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Passage: {text} &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 片段: {text} &#124;'
- en: '&#124; Query: {query} &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 查询: {query} &#124;'
- en: '&#124; Is this passage relevant to the query? &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 这个片段与查询相关吗？ &#124;'
- en: '&#124; Please answer True/False. Answer: &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 请回答真/假。回答: &#124;'
- en: '|'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; For the following query and document, &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对于以下查询和文档，&#124;'
- en: '&#124; judge whether they are ’Highly Relevant’, ’Somewhat Relevant’, or ’Not
    Relevant’. &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 判断它们是‘高度相关’，‘有些相关’，还是‘不相关’。 &#124;'
- en: '&#124; Query: {query} &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 查询: {query} &#124;'
- en: '&#124; Document:{text} &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文档:{text} &#124;'
- en: '&#124; Output: &#124;'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输出: &#124;'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; From a scale of 0 to 4, &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 从 0 到 4 的评分中，&#124;'
- en: '&#124; judge the relevance between the query and the document. &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 判断查询与文档之间的相关性。 &#124;'
- en: '&#124; Query: {query} &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 查询：{query} &#124;'
- en: '&#124; Document:{text} &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文档:{text} &#124;'
- en: '&#124; Output: &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输出： &#124;'
- en: '|'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Pairwise |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 成对比较 |'
- en: '&#124; Given a query: {query}, which of the following two passages is more
    relevant to the query? &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 给定查询：{query}，以下哪段文字与查询更相关？ &#124;'
- en: '&#124; Passage A: {doc1} &#124;'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 段落 A: {doc1} &#124;'
- en: '&#124; Passage B: {doc2} &#124;'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 段落 B: {doc2} &#124;'
- en: '&#124; Output Passage A or Passage B: &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 输出 Passage A 或 Passage B: &#124;'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Listwise |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 列表比较 |'
- en: '&#124; {Several Passages} &#124;'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; {Several Passages} &#124;'
- en: '&#124; Query = {query} &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 查询 = {query} &#124;'
- en: '&#124; Passages = [Passage 1, Passage2, Passage3, Passage4] &#124;'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 段落 = [Passage 1, Passage2, Passage3, Passage4] &#124;'
- en: '&#124; Sort the Passages by their relevance to the Query. &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 根据段落与查询的相关性对段落进行排序。 &#124;'
- en: '&#124; Sorted Passages = [ &#124;'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 排序段落 = [ &#124;'
- en: '|'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; You are RankGPT, an intelligent assistant that can rank passages based
    on their relevancy to the query. &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 你是 RankGPT，一个可以根据段落与查询的相关性对段落进行排名的智能助手。 &#124;'
- en: '&#124; I will provide you with {num} passages, each indicated by number identifier
    []. &#124;'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 我将提供给你 {num} 个段落，每个段落由编号标识符 [] 表示。 &#124;'
- en: '&#124; Rank the passages based on their relevance to query: {query} &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 根据段落与查询的相关性对段落进行排名：{query} &#124;'
- en: '&#124; {Several Passages} &#124;'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; {Several Passages} &#124;'
- en: '&#124; Search Query: {query}. &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 搜索查询：{query}。 &#124;'
- en: '&#124; Rank the {num} passages above based on their relevance to the search
    query. &#124;'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 根据段落与搜索查询的相关性对 {num} 个段落进行排序。 &#124;'
- en: '&#124; The passages should be listed in descending order using identifiers.
    &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 段落应该按降序列出，使用标识符。 &#124;'
- en: '&#124; The most relevant passages should be listed first. The output format
    should be [] 
    [2]. &#124;'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 最相关的段落应该排在前面。输出格式应为 []  [2]。 &#124;'
- en: '&#124; Only response the ranking results, do not say any word or explain. &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 只输出排名结果，不要说任何话或进行解释。 &#124;'
- en: '|'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Setwise |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 集合比较 |'
- en: '&#124; Given a query "{query}", which of the following passages is the most
    relevant one to the query? &#124;'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 给定查询 "{query}"，以下哪个段落与查询最相关？ &#124;'
- en: '&#124; Passages [Passage A, Passage B, Passage C] &#124;'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 段落 [Passage A, Passage B, Passage C] &#124;'
- en: '&#124; Output only the passage label of the most relevant passage: &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅输出最相关段落的标签： &#124;'
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The original prompts associated to each LLM ranking method are reported in Table [4](#A1.T4
    "Table 4 ‣ Appendix A Original Prompts of LLM rankers ‣ An Investigation of Prompt
    Variations for Zero-shot LLM-based Rankers"); these prompts were collected from
    the works of Zhuang et al. ([2023a](#bib.bib33)); Qin et al. ([2023](#bib.bib18));
    Ma et al. ([2023](#bib.bib14)); Sun et al. ([2023](#bib.bib22)); Tang et al. ([2023](#bib.bib23));
    Zhuang et al. ([2023b](#bib.bib36)),.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 LLM 排名方法的原始提示在表格 [4](#A1.T4 "Table 4 ‣ Appendix A Original Prompts of LLM
    rankers ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers")
    中列出；这些提示来自 Zhuang 等人的研究 ([2023a](#bib.bib33))；Qin 等人 ([2023](#bib.bib18))；Ma 等人
    ([2023](#bib.bib14))；Sun 等人 ([2023](#bib.bib22))；Tang 等人 ([2023](#bib.bib23))；Zhuang
    等人 ([2023b](#bib.bib36))。
- en: 'In the original prompts for listwise the number of passages included in a single
    prompt is a parameter of the method. All passages included are denoted by “{Several
    Passages}”, and “{num}” indicates the actual number of passages. Furthermore,
    in the first original listwise prompt, there is an instruction specifying the
    format as follows: “Passages = [Passage 1, Passage 2, Passage 3, Passage 4]”.
    It is important to note that these entries are placeholders, not actual passages,
    and their count is determined by “{num}”.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始的 listwise 提示中，单个提示中包含的段落数量是方法的一个参数。所有包含的段落用“{Several Passages}”表示，“{num}”表示实际的段落数量。此外，在第一个原始
    listwise 提示中，有一个指定格式的指令，如下所示：“Passages = [Passage 1, Passage 2, Passage 3, Passage
    4]”。需要注意的是，这些条目是占位符，而非实际的段落，其数量由“{num}”确定。
- en: Appendix B Best Prompt Variations
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 最佳提示变体
- en: Table [5](#A2.T5 "Table 5 ‣ Appendix B Best Prompt Variations ‣ An Investigation
    of Prompt Variations for Zero-shot LLM-based Rankers") reports the template combinations
    that lead to the best performing prompts across combinations of ranking method,
    LLM backbones, and dataset.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [5](#A2.T5 "Table 5 ‣ Appendix B Best Prompt Variations ‣ An Investigation
    of Prompt Variations for Zero-shot LLM-based Rankers") 报告了在排名方法、LLM 骨干和数据集的组合中，表现最佳的提示模板组合。
- en: 'Table 5: Prompt templates containing the components that lead to the most effective
    results across combinations of ranking method, LLM backbones, and dataset.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 包含导致最有效结果的提示模板，涵盖了排名方法、LLM骨干和数据集的组合。'
- en: '(a) Dataset: DL19'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '(a) 数据集: DL19'
- en: '| Model | Pointwise | Pairwise | Listwise | Setwise |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pointwise | Pairwise | Listwise | Setwise |'
- en: '| FlanT5-large | TI_3, OT_1, TW_0, PF, B, RP_1 | TI_1, OT_1, TW_4, QF, B, RP_None
    | TI_3, OT_2, TW_2, PF, E, RP_None | TI_1, OT_3, TW_0, QF, B, RP_None |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-large | TI_3, OT_1, TW_0, PF, B, RP_1 | TI_1, OT_1, TW_4, QF, B, RP_None
    | TI_3, OT_2, TW_2, PF, E, RP_None | TI_1, OT_3, TW_0, QF, B, RP_None |'
- en: '| FlanT5-xl | TI_2, OT_3, TW_4, PF, E, RP_1 | TI_1, OT_1, TW_2, QF, B, RP_1
    | TI_3, OT_2, TW_3, QF, B, RP_None | TI_1, OT_3, TW_3, QF, E, RP_1 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-xl | TI_2, OT_3, TW_4, PF, E, RP_1 | TI_1, OT_1, TW_2, QF, B, RP_1
    | TI_3, OT_2, TW_3, QF, B, RP_None | TI_1, OT_3, TW_3, QF, E, RP_1 |'
- en: '| FlanT5-xxl | TI_4, OT_3, TW_1, PF, B, RP_None | TI_1, OT_1, TW_0, QF, E,
    RP_1 | TI_1, OT_2, TW_2, QF, B, RP_None | TI_1, OT_1, TW_4, QF, B, RP_1 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-xxl | TI_4, OT_3, TW_1, PF, B, RP_None | TI_1, OT_1, TW_0, QF, E,
    RP_1 | TI_1, OT_2, TW_2, QF, B, RP_None | TI_1, OT_1, TW_4, QF, B, RP_1 |'
- en: '| Mistral-7B | TI_1, OT_4, TW_3, PF, E, RP_1 | TI_1, OT_1, TW_0, QF, E, RP_1
    | TI_1, OT_1, TW_2, QF, B, RP_None | TI_1, OT_3, TW_0, QF, E, RP_1 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | TI_1, OT_4, TW_3, PF, E, RP_1 | TI_1, OT_1, TW_0, QF, E, RP_1
    | TI_1, OT_1, TW_2, QF, B, RP_None | TI_1, OT_3, TW_0, QF, E, RP_1 |'
- en: '| Llama3-8B | TI_2, OT_3, TW_2, PF, B, RP_1 | TI_1, OT_1, TW_0, QF, B, RP_None
    | TI_1, OT_2, TW_0, QF, B, RP_None | TI_1, OT_1, TW_3, PF, E, RP_None |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B | TI_2, OT_3, TW_2, PF, B, RP_1 | TI_1, OT_1, TW_0, QF, B, RP_None
    | TI_1, OT_2, TW_0, QF, B, RP_None | TI_1, OT_1, TW_3, PF, E, RP_None |'
- en: '(b) Dataset: DL20'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '(b) 数据集: DL20'
- en: '| Model | Pointwise | Pairwise | Listwise | Setwise |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pointwise | Pairwise | Listwise | Setwise |'
- en: '| FlanT5-large | TI_3, OT_3, TW_3, PF, B, RP_1 | TI_1, OT_1, TW_2, PF, E, RP_None
    | TI_3, OT_2, TW_1, PF, E, RP_None | TI_1, OT_1, TW_2, PF, E, RP_1 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-large | TI_3, OT_3, TW_3, PF, B, RP_1 | TI_1, OT_1, TW_2, PF, E, RP_None
    | TI_3, OT_2, TW_1, PF, E, RP_None | TI_1, OT_1, TW_2, PF, E, RP_1 |'
- en: '| FlanT5-xl | TI_3, OT_3, TW_4, QF, B, RP_1 | TI_1, OT_1, TW_5, QF, B, RP_1
    | TI_2, OT_2, TW_3, PF, E, RP_1 | TI_1, OT_2, TW_5, PF, B, RP_None |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-xl | TI_3, OT_3, TW_4, QF, B, RP_1 | TI_1, OT_1, TW_5, QF, B, RP_1
    | TI_2, OT_2, TW_3, PF, E, RP_1 | TI_1, OT_2, TW_5, PF, B, RP_None |'
- en: '| FlanT5-xxl | TI_3, OT_1, TW_3, PF, B, RP_None | TI_1, OT_1, TW_2, PF, E,
    RP_1 | TI_2, OT_2, TW_3, PF, B, RP_None | TI_1, OT_3, TW_3, PF, B, RP_1 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-xxl | TI_3, OT_1, TW_3, PF, B, RP_None | TI_1, OT_1, TW_2, PF, E,
    RP_1 | TI_2, OT_2, TW_3, PF, B, RP_None | TI_1, OT_3, TW_3, PF, B, RP_1 |'
- en: '| Mistral-7B | TI_1, OT_4, TW_2, PF, E, RP_1 | TI_1, OT_1, TW_1, QF, E, RP_None
    | TI_3, OT_1, TW_0, QF, B, RP_1 | TI_1, OT_3, TW_2, QF, E, RP_1 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | TI_1, OT_4, TW_2, PF, E, RP_1 | TI_1, OT_1, TW_1, QF, E, RP_None
    | TI_3, OT_1, TW_0, QF, B, RP_1 | TI_1, OT_3, TW_2, QF, E, RP_1 |'
- en: '| Llama3-8B | TI_1, OT_3, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_3, PF, E, RP_1
    | TI_3, OT_2, TW_0, QF, B, RP_None | TI_1, OT_2, TW_4, PF, E, RP_None |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B | TI_1, OT_3, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_3, PF, E, RP_1
    | TI_3, OT_2, TW_0, QF, B, RP_None | TI_1, OT_2, TW_4, PF, E, RP_None |'
- en: '(c) Dataset: COVID'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '(c) 数据集: COVID'
- en: '| Model | Pointwise | Pairwise | Listwise | Setwise |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pointwise | Pairwise | Listwise | Setwise |'
- en: '| FlanT5-large | TI_3, OT_1, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_5, PF, B, RP_1
    | TI_3, OT_2, TW_4, PF, E, RP_None | TI_1, OT_1, TW_1, PF, B, RP_None |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-large | TI_3, OT_1, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_5, PF, B, RP_1
    | TI_3, OT_2, TW_4, PF, E, RP_None | TI_1, OT_1, TW_1, PF, B, RP_None |'
- en: '| FlanT5-xl | TI_3, OT_1, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_5, PF, B, RP_1
    | TI_2, OT_2, TW_1, QF, E, RP_None | TI_1, OT_3, TW_5, QF, E, RP_None |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-xl | TI_3, OT_1, TW_1, PF, B, RP_1 | TI_1, OT_1, TW_5, PF, B, RP_1
    | TI_2, OT_2, TW_1, QF, E, RP_None | TI_1, OT_3, TW_5, QF, E, RP_None |'
- en: '| FlanT5-xxl | TI_3, OT_1, TW_1, PF, B, RP_None | TI_1, OT_1, TW_0, PF, B,
    RP_1 | TI_1, OT_2, TW_3, PF, B, RP_None | TI_1, OT_3, TW_5, PF, B, RP_1 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| FlanT5-xxl | TI_3, OT_1, TW_1, PF, B, RP_None | TI_1, OT_1, TW_0, PF, B,
    RP_1 | TI_1, OT_2, TW_3, PF, B, RP_None | TI_1, OT_3, TW_5, PF, B, RP_1 |'
- en: '| Mistral-7B | TI_2, OT_2, TW_4, PF, B, RP_1 | TI_1, OT_1, TW_3, QF, B, RP_1
    | TI_1, OT_1, TW_4, QF, B, RP_None | TI_1, OT_1, TW_2, QF, B, RP_None |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | TI_2, OT_2, TW_4, PF, B, RP_1 | TI_1, OT_1, TW_3, QF, B, RP_1
    | TI_1, OT_1, TW_4, QF, B, RP_None | TI_1, OT_1, TW_2, QF, B, RP_None |'
- en: '| Llama3-8B | TI_2, OT_3, TW_5, QF, E, RP_1 | TI_1, OT_1, TW_4, QF, B, RP_None
    | TI_1, OT_2, TW_0, QF, B, RP_None | TI_1, OT_3, TW_1, QF, B, RP_1 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Llama3-8B | TI_2, OT_3, TW_5, QF, E, RP_1 | TI_1, OT_1, TW_4, QF, B, RP_None
    | TI_1, OT_2, TW_0, QF, B, RP_None | TI_1, OT_3, TW_1, QF, B, RP_1 |'
- en: Appendix C Effectiveness of Original and Best Prompts
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 原始和最佳提示的有效性
- en: Table [6](#A3.T6 "Table 6 ‣ Appendix C Effectiveness of Original and Best Prompts
    ‣ An Investigation of Prompt Variations for Zero-shot LLM-based Rankers") reports
    the nDCG@10 obtained by the original prompts for each ranking method and the best
    prompt variation for each ranking family.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [6](#A3.T6 "表 6 ‣ 附录 C 原始和最佳提示的有效性 ‣ 零样本 LLM 基于排名器的提示变体研究") 报告了每个排名方法的原始提示和每个排名系列的最佳提示变体所获得的
    nDCG@10。
- en: 'Table 6: Comparison of nDCG@10 across different rankers, LLMs and datasets.
    Po: pointwise, Pa: pairwise, Li: listwise, Se: setwise. Original: the original
    (adapted) prompt. Best: the best prompt found in our experiments. For each dataset
    and backbone, we highlighted in italics the best performing method that used the
    original prompt, and in bold the best performing method overall.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：不同排名器、LLM和数据集的nDCG@10比较。Po：逐点，Pa：逐对，Li：逐列表，Se：逐集。Original：原始（调整）提示。Best：在我们实验中找到的最佳提示。对于每个数据集和骨干，我们用*斜体*突出显示了使用原始提示的最佳表现方法，用**粗体**突出显示了整体最佳表现的方法。
- en: '|  |  | Mistral-7B | Llama 3-8B |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |  | Mistral-7B | Llama 3-8B |'
- en: '| DL19 | DL20 | COVID | DL19 | DL20 | COVID |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| DL19 | DL20 | COVID | DL19 | DL20 | COVID |'
- en: '| Ranker | Original | Best | Original | Best | Original | Best | Original |
    Best | Original | Best | Original | Best |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 排名器 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳 |'
- en: '| Po | Qin et al. ([2023](#bib.bib18)) | 0.5851 | 0.6699 | 0.5237 | 0.6486
    | 0.6897 | 0.7988 | 0.4176 | 0.5328 | 0.3383 | 0.4950 | 0.6678 | 0.7916 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Po | Qin et al. ([2023](#bib.bib18)) | 0.5851 | 0.6699 | 0.5237 | 0.6486
    | 0.6897 | 0.7988 | 0.4176 | 0.5328 | 0.3383 | 0.4950 | 0.6678 | 0.7916 |'
- en: '| Po | Ma et al. ([2023](#bib.bib14)) | 0.6254 | 0.5948 | 0.7379 | 0.4808 |
    0.3794 | 0.7527 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Po | Ma et al. ([2023](#bib.bib14)) | 0.6254 | 0.5948 | 0.7379 | 0.4808 |
    0.3794 | 0.7527 |'
- en: '| Po | Zhuang et al. ([2023a](#bib.bib33)).1 | 0.6414 | 0.5951 | 0.7758 | 0.3764
    | 0.3088 | 0.7485 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Po | Zhuang et al. ([2023a](#bib.bib33)).1 | 0.6414 | 0.5951 | 0.7758 | 0.3764
    | 0.3088 | 0.7485 |'
- en: '| Po | Zhuang et al. ([2023a](#bib.bib33)).2 | 0.5577 | 0.4896 | 0.6967 | 0.3890
    | 0.3466 | 0.7242 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| Po | Zhuang et al. ([2023a](#bib.bib33)).2 | 0.5577 | 0.4896 | 0.6967 | 0.3890
    | 0.3466 | 0.7242 |'
- en: '| Pa | Qin et al. ([2023](#bib.bib18)) | 0.6263 | 0.6450 | 0.6036 | 0.6059
    | 0.7724 | 0.7738 | 0.6738 | 0.6738 | 0.6163 | 0.6340 | 0.7966 | 0.8012 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| Pa | Qin et al. ([2023](#bib.bib18)) | 0.6263 | 0.6450 | 0.6036 | 0.6059
    | 0.7724 | 0.7738 | 0.6738 | 0.6738 | 0.6163 | 0.6340 | 0.7966 | 0.8012 |'
- en: '| Li | Ma et al. ([2023](#bib.bib14)) | 0.5863 | 0.6686 | 0.5492 | 0.6348 |
    0.7101 | 0.7758 | 0.6364 | 0.7004 | 0.6088 | 0.6619 | 0.7393 | 0.8123 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| Li | Ma et al. ([2023](#bib.bib14)) | 0.5863 | 0.6686 | 0.5492 | 0.6348 |
    0.7101 | 0.7758 | 0.6364 | 0.7004 | 0.6088 | 0.6619 | 0.7393 | 0.8123 |'
- en: '| Li | Sun et al. ([2023](#bib.bib22)) | 0.5402 | 0.5184 | 0.6204 | 0.5926
    | 0.5377 | 0.6589 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| Li | Sun et al. ([2023](#bib.bib22)) | 0.5402 | 0.5184 | 0.6204 | 0.5926
    | 0.5377 | 0.6589 |'
- en: '| Se | Zhuang et al. ([2023b](#bib.bib36)) | 0.6567 | 0.6811 | 0.6180 | 0.6256
    | 0.7846 | 0.8053 | 0.6733 | 0.7000 | 0.5990 | 0.6486 | 0.7897 | 0.8035 |  |  |  |
    FlanT5-Large | FlanT5-XL | FlanT5-XXL |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| Se | Zhuang et al. ([2023b](#bib.bib36)) | 0.6567 | 0.6811 | 0.6180 | 0.6256
    | 0.7846 | 0.8053 | 0.6733 | 0.7000 | 0.5990 | 0.6486 | 0.7897 | 0.8035 |  |  |  |
    FlanT5-Large | FlanT5-XL | FlanT5-XXL |'
- en: '| DL19 | DL20 | COVID | DL19 | DL20 | COVID | DL19 | DL20 | COVID |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| DL19 | DL20 | COVID | DL19 | DL20 | COVID | DL19 | DL20 | COVID |'
- en: '| Ranker | Original | Best | Original | Best | Original | Best | Original |
    Best | Original | Best | Original | Best | Original | Best | Original | Best |
    Original | Best |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 排名器 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳 | 原始 | 最佳
    | 原始 | 最佳 | 原始 | 最佳 |'
- en: '| Po | Qin et al. ([2023](#bib.bib18)) | 0.6180 | 0.6918 | 0.5984 | 0.6266
    | 0.6490 | 0.7570 | 0.6157 | 0.7010 | 0.6439 | 0.6727 | 0.6786 | 0.7742 | 0.6289
    | 0.6860 | 0.6514 | 0.6733 | 0.6642 | 0.7784 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| Po | Qin et al. ([2023](#bib.bib18)) | 0.6180 | 0.6918 | 0.5984 | 0.6266
    | 0.6490 | 0.7570 | 0.6157 | 0.7010 | 0.6439 | 0.6727 | 0.6786 | 0.7742 | 0.6289
    | 0.6860 | 0.6514 | 0.6733 | 0.6642 | 0.7784 |'
- en: '| Po | Ma et al. ([2023](#bib.bib14)) | 0.6483 | 0.6128 | 0.6971 | 0.6690 |
    0.6426 | 0.7301 | 0.6563 | 0.6533 | 0.7321 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Po | Ma et al. ([2023](#bib.bib14)) | 0.6483 | 0.6128 | 0.6971 | 0.6690 |
    0.6426 | 0.7301 | 0.6563 | 0.6533 | 0.7321 |'
- en: '| Po | Zhuang et al. ([2023a](#bib.bib33)).1 | 0.6468 | 0.5698 | 0.6672 | 0.6300
    | 0.6444 | 0.7023 | 0.6451 | 0.6274 | 0.6983 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Po | Zhuang et al. ([2023a](#bib.bib33)).1 | 0.6468 | 0.5698 | 0.6672 | 0.6300
    | 0.6444 | 0.7023 | 0.6451 | 0.6274 | 0.6983 |'
- en: '| Po | Zhuang et al. ([2023a](#bib.bib33)).2 | 0.5164 | 0.4387 | 0.6206 | 0.6054
    | 0.5971 | 0.7095 | 0.4068 | 0.3301 | 0.4335 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Po | Zhuang et al. ([2023a](#bib.bib33)).2 | 0.5164 | 0.4387 | 0.6206 | 0.6054
    | 0.5971 | 0.7095 | 0.4068 | 0.3301 | 0.4335 |'
- en: '| Pa | Qin et al. ([2023](#bib.bib18)) | 0.6677 | 0.6724 | 0.6237 | 0.6382
    | 0.7558 | 0.7788 | 0.6845 | 0.6986 | 0.6766 | 0.6823 | 0.7536 | 0.7750 | 0.6915
    | 0.7135 | 0.6992 | 0.7126 | 0.7452 | 0.7917 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Pa | Qin et al. ([2023](#bib.bib18)) | 0.6677 | 0.6724 | 0.6237 | 0.6382
    | 0.7558 | 0.7788 | 0.6845 | 0.6986 | 0.6766 | 0.6823 | 0.7536 | 0.7750 | 0.6915
    | 0.7135 | 0.6992 | 0.7126 | 0.7452 | 0.7917 |'
- en: '| Li | Ma et al. ([2023](#bib.bib14)) | 0.5465 | 0.6443 | 0.5081 | 0.6125 |
    0.6067 | 0.7521 | 0.5688 | 0.6441 | 0.5576 | 0.6300 | 0.6312 | 0.7438 | 0.5915
    | 0.7011 | 0.5852 | 0.6897 | 0.6955 | 0.7834 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| Li | Ma et al. ([2023](#bib.bib14)) | 0.5465 | 0.6443 | 0.5081 | 0.6125 |
    0.6067 | 0.7521 | 0.5688 | 0.6441 | 0.5576 | 0.6300 | 0.6312 | 0.7438 | 0.5915
    | 0.7011 | 0.5852 | 0.6897 | 0.6955 | 0.7834 |'
- en: '| Li | Sun et al. ([2023](#bib.bib22)) | 0.6199 | 0.5552 | 0.7445 | 0.6129
    | 0.5966 | 0.6938 | 0.6920 | 0.6672 | 0.7736 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| Li | Sun et al. ([2023](#bib.bib22)) | 0.6199 | 0.5552 | 0.7445 | 0.6129
    | 0.5966 | 0.6938 | 0.6920 | 0.6672 | 0.7736 |'
- en: '| Se | Zhuang et al. ([2023b](#bib.bib36)) | 0.6503 | 0.6693 | 0.5754 | 0.6525
    | 0.7440 | 0.7932 | 0.6812 | 0.6959 | 0.6747 | 0.6855 | 0.7540 | 0.7745 | 0.6925
    | 0.7047 | 0.6776 | 0.7036 | 0.7617 | 0.7890 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| Se | 庄等人 ([2023b](#bib.bib36)) | 0.6503 | 0.6693 | 0.5754 | 0.6525 | 0.7440
    | 0.7932 | 0.6812 | 0.6959 | 0.6747 | 0.6855 | 0.7540 | 0.7745 | 0.6925 | 0.7047
    | 0.6776 | 0.7036 | 0.7617 | 0.7890 |'
