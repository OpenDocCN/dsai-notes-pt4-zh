- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:41:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:41:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering
    Without Fine-Tuning ††thanks: Citation: Authors. Title. Pages…. DOI:000000/11111.'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 仅使用提示工程实现LLMs的工具调用功能，无需微调 ††感谢：引用：作者。标题。页码…… DOI:000000/11111。
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.04997](https://ar5iv.labs.arxiv.org/html/2407.04997)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.04997](https://ar5iv.labs.arxiv.org/html/2407.04997)
- en: Shengtao He
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 盛涛·何
- en: Hunan University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 湖南大学
- en: Changsha, Hunan
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 长沙，湖南
- en: '[hst97@qq.com](mailto:hst97@qq.com)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[hst97@qq.com](mailto:hst97@qq.com)'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Currently, the vast majority of locally deployed open-source large language
    models (LLMs) and some commercial model interfaces do not support stable tool
    calling functionality. The existing solution involves fine-tuning LLMs, which
    results in significant time and computational resource consumption. This paper
    proposes a method that enables LLMs to achieve stable tool calling capabilities
    using only prompt engineering and some ingenious code design. We conducted experiments
    on multiple LLMs that lack tool calling capabilities across various tool calling
    tasks, achieving a success rate of 100%.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，绝大多数本地部署的开源大型语言模型（LLMs）和一些商业模型接口不支持稳定的工具调用功能。现有的解决方案涉及微调LLMs，这会消耗大量时间和计算资源。本文提出了一种仅使用提示工程和一些巧妙的代码设计使LLMs实现稳定工具调用能力的方法。我们对多种缺乏工具调用能力的LLMs进行了各种工具调用任务的实验，成功率达到100%。
- en: '*K*eywords Large Language Models  $\cdot$ Deep Learning'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*eywords 大型语言模型  $\cdot$ 深度学习'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Currently, the primary method for enabling large language models (LLMs) to achieve
    tool calling capabilities is through fine-tuning. For example, ToolLLM is a general
    framework for tool usage. Y. Qin et al. proposed a fine-tuning technique to enable
    tool calling capabilities in LLMs, providing a comprehensive API dataset [[1](#bib.bib1)].
    After extensive training on a large number of real API datasets, the ToolLlama
    model achieved stable tool calling capabilities. I. Abdelaziz et al. enabled function
    calling capabilities (i.e., tool calling) in LLMs through fine-grained multi-task
    learning [[2](#bib.bib2)]. Fine-tuning LLMs requires significant time and computational
    resources, and may even result in an LLM with lower intelligence levels than before
    fine-tuning. This leads to high trial-and-error costs. When a different type or
    more complex API structure needs to be called, the fine-tuned LLM requires additional
    time and computational resources for further fine-tuning to meet new demands.
    This is clearly detrimental to the practical application of LLMs in industry.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，使大型语言模型（LLMs）实现工具调用功能的主要方法是通过微调。例如，ToolLLM是一个通用的工具使用框架。Y. Qin等人提出了一种微调技术，以使LLMs具备工具调用能力，提供了一个全面的API数据集[[1](#bib.bib1)]。在大量真实API数据集上进行了广泛训练后，ToolLlama模型实现了稳定的工具调用能力。I.
    Abdelaziz等人通过细粒度的多任务学习在LLMs中实现了功能调用能力（即工具调用）[[2](#bib.bib2)]。微调LLMs需要大量的时间和计算资源，甚至可能导致LLM的智能水平低于微调前，这会导致高试错成本。当需要调用不同类型或更复杂的API结构时，微调后的LLM需要额外的时间和计算资源进行进一步微调以满足新需求。这显然对LLMs在工业中的实际应用是有害的。
- en: Prompt engineering can enable LLMs to achieve tool calling capabilities with
    almost no cost and high efficiency. By dynamically adjusting prompts for different
    application scenarios, LLMs can adapt to new tool libraries. However, the drawback
    of using prompt engineering is its instability. This paper proposes a method to
    achieve stable tool calling capabilities in LLMs using only prompt engineering.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 提示工程可以使LLMs以几乎没有成本和高效率实现工具调用能力。通过动态调整不同应用场景的提示，LLMs可以适应新的工具库。然而，使用提示工程的缺点是其不稳定性。本文提出了一种仅使用提示工程实现LLMs稳定工具调用能力的方法。
- en: 2 Principle
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 原则
- en: 'The prompt engineering method used in this paper consists of two main parts:
    prompt injection and tool result feedback. Prompt injection is used to add tool
    information and prompts for using the tool into the system prompt. Tool result
    feedback involves parsing the output of the tool calling and embedding the content
    returned by the tool back into the LLMs.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文所使用的提示工程方法由两个主要部分组成：提示注入和工具结果反馈。提示注入用于将工具信息和使用工具的提示添加到系统提示中。工具结果反馈涉及解析工具调用的输出，并将工具返回的内容嵌入回LLMs中。
- en: 'During the prompt injection phase, the prompts used are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在提示注入阶段，使用的提示如下：
- en: 'if  tools  is  not  None:tools_dis  =  json.loads(tools)for  tool_dis  in  tools_dis:tools_list.append(tool_dis["function"])tools_instructions  =  ""tools_instruction_list  =  []for  tool  in  tools_list:tools_instruction_list.append(tool["name"])tools_instructions  +=  (str(tool["name"])+  ":"+  "Call  this  tool  to  interact  with  the  "+  str(tool["name"])+  "  API.  What  is  the  "+  str(tool["name"])+  "  API  useful  for?  "+  str(tool["description"])+  ".  Parameters:"+  str(tool["parameters"])+  "Required  parameters:"+  str(tool["parameters"]["required"])+  "\n")TOOL_EAXMPLE  =  "You  will  receive  a  JSON  string  containing  a  list  of  callable  tools.  Please  parse  this  JSON  string  and  return  a  JSON  object  containing  the  tool  name  and  tool  parameters.  Here  is  an  example  of  the  tool  list:\n\n{\"tools\":  [{\"name\":  \"plus_one\",  \"description\":  \"Add  one  to  a  number\",  \"parameters\":  {\"type\":  \"object\",\"properties\":  {\"number\":  {\"type\":  \"string\",\"description\":  \"The  number  that  needs  to  be  changed,  for  example:  1\",\"default\":  \"1\",}},\"required\":  [\"number\"]}},{\"name\":  \"minus_one\",  \"description\":  \"Minus  one  to  a  number\",  \"parameters\":  {\"type\":  \"object\",\"properties\":  {\"number\":  {\"type\":  \"string\",\"description\":  \"The  number  that  needs  to  be  changed,  for  example:  1\",\"default\":  \"1\",}},\"required\":  [\"number\"]}}]}\n\nBased  on  this  tool  list,  generate  a  JSON  object  to  call  a  tool.  For  example,  if  you  need  to  add  one  to  number  77,  return:\n\n{\"tool\":  \"plus_one\",  \"parameters\":  {\"number\":  \"77\"}}\n\nPlease  note  that  the  above  is  just  an  example  and  does  not  mean  that  the  plus_one  and  minus_one  tools  are  currently  available."REUTRN_FORMAT="{\"tool\":  \"tool  name\",  \"parameters\":  {\"parameter  name\":  \"parameter  value\"}}"INSTRUCTION  =  f"""{TOOL_EAXMPLE}Answer  the  following  questions  as  best  you  can.  You  have  access  to  the  following  APIs:{tools_instructions}Use  the  following  format:‘‘‘tool_json{REUTRN_FORMAT}‘‘‘Please  choose  the  appropriate  tool  according  to  the  user’s  question.  If  you  don’t  need  to  call  it,  please  reply  directly  to  the  user’s  question.  When  the  user  communicates  with  you  in  a  language  other  than  English,  you  need  to  communicate  with  the  user  in  the  same  language.When  you  have  enough  information  from  the  tool  results,  respond  directly  to  the  user  with  a  text  message  without  having  to  call  the  tool  again."""system_prompt=INSTRUCTION'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '如果`tools`不为`None`：`tools_dis`  =  `json.loads(tools)`，对每个`tool_dis`，将`tool_dis["function"]`添加到`tools_list`中。`tools_instructions`
    = `""`，`tools_instruction_list` = `[]`。对每个`tool`在`tools_list`中，`tools_instruction_list`添加`tool["name"]`。`tools_instructions`  +=  (str(`tool["name"]`)+  ":"+  "调用此工具以与"+  str(`tool["name"]`)+  "
    API进行交互。"+  str(`tool["name"]`)+  " API的用途是什么？"+  str(`tool["description"]`)+  "。参数："+  str(`tool["parameters"]`)+  "必需参数："+  str(`tool["parameters"]["required"]`)+  "\n")`工具示例`
    =  "您将收到一个包含可调用工具列表的JSON字符串。请解析此JSON字符串，并返回一个包含工具名称和工具参数的JSON对象。以下是工具列表的示例：\n\n{\"tools\":  [{\"name\":  \"plus_one\",  \"description\":  \"将一个数字加一\",  \"parameters\":  {\"type\":  \"object\",\"properties\":  {\"number\":  {\"type\":  \"string\",\"description\":  \"需要更改的数字，例如：1\",\"default\":  \"1\",}},\"required\":  [\"number\"]}},{\"name\":  \"minus_one\",  \"description\":  \"将一个数字减一\",  \"parameters\":  {\"type\":  \"object\",\"properties\":  {\"number\":  {\"type\":  \"string\",\"description\":  \"需要更改的数字，例如：1\",\"default\":  \"1\",}},\"required\":  [\"number\"]}}]}\n\n根据此工具列表，生成一个JSON对象来调用工具。例如，如果你需要将数字77加一，请返回：\n\n{\"tool\":  \"plus_one\",  \"parameters\":  {\"number\":  \"77\"}}\n\n请注意，以上只是示例，并不意味着`plus_one`和`minus_one`工具当前可用。"返回格式="{\"tool\":  \"tool
    name\",  \"parameters\":  {\"parameter name\":  \"parameter value\"}}"指令  =  f"""{工具示例}尽可能最佳地回答以下问题。你可以访问以下API：{tools_instructions}使用以下格式：‘’‘tool_json{返回格式}‘’‘根据用户的问题选择合适的工具。如果不需要调用工具，请直接回复用户的问题。当用户用非英语语言与你沟通时，你需要使用相同的语言与用户沟通。当你从工具结果中获得足够的信息时，直接以文本消息回复用户，而无需再次调用工具。"""'
- en: 'INSTRUCTION is the final string injected into the system prompt, which includes
    three parts: TOOL EXAMPLE, tools instructions, and RETURN FORMAT. TOOL EXAMPLE
    is used to guide the LLMs on how to understand and use the tool. When writing
    TOOL EXAMPLE, it is important to use trivial tools as examples, such as the tools
    used in this paper for incrementing and decrementing numbers, to avoid confusing
    the LLMs with actual usable tools. tools instructions is a list of currently available
    tools converted into a format readable by the LLMs. When using the LLMs in practice,
    tools instructions can be dynamically adjusted by inputting different tools, allowing
    the LLMs to know which tools are available and how to use them. RETURN FORMAT
    defines the format for calling the API.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: INSTRUCTION 是注入系统提示中的最终字符串，包括三个部分：TOOL EXAMPLE、工具说明和 RETURN FORMAT。TOOL EXAMPLE
    用于指导 LLMs 如何理解和使用工具。在编写 TOOL EXAMPLE 时，重要的是使用简单的工具作为示例，例如本文中用于递增和递减数字的工具，以避免用实际可用的工具混淆
    LLMs。工具说明是当前可用工具的列表，转换为 LLMs 可读的格式。在实际使用 LLMs 时，可以通过输入不同的工具动态调整工具说明，使 LLMs 知道哪些工具可用以及如何使用它们。RETURN
    FORMAT 定义了调用 API 的格式。
- en: 'During the tool result feedback phase, regular expressions are used to extract
    the “tool” and “parameters” from the output. For the interpreter tool, another
    regular expression is used to extract the code output by the LLMs, increasing
    the success rate of the LLMs using the interpreter tool. The code used in this
    paper is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在工具结果反馈阶段，使用正则表达式从输出中提取“tool”和“parameters”。对于解释器工具，使用另一种正则表达式提取 LLMs 输出的代码，提高
    LLMs 使用解释器工具的成功率。本文使用的代码如下：
- en: 'history.append({"role":  "user","content":  user_prompt.strip()})response=  model.create_chat_completion(messages  =  history,max_tokens=max_length,temperature=temperature,)response_content=response[’choices’][0][’message’][’content’]pattern  =  r’\{\s*"tool":\s*"(.*?)",\s*"parameters":\s*\{(.*?)\}\s*\}’while  re.search(pattern,  response_content,  re.DOTALL)!=None:match=re.search(pattern,  response_content,  re.DOTALL)tool  =  match.group(1)parameters  =  match.group(2)json_str  =  ’{"tool":  "’  +  tool  +  ’",  "parameters":  {’  +  parameters  +  ’}}’parameters  =  json.loads(’{’  +parameters+  ’}’)results  =  dispatch_tool(tool,  parameters)print(results)history.append({"role":"assistant",  "content":  json_str})history.append({"role":  "observation",  "content":  results})response=  model.create_chat_completion(messages  =  history,max_tokens=max_length,temperature=temperature,)response_content  =  response.choices[0].message.contentpattern  =  r"‘‘‘python\n(.*?)\n‘‘‘"while  re.search(pattern,  response,  re.DOTALL)  !=None:matches  =  re.search(pattern,  response,  re.DOTALL)code  =  matches.group(1)results  =  interpreter(code)print(results)json_str  =  ’{"tool":  "interpreter",  "parameters":  ’+code+’}’history.append({"role":"assistant",  "content":  json_str})history.append({"role":  "observation",  "content":  results})response=  model.create_chat_completion(messages  =  history,max_tokens=max_length,temperature=temperature,)response_content  =  response.choices[0].message.content'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 'history.append({"role":  "user","content":  user_prompt.strip()})response=  model.create_chat_completion(messages  =  history,max_tokens=max_length,temperature=temperature,)response_content=response[’choices’][0][’message’][’content’]pattern  =  r’\{\s*"tool":\s*"(.*?)",\s*"parameters":\s*\{(.*?)\}\s*\}’while  re.search(pattern,  response_content,  re.DOTALL)!=None:match=re.search(pattern,  response_content,  re.DOTALL)tool  =  match.group(1)parameters  =  match.group(2)json_str  =  ’{"tool":  "’  +  tool  +  ’",  "parameters":  {’  +  parameters  +  ’}}’parameters  =  json.loads(’{’  +parameters+  ’}’)results  =  dispatch_tool(tool,  parameters)print(results)history.append({"role":"assistant",  "content":  json_str})history.append({"role":  "observation",  "content":  results})response=  model.create_chat_completion(messages  =  history,max_tokens=max_length,temperature=temperature,)response_content  =  response.choices[0].message.contentpattern  =  r"‘‘‘python\n(.*?)\n‘‘‘"while  re.search(pattern,  response,  re.DOTALL)  !=None:matches  =  re.search(pattern,  response,  re.DOTALL)code  =  matches.group(1)results  =  interpreter(code)print(results)json_str  =  ’{"tool":  "interpreter",  "parameters":  ’+code+’}’history.append({"role":"assistant",  "content":  json_str})history.append({"role":  "observation",  "content":  results})response=  model.create_chat_completion(messages  =  history,max_tokens=max_length,temperature=temperature,)response_content  =  response.choices[0].message.content'
- en: 'By identifying the dictionary of tools called by the LLM and extracting the
    corresponding values, these values are then passed into the appropriate tool functions.
    Finally, the results returned by the tools are sent back to the LLM in the role
    of “observation.” For some LLM interfaces that do not accept the roles of “observation,”
    “tool,” or “function,” the results can be returned to the “user” role instead.
    For example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过识别LLM调用的工具字典并提取相应的值，这些值随后被传递到适当的工具函数中。最后，工具返回的结果作为“观察”的角色返回给LLM。对于一些不接受“观察”、“工具”或“函数”角色的LLM接口，结果可以返回给“用户”角色。例如：
- en: 'history.append({"role":  "user",  "content":  "Call"  +  tool  +  "The  result  returned  by  the  tool  is:"  +  results  +  ".  Please  continue  to  answer  my  previous  question  based  on  the  result  returned  by  the  tool."})'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 'history.append({"role":  "user",  "content":  "调用"  +  tool  +  "工具返回的结果是："  +  results  +  "。请根据工具返回的结果继续回答我之前的问题。"})'
- en: By using the above prompt engineering method, it is possible to avoid fine-tuning
    and enable LLMs that originally lack tool calling capabilities to achieve stable
    tool calling functionality.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用上述提示工程方法，可以避免微调，并使原本缺乏工具调用功能的LLMs实现稳定的工具调用功能。
- en: 3 Experimental Results
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验结果
- en: 'In this study, we used the quantized versions of the current mainstream small
    open-source models llama3-8b, gemma2-9b, qwen2-7b, and mistral-7b from Ollama
    as test models [[3](#bib.bib3)]. The following tool calling tasks were tested,
    each with 10 different queries:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究中，我们使用了当前主流的小型开源模型 llama3-8b、gemma2-9b、qwen2-7b 和 mistral-7b 的量化版本作为测试模型
    [[3](#bib.bib3)]。测试了以下工具调用任务，每项任务进行了10个不同的查询：
- en: 1\. Querying real-time time in different time zones.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 查询不同时间区域的实时时间。
- en: 2\. Querying real-time weather in different locations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 查询不同地点的实时天气。
- en: 3\. Answering recent events after performing a Google search.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. 在进行谷歌搜索后回答近期事件。
- en: 4\. Solving mathematical problems using a Python interpreter.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. 使用 Python 解释器解决数学问题。
- en: 5\. Searching local file information to answer questions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. 搜索本地文件信息以回答问题。
- en: 6\. Querying relevant papers on arXiv.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 查询 arXiv 上的相关论文。
- en: 7\. Searching local knowledge graphs to answer questions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. 搜索本地知识图谱以回答问题。
- en: 'The tests were conducted on an NVIDIA GeForce RTX 4080, using a platform developed
    by the author: the open-source project ComfyUI LLM Party [[4](#bib.bib4)]. This
    project is available on GitHub. To quickly reproduce the results of this paper,
    you can download the project for testing. Table 1 shows the number of successful
    tool calls for multiple models across various tool calling tasks using the prompt
    engineering method proposed in this paper. For models that do not use the prompt
    injection method, tool information is passed to these models via the tool interface.
    However, since these models do not support tool calling functionality, they cannot
    call these tools.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 测试在 NVIDIA GeForce RTX 4080 上进行，使用作者开发的平台：开源项目 ComfyUI LLM Party [[4](#bib.bib4)]。该项目在
    GitHub 上可用。要快速重现本文的结果，可以下载该项目进行测试。表1显示了使用本文提出的提示工程方法在多个模型上进行的各种工具调用任务中的成功工具调用次数。对于不使用提示注入方法的模型，工具信息通过工具接口传递给这些模型。然而，由于这些模型不支持工具调用功能，因此无法调用这些工具。
- en: 'Table 1: Number of Successful Tool Calls Using Prompt Engineering'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：使用提示工程方法的成功工具调用次数
- en: '| Model | Time Zone Query | Weather Query | Google Search | Python Interpreter
    | Local File Search | ArXiv Query | Knowledge Graph Search |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 时间区域查询 | 天气查询 | 谷歌搜索 | Python 解释器 | 本地文件搜索 | ArXiv 查询 | 知识图谱搜索 |'
- en: '| llama3-8b | 10 | 10 | 10 | 1 | 10 | 10 | 10 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| llama3-8b | 10 | 10 | 10 | 1 | 10 | 10 | 10 |'
- en: '| gemma2-9b | 10 | 10 | 10 | 10 | 10 | 10 | 10 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| gemma2-9b | 10 | 10 | 10 | 10 | 10 | 10 | 10 |'
- en: '| qwen2-7b | 10 | 10 | 10 | 10 | 10 | 10 | 1 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| qwen2-7b | 10 | 10 | 10 | 10 | 10 | 10 | 1 |'
- en: '| mistral-7b | 10 | 10 | 10 | 2 | 10 | 10 | 1 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| mistral-7b | 10 | 10 | 10 | 2 | 10 | 10 | 1 |'
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3 Experimental Results ‣ Achieving Tool Calling
    Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning Citation:
    Authors. Title. Pages…. DOI:000000/11111.") shows that all models successfully
    executed the tool calling step and correctly output dictionaries that could be
    captured by regular expressions. However, due to limitations in code generation
    capabilities, the Ollama-quantized versions of the llama3-8b and mistral-7b models
    did not consistently output correct code in the Python interpreter task, resulting
    in unstable completion of computational tasks. In the knowledge graph search task,
    all models successfully returned relevant knowledge using the tools. However,
    due to limitations in logical understanding capabilities, the Ollama-quantized
    versions of the qwen2-7b and mistral-7b models could not consistently understand
    the logical relationships between multiple edges in the knowledge graph.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S3.T1 "表 1 ‣ 3 实验结果 ‣ 仅通过提示工程实现LLM中的工具调用功能，无需微调 引用：作者。标题。页码…. DOI:000000/11111.")
    显示所有模型都成功执行了工具调用步骤，并正确输出了可以被正则表达式捕获的字典。然而，由于代码生成能力的限制，Ollama量化版本的llama3-8b和mistral-7b模型在Python解释器任务中未能始终输出正确的代码，导致计算任务完成不稳定。在知识图谱搜索任务中，所有模型都成功地使用工具返回了相关知识。然而，由于逻辑理解能力的限制，Ollama量化版本的qwen2-7b和mistral-7b模型未能始终理解知识图谱中多个边之间的逻辑关系。
- en: These experimental results demonstrate that prompt engineering can enable LLMs
    that originally lack tool calling capabilities to achieve tool calling functionality.
    However, the ability to effectively utilize the information returned by the tools
    to solve user problems is still limited by the LLM’s own intelligence level. Larger
    models, such as gemma2-9b, show significantly more stable capabilities in utilizing
    the results returned by the tools.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验结果表明，提示工程可以使原本不具备工具调用能力的LLM实现工具调用功能。然而，能够有效利用工具返回的信息来解决用户问题的能力仍然受限于LLM自身的智能水平。像gemma2-9b这样的较大模型在利用工具返回的结果方面表现出显著更稳定的能力。
- en: '![Refer to caption](img/6260e02269e7f43d72b7ec9a4a37e316.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6260e02269e7f43d72b7ec9a4a37e316.png)'
- en: (a)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: '![Refer to caption](img/34888cf0dfdfc8b7b4466ec282b6c072.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34888cf0dfdfc8b7b4466ec282b6c072.png)'
- en: (b)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: 'Figure 1: (a) Output of gemma2-9b without using prompt engineering; (b) Output
    of gemma2-9b using prompt engineering'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图1： (a) 未使用提示工程的gemma2-9b的输出；(b) 使用提示工程的gemma2-9b的输出
- en: 'The Figure [1](#S3.F1 "Figure 1 ‣ 3 Experimental Results ‣ Achieving Tool Calling
    Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning Citation:
    Authors. Title. Pages…. DOI:000000/11111.") shows the output of the gemma2-9b
    model when calling the weather tool. When prompt engineering is not used, the
    ‘is tools in sys prompt’ attribute is set to disable, and the gemma2-9b model
    believes it cannot obtain real-time information, thus rejecting the user’s request.
    When prompt engineering is used, the ‘is tools in sys prompt’ attribute is set
    to enable, and the gemma2-9b model provides the correct real-time weather information.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S3.F1 "图 1 ‣ 3 实验结果 ‣ 仅通过提示工程实现LLM中的工具调用功能，无需微调 引用：作者。标题。页码…. DOI:000000/11111.")
    显示了gemma2-9b模型调用天气工具时的输出。当未使用提示工程时，‘is tools in sys prompt’属性设置为禁用，gemma2-9b模型认为无法获取实时信息，因此拒绝用户请求。当使用提示工程时，‘is
    tools in sys prompt’属性设置为启用，gemma2-9b模型提供了正确的实时天气信息。
- en: 4 Conclusion
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: This study demonstrates that prompt engineering alone can enable LLMs to achieve
    tool calling capabilities, significantly saving time and computational resources
    required for fine-tuning. All models in the experiment successfully output the
    correct format for regular expression recognition. However, due to the intelligence
    level limitations of small LLMs, some models lacked the programming or logical
    capabilities to produce correct results for certain complex tasks. Due to the
    limitations of the author’s equipment, the effects of prompt engineering were
    not tested on larger LLMs. Researchers who have doubts about the experimental
    results can use the author’s open-source project on GitHub, ComfyUI LLM Party
    [[4](#bib.bib4)], to reproduce the work presented in this paper.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这项研究表明，仅通过提示工程就可以使 LLM 实现工具调用能力，从而显著节省了微调所需的时间和计算资源。实验中的所有模型都成功输出了正则表达式识别的正确格式。然而，由于小型
    LLM 的智能水平限制，某些模型缺乏编程或逻辑能力，无法对某些复杂任务产生正确结果。由于作者设备的限制，提示工程的效果未在更大规模的 LLM 上进行测试。对实验结果有疑问的研究人员可以使用作者在
    GitHub 上的开源项目 ComfyUI LLM Party [[4](#bib.bib4)]，以重现本文所述的工作。
- en: References
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. Qin et al. Toolllm: Facilitating large language models to master 16000+
    real-world apis. arXiv preprint arXiv:2307.16789, 2023.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. Qin 等。Toolllm: 使大型语言模型掌握16000+实际应用接口。arXiv 预印本 arXiv:2307.16789, 2023。'
- en: '[2] I. Abdelaziz et al. Granite-function calling model: Introducing function
    calling abilities via multi-task learning of granular tasks. arXiv preprint arXiv:2407.00121,
    2024.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] I. Abdelaziz 等。Granite-function calling model: 通过对细粒度任务的多任务学习引入函数调用能力。arXiv
    预印本 arXiv:2407.00121, 2024。'
- en: '[3] Ollama. Quantized versions of llama3-8b, gemma2-9b, qwen2-7b, and mistral-7b,
    2024. Ollama Documentation, [https://ollama.com/library](https://ollama.com/library).'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Ollama。量化版本的 llama3-8b、gemma2-9b、qwen2-7b 和 mistral-7b，2024。Ollama 文档，[https://ollama.com/library](https://ollama.com/library)。'
- en: '[4] S. He. Comfyui llm party: An open-source project for llm tool calling,
    2024. GitHub Repository, [https://github.com/heshengtao/comfyui_LLM_party](https://github.com/heshengtao/comfyui_LLM_party).'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. He. Comfyui llm party: 一个用于 llm 工具调用的开源项目，2024。GitHub 仓库，[https://github.com/heshengtao/comfyui_LLM_party](https://github.com/heshengtao/comfyui_LLM_party)。'
