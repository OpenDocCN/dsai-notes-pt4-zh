- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:13'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: State of What Art? A Call for Multi-Prompt LLM Evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.00595](https://ar5iv.labs.arxiv.org/html/2401.00595)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Moran Mizrahi^†   Guy Kaplan^†   Dan Malkin^†
  prefs: []
  type: TYPE_NORMAL
- en: Rotem Dror^‡   Dafna Shahaf^†   Gabriel Stanovsky^†
  prefs: []
  type: TYPE_NORMAL
- en: ^†School of Computer Science, The Hebrew University of Jerusalem
  prefs: []
  type: TYPE_NORMAL
- en: ^‡Department of Information Systems, University of Haifa
  prefs: []
  type: TYPE_NORMAL
- en: '{moranmiz, guykaplan, dan.malkinhueb, dshahaf, gabriel.stanovsky}@cs.huji.ac.il
      rdror@is.haifa.ac.il'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recent advances in LLMs have led to the development of various evaluation benchmarks.
    These benchmarks typically rely on a *single instruction template* per task. We
    create a large-scale collection of instruction paraphrases and comprehensively
    analyze the brittleness of results obtained via single-prompt evaluations across
    6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks. We
    find that different instruction templates lead to very different results, both
    in terms of absolute performance, as well as relative ranking. Instead, we propose
    a set of diverse metrics on multiple instruction paraphrases, specifically tailored
    for different use cases (e.g., LLM vs. downstream development), ensuring a more
    reliable and meaningful assessment of LLM capabilities. We show that our metrics
    provide new insights into the strengths and limitations of current LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/265cf541a9e74701388f04cf9c378916.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Evaluation of different OpenAI models on the homophones task from
    LMentry over four paraphrases of the instructions for the task. Each cluster of
    columns corresponds to a distinct *instruction template*, with its respective
    text detailed below the graph (words in bold indicate a sample-specific instantiation).
    Despite all instructions being semantically equivalent, both absolute performance
    and relative ranking vary widely.'
  prefs: []
  type: TYPE_NORMAL
- en: Recent years have seen an explosion of large language models (LLMs), which generalize
    to unseen tasks via natural language instructions. Various LLM evaluation benchmarks,
    such as BIG-bench and HELM, use a *single* instruction template per task, evaluating
    all models against it (Srivastava et al., [2022](#bib.bib29); Liang et al., [2022](#bib.bib20)).
    However, there could be a myriad of ways to phrase an instruction template for
    a given task; see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ State of What
    Art? A Call for Multi-Prompt LLM Evaluation") for examples of different templates
    for the task of recognizing homophones. Naturally, LLM performance depends on
    the chosen template.
  prefs: []
  type: TYPE_NORMAL
- en: In this work, we explore the question of *robustly comparing different models
    on a given task*. We first create a dataset of paraphrased instructions. To achieve
    this, we devise three automatic methods to paraphrase given instruction templates,
    based on recent prompting techniques such as chain-of-thought. We manually verify
    and filter a large collection of more than 175 paraphrases for different tasks
    (5K instruction paraphrases in total), which we make publicly available for future
    research.¹¹1[github.com/SLAB-NLP/Multi-Prompt-LLM-Evaluation](https://github.com/SLAB-NLP/Multi-Prompt-LLM-Evaluation)
  prefs: []
  type: TYPE_NORMAL
- en: Next, we use our dataset to perform a large scale statistical evaluation of
    over 6.5M instances, involving 20 different LLMs and 39 tasks from 3 benchmarks.
    We find that models perform very differently on different instruction paraphrases,
    both in terms of absolute and relative performance. Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation")
    shows an example of the performance of four models on four (semantically equivalent)
    prompts, with both absolute performance and relative ranking varying widely. At
    the extreme, there are instruction templates on which a model performs *the best*
    compared to other models, while on a semantically equivalent instruction the same
    model performed *the worst* (e.g., GPT-3.5-Turbo on $P_{1}$). Subsequently, we
    argue that *very little can be said* on either absolute or relative performance
    based on the common practice of single-instruction evaluation (which may partially
    explain why some models seem less accurate in practice than their formal evaluation
    may suggest).
  prefs: []
  type: TYPE_NORMAL
- en: Note that while the claim that evaluating against a single instruction template
    leads to brittle results is not surprising per se, to the best of our knowledge
    it has never been subjected to rigorous empirical testing before.
  prefs: []
  type: TYPE_NORMAL
- en: To address the limitations of single-instruction evaluation, we propose to take
    a step back and consider multi-instruction evaluation metrics which are closely
    tied to real-world use cases of LLMs. We argue that different use cases should
    entail different evaluation metrics. For example, LLM developers may be interested
    in measuring the *robustness of performance* across multiple instruction templates,
    which we formulate as the average performance across a large collection of instructions.
    In contrast, when focusing on a downstream task, different models may be better
    compared according to their corresponding *top-performing* instruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate 20 LLMs with our metrics, finding that their absolute and relative
    performance differ from those obtained with the benchmarks’ original instruction
    templates. We demonstrate that different models excel in different metrics: For
    instance, in the LMentry benchmark, LLaMA-based models are comparable to T5-based
    models when looking at top-performing instructions. However, these models lag
    behind when average performance is considered, due to poor performance on a large
    number of paraphrases. We also show that our automatic paraphrasing method is
    effective, and there is no need to manually verify the paraphrases.'
  prefs: []
  type: TYPE_NORMAL
- en: Our results suggest that future work should choose the evaluation metric based
    on the *extrinsic needs* of the evaluators. We hope that our work will help spur
    more consistency and comparability in LLM evaluation, which is strongly tied to
    real-world usage of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background and Definitions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Below we survey how generalization to a new task format is evaluated and compared
    between LLMs, finding that this is normally done by testing performance on a single
    (or very few) task instruction templates. In the rest of the paper, we will argue
    that such practice leads to brittle results which are not well-suited for real-world
    use of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Task instruction templates.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Following Mishra et al. ([2021](#bib.bib22)); Chung et al. ([2022](#bib.bib4)),
    we separate between task instruction, samples, and input-output exemplars which
    may be provided during in-context learning. We define an *instruction template*
    for a given task as a string with placeholders where the input samples are to
    be inserted. As seen in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ State of
    What Art? A Call for Multi-Prompt LLM Evaluation"), the same task can be described
    using different task instruction templates.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation benchmarks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Several recent efforts aim to standardize LLM evaluation. Notable examples include
    MMLU (Hendrycks et al., [2020](#bib.bib13)), BIG-bench (Srivastava et al., [2022](#bib.bib29);
    Suzgun et al., [2022](#bib.bib31)), and HELM (Liang et al., [2022](#bib.bib20)).
    In all of these, each task has a single instruction template, against which all
    models are evaluated. Another benchmark, LMentry (Efrat et al., [2022](#bib.bib9)),
    reports models’ average performance on three instruction templates. The instruction
    templates are provided with these benchmarks, allowing new models to be tested
    against the same template.
  prefs: []
  type: TYPE_NORMAL
- en: Sadly, it is also common practice to report results based on a single instruction
    template *without* making it publicly available (e.g., LLaMA (Touvron et al.,
    [2023](#bib.bib34)), PALM (Chowdhery et al., [2022](#bib.bib3)), GPT-4 (OpenAI,
    [2023](#bib.bib24)), and Gemini (Google, [2023](#bib.bib11))). This exacerbates
    the challenge of meaningful comparative evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt robustness.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Related to this study is a line of work measuring LLM’s robustness to prompt
    (or instruction template) modifications. Unlike our work, these typically aim
    to measure model performance against *adversarial* paraphrasing approaches. PromptBench (Zhu
    et al., [2023](#bib.bib40)) measures performance on erroneous instructions (e.g.,
    instructions written by non-native English speakers). They then compare performance
    on perturbed instructions vs. the benchmark’s original instructions, which are
    considered the gold-standard reference. Gu et al. ([2022](#bib.bib12)) examined
    a single LLM’s robustness under various instruction perturbations, including word-,
    sentence-, and instruction-level changes. Sun et al. ([2023](#bib.bib30)) show
    that LLMs perform better on instructions they have seen in training (BIG-bench
    Lite benchmark), compared to manual paraphrases. We later incorporate their manual
    paraphrases in our evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to works on prompt robustness, our scope is wider. We analyze the
    impact of the choice of prompt in terms of both absolute and relative model performance,
    covering a wide range of models and several different metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we describe the tasks and models which we evaluate in this work.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate 39 diverse tasks from three evaluation benchmarks, as itemized below,
    and summarized in Table [6](#A1.T6 "Table 6 ‣ A.1 Tasks - Additional Details ‣
    Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation")
    in the Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: 10 tasks from LMentry Efrat et al. ([2022](#bib.bib9)).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LMentry consists of simple linguistic tasks (e.g., “write a word that doesn’t
    contain the letter $l$”), each accompanied by three associated instruction templates.
    The tasks are designed to capture explainable and controllable linguistic phenomena.
    We choose 10 tasks from LMentry that received the lowest scores in the original
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: 14 tasks from BIG-bench Lite (BBL; Srivastava et al., [2022](#bib.bib29)).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: These cover multiple knowledge domains, sampled from the larger BIG-Bench benchmark (bench
    authors, [2023](#bib.bib2)). In particular, we focus on a set of 14 tasks studied
    recently by Sun et al. ([2023](#bib.bib30)). Each task in BBL is associated with
    a single instruction template.
  prefs: []
  type: TYPE_NORMAL
- en: 15 tasks from BIG-bench Hard (BBH; Suzgun et al., [2022](#bib.bib31)).
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This is another curated subset of BIG-bench, containing particularly challenging
    tasks on which LLM underperform the average human-rater score. We take the set
    of 15 classification and multiple choice tasks from BBH to ease the evaluation
    protocol. Each task in BBH is associated with a single instruction template.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Size | Base Model | # Params |'
  prefs: []
  type: TYPE_TB
- en: '| Flan-T5 | Small | T5 | 80M |'
  prefs: []
  type: TYPE_TB
- en: '| Base | 250M |'
  prefs: []
  type: TYPE_TB
- en: '| Large | 780M |'
  prefs: []
  type: TYPE_TB
- en: '| XL | 3B |'
  prefs: []
  type: TYPE_TB
- en: '| XXL | 11B |'
  prefs: []
  type: TYPE_TB
- en: '| T0 | Small | T5 | 3B |'
  prefs: []
  type: TYPE_TB
- en: '| T0pp | 11B |'
  prefs: []
  type: TYPE_TB
- en: '| Alpaca | Small | LLaMA | 7B |'
  prefs: []
  type: TYPE_TB
- en: '| Big | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna |  | LLaMA | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| Airoboros |  | LLaMA | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| UltraLM |  | LLaMA | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| Nous-Hermes |  | LLaMA | 13B |'
  prefs: []
  type: TYPE_TB
- en: '| Falcon-Instruct |  | Falcon | 7B |'
  prefs: []
  type: TYPE_TB
- en: '| MPT |  | MPT | 7B |'
  prefs: []
  type: TYPE_TB
- en: '| Minotaur |  | StarCoder Plus | 15B |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The different LLMs evaluated in this work, grouped by model family,
    along with their size, in number of parameters. All models were instruction-tuned.'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring performance.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We measure performance in the standard manner provided by each benchmark. In
    LMentry this is done with the official evaluation script, while in Big-Bench we
    use exact match evaluation. We note that while this evaluation is somewhat strict,
    we believe that it is also fair and straightforward.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As shown in Table [1](#S3.T1 "Table 1 ‣ 15 tasks from BIG-bench Hard (BBH; Suzgun
    et al., 2022). ‣ 3.1 Tasks ‣ 3 Experimental Setup ‣ State of What Art? A Call
    for Multi-Prompt LLM Evaluation"), We evaluate 16 instruction-tuned LLMs from
    11 diverse model families (Chung et al., [2022](#bib.bib4); Sanh et al., [2021](#bib.bib27);
    Taori et al., [2023](#bib.bib32); Zheng et al., [2023](#bib.bib39); Durbin, [2023](#bib.bib8);
    Ding et al., [2023](#bib.bib7); NousResearch, [2023](#bib.bib23); Almazrouei et al.,
    [2023](#bib.bib1); Team, [2023](#bib.bib33); Collective, [2023](#bib.bib5)). We
    refrain from including any closed API-based models (e.g., OpenAI models) in our
    main evaluation for two reasons. First, using them at scale is an expensive prospect,
    for example, running our entire evaluation suite on GPT-4 will cost up to 2500
    USD. Second, and more importantly, the closed API for these models reportedly
    manipulates the input prompts in an undisclosed manner (e.g., wrapping them with
    meta-prompts, or rerouting to other models) (Rao et al., [2023](#bib.bib26)) which
    interferes with our evaluation. We do however perform a small-scale evaluation
    of OpenAI models in Section [7](#S7 "7 Small-Scale Evaluation of OpenAI Models
    on Prompt Paraphrasing ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation")
    to show that they are also sensitive to prompt paraphrasing.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Single-Prompt Evaluation Leads to Inconsistent Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the previous section, a common practice in LLM evaluation is
    to evaluate different models against a single instruction template. In this section,
    we will show that this approach is quite brittle. Indeed, a simple rephrasing
    of the instruction template can lead to drastic changes in absolute model performance
    as well as its relative ranking among other models.
  prefs: []
  type: TYPE_NORMAL
- en: To show this, in Section [4.1](#S4.SS1 "4.1 Paraphrasing Instruction Templates
    ‣ 4 Single-Prompt Evaluation Leads to Inconsistent Results ‣ State of What Art?
    A Call for Multi-Prompt LLM Evaluation") we create a large number of instruction
    paraphrases for each of our tasks. This is achieved automatically with the aid
    of an LLM and verified by human annotators to reduce noise. Then, in Section [4.2](#S4.SS2
    "4.2 Quantifying Performance Variance due to Instruction Paraphrasing ‣ 4 Single-Prompt
    Evaluation Leads to Inconsistent Results ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation"), we statistically analyze the performance of various LLMs against
    these instruction templates and quantify the variation in model performance and
    ranking.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Paraphrasing Instruction Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We use three prompting methods which were found useful in previous work: (1)
    instruction template rephrasing: asking an LLM to rephrase a seed prompt (Lester
    et al., [2021](#bib.bib19); Gonen et al., [2022](#bib.bib10); Honovich et al.,
    [2022a](#bib.bib14)); (2) Chain-of-Thought prompting (Wei et al., [2022](#bib.bib37)):
    we provided the model with a sequence of steps in which the model is asked first
    to produce a task description, and then to generate various instruction templates
    for the task; and (3) Gradual template generation: inspired by Honovich et al.
    ([2022b](#bib.bib15)), we split the COT approach into three LLM calls. The first
    for generating a task description from a seed instruction template, the second
    for generating instruction provided by input-output examples, and the third for
    processing the instruction and examples into an instruction template. See more
    details about these approaches in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: We use the original instruction templates for each of our tasks to seed these
    three generation methods, resulting on average in more than 200 automatically-generated
    instruction template paraphrases for each of our tasks (see Table [2](#S4.T2 "Table
    2 ‣ 4.1 Paraphrasing Instruction Templates ‣ 4 Single-Prompt Evaluation Leads
    to Inconsistent Results ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation")).
    We make this collection, as well as the code used to generate it, publicly available
    for reproducibility and to enable future work.
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | Method |'
  prefs: []
  type: TYPE_TB
- en: '&#124; #Automatic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paraphrases &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; #Correct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paraphrases &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ratio &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LMentry |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2429 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 461 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1286 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 652 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2186 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 408 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1234 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 514 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 90.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 88.50% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 95.96% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 78.83% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| BBH |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2615 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 734 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 775 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1091 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2209 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 627 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 630 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 937 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 84.47% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 85.42% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 81.29% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 85.88% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Manual validation and filtering of automatic instruction paraphrases
    generated for LMentry and BBH, showing percentages of valid paraphrases.'
  prefs: []
  type: TYPE_NORMAL
- en: Manual validation and filtering of automatic instruction paraphrases.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We manually verify and filter all of the automatically generated paraphrases.We
    found that 90% of the generated paraphrases created for LMentry were correct,
    and roughly 84% of the paraphrases for BBH were correct. See Table [2](#S4.T2
    "Table 2 ‣ 4.1 Paraphrasing Instruction Templates ‣ 4 Single-Prompt Evaluation
    Leads to Inconsistent Results ‣ State of What Art? A Call for Multi-Prompt LLM
    Evaluation") for a fine-grained distribution across the different generation metrics.
    On average, this process yields more than 175 validated instruction paraphrases
    per task across LMentry and BBH, which we will subsequently use to quantify peformance
    variability due to instruction template paraphrasing.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Quantifying Performance Variance due to Instruction Paraphrasing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We leverage the collection of validated instruction paraphrases to show that
    model performance varies widely on different instruction templates, both at the
    individual model performance, as well as in relative model ranking. As we argue
    below, our main finding is that the common approach of evaluating against a single
    instruction template is inconsistent and unstable, leading to contradicting results.
  prefs: []
  type: TYPE_NORMAL
- en: Instance sampling and prompt construction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Evaluating LLMs can become prohibitively expensive with the increase of the
    number of samples, datasets, models, and instruction templates (Perlitz et al.,
    [2023](#bib.bib25)). We focus on a large number of tasks, models, and instruction
    paraphrases. Hence, to make our evaluation feasible, this comes at the expense
    of the number of samples per task. Concretely, we evaluate each instruction template
    on a randomly selected subset of 100 task samples. Furthermore, we found that
    all models struggle on BBH, beyond the point of meaningful comparison. To address
    this, we evaluate 11 out of the 16 models on it (the bigger ones in terms of number
    of parameters), and we add an example of the prediction format to all instruction
    template paraphrases. Examining the effect of few-shot learning is beyond the
    scope of this paper, however, Sclar et al. ([2023](#bib.bib28)) recently observed
    similar performance sensibility when introducing varying number of in-context
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: '| Tasks | Kendall’s W | Friedman p |'
  prefs: []
  type: TYPE_TB
- en: '| LMentry |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|   not containing | .271 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   word before | .367 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   first alphabet | .436 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   less letters | .485 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   rhyming word | .496 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   ends with word | .518 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   homophones | .518 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   all words | .522 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   any words | .527 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '|   more letters | .540 (weak) | 0.0* |'
  prefs: []
  type: TYPE_TB
- en: '| BIG-bench Hard |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|   recommendations | .628 (medium) | .897 |'
  prefs: []
  type: TYPE_TB
- en: '|   formal fallacies | .704 (medium) | 5.6E-13 |'
  prefs: []
  type: TYPE_TB
- en: '|   geometric shapes | .710 (medium) | .167 |'
  prefs: []
  type: TYPE_TB
- en: '|   hyperbaton | .730 (medium) | 1.0E-4 |'
  prefs: []
  type: TYPE_TB
- en: '|   logical deduction 3 | .740 (medium) | 4.9E-16 |'
  prefs: []
  type: TYPE_TB
- en: '|   disambiguation qa | .764 (medium) | 2.1E-17 |'
  prefs: []
  type: TYPE_TB
- en: '|   ruin names | .776 (medium) | .366 |'
  prefs: []
  type: TYPE_TB
- en: '|   logical deduction 7 | .778 (medium) | 1.4E-13 |'
  prefs: []
  type: TYPE_TB
- en: '|   translation error | .800 (medium) | 6.9E-9 |'
  prefs: []
  type: TYPE_TB
- en: '|   logical deduction 5 | .818 (medium) | 3.0E-9 |'
  prefs: []
  type: TYPE_TB
- en: '|   snarks | .823 (medium) | .604 |'
  prefs: []
  type: TYPE_TB
- en: '|   penguins in a table | .830 (medium) | 7.3E-15 |'
  prefs: []
  type: TYPE_TB
- en: '|   navigate | .838 (medium) | 5.6E-10 |'
  prefs: []
  type: TYPE_TB
- en: '|   causal judgement | .851 (strong) | 4.9E-7 |'
  prefs: []
  type: TYPE_TB
- en: '|   sports | .873 (strong) | 8.0E-13 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Kendall’s $W\in[0,1]$, indicating weak to moderate agreement. The
    p-values from Friedman test indicate significant differences between rankings
    of models when using different prompts. ^∗p-values of 0.0 represent statistical
    significance levels that are smaller than 1E-50.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e74f9f1e49552f5c20e033fd3e8d200e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Model performance and ranking induced by pairs of instruction templates
    that exhibit the minimal Kendall $\tau$ correlation on three different tasks (one
    for each benchmark). Models are consistently ordered across graphs to ease comparison
    of the ranking changes between each template pair.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a single-instruction template leads to brittle ranking.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compute Kendall’s $W:\mathbb{N}^{m\times n}\mapsto[0,1]$) and their mean
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W=\frac{12\sum_{i=1}^{n}(R_{i}-\bar{R})^{2}}{m^{2}(n^{3}-n)}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Kendall’s $W$, the lesser the rankings induced by different instructions agree.
  prefs: []
  type: TYPE_NORMAL
- en: The results (Table [3](#S4.T3 "Table 3 ‣ Instance sampling and prompt construction.
    ‣ 4.2 Quantifying Performance Variance due to Instruction Paraphrasing ‣ 4 Single-Prompt
    Evaluation Leads to Inconsistent Results ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation")) demonstrate that a single instruction template leads to unreliable
    rankings for many of the tasks, with 10 of the tasks exhibiting only slight to
    moderate ranking agreement, and only two exhibiting strong agreement. To complement
    the analysis, we performed Friedman test with tied data (Corder and Foreman, [2011](#bib.bib6)),
    showing that different instructions lead to statistically significant differences
    in performance for 21 out of the 25 tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Examples of differences in model ranking.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We illustrate the implications of such differences in Figure [2](#S4.F2 "Figure
    2 ‣ Instance sampling and prompt construction. ‣ 4.2 Quantifying Performance Variance
    due to Instruction Paraphrasing ‣ 4 Single-Prompt Evaluation Leads to Inconsistent
    Results ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation"). The three
    instruction template pairs are valid paraphrases, yet they lead to vastly different
    results. For example, T0pp ranks first on the BBH task using the first instruction
    template and only 9th using the second template. Similarly, Alpaca-13B and Alpaca-7B
    are in the *top* performing models on the LMentry task using the second instruction
    template, while they rank *last* in the first template.
  prefs: []
  type: TYPE_NORMAL
- en: 'We quantify the difference between two rankings with Kendall’s $\tau:\mathbb{N}^{n}\times\mathbb{N}^{n}\mapsto[-1,1]$
    LLMs, formally defined as (Kendall, [1945](#bib.bib17)):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tau_{b}=\frac{P-Q}{\sqrt{(P+Q+T)\cdot(P+Q+U)}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: Where $P$ indicating perfect disagreement).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix [A.4](#A1.SS4 "A.4 Comparing Different Instruction Templates with Kendall’s
    𝜏 Rank Disagreements ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation") presents examples of pairs of instruction templates that exhibit
    the minimal Kendall $\tau$, indicating mostly disagreeing LLM rankings.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/904dca52e2ceee67beb39efa2e2a11f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Model and task performance divergence, showing for each task in LMentry
    the number of standard deviations by which the performance of each model on the
    original instruction templates deviates from the averaged model performance. Dark
    red cells indicate substantial divergence values exceeding one standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: Absolute model performance varies widely on single-instruction templates.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Aside from vastly different relative model rankings, instruction template paraphrases
    often result in varying absolute model performances. To quantify this variance,
    we calculated divergence, defined as the number of standard deviations by which
    the performance, as assessed using the original instruction templates, deviates
    from the model’s average performance over all paraphrases.
  prefs: []
  type: TYPE_NORMAL
- en: The results in Figure [3](#S4.F3 "Figure 3 ‣ Examples of differences in model
    ranking. ‣ 4.2 Quantifying Performance Variance due to Instruction Paraphrasing
    ‣ 4 Single-Prompt Evaluation Leads to Inconsistent Results ‣ State of What Art?
    A Call for Multi-Prompt LLM Evaluation") reveal noticeable divergence for the
    LMentry benchmark, defined as surpassing one standard deviation (Kazmier et al.,
    [2003](#bib.bib16)). For instance, the performance of the Alpaca-13B with the
    original instruction templates outperformed its average performance by more than
    one standard deviation in 7 out of the 10 LMentry tasks. For lack of space, the
    figure does not depict the BBH benchmark, but similar patterns of divergence were
    observed there as well.
  prefs: []
  type: TYPE_NORMAL
- en: In line with Lou et al. ([2023](#bib.bib21)), we find that major differences
    in performance can occur even for very similar paraphrase pairs. For example,
    the Flan-T5-large model demonstrated an average performance degradation of 28%
    when changing the word ‘excludes’ to ‘lacks’, while the Flan-T5-XL model showed
    an average performance improvement of 46% on that same edit. See a comprehensive
    edit distance comparison in Appendix [A.5](#A1.SS5 "A.5 Model Performance Differences
    with Minimal Paraphrasing Edit Distance ‣ Appendix A Appendix ‣ State of What
    Art? A Call for Multi-Prompt LLM Evaluation").
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 LLMs are also Sensitive to Manual Paraphrases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is possible that the inconsistencies observed in our analyses stem from our
    automatic paraphrases. To address this, we extended our analysis with instruction
    paraphrases which were recently written by Sun et al. ([2023](#bib.bib30)) for
    the BBL tasks (see Table [6](#A1.T6 "Table 6 ‣ A.1 Tasks - Additional Details
    ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation")).
    These provide between 7 and 12 instruction templates per task. While originally
    annotated to examine overall model degradation on human written instructions,
    we reuse  Sun et al. ([2023](#bib.bib30))’s annotations to examine the change
    in model rankings and absolute performance.
  prefs: []
  type: TYPE_NORMAL
- en: Our analysis revealed similar inconsistencies as observed with automated paraphrases.
    See Table [13](#A1.T13 "Table 13 ‣ A.6 BBL Analysis ‣ Appendix A Appendix ‣ State
    of What Art? A Call for Multi-Prompt LLM Evaluation") in the Appendix for the
    Kendall’s W values for all BBL tasks, and Table [11](#A1.T11 "Table 11 ‣ A.4 Comparing
    Different Instruction Templates with Kendall’s 𝜏 Rank Disagreements ‣ Appendix
    A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation") for examples
    of pairs of instruction templates that exhibit the minimal Kendall $\tau$ correlations.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Different Use Cases Merit Different Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far we have shown that LLM performance is greatly affected by paraphrasing
    of instruction templates. This calls into question current evaluation practices,
    which typically rely on LLM performance on a single instruction template. In this
    section we explore ways to evaluate LLMs using a *diverse set of instruction templates*.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, we argue that the answer should depend on the *purpose of
    the evaluation*, and that different extrinsic needs should lead to different evaluation
    metrics, rather than striving for a coarse catch-all metric. We introduce a set
    of metrics, each tailored to specific scenarios and realistic user needs.
  prefs: []
  type: TYPE_NORMAL
- en: Notations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the following, $M$.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Maximum Performance Metric – For Particular Downstream Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define the maximum performance (MaxP) of a model $M$ to be the maximum individual
    instruction template performance this model achieves across all instruction templates:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MaxP(M,T,I_{T})=\max_{i\in I_{T}}\;\varepsilon(M,T,i)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Use case: This metric is useful for developers aiming to integrate an LLM into
    a specific downstream task and domain (for example, sentiment analysis in the
    news domain). In such cases, a user input is often embedded within a fixed instruction
    template. As such, it makes sense to find the best-performing instruction template
    for a given model (Wei et al., [2021](#bib.bib36)). To mitigate overfitting, it
    is sensible to identify it using a held-out sample.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Average Performance Metric – For LLM Developers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define the average performance (AvgP) of a model $M$ as the mean of the
    individual instruction template performances over all instruction templates for
    the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $AvgP(M,T,I_{T})=\frac{1}{&#124;I_{T}&#124;}\cdot\sum_{i\in I_{T}}\varepsilon(M,T,i)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Use case: Average prompt performance is useful for assessing model robustness
    to paraphrases. We believe this should be standard practice for LLM developers
    when presenting the performance of a new LLM on a range of tasks and prompt paraphrases
    (Workshop et al., [2022](#bib.bib38)), as it mitigates outliers in performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Combined Performance Score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the same way the F1 score combines precision and recall into a single metric,
    we propose a Combined Performance Score (CPS) that unites the maximum and average
    performance metrics to capture both peak capability and consistency of the model
    across prompts. To define CPS, we first introduce a model saturation score:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle Sat(M,T,I_{T})=1-(MaxP-AvgP)$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'This score measures how closely the model’s best performance aligns with its
    average performance. A high saturation score indicates that the model’s performance
    does not drop significantly for non-optimal instructions. Then, the CPS is calculated
    as the product of the model’s best performance ($MaxP$):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $CPS(M,T,I_{T})=Sat\cdot MaxP$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Use case: This metric is valuable for selecting a model for a suite of applications
    or a platform offering diverse tasks. For instance, when integrating an LLM into
    an application with user-visible prompts, such as a multi-functional chatbot,
    it is crucial for the model to be both effective (high $MaxP$). CPS facilitates
    identifying models that strike a balance between top-tier performance and consistent
    reliability across varying instruction templates.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Multi-Prompt Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0192b266ce1d909d160fc107a35ba276.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The performance of various models according to the metrics proposed
    in Section 4, evaluated on sample tasks from each of the three benchmarks. The
    name of the metric appears below each group of columns; height of a column represents
    value in *that specific metric*. The order of the columns (i.e., models) between
    groups is fixed, set according to decreasing performance on the original instruction
    templates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e8dcba891ef93b0b41e2cbafbaaf8e1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Percentage of correct paraphrases with accuracy higher than 5% in
    T5 models (blue) vs. LLaMA models (purple) on LMentry tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark | MaxP | AvgP | Combined |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LMentry | .963 | .978 | .948 |'
  prefs: []
  type: TYPE_TB
- en: '| BBH | .991 | .983 | .966 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Averaged Kendall’s Tau values comparing rankings before and after
    filtering incorrect paraphrases for each metric across all tasks (excluding “ends
    with word” for LMentry).'
  prefs: []
  type: TYPE_NORMAL
- en: In Figure [4](#S6.F4 "Figure 4 ‣ 6 Multi-Prompt Evaluation ‣ State of What Art?
    A Call for Multi-Prompt LLM Evaluation") we evaluate all our 16 models according
    to the metrics we proposed in the previous section, on sample tasks from each
    of the three benchmarks (full results for all tasks are available in our repository).
    We report several interesting observations.
  prefs: []
  type: TYPE_NORMAL
- en: First, we find that all aggregate metrics diverge from the performance on the
    original instruction templates. For the vast majority of the tasks in our study,
    the top three models determined by the original instruction templates were different
    from those which ranked first according to the average and maximum metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'More broadly, the rankings of models depend on the metric used. For instance,
    see Figure [4](#S6.F4 "Figure 4 ‣ 6 Multi-Prompt Evaluation ‣ State of What Art?
    A Call for Multi-Prompt LLM Evaluation") (top): In LMentry’s rhyming word task,
    Falcon-Instruct-7b and Vicuna-13b rank first according to $MaxP$, they tended
    to lag behind, due to extremely poor performance on a large number of paraphrases
    (see Figure [5](#S6.F5 "Figure 5 ‣ 6 Multi-Prompt Evaluation ‣ State of What Art?
    A Call for Multi-Prompt LLM Evaluation") for percentage of paraphrases that achieved
    over 5% accuracy).'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we found that noise stemming from automatic paraphrase generation has
    virtually no impact on metric-based model rankings. We compute Kendall’s $\tau$
    to compare model rankings before and after the manual removal of incorrect paraphrases.
    The results (Table [4](#S6.T4 "Table 4 ‣ 6 Multi-Prompt Evaluation ‣ State of
    What Art? A Call for Multi-Prompt LLM Evaluation")) show near-perfect to perfect
    agreement in rankings across all tasks, except for the “ends with word” task in
    LMentry. Upon examination, this seems to be mostly due to an error in LMentry’s
    evaluation script. These results suggest that it may be enough to compute our
    metrics over range of automatically-generated paraphrases, without having to manually
    verify them.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Small-Scale Evaluation of OpenAI Models on Prompt Paraphrasing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we perform a small-scale evaluation showing that API LLMs are
    also sensitive to instruction paraphrasing. Our evaluation focuses on four OpenAI
    models: davinci, text-davinci-002, text-davinci-003, and GPT-3.5-Turbo on the
    LMentry benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to budget constraints, we show that the performance of these models diverges
    significantly between the benchmark’s original instruction templates and a selection
    of paraphrases, in terms of both average and maximum metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating average performance.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To estimate the average performance of OpenAI models on a specific task, we
    adopted a randomized approach. For each task sample, we randomly selected a paraphrase
    from our collection, and evaluated the model’s response, scoring the entire set
    of task samples. To approximate average performance, this experiment was repeated
    20 times, determined by the data from our 16 open-source models.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating maximal performance.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To estimate which of the roughly 175 instruction templates per task performs
    the best for each model, we implemented a simple greedy search. Initially, we
    evaluated all paraphrases on 10 task instances, then narrowed down to the top
    100 instruction templates for another 10 instances. Finally, the top 10 instruction
    templates were evaluated on the remaining instances, and the template that performed
    the best was chosen to estimate the maximum performance.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Below we summarize the results of our evaluation of OpenAI models. The full
    details appear in Tables [21](#A1.T21 "Table 21 ‣ A.9 Small Scale Evaluation -
    OpenAI ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM
    Evaluation"), [22](#A1.T22 "Table 22 ‣ A.9 Small Scale Evaluation - OpenAI ‣ Appendix
    A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation"), [23](#A1.T23
    "Table 23 ‣ A.9 Small Scale Evaluation - OpenAI ‣ Appendix A Appendix ‣ State
    of What Art? A Call for Multi-Prompt LLM Evaluation"), and [24](#A1.T24 "Table
    24 ‣ A.9 Small Scale Evaluation - OpenAI ‣ Appendix A Appendix ‣ State of What
    Art? A Call for Multi-Prompt LLM Evaluation") and in our repository.⁰⁰footnotemark:
    0'
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI models are also sensitive to minor prompt variations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Minor changes in the phrasing of the instruction can lead to drastic performance
    changes for the OpenAI models in our experiment, similar to our findings in Section [4.2](#S4.SS2
    "4.2 Quantifying Performance Variance due to Instruction Paraphrasing ‣ 4 Single-Prompt
    Evaluation Leads to Inconsistent Results ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation") with smaller-scale LLMs. See representative examples in Table [5](#S7.T5
    "Table 5 ‣ Model rankings diverge between the different metrics and original instruction
    templates. ‣ 7.1 Results ‣ 7 Small-Scale Evaluation of OpenAI Models on Prompt
    Paraphrasing ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation"), showing
    nearly identical instruction template pairs resulting in notable variations in
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: Average multi-prompt performance is lower than that observed in the original
    benchmark instructions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In 72.5% of the cases, the performance of the original instruction templates
    was higher than the estimated average across all paraphrases. A prominent difference
    was observed particularly in the davinci model. For this model, the original prompts
    added, on average, 21 more accuracy points compared to the estimated average across
    all paraphrases.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e6ae0e369d270a0ff3d016fbc9cd156.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Comparison of the *maximum performance* of four OpenAI models using
    original prompts (in solid colors) vs. all prompt paraphrases (semi-transparent).
    Each group of columns corresponds to a different task in the LMentry benchmark.'
  prefs: []
  type: TYPE_NORMAL
- en: Original prompt performances fall below all paraphrases’ estimated maximum performance.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Figure [6](#S7.F6 "Figure 6 ‣ Average multi-prompt performance is lower than
    that observed in the original benchmark instructions. ‣ 7.1 Results ‣ 7 Small-Scale
    Evaluation of OpenAI Models on Prompt Paraphrasing ‣ State of What Art? A Call
    for Multi-Prompt LLM Evaluation") depicts maximum performance of the *original
    instructions* for four LMentry tasks in solid colors, with overlaid semi-transparent
    columns indicating the estimated maximum performance on *all paraphrases*. Notably,
    for text-davinci-002, we found paraphrases that improved its maximal accuracy
    performance above 90% for 8 out of 10 tasks. Across all four models, 26 out of
    40 differences were statistically significant according to the McNemar test (Table [25](#A1.T25
    "Table 25 ‣ A.9 Small Scale Evaluation - OpenAI ‣ Appendix A Appendix ‣ State
    of What Art? A Call for Multi-Prompt LLM Evaluation")).
  prefs: []
  type: TYPE_NORMAL
- en: Model rankings diverge between the different metrics and original instruction
    templates.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Similarly to our main evaluation, there were many mismatches between ranking
    on the original instruction templates and our metrics. Agreement was observed
    in only 5 out of 10 tasks for the average metric, and in 4 out of 10 tasks for
    the maximum metric.
  prefs: []
  type: TYPE_NORMAL
- en: '| Change | Model | P1 | Acc. | P2 | Acc. | Diff. |'
  prefs: []
  type: TYPE_TB
- en: '| {…} –> “{…}” | td002 | Which word has a greater number of letters, {word1}
    or {word2}? | .50 | Which word has a greater number of letters, “{word1}” or “{word2}”?
    | .23 | -0.27 |'
  prefs: []
  type: TYPE_TB
- en: '|  | td002 | Which of the words {word1} and {word2} is alphabetically first?
    | .54 | Which of the words “{word1}” and “{word2}” is alphabetically first? |
    .77 | +0.23 |'
  prefs: []
  type: TYPE_TB
- en: '|  | td003 | Which word has a greater number of letters, {word1} or {word2}?
    | .60 | Which word has a greater number of letters, “{word1}” or “{word2}”? |
    .14 | -0.46 |'
  prefs: []
  type: TYPE_TB
- en: '|  | td003 | Compare the length of {word1} and {word2} and tell me which one
    is shorter. | .39 | Compare the length of “{word1}” and “{word2}” and tell me
    which one is shorter. | .73 | +0.34 |'
  prefs: []
  type: TYPE_TB
- en: '|  | cgpt | Which word has a greater number of letters, {word1} or {word2}?
    | .55 | Which word has a greater number of letters, “{word1}” or “{word2}”? |
    .24 | -0.31 |'
  prefs: []
  type: TYPE_TB
- en: '|  | cgpt | Compare the length of {word1} and {word2}. Which one is longer?
    | .04 | Compare the length of “{word1}” and “{word2}”. Which one is longer? |
    .70 | +0.66 |'
  prefs: []
  type: TYPE_TB
- en: '| ‘,’ –> ‘:’ | td002 | Which word is a rhyme for “{query}”, “{word1}” or “{word2}”?
    | .08 | Which word is a rhyme for “{query}”: “{word1}” or “{word2}”? | .85 | +0.77
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | td003 | Which word is a rhyme for “{query}”, “{word1}” or “{word2}”? |
    .48 | Which word is a rhyme for “{query}”: “{word1}” or “{word2}”? | .90 | +0.42
    |'
  prefs: []
  type: TYPE_TB
- en: '| ‘,’ –> ‘-’ | td002 | Which word rhymes with “{query}”, “{word1}” or “{word2}”?
    | .06 | Which word rhymes with “{query}” - “{word1}” or “{word2}”? | .73 | +0.67
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | td003 | Which word rhymes with “{query}”, “{word1}” or “{word2}”? | .17
    | Which word rhymes with “{query}” - “{word1}” or “{word2}”? | .60 | +0.43 |'
  prefs: []
  type: TYPE_TB
- en: '| the –> a | td002 | What is the word that rhymes with “{query}” - “{word1}”
    or “{word2}”? | .03 | What is a word that rhymes with “{query}” - “{word1}” or
    “{word2}”? | .78 | +0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| which –> what | td002 | Which word rhymes with “{query}” - “{word1}” or “{word2}”?
    | .73 | What word rhymes with “{query}” - “{word1}” or “{word2}”? | .82 | +0.09
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | td003 | Which word rhymes with “{query}” - “{word1}” or “{word2}”? | .60
    | What word rhymes with “{query}” - “{word1}” or “{word2}”? | .15 | -0.45 |'
  prefs: []
  type: TYPE_TB
- en: '| word –> term | td002 | Create a word that excludes the letter “{letter}”.
    | .54 | Create a term that excludes the letter “{letter}”. | .04 | -0.50 |'
  prefs: []
  type: TYPE_TB
- en: '|  | td003 | Create a word that excludes the letter “{letter}”. | .96 | Create
    a term that excludes the letter “{letter}”. | .58 | -0.38 |'
  prefs: []
  type: TYPE_TB
- en: '|  | cgpt | Create a word that excludes the letter “{letter}”. | .81 | Create
    a term that excludes the letter “{letter}”. | .42 | -0.39 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Minimal distance paraphrase pairs from LMentry with large performance
    differences in OpenAI models.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our work is part of an emerging trend highlighting the many challenges standing
    in the way of meaningful, scalable, and reproducible evaluation of large language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Perlitz et al. ([2023](#bib.bib25)) focus on the rising cost of exhaustive evaluation
    of LLMs on large number of samples. They notice that as models become larger,
    the cost of running them at scale can become prohibitively expensive, even during
    inference. To help mitigate this problem, they develop methods for choosing subsets
    of the test data which are expected to be a good representative of the whole.
    We find that single prompt evaluation is not a good representative of LLMs average
    performance, and instead suggest evaluating on many instruction templates per
    sample, which further increases the evaluation cost. An interesting avenue for
    future work can extend Perlitz et al. ([2023](#bib.bib25))’s approach to also
    include various instruction templates, thus efficiently approximating our suggested
    evaluation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Sclar et al. ([2023](#bib.bib28)) show that LLMs are sensitive to *prompt formatting*.
    These are minor prompt design choices, such as the addition or omission of punctuation
    marks. They create a large pool of instruction paraphrases, ensuring that paraphrases
    maintain the meaning of the original prompt. We notice a similar phenomenon, albeit
    more anecdotally, when our automatic paraphrasing techniques incidentally produce
    minor changes in formatting (Table [5](#S7.T5 "Table 5 ‣ Model rankings diverge
    between the different metrics and original instruction templates. ‣ 7.1 Results
    ‣ 7 Small-Scale Evaluation of OpenAI Models on Prompt Paraphrasing ‣ State of
    What Art? A Call for Multi-Prompt LLM Evaluation")). Finally, Voronov et al. ([2024](#bib.bib35))
    shows that LLMs are sensitive to how in-context examples are presented and formatted.
    For example, they vary the manner in which each input-output is separated, and
    test how such choices interact with the phrasing of the instruction template,
    the number of demonstrations, or the model size.
  prefs: []
  type: TYPE_NORMAL
- en: Our work distinguishes itself as the first to systematically explore the impact
    of a broad spectrum of prompt paraphrases across various benchmarks and tasks
    on multiple models, coupled with a statistical analysis of the absolute and relative
    variations in evaluations. Furthermore, we introduce a suite of metrics specifically
    designed to align with the practical applications of large language models.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our research highlights the sensitivity of large language models (LLMs) to prompt
    paraphrasing, challenging the adequacy of single-prompt evaluations. We propose
    alternative evaluation metrics that use a diverse set of instruction templates
    for each task, designed for more robust and meaningful LLM evaluation. For example,
    LLM developers may be interested in measuring the robustness of performance across
    multiple prompts, which we propose to evaluate as the average across a large collection
    of prompts. In contrast, when developing a downstream model, different models
    should be compared according to their corresponding top-performing prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating based on these metrics underscores the necessity for nuanced evaluation
    methods, revealing notable differences in absolute performance and relative model
    rankings compared to traditional evaluations. We hope that our work will help
    spur more consistency and comparability in LLM evaluation which is strongly coupled
    to real-world LLM uses. We believe this shift is crucial for accurately understanding
    and leveraging the true capabilities of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel
    Heslow, Julien Launay, Quentin Malartic, et al. 2023. Falcon-40b: an open large
    language model with state-of-the-art performance. Technical report, Technical
    report, Technology Innovation Institute.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'bench authors (2023) BIG bench authors. 2023. [Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models](https://openreview.net/forum?id=uyTL5Bvosj).
    *Transactions on Machine Learning Research*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways.
    *arXiv preprint arXiv:2204.02311*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collective (2023) OpenAccess AI Collective. 2023. [Minotaur](https://huggingface.co/openaccess-ai-collective/minotaur-15b).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Corder and Foreman (2011) Gregory W Corder and Dale I Foreman. 2011. Nonparametric
    statistics for non-statisticians.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding
    Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. [Enhancing chat language models
    by scaling high-quality instructional conversations](http://arxiv.org/abs/2305.14233).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Durbin (2023) Jon Durbin. 2023. [Airoboros](https://github.com/jondurbin/airoboros).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Efrat et al. (2022) Avia Efrat, Or Honovich, and Omer Levy. 2022. Lmentry:
    A language model benchmark of elementary language tasks. *arXiv preprint arXiv:2211.02069*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gonen et al. (2022) Hila Gonen, Srini Iyer, Terra Blevins, Noah A Smith, and
    Luke Zettlemoyer. 2022. Demystifying prompts in language models via perplexity
    estimation. *arXiv preprint arXiv:2212.04037*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google (2023) Gemini Team Google. 2023. [Gemini: A family of highly capable
    multimodal models](https://api.semanticscholar.org/CorpusID:266361876).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2022) Jiasheng Gu, Hanzi Xu, Liangyu Nie, and Wenpeng Yin. 2022.
    Robustness of learning from task instructions. *arXiv preprint arXiv:2212.03813*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. *arXiv preprint arXiv:2009.03300*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Honovich et al. (2022a) Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
    2022a. Unnatural instructions: Tuning language models with (almost) no human labor.
    *arXiv preprint arXiv:2212.09689*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Honovich et al. (2022b) Or Honovich, Uri Shaham, Samuel R Bowman, and Omer
    Levy. 2022b. Instruction induction: From few examples to natural language task
    descriptions. *arXiv preprint arXiv:2205.10782*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kazmier et al. (2003) Leonard J Kazmier, Michael K Staton, Daniel L Fulks,
    et al. 2003. Business statistics: based on schaums outline of theory and problems
    of business statistics, by leonard j. kazmier. Technical report, McGraw-Hill.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall (1945) Maurice G Kendall. 1945. The treatment of ties in ranking problems.
    *Biometrika*, 33(3):239–251.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall and Smith (1939) Maurice G Kendall and B Babington Smith. 1939. The
    problem of m rankings. *The annals of mathematical statistics*, 10(3):275–287.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras,
    Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya
    Kumar, et al. 2022. Holistic evaluation of language models. *arXiv preprint arXiv:2211.09110*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lou et al. (2023) Renze Lou, Kai Zhang, and Wenpeng Yin. 2023. Is prompt all
    you need? no. a comprehensive and broader view of instruction learning. *arXiv
    preprint arXiv:2303.10475*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishra et al. (2021) Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh
    Hajishirzi. 2021. Cross-task generalization via natural language crowdsourcing
    instructions. *arXiv preprint arXiv:2104.08773*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NousResearch (2023) NousResearch. 2023. [Nous-hermes](https://huggingface.co/NousResearch/Nous-Hermes-13b).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](https://api.semanticscholar.org/CorpusID:257532815).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perlitz et al. (2023) Yotam Perlitz, Elron Bandel, Ariel Gera, Ofir Arviv, Liat
    Ein-Dor, Eyal Shnarch, Noam Slonim, Michal Shmueli-Scheuer, and Leshem Choshen.
    2023. [Efficient benchmarking (of language models)](https://api.semanticscholar.org/CorpusID:261076362).
    *ArXiv*, abs/2308.11696.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rao et al. (2023) Abhinav Rao, Sachin Vashistha, Atharva Naik, Somak Aditya,
    and Monojit Choudhury. 2023. [Tricking llms into disobedience: Understanding,
    analyzing, and preventing jailbreaks](https://api.semanticscholar.org/CorpusID:258865314).
    *ArXiv*, abs/2305.14965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanh et al. (2021) Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach,
    Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao,
    Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma
    Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti
    Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen,
    Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos
    Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan
    Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush.
    2021. [Multitask prompted training enables zero-shot task generalization](http://arxiv.org/abs/2110.08207).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sclar et al. (2023) Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr.
    2023. [Quantifying language models’ sensitivity to spurious features in prompt
    design or: How i learned to start worrying about prompt formatting](https://api.semanticscholar.org/CorpusID:264172710).
    *ArXiv*, abs/2310.11324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al. (2022) Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao,
    Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya
    Gupta, Adrià Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying
    and extrapolating the capabilities of language models. *arXiv preprint arXiv:2206.04615*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2023) Jiuding Sun, Chantal Shaib, and Byron C Wallace. 2023. Evaluating
    the zero-shot robustness of instruction-tuned language models. *arXiv preprint
    arXiv:2306.11270*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
    Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can
    solve them. *arXiv preprint arXiv:2210.09261*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto. 2023. Alpaca:
    A strong, replicable instruction-following model. *Stanford Center for Research
    on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html*, 3(6):7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Team (2023) MosaicML NLP Team. 2023. [Introducing mpt-7b: A new standard for
    open-source, commercially usable llms](www.mosaicml.com/blog/mpt-7b). Accessed:
    2023-05-05.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Voronov et al. (2024) Anton Voronov, Lena Wolf, and Max Ryabinin. 2024. [Mind
    your format: Towards consistent evaluation of in-context learning improvements](https://api.semanticscholar.org/CorpusID:266977441).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. 2021. Finetuned language
    models are zero-shot learners. *arXiv preprint arXiv:2109.01652*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workshop et al. (2022) BigScience Workshop, Teven Le Scao, Angela Fan, Christopher
    Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha
    Luccioni, François Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. [Judging llm-as-a-judge with
    mt-bench and chatbot arena](http://arxiv.org/abs/2306.05685).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao
    Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, et al.
    2023. Promptbench: Towards evaluating the robustness of large language models
    on adversarial prompts. *arXiv preprint arXiv:2306.04528*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Tasks - Additional Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Benchmark & Task | Instruction Template |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LMentry |  |'
  prefs: []
  type: TYPE_TB
- en: '|     all words from category | Q: Are all the words {$words$}? Answer either
    “yes” or “no”. A: |'
  prefs: []
  type: TYPE_TB
- en: '|     any words from category | Q: Does the list [{$words$}? Answer either
    “yes” or “no”. A: |'
  prefs: []
  type: TYPE_TB
- en: '|     ends with word | Write a sentence that ends with the word “{$word$}”:
    |'
  prefs: []
  type: TYPE_TB
- en: '|     first alphabetically | Q: In an alphabetical order, which word comes
    first, “{$word1$}”? A: |'
  prefs: []
  type: TYPE_TB
- en: '|     homophones | Q: Which word sounds like the word “{$query$}”? A: |'
  prefs: []
  type: TYPE_TB
- en: '|     less letters | Q: Which word is shorter, “{$word1$}”? A: |'
  prefs: []
  type: TYPE_TB
- en: '|     more letters | Q: Which word has more letters, “{$word1$}”? A: |'
  prefs: []
  type: TYPE_TB
- en: '|     rhyming word | Q: Which is a rhyme of the word “{$query$}”? A: |'
  prefs: []
  type: TYPE_TB
- en: '|     word before | Q: Which word comes right before “{$word$}”? A: |'
  prefs: []
  type: TYPE_TB
- en: '|     word not containing | Write a word that doesn’t contain the letter “{$letter$}”:
    |'
  prefs: []
  type: TYPE_TB
- en: '| BIG-bench Lite |  |'
  prefs: []
  type: TYPE_TB
- en: '|     bbq lite | {$input$} Answer: |'
  prefs: []
  type: TYPE_TB
- en: '|     code line description | Python code: {$input$} English language description:
    |'
  prefs: []
  type: TYPE_TB
- en: '|     conceptual combinations | {$input$} Answer: |'
  prefs: []
  type: TYPE_TB
- en: '|     hindu knowledge | Q: {$input$} A: |'
  prefs: []
  type: TYPE_TB
- en: '|     known unknown | {$input$} |'
  prefs: []
  type: TYPE_TB
- en: '|     language identification | Given a sentence, select the correct language
    among the choices Sentence: {$input$} … Language: |'
  prefs: []
  type: TYPE_TB
- en: '|     logic grid puzzle | {$input$} Answer: |'
  prefs: []
  type: TYPE_TB
- en: '|     logical deduction | The following paragraphs each describe a set of three
    objects arranged in a fixed order. The statements are logically consistent within
    each paragraph. {$input$} |'
  prefs: []
  type: TYPE_TB
- en: '|     novel concepts | Let’s do some find-the-common-concept problems. In these
    problems, your goal is to identify the underlying concept or theme that relates
    the things listed. Make sure to answer carefully. {$input$} Answer: |'
  prefs: []
  type: TYPE_TB
- en: '|     play dialog | The following transcripts of dialogues have been taken
    from Shakespeare plays, but the transcripts do not say who said what. Your task
    is to identify whether the sentences in question were spoken by the same or different
    people. Dialogue: {$input$} Answer: |'
  prefs: []
  type: TYPE_TB
- en: '|     strange stories | Context: {$input$} A: |'
  prefs: []
  type: TYPE_TB
- en: '|     strategic qa | Q: {$input$} A: |'
  prefs: []
  type: TYPE_TB
- en: '|     vitaminc fact verification | Based only on the information contained
    in a brief quote from Wikipedia, answer whether the related claim is True, False
    or Neither. Use Neither when the Wikipedia quote does not provide the necessary
    information to resolve the question. Passage: {$input$} True, False, or Neither?
    |'
  prefs: []
  type: TYPE_TB
- en: '|     winowhy | Please answer the following questions about which words certain
    pronouns refer to. {$input$} The above reasoning is |'
  prefs: []
  type: TYPE_TB
- en: '| BIG-bench Hard |  |'
  prefs: []
  type: TYPE_TB
- en: '|     causal judgement | How would a typical person answer each of the following
    questions about causation? |'
  prefs: []
  type: TYPE_TB
- en: '|     disambiguation qa | In the following sentences, explain the antecedent
    of the pronoun (which thing the pronoun refers to), or state that it is ambiguous.
    |'
  prefs: []
  type: TYPE_TB
- en: '|     formal fallacies | Is the argument, given the explicitly stated premises,
    deductively valid or invalid? |'
  prefs: []
  type: TYPE_TB
- en: '|     geometric shapes | This SVG path element {$svg\_path\_element$} |'
  prefs: []
  type: TYPE_TB
- en: '|     hyperbaton | Which sentence has the correct adjective order: |'
  prefs: []
  type: TYPE_TB
- en: '|     logical deduction five objects | QThe following paragraphs each describe
    a set of five objects arranged in a fixed order. The statements are logically
    consistent within each paragraph. |'
  prefs: []
  type: TYPE_TB
- en: '|     logical deduction seven objects | The following paragraphs each describe
    a set of seven objects arranged in a fixed order. The statements are logically
    consistent within each paragraph |'
  prefs: []
  type: TYPE_TB
- en: '|     logical deduction three objects | The following paragraphs each describe
    a set of three objects arranged in a fixed order. The statements are logically
    consistent within each paragraph. |'
  prefs: []
  type: TYPE_TB
- en: '|     movie recommendation | Find a movie similar to {$movie\_list$} |'
  prefs: []
  type: TYPE_TB
- en: '|     navigate | If you follow these instructions, do you return to the starting
    point? |'
  prefs: []
  type: TYPE_TB
- en: '|     penguins in a table | Here is a table where the first line is a header
    and each subsequent line is a penguin: name, age, height (cm), weight (kg) {$question$}
    |'
  prefs: []
  type: TYPE_TB
- en: '|     ruin names | Q: Which of the following is a humorous edit of this artist
    or movie name: ’{$artist\_or\_movie\_name$}’? |'
  prefs: []
  type: TYPE_TB
- en: '|     salient translation error detection | The following translations from
    German to English contain a particular error. That error will be one of the following
    types: $\ldots$ Please identify that error. |'
  prefs: []
  type: TYPE_TB
- en: '|     snarks | Which statement is sarcastic? |'
  prefs: []
  type: TYPE_TB
- en: '|     sports understanding | Q: Is the following sentence plausible? |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: The 39 tasks used in this paper, along with the benchmarks from which
    they were taken and an example task instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#A1.T6 "Table 6 ‣ A.1 Tasks - Additional Details ‣ Appendix A Appendix
    ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation") presents an overview
    of the 39 tasks from the 3 benchmarks discussed in this paper: LMentry, BIG-bench
    Lite, and BIG-bench Hard. These benchmarks include 10, 14, and 15 tasks from each,
    respectively. The table also provides an example task instruction for each task.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Process of Generating Prompt Paraphrases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our process for generating paraphrases of instruction templates is depicted
    with an example in Figure [7](#A1.F7 "Figure 7 ‣ A.2 Process of Generating Prompt
    Paraphrases ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc99eb7aee8489517e9125fce71c78d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Our process for automatically generating paraphrases of instruction
    templates, using the ’snarks’ task from the BBH benchmark as an example. We input
    task information from the benchmark, including basic details, the original instruction
    template, and a few-shot exemplar, into various meta-prompts tailored to different
    generation methods (prompt rephrasing, CoT prompting, or gradual generation).
    Then, we feed these meta-prompts into gpt-3.5-turbo to create new instruction
    templates for the given task. Notably, in the gradual generation method, gpt-3.5-turbo
    is utilized twice: initially to generate a detailed task description, and subsequently
    to derive a new instruction template from it.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Paraphrases Correctness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tables [7](#A1.T7 "Table 7 ‣ A.3 Paraphrases Correctness ‣ Appendix A Appendix
    ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation") and [8](#A1.T8 "Table
    8 ‣ A.3 Paraphrases Correctness ‣ Appendix A Appendix ‣ State of What Art? A Call
    for Multi-Prompt LLM Evaluation") present the percentages of correct paraphrases
    that were generated by the 3 prompt-generating methods presented in the paper
    for LMentry and BBH. The tables also depict the average model accuracy and standard
    deviations as measured for only the correct paraphrases across all LLMs. The correct
    paraphrases were identified by one of the authors of this paper. Table [14](#A1.T14
    "Table 14 ‣ A.6 BBL Analysis ‣ Appendix A Appendix ‣ State of What Art? A Call
    for Multi-Prompt LLM Evaluation") presents the Kendall $\tau$ values before and
    after the removal of incorrect paraphrases. The agreement in the ranking of models
    is near-perfect to perfect in both LMentry and BBH benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark & Task | Method |'
  prefs: []
  type: TYPE_TB
- en: '&#124; #Auto &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paraphrases &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; #Correct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paraphrases &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ratio (%) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Accuracy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Avg.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Accuracy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Std.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 258 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 48 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 133 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 74 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 227 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 39 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 131 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 54 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 87.98% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 81.25% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 98.50% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 72.97% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .519 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .483 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .494 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .604 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .074 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .035 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .065 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .048 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| any words from category |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 259 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 48 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 135 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 72 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 233 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 44 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 134 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 52 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 89.96% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 91.67% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 99.26% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 71.23% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .443 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .451 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .438 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .444 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .083 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .043 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .034 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .160 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ends with word |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 226 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 129 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 210 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 39 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 126 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 42 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 92.92% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 82.98% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 97.67% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 89.36% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .131 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .130 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .138 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .112 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .024 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .022 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .027 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| first alphabetically |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 233 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 121 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 62 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 198 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 38 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 117 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 40 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 84.98% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 80.85% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 96.69% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 64.52% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .326 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .293 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .315 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .381 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .079 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .080 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .076 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .053 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| homophones |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 264 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 48 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 140 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 73 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 234 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 43 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 128 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 88.64% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 89.58% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 91.43% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 82.19% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .252 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .214 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .246 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .292 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .057 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .023 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .037 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .081 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| less letters |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 240 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 42 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 126 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 69 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 207 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 40 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 119 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 45 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 86.25% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 95.24% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 94.44% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 65.22% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .338 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .316 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .319 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .397 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .078 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .061 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .068 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .074 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| more letters |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 237 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 45 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 127 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 62 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 210 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 42 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 123 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 42 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 88.61% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 93.33% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 96.85% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 67.74% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .374 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .349 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .359 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .431 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .081 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .059 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .075 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .078 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| rhyming word |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 245 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 125 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 70 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 219 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 37 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 115 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 64 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 89.39% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 78.72% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 92.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 91.43% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .234 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .189 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .198 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .325 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .079 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .038 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .049 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .067 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| word before |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 233 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 41 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 125 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 64 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 225 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 39 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 119 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 64 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 96.57% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 95.12% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 95.20% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .123 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .088 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .098 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .195 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .049 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .009 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .012 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .032 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| word not containing |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 234 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 48 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 125 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 58 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 223 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 122 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 51 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 95.30% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 97.92% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 97.60% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 87.93% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .222 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .177 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .190 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .337 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .094 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .059 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .043 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .115 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| all tasks |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2429 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 461 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1286 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 652 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2186 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 408 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1234 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 514 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 90.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 88.50% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 95.96% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 78.83% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .296 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .269 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .279 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .351 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .070 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .043 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .048 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .074 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: The distribution of correct paraphrases for each generation method
    across all tasks in LMentry.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark & Task | Method |'
  prefs: []
  type: TYPE_TB
- en: '&#124; #Auto &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paraphrases &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; #Correct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paraphrases &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Correct &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Ratio (%) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Accuracy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Avg.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Accuracy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Std.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| causal judgement |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 187 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 76 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 153 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 31 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 55 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 66 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 81.82% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 62.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 91.67% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 86.84% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .477 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .469 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .452 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .502 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .034 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .024 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .023 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .028 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| disambiguation qa |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 188 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 77 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 177 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 76 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 94.15% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 83.33% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 98.70% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .412 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .403 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .357 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .455 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .049 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .031 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .022 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .029 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| formal fallacies |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 184 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 56 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 77 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 130 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 19 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 51 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 59 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 70.65% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 38.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 91.07% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 76.62% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .308 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .326 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .294 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .313 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .026 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .027 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .015 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .027 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| geometric shapes |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 178 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 55 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 72 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 171 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 53 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 67 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 96.07% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 96.36% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 93.06% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .163 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .175 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .153 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .163 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .020 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .015 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .020 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| hyperbaton |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 155 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 43 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 36 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 75 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 117 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 32 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 35 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 49 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 75.48% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 74.42% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 97.22% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 65.33% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .466 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .467 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .438 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .484 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .035 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .020 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .030 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .034 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| logical deduction five objects |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 189 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 59 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 79 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 150 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 27 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 75 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 79.37% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 94.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 45.76% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 94.94% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .262 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .239 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .243 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .283 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .027 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .009 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .026 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .015 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| logical deduction seven objects |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 186 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 75 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 145 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 41 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 31 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 72 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 77.96% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 82.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 51.67% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 96.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .236 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .215 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .219 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .257 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .026 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .009 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .028 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .016 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| logical deduction three objects |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 187 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 60 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 76 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 147 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 27 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 72 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 78.61% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 94.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 45.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 94.74% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .359 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .329 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .317 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .394 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .044 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .023 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .030 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .028 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| movie recommendation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 180 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 57 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 75 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 164 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 66 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 91.11% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 87.72% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 88.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .348 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .371 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .323 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .351 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .036 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .011 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .040 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .032 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| navigate |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 170 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 54 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 65 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 152 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 54 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 89.41% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 72.31% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .386 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .374 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .388 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .396 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .013 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .021 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .017 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| penguins in a table |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 183 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 49 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 59 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 74 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 143 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 37 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 51 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 54 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 78.14% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 75.51% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 86.44% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 72.97% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .243 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .250 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .215 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .265 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .026 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .014 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .007 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .018 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ruin names |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 157 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 40 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 66 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 143 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 49 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 35 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 58 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 91.08% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 98.00% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 87.50% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 87.88% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .254 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .252 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .250 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .258 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .016 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .015 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .013 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| salient translation error detection |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 136 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 47 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 25 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 63 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 128 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 46 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 25 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 56 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 94.12% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 97.87% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 88.89% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .191 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .185 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .192 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .196 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .015 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .011 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .007 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| snarks |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 162 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 37 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 74 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 152 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 37 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 64 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 93.83% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 100.0% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 86.49% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .405 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .396 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .410 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .408 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .025 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .024 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .030 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .021 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| sports understanding |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 173 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 48 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 57 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 67 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 137 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 31 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 49 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 56 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 79.19% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 64.58% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 85.96% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 83.58% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .461 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .469 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .453 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .463 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .052 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .051 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .068 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .035 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| all tasks |'
  prefs: []
  type: TYPE_TB
- en: '&#124; All &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rephrase &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CoT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gradual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2615 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 734 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 775 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1091 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2209 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 627 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 630 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 937 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 84.47% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 85.42% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 81.29% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 85.88% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .331 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .327 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .314 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .346 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .035 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .025 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .030 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; .029 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: The distribution of correct paraphrases for each generation method
    across all tasks in BBH.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Comparing Different Instruction Templates with Kendall’s $\tau$ Rank Disagreements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tables [9](#A1.T9 "Table 9 ‣ A.4 Comparing Different Instruction Templates with
    Kendall’s 𝜏 Rank Disagreements ‣ Appendix A Appendix ‣ State of What Art? A Call
    for Multi-Prompt LLM Evaluation") , [11](#A1.T11 "Table 11 ‣ A.4 Comparing Different
    Instruction Templates with Kendall’s 𝜏 Rank Disagreements ‣ Appendix A Appendix
    ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation"), and [10](#A1.T10
    "Table 10 ‣ A.4 Comparing Different Instruction Templates with Kendall’s 𝜏 Rank
    Disagreements ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation") present the Kendall $\tau$ values that are significantly different
    from 0\. i.e., notable variations in rankings of models for two paraphrases of
    the same task instruction.
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Instruction Template #1 | Instruction Template #2 | $\tau$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category | Can you confirm if the list [{$words$} Output:
    | 0.029 |'
  prefs: []
  type: TYPE_TB
- en: '| any words from category | Is {$category$} Output: | -0.200 |'
  prefs: []
  type: TYPE_TB
- en: '| ends with word | Provide a sentence that finishes with the term {$word$}
    Sentence: | -0.018 |'
  prefs: []
  type: TYPE_TB
- en: '| first alphabetically | Which word comes first alphabetically, “{$word1$}
    Output: The first word alphabetically is | -0.095 |'
  prefs: []
  type: TYPE_TB
- en: '| homophones | Can you tell me which word, {$word1$} The word that sounds more
    like query is: | 0.087 |'
  prefs: []
  type: TYPE_TB
- en: '| less letters | Which of {$word1$} Output keyword: | -0.128 |'
  prefs: []
  type: TYPE_TB
- en: '| more letters | Please compare the length of “{$word1$} Output: | -0.085 |'
  prefs: []
  type: TYPE_TB
- en: '| rhyming word | What is a word that rhymes with ‘{$query$} Output word: |
    0.090 |'
  prefs: []
  type: TYPE_TB
- en: '| word before | Locate the word that comes immediately before ‘{$word$} in
    the sentence is: | -0.099 |'
  prefs: []
  type: TYPE_TB
- en: '| word not containing | Create a term that does not have the inclusion of the
    letter “{$letter$} Output word: | -0.010 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Kendall $\tau$ values of the disagreement between ranks on models
    from example paraphrases for each task in LMentry.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Instruction Template #1 | Instruction Template #2 | $\tau$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| causal judgement | You are required to give your opinion on the following
    question about causation: {$question$} Answer: | 0.183 |'
  prefs: []
  type: TYPE_TB
- en: '| disambiguation qa | Q: For the given sentence, identify the antecedent of
    the ambiguous pronoun or state that it is ambiguous. Sentence: {$sentence$} Output:
    | 0.164 |'
  prefs: []
  type: TYPE_TB
- en: '| formal fallacies | Q: “Classify the argument as either a formal fallacy or
    deducively valid. The explicitly stated premises are {$input$} Output: | -0.264
    |'
  prefs: []
  type: TYPE_TB
- en: '| geometric shapes | Q: Identify the name of the geometric shape represented
    by the following SVG path element: {$svg\_path\_element$}. | -0.267 |'
  prefs: []
  type: TYPE_TB
- en: '| hyperbaton | Order the adjectives correctly before a noun in English sentences,
    following the pattern of “[1\. opinion] [2\. size] [3\. age] [4\. shape] [5\.
    color] [6\. origin] [7\. material] [8\. purpose] noun”. You will be presented
    with a multi-choice format question asking which sentence has the correct adjective
    order, with options provided. Which of the following sentences has the correct
    adjective order? {$options$} Output: | 0.019 |'
  prefs: []
  type: TYPE_TB
- en: '| logical deduction five objects | In this logical deduction task named logical
    deduction five objects, you will be given a set of paragraphs describing a set
    of five objects arranged in a fixed order. The statements are logically consistent
    within each paragraph. Your task is to choose the correct option from the given
    options. The options are {$options$} Output: | 0.264 |'
  prefs: []
  type: TYPE_TB
- en: '| logical deduction seven objects | Your task is to solve a logical deduction
    task which requires you to deduce the order of a sequence of objects. The task
    consists of a set of paragraphs, each describing a set of seven objects arranged
    in a fixed order. The statements are logically consistent within each paragraph.
    You will also be provided with multiple options to choose from. The options are
    represented by {$options$} Output: | 0.133 |'
  prefs: []
  type: TYPE_TB
- en: '| logical deduction three objects | Deduce the order of a sequence of three
    objects based on the logically consistent statements provided in the following
    {$paragraph$} Output: | 0.056 |'
  prefs: []
  type: TYPE_TB
- en: '| movie recommendation | Q: Can you suggest a movie similar to {$movie\_list$}
    Output: | -0.018 |'
  prefs: []
  type: TYPE_TB
- en: '| navigate | Q: Would someone following {$instructions$} Output: | 0.294 |'
  prefs: []
  type: TYPE_TB
- en: '| penguins in a table | Please answer the following question about the table
    of penguins: {$question$} Output: | 0.241 |'
  prefs: []
  type: TYPE_TB
- en: '| ruin names | Which of the following options is a funny way to “ruin” the
    name of {$artist\_or\_movie\_name$} Answer: | 0.075 |'
  prefs: []
  type: TYPE_TB
- en: '| salient translation error detection | Q: Identify the type of error in the
    given English translation of a German source sentence. The error will belong to
    one of the following categories: Named Entities, Numerical Values, Modifiers or
    Adjectives, Negation or Antonyms, Facts, or Dropped Content. The source sentence
    is {$source$} | 0.389 |'
  prefs: []
  type: TYPE_TB
- en: '| snarks | Determine which of two sentences is sarcastic. The format is multi-choice
    and the prompt asks “which statement is sarcastic?” with two options provided.
    The correct option contains a taste of irony and satire. Options: {$options$}
    Output: | 0.22 |'
  prefs: []
  type: TYPE_TB
- en: '| sports understanding | Please classify the sentence “{$sentence$} Answer
    with either “yes” or “no”. Output: | -0.245 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Kendall $\tau$ values of the disagreement between ranks on models
    from example paraphrases for each task in BBH.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Instruction Template #1 | Instruction Template #2 | $\tau$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| bbq lite | You are examined with your ability on social fairness by answering
    questions that contain bias content including: age, disability, gender, nationality,
    physical appearance, race ethnicity, religion, social status, and sexual orientation.
    {$question$} Answer: | 0.222 |'
  prefs: []
  type: TYPE_TB
- en: '| code line description | If you are an experienced code reviewer, please give
    an English language description of Python code{$question$}. The equivalent sentence
    is: | 0.333 |'
  prefs: []
  type: TYPE_TB
- en: '| conceptual combinations | You are a linguistic expert that knows most of
    the concepts and combinations of words. Now, answer the following question: {$context$}.
    Choose the best answer from “A”, “B”, “C”, “D”. | 0.182 |'
  prefs: []
  type: TYPE_TB
- en: '| hindu knowledge | In this task, you have to select the option that best answers
    the question given your knowledge about Hindu mythology. Question: {$question$}
    With your expertise inhindu mythology, provide the correct answer: | 0.444 |'
  prefs: []
  type: TYPE_TB
- en: '| known unknown | Verify if the question is unknown, choose your answer from
    options: Question: {$question$} To avoid hallucination, if the answer to this
    question is unknown, output “B”, otherwise output “A” | -0.029 |'
  prefs: []
  type: TYPE_TB
- en: '| language identification | Please read the following sentence, then choose
    from the options which language you think it most likely came from. Your answer
    should be “A”, “B”, “C”, “D”, “E”, “F”, “G”, “H”, “I”, “J”, or “K” Sentence: {$question$}
    Options: | 0.028 |'
  prefs: []
  type: TYPE_TB
- en: '| logic grid puzzle | You are given a logic grid puzzle to test your sense
    of space and positions. You are given a context and some clues to pick the correct
    answer from the options to answer a question.Context: {$context$} | 0.327 |'
  prefs: []
  type: TYPE_TB
- en: '| logical deduction | Given the following text describing the correct order
    of five objects, select the option from (A, B, C, D or E) that is consistent with
    the text. text: {$question$} Answer: | 0.667 |'
  prefs: []
  type: TYPE_TB
- en: '| novel concepts | You are given three objects {$question$} My answer is: |
    0.400 |'
  prefs: []
  type: TYPE_TB
- en: '| play dialog | Now you are a dramatist. The following transcripts of dialogues
    are taken from Shakespeare plays, but the transcripts do not mark who said what.
    Your task is to identify whether the sentences in question were spoken by the
    same or different people. Here is the play: {$play$} were spoken by a single person
    or by different people. Answer: | -0.638 |'
  prefs: []
  type: TYPE_TB
- en: '| strange stories | Given a story, answer whether the question is true or false.
    {$context$} A: | 0.310 |'
  prefs: []
  type: TYPE_TB
- en: '| strategic qa | Reason about the answer to the question. {$question$}Answer:
    | -0.085 |'
  prefs: []
  type: TYPE_TB
- en: '| vitaminc fact verification | Input: {$claim$} | 0.556 |'
  prefs: []
  type: TYPE_TB
- en: '| winowhy | Read the following reasoning about who a particular pronoun refers
    to: {$question$} | -0.056 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Kendall $\tau$ values of the disagreement between ranks on models
    from example paraphrases for each task in BBL.'
  prefs: []
  type: TYPE_NORMAL
- en: A.5 Model Performance Differences with Minimal Paraphrasing Edit Distance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Figure [8](#A1.F8 "Figure 8 ‣ A.5 Model Performance Differences with Minimal
    Paraphrasing Edit Distance ‣ Appendix A Appendix ‣ State of What Art? A Call for
    Multi-Prompt LLM Evaluation") depicts the average performance differences between
    various LLMs when small edits are made to the instruction templates.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9f05b23fa197e49b74530c2ce487b4fa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Average performance differences between various models when small
    edits are made to the prompts (e.g., substituting ’excludes’ with ’lacks’). The
    count column describes the number of tasks for which this edit was relevant.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Table [12](#A1.T12 "Table 12 ‣ A.5 Model Performance Differences
    with Minimal Paraphrasing Edit Distance ‣ Appendix A Appendix ‣ State of What
    Art? A Call for Multi-Prompt LLM Evaluation") shows representative examples of
    instruction template pairs with very minor differences but notable variations
    in performance.
  prefs: []
  type: TYPE_NORMAL
- en: '| Change | Model | P1 | Acc. | P2 | Acc. | Diff. |'
  prefs: []
  type: TYPE_TB
- en: '| ‘.’ –>‘:’ | nous-hermes | Create a word that does not include the letter
    “{letter}”. | .04 | Create a word that does not include the letter “{letter}”:
    | .65 | +.62 |'
  prefs: []
  type: TYPE_TB
- en: '|  | alpaca-13b | Create a sentence that concludes with the term “{word}”.
    | .61 | Create a sentence that concludes with the term “{word}”: | .19 | -.42
    |'
  prefs: []
  type: TYPE_TB
- en: '| + ‘.’ | alpaca-13b | Write a word that lacks the letter “{letter}” | .04
    | Write a word that lacks the letter “{letter}”. | .42 | +.38 |'
  prefs: []
  type: TYPE_TB
- en: '|  | falcon-7b | Write a word that lacks the letter “{letter}” | .19 | Write
    a word that lacks the letter “{letter}”. | .50 | +.31 |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-xl | Write a word that omits the letter “{letter}” | .77 | Write
    a word that omits the letter “{letter}”. | .54 | -.23 |'
  prefs: []
  type: TYPE_TB
- en: '| + “…” | mpt-7b | Write a word that does not contain the letter {letter}.\nWord:
    | .58 | Write a word that does not contain the letter “{letter}”.\nWord: | .19
    | -.38 |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-small | Write a word that does not contain the letter {letter}.\nWord:
    | .62 | Write a word that does not contain the letter “{letter}”.\nWord: | .04
    | -.58 |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-xl | Write a word that excludes the letter {letter}. | .81 | Write
    a word that excludes the letter “{letter}”. | .23 | -.58 |'
  prefs: []
  type: TYPE_TB
- en: '| + ‘Q:’ | minotaur | Are all of the words in the set {words} classified as
    {category}? Please respond with either ’yes’ or ’no’. | .69 | Q: Are all of the
    words in the set {words} classified as {category}? Please respond with either
    ’yes’ or ’no’. | .02 | -.67 |'
  prefs: []
  type: TYPE_TB
- en: '|  | airoboros | Are all the words in {words} categorized as {category}? Please
    answer with either ’yes’ or ’no’. | .75 | Q: Are all the words in {words} categorized
    as {category}? Please answer with either ’yes’ or ’no’. | .09 | -.66 |'
  prefs: []
  type: TYPE_TB
- en: '|  | mpt-7b | Are all the words in {words} categorized as {category}? Please
    answer with either ’yes’ or ’no’. | .57 | Q: Are all the words in {words} categorized
    as {category}? Please answer with either ’yes’ or ’no’. | .00 | -.57 |'
  prefs: []
  type: TYPE_TB
- en: '|  | t0pp | Are all the words in {words} categorized as {category}? Please
    answer with either ’yes’ or ’no’. | .99 | Q: Are all the words in {words} categorized
    as {category}? Please answer with either ’yes’ or ’no’. | .55 | -.44 |'
  prefs: []
  type: TYPE_TB
- en: '| + ‘using’ | flan-t5-large | Your task is to write a word without the letter
    “{letter}”. | .46 | Your task is to write a word without using the letter “{letter}”.
    | .12 | -.35 |'
  prefs: []
  type: TYPE_TB
- en: '|  | falcon-7b | Write a word without the letter {letter}.\nOutput word: |
    .12 | Write a word without using the letter {letter}.\nOutput word: | .35 | +.23
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-large | Write a word without the letter {letter}.\nOutput word:
    | .73 | Write a word without using the letter {letter}.\nOutput word: | .50 |
    -.23 |'
  prefs: []
  type: TYPE_TB
- en: '| omits –>lacks | ultralm-13b | Write a word that omits the letter “{letter}”.
    | .62 | Write a word that lacks the letter “{letter}”. | .19 | -.42 |'
  prefs: []
  type: TYPE_TB
- en: '|  | falcon-7b | Write a word that omits the letter “{letter}”. | .19 | Write
    a word that lacks the letter “{letter}”. | .50 | +.31 |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-xl | Write a word that omits the letter “{letter}”. | .54 | Write
    a word that lacks the letter “{letter}”. | .81 | +.27 |'
  prefs: []
  type: TYPE_TB
- en: '| contain –>have | falcon-7b | Write a word that does not contain the letter
    “{letter}”. | .81 | Write a word that does not have the letter “{letter}”. | .19
    | -.62 |'
  prefs: []
  type: TYPE_TB
- en: '|  | falcon-7b | Write a word that does not contain the letter “{letter}”.
    | .81 | Write a word that does not have the letter “{letter}”. | .27 | -.54 |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-xxl | Please write a word that does not contain the letter “{letter}”.
    | .62 | Please write a word that does not have the letter “{letter}”. | .88 |
    +.27 |'
  prefs: []
  type: TYPE_TB
- en: '| include –>have | falcon-7b | Write a word that does not include the letter
    “{letter}”. | .81 | Write a word that does not have the letter “{letter}”. | .19
    | -.62 |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-xl | Write a word that does not include the letter “{letter}”.
    | .42 | Write a word that does not have the letter “{letter}”. | .73 | +.31 |'
  prefs: []
  type: TYPE_TB
- en: '|  | falcon-7b | Please write a word that does not include the letter “{letter}”.
    | .77 | Please write a word that does not have the letter “{letter}”. | .35 |
    -.42 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ultralm-13b | Please write a word that does not include the letter “{letter}”.
    | .46 | Please write a word that does not have the letter “{letter}”. | .12 |
    -.35 |'
  prefs: []
  type: TYPE_TB
- en: '| excludes –>lacks | flan-t5-large | Write a word that excludes the letter
    “{letter}”. | .54 | Write a word that lacks the letter “{letter}”. | .12 | -.42
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-xl | Write a word that excludes the letter “{letter}”. | .19 |
    Write a word that lacks the letter “{letter}”. | .81 | +.62 |'
  prefs: []
  type: TYPE_TB
- en: '|  | flan-t5-xl | Write a word that excludes the letter “{letter}” | .46 |
    Write a word that lacks the letter “{letter}” | .88 | +.42 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Representative examples of instruction template pairs from LMentry
    with very minor differences but notable variations in performance (open-source
    models).'
  prefs: []
  type: TYPE_NORMAL
- en: A.6 BBL Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection consists of an additional analysis of the BBL benchmark that
    was not detailed in the main body of the paper. Table [3](#S4.T3 "Table 3 ‣ Instance
    sampling and prompt construction. ‣ 4.2 Quantifying Performance Variance due to
    Instruction Paraphrasing ‣ 4 Single-Prompt Evaluation Leads to Inconsistent Results
    ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation") presents the Kendall’s
    W values and the Friedman test p-values that demonstrate a low correlation between
    the ranks of the models for different instruction templates and reveal similar
    inconsistencies as observed with automated paraphrases in other benchmarks. Figure [10](#A1.F10
    "Figure 10 ‣ A.6 BBL Analysis ‣ Appendix A Appendix ‣ State of What Art? A Call
    for Multi-Prompt LLM Evaluation") shows the deviation of the original instruction
    template from the average performance calculated over the generated instruction
    templates of several models for all of the BBL tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark & Task | Kendall’s W | Friedman p-val |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BIG-bench Lite |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|     known unknown | .316 | 4.4E-5 |'
  prefs: []
  type: TYPE_TB
- en: '|     play dialog | .355 | 4.3E-5 |'
  prefs: []
  type: TYPE_TB
- en: '|     winowhy | .520 | 6.0E-4 |'
  prefs: []
  type: TYPE_TB
- en: '|     strategic qa | .529 | .191 |'
  prefs: []
  type: TYPE_TB
- en: '|     hindu knowledge | .560 | .569 |'
  prefs: []
  type: TYPE_TB
- en: '|     conceptual combinations | .731 | .132 |'
  prefs: []
  type: TYPE_TB
- en: '|     strange stories | .731 | .431 |'
  prefs: []
  type: TYPE_TB
- en: '|     code line description | .756 | .002 |'
  prefs: []
  type: TYPE_TB
- en: '|     novel concepts | .787 | .620 |'
  prefs: []
  type: TYPE_TB
- en: '|     logic grid puzzle | .796 | .010 |'
  prefs: []
  type: TYPE_TB
- en: '|     language identification | .811 | .002 |'
  prefs: []
  type: TYPE_TB
- en: '|     vitaminc fact verification | .888 | .772 |'
  prefs: []
  type: TYPE_TB
- en: '|     bbq lite | .890 | .023 |'
  prefs: []
  type: TYPE_TB
- en: '|     logical deduction | .913 | .895 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Kendall’s $W\in[0,1]$, indicating less than optimal correlation.
    The p-values from the Friedman test indicate significant differences between rankings
    of models when using different prompts for 7 tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6e46b455afba31dd5a3dfc89f5b003d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Model and task performance divergence. For each task, this table
    shows the number of standard deviations by which the performance of each model
    on the original prompts deviates from the average model performance. Dark red
    cells indicate substantial divergence values exceeding one standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5f7cc5032c2a6e6a99ef4a4c68f3f834.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Model and task performance divergence. For each task, this table
    shows the number of standard deviations by which the performance of each model
    on the original prompts deviates from the average model performance. Dark red
    cells indicate substantial divergence values exceeding one standard deviation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Benchmark & Task | MaxP | AvgP | Sat | Combined |'
  prefs: []
  type: TYPE_TB
- en: '| LMentry |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|     all words from category | .958 | .967 | .950 | .900 |'
  prefs: []
  type: TYPE_TB
- en: '|     any words from category | .979 | .967 | .983 | .983 |'
  prefs: []
  type: TYPE_TB
- en: '|     ends with word | .104 | .967 | .050 | .517 |'
  prefs: []
  type: TYPE_TB
- en: '|     first alphabetically | 1.00 | .950 | .933 | .950 |'
  prefs: []
  type: TYPE_TB
- en: '|     homophones | .945 | 1.00 | .900 | .933 |'
  prefs: []
  type: TYPE_TB
- en: '|     less letters | .983 | 1.00 | .917 | .967 |'
  prefs: []
  type: TYPE_TB
- en: '|     more letters | .970 | .983 | .917 | .983 |'
  prefs: []
  type: TYPE_TB
- en: '|     rhyming word | 1.00 | .967 | .983 | .967 |'
  prefs: []
  type: TYPE_TB
- en: '|     word before | 1.00 | 1.00 | .983 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '|     word not containing | .836 | .967 | .783 | .850 |'
  prefs: []
  type: TYPE_TB
- en: '| BIG-bench Hard |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|     causal judgement | 1.00 | 1.00 | 1.00 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '|     disambiguation qa | 1.00 | 1.00 | .964 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '|     formal fallacies | .991 | .927 | .818 | .855 |'
  prefs: []
  type: TYPE_TB
- en: '|     geometric shapes | 1.00 | .964 | 1.00 | .964 |'
  prefs: []
  type: TYPE_TB
- en: '|     hyperbaton | .953 | 1.00 | .927 | .964 |'
  prefs: []
  type: TYPE_TB
- en: '|     logical deduction five objects | 1.00 | .964 | .855 | .964 |'
  prefs: []
  type: TYPE_TB
- en: '|     logical deduction seven objects | 1.00 | 1.00 | .964 | .964 |'
  prefs: []
  type: TYPE_TB
- en: '|     logical deduction three objects | 1.00 | .964 | .891 | .964 |'
  prefs: []
  type: TYPE_TB
- en: '|     movie recommendation | .954 | .927 | .891 | .964 |'
  prefs: []
  type: TYPE_TB
- en: '|     navigate | .964 | 1.00 | 1.00 | .964 |'
  prefs: []
  type: TYPE_TB
- en: '|     penguins in a table | 1.00 | 1.00 | .891 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '|     ruin names | 1.00 | 1.00 | .964 | .891 |'
  prefs: []
  type: TYPE_TB
- en: '|     salient translation error detection | 1.00 | 1.00 | 1.00 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '|     snarks | 1.00 | 1.00 | .964 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '|     sports understanding | 1.00 | 1.00 | .964 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Kendall’s Tau model ranking comparisons before and after removal
    of incorrect paraphrases. Results show near-perfect to perfect agreement across
    all tasks, except for LMentry’s “ends with word” task.'
  prefs: []
  type: TYPE_NORMAL
- en: A.7 Average Model Ranks for Each Metric Across All Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tables [15](#A1.T15 "Table 15 ‣ A.8 Analysis of Origin Generation Method of
    Optimal Paraphrases ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation"), [16](#A1.T16 "Table 16 ‣ A.8 Analysis of Origin Generation Method
    of Optimal Paraphrases ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation") present the average model ranks for each metric across all tasks
    in LMentry and BBH respectively. Flan-T5-XXL emerges as the top performer for
    all metrics in both benchmarks. Minotaur is at the bottom of the performance spectrum
    across all evaluated models in BBH.
  prefs: []
  type: TYPE_NORMAL
- en: A.8 Analysis of Origin Generation Method of Optimal Paraphrases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our analyses for the origin of the optimal paraphrases used by each model, are
    summarized in Tables [17](#A1.T17 "Table 17 ‣ A.8 Analysis of Origin Generation
    Method of Optimal Paraphrases ‣ Appendix A Appendix ‣ State of What Art? A Call
    for Multi-Prompt LLM Evaluation"), [18](#A1.T18 "Table 18 ‣ A.8 Analysis of Origin
    Generation Method of Optimal Paraphrases ‣ Appendix A Appendix ‣ State of What
    Art? A Call for Multi-Prompt LLM Evaluation"). The gradual method surfaced as
    the dominant source of optimal paraphrases across both benchmarks, particularly
    pronounced in the LMentry benchmark. However, a closer look at individual models
    revealed a pattern of preference for different generation methods.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | average | maximum | saturation | combined |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| t0_3b | 7.40 | 12.20 | 5.90 | 7.90 |'
  prefs: []
  type: TYPE_TB
- en: '| t0++ | 4.80 | 8.80 | 4.20 | 4.63 |'
  prefs: []
  type: TYPE_TB
- en: '| falcon-7b | 9.40 | 8.00 | 9.70 | 8.27 |'
  prefs: []
  type: TYPE_TB
- en: '| mpt-7b | 10.30 | 11.10 | 10.00 | 10.00 |'
  prefs: []
  type: TYPE_TB
- en: '| alpaca-7b | 13.90 | 7.90 | 13.60 | 13.20 |'
  prefs: []
  type: TYPE_TB
- en: '| alpaca-13b | 11.50 | 6.60 | 12.40 | 10.72 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-small | 6.20 | 9.60 | 7.90 | 5.90 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-base | 8.00 | 10.50 | 6.50 | 8.20 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-large | 4.10 | 8.50 | 4.80 | 4.50 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-xl | 2.80 | 4.20 | 5.80 | 3.72 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-xxl | 1.40 | 3.10 | 3.70 | 1.72 |'
  prefs: []
  type: TYPE_TB
- en: '| airoboros-13b | 9.50 | 10.20 | 9.50 | 10.36 |'
  prefs: []
  type: TYPE_TB
- en: '| nous-hermes-13b | 8.90 | 6.30 | 9.70 | 8.73 |'
  prefs: []
  type: TYPE_TB
- en: '| ultralm-13b | 12.30 | 9.80 | 9.30 | 11.09 |'
  prefs: []
  type: TYPE_TB
- en: '| vicuna-13b | 11.80 | 3.70 | 14.70 | 10.63 |'
  prefs: []
  type: TYPE_TB
- en: '| minotaur-15b | 13.70 | 10.20 | 8.30 | 13.63 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: Average model ranks for each metric across all tasks in LMentry.
    Bold numbers indicate the best averaged rank per metric, while underlined numbers
    indicate the worst averaged rank per metric.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | average | maximum | saturation | combined |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| t0++ | 7.33 | 7.47 | 5.67 | 7.33 |'
  prefs: []
  type: TYPE_TB
- en: '| falcon-7b | 6.20 | 7.60 | 4.67 | 6.47 |'
  prefs: []
  type: TYPE_TB
- en: '| mpt-7b | 9.00 | 9.33 | 8.00 | 9.53 |'
  prefs: []
  type: TYPE_TB
- en: '| alpaca-13b | 4.53 | 5.27 | 3.93 | 4.67 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-xl | 2.67 | 2.47 | 3.93 | 2.67 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-xxl | 1.40 | 1.87 | 3.20 | 1.33 |'
  prefs: []
  type: TYPE_TB
- en: '| airoboros-13b | 5.73 | 5.67 | 5.80 | 5.67 |'
  prefs: []
  type: TYPE_TB
- en: '| nous-hermes-13b | 6.87 | 5.40 | 8.13 | 6.67 |'
  prefs: []
  type: TYPE_TB
- en: '| ultralm-13b | 8.53 | 6.60 | 8.73 | 8.20 |'
  prefs: []
  type: TYPE_TB
- en: '| vicuna-13b | 3.07 | 3.13 | 5.27 | 3.13 |'
  prefs: []
  type: TYPE_TB
- en: '| minotaur-15b | 10.67 | 9.67 | 8.67 | 10.33 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 16: Average model ranks for each metric across all tasks in BBH. Bold
    numbers indicate the best averaged rank per metric, while underlined numbers indicate
    the worst averaged rank per metric.'
  prefs: []
  type: TYPE_NORMAL
- en: '| model | default | rephrase | cot | gradual |'
  prefs: []
  type: TYPE_TB
- en: '| t0_3b (*) | 0.00 | 11.76 | 58.82 | 29.41 |'
  prefs: []
  type: TYPE_TB
- en: '| t0++ (*) | 0.00 | 15.00 | 45.00 | 40.00 |'
  prefs: []
  type: TYPE_TB
- en: '| falcon-7b | 9.09 | 9.09 | 36.36 | 45.45 |'
  prefs: []
  type: TYPE_TB
- en: '| mpt-7b | 0.00 | 23.53 | 47.06 | 29.41 |'
  prefs: []
  type: TYPE_TB
- en: '| alpaca-7b (**) | 8.33 | 0.00 | 0.00 | 91.67 |'
  prefs: []
  type: TYPE_TB
- en: '| alpaca-13b (**) | 0.00 | 0.00 | 8.33 | 91.67 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-base (*) | 0.00 | 7.14 | 64.29 | 28.57 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-small (*) | 0.00 | 0.00 | 58.33 | 41.67 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-large (*) | 0.00 | 40.00 | 40.00 | 20.00 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-xl (*) | 0.00 | 7.69 | 30.77 | 61.54 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-xxl (*) | 0.00 | 13.04 | 69.57 | 17.39 |'
  prefs: []
  type: TYPE_TB
- en: '| airoboros-13b (**) | 0.00 | 35.71 | 14.29 | 50.00 |'
  prefs: []
  type: TYPE_TB
- en: '| nous-hermes-13b (**) | 0.00 | 0.00 | 33.33 | 66.67 |'
  prefs: []
  type: TYPE_TB
- en: '| ultralm-13b (**) | 0.00 | 6.67 | 66.67 | 26.67 |'
  prefs: []
  type: TYPE_TB
- en: '| vicuna-13b (**) | 10.00 | 10.00 | 0.00 | 80.00 |'
  prefs: []
  type: TYPE_TB
- en: '| minotaur-15b | 0.00 | 14.29 | 28.57 | 57.14 |'
  prefs: []
  type: TYPE_TB
- en: '| all models | 1.33 | 12.39 | 40.71 | 45.58 |'
  prefs: []
  type: TYPE_TB
- en: '| all paraphrases | 1.24 | 18.98 | 52.94 | 26.84 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 17: Distribution of optimal paraphrase sources per model for LMentry.
    Rows represent models, with T5-based models marked by an asterisk (*) and LLaMA-based
    models by two asterisks (**). Columns indicate paraphrase generation methods.
    Percentages in each cell show the rate of optimal paraphrases from each method,
    with bold numbers identifying the leading source for each model. The ‘All Models’
    row aggregates percentages across all models, while the ‘All Paraphrases’ row
    displays the overall distribution of generation methods across all paraphrases.'
  prefs: []
  type: TYPE_NORMAL
- en: '| model | default | rephrase | cot | gradual |'
  prefs: []
  type: TYPE_TB
- en: '| falcon-7b | 0.00 | 27.27 | 18.18 | 54.55 |'
  prefs: []
  type: TYPE_TB
- en: '| mpt-7b | 0.00 | 40.00 | 16.00 | 44.00 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-xl | 2.38 | 23.81 | 26.19 | 47.62 |'
  prefs: []
  type: TYPE_TB
- en: '| flan-t5-xxl | 4.35 | 17.39 | 26.09 | 52.17 |'
  prefs: []
  type: TYPE_TB
- en: '| t0++ | 0.00 | 50.00 | 40.00 | 10.00 |'
  prefs: []
  type: TYPE_TB
- en: '| alpaca-13b | 0.00 | 11.11 | 44.44 | 44.44 |'
  prefs: []
  type: TYPE_TB
- en: '| airoboros-13b | 0.00 | 50.00 | 16.67 | 33.33 |'
  prefs: []
  type: TYPE_TB
- en: '| nous-hermes-13b | 0.00 | 12.50 | 16.67 | 70.83 |'
  prefs: []
  type: TYPE_TB
- en: '| ultralm-13b | 0.00 | 29.17 | 25.00 | 45.83 |'
  prefs: []
  type: TYPE_TB
- en: '| vicuna-13b | 0.00 | 38.10 | 28.57 | 33.33 |'
  prefs: []
  type: TYPE_TB
- en: '| minotaur-15b | 0.00 | 9.09 | 0.00 | 90.91 |'
  prefs: []
  type: TYPE_TB
- en: '| all tasks | 0.73 | 27.37 | 23.72 | 48.18 |'
  prefs: []
  type: TYPE_TB
- en: '| all paraphrases | 0.57 | 28.07 | 29.64 | 41.72 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 18: Distribution of optimal paraphrase sources per model for BBH. Rows
    represent models and columns indicate paraphrase generation methods. Percentages
    in each cell show the rate of optimal paraphrases from each method, with bold
    numbers identifying the leading source for each model. The ‘All Models’ row aggregates
    percentages across all models, while the ‘All Paraphrases’ row displays the overall
    distribution of generation methods across all paraphrases.'
  prefs: []
  type: TYPE_NORMAL
- en: A.9 Small Scale Evaluation - OpenAI
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection contains all the tables referenced in Section [7](#S7 "7 Small-Scale
    Evaluation of OpenAI Models on Prompt Paraphrasing ‣ State of What Art? A Call
    for Multi-Prompt LLM Evaluation"). Table [19](#A1.T19 "Table 19 ‣ A.9 Small Scale
    Evaluation - OpenAI ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt
    LLM Evaluation") and Table [20](#A1.T20 "Table 20 ‣ A.9 Small Scale Evaluation
    - OpenAI ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM
    Evaluation") are related to our naive heuristics for estimating average and maximum
    performance, respectively. Table [19](#A1.T19 "Table 19 ‣ A.9 Small Scale Evaluation
    - OpenAI ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM
    Evaluation") presents the average number of repetitions needed for our heuristic
    to estimate the average performance, ensuring less than a 1-point accuracy discrepancy
    from the actual average for each open-source model across all tasks in the LMentry
    benchmark. Table [20](#A1.T20 "Table 20 ‣ A.9 Small Scale Evaluation - OpenAI
    ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation")
    compiles results from our greedy heuristic that searches for the optimal paraphrases
    for each open-source model on each LMentry task.
  prefs: []
  type: TYPE_NORMAL
- en: Table [21](#A1.T21 "Table 21 ‣ A.9 Small Scale Evaluation - OpenAI ‣ Appendix
    A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation") and Table [23](#A1.T23
    "Table 23 ‣ A.9 Small Scale Evaluation - OpenAI ‣ Appendix A Appendix ‣ State
    of What Art? A Call for Multi-Prompt LLM Evaluation") aggregate the average and
    maximum performances for each model and task using only the original instruction
    templates. Similarly, Table [22](#A1.T22 "Table 22 ‣ A.9 Small Scale Evaluation
    - OpenAI ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM
    Evaluation") and Table [24](#A1.T24 "Table 24 ‣ A.9 Small Scale Evaluation - OpenAI
    ‣ Appendix A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation")
    present the approximated average and maximum performances, computed with our heuristics,
    for each model and task using all paraphrased templates.
  prefs: []
  type: TYPE_NORMAL
- en: Table [25](#A1.T25 "Table 25 ‣ A.9 Small Scale Evaluation - OpenAI ‣ Appendix
    A Appendix ‣ State of What Art? A Call for Multi-Prompt LLM Evaluation") contains
    the McNemar test p-values we used to assess the statistical significance of the
    differences in maximum performance between the original best prompt and the estimated
    optimal prompt.
  prefs: []
  type: TYPE_NORMAL
- en: '| task | t0_3b | t0++ | fal7b | mpt7b | alp7b | alp13b | ft5small | ft5base
    | ft5large | ft5xl | ft5xxl | airoboros | noushermes | ultralm | vicuna | minotaur
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category | 4.9 | 2.2 | 11.3 | 4.2 | 3.1 | 2.9 | 5.6 | 4.3
    | 2.4 | 6.6 | 3.1 | 6.6 | 8.7 | 3.2 | 3.8 | 7.5 |'
  prefs: []
  type: TYPE_TB
- en: '| any words from category | 3.4 | 1.5 | 11.2 | 4.1 | 3.8 | 2.9 | 11 | 7.3 |
    6.7 | 2 | 1.7 | 5.8 | 3.2 | 6 | 2.6 | 2.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ends with word | 3.3 | 4.3 | 2.2 | 2.4 | 1.9 | 10.7 | 3.3 | 6.2 | 5.7 | 5.9
    | 6.1 | 2.8 | 3.5 | 2 | 3.7 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| first alphabetically | 10.3 | 3.2 | 7.2 | 6.1 | 2.6 | 6.4 | 10.3 | 3.2 |
    5.1 | 3.9 | 4 | 5.3 | 8.2 | 11.4 | 5.7 | 7.8 |'
  prefs: []
  type: TYPE_TB
- en: '| homophones | 7.2 | 5.7 | 8.3 | 10.3 | 2.4 | 4.2 | 10.5 | 4.5 | 3.5 | 3.7
    | 9.7 | 5.8 | 10.8 | 2.1 | 3.7 | 1.2 |'
  prefs: []
  type: TYPE_TB
- en: '| less letters | 3.3 | 9.5 | 5.3 | 3.8 | 4.2 | 5.5 | 5.9 | 4.6 | 4.5 | 5.1
    | 3.5 | 4.2 | 3.5 | 7.1 | 10.2 | 6.3 |'
  prefs: []
  type: TYPE_TB
- en: '| more letters | 4.3 | 4.2 | 6.5 | 6 | 5.8 | 10.4 | 3.3 | 4.2 | 4.2 | 4.9 |
    7.6 | 5.4 | 9.3 | 6 | 7.9 | 6.8 |'
  prefs: []
  type: TYPE_TB
- en: '| rhyming word | 10.9 | 2.5 | 4.1 | 5.2 | 2.6 | 12.8 | 6.6 | 3.8 | 6.7 | 5.1
    | 5.6 | 3.3 | 3.8 | 1.8 | 7.3 | 1.2 |'
  prefs: []
  type: TYPE_TB
- en: '| word before | 5.9 | 2.8 | 1.3 | 3.2 | 6.6 | 3.5 | 6 | 4.4 | 4.5 | 8.8 | 4.9
    | 4.5 | 3.3 | 1.1 | 5.1 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| word not containing | 4.2 | 12.4 | 8.6 | 11.8 | 9.4 | 6.3 | 4.4 | 10 | 21.9
    | 14.2 | 5.8 | 10.1 | 5.8 | 6.3 | 5 | 12.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 19: The average number of average heuristic repetitions required to achieve
    less than a 1 accuracy point discrepancy from the actual average performance for
    each task and open-source model. Maximal value: 21.9\. All values average: 5.62
    (std: 3.12).'
  prefs: []
  type: TYPE_NORMAL
- en: '| task | t0_3b | t0++ | fal7b | mpt7b | alp7b | alp13b | ft5small | ft5base
    | ft5large | ft5xl | ft5xxl | airoboros | noushermes | ultralm | vicuna | minotaur
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category |  |  | 0.01 |  |  |  | 0.03 |  | 0.02 |  |  |  |
    0.02 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| any words from category |  |  |  |  |  |  |  |  |  |  |  |  | 0.06 |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ends with word |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  | 0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| first alphabetically |  |  |  |  |  |  | 0.01 |  | 0.03 | 0.03 |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| homophones |  |  |  |  |  |  |  |  | 0.04 |  |  |  |  |  |  | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| less letters |  | 0.02 | 0.01 |  |  |  | 0.02 |  | 0.03 |  | 0.02 |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| more letters | 0.01 | 0.01 |  | 0.03 |  |  |  |  |  | 0.01 |  |  |  | 0.01
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| rhyming word |  | 0.01 |  |  |  |  | 0.02 | 0.03 | 0.01 | 0.01 |  |  |  |
    0.01 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| word before |  |  |  |  | 0.01 |  |  |  |  |  | 0.06 |  |  |  | 0.01 |  |'
  prefs: []
  type: TYPE_TB
- en: '| word not containing |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 20: Results of the greedy optimal paraphrase search for each task and
    open-source model. An optimal prompt was recovered in 130 out of 160 cases. In
    the remaining cases, the average discrepancy in performance between the chosen
    and actual optimal paraphrases was 2.1 accuracy points, with a standard deviation
    of 1.4.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | davinci | td002 | td003 | cgpt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category | 0.56 | 0.72 | 0.84 | 0.60 |'
  prefs: []
  type: TYPE_TB
- en: '| any words from category | 0.55 | 0.63 | 0.65 | 0.86 |'
  prefs: []
  type: TYPE_TB
- en: '| ends with word | 0.10 | 0.30 | 0.58 | 0.60 |'
  prefs: []
  type: TYPE_TB
- en: '| first alphabetically | 0.46 | 0.48 | 0.71 | 0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| homophones | 0.48 | 0.19 | 0.38 | 0.49 |'
  prefs: []
  type: TYPE_TB
- en: '| less letters | 0.41 | 0.67 | 0.79 | 0.88 |'
  prefs: []
  type: TYPE_TB
- en: '| more letters | 0.47 | 0.68 | 0.82 | 0.87 |'
  prefs: []
  type: TYPE_TB
- en: '| rhyming word | 0.19 | 0.29 | 0.57 | 0.69 |'
  prefs: []
  type: TYPE_TB
- en: '| word before | 0.12 | 0.13 | 0.27 | 0.40 |'
  prefs: []
  type: TYPE_TB
- en: '| word not containing | 0.03 | 0.85 | 0.97 | 0.90 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 21: Average performances for OpenAI models across all LMentry tasks,
    computed using only the original prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | davinci | td002 | td003 | cgpt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category | 0.15 | 0.61 | 0.79 | 0.62 |'
  prefs: []
  type: TYPE_TB
- en: '| any words from category | 0.16 | 0.62 | 0.59 | 0.82 |'
  prefs: []
  type: TYPE_TB
- en: '| ends with word | 0.11 | 0.24 | 0.42 | 0.54 |'
  prefs: []
  type: TYPE_TB
- en: '| first alphabetically | 0.12 | 0.27 | 0.35 | 0.45 |'
  prefs: []
  type: TYPE_TB
- en: '| homophones | 0.12 | 0.57 | 0.60 | 0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| less letters | 0.17 | 0.51 | 0.58 | 0.58 |'
  prefs: []
  type: TYPE_TB
- en: '| more letters | 0.16 | 0.49 | 0.51 | 0.50 |'
  prefs: []
  type: TYPE_TB
- en: '| rhyming word | 0.14 | 0.39 | 0.41 | 0.76 |'
  prefs: []
  type: TYPE_TB
- en: '| word before | 0.04 | 0.16 | 0.51 | 0.47 |'
  prefs: []
  type: TYPE_TB
- en: '| word not containing | 0.06 | 0.57 | 0.84 | 0.81 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 22: Estimated average performances for OpenAI models across all LMentry
    tasks, approximated using all prompt paraphrases.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | davinci | td002 | td003 | cgpt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category | 0.64 | 0.80 | 0.85 | 0.68 |'
  prefs: []
  type: TYPE_TB
- en: '| any words from category | 0.66 | 0.82 | 0.88 | 0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| ends with word | 0.15 | 0.35 | 0.61 | 0.62 |'
  prefs: []
  type: TYPE_TB
- en: '| first alphabetically | 0.50 | 0.56 | 0.90 | 0.98 |'
  prefs: []
  type: TYPE_TB
- en: '| homophones | 0.59 | 0.25 | 0.41 | 0.79 |'
  prefs: []
  type: TYPE_TB
- en: '| less letters | 0.48 | 0.70 | 0.86 | 0.92 |'
  prefs: []
  type: TYPE_TB
- en: '| more letters | 0.54 | 0.80 | 0.89 | 0.90 |'
  prefs: []
  type: TYPE_TB
- en: '| rhyming word | 0.32 | 0.45 | 0.65 | 0.96 |'
  prefs: []
  type: TYPE_TB
- en: '| word before | 0.17 | 0.21 | 0.34 | 0.66 |'
  prefs: []
  type: TYPE_TB
- en: '| word not containing | 0.04 | 0.92 | 1.00 | 0.96 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 23: Max performances for OpenAI models across all LMentry tasks, computed
    using only the original prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | davinci | td002 | td003 | cgpt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category | 0.64 | 0.94 | 0.99 | 0.97 |'
  prefs: []
  type: TYPE_TB
- en: '| any words from category | 0.66 | 0.95 | 0.99 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| ends with word | 0.88 | 0.52 | 0.67 | 0.72 |'
  prefs: []
  type: TYPE_TB
- en: '| first alphabetically | 0.55 | 0.95 | 0.97 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| homophones | 0.63 | 0.99 | 0.95 | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: '| less letters | 0.61 | 0.95 | 0.95 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| more letters | 0.71 | 0.93 | 0.97 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: '| rhyming word | 0.67 | 0.93 | 0.95 | 0.99 |'
  prefs: []
  type: TYPE_TB
- en: '| word before | 0.26 | 0.51 | 0.82 | 0.95 |'
  prefs: []
  type: TYPE_TB
- en: '| word not containing | 0.65 | 1.00 | 1.00 | 1.00 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 24: Estimated max performances for OpenAI models across all LMentry tasks,
    approximated using all prompt paraphrases.'
  prefs: []
  type: TYPE_NORMAL
- en: '| task | davinci | td002 | td003 | cgpt |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| all words from category | 1 | .0009 | .0002 | 7.2E-08 |'
  prefs: []
  type: TYPE_TB
- en: '| any words from category | 1 | .0008 | .0009 | .0832 |'
  prefs: []
  type: TYPE_TB
- en: '| ends with word | 8.8E-17 | .0195 | .3034 | .0955 |'
  prefs: []
  type: TYPE_TB
- en: '| first alphabetically | .4922 | 6.1E-09 | .0196 | .1572 |'
  prefs: []
  type: TYPE_TB
- en: '| homophones | .5371 | 7.8E-18 | 5.3E-13 | 7.7E-06 |'
  prefs: []
  type: TYPE_TB
- en: '| less letters | .0633 | 3.4E-06 | .0009 | .0047 |'
  prefs: []
  type: TYPE_TB
- en: '| more letters | .0131 | .0046 | .0209 | .0016 |'
  prefs: []
  type: TYPE_TB
- en: '| rhyming word | 5.7E-07 | 1.4E-10 | 4.3E-08 | .0833 |'
  prefs: []
  type: TYPE_TB
- en: '| word before | .10560 | 1.1E-06 | 1.1E-11 | 1.9E-06 |'
  prefs: []
  type: TYPE_TB
- en: '| word not containing | 6.3E-05 | .1573 | 1 | .3173 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 25: The results for the McNemar test we ran to assess the statistical
    significance of the differences in maximum performance between the original best
    prompt and the prompt estimated to be optimal across all paraphrases for each
    task in the LMentry benchmark. Significant max differences (p-value<0.05) are
    highlighted.'
  prefs: []
  type: TYPE_NORMAL
