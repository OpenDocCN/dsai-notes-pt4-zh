- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:42:25'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating
    LLMs, Knowledge Graphs, and Controllable Diffusion Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16333](https://ar5iv.labs.arxiv.org/html/2406.16333)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yichen Sun, Zhixuan Chu^∗, Zhan Qin^∗, Kui Ren
  prefs: []
  type: TYPE_NORMAL
- en: Zhejiang University
  prefs: []
  type: TYPE_NORMAL
- en: '{yichensun,zhixuanchu,qinzhan,kuiren}@zju.edu.cn'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The rapid advancement of Text-to-Image(T2I) generative models has enabled the
    synthesis of high-quality images guided by textual descriptions. Despite this
    significant progress, these models are often susceptible in generating contents
    that contradict the input text, which poses a challenge to their reliability and
    practical deployment. To address this problem, we introduce a novel diffusion-based
    framework to significantly enhance the alignment of generated images with their
    corresponding descriptions, addressing the inconsistency between visual output
    and textual input. Our framework is built upon a comprehensive analysis of inconsistency
    phenomena, categorizing them based on their manifestation in the image. Leveraging
    a state-of-the-art large language module, we first extract objects and construct
    a knowledge graph to predict the locations of these objects in potentially generated
    images. We then integrate a state-of-the-art controllable image generation model
    with a visual text generation module to generate an image that is consistent with
    the original prompt, guided by the predicted object locations. Through extensive
    experiments on an advanced multimodal hallucination benchmark, we demonstrate
    the efficacy of our approach in accurately generating the images without the inconsistency
    with the original prompt. The code can be accessed via [https://github.com/TruthAI-Lab/PCIG](https://github.com/TruthAI-Lab/PCIG).
  prefs: []
  type: TYPE_NORMAL
- en: '^*^*footnotetext: Corresponding author.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rapid advancement of Text-to-Image (T2I) generative models has revolutionized
    the field of computer vision, enabling the synthesis of high-quality images guided
    by textual descriptions. These models, such as DALL-E [[1](#bib.bib1), [2](#bib.bib2)],
    Stable Diffusion [[3](#bib.bib3)], and GLIDE [[4](#bib.bib4)], have shown remarkable
    progress in generating visually appealing and semantically relevant images. However,
    despite their impressive performance, these models often generate contents that
    contradict the input text, posing significant challenges to their reliability
    and practical deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1b8cf29f2b21e94bc8ed1b9c05fdec50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Selected samples generated by DALL-E 3\. Each image represents one
    specific hallucination type. The inconsistency part for each image is highlighted
    in red.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inconsistencies between the visual output and textual input can manifest in
    various forms, such as mismatched object attributes (First image in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Prompt-Consistency Image Generation (PCIG): A Unified
    Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models")),
    inaccurate object placement or count (Fourth image in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework
    Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models")), illegible
    or incorrect text within the image (Second image in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework
    Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models")), and
    the inability to accurately depict real-world entities (Third image in Figure
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Prompt-Consistency Image Generation (PCIG):
    A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion
    Models")). These inconsistencies, also known as hallucinations, can severely impact
    the usefulness and trustworthiness of the generated images, especially in domains
    where accuracy is crucial, such as medical imaging [[5](#bib.bib5), [6](#bib.bib6)],
    autonomous vehicles [[7](#bib.bib7)], and criminal investigation [[8](#bib.bib8)].'
  prefs: []
  type: TYPE_NORMAL
- en: Existing methods have attempted to address these challenges by improving the
    alignment between the input text and the generated image. Attention-based approaches
    [[9](#bib.bib9), [10](#bib.bib10)] have been proposed to better capture the relationships
    between words and visual features, while adversarial training techniques [[11](#bib.bib11)]
    have been employed to enhance the realism and consistency of the generated images.
    However, these methods often struggle with complex scenes and fail to address
    the specific types of inconsistencies mentioned earlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'To tackle these limitations, we introduce Prompt-Consistency Image Generation
    (PCIG), a novel diffusion-based framework that significantly enhances the alignment
    of generated images with their corresponding descriptions. PCIG addresses three
    key aspects of consistency: (1) general objects (GO), ensuring accurate depiction
    of object attributes and placement; (2) text within the image (TEXT), generating
    legible and correct text; and (3) objects that refer to proper nouns existing
    in the real world (PN), which cannot be directly generated by the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Our framework leverages state-of-the-art techniques in natural language processing
    and computer vision. We first employ large language models (LLMs) [[12](#bib.bib12)]
    to extract objects from the input prompt and construct a knowledge graph to predict
    the locations of these objects in the generated image. LLMs, such as GPT-3 [[13](#bib.bib13)]
    and BERT [[14](#bib.bib14)], have shown remarkable capabilities in understanding
    and generating human language. By integrating LLMs into our framework, we enable
    a deeper understanding of the prompt and its relationships, guiding the subsequent
    image generation process.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we utilize a controllable diffusion model [[15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17)] to generate an image consistent with the original prompt, guided
    by the predicted object locations. Controllable diffusion models allow for more
    fine-grained control over the image generation process by incorporating additional
    constraints or conditions. For general objects (GO), the model focuses on accurate
    attribute depiction and spatial arrangement. To handle text within the image (TEXT),
    we incorporate a visual text generation module [[18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)] that specializes in rendering legible and semantically correct
    text. Recent advances in visual text generation have shown promising results in
    producing realistic and readable text in images. Finally, for objects referring
    to proper nouns (PN), we propose a novel approach that searches for representative
    images of the entities and seamlessly integrates them into the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: Through extensive experiments on an advanced multimodal hallucination benchmark
    [[21](#bib.bib21)], we demonstrate the efficacy of PCIG in generating images that
    align with the original prompt, significantly reducing inconsistencies across
    all three key aspects. Our unified framework achieves state-of-the-art performance,
    outperforming existing T2I models in terms of object hallucination accuracy, textual
    hallucination accuracy, and factual hallucination accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Contributions.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '(1) We introduce PCIG, a novel framework that integrates LLMs, knowledge graphs,
    and controllable diffusion models to generate prompt-consistent images. (2) We
    propose a comprehensive approach to address three key aspects of consistency:
    general objects, text within the image, and objects referring to proper nouns.
    (3) We conduct extensive experiments on a multimodal hallucination benchmark,
    demonstrating the superiority of PCIG over existing T2I models in terms of consistency
    and accuracy. (4) We provide insights into the effectiveness of integrating LLMs
    and knowledge graphs for prompt understanding and object localization in the image
    generation process.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Text-to-image Diffusion Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Denoising Diffusion Probabilistic Model [[9](#bib.bib9), [10](#bib.bib10)] and
    its subsequent studies [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24),
    [25](#bib.bib25), [4](#bib.bib4), [2](#bib.bib2)] have showcased impressive capabilities
    in generating high-quality images guided by textual prompts. These models employ
    iterative denoising steps starting from a random noise map to learn the process
    of text-to-image generation. Latent Diffusion Model (LDM) [[25](#bib.bib25)] takes
    advantage of iterative denoising steps in a latent space, aiming to enhance text-to-image
    alignment and reduce training complexity while generating high-quality images
    from textual descriptions. Stable Diffusion and SDXL [[3](#bib.bib3)] are applications
    of the Latent Diffusion method in text-to-image generation but trained with additional
    data and a powerful CLIP [[26](#bib.bib26)] text encoder. DALL-E 2 [[2](#bib.bib2)]
    and DALL-E 3 [[1](#bib.bib1)], state-of-the-art text-to-image generation model
    developed by OpenAI, achieve photorealistic T2I generation using diffusion-based
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Controllable Image Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As text description cannot precisely control the position of generated instances,
    Some controllable text-to-image generation methods [[27](#bib.bib27), [15](#bib.bib15),
    [28](#bib.bib28), [29](#bib.bib29), [17](#bib.bib17), [16](#bib.bib16), [30](#bib.bib30),
    [31](#bib.bib31)] introduce spatial conditioning controls to guide the image generation
    process. They extend the pre-trained T2I model [[25](#bib.bib25)] to integrate
    layout information into the generation and achieve control of instances’ position.
    GLIGEN [[15](#bib.bib15)], MIGC [[17](#bib.bib17)], and InstanceDiffusion [[16](#bib.bib16)]
    are state-of-the-art methods which can support controlled image generation using
    discrete conditions such as bounding boxes. By integrating spatial conditioning
    controls, these methods enable users to have control over the positioning of instances
    in generated images. This advancement allows for fine-grained manipulation and
    customization in the image generation process.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Visual Text Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current mainstream text-to-image generation models, like Stable Diffusion, excel
    at producing high-quality images. However, they struggle to generate accurate
    and legible text on these images. To address this limitation, recent research
    studies [[19](#bib.bib19), [20](#bib.bib20), [18](#bib.bib18), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34)] have focused on integrating clear and readable
    text into images by introducing glyph conditions in the latent space. These advancements,
    particularly, GlyphControl [[20](#bib.bib20)] and AnyText [[18](#bib.bib18)],
    can be seamlessly plugged into existing diffusion models, allowing for more precise
    rendering of text on generated images.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Knowledge Graph and LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Knowledge Graph (KGs) are structured multirelational knowledge bases that typically
    contain a set of facts. Each fact in a KG is stored in the form of triplet $(s,r,o)$
    denotes the relation connecting the subject and object entity. KGs are crucial
    for various applications as they offer accurate explicit knowledge [[35](#bib.bib35),
    [36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]. LLM, pre-trained on the
    large-scale corpus, such as ChatGPT [[13](#bib.bib13)] and GPT-4 [[12](#bib.bib12)]
    have showcased their remarkable capabilities in engaging in human-like communication
    and understanding complex queries, bringing a trend of incorporating LLMs in various
    fields [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42),
    [43](#bib.bib43), [44](#bib.bib44)]. By incorporating KGs, LLMs can benefit from
    the extensive knowledge stored in a structured and explicit manner. This integration
    enables LLMs to have a better understanding of the information contained in KGs,
    which also enhance the performance and interpretability of LLMs in various downstream
    tasks [[45](#bib.bib45)]. In our work, we leverage the knowledge retrieved from
    KGs to improve prompt analysis and object localization, enhancing the overall
    effectiveness of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c79bf186abc794c92e73027cae174b77.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The pipeline of our PCIG method, using the example "A blue basketball
    jersey with the Golden State Warriors logo and ’Stephen Curry’ written on it."'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we first provide a detailed definition of consistency hallucination
    in Sec [3.1](#S3.SS1 "3.1 Consistency Hallucination Definition ‣ 3 Method ‣ Prompt-Consistency
    Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs,
    and Controllable Diffusion Models"). Following that, we delve into the details
    of our framework with object extraction and classification in Sec [3.2](#S3.SS2
    "3.2 Object Extraction and Classification ‣ 3 Method ‣ Prompt-Consistency Image
    Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and
    Controllable Diffusion Models"), relation extraction in Sec [3.3](#S3.SS3 "3.3
    Relation Extraction ‣ 3 Method ‣ Prompt-Consistency Image Generation (PCIG): A
    Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion
    Models"), object localization in Sec [3.4](#S3.SS4 "3.4 Object Localization ‣
    3 Method ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating
    LLMs, Knowledge Graphs, and Controllable Diffusion Models"), and non-hallucinatory
    image generation in Sec [3.5](#S3.SS5 "3.5 Prompt-Consistency Image Generation
    ‣ 3 Method ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating
    LLMs, Knowledge Graphs, and Controllable Diffusion Models"). More details of our
    PCIG framework can be seen in Figure [2](#S3.F2 "Figure 2 ‣ 3 Method ‣ Prompt-Consistency
    Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs,
    and Controllable Diffusion Models").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Consistency Hallucination Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before delving into the methodology, it is essential to first define the types
    of hallucinations more detailed. Based on the MHaluBench [[21](#bib.bib21)] benchmark,
    our focus is centered upon four primary types of hallucinations that arise in
    text-to-image generation: (1) AH(attribute hallucinations), where the attributes
    of objects in the generated images are incongruous with the provided prompts;
    (2) OH(object hallucinations), where the number, placement, or other aspects of
    objects differ from the provided prompts; (3) SCH(scene-text hallucinations),
    where the textual content within the generated images does not align with the
    given prompts;(4) FH(factual hallucinations), where the depicted properties of
    objects contradict their real-world counterparts. While there are numerous other
    issues related to image hallucinations, this paper focuses chiefly on the aforementioned
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Object Extraction and Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object Extraction. The initial step of the method involves a meticulous process
    of identifying objects and their attributes from the textual prompt. Given the
    initial prompt $P$ is related to factual hallucinations (FH). This categorization
    is critical as it not only enables us to handle each hallucination problems separately
    for different types of objects but also lays the groundwork for subsequent relational
    and spatial analyses by clearly defining the nature and context of each object
    within the image.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Relation Extraction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Relationship Recognition. Once the objects are detected, GPT-4 determines the
    spatial relationships and interactions between the detected objects for the initial
    prompt. Let $R=\{r_{i}\}_{i=1...N_{r}}$ being the set of all possible relationships
    in the initial prompt. The construction of the knowledge graph using GPT-4 is
    a critical step in our method. It provides a structured and detailed representation
    of the initial prompt, capturing the relationships and interactions between objects
    in a way that goes beyond simple semantic features [[46](#bib.bib46)]. This enriched
    representation proves advantageous for subsequent steps of object localization
    and image generation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Object Localization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Spatial Organization. Building upon the relationship extraction, this part focuses
    on the spatial organization of objects within the canvas. The process begins by
    identifying the node with the max degree $V_{m}$ indicates the object’s dimensions
    within the space. The bounding boxes are precisely structured, conforming to exact
    dimensional specifications and coordinate precisions, ensuring that every object
    is proportionately and accurately depicted within the generated image.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Prompt-Consistency Image Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building upon the previously described steps, we have successfully secured a
    series of well-defined bounding boxes for every object on the canvas. Our objective
    is to leverage these bounding boxes to generate corresponding images wherein the
    positioning of objects closely mirrors the layout specified by $BB$. To this end,
    we employ a controllable text-to-image model as our primary framework of the model
    that is specifically designed to accept bounding boxes as input, enabling precise
    manipulation of image outcomes. A visual text generation module is incorporated
    with the model, designated to handle linguistic elements, forming the essence
    of our integrated system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our system categorizes inputs into three segments as mentioned in Sec. [3.2](#S3.SS2
    "3.2 Object Extraction and Classification ‣ 3 Method ‣ Prompt-Consistency Image
    Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and
    Controllable Diffusion Models"): GO, TEXT, and PN for image generation. For GO,
    we input both the bounding box and its caption directly into the main framework
    of the model. For TEXT, we input the textual content and its corresponding bounding
    box into a visual text generation module. This module incorporates narrative elements
    into the visual output. For PN, we use a search engine to find representative
    images of the objects. These images, along with their bounding boxes, are seamlessly
    integrated into the model’s primary input stream. By categorizing objects and
    applying specific generation paradigms, our model prevents the generation of images
    with hallucination features. This methodical approach results in photorealistic
    and prompt-consistency images, eliminating the challenges posed by hallucinations.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experiments Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dataset.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/27d008cfcdd78712187e0c86c4ab2fce.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Compared with multiple text-to-image generation methods. Our method
    shows comparable performance in all aspects.'
  prefs: []
  type: TYPE_NORMAL
- en: MHaluBench [[21](#bib.bib21)] is a benchmark which encompasses the content from
    text-to-image generation, aiming to rigorously assess the advancements in multimodal
    hallucination detectors. The benchmark has been meticulously curated to include
    220 exemplars dedicated to Text-to-Image Generation with 158 are hallucinatory
    and 62 are non-hallucinatory. Specifically, it includes 137 prompts which will
    generate images with object and attribute hallucination potentially, 63 prompts
    with textual hallucination, 18 prompts with factual hallucination, 2 prompts with
    combination of factual hallucination and textual hallucination. In one exemplar,
    it contains the original prompt augmented through ChatGPT to include more specific
    information, the presence of hallucination, the analysis for hallucination, and
    the generated image based on the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: Implement Details and Evaluation Metrics.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our pipeline is training-free and comprises three pre-trained models. We employ
    the GPT-4 [[12](#bib.bib12)] as the base LLMs to generate bounding box for identified
    objects and choose InstanceDiffusion [[16](#bib.bib16)] as primary controllable
    text-to-image model while AnyText [[18](#bib.bib18)] as text-generation module.
    We utilize UniHD [[21](#bib.bib21)], which will return a label represents whether
    the input image with corresponding prompt is hallucinatory or not, as our hallucination
    detection method for generated images. We calculate the accuracy for each hallucination
    type mentioned above, including object hallucination accuracy (OH acc.), textual
    hallucination accuracy (TH acc.), factual hallucination accuracy (FH acc.), textual
    and factual hallucination accuracy (TFH acc.), and overall accuracy for evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | OH Acc.(%) | TH Acc.(%) | FH Acc.(%) | TFH Acc.(%) | Overall
    Acc.(%) |'
  prefs: []
  type: TYPE_TB
- en: '| Text-to-Image | SDv1.6 [[25](#bib.bib25)] | 15.33 | 11.11 | 22.22 | 0.00
    | 14.55 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SDXL [[3](#bib.bib3)] | 18.98 | 9.52 | 8.33 | 0.00 | 15.91 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DALL-E 2 [[2](#bib.bib2)] | 24.82 | 7.94 | 0.00 | 0.00 | 17.73 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DALL-E 3 [[1](#bib.bib1)] | 60.58 | 26.98 | 9.99 | 0.00 | 45.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Layout-to-Image | GLIGEN [[15](#bib.bib15)] | 88.32 | 7.94 | 22.22 | 0.00
    | 59.09 |'
  prefs: []
  type: TYPE_TB
- en: '|  | MIGC [[17](#bib.bib17)] | 94.16 | 11.11 | 8.33 | 0.00 | 63.18 |'
  prefs: []
  type: TYPE_TB
- en: '|  | InstanceDiffusion [[16](#bib.bib16)] | 95.62 | 9.52 | 22.22 | 0.00 | 64.09
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | PCIG (ours) | 94.89 | 82.54 | 77.78 | 50.00 | 89.55 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Experimental results of our framework and various baseline on MHaluBench
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Baseline.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our baseline divides into two parts. The first part is the comparison with the
    most representative generative models, including Stable Diffusion v1.6 [[25](#bib.bib25)],
    SDXL [[3](#bib.bib3)], DALL-E 2 [[2](#bib.bib2)], and DALL-E 3 [[1](#bib.bib1)],
    which generate visually detailed images directly based on the prompt in the benchmark.
    The second part is the comparison with the state-of-the-art controllable text-to-image
    models, also named layout-to-image models, including GLIGEN [[15](#bib.bib15)],
    MIGC [[17](#bib.bib17)], and InstanceDiffusion [[16](#bib.bib16)], which introduce
    spatial conditioning controls to guide the image generation process. We will use
    the bounding box generated in the first three steps to guide the process of image
    generation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/693c6862f9f8df0ebc982ba90c4a9793.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Ablation study on knowledge graph construction. Results become inaccurate
    in object locations when the proposed module is disable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/20b0d9694c49f683efe70c9437772e50.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Ablation study on object extraction. Results become inaccurate in
    object count and attribute when the proposed module is disable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37830df24463ef7ed9cc02cd658aa5d8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Ablation study on text generation module. Results become inaccurate
    in visual text when the proposed module is disable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2533ee03cd82fa1e135239621b8e3812.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Bounding box generated by different LLM with original prompt "Six
    giraffes in a grassy plain with trees in the background.".'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | OH Acc.(%) | TH Acc.(%) | FH Acc.(%) | TFH Acc.(%) | Overall Acc.(%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| w/o KG extraction | 64.96 | 82.54 | 77.78 | 0.00 | 70.45 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Object extraction | 75.91 | 7.94 | 11.11 | 0.00 | 50.45 |'
  prefs: []
  type: TYPE_TB
- en: '| w/o Text module | 95.62 | 9.52 | 22.22 | 0.00 | 64.09 |'
  prefs: []
  type: TYPE_TB
- en: '| model (ours) | 94.89 | 82.54 | 77.78 | 50.00 | 89.55 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Ablation results of our PCIG method on MHaluBench dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Experimental Results and Qualitative Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Experimental Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [1](#S4.T1 "Table 1 ‣ Implement Details and Evaluation Metrics. ‣ 4.1
    Experiments Settings ‣ 4 Experiments ‣ Prompt-Consistency Image Generation (PCIG):
    A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion
    Models") shows that PCIG outperforms the baseline models in all metrics. It is
    worth noting that the object hallucination accuracy of all text-to-image models,
    especially in Stable Diffusion [[25](#bib.bib25)] and DALL-E 2 [[2](#bib.bib2)],
    is extremely low. This suggests that these models struggle to generate images
    that align with the given prompts under such conditions. On the other hand, PCIG
    and other competitive layout-to-image models demonstrate exceptional abilities
    in accurately generating objects and their attributes in image generation. In
    terms of text hallucination accuracy, factual hallucination accuracy, and textual
    and factual hallucination accuracy, all baseline models perform poorly. In contrast,
    PCIG stands out from the rest. With the help of prompt analysis and text generation
    module, PCIG showcases exceptional performance in both text hallucination accuracy
    and factual hallucination accuracy. It surpasses the baseline models, highlighting
    its impressive capabilities in text generation and factual object generation.
    The corresponding prompt template is shown in Figure [8](#A1.F8 "Figure 8 ‣ Appendix
    A Appendix / supplemental material ‣ Prompt-Consistency Image Generation (PCIG):
    A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion
    Models")'
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative Analysis.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Through visualization of generated images, we compare our PCIG with competitive
    text-to-image generation models (SD [[25](#bib.bib25)], SDXL [[3](#bib.bib3)],
    DALL-E 2 [[2](#bib.bib2)], and DALL-E 3 [[1](#bib.bib1)]). As depicted in Figure
    [3](#S4.F3 "Figure 3 ‣ Dataset. ‣ 4.1 Experiments Settings ‣ 4 Experiments ‣ Prompt-Consistency
    Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs,
    and Controllable Diffusion Models"), The first row represents the prompts given
    to generate images and the left column represents the model used to generate images
    based on prompts. As a result, Stable Diffusion, SDXL, DALL-E 2, and DALL-E 3
    shows different types of visualization errors during generation, including the
    inconsistency between the prompts and object attributes in generated images (column
    1 and 2), the inconsistency between the prompts and object locations in generated
    images (column 3), the inconsistency between the prompts and the number of objects
    in generated images (column 4), text generation error (column 5), and factual
    object generation error (column 6). In contrast, Leveraging the capabilities of
    prompt analysis and text generation module, our PCIG presents accurate and vivid
    images consistent with the original prompts, as shown in the last row.'
  prefs: []
  type: TYPE_NORMAL
- en: '| model | GLIGEN [[15](#bib.bib15)] | MIGC [[17](#bib.bib17)] | InstanceDiffusion
    [[16](#bib.bib16)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | OH Acc.(%) | TH Acc.(%) | OH Acc.(%) | TH Acc.(%) | OH Acc.(%) | TH Acc.(%)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Base. | 88.32 | 7.94 | 94.16 | 11.11 | 95.62 | 9.52 |'
  prefs: []
  type: TYPE_TB
- en: '| Base. w/ text module | 89.05 | 76.19 | 94.16 | 79.37 | 94.89 | 82.54 |'
  prefs: []
  type: TYPE_TB
- en: '| $\Delta$ | +0.73 | +68.25 | +0.00 | +68.25 | -4.27 | +73.02 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison results of different base controllable text-to-image model
    with and without text module on MHaluBench dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: w/o KG extraction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For w/o KG extraction, the ablation experiment locate the identified objects
    without relation extraction and knowledge graph construction, which means the
    lack of relation and spatial analysis for prompts. The corresponding prompt template
    is shown in Figure [9](#A1.F9 "Figure 9 ‣ Appendix A Appendix / supplemental material
    ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating
    LLMs, Knowledge Graphs, and Controllable Diffusion Models"). Table [2](#S4.T2
    "Table 2 ‣ Baseline. ‣ 4.1 Experiments Settings ‣ 4 Experiments ‣ Prompt-Consistency
    Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs,
    and Controllable Diffusion Models") reveals that when the model lacks the guidance
    of a knowledge graph, it struggles to fully comprehend the relationship between
    identified objects. As a result, it is unable to provide accurate localization
    of objects. In Figure [4](#S4.F4 "Figure 4 ‣ Baseline. ‣ 4.1 Experiments Settings
    ‣ 4 Experiments ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework
    Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models"), the comparison
    between our method and the ablation results is depicted. The ablation experiments
    clearly illustrate the problems that arise when there is a lack of objects mentioned
    in the prompt (column 1 and 4). Additionally, they highlight the inaccuracies
    in the positional relationships between the objects (rest of column). These findings
    emphasize the importance of relation extraction and knowledge graph construction
    in prompt analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: w/o object extraction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For w/o object extraction, the ablation experiment focuses on extracting relationships
    between objects without considering specific object information. The corresponding
    prompt template is shown in Figure [10](#A1.F10 "Figure 10 ‣ Appendix A Appendix
    / supplemental material ‣ Prompt-Consistency Image Generation (PCIG): A Unified
    Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion Models").
    Table [2](#S4.T2 "Table 2 ‣ Baseline. ‣ 4.1 Experiments Settings ‣ 4 Experiments
    ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating
    LLMs, Knowledge Graphs, and Controllable Diffusion Models") clearly demonstrates
    that when the model lacks object information, it faces challenges in accurately
    identifying object attributes and the number of objects while extracting relationships
    between them. Furthermore, it also struggles in correctly identifying object categories
    when generating textual and factual object. Figure [5](#S4.F5 "Figure 5 ‣ Baseline.
    ‣ 4.1 Experiments Settings ‣ 4 Experiments ‣ Prompt-Consistency Image Generation
    (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable
    Diffusion Models") presents a comparison between our method and the ablation results.
    The ablation experiment vividly highlights the problem of inconsistency between
    the number of objects in the generated image and the expected number of objects
    mentioned in the original prompt. Consequently, the model fails to provide precise
    object number and attribute information due to the absence of object guidance.
    which prove the importance of object extraction in prompt analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: w/o text module.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For w/o text module, the ablation experiment aimed to examine the impact of
    removing the text generation module in our model. The results, shown in Table
    [2](#S4.T2 "Table 2 ‣ Baseline. ‣ 4.1 Experiments Settings ‣ 4 Experiments ‣ Prompt-Consistency
    Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs,
    and Controllable Diffusion Models"), highlight that without the text generation
    module, the model faced challenges in generating accurate text. Figure [6](#S4.F6
    "Figure 6 ‣ Baseline. ‣ 4.1 Experiments Settings ‣ 4 Experiments ‣ Prompt-Consistency
    Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs,
    and Controllable Diffusion Models") provides a visual comparison between our method
    and the ablation results. The ablation experiments demonstrate that errors, such
    as missing and incorrect text, were prevalent without the text generation module.
    These findings reinforce the significance of the text generation module in our
    approach.'
  prefs: []
  type: TYPE_NORMAL
- en: Different base controllable text-to-image models.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this section, we conducted ablation experiments using different baseline
    controllable text-to-image generation models, including GLIGEN [[15](#bib.bib15)],
    MIGC [[17](#bib.bib17)], and InstanceDiffusion [[16](#bib.bib16)], with and without
    a text generation module. Table [3](#S4.T3 "Table 3 ‣ Qualitative Analysis. ‣
    4.2 Experimental Results and Qualitative Analysis ‣ 4 Experiments ‣ Prompt-Consistency
    Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs,
    and Controllable Diffusion Models") displays the outcomes of ablation experiments
    conducted on various controllable T2I models, focusing on object hallucination
    accuracy and text hallucination accuracy. The findings reveal that utilizing different
    models, with or without a text generation module, both yields outstanding results
    for object hallucination accuracy. Furthermore, the presence of a text generation
    module significantly enhances text hallucination accuracy. This implies that the
    text generation module can be seamlessly integrated into different base models
    to improve text generation capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Different LLM for prompt analysis.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this ablation experiment, we test the performance of different language
    models (LLMs) for prompt analysis. The LLMs we used are GPT4-turbo [[12](#bib.bib12)],
    GPT3.5-turbo [[13](#bib.bib13)], LLAMA2-7B [[47](#bib.bib47)], LLAMA2-13B, and
    LLAMA2-70B. We measure the overall accuracies of these models, and the results
    are summarized in Table [4](#S4.T4 "Table 4 ‣ Different LLM for prompt analysis.
    ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ Prompt-Consistency Image Generation (PCIG):
    A Unified Framework Integrating LLMs, Knowledge Graphs, and Controllable Diffusion
    Models"). According to the results, GPT4-turbo demonstrats the highest level of
    competitiveness among the LLMs tested. On the other hand, LLAMA2-7B performs the
    least effectively compared to the other models. Figure [7](#S4.F7 "Figure 7 ‣
    Baseline. ‣ 4.1 Experiments Settings ‣ 4 Experiments ‣ Prompt-Consistency Image
    Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and
    Controllable Diffusion Models") displays the bounding boxes generated by different
    language models when analyzing the prompt "Six giraffes in a grassy plain with
    trees in the background". GPT4-turbo accurately identifies all objects and provides
    reasonable positions. GPT3.5-turbo can identify all objects, but the positions
    it generates are unreasonable. All LLAMA model fail to recognize objects and also
    generate unreasonable positions.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | GPT4-turbo [[12](#bib.bib12)] | LLAMA2-7B [[47](#bib.bib47)] | LLAMA2-13B
    [[47](#bib.bib47)] | LLAMA2-70B [[47](#bib.bib47)] | GPT3.5-turbo [[13](#bib.bib13)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Overall Acc.(%) | 89.54 | 32.27 | 42.27 | 67.27 | 70.91 |'
  prefs: []
  type: TYPE_TB
- en: '| $\Delta$ | +0.00 | -57.27 | -47.27 | -22.27 | -18.64 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparison results of different LLM for our PCIG method on MHaluBench
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we introduced the Prompt-Consistency Image Generation(PCIG),
    a effective approach that significantly enhances the alignment of generated images
    with their corresponding descriptions. Leveraging a state-of-the-art large language
    module, we make a comprehensive prompt analysis and generate bounding box for
    each identified objects. We further integrate a state-of-the-art controllable
    image generation model with a visual text generation module to generate an image
    guided by bounding box. We demonstrate our method could handle various type of
    object category based on the integration of text generation module and search
    engine. Both qualitative and quantitative results demonstrate our superior performance.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our method uses GPT4-turbo as our LLM to finish object extraction, relation
    extraction, and object localization, which costs approximately 0.08$ in one generation
    process. Furthermore, our method have difficulties in generating images with complex
    relationship and interaction between objects as well as with small text. To address
    this concern, A more powerful basic diffusion model would be of great help.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] DALL-E 3, 2023. Available at [https://openai.com/index/dall-e-3/](https://openai.com/index/dall-e-3/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
    Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In
    International conference on machine learning, pages 8821–8831\. Pmlr, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn,
    Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models
    for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,
    Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image
    generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Amirhossein Kazerouni, Ehsan Khodapanah Aghdam, Moein Heidari, Reza Azad,
    Mohsen Fayyaz, Ilker Hacihaliloglu, and Dorit Merhof. Diffusion models in medical
    imaging: A comprehensive survey. Medical Image Analysis, page 102846, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Lei Liu, Xiaoyan Yang, Junchi Lei, Xiaoyang Liu, Yue Shen, Zhiqiang Zhang,
    Peng Wei, Jinjie Gu, Zhixuan Chu, Zhan Qin, et al. A survey on medical large language
    models: Technology, application, trustworthiness, and future directions. arXiv
    preprint arXiv:2406.03712, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Jiaqi Liu, Peng Hang, Xiaocong Zhao, Jianqiang Wang, and Jian Sun. Ddm-lag:
    A diffusion-based decision-making model for autonomous vehicles with lagrangian
    safety enhancement. arXiv preprint arXiv:2401.03629, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Ehsan Nowroozi, Ali Dehghantanha, Reza M Parizi, and Kim-Kwang Raymond
    Choo. A survey of machine learning techniques in adversarial image forensics.
    Computers & Security, 100:102092, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic
    models. Advances in neural information processing systems, 33:6840–6851, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit
    models. arXiv preprint arXiv:2010.02502, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Stanislav Frolov, Tobias Hinz, Federico Raue, Jörn Hees, and Andreas Dengel.
    Adversarial text-to-image synthesis: A review. Neural Networks, 144:187–209, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng
    Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 22511–22521, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and
    Ishan Misra. Instancediffusion: Instance-level control for image generation. arXiv
    preprint arXiv:2402.03290, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc: Multi-instance
    generation controller for text-to-image synthesis. arXiv preprint arXiv:2402.05408,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie.
    Anytext: Multilingual visual text generation and editing. arXiv preprint arXiv:2311.03054,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and
    Xiaodong Lin. Glyphdraw: Learning to draw chinese characters in image synthesis
    models coherently. arXiv preprint arXiv:2303.17870, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yukang Yang, Dongnan Gui, Yuhui Yuan, Weicong Liang, Haisong Ding, Han
    Hu, and Kai Chen. Glyphcontrol: Glyph conditional control for visual text generation.
    Advances in Neural Information Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li,
    Yue Shen, Jinjie Gu, and Huajun Chen. Unified hallucination detection for multimodal
    large language models. arXiv preprint arXiv:2402.03190, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv
    preprint arXiv:2207.12598, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
    Hierarchical text-conditional image generation with clip latents. arXiv preprint
    arXiv:2204.06125, 1(2):3, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L
    Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,
    et al. Photorealistic text-to-image diffusion models with deep language understanding.
    Advances in neural information processing systems, 35:36479–36494, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn
    Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
    Learning transferable visual models from natural language supervision. In International
    conference on machine learning, pages 8748–8763\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and
    Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors.
    In European Conference on Computer Vision, pages 89–106\. Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi
    Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation
    for controllable image generation. In Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, pages 18370–18380, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion:
    Fusing diffusion paths for controlled image generation. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control
    to text-to-image diffusion models. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision, pages 3836–3847, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng
    Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free
    box-constrained diffusion. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pages 7452–7461, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Pu Cao, Feng Zhou, Qing Song, and Lu Yang. Controllable generation with
    text-to-image diffusion models: A survey. arXiv preprint arXiv:2403.04279, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu
    Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information
    Processing Systems, 36, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Zhixuan Chu, Yan Wang, Longfei Li, Zhibo Wang, Zhan Qin, and Kui Ren.
    A causal explainable guardrails for large language models. arXiv preprint arXiv:2405.04160,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and S Yu Philip.
    A survey on knowledge graphs: Representation, acquisition, and applications. IEEE
    transactions on neural networks and learning systems, 33(2):494–514, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Yan Wang, Zhixuan Chu, Xin Ouyang, Simeng Wang, Hongyan Hao, Yue Shen,
    Jinjie Gu, Siqiao Xue, James Y Zhang, Qing Cui, et al. Enhancing recommender systems
    with large language model reasoning graphs. arXiv preprint arXiv:2308.10835, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Jing Zhang, Bo Chen, Lingxi Zhang, Xirui Ke, and Haipeng Ding. Neural,
    symbolic and neural-symbolic reasoning on knowledge graphs. AI Open, 2:14–35,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Heng-Shiou Sheu, Zhixuan Chu, Daiqing Qi, and Sheng Li. Knowledge-guided
    article embedding refinement for session-based news recommendation. IEEE Transactions
    on Neural Networks and Learning Systems, 33(12):7921–7927, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,
    Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,
    et al. Palm 2 technical report. arXiv preprint arXiv:2305.10403, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie
    Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa,
    Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Gangwei Jiang, Caigao Jiang, Siqiao Xue, James Y Zhang, Jun Zhou, Defu
    Lian, and Ying Wei. Towards anytime fine-tuning: Continually pre-trained language
    models with hypernetwork prompt. arXiv preprint arXiv:2310.13024, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Siqiao Xue, Fan Zhou, Yi Xu, Hongyu Zhao, Shuo Xie, Caigao Jiang, James
    Zhang, Jun Zhou, Peng Xu, Dacheng Xiu, et al. Weaverbird: Empowering financial
    decision-making with large language model, knowledge base, and search engine.
    arXiv preprint arXiv:2308.05361, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Yan Wang, Zhixuan Chu, Xin Ouyang, Simeng Wang, Hongyan Hao, Yue Shen,
    Jinjie Gu, Siqiao Xue, James Zhang, Qing Cui, et al. Llmrg: Improving recommendations
    through large language model reasoning graphs. In Proceedings of the AAAI Conference
    on Artificial Intelligence, volume 38, pages 19189–19196, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Zhixuan Chu, Yan Wang, Qing Cui, Longfei Li, Wenqing Chen, Sheng Li, Zhan
    Qin, and Kui Ren. Llm-guided multi-view hypergraph learning for human-centric
    explainable recommendation. arXiv preprint arXiv:2401.08217, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong
    Wu. Unifying large language models and knowledge graphs: A roadmap. IEEE Transactions
    on Knowledge and Data Engineering, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Zhixuan Chu, Lei Zhang, Yichen Sun, Siqiao Xue, Zhibo Wang, Zhan Qin,
    and Kui Ren. Sora detector: A unified hallucination detection for large text-to-video
    models. arXiv preprint arXiv:2405.04180, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix / supplemental material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we first outline the prompt template in Figure [8](#A1.F8
    "Figure 8 ‣ Appendix A Appendix / supplemental material ‣ Prompt-Consistency Image
    Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs, and
    Controllable Diffusion Models") designed to guide the object extraction, relation
    extraction, and object localization. Then we present the prompt template designed
    for ablation study in Figure [9](#A1.F9 "Figure 9 ‣ Appendix A Appendix / supplemental
    material ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating
    LLMs, Knowledge Graphs, and Controllable Diffusion Models") and Figure [10](#A1.F10
    "Figure 10 ‣ Appendix A Appendix / supplemental material ‣ Prompt-Consistency
    Image Generation (PCIG): A Unified Framework Integrating LLMs, Knowledge Graphs,
    and Controllable Diffusion Models"). Furthermore, we present more results of our
    PCIG method in Figure [11](#A1.F11 "Figure 11 ‣ Appendix A Appendix / supplemental
    material ‣ Prompt-Consistency Image Generation (PCIG): A Unified Framework Integrating
    LLMs, Knowledge Graphs, and Controllable Diffusion Models").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/09be7fd30027ff77dbcad23cdbd01a19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Prompt template of prompt analysis in PCIG method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ca932660f3e5bd44d366ed855bcb5fba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Prompt template of ablation study on knowledge graph construction
    in PCIG method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/21808bf7f2f51ffdb5fbcb1a5ce11d1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Prompt template of ablation study on object extraction in PCIG method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5f752db32834cb3478dec96a36b53c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: More results of our PCIG method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/74705dc0b7fbe2ca7cbb65eb9a4648bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: More hallucination results of DALL-E 2.'
  prefs: []
  type: TYPE_NORMAL
