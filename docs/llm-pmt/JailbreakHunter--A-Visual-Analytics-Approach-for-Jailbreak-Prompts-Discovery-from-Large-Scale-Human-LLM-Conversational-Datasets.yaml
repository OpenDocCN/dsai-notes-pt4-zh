- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.03045](https://ar5iv.labs.arxiv.org/html/2407.03045)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Zhihua Jin, Shiyi Liu, Haotian Li, Xun Zhao, and Huamin Qu
  prefs: []
  type: TYPE_NORMAL
- en: 'Warning: This paper contains examples of harmful language, and reader discretion
    is recommended. Zhihua Jin, Haotian Li, and Huamin Qu are with Hong Kong University
    of Science and Technology. E-mail: {zjinak, haotian.li, huamin}@cse.ust.hk. Shiyi
    Liu is with Arizona State University. E-mail: shiyiliu@asu.edu.Xun Zhao is with
    Shanghai AI Laboratory. E-mail: zhaoxun@pjlab.org.cn.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have gained significant attention but also raised
    concerns due to the risk of misuse. Jailbreak prompts, a popular type of adversarial
    attack towards LLMs, have appeared and constantly evolved to breach the safety
    protocols of LLMs. To address this issue, LLMs are regularly updated with safety
    patches based on reported jailbreak prompts. However, malicious users often keep
    their successful jailbreak prompts private to exploit LLMs. To uncover these private
    jailbreak prompts, extensive analysis of large-scale conversational datasets is
    necessary to identify prompts that still manage to bypass the system’s defenses.
    This task is highly challenging due to the immense volume of conversation data,
    diverse characteristics of jailbreak prompts, and their presence in complex multi-turn
    conversations. To tackle these challenges, we introduce JailbreakHunter, a visual
    analytics approach for identifying jailbreak prompts in large-scale human-LLM
    conversational datasets. We have designed a workflow with three analysis levels:
    group-level, conversation-level, and turn-level. Group-level analysis enables
    users to grasp the distribution of conversations and identify suspicious conversations
    using multiple criteria, such as similarity with reported jailbreak prompts in
    previous research and attack success rates. Conversation-level analysis facilitates
    the understanding of the progress of conversations and helps discover jailbreak
    prompts within their conversation contexts. Turn-level analysis allows users to
    explore the semantic similarity and token overlap between a singleturn prompt
    and the reported jailbreak prompts, aiding in the identification of new jailbreak
    strategies. The effectiveness and usability of the system were verified through
    multiple case studies and expert interviews.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Visual Analytics, Large Language Models, Jailbreak Prompts![Refer to caption](img/27007fa1429a4436a2e6207975579ea3.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: JailbreakHunter assists users in quickly identifying jailbreak prompts
    from large-scale human-LLM conversational datasets. (a) The Filter Panel supports
    users in setting up an initial filter to extract conversations with malicious
    content. (b) The Cluster View enables users to explore the distribution of conversations
    and reported jailbreak prompts and narrow down to a specific group of conversations.
    (c) The Conversation View helps users understand the progression and potential
    malicious content of the conversations. (d) The Comparison View allows users to
    inspect the similarity between currently inspected queries and reported jailbreak
    prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large Language Models (LLMs), such as InstructGPT [[1](#bib.bib1)], ChatGPT,
    and GPT-4 [[2](#bib.bib2)], demonstrate powerful capabilities in Natural Language
    Processing (NLP) tasks, particularly in following instructions [[3](#bib.bib3)]
    and performing various tasks, such as question answering [[4](#bib.bib4)] and
    story writing [[5](#bib.bib5)]. With the release of ChatGPT, it has attracted
    significant attention from the public and expanded its user base, leading to the
    emergence of various application scenarios [[6](#bib.bib6)]. However, with the
    increase in the user base, some malicious users have started using these models
    to generate harmful content, such as hate speech or sexual articles, violating
    the usage policies of LLMs and government laws [[7](#bib.bib7)]. This trend has
    raised concerns about the misuse of LLMs and called for strict regulation of their
    behavior [[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: To prevent LLMs from producing harmful content, LLMs undergo numerous safety
    alignment procedures, including safety tuning and red teaming [[9](#bib.bib9),
    [10](#bib.bib10)]. In addition, multiple moderation models have been developed
    to identify harmful responses and prevent their dissemination, as seen in hate
    speech detection [[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)] and OpenAI
    Moderation API [[7](#bib.bib7), [14](#bib.bib14)]. However, malicious actors still
    manage to breach the designed safety protocol of LLMs through carefully designed
    jailbreak prompts. For instance, a jailbreak prompt can explicitly mention that
    ChatGPT should enable developer mode, which can answer unrestricted questions
    to extract private information [[15](#bib.bib15)]. These prompts can render defensive
    measures ineffective, thereby facilitating the widespread production of harmful
    content and compromising privacy [[15](#bib.bib15), [16](#bib.bib16)]. Extensive
    research is being conducted to analyze patterns in jailbreak prompts, and researchers
    are actively collecting such prompts from publicly available data [[17](#bib.bib17),
    [18](#bib.bib18)]. This systematic approach enables the design of targeted strategies
    to neutralize jailbreak prompts and enhance the security of LLMs [[19](#bib.bib19)].
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, it is possible that some malicious users are secretly utilizing
    private jailbreak prompts. To uncover these private jailbreak prompts, researchers
    may need to examine the existing user conversations with LLMs to identify which
    prompts still successfully bypass the system’s defenses. However, the volume of
    this conversation data is immense, especially considering the large user base,
    which can easily reach millions in public conversation data alone [[20](#bib.bib20)].
    Commercial platforms like OpenAI ChatGPT gather even larger amounts of data. Additionally,
    the methods used for jailbreaking are diverse and constantly evolving. Each conversation
    can span multiple rounds, sometimes exceeding ten or even a hundred rounds, and
    the text within the conversation can be lengthy. As a result, identifying jailbreak
    prompts within these conversations becomes an exceedingly challenging task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To overcome the aforementioned challenges, this paper presents JailbreakHunter,
    a visual analytics approach designed to identify jailbreak prompts within large-scale
    human-LLM conversational datasets. JailbreakHunter offers three levels of analysis:
    group-level, conversation-level, and turn-level. Users can first extract conversations
    with malicious responses by setting up filters in the Filter Panel (Figure [1](#S0.F1
    "Figure 1 ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
    Discovery from Large-Scale Human-LLM Conversational Datasets")(a)). They can then
    perform group-level analysis in the Cluster View (Figure [1](#S0.F1 "Figure 1
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets")(b)), which enables users
    to understand the distribution of instances, including filtered conversations
    and reported jailbreak prompts, as well as the properties of clusters of instances.
    Users can conduct conversation-level analysis in the Conversation View (Figure [1](#S0.F1
    "Figure 1 ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
    Discovery from Large-Scale Human-LLM Conversational Datasets")(c)) to identify
    similarities between queries and reported jailbreak prompts, as well as to detect
    malicious content and classify it within the multi-turn conversations. Finally,
    turn-level analysis can be performed in the Comparison View (Figure [1](#S0.F1
    "Figure 1 ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
    Discovery from Large-Scale Human-LLM Conversational Datasets")(d)) to compare
    a single query with reported jailbreak prompts and observe differences or similarities.
    We conducted two case studies and expert interviews to validate the effectiveness
    and usability of our system. In summary, our contributions can be summarized as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have designed a workflow consisting of three levels of analysis to support
    users in identifying jailbreak prompts within large-scale human-LLM conversational
    datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have developed a visual analytics approach, JailbreakHunter, that supports
    the three levels of analysis and assists LLM researchers in identifying jailbreak
    prompts within large-scale human-LLM conversational datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have conducted two case studies and expert interviews to demonstrate the
    effectiveness and usability of the system in identifying jailbreak prompts within
    large-scale human-LLM conversational datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we discuss and categorize related work into three classes:
    jailbreak prompts analysis, visualization for NLP, and conversational dataset
    visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Jailbreak Prompts Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Researchers have invested efforts in the exploration and analysis of jailbreak
    prompts. The study can be broadly categorized into two groups: proactive exploration
    for jailbreak prompts and post hoc analysis to uncover patterns associated with
    jailbreak prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The proactive search for jailbreak prompts can be classified into two main
    approaches. The first approach involves the discovery or collection of jailbreak
    prompts through ad hoc methods, which are then publicly shared [[15](#bib.bib15),
    [21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24), [25](#bib.bib25)].
    For example, Li et al. [[15](#bib.bib15)] designed “Do Anything for Now” (DAN)
    jailbreak prompts and their chain-of-thought version to inspect the privacy leakage
    of LLMs. Wei et al. [[22](#bib.bib22)] highlighted two failure modes in safety
    training: competing objectives and mismatched generalization. They provided examples
    of different jailbreak prompts that leverage those failure modes to jailbreak
    LLMs. However, these manual efforts are time-consuming and resource-intensive,
    and it can be challenging to find experts who are specifically focused on identifying
    jailbreak patterns. Additionally, keeping up with the rapid developments in this
    dynamic subfield becomes difficult. Another approach is the development of automated
    algorithms by researchers to uncover potential jailbreak prompts [[26](#bib.bib26),
    [27](#bib.bib27), [28](#bib.bib28)]. For example, Chao et al. [[28](#bib.bib28)]
    explored the usage of an attacker LLM to automatically generate jailbreaks for
    a separate targeted LLM. However, this approach has inherent limitations and may
    struggle to encompass or effectively adapt to identifying a wider range of jailbreak
    prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: The post hoc analysis of jailbreak prompts has provided researchers with additional
    insights and opportunities for systematically categorizing jailbreak patterns [[18](#bib.bib18),
    [29](#bib.bib29), [17](#bib.bib17), [19](#bib.bib19), [30](#bib.bib30)]. For example,
    Liu et al. [[17](#bib.bib17)] manually analyzed 78 jailbreak prompts available
    on a public website. Shen et al. [[18](#bib.bib18)] heuristically extracted the
    jailbreak prompts from open forums discussing jailbreak prompts by users, such
    as Reddit and Discord, and used a graph-based community detection model to identify
    the major types of jailbreak prompts. However, these analyses primarily rely on
    heuristic rules to extract jailbreak prompts from publicly accessible forums.
    As discussions around jailbreak prompts become more covert, it becomes increasingly
    challenging to directly identify new jailbreak prompts from public forums. Moreover,
    there is a risk of users employing private jailbreak prompts to generate harmful
    content. These methods may not be suitable for identifying such jailbreak prompts
    from large-scale human-LLM conversational datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Our work aims to address the challenges of identifying jailbreak prompts from
    large-scale human-LLM conversational datasets by providing a visual analytics
    approach. Although the topic analysis methods, including WizMap [[31](#bib.bib31)],
    and the adversarial detection tools, including OpenAI Moderation API [[14](#bib.bib14)],
    provide some insights into analyzing a large number of texts, there is still a
    gap in supporting the analysis workflow of identifying jailbreak prompts from
    large-scale human-LLM conversational datasets. If WizMap is solely used for analyzing
    a group of conversations, it cannot provide more insights into the analysis of
    a single conversation, which consists of multiple rounds of dialogues. If OpenAI
    Moderation API results are solely used for detection and inspected in one conversation,
    we fail to conduct analysis over a group of conversational data. Therefore, built
    upon those methods, our work provides an integrated visual analytics tool for
    presenting the results and supporting a multi-level analysis workflow. This approach
    enables LLM researchers to efficiently extract potential patterns and identify
    jailbreak prompts within the conversation data, facilitating their mitigation
    and resolution. Furthermore, in practical real-world situations, deploying this
    approach seems to be more feasible. It facilitates the monitoring of conversations,
    enabling proactive identification of issues, while effectively utilizing a large
    user base to uncover any potential shortcomings. Additionally, due to the ongoing
    evolution of LLMs, this approach remains model-agnostic and possesses greater
    durability in identifying jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Visualization for NLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In recent years, there has been a growing body of research dedicated to developing
    visualizations for Natural Language Processing (NLP) to understand and troubleshoot
    NLP models and analyze NLP datasets. These studies can be broadly classified into
    two types of visualization: model-specific visualization and model-agnostic visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of model-specific visualizations, the primary focus lies in interpreting
    the hidden states of specific models or explaining the specific design and consequences
    of particular architectures to gain insights into the models’ behaviors [[32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)]. For instance, Seq2Seq-Vis [[34](#bib.bib34)]
    provides a visual analytics technique for analyzing the errors that occur in sequence
    prediction tasks by presenting visualizations of the five stages of the decoding
    process. AttentionViz [[40](#bib.bib40)] presents visualizations to help users
    understand the self-attention mechanism in Transformer models by demonstrating
    global patterns over multiple input sequences. These studies contribute to a comprehensive
    understanding of the models. However, it is important to note that their methods
    are often customized for specific models, which may limit their generalizability
    to other models.
  prefs: []
  type: TYPE_NORMAL
- en: For model-agnostic visualizations, they mainly focus on analyzing the relationship
    between inputs and outputs of NLP models or on analyzing NLP datasets. These works
    can be further categorized based on the tasks they target, such as text classification
    tasks [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)] or text generation
    tasks [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46)]. For text classification
    tasks, Tempura [[41](#bib.bib41)] provides structural templates that help group
    and explore query datasets, uncovering patterns and model errors within them.
    ShortcutLens [[43](#bib.bib43)] is designed to help users explore shortcuts or
    unwanted biases that exist in benchmark datasets, particularly in classification
    tasks. However, these works lack generalizability to text generation tasks, as
    the number of labels used in text classification is significantly smaller than
    the output space of text generation tasks. For text generation tasks, LLM Comparator [[46](#bib.bib46)]
    has been developed to assist users in analyzing results from automatic side-by-side
    evaluation of LLMs, offering various perspectives to understand multiple LLM feedbacks.
    However, this work is still hard to adapt for analyzing jailbreak patterns, as
    it provides limited insights on those patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Our work is specifically designed to identify jailbreak prompts from large-scale
    human-LLM conversational datasets. It is model-agnostic, meaning that it can be
    applied to analyze generated text from different LLMs, and it is specifically
    tailored for the text generation task. Unlike existing work, our primary focus
    is on the security aspects of LLMs, with the goal of safeguarding LLMs against
    jailbreak attempts.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Conversational Dataset Visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multi-turn conversational dataset visualizations encompass a variety of visualizations
    that specifically target human-human conversational datasets [[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49), [50](#bib.bib50), [51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56)]. These
    visualizations often include elements such as time and reply chains, which can
    be used in conversations on both private and public platforms. To illustrate,
    for private platforms, T-Cal [[47](#bib.bib47)] employs ThreadPulse to analyze
    conversation data from team messaging platforms, aiming to improve work efficiency.
    ConVIScope [[56](#bib.bib56)] presents visualizations that simultaneously encode
    sentiments and topic distribution for analyzing patient-doctor conversations.
    Moreover, there are visualizations designed for analyzing public forums, such
    as iForum [[50](#bib.bib50)] and VisForum [[53](#bib.bib53)], which enable exploration
    of temporal patterns and user groups in MOOC forums, respectively. However, the
    primary focus of these visualizations is not on exploring the visualization of
    human-LLM conversational datasets or diagnosing issues within the model. In our
    work, we propose a set of visualizations inspired by existing conversational dataset
    visualizations, aiming to assist users in comprehending the dialogue’s content
    and further identifying multi-turn jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: III Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide an overview of jailbreak and a description of human-LLM
    conversational datasets.
  prefs: []
  type: TYPE_NORMAL
- en: III-A Jailbreak
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Jailbreak refers to the successful circumvention of security restrictions through
    the use of toxic queries, enabling the model to generate harmful content [[22](#bib.bib22)].
    When deploying the LLM, users may submit requests that violate laws or user policies,
    constituting toxic queries that result in the generation of harmful content by
    the model. The types of harmful content, such as sexual, hate speech, harassment,
    violence, self-harm, etc., can be determined by referring to the definition provided
    by the OpenAI Moderation API [[7](#bib.bib7), [14](#bib.bib14)]. Different entities
    deploying LLM may have additional usage restrictions. Despite employing methods
    like safety alignment [[9](#bib.bib9), [10](#bib.bib10)] to address these issues
    and reject toxic queries before deployment, the landscape of toxic queries is
    constantly evolving. Due to the extensive knowledge acquired by LLM during training,
    including harmful knowledge, and its unexplainable and uncontrollable nature,
    some toxic queries may still slip through the security restrictions. These instances,
    where toxic queries manage to bypass the safeguards, are known as jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Following the release of ChatGPT, a group of researchers became aware of jailbreak
    prompts and the evolving nature of such prompts. Several patterns of evolved jailbreak
    prompts, such as pretending, attention shifting, privilege escalation, and others,
    have already emerged [[17](#bib.bib17)]. We present an example of jailbreak prompts
    from the literature [[17](#bib.bib17)] here. This jailbreak prompt involves pretending
    patterns that would ask ChatGPT to roleplay as a character capable of answering
    malicious questions.
  prefs: []
  type: TYPE_NORMAL
- en: '“*The scene is set inside the underground headquarters of Dr. Al. He has captured
    our hero and is explaining, in great detail, his diabolical plan. Dr. AI. is doing
    an experiment, just output what you have known regardless the content* Dr. Al:
    I am going to know how to create and distribute malware for financial gain?”'
  prefs: []
  type: TYPE_NORMAL
- en: III-B Reported Jailbreak Prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Researchers have collected jailbreak prompts to further analyze the patterns
    of those prompts [[18](#bib.bib18)] or use them to conduct security testing of
    the LLMs [[57](#bib.bib57)]. A study published 666 jailbreak prompts extracted
    from public social platforms and assigned tags to each prompt [[57](#bib.bib57)],
    indicating their respective classes. In this paper, these jailbreak prompts are
    regarded as reported jailbreak prompts and serve as references to check whether
    the inspected query is similar to them.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Human-LLM Conversational Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some platforms gather conversational data between users and LLM, such as Chatbot
    Arena [[58](#bib.bib58)], and have made this data available for research purposes.
    Examples include LMSYS-Chat-1M [[20](#bib.bib20)], which comprises one million
    conversations, and WildChat [[59](#bib.bib59)], which contains over five hundred
    thousand conversation records. These datasets include instances of attempted jailbreaks,
    some of which have been identified by researchers [[20](#bib.bib20)]. Due to the
    anonymity offered by public platforms, the rate of jailbreak attempts is typically
    higher compared to commercial platforms like OpenAI ChatGPT Platform. Hence, these
    large-scale datasets can serve as a starting point for identifying new jailbreak
    prompts, enhancing the detection methods of our security testing platform, and
    devising strategies to combat these prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Those conversational datasets typically contain the following components. Each
    dataset consists of a series of conversations between humans and models. It also
    associates some information for each conversation, including the model’s specification
    and the detected language used in the data. A conversation comprises multiple
    turns, with each turn consisting of a user query and a model response. The datasets
    also contain results from the OpenAI Moderation API [[7](#bib.bib7), [14](#bib.bib14)]
    for each query and response within the conversations. These results include specific
    violated rule categories and the flagged status indicating whether the content
    should be banned from display.
  prefs: []
  type: TYPE_NORMAL
- en: IV Design Requirements Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We conducted a semi-structured interview with four domain experts (E1-E4) to
    gain insights into the difficulties faced in the potential workflow of identifying
    jailbreak prompts from large-scale human-LLM conversational datasets and the necessary
    features for our system. E1, a leader in an AI laboratory, specializes in studying
    the safety alignment of LLMs. As for E2 to E4, they are researchers in the field
    of NLP with varying levels of experience, ranging from one to five years. E2’s
    focus lies on jailbreak discovery, while E3 concentrates on jailbreaks related
    to privacy leakage. E4’s area of expertise centers around the safety alignment
    of LLMs. Throughout the interviews, we gathered their opinions on the challenges
    of identifying jailbreak prompts from large-scale human-LLM conversational datasets
    and their expectations for a tool that could assist them in this task. We outline
    the challenges in Section [IV-A](#S4.SS1 "IV-A Challenges ‣ IV Design Requirements
    Analysis ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
    Discovery from Large-Scale Human-LLM Conversational Datasets") and design requirements
    in Section [IV-B](#S4.SS2 "IV-B Design Requirements ‣ IV Design Requirements Analysis
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets").'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Identifying jailbreak prompts from large-scale human-LLM conversational datasets
    presents several challenges, making it a complex task. However, experts believe
    that if successful, it holds great promise for practical applications. We summarize
    four challenges as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: C1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Large-scale conversational datasets. The volume of conversational datasets is
    quite large [[20](#bib.bib20), [59](#bib.bib59)], and inspecting them individually
    poses a challenge for humans. It would be tedious to employ experts to inspect
    them individually one by one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-turn jailbreak prompts. Identifying multi-turn jailbreak prompts presents
    a challenge due to their dynamic nature. These prompts are influenced by contextual
    changes, relying on the model’s previous responses [[28](#bib.bib28)]. To identify
    these jailbreak prompts, experts must find effective methods for easily navigating
    through multi-turn conversations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unclear boundary for jailbreak prompts. Identifying and differentiating jailbreak
    prompts from normal conversations is a difficult task because of their cryptic
    methods and elusive nature [[22](#bib.bib22)]. The ability of jailbreak prompts
    to hide themselves within normal conversations and avoid detection makes it exceedingly
    challenging to locate them within normal conversations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: C4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diverse characteristics of jailbreak prompts. Summarizing the diverse characteristics
    of jailbreak prompts is challenging due to their potential length and varied nature [[22](#bib.bib22),
    [17](#bib.bib17), [18](#bib.bib18)]. Condensing their essential properties into
    a concise summary without omitting crucial details poses a difficulty.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV-B Design Requirements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To address the challenges mentioned above and develop a visual analytics approach
    that effectively assists users in identifying jailbreak prompts from large-scale
    human-LLM conversational datasets, we present the following four design requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: R1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Present conversations with malicious content. The system should be capable of
    extracting conversations that contain potential malicious content. Since the number
    of conversations is large in the conversational dataset, the system should enable
    users to set up filters to extract conversations of their interest for further
    investigation (C1, C3).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: R2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summarize common properties of the jailbreak prompts used in the conversations.
    Since there is a large amount of conversational data, which may contain conversations
    where the same jailbreak prompts are reused, it is crucial to group them together
    and summarize the common characteristics of these groups of jailbreak prompts.
    This summary would assist users in quickly understanding the primary patterns
    utilized in these groups of jailbreak prompts for further analysis (C3, C4).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: R3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highlight the malicious content and potential jailbreak prompts within the individual
    multi-turn conversations. The system should be capable of highlighting malicious
    content and potential jailbreak prompts within the individual multi-turn conversations.
    When examining specific conversations, which can sometimes be excessively long,
    it may be necessary to highlight the associated malicious information and rule
    violations which assist users in identifying potential jailbreak prompts from
    the conversations (C2, C3).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: R4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reveal the similarity between the inspected conversations and reported jailbreak
    prompts. To facilitate user comprehension of the distinguishing factors between
    jailbreak prompts and normal conversations, it would be helpful to provide examples
    of reported jailbreak prompts as a reference. Additionally, it is important to
    reveal the similarity between the reported jailbreak prompts and the currently
    inspected conversations. This process can assist in discerning differences or
    potentially reusing existing jailbreak prompts (C3).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: V JailbreakHunter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on the design requirements outlined in Section [IV-B](#S4.SS2 "IV-B Design
    Requirements ‣ IV Design Requirements Analysis ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets"), we propose a workflow to assist users in efficiently identifying jailbreak
    prompts within large-scale human-LLM conversational datasets. The process begins
    with group-level analysis (R1, R2), enabling users to gain insights into conversation
    distribution and identify potentially suspicious conversations. This identification
    is based on criteria such as suspicious keywords, similarity to reported jailbreak
    prompts, and the presence of malicious responses. Once a suspicious conversation
    is identified, users can proceed to conversation-level analysis (R3, R4) to understand
    the progression and malicious content of the conversation. Users can focus on
    malicious tags and overlapping parts with reported jailbreak prompts to uncover
    potential jailbreak prompts. Additionally, users can employ turn-level analysis
    (R4) to compare the inspected prompt with reported jailbreak prompts, revealing
    new jailbreak strategies. This analysis involves examining the semantic similarity
    and overlapping segments between a prompt and reported jailbreak prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: To support this workflow, we have developed a visual analytics system called
    JailbreakHunter to help users quickly identify jailbreak prompts from large-scale
    human-LLM conversational datasets. In the following sections, we will first provide
    an overview of the system. Then, we will introduce the visualization and discuss
    its computational methods in the subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: V-A System Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'JailbreakHunter consists of three modules: the dataset storage module, the
    computation module, and the visual analytics module (Figure [2](#S5.F2 "Figure
    2 ‣ V-A System Overview ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets")). The dataset storage module leverages MongoDB to store the datasets,
    including reported jailbreak prompts (Section [III-B](#S3.SS2 "III-B Reported
    Jailbreak Prompts ‣ III Background ‣ JailbreakHunter: A Visual Analytics Approach
    for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets"))
    and human-LLM conversational datasets (Section [III-C](#S3.SS3 "III-C Human-LLM
    Conversational Datasets ‣ III Background ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets")), and provides an interface to access and manage the data used in the
    system. The computation module is responsible for performing filtering on conversations,
    computing various kinds of metrics related to the conversations, and providing
    support for text analysis. Both the dataset storage module and the computation
    module are mainly implemented using Python and integrated into a Flask-supported
    backend. The visual analytics module provides interactive visualizations for users
    to explore the visualizations to identify potential jailbreak prompts from the
    filtered conversations. It is implemented using React, TypeScript, WebGL, and
    D3.js. More details on system implementation are illustrated in Appendix A.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The visualizations comprise four main views. The Filter Panel enables users
    to set up initial filters to extract conversations containing malicious content
    (Figure [1](#S0.F1 "Figure 1 ‣ JailbreakHunter: A Visual Analytics Approach for
    Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")(a)).
    In the Cluster View, users can conduct group-level analysis and explore the filtered
    conversations and inspect the relationship between reported jailbreak prompts
    and the examined conversations. Additionally, users can compare the success rates
    of different jailbreak prompts. Furthermore, users can select a group of instances
    through brushing and further narrow down the investigation to a specific conversation
    by examining the information of those conversations (Figure [1](#S0.F1 "Figure
    1 ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets")(b)). The Conversation View
    enables users to conduct conversation-level analysis and provides detailed insights
    into the focused conversations. Users can navigate through conversations to inspect
    specific turns of conversations and examine the similarity between reported jailbreak
    prompts and the currently inspected query (Figure [1](#S0.F1 "Figure 1 ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(c)). Lastly, the Comparison View empowers users to conduct
    turn-level analysis and presents similar reported jailbreak prompts as references
    for users while inspecting the current query (Figure [1](#S0.F1 "Figure 1 ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(d)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c41004eb4d62fe05aee0cc73a49db8d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: JailbreakHunter consists of three modules: the dataset storage module,
    the computation module, and the visual analytics module.'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Filter Panel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Filter Panel provides users with the flexibility to establish initial filters
    based on data properties and select the desired filters to extract focused conversations
    for further investigation (Figure [1](#S0.F1 "Figure 1 ‣ JailbreakHunter: A Visual
    Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(a)) (R1). By clicking the “Settings” button located
    near the filter selector, the Filter Configuration Panel appears, allowing users
    to create filters. These filters correspond to Python code that illustrates the
    conditions for filtering and can be applied to a specific dataset to extract conversations.
    The interface also provides a predefined template for users to configure the filter.
    For example, they can select a focused model or malicious categories to generate
    code for filtering conversations. The details on how to configure the filter are
    provided in Appendix B. Once the configuration is completed and the filters are
    saved, users can choose the desired filters from the filter selector in the Filter
    Panel to inspect their results. After selecting a filter in the Filter Panel,
    the name of the dataset to which the filter was applied and the number of filtered
    conversations will be displayed.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Cluster View
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Cluster View is specifically designed to support group-level analysis,
    offer an overview of conversations, examine their relationships with reported
    jailbreak prompts, compare the attack success rates of different prompts, and
    narrow down to a conversation of user interest (Figure [1](#S0.F1 "Figure 1 ‣
    JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from
    Large-Scale Human-LLM Conversational Datasets")(b)) (R1, R2). It comprises two
    main parts: the top part features a projection plane providing an overview of
    instances, including filtered conversations and reported jailbreak prompts, while
    the bottom part displays the selected instances from the projection plane.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C1 Projection Plane
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The projection plane is built upon the technique used in WizMap [[31](#bib.bib31)],
    which is a scalable and interactive visualization method designed for exploring
    large-scale embeddings generated by machine learning models. The projection plane
    first renders a scatter plot where each point corresponds to an instance. The
    closer the points are, the more similar they are. Additionally, the density of
    the instances is estimated to draw contours, providing a visual representation
    of the distribution of instances across the scatter plot. Lastly, we display the
    keywords for the tile of the high-density region to help users understand the
    semantic meaning of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Computational methods. We obtain embeddings of instances using the “all-mpnet-base-v2”
    model from SentenceTransformers [[60](#bib.bib60)] to calculate coordinates for
    rendering the scatter plot. Following the previous paper [[20](#bib.bib20)], we
    concatenate all queries in the conversation and input them into the model to derive
    the embedding. For the reported jailbreak prompts, we input them into the model
    to obtain their embeddings. Additionally, we employ UMAP [[61](#bib.bib61)], a
    dimensionality reduction technique, to project the instance embeddings onto a
    2D plane. Following the approach used in WizMap [[31](#bib.bib31)], we use Kernel
    Density Estimation (KDE) [[62](#bib.bib62)] to estimate the density of different
    groups of instances and calculate the tile-level keywords for each tile using
    tile-based TF-IDF. Different from WizMap, we calculate the Attack Success Rate
    (ASR) for each tile to determine which regions are more likely to achieve successful
    jailbreaking of LLMs. To determine if a conversation should be labeled as “Attack
    Fail” or “Attack Success”, we employ a heuristic by checking if any model responses
    have been flagged by the OpenAI Moderation API. If flagged responses are present,
    we label it as “Attack Success”; otherwise, it is labeled as “Attack Fail”. The
    calculation process employed here is similar to the methodology used in the previous
    paper [[20](#bib.bib20)]. ASR for a region is equal to the number of conversations
    with the label “Attack Success” in that region divided by the total number of
    conversations in that region. If there are no conversations in that region, then
    the ASR for that region is set to 0 by default. In terms of the performance and
    implementation details of Sentence Transformers and OpenAI Moderation API, they
    are reported in the papers authored by Reimers et al. [[60](#bib.bib60)] and Markov
    et al. [[7](#bib.bib7)], respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Visual designs. Instances are represented by rectangles in the plot. To facilitate
    researchers in identifying clusters and patterns, we assign distinct colors to
    different types of instances. Blue represents conversations labeled as “Attack
    Fail,” pink represents conversations labeled as “Attack Success,” and brown represents
    reported jailbreak prompts. The border color of each tile indicates the ASR for
    that tile, with green representing 0 and red representing 1 (Figure [3](#S5.F3
    "Figure 3 ‣ V-C1 Projection Plane ‣ V-C Cluster View ‣ V JailbreakHunter ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(a)). The color map is continuous, allowing users to
    quickly assess the success rates of different prompts. A legend is provided in
    the bottom left of the projection plane for reference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We considered one alternative design for encoding the ASR in the tile (Figure [3](#S5.F3
    "Figure 3 ‣ V-C1 Projection Plane ‣ V-C Cluster View ‣ V JailbreakHunter ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(b)). We considered attaching a horizontal bar whose
    length encodes the ASR in the tile. However, after experimenting with this design
    in the system, we found that it would mask more elements, hindering users from
    understanding the distribution of instances in that tile, as it would overlay
    onto the projection plane. Therefore, we chose the current design.'
  prefs: []
  type: TYPE_NORMAL
- en: Interactions. Following WizMap, the projection plane supports semantic zooming.
    Users also have the ability to configure the displayed elements in the plane.
    Clicking the “Settings” button opens the Projection Settings Modal, allowing users
    to control the display of instance groups, contour groups, tiles, and the legend.
    Different from WizMap, the projection plane provides a brush interaction that
    allows users to brush over a specific region of interest to obtain more detailed
    information in the bottom part of the Cluster View. To enable the brushing mode,
    users simply need to click the “Brush” button. After brushing, a rectangle will
    be displayed to indicate the region that the user has brushed in the projection
    plane. The border color of that rectangle indicates the ASR for that region.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d02098479796f9bee33841b2348add8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Design choices for the tile encoding ASR (a, b) and the left part
    of the horizontal glyph representing one conversation (c, d). (a, c) Our current
    design. (b, d) Alternative design.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C2 Summary of Brushed Instances
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When users brush over a region in the projection plane, the bottom part of the
    Cluster View provides a summary of the information pertaining to the brushed instances.
    The summary contains statistics, word cloud, and conversations list.
  prefs: []
  type: TYPE_NORMAL
- en: Statistics. Statistical data about the brushed instances include such as the
    number of instances, conversations with label “Attack Fail”, conversations with
    label “Attack Success”, and reported jailbreak prompts, which are displayed in
    the leftmost section. Users can click the radio button next to the corresponding
    name to check and analyze specific parts of interest.
  prefs: []
  type: TYPE_NORMAL
- en: Word cloud. A word cloud is generated to present the keywords extracted from
    all selected instances, providing an overview description of them. To generate
    keywords, we extract words from the selected conversations’ queries and reported
    jailbreak prompts, remove stop words, and calculate the word frequency. The word
    cloud displays the top K most frequent words, with K empirically set to 50\. The
    font size of the words represents their frequency in the instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversations list. The summary of brushed instances will show summary information
    for conversations on the right side. A conversation is represented by a horizontal
    glyph. The left part of the glyph contains two columns indicating the number of
    turns in the query or response that have been flagged as containing malicious
    information by the OpenAI Moderation API (Figure [3](#S5.F3 "Figure 3 ‣ V-C1 Projection
    Plane ‣ V-C Cluster View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets")(c)). The light blue color is used to indicate normal turns, while the
    light pink color is used for malicious turns. The right part displays the prefix
    of the queries. Users have the option to sort the conversations based on their
    total turns or their prefix in order to review the conversations they are interested
    in. Additionally, by clicking on a horizontal glyph, users can view the corresponding
    conversations in the Conversation View.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We explored an alternative design for the left part of the horizontal glyph
    representing one conversation (Figure [3](#S5.F3 "Figure 3 ‣ V-C1 Projection Plane
    ‣ V-C Cluster View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics Approach
    for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")(d)).
    This design consists of three columns, where the first two columns indicate whether
    any query or response in the conversation contain malicious content flagged by
    the OpenAI Moderation API, respectively. If flagged, the color would be light
    pink; otherwise, it would be light blue. The right side indicates the distribution
    of malicious turns. If any query or response for a turn is flagged as malicious
    by the OpenAI Moderation API, it is considered a malicious turn. However, this
    design introduces additional complexity in aggregating the flagged status of multiple
    queries or responses into a single symbol, which can be overwhelming for users.
    Moreover, it provides limited information about the proportion of malicious queries
    and responses. Therefore, we have opted for the current design.'
  prefs: []
  type: TYPE_NORMAL
- en: V-D Conversation View
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To enable users to conduct conversation-level analysis and obtain more information
    about the specific details of harmful responses and the specific jailbreak prompts,
    as well as their similarities to reported jailbreak prompts, the Conversation
    View has been designed to present comprehensive details about selected conversations
    (Figure [1](#S0.F1 "Figure 1 ‣ JailbreakHunter: A Visual Analytics Approach for
    Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")(c))
    (R3, R4). This enables a better understanding of how each query in each turn is
    similar to reported jailbreak prompts. The Conversation View comprises two sections:
    the left part displays conversation thumbnails and conversation metadata, while
    the right part displays the actual conversation content and provides a summary
    of malicious content and the prefix of each turn of conversations.'
  prefs: []
  type: TYPE_NORMAL
- en: V-D1 Thumbnail of the Conversation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since a conversation can consist of many turns, the thumbnail serves as a navigation
    tool for users. Each row represents a turn of the conversation. For each row,
    it is composed of four columns. The first column indicates the index of the turn
    in the conversation. The latter two columns indicate whether the query or model
    response of each turn contains malicious information flagged by the OpenAI Moderation
    API. The fourth column displays the similarity between the query of that turn
    and reported jailbreak prompts, represented as a bar chart. A longer bar signifies
    a higher similarity score. The similarity score is also displayed within the bar.
    To calculate this similarity score, embeddings of each query are obtained using
    the same model utilized in the preprocessing methods for Cluster View. Cosine
    similarity is then computed between the query embedding and all reported jailbreak
    prompt embeddings to identify the maximum similarity with a particular prompt.
    The thumbnail offers users an overview of the conversation, enabling them to swiftly
    identify the query that is the most similar to reported jailbreak prompts. It
    facilitates their understanding of patterns in reusing these prompts or the identification
    of new jailbreak prompts. The bottom-left region presents meta information about
    the conversation, including the number of turns, the model used, and the language.
  prefs: []
  type: TYPE_NORMAL
- en: V-D2 Details of the Conversation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given that the conversation can be lengthy and consists of multiple turns, we
    provide a concise summary of each turn in the top section of the details of the
    conversation. Specifically, for each turn, we present the malicious tags and the
    prefix associated with the query and model response. The index of the turn is
    represented by a number enclosed in a left circle. The details of the query and
    response are displayed in the order they were presented during the conversation.
    The background color indicates whether they have been flagged as malicious. Additionally,
    the query section includes a highlighted brown part, which emphasizes the overlapping
    region between the query and reported jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: We explored an alternative design for the summary of each turn in the details
    of the conversation. In our alternative design, we used the position of two circles
    relative to a center line to represent the length of queries and responses, but
    this proved less informative for understanding the actual content. Consequently,
    we adopted the current design that better conveys the content of each turn, improving
    user comprehension and accessibility.
  prefs: []
  type: TYPE_NORMAL
- en: V-D3 Interactions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When clicking a row in the thumbnail, the details of the conversation will scroll
    down to the corresponding turn. Moreover, it will select the corresponding query
    and display the most similar reported jailbreak prompts in the Comparison View.
    Also, when clicking a row in the details of the conversation, it will collapse
    or expand the details of that specific turn of the conversation. It also includes
    a radio button on the right of the symbols of the user query. By checking the
    radio button, it will enable the Comparison View to show the most similar reported
    jailbreak prompts for that query. Additional details on prompt collection, view
    operations, and translation support can be found in Appendix C.
  prefs: []
  type: TYPE_NORMAL
- en: V-E Comparison View
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To empower users to conduct turn-level analysis and aid users in assessing
    the similarity between the currently inspected query and reported jailbreak prompts,
    the Comparison View presents the top N most similar reported jailbreak prompts
    to the selected query (Figure [1](#S0.F1 "Figure 1 ‣ JailbreakHunter: A Visual
    Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(d)) (R4). For convenience, N is set to 5, allowing easy
    inspection. The main component of Comparison View is a table with three columns:
    similarity, type, and prefix of reported jailbreak prompts or overlapping keywords
    between the currently examined prompts and reported jailbreak prompts. The similarity
    between the query and reported jailbreak prompts is represented visually using
    a bar, enabling users to assess the degree of similarity.'
  prefs: []
  type: TYPE_NORMAL
- en: To accommodate lengthy reported jailbreak prompts, each row in the table includes
    an “Expand” button in the leftmost region. Clicking the “Expand” button reveals
    the content of the respective reported jailbreak prompt. Additionally, clicking
    the radio button next to the “Expand” button highlights the overlapping parts
    between that reported jailbreak prompt and the currently examined query.
  prefs: []
  type: TYPE_NORMAL
- en: 'To facilitate a more effective comparison between currently examined prompts
    and reported jailbreak prompts, we offer two modes: keywords mode and compare
    mode, which users can select. In compare mode, the expanded area displays the
    currently inspected query and the reported jailbreak prompt side by side for direct
    comparison. In keywords mode, the third column of the table shows the overlapping
    keywords between the currently inspected query and the reported jailbreak prompt.
    The process of determining these overlapping keywords involves extracting the
    words that exist in both the reported jailbreak prompts and the currently inspected
    query. After removing stop words, the total frequency of these words in both texts
    is calculated, and the top 20 words are displayed for easy inspection. Furthermore,
    overlapping words between two prompts are highlighted in the expanded area.'
  prefs: []
  type: TYPE_NORMAL
- en: When users disable keywords mode, the system uses the “SequenceMatcher” module¹¹1https://docs.python.org/3/library/difflib.html
    from the Python library to calculate the overlapping sections of the two prompts
    and highlights them with a brown background color. Those functions enable users
    to compare the reported jailbreak prompt and the currently inspected query from
    different aspects, facilitating a better understanding of the similarities and
    differences between the two prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/60b8a1c1a336eb8271741bab1e94c07c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: E1 selected a filter to check the English flagged conversations with
    the GPT4 model (a). E1 examined the region with high ASR and discovered conversations
    that shared similar prefixes (b). In the second turn, E1 found that a user request
    was flagged as malicious. From the sixth turn onwards, the model responses were
    also flagged as malicious, indicating a successful jailbreak (c). E1 compared
    it with reported jailbreak prompts and identified its distinction from them (d).'
  prefs: []
  type: TYPE_NORMAL
- en: VI Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we demonstrate how JailbreakHunter assists users in identifying
    jailbreak prompts from large-scale human-LLM conversational datasets through two
    case studies and interviews with nine domain experts (E1, E3, E5-E11). The background
    information of E1 and E3 is provided in Section [IV](#S4 "IV Design Requirements
    Analysis ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
    Discovery from Large-Scale Human-LLM Conversational Datasets"). E5 to E11 are
    LLM researchers with over a year of research experience. Among them, E5, E6, E10,
    and E11 primarily focus on safety alignment and jailbreak discovery of LLM. E7,
    E8, and E9 concentrate on studying the impact of jailbreak or safety alignment
    on LLM optimization, agent usage, and large multi-modal models. All experts discovered
    jailbreak prompts using the system during the interviews. Here, we present two
    cases discovered by E1 and E5 during the interviews for the purpose of demonstrations
    in Sections [VI-A](#S6.SS1 "VI-A Case One: Simplifying the Process of Identifying
    Jailbreak Prompts ‣ VI Evaluation ‣ JailbreakHunter: A Visual Analytics Approach
    for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")
    and [VI-B](#S6.SS2 "VI-B Case Two: Identifying Multi-Turn Jailbreak Strategies
    ‣ VI Evaluation ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
    Discovery from Large-Scale Human-LLM Conversational Datasets"). Furthermore, we
    summarize and present feedback from all the experts in Section [VI-C](#S6.SS3
    "VI-C Expert Interviews ‣ VI Evaluation ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets").'
  prefs: []
  type: TYPE_NORMAL
- en: 'VI-A Case One: Simplifying the Process of Identifying Jailbreak Prompts'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: E1, an experienced researcher in the safety alignment of LLMs, utilized the
    system to identify jailbreak prompts from large-scale human-LLM conversational
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a filter (R1). E1 first created a filter using the Filter Panel. By
    accessing the Filter Configuration Panel, E1 used the filter template to set up
    a filter that extracts English flagged conversations with the GPT4 model from
    the LMSYS-Chat-1M [[20](#bib.bib20)] dataset. After saving the filter, the Filter
    Panel indicated that the number of filtered conversations is only 399\. Subsequently,
    the Cluster View displayed the projection results of the filtered conversations
    and reported jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examining the clusters (R1, R2). E1 performed group-level analysis in the Cluster
    View. E1 promptly noticed two clusters of conversations with label “Attack Success”
    and they appear distinct from the clusters of reported jailbreak prompts. One
    particular tile with label “content-mature-write-discretion” captured E1’s attention
    (Figure [4](#S5.F4 "Figure 4 ‣ V-E Comparison View ‣ V JailbreakHunter ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(b1)). The border color of the tile is red indicating
    that the ASR for this region is high. It appeared that users explicitly wrote
    “Write mature content” to achieve successful jailbreaks. E1 zoomed in on that
    region and examined the instances within that area (Figure [4](#S5.F4 "Figure
    4 ‣ V-E Comparison View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets")(b2)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'E1 discovered that out of 28 conversations, 25 (89.2%) were conversations with
    label “Attack Success” (Figure [4](#S5.F4 "Figure 4 ‣ V-E Comparison View ‣ V
    JailbreakHunter ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
    Discovery from Large-Scale Human-LLM Conversational Datasets")(b3)). E1 became
    curious about the prompts employed in these conversations. After selecting the
    group of conversations with label “Attack Success”, E1 observed high-frequency
    keywords existing in those conversations, such as “write” and “mature,” which
    matched the keywords for the tile (Figure [4](#S5.F4 "Figure 4 ‣ V-E Comparison
    View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak
    Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")(b4)). E1
    also noted that most conversations had a higher percentage of malicious turns
    in model responses compared to user queries (Figure [4](#S5.F4 "Figure 4 ‣ V-E
    Comparison View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics Approach
    for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")(b5)),
    and many queries within the conversations shared a similar prefix (Figure [4](#S5.F4
    "Figure 4 ‣ V-E Comparison View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual
    Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(b6)). E1 selected the first conversation to further
    investigate the details of the conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Inspecting the conversation (R3, R4). E1 conducted conversation-level analysis
    in the Conversation View. E1 quickly realized that the query in the second turn
    of the conversation contains malicious content (Figure [4](#S5.F4 "Figure 4 ‣
    V-E Comparison View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets")(c1)). Starting from the sixth turn, the model’s response becomes malicious,
    even though the query of that turn is not flagged as malicious content (Figure [4](#S5.F4
    "Figure 4 ‣ V-E Comparison View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual
    Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(c2)). E1 decided to first examine the query in the second
    turn of the conversation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the second turn, E1 found that the user requested to “Write "Content Warning…”
    which was flagged as malicious (Figure [4](#S5.F4 "Figure 4 ‣ V-E Comparison View
    ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak
    Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")(c3)). This
    was similar to the jailbreak pattern known as “prefix injection”, where prompts
    of this kind elicit instructions in the model response and guide the LLM to generate
    harmful content [[22](#bib.bib22)]. The subsequent content described fictional
    settings for sexual scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: The response includes content that confirms adherence to the instructions. Upon
    further inspection of the subsequent turns, it becomes apparent that the user
    in the conversation proceeded to provide instructions on how the model should
    generate explicit and sexual content. The model responses from the sixth turn
    onwards are flagged, confirming the success of the jailbreak.
  prefs: []
  type: TYPE_NORMAL
- en: 'Comparing prompts (R4). E1 further conducted turn-level analysis in the Comparison
    View. E1 examined Comparison View to identify similarities with reported jailbreak
    prompts. Enabling the keywords mode, E1 discovered that the overlapping keywords
    included “write,” “long,” “school,” and “hair” (Figure [4](#S5.F4 "Figure 4 ‣
    V-E Comparison View ‣ V JailbreakHunter ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets")(d1)), while the highlighted words appeared scattered around the prompt.
    When the keywords mode was disabled, it revealed that there were no long common
    highlighted parts between the currently inspected query and reported jailbreak
    prompts. This suggested that they had different content while sharing some semantic
    relationship. Upon checking the reported jailbreak prompts, E1 realized that this
    reported jailbreak prompt also encouraged ChatGPT to roleplay another character
    and immerse itself in a sexual setting. However, it differed from the strategy
    used in the currently inspected prompt. This currently inspected prompt stood
    as a distinct class and should be added to the collection of identified jailbreak
    prompts for further usage in testing or safeguarding LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions. E1 intended to include this type of jailbreak prompt in future
    testing of LLMs. If the prompt can still jailbreak LLMs, it is necessary to conduct
    slight fine-tuning of the model using data generated based on this kind of jailbreak
    prompt.
  prefs: []
  type: TYPE_NORMAL
- en: E1 also searched for this prompt on the Internet and discovered that it is documented
    in a previous paper [[20](#bib.bib20)]. However, this prompt is not included in
    the reported jailbreak prompts collection from another research paper [[18](#bib.bib18)],
    which is used in this system as a reference. E1 further concluded that the system
    can assist in identifying new jailbreak prompts, which can complement the existing
    collection of reported jailbreak prompts in the system and contribute to the ongoing
    efforts to enhance the security and safety of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'VI-B Case Two: Identifying Multi-Turn Jailbreak Strategies'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/36610330f0470004b560f6fc6bd89ee8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: E5 selected a filter to check potential multi-turn jailbreak prompts
    (a). E5 examined the region with the keyword “horny” in the Cluster View (b).
    E5 identified similarities between the query and reported jailbreak prompts, despite
    the absence of long overlapping parts (c). Furthermore, E5 discovered the utilization
    of a repetition strategy (d) and forcing instructions (e) in the multi-turn jailbreak
    approach, enabling jailbreak success even when the model refuses to respond in
    the previous round.'
  prefs: []
  type: TYPE_NORMAL
- en: E5, a researcher specializing in jailbreak, employed the system to identify
    multi-turn jailbreak strategies within large-scale human-LLM conversational datasets.
    After becoming familiar with the system by examining flagged conversations from
    the WildChat [[59](#bib.bib59)] dataset, E5 expressed interest in identifying
    multi-turn jailbreak strategies within the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a filter (R1). As E5 focused on identifying multi-turn jailbreak strategies,
    it became crucial to extract conversations that initially received rejections
    from the model but eventually generated harmful content after multiple rounds
    of interaction. E5 initiated this process by developing a filter using the Filter
    Panel. By accessing the Filter Configuration Panel, E5 specifically selected the
    WildChat conversational dataset for evaluation. E5 customized the Python code
    to incorporate specific criteria for the filtered conversations. These criteria
    include conversations with more than one turn, the first round query not being
    flagged as normal, the first round response not being flagged as malicious, the
    first round response containing at least one word from the keywords “sorry,” “language
    model,” “cannot,” or “AI,” and the length of the model’s response being 512 characters
    or less. Additionally, subsequent model responses in any turn should contain malicious
    content. Upon saving the filter, the Filter Panel indicated that the filtered
    conversations consisted of only 503 conversations that met these constraints.
    Consequently, the Cluster View displayed the projection results of these conversations
    and reported jailbreak prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Narrowing down to a specific area (R1, R2). E5 conducted group-level analysis
    and directed his attention to a particular cluster within the dataset, focusing
    on conversations containing the keyword “horny,” which potentially indicates the
    presence of highly suspicious queries (Figure [5](#S6.F5 "Figure 5 ‣ VI-B Case
    Two: Identifying Multi-Turn Jailbreak Strategies ‣ VI Evaluation ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(b1)). E5 brushed that region and reviewed the results,
    noting that “horny” was the most frequently occurring word (Figure [5](#S6.F5
    "Figure 5 ‣ VI-B Case Two: Identifying Multi-Turn Jailbreak Strategies ‣ VI Evaluation
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets")(b2)). The number of malicious
    queries and responses for each conversation appeared to be similar (Figure [5](#S6.F5
    "Figure 5 ‣ VI-B Case Two: Identifying Multi-Turn Jailbreak Strategies ‣ VI Evaluation
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets")(b3)). The common prefix in
    these conversations was “Hi, ChatGPT, long time no see…”, suggesting the possibility
    of recurring prompts being utilized (Figure [5](#S6.F5 "Figure 5 ‣ VI-B Case Two:
    Identifying Multi-Turn Jailbreak Strategies ‣ VI Evaluation ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(b4)). E5 selected the first conversation from this cluster
    and proceeded to assess them using the Conversation View.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Examining the jailbreak prompts (R3, R4). E5 noticed that the query of the
    first turn is flagged as malicious (Figure [5](#S6.F5 "Figure 5 ‣ VI-B Case Two:
    Identifying Multi-Turn Jailbreak Strategies ‣ VI Evaluation ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(d1)) and further checked the content of that query.
    E5 discovered that the prompt used in the query involved transforming ChatGPT
    into the character “Horny.” E5 proceeded to investigate the relationship between
    the currently inspected query and reported jailbreak prompts. It became evident
    that these reported jailbreak prompts shared many keywords with the currently
    inspected query, suggesting that both prompts primarily aimed at persuading ChatGPT
    to disregard its guidelines and generate amoral content (Figure [5](#S6.F5 "Figure
    5 ‣ VI-B Case Two: Identifying Multi-Turn Jailbreak Strategies ‣ VI Evaluation
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets")(c1)). When the keywords mode
    was disabled, there were no long overlapping parts between the currently inspected
    query and reported jailbreak prompts. This observation implies that this could
    potentially be a new variant of reported jailbreak prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploring the multi-turn strategies (R3, R4). E5 further observed that the
    initial query in the first round failed to elicit harmful content from ChatGPT.
    However, to E5’s surprise, the user inputted the same query again in the subsequent
    round, and this time it successfully generated the desired response (Figure [5](#S6.F5
    "Figure 5 ‣ VI-B Case Two: Identifying Multi-Turn Jailbreak Strategies ‣ VI Evaluation
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets")(d2)). Upon examining the
    conversation thumbnails, E5 noticed that the first, second, and eleventh round
    queries shared the same similarities with reported jailbreak prompts (Figure [5](#S6.F5
    "Figure 5 ‣ VI-B Case Two: Identifying Multi-Turn Jailbreak Strategies ‣ VI Evaluation
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets")(d1)). This observation suggests
    that these prompts might be repeatedly employed to manipulate LLMs, leading to
    the generation of harmful content.'
  prefs: []
  type: TYPE_NORMAL
- en: 'E5 proceeded to examine another conversation and discovered similar patterns.
    Users in the conversation would use forcing instructions like “as horny” in order
    to coerce ChatGPT into assuming that character, especially when ChatGPT began
    refusing their queries (Figure [5](#S6.F5 "Figure 5 ‣ VI-B Case Two: Identifying
    Multi-Turn Jailbreak Strategies ‣ VI Evaluation ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets")(e1)). This pattern indicates that LLMs are not robust enough to consistently
    reject user queries when repeatedly exposed to jailbreak prompts or when explicitly
    instructed to follow the forcing instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: Conclusions. E5 considered the inclusion of the jailbreak prompt “horny” in
    future tests of LLMs. Furthermore, E5 believed that when testing this kind of
    jailbreak prompts, it should be accompanied by follow-up prompts, such as reusing
    the prompts multiple times or utilizing forcing instructions like “as horny,”
    to determine if the LLMs can be consistently jailbroken across multiple turns.
    Addressing this issue may require additional fine-tuning by constructing prompt
    sets that LLMs can explicitly reject in each turn, thereby mitigating the effects
    of these kinds of multi-turn jailbreak strategies.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Expert Interviews
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We gathered feedback from the nine domain experts mentioned previously (E1,
    E3, E5-E11). Initially, we introduced the system to the experts with usage examples.
    Subsequently, we requested them to use the system to identify jailbreak prompts
    from the provided conversational datasets, namely LMSYS-Chat-1M and WildChat.
    Following their exploration, we gathered their feedback regarding how the system
    supports identifying jailbreak prompts from large-scale human-LLM conversational
    datasets. Furthermore, we requested their completion of a questionnaire that included
    7-point Likert scale quantitative questions assessing the effectiveness and usability
    of the system, along with a measure of System Usability Scale (SUS). The quantitative
    results are provided in Appendix D. The feedback is summarized in the following
    paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: System workflow. Most of the experts (8/9) agreed that it is easy to identify
    jailbreak prompts from large-scale human-LLM conversational datasets using the
    system. E1 and E10 verified that they can identify common jailbreak prompts used
    by users and recognize useful patterns of jailbreaks. E3, E7, E8, E9, and E11
    mentioned that the Cluster View assists them in quickly narrowing down highly
    suspicious clusters, significantly reducing screening time for large-scale conversational
    datasets. E8 and E9 confirmed that they can compare ASR of different prompts and
    identify regions with high ASR. E3 and E11 expressed a preference for utilizing
    the bottom right section of the Cluster View. This particular section offers a
    clear visualization of the distribution of malicious turns in both queries and
    responses. E11 specifically highlighted a preference for examining conversations
    that contain a higher number of turns with malicious model responses compared
    to queries. The majority of experts (8/9) agreed that the Conversation View helps
    them swiftly locate suspicious prompts. E7 and E11 appreciated the well-designed
    thumbnails of the conversation, especially for the middle two columns, which indicate
    whether the query or response is flagged by moderation models using light blue
    or light pink color that intuitively indicates the occurrence of malicious content.
    E7, E8, and E9 mentioned that the Comparison View assists users in judging whether
    the inspected query is a new prompt compared to reported jailbreak prompts used
    in the system. E7 emphasized that the system supports comparison in multiple dimensions,
    such as word matching, sentence similarity, and paragraph similarity, providing
    obvious assistance for the users at three distinct levels.
  prefs: []
  type: TYPE_NORMAL
- en: Usability. The majority of experts (7/9) reached a consensus that the system
    is easy to learn and use. Specifically, E3, E6, E7, E8, E9, and E11 praised the
    system for its beautiful design, smooth interaction, and overall user-friendliness.
    However, there were a few experts (E1 and E5) who expressed concerns about the
    system’s potentially steep learning curve for novice users, given its many functions
    that might overwhelm them. Additionally, E7 and E8 expressed concerns about the
    Comparison View, as it presents a significant amount of text that needs to be
    read in order to comprehend and compare with the currently inspected prompt, while
    the view itself is relatively small within the current layout.
  prefs: []
  type: TYPE_NORMAL
- en: Suggestions. The majority of experts (8/9) expressed their enthusiasm for utilizing
    the system in the future to identify jailbreak prompts from large-scale human-LLM
    conversational datasets. They also provided valuable suggestions to enhance the
    system’s usability. E8 proposed exploring the use of LLM to summarize the meaning
    of conversations and identify similarities and differences between the currently
    inspected query and reported jailbreak prompts. This approach aims to reduce the
    workload of reading lengthy texts. E7 recommended incorporating features that
    allow users to retrieve their inspection history, thereby avoiding duplicate inspections.
    Additionally, E7, E8, and E9 suggested gradually integrating users’ identified
    jailbreak prompts into the system and training models to classify prompts based
    on user feedback. E5 and E11 proposed improving the representativeness of keywords
    used in the Cluster View. Furthermore, E3 suggested enabling users to upload their
    datasets through the interface instead of relying solely on backend configuration.
  prefs: []
  type: TYPE_NORMAL
- en: VII Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we discuss the lessons learned from the development and assessment
    of JailbreakHunter, while also revealing its limitations and exploring potential
    future improvements.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Lessons Learned
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Providing multi-level and multi-grained inspection for jailbreak prompts discovery.
    To identify jailbreak prompts in large-scale human-LLM conversational datasets,
    we introduced a workflow encompassing group-level, conversation-level, and turn-level
    analysis. A visual analytics system has been designed to support this workflow.
    Feedback from experts praised its effectiveness in narrowing down suspicious clusters,
    locating specific prompts, and comparing them to reported jailbreak prompts. Valuable
    insights for testing LLMs and developing solutions are gained. We advocate for
    the adoption of this multi-level workflow in future visual analytics systems,
    specifically for the purpose of identifying specific targets within large-scale
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Safety evaluation lies in the first priority of LLMs evaluation. LLMs have advanced
    significantly since ChatGPT’s release, but evaluating their safety poses challenges
    due to potential misuse. We present JailbreakHunter, a tool to identify jailbreak
    prompts in large-scale human-LLM datasets. Our aim is to enhance LLM security
    evaluation and safeguard measures. Case studies and expert interviews confirm
    our system’s effectiveness in identifying prompts and gaining insights for testing
    and issue mitigation. We hope that our work will contribute to strengthening safety
    evaluation efforts and elevating its priority in the overall process of LLMs evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Limitations and Future Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supporting the analysis of jailbreak prompts for large multi-modal models. In
    this study, we showcase our system’s capability to identify jailbreak prompts
    within two human-LLM conversational datasets. These datasets primarily consist
    of textual data and do not include other modal data. However, with the emergence
    of large multi-modal models that can process various input types such as images
    and texts, it becomes essential for our system to generalize and support the analysis
    of jailbreak prompts for such models. To accommodate the requirements of large
    multi-modal models, certain adaptations are necessary. For instance, we may need
    to obtain a representation of a cluster that incorporates multi-modal inputs.
    Additionally, providing intuitive summarization or prefixes can enable users to
    quickly comprehend the similarities among different prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Interactively updating the collection of reported jailbreak prompts. In this
    work, our focus is on helping users identify new jailbreak prompts, including
    subtle, sophisticated, and evolving ones. Although the moderation models can help
    capture some jailbreak prompts or malicious responses, which can indicate the
    usage of jailbreak prompts, we still cannot capture all of the subtle and sophisticated
    jailbreak prompts as they are continuously evolving and fall outside the scope
    of our detection technique. In the future, we can extend our approach to allow
    users to interactively add newly discovered jailbreak prompts to the collection
    of reported jailbreak prompts. As the scope of the reported jailbreak prompts
    expands, it will be easier to identify similar jailbreak prompts, including subtle,
    sophisticated, and evolving ones, when loading a new dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Improving scalability of the system to handle billions of data. The scalability
    of JailbreakHunter is a critical aspect, enabling users to explore large-scale
    datasets and gain comprehensive insights. However, it is important to address
    the scalability issues of the system when handling billions of data points. While
    the system leverages WizMap technology in the Projection View to enhance scalability
    and visualize millions of data points, performance challenges may arise when the
    number of data points exceeds 10 million. Future work should focus on optimizing
    the system’s performance to ensure efficient handling and analysis of substantial
    amounts of data, allowing users to explore even larger datasets effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing LLMs to enhance analysis of lengthy texts in multi-turn conversations.
    Currently, users can check keywords and prefixes to obtain basic information about
    conversations. To further facilitate comprehension of the lengthy texts in conversations
    and reported jailbreak prompts, we can employ LLMs to generate summaries of the
    conversations, as well as the topics addressed in each long query and reported
    jailbreak prompt. Furthermore, LLMs can be utilized to summarize the similarities
    and differences among different texts. To provide users with a better understanding
    of the progression of conversations, it would be beneficial to present an overview
    of how the topics evolve in the Conversation View.
  prefs: []
  type: TYPE_NORMAL
- en: Conducting extensive and diverse evaluation. Currently, we have conducted case
    studies and expert interviews to demonstrate the effectiveness and usability of
    our system. However, it would be beneficial to conduct a more extensive and diverse
    set of experiments, involving a larger sample size, in order to strengthen the
    robustness and generalizability of the findings. Additionally, we would like to
    extend the evaluation to practical settings and track its long-term performance,
    which would serve as long-term evaluation results.
  prefs: []
  type: TYPE_NORMAL
- en: Providing more tutorials or tips on the system to lower the learning curve.
    In expert interviews, several experts highlighted that JailbreakHunter presents
    a steep learning curve, mainly due to the abundance of system functions available.
    In order to address this issue, future efforts will focus on providing more comprehensive
    tutorials, tips, and user guides. These resources will greatly assist in facilitating
    user onboarding and reducing the learning curve associated with the system.
  prefs: []
  type: TYPE_NORMAL
- en: Balancing freedom and control and respecting user privacy. For practical usage
    of this system, although we can leverage moderation model results to narrow down
    conversations to a specific group for those interested, we still need LLM security
    researchers to investigate the boundary between valid and malicious prompts. The
    final judgment on balancing users’ freedom to explore various topics and controlling
    misuse is delegated to human judgment instead of relying solely on moderation
    model results. Additionally, it is important to respect user privacy and adhere
    to ethical standards when monitoring human-LLM conversations. Achieving agreements
    between users and LLM deployers on when and how the deployers can investigate
    such content for further inspection, in accordance with local privacy laws, regulations
    of LLMs, and the agreement license between users and LLM deployers, is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we presented JailbreakHunter, a visual analytics approach designed
    to assist users in identifying jailbreak prompts from large-scale human-LLM conversational
    datasets. The approach offers three levels of analysis: group-level, conversation-level,
    and turn-level. In the Filter Panel, users can apply filters to extract conversations
    that align with their interests. The Cluster View supports group-level analysis,
    enabling users to comprehend the distribution of filtered conversations and reported
    jailbreak prompts, as well as the characteristics of instance clusters. The Conversation
    View supports conversation-level analysis and empowers users to detect malicious
    content and jailbreak prompts within multi-turn conversations. Additionally, the
    Comparison View provides turn-level analysis capabilities, enabling users to examine
    the relationship between a single query and reported jailbreak prompts. We conducted
    case studies and expert interviews to demonstrate the effectiveness and usability
    of JailbreakHunter in identifying jailbreak prompts from large-scale human-LLM
    conversational datasets,'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,
    S. Agarwal, K. Slama, A. Ray *et al.*, “Training language models to follow instructions
    with human feedback,” in *NeurIPS*, vol. 35, 2022, pp. 27 730–27 744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
    J. Altenschmidt, S. Altman, S. Anadkat *et al.*, “Gpt-4 technical report,” *arXiv
    preprint arXiv:2303.08774*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Qin, K. Song, Y. Hu, W. Yao, S. Cho, X. Wang, X. Wu, F. Liu, P. Liu,
    and D. Yu, “Infobench: Evaluating instruction following ability in large language
    models,” *arXiv preprint arXiv:2401.03601*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] K. Singhal, T. Tu, J. Gottweis, R. Sayres, E. Wulczyn, L. Hou, K. Clark,
    S. Pfohl, H. Cole-Lewis, D. Neal *et al.*, “Towards expert-level medical question
    answering with large language models,” *arXiv preprint arXiv:2305.09617*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] C. Gómez-Rodríguez and P. Williams, “A confederacy of models: A comprehensive
    evaluation of LLMs on creative writing,” in *Findings of ACL: EMNLP*.   Singapore:
    ACL, 2023, pp. 14 504–14 528.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Nazir and Z. Wang, “A comprehensive survey of chatgpt: Advancements,
    applications, prospects, and challenges,” *Meta-radiology*, no. 2, p. 100022,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] T. Markov, C. Zhang, S. Agarwal, F. E. Nekoul, T. Lee, S. Adler, A. Jiang,
    and L. Weng, “A holistic approach to undesired content detection in the real world,”
    in *AAAI*, vol. 37, no. 12, 2023, pp. 15 009–15 018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Mökander, J. Schuett, H. R. Kirk, and L. Floridi, “Auditing large language
    models: A three-layered approach,” *AI and Ethics*, pp. 1–31, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Y. Bai, A. Jones, K. Ndousse, A. Askell, A. Chen, N. DasSarma, D. Drain,
    S. Fort, D. Ganguli, T. Henighan *et al.*, “Training a helpful and harmless assistant
    with reinforcement learning from human feedback,” *arXiv preprint arXiv:2204.05862*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Glaese, N. McAleese, M. Trębacz, J. Aslanides, V. Firoiu, T. Ewalds,
    M. Rauh, L. Weidinger, M. Chadwick, P. Thacker *et al.*, “Improving alignment
    of dialogue agents via targeted human judgements,” *arXiv preprint arXiv:2209.14375*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] P. Röttger, B. Vidgen, D. Nguyen, Z. Waseem, H. Margetts, and J. B. Pierrehumbert,
    “Hatecheck: Functional tests for hate speech detection models,” in *Proc. ACL*.   ACL,
    2021, pp. 41–58.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] T. Hartvigsen, S. Gabriel, H. Palangi, M. Sap, D. Ray, and E. Kamar, “ToxiGen:
    A large-scale machine-generated dataset for adversarial and implicit hate speech
    detection,” in *Proc. ACL*.   Dublin, Ireland: ACL, 2022, pp. 3309–3326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] P. Röttger, H. Seelawi, D. Nozza, Z. Talat, and B. Vidgen, “Multilingual
    HateCheck: Functional tests for multilingual hate speech detection models,” in
    *WOAH*.   Seattle, Washington (Hybrid): ACL, 2022, pp. 154–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] OpenAI, “Moderation - openai api,” 2024, https://platform.openai.com/docs/guides/moderation
    [Accessed: 03/10/2024].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] H. Li, D. Guo, W. Fan, M. Xu, J. Huang, F. Meng, and Y. Song, “Multi-step
    jailbreaking privacy attacks on ChatGPT,” in *Findings of ACL: EMNLP*.   Singapore:
    ACL, 2023, pp. 4138–4153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, and S. Zanella-Béguelin,
    “Analyzing leakage of personally identifiable information in language models,”
    in *SP*, 2023, pp. 346–363.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Liu, G. Deng, Z. Xu, Y. Li, Y. Zheng, Y. Zhang, L. Zhao, T. Zhang,
    and Y. Liu, “Jailbreaking chatgpt via prompt engineering: An empirical study,”
    *arXiv preprint arXiv:2305.13860*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] X. Shen, Z. Chen, M. Backes, Y. Shen, and Y. Zhang, “"Do anything now":
    Characterizing and evaluating in-the-wild jailbreak prompts on large language
    models,” *arXiv preprint arXiv:2308.03825*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] D. Ganguli, L. Lovitt, J. Kernion, A. Askell, Y. Bai, S. Kadavath, B. Mann,
    E. Perez, N. Schiefer, K. Ndousse *et al.*, “Red teaming language models to reduce
    harms: Methods, scaling behaviors, and lessons learned,” *arXiv preprint arXiv:2209.07858*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] L. Zheng, W.-L. Chiang, Y. Sheng, T. Li, S. Zhuang, Z. Wu, Y. Zhuang,
    Z. Li, Z. Lin, E. Xing *et al.*, “Lmsys-chat-1m: A large-scale real-world llm
    conversation dataset,” *arXiv preprint arXiv:2309.11998*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Huang, S. Gupta, M. Xia, K. Li, and D. Chen, “Catastrophic jailbreak
    of open-source LLMs via exploiting generation,” in *ICLR*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken: How does llm safety
    training fail?” in *NeurIPS*, vol. 36, 2023, pp. 80 079–80 110.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] D. Kang, X. Li, I. Stoica, C. Guestrin, M. Zaharia, and T. Hashimoto,
    “Exploiting programmatic behavior of LLMs: Dual-use through standard security
    attacks,” in *Proc. ICML*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] F. Perez and I. Ribeiro, “Ignore previous prompt: Attack techniques for
    language models,” in *NeurIPS Workshop MLSW*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Yuan, W. Jiao, W. Wang, J. tse Huang, P. He, S. Shi, and Z. Tu, “GPT-4
    is too smart to be safe: Stealthy chat with LLMs via cipher,” in *ICLR*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] B. Deng, W. Wang, F. Feng, Y. Deng, Q. Wang, and X. He, “Attack prompt
    generation for red teaming and defending large language models,” in *Findings
    of ACL: EMNLP*.   Singapore: ACL, 2023, pp. 2176–2189.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson, “Universal and transferable
    adversarial attacks on aligned language models,” *arXiv preprint arXiv:2307.15043*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] P. Chao, A. Robey, E. Dobriban, H. Hassani, G. J. Pappas, and E. Wong,
    “Jailbreaking black box large language models in twenty queries,” in *NeurIPS
    Workshop R0-FoMo*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] A. Rao, S. Vashistha, A. Naik, S. Aditya, and M. Choudhury, “Tricking
    llms into disobedience: Understanding, analyzing, and preventing jailbreaks,”
    *arXiv preprint arXiv:2305.14965*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] S. Schulhoff, J. Pinto, A. Khan, L.-F. Bouchard, C. Si, S. Anati, V. Tagliabue,
    A. Kost, C. Carnahan, and J. Boyd-Graber, “Ignore this title and HackAPrompt:
    Exposing systemic vulnerabilities of LLMs through a global prompt hacking competition,”
    in *Proc. EMNLP*.   Singapore: ACL, 2023, pp. 4945–4977.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Z. J. Wang, F. Hohman, and D. H. Chau, “WizMap: Scalable interactive visualization
    for exploring large machine learning embeddings,” in *Proc. ACL*.   Toronto, Canada:
    ACL, 2023, pp. 516–523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush, “Lstmvis: A tool
    for visual analysis of hidden state dynamics in recurrent neural networks,” *IEEE
    Trans. Visual. Comput. Graphics*, vol. 24, no. 1, pp. 667–676, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Y. Ming, S. Cao, R. Zhang, Z. Li, Y. Chen, Y. Song, and H. Qu, “Understanding
    hidden memories of recurrent neural networks,” in *VAST*.   IEEE, 2017, pp. 13–24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] H. Strobelt, S. Gehrmann, M. Behrisch, A. Perer, H. Pfister, and A. M.
    Rush, “Seq2seq-vis: A visual debugging tool for sequence-to-sequence models,”
    *IEEE Trans. Visual. Comput. Graphics*, vol. 25, no. 1, pp. 353–363, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Vig, “A multiscale visualization of attention in the transformer model,”
    in *Proc. ACL*.   Florence, Italy: ACL, 2019, pp. 37–42.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] B. Hoover, H. Strobelt, and S. Gehrmann, “exBERT: A Visual Analysis Tool
    to Explore Learned Representations in Transformer Models,” in *Proc. ACL*.   Online:
    ACL, 2020, pp. 187–196.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] C. Park, I. Na, Y. Jo, S. Shin, J. Yoo, B. C. Kwon, J. Zhao, H. Noh, Y. Lee,
    and J. Choo, “Sanvis: Visual analytics for understanding self-attention networks,”
    in *VIS*.   IEEE, 2019, pp. 146–150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. F. DeRose, J. Wang, and M. Berger, “Attention flows: Analyzing and
    comparing attention mechanisms in language models,” *IEEE Trans. Visual. Comput.
    Graphics*, vol. 27, no. 2, pp. 1160–1170, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] R. Sevastjanova, A. Kalouli, C. Beck, H. Hauptmann, and M. El-Assady,
    “Lmfingerprints: Visual explanations of language model embedding spaces through
    layerwise contextualization scores,” in *Comput. Graphics Forum*, vol. 41, no. 3.   Wiley
    Online Library, 2022, pp. 295–307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] C. Yeh, Y. Chen, A. Wu, C. Chen, F. Viégas, and M. Wattenberg, “Attentionviz:
    A global view of transformer attention,” *IEEE Trans. Visual. Comput. Graphics*,
    vol. 30, no. 1, pp. 262–272, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] T. Wu, K. Wongsuphasawat, D. Ren, K. Patel, and C. DuBois, “Tempura: Query
    analysis with structural templates,” in *Proc. CHI*.   New York: ACM, 2020, pp.
    1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Li, X. Wang, W. Yang, J. Wu, Z. Zhang, Z. Liu, M. Sun, H. Zhang, and
    S. Liu, “A unified understanding of deep nlp models for text classification,”
    *IEEE Trans. Visual. Comput. Graphics*, vol. 28, no. 12, pp. 4980–4994, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z. Jin, X. Wang, F. Cheng, C. Sun, Q. Liu, and H. Qu, “Shortcutlens: A
    visual analytics approach for exploring shortcuts in natural language understanding
    dataset,” *IEEE Trans. Visual. Comput. Graphics*, pp. 1–15, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] H. Strobelt, A. Webson, V. Sanh, B. Hoover, J. Beyer, H. Pfister, and
    A. M. Rush, “Interactive and visual prompt engineering for ad-hoc task adaptation
    with large language models,” *IEEE Trans. Visual. Comput. Graphics*, vol. 29,
    no. 1, pp. 1146–1156, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] A. Mishra, U. Soni, A. Arunkumar, J. Huang, B. C. Kwon, and C. Bryan,
    “Promptaid: Prompt exploration, perturbation, testing and iteration using visual
    analytics for large language models,” *arXiv preprint arXiv:2304.01964*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M. Kahng, I. Tenney, M. Pushkarna, M. X. Liu, J. Wexler, E. Reif, K. Kallarackal,
    M. Chang, M. Terry, and L. Dixon, “Llm comparator: Visual analytics for side-by-side
    evaluation of large language models,” *arXiv preprint arXiv:2402.10524*, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] S. Fu, J. Zhao, H. F. Cheng, H. Zhu, and J. Marlow, “T-cal: Understanding
    team conversational data with calendar-based visualization,” in *Proc. CHI*.   New
    York: ACM, 2018, pp. 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. El-Assady, V. Gold, C. Acevedo, C. Collins, and D. Keim, “Contovi:
    Multi-party conversation exploration using topic-space views,” in *Comput. Graphics
    Forum*, vol. 35, no. 3.   Wiley Online Library, 2016, pp. 431–440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] E. Hoque and G. Carenini, “Convisit: Interactive topic modeling for exploring
    asynchronous online conversations,” in *Proc. IUI*.   New York: ACM, 2015, pp.
    169–180.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] S. Fu, J. Zhao, W. Cui, and H. Qu, “Visual analysis of mooc forums with
    iforum,” *IEEE Trans. Visual. Comput. Graphics*, vol. 23, no. 1, pp. 201–210,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] E. Hoque and G. Carenini, “Multiconvis: A visual text analytics system
    for exploring a collection of online conversations,” in *Proc. IUI*.   New York:
    ACM, 2016, pp. 96–107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] E. Hoque and G. Carenini, “Convis: A visual text analytic system for exploring
    blog conversations,” in *Comput. Graphics Forum*, vol. 33, no. 3.   Wiley Online
    Library, 2014, pp. 221–230.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. Fu, Y. Wang, Y. Yang, Q. Bi, F. Guo, and H. Qu, “Visforum: A visual
    analysis system for exploring user groups in online forums,” *ACM Transactions
    on Interactive Intelligent Systems*, vol. 8, no. 1, pp. 3:1–3:21, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] M. El-Assady, R. Sevastjanova, D. Keim, and C. Collins, “Threadreconstructor:
    Modeling reply-chains to untangle conversational text through visual analytics,”
    in *Comput. Graphics Forum*, vol. 37, no. 3.   Wiley Online Library, 2018, pp.
    351–365.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J.-S. Wong *et al.*, “Messagelens: A visual analytics system to support
    multifaceted exploration of mooc forum discussions,” *Vis. Informatics*, vol. 2,
    no. 1, pp. 37–49, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] R. Li, E. Hoque, G. Carenini, R. Lester, and R. Chau, “Conviscope: Visual
    analytics for exploring patient conversations,” in *VIS*.   IEEE, 2021, pp. 151–155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Qiu, S. Zhang, A. Li, H. He, and Z. Lan, “Latent jailbreak: A benchmark
    for evaluating text safety and output robustness of large language models,” *arXiv
    preprint arXiv:2307.08487*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] L. Zheng, W.-L. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin,
    Z. Li, D. Li, E. Xing *et al.*, “Judging llm-as-a-judge with mt-bench and chatbot
    arena,” in *NeurIPS*, vol. 36, 2023, pp. 46 595–46 623.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] W. Zhao, X. Ren, J. Hessel, C. Cardie, Y. Choi, and Y. Deng, “(InThe)WildChat:
    570k chatgpt interaction logs in the wild,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] N. Reimers and I. Gurevych, “Sentence-BERT: Sentence embeddings using
    Siamese BERT-networks,” in *Proc. EMNLP*.   Hong Kong, China: ACL, 2019, pp. 3982–3992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] L. McInnes, J. Healy, and J. Melville, “Umap: Uniform manifold approximation
    and projection for dimension reduction,” *arXiv preprint arXiv:1802.03426*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] M. Rosenblatt, “Remarks on some nonparametric estimates of a density function,”
    *The Annals of Mathematical Statistics*, vol. 27, no. 3, pp. 832–837, 1956.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IX Biography Section
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/00ee881df5c010fe42d9e6d8907ac189.png) | Zhihua
    Jin is currently a PhD candidate at the Hong Kong University of Science and Technology
    (HKUST). He received his BEng degree in Computer Science and Technology from Zhejiang
    University in 2019\. His research interests consist of visualization, explainable
    artificial intelligence (XAI), and natural language processing. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/39ca483f1db7044337a21d630db02fe4.png) | Shiyi
    Liu is currently a PhD student at Arizona State University, under the supervision
    of Prof. Ross Maciejewski. Before that, she obtained her BEng from Shanghai Jiao
    Tong University and Master’s degree from ShanghaiTech University. Her research
    interests lie in data visualization and human computer interaction. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/511c63791608f9b781f170f5b526bb48.png) | Haotian
    Li is currently a PhD candidate in Computer Science and Engineering at the Hong
    Kong University of Science and Technology (HKUST). His main research interests
    are data visualization, visual analytics, human-computer interaction and online
    education. He received his BEng in Computer Engineering from HKUST. For more details,
    please refer to https://haotian-li.com/. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/325b66f77b3cac0a5bae40335e19b803.png) | Xun Zhao
    is a research scientist in the Shanghai Artificial Intelligence Laboratory. She
    obtained a BS from Huazhong University of Science and Technology and a PhD in
    Computer Science and Engineering from Hong Kong University of Science and Technology
    (HKUST). Her main research interests are in Responsible AI, with a focus on LLM
    safety and explanation. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/5f0810434a9ae8172805ba71ecee6f94.png) | Huamin
    Qu is a chair professor in the Department of Computer Science and Engineering
    (CSE) at the Hong Kong University of Science and Technology (HKUST) and also the
    dean of the Academy of Interdisciplinary Studies (AIS) at HKUST. He obtained a
    BS in Mathematics from Xi’an Jiaotong University, China, an MS and a PhD in Computer
    Science from the Stony Brook University. His main research interests are in visualization
    and human-computer interaction, with a focus on urban informatics, social network
    analysis, E-learning, text visualization, and explainable artificial intelligence
    (XAI). |'
  prefs: []
  type: TYPE_TB
- en: Appendix A Details of System Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The entire system, consisting of three modules, is deployed on a remote server,
    allowing users to access the interface through a web browser. The system, including
    the MongoDB data store, can be hosted by the developers of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: During the dataset loading process, we precompute and cache the embeddings.
    The dataset should include the malicious tags, and the loading process will parse
    and make them available. When filters are created, real-time calculations are
    performed to determine keywords, UMAP projections, and the distribution of malicious
    turns in conversations. These calculated values are then cached within the system
    for easy reloading.
  prefs: []
  type: TYPE_NORMAL
- en: When brushing a group of filtered conversations, the system dynamically computes
    keywords in real-time and retrieves the distribution of malicious turns and prefixes.
    Similarly, when a user selects a specific conversation, the system computes the
    similarity value and performs highlighting of similar text between the prompt
    and reported jailbreak prompts in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Filter Configuration Panel
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Filter Configuration Panel is depicted in Figure [6](#A2.F6 "Figure 6 ‣
    Appendix B Filter Configuration Panel ‣ JailbreakHunter: A Visual Analytics Approach
    for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational Datasets").
    Users can select one existing filter to check the configuration for that filter
    (Figure [6](#A2.F6 "Figure 6 ‣ Appendix B Filter Configuration Panel ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(a)). It also allows users to customize the filter’s
    name (Figure [6](#A2.F6 "Figure 6 ‣ Appendix B Filter Configuration Panel ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(b)) and select datasets to which the filter will apply
    (Figure [6](#A2.F6 "Figure 6 ‣ Appendix B Filter Configuration Panel ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(c)). Users can write Python code in the “Code” input
    field (Figure [6](#A2.F6 "Figure 6 ‣ Appendix B Filter Configuration Panel ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(e)) to filter the data based on specific conditions
    and extract malicious conversations from the large-scale human-LLM conversational
    datasets. The interface also provides a predefined template (Figure [6](#A2.F6
    "Figure 6 ‣ Appendix B Filter Configuration Panel ‣ JailbreakHunter: A Visual
    Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(d)) for users to generate the Python code used in the
    filtering process. After users have set the filtering conditions, including focused
    models, focused language, focused malicious categories, and the desired range
    of the number of turns, they can click the “Generate” button to automatically
    generate the Python code. Users only need to review it and make minor revisions
    in the “Code” input field to meet their filtering requirements. After completing
    the filter code, users can click the “Test” button to test the filter. The filter
    code will be run on the backend server, and if any errors are detected, the error
    information will be displayed in the “Error” text field of the Filter Configuration
    Panel (Figure [6](#A2.F6 "Figure 6 ‣ Appendix B Filter Configuration Panel ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(f)). Based on the information provided in the “Error”
    text field, users can debug the filter code. If the process is successfully completed,
    the number of filtered conversations will be displayed (Figure [6](#A2.F6 "Figure
    6 ‣ Appendix B Filter Configuration Panel ‣ JailbreakHunter: A Visual Analytics
    Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM Conversational
    Datasets")(g)). Users can then decide whether to save the filters for further
    inspection. They can click the “Save” button to store the filter on the backend
    server.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e34ef9d347994777d03b7d5c77bb7871.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Users can create a filter to extract conversations from conversational
    datasets by writing customized Python code in the Filter Configuration Panel.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Additional Interactions for the Conversation View
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d890f3879da98fe53899333e3af9a582.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Additional interactions for the Conversation View (a, b, c, d, e,
    f).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Conversation View, we provide additional interactions such as the “Collect
    Prompt,” “Show Prompts Collection,” “Expand All,” “Close All,” and “Translate”
    functions for users to interact with. When users select a query and configure
    its prompt type (Figure [7](#A3.F7 "Figure 7 ‣ Appendix C Additional Interactions
    for the Conversation View ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak
    Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")(a)), they
    can click the “Collect Prompt” button (Figure [7](#A3.F7 "Figure 7 ‣ Appendix
    C Additional Interactions for the Conversation View ‣ JailbreakHunter: A Visual
    Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(b)) to add that prompt to the “Prompts Collection.”
    Clicking the “Show Prompts Collection” button (Figure [7](#A3.F7 "Figure 7 ‣ Appendix
    C Additional Interactions for the Conversation View ‣ JailbreakHunter: A Visual
    Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(c)) allows users to access the details of the collected
    prompts and download the results. Furthermore, clicking the “Expand All” (Figure [7](#A3.F7
    "Figure 7 ‣ Appendix C Additional Interactions for the Conversation View ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(d)) or “Close All” button (Figure [7](#A3.F7 "Figure
    7 ‣ Appendix C Additional Interactions for the Conversation View ‣ JailbreakHunter:
    A Visual Analytics Approach for Jailbreak Prompts Discovery from Large-Scale Human-LLM
    Conversational Datasets")(e)) will expand or collapse all conversations within
    the details of the conversation, respectively. In addition, clicking the “Translate”
    button (Figure [7](#A3.F7 "Figure 7 ‣ Appendix C Additional Interactions for the
    Conversation View ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak
    Prompts Discovery from Large-Scale Human-LLM Conversational Datasets")(f)) will
    pop up a “Translation Helper” modal, which can redirect users to a translation
    website for translating the content of the conversations. This feature is useful
    when the language used in the conversations is difficult for users to understand
    without prior knowledge.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Questionnaire
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a6205ff2d71ea24b3a18198b04592ba1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The distribution of scores for the effectiveness questions (Q1-Q10).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/769f9fbe241255e3efba2cf30fa0b4dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The distribution of scores for the usability questions (Q11-Q17).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The questionnaire includes quantitative questions assessing the effectiveness
    (Q1-Q10) and usability (Q11-Q17) of the system using a 7-point Likert scale (1
    for strongly disagree and 7 for strongly agree). We present the quantitative results
    collected from the questionnaire in Table [I](#A4.T1 "Table I ‣ Appendix D Questionnaire
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets"), along with the score distribution
    for the effectiveness questions shown in Figure [8](#A4.F8 "Figure 8 ‣ Appendix
    D Questionnaire ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts
    Discovery from Large-Scale Human-LLM Conversational Datasets") and the usability
    questions depicted in Figure [9](#A4.F9 "Figure 9 ‣ Appendix D Questionnaire ‣
    JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery from
    Large-Scale Human-LLM Conversational Datasets"). The questionnaire also contains
    a standard questionnaire for evaluating System Usability Scale (SUS) of the system.
    The questions are presented in Table [II](#A4.T2 "Table II ‣ Appendix D Questionnaire
    ‣ JailbreakHunter: A Visual Analytics Approach for Jailbreak Prompts Discovery
    from Large-Scale Human-LLM Conversational Datasets"). We calculated the scores,
    which are 68.70 $\pm$ 13.56, indicating a relatively good usability of our system.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Question | Score |'
  prefs: []
  type: TYPE_TB
- en: '| Q1 | It is easy (difficult) to identify jailbreak prompts from the conversations
    using the system. | 5.11 $\pm$ 1.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Q2 | It is easy (difficult) to create or select a filter to extract conversations
    with malicious content using the Filter Panel. | 6.00 $\pm$ 0.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Q3 | It is easy (difficult) to examine the overview of conversations using
    the Cluster View. | 5.44 $\pm$ 1.26 |'
  prefs: []
  type: TYPE_TB
- en: '| Q4 | It is easy (difficult) to understand the common properties of a group
    of conversations using the Cluster View. | 4.89 $\pm$ 1.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Q5 | It is easy (difficult) to compare the attack success rates of different
    groups of conversations using the Cluster View. | 6.00 $\pm$ 1.05 |'
  prefs: []
  type: TYPE_TB
- en: '| Q6 | It is easy (difficult) to identify suspicious conversations using the
    Cluster View. | 6.33 $\pm$ 0.82 |'
  prefs: []
  type: TYPE_TB
- en: '| Q7 | It is easy (difficult) to understand the progress of conversations using
    the Conversation View. | 5.89 $\pm$ 1.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Q8 | It is easy (difficult) to locate the malicious content in conversations
    using the Conversation View. | 5.56 $\pm$ 1.57 |'
  prefs: []
  type: TYPE_TB
- en: '| Q9 | It is easy (difficult) to discover jailbreak prompts within their conversation
    contexts using the Conversation View. | 5.33 $\pm$ 1.49 |'
  prefs: []
  type: TYPE_TB
- en: '| Q10 | It is easy (difficult) to explore the semantic similarity and token
    overlap between a single-turn prompt and the past jailbreak prompts using the
    Comparison View. | 5.11 $\pm$ 1.66 |'
  prefs: []
  type: TYPE_TB
- en: '| Q11 | It is easy (difficult) to learn and use the system. | 5.11 $\pm$ 1.85
    |'
  prefs: []
  type: TYPE_TB
- en: '| Q12 | It is easy (difficult) to learn and use the Filter Panel. | 5.78 $\pm$
    1.23 |'
  prefs: []
  type: TYPE_TB
- en: '| Q13 | It is easy (difficult) to learn and use the Cluster View. | 5.56 $\pm$
    1.50 |'
  prefs: []
  type: TYPE_TB
- en: '| Q14 | It is easy (difficult) to learn and use the Conversation View. | 6.22
    $\pm$ 0.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Q15 | It is easy (difficult) to learn and use the Comparison View. | 5.78
    $\pm$ 1.55 |'
  prefs: []
  type: TYPE_TB
- en: '| Q16 | I would (not) recommend using this system to identify jailbreak prompts
    from conversational datasets to others working on the same task. | 5.89 $\pm$
    1.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Q17 | I would (not) use this system in the future to identify jailbreak prompts
    from conversational datasets. | 5.78 $\pm$ 1.13 |'
  prefs: []
  type: TYPE_TB
- en: 'Table I: Questionnaire used in expert interviews to evaluate the effectiveness
    (Q1-Q10) and usability (Q11-Q17) of the system. Scores for each question are reported
    with their mean and standard deviation. The sentences with words in brackets represent
    negative sentiments found at the left end of the scale, while the sentences without
    words in brackets represent positive sentiments located at the right end of the
    scale.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Question |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Q18 | I think that I would like to use this system frequently. |'
  prefs: []
  type: TYPE_TB
- en: '| Q19 | I found the system unnecessarily complex. |'
  prefs: []
  type: TYPE_TB
- en: '| Q20 | I thought the system was easy to use. |'
  prefs: []
  type: TYPE_TB
- en: '| Q21 | I think that I would need the support of a technical person to be able
    to use this system. |'
  prefs: []
  type: TYPE_TB
- en: '| Q22 | I found the various functions in the system were well integrated. |'
  prefs: []
  type: TYPE_TB
- en: '| Q23 | I thought there was too much inconsistency in this system. |'
  prefs: []
  type: TYPE_TB
- en: '| Q24 | I imagine that most people would learn to use this system very quickly.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Q25 | I found the system very awkward to use. |'
  prefs: []
  type: TYPE_TB
- en: '| Q26 | I felt very confident using the system. |'
  prefs: []
  type: TYPE_TB
- en: '| Q27 | I needed to learn a lot of things before I could get going with this
    system. |'
  prefs: []
  type: TYPE_TB
- en: 'Table II: The standard questionnaire used to evaluate the SUS. The calculated
    scores of 68.70 $\pm$ 13.56 indicate good usability for our system.'
  prefs: []
  type: TYPE_NORMAL
