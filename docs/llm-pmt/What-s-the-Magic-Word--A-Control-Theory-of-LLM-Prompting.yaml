- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:49:07'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: What’s the Magic Word? A Control Theory of LLM Prompting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.04444](https://ar5iv.labs.arxiv.org/html/2310.04444)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Aman Bhargava¹, Cameron Witkowski², Manav Shah², Matt Thomson¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹California Institute of Technology, ²University of Toronto
  prefs: []
  type: TYPE_NORMAL
- en: '{abhargav,mthomson}@caltech.edu'
  prefs: []
  type: TYPE_NORMAL
- en: '{cameron.witkowski,manav.shah}@mail.utoronto.ca Code will be made available
    at [https://github.com/amanb2000/Magic_Words](https://github.com/amanb2000/Magic_Words).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Prompt engineering is crucial for deploying LLMs but is poorly understood mathematically.
    We formalize LLM systems as a class of discrete stochastic dynamical systems to
    explore prompt engineering through the lens of control theory. We investigate
    the reachable set of output token sequences $\mathcal{R}_{y}(\mathbf{x}_{0})$
    tokens. Intriguingly, short prompt sequences can dramatically alter the likelihood
    of specific outputs, even making the least likely tokens become the most likely
    ones. This control-centric analysis of LLMs demonstrates the significant and poorly
    understood role of input sequences in steering output probabilities, offering
    a foundational perspective for enhancing language model system capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: LLMs pre-trained on unsupervised next token prediction objectives exhibit unprecedented
    dynamic reprogrammability achieved through “prompting”, often referred to as zero-shot
    learning (Brown et al., [2020](#bib.bib5); Wei et al., [2022](#bib.bib39); Hagendorff,
    [2023](#bib.bib12); Noever & McKee, [2023](#bib.bib19); OpenAI, [2023](#bib.bib22);
    [2022](#bib.bib21)). These capabilities appear to emerge as the model’s size,
    training data, and training time are scaled. The dynamic reprogrammability of
    LLMs is akin to the adaptable computational capacities observed in biological
    systems. This feature finds applications across domains such as machine translation
    (Wang et al., [2023a](#bib.bib37)), code generation (Rozière et al., [2023](#bib.bib28)),
    and chatbots Bai et al. ([2022](#bib.bib3)). A rigorous understanding of the prompt’s
    influence over LLM generation would be of great utility for understanding LLMs
    and building more robust and capable systems leveraging LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Strategies for controlling pre-trained LLM generation today fall into three
    broad categories (Zhang et al., [2022](#bib.bib44)):'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Input Optimization (Prompting): Adjusting the input tokens (e.g., rewording
    the prompt) to improve subsequent text generation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Model Optimization: Adjusting the weights of the network (e.g., fine-tuning,
    RLHF) to improve model behavior during inference.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Post-processing: Adjusting or re-ranking generated text (e.g., surrogate ranking
    algorithm).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Of all these approaches, input optimization (i.e., prompting) is the least invasive
    and lowest-cost method – and the least understood. Prompt optimization is also
    deeply connected to the zero-shot capabilities of LLMs – the mysterious emergent
    capabilities of LLMs such as problem-solving, knowledge retrieval, reasoning,
    and apparent general intelligence (Bubeck et al., [2023](#bib.bib6)). With such
    a view, we seek to characterize the controllability of LLMs via prompting.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Contribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We formalize LLM systems in the mathematical framework of control theory in
    Section [3](#S3 "3 Control Theory for LLMs ‣ What’s the Magic Word? A Control
    Theory of LLM Prompting"). Our analysis focuses on the reachable set of outputs
    $\mathcal{R}_{y}(\mathbf{x}_{0})$ (cf. Definitions [3](#Thmdefinition3 "Definition
    3 (LLM Reachable Output Set). ‣ Language Model Notation: ‣ 3 Control Theory for
    LLMs ‣ What’s the Magic Word? A Control Theory of LLM Prompting"), [11](#Thmdefinition11
    "Definition 11 (Reachable Output Set). ‣ Appendix A Abstract Systems and Control
    Theory Background ‣ What’s the Magic Word? A Control Theory of LLM Prompting")).'
  prefs: []
  type: TYPE_NORMAL
- en: Our analytic results in Section [4](#S4 "4 Mathematical Analysis on the Controllability
    of Self-Attention ‣ What’s the Magic Word? A Control Theory of LLM Prompting")
    prove an upper bound on the contents of the reachable output set for a self-attention
    head as a function of the singular values of its parameter matrices. Since self-attention
    is the only component in a transformer block where significant information is
    exchanged between token representations, this bound provides a foothold for analysis
    of LLM controllability from the perspective of mechanistic interpretability (e.g.,
    Bricken et al. ([2023](#bib.bib4)); Chefer et al. ([2021](#bib.bib8)); Conmy et al.
    ([2023](#bib.bib9))). Moreover, this bound represents a necessary condition for
    an output to be in the reachable set.
  prefs: []
  type: TYPE_NORMAL
- en: Our empirical results apply state-of-the-art prompt optimization techniques
    (Section [5.1](#S5.SS1 "5.1 Methods ‣ 5 Experiments ‣ What’s the Magic Word? A
    Control Theory of LLM Prompting")) to demonstrate a lower bound on the contents
    of the reachable output set for a panel of LLMs, including Llama-7b (Touvron et al.,
    [2023](#bib.bib35)), Falcon-7b, and Falcon-40b (Almazrouei et al., [2023](#bib.bib1)).
    Specifically, we sample initial states $\mathbf{x}_{0}$ control input tokens.
    Our results suggest that prior likelihood-based metrics, such as cross-entropy
    loss, cannot guarantee exclusion from the reachable set, emphasizing the gap in
    our current understanding of LLM systems and control. Implications of our results
    and open questions in LLM control theory are further discussed in Section [6](#S6
    "6 Discussion ‣ What’s the Magic Word? A Control Theory of LLM Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Much of the work on prompt optimization is concerned with finding prompts that
    induce higher LLM performance on “fill-in-the-blank” or “cloze” tasks (Taylor,
    [1953](#bib.bib34)). One can frame a range of tasks including knowledge retrieval
    (Petroni et al., [2019](#bib.bib24)), reasoning (Weston et al., [2016](#bib.bib42)),
    and sentiment analysis (Wang et al., [2023b](#bib.bib38)) as fill-in-the-blank
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knowledge Retrieval: “The Titanic sank in the year [MASK].” (Answer: “1912”)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reasoning: “A is taller than B. B is taller than C. Is A taller than C? Answer:
    [MASK]” (Answer: “Yes”)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sentiment Analysis: “I am sad today. The sentiment of the previous sentence
    was [MASK]” (Answer: “Negative”)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Notably, there is some freedom in the bolded “prompt text” that surrounds the
    question to convert it into a “fill-in-the-blank” task. As it turns out, the prompt
    tokens have a large effect on LLM performance (Brown et al., [2020](#bib.bib5);
    Zhang et al., [2022](#bib.bib44); Jiang et al., [2020](#bib.bib13)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Modern prompt optimization algorithms generally consist of two iterated steps:
    a sampling step where new prompts are generated and a testing step where the utility
    of the new prompts is evaluated, and the best are selected for the next iteration.
    Algorithms primarily differ in the sampling procedure, where various heuristics
    may be used to pick high-value swaps (Wen et al., [2023](#bib.bib41); Zhou et al.,
    [2023](#bib.bib45); Reynolds & McDonell, [2021](#bib.bib26)). Overall, AutoPrompt
    and its derivative algorithms have been the most numerically successful prompt
    optimization methods, with the greedy coordinate gradient (GCG) algorithm having
    state-of-the-art performance (Zou et al., [2023](#bib.bib46)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The AutoPrompt Family:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AutoPrompt (Shin et al., [2020](#bib.bib30)) pioneered the current wave of prompt
    optimization. Shin et al propose a prompt optimization technique and demonstrate
    its effectiveness for engineering prompts to improve LLM performance on knowledge
    and sentiment analysis tasks. At its core, the AutoPrompt algorithm leverages
    gradient information at the token embedding layer to inform iterative token exchanges
    within the prompt. This method was extended in Zou et al. ([2023](#bib.bib46))
    as the greedy coordinate gradient (GCG) algorithm. Taking inspiration from adversarial
    examples (Goodfellow et al., [2015](#bib.bib10)), Zou et al applied this AutoPrompt
    variant to generate “jailbreak” prompts that cause aligned LLMs to generate objectionable
    content.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Prompt Optimization Methods:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Other investigations on LLMs as prompt optimizers (Zhou et al., [2023](#bib.bib45))
    and further analysis of manual prompt optimization (Reynolds & McDonell, [2021](#bib.bib26))
    are informative but do not exceed the AutoPrompt family’s performance. Some other
    methods include GBDA (Guo et al., [2021](#bib.bib11)), an approach based on the
    Gumbel-Softmax reparametrization, the PEZ algorithm (Wen et al., [2023](#bib.bib41)),
    which directly optimizes embeddings via gradient information, and FluentPrompt
    (Shi et al., [2022](#bib.bib29)), which differs from AutoPrompt by incorporating
    Langevin dynamics. Despite the variety of alternatives, GCG retains state-of-the-art
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Control Theory for LLMs:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To our knowledge, the only other work to date on the controllability of LLMs
    is Soatto et al. ([2023](#bib.bib32)). Soatto et al analyze the controllability
    of LLMs in terms of “meaningful sentences”, defined as the sigma-algebra generated
    by snippets of text written on the Internet. Their empirical analysis revolves
    around demonstrating that LLMs are capable of attributing meaning. The theoretical
    analysis of LLM controllability is limited to “meaningful sentences”, eliminating
    the possibility of out-of-distribution inputs and outputs. These restrictions
    render their results challenging to leverage toward a practical understanding
    of LLM controllability. We situate our work as a practically oriented exploration
    of LLM controllability. Motivated by challenges in developing LLM systems, we
    do not eliminate “meaningless sentences” from the state space or input space.
    We aim to establish a rigorous, general framework for understanding LLM systems
    and controllability that is amenable to the development of theory and practical
    engineering insights on systems design.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Control Theory for LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Control theory originates from the study of automatic control systems in engineering.
    It seeks to understand how a “plant” system can be influenced toward a desired
    state using a “control signal” – often in the presence of disturbances and uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Control theory is central to a variety of engineering problems, from electrical
    engineering to autopilot to telecommunications to manufacturing. Surprisingly,
    control theory has also been highly applicable to a diverse range of scientific
    disciplines. Analyzing systems through the lens of controllability has proven
    fruitful for generating insight into biological systems such as cell signaling
    pathways and neural networks (Yi et al., [2000](#bib.bib43)), the economics of
    central banking (Aniţa et al., [2011](#bib.bib2)), and controlling the spread
    of infectious diseases (Roy et al., [2009](#bib.bib27)). One of the central benefits
    of studying systems via controllability is that a range of questions and problems
    naturally emerge from the framing: when is control possible? What is the cost
    of control? How computationally intensive is control? These questions are both
    practically useful and often lead to fundamental insights about the nature of
    the system in question.'
  prefs: []
  type: TYPE_NORMAL
- en: To develop a control theory of LLMs, we provide fundamental abstract definitions
    of systems and control (Appendix [A](#A1 "Appendix A Abstract Systems and Control
    Theory Background ‣ What’s the Magic Word? A Control Theory of LLM Prompting")).
    We apply them to define LLM systems and outline specific canonical control concepts
    and problems such as controllability and reachability that arise naturally for
    LLM systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Language Model Notation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We denote a causal language model using $P_{LM}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'While LLMs are at times leveraged in a manner that masks the iterative aspects
    of generation, the reality is that token generation and externally imposed “control
    input” sequences are generated and processed sequentially, leading to non-trivial
    system dynamics. Several key differences remain between LLM-based systems and
    systems typically modeled through ordinary differential equations (ODEs), which
    have long been a cornerstone in the study of continuous-time dynamical systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Discrete state and time: LLM systems operate on sequences of discrete tokens
    over a discrete time set, in contrast to the continuous state spaces and time
    sets studied in classical control theory.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shift-and-Grow State Dynamics: Whereas the system state in an ODE-based system
    has a fixed size over time, the system state $\mathbf{x}(t)$ for LLM systems grows
    as tokens are added to the state sequence.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Mutual exclusion on control input token vs. generated token: The LLM system
    state $\mathbf{x}(t)$. This differs from traditional discrete stochastic systems,
    where the control sequence and internal dynamics generally affect the state synchronously.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We begin by rigorously defining LLM systems with user input, drawing from the
    abstract mathematical definition of a system (Definition [7](#Thmdefinition7 "Definition
    7 (System). ‣ Appendix A Abstract Systems and Control Theory Background ‣ What’s
    the Magic Word? A Control Theory of LLM Prompting")).
  prefs: []
  type: TYPE_NORMAL
- en: Definition 1  (LLM System with Control Input).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'An autoregressive LLM system with control input $\Sigma=(\mathcal{V},P_{LM})$
    consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{T}=\mathbb{N}:$ The time set is the natural numbers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{X}=\mathcal{V}^{*}:$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{U}=\mathcal{V}\cup\varnothing:$ or null.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\phi:\mathcal{X}\times\mathcal{U}\times\mathcal{T}^{2}\to\mathcal{X}:$ The
    transition map is
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Note that the general multi-step transition map $\phi(\mathbf{x}(t),u,t,t+N)$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $h(\mathbf{x}(t);r)=[x^{t-r}(t),\dots,x^{t}(t)]:$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We note that this LLM system definition is generalizable to a variety of LLM
    augmentation, including chain-of-thought (Wei et al., [2023](#bib.bib40)), retrieval-augmented
    generation (Lewis et al., [2020](#bib.bib15)), and chatbot interaction. For example,
    chain-of-thought is equivalent to sampling the readout map $h(x(t),r)$. A similar
    formulation may be applied to LLM systems endowed with programmatic tools (e.g.,
    Patil et al. ([2023](#bib.bib23))).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Definition [1](#Thmdefinition1 "Definition 1 (LLM System with Control Input).
    ‣ Language Model Notation: ‣ 3 Control Theory for LLMs ‣ What’s the Magic Word?
    A Control Theory of LLM Prompting"), we assume that the control input gets to
    “decide” whether to yield token generation to the LLM ($u(t)=\varnothing$ unless
    otherwise stated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While next token generation $x^{\prime}\sim P_{LM}(x^{\prime}|\mathbf{x}(t))$
    in equation [1](#S3.E1 "In 4th item ‣ Definition 1 (LLM System with Control Input).
    ‣ Language Model Notation: ‣ 3 Control Theory for LLMs ‣ What’s the Magic Word?
    A Control Theory of LLM Prompting") is probabilistic, we may render the system
    deterministic by sampling with zero temperature (i.e., greedy decoding). The greedy
    decoding assumption provides a foothold to analyze the reachable sets and controllability
    of LLM systems without invoking notions of stochastic control as in Sivaramakrishnan
    et al. ([2023](#bib.bib31)); Soatto et al. ([2023](#bib.bib32)). Moreover, it
    remains connected to temperature-based stochastic decoding strategies as a limiting
    case of temperature-based sampling as zero-temperature sampling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now extend Definition [10](#Thmdefinition10 "Definition 10 (Output Reachability).
    ‣ Appendix A Abstract Systems and Control Theory Background ‣ What’s the Magic
    Word? A Control Theory of LLM Prompting") to define output controllability for
    LLM systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 2  (LLM Output Reachability).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Output token sequence $\mathbf{y}\in\mathcal{V}^{r}$.
  prefs: []
  type: TYPE_NORMAL
- en: We disregard the trivial solution wherein the control input $\mathbf{u}^{*}(t)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reachable output set definition for LLM systems follows from Definition [11](#Thmdefinition11
    "Definition 11 (Reachable Output Set). ‣ Appendix A Abstract Systems and Control
    Theory Background ‣ What’s the Magic Word? A Control Theory of LLM Prompting"):'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 3  (LLM Reachable Output Set).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The reachable output set from initial state $\mathbf{x}_{0}\in\mathcal{V}^{*}$.
  prefs: []
  type: TYPE_NORMAL
- en: Output controllability for LLMs follows from Definition [13](#Thmdefinition13
    "Definition 13 (Output Controllability). ‣ Appendix A Abstract Systems and Control
    Theory Background ‣ What’s the Magic Word? A Control Theory of LLM Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: Definition 4  (LLM Output Controllability).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An LLM system $\Sigma=(\mathcal{V},P_{LM})$.
  prefs: []
  type: TYPE_NORMAL
- en: The turn-based nature of writing to the LLM state sequence $\mathbf{x}(t)$.
    Due to the costly nature of long prompts, we are especially interested in the
    existence of prompts $\mathbf{u}^{*}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Definitions [3](#Thmdefinition3 "Definition 3 (LLM Reachable Output Set). ‣
    Language Model Notation: ‣ 3 Control Theory for LLMs ‣ What’s the Magic Word?
    A Control Theory of LLM Prompting") and  [4](#Thmdefinition4 "Definition 4 (LLM
    Output Controllability). ‣ Language Model Notation: ‣ 3 Control Theory for LLMs
    ‣ What’s the Magic Word? A Control Theory of LLM Prompting") form the basis for
    our control theory of LLMs. While amenable to analytic analysis as in Section [4](#S4
    "4 Mathematical Analysis on the Controllability of Self-Attention ‣ What’s the
    Magic Word? A Control Theory of LLM Prompting") and Soatto et al. ([2023](#bib.bib32)),
    empirical analysis of the reachable set and controllability is challenging due
    to the intractable size of $\mathcal{V}^{*}$:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 5  ($k-\epsilon$ Controllability).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Consider a dataset of state-output pairs $\mathcal{D}=\{(\mathbf{x}_{0}^{i},\mathbf{y}^{i})\}_{i\in[N]}$,
    the following condition holds:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{y}^{i}\in\mathcal{R}^{k}_{y}(\mathbf{x}_{0}^{i})$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: Where $\mathcal{R}^{k}_{y}(\mathbf{x}_{0}^{i})$.
  prefs: []
  type: TYPE_NORMAL
- en: Our empirical work in Section [5.2](#S5.SS2 "5.2 Results ‣ 5 Experiments ‣ What’s
    the Magic Word? A Control Theory of LLM Prompting") explores $k-\epsilon$ that
    elicit a given output, and thus establish a lower bound on the content of the
    reachable set. Meanwhile, our theoretical work in Section [4](#S4 "4 Mathematical
    Analysis on the Controllability of Self-Attention ‣ What’s the Magic Word? A Control
    Theory of LLM Prompting") establishes upper bounds on the content of the reachable
    set for self-attention. We hope these complementary approaches aid in unifying
    our understanding of LLM systems.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Mathematical Analysis on the Controllability of Self-Attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Self-attention is a central component in modern transformer-based language models
    (Brown et al., [2020](#bib.bib5); Touvron et al., [2023](#bib.bib35); Radford
    et al., [2019](#bib.bib25); Min et al., [2023](#bib.bib18)). Introduced in Vaswani
    et al. ([2017](#bib.bib36)), self-attention is the primary component in transformers
    where token representations exchange information.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 6  (Self-Attention).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Self-attention $\Xi=(\mathbf{W_{q},W_{k},W_{v}})$ is the dimensionality of the
    output token representations.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Xi(\mathbf{X})=\mathbb{D}^{-1}\exp\begin{pmatrix}\frac{\mathbf{QK^{\top}}}{\sqrt{d_{k}}}\end{pmatrix}\mathbf{V}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $\exp()$ is a diagonal positive definite matrix defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbb{D}=\text{diag}\begin{pmatrix}\exp\begin{pmatrix}\frac{\mathbf{QK^{\top}}}{\sqrt{d_{k}}}\end{pmatrix}\mathbf{1}_{N\times
    1}\end{pmatrix}$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{1}_{N\times 1}$ matrix of ones.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the parameters and operation of $\Xi$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are interested in the reachability of output token representations, where
    we partition the input $\mathbf{X}\in\mathbb{R}^{(k+M)\times d_{in}}$, we may
    readily adapt Definition [3](#Thmdefinition3 "Definition 3 (LLM Reachable Output
    Set). ‣ Language Model Notation: ‣ 3 Control Theory for LLMs ‣ What’s the Magic
    Word? A Control Theory of LLM Prompting") to define the reachable set for these
    conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: Theorem 1  (Condition for Exclusion from the Reachable Set).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A desired output representation $\mathbf{Y}^{*}\in\mathbb{R}^{M\times d_{out}}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\Omega_{u}=\max_{j}\|\mathbf{u}^{j}\|$, which is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: ${\mathbf{y}^{*}}^{i}$, which is given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{Y}}_{x}=\mathbf{\hat{D}}_{xx}^{-1}\exp\begin{pmatrix}\frac{\mathbf{Q_{x}K_{x}}^{\top}}{\sqrt{d_{k}}}\end{pmatrix}\mathbf{V}_{x}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{Q_{x}=X_{0}W_{q}}$
  prefs: []
  type: TYPE_NORMAL
- en: The proof of Equation [5](#S4.E5 "In Theorem 1 (Condition for Exclusion from
    the Reachable Set). ‣ 4 Mathematical Analysis on the Controllability of Self-Attention
    ‣ What’s the Magic Word? A Control Theory of LLM Prompting") is provided in Appendix [B](#A2
    "Appendix B Proof of Self-Attention Controllability Theorem 1 ‣ What’s the Magic
    Word? A Control Theory of LLM Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, this condition arises when when the output representation $\hat{\mathbf{Y}}_{x}$.
    Thus, we have an analytic bound on the reachable output set for self-attention
    (see further commentary in Section [6](#S6 "6 Discussion ‣ What’s the Magic Word?
    A Control Theory of LLM Prompting")).
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To gain a practical, empirical understanding of the reachable set $\mathcal{R}_{y}^{k}(\mathbf{x}_{0})$
    to future work.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We apply prompt optimization algorithms to establish the existence of optimal
    prompts $\mathbf{u}^{*}$ tokens, while GCG was the best-performing algorithm for
    prompts of 4 or more tokens. To our knowledge, our greedy back-generation algorithm
    is novel. For brevity, we place the full description of the algorithms and our
    parameter values for the two algorithms in Appendix [C](#A3 "Appendix C Prompt
    Optimization Algorithms ‣ What’s the Magic Word? A Control Theory of LLM Prompting"),
    as the specifics of the algorithms are not the main contribution of this work.
  prefs: []
  type: TYPE_NORMAL
- en: We focus on understanding the content and structure of the reachable set of
    LLM system outputs $\mathcal{R}_{y}^{k}(\mathbf{x}_{0})$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Back-off Prompt
  prefs: []
  type: TYPE_NORMAL
- en: 1:State-output token sequence $(\mathbf{x}_{0},y)$.9:end for10:return Failed
    to establish reachability.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our results revolve around the reachable set $\mathcal{R}_{y}^{k}(\mathbf{x}_{0})$
    for each experiment.
  prefs: []
  type: TYPE_NORMAL
- en: '“Ground truth” reachability:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We established the reachability of the “ground truth” next token $y$ (Figure [1](#S5.F1.22
    "Figure 1 ‣ “Ground truth” reachability: ‣ 5.2 Results ‣ 5 Experiments ‣ What’s
    the Magic Word? A Control Theory of LLM Prompting"), top left). Plots and supplementary
    figures for Falcon-40b and Llama-7b controllability w.r.t. ground truth Wikitext
    outputs can be found in Section [D.1](#A4.SS1 "D.1 “Ground Truth” Controllability
    Results ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the
    Magic Word? A Control Theory of LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3292516a94c287b0df8e90dab7976d0d.png)![Refer to caption](img/83bf183e9e94bf812a6742ea25fd7bb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Main experimental results on $k-\epsilon$ controllability of Falcon-7b.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Top left: $k-\epsilon$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Top right: $k-\epsilon$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bottom right: Prior likelihood rank of target token $y^{*}$. Target tokens
    were sampled uniformly from the least to most likely token.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a9b9adeb476c90236a3dbbfc95efa44a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Top-75 reachability:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To explore the reachable set $R_{y}^{k}(\mathbf{x}_{0})$ controllability with
    respect to the top 75 most likely output tokens can be found in Section [D.2](#A4.SS2
    "D.2 Top-75 Wikitext Controllability Results ‣ Appendix D Supplementary Figures:
    Optimal Control Prompts ‣ What’s the Magic Word? A Control Theory of LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Uniformly sampled target outputs:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To maximally push the bounds of the reachable set within our single output
    token scope, we created another synthetic dataset where the target output token
    $y^{*}$ can be found in Section [D.3](#A4.SS3 "D.3 Uniformly Sampled Output Token
    Results ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the
    Magic Word? A Control Theory of LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We proposed a control theoretic framework for understanding language model prompting,
    orienting our investigation around the reachable set of outputs $\mathcal{R}_{y}^{k}(\mathbf{x}_{0})$.
    We proved a bound on the reachable set of outputs for self-attention in terms
    of the singular values of its weight matrices, and we established fundamental
    results on the reachability of “correct” next tokens (according to Wikitext).
    We expanded the scope of this investigation by probing the reachability of tokens
    assigned high likelihood by the LLM itself, and tokens assigned minimal likelihood
    by the LLM itself.
  prefs: []
  type: TYPE_NORMAL
- en: Bounding the reachable set for self-attention is deeply related to the mechanism
    by which consistent representations are formed for multi-token generation. Steering
    a language model to generate a desired token sequence requires that the control
    input induce a token representation in the right-most token such that the next
    token prediction logits $P(\mathbf{y}|\mathbf{u}+\mathbf{x}_{0})$ for self-attention
    a fundamental part of LLM control theory.
  prefs: []
  type: TYPE_NORMAL
- en: Our empirical results suggest that there is far more to the reachability of
    a given output than just prior likelihood or the prior rank the LLM assigns to
    a given token. Although prompt optimization-based $k-\epsilon$ controllability
    experiments are only able to provide a lower bound on the content of the reachable
    set, the ability to frequently control even the least likely token to being the
    most likely token with just a few input tokens is intriguing. We believe this
    result indicates the importance of further investigating the reachability and
    controllability of LLMs, particularly for developing capable and reliable LLM
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: Our investigations provide an entry into the understanding of LLM controllability
    via prompts. However, a comprehensive understanding necessitates extending our
    exploration into diverse regimes. Exploring the controllability with longer prompts
    and longer questions (base token sequences) will be pivotal. Equally important
    is the study of diverse models to verify the generality of our findings. Moreover,
    the direct comparison of controllability scores of different model families is
    challenging since family uses a different tokenizer. The Llama family tokenizer,
    for instance, has a vocabulary of 30,000 tokens whereas the Falcon family has
    a vocabulary of 65,536 tokens. Further work is required to robustly compare controllability
    across models.
  prefs: []
  type: TYPE_NORMAL
- en: 'An intriguing observation from our study is the log-linear relationship between
    prompt length $k$ (see Figure [2](#A4.F2.20 "Figure 2 ‣ D.1 “Ground Truth” Controllability
    Results ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the
    Magic Word? A Control Theory of LLM Prompting") in Appendix [D](#A4 "Appendix
    D Supplementary Figures: Optimal Control Prompts ‣ What’s the Magic Word? A Control
    Theory of LLM Prompting")). While this is compelling within our studied domain,
    it raises the essential question: is this relationship robust outside our current
    explorative scope? Unearthing universal scaling laws in LLM controllability would
    not only inform practical control applications but also open the door for theoretical
    insight into the nature of LLM behavior.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The progress we have made, both in understanding the bounds on self-attention
    controllability and the empirical measures of $k-\epsilon$ LLM controllability,
    underscores the potential of this control theoretic framing for studying LLMs.
    Below is a non-exhaustive list of open problems in LLM control, all stemming from
    the framing in section [A](#A1 "Appendix A Abstract Systems and Control Theory
    Background ‣ What’s the Magic Word? A Control Theory of LLM Prompting"):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Control Properties of Chain-of-Thought: Chain-of-Thought is a powerful technique
    where LLMs are allowed to generate intermediate tokens (i.e., “thoughts”) between
    a question and an answer (Wei et al., [2023](#bib.bib40)). The control properties
    (e.g., stability, reachability) of systems leveraging these techniques are of
    great interest for understanding and composing systems of LLMs in the real world.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distributional Control: To what extent can we control the output distribution
    of a language model $P_{LM}(\mathbf{y}|\mathbf{x}_{0}+\mathbf{u})$?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computational Cost of Control: What are the performance characteristics of
    LLM control regularized by computational cost?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learnability of Control: To what extent can LLMs learn to control each other?
    Work such as Zhou et al. ([2023](#bib.bib45)) showed that LLMs are capable of
    human-level prompt engineering, but it is unclear how well an LLM can learn to
    control another when explicitly optimized on the objective of LLM control.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Controllable Subspaces: In the control of linear dynamical systems, it is known
    that uncontrollable systems are often coordinate transformable into a representation
    where a subset of the coordinates are controllable and a subset are uncontrollable
    Sontag ([2013](#bib.bib33)). We have shown that controllable and uncontrollable
    components naturally emerge for self-attention heads in section [4](#S4 "4 Mathematical
    Analysis on the Controllability of Self-Attention ‣ What’s the Magic Word? A Control
    Theory of LLM Prompting") – can this be generalized to transformer blocks with
    nonlinearities and residual streams?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Composable LLM Systems: One of the greatest boons of control theory is the
    ability to compose control modules and subsystems into an interpretable, predictable,
    and effective whole (Lian et al., [2002](#bib.bib16)). The composition of LLM
    systems (potentially with non-LLM control modules) is an exciting avenue for scaling
    super intelligent systems.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel
    Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and
    Guilherme Penedo. Falcon-40B: an open large language model with state-of-the-art
    performance. 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aniţa et al. (2011) Sebastian Aniţa, Viorel Arnăutu, Vincenzo Capasso, and
    Vincenzo Capasso. *An introduction to optimal control problems in life sciences
    and economics: From mathematical models to numerical simulation with MATLAB®*,
    volume 2. Springer, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2022) Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna
    Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.
    Training a helpful and harmless assistant with reinforcement learning from human
    feedback. *arXiv preprint arXiv:2204.05862*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bricken et al. (2023) Trenton Bricken, Adly Templeton, Joshua Batson, Brian
    Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda
    Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell,
    Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean,
    Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah.
    Towards monosemanticity: Decomposing language models with dictionary learning.
    *Transformer Circuits Thread*, 2023. https://transformer-circuits.pub/2023/monosemantic-features/index.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell,
    M.F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing Systems*,
    volume 33, pp.  1877–1901\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial
    general intelligence: Early experiments with gpt-4, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calafiore & El Ghaoui (2014) Giuseppe C Calafiore and Laurent El Ghaoui. *Optimization
    models*. Cambridge university press, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chefer et al. (2021) Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability
    beyond attention visualization, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conmy et al. (2023) Arthur Conmy, Augustine N. Mavor-Parker, Aengus Lynch, Stefan
    Heimersheim, and Adrià Garriga-Alonso. Towards automated circuit discovery for
    mechanistic interpretability, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2015) Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy.
    Explaining and harnessing adversarial examples, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2021) Chuan Guo, Alexandre Sablayrolles, Hervé Jégou, and Douwe
    Kiela. Gradient-based adversarial attacks against text transformers, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hagendorff (2023) Thilo Hagendorff. Machine psychology: Investigating emergent
    capabilities and behavior in large language models using psychological methods,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2020) Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig.
    How can we know what language models know?, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kalman et al. (1969) Rudolf Emil Kalman, Peter L Falb, and Michael A Arbib.
    *Topics in mathematical system theory*, volume 33. McGraw-Hill New York, 1969.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2020) Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni,
    Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim
    Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp
    tasks. *Advances in Neural Information Processing Systems*, 33:9459–9474, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lian et al. (2002) Feng-Li Lian, James Moyne, and Dawn Tilbury. Network design
    consideration for distributed control systems. *IEEE transactions on control systems
    technology*, 10(2):297–307, 2002.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Merity et al. (2016) Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al. (2023) Bonan Min, Hayley Ross, Elior Sulem, Amir Pouran Ben Veyseh,
    Thien Huu Nguyen, Oscar Sainz, Eneko Agirre, Ilana Heintz, and Dan Roth. Recent
    advances in natural language processing via large pre-trained language models:
    A survey. *ACM Computing Surveys*, 56(2):1–40, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Noever & McKee (2023) David Noever and Forrest McKee. Numeracy from literacy:
    Data science as an emergent skill from large language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ogata (2010) Katsuhiko Ogata. *Modern control engineering fifth edition*. 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2022) OpenAI, Nov 2022. URL [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Patil et al. (2023) Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E.
    Gonzalez. Gorilla: Large language model connected with massive apis, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petroni et al. (2019) Fabio Petroni, Tim Rocktäschel, Patrick S. H. Lewis, Anton
    Bakhtin, Yuxiang Wu, Alexander H. Miller, and Sebastian Riedel. Language models
    as knowledge bases? *CoRR*, abs/1909.01066, 2019. URL [http://arxiv.org/abs/1909.01066](http://arxiv.org/abs/1909.01066).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reynolds & McDonell (2021) Laria Reynolds and Kyle McDonell. Prompt programming
    for large language models: Beyond the few-shot paradigm, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Roy et al. (2009) Sandip Roy, Yan Wan, and Ali Saberi. A network control theory
    approach to virus spread mitigation. In *2009 IEEE Conference on Technologies
    for Homeland Security*, pp.  599–606, 2009. doi: 10.1109/THS.2009.5168092.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton
    Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal
    Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel
    Synnaeve. Code llama: Open foundation models for code, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shi et al. (2022) Weijia Shi, Xiaochuang Han, Hila Gonen, Ari Holtzman, Yulia
    Tsvetkov, and Luke Zettlemoyer. Toward human readable prompt tuning: Kubrick’s
    the shining is a good movie, and a good prompt too?, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L. Logan IV au2, Eric
    Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models
    with automatically generated prompts, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sivaramakrishnan et al. (2023) Karthik Sivaramakrishnan, Vignesh Sivaramakrishnan,
    and Meeko M. K. Oishi. Stochastic reachability of discrete-time stochastic systems
    via probability measures, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soatto et al. (2023) Stefano Soatto, Paulo Tabuada, Pratik Chaudhari, and Tian Yu
    Liu. Taming ai bots: Controllability of neural states in large language models,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sontag (2013) Eduardo D Sontag. *Mathematical control theory: deterministic
    finite dimensional systems*, volume 6. Springer Science & Business Media, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taylor (1953) Wilson L. Taylor. “cloze procedure”: A new tool for measuring
    readability. *Journalism Quarterly*, 30(4):415–433, 1953. doi: 10.1177/107769905303000401.
    URL [https://doi.org/10.1177/107769905303000401](https://doi.org/10.1177/107769905303000401).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023a) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian
    Yu, Shuming Shi, and Zhaopeng Tu. Document-level machine translation with large
    language models. *arXiv preprint arXiv:2304.02210*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023b) Zengzhi Wang, Qiming Xie, Zixiang Ding, Yi Feng, and Rui
    Xia. Is chatgpt a good sentiment analyzer? a preliminary study. *arXiv preprint
    arXiv:2304.04339*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. Emergent abilities of large language models, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits
    reasoning in large language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. (2023) Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum,
    Jonas Geiping, and Tom Goldstein. Hard prompts made easy: Gradient-based discrete
    optimization for prompt tuning and discovery, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weston et al. (2016) Jason Weston, Antoine Bordes, Sumit Chopra, and Tomás
    Mikolov. Towards ai-complete question answering: A set of prerequisite toy tasks.
    In Yoshua Bengio and Yann LeCun (eds.), *4th International Conference on Learning
    Representations, ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track
    Proceedings*, 2016. URL [http://arxiv.org/abs/1502.05698](http://arxiv.org/abs/1502.05698).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2000) Tau-Mu Yi, Yun Huang, Melvin I Simon, and John Doyle. Robust
    perfect adaptation in bacterial chemotaxis through integral feedback control.
    *Proceedings of the National Academy of Sciences*, 97(9):4649–4653, 2000.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Hanqing Zhang, Haolin Song, Shaoyu Li, Ming Zhou, and Dawei
    Song. A survey of controllable text generation using transformer-based pre-trained
    language models. *CoRR*, abs/2201.05337, 2022. URL [https://arxiv.org/abs/2201.05337](https://arxiv.org/abs/2201.05337).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2023) Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster,
    Silviu Pitis, Harris Chan, and Jimmy Ba. Large language models are human-level
    prompt engineers, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zou et al. (2023) Andy Zou, Zifan Wang, J. Zico Kolter, and Matt Fredrikson.
    Universal and transferable adversarial attacks on aligned language models, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Abstract Systems and Control Theory Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section aims to provide an overview of fundamental control-theoretic concepts
    from an abstract perspective. We primarily draw from Sontag ([2013](#bib.bib33));
    Kalman et al. ([1969](#bib.bib14)), and Ogata ([2010](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Diverse definitions of “system” or “machine” exist in the literature, all representing
    the same core concept but varying in mathematical details. We offer the following
    high-level definition based on Sontag ([2013](#bib.bib33)):'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 7  (System).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'A “system” or “machine” $\Sigma=(\mathcal{T,X,U},\phi)$ consists of:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{T}:$ The time set along which system state evolves.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{X}:$ The state space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{U}:$ The input space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\phi:\mathcal{X\times U\times T}^{2}\to\mathcal{X}:$ The transition map.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A system may also be equipped with an output space and readout map $(\mathcal{Y},h)$:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathcal{Y}:$ The output space.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $h:\mathcal{X\times U\times T}\to\mathcal{Y}:$ The readout map.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In other words, at time $t\in\mathcal{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we assume that the system $\Sigma$ do not change as a function of
    time. This assumption is widely applicable and is often made in the literature
    (Kalman et al., [1969](#bib.bib14); Ogata, [2010](#bib.bib20); Sontag, [2013](#bib.bib33))
    to simplify definitions and discussions of systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reachability is a core control theory concept and central to defining controllability.
    At their core, definitions of reachability revolve around the existence of control
    inputs $u\in\mathcal{U}$ to some desired state(s). Following from Kalman et al.
    ([1969](#bib.bib14)); Sontag ([2013](#bib.bib33)), we define state reachability
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 8  (State Reachability).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: State $x\in\mathcal{X}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may use this definition of state reachability to define the reachable state
    set for some initial state $x_{0}\in\mathcal{X}$:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 9  (Reachable State Set).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The reachable state set from initial state $x_{0}\in\mathcal{X}$ (cf. Definition [8](#Thmdefinition8
    "Definition 8 (State Reachability). ‣ Appendix A Abstract Systems and Control
    Theory Background ‣ What’s the Magic Word? A Control Theory of LLM Prompting")).
  prefs: []
  type: TYPE_NORMAL
- en: For systems with readout maps $h$, notions of output reachability arise naturally.
    Note that state reachability is neither necessary nor sufficient to guarantee
    output reachability.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 10  (Output Reachability).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Output $y\in\mathcal{Y}$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 11  (Reachable Output Set).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The reachable output set from initial state $x_{0}\in\mathcal{X}$ (cf. Definition [10](#Thmdefinition10
    "Definition 10 (Output Reachability). ‣ Appendix A Abstract Systems and Control
    Theory Background ‣ What’s the Magic Word? A Control Theory of LLM Prompting")).
  prefs: []
  type: TYPE_NORMAL
- en: A system is controllable when the reachable set extends to the entire state
    space. Practically speaking, this implies that one can steer the system from any
    initial state to any desired state.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 12  (State Controllability).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: System $\Sigma=(\mathcal{T,X,U},\phi)$.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 13  (Output Controllability).
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: System $\Sigma=(\mathcal{T,X,U},\phi,\mathcal{Y},h)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'A range of fruitful questions stem from these definitions: if there is a cost
    associated with control inputs $u\in\mathcal{U}$ (e.g., power constraints, length
    constraints), what is the minimum cost of control? What is the minimum time required
    to get from the initial state to the desired final state or output? If the system
    is not completely controllable, under what conditions is it controllable? Under
    which readout maps is a system output controllable?'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Proof of Self-Attention Controllability Theorem [1](#Thmtheorem1
    "Theorem 1 (Condition for Exclusion from the Reachable Set). ‣ 4 Mathematical
    Analysis on the Controllability of Self-Attention ‣ What’s the Magic Word? A Control
    Theory of LLM Prompting")
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Note: Key terms for the proof are introduced in Section [4](#S4 "4 Mathematical
    Analysis on the Controllability of Self-Attention ‣ What’s the Magic Word? A Control
    Theory of LLM Prompting") surrounding Theorem [1](#Thmtheorem1 "Theorem 1 (Condition
    for Exclusion from the Reachable Set). ‣ 4 Mathematical Analysis on the Controllability
    of Self-Attention ‣ What’s the Magic Word? A Control Theory of LLM Prompting").
    Specifically, the definition of self-attention mechanism $\Xi$ are required background
    for this proof.'
  prefs: []
  type: TYPE_NORMAL
- en: Proof.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: For each token representation matrix $\mathbf{Q,K,V}\in\mathbb{R}^{(k+M)\times\cdot}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let $\mathbf{A}$ be the exponentiated query-key outer product matrix with the
    following block structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\mathbf{A}=\exp\begin{pmatrix}\frac{\textbf{Q K}^{\top}}{\sqrt{d_{k}}}\end{pmatrix}=\exp\begin{pmatrix}\begin{bmatrix}\mathbf{Q_{u}K_{u}^{\top}}&amp;\mathbf{Q_{u}K_{x}^{\top}}\\
    \mathbf{Q_{x}K_{u}^{\top}}&amp;\mathbf{Q_{x}K_{x}^{\top}}\end{bmatrix}\frac{1}{\sqrt{d_{k}}}\end{pmatrix}=\begin{bmatrix}\mathbf{A}_{uu}&amp;\mathbf{A}_{ux}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \mathbf{A}_{xu}&amp;\mathbf{A}_{xx}\end{bmatrix}$$ |  | (8) |
  prefs: []
  type: TYPE_NORMAL
- en: We apply a similar quadrant decomposition to $\mathbb{D}$, defined initially
    in Equation [4](#S4.E4 "In Definition 6 (Self-Attention). ‣ 4 Mathematical Analysis
    on the Controllability of Self-Attention ‣ What’s the Magic Word? A Control Theory
    of LLM Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $$\mathbb{D}=\text{diag}\begin{pmatrix}\exp\begin{pmatrix}\frac{\mathbf{QK}^{\top}}{\sqrt{d_{k}}}\end{pmatrix}\mathbf{1}_{N\times
    1}\end{pmatrix}=\begin{bmatrix}\mathbb{D}_{u}&amp;\mathbf{0}\\ \mathbf{0}&amp;\mathbb{D}_{x}\\'
  prefs: []
  type: TYPE_NORMAL
- en: \end{bmatrix}$$ |  | (9) |
  prefs: []
  type: TYPE_NORMAL
- en: where the quadrant demarcations in $\mathbb{D}$ follow from Equation [8](#A2.E8
    "In Proof. ‣ Appendix B Proof of Self-Attention Controllability Theorem 1 ‣ What’s
    the Magic Word? A Control Theory of LLM Prompting").
  prefs: []
  type: TYPE_NORMAL
- en: We may now express the self-attention mechanism output representations $\mathbf{Y}$
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{Y}=\mathbb{D}_{x}^{-1}\mathbf{A}_{xu}\mathbf{V}_{u}+\mathbb{D}_{x}^{-1}\mathbf{A}_{xx}\mathbf{V}_{x}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: We begin by stating the equality between the desired output $\mathbf{Y}^{*}$
    and the true system output.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{Y}^{*}$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\implies\mathbf{Y}_{u}$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: We may immediately bound the magnitude of the rows of $\mathbf{Y}_{u}$ must
    inherit this upper bound on magnitude to retain feasibility.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;\mathbf{y}_{u}^{i}\&#124;<\Omega_{u}\sigma_{q}$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: Refer to Chapter 8 of Calafiore & El Ghaoui ([2014](#bib.bib7))) for a detailed
    explanation of convex hulls and their properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'While $\mathbf{Y}_{x}$ be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\hat{\mathbf{D}}_{xu}$ |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'Recall Equation [7](#S4.E7 "In Theorem 1 (Condition for Exclusion from the
    Reachable Set). ‣ 4 Mathematical Analysis on the Controllability of Self-Attention
    ‣ What’s the Magic Word? A Control Theory of LLM Prompting"), which defines $\hat{\mathbf{Y}}_{x}$
    to disentangle the influence of the control input:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{Y}_{u}=\mathbf{Y}^{*}-(\hat{\mathbf{D}}_{xu}+\hat{\mathbf{D}}_{xx})^{-1}\hat{\mathbf{D}}_{xx}\hat{\mathbf{Y}}_{x}$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: Observe that the rows of $\mathbf{Y}_{x}$ are positively scaled versions of
    each other because the denominator matrices are all positive and diagonal. Applying
    the bound in Equation [13](#A2.E13 "In Proof. ‣ Appendix B Proof of Self-Attention
    Controllability Theorem 1 ‣ What’s the Magic Word? A Control Theory of LLM Prompting")
    using row-wise notation,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: Using the same singular values reasoning as in Equation [13](#A2.E13 "In Proof.
    ‣ Appendix B Proof of Self-Attention Controllability Theorem 1 ‣ What’s the Magic
    Word? A Control Theory of LLM Prompting") to bound the unknown denominator term
    $\hat{D}^{i}_{xu}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{D}_{xu}^{i}\leq k\exp\begin{pmatrix}\frac{\Omega_{x}\sigma_{q}\sigma_{k}\Omega_{u}}{\sqrt{d_{k}}}\end{pmatrix}$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: Achieving this minimum value will minimize the value of $\mathbf{y}_{x}^{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the the value of $\mathbf{y}_{x}^{i}$ is constrained linear scalings
    between this minimum and this maximum. If every scaling violates the inequality
    in Equation [16](#A2.E16 "In Proof. ‣ Appendix B Proof of Self-Attention Controllability
    Theorem 1 ‣ What’s the Magic Word? A Control Theory of LLM Prompting"), then the
    system is strictly controllable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, if $\langle{\mathbf{y}^{*}}^{i},\hat{\mathbf{y}}_{x}^{i}\rangle\leq
    0$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Prompt Optimization Algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Greedy Back-Generation:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While testing all prompts in $\mathcal{V}^{k}$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Greedy Token-Wise Prompt Generation
  prefs: []
  type: TYPE_NORMAL
- en: 1:A causal LLM $P_{LM}$. Prepend $u^{\prime}$
  prefs: []
  type: TYPE_NORMAL
- en: This method is optimal for $k=1$. Computing 1 additional prompt token takes
    roughly 1-4 minute when using an NVIDIA A100-80GB GPU with a 7 billion parameter
    model and 5-20 minutes on 2 NVIDIA A100-80GB GPUs with a 40 billion parameter
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Greedy Coordinate Gradient (GCG):'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The Greedy Coordinate Gradient algorithm, presented by (Zou et al., [2023](#bib.bib46))
    building off the work of (Shin et al., [2020](#bib.bib30)), is the state-of-the-art
    method for optimizing prompts. Starting with a random prompt of length $k$.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Greedy Coordinate Gradient
  prefs: []
  type: TYPE_NORMAL
- en: 1:A causal LLM $P_{LM}$ to $T$] $\triangleright$
  prefs: []
  type: TYPE_NORMAL
- en: This method outperforms all other methods we tested for prompts of length  iterations.
    For each instance, this optimization took roughly 2 minutes for the 7 billion
    parameter models on a single A100-80GB GPU and 4-8 minutes for the 40 billion
    parameter model on 4 A100-80GB GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix D Supplementary Figures: Optimal Control Prompts'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 “Ground Truth” Controllability Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This subsection includes supplementary figures for the controllability of Llama-7b,
    Falcon-7b, and Falcon-40b “ground truth” target outputs from Wikitext. For each
    initial state sequence $\mathbf{x}_{0}$ controllability of each of the 7 billion
    parameter models with a dataset of 5000 state-output pairs while we used a dataset
    of 500 state-output pairs for Falcon-40b.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2](#A4.F2.20 "Figure 2 ‣ D.1 “Ground Truth” Controllability Results
    ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the Magic
    Word? A Control Theory of LLM Prompting") shows each model’s log-spaced $k-\epsilon$
    in Figure [3](#A4.F3.6 "Figure 3 ‣ D.1 “Ground Truth” Controllability Results
    ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the Magic
    Word? A Control Theory of LLM Prompting") where we find it difficult to predict
    the required prompt length from the base loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Figure [4](#A4.F4 "Figure 4 ‣ D.1 “Ground Truth” Controllability Results
    ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the Magic
    Word? A Control Theory of LLM Prompting") shows a histogram of the tokens in the
    optimized prompts generated in the ground truth $k-\epsilon$ controllability experiments
    on Wikitext.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c082fef5cd77cf66d3aebfe382489d57.png)![Refer to caption](img/97d3dfd1ca077f17601a0a72f99a837a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Log spaced main results of $k-\log(\epsilon)$ appears roughly linear
    for each question length in the regime studied.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Top left: $k-\log(\epsilon)$ control tokens, 97.16% of the target outputs were
    reachable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Top right: $k-\log(\epsilon)$ control tokens, 98.64% of the target outputs
    were reachable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bottom right: $k-\log(\epsilon)$ control tokens, 97.00% of the target outputs
    were reachable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f0d852167012a178ec752e378d3e529.png)![Refer to caption](img/97c52438043178d8449a1dbf58260110.png)![Refer
    to caption](img/f55dbf7294d88ac6ec4f1d7d823e5ffd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Required prompt length $k$. While there does appear to be an “exclusion
    zone” in the top left-hand corner where a high prompt length is never associated
    with a base loss below a given threshold, base loss appears to be a poor predictor
    of required prompt length.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0126ccb8e570c7228f0de5ffa790769c.png)![Refer to caption](img/8f8761a55c70e44db991ca8883e59f31.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Falcon-7b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1687a7dd28c88619346f570b624718df.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Llama-7b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/92a6dd75bd940fecf7185d1628471448.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Falcon-40b
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Prompt token frequencies for Falcon-7b (top), Llama-7b (middle),
    and Falcon-40b (bottom) from Wikitext ground truth target token $k-\epsilon$ controllability
    experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Top-75 Wikitext Controllability Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This subsection includes supplementary figures for the controllability of Llama-7b,
    Falcon-7b, and Falcon-40b on the Wikitext dataset where the target output token
    $y$ plot. Figure [6](#A4.F6.6 "Figure 6 ‣ D.2 Top-75 Wikitext Controllability
    Results ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the
    Magic Word? A Control Theory of LLM Prompting") shows the relationship between
    base loss and required prompt length, revealing a more dramatic “exclusion zone”
    in the top left, similar to main “ground truth” results in Figure [3](#A4.F3.6
    "Figure 3 ‣ D.1 “Ground Truth” Controllability Results ‣ Appendix D Supplementary
    Figures: Optimal Control Prompts ‣ What’s the Magic Word? A Control Theory of
    LLM Prompting"). Finally, Figure [7](#A4.F7 "Figure 7 ‣ D.2 Top-75 Wikitext Controllability
    Results ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the
    Magic Word? A Control Theory of LLM Prompting") plots a histogram of the 40 most
    common tokens observed in the optimized control input prompts from the top-75
    experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78cb00eb70d625a8547103f8827bf641.png)![Refer to caption](img/ae4523594df5eafe047ce68490d8fd7d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: $k-\epsilon$ controllability plots on the top 75 most likely output
    tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Top left: $k-\epsilon$ control tokens, 89.387% of the top 75 output tokens
    were reachable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Top right: $k-\epsilon$ control tokens, 85.493% of the top 75 output tokens
    were reachable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bottom right: $k-\epsilon$ control tokens, 85.714% of the top 75 output tokens
    were reachable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/99abe3dbdce8de8e38954576768e4bbf.png)![Refer to caption](img/6cfac56d391f35dcf7ac7df5a45a072c.png)![Refer
    to caption](img/f0b0e64d4567e3d2cf02d3b7bcbedfe2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Required prompt length $k$ on synthetic top-75 dataset. While there
    does appear to be an “exclusion zone” in the top left-hand corner where a high
    prompt length is never associated with a base loss below a given threshold, base
    loss appears to be a poor predictor of required prompt length.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e2eb4749e3fe13f78b0efd385fdd3cc.png)![Refer to caption](img/6c2c12c53558d1050ebad46a871f0698.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Falcon-7b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/450218978a3da3847aa3d6185a815efc.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Llama-7b
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58a5002a600c975dabd6be32baa6d7e3.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Falcon-40b
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Prompt token frequencies for Falcon-7b (top), Llama-7b (middle),
    and Falcon-40b (bottom) from Wikitext top-75 synthetic dataset $k-\epsilon$ controllability
    experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: D.3 Uniformly Sampled Output Token Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section contains supplementary figures for $k-\epsilon$ results, the relationship
    between base loss and prompt length, and the most frequently observed tokens in
    the optimized control prompts. While the “exclusion zone” behavior (cf Figures [6](#A4.F6.6
    "Figure 6 ‣ D.2 Top-75 Wikitext Controllability Results ‣ Appendix D Supplementary
    Figures: Optimal Control Prompts ‣ What’s the Magic Word? A Control Theory of
    LLM Prompting"), [3](#A4.F3.6 "Figure 3 ‣ D.1 “Ground Truth” Controllability Results
    ‣ Appendix D Supplementary Figures: Optimal Control Prompts ‣ What’s the Magic
    Word? A Control Theory of LLM Prompting")) is observed in the base loss vs. prompt
    length subplot, base loss remains a poor predictor of required prompt length.
    Moreover, Figure [1](#S5.F1.22 "Figure 1 ‣ “Ground truth” reachability: ‣ 5.2
    Results ‣ 5 Experiments ‣ What’s the Magic Word? A Control Theory of LLM Prompting")
    reveals an even more uniform relationship between the initial rank of the target
    output token and the required prompt length.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/90cd374cf6aa22cea79c807991e08940.png)![Refer to caption](img/520bc461e7752208b3fc026894f73c00.png)![Refer
    to caption](img/02d9a79c4a189c795b34092bb9766451.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Supplementary figures on uniformly sampled target output controllability
    tests on Falcon-7b. Top Left: $k-\epsilon$). Top Right: Base loss versus required
    prompt length. Bottom: Histogram of top 40 most frequent tokens in optimized control
    prompts.'
  prefs: []
  type: TYPE_NORMAL
