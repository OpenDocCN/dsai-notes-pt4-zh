- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:47:48'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt Highlighter: Interactive Control for Multi-Modal LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.04302](https://ar5iv.labs.arxiv.org/html/2312.04302)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yuechen Zhang¹         Shengju Qian¹         Bohao Peng¹         Shu Liu²         Jiaya
    Jia^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: ¹The Chinese University of Hong Kong    ²SmartMore
  prefs: []
  type: TYPE_NORMAL
- en: '[https://julianjuaner.github.io/projects/PromptHighlighter/](https://julianjuaner.github.io/projects/PromptHighlighter/)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This study targets a critical aspect of multi-modal LLMs’  (LLMs&VLMs) inference:
    explicit controllable text generation. Multi-modal LLMs empower multi-modality
    understanding with the capability of semantic generation yet bring less explainability
    and heavier reliance on prompt contents due to their autoregressive generative
    nature. While manipulating prompt formats could improve outputs, designing specific
    and precise prompts per task can be challenging and ineffective. To tackle this
    issue, we introduce a novel inference method, Prompt Highlighter, which enables
    users to highlight specific prompt spans to interactively control the focus during
    generation. Motivated by the classifier-free diffusion guidance, we form regular
    and unconditional context pairs based on highlighted tokens, demonstrating that
    the autoregressive generation in models can be guided in a classifier-free way.
    Notably, we find that, during inference, guiding the models with highlighted tokens
    through the attention weights leads to more desired outputs. Our approach is compatible
    with current LLMs and VLMs, achieving impressive customized generation results
    without training. Experiments confirm its effectiveness in focusing on input contexts
    and generating reliable content. Without tuning on LLaVA-v1.5, our method secured
    69.5 in the MMBench test and 1552.5 in MME-perception. Code is available at: [https://github.com/dvlab-research/Prompt-Highlighter/](https://github.com/dvlab-research/Prompt-Highlighter/).'
  prefs: []
  type: TYPE_NORMAL
- en: '{strip}![[Uncaptioned image]](img/479da94bbbc5fb0cf9d431f747733dd9.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Prompt Highlighter facilitates token-level user interactions for
    customized generation, compatible with both LLMs and VLMs. Compared with vanilla
    inference and prompt engineering, the context-highlighted inference provided by
    our method offers controllable generations and produces customized results. Outputs
    correlated with the highlighted parts are underlined.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Large Language Models (LLMs) have driven significant progress in a multitude
    of natural language processing tasks [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9)]. Further advancements have been achieved by extending these models
    to handle vision-language tasks [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)] through visual-language
    alignment and instruction tuning. These efforts have led to the development of
    Vision Language Models (VLMs), which can generate text based on multi-modal inputs.
    Due to its autoregressive nature, the typical generation process in LLMs and VLMs (multi-modal
    LLMs) is primarily conditioned on input contexts. Prompt engineering [[16](#bib.bib16),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19)] has emerged as a common
    interaction mechanism between humans and language models, where diverse formats
    and content of prompts are employed to steer the generation towards desired outcomes.
    However, prompt engineering often relies on empirical intuition and requires careful
    design of the context, making it less accessible for non-experts. As illustrated
    in the left part of Prompt Highlighter: Interactive Control for Multi-Modal LLMs,
    even the meticulously crafted prompts, which convey the concept of ‘compactness’
    clearly, can lead to unpredictable outputs that fail to meet the requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Instead of manipulating prompt-level contexts (*i.e*., prompt engineering) to
    control LMs’ generation process, we propose a novel inference approach, Prompt
    Highlighter, that enables token-level user interactions for personalized generations.
    Our method allows users to interact with multi-modal LLMs in a manner analogous
    to applying a highlighter tool on the input context in the text editor, enabling
    users to emphasize desired parts by highlighting them.
  prefs: []
  type: TYPE_NORMAL
- en: This highlighting mechanism is achieved by constructing a regular and unconditional
    input context pair with different textual embeddings in the highlighted tokens.
    Subsequently, we can adjust the model’s focus on the highlighted components by
    employing the classifier-free guidance [[20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]
    on predicted token probabilities. Moreover, by probing cross-token attention maps,
    we discover a robust correlation between attention scores and the semantic significance
    of tokens. This suggests that, in the autoregressive generation process of language
    models, the semantic relationship between tokens can be represented to a certain
    extent by their attention scores. Building on this insight, we introduce an attention
    activation strategy that adjusts the attention weights associated with a highlighted
    part. Specifically, Prompt Highlighter employs an adjusted attention mask to reweight
    corresponding attention scores, enabling a more focused generation on highlighted
    parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in Prompt Highlighter: Interactive Control for Multi-Modal LLMs,
    compared to vanilla inference, our highlighted inference can guide the generation
    process to produce controllable results that align more closely with user needs.
    Prompt Highlighter is compatible with mainstream transformer-based multi-modal
    LLMs. This compatibility encompasses VLMs that use precise patch-wise visual token
    mapping, such as LLaVA [[10](#bib.bib10), [23](#bib.bib23), [24](#bib.bib24)],
    as well as methods that employ implicit query-ba qw[]'
  prefs: []
  type: TYPE_NORMAL
- en: sed visual token mapping, like those based on Q-Former [[13](#bib.bib13), [15](#bib.bib15),
    [14](#bib.bib14), [11](#bib.bib11)]. This novel interaction paradigm with highlighted
    sections during the generation process goes beyond what prompt engineering can
    offer.
  prefs: []
  type: TYPE_NORMAL
- en: We further demonstrate the effectiveness of Prompt Highlighter by evaluating
    it using comprehensive multi-modal benchmarks. We verify that directly highlighting
    the full image context in VLMs can significantly improve the quality of generated
    image captions [[25](#bib.bib25)] and question-answering results. Specifically,
    our method can effectively mitigate the model’s propensity to hallucinate by guiding
    its focus toward reliable contexts, thereby enhancing overall performance. Notably,
    without additional training, our method improves the performance of the baseline
    LLaVA-v1.5, securing 2${}^{\text{nd}}$ place in both MMBench [[26](#bib.bib26)]
    and MME-perception [[27](#bib.bib27)] leaderboards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions can be summarized as follows: (1) We pioneer the exploration
    of fine-grained human-model interactions in multi-modal LLMs, proposing a plug-and-play
    pipeline that enables token-level user interactions for controllable generation.
    (2) We conduct extensive experiments on comprehensive benchmarks, demonstrating
    that our method significantly enhances the overall performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Multi-Modal LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent Large Language Models (LLMs)  [[7](#bib.bib7), [28](#bib.bib28), [8](#bib.bib8),
    [29](#bib.bib29), [30](#bib.bib30), [1](#bib.bib1), [9](#bib.bib9)] play a significant
    role in natural language processing tasks, particularly in language generation
    and question answering. Building upon these pre-trained language models, Vision-Language
    Models (VLMs)  [[10](#bib.bib10), [14](#bib.bib14), [31](#bib.bib31), [13](#bib.bib13),
    [11](#bib.bib11), [15](#bib.bib15)] further introduce the alignment between vision
    and language modalities by leveraging extensive training on image-caption pairs
    or image-question conversations. There are two prevalent methods for aligning
    vision and verbiage modalities. The first method, exemplified by LLaVA [[10](#bib.bib10)],
    directly maps image patches to tokens using a projector, establishing a one-to-one
    correspondence. The second method, represented by models like BLIP2 [[13](#bib.bib13),
    [32](#bib.bib32)], employs a Query Transformer (Q-Former) after getting image
    features to establish a non-uniform patch-token mapping. These methods use learnable
    queries to get compressed image features, yielding visual tokens rich with semantic
    information.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Interactions with Multi-Modal LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt engineering and interactions.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Based on the autoregressive property of LLMs, users aim to control the generation
    results by modifying the input contexts. This largely determines the test-time
    interactions with LLMs, primarily executed through prompt engineering. Representative
    methods such as CoT [[17](#bib.bib17)] introduce demonstrations in the context
    to enhance reasoning ability. Other multi-branch designs like ToT and GoT [[16](#bib.bib16),
    [33](#bib.bib33), [18](#bib.bib18), [34](#bib.bib34), [19](#bib.bib19)] have been
    proposed for rich and reliable context generation and self-checking. Aside from
    prompt engineering, human-model interactions have not been extensively explored
    in VLMs. Methods like Kosmos-2 [[31](#bib.bib31)], LLaVAInteractive [[35](#bib.bib35)],
    and LISA [[36](#bib.bib36)] enable grounding perception tasks such as detection,
    segmentation, and image editing through interaction with the language model. These
    task-oriented interactions require additional data collection and task-specific
    tunning. In contrast, Prompt Highlighter is plug-and-play for general text generation
    in pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6b60dbb82989c3dc3efd0f07123bdf25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An abstract pipeline of Prompt Highlighter. Users can control the
    focus of generation by marking out specific image regions or text spans. Then
    a token-level mask $\mathbf{m}$ is created to guide the language model’s inference.'
  prefs: []
  type: TYPE_NORMAL
- en: Classifier-free guidance and controllable generation.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Classifier-Free Guidance (CFG)  [[20](#bib.bib20)] enables a control on Diffusion
    Models’ generation process without a conventional classifier. Specifically, CFG’s
    step-wise sampling allows users to employ a negative prompt within the unconditional
    branch, effectively guiding the generation away from harmful distributions. This
    approach has been extended to language models by LLM-CFG  [[21](#bib.bib21)],
    allowing a controllable text generation and improved performance. However, LLM-CFG
    still requires a pair-wise prompt design and does not support partial token-level
    reweighting within the context, which is vital for controlling VLM’s generation.
    Besides, methods in Diffusion Models  [[37](#bib.bib37), [38](#bib.bib38)] achieve
    fine-grained control over image generation using text prompts by emphasizing areas
    within cross-attention maps. Despite these advancements, fine-grained control
    over autoregressive generation in LLMs and VLMs is still challenging. Prompt Highlighter
    is proposed to tackle this issue.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Prompt Highlighter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An overview of Prompt Highlighter is presented in [Fig. 2](#S2.F2 "In Prompt
    engineering and interactions. ‣ 2.2 Interactions with Multi-Modal LLMs ‣ 2 Related
    Works ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"). Given
    a pre-trained generative model $\mathbf{P}_{\Theta}$ autoregressively. The following
    section will delve into the specifics of our method.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Token-Level Highlight Guidance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In conditioned Diffusion Models  [[39](#bib.bib39)], given a noisy image $x$,
    the sampling process of the Classifier-Free Guidance (CFG) can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{\hat{P}}_{\Theta}(x&#124;c)\varpropto\mathbf{P}_{\Theta}(x&#124;c)^{\gamma}/\mathbf{P}_{\Theta}(x)^{\gamma-1}\text{,}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\log\mathbf{\hat{P}}_{\Theta}(\epsilon_{t}&#124;x_{t+1},c)$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle-(\gamma-1)\log\mathbf{P}_{\Theta}(\epsilon_{t}&#124;x_{t+1})\text{,}\vspace{-1mm}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'in which $\epsilon_{t}$. The model samples each subsequent token from the conditional
    probability distribution. Based on [Eq. 1](#S3.E1 "In 3.1 Token-Level Highlight
    Guidance ‣ 3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control for
    Multi-Modal LLMs"), the CFG sampling on the language model can be denoted as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'Similar to the transaction from [Eq. 1](#S3.E1 "In 3.1 Token-Level Highlight
    Guidance ‣ 3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control for
    Multi-Modal LLMs") to [Eq. 2](#S3.E2 "In 3.1 Token-Level Highlight Guidance ‣
    3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control for Multi-Modal
    LLMs"), the likelihood in LLM is represented as the next-token classification
    probability. Thus next token’s logit prediction $\mathbb{P}{x_{i}}=\log\mathbf{\hat{P}}_{\Theta}(x_{i}|x_{j is predicted.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Attention Activation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The token-level highlight guidance anchors the generative process with a token-wise
    logit reweighting. However, its effectiveness may diminish when facing long and
    complex input contexts with few highlighted tokens, making it difficult to distinguish
    $\mathbf{s}$. To further investigate token-wise correlations and their impact
    on generation results, we exclude sink tokens that dominate the attention score [[40](#bib.bib40)]
    and visualize cross-token self-attention score maps during inference. For instance,
    in the left of [Fig. 3](#S3.F3 "In 3.1 Token-Level Highlight Guidance ‣ 3 Prompt
    Highlighter ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"),
    pivotal tokens form a band-like pattern on the attention map, drawing attention
    from nearly all following tokens. This pattern endures with changes in the model’s
    layer number or attention heads, suggesting the attention mechanism’s consistency
    and robustness in well pre-trained LLMs [[8](#bib.bib8), [7](#bib.bib7)]. Meanwhile,
    it implies that attention scores within the model can represent the semantic correlation
    between tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When addressing diverse requirements, LLMs need to balance attention among
    multiple tokens. For instance, as seen in [Fig. 3](#S3.F3 "In 3.1 Token-Level
    Highlight Guidance ‣ 3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control
    for Multi-Modal LLMs") (left), as one of the requirements in the prompt, ‘compactness’
    might not get enough attention during the generation process, resulting in an
    output that is less compact than expected. Given the direct correlation observed
    between attention and tokens, we propose an attention activation strategy to activate
    the attention scores on highlighted tokens within the attention mechanism. This
    strategy can effectively steer the model’s focus towards or away from specific
    tokens, allowing for more nuanced and precise control over the output. We reformulate
    the model inference function in [Sec. 3.1](#S3.Ex2 "3.1 Token-Level Highlight
    Guidance ‣ 3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control for
    Multi-Modal LLMs") to a mask-conditioned one $\tilde{{\mathbf{P}}}_{\Theta}(x_{i}|s_{j. Consequently, attention activation leads to results that
    adhere more closely to the input context.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9449b2ef46a0fac3f0a21d196a6d0b4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: An example of token changed with CFG.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a0e6be33241affa4001c3f0f51951b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Left: A simple verification of the vertical band-like pattern in
    the attention map, with a report on the gradient summation. Right: Following [[45](#bib.bib45)],
    we present a visualization displaying the average contribution of context’s attention
    during generation.'
  prefs: []
  type: TYPE_NORMAL
- en: Limitations and future work.
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'While our approach introduces a novel method for controlling generation in
    multi-modal LLMs, it has certain limitations: (a). Additional computations: Our
    method requires an extra decoding branch, which brings additional computational
    overhead and GPU memory requirements. However, these additional loads are marginal
    and acceptable with the batched inference. We validate this by the caption experiment
    in [Tab. 5](#S4.T5 "In Hyper-parameters. ‣ 4.4 Ablation Study ‣ 4 Experiments
    ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"). (b). Dependence
    on base model: Content generation quality is tied to the base model’s capabilities,
    which may result in over-emphasis or miss-emphasis on highlighted parts when using
    poorly-trained base models.'
  prefs: []
  type: TYPE_NORMAL
- en: One direction for future work will be to create a more intuitive highlighting
    scheme. We also aim to extend our method to support a greater variety of interactions.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We introduce Prompt Highlighter, a novel paradigm for user-model interactions
    in multi-modal LLMs, offering output control through a token-level highlighting
    mechanism. This approach, requiring no extra training, competes well on standard
    benchmarks and provides reliable generation outputs by merely highlighting input
    context. Further, diverse applications demonstrate its intuitive usability and
    effectiveness in enhancing control over the generation process. This work represents
    a promising direction for enhancing user control in multi-modal LLMs, and we anticipate
    it will inspire further research.
  prefs: []
  type: TYPE_NORMAL
- en: \etoctoccontentsline
  prefs: []
  type: TYPE_NORMAL
- en: partSupplementary Material
  prefs: []
  type: TYPE_NORMAL
- en: \cftsetindents
  prefs: []
  type: TYPE_NORMAL
- en: section0em1.8em \cftsetindentssubsection1em2.5em \cftsetindentssubsubsection3.0em3.5em
  prefs: []
  type: TYPE_NORMAL
- en: \localtableofcontents
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Experiment Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Workflow of the Prompt Highlighter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For a more comprehensive understanding of the method part of Prompt Highlighter,
    we provide pseudo-code algorithmic workflows in [Algorithm 1](#alg1 "In A.1 Workflow
    of the Prompt Highlighter ‣ Appendix A Experiment Details ‣ Prompt Highlighter:
    Interactive Control for Multi-Modal LLMs") and [Algorithm 2](#alg2 "In A.1 Workflow
    of the Prompt Highlighter ‣ Appendix A Experiment Details ‣ Prompt Highlighter:
    Interactive Control for Multi-Modal LLMs"). In this context, the attention activation
    outlined in [Algorithm 2](#alg2 "In A.1 Workflow of the Prompt Highlighter ‣ Appendix
    A Experiment Details ‣ Prompt Highlighter: Interactive Control for Multi-Modal
    LLMs") acts as the modified forward function in all Self-Attention layers during
    the multi-modal LLMs’ inference $\tilde{\mathbf{P}}_{\Theta}(x_{\eta}|\mathbf{s},\mathbf{m})$,
    as discussed in [Sec. 3.2](#S3.SS2 "3.2 Attention Activation ‣ 3 Prompt Highlighter
    ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs") and illustrated
    in [Algorithm 1](#alg1 "In A.1 Workflow of the Prompt Highlighter ‣ Appendix A
    Experiment Details ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 Highlighted Guidance Control
  prefs: []
  type: TYPE_NORMAL
- en: 0:Pre-trained LLM or VLM decoder $\mathbf{P}_{\Theta}$
  prefs: []
  type: TYPE_NORMAL
- en: '4:  while $x_{\eta}$ in[Sec. 3.1](#S3.Ex2 "3.1 Token-Level Highlight Guidance
    ‣ 3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control for Multi-Modal
    LLMs"):6: <math id="alg1.l6.m1.38" class="ltx_Math" alttext="\begin{multlined}x_{\eta+1}=\operatorname{argmax}(\gamma\log\bar{\mathbf{P}}_{\Theta}(x_{\eta+1}|\mathbf{s},\mathbf{m})\\'
  prefs: []
  type: TYPE_NORMAL
- en: -(\gamma-1)\log\bar{\mathbf{P}}_{\Theta}(\bar{x}_{\eta+1}|\mathbf{\bar{s}},-\delta\mathbf{m}))\end{multlined}x_{\eta+1}=\operatorname{argmax}(\gamma\log\bar{\mathbf{P}}_{\Theta}(x_{\eta+1}|\mathbf{s},\mathbf{m})\\
  prefs: []
  type: TYPE_NORMAL
- en: -(\gamma-1)\log\bar{\mathbf{P}}_{\Theta}(\bar{x}_{\eta+1}|\mathbf{\bar{s}},-\delta\mathbf{m}))$$7:     Update
    output sequence:8:     $\mathbf{y}\text{.append}(\mathbf{P}_{\Theta}\text{.tokenizer.decode}(x_{\eta+1}))$
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Attention Activation in Self-Attention Layer. For simplicity, we
    demonstrate with single-head attention.
  prefs: []
  type: TYPE_NORMAL
- en: 0:Input hidden state $\mathbf{q}=\{q_{1},\ldots,q_{\eta-1}\}$.
  prefs: []
  type: TYPE_NORMAL
- en: 1:  $(Q,K,V)=M_{\text{SA}}$)
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Quantitative Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.2.1 VLM Benchmarks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We use the same prompts and test scripts for our benchmark tests as those in
    the LLaVA-v1.5 codebase [[23](#bib.bib23)]. The only difference is our highlighting
    of the image context in the inputs (*i.e*., $m_{i}=1$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | MMBench-dev | MMBench-test |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| method | Overall | LR | AR | RR | FP-S | FP-C | CP | Overall | LR | AR |
    RR | FP-S | FP-C | CP |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| MMICL [[46](#bib.bib46)] | 67.9 | 49.2 | 71.6 | 73.0 | 66.7 | 57.2 | 77.2
    | 65.2 | 44.3 | 77.9 | 64.8 | 66.5 | 53.6 | 70.6 |'
  prefs: []
  type: TYPE_TB
- en: '| mPLUG-Owl2 [[14](#bib.bib14)] | 66.5 | 32.2 | 72.4 | 60.9 | 68.6 | 60.1 |
    79.4 | 66.0 | 43.4 | 76.0 | 62.1 | 68.6 | 55.9 | 73.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Sphinx [[47](#bib.bib47)] | 67.2 | 33.1 | 67.3 | 58.3 | 74.4 | 59.4 | 80.7
    | 67.5 | 32.9 | 73.6 | 57.8 | 72.1 | 63.2 | 79.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaVA-v1.5 [[23](#bib.bib23)] | 67.7 | 41.7 | 69.7 | 63.5 | 70.0 | 59.3 |
    80.2 | 67.0 | 39.9 | 74.7 | 61.6 | 70.9 | 59.9 | 75.4 |'
  prefs: []
  type: TYPE_TB
- en: '| + Prompt Highlighter | 69.7 | 44.2 | 70.6 | 68.7 | 73.7 | 59.3 | 80.9 | 69.5
    | 42.6 | 77.5 | 64.3 | 75.0 | 62.0 | 76.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Improvement | +2.0 | +2.5 | +0.9 | +5.2 | +3.7 | 0.0 | +0.7 | +2.5 | +2.7
    | +2.8 | +2.7 | +4.1 | +2.1 | +1.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Detailed comparison in MMBench-dev and MMBench-test[[26](#bib.bib26)].
    The categories include Logic Reasoning (LR), Attribute Reasoning (AR), Relation
    Reasoning (RR), Instance-Level Fine-Grained Perception (FP-S), Cross-Instance
    Fine-Grained Perception (FP-C), and Coarse Perception (CP). The improvement of
    our method over the LLaVA-v1.5 [[23](#bib.bib23)] baseline is reported in the
    last row for each category. We highlight the best and underline the second best
    result for each column.'
  prefs: []
  type: TYPE_NORMAL
- en: '| method | MME | MMB${}_{\texttt{dev}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaVA-v1.5 [[23](#bib.bib23)] | 1531.3 | 67.7 | 67.0 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PH (0.01, 3.0, 1.3) | 1537.3 | 69.7 | 68.7 |'
  prefs: []
  type: TYPE_TB
- en: '| PH (0.01, 2.0, 1.3) | 1552.5 | 68.7 | 69.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Hyper-parameter settings in VLM benchmarks. Evaluations are conducted
    on MME Perception [[27](#bib.bib27)] and MMBench  (MMB) [[26](#bib.bib26)]. We
    list a combination of hyper-parameters $(\alpha,\beta,\gamma)$ for our method (PH
    for Prompt Highlighter).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We adopt the MME and MMBench datasets because they offer comprehensive evaluations
    of VLMs across multiple dimensions and feature over 10K of VQA questions. This
    makes them ideal for assessing the overall competency of Vision-Language Models.
    We present a detailed evaluation result of MMBench in [Tab. 6](#A1.T6 "In A.2.1
    VLM Benchmarks ‣ A.2 Quantitative Evaluation ‣ Appendix A Experiment Details ‣
    Prompt Highlighter: Interactive Control for Multi-Modal LLMs"). With the same
    pre-trained weight as LLaVA-v1.5, our training-free method consistently outperforms
    the baseline across nearly all evaluation dimensions. Given that our approach
    is compatible with VLM frameworks that use token-level embedding input and a Transformer-based
    language decoder, it positions the Prompt Highlighter as a training-free plugin
    for integrating context-highlighting abilities into these models. We further evaluate
    the MMBench benchmark using the current state-of-the-art model, InternLM-VLComposer [[32](#bib.bib32)].
    Our method demonstrated a performance improvement, increasing the SOTA performance
    from 74.8 to 75.3 in the dev split with this representative Q-Former-based approach.
    These results suggest that though our method is designed to create new interactive
    ways instead of focusing on benchmark improvement, it still proves to be competitive
    in general applications.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.2 Reliable Descriptions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As discussed in [Sec. 4.4](#S4.SS4 "4.4 Ablation Study ‣ 4 Experiments ‣ Prompt
    Highlighter: Interactive Control for Multi-Modal LLMs"), we observe a trade-off
    between the attention concentration and QA performance that appeared in the benchmarks.
    Therefore, compared with experiments in VLM benchmarks, we adopt a relatively
    more aggressive parameter setting in the MSCOCO caption experiment: $(\alpha,\beta,\gamma)=(0.01,7.0,2.0)$,
    while it remains the same S-CLIP=0.808.. As CLIP has a maximum token length limitation
    of 77, we split captions at semicolons and periods and input the longest possible
    concatenated sequence to circumvent this limitation. For all text-to-image generation
    results in figures, we query the generated text description to the Bing Image
    Creator powered by DALLE-3 [[41](#bib.bib41)] [[www.bing.com/images/create](www.bing.com/images/create)].'
  prefs: []
  type: TYPE_NORMAL
- en: A.2.3 User Study
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We present the detailed results of the user study in [Tab. 8](#A1.T8 "In A.2.3
    User Study ‣ A.2 Quantitative Evaluation ‣ Appendix A Experiment Details ‣ Prompt
    Highlighter: Interactive Control for Multi-Modal LLMs"). Using different sub-task
    examples, we engaged various baseline models for different tasks to demonstrate
    the consistent preference for Prompt Highlighter across a broad array of frameworks
    and tasks. The user preference results affirm that our method delivers superior
    performance and flexibility, consistently outperforming the baseline models across
    a wide range of tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| sub-task | baseline | preference |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Text Partial Highlighter | Vicuna-13B | 70.6% |'
  prefs: []
  type: TYPE_TB
- en: '| Image Partial Highlighter | LLaVA | 74.5% |'
  prefs: []
  type: TYPE_TB
- en: '| Image Understanding | InstructBLIP | 76.5% |'
  prefs: []
  type: TYPE_TB
- en: '| Image Caption | LLaVA-v1.5 | 80.4% |'
  prefs: []
  type: TYPE_TB
- en: '| Description $\rightarrow$ Image | LLaVA-v1.5 + DALLE-3 | 84.3% |'
  prefs: []
  type: TYPE_TB
- en: '| Overall | - | 77.3% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: A fine-grained user study result.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/022807eeb1cb405e7dd91fa33c25c687.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: An additional example of attention maps across different layer segments (left)
    and the averaged visualization with more generated tokens (right). We mark tokens
    representing punctuation, which draw significant attention in the initial layers.
    Beyond these tokens, a consistent correlation between tokens forms a band-like
    pattern that can be observed across all layers.'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Attention Map Visualization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we delve into the attention map visualization, expanding on
    the verification experiments discussed in [Sec. 4.5](#S4.SS5 "4.5 Discussions
    ‣ 4 Experiments ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs").
    In [Fig. 10](#A1.F10 "In A.2.3 User Study ‣ A.2 Quantitative Evaluation ‣ Appendix
    A Experiment Details ‣ Prompt Highlighter: Interactive Control for Multi-Modal
    LLMs"), we illustrate an example akin to [Fig. 3](#S3.F3 "In 3.1 Token-Level Highlight
    Guidance ‣ 3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control for
    Multi-Modal LLMs"), demonstrating the persistence of the band-like pattern in
    different layer segments in the Vicuna-13B model [[8](#bib.bib8)]. The band-like
    property is clear and consistently evident across various layers. Given that these
    band-like token activation patterns are distributed across different layers, we
    simply incorporate the attention activation modification in all self-attention
    layers. Furthermore, the attention maps referenced in [Fig. 9](#S4.F9 "In Attention
    activation. ‣ 4.5 Discussions ‣ 4 Experiments ‣ Prompt Highlighter: Interactive
    Control for Multi-Modal LLMs") to calculate gradients and attention contributions
    are computed based on the averaged attention map across layers and heads.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ab19c318e081009c929936ab994987c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: An example illustrating the influence of different base models.
    Given the same highlighted context, the output of the model may be constrained
    by the capabilities of the base model.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Limitation Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A.4.1 Speed and Memory
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As discussed in [Sec. 4.5](#S4.SS5 "4.5 Discussions ‣ 4 Experiments ‣ Prompt
    Highlighter: Interactive Control for Multi-Modal LLMs"), Prompt Highlighter imposes
    an additional computational load on the unconditional branch, and we validate
    its tangible impact in [Tab. 5](#S4.T5 "In Hyper-parameters. ‣ 4.4 Ablation Study
    ‣ 4 Experiments ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs").
    Memory consumption with 8-bit loading confirms that the Prompt Highlighter is
    designed to be memory-efficient, making it feasible to run inference on a 13B
    model using commonly available GPUs, such as the NVIDIA-3090\. This added load
    becomes noticeable when evaluating single-token prediction tasks, as the model
    is required to compute double KV values among tokens in its initial inference
    step. However, when employing KV-cache in the multiple-token generation, this
    extra load can be alleviated, as the doubled calculation with cached KVs does
    not become the bottleneck for inference speed or GPU memory consumption. This
    results our method only has a 10.5% overall additional time and 7% additional
    GPU memory. We also confirm that the commonly utilized inference strategy of introducing
    an extra beam search branch does not impair the quality of the descriptive task (S-CLIP
    0.809$\rightarrow$0.807 in [Tab. 5](#S4.T5 "In Hyper-parameters. ‣ 4.4 Ablation
    Study ‣ 4 Experiments ‣ Prompt Highlighter: Interactive Control for Multi-Modal
    LLMs")), despite imposing an additional computational load. This also illustrates
    the practical effectiveness of Prompt Highlighter.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4.2 Constrained by the Base-Model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Our method stems from the base model and its plug-and-play nature. As illustrated
    in [Fig. 11](#A1.F11 "In A.3 Attention Map Visualization ‣ Appendix A Experiment
    Details ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"), the
    base model InstructBLIP [[15](#bib.bib15)] is not adept at dialog generation.
    Consequently, with a highlighted prompt context, despite the output text being
    more related to the highlighted part, it fails to generate a dialogue effectively.
    This suggests that while the Prompt Highlighter can augment existing abilities
    in pre-trained multi-modal LLMs, it cannot endow them with new capabilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Disccusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3616c34ba8bf47b8a80aec91d235d541.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: One example of the comparison with LLM-CFG [[21](#bib.bib21)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/038bf86366f557e8021bcb47e0989cb1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Output example for the unconditional branch with different unconditional
    context designs.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/01774f483591e849ce111e7f5ed4c82a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: A comparison of Prompt Highlighter with different vision-language
    prompt manipulation approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f08073b87b92d4b4064b964353cb8dd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Two more examples in pure text partial highlighter. A Vicuna-13B-v1.1 [[8](#bib.bib8)]
    is used as the base model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/503625423923c1472947fe52887b48dc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: More results with multi-modal inputs. We demonstrate three applications (1,2,3,5)
    and one hard case (4) for image understanding. We use InstructBLIP-Vicuna-13B [[15](#bib.bib15)]
    as the base model of cases 1,2,5 and LLaVA-v1.5 [[23](#bib.bib23)] 13B as the
    base model of cases 3,4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b0865574ca68fdbe766f53aeee0690ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: More text-to-image examples with image generation by DALLE-3 [[41](#bib.bib41)].
    We have marked areas in some figures that generate better results due to different
    captions.'
  prefs: []
  type: TYPE_NORMAL
- en: B.1 Compare with LLM-CFG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The comparison between Prompt Highlighter and LLM-CFG [[21](#bib.bib21)] is
    visually demonstrated in [Fig. 12](#A2.F12 "In Appendix B Additional Disccusions
    ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"). Our method stands
    out by offering a more granular control mechanism that allows specific tokens
    within the context to be highlighted. Conversely, LLM-CFG, with its basic prompt-level
    differentiation and without a mechanism to directly manipulate model feature interactions
    (*e.g*., our attention activation strategy), often generates outputs that bear
    noticeable similarity to the original, regardless of substantial $\gamma$ increases.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the identical prompt as in [Fig. 12](#A2.F12 "In Appendix B Additional
    Disccusions ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"),
    we consolidate this claim by exhibiting the independent inference outcomes of
    the unconditional branch in [Fig. 13](#A2.F13 "In Appendix B Additional Disccusions
    ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"). In addition,
    we demonstrate the significance of assigning the $\alpha$ in the embedding space
    leaves the overall contextual understanding of the unconditional branch largely
    unaffected. Such a fine-grained difference assists in making more distinguishing
    token choices during the inference process in [Sec. 3.1](#S3.Ex2 "3.1 Token-Level
    Highlight Guidance ‣ 3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control
    for Multi-Modal LLMs"). In contrast, LLM-CFG’s unconditional branch omits part
    of the prompt, resulting in an obstacle to generating results that effectively
    underscore the highlighted sections.'
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Compare with other Highlight Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate the non-trivial nature of our method, we evaluated it alongside
    several intuitive prompt engineering approaches, which include adding adjectives,
    capitalizing text, and explicitly marking in images. Comparative examples in LLM
    and VLM are provided in Prompt Highlighter: Interactive Control for Multi-Modal
    LLMs and [14](#A2.F14 "Figure 14 ‣ Appendix B Additional Disccusions ‣ Prompt
    Highlighter: Interactive Control for Multi-Modal LLMs"), respectively. Our observations
    suggest that the employment of additional prompts or explicit emphasis can occasionally
    lead to unpredictable results. This is primarily due to the introduction of extra
    information, while models may not truly emphasize correlated parts. In contrast,
    our method seamlessly accommodates all context information and produces focused
    text outputs. This demonstrates that the design of our method not only offers
    a high degree of flexibility but also enables the generation of more contextually
    appropriate outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Showcases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: C.1 More Visual Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Additional results of Prompt Highlighter in LLM and VLM are provided in [Fig. 15](#A2.F15
    "In Appendix B Additional Disccusions ‣ Prompt Highlighter: Interactive Control
    for Multi-Modal LLMs") and [Fig. 16](#A2.F16 "In Appendix B Additional Disccusions
    ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first example of [Fig. 15](#A2.F15 "In Appendix B Additional Disccusions
    ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs"), we showcase
    the primary part of the output of the example in [Fig. 6](#S3.F6 "In 3.3 Highlighting
    Visual Tokens ‣ 3 Prompt Highlighter ‣ Prompt Highlighter: Interactive Control
    for Multi-Modal LLMs"). Our results concentrate more on the highlighted section,
    offering professional reference books to support user learning “computer vision”.
    The second example exhibits a case for a multi-segment mask input, where the model
    is steered to output code snippets at the appropriate positions by highlighting
    the requirement part of the text and the comment as the position identifier. [Fig. 16](#A2.F16
    "In Appendix B Additional Disccusions ‣ Prompt Highlighter: Interactive Control
    for Multi-Modal LLMs") demonstrates a range of applications and capabilities of
    Prompt Highlighter in VLM from various perspectives, including image content comprehension
    (case 1), focused generation (case 2), improved image descriptions (case 3), and
    explicit generation control (case 5). These instances underscore the performance
    enhancement and customizable generation control facilitated by Prompt Highlighter.
    We also present some challenging cases due to the limited perceptual capacity
    of the base model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we present results of text-to-image generation based on the text
    descriptions generated by our method in [Fig. 17](#A2.F17 "In Appendix B Additional
    Disccusions ‣ Prompt Highlighter: Interactive Control for Multi-Modal LLMs").
    These outcomes vouch for the superior capability of our approach in descriptive
    tasks and its proficiency in generating higher-quality image-text matching data.'
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Multi-Round Interactive Conversation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Up to this point, the examples and experimental scenarios we’ve showcased are
    for single-round dialogues or QAs. As a general-purpose interactive inference
    pipeline, our method can also support multi-round interactions and conversations.
    We provide a schematic diagram of multi-round conversation at the top of [Fig. 18](#A3.F18
    "In C.2 Multi-Round Interactive Conversation ‣ Appendix C Showcases ‣ Prompt Highlighter:
    Interactive Control for Multi-Modal LLMs"). The multi-round conversations’ generation
    can be decomposed into single-round generations with continuously growing context,
    allowing Prompt Highlighter to change the focus of user input in each round by
    updating the input mask $\mathbf{m}$ and resetting previous KV caches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below the pipeline, we demonstrate how to resolve the image understanding problem,
    presented in [Fig. 16](#A2.F16 "In Appendix B Additional Disccusions ‣ Prompt
    Highlighter: Interactive Control for Multi-Modal LLMs") case 4, through multi-round
    conversation and human interactions. By highlighting multi-modal tokens and guiding
    the model to understand the image content in a segmented manner, users can “teach”
    the model to solve problems that it was previously unable to solve.'
  prefs: []
  type: TYPE_NORMAL
- en: Figures 15-18 are presented on the following pages $\downarrow$
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c074cc73dc5cc90fe1be575fd5cf3109.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Multi-round interactive conversation powered by Prompt Highlighter.
    We illustrate the multi-round interactive conversation pipeline on the top. We
    provide a comparison in a multi-round conversation between the vanilla inference (left)
    and Prompt Highlighter (right). In this example, the user highlights different
    contexts in two rounds.'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al.
    Improving language understanding by generative pre-training. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,
    et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. NeurIPS, 33:1877–1901, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
    Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly
    optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
    pre-training for natural language generation, translation, and comprehension.
    arXiv preprint arXiv:1910.13461, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna:
    An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.
    lmsys. org (accessed 14 April 2023), 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,
    Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned
    language models. arXiv preprint arXiv:2210.11416, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction
    tuning. arXiv preprint arXiv:2304.08485, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4:
    Enhancing vision-language understanding with advanced large language models. arXiv
    preprint arXiv:2304.10592, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping
    language-image pre-training for unified vision-language understanding and generation.
    In ICML, pages 12888–12900\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping
    language-image pre-training with frozen image encoders and large language models.
    arXiv preprint arXiv:2301.12597, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
    Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers
    large language models with multimodality. arXiv preprint arXiv:2304.14178, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao,
    Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards
    general-purpose vision-language models with instruction tuning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths,
    Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving
    with large language models. arXiv preprint arXiv:2305.10601, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,
    Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in
    large language models. NeurIPS, 35:24824–24837, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Significant-Gravitas. Significant-gravitas/autogpt: An experimental open-source
    attempt to make gpt-4 fully autonomous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang,
    Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought
    reasoning in language models. arXiv preprint arXiv:2203.11171, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv
    preprint arXiv:2207.12598, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Guillaume Sanchez, Honglu Fan, Alexander Spangher, Elad Levi, Pawan Sasanka
    Ammanamanchi, and Stella Biderman. Stay on topic with classifier-free guidance.
    arXiv preprint arXiv:2306.17806, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Simon Kornblith, Lala Li, Zirui Wang, and Thao Nguyen. Guiding image captioning
    models toward more specific captions. In ICCV, pages 15259–15269, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines
    with visual instruction tuning. arXiv preprint arXiv:2310.03744, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell Nye, Augustus Odena,
    Arushi Somani, and Sağnak Taşırlar. Introducing our multimodal models, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona,
    Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects
    in context. In ECCV, pages 740–755\. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao,
    Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal
    model an all-around player? arXiv preprint arXiv:2307.06281, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin,
    Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. Mme: A comprehensive evaluation
    benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav
    Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
    Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
    Learning transferable visual models from natural language supervision. In ICML,
    pages 8748–8763\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming
    Ma, and Furu Wei. Kosmos-2: Grounding multimodal large language models to the
    world. arXiv preprint arXiv:2306.14824, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Pan Zhang, Xiaoyi Dong, Bin Wang, Yuhang Cao, Chao Xu, Linke Ouyang, Zhiyuan
    Zhao, Shuangrui Ding, Songyang Zhang, Haodong Duan, Wenwei Zhang, Hang Yan, Xinyue
    Zhang, Wei Li, Jingwen Li, Kai Chen, Conghui He, Xingcheng Zhang, Yu Qiao, Dahua
    Lin, and Jiaqi Wang. Internlm-xcomposer: A vision-language large model for advanced
    text-image comprehension and composition, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi,
    Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk,
    et al. Graph of thoughts: Solving elaborate problems with large language models.
    arXiv preprint arXiv:2308.09687, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Yuxi Xie, Kenji Kawaguchi, Yiran Zhao, Xu Zhao, Min-Yen Kan, Junxian He,
    and Qizhe Xie. Decomposition enhances reasoning via self-evaluation guided decoding.
    arXiv preprint arXiv:2305.00633, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Wei-Ge Chen, Irina Spiridonova, Jianwei Yang, Jianfeng Gao, and Chunyuan
    Li. Llava-interactive: An all-in-one demo for image chat, segmentation, generation
    and editing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and
    Jiaya Jia. Lisa: Reasoning segmentation via large language model. arXiv preprint
    arXiv:2308.00692, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Songwei Ge, Taesung Park, Jun-Yan Zhu, and Jia-Bin Huang. Expressive text-to-image
    generation with rich text. In ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or.
    Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion
    models. arXiv preprint arXiv:2301.13826, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on
    image synthesis. NeurIPS, 34:8780–8794, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks. arXiv, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie
    Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla
    Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation
    with better captions. Technical report, OpenAI, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang,
    Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language
    model with versatile abilities. arXiv preprint arXiv:2308.12966, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Noam Rotstein, David Bensaid, Shaked Brody, Roy Ganz, and Ron Kimmel.
    Fusecap: Leveraging large language models to fuse visual data into enriched image
    captions. arXiv preprint arXiv:2305.17718, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.
    Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint
    arXiv:2104.08718, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-world image variation
    by aligning diffusion inversion chain. In NeurIPS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai An, Liang Chen,
    Zixuan Liu, Sheng Wang, Wenjuan Han, and Baobao Chang. Mmicl: Empowering vision-language
    model with multi-modal in-context learning, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Ziyi Lin, Chris Liu, Renrui Zhang, Peng Gao, Longtian Qiu, Han Xiao, Han
    Qiu, Chen Lin, Wenqi Shao, Keqin Chen, Jiaming Han, Siyuan Huang, Yichi Zhang,
    Xuming He, Hongsheng Li, and Yu Qiao. Sphinx: The joint mixing of weights, tasks,
    and visual embeddings for multi-modal large language models. [https://github.com/Alpha-VLLM/LLaMA2-Accessory/blob/main/SPHINX/SPHINX_paper.pdf](https://github.com/Alpha-VLLM/LLaMA2-Accessory/blob/main/SPHINX/SPHINX_paper.pdf),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:
    Transformers for image recognition at scale. In ICLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
