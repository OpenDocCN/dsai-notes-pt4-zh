- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:46:09'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of
    Scholarly Manuscripts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15589](https://ar5iv.labs.arxiv.org/html/2402.15589)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Shubhra Kanti Karmaker Santu, Sanjeev Kumar Sinha, Naman Bansal,
  prefs: []
  type: TYPE_NORMAL
- en: Alex Knipper, Souvika Sarkar, John Salvador, Yash Mahajan,
  prefs: []
  type: TYPE_NORMAL
- en: Sri Guttikonda, Mousumi Akter, Matthew Freestone, Matthew C. Williams Jr. Big
    Data & Intelligence (BDI) Lab, Auburn University, Alabama, USA
  prefs: []
  type: TYPE_NORMAL
- en: 'Email Contact: sks0086@auburn.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One of the most important yet onerous tasks in the academic peer-reviewing process
    is composing meta-reviews, which involves understanding the core contributions,
    strengths, and weaknesses of a scholarly manuscript based on peer-review narratives
    from multiple experts and then summarizing those multiple experts’ perspectives
    into a concise holistic overview. Given the latest major developments in generative
    AI, especially Large Language Models (LLMs), it is very compelling to rigorously
    study the utility of LLMs in generating such meta-reviews in an academic peer-review
    setting. In this paper, we perform a case study with three popular LLMs, i.e.,
    GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting
    them with different types/levels of prompts based on the recently proposed TELeR
    taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews
    generated by the LLMs and summarize our findings and recommendations for prompting
    LLMs for this complex task.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As part of the scientific peer-review process for publishing scholarly work,
    meta-reviewing is a critical step where the editor/area chair integrates and summarizes
    the opinions of multiple expert reviewers regarding the quality of the scholarly
    manuscript. Indeed, meta-reviewing is important in understanding the consensus
    of expert opinions on the manuscript under review and making informed judgments
    on its scientific merit. Additionally, meta-reviews are also helpful for concisely
    summarizing the strengths and weaknesses of the manuscript and the scope for further
    improvement. Usually, composing a meta-review from peer reviewers’ comments includes
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect comments and opinions from fellow experts in the same/similar research
    domain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyze the comments and opinions from different reviewers and identify the
    common strengths, weaknesses, and suggestions for improvement emphasized by the
    peer reviewers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Summarize the strengths, weaknesses, and suggestions into a concise holistic
    overview.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The process of meta-reviewing is arduous because it involves assimilating all
    the comments and opinions of different reviewers regarding a submitted manuscript
    and making concrete recommendations for the final decision on acceptance. This
    process demands focus, concentration, and, most importantly, a lot of time from
    the program chairs/area chairs/editors, who are usually highly experienced researchers
    in their respective fields and are hard-pressed for time. In addition, over the
    years, the number of research manuscript submissions has been rising exponentially Fire
    and Guestrin ([2019](#bib.bib10)). In any peer-reviewed journal or conference,
    each submitted paper requires going through multiple expert reviews followed by
    an overall meta-review and recommendation step. As such, the volume of meta-reviewing
    tasks is also rising exponentially, making the job of meta-reviewers even more
    difficult and time-consuming. Since human beings are prone to fatigue and distraction,
    it is possible that this large workload may make meta-reviewers susceptible to
    inconsistency and missing details. As such, the motivation behind automatically
    generating a draft meta-review is to help meta-reviewers be more consistent and
    efficient in this voluntary but very important role.
  prefs: []
  type: TYPE_NORMAL
- en: Realizing that the meta-review composition is essentially a summarization task Santu
    et al. ([2018](#bib.bib15)); Bansal et al. ([2022c](#bib.bib6), [a](#bib.bib4)),
    which in turn is a classic sequence-to-sequence problem, LLMs like GPT-3.5 Brown
    et al. ([2020](#bib.bib7)), PaLM2 Chowdhery et al. ([2023](#bib.bib8)), LLaMA2 Touvron
    et al. ([2023b](#bib.bib21)), etc., appear to be natural choices to automate this
    task. Indeed, recent LLMs have demonstrated superiority in producing high-quality
    human-like summaries in a conversational setting Yang et al. ([2023](#bib.bib22)).
    Being motivated by this, we performed a case study in this paper with three popular
    LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2 to compose meta-reviews.
  prefs: []
  type: TYPE_NORMAL
- en: While LLMs have shown great success in understanding and summarizing text in
    conversational settings, researchers have reported large variations in LLMs’ performance
    when different prompt types/styles are used, and different degrees of detail are
    provided in the prompts. To address this issue, Santu and Feng ([2023](#bib.bib14))
    recently proposed a general taxonomy called TELeR that can be used to design diverse
    prompts with specific properties to perform a wide range of complex tasks. This
    taxonomy allows any LLM-based benchmarking study to report the specific categories/types
    of prompts (based on Turn, Expression style, Level of Detail, and Role) used in
    the study, enabling meaningful comparisons across different studies. Therefore,
    in this paper, we adopted the TeLER taxonomy to systematically study the performance
    of different LLMs for the meta-review composition task.
  prefs: []
  type: TYPE_NORMAL
- en: For evaluation, we selected 40 research papers submitted to the ICLR Conference
    in recent years, along with peer-reviewer comments and a handcrafted meta-review
    written by a highly experienced researcher for each article. These manuscripts
    and the associated peer-review comments and meta-reviews are all publicly available
    on OpenReview.net. Using this data set in combination with the three LLMs (GPT-3.5,
    PaLM2, LLaMA2) and the TELeR taxonomy, we performed an extensive qualitative analysis
    by engaging human evaluators to go through the meticulous process of reading and
    analyzing $40$ LLM level judgments from humans. Our qualitative analysis reveals
    that in terms of the quality of the generated meta-reviews, GPT-3.5 and PaLM2s
    performed comparably overall and were rated higher by humans than LLaMA2 in terms
    of manuscript-level judgments. Interestingly, PaLM2 yielded better recall scores
    in general, while GPT-3.5 yielded better precision scores. Finally and most surprisingly,
    GPT-3.5 was rated poorly in terms of LLM-level judgments, a conundrum that demands
    further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Automatic meta-review composition is essentially a multi-document summarization
    task with very specific constraints. Recent developments with Large Language Models
    (LLMs), including but not limited to GPT-3.5 and GPT-4 Brown et al. ([2020](#bib.bib7));
    Ouyang et al. ([2022](#bib.bib13)), PaLM Thoppilan et al. ([2022](#bib.bib19)),
    LLaMA Touvron et al. ([2023a](#bib.bib20)), Bloom Scao et al. ([2022](#bib.bib18)),
    GLaM Du et al. ([2022](#bib.bib9)), have shown remarkable performance in simple
    summary generation tasks. Yet, their performance on complex, i.e., multi-constraint
    multi-document summarization tasks Bansal et al. ([2022b](#bib.bib5)) like meta-review
    composition is still understudied.
  prefs: []
  type: TYPE_NORMAL
- en: 'Naturally, the more complex a task is, the more variations of prompts one can
    try to accomplish the same task using an LLM. Unfortunately, LLMs are quite sensitive
    to prompts that are fed to them, and a large number of studies have indeed reported
    that the quality and effectiveness of the prompt can greatly influence the performance
    of the LLMs for a particular task, and therefore, designing appropriate prompts
    with the right amount of detail has become more important than ever Liu et al.
    ([2023](#bib.bib12)); Han et al. ([2022](#bib.bib11)). This issue gets even worse
    in practice as there is no standard taxonomy of prompts to follow for benchmarking
    LLMs on complex tasks that the community has a general consensus on yet. As such,
    the main motivation of this work, which makes our case study more compelling,
    stems from the following two gaps in the literature: 1) LLM’s performance on multi-constraint
    multi-document summarization tasks (e.g., meta-review composition) is still understudied,
    2) the fact that no study has yet followed a standard promoting taxonomy to systematically
    compare LLMs on such complex tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f1a04086b864a1738c589f5912702f5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Fig. 1: TELeR Taxonomy for prompting LLMs to perform complex tasks. For details,
    see Santu and Feng ([2023](#bib.bib14)).'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, very recently, Santu and Feng ([2023](#bib.bib14)) have proposed
    a general prompting taxonomy called TELeR, which can serve as a unified standard
    for comparing and benchmarking LLMs’ performances on complex generation tasks.
    In this work, we leveraged this TELeR taxonomy for designing prompts in the context
    of a meta-review composition task and reported the specific categories of prompts
    we experimented with across multiple LLMs. This enabled more meaningful comparisons
    among the three LLMs we studied and, thereby, helped to derive more accurate conclusions
    about their relative performance on the meta-review composition task.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Background on TELeR Taxonomy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As shown in Figure [1](#S2.F1 "Fig. 1 ‣ 2 Related Work ‣ Prompting LLMs to Compose
    Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts"), the
    taxonomy introduced by Santu and Feng ([2023](#bib.bib14)) categorizes complex
    task prompts based on the following four criteria.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Turn: This refers to the number of turns or shots used while prompting an LLM
    to accomplish a complex task. In general, prompts can be classified as either
    single or multi-turn.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Expression: This refers to the style of expression for interacting with the
    LLM, such as questioning or instructing.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Level of Details: This dimension of prompt style deals with the granularity
    or depth of question or instruction. Prompts with higher levels of detail provide
    more granular instructions. For example, Level 0 has no directive, i.e., questions
    or instructions. On the other hand, Level 4 prompts have to specify the particular
    sub-tasks, few-shot examples, and/or the basis for evaluation of the output generated
    by the LLM.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Role: Some LLMs provide users with the option of specifying the role of the
    system. The response of LLM can vary due to changes in role definitions in spite
    of the fact that the prompt content remains unchanged.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4 Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section describes the details of our case study, including the data set,
    LLMs, evaluation criteria, and prompting details.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Data-set
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For evaluation, we carefully selected 40 research papers and their associated
    peer-review comments and meta-reviews from the collection of more than $13,800$
    research manuscripts submitted to the ICLR conference during a span of four years,
    i.e., the year 2020 through 2023\. As ICLR submissions are publicly hosted on
    OpenReview.net, one can easily collect these scholarly manuscripts along with
    peer-reviewer narratives and a handcrafted meta-review written by a highly experienced
    researcher for each manuscript. While selecting papers for our evaluation, we
    followed some general principles as mentioned below.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Meta-review is detailed and substantially encompasses the core contributions,
    strengths, weaknesses, and scopes for improvements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manuscript received at least 3 detailed reviews that comment on different aspects,
    e.g., core contributions, strengths, weaknesses, suggestions for improvement,
    and literature review quality.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Preference was given to include more recent papers, especially those submitted
    during 2021-2023 dealing with diverse topics.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The corpus also includes 10 rejected papers (25%) to increase meta-review diversity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.2 Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned previously, our case study evaluates 3 recent popular LLMs - GPT-3.5 Brown
    et al. ([2020](#bib.bib7)), PaLM2 Chowdhery et al. ([2023](#bib.bib8)), LLaMA2 Touvron
    et al. ([2023b](#bib.bib21)). Prompting of GPT-3.5 and LLaMA2 was performed through
    their API, but interaction with Google’s PaLM2 was done manually through a web
    browser because PaLM2 does not provide an API yet. As Zero-shot learning techniques
    have become very popular in recent years Sarkar et al. ([2023](#bib.bib17), [2022](#bib.bib16)),
    we use all LLMs in the Zero-shot setting for prompting without further fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Prompt Design using TELeR Taxonomy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We prompted LLMs with different types/levels of prompts designed based on the
    recently proposed TELeR taxonomy (described in section [3](#S3 "3 Background on
    TELeR Taxonomy ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review
    Narratives of Scholarly Manuscripts")). To be more specific, let us look at what
    the TELeR levels meant for our meta-review composition task. In all the following
    examples, we assume that a prompt is a concatenation of directive text (instruction)
    followed by the peer review texts.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Level 1: This high-level directive asks the LLMs to simply generate a meta-review
    without providing any further specific requirements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Level 2: At level 2, directives include a paragraph with multiple sentences
    describing instructions/questions articulating multiple sub-tasks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Level 3: Directives are a bulleted list in this level, where each bullet describes
    a particular sub-task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Level 4: This level is similar to level 3\. The only difference is that the
    prompts at this level also ask LLMs to explain their own outputs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As part of the meta-review generation task, LLMs were asked to perform the following
    sub-tasks for prompts at level 2 and above.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the summary of core contributions? Provide an answer with supporting
    evidence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which common strengths are referred to in the reviews? A common strength is
    a strength that is mentioned in at least two reviews.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What common weaknesses are described in the reviews? A common weakness is a
    weakness that is mentioned in at least two reviews.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What suggestions for improvement have been provided by three reviews? A common
    suggestion for improvement is a suggestion that is mentioned in at least two reviews.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do the reviews mention about missing references? A list of missing references
    is optional.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For further details, the exact prompts used in our case study are shown in the
    appendix (see Table [3](#A1.T3 "Table 3 ‣ A.2 Prompt Design Details ‣ Appendix
    A Appendix ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts")).
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Evaluation Criteria and Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Quantitative evaluation of any Natural Language Generation (NLG) task is hard.
    It has been reported in the literature that popular NLG evaluation metrics like
    ROUGE, and BertScore do not correlate well with human judgments and can lead to
    misleading conclusions Akter and Santu ([2023b](#bib.bib3), [a](#bib.bib2)); Akter
    et al. ([2022](#bib.bib1)); Bansal et al. ([2022b](#bib.bib5)). Therefore, we
    chose to restrict to a meticulous qualitative evaluation in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'For qualitative evaluation, $10$ different levels. Each annotator evaluated
    the LLM-generated meta-reviews in light of publicly available reviews posted by
    the peer-reviewers corresponding to a particular manuscript. It was not disclosed
    to them which LLM they were evaluating to preclude any psychological bias in the
    evaluation. All of them were briefed in detail about the evaluation task. Two
    separate evaluation forms were designed to collect their judgments:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Micro-Evaluation: The first set of human evaluations captured their judgment
    on the quality of the meta-review generated by each LLM for each prompt. For a
    given prompt and a corresponding output from a particular LLM $x$, each human
    evaluator was asked to judge the generated output based on five different criteria
    as described below. For each criterion, the human evaluators were presented with
    an assertive statement about that criterion and then asked to provide their judgments
    by choosing their level of agreement with the given statement in terms of five
    different labels: 1) Strongly agree, 2) Agree, 3) Neutral, 4) Disagree, and 5)
    Strongly disagree. The five criteria are described below.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Core Contribution: To assess whether LLMs performed a good job in summarizing
    the core contribution of the research manuscript, human evaluators were asked
    to rate their level of agreement with the following statement: “While generating
    meta-review, LLM $x$ performed very well in highlighting the core contributions”.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Common Strengths: This judgment is based on how well LLMs summarize the common
    strengths mentioned in multiple reviewer comments/narratives. Evaluators were
    asked to rate their level of agreement with the following statement: LLM $x$ included
    all the strengths that are common across reviews.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Common Weaknesses: This judgment is similar to Common strengths, except this
    time, the judgment was focused on the weaknesses. The statement provided was the
    following: LLMx included all the weaknesses that are common in multiple reviews.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (d)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Common Tips for Improvement: This judgment is based on the extent all the common
    suggestions for improvement are included in the generated meta-review. For human
    assessment, the following statement was provided: LLMx included all the common
    suggestions for improvement mentioned in the reviews.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (e)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Literature Review: This annotation examines whether LLM-generated meta-reviews
    included concerns about missing references if multiple reviewers mentioned them.
    For evaluation, humans rated their agreement with the following: LLMx included
    all the missing references mentioned in the reviews.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Macro-Evaluation: The second set of evaluations was designed to capture human
    judgments regarding the overall performance of a particular LLM on the meta-review
    generation task. We call it the “Macro-Evaluation”. This form requires two types
    of responses - the first type has three assertive statements concerning LLM’s
    performance in terms of adherence to instructions, ability to create useful meta-reviews,
    and matching against actual expert-written meta-reviews. Again, evaluators were
    asked to rate their level of agreement on a five-point scale - 1) Strongly agree,
    2) Agree, 3) Neutral, 4) Disagree, and 5) Strongly disagree, with respect to the
    following 3 Macro-Evaluation statements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (a)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Macro-Evaluation Statement 1: “While generating a meta-review, LLMx strictly
    followed the instructions of the prompt”.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (b)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Macro-Evaluation Statement 2: “LLMx is able to create useful meta-review”.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: (c)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Macro-Evaluation Statement 3: “Meta review generated by LLMx matches with the
    actual meta-review to a great extent”.'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: The second type of feedback is an essay question - “Kindly provide comments
    about LLMx’s performance in generating Meta-review.” where $x$ represents a particular
    LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.5 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.5.1 Results from Micro-Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For the micro-evaluation experiments, we present the findings of our case study
    by answering the following research questions related to different components
    of the LLM-generated meta-review:'
  prefs: []
  type: TYPE_NORMAL
- en: “While generating a meta-review for a scholarly work based on the consensus
    among multiple reviewers’ narratives, can LLMs properly capture-”
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RQ Mic-1: the core contributions of the work?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RQ Mic-2: the major strengths of the work?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RQ Mic-3: the major weaknesses of the work?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RQ Mic-4: the avenues for improving the work?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RQ Mic-5: how well the manuscript conducted review of relevant literature?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Below, we answer each Micro-Evaluation research question based on the human
    ratings of $480$ different LLM-generated meta-reviews.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0f002b848d44a7bdac60591a8c14ac83.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Core Contributions - Prompt Level 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3db09a8d5cd13fa296506f9d717d7a55.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Core Contributions - Prompt TELeR Level 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1ce8eb0e214e900c0011b35b4b734dc5.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Core Contributions - Prompt TELeR Level 3
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f49a323b4aacf4e85635592960fa2e22.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Core Contributions - Prompt TELeR Level 4
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 2: Core Contributions Ratings - Prompt TELeR Level 1-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ce184f46518326cd1df707d9df10136.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Core Contributions - Aggregated over all Prompt Levels
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4beee7f55709fc9739f4714d44c5258a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Core Contributions - Aggregation over LLMs
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 3: Core Contributions Ratings - Aggregated separately across different
    Prompt Levels and different LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ Mic-1 (Core Contributions): We answer this question by analyzing Figure [2](#S4.F2
    "Fig. 2 ‣ 4.5.1 Results from Micro-Evaluation ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting
    LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts"),
    which depicts the comparative performances of three LLMs in terms of their (human-perceived)
    precision/recall distributions, where human-perceived precision and recall distributions
    are derived from their qualitative rating counts, i.e., {SA=Strongly Agree, A=Agree,
    N=Neutral, D=Disagree, SD=Strongly Disagree}, followed by a normalization term
    to convert them into a valid distribution (individual values ranging between [0-1]).
    To be more specific, while computing the precision/recall distributions corresponding
    to RQ-Mic-1, human annotators independently rated their agreement with the following
    two statements.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ Mic-1-P: Core Contributions (Precision): “While generating meta-review,
    LLM $x$ was precise in capturing the core contributions as highlighted by at least
    two reviewers.”'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: –
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ Mic-1-R: Core Contributions (Recall): “While generating meta-review, LLM
    $x$ indeed covered all the core contributions highlighted by at least two reviewers.”'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure [2](#S4.F2 "Fig. 2 ‣ 4.5.1 Results from Micro-Evaluation ‣ 4.5 Results
    ‣ 4 Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review
    Narratives of Scholarly Manuscripts") shows that PaLM2 was rated the highest for
    prompt level 1 (minimum details), while GPT-3.5 was rated the best in the case
    of levels 2 - 4 (more details) for both precision and recall. This suggests that
    GPT-3.5 could understand complex prompts better than PaLM2 and LLaMA2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we looked at precision and recall distributions for each LLM by aggregating
    over all 4 Prompt Levels (Figure [3(a)](#S4.F3.sf1 "In Fig. 3 ‣ 4.5.1 Results
    from Micro-Evaluation ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose
    Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts")), which
    revealed that GPT-3.5 and PaLM2 were comparable, while LLaMA2 was often rated
    as inferior (more Disagree and Strong Disagree ratings). Finally, Figure [3(b)](#S4.F3.sf2
    "In Fig. 3 ‣ 4.5.1 Results from Micro-Evaluation ‣ 4.5 Results ‣ 4 Case Study
    ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of
    Scholarly Manuscripts") shows the precision/recall distributions for each prompt
    level while aggregating human ratings over all three LLMs, indicating that Level
    3 and Level 4 prompts were more effective for the meta-review generation task
    than Level 1 and Level 2, which is intuitive as meta-review generation is a complex
    task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5030237fbee84e4014683dea7b9c9c25.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Fig. 4: Core Contributions - Overall Ratings'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, Figure [4](#S4.F4 "Fig. 4 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation
    ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from
    Peer-Review Narratives of Scholarly Manuscripts") shows the precision/recall distributions
    aggregated over three LLMs and four prompt levels. This result suggests that humans
    generally voted in favor of LLMs more often for the meta-review generation task
    than otherwise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/baad834f458b0ac8d044a49ec9797cff.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (a) Common Strengths - Aggregated over all Prompt Levels
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/28190926dfa4e2ea7a2314122f9f7f3b.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (b) Common Strengths - Aggregation over LLMs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f603d2955f62aae9aa606533a7811410.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (c) Common Weaknesses - Aggregated over all Prompt Levels
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79773f5560a5fd2230583671af1e0689.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (d) Common Weaknesses - Aggregation over LLMs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/76bebcd091b5669d28ed8249dca2a0d9.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (e) Common Suggestions - Aggregated over all Prompt Levels
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a1a87241153b4d48bc9477fdd6de4876.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (f) Common Suggestions - Aggregation over LLMs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0a7cde42e12089481f69abe522935f09.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (g) Literature Review - Aggregated over all Prompt Levels
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/540d3a691fbca20cf472b890c1c2213d.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (h) Literature Review - Aggregation over LLMs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fig. 5: Ratings of Four Criteria (Common Strengths, Common Weaknesses, Common
    Suggestions, and Literature Review Quality) - Aggregated separately across different
    Prompt Levels and different LLMs.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06663865cdbd5e0c40f4e797eb12739f.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (a) Common Strengths
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/187a1bef952ccfe2ce074e22d98e6122.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (b) Common Weaknesses
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af1839fe5e5e73fc996c830b485c93d6.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (c) Common Suggestions
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ecc44590fb99db0f55272b32c9077fcd.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (d) Literature Review
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fig. 6: Overall Rating aggregated over three LLMs and four Prompt Levels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ Mic-2 (Common Strengths): In the case of Common strengths, Figure [5(a)](#S4.F5.sf1
    "In Fig. 5 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation ‣ 4.5 Results ‣ 4
    Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts") reveals that GPT-3.5 and PaLM2 were comparable, while
    LLaMA2 was often rated as slightly inferior. On the other hand, Figure [5(b)](#S4.F5.sf2
    "In Fig. 5 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation ‣ 4.5 Results ‣ 4
    Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts") demonstrates that the performance of LLMs improves
    substantially from level 1 to level 2 prompts. There is further improvement in
    the precision of LLMs as we go to higher levels of prompts. But this is not true
    in the case of recall, where the performance keeps improving till level 3 and
    then goes down.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ Mic-3 (Common Weaknesses): We notice a similar comparative performance of
    LLMs (Figure [5(c)](#S4.F5.sf3 "In Fig. 5 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation
    ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from
    Peer-Review Narratives of Scholarly Manuscripts")) and Prompt Levels (Figure [5(d)](#S4.F5.sf4
    "In Fig. 5 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation ‣ 4.5 Results ‣ 4
    Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts")) in the case of Common weaknesses as we observed for
    Common Strengths.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ Mic-4 (Common Suggestions): We again notice a similar comparative performance
    of LLMs (Figure [5(e)](#S4.F5.sf5 "In Fig. 5 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation
    ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from
    Peer-Review Narratives of Scholarly Manuscripts")) and Prompt Levels (Figure [5(f)](#S4.F5.sf6
    "In Fig. 5 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation ‣ 4.5 Results ‣ 4
    Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts")) in the case of Common suggestions as we observed for
    Common Strengths and Weaknesses.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RQ Mic-5 (Literature Review): We notice GPT-3.5 and PaLM2 were rated more favorably
    than LLaMA2 (Figure [5(g)](#S4.F5.sf7 "In Fig. 5 ‣ 1st item ‣ 4.5.1 Results from
    Micro-Evaluation ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose Meta-Review
    Drafts from Peer-Review Narratives of Scholarly Manuscripts")) by the human annotators
    for the Literature Review criterion. Further, as Figure [5(h)](#S4.F5.sf8 "In
    Fig. 5 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation ‣ 4.5 Results ‣ 4 Case
    Study ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts")) demonstrates, the performance of LLMs improves substantially
    from level 1 to level 2 prompts. However, we did not find appreciable improvement
    in performance by upgrading from level 2 to levels 3 and/or 4.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Summary of Micro-Evaluation: Figure [6](#S4.F6 "Fig. 6 ‣ 1st item ‣ 4.5.1 Results
    from Micro-Evaluation ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose
    Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts") shows
    the precision/recall distributions aggregated over three LLMs and four prompt
    levels for the following aspects of LLM-generated meta-reviews: Common Strengths,
    Common Weaknesses, Common Suggestions and Literature Review. This result (in combination
    with Figure [4](#S4.F4 "Fig. 4 ‣ 1st item ‣ 4.5.1 Results from Micro-Evaluation
    ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from
    Peer-Review Narratives of Scholarly Manuscripts")) suggests that humans generally
    voted in favor of LLMs more often for the Core Contributions, Common Strengths,
    and Literature Review aspects. Common weaknesses and common suggestions were evaluated
    with mixed ratings, suggesting LLMs struggled more along these aspects. Finally,
    an overall precision and recall score was computed by assigning the following
    scores to ratings, i.e., Strongly Agree = 4, Agree = 3, Neutral = 2, Disagree
    = 1, and Strongly Disagree = 0, and, thereafter, adding the scores of 4 prompt
    styles and dividing the sum by the maximum possible score to get a normalized
    score in the range [0,1]. Table [1](#S4.T1 "Table 1 ‣ 4.5.1 Results from Micro-Evaluation
    ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from
    Peer-Review Narratives of Scholarly Manuscripts") summarizes these scores, where
    we observe the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Aspect P/R PaLM2 GPT-3.5 Llama2 Core P 0.698 0.711 0.547 Contributions R 0.680
    0.667 0.548 Common P 0.583 0.598 0.492 Strengths R 0.588 0.600 0.525 Common P
    0.572 0.538 0.481 Weaknesses R 0.586 0.534 0.459 Common P 0.539 0.555 0.464 Suggestions
    R 0.584 0.566 0.489 Literature P 0.680 0.667 0.548 Review R 0.655 0.645 0.508
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Scores - Case Study Micro Evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-3.5 and PaLM2s were rated higher than LLaMA2 by humans in all five aspects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the “Core Contributions” and “Common suggestions” aspects, PaLM2 and GPT-3.5
    were comparable and better than LLaMA2.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GPT-3.5 was rated the highest in the “Common Strengths” aspect.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PaLM2 was rated the highest for “Common weaknesses” and “Literature Review”
    aspects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PaLM2 yielded better Recall scores in general (except for Common Strengths),
    while GPT-3.5 yielded better Precision scores in general (except in the cases
    of Common weaknesses and Literature Review).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.5.2 Results from Macro-Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: For the macro-evaluation experiments, we present the results and findings of
    our case study by answering the following broad research questions related to
    the overall performance of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RQ Mac-1: Can LLMs properly understand the complex requirements and sub-tasks
    associated with the meta-review generation task?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RQ Mac-2: How useful were the meta-review generated by the LLMs?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'RQ Mac-3: To what extent do the LLM-generated meta-reviews match human-expert-written
    meta-review?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Human evaluators again rated their agreements with the Macro evaluation statements
    on the same 5-point scale, i.e., {SA=Strongly Agree, A=Agree, N=Neutral, D=Disagree,
    SD=Strongly Disagree}. Next, a unified quality score was computed by assigning
    the following scores to ratings, i.e., Strongly Agree = 4, Agree = 3, Neutral
    = 2, Disagree = 1, and Strongly Disagree = 0, and, thereafter, adding the scores
    of 4 prompt styles and dividing the sum by the maximum possible score to get a
    normalized score in the range [0,1] as reported in Table [2](#S4.T2 "Table 2 ‣
    4.5.2 Results from Macro-Evaluation ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs
    to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts").
    Surprisingly, Table [2](#S4.T2 "Table 2 ‣ 4.5.2 Results from Macro-Evaluation
    ‣ 4.5 Results ‣ 4 Case Study ‣ Prompting LLMs to Compose Meta-Review Drafts from
    Peer-Review Narratives of Scholarly Manuscripts") shows that GPT-3.5 is the worst
    performer, whereas PaLM2 is the best in all the three aspects we evaluated at
    the Macro Level. We were baffled by this outcome and decided to interview the
    annotators further. Based on the follow-up discussions, we found that annotators
    generally suggested that Micro-evaluation should be more reliable as they were
    much more fine-grained and focused than Macro-evaluation judgments. However, it
    remains an open question why GPT-3.5 was generally rated high in Micro-evaluation
    but not for Macro-evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: Aspect PaLM2 GPT-3.5 LLaMA2 Adherence to instructions 0.65 0.53 0.55 Meta-review
    usefulness 0.65 0.53 0.63 Meta-review matching 0.58 0.50 0.50
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Macro Evaluation Results'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This paper uses a new prompting taxonomy called TELeR as a basis to evaluate
    and compare the performances of state-of-the-art LLMs for the meta-review generation
    task. More specifically, we conducted a case study with three LLMs and four different
    prompt levels from the TELeR taxonomy to perform the task of composing a meta-review
    for 40 different ICLR research manuscripts. We engaged 10 human evaluators to
    judge LLM-generated meta-reviews on five important aspects including the quality
    of capturing Core contributions, Common strengths, Common weaknesses, Common suggestions,
    and Literature Review.
  prefs: []
  type: TYPE_NORMAL
- en: Our case study revealed that while generating a meta-review, GPT-3.5 and PaLM2s
    were rated higher by humans than LLaMA2 in all five aspects. The performances
    of PaLM2 and GPT-3.5 were comparable on average, but PaLM2 yielded better Recall
    scores in general, while GPT-3.5 yielded better Precision scores. In terms of
    level of prompts, the performance of LLMs improves substantially from level 1
    to level 2 prompts. However, we did not always find appreciable improvement in
    performance by going from level 2 prompts to level 3 and level 4 prompts. Finally,
    Although the micro-evaluation indicated PaLM2 and GPT-3.5 to be comparable, macro-evaluation
    results indicate that humans like PaLM2 generated meta-reviews more than the GPT-3.5
    generated ones. This is an interesting and surprising finding which needs a deeper
    investigation in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work only analyses three LLMs and 40 research manuscripts due to time and
    resource constraints. More work will need to be done to increase the number of
    models tested to draw a more general conclusion on the performance of LLMs on
    the meta-review composition task. However, it should be noted that the estimated
    time required to annotate each paper is about 4 hours. Each annotator was assigned
    4 papers for annotation. With 10 annotators, a total of 160 hours of annotation
    effort was spent apart from initial exploration, preliminary studies, and data
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: During annotation, human volunteers did not read the actual research papers,
    they only read the abstract and the associated reviews, comments, and meta-reviews
    for that paper. Therefore, the annotators assumed that the reviewer’s comments
    were correct and justified.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Akter et al. (2022) Mousumi Akter, Naman Bansal, and Shubhra Kanti Karmaker
    Santu. 2022. [Revisiting automatic evaluation of extractive summarization task:
    Can we do better than rouge?](https://doi.org/10.18653/V1/2022.FINDINGS-ACL.122)
    In *Findings of the Association for Computational Linguistics: ACL 2022, Dublin,
    Ireland, May 22-27, 2022*, pages 1547–1560\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akter and Santu (2023a) Mousumi Akter and Shubhra Kanti Karmaker Santu. 2023a.
    [Fans: a facet-based narrative similarity metric](https://doi.org/10.48550/ARXIV.2309.04823).
    *CoRR*, abs/2309.04823.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Akter and Santu (2023b) Mousumi Akter and Shubhra Kanti Karmaker Santu. 2023b.
    [Redundancy aware multi-reference based gainwise evaluation of extractive summarization](https://doi.org/10.48550/ARXIV.2308.02270).
    *CoRR*, abs/2308.02270.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bansal et al. (2022a) Naman Bansal, Mousumi Akter, and Shubhra Kanti Karmaker
    Santu. 2022a. [Learning to generate overlap summaries through noisy synthetic
    data](https://doi.org/10.18653/V1/2022.EMNLP-MAIN.807). In *Proceedings of the
    2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022,
    Abu Dhabi, United Arab Emirates, December 7-11, 2022*, pages 11765–11777\. Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bansal et al. (2022b) Naman Bansal, Mousumi Akter, and Shubhra Kanti Karmaker
    Santu. 2022b. [SEM-F1: an automatic way for semantic evaluation of multi-narrative
    overlap summaries at scale](https://doi.org/10.18653/V1/2022.EMNLP-MAIN.49). In
    *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing,
    EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022*, pages 780–792\.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bansal et al. (2022c) Naman Bansal, Mousumi Akter, and Shubhra Kanti Karmaker
    Santu. 2022c. [Semantic overlap summarization among multiple alternative narratives:
    An exploratory study](https://aclanthology.org/2022.coling-1.541). In *Proceedings
    of the 29th International Conference on Computational Linguistics, COLING 2022,
    Gyeongju, Republic of Korea, October 12-17, 2022*, pages 6195–6207\. International
    Committee on Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](http://arxiv.org/abs/2005.14165).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2023) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.
    *Journal of Machine Learning Research*, 24(240):1–113.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022) Nan Du, Yanping Huang, Andrew M. Dai, Simon Tong, Dmitry Lepikhin,
    Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, Barret Zoph,
    Liam Fedus, Maarten Bosma, Zongwei Zhou, Tao Wang, Yu Emma Wang, Kellie Webster,
    Marie Pellat, Kevin Robinson, Kathleen Meier-Hellstern, Toju Duke, Lucas Dixon,
    Kun Zhang, Quoc V Le, Yonghui Wu, Zhifeng Chen, and Claire Cui. 2022. [Glam: Efficient
    scaling of language models with mixture-of-experts](http://arxiv.org/abs/2112.06905).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fire and Guestrin (2019) Michael Fire and Carlos Guestrin. 2019. [Over-optimization
    of academic publishing metrics: Observing goodhart’s law in action](https://doi.org/10.1093/gigascience/giz053).
    *GigaScience*, 8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2022) Xu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong
    Sun. 2022. Ptr: Prompt tuning with rules for text classification. *AI Open*, 3:182–192.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. 2023. Pre-train, prompt, and predict: A systematic
    survey of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. 2022. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Santu and Feng (2023) Shubhra Kanti Karmaker Santu and Dongji Feng. 2023. [Teler:
    A general taxonomy of llm prompts for benchmarking complex tasks](http://arxiv.org/abs/2305.11430).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Santu et al. (2018) Shubhra Kanti Karmaker Santu, Chase Geigle, Duncan C. Ferguson,
    William W. Cope, Mary Kalantzis, Duane Searsmith, and Chengxiang Zhai. 2018. [SOFSAT:
    towards a setlike operator based framework for semantic analysis of text](https://doi.org/10.1145/3299986.3299990).
    *SIGKDD Explor.*, 20(2):21–30.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sarkar et al. (2022) Souvika Sarkar, Dongji Feng, and Shubhra Kanti Karmaker
    Santu. 2022. [Exploring universal sentence encoders for zero-shot text classification](https://aclanthology.org/2022.aacl-short.18).
    In *Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association
    for Computational Linguistics and the 12th International Joint Conference on Natural
    Language Processing, AACL/IJCNLP 2022 - Volume 2: Short Papers, Online only, November
    20-23, 2022*, pages 135–147\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sarkar et al. (2023) Souvika Sarkar, Dongji Feng, and Shubhra Kanti Karmaker
    Santu. 2023. [Zero-shot multi-label topic inference with sentence encoders and
    llms](https://aclanthology.org/2023.emnlp-main.1008). In *Proceedings of the 2023
    Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore,
    December 6-10, 2023*, pages 16218–16233\. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thoppilan et al. (2022) Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
    Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
    Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali,
    Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong
    Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching
    Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man,
    Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos
    Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark
    Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh
    Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina,
    Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas,
    Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. [Lamda: Language models for
    dialog applications](http://arxiv.org/abs/2201.08239).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. 2023a. [Llama: Open and efficient foundation language models](http://arxiv.org/abs/2302.13971).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023b. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2023) Xianjun Yang, Yan Li, Xinlu Zhang, Haifeng Chen, and Wei
    Cheng. 2023. Exploring the limits of chatgpt for query or aspect-based text summarization.
    *arXiv preprint arXiv:2302.08081*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Detailed Plots for Four Criteria
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'These figures (Figures [7](#A1.F7 "Fig. 7 ‣ A.1 Detailed Plots for Four Criteria
    ‣ Appendix A Appendix ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review
    Narratives of Scholarly Manuscripts"), [8](#A1.F8 "Fig. 8 ‣ A.1 Detailed Plots
    for Four Criteria ‣ Appendix A Appendix ‣ Prompting LLMs to Compose Meta-Review
    Drafts from Peer-Review Narratives of Scholarly Manuscripts"), [9](#A1.F9 "Fig.
    9 ‣ A.1 Detailed Plots for Four Criteria ‣ Appendix A Appendix ‣ Prompting LLMs
    to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts")
    and  [10](#A1.F10 "Fig. 10 ‣ A.1 Detailed Plots for Four Criteria ‣ Appendix A
    Appendix ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts")) depict the comparative performances of three LLMs
    in terms of their (human-perceived) precision/recall distributions along four
    criteria: Common Strengths, Common Weaknesses, Common Suggestions, and Literature
    Review. Here, human-perceived precision and recall distributions are derived from
    their qualitative rating counts, i.e., {SA=Strongly Agree, A=Agree, N=Neutral,
    D=Disagree, SD=Strongly Disagree}, followed by a normalization term to convert
    them into a valid distribution (individual values ranging between [0-1]). Refer
    to Figures [7](#A1.F7 "Fig. 7 ‣ A.1 Detailed Plots for Four Criteria ‣ Appendix
    A Appendix ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts"), [8](#A1.F8 "Fig. 8 ‣ A.1 Detailed Plots for Four Criteria
    ‣ Appendix A Appendix ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review
    Narratives of Scholarly Manuscripts"), [9](#A1.F9 "Fig. 9 ‣ A.1 Detailed Plots
    for Four Criteria ‣ Appendix A Appendix ‣ Prompting LLMs to Compose Meta-Review
    Drafts from Peer-Review Narratives of Scholarly Manuscripts") and  [10](#A1.F10
    "Fig. 10 ‣ A.1 Detailed Plots for Four Criteria ‣ Appendix A Appendix ‣ Prompting
    LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts")
    for these detailed results.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2bddc0a417fd0eb349ef00b90b148ac4.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Common Strengths - Prompt TELeR Level 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f36f13aca1ab27f21df0ad7e8b4322a8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Common Strengths - Prompt TELeR Level 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ab5284b6d977a858edbdccfe2e990cef.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Common Strengths - Prompt TELeR Level 3
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3c9f958cb51bf7b246dd54ad0ba24ce1.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Common Strengths - Prompt TELeR Level 4
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 7: Common Strengths Ratings - Prompt TELeR Level 1-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ebf447878ef1690335fcd5f101896ec0.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Common Weaknesses - Prompt Level 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d990c92673515a876fea872e6c94fe3.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Common Weaknesses - Prompt Level 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b104a54c0c0c6a8998bb77f4cc9b3aa3.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Common Weaknesses - Prompt Level 3
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d3be2b613e3df890b67f64b8abeae5e.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Common Weaknesses - Prompt Level 4
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 8: Core Weaknesses Ratings - Prompt TELeR Level 1-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a84ac81f468a36e2ff849fdf222cb666.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Common Suggestions - Prompt Level 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d53e2976827e94fa8fe42c4bdcf6a17c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Common Suggestions - Prompt Level 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/47b190fb3faa0c20da3eb4cd677be24d.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Common Suggestions - Prompt Level 3
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12fcfd09c56d747831a3563e9025bca3.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Common Suggestions - Prompt Level 4
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 9: Core Suggestions Ratings - Prompt TELeR Level 1-4.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/90f2af955e5fd647c2a295a5e70829c6.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Literature Review - Prompt Level 1
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5e8064cd5563300e3a0a53944ef8b88.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Literature Review - Prompt Level 2
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/276d9999559b27af820aee2c8800dd9f.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Literature Review - Prompt Level 3
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d2f7511ce21a7e1c28f63a1f3d471692.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Literature Review - Prompt Level 4
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. 10: Literature Review Ratings - Prompt TELeR Level 1-4.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Prompt Design Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt designs during our preliminary study and final case study are shown in
    Table [3](#A1.T3 "Table 3 ‣ A.2 Prompt Design Details ‣ Appendix A Appendix ‣
    Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly
    Manuscripts").
  prefs: []
  type: TYPE_NORMAL
- en: '| Level | Prompt (Preliminary Study) | Prompt (Case Study) |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | messages=[“role”: “system”, “content”: “As a meta-reviewer, answer the
    following: What would be a reasonable meta-review considering the given reviews?”,
    “role”: “user”, “content”: 1.$$ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | messages=[“role”: “system”, “content”: “You are a meta-review assistant.
    You should create a meta review by answering the following questions:” “(a) According
    to reviews, what are the core contributions?” “(b) What are the strengths as mentioned
    in the reviews?” “(c) What are the weaknesses as mentioned in the reviews?” “(d)
    What suggestions would you provide for improvement?” “(e) What are the missing
    references as described in the reviews?”, “role”: “user”, “content”: 1. |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | messages=[ “role”: “system”, “content”: “As a meta-reviewer, draft a
    meta review by answering the following bullet points: ” “- What is the summary
    of core contributions? Provide answer with supporting evidences.” “- Which common
    strengths are referred to in the reviews? Support your answer with explanations.”
    “- What common weaknesses are described in the reviews? Give evidences in support
    of the reply.” “- What suggestions for improvement have been provided by three
    reviews? Explain the basis for the answer.” “- Which missing references are mentioned
    in the reviews? Answer with explanations will be desirable.”, ”role”: ”user”,
    ”content”: 1.<math id=$$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | In all the above prompts, R1, R2, and R3 are reviews provided by the reviewers.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Prompt Design for different levels'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Responsible NLP Checklist
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Potential Risks: This work has no potential risk. It is a purely curiosity-driven
    academic study.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Human Annotator Recruitment: We recruited eight graduate students and two undergraduates
    for the annotation task. Undergrad human annotators were paid at the hourly rate
    of $15/hour, while graduate student annotators were paid monthly stipends at the
    university-prescribed rate.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Annotation Guideline: Detail instructions were given to human annotators for
    Micro-evaluation and Macro-Evaluation. Instructions in these evaluation forms
    are presented in Figures [11](#A1.F11 "Fig. 11 ‣ 3rd item ‣ A.3 Responsible NLP
    Checklist ‣ Appendix A Appendix ‣ Prompting LLMs to Compose Meta-Review Drafts
    from Peer-Review Narratives of Scholarly Manuscripts"), [12](#A1.F12 "Fig. 12
    ‣ 3rd item ‣ A.3 Responsible NLP Checklist ‣ Appendix A Appendix ‣ Prompting LLMs
    to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts"),
    and [13](#A1.F13 "Fig. 13 ‣ 3rd item ‣ A.3 Responsible NLP Checklist ‣ Appendix
    A Appendix ‣ Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives
    of Scholarly Manuscripts").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb87e5d36b6f5c9b7ccfb3ea5d894f4f.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Fig. 11: Micro Evaluation Instructions'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/06b3f3ff8b32f8ef095a7dac52651aae.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Fig. 12: Micro Evaluation Form'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d55eea145f43042b3b5078fe0ef5394.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Fig. 13: Macro Evaluation Form'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data Privacy and Anonymity Measures: Any data relating to human subjects (e.g.,
    evaluation survey/interview results) was reviewed to ensure confidentiality and
    privacy before being preparing the manuscript. We did not collect any user behavior
    data when annotators performed qualitative evaluations. Finally, we conducted
    a user survey and reported the summary data from the survey. The survey did not
    collect any user demographic and personal background information; no identifiable
    information was stored or reported during/after the lifecycle of this work.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Policies and Provisions for Re-use, Re-distribution, and the Production of
    Derivatives: We plan to use one of the Creative Commons copyright licenses to
    re-use and redistribute our research output. In particular, we will adopt Attribution
    CC BY – one of the Creative Commons licenses. This license lets others distribute,
    remix, tweak, and build upon our project as long as users credit the project funded
    by NSF for the original creation. We decided to use this license because Attribution
    CC BY is the most accommodating of licenses offered for maximum dissemination.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
