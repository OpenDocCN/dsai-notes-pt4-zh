- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:43:40'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.10443](https://ar5iv.labs.arxiv.org/html/2405.10443)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Matthew Raffel     Victor Agostinelli     Lizhong Chen
  prefs: []
  type: TYPE_NORMAL
- en: Oregon State University
  prefs: []
  type: TYPE_NORMAL
- en: '{raffelm, agostinv, chenliz}@oregonstate.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large language models (LLMs) have achieved state-of-the-art performance in various
    language processing tasks, motivating their adoption in simultaneous translation.
    Current fine-tuning methods to adapt LLMs for simultaneous translation focus on
    prompting optimization strategies using either data augmentation or prompt structure
    modifications. However, these methods suffer from several issues, such as an unnecessarily
    expanded training set, computational inefficiency from dumping the KV cache, increased
    prompt sizes, or restriction to a single decision policy. To eliminate these issues,
    we propose a new paradigm in fine-tuning LLMs for simultaneous translation, called
    SimulMask. It utilizes a novel attention mask technique that models simultaneous
    translation during fine-tuning by masking attention connections under a desired
    decision policy. Applying the proposed SimulMask on a Falcon LLM for the IWSLT
    2017 dataset, we have observed a significant translation quality improvement compared
    to state-of-the-art prompting optimization strategies on three language pairs
    when averaged across four different latency regimes while reducing the computational
    cost.
  prefs: []
  type: TYPE_NORMAL
- en: 'Simultaneous Masking, Not Prompting Optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation
  prefs: []
  type: TYPE_NORMAL
- en: Matthew Raffel     Victor Agostinelli     Lizhong Chen Oregon State University
    {raffelm, agostinv, chenliz}@oregonstate.edu
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Simultaneous translation refers to the process of producing a target output
    translation concurrently with an oncoming source input. In our increasingly interconnected
    world, where communication across languages in real-time is desired, simultaneous
    translation is becoming a requirement. Unfortunately, the task of simultaneous
    translation is extremely straining for human interpreters, and among the limited
    number of simultaneous human interpreters, their maximum translation length is
    around thirty minutes Moser-Mercer et al. ([1998](#bib.bib14)). As such, there
    is a need for machine learning models to alleviate the burden placed on simultaneous
    human interpreters.
  prefs: []
  type: TYPE_NORMAL
- en: Current literature has primarily focused on adapting end-to-end Transformer
    models Vaswani et al. ([2017](#bib.bib22)) to overcome the difficulties of simultaneous
    translation due to their reduced parameter counts and greater inference speedMa
    et al. ([2020b](#bib.bib13)). However, the recent successes of large language
    models (LLMs) Touvron et al. ([2023](#bib.bib21)); Jiang et al. ([2023](#bib.bib8));
    Almazrouei et al. ([2023](#bib.bib2)) has prompted preliminary research applying
    them to simultaneous translation through fine-tuning and inference techniques
    Agostinelli et al. ([2023](#bib.bib1)); Wang et al. ([2023](#bib.bib25)); Koshkin
    et al. ([2024](#bib.bib9)); Wang et al. ([2024](#bib.bib24)); Guo et al. ([2024](#bib.bib6)).
    Unfortunately, most modern works have neglected the computational increases created
    by dumping the target sequence’s key and value (KV) cache Wang et al. ([2024](#bib.bib24)).
    Furthermore, there has yet to be a universal approach to fine-tuning LLMs for
    simultaneous translation that is not unnecessarily computationally expensive by
    either expanding the dataset through data augmentation, a process referred to
    as prefix fine-tuning Agostinelli et al. ([2023](#bib.bib1)); Wang et al. ([2023](#bib.bib25));
    Koshkin et al. ([2024](#bib.bib9)) or increasing the prompt length through prompt
    restructuring Koshkin et al. ([2024](#bib.bib9)); Wang et al. ([2024](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: The lack of an efficient fine-tuning strategy of LLMs for simultaneous translation
    has led us to propose a new paradigm, referred to as SimulMask. SimulMask is a
    novel attention mask to model simultaneous translation during fine-tuning by redistributing
    the attention connections under a decision policy. By design, SimulMask is broadly
    applicable to both flexible and fixed decision policies, creating a path forward
    for future work to build upon it. Furthermore, if we eliminate injecting positional
    information into the keys and values through a modified ALiBi Press et al. ([2021](#bib.bib20)),
    SimulMask allows for key and value caching during simultaneous translation without
    accuracy degradation.
  prefs: []
  type: TYPE_NORMAL
- en: To validate the efficacy of SimulMask, we fine-tuned and evaluated 1.3 billion
    parameter Falcon models pre-trained on the RefinedWeb dataset Almazrouei et al.
    ([2023](#bib.bib2)); Penedo et al. ([2023](#bib.bib18)) using the IWSLT 2017 dataset
    Cettolo et al. ([2017](#bib.bib3)). All models were fine-tuned using a wait-k
    decision policy. From the averaged evaluation results across all wait-k values,
    we found an LLM fine-tuned with SimulMask and evaluated with KV caching outperforms
    an LLM fine-tuned with prefix fine-tuning and evaluated without KV caching by
    7.34, 3.00, and 2.98 BLEU on the English-French, English-Dutch, and English-Italian
    language pairs, respectively. Furthermore, on average, with the wait-3 policy,
    recomputing the KV cache contributes toward 26.8 % of the computation time and
    87.9 % of the FLOPs to generate a target translation on an A40 GPU, substantiating
    the need for SimulMask, which enables inference time KV caching to avoid those
    computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the main contributions of the paper include:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Providing insights on the shortcomings of current literature in adapting LLMs
    to simultaneous translation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Proposing a novel attention masking approach to fine-tune LLMs for simultaneous
    translation that allows for efficient training and inference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Demonstrating the efficacy of our proposed approach in terms of translation
    quality and computational costs by evaluating them on multiple language pairs
    across many wait-k values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 Masked Transformer Self-Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We briefly review self-attention functionality in transformers (Vaswani et al.,
    [2017](#bib.bib22)) focusing on masking behavior in Equation [1](#S2.E1 "In 2.1
    Masked Transformer Self-Attention ‣ 2 Background and Related Work ‣ Simultaneous
    Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for
    Simultaneous Translation"), which describes per-head attention calculations for
    some attention head $h$ is defined as an optional attention mask.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{A}}^{(h)}=\texttt{softmax}\left(\frac{{\bm{Q}}^{(h)}{{\bm{K}}^{(h)}}^{T}+{\bm{M}}}{\sqrt{d_{head}}}\right){\bm{V}}^{(h)}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'Critically, ${\bm{M}}$ is represented by Equation [2](#S2.E2 "In 2.1 Masked
    Transformer Self-Attention ‣ 2 Background and Related Work ‣ Simultaneous Masking,
    Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation").'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{M}}_{ij}=\begin{cases}0,&amp;\text{if }j\leq i\\ -\infty,&amp;\text{otherwise}\end{cases}$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 2.2 Simultaneous Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Simultaneous translation is dictated by read-write decision policies, whereby
    a model will wait a specific amount of time before alternating between reading
    and writing in fixed or flexible intervals. One fixed decision policy for simultaneous
    translation that is broadly adopted as a common baseline to build on due to its
    effectiveness and simplicity is the wait-k policy Ma et al. ([2019](#bib.bib11)).
    As the name suggests, the wait-k policy will wait for k words before alternating
    between writing and reading a word. Although effective in simultaneous translation,
    alternative adaptive policies have gained traction, which base reading and writing
    on an auxiliary model or a predefined set of rules Cho and Esipova ([2016](#bib.bib4));
    Gu et al. ([2017](#bib.bib5)); Zheng et al. ([2019](#bib.bib28)). While capable
    of impressive results, such adaptive policies often incur additional computational
    costs and delays.
  prefs: []
  type: TYPE_NORMAL
- en: 'A Transformer is trained for a simultaneous translation decision policy by
    masking attention scores in the encoder self-attention and the decoder cross-attention.
    In the case of the encoder self-attention, each source token is prevented from
    attending to future source tokens following a decision policy Ma et al. ([2019](#bib.bib11)).
    For example, if the source sequence length was 5 tokens and the first read step
    reads 2 tokens, the second read step reads 1 token, and the third read step reads
    2 tokens, then the respective attention mask is provided in Figure [1](#S2.F1
    "Figure 1 ‣ 2.2 Simultaneous Translation ‣ 2 Background and Related Work ‣ Simultaneous
    Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for
    Simultaneous Translation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1e6a4f34201ea2a2f363dbc74abb79ff.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: An attention mask to model simultaneous translation for a transformer
    encoder during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, in decoder cross-attention, each target token is prevented from
    attending to future source hidden states following the decision policy Papi et al.
    ([2022a](#bib.bib16)). Equation [3](#S2.E3 "In 2.2 Simultaneous Translation ‣
    2 Background and Related Work ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation") expresses
    each entry of the decoder cross-attention mask, ${\bm{M}}_{tj}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{M}}_{tj}=\begin{cases}0,&amp;\text{if }j\leq f(t)\\ -\infty,&amp;\text{otherwise}\end{cases}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'In Equation [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background and
    Related Work ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation"), $f(t)$. Each source hidden
    state aligns with one source input token.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Applying Large Language Models for Simultaneous Translation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: LLMs have demonstrated remarkable performance on the task of neural machine
    translation Moslem et al. ([2023](#bib.bib15)); Vilar et al. ([2023](#bib.bib23));
    Xu et al. ([2023](#bib.bib26)); Zhang et al. ([2023](#bib.bib27)); Iyer et al.
    ([2023](#bib.bib7)). Such successes have prompted recent works to extend the reach
    of LLMs into the realm of simultaneous translation Agostinelli et al. ([2023](#bib.bib1));
    Wang et al. ([2023](#bib.bib25)); Koshkin et al. ([2024](#bib.bib9)); Wang et al.
    ([2024](#bib.bib24)); Guo et al. ([2024](#bib.bib6)). LLMs are especially promising
    for the field of simultaneous translation due to their strong understanding of
    language semantics and meaning. Intuitively, SimulMT LLMs inject holistic linguistic
    knowledge that could allow for correct translation decisions when facing difficult
    contextual obstacles (e.g., translating a verb in a target language without access
    to that verb in the source language).
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, Equation [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background
    and Related Work ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation") is no longer effective
    in modeling simultaneous translation for decoder-only LLMs as with the decoder
    of a Transformer. The reason is Equation [3](#S2.E3 "In 2.2 Simultaneous Translation
    ‣ 2 Background and Related Work ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation") is constructed
    specifically for the cross-attention calculation between keys exclusively from
    the source and queries exclusively from the target, as in Transformers. Since
    LLMs perform a self-attention, including the prompt, the source, and the target,
    Equation [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background and Related
    Work ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation") cannot properly mask the source from the target
    with the additional prompt and target sequences included in the keys, and the
    additional prompt and source sequences included in the queries. Furthermore, Equation
    [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background and Related Work ‣
    Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation") does not enforce the autoregressive language
    modeling behavior of LLMs. As such, alternative means to model simultaneous translation
    have been proposed, leveraging prompting optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Prompting Optimization Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Current methods of fine-tuning LLMs for simultaneous translation fall into the
    general category of prompting optimization. We define prompting optimization as
    either employing data augmentation to help with prompting Koshkin et al. ([2024](#bib.bib9));
    Wang et al. ([2023](#bib.bib25)); Agostinelli et al. ([2023](#bib.bib1)) or redefining
    the prompt structure Wang et al. ([2024](#bib.bib24)); Koshkin et al. ([2024](#bib.bib9))
    to somewhat simulate simultaneous translation. In this section, we will cover
    these prompting optimization strategies in depth.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompting optimization focusing on data augmentation resorts to subdividing
    each sentence in a dataset into multiple partial sentence pairs. These partial
    sentence pairs mimic simultaneous translation, as simultaneous translation produces
    outputs with a partial input. We label such a method as prefix fine-tuning, and
    although the high-level procedure is identical amongst current works, the algorithms
    employed to obtain these partial sentence pairs are unique. In the case of Agostinelli
    et al. ([2023](#bib.bib1)), each source-target sentence pair is subdivided according
    to the wait-k policy such that if we order the new samples from smallest to largest,
    each subsequent sentence pair will have one additional target word and source
    word so long as the end of the target or source is not reached. Upon completion
    there will be $\texttt{max}(|S|-(k-1),|T|)$ are the original source and target
    sequence lengths. The approach requires the model to predict only the final target
    word in the sequence during fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, Wang et al. ([2023](#bib.bib25)) randomly sampled a subset of
    sentence pairs from the dataset and truncated the source sentence to be 20% to
    80% of the full length according to a uniform distribution. They obtained the
    respective target translations by prompting ChatGPT. The new truncated source-target
    sentence pairs were then added to the complete dataset to expand the overall fine-tuning
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Prompt Restructuring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompting optimization that modifies the prompting structure adjusts the prompt
    to include the decision policy. In the case of Wang et al. ([2024](#bib.bib24)),
    a conversational prompting structure is adopted for the LLM, alternating between
    source and target subsequences of the original complete sequences using delimiting
    tokens to separate regions. For instance, if we have the source sequence $S=[s_{1},s_{2},...,s_{n}]$,
    then one potential conversational prompt could be $$\texttt{},\texttt{[U]},s_{1},s_{2},\texttt{[A]},t_{1},t_{2},\texttt{},...,\texttt{},\texttt{[U]},\\
  prefs: []
  type: TYPE_NORMAL
- en: s_{n},\texttt{[A]},t_{m},\texttt{}$$, where , , [A], [U] are delimiting
    tokens. During fine-tuning, the choice of alternating subsequences is arrived
    at by attempting to maximize the relevant source context before each target sequence
    in the form of an oracle decision policy. For instance, the prompt will ensure
    an arbitrary target verb prediction only after the respective source verb is read.
    Some minor perturbations are added to the oracle decision policy to improve generalizability.
    Then, at inference, a prompt constructor provides the source sequence in fixed-size
    chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, Koshkin et al. ([2024](#bib.bib9)) leverages prompt restructuring;
    however, it also employs prefix finetuning. Like the conversational prompting
    structure, it constructs a fine-tuning prompt by aligning words between the source
    and target sequence to mimic an oracle decision policy. However, it deviates from
    conversational prompting by ensuring the alignment using padding tokens in the
    target sequence. Then, the causally aligned sentence prompt is subdivided using
    a prefix fine-tuning strategy to expand the dataset with partially filled source-target
    sentence pairs. At inference, the LLM contains the decision policy outputting
    padding tokens whenever it requires more source context tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Analysis and Shortcomings of Prompting Optimization Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prompting optimization, while works to a certain degree, is inherently deficient,
    possessing a host of fine-tuning and inference issues. These issues include a
    persistent fine-tuning-inference mismatch, consistent positional confusion in
    the target sequence, and high computational costs. We will first present the fine-tuning-inference
    mismatch, followed by the positional confusion problem, and finish with the computational
    burdens facing the current paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Fine-tuning/Inference Mismatch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A fine-tuning-inference mismatch is a mismatch between an LLM’s fine-tuning
    and inference environments. For instance, fine-tuning an LLM for neural machine
    translation where the entire sentence is available and deploying it for simultaneous
    translation where little of the sentence is available when beginning generation
    will create a massive inference time fine-tuning-inference mismatch. Furthermore,
    the LLM must be fine-tuned to accommodate KV caching, the process of caching the
    keys and values at inference to prevent recomputation. Overall, fine-tuning for
    simultaneous translation aims to minimize the fine-tuning-inference mismatch.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, prefix fine-tuning precludes high-quality simultaneous translation
    with KV caching Agostinelli et al. ([2023](#bib.bib1)); Koshkin et al. ([2024](#bib.bib9));
    Wang et al. ([2023](#bib.bib25)) as with the continuously increasing prompt size,
    each subsequent entry in the KV cache continuously becomes outdated, a scenario
    the LLM was not fine-tuned to handle. For example, suppose we have an LLM in the
    middle of simultaneous translation using KV caching adhering to a wait-1 policy
    with the following prompting structure: “Translate the following sentence from
    English to German: $s_{1},s_{2},...,s_{i}$. Such fine-tuning-inference mismatch
    is unsolved through conventional prompting structures and is necessitated to increase
    translation quality.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompting restructuring also creates additional fine-tuning-inference mismatches.
    In Koshkin et al. ([2024](#bib.bib9)); Wang et al. ([2024](#bib.bib24)), they
    all fine-tune for an oracle decision policy. However, at inference, such an oracle
    decision policy is not truly achievable, creating a mismatch. Furthermore, since
    the LLMs that leverage prompt restructuring encapsulate a specific oracle decision
    policy into their fine-tuning curriculum, extending them to alternative decision
    policies at inference is infeasible without incurring a mismatch. As such, there
    is a need for a flexible method adaptable to a range of decision policies that
    also eliminates the fine-tuning-inference mismatch.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Positional Confusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Positional confusion describes the process whereby the relative and/or global
    positional information during simultaneous translation progressively becomes incorrect.
    Unfortunately, most simultaneous translation LLMs using KV caching suffer from
    this positional confusion Agostinelli et al. ([2023](#bib.bib1)); Koshkin et al.
    ([2024](#bib.bib9)); Wang et al. ([2023](#bib.bib25)). The reason is that as the
    source sequence grows with simultaneous translation, the target sequence shifts,
    necessitating the target sequence’s positional information to follow suit. However,
    since KV caching is employed, updating the keys and values is infeasible, causing
    them to hold incorrect positional information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aligning with our previous example, the sequence portion that would experience
    this positional confusion would be “[a]: $t_{1},t_{2},...,t_{i}$ would change
    to 2 and 3, respectively. However, while using KV caching, this positional distance
    would remain 1 and 2 in the keys and/or values for subsequent predictions, causing
    positional confusion. Continuing translation would see the gap between the true
    positional distance and the positional distance in the KV cache grow. Identifying
    an effective method to deal with positional confusion is a requirement to prevent
    LLM hallucinations.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Computational Inefficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Avoiding KV caching and recomputing all the keys and values at each prediction
    step is the default solution for resolving the aforementioned fine-tuning-inference
    mismatch and positional confusion problems while employing prefix fine-tuning.
    Although effective from a translation quality standpoint, doing so incurs a large
    computational cost, an undesirable result for streaming tasks like simultaneous
    translation where latency is equally important.
  prefs: []
  type: TYPE_NORMAL
- en: Outside of KV caching, the computational costs necessary for prefix fine-tuning
    methods are excessive. By subdividing each sample into multiple, the dataset drastically
    expands, contributing toward an increased cost to complete each epoch Agostinelli
    et al. ([2023](#bib.bib1)); Koshkin et al. ([2024](#bib.bib9)); Wang et al. ([2023](#bib.bib25)).
    Such an increase causes the duration of each epoch to rise by upwards of a factor
    of 5\. However, unlike normal methods of expanding a dataset through data augmentation,
    prefix fine-tuning does not add additional information. It is from this added
    computational burden that Agostinelli et al. ([2023](#bib.bib1)); Wang et al.
    ([2023](#bib.bib25)) are forced to fine-finetune with a subset of their entire
    prefix datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, methods of restructuring the prompt as in Koshkin et al. ([2024](#bib.bib9));
    Wang et al. ([2024](#bib.bib24)) have computational burdens of their own. For
    instance, Wang et al. ([2024](#bib.bib24)) requires adding delimiting tokens in
    the prompt sequence, expanding the sequence length. Similarly, the requirement
    of padding tokens to induce a causal alignment between the source and target sequences,
    as in Koshkin et al. ([2024](#bib.bib9)), also expands the sequence length. Since
    the computational cost of the self-attention cost in the LLM scales quadratically
    with the sequence length, such a method is undesirable for both inference and
    fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, no computationally efficient fine-tuning approach exists that enables
    computationally efficient inference. Identifying such a method is necessitated
    by the desire for low latency and high-quality simultaneous translations and reducing
    the already high computational costs of fine-tuning LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9cec84601e1d8021e51298534928f75.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The attention for the first prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb6cf01e602e56b7184f9fd80ad0eb8f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The inference mirrored attention for the first prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: The matching attention connections during inference and finetuning
    for simultaneous translation.'
  prefs: []
  type: TYPE_NORMAL
- en: '5 SimulMask: A Paradigm Shift'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we propose SimulMask, a paradigm shift in fine-tuning LLMs for
    simultaneous translation that eschews current methods of prompting optimization.
    It is through SimulMask, a method for restricting attention connections during
    fine-tuning, that we efficiently solve the fine-tuning-inference mismatch and
    positional confusion problem. Although we specifically apply SimulMask for the
    wait-k decision policy, it is broadly applicable to a range of decision policies.
    In this section, we will (1) explain the process of restricting attention connections
    to model simultaneous translation during fine-tuning, (2) describe SimulMask,
    a masking method for inducing these restricted attention connections, and (3)
    present a solution for positional confusion that enabled by SimulMask.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Inference Mirrored Attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Under simultaneous translation, the latest translation token at each prediction
    step is conditioned only on the running source tokens. Specialized attention masks
    on the Transformer could achieve such conditioning; however, directly mapping
    these to LLMs is impossible since they fail to enforce autoregressive language
    modeling and cannot mask properly when the prompt, source, and target sequences
    are collectively included in the queries and keys. As such, prior works attempted
    to achieve such conditioning during fine-tuning using prompting optimization strategies
    littered with shortcomings. We aim to return to a modeling simultaneous translation
    with attention masks by creating inference mirrored attention. Through inference
    mirrored attention, we mirror the attention connections during inference at fine-tuning
    according to the chosen decision policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, suppose we model the attention connections for a wait-1 decision
    policy where the complete oracle input sequence is “$p_{1},s_{1},s_{2},s_{3},s_{4},p_{2},t_{1},t_{2},t_{3},t_{4}$
    attends to identical keys as its inference step. We provide the attention connections
    for the remainder of the queries in our example in Appendix [A](#A1 "Appendix
    A Inference Mirrored Attention Example ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 SimulMask
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To achieve the above inference mirrored attention, we opt for an attention mask
    to restrict attention during fine-tuning to mimic an arbitrary decision policy
    during simultaneous translation. An attention mask is preferable to prompting
    optimization as it is flexible and directly extends the LLM causal attention mask.
    We call such an attention mask SimulMask.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a demonstration, let us create a SimulMask for the wait-1 policy that extends
    our example from Section [5.1](#S5.SS1 "5.1 Inference Mirrored Attention ‣ 5 SimulMask:
    A Paradigm Shift ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation"). Since the LLM is autoregressive,
    SimulMask begins with a causal attention mask from which attention connections
    are removed to be identical to attention connections during simultaneous translation.
    If we recall from our example, the prompt, $p_{2}$. The SimulMask for the aforementioned
    wait-1 policy is provided in Figure [3](#S5.F3 "Figure 3 ‣ 5.2 SimulMask ‣ 5 SimulMask:
    A Paradigm Shift ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5d19a3c4f060282c5a28dff95ddb9252.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The SimulMask for modeling simultaneous translation according to
    a wait-1 decision policy during fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since each decision policy performs read/write decisions differently and each
    limits attention differently, this requires a unique attention mask for every
    sentence. However, this can be done straightforwardly. The general procedure to
    construct a SimulMask for a given policy and sentence consists of the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Construct a causal attention mask using Equation [2](#S2.E2 "In 2.1 Masked
    Transformer Self-Attention ‣ 2 Background and Related Work ‣ Simultaneous Masking,
    Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation") as a starting point for SimulMask.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Starting from the intersection between the query that predicts the first target
    token and the first source key, apply the sub-attention mask expressed in Equation
    [3](#S2.E3 "In 2.2 Simultaneous Translation ‣ 2 Background and Related Work ‣
    Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation"). The sub-attention mask prevents the target
    queries from attending to source keys following the arbitrary decision policy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mask any non-source queries before the query predicting the first target token
    from attending to the source keys not included in the first read decision. Such
    a step is necessary to prevent the hidden states associated with these queries
    from holding information of the entire source sequence at later layers in the
    LLM.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The computation for constructing an arbitrary SimulMask is negligible compared
    with the computation of the LLM’s forward and backward passes during fine-tuning.
    Since SimulMask is not applied during inference, it does not impact computational
    cost at deployment. Therefore, SimulMask is a necessary option for mimicking simultaneous
    translation during fine-tuning and providing low-latency translations at inference.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Positional Reordering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since positional confusion during inference is a byproduct of retaining outdated
    positional information in either the keys or values, bypassing it requires providing
    a form of positional information without injecting it directly into the sequence
    or KV cache. One positioning method that satisfies such a constraint is the popular
    ALiBi, which supplies positional information through biases in attention Press
    et al. ([2021](#bib.bib20)). The bias is applied to each query-key dot product
    row in the attention calculation as shown in Equation [4](#S5.E4 "In 5.3 Positional
    Reordering ‣ 5 SimulMask: A Paradigm Shift ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation"),
    where $m$ is a head-specific constant.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\bm{q}}^{(h)}_{i}{{\bm{K}}^{(h)}}^{T}+{\bm{M}}_{i}+m\cdot[-(i-1),...,-1,0]$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: Though simple, ALiBi has demonstrated an ability to extrapolate to much larger
    sequence lengths than other state-of-the-art positional encodings, making it desirable
    for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, ALiBi, by default, does not mesh with SimulMask due to SimulMask
    removing attention connections between the target queries and source keys. These
    removed attention connections create a gap in ALiBi biases during fine-tuning
    that are not present at inference. An example of such a gap is provided in Figure
    [4(a)](#S5.F4.sf1 "In Figure 4 ‣ 5.3 Positional Reordering ‣ 5 SimulMask: A Paradigm
    Shift ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in
    Fine-tuning LLMs for Simultaneous Translation"). In Figure [4(a)](#S5.F4.sf1 "In
    Figure 4 ‣ 5.3 Positional Reordering ‣ 5 SimulMask: A Paradigm Shift ‣ Simultaneous
    Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for
    Simultaneous Translation"), the gap is present for $q_{4}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6f65d6e186d2654f9ca548e493be30b7.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Original ALiBi.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae67c9af116f24d391c053857f087657.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Modified ALiBi.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: The ALiBi biases for a 5x5 attention matrix with SimulMask.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To eliminate the bias gap, we modify ALiBi by reducing the bias values of all
    query rows influenced by SimulMask. For each query row, the reduction in bias
    values is equivalent to the number of attention connections removed along the
    row using SimulMask. Figure [4(b)](#S5.F4.sf2 "In Figure 4 ‣ 5.3 Positional Reordering
    ‣ 5 SimulMask: A Paradigm Shift ‣ Simultaneous Masking, Not Prompting Optimization:
    A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation") provides an
    example of such a modification. In the case of $q_{4}$; therefore, the bias on
    the right of the gap is reduced by 2\. In modifying SimulMask, we eliminate positional
    confusion from the LLM during simultaneous translation.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Experimental Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our fine-tuning experiments used the Simul-LLM framework Agostinelli et al.
    ([2023](#bib.bib1)). Each experiment used a 1.3 billion parameter Falcon model
    pre-trained on the RefinedWeb dataset Penedo et al. ([2023](#bib.bib18)). The
    model architecture consisted of 24 layers. Each self-attention calculation had
    32 attention heads, and the hidden size was 2048\. We refer to models fine-tuned
    with prefix fine-tuning that recompute and do not recompute the KV cache as falcon-prefix-rec
    and falcon-prefix-norec, respectively. The models fine-tuned with SimulMask with
    and without modifying ALiBi are named as falcon-simul-norec-mod falcon-simul-norec,
    respectively. Finally, the model fine-tuned with a causal attention mask and evaluated
    recomputing the KV cache is named as falcon-causal-rec. For comparison purposes,
    we fine-tuned a Falcon model with a causal attention mask and evaluated it for
    neural machine translation. We refer to it as falcon-offline.
  prefs: []
  type: TYPE_NORMAL
- en: All fine-tuning was conducted on a single H100 GPU with bfloat16 precision.
    The non-prefix fine-tuning duration consisted of 2 epochs using a batch size of
    64, whereas the prefix fine-tuning duration consisted of 1 epoch using a batch
    size of 1024\. We applied a learning rate of $2e^{-4}$, a weight decay of 0.1,
    and an inverse square root scheduler. Our optimizer was AdamW Loshchilov and Hutter
    ([2017](#bib.bib10)). The fine-tuned falcon models all used ALiBi. All fine-tuning
    used the English-French (en-fr), English-Italian (en-it), and English-Dutch (en-nl)
    language pairs of the IWSLT 2017 training set Cettolo et al. ([2017](#bib.bib3)).
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluated translation quality and latency for simultaneous translation using
    Simul-LLM inference agents Agostinelli et al. ([2023](#bib.bib1)) interfacing
    with the SimulEval toolkit Ma et al. ([2020a](#bib.bib12)). The translation quality
    was determined using detokenized BLEU with SacreBLEU Post ([2018](#bib.bib19)).
    Alternatively, the latency was determined using Length-Adaptive Average Lagging
    (LAAL) Papi et al. ([2022b](#bib.bib17)). The computational cost of simultaneous
    translation was recorded with FLOPs, and time in seconds to complete translation.
    All metrics were obtained on a single A40 GPU with bfloat16 precision. Each model
    was evaluated at a wait-k four lower, for which it was fine-tuned. For instance,
    a model fine-tuned for wait-5 was evaluated at wait-1.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Target Length | Frequency | Pred Time (s) | Rec Time (s) | Pred GFLOPs |
    Rec GFLOPs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0-10 | 5.3 % | 0.40 | 0.07 | 27.60 | 19.87 |'
  prefs: []
  type: TYPE_TB
- en: '| 10-20 | 24.5 % | 0.80 | 0.23 | 46.91 | 95.08 |'
  prefs: []
  type: TYPE_TB
- en: '| 20-30 | 24.1 % | 1.31 | 0.44 | 71.79 | 252.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 30-40 | 18.4 % | 1.81 | 0.70 | 96.54 | 524.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 40-50 | 8.6 % | 2.39 | 0.93 | 124.62 | 866.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 50-60 | 7.5 % | 2.95 | 1.21 | 151.98 | 1352.39 |'
  prefs: []
  type: TYPE_TB
- en: '| 60-70 | 3.1 % | 3.43 | 1.41 | 175.44 | 1839.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 70-80 | 2.4 % | 3.94 | 1.42 | 200.67 | 1897.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 80-90 | 2.1 % | 4.53 | 1.67 | 229.13 | 2467.18 |'
  prefs: []
  type: TYPE_TB
- en: '| 90-100 | 1.5 % | 5.07 | 1.84 | 255.48 | 2870.21 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: The computation measurements using the wait-3 policy for falcon-causal-rec.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Translation Quality and Latency Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we demonstrate the efficacy of fine-tuning with SimulMask
    compared to fine-tuning with a causal attention mask or with prefix fine-tuning
    using BLEU scores and LAAL. All evaluations are performed across wait-1, wait-3,
    wait-5, and wait-7 policies. The translation quality and latency results on the
    English-French, English-Dutch, and English-French language pairs are provided
    in Figures [5](#S7.F5 "Figure 5 ‣ 7.1 Translation Quality and Latency Results
    ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation"), [6](#S7.F6 "Figure 6 ‣ 7.1
    Translation Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not
    Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation"), and [7](#S7.F7 "Figure 7 ‣ 7.1 Translation Quality and Latency
    Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3ba0180be8da62fee2448d82f90a835.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: The translation quality plotted against latency for LLMs on the English-French
    language pair.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ea06bc996b001a3a08eb23ea10831bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The translation quality plotted against latency for LLMs on the English-Dutch
    language pair.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4e21d4e86c37baa873880e45c217770.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: The translation quality plotted against latency for LLMs on the English-Italian
    language pair.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notably, in Figures [5](#S7.F5 "Figure 5 ‣ 7.1 Translation Quality and Latency
    Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation"), [6](#S7.F6 "Figure 6
    ‣ 7.1 Translation Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking,
    Not Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation"), and [7](#S7.F7 "Figure 7 ‣ 7.1 Translation Quality and Latency
    Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation"), falcon-simul-norec-mod
    outperforms falcon-causal-rec across all wait-k values. On average, the increase
    across these wait-k values in terms of translation quality is 5.44, 8.38, and
    4.78 BLEU on the English-French, English-Dutch, and English-Italian language pairs.
    Even more impressive, falcon-simul-norec-mod outperforms falcon-prefix-rec at
    low wait-k values and matches it for higher wait-k values where, on average, across
    all wait-k values, it increases BLEU score by 7.34, 3.00, and 2.98 BLEU on the
    English-French, English-Dutch, and English-Italian language pairs. Such increases
    in translation quality reflect the efficacy of SimulMask in adapting an LLM for
    simultaneous translation during fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figures [5](#S7.F5 "Figure 5 ‣ 7.1 Translation Quality and Latency Results
    ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation"), [6](#S7.F6 "Figure 6 ‣ 7.1
    Translation Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not
    Prompting Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous
    Translation"), and [7](#S7.F7 "Figure 7 ‣ 7.1 Translation Quality and Latency
    Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation") also provide an ablation
    demonstrating the importance of modifying ALiBi with SimulMask for high-quality
    translations by comparing falcon-simul-norec-mod with falcon-simul-norec. For
    each wait-k value and language pair, falcon-simul-norec-mod outperforms falcon-simul-norec.
    Quantified when averaged over all wait-k values, falcon-simul-norec-mod provides
    an increase of 1.20, 4.56, and 3.03 BLEU on the English-French, English-Dutch,
    and English-Italian language pairs over falcon-simul-norec. Unsurprisingly, at
    higher wait-k values where the setting approaches neural machine translation,
    the difference in BLEU scores becomes less pronounced between the models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A secondary ablation is provided in Figures [5](#S7.F5 "Figure 5 ‣ 7.1 Translation
    Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation"),
    [6](#S7.F6 "Figure 6 ‣ 7.1 Translation Quality and Latency Results ‣ 7 Results
    ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift in Fine-tuning
    LLMs for Simultaneous Translation"), and [7](#S7.F7 "Figure 7 ‣ 7.1 Translation
    Quality and Latency Results ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation")
    by comparing falcon-prefix-rec and falcon-prefix-norec. Doing so demonstrates
    translation quality increasing by recomputing the KV cache across all wait-k values.
    When averaged across these wait-k values, the increase is 3.58, 7.48, and 1.72
    BLEU on the English-French, English-Dutch, and English-Italian language pairs.
    Furthermore, as with the previous ablation, the difference in the BLEU score becomes
    less pronounced for the higher wait-k values.'
  prefs: []
  type: TYPE_NORMAL
- en: One observation that initially may seem strange is that models evaluated at
    lower wait-k values have their LAAL deviate from their respective k to a greater
    degree than those evaluated at higher wait-k. Such an increase is a byproduct
    of the lower wait-k models generating longer predictions than their corresponding
    references. The increased generation length is a byproduct of the model hallucinating
    on sequences provided insufficient context, causing them to repeat a single token
    multiple times. The most pronounced occurrence of this hallucination is provided
    by falcon-prefix-norec and falcon-prefix-rec on the English-French language pair.
    The output predictions from both these models showcase repeating single tokens
    or phrases causing not only increased LAAL but decreased BLEU score.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Compuational Saving Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While not using KV caching, the computation for each write step of simultaneous
    translation comprises of (1) recomputing the keys and values to reflect the source
    sequence update and (2) generating the new target tokens. In this section, we
    report the average time and GFLOPs required to recompute keys and values of the
    predicted target sequence (Rec Time and Rec GFLOPs) and generate the new target
    tokens (Pred Time and Pred GFLOPs) for each translation. In doing so, we demonstrate
    the necessity of enabling KV caching for simultaneous translation.
  prefs: []
  type: TYPE_NORMAL
- en: 'All computational evaluations are performed with falcon-causal-rec using the
    wait-3 policy on a random sampling of 1000 samples from the English-French language
    pair of the IWSLT 2017 test set and are reported in Table [1](#S7.T1 "Table 1
    ‣ 7 Results ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm Shift
    in Fine-tuning LLMs for Simultaneous Translation"). The computation measurements
    in Table [1](#S7.T1 "Table 1 ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation")
    are an average of samples grouped by the number of tokens in their output translations
    (Target Length). The frequency that a predicted translation from falcon-causal-rec
    is of a specified length range is also reported (Frequency). The results for wait-1,
    wait-5, and wait-7 policies are provided in Appendix [B](#A2 "Appendix B Extended
    Computational Costs ‣ Simultaneous Masking, Not Prompting Optimization: A Paradigm
    Shift in Fine-tuning LLMs for Simultaneous Translation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'From Table [1](#S7.T1 "Table 1 ‣ 7 Results ‣ Simultaneous Masking, Not Prompting
    Optimization: A Paradigm Shift in Fine-tuning LLMs for Simultaneous Translation"),
    in terms of GFLOPs, the recomputation of the keys and values contributes toward
    a greater portion of the total computation than the new target token generation.
    However, inversely, more time is spent generating new target tokens than recomputing
    the keys and values. The reason for the discrepancy is the recomputation of the
    keys and values is performed in parallel on the A40 GPU whereas the generation
    of the new target tokens is a sequential operation. On average, across all target
    lengths, taking into account their frequencies, the recomputation of the KV cache
    contributed to 26.8% of the computation time and 87.9% of the FLOPs required to
    generate a target translation at wait-3.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we examine current LLM fine-tuning approaches for simultaneous
    translation and identify their shortcomings. We then propose a new paradigm for
    fine-tuning LLM for simultaneous translation using a novel SimulMask that avoids
    the shortcomings of previous methods. Using SimulMask, the target sequence is
    prevented from attending to a portion of the source sequence according to an arbitrary
    decision policy modeling simultaneous translation. Through the application of
    SimulMask, we can efficiently fine-tune an LLM for simultaneous translation and
    reduce the computational costs of inference by eliminating the recomputation of
    the KV cache for the target sequence, unlike prior works. Furthermore, we can
    exceed or match the translation quality of prior works at all wait-k values across
    multiple language pairs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Agostinelli et al. (2023) Victor Agostinelli, Max Wild, Matthew Raffel, Kazi Asif
    Fuad, and Lizhong Chen. 2023. Simul-llm: A framework for exploring high-quality
    simultaneous translation with large language models. *arXiv preprint arXiv:2312.04691*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Almazrouei et al. (2023) Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi,
    Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel
    Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open
    language models. *arXiv preprint arXiv:2311.16867*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cettolo et al. (2017) Mauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan
    Niehues, Sebastian Stüker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann.
    2017. [Overview of the IWSLT 2017 evaluation campaign](https://aclanthology.org/2017.iwslt-1.1).
    In *Proceedings of the 14th International Conference on Spoken Language Translation*,
    pages 2–14, Tokyo, Japan. International Workshop on Spoken Language Translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho and Esipova (2016) Kyunghyun Cho and Masha Esipova. 2016. Can neural machine
    translation do simultaneous translation? *arXiv preprint arXiv:1606.02012*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2017) Jiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor O.K. Li.
    2017. [Learning to translate in real-time with neural machine translation](https://aclanthology.org/E17-1099).
    In *Proceedings of the 15th Conference of the European Chapter of the Association
    for Computational Linguistics: Volume 1, Long Papers*, pages 1053–1062, Valencia,
    Spain. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2024) Shoutao Guo, Shaolei Zhang, Zhengrui Ma, Min Zhang, and Yang
    Feng. 2024. Sillm: Large language models for simultaneous machine translation.
    *arXiv preprint arXiv:2402.13036*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iyer et al. (2023) Vivek Iyer, Pinzhen Chen, and Alexandra Birch. 2023. [Towards
    effective disambiguation for machine translation with large language models](https://doi.org/10.18653/v1/2023.wmt-1.44).
    In *Proceedings of the Eighth Conference on Machine Translation*, pages 482–495,
    Singapore. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Koshkin et al. (2024) Roman Koshkin, Katsuhito Sudoh, and Satoshi Nakamura.
    2024. Transllama: Llm-based simultaneous translation system. *arXiv preprint arXiv:2402.04636*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov and Hutter (2017) Ilya Loshchilov and Frank Hutter. 2017. Decoupled
    weight decay regularization. *arXiv preprint arXiv:1711.05101*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2019) Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu,
    Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, Hua Wu, and
    Haifeng Wang. 2019. [STACL: Simultaneous translation with implicit anticipation
    and controllable latency using prefix-to-prefix framework](https://doi.org/10.18653/v1/P19-1289).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 3025–3036, Florence, Italy. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2020a) Xutai Ma, Mohammad Javad Dousti, Changhan Wang, Jiatao Gu,
    and Juan Pino. 2020a. [SIMULEVAL: An evaluation toolkit for simultaneous translation](https://doi.org/10.18653/v1/2020.emnlp-demos.19).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pages 144–150, Online. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2020b) Xutai Ma, Juan Pino, and Philipp Koehn. 2020b. Simulmt to
    simulst: Adapting simultaneous text translation to end-to-end simultaneous speech
    translation. *arXiv preprint arXiv:2011.02048*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Moser-Mercer et al. (1998) Barbara Moser-Mercer, Alexander Künzli, and Marina
    Korac. 1998. Prolonged turns in interpreting: Effects on quality, physiological
    and psychological stress (pilot study). *Interpreting*, 3(1):47–64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moslem et al. (2023) Yasmin Moslem, Rejwanul Haque, John D. Kelleher, and Andy
    Way. 2023. [Adaptive machine translation with large language models](https://aclanthology.org/2023.eamt-1.22).
    In *Proceedings of the 24th Annual Conference of the European Association for
    Machine Translation*, pages 227–237, Tampere, Finland. European Association for
    Machine Translation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papi et al. (2022a) Sara Papi, Marco Gaido, Matteo Negri, and Marco Turchi.
    2022a. [Does simultaneous speech translation need simultaneous models?](https://doi.org/10.18653/v1/2022.findings-emnlp.11)
    In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages
    141–153, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papi et al. (2022b) Sara Papi, Marco Gaido, Matteo Negri, and Marco Turchi.
    2022b. [Over-generation cannot be rewarded: Length-adaptive average lagging for
    simultaneous speech translation](https://doi.org/10.18653/v1/2022.autosimtrans-1.2).
    In *Proceedings of the Third Workshop on Automatic Simultaneous Translation*,
    pages 12–17, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. 2023. [The RefinedWeb dataset for Falcon LLM: outperforming
    curated corpora with web data, and web data only](https://arxiv.org/abs/2306.01116).
    *arXiv preprint arXiv:2306.01116*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post (2018) Matt Post. 2018. A call for clarity in reporting bleu scores. *arXiv
    preprint arXiv:1804.08771*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Press et al. (2021) Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short,
    test long: Attention with linear biases enables input length extrapolation. *arXiv
    preprint arXiv:2108.12409*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. [Attention
    is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 30\. Curran Associates,
    Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vilar et al. (2023) David Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,
    Viresh Ratnakar, and George Foster. 2023. [Prompting PaLM for translation: Assessing
    strategies and performance](https://doi.org/10.18653/v1/2023.acl-long.859). In
    *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics
    (Volume 1: Long Papers)*, pages 15406–15427, Toronto, Canada. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2024) Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, and Gholamreza
    Haffari. 2024. Conversational simulmt: Efficient simultaneous translation with
    large language models. *arXiv preprint arXiv:2402.10552*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2023) Minghan Wang, Jinming Zhao, Thuy-Trang Vu, Fatemeh Shiri,
    Ehsan Shareghi, and Gholamreza Haffari. 2023. Simultaneous machine translation
    with large language models. *arXiv preprint arXiv:2309.06706*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Haoran Xu, Young Jin Kim, Amr Sharaf, and Hany Hassan Awadalla.
    2023. A paradigm shift in machine translation: Boosting translation performance
    of large language models. *arXiv preprint arXiv:2309.11674*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2023) Xuan Zhang, Navid Rajabi, Kevin Duh, and Philipp Koehn.
    2023. [Machine translation with large language models: Prompting, few-shot learning,
    and fine-tuning with QLoRA](https://doi.org/10.18653/v1/2023.wmt-1.43). In *Proceedings
    of the Eighth Conference on Machine Translation*, pages 468–481, Singapore. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2019) Baigong Zheng, Renjie Zheng, Mingbo Ma, and Liang Huang.
    2019. Simpler and faster learning of adaptive policies for simultaneous translation.
    *arXiv preprint arXiv:1909.01559*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Inference Mirrored Attention Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9cec84601e1d8021e51298534928f75.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The attention for the first prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb6cf01e602e56b7184f9fd80ad0eb8f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The inference mirrored attention for the first prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e17fd2888d91b96847da8129e15079d.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The attention for the second prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/755d92e3ebc8a61acb90053579b3d751.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) The inference mirrored attention for the second prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/04602375295f9b9d7e6f8b4466d7d5f2.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) The attention for the third prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5abd215725db8657c0f20a753aed436.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) The inference mirrored attention for the third prediction step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: The attention during inference and finetuning for simultaneous translation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65e36fadd6b661a3a74cb64e22f1439c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: A color-coded SimulMask for the wait-1 policy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94a5337d7116679066869f502da43772.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Color-coded inference mirrored attention produced by SimulMask for
    the wait-1 policy.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Extended Computational Costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Wait-k | Length | Frequency | Pred Time (s) | Rec Time (s) | Pred GFLOPs
    | Rec GFLOPs |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0-10 | 27.9 % | 0.30 | 0.09 | 22.32 | 27.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 10-20 | 33.0 % | 0.73 | 0.29 | 43.46 | 125.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 20-30 | 17.2 % | 1.28 | 0.52 | 70.76 | 315.79 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 30-40 | 8.5 % | 1.82 | 0.76 | 96.91 | 592.41 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 40-50 | 4.7 % | 2.35 | 0.98 | 123.13 | 920.19 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 50-60 | 3.2 % | 2.90 | 1.19 | 149.76 | 1314.32 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 60-70 | 1.6 % | 3.46 | 1.43 | 176.84 | 1868.62 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 70-80 | 1.1 % | 3.99 | 1.55 | 202.72 | 2197.84 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 80-90 | 0.9 % | 4.51 | 1.72 | 229.44 | 2725.79 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 90-100 | 0.6 % | 5.04 | 1.70 | 254.21 | 2682.38 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0-10 | 5.3 % | 0.40 | 0.07 | 27.60 | 19.87 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 10-20 | 24.5 % | 0.80 | 0.23 | 46.91 | 95.08 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 20-30 | 24.1 % | 1.31 | 0.44 | 71.79 | 252.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 30-40 | 18.4 % | 1.81 | 0.70 | 96.54 | 524.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 40-50 | 8.6 % | 2.39 | 0.93 | 124.62 | 866.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 50-60 | 7.5 % | 2.95 | 1.21 | 151.98 | 1352.39 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 60-70 | 3.1 % | 3.43 | 1.41 | 175.44 | 1839.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 70-80 | 2.4 % | 3.94 | 1.42 | 200.67 | 1897.82 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 80-90 | 2.1 % | 4.53 | 1.67 | 229.13 | 2467.18 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 90-100 | 1.5 % | 5.07 | 1.84 | 255.48 | 2870.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0-10 | 4.5 % | 0.35 | 0.01 | 24.98 | 3.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 10-20 | 20.0 % | 0.80 | 0.11 | 47.13 | 40.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 20-30 | 24.0 % | 1.31 | 0.35 | 71.70 | 178.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 30-40 | 17.6 % | 1.82 | 0.58 | 96.96 | 393.19 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 40-50 | 11.6 % | 2.36 | 0.84 | 123.17 | 720.18 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 50-60 | 6.9 % | 2.92 | 1.09 | 150.63 | 1137.89 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 60-70 | 4.8 % | 3.40 | 1.26 | 174.20 | 1527.99 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 70-80 | 3.3 % | 4.02 | 1.36 | 204.38 | 1744.19 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 80-90 | 2.3 % | 4.60 | 1.80 | 232.77 | 2856.33 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 90-100 | 1.6 % | 5.07 | 1.88 | 255.55 | 3146.94 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0-10 | 3.7 % | 0.38 | 0 | 26.05 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 10-20 | 19.8 % | 0.80 | 0.04 | 60.47 | 12.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 20-30 | 22.1 % | 1.31 | 0.22 | 71.96 | 95.39 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 30-40 | 18.5 % | 1.83 | 0.49 | 97.44 | 303.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 40-50 | 12.0 % | 2.35 | 0.71 | 122.54 | 560.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 50-60 | 7.8 % | 2.94 | 0.98 | 151.09 | 972.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 60-70 | 5.1 % | 3.44 | 1.16 | 175.45 | 1311.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 70-80 | 2.9 % | 4.03 | 1.25 | 204.60 | 1491.26 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 80-90 | 2.3 % | 4.58 | 1.55 | 230.83 | 2190.85 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 90-100 | 1.3 % | 5.07 | 1.80 | 255.25 | 2262.61 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: The relationship between the target translation length and computational
    costs.'
  prefs: []
  type: TYPE_NORMAL
