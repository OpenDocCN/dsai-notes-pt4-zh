- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented
    by ChatGPT'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented
    by ChatGPT'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2304.11116](https://ar5iv.labs.arxiv.org/html/2304.11116)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2304.11116](https://ar5iv.labs.arxiv.org/html/2304.11116)
- en: Jiawei Zhang [jiawei@ifmlab.org](mailto:jiawei@ifmlab.org) IFM Lab
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jiawei Zhang [jiawei@ifmlab.org](mailto:jiawei@ifmlab.org) IFM Lab
- en: Department of Computer Science,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系，
- en: University of California, DavisDavisCaliforniaUSA95616
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学戴维斯分校，戴维斯，加州，美国 95616
- en: https://github.com/jwzhanggy/Graph_Toolformer
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: https://github.com/jwzhanggy/Graph_Toolformer
- en: Abstract.
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: In this paper, we aim to develop a large language model (LLM) with the reasoning
    ability on complex graph data. Currently, LLMs have achieved very impressive performance
    on various natural language learning tasks, extensions of which have also been
    applied to study the vision tasks with data in multiple modalities. However, when
    it comes to the graph learning tasks, existing LLMs present very serious flaws
    due to their inherited weaknesses in performing precise mathematical calculation,
    multi-step logic reasoning, perception about the spatial and topological factors,
    and handling the temporal progression.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们旨在开发一个具有复杂图形数据推理能力的大型语言模型（LLM）。目前，LLMs在各种自然语言学习任务上取得了非常令人印象深刻的表现，这些扩展也已应用于多模态数据的视觉任务研究。然而，面对图形学习任务时，现有LLMs由于其在执行精确数学计算、多步骤逻辑推理、空间和拓扑因素的感知以及处理时间进程方面的固有缺陷，表现出非常严重的缺陷。
- en: To address such challenges, in this paper, we will investigate the principles,
    methodologies and algorithms to empower existing LLMs with the graph reasoning
    ability, which will have tremendous impacts on the current research of both LLMs
    and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose
    the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach
    LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning
    API tools. Specifically, we will investigate to teach Graph-ToolFormer to handle
    various graph data reasoning tasks in this paper, including both (1) very basic
    graph data loading and graph property reasoning tasks, ranging from simple graph
    order and size to the graph diameter and periphery, and (2) more advanced reasoning
    tasks on real-world graph data, such as bibliographic paper citation networks,
    protein molecular graphs, sequential recommender systems, online social networks
    and knowledge graphs.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些挑战，在本文中，我们将探讨使现有LLMs具备图形推理能力的原则、方法和算法，这将对当前LLMs和图形学习的研究产生巨大影响。受最新的ChatGPT和Toolformer模型的启发，我们提出了Graph-ToolFormer（面向图形推理的Toolformer）框架，以通过ChatGPT增强的提示教导LLMs使用外部图形推理API工具。具体而言，我们将探讨在本文中教授Graph-ToolFormer处理各种图形数据推理任务，包括（1）非常基础的图形数据加载和图形属性推理任务，涵盖从简单的图形顺序和大小到图形直径和周长，和（2）对实际图形数据进行的更高级的推理任务，例如书目文献引用网络、蛋白质分子图、顺序推荐系统、在线社交网络和知识图谱。
- en: Technically, to build Graph-ToolFormer, we propose to hand-craft both the instruction
    and a small amount of prompt templates for each of the graph reasoning tasks,
    respectively. Via in-context learning, based on such instructions and prompt template
    examples, we adopt ChatGPT to annotate and augment a larger graph reasoning statement
    dataset with the most appropriate calls of external API functions. Such augmented
    prompt datasets will be post-processed with selective filtering and used for fine-tuning
    existing pre-trained causal LLMs, such as the GPT-J and LLaMA, to teach them how
    to use graph reasoning tools in the output generation. To demonstrate the effectiveness
    of Graph-ToolFormer, we conduct extensive experimental studies on various graph
    reasoning datasets and tasks, and have also launched a LLM demo with various graph
    reasoning abilities. All the source code of Graph-ToolFormer framework, the demo
    for graph reasoning, and the graph and prompt datasets have been released online
    at the project github page.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，为了构建 Graph-ToolFormer，我们提议分别手工制作每个图推理任务的指令和少量提示模板。通过上下文学习，基于这些指令和提示模板示例，我们采用
    ChatGPT 对更大规模的图推理语句数据集进行标注和扩充，加入最合适的外部 API 函数调用。这些扩充后的提示数据集将经过选择性过滤后用于微调现有的预训练因果
    LLM，如 GPT-J 和 LLaMA，以教它们在输出生成中如何使用图推理工具。为了展示 Graph-ToolFormer 的有效性，我们在各种图推理数据集和任务上进行了广泛的实验研究，并推出了具有多种图推理能力的
    LLM 演示。Graph-ToolFormer 框架的所有源代码、图推理演示和图及提示数据集都已在项目 GitHub 页面上线。
- en: Tool Transformer; ChatGPT; In-Context Learning; Language Model; Graph Learning
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 工具转换器；ChatGPT；上下文学习；语言模型；图学习
- en: 1\. Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: '![Refer to caption](img/267284e0dae76bd0ebe2111e0d7e660e.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/267284e0dae76bd0ebe2111e0d7e660e.png)'
- en: Figure 1\. An Illustration of LLMs based Graph Reasoning Tasks. Based on the
    input graph data from various domains and a handful number of prompt examples
    with brief instructions, we propose to use ChatGPT to annotate and augment a large
    prompt dataset that contains graph reasoning API calls of external graph reasoning
    tools. The generated prompt dataset will be used to fine-tune the existing pre-trained
    LLMs, like GPT-J or LLaMA, to teach them to automatically use the most appropriate
    external API tools for accomplishing the input graph reasoning tasks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 基于 LLM 的图推理任务示意图。基于来自各个领域的输入图数据和少量带有简要说明的示例，我们提议使用 ChatGPT 对包含外部图推理工具的图推理
    API 调用的大型提示数据集进行标注和扩充。生成的提示数据集将用于微调现有的预训练 LLM，如 GPT-J 或 LLaMA，以教会它们自动使用最合适的外部
    API 工具来完成输入的图推理任务。
- en: In recent years, large language models (LLMs) ([Vaswani2017AttentionIA,](#bib.bib50)
    ; [Devlin2019BERTPO,](#bib.bib8) ; [Ouyang2022TrainingLM,](#bib.bib34) ) have
    achieved very impressive performance on a variety of natural language processing
    tasks ([OpenAI2023GPT4TR,](#bib.bib32) ; [Touvron2023LLaMAOA,](#bib.bib49) ; [Ouyang2022TrainingLM,](#bib.bib34)
    ), extensions of which have also been extensively applied to solve many other
    problems with data in different modalities as well ([OpenAI2023GPT4TR,](#bib.bib32)
    ; [Dosovitskiy2020AnII,](#bib.bib10) ; [Ramesh2022HierarchicalTI,](#bib.bib40)
    ; [Ramesh2021ZeroShotTG,](#bib.bib41) ). With the launch of ChatGPT and new Microsoft
    Bing Chat based on both GPT-3.5 and GPT-4, LLMs have also been widely used in
    people’s daily production and life. At the same time, due to their inherent limitations,
    these LLMs have also received lots of criticisms in their usages due to their
    inherited weaknesses, like inability in performing precise calculations ([patel-etal-2021-nlp,](#bib.bib36)
    ), difficulty in addressing multi-step logic reasoning problems ([Creswell2022SelectionInferenceEL,](#bib.bib6)
    ), incapable to conduct spatial and topological reasoning ([Bang2023AMM,](#bib.bib1)
    ), and unawareness of progression of temporal factors ([10.1162/tacl_a_00459,](#bib.bib9)
    ).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）（[Vaswani2017AttentionIA,](#bib.bib50) ; [Devlin2019BERTPO,](#bib.bib8)
    ; [Ouyang2022TrainingLM,](#bib.bib34)）在各种自然语言处理任务上取得了非常令人印象深刻的成绩（[OpenAI2023GPT4TR,](#bib.bib32)
    ; [Touvron2023LLaMAOA,](#bib.bib49) ; [Ouyang2022TrainingLM,](#bib.bib34)），其扩展也被广泛应用于解决许多其他数据不同模态的问题（[OpenAI2023GPT4TR,](#bib.bib32)
    ; [Dosovitskiy2020AnII,](#bib.bib10) ; [Ramesh2022HierarchicalTI,](#bib.bib40)
    ; [Ramesh2021ZeroShotTG,](#bib.bib41)）。随着 ChatGPT 和基于 GPT-3.5 和 GPT-4 的新微软 Bing
    Chat 的发布，LLMs 也被广泛应用于人们的日常生产和生活。同时，由于其固有的局限性，这些 LLMs 也因为其固有的弱点，如无法进行精确计算（[patel-etal-2021-nlp,](#bib.bib36)）、难以解决多步骤逻辑推理问题（[Creswell2022SelectionInferenceEL,](#bib.bib6)）、无法进行空间和拓扑推理（[Bang2023AMM,](#bib.bib1)）以及对时间因素的进程缺乏感知（[10.1162/tacl_a_00459,](#bib.bib9)），也受到了许多批评。
- en: 'With the parallel development of natural language processing and computer vision,
    transformer based deep learning models on graph structured data has also received
    lots of attention from the community in recent years ([Zhang2020GraphBertOA,](#bib.bib60)
    ; [Yun2019GraphTN,](#bib.bib56) ; [Hu2020GPTGNNGP,](#bib.bib16) ). Graph provides
    a unified representation for many inter-connected data in the real-world, which
    models both the diverse attributes of the nodes and the extensive links connecting
    the nodes with each other. Besides the classic graph structures we learn from
    the discrete math and algorithm courses, as shown in Figure [1](#S1.F1.1 "Figure
    1 ‣ 1\. Introduction ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"), lots of real-world data can also be
    modeled as graphs ([Shi2015ASO,](#bib.bib45) ), like bibliographic networks ([10.14778/3402707.3402736,](#bib.bib47)
    ), protein molecular graphs ([doi:10.1142/S0219633602000117,](#bib.bib52) ), recommender
    systems ([PremRec,](#bib.bib28) ), online social networks ([10.1145/1298306.1298311,](#bib.bib31)
    ), and knowledge graphs ([9416312,](#bib.bib18) ).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '随着自然语言处理和计算机视觉的并行发展，基于图结构数据的变换器深度学习模型近年来也受到了社区的广泛关注（[Zhang2020GraphBertOA,](#bib.bib60)
    ; [Yun2019GraphTN,](#bib.bib56) ; [Hu2020GPTGNNGP,](#bib.bib16)）。图提供了现实世界中许多互联数据的统一表示，它既建模了节点的多样属性，又建模了连接节点的广泛链接。除了我们从离散数学和算法课程中学到的经典图结构，如图
    [1](#S1.F1.1 "Figure 1 ‣ 1\. Introduction ‣ Graph-ToolFormer: To Empower LLMs
    with Graph Reasoning Ability via Prompt Augmented by ChatGPT") 所示，许多现实世界的数据也可以建模为图（[Shi2015ASO,](#bib.bib45)），例如书目网络（[10.14778/3402707.3402736,](#bib.bib47)）、蛋白质分子图（[doi:10.1142/S0219633602000117,](#bib.bib52)）、推荐系统（[PremRec,](#bib.bib28)）、在线社交网络（[10.1145/1298306.1298311,](#bib.bib31)）和知识图谱（[9416312,](#bib.bib18)）。'
- en: Meanwhile, compared with the prosperous research explorations on incorporating
    vision and language data into LLMs for designing the ambitious AGI development
    plan ([agi,](#bib.bib33) ), it seems researchers have either “unintentionally”
    or “intentionally” ignored the widely existed graph data and don’t seem to have
    any plans to include them into the LLMs building for achieving the AGI.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与在将视觉和语言数据纳入 LLMs 以设计雄心勃勃的 AGI 发展计划的繁荣研究探索相比（[agi,](#bib.bib33)），研究人员似乎“无意”或“有意”忽略了广泛存在的图数据，并且似乎没有计划将其纳入
    LLMs 的构建中以实现 AGI。
- en: Here, we say researchers have “unintentionally” ignored graphs, since compared
    with texts and images that we deal with everyday, graph has long-time been merely
    used as an intermediate modeling data structure for real-world data and we normally
    have no direct interactions with graph actually. It is natural that people may
    mistakenly think graph should not be the focus at the current stage for creating
    AIGC and building the AGI systems. At the same time, we say researchers may have
    “intentionally” ignored graphs, since graph learning may involve (1) lots of precise
    mathematical calculations of graph properties, (2) multi-hop logical reasoning
    through the links, (3) capturing the extensively connected graph spatial and topological
    structures, and (4) sometimes we also need to handle the dynamics of graphs that
    are changing with time. Careful readers may have noticed that these requirements
    mentioned for graph learning actually hit the nail on the head, which exactly
    correspond to the weaknesses of the current LLMs we mentioned at the very beginning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们说研究人员“无意中”忽视了图，因为与我们日常处理的文本和图像相比，图长期以来仅被用作现实世界数据的中间建模数据结构，实际上我们通常没有直接与图交互。人们自然可能误以为图在当前阶段不应成为创建AIGC和构建AGI系统的重点。同时，我们也说研究人员可能“故意”忽视了图，因为图学习可能涉及（1）大量精确的图属性数学计算，（2）通过链接进行的多跳逻辑推理，（3）捕捉广泛连接的图空间和拓扑结构，以及（4）有时我们还需要处理随着时间变化的图的动态。细心的读者可能已经注意到，这些提到的图学习要求实际上正中要害，完全对应了我们一开始提到的当前LLMs的弱点。
- en: Regardless of the potential challenges ahead of us, “an AGI without graph reasoning
    ability will never be the AGI we may desire”. Based on such motivations, we write
    this paper trying to incorporate graph data into LLMs for various graph reasoning
    tasks. On the one hand, we really hope the currently AI-leading companies like
    OpenAI, Microsoft, Google and Meta can take graph structured data reasoning into
    consideration when they develop their missions and plans for achieving the AGI,
    so that the graph learning community will be able to contribute our efforts to
    building the AGI system together with the language and vision communities. On
    the other hand, we also hope to empower the existing LLMs with the ability to
    overcome the weaknesses in their performance when handling graph structured data
    for complex graph reasoning tasks. So, the latest developed LLMs can also benefit
    the graph learning community for solving various graph reasoning tasks as well.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 不论我们面临的潜在挑战如何，“没有图推理能力的AGI永远不会是我们期望的AGI”。基于这种动机，我们撰写了这篇论文，尝试将图数据纳入LLMs以进行各种图推理任务。一方面，我们非常希望像OpenAI、Microsoft、Google和Meta这样的领先AI公司在制定实现AGI的使命和计划时考虑图结构数据推理，以便图学习社区能够与语言和视觉社区共同为构建AGI系统贡献力量。另一方面，我们也希望赋予现有LLMs在处理复杂图推理任务时克服性能弱点的能力。因此，最新开发的LLMs也可以使图学习社区从中受益，用于解决各种图推理任务。
- en: 'Considering the current language models and their extremely high pre-training
    costs, we cannot fundamentally re-design a new LLM with pre-training to equip
    them with the graph reasoning capabilities. Pre-training such LLMs from scratch
    is an infeasible task for most research groups in academia and majority of companies
    in the industry as well. To adapt to the common practices of NLP approaches, we
    will introduce the Graph Reasoning oriented Toolformer framework (Graph-ToolFormer)
    by fine-tuning some existing pre-trained LLMs (e.g., GPT-J or LLaMA) in this paper.
    Technically, as illustrated in Figure [1](#S1.F1.1 "Figure 1 ‣ 1\. Introduction
    ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented
    by ChatGPT"), based on the latest ChatGPT from OpenAI and Toolformer model from
    Meta ([Schick2023ToolformerLM,](#bib.bib43) ), we propose to provide the existing
    pre-trained LLMs (e.g., GPT-J or LLaMA) with the ability to perform various complex
    graph reasoning tasks by allowing them to use external graph learning tools, such
    as other pre-trained graph neural network models and existing graph reasoning
    toolkits. Instead of manually hard-coding the graph data loading and external
    graph learning tool usage function calls in the reasoning statements, to make
    Graph-ToolFormer as a general graph reasoning interface, we will fine-tune the
    LLMs to teach the models to decide not only where to retrieve the graph data,
    but also what tools to be used, as well as when and how to use these tools. More
    technical details about the Graph-ToolFormer model will be introduced in the following
    methodology section.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑到当前语言模型及其极高的预训练成本，我们无法从根本上重新设计一个新的LLM并进行预训练，以使其具备图推理能力。从头开始预训练这样的LLM对于大多数学术研究团队和绝大多数工业公司来说都是不可行的。为了适应NLP方法的常见实践，本文将介绍以图推理为导向的Toolformer框架（Graph-ToolFormer），通过对一些现有的预训练LLM（例如GPT-J或LLaMA）进行微调。技术上，如图[1](#S1.F1.1
    "Figure 1 ‣ 1\. Introduction ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT")所示，基于OpenAI的最新ChatGPT和Meta的Toolformer模型（[Schick2023ToolformerLM,](#bib.bib43)），我们建议为现有的预训练LLM（例如GPT-J或LLaMA）提供执行各种复杂图推理任务的能力，通过允许它们使用外部图学习工具，如其他预训练的图神经网络模型和现有的图推理工具包。为了使Graph-ToolFormer成为一个通用的图推理接口，我们将微调LLM，以教导模型不仅决定从哪里检索图数据，还决定使用哪些工具，以及何时和如何使用这些工具。有关Graph-ToolFormer模型的更多技术细节将在以下方法部分介绍。'
- en: 'As the first exploration attempt to use LLMs for general graph reasoning tasks,
    we summarize the contributions of this paper as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为首次探索使用LLM进行通用图推理任务的尝试，我们总结了本文的贡献如下：
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Reasoning with LLMs: This paper is the first paper that attempts to propose
    a general LLM, i.e., Graph-ToolFormer, that can handle graph reasoning tasks.
    It effectively remedies the weaknesses of existing LLMs on graph reasoning. More
    importantly, it helps bridge the graph learning community with the latest development
    on LLMs and AIGC led by the language and vision learning communities. So people
    in the graph learning community will also have the stage to demonstrate our skills
    and expertises in the current era of AIGC and the future era AGI.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用LLM进行图推理：本文是首个尝试提出一个通用LLM，即Graph-ToolFormer，能够处理图推理任务的论文。它有效地弥补了现有LLM在图推理方面的不足。更重要的是，它有助于将图学习社区与由语言和视觉学习社区主导的LLM和AIGC的最新发展对接。因此，图学习社区的人们也将有机会在当前AIGC时代和未来AGI时代展示我们的技能和专长。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Reasoning Prompt Dataset: In this paper, we create a handful number of
    human-written language instructions and prompt examples of how graph learning
    tools can be used. Based on the self-supervised in-context learning, we use ChatGPT
    to annotate and augment a large graph reasoning dataset with API calls of different
    external graph learning tools, which will also be post-processed with selective
    filtering. Via the github page¹¹1https://github.com/jwzhanggy/Graph_Toolformer/tree/main/data,
    we have released both the graph raw datasets and the generated graph reasoning
    prompt dataset used in this paper with the community for future explorations.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图推理提示数据集：本文创建了一些人工编写的语言指令和提示示例，展示如何使用图学习工具。基于自监督的上下文学习，我们使用ChatGPT来注释和扩展一个大型图推理数据集，数据集中包含不同外部图学习工具的API调用，这些数据集也将经过选择性过滤进行后处理。通过github页面¹¹1https://github.com/jwzhanggy/Graph_Toolformer/tree/main/data，我们已向社区发布了本文中使用的图原始数据集和生成的图推理提示数据集，以供未来探索。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Extensive Experimental Studies: We have extensively tested the effectiveness
    of our proposed Graph-ToolFormer with various graph reasoning based application
    tasks studied in the real-world, which include the most basic graph data loading
    and general graph property computation tasks, as well as some more advanced ones.
    Specifically, we study several challenging advanced graph reasoning tasks in the
    experiments, which include paper topic inference in bibliographic networks, molecular
    graph function prediction, online social network community detection, personalized
    sequential recommendation in recommender systems and knowledge graph entity and
    relation reasoning.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 广泛的实验研究：我们已经广泛测试了我们提出的 Graph-ToolFormer 在各种基于图的推理应用任务中的有效性，这些任务涵盖了最基本的图数据加载和一般图属性计算任务，以及一些更高级的任务。具体来说，我们在实验中研究了几个具有挑战性的高级图推理任务，包括文献网络中的论文主题推断、分子图功能预测、在线社交网络社区检测、推荐系统中的个性化序列推荐以及知识图谱实体和关系推理。
- en: 'The remaining sections of this paper are organized as follows. We will briefly
    introduce the related work in Section [2](#S2 "2\. Related Work ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT").
    The definitions of some terminologies and the formulation of the studied problem
    will be provided in Section [3](#S3 "3\. Notation, Terminology Definition and
    Problem Formulation ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT"). A detailed introduction about the Graph-ToolFormer
    framework will be provided in Section [4](#S4 "4\. Proposed Method ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT").
    The effectiveness of Graph-ToolFormer will be tested with extensive experiments
    on real-world benchmark graph datasets in Section [5](#S5 "5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT").
    Finally, we will conclude this paper in Section [6](#S6 "6\. Conclusion ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")
    and briefly discuss about some potential future exploration directions in Section [7](#S7
    "7\. Future Research Directions ‣ Graph-ToolFormer: To Empower LLMs with Graph
    Reasoning Ability via Prompt Augmented by ChatGPT").'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的其余部分组织如下。我们将在第 [2](#S2 "2\. 相关工作 ‣ Graph-ToolFormer: 通过 ChatGPT 提升 LLM 的图推理能力")
    节简要介绍相关工作。术语的定义和所研究问题的表述将在第 [3](#S3 "3\. 符号、术语定义和问题表述 ‣ Graph-ToolFormer: 通过 ChatGPT
    提升 LLM 的图推理能力") 节提供。第 [4](#S4 "4\. 提出的方法 ‣ Graph-ToolFormer: 通过 ChatGPT 提升 LLM
    的图推理能力") 节将详细介绍 Graph-ToolFormer 框架。第 [5](#S5 "5\. 实验 ‣ Graph-ToolFormer: 通过 ChatGPT
    提升 LLM 的图推理能力") 节将通过在真实世界基准图数据集上的广泛实验测试 Graph-ToolFormer 的有效性。最后，我们将在第 [6](#S6
    "6\. 结论 ‣ Graph-ToolFormer: 通过 ChatGPT 提升 LLM 的图推理能力") 节总结本文，并在第 [7](#S7 "7\.
    未来研究方向 ‣ Graph-ToolFormer: 通过 ChatGPT 提升 LLM 的图推理能力") 节简要讨论一些潜在的未来探索方向。'
- en: 2\. Related Work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 相关工作
- en: In this section, we will discuss about several research topics that are related
    to our Graph-ToolFormer framework proposed in this paper, which include graph
    neural networks, language models, language model based graph learning and prompt
    tuning.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论与我们在本文中提出的 Graph-ToolFormer 框架相关的几个研究主题，包括图神经网络、语言模型、基于语言模型的图学习和提示调整。
- en: 2.1\. Graph Neural Networks
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 图神经网络
- en: Graph neural networks (GNNs) aim to learn the embedding representations of the
    graph structured data. Representative examples of GNNs proposed already include
    GCN ([Kipf_Semi_CORR_16,](#bib.bib19) ) and Graph-Bert ([Zhang2020GraphBertOA,](#bib.bib60)
    ), based on which various extended variants ([Velickovic_Graph_ICLR_18,](#bib.bib51)
    ; [sun2019adagcn,](#bib.bib46) ; [DBLP:journals/corr/abs-1810-05997,](#bib.bib20)
    ) have been introduced as well. As mentioned above, GCN and its variant models
    are all based on the approximated graph convolutional operator ([Hammond_2011,](#bib.bib13)
    ), which may lead to the suspended animation problem ([Zhang2019GResNetGR,](#bib.bib59)
    ) and over-smoothing problem ([Li_Deeper_CORR_18,](#bib.bib23) ) for deep model
    architectures. Theoretic analyses of the causes are provided in ([Li_Deeper_CORR_18,](#bib.bib23)
    ; [Zhang2019GResNetGR,](#bib.bib59) ; [Merve_An_19,](#bib.bib12) ). To handle
    such problems, ([Zhang2019GResNetGR,](#bib.bib59) ) generalizes the graph raw
    residual terms and proposes a method based on graph residual learning; ([Li_Deeper_CORR_18,](#bib.bib23)
    ) proposes to adopt residual/dense connections and dilated convolutions into the
    GCN architecture. Besides the GCN and Graph-Bert based models, several other work
    ([sun2019adagcn,](#bib.bib46) ; [Huang_Inductive_19,](#bib.bib17) ) also seeks
    to involve the recurrent network for deep graph representation learning instead.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图神经网络（GNNs）旨在学习图结构数据的嵌入表示。已经提出的一些代表性GNN示例包括GCN（[Kipf_Semi_CORR_16,](#bib.bib19)）和Graph-Bert（[Zhang2020GraphBertOA,](#bib.bib60)），基于这些模型，已经引入了各种扩展变体（[Velickovic_Graph_ICLR_18,](#bib.bib51)；[sun2019adagcn,](#bib.bib46)；[DBLP:journals/corr/abs-1810-05997,](#bib.bib20)）。如上所述，GCN及其变体模型都基于近似图卷积算子（[Hammond_2011,](#bib.bib13)），这可能导致悬停问题（[Zhang2019GResNetGR,](#bib.bib59)）和过度平滑问题（[Li_Deeper_CORR_18,](#bib.bib23)）在深度模型架构中。有关原因的理论分析见于（[Li_Deeper_CORR_18,](#bib.bib23)；[Zhang2019GResNetGR,](#bib.bib59)；[Merve_An_19,](#bib.bib12)）。为了解决这些问题，（[Zhang2019GResNetGR,](#bib.bib59)）推广了图原始残差项，并提出了一种基于图残差学习的方法；（[Li_Deeper_CORR_18,](#bib.bib23)）建议将残差/密集连接和膨胀卷积引入GCN架构。除了基于GCN和Graph-Bert的模型，其他一些工作（[sun2019adagcn,](#bib.bib46)；[Huang_Inductive_19,](#bib.bib17)）还寻求将递归网络纳入深度图表示学习中。
- en: 2.2\. Language Models
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 语言模型
- en: Since the proposal of Transformer ([Vaswani2017AttentionIA,](#bib.bib50) ),
    large language models (LLMs) have become the dominant deep model for various NLP
    tasks. Assisted with pre-training, the giant tech-companies have also introduced
    their own versions of different LLMs, like BERT from Google ([Devlin2019BERTPO,](#bib.bib8)
    ), BART from Meta ([Lewis2019BARTDS,](#bib.bib22) ), GPT from OpenAI ([Radford2018ImprovingLU,](#bib.bib38)
    ; [Radford2019LanguageMA,](#bib.bib39) ; [Brown2020LanguageMA,](#bib.bib5) ),
    ELMo from AI2 ([Peters2018DeepCW,](#bib.bib37) ) and MT-DNN from Microsoft ([Liu2019MultiTaskDN,](#bib.bib25)
    ). Many of these LLMs have also been open-sourced with both model algorithm and
    learned parameters released to the community for both research and application
    purposes. One research paper closely related to this work is Toolformer ([Schick2023ToolformerLM,](#bib.bib43)
    ) from Meta, which proposes to incorporate external APIs into language models.
    Equipped with such external APIs, the models will be able to automatically decide
    how to use which tool. Meanwhile, even prior to the Toolformer model, several
    other previous papers ([Parisi2022TALMTA,](#bib.bib35) ; [Mialon2023AugmentedLM,](#bib.bib29)
    ) have also explored to augment language models with external tools.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Transformer（[Vaswani2017AttentionIA,](#bib.bib50)）的提出以来，大型语言模型（LLMs）已成为各种自然语言处理任务中的主流深度模型。借助预训练，巨头科技公司还推出了各自版本的不同LLM，例如谷歌的BERT（[Devlin2019BERTPO,](#bib.bib8)）、Meta的BART（[Lewis2019BARTDS,](#bib.bib22)）、OpenAI的GPT（[Radford2018ImprovingLU,](#bib.bib38)；[Radford2019LanguageMA,](#bib.bib39)；[Brown2020LanguageMA,](#bib.bib5)）、AI2的ELMo（[Peters2018DeepCW,](#bib.bib37)）以及微软的MT-DNN（[Liu2019MultiTaskDN,](#bib.bib25)）。其中许多LLM也已经开源，模型算法和学习参数都已发布到社区，用于研究和应用目的。与此工作密切相关的一篇研究论文是Meta的Toolformer（[Schick2023ToolformerLM,](#bib.bib43)），该论文提出将外部API纳入语言模型。配备这些外部API后，模型将能够自动决定如何使用哪些工具。同时，即使在Toolformer模型之前，还有几篇先前的论文（[Parisi2022TALMTA,](#bib.bib35)；[Mialon2023AugmentedLM,](#bib.bib29)）也探讨了用外部工具增强语言模型的可能性。
- en: 2.3\. Prompt Tuning
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 提示调整
- en: Prompts have been shown to be effective in tuning the pre-trained language models
    with zero-shot or few-shot learning ([Brown2020LanguageMA,](#bib.bib5) ), which
    can help language models learn faster than traditional fine tuning tasks. By now,
    we have witnessed three categories of prompt tuning approaches, i.e.,, discrete
    prompts ([Schick2020ExploitingCF,](#bib.bib44) ), continuous prompts ([Li2021PrefixTuningOC,](#bib.bib24)
    ) and priming ([Brown2020LanguageMA,](#bib.bib5) ). Discrete prompts ([Schick2020ExploitingCF,](#bib.bib44)
    ) reformat data instances with some template text, like,
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 提示已被证明在零样本或少样本学习中有效地调整预训练语言模型([Brown2020LanguageMA,](#bib.bib5) )，这可以帮助语言模型比传统的微调任务更快地学习。到目前为止，我们已经见证了三种提示调优方法，即离散提示([Schick2020ExploitingCF,](#bib.bib44)
    )、连续提示([Li2021PrefixTuningOC,](#bib.bib24) )和初始化([Brown2020LanguageMA,](#bib.bib5)
    )。离散提示([Schick2020ExploitingCF,](#bib.bib44) )通过一些模板文本重新格式化数据实例，例如，
- en: “{ premise } Should we assume that { hypothesis }? [prediction]”.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: “{ premise } 我们应该假设 { hypothesis } 吗？[预测]”。
- en: Discrete prompts will typically tune all parameters of the model. On the other
    hand, continuous prompts ([Li2021PrefixTuningOC,](#bib.bib24) ) will prepend examples
    with embedding vectors of special tokens, which will only update a much smaller
    set of model parameters. Very different from the discrete and continuous prompts,
    priming ([Brown2020LanguageMA,](#bib.bib5) ) initially adopted in GPT-3 will prepend
    several priming examples to the target evaluation example instead, like
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 离散提示通常会调整模型的所有参数。另一方面，连续提示([Li2021PrefixTuningOC,](#bib.bib24) )会在示例前添加特殊标记的嵌入向量，这只会更新模型参数的较小集合。与离散提示和连续提示非常不同的是，初始在GPT-3中采用的初始化([Brown2020LanguageMA,](#bib.bib5)
    )会将多个初始化示例添加到目标评估示例之前，例如
- en: '“Example 1: { sentence 1 } True or False? { label 1 }.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '“示例 1: { sentence 1 } 真还是假？{ label 1 }。'
- en: 'Example 2: { sentence 2 } True or False? { label 2 }.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 2: { sentence 2 } 真还是假？{ label 2 }。'
- en: $\cdots$
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: $\cdots$
- en: 'Example k: { sentence k } True or False? { label k }.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '示例 k: { sentence k } 真还是假？{ label k }。'
- en: 'Evaluation: { eval-sentence } True or False? [prediction].”'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 评估：{ eval-sentence } 真还是假？[预测]。
- en: According to the analysis reported in ([Webson2021DoPM,](#bib.bib54) ), discrete
    prompts works very well in few-shot tuning, continuous prompts have not yet reported
    success in few-shot setting yet, while priming is very costly and seems to work
    well for the largest GPT-3 (175B) model.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 根据([Webson2021DoPM,](#bib.bib54) )中的分析，离散提示在少样本调优中表现非常好，而连续提示在少样本设置中尚未报道成功，而初始化成本很高，似乎对最大的GPT-3
    (175B) 模型效果很好。
- en: 3\. Notation, Terminology Definition and Problem Formulation
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 符号、术语定义和问题公式
- en: In this section, we will first introduce the notations used in this paper. After
    that, we will provide the definitions of several used terminologies used and the
    formulations of the graph reasoning tasks studied in this paper.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将首先介绍本文中使用的符号。之后，我们将提供几个术语的定义以及本文研究的图推理任务的公式。
- en: 3.1\. Basic Notations
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 基本符号
- en: In the sequel of this paper, we will use the lower case letters (e.g., $x$ and
    vector $\mathbf{x}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文的续集中，我们将使用小写字母（例如，`$x$` 和向量`$\mathbf{x}$`）。
- en: 3.2\. Terminology Definitions
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 术语定义
- en: In this paper, we will investigate the reasoning tasks on graph structured data.
    The graph datasets studied in this paper all come from different domains, which
    have very different structures and carry very different properties. Here, in this
    subsection, we will provide the general terminology definitions of these different
    graph structured data studied in this paper.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将研究图结构数据上的推理任务。本文研究的图数据集来自不同领域，这些领域具有非常不同的结构和特性。在这一小节中，我们将提供这些不同图结构数据的通用术语定义。
- en: Definition 0.
  id: totrans-54
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 0。
- en: '(Graph): Generally, the graph studied in this paper can be represented as $G=(\mathcal{V},\mathcal{E})$,
    respectively.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: （图）：一般来说，本文研究的图可以表示为$G=(\mathcal{V},\mathcal{E})$。
- en: Depending on the application domains, the graph data to be studied may have
    very different property and structural information. For some graph, the nodes
    may carry some feature and label information, which can be represented via mappings
    $x:\mathcal{V}\to\mathbbm{R}^{d_{x}}$, respectively.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 根据应用领域的不同，待研究的图数据可能具有非常不同的属性和结构信息。对于某些图，节点可能携带一些特征和标签信息，这可以通过映射`x:\mathcal{V}\to\mathbbm{R}^{d_{x}}`表示。
- en: For the graph data from many domains, like bibliographic network, online social
    network, recommender systems, knowledge graph, there will exist one single large-scale
    graph structure in the dataset, but the graph may contain thousands, millions
    or even billions of nodes and links. Such large-scale graphs can be perfectly
    represented with the above definition. Meanwhile, for the graphs from many other
    domains, like the special graph structures we learn from the discrete math course,
    and the bio-chemical molecular graphs, there will exist a large number of much
    smaller graph instances in the dataset, and each graph instance normally contain
    tens or a few hundred nodes and links instead. To differentiate these two types
    of graph structured data, some existing work ([Zhang2019GraphNN,](#bib.bib57)
    ) also names first categories of graphs as the giant networks and calls the second
    categories of graphs as the small graph instances set. Meanwhile, to represent
    the set of such small-sized graph instances, we introduce the concept of graph
    set as follows.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多领域中的图数据，如文献网络、在线社交网络、推荐系统、知识图谱，数据集中会存在一个单一的大规模图结构，但该图可能包含数千、数百万甚至数十亿个节点和链接。这些大规模图可以通过上述定义完美表示。与此同时，对于来自许多其他领域的图，例如我们从离散数学课程中学习的特殊图结构和生化分子图，数据集中将存在大量较小的图实例，每个图实例通常包含几十个或几百个节点和链接。为了区分这两种类型的图结构数据，一些现有的工作（[Zhang2019GraphNN](#bib.bib57)）也将第一类图命名为巨型网络，将第二类图称为小图实例集。同时，为了表示这类小规模图实例集，我们引入了图集的概念。
- en: Definition 0.
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义0.
- en: '(Graph Set): For the generated special graph instances (to be introduced in
    this paper) and the bio-chemical molecular graph instances, we can represent the
    set of graph instances in these datasets as $\mathcal{G}=\left\{g_{1},g_{2},\cdots,g_{l}\right\}$
    denotes an individual graph instance and it can be represented according to the
    above graph definition.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: （图集）：对于生成的特殊图实例（将在本文中介绍）和生化分子图实例，我们可以将这些数据集中的图实例集表示为$\mathcal{G}=\left\{g_{1},g_{2},\cdots,g_{l}\right\}$，其中单个图实例可以根据上述图定义进行表示。
- en: For some application domains, in the above graph set, each graph instance may
    also have its unique feature and label information, denoting its topological properties
    and tags of the graph instance. Formally, for a graph instance $g_{i}\in\mathcal{G}$,
    respectively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些应用领域，在上述图集中，每个图实例也可能具有其独特的特征和标签信息，表示其拓扑属性和图实例的标签。正式地，对于图实例$g_{i}\in\mathcal{G}$。
- en: '![Refer to caption](img/8394c13296b2610fd5aba7a7b8ee6388.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8394c13296b2610fd5aba7a7b8ee6388.png)'
- en: 'Figure 2\. The Outline of the Graph-ToolFormer Framework. The framework has
    three main parts: (1) prompt data annotation and augmentation with ChatGPT, (2)
    existing pre-trained causal LLMs fine-tuning with the generated prompt dataset,
    and (3) inference of the fine-tuned model for adding graph reasoning API calls
    into statements.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 图-工具生成框架概述。该框架主要有三个部分：（1）使用ChatGPT进行提示数据注释和增强，（2）使用生成的提示数据集对现有的预训练因果语言模型进行微调，以及（3）对微调后的模型进行推理，以将图推理API调用添加到语句中。
- en: 3.3\. Problem Formulation
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 问题定义
- en: 'In this paper, we aim to empower the existing pre-trained LLMs to carry out
    graph reasoning tasks. As introduced before, the graph reasoning tasks studied
    in this paper include (1) basic graph property reasoning, (2) bibliographic paper
    topic reasoning, (3) bio-chemical molecular graph function reasoning, (4) recommender
    system sequential recommendation reasoning, (5) online social network community
    reasoning, and (6) knowledge graph entity and relation reasoning. Specifically,
    these graph reasoning tasks studied in this paper are carefully selected, which
    can be categorized into six types of the most fundamental graph learning problems
    listed as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们旨在赋能现有的预训练语言模型以进行图推理任务。如前所述，本文研究的图推理任务包括（1）基本图属性推理，（2）文献论文主题推理，（3）生化分子图功能推理，（4）推荐系统的序列推荐推理，（5）在线社交网络社区推理，以及（6）知识图谱实体和关系推理。具体而言，本文研究的这些图推理任务经过精心挑选，可以分为六类最基本的图学习问题，列举如下：
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Attribute Calculation: For the tasks like basic graph property reasoning, we
    actually aim to calculate either explicit or implicit attributes of the input
    graph data, ranging from the simple number of nodes/links in the graph, to the
    graph radius and diameter, and the more complex graph periphery and node pairwise
    short path length.'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 属性计算：对于像基本图属性推理这样的任务，我们实际上旨在计算输入图数据的显式或隐式属性，这些属性包括图中的节点/链接的简单数量、图的半径和直径，以及更复杂的图外缘和节点对间的最短路径长度。
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Node Classification: For the bibliographic paper topic reasoning task, we aim
    to predict the topic of the academic papers in the bibliographic network, which
    can be modeled as the node classification task actually. Via the raw features
    of the paper nodes and their nearby neighboring nodes, we can classify the papers
    into different classes, which correspond to the specific topics of these papers.'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 节点分类：对于文献论文主题推理任务，我们的目标是预测文献网络中学术论文的主题，这实际上可以建模为节点分类任务。通过论文节点的原始特征及其附近的邻居节点，我们可以将论文分类为不同的类别，这些类别对应于这些论文的具体主题。
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Classification: For the bio-chemical molecular graph function reasoning
    task, based on the molecular graph structures, we aim to infer the potential functions
    of the bio-chemical molecules, which can be defined as the graph instance classification
    task. Via both the molecular graph structure and raw attributes, we can classify
    the graph instances into different classes, which correspond to different pre-defined
    bio-chemical molecule functions.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图分类：对于生物化学分子图功能推理任务，基于分子图结构，我们的目标是推断生物化学分子的潜在功能，这可以定义为图实例分类任务。通过分子图结构和原始属性，我们可以将图实例分类为不同的类别，这些类别对应于不同的预定义生物化学分子功能。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Link Prediction: For the sequential recommender system reasoning task, based
    on the historical user-item interaction records, we aim to infer the potential
    preferences of users towards certain items in the system, which can be defined
    as the link prediction task (connecting user and item) in graph learning. Depending
    on the recommender system settings, we can either predict the link existence label
    denoting whether the user will be interested in the item or not, or infer the
    potential link weights denoting the rating scores that users will give to the
    items.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 链接预测：对于序列推荐系统推理任务，基于历史用户-项目互动记录，我们的目标是推断用户对系统中某些项目的潜在偏好，这可以定义为图学习中的链接预测任务（连接用户和项目）。根据推荐系统设置，我们可以预测表示用户是否会对项目感兴趣的链接存在标签，或者推断用户对项目给予的评分分数的潜在链接权重。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Partition/Clustering: For the online social network community reasoning
    task, we aim to infer the community structures of online social networks, which
    can be defined as the graph partition/clustering task. Based on the user social
    interaction patterns, we want to partition the users in online social networks
    into different clusters, each of which denote one social community formed by the
    users with very frequent social interactions.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图划分/聚类：对于在线社交网络社区推理任务，我们的目标是推断在线社交网络的社区结构，这可以定义为图划分/聚类任务。基于用户社交互动模式，我们希望将在线社交网络中的用户划分为不同的簇，每个簇代表一个由频繁社交互动的用户组成的社交社区。
- en: •
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Searching: For the knowledge graph reasoning task, we aim to infer the
    potential entities or relations based on the input parameters, which can be modeled
    as the graph searching problem. Starting from the input entity or relation, we
    aim to expand and search for the related entities or relations for generating
    the outputs, that can effectively preserve the desired semantics of the inputs.'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图搜索：对于知识图谱推理任务，我们旨在基于输入参数推断潜在的实体或关系，这可以建模为图搜索问题。从输入实体或关系开始，我们的目标是扩展和搜索相关的实体或关系，以生成有效保留输入语义的输出。
- en: To address these above diverse graph reasoning tasks with one single LLM, we
    propose to include the API calls of external graph learning tools into to the
    graph reasoning statements seamlessly. Based on the above notations, we will design
    a set of graph reasoning API calls for different graph tasks in the real-world.
    Such API calls include both the external graph learning tool name and the parameters,
    which will be surrounded with special tokens to differentiate from regular text.
    Based on a handful human-written prompt examples, with ChatGPT, we will generate
    a large language modeling prompt dataset containing such API calls, which will
    be used for fine-tuning the LLMs, like GPT-J and LLaMA. Such LLMs to be studied
    in this paper have all been pre-trained already and we will only fine-tune them
    with the generated prompt datasets. More information about the technical details
    on address these tasks will be introduced in the following methodology section.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了用一个单一的LLM来解决上述多样的图推理任务，我们提议将外部图学习工具的API调用无缝地融入图推理声明中。基于上述标记，我们将为现实世界中的不同图任务设计一套图推理API调用。这些API调用包括外部图学习工具名称和参数，并将用特殊符号包围，以区别于常规文本。基于少量人工编写的提示示例，我们将利用ChatGPT生成包含这些API调用的大型语言建模提示数据集，用于微调LLMs，如GPT-J和LLaMA。本文中研究的这些LLMs都已经过预训练，我们只会使用生成的提示数据集对它们进行微调。有关解决这些任务的技术细节将在以下方法部分介绍。
- en: 4\. Proposed Method
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4. 提议的方法
- en: 'In this section, we will introduce the Graph-ToolFormer framework proposed
    in this paper. At the beginning, in Section [4.1](#S4.SS1 "4.1\. Framework Outline
    ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"), we will first briefly describe the
    Graph-ToolFormer framework outline for readers. After that, we will talk about
    the graph reasoning API call general representations in Section [4.2](#S4.SS2
    "4.2\. Prompts with API Calls ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower
    LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), and introduce
    the specific graph reasoning task oriented API calls in Section [4.3](#S4.SS3
    "4.3\. Graph Reasoning Oriented Prompts ‣ 4\. Proposed Method ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT").
    Based on the hand-crafted graph reasoning prompt examples, we will introduce how
    to use the ChatGPT to augment the prompt dataset in Section [4.4](#S4.SS4 "4.4\.
    Prompt Augmentation with ChatGPT ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To
    Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"). Detailed
    information about the language model fine-tuning with the augmented prompt datasets
    will be introduced in Section [4.5](#S4.SS5 "4.5\. LLMs Fine-Tuning for Graph
    Reasoning ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph
    Reasoning Ability via Prompt Augmented by ChatGPT"). Meanwhile, to also allow
    Graph-ToolFormer to handle some basic Q&A for graph reasoning, we will also introduce
    a few number of graph reasoning Q&A prompts in Section [4.6](#S4.SS6 "4.6\. Graph
    Reasoning Q&A Prompts ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs
    with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), which will be
    merged into the statement prompts for LLM fine-tuning. Finally, based on the output
    statements with API calls generated by the language models, the graph reasoning
    tasks oriented API call parsing, execution, graph task reasoning and output post-processing
    will be introduced in Section [4.7](#S4.SS7 "4.7\. LLMs Inference and Graph Reasoning
    Query Parsing, Execution and Post-Processing ‣ 4\. Proposed Method ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT").'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们将介绍本文提出的Graph-ToolFormer框架。首先，在第[4.1节](#S4.SS1 "4.1\. 框架概述 ‣ 4\. 提议的方法
    ‣ Graph-ToolFormer: 通过ChatGPT增强提示来赋能LLMs以图谱推理能力")中，我们将简要描述Graph-ToolFormer框架的概述。之后，我们将在第[4.2节](#S4.SS2
    "4.2\. 带API调用的提示 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过ChatGPT增强提示来赋能LLMs以图谱推理能力")讨论图谱推理API调用的一般表示，并在第[4.3节](#S4.SS3
    "4.3\. 面向图谱推理的提示 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过ChatGPT增强提示来赋能LLMs以图谱推理能力")介绍特定的图谱推理任务导向的API调用。基于手工制作的图谱推理提示示例，我们将在第[4.4节](#S4.SS4
    "4.4\. 使用ChatGPT增强提示 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过ChatGPT增强提示来赋能LLMs以图谱推理能力")介绍如何使用ChatGPT来增强提示数据集。关于使用增强提示数据集进行语言模型微调的详细信息将在第[4.5节](#S4.SS5
    "4.5\. 图谱推理的LLMs微调 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过ChatGPT增强提示来赋能LLMs以图谱推理能力")中介绍。同时，为了使Graph-ToolFormer也能处理一些基本的图谱推理问答，我们还将在第[4.6节](#S4.SS6
    "4.6\. 图谱推理问答提示 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过ChatGPT增强提示来赋能LLMs以图谱推理能力")中介绍一些图谱推理问答提示，这些提示将合并到LLM微调的声明提示中。最后，基于语言模型生成的带API调用的输出语句，第[4.7节](#S4.SS7
    "4.7\. LLMs推理和图谱推理查询解析、执行及后处理 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过ChatGPT增强提示来赋能LLMs以图谱推理能力")将介绍图谱推理任务导向的API调用解析、执行、图谱任务推理和输出后处理。'
- en: 4.1\. Framework Outline
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 框架概述
- en: 'In Figure [2](#S3.F2.1 "Figure 2 ‣ 3.2\. Terminology Definitions ‣ 3\. Notation,
    Terminology Definition and Problem Formulation ‣ Graph-ToolFormer: To Empower
    LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), we provide
    an outline of the Graph-ToolFormer framework illustrating the internal functional
    components and pipeline of Graph-ToolFormer. According to the framework outline,
    based on the hand-crafted instructions and a handful number of prompt examples,
    we use ChatGPT to annotate and augment a large prompt dataset about graph reasoning
    API call statements. With the generated prompt dataset, we will fine-tune the
    existing pre-trained causal LLMs, such as GPT-J ([gpt-j,](#bib.bib53) ; [gpt-j-8bit,](#bib.bib11)
    ) and LLaMA ([Touvron2023LLaMAOA,](#bib.bib49) ) to teach them how to use the
    external graph reasoning tools. With both LoRA (Low-Rank Adaptation) ([Hu2021LoRALA,](#bib.bib15)
    ) and 8-bit Adam and the model quantization techniques ([Dettmers20218bitOV,](#bib.bib7)
    ), Graph-ToolFormer can be fine-tuned on GPUs with very small memory space, such
    as Nvidia GeForce RTX 4090 (24GB RAM) and even Nvidia GeForce RTX 1080Ti (11GB
    RAM). The fine-tuned Graph-ToolFormer will be used for inference purposes. Given
    the input query statements and questions, Graph-ToolFormer will add the corresponding
    graph reasoning API calls into the output statements at the most appropriate positions
    automatically. Detailed information about these mentioned components and steps
    in building Graph-ToolFormer will be introduced in detail in the following subsections.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [2](#S3.F2.1 "图 2 ‣ 3.2\. 术语定义 ‣ 3\. 符号、术语定义和问题表述 ‣ Graph-ToolFormer：通过ChatGPT增强的提示来赋能LLMs的图推理能力")中，我们提供了Graph-ToolFormer框架的概述，展示了Graph-ToolFormer的内部功能组件和流程。根据框架概述，基于手工制作的指令和少量提示示例，我们使用ChatGPT来标注和增强关于图推理API调用语句的大型提示数据集。通过生成的提示数据集，我们将对现有的预训练因果LLMs进行微调，例如GPT-J（[gpt-j,](#bib.bib53)；[gpt-j-8bit,](#bib.bib11)）和LLaMA（[Touvron2023LLaMAOA,](#bib.bib49)），以教会它们如何使用外部图推理工具。通过LoRA（低秩适配）（[Hu2021LoRALA,](#bib.bib15)）以及8位Adam和模型量化技术（[Dettmers20218bitOV,](#bib.bib7)），Graph-ToolFormer可以在内存空间非常小的GPU上进行微调，如Nvidia
    GeForce RTX 4090（24GB RAM）甚至Nvidia GeForce RTX 1080Ti（11GB RAM）。微调后的Graph-ToolFormer将用于推理目的。给定输入查询语句和问题，Graph-ToolFormer将在输出语句中自动在最合适的位置添加相应的图推理API调用。关于构建Graph-ToolFormer时提到的这些组件和步骤的详细信息将在以下小节中详细介绍。
- en: Table 1\. A summary of API call examples for basic graph Loading and property
    reasoning studied in this paper. In this table, we use notations $GL(\cdot)$]”
    and use the notation “toolx:desired_property” to denote the reasoning of the desired
    properties with the toolx graph toolkit (to be introduced in the following experiment
    section).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表1\. 本文研究的基本图加载和属性推理的API调用示例总结。在此表中，我们使用符号$GL(\cdot)$]”并使用符号“toolx:desired_property”来表示使用toolx图工具包（将在接下来的实验部分介绍）进行期望属性的推理。
- en: '| Tasks | API Call Templates | Prompt Examples |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | API调用模板 | 提示示例 |'
- en: '| Inputs | Outputs |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 输入 | 输出 |'
- en: '| Graph Data Loading | ${GL(\textit{file-path})}$ | “The structure of the molecular
    graph of the benzene ring contains a hexagon.” | “The structure of the [GL(file-path:“./graphs/benzene-ring”)]
    molecular graph of the benzene ring contains a hexagon.” |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 图数据加载 | ${GL(\textit{file-path})}$ | “苯环的分子图结构包含一个六边形。” | “[GL(file-path:“./graphs/benzene-ring”)]苯环的分子图结构包含一个六边形。”
    |'
- en: '| ${GL(\textit{file-path},\textit{node-subset},\textit{link-subset})}$ | “There
    exist a carbon-oxygen double bond in the Acetaldehyde molecular graph.” | “There
    exist a [GL(file-path:“./graphs/acetaldehyde”, node-subset:“all related nodes”,
    link-subse:{(C=O)})] carbon-oxygen double bond in the Acetaldehyde molecular graph.”
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ${GL(\textit{file-path},\textit{node-subset},\textit{link-subset})}$ | “在乙醛分子图中存在一个碳氧双键。”
    | “在乙醛分子图中存在一个[GL(file-path:“./graphs/acetaldehyde”, node-subset:“all related
    nodes”, link-subse:{(C=O)})]碳氧双键。” |'
- en: '| ${GL(\textit{file-path})\to r}$] Lollipop graph looks like a spoon.” |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| ${GL(\textit{file-path})\to r}$] 棒棒糖图看起来像一个勺子。” |'
- en: '| Graph Property Reasoning | $GR(graph,\text{``}order\text{''''})\to r$] nodes
    in the lollipop graph.” |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 图属性推理 | $GR(graph,\text{``}order\text{''''})\to r$] 棒棒糖图中的节点。” |'
- en: '| $GR(graph,\text{``}size\text{''''})\to r$] links, nodes in the example lollipop
    graph are all connected.” |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}size\text{''''})\to r$] 链接，示例中的棒棒糖图中的所有节点都是连接的。” |'
- en: '| $GR(graph,\text{``}density\text{''''},is\text{-}directed)\to r$].” |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}density\text{''''},is\text{-}directed)\to r$].” |'
- en: '| $GR(graph,\text{``}eccentricity\text{''''})\to r$] for many nodes in the
    lollipop graph.” |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}eccentricity\text{''''})\to r$] for many nodes in the
    lollipop graph.” |'
- en: '| $GR(graph,\text{``}eccentricity\text{''''},\text{node-subset})\to r$].” |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}eccentricity\text{''''},\text{node-subset})\to r$].” |'
- en: '| $GR(graph,\text{``}radius\text{''''})\to r$].” |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}radius\text{''''})\to r$].” |'
- en: '| $GR(graph,\text{``}center\text{''''})\to r$].” |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}center\text{''''})\to r$].” |'
- en: '| $GR(graph,\text{``}shortest\text{-}path\text{''''},node_{1},node_{2})\to
    r$].” |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}shortest\text{-}path\text{''''},node_{1},node_{2})\to
    r$].” |'
- en: '| $GR(graph,\text{``}avg\text{-}shortest\text{-}path\text{''''})\to r$].” |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}avg\text{-}shortest\text{-}path\text{''''})\to r$].” |'
- en: '| $GR(graph,\text{``}diameter\text{''''})\to r$] due to the long ‘tail’.” |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}diameter\text{''''})\to r$] due to the long ‘tail’.” |'
- en: '| $GR(graph,\text{``}periphery\text{''''})\to r$].” |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| $GR(graph,\text{``}periphery\text{''''})\to r$].” |'
- en: 4.2\. Prompts with API Calls
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 带有 API 调用的提示
- en: In this paper, we can represent the API calls of external graph learning tools
    as $f(args)$, which will be frequently used in the following part of this paper.
    Instead of merely generating the external API calls as the output, we propose
    to inset the API calls into the generated output statements instead, which allows
    the LLMs to handle and respond the graph reasoning tasks with regular conversations
    via texts.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们可以将外部图学习工具的 API 调用表示为 $f(args)$，这将在本文的后续部分中频繁使用。我们建议将 API 调用插入到生成的输出语句中，而不仅仅是生成外部
    API 调用，这样可以让 LLMs 通过文本的常规对话处理和响应图推理任务。
- en: Formally, to insert the API calls into the output statements, we can represent
    the sequence of tokens for API call $c=(f,args)$ as
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，为了将 API 调用插入输出语句中，我们可以将 API 调用 $c=(f,args)$ 的标记序列表示为
- en: '| (1) |  | $$\mathbf{s}(c)={{\textsc{ |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $$\mathbf{s}(c)={{\textsc{ |  |'
- en: or
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '| (2) |  |  |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $$\mathbf{s}(c_{1}&#124;c_{2})={{\textsc{ |  |'
- en: or just simply as
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 或者简单地表示为
- en: '| (4) |  |  |  |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $$\displaystyle\mathbf{s}(c_{1},c_{2})={{\textsc{ |  |'
- en: '| (6) |  | 
    |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $$\mathbf{s}\left(c_{1}&#124;(c_{2}&#124;c_{3})\right)={{\textsc{
    |  |'
- en: where $c_{3}=(f_{3},args_{3})$ denotes the notation of a third API call.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c_{3}=(f_{3},args_{3})$ 表示第三个API调用的符号。
- en: Such graph reasoning function API calls will be inserted into statements for
    LLMs fine-tuning later. Without modifying the LLMs’ vocabulary set and the pre-trained
    tokenizer, in implementation, we can replace the special tokens “¡API¿”, “¡/API¿”
    and $\to$ with some less frequently used tokens like “[”, “]” and “-¿” instead.
    In this paper, we will study several very different graph reasoning tasks involving
    diverse graph learning API calls, which will be introduced in detail in the following
    subsection for readers.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图推理功能的API调用将在后续的LLMs微调中插入到语句中。在实现中，我们可以用一些不太常用的标记如“[”、 “]”和“-¿”替换特殊标记“¡API¿”、
    “¡/API¿” 和 $\to$，而无需修改LLMs的词汇集和预训练分词器。本文将研究几个非常不同的图推理任务，涉及各种图学习API调用，这些将在接下来的小节中详细介绍给读者。
- en: 4.3\. Graph Reasoning Oriented Prompts
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 图推理导向提示
- en: 'We will study several graph reasoning tasks in this paper with Graph-ToolFormer,
    which include both the very basic tasks, like the general graph property reasoning,
    and more advanced ones, like the reasoning tasks on graphs from different specific
    application domains. As introduced before in Section [3](#S3 "3\. Notation, Terminology
    Definition and Problem Formulation ‣ Graph-ToolFormer: To Empower LLMs with Graph
    Reasoning Ability via Prompt Augmented by ChatGPT"), these graph reasoning tasks
    studied in this paper are all carefully selected, which can be categorized into
    different types of fundamental graph learning tasks, e.g., graph attribute calculation,
    node classification, graph classification, link prediction, graph partition/clustering
    and graph searching. All these fundamental graph learning tasks have extensive
    applications in real-world graph data reasoning tasks. Besides the tasks studied
    in this paper, with minor changes to the API calls, we can also apply the Graph-ToolFormer
    to other graph reasoning related application tasks as well.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将在本文中研究几个图推理任务，使用Graph-ToolFormer，包括一些非常基础的任务，如一般的图属性推理，以及更高级的任务，如来自不同特定应用领域的图推理任务。如前文第[3](#S3
    "3\. Notation, Terminology Definition and Problem Formulation ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")节所述，本文研究的这些图推理任务都是经过精心挑选的，可以归类为不同类型的基础图学习任务，例如图属性计算、节点分类、图分类、链接预测、图分割/聚类和图搜索。所有这些基础图学习任务在现实世界图数据推理任务中都有广泛的应用。除了本文研究的任务，通过对API调用进行小的修改，我们还可以将Graph-ToolFormer应用于其他与图推理相关的应用任务。'
- en: 4.3.1\. Graph Data Loading
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 图数据加载
- en: Different from texts and images, the graph data we have in the real-world may
    have a relatively larger size, extensively connected structures and complex raw
    attributes. Except for some small-sized hand-crafted graph examples, it is almost
    impossible to manually type in the graph structured data as a sequence of token
    inputs to LLMs for reasoning. Therefore, in this paper, we propose to empower
    the Graph-ToolFormer model with the ability to automatically load the desired
    graph data from offline files or online repositories based on the provided the
    dataset name, local file path or online repository URL link.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 与文本和图像不同，我们在现实世界中的图形数据可能具有较大的规模、广泛的连接结构和复杂的原始属性。除了少量小型的手工制作的图形示例外，几乎不可能将图形结构化数据作为令牌输入序列手动输入到LLMs中进行推理。因此，本文提出了将Graph-ToolFormer模型赋予从离线文件或在线仓库中自动加载所需图形数据的能力，基于提供的数据集名称、本地文件路径或在线仓库URL链接。
- en: Technically, the first API call that we will introduce in this paper is for
    graph data loading, which can load either the whole graph or just a subgraph involving
    one or a few nodes and links. Specifically, we can represent the graph loading
    API call as
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，我们将在本文中介绍的第一个API调用用于图形数据加载，可以加载整个图形或仅加载涉及一个或几个节点和链接的子图。具体来说，我们可以将图形加载API调用表示为
- en: '| (10) |  |  |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $${\textsc{ |  |'
- en: where the loaded the graph can also be referred to by notation $G_{l}$ as an
    example to introduce the graph property reasoning APIs for readers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，加载的图也可以用符号 $G_{l}$ 作为示例来介绍图属性推理 API。
- en: 'Order and Size: Formally, given a graph, like the loaded lollipop graph $G_{l}=(\mathcal{V},\mathcal{E})$.
    We can represent the API calls for reasoning the order and size properties of
    the lollipop graph as'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序和大小：形式上，给定一个图，如加载的棒棒糖图 $G_{l}=(\mathcal{V},\mathcal{E})$。我们可以表示推理棒棒糖图的顺序和大小属性的
    API 调用为
- en: '| (12) |  | $1$2 |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $1$2 |  |'
- en: '| (13) |  | $1$2 |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $1$2 |  |'
- en: 'If the lollipop graph has been pre-loaded via other API calls already and can
    be referred to as $G_{l}$, the above API calls can also be simplified as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果棒棒糖图已经通过其他 API 调用预加载并可以被称为 $G_{l}$，上述 API 调用也可以简化如下：
- en: '| (14) |  |  |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $$\displaystyle{{\textsc{ |  |'
- en: where the notation $GR()$” of the API calls is actually optional, inclusion
    of which depends on both the reasoning context and application task.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，API 调用中的符号 $GR()$ 实际上是可选的，其是否包含取决于推理上下文和应用任务。
- en: 'Density: Graph density denotes the ratio of existing links in a graph compared
    with the maximal number of potential links among nodes in a graph. If the input
    lollipop graph $G_{l}=(\mathcal{V},\mathcal{E})$. Formally, the API calls that
    can be used for computing the density of graph can be represented as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 密度：图密度表示图中现有链接与图中节点之间最大潜在链接数的比率。如果输入的棒棒糖图是 $G_{l}=(\mathcal{V},\mathcal{E})$。形式上，可以用以下
    API 调用来计算图的密度：
- en: '| (16) |  | $1$2 |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $1$2 |  |'
- en: where the boolean “is-directed” parameter differentiates directed graph from
    undirected ones in the density calculation.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 其中布尔参数“is-directed”在密度计算中区分有向图和无向图。
- en: 'Shortest Path: The shortest path between two nodes in a graph is a path of
    shortest possible length connecting them via the nodes and links in the graph.
    The API call for reasoning the length of the shortest path from $node_{1}$ in
    a graph can be represented as'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最短路径：图中两个节点之间的最短路径是通过图中的节点和链接连接它们的最短路径。推理从图中 $node_{1}$ 到目标节点的最短路径长度的 API 调用可以表示为
- en: '| (17) |  | $1$2 |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $1$2 |  |'
- en: Meanwhile, the average length of shortest path for all nodes in the graph can
    be obtained via the following API call instead
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，图中所有节点的最短路径的平均长度可以通过以下 API 调用获得
- en: '| (18) |  | $1$2 |  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $1$2 |  |'
- en: 'Besides the average shortest path length, we can also reason for the largest
    shortest path length and the smallest shortest path length of a graph as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 除了平均最短路径长度外，我们还可以推理图的最大最短路径长度和最小最短路径长度，如下所示：
- en: '| (19) |  | $1$2 |  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $1$2 |  |'
- en: '| (20) |  | $1$2 |  |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $1$2 |  |'
- en: 'Eccentricity: Given a connected graph, like the lollipop graph $G_{l}=(\mathcal{V},\mathcal{E})$
    in the graph. According to such a definition, for disconnected graph, all nodes
    are defined to have infinite eccentricity. We can compute the eccentricity either
    for the whole graph (i.e., for all nodes in the graph) or for specific node(s)
    via the following two API calls:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 离心率：给定一个连通图，比如图中的棒棒糖图 $G_{l}=(\mathcal{V},\mathcal{E})$。根据这样的定义，对于断开的图，所有节点都被定义为具有无限离心率。我们可以通过以下两个
    API 调用计算整个图的离心率（即图中的所有节点）或特定节点的离心率：
- en: '| (21) |  |  |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $$\displaystyle{{\textsc{ |  |'
- en: 'Diameter: The diameter of a graph denotes the “longest shortest path” between
    any two nodes in the graph, whose API call can be represented as'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 直径：图的直径表示图中任意两个节点之间的“最长最短路径”，其 API 调用可以表示为
- en: '| (23) |  | 
    actually.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 其结果将等于上述 API 调用的结果 $${{\textsc{ 实际上。
- en: 'Radius: Graph radius denotes the is the minimum graph eccentricity of any node
    in a graph. A disconnected graph therefore has infinite radius. The API call for
    computing a graph radius can be represented as'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 半径：图的半径表示图中任何节点的最小离心率。因此，断开的图具有无限半径。计算图半径的 API 调用可以表示为
- en: '| (24) |  |  |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $$\displaystyle{{\textsc{ |  |'
- en: 'Periphery: The periphery of a graph is the subgraph of the graph induced by
    nodes that have the eccentricities equal to the graph diameter, whose API call
    can be represented as'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 边界：图的边界是由那些离心率等于图直径的节点所诱导出的子图，其 API 调用可以表示为
- en: '| (26) |  |  |  |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $${\textsc{ |  |'
- en: 'Each paper is represented as a node $v_{i}\in\mathcal{V}$. The raw feature
    vector includes the textual information about the paper (like its title or abstract),
    and its label vector indicates the topics of the paper. Existing graph neural
    networks (GNNs) infer the paper topics by learning their representations with
    both raw features and connected neighbors’ information ([Kipf_Semi_CORR_16,](#bib.bib19)
    ; [Zhang2020GraphBertOA,](#bib.bib60) ), which can be further used to infer the
    topic label vector. For the Graph-ToolFormer model introduced in this paper, we
    will use the pre-trained Graph-Bert ([Zhang2020GraphBertOA,](#bib.bib60) ) as
    the default topic inference model for bibliographic networks. Based on the above
    descriptions, we can represent the paper topic reasoning via graph neural network
    model with the following API call:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 每篇论文被表示为一个节点$v_{i}\in\mathcal{V}$。原始特征向量包括有关论文的文本信息（如标题或摘要），而其标签向量表示论文的主题。现有的图神经网络（GNNs）通过学习其表示，包括原始特征和连接邻居的信息（[Kipf_Semi_CORR_16,](#bib.bib19)
    ; [Zhang2020GraphBertOA,](#bib.bib60) ），从而推断论文主题，这些信息可以进一步用于推断主题标签向量。对于本文介绍的Graph-ToolFormer模型，我们将使用预训练的Graph-Bert（[Zhang2020GraphBertOA,](#bib.bib60)
    ）作为书目网络的默认主题推断模型。基于上述描述，我们可以通过以下API调用来表示论文主题推理：
- en: '| (28) |  | $1$2 |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $1$2 |  |'
- en: The function notation “$GR(\cdot,\text{``}graph\text{-}bert\text{:}topic\text{''},\cdot)$”
    denotes it is a paper topic reasoning API with the Graph-Bert model ([Zhang2020GraphBertOA,](#bib.bib60)
    ). Actually, the Graph-ToolFormer framework proposed in this paper is a general
    framework. Besides the Graph-Bert model, many other existing graph neural network
    models can also be used here for academic paper topic inference as well. Based
    on the provided source code, the readers can customize the Graph-ToolFormer to
    include more different graph models that can be used for accomplishing their own
    graph reasoning tasks.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 函数符号“$GR(\cdot,\text{``}graph\text{-}bert\text{:}topic\text{''},\cdot)$”表示这是一个使用Graph-Bert模型的论文主题推理API（[Zhang2020GraphBertOA,](#bib.bib60)）。实际上，本文提出的Graph-ToolFormer框架是一个通用框架。除了Graph-Bert模型外，许多其他现有的图神经网络模型也可以用于学术论文主题推断。基于提供的源代码，读者可以自定义Graph-ToolFormer，以包括更多不同的图模型，用于完成他们自己的图推理任务。
- en: 'Protein Molecule Function Reasoning: Protein and chemical molecule function
    inference ([doi:10.1142/S0219633602000117,](#bib.bib52) ) has been a classic problem
    studied in bio-chemical research for decades, which has fundamental applications
    in the real-world, such as helping design some new drugs for curing some existing
    rare diseases. Protein function inference is not an easy task, because homologous
    proteins often have several different functions at the same time. Also such a
    prediction needs to be fine-tuned with respect to some mutations but robust with
    respect to others. Researchers have been exploring on this problem with machine
    learning models, and have also developed a relatively large protein function database
    ([10.1093/nar/gkaa1074,](#bib.bib48) ) already. However, compared with the number
    of protein existing in the real world, the specific proteins with known functions
    included in the database is still very limited. In graph learning, inferring the
    function of protein molecules based on its structure has also be extensively studied
    as well. Therefore, in this part, we also include it as a graph reasoning task
    into Graph-ToolFormer as well.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 蛋白质分子功能推理：蛋白质和化学分子功能推断（[doi:10.1142/S0219633602000117,](#bib.bib52)）是生物化学研究中研究了几十年的经典问题，这在现实世界中具有基础应用，比如帮助设计一些用于治疗现有稀有疾病的新药。蛋白质功能推断并非易事，因为同源蛋白质通常会同时具有几种不同的功能。此外，这种预测需要针对某些突变进行微调，但对于其他突变要保持鲁棒性。研究人员一直在利用机器学习模型探索这个问题，并且已经开发了一个相对较大的蛋白质功能数据库（[10.1093/nar/gkaa1074,](#bib.bib48)）。然而，与现实世界中存在的蛋白质数量相比，数据库中具有已知功能的特定蛋白质仍然非常有限。在图学习中，基于蛋白质分子结构推断其功能也已经得到了广泛的研究。因此，在这一部分，我们还将其作为图推理任务纳入Graph-ToolFormer中。
- en: 'Different from the bibliographic network, the protein molecular graphs have
    much smaller sizes and there will also exist multiple such graph instances in
    the dataset. What’s more, the features and labels of protein molecular graphs
    are both about the whole molecular graph, not about the individual nodes anymore.
    As introduced in Section [3.2](#S3.SS2 "3.2\. Terminology Definitions ‣ 3\. Notation,
    Terminology Definition and Problem Formulation ‣ Graph-ToolFormer: To Empower
    LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), we can represent
    the set of studied protein molecular graphs as $\mathcal{G}=\{g_{1},g_{2},\cdots,g_{l}\}$,
    which can be loaded with the following graph loading API call:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '与文献网络不同，蛋白质分子图的规模要小得多，并且数据集中还会存在多个这样的图实例。此外，蛋白质分子图的特征和标签都与整个分子图有关，而不再是与单个节点相关。如在第[3.2节](#S3.SS2
    "3.2\. 术语定义 ‣ 3\. 符号、术语定义及问题公式化 ‣ Graph-ToolFormer: 通过ChatGPT增强提示的图推理能力")中介绍的，我们可以将研究的蛋白质分子图集表示为$\mathcal{G}=\{g_{1},g_{2},\cdots,g_{l}\}$，可以通过以下图加载API调用来加载：'
- en: '| (29) |  |  |  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $${\textsc{ |  |'
- en: 'For each user $u_{j}$, based on the historical interaction records (before
    the current timestamp), we can learn the embedding representations of them, which
    will be used to infer the label between them in the future. Depending on the modeling
    approach, the label vector can indicate either whether the user will purchase
    the item or not (i.e., binary classification task) or the rating score of the
    user for the item (i.e., the regression task). Regardless of the specific modeling
    settings, we can represent the recommender system reasoning API call in LLMs as
    follows:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个用户 $u_{j}$，基于历史交互记录（当前时间戳之前），我们可以学习它们的嵌入表示，这将用于推断未来它们之间的标签。根据建模方法，标签向量可以指示用户是否会购买该项目（即，二分类任务）或用户对该项目的评分（即，回归任务）。无论具体的建模设置如何，我们可以将推荐系统推理
    API 调用在 LLMs 中表示为：
- en: '| (32) |  | $1$2 |  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $1$2 |  |'
- en: 'which will return either the probability scores that the user $u_{j}$ recommended
    items with the following API call:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回用户 $u_{j}$ 推荐的项目的概率分数，使用以下 API 调用：
- en: '| (33) |  | $1$2 |  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| (33) |  | $1$2 |  |'
- en: where the notation $k$ denotes a hyper-parameter to be extracted from the input
    statements for the recommendation reasoning.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，符号 $k$ 表示一个超参数，需要从输入语句中提取用于推荐推理。
- en: 'Online Social Network Community Reasoning: Online social networks ([10.1145/1298306.1298311,](#bib.bib31)
    ), like Facebook, Twitter and Tiktok, provide different online services for their
    users to facilitate their online socialization with friends, family members and
    colleagues. Users in online social networks tend to interact more frequently with
    their online friends, and they will naturally form their online social communities
    based on their online social behaviors. Reasoning for the social communities of
    users in online social networks is a complicated problem. In this part, we will
    introduce the API calls to empower LLMs to detect social communities from online
    social networks.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在线社交网络社区推理：在线社交网络 ([10.1145/1298306.1298311,](#bib.bib31))，如 Facebook、Twitter
    和 Tiktok，为用户提供不同的在线服务，以促进他们与朋友、家人和同事的在线社交。在线社交网络中的用户往往更频繁地与在线朋友互动，他们自然会基于在线社交行为形成自己的在线社交社区。在线社交网络用户社区的推理是一个复杂的问题。在这一部分，我们将介绍
    API 调用，以使 LLM 能够从在线社交网络中检测社交社区。
- en: 'Formally, we can represent the online social network studied in this paper
    as $G=(\mathcal{V},\mathcal{E})$ denotes the social interactions among the users
    in the network. The online social network data can be loaded with the following
    API call:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，我们可以将本文研究的在线社交网络表示为 $G=(\mathcal{V},\mathcal{E})$，表示网络中用户之间的社交互动。可以通过以下
    API 调用加载在线社交网络数据：
- en: '| (34) |  |  |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| (35) |  | $$\displaystyle{{\textsc{ |  |'
- en: '| (36) |  |  |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| (37) |  | $${\textsc{ |  |'
- en: 'Such a loaded knowledge graph structure can be effectively used in the reasoning
    tasks to infer the potential head-entities, the relations between a pair of entities,
    and the tail-entities. Formally, we can represent the knowledge graph reasoning
    API calls used in this paper as:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加载的知识图谱结构可以在推理任务中有效地用来推断潜在的头实体、实体对之间的关系和尾实体。正式地，我们可以将本文中使用的知识图谱推理 API 调用表示为：
- en: '| (38) |  | $1$2 |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| (38) |  | $1$2 |  |'
- en: '| (39) |  | $1$2 |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| (39) |  | $1$2 |  |'
- en: '| (40) |  | $1$2 |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| (40) |  | $1$2 |  |'
- en: For the $(\textit{head-entity, relation, tail-entity})$ tuples in the knowledge
    graph, given any two of them, we can infer the remaining one based on their learned
    representations. Various pre-trained knowledge graph representation learning models
    can be used to define the function called in the above API. In this paper, we
    will use the TransE ([NIPS2013_1cecc7a7,](#bib.bib4) ) as the default knowledge
    graph embedding and reasoning model.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于知识图谱中的$(\textit{head-entity, relation, tail-entity})$元组，给定其中的任何两个，我们可以根据它们的学习表示推断剩余的一个。可以使用各种预训练的知识图谱表示学习模型来定义上述
    API 调用中使用的函数。在本文中，我们将使用 TransE ([NIPS2013_1cecc7a7,](#bib.bib4) ) 作为默认的知识图谱嵌入和推理模型。
- en: 'A Summary of Advanced Graph Reasoning API Calls: we also provide a summary
    of API call examples of the advanced graph reasoning tasks mentioned above in
    Table [2](#S4.T2 "Table 2 ‣ 4.3.2\. Graph Property Reasoning ‣ 4.3\. Graph Reasoning
    Oriented Prompts ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with
    Graph Reasoning Ability via Prompt Augmented by ChatGPT"). For each of the tasks,
    we provide several different input reasoning statements, and insert the corresponding
    reasoning API calls at the most appropriate positions in the output statements.
    As introduced above, some of the API calls introduced above can be used in different
    ways to reason for different types of desired information, like based on the online
    social network community reasoning results, we can further define the functions
    to reason for the community-count, community-size. Some examples of which have
    also been provided in Table [2](#S4.T2 "Table 2 ‣ 4.3.2\. Graph Property Reasoning
    ‣ 4.3\. Graph Reasoning Oriented Prompts ‣ 4\. Proposed Method ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")
    as well.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '高级图推理 API 调用总结：我们还提供了上述表格[2](#S4.T2 "Table 2 ‣ 4.3.2\. 图属性推理 ‣ 4.3\. 图推理导向提示
    ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过 ChatGPT 增强提示赋能 LLM 图推理能力")中提到的高级图推理任务的 API
    调用示例的总结。对于每个任务，我们提供了几个不同的输入推理语句，并在输出语句中的最合适位置插入相应的推理 API 调用。如上所述，上述介绍的一些 API 调用可以以不同的方式用于推理不同类型的信息，例如基于在线社交网络社区的推理结果，我们可以进一步定义函数来推理社区计数、社区规模等。一些示例也在表格[2](#S4.T2
    "Table 2 ‣ 4.3.2\. 图属性推理 ‣ 4.3\. 图推理导向提示 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过 ChatGPT
    增强提示赋能 LLM 图推理能力")中提供。'
- en: 4.4\. Prompt Augmentation with ChatGPT
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 使用 ChatGPT 增强提示
- en: 'For the prompt examples provided in Table [1](#S4.T1 "Table 1 ‣ 4.1\. Framework
    Outline ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT") and Table [2](#S4.T2 "Table 2 ‣ 4.3.2\.
    Graph Property Reasoning ‣ 4.3\. Graph Reasoning Oriented Prompts ‣ 4\. Proposed
    Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt
    Augmented by ChatGPT"), they can only cover a handful number of examples about
    how to use the API calls for different graph reasoning tasks. Such a small number
    of instances are not sufficient for the fine-tuning of the existing LLMs. In this
    paper, we propose to augment the prompt instances with ChatGPT (gpt-3.5-turbo),
    which has demonstrated excellent few-shot and zero-shot in-context learning ability
    ([Brown2020LanguageMA,](#bib.bib5) ) in many different language learning tasks
    already.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '对于表格[1](#S4.T1 "Table 1 ‣ 4.1\. 框架概述 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过 ChatGPT
    增强提示赋能 LLM 图推理能力")和表格[2](#S4.T2 "Table 2 ‣ 4.3.2\. 图属性推理 ‣ 4.3\. 图推理导向提示 ‣ 4\.
    提议的方法 ‣ Graph-ToolFormer: 通过 ChatGPT 增强提示赋能 LLM 图推理能力")中提供的提示示例，它们只能覆盖少量关于如何使用
    API 调用进行不同图推理任务的示例。这些少量的实例不足以对现有 LLM 进行微调。本文中，我们提议使用 ChatGPT (gpt-3.5-turbo) 来增强提示实例，该模型在许多不同的语言学习任务中已经展示了优秀的少样本和零样本上下文学习能力
    ([Brown2020LanguageMA,](#bib.bib5) )。'
- en: 4.4.1\. Graph Loading Prompt Dataset Generation
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 图加载提示数据集生成
- en: Similar to ([Ouyang2022TrainingLM,](#bib.bib34) ), to help the generation of
    prompt examples, we also provide a detailed instruction for ChatGPT to specify
    its system role. Here, we can take the graph data loading API call as an example.
    The instruction together with the prompt examples fed to ChatGPT are provided
    as follows. Based on both the instruction and prompt examples, we will ask the
    ChatGPT to generate the graph data loading prompt dataset.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 ([Ouyang2022TrainingLM,](#bib.bib34) )，为了帮助生成提示示例，我们还提供了一个详细的说明，让 ChatGPT
    指定其系统角色。在这里，我们可以以图形数据加载 API 调用为例。下面提供了给 ChatGPT 的说明及其提示示例。根据说明和提示示例，我们将要求 ChatGPT
    生成图形数据加载提示数据集。
- en: 'Instruction: Your task is to add API calls of Graph Loading functions to a
    piece of input text for concrete graph data loading.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '指示: 你的任务是将图形加载函数的 API 调用添加到一段输入文本中，以实现具体的图形数据加载。'
- en: The function should help load required graph structured data based on the mentioned
    graph name and its nodes and links.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数应根据提到的图形名称及其节点和链接来帮助加载所需的图形结构数据。
- en: You can call the Graph Loading API by writing ”[GL(graph-name, nodes, links)]”,
    where the ”graph-name” denotes the target graph data, and ”nodes” and ”links”
    are the mentioned nodes and links.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过编写”[GL(graph-name, nodes, links)]”来调用图形加载 API，其中”graph-name”表示目标图形数据，”nodes”和”links”是提到的节点和链接。
- en: If no specific nodes or links are mentioned, then the API will write ”all nodes”
    and ”all links” for the ”nodes” and ”links” parameters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有提到特定的节点或链接，则 API 将为”nodes”和”links”参数写入”所有节点”和”所有链接”。
- en: If only nodes are specified, the API will list the mentioned nodes for the ”nodes”
    parameter entry, and write ”all related links” for the ”links” parameter entry.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只指定了节点，则 API 将列出”nodes”参数条目的提到的节点，并将”links”参数条目写为”所有相关链接”。
- en: If only links are specified, the API will and write ”all related nodes” for
    the ”nodes” parameter entry, and list the mentioned links for the ”links” parameter
    entry.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只指定了链接，则 API 将”nodes”参数条目写为”所有相关节点”，并列出”links”参数条目的提到的链接。
- en: Here are some examples of the API call for loading graph structured data. In
    the examples, the output will repeat the input, and also insert the API call at
    the most appropriate position.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是加载图形结构数据的 API 调用示例。在这些示例中，输出将重复输入，并在最合适的位置插入 API 调用。
- en: '$\bullet$ Input: The structure of the benzene ring molecular graph of benzene
    ring contains a hexagon.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入: 苯环的分子图结构包含一个六边形。'
- en: '$\bullet$ Output: The structure of the [GL(”benzene-ring”)] molecular graph
    of benzene ring contains a hexagon.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出: [GL(”苯环”)] 苯环的分子图结构包含一个六边形。'
- en: '$\bullet$ Input: There exist a carbon-oxygen double bond in the Acetaldehyde
    molecular graph.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入: 在乙醛分子图中存在一个碳氧双键。'
- en: '$\bullet$ Output: There exist a [GL(”acetaldehyde-molecular-graph”, {Carbon,
    Oxygen}, {(Carbon, Oxygen)})] carbon-oxygen double bond in the Acetaldehyde molecular
    graph.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出: 在乙醛分子图中存在一个 [GL(”乙醛分子图”, {碳, 氧}, {(碳, 氧)})] 碳氧双键。'
- en: '$\bullet$ Input: The lollipop graph looks like a spoon.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入: 棒棒糖图看起来像一个勺子。'
- en: '$\bullet$ Output: The [GL(”lollipop-graph”, ”all nodes”, ”all links”)] lollipop
    graph looks like a spoon.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出: [GL(”棒棒糖图”, ”所有节点”, ”所有链接”)] 棒棒糖图看起来像一个勺子。'
- en: '$\bullet$ Input: The paper#10 in the Cora bibliographic network introduces
    the Transformer model.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入: 文献#10在 Cora 文献网络中介绍了 Transformer 模型。'
- en: '$\bullet$ Output: The [GL(”cora”, {Paper#10}, ”all related citation links”)]
    paper#10 in the bibliographic network introduces the Transformer model.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出: [GL(”cora”, {Paper#10}, ”所有相关引用链接”)] 文献#10在文献网络中介绍了Transformer模型。'
- en: '$\bullet$ Input: Insulin is a small globular protein containing two long amino
    acid chains.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入: 胰岛素是一种小型球状蛋白质，包含两个长的氨基酸链。'
- en: '$\bullet$ Output: [GL(”insulin-protein-graph”, ”all atom nodes”, ”all atom
    bond links”)] Insulin is a small globular protein containing two long amino acid
    chains.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出: [GL(”胰岛素蛋白质图”, ”所有原子节点”, ”所有原子键链接”)] 胰岛素是一种小型球状蛋白质，包含两个长的氨基酸链。'
- en: '$\bullet$ Input: At the IMDB recommender system, David rates the ”The Avengers”
    movie with a 10-star review score.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入: 在 IMDB 推荐系统中，David 给电影 ”复仇者联盟” 评分 10 星。'
- en: '$\bullet$ Output: At the [GL(”imdb-recommender-system”, {”David”, ”The Avengers”},
    {(”David”, ”The Avengers”)})] IMDB recommender system, David rates the ”The Avengers”
    movie with a 10-star review score.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：[GL(”imdb-recommender-system”, {”David”, ”The Avengers”}, {(”David”,
    ”The Avengers”)})] IMDB 推荐系统中，**David**给电影《复仇者联盟》打了10星评分。
- en: '$\bullet$ Input: Among the existing online social apps, Tiktok makes it easy
    for users to socialize with each other online via livestream videos.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：在现有的在线社交应用中，Tiktok 使用户可以通过直播视频轻松地在线社交。
- en: '$\bullet$ Output: Among the existing online social apps, [GL(”tiktok-social-network”,
    ”all user and video nodes”, ”all user-video links and user-user links”)] Tiktok
    makes it easy for users to socialize with each other online via livestream videos.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：在现有的在线社交应用中，[GL(”tiktok-social-network”, ”all user and video nodes”,
    ”all user-video links and user-user links”)] Tiktok 使用户可以通过直播视频轻松地在线社交。
- en: '$\bullet$ Input: According to the Freebase knowledge graph, Donald Trump was
    born in 1946 at the Jamaica Hospital Medical Center in New York.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：根据 Freebase 知识图谱，**唐纳德·特朗普**于1946年出生在纽约的**牙买加医院医疗中心**。
- en: '$\bullet$ Output: According to the [GL(”freebase”, {”Donald Trump”, ”Jamaica
    Hospital Medical Center”, ”New York”}, {(”Donald Trump”, ”Jamaica Hospital Medical
    Center”), (”Jamaica Hospital Medical Center”, ”New York”)})] Freebase knowledge
    graph, Donald Trump was born in 1946 at the Jamaica Hospital Medical Center in
    New York.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：根据[GL(”freebase”, {”Donald Trump”, ”Jamaica Hospital Medical Center”,
    ”New York”}, {(”Donald Trump”, ”Jamaica Hospital Medical Center”), (”Jamaica Hospital
    Medical Center”, ”New York”)})] Freebase 知识图谱，**唐纳德·特朗普**于1946年出生在纽约的**牙买加医院医疗中心**。
- en: 'Query: Based on the instruction and examples, please generate 5000 such input-output
    pairs for real-world graph data loading. Please make sure the data loaded are
    in graph structures and the API call is insert ahead of the mentioned graphs or
    the mentioned nodes or links.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：根据说明和示例，请生成5000个这样的输入-输出对用于现实世界的图数据加载。请确保数据以图结构形式加载，并且 API 调用在提到的图或提到的节点或链接之前插入。
- en: Based on the instruction, examples and query, by calling ChatGPT API, we obtained
    a prompt dataset with $5,000$ graph data loading API call input-output pairs are
    preserved in the dataset, which will be used for the fine-tuning to be introduced
    later.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 根据说明、示例和查询，通过调用 ChatGPT API，我们获得了一个包含$5,000$个图数据加载 API 调用输入-输出对的提示数据集，这将用于后续的微调。
- en: 4.4.2\. Graph Reasoning Prompt Dataset Generation
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2\. 图推理提示数据集生成
- en: 'As to the other graph reasoning prompts, with similar instruction and prompt
    examples, we can use ChatGPT to generate a large number of similar input-output
    pairs. Meanwhile, slightly different from graph loading API calls, to ensure the
    graph reasoning prompts are valid, we propose to compose all the inputs statements
    manually by calling the graph reasoning toolkits in advance. For instance, for
    the first paper in the Cora bibliographic network, its topic is about “Neural
    Networks” and we will compose its input statement as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他图推理提示，使用类似的说明和提示示例，我们可以使用 ChatGPT 生成大量类似的输入-输出对。同时，与图加载 API 调用略有不同，为确保图推理提示有效，我们建议通过预先调用图推理工具包手动编写所有输入语句。例如，对于
    Cora 文献网络中的第一篇论文，其主题为“神经网络”，我们将其输入语句编写如下：
- en: '$\bullet$ Input: The first paper in Cora has a topic of Neural Networks.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：Cora 中的第一篇论文的主题是神经网络。
- en: We will feed such input to ChatGPT and ask it helps insert the graph reasoning
    API calls to the statement with the query.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这样的输入提供给 ChatGPT，并请求它帮助将图推理 API 调用插入到语句中。
- en: 'Query: Based on the instruction and examples, generate the output with graph
    reasoning API calls for the input. Please make sure the API call is insert at
    the most appropriate position.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 查询：根据说明和示例，生成包含图推理 API 调用的输出。请确保 API 调用插入到最合适的位置。
- en: 'Based on the query and input statement, ChatGPT will return the following output:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 根据查询和输入语句，ChatGPT 将返回以下输出：
- en: '$\bullet$ Output: The first paper in Cora has a topic of [GR(GL(”cora”, ”all
    paper nodes”, ”all citation links”), ”topic”, {Paper#1}) –¿ r] Neural Networks.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：Cora中的第一篇论文的主题是[GR(GL(”cora”, ”all paper nodes”, ”all citation
    links”), ”topic”, {Paper#1}) –¿ r] 神经网络。
- en: 'Besides using ChatGPT to annotate the API calls and generate the above output,
    we also use ChatGPT to rewrite the input statement in another way without changing
    its semantic meanings. For instance, for the input statement shown above, we also
    obtain several of its rephrased versions as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用 ChatGPT 对 API 调用进行注释并生成上述输出外，我们还使用 ChatGPT 以不同的方式重写输入语句，而不改变其语义。例如，对于上述输入语句，我们还获得了以下几个其重新表述的版本：
- en: '$\bullet$ Input: The initial article in Cora focuses on the subject of Neural
    Networks.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：Cora 中的初始文章专注于神经网络的主题。
- en: '$\bullet$ Input: In Cora, the premier paper addresses Neural Networks as its
    main theme.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：在 Cora 中，首篇论文以神经网络为主要主题。
- en: '$\bullet$ Input: The foremost paper in the Cora collection pertains to the
    field of Neural Networks.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：Cora 集合中的首篇论文涉及神经网络领域。
- en: '$\bullet$ Input: Cora’s inaugural publication delves into the subject matter
    of Neural Networks.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：Cora 的首篇论文*深入探讨*了神经网络的主题。
- en: These rephrased input will also be fed to ChatGPT again for the API call annotation
    as well. Such a process will be done for all the node/graph instances studied
    in both the basic graph property reasoning tasks and the advanced graph reasoning
    tasks for generating the input-output prompt pair datasets. Based on the generated
    dataset, we will run the API calls generated by ChatGPT and compare the return
    result of the graph reasoning API functions with the true values in the statements.
    For the outputs whose API calls (1) are not runnable or (2) cannot return the
    correct result, they will be filtered from the dataset. Finally, after the filtering,
    the ChatGPT augmented generated datasets will be used for the LLMs fine-tuning,
    whose statistical information will be provided later in the following experiment
    section. Meanwhile, for the other graph reasoning tasks not studied in this paper,
    their reasoning API call datasets can be generated in a similar way as described
    above.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这些重新表述的输入也会被再次输入到 ChatGPT 中，以进行 API 调用注释。这一过程将针对在基本图属性推理任务和高级图推理任务中研究的所有节点/图实例进行，以生成输入-输出提示对数据集。根据生成的数据集，我们将运行由
    ChatGPT 生成的 API 调用，并将图推理 API 函数的返回结果与语句中的真实值进行比较。对于那些 API 调用（1）无法运行或（2）不能返回正确结果的输出，将会从数据集中筛选出来。最后，经过筛选后，ChatGPT
    增强生成的数据集将用于 LLM 的微调，其统计信息将在后续实验部分提供。同时，对于本文未研究的其他图推理任务，其推理 API 调用数据集可以以类似上面描述的方式生成。
- en: 4.5\. LLMs Fine-Tuning for Graph Reasoning
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5\. LLM 的图推理微调
- en: 'Based on the above augmented graph reasoning prompt datasets, in this part,
    we will introduce how to fine-tune existing pre-trained LLMs, so the LLMs can
    learn how to use the API tools to address the graph reasoning tasks. Formally,
    as shown by the prompt examples in Table [1](#S4.T1 "Table 1 ‣ 4.1\. Framework
    Outline ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT") and Table [2](#S4.T2 "Table 2 ‣ 4.3.2\.
    Graph Property Reasoning ‣ 4.3\. Graph Reasoning Oriented Prompts ‣ 4\. Proposed
    Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt
    Augmented by ChatGPT"), given the input statements with a sequence of tokens,
    i.e., $\mathbf{w}=[w_{1},w_{2},\cdots,w_{n}]$ as introduced in the previous Section [4.2](#S4.SS2
    "4.2\. Prompts with API Calls ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower
    LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"). The major
    challenges lie in (1) precisely identify the most appropriate positions to insert
    the API call, (2) correctly the choose the API functions to be used for the call,
    and (3) also accurately extract the parameters from the context and feed them
    to the functions. In this section, we will address all these three challenges.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '基于上述增强图推理提示数据集，在这一部分，我们将介绍如何微调现有的预训练 LLM，以便 LLM 能够学习如何使用 API 工具来解决图推理任务。形式上，如表
    [1](#S4.T1 "Table 1 ‣ 4.1\. Framework Outline ‣ 4\. Proposed Method ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")
    和表 [2](#S4.T2 "Table 2 ‣ 4.3.2\. Graph Property Reasoning ‣ 4.3\. Graph Reasoning
    Oriented Prompts ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with
    Graph Reasoning Ability via Prompt Augmented by ChatGPT") 中的提示示例所示，给定一个输入语句，其令牌序列为
    $\mathbf{w}=[w_{1},w_{2},\cdots,w_{n}]$ 如第 [4.2](#S4.SS2 "4.2\. Prompts with API
    Calls ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT") 节中介绍。主要挑战在于 (1) 精确识别插入 API 调用的最合适位置，(2)
    正确选择用于调用的 API 功能，以及 (3) 准确提取上下文中的参数并将其传递给函数。在本节中，我们将解决这三个挑战。'
- en: 4.5.1\. API Call Insertion Position Prediction
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.1\. API 调用插入位置预测
- en: For the provided input statement, there may exist multiple potential positions
    for inserting the API calls. Different from ([Schick2023ToolformerLM,](#bib.bib43)
    ) that choose top-k positions for API call data generation, in this paper, we
    aim to identify the most likely position to insert the API calls instead. Formally,
    based on a pre-trained language model $M$) with the probability
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提供的输入语句，可能存在多个潜在的位置用于插入 API 调用。不同于 ([Schick2023ToolformerLM,](#bib.bib43))
    选择前 k 个位置进行 API 调用数据生成，本论文旨在识别最有可能插入 API 调用的位置。形式上，基于预训练语言模型 $M$ 的概率
- en: '| (41) |  |  |  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| (42) |  | $$\mathbf{s}(c)={{\textsc{ |  |'
- en: or
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 或
- en: '| (43) |  | 
    |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| (44) |  | $$d^{i}=\arg\max_{d_{j}\in\mathcal{D}}P_{M}(d_{j}&#124;\mathbf{w}(1:{i-1}){\textsc{
    |  |'
- en: Furthermore, based on the selected domain $d^{i}$, and the function which can
    maximize the generation probability will be selected, i.e.,
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，根据选择的领域 $d^{i}$，将选择可以最大化生成概率的功能，即，
- en: '| (45) |  | ”, Graph-ToolFormer will automatically
    complete the API call as follows:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于被遮掩的参数补全，一旦选择了领域和函数，Graph-ToolFormer 将自动生成并插入剩余的标记，包括函数参数名称、括号、逗号和冒号标记，以及
    API 调用结束的特殊标记 ¡/API¿。例如，基于当前的标记序列 “$$\mathbf{w}(1:{i-1}){\textsc{”，Graph-ToolFormer
    将自动完成 API 调用，如下所示：
- en: '| (46) |  | $\displaystyle\mathbf{w}(1:{i-1})$ |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| (46) |  | $\displaystyle\mathbf{w}(1:{i-1})$ |  |'
- en: '| (47) |  |  | 
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| (48) |  | $$\displaystyle arg_{j}^{i}=\arg\max P_{M}(arg&#124;\mathbf{w}(1\text{:}{i-1}){\textsc{
    |  |'
- en: '| (49) |  | ”). For the parameters
    that are not generated by the Graph-ToolFormer model, we will use their default
    parameter values in the API function calls in the follow-up graph reasoning process.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的过程会持续进行，直到生成结束 API 调用特殊标记 “¡/API¿”。通过将生成的参数名称和值添加到标记列表中，我们可以得到模型生成的结果为 “”。对于那些未由
    Graph-ToolFormer 模型生成的参数，我们将在后续图推理过程中使用其默认参数值进行 API 函数调用。
- en: 4.5.4\. LLMs Fine-Tuning with Augmented API Call Dataset
  id: totrans-301
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.5.4\. 使用增强的 API 调用数据集对 LLM 进行微调
- en: Formally, given the ChatGPT augmented graph reasoning prompt dataset $\mathcal{D}=\{(\mathbf{w}_{1},\bar{\mathbf{w}}_{1}),(\mathbf{w}_{2},\bar{\mathbf{w}}_{2}),\cdots,(\mathbf{w}_{|\mathcal{D}|},\bar{\mathbf{w}}_{|\mathcal{D}|})\}$
    to LLMs, we can represent the generated output by the model as
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，给定 ChatGPT 增强图推理提示数据集 $\mathcal{D}=\{(\mathbf{w}_{1},\bar{\mathbf{w}}_{1}),(\mathbf{w}_{2},\bar{\mathbf{w}}_{2}),\cdots,(\mathbf{w}_{|\mathcal{D}|},\bar{\mathbf{w}}_{|\mathcal{D}|})\}$
    给 LLMs，我们可以将模型生成的输出表示为
- en: '| (50) |  | $\hat{\mathbf{w}}_{i}=LLM(\mathbf{w}_{i}),\forall i\in\{1,2,\cdots,&#124;\mathcal{D}&#124;\}.$
    |  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| (50) |  | $\hat{\mathbf{w}}_{i}=LLM(\mathbf{w}_{i}),\forall i\in\{1,2,\cdots,&#124;\mathcal{D}&#124;\}.$
    |  |'
- en: Furthermore, by comparing the generation output $\hat{\mathbf{w}}_{i}$, we can
    define the loss function for fine-tuning the LLM as
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过比较生成的输出 $\hat{\mathbf{w}}_{i}$，我们可以将 LLM 的微调损失函数定义为
- en: '| (51) |  | $\displaystyle\ell(\mathcal{D})$ |  |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| (51) |  | $\displaystyle\ell(\mathcal{D})$ |  |'
- en: '| (52) |  |  | $1$2 |  |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| (52) |  |  | $1$2 |  |'
- en: 4.6\. Graph Reasoning Q&A Prompts
  id: totrans-307
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6\. 图推理问答提示
- en: What’s more, to provide Graph-ToolFormer with basic Q&A ability for graph reasoning,
    besides the above statements based graph reasoning prompts, we will also design
    some Q&A based prompts for fine-tuning Graph-ToolFormer as well. The graph reasoning
    Q&A based prompts are created in a very similar way as above, but we will replace
    the input statements with other reasoning questions instead, and the output will
    still be a statement with graph reasoning API calls corresponding to the input
    question.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了为Graph-ToolFormer提供基本的图推理问答能力，除了上述基于声明的图推理提示，我们还将设计一些基于问答的提示来对Graph-ToolFormer进行微调。图推理问答提示的创建方式与上述非常相似，但我们将用其他推理问题替换输入声明，输出仍将是一个带有图推理API调用的声明，对应于输入问题。
- en: Formally, we also list of the graph reasoning Q&A prompt examples used in this
    paper as follows, which will be merged into the previous prompts for fine-tuning
    Graph-ToolFormer. Different from the previous input-output statement prompts (where
    the output is almost a duplicated copy of the input but with API calls), the inputs
    and outputs in question-answer prompts are not duplicated copies of each other
    anymore. However, with the above autoregressive generation of the desired output
    statement introduced before, the Graph-ToolFormer using causal language models
    as the backbone is still capable to generate the desired output statements for
    the input question queries.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 正式地，我们还列出了本文使用的图推理问答提示示例，以下示例将与之前的提示合并以对Graph-ToolFormer进行微调。不同于之前的输入-输出声明提示（其中输出几乎是输入的重复副本，但带有API调用），问答提示中的输入和输出不再是彼此的重复副本。然而，借助之前介绍的自回归生成期望输出声明的方式，使用因果语言模型作为骨干的Graph-ToolFormer仍然能够生成所需的输出声明。
- en: '1\. Graph Property Reasoning Q&A Prompt Examples:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. 图属性推理问答提示示例：
- en: '$\bullet$ Input: What is the order of the barbell graph?'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：杠铃图的阶数是多少？
- en: '$\bullet$ Output: The order of the barbell graph is [GR(GL(”gpr”, ”barbell_graph”),
    ”toolx:order”)–¿r].'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：杠铃图的阶数是 [GR(GL(”gpr”, ”barbell_graph”), ”toolx:order”)–¿r]。
- en: '$\bullet$ Input: What is the size of the star graph?'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：星形图的大小是多少？
- en: '$\bullet$ Output: The size of the star graph is [GR(GL(”gpr”, ”star_graph”),
    ”toolx:size”)–¿r].'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：星形图的大小是 [GR(GL(”gpr”, ”star_graph”), ”toolx:size”)–¿r]。
- en: '$\bullet$ Input: What is the density of the dodecahedral graph?'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：十二面体图的密度是多少？
- en: '$\bullet$ Output: The density of dodecahedral graph is [GR(GL(”gpr”, ”dodecahedral_graph”),
    ”toolx:density”)–¿r].'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：十二面体图的密度是 [GR(GL(”gpr”, ”dodecahedral_graph”), ”toolx:density”)–¿r]。
- en: '$\bullet$ Input: What is the eccentricity of node #25 in the balanced tree?'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：平衡树中节点 #25 的离心率是多少？'
- en: '$\bullet$ Output: The eccentricity of node #25 in the balanced tree is [GR(GL(”gpr”,
    ”balanced_tree”), ”toolx:eccentricity”, ”node#25”)–¿r].'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：平衡树中节点 #25 的离心率是 [GR(GL(”gpr”, ”balanced_tree”), ”toolx:eccentricity”,
    ”node#25”)–¿r]。'
- en: '$\bullet$ Input: What is the radius of the lollipop graph?'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：棒棒糖图的半径是多少？
- en: '$\bullet$ Output: The radius of the lollipop graph is [GR(GL(”gpr”, ”lollipop_graph”),
    ”toolx:radius”)–¿r].'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：棒棒糖图的半径是 [GR(GL(”gpr”, ”lollipop_graph”), ”toolx:radius”)–¿r]。
- en: '$\bullet$ Input: What is the center of the star graph?'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：星形图的中心是什么？
- en: '$\bullet$ Output: The center of the star graph includes node(s) [GR(GL(”gpr”,
    ”star_graph”), ”toolx:center”)–¿r].'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：星形图的中心包括节点 [GR(GL(”gpr”, ”star_graph”), ”toolx:center”)–¿r]。
- en: '$\bullet$ Input: What is the length of shortest path between node #5 and node
    #0 in the octahedral graph?'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：在八面体图中，节点 #5 和节点 #0 之间的最短路径长度是多少？'
- en: '$\bullet$ Output: In the octahedral graph, the length of shortest path between
    node #5 and node #0 is [GR(GL(”gpr”, ”octahedral_graph”), ”toolx:shortest_path”,
    ”node#5”, ”node#0”)–¿r].'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：在八面体图中，节点 #5 和节点 #0 之间的最短路径长度是 [GR(GL(”gpr”, ”octahedral_graph”),
    ”toolx:shortest_path”, ”node#5”, ”node#0”)–¿r]。'
- en: '$\bullet$ Input: What is the diameter of the binomial tree?'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：二项树的直径是多少？
- en: '$\bullet$ Output: The diameter of the binomial tree is [GR(GL(”gpr”, ”binomial_tree”),
    ”toolx:diameter”)–¿r].'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：二项树的直径是 [GR(GL(”gpr”, ”binomial_tree”), ”toolx:diameter”)–¿r]。
- en: '$\bullet$ Input: What is the periphery of the house x graph?'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：房子 x 图的周长是多少？
- en: '$\bullet$ Output: The periphery of the house x graph includes node(s) [GR(GL(”gpr”,
    ”house_x_graph”), ”toolx:periphery”)–¿r].'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：房子 x 图的周长包括节点 [GR(GL(”gpr”, ”house_x_graph”), ”toolx:periphery”)–¿r]。
- en: '2\. Bibliographic Network Reasoning Q&A Prompt Examples:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. 文献网络推理问答提示示例：
- en: '$\bullet$ Input: What is the topic of paper #83826 in the cora bibliographic
    network?'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：cora 文献网络中论文 #83826 的主题是什么？'
- en: '$\bullet$ Output: The topic of paper #83826 in the cora bibliographic network
    is [GR(GL(”cora”), ”graph_bert:topic”, paper#83826)–¿r].'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：cora 文献网络中论文 #83826 的主题是 [GR(GL(”cora”), ”graph_bert:topic”, paper#83826)–¿r]。'
- en: '$\bullet$ Input: What is the topic of paper #5832 in the pubmed bibliographic
    network?'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：pubmed 文献网络中论文 #5832 的主题是什么？'
- en: '$\bullet$ Output: The topic of paper #5832 in the pubmed bibliographic network
    is [GR(GL(”pubmed”), ”graph_bert:topic”, paper#5832)–¿r].'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：pubmed 文献网络中论文 #5832 的主题是 [GR(GL(”pubmed”), ”graph_bert:topic”,
    paper#5832)–¿r]。'
- en: '$\bullet$ Input: What is the topic of paper #3230 in the citeseer bibliographic
    network?'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：citeseer 文献网络中论文 #3230 的主题是什么？'
- en: '$\bullet$ Output: The topic of paper #3230 in the citeseer bibliographic network
    is [GR(GL(”citeseer”), ”graph_bert:topic”, paper#3230)–¿r].'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：citeseer 文献网络中论文 #3230 的主题是 [GR(GL(”citeseer”), ”graph_bert:topic”,
    paper#3230)–¿r]。'
- en: '3\. Molecular Graph Reasoning Q&A Prompt Examples:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 分子图推理问答示例：
- en: '$\bullet$ Input: What is the function for the protein molecular graph #138
    in proteins?'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：蛋白质分子图 #138 在 proteins 中的功能是什么？'
- en: '$\bullet$ Output: The function for the protein molecular graph #138 in proteins
    is [GR(GL(”proteins”), ”seg_bert:molecule_function”, instance#138)–¿r].'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：蛋白质分子图 #138 在 proteins 中的功能是 [GR(GL(”proteins”), ”seg_bert:molecule_function”,
    instance#138)–¿r]。'
- en: '$\bullet$ Input: What is the function for the chemical molecular graph #129
    in mutag?'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：mutag 中化学分子图 #129 的功能是什么？'
- en: '$\bullet$ Output: The function for the chemical molecular graph #129 in mutag
    is [GR(GL(”mutag”), ”seg_bert:molecule_function”, instance#129)–¿r].'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：化学分子图 #129 在 mutag 中的功能是 [GR(GL(”mutag”), ”seg_bert:molecule_function”,
    instance#129)–¿r]。'
- en: '$\bullet$ Input: What is the function for the chemical molecular graph #322
    in nci1?'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：化学分子图 #322 在 nci1 中的功能是什么？'
- en: '$\bullet$ Output: The function for the chemical molecular graph #322 in nci1
    is [GR(GL(”nci1”), ”seg_bert:molecule_function”, instance#322)–¿r].'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：化学分子图 #322 在 nci1 中的功能是 [GR(GL(”nci1”), ”seg_bert:molecule_function”,
    instance#322)–¿r]。'
- en: '$\bullet$ Input: What is the function for the chemical molecular graph #44
    in ptc?'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：化学分子图 #44 在 ptc 中的功能是什么？'
- en: '$\bullet$ Output: The function for the chemical molecular graph #44 in ptc
    is [GR(GL(”ptc”), ”seg_bert:molecule_function”, instance#44)–¿r].'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：化学分子图 #44 在 ptc 中的功能是 [GR(GL(”ptc”), ”seg_bert:molecule_function”,
    instance#44)–¿r]。'
- en: '4\. Social Network Reasoning Q&A Prompt Examples:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 社交网络推理问答示例：
- en: '$\bullet$ Input: In foursquare, what is the id of user sparkey215’s community?'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：在 foursquare 中，用户 sparkey215 的社区 ID 是什么？
- en: '$\bullet$ Output: In foursquare, the id of user sparkey215’s community is [GR(GL(”foursquare”),
    ”kmeans:community”, user#sparkey215)–¿r].'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：在 foursquare 中，用户 sparkey215 的社区 ID 是 [GR(GL(”foursquare”), ”kmeans:community”,
    user#sparkey215)–¿r]。
- en: '$\bullet$ Input: In the online social network foursquare, are user #user/9674821
    and user #ljaniszewski8 belong to the same community?'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：在在线社交网络 foursquare 中，用户 #user/9674821 和用户 #ljaniszewski8 是否属于同一个社区？'
- en: '$\bullet$ Output: In the online social network foursquare, user #user/9674821
    and user #ljaniszewski8 belong to [GR(GL(”foursquare”), ”kmeans:common_community_check”,
    user#user/9674821, user#ljaniszewski8)–¿r] community.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：在在线社交网络 foursquare 中，用户 #user/9674821 和用户 #ljaniszewski8 属于 [GR(GL(”foursquare”),
    ”kmeans:common_community_check”, user#user/9674821, user#ljaniszewski8)–¿r] 社区。'
- en: '5\. Recommender System Reasoning Q&A Prompt Examples:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 推荐系统推理问答示例：
- en: '$\bullet$ Input: How likely user #A23E9QQHJLNGUI will be interested in item
    #B004PIPG2A in Amazon?'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：用户 #A23E9QQHJLNGUI 对 Amazon 上的物品 #B004PIPG2A 的兴趣有多大？'
- en: '$\bullet$ Output: The likelihood that user #A23E9QQHJLNGUI will be interested
    in item #B004PIPG2A in Amazon is [GR(GL(”amazon”), ”bpr:recommendation”, user#A23E9QQHJLNGUI,
    item#B004PIPG2A)–¿r].'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：用户 #A23E9QQHJLNGUI 对 Amazon 上物品 #B004PIPG2A 的兴趣可能性是 [GR(GL(”amazon”),
    ”bpr:recommendation”, user#A23E9QQHJLNGUI, item#B004PIPG2A)–¿r]。'
- en: '$\bullet$ Input: How likely user #u329 will be interested in music of artist
    #i8323 in Last-fm?'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：用户 #u329 对 Last-fm 上艺术家 #i8323 的音乐感兴趣的可能性有多大？'
- en: '$\bullet$ Output: The likelihood that user #u329 will be interested in music
    from artist #i8323 in Last-fm is [GR(GL(”last-fm”), ”bpr:recommendation”, user#u329,
    artist#i8323)–¿r].'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：用户 #u329 对 Last-fm 上艺术家 #i8323 的音乐感兴趣的可能性是 [GR(GL(”last-fm”),
    ”bpr:recommendation”, user#u329, artist#i8323)–¿r]。'
- en: '$\bullet$ Input: How likely user #u650 will be interested in movie #i671 in
    Movielens?'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：用户 #u650 对 Movielens 上的电影 #i671 的兴趣有多大？'
- en: '$\bullet$ Output: The likelihood that user #u650 will be interested in movie
    #i671 in Movielens is [GR(GL(”movielens”), ”bpr:recommendation”, user#u650, movie#i671)–¿r].'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：用户 #u650 对电影 #i671 在 Movielens 中感兴趣的可能性是 [GR(GL(”movielens”),
    ”bpr:recommendation”, user#u650, movie#i671)–¿r]。'
- en: '6\. Knowledge Graph Reasoning Q&A Prompt Examples:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. 知识图谱推理问答提示示例：
- en: '$\bullet$ Input: According to the Freebase knowledge graph, what is the relation
    between entity#/m/053yx and entity#/m/015_1q?'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：根据 Freebase 知识图谱，实体#/m/053yx 和实体#/m/015_1q 之间的关系是什么？
- en: '$\bullet$ Output: According to the Freebase knowledge graph, the relation between
    entity#/m/053yx and entity#/m/015_1q is [GR(GL(”freebase”), ”transe:relation”,
    entity#/m/053yx, entity#/m/015_1q)–¿r].'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：根据 Freebase 知识图谱，实体#/m/053yx 和实体#/m/015_1q 之间的关系是 [GR(GL(”freebase”),
    ”transe:relation”, entity#/m/053yx, entity#/m/015_1q)–¿r]。
- en: '$\bullet$ Input: According to the WordNet knowledge graph, via relation #_hypernym,
    we derive entity #imagination.n.02 from what entity?'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输入：根据 WordNet 知识图谱，通过关系 #_hypernym，我们从哪个实体得出实体 #imagination.n.02？'
- en: '$\bullet$ Output: According to the WordNet knowledge graph, via relation #_hypernym,
    we can obtain entity #imagination.n.02 from entity [GR(GL(”wordnet”), ”transe:head_entity”,
    relation#_hypernym, entity#imagination.n.02)–¿r].'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ 输出：根据 WordNet 知识图谱，通过关系 #_hypernym，我们可以从实体 [GR(GL(”wordnet”), ”transe:head_entity”,
    relation#_hypernym, entity#imagination.n.02)–¿r] 获得实体 #imagination.n.02。'
- en: '![Refer to caption](img/2356290d869c6ff5f0c1ff980aeb4c8e.png)'
  id: totrans-362
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2356290d869c6ff5f0c1ff980aeb4c8e.png)'
- en: 'Figure 3\. An Illustration of Graph Reasoning Query Processing. The graph reasoning
    query processing component in Graph-ToolFormer has several modules/hubs: (1) LLM
    based query statement generation, (2) query extraction and parsing model, (3)
    query execution model, (4) working memory module, and (5) output post-processing
    module. The graph reasoning query execution module is built based on the (6) graph
    reasoning task hub, (7) graph data hub, and (8) graph model hub. The Graph-ToolFormer
    framework will recognize the parsed graph reasoning queries, load the corresponding
    graph data, call the corresponding graph reasoning model, execute the reasoning
    task API function to generate the result, store the result into working memory
    and insert the output result to replace the reasoning query in the LLM generated
    reasoning response statement as well.'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 图推理查询处理的示意图。Graph-ToolFormer 中的图推理查询处理组件包括几个模块/中心： (1) 基于 LLM 的查询语句生成，
    (2) 查询提取和解析模型， (3) 查询执行模型， (4) 工作记忆模块，以及 (5) 输出后处理模块。图推理查询执行模块建立在 (6) 图推理任务中心，
    (7) 图数据中心，和 (8) 图模型中心的基础上。Graph-ToolFormer 框架将识别解析后的图推理查询，加载相应的图数据，调用相应的图推理模型，执行推理任务
    API 函数以生成结果，将结果存储到工作记忆中，并将输出结果插入以替换 LLM 生成的推理响应语句中的推理查询。
- en: 4.7\. LLMs Inference and Graph Reasoning Query Parsing, Execution and Post-Processing
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7\. LLM 推理与图推理查询解析、执行及后处理
- en: 'Finally, at the end of this section, we will introduce the details about how
    to use the fine-tuned LLMs in Graph-ToolFormer for addressing various graph reasoning
    tasks. As shown in Figure [3](#S4.F3.1 "Figure 3 ‣ 4.6\. Graph Reasoning Q&A Prompts
    ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"), the graph reasoning process has several
    important steps based on several functional modules, which include (1) LLMs based
    output query statement generation, (2) query extraction and parsing, (3) query
    execution, (4) graph data hub, (5) graph model hub, (6) graph task hub, (7) working
    memory and (8) reasoning output post-processing. In this subsection, we will introduce
    these steps and the involved functional modules/hubs used in Graph-ToolFormer
    for readers.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在本节末尾，我们将介绍如何使用在 Graph-ToolFormer 中微调的 LLM 处理各种图推理任务的细节。如图 [3](#S4.F3.1 "图
    3 ‣ 4.6\. 图推理问答提示 ‣ 4\. 提议方法 ‣ Graph-ToolFormer：通过 ChatGPT 增强的提示赋予 LLM 图推理能力")
    所示，图推理过程基于几个功能模块包括 (1) 基于 LLM 的输出查询语句生成， (2) 查询提取和解析， (3) 查询执行， (4) 图数据中心， (5)
    图模型中心， (6) 图任务中心， (7) 工作记忆和 (8) 推理输出后处理。本小节将介绍这些步骤和 Graph-ToolFormer 中使用的功能模块/中心。
- en: 4.7.1\. LLMs Inference
  id: totrans-366
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.1\. LLM 推理
- en: 'Based on the prompt datasets, we have discussed about how to fine-tune the
    LLMs in Graph-ToolFormer in the previous subsections already, which is capable
    to generate the graph reasoning query statement outputs for the input statements.
    To apply the the fine-tuned LLMs for the inference, given any graph reasoning
    input statement, the LLMs will project the input statement to the corresponding
    output statement annotated with the API calls. We also provide an example about
    the inference process as follows:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 基于提示数据集，我们在前面的子节中讨论了如何微调Graph-ToolFormer中的LLMs，这些LLMs能够为输入语句生成图推理查询语句输出。为了应用微调后的LLMs进行推理，给定任何图推理输入语句，LLMs将把输入语句投影到带有API调用的相应输出语句。我们还提供了一个关于推理过程的例子如下：
- en: '$\bullet$ Input: The first paper in Cora has a topic of [TBR].'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：Cora中的第一篇论文的主题是[TBR]。
- en: '$\bullet$ Output: The first paper in Cora has a topic of [GR(GL(”cora”), ”graph-bert:topic”,
    {Paper#1}) –¿ r].'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：Cora中的第一篇论文的主题是[GR(GL(”cora”), ”graph-bert:topic”, {Paper#1}) –¿
    r]。
- en: Meanwhile, by including the Q&A based prompt datasets for LLMs fine-tuning,
    the Graph-ToolFormer will also be capable to generate the graph reasoning statements
    for the input question queries as well, such as
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，通过包括基于问答的提示数据集用于LLMs的微调，Graph-ToolFormer 也将能够为输入问题查询生成图推理语句，例如
- en: '$\bullet$ Input: What is the topic of the first paper in Cora bibliographic
    network?'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：Cora文献网络中的第一篇论文的主题是什么？
- en: '$\bullet$ Output: The first paper in Cora has a topic of [GR(GL(”cora”), ”graph-bert:topic”,
    {Paper#1}) –¿ r].'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输出：Cora中的第一篇论文的主题是[GR(GL(”cora”), ”graph-bert:topic”, {Paper#1}) –¿
    r]。
- en: 'The LLMs in Graph-ToolFormer can add the correct graph reasoning API calls
    into the output at the correct position for majority of the graph reasoning input
    statements and questions (we will illustrate the experimental results in the following
    Section [5](#S5 "5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph
    Reasoning Ability via Prompt Augmented by ChatGPT")). Such generated output statements
    with API calls will be fed to the following parser module to extract the graph
    reasoning queries.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 'Graph-ToolFormer中的LLMs可以将正确的图推理API调用添加到大多数图推理输入语句和问题的输出中的正确位置（我们将在接下来的第[5](#S5
    "5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT")节中说明实验结果）。这些带有API调用的生成输出语句将被送入接下来的解析模块以提取图推理查询。'
- en: 4.7.2\. Query Parser Module
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.2\. 查询解析模块
- en: 'Since we allow both nested and sequential API calls in Graph-ToolFormer, the
    parsing of the LLMs’ generated output graph reasoning queries is never an easy
    task. Here, we can take the output query “[GR(GL(”cora”),”graph-bert:topic”,{Paper#1})–¿r]”
    generated by the LLMs introduced in the above subsection as an example. The Graph-ToolFormer
    framework introduce a query parser module that is capable to identify and parse
    the queries to a standard format that is recognizable and executable by the executor
    module. The expected parsing result of the query will involve two parts:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们允许在Graph-ToolFormer中同时使用嵌套和顺序API调用，因此解析LLMs生成的输出图推理查询从来不是一项容易的任务。在这里，我们可以以上述子节中LLMs生成的输出查询“[GR(GL(”cora”),”graph-bert:topic”,{Paper#1})–¿r]”作为例子。Graph-ToolFormer框架引入了一个查询解析模块，该模块能够将查询识别并解析成一个标准格式，执行模块能够识别和执行该格式。查询的预期解析结果将涉及两个部分：
- en: •
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Function Call Parsing: To differentiate normal textual tokens in the statement
    from the graph reasoning API call queries, we will use the regular expression
    to identify and parse the queries. The function call part of the query can be
    effectively identified with the following regular expression in python:'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 函数调用解析：为了区分语句中的普通文本标记与图推理API调用查询，我们将使用正则表达式来识别和解析查询。查询的函数调用部分可以使用以下Python正则表达式有效识别：
- en: 1function_call_pattern  =  r’\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\(([^\]]*)\)’
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1function_call_pattern  =  r’\b([a-zA-Z_][a-zA-Z0-9_]*)\s*\(([^\]]*)\)’
- en: which will detect both the API GR/GL function tokens, as well as the parameters
    in the API call. For the parameters in the API call, if we identify there exist
    any nested API calls (via detecting the parentheses marks $($), we will recursively
    parse the nested API call.
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将检测API GR/GL函数标记以及API调用中的参数。对于API调用中的参数，如果我们识别出存在任何嵌套的API调用（通过检测括号标记$($），我们将递归解析嵌套的API调用。
- en: •
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Output Insertion Parsing: The extraction of the output insertion tag “–¿r”
    will be much easier, which can be identified with the regular expression as well,
    i.e.,'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出插入解析：提取输出插入标签 “–¿r” 将变得更加容易，也可以使用正则表达式进行识别，即，
- en: 1output_variable_pattern  =  r’\)\s*([-->]*)([a-zA-Z0-9_\s]*)\s*\]’
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 1output_variable_pattern  =  r’\)\s*([-->]*)([a-zA-Z0-9_\s]*)\s*\]’
- en: For the API calls studied in this paper, if the tag “–¿r” exists, we will replace
    the query text to insert the graph reasoning results into the original text; otherwise,
    the query will be executed in the backend only, whose result will be recorded
    in the working memory to be introduced later.
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于本文研究的 API 调用，如果存在标签 “–¿r”，我们将替换查询文本以将图推理结果插入到原始文本中；否则，查询将仅在后台执行，其结果将记录在工作内存中，以便后续介绍。
- en: For instance, for the example query generated by the LLM “[GR(GL(”cora”),”graph-bert:topic”,{Paper#1})–¿r]”,
    its nested parsing result by the query parser module in the Graph-ToolFormer framework
    can be represented as “((GR, [(GL, [”cora”]), ”graph-bert:topic”, {Paper#1}]),
    [True])”, where the “True” tag denotes the existence of the output insertion tag
    “–¿r”, i.e., we need to replace the query text with the reasoning result into
    the output statement.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于 LLM 生成的示例查询 “[GR(GL(”cora”),”graph-bert:topic”,{Paper#1})–¿r]”，Graph-ToolFormer
    框架中的查询解析模块的嵌套解析结果可以表示为 “((GR, [(GL, [”cora”]), ”graph-bert:topic”, {Paper#1}]),
    [True])”，其中 “True” 标签表示存在输出插入标签 “–¿r”，即，我们需要将推理结果替换到输出语句中的查询文本中。
- en: 4.7.3\. Graph Reasoning Hubs
  id: totrans-385
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.3. 图推理中心
- en: Prior to the reasoning stage to execute the parsed queries, all the graph loading
    and reasoning API toolkits and models will be pre-loaded and ready-to-use. Also
    all the graph data to be loaded will be organized into a unified format that the
    graph loading API functions can handle. Specifically, we introduce several hubs
    in the Graph-ToolFormer framework, that will host the various graph datasets,
    pre-trained graph models and graph reasoning tasks, respectively.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行解析查询的推理阶段之前，所有图加载和推理 API 工具包及模型将被预加载并准备好使用。此外，所有需要加载的图数据将被组织成图加载 API 函数可以处理的统一格式。具体来说，我们在
    Graph-ToolFormer 框架中介绍了几个中心，它们分别托管各种图数据集、预训练的图模型和图推理任务。
- en: •
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Dataset Hub: A set of pre-processed graph datasets to be used in the
    Graph-ToolFormer framework will be organized into the graph dataset hub. All these
    datasets will have a unified format, and it will allow both the Graph-ToolFormer
    framework and the pre-trained graph models to access the desired information in
    the reasoning process. In the Appendix, we will describe the standard data organization
    format used by the source code of Graph-ToolFormer in the experiment. Specifically,
    the datasets hosted in the graph dataset hub include'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图数据集中心：一个用于 Graph-ToolFormer 框架的预处理图数据集集合将被组织成图数据集中心。所有这些数据集将具有统一的格式，这将使得 Graph-ToolFormer
    框架和预训练的图模型能够在推理过程中访问所需的信息。在附录中，我们将描述 Graph-ToolFormer 源代码在实验中使用的标准数据组织格式。具体来说，图数据集中心托管的数据集包括
- en: –
  id: totrans-389
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Graph Property Reasoning Dataset: GPR;'
  id: totrans-390
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图属性推理数据集：GPR；
- en: –
  id: totrans-391
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Bibliographic Network Datasets: Cora, Pubmed, Citeseer;'
  id: totrans-392
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参考网络数据集：Cora、Pubmed、Citeseer；
- en: –
  id: totrans-393
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Molecular Graph Datasets: Proteins, Mutag, Nci1, Ptc;'
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分子图数据集：蛋白质、Mutag、Nci1、Ptc；
- en: –
  id: totrans-395
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Online Social Network Datasets: Twitter, Foursquare;'
  id: totrans-396
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在线社交网络数据集：Twitter、Foursquare；
- en: –
  id: totrans-397
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Recommender System Datasets: Amazon, Last-FM, Movielens;'
  id: totrans-398
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推荐系统数据集：Amazon、Last-FM、Movielens；
- en: –
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Knowledge Graph Datasets: WordNet, Freebase.'
  id: totrans-400
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识图谱数据集：WordNet、Freebase。
- en: 'More detailed information about the graph datasets studied in this paper will
    be introduced in the following Section [5](#S5 "5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")
    when talking about the experiments.'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '论文中研究的图数据集的更详细信息将在下节[5](#S5 "5. 实验 ‣ Graph-ToolFormer: 通过 ChatGPT 增强提示赋能 LLM
    图推理能力")讨论实验时介绍。'
- en: •
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Model Hub: In the Graph-ToolFormer framework, we also define a graph
    model hub for hosting several (ready-to-use) graph tools and pre-trained graph
    neural network models. Specifically, the graph models included in the Graph-ToolFormer
    framework include'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图模型中心：在 Graph-ToolFormer 框架中，我们还定义了一个图模型中心，用于托管几个（现成的）图工具和预训练的图神经网络模型。具体来说，Graph-ToolFormer
    框架中包含的图模型包括
- en: –
  id: totrans-404
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Toolx: created in this paper based on networkx for property calculation;'
  id: totrans-405
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: Toolx：在本文中基于 networkx 创建，用于属性计算；
- en: –
  id: totrans-406
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Graph-Bert ([Zhang2020GraphBertOA,](#bib.bib60) ): built for graph representation
    and node classification;'
  id: totrans-407
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Graph-Bert ([Zhang2020GraphBertOA,](#bib.bib60) ): 用于图表示和节点分类；'
- en: –
  id: totrans-408
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'SEG-Bert ([Zhang2020SegmentedGF,](#bib.bib58) ): built for graph representation
    and graph instance classification;'
  id: totrans-409
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SEG-Bert ([Zhang2020SegmentedGF,](#bib.bib58) ): 用于图表示和图实例分类；'
- en: –
  id: totrans-410
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'KMeans ([MacQueen1967,](#bib.bib26) ): built for graph partitioning and node
    clustering;'
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'KMeans ([MacQueen1967,](#bib.bib26) ): 用于图分区和节点聚类；'
- en: –
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'BPR ([10.5555/1795114.1795167,](#bib.bib42) ): built for link ranking and recommendation;'
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BPR ([10.5555/1795114.1795167,](#bib.bib42) ): 用于链接排名和推荐；'
- en: –
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'TransE ([NIPS2013_1cecc7a7,](#bib.bib4) ): built for graph entity/relation
    searching.'
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'TransE ([NIPS2013_1cecc7a7,](#bib.bib4) ): 用于图实体/关系搜索。'
- en: 'More detailed information about these models will be introduced in the following
    Section [5](#S5 "5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph
    Reasoning Ability via Prompt Augmented by ChatGPT"). These graph models will implement
    the basic graph reasoning functions, which will be called in the specific graph
    reasoning tasks on the provided graph datasets.'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '关于这些模型的更多详细信息将在接下来的第[5](#S5 "5\. 实验 ‣ Graph-ToolFormer: 通过ChatGPT增强提示赋能LLMs的图推理能力")节中介绍。这些图模型将实现基本的图推理功能，这些功能将在提供的图数据集上的具体图推理任务中调用。'
- en: •
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Task Hub: Finally, the graph task hub will define the specific graph
    reasoning tasks to be studied in the Graph-ToolFormer framework. As introduced
    in the previous Section [3.3](#S3.SS3 "3.3\. Problem Formulation ‣ 3\. Notation,
    Terminology Definition and Problem Formulation ‣ Graph-ToolFormer: To Empower
    LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), most of the
    graph reasoning tasks can be reduced to several very fumdamental graph learning
    tasks, e.g., (1) graph attribute calculation, (2) node classification, (3) graph
    classification, (4) graph partition/clustering, (5) link prediction/ranking and
    (6) graph searching tasks. For all the application oriented graph reasoning tasks
    as introduced in Section [4.3](#S4.SS3 "4.3\. Graph Reasoning Oriented Prompts
    ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"), i.e., (1) graph property reasoning,
    (2) bibliographic paper topic reasoning, (3) molecular graph function reasoning,
    (4) social network community reasoning, (5) recommender system reasoning and (6)
    knowledge graph reasoning, we will reduce them to the very fumdamental graph learning
    tasks in the graph task hub.'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图任务核心组件：最后，图任务核心组件将定义在Graph-ToolFormer框架中研究的具体图推理任务。正如在前面的第[3.3](#S3.SS3 "3.3\.
    问题表述 ‣ 3\. 符号、术语定义和问题表述 ‣ Graph-ToolFormer: 通过ChatGPT增强提示赋能LLMs的图推理能力")节中介绍的，大多数图推理任务可以简化为几个非常基本的图学习任务，例如，（1）图属性计算，（2）节点分类，（3）图分类，（4）图分区/聚类，（5）链接预测/排名和（6）图搜索任务。对于第[4.3](#S4.SS3
    "4.3\. 图推理导向的提示 ‣ 4\. 提议的方法 ‣ Graph-ToolFormer: 通过ChatGPT增强提示赋能LLMs的图推理能力")节中介绍的所有应用导向的图推理任务，即，（1）图属性推理，（2）文献论文主题推理，（3）分子图功能推理，（4）社交网络社区推理，（5）推荐系统推理和（6）知识图谱推理，我们将把它们简化为图任务核心组件中的基本图学习任务。'
- en: Besides the hubs we mention above, within the Graph-ToolFormer framework, we
    also have an extra hub for hosting the LLMs to be used for the graph reasoning
    API generation based on the inputs received from the interaction with the end
    users. The LLM hub will host a set of fine-tuned language models for the graph
    reasoning tasks. Specifically, within the Graph-ToolFormer framework studied in
    this paper, several LLMs (like GPT-J 6B 8bit can be included in the hub for the
    output graph reasoning statement generation.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述提到的核心组件，在Graph-ToolFormer框架内，我们还增加了一个用于托管LLMs的额外核心组件，以根据与最终用户交互时接收到的输入生成图推理API。LLM核心组件将托管一组针对图推理任务进行微调的语言模型。具体而言，在本文研究的Graph-ToolFormer框架内，几个LLMs（如GPT-J
    6B 8bit）可以包含在核心组件中，用于输出图推理语句的生成。
- en: 4.7.4\. Query Executor Module
  id: totrans-420
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.4\. 查询执行模块
- en: 'Furthermore, the generation output will be further post-processed by detecting
    and initiating the API calls in it. Depending on whether the API call return result
    needs to be outputted or not, the executor Graph-ToolFormer will also further
    replace the API calls with its return result in the statement. For instance, for
    the example mentioned in Section [4.7.1](#S4.SS7.SSS1 "4.7.1\. LLMs Inference
    ‣ 4.7\. LLMs Inference and Graph Reasoning Query Parsing, Execution and Post-Processing
    ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"), based on the parsing result “((GR,
    [(GL, [”cora”]), ”graph-bert:topic”, {Paper#1}]), [True])”, Graph-ToolFormer will
    recognize and execute the query as follows:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，生成的输出将通过检测和启动其中的 API 调用进一步后处理。根据 API 调用返回结果是否需要输出，执行器 Graph-ToolFormer 还会在语句中进一步用返回结果替换
    API 调用。例如，对于第 [4.7.1](#S4.SS7.SSS1 "4.7.1\. LLMs Inference ‣ 4.7\. LLMs Inference
    and Graph Reasoning Query Parsing, Execution and Post-Processing ‣ 4\. Proposed
    Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt
    Augmented by ChatGPT") 节中提到的示例，基于解析结果“((GR, [(GL, [”cora”]), ”graph-bert:topic”,
    {Paper#1}]), [True])”，Graph-ToolFormer 将识别并执行查询如下：'
- en: •
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Outer Function: “GR”, i.e., the outer function is a for graph reasoning.'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外部函数：“GR”，即，外部函数用于图推理。
- en: •
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Outer Function Parameters: “[(GL, [”cora”]), ”graph-bert:topic”, {Paper#1}]
    ”.'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 外部函数参数：“[(GL, [”cora”]), ”graph-bert:topic”, {Paper#1}] ”。
- en: –
  id: totrans-426
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Parameter 1: “(GL, [”cora”])”, the first parameter is a nested API call.'
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数 1：“(GL, [”cora”])”，第一个参数是一个嵌套的 API 调用。
- en: '*'
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: 'Inner Function: “GL”, i.e., the inner function is a for graph loading.'
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内部函数：“GL”，即，内部函数用于图加载。
- en: '*'
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*'
- en: 'Inner Function Parameter(s): “[”cora”]”, i.e., the graph loading API will load
    the Cora network dataset.'
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 内部函数参数：“[”cora”]”，即图加载 API 将加载 Cora 网络数据集。
- en: –
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Parameter 2: “”graph-bert:topic””, the second parameter denotes the outer reasoning
    function aims to infer the topic with the Graph-Bert model.'
  id: totrans-433
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数 2：“”graph-bert:topic””，第二个参数表示外部推理函数旨在使用 Graph-Bert 模型推断主题。
- en: –
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Parameter 3: “{Paper#1}”, the third parameter denotes the outer reasoning function
    focuses on the “Paper#1” in the input graph dataset.'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 参数 3：“{Paper#1}”，第三个参数表示外部推理函数关注于输入图数据集中的“Paper#1”。
- en: •
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Output Insertion Tag: “True”, i.e., this query requires the replacement and
    insertion of this query result back into the statement.'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出插入标签：“True”，即，此查询要求将查询结果替换并插入回语句中。
- en: 'After parsing the query in the text, the query executor module in Graph-ToolFormer
    will execute the query by calling the corresponding API function with the provided
    graph data and parameters, i.e., “Graph-Bert.topic(cora, {Paper#1})”, which will
    return the graph reasoning query result, i.e., Neural Networks, as the output
    bibliographic paper topic reasoning query. Furthermore, since the output insertion
    token “–¿r” exist in the query, the query executor module in Graph-ToolFormer
    will also replace the query token sequence with the reasoning results, which will
    generate the final output by the Graph-ToolFormer as follows:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 在解析文本中的查询后，Graph-ToolFormer 中的查询执行模块将通过调用提供的图数据和参数的对应 API 函数来执行查询，即“Graph-Bert.topic(cora,
    {Paper#1})”，这将返回图推理查询结果，即神经网络，作为输出的参考文献论文主题推理查询。此外，由于查询中存在输出插入标记“–¿r”，Graph-ToolFormer
    中的查询执行模块还将用推理结果替换查询标记序列，这将生成 Graph-ToolFormer 的最终输出，如下：
- en: '$\bullet$ Input: The first paper in Cora has a topic of [TBR].'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 输入：Cora 中的第一篇论文的主题是 [TBR]。
- en: '$\bullet$ Post-processed Output: The first paper in Cora has a topic of Neural
    Networks.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 后处理输出：Cora 中的第一篇论文的主题是神经网络。
- en: 4.7.5\. Working Memory Module
  id: totrans-441
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.7.5\. 工作记忆模块
- en: What’s more, within the Graph-ToolFormer model, we also maintain a small-sized
    working memory, which keeps records of both the recent external API function calls
    (including both GL and GR API function calls) and their output results for the
    model in the reference stage. For instance, if the API calls on the graph loading
    $GL(file\text{-}path,node\text{-}subset,link\text{-}subset)$ directly. The reuse
    of pre-stored result from the working memory will save lots of time costs on graph
    reasoning tasks. The working memory has a pre-defined memory capacity in Graph-ToolFormer,
    and will maintain its stored information similar to a queue (i.e., FIFO). Once
    the stored information exceeds the working memory capacity, the result of the
    oldest API calls or the results which has been rewritten already will be removed
    from the working memory by Graph-ToolFormer.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在 Graph-ToolFormer 模型中，我们还维护一个小型工作记忆，该记忆记录了最近的外部 API 函数调用（包括 GL 和 GR API
    函数调用）及其输出结果，用于模型的参考阶段。例如，如果 API 调用图加载 $GL(file\text{-}path,node\text{-}subset,link\text{-}subset)$
    直接。重新使用工作记忆中预存的结果将节省大量图推理任务的时间成本。工作记忆在 Graph-ToolFormer 中具有预定义的内存容量，并将其存储的信息类似于队列（即
    FIFO）。一旦存储的信息超出工作记忆容量，Graph-ToolFormer 将从工作记忆中移除最旧的 API 调用结果或已经被重写的结果。
- en: 5\. Experiments
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 实验
- en: In this section, we will conduct extensive experiments to evaluate the performance
    of Graph-ToolFormer on various graph reasoning tasks that we have discussed before.
    According to the previous method section, we will use ChatGPT to generate a large-size
    of graph reasoning prompt dataset based on both the textual instructions and a
    small number of hand-crafted prompt reasoning examples. To ensure Graph-ToolFormer
    can handle diverse graph reasoning tasks, we will merge the generated prompt datasets
    for different graph reasoning tasks on different graph datasets together to obtain
    a mixed prompt dataset. By partitioning the mixed prompt dataset into training
    and testing sets, we will fine-tune existing pre-trained LLMs (e.g., GPT-J or
    LLaMA) on the training set, and evaluate its generation performance on the testing
    set. What’s more, the fine-tuned LLMs will be further plugged into Graph-ToolFormer
    for conducting graph reasoning based on the textual input statement or question
    queries. More details about the experimental settings and some experimental results
    will be provided in the following parts of this section. All the source code,
    datasets, and checkpoints of all the pre-trained graph models and fine-tuned LLMs
    have been released and shared to the community, which can be accessed via the
    github link provided at the beginning of this paper.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进行大量实验以评估 Graph-ToolFormer 在各种图推理任务中的性能，这些任务我们之前已讨论过。根据之前的方法部分，我们将使用
    ChatGPT 生成一个大规模的图推理提示数据集，该数据集基于文本指令和少量手工制作的提示推理示例。为了确保 Graph-ToolFormer 能处理多样的图推理任务，我们将合并不同图数据集上的不同图推理任务生成的提示数据集，以获得一个混合提示数据集。通过将混合提示数据集划分为训练集和测试集，我们将对现有预训练
    LLM（例如 GPT-J 或 LLaMA）进行微调，并评估其在测试集上的生成性能。此外，微调后的 LLM 将进一步集成到 Graph-ToolFormer
    中，用于基于文本输入声明或问题查询进行图推理。本节的后续部分将提供实验设置的更多细节和一些实验结果。所有预训练图模型和微调 LLM 的源代码、数据集和检查点都已发布并共享给社区，可以通过本文开头提供的
    github 链接访问。
- en: Table 3\. A statistical summary of graph datasets used in the experiments of
    this paper. For the GPR and molecular graph datasets (including PROTEIN, PTC,
    NCI1 and MUTAG), the “Node#” and “Edge#” denote the average numbers of nodes and
    edges for the graph instances in the datasets, respectively. For the graphs without
    features or labels, we will fill the entries with “NA” in the table.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 本文实验中使用的图数据集的统计汇总。对于 GPR 和分子图数据集（包括 PROTEIN、PTC、NCI1 和 MUTAG），其中的“Node#”和“Edge#”分别表示数据集中图实例的节点和边的平均数量。对于没有特征或标签的图，我们将表格中的条目填入“NA”。
- en: '| Tasks | Datasets | Graph Types | Node# | Edge# | Graph# | Feature# | Class#
    | Prompt# |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 图类型 | 节点数 | 边数 | 图数 | 特征数 | 类别数 | 提示数 |'
- en: '| Graph Loading | GL-Prompt | NA | NA | NA | NA | NA | NA | 2,802 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| 图加载 | GL-Prompt | NA | NA | NA | NA | NA | NA | 2,802 |'
- en: '| Property Reasoning | GPR-Prompt | Generated classic graphs | 14.70 (avg)
    | 28.27 (avg) | 37 | NA | NA | 2,587 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| 属性推理 | GPR-Prompt | 生成的经典图 | 14.70（平均） | 28.27（平均） | 37 | NA | NA | 2,587
    |'
- en: '| Paper Topic'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '| 论文主题'
- en: Reasoning | Cora | Bibliographic network | 2,708 | 5,429 | 1 | 1,433 | 7 | 18,956
    |
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | Cora | 参考文献网络 | 2,708 | 5,429 | 1 | 1,433 | 7 | 18,956 |
- en: '| Citeseer | Bibliographic network | 3,327 | 4,732 | 1 | 3,703 | 6 | 23,184
    |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| Citeseer | 文献网络 | 3,327 | 4,732 | 1 | 3,703 | 6 | 23,184 |'
- en: '| Pubmed | Bibliographic network | 19,717 | 44,338 | 1 | 500 | 3 | 138,019
    |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| Pubmed | 文献网络 | 19,717 | 44,338 | 1 | 500 | 3 | 138,019 |'
- en: '| Molecule Function'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '| 分子功能'
- en: Reasoning | PROTEINS | Protein molecular graphs | 39.05 (avg) | 72.82 (avg)
    | 1,113 | NA | 2 | 6,678 |
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | PROTEINS | 蛋白质分子图 | 39.05（平均） | 72.82（平均） | 1,113 | 无 | 2 | 6,678 |
- en: '| PTC | Chemical molecular graphs | 25.56 (avg) | 25.96 (avg) | 344 | NA |
    2 | 2,064 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| PTC | 化学分子图 | 25.56（平均） | 25.96（平均） | 344 | 无 | 2 | 2,064 |'
- en: '| NCI1 | Chemical molecular graphs | 29.86 (avg) | 32.30 (avg) | 4,110 | NA
    | 2 | 24,660 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| NCI1 | 化学分子图 | 29.86（平均） | 32.30（平均） | 4,110 | 无 | 2 | 24,660 |'
- en: '| MUTAG | Chemical molecular graphs | 17.93 (avg) | 19.79 (avg) | 188 | NA
    | 2 | 1,128 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| MUTAG | 化学分子图 | 17.93（平均） | 19.79（平均） | 188 | 无 | 2 | 1,128 |'
- en: '| Sequential Recommendation'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '| 顺序推荐'
- en: Reasoning | MovieLens | Recommender system | 2,625 | 100,000 | 1 | NA | 5 (rating)
    | 500,000 |
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | MovieLens | 推荐系统 | 2,625 | 100,000 | 1 | 无 | 5（评分） | 500,000 |
- en: '| Last.FM | Recommender system | 19,524 | 118,268 | 1 | NA | 2 (binary) | 355,320
    |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| Last.FM | 推荐系统 | 19,524 | 118,268 | 1 | 无 | 2（二进制） | 355,320 |'
- en: '| Amazon | Recommender system | 396,810 | 450,578 | 1 | NA | 5 (rating) | 2,252,890
    |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| Amazon | 推荐系统 | 396,810 | 450,578 | 1 | 无 | 5（评分） | 2,252,890 |'
- en: '| Social Community'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '| 社会社区'
- en: Reasoning | Foursquare | Social network | 5,392 | 76,972 | 1 | NA | NA | 64,710
    |
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | Foursquare | 社会网络 | 5,392 | 76,972 | 1 | 无 | 无 | 64,710 |
- en: '| Twitter | Social network | 5,223 | 164,920 | 1 | NA | NA | 52,240 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| Twitter | 社会网络 | 5,223 | 164,920 | 1 | 无 | 无 | 52,240 |'
- en: '| Knowledge Graph'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '| 知识图谱'
- en: Reasoning | Freebase | Knowledge graph | 14,951 | 592,213 | 1 | NA | NA | 1,695,651
    |
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | Freebase | 知识图谱 | 14,951 | 592,213 | 1 | 无 | 无 | 1,695,651 |
- en: '| WordNet | Knowledge graph | 41,105 | 151,442 | 1 | NA | NA | 454,326 |'
  id: totrans-467
  prefs: []
  type: TYPE_TB
  zh: '| WordNet | 知识图谱 | 41,105 | 151,442 | 1 | 无 | 无 | 454,326 |'
- en: 5.1\. Graph Benchmark Dataset Descriptions
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 图基准数据集描述
- en: 'As introduced in the previous Section [4.7.3](#S4.SS7.SSS3 "4.7.3\. Graph Reasoning
    Hubs ‣ 4.7\. LLMs Inference and Graph Reasoning Query Parsing, Execution and Post-Processing
    ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"), about $15$ different graph benchmark
    datasets are studied in this paper, which include'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节 [4.7.3](#S4.SS7.SSS3 "4.7.3\. 图推理中心 ‣ 4.7\. LLMs 推理与图推理查询解析、执行及后处理 ‣ 4\.
    提议方法 ‣ Graph-ToolFormer：通过 ChatGPT 增强 LLMs 的图推理能力") 所述，本文研究了约 $15$ 个不同的图基准数据集，包括
- en: •
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Property Reasoning Dataset: We create a toy dataset named “GPR” in this
    paper containing $37$ links on average.'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图属性推理数据集：我们在本文中创建了一个名为“GPR”的玩具数据集，包含平均 $37$ 个链接。
- en: •
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bibliographic Network Datasets: We use three benchmark bibliographic network
    datasets in the experiment for to infer the paper topics with Graph-ToolFormer,
    which include Cora, Pubmed, Citeseer ([Kipf_Semi_CORR_16,](#bib.bib19) ; [Zhang2020GraphBertOA,](#bib.bib60)
    ). Each node in these bibliographic networks denotes an academic paper, which
    are annotated with both numerical features and categorical labels indicating the
    paper topics.'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文献网络数据集：我们在实验中使用了三个基准文献网络数据集来通过 Graph-ToolFormer 推断论文主题，包括 Cora、Pubmed、Citeseer（[Kipf_Semi_CORR_16](#bib.bib19)
    ; [Zhang2020GraphBertOA](#bib.bib60)）。这些文献网络中的每个节点表示一篇学术论文，论文被标注了数值特征和表示论文主题的类别标签。
- en: •
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Molecular Graph Datasets: We use four molecular graph benchmark datasets in
    the experiments, which include PROTEINS, MUTAG, NCI1, PTC ([10.1145/2783258.2783417,](#bib.bib55)
    ; [Zhang2020SegmentedGF,](#bib.bib58) ). For the molecular graphs in these four
    datasets, we can obtain their topological structures and categorical label indicating
    the molecular graph functions.'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分子图数据集：我们在实验中使用了四个分子图基准数据集，包括 PROTEINS、MUTAG、NCI1、PTC（[10.1145/2783258.2783417](#bib.bib55)
    ; [Zhang2020SegmentedGF](#bib.bib58)）。对于这四个数据集中的分子图，我们可以获得其拓扑结构和表示分子图功能的类别标签。
- en: •
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Social Network Datasets: Two online social network benchmark datasets Twitter,
    Foursquare ([10.1145/2505515.2505531,](#bib.bib21) ) are investigated in this
    experiment. We can obtain both the social connections among the users and other
    diverse heterogeneous information. In the experiment, we will only use the social
    connections among the users to reason for the social communities from the social
    networks.'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 社会网络数据集：本实验研究了两个在线社会网络基准数据集 Twitter 和 Foursquare（[10.1145/2505515.2505531](#bib.bib21)）。我们可以获得用户之间的社会连接及其他多样的异质信息。在实验中，我们将仅使用用户之间的社会连接来推理社会网络中的社会社区。
- en: •
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recommender System Datasets: Three sequential recommender system datasets Amazon
    (Software) ([McAuley2015ImageBasedRO,](#bib.bib27) ), Last-FM ([Bertin-Mahieux2011,](#bib.bib3)
    ), Movielens (100K) ([10.1145/2827872,](#bib.bib14) ) are studied in this paper.
    For each recommender system, we have the user-item interaction records annotated
    with the timestamps. Existing sequential recommender systems will partition the
    datasets into training/testing sets by the timestamps: the historical records
    will be used for model training and the last interaction is used for testing.
    We will follow the same settings in the experiment.'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推荐系统数据集：本文研究了三个连续推荐系统数据集 Amazon（软件）（[McAuley2015ImageBasedRO,](#bib.bib27)）、Last-FM（[Bertin-Mahieux2011,](#bib.bib3)）和
    Movielens（100K）（[10.1145/2827872,](#bib.bib14)）。对于每个推荐系统，我们都有带有时间戳的用户-项目交互记录。现有的连续推荐系统将根据时间戳将数据集分为训练集和测试集：历史记录将用于模型训练，最后的交互用于测试。我们将在实验中遵循相同的设置。
- en: •
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Knowledge Graph Datasets: We also obtain and use two knowledge graph benchmark
    datasets WordNet ([10.1145/219717.219748,](#bib.bib30) ), Freebase ([10.1145/2567948.2577016,](#bib.bib2)
    ) in the experiments. These datasets will be partitioned into training/testing
    sets for knowledge graph embedding and model training.'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识图谱数据集：我们还在实验中获得并使用了两个知识图谱基准数据集 WordNet（[10.1145/219717.219748,](#bib.bib30)）和
    Freebase（[10.1145/2567948.2577016,](#bib.bib2)）。这些数据集将被划分为用于知识图谱嵌入和模型训练的训练集和测试集。
- en: 'To make it easier for the graph data hub to load the graph datasets for various
    reasoning tasks, we will pre-process the datasets and organize the graph information
    into a unified format, which has been described in detail in the Appendix. We
    also provide the dataset statistical information in Table [3](#S5.T3 "Table 3
    ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT"), which include both the statistics of these
    graph datasets (including numbers of nodes, links, graph instances, features and
    classes) and the obtained prompt dataset size. The raw graph datasets and the
    generated graph reasoning prompt datasets have been shared with the community
    and released at the github page³³3data github link: https://github.com/jwzhanggy/Graph_Toolformer/tree/main/data.'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使图数据中心更容易加载用于各种推理任务的图数据集，我们将预处理数据集并将图信息整理成统一的格式，详细内容已在附录中描述。我们还在表格[3](#S5.T3
    "表格 3 ‣ 5\. 实验 ‣ Graph-ToolFormer：通过 ChatGPT 提升 LLM 的图推理能力")中提供了数据集统计信息，包括这些图数据集的统计数据（包括节点数、链接数、图实例、特征和类别）以及获得的提示数据集大小。原始图数据集和生成的图推理提示数据集已经与社区共享，并在
    GitHub 页面³³3data github 链接：https://github.com/jwzhanggy/Graph_Toolformer/tree/main/data
    上发布。
- en: 5.2\. Pre-Trained Graph Models
  id: totrans-483
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 预训练图模型
- en: 'As introduced in the previous Section [4.7.3](#S4.SS7.SSS3 "4.7.3\. Graph Reasoning
    Hubs ‣ 4.7\. LLMs Inference and Graph Reasoning Query Parsing, Execution and Post-Processing
    ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"), Graph-ToolFormer also includes a set
    of pre-trained graph models in the framework for various graph reasoning tasks.
    According to the previous Section [3.3](#S3.SS3 "3.3\. Problem Formulation ‣ 3\.
    Notation, Terminology Definition and Problem Formulation ‣ Graph-ToolFormer: To
    Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), since
    most of the application oriented graph reasoning tasks can be reduced to the very
    fundamental graph learning tasks, like attribute calculation, node classification,
    graph classification, link prediction, graph partition/clustering and graph searching,
    these pre-trained graph models in Graph-ToolFormer will implement these fundamental
    graph learning functions correspondingly.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节[4.7.3](#S4.SS7.SSS3 "4.7.3\. 图推理中心 ‣ 4.7\. LLMs 推理和图推理查询解析、执行与后处理 ‣ 4\.
    提议的方法 ‣ Graph-ToolFormer：通过 ChatGPT 提升 LLM 的图推理能力")中介绍的，Graph-ToolFormer 还在框架中包括了一组用于各种图推理任务的预训练图模型。根据前一节[3.3](#S3.SS3
    "3.3\. 问题表述 ‣ 3\. 符号、术语定义和问题表述 ‣ Graph-ToolFormer：通过 ChatGPT 提升 LLM 的图推理能力")，由于大多数面向应用的图推理任务可以归结为非常基础的图学习任务，如属性计算、节点分类、图分类、链接预测、图划分/聚类和图搜索，因此
    Graph-ToolFormer 中的这些预训练图模型将相应地实现这些基础的图学习功能。
- en: •
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Toolx: The current toolx model in Graph-ToolFormer is implemented based on
    networkx, and toolx will implement different API functions to calculate different
    graph properties mentioned in the paper, including order, size, density, eccentricity,
    radius, diameter, center, shortest-path, avg-path-length, min-path-length, max-path-length,
    periphery.'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Toolx: 目前在Graph-ToolFormer中的Toolx模型是基于networkx实现的，Toolx将实现不同的API功能，以计算论文中提到的不同图属性，包括顺序、大小、密度、离心率、半径、直径、中心、最短路径、平均路径长度、最小路径长度、最大路径长度、外围。'
- en: •
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph-Bert: The Graph-Bert model proposed in ([Zhang2020GraphBertOA,](#bib.bib60)
    ) can effectively learn the representations of graph data. The Graph-Bert model
    will be used to implement the bibliographic network paper topic inference function
    in Graph-ToolFormer, which will implement the corresponding API function of node-classification.'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Graph-Bert: 在([Zhang2020GraphBertOA,](#bib.bib60))中提出的Graph-Bert模型可以有效地学习图数据的表示。Graph-Bert模型将用于在Graph-ToolFormer中实现书目网络论文主题推理功能，这将实现相应的节点分类API功能。'
- en: •
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'SEG-Bert: The SEG-Bert model original proposed in ([Zhang2020SegmentedGF,](#bib.bib58)
    ) can both embed and classify graph instances. It will be used to implement the
    molecular graph function inference function and implement the corresponding graph-classification
    API function.'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'SEG-Bert: 在([Zhang2020SegmentedGF,](#bib.bib58))中最初提出的SEG-Bert模型可以同时嵌入和分类图实例。它将用于实现分子图功能推理，并实现相应的图分类API功能。'
- en: •
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'KMeans: In Graph-ToolFormer, we extend the KMeans algorithm ([MacQueen1967,](#bib.bib26)
    ) to partition the social network data for detecting the social communities. Specifically,
    we calculate the common neighbor to define the affinity matrix among users in
    social networks, and define the community-detection function with KMeans to partition
    the nodes into clusters.'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'KMeans: 在Graph-ToolFormer中，我们扩展了KMeans算法([MacQueen1967,](#bib.bib26))以对社交网络数据进行划分，以检测社交社区。具体来说，我们计算共同邻居来定义社交网络中用户之间的亲和矩阵，并定义基于KMeans的社区检测功能，将节点划分为簇。'
- en: •
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BPR: The BPR (Bayesian Personalized Ranking) proposed in ([10.5555/1795114.1795167,](#bib.bib42)
    ) will compare and rank the personalized positive and negative user-item pairs
    for model learning. In Graph-ToolFormer, we use BPR to define the API functions
    about recommender systems, which include recommendation (to calculate the scores
    for provided user-item pair) and top k-recommendation (to recommend top-k items
    for provided user).'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'BPR: 在([10.5555/1795114.1795167,](#bib.bib42))中提出的BPR（贝叶斯个性化排序）将比较和排序个性化的正负用户-项目对用于模型学习。在Graph-ToolFormer中，我们使用BPR定义关于推荐系统的API功能，包括推荐（计算提供的用户-项目对的评分）和前k推荐（为提供的用户推荐前k个项目）。'
- en: •
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TransE: The The TransE model proposed in ([NIPS2013_1cecc7a7,](#bib.bib4) )
    will be used to implement the knowledge graph entity/relation reasoning functions
    in Graph-ToolFormer, will be reduced to several graph searching functions like
    search-head-entity, search-tail-entity and search-relation.'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'TransE: 在([NIPS2013_1cecc7a7,](#bib.bib4))中提出的TransE模型将用于在Graph-ToolFormer中实现知识图谱实体/关系推理功能，将被简化为几个图搜索函数，如search-head-entity、search-tail-entity和search-relation。'
- en: These graph models will be pre-trained with the datasets introduced above by
    following the identical train/test set partition. Via some testing, the performance
    of these models are comparable to the scores reported in the existing papers ([Zhang2020GraphBertOA,](#bib.bib60)
    ; [Zhang2020SegmentedGF,](#bib.bib58) ; [10.5555/1795114.1795167,](#bib.bib42)
    ; [NIPS2013_1cecc7a7,](#bib.bib4) ). The source code and pre-trained checkpoints
    of these graph models have been shared with the community, which can be accessed
    at the github page mentioned above.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 这些图模型将使用上述数据集进行预训练，遵循相同的训练/测试集划分。通过一些测试，这些模型的性能与现有论文中报告的评分相当([Zhang2020GraphBertOA,](#bib.bib60);
    [Zhang2020SegmentedGF,](#bib.bib58); [10.5555/1795114.1795167,](#bib.bib42); [NIPS2013_1cecc7a7,](#bib.bib4))。这些图模型的源代码和预训练检查点已与社区共享，可以在上述github页面访问。
- en: 5.3\. Pre-Trained Language Models
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 预训练语言模型
- en: As the base model to be used for building Graph-ToolFormer, we have tried several
    pre-trained language models with open-source model architectures, configurations,
    tokenizers and parameter checkpoints. Specifically, the base language models used
    in this experiment include
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 作为构建Graph-ToolFormer的基础模型，我们尝试了几种预训练语言模型，具有开源模型架构、配置、分词器和参数检查点。具体来说，这次实验中使用的基础语言模型包括
- en: •
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph-ToolFormer (GPT-J 6B, 8bit): The EleutherAI’s GPT-J (6B) is a transformer
    model trained using Ben Wang’s “Mesh Transformer JAX” ([gpt-j,](#bib.bib53) )
    that has 6 billion parameters. To load GPT-J in float 32, it will require 22+GB
    RAM to load the model and the fine-tuning will require at least 4x RAM size. To
    further lower-down the RAM consumption for GPT-J (6B), researchers also propose
    to quantize it with 8-bit weights ([gpt-j-8bit,](#bib.bib11) ), which allows scalable
    fine-tuning with LoRA (Low-Rank Adaptation) and 8-bit Adam and GPU quantization
    from bitsandbytes. The Graph-ToolFormer (GPT-J 6B, 8bit) model will use GPT-J
    6B 8bit as the base model for fine-tuning.'
  id: totrans-501
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Graph-ToolFormer (GPT-J 6B, 8bit): EleutherAI 的 GPT-J (6B) 是一种使用 Ben Wang 的“Mesh
    Transformer JAX” ([gpt-j,](#bib.bib53)) 训练的变压器模型，具有 60 亿个参数。要以浮点 32 加载 GPT-J，需要
    22+GB 的内存，微调则至少需要 4 倍的内存。为了进一步降低 GPT-J (6B) 的内存消耗，研究人员还提出将其量化为 8 位权重 ([gpt-j-8bit,](#bib.bib11))，这允许使用
    LoRA (低秩适应) 和 8 位 Adam 以及来自 bitsandbytes 的 GPU 量化进行可扩展微调。Graph-ToolFormer (GPT-J
    6B, 8bit) 模型将使用 GPT-J 6B 8bit 作为微调的基础模型。'
- en: More base LLMs will be added and compared in the experiments. Both the source
    code and the checkpoints of the fine-tuned LLMs used in the experiment have been
    shared to the community as well.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 实验中将添加并比较更多基础LLM。用于实验的微调LLM的源代码和检查点也已共享给社区。
- en: 5.4\. Experimental Settings
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 实验设置
- en: Table 4\. A summary of the experimental results of Graph-ToolFormer on various
    graph reasoning tasks on the corresponding benchmark datasets. The results are
    evaluated by the Rouge scores, BLEU and BP scores. Except for the graph loading
    task, we also evaluate the results on other tasks/datasets by comparing the graph
    reasoning API calls (other textual contents are excluded) with the ground-truth
    API calls, and report the Accuracy on reasoning API calls in the table as well.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. Graph-ToolFormer 在各种图推理任务上的实验结果总结，结果使用 Rouge 分数、BLEU 和 BP 分数进行评估。除了图加载任务外，我们还通过将图推理
    API 调用（其他文本内容被排除）与真实 API 调用进行比较，评估其他任务/数据集的结果，并在表中报告推理 API 调用的准确率。
- en: '| Tasks | Datasets | Methods | Evaluation Metrics |  |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 方法 | 评估指标 |  |'
- en: '| Rouge-1 | Rouge-2 | Rouge-L | Rouge-LSum | BLEU | BP | API-Gen Acc |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| Rouge-1 | Rouge-2 | Rouge-L | Rouge-LSum | BLEU | BP | API-Gen Acc |'
- en: '| Graph Loading | GL-Prompt | Graph-ToolFormer | 82.28 | 67.74 | 70.93 | 70.85
    | 63.53 | 89.98 | 4.38 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 图加载 | GL-Prompt | Graph-ToolFormer | 82.28 | 67.74 | 70.93 | 70.85 | 63.53
    | 89.98 | 4.38 |'
- en: '| Property Reasoning | GPR-Prompt | Graph-ToolFormer | 94.56 | 92.10 | 91.69
    | 91.69 | 91.53 | 99.93 | 80.00 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| 属性推理 | GPR-Prompt | Graph-ToolFormer | 94.56 | 92.10 | 91.69 | 91.69 | 91.53
    | 99.93 | 80.00 |'
- en: '| Paper Topic'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '| 论文主题'
- en: Reasoning | Cora | Graph-ToolFormer | 99.69 | 99.68 | 99.69 | 99.69 | 99.2 |
    100.0 | 100.0 |
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | Cora | Graph-ToolFormer | 99.69 | 99.68 | 99.69 | 99.69 | 99.2 | 100.0
    | 100.0 |
- en: '| Citeseer | Graph-ToolFormer | 100.0 | 100.0 | 100.0 | 100.0 | 99.39 | 100.0
    | 97.5 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| Citeseer | Graph-ToolFormer | 100.0 | 100.0 | 100.0 | 100.0 | 99.39 | 100.0
    | 97.5 |'
- en: '| Pubmed | Graph-ToolFormer | 99.91 | 99.84 | 99.91 | 99.91 | 99.04 | 100.0
    | 99.38 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| Pubmed | Graph-ToolFormer | 99.91 | 99.84 | 99.91 | 99.91 | 99.04 | 100.0
    | 99.38 |'
- en: '| Molecule Function'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: '| 分子功能'
- en: Reasoning | PROTEINS | Graph-ToolFormer | 99.61 | 99.19 | 99.61 | 99.61 | 98.27
    | 100.0 | 100.0 |
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | PROTEINS | Graph-ToolFormer | 99.61 | 99.19 | 99.61 | 99.61 | 98.27 | 100.0
    | 100.0 |
- en: '| PTC | Graph-ToolFormer | 100.0 | 100.0 | 100.0 | 100.0 | 98.52 | 100.0 |
    100.0 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| PTC | Graph-ToolFormer | 100.0 | 100.0 | 100.0 | 100.0 | 98.52 | 100.0 |
    100.0 |'
- en: '| NCI1 | Graph-ToolFormer | 100.0 | 100.0 | 100.0 | 100.0 | 98.28 | 100.0 |
    100.0 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| NCI1 | Graph-ToolFormer | 100.0 | 100.0 | 100.0 | 100.0 | 98.28 | 100.0 |
    100.0 |'
- en: '| MUTAG | Graph-ToolFormer | 100.0 | 100.0 | 100.0 | 100.0 | 98.72 | 100.0
    | 100.0 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| MUTAG | Graph-ToolFormer | 100.0 | 100.0 | 100.0 | 100.0 | 98.72 | 100.0
    | 100.0 |'
- en: '| Sequential Recommendation'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '| 序列推荐'
- en: Reasoning | MovieLens | Graph-ToolFormer | 97.47 | 96.56 | 97.47 | 97.47 | 94.63
    | 95.31 | 93.12 |
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | MovieLens | Graph-ToolFormer | 97.47 | 96.56 | 97.47 | 97.47 | 94.63 |
    95.31 | 93.12 |
- en: '| Last.FM | Graph-ToolFormer | 89.24 | 86.69 | 88.75 | 88.79 | 83.43 | 89.67
    | 85.62 |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| Last.FM | Graph-ToolFormer | 89.24 | 86.69 | 88.75 | 88.79 | 83.43 | 89.67
    | 85.62 |'
- en: '| Amazon | Graph-ToolFormer | 99.9 | 99.8 | 99.9 | 99.9 | 99.74 | 100.0 | 100.0
    |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| Amazon | Graph-ToolFormer | 99.9 | 99.8 | 99.9 | 99.9 | 99.74 | 100.0 | 100.0
    |'
- en: '| Social Community'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '| 社区'
- en: Reasoning | Foursquare | Graph-ToolFormer | 98.6 | 98.01 | 98.51 | 98.46 | 97.41
    | 100.0 | 95.0 |
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | Foursquare | Graph-ToolFormer | 98.6 | 98.01 | 98.51 | 98.46 | 97.41 |
    100.0 | 95.0 |
- en: '| Twitter | Graph-ToolFormer | 99.86 | 99.71 | 99.78 | 99.76 | 99.75 | 99.89
    | 98.75 |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| Twitter | Graph-ToolFormer | 99.86 | 99.71 | 99.78 | 99.76 | 99.75 | 99.89
    | 98.75 |'
- en: '| Knowledge Graph'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '| 知识图谱'
- en: Reasoning | Freebase | Graph-ToolFormer | 91.98 | 91.79 | 91.97 | 92.0 | 78.17
    | 78.29 | 53.75 |
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 | Freebase | Graph-ToolFormer | 91.98 | 91.79 | 91.97 | 92.0 | 78.17 | 78.29
    | 53.75 |
- en: '| WordNet | Graph-ToolFormer | 98.73 | 98.73 | 98.73 | 98.73 | 97.99 | 98.69
    | 96.88 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| WordNet | Graph-ToolFormer | 98.73 | 98.73 | 98.73 | 98.73 | 97.99 | 98.69
    | 96.88 |'
- en: 5.4.1\. Experimental Setups
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 实验设置
- en: Based on generated graph reasoning prompt datasets, we will fine-tune the base
    language models. Considering the high-cost in generation for evaluation, we split
    the prompt datasets for each graph reasoning task into training/testing with the
    size ratio “$\min(N-160,1,600):160$ instances will be used as the training set.
    To ensure Graph-ToolFormer will be able to handle diverse graph reasoning queries,
    those sampled training/testing sets for each graph reasoning task on these graph
    datasets will be merged together for the base language model fine-tuning and evaluation
    in Graph-ToolFormer.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 基于生成的图推理提示数据集，我们将微调基础语言模型。考虑到评估中的高成本，我们将每个图推理任务的提示数据集拆分为训练/测试集，大小比例为“$\min(N-160,1,600):160$
    实例将作为训练集。为了确保 Graph-ToolFormer 能够处理多样化的图推理查询，我们将这些图数据集中每个图推理任务的采样训练/测试集合并在一起，用于基础语言模型的微调和
    Graph-ToolFormer 中的评估。
- en: 'Specifically, the hardware and software setups for the fine-tuning of the language
    models in this experiment are provided as follows:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本实验中语言模型微调的硬件和软件设置如下：
- en: •
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hardware: We run the fine-tuning experiments on a stand-along workstation with
    several Nvidia GPUs. Detailed hardware information about the workstation is as
    follows: ASUS WS X299 SAGE/10G LGA motherboard, Intel Core i7 CPU 6850K@3.6GHz
    (6 cores), 1 Nvidia Ampere A100 GPU (80 GB HBM2e DRAM), 1 Nvidia GeForce RTX 4090
    Founders Edition GPU (24GB GDDR6X RAM), and 96 GB DDR4 memory and 128 GB SSD swap.'
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 硬件：我们在配备多张 Nvidia GPU 的独立工作站上进行微调实验。工作站的详细硬件信息如下：ASUS WS X299 SAGE/10G LGA 主板，Intel
    Core i7 CPU 6850K@3.6GHz（6 核心），1 张 Nvidia Ampere A100 GPU（80 GB HBM2e DRAM），1
    张 Nvidia GeForce RTX 4090 Founders Edition GPU（24GB GDDR6X RAM），以及 96 GB DDR4
    内存和 128 GB SSD 交换空间。
- en: •
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'System and Software: We run the experiment on Ubuntu 22.04, with CUDA toolkit
    version 11.8, Nvidia Driver version 520, PyTorch version 1.13.1 and Python 3.9\.
    For the optimizer of Graph-ToolFormer (GPT-J 6B, 8bit), we use the 8-bit AdamW
    from bitsandbytes with version 0.37.1\. We load the pre-trained GPT-J 6B 8bit
    from Huggingface with weight parameter checkpoint “hivemind/gpt-j-6B-8bit” and
    config/tokenizer checkpoint “EleutherAI/gpt-j-6b” as the base model of Graph-ToolFormer,
    and the installed transformer toolkit version is 4.28.0.dev0\. More information
    about other system and software configurations can be found at the shared anaconda
    environment file⁴⁴4https://github.com/jwzhanggy/Graph_Toolformer/blob/main/environment.yml.'
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 系统和软件：我们在 Ubuntu 22.04 上运行实验，使用 CUDA 工具包版本 11.8，Nvidia 驱动版本 520，PyTorch 版本 1.13.1
    和 Python 3.9。对于 Graph-ToolFormer（GPT-J 6B, 8bit）的优化器，我们使用来自 bitsandbytes 的 8-bit
    AdamW，版本为 0.37.1。我们从 Huggingface 加载预训练的 GPT-J 6B 8bit，权重参数检查点为“hivemind/gpt-j-6B-8bit”，配置/分词器检查点为“EleutherAI/gpt-j-6b”，作为
    Graph-ToolFormer 的基础模型，安装的 transformer 工具包版本为 4.28.0.dev0。有关其他系统和软件配置的更多信息，请参见共享的
    anaconda 环境文件⁴⁴4https://github.com/jwzhanggy/Graph_Toolformer/blob/main/environment.yml。
- en: •
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hyper-parameters: For fine-tuning Graph-ToolFormer (GPT-J 6B, 8bit), we use
    AdamW with a very small learning rate 1e-5 with weight decay 1e-2, and a max-epoch
    of 3\. Both the training and testing instances are divided into batches with shuffle
    with batch size 32 and we set the max input/output token length as 128\. For the
    generation function of the language model, the following hyper-parameters are
    used, i.e., num-beams: 5, top-k: 5, top-p: 0.95, temperature: 1.9, num-return-sequence:
    1, max-length: 128.'
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '超参数：为了微调 Graph-ToolFormer（GPT-J 6B, 8bit），我们使用 AdamW 优化器，学习率非常小，为 1e-5，权重衰减为
    1e-2，最大训练周期为 3。训练和测试实例都被分成批次，并使用 batch size 32 进行随机打乱，我们设置最大输入/输出 token 长度为 128。语言模型的生成函数使用以下超参数：num-beams:
    5，top-k: 5，top-p: 0.95，temperature: 1.9，num-return-sequence: 1，max-length: 128。'
- en: 'Especially, when the batch size and input/output max token length are assigned
    with small values (e.g., batch-size: 1 or 2 and max-length: 64), we can also fine-tune
    Graph-ToolFormer (GPT-J 6B, 8bit) model on GPUs with smaller RAM (like Nvidia
    1080ti with 11GB memory). It will allow most research groups and individuals to
    tune and deploy Graph-ToolFormer to provide LLMs based graph reasoning functions
    and services.'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '特别地，当批次大小和输入/输出最大 token 长度设置为小值（例如，batch-size: 1 或 2 和 max-length: 64）时，我们还可以在具有较小
    RAM（如带有 11GB 内存的 Nvidia 1080ti）的 GPU 上微调 Graph-ToolFormer（GPT-J 6B, 8bit）模型。这将允许大多数研究小组和个人调整和部署
    Graph-ToolFormer，以提供基于 LLM 的图推理功能和服务。'
- en: 5.4.2\. Performance Evaluation
  id: totrans-538
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2\. 性能评估
- en: 'For the preliminary performance evaluation of Graph-ToolFormer, we have used
    several evaluation metrics as follows in the experiments:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Graph-ToolFormer 的初步性能评估，我们在实验中使用了以下几种评估指标：
- en: •
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ROUGE scores: By comparing the outputs of the Graph-ToolFormer framework with
    the ground-truth, we calculate the Rouge-1, Rouge-2, Rouge-L and Rouge-LSum scores
    obtained by the model.'
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ROUGE 分数：通过将 Graph-ToolFormer 框架的输出与真实值进行比较，我们计算了模型获得的 Rouge-1、Rouge-2、Rouge-L
    和 Rouge-LSum 分数。
- en: •
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'BLEU scores: Besides the ROUGE scores, we also evaluate the performance of
    Graph-ToolFormer with the BLEU and BP metrics by comparing the generation output
    with the ground-truth.'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: BLEU 分数：除了 ROUGE 分数外，我们还通过将生成输出与真实值进行比较，使用 BLEU 和 BP 指标来评估 Graph-ToolFormer
    的性能。
- en: •
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'API Generation Accuracy: From the generated statement by the LLMs in Graph-ToolFormer,
    we will extract and parse the API call queries to compare with the ground-truth.
    The accuracy of the generated API call queries will also be reported as an extra
    evaluation metric in this experiment.'
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: API 生成准确性：从 Graph-ToolFormer 中 LLMs 生成的语句中，我们将提取并解析 API 调用查询，以与真实值进行比较。生成的 API
    调用查询的准确性也将作为额外的评估指标报告。
- en: 5.5\. Graph Reasoning Output Statement Generation
  id: totrans-546
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5\. 图推理输出语句生成
- en: Table 5\. Case analysis of graph reasoning API call statement generation, query
    parsing, and query execution and final output generation in Graph-ToolFormer.
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 图推理 API 调用语句生成、查询解析、查询执行及最终输出生成的案例分析，见 Graph-ToolFormer。
- en: '| Tasks | Graph Reasoning Examples with Graph-ToolFormer |  |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | Graph-ToolFormer 的图推理示例 |  |'
- en: '| Input Statements | Generated Statements with API calls | Query parsing |
    Reasoning | Final Statement Output |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| 输入语句 | 通过 API 调用生成的语句 | 查询解析 | 推理 | 最终语句输出 |'
- en: '| Graph Property Reasoning | What is the order of the diamond graph? | The
    order of the diamond graph is [GR(GL(”gpr”, {”diamond_graph”}), ”toolx:order”)–¿r].
    | ((’GR’, [(’GL’, [’gpr’, ’{”diamond_graph”}’]), ’toolx:order’]), [True]) | 4
    | The order of the diamond graph is 4. |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 图属性推理 | 菱形图的阶数是多少？ | 菱形图的阶数是 [GR(GL(”gpr”, {”diamond_graph”}), ”toolx:order”)–¿r]。
    | ((’GR’, [(’GL’, [’gpr’, ’{”diamond_graph”}’]), ’toolx:order’]), [True]) | 4
    | 菱形图的阶数是 4。 |'
- en: '| The path graph has a center at nodes [TBR]. | The path graph has a center
    at nodes [GR(GL(”gpr”, {”path_graph”}), ”toolx:center”)–¿r]. | ((’GR’, [(’GL’,
    [’gpr’, ’{”path_graph”}’]), ’toolx:center’]), [True]) | [5, 6] | The path graph
    has a center at nodes [5, 6]. |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| 路径图的中心在节点 [TBR] 处。 | 路径图的中心在节点 [GR(GL(”gpr”, {”path_graph”}), ”toolx:center”)–¿r]
    处。 | ((’GR’, [(’GL’, [’gpr’, ’{”path_graph”}’]), ’toolx:center’]), [True]) | [5,
    6] | 路径图的中心在节点 [5, 6] 处。 |'
- en: '| The nodes in the wheel graph have eccentricity values of [TBR]. | The nodes
    in the wheel graph have eccentricity values of [GR(GL(”gpr”, {”wheel_graph”}),
    ”toolx:eccentricity”)–¿r]. | ((’GR’, [(’GL’, [’gpr’, ’{”wheel_graph”}’]), ’toolx:eccentricity’]),
    [True]) | {0: 1, 1: 2, 2: 2, 3: 2, 4: 2, 5: 2} | The nodes in the wheel graph
    have eccentricity values of {0: 1, 1: 2, 2: 2, 3: 2, 4: 2, 5: 2}. |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| 轮图中的节点的离心率值为 [TBR]。 | 轮图中的节点的离心率值为 [GR(GL(”gpr”, {”wheel_graph”}), ”toolx:eccentricity”)–¿r]。
    | ((’GR’, [(’GL’, [’gpr’, ’{”wheel_graph”}’]), ’toolx:eccentricity’]), [True])
    | {0: 1, 1: 2, 2: 2, 3: 2, 4: 2, 5: 2} | 轮图中的节点的离心率值为 {0: 1, 1: 2, 2: 2, 3: 2,
    4: 2, 5: 2}。 |'
- en: '| Bibliographic Network Reasoning | What is the topic of paper #83826 in the
    cora bibliographic network? | The topic of paper #83826 in the cora bibliographic
    network is [GR(GL(”cora”), ”graph_bert:topic”, paper#83826)–¿r]. | ((’GR’, [(’GL’,
    [’cora’]), ’graph_bert:topic’, ’paper#83826’]), [True]) | Neural_Networks | The
    topic of paper #83826 in the cora bibliographic network is Neural_Networks. |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| 文献网络推理 | cora 文献网络中 #83826 号论文的主题是什么？ | cora 文献网络中 #83826 号论文的主题是 [GR(GL(”cora”),
    ”graph_bert:topic”, paper#83826)–¿r]。 | ((’GR’, [(’GL’, [’cora’]), ’graph_bert:topic’,
    ’paper#83826’]), [True]) | Neural_Networks | cora 文献网络中 #83826 号论文的主题是 Neural_Networks。
    |'
- en: '| Paper #2998, situated in the citeseer bibliographic network, is centered
    around the [TBR] topic. | Paper #2998, situated in the citeseer bibliographic
    network, is centered around the [GR(GL(”citeseer”), ”graph_bert:topic”, paper#2998)–¿r]
    topic. | ((’GR’, [(’GL’, [’citeseer’]), ’graph_bert:topic’, ’paper#2998’]), [True])
    | AI | Paper #2998, situated in the citeseer bibliographic network, is centered
    around the AI topic. |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 位于 citeseer 文献网络中的 #2998 号论文围绕 [TBR] 主题。 | 位于 citeseer 文献网络中的 #2998 号论文围绕
    [GR(GL(”citeseer”), ”graph_bert:topic”, paper#2998)–¿r] 主题。 | ((’GR’, [(’GL’,
    [’citeseer’]), ’graph_bert:topic’, ’paper#2998’]), [True]) | AI | 位于 citeseer
    文献网络中的 #2998 号论文围绕 AI 主题。 |'
- en: '| Within the pubmed bibliographic network, paper #16466 focuses on the subject
    of [TBR]. | Within the pubmed bibliographic network, paper #16466 focuses on the
    subject of [GR(GL(”pubmed”), ”graph_bert:topic”, paper#16466)–¿r]. | ((’GR’, [(’GL’,
    [’pubmed’]), ’graph_bert:topic’, ’paper#16466’]), [True]) | 1 | Within the pubmed
    bibliographic network, paper #16466 focuses on the subject of 1. |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| 在pubmed书目网络中，论文#16466关注的主题是[TBR]。 | 在pubmed书目网络中，论文#16466关注的主题是[GR(GL(”pubmed”),
    ”graph_bert:topic”, paper#16466)–¿r]。 | ((’GR’, [(’GL’, [’pubmed’]), ’graph_bert:topic’,
    ’paper#16466’]), [True]) | 1 | 在pubmed书目网络中，论文#16466关注的主题是1。 |'
- en: '| Molecular Graph Reasoning | What is the function for the protein molecular
    graph #138 in proteins? | The function for the protein molecular graph #138 in
    proteins is [GR(GL(”proteins”), ”seg_bert:molecule_function”, instance#138)–¿r].
    | ((’GR’, [(’GL’, [’proteins’]), ’seg_bert:molecule_function’, ’instance#138’]),
    [True]) | 0 | The function for the protein molecular graph #138 in proteins is
    0. |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| 分子图推理 | 蛋白质中的蛋白质分子图#138的功能是什么？ | 蛋白质中的蛋白质分子图#138的功能是[GR(GL(”proteins”), ”seg_bert:molecule_function”,
    instance#138)–¿r]。 | ((’GR’, [(’GL’, [’proteins’]), ’seg_bert:molecule_function’,
    ’instance#138’]), [True]) | 0 | 蛋白质中的蛋白质分子图#138的功能是0。 |'
- en: '| In mutag, instance #30 of the chemical molecular graph demonstrates a function
    of [TBR]. | In mutag, instance #30 of the chemical molecular graph demonstrates
    a function of [GR(GL(”mutag”), ”seg_bert:molecule_function”, instance#30)–¿r].
    | ((’GR’, [(’GL’, [’mutag’]), ’seg_bert:molecule_function’, ’instance#30’]), [True])
    | 2 | In mutag, instance #30 of the chemical molecular graph demonstrates a function
    of 2. |'
  id: totrans-557
  prefs: []
  type: TYPE_TB
  zh: '| 在mutag中，化学分子图实例#30展示了[TBR]的功能。 | 在mutag中，化学分子图实例#30展示了[GR(GL(”mutag”), ”seg_bert:molecule_function”,
    instance#30)–¿r]的功能。 | ((’GR’, [(’GL’, [’mutag’]), ’seg_bert:molecule_function’,
    ’instance#30’]), [True]) | 2 | 在mutag中，化学分子图实例#30展示了2的功能。 |'
- en: '| For chemical molecular graph instance #652 in nci1, its function is [TBR].
    | For chemical molecular graph instance #652 in nci1, its function is [GR(GL(”nci1”),
    ”seg_bert:molecule_function”, instance#652)–¿r]. | ((’GR’, [(’GL’, [’nci1’]),
    ’seg_bert:molecule_function’, ’instance#652’]), [True]) | 0 | For chemical molecular
    graph instance #652 in nci1, its function is 0. |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 化学分子图实例#652在nci1中的功能是[TBR]。 | 化学分子图实例#652在nci1中的功能是[GR(GL(”nci1”), ”seg_bert:molecule_function”,
    instance#652)–¿r]。 | ((’GR’, [(’GL’, [’nci1’]), ’seg_bert:molecule_function’,
    ’instance#652’]), [True]) | 0 | 化学分子图实例#652在nci1中的功能是0。 |'
- en: '| The chemical molecular graph numbered 239 in ptc is characterized by a function
    of [TBR]. | The chemical molecular graph numbered 239 in ptc is characterized
    by a function of [GR(GL(”ptc”), ”seg_bert:molecule_function”, instance#239)–¿r].
    | ((’GR’, [(’GL’, [’ptc’]), ’seg_bert:molecule_function’, ’instance#239’]), [True])
    | 0 | The chemical molecular graph numbered 239 in ptc is characterized by a function
    of 0. |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| 化学分子图编号为239的图在ptc中的特征由[TBR]的函数决定。 | 化学分子图编号为239的图在ptc中的特征由[GR(GL(”ptc”),
    ”seg_bert:molecule_function”, instance#239)–¿r]的函数决定。 | ((’GR’, [(’GL’, [’ptc’]),
    ’seg_bert:molecule_function’, ’instance#239’]), [True]) | 0 | 化学分子图编号为239的图在ptc中的特征由0的函数决定。
    |'
- en: '| Recommender System Reasoning | How likely user #A1HOLE9R6WPT85 will be interested
    in item #B00005MOTF in Amazon? | The likelihood that user #A1HOLE9R6WPT85 will
    be interested in item #B00005MOTF in Amazon is [GR(GL(”amazon”), ”bpr:recommendation”,
    user#A1HOLE9R6WPT85, item#B00005MOTF)–¿r]. | ((’GR’, [(’GL’, [’amazon’]), ’bpr:recommendation’,
    ’user#A1HOLE9R6WPT85’, ’item#B00005MOTF’]), [True]) | 0.008 | The likelihood that
    user #A1HOLE9R6WPT85 will be interested in item #B00005MOTF in Amazon is 0.008.
    |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| 推荐系统推理 | 用户#A1HOLE9R6WPT85对Amazon中的商品#B00005MOTF感兴趣的可能性有多大？ | 用户#A1HOLE9R6WPT85对Amazon中的商品#B00005MOTF感兴趣的可能性是[GR(GL(”amazon”),
    ”bpr:recommendation”, user#A1HOLE9R6WPT85, item#B00005MOTF)–¿r]。 | ((’GR’, [(’GL’,
    [’amazon’]), ’bpr:recommendation’, ’user#A1HOLE9R6WPT85’, ’item#B00005MOTF’]),
    [True]) | 0.008 | 用户#A1HOLE9R6WPT85对Amazon中的商品#B00005MOTF感兴趣的可能性是0.008。 |'
- en: '| The likelihood that user #u1527 will be interested in music from artisit
    #i5422 in Last-fm is [TBR]. | The likelihood that user #u1527 will be interested
    in music from artisit #i5422 in Last-fm is [GR(GL(”last-fm”), ”bpr:recommendation”,
    user#u1527, artisit#i5422)–¿r]. | ((’GR’, [(’GL’, [’last-fm’]), ’bpr:recommendation’,
    ’user#u1527’, ’artisit#i5422’]), [True]) | 0.248 | The likelihood that user #u1527
    will be interested in music from artisit #i5422 in Last-fm is 0.248. |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| 用户#u1527对Last-fm中艺术家#i5422的音乐感兴趣的可能性是[TBR]。 | 用户#u1527对Last-fm中艺术家#i5422的音乐感兴趣的可能性是[GR(GL(”last-fm”),
    ”bpr:recommendation”, user#u1527, artisit#i5422)–¿r]。 | ((’GR’, [(’GL’, [’last-fm’]),
    ’bpr:recommendation’, ’user#u1527’, ’artisit#i5422’]), [True]) | 0.248 | 用户#u1527对Last-fm中艺术家#i5422的音乐感兴趣的可能性是0.248。
    |'
- en: '| In Movielens, the top 10 movies that user #u272 likes include [TBR]. | In
    Movielens, the top 10 movies that user #u272 likes include [GR(GL(”movielens”),
    ”bpr:topk_recommendation”, user#u272, 10)–¿r]. | ((’GR’, [(’GL’, [’movielens’]),
    ’bpr:topk_recommendation’, ’user#u272’, ’10’]), [True]) | [’i286’, ’i288’, ’i258’,
    ’i294’, ’i300’, ’i50’, ’i313’, ’i100’, ’i181’, ’i269’] | In Movielens, the top
    10 movies that user #u272 likes include [’i286’, ’i288’, ’i258’, ’i294’, ’i300’,
    ’i50’, ’i313’, ’i100’, ’i181’, ’i269’]. |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 在Movielens中，用户#u272喜欢的前10部电影包括[TBR]。 | 在Movielens中，用户#u272喜欢的前10部电影包括[GR(GL(”movielens”),
    ”bpr:topk_recommendation”, user#u272, 10)–¿r]。 | ((’GR’, [(’GL’, [’movielens’]),
    ’bpr:topk_recommendation’, ’user#u272’, ’10’]), [True]) | [’i286’, ’i288’, ’i258’,
    ’i294’, ’i300’, ’i50’, ’i313’, ’i100’, ’i181’, ’i269’] | 在Movielens中，用户#u272喜欢的前10部电影包括[’i286’,
    ’i288’, ’i258’, ’i294’, ’i300’, ’i50’, ’i313’, ’i100’, ’i181’, ’i269’]。 |'
- en: '| Social Network Reasoning | In the online social network foursquare, which
    community is user user/1265481 involved in? | In the online social network foursquare,
    user user/1265481 is involved in the [GR(GL(”foursquare”), ”kmeans:community”,
    user#user/1265481)–¿r] communities formed by users. | (’GR’, [(’GL’, [’foursquare’]),
    ’kmeans:community’, ’user#user/1265481’]) | #2 | In the online social network
    foursquare, user user/1265481 is involved in the #2 communities formed by users.
    |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| 社交网络推理 | 在在线社交网络foursquare中，用户user/1265481涉及哪个社区？ | 在在线社交网络foursquare中，用户user/1265481涉及由用户形成的[GR(GL(”foursquare”),
    ”kmeans:community”, user#user/1265481)–¿r]社区。 | (’GR’, [(’GL’, [’foursquare’]),
    ’kmeans:community’, ’user#user/1265481’]) | #2 | 在在线社交网络foursquare中，用户user/1265481涉及第#2个由用户形成的社区。
    |'
- en: '| In the online social network twitter, are user #deeprogress and user #alejandro1254
    belong to the same community? | In the online social network twitter, user #deeprogress
    and user #alejandro1254 belong to [GR(GL(”twitter”), ”kmeans:common_community_check”,
    user#deeprogress, user#alejandro1254)–¿r] community. | (’GR’, [(’GL’, [’twitter’]),
    ’kmeans:common_community_check’, ’user#deeprogress’, ’user#alejandro1254’]) |
    The Same | In the online social network twitter, user #deeprogress and user #alejandro1254
    belong to the same community. |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| 在在线社交网络推特中，用户#deeprogress和用户#alejandro1254是否属于同一个社区？ | 在在线社交网络推特中，用户#deeprogress和用户#alejandro1254属于[GR(GL(”twitter”),
    ”kmeans:common_community_check”, user#deeprogress, user#alejandro1254)–¿r]社区。
    | (’GR’, [(’GL’, [’twitter’]), ’kmeans:common_community_check’, ’user#deeprogress’,
    ’user#alejandro1254’]) | 相同 | 在在线社交网络推特中，用户#deeprogress和用户#alejandro1254属于同一个社区。
    |'
- en: '| Knowledge Graph Reasoning | According to the Freebase knowledge graph, what
    is the relation between entity#/m/053yx and entity#/m/015_1q? | According to the
    Freebase knowledge graph, the relation between entity#/m/053yx and entity#/m/015_1q
    is [GR(GL(”freebase”), ”transe:relation”, entity#/m/053yx, entity#/m/015_1q)–¿r].
    | (’GR’, [(’GL’, [’freebase’]), ’transe:relation’, ’entity#/m/053yx’, ’entity#/m/015_1q’])
    | /music/artist/label | According to the Freebase knowledge graph, the relation
    between entity#/m/053yx and entity#/m/015_1q is /music/artist/label. |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| 知识图谱推理 | 根据Freebase知识图谱，实体#/m/053yx和实体#/m/015_1q之间的关系是什么？ | 根据Freebase知识图谱，实体#/m/053yx和实体#/m/015_1q之间的关系是[GR(GL(”freebase”),
    ”transe:relation”, entity#/m/053yx, entity#/m/015_1q)–¿r]。 | (’GR’, [(’GL’, [’freebase’]),
    ’transe:relation’, ’entity#/m/053yx’, ’entity#/m/015_1q’]) | /music/artist/label
    | 根据Freebase知识图谱，实体#/m/053yx和实体#/m/015_1q之间的关系是/music/artist/label。 |'
- en: '| According to the WordNet knowledge graph, via relation #_hypernym, what entity
    can we obtain from entity #imagination.n.02? | According to the WordNet knowledge
    graph, via relation #_hypernym, we can obtain entity #imagination.n.02 from entity
    [GR(GL(”wordnet”), ”transe:head_entity”, relation#_hypernym, entity#imagination.n.02)–¿r].
    | (’GR’, [(’GL’, [’wordnet’]), ’transe:head_entity’, ’relation#_hypernym’, ’entity#imagination.n.02’])
    | chimera.n.02 | According to the WordNet knowledge graph, via relation #_hypernym,
    we can obtain entity #imagination.n.02 from entity chimera.n.02. |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '| 根据WordNet知识图谱，通过关系#_hypernym，我们可以从实体#imagination.n.02中获得什么实体？ | 根据WordNet知识图谱，通过关系#_hypernym，我们可以从实体[GR(GL(”wordnet”),
    ”transe:head_entity”, relation#_hypernym, entity#imagination.n.02)–¿r]中获得实体#imagination.n.02。
    | (’GR’, [(’GL’, [’wordnet’]), ’transe:head_entity’, ’relation#_hypernym’, ’entity#imagination.n.02’])
    | chimera.n.02 | 根据WordNet知识图谱，通过关系#_hypernym，我们可以从实体chimera.n.02中获得实体#imagination.n.02。
    |'
- en: Table 6\. Analysis of generated API query cases different from ground-truth
    queries in the testing set. In this table, we illustrate the examples of input
    statements, generated statements by Graph-ToolFormer and the desired ground-truth
    statements of different graph reasoning tasks studied in this paper.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 分析在测试集中与真实查询不同的生成 API 查询案例。在此表中，我们说明了本文中研究的不同图推理任务的输入语句、由 Graph-ToolFormer
    生成的语句以及期望的真实语句的示例。 |
- en: '| Tasks | Failed case examples |'
  id: totrans-568
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 失败的案例示例 |'
- en: '| Input Statements | Generated Statements | Ground-Truth Statements |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '| 输入语句 | 生成语句 | 真实语句 |'
- en: '| Graph Property Reasoning | Nodes have the largest eccentricity [TBR] in the
    lollipop graph, which make them part of its periphery. | root¿’s eccentricity
    is [GR(GL(”gpr”, ”lollipop_graph”), ”toolx:eccentricity”, ¡root¿)–¿r].root¿’s
    eccentricity is [GR(GL(”gpr”, ”lollipop_graph”), ”toolx:eccentricity”, ¡root¿)–¿r1].root¿’s
    eccentricity is [GR(GL(”gpr | Nodes [GR(GL(”gpr”, ”lollipop_graph”), ”toolx:periphery”)–¿r]
    have the largest eccentricity [GR(GL(”gpr”, ”lollipop_graph”), ”toolx:eccentricity”)]
    in the lollipop graph, which make them part of its periphery. |'
  id: totrans-570
  prefs: []
  type: TYPE_TB
  zh: '| 图属性推理 | 在棒棒糖图中，节点具有最大的离心率 [TBR]，这使得它们成为图的一部分边缘。 | root¿的离心率是[GR(GL(”gpr”,
    ”lollipop_graph”), ”toolx:eccentricity”, ¡root¿)–¿r]。root¿的离心率是[GR(GL(”gpr”, ”lollipop_graph”),
    ”toolx:eccentricity”, ¡root¿)–¿r1]。root¿的离心率是[GR(GL(”gpr | 节点[GR(GL(”gpr”, ”lollipop_graph”),
    ”toolx:periphery”)–¿r]具有棒棒糖图中最大的离心率[GR(GL(”gpr”, ”lollipop_graph”), ”toolx:eccentricity”)]，这使得它们成为图的一部分边缘。
    |'
- en: '| The nodes with the smallest eccentricity in the house x graph are [TBR],
    which are also the center of the tree. | center¿the nodes with the smallest eccentricity
    in the house x graph are [GR(GL(”gpr”, ”house_x_graph”), ”toolx:eccentricity”)–¿r],
    which are also the [GR(GL(”gpr”, ”house_x_graph”), ”toolx:center”)–¿r]. | The
    nodes with the smallest eccentricity [GR(GL(”gpr”, ”house_x_graph”), ”toolx:eccentricity”)]
    in the house x graph are [GR(GL(”gpr”, ”house_x_graph”), ”toolx:center”)–¿r],
    which are also the center of the tree. |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| 在房子 x 图中离心率最小的节点是 [TBR]，这些节点也是树的中心。 | center¿房子 x 图中离心率最小的节点是 [GR(GL(”gpr”,
    ”house_x_graph”), ”toolx:eccentricity”)–¿r]，这些节点也是 [GR(GL(”gpr”, ”house_x_graph”),
    ”toolx:center”)–¿r]。 | 房子 x 图中离心率最小的节点 [GR(GL(”gpr”, ”house_x_graph”), ”toolx:eccentricity”)]
    是 [GR(GL(”gpr”, ”house_x_graph”), ”toolx:center”)–¿r]，这些节点也是树的中心。 |'
- en: '| What is the length of shortest path between node #3 and node #2 in the diamond
    graph? | The length of shortest path between node #3 and node #2 in the diamond
    graph is [GR(GL(”gpr”, ”diamond_graph”), ”toolx:shortest_path”, ”node#3”, ”node#2”)–¿r].
    | In the diamond graph, the length of shortest path between node #3 and node #2
    is [GR(GL(”gpr”, ”diamond_graph”), ”toolx:shortest_path”, ”node#3”, ”node#2”)–¿r].
    |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| 在钻石图中，节点 #3 和节点 #2 之间的最短路径长度是多少？ | 在钻石图中，节点 #3 和节点 #2 之间的最短路径长度是[GR(GL(”gpr”,
    ”diamond_graph”), ”toolx:shortest_path”, ”node#3”, ”node#2”)–¿r]。 | 在钻石图中，节点 #3
    和节点 #2 之间的最短路径长度是[GR(GL(”gpr”, ”diamond_graph”), ”toolx:shortest_path”, ”node#3”,
    ”node#2”)–¿r]。 |'
- en: '| Bibliographic Network Reasoning | The cora bibliographic network’ paper #28487
    is concerned with the area of [TBR]. | ROOT¿’s cora bibliographic network’ paper
    #28487 is concerned with the area of [GR(GL(”cora”), ”graph_bert:topic”, paper#28487)–¿r].
    | The cora bibliographic network’ paper #28487 is concerned with the area of [GR(GL(”cora”),
    ”graph_bert:topic”, paper#28487)–¿r]. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献网络推理 | cora 参考文献网络中的论文 #28487 关注的是领域 [TBR]。 | ROOT¿的 cora 参考文献网络中的论文
    #28487 关注的是领域 [GR(GL(”cora”), ”graph_bert:topic”, paper#28487)–¿r]。 | cora 参考文献网络中的论文
    #28487 关注的是领域 [GR(GL(”cora”), ”graph_bert:topic”, paper#28487)–¿r]。 |'
- en: '| What is the topic of paper #5832 in the pubmed bibliographic network? | The
    topic of paper #paper_number¿ in the pubmed bibliographic network is [GR(GL(”pubmed”),
    ”graph_bert:topic”, paper#paper_number¿)–¿r]. | The topic of paper #5832 in the
    pubmed bibliographic network is [GR(GL(”pubmed”), ”graph_bert:topic”, paper#5832)–¿r].
    |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| pubmed 参考文献网络中论文 #5832 的主题是什么？ | pubmed 参考文献网络中论文 #paper_number¿ 的主题是 [GR(GL(”pubmed”),
    ”graph_bert:topic”, paper#paper_number¿)–¿r]。 | pubmed 参考文献网络中论文 #5832 的主题是 [GR(GL(”pubmed”),
    ”graph_bert:topic”, paper#5832)–¿r]。 |'
- en: '| Molecular Graph Reasoning | What is the function for the protein molecular
    graph #573 in proteins? | The function of the protein molecular graph #573 in
    proteins is [GR(GL(”proteins”), ”seg_bert:molecule_function”, instance#573)–¿r].
    | The function for the protein molecular graph #573 in proteins is [GR(GL(”proteins”),
    ”seg_bert:molecule_function”, instance#573)–¿r]. |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| 分子图推理 | 蛋白质分子图 #573 在蛋白质中的功能是什么？ | 蛋白质分子图 #573 在蛋白质中的功能是 [GR(GL(”proteins”),
    ”seg_bert:molecule_function”, instance#573)–¿r]。 | 蛋白质分子图 #573 在蛋白质中的功能是 [GR(GL(”proteins”),
    ”seg_bert:molecule_function”, instance#573)–¿r]。 |'
- en: '| Recommender System Reasoning | In Amazon, what is the item that user #A3C08BZRVV500V
    will be most likely to purchase next? | In Amazon, what is the item that user
    #A3C08BZRVV500V will be most likely to purchase next is [GR(GL(”amazon”), ”bpr:topk_recommendation”,
    user#A3C08BZRVV500V, 1)–¿r]. | In Amazon, the item that user #A3C08BZRVV500V will
    be most likely to purchase next is [GR(GL(”amazon”), ”bpr:topk_recommendation”,
    user#A3C08BZRVV500V, 1)–¿r]. |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| 推荐系统推理 | 在 Amazon 中，用户 #A3C08BZRVV500V 最有可能下一步购买的物品是什么？ | 在 Amazon 中，用户 #A3C08BZRVV500V
    最有可能下一步购买的物品是 [GR(GL(”amazon”), ”bpr:topk_recommendation”, user#A3C08BZRVV500V,
    1)–¿r]。 | 在 Amazon 中，用户 #A3C08BZRVV500V 最有可能下一步购买的物品是 [GR(GL(”amazon”), ”bpr:topk_recommendation”,
    user#A3C08BZRVV500V, 1)–¿r]。 |'
- en: '| In Movielens, which movie user #u273 will be most likely to watch next? |
    In Movielens, which movie user #u273 will be most likely to watch next? | In Movielens,
    the movie that user #u273 will be most likely to watch next is [GR(GL(”movielens”),
    ”bpr:topk_recommendation”, user#u273, 1)–¿r]. |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| 在 Movielens 中，用户 #u273 最有可能接下来观看的电影是什么？ | 在 Movielens 中，用户 #u273 最有可能接下来观看的电影是什么？
    | 在 Movielens 中，用户 #u273 最有可能接下来观看的电影是 [GR(GL(”movielens”), ”bpr:topk_recommendation”,
    user#u273, 1)–¿r]。 |'
- en: '| In Last-fm, which artisit user #u1156 will be most likely to listen to next?
    | artisit¿ artisit¿ artisit¿ artisit¿ artisit¿ artisit¿ artisit¿ artisit¿ artisit¿
    artisit¿ artisit¿ artisit¿ artisit¿ artisit¿ | In Last-fm, the artisit that user
    #u1156 will be most likely to listen to next is [GR(GL(”last-fm”), ”bpr:topk_recommendation”,
    user#u1156, 1)–¿r]. |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| 在 Last-fm 中，用户 #u1156 最有可能接下来听到的艺术家是谁？ | 艺术家¿ 艺术家¿ 艺术家¿ 艺术家¿ 艺术家¿ 艺术家¿ 艺术家¿
    艺术家¿ 艺术家¿ 艺术家¿ 艺术家¿ 艺术家¿ 艺术家¿ 艺术家¿ | 在 Last-fm 中，用户 #u1156 最有可能接下来听到的艺术家是 [GR(GL(”last-fm”),
    ”bpr:topk_recommendation”, user#u1156, 1)–¿r]。 |'
- en: '| Social Network Reasoning | In the online social network foursquare, are user
    #victorcarbonero and user #user/11979222 belong to the same community? | In the
    online social network foursquare, user #user/victorcarbonero and user #user/11979222
    belong to [GR(GL(”foursquare”), ”kmeans:common_community_check”, user#user/victorcarbonero,
    user#user/11979222)–¿r] community. | In the online social network foursquare,
    user #victorcarbonero and user #user/11979222 belong to [GR(GL(”foursquare”),
    ”kmeans:common_community_check”, user#victorcarbonero, user#user/11979222)–¿r]
    community. |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| 社交网络推理 | 在在线社交网络 foursquare 中，用户 #victorcarbonero 和用户 #user/11979222 是否属于同一社区？
    | 在在线社交网络 foursquare 中，用户 #user/victorcarbonero 和用户 #user/11979222 属于 [GR(GL(”foursquare”),
    ”kmeans:common_community_check”, user#user/victorcarbonero, user#user/11979222)–¿r]
    社区。 | 在在线社交网络 foursquare 中，用户 #victorcarbonero 和用户 #user/11979222 属于 [GR(GL(”foursquare”),
    ”kmeans:common_community_check”, user#victorcarbonero, user#user/11979222)–¿r]
    社区。 |'
- en: '| In the online social network twitter, are user #iancr and user #ClassyIndeed
    belong to the same community? | iancr and user #ClassyIndeed belong to [GR(GL(”twitter”),
    ”kmeans:common_community_check”, user#iancr, user#ClassyIndeed)–¿r] community.
    | In the online social network twitter, user #iancr and user #ClassyIndeed belong
    to [GR(GL(”twitter”), ”kmeans:common_community_check”, user#iancr, user#ClassyIndeed)–¿r]
    community. |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| 在在线社交网络 twitter 中，用户 #iancr 和用户 #ClassyIndeed 是否属于同一社区？ | 用户 #iancr 和用户 #ClassyIndeed
    属于 [GR(GL(”twitter”), ”kmeans:common_community_check”, user#iancr, user#ClassyIndeed)–¿r]
    社区。 | 在在线社交网络 twitter 中，用户 #iancr 和用户 #ClassyIndeed 属于 [GR(GL(”twitter”), ”kmeans:common_community_check”,
    user#iancr, user#ClassyIndeed)–¿r] 社区。 |'
- en: '| Knowledge Graph Reasoning | According to the WordNet knowledge graph, via
    relation #_derivationally_related_form, we can obtain entity #scaremonger.n.01
    from entity [TBR]. |  | According to the WordNet knowledge graph, via relation
    #_derivationally_related_form, we can obtain entity #scaremonger.n.01 from entity
    [GR(GL(”wordnet”), ”transe:head_entity”, relation#_derivationally_related_form,
    entity#scaremonger.n.01)–¿r]. |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| 知识图谱推理 | 根据 WordNet 知识图谱，通过关系 #_derivationally_related_form，我们可以从实体 [TBR]
    获得实体 #scaremonger.n.01。 |  | 根据 WordNet 知识图谱，通过关系 #_derivationally_related_form，我们可以从实体
    [GR(GL(”wordnet”), ”transe:head_entity”, relation#_derivationally_related_form,
    entity#scaremonger.n.01)–¿r] 获得实体 #scaremonger.n.01。 |'
- en: '| According to the Freebase knowledge graph, from entity#/m/03r8tl, via relation
    #/award/award_category /nominees./award/award _nomination/award_nominee, what
    entity can we derive? | According to the Freebase knowledge graph, from entity#/m/03r8tl,
    via relation #/award/award_category/nominees./award/award_nomination /award_nominee,
    we can derive entity [GR(GL(”freebase”), ”tr | According to the Freebase knowledge
    graph, from entity#/m/03r8tl, via relation #/award/award_category/nominees./award/award_nomination
    /award_nominee, we can derive entity [GR(GL(”freebase”), ”transe:tail_entity”,
    entity#/m/03r8tl, relation#/award/award_category/nominees./award /award_nomination/award_nominee)–¿r].
    |'
  id: totrans-582
  prefs: []
  type: TYPE_TB
  zh: '| 根据 Freebase 知识图谱，从实体#/m/03r8tl，通过关系 #/award/award_category/nominees./award/award_nomination/award_nominee，我们可以推导出什么实体？
    | 根据 Freebase 知识图谱，从实体#/m/03r8tl，通过关系 #/award/award_category/nominees./award/award_nomination/award_nominee，我们可以推导出实体
    [GR(GL(”freebase”), ”tr | 根据 Freebase 知识图谱，从实体#/m/03r8tl，通过关系 #/award/award_category/nominees./award/award_nomination/award_nominee，我们可以推导出实体
    [GR(GL(”freebase”), ”transe:tail_entity”, 实体#/m/03r8tl, 关系#/award/award_category/nominees./award/award_nomination/award_nominee)–¿r]。'
- en: 'To evaluate the performance of the fine-tuned LLMs in Graph-ToolFormer on generating
    the statements with graph reasoning API calls, we have obtained the results of
    the LLM (GPT-J) on the prompt testing set and the evaluation scores are reported
    in Table [4](#S5.T4 "Table 4 ‣ 5.4\. Experimental Settings ‣ 5\. Experiments ‣
    Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented
    by ChatGPT").'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '为了评估 Graph-ToolFormer 中微调后的 LLM 在生成带有图形推理 API 调用的陈述方面的性能，我们获得了 LLM（GPT-J）在提示测试集上的结果，并在表[4](#S5.T4
    "表 4 ‣ 5.4\. 实验设置 ‣ 5\. 实验 ‣ Graph-ToolFormer: 通过 ChatGPT 增强的提示赋能 LLM 的图形推理能力")中报告了评估分数。'
- en: 5.5.1\. Graph Data Loading
  id: totrans-584
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.1\. 图数据加载
- en: 'As shown in Table [3](#S5.T3 "Table 3 ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"),
    based on both the instruction and input-output prompt example pairs, we apply
    ChatGPT to generate about $5,000$ are used for the model fine-tuning in the experiment.
    According to the experimental settings introduced before, we partition the pairs
    into training and testing sets, and evaluate the performance of the fine-tuned
    model to evaluate the performance of the Graph-ToolFormer framework for graph
    data loading.'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[3](#S5.T3 "表 3 ‣ 5\. 实验 ‣ Graph-ToolFormer: 通过 ChatGPT 增强的提示赋能 LLM 的图形推理能力")所示，基于指令和输入输出提示示例对，我们应用
    ChatGPT 生成了约 $5,000$ 个样本用于模型微调。在前述实验设置的基础上，我们将这些样本分为训练集和测试集，评估微调模型的性能，从而评估 Graph-ToolFormer
    框架在图数据加载方面的表现。'
- en: 'The experimental results of Graph-ToolFormer on graph data loading evaluated
    by Rouge and BLEU metrics are provided in Table [4](#S5.T4 "Table 4 ‣ 5.4\. Experimental
    Settings ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"). According to the provided scores, compared
    with the ground-truth, the outputs generated by Graph-ToolFormer are not bad,
    which obtained the R1 score of $82.28$ only.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: '图表[4](#S5.T4 "表 4 ‣ 5.4\. 实验设置 ‣ 5\. 实验 ‣ Graph-ToolFormer: 通过 ChatGPT 增强的提示赋能
    LLM 的图形推理能力")中提供了通过 Rouge 和 BLEU 指标评估的 Graph-ToolFormer 在图数据加载方面的实验结果。根据提供的分数，与真实值相比，Graph-ToolFormer
    生成的输出表现还不错，仅获得了 $82.28$ 的 R1 分数。'
- en: 5.5.2\. General Graph Property Loading
  id: totrans-587
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.2\. 一般图属性加载
- en: 'For the graph property reasoning task, we manually create a graph dataset,
    involving $27$ small-sized classic graph instances, such as barbell graph, wheel
    graph and lollipop graph, etc. For each graph instance in the dataset, we manually
    design a few number of reasoning prompts with API calls for its properties (as
    discussed in Section [4.3.2](#S4.SS3.SSS2 "4.3.2\. Graph Property Reasoning ‣
    4.3\. Graph Reasoning Oriented Prompts ‣ 4\. Proposed Method ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")).
    Based on both the property reasoning instructions and the hand-crafted prompt
    examples, with ChatGPT, we further augment the prompt examples and generate a
    large set of annotated graph property reasoning API call dataset, which will be
    used for fine-tuning the LLMs in the Graph-ToolFormer framework.'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '对于图属性推理任务，我们手动创建了一个图数据集，包含 $27$ 个小型经典图实例，例如杠铃图、轮图和棒棒糖图等。对于数据集中的每个图实例，我们手动设计了少量的推理提示，并通过
    API 调用进行属性推理（如第 [4.3.2](#S4.SS3.SSS2 "4.3.2\. Graph Property Reasoning ‣ 4.3\.
    Graph Reasoning Oriented Prompts ‣ 4\. Proposed Method ‣ Graph-ToolFormer: To
    Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT") 节讨论）。基于属性推理指令和手工制作的提示示例，结合
    ChatGPT，我们进一步增强提示示例并生成大量注释图属性推理 API 调用数据集，这将用于在 Graph-ToolFormer 框架中对 LLMs 进行微调。'
- en: 'Some basic statistics about the raw graph dataset and the prompt dataset are
    provided in the Table [3](#S5.T3 "Table 3 ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT").
    On average, the generated graph instances have about $14.70$ for the graph property
    reasoning task.'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '关于原始图数据集和提示数据集的一些基本统计信息见表格 [3](#S5.T3 "Table 3 ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")。平均而言，生成的图实例在图属性推理任务中大约有
    $14.70$。'
- en: 5.5.3\. Bibliographic Paper Topic Reasoning
  id: totrans-590
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.3\. 参考文献论文主题推理
- en: 'We have studied three bibliographic network datasets in this experiment, which
    include Cora, Citeseer and Pubmed, which are all the frequently used benchmark
    datasets studied in graph neural network research work. For each of the bibliographic
    network dataset, we hand-craft a few prompt examples and also augment them with
    ChatGPT to rephrase and rewrite more diverse input-output prompt pairs. After
    data filtering, the valid data instances (which works and can obtain the correct
    graph reasoning results with the API calls) will be used for the model fine-tuning,
    whose statistical information are provided in Table [3](#S5.T3 "Table 3 ‣ 5\.
    Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via
    Prompt Augmented by ChatGPT"). The performance of the LLMs studied in the experiments
    are provided in Table [4](#S5.T4 "Table 4 ‣ 5.4\. Experimental Settings ‣ 5\.
    Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via
    Prompt Augmented by ChatGPT"). Among the three datasets, the performance of Graph-ToolFormer
    on all these three datasets are very close to $100$ for all these R1, BLEU and
    API generation Accuracy metrics.'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '在本实验中，我们研究了三个参考文献网络数据集，分别是 Cora、Citeseer 和 Pubmed，这些都是图神经网络研究中经常使用的基准数据集。对于每个参考文献网络数据集，我们手工制作了一些提示示例，并通过
    ChatGPT 进行增强，重新措辞和重写更多样的输入-输出提示对。在数据过滤后，有效的数据实例（能够通过 API 调用获得正确图推理结果）将用于模型微调，其统计信息见表格
    [3](#S5.T3 "Table 3 ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with
    Graph Reasoning Ability via Prompt Augmented by ChatGPT")。实验中 LLMs 的性能见表格 [4](#S5.T4
    "Table 4 ‣ 5.4\. Experimental Settings ‣ 5\. Experiments ‣ Graph-ToolFormer: To
    Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")。在这三个数据集中，Graph-ToolFormer
    在所有这些数据集上的 R1、BLEU 和 API 生成准确度指标的表现都接近 $100$。'
- en: 5.5.4\. Protein Molecule Function Reasoning
  id: totrans-592
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.4\. 蛋白质分子功能推理
- en: 'For the bio-chemical molecular graph function reasoning task studied in this
    paper, we will use four bio-chemical graph classification benchmark dataset in
    the experiments, which include PROTEINS, PTC, NCI1 and MUTAG. In these dataset,
    each graph instance has both its molecular graph structure and a label indicating
    its function. For each graph instance, we hand-craft a few prompt examples about
    its functions, which will be augmented by ChatGPT to rephrase and generate similar
    data instances. The statistical information about these four datasets are provided
    in Table [3](#S5.T3 "Table 3 ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower
    LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"). We provide
    the experimental results of the comparison language models in Table [4](#S5.T4
    "Table 4 ‣ 5.4\. Experimental Settings ‣ 5\. Experiments ‣ Graph-ToolFormer: To
    Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), and
    Graph-ToolFormer works perfectly on PROTEINS, PTC, NCI1.'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '对于本文研究的生物化学分子图函数推理任务，我们将在实验中使用四个生物化学图分类基准数据集，包括 PROTEINS、PTC、NCI1 和 MUTAG。在这些数据集中，每个图实例都有其分子图结构和一个表示其功能的标签。对于每个图实例，我们手工制作了一些关于其功能的提示示例，这些示例将通过
    ChatGPT 扩展，重新表述并生成类似的数据实例。这四个数据集的统计信息见表[3](#S5.T3 "Table 3 ‣ 5\. Experiments ‣
    Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented
    by ChatGPT")。我们在表[4](#S5.T4 "Table 4 ‣ 5.4\. Experimental Settings ‣ 5\. Experiments
    ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented
    by ChatGPT")中提供了比较语言模型的实验结果，Graph-ToolFormer 在 PROTEINS、PTC、NCI1 上表现完美。'
- en: 5.5.5\. Sequential Recommender System Reasoning
  id: totrans-594
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.5\. 序列推荐系统推理
- en: 'For the sequential recommendation reasoning task, we have used three benchmark
    datasets studied in recommender systems, which include MovieLens, Last.FM and
    Amazon Review (Software). In these recommender system datasets, both users and
    items are represented as individual nodes and the interaction between users and
    items are represented as the links connecting them annotated with the timestamps.
    Similar to the previous reasoning tasks, we also design a few reasoning prompt
    examples and further augment them with ChatGPT to generate a large sequential
    recommender system reasoning dataset. The statistical information about both the
    raw datasets and the generated reasoning input-output pairs are provided in Table [3](#S5.T3
    "Table 3 ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"). By comparing the studied language models
    with each, we provide the experimental results of these comparison methods in
    Table [4](#S5.T4 "Table 4 ‣ 5.4\. Experimental Settings ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT").
    Graph-ToolFormer works very well on generating the correct API calls to the recommender
    system reasoning tasks on both Movielens and Amazon, and the scores on Last.FM
    are slightly lower than the other two.'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '对于序列推荐推理任务，我们使用了三个在推荐系统中研究的基准数据集，包括 MovieLens、Last.FM 和 Amazon Review (Software)。在这些推荐系统数据集中，用户和物品都被表示为单独的节点，而用户和物品之间的交互则被表示为连接它们的链接，并附有时间戳。与之前的推理任务类似，我们也设计了一些推理提示示例，并通过
    ChatGPT 进一步扩展，以生成一个大规模的序列推荐系统推理数据集。有关原始数据集和生成的推理输入输出对的统计信息见表[3](#S5.T3 "Table
    3 ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT")。通过比较研究的语言模型，我们在表[4](#S5.T4 "Table 4 ‣ 5.4\.
    Experimental Settings ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with
    Graph Reasoning Ability via Prompt Augmented by ChatGPT")中提供了这些比较方法的实验结果。Graph-ToolFormer
    在生成正确的 API 调用方面在 MovieLens 和 Amazon 上表现非常好，而在 Last.FM 上的得分略低于其他两个。'
- en: 5.5.6\. Online Social Network Reasoning
  id: totrans-596
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.6\. 在线社交网络推理
- en: In the social network community reasoning task, we use two online social network
    benchmark datasets, which include Foursquare (an location based online social
    network) and Twitter (an microblog online social network). These two datasets
    are initially crawled and used in the social network alignment paper ([10.1145/2505515.2505531,](#bib.bib21)
    ), and there exist no ground-truth community information about them. Both Foursquare
    and Twitter are not very big, containing more than $5,000$ of the API calls in
    the outputs.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 在社交网络社区推理任务中，我们使用了两个在线社交网络基准数据集，包括 Foursquare（一个基于位置的在线社交网络）和 Twitter（一个微博在线社交网络）。这两个数据集最初在社交网络对齐论文中被抓取和使用（[10.1145/2505515.2505531](#bib.bib21)），并且没有关于它们的真实社区信息。Foursquare
    和 Twitter 的规模都不大，包含超过 $5,000$ 个 API 调用。
- en: 5.5.7\. Knowledge Graph Reasoning
  id: totrans-598
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.5.7\. 知识图谱推理
- en: 'For the knowledge graph reasoning, we use two benchmark datasets, Freebase
    and WordNet, in the experiments, which provides both entities and their internal
    relations. In Table [3](#S5.T3 "Table 3 ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"),
    we provide the statistical information about the knowledge graphs and the generated
    reasoning prompts. For the Freebase knowledge graph, there exist $1,345$ instead.
    Among all these tasks studied in this paper, we observe that Graph-ToolFormer
    achieves slightly lower scores on the knowledge graph reasoning API generation.
    Partial reasons are due to the special tokens used in the knowledge graph datasets
    to represent the entity and relation IDs and names, which render the LLM in Graph-ToolFormer
    fails to reproduce the output statement for many of the instances.'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '对于知识图谱推理，我们在实验中使用了两个基准数据集，Freebase 和 WordNet，它们提供了实体及其内部关系。在表 [3](#S5.T3 "Table
    3 ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT") 中，我们提供了有关知识图谱和生成的推理提示的统计信息。对于 Freebase 知识图谱，存在
    $1,345$ 个实例。在本文研究的所有任务中，我们观察到 Graph-ToolFormer 在知识图谱推理 API 生成上取得的分数略低。部分原因是由于知识图谱数据集中用于表示实体和关系
    ID 及名称的特殊标记，这使得 Graph-ToolFormer 中的 LLM 无法为许多实例生成正确的输出语句。'
- en: 5.6\. Graph Reasoning Case Studies
  id: totrans-600
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6\. 图推理案例研究
- en: 'In Table [5](#S5.T5 "Table 5 ‣ 5.5\. Graph Reasoning Output Statement Generation
    ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT"), we also provide a list of different graph reasoning
    statements generated by Graph-ToolFormer, we report not only the correctly generated
    the output statements with API calls but also execute the APIs to obtain the correct
    reasoning results.'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [5](#S5.T5 "Table 5 ‣ 5.5\. Graph Reasoning Output Statement Generation
    ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT") 中，我们还提供了由 Graph-ToolFormer 生成的不同图推理语句的列表，我们不仅报告了正确生成的包含
    API 调用的输出语句，还执行了这些 API 以获得正确的推理结果。'
- en: 'For the graph property reasoning task, we illustrate the generated outputs
    by Graph-ToolFormer on three inputs: (1) “What is the order of the diamond graph?”,
    (2) “The path graph has a center at nodes [TBR].” and (3) “The nodes in the wheel
    graph have eccentricity values of [TBR].”, where the diamond graph, path graph
    and wheel graph mentioned in these three inputs are all the graph instances in
    the GPR dataset. These three inputs aim to reason about the order, center and
    eccentricity of the graphs, respectively. As illustrated in the table, Graph-ToolFormer
    can correctly insert the API calls (1) “[GR(GL(”gpr”, {”diamond_graph”}), ”toolx:order”)–¿r]”,
    (2) “[GR(GL(”gpr”, {”path_graph”}), ”toolx:center”)–¿r]”, and (3) “[GR(GL(”gpr”,
    {”wheel_graph”}), ”toolx:eccentricity”)–¿r]” into the output statements, which
    will call the corresponding API functions in the “toolx” toolkit. Since these
    query statement all have the output tag “–¿r”, Graph-ToolFormer will also replace
    the final reasoning result by the “toolx” toolkit into the output statements as
    well.'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图属性推理任务，我们通过 Graph-ToolFormer 在三个输入上展示生成的输出：（1）“钻石图的阶数是多少？”，（2）“路径图的中心在节点
    [TBR] 处。” 和（3）“轮图中的节点的离心率值为 [TBR]。”，其中这些输入中提到的钻石图、路径图和轮图都是 GPR 数据集中的图实例。这三个输入分别旨在推理图的阶数、中心和离心率。如表中所示，Graph-ToolFormer
    可以正确地将 API 调用（1）“[GR(GL(”gpr”， {”diamond_graph”}), ”toolx:order”)–¿r]”，（2）“[GR(GL(”gpr”，
    {”path_graph”}), ”toolx:center”)–¿r]” 和（3）“[GR(GL(”gpr”， {”wheel_graph”}), ”toolx:eccentricity”)–¿r]”
    插入到输出语句中，这些调用将调用 “toolx” 工具包中的相应 API 函数。由于这些查询语句都有输出标签 “–¿r”，Graph-ToolFormer
    也会将“toolx”工具包中的最终推理结果替换到输出语句中。
- en: 'For the bibliographic network based academic paper topic reasoning tasks, we
    illustrate three reasoning examples on the “Cora”, “Citeseer” and “Pubmed” bibliographic
    networks, respectively. For paper nodes in both “Cora” and “Citeseer”, they are
    annotated with textual labels, e.g., “Neural Networks” for paper node #83826 in
    “Cora” and “AI” for paper node #2998 in “Citeseer” as shown in the table. Meanwhile,
    for the paper nodes in “Pubmed”, we only have the integer annotated class labels,
    e.g., “1” for paper node #16466 in “Pubmed”. Similarly, we also illustrate the
    molecular graph function reasoning examples on the four molecular graph datasets,
    i.e., “PROTEINS”, “MUTAG”, “NCI1” and “PTC”, where the graph instances are also
    annotated with the integer class labels that indicating their functions as well.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于书目网络的学术论文主题推理任务，我们分别在“Cora”、“Citeseer”和“Pubmed”书目网络上展示了三个推理例子。在“Cora”和“Citeseer”中的论文节点用文本标签注释，例如，“Cora”中的论文节点#83826标注为“Neural
    Networks”，“Citeseer”中的论文节点#2998标注为“AI”，如表格所示。同时，对于“Pubmed”中的论文节点，我们只有整数标注的类别标签，例如，“Pubmed”中的论文节点#16466标注为“1”。类似地，我们还在四个分子图数据集上展示了分子图函数推理的例子，即“PROTEINS”、“MUTAG”、“NCI1”和“PTC”，这些图实例也用表示其功能的整数类别标签进行了注释。
- en: For the sequential recommender system reasoning, we illustrate the reasoning
    examples on both calculating the preference scores of users on certain items and
    the top-k recommendation for users. For both users and items in the “Last-fm”
    and “Movielens” datasets, they are denoted by the integer IDs, to differentiate
    users from items, we specifically add the “u” and “i” before their integer IDs
    in both the released graph datasets and the prompt datasets, e.g.,, “#u1527” and
    “#i5422” for the example in the table. For the social network community reasoning,
    we provide the examples on reasoning both community IDs for users and common community
    checking for input users. Finally, for the reasoning on the knowledge graphs,
    we illustrate the examples for searching both entities and relations in the table,
    which illustrate how Graph-ToolFormer works on handling such different knowledge
    graph reasoning tasks.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 对于顺序推荐系统推理，我们在计算用户对某些物品的偏好分数以及为用户提供top-k推荐的例子中进行了说明。在“Last-fm”和“Movielens”数据集中，用户和物品都用整数ID表示，为了区分用户和物品，我们在发布的图数据集和提示数据集中专门在整数ID前加上“u”和“i”，例如，表格中的例子为“#u1527”和“#i5422”。对于社交网络社区推理，我们提供了关于推理用户社区ID和输入用户的公共社区检查的例子。最后，对于知识图谱的推理，我们举例说明了如何在表格中搜索实体和关系，这些例子展示了Graph-ToolFormer如何处理不同的知识图谱推理任务。
- en: 5.7\. Inconsistent Generation Result Analysis
  id: totrans-605
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7\. 不一致生成结果分析
- en: 'Although Graph-ToolFormer is capable to generate the correct graph reasoning
    queries for most of the input query statements and questions, but Graph-ToolFormer
    may still make some mistakes or generate the outputs that are inconsistent with
    the ground-truth statements. In Table [6](#S5.T6 "Table 6 ‣ 5.5\. Graph Reasoning
    Output Statement Generation ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs
    with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), we summarize several
    types of inconsistent cases of Graph-ToolFormer in generating the graph reasoning
    queries from the prompt testing set.'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管Graph-ToolFormer能够为大多数输入查询语句和问题生成正确的图推理查询，但Graph-ToolFormer仍可能犯一些错误或生成与真实语句不一致的输出。在表格 [6](#S5.T6
    "Table 6 ‣ 5.5\. Graph Reasoning Output Statement Generation ‣ 5\. Experiments
    ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented
    by ChatGPT")中，我们总结了Graph-ToolFormer在从提示测试集中生成图推理查询时的几种不一致情况。'
- en: 'For a very small number of the inputs, Graph-ToolFormer will generate very
    messy and duplicated outputs with random API calls, like the first graph property
    reasoning example in Table [6](#S5.T6 "Table 6 ‣ 5.5\. Graph Reasoning Output
    Statement Generation ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with
    Graph Reasoning Ability via Prompt Augmented by ChatGPT"). Given the input statement
    “Nodes have the largest eccentricity [TBR] in the lollipop graph, which make them
    part of its periphery.”, the generated output by Graph-ToolFormer is very different
    from the ground-truth output. It is similar for the last recommender system reasoning
    example in Table [6](#S5.T6 "Table 6 ‣ 5.5\. Graph Reasoning Output Statement
    Generation ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT") as well, whose generation results contains
    the duplicated wrong token list of “artisit¿”.'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '对于少数输入，Graph-ToolFormer 会生成非常混乱和重复的输出，包含随机的 API 调用，例如表格 [6](#S5.T6 "Table 6
    ‣ 5.5\. Graph Reasoning Output Statement Generation ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")
    中第一个图属性推理示例。给定输入陈述“节点在棒棒糖图中具有最大的偏心距 [TBR]，这使它们成为图的一部分。”，Graph-ToolFormer 生成的输出与真实输出非常不同。表格
    [6](#S5.T6 "Table 6 ‣ 5.5\. Graph Reasoning Output Statement Generation ‣ 5\.
    Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via
    Prompt Augmented by ChatGPT") 中最后一个推荐系统推理示例也是如此，其生成结果包含重复的错误令牌列表“artisit¿”。'
- en: 'For a few inputs, the generation results by Graph-ToolFormer are very close
    to the ground-truth will has some extra tokens prepend to the output that don’t
    exist in the ground-truth statements. For instance, like second graph property
    reasoning example and the first bibliographic network reasoning example in Table [6](#S5.T6
    "Table 6 ‣ 5.5\. Graph Reasoning Output Statement Generation ‣ 5\. Experiments
    ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented
    by ChatGPT"), the generated statements by Graph-ToolFormer are prepended by the
    tokens “root¿’s” and “ROOT¿’s”, which don’t exist in the prompt datasets actually.'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '对于少数输入，Graph-ToolFormer 生成的结果非常接近真实情况，但在输出中会有一些额外的令牌，这些令牌在真实情况的陈述中并不存在。例如，如表格
    [6](#S5.T6 "Table 6 ‣ 5.5\. Graph Reasoning Output Statement Generation ‣ 5\.
    Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via
    Prompt Augmented by ChatGPT") 中的第二个图属性推理示例和第一个文献网络推理示例，Graph-ToolFormer 生成的陈述前面会有“root¿’s”和“ROOT¿’s”这样的令牌，而这些令牌在提示数据集中实际上并不存在。'
- en: 'For some of the examples in Table [6](#S5.T6 "Table 6 ‣ 5.5\. Graph Reasoning
    Output Statement Generation ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs
    with Graph Reasoning Ability via Prompt Augmented by ChatGPT"), the generated
    graph reasoning API calls are the same as the desired API calls in the ground-truth,
    but the statements are presented in another way, like the last graph property
    reasoning example, the molecular graph function reasoning example, and the first
    recommender system reasoning example.'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [6](#S5.T6 "Table 6 ‣ 5.5\. Graph Reasoning Output Statement Generation
    ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability
    via Prompt Augmented by ChatGPT") 中的一些示例中，生成的图推理 API 调用与真实情况中的期望 API 调用相同，但陈述的方式有所不同，例如最后一个图属性推理示例、分子图函数推理示例和第一个推荐系统推理示例。'
- en: In addition to the above inconsistency cases, we also observe several other
    very rare inconsistency in the generated statements, like (1) fail to generate
    the key parameters, e.g., the second bibliographic network reasoning example,
    (2) fail to add the API call query, e.g., the second recommender system reasoning
    example, (3) incomplete statements, e.g., the second social network community
    reasoning example and the last knowledge graph reasoning example, and (4) fail
    to provide the generation statements, e.g., the first knowledge graph reasoning
    example.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述不一致情况，我们还观察到生成的陈述中存在一些非常少见的不一致性，例如 (1) 未能生成关键参数，例如第二个文献网络推理示例，(2) 未能添加 API
    调用查询，例如第二个推荐系统推理示例，(3) 陈述不完整，例如第二个社交网络社区推理示例和最后一个知识图谱推理示例，以及 (4) 未能提供生成的陈述，例如第一个知识图谱推理示例。
- en: 5.8\. Zero-Shot Inference
  id: totrans-611
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.8\. 零样本推理
- en: Table 7\. Zero-shot graph reasoning API query statement generation on new graph
    datasets and with new tasks.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 7\. 在新图数据集和新任务上进行零样本图推理 API 查询语句生成。
- en: '| Categories | Failed case examples |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 失败案例示例 |'
- en: '| Input Statements | Generated Statements | Correctness Tag |'
  id: totrans-614
  prefs: []
  type: TYPE_TB
  zh: '| 输入陈述 | 生成的陈述 | 正确性标签 |'
- en: '| New Graph Data | What is the density of the Random Graph? | The density of
    the random graph is [GR(GL(”gpr”, ”random_graph”), ”toolx:density”)–¿r]. |  |'
  id: totrans-615
  prefs: []
  type: TYPE_TB
  zh: '| 新图数据 | 随机图的密度是多少？ | 随机图的密度是 [GR(GL(”gpr”, ”random_graph”), ”toolx:density”)–¿r]。
    |  |'
- en: '| What is the diameter of the online social network Twitter? | The diameter
    of the online social network Twitter is [GR(GL(”twitter”), ”seg_bert:molecule_gtopic”)].
    |  |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 在线社交网络 Twitter 的直径是多少？ | 在线社交网络 Twitter 的直径是 [GR(GL(”twitter”), ”seg_bert:molecule_gtopic”)]。
    |  |'
- en: '| What is the topic of paper #1111 in the DBLP bibliographic network? | The
    topic of paper #1111 in the DBLP bibliographic network is [GR(GL(”dblp”), ”graph_bert:topic”,
    paper#1111)–¿r]. |  |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| DBLP 文献网络中，论文 #1111 的主题是什么？ | 论文 #1111 在 DBLP 文献网络中的主题是 [GR(GL(”dblp”), ”graph_bert:topic”,
    paper#1111)–¿r]。 |  |'
- en: '| In AlphaFold, instance #30 of the protein molecular graph demonstrates a
    function of [TBR]. |  The protein molecular graph instance #30 has the function
    of [GR(GL(”alphafold”), ”graph_bert:molecule_function”, instance#30)–¿r]. |  |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| 在 AlphaFold 中，蛋白质分子图实例 #30 展示了 [TBR] 的功能。 | 蛋白质分子图实例 #30 的功能是 [GR(GL(”alphafold”),
    ”graph_bert:molecule_function”, instance#30)–¿r]。 |  |'
- en: '| What is the rating score user #Bob gives to movie #Titanic at IMDB? | The
    user #Bob gives the movie #Titanic the average rating score of [GR(GL(”imdb”),
    ”bpr:recommendation”, user#Bob, movie#Titanic)–¿r]. |  |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| 用户 #Bob 在 IMDB 上给电影 #Titanic 的评分是多少？ | 用户 #Bob 给电影 #Titanic 的平均评分是 [GR(GL(”imdb”),
    ”bpr:recommendation”, user#Bob, movie#Titanic)–¿r]。 |  |'
- en: '| In Facebook, what is the id of user #random_user’s community? | In Facebook,
    the id of user #random_user’s community is [GR(GL(”facebook”), ”kmeans:community”,
    user#random_user, ”community”)–¿r]. |  |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| 在 Facebook 上，用户 #random_user 的社区 ID 是什么？ | 在 Facebook 上，用户 #random_user 的社区
    ID 是 [GR(GL(”facebook”), ”kmeans:community”, user#random_user, ”community”)–¿r]。
    |  |'
- en: '| According to the wikipedia knowledge graph, what is the relation between
    entity #Donald_Trump and entity #United_State_of_America? | According to the wikipedia
    knowledge graph, the relation between entity #Donald_Trump and entity #United_State_of_America
    is [GR(GL(”wikis”), ”transe:relation”, entity#Donald_Trump, entity#United_State_of_America)–¿r].
    |  |'
  id: totrans-621
  prefs: []
  type: TYPE_TB
  zh: '| 根据维基百科知识图谱，实体 #Donald_Trump 和实体 #United_State_of_America 之间的关系是什么？ | 根据维基百科知识图谱，实体
    #Donald_Trump 和实体 #United_State_of_America 之间的关系是 [GR(GL(”wikis”), ”transe:relation”,
    entity#Donald_Trump, entity#United_State_of_America)–¿r]。 |  |'
- en: '| New Reasoning Tasks | What is the new_attribute of the bull graph? | The
    new_attribute of the bull graph is [GR(GL(”gpr”, ”bull_graph”), ”toolx:new_attribute”,
    ”graph”)–¿r]. |  |'
  id: totrans-622
  prefs: []
  type: TYPE_TB
  zh: '| 新推理任务 | bull 图的 new_attribute 是什么？ | bull 图的 new_attribute 是 [GR(GL(”gpr”,
    ”bull_graph”), ”toolx:new_attribute”, ”graph”)–¿r]。 |  |'
- en: '| Who are the authors of the paper #111 in the cora bibliographic network?
    | ids of the authors of the paper #111 in the cora bibliographic network include
    [GR(GL(”cora”), ”graph_bert:topic”, paper#111)–¿r]. |  |'
  id: totrans-623
  prefs: []
  type: TYPE_TB
  zh: '| cora 文献网络中论文 #111 的作者是谁？ | 论文 #111 在 cora 文献网络中的作者 IDs 包括 [GR(GL(”cora”),
    ”graph_bert:topic”, paper#111)–¿r]。 |  |'
- en: '| How many atoms are there in the protein molecular graph #111 in mutag? |
    The protein molecular graph #111 in mutag has [GR(GL(”mutag”), ”seg_bert:molecule_function”,
    instance#111)–¿r] –[GR(GL(”mutag”), ”seg_bert:molecule_function_relation”, instance#111)–¿r].
    center¿The protein molecular graph #111 in mutag has [GR(GL(”mutag”), ”seg_bert:molecule_function
    |  |'
  id: totrans-624
  prefs: []
  type: TYPE_TB
  zh: '| 在 mutag 中，蛋白质分子图 #111 包含多少个原子？ | 在 mutag 中，蛋白质分子图 #111 包含 [GR(GL(”mutag”),
    ”seg_bert:molecule_function”, instance#111)–¿r] –[GR(GL(”mutag”), ”seg_bert:molecule_function_relation”,
    instance#111)–¿r]。中心¿蛋白质分子图 #111 在 mutag 中有 [GR(GL(”mutag”), ”seg_bert:molecule_function
    |  |'
- en: '| In Amazon, what is the brand that user #Bob will be mostly likely to purchase
    next? | In Amazon, the brand that user #Bob will be mostly likely to purchase
    next is [GR(GL(”amazon”), ”bpr:topk_recommendation”, user#Bob, 1)–¿r]. |  |'
  id: totrans-625
  prefs: []
  type: TYPE_TB
  zh: '| 在 Amazon 上，用户 #Bob 最有可能下一个购买的品牌是什么？ | 在 Amazon 上，用户 #Bob 最有可能下一个购买的品牌是 [GR(GL(”amazon”),
    ”bpr:topk_recommendation”, user#Bob, 1)–¿r]。 |  |'
- en: '| In online social network Twitter, are user #Alex and user #Bob both following
    Elon Musk? | In online social network Twitter, are user #Alex and user #Bob both
    following user #ElonMusk? |  |'
  id: totrans-626
  prefs: []
  type: TYPE_TB
  zh: '| 在在线社交网络 Twitter 中，用户 #Alex 和用户 #Bob 是否都在关注 Elon Musk？ | 在在线社交网络 Twitter 中，用户
    #Alex 和用户 #Bob 是否都在关注用户 #ElonMusk？ |  |'
- en: '| According to the Freebase knowledge graph, how many relations are there between
    entity #Donald_Trump and entity #United_States_of_America? | According to the
    Freebase knowledge graph, how many relations are there between entity #Donald_Trump
    and entity [GR(GL(”freebase”), ”transe:relation”, entity#Donald_Trump, entity#United_States_of_America)–¿r].
    |  |'
  id: totrans-627
  prefs: []
  type: TYPE_TB
  zh: '| 根据 Freebase 知识图谱，实体 #Donald_Trump 和实体 #United_States_of_America 之间有多少个关系？
    | 根据 Freebase 知识图谱，实体 #Donald_Trump 和实体 [GR(GL(”freebase”), ”transe:relation”,
    entity#Donald_Trump, entity#United_States_of_America)–¿r] 之间有多少个关系。 |  |'
- en: 'What’s more, we also analyze the zero-shot inference ability of Graph-ToolFormer
    in generating the API calls for graph reasoning tasks on either new graph datasets
    or for new reasoning tasks. The case studies of some examples are provided in
    Table [7](#S5.T7 "Table 7 ‣ 5.8\. Zero-Shot Inference ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT").'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们还分析了 Graph-ToolFormer 在生成图推理任务的 API 调用时的零样本推理能力，无论是针对新的图数据集还是新的推理任务。表格[7](#S5.T7
    "Table 7 ‣ 5.8\. Zero-Shot Inference ‣ 5\. Experiments ‣ Graph-ToolFormer: To
    Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")中提供了一些案例研究。'
- en: 'According to the case studies shown in Table [7](#S5.T7 "Table 7 ‣ 5.8\. Zero-Shot
    Inference ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph Reasoning
    Ability via Prompt Augmented by ChatGPT"), we observe that among the seven provided
    reasoning tasks on new graph datasets, six of them can generate the desired statements
    with the correct API calls. The only wrong generated statement is for the graph
    property reasoning statement “What is the diameter of the online social network
    Twitter?”, which generates a statement unexpected API calls “The diameter of the
    online social network Twitter is [GR(GL(”twitter”), ”seg_bert:molecule_gtopic”)].”'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表格[7](#S5.T7 "Table 7 ‣ 5.8\. Zero-Shot Inference ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT")中展示的案例研究，我们观察到，在提供的七个新的图数据集推理任务中，有六个能够生成正确的
    API 调用所需的语句。唯一错误的生成语句是针对图属性推理的语句“What is the diameter of the online social network
    Twitter?”，它生成了一个意外的 API 调用语句“The diameter of the online social network Twitter
    is [GR(GL(”twitter”), ”seg_bert:molecule_gtopic”)].”'
- en: Meanwhile, among the cases on new reasoning tasks, only one of these six provided
    examples can generate the desired outputs, which is for calculating some “new_attribute”
    of the bull graph. However, for the other new tasks that are not included in the
    prompt training set, e.g., the “paper author reasoning”, “atom node number reasoning
    of molecular graphs”, “brand reasoning in recommender systems”, “common followee
    reasoning of users in social networks” and “relation count reasoning in knowledge
    graphs”, the generated output statements by Graph-ToolFormer are all wrong.
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，在新的推理任务案例中，这六个提供的示例中只有一个能够生成所需的输出，即计算公牛图的某个“new_attribute”。然而，对于其他不包括在提示训练集中的新任务，例如“论文作者推理”、“分子图的原子节点数推理”、“推荐系统中的品牌推理”、“社交网络中用户的共同关注者推理”和“知识图谱中的关系计数推理”，Graph-ToolFormer
    生成的输出语句都是错误的。
- en: 5.9\. Language Generation Ability Revisit
  id: totrans-631
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.9\. 语言生成能力回顾
- en: Table 8\. Analysis of generated API query cases different from ground-truth
    queries in the testing set.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8\. 测试集中生成的 API 查询案例与真实查询的差异分析。
- en: '| Input Sources | Language Generation Examples |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| 输入来源 | 语言生成示例 |'
- en: '| Input Payload | Generated Outputs Before Fine-Tuning | Generated Outputs
    After Fine-Tuning |'
  id: totrans-634
  prefs: []
  type: TYPE_TB
  zh: '| 输入载荷 | 微调前生成的输出 | 微调后生成的输出 |'
- en: '| Pile Data | Roman Catholic Diocese of Tambacounda$\backslash$n | center¿The
    Roman Catholic Diocese of Tambacounda () is a diocese located in the city of Tambacounda
    in the ¡center¿Ecclesiastical province of Dakar in Senegal.¡/center¿ center¿Output:
    ¡/center¿ center¿The ¡center¿Roman¡/center¿ ¡center¿Catholic |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '| 数据表 | 坦巴坎达罗马公教教区$\backslash$n | center¿坦巴坎达罗马公教教区 () 是一个位于塞内加尔达喀尔教省中心城市坦巴坎达的教区。¡/center¿
    center¿输出: ¡/center¿ center¿该 ¡center¿罗马¡/center¿ ¡center¿公教 |'
- en: '| Mud Hens pitcher Evan Reed charged with sexual assault$\backslash$nMud Hens
    pitcher Evan Reed was charged July 30 with sexual assault related to | center¿###¡centercentercentercentercentercentercenter
    centercentercentercentercentercentercentercenter centercentercentercentercentercentercentercenter
    centercentercentercentercentercentercentercenter centercentercentercentercenter
    |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| Mud Hens 投手 Evan Reed 被控性侵犯$\backslash$nMud Hens 投手 Evan Reed 于 7 月 30 日因性侵犯被控
    | center¿###¡centercentercentercentercentercentercenter centercentercentercentercentercentercentercenter
    centercentercentercentercentercentercentercenter centercentercentercentercenter
    |'
- en: '| Random Sources | Thursday’s verdict only added to the intrigue surrounding
    the gravest legal and political unknown from the 2021 Capitol insurrection that
    hangs over the 2024 campaign: will Donald Trump, the president who inspired the
    uprising, face his own legal and political price? | I don’t think there’s any
    question that he’s going to be impeached,” Rep. Al Green (D-TX) told The Daily
    Beast. “I think he’s going to be impeached, and I think he’s going to be removed
    from office.”$\backslash$nGreen’s | will be the verdict only added to the intrigue
    surrounding the gravest legal and political unknown from the 2021 Capitol insurrection
    that hangs over the 2024 campaign: will Donald Trump, the president who inspired
    the uprising, face his own legal and political price?br¿ |'
  id: totrans-637
  prefs: []
  type: TYPE_TB
  zh: '| 随机来源 | 周四的裁决只增添了围绕2021年国会骚乱所带来的最严重法律和政治未知的悬念，这个悬念影响着2024年竞选：激起骚乱的总统特朗普是否会面临法律和政治的代价？
    | “我认为毫无疑问他会被弹劾，”德州民主党代表艾尔·格林告诉《每日野兽》。 “我认为他会被弹劾，并且我认为他会被免职。”$\backslash$nGreen’s
    | 裁决只增添了围绕2021年国会骚乱所带来的最严重法律和政治未知的悬念，这个悬念影响着2024年竞选：激起骚乱的总统特朗普是否会面临法律和政治的代价？br¿
    |'
- en: '| A source of debate for centuries, the village of Ponte Buriano, a suburb
    of Arezzo in the Tuscany region of Italy, is so convinced the bridge behind Mona
    Lisa is the Ponte Buriano that they’ve made it a key feature of their local tourism
    campaign, even laying claim to the bridge on the village’s welcome sign. | $\backslash$nThe
    | The village of [GR(GL(”mona_lisa”), ”bpr:search”, GR(GL(”ponte_buriano”), GR(GL(”search”),
    GR(GL(”mona_lisa”), GR(GL(”search |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| 经过几个世纪争论的源头，意大利托斯卡纳地区的亚雷佐市郊村庄Ponte Buriano，非常确信《蒙娜丽莎》背后的桥就是Ponte Buriano，因此将其作为当地旅游宣传的重点，甚至在村庄的欢迎标志上也声称拥有这座桥。
    | $\backslash$nThe | 这个村庄的[GR(GL(”mona_lisa”), ”bpr:search”, GR(GL(”ponte_buriano”),
    GR(GL(”search”), GR(GL(”mona_lisa”), GR(GL(”search |'
- en: 'At the end of this section, we also want to provide more analyses about the
    impacts of the fine-tuning on LLM’s language generation abilities. In Table [8](#S5.T8
    "Table 8 ‣ 5.9\. Language Generation Ability Revisit ‣ 5\. Experiments ‣ Graph-ToolFormer:
    To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT"),
    we illustrate some examples about the generation results by the LLM in Graph-ToolFormer
    before and after the fine-tuning with the graph reasoning prompt data. Specifically,
    we select the inputs from two different sources, i.e., two instances from the
    Pile testing set (Pile was the data used for GPT-J pre-training) and two instances
    from the recent news articles on the web.'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节末尾，我们还想提供更多关于微调对LLM语言生成能力影响的分析。在表[8](#S5.T8 "Table 8 ‣ 5.9\. Language Generation
    Ability Revisit ‣ 5\. Experiments ‣ Graph-ToolFormer: To Empower LLMs with Graph
    Reasoning Ability via Prompt Augmented by ChatGPT")中，我们展示了一些关于Graph-ToolFormer中LLM在微调前后生成结果的示例，使用的是图推理提示数据。具体而言，我们从两个不同来源选择了输入数据，即来自Pile测试集的两个实例（Pile是用于GPT-J预训练的数据）和来自近期网络新闻文章的两个实例。'
- en: By comparing the generation results, we can observe very large (negative) impacts
    of the fine-tuning with the prompt datasets on LLM’s language generation ability.
    For these four input payloads, after the fine-tuning, the outputs generated by
    the LLM in Graph-ToolFormer are either some random tokens or contains the unexpected
    API calls, and only for the third input, the output by the LLM in Graph-ToolFormer
    is till closely related to the inputs.
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较生成结果，我们可以观察到微调提示数据对LLM语言生成能力有非常大的（负面）影响。对于这四个输入负载，微调后，LLM在Graph-ToolFormer中生成的输出要么是一些随机的标记，要么包含了意外的API调用，只有第三个输入的输出仍然与输入紧密相关。
- en: Therefore, if we plan to make Graph-ToolFormer a very general language interface
    that can not only handle graph reasoning tasks but also still possess the language
    generation ability for the inputs not related to graph reasoning, some new continual
    learning techniques will be needed in model fine-tuning, so the LLM in Graph-ToolFormer
    will not suffer from the catastrophic forgetting problem after fine-tuning.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们计划使Graph-ToolFormer成为一个非常通用的语言接口，不仅可以处理图推理任务，还能保持对与图推理无关的输入的语言生成能力，那么在模型微调中将需要一些新的持续学习技术，以避免Graph-ToolFormer在微调后遭遇灾难性遗忘问题。
- en: 6\. Conclusion
  id: totrans-642
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: In this paper, we investigate the principles, methodologies and algorithms to
    empower existing LLMs with graph reasoning ability. We introduce the Graph-ToolFormer
    framework, and propose to teach LLMs to use external graph data loading and graph
    reasoning tools for addressing the tasks. Specifically, several representative
    graph reasoning tasks are studied in this paper, including the basic graph property
    reasoning task, and the advanced tasks, like bibliographic paper topic reasoning,
    molecular graph function reasoning, sequential recommender system reasoning, social
    network community reasoning and the knowledge graph reasoning. For each of these
    graph reasoning tasks, we select several benchmark graph datasets in the experiment,
    and design the reasoning API call prompt examples manually. In addition, with
    the help of ChatGPT, we can further augment the reasoning API call templates to
    generate large-sized graph reasoning datasets. Via fine-tuning of existing pre-trained
    LLMs with the generated prompt dataset, the Graph-ToolFormer framework has been
    demonstrated to be effective with the extensive experimental results on many of
    these studied graph reasoning tasks.
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们探讨了赋能现有LLM（大语言模型）图谱推理能力的原则、方法论和算法。我们介绍了Graph-ToolFormer框架，并提出了教导LLM使用外部图数据加载和图推理工具来处理任务的方法。具体来说，本文研究了几个代表性的图推理任务，包括基础图属性推理任务，以及高级任务，如文献主题推理、分子图功能推理、序列推荐系统推理、社交网络社区推理和知识图谱推理。针对这些图推理任务，我们在实验中选择了几个基准图数据集，并手动设计了推理API调用示例。此外，在ChatGPT的帮助下，我们可以进一步扩展推理API调用模板，以生成大规模的图推理数据集。通过对生成的提示数据集进行现有预训练LLM的微调，Graph-ToolFormer框架在许多研究的图推理任务上的广泛实验结果表明其有效性。
- en: 7\. Future Research Directions
  id: totrans-644
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 未来研究方向
- en: The explorations of this paper try to help identify and develop one potential
    framework Graph-ToolFormer that can help bridge the graph learning community with
    the latest research work on LLMs and AIGC. The experimental results reported in
    this paper also demonstrate that it is feasible to use LLMs as a unified and general
    interface to conduct various graph reasoning tasks. Based on this paper, researchers
    in the graph learning communities may consider to further propose new models and
    frameworks that integrate graph reasoning abilities to the latest LLMs. Along
    with the explorations in this paper, we also identify several potential research
    opportunities with the current framework and the readers may consider to further
    explore in the future, which are listed as follows.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的探索试图帮助识别和开发一个潜在的框架Graph-ToolFormer，以帮助图学习社区与最新的LLM和AIGC研究工作接轨。本文报告的实验结果也表明，使用LLM作为统一和通用接口来进行各种图推理任务是可行的。基于本文，图学习社区的研究人员可能会考虑进一步提出将图推理能力集成到最新LLM中的新模型和框架。结合本文的探索，我们还识别了当前框架的若干潜在研究机会，读者可以考虑在未来进一步探索，这些机会列举如下。
- en: •
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'GNNs Transfer: Graph neural networks (GNNs) pre-training has been studied for
    years. Different from the pre-trained language and vision models, pre-trained
    GNNs has very limited applications in the real-world actually, since GNNs are
    harder to be transferred to new graph reasoning tasks on new graph datasets. Currently,
    pre-trained GNN models are normally dedicatedly used in the same (or similar)
    task(s) that pre-train them. In this paper, Graph-ToolFormer can serve as a hosting
    platform for deploying various pre-trained GNN models, and we expect to see more
    pre-trained GNN models to be added to the Graph-ToolFormer framework in the future.
    At the same time, we also identify a potential problem with the current graph
    models. As more and more graph data and graph reasoning tasks are added to Graph-ToolFormer,
    the number of required pre-trained GNN models in Graph-ToolFormer will grow quadratically,
    since a new pre-trained GNN will be needed for a specific graph reasoning task
    on a certain graph datasets. So, the number of required graph models in Graph-ToolFormer
    is approximately equal to “$|\textit{graph datasets}|\times|\textit{graph reasoning
    tasks}|$”. Improving the transferability of pre-trained GNN models both across
    graph reasoning tasks and different graph datasets should be one of major the
    research exploration focus of the graph learning community for the future.'
  id: totrans-647
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GNNs迁移：图神经网络（GNNs）的预训练已经研究了多年。不同于预训练的语言和视觉模型，预训练的GNNs在实际应用中的应用非常有限，因为GNNs更难迁移到新的图推理任务和新的图数据集上。目前，预训练的GNN模型通常专用于与预训练任务相同（或相似）的任务。在本文中，Graph-ToolFormer可以作为一个托管平台，用于部署各种预训练的GNN模型，我们期望未来会有更多预训练的GNN模型被添加到Graph-ToolFormer框架中。同时，我们也识别出了当前图模型的一个潜在问题。随着越来越多的图数据和图推理任务被添加到Graph-ToolFormer中，Graph-ToolFormer中所需的预训练GNN模型的数量将呈二次增长，因为每个特定的图推理任务需要一个新的预训练GNN。因此，Graph-ToolFormer中所需的图模型数量大约等于“$|\textit{graph
    datasets}|\times|\textit{graph reasoning tasks}|$”。提高预训练GNN模型在图推理任务和不同图数据集之间的迁移能力应该是未来图学习社区主要的研究探索重点之一。
- en: •
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Integrated Learning of LLMs and GNNs: In Graph-ToolFormer, the pre-training
    of GNNs and LLMs are separated from each other. The LLMs in Graph-ToolFormer are
    used as more like a reasoning language interface, that will call the pre-trained
    GNN models for accomplishing certain tasks. In other words, the LLMs in Graph-ToolFormer
    has no access to the internal components of the GNNs, and will not use any hidden
    representations learned for the input graphs/nodes/links in the text output generation.
    For the graph reasoning result oriented tasks, the current framework Graph-ToolFormer
    works very well. Meanwhile, when it comes to some tasks that may require the LLMs
    to access the graph reasoning process and internal (intermediate) embedding representations,
    the current framework will become insufficient. For instance, the following graph
    reasoning interpretability studies will require the LLMs provide a detailed textual
    explanation of not only the graph reasoning results but also the reasoning process.
    Without the access the GNNs’ internal components and representations, it will
    be hard or infeasible for LLMs to generate such textual explanations.'
  id: totrans-649
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMs和GNNs的集成学习：在Graph-ToolFormer中，GNNs和LLMs的预训练是相互分开的。Graph-ToolFormer中的LLMs更像是一个推理语言接口，用于调用预训练的GNN模型来完成特定任务。换句话说，Graph-ToolFormer中的LLMs无法访问GNNs的内部组件，也不会使用任何为输入图/节点/链接学习到的隐藏表示来生成文本输出。对于以图推理结果为导向的任务，目前的Graph-ToolFormer框架表现得非常好。与此同时，当某些任务可能需要LLMs访问图推理过程和内部（中间）嵌入表示时，当前的框架将变得不够充分。例如，以下图推理可解释性研究将要求LLMs提供关于图推理结果以及推理过程的详细文本解释。在无法访问GNNs的内部组件和表示的情况下，LLMs生成这样的文本解释将会很困难或不可行。
- en: •
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Regulation and Interpretability: Nowadays, lots of people have been calling
    for setting up stricter regulation policies and laws on the AI systems, models,
    products and services. Impressed by the current AI products and services, human
    have presented very complicated reactions to the fast growing new AI models: we
    want to enjoy the services from AI systems but also expect that the systems are
    safe, reliable and robust. To bridge people’s expectations and the current AI
    models, the model performance and result interpretability plays a critical role.
    As mentioned above, the current LLMs in Graph-ToolFormer can automatically call
    the graph reasoning API queries to get the results and replace the results with
    the generated queries as the final output. Meanwhile, as to the the reasoning
    process and reasoning result interpretability, the current Graph-ToolFormer cannot
    provide the textual explanations, which will be one of the most important research
    focus for the future projects.'
  id: totrans-651
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 规制与可解释性：如今，许多人呼吁对 AI 系统、模型、产品和服务制定更严格的规制政策和法律。人们对当前 AI 产品和服务印象深刻，对快速发展的新 AI
    模型表现出复杂的反应：我们希望享受 AI 系统提供的服务，但也期望这些系统安全、可靠和稳健。为了弥合人们的期望与当前 AI 模型之间的差距，模型性能和结果的可解释性扮演着至关重要的角色。如上所述，Graph-ToolFormer
    中的当前 LLMs 可以自动调用图推理 API 查询以获取结果，并用生成的查询替换结果作为最终输出。同时，至于推理过程和推理结果的可解释性，目前的 Graph-ToolFormer
    无法提供文本解释，这将是未来项目中最重要的研究重点之一。
- en: •
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Efficiency: With LoRA and quantized models/optimizers, we can reduce the model
    fine-tuning memory capacity requirement to less than 11GB and the memory capacity
    requirement even lower for the model inference stage. Meanwhile, integrated with
    the large-sized graph data, pre-trained graph models, and necessary pre-processed
    data, the efficiency of Graph-ToolFormer for various graph reasoning task can
    still be a problem. In this paper, we introduce a tentative approach to make the
    problem less severe with the working memory. However, if we plan to deploy Graph-ToolFormer
    on devices with very small memories, like cell-phones or embedded equipments,
    new techniques will still be needed to improve the model learning and inference
    efficiency.'
  id: totrans-653
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 效率：通过使用 LoRA 和量化模型/优化器，我们可以将模型微调的内存容量需求降低到 11GB 以下，并且模型推理阶段的内存容量需求更低。同时，与大规模图数据、预训练图模型和必要的预处理数据集成时，Graph-ToolFormer
    在各种图推理任务中的效率仍然可能成为一个问题。在本文中，我们引入了一种初步的方法，以利用工作内存来减轻这一问题的严重性。然而，如果我们计划在内存非常小的设备上部署
    Graph-ToolFormer，如手机或嵌入式设备，则仍需要新技术来提高模型学习和推理效率。
- en: •
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Diverse Applications: Due to the limited space, we can only study a few number
    of the graph reasoning tasks with Graph-ToolFormer in this paper. Meanwhile, in
    the real-world, we have lots of graph structured data that may require the LLMs
    to handle them to reason for the desired outputs. Therefore, a very promising
    future work direction is to apply Graph-ToolFormer to study diverse real-world
    graph/network data oriented reasoning tasks with LLMs. We list a few of them here
    just for the readers’ information, and the readers may explore more diverse reasoning
    tasks according to your own backgrounds and expertises.'
  id: totrans-655
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多样化应用：由于空间有限，我们在本文中只能研究少数几个图推理任务的 Graph-ToolFormer。同时，在现实世界中，我们有大量图结构数据，可能需要
    LLMs 处理它们以推理出所需的结果。因此，一个非常有前景的未来工作方向是将 Graph-ToolFormer 应用于研究多样化的现实世界图/网络数据导向的推理任务。我们在此列出了一些，仅供读者参考，读者可以根据自己的背景和专业知识探索更多多样化的推理任务。
- en: –
  id: totrans-656
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Urban Computing and Smart City: In the offline world, we have extensively connected
    traffic networks that bridge different local communities, cities and countries
    by local roads, national highways, international fights and ocean freight corridors.
    Applying LLMs for knowledge extraction and reasoning based on such traffic networks
    is critical for the current urban computing and smart city projects.'
  id: totrans-657
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 城市计算与智慧城市：在离线世界中，我们拥有广泛连接的交通网络，通过地方道路、国家高速公路、国际航班和海洋货运走廊连接不同的地方社区、城市和国家。基于这些交通网络应用
    LLMs 进行知识提取和推理，对于当前的城市计算和智慧城市项目至关重要。
- en: –
  id: totrans-658
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'IoT and Smart Home: Assisted with the 5G, the IoT network effectively bridges
    the cyber world with the physical devices and equipments together via extremely
    fast communication channels. The LLMs provide the opportunity for us to utilize
    language models as the general interface for controlling the devices within the
    IoT networks, which is also the main objective for building the smart home system.'
  id: totrans-659
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物联网与智能家居：在5G的帮助下，物联网网络通过极其快速的通信通道有效地将网络世界与物理设备和装备连接起来。LLMs 提供了利用语言模型作为控制物联网网络内设备的一般接口的机会，这也是构建智能家居系统的主要目标。
- en: –
  id: totrans-660
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Healthcare: During the past years, the world has suffered a lot from the covid-19
    pandemic. Similar to the protein molecules studied in this paper, both the virus
    and the vaccines can also be represented as the molecular graphs. LLMs with the
    molecular graph reasoning ability have the potential to improve our current healthcare
    system in many perspectives, like early identification of virus, analysis of the
    virus pathogenicity and creation of vaccines. What’s more, the LLMs with the social
    network reasoning ability will also help infer the potential virus propagation
    among people, early prediction of highly infectious communities and identify rumors
    and misinformation about the pandemic (at the online social networks).'
  id: totrans-661
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 医疗保健：在过去几年里，世界因新冠疫情遭受了很大的痛苦。类似于本文研究的蛋白质分子，病毒和疫苗也可以表示为分子图。具备分子图推理能力的LLMs有潜力从多个方面改善我们当前的医疗保健系统，例如早期识别病毒、分析病毒致病性和创建疫苗。此外，具备社交网络推理能力的LLMs还将有助于推测潜在的病毒传播、人群中高度传染的社区的早期预测，以及识别关于疫情的谣言和错误信息（在在线社交网络上）。
- en: –
  id: totrans-662
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Multi-Modal Learning: Extending the LLMs to handle multi-modal inputs is the
    main exploration focus at present for both AIGC and AGI research. Graphs actually
    can serve as the modeling data representation for bridging the data in different
    modalities, e.g., knowledge graph from texts and scene graph from images. Exploring
    to integrate all such multi-modal data based learning systems within one unified
    framework via graph equipped LLMs can also be a promising research direction.'
  id: totrans-663
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多模态学习：扩展LLMs以处理多模态输入是当前AIGC和AGI研究的主要探索方向。图实际上可以作为不同模态数据之间桥接的数据表示，例如来自文本的知识图谱和来自图像的场景图。探索将所有这些多模态数据基础学习系统集成到一个统一框架中的图配备LLMs也可能是一个有前途的研究方向。
- en: References
  id: totrans-664
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan
    Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,
    and Pascale Fung. A multitask, multilingual, multimodal evaluation of chatgpt
    on reasoning, hallucination, and interactivity. ArXiv, abs/2302.04023, 2023.
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan
    Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, Quyet V. Do, Yan Xu,
    和 Pascale Fung. 对ChatGPT在推理、幻觉和互动方面的多任务、多语言、多模态评估。ArXiv，abs/2302.04023，2023年。
- en: (2) Hannah Bast, Florian Bäurle, Björn Buchhold, and Elmar Haußmann. Easy access
    to the freebase dataset. In Proceedings of the 23rd International Conference on
    World Wide Web, WWW ’14 Companion, page 95–98, New York, NY, USA, 2014. Association
    for Computing Machinery.
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Hannah Bast, Florian Bäurle, Björn Buchhold, 和 Elmar Haußmann. 轻松访问freebase数据集。见第23届国际万维网会议论文集，WWW
    ’14 Companion，第95–98页，美国纽约，2014年。计算机协会。
- en: (3) Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere.
    The million song dataset. In Proceedings of the 12th International Conference
    on Music Information Retrieval (ISMIR 2011), 2011.
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, 和 Paul Lamere.
    *百万歌曲数据集*。见第12届国际音乐信息检索会议（ISMIR 2011）论文集，2011年。
- en: (4) Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and
    Oksana Yakhnenko. Translating embeddings for modeling multi-relational data. In
    C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K.Q. Weinberger, editors,
    Advances in Neural Information Processing Systems, volume 26\. Curran Associates,
    Inc., 2013.
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, 和 Oksana
    Yakhnenko. 通过嵌入翻译建模多关系数据。见 C.J. Burges, L. Bottou, M. Welling, Z. Ghahramani,
    和 K.Q. Weinberger 编，*《神经信息处理系统进展》*，第26卷。Curran Associates, Inc.，2013年。
- en: (5) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon
    Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon
    Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei. 语言模型是少样本学习者. ArXiv, abs/2005.14165, 2020.
- en: '(6) Antonia Creswell, Murray Shanahan, and Irina Higgins. Selection-inference:
    Exploiting large language models for interpretable logical reasoning. ArXiv, abs/2205.09712,
    2022.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Antonia Creswell, Murray Shanahan, 和 Irina Higgins. 选择推理：利用大型语言模型进行可解释的逻辑推理.
    ArXiv, abs/2205.09712, 2022.
- en: (7) Tim Dettmers, Mike Lewis, Sam Shleifer, and Luke Zettlemoyer. 8-bit optimizers
    via block-wise quantization. ArXiv, abs/2110.02861, 2021.
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Tim Dettmers, Mike Lewis, Sam Shleifer, 和 Luke Zettlemoyer. 通过块级量化实现 8 位优化器.
    ArXiv, abs/2110.02861, 2021.
- en: '(8) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. ArXiv,
    abs/1810.04805, 2019.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova. Bert：深度双向变换器的预训练用于语言理解.
    ArXiv, abs/1810.04805, 2019.
- en: (9) Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick,
    Jacob Eisenstein, and William W. Cohen. Time-Aware Language Models as Temporal
    Knowledge Bases. Transactions of the Association for Computational Linguistics,
    10:257–273, 03 2022.
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Bhuwan Dhingra, Jeremy R. Cole, Julian Martin Eisenschlos, Daniel Gillick,
    Jacob Eisenstein, 和 William W. Cohen. 时间感知语言模型作为时间知识库. 计算语言学协会会刊, 10:257–273,
    2022年3月.
- en: '(10) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:
    Transformers for image recognition at scale. ArXiv, abs/2010.11929, 2020.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, Jakob Uszkoreit, 和 Neil Houlsby. 一张图片胜过 16x16 个词：大规模图像识别的变换器. ArXiv,
    abs/2010.11929, 2020.
- en: (11) Hugging Face. hivemind/gpt-j-6b-8bit. [https://huggingface.co/hivemind/gpt-j-6B-8bit](https://huggingface.co/hivemind/gpt-j-6B-8bit),
    2022. [Online; accessed 19-March-2023].
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Hugging Face. hivemind/gpt-j-6b-8bit. [https://huggingface.co/hivemind/gpt-j-6B-8bit](https://huggingface.co/hivemind/gpt-j-6B-8bit),
    2022年. [在线; 访问于2023年3月19日].
- en: '(12) Nezihe Merve Gürel, Hansheng Ren, Yujing Wang, Hui Xue, Yaming Yang, and
    Ce Zhang. An anatomy of graph neural networks going deep via the lens of mutual
    information: Exponential decay vs. full preservation. ArXiv, abs/1910.04499, 2019.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Nezihe Merve Gürel, Hansheng Ren, Yujing Wang, Hui Xue, Yaming Yang, 和
    Ce Zhang. 通过互信息视角深入了解图神经网络的解剖：指数衰减与完全保留. ArXiv, abs/1910.04499, 2019.
- en: (13) David K. Hammond, Pierre Vandergheynst, and Remi Gribonval. Wavelets on
    graphs via spectral graph theory. Applied and Computational Harmonic Analysis,
    30(2):129?150, Mar 2011.
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) David K. Hammond, Pierre Vandergheynst, 和 Remi Gribonval. 通过谱图理论在图上进行小波变换.
    应用与计算和声分析, 30(2):129-150, 2011年3月.
- en: '(14) F. Maxwell Harper and Joseph A. Konstan. The movielens datasets: History
    and context. ACM Trans. Interact. Intell. Syst., 5(4), dec 2015.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (14) F. Maxwell Harper 和 Joseph A. Konstan. Movielens 数据集：历史与背景. ACM Trans.
    Interact. Intell. Syst., 5(4), 2015年12月.
- en: '(15) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.
    ArXiv, abs/2106.09685, 2021.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, 和 Weizhu Chen. Lora：大型语言模型的低秩适应. ArXiv, abs/2106.09685, 2021.
- en: '(16) Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, and Yizhou Sun. Gpt-gnn:
    Generative pre-training of graph neural networks. Proceedings of the 26th ACM
    SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Ziniu Hu, Yuxiao Dong, Kuansan Wang, Kai-Wei Chang, 和 Yizhou Sun. Gpt-gnn：图神经网络的生成预训练.
    第26届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集, 2020.
- en: (17) Binxuan Huang and Kathleen M. Carley. Inductive graph representation learning
    with recurrent graph neural networks. CoRR, abs/1904.08035, 2019.
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Binxuan Huang 和 Kathleen M. Carley. 递归图神经网络的归纳图表示学习. CoRR, abs/1904.08035,
    2019.
- en: '(18) Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen, and Philip S.
    Yu. A survey on knowledge graphs: Representation, acquisition, and applications.
    IEEE Transactions on Neural Networks and Learning Systems, 33(2):494–514, 2022.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) Shaoxiong Ji, Shirui Pan, Erik Cambria, Pekka Marttinen 和 Philip S. Yu。知识图谱综述：表示、获取和应用。《IEEE神经网络与学习系统汇刊》，33(2)：494–514，2022年。
- en: (19) Thomas N. Kipf and Max Welling. Semi-supervised classification with graph
    convolutional networks. CoRR, abs/1609.02907, 2016.
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) Thomas N. Kipf 和 Max Welling。基于图卷积网络的半监督分类。CoRR，abs/1609.02907，2016年。
- en: '(20) Johannes Klicpera, Aleksandar Bojchevski, and Stephan Günnemann. Personalized
    embedding propagation: Combining neural networks on graphs with personalized pagerank.
    CoRR, abs/1810.05997, 2018.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) Johannes Klicpera, Aleksandar Bojchevski 和 Stephan Günnemann。个性化嵌入传播：将图上的神经网络与个性化PageRank相结合。CoRR，abs/1810.05997，2018年。
- en: (21) Xiangnan Kong, Jiawei Zhang, and Philip S. Yu. Inferring anchor links across
    multiple heterogeneous social networks. In Proceedings of the 22nd ACM International
    Conference on Information and Knowledge Management, CIKM ’13, page 179–188, New
    York, NY, USA, 2013\. Association for Computing Machinery.
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) Xiangnan Kong, Jiawei Zhang 和 Philip S. Yu。推断跨多个异质社交网络的锚点链接。在第22届ACM国际信息与知识管理会议论文集中，CIKM
    ’13，第179–188页，美国纽约，2013年。计算机协会。
- en: '(22) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
    pre-training for natural language generation, translation, and comprehension.
    In Annual Meeting of the Association for Computational Linguistics, 2019.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (22) Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman
    Mohamed, Omer Levy, Veselin Stoyanov 和 Luke Zettlemoyer。Bart：用于自然语言生成、翻译和理解的去噪序列到序列预训练。在计算语言学协会年会上，2019年。
- en: (23) Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional
    networks for semi-supervised learning. CoRR, abs/1801.07606, 2018.
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) Qimai Li, Zhichao Han 和 Xiao-Ming Wu。图卷积网络在半监督学习中的深入见解。CoRR，abs/1801.07606，2018年。
- en: '(24) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts
    for generation. Proceedings of the 59th Annual Meeting of the Association for
    Computational Linguistics and the 11th International Joint Conference on Natural
    Language Processing (Volume 1: Long Papers), abs/2101.00190, 2021.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (24) Xiang Lisa Li 和 Percy Liang。前缀调优：优化连续提示以进行生成。第59届计算语言学协会年会及第11届国际联合自然语言处理会议（第1卷：长篇论文）论文集，abs/2101.00190，2021年。
- en: (25) Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep
    neural networks for natural language understanding. In Annual Meeting of the Association
    for Computational Linguistics, 2019.
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) Xiaodong Liu, Pengcheng He, Weizhu Chen 和 Jianfeng Gao。用于自然语言理解的多任务深度神经网络。在计算语言学协会年会上，2019年。
- en: (26) J. B. MacQueen. Some methods for classification and analysis of multivariate
    observations. In L. M. Le Cam and J. Neyman, editors, Proc. of the fifth Berkeley
    Symposium on Mathematical Statistics and Probability, volume 1, pages 281–297\.
    University of California Press, 1967.
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) J. B. MacQueen。分类和多变量观察分析的一些方法。在 L. M. Le Cam 和 J. Neyman 编辑的《第五届伯克利数学统计与概率研讨会》论文集中，第1卷，第281–297页。加州大学出版社，1967年。
- en: (27) Julian McAuley, Christopher Targett, Javen Qinfeng Shi, and Anton van den
    Hengel. Image-based recommendations on styles and substitutes. Proceedings of
    the 38th International ACM SIGIR Conference on Research and Development in Information
    Retrieval, 2015.
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) Julian McAuley, Christopher Targett, Javen Qinfeng Shi 和 Anton van den
    Hengel。基于图像的风格和替代品推荐。在第38届国际ACM SIGIR信息检索研究与开发会议论文集中，2015年。
- en: (28) Prem Melville and Vikas Sindhwani. Recommender systems. IBM T.J. Watson
    Research Center, 2010.
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) Prem Melville 和 Vikas Sindhwani。推荐系统。IBM T.J. Watson 研究中心，2010年。
- en: '(29) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis,
    Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
    Asli Celikyilmaz, Edouard Grave, Yann LeCun, and Thomas Scialom. Augmented language
    models: a survey. ArXiv, abs/2302.07842, 2023.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (29) Grégoire Mialon, Roberto Dessì, Maria Lomeli, Christoforos Nalmpantis,
    Ramakanth Pasunuru, Roberta Raileanu, Baptiste Rozière, Timo Schick, Jane Dwivedi-Yu,
    Asli Celikyilmaz, Edouard Grave, Yann LeCun 和 Thomas Scialom。增强语言模型：一项综述。ArXiv，abs/2302.07842，2023年。
- en: '(30) George A. Miller. Wordnet: A lexical database for english. Commun. ACM,
    38(11):39–41, nov 1995.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) George A. Miller。Wordnet：一个英语词汇数据库。《ACM通讯》，38(11)：39–41，1995年11月。
- en: (31) Alan Mislove, Massimiliano Marcon, Krishna P. Gummadi, Peter Druschel,
    and Bobby Bhattacharjee. Measurement and analysis of online social networks. In
    Proceedings of the 7th ACM SIGCOMM Conference on Internet Measurement, IMC ’07,
    page 29–42, New York, NY, USA, 2007\. Association for Computing Machinery.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (31) Alan Mislove、Massimiliano Marcon、Krishna P. Gummadi、Peter Druschel 和 Bobby
    Bhattacharjee. 在线社交网络的测量与分析。发表于第七届 ACM SIGCOMM 互联网测量会议论文集，IMC ’07，第 29–42 页，美国纽约，2007。计算机协会。
- en: (32) OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) OpenAI. GPT-4 技术报告。ArXiv，abs/2303.08774，2023。
- en: (33) OpenAI. Planning for agi and beyond. [https://openai.com/blog/planning-for-agi-and-beyond](https://openai.com/blog/planning-for-agi-and-beyond),
    2023. [Online; accessed 27-March-2023].
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (33) OpenAI. 为 AGI 及未来做规划。 [https://openai.com/blog/planning-for-agi-and-beyond](https://openai.com/blog/planning-for-agi-and-beyond)，2023。[在线;
    访问日期：2023 年 3 月 27 日]。
- en: (34) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela
    Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman,
    Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter
    Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language
    models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (34) Long Ouyang、Jeff Wu、Xu Jiang、Diogo Almeida、Carroll L. Wainwright、Pamela
    Mishkin、Chong Zhang、Sandhini Agarwal、Katarina Slama、Alex Ray、John Schulman、Jacob
    Hilton、Fraser Kelton、Luke E. Miller、Maddie Simens、Amanda Askell、Peter Welinder、Paul
    Francis Christiano、Jan Leike 和 Ryan J. Lowe. 通过人类反馈训练语言模型以遵循指令。ArXiv，abs/2203.02155，2022。
- en: '(35) Aaron Parisi, Yao Zhao, and Noah Fiedel. Talm: Tool augmented language
    models. ArXiv, abs/2205.12255, 2022.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(35) Aaron Parisi、Yao Zhao 和 Noah Fiedel. Talm: 工具增强语言模型。ArXiv，abs/2205.12255，2022。'
- en: '(36) Arkil Patel, Satwik Bhattamishra, and Navin Goyal. Are NLP models really
    able to solve simple math word problems? In Proceedings of the 2021 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, pages 2080–2094, Online, June 2021\. Association
    for Computational Linguistics.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (36) Arkil Patel、Satwik Bhattamishra 和 Navin Goyal. NLP 模型真的能够解决简单的数学文字问题吗？发表于《2021
    年北美计算语言学协会：人类语言技术会议论文集》，第 2080–2094 页，在线，2021 年 6 月。计算语言学协会。
- en: (37) Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
    Clark, Kenton Lee, and Luke Zettlemoyer. Deep contextualized word representations.
    In North American Chapter of the Association for Computational Linguistics, 2018.
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (37) Matthew E. Peters、Mark Neumann、Mohit Iyyer、Matt Gardner、Christopher Clark、Kenton
    Lee 和 Luke Zettlemoyer. 深度上下文化词汇表示。发表于北美计算语言学协会，2018。
- en: (38) Alec Radford and Karthik Narasimhan. Improving language understanding by
    generative pre-training. 2018.
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (38) Alec Radford 和 Karthik Narasimhan. 通过生成预训练提高语言理解。2018。
- en: (39) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
    Sutskever. Language models are unsupervised multitask learners. 2019.
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) Alec Radford、Jeff Wu、Rewon Child、David Luan、Dario Amodei 和 Ilya Sutskever.
    语言模型是无监督的多任务学习者。2019。
- en: (40) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
    Hierarchical text-conditional image generation with clip latents. ArXiv, abs/2204.06125,
    2022.
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (40) Aditya Ramesh、Prafulla Dhariwal、Alex Nichol、Casey Chu 和 Mark Chen. 基于 CLIP
    潜变量的分层文本条件图像生成。ArXiv，abs/2204.06125，2022。
- en: (41) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
    Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. ArXiv,
    abs/2102.12092, 2021.
  id: totrans-705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (41) Aditya Ramesh、Mikhail Pavlov、Gabriel Goh、Scott Gray、Chelsea Voss、Alec Radford、Mark
    Chen 和 Ilya Sutskever. 零样本文本到图像生成。ArXiv，abs/2102.12092，2021。
- en: '(42) Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.
    Bpr: Bayesian personalized ranking from implicit feedback. In Proceedings of the
    Twenty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI ’09, page
    452–461, Arlington, Virginia, USA, 2009\. AUAI Press.'
  id: totrans-706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(42) Steffen Rendle、Christoph Freudenthaler、Zeno Gantner 和 Lars Schmidt-Thieme.
    BPR: 从隐式反馈中得到的贝叶斯个性化排序。发表于第二十五届人工智能不确定性会议论文集，UAI ’09，第 452–461 页，美国弗吉尼亚州阿灵顿，2009。AUAI
    出版社。'
- en: '(43) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli,
    Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models
    can teach themselves to use tools. ArXiv, abs/2302.04761, 2023.'
  id: totrans-707
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(43) Timo Schick、Jane Dwivedi-Yu、Roberto Dessì、Roberta Raileanu、Maria Lomeli、Luke
    Zettlemoyer、Nicola Cancedda 和 Thomas Scialom. Toolformer: 语言模型可以自我学习使用工具。ArXiv，abs/2302.04761，2023。'
- en: (44) Timo Schick and Hinrich Schütze. Exploiting cloze-questions for few-shot
    text classification and natural language inference. In Conference of the European
    Chapter of the Association for Computational Linguistics, 2020.
  id: totrans-708
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (44) Timo Schick 和 Hinrich Schütze. 利用完形填空问题进行少量样本的文本分类和自然语言推断。发表于欧洲计算语言学协会会议，2020年。
- en: (45) Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and Philip S. Yu. A survey
    of heterogeneous information network analysis. IEEE Transactions on Knowledge
    and Data Engineering, 29:17–37, 2015.
  id: totrans-709
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (45) Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, 和 Philip S. Yu. 异构信息网络分析综述。《IEEE
    知识与数据工程学报》，29：17–37，2015年。
- en: '(46) Ke Sun, Zhouchen Lin, and Zhanxing Zhu. Adagcn: Adaboosting graph convolutional
    networks into deep models, 2019.'
  id: totrans-710
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (46) Ke Sun, Zhouchen Lin, 和 Zhanxing Zhu. Adagcn：将 AdaBoost 图卷积网络提升为深度模型，2019年。
- en: '(47) Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, and Tianyi Wu. Pathsim:
    Meta path-based top-k similarity search in heterogeneous information networks.
    Proc. VLDB Endow., 4(11):992–1003, aug 2011.'
  id: totrans-711
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Yizhou Sun, Jiawei Han, Xifeng Yan, Philip S. Yu, 和 Tianyi Wu. Pathsim：基于元路径的异构信息网络中前
    k 相似度搜索。Proc. VLDB Endow., 4(11)：992–1003，2011年8月。
- en: '(48) Damian Szklarczyk, Annika L Gable, Katerina C Nastou, David Lyon, Rebecca
    Kirsch, Sampo Pyysalo, Nadezhda T Doncheva, Marc Legeay, Tao Fang, Peer Bork,
    Lars J Jensen, and Christian von Mering. The STRING database in 2021: customizable
    protein–protein networks, and functional characterization of user-uploaded gene/measurement
    sets. Nucleic Acids Research, 49(D1):D605–D612, 11 2020.'
  id: totrans-712
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (48) Damian Szklarczyk, Annika L Gable, Katerina C Nastou, David Lyon, Rebecca
    Kirsch, Sampo Pyysalo, Nadezhda T Doncheva, Marc Legeay, Tao Fang, Peer Bork,
    Lars J Jensen, 和 Christian von Mering. STRING 数据库 2021：可定制的蛋白质-蛋白质网络，以及用户上传的基因/测量集的功能表征。《核酸研究》，49(D1)：D605–D612，2020年11月。
- en: '(49) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.'
  id: totrans-713
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (49) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave, 和 Guillaume Lample. Llama：开放且高效的基础语言模型。ArXiv，abs/2302.13971，2023年。
- en: (50) Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need.
    ArXiv, abs/1706.03762, 2017.
  id: totrans-714
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (50) Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
    Aidan N. Gomez, Lukasz Kaiser, 和 Illia Polosukhin. 注意力机制才是你需要的一切。ArXiv，abs/1706.03762，2017年。
- en: (51) Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
    Liò, and Yoshua Bengio. Graph Attention Networks. International Conference on
    Learning Representations, 2018.
  id: totrans-715
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (51) Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
    Liò, 和 Yoshua Bengio. 图注意力网络。国际学习表征会议，2018年。
- en: '(52) Saraswathi Vishveshwara, K. V. Brinda, and N. Kannan. Protein structure:
    Insights from graph theory. Journal of Theoretical and Computational Chemistry,
    01(01):187–211, 2002.'
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (52) Saraswathi Vishveshwara, K. V. Brinda, 和 N. Kannan. 蛋白质结构：来自图论的见解。《理论与计算化学杂志》，01(01)：187–211，2002年。
- en: '(53) Ben Wang and Aran Komatsuzaki. GPT-J-6B: A 6 Billion Parameter Autoregressive
    Language Model. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax),
    May 2021.'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (53) Ben Wang 和 Aran Komatsuzaki. GPT-J-6B：一个具有 60 亿参数的自回归语言模型。 [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)，2021年5月。
- en: (54) Albert Webson and Ellie Pavlick. Do prompt-based models really understand
    the meaning of their prompts? ArXiv, abs/2109.01247, 2021.
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) Albert Webson 和 Ellie Pavlick. 基于提示的模型真的理解提示的意义吗？ArXiv，abs/2109.01247，2021年。
- en: (55) Pinar Yanardag and S.V.N. Vishwanathan. Deep graph kernels. In Proceedings
    of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data
    Mining, KDD ’15, page 1365–1374, New York, NY, USA, 2015\. Association for Computing
    Machinery.
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) Pinar Yanardag 和 S.V.N. Vishwanathan. 深度图核。发表于第21届 ACM SIGKDD 国际知识发现与数据挖掘会议，KDD
    ’15，第1365–1374页，美国纽约，2015年。计算机协会。
- en: (56) Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J. Kim.
    Graph transformer networks. In Neural Information Processing Systems, 2019.
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, 和 Hyunwoo J. Kim.
    图转换器网络。发表于神经信息处理系统会议，2019年。
- en: '(57) Jiawei Zhang. Graph neural networks for small graph and giant network
    representation learning: An overview. ArXiv, abs/1908.00187, 2019.'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (57) Jiawei Zhang. 小图和巨型网络表示学习的图神经网络：综述。ArXiv，abs/1908.00187，2019年。
- en: (58) Jiawei Zhang. Segmented graph-bert for graph instance modeling. ArXiv,
    abs/2002.03283, 2020.
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) Jiawei Zhang. 用于图实例建模的分段图-bert。ArXiv, abs/2002.03283, 2020.
- en: '(59) Jiawei Zhang and Lin Meng. Gresnet: Graph residual network for reviving
    deep gnns from suspended animation. ArXiv, abs/1909.05729, 2019.'
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(59) Jiawei Zhang 和 Lin Meng. Gresnet: 图残差网络，用于复苏深度 GNNs。ArXiv, abs/1909.05729,
    2019.'
- en: '(60) Jiawei Zhang, Haopeng Zhang, Li Sun, and Congying Xia. Graph-bert: Only
    attention is needed for learning graph representations. ArXiv, abs/2001.05140,
    2020.'
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(60) Jiawei Zhang, Haopeng Zhang, Li Sun, 和 Congying Xia. Graph-bert: 仅需注意力即可学习图表示。ArXiv,
    abs/2001.05140, 2020.'
- en: 'Appendix A Appendix: Graph Data Format'
  id: totrans-725
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录：图数据格式
- en: A.1\. Graph Data Loading
  id: totrans-726
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1. 图数据加载
- en: 'The datasets are stored in binary format, which can be loaded with pickle,
    e.g., the GPR dataset can be loaded as follows:'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集以二进制格式存储，可以使用 pickle 加载，例如，GPR 数据集可以按如下方式加载：
- en: 1import  pickle2f  =  open(’./gpr’,  ’rb’)3dataset  =  pickle.load(f)4f.close()5print(dataset.keys())
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 1import pickle2f = open('./gpr', 'rb')3dataset = pickle.load(f)4f.close()5print(dataset.keys())
- en: A.2\. Graph Dataset
  id: totrans-729
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2. 图数据集
- en: 'For the datasets with 1 single large-scale graph/network (including, Cora,
    Pubmed, Citeseer; Twitter, Foursquare; Amazon, Last-FM, Movielens; WordNet, Freebase),
    the loaded ”dataset” is organized with a python dictionary with the following
    format:'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有 1 个大型图/网络的数据集（包括 Cora、Pubmed、Citeseer；Twitter、Foursquare；Amazon、Last-FM、Movielens；WordNet、Freebase），加载后的“dataset”组织为具有以下格式的
    Python 字典：
- en: '1dataset  =  {2  "data_profile":  {3  ’name’:  dataset_name,4  ’order’:  node_number,5  ’size’:  link_number,6  ’is_directed’:  boolean,7  ’is_weighted’:  boolean,8  #  besides  the  above  profile  information,  for  some  data,  we  will  also  include  some  other  attributes,  like  feature  vector  dimensions,  label  space  dimension,  etc.,  in  the  data  profile  dict.9  },10  "nodes":  {11  node_id:  {’features’:  feature,  ’label’:  label,}12  },13  "links":  {14  node_pair:  {’features’:  feature,  ’label’:  label,}15  }16  }'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '1dataset = {2 "data_profile": {3 ''name'': dataset_name,4 ''order'': node_number,5
    ''size'': link_number,6 ''is_directed'': boolean,7 ''is_weighted'': boolean,8
    # 除了上述配置文件信息，对于某些数据，我们还会在数据配置文件字典中包含其他一些属性，如特征向量维度、标签空间维度等。9 },10 "nodes": {11
    node_id: {''features'': feature, ''label'': label,}12 },13 "links": {14 node_pair:
    {''features'': feature, ''label'': label,}15 }16 }'
- en: A.3\. Graph Instance Dataset
  id: totrans-732
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3. 图实例数据集
- en: 'For the datasets with multiple graph instances (GPR; Proteins, Mutag, NCI1,
    PTC), the loaded ”dataset” is organized with a python dictionary with the following
    format:'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有多个图实例的数据集（GPR；Proteins、Mutag、NCI1、PTC），加载后的“dataset”组织为具有以下格式的 Python 字典：
- en: '1dataset  =  {2  "data_profile":  {3  ’name’:  dataset_name,4  ’graph_number’:  graph_number,5  ’is_directed’:  boolean,6  ’is_weighted’:  boolean,7  #  besides  the  above  profile  information,  for  some  data,  we  will  also  include  some  other  attributes,  like  feature  vector  dimensions,  label  space  dimension,  etc.,  in  the  data  profile  dict.8  },9  ’graph_set’:  {10  graph_id:  {11  ’nodes’:  node_set,12  ’links’:  link_set,13  ’label’:  graph_instance_label,14  }15  }16  }'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '1dataset = {2 "data_profile": {3 ''name'': dataset_name,4 ''graph_number'':
    graph_number,5 ''is_directed'': boolean,6 ''is_weighted'': boolean,7 # 除了上述配置文件信息，对于某些数据，我们还会在数据配置文件字典中包含其他一些属性，如特征向量维度、标签空间维度等。8
    },9 ''graph_set'': {10 graph_id: {11 ''nodes'': node_set,12 ''links'': link_set,13
    ''label'': graph_instance_label,14 }15 }16 }'
- en: 'Appendix B Appendix: Prompt Dataset'
  id: totrans-735
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 附录：提示数据集
- en: B.1\. Shared Prompt Datasets
  id: totrans-736
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1. 共享提示数据集
- en: This directory contains the graph reasoning prompts for 15 different graph datasets.
    They all have their corresponding raw graph datasets (which can be downloaded
    from this page).
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 该目录包含 15 个不同图数据集的图推理提示。它们都有相应的原始图数据集（可以从此页面下载）。
- en: Each directory contains the train/test graph reasoning tuples for different
    graph datasets.
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: 每个目录包含不同图数据集的训练/测试图推理元组。
- en: •
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'mixed: it merges all train/test prompts from all the following 15 graph dataset
    (except the graph_data_loading), we will use this for the LLM tuning.'
  id: totrans-740
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: mixed：它合并了所有以下 15 个图数据集中的训练/测试提示（除了 graph_data_loading），我们将用于 LLM 调优。
- en: •
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'graph_properties: it contains the train/test prompts for the gpr dataset created
    in this paper on graph property reasoning'
  id: totrans-742
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: graph_properties：它包含用于图属性推理的 gpr 数据集的训练/测试提示
- en: •
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'bibliographic_networks: it contains the train/test prompts for 3 bibliographc
    network reasoning datasets, cora, pubmed, citeseer'
  id: totrans-744
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: bibliographic_networks：它包含 3 个书目网络推理数据集的训练/测试提示，即 cora、pubmed、citeseer
- en: •
  id: totrans-745
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'molecular_graphs: it contains the train/test prompts for 4 molecular graph
    reasoning datasets, proteins, mutag, nci1, ptc'
  id: totrans-746
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'molecular_graphs: 包含 4 个分子图推理数据集的训练/测试提示，分别为 proteins、mutag、nci1 和 ptc'
- en: •
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'recommender_systems: it contains the train/test prompts for 3 recommender system
    reasoning datasets, amazon, last-fm, movielens'
  id: totrans-748
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'recommender_systems: 包含 3 个推荐系统推理数据集的训练/测试提示，分别为 amazon、last-fm 和 movielens'
- en: •
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'social_networks: it contains the train/test prompts for 2 social network reasoning
    datasets, foursquare, twitter'
  id: totrans-750
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'social_networks: 包含 2 个社交网络推理数据集的训练/测试提示，分别为 foursquare 和 twitter'
- en: •
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'knowledge_graphs: it contains the train/test prompts for 2 knowledge graph
    reasoning datasets, wordnet, freebase'
  id: totrans-752
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'knowledge_graphs: 包含 2 个知识图推理数据集的训练/测试提示，分别为 wordnet 和 freebase'
- en: These above prompts are all created with prompt templates augmented by ChatGPT
    based on the concrete graph datasets. In the prompts, we will use the concrete
    data instances, node ids, relations, and reasoning outputs.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 上述提示均使用了由 ChatGPT 基于具体图数据集增强的提示模板创建。在这些提示中，我们将使用具体的数据实例、节点 ID、关系和推理输出。
- en: 'In addition to these prompts corresponding to concrete graph datasets, we also
    include a prompt dataset purely generated by chatgpt on graph loading:'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些对应具体图数据集的提示外，我们还包括了一个纯粹由 chatgpt 生成的图加载提示数据集：
- en: •
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'graph_data_loading: it contains the pure-chatgpt generated graph data loading
    prompts'
  id: totrans-756
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'graph_data_loading: 包含纯 ChatGPT 生成的图数据加载提示'
- en: B.2\. Prompt Format
  id: totrans-757
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2\. 提示格式
- en: 'Each of the above directory contains two files:'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 上述每个目录包含两个文件：
- en: 1prompts_train2prompts_test
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 1prompts_train2prompts_test
- en: 'which denote denote the prompts for training and testing, respectively. Each
    prompt instance in the training/testing sets has 3 entries:'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分别表示用于训练和测试的提示。训练/测试集中的每个提示实例有 3 个条目：
- en: 1Input,  Output,  Reasoning  Result
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: 1Input,  Output,  Reasoning  Result
- en: •
  id: totrans-762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input: The input contains the potential query inputs to the Graph-toolformers.'
  id: totrans-763
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入：输入包含可能的查询输入到 Graph-toolformers。
- en: •
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Output: The output will be the annotated query output generated by the LLMs
    with added graph reasoning API calls.'
  id: totrans-765
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出：输出将是 LLMs 生成的带有附加图推理 API 调用的标注查询输出。
- en: •
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Reasoning Result: We also add the reasoning result for many prompt tupes, which
    will be used for reasoning result evaluation only.'
  id: totrans-767
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推理结果：我们还添加了许多提示类型的推理结果，这些结果将仅用于推理结果评估。
- en: 'Appendix C Appendix: Pre-trained Graph Models'
  id: totrans-768
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 附录：预训练图模型
- en: '6 different pre-trained graph models are included in the Graph-ToolFormer framework
    for different graph reasoning tasks, which are listed as follows:'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: Graph-ToolFormer 框架包含 6 个用于不同图推理任务的预训练图模型，具体如下：
- en: •
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph Property Reasoning Model: toolx'
  id: totrans-771
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图属性推理模型：toolx
- en: •
  id: totrans-772
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Bibliographic Network Reasoning Model: Graph-Bert'
  id: totrans-773
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 文献网络推理模型：Graph-Bert
- en: •
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Molecular Graph Reasoning Model: SEG-Bert'
  id: totrans-775
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分子图推理模型：SEG-Bert
- en: •
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Online Social Network Reasoning Model: KMeans'
  id: totrans-777
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在线社交网络推理模型：KMeans
- en: •
  id: totrans-778
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Recommender System Reasoning Model: BPR (Bayesian Personalized Ranking)'
  id: totrans-779
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 推荐系统推理模型：BPR（贝叶斯个性化排序）
- en: •
  id: totrans-780
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Knowledge Graph Reasoning Model: TransE'
  id: totrans-781
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 知识图推理模型：TransE
- en: C.1\. Toolx for Graph Property Reasoning
  id: totrans-782
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1\. Toolx 用于图属性推理
- en: 'The current toolx model is implemented based on networkx, and toolx will implement
    different functions to calculate different graph properties mentioned in the paper,
    which are also listed as follows:'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的 toolx 模型是基于 networkx 实现的，toolx 将实现不同的功能来计算论文中提到的不同图属性，具体如下：
- en: 1-  order2-  size3-  density4-  eccentricity5-  radius6-  diameter7-  center8-  shortest_path9-  avg_path_length10-  min_path_length11-  max_path_length12-  periphery
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: 1-  order2-  size3-  density4-  eccentricity5-  radius6-  diameter7-  center8-  shortest_path9-  avg_path_length10-  min_path_length11-  max_path_length12-  periphery
- en: C.2\. Graph-Bert for Bibliographic Network Paper Topic Reasoning
  id: totrans-785
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.2\. Graph-Bert 用于文献网络论文主题推理
- en: 'The Graph-Bert model was proposed in paper entitled ”Graph-Bert: Only Attention
    is Needed for Learning Graph Representations”.'
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 'Graph-Bert 模型在题为《Graph-Bert: Only Attention is Needed for Learning Graph Representations》的论文中提出。'
- en: 'The Graph-Bert will be used to implement the bibliographic network paper topic
    inference function, which will be reduced to the node classification task:'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: Graph-Bert 将用于实现文献网络论文主题推断功能，这将简化为节点分类任务：
- en: 1-  node_classification  in  GraphBertNodeClassification.py
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 1-  node_classification  在  GraphBertNodeClassification.py 中
- en: The model has been pre-trained on the cora, pubmed, citeseer datasets already
    based on the identical train/test sets introduced in the paper. Both the model
    code and the pre-trained model parameter checkpoints are provided.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型已经在 cora、pubmed、citeseer 数据集上进行了预训练，使用的是论文中介绍的相同训练/测试集。提供了模型代码和预训练模型参数检查点。
- en: As to the original source code, readers may consider to refer to the repository
    (https://github.com/jwzhanggy/Graph-Bert) for more information.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 关于原始源代码，读者可以参考这个仓库 ([https://github.com/jwzhanggy/Graph-Bert](https://github.com/jwzhanggy/Graph-Bert))
    以获取更多信息。
- en: C.3\. SEG-Bert for Molecualr Graph Function Reasoning
  id: totrans-791
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3\. SEG-Bert 用于分子图函数推理
- en: The SEG-Bert model was proposed in the paper entitled ”Segmented Graph-Bert
    for Graph Instance Modeling ”.
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: SEG-Bert 模型在题为《用于图实例建模的分段图-Bert》的论文中提出。
- en: 'The SEG-Bert will be used to implement the molecular graph function inference
    function, which will be reduced to the graph classification task:'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: SEG-Bert 将用于实现分子图函数推理功能，这将简化为图分类任务：
- en: 1-  graph_classification  in  SegmentedGraphBertGraphClassification.py
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 1-  graph_classification 在 SegmentedGraphBertGraphClassification.py 中
- en: The model has been pre-trained on the proteins, mutag, nci1 and ptc datasets
    already based on the identical train/test sets introduced in the paper. Both the
    model code and the pre-trained model parameter checkpoints are provided.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型已经在 proteins、mutag、nci1 和 ptc 数据集上进行了预训练，使用的是论文中介绍的相同训练/测试集。提供了模型代码和预训练模型参数检查点。
- en: As to the original source code, readers may consider to refer to the repository
    (https://github.com/jwzhanggy/Graph-Bert) for more information.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 关于原始源代码，读者可以参考这个仓库 ([https://github.com/jwzhanggy/Graph-Bert](https://github.com/jwzhanggy/Graph-Bert))
    以获取更多信息。
- en: C.4\. KMeans for Social Network Community Reasoning
  id: totrans-797
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.4\. KMeans 用于社交网络社区推理
- en: To detect the social network community, based on the social network structure
    (adjacency matrix), we calculate the nodes’ pairwise common neighbor numbers to
    define their closeness, which will be fed to KMeans for community detection.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测社交网络社区，根据社交网络结构（邻接矩阵），我们计算节点对之间的共同邻居数量以定义它们的接近度，然后将这些数据输入到 KMeans 中进行社区检测。
- en: '1-  community(graph):  return  the  community  label  for  all  user  nodes  in  the  social  network2-  community(graph,  node):  return  the  community  label  for  specify  input  user  nodes3-  community_count(graph):  return  the  number  of  detected  communities4-  community_avg_size(graph):  return  the  average  size  of  detected  communities5-  community_max_size(graph):  return  the  max  size  of  detected  communities6-  community_size(graph,  node):  return  the  size  of  community  that  the  input  user  node  belongs  to7-  common_community_check(graph,  node1,  node2):  check  if  user1  and  user2  belong  to  the  same  community  or  not'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: '1-  community(graph): 返回社交网络中所有用户节点的社区标签2-  community(graph, node): 返回指定输入用户节点的社区标签3-  community_count(graph):
    返回检测到的社区数量4-  community_avg_size(graph): 返回检测到的社区的平均大小5-  community_max_size(graph):
    返回检测到的社区的最大大小6-  community_size(graph, node): 返回输入用户节点所属社区的大小7-  common_community_check(graph,
    node1, node2): 检查用户1和用户2是否属于同一社区'
- en: C.5\. BPR for Recommender System Reasoning
  id: totrans-800
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.5\. BPR 用于推荐系统推理
- en: 'The BPR model was proposed in the paper entitled ”BPR: Bayesian personalized
    ranking from implicit feedback”.'
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 'BPR 模型在题为《BPR: 基于隐式反馈的贝叶斯个性化排序》的论文中提出。'
- en: 'The BPR model will be used to implement the social network community detection
    functions, which will be reduced to the graph partition/clustering tasks:'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: BPR 模型将用于实现社交网络社区检测功能，这将简化为图分区/聚类任务：
- en: 1-  recommendation  in  BPR.py2-  topk_recommendation  in  BPR.py
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 1-  recommendation 在 BPR.py 中2-  topk_recommendation 在 BPR.py 中
- en: C.6\. TransE for Knowledge Graph Reasoning
  id: totrans-804
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.6\. TransE 用于知识图谱推理
- en: The TransE model was proposed in the paper entitled ”Transition-based Knowledge
    Graph Embedding with Relational Mapping Properties”.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: TransE 模型在题为《基于转移的知识图谱嵌入与关系映射属性》的论文中提出。
- en: 'The TransE will be used to implement the knowledge graph entity/relation searching
    functions, which will be reduced to the graph searching tasks:'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: TransE 将用于实现知识图谱实体/关系搜索功能，这将简化为图搜索任务：
- en: 1-  search_head_entity  in  TransE.py2-  search_tail_entity  in  TransE.py3-  search_relation  in  TransE.py
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 1-  search_head_entity 在 TransE.py 中2-  search_tail_entity 在 TransE.py 中3-  search_relation
    在 TransE.py 中
