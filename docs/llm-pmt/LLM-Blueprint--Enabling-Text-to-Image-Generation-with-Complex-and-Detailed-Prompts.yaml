- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:48:47'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed
    Prompts'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2310.10640](https://ar5iv.labs.arxiv.org/html/2310.10640)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Hanan Gani¹, Shariq Farooq Bhat², Muzammal Naseer¹, Salman Khan^(1,3), Peter
    Wonka²
  prefs: []
  type: TYPE_NORMAL
- en: ¹Mohamed Bin Zayed University of AI   ²KAUST   ³Australian National University
  prefs: []
  type: TYPE_NORMAL
- en: '{hanan.ghani, muzammal.naseer, salman.khan}@mbzuai.ac.ae'
  prefs: []
  type: TYPE_NORMAL
- en: shariq.bhat@kaust.edu.sa,  pwonka@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Diffusion-based generative models have significantly advanced text-to-image
    generation but encounter challenges when processing lengthy and intricate text
    prompts describing complex scenes with multiple objects. While excelling in generating
    images from short, single-object descriptions, these models often struggle to
    faithfully capture all the nuanced details within longer and more elaborate textual
    inputs. In response, we present a novel approach leveraging Large Language Models
    (LLMs) to extract critical components from text prompts, including bounding box
    coordinates for foreground objects, detailed textual descriptions for individual
    objects, and a succinct background context. These components form the foundation
    of our layout-to-image generation model, which operates in two phases. The initial
    Global Scene Generation utilizes object layouts and background context to create
    an initial scene but often falls short in faithfully representing object characteristics
    as specified in the prompts. To address this limitation, we introduce an Iterative
    Refinement Scheme that iteratively evaluates and refines box-level content to
    align them with their textual descriptions, recomposing objects as needed to ensure
    consistency. Our evaluation on complex prompts featuring multiple objects demonstrates
    a substantial improvement in recall compared to baseline diffusion models. This
    is further validated by a user study, underscoring the efficacy of our approach
    in generating coherent and detailed scenes from intricate textual inputs.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dfcc4b5eaeebd0fd092261b93549e0be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Current state-of-the-art text-to-image models (Columns 1-4) face
    challenges when dealing with lengthy and detailed text prompts, resulting in the
    exclusion of objects and fine-grained details. Our approach (Column 5) adeptly
    encompasses all the objects described, preserving their intricate features and
    spatial characteristics as outlined in the two white boxes.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Modern generative diffusion models, e.gRombach et al. ([2022](#bib.bib38));
    Ho et al. ([2020](#bib.bib17)); Saharia et al. ([2022](#bib.bib40)); Ruiz et al.
    ([2023](#bib.bib39)), provided a massive leap forward in the problem of text-to-image
    generation and have emerged as powerful tools for creating diverse images and
    graphics from plain text prompts. Their success can be attributed to several factors,
    including the availability of internet-scale multi-modal datasets, increased computational
    resources, and the scaling up of model parameters. These models are trained using
    shorter prompts and especially excel at generating images of one prominent foreground
    object. However, as the description length and the number of objects in the scene
    increase, modern diffusion models tend to ignore parts of the prompt often leading
    to critical omissions, misrepresentations, or the generation of objects that do
    not align with the nuanced details described in the prompts. Fig. [1](#S0.F1 "Figure
    1 ‣ LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed
    Prompts") shows a scenario where existing state-of-the-art text-to-image diffusion
    models struggle to follow all the details. This failure can partly be ascribed
    to the diffusion model’s CLIP text encoder  Radford et al. ([2021](#bib.bib34))
    which can only process the first $77$ text tokens, effectively truncating longer
    prompts and potentially omitting critical details. Indeed, a single prompt describing
    a complex scene can span far beyond these token limits, making it a challenge
    for existing models to process and translate long prompts comprehensively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recent efforts (Epstein et al., [2023](#bib.bib7); Kang et al., [2023](#bib.bib18))
    have been dedicated to improving the capabilities of pre-trained diffusion models
    to faithfully follow the intricate details within text prompts. These works predominantly
    revolve around aspects such as object count (e.g. “2 oranges and 4 apples on the
    table”), and/or capturing spatial relationships among objects (e.g. “an orange
    on the left of an apple”). In the context of longer and more complex prompts,
    these models still tend to struggle to generate coherent images that faithfully
    reflect the complexity of the text prompts, especially when tasked with the placement
    of object instances at considerable spatial separations, often falling short of
    comprehensively capturing all instances of objects as intended. More recently
    layout-based diffusion models Feng et al. ([2023](#bib.bib10)); Li et al. ([2023](#bib.bib24));
    Yang et al. ([2023b](#bib.bib49)) have proven to be effective in capturing the
    count and spatial characteristics of the objects in the prompt. Such models first
    generate bounding boxes of all the objects and then condition the diffusion model
    jointly on the bounding boxes and the text prompt to generate the final image.
    While effective in the case of small prompts, these models still struggle when
    presented with long text descriptions that feature multiple diverse objects and
    hence fail to generate the desired output (See Fig. [1](#S0.F1 "Figure 1 ‣ LLM
    Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To address these challenges, our approach seeks to improve text-to-image generation
    from lengthy prompts. We introduce a framework that divides image generation into
    two phases: generating a global scene, followed by iterative refinement of individual
    object representations. We exploit LLMs to break down long prompts into smaller
    components organized in a data structure that we call Scene Blueprint. This allows
    us to generate the image in a step-wise manner. Our framework ensures that the
    final image faithfully adheres to the details specified in lengthy and complex
    text prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We evaluate our framework on challenging prompts containing 3 to 10 unique
    foreground objects in varied scenes. Our results showcase a significant improvement
    in recall ($\sim$69%) (+16 % improvement). We also include a user study that demonstrates
    that our proposed method consistently produces coherent images that closely align
    with their respective textual descriptions, whereas existing approaches struggle
    to effectively handle longer text prompts (See Fig. [4](#S4.F4 "Figure 4 ‣ 4 Experiments
    ‣ LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts"))'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, our main contributions are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iterative image generation from long prompts: We introduce a two-phase framework
    for generating images from long textual descriptions, ensuring a faithful representation
    of details.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scene Blueprints using LLMs: We propose Scene Blueprints as a structured scene
    representation encompassing scene layout and object descriptions that enable a
    coherent step-wise generation of images from complex and lengthy prompts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'State-of-the-art results: We present quantitative and qualitative results showcasing
    the effectiveness of our method, in terms of adherence to textual descriptions,
    demonstrating its applicability and superiority in text-to-image synthesis from
    lengthy prompts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Text-to-Image Diffusion. Over the years, Generative Adversarial Networks (GAN)
    (Goodfellow et al., [2014](#bib.bib12)) have been the default choice for image
    synthesis (Brock et al., [2018](#bib.bib3); Reed et al., [2016](#bib.bib37); Xu
    et al., [2018](#bib.bib47); Zhang et al., [2017](#bib.bib52); [2021](#bib.bib54);
    Tao et al., [2022](#bib.bib46); Zhang et al., [2018a](#bib.bib53); Karras et al.,
    [2019](#bib.bib19)). However, more recently, the focus has shifted towards text
    conditioned autoregressive models (Ding et al., [2021](#bib.bib6); Gafni et al.,
    [2022](#bib.bib11); Ramesh et al., [2021](#bib.bib35); Yu et al., [2022](#bib.bib51))
    and diffusion models (Rombach et al., [2022](#bib.bib38); Gu et al., [2022](#bib.bib13);
    Nichol et al., [2021](#bib.bib30); Ramesh et al., [2022](#bib.bib36); Saharia
    et al., [2022](#bib.bib40)) which have exhibited impressive capabilities in producing
    high-quality images while avoiding the training challenges, such as instability
    and mode collapse, commonly associated with GANs (Arjovsky et al., [2017](#bib.bib1);
    Gulrajani et al., [2017](#bib.bib14); Kodali et al., [2017](#bib.bib21)). In particular,
    diffusion models are trained on large-scale multi-modal data and are capable of
    generating high-resolution images conditioned on text input. Nevertheless, effectively
    conveying all the nuances of an image solely from a text prompt can present a
    considerable hurdle. Recent studies have demonstrated the effectiveness of classifier-free
    guidance (Ho & Salimans, [2022](#bib.bib16)) in improving the faithfulness of
    the generations in relation to the input prompt. However, all these approaches
    are designed to accept shorter text prompts, but they tend to fail in scenarios
    where the prompt describing a scene is longer. In contrast, our proposed approach
    generates images from longer text prompts, offering an efficient solution to address
    this challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Layout-to-Image Generation. Generating images from layouts either in the form
    of labeled bounding boxes or semantic maps was recently explored in (Sun & Wu,
    [2019](#bib.bib44); Sylvain et al., [2021](#bib.bib45); Yang et al., [2022](#bib.bib50);
    Fan et al., [2023](#bib.bib9); Zhao et al., [2019](#bib.bib56); Park et al., [2019](#bib.bib32)).
    Critically, these layout to image generation methods are only conditioned on bounding
    boxes and are closed-set, i.e., they can only generate limited localized visual
    concepts observed in the training set. With the inception of large multi-modal
    foundational models such as CLIP (Radford et al., [2021](#bib.bib34)), it has
    now been possible to generate images in an open-set fashion. Diffusion-based generative
    models can be conditioned on multiple inputs, however, they have been shown to
    struggle in following the exact object count and spatial locations in the text
    prompts  Chen et al. ([2023](#bib.bib4)); Kang et al. ([2023](#bib.bib18)). More
    recently layout conditioned diffusion models have been proposed to solve this
    problem (Chen et al., [2023](#bib.bib4); Li et al., [2023](#bib.bib24); Yang et al.,
    [2023b](#bib.bib49); Phung et al., [2023](#bib.bib33)). Chen et al. ([2023](#bib.bib4))
    manipulates the cross-attention layers that the model uses to interface textual
    and visual information and steers the reconstruction in the desired user-specified
    layout. GLIGEN (Li et al., [2023](#bib.bib24)) uses a gated self-attention layer
    that enables additional inputs (e.g., bounding boxes) to be processed. ReCo (Yang
    et al., [2023b](#bib.bib49)) achieves layout control through regional tokens encoded
    as part of the text prompt.  Zheng et al. ([2023](#bib.bib57)) introduce LayoutDiffusion
    which treats each patch of the image as a special object for multimodal fusion
    of layout and image and generates images with both high quality and diversity
    while maintaining precise control over the position and size of multiple objects.
    In addition to this, there have been few works on LLM-based layout generation
    (Feng et al., [2023](#bib.bib10); Lian et al., [2023](#bib.bib25)). These works
    exploit the LLMs’ abilities to reason over numerical and spatial concepts in text
    conditions (Li et al., [2022a](#bib.bib22)). Building upon these works, we extend
    LLMs’ powerful generalization and reasoning capabilities to extract layouts, background
    information, and foreground object descriptions from longer text prompts.
  prefs: []
  type: TYPE_NORMAL
- en: Diffusion Based Image Editing and Composition. Diffusion-based image editing
    has received overwhelming attention due to its ability to condition on multiple
    modalities. Recent works utilize text-based image editing using diffusion models
    to perform region modification. DiffusionCLIP (Kim et al., [2022](#bib.bib20))
    uses diffusion models for text-driven global multi-attribute image manipulation
    on varied domains.  Liu et al. ([2023](#bib.bib27)) provides both text and semantic
    guidance for global image manipulation. In addition, GLIDE (Nichol et al., [2021](#bib.bib30))
    trains a diffusion model for text-to-image synthesis, as well as local image editing
    using text guidance. Image composition refers to a form of image manipulation
    where a foreground reference object is affixed onto a designated source image.
    A naive way to blend a foreground object on a background image may result in an
    unrealistic composition. However, more recent works (Avrahami et al., [2021](#bib.bib2);
    Yang et al., [2023a](#bib.bib48); Ho & Salimans, [2022](#bib.bib16); Lu et al.,
    [2023](#bib.bib28)) use diffusion models to overcome the challenges posed due
    to fusion inconsistency and semantic disharmony for efficient image composition.
    Avrahami et al. ([2021](#bib.bib2)) takes the target region mask and simply blends
    the noised version of the input image with local text-guided diffusion latent.
    Yang et al. ([2023a](#bib.bib48)) trains a diffusion model to blend an exemplar
    image on the source image at the position specified by an arbitrary shape mask
    and leverages the classifier-free guidance (Ho & Salimans, [2022](#bib.bib16))
    to increase the similarity to the exemplar image. Lu et al. ([2023](#bib.bib28))
    introduces TF-ICON which leverages off-the-shelf diffusion models to perform cross-domain
    image-guided composition without requiring additional training, fine-tuning, or
    optimization. Based on these works, we build an iterative refining scheme, which
    performs region-based composition at the layout level, utilizing a given mask
    shape and modifies each layout based on the object characteristics guided by a
    multi-modal loss signal.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9219aa8d46bf9407179a8bd11cd91dfe.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Global Scene Generation: Our proposed approach takes a long text
    prompt describing a complex scene and leverages an LLM to generate $k$ layouts
    which are then interpolated to a single layout, ensuring the spatial accuracy
    of object placement. Along with the layouts, we also query an LLM to generate
    object descriptions along with a concise background prompt summarizing the scene’s
    essence. A Layout-to-Image model is employed which transforms the layout into
    an initial image. Iterative Refinement Scheme: The content of each box proposal
    is refined using a diffusion model conditioned on a box mask, a (generated) reference
    image for the box, and the source image, guided by a multi-modal signal.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Preliminaries on Diffusion Models.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Diffusion models are generative models that learn the data distribution of
    complex datasets. They consist of a forward diffusion process and a reverse diffusion
    process. During the forward process, noise is added to the input data point $\bm{x_{0}}$
    can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\bm{x}_{t}=\sqrt{\alpha_{t}}\bm{x}_{0}+\sqrt{1-\alpha_{t}}\bm{\epsilon},\
    \ \ \bm{\epsilon}\sim\mathcal{N}(\mathbf{0,I}),$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\alpha_{t}:=\prod_{s=1}^{t}{(1-\beta_{s})}.$,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{\epsilon}_{\theta}(\bm{x_{t}},t)\approx\bm{\epsilon}=\frac{\bm{x_{t}}-\sqrt{\alpha_{t}}\bm{x_{0}}}{\sqrt{1-\alpha_{t}}}.$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: After training $\bm{\epsilon}_{\theta}({\bm{x}},t)$ to perform sampling. In
    our work, we utilize the denoising diffusion implicit model (DDIM) Song et al.
    ([2020a](#bib.bib41)) to predict the clean data point.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our core objective is to generate images from long textual descriptions, ensuring
    that the resulting images faithfully represent the intricate details outlined
    in the input text. We generate the output image in a multi-step manner; generating
    an initial image that serves as a template at a global level, followed by a box-level
    refinement phase that serves as a corrective procedure. 1) Global scene generation:
    We begin by decomposing lengthy text prompts into “Scene Blueprints.” Scene Blueprints
    provide a structured representation of the scene containing: object bounding boxes
    in image space, detailed text descriptions for each box, and a background text
    prompt. We use an LLM to extract the Scene Blueprint from the given long prompt
    and use layout-conditioned text-to-image models to generate the initial image.
    Unlike prior works, we support additional user control on the box layouts by generating
    multiple proposal blueprints and providing an option to smoothly interpolate between
    the candidate layouts. This interpolation not only facilitates (optional) user
    control but also helps mitigate any potential errors introduced by the LLM. 2)
    Box-level content refinement: In the second phase, we iterate through all the
    boxes and evaluate and refine their content in terms of quality and adherence
    to the prompt. We use a multi-modal guidance procedure that maximizes a quality
    score for each box.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dbec518d89a1f2a185358302176059ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Effect of interpolation factor $\eta$ values from 0.1 to 0.9 with
    increments of 0.1\. Best viewed in zoom.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Global Scene Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Textual representation of a scene contains various characteristics that provide
    information about objects, their spatial properties, semantics, attributes, and
    more. To coherently capture these properties, we employ an off-the-shelf pre-trained
    large language model (LLM)(OpenAI, [2021](#bib.bib31)). We instruct the LLM with
    an appropriately engineered prompt (see supplementary Sec. [A.4](#A1.SS4 "A.4
    Instruct prompts for LLM ‣ Appendix A Appendix ‣ LLM Blueprint: Enabling Text-to-Image
    Generation with Complex and Detailed Prompts")) to generate a Scene Blueprint
    containing the following three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Layout: Bounding box coordinates for each object - $\{object:(x,y,width,height)\}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Object description: Description associated with each object - $\{object:description\}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Background Prompt: A general prompt describing the overall essence of the scene.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The layout and the background prompt generated by the LLM are then used to condition
    the diffusion model to generate an image. We follow recent work by Lian et al.
    ([2023](#bib.bib25)) which generates the image from the layouts in two steps,
    1) generating masked latent inversion for each object bounding box, and 2) composing
    the latent inversion as well as generating the corresponding background from the
    background prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Layouts Interpolation and Noise Correction. While LLMs have advanced spatial
    reasoning abilities, we observed that they struggle to model the spatial positions
    of the objects when presented with longer descriptions, often resulting in abnormal
    relative sizes of the objects and unnatural placement of object boxes in the scene
    (Sec.[4.1](#S4.SS1 "4.1 Ablations ‣ 4 Experiments ‣ LLM Blueprint: Enabling Text-to-Image
    Generation with Complex and Detailed Prompts")). A naive approach to resolve these
    issues is to fine-tune an LLM on handcrafted data of text descriptions and bounding
    boxes. However, this approach requires extensive resources in terms of human annotators
    and compute requirements, and risks catastrophic forgetting (Luo et al., [2023](#bib.bib29)).
    On the other hand, correcting these errors manually by adjusting the boxes is
    also a daunting task and defeats the purpose of using an LLM to extract layouts.
    To address these challenges, we propose a simple yet effective solution - layout
    interpolation. Instead of generating only one proposal layout, we query the LLM
    to generate $k$ is the interpolation factor which controls the influence of the
    individual bounding boxes on the final interpolated box.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the long complex prompts may result in a large number of boxes, the images
    generated by the layout guidance tend to have color noise and small artifacts
    (See Fig. [A.2](#A1.SS2 "A.2 Noise and artifact correction ‣ Appendix A Appendix
    ‣ LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts")
    in supplementary). Therefore, we optionally perform an image-to-image translation
    step using a diffusion model Rombach et al. ([2022](#bib.bib38)), resulting in
    a cleaner image while preserving the semantics. We refer to this image as $x_{initial}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Despite conditioning on layout, we observe that the diffusion model is unable
    to generate all scene objects effectively. It struggles to compose multiple diverse
    objects having varied spatial and semantic properties in one shot. Consequently,
    $x_{initial}$ often has missing objects or fails to represent them accurately
    in accordance with their descriptions. Therefore, we employ a box-level refinement
    strategy that evaluates and refines the content within each bounding box of the
    layout in terms of quality and adherence to the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Box-level refinement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current diffusion models have certain limitations in terms of composing multiple
    objects in the image when presented with longer text prompts, a problem that is
    still unexplored. To overcome this issue and ensure faithful generation of all
    the objects, we introduce an Iterative Refinement Scheme (IRS). Our proposed IRS
    works at the bounding box level and ensures the corresponding object at each bounding
    box is characterized by its properties given in the textual description. To achieve
    this, we iterate across each object’s bounding box in $x_{initial}$ is below a
    certain threshold, we modify the content of the bounding box such that it follows
    its corresponding description.
  prefs: []
  type: TYPE_NORMAL
- en: Any reasonable modification of the content within a bounding box must improve
    its fidelity and adherence to the prompt. Since diffusion models, e.g. stable
    diffusion (Rombach et al., [2022](#bib.bib38)), are already good at generating
    high-fidelity images from shorter prompts, we exploit this ability and follow
    a paint-by-example approach. We generate a new object for the designated box by
    passing the object description $d_{j}$ to compose the reference object at the
    designated position on the source image specified by the mask. To ensure the composition
    generates the object that follows the properties described in the text prompt
    as closely as possible, we guide the sampling process by an external multi-modal
    loss signal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Box-level Multi-modal Guidance. Given the initial image $x_{initial}$ during
    the denoising diffusion process via Eqn. [1](#S3.E1 "In 3.1 Preliminaries on Diffusion
    Models. ‣ 3 Methodology ‣ LLM Blueprint: Enabling Text-to-Image Generation with
    Complex and Detailed Prompts") as follows,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\bm{x_{0}}}=\frac{\bm{x_{t}}-(\sqrt{1-\alpha_{t}})\bm{\epsilon_{\theta}}(\bm{x_{t}},t)}{\sqrt{\alpha_{t}}}.$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Our CLIP based multimodal guidance loss can then be expressed as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\mathcal{L}_{CLIP}&amp;=\mathcal{L}_{cosine}\Bigl{(}\texttt{CLIP}_{\texttt{image}}(\widehat{x}_{0}\odot
    m_{j}),\texttt{CLIP}_{\texttt{text}}(d_{j})\Bigl{)}+\\ &amp;\quad\quad\lambda\cdot\mathcal{L}_{cosine}\Bigl{(}\texttt{CLIP}_{\texttt{image}}(\widehat{x}_{0}\odot
    m_{j}),\texttt{CLIP}_{\texttt{image}}(x_{j}^{ref})\Bigl{)},\end{split}$ |  | (4)
    |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{L}_{cosine}$ given as,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{guidance}=\mathcal{L}_{CLIP}+\gamma\cdot\mathcal{L}_{bg}.$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: The gradient of the resultant loss $\nabla_{\widehat{x}_{0}}\mathcal{L}_{guidance}$.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5b31ccc395080cea7f241891e631e7a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: User study. A majority of users picked our method compared to prior
    works when presented with a 2-AFC task of selecting the image that adheres to
    the given prompt the most.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Settings: Our framework uses a combination of several components. For acquiring
    the long text descriptions, we ask ChatGPT to generate scenes on various themes.
    In addition to this, we also use the textual descriptions from some COCO (Lin
    et al., [2014](#bib.bib26)) and PASCAL (Everingham et al., [2010](#bib.bib8))
    images by querying an image captioning model (Zhu et al., [2023](#bib.bib58))
    to generate a detailed description spanning 80-100 words. For extracting layouts,
    bounding boxes, and background prompt, we make use of ChatGPT completion api (OpenAI,
    [2021](#bib.bib31)) with an appropriate instruction template (See Supplementary
    Sec. [A.4](#A1.SS4 "A.4 Instruct prompts for LLM ‣ Appendix A Appendix ‣ LLM Blueprint:
    Enabling Text-to-Image Generation with Complex and Detailed Prompts")). We generate
    3 layouts for each text prompt and interpolate them to a single layout to account
    for the spatial location correctness. To avoid layout overlap, we push the boxes
    away from each other until there is minimal contact wherever feasible. For base
    layout-to-image generation, we use the work of Lian et al. ([2023](#bib.bib25))
    and scale it for longer text prompts. We use 20 diffusion steps at this point.
    For box refinement, we use the image composition model of Yang et al. ([2023a](#bib.bib48))
    which conditions on a reference image. For each box refinement, we use 50 diffusion
    steps. For implementation, we use Pytorch 2.0\. Finally, our entire pipeline runs
    on a single Nvidia A100 40GB GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantitative Results: Our work stands as a first effort to address the challenge
    of generating images from extensive text prompts. As discussed in Section [1](#S1
    "1 Introduction ‣ LLM Blueprint: Enabling Text-to-Image Generation with Complex
    and Detailed Prompts"), current diffusion-based text-to-image generation methods
    typically utilize the CLIP tokenizer to condition the diffusion model for image
    generation. However, this approach can lead to inconsistent images when confronted
    with lengthy text prompts with intricate details. To the best of our knowledge,
    there is currently no established metric for assessing the performance of diffusion
    models in handling lengthy text descriptions. Hence we propose to use the Prompt
    Adherence Recall (PAR) score to quantify adherence to the prompt defined as Mean
    of object presence over all objects over all prompts where we use an off-the-shelf
    object detector (Li et al., [2022b](#bib.bib23)) to check if the object is actually
    present in the generated image such that object presence is $1$ which is significantly
    better than Stable Diffusion (49%), GLIGEN (57%) and LayoutGPT (69%). We also
    conducted an extensive user study to assess the effectiveness of our method in
    comparison to four established baseline approaches: GLIGEN (Li et al., [2023](#bib.bib24)),
    LayoutGPT (Feng et al., [2023](#bib.bib10)) and LLM-Grounded Diffusion (Lian et al.,
    [2023](#bib.bib25)). To mitigate any bias, participants were instructed to select
    one image from a pair of images randomly selected from two distinct approaches.
    Their goal was to choose the image that most accurately represented the provided
    textual descriptions regarding spatial arrangement, object characteristics, and
    overall scene dynamics. The outcomes of the user study are presented in Fig. [4](#S4.F4
    "Figure 4 ‣ 4 Experiments ‣ LLM Blueprint: Enabling Text-to-Image Generation with
    Complex and Detailed Prompts"). Our findings demonstrate that, on average, our
    proposed method consistently produces coherent images that closely align with
    their respective textual descriptions, whereas existing approaches struggle to
    effectively handle longer text prompts.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/985bf6ccdbba7fee041b3d89586473bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Qualitative comparisons: We compare our image generation method to
    state-of-the-art baselines, including those using layouts. The underlined text
    in the text prompts represents the objects, their characteristics, and spatial
    properties. Red text highlights missing objects, purple signifies inaccuracies
    in object positioning, and black text points out implausible or deformed elements.
    Baseline methods often omit objects and struggle with spatial accuracy (first
    four columns), while our approach excels in capturing all objects and preserving
    spatial attributes (last column).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bbbc8a0f527efbf8b0d5fc11481c5315.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Effect of Layout Interpolation: Our layout interpolation method (last
    column) significantly improves object spatial positioning compared to non-interpolated
    cases (first two columns). Best viewed in zoom.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitative Analysis: Fig. [5](#S4.F5 "Figure 5 ‣ 4 Experiments ‣ LLM Blueprint:
    Enabling Text-to-Image Generation with Complex and Detailed Prompts") presents
    a qualitative assessment of our method in comparison to established state-of-the-art
    methods. The text descriptions include specific phrases denoted by underlined
    italics, conveying information about objects, their attributes, and spatial arrangements.
    Notably, the red text beneath each image highlights instances of missing objects,
    while the purple text indicates spatial inaccuracies, and the black text identifies
    elements of implausibility or distortion. From the figure, the stable diffusion
    baseline (column 1) frequently falls short in incorporating certain objects from
    the prompt due to its inability to efficiently handle lengthy text prompts. In
    some instances (rows 2 and 4), the generated images exhibit unrealistic features.
    Layout-based approaches (columns 2, 3, and 4) also encounter difficulties in fully
    capturing the nuances of the text prompt, resulting in occasional omissions of
    objects and instances of deformations (column 3, row 5). In contrast, our approach
    excels by accurately capturing all objects from the text, including their intricate
    details and precise spatial positions.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Ablations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Effect of Layout Interpolation. We query the LLM to generate multiple layouts
    from the input text prompt and then employ linear interpolation to merge them
    into a single layout. However, for lengthy prompts, LLMs can occasionally generate
    layouts with random object placements, resulting in unnatural images. As shown
    in Fig. [6](#S4.F6.fig1 "Figure 6 ‣ 4 Experiments ‣ LLM Blueprint: Enabling Text-to-Image
    Generation with Complex and Detailed Prompts"), the first two columns depict images
    without layout interpolation, while the last column shows the interpolated image.
    The underlined phrases in the text prompt indicate object spatial characteristics.
    In contrast, the last column demonstrates the improved result with interpolation,
    aligning nearly every object with its textual spatial description.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Effect of Guidance The external guidance in the form of CLIP multi-modal loss
    used in the refinement stage steers sampling of the specific box proposal towards
    its corresponding description and reference prototype. We present a visual illustration
    of this phenomenon in Fig. [7](#S4.F7.fig1 "Figure 7 ‣ 4.1 Ablations ‣ 4 Experiments
    ‣ LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts").
    As seen from the figure, the properties of the cat get more aligned with the prototype
    image and text description in the presence of a guidance signal.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ab5ac311e5a61cf77ca71dbd6a124dd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Effect of Guidance: Without guidance signal, the composed image does
    not follow the properties corresponding to its description and visual appearance.
    In contrast, the one with the guidance (right) adheres to the visual prototype
    and description.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we identified the limitations of prior text-to-image models in
    handling complex and lengthy text prompts. In response, we introduced a framework
    involving a data structure (Scene Blueprint) and a multi-step procedure involving
    global scene generation followed by an iterative refinement scheme to generate
    images that faithfully adhere to the details in such lengthy prompts. Our framework
    offers a promising solution for accurate and diverse image synthesis from complex
    text inputs, bridging a critical gap in text-to-image synthesis capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: While we presented a simple interpolation technique to combine various bounding
    box proposals, we maintained fixed box layouts for the second phase. A promising
    avenue for future research lies in exploring dynamic adjustments of boxes within
    the iterative refinement loop. Another area warranting further examination pertains
    to the handling of overlapping boxes. While we currently address this challenge
    by sorting boxes by size prior to the box-level refinement phase, there is an
    opportunity to explore more advanced techniques for managing overlaps. Additionally,
    our current approach to box-level refinement treats each object in isolation,
    overlooking the relationships that exist among objects within a scene. A compelling
    avenue for future research is to incorporate and leverage these object relationships,
    with the aim of achieving more comprehensive and contextually aware image generation.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein
    generative adversarial networks. In *International conference on machine learning*,
    pp.  214–223\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avrahami et al. (2021) Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended
    diffusion for text-driven editing of natural images. *2022 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, pp.  18187–18197, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brock et al. (2018) Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale
    gan training for high fidelity natural image synthesis. *arXiv preprint arXiv:1809.11096*,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023) Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free
    layout control with cross-attention guidance. *arXiv preprint arXiv:2304.03373*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhariwal & Nichol (2021) Prafulla Dhariwal and Alexander Nichol. Diffusion models
    beat gans on image synthesis. *Advances in neural information processing systems*,
    34:8780–8794, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ding et al. (2021) Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou,
    Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering
    text-to-image generation via transformers. *Advances in Neural Information Processing
    Systems*, 34:19822–19835, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Epstein et al. (2023) Dave Epstein, Allan Jabri, Ben Poole, Alexei A Efros,
    and Aleksander Holynski. Diffusion self-guidance for controllable image generation.
    *arXiv preprint arXiv:2306.00986*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Everingham et al. (2010) M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn,
    and A. Zisserman. The pascal visual object classes (voc) challenge. *International
    Journal of Computer Vision*, 88(2):303–338, June 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2023) Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu Cheng, Lu Yuan,
    and Yu-Chiang Frank Wang. Frido: Feature pyramid diffusion for complex scene image
    synthesis. In *Proceedings of the AAAI Conference on Artificial Intelligence*,
    volume 37, pp.  579–587, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2023) Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun
    Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt:
    Compositional visual planning and generation with large language models. *arXiv
    preprint arXiv:2305.15393*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gafni et al. (2022) Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi
    Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation
    with human priors. In *European Conference on Computer Vision*, pp.  89–106\.
    Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative
    adversarial nets. *Advances in neural information processing systems*, 27, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2022) Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong
    Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image
    synthesis. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pp.  10696–10706, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulrajani et al. (2017) Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent
    Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. *Advances
    in neural information processing systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hessel et al. (2021) Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,
    and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning.
    *arXiv preprint arXiv:2104.08718*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho & Salimans (2022) Jonathan Ho and Tim Salimans. Classifier-free diffusion
    guidance. *arXiv preprint arXiv:2207.12598*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ho et al. (2020) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion
    probabilistic models. *Advances in neural information processing systems*, 33:6840–6851,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kang et al. (2023) Wonjun Kang, Kevin Galim, and Hyung Il Koo. Counting guidance
    for high fidelity text-to-image synthesis. *arXiv preprint arXiv:2306.17567*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karras et al. (2019) Tero Karras, Samuli Laine, and Timo Aila. A style-based
    generator architecture for generative adversarial networks. In *Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition*, pp.  4401–4410,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kim et al. (2022) Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip:
    Text-guided diffusion models for robust image manipulation. In *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  2426–2435,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kodali et al. (2017) Naveen Kodali, Jacob Abernethy, James Hays, and Zsolt Kira.
    On convergence and stability of gans. *arXiv preprint arXiv:1705.07215*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022a) Boyi Li, Rodolfo Corona, Karttikeya Mangalam, Catherine Chen,
    Daniel Flaherty, Serge Belongie, Kilian Q. Weinberger, Jitendra Malik, Trevor
    Darrell, and Dan Klein. Does unsupervised grammar induction need pixels? *arXiv
    preprint arXiv:2212.10564*, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022b) Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei
    Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang,
    Kai-Wei Chang, and Jianfeng Gao. Grounded language-image pre-training, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023) Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei
    Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image
    generation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pp.  22511–22521, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lian et al. (2023) Long Lian, Boyi Li, Adam Yala, and Trevor Darrell. Llm-grounded
    diffusion: Enhancing prompt understanding of text-to-image diffusion models with
    large language models. *arXiv preprint arXiv:2305.13655*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2014) Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *Computer Vision–ECCV 2014: 13th European Conference,
    Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13*, pp.  740–755\.
    Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Xingchao Liu, Dahun Hwang Park, Samaneh Azadi, Guandao Zhang,
    Armen Chopikyan, Yizhe Hu, …, and Trevor Darrell. More control for free! image
    synthesis with semantic diffusion guidance. In *Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pp.  289–299, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023) Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based
    training-free cross-domain image composition. In *Proceedings of the IEEE/CVF
    International Conference on Computer Vision*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2023) Yun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie Zhou, and Yue
    Zhang. An empirical study of catastrophic forgetting in large language models
    during continual fine-tuning. *arXiv preprint arXiv:2308.08747*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nichol et al. (2021) Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
    Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards
    photorealistic image generation and editing with text-guided diffusion models.
    *arXiv preprint arXiv:2112.10741*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2021) OpenAI. Chatgpt: A large-scale generative model for conversations.
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2019) Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu.
    Semantic image synthesis with spatially-adaptive normalization. In *Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition*, pp.  2337–2346,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phung et al. (2023) Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image
    synthesis with attention refocusing. *arXiv preprint arXiv:2306.05427*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pp.  8748–8763\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2021) Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
    Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image
    generation. In *International Conference on Machine Learning*, pp.  8821–8831\.
    PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ramesh et al. (2022) Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
    and Mark Chen. Hierarchical text-conditional image generation with clip latents.
    *arXiv preprint arXiv:2204.06125*, 1(2):3, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed et al. (2016) Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran,
    Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis.
    In *International conference on machine learning*, pp.  1060–1069\. PMLR, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rombach et al. (2022) Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick
    Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion
    models. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, pp.  10684–10695, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ruiz et al. (2023) Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
    Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion
    models for subject-driven generation. In *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pp.  22500–22510, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saharia et al. (2022) Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,
    Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan,
    Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language
    understanding. *Advances in Neural Information Processing Systems*, 35:36479–36494,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020a) Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising
    diffusion implicit models. *arXiv preprint arXiv:2010.02502*, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song & Ermon (2019) Yang Song and Stefano Ermon. Generative modeling by estimating
    gradients of the data distribution. *Advances in neural information processing
    systems*, 32, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020b) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek
    Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic
    differential equations. *arXiv preprint arXiv:2011.13456*, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun & Wu (2019) Wei Sun and Tianfu Wu. Image synthesis from reconfigurable layout
    and style. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, pp.  10531–10540, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sylvain et al. (2021) Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon
    Hjelm, and Shikhar Sharma. Object-centric image generation from layouts. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 35, pp.  2647–2655,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. (2022) Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao,
    and Changsheng Xu. Df-gan: A simple and effective baseline for text-to-image synthesis.
    In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pp.  16515–16525, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2018) Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan,
    Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation
    with attentional generative adversarial networks. In *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, pp.  1316–1324, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023a) Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen,
    Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing
    with diffusion models. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp.  18381–18391, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023b) Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin
    Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled
    text-to-image generation. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp.  14246–14255, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022) Zuopeng Yang, Daqing Liu, Chaoyue Wang, Jie Yang, and Dacheng
    Tao. Modeling image composition for complex scene generation. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pp.  7764–7773,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2022) Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid,
    Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al.
    Scaling autoregressive models for content-rich text-to-image generation. *arXiv
    preprint arXiv:2206.10789*, 2(3):5, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2017) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. Stackgan: Text to photo-realistic
    image synthesis with stacked generative adversarial networks. In *Proceedings
    of the IEEE international conference on computer vision*, pp.  5907–5915, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2018a) Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang
    Wang, Xiaolei Huang, and Dimitris N Metaxas. Stackgan++: Realistic image synthesis
    with stacked generative adversarial networks. *IEEE transactions on pattern analysis
    and machine intelligence*, 41(8):1947–1962, 2018a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021) Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and
    Yinfei Yang. Cross-modal contrastive learning for text-to-image generation. In
    *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pp.  833–842, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018b) Richard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman,
    and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual
    metric, 2018b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2019) Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. Image
    generation from layout. In *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pp.  8584–8593, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2023) Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi,
    Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion model for layout-to-image
    generation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*, pp.  22490–22499, June 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed
    Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large
    language models. *arXiv preprint arXiv:2304.10592*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Sec. [3](#S3 "3 Methodology ‣ LLM Blueprint: Enabling Text-to-Image Generation
    with Complex and Detailed Prompts"), we describe our proposed approach LLM Blueprint.
    Our method uses an LLM to first generate bounding box layouts and descriptions
    pertaining to each object in the description, along with a background prompt that
    describes the overall scene in the text. We use specific instruct prompts to do
    so (see Appendix.[A.4](#A1.SS4 "A.4 Instruct prompts for LLM ‣ Appendix A Appendix
    ‣ LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts")).
    Due to possibly inconsistent box positions, we query the LLM to generate $k$ and
    source image $x_{initial}$ to avoid the composition process affecting the background.
    We provide a pseudo code of our algorithm in Algorithm [1](#alg1 "Algorithm 1
    ‣ A.1 Algorithm ‣ Appendix A Appendix ‣ LLM Blueprint: Enabling Text-to-Image
    Generation with Complex and Detailed Prompts").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: long textual description $\mathcal{C}$// Optional noise correction.  $x_{initial}\leftarrow
    SD_{image-image}(x_{initial})$  end for  return  $x_{0}$'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 LLM Blueprint
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Noise and artifact correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As discussed in Sec. [3.3](#S3.SS3 "3.3 Global Scene Generation ‣ 3 Methodology
    ‣ LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts"),
    the generated image after the first phase sometimes suffers from noise or artifacts.
    We do an optional noise correction step after Global scene generation, which essentially
    removes the unwanted noise and artifacts. We accomplish this by passing the image
    to a diffusion model which preserves the structure and semantics of the scene
    and produces a cleaner image (See Figure [9](#A1.F9 "Figure 9 ‣ A.3 Further Qualitative
    Comparisons ‣ Appendix A Appendix ‣ LLM Blueprint: Enabling Text-to-Image Generation
    with Complex and Detailed Prompts")).'
  prefs: []
  type: TYPE_NORMAL
- en: A.3 Further Qualitative Comparisons
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We present additional qualitative comparisons of our method with the latest
    benchmark techniques in Figure [8](#A1.F8 "Figure 8 ‣ A.3 Further Qualitative
    Comparisons ‣ Appendix A Appendix ‣ LLM Blueprint: Enabling Text-to-Image Generation
    with Complex and Detailed Prompts"). The baseline methods (rows 1-4) struggle
    to capture the intricate details of lengthy prompts, leading to instances where
    they miss certain objects highlighted in red text beneath each image. On occasion,
    these baseline approaches generate distorted objects or objects that are not mentioned
    in the text, as indicated by the black text under the corresponding images. Furthermore,
    baseline models occasionally exhibit spatial inaccuracies, which results in their
    inability to accurately represent the precise locations of objects, as highlighted
    in pink text beneath the respective images. We also showcase instances where our
    approach encounters difficulties in generating one or more objects mentioned in
    the text. For example, in the first row and fifth column, our method was unable
    to produce an image of a tree. Likewise, in the fifth row and fifth column, our
    approach overlooks generating the picture frame hanging on the wall. Note that
    we start the column count from Stable diffusion baseline.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6779d5e194c8069c2b8740f3a81f920b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Qualitative comparisons: We provide further qualitative comparisons
    to our approach against the state-of-the-art baselines. The underlined text in
    the text prompts represents the objects, their characteristics, and spatial properties.
    Baseline methods often omit objects and struggle with spatial accuracy (first
    four columns), while our approach excels in capturing all objects and preserving
    spatial attributes (last column).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/44931a629f2b66eb13d52b36db03343b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Noise Correction: The noise correction strategy removes noise and
    certain redundant artifacts from the image that are unnecessary.'
  prefs: []
  type: TYPE_NORMAL
- en: A.4 Instruct prompts for LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our approach utilizes LLMs’ advanced spatial and reasoning capabilities to derive
    layouts, object descriptions, and background prompt from a long textual description.
    For extracting layouts and background prompts, we use the prompt designed by Lian
    et al. ([2023](#bib.bib25)) and scale it to work on longer textual descriptions.
    For extracting object descriptions, we designed our own unique prompt. Below are
    the instruct prompts utilized in our work.
  prefs: []
  type: TYPE_NORMAL
- en: Instruct prompt for extracting layouts and background prompt.
  prefs: []
  type: TYPE_NORMAL
- en: You are an intelligent bounding box generator. I will provide you with a caption
    for a photo, image, a detailed scene, or a painting. Your task is to generate
    the bounding boxes for the objects mentioned in the caption, along with a background
    prompt describing the scene. The images are of size 512x512\. The top-left corner
    has coordinates [0, 0]. The bottom-right corner has coordinates [512, 512]. The
    bounding boxes should not overlap or go beyond the image boundaries. Each bounding
    box should be in the format of (object name, [top-left x coordinate, top-left
    y coordinate, box width, box height]) and include exactly one object (i.e., start
    the object name with "a" or "an" if possible). Do not put objects that are already
    provided in the bounding boxes into the background prompt. Do not include non-existing
    or excluded objects in the background prompt. If needed, you can make reasonable
    guesses. Please refer to the example below for the desired format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Caption: In the quiet countryside, a red farmhouse stands with an old-fashioned
    charm. Nearby, a weathered picket fence surrounds a garden of wildflowers. An
    antique tractor, though worn, rests as a reminder of hard work. A scarecrow watches
    over fields of swaying crops. The air carries the scent of earth and hay. Set
    against rolling hills, this farmhouse tells a story of connection to the land
    and its traditions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objects: [(’a red farmhouse’, [105, 228, 302, 245]), (’a weathered picket fence’,
    [4, 385, 504, 112]), (’an antique tractor’, [28, 382, 157, 72]), (’a scarecrow’,
    [368, 271, 66, 156]) ]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Background prompt: A realistic image of a quiet countryside with rolling hills'
  prefs: []
  type: TYPE_NORMAL
- en: 'Caption: A realistic image of landscape scene depicting a green car parking
    on the left of a blue truck, with a red air balloon and a bird in the sky'
  prefs: []
  type: TYPE_NORMAL
- en: 'Objects: [(’a green car’, [21, 181, 211, 159]), (’a blue truck’, [269, 181,
    209, 160]), (’a red air balloon’, [66, 8, 145, 135]), (’a bird’, [296, 42, 143,
    100])]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Background prompt: A realistic image of a landscape scene'
  prefs: []
  type: TYPE_NORMAL
- en: Instruct prompt for extracting object descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are an intelligent description extractor. I will give you a list of the
    objects and a corresponding text prompt. For each object, extract its respective
    description or details mentioned in the text prompt. The description should strictly
    contain fine details about the object and must not contain information regarding
    location or abstract details about the object. The description must also contain
    the name of the object being described. For objects that do not have concrete
    descriptions mentioned, return the object itself in that case. The output should
    be a Python dictionary with the key as object and the value as description. The
    description should start with ’A realistic photo of object’ followed by its characteristics.
    Sort the entries as per objects that are spatially behind (background) followed
    by objects that are spatially ahead (foreground). For instance object "a garden
    view" should precede the "table". Make an intelligent guess if possible. Here
    are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'list of objects: [a Golden Retriever,a white cat,a wooden table,a vase of vibrant
    flowers,a sleek modern television]'
  prefs: []
  type: TYPE_NORMAL
- en: 'text prompt: In a cozy living room, a heartwarming scene unfolds. A friendly
    and affectionate Golden Retriever with a soft, golden-furred coat rests contently
    on a plush rug, its warm eyes filled with joy. Nearby, a graceful and elegant
    white cat stretches leisurely, showcasing its pristine and fluffy fur. A sturdy
    wooden table with polished edges stands gracefully in the center, adorned with
    a vase of vibrant flowers adding a touch of freshness. On the wall, a sleek modern
    television stands ready to provide entertainment. The ambiance is warm, inviting
    and filled with a sense of companionship and relaxation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'output: {a sleek modern television: A realistic photo of a sleek modern television.,
    a wooden table: A realistic photo of a sturdy wooden table with polished edges.,
    vase of vibrant flowers: A realistic photo of a vase of vibrant flowers adding
    a touch of freshness., a Golden Retriever: ’A realistic photo of a friendly and
    affectionate Golden Retriever with a soft, golden-furred coat and its warm eyes
    filled with joy., a white cat: ’A realistic photo of a graceful and elegant white
    cat stretches leisurely, showcasing its pristine and fluffy fur.}'
  prefs: []
  type: TYPE_NORMAL
