- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:23'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based Chip
    Design'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.18276](https://ar5iv.labs.arxiv.org/html/2407.18276)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Andre Nakkab New York University [andre.nakkab@nyu.edu](mailto:andre.nakkab@nyu.edu)
    [0009-0006-1345-5444](https://orcid.org/0009-0006-1345-5444 "ORCID identifier")
    ,  Sai Qian Zhang New York University [sai.zhang@nyu.edu](mailto:sai.zhang@nyu.edu)
    ,  Ramesh Karri New York University [rkarri@nyu.edu](mailto:rkarri@nyu.edu)  and 
    Siddharth Garg New York University [siddharth.garg@nyu.edu](mailto:siddharth.garg@nyu.edu)(2024)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Large Language Models (LLMs) are effective in computer hardware synthesis via
    hardware description language (HDL) generation. However, LLM-assisted approaches
    for HDL generation struggle when handling complex tasks. We introduce a suite
    of hierarchical prompting techniques which facilitate efficient stepwise design
    methods, and develop a generalizable automation pipeline for the process. To evaluate
    these techniques, we present a benchmark set of hardware designs which have solutions
    with or without architectural hierarchy. Using these benchmarks, we compare various
    open-source and proprietary LLMs, including our own fine-tuned Code Llama-Verilog
    model. Our hierarchical methods automatically produce successful designs for complex
    hardware modules that standard flat prompting methods cannot achieve, allowing
    smaller open-source LLMs to compete with large proprietary models. Hierarchical
    prompting reduces HDL generation time and yields savings on LLM costs. Our experiments
    detail which LLMs are capable of which applications, and how to apply hierarchical
    methods in various modes. We explore case studies of generating complex cores
    using automatic scripted hierarchical prompts, including the first-ever LLM-designed
    processor with no human feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM, Hardware design, Hierarchy, Automation^†^†copyright: acmlicensed^†^†journalyear:
    2024^†^†conference: MLCAD; Sept. 9-11, 2024; Snowbird, UT'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hierarchical design is a key concept for creating complex computer hardware
    in an organized fashion. The goal of hierarchy is to break complex modules into
    manageable submodules, the way one might define a function in high-level code.
    However, recent efforts into LLM-based generation of hardware description language
    (HDL) code  (Thakur et al., [2023b](#bib.bib19); Liu et al., [2024a](#bib.bib12);
    Thakur et al., [2023c](#bib.bib20)) generate modules non-hierarchically, i.e.,
    as single blocks of straight-line code. Although these methods succeed on simple
    designs like bit-parallel adders and shift registers  (Thakur et al., [2023a](#bib.bib18)),
    they struggle on complex designs in recent benchmarks, such as finite-state machines
    (FSM), large-scale many-to-1 multiplexers, and larger arithmetic blocks  (Liu
    et al., [2023](#bib.bib11)). Since straight-line code blocks for complex designs
    are longer than the hierarchical alternatives, they may hallucinate (Liu et al.,
    [2024b](#bib.bib10)); the LLM generates incorrect or unrelated text. Additionally,
    long outputs increase response latency and sometimes fail due to output length
    limits.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we develop and evaluate *hierarchical prompting* techniques to
    facilitate automated generation of *modular* HDL code. We explore hierarchical
    Verilog generation in two major modalities, each occurring in the real-world.
    In the *human-driven* mode, the prompt contains a human-proposed hierarchy that
    the LLM must extract and implement, as well as iterative compiler feedback from
    unit tests for each submodule. In the more challenging *purely generative* mode,
    the LLM gets only a basic (non-hierarchical) prompt and therefore must make its
    own design decisions to implement the target module. We implement an 8-stage pipeline
    to automate these techniques in a generalizable fashion. This allows an LLM to
    closely emulate human HDL development practices.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5db3dcbc149e97d52bc8987be57da7c3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1. Automatic Hierarchical Prompting Pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing benchmark suites like VerilogEval (Liu et al., [2023](#bib.bib11))
    and RTLLM  (Lu et al., [2023](#bib.bib13)) do not address hierarchy. We introduce
    a new benchmark suite of complex modules with explicit hierarchical solutions,
    including associated prompts and testbenches for both the top-level modules, and
    unit tests for submodules. The target modules in our benchmark pose unique challenges
    to LLMs. These include a 32-bit data-dependent left rotation (also known as a
    barrel shifter) that requires rarely-seen syntax which is difficult to generate
    from scratch; an Advanced Encryption Standard (AES) block cipher which has multiple
    complex submodules that are difficult to organize within a single large Verilog
    script; and, a universal asynchronous receiver and transmitter (UART) interface
    which requires a multi-part FSM to function. These designs could not be implemented
    at all by any of the open-source or commercial LLMs we tried in non-hierarchical
    mode, motivating the need for advanced hierarchical prompting methods. The full
    list of benchmarks are in Table [5](#A0.T5 "Table 5 ‣ Rome was Not Built in a
    Single Step: Hierarchical Prompting for LLM-based Chip Design") in the Appendix.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluations on 8 state-of-art LLMs demonstrate that hierarchical prompt structuring
    dramatically improves LLM performance on hardware design tasks, enabling successful
    generation of modules that would otherwise be impossible. Finally, we report case
    studies on the generation of complex hardware modules outside of the context of
    the hierarchical prompting benchmarks. We target a 16-bit MIPS processor and a
    32-bit RISC-V processor, and present the first-ever purely LLM-designed processor
    with no human feedback.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. An Introduction of LLM Operation for Text Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transformer-based deep neural networks (DNN) have enabled advances across a
    wide range of domains, excelling in language-related tasks. LLMs operate by processing
    text inputs structured as tokens, When presented with a sequence of input tokens,
    LLMs output a probability distribution spanning the complete vocabulary to predict
    next token in the sequence. This process repeats until a full sequence of tokens,
    referred to as a completion, is produced. Consider an LLM $P_{\phi}(\vec{y}|\vec{x})$
    is produced autoregressively:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $y_{n}=\operatorname*{arg\,max}_{v\in V}P_{\phi}(v&#124;\vec{x},y_{Our goal is to provide complete
    Verilog modules based on user-provided specifications. Only the specified module
    is necessary, no testbenches or supplementary modules are needed. Examples of
    compiler and simulation errors for a given module may be provided, in which case
    we will proceed to correct the module. All modules will be provided in their complete
    and correct form.include  prev_submods.v  #contains  previously  generated  Verilog  submodulestop_module  =  "64-to-1  multiplexer"prev_modules  =  ["2-to-1  multiplexer","4-to-1  multiplexer"]next_module  =  "8-to-1  multiplexer"next_io  =  "mux8_1(input  [2:0]  sel,  input  [7:0]  in,  output  reg  out)"prompt  =  "We  will  be  designing  a    in  Verilog  using  hierarchical  submodules.  We  have  generated  the  following  submodules:    implemented  as:Please  use  the  previous  submodules  to  hierarchically  generate  a    defined  as:module  ;//Insert  code  hereendmodule"next_output  =  Feedback_Loop(prompt)append  next_output  to  prev_submods.vMove  to  next  step  in  hierarchy'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Pseudocode for automatic creation of a hierarchical prompt from a template
    which references previously generated hierarchy steps in the form of Verilog modules,
    as well as a call to the generative loop which creates the output module.
  prefs: []
  type: TYPE_NORMAL
- en: 'module  mux2_1(input  in1,  input  in2,  input  select,  output  out);assign  out  =  select  ?  in2  :  in1;endmodulemodule  mux4_1(input  [1:0]  sel,  input  [3:0]  in,  output  out);wire  out1,  out2;//  First  level  of  multiplexermux2_1  m1(.in1(in[0]),  .in2(in[1]),  .select(sel[0]),  .out(out1));mux2_1  m2(.in1(in[2]),  .in2(in[3]),  .select(sel[0]),  .out(out2));//  Second  level  of  multiplexermux2_1  m3(.in1(out1),  .in2(out2),  .select(sel[1]),  .out(out));endmodule'
  prefs: []
  type: TYPE_NORMAL
- en: (b) The LLM-generated hierarchical Verilog module, *prev_submods.v*, which the
    above pseudocode includes as part of the prompt. Note that it contains all prior
    hierarchical modules, i.e., *mux2_1* and *mux4_1*.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2. Structure of a hierarchical step, which uses automated prompting and
    existing hierarchy to generate a new module.
  prefs: []
  type: TYPE_NORMAL
- en: We then use a benchmark-specific global prompt describing the overall design
    objective, akin to non-hierarchical approaches and, in the HDHP mode, we iteratively
    append a submodule prompt, one for each submodule in the list provided by the
    designer. Therefore, the first prompt provided to the LLM for a 64-to-1 multiplexer
    designer looks as below, where the top-level prompt is in blue and the first submodule
    prompt is in green. Note that for each submodule, we only provide its name and
    the submodule interface to the LLM.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be designing a 64-to-1
    multiplexer in Verilog using hierarchical submodules. We begin by generating a
    2-to-1 multiplexer with the following structure: module mux2_1(in1, in2, select,
    out)'
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of automatic hierarchical prompt creation can be seen in Figure
    [2](#S3.F2 "Figure 2 ‣ 3.3\. Prompting Structure and Techniques ‣ 3\. Hierarchical
    Prompting ‣ Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based
    Chip Design"), with Figure [2(a)](#S3.F2.sf1 "In Figure 2 ‣ 3.3\. Prompting Structure
    and Techniques ‣ 3\. Hierarchical Prompting ‣ Rome was Not Built in a Single Step:
    Hierarchical Prompting for LLM-based Chip Design") showing the pseudocode instantiating
    the prompt template and Figure [2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3.3\. Prompting
    Structure and Techniques ‣ 3\. Hierarchical Prompting ‣ Rome was Not Built in
    a Single Step: Hierarchical Prompting for LLM-based Chip Design") representing
    the file containing the modules generated so far.'
  prefs: []
  type: TYPE_NORMAL
- en: A subtlety in prompting is the distinction between conversational and non-conversational
    (or text completion) LLMs. In conversational LLMs, prior prompts and responses
    are automatically added to the LLM’s context. For conversational LLMs, our hierarchical
    prompting approach starts with the global prompt that describes the top-level
    module and the first submodule. In subsequent iterations, we can simply request
    the next submodule since previously generated submodules are implicitly remembered
    in the LLM’s context.
  prefs: []
  type: TYPE_NORMAL
- en: 'For text completion LLMs, however, we have to strategically insert responses
    from prior steps. We begin with the same global prompt as above that describes
    the top-level design and asks for the first submodule. For models with large context
    windows, we could simply include the code of all the previous submodules within
    the prompt as we progress. For many open-source LLMs with shorter context windows,
    however, we must implement what we call the *relay prompt* technique as we progress
    deeper into the hierarchy, as shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.3\. Prompting
    Structure and Techniques ‣ 3\. Hierarchical Prompting ‣ Rome was Not Built in
    a Single Step: Hierarchical Prompting for LLM-based Chip Design").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, for a given step, we include the full code of the submodule generated
    immediately prior to the current step, but only provide the module instantiation
    line for earlier steps. This becomes a list of all prior submodules that the LLM
    has access to, always including the complete elaboration of the most recently
    generated submodule. This allows “leap-frogging” from less complex modules to
    more complex ones without running into length limits. Along with this prior context,
    we ask for the next submodule in the design hierarchy. For example, the final
    text-completion prompt for the decoder hierarchy can be seen in Figure [3(b)](#S3.F3.sf2
    "In Figure 3 ‣ 3.3\. Prompting Structure and Techniques ‣ 3\. Hierarchical Prompting
    ‣ Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based Chip
    Design"). The full code for the 3-to-8 decoder is included, in addition to the
    module instantiation for the 2-to-4 decoder. This is sufficient information to
    improve performance. Though we give the example of the decoder for simplicity,
    relay prompting is an efficiency tool which becomes much more important for large
    hierarchies, like the 128-bit AES cipher which consists of hundreds of lines of
    code. Most open-source LLMs are text-completion models, and this prompting method
    allows them to leverage hierarchy the same way a conversational LLM like GPT-4
    would. All of our benchmarks use relay prompts by default for standardization
    and efficiency, though it is possible to include full prior submodule information
    at each step for smaller-scale hierarchies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We  will  be  designing  a  5-to-32  decoder  in  Verilog  using  hierarchical  submodules.  We  begin  by  generating  a  2-to-4  decoder:module  decoder2to4()endmoduleWe  can  then  use  that  module  hierarchically  to  generate  a  3-to-8  decoder:module  decoder3to8('
  prefs: []
  type: TYPE_NORMAL
- en: (a) Initial prompt for 2-to-4 decoder and follow-up prompt for 3-to-8 decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The  following  Verilog  implements  a  5-to-35  decoder  utilizing  hierarchical  submodules.  We  have  the  following  module(s)  already  available  for  use:module  decoder2to4()We  can  then  use  that  module  hierarchically  to  generate  a  3-to-8  decoder:module  decoder3to8()endmoduleWe  can  then  use  these  modules  to  hierarchically  generate  a  5-to-32  decoder:module  decoder5to32('
  prefs: []
  type: TYPE_NORMAL
- en: (b) Final prompt for 5-to-32 decoder with compressed version of 2-to-4 decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3. Example of Hierarchical Verilog Decoder Implementation using text-completion
    LLM. Model outputs are in blue.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Methods and Model Selection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We selected eight relevant LLMs that are at the cutting edge of the field.
    Table [1](#S5.T1 "Table 1 ‣ 5\. Results and Evaluation ‣ Rome was Not Built in
    a Single Step: Hierarchical Prompting for LLM-based Chip Design") shows the full
    list of models selected and their results on each of the module benchmarks. Of
    those eight, six are open-source. For those open-source models, all benchmarking
    inference was run on a single NVIDIA A100 80GB GPU. Inference for the GPT models
    was run using the OpenAI API. We lock two important model parameters during inference:
    temperature, which is commonly thought of as a “creativity” value and determines
    how random the LLMs token generation is, and top-p, which sets a probability threshold
    for generated tokens, allowing only those tokens above the threshold to be selected
    from the probability distribution. Higher temperature and lower top-p lead to
    more random outputs, and vice-versa. Temperature values for each model were locked
    at 0.5, and top-p for each model was set at 0.9.'
  prefs: []
  type: TYPE_NORMAL
- en: We chose to test the unspecialized Llama 2 (Touvron et al., [2023](#bib.bib21)),
    and the more recent Llama 3 (AI@Meta, [2024](#bib.bib2)) models to see how generalist
    open-source LLMs compare to specialized models. We hypothesized that even these
    models would see considerable performance improvements via hierarchical prompting.
    As a more specialized option, we include the Code Llama (Rozière et al., [2023](#bib.bib17))
    model which is fine-tuned on code. It is effective at HDL generation despite not
    being its intended purpose. We test a pair of LLMs fine-tuned for Verilog generation,
    namely VeriGen 16b (Thakur et al., [2023b](#bib.bib19)) and RTL-Coder (Liu et al.,
    [2024a](#bib.bib12)).We elected not to use the baseline models for each of these,
    as generalist Llama models of similar size are included in our list. Finally,
    we fine-tuned our own Code Llama-Verilog model to make the already competitive
    baseline Code Llama more effective. Our open-source contenders were compared against
    GPT-3.5 Turbo, GPT-4 black-box LLMs (OpenAI, [2023](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Results and Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We ran each benchmark using the selected LLMs for 10 iterations per model-method-module
    combination to get a more statistical sense of how each model performs. The NH
    experiments still utilized the tool feedback loop for error fixing, but had no
    hierarchy applied to prompting. Ten iterations per module were allowed. We tracked
    both the *pass@k* values and the wall-clock time it took to generate the outputs.
    The *pass@k* metric is defined as the likelihood that one or more of the top-*k*
    LLM-generated modules will pass the testbench  (Chen et al., [2021](#bib.bib5)).
    Mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $pass@k=1-\begin{bmatrix}\frac{{n-c\choose k}}{{n\choose k}}\end{bmatrix}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '*n* is the number of generation attempts, *c* is the number of correct attempts
    that pass testing, and *k* is success threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Comp. Results | LLM Used | Llama 2 | Code Llama | VeriGen | CL-Verilog |
    RTL-Coder | Llama 3 | GPT-3.5 | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Parameters | 13b | 13b | 16b | 13b | 7b | 8b | ~ 20b* | ~ 1.8t* |'
  prefs: []
  type: TYPE_TB
- en: '|  | Open/Closed | Open | Open | Open | Open | Open | Open | Closed | Closed
    |'
  prefs: []
  type: TYPE_TB
- en: '| Module benchmark |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Prompt method &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Results &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| NH | H | NH | H | NH | H | NH | H | NH | H | NH | H | NH | H | NH | H |'
  prefs: []
  type: TYPE_TB
- en: '| 64-to-1 Multiplexer |'
  prefs: []
  type: TYPE_TB
- en: '&#124; pass@1: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@5: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.976 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.3 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.916 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.976 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.78 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.9 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Avg. Time (s): | 621.54 | 342.63 | 634.01 | 302.65 | 642.11 | 310.64 | 573.21
    | 315.28 | 407.88 | 345.72 | 521.04 | 306.27 | 606.43 | 327.34 | 1325.69 | 507.54
    |'
  prefs: []
  type: TYPE_TB
- en: '| 5-to-32 Decoder |'
  prefs: []
  type: TYPE_TB
- en: '&#124; pass@1: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@5: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.996 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.9 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.78 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.976 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Avg. Time (s): | 567.85 | 274.33 | 543.24 | 310.41 | 577.98 | 333.83 | 566.48
    | 329.65 | 475.2 | 361.73 | 488.92 | 301.97 | 532.41 | 215.33 | 829.26 | 379.65
    |'
  prefs: []
  type: TYPE_TB
- en: '| 32-bit Barrel Shifter |'
  prefs: []
  type: TYPE_TB
- en: '&#124; pass@1: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@5: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.78 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.3 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.916 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.976 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.3 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.917 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Avg. Time (s): | 322.47 | 79.51 | 301.68 | 51.23 | 401.65 | 43.7 | 296.54
    | 35.21 | 256.32 | 61.23 | 309.22 | 29.64 | 312.08 | 18.27 | 450.66 | 42.44 |'
  prefs: []
  type: TYPE_TB
- en: '| 4x4 Systolic Array |'
  prefs: []
  type: TYPE_TB
- en: '&#124; pass@1: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@5: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.78 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.996 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.996 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.996 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Avg. Time (s): | 1358.99 | 456.22 | 1312.05 | 422.45 | 1472.91 | 467.38 |
    1342.15 | 481.27 | 1021.44 | 503.24 | 1101.77 | 325.18 | 1276.39 | 297.63 | 2452.41
    | 402.26 |'
  prefs: []
  type: TYPE_TB
- en: '| UART 8-bit |'
  prefs: []
  type: TYPE_TB
- en: '&#124; pass@1: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@5: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.78 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.976 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.976 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.6 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.78 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.976 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.8 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Avg. Time (s): | 2482.17 | 672.9 | 2100.58 | 614.07 | 2603.12 | 682 | 2529.74
    | 673.21 | 1763.06 | 699.53 | 1754.22 | 573.64 | 1800.45 | 564.23 | 3144.22 |
    752.76 |'
  prefs: []
  type: TYPE_TB
- en: '| AES Block Cipher |'
  prefs: []
  type: TYPE_TB
- en: '&#124; pass@1: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pass@5: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.2 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.78 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.3 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.916 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.3 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.916 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.0 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.996 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Avg. Time (s): | 3212 | 783.54 | 3201.78 | 795.4 | 3456.11 | 809.54 | 3203.55
    | 732.14 | 2387.92 | 960.29 | 2603.51 | 706.59 | 2652.42 | 694.71 | 3822.64 |
    806.89 |'
  prefs: []
  type: TYPE_TB
- en: Table 1. Results on hierarchical design benchmarks for open-source and closed-source
    proprietary LLMs. The results for each model are separated by prompting method
    — (i) flat, non-hierarchical (NH) or hierarchical (H) . We report *pass@k* for
    $n=10$ attempts, and the average generation time per benchmark. Top open-source
    hierarchical performers are in green, and top non-hierarchical performers are
    in blue; generation time is the tie-breaker where needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5\. Results and Evaluation ‣ Rome was Not Built
    in a Single Step: Hierarchical Prompting for LLM-based Chip Design") shows the
    *pass@k* values for each model-method-module permutation on the benchmark. Hierarchical
    prompting boosts performance on complex designs, and especially so on on weaker
    models that fail completely with standard NH prompting. Consider the performance
    of Llama-2, which is not intended for code generation, much less HDL. Without
    hierarchical prompting, it fails at every task on the benchmark as it often cannot
    generate Verilog syntax. It fails on simpler submodules like 8-to-1 multiplexers.
    However, when guided hierarchically, it has the potential to succeed at even some
    complicated tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: When applied to specialized models, the results are even more impressive. Hierarchical
    prompting enables open-source LLMs to outperform standard flat prompting outputs
    from GPT-3.5 and GPT-4\. Furthermore, these techniques on the GPT models yield
    further performance improvement, allowing GPT-3.5 and GPT-4 them to succeed consistently
    on difficult modules. Overall, every LLM sees improvement with hierarchical prompting.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08e9b71f955ea055f47f03aa9d792423.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4. Hierarchical prompting yields consistent time savings vs. flat prompting,
    as seen by average % latency reduction. More time savings are seen on modules
    which are difficult or impossible to generate non-hierarchically, or on those
    for which flat outputs are longer than hierarchical alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hiearchical Prompting also reduces code generation time. Figure [4](#S5.F4
    "Figure 4 ‣ 5\. Results and Evaluation ‣ Rome was Not Built in a Single Step:
    Hierarchical Prompting for LLM-based Chip Design") reports the average percent
    time reduction due to hierarchical prompting across all LLMs for each of our benchmark.
    We see that the more difficult modules tend to have a much greater reduction in
    time, as successful hierarchical generation allows us to skip the lengthy error-handling
    process which sees the models re-generating past outputs and accounts for the
    majority of generation time.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Purely Generative Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We implemented PGHP techniques for our benchmarks, but saw consistent failure
    for models *except for* GPT-4\. To diagnose the source, we selected 3 of the simplest
    benchmarks, and used a subset of our LLMs to generate 20 hierarchies each and
    evaluated them against the golden hierarchy plan from the HDHP version. Most LLMs
    performed inconsistently on this task, missing key submodules or inserting extraneous
    submodules (Table [2](#S5.T2 "Table 2 ‣ 5.1\. Purely Generative Results ‣ 5\.
    Results and Evaluation ‣ Rome was Not Built in a Single Step: Hierarchical Prompting
    for LLM-based Chip Design")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, we find that GPT-4 significantly improves over flat prompting
    with PGHP, as shown in Table [3](#S5.T3 "Table 3 ‣ 5.1\. Purely Generative Results
    ‣ 5\. Results and Evaluation ‣ Rome was Not Built in a Single Step: Hierarchical
    Prompting for LLM-based Chip Design"). PGHP is able to generate valid implementations
    for Systolic Array and UART on which GPT-4 fails completely in flat NH mode. Further,
    we see substantial gains in accuracy for the three simpler benchmarks. As we will
    see next, PGHP with GPT-4 is also successful in automatically designing a single-cycle
    MIPS processor.'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PGHP Accuracy &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; by LLM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Code Llama | VeriGen | CL-Verilog | RTL-Coder | GPT-3.5 | GPT-4 |'
  prefs: []
  type: TYPE_TB
- en: '| Multiplexer | 0.15 | 0.15 | 0.25 | 0.05 | 0.2 | 0.85 |'
  prefs: []
  type: TYPE_TB
- en: '| Decoder | 0.05 | 0.1 | 0.1 | 0.15 | 0.15 | 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Barrel Shifter | 0.05 | 0.0 | 0.1 | 0.0 | 0.0 | 0.35 |'
  prefs: []
  type: TYPE_TB
- en: Table 2. Accuracy of LLM-decided architectural hierarchy out of 20 iterations.
    The inconsistency of most models when generating their own hierarchical plan is
    a major contributing factor to the failure of PGHP outside of GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: '| GPT-4 PGHP | Multiplexer | Decoder | Barrel Shift. | Sys. Array | UART |
    AES |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| pass@1 | 0.5 | 1.0 | 0.3 | 0.1 | 0.3 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| pass@5 | 0.996 | 1.0 | 0.917 | 0.5 | 0.916 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: Table 3. *pass@k* when applying purely generative hierarchical prompting (PGHP)
    to GPT-4 on each benchmark. Note improvement over flat prompting for most modules.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Identifying Common Failure Modes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We often see errors when LLMs generate text for too long and lose the original
    context of their goal. This occurs both for conversational and text-completion
    LLMs. We circumvented this by requesting no additional elements be generated via
    our system prompt, and re-inputting earlier context as a global prompt. Once a
    task is completed, text-completion LLMs tend to hallucinate. A common example
    is the unnecessary generation of testbenches or a random additional module. This
    is avoided by truncating outputs at a useful end-token, usually the “endmodule”
    in Verilog.
  prefs: []
  type: TYPE_NORMAL
- en: 'Conversational LLMs can fall into “perseverative” loops, a termed borrowed
    from neurology (Buckingham and Christman, [2008](#bib.bib4)), continuing to repeat
    actions or words when the stimulus that brought on those behaviors has stopped,
    or when a competing stimulus has occurred that would normally trigger new behavioral
    routes. One example is the continued use of an unnecessary always block when writing
    barrel shifter with non-hierarchical prompting, which can occur even when the
    LLM receives direct/detailed human feedback. As seen in Figure [5](#A0.F5 "Figure
    5 ‣ Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based
    Chip Design") in the Appendix, the LLM will confirm it has done as asked, while
    continuing to output the same syntax as before. Avoiding such behavioral loops
    is another benefit of hierarchical prompting.'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Case Studies and Processor Generation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To stress test our techniques, we hierarchically generated a full MIPS 16-bit
    single-cycle processor using GPT-3.5 and our Code Llama-Verilog model based on
    the PGHP paradigm. Flat prompting is unable to approach a functional processor
    design without considerable human oversight (Blocklove et al., [2023](#bib.bib3)),
    but we hypothesized that hierarchy would bridge this gap and allow for automation.
    We tasked each model to first define a hierarchical structure for the processor
    as a list of submodules, then generate the processor stepwise.
  prefs: []
  type: TYPE_NORMAL
- en: The models generated most necessary submodules, but missed key elements and
    struggled with assigning wire and signal names uniformly across modules, as seen
    in prior experiments. Tool feedback was helpful, but insufficient to bridge these
    issues. Ensuring all input and output wires/signals were named appropriately required
    human intervention and certain submodules like the control unit had to be directly
    requested, but all functional components were LLM-generated. After these interventions,
    we were able to synthesize the processors in Vivado, and successfully simulate
    processor instructions. We repeat this process by generating a RISC-V 32-bit processor
    utilizing GPT-4\. Many of the issues present in GPT-3.5 are less problematic in
    GPT-4, and required much less human intervention. The newer model is better at
    wiring up interconnected modules and produces detailed descriptions of hierarchical
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fully test this capability, we implemented the PGHP technique once more
    to generate another MIPS core via GPT-4 with no human intervention. After iterative
    tool feedback, GPT-4 converged on a synthesizeable processor that covered a version
    of the full MIPS ISA. Design and simulation results are shown in the Appendix.
    Figure [6](#A0.F6 "Figure 6 ‣ Rome was Not Built in a Single Step: Hierarchical
    Prompting for LLM-based Chip Design") shows the RTL and Figure [7](#A0.F7 "Figure
    7 ‣ Rome was Not Built in a Single Step: Hierarchical Prompting for LLM-based
    Chip Design") shows a waveform for this PGHP-sourced processor.'
  prefs: []
  type: TYPE_NORMAL
- en: We posit that this is the first-ever purely LLM-designed processor. That is,
    the design decisions were made entirely by the LLM with no human input, and all
    error handling was done automatically with tool feedback. Beyond the initial prompt
    of *”Please define the necessary submodules in a 16-bit single cycle MIPS processor,”*
    no human design intervention was required. We also see that the time taken to
    generate our processors is on the order of minutes, rather than multiple hours
    as seen in past methods. The time taken to complete the PGHP-based processor was
    23 minutes, 37.85 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get a sense of financial cost savings, we calculate the price-per-token
    and number of tokens generated when applying our hierarchical pipeline to GPT-3.5\.
    We compare results for our full multiplexer hierarchy, our 32-bit barrel shifter,
    our MIPS processor, and our RISC-V processor. Table [4](#S6.T4 "Table 4 ‣ 6\.
    Case Studies and Processor Generation ‣ Rome was Not Built in a Single Step: Hierarchical
    Prompting for LLM-based Chip Design") contains the full cost analysis. As one
    might expect, complex modules lead to higher costs and achieve more financial
    savings when generated hierarchically.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Hierarchical | Non-Hierarchical | Savings |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | I/P | O/P | Cost | I/P | O/P | Cost | % |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | (tokens) | $ | (tokens) | $ |  |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Multiplexer | 92 | 2376 | 0.00484 | 91 | 3283 | 0.00666 | 27.23 |'
  prefs: []
  type: TYPE_TB
- en: '| 32-b Barrel | 262 | 1977 | 0.00422 | 191 | 4268 | 0.00873 | 51.69 |'
  prefs: []
  type: TYPE_TB
- en: '| 16-b MIPS | 434 | 14226 | 0.02868 | 1243 | 31033 | 0.06314 | 54.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 32-b RISC-V | 795 | 17310 | 0.03542 | 1593 | 42338 | 0.08593 | 58.8 |'
  prefs: []
  type: TYPE_TB
- en: Table 4. Cost of input (I/P) tokens processed and output (O/P) tokens generated
    of the modules using hierarchical and non-hierarchical prompting. Values based
    on GPT-3.5 tokenizer pricing.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we have proposed and evaluated Hierachical Prompting as a key
    tool for automated HDL code generation for complex modules. We show that with
    hierarchical prompting, even smaller fine-tuned LLMs can correctly generate HDL
    for complex modules, when traditional flat prompting fails. On powerful models
    like GPT-4, hierarchical prompting is even more impressive, enabling the automatic
    generation of a single-cycle MIPS core. Overall, these methods give considerable
    insight into the potential of LLMs with either manually specified or automatically
    extracted design hierarchy.
  prefs: []
  type: TYPE_NORMAL
- en: There is considerable potential in this line of inquiry. We hope to include
    additional hardware design methods as part of a larger pipeline in the future.
    Considering the successes of methods like high-level synthesis (HLS), it stands
    to reason that leveraging different tools for different tasks could further improve
    results. We plan to fine-tune additional models with hierarchy in mind. Careful
    training dataset formulation could potentially lead to models which excel at hierarchical
    tasks, and may bridge the gap on PGHP performance for smaller models. We hope
    to expand evaluation resources for future benchmarking efforts to increase the
    strength of our *pass@k* metric, ideally $n=200$ samples per test.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI@Meta (2024) AI@Meta. 2024. Llama 3 Model Card. (2024). [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blocklove et al. (2023) Jason Blocklove, Siddharth Garg, Ramesh Karri, and
    Hammond Pearce. 2023. Chip-Chat: Challenges and Opportunities in Conversational
    Hardware Design. *arXiv preprint arXiv:2305.13243* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Buckingham and Christman (2008) Hugh W. Buckingham and Sarah S. Christman. 2008.
    Chapter 12 - Disorders of Phonetics and Phonology. In *Handbook of the Neuroscience
    of Language*, Brigitte Stemmer and Harry A. Whitaker (Eds.). Elsevier, San Diego,
    127–136. [https://doi.org/10.1016/B978-0-08-045352-1.00012-4](https://doi.org/10.1016/B978-0-08-045352-1.00012-4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374* (2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. (2023) S. Cheng, P. Jin, Q. Guo, Z. Du, R. Zhang, Y. Tian, and
    Y. Chen. 2023. Pushing the Limits of Machine Design: Automated CPU Design with
    AI. *arXiv preprint arXiv:2306.12456* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
    Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, et al. 2020. Codebert:
    A pre-trained model for programming and natural languages. *arXiv preprint arXiv:2002.08155*
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fried et al. (2022) Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric
    Wallace, Freda Shi, Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
    2022. Incoder: A generative model for code infilling and synthesis. *arXiv preprint
    arXiv:2204.05999* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) X. Jiang, Y. Dong, L. Wang, Q. Shang, and G. Li. 2023. Self-planning
    code generation with large language model. *arXiv preprint arXiv:2303.06689* (2023).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2024b) Fang Liu, Yang Liu, Lin Shi, Houkun Huang, Ruifeng Wang,
    Zhen Yang, Li Zhang, Zhongqi Li, and Yuchi Ma. 2024b. Exploring and Evaluating
    Hallucinations in LLM-Powered Code Generation. arXiv:2404.00971 [cs.SE]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Mingjie Liu, Nathaniel Pinckney, Brucek Khailany, and Haoxing
    Ren. 2023. VerilogEval: Evaluating Large Language Models for Verilog Code Generation.
    arXiv:2309.07544 [cs.LG]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2024a) Shang Liu, Wenji Fang, Yao Lu, Qijun Zhang, Hongce Zhang,
    and Zhiyao Xie. 2024a. RTLCoder: Outperforming GPT-3.5 in Design RTL Generation
    with Our Open-Source Dataset and Lightweight Solution. arXiv:2312.08617 [cs.PL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2023) Yao Lu, Shang Liu, Qijun Zhang, and Zhiyao Xie. 2023. RTLLM:
    An Open-Source Benchmark for Design RTL Generation with Large Language Model.
    arXiv:2308.05345 [cs.LG]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Manyika (2023) James Manyika. 2023. *An overview of Bard: an early experiment
    with generative AI*. Technical Report. Technical report, Google AI.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nijkamp et al. (2022) Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan
    Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open
    large language model for code with multi-turn program synthesis. *arXiv preprint
    arXiv:2203.13474* (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. [http://arxiv.org/abs/2303.08774](http://arxiv.org/abs/2303.08774).
    [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
    arXiv:2303.08774 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. 2023. Code Llama: Open Foundation Models for Code. *arXiv preprint
    arXiv:2308.12950* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thakur et al. (2023a) Shailja Thakur, Baleegh Ahmad, Zhenxing Fan, Hammond Pearce,
    Benjamin Tan, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. 2023a. Benchmarking
    Large Language Models for Automated Verilog RTL Code Generation. In *2023 Design,
    Automation & Test in Europe Conference & Exhibition (DATE)*. IEEE, 1–6.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thakur et al. (2023b) Shailja Thakur, Baleegh Ahmad, Hammond Pearce, Benjamin
    Tan, Brendan Dolan-Gavitt, Ramesh Karri, and Siddharth Garg. 2023b. VeriGen: A
    Large Language Model for Verilog Code Generation. *arXiv preprint arXiv:2308.00708*
    (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thakur et al. (2023c) Shailja Thakur, Jason Blocklove, Hammond Pearce, Benjamin
    Tan, Siddharth Garg, and Ramesh Karri. 2023c. AutoChip: Automating HDL Generation
    Using LLM Feedback. arXiv:2311.04887 [cs.PL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models.
    *arXiv preprint arXiv:2307.09288* (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. 2021.
    Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding
    and generation. *arXiv preprint arXiv:2109.00859* (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting
    Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Top Modules | Submodules |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 64-to-1 Multiplexer |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 2-to-1 mux &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 4-to-1 mux &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 8-to-1 mux &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 16-to-1 mux &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 32-to-1 mux &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 64-to-1 mux &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 5-to-32 Decoder |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 2-to-4 decoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-to-8 decoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 5-to-32 decoer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 32-bit Barrel Shifter |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 8-bit Barrel Shifter &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Rotation Control &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 32-bit Barrel Shifter &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 4x4 Systolic Array |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Processor Element &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Control Logic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Top-level 4x4 Array &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 8-bit UART |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Baud Rate Generator &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Receiver &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Transmitter &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; State definitions &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Top-level UART &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| 128-bit AES Block Cipher |'
  prefs: []
  type: TYPE_TB
- en: '&#124; S-box &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Key Memory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Encipher Block &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Decipher Block &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Control Logic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Helper Functions(e.g.: &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; inverse shift rows, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mix columns, etc.) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5. Top-level modules and corresponding submodules of the hierarchical
    benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2465aae5c2cf44ffecf10ab7a61266bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5. Perseveration-like behavior in GPT-3.5 when asked for rare syntax.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f491a373b53ea4b85d85b31dfe5d98a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6. Elaborated Design Schematic of LLM-Generated MIPS Processor. Zoom
    in for submodule information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f88378a1cba4c47dc343275352c08a20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7. Sample waveform of running a series of instructions. Our first two
    instructions test the load and store functions, moving values between the registers
    and data memory. We then load two values from data memory, 12,006 and 13,663,
    into the registers R4 and R5 respectively. We then test the ADD function to sum
    them to 25669, or 6445 in hex. This intentionally echoes the value of the instruction
    for easy confirmation on the waveform. We then load two more values, 38,776 and
    1,104 to test the OR instruction, to once again echo the instruction value of
    9778 hex. We then test the SUBI instruction by subtracting 35,498 from 38,776
    to get 3,278, or 0CCE hex. We then store all of our outputs thus far into data
    memory at various addresses to confirm successful saving. More thorough testing
    omitted for brevity. Though the LLM implements opcodes that are notably different
    from the standard MIPS ISA, all instructions are present and functional. Full
    instruction series: LW R7, 8(R7); SW R8, 9(R8); LW R4, 1(R4); LW R5, 3(R5); ADD
    R6, R4, R5; LW R7, 5(R7); LW R8, 6(R8); OR R9, R7, R8; SUBI R10, R9, 35498; SW
    R6, 9(R0); SW R9, 10(R0); SW R10, 11(R0);'
  prefs: []
  type: TYPE_NORMAL
