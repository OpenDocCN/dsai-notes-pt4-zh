- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:40:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:40:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM 能否成为基于提示工程的优秀路径规划器？减轻路径规划中的幻觉
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.13184](https://ar5iv.labs.arxiv.org/html/2408.13184)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2408.13184](https://ar5iv.labs.arxiv.org/html/2408.13184)
- en: Hourui Deng
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 邓厚瑞
- en: College of Computer Science
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机学院
- en: Sichuan Normal University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 四川师范大学
- en: Chengdu, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 成都，中国
- en: herry.liquor@gmail.com
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: herry.liquor@gmail.com
- en: '&Hongjie Zhang'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '&洪杰·张'
- en: College of Computer Science
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机学院
- en: Sichuan Normal University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 四川师范大学
- en: Chengdu, China
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 成都，中国
- en: zhanghongjie@sicnu.edu.cn
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: zhanghongjie@sicnu.edu.cn
- en: '&Jie Ou'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '&解欧'
- en: School of Information and Software Engineering
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 信息与软件工程学院
- en: University of Electronic Science and Technology of China
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 电子科技大学
- en: Chengdu, China
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 成都，中国
- en: oujieww6@gmail.com
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: oujieww6@gmail.com
- en: '&Chaosheng Feng'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '&超生·冯'
- en: College of Computer Science
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机学院
- en: Sichuan Normal University
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 四川师范大学
- en: Chengdu, China
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 成都，中国
- en: csfenggy@sicnu.edu.cn Corresponding Author
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: csfenggy@sicnu.edu.cn 通讯作者
- en: Abstract
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Spatial reasoning in Large Language Models (LLMs) is the foundation for embodied
    intelligence. However, even in simple maze environments, LLMs still encounter
    challenges in long-term path-planning, primarily influenced by their spatial hallucination
    and context inconsistency hallucination by long-term reasoning. To address this
    challenge, this study proposes an innovative model, Spatial-to-Relational Transformation
    and Curriculum Q-Learning (S2RCQL). To address the spatial hallucination of LLMs,
    we propose the Spatial-to-Relational approach, which transforms spatial prompts
    into entity relations and paths representing entity relation chains. This approach
    fully taps the potential of LLMs in terms of sequential thinking. As a result,
    we design a path-planning algorithm based on Q-learning to mitigate the context
    inconsistency hallucination, which enhances the reasoning ability of LLMs. Using
    the Q-value of state-action as auxiliary information for prompts, we correct the
    hallucinations of LLMs, thereby guiding LLMs to learn the optimal path. Finally,
    we propose a reverse curriculum learning technique based on LLMs to further mitigate
    the context inconsistency hallucination. LLMs can rapidly accumulate successful
    experiences by reducing task difficulty and leveraging them to tackle more complex
    tasks. We performed comprehensive experiments based on Baidu’s self-developed
    LLM: ERNIE-Bot 4.0\. The results showed that our S2RCQL achieved a 23%–40% improvement
    in both success and optimality rates compared with advanced prompt engineering.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）中的空间推理是具身智能的基础。然而，即使在简单的迷宫环境中，LLMs 在长期路径规划中仍然面临挑战，这主要受到其空间幻觉和长期推理中的上下文不一致幻觉的影响。为了解决这个挑战，本研究提出了一种创新模型：空间到关系转化与课程
    Q 学习（S2RCQL）。为了应对 LLMs 的空间幻觉，我们提出了空间到关系的方法，该方法将空间提示转化为实体关系和表示实体关系链的路径。这种方法充分挖掘了
    LLMs 在序列思维方面的潜力。因此，我们设计了一种基于 Q 学习的路径规划算法，以减轻上下文不一致幻觉，从而增强 LLMs 的推理能力。通过使用状态-行动的
    Q 值作为提示的辅助信息，我们纠正了 LLMs 的幻觉，从而引导 LLMs 学习最佳路径。最后，我们提出了一种基于 LLMs 的反向课程学习技术，以进一步减轻上下文不一致幻觉。LLMs
    可以通过降低任务难度并利用成功经验迅速积累成功经验，从而应对更复杂的任务。我们基于百度自研的 LLM：ERNIE-Bot 4.0 进行了全面实验。结果显示，我们的
    S2RCQL 在成功率和最佳性方面比先进的提示工程提高了 23%–40%。
- en: 1 Introduction
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models are remarkable artificial intelligence (AI) technology
    that has gained remarkable attention in various fields. The LLMs have implemented
    Artificial Intelligence-Generated Content (AIGC) through massive corpora and advanced
    transformer frameworks. With the support of various prompt engineering, they have
    demonstrated a considerable level of intelligence and accomplished a wide range
    of decision-making tasks, such as mathematical reasoning  [[1](#bib.bib1)], embodied
    AI agent  [[2](#bib.bib2)], UAV control  [[3](#bib.bib3)], and complete open-world
    game Minecraft  [[4](#bib.bib4)] using chain-of-thought technology. However, LLMs
    exhibit significant limitations in spatial reasoning and long-term planning, which
    caused by their spatial hallucination and context inconsistency hallucination
    by long-term reasoning.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型是令人瞩目的人工智能（AI）技术，在多个领域引起了显著的关注。这些LLM通过庞大的语料库和先进的变换器框架实现了人工智能生成内容（AIGC）。在各种提示工程的支持下，它们展示了相当高的智能水平，完成了多种决策任务，如数学推理
    [[1](#bib.bib1)]、具身AI代理 [[2](#bib.bib2)]、无人机控制 [[3](#bib.bib3)] 和利用链式思维技术完成开放世界游戏《Minecraft》
    [[4](#bib.bib4)]。然而，LLM在空间推理和长期规划方面表现出显著的局限性，这些局限性是由它们的空间幻觉和长期推理中的上下文不一致幻觉所造成的。
- en: 'Many studies have proposed various solutions to address hallucination problems,
    mainly focusing on three aspects: instruction fine-tuning, prompt engineering,
    and reinforcement learning. Instruction fine-tuning involves parameter adjustment
    of pre-trained LLMs, encompassing dataset curation and neural network training,
    to enhance performance on the specific task [[5](#bib.bib5)]. However, the significant
    computational costs required for fine-tuning LLMs pose a challenge for rapid expansion
    to new tasks. Prompt engineering aims to improve the inference accuracy of LLMs
    by designing instructions that guide the models to reason according to specific
    requirements. Advanced techniques in this domain include chain-of-thought (CoT) [[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)], tree-of-thought (ToT) [[9](#bib.bib9)], graph-of-thought
    (GoT) [[10](#bib.bib10), [11](#bib.bib11)], chain of experts (CoE) [[13](#bib.bib13)],
    ReAct [[14](#bib.bib14), [15](#bib.bib15)], and Reflexion [[16](#bib.bib16)].
    Reinforcement learning (RL) has long been an effective technique for addressing
    complex planning problems by allowing an agent to interact with its environment
    through trial and error. This RL model continuously adjusts its strategies to
    achieve optimal path planning. By combining RL with LLMs, RL can reduce the cost
    of exploration [[17](#bib.bib17), [18](#bib.bib18)]. However, prompt and RL models
    perform poorly in spatial reasoning tasks, particularly maze path planning. As
    shown in Figure [1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ Can LLM be a
    Good Path Planner based on Prompt Engineering? Mitigating the Hallucination for
    Path Planning"), this maze contains three forbidden zones. Therefore, a path must
    be planned from the starting point to the ending point. However, finding the shortest
    path at a glance is intuitive for humans. Still, when using CoT (Figure [1(b)](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ Can LLM be a Good Path Planner based on Prompt
    Engineering? Mitigating the Hallucination for Path Planning")) and Rememberer [[17](#bib.bib17)](Figure [1(c)](#S1.F1.sf3
    "In Figure 1 ‣ 1 Introduction ‣ Can LLM be a Good Path Planner based on Prompt
    Engineering? Mitigating the Hallucination for Path Planning")), agents often get
    stuck and return with a failure.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 许多研究提出了各种解决幻觉问题的方案，主要集中在三个方面：指令微调、提示工程和强化学习。指令微调涉及对预训练的LLM进行参数调整，包括数据集策划和神经网络训练，以提高在特定任务上的表现[[5](#bib.bib5)]。然而，微调LLM所需的高计算成本对于快速扩展到新任务来说是一个挑战。提示工程旨在通过设计指导模型按特定要求推理的指令来提高LLM的推理准确性。该领域的先进技术包括链式思维（CoT）[[6](#bib.bib6),
    [7](#bib.bib7), [8](#bib.bib8)]，思维树（ToT）[[9](#bib.bib9)]，思维图（GoT）[[10](#bib.bib10),
    [11](#bib.bib11)]，专家链（CoE）[[13](#bib.bib13)]，ReAct[[14](#bib.bib14), [15](#bib.bib15)]，以及Reflexion[[16](#bib.bib16)]。强化学习（RL）长期以来是一种有效的技术，通过允许代理通过试错与环境互动来解决复杂的规划问题。这个RL模型不断调整其策略以实现最佳路径规划。通过将RL与LLM结合，RL可以减少探索的成本[[17](#bib.bib17),
    [18](#bib.bib18)]。然而，提示和RL模型在空间推理任务中表现不佳，特别是在迷宫路径规划中。如图[1(a)](#S1.F1.sf1 "在图1
    ‣ 1 引言 ‣ 基于提示工程的LLM能否成为一个好的路径规划器？缓解路径规划的幻觉")所示，这个迷宫包含三个禁区。因此，必须从起点规划到终点的路径。然而，找出最短路径对于人类来说是一种直观的任务。但当使用CoT（图[1(b)](#S1.F1.sf2
    "在图1 ‣ 1 引言 ‣ 基于提示工程的LLM能否成为一个好的路径规划器？缓解路径规划的幻觉")）和Rememberer[[17](#bib.bib17)](图[1(c)](#S1.F1.sf3
    "在图1 ‣ 1 引言 ‣ 基于提示工程的LLM能否成为一个好的路径规划器？缓解路径规划的幻觉"))时，代理往往会陷入困境并返回失败。
- en: '![Refer to caption](img/0c60cba50a64024b1dbc466b44741595.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/0c60cba50a64024b1dbc466b44741595.png)'
- en: (a) This is a maze that contains three forbidden zones.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 这是一个包含三个禁区的迷宫。
- en: '![Refer to caption](img/9c2b827cb8401df4b95198b84a502492.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/9c2b827cb8401df4b95198b84a502492.png)'
- en: (b) The heatmap of the path using CoT as a prompt to solve this maze.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用CoT作为提示解决这个迷宫的路径热图。
- en: '![Refer to caption](img/cecb274a6fc5033f5e4113a865a702e7.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/cecb274a6fc5033f5e4113a865a702e7.png)'
- en: (c) The heatmap of the path using Rememberer to solve this maze step by step.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 使用Rememberer一步步解决这个迷宫的路径热图。
- en: 'Figure 1: An example of a maze. Solving this maze path planning task is challenging
    using both CoT and Rememberer, an LLMs with RL method.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：迷宫的一个例子。使用CoT和Rememberer进行这个迷宫路径规划任务非常具有挑战性，这些LLM结合了RL方法。
- en: 'We have analyzed the motives behind LLMs’ inadequate understanding of spatial
    relationships and their tendency to navigate naive paths in the direction of the
    shortest straight-line distance while ignoring obstacles. We propose an innovative
    method called Spatial-to-Relational Transformation and Curriculum Q-Learning (S2RCQL)
    to improve the LLM’s performance and solve maze problems. We have introduced the
    Spatial-to-Relational transformation to address the issue of LLM’s spatial hallucination.
    This transformation converts implicit spatial relationships into an explicit entity
    relation, describing the connectivity of paths through the relationships between
    nodes. Then, we introduced a Q-learning-assisted path-planning algorithm for LLMs
    to eliminate the LLMs’ context inconsistency hallucination by long-term reasoning.
    We guided LLMs to avoid dead end by inserting Q-values into the prompts. Finally,
    we have designed a reverse curriculum learning algorithm to mitigate the context
    inconsistency hallucination further. This algorithm gradually increases task difficulty,
    allowing LLMs to reduce the number of reasoning steps. We performed extensive
    experiments based on Baidu’s self-developed LLM: ERNIE-Bot 4.0\. The results indicate
    that our S2RCQL achieves a significant improvement of 23%–40% in success and optimality
    rates compared to the state-of-the-art CoTs baselines. This study offers several
    contributions:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们分析了大型语言模型（LLMs）在空间关系理解不足的动机，以及它们在忽视障碍物的情况下倾向于沿最短直线距离的天真路径导航的问题。我们提出了一种创新的方法，称为**空间到关系的转换和课程Q学习**（S2RCQL），以提高LLM的性能并解决迷宫问题。我们引入了空间到关系的转换，以解决LLM的空间幻觉问题。此转换将隐式空间关系转化为显式实体关系，通过节点之间的关系描述路径的连通性。接着，我们引入了一种Q学习辅助的路径规划算法，以通过长期推理消除LLM的上下文不一致幻觉。我们通过将Q值插入提示中来指导LLM避免死胡同。最后，我们设计了一个反向课程学习算法，进一步缓解上下文不一致的幻觉。该算法逐渐增加任务难度，使LLM减少推理步骤。我们基于百度自主研发的LLM：ERNIE-Bot
    4.0进行了广泛的实验。结果表明，我们的S2RCQL在成功率和最优性率上比最先进的CoTs基线取得了显著提升，提升幅度达到23%–40%。本研究提供了几个贡献：
- en: '1.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: This study is the first to propose converting spatial path planning tasks into
    entity relations for path planning. As a result, we have designed the Spatial-to-Relational
    transformation, which has been successfully applied to maze navigation tasks.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本研究首次提出将空间路径规划任务转换为实体关系进行路径规划。因此，我们设计了空间到关系的转换，并成功应用于迷宫导航任务。
- en: '2.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We have also proposed a path-planning algorithm based on LLMs and reverse curriculum
    Q-learning, representing a step forward in addressing LLM’s context inconsistency
    hallucination.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还提出了一种基于LLM和反向课程Q学习的路径规划算法，代表了在解决LLM上下文不一致幻觉方面的进展。
- en: '3.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We performed comprehensive experiments based on the ERNIE-Bot model to verify
    the reliability and effectiveness of our algorithm.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们基于ERNIE-Bot模型进行了全面实验，以验证我们算法的可靠性和有效性。
- en: 2 Methodology
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 2.1 Overview
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 概述
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Overview ‣ 2 Methodology ‣ Can LLM be a Good
    Path Planner based on Prompt Engineering? Mitigating the Hallucination for Path
    Planning") presents an overview of S2RCQL. Generally, S2RCQL comprises three main
    components: the environment, the agent, and the course module. The agent continuously
    interacts with the environment, seeking the shortest path from the starting point
    to the endpoint through trial and error. The environment module declares the maze
    in text with coordinates, including the maze size, obstacles, starting point,
    and goal. Then, we extract this format information using LLMs. To facilitate Python
    parsing, we control LLM output in JSON format. Moreover, we automatically construct
    a graph from the JSON response of LLMs to explicitly represent the maze connectivity,
    facilitating the LLMs’ reasoning. In the agent module, we construct a state description
    of the current maze, including the graph structure and node. In addition, we retrieve
    the most similar experience and its corresponding Q-value from the experience
    replay buffer. These elements are then used to create a few-shot example concatenated
    with the current maze to form the final prompt. The agent outputs the final action,
    the next-hop node, based on the $\epsilon-greedy$ algorithm and updates the environmental
    state. This process is repeated iteratively. In the top right corner of Figure [2](#S2.F2
    "Figure 2 ‣ 2.1 Overview ‣ 2 Methodology ‣ Can LLM be a Good Path Planner based
    on Prompt Engineering? Mitigating the Hallucination for Path Planning"), we identify
    the reverse course generation module. We construct intermediate starting points
    from the current graph based on hand-craft or LLMs. These starting points can
    reduce task difficulty and eliminate the context inconsistency hallucination by
    reducing the number of reasoning steps.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S2.F2 "Figure 2 ‣ 2.1 Overview ‣ 2 Methodology ‣ Can LLM be a Good Path
    Planner based on Prompt Engineering? Mitigating the Hallucination for Path Planning")
    展示了 S2RCQL 的概述。一般来说，S2RCQL 包括三个主要组件：环境、代理和课程模块。代理不断与环境互动，通过试错方法寻找从起点到终点的最短路径。环境模块用文本和坐标声明迷宫，包括迷宫的大小、障碍物、起点和目标。然后，我们使用
    LLM 提取这种格式信息。为了方便 Python 解析，我们将 LLM 输出控制为 JSON 格式。此外，我们从 LLM 的 JSON 响应中自动构建图，以明确表示迷宫的连通性，便于
    LLM 的推理。在代理模块中，我们构建当前迷宫的状态描述，包括图结构和节点。此外，我们从经验回放缓冲区中检索最相似的经验及其对应的 Q 值。这些元素随后被用来创建一个少量示例，结合当前迷宫形成最终提示。代理基于
    $\epsilon-greedy$ 算法输出最终动作，即下一个节点，并更新环境状态。这个过程会不断重复。在图[2](#S2.F2 "Figure 2 ‣ 2.1
    Overview ‣ 2 Methodology ‣ Can LLM be a Good Path Planner based on Prompt Engineering?
    Mitigating the Hallucination for Path Planning") 的右上角，我们识别了逆向课程生成模块。我们基于手工制作或
    LLM 从当前图中构建中间起点。这些起点可以减少任务难度，通过减少推理步骤来消除上下文不一致的幻觉。
- en: '![Refer to caption](img/67ffc4bd3b55c57261315461c1f0f7d3.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/67ffc4bd3b55c57261315461c1f0f7d3.png)'
- en: 'Figure 2: This diagram provides an overview of our approach. First, we convert
    arbitrary text maze descriptions into entity relations using LLMs and Python code.
    Then, we combined the Q-learning and LLMs to select actions through $\epsilon-greedy$
    with reverse curriculum learning.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：此图提供了我们方法的概述。首先，我们使用 LLM 和 Python 代码将任意文本迷宫描述转换为实体关系。然后，我们将 Q 学习和 LLM 结合起来，通过
    $\epsilon-greedy$ 和逆向课程学习选择动作。
- en: 2.2 Spatial-to-Relational Transformation
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 空间到关系的转换
- en: Researchers from Google DeepMind and University College London have comprehensively
    analyzed LLMs’ capabilities in performing potential multi-step reasoning [[19](#bib.bib19)].
    Multi-step reasoning requires models to retrieve relevant information sequentially
    and piece it together to solve problems or respond to queries. As a result, the
    relevant information required for LLMs’ reasoning is crucial. CoT and ToT techniques
    involve generating intermediate reasoning processes using LLMs, including relevant
    information and piecing together this information to generate answers. In our
    maze planning task, we automatically convert map information into an entity relation
    format, enabling the relevant information required for LLMs’ reasoning to be directly
    presented in the prompt, eliminating spatial hallucination of LLMs.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 来自 Google DeepMind 和伦敦大学学院的研究人员全面分析了 LLMs 在执行潜在多步骤推理中的能力 [[19](#bib.bib19)]。多步骤推理要求模型顺序检索相关信息并将其拼接在一起以解决问题或回应查询。因此，LLMs
    推理所需的相关信息至关重要。CoT 和 ToT 技术涉及使用 LLMs 生成中间推理过程，包括相关信息，并将这些信息拼接在一起以生成答案。在我们的迷宫规划任务中，我们自动将地图信息转换为实体关系格式，使
    LLMs 推理所需的相关信息能够直接呈现在提示中，消除 LLMs 的空间幻觉。
- en: This study transforms the coordinates that describe the locations in the maze,
    such as (1,0) into Node F and (0,0) into Node A. The reachability between coordinates
    is converted into relationships between nodes. For example, if (1,0) and (0,0)
    are reachable, it is translated into (A,F), indicating a direct relationship between
    Node A and Node F. As a result, we used the letters A and F to represent nodes
    instead of numbers or coordinates. Our preliminary experiments indicate that LLMs
    exhibit generalization toward coordinates, perceiving (1,0) and (1,1), favoring
    movement toward (1,1), and leading to dead ends. By employing character representations,
    we can reduce node similarity and focus on relationships rather than node similarity.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究将描述迷宫中位置的坐标，如 (1,0) 转换为节点 F，(0,0) 转换为节点 A。坐标之间的可达性被转换为节点之间的关系。例如，如果 (1,0)
    和 (0,0) 是可达的，则转换为 (A,F)，表示节点 A 和节点 F 之间有直接关系。因此，我们使用字母 A 和 F 来表示节点，而不是数字或坐标。我们的初步实验表明，LLMs
    对坐标有泛化能力，感知 (1,0) 和 (1,1)，偏好向 (1,1) 移动，导致死胡同。通过使用字符表示，我们可以减少节点相似性，专注于关系而非节点相似性。
- en: '![Refer to caption](img/89fa5ba31f8bee1da8db19b6f3e72772.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/89fa5ba31f8bee1da8db19b6f3e72772.png)'
- en: 'Figure 3: This module can process any maze map description and convert it into
    a relational network.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：此模块可以处理任何迷宫地图描述，并将其转换为关系网络。
- en: Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Spatial-to-Relational Transformation ‣ 2 Methodology
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning") illustrates the process of Spatial-to-Relational Transformation.
    Initially, we transform the generically described maze into a structured representation
    using LLMs. In addition, we employed a JSON format to represent maze information,
    including maze size, start and end points, and obstacle coordinates. Through this
    process, we can effectively extract structured information by instructing LLMs
    to output in JSON format. For this purpose, we employ a one-shot exemplar prompt.
    Subsequently, we leverage the OpenAI Gym [[21](#bib.bib21)] environment to transform
    the structured maze into an interactive simulation environment, encompassing essential
    functions such as action execution, state updates, and reward calculations. Finally,
    we convert the state output from Gym into text, which includes information about
    the node relationship network.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S2.F3 "图 3 ‣ 2.2 空间到关系转换 ‣ 2 方法论 ‣ 基于提示工程的 LLM 是否能成为良好的路径规划器？减轻路径规划中的幻觉")
    说明了空间到关系转换的过程。最初，我们使用 LLMs 将一般描述的迷宫转换为结构化表示。此外，我们采用 JSON 格式来表示迷宫信息，包括迷宫的大小、起点和终点以及障碍物坐标。通过这一过程，我们可以通过指示
    LLMs 输出 JSON 格式来有效提取结构化信息。为此，我们使用了一次性示例提示。随后，我们利用 OpenAI Gym [[21](#bib.bib21)]
    环境将结构化迷宫转换为互动模拟环境，包括行动执行、状态更新和奖励计算等基本功能。最后，我们将 Gym 的状态输出转换为文本，其中包括节点关系网络的信息。
- en: 2.3 Curriculum Q-Learning
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 课程 Q 学习
- en: Reverse Curriculum Generator with LLMs
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 LLMs 的反向课程生成器
- en: We used reverse curriculum learning (RCL) [[22](#bib.bib22)] for LLMs inference,
    generating curricula from easy to complex tasks based on the prompt engineering
    of LLMs. The RCL begins by starting from a state close to the goal using random
    walks to find a reachable initial state X1, which is a simplified task. Then,
    based on X1, a more challenging initial state, X2, is generated, which continues
    iteratively. Therefore, the agent must learn according to the curriculum difficulty
    and transfer the experience from simple tasks to complex ones, thereby enhancing
    the efficiency of policy learning.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了反向课程学习（RCL）[[22](#bib.bib22)]进行LLMs推理，从LLMs的提示工程生成从简单到复杂任务的课程。RCL从接近目标的状态开始，使用随机游走找到一个可达的初始状态X1，这是一个简化的任务。然后，基于X1生成更具挑战性的初始状态X2，并持续迭代。因此，代理必须根据课程难度进行学习，并将经验从简单任务转移到复杂任务，从而提高策略学习的效率。
- en: '![Refer to caption](img/c7d29443f1a8d7c74b981ff6d0a09c96.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c7d29443f1a8d7c74b981ff6d0a09c96.png)'
- en: 'Figure 4: Generate curriculums by LLMs.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：通过LLMs生成课程。
- en: We extend this approach to the LLMs’ prompts by describing the reverse curriculum
    generation process in natural language, allowing the LLMs to autonomously determine
    the simplified starting point, shown in Figure [4](#S2.F4 "Figure 4 ‣ Reverse
    Curriculum Generator with LLMs ‣ 2.3 Curriculum Q-Learning ‣ 2 Methodology ‣ Can
    LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"). We incorporate hand-crafted curricula into S2RCQL to obtain
    a better curriculum, which proves beneficial for handling highly complex mazes.
    We performed experiments to thoroughly compare the quality of the two types of
    curricula generation and their impact on the algorithm.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过用自然语言描述反向课程生成过程来扩展这种方法，使LLMs能够自主确定简化的起点，如图 [4](#S2.F4 "图 4 ‣ 反向课程生成器与LLMs
    ‣ 2.3 课程Q学习 ‣ 2 方法论 ‣ LLM能否成为基于提示工程的良好路径规划器？减轻路径规划的幻觉")所示。我们将手工设计的课程纳入S2RCQL，以获得更好的课程，这对于处理高度复杂的迷宫证明是有益的。我们进行了实验，以彻底比较这两种课程生成的质量及其对算法的影响。
- en: Curriculum Q-learning
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 课程Q学习
- en: We used LLMs or hand-crafted method courses to design a staged Q-learning optimization.
    By starting from a given initial point, we used the Q-learning algorithm to update
    the Q-table and store the experience data tuple $(s,a,r,s^{\prime},q)$ upon reaching
    the goal, encouraging the agent to search the shortest path.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了LLMs或手工设计的方法课程来设计分阶段的Q学习优化。通过从给定的初始点开始，我们使用Q学习算法来更新Q表并在达到目标时存储经验数据元组`(s,a,r,s^{\prime},q)`，以鼓励代理寻找最短路径。
- en: Equation [1](#S2.E1 "In Curriculum Q-learning ‣ 2.3 Curriculum Q-Learning ‣
    2 Methodology ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating
    the Hallucination for Path Planning") presents the action sampling function of
    Q-learning based on LLMs, using the $\epsilon-greedy$ is a random number sampled
    at each step.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 方程 [1](#S2.E1 "在课程Q学习 ‣ 2.3 课程Q学习 ‣ 2 方法论 ‣ LLM能否成为基于提示工程的良好路径规划器？减轻路径规划的幻觉")展示了基于LLMs的Q学习动作采样函数，使用$\epsilon-greedy$是每步采样的随机数。
- en: '|  | $1$2 |  | (1) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: Algorithm 1 S2RCQL
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 S2RCQL
- en: 0:  $Maze\ text$
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 0:  $迷宫\ 文本$
- en: Pseudocode of Curriculum Q-learning
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 课程Q学习的伪代码
- en: Algorithm [1](#alg1 "Algorithm 1 ‣ Curriculum Q-learning ‣ 2.3 Curriculum Q-Learning
    ‣ 2 Methodology ‣ Can LLM be a Good Path Planner based on Prompt Engineering?
    Mitigating the Hallucination for Path Planning") describes the pseudocode of S2RCQL.
    This study first inputs the general description of the maze. Then, we translate
    the description into a Gym environment based on LLMs and Python. LLMs generate
    the two courses ($C_{1}$, representing the path plan policy.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 [1](#alg1 "算法 1 ‣ 课程Q学习 ‣ 2.3 课程Q学习 ‣ 2 方法论 ‣ LLM能否成为基于提示工程的良好路径规划器？减轻路径规划的幻觉")描述了S2RCQL的伪代码。本研究首先输入迷宫的一般描述。然后，我们基于LLMs和Python将描述转换为Gym环境。LLMs生成两个课程`(C_{1})`，表示路径规划策略。
- en: 3 Experiments
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: 3.1 Experiment Settings
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: 'We performed numerous maze experiments based on Baidu’s LLM ERNIE-Bot 4.0.
    To verify the inhibitory effect of S2RQL on hallucination, we compare to algorithms
    based on prompt engineering included CoT, ToT, React, Q-learning, and Rememberer.
    Mazes of varying sizes were designed for the experiments, including 30 with $5\times
    5$ mazes. The baseline Prompt Engineering are as follows: Naive, CoT [[6](#bib.bib6)],
    ToT [[9](#bib.bib9)], ReAct [[15](#bib.bib15)], Rememberer [[17](#bib.bib17)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于百度的 LLM ERNIE-Bot 4.0 进行了大量迷宫实验。为了验证 S2RQL 对幻觉的抑制效果，我们与基于提示工程的算法进行了比较，包括
    CoT、ToT、React、Q-learning 和 Rememberer。实验设计了不同大小的迷宫，包括 30 个 $5\times 5$ 迷宫。基线提示工程如下：天真提示、CoT [[6](#bib.bib6)]、ToT [[9](#bib.bib9)]、ReAct [[15](#bib.bib15)]、Rememberer [[17](#bib.bib17)]。
- en: 'We evaluate the effectiveness of LLMs in maze planning by two indicators:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过两个指标评估 LLM 在迷宫规划中的有效性：
- en: (1) Success Rate($\%$ represents the total number of runs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: (1) 成功率（$\%$ 表示总运行次数）。
- en: (2) Optimality Rate($\%$ represents the number of optimal cases. Specifically,
    many multiple shortest paths are found in a MAZE. However, the length of the shortest
    path is unique as long as the length of the resulting path reaches the minimum
    value.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: (2) 最优率（$\%$ 表示最优情况的数量。具体而言，在迷宫中发现了多个最短路径。然而，只要结果路径的长度达到最小值，最短路径的长度是唯一的。
- en: 3.2 Main Results
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 主要结果
- en: 'Table 1: The results for each model are in all mazes, where $(n)$ episodes.
    The best results are highlighted in Bold, and the best baseline models are underlined'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：每种模型在所有迷宫中的结果，其中 $(n)$ 次实验。最佳结果以**粗体**标出，最佳基线模型以*下划线*标出
- en: .
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: .
- en: '| Method | $5\times 5$ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $5\times 5$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Success | Optimality | Success | Optimality | Success | Optimality |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 成功率 | 最优率 | 成功率 | 最优率 | 成功率 | 最优率 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| naive prompt | 11.4% | 10.8% | 10.3% | 12.5% | 9.1% | 8.9% |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 天真提示 | 11.4% | 10.8% | 10.3% | 12.5% | 9.1% | 8.9% |'
- en: '| CoT [[6](#bib.bib6)] | 15.0% | 14.5% | 14.9% | 13.5% | 10.5% | 10.1% |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| CoT [[6](#bib.bib6)] | 15.0% | 14.5% | 14.9% | 13.5% | 10.5% | 10.1% |'
- en: '| ToT [[9](#bib.bib9)] | 17.1% | 13.8% | 16.6% | 13.1% | 10.3% | 12.9% |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ToT [[9](#bib.bib9)] | 17.1% | 13.8% | 16.6% | 13.1% | 10.3% | 12.9% |'
- en: '| ReAct [[15](#bib.bib15)] | 17.4% | 22.8% | 16.1% | 21.6% | 15.4% | 20.7%
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| ReAct [[15](#bib.bib15)] | 17.4% | 22.8% | 16.1% | 21.6% | 15.4% | 20.7%
    |'
- en: '| $Rememberer_{(30)}$ [[17](#bib.bib17)] | 45.1% | 50.8% | 40.2% | 44.2% |
    34.8% | 35.7% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| $Rememberer_{(30)}$ [[17](#bib.bib17)] | 45.1% | 50.8% | 40.2% | 44.2% |
    34.8% | 35.7% |'
- en: '| $S2RCQL_{(30)}$ | 85.6% | 73.8% | 73.4% | 69.6% | 64.7% | 65.7% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| $S2RCQL_{(30)}$ | 85.6% | 73.8% | 73.4% | 69.6% | 64.7% | 65.7% |'
- en: '![Refer to caption](img/7589486e977162df55f8e1c531a0189b.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7589486e977162df55f8e1c531a0189b.png)'
- en: (a) in 5$\times$5 maze
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在 5$\times$5 迷宫中
- en: '![Refer to caption](img/521d0bcba7dbf89541d422c15373e210.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/521d0bcba7dbf89541d422c15373e210.png)'
- en: (b) in 7$\times$7 maze
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 在 7$\times$7 迷宫中
- en: '![Refer to caption](img/487cafc126e424854c223a0536cd5d5b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/487cafc126e424854c223a0536cd5d5b.png)'
- en: (c) in 10$\times$10 maze
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 在 10$\times$10 迷宫中
- en: '![Refer to caption](img/d21db182384dd1feef4e6ef4d16033cd.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d21db182384dd1feef4e6ef4d16033cd.png)'
- en: (d) in 5$\times$5 maze
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 在 5$\times$5 迷宫中
- en: '![Refer to caption](img/2b0e5712235a09e2c2f6823407566287.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2b0e5712235a09e2c2f6823407566287.png)'
- en: (e) in 7$\times$7 maze
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 在 7$\times$7 迷宫中
- en: '![Refer to caption](img/2d0f86e5f768bcd2ce38ddfaadbbf653.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2d0f86e5f768bcd2ce38ddfaadbbf653.png)'
- en: (f) in 10$\times$10 maze
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 在 10$\times$10 迷宫中
- en: 'Figure 5: Comparison of the performance of S2RCQL and Rememberer.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：S2RCQL 和 Rememberer 性能的比较。
- en: Comparison of the performance of S2RCQL and Rememberer algorithms in mazes of
    different sizes. Table [1](#S3.T1 "Table 1 ‣ 3.2 Main Results ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning") displays the main results of each method based on the Success
    and Optimal indicators. From the experimental results, our S2RCQL outperforms
    the Rememberer algorithm by 25%–40% in terms of Success Rate and 23%–30% in terms
    of Optimality Rate. We also compare to Rememberer along with the training episode,
    which is shown in Figure [5](#S3.F5 "Figure 5 ‣ 3.2 Main Results ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"). Furthermore, as the maze size increases, the Success Rate
    and Optimality Rate gradually decline because the maze size determines the search
    space.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: S2RCQL 和记忆器算法在不同大小迷宫中的性能比较。表 [1](#S3.T1 "表 1 ‣ 3.2 主要结果 ‣ 3 实验 ‣ LLM 是否可以成为基于提示工程的良好路径规划器？减轻路径规划中的幻觉")
    显示了每种方法基于成功率和最优性指标的主要结果。从实验结果来看，我们的 S2RCQL 在成功率方面比记忆器算法高出 25%–40%，在最优性方面高出 23%–30%。我们还与记忆器进行比较，训练阶段的结果见图 [5](#S3.F5
    "图 5 ‣ 3.2 主要结果 ‣ 3 实验 ‣ LLM 是否可以成为基于提示工程的良好路径规划器？减轻路径规划中的幻觉")。此外，随着迷宫大小的增加，成功率和最优性逐渐下降，因为迷宫大小决定了搜索空间。
- en: 'Table 2: Enhancement of various prompt engineering with the aid of the S2R
    module.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在 S2R 模块的帮助下，各种提示工程的提升。
- en: '| Method | $5\times 5$ |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $5\times 5$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Success | Optimality | Success | Optimality | Success | Optimality |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 成功率 | 最优性 | 成功率 | 最优性 | 成功率 | 最优性 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| naive prompt | 11.4% | 10.8% | 10.3% | 12.5% | 9.1% | 8.9% |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| naive prompt | 11.4% | 10.8% | 10.3% | 12.5% | 9.1% | 8.9% |'
- en: '| naive prompt w/ S2R | 25.3% | 27.1% | 13.3% | 23.9% | 10.1% | 18.7% |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| naive prompt w/ S2R | 25.3% | 27.1% | 13.3% | 23.9% | 10.1% | 18.7% |'
- en: '| CoT | 15.0% | 14.5% | 14.9% | 13.5% | 10.5% | 10.1% |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| CoT | 15.0% | 14.5% | 14.9% | 13.5% | 10.5% | 10.1% |'
- en: '| CoT w/ S2R | 30.9% | 33.5% | 20.9% | 23.1% | 13.5% | 19.4% |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| CoT w/ S2R | 30.9% | 33.5% | 20.9% | 23.1% | 13.5% | 19.4% |'
- en: '| ToT | 17.1% | 13.8% | 16.6% | 13.1% | 10.3% | 12.9% |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ToT | 17.1% | 13.8% | 16.6% | 13.1% | 10.3% | 12.9% |'
- en: '| ToT w/ S2R | 33.4% | 38.8% | 24.6% | 27.5% | 14.5% | 20.9% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| ToT w/ S2R | 33.4% | 38.8% | 24.6% | 27.5% | 14.5% | 20.9% |'
- en: '| ReAct | 17.4% | 22.8% | 16.1% | 21.6% | 15.4% | 20.7% |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| ReAct | 17.4% | 22.8% | 16.1% | 21.6% | 15.4% | 20.7% |'
- en: '| ReAct w/ S2R | 35.7% | 40.8% | 27.6% | 30.3% | 19.7% | 23.6% |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ReAct w/ S2R | 35.7% | 40.8% | 27.6% | 30.3% | 19.7% | 23.6% |'
- en: '| $Rememberer_{(30)}$ | 45.1% | 50.8% | 40.2% | 44.2% | 34.8% | 35.7% |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| $记忆器_{(30)}$ | 45.1% | 50.8% | 40.2% | 44.2% | 34.8% | 35.7% |'
- en: '| $Rememberer\ w/\ S2R_{(30)}$ | 61.9% | 59.6% | 55.6% | 47.9% | 47.5% | 41.1%
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| $记忆器\ w/\ S2R_{(30)}$ | 61.9% | 59.6% | 55.6% | 47.9% | 47.5% | 41.1% |'
- en: '![Refer to caption](img/b97e90407c8bd8c9eb2a4cd40d19d754.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b97e90407c8bd8c9eb2a4cd40d19d754.png)'
- en: (a) in 5$\times$5 maze
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 在 5$\times$5 迷宫中
- en: '![Refer to caption](img/ce567c48c6af50dd2e9f6a8de436333d.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce567c48c6af50dd2e9f6a8de436333d.png)'
- en: (b) in 7$\times$7 maze
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 在 7$\times$7 迷宫中
- en: '![Refer to caption](img/91b62b092c82d2e38fe61baf130ef468.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/91b62b092c82d2e38fe61baf130ef468.png)'
- en: (c) in 10$\times$10 maze
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 在 10$\times$10 迷宫中
- en: '![Refer to caption](img/db5bbae27a8e9e3faabfda0c1477741c.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/db5bbae27a8e9e3faabfda0c1477741c.png)'
- en: (d) in 5$\times$5 maze
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 在 5$\times$5 迷宫中
- en: '![Refer to caption](img/48bb6373cc25b10001ce1ad628a3abca.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/48bb6373cc25b10001ce1ad628a3abca.png)'
- en: (e) in 7$\times$7 maze
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 在 7$\times$7 迷宫中
- en: '![Refer to caption](img/5c0f71e3ece6808c7f0d378c92308b6b.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5c0f71e3ece6808c7f0d378c92308b6b.png)'
- en: (f) in 10$\times$10 maze
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 在 10$\times$10 迷宫中
- en: 'Figure 6: Comparison of the effectiveness of the S2RCQL algorithm without course
    or S2R and under different curriculum generation schemes.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：S2RCQL 算法在不同课程生成方案下与没有课程或 S2R 的效果比较。
- en: 3.3 Ablation study
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 消融研究
- en: We applied the S2R module to various prompt engineering for verifying the generality
    and effectiveness of S2R. The experimental results are presented in Table[2](#S3.T2
    "Table 2 ‣ 3.2 Main Results ‣ 3 Experiments ‣ Can LLM be a Good Path Planner based
    on Prompt Engineering? Mitigating the Hallucination for Path Planning"). The results
    demonstrate that our S2R significantly improves both Success and Optimality rates
    across various algorithms by mitigating the LLM’s spatial hallucination. We remove
    the S2R module from S2RCQL. The results indicated a decline of approximately 15%
    in both the Success and Optimality rates, shown in Figure [6](#S3.F6 "Figure 6
    ‣ 3.2 Main Results ‣ 3 Experiments ‣ Can LLM be a Good Path Planner based on Prompt
    Engineering? Mitigating the Hallucination for Path Planning"). We removed the
    CL module from S2RCQL. The results indicate that the Success and Optimality rates
    decrease by approximately 20% when there is an absence of curriculum learning.
    As the maze size increases, the impact of CL becomes more significant. Furthermore,
    we conducted a comparison between hand-crafted and LLM-generated reverse curricula,
    as shown in Figure [6](#S3.F6 "Figure 6 ‣ 3.2 Main Results ‣ 3 Experiments ‣ Can
    LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"). The curriculum generated by LLMs demonstrates an improvement
    of approximately 10% compared with algorithms without curriculum learning.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将S2R模块应用于各种提示工程，以验证S2R的普遍性和有效性。实验结果见表格[2](#S3.T2 "Table 2 ‣ 3.2 Main Results
    ‣ 3 Experiments ‣ Can LLM be a Good Path Planner based on Prompt Engineering?
    Mitigating the Hallucination for Path Planning")。结果表明，我们的S2R通过减轻LLM的空间幻觉，显著提高了各种算法的成功率和最优性。我们从S2RCQL中移除了S2R模块。结果显示成功率和最优性均下降约15%，如图[6](#S3.F6
    "Figure 6 ‣ 3.2 Main Results ‣ 3 Experiments ‣ Can LLM be a Good Path Planner
    based on Prompt Engineering? Mitigating the Hallucination for Path Planning")所示。我们从S2RCQL中移除了CL模块。结果表明，在没有课程学习的情况下，成功率和最优性下降约20%。随着迷宫大小的增加，CL的影响变得更加显著。此外，我们对比了手工制作的反向课程与LLM生成的反向课程，如图[6](#S3.F6
    "Figure 6 ‣ 3.2 Main Results ‣ 3 Experiments ‣ Can LLM be a Good Path Planner
    based on Prompt Engineering? Mitigating the Hallucination for Path Planning")所示。LLM生成的课程相比于没有课程学习的算法，表现出大约10%的提升。
- en: '![Refer to caption](img/22b6cfbbb028cde099cecb219e959706.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22b6cfbbb028cde099cecb219e959706.png)'
- en: (a) Errors caused by various prompt engineering over long distances.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 由于长距离的提示工程导致的错误。
- en: '![Refer to caption](img/656372252eed81d21c6046c6dfeadea7.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/656372252eed81d21c6046c6dfeadea7.png)'
- en: (b) Errors with prompting caused by various prompt engineering over short distances.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 由于短距离的提示工程导致的错误。
- en: '![Refer to caption](img/d1de2e0f8c18e0e88fc20807753d17b7.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d1de2e0f8c18e0e88fc20807753d17b7.png)'
- en: (c) Errors with Rememberer w/ S2R caused by prompt engineering over long distances.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 由于长距离的提示工程导致的记忆体错误。
- en: '![Refer to caption](img/8ebaaeb67e5a15607ece7ba5711aa519.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8ebaaeb67e5a15607ece7ba5711aa519.png)'
- en: (d) Corrections caused by S2RCQL prompt engineering over long distances.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 由于S2RCQL提示工程导致的长距离纠正。
- en: 'Figure 7: Toy examples of the most common errors produced by each prompt engineering.
    We show the shortcomings of each method.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：每种提示工程产生的最常见错误的玩具示例。我们展示了每种方法的缺陷。
- en: 3.4 Toy Example
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 玩具示例
- en: 'We conducted several case studies to intuitively demonstrate the execution
    process of LLMs, as shown in Figure [7](#S3.F7 "Figure 7 ‣ 3.3 Ablation study
    ‣ 3 Experiments ‣ Can LLM be a Good Path Planner based on Prompt Engineering?
    Mitigating the Hallucination for Path Planning"):'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了几个案例研究，以直观地展示LLM的执行过程，如图[7](#S3.F7 "Figure 7 ‣ 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning")所示。
- en: (1) As demonstrated in Figure [7(a)](#S3.F7.sf1 "In 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"), the agent reaches (2,1) and (2,2) in the prompt engineering.
    After that, due to spatial hallucination, the direction of action is lost, resulting
    in an unsolvable result. As a result, the agent directly faces the obstacle, and
    the prompt declares that the obstacle cannot be entered. This indicates context
    inconsistency hallucination in the Path Planning problem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: （1）如图[7(a)](#S3.F7.sf1 "在3.3消融研究 ‣ 3 实验 ‣ 基于提示工程的LLM是否能成为良好的路径规划器？缓解路径规划中的幻觉")所示，代理在提示工程中到达了（2,1）和（2,2）。之后，由于空间幻觉，失去了行动方向，导致无法解决的结果。因此，代理直接面对障碍物，提示声明障碍物不能进入。这表明在路径规划问题中存在上下文不一致的幻觉。
- en: (2) As demonstrated in Figure [7(b)](#S3.F7.sf2 "In 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"), when using the Rememberer model to navigate the maze, the
    shortest distance of this maze is small, and LLMs attempt to incorrectly navigate
    to position (1,2) at the point of the obstacle. However, when we use entity relations,
    a simple prompt is required to navigate to the destination, proving that entity
    relations can enhance LLMs’ space understanding and relieve the spatial hallucination
    in LLMs. In our experiment, we found that when navigating from the position (0,3)
    to the target position (1,0), LLMs can get (1,0) without error.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: （2）如图[7(b)](#S3.F7.sf2 "在3.3消融研究 ‣ 3 实验 ‣ 基于提示工程的LLM是否能成为良好的路径规划器？缓解路径规划中的幻觉")所示，当使用Rememberer模型导航迷宫时，该迷宫的最短距离较小，LLM尝试在障碍物处错误地导航到位置（1,2）。然而，当我们使用实体关系时，只需一个简单的提示即可导航到目的地，证明了实体关系可以增强LLM的空间理解，并缓解LLM中的空间幻觉。在我们的实验中，我们发现当从位置（0,3）导航到目标位置（1,0）时，LLM可以无误地到达（1,0）。
- en: (3) As shown in Figure [7(c)](#S3.F7.sf3 "In 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"), we use Rememberer with S2R module. While the success rate
    is nearly 100% for short-distance navigation, the performance is still poor for
    long-distance navigation. In long-term reasoning, the Rememberer still mistakenly
    assumes the existence of a relation (J, O). This shows that S2R alone is not sufficient
    to address LLMs’ context inconsistency hallucination by long-term reasoning. As
    shown in Figure [7(d)](#S3.F7.sf4 "In Figure 7 ‣ 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning"), we propose the S2RCQL model to solve this problem. LLMs can
    accurately navigate to positions they have previously reached (such as point S
    depicted in Figure [7(d)](#S3.F7.sf4 "In Figure 7 ‣ 3.3 Ablation study ‣ 3 Experiments
    ‣ Can LLM be a Good Path Planner based on Prompt Engineering? Mitigating the Hallucination
    for Path Planning")). This to some extent alleviates the context inconsistency
    hallucination. By introducing curriculum learning and the S2R module, S2RCQL can
    better utilize historical experience and find solutions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: （3）如图[7(c)](#S3.F7.sf3 "在3.3消融研究 ‣ 3 实验 ‣ 基于提示工程的LLM是否能成为良好的路径规划器？缓解路径规划中的幻觉")所示，我们使用了带有S2R模块的Rememberer。虽然短距离导航的成功率接近100%，但长距离导航的表现仍然较差。在长期推理中，Rememberer仍错误地假设存在一个关系（J,
    O）。这表明仅使用S2R不足以解决LLM在长期推理中的上下文不一致幻觉。如图[7(d)](#S3.F7.sf4 "在图7 ‣ 3.3消融研究 ‣ 3 实验
    ‣ 基于提示工程的LLM是否能成为良好的路径规划器？缓解路径规划中的幻觉")所示，我们提出了S2RCQL模型来解决这个问题。LLM可以准确导航到它们以前到达的位置（如图[7(d)](#S3.F7.sf4
    "在图7 ‣ 3.3消融研究 ‣ 3 实验 ‣ 基于提示工程的LLM是否能成为良好的路径规划器？缓解路径规划中的幻觉")所示的点S）。这在一定程度上缓解了上下文不一致的幻觉。通过引入课程学习和S2R模块，S2RCQL可以更好地利用历史经验并找到解决方案。
- en: 4 Conclusion
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结论
- en: This study proposes the S2RCQL algorithm to improve LLMs’ path planning ability
    by alleviating spatial hallucinations and context inconsistency hallucinations
    in LLMs. Through S2R, we automatically convert the maze described by coordinates
    into an entity relation graph structure. The proposed S2R exhibits generality,
    and its application to various prompt engineering yields significant improvements.
    In addition, we design a reverse curriculum generation based on LLMs and a curriculum
    Q-learning algorithm that significantly improves the success rate and optimality
    rate of the maze path planning task. In future work, we will research how to enhance
    LLMs’ reverse curriculum generation capabilities.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了S2RCQL算法，通过缓解LLM中的空间幻觉和上下文不一致幻觉来提高LLM的路径规划能力。通过S2R，我们将由坐标描述的迷宫自动转换为实体关系图结构。所提出的S2R具有通用性，其在各种提示工程中的应用带来了显著的改进。此外，我们设计了一种基于LLM的逆课程生成和课程Q学习算法，这显著提高了迷宫路径规划任务的成功率和最优率。在未来的工作中，我们将研究如何增强LLM的逆课程生成能力。
- en: References
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Imani S, Du L, Shrivastava H. MathPrompter: Mathematical Reasoning using
    Large Language Models. InICLR 2023 Workshop on Trustworthy and Reliable Large-Scale
    Machine Learning Models 2023 Apr 16.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Imani S, Du L, Shrivastava H. MathPrompter：使用大语言模型进行数学推理。在ICLR 2023 可信赖的大规模机器学习模型研讨会，2023年4月16日。'
- en: '[2] Dorbala VS, Mullen Jr JF, Manocha D. Can an Embodied Agent Find Your “Cat-shaped
    Mug”? LLM-Based Zero-Shot Object Navigation. IEEE Robotics and Automation Letters.
    2023 Dec 25.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Dorbala VS, Mullen Jr JF, Manocha D. 具身智能体能找到你的“猫形杯”吗？基于LLM的零样本物体导航。IEEE机器人与自动化通讯。2023年12月25日。'
- en: '[3] Piggott B, Patil S, Feng G, Odat I, Mukherjee R, Dharmalingam B, Liu A.
    Net-GPT: A LLM-Empowered Man-in-the-Middle Chatbot for Unmanned Aerial Vehicle.
    In2023 IEEE/ACM Symposium on Edge Computing (SEC) 2023 Dec 6 (pp. 287-293). IEEE.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Piggott B, Patil S, Feng G, Odat I, Mukherjee R, Dharmalingam B, Liu A.
    Net-GPT: 一种由LLM赋能的中间人聊天机器人，用于无人机。2023年12月6日，IEEE/ACM边缘计算研讨会（SEC），第287-293页。IEEE。'
- en: '[4] Zhu X, Chen Y, Tian H, Tao C, Su W, Yang C, Huang G, Li B, Lu L, Wang X,
    Qiao Y. Ghost in the minecraft: Generally capable agents for open-world enviroments
    via large language models with text-based knowledge and memory. arXiv preprint
    arXiv:2305.17144\. 2023 May 25.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Zhu X, Chen Y, Tian H, Tao C, Su W, Yang C, Huang G, Li B, Lu L, Wang X,
    Qiao Y. 我的世界中的幽灵：通过具有文本知识和记忆的大语言模型实现开放世界环境的通用智能体。arXiv 预印本 arXiv:2305.17144。2023年5月25日。'
- en: '[5] Ye S, Hwang H, Yang S, Yun H, Kim Y, Seo M. In-context instruction learning.
    arXiv e-prints. 2023 Feb:arXiv-2302.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Ye S, Hwang H, Yang S, Yun H, Kim Y, Seo M. 上下文内指令学习。arXiv 预印本。2023年2月:arXiv-2302。'
- en: '[6] Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV, Zhou D. Chain-of-thought
    prompting elicits reasoning in large language models. Advances in neural information
    processing systems. 2022 Dec 6;35:24824-37.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Wei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV, Zhou D. 链式思考提示引发大语言模型的推理。神经信息处理系统进展。2022年12月6日;35:24824-37。'
- en: '[7] Chu Z, Chen J, Chen Q, Yu W, He T, Wang H, Peng W, Liu M, Qin B, Liu T.
    A survey of chain of thought reasoning: Advances, frontiers and future. arXiv
    preprint arXiv:2309.15402\. 2023 Sep 27.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Chu Z, Chen J, Chen Q, Yu W, He T, Wang H, Peng W, Liu M, Qin B, Liu T.
    链式思考推理的调查：进展、前沿和未来。arXiv 预印本 arXiv:2309.15402。2023年9月27日。'
- en: '[8] Feng G, Zhang B, Gu Y, Ye H, He D, Wang L. Towards revealing the mystery
    behind chain of thought: a theoretical perspective. Advances in Neural Information
    Processing Systems. 2024 Feb 13;36.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Feng G, Zhang B, Gu Y, Ye H, He D, Wang L. 揭示思维链背后的神秘：一种理论视角。神经信息处理系统进展。2024年2月13日;36。'
- en: '[9] Yao S, Yu D, Zhao J, Shafran I, Griffiths T, Cao Y, Narasimhan K. Tree
    of thoughts: Deliberate problem solving with large language models. Advances in
    Neural Information Processing Systems. 2024 Feb 13;36.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Yao S, Yu D, Zhao J, Shafran I, Griffiths T, Cao Y, Narasimhan K. 思维树：使用大语言模型进行深思熟虑的问题解决。神经信息处理系统进展。2024年2月13日;36。'
- en: '[10] Besta M, Blach N, Kubicek A, Gerstenberger R, Podstawski M, Gianinazzi
    L, Gajda J, Lehmann T, Niewiadomski H, Nyczyk P, Hoefler T. Graph of thoughts:
    Solving elaborate problems with large language models. In Proceedings of the AAAI
    Conference on Artificial Intelligence 2024 Mar 24 (Vol. 38, No. 16, pp. 17682-17690).'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Besta M, Blach N, Kubicek A, Gerstenberger R, Podstawski M, Gianinazzi
    L, Gajda J, Lehmann T, Niewiadomski H, Nyczyk P, Hoefler T. 思维图：使用大语言模型解决复杂问题。在2024年3月24日的AAAI人工智能会议论文集中（第38卷，第16期，第17682-17690页）。'
- en: '[11] Yao Y, Li Z, Zhao H. Beyond chain-of-thought, effective graph-of-thought
    reasoning in large language models. arXiv preprint arXiv:2305.16582\. 2023 May
    26.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Yao Y, Li Z, Zhao H. 超越思维链，大型语言模型中的有效图思维推理。arXiv预印本 arXiv:2305.16582。2023年5月26日。'
- en: '[12] Wang X, Wei J, Schuurmans D, Le QV, Chi EH, Narang S, Chowdhery A, Zhou
    D. Self-Consistency Improves Chain of Thought Reasoning in Language Models. In
    The Eleventh International Conference on Learning Representations 2022 Sep 29.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Wang X, Wei J, Schuurmans D, Le QV, Chi EH, Narang S, Chowdhery A, Zhou
    D. 自我一致性提升语言模型中的思维链推理。在第十一届国际学习表征会议 2022年9月29日。'
- en: '[13] Xiao Z, Zhang D, Wu Y, Xu L, Wang YJ, Han X, Fu X, Zhong T, Zeng J, Song
    M, Chen G. Chain-of-Experts: When LLMs Meet Complex Operations Research Problems.
    In The Twelfth International Conference on Learning Representations 2023 Oct 13.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Xiao Z, Zhang D, Wu Y, Xu L, Wang YJ, Han X, Fu X, Zhong T, Zeng J, Song
    M, Chen G. 专家链：当大型语言模型遇到复杂的运筹学问题。在第十二届国际学习表征会议 2023年10月13日。'
- en: '[14] Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, Cao Y. ReAct: Synergizing
    Reasoning and Acting in Language Models. In International Conference on Learning
    Representations (ICLR) 2023 Jan.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Yao S, Zhao J, Yu D, Du N, Shafran I, Narasimhan K, Cao Y. ReAct：语言模型中的推理与行动的协同。在国际学习表征会议（ICLR）
    2023年1月。'
- en: '[15] Aghzal M, Plaku E, Yao Z. Can large language models be good path planners?
    a benchmark and investigation on spatial-temporal reasoning. arXiv preprint arXiv:2310.03249\.
    2023 Oct 5.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Aghzal M, Plaku E, Yao Z. 大型语言模型能否成为优秀的路径规划器？空间-时间推理的基准和调查。arXiv预印本 arXiv:2310.03249。2023年10月5日。'
- en: '[16] Shinn N, Cassano F, Gopinath A, Narasimhan K, Yao S. Reflexion: Language
    agents with verbal reinforcement learning. Advances in Neural Information Processing
    Systems. 2024 Feb 13;36.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Shinn N, Cassano F, Gopinath A, Narasimhan K, Yao S. Reflexion: 具有语言能力的强化学习智能体。神经信息处理系统进展。2024年2月13日；36。'
- en: '[17] Zhang D, Chen L, Zhang S, Xu H, Zhao Z, Yu K. Large Language Models Are
    Semi-Parametric Reinforcement Learning Agents. Advances in Neural Information
    Processing Systems. 2024 Feb 13;36.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Zhang D, Chen L, Zhang S, Xu H, Zhao Z, Yu K. 大型语言模型是半参数化强化学习智能体。神经信息处理系统进展。2024年2月13日；36。'
- en: '[18] Carta T, Romac C, Wolf T, Lamprier S, Sigaud O, Oudeyer PY. Grounding
    large language models in interactive environments with on-policy reinforcement
    learning. c In International Conference on Machine Learning 2023 Jul 3 (pp. 3676-3713).
    PMLR.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Carta T, Romac C, Wolf T, Lamprier S, Sigaud O, Oudeyer PY. 在交互环境中以策略性强化学习来赋予大型语言模型实际意义。发表于2023年7月3日的国际机器学习会议（第3676-3713页）。PMLR。'
- en: '[19] Yang S, Gribovskaya E, Kassner N, Geva M, Riedel S. Do Large Language
    Models Latently Perform Multi-Hop Reasoning?. arXiv preprint arXiv:2402.16837\.
    2024 Feb 26.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Yang S, Gribovskaya E, Kassner N, Geva M, Riedel S. 大型语言模型是否隐性地进行多跳推理？arXiv预印本
    arXiv:2402.16837。2024年2月26日。'
- en: '[20] Zhao Z, Lee WS, Hsu D. Large language models as commonsense knowledge
    for large-scale task planning. Advances in Neural Information Processing Systems.
    2024 Feb 13;36.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Zhao Z, Lee WS, Hsu D. 大型语言模型作为大规模任务规划的常识知识。神经信息处理系统进展。2024年2月13日；36。'
- en: '[21] Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J, Zaremba
    W. Openai gym. arXiv preprint arXiv:1606.01540\. 2016 Jun 5.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Brockman G, Cheung V, Pettersson L, Schneider J, Schulman J, Tang J, Zaremba
    W. Openai gym。arXiv预印本 arXiv:1606.01540。2016年6月5日。'
- en: '[22] Florensa C, Held D, Wulfmeier M, Zhang M, Abbeel P. Reverse curriculum
    generation for reinforcement learning. InConference on robot learning 2017 Oct
    18 (pp. 482-495). PMLR.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Florensa C, Held D, Wulfmeier M, Zhang M, Abbeel P. 强化学习的逆向课程生成。在2017年10月18日的机器人学习会议（第482-495页）。PMLR。'
