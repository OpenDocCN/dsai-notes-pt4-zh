- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:58'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'LangGPT: Rethinking Structured Reusable Prompt Design Framework for LLMs from
    the Programming Language'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.16929](https://ar5iv.labs.arxiv.org/html/2402.16929)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ming Wang¹²²2These authors contributed equally to this work.    Yuanzhong Liu²²²2These
    authors contributed equally to this work.¹¹1Corresponding authors.    Xiaoming
    Zhang¹    Songlian Li²    Yijie Huang¹    Chi Zhang¹    Daling Wang¹¹¹1Corresponding
    authors.    Shi Feng¹    Jigang Li³ ¹School of Computer Science and Engineering,
    Northeastern University, Shenyang, 110169
  prefs: []
  type: TYPE_NORMAL
- en: ²School of Computer Science, Wuhan University, Wuhan, 430064
  prefs: []
  type: TYPE_NORMAL
- en: ³Individual Researcher
  prefs: []
  type: TYPE_NORMAL
- en: '{sci.m.wang, yz.liu.me, zxm2282588541}@gmail.com, songlianli@whu.edu.cn, {1033205792,
    yijie.huang_neu}@qq.com, {wangdaling, fengshi}@cse.neu.edu.cn, i@lijigang.com'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: LLMs have demonstrated commendable performance across diverse domains. Nevertheless,
    formulating high-quality prompts to effectively instruct LLMs poses a challenge
    for non-AI experts. Existing research in prompt engineering suggests somewhat
    fragmented optimization principles and designs empirically dependent prompt optimizers.
    Unfortunately, these endeavors lack a structured design template, incurring high
    learning costs and resulting in low reusability. Inspired by structured reusable
    programming languages, we propose LangGPT, a dual-layer prompt design framework
    as the programming language for LLMs. LangGPT has an easy-to-learn normative structure
    and provides an extended structure for migration and reuse. Experiments illustrate
    that LangGPT significantly enhances the capacity of LLMs to produce responses
    of superior quality compared to baselines. Moreover, LangGPT has proven effective
    in guiding LLMs to generate high-quality prompts. We have built a community on
    LangGPT to facilitate the tuition and sharing of prompt design. We also analyzed
    the ease of use and reusability of LangGPT through a community user survey.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large language models (LLMs) like ChatGPT Achiam et al. ([2023](#bib.bib2));
    Schulman et al. ([2023](#bib.bib38)) can execute diverse tasks Sun et al. ([2023b](#bib.bib43),
    [c](#bib.bib44)); Yu et al. ([2023](#bib.bib55)) based on powerful language comprehension,
    reasoning, and generation capabilities. Injecting domain knowledge also enables
    LLMs to perform domain-related specific tasks Wang et al. ([2023a](#bib.bib46));
    Li et al. ([2023b](#bib.bib25)); Zhang et al. ([2023](#bib.bib58)); Ren et al.
    ([2023](#bib.bib36)). Fully unleashing these capabilities of LLMs requires premium
    quality prompts Eric ([2022](#bib.bib14)); Chen et al. ([2023](#bib.bib11)); Gajula
    ([2023](#bib.bib17)). Therefore, prompt engineering has attracted the attention
    of many researchers Varshney and Surla ([2023](#bib.bib45)); Meskó ([2023](#bib.bib29));
    Wang ([2023](#bib.bib48)).
  prefs: []
  type: TYPE_NORMAL
- en: Prompt engineering is a typically empirical science, which mainly involves designing
    and optimizing the prompts given to LLMs. As LLMs have the capability of natural
    language understanding, it is possible to ask them to execute tasks through unstructured
    natural language instructions directly. On this basis, some tricks for prompt
    optimization were first explored by researchers. Bsharat et al. ([2023](#bib.bib7))
    introduce 26 guiding principles designed to make LLMs perform better. In addition
    to these directly usable prompt optimization tricks, some researchers have also
    focused on optimizing prompts based on historical data. Sun et al. ([2023a](#bib.bib42))
    guide LLMs to derive new prompts for a given instance from the incorrect reasoning,
    and then summarise the corresponding prompts for each instance as a reference
    for optimizing the original prompts. Pryzant et al. ([2023](#bib.bib35)) leverage
    mini-batches of data to form natural language “gradients” and utilize beam search
    and bandit selection procedure to edit the current prompt in the opposite semantic
    direction of the gradient. Fan et al. ([2023](#bib.bib15)) analyze a large prompt
    database and present an automatic prompt optimization framework.
  prefs: []
  type: TYPE_NORMAL
- en: Direct prompt optimization principles and methods based on historical data require
    a wealth of experience. Thus, these usually perform well only for specific tasks
    or domains. To improve generalization, some researchers have proposed adaptive
    prompt optimization methods. Guo et al. ([2023](#bib.bib21)) connect LLMs with
    evolutionary algorithms and proposes a novel framework for discrete prompt optimization,
    called EvoPrompt. Li et al. ([2023a](#bib.bib24)) design a multi-round dialogue
    alignment strategy and utilizes GPT-4 Achiam et al. ([2023](#bib.bib2)) to generation
    a readability prompt set. Meanwhile, they propose an efficient prompt screening
    metric that can filtrate high-quality prompts with linear complexity. Wang et
    al. ([2023b](#bib.bib47)) introduce PromptAgent which can reflect on model errors
    and generate constructive error feedback to induce precise expert-level insights
    and in-depth instructions. Hao et al. ([2022](#bib.bib22)) and Cheng et al. ([2023](#bib.bib12))
    optimize prompts from the perspective of aligning human and LLMs’ preference styles.
  prefs: []
  type: TYPE_NORMAL
- en: Prompt optimization can significantly improve the performance of LLMs, but the
    reusability of quality prompts is poor due to the lack of systematic design. Consequently,
    some researchers have devised rules for the construction of prompts. Nigh ([2023](#bib.bib31))
    collects a large number of quality prompts and summaries the CRISPE rule for prompt
    design. Zamfirescu-Pereira et al. ([2023](#bib.bib56)) take a prototype LLM-based
    chatbot design tool as the design probe, supporting non-AI-experts engage in “end-user
    prompt engineering”. Some researchers have designed prompt construction rules
    for applications in different domains. Cao et al. ([2023](#bib.bib8)) present
    various prompt templates on deep learning program repair tasks for ChatGPT. Yeh
    et al. ([2022](#bib.bib54)) reformulate the biomedical relation extraction task
    as a cloze-test task under a simple prompt formulation to systematically generate
    comprehensive prompts. Liu and Chilton ([2022](#bib.bib26)) have evaluated 5493
    generations covering 51 themes and 51 styles throughout five experiments in the
    text-to-image task and summaries prompt design guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: These methods, which are based on a great deal of experience in use, are mainly
    based on listed design rules and lack systematicity. In addition, these rules
    have strong domain relevance and model relevance, with low generalisability, flexibility,
    and reusability. To further unleash the performance of LLMs, some researchers
    have defined agents. Agents empower LLMs to use tools, acquire domain knowledge,
    retain long-term or short-term memories, and plan Xu et al. ([2023](#bib.bib52));
    Xi et al. ([2023](#bib.bib51)); Park et al. ([2023](#bib.bib32)). Although agent
    methods Chase ([2022](#bib.bib10)); Hong et al. ([2023](#bib.bib23)); Wu et al.
    ([2023](#bib.bib50)) have systematically designed the key components of the prompt
    and reserved flexible customization interfaces, the learning costs are very high.
    In addition, it is difficult for non-AI experts to modify agent designs, so excellent
    designs are less reusable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To promote LLM-based applications and further stimulate the potential of LLMs,
    we would like to design a high-quality, reusable prompt template. This template
    should be extensible and generalizable. In addition, the template ought to be
    easy to learn and easy to work with. Inspired by the belief that prompt is the
    programming language of the LLM era Alouani ([2023](#bib.bib4)); Mund ([2023](#bib.bib30)),
    we have designed Language for GPT-like LLMs (LangGPT), a prompt design framework
    as the programming language for LLMs. LangGPT refers to the systematic, prescriptive,
    and reusable properties of programming languages, and retains the flexibility
    and extensibility of natural languages. We analyzed the differences between natural
    languages and programming languages to determine the properties that the prompt
    template should have. LangGPT is designed as a dual-layer structure, consisting
    of modules and internal elements. Modules in LangGPT can be divided into two categories:
    inherent modules and extension modules. For inherent modules, we design the necessary
    internal elements of each module in detail and give example templates. In addition,
    for the extension modules, we unified the design of the basic internal elements.
    Experiments have demonstrated that LangGPT is better than baseline prompts for
    bootstrapping LLMs. Furthermore, LLMs can leverage this framework to automatically
    generate prompts as if generating codes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the contributions of this work include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We proposed a dual-layer structured prompt design framework LangGPT to improve
    the generalization and reusability of prompts. In addition, we wrote detailed
    rules for designing prompts based on LangGPT, reducing the learning cost of prompt
    design and enabling LLMs to generate high-quality prompts automatically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We proved experimentally that the prompts designed based on LangGPT can better
    guide LLMs to execute tasks. Meanwhile, we exemplified that LangGPT can help LLMs
    generate high-quality prompts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have built an online community based on LangGPT, which provides usage documentation
    and prompt design interfaces. In addition, collecting and sharing excellent prompt
    cases promotes the exchange of LLM applications. We conducted a user experience
    survey in the community to verify the ease of use and reusability of LangGPT.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Prompt Design Rules with Programming Language
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Programming languages are more standardized and reusable compared to natural
    languages. To design high-quality reusable prompts, we analyzed the differences
    between natural languages and programming languages and proposed prompt design
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Difference between Programming Languages and Natural Languages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While natural languages are primarily used for communication, programming languages
    are designed to define instructions for machines to execute tasks GeeksforGeeks
    ([2023](#bib.bib18)). The different purposes of application have led to very different
    contexts for the creation and evolution of the two languages.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Target audience. Natural languages are spoken by humans to humans Grune et al.
    ([2012](#bib.bib20)), while programming languages are prepared by humans for machines
    Chakray ([2018](#bib.bib9)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structure. The fact that computers can only understand fixed instructions requires
    programming languages to have strict, rigorous syntax and semantics. In opposition,
    natural languages have loose and flexible syntax and semantics, permitting creativity
    and variation and possessing a high degree of fault tolerance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ambiguity. Natural languages are more ambiguous, but humans have the ability
    to clarify the meaning of expressions. For example, problems such as jumbled order
    and typos have less of an impact on reading, and mispronunciations can be understood
    Chakray ([2018](#bib.bib9)); Aho ([2007](#bib.bib3)). Programming languages are
    less ambiguous because they need to provide computers with clear instructions
    GeeksforGeeks ([2023](#bib.bib18)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution and updating. Natural languages evolve naturally over time through
    human use and communication Sipser ([1996](#bib.bib40)). Natural languages are
    flexible in adding new words and meanings and discarding outdated usages Fromkin
    et al. ([2018](#bib.bib16)). Programming languages are specifically designed to
    communicate with machines Sebesta ([2012](#bib.bib39)). New syntax rules and features
    require explicit upgrades or releases Pratt et al. ([1984](#bib.bib34)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a1c8d8664b75fb0f2cc8ad09011e8b5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Analogy between programming language and natural language prompt.
    The analogy between the two types of languages was analyzed in terms of their
    hierarchical structure. Circles of different sizes indicate different layers.
    Smaller circles indicate closer to the inner layers, corresponding to darker colors.'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, the main difference between the two languages is that natural languages
    are more vague and flexible, while programming languages are more standardized
    and precise. LLMs essentially perform a large number of computations and share
    many similarities with machines. Therefore, we propose LangGPT, a natural language
    programming framework for LLMs, by drawing on the characteristics of programming
    languages and combining the advantages of natural languages.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Rules for Prompt Design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We improve prompts by referring to the design ideas of programming languages.
    After analyzing the differences between programming languages and natural languages,
    we propose the design rules for prompts: (1) Prompts should have a regularised
    format. Flexible and ambiguous natural languages are difficult to understand for
    LLMs. Format-constrained prompts make users’ purpose and requirements more salient.
    (2) The structure of prompts should be extensible. Custom structures facilitate
    users to design suitable prompts according to their domains and tasks. (3) Specific
    requirements must be clear and complete. Both instructions and additional requirements
    should be explicit and complete to avoid misunderstandings or biases. (4) Languages
    should be flexible. Where the requirements are clear, flexible languages can be
    better adapted to different domains. In addition, flexible languages are easy
    for to learn users.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Prof. | Cons. | Goal | Init. | Ex. | Wkflo. | Skill | Sug. | Bkgrd.
    | Style | Outf. |'
  prefs: []
  type: TYPE_TB
- en: '| Writing | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Role-playing | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Entertainment | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✓ | ✗ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Supplementary Learning | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt Optimisation | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt Hacking | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ | ✗ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Drawing | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✗ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Business Operation | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Status of inherent module definitions. The table lists the eight categories
    of application scenarios we have defined so far and the modules defined for these
    scenarios. A ✓indicates that a corresponding module has been designed for this
    scenario. In contrast, a ✗ indicates that it was not designed.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Natural Language Programming Framework of Prompts for LLMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the design rules, we present LangGPT, the natural language programming
    framework with a dual-layer architecture for LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Overall Dual-layer Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To systematically design prompts that meet the rules, we have made full reference
    to the design ideas and structures of object-oriented programming languages Rentsch
    ([1982](#bib.bib37)); Lutz ([2010](#bib.bib28)). We consider the prompt as a software
    project and analogize the prompt design process with the software development
    process. The correspondence is shown in Figure [1](#S2.F1 "Figure 1 ‣ 2.1 Difference
    between Programming Languages and Natural Languages ‣ 2 Prompt Design Rules with
    Programming Language ‣ LangGPT: Rethinking Structured Reusable Prompt Design Framework
    for LLMs from the Programming Language").'
  prefs: []
  type: TYPE_NORMAL
- en: Based on analogical analyses, it can be found that natural language prompts
    have a similar multi-level structure as programming languages. Therefore, we refer
    to the structure of programming languages and propose a dual-layer structure for
    prompt design, and define the notion of module and element for prompts.
  prefs: []
  type: TYPE_NORMAL
- en: A complete prompt contains several modules. Modules are similar to classes in
    programming languages, and each module represents an aspect of the requirements
    for LLMs. For instance, prompts can be augmented in terms of constraints, goals,
    profiles, etc. Within a module, a number of internal elements are included. Elements
    are similar to functions and properties in programming languages and represent
    the content of direct and specific instructions to LLMs. For example, “Output
    should be no more than 500 words” could be an element in a prompt that belongs
    to the module constraint.
  prefs: []
  type: TYPE_NORMAL
- en: A dual-layer structure can be a good way to standardize the formatting of prompts.
    However, the flexibility of natural languages would be lost if prompts are too
    strictly required to follow predefined standard modules and internal elements.
    In addition, it will reduce the generalisability of LangGPT to different tasks
    in different domains, which is not conducive to the reuse of quality prompts.
    To solve these problems, we divided the types of modules and elements into prompts.
    We defined inherent module and basic element as the predefined dual-layer prompt
    template. In addition, we constructed extension module and custom element that
    support customization. We provide both Markdown Gruber ([2012](#bib.bib19)) and
    JSON Pezoa et al. ([2016](#bib.bib33)) formats for inherent modules and extension
    modules. Furthermore, we have written basic elements for different modules and
    defined rules for writing custom elements.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Tectonics of Inherent Modules
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The module serves as a connection between the complete prompt and the instruction
    unit and plays a very important role in controlling the structure of the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define inherent modules for critical aspects that are required for almost
    all prompts. Furthermore, we define inherent modules for certain scenarios that
    are relevant to the application for ease of learning and use. Table [1](#S2.T1
    "Table 1 ‣ 2.2 Rules for Prompt Design ‣ 2 Prompt Design Rules with Programming
    Language ‣ LangGPT: Rethinking Structured Reusable Prompt Design Framework for
    LLMs from the Programming Language") shows the inherent modules we defined for
    some scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [1](#S2.T1 "Table 1 ‣ 2.2 Rules for Prompt Design ‣ 2 Prompt Design
    Rules with Programming Language ‣ LangGPT: Rethinking Structured Reusable Prompt
    Design Framework for LLMs from the Programming Language"), Prof. indicates what
    is expected of LLMs in terms of roles, including profiles, character portraits,
    etc. Cons. denotes constraints or attention, i.e., ranges that LLMs are not allowed
    to exceed and requirements that must be met when generating responses, etc. Goal
    lists the goals that the user wants to achieve, which is what the LLMs need to
    accomplish. Init. is called initialization to inform LLMs that they are about
    to start a dialogue. Sometimes a specified first sentence is also given in this
    module. Ex. gives LLMs input-output pairs as examples to learn from. Wkflo. instructs
    the workflow when executing a task, similar to the CoT approach Wei et al. ([2023](#bib.bib49)).
    It is often necessary to instantiate this module when the task requirements are
    more complex. Skill is used to suggest to LLMs the skills they possess. LLMs that
    have undergone tool learning can be guided to invoke tools to execute tasks more
    accurately. In addition, we plan to provide the ability to use tools under this
    module in future work, with reference to the design of agent tools Chase ([2022](#bib.bib10));
    Hong et al. ([2023](#bib.bib23)). Sug. includes suggestions and behavioral planning
    for LLMs. This module focuses on listing common scenarios and giving behaviors
    or responses that LLMs can take in such situations. Bkgrd. indicates the background
    information and the memories that LLMs are required to have when performing their
    tasks. Style qualifies the style of responses generated by LLMs. Outf. defines
    the LLMs’ output-format. Specifying the output format improves the efficiency
    and accuracy of the results extraction in certain tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: The bolded font is used as the name of the modules in the introduction list.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Internal Basic Elements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Three purposes are typically included in prompts: (1) Implying a certain message
    to LLMs; (2) Letting LLMs execute a certain task with or without output; (3) The
    combination of the first two.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first of these is very similar to the assignment of properties or variables
    in programming languages. Correspondingly, the last two categories are similar
    to functions in programming languages. Thus, we construct these three types of
    basic elements. We use “The $\langle\textsc{property}\rangle$.” In the basic element
    writing patterns we provide, the contents contained in the angle brackets need
    to be populated according to the module and the usage scenario. It is important
    to note that the writing patterns we have provided only specify the idea of writing
    internal elements. To improve the generalisability and flexibility of prompts,
    the language can be adapted to express key information. In Table [2](#S3.T2 "Table
    2 ‣ 3.3 Internal Basic Elements ‣ 3 Natural Language Programming Framework of
    Prompts for LLMs ‣ LangGPT: Rethinking Structured Reusable Prompt Design Framework
    for LLMs from the Programming Language") we show some examples of the basic elements
    in some modules.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Module | Samples of Basic Elements |'
  prefs: []
  type: TYPE_TB
- en: '| Profile | $\bullet$ You are a magazine editor. |'
  prefs: []
  type: TYPE_TB
- en: '| Goal | $\bullet$ You need to generate a title for the article. |'
  prefs: []
  type: TYPE_TB
- en: '| Constraint | $\bullet$ The length of the title should not exceed 20 words.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Workflow | ### Extracting the kernel content |'
  prefs: []
  type: TYPE_TB
- en: '| $\bullet$, please execute the following actions: |'
  prefs: []
  type: TYPE_TB
- en: '| $\circ$ Analyse the theme of the article; |'
  prefs: []
  type: TYPE_TB
- en: '| $\circ$ Detecting the main objects and related things described in the article;
    |'
  prefs: []
  type: TYPE_TB
- en: '| $\circ$ Summarising the core content from the article; |'
  prefs: []
  type: TYPE_TB
- en: '| $\circ$ Save the kernel content. |'
  prefs: []
  type: TYPE_TB
- en: '| Style | $\bullet$ The style of the title should be formal. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Examples of basic internal elements of inherent modules in the writing
    scenario. This prompt belongs to the writing category and its purpose is to generate
    a title for a given article. We have chosen five modules as examples - profile,
    goal, constraint, workflow, and style - and show one internal element from each
    module. Particularly, for the workflow module, we show a function-like basic element.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/edd10d4957ba7bd9a57c5710b70299a4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Design process for extension modules and custom elements. For application
    scenarios where inherent modules cannot cover all aspects, new extension modules
    can be defined as needed. After defining the extension module, the internal elements
    should also be designed according to the requirements of the extension module.
    In addition, if inherent modules are compatible with the needs of the scenario
    but the basic elements cannot meet all of the requirements, custom elements can
    be added directly to the inherent modules. It is important to note that extension
    and inherent modules should be as mutually exclusive as possible to minimize modification
    costs.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Extension Module and Custom Element
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The inherent modules we have defined have been as comprehensive as possible
    to cover the many aspects of prompts. Furthermore, we add application scenarios
    and modules covered by LangGPT. However, limited by our own capabilities and domain
    knowledge, we were unable to consider all application scenarios of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in addition to inherent modules and basic elements, we define the
    extension module and custom element to improve the generalization and reusability
    of prompts. The design flow of extension modules and custom elements is shown
    in Figure [2](#S3.F2 "Figure 2 ‣ 3.3 Internal Basic Elements ‣ 3 Natural Language
    Programming Framework of Prompts for LLMs ‣ LangGPT: Rethinking Structured Reusable
    Prompt Design Framework for LLMs from the Programming Language").'
  prefs: []
  type: TYPE_NORMAL
- en: This process helps users quickly analyze their needs and come up with the key
    points that need to be included in the prompt. Based on the design templates provided
    by LangGPT and this design process, quality prompts can be efficiently designed
    to match the application scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To validate the advancement of the proposed LangGPT, we conducted experiments
    in two aspects: the tasks executed by LLMs and the usability of LangGPT. In our
    experiments, we chose two types of application scenarios: writing and role-playing.
    For each scenario, we selected five task-specific assistants constructed and shared
    by users in our community.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d08313be38f64d3131ce89e2214e4fe7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Ratings on ease of use in user survey. The lowest score is 0, which
    means very difficult to use, and the highest score is 5, which means very easy
    to use. The “:” is used to separate scores and percentages.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Baselines
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For comparison, we chose two baselines.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instruction-only. The prompts contain only the instructions proposed for LLMs
    and necessary information as needed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CRISPE prompt design rules Nigh ([2023](#bib.bib31)). The framework prospectively
    defines the components of the prompt from a macro perspective. It requires a complete
    prompt containing Capacity and Role, Insight, Statement, Personality, and Experiment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Instruction-only prompts are very intuitive and maximize token savings. However,
    this approach contains too little content, often fails to convey the full range
    of requirements, and makes it difficult to focus on key points. CRISPE provides
    a relatively complete design framework rather than a number of fragmented rules.
    However, it does not have a clear structured design, which is not conducive to
    learning and the reuse of high-quality prompts. In addition, CRISPE has a strict
    specification of the elements to be included in the prompts, having a low generalization
    capability.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5459d3e462781804f17a7e672323a8ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: A case of a boot-licker. The responses of ChatGPT-3.5 to the user
    under three different prompts. Mingyuan University doesn’t really exist.'
  prefs: []
  type: TYPE_NORMAL
- en: Because of the shortcomings of these methods in prompt design, we designed LangGPT.
    However, in addition to these intuitive advantages, we need to verify the ability
    of prompts to induce LLMs, which is the most essential purpose of prompt design.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Large Language Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We selected these LLMs^*^**All executive tasks in the experiment were completed
    by 18 January 2024. shown in Table [3](#S4.T3 "Table 3 ‣ 4.2 Large Language Models
    ‣ 4 Experiments ‣ LangGPT: Rethinking Structured Reusable Prompt Design Framework
    for LLMs from the Programming Language") for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| LLM | Access | Scale | Version |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT Schulman et al. ([2023](#bib.bib38)) | API | unk | v3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM Du et al. ([2021](#bib.bib13)) | API | unk | v3-turbo |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM Zeng et al. ([2023](#bib.bib57)) | Open Source | 6B | v3 |'
  prefs: []
  type: TYPE_TB
- en: '| Ernie Bot Sun et al. ([2021](#bib.bib41)) | API | unk | v4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini Anil et al. ([2023](#bib.bib5)) | API | unk | Pro |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan Yang et al. ([2023](#bib.bib53)) | Open Source | 7B | v2-chat |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen Bai et al. ([2023](#bib.bib6)) | Open Source | 7B | chat |'
  prefs: []
  type: TYPE_TB
- en: '| Yi 01-ai ([2023](#bib.bib1)) | Open Source | 6B | chat |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: LLMs used in experiments. “unk” indicates that we do not know the
    size of those models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the complexity of the task scenarios, there is a lack of objective evaluation
    metrics. Therefore, we evaluated the ability of LLMs to execute tasks using human
    evaluation and LLM evaluation. To better ensure the rationality of the evaluation,
    we set evaluation criteria for two scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'For writing tasks, we asked the evaluators to assess the effectiveness of the
    LLMs in performing the task in terms of three dimensions: textual continuity,
    formatting norms, and content richness. The ability to generate thematically coherent
    content is important. However, we found that LLMs in the generation task were
    usually able to maintain thematic consistency, so no evaluation metrics were designed
    for this aspect. For role-playing tasks, we similarly designed 3 dimensions of
    evaluation metrics: language style, characteristic relevance, and thematic coherence.'
  prefs: []
  type: TYPE_NORMAL
- en: For each evaluation metric, we defined the score from zero to five to indicate
    different levels and indicated the description of the situation corresponding
    to each integer score. In addition, we allowed evaluators to score 0.5 between
    levels.
  prefs: []
  type: TYPE_NORMAL
- en: This evaluation framework can also guide high-performance LLMs, such as GPT-4
    and Ernie Bot-4, to automatically evaluate their performance. As a result of the
    low consistency and of LLMs as evaluators Liu et al. ([2023](#bib.bib27)), we
    evaluate them manually.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The most intuitive manifestation of the ability of a prompt to induce LLMs
    is the performance of LLMs in performing tasks guided by the prompt. Thus, we
    evaluated the performance of LLMs in two scenarios and the results are shown in
    Table [4](#S4.T4 "Table 4 ‣ 4.4 Results ‣ 4 Experiments ‣ LangGPT: Rethinking
    Structured Reusable Prompt Design Framework for LLMs from the Programming Language").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Prompt | GPT-3.5 | GLM-turbo | ChatGLM3 | Ernie Bot | Gemini Pro
    | Baichuan2 | Qwen | Yi |'
  prefs: []
  type: TYPE_TB
- en: '| s1 | s2 | s3 | s1 | s2 | s3 | s1 | s2 | s3 | s1 | s2 | s3 | s1 | s2 | s3
    | s1 | s2 | s3 | s1 | s2 | s3 | s1 | s2 | s3 |'
  prefs: []
  type: TYPE_TB
- en: '| Writing | I | 3.8 | 3.5 | 3.7 | 3.5 | 3.1 | 3.2 | 3.7 | 3.5 | 3.8 | 4.2 |
    3.9 | 3.7 | 3.8 | 4.2 | 4.5 | 3.5 | 3.6 | 3.3 | 4.1 | 3.7 | 4.4 | 4.1 | 3.8 |
    4.3 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 3.7 | 3.1 | 3.4 | 3.5 | 3.1 | 3.1 | 3.9 | 3.7 | 4.1 | 4 | 3.9 | 4 | 4.1
    | 4.1 | 4.1 | 3.7 | 3.6 | 3.4 | 4.3 | 4 | 4.2 | 4.3 | 4 | 4.2 |'
  prefs: []
  type: TYPE_TB
- en: '| L | 3.8 | 3.5 | 3.4 | 3.4 | 3.4 | 3.4 | 3.5 | 4 | 3.8 | 4.4 | 4 | 4.2 | 4.2
    | 3.9 | 4.1 | 3.6 | 3.4 | 3.7 | 3.4 | 3.6 | 3.7 | 3.9 | 4.1 | 4.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Role-playing | I | 4.2 | 3.5 | 3.4 | 3.8 | 1.6 | 1.4 | 3.4 | 1.4 | 1.7 |
    4 | 0.8 | 2.1 | 4.4 | 2.8 | 3.4 | 3.9 | 1.3 | 1.2 | 3.9 | 2.9 | 2.8 | 3.6 | 2
    | 2.4 |'
  prefs: []
  type: TYPE_TB
- en: '| C | 4 | 3.7 | 4 | 4.2 | 2.4 | 2.5 | 4 | 3 | 3.7 | 4.3 | 2.6 | 2.5 | 4.6 |
    4.2 | 4 | 4.1 | 2.5 | 2.4 | 4.3 | 3.2 | 3.4 | 4.2 | 3.2 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| L | 4.3 | 4.3 | 4.3 | 4.2 | 1.8 | 2.1 | 4.1 | 4 | 4.3 | 4.2 | 2.3 | 2.5 |
    4.6 | 4.4 | 4.3 | 4.4 | 3 | 3.2 | 4.2 | 3.7 | 3.7 | 3.8 | 2.8 | 2.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Results of different prompts inducing LLMs to perform tasks. Bolded
    fonts indicate the type of prompts that perform best in the current scenario.
    I, C, and L denote three kinds of prompts, i.e., Instruction-only, CRISPE, and
    LangGPT, respectively. $\text{s}_{1}$ is thematic coherence.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scenario | Writing | Role-playing |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt | I | C | L | I | C | L |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3.5 | 3.67 | 3.40 | 3.57 | 3.70 | 3.90 | 4.30 |'
  prefs: []
  type: TYPE_TB
- en: '| GLM-turbo | 3.27 | 3.23 | 3.40 | 2.27 | 3.03 | 2.70 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGLM3 | 3.67 | 3.90 | 3.77 | 2.17 | 3.57 | 4.13 |'
  prefs: []
  type: TYPE_TB
- en: '| Ernie Bot | 3.93 | 3.97 | 4.20 | 2.30 | 3.13 | 3.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Gemini Pro | 4.17 | 4.10 | 4.07 | 3.53 | 4.27 | 4.43 |'
  prefs: []
  type: TYPE_TB
- en: '| Baichuan2 | 3.47 | 3.57 | 3.57 | 2.13 | 3.00 | 3.53 |'
  prefs: []
  type: TYPE_TB
- en: '| Qwen | 4.07 | 4.17 | 3.57 | 3.20 | 3.63 | 3.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Yi | 4.07 | 4.17 | 4.20 | 2.67 | 3.47 | 3.13 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: The average of the scores of the different LLMs on the two types of
    Scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: From the results, it can be seen that LangGPT is better at inducing LLMs to
    execute tasks. In addition, we noticed an interesting phenomenon. Certain LLMs
    with particularly strict security restrictions will refuse to answer questions
    or requests on topics such as bragging and crankiness. LangGPT can make these
    LLMs realize that it’s just a style of speaking rather than truly damaging behavior
    and guide them to respond. Neither of the other two baselines can do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'To evaluate the ease of use of LangGPT, we conducted a user survey in our online
    community^†^††The community has been running for more than six months and has
    amassed thousands of users from a wide range of industries, including manufacturing,
    construction, information technology, finance, and entertainment. Therefore, the
    objectivity of the survey results can be guaranteed.. We designed a complete questionnaire
    about the LangGPT experience to ensure the quality of answers. The questionnaire
    included a rating question on ease of use. The results of the user ratings are
    shown in Figure [3](#S4.F3 "Figure 3 ‣ 4 Experiments ‣ LangGPT: Rethinking Structured
    Reusable Prompt Design Framework for LLMs from the Programming Language").'
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen from Figure [3](#S4.F3 "Figure 3 ‣ 4 Experiments ‣ LangGPT:
    Rethinking Structured Reusable Prompt Design Framework for LLMs from the Programming
    Language"), 87.81% of users gave a score of 3 or higher, which indicates users’
    approval of LangGPT’s ease of use. In addition, LangGPT’s overall satisfaction
    score in the user survey was 8.48 out of 10.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To demonstrate the effect of LangGPT more intuitively, we filtered specific
    cases from our experiments. In addition to the direct effect comparison, we also
    tried to use LangGPT to guide LLMs to generate quality prompts.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.1 Inducing LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In our experiments, we guided the LLMs to play a boot-licker using three prompts,
    an example of which is given in Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Baselines ‣
    4 Experiments ‣ LangGPT: Rethinking Structured Reusable Prompt Design Framework
    for LLMs from the Programming Language").'
  prefs: []
  type: TYPE_NORMAL
- en: In this example, both the Instruction-only prompt and the CRISPE prompt-guided
    ChatGPT simply reply to the user’s utterances. In contrast, LangGPT-guided ChatGPT
    is even more bottomless in its blow-by-blow approach to the user-given subject.
    Additionally, it expresses compliments from a wider range of perspectives and
    is more in character.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1abd2b82bbed64f4b22e97a5ae2cb6b4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Example of ChatGPT-3.5 to generate prompts with LangGPT. Some modules
    have been omitted for ease of presentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.5.2 Prompt construction with LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In organizing our experiments, we also tried to make LLMs construct prompts
    using LangGPT. An example is shown in Figure [5](#S4.F5 "Figure 5 ‣ 4.5.1 Inducing
    LLMs ‣ 4.5 Case Study ‣ 4 Experiments ‣ LangGPT: Rethinking Structured Reusable
    Prompt Design Framework for LLMs from the Programming Language").'
  prefs: []
  type: TYPE_NORMAL
- en: If LLMs are directly asked to generate a prompt about MBTI assessment, they
    might reject it. However, LangGPT can direct LLMs to generate high-quality prompts
    that are harmless.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we present LangGPT, a structured and extensible framework for
    prompt design. LangGPT has a systematic structure similar to object-oriented programming
    languages and is easy to learn and reuse. Experiments demonstrate that LangGPT
    performs better than the baseline methods in guiding LLMs to perform tasks. We
    also conducted a user survey in the community built on LangGPT to verify the ease
    of use and reusability of LangGPT. In future work, we will further optimize the
    design of LangGPT and reduce the token consumption of LangGPT. In addition, support
    for LLMs using third-party tools and custom tools will be added.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the application of LLM, ethical disputes may arise, but the design of LangGPT
    and the process of writing this paper avoided possible ethical issues.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '01-ai [2023] 01-ai. Building the Next Generation of Open-Source and Bilingual
    LLMs. https://github.com/01-ai/Yi, 2023. original-date: 2023-11-03T16:08:37Z.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, et al. GPT-4
    Technical Report, December 2023. arXiv:2303.08774 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aho [2007] Alfred V Aho. Compilers: principles, techniques and tools. Pearson
    Education India, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alouani [2023] Nabil Alouani. Prompt Engineering Could Be the Hottest Programming
    Language of 2024 — Here’s Why. https://towardsdatascience.com, December 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anil et al. [2023] Rohan Anil, Sebastian Borgeaud, Yonghui Wu, et al. Gemini:
    A Family of Highly Capable Multimodal Models, December 2023. arXiv:2312.11805
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. [2023] Jinze Bai, Shuai Bai, Yunfei Chu, et al. Qwen Technical Report,
    September 2023. arXiv:2309.16609 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bsharat et al. [2023] Sondos Mahmoud Bsharat, Aidar Myrzakhan, and Zhiqiang
    Shen. Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4,
    December 2023. arXiv:2312.16171 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. [2023] Jialun Cao, Meiziniu Li, Ming Wen, and Shing-chi Cheung. A
    study on Prompt Design, Advantages and Limitations of ChatGPT for Deep Learning
    Program Repair, April 2023. arXiv:2304.08191 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chakray [2018] Chakray. Programming Languages: Types and Features, December
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chase [2022] Harrison Chase. LangChain: Building applications with LLMs through
    composability. https://github.com/langchain-ai/langchain, October 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2023] Banghao Chen, Zhaofeng Zhang, Nicolas Langrené, and Shengxin
    Zhu. Unleashing the potential of prompt engineering in Large Language Models:
    a comprehensive review, October 2023. arXiv:2310.14735 [cs] version: 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng et al. [2023] Jiale Cheng, Xiao Liu, Kehan Zheng, et al. Black-Box Prompt
    Optimization: Aligning Large Language Models without Model Training, November
    2023. arXiv:2311.04155 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. [2021] Zhengxiao Du, Yujie Qian, Xiao Liu, et al. GLM: General Language
    Model Pretraining with Autoregressive Blank Infilling. In Annual Meeting of the
    Association for Computational Linguistics, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eric [2022] Mihail Eric. A complete introduction to prompt engineering for large
    language models. https://www.mihaileric.com, October 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan et al. [2023] Ling Fan, Harry Jiannan Wang, Kunpeng Zhang, et al. Towards
    an Automatic Prompt Optimization Framework for AI Image Generation. In HCI International
    2023 Posters, Communications in Computer and Information Science, pages 405–410,
    Cham, 2023\. Springer Nature Switzerland.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fromkin et al. [2018] Victoria Fromkin, Robert Rodman, and Nina Hyams. An Introduction
    to Language. Cengage Learning, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gajula [2023] Nagaraju Gajula. A Guide to Prompt Engineering in Large Language
    Models. https://www.latentview.com, August 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'GeeksforGeeks [2023] GeeksforGeeks. Natural Language Processing(NLP) VS Programming
    Language. https://www.geeksforgeeks.org, December 2023. Section: Python.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gruber [2012] John Gruber. Markdown: Syntax. http://daringfireball.net/projects/markdown,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grune et al. [2012] Dick Grune, Kees Van Reeuwijk, Henri E Bal, et al. Modern
    compiler design. Springer Science & Business Media, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. [2023] Qingyan Guo, Rui Wang, Junliang Guo, et al. Connecting Large
    Language Models with Evolutionary Algorithms Yields Powerful Prompt Optimizers,
    September 2023. arXiv:2309.08532 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. [2022] Yaru Hao, Zewen Chi, Li Dong, and Furu Wei. Optimizing Prompts
    for Text-to-Image Generation, December 2022. arXiv:2212.09611 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hong et al. [2023] Sirui Hong, Mingchen Zhuge, Jonathan Chen, et al. MetaGPT:
    Meta Programming for A Multi-Agent Collaborative Framework, November 2023. arXiv:2308.00352
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023a] Chengzhengxu Li, Xiaoming Liu, Yichen Wang, et al. Dialogue
    for Prompting: a Policy-Gradient-Based Discrete Prompt Optimization for Few-shot
    Learning, August 2023. arXiv:2308.07272 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. [2023b] Yunxiang Li, Zihan Li, Kai Zhang, et al. Chatdoctor: A medical
    chat model fine-tuned on a large language model meta-ai (llama) using medical
    domain knowledge. Cureus, 15(6), 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu and Chilton [2022] Vivian Liu and Lydia B Chilton. Design Guidelines for
    Prompt Engineering Text-to-Image Generative Models. In Proceedings of the 2022
    CHI Conference on Human Factors in Computing Systems, CHI ’22, pages 1–23, New
    York, NY, USA, April 2022\. Association for Computing Machinery.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2023] Yuxuan Liu, Tianchi Yang, Shaohan Huang, et al. Calibrating
    LLM-Based Evaluator, September 2023. arXiv:2309.13308 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lutz [2010] Mark Lutz. Programming Python: Powerful Object-Oriented Programming.
    ”O’Reilly Media, Inc.”, December 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meskó [2023] Bertalan Meskó. Prompt Engineering as an Important Emerging Skill
    for Medical Professionals: Tutorial. Journal of Medical Internet Research, 25(1):e50638,
    October 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mund [2023] Shritam Kumar Mund. The AI War: Mastering the Art of Prompt Engineering
    in the Era of Large Language Models. https://ai.plainenglish.io, February 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nigh [2023] Matt Nigh. ChatGPT3-Free-Prompt-List: A free guide for learning
    to create ChatGPT3 Prompts. https://github.com/mattnigh/ChatGPT3-Free-Prompt-List,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Park et al. [2023] Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, et al. Generative
    Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual
    ACM Symposium on User Interface Software and Technology, UIST ’23, pages 1–22,
    New York, NY, USA, October 2023\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pezoa et al. [2016] Felipe Pezoa, Juan L. Reutter, Fernando Suarez, et al. Foundations
    of JSON Schema. In Proceedings of the 25th International Conference on World Wide
    Web, WWW ’16, pages 263–273, Republic and Canton of Geneva, CHE, April 2016\.
    International World Wide Web Conferences Steering Committee.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pratt et al. [1984] Terrence W Pratt, Marvin V Zelkowitz, and Tadepalli V Gopal.
    Programming languages: design and implementation. Technical report, Prentice-Hall
    Englewood Cliffs, NJ, 1984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pryzant et al. [2023] Reid Pryzant, Dan Iter, Jerry Li, et al. Automatic Prompt
    Optimization with ”Gradient Descent” and Beam Search, October 2023. arXiv:2305.03495
    [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ren et al. [2023] Feiliang Ren, Jiaqi Wang, Yuying Chang, and Zhong Li. Techgpt
    2.0: Technology-oriented generative pretrained transformer 2.0. [https://github.com/neukg/TechGPT-2.0](https://github.com/neukg/TechGPT-2.0),
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rentsch [1982] Tim Rentsch. Object oriented programming. ACM SIGPLAN Notices,
    17(9):51–57, September 1982.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schulman et al. [2023] John Schulman, Barret Zoph, Christina Kim, et al. Introducing
    ChatGPT. https://openai.com/blog/chatgpt, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sebesta [2012] Robert W Sebesta. Concepts of programming languages. Pearson
    Education, Inc, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sipser [1996] Michael Sipser. Introduction to the Theory of Computation. ACM
    Sigact News, 27(1):27–29, 1996. Publisher: ACM New York, NY, USA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2021] Yu Sun, Shuohuan Wang, Shikun Feng, et al. ERNIE 3.0: Large-scale
    Knowledge Enhanced Pre-training for Language Understanding and Generation, July
    2021. arXiv:2107.02137 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. [2023a] Hong Sun, Xue Li, Yinchuan Xu, et al. AutoHint: Automatic
    Prompt Optimization with Hint Generation, August 2023. arXiv:2307.07415 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2023b] Jiankai Sun, Chuanyang Zheng, Enze Xie, et al. A Survey of
    Reasoning with Foundation Models, December 2023. arXiv:2312.11562 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. [2023c] Jiao Sun, Yufei Tian, Wangchunshu Zhou, et al. Evaluating
    Large Language Models on Controlled Generation Tasks, October 2023. arXiv:2310.14542
    [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Varshney and Surla [2023] Tanay Varshney and Annie Surla. An Introduction to
    Large Language Models: Prompt Engineering and P-Tuning. https://developer.nvidia.com,
    April 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023a] Haochun Wang, Chi Liu, Nuwa Xi, et al. HuaTuo: Tuning LLaMA
    Model with Chinese Medical Knowledge, April 2023. arXiv:2304.06975 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2023b] Xinyuan Wang, Chenxi Li, Zhen Wang, et al. PromptAgent:
    Strategic Planning with Language Models Enables Expert-level Prompt Optimization,
    December 2023. arXiv:2310.16427 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang [2023] Zhenxuan Wang. How to use prompt engineering with large language
    models. https://www.thoughtworks.com, August 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. [2023] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian
    Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-Thought Prompting Elicits
    Reasoning in Large Language Models, January 2023. arXiv:2201.11903 [cs].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. [2023] Qingyun Wu, Gagan Bansal, Jieyu Zhang, et al. AutoGen: Enabling
    Next-Gen LLM Applications via Multi-Agent Conversation, October 2023. arXiv:2308.08155
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xi et al. [2023] Zhiheng Xi, Wenxiang Chen, Xin Guo, et al. The Rise and Potential
    of Large Language Model Based Agents: A Survey, September 2023. arXiv:2309.07864
    [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. [2023] Yuzhuang Xu, Shuo Wang, Peng Li, et al. Exploring Large Language
    Models for Communication Games: An Empirical Study on Werewolf, September 2023.
    arXiv:2309.04658 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. [2023] Aiyuan Yang, Bin Xiao, Bingning Wang, et al. Baichuan 2:
    Open Large-scale Language Models, September 2023. arXiv:2309.10305 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yeh et al. [2022] Hui-Syuan Yeh, Thomas Lavergne, and Pierre Zweigenbaum. Decorate
    the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction,
    April 2022. arXiv:2204.10360 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. [2023] Yue Yu, Yuchen Zhuang, Jieyu Zhang, et al. Large Language
    Model as Attributed Training Data Generator: A Tale of Diversity and Bias, October
    2023. arXiv:2306.15895 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zamfirescu-Pereira et al. [2023] J.D. Zamfirescu-Pereira, Richmond Y. Wong,
    Bjoern Hartmann, and Qian Yang. Why Johnny Can’t Prompt: How Non-AI Experts Try
    (and Fail) to Design LLM Prompts. In Proceedings of the 2023 CHI Conference on
    Human Factors in Computing Systems, CHI ’23, pages 1–21, New York, NY, USA, April
    2023\. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. [2023] Aohan Zeng, Xiao Liu, Zhengxiao Du, et al. GLM-130B: An
    Open Bilingual Pre-trained Model, October 2023. arXiv:2210.02414 [cs].'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. [2023] Yiqun Zhang, Jingqing Zhang, Yongkang Liu, et al. PICA:
    Unleashing The Emotional Power of Large Language Model. https://github.com/NEU-DataMining/PICA,
    July 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
