- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:17'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and
    Prompt Tuning with LLMs'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19616](https://ar5iv.labs.arxiv.org/html/2407.19616)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \NewDocumentCommand\vect
  prefs: []
  type: TYPE_NORMAL
- en: 'O O m #3^(#1)_#2 \NewDocumentCommand\mat O O m #3^(#1)_#2 \NewDocumentCommand\ten
    O O m #3^(#1)_#2'
  prefs: []
  type: TYPE_NORMAL
- en: Selma Wanna ,  Nicholas Solovyev Advanced Research in Cyber SystemsTheoretical
    Division
  prefs: []
  type: TYPE_NORMAL
- en: Los Alamos National LaboratoryUSA ,  Ryan Barron ,  Maksim E. Eren Theoretical
    DivisionAdvanced Research in Cyber Systems
  prefs: []
  type: TYPE_NORMAL
- en: Los Alamos National LaboratoryUSA ,  Manish Bhattarai ,  Kim Ø. Rasmussen  and 
    Boian S. Alexandrov Theoretical Division
  prefs: []
  type: TYPE_NORMAL
- en: Los Alamos National LaboratoryUSA(20xx)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Topic modeling is a technique for organizing and extracting themes from large
    collections of unstructured text. Non-negative matrix factorization (NMF) is a
    common unsupervised approach that decomposes a term frequency-inverse document
    frequency (TF-IDF) matrix to uncover latent topics and segment the dataset accordingly.
    While useful for highlighting patterns and clustering documents, NMF does not
    provide explicit topic labels, necessitating subject matter experts (SMEs) to
    assign labels manually. We present a methodology for automating topic labeling
    in documents clustered via NMF with automatic model determination (NMFk). By leveraging
    the output of NMFk and employing prompt engineering, we utilize large language
    models (LLMs) to generate accurate topic labels. Our case study on over 34,000
    scientific abstracts on Knowledge Graphs demonstrates the effectiveness of our
    method in enhancing knowledge management and document organization.
  prefs: []
  type: TYPE_NORMAL
- en: 'nmf, topic labeling, llm, chain of thought, prompt tuning^†^†copyright: acmlicensed^†^†journalyear:
    20xx^†^†doi: XXXXXXX.XXXXXXX^†^†conference: The 24th ACM Symposium on Document
    Engineering; August 20–23, 2024; San Jose, CA, USA^†^†isbn: 978-1-4503-XXXX-X/18/06^†^†ccs:
    Computing methodologies Natural language generation'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The rapid growth of digital text data has necessitated the development of advanced
    techniques for organizing, cataloging, and extracting information from large datasets.
    Topic modeling, a powerful document engineering technique, has been widely used
    to segment large datasets into manageable clusters of related documents. Another
    critical task in document organization is assigning labels that summarize the
    themes of these topics. Traditionally, this has been done by subject-matter experts
    (SMEs) through a manual and time-consuming investigation of each topic. In this
    work, we introduce a novel, automated topic labeling technique that leverages
    patterns obtained through dimensionality reduction for prompt-tuning, utilizing
    Large Language Models (LLMs).
  prefs: []
  type: TYPE_NORMAL
- en: One common approach to topic modeling is through non-negative matrix factorization
    (NMF) of the term frequency-inverse document frequency (TF-IDF) matrix. Given
    a TF-IDF matrix, $\mat{X}\in\rm I\!R_{+}^{m\times n}$ will result in noisy topics
    (over-fitting). In this work, we use NMF with automatic model determination (NMFk)¹¹1NMFk
    is available in [https://github.com/lanl/T-ELF](https://github.com/lanl/T-ELF).
    to heuristically estimate the number of topics ($k$) (Eren et al., [2022](#bib.bib7),
    [2023](#bib.bib6)). Despite its effectiveness, NMFk does not inherently provide
    labels for the discovered topics, necessitating the involvement of SMEs to review
    the latent factors, clustered documents, and other derivatives of the factorization
    such as word-clouds to assign meaningful topic labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/37e5d3801a1660292c26123de59dd57c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The TopicTag pipeline. Stage 1 (left) illustrates the prompt-optimization
    framework applied to our training set of topic clusters. Documents are processed
    by the NMFk algorithm to generate feature information, which is then integrated
    into prompts. These prompts are evaluated by LLMs, with their label predictions
    compared against ground truth labels. Prompts are refined by maximizing NLG or
    human rater scores. In Stage 2, the optimal prompts are assessed on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Our approach uniquely integrates the output of an NMFk decomposition with prompt
    engineering, chain of thought prompting, and Optuna (Akiba et al., [2019](#bib.bib3))
    for prompt tuning to generate accurate and descriptive topic labels, significantly
    reducing the need for SME intervention. This paper details our methodology and
    demonstrates its efficacy through a case study on over 34,000 abstracts of scientific
    literature on Knowledge Graphs (KG). The results highlight the potential of our
    method to enhance knowledge management and information discovery, making it a
    valuable tool for organizing and understanding large text datasets across various
    domains.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Relevant Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The outputs of topic modeling are typically labeled by an SME using some top-ranked
    vocabulary based on the conditional probability $p(w_{i}\mid k)$ (Blei et al.,
    [2003](#bib.bib4)). This approach is both exhausting and prone to error. It demands
    that an SME in the target domain quickly analyze a topic cluster’s top vocabulary
    and produce a label. Even with a highly experienced SME, the process is tedious
    and is susceptible to subjectivity. To this end, automatic topic label generation
    has been a long standing problem.
  prefs: []
  type: TYPE_NORMAL
- en: One common method of creating topic labels is to extract many candidate labels
    from inputs such as documents titles, frequent phrases, and the top vocabulary
    and then select the best candidate by maximizing mutual information between a
    label and the topic model (Mei et al., [2007](#bib.bib15); Kou et al., [2015](#bib.bib12);
    Cano Basave and Xu, [2014](#bib.bib5)). Other approaches follow a similar extractive
    candidate selection and evaluation paradigm but incorporate Information Retrieval
    to enrich the source of potential candiate labels (Giorgi et al., [2023](#bib.bib10)).
    More recently, abstractive summarization tasks using deep learning techniques
    have attracted considerable research attention (Zakkas et al., [2024](#bib.bib20);
    Ma et al., [2020](#bib.bib14); Ghadimi and Beigy, [2022](#bib.bib9)). These methods
    can provide detailed summaries that offer more comprehensive insights into topics
    but come with certain trade-offs. Generating and verifying the relevance and accuracy
    of long-form summaries for the multiple documents composing a topic poses a significant
    challenge (Giorgi et al., [2023](#bib.bib10)). Conversely, short labels often
    generalize better and provide a quick, at-a-glance overview of the topic. In the
    rest of this paper we show how the latent features from NMFk topic modeling can
    be used with prompt engineering to produce high quality topic labels.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we define the labeling task and outline the construction of
    the TopicTag pipeline, as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ TopicTag: Automatic Annotation of NMF Topic Models Using Chain of Thought and
    Prompt Tuning with LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Task Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several criterion must be met for a topic label to be effective. This criterion
    can be summed up as a maximization of relevance, coverage, and discrimination
    for topic label $t\in T$ (Wan and Wang, [2016](#bib.bib18)). High relevance is
    essential; the label must be semantically aligned with the topic and closely related
    to all representative documents. Secondly, high coverage requires that the label
    should encapsulate as much semantic information about the topic as possible and
    should at a high level be applicable to all documents in the topic. Additionally,
    when creating a set of labels for all topics identified in a document collection,
    high discrimination is necessary. Labels for different topics must exhibit inter-topic
    discrimination to help users understand each topic. If labels for multiple topics
    are too similar, users may find it difficult to distinguish between them. Therefore,
    the higher the inter-topic discrimination, the better the labels.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Prompt
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our experiments, we focus on developing zero-shot prompts, comprising a system
    and task instruction for the LLM. To mitigate the high context window space required
    by few-shot examples, we employ various Chain-of-Thought templates (Wei et al.,
    [2022](#bib.bib19)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate four LLMs in this study. Initially, Mistral-7B-
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruct-v0.2 (Jiang et al., [2023](#bib.bib11)) was employed for efficient
    prompt engineering and optimization, based on the premise that effective prompts
    would generalize across varying model scales and architectures. Subsequently,
    the optimized prompt derived from Mistral-7B-Instruct-v0.2 was used to initiate
    the prompt engineering process for four other LLMs: Mistral-7B-Instruct-v0.3,
    Meta-'
  prefs: []
  type: TYPE_NORMAL
- en: Llama-3-8B-Instruct (AI@Meta, [2024](#bib.bib2)), Meta-Llama-3-70B-Instruct (AI@Meta,
    [2024](#bib.bib2)), and gpt-4o.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For each topic model cluster, we developed manually assigned SME labels to serve
    as ground truths. During optimization, to assess the quality of the LLM responses,
    we utilized several Natural Language Generation (NLG) metrics, including BERTScore (Zhang*
    et al., [2020](#bib.bib21)), BLEU (Papineni et al., [2002](#bib.bib16)), and ROUGE (Lin,
    [2004](#bib.bib13)). For the second stage of LLM filtering, responses were qualitatively
    evaluated using a 3-point scoring system based on the concept of ”relevance.”
    This concept, as defined by text summarization researchers (Ernst et al., [2023](#bib.bib8)),
    examines whether the most salient concepts from the text are accurately represented.
    For our final evaluation, we employ a 5-point version of this metric.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Prompt Filtering as Prompt Engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ TopicTag: Automatic Annotation
    of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs") provides
    an overview of our TopicTag pipeline. The initial stage involves processing the
    output of our NMFk model. Each $n$ can be processed to extract the top m tokens
    for each cluster. This post-processing then yields the top titles, abstracts,
    keywords, n-grams for a given topic. These features are assessed for their suitability
    in the topic labeling task by integrating them into various Chain-of-Thought prompt
    templates (Wei et al., [2022](#bib.bib19)).'
  prefs: []
  type: TYPE_NORMAL
- en: To identify the most effective features, we employ a two-step filtering algorithm
    that compares the responses of an LLM against ground-truth labels assigned to
    each topic model cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In the first step, an automated coarse-grain search is conducted using the Tree-structured
    Parzen Estimator (TPE) sampling algorithm from Optuna (Akiba et al., [2019](#bib.bib3)).
    This algorithm explores the document feature space (such as top SME keywords,
    top words, top n-grams, top titles, document coordinates, TF-IDF weights, etc.),
    obtained from NMFk, and hyperparameter configurations of Mistral-7B-Instruct-v0.2
    to maximize BERTScore. This step typically involves 2-3 iterations to find features
    that significantly reduce the BERTScore, allowing us to narrow our search space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step involves qualitative filtering through SME preference ratings.
    Evaluators categorize responses as bad, okay, or good. Good responses are further
    analyzed to identify commonalities in prompting features and LLM generation hyperparameters.
    This step is repeated 3-4 times to refine the prompts, which are then used to
    guide the search for additional LLM prompts (see Sec. [3.3](#S3.SS3 "3.3\. Models
    ‣ 3\. Methods ‣ TopicTag: Automatic Annotation of NMF Topic Models Using Chain
    of Thought and Prompt Tuning with LLMs")). The qualitative filtering process is
    repeated an additional 1-4 times to ensure the selection of ideal prompts for
    the new models.'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section details the experimental setups and results of two studies. The
    first study examines the correlation between human rater agreement and traditional
    NLG metrics. The second study investigates the factors contributing to more successful
    prompts and identifies the best-performing LLM and prompt combination.
  prefs: []
  type: TYPE_NORMAL
- en: The experimental dataset and its decomposition come from a previously collected
    set of documents for another work that focuses on knowledge graphs and knowledge
    base management. For both studies, we implemented a train/test split on the 28
    topics generated by the NMFk. The training set, on which we applied our TopicTag
    pipeline, comprises the first seven topics produced by NMFk. The remaining 21
    topics serve as the evaluation set.
  prefs: []
  type: TYPE_NORMAL
- en: Given the extensive output space encompassing document features, prompt templates,
    and LLM hyperparameter configurations, we generated approximately 840 LLM outputs.
    From these, we randomly sampled roughly 160 responses for manual analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '4.1\. Experiment 1: NLG Metric Study'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this study, we enlisted seven raters (authors of this paper) to score the
    outputs of LLMs and measure the correlation between NLG metrics and human preferences.
    Scores were reported on a 5-point scale. Table [1](#S4.T1 "Table 1 ‣ 4.1\. Experiment
    1: NLG Metric Study ‣ 4\. Results ‣ TopicTag: Automatic Annotation of NMF Topic
    Models Using Chain of Thought and Prompt Tuning with LLMs") summarizes these results.
    Overall, the studied metrics do not exhibit strong correlations with human judgments.
    Due to its weak signal, we have omitted the BLEU score from our results. ROUGE-L
    shows the highest correlation with human judgments; however, an average $r^{2}=0.139$
    still indicates poor performance. Given our scores are not correlated with human
    SME raters, we employ these metrics solely to filter out egregiously incorrect
    responses. This highlights the complexity of evaluating topic labels that may
    not be prominent in a cluster’s vocabulary. Unlike translation and summarization,
    current NLG metrics fail to capture the trade-offs between granularity and broadness
    in labeling. Therefore, there is a need for more robust alternatives that can
    capture the nuances of SME labeling in future work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1\. We report the Inter-Annotator Agreement (IAA) as Cohen’s Kappa. Our
    scores are consistent with ”fair” agreement. We also report the $r^{2}$ values
    between human rater scores and NLG metrics: BERTScore and ROUGE-L.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Annotators | IAA | $r^{2}$: ROUGE-L |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Group 1 | 0.459 | 0.037 | 0.112 |'
  prefs: []
  type: TYPE_TB
- en: '| Group 2 | 0.419 | 0.081 | 0.204 |'
  prefs: []
  type: TYPE_TB
- en: '| Group 3 | 0.369 | 0.156 | 0.127 |'
  prefs: []
  type: TYPE_TB
- en: '| Group 4 | 0.425 | 0.037 | 0.112 |'
  prefs: []
  type: TYPE_TB
- en: Despite ROUGE-L’s better performance, its n-gram nature does not provide a smooth
    score suitable for the TPE optimizer to effectively search over. Therefore, we
    leverage BERTScore at the initial stage of our optimization to eliminate document
    information that results in substantially poor prompts. In practice, features
    frequently filtered out included TF-IDF scores on keyword information, bi- and
    tri-gram information, and $\mat{H}$ coordinates distances between a document and
    their topic’s centroid. However, ordering abstracts and title information based
    on distance to the centroid did enhance LLM generation quality.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2\. Experiment 2: Evaluating LLM Topic Labels'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 2\. Qualitative results comparing LLM label predictions against SME ground
    truths.
  prefs: []
  type: TYPE_NORMAL
- en: '| Topic # | SME Rating | Ground Truth | Llama-3-8B-Instruct |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 5 | ”Domain Ontology Construction” | ”Ontology Construction Management
    and Extraction” |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 5 | ”Natural Language Processing” | ”Natural Language Processing and
    Formal Language Theory” |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 4 | ”Graph Embeddings” | ”Graph Neural Networks for Node Classification”
    |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 3 | ”Dependency Relation Extraction” | ”Knowledge Graph Completion and
    Relation Extraction” |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 2 | ”Applications and Use Cases of GNNs” | ”Machine Learning Applications
    in Science” |'
  prefs: []
  type: TYPE_TB
- en: '| 26 | 2 | ”Knowledge Base Construction” | ”Construction Project Management”
    |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 1 | ”Semantic Knowledge Representation” | ”Chinese Lexical Semantics”
    |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/5db49781b2fc124fabb2893a4b3a22ac.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2\. The average performance for each tested LLM marginalized over our
    test set, prompt templates, document features, and LLM hyperparameter configurations.
    We believe these variations in conjunction with the reported IAA in Table [1](#S4.T1
    "Table 1 ‣ 4.1\. Experiment 1: NLG Metric Study ‣ 4\. Results ‣ TopicTag: Automatic
    Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs")
    lead to the reported variance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This study evaluates the efficacy of the TopicTag pipeline using human rater
    data collected in Section [4.1](#S4.SS1 "4.1\. Experiment 1: NLG Metric Study
    ‣ 4\. Results ‣ TopicTag: Automatic Annotation of NMF Topic Models Using Chain
    of Thought and Prompt Tuning with LLMs") and summarized in Figure [2](#S4.F2 "Figure
    2 ‣ 4.2\. Experiment 2: Evaluating LLM Topic Labels ‣ 4\. Results ‣ TopicTag:
    Automatic Annotation of NMF Topic Models Using Chain of Thought and Prompt Tuning
    with LLMs"). The results demonstrate that our approach generalizes well across
    LLMs of varying architecture and size. Notably, Meta-Llama-3-8B-Instruct was the
    top-performing model with an average score of 3.78\. Examples of Meta-Llama-3-8B-Instruct’s
    predicted topic labels are reported in Table [2](#S4.T2 "Table 2 ‣ 4.2\. Experiment
    2: Evaluating LLM Topic Labels ‣ 4\. Results ‣ TopicTag: Automatic Annotation
    of NMF Topic Models Using Chain of Thought and Prompt Tuning with LLMs"). Mistral-7B-Instruct-v0.3
    had the lowest performance with an average score of 3.19\. However, the difference
    between these averages is 0.59, which is less than a single point on our 5-point
    scale, perhaps indicating a relatively minor distinction. An important consideration
    is the relative optimization speed of smaller models compared to larger ones.
    Due to time and resource constraints, we may have undertuned Meta-Llama-3-70B-Instruct
    and gpt-4o, applying only 1-2 optimization steps, whereas smaller models underwent
    3-4 optimization steps. This difference in optimization effort may have impacted
    the performance outcomes of the larger models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From our analysis, we determined that using the following CoT prompt template:
    "You are a document understander. Using your expertise, label this topic cluster
    by thinking step-by-step:\nStep 1: Review the document information and make four
    guesses on the topic label.\nStep 2: Review the top words and refine each response.
    \nStep 3: Choose the best answer from your guesses and format it like so: <<[ANSWER]>>.\nHere
    is the Document Information:" with the following features: 3 sampled words from
    the top abstracts; sampling from the top 4 titles; and sampling 5 words from the
    top 8 words, produced the best labels with Meta-Llama-3- 8B-Instruct for our knowledge
    graph dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This case study demonstrates that our TopicTag algorithm can effectively generalize
    in-domain to provide concise label summarizations for document clusters. This
    is achieved by incorporating the outputs of NMFk as document features within LLM
    prompts. Llama-3-8B-Instruct exhibited the best performance (with an average SME
    rating of 3.78 out of a 5-point scale); however, it is noteworthy that smaller
    models underwent more rounds of optimization. This finding is significant as it
    suggests that a less computationally intensive and less expensive model can be
    tuned to outperform state-of-the-art models for this task.
  prefs: []
  type: TYPE_NORMAL
- en: This area of research is promising yet underexplored, particularly in terms
    of aligning automated optimization processes with stronger NLG metrics. Future
    work could address this by training an additional LLM to generate task-specific
    embeddings for labeling, using these embeddings in a manner similar to BERTScore.
    Alternatively, investigating inter-annotator agreement (IAA) variance could enhance
    reliability, enabling the application of Reinforcement Learning from Human Feedback
    (RLHF) for labeling. This approach aligns with other studies highlighting the
    importance of human preference ratings in achieving superior NLP task performance.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This manuscript has been assigned LA-UR-24-25486\. This research was funded
    by the LANL LDRD grant 20230067DR and the LANL Institutional Computing Program,
    supported by the U.S. DOE NNSA under Contract No. 89233218CNA000001.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AI@Meta (2024) AI@Meta. 2024. Llama 3 Model Card. (2024). [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Akiba et al. (2019) Takuya Akiba, Shotaro Sano, et al. 2019. Optuna: A next-generation
    hyperparameter optimization framework. In *Proceedings of the 25th ACM SIGKDD
    international conference on knowledge discovery & data mining*. 2623–2631.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blei et al. (2003) David M. Blei, Andrew Y. Ng, and Michael I. Jordan. 2003.
    Latent dirichlet allocation. *J. Mach. Learn. Res.* 3, null (mar 2003), 993–1022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cano Basave and Xu (2014) Amparo Cano Basave and Ruifeng Xu. 2014. Automatic
    Labelling of Topic Models Learned from Twitter by Summarisation. [https://doi.org/10.3115/v1/P14-2101](https://doi.org/10.3115/v1/P14-2101)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eren et al. (2023) Maksim Eren, Nick Solovyev, Ryan Barron, et al. 2023. Tensor
    Extraction of Latent Features (T-ELF). [https://doi.org/10.5281/zenodo.10257897](https://doi.org/10.5281/zenodo.10257897)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eren et al. (2022) Maksim E. Eren, Nick Solovyev, Manish Bhattarai, et al.
    2022. SeNMFk-SPLIT: Large Corpora Topic Modeling by Semantic Non-Negative Matrix
    Factorization with Automatic Model Selection. In *Proceedings of the 22nd ACM
    Symposium on Document Engineering* (San Jose, California) *(DocEng ’22)*. Association
    for Computing Machinery, New York, NY, USA, Article 10, 4 pages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ernst et al. (2023) Ori Ernst, Ori Shapira, et al. 2023. Re-Examining Summarization
    Evaluation across Multiple Quality Criteria. In *Findings of the Association for
    Computational Linguistics: EMNLP 2023*, Houda Bouamor, Juan Pino, and Kalika Bali
    (Eds.). Association for Computational Linguistics, Singapore, 13829–13838. [https://doi.org/10.18653/v1/2023.findings-emnlp.924](https://doi.org/10.18653/v1/2023.findings-emnlp.924)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ghadimi and Beigy (2022) Alireza Ghadimi and Hamid Beigy. 2022. Hybrid multi-document
    summarization using pre-trained language models. *Expert Systems with Applications*
    192 (2022), 116292. [https://doi.org/10.1016/j.eswa.2021.116292](https://doi.org/10.1016/j.eswa.2021.116292)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Giorgi et al. (2023) John Giorgi, Luca Soldaini, Bo Wang, Gary Bader, et al.
    2023. Open Domain Multi-document Summarization: A Comprehensive Study of Model
    Brittleness under Retrieval. arXiv:2212.10526 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jiang et al. (2023) Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,
    et al. 2023. Mistral 7B. arXiv:2310.06825 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kou et al. (2015) Wanqiu Kou, Fang Li, and Timothy Baldwin. 2015. Automatic
    Labelling of Topic Models Using Word Vectors and Letter Trigram Vectors. In *Information
    Retrieval Technology*, Guido Zuccon, Shlomo Geva, Hideo Joho, Falk Scholer, Aixin
    Sun, and Peng Zhang (Eds.). Springer International Publishing, Cham, 253–264.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin (2004) Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of
    Summaries. In *Text Summarization Branches Out*. Association for Computational
    Linguistics, Barcelona, Spain, 74–81. [https://aclanthology.org/W04-1013](https://aclanthology.org/W04-1013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2020) Congbo Ma, Wei Emma Zhang, Mingyu Guo, et al. 2020. Multi-document
    Summarization via Deep Learning Techniques: A Survey. *CoRR* abs/2011.04843 (2020).
    arXiv:2011.04843 [https://arxiv.org/abs/2011.04843](https://arxiv.org/abs/2011.04843)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mei et al. (2007) Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai. 2007. Automatic
    labeling of multinomial topic models. In *Proceedings of the 13th ACM SIGKDD International
    Conference on Knowledge Discovery and Data Mining* (San Jose, California, USA)
    *(KDD ’07)*. Association for Computing Machinery, New York, NY, USA, 490–499.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, et al. 2002. Bleu: a
    Method for Automatic Evaluation of Machine Translation. In *Proceedings of the
    40th Annual Meeting of the Association for Computational Linguistics*, Pierre
    Isabelle, Eugene Charniak, and Dekang Lin (Eds.). Association for Computational
    Linguistics, Philadelphia, Pennsylvania, USA, 311–318. [https://doi.org/10.3115/1073083.1073135](https://doi.org/10.3115/1073083.1073135)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vangara et al. (2021) Raviteja Vangara, Manish Bhattarai, Erik Skau, Gopinath
    Chennupati, et al. 2021. Finding the Number of Latent Topics With Semantic Non-Negative
    Matrix Factorization. *IEEE Access* 9 (2021), 117217–117231.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan and Wang (2016) Xiaojun Wan and Tianming Wang. 2016. Automatic Labeling
    of Topic Models Using Text Summaries. In *Proceedings of the 54th Annual Meeting
    of the Association for Computational Linguistics (Volume 1: Long Papers)*, Katrin
    Erk and Noah A. Smith (Eds.). Association for Computational Linguistics, Berlin,
    Germany, 2297–2305. [https://doi.org/10.18653/v1/P16-1217](https://doi.org/10.18653/v1/P16-1217)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, et al. 2022. Chain of Thought Prompting
    Elicits Reasoning in Large Language Models. In *Advances in Neural Information
    Processing Systems*, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
    Cho (Eds.). [https://openreview.net/forum?id=_VjQlMeSB_J](https://openreview.net/forum?id=_VjQlMeSB_J)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zakkas et al. (2024) Pavlos Zakkas, Suzan Verberne, and Jakub Zavrel. 2024.
    SumBlogger: Abstractive Summarization of Large Collections of Scientific Articles.
    In *Advances in Information Retrieval*, Nazli Goharian, Nicola Tonellotto, Yulan
    He, et al. (Eds.). Springer Nature Switzerland, Cham, 371–386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang* et al. (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. BERTScore: Evaluating Text Generation with BERT. In *International
    Conference on Learning Representations*. [https://openreview.net/forum?id=SkeHuCVFDr](https://openreview.net/forum?id=SkeHuCVFDr)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
