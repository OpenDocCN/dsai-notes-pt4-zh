- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:51:24'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.09067](https://ar5iv.labs.arxiv.org/html/2305.09067)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Xiaoying Zhang¹, Baolin Peng^(2∗), Kun Li¹, Jingyan Zhou¹, Helen Meng¹
  prefs: []
  type: TYPE_NORMAL
- en: ¹The Chinese University of Hong Kong, Hong Kong
  prefs: []
  type: TYPE_NORMAL
- en: ²Microsoft Research, Redmond
  prefs: []
  type: TYPE_NORMAL
- en: '{zhangxy, kunli, jyzhou, hmmeng}@se.cuhk.edu.hk'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Building end-to-end task bots and maintaining their integration with new functionalities
    using minimal human efforts is a long-standing challenge in dialog research. Recently
    large language models (LLMs) have demonstrated exceptional proficiency in conversational
    engagement and adherence to instructions across various downstream tasks. In this
    work, we introduce SGP-TOD, S̱chema-G̱uided P̱rompting for building Ṯask-O̱riented
    Ḏialog systems effortlessly based on LLMs. Utilizing the symbolic knowledge –
    task schema, we instruct fixed LLMs to generate appropriate responses on novel
    tasks, circumventing the need for training data. Specifically, SGP-TOD comprises
    three components: a LLM for engaging with users, a DST Prompter to aid the LLM
    with dialog state tracking, which is then used to retrieve database items, and
    a Policy Prompter to elicit proper responses adhering to the provided dialog policy.
    Experimental results on Multiwoz, RADDLE and STAR datasets show that our training-free
    strategy SGP-TOD, without any task-specific data, yields state-of-the-art (SOTA)
    zero-shot performance, greatly surpasses the few-shot approaches. In a domain-extension
    setting, SGP-TOD aptly adapts to new functionalities by merely adding supplementary
    schema rules. We make our code and data publicly available.^†^†^∗Currently at
    Tencent AI Lab. Work done at Microsoft Research.    ¹¹1[https://github.com/zhangxy-2019/sgp-tod](https://github.com/zhangxy-2019/sgp-tod)
    Preprint. Work in progress.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/decb35adcea03de94202bbcfdf17aa01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The proposed SGP-TOD is depicted with a dialog example, where the
    prompters integrate task schema (right) to assist the frozen LLM in generating
    an appropriate response (left).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Building task-oriented dialog (TOD) systems has been a long-standing challenge
    in artificial intelligence. The prevailing approach for creating task bots Hosseini-Asl
    et al. ([2020](#bib.bib14)); Peng et al. ([2021a](#bib.bib34)); Sun et al. ([2022](#bib.bib49))
    is to fine-tune pre-trained language models (PLMs), such as T5 Raffel et al. ([2020](#bib.bib40))
    and GPT-2 Radford et al. ([2019](#bib.bib39)). Despite their great success, developing
    and maintaining such task bots generally requires adequate annotated data and
    extensive fine-tuning/re-training. Recently, large Language Models (LLMs), such
    as ChatGPT²²2[https://chat.openai.com/](https://chat.openai.com/) and GPT-4 OpenAI
    ([2023](#bib.bib30)), have revolutionized natural language processing (NLP) applications
    Wang et al. ([2023](#bib.bib50)); Wei et al. ([2022](#bib.bib53)), owing to their
    remarkable conversational skills Qin et al. ([2023](#bib.bib38)), instruction-following
    abilities Ouyang et al. ([2022](#bib.bib31)) and zero-shot generalization capabilities Chowdhery
    et al. ([2022a](#bib.bib7)); Hu et al. ([2022](#bib.bib15)). This raises a research
    question: can LLMs be effectively utilized for building task bots with minimum
    human efforts?'
  prefs: []
  type: TYPE_NORMAL
- en: A contemporary study Hudecek and Dusek ([2023](#bib.bib16)) explores the potential
    of LLMs for rapidly building task bots via few-shot prompting, *a.k.a.* in-context
    learning (ICL) paradigm Brown et al. ([2020](#bib.bib1)); Madotto et al. ([2021](#bib.bib25)).
    Though demonstrably effective, the ICL performance is highly influenced by the
    quality of the in-context exemplars Liu et al. ([2022](#bib.bib24)); Zhao et al.
    ([2021](#bib.bib65)); Dong et al. ([2023](#bib.bib11)), as they exhibit instability
    in conveying all requisite task instructions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we introduce the symbolic knowledge Cheng et al. ([2023](#bib.bib6));
    Nye et al. ([2021](#bib.bib29)) – task schema into LLMs for creating task bots.
    Task schema Mosig et al. ([2020](#bib.bib28)); Mehri and Eskenazi ([2021](#bib.bib27))
    encompasses a concise symbolic representation of a task, supplying LLMs with a
    comprehensive blueprint. It comprises $(\textup{\it i})$ a dialog flow explicitly
    outlining fundamental interaction patterns Peng et al. ([2021b](#bib.bib35)).
    Specifically, we propose SGP-TOD (as depicted in Figure [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM
    Prompting")), a schema-guided prompting method for rapidly building task bots.
    We integrate the predefined task schema and dialogue context into prompts through
    the use of two specifically-designed prompters, namely a DST Prompter and a Policy
    Prompter. Utilizing these prompters, we adeptly guide fixed LLMs to track dialogue
    states, retrieve database entries, and generate appropriate responses for novel
    tasks in a zero-shot manner, without the need for additional training or fine-tuning.
    By incorporating task-specific symbolic knowledge into LLMs, SGP-TOD provides
    knowledge-based, coherent and human-like responses. Moreover, this training-free
    design empowers developers to flexibly prototype dialog systems on new tasks,
    while seamlessly extending their functionalities through modifying the task schema.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conduct empirical experiments on two multi-domain datasets: Multiwoz 2.0
    and 2.2 Budzianowski et al. ([2018](#bib.bib3)); Zang et al. ([2020](#bib.bib59)),
    and two single-domain/task datasets: RADDLE Peng et al. ([2021a](#bib.bib34))
    and STAR Mosig et al. ([2020](#bib.bib28)) in zero-shot scenarios. The results
    indicate that SGP-TOD, employing merely task schema devoid of any training or
    fine-tuning, substantially boosts the SOTA zero-shot results, markedly outperforming
    few-shot prompting/fine-tuning methods, and even attaining competitive results
    with full-shot fine-tuning approaches. In a domain-extension context, SGP-TOD
    proficiently adapts to new functionalities by simply adding a handful of schema
    rules without necessitating further data collection, significantly exceeding few-shot
    prompting/fine-tuning methods reinforced by machine teaching Williams and Liden
    ([2017](#bib.bib54)).'
  prefs: []
  type: TYPE_NORMAL
- en: In summary, our contributions are three-fold.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose SGP-TOD, a schema-guided prompting-only strategy that allows effortlessly
    building task bots based on LLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We integrate symbolic knowledge – task schema into LLMs, allowing them to generate
    schema-compliant responses and adaptively expand their functionalities to tackle
    new tasks by solely modifying the task schema.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We demonstrate the effectiveness of SGP-TOD on Multiwoz, RADDLE, STAR datasets
    in zero-shot settings. SGP-TOD attains SOTA zero-shot performance, notably surpassing
    few-shot prompting/fine-tuning methods and exhibiting favorable performance compared
    to full-shot fine-tuning methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Model | Task | Schema types | Training strategy |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning | Pre-training | Prompting |'
  prefs: []
  type: TYPE_TB
- en: '| RobustSF Shah et al. ([2019](#bib.bib44)) | SF | slot names/value examples
    | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TRADE Wu et al. ([2019](#bib.bib55)) | DST | slot names/value examples |
    ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ZSTL-SD Campagna et al. ([2020](#bib.bib4)) | DST | ontology, dialog templates
    | ✓(+synthesized data) |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| S-DST Rastogi et al. ([2020a](#bib.bib41)) | DST | slot names/descriptions
    | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| +service, intent names/descriptions |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| T5DST Lin et al. ([2021b](#bib.bib21)) | DST | slot names/descriptions |
    ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| TransferQA Lin et al. ([2021a](#bib.bib20)) | DST | slot names/value examples
    |  | ✓(QA tasks) |  |'
  prefs: []
  type: TYPE_TB
- en: '| IC-DST Hu et al. ([2022](#bib.bib15)) | DST | slot names/value examples |  |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| SDM-DST Wang et al. ([2022a](#bib.bib51)) | DST | slot names/value examples
    | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Bert+S Mosig et al. ([2020](#bib.bib28)) | E2E policy | system-side policy
    skeletons | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SAM Mehri and Eskenazi ([2021](#bib.bib27)) | E2E policy | user-aware policy
    skeletons | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| ZSDG Zhao and Eskenazi ([2018](#bib.bib64)) | E2E dialog | ontology, response
    templates | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| DAML Qian and Yu ([2019](#bib.bib37)) | E2E dialog | ontology, response templates
    | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| AnyTOD Zhao et al. ([2022](#bib.bib63)) | E2E dialog | policy programs |
    ✓ | ✓(heterogeneous tasks) |  |'
  prefs: []
  type: TYPE_TB
- en: '| +slot names/value examples |  |'
  prefs: []
  type: TYPE_TB
- en: '| +slot descriptions |  |'
  prefs: []
  type: TYPE_TB
- en: '| +user action names/states/descriptions |  |'
  prefs: []
  type: TYPE_TB
- en: '| IT-LLM Hudecek and Dusek ([2023](#bib.bib16)) | E2E dialog | slot names |  |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| +slot descriptions |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD (ours) | E2E dialog | user-aware policy skeletons |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| (+slot names/value examples) |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Zero-shot task-oriented dialog modeling. (Schema items enclosed in
    parentheses are required only when accessible.)'
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Task-Oriented Dialog Modeling.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Zero-shot generalization is an essential yet challenging task in task-oriented
    dialog research. As shown in Table [1](#S2.T1 "Table 1 ‣ 2 Related work ‣ SGP-TOD:
    Building Task Bots Effortlessly via Schema-Guided LLM Prompting"), there are four
    main research directions: slot filling (SF), dialog state tracking (DST), end-to-end
    policy management (E2E policy) and end-to-end dialog generation (E2E dialog).'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we focus on zero-shot end-to-end dialog modeling, including policy
    management and dialog generation. Zhao and Eskenazi ([2018](#bib.bib64)); Qian
    and Yu ([2019](#bib.bib37)) utilize ontology³³3Ontology is a structured representation
    of the back-end database, defining all slots and their possible values Budzianowski
    et al. ([2018](#bib.bib3)). and response templates to train the dialog model ,
    enabling the discovery of shared dialog policies (*i.e.,* discourse-level patterns)
    between the source and target domains. To facilitate broader adaptation to previously
    unseen tasks or domains with diverse dialog policies, Mosig et al. ([2020](#bib.bib28));
    Mehri and Eskenazi ([2021](#bib.bib27)) implement task-specific policy skeletons,
    training dialog models to adhere to novel policies. Furthermore, Zhao et al. ([2022](#bib.bib63))
    employs a neural language model (LM) for tracking dialog states and user actions
    using slot and action descriptions; subsequently, a policy program is executed
    to recommend the next system actions; ultimately, an LM generates the final system
    action and corresponding template response. Our SGP-TOD diverges by necessitating
    fewer annotations, *i.e.,* we do not require state or action descriptions and
    integrate slot names and value examples into the task schema exclusively when
    accessible. Despite the effectiveness of previous approaches, they still require
    sufficient fine-tuning and annotated dialog corpora on source domains/tasks or
    heterogeneous tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'A concurrent study to ours is Hudecek and Dusek ([2023](#bib.bib16)), which
    employs a prompting strategy to guide frozen LLMs in generating appropriate responses.
    Specifically, Hudecek and Dusek ([2023](#bib.bib16)) first tracks belief states
    based on the dialog history by utilizing slot descriptions as prompts, then retrieves
    database entries, and generates suitable actions and responses. In contrast, our
    SGP-TOD differs in that: $(\textup{\it i})$ we offer a policy skeleton within
    the Policy Prompter to guide LLMs in producing responses that comply with the
    predefined dialog policy. In addition, experimental results indicate that SGP-TOD
    substantially outperforms IT-LLM Hudecek and Dusek ([2023](#bib.bib16)).'
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging LLMs for Dialog Tasks.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: LLMs Brown et al. ([2020](#bib.bib1)); Chowdhery et al. ([2022b](#bib.bib8));
    Chen et al. ([2021](#bib.bib5)); OpenAI ([2023](#bib.bib30)) have exhibited unparalleled
    mastery of natural language understanding, reasoning and generation Wei et al.
    ([2022](#bib.bib53)); Bubeck et al. ([2023](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: Three primary research directions have obtained substantial success in numerous
    dialog tasks by utilizing LLMs. $(\textup{\it i})$ Recently, several studies endeavor
    to support LLMs in specialized tasks by incorporating external knowledge Peng
    et al. ([2023](#bib.bib33)); Liang et al. ([2023](#bib.bib19)). Peng et al. ([2023](#bib.bib33))
    advocates for enhancing LLMs’ responses with external knowledge and automated
    feedback to reduce hallucination in their responses. Liang et al. ([2023](#bib.bib19))
    suggests connecting LLMs with millions of APIs to accomplish a variety of specialized
    tasks. Different from the aforementioned works, we aim to employ LLMs in constructing
    an end-to-end task-oriented dialog system in a zero-shot manner by using pre-defined
    task schema as prompts.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e23bda5705c984a1678624a8dda4d417.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Overview of SGP-TOD. Detailed illustration with a dialog example
    is shown in Appendix [D](#A4 "Appendix D Our Proposed SGP-TOD with a Dialog Example
    ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1 Overview
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The overall architecture of the proposed SGP-TOD (Figure [2](#S2.F2 "Figure
    2 ‣ Leveraging LLMs for Dialog Tasks. ‣ 2 Related work ‣ SGP-TOD: Building Task
    Bots Effortlessly via Schema-Guided LLM Prompting")) consists of three key components:
    $(\textup{\it i})$ a Policy Prompter, guiding the LLM to adhere to the predefined
    task policy for providing suitable system actions and responses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As depicted in Figure [2](#S2.F2 "Figure 2 ‣ Leveraging LLMs for Dialog Tasks.
    ‣ 2 Related work ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided
    LLM Prompting"), at each dialog turn $t$, and $\boldsymbol{c}_{t}$ (Equation [4](#S3.E4
    "In 3.1 Overview ‣ 3 Methodology ‣ SGP-TOD: Building Task Bots Effortlessly via
    Schema-Guided LLM Prompting")). Ultimately, the delexicalized dialog system is
    automatically post-processed to generate system response in natural language.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{b}_{t}=$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{c}_{t}=$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{a}_{t}=$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{r}_{t}=$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 3.2 LLM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A LLM is responsible for following task-specific instructions and generating
    appropriate responses.
  prefs: []
  type: TYPE_NORMAL
- en: Many off-the-shelf LLMs, *e.g.,* ChatGPT, Codex Chen et al. ([2021](#bib.bib5)),
    are pre-trained on massive corpora of text data and/or code data. In addition,
    they are trained to follow instructions in the prompts Ouyang et al. ([2022](#bib.bib31))
    and provide pertinent responses. Exhibiting remarkable proficiencies in natural
    language processing, instruction compliance, and zero-shot generalization across
    diverse downstream dialog tasks, these LLMs serve as valuable foundation models
    for our approach.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 DST Prompter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Given the dialog history $\boldsymbol{h}_{t}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{b}_{t}=\boldsymbol{d}_{t},\left\{(\boldsymbol{s}_{t}^{i},\boldsymbol{v}_{t}^{i});i=1,\dots,n_{t}\right\}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $n_{t}$ is the total number of pairs in the set.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [3](#S3.F3 "Figure 3 ‣ Belief Instruction. ‣ 3.3 DST Prompter
    ‣ 3 Methodology ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM
    Prompting"), the proposed DST prompter contains four parts: $(\textup{\it i})$.
    Since the prompt is fixed and no labeled data from the target task or domain is
    used, we refer to this setting as "zero-shot", following Wang et al. ([2022b](#bib.bib52)).'
  prefs: []
  type: TYPE_NORMAL
- en: Belief Instruction.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b6850c73798da3137f9e1bb46a3878e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of belief state prediction utilizing DST Prompter. The
    predicted belief state is highlighted.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each task/domain, the belief instruction contains the task/domain name,
    all potential slot names, and their possible values (Figure [3](#S3.F3 "Figure
    3 ‣ Belief Instruction. ‣ 3.3 DST Prompter ‣ 3 Methodology ‣ SGP-TOD: Building
    Task Bots Effortlessly via Schema-Guided LLM Prompting")). Regarding categorical
    slots, such as the "price range" in the restaurant domain, all plausible values
    are included, *i.e.,* "don’t care", "cheap", "moderate", and "expensive"; whereas,
    for non-categorical slots, such as "name", only a select few value examples are
    injected, *e.g.,* Pizza Hut City, Golden Wok, etc.⁶⁶6We assess belief instructions
    with diverse slot value examples, revealing minor performance variations. Detailed
    belief instructions for all tasks/domains can be found in Appendix [A](#A1 "Appendix
    A Detailed Belief Instructions in DST Prompter ‣ SGP-TOD: Building Task Bots Effortlessly
    via Schema-Guided LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f748f9a8deaf54cc88a8dbf1d5e46ec1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of system action determination and response generation
    employing the Policy Prompter. The pertinent template turn, previous predicted
    belief state, retrieved DB state within the input, alongside the generated system
    action and generated response in the output are accentuated.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Policy Prompter
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Dialog policy, governing the behavior of task bots, plays a crucial role in
    task-oriented dialogs Zhao et al. ([2022](#bib.bib63)). To represent the dialog
    policy for a given task, we utilize a policy skeleton, which delineates interaction
    patterns and encompasses business logic in the form of template dialog flows Peng
    et al. ([2021b](#bib.bib35)). The Policy Prompter is devised to guide the static
    LLM in adhering to the policy skeleton $\boldsymbol{PS}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Analogous to the DST Prompter, the Policy Prompter (Figure [4](#S3.F4 "Figure
    4 ‣ Belief Instruction. ‣ 3.3 DST Prompter ‣ 3 Methodology ‣ SGP-TOD: Building
    Task Bots Effortlessly via Schema-Guided LLM Prompting")) comprises four components:
    $(\textup{\it i})$.'
  prefs: []
  type: TYPE_NORMAL
- en: Policy Skeleton.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Given that user behaviors and DB results jointly determine system actions and
    responses, policy skeleton is designed to cover all fundamental user behaviors
    and characteristic DB results, along with their corresponding system actions and
    responses.⁷⁷7It should be noted that we do not enumerate every conceivable combination
    of user behaviors or potential database outcomes, as schema engineering is not
    the primary focus of this study.) Considering the infeasibility of developing
    a multi-task/domain policy skeleton for every possible combination of tasks and
    domains, we opt to develop a distinct policy skeleton tailored to each specific
    task and domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following Mehri and Eskenazi ([2021](#bib.bib27)), our strategy converts the
    established dialog policy into a series of template dialog turns $\mathcal{X}$
    that are logically arranged and concentrate on task completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{X}=$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\boldsymbol{x}_{i}=$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\boldsymbol{x}_{i}$ denotes the total number of template turns within
    the policy skeleton (around 10-20 template turns depending on the task complexity).
    In order to equip the frozen LLM with additional capabilities or modify current
    ones, we only need insert, amend, or eliminate a few template turns within the
    policy skeleton.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We validate the effectiveness of SGP-TOD on the following dialog datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiwoz 2.0 Budzianowski et al. ([2018](#bib.bib3)) is a multi-domain task-oriented
    dataset, which contains 8,438/1,000/1,000 dialogs for training/validating/testing,
    spanning seven domains: restaurant, attraction, train, hotel, taxi, police, and
    hospital. Multiwoz 2.0 is annotated with belief states and system actions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiwoz 2.2 Zang et al. ([2020](#bib.bib59)) is a improved version of Multiwoz
    2.0, encompassing refined belief state annotations, slot descriptions, user action
    annotations, *etc.*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RADDLE Peng et al. ([2021a](#bib.bib34), [c](#bib.bib36)) consists of four single-domain
    dialog datasets derived from Multiwoz 2.0 (*i.e.,* restaurant, train, hotel, attraction),
    reorganized by Peng et al. ([2021a](#bib.bib34)). Each corpus contains 50/50/100-200
    dialogs for training/validating/testing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: STAR Mosig et al. ([2020](#bib.bib28)) includes 24 tasks in 13 domains (*e.g.,*
    "apartment" domain comprises "apartment-search" and "apartment-schedule"), requiring
    the dialog model to conform to the provided task schema. We use 2,688 single-task
    dialogs from the corpus, which follow a "happy path", *i.e.,* the user is not
    instructed to execute any action exceeding the schema’s expectations. Without
    additional annotations, STAR only provides a flow chart diagram that outlines
    the dialog policy for each task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Automatic Evaluation Metrics.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We evaluate the end-to-end dialog generation performance using the same metrics
    as those listed in Budzianowski et al. ([2018](#bib.bib3)): $(\textup{\it i})$
    Zhang* et al. ([2020](#bib.bib60)), which focuses on computing semantic similarity
    between the generated responses and the ground truth, and correlates better with
    human judgments.'
  prefs: []
  type: TYPE_NORMAL
- en: Following Mehri and Eskenazi ([2021](#bib.bib27)), we perform the next action
    prediction task on STAR, which predicts next system action based on the dialog
    history. Since the system actions and deterministic response templates are mapped
    one to one in STAR corpus, we believe the end-to-end next action prediction task
    falls within end-to-end dialog modeling, following Mosig et al. ([2020](#bib.bib28));
    Mehri and Eskenazi ([2021](#bib.bib27)). In addition, we report the results using
    weighted $\mathtt{F1score}(\%)$.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Multiwoz 2.0 | Multiwoz 2.2 |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{Inform}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Full-shot fine-tuning (with 8.4k+ training dialogs): |'
  prefs: []
  type: TYPE_TB
- en: '| DAMD Zhang et al. ([2020](#bib.bib62)) | 76.33 | 60.40 | 16.60 | 84.97 |
    - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| SimpleTOD Hosseini-Asl et al. ([2020](#bib.bib14)) | 84.40 | 70.10 | 15.01
    | 92.26 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| UBAR Yang et al. ([2021](#bib.bib57)) | 85.10 | 71.02 | 16.21 | 94.27 | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| MinTL Lin et al. ([2020](#bib.bib22)) | 80.04 | 72.71 | 19.11 | 95.49 | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Soloist Peng et al. ([2021a](#bib.bib34)) | 85.50 | 72.90 | 16.54 | 95.74
    | 81.70 | 67.10 | 13.60 | 88.00 |'
  prefs: []
  type: TYPE_TB
- en: '| PPTOD Su et al. ([2022](#bib.bib48)) | 89.20 | 79.40 | 18.62 | 102.92 | -
    | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Mars Sun et al. ([2022](#bib.bib49)) | 88.90 | 78.00 | 19.90 | 103.35 | 88.90
    | 78.00 | 19.60 | 103.05 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot transfer method (pre-trained on SGD): |'
  prefs: []
  type: TYPE_TB
- en: '| AnyTOD-XXLZhao et al. ([2022](#bib.bib63)) | - | - | - | - | 73.90 | 24.40
    | 3.40 | 52.55 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot prompting: |'
  prefs: []
  type: TYPE_TB
- en: '| IT-LLM-ChatGPT-fs Hudecek and Dusek ([2023](#bib.bib16)) | - | - | - | -
    | - | 20.00 | 7.17 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot-ChatGPT | 44.74 | 24.32 | 7.88 | 42.41 | 45.40 | 24.50 | 7.72 |
    42.67 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot prompting: |'
  prefs: []
  type: TYPE_TB
- en: '| IT-LLM-ChatGPT-zs Hudecek and Dusek ([2023](#bib.bib16)) | - | - | - | -
    | - | 15.00 | 3.58 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-ChatGPT (Ours) | 64.56 | 54.05 | 7.17 | 66.48 | 64.70 | 54.70 | 6.96
    | 66.66 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-Codex (Ours) | 71.67 | 52.55 | 7.91 | 70.02 | 75.50 | 52.30 | 6.62
    | 70.53 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-GPT3.5 (Ours) | 83.88 | 69.87 | 9.09 | 85.97 | 82.00 | 72.50 | 9.22
    | 86.47 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: End-to-end dialog generation evaluation on Multiwoz. Results of Soloist,
    Mars, AnyTOD-XXL on Multiwoz 2.2 are cited from Zhao et al. ([2022](#bib.bib63)).
    Results of IT-LLM-ChatGPT are cited from Hudecek and Dusek ([2023](#bib.bib16)).
    Other results of the full-shot fine-tuning methods are cited from He et al. ([2022](#bib.bib13))
    and Sun et al. ([2022](#bib.bib49)). (We do not report the performance of AnyTOD-XXL
    and IT-LLM-ChatGPT on Multiwoz 2.0, because their code is not publicly availvable.
    Difference in mean is significant with p<0.01.)'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Attraction | Train | Hotel | Restaurant |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{Info.}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot fine-tuning (with 50 training dialogs): |'
  prefs: []
  type: TYPE_TB
- en: '| DAMD | 70.00 | 15.00 | 6.90 | 49.40 | 75.00 | 39.50 | 6.20 | 63.45 | 62.50
    | 20.50 | 7.60 | 49.10 | 68.00 | 19.50 | 10.50 | 54.50 |'
  prefs: []
  type: TYPE_TB
- en: '| SimpleTOD | 65.66 | 46.97 | 5.85 | 62.17 | 59.00 | 44.00 | 7.07 | 58.57 |
    62.50 | 40.00 | 7.70 | 58.95 | 75.50 | 44.50 | 11.00 | 71.00 |'
  prefs: []
  type: TYPE_TB
- en: '| Soloist | 86.00 | 65.00 | 12.90 | 88.40 | 80.81 | 64.65 | 9.96 | 82.69 |
    74.50 | 43.50 | 8.12 | 67.12 | 81.00 | 55.50 | 12.80 | 81.50 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot prompting: |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot-ChatGPT | 75.00 | 67.00 | 8.22 | 79.23 | 79.80 | 65.66 | 8.12 |
    80.85 | 51.00 | 26.50 | 5.80 | 44.55 | 80.00 | 55.50 | 7.71 | 75.46 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot prompting: |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-ChatGPT | 95.00 | 94.00 | 7.13 | 101.63 | 76.77 | 74.24 | 6.75 |
    82.26 | 76.50 | 57.00 | 5.16 | 71.91 | 90.00 | 82.50 | 6.72 | 92.97 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-Codex | 98.00 | 93.00 | 10.45 | 105.95 | 78.79 | 70.20 | 8.56 | 83.06
    | 83.50 | 69.50 | 7.86 | 84.36 | 91.00 | 85.00 | 10.50 | 98.50 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-GPT3.5 | 96.00 | 93.00 | 9.53 | 104.03 | 82.83 | 77.27 | 8.72 | 88.77
    | 82.50 | 71.50 | 7.05 | 84.05 | 91.50 | 84.00 | 12.90 | 100.65 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: End-to-end dialog generation evaluation on RADDLE. The few-shot fine-tuning
    results are cited from Peng et al. ([2021a](#bib.bib34)). (Difference in mean
    is significant with p<0.01.)'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison Methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We evaluate the zero-shot performance of the proposed SGP-TOD by comparing it
    to SOTA zero-shot transfer methods and zero-shot/few-shot prompting strategies.
    (We report the mean results of three different runs.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero-shot transfer methods:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BERT+S Mosig et al. ([2020](#bib.bib28)) is a schema-guided method that augments
    a BERT-base classifier Devlin et al. ([2019](#bib.bib10)) with a provided system-side
    schema to predict the next system action.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAM Mehri and Eskenazi ([2021](#bib.bib27)) represents a schema-guided model
    based on BERT-base, which aligns the dialog context to a user-aware schema to
    predict the next system action.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AnyTOD-XXL Zhao et al. ([2022](#bib.bib63)) adopts a neural LM to track dialog
    states and user actions utilizing slot and action descriptions. Then a program
    that outlines a predefined task policy is executed to recommend appropriate system
    actions. Upon considering these system actions, a LM generates the ultimate system
    action and formulates the corresponding template response using the approach proposed
    by Kale and Rastogi ([2020](#bib.bib17)). AnyTOD-XXL is implemented on T5-XXL
    Roberts et al. ([2022](#bib.bib43)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prompting methods:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IT-LLM-ChatGPT Hudecek and Dusek ([2023](#bib.bib16)) is a prompting approach
    based on ChatGPT that leverages the dialog context and manually-crafted slot descriptions
    as the prompt, to track dialog states, fetch DB entries, and produce responses.
    IT-LLM-ChatGPT incorporates one formatting example in zero-shot scenarios, denoted
    as IT-LLM-ChatGPT-zs, and integrates four task-specific examples retrieved from
    the training corpus in few-shot settings, *i.e.,* IT-LLM-ChatGPT-fs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-Shot-ChatGPT is a few-shot prompting strategy implemented on ChatGPT, where
    we use a few (*i.e.,* $\boldsymbol{k}$, the optimal results manifest with 15 on
    Multiwoz (2.0 and 2.2), and 10 on RADDLE, exhibiting no further substantial enhancements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SGP-TOD (Ours) is a schema-guided prompting strategy, which is compatible with
    any off-the-shelf LLMs. In this paper, we employ ChatGPT ("gpt-3.5-turbo"), GPT-3.5
    ("text-davinci-003") and Codex ("code-davinci-002") as the fixed LLMs. Following
    the zero-shot scenario in Wang et al. ([2022b](#bib.bib52)), we insert one formatting
    example from different tasks (fixed through the experimental procedure) into the
    prompt. More implementation details are provided in Appendix [C](#A3 "Appendix
    C Implementation Details ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided
    LLM Prompting").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46c8ccc61fe04b3a20dc7fde771a2624.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Task transfer
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/96fd2a87231fc3e8890a3ca13675a809.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Domain transfer
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Zero-shot evaluation results on STAR with different number of training
    dialogs (1, 10, 100, 1,000) / formatting example turns (1, 10) randomly sampled
    from the source domain/tasks. (Note the numbers are represented in logarithmic
    form with base 10.)'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 End-to-End Evaluation on Multiwoz
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setup.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: AnyTOD-XXL is pre-trained on SGD dataset Rastogi et al. ([2020b](#bib.bib42))⁸⁸8SGD
    is a large-scale schema-guided multi-domain dialog dataset, spanning 45 domains.
    then evaluated on Multiwoz in a zero-shot manner.
  prefs: []
  type: TYPE_NORMAL
- en: Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We present the evaluation results in multi-domain contexts on Multiwoz in Table
    [2](#S4.T2 "Table 2 ‣ Automatic Evaluation Metrics. ‣ 4.1 Experimental Setup ‣
    4 Experiments ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM
    Prompting"). In addition to the aforementioned methods, we include the results
    of SOTA full-shot fine-tuning approaches to facilitate a more comprehensive comparison.
    SGP-TOD obtains SOTA zero-shot performance, substantially outperforming few-shot
    approaches across all evaluation metrics, while even exhibiting competitive results
    in comparison to full-shot fine-tuning methods concerning $\mathtt{Success}$.
    This confirms the effectiveness of integrating the task schema with the LLMs’
    proficient language processing capabilities and instruction-following abilities.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Prompting Methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SGP-TOD-ChatGPT distinctly surpasses the zero-shot prompting approach IT-LLM-ChatGPT-zs
    with respect to $\mathtt{Success}$). This suggests that providing explicit, concise
    and comprehensive task instructions via task schema is preferable to imparting
    implicit task guidance through the selected dialog turns from the training corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Zero-Shot Transfer Methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our SGP-TOD demonstrates a substantial advantage over the AnyTOD-XXL, which
    necessitates extensive pre-training and supplementary annotations, *e.g.,* slot/action
    descriptions, over all the metrics. This exemplifies the potency of SGP-TOD, which
    markedly reduces the necessity for human labor and computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Full-Shot Fine-Tuning Methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SGP-TOD, without training on any task-specific data, exhibits competitive performance
    over $\mathtt{Inform}$ is due to a lack of linguistic variations of the template
    utterances, which is acceptable considering the trade-off between human effort
    and efficacy
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Different LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We find that SGP-TOD-GPT3.5 performs better than SGP-TOD-Codex and SGP-TOD-ChatGPT.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 End-to-End Evaluation on RADDLE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Table [3](#S4.T3 "Table 3 ‣ Automatic Evaluation Metrics. ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided
    LLM Prompting") reports the end-to-end performance in single-domain settings on
    RADDLE. On all four dialog tasks, SGP-TOD achieves considerably superior performance
    over $\mathtt{Combined}$ compared to few-shot prompting and fine-tuning approaches.
    This indicates that even within a single-domain context (*i.e.,* encompassing
    a notably simpler task policy), furnishing explicit task instructions employing
    task schema remains preferable to rendering implicit task directions within dialog
    turns.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Prompting Method.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Our SGP-TOD significantly surpasses Few-Shot-ChatGPT across all the metrics,
    aligning with the results observed in multi-domain contexts and further substantiating
    its academic significance.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Few-Shot Fine-Tuning Methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SGP-TOD exhibits a zero-shot performance that consistently surpasses few-shot
    fine-tuning approaches in regard to $\mathtt{Inform}$. The exceptional zero-shot
    performance of SGP-TOD is noteworthy, considering that Soloist is a powerful dialog
    model pre-trained on a number of heterogeneous dialog corpora and subsequently
    fine-tuned on RADDLE.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 End-to-End Evaluation on STAR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Model | Task transfer | Domain transfer |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{F1}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot transfer |'
  prefs: []
  type: TYPE_TB
- en: '| (leave-one fune-tuning with 2.5k training dialogs): |'
  prefs: []
  type: TYPE_TB
- en: '| BERT+S | 24.25 | 24.89 | 25.70 | 28.56 |'
  prefs: []
  type: TYPE_TB
- en: '| SAM | 49.82 | 51.30 | 55.91 | 57.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Zero-shot prompting: |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-Codex-INI | 45.18 | 47.99 | 47.21 | 49.97 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-GPT3.5 | 47.67 | 48.27 | 49.76 | 50.39 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-Codex | 49.78 | 51.01 | 52.72 | 53.66 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-GPT3.5-E2E | 50.84 | 50.74 | 53.50 | 53.21 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Zero-shot end-to-end next action prediction evaluation on STAR. (Difference
    in mean is significant with p<0.01.)'
  prefs: []
  type: TYPE_NORMAL
- en: Setup.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As mentioned in Section [4.1](#S4.SS1 "4.1 Experimental Setup ‣ 4 Experiments
    ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting"),
    we conduct next action prediction task on STAR, following Mehri and Eskenazi ([2021](#bib.bib27)).
    BERT+S, SAM are assessed in the leave-one fine-tuning scenario, where the models
    are fine-tuned on source tasks/domains in STAR then zero-shot on the held-out
    task/domain.⁹⁹9AnyTOD-XXL requires additional annotations, *e.g.,* belief descriptions,
    which makes it not suitable for STAR. SGP-TOD is presented merely with two formatting
    sample turns from the source tasks/domains in the prompt.'
  prefs: []
  type: TYPE_NORMAL
- en: Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Following Mehri and Eskenazi ([2021](#bib.bib27)); Mosig et al. ([2020](#bib.bib28)),
    we report the zero-shot evaluation results in two settings, *i.e.,* task transfer
    and domain transfer in Table [4](#S4.T4 "Table 4 ‣ 4.4 End-to-End Evaluation on
    STAR ‣ 4 Experiments ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided
    LLM Prompting"). SGP-TOD demonstrates exceptional performance, surpassing or rivaling
    zero-shot transfer methods in both settings. This outcome signifies that, even
    when faced with complicated business logic and system actions in dialog policies,
    the proposed SGP-TOD continues to exhibit commendable performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Zero-Shot Transfer Methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SGP-TOD, merely with two formatting sample turns, achieves superior or comparable
    performance compared to BERT+S, SAM, which are fine-tuned on adequate source data.
    Figure [5](#S4.F5 "Figure 5 ‣ Comparison Methods. ‣ 4.1 Experimental Setup ‣ 4
    Experiments ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting")
    shows the impact of changing the number of training dialogs (ranging from 1 to
    1,000) and formatting example turns (spanning from 1 to 10) from source domains/tasks.
    We observe that SGP-TOD, employing only two formatting sample turns, attains superior
    or commensurate performance in terms of both $\mathtt{F1score}$ (as detailed in
    Appendix [E](#A5 "Appendix E Zero-Shot End-to-End Evaluation Results on STAR ‣
    SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting")), when
    compared to SAM trained with 1,000 dialogs. Given that a single dialog contains
    more than 10 dialogue turns, this result suggests that SGP-TOD diminishes labeling
    expenses by a minimum factor of 1,000. Furthermore, it is noteworthy that augmenting
    the quantity of formatting exemplar turns exerts a negligible influence on the
    performance of SGP-TOD.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Different LLMs and Prompting Formats.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SGP-TOD-Codex surpasses SGP-TOD-GPT3.5 while rivaling SGP-TOD-GPT3.5-E2E (with
    template responses affixed to action labels in the policy prompt, demonstrated
    in Figure [11](#A6.F11 "Figure 11 ‣ Appendix F Prompt Examples for STAR Dataset
    ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting") in
    Appendix [F](#A6 "Appendix F Prompt Examples for STAR Dataset ‣ SGP-TOD: Building
    Task Bots Effortlessly via Schema-Guided LLM Prompting")). We conjecture that
    Codex, benefiting from extensive pre-training on copious code data, demonstrates
    enhanced proficiency compared to GPT-3.5 in interpreting action labels. In addition,
    appending template responses is presumed to facilitate the explication of action
    labels for GPT-3.5.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Different Task Schema.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SGP-TOD-Codex-INI, utilizing an identical task schema as employed in training
    SAM Mehri and Eskenazi ([2021](#bib.bib27)), manifests commendable performance.
    This result highlights that SGP-TOD as a flexible prompting strategy, compatible
    with any manually-crafted task schema. Though the current study does not center
    around task schema design, future research endeavors may investigate the influence
    of varying task schemas, encompassing diverse formats and coverage.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 End-to-End Evaluation on Domain Extension
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Setup.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We conduct experiments in a domain extension setting Lipton et al. ([2018](#bib.bib23));
    Gasic et al. ([2014](#bib.bib12)) to assess the efficacy of SGP-TOD in adapting
    deployed task bots to incorporate novel functionalities. Following Zhang et al.
    ([2022](#bib.bib61)), we construct the Restaurant-ext corpus by extending the
    pre-existing Restaurant in RADDLE Peng et al. ([2021c](#bib.bib36)) with additional
    functions. Specifically, we introduce four new slots: [restaurant_dish], [value_price],
    [start_time], and [end_time]. The initial slot pertains to recommendations for
    signature restaurant meals, while the final three concern delivery service details.
    All database entries are updated with corresponding values. A dialog example and
    a DB entry in Restaurant-ext can be found in Appendix [G](#A7 "Appendix G An Example
    of Domain Extension ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided
    LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | FT/FS/ZS | Restaurant-Ext |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{Info.}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Without domain-relevant knowledge |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT | ZS | 44.00 | 6.00 | 4.31 | 85.96 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT3.5 | ZS | 34.00 | 16.00 | 8.70 | 84.31 |'
  prefs: []
  type: TYPE_TB
- en: '| With prior knowledge on Restaurant |'
  prefs: []
  type: TYPE_TB
- en: '| Soloist | FT | 78.00 | 0.00 | 10.62 | 87.24 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-ChatGPT | ZS | 88.00 | 34.00 | 5.45 | 86.11 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-GPT3.5 | ZS | 94.00 | 30.00 | 10.68 | 87.30 |'
  prefs: []
  type: TYPE_TB
- en: '| With knowledge on Restaurant-Ext |'
  prefs: []
  type: TYPE_TB
- en: '| Soloist+Teach | FT | 82.00 | 38.00 | 10.99 | 87.66 |'
  prefs: []
  type: TYPE_TB
- en: '| Few-shot-GPT3.5 | FS | 88.00 | 54.00 | 12.95 | 88.90 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-ChatGPT-Ext | ZS | 88.00 | 78.00 | 6.25 | 86.15 |'
  prefs: []
  type: TYPE_TB
- en: '| SGP-TOD-GPT3.5-Ext | ZS | 96.00 | 86.00 | 14.57 | 89.01 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: End-to-end evaluation on domain extension. FT: fine-tuning, FS: few-shot
    prompting, ZS: zero-shot prompting, Info.: Inform, Succ.: Success, BERTS.: BERTScore.
    (Difference in mean is significant with p<0.01.)'
  prefs: []
  type: TYPE_NORMAL
- en: Compared Methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ChatGPT, GPT-3.5 denote zero-shot prompting with base LLMs that receive merely
    two formatting example turns from other domains in RADDLE.^(10)^(10)10We utilize
    the same formatting example turns in all zero-shot prompting methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SGP-TOD-ChatGPT, SGP-TOD-GPT3.5 represent our SGP-TOD implementation, with the
    Restaurant policy skeleton.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Soloist is trained with 50 training dialogs in the Restaurant domain (previously
    reported in Table [3](#S4.T3 "Table 3 ‣ Automatic Evaluation Metrics. ‣ 4.1 Experimental
    Setup ‣ 4 Experiments ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided
    LLM Prompting")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soloist+Teach is fine-tuning method enhanced with machine teaching Simard et al.
    ([2017](#bib.bib47)). Machine teaching is an efficient approach to equip deployed
    task bots with the ability to handle new functions by correcting representative
    failed human-bot dialogs. We deploy Soloist to converse with real users, then
    implement machine teaching via Conversational learner Shukla et al. ([2020](#bib.bib46)),
    an effective machine teaching tool, to obtain 10/50/50 examples in Restaurant-ext
    for training, validating, and testing. Finally, we fine-tune Soloist with gathered
    10 training dialogs covering four new slots, resulting in dialog agent Soloist+Teach.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Few-shot-GPT3.5 is the few-shot prompting strategy augmented with machine teaching.
    Based on GPT-3.5, we utilize 10 randomly selected dialog turns from the collected
    10 training dialogs as the prompt (with peak performance at 10), resulting in
    Few-shot-GPT3.5.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SGP-TOD-ChatGPT-Ext, SGP-TOD-GP3.5-Ext refer to SGP-TOD with Restaurant-Ext
    policy skeleton, where we only add four template turns about four new slots to
    the policy skeleton of Restaurant.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Results.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The evaluation results are presented in Table [5](#S4.T5 "Table 5 ‣ Setup.
    ‣ 4.5 End-to-End Evaluation on Domain Extension ‣ 4 Experiments ‣ SGP-TOD: Building
    Task Bots Effortlessly via Schema-Guided LLM Prompting"). SGP-TOD with Restaurant-Ext
    policy skeleton, *i.e.,* SGP-TOD-ChatGPT-Ext, and notably SGP-TOD-GPT3.5-Ext surpasses
    all other evaluated approaches by a substantial margin over all the metrics. This
    demonstrates the adaptability of our SGP-TOD in accommodating novel functionalities,
    thereby revealing its immense potential for lifelong learning.'
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Approaches Augmented by Machine Teaching.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SGP-TOD-GP3.5-Ext obtains substantially higher $\mathtt{Success}$). This remarkable
    zero-shot performance can be ascribed to the adequate coverage and exceptional
    interpretability of the task schema in SGP-TOD. Furthermore, in contrast to fine-tuning/prompting
    strategies utilizing additional dialogues corrected through machine teaching,
    SGP-TOD facilitates a more agile adaptation to novel functionalities by merely
    modifying template turns within the task schema.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Fine-Tuning Methods.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Soloist yields zero $\mathtt{Success}$ (by more than 30 points). This illustrates
    that our SGP-TOD provides enhanced robustness in zero-shot generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Comparison with Base LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The substantial improvement of SGP-TOD-ChatGPT-Ext and SGP-TOD-GPT3.5-Ext over
    ChatGPT and GPT-3.5 illustrates SGP-TOD’s efficiency in supplying task-specific
    knowledge in a zero-shot way.
  prefs: []
  type: TYPE_NORMAL
- en: Impact of Different LLMs.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: SGP-TOD-ChatGPT-Ext attains a lower $\mathtt{BLEU}$, suggesting that ChatGPT
    generates more diverse responses relative to GPT-3.5.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Ablation Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '| Model | Multiwoz 2.0 | Multiwoz 2.2 |'
  prefs: []
  type: TYPE_TB
- en: '| $\mathtt{Inform}$ |'
  prefs: []
  type: TYPE_TB
- en: '| SP-TOD-GPT3.5 | 83.88 | 69.87 | 9.09 | 85.97 | 82.00 | 72.50 | 9.22 | 86.47
    |'
  prefs: []
  type: TYPE_TB
- en: '| -policy | 82.28 | 55.65 | 6.51 | 75.48 | 81.80 | 56.20 | 6.63 | 75.63 |'
  prefs: []
  type: TYPE_TB
- en: '| -policy -DB | 81.20 | 50.95 | 6.48 | 72.56 | 81.40 | 52.30 | 6.57 | 73.42
    |'
  prefs: []
  type: TYPE_TB
- en: '| -policy -DB -belief | 38.74 | 33.13 | 6.18 | 42.12 | 38.60 | 33.90 | 6.29
    | 42.54 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Ablation study on the impact of the three components in the proposed
    SGP-TOD and the database expertise on Multiwoz using GPT-3.5\. -policy: removing
    policy prompter, -DB: removing database retriever, -belief: removing DST prompter.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Turn | Interaction between a user and ChatGPT | Turn | Interaction between
    a user and SP-TOD-ChatGPT-Ext |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | User: Hi, i’m looking for a place with Tuscan food in any price range.
    | 1 | User: Hi, i’m looking for a place with Tuscan food in any price range. |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | System: Certainly, there are 3 Tuscan restaurants in town. What is your
    preferred price range? | 2 | System: I am sorry, there are no Tuscan restaurants.
    Here are some other options: Korean, Italian, Chinese. Which one would you like
    more information on? |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | User: I don’t care about the price. Please recommend one for me, i also
    need the phone number please. | 3 | User: How about any Korean restaurants? I
    also need the phone number please. |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | System: The phone number for Pizza Hut City Centre is 01223323737. Is
    there anything else i can help you with today? | 4 | System: Little Seoul is a
    Korean restaurant. Their phone number is 01223308681. |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | User: Does the restaurant offer delivery service? How much does the delivery
    charge? | 5 | User: Does the restaurant offer delivery service? How much does
    the delivery charge? |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | System: I’m sorry, but I could not find any information on delivery fee
    in our database. You may want to contact the restaurant directly to inquire about
    this. Is there anything else I can assist you with today? | 6 | System: Yes, they
    offer delivery service and the delivery charge is 4 pounds. Can I help you with
    anything else? |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | User: No. Thank you, goodbye. | 7 | User: No. Thank you, goodbye. |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | System: Glad i could assist you. Have a good day. | 8 | System: You’re
    welcome, have a great meal! Goodbye! |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Illustration of two interactive examples between a user and ChatGPT
    (left), SGP-TOD-ChatGPT-Ext (right). Appropriate responses and Inappropriate responses
    are highlighted.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [6](#S5.T6 "Table 6 ‣ 5.1 Ablation Study ‣ 5 Discussion ‣ SGP-TOD:
    Building Task Bots Effortlessly via Schema-Guided LLM Prompting"), we study the
    impact of the three components of SGP-TOD (namely, Policy Prompter, DST Prompter,
    and LLM) as well as the database expertise, on Multiwoz (2.0 and 2.2) utlizing
    GPT-3.5.^(11)^(11)11We inject the same two formatting example turns into the prompt
    throughout the evaluation. Combining the three elements in SGP-TOD with the database
    expertise produces optimal results across both datasets. The removal of the Policy
    Prompter, database knowledge, and DST Prompter leads to consistent declines in
    all evaluation metrics, underscoring the value of enhancing the fixed LLM with
    the task schema and external database information.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, GPT-3.5 (in the final row) exhibits commendable zero-shot performance,
    highlighting the need of exploiting its superior zero-shot generalization capabilities
    in dialog generation tasks. Additionally, Disabling the Policy Prompter incurs
    a discernible decline in performance regarding $\mathtt{Success}$), due to the
    DST Prompter’s intended purpose of assisting the frozen LLM in apprehending the
    dialogue context.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Case Study
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite the superior performance of the proposed SGP-TOD on GPT-3.5, we showcase
    interactive examples utilizing ChatGPT, a renowned and potent chatbot. In Table
    [7](#S5.T7 "Table 7 ‣ 5.1 Ablation Study ‣ 5 Discussion ‣ SGP-TOD: Building Task
    Bots Effortlessly via Schema-Guided LLM Prompting"), a user engages with ChatGPT
    (left) and SGP-TOD-ChatGPT-Ext (right) to complete the identical task on Restaurant-Ext.^(12)^(12)12ChatGPT
    and SGP-TOD-ChatGPT-Ext are previously reported in Table [5](#S4.T5 "Table 5 ‣
    Setup. ‣ 4.5 End-to-End Evaluation on Domain Extension ‣ 4 Experiments ‣ SGP-TOD:
    Building Task Bots Effortlessly via Schema-Guided LLM Prompting"). The same two
    formatting example turns are incorporated into the prompt for both zero-shot strategies.
    The user initiates the conversation by seeking recommendations for a Tuscan restaurant
    with no price range preference. Lacking external database information, ChatGPT
    conveys inaccurate details (Turn 2), whereas SGP-TOD-ChatGPT-Ext informs users
    of the absence of matching restaurants and proposes alternatives (Turn 2). This
    exemplifies the benefits of integrating real-world expertise into the fixed LLM.
    Furthermore, ChatGPT persistently inquires about the desired price range despite
    the user’s indifference. We argue that SGP-TOD assists the frozen LLM in discerning
    user intentions. In Turn 4, ChatGPT continues to furnish fabricated details (*i.e.,*
    the restaurant name and phone number) concerning the nonexistent eatery, while
    SGP-TOD-ChatGPT-Ext identifies a suitable Korean restaurant and the corresponding
    factual information. In contrast with ChatGPT, SGP-TOD-ChatGPT-Ext adeptly addresses
    inquiries about the delivery service (Turn 6), indicating that SGP-TOD is capable
    of endowing the frozen LLM with novel functionalities.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we present SGP-TOD, a schema-guided prompting strategy aimed
    at the expeditious construction of end-to-end task bots, relying exclusively on
    LLMs and the corresponding task schema. Employing the symbolic knowledge – the
    pre-defined task schema – SGP-TOD guides fixed LLMs to generate suitable responses
    for novel tasks in a zero-shot fashion. Experimental results on Multiwoz (2.0
    and 2.2), RADDLE, and STAR in zero-shot settings show that SGP-TOD attains SOTA
    zero-shot performance, substantially outpacing few-shot approaches and even even
    demonstrating competitive results in relation to full-shot fine-tuning methods.
    This suggests that our SGP-TOD presents an appealing starting point for utilizing
    LLMs to construct task bots with minimal human intervention. In a domain-extension
    scenario, SGP-TOD exhibits remarkable adaptability to new functionalities, showcasing
    its impressive potential for lifelong learning. For future work, we plan to explore
    the use of SGP-TOD to develop personalized chatbots by utilizing pertinent task
    schema.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. 2020. [Language models are few-shot learners](https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
    In *Advances in Neural Information Processing Systems*, volume 33, pages 1877–1901\.
    Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck et al. (2023) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. 2023. [Sparks of
    artificial general intelligence: Early experiments with gpt-4](http://arxiv.org/abs/2303.12712).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Budzianowski et al. (2018) Paweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng,
    Iñigo Casanueva, Ultes Stefan, Ramadan Osman, and Milica Gašić. 2018. Multiwoz
    - a large-scale multi-domain wizard-of-oz dataset for task-oriented dialogue modelling.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Campagna et al. (2020) Giovanni Campagna, Agata Foryciarz, Mehrad Moradshahi,
    and Monica Lam. 2020. [Zero-shot transfer learning with synthesized data for multi-domain
    dialogue state tracking](https://doi.org/10.18653/v1/2020.acl-main.12). In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics*,
    pages 122–132, Online. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. 2021. Evaluating large language models trained on code.
    *arXiv preprint arXiv:2107.03374*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cheng et al. (2023) Zhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul
    Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer,
    Noah A. Smith, and Tao Yu. 2023. Binding language models in symbolic languages.
    *ICLR*, abs/2210.02875.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022a) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
    Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles
    Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua
    Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,
    Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,
    Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay
    Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,
    Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
    Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
    Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
    Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
    Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
    Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022a.
    [Palm: Scaling language modeling with pathways](http://arxiv.org/abs/2204.02311).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. (2022b) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
    Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles
    Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua
    Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran,
    Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin,
    Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay
    Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson,
    Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph,
    Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,
    Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz,
    Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi
    Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy
    Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022b.
    [Palm: Scaling language modeling with pathways](https://doi.org/10.48550/arXiv.2204.02311).
    *CoRR*, abs/2204.02311.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dai et al. (2023) Haixing Dai, Zhengliang Liu, Wenxiong Liao, Xiaoke Huang,
    Yihan Cao, Zihao Wu, Lin Zhao, Shaochen Xu, Wei Liu, Ninghao Liu, Sheng Li, Dajiang
    Zhu, Hongmin Cai, Lichao Sun, Quanzheng Li, Dinggang Shen, Tianming Liu, and Xiang
    Li. 2023. [Auggpt: Leveraging chatgpt for text data augmentation](http://arxiv.org/abs/2302.13007).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. [BERT: pre-training of deep bidirectional transformers for language
    understanding](https://doi.org/10.18653/v1/n19-1423). In *Proceedings of the 2019
    Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA,
    June 2-7, 2019, Volume 1 (Long and Short Papers)*, pages 4171–4186\. Association
    for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao
    Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang Sui. 2023. [A survey on in-context
    learning](http://arxiv.org/abs/2301.00234).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gasic et al. (2014) Milica Gasic, Dongho Kim, Pirros Tsiakoulis, Catherine Breslin,
    Matthew Henderson, Martin Szummer, Blaise Thomson, and Steve J. Young. 2014. [Incremental
    on-line adaptation of pomdp-based dialogue managers to extended domains](http://www.isca-speech.org/archive/interspeech_2014/i14_0140.html).
    In *INTERSPEECH 2014, 15th Annual Conference of the International Speech Communication
    Association, Singapore, September 14-18, 2014*, pages 140–144\. ISCA.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'He et al. (2022) Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao,
    Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, et al. 2022. Galaxy: A generative
    pre-trained model for task-oriented dialog with semi-supervised learning and explicit
    policy injection. *Proceedings of the AAAI Conference on Artificial Intelligence*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosseini-Asl et al. (2020) Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu,
    Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented
    dialogue. *Advances in Neural Information Processing Systems*, 33:20179–20191.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu et al. (2022) Yushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu, Noah A. Smith,
    and Mari Ostendorf. 2022. [In-context learning for few-shot dialogue state tracking](https://aclanthology.org/2022.findings-emnlp.193).
    In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages
    2627–2643, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hudecek and Dusek (2023) Vojtech Hudecek and Ondrej Dusek. 2023. [Are llms all
    you need for task-oriented dialogue?](https://doi.org/10.48550/arXiv.2304.06556)
    *CoRR*, abs/2304.06556.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kale and Rastogi (2020) Mihir Kale and Abhinav Rastogi. 2020. [Template guided
    text generation for task-oriented dialogue](https://doi.org/10.18653/v1/2020.emnlp-main.527).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2020, Online, November 16-20, 2020*, pages 6505–6520\. Association
    for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022) Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and
    Xifeng Yan. 2022. [Controllable dialogue simulation with in-context learning](https://aclanthology.org/2022.findings-emnlp.318).
    In *Findings of the Association for Computational Linguistics: EMNLP 2022*, pages
    4330–4347, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liang et al. (2023) Yaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu, Yan Xia,
    Yu Liu, Yang Ou, Shuai Lu, Lei Ji, Shaoguang Mao, Yun Wang, Linjun Shou, Ming
    Gong, and Nan Duan. 2023. [Taskmatrix.ai: Completing tasks by connecting foundation
    models with millions of apis](http://arxiv.org/abs/2303.16434).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2021a) Zhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan Moon,
    Zhenpeng Zhou, Paul A Crook, Zhiguang Wang, Zhou Yu, Eunjoon Cho, Rajen Subba,
    et al. 2021a. Zero-shot dialogue state tracking via cross-task transfer. In *Proceedings
    of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages
    7890–7900.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2021b) Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A Crook, Zhenpeng
    Zhou, Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, and Rajen Subba. 2021b.
    Leveraging slot descriptions for zero-shot cross-domain dialogue statetracking.
    In *Proceedings of the 2021 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 5640–5648.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2020) Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, and Pascale
    Fung. 2020. [MinTL: Minimalist transfer learning for task-oriented dialogue systems](https://doi.org/10.18653/v1/2020.emnlp-main.273).
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing (EMNLP)*, pages 3391–3405, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lipton et al. (2018) Zachary Lipton, Xiujun Li, Jianfeng Gao, Lihong Li, Faisal
    Ahmed, and Li Deng. 2018. Bbq-networks: Efficient exploration in deep reinforcement
    learning for task-oriented dialogue systems. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 32.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022) Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence
    Carin, and Weizhu Chen. 2022. [What makes good in-context examples for GPT-3?](https://doi.org/10.18653/v1/2022.deelio-1.10)
    In *Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on
    Knowledge Extraction and Integration for Deep Learning Architectures*, pages 100–114,
    Dublin, Ireland and Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madotto et al. (2021) Andrea Madotto, Zhaojiang Lin, Genta Indra Winata, and
    Pascale Fung. 2021. Few-shot bot: Prompt-based learning for dialogue systems.
    *arXiv preprint arXiv:2110.08118*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mehri et al. (2022) Shikib Mehri, Yasemin Altun, and Maxine Eskenazi. 2022.
    [LAD: Language models as data for zero-shot dialog](https://aclanthology.org/2022.sigdial-1.55).
    In *Proceedings of the 23rd Annual Meeting of the Special Interest Group on Discourse
    and Dialogue*, pages 595–604, Edinburgh, UK. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mehri and Eskenazi (2021) Shikib Mehri and Maxine Eskenazi. 2021. Schema-guided
    paradigm for zero-shot dialog. *arXiv preprint arXiv:2106.07056*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mosig et al. (2020) Johannes EM Mosig, Shikib Mehri, and Thomas Kober. 2020.
    Star: A schema-guided dialog dataset for transfer learning. *arXiv preprint arXiv:2010.11853*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nye et al. (2021) Maxwell I. Nye, Michael Henry Tessler, Joshua B. Tenenbaum,
    and Brenden M. Lake. 2021. [Improving coherence and consistency in neural sequence
    models with dual-system, neuro-symbolic reasoning](https://proceedings.neurips.cc/paper/2021/hash/d3e2e8f631bd9336ed25b8162aef8782-Abstract.html).
    In *Advances in Neural Information Processing Systems 34: Annual Conference on
    Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,
    virtual*, pages 25192–25204.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. [Gpt-4 technical report](http://arxiv.org/abs/2303.08774).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. [Training
    language models to follow instructions with human feedback](https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf).
    In *Advances in Neural Information Processing Systems*, volume 35, pages 27730–27744\.
    Curran Associates, Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Papineni et al. (2002) Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing
    Zhu. 2002. [Bleu: a method for automatic evaluation of machine translation](https://doi.org/10.3115/1073083.1073135).
    In *Proceedings of the 40th Annual Meeting of the Association for Computational
    Linguistics*, pages 311–318, Philadelphia, Pennsylvania, USA. Association for
    Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2023) Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia
    Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check
    your facts and try again: Improving large language models with external knowledge
    and automated feedback. *arXiv preprint arXiv:2302.12813*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2021a) Baolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayandeh,
    Lars Liden, and Jianfeng Gao. 2021a. Soloist: Building task bots at scale with
    transfer learning and machine teaching. *Transactions of the Association for Computational
    Linguistics*, 9:807–824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2021b) Baolin Peng, Chunyuan Li, Zhu Zhang, Jinchao Li, Chenguang
    Zhu, and Jianfeng Gao. 2021b. Synergy: Building task bots at scale using symbolic
    knowledge and machine teaching. *arXiv preprint arXiv:2110.11514*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2021c) Baolin Peng, Chunyuan Li, Zhu Zhang, Chenguang Zhu, Jinchao
    Li, and Jianfeng Gao. 2021c. [RADDLE: an evaluation benchmark and analysis platform
    for robust task-oriented dialog systems](https://doi.org/10.18653/v1/2021.acl-long.341).
    In *Proceedings of the 59th Annual Meeting of the Association for Computational
    Linguistics and the 11th International Joint Conference on Natural Language Processing,
    ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021*, pages
    4418–4429\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qian and Yu (2019) Kun Qian and Zhou Yu. 2019. [Domain adaptive dialog generation
    via meta learning](https://doi.org/10.18653/v1/P19-1253). In *Proceedings of the
    57th Annual Meeting of the Association for Computational Linguistics*, pages 2639–2649,
    Florence, Italy. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2023) Chengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro
    Yasunaga, and Diyi Yang. 2023. [Is chatgpt a general-purpose natural language
    processing task solver?](http://arxiv.org/abs/2302.06476)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask
    learners. *OpenAI blog*, 1(8):9.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. [Exploring
    the limits of transfer learning with a unified text-to-text transformer](http://jmlr.org/papers/v21/20-074.html).
    *Journal of Machine Learning Research*, 21(140):1–67.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastogi et al. (2020a) Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav
    Gupta, and Pranav Khaitan. 2020a. Towards scalable multi-domain conversational
    agents: The schema-guided dialogue dataset. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 34, pages 8689–8696.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rastogi et al. (2020b) Abhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav
    Gupta, and Pranav Khaitan. 2020b. [Towards scalable multi-domain conversational
    agents: The schema-guided dialogue dataset](https://ojs.aaai.org/index.php/AAAI/article/view/6394).
    In *The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The
    Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI
    2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence,
    EAAI 2020, New York, NY, USA, February 7-12, 2020*, pages 8689–8696\. AAAI Press.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roberts et al. (2022) Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav
    Mishra, James Bradbury, Daniel Andor, Sharan Narang, Brian Lester, Colin Gaffney,
    Afroz Mohiuddin, Curtis Hawthorne, Aitor Lewkowycz, Alex Salcianu, Marc van Zee,
    Jacob Austin, Sebastian Goodman, Livio Baldini Soares, Haitang Hu, Sasha Tsvyashchenko,
    Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bulian, Xavier Garcia, Jianmo Ni,
    Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan Lee, Dan Garrette, James
    Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten Bosma, Alexandre
    Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan Saeta, Ryan
    Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. 2022. [Scaling
    up models and data with t5x and seqio](http://arxiv.org/abs/2203.17189).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shah et al. (2019) Darsh J Shah, Raghav Gupta, Amir A Fayazi, and Dilek Hakkani-Tur.
    2019. Robust zero-shot cross-domain slot filling with example values. *arXiv preprint
    arXiv:1906.06870*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin and Van Durme (2022) Richard Shin and Benjamin Van Durme. 2022. [Few-shot
    semantic parsing with language models trained on code](https://doi.org/10.18653/v1/2022.naacl-main.396).
    In *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pages 5417–5425,
    Seattle, United States. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shukla et al. (2020) Swadheen Shukla, Lars Liden, Shahin Shayandeh, Eslam Kamal,
    Jinchao Li, Matt Mazzola, Thomas Park, Baolin Peng, and Jianfeng Gao. 2020. [Conversation
    Learner - a machine teaching tool for building dialog managers for task-oriented
    dialog systems](https://doi.org/10.18653/v1/2020.acl-demos.39). In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics: System
    Demonstrations*, pages 343–349, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simard et al. (2017) Patrice Y. Simard, Saleema Amershi, David Maxwell Chickering,
    Alicia Edelman Pelton, Soroush Ghorashi, Christopher Meek, Gonzalo A. Ramos, Jina
    Suh, Johan Verwey, Mo Wang, and John Wernsing. 2017. [Machine teaching: A new
    paradigm for building machine learning systems](http://arxiv.org/abs/1707.06742).
    *CoRR*, abs/1707.06742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. (2022) Yixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai,
    Yi-An Lai, and Yi Zhang. 2022. [Multi-task pre-training for plug-and-play task-oriented
    dialogue system](https://arxiv.org/abs/2109.14739). *Proceedings of the 60th Annual
    Meeting of the Association for Computational Linguistics (ACL)*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2022) Haipeng Sun, Junwei Bao, Youzheng Wu, and Xiaodong He. 2022.
    Mars: Semantic-aware contrastive learning for end-to-end task-oriented dialog.
    *arXiv preprint arXiv:2210.08917*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen, Runkai Zheng,
    Yidong Wang, Linyi Yang, Haojun Huang, Wei Ye, Xiubo Geng, Binxin Jiao, Yue Zhang,
    and Xing Xie. 2023. [On the robustness of chatgpt: An adversarial and out-of-distribution
    perspective](http://arxiv.org/abs/2302.12095).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Qingyue Wang, Yanan Cao, Piji Li, Yanhe Fu, Zheng Lin, and
    Li Guo. 2022a. [Slot dependency modeling for zero-shot cross-domain dialogue state
    tracking](https://aclanthology.org/2022.coling-1.42). In *Proceedings of the 29th
    International Conference on Computational Linguistics*, pages 510–520, Gyeongju,
    Republic of Korea. International Committee on Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh
    Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran,
    Anjana Arunkumar, David Stap, et al. 2022b. Super-naturalinstructions: Generalization
    via declarative instructions on 1600+ nlp tasks. In *Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing*, pages 5085–5109.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. 2022. [Emergent abilities of large language models](http://arxiv.org/abs/2206.07682).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Williams and Liden (2017) Jason D. Williams and Lars Liden. 2017. [Demonstration
    of interactive teaching for end-to-end dialog control with hybrid code networks](https://doi.org/10.18653/v1/w17-5511).
    In *Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue,
    Saarbrücken, Germany, August 15-17, 2017*, pages 82–85. Association for Computational
    Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. (2019) Chien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming
    Xiong, Richard Socher, and Pascale Fung. 2019. [Transferable multi-domain state
    generator for task-oriented dialogue systems](https://doi.org/10.18653/v1/P19-1078).
    In *Proceedings of the 57th Annual Meeting of the Association for Computational
    Linguistics*, pages 808–819, Florence, Italy. Association for Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xie et al. (2022) Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten
    Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I.
    Wang, Victor Zhong, Bailin Wang, Chengzu Li, Connor Boyle, Ansong Ni, Ziyu Yao,
    Dragomir Radev, Caiming Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith, Luke Zettlemoyer,
    and Tao Yu. 2022. [UnifiedSKG: Unifying and multi-tasking structured knowledge
    grounding with text-to-text language models](https://aclanthology.org/2022.emnlp-main.39).
    In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language
    Processing*, pages 602–631, Abu Dhabi, United Arab Emirates. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2021) Yunyi Yang, Yunhao Li, and Xiaojun Quan. 2021. Ubar: Towards
    fully end-to-end task-oriented dialog system with gpt-2. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 35, pages 14230–14238.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2021) Dian Yu, Luheng He, Yuan Zhang, Xinya Du, Panupong Pasupat,
    and Qi Li. 2021. [Few-shot intent classification and slot filling with retrieved
    examples](https://doi.org/10.18653/v1/2021.naacl-main.59). In *Proceedings of
    the 2021 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021*,
    pages 734–749. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zang et al. (2020) Xiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav
    Gupta, Jianguo Zhang, and Jindong Chen. 2020. [MultiWOZ 2.2 : A dialogue dataset
    with additional annotation corrections and state tracking baselines](https://doi.org/10.18653/v1/2020.nlp4convai-1.13).
    In *Proceedings of the 2nd Workshop on Natural Language Processing for Conversational
    AI*, pages 109–117, Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang* et al. (2020) Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger,
    and Yoav Artzi. 2020. [Bertscore: Evaluating text generation with bert](https://openreview.net/forum?id=SkeHuCVFDr).
    In *International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022) Xiaoying Zhang, Baolin Peng, Jianfeng Gao, and Helen Meng.
    2022. Toward self-learning end-to-end task-oriented dialog systems. In *Proceedings
    of the 23rd Annual Meeting of the Special Interest Group on Discourse and Dialogue*,
    pages 516–530.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) Yichi Zhang, Zhijian Ou, and Zhou Yu. 2020. Task-oriented
    dialog systems that consider multiple appropriate responses under the same context.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34,
    pages 9604–9611.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2022) Jeffrey Zhao, Yuan Cao, Raghav Gupta, Harrison Lee, Abhinav
    Rastogi, Mingqiu Wang, Hagen Soltau, Izhak Shafran, and Yonghui Wu. 2022. Anytod:
    A programmable task-oriented dialog system. *arXiv preprint arXiv:2212.09939*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao and Eskenazi (2018) Tiancheng Zhao and Maxine Eskenazi. 2018. Zero-shot
    dialog generation with cross-domain latent actions. *arXiv preprint arXiv:1805.04803*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao et al. (2021) Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer
    Singh. 2021. [Calibrate before use: Improving few-shot performance of language
    models](https://proceedings.mlr.press/v139/zhao21c.html). In *Proceedings of the
    38th International Conference on Machine Learning*, volume 139 of *Proceedings
    of Machine Learning Research*, pages 12697–12706\. PMLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Detailed Belief Instructions in DST Prompter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf5088c8a75710c083efec92da9eb9c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Detailed belief instructions in DST Prompter.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B A Formatting Example in Policy Prompter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Figure 7: A formatting example in Policy Prompter.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '$(\textup{\it i})$ Policy Prompter – policy skeleton: For the Multiwoz datasets,
    we manually construct the policy skeleton through observing a few dialogs in the
    training corpus, following Mehri and Eskenazi ([2021](#bib.bib27)); Mosig et al.
    ([2020](#bib.bib28)). In the case of the STAR corpus, we employ flow chart diagrams
    and several dialogs to develop the policy skeleton, following the guidelines set
    forth by Mehri and Eskenazi ([2021](#bib.bib27)). Furthermore, we incorporate
    the pertinent user template utterance into the system action within the policy
    skeleton to facilitate the LLM’s comprehension of directives, in the absence of
    belief annotations. The prompt examples for the STAR dataset are shown in Appendix
    [F](#A6 "Appendix F Prompt Examples for STAR Dataset ‣ SGP-TOD: Building Task
    Bots Effortlessly via Schema-Guided LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Our Proposed SGP-TOD with a Dialog Example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ba33c2ae23d22f3fe71402fd772c2cba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Illustration of the proposed SGP-TOD with a dialog example. Note
    that the belief state in the represented in the SQL format, the details of which
    are described in Section [3.3](#S3.SS3 "3.3 DST Prompter ‣ 3 Methodology ‣ SGP-TOD:
    Building Task Bots Effortlessly via Schema-Guided LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Zero-Shot End-to-End Evaluation Results on STAR
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a0a6d58b40e47881275368f4514103fd.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Task transfer
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c3567ae50ae918a6b8b67d2ec5570c43.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Domain transfer
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Zero-shot end-to-end evaluation results on STAR with different number
    of training dialogs (1, 10, 100, 1,000) / demonstration example turns (1, 10)
    from source domain/tasks. (Note the numbers are represented in logarithm to base
    10.)'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Prompt Examples for STAR Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b87022502a5620ed890eaeec2842076.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Policy Prompter of SGP-TOD on STAR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a3f654d2586a4f4b32d904ce3edd68f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Policy Prompter of SGP-TOD-E2E on STAR.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G An Example of Domain Extension
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A dialog example of domain extension.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Figure [12](#A7.F12 "Figure 12 ‣ A dialog example of domain extension. ‣ Appendix
    G An Example of Domain Extension ‣ SGP-TOD: Building Task Bots Effortlessly via
    Schema-Guided LLM Prompting") depicts an example of domain extension.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c806bc81ad628f9e2de6deb9230e68da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: A dialog example of domain extension cited from Zhang et al. ([2022](#bib.bib61)).
    Task bots need learn to provide pertinent responses concerning the expanded delivery
    service in additional dialog turns (highlighted in red), as user or business requirements
    evolve.'
  prefs: []
  type: TYPE_NORMAL
- en: An example of Restaurant-Ext DB entry.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An example of Restaurant-Ext DB entry is shown in Figure [13](#A7.F13 "Figure
    13 ‣ An example of Restaurant-Ext DB entry. ‣ Appendix G An Example of Domain
    Extension ‣ SGP-TOD: Building Task Bots Effortlessly via Schema-Guided LLM Prompting").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/769fd5c8c7d0a66181b6be7a6b48c33f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: An example of Restaurant-Ext DB entry cited from Zhang et al. ([2022](#bib.bib61)).
    The supplementary information pertaining to the extended functionality delineated
    within the red square.'
  prefs: []
  type: TYPE_NORMAL
