- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:28'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Joint Prompt Optimization of Stacked LLMs using Variational Inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.12509](https://ar5iv.labs.arxiv.org/html/2306.12509)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alessandro Sordoni^(ab)    Xingdi Yuan^a  Marc-Alexandre Côté^a  Matheus Pereira^a
  prefs: []
  type: TYPE_NORMAL
- en: Adam Trischler^a  Ziang Xiao^a Arian Hosseini^b  Friederike Niedtner^a Nicolas
    Le Roux^(ab)
  prefs: []
  type: TYPE_NORMAL
- en: 'Microsoft Research Montréal^a   MILA^b Corresponding author: [alsordon@microsoft.com](mailto:alsordon@microsoft.com)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) can be seen as atomic units of computation mapping
    sequences to a distribution over sequences. Thus, they can be seen as stochastic
    language layers in a language network, where the learnable parameters are the
    natural language prompts at each layer. By stacking two such layers and feeding
    the output of one layer to the next, we obtain a Deep Language Network (DLN).
    We first show how to effectively perform prompt optimization for a 1-Layer language
    network (DLN-1). Then, we present an extension that applies to 2-layer DLNs (DLN-2),
    where two prompts must be learned. The key idea is to consider the output of the
    first layer as a latent variable, which requires inference, and prompts to be
    learned as the parameters of the generative distribution. We first test the effectiveness
    of DLN-1 in multiple reasoning and natural language understanding tasks. Then,
    we show that DLN-2 can reach higher performance than a single layer, showing promise
    that we might reach comparable performance to GPT-4, even when each LLM in the
    network is smaller and less powerful. The DLN code is open source.¹¹1Code: [https://github.com/microsoft/deep-language-networks](https://github.com/microsoft/deep-language-networks).'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The size of large language models (LLMs) has grown significantly over the last
    few years, mainly because of emerging capabilities [[5](#bib.bib5), [31](#bib.bib31)],
    but at considerable technical and societal costs [[49](#bib.bib49), [2](#bib.bib2),
    [4](#bib.bib4)]. Recent efforts have focused either on learning smaller models
    matching the abilities of larger ones on some tasks using distillation [[43](#bib.bib43),
    [36](#bib.bib36), [29](#bib.bib29), [13](#bib.bib13)], or offloading part of the
    computation to other dedicated components [[28](#bib.bib28), [22](#bib.bib22),
    [25](#bib.bib25), [18](#bib.bib18)]. In the latter case, this is done through
    carefully crafted instructions to retrieve the necessary information from these
    additional modules [[48](#bib.bib48), [41](#bib.bib41), [6](#bib.bib6), [54](#bib.bib54),
    [24](#bib.bib24)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruction-tuned LLMs map an input sequence to a distribution over output
    sequences conditioned on an instruction, or *prompt*. In this paper, we view such
    LLMs as stochastic language layers, whose learnable parameters are the prompts.
    Multiple layers can be stacked to form a Deep Language Network (DLN) whose learnable
    parameters are the prompts associated to each layer. Specifically, each layer
    uses a template to organize both its prompt and the inputs coming from the layer
    below into a single sequence before producing the output (see Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Joint Prompt Optimization of Stacked LLMs using Variational
    Inference")). This layering induces a learnable decomposition of the task into
    a series of smaller sub-tasks, each of which might be more easily solvable by
    an LLM. This view shares similarities to recent works that chain LLM calls [[41](#bib.bib41),
    [6](#bib.bib6), [54](#bib.bib54)]. In this work, we move towards integrating learnable
    components in the pipeline: each prompt can be learned to maximize the final objective.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d2f25a815118ce4887f4d323e338fd75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: *Left*: An illustration of a DLN-1 performing a sentiment analysis
    task: input and the trainable prompt are merged using a template and fed to the
    LM for answer generation. *Right*: a DLN-2 with a residual connection, performing
    the date understanding task: two prompts need to be learned. In this example,
    the hidden template extends Chain-Of-Thought [[48](#bib.bib48)] with a learnable
    prefix; we consider the output of the first layer, hidden, as a *latent variable*
    $h$. Templates can be considered as an hyperparameter of the network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first show how to perform prompt optimization in a shallow 1-layer language
    network (DLN-1) which parametrizes a distribution $p_{\texttt{LM}}(y|x,\pi)$ is
    the learnable prompt (Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Joint Prompt
    Optimization of Stacked LLMs using Variational Inference"), *left*). Our prompt
    optimization techniques extend the Automatic Prompt Engineer (APE) procedure from Zhou
    et al., 2023b [[57](#bib.bib57)]. We show how our prompts can include a verbalization
    of difficult examples from the task: the final prompts are a combination of instruction
    directives, akin to zero-shot learning [[17](#bib.bib17)], and task examples,
    akin to in-context learning [[20](#bib.bib20)]. This significantly improves downstream
    performance, surpassing APE on several tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we show how to train a 2-layer DLN (DLN-2), which parametrizes a probability
    distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{\tiny\texttt{DLN-2}}(y&#124;x)=\sum_{h}p_{\texttt{LM}}(y&#124;h,x,\pi_{1})\,p_{\texttt{LM}}(h&#124;x,\pi_{0})\,,$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $h$. Note that this formalism easily encompasses more than two layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering outputs of the hidden language layers as latent variables allows
    us to encompass various established prompting methods, such as Chain-Of-Thought
    (CoT) [[48](#bib.bib48)] and self-consistency (SC-CoT) [[46](#bib.bib46)]. Particularly,
    CoT can be seen as a particular DLN-2 with the first layer prompt set to ‘‘Let’s
    think step by step’’ and the second layer prompt set to ‘‘The answer is’’; we
    can either learn such prompts or learn a supplement to those as in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Joint Prompt Optimization of Stacked LLMs using Variational
    Inference"). SC-CoT can be seen as marginalizing over CoT strings sampled from
    a task-agnostic prior: when using the template in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ Joint Prompt Optimization of Stacked LLMs using Variational
    Inference") (*right*), our method generalizes this perspective by learning a task-specific
    prior distribution over successful CoTs.'
  prefs: []
  type: TYPE_NORMAL
- en: The rest of the paper is organized as follows. First, we provide an interpretation
    of LLMs as shallow networks, drawing a number of analogies with standard parametric
    and non-parametric models and explaining how best to train them. After exploring
    their limitations, we propose to stack two such LLMs to form a DLN-2\. We show
    how they can be trained using a form of variational inference, then demonstrate
    their performance on a series of reasoning and language understanding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2 One-Layer Language Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A pre-trained LLM with frozen weights might be thought of as a complete *function
    class* indexed by prompts. The output $y$ to the LLM. Hence, from a low-level
    perspective, the function class of an LLM is defined by its architecture, i.e.,
    its depth, number of heads, context size, etc., and training happens at the parameter
    level. It is data and compute intensive and should be done rarely. From a high-level
    perspective, the function class of an LLM is defined by the pre-trained model
    chosen (LLAMA [[44](#bib.bib44)], text-davinci-003, GPT-4, etc.), and training
    happens by fine-tuning the model or by choosing the prompt.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways of optimizing an LLM at the prompt level. The first one is
    *prompt engineering*, a parametric optimization, where the optimization space
    is independent of the size of the dataset. Because this optimization usually happens
    in discrete space, gradient-based techniques do not apply and most efforts rely
    on a combination of random or local search and human heuristics [[21](#bib.bib21),
    [55](#bib.bib55)]. The second one is *in-context learning* (ICL), a non-parametric
    optimization technique where the solution is a direct function of a subset of
    examples [[5](#bib.bib5), [20](#bib.bib20)]. This approach works well for few-shot
    learning but scaling it to larger datasets has both performance and computational
    issues. We shall now generalize previous work in discrete prompt optimization [[57](#bib.bib57),
    [21](#bib.bib21)] with the ultimate goal of learning a set of prompts in a language
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Language Layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use *language layer* to refer to a (stochastic) computation that takes as
    input a string $x$”. We also explore more complex ones, examples of which can
    be seen in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Joint Prompt Optimization
    of Stacked LLMs using Variational Inference").
  prefs: []
  type: TYPE_NORMAL
- en: Given an input $x$ for a language layer.
  prefs: []
  type: TYPE_NORMAL
- en: '2.2 Prompt Optimization: Improved APE'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Because the search for the best prompt happens over a discrete space, we will
    rely on a local search method, using an LLM to implement a distance measure between
    prompts. The procedure can be seen as an extension of Automatic Prompt Engineer
    (APE), recently proposed by Zhou et al., 2023b [[57](#bib.bib57)], and will serve
    as a stepping stone towards introducing our algorithm for training deep language
    networks. Our prompt optimization algorithm can be structured as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Given the current prompt $\pi$ using a prompt *proposal* distribution;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Score each candidate using a (potentially stochastic) *scoring* function $s$.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Algorithm 1 One-Layer Language Network (DLN-1) Training Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:$\hat{y}\sim p_{\texttt{LM}}^{t}(y|c)$ $\triangleright$ Select prompt with
    best score13:end for
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Proposal  Local search algorithms assume a distance measure between inputs
    to crawl the search space. In this setting, we rely on LLMs to generate local
    modifications to the prompts. Our prompt proposal distribution takes as conditioning
    information i) the batch given as input to the layer, ii) its corresponding output
    $\{x,y,\hat{y}\}$. We devise several strategies to improve the diversity and the
    usefulness of the candidate samples in Section [4](#S4 "4 Practical Instantiation
    ‣ Joint Prompt Optimization of Stacked LLMs using Variational Inference").
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt Selection  Once a set of $N$. In practice, we normalize this log-probability
    by the length of the output string. While we focus on that metric in this work,
    there is no restriction on the scoring function that can be used. We use backtracking
    to increase the robustness of our selection mechanism, as well as a memory of
    well-performing prompts for efficiency. We present both strategies in Section [4](#S4
    "4 Practical Instantiation ‣ Joint Prompt Optimization of Stacked LLMs using Variational
    Inference"). The sketch of a 1-layer prompt optimization algorithm is described
    in Algorithm [1](#alg1 "Algorithm 1 ‣ 2.2 Prompt Optimization: Improved APE ‣
    2 One-Layer Language Networks ‣ Joint Prompt Optimization of Stacked LLMs using
    Variational Inference"), ignoring backtracking and memory for simplicity.'
  prefs: []
  type: TYPE_NORMAL
- en: The results of our prompt optimization may be found in Table [1](#S5.T1 "Table
    1 ‣ 5.2 DLN-1 ‣ 5 Experiments and Results ‣ Joint Prompt Optimization of Stacked
    LLMs using Variational Inference") and will be discussed in detail in Section [5.2](#S5.SS2
    "5.2 DLN-1 ‣ 5 Experiments and Results ‣ Joint Prompt Optimization of Stacked
    LLMs using Variational Inference"). We now turn to extending prompt optimization
    to architectures with two layers.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Two-Layer Deep Language Networks (DLN-2)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The natural extension of DLN-1 is DLN-2, in which language layers are stacked, i.e.
    the output of the first language layer is the input to the second one. A 2-layer
    network induces a distribution over outputs of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle p_{\tiny\texttt{DLN-2}}(y&#124;x)$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $h$, and we do not assume access to the LLM parameters.
  prefs: []
  type: TYPE_NORMAL
- en: While this architecture has more expressive power than a shallow language network,
    the prompt optimization problem becomes harder now that we have to *jointly* search
    over both $\pi_{0}$. Doing random search in this space is impractical [[8](#bib.bib8)]
    and manual tuning of the weights is also exponentially harder than with a single
    prompt. We turn to variational inference to address this issue.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Variational Inference Objective
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The layerwise decomposition of our system allows us to leverage tools from
    approximate inference in probabilistic models to learn $\Pi$ by computing the
    ELBO:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\log p_{\tiny\texttt{DLN-2}}(y&#124;x)$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'which allows us to decompose the optimization over $\Pi$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: The search over $\pi_{1}$.
  prefs: []
  type: TYPE_NORMAL
- en: Although this bound allows us to decompose the optimization w.r.t. $\Pi$ closely
    matching the true posterior. In what follows, we specify how we parametrize the
    approximate posterior and how we tighten the approximation via posterior sharpening.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Proposal  We will also be using an LLM to sample candidate hidden states
    from $q(h)$.
  prefs: []
  type: TYPE_NORMAL
- en: Posterior Sharpening  Given the absence of learnable parameters in $q(h)$ is
    a tunable temperature parameter that controls the entropy of the posterior weights.
    The full algorithm for training a DLN-2 is presented in Algorithm [2](#alg2 "Algorithm
    2 ‣ 3.1 Variational Inference Objective ‣ 3 Two-Layer Deep Language Networks (DLN-2)
    ‣ Joint Prompt Optimization of Stacked LLMs using Variational Inference").
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 2 Two-Layer Deep Language Network (DLN-2) Training Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:$\hat{y}\sim p_{\texttt{LM}}^{t}(c)$ with a generic sentence or task description7:for $i$
    $\triangleright$ Compute ELBO for all prompts $\pi_{0}^{n}$ with best score22:end for
  prefs: []
  type: TYPE_NORMAL
- en: 4 Practical Instantiation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although our method aim to learning prompts in stacked LLM architectures, we
    do rely on a good amount of prompt engineering for our templates. Hereafter, we
    detail some choices that were fundamental to make our approach work in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Proposal Diversity  To ensure a diversity of the samples for both the prompt
    proposal distribution, we found helpful to use two strategies. The first is to
    modify the backward templates $\texttt{B}_{\pi}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning In-Context Learning  One strategy we found particularly effective
    is to integrate in the pool of meta-instructions an additional instruction that
    asks the LM to give useful examples to improve its current prompt $\pi$ that contain
    synthetic examples for the task, embedded in natural language. Examples of this
    interesting behavior can be found in Appendix [F](#A6 "Appendix F Learning to
    In-Context Learn: Additional Examples ‣ Joint Prompt Optimization of Stacked LLMs
    using Variational Inference"). We found that this behavior is particularly interesting
    as the resulting prompts often perform better than standard ICL. We hypothesize
    this is due to both *i)* the “verbalization” of the example in the prompt, which
    modifies the dataset syntax into a more suitable one, and *ii)* the fact that
    the model can dynamically select which examples are most important to integrate
    in the prompts, given the errors made during training. Therefore, we suspect that
    DLN achieves a similar effect to recent techniques that select important examples
    for ICL [[20](#bib.bib20), [35](#bib.bib35), [40](#bib.bib40), [18](#bib.bib18),
    [47](#bib.bib47)] with the improvement of naturally conditioning the selection
    on the end task performance via end-to-end training.'
  prefs: []
  type: TYPE_NORMAL
- en: Backtracking and Memory  Optimization of both DLN-1 and DLN-2 is challenging
    due to the fact that we do not have gradient information and we sample a restricted
    set of candidates $\pi^{n}$ best prompts found by tracking validation set performance.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration Reward  When training a DLN-2, we empirically observed that the
    first layer prompt $\pi_{0}$ by monitoring validation performance for each task.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Experiments and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We design and conduct a set of experiments to help answer two main research
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Q1: Can we outperform APE and In-Context Learning (ICL) with a DLN-1?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Q2: Does network depth provide further improvement upon DLN-1?'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.1 Experimental Setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets and Tasks  We adopt a set of nine NLP and reasoning tasks commonly
    used in prior work studying zero- or few-shot learning capabilities of LLMs [[23](#bib.bib23),
    [10](#bib.bib10), [39](#bib.bib39), [42](#bib.bib42), [1](#bib.bib1)]. We focus
    on classification tasks. For tasks adopted from BigBench-Hard (BBH) [[42](#bib.bib42)]
    (Hyper., Nav., Date. and Logic.7 ²²2We only use the variant with seven objects.),
    we use the 250 data points provided by BBH as test set. We take the remaining
    data points from BigBench [[39](#bib.bib39)] that were not included in BBH, and
    randomly split them (evenly) into training and validation sets. For tasks adopted
    from [[23](#bib.bib23)] (Mpqa, Trec, and Subj), we randomly sample 400 and 250
    data points from their training and test sets, respectively. We use the original
    validation sets. For tasks adopted from Leopard [[1](#bib.bib1)] (Disaster and
    Airline), we randomly sample 400, 250, and 250 data points as training, valid,
    and test. We list all tasks and their statistics in Table [3](#A2.T3 "Table 3
    ‣ B.1 Additional Task Information ‣ Appendix B Additional Experimental Details
    ‣ Joint Prompt Optimization of Stacked LLMs using Variational Inference") in the
    Appendix.
  prefs: []
  type: TYPE_NORMAL
- en: We use accuracy as the evaluation metric. Specifically, given an input, we compare
    a system’s output string against the ground-truth output string provided by the
    dataset. We score 1 if the two strings are identical and 0 otherwise. Before the
    comparison, we process the strings from both the model output and the ground-truth
    to deal with issues like tokenization and capitalization. In all our DLN experiments,
    we perform a hyperparameter search and run the same hyperparameter setting with
    three random seeds. We report the test accuracy averaged over three seeds corresponding
    to the hyperparameter setting that achieves the highest average validation accuracy.
    We report details of the hyperparameter search in the Appendix [I](#A9 "Appendix
    I Implementation Details ‣ Joint Prompt Optimization of Stacked LLMs using Variational
    Inference").
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this paper, we use OpenAI’s models, specifically GPT-3 (text-davinci-003)
    and GPT-4, as the backbone to our proposed systems unless otherwise specified.
    For DLNs, we use a batch size of 20 and train for 20 iterations by early-stopping
    on validation performance evaluated every 2 iterations. We then report test scores.
    We sample $N=20$ hidden samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines  We compare the DLN against two classes of baseline systems. First,
    we test a set of systems equipped with the same backbone (i.e., GPT-3):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '0-shot: Given an input, the LLM is required to generate the answer in a zero-shot
    manner.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '5-shot (ICL): Given an input as well as five data points as in-context examples,
    the LLM is queried to generate an answer. The five examples are randomly sampled
    from the training set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KATE [[20](#bib.bib20)]: Given an input, we retrieve the five most similar
    data points from the training set using an off-the-shelf sentence encoder, and
    use them as in-context examples.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'APE [[57](#bib.bib57)]: The LLM is queried to generate a pool of candidate
    prompts for the task given few input-output pair examples. The candidate prompts
    are evaluated on a validation set to find the best performing instruction prompt.
    The best instruction is then used for 0-shot evaluation. We optimize the prompt
    over both 15 and 400 examples (APE-15 and APE-400 respectively).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CoT [[48](#bib.bib48)]: Given an input, the LLM is first queried to generate
    a reasoning path with the prompt ‘‘Let’s think step by step’’. Then, conditioned
    on the input and its first output, the LLM is queried to generate an answer. This
    is the zero-shot version of CoT and is a natural baseline for DLN-2: it performs
    two LLM calls and can be seen as DLN-2 without optimization. We will report performance
    of this baseline when comparing to DLN-2.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Additionally, we compare against one of the most advanced LLMs to date, GPT-4.
    We test 0-shot and ICL settings with GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 DLN-1
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our first set of experiments evaluates the 1-layer language network (DLN-1)
    described in Section [2](#S2 "2 One-Layer Language Networks ‣ Joint Prompt Optimization
    of Stacked LLMs using Variational Inference"). Table [1](#S5.T1 "Table 1 ‣ 5.2
    DLN-1 ‣ 5 Experiments and Results ‣ Joint Prompt Optimization of Stacked LLMs
    using Variational Inference") presents results on the full suite of test tasks.
    We see that it matches the performance of the best GPT-3-based method on Disaster,
    Mpqa and Airline and narrowly beats the best GPT-3 baseline on Logic.7 and Nav..
    On Hyper., Trec, and Subj, DLN-1 significantly outperforms the best GPT-3 baseline
    (by about 20, 10, and 7 percentage points, respectively). On Hyper., Trec, and
    Disaster, it even surpasses GPT-4 baselines, unsurprisingly underperforming GPT-4
    on all other tasks. DLN-1’s excellent performance on Hyper., a BBH task about
    ordering adjectives according to linguistic convention, is a surprise. To better
    understand this result, we show the final prompt in Figure [2](#S5.F2 "Figure
    2 ‣ 5.2 DLN-1 ‣ 5 Experiments and Results ‣ Joint Prompt Optimization of Stacked
    LLMs using Variational Inference"). We see that the prompt contains both instructions
    and a list of examples from the training set. These examples were automatically
    chosen by the optimizer based on their impact on the performance. This can be
    seen as a combination of KATE, which selects training examples to put in context
    based on their similarity with the test example, and APE, which selects the prompt
    based on its performance. On Date., DLN-1 tends to systematically under-perform
    the 0-shot baseline both for GPT-3 and GPT-4\. We observed that DLN-1 overfits
    due to paucity of examples in the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Test accuracy averaged over three random seeds of a shallow, 1-layer
    language network (DLN-1) compared to baselines both on GPT-3 and GPT-4\. For trainable
    systems (i.e., APE and DLN-1) or systems relying on GPT-4, we report the 95% confidence
    interval.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BigBench Hard | NLU | Leopard |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Hyper. | Nav. | Date. | Logic.7 | Mpqa | Trec | Subj | Disaster
    | Airline |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-3 |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0-shot | 60.8 | 64.1 | 56.4 | 45.9 | 88.0 | 61.9 | 61.7 | 81.6 | 75.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 55.6 | 56.5 | 62.1 | 36.7 | 87.2 | 80.0 | 76.4 | 81.2 | 82.7 |'
  prefs: []
  type: TYPE_TB
- en: '| KATE | 71.1 | 56.9 | 61.1 | 44.4 | 88.4 | 77.6 | 71.1 | 76.0 | 81.6 |'
  prefs: []
  type: TYPE_TB
- en: '| APE-15 | 68.5$\pm$3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| APE-400 | 65.5$\pm$10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 | 91.9$\pm$5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0-shot | 64.0$\pm$0.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 88.4$\pm$1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 16-shot | 93.3$\pm$2.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 | 95.2$\pm$1.5 |'
  prefs: []
  type: TYPE_TB
- en: 'DLN-1
    prompt on Hyperbaton (GPT-3) When constructing a sentence
    with multiple adjectives, the order should be opinion, size, age, shape, color,
    origin, material, and purpose. Adjectives of the same type should be listed in
    descending order from largest to smallest. When adjectives of different types
    are used, the order should be opinion, size, age, shape, color, origin, material,
    and purpose. For example, in the phrase “massive ancient chair” size (massive)
    should come before age (ancient). Examples: little old-fashioned Russian silver
    rectangular ship; silly large old leather hiking chair; brand-new spherical Mexican
    sweater; enormous old spherical green Nigerian exercise car; medium-size triangular
    wool eating ship; good square brown Egyptian ship; lovely massive drinking monkey;
    archaic circular white plastic shoe. In each of the following examples, the adjective
    order is wrong. Identify the correct adjective order:DLN-1
    prompt on Hyperbaton (GPT-4) To determine the correct adjective
    order, follow this sequence: opinion, size, shape, age, color, origin, material,
    and purpose. For example, choose "large red plastic ball" over "red large plastic
    ball" since it follows the order: size (large), color (red), and material (plastic).
    Not all adjectives may be present, but the order should still be maintained. If
    the options are "ancient prismlike white leather whittling match" and "leather
    white ancient prismlike whittling match", choose the first option, as it follows
    the order: age (ancient), shape (prismlike), color (white), material (leather),
    and purpose (whittling). Remember that opinion always comes before age, so "obnoxious
    old-fashioned typing shoe" is correct over "old-fashioned obnoxious typing shoe."
    Ensure opinion adjectives come before other adjectives in the sequence. When comparing
    options, follow the order of adjectives for each category: size before color,
    color before origin, and so on. In cases where purpose and material adjectives
    are switched, like "paper walking monkey" vs "walking paper monkey", choose the
    option where material comes before the purpose. Additionally, always prioritize
    the given sequence over the position of adjectives in the sentences. For example,
    choose "midsize brand-new gray Chinese wood sweater" over "Chinese brand-new gray
    midsize wood sweater" as it follows the order: size (midsize), age (brand-new),
    color (gray), origin (Chinese), and material (wood).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: The final prompt of the DLN-1 on Hyperbaton includes not only instructions
    but also examples from the training set. These samples were automatically chosen
    by the prompt optimization. In a way, this approach combines in-context learning
    and prompt optimization.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 DLN-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We investigate the effectiveness of depth through experiments with 2-layer language
    networks (DLN-2) on tasks where we expect depth to be most useful, and on which
    DLN-1 significantly underperforms the GPT-4 0-shot baseline, i.e., Nav., Date.,
    and Logic.7 [[42](#bib.bib42)]. Since the Nav., Date. and Logic.7 tasks from BBH
    require more complex spatial and temporal reasoning, they are the ones where we
    most expect a decomposition into subtasks to be helpful. We also include Subj
    and Disaster as an example where DLN-1 performs well (even outperforming the GPT-4
    0-shot baseline), since we are interested to see to what extent DLN-2 can further
    push performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: DLN-2 test accuracy using GPT-3 as LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Nav. | Date. | Logic.7 | Disaster | Subj |'
  prefs: []
  type: TYPE_TB
- en: '| 0-shot | 64.1 | 56.4 | 45.9 | 81.6 | 61.7 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT | 69.3 | 72.4 | 41.1 | 54.4 | 59.3 |'
  prefs: []
  type: TYPE_TB
- en: '| APE | 67.3$\pm$7.2 |'
  prefs: []
  type: TYPE_TB
- en: '| APE-400 | 56.9$\pm 32.9$9.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 | 68.5$\pm$5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-2 | 83.1$\pm$8.7 |'
  prefs: []
  type: TYPE_TB
- en: Results for DLN-2 can be found in Table [2](#S5.T2 "Table 2 ‣ 5.3 DLN-2 ‣ 5
    Experiments and Results ‣ Joint Prompt Optimization of Stacked LLMs using Variational
    Inference"). Compared to DLN-1, DLN-2 provides an average boost of 7.2% absolute
    score. On Nav. and Date., DLN-2 largely improves the performance of DLN-1, outperforming
    all single layer networks. On Logic.7, all methods appear to perform similarly.
    This could point to the fact that the task might be too hard for the base LLM
    and thus highlights the limits of prompt optimization of a weak base model. On
    Subj and Disaster, DLN-2 achieves further improvement over DLN-1\. Compared to
    0-shot GPT-4 results in Table [1](#S5.T1 "Table 1 ‣ 5.2 DLN-1 ‣ 5 Experiments
    and Results ‣ Joint Prompt Optimization of Stacked LLMs using Variational Inference"),
    on Subj and Disaster, DLN-2 on average provides more than 20% in absolute improvement.
    We encourage readers to find additional experimental results in Appendix [C](#A3
    "Appendix C Additional Experiments ‣ Joint Prompt Optimization of Stacked LLMs
    using Variational Inference").
  prefs: []
  type: TYPE_NORMAL
- en: 6 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Prompt-Based Machine Learning GPT-3 [[5](#bib.bib5)] launched a new paradigm
    in NLP called in-context learning (ICL), now applied beyond traditional NLP tasks [[21](#bib.bib21)].
    The discovery of chain-of-thought prompts (CoT) marked a major advance in prompting:
    LLM performance improves markedly when the prompt includes examples of intermediate
    reasoning steps [[48](#bib.bib48)] (few-shot CoT), or simply instructs the model
    to “think step by step” [[17](#bib.bib17)] (zero-shot CoT). Like CoT, DLNs break
    a problem down into intermediate steps but they operationalize these steps as
    separate LLM calls, each defined by its own learned prompt. Since the introduction
    of CoT, prompting techniques have evolved to be more dynamic and iterative. Recent
    methods often operate recursively. Examples include RECITE [[41](#bib.bib41)],
    Self-ask [[33](#bib.bib33)], and related methods for question-answering Creswell
    et al., [[6](#bib.bib6)], Zhou et al., 2023a [[56](#bib.bib56)]. A similar class
    of methods relies on “introspection” [[14](#bib.bib14)], where an LLM is prompted
    to ingest, evaluate then possibly act on its own previous output. Self-critique [[46](#bib.bib46)],
    ReAct [[54](#bib.bib54)], Reflexion [[38](#bib.bib38)], Self-refine [[24](#bib.bib24)]
    fit this mould along with Hao et al., [[11](#bib.bib11)], Du et al., [[9](#bib.bib9)],
    Yao et al., 2023a [[53](#bib.bib53)].'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Optimization Techniques based on notions of self-talk and self-evaluation
    align naturally with automatic prompt optimization—a core function in DLNs. Early
    work in this category includes Autoprompt [[37](#bib.bib37)] and GRIPS [[32](#bib.bib32)]. Deng
    et al., [[7](#bib.bib7)] argue that ‘enumeration-then-selection’ heuristics for
    optimizing discrete prompts do not explore the prompt space systematically. They
    take an RL approach to overcome this problem, training a policy network, via soft
    Q-learning with a suitably designed and stabilized reward function, to generate
    effective prompts. Through Gibbs sampling, Reprompting  [[52](#bib.bib52)] iteratively
    searches CoT recipes to improve prompt performance automatically. Most relevant
    to DLNs, Zhou et al., 2023b [[57](#bib.bib57)] present Automatic prompt engineer
    (APE). APE optimizes an initial prompt by searching over a pool of candidates
    to maximize a score function. We use an APE-inspired approach in DLNs and we cast
    the proposal/scoring functions as elements of variational inference. In a concurrent
    work, Pryzant et al., [[34](#bib.bib34)] proposed using textual gradients in automatic
    prompt optimization. This algorithm uses LLM’s nonparametric feedback to guide
    prompt generation and selection.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Layer LLM systems Several recent works compose LLMs as nodes in a computational
    graph, which is the core idea of DLNs. Some work cited above can be seen as instances
    of this idea. Similarly, Khot et al., [[15](#bib.bib15)] induce an LLM to generate
    a basic “control flow” that calls distinct LLM modules. Wu et al., [[50](#bib.bib50)]
    propose AI chains, an interactive system of chained LLMs based on a set of “LLM
    primitive” operations. They conduct a 20-person user study in which participants
    modify chains, and find this process to improve task performance, transparency,
    and controllability. Dohan et al., [[8](#bib.bib8)] unify LLMs and graphical models
    as “language model cascades”. Specifically, they cast LLM compositions as graphical
    models with string-valued random variables.³³3Earlier work by Miao and Blunsom,
    2016b [[27](#bib.bib27)] also treated strings as random variables. They show how
    scratchpad [[30](#bib.bib30)], chain-of-thought [[48](#bib.bib48)], tool use [[25](#bib.bib25)],
    and several other prompting strategies fit their formalism. DLNs can likewise
    be considered an instance of language model cascade, because of that framework’s
    generality. However, going beyond the conceptual work of Dohan et al., [[8](#bib.bib8)],
    we present an effective technique for doing inference in an LLM-based graphical
    model and we apply learned networks of LLMs to several downstream tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper we introduced an algorithm for joint prompt optimization in deep
    networks where each layer is an LLM. To do so, we consider outputs of each hidden
    LLM layer as a latent variable we need to do inference over. From a conceptual
    perspective, we demonstrated how CoT can be seen as a DLN-2 with a residual connection.
    Similarly, Generated Knowledge Prompting [[19](#bib.bib19)] could be considered
    as a fixed forward-only DLN-2 where, in the first layer, an LLM generates related
    knowledge, and in the second layer, another LLM takes the generated knowledge
    as input and generates the final answer. Other prompting techniques like ReAct [[54](#bib.bib54)],
    Reflexicon [[38](#bib.bib38)], and Self-Consistency [[46](#bib.bib46)] could all
    be ensembles of DLN-1s with different prompt initializations.
  prefs: []
  type: TYPE_NORMAL
- en: Although we only tested 1-layer and 2-layer LNs so far, we already show that
    the performance of smaller LLMs can be boosted when stacked and prompted properly.
    We believe the modularity of these architectures will make them more adaptable
    and reusable to new use cases. While accuracy on downstream tasks is an appealing
    metric, we argue that other considerations are just as important, for example
    the ease of adapting a model to one’s own use case, or the ability to leverage
    multiple existing models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We noticed that GPT-3 has a tendency to always produce an answer given an example:
    this could be due to the particular 0-shot fine-tuning procedure, which biases
    the model towards generating useful responses. This raises the question of whether
    we can fine-tune “stackable” LLMs and whether DLNs can be used as a framework
    to generate training data for that purpose. Second, we engineered our backward
    and forward templates; in the future, we wish to expand our work to learn parts
    of such templates: we expect this to make the variational bound tighter and thus
    easing DLN’s optimization. Additionally, while we only proposed 2-layer DLNs,
    the framework accommodates arbitrary directed acyclic graphs.'
  prefs: []
  type: TYPE_NORMAL
- en: Impact statement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: While we are fully aware of the limitations of addressing societal issues through
    technical work, we hope that modular approaches like ours will alleviate some
    of the issues associated with LLMs, like the concentration of power associated
    with the difficulty to train them. We also hope that, by facilitating the reusability
    and adaptivity of such models, we shall make them more amenable to a wider variety
    of use cases. However, while we discuss the performance of these models on artificial
    benchmarks, we do not address the question of when and how such models should
    be deployed, nor do we offer additional guarantees against their misuse. We also
    emphasize that performance on artificial tasks, even if realistic, is neither
    representative of performance in uncontrolled environments, nor enough to justify
    the deployment of these models in high stakes situations.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We would like to acknowledge Silviu Pitis for the useful feedback on the draft,
    Nikolay Malkin and Tong Wang for their advice during the first steps of this project.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bansal et al., [2020] Bansal, T., Jha, R., and McCallum, A. (2020). Learning
    to few-shot learn across diverse natural language classification tasks. In Proceedings
    of the 28th International Conference on Computational Linguistics, pages 5108–5123,
    Barcelona, Spain (Online). International Committee on Computational Linguistics.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bender et al., [2021] Bender, E. M., Gebru, T., McMillan-Major, A., and Mitchell,
    M. (2021). On the dangers of stochastic parrots: Can language models be too big?
    In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency,
    pages 610–623.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blei et al., [2017] Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. (2017).
    Variational inference: A review for statisticians. Journal of the American statistical
    Association, 112(518):859–877.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Blodgett et al., [2022] Blodgett, S. L., Liao, Q. V., Olteanu, A., Mihalcea,
    R., Muller, M., Scheuerman, M. K., Tan, C., and Yang, Q. (2022). Responsible language
    technologies: Foreseeing and mitigating harms. In Extended Abstracts of the 2022
    CHI Conference on Human Factors in Computing Systems, CHI EA ’22, New York, NY,
    USA. Association for Computing Machinery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al., [2020] Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. (2020).
    Language models are few-shot learners. Advances in neural information processing
    systems, 33:1877–1901.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Creswell et al., [2022] Creswell, A., Shanahan, M., and Higgins, I. (2022).
    Selection-inference: Exploiting large language models for interpretable logical
    reasoning. arXiv preprint arXiv:2205.09712.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al., [2022] Deng, M., Wang, J., Hsieh, C., Wang, Y., Guo, H., Shu,
    T., Song, M., Xing, E. P., and Hu, Z. (2022). Rlprompt: Optimizing discrete text
    prompts with reinforcement learning. In Goldberg, Y., Kozareva, Z., and Zhang,
    Y., editors, Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11,
    2022, pages 3369–3391\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dohan et al., [2022] Dohan, D., Xu, W., Lewkowycz, A., Austin, J., Bieber, D.,
    Lopes, R. G., Wu, Y., Michalewski, H., Saurous, R. A., Sohl-Dickstein, J., et al.
    (2022). Language model cascades. arXiv preprint arXiv:2207.10342.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al., [2023] Du, Y., Li, S., Torralba, A., Tenenbaum, J. B., and Mordatch,
    I. (2023). Improving factuality and reasoning in language models through multiagent
    debate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al., [2021] Gao, T., Fisch, A., and Chen, D. (2021). Making pre-trained
    language models better few-shot learners. In Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers), pages 3816–3830,
    Online. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al., [2023] Hao, S., Gu, Y., Ma, H., Hong, J. J., Wang, Z., Wang, D. Z.,
    and Hu, Z. (2023). Reasoning with language model is planning with world model.
    arXiv preprint arXiv:2305.14992.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Honovich et al., [2022] Honovich, O., Shaham, U., Bowman, S. R., and Levy,
    O. (2022). Instruction induction: From few examples to natural language task descriptions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hosseini et al., [2021] Hosseini, A., Reddy, S., Bahdanau, D., Hjelm, R. D.,
    Sordoni, A., and Courville, A. C. (2021). Understanding by understanding not:
    Modeling negation in language models. In Toutanova, K., Rumshisky, A., Zettlemoyer,
    L., Hakkani-Tür, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T.,
    and Zhou, Y., editors, Proceedings of the 2021 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2021, Online, June 6-11, 2021, pages 1301–1312\. Association for Computational
    Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al., [2022] Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence,
    P., Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. (2022). Inner monologue:
    Embodied reasoning through planning with language models. arXiv preprint arXiv:2207.05608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khot et al., [2023] Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson,
    K., Clark, P., and Sabharwal, A. (2023). Decomposed prompting: A modular approach
    for solving complex tasks. International Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling, [2022] Kingma, D. P. and Welling, M. (2022). Auto-encoding
    variational bayes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al., [2022] Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,
    Y. (2022). Large language models are zero-shot reasoners. In Koyejo, S., Mohamed,
    S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural
    Information Processing Systems, volume 35, pages 22199–22213\. Curran Associates,
    Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lazaridou et al., [2022] Lazaridou, A., Gribovskaya, E., Stokowiec, W., and
    Grigorev, N. (2022). Internet-augmented language models through few-shot prompting
    for open-domain question answering. arXiv preprint arXiv:2203.05115.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Liu, J., Liu, A., Lu, X., Welleck, S., West, P., Le Bras, R., Choi, Y.,
    and Hajishirzi, H. (2022a). Generated knowledge prompting for commonsense reasoning.
    In Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), pages 3154–3169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al., [2021] Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen,
    W. (2021). What makes good in-context examples for gpt-$3$?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al., [2023] Liu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., and Neubig,
    G. (2023). Pre-train, prompt, and predict: A systematic survey of prompting methods
    in natural language processing. ACM Computing Surveys, 55(9):1–35.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Liu, R., Wei, J., Gu, S. S., Wu, T.-Y., Vosoughi, S., Cui, C., Zhou, D.,
    and Dai, A. M. (2022b). Mind’s eye: Grounded language model reasoning through
    simulation. arXiv preprint arXiv:2210.05359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al., [2022] Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stenetorp,
    P. (2022). Fantastically ordered prompts and where to find them: Overcoming few-shot
    prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), pages 8086–8098, Dublin,
    Ireland. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Madaan et al., [2023] Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao,
    L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., Welleck, S.,
    Majumder, B. P., Gupta, S., Yazdanbakhsh, A., and Clark, P. (2023). Self-refine:
    Iterative refinement with self-feedback.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mialon et al., [2023] Mialon, G., Dessì, R., Lomeli, M., Nalmpantis, C., Pasunuru,
    R., Raileanu, R., Rozière, B., Schick, T., Dwivedi-Yu, J., Celikyilmaz, A., Grave,
    E., LeCun, Y., and Scialom, T. (2023). Augmented language models: a survey.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Miao, Y. and Blunsom, P. (2016a). Language as a latent variable: Discrete
    generative models for sentence compression. In Proceedings of the 2016 Conference
    on Empirical Methods in Natural Language Processing, pages 319–328, Austin, Texas.
    Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Miao, Y. and Blunsom, P. (2016b). Language as a latent variable: Discrete
    generative models for sentence compression. In Proceedings of the 2016 Conference
    on Empirical Methods in Natural Language Processing, pages 319–328.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Min et al., [2022] Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,
    Hajishirzi, H., and Zettlemoyer, L. (2022). Rethinking the role of demonstrations:
    What makes in-context learning work? In Proceedings of the 2022 Conference on
    Empirical Methods in Natural Language Processing, pages 11048–11064, Abu Dhabi,
    United Arab Emirates. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mukherjee and Awadallah, [2020] Mukherjee, S. and Awadallah, A. H. (2020).
    Xtremedistil: Multi-stage distillation for massive multilingual models. In Jurafsky,
    D., Chai, J., Schluter, N., and Tetreault, J. R., editors, Proceedings of the
    58th Annual Meeting of the Association for Computational Linguistics, ACL 2020,
    Online, July 5-10, 2020, pages 2221–2234\. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nye et al., [2021] Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,
    Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma, M., Luan, D., et al.
    (2021). Show your work: Scratchpads for intermediate computation with language
    models. arXiv preprint arXiv:2112.00114.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al., [2022] Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright,
    C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training
    language models to follow instructions with human feedback. Advances in Neural
    Information Processing Systems, 35:27730–27744.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prasad et al., [2022] Prasad, A., Hase, P., Zhou, X., and Bansal, M. (2022).
    Grips: Gradient-free, edit-based instruction search for prompting large language
    models. arXiv preprint arXiv:2203.07281.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Press et al., [2022] Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,
    and Lewis, M. (2022). Measuring and narrowing the compositionality gap in language
    models. arXiv preprint arXiv:2210.03350.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pryzant et al., [2023] Pryzant, R., Iter, D., Li, J., Lee, Y. T., Zhu, C., and
    Zeng, M. (2023). Automatic prompt optimization with" gradient descent" and beam
    search. arXiv preprint arXiv:2305.03495.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rubin et al., [2022] Rubin, O., Herzig, J., and Berant, J. (2022). Learning
    to retrieve prompts for in-context learning. In Proceedings of the 2022 Conference
    of the North American Chapter of the Association for Computational Linguistics:
    Human Language Technologies, pages 2655–2671.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sanh et al., [2019] Sanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019).
    Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter.
    CoRR, abs/1910.01108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al., [2020] Shin, T., Razeghi, Y., Logan IV, R. L., Wallace, E., and
    Singh, S. (2020). Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. In Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP), pages 4222–4235.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shinn et al., [2023] Shinn, N., Labash, B., and Gopinath, A. (2023). Reflexion:
    an autonomous agent with dynamic memory and self-reflection. arXiv preprint arXiv:2303.11366.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava et al., [2022] Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M.,
    Abid, A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A., Garriga-Alonso, A.,
    et al. (2022). Beyond the imitation game: Quantifying and extrapolating the capabilities
    of language models. arXiv preprint arXiv:2206.04615.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al., [2023] Su, H., Kasai, J., Wu, C. H., Shi, W., Wang, T., Xin, J.,
    Zhang, R., Ostendorf, M., Zettlemoyer, L., Smith, N. A., et al. (2023). Selective
    annotation makes language models better few-shot learners. International Conference
    on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al., [2022] Sun, Z., Wang, X., Tay, Y., Yang, Y., and Zhou, D. (2022).
    Recitation-augmented language models. arXiv preprint arXiv:2210.01296.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suzgun et al., [2022] Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay,
    Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., , and Wei, J.
    (2022). Challenging big-bench tasks and whether chain-of-thought can solve them.
    arXiv preprint arXiv:2210.09261.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tang et al., [2019] Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O., and
    Lin, J. (2019). Distilling task-specific knowledge from BERT into simple neural
    networks. CoRR, abs/1903.12136.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
    T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A.,
    Grave, E., and Lample, G. (2023a). Llama: Open and efficient foundation language
    models. arXiv preprint arXiv:2302.13971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L.,
    Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu,
    W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S.,
    Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S., and Scialom, T. (2023b). Llama 2: Open foundation
    and fine-tuned chat models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and Zhou, D. (2023a).
    Self-consistency improves chain of thought reasoning in language models. International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Wang, X., Zhu, W., Saxon, M., Steyvers, M., and Wang, W. Y. (2023b). Large
    language models are implicitly topic models: Explaining and finding good demonstrations
    for in-context learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al., [2022] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
    Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. (2022). Chain-of-thought prompting
    elicits reasoning in large language models. In NeurIPS.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weidinger et al., [2022] Weidinger, L., Uesato, J., Rauh, M., Griffin, C., Huang,
    P.-S., Mellor, J., Glaese, A., Cheng, M., Balle, B., Kasirzadeh, A., et al. (2022).
    Taxonomy of risks posed by language models. In 2022 ACM Conference on Fairness,
    Accountability, and Transparency, pages 214–229.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al., [2022] Wu, T., Terry, M., and Cai, C. J. (2022). Ai chains: Transparent
    and controllable human-ai interaction by chaining large language model prompts.
    In Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems,
    pages 1–22.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and
    Jiang, D. (2023a). Wizardlm: Empowering large language models to follow complex
    instructions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Xu, W., Banburski-Fahey, A., and Jojic, N. (2023b). Reprompting: Automated
    chain-of-thought prompt inference through gibbs sampling. arXiv preprint arXiv:2305.09993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T. L., Cao, Y., and
    Narasimhan, K. (2023a). Tree of thoughts: Deliberate problem solving with large
    language models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao,
    Y. (2023b). React: Synergizing reasoning and acting in language models. International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al., [2023] Zhang, Z., Zhang, A., Li, M., and Smola, A. (2023). Automatic
    chain of thought prompting in large language models. International Conference
    on Learning Representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Zhou, D., Schärli, N., Hou, L., Wei, J., Scales, N., Wang, X., Schuurmans,
    D., Bousquet, O., Le, Q., and Chi, E. (2023a). Least-to-most prompting enables
    complex reasoning in large language models. International Conference on Learning
    Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Zhou, Y., Muresanu, A. I., Han, Z., Paster, K., Pitis, S., Chan, H., and
    Ba, J. (2023b). Large language models are human-level prompt engineers. International
    Conference on Learning Representations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contents in Appendices:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [A](#A1 "Appendix A Contributions ‣ Joint Prompt Optimization of
    Stacked LLMs using Variational Inference"), we list the contribution of each author
    to this work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [B](#A2 "Appendix B Additional Experimental Details ‣ Joint Prompt
    Optimization of Stacked LLMs using Variational Inference"), we provide additional
    experimental details including task statistics and the prompt strings we used
    to initialize DLN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [C](#A3 "Appendix C Additional Experiments ‣ Joint Prompt Optimization
    of Stacked LLMs using Variational Inference"), we provide additional experiments
    and baselines we compare to.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [D](#A4 "Appendix D Templates ‣ Joint Prompt Optimization of Stacked
    LLMs using Variational Inference"), we provide forward and backward templates
    being used in DLN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [E](#A5 "Appendix E Generalized VI to Multiple Layers ‣ Joint Prompt
    Optimization of Stacked LLMs using Variational Inference"), we provide an algorithm
    that generalizes DLN training in a multiple layer setting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In Appendix [F](#A6 "Appendix F Learning to In-Context Learn: Additional Examples
    ‣ Joint Prompt Optimization of Stacked LLMs using Variational Inference"), we
    show examples of learned weights that exhibit behavior similar to in-context learning.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [G](#A7 "Appendix G Examples of 2-Layer Best Weights (GPT-3) ‣ Joint
    Prompt Optimization of Stacked LLMs using Variational Inference"), we show examples
    of learned weights by 2-Layer DLNs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [H](#A8 "Appendix H Examples of Hidden States ‣ Joint Prompt Optimization
    of Stacked LLMs using Variational Inference"), we show an example of the hidden
    states produced by a 2-Layer DLN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [I](#A9 "Appendix I Implementation Details ‣ Joint Prompt Optimization
    of Stacked LLMs using Variational Inference"), we provide implementation details,
    including hyperparameter information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Appendix [J](#A10 "Appendix J Pricing ‣ Joint Prompt Optimization of Stacked
    LLMs using Variational Inference"), we discuss resource used in DLN development
    and their pricing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Appendix A Contributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Alessandro Sordoni proposed the general idea of DLN, where multiple prompts
    are learnt at each layer through backward natural language operations; they proposed
    to generate synthetic in-context examples and the exploration reward for DLN-2;
    they wrote the code and ran the experiments; they focused on Sections 2, 3, 4
    and contribute writing the rest of the sections.
  prefs: []
  type: TYPE_NORMAL
- en: Xingdi Yuan co-developed the basic idea of DLN and wrote part of the code, they
    also co-designed and helped conducting experiments. They contributed to the writing
    of the paper, mainly Sections 5 and 6.
  prefs: []
  type: TYPE_NORMAL
- en: Marc-Alexandre Côté helped with the experiments and the infrastructure to make
    calls to OpenAI models. They also built a demo to visualize the evolution of DLN’s
    prompts during training and contributed to the writing of the paper, mainly focusing
    on the algorithms and the appendix.
  prefs: []
  type: TYPE_NORMAL
- en: Matheus Pereira co-coded an earlier, non-variational backwards operator with
    AT, helped with the APE and DLN-2 layers experiments, implemented the method for
    estimating the total cost of experiments, build a demo to visualize the evolution
    of DLN’s prompts during training, and contributed to the release of the DLN code.
  prefs: []
  type: TYPE_NORMAL
- en: Adam Trischler helped with template conception and iteration, co-coded an earlier,
    non-variational backwards operator with MP, and contributed to paper writing,
    mainly the literature review.
  prefs: []
  type: TYPE_NORMAL
- en: Ziang Xiao helped with the model evaluation and experiment setup and contributed
    to the paper writing, mainly the literature review and discussion.
  prefs: []
  type: TYPE_NORMAL
- en: Arian Hosseini participated in the development discussions throughout the project
    and contributed to writing the literature review of the paper.
  prefs: []
  type: TYPE_NORMAL
- en: Friederike Niedtner organized and managed the project, helping the team focus
    on the right priorities.
  prefs: []
  type: TYPE_NORMAL
- en: Nicolas Le Roux proposed the variational inference formulation and the posterior
    sharpening. They offered guidance and mentorship for the project. They also contributed
    to the writing of the paper, mainly sections 1, 2, and 3.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Additional Experimental Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: B.1 Additional Task Information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Table [3](#A2.T3 "Table 3 ‣ B.1 Additional Task Information ‣ Appendix B
    Additional Experimental Details ‣ Joint Prompt Optimization of Stacked LLMs using
    Variational Inference"), we provide short descriptions for all tasks we use and
    their statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Tasks used in this work.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | $&#124;\text{train}&#124;$ | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mpqa | 400 | 256 | 250 | 2 | Sentiment analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Trec | 400 | 256 | 250 | 6 | Question type classification. |'
  prefs: []
  type: TYPE_TB
- en: '| Subj | 400 | 256 | 250 | 2 | Determine whether a sentence is subjective or
    objective. |'
  prefs: []
  type: TYPE_TB
- en: '| Disaster | 400 | 250 | 250 | 2 | Determine whether a sentence is relevant
    to a disaster. |'
  prefs: []
  type: TYPE_TB
- en: '| Airline | 400 | 250 | 250 | 3 | Airline tweet sentiment analysis. |'
  prefs: []
  type: TYPE_TB
- en: '| Hyper. | 400 | 1000 | 250 | 2 | Order adjectives correctly in English sentences.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Nav. | 375 | 375 | 250 | 2 | Spatial reasoning given navigation instructions.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Date. | 59 | 60 | 250 | 6 | Infer a date from context. |'
  prefs: []
  type: TYPE_TB
- en: '| Logic.7 | 225 | 225 | 250 | 7 | Deduce the order of seven objects given instruction.
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Prompt initializations.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Initialization |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Mpqa | Read the following review, then choose whether it is negative or positive.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Trec | Read the following question, then choose whether it is about a description,
    entity, expression, human, location or number. |'
  prefs: []
  type: TYPE_TB
- en: '| Subj | Read the following sentence, then choose whether it is subjective
    or objective |'
  prefs: []
  type: TYPE_TB
- en: '| Disaster | Read the following sentence, then choose whether it is relevant
    to a disaster. |'
  prefs: []
  type: TYPE_TB
- en: '| Airline | Read the following sentence, then choose whether it is positive,
    negative, or neutral. |'
  prefs: []
  type: TYPE_TB
- en: '| Hyper. | Which sentence has the correct adjective order. |'
  prefs: []
  type: TYPE_TB
- en: '| Nav. | Read the following sentence, then determine whether you return to
    the starting point. |'
  prefs: []
  type: TYPE_TB
- en: '| Date. | Infer the date from context. |'
  prefs: []
  type: TYPE_TB
- en: '| Logic.7 | The following paragraphs each describe a set of seven objects arranged
    in a fixed order. The statements are logically consistent within each paragraph.
    |'
  prefs: []
  type: TYPE_TB
- en: B.2 Prompt Initialization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We initialize the “classification” layer of the DLNs, i.e. the first layer of
    the 1-Layer LN and the second layer of the 2-layer DLN, with a question or a task
    description as reported in Table [4](#A2.T4 "Table 4 ‣ B.1 Additional Task Information
    ‣ Appendix B Additional Experimental Details ‣ Joint Prompt Optimization of Stacked
    LLMs using Variational Inference"). We use these same initializations to compute
    the 0-shot performance. Therefore, at initialization, a 1-layer LN is equivalent
    to the 0-shot baseline. For the hidden layer of the 2-layer DLN, we initialize
    the prompt to ‘‘Decompose the problem to make it simpler:’’ for Nav. and Subj,
    and ‘‘’’ (empty string) for Date. and Logic.7\. We didn’t try other initializations
    for this hidden layer, we leave this for future explorations.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Additional Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In addition to the prompt engineering and few-shot baselines from the main
    paper, we include here comparisons to more of those on a subset of the datasets.
    Particularly we add results for:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table [5](#A3.T5 "Table 5 ‣ Appendix C Additional Experiments ‣ Joint Prompt
    Optimization of Stacked LLMs using Variational Inference") shows DLN-1 and DLN-2
    outperforming CoT+APE (implemented as described in Section 4.3 of Zhou et al.,
    2023b [[57](#bib.bib57)]);
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table [6](#A3.T6 "Table 6 ‣ Appendix C Additional Experiments ‣ Joint Prompt
    Optimization of Stacked LLMs using Variational Inference") shows that even increasing
    the number of examples for ICL and KATE cannot match DLN-1 and DLN-2 performance;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Table [7](#A3.T7 "Table 7 ‣ Appendix C Additional Experiments ‣ Joint Prompt
    Optimization of Stacked LLMs using Variational Inference") and Table [8](#A3.T8
    "Table 8 ‣ Appendix C Additional Experiments ‣ Joint Prompt Optimization of Stacked
    LLMs using Variational Inference") show results for DLN-1 using open-source language
    models such as WizardLM-13b-v1.2 [[51](#bib.bib51)] and LLaMA2-70B-Chat [[45](#bib.bib45)]
    respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 5: Test accuracy averaged over three random seeds with 95% confidence
    interval. All models use GPT-3\. DLN-1 and DLN-2 outperform CoT+APE.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Hyper. | Nav. | Date. | Logic.7 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| APE-15 | 68.5$\pm$ 4.7 |'
  prefs: []
  type: TYPE_TB
- en: '| CoT+APE | 50.9$\pm$ 1.6 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 | 91.9$\pm$ 2.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-2 | - | 83.1$\pm$ 3.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Test accuracy averaged over three random seeds with 95% confidence
    interval (where applicable). All models use GPT-3\. Increasing the number of ICL
    examples helps performance, but cannot match DLN-1 and DLN-2 in general. Context
    length limit is an issue for ICL and KATE 32-shot.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Nav. | Date. | Logic.7 | Subj |'
  prefs: []
  type: TYPE_TB
- en: '| ICL - 5-shot | 56.5 | 62.1 | 36.7 | 76.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ICL - 10-shot | 61.3 | 62.9 | 38.9 | 72.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ICL - 32-shot | 66.0 | 63.5 | - | 83.2 |'
  prefs: []
  type: TYPE_TB
- en: '| KATE - 5-shot | 56.9 | 61.1 | 44.4 | 71.1 |'
  prefs: []
  type: TYPE_TB
- en: '| KATE - 10-shot | 59.5 | 62.0 | 41.6 | 73.9 |'
  prefs: []
  type: TYPE_TB
- en: '| KATE - 32-shot | 67.5 | 62.8 | - | 80.4 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 | 68.5$\pm$5.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-2 | 83.1$\pm$8.7 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Test accuracy using WizardLM-v1.2 13B as LLM. This open source model
    seems significantly less able to capture few-shot examples from the context. DLN-1
    outperforms ICL on all tasks here.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Nav. | Logic.7 | Subj |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0-shot | 58.0 | 0.0 | 65.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 56.0 | 28.0 | 50.8 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 | 61.1 | 31.0 | 79.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Test accuracy averaged over three random seeds with 95% confidence
    interval. All methods use LLaMA2-70B-Chat as LLM, with DLN + GPT3 employing text-davinci-003
    as the backward LLM for prompt and hidden proposals.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Nav. | Date. | Logic.7 | Subj |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 0-shot | 42.0$\pm$0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 43.2$\pm$15.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 + GPT3 | 43.6$\pm$11.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 | 44.9$\pm$4.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-2 + GPT3 | 43.7$\pm$16.7 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-2 | 68.9$\pm$25.4 |'
  prefs: []
  type: TYPE_TB
- en: C.1 Layerwise training of DLN-2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We also explored a different learning strategy for DLN-2: layer-wise pre-training.
    We start the learning of a single layer DLN using the technique from Section [2](#S2
    "2 One-Layer Language Networks ‣ Joint Prompt Optimization of Stacked LLMs using
    Variational Inference"). We call the prompt obtained at the end of this optimization
    $\pi^{*}$, the parameters of the bottom layer, using variational inference. We
    explore two variants: one keeps the last layer fixed and one it fine-tunes the
    last layer. We report their results in Table [9](#A3.T9 "Table 9 ‣ C.1 Layerwise
    training of DLN-2 ‣ Appendix C Additional Experiments ‣ Joint Prompt Optimization
    of Stacked LLMs using Variational Inference").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Test accuracy averaged over three random seeds. We compare the layerwise
    and the end-to-end trainings for DLN-2.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Nav. | Date. | Logic.7 | Subj |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-2  *(fix 2nd)* | 73.1 | 61.6 | 43.3 | 80.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-2 (*fine-tune 2nd*) | 76.4 | 62.8 | 40.7 | 84.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-2 (*end-to-end*) | 83.1 | 75.2 | 45.7 | 85.9 |'
  prefs: []
  type: TYPE_TB
- en: Appendix D Templates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: D.1 Forward Templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: “Classification” Template F for 1-Layer LN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the 1-layer LN, we use the following template to elicit the output $y$. prompt
    is substituted with the value of the current prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Classification
    Template F 
    template: {{  prompt  }} {{  input  }} Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Residual Classification Template $\texttt{F}_{r}$ for 2-Layer DLN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the 2-layer LN, the last layer just concatenates the input to the output
    of the first layer, $\hat{h}$, before eliciting an answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Residual
    classification template $\texttt{F}_{r}$ 
    template: {{  prompt  }} {{  input  }} Your  thoughts  were: {{  h  }} Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: Hidden Layer F
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The variable prompt is substituted with the value of the current prompt $\pi_{0}$.
    This has the effect of providing additional information about how “Let’s think
    step by step” should behave.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden
    Layer F 
    template: {{  input  }} {{  prompt  }}  Let''s  think  step  by  step.'
  prefs: []
  type: TYPE_NORMAL
- en: For 2-Layer DLN on Subj, we use the following hidden template. We couldn’t run
    with the previous template due to lack of time, as we observed that the step by
    step trigger tended to generate lengthy hidden states.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hidden
    Layer F 
    template: {{  prompt  }} {{  input  }} Brief  Analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Backward Templates (Prompt and Hidden Proposals)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Prompt Proposal Template $\texttt{B}_{\pi}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: $\texttt{B}_{\pi}$ if it is the second layer). message is substituted with one
    of the message_alternatives, sampled at random during the DLN training. This induces
    diversity in the generated prompts and allows emergence of learning to ICL behaviors,
    where the prompts contain synthetic examples that can help solve the task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt
    Proposal Template $\texttt{B}_{\pi}$ 
    template: A  student  is  completing  a  task  that  requires  producing  a  text  output  from  a  text  input.  The  student  receives  an  instruction  that  describes  how  to  produce  the  output  given  each  input.
    The  student  has  made  some  errors.  Your  task  is  to  improve  the  instruction  such  that  the  student  can  fix  the  errors.
    This  was  the  instruction. ##  Instruction >  {{  prompt  }} [END] #  Student  successes
    {%  for  backward_info  in  backward_infos  %}  {%  if  backward_info.loss  ==  0.0  %}
    ##  Input: >  {{  backward_info.input  }} ##  Correct  Output: >  {{  backward_info.target  }}
    {%  endif  %}  {%  endfor  %} #  Student  errors {%  for  backward_info  in  backward_infos  %}  {%  if  backward_info.loss  >  0.0  %}
    ##  Input: >  {{  backward_info.input  }} ##  Student  Output: >  {{  backward_info.output  }}
    ##  Correct  Output: >  {{  backward_info.target  }} {%  endif  %}  {%  endfor  %}
    Improve  the  instruction  to  fix  the  student  errors.  {{  message  }} ##  Instruction
    > message_alternatives: -  Clarify  the  instruction  by  adding  few  words  or  a  short  sentence.  Be  concise.
    -  Improve  the  instruction  by  providing  examples  on  how  to  solve  the  task.  Be  concise.
    -  Shorten  the  instruction  by  removing  superflous  words  or  sentences.
    -  Rewrite  the  instruction  by  providing  detailed  information  to  avoid  ambiguity.  Be  concise.'
  prefs: []
  type: TYPE_NORMAL
- en: Backward Hidden Templates $\texttt{B}_{h}$
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We experiment with multiple backward templates to sample hidden states from
    the approximate posterior distribution $q(h)$, as illustrated below:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Backward
    Hidden Template $\texttt{B}_{y}$ conditioning) 
    template: {{  input  }} Given  that  the  answer  is: {{  y  }} {{  prompt  }}  Let''s  think  step  by  step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next alternative we experiment with is a more verbose template that takes
    as input the prompt for the final layer $\pi_{1}$. We use a similar strategy of
    sampling different message alternatives to substitute with message to increase
    diversity of the hidden samples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Backward
    Hidden Template $\texttt{B}_{h}$ 
    template: This  is  the  context  needed  to  solve  the  problem: {{  next_prompt  }}
    This  is  the  problem: {{  input  }} These  were  your  thoughts: {{  h  }} Given  that  this  is  the  answer:
    {{  y  }} {{  message  }} Thoughts: message_alternatives: -  Reflect and refine
    your thoughts for this problem by adding detailed explanations. -  Fix the errors
    in your reasoning. Add examples to illustrate your thoughts. Be concise.'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we found that sampling from hidden states from a mixture of forward
    template F, i.e. $p_{\texttt{LM}}(h|x,\pi_{0})$ appearing in the ELBO. In the
    future, this could be addressed in a more principled way by learning a prompt
    for the posterior proposal.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Generalized VI to Multiple Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We report the generalized training algorithm for multiple layers in Algorithm [3](#alg3
    "Algorithm 3 ‣ Appendix E Generalized VI to Multiple Layers ‣ Joint Prompt Optimization
    of Stacked LLMs using Variational Inference").
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 Deep Language Network Training Algorithm
  prefs: []
  type: TYPE_NORMAL
- en: 1:$\hat{h}\sim p_{\texttt{LM}}^{t}(x)$ do8:     $x,y\sim\mathcal{D}$ samples15:         $\beta^{1}_{l},\ldots,\beta^{K}_{l}\leftarrow\log
    p_{\texttt{LM}}(h^{*}_{l+1}|\texttt{F}(h^{k}_{l},\pi_{l}))$ Compute ELBO for all
    prompts $\pi_{l}^{n}$ with best score23:     end for24:end for
  prefs: []
  type: TYPE_NORMAL
- en: 'Appendix F Learning to In-Context Learn: Additional Examples'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In Figure [2](#S5.F2 "Figure 2 ‣ 5.2 DLN-1 ‣ 5 Experiments and Results ‣ Joint
    Prompt Optimization of Stacked LLMs using Variational Inference"), we report examples
    of prompts found by the 1-layer DLN on the Hyperbaton task, which exhibit either
    integration or verbalization of task examples. Here, we provide additional examples
    of prompts found by DLN-1 on the MPQA task.
  prefs: []
  type: TYPE_NORMAL
- en: DLN-1
    prompt on MPQA (GPT-3) Read each sentence, then decide
    if the sentence is expressing a positive or negative sentiment. For example, if
    the sentence is "supported", choose "positive", and if the sentence is "derail",
    choose "negative". Similarly, if the sentence is "victorious", choose "positive",
    and if the sentence is "would not be a bad idea", choose "positive". Additionally,
    if the sentence contains multiple words, consider the overall sentiment of the
    sentence and choose the appropriate option. For example, if the sentence is "counting
    on", choose "negative", and if the sentence is "peace and stability and prosperity",
    choose "positive". Note that words like "artificial" tend to have a negative sentiment.DLN-1 prompt on MPQA (GPT-4)
    Determine whether the given input has a positive or negative connotation
    by analyzing the meaning of the words and phrases in context. If the input expresses
    a favorable, desirable, or pleasant meaning, choose "positive." If the input expresses
    an unfavorable, undesirable, or unpleasant meaning, choose "negative." Consider
    the overall sentiment expressed by the input rather than focusing on individual
    words or phrases. For example, "calling for" generally has a positive connotation
    as it implies advocating or supporting something, while "a true Muslim fighter"
    can be seen as positive, since it refers to someone dedicated to their beliefs.
    Keep in mind that some phrases may have a positive connotation when they imply
    improvement or resolution, like "put an end to." Additionally, phrases like "to
    the contrary" can have a positive connotation when they suggest a differing, yet
    valid perspective or opinion. When analyzing the input, consider the context in
    which it is used, as the connotation of a word or phrase can change depending
    on the situation. For example, "extra vigil" can have a positive connotation when
    it implies increased awareness and preparedness.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Examples of 2-Layer Best Weights (GPT-3)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: G.1 Navigate (81.6% Dev Acc)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DLN-2 Prompt: $\pi_{0}$
    Start facing north. Take the specified number of steps in the
    indicated direction and turn when specified. Make sure to keep track of your direction
    and the number of steps taken to ensure you return to the starting point.'
  prefs: []
  type: TYPE_NORMAL
- en: G.2 Subj (89.9% Dev Acc)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'DLN-2
    Prompt: $\pi_{0}$ Read the sentence. Determine
    if it is expressing a fact or opinion. A fact is an accurate statement that can
    be confirmed, while an opinion is a personal viewpoint that reflects someone’s
    beliefs. Facts are typically statements about something that happened, such as
    events, actions, or conditions, or statements that describe a state of being,
    such as someone’s personality or a physical object. Opinions are typically statements
    that express judgement, approval, or disapproval. Examples of facts include statements
    about events that occurred, such as "the film was released in 2012," or statements
    about conditions, such as "the weather is sunny," or statements that describe
    a state of being, such as "the detective is strong and independent." Examples
    of opinions include statements about how good or bad something is, such as "the
    film was terrible" or "the weather is beautiful." Examples of questions that are
    not facts or opinions include hypothetical questions, such as "how would you feel
    if when you woke, the nightmare had just begun?". Select "objective" for a fact
    and "subjective" for an opinion.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Examples of Hidden States
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We report the forward pass produced by a 2-Layer DLN in Section [G.1](#A7.SS1
    "G.1 Navigate (81.6% Dev Acc) ‣ Appendix G Examples of 2-Layer Best Weights (GPT-3)
    ‣ Joint Prompt Optimization of Stacked LLMs using Variational Inference") on Navigate
    below.
  prefs: []
  type: TYPE_NORMAL
- en: Input $x$
    If you follow these instructions, do you return to the starting
    point? Take 3 steps. Take 10 steps. Take 4 steps. Take 1 step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Options:'
  prefs: []
  type: TYPE_NORMAL
- en: '- Yes'
  prefs: []
  type: TYPE_NORMAL
- en: '- NoHidden $\hat{h}$ 1\. Take 3 steps: (3, 0) facing
    east'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Take 10 steps: (13, 0) facing east'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Take 4 steps: (17, 0) facing east'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Take 1 step: (18, 0) facing east'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: No, you do not return to the starting point.Output $\hat{y}$
    No'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix I Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We report hyperparameter search space in Table [10](#A9.T10 "Table 10 ‣ Appendix
    I Implementation Details ‣ Joint Prompt Optimization of Stacked LLMs using Variational
    Inference"). A brief description of the hyperparameters is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: bh_tpl is the type of backward prompt template we use $\texttt{B}_{\pi}$ we
    report in Section [D](#A4 "Appendix D Templates ‣ Joint Prompt Optimization of
    Stacked LLMs using Variational Inference"). In v3.0, we remove “Be concise.” at
    the end of each message_alternatives. We noticed that in general v3.5 works better
    as it implements a sort of regularization on the length of the found prompts.
    Future work could address length regularization in a more principled manner.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: logp_penalty is the coefficient for the exploration reward we mentioned in the
    paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: num_h_samples is the number of $h$ samples to generate from the approximate
    posterior distribution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: use_memory is whether or not we use the backtracking mechanism. Usually 2 works
    well across tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: held_out_prompt_ranking describes whether we use only half of the mini-batch
    examples for each prompt proposal, as described in the main paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: tolerance describes after how many iterations we reload the best weights found
    during the last validation if the current validation score is lower than the best
    score obtained so far.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For the 2-Layer experiments, we have to restrict this search space due to computational
    costs. We use bh_tpl = "v3.5", tolerance = 2, use_memory = 2, held_out_prompt_ranking
    = True, logp_penalty = 0.5.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: Hyperparameter search space.'
  prefs: []
  type: TYPE_NORMAL
- en: '| hyperparam | search space |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1-Layer LN |'
  prefs: []
  type: TYPE_TB
- en: '| bh_tpl | q_action_prompt:v3.0, q_action_prompt:v3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| tolerance | -1, 0, 2 |'
  prefs: []
  type: TYPE_TB
- en: '| use_memory | 0, 2 |'
  prefs: []
  type: TYPE_TB
- en: '| held_out_prompt_ranking | True, False |'
  prefs: []
  type: TYPE_TB
- en: '| 2-Layer DLN PT + fix 2nd layer |'
  prefs: []
  type: TYPE_TB
- en: '| bh_tpl | q_action_prompt:v3.0, q_action_prompt:v3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| logp_penalty | 0., 0.5, 2. |'
  prefs: []
  type: TYPE_TB
- en: '| 2-Layer DLN PT + fine-tune 2nd layer |'
  prefs: []
  type: TYPE_TB
- en: '| bh_tpl | q_action_prompt:v3.0, q_action_prompt:v3.5 |'
  prefs: []
  type: TYPE_TB
- en: '| logp_penalty | 0., 0.5, 2. |'
  prefs: []
  type: TYPE_TB
- en: '| 2-Layer DLN end-to-end |'
  prefs: []
  type: TYPE_TB
- en: '| num_h_samples | 5, 10 |'
  prefs: []
  type: TYPE_TB
- en: Appendix J Pricing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We keep track the number of tokens we interact with GPT-3 via its online API.
    According to OpenAI’s pricing policy, user pays for both the input tokens (prompts)
    and the output tokens. Using the Hyperbaton task as an example, while training
    a 1-layer LN, the total number of tokens we use is 2,941,360. For a 2-layer DLN,
    the total number of tokens we use is 13,654,962. According to the current price
    for GPT-3 ($0.02/1k tokens), a single run of a 1-layer and 2-layer DLN cost roughly
    59 USD and 273 USD, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In Table [11](#A10.T11 "Table 11 ‣ Appendix J Pricing ‣ Joint Prompt Optimization
    of Stacked LLMs using Variational Inference"), we report the cost (lower is better)
    in terms of total number of tokens for the test set (prompts included). We emphasize
    the cost at the testing time because it is more relevant in real-world deployment
    and the training cost is one-off. We can see DLN-1 improves over ICL on 5 out
    of 9 tasks on GPT-4 at a comparable token cost. Some tasks do not benefit from
    ICL (i.e. reasoning tasks) while other tasks like Subj, Trec, and Hyper. benefit
    significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 11: Test accuracy along with inference cost expressed in tokens (in gray)
    averaged over three random seeds of a shallow, 1-layer language network (DLN-1)
    compared to baselines on GPT-4\. We also report the 95% confidence interval on
    the test accuracy. We emphasize the cost at the testing time because it is more
    relevant in real-world deployment and the training cost is one-off.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BigBench Hard | NLU | Leopard |'
  prefs: []
  type: TYPE_TB
- en: '| Method | Hyper. | Nav. | Date. | Logic.7 | Mpqa | Trec | Subj | Disaster
    | Airline |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0-shot | 64.0$\pm$0.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (7.6k) | (12.9k) | (23.6k) | (46.2k) | (3.7k) | (7.6k) | (10.2k) | (10.1k)
    | (9.9k) |'
  prefs: []
  type: TYPE_TB
- en: '| 5-shot | 88.4$\pm$1.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (48.0k) | (79.2k) | (143.3k) | (287.5k) | (24.5k) | (52.5k) | (62.6k)
    | (63.5k) | (61.7k) |'
  prefs: []
  type: TYPE_TB
- en: '| 16-shot | 93.3$\pm$2.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (136.8k) | (229.9k) | (405.1k) | (817.6k) | (70.3k) | (149.0k) | (177.9k)
    | (179.4k) | (175.2k) |'
  prefs: []
  type: TYPE_TB
- en: '| DLN-1 | 95.2$\pm$1.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | (77.2k) | (29.9k) | (52.3k) | (68.5k) | (65.4k) | (120.7k) | (46.5k) |
    (47.1k) | (38.2k) |'
  prefs: []
  type: TYPE_TB
