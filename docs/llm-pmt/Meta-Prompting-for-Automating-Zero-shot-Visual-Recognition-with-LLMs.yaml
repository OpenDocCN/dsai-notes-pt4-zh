- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:45:36'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.11755](https://ar5iv.labs.arxiv.org/html/2403.11755)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '(eccv) Package eccv Warning: Package ‘hyperref’ is loaded with option ‘pagebackref’,
    which is *not* recommended for camera-ready version'
  prefs: []
  type: TYPE_NORMAL
- en: M. Jehanzeb Mirza Leonid Karlinsky Wei Lin
  prefs: []
  type: TYPE_NORMAL
- en: Sivan Doveh Jakub Micorek Mateusz Kozinski
  prefs: []
  type: TYPE_NORMAL
- en: Hilde Kuhene Horst Possegger ¹ICG, TU Graz, Austria.  ²CDL-EML.  ³MIT-IBM Watson
    AI Lab, USA.  ⁴JKU, Austria.  ⁵IBM Research, Israel.  ⁶Weizmann Institute of Science,
    Israel.  ⁷University of Bonn, Germany.1122334455661111337711
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Prompt ensembling of Large Language Model (LLM) generated category-specific
    prompts has emerged as an effective method to enhance zero-shot recognition ability
    of Vision-Language Models (VLMs). To obtain these category-specific prompts, the
    present methods rely on hand-crafting the prompts to the LLMs for generating VLM
    prompts for the downstream tasks. However, this requires manually composing these
    task-specific prompts and still, they might not cover the diverse set of visual
    concepts and task-specific styles associated with the categories of interest.
    To effectively take humans out of the loop and completely automate the prompt
    generation process for zero-shot recognition, we propose Meta-Prompting for Visual
    Recognition (MPVR). Taking as input only minimal information about the target
    task, in the form of its short natural language description, and a list of associated
    class labels, MPVR automatically produces a diverse set of category-specific prompts
    resulting in a strong zero-shot classifier. MPVR generalizes effectively across
    various popular zero-shot image recognition benchmarks belonging to widely different
    domains when tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot
    recognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on average
    over $20$ datasets) leveraging GPT and Mixtral LLMs, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Page: [https://jmiemirza.github.io/Meta-Prompting/](https://jmiemirza.github.io/Meta-Prompting/)'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dual encoder Vision-Language Models (VLMs) [[35](#bib.bib35), [47](#bib.bib47)]
    attain unprecedented performance in zero-shot image classification. They comprise
    a text encoder and an image encoder trained to map text and images to a shared
    embedding space. Zero-shot classification with dual encoder VLMs consists in evaluating
    the cosine similarity between the embedding of a test image and the embeddings
    of texts representing candidate classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The composition of the class-representing text has a significant impact on
    the accuracy of zero-shot classification. Already the authors of CLIP [[35](#bib.bib35)],
    the first large-scale vision-language model, highlighted its importance and reported
    that embedding class names in a *prompt* of the form ‘A photo of a ’
    resulted in considerable performance growth over using raw class names. Moreover,
    specializing the prompt to the data set by adding high-level concepts, for example,
    embedding the class name in the sentence ‘A photo of a , a type of
    flower’ for fine-grained flower recognition, brought further improvement. Finally,
    a substantial performance boost was achieved by ensembling multiple different
    prompts, tailored towards the downstream task (dataset). Since ensembling a larger
    number of dataset- and class-specific prompts is beneficial, and manually designing
    a large number of class-specific prompts is prohibitively time-consuming, several
    authors delegated prompt generation to a Large Language Model (LLM) [[34](#bib.bib34),
    [28](#bib.bib28), [37](#bib.bib37)]. These approaches consist in asking an LLM
    to generate class descriptions [[34](#bib.bib34)], or class attributes [[28](#bib.bib28)],
    and mix them with manually defined prompt templates [[37](#bib.bib37)]. They enable
    generating large sets of prompts adapted to the downstream task, which would be
    prohibitively time-consuming when performed manually. However, they still require
    hand-crafting prompts to the LLM [[34](#bib.bib34)] or dataset-specific LLM prompt
    templates [[37](#bib.bib37)], or rely on the assumption that class attributes
    are discriminative [[28](#bib.bib28), [37](#bib.bib37)]. In other words, they
    do not eliminate the manual effort completely, but shift some of it from manually
    designing prompts for the VLMs (as in [[35](#bib.bib35)]) to manually designing
    LLM prompts. This raises the following question: Does the manual design of the
    LLM prompts bias the resulting VLM prompts, possibly affecting performance? In
    this work, we answer this question affirmatively: we minimize manual interventions
    in the prompt generation process and show that this significantly boosts zero-shot
    recognition accuracy. ¹¹1To avoid confusion between the ‘prompts’ used to query
    the LLMs and the ‘prompts’ used to compute the text embedding by the VLMs, in
    the remaining part of this manuscript, we call the first one ‘LLM query’ and the
    second one ‘VLM prompt’.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cac78d4683a5cc495b867a61a28f0574.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Our MPVR utilizes a Meta Prompt, comprising a system prompt (instruction),
    in-context example demonstrations (fixed throughout), and metadata (name and description)
    for a downstream task of interest. The Meta Prompt instructs an LLM to generate
    diverse task-specific LLM queries, which are used to obtain category-specific
    VLM prompts (visual text descriptions) by again querying the LLM after specifying
    the . These category-specific VLM prompts are then ensembled into
    a zero-shot classifier for recognizing the downstream task categories.'
  prefs: []
  type: TYPE_NORMAL
- en: The gist of our approach lies in automating the prompt generation process. To
    that end, we draw inspiration from methods for reducing the prompt engineering
    effort in natural language processing [[13](#bib.bib13), [40](#bib.bib40)] and
    propose to meta-prompt the LLM to produce LLM query templates tailored to the
    downstream task. We call our method Meta-Prompting for Visual Recognition (MPVR).
    Its overview is presented in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Meta-Prompting
    for Automating Zero-shot Visual Recognition with LLMs"). MPVR comprises a ‘system
    prompt’ that describes the meta-prompting task for the LLM, a description of the
    downstream task, and an in-context example. The in-context example contains a
    description (metadata) of another task and its corresponding ‘LLM queries’, and
    serves to bootstrap the LLM with examples of expected results. They are kept the
    same across different downstream tasks and for all our experiments. MPVR extracts
    the LLM’s knowledge of the visual world gradually, in two steps. The first query
    to the LLM contains the system prompt, in-context example, and the downstream
    task (dataset) description, and produces a diverse set of LLM *query templates*,
    containing a  placeholder. These templates are infused (by the LLM)
    with information on visual styles specific to the downstream task of interest,
    but they are still category-agnostic. In the second step, for each class, we populate
    its label into all the task-specific LLM query templates generated in the first
    step and use them to query the LLM to generate (category-specific) VLM prompts
    describing the category in visually diverse ways and also containing task-specific
    visual styles infused by the LLM in the first step. We use the resulting VLM prompts
    to create an ensemble of zero-shot classifiers. In section [4](#S4 "4 Experimental
    Evaluation ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs"),
    we show that MPVR’s two-step process results in state-of-the-art zero-shot classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our meta-prompting strategy does not take any parameters specific to the dataset,
    other than the dataset description, which can be easily obtained through public
    APIs or from its webpage. Yet, we show that prompts generated by MPVR cover diverse
    visual concepts and styles specific to the downstream task. As a result, MPVR
    yields significant performance gains on a range of zero-shot benchmarks. Our contributions
    can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We propose MPVR: a general, automated framework requiring minimal human involvement
    for tapping into the visual world knowledge of LLMs through meta-prompting for
    zero-shot classification.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MPVR generalizes beyond closed models (like GPT [[2](#bib.bib2)]). We are the
    first to show that category-specific descriptions generated from open-source models
    (like Mixtral [[16](#bib.bib16)]) can also enhance the zero-shot recognition abilities
    of state-of-the-art VLMs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We open-source a dataset of $\sim 2.5\text{M}$ unique class descriptions harnessed
    from GPT and Mixtral with our meta-prompting framework. This is the first large-scale
    dataset encompassing the breadth of LLM knowledge of the visual world.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We first provide an overview of the zero-shot vision-language foundation models,
    then touch upon approaches that propose to improve these models by requiring visual
    data (relying on additional training), and later discuss different methods that
    follow our line of work, *i.e.,* improving zero-shot models in a training-free
    manner by generating textual data through LLMs and finally provide an overview
    of the prompt engineering literature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Large Scale Vision-Language Foundation Models:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: VLMs have shown impressive performance for many vision-language understanding
    tasks, *e.g.,* zero-shot recognition, visual question-answering (VQA), image captioning, *etc*.
    The present-day VLMs can be placed in two distinct groups in a broader categorization.
    One group of methods relies on dual-encoders (vision and text encoder) and usually
    trains the encoders with a contrastive objective by using a large corpus of paired
    image-text data scraped from the web. The most common among these methods are
    CLIP [[35](#bib.bib35)], ALIGN [[15](#bib.bib15)], OpenCLIP [[38](#bib.bib38)],
    and the very recent MetaCLIP [[47](#bib.bib47)]. The zero-shot classification
    is performed by measuring the similarity between the image embeddings and encoded
    text features, usually obtained by using the default template ‘a photo of a ’. The other group of methods aligns the visual modality with a frozen LLM.
    BLIP-2 [[22](#bib.bib22)] bridges the modality gap between a pre-trained visual
    encoder and an LLM by using a querying transformer. Instruct-BLIP [[8](#bib.bib8)]
    proposes to improve [[22](#bib.bib22)] by employing instruction tuning. MiniGPT [[53](#bib.bib53)]
    aligns a vision encoder with a frozen LLM (Vicuna [[6](#bib.bib6)]) by only using
    a (trainable) linear projection layer between the two. MiniGPT-V2 [[3](#bib.bib3)]
    replaces the LLM with Llama-2 [[41](#bib.bib41)] and also proposes to unfreeze
    it during the training/finetuning phases. Llava [[26](#bib.bib26)] also aligns
    an LLM with a pre-trained visual encoder and also proposes Visual Instruction
    Tuning, by carefully curating instruction-response pairs, to enhance the performance.
    Furthermore, the performance of LLaVA is also enhanced with better data curation [[24](#bib.bib24)]
    and slight architectural changes [[25](#bib.bib25)]. In our work, we focus on
    the contrastively pre-trained zero-shot models widely used for object recognition
    (*e.g.,* CLIP [[35](#bib.bib35)]), and improve the recognition abilities of these
    models by generating the text embeddings from a variety of descriptions (instead
    of the default templates) harnessed through our proposed meta-prompting technique.
    Furthermore, we show that MPVR-enhanced CLIP [[35](#bib.bib35)] outperforms even
    the leading LLM-decoder-based methods (*e.g.,* [[25](#bib.bib25)]) in visual recognition
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training-based Approaches for Improving VLMs:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different approaches propose to improve the zero-shot recognition performance
    of the contrastively pre-trained models through parameter-efficient fine-tuning.
    CoOp [[52](#bib.bib52)] proposed to learn randomly initialized text prompts in
    a few-shot manner. CoCoOp [[51](#bib.bib51)] further conditions the learnable
    text prompts on the visual inputs to enhance the performance. Maple [[18](#bib.bib18)]
    proposes to learn both the visual and text prompts in conjunction. Contrary to
    relying on few-shot labeled visual samples, UPL [[14](#bib.bib14)] proposes to
    learn the text prompts on unlabeled image data and LaFTer [[31](#bib.bib31)] learns
    visual prompts by leveraging the cross-modal transfer capabilities of CLIP. While
    these approaches propose to adapt the VLM on image data, MAXI [[23](#bib.bib23)]
    proposes to fine-tune CLIP in an unsupervised manner for video inputs. In contrast
    to the methods proposed to improve the zero-shot recognition abilities of CLIP,
    our work does not rely on visual inputs and gradient-based updates of network
    parameters. Instead, it improves the zero-shot recognition performance by harnessing
    fine-grained textual concepts generated through our MPVR, thus supporting the
    capability to scale zero-shot recognition performance improvements to visual domains
    where *no visual data* might be available for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zero-shot Recognition with Additional Textual Data from LLMs:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: It was initially highlighted in CLIP [[35](#bib.bib35)] that generating the
    text embeddings through an ensemble of (dataset specific) hand-crafted prompts²²2[https://github.com/openai/CLIP/blob/main/data/prompts.md](https://github.com/openai/CLIP/blob/main/data/prompts.md)
    improved the zero-shot recognition performance on the downstream datasets, hinting
    towards the sensitivity of CLIP’s text encoder towards fine-grained textual concepts.
    Following up on this idea, DCLIP [[28](#bib.bib28)] enhances visual recognition
    by generating category-specific descriptors through an LLM (GPT-3 [[2](#bib.bib2)]).
    On the other hand, CUPL [[34](#bib.bib34)] proposes to obtain the category-level
    text embeddings from the prompts generated with the dataset-specific hand-crafted
    queries fed to the LLM. Waffle [[37](#bib.bib37)] hints towards the potential *bag-of-words*
    behavior of the CLIP text encoder and performs zero-shot classification by adding
    random descriptors to broad concepts and DCLIP-generated attributes. Our work
    also takes inspiration from the prompt ensembling in [[35](#bib.bib35), [34](#bib.bib34),
    [28](#bib.bib28), [37](#bib.bib37)] and performs zero-shot classification by generating
    category-level prompts through an LLM. However, contrary to these approaches, MPVR
    proposes a more general prompting framework to alleviate the human effort spent
    for handcrafting the LLM queries (CUPL [[34](#bib.bib34)]), dataset-specific concepts
    (Waffle [[37](#bib.bib37)]), or reduce reliance on individually recognizable visual
    attributes (DCLIP [[28](#bib.bib28)]). By effectively incorporating general downstream
    task information (description) into the first phase of MPVR (*i.e.,* meta-prompting),
    we automatically produce task-tailored LLM query templates ready to be populated
    by task categories and used to query an LLM for a diverse spectrum of category-level
    VLM prompts comprising an enhanced set of visual details for recognizing those
    categories. The performance gains by using MPVR with both closed and open-source
    LLMs (GPT [[2](#bib.bib2)] and Mixtral [[16](#bib.bib16)]) on $20$ different datasets
    when compared to relevant baselines highlight the generalization capabilities
    and benefits of our approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt Engineering:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Manually manipulating the text inputs (prompts) to the LLMs for enhancing performance
    for various natural language processing (NLP) tasks has been an active field of
    research, which is formalized as prompt engineering. In this context, providing
    demonstrations to the LLM for solving related downstream tasks has been referred
    to in the NLP literature as in-context learning (ICL) [[29](#bib.bib29), [44](#bib.bib44),
    [4](#bib.bib4), [49](#bib.bib49)]. Orthogonal to the idea of in-context learning,
    some approaches rely on breaking down a complex task into a series of events.
    To this end, Chain-of-Thought (CoT) [[45](#bib.bib45)] achieved impressive performance
    gains by prompting the model to perform intermediate reasoning steps. Other approaches
    following this line of work include [[19](#bib.bib19), [48](#bib.bib48)]. Our
    MPVR also employs ICL and manipulates the input prompts to the LLMs, but effectively
    alleviates the need for human involvement for this manipulation by probing an
    LLM for more diverse concepts (LLM query templates – incorporating general information
    about the task), which are then populated with specific task categories and fed
    again to the LLM for generating VLM prompts - both task- and category-specific
    text descriptions of visual concepts. To the best of our knowledge, such a two-stage
    (meta-) prompting strategy for tapping into the visual world knowledge of LLMs
    does not exist in literature.
  prefs: []
  type: TYPE_NORMAL
- en: '3 MPVR: Meta-Prompting for Visual Recognition'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Zero-shot classification with a dual encoder VLM consists in projecting a test
    image and each candidate class to the common embedding space, and evaluating the
    cosine similarity between the embeddings. The image embedding is produced by the
    VLM’s vision encoder $\phi$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: and $\tau$ denotes the temperature constant. This approach forms the point of
    departure for our method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensembling a larger number of class-specific VLM prompts improves the performance
    of the zero-shot classifier, but generating these prompts manually would be prohibitively
    time-consuming. Several methods [[34](#bib.bib34), [28](#bib.bib28), [37](#bib.bib37),
    [30](#bib.bib30)] address this problem by generating the VLM prompts with a large
    language model (LLM), for example GPT [[2](#bib.bib2)]. They enhance the performance
    of the zero-shot classifiers, but still require manual construction of the LLM
    queries, which scales poorly: A prohibitively large human effort might be needed
    to creatively design prompts that cover the diverse ways the visual aspects of
    a certain class can be described in text. Moreover, manually specified queries
    can be influenced by the subjective bias of the person who composes them, which
    could affect zero-shot recognition performance.'
  prefs: []
  type: TYPE_NORMAL
- en: To improve the scaling of VLM prompt generation and eliminate subjectivity from
    the process, we design Meta Prompting for Visual Recognition (MPVR), an approach
    to VLM prompt generation that reduces human input to the necessary minimum. MPVR
    taps into the visual world knowledge possessed by the VLM and extracts it in two
    steps. In the first step, MPVR meta-prompts the LLM with generic instructions
    and coarse information about the downstream task to generate diverse task-specific
    LLM query templates. These LLM query templates encode elements of the LLM’s knowledge
    about the visual styles characteristic of the downstream task but are still class-agnostic.
    In the second step, the LLM query templates are populated with names of candidate
    classes and fed to the LLM to obtain VLM prompts. The resulting VLM prompts are
    both task- and class-specific. Each prompt carries LLM’s diverse visual knowledge
    about the possible appearance of objects representing the class in the style defined
    by the downstream task.
  prefs: []
  type: TYPE_NORMAL
- en: 'For ease of assimilation, we divide our MPVR into two distinct stages and provide
    an overview in Figure [2](#S3.F2 "Figure 2 ‣ 3 MPVR: Meta-Prompting for Visual
    Recognition ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with
    LLMs"). In Section [3.1](#S3.SS1 "3.1 Meta-Prompting a Large Language Model ‣
    3 MPVR: Meta-Prompting for Visual Recognition ‣ Meta-Prompting for Automating
    Zero-shot Visual Recognition with LLMs"), we describe how to effectively meta-prompt
    LLMs to generate diverse, task-specific LLM query templates (stage 1). Later in
    Section [3.2](#S3.SS2 "3.2 Category-Specific VLM prompts ‣ 3 MPVR: Meta-Prompting
    for Visual Recognition ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs") we describe how to use these task-specific LLM query templates to
    obtain category-specific VLM prompts (stage 2).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c4100978ba7209a637173552e7230f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: MPVR framework. In the first stage, a meta-prompt comprising of a
    system prompt, in-context examples, and metadata consisting of downstream task
    specification is queried to the LLM instructing it to generate multiple diverse
    task-specific LLM queries, which are populated with the category of interest and
    again queried to the LLM to obtain the category-level prompts for assembling a
    zero-shot classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Meta-Prompting a Large Language Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aligning with the true motivation of our MPVR, the goal of meta-prompting is
    to extract the abundant visual world knowledge possessed by the LLMs by querying
    it to generate multiple diverse LLM query templates with minimal human intervention.
    To that end, we compose a meta-prompt of three parts: the system prompt, an in-context
    example, and the downstream task specification. We illustrate the meta-prompt
    in Figure [3](#S3.F3 "Figure 3 ‣ Downstream task specification ‣ 3.1 Meta-Prompting
    a Large Language Model ‣ 3 MPVR: Meta-Prompting for Visual Recognition ‣ Meta-Prompting
    for Automating Zero-shot Visual Recognition with LLMs").'
  prefs: []
  type: TYPE_NORMAL
- en: System prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: is a generic set of instructions that describe the elements of the meta-prompt
    and specify the expected output of the LLM. It instructs the LLM to generate a
    variety of query templates for the downstream dataset and conveniently format
    them to be employed in a Python script.
  prefs: []
  type: TYPE_NORMAL
- en: In-context example
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'serves to bootstrap the LLM to the type of output that is expected. It comprises
    a description of an example downstream task and a list of the corresponding LLM
    query templates. Since we expect the output from the LLM to be suitable for use
    in a Python script thus, it contains the prompts listed as Python code (*c.f.,*
    Figure [3](#S3.F3 "Figure 3 ‣ Downstream task specification ‣ 3.1 Meta-Prompting
    a Large Language Model ‣ 3 MPVR: Meta-Prompting for Visual Recognition ‣ Meta-Prompting
    for Automating Zero-shot Visual Recognition with LLMs"), middle left & right).'
  prefs: []
  type: TYPE_NORMAL
- en: Downstream task specification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'is the only part of the meta-prompt that is specific to the downstream task.
    It is scraped from a public API or the webpage of the dataset associated with
    the task and contains a general description of the task data (*c.f.,* Figure [3](#S3.F3
    "Figure 3 ‣ Downstream task specification ‣ 3.1 Meta-Prompting a Large Language
    Model ‣ 3 MPVR: Meta-Prompting for Visual Recognition ‣ Meta-Prompting for Automating
    Zero-shot Visual Recognition with LLMs"), bottom left & right). This coarse information
    about the downstream task of interest is critical for the LLM to generate task-specific
    LLM queries, which are employed in stage 2 of MPVR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/02ade9f7af33b96823d656e3ea9a0979.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Our meta-prompt comprises $3$ parts: A system prompt provides an
    overview of what is included in the overall prompt and what is expected from the
    LLM as a response (top). An in-context example consisting of metadata, dataset
    name, and hand-crafted prompts for the dataset (middle left). The downstream task
    metadata for which a diverse set of prompts are requested from the LLM (bottom
    left). For completeness, we also provide the in-context demonstrations (middle
    right) we use throughout, and the diverse LLM-generated queries for the example
    ImageNet-R dataset (bottom right).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that the system prompt and the in-context example demonstrations are generic
    and are kept fixed across different tasks in all of our experiments. The downstream
    task specification is the only part of the meta-prompt that is specific to the
    downstream task. Our experiments highlight that all the individual parts of the
    meta-prompt are extremely vital for our MPVR to obtain effective category-specific
    VLM prompts and are extensively ablated in Table [5](#S4.T5 "Table 5 ‣ MMLMs for
    Zero-shot Classification. ‣ 4.3 Ablations ‣ 4 Experimental Evaluation ‣ Meta-Prompting
    for Automating Zero-shot Visual Recognition with LLMs").
  prefs: []
  type: TYPE_NORMAL
- en: The three elements of the meta-prompt are embedded in the template presented
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Meta-Prompting for Automating
    Zero-shot Visual Recognition with LLMs") (left). The resulting meta-prompt is
    then fed to the LLM (GPT [[2](#bib.bib2)] or Mixtral [[16](#bib.bib16)]) to generate
    $N$ of our MPVR explained next.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Category-Specific VLM prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The LLM response to the meta-prompt in stage 1 is a diverse set of LLM query
    templates, which contain task-specific knowledge about the downstream task of
    interest, but are still generic. To instill the category information, for obtaining
    the category-specific VLM prompts, we replace the generic  placeholders
    in the LLM query templates with the actual class of interest. These diverse category-specific
    queries constitute our second call to the LLM, which generates category-specific
    VLM prompts. They carry the LLM’s knowledge of the appearance of objects of the
    queried classes in the context of the downstream task and are ready to be plugged
    into Eq. ([1](#S3.E1 "Equation 1 ‣ 3 MPVR: Meta-Prompting for Visual Recognition
    ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs")). We
    repeat this procedure for each class from 20 different datasets (used for evaluations)
    with both the GPT [[2](#bib.bib2)] and Mixtral [[16](#bib.bib16)] LLMs and obtain
    a huge corpus of $\sim 2.5\text{M}$ VLM prompts. In section [4](#S4 "4 Experimental
    Evaluation ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs"),
    we show that the ensemble of these VLM prompts results in a zero-shot classifier
    that outperforms previous methods by a significant margin.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The VLM prompts can be thought of as visually diverse descriptions of the queried
    classes in the context of the downstream tasks, and their corpus represents a
    chunk of the LLM’s knowledge about our visual world. This diversity stems from
    our proposed two-stage approach³³3We also experimented with generating category-specific
    VLM prompts in a single step with meta-prompting, but it performs worse than our
    2-stage framework. These results are provided in the ablations Table [6](#S4.T6
    "Table 6 ‣ MMLMs for Zero-shot Classification. ‣ 4.3 Ablations ‣ 4 Experimental
    Evaluation ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs")..
    The first stage can already provide diverse LLM query templates, which resemble
    the dataset-specific templates for prompt ensembling \@footnotemark (but more
    diverse and automatically generated with our MPVR). Interestingly, even by generating
    the ensemble of zero-shot classifiers by populating these generic query templates
    from stage 1 with category information, we can already achieve enhanced zero-shot
    recognition, as reported in an ablation in Table [6](#S4.T6 "Table 6 ‣ MMLMs for
    Zero-shot Classification. ‣ 4.3 Ablations ‣ 4 Experimental Evaluation ‣ Meta-Prompting
    for Automating Zero-shot Visual Recognition with LLMs"). To conclude, after the
    second call to the LLM, the VLM prompts constitute fine-grained details about
    the specific category, reflecting the true diversity of the visual LLM knowledge
    and resulting in a huge category-specific text corpus, already incorporated in
    our codebase released on this public Github repository: [https://github.com/jmiemirza/Meta-Prompting](https://github.com/jmiemirza/Meta-Prompting).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experimental Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first briefly describe the datasets and the baselines we
    use to evaluate and compare our MPVR, then explain our implementation details
    and finally provide a detailed discussion of the results.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Evaluation Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We extensively evaluate our MPVR on $20$ object recognition datasets belonging
    to widely different domains. These domains can be narrowed down to datasets containing
    commonly occurring natural categories: ImageNet [[9](#bib.bib9)], ImageNet-V2 [[36](#bib.bib36)],
    CIFAR-10/100 [[21](#bib.bib21)], Caltech-101 [[10](#bib.bib10)]. Fine-grained
    classification datasets containing different task-specific images: Flowers [[32](#bib.bib32)],
    Standford Cars [[20](#bib.bib20)], CUBS-200 [[42](#bib.bib42)], Oxford Pets [[33](#bib.bib33)],
    Describable Textures dataset (DTD) [[7](#bib.bib7)], Food-101 [[1](#bib.bib1)],
    FGVC-Aircraft [[27](#bib.bib27)]. Datasets used for scene classification: Places365 [[50](#bib.bib50)]
    and SUN397 [[46](#bib.bib46)], action recognition datasets: UCF101 [[39](#bib.bib39)]
    and Kinetics400 [[17](#bib.bib17)]. Datasets consisting of out-of-distribution
    images: ImageNet-(R)endition [[12](#bib.bib12)] and ImageNet-(S)ketch [[43](#bib.bib43)]
    and also datasets which contain images taken from a satellite or an aerial view:
    EuroSAT [[11](#bib.bib11)] and RESISC45 [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Baselines:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare to the following baselines and state-of-the-art methods:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CLIP [[35](#bib.bib35)] denotes the zero-shot classification scores obtained
    by using the simple ‘{a photo of a }’ template (S-TEMP) and dataset-specific
    templates (DS-TEMP\@footnotemark).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CUPL [[34](#bib.bib34)] proposes to generate category-level descriptions from
    an LLM with hand-crafted prompts for each dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DCLIP [[28](#bib.bib28)] proposes to obtain a zero-shot classifier with category-specific
    descriptors (from an LLM) consisting of usual visual attributes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Waffle [[37](#bib.bib37)] employs hand-crafted task-specific broad concepts
    and adds random descriptors to the prompts for zero-shot classification. Following
    their evaluation setting, we compare with different variants: (i) Waffle (prompt
    + random descriptors), (ii) WaffleCon (Waffle + high-level concepts), and (iii)
    WaffleConGPT (WaffleCon + DCLIP descriptors).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP (S-TEMP) | 61.9 | 54.8 | 88.3 | 64.4 | 91.4 | 64.0 | 60.2 | 51.6 | 85.0
    | 40.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP (DS-TEMP) | 63.3 | 56.0 | 89.2 | 65.1 | 89.9 | 66.7 | 60.0 | 53.0 |
    87.4 | 42.4 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 64.3 | 56.9 | 89.0 | 65.3 | 92.1 | 68.8 | 60.0 | 51.9 | 87.2 | 48.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| DCLIP | 63.1 | 55.8 | 86.7 | 64.2 | 92.5 | 64.6 | 57.9 | 52.6 | 83.5 | 44.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 63.4 | 56.3 | 89.4 | 65.2 | 90.8 | 67.8 | 59.9 | 52.8 | 87.7 | 40.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 63.4 | 56.3 | 89.4 | 65.2 | 89.7 | 65.2 | 59.5 | 52.1 | 86.8
    | 41.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 63.4 | 56.3 | 89.4 | 65.2 | 91.9 | 68.2 | 59.6 | 52.6 |
    87.9 | 41.8 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (MIXTRAL) | 63.8 | 56.5 | 89.5 | 65.5 | 92.8 | 75.2 | 58.3 | 55.5 |
    88.0 | 50.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 65.0 | 57.3 | 89.9 | 66.3 | 92.9 | 73.9 | 59.5 | 55.9 | 88.1
    | 50.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP (S-TEMP) | 77.6 | 18.1 | 39.4 | 62.1 | 60.4 | 39.7 | 66.3 | 41.1 | 35.9
    | 54.1 |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP (DS-TEMP) | 79.2 | 19.5 | 40.0 | 63.0 | 62.4 | 42.1 | 69.3 | 42.7 |
    45.8 | 57.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 81.0 | 20.4 | _ | 66.5 | 65.2 | 41.7 | _ | _ | _ | 61.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DCLIP | 79.7 | 19.8 | 40.9 | 63.1 | 62.6 | 39.1 | 66.0 | 42.3 | 48.9 | 56.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 81.6 | 20.1 | 41.1 | 63.3 | 62.7 | 40.4 | 68.8 | 43.4 | 42.7 | 61.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 81.1 | 19.0 | 39.3 | 60.7 | 62.2 | 39.1 | 68.1 | 42.5 | 44.8
    | 58.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 81.2 | 19.8 | 41.5 | 64.0 | 63.4 | 40.4 | 68.5 | 43.7 |
    47.0 | 62.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (MIXTRAL) | 81.3 | 22.4 | 42.1 | 66.5 | 66.0 | 42.2 | 70.2 | 43.6 |
    54.0 | 64.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 81.0 | 21.5 | 42.2 | 67.0 | 67.9 | 43.9 | 70.2 | 44.2 | 55.6
    | 64.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Top-1 accuracy (%) for $20$ random runs are reported, following the
    original publication.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementation Details:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To report the results for each dataset we use the test splits provided by [[52](#bib.bib52)]
    and further build upon their framework for all our evaluations on datasets that
    are not present in their framework. All the baselines are also implemented in
    the same framework. To generate the diverse set of task-specific LLM queries for
    our MPVR in the first stage, we use the public web API of ChatGPT⁴⁴4[https://chat.openai.com/](https://chat.openai.com/)
    and the Hugging Face API for Mixtral-7B (8x)⁵⁵5[https://huggingface.co/chat/](https://huggingface.co/chat/).
    To obtain the category-level VLM prompts after querying an LLM in the second stage
    of MPVR, we use GPT-3.5 [[2](#bib.bib2)] and the open source weights of Mixtral-7B
    (8x) [[16](#bib.bib16)], accessed through Hugging Face. In the first stage, we
    instruct the LLM to generate $30$ tokens. The in-context dataset is (arbitrarily)
    chosen to be DTD [[7](#bib.bib7)] for all experiments, however, to avoid information
    contamination, we switch the in-context dataset to EuroSat [[11](#bib.bib11)]
    when DTD is the target dataset. We ablate the choice of DTD for in-context example
    and provide the complete meta prompts in the supplementary. Unless otherwise specified,
    we obtain the zero-shot classifier as the mean of the class embeddings obtained
    from the category-specific VLM prompts (from stage 2 of MPVR) using Eq. ([1](#S3.E1
    "Equation 1 ‣ 3 MPVR: Meta-Prompting for Visual Recognition ‣ Meta-Prompting for
    Automating Zero-shot Visual Recognition with LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We test our MPVR extensively on $20$ datasets, with GPT and Mixtral LLMs respectively.
    Similarly, while compared to the more expressive CLIP zero-shot baseline, which
    uses the hand-crafted dataset-specific templates\@footnotemark, we still observe
    considerable average gains of 3.1% and 2.7% with the two LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Our MPVR also shows strong gains when compared to CUPL [[34](#bib.bib34)], which
    obtains category-level prompts by hand-crafting the LLM queries for each downstream
    task of interest. Our MPVR not only alleviates this extensive human effort spent
    while generating the category-level prompts (as in CUPL [[34](#bib.bib34)]) but
    also out-performs CUPL on most of the datasets we compare to. For example, obtaining
    up to $5.1\%$ performance gains on Flowers-102 [[32](#bib.bib32)] dataset with
    GPT and Mixtral LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | OpenAI CLIP | MetaCLIP 400m |'
  prefs: []
  type: TYPE_TB
- en: '|  | B/16 | L/14 | B/32 | B/16 | L/14 |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 61.9 | 69.2 | 62.4 | 65.9 | 71.0 |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 63.8 | 71.2 | 64.0 | 67.3 | 72.8 |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 64.4 | 70.7 | 62.8 | 66.4 | 72.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 64.0 | 70.7 | 62.8 | 66.5 | 72.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 62.7 | 69.1 | 61.7 | 65.7 | 71.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 64.6 | 71.0 | 63.2 | 66.9 | 72.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 66.4 | 72.5 | 65.6 | 68.7 | 73.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 66.7 | 73.4 | 65.8 | 68.7 | 74.3 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Mean top-1 accuracy (%) over $20$ datasets for different backbones
    from OpenAI [[35](#bib.bib35)] and MetaCLIP-400m [[47](#bib.bib47)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we also observe that while comparing with the baselines which
    do not generate (descriptive) VLM prompts but rely on other cues like category-level
    (attribute) descriptors, our MPVR also performs favorably. For example, we outperform
    DCLIP [[28](#bib.bib28)] on all the $20$ on UCF-101 with GPT and Mixtral. These
    results indicate that the generic attributes generated for a category by DCLIP
    for classification might not capture fine-grained task-specific details required
    to enhance the classification of categories in domain-specific benchmarks (*e.g.,* action
    recognition in UCF-101). Finally, from Table [1](#S4.T1 "Table 1 ‣ Baselines:
    ‣ 4.1 Evaluation Settings ‣ 4 Experimental Evaluation ‣ Meta-Prompting for Automating
    Zero-shot Visual Recognition with LLMs") we also observe that our MPVR (on average)
    also outperforms all the variants proposed by Waffle [[37](#bib.bib37)], which
    also highlights that the CLIP text encoder responds favorably to semantically
    rich text descriptions (prompts), instead of randomly generated descriptors as
    in Waffle [[37](#bib.bib37)]. In summary, our MPVR demonstrates better performance
    across the board, outperforming all baselines on 18 out of 20 datasets. On the
    Food-101 [[1](#bib.bib1)] dataset, our MPVR comes in second, trailing by a narrow
    margin of 0.3%. Similarly, on Standford Cars [[20](#bib.bib20)], our results indicate
    that even the dataset-specific prompt ensembling proposed by CLIP fails to enhance
    performance, underscoring the unique challenges posed by this particular dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: To test the generalization ability of our MPVR beyond different LLMs, we also
    evaluate it with different backbones from CLIP [[35](#bib.bib35)] and also employ
    MetaCLIP [[47](#bib.bib47)], which is trained with a different training recipe
    than CLIP. These results are listed in Table [2](#S4.T2 "Table 2 ‣ 4.2 Results
    ‣ 4 Experimental Evaluation ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs"). We observe that even while testing with more expressive backbones,
    like MetaCLIP ViT-L/14, our visually diverse text descriptions (prompts) help
    to improve the zero-shot accuracy from $71.0\%\to 74.3\%$ datasets. Due to space
    limitations, we defer the individual dataset results for these backbones to the
    supplementary.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Ablations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Here, we study the significance of different components that constitute our
    MPVR. Specifically, we first examine the effect of combining multiple text sources,
    and then motivate our choice of using dual encoder models like CLIP [[35](#bib.bib35)]
    instead of multi-modal language models (MMLMs) by evaluating them for image classification.
    Later we extensively ablate our prompting strategy and finally conclude with ablations
    on robustness of the obtained results and scaling analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  |  | GPT | Mixtral | Temp | GPT+Temp | Mixtral+Temp | GPT+Mixtral | GPT+Mixtral+Temp
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Embedding | Average | ViT-B/32 | 62.9 | 62.4 | 59.7 | 57.0 | 56.1 | 63.0
    | 57.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-B/16 | 66.7 | 66.4 | 63.8 | 60.5 | 59.6 | 67.0 | 61.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-L/14 | 73.4 | 72.5 | 71.2 | 68.6 | 67.3 | 73.4 | 69.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Softmax | Average | ViT-B/32 | _ | _ | 59.8 | 62.8 | 62.3 | 62.4 | 62.4 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-B/16 | _ | _ | 63.8 | 66.7 | 66.4 | 66.4 | 66.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-L/14 | _ | _ | 71.1 | 73.3 | 72.4 | 72.6 | 72.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of mean top-1 accuracy (%) for MPVR over 20 datasets while
    constructing the zero-shot classifier by ensembling with the mean of the embeddings
    from different text sources (top) and mean of softmax (bottom). For GPT and Mixtral,
    we only report the results with the mean of the embeddings, since ensembling the
    softmax of individual descriptions is prohibitively expensive (also noted in [[35](#bib.bib35)]).
    For datasets with fewer classes, we performed softmax ensembling but did not find
    any major deviation in results. These results are provided in the supplementary.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | EuroSAT | DTD | Caltech | CIFAR-100 | Resisc | Mean |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| CLIP (ViT-B/32) | 35.9 | 40.2 | 91.4 | 64.4 | 54.1 | 57.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LLAVA-1.6 (7B) | 41.3 | 16.2 | 33.0 | 25.7 | 33.8 | 30.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (ViT-B/32) | 55.6 | 50.8 | 92.9 | 66.3 | 64.0 | 65.5 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Comparison of top-1 accuracy (%) with LLAVA-1.6-Vicuna7b model [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: Ensembling Text Sources.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'From Tables [1](#S4.T1 "Table 1 ‣ Baselines: ‣ 4.1 Evaluation Settings ‣ 4
    Experimental Evaluation ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs") & [2](#S4.T2 "Table 2 ‣ 4.2 Results ‣ 4 Experimental Evaluation ‣
    Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs") we gather
    that in addition to the enhanced zero-shot classification with GPT and Mixtral
    generated VLM prompts with our MPVR, the dataset-specific templates\@footnotemark
    from CLIP can also show improvement in results, in comparison to only using the
    default templates. To evaluate the combined performance of these text sources,
    we ensemble the $3$ different sources and provide the results in Table [3](#S4.T3
    "Table 3 ‣ 4.3 Ablations ‣ 4 Experimental Evaluation ‣ Meta-Prompting for Automating
    Zero-shot Visual Recognition with LLMs"). We observe that when the category-specific
    VLM prompts and templates are ensembled over the embedding space, the resulting
    classifier is weaker than the classifier obtained from only the LLM-generated
    VLM prompts. However, the mean of the embeddings from both GPT and Mixtral prompts
    performs the best. These results hint that the prompts from both the LLMs are
    clustered closely in the CLIP latent space suggesting that these sources describe
    the categories of interest in a similar (more detailed) way, yet differently from
    the ‘more mechanical’ CLIP dataset-specific prompts that do not provide much detail.
    We also test ensembling the probability spaces from both sources and find that
    the degradation in performance as a consequence of mixing the descriptions and
    templates is alleviated.'
  prefs: []
  type: TYPE_NORMAL
- en: MMLMs for Zero-shot Classification.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, multi-modal language models such as LLaVA [[26](#bib.bib26), [24](#bib.bib24)]
    have emerged as the preferred choice for various vision-language tasks. Here,
    we extended their evaluation to zero-shot classification, and the findings are
    summarized in Table [4](#S4.T4 "Table 4 ‣ 4.3 Ablations ‣ 4 Experimental Evaluation
    ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs"). Notably,
    our results indicate that, for the specific task of object recognition, CLIP [[35](#bib.bib35)]
    outperforms LLaVA by a substantial margin, reinforcing our decision to employ
    CLIP for the discriminative task, which is the focus of our study. We ablate and
    detail the sensitivity of MMLMs to different prompting strategies in the supplementary,
    here we report only its best prompting strategy result.
  prefs: []
  type: TYPE_NORMAL
- en: '| dataset name | dataset metadata | in-context (prompts) | class names | Top-1
    |'
  prefs: []
  type: TYPE_TB
- en: '| ✗ | ✓ | ✓ | ✗ | 46.7 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✗ | ✓ | ✗ | 42.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | ✗ | ✗ | _ |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | ✓ | ✓ | 53.5 |'
  prefs: []
  type: TYPE_TB
- en: '| ✓ | ✓ | ✓ | ✗ | 55.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Top-1 accuracy (%) for EuroSAT [[11](#bib.bib11)] with GPT as LLM
    and the ViT-B/16 backbone [[35](#bib.bib35)] while ablating the different parts
    of our Meta Prompt. The last row represents the results obtained by our MPVR.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | CLIP (S-TEMP) | CLIP (DS-TEMP) | Prompts-Only | 1-Step | MPVR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EuroSAT | 35.9 | 45.8 | 47.2 | 51.2 | 55.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Comparison of top-1 accuracy (%) from the zero-shot classifier obtained
    with the prompts generated in the first stage and generating category-level descriptions
    directly from stage-1 of MPVR.'
  prefs: []
  type: TYPE_NORMAL
- en: Meta Prompt.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [5](#S4.T5 "Table 5 ‣ MMLMs for Zero-shot Classification. ‣ 4.3 Ablations
    ‣ 4 Experimental Evaluation ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs") we ablate different components of our meta-prompt (outlined in Figure [3](#S3.F3
    "Figure 3 ‣ Downstream task specification ‣ 3.1 Meta-Prompting a Large Language
    Model ‣ 3 MPVR: Meta-Prompting for Visual Recognition ‣ Meta-Prompting for Automating
    Zero-shot Visual Recognition with LLMs")) and report the results on the EuroSAT
    dataset. We see that all the major components have a strong effect on the downstream
    performance. For example, if we do not populate the meta-prompt with the in-context
    demonstrations of example LLM queries for a dataset, the LLM fails to generate
    the task-specific queries from the first stage. Similarly, removing the metadata
    (description of datasets) from the in-context example and the resulting dataset
    of interest also results in a huge performance drop $55.6\%\to 42.0\%$. We also
    noticed that interestingly, providing the category names for the datasets in the
    meta prompt (for stage 1) as extra information did not improve the results, potentially
    hinting that LLM prefers more simple and succinct instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: Altering Meta Prompting Stages.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [6](#S4.T6 "Table 6 ‣ MMLMs for Zero-shot Classification. ‣ 4.3 Ablations
    ‣ 4 Experimental Evaluation ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs") we report the results by altering our meta-prompting strategy in two
    distinct ways: By generating the category-level VLM prompts directly in one step,
    by incorporating the class name already in stage 1 of our MPVR, and populating
    the  in the generated task-specific LLM queries from stage 1 (which
    resembles the prompt ensembling performed by CLIP [[35](#bib.bib35)]). The results
    indicate that our 2-stage approach performs better than altering it to a single
    stage, and even our generated prompts from stage 1 can offer a more robust zero-shot
    classifier than templates ensembling\@footnotemark, highlighting the visual diversity
    of our generated task-specific queries, which later effectively translates to
    the VLM prompts as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | accuracy(%) | std |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-B/32 | 62.8 | $\pm$0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-B/16 | 66.7 | $\pm$0.04 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-L/14 | 73.3 | $\pm$0.03 |'
  prefs: []
  type: TYPE_TB
- en: 'Figure 4: Top-1 mean accuracy (%) for CLIP and standard deviation for $10$
    random runs, for all datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d06417bd98e816f70e92ece70f72ad7f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Top-1 mean accuracy (%) over DTD, EuroSat, Flowers, Resisc45, subsampling
    the VLM prompts sets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Results Robustness and Scaling Analysis:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [5](#S4.F5 "Figure 5 ‣ Altering Meta Prompting Stages. ‣ 4.3 Ablations
    ‣ 4 Experimental Evaluation ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs") we study the robustness of MPVR results by reporting the mean and
    variance with randomly sampling MPVR-generated VLM prompts $10$ datasets. We observe
    that the variances are negligible w.r.t. the obtained gains (in Table [1](#S4.T1
    "Table 1 ‣ Baselines: ‣ 4.1 Evaluation Settings ‣ 4 Experimental Evaluation ‣
    Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs")). In Figure [5](#S4.F5
    "Figure 5 ‣ Altering Meta Prompting Stages. ‣ 4.3 Ablations ‣ 4 Experimental Evaluation
    ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs") we show
    the scaling potential by sampling more VLM category- and task-specific prompts.
    The results highlight that sampling an increasing number of generated VLM prompts
    significantly boosts performance showing promising scaling potential.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have presented meta-prompting for enhancing zero-shot visual recognition
    with LLMs, which essentially alleviates any human involvement in VLM prompt design
    for new tasks. Our MPVR generates task-specific category-level VLM prompts by
    only requiring minimal information about the downstream task of interest. MPVR
    first queries the LLM to generate different high-level queries letting it discover
    the diverse ways of querying itself to generate visually diverse category-level
    prompts. These prompts are ensembled to construct a robust zero-shot classifier,
    that achieves enhanced zero-shot classification on a diverse set of $20$ category-level
    text descriptions dataset, harnessed from GPT and Mixtral, covering the breadth
    of the LLM knowledge of our visual world. This large-scale dataset can be employed
    in many exciting future work directions, *e.g.,* fine-tuning multi-modal language
    models for enhanced fine-grained visual classification, or constructing large-scale
    synthetic datasets via generative text-to-image models for VLM pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Bossard, L., Guillaumin, M., Van Gool, L.: Food-101 – Mining Discriminative
    Components with Random Forests. In: Proc. ECCV (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Brown, T.B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
    Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.M., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., Amodei, D.: Language Models
    are Few-Shot Learners. arXiv:2005.14165 (2020)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Chen, J., Li, D.Z.X.S.X., Zhang, Z.L.P., Xiong, R.K.V.C.Y., Elhoseiny,
    M.: MiniGPT-v2: Large Language Model as a Unified Interface for Vision-Language
    Multi-task Learning. arXiv preprint arXiv:2310.09478 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Chen, M., Du, J., Pasunuru, R., Mihaylov, T., Iyer, S., Stoyanov, V., Kozareva,
    Z.: Improving In-Context Few-Shot Learning via Self-Supervised Training. arXiv
    preprint arXiv:2205.01703 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Cheng, G., Han, J., Lu, X.: Remote Sensing Image Scene Classification:
    Benchmark and State of the Art. Proceedings of the IEEE 105(10), 1865–1883 (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L.,
    Zhuang, S., Zhuang, Y., Gonzalez, J.E., Stoica, I., Xing, E.P.: Vicuna: An Open-Source
    Chatbot Impressing GPT-4 with 90%* ChatGPT Quality (March 2023), [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Cimpoi, M., Maji, S., Kokkinos, I., Mohamed, S., , Vedaldi, A.: Describing
    Textures in the Wild. In: Proc. CVPR (2014)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P.,
    Hoi, S.: InstructBLIP: Towards General-purpose Vision-Language Models with Instruction
    Tuning. In: NeurIPS (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: ImageNet:
    A large-scale hierarchical image database. In: Proc. CVPR (2009)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Fei-Fei, L., Fergus, R., Perona, P.: Learning Generative Visual Models
    from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object
    Categories. In: Proc. CVPR (2004)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Helber, P., Bischke, B., Dengel, A., Borth, D.: EuroSAT: A Novel Dataset
    and Deep Learning Benchmark for Land Use and Land Cover Classification. In: Proc.
    IGARSS (2018)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E.,
    Desai, R., Zhu, T., Parajuli, S., Guo, M., Song, D., Steinhardt, J., Gilmer, J.:
    The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization.
    In: Proc. ICCV (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Hou, Y., Dong, H., Wang, X., Li, B., Che, W.: MetaPrompting: Learning
    to Learn Better Prompts. arXiv preprint arXiv:2209.11486 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Huang, T., Chu, J., Wei, F.: Unsupervised Prompt Learning for Vision-Language
    Models. arXiv:2204.03649 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Jia, C., Yang, Y., Xia, Y., Chen, Y.T., Parekh, Z., Pham, H., Le, Q.V.,
    Sung, Y., Li, Z., Duerig, T.: Scaling Up Visual and Vision-Language Representation
    Learning With Noisy Text Supervision. In: Proc. ICML (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Jiang, A.Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford,
    C., Chaplot, D.S., Casas, D.d.l., Hanna, E.B., Bressand, F., et al.: Mixtral of
    Experts. arXiv preprint arXiv:2401.04088 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
    S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The
    Kinetics Human Action Video Dataset (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Khattak, M.U., Rasheed, H., Maaz, M., Khan, S., Khan, F.S.: MaPLe: Multi-Modal
    Prompt Learning. In: Proc. CVPR (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large Language
    Models are Zero-Shot Reasoners. NeurIPS 35, 22199–22213 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Krause, J., Stark, M., Deng, J., Fei-Fei, L.: 3D Object Representations
    for Fine-Grained Categorization. In: Proc. ICCVW (2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Krizhevsky, A., Hinton, G.: Learning Multiple Layers of Features from
    Tiny Images. Tech. rep., Department of Computer Science, University of Toronto
    (2009)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: Bootstrapping Language-Image
    Pre-training with Frozen Image Encoders and Large Language Models. arXiv preprint
    arXiv:2301.12597 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Lin, W., Karlinsky, L., Shvetsova, N., Possegger, H., Kozinski, M., Panda,
    R., Feris, R., Kuehne, H., Bischof, H.: MAtch, eXpand and Improve: Unsupervised
    Finetuning for Zero-Shot Action Recognition with Language Knowledge. In: Proc.
    ICCV (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved Baselines with Visual Instruction
    Tuning. arXiv:2310.03744 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Liu, H., Li, C., Li, Y., Lee, Y.J.: LLaVA-Next (LLaVA 1.6). arXiv:2310.03744
    (2023), [https://llava-vl.github.io/blog/2024-01-30-llava-next/](https://llava-vl.github.io/blog/2024-01-30-llava-next/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual Instruction Tuning. In: NeurIPS
    (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Maji, S., Rahtu, E., Kannala, J., Blaschko, M., Vedaldi, A.: Fine-Grained
    Visual Classification of Aircraft. arXiv preprint arXiv:1306.5151 (2013)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Menon, S., Vondrick, C.: Visual Classification via Description from Large
    Language Models. Proc. ICLR (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Min, S., Lewis, M., Zettlemoyer, L., Hajishirzi, H.: MetaICL: Learning
    to Learn In Context. arXiv preprint arXiv:2110.15943 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Mirza, M.J., Karlinsky, L., Lin, W., Possegger, H., Feris, R., Bischof,
    H.: TAP: Targeted Prompting for Task Adaptive Generation of Textual Training Instances
    for Visual Classification. arXiv preprint arXiv:2309.06809 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Mirza, M.J., Karlinsky, L., Lin, W., Possegger, H., Kozinski, M., Feris,
    R., Bischof, H.: LaFTer: Label-Free Tuning of Zero-shot Classifier using Language
    and Unlabeled Image Collections. In: NeurIPS (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Nilsback, M.E., Zisserman, A.: Automated Flower Classification Over a
    Large Number of Classes. In: Proc. ICVGIP (2008)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Parkhi, O.M., Vedaldi, A., Zisserman, A., Jawahar, C.V.: Cats and dogs.
    In: Proc. CVPR. pp. 3498–3505 (2012). https://doi.org/10.1109/CVPR.2012.6248092'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Pratt, S., Liu, R., Farhadi, A.: What does a platypus look like? Generating
    customized prompts for zero-shot image classification. arXiv:2209.03320 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S.,
    Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning Transferable
    Visual Models from Natural Language Supervision. In: Proc. ICML (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Recht, B., Roelofs, R., Schmidt, L., Shankar, V.: Do ImageNet Classifiers
    Generalize to ImageNet? In: Proc. ICML. pp. 5389–5400\. PMLR (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Roth, K., Kim, J.M., Koepke, A., Vinyals, O., Schmid, C., Akata, Z.: Waffling
    around for Performance: Visual Classification with Random Words and Broad Concepts.
    arXiv preprint arXiv:2306.07282 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Schuhmann, C., Beaumont, R., Vencu, R., Gordon, C.W., Wightman, R., Cherti,
    M., Coombes, T., Katta, A., Mullis, C., Wortsman, M., Schramowski, P., Kundurthy,
    S.R., Crowson, K., Schmidt, L., Kaczmarczyk, R., Jitsev, J.: LAION-5b: An open
    large-scale dataset for training next generation image-text models. In: NeurIPS
    (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Soomro, K., Zamir, A.R., Shah, M.: UCF101: A Dataset of 101 Human Actions
    Classes From Videos in The Wild. arXiv:1212.0402 (2012)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Suzgun, M., Kalai, A.T.: Meta-Prompting: Enhancing Language Models with
    Task-Agnostic Scaffolding. arXiv preprint arXiv:2401.12954 (2024)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei,
    Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open
    Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Wah, C., Branson, S., Welinder, P., Perona, P., Belongie, S.: The Caltech-UCSD
    Birds-200-2011 Dataset. Tech. rep., California Institute of Technology (2011)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Wang, H., Ge, S., Lipton, Z., Xing, E.P.: Learning Robust Global Representations
    by Penalizing Local Predictive Power. In: NeurIPS (2019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Wei, J., Bosma, M., Zhao, V.Y., Guu, K., Yu, A.W., Lester, B., Du, N.,
    Dai, A.M., Le, Q.V.: Finetuned Language Models Are Zero-Shot Learners. arXiv preprint
    arXiv:2109.01652 (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V.,
    Zhou, D., et al.: Chain-of-Thought Prompting Elicits Reasoning in Large Language
    Models. NeurIPS 35, 24824–24837 (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: SUN Database:
    Large-scale Scene Recognition from Abbey to Zoo. In: Proc. CVPR (2010)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Xu, H., Xie, S., Tan, X.E., Huang, P.Y., Howes, R., Sharma, V., Li, S.W.,
    Ghosh, G., Zettlemoyer, L., Feichtenhofer, C.: Demystifying CLIP Data. In: Proc.
    ICLR (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T., Cao, Y., Narasimhan,
    K.: Tree of Thoughts: Deliberate Problem Solving with Large Language Models. NeurIPS
    36 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Zhao, Z., Wallace, E., Feng, S., Klein, D., Singh, S.: Calibrate Before
    Use: Improving Few-Shot Performance of Language Models. In: Proc. ICML. pp. 12697–12706\.
    PMLR (2021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., Torralba, A.: Places:
    A 10 million Image Database for Scene Recognition. IEEE TPAMI 40(6), 1452–1464
    (2017)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional Prompt Learning for
    Vision-Language Models. In: Proc. CVPR (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to Prompt for Vision-Language
    Models. IJCV (2022)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: MiniGPT-4: Enhancing
    Vision-Language Understanding with Advanced Large Language Models. arXiv preprint
    arXiv:2304.10592 (2023)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supplementary Material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As supplementary material for our MPVR: Meta Prompting for Visual Recognition,
    we first list some additional implementation details (Section [0.A](#Pt0.A1 "Appendix
    0.A Implementation Details ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs")). Then, for additional insights, we provide an ablation on the use
    of the in-context dataset employed for meta-prompting (Section [0.B](#Pt0.A2 "Appendix
    0.B Meta Prompt ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with
    LLMs")). Moving forward, we provide results with different strategies employed
    for prompting multimodal language models (MMLMs) for the task of object recognition
    (Section [0.C](#Pt0.A3 "Appendix 0.C Prompt Engineering for MMLM ‣ Meta-Prompting
    for Automating Zero-shot Visual Recognition with LLMs")), demonstrating we used
    the best performing available strategy for the MMLM baseline in the main paper.
    Then, we provide results for ensembling (in probability space) the vision language
    model (VLM) prompts generated through our MPVR (Section [0.D](#Pt0.A4 "Appendix
    0.D Ensembling Descriptions ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs")). Later, we conclude with the detailed (dataset-wise) results (Section [0.E](#Pt0.A5
    "Appendix 0.E Detailed Results ‣ Meta-Prompting for Automating Zero-shot Visual
    Recognition with LLMs")).'
  prefs: []
  type: TYPE_NORMAL
- en: To encourage reproducibility for our MPVR, the code for all the $20$ category-level
    text dataset. This dataset will also be open-sourced for the community, upon acceptance.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.A Implementation Details
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All our experiments are performed on a single NVIDIA 3090 GPU. To obtain the
    results for the baselines, we use their official codebase and run the baselines
    locally with all their recommended parameters and settings. For CUPL [[34](#bib.bib34)]
    we only report the results on the datasets, for which the authors provided the
    category-level VLM prompts. Since CUPL uses hand-crafted dataset-specific LLM
    queries to generate the category-level VLM prompts, for some datasets these queries
    are not available, so we were not able to generate the VLM prompts for those datasets.
    We used the category-level VLM attributes provided by DCLIP [[28](#bib.bib28)]
    in their official repository⁶⁶6[https://github.com/sachit-menon/classifybydescriptionrelease](https://github.com/sachit-menon/classify_by_description_release).
    For the datasets, not listed in their repository, we used their official code
    to generate the attributes and used them for obtaining the Waffle [[37](#bib.bib37)]
    results, following the official publication. In contrast to CUPL [[34](#bib.bib34)],
    the attributes can be generated for any dataset, only by providing the class names
    from the downstream datasets. Similarly, following the official publication and
    settings proposed in Waffle [[37](#bib.bib37)], the datasets for which the high-level
    concepts are not available (*i.e.,* ImageNet [[9](#bib.bib9)], ImageNetv2 [[36](#bib.bib36)],
    CIFAR10/100 [[21](#bib.bib21)]), their two variants, Waffle+Con and Waffle+Con+GPT,
    collapse to only the Waffle results, in all the tables.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.B Meta Prompt
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the main manuscript, we arbitrarily employed the Describable Textures Dataset
    (DTD) [[7](#bib.bib7)] as the in-context example dataset for all our experiments.
    However, when the target dataset is DTD, we switched the in-context example dataset
    to EuroSAT [[11](#bib.bib11)]. Here, we studied the effect of employing different
    in-context datasets. For example, when employing an alternative in-context dataset,
    such as Flowers [[32](#bib.bib32)] or CUBS [[42](#bib.bib42)] for DTD (as the
    target dataset), the variance in results is only $\pm 0.71$) obtained over the
    baseline of CLIP + ‘dataset-specific templates’, for the ViT-B/32 backbone from
    CLIP [[35](#bib.bib35)].
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, while using an alternative in-context dataset, Flowers or Cubs, for
    the target dataset EuroSAT, the variance in obtained results is only $\pm 0.44$)
    obtained over the baseline of CLIP + ‘dataset-specific templates’.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/03b0c8cb7febc06864cf5276cceb59a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Exemplary meta-prompts (and a few LLM generated responses) for MPVR
    using different in-context (left: DTD [[7](#bib.bib7)], right: Flowers [[32](#bib.bib32)])
    and target (left: ImageNet-R [[12](#bib.bib12)], right: DTD [[7](#bib.bib7)])
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, for completeness, we also provide $2$ complete meta-prompt examples
    in Figure [6](#Pt0.A2.F6 "Figure 6 ‣ Appendix 0.B Meta Prompt ‣ Meta-Prompting
    for Automating Zero-shot Visual Recognition with LLMs") while choosing different
    in-context demonstrators (*i.e.,* DTD [[7](#bib.bib7)] and Flowers [[32](#bib.bib32)])
    and target datasets (*i.e.,* ImageNet-R [[12](#bib.bib12)] and DTD [[7](#bib.bib7)]).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.C Prompt Engineering for MMLM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To address the sensitivity of MMLMs to different prompting strategies, we extensively
    tested the following different prompting variations used for the task of category
    recognition for MMLMs. These prompting strategies are also illustrated in Figure [7](#Pt0.A3.F7
    "Figure 7 ‣ Categories as List: ‣ Appendix 0.C Prompt Engineering for MMLM ‣ Meta-Prompting
    for Automating Zero-shot Visual Recognition with LLMs") for the EuroSAT dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Categories as Numbered Options:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The prompt to the MMLM [[25](#bib.bib25)] contained the categories (the model
    needed to choose from) listed as numbered options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Categories as Alphabet Options:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The prompt to the MMLM [[25](#bib.bib25)] contained the categories (the model
    needed to choose from) listed as English alphabet options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Categories as List:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this prompting strategy, we provided the category names as a list and the
    MMLM was prompted to output the exact name of the category for each test image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Table [7](#Pt0.A3.T7 "Table 7 ‣ Categories as List: ‣ Appendix 0.C Prompt
    Engineering for MMLM ‣ Meta-Prompting for Automating Zero-shot Visual Recognition
    with LLMs") we list the results for different prompting strategies and find that
    the best results were obtained when LLAVA-1.6 [[25](#bib.bib25)] was prompted
    with categories (to choose from) as numbered options. For the fairest comparison,
    the LLAVA-1.6 [[25](#bib.bib25)] baseline results reported in Table $4$ of the
    main manuscript were obtained using this (top-performing) prompting option for
    all the tested datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Numbered Options | Alphabet Options | List Option |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| EuroSAT | 41.3 | 38.7 | 34.4 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Top-1 accuracy (%) with different prompting strategies for LLAVA-1.6 [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Example of different prompting options explored for LLAVA for EuroSAT [[11](#bib.bib11)].'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.D Ensembling Descriptions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Ensemble in Embedding Space |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | EuroSAT | Flowers | DTD | Resisc | Mean |'
  prefs: []
  type: TYPE_TB
- en: '| Top-1 (%) - ViT-B/32 | 55.6 | 73.9 | 50.8 | 64.0 | 61.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Ensemble in Probability Space |'
  prefs: []
  type: TYPE_TB
- en: '|  | EuroSAT | Flowers | DTD | Resisc | Mean |'
  prefs: []
  type: TYPE_TB
- en: '| Top-1 (%) - ViT-B/32 | 54.5 | 73.0 | 51.0 | 61.3 | 60.0 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 8: Comparison of constructing the zero-shot classifier by ensembling
    the GPT MPVR prompts over the embedding or probability space.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the main manuscript (Table 3) we provide results by constructing the zero-shot
    classifier by ensembling the VLM prompts in two different ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Embedding Space:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The zero-shot classifier is constructed as the mean of the embeddings (from
    the text encoder of CLIP [[35](#bib.bib35)]) from the different sources (*e.g.,*
    Mixtral [[16](#bib.bib16)] or GPT [[2](#bib.bib2)]) VLM prompts).
  prefs: []
  type: TYPE_NORMAL
- en: 'Probability Space:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The zero-shot classifier is constructed as the mean of the probabilities (*e.g.,*
    from softmax) obtained by different VLM prompt sources (*e.g.,* Mixtral [[16](#bib.bib16)]
    or GPT [[2](#bib.bib2)]) for MPVR.
  prefs: []
  type: TYPE_NORMAL
- en: In Table 3 (main manuscript) we observed different behaviors (in terms of the
    obtained results) from these two sources of ensembling. In theory, an ensemble
    over the probability space can also be obtained for the individual category-specific
    VLM prompts (from stage 2) of the MPVR. However, for datasets with a larger number
    of classes (*e.g.,* ImageNet [[9](#bib.bib9)] with $1000$ classes), such an ensemble
    is prohibitively expensive (as also noted in [[35](#bib.bib35)]). Nevertheless,
    for completeness, in Table [8](#Pt0.A4.T8 "Table 8 ‣ Appendix 0.D Ensembling Descriptions
    ‣ Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs"), we provide
    results for the two ensembling methods for datasets with a smaller number of classes.
    From these results, we observe that the two different ensembling methods do not
    result in a huge deviation in performance. Note, to obtain all the MPVR results
    in all our experiments reported in the main paper, we always construct the zero-shot
    classifier as the mean of the embeddings from the VLM prompts for each category.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix 0.E Detailed Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| ViT-B/32 |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 45.9 | 40.2 | 87.6 | 59.0 | 88.1 | 72.6 | 40.8 | 54.6 | 86.2 |
    48.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 46.7 | 41.2 | 87.6 | 59.0 | 88.8 | 74.4 | 40.0 | 55.4 | 86.5
    | 47.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 64.9 | 57.4 | 89.8 | 66.0 | 92.8 | 76.0 | 59.2 | 55.8 | 88.6
    | 51.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 46.1 | 40.8 | 86.7 | 60.7 | 89.9 | 76.0 | 40.7 | 55.7
    | 87.9 | 49.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 80.1 | 20.9 | 42.3 | 66.1 | 62.8 | 36.6 | 58.8 | 29.3 | 56.6 |
    62.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 80.8 | 22.2 | 41.9 | 66.0 | 59.8 | 35.7 | 61.1 | 15.5 | 50.6
    | 61.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 81.1 | 22.3 | 42.7 | 67.1 | 67.1 | 43.4 | 70.3 | 44.0 | 56.4
    | 65.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 80.2 | 21.1 | 42.6 | 66.2 | 63.0 | 37.7 | 61.9 | 29.5
    | 55.3 | 63.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-B/16 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 49.9 | 44.8 | 84.6 | 63.7 | 89.2 | 76.1 | 45.8 | 58.6 | 87.6 |
    53.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 50.5 | 45.5 | 86.9 | 62.6 | 89.8 | 78.3 | 44.5 | 59.7 | 89.6
    | 50.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 69.7 | 63.4 | 91.2 | 69.6 | 94.4 | 79.2 | 65.6 | 59.8 | 90.7
    | 55.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 50.2 | 45.1 | 85.8 | 64.3 | 91.5 | 78.9 | 46.4 | 60.1
    | 90.1 | 53.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 85.8 | 27.8 | 43.1 | 67.9 | 67.0 | 40.8 | 64.2 | 36.0 | 58.5 |
    65.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 86.2 | 29.3 | 42.7 | 68.2 | 65.3 | 39.8 | 66.4 | 17.1 | 55.0
    | 63.8 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 86.5 | 28.3 | 43.6 | 68.8 | 70.1 | 48.0 | 78.4 | 50.6 | 60.2
    | 67.2 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 86.0 | 28.1 | 43.6 | 68.1 | 67.4 | 42.1 | 67.8 | 36.0
    | 59.1 | 66.1 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-L/14 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 62.2 | 57.3 | 90.1 | 75.5 | 94.3 | 81.9 | 58.9 | 64.8 | 92.9 |
    62.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 62.4 | 56.9 | 93.3 | 76.7 | 93.6 | 82.0 | 54.9 | 66.6 | 92.2
    | 60.4 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 76.9 | 71.0 | 96.2 | 79.4 | 95.5 | 83.9 | 78.1 | 67.3 | 93.8
    | 62.9 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 62.4 | 57.3 | 91.9 | 76.8 | 94.7 | 82.5 | 59.2 | 67.2
    | 93.2 | 62.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 91.2 | 33.1 | 43.4 | 72.6 | 73.2 | 50.7 | 81.4 | 49.5 | 67.2 |
    70.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 91.3 | 36.1 | 42.7 | 72.3 | 73.7 | 49.8 | 82.9 | 28.2 | 60.4
    | 69.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 91.6 | 34.3 | 43.8 | 72.9 | 77.4 | 55.5 | 88.6 | 61.2 | 65.6
    | 71.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 91.4 | 33.5 | 43.8 | 72.7 | 74.4 | 51.7 | 83.5 | 49.4
    | 65.4 | 70.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 9: Top-1 accuracy (%) while ensembling different text sources in the
    embedding space. Here, the zero-shot classifier is constructed by taking the mean
    of the embeddings from the different individual text sources.'
  prefs: []
  type: TYPE_NORMAL
- en: '| ViT-B/32 |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 64.8 | 57.2 | 89.9 | 66.3 | 92.7 | 73.7 | 59.4 | 55.8 | 88.3 |
    51.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 63.8 | 56.3 | 89.5 | 65.5 | 93.0 | 75.2 | 58.2 | 55.5 | 88.4
    | 50.3 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 62.9 | 55.9 | 89.7 | 66.1 | 92.8 | 76.1 | 52.4 | 55.9 | 88.8
    | 51.2 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 62.9 | 55.8 | 89.8 | 66.1 | 92.9 | 76.2 | 52.4 | 55.8
    | 89.0 | 51.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 81.1 | 21.9 | 42.1 | 67.0 | 68.0 | 43.7 | 70.1 | 44.2 | 55.0 |
    63.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 81.2 | 22.3 | 42.1 | 66.5 | 66.4 | 42.2 | 70.1 | 42.7 | 53.3
    | 64.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 79.2 | 22.2 | 42.6 | 66.2 | 67.2 | 43.4 | 70.3 | 43.8 | 56.4
    | 65.0 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 79.2 | 22.3 | 42.7 | 66.2 | 67.2 | 43.4 | 70.3 | 43.9
    | 56.1 | 65.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-B/16 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 69.8 | 63.2 | 90.8 | 69.7 | 94.0 | 76.8 | 65.4 | 58.8 | 89.8 |
    56.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 68.8 | 62.2 | 91.1 | 69.2 | 94.0 | 78.2 | 62.2 | 60.3 | 90.4
    | 54.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 67.7 | 61.6 | 91.1 | 69.7 | 94.4 | 79.4 | 57.0 | 60.1 | 91.0
    | 55.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 67.7 | 61.5 | 91.2 | 69.8 | 94.4 | 79.7 | 57.0 | 60.1
    | 91.1 | 55.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 86.4 | 27.5 | 43.0 | 68.9 | 70.7 | 48.0 | 78.3 | 50.7 | 59.1 |
    66.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 86.6 | 29.8 | 42.6 | 68.7 | 68.8 | 46.9 | 78.4 | 49.7 | 59.0
    | 66.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 84.7 | 28.1 | 43.5 | 68.4 | 70.0 | 48.0 | 78.4 | 50.8 | 60.3
    | 67.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 84.6 | 28.0 | 43.5 | 68.4 | 69.8 | 47.9 | 78.5 | 50.9
    | 60.0 | 67.0 |'
  prefs: []
  type: TYPE_TB
- en: '| ViT-L/14 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 76.8 | 70.9 | 95.8 | 79.1 | 96.2 | 83.7 | 78.4 | 65.2 | 93.6 |
    62.9 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 75.9 | 69.6 | 96.1 | 79.3 | 95.3 | 83.6 | 70.9 | 67.5 | 93.0
    | 61.6 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 75.2 | 69.7 | 96.2 | 79.5 | 95.8 | 84.4 | 65.7 | 67.3 | 93.9
    | 62.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 75.2 | 69.7 | 96.2 | 79.5 | 95.7 | 84.3 | 65.8 | 67.4
    | 93.9 | 62.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+TEMP | 91.5 | 34.3 | 43.5 | 72.9 | 77.9 | 55.7 | 88.5 | 61.1 | 67.6 |
    71.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MIXTRAL+TEMP | 91.4 | 37.5 | 42.5 | 72.4 | 75.9 | 54.6 | 88.6 | 60.0 | 61.9
    | 71.1 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL | 90.6 | 35.6 | 43.8 | 72.1 | 77.5 | 55.6 | 88.6 | 61.1 | 65.2
    | 71.7 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT+MIXTRAL+TEMP | 90.6 | 35.5 | 43.8 | 72.2 | 77.5 | 55.6 | 88.6 | 61.2
    | 65.1 | 71.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 10: Top-1 accuracy (%) while ensembling different text sources in the
    probability space. Here, the zero-shot classifier is constructed by taking the
    mean of the softmax probabilities from the different individual classifiers.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 66.7 | 60.9 | 90.1 | 68.4 | 93.3 | 67.5 | 65.5 | 55.1 | 88.2 | 43.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 68.3 | 61.9 | 90.8 | 68.2 | 92.9 | 70.7 | 66.2 | 56.1 | 89.1 |
    43.2 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 69.7 | 63.4 | 90.3 | 69.0 | 94.4 | 70.9 | 60.0 | 56.0 | 91.2 | 53.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 68.6 | 62.2 | 89.6 | 68.4 | 94.5 | 72.1 | 63.7 | 56.7 | 90.3 | 42.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 68.3 | 62.3 | 90.8 | 68.8 | 93.7 | 72.2 | 64.0 | 57.0 | 89.2 | 41.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 68.3 | 62.3 | 90.8 | 68.8 | 90.7 | 69.0 | 63.9 | 56.5 | 89.4
    | 42.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 68.3 | 62.3 | 90.8 | 68.8 | 94.4 | 72.3 | 63.8 | 56.8 |
    89.7 | 42.8 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 68.8 | 62.2 | 91.1 | 69.1 | 94.2 | 78.4 | 62.2 | 60.4 |
    90.3 | 53.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 69.7 | 63.4 | 90.8 | 69.5 | 94.1 | 76.9 | 65.4 | 59.0 | 89.9
    | 56.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 85.2 | 23.8 | 39.3 | 62.5 | 65.1 | 43.7 | 74.0 | 46.2 | 42.3 | 56.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 85.9 | 24.3 | 40.9 | 65.3 | 68.5 | 47.4 | 77.7 | 48.8 | 48.9 |
    60.1 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 86.1 | 26.6 | _ | 69.0 | 68.9 | 46.0 | _ | _ | _ | 66.2 |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 86.1 | 24.0 | 42.0 | 66.1 | 67.5 | 45.2 | 76.5 | 48.9 | 58.5 | 64.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 86.9 | 24.9 | 42.0 | 65.4 | 67.1 | 46.1 | 77.0 | 49.1 | 49.6 | 64.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 86.5 | 24.2 | 39.8 | 62.9 | 66.5 | 45.1 | 76.3 | 48.2 | 48.1
    | 61.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 86.7 | 24.9 | 42.4 | 66.4 | 68.4 | 46.0 | 77.0 | 49.5 |
    55.6 | 65.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 86.6 | 29.9 | 42.7 | 68.9 | 68.9 | 46.9 | 78.4 | 49.7 |
    59.2 | 66.7 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 86.4 | 28.0 | 43.1 | 68.8 | 70.9 | 48.0 | 78.2 | 50.6 | 59.6
    | 66.2 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/16
    backbone from OpenAI CLIP [[35](#bib.bib35)]. S-TEMP refers to the results obtained
    by using the default template (a photo of a ), while DS-TEMP refers
    to the results obtained by using the ensemble of dataset-specific prompts. An
    empty placeholder for CUPL [34] indicates that the respective baseline did not
    provide the handcrafted prompts for the dataset. For Waffle [[37](#bib.bib37)],
    mean results from 7 random runs are reported, following the original publication.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 73.5 | 67.8 | 95.2 | 77.2 | 94.3 | 76.2 | 76.9 | 62.1 | 93.1 | 52.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 75.5 | 69.9 | 95.7 | 78.3 | 93.7 | 79.5 | 78.1 | 61.8 | 93.5 |
    54.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 76.7 | 70.8 | 95.8 | 78.6 | 96.1 | 79.6 | 64.2 | 60.3 | 94.3 | 61.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 75.1 | 69.0 | 95.2 | 78.4 | 97.0 | 79.5 | 75.1 | 61.7 | 93.0 | 56.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 75.1 | 68.9 | 96.0 | 78.4 | 96.2 | 78.3 | 76.5 | 62.3 | 93.2 | 55.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 75.1 | 68.9 | 96.0 | 78.4 | 93.9 | 77.3 | 76.7 | 63.1 | 93.4
    | 53.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 75.1 | 68.9 | 96.0 | 78.4 | 96.9 | 79.0 | 75.9 | 62.0 |
    93.1 | 56.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 75.9 | 69.6 | 96.1 | 79.3 | 95.4 | 83.8 | 70.6 | 67.7 |
    93.1 | 61.6 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 76.8 | 70.9 | 96.0 | 79.2 | 96.1 | 83.6 | 78.3 | 65.5 | 93.7
    | 62.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 90.3 | 30.0 | 40.1 | 67.6 | 73.8 | 51.3 | 85.4 | 58.3 | 55.1 | 63.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 90.9 | 31.8 | 41.2 | 69.0 | 76.2 | 55.0 | 87.8 | 59.8 | 63.2 |
    68.0 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 91.4 | 35.1 | _ | 72.8 | 75.8 | 54.4 | _ | _ | _ | 71.8 |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 91.1 | 31.8 | 42.3 | 69.6 | 76.2 | 52.5 | 86.8 | 59.0 | 54.6 | 70.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 91.5 | 32.5 | 42.6 | 69.4 | 76.0 | 53.4 | 87.4 | 59.1 | 50.4 | 71.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 91.2 | 31.3 | 41.1 | 66.2 | 74.2 | 52.0 | 86.2 | 58.6 | 44.2
    | 66.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 91.4 | 32.1 | 42.9 | 70.1 | 76.4 | 53.5 | 87.3 | 59.3 |
    53.7 | 71.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 91.4 | 37.6 | 42.5 | 72.5 | 75.8 | 54.6 | 88.5 | 60.0 |
    62.2 | 71.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 91.5 | 34.4 | 43.5 | 73.0 | 78.1 | 55.7 | 88.4 | 61.0 | 67.3
    | 71.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 12: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-L/14
    backbone from OpenAI CLIP [[35](#bib.bib35)].'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 64.1 | 56.3 | 91.2 | 66.8 | 95.5 | 69.8 | 71.7 | 61.8 | 86.9 | 47.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 65.6 | 57.5 | 91.3 | 70.2 | 93.8 | 71.3 | 72.1 | 62.5 | 88.7 |
    51.8 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 66.0 | 57.5 | 90.3 | 68.4 | 95.5 | 68.3 | 61.3 | 61.5 | 88.5 | 58.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 64.0 | 55.0 | 90.9 | 68.4 | 94.9 | 67.6 | 66.6 | 62.1 | 87.9 | 50.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 63.5 | 55.5 | 90.9 | 67.2 | 93.9 | 69.7 | 68.8 | 61.7 | 88.6 | 50.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 63.5 | 55.5 | 90.9 | 67.2 | 88.2 | 68.6 | 69.1 | 61.8 | 88.7
    | 48.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 63.5 | 55.5 | 90.9 | 67.2 | 94.8 | 68.7 | 68.1 | 62.1 |
    88.1 | 51.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 64.8 | 57.4 | 91.2 | 68.9 | 94.3 | 78.4 | 68.3 | 65.2 |
    88.1 | 61.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 66.0 | 57.6 | 91.4 | 69.2 | 94.5 | 74.8 | 71.2 | 64.6 | 88.0
    | 61.5 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 76.7 | 24.3 | 39.6 | 64.8 | 64.4 | 36.9 | 71.5 | 52.3 | 49.4 | 56.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 77.3 | 26.9 | 40.1 | 65.3 | 66.1 | 39.1 | 74.8 | 53.9 | 50.4 |
    60.6 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 77.0 | 32.3 | _ | 67.7 | 64.2 | 39.3 | _ | _ | _ | 63.9 |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 76.7 | 25.3 | 42.0 | 64.3 | 65.6 | 37.6 | 73.2 | 52.3 | 49.0 | 62.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 77.2 | 26.0 | 42.1 | 65.8 | 64.1 | 38.1 | 73.9 | 52.9 | 42.3 | 64.4
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 77.1 | 25.4 | 41.4 | 66.0 | 63.5 | 37.4 | 72.8 | 52.8 | 37.8
    | 63.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 77.2 | 25.7 | 42.4 | 65.6 | 65.8 | 38.4 | 73.8 | 52.9 |
    46.7 | 63.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 77.0 | 35.4 | 41.4 | 67.3 | 65.4 | 39.9 | 75.7 | 53.1 |
    56.3 | 61.4 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 77.1 | 31.8 | 42.4 | 65.8 | 67.3 | 40.6 | 75.6 | 54.1 | 58.7
    | 63.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 13: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/32
    backbone from MetaCLIP [[47](#bib.bib47)].'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 70.0 | 61.8 | 89.9 | 64.9 | 95.7 | 71.7 | 74.7 | 69.5 | 88.5 | 53.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 70.8 | 62.6 | 90.1 | 66.5 | 95.6 | 73.8 | 75.8 | 69.7 | 90.5 |
    56.3 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 70.9 | 62.5 | 89.2 | 65.5 | 95.5 | 70.8 | 0.5 | 68.9 | 89.8 | 62.2
    |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 69.0 | 60.7 | 88.6 | 64.6 | 95.7 | 72.7 | 71.9 | 68.4 | 90.1 | 53.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 69.1 | 61.0 | 87.9 | 64.9 | 95.0 | 73.3 | 73.1 | 68.2 | 90.8 | 53.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 69.1 | 61.0 | 87.9 | 64.9 | 94.1 | 72.1 | 72.3 | 68.5 | 91.0
    | 52.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 69.1 | 61.0 | 87.9 | 64.9 | 95.8 | 72.9 | 73.0 | 68.1 |
    90.7 | 55.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 69.8 | 62.0 | 89.8 | 65.6 | 95.5 | 80.6 | 74.0 | 71.2 |
    90.4 | 64.1 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 71.2 | 62.9 | 89.8 | 66.6 | 94.8 | 75.9 | 75.9 | 71.4 | 89.9
    | 64.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 83.8 | 26.3 | 41.6 | 68.8 | 67.0 | 39.9 | 80.1 | 56.5 | 50.9 | 63.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 84.1 | 28.3 | 41.7 | 68.4 | 69.0 | 43.2 | 81.8 | 58.7 | 55.2 |
    63.9 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 84.0 | 34.4 | _ | 69.5 | 66.6 | 43.3 | _ | _ | _ | 67.0 |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 83.7 | 30.1 | 42.5 | 66.8 | 67.2 | 41.6 | 79.5 | 57.1 | 56.1 | 67.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 83.9 | 30.5 | 42.3 | 67.7 | 68.4 | 42.4 | 80.0 | 56.9 | 53.3 | 67.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 83.9 | 30.2 | 41.5 | 68.2 | 66.3 | 41.7 | 79.4 | 57.5 | 49.7
    | 67.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 84.0 | 30.4 | 42.8 | 68.0 | 68.5 | 42.4 | 80.1 | 57.2 |
    55.9 | 68.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 84.0 | 37.8 | 41.4 | 69.4 | 67.9 | 44.2 | 82.2 | 57.2 |
    59.7 | 67.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 83.6 | 34.0 | 43.0 | 69.4 | 68.8 | 44.9 | 82.1 | 58.2 | 57.8
    | 69.1 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 14: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/16
    backbone from MetaCLIP [[47](#bib.bib47)].'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ImageNet | ImageNetv2 | C10 | C100 | Caltech101 | Flowers | Stanford Cars
    | Cubs | Pets | DTD |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 75.1 | 68.5 | 94.9 | 74.4 | 96.8 | 76.7 | 84.5 | 76.0 | 88.7 | 58.8
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 76.2 | 69.9 | 95.7 | 77.4 | 96.3 | 77.4 | 84.9 | 75.2 | 93.7 |
    60.5 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 76.5 | 69.9 | 95.0 | 76.3 | 97.0 | 75.8 | 81.1 | 74.4 | 92.7 | 64.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 74.4 | 67.8 | 95.7 | 75.9 | 97.0 | 76.7 | 82.9 | 74.7 | 93.0 | 58.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 74.3 | 67.9 | 95.6 | 76.6 | 96.2 | 78.3 | 83.0 | 74.5 | 92.9 | 59.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 74.3 | 67.9 | 95.6 | 76.6 | 95.3 | 78.6 | 83.8 | 75.0 | 93.2
    | 57.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 74.3 | 67.9 | 95.6 | 76.6 | 97.4 | 77.5 | 83.2 | 74.5 |
    93.0 | 60.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 75.5 | 68.6 | 95.9 | 76.5 | 96.6 | 85.5 | 82.2 | 77.9 |
    92.6 | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 76.6 | 70.1 | 95.1 | 76.0 | 96.0 | 84.9 | 83.7 | 77.6 | 93.0
    | 65.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Food101 | Aircraft | Places365 | SUN397 | UCF101 | K400 | IN-R | IN-S
    | EuroSAT | Resisc45 |'
  prefs: []
  type: TYPE_TB
- en: '| S-TEMP | 88.6 | 35.6 | 42.2 | 72.1 | 75.2 | 48.6 | 87.7 | 63.9 | 49.7 | 61.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| DS-TEMP | 88.5 | 40.0 | 42.0 | 72.0 | 75.9 | 51.0 | 88.9 | 65.3 | 56.8 |
    69.1 |'
  prefs: []
  type: TYPE_TB
- en: '| CUPL | 89.0 | 41.2 | _ | 71.9 | 75.0 | 51.1 | _ | _ | _ | 71.2 |'
  prefs: []
  type: TYPE_TB
- en: '| D-CLIP | 88.4 | 39.5 | 43.5 | 71.1 | 75.9 | 49.5 | 87.7 | 64.2 | 61.3 | 67.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle | 88.7 | 39.0 | 43.3 | 71.7 | 75.9 | 49.8 | 87.5 | 64.0 | 59.6 | 69.5
    |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con | 88.9 | 38.8 | 41.4 | 71.4 | 75.1 | 49.2 | 87.2 | 64.1 | 59.3
    | 65.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Waffle+Con+GPT | 88.7 | 39.7 | 43.0 | 72.3 | 76.3 | 50.2 | 87.9 | 64.3 |
    61.3 | 69.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (Mixtral) | 89.1 | 49.5 | 40.2 | 73.1 | 74.8 | 51.8 | 89.4 | 65.1 |
    56.3 | 70.5 |'
  prefs: []
  type: TYPE_TB
- en: '| MPVR (GPT) | 88.8 | 46.7 | 43.8 | 72.5 | 77.5 | 52.3 | 89.2 | 65.5 | 58.0
    | 72.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 15: Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-L/14
    backbone from MetaCLIP [[47](#bib.bib47)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'For completeness, here we provide dataset-wise results for two experiments
    in the main manuscript: ensembling different text sources (Table 3) and mean results
    (over $20$ different backbones (ViT-B/32, ViT-B/16 and ViT-L/14) from MetaCLIP [[47](#bib.bib47)].
    The detailed (dataset-wise) results also highlight that our MPVR performs favorably
    on most datasets when compared to the state-of-the-art methods.'
  prefs: []
  type: TYPE_NORMAL
