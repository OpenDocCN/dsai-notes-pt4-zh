- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:50:34'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:50:34'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用语言多样化提示，通过发挥LLMs在英语中的主导能力来使低资源语言民主化
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.11372](https://ar5iv.labs.arxiv.org/html/2306.11372)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.11372](https://ar5iv.labs.arxiv.org/html/2306.11372)
- en: Xuan-Phi Nguyen¹ , Sharifah Mahani Aljunied¹ , Shafiq Joty² & Lidong Bing¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xuan-Phi Nguyen¹，Sharifah Mahani Aljunied¹，Shafiq Joty² 和 Lidong Bing¹
- en: ¹DAMO Academy, Alibaba Group
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹达摩学院，阿里巴巴集团
- en: '²Nanyang Technological University, Singapore Corresponding author: x.nguyen@alibaba-inc.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ²南洋理工大学，新加坡 通讯作者：x.nguyen@alibaba-inc.com
- en: Tóm tắt nội dung
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 内容摘要
- en: Large language models (LLMs) are known to effectively perform tasks by simply
    observing few exemplars. However, in low-resource languages, obtaining such hand-picked
    exemplars can still be challenging, where unsupervised techniques may be necessary.
    Moreover, competent generative capabilities of LLMs are observed only in high-resource
    languages, while their performances among under-represented languages fall behind
    due to pre-training data imbalance. To elicit LLMs’ ability onto low-resource
    languages without any supervised data, we propose to assemble synthetic exemplars
    from a diverse set of high-resource languages to prompt the LLMs to translate
    from any language into English. These prompts are then used to create intra-lingual
    exemplars to perform tasks in the target languages. Our unsupervised prompting
    method performs on par with supervised few-shot learning in LLMs of different
    sizes for translations between English and 13 Indic and 21 African low-resource
    languages. We also show that fine-tuning a 7B model on data generated from our
    method helps it perform competitively with a 175B model. In non-English translation
    tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many
    low-resource languages. When evaluated on zero-shot multilingual summarization,
    our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is
    also favored by GPT-4.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）通过观察少量示例有效地执行任务是众所周知的。然而，在低资源语言中，获取这些精心挑选的示例仍然具有挑战性，此时可能需要无监督技术。此外，LLMs的强大生成能力仅在高资源语言中得到体现，而在代表性不足的语言中的表现则因预训练数据的不平衡而滞后。为了在没有任何监督数据的情况下引发LLMs在低资源语言中的能力，我们建议从多样化的高资源语言中组装合成示例，以促使LLMs从任何语言翻译成英语。这些提示随后用于创建语言内示例，以执行目标语言中的任务。我们的方法在不同大小的LLMs中与监督的少量学习在英汉翻译以及13种印度语言和21种非洲低资源语言之间的翻译表现相当。我们还展示了，在我们的方式生成的数据上对7B模型进行微调，使其在性能上与175B模型竞争。在非英语翻译任务中，我们的方法甚至在许多低资源语言中比监督提示高出多达3
    chrF++。在零-shot多语言总结评估中，我们的方法超越了其他英语中介基线，ROUGE-L提高了多达4，并且也受到GPT-4的青睐。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent scaling effort in foundation large language models (Brown et al., [2020](#bib.bib3);
    Chowdhery et al., [2022](#bib.bib4); Scao et al., [2022](#bib.bib33); Touvron
    et al., [2023](#bib.bib39)) with massive pre-training data has enabled them to
    learn a broad range of natural language tasks through few-shot in-context learning
    - where a few input-output exemplars are shown as context prepended to the test
    input to prompt the model to predict the target answer with impressive qualities
    without any gradient update. While most LLMs were pre-trained with multilingual
    corpora in addition to the gigantic English corpus, and were shown to demonstrate
    impressive abilities in other languages (Brown et al., [2020](#bib.bib3); Chowdhery
    et al., [2022](#bib.bib4); Scao et al., [2022](#bib.bib33); Shi et al., [2022](#bib.bib35);
    Huang et al., [2023](#bib.bib17)), they only excel in high-resource languages,
    such as French. Further, they may still require pivoting the inputs into English,
    that is, performing tasks in English before reverting the response back to native
    outputs (Shi et al., [2022](#bib.bib35); Huang et al., [2023](#bib.bib17)). Improving
    LLMs abilities in extremely low-resource languages can be even more challenging,
    particularly where the data coverage is less than 0.0001% (Scao et al., [2022](#bib.bib33))
    or none at all (Touvron et al., [2023](#bib.bib39)). We also found that the models
    may confusely generate the wrong language and struggle to process low-resource
    non-latin scripts due to fragmented tokenization, where short texts are broken
    into extremely long byte-level tokens.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，在基础大规模语言模型的扩展工作中（Brown et al., [2020](#bib.bib3); Chowdhery et al., [2022](#bib.bib4);
    Scao et al., [2022](#bib.bib33); Touvron et al., [2023](#bib.bib39)），通过大量预训练数据，使其能够通过少量示例的上下文学习来掌握广泛的自然语言任务——即在测试输入前插入少量输入-输出示例作为上下文，以促使模型预测目标答案，而无需任何梯度更新。虽然大多数大型语言模型除了巨大的英语语料库外，还使用了多语言语料库，并且在其他语言中表现出令人印象深刻的能力（Brown
    et al., [2020](#bib.bib3); Chowdhery et al., [2022](#bib.bib4); Scao et al., [2022](#bib.bib33);
    Shi et al., [2022](#bib.bib35); Huang et al., [2023](#bib.bib17)），它们仅在高资源语言（如法语）中表现出色。此外，它们可能仍需将输入转换为英语，即在英语中执行任务，然后再将响应转换回本地输出（Shi
    et al., [2022](#bib.bib35); Huang et al., [2023](#bib.bib17)）。在极低资源语言中提升大型语言模型的能力可能更加具有挑战性，特别是在数据覆盖率低于
    0.0001%（Scao et al., [2022](#bib.bib33)）或完全没有数据（Touvron et al., [2023](#bib.bib39)）的情况下。我们还发现，由于分散的分词，模型可能会混淆生成错误的语言，并且难以处理低资源的非拉丁文字，这些短文本被拆分为极长的字节级标记。
- en: '$\mathcal{L}_{\rightarrow en}$English:
    Machine learningIgbo:Ịmụ
    igwe✓language, ✓translation'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '$\mathcal{L}_{\rightarrow en}$English:
    Machine learningIgbo:Ịmụ
    igwe✓language, ✓translation'
- en: 'Hình 1: (Left) $\mathcal{L}_{\rightarrow en}$.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：（左）$\mathcal{L}_{\rightarrow en}$。
- en: In this work, we focus on unsupervised, and zero-shot, generative translation
    and summarization tasks in low-resource languages, where no supervised few-shot
    prompts are used. We focus only on foundation multilingual LLMs (Scao et al.,
    [2022](#bib.bib33); Touvron et al., [2023](#bib.bib39)) to maximally avoid leakage
    of human-annotated data inherent in instruction-tuned models (Ouyang et al., [2022](#bib.bib29)).
    To this end, in recognition of LLMs’ dominant abilities in English and some evidence
    that in-context exemplars primarily help the model locate the task (Xie et al.,
    [2021](#bib.bib44)), we propose Linguistically-Diverse Prompting (LDP), a technique
    that promotes the models to locate the task of “translate any language $X$En exemplars
    from a diverse set of high-resource languages using off-the-shelf unsupervised
    MT models (Tran et al., [2020](#bib.bib40)). To ensure disversity, languages with
    script types ranging from Latin (Fr) to Arabic (Ar) and Chinese (Zh) characters
    are used. An example of the method is shown in [Figure 1](#S1.F1 "In 1 Introduction
    ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts").
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们专注于无监督的和零-shot的生成翻译和总结任务，针对低资源语言，不使用监督的少量示例提示。我们只关注基础多语言大型语言模型（Scao
    et al., [2022](#bib.bib33); Touvron et al., [2023](#bib.bib39)），以最大程度避免在指令调优模型中固有的人类注释数据泄露（Ouyang
    et al., [2022](#bib.bib29)）。为此，鉴于大型语言模型在英语中的主导能力以及在上下文示例主要帮助模型定位任务的一些证据（Xie et
    al., [2021](#bib.bib44)），我们提出了语言多样化提示（LDP）技术，该技术促使模型通过现成的无监督机器翻译模型（Tran et al.,
    [2020](#bib.bib40)）来定位“翻译任何语言 $X$En 示例”。为了确保多样性，我们使用了从拉丁文（Fr）到阿拉伯文（Ar）以及汉字（Zh）的不同脚本类型的语言。该方法的示例如
    [图 1](#S1.F1 "在 1 引言 ‣ 利用语言模型在低资源语言中的英语主导能力进行民主化") 所示。
- en: Our method is shown to translate any low-resource language into English with
    quality on par with supervised prompting, which allows us to build intra-lingual
    exemplars with unlabeled data to prompt the models to translate into low-resource
    languages. In our experiments with BLOOM (Scao et al., [2022](#bib.bib33)) and
    InstructGPT (text-davinci-003) (Ouyang et al., [2022](#bib.bib29)), our unsupervised
    LDP method performs on par with supervised prompting in X$\rightarrow$Y non-English
    directions even outperforms supervised promptings by up to 3 chrF++ in pairs involving
    low-resource languages. In multilingual summarization tasks (Narayan et al., [2018](#bib.bib25)),
    our zero-shot LDP method outperforms both basic prompting and other English-pivoting
    methods by up to 4 ROUGE-L and is generally favored by GPT-4-EVAL (Liu et al.,
    [2023](#bib.bib21)), which shows good human alignment.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法被证明能够将任何低资源语言翻译成英语，质量与监督提示相当，这使我们能够用未标记数据构建同语言示例，以提示模型翻译成低资源语言。在我们与 BLOOM（Scao
    等，[2022](#bib.bib33)）和 InstructGPT（text-davinci-003）（Ouyang 等，[2022](#bib.bib29)）的实验中，我们的无监督
    LDP 方法在 X$\rightarrow$Y 非英语方向上的表现与监督提示相当，甚至在涉及低资源语言的对中超越了监督提示，最多提高了 3 chrF++。在多语言摘要任务中（Narayan
    等，[2018](#bib.bib25)），我们零样本 LDP 方法超越了基本提示和其他英文枢纽方法，最多提高了 4 ROUGE-L，并且通常被 GPT-4-EVAL（Liu
    等，[2023](#bib.bib21)）所青睐，显示出良好的人工对齐。
- en: 2 Related Work
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Large language models (LLMs) display outstanding capabilities because they
    are massively large and are pre-trained on massive amounts of internet text data
    (Radford et al., [2019](#bib.bib32); Brown et al., [2020](#bib.bib3); Chowdhery
    et al., [2022](#bib.bib4); Scao et al., [2022](#bib.bib33); Touvron et al., [2023](#bib.bib39)).
    Without any gradient update, foundation LLMs are able to perform in-context few-shot
    learning by simply providing the models with a prompt comprising a list of high-quality
    input-output exemplars before appending the actual test input (Brown et al., [2020](#bib.bib3);
    Wei et al., [2023](#bib.bib42)). This technique works across a broad range of
    tasks, from natural language understanding to reasoning (Brown et al., [2020](#bib.bib3);
    Wei et al., [2022](#bib.bib41); Shi et al., [2022](#bib.bib35)). Much research
    have been done to understand in-context learning. Some suggest that the models
    secretly perform gradient descent on the exemplars (Dai et al., [2022](#bib.bib8)).
    Others demonstrate that most of the knowledge is learned during pre-training,
    and in-context exemplars $x$ are only to provide evidence for the model to marginalize
    and locate the intended task via a bayesian inference process as follows (Xie
    et al., [2021](#bib.bib44); Min et al., [2022](#bib.bib23); Zhou et al., [2023](#bib.bib46)):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）表现出卓越的能力，因为它们的规模庞大，并且在大量互联网文本数据上进行了预训练（Radford 等，[2019](#bib.bib32)；Brown
    等，[2020](#bib.bib3)；Chowdhery 等，[2022](#bib.bib4)；Scao 等，[2022](#bib.bib33)；Touvron
    等，[2023](#bib.bib39)）。在没有任何梯度更新的情况下，基础 LLMs 能够通过简单地提供一个包含高质量输入-输出示例的提示来进行上下文少样本学习，然后再附加实际测试输入（Brown
    等，[2020](#bib.bib3)；Wei 等，[2023](#bib.bib42)）。这种技术适用于从自然语言理解到推理的广泛任务（Brown 等，[2020](#bib.bib3)；Wei
    等，[2022](#bib.bib41)；Shi 等，[2022](#bib.bib35)）。已经进行了大量研究以理解上下文学习。有些研究认为模型在示例上秘密执行梯度下降（Dai
    等，[2022](#bib.bib8)）。其他研究则表明，大多数知识是在预训练期间学习的，上下文示例 $x$ 仅用于为模型提供证据，以通过以下贝叶斯推理过程边缘化和定位预期任务（Xie
    等，[2021](#bib.bib44)；Min 等，[2022](#bib.bib23)；Zhou 等，[2023](#bib.bib46)）：
- en: '|  | $p(y&#124;\text{x})=\int_{\text{task}}p(y&#124;\text{task,x})p(\text{task}&#124;\text{x})d(\text{task})$
    |  | (1) |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  | $p(y&#124;\text{x})=\int_{\text{task}}p(y&#124;\text{task,x})p(\text{task}&#124;\text{x})d(\text{task})$
    |  | (1) |'
- en: $10^{-4}$AfricanIndic
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: $10^{-4}$AfricanIndic
- en: 'Hình 2: Low-resource language coverage % of ROOTS corpus (Laurençon et al.,
    [2022](#bib.bib19)) used to train BLOOM (Scao et al., [2022](#bib.bib33)). The
    highest-resource language for Indic and African are Hindi and Swahili. Hindi accounts
    for $0.7$% of the corpus.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: ROOTS 语料库（Laurençon 等，[2022](#bib.bib19)）在训练BLOOM（Scao 等，[2022](#bib.bib33)）时用于低资源语言覆盖的百分比。对于印度语言和非洲语言，资源最丰富的分别是印地语和斯瓦希里语。印地语占语料库的
    $0.7$%。'
- en: Most large language models are trained with multilingual corpora (Wenzek et al.,
    [2020](#bib.bib43)), even if these make up a tiny fraction of the largely English
    corpora (Radford et al., [2019](#bib.bib32); Brown et al., [2020](#bib.bib3)).
    Despite that, LLMs still exhibit strong capabilities in multilingual setups with
    high-resource languages like De and Zh, often with the help of English-pivoting
    using supervised translation systems (Shi et al., [2022](#bib.bib35)) or prompting
    the model to firstly generate intermediate English text before arriving at the
    answer (Huang et al., [2023](#bib.bib17)). The most multilingual LLM is BLOOM
    (Scao et al., [2022](#bib.bib33)), which was trained on 46 languages in the ROOTS
    corpus (Laurençon et al., [2022](#bib.bib19)). This corpus includes 34 Indic and
    African languages regarded as low-resource, with each language having a pre-training
    coverage of less than 1% in Hindi for the Indic group, to $2e^{-5}$% in Tumbuka
    for the African group, as shown in [Figure 2](#S2.F2 "In 2 Related Work ‣ Democratizing
    LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities
    with Linguistically-Diverse Prompts"). Interestingly, some common high-resource
    languages, such as Russian, were not used in the training of BLOOM, making it
    a good subject to study unseen languages. Therefore, we use BLOOM as the main
    model to evaluate our methods and baselines in such 34 low-resource languages.
    Our linguistically-diverse prompting strategy is also an English-pivoting method,
    but it is different from other cross-lingual counterparts (Shi et al., [2022](#bib.bib35);
    Huang et al., [2023](#bib.bib17)) in that while others only pivot inputs to English
    intermediates, we use in-context pairs between English and a diverse set of high-resource
    languages to promote the intended task in the target language.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数大型语言模型使用多语言语料库进行训练（Wenzek et al., [2020](#bib.bib43)），尽管这些语料库在主要以英语为主的语料库中所占比例很小（Radford
    et al., [2019](#bib.bib32); Brown et al., [2020](#bib.bib3)）。尽管如此，LLMs在处理如德语和中文这样的高资源语言的多语言设置中仍表现出强大的能力，通常借助于使用监督翻译系统的英语枢纽法（Shi
    et al., [2022](#bib.bib35)）或让模型首先生成中间的英文文本再得出答案（Huang et al., [2023](#bib.bib17)）。最具多语言能力的LLM是BLOOM（Scao
    et al., [2022](#bib.bib33)），它在ROOTS语料库（Laurençon et al., [2022](#bib.bib19)）上进行了46种语言的训练。该语料库包括34种被视为低资源的印地语和非洲语言，每种语言在印地语组中预训练覆盖率不到1%，在非洲组中如Tumbuka的覆盖率为$2e^{-5}$%，如[图2](#S2.F2
    "在相关工作中 ‣ 利用其英语主导能力通过语言多样化提示来实现低资源语言的民主化")所示。有趣的是，一些常见的高资源语言，如俄语，并未用于BLOOM的训练，使其成为研究未见语言的良好对象。因此，我们使用BLOOM作为主要模型来评估我们的方法和基线在这34种低资源语言中的表现。我们的语言多样化提示策略也是一种英语枢纽法，但与其他跨语言方法（Shi
    et al., [2022](#bib.bib35); Huang et al., [2023](#bib.bib17)）不同的是，虽然其他方法仅将输入转换为英文中间文本，我们则使用英语与一系列高资源语言之间的上下文对，以促进目标语言中的预期任务。
- en: Our work also intersects with unsupervised multilingual machine translation
    (UMT), where iterative back-translation is proven to be effective (Sennrich et al.,
    [2016](#bib.bib34); Edunov et al., [2018](#bib.bib9); Lample et al., [2018](#bib.bib18);
    Conneau & Lample, [2019](#bib.bib5); Liu et al., [2020](#bib.bib22); Nguyen et al.,
    [2022b](#bib.bib27)), along with other techniques such as bi-text mining (Tran
    et al., [2020](#bib.bib40); Nguyen et al., [2022a](#bib.bib26)). English-pivoting
    is also prominent in the realm of machine translation, where training models on
    high-resource En$\leftrightarrow$Y tasks (Garcia et al., [2020](#bib.bib11); [2021](#bib.bib12)).
    Nonetheless, the noteworthy gap between existing UMT and LLMs is that their language
    coverages do not overlap much, preventing us from using UMT models to enhance
    LLMs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作还涉及无监督多语言机器翻译（UMT），其中迭代反向翻译被证明是有效的（Sennrich et al., [2016](#bib.bib34);
    Edunov et al., [2018](#bib.bib9); Lample et al., [2018](#bib.bib18); Conneau &
    Lample, [2019](#bib.bib5); Liu et al., [2020](#bib.bib22); Nguyen et al., [2022b](#bib.bib27)），以及其他技术如双语文本挖掘（Tran
    et al., [2020](#bib.bib40); Nguyen et al., [2022a](#bib.bib26)）。在机器翻译领域中，英语枢纽法也很突出，其中在高资源的
    En$\leftrightarrow$Y 任务上训练模型（Garcia et al., [2020](#bib.bib11); [2021](#bib.bib12)）。尽管如此，现有UMT和LLMs之间的显著差距在于它们的语言覆盖面不重叠，这阻碍了我们使用UMT模型来提升LLMs的能力。
- en: Analyses of machine translation using LLMs have also been done. Hendy et al.
    ([2023](#bib.bib15)) show that GPT models can perform competitively alongside
    state-of-the-art MT models. Zhu et al. ([2023](#bib.bib47)) focus on optimizing
    supervised exemplars selection and searching strategies. Sia & Duh ([2023](#bib.bib36))
    discover that using specific coherent prompts for each input helps improve performance.
    Nonetheless, such work only study supervised instruction-tuned models (Ouyang
    et al., [2022](#bib.bib29); Muennighoff et al., [2022](#bib.bib24)), which may
    risk test-set contamination. Thus, there is still limited research involving low-resource
    languages in completely zero-shot setups. As such, since low-resource languages
    may not enjoy the privilege of having large unlabeled data to conduct searching,
    only random selection is used in this study, while optimal exemplar selection
    is not within the current scope.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LLM的机器翻译分析也已经完成。Hendy et al. ([2023](#bib.bib15))表明GPT模型可以与最先进的MT模型竞争。Zhu
    et al. ([2023](#bib.bib47))专注于优化监督示例选择和搜索策略。Sia & Duh ([2023](#bib.bib36))发现对每个输入使用特定的连贯提示有助于提高性能。然而，这些工作只研究了监督指令调整模型（Ouyang
    et al., [2022](#bib.bib29); Muennighoff et al., [2022](#bib.bib24)），这可能会有测试集污染的风险。因此，涉及低资源语言的完全零样本设置的研究仍然有限。因此，由于低资源语言可能没有大规模未标记数据进行搜索，本研究中仅使用随机选择，而不在当前范围内进行最优示例选择。
- en: $\mathcal{L}^{mt}_{\text{x}\rightarrow\text{en}}$][x][en][y]
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L}^{mt}_{\text{x}\rightarrow\text{en}}$][x][en][y]
- en: (a) LDP for translation for $X$.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LDP用于$X$的翻译。
- en: $\mathcal{L}^{sum}_{\text{x}}$][d${}_{\text{x}}$]
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L}^{sum}_{\text{x}}$][d${}_{\text{x}}$]
- en: (b) LDP for summarization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LDP用于摘要。
- en: 'Hình 3: Compact illustrations of adopting LDP for $X$$Y$].'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Hình 3：采用LDP进行$X$$Y$的紧凑插图。
- en: 3 Method
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 3.1 Linguistically-Diverse Prompting (LDP)
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 语言多样性提示（LDP）
- en: Our linguistically-diverse prompting (LDP) method is inspired from three intuitive
    assumptions. (i) The first one, which has been theoretically and empirically supported,
    is that LLMs have already learned most of the knowledge and task concepts implicitly
    during pre-training, and that in-context exemplars play a larger role in providing
    evidence for the models to identify and marginalize over the probability of the
    intended task (Xie et al., [2021](#bib.bib44); Min et al., [2022](#bib.bib23);
    Zhou et al., [2023](#bib.bib46)).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的语言多样性提示（LDP）方法受到三种直观假设的启发。（i）第一种假设，已经得到理论和经验的支持，是LLM在预训练过程中已经隐含地学习了大部分知识和任务概念，在上下文示例中发挥了更大的作用，提供证据让模型识别和边际化目标任务的概率（Xie
    et al., [2021](#bib.bib44); Min et al., [2022](#bib.bib23); Zhou et al., [2023](#bib.bib46)）。
- en: (ii) The second assumption is that the models intuitively learn to perform language
    encoding and understanding at an earlier time, before learning to generate language.
    This means that, rather like human, the models may be able to comprehend any language
    with reasonable competency, and only struggle to generate the intended language.
    In other words, generative abilities are improved later on when more data is seen.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: （ii）第二种假设是模型直观地学习语言编码和理解的时间早于学习生成语言的时间。这意味着，与人类类似，模型可能能够以合理的能力理解任何语言，而只是难以生成预期的语言。换句话说，当看到更多数据时，生成能力会在后期得到提高。
- en: '(iii) The third assumption is that LLMs can already exhibit near-human generative
    abilities in the dominant language $E$ are no longer symmetric and can be interpreted
    more broadly as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: （iii）第三种假设是LLM在主导语言$E$中已经能够展示近似人类的生成能力，这种能力不再对称，可以更广泛地解释如下：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $X\rightarrow E$, it can effortlessly perform any NLU task.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $X\rightarrow E$，它可以轻松执行任何NLU任务。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $E\rightarrow X$ can involve different techniques.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $E\rightarrow X$可以涉及不同的技术。
- en: Combining the three assumptions, we design in-context exemplars so that the
    model locates the task of “translate from any language $X$ equivalents (*e.g.,*English),
    which can be translated using existing multilingual unsupervised MT models (Tran
    et al., [2020](#bib.bib40); Nguyen et al., [2022b](#bib.bib27)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这三种假设，我们设计上下文示例，使模型能够定位“从任何语言$X$翻译等效物（*例如*，英语），可以使用现有的多语言无监督MT模型进行翻译（Tran
    et al., [2020](#bib.bib40); Nguyen et al., [2022b](#bib.bib27)）。
- en: '[Figure 1](#S1.F1 "In 1 Introduction ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts") illustrates how LDP works. In the example on the left, we use synthetic
    pairs from diverse high-resource languages as in-context exemplars to prompt the
    models to translate the target low-resource language $X$ with much higher quality.
    This is because the target-side prompt distribution is now realistic and consistently
    close to the true target distribution we expect the model to generate, which has
    been shown to be crucial for in-context learning to work (Xie et al., [2021](#bib.bib44)).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1](#S1.F1 "在 1 介绍 ‣ 通过利用其英语主导能力和语言多样化提示来实现低资源语言的民主化") 说明了LDP的工作原理。在左侧的示例中，我们使用来自不同高资源语言的合成对作为上下文示例，提示模型以更高质量翻译目标低资源语言$X$。这是因为目标侧提示分布现在真实且始终接近我们期望模型生成的真实目标分布，这被证明对于上下文学习的有效性至关重要（Xie
    et al., [2021](#bib.bib44)）。'
- en: 3.2 LDP for Translation Tasks
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LDP在翻译任务中的应用
- en: We adopt LDP in translation tasks for $X\rightarrow E$ for better comprehensibility.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在$X\rightarrow E$的翻译任务中采用LDP，以提高可理解性。
- en: $X\rightarrow E$ task.
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $X\rightarrow E$任务。
- en: 'As mentioned above, we first gather $n$ by conditioning the LDP prompts as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，我们首先通过调整LDP提示来收集$n$：
- en: '|  | $\mathcal{L}^{mt}_{X\rightarrow E}(s_{X})\sim p_{\theta}(y&#124;s_{X},s_{Z_{1}},t^{\scriptstyle
    1}_{E},..,s_{Z_{n}},t^{\scriptstyle n}_{E})$ |  | (2) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}^{mt}_{X\rightarrow E}(s_{X})\sim p_{\theta}(y\mid s_{X},s_{Z_{1}},t^{\scriptstyle
    1}_{E},..,s_{Z_{n}},t^{\scriptstyle n}_{E})$ |  | (2) |'
- en: $E\rightarrow X$ task.
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $E\rightarrow X$任务。
- en: 'We leverage $\mathcal{L}^{mt}_{X\rightarrow E}$:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用$\mathcal{L}^{mt}_{X\rightarrow E}$：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: The intra-lingual exemplars with consistent language in the target side helps
    the model locate the intended language to generate more effectively than a standard
    language tag, as these exemplars show the model what the intended language looks
    like. In addition, we can also use $\mathcal{L}^{mtbt}$ LDP by using native language
    tags.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 目标语言中具有一致语言的内部语言示例帮助模型定位目标语言，从而比标准语言标签更有效地生成内容，因为这些示例展示了目标语言的样子。此外，我们还可以通过使用本国语言标签来使用$\mathcal{L}^{mtbt}$
    LDP。
- en: $X\rightarrow Y$ task.
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $X\rightarrow Y$任务。
- en: 'We leverage $\mathcal{L}^{mtbt}_{X\rightarrow E}$ is computed as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用$\mathcal{L}^{mtbt}_{X\rightarrow E}$计算如下：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: Unsupervised fine-tuning.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无监督微调。
- en: The ability to generate synthetic $X$ allows us to create larger-scale back-translation
    data from unlabeled corpora to fine-tune the LLM model for translation tasks without
    any in-context prompt at inference time. Specifically, we use a generic [input][output]
    template to construct multilingual training samples with the generated synthetic
    data pairs from multiple low-resource languages. During training, we only compute
    loss on the [output] part to train the model to generate the right language. While
    it may be tempting to use parameter-efficient fine-tuning (PEFT) approaches, such
    as LoRA (Hu et al., [2021](#bib.bib16)), we empirically found that the model fails
    to learn to generate the low-resource languages unless we increase the learnable
    parameter counts significantly, which seems to defeat the purpose of using PEFT.
    Instead, we propose to directly fine-tune the query-key-value linear weights of
    all attention layers, which account for 20-30% of the total parameters to avoid
    memory issues and overfitting.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 生成合成$X$的能力使我们能够从未标记的语料库中创建大规模的回译数据，以便在推理时无需任何上下文提示即可微调LLM模型。具体而言，我们使用通用的 [input][output]
    模板，通过来自多个低资源语言的合成数据对构造多语言训练样本。在训练过程中，我们仅计算[output]部分的损失，以训练模型生成正确的语言。虽然使用参数高效微调（PEFT）方法（如LoRA
    (Hu et al., [2021](#bib.bib16))）可能很诱人，但我们通过实验证明，除非我们显著增加可学习参数的数量，否则模型无法学习生成低资源语言，这似乎违背了使用PEFT的目的。因此，我们建议直接微调所有注意力层的查询-键-值线性权重，这些权重占总参数的20-30%，以避免内存问题和过拟合。
- en: 3.3 LDP for Multilingual Summarization
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LDP在多语言摘要中的应用
- en: 'For multilingual summarization tasks with instruction-tuned models (Ouyang
    et al., [2022](#bib.bib29)), we extend XLT (Huang et al., [2023](#bib.bib17)),
    a recent English-pivoting cross-lingual prompting technique, with document-summarization
    pairs from diverse high-resource non-English languages. This technique is illustrated
    in [Figure 3(b)](#S2.F3.sf2 "In Figure 3 ‣ 2 Related Work ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts"). Formally, given an input document $d_{X}$ as:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多语言摘要任务，使用了经过指令调优的模型（Ouyang 等，[2022](#bib.bib29)），我们扩展了 XLT（Huang 等，[2023](#bib.bib17)），这是一种最近的英文主导跨语言提示技术，结合了来自各种高资源非英语语言的文档摘要对。这项技术在[图
    3(b)](#S2.F3.sf2 "在图 3 ‣ 2 相关工作 ‣ 利用其英文主导能力与语言多样化提示使 LLMs 适用于低资源语言")中进行了说明。形式上，给定一个输入文档
    $d_{X}$ 如下：
- en: '|  | $\mathcal{L}_{X}^{sum}(d_{X})\sim p_{\theta}(y&#124;d_{X},d_{Z_{1}},s_{Z_{1}},..,d_{Z_{n}},s_{Z_{n}})$
    |  | (5) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{X}^{sum}(d_{X})\sim p_{\theta}(y&#124;d_{X},d_{Z_{1}},s_{Z_{1}},..,d_{Z_{n}},s_{Z_{n}})$
    |  | (5) |'
- en: 'Similar to $E\rightarrow X$ as:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 $E\rightarrow X$ 如下：
- en: '|  | $\hat{\mathcal{L}}_{X}^{sum}(d_{X})\sim p_{\theta}(y&#124;d_{X},d^{1}_{X},s^{1}_{X},..,d^{m}_{X},s^{m}_{X})$
    |  | (6) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathcal{L}}_{X}^{sum}(d_{X})\sim p_{\theta}(y&#124;d_{X},d^{1}_{X},s^{1}_{X},..,d^{m}_{X},s^{m}_{X})$
    |  | (6) |'
- en: 4 Experiments
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In this section, we evaluate our method in various translation and summarization
    tasks which include translation between English and low-resource languages ([4.1](#S4.SS1
    "4.1 Low-resource ↔ English Translation ‣ 4 Experiments ‣ Democratizing LLMs for
    Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts")), non-English-centric translation ([4.2](#S4.SS2 "4.2 Non-English-centric
    Translation ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages by
    Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")),
    higher-resource translation with LLaMA (Touvron et al., [2023](#bib.bib39)) ([4.3](#S4.SS3
    "4.3 Translation with LLaMA ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts")) and multilingual summarization ([4.4](#S4.SS4 "4.4 Zero-shot Multilingual
    Summarization ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")).
    We also conduct extensive analyses to provide further insight into our method
    ([4.5](#S4.SS5 "4.5 Ablation Study ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts")).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了我们的方法在各种翻译和摘要任务中的表现，包括英汉翻译（[4.1](#S4.SS1 "4.1 低资源 ↔ 英语翻译 ‣ 4 实验 ‣
    利用其英文主导能力与语言多样化提示使 LLMs 适用于低资源语言")）、非英语中心翻译（[4.2](#S4.SS2 "4.2 非英语中心翻译 ‣ 4 实验
    ‣ 利用其英文主导能力与语言多样化提示使 LLMs 适用于低资源语言")）、使用 LLaMA（Touvron 等，[2023](#bib.bib39)）的高资源翻译（[4.3](#S4.SS3
    "4.3 使用 LLaMA 的翻译 ‣ 4 实验 ‣ 利用其英文主导能力与语言多样化提示使 LLMs 适用于低资源语言")）以及多语言摘要（[4.4](#S4.SS4
    "4.4 零样本多语言摘要 ‣ 4 实验 ‣ 利用其英文主导能力与语言多样化提示使 LLMs 适用于低资源语言")）。我们还进行了广泛的分析，以提供对我们方法的进一步洞察（[4.5](#S4.SS5
    "4.5 消融研究 ‣ 4 实验 ‣ 利用其英文主导能力与语言多样化提示使 LLMs 适用于低资源语言")）。
- en: 'Bảng 1: Averaged performances of different prompting techniques across various
    model sizes and types, namely BLOOM (Scao et al., [2022](#bib.bib33)) and InstructGPT
    text-davinci-003 (Brown et al., [2020](#bib.bib3); Ouyang et al., [2022](#bib.bib29)),
    in translation tasks between English (En) and 13 Indic (Indic13) and 21 African
    (Afri21) low-resource languages present in the ROOTS corpus (Laurençon et al.,
    [2022](#bib.bib19)).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：不同提示技术在各种模型规模和类型下的平均表现，即 BLOOM（Scao 等，[2022](#bib.bib33)）和 InstructGPT text-davinci-003（Brown
    等，[2020](#bib.bib3)；Ouyang 等，[2022](#bib.bib29)），在英语（En）与 13 种印地语（Indic13）和 21
    种非洲语（Afri21）低资源语言之间的翻译任务中的表现，数据来源于 ROOTS 语料库（Laurençon 等，[2022](#bib.bib19)）。
- en: '|  | Indic13-En | En-Indic13 | Afri21-En | En-Afri21 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | Indic13-En | En-Indic13 | Afri21-En | En-Afri21 |'
- en: '|  | chrF++ | BLEU | chrF++ | BLEU | chrF++ | BLEU | chrF++ | BLEU |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | chrF++ | BLEU | chrF++ | BLEU | chrF++ | BLEU | chrF++ | BLEU |'
- en: '| Foundation BLOOM-175B |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 基础 BLOOM-175B |'
- en: '| Supervised-8-shot | 47.31 | 22.32 | 34.66 | 9.02 | 28.64 | 8.35 | 14.93 |
    2.00 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 监督-8-shot | 47.31 | 22.32 | 34.66 | 9.02 | 28.64 | 8.35 | 14.93 | 2.00 |'
- en: '| Unsupervised-LDP | 47.62 | 22.38 | 34.54 | 8.88 | 28.72 | 8.71 | 14.57 |
    1.89 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 无监督-LDP | 47.62 | 22.38 | 34.54 | 8.88 | 28.72 | 8.71 | 14.57 | 1.89 |'
- en: '| Foundation BLOOM-7B1 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 基础 BLOOM-7B1 |'
- en: '| Supervised-8-shot | 39.86 | 14.77 | 24.02 | 4.42 | 21.51 | 4.33 | 11.27 |
    0.59 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 监督-8-shot | 39.86 | 14.77 | 24.02 | 4.42 | 21.51 | 4.33 | 11.27 | 0.59 |'
- en: '| Unsupervised-LDP | 39.88 | 14.96 | 24.41 | 4.52 | 20.47 | 3.65 | 12.04 |
    0.62 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 无监督-LDP | 39.88 | 14.96 | 24.41 | 4.52 | 20.47 | 3.65 | 12.04 | 0.62 |'
- en: '| Fine-tune QKV (2B params) | 42.19 | 17.13 | 32.72 | 8.33 | 21.14 | 5.15 |
    15.73 | 2.13 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 微调 QKV (2B 参数) | 42.19 | 17.13 | 32.72 | 8.33 | 21.14 | 5.15 | 15.73 | 2.13
    |'
- en: '| Supervised RLHF InstructGPT (text-davinci-003) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 监督 RLHF InstructGPT (text-davinci-003) |'
- en: '| Zero-shot with instruction | 35.37 | 11.48 | 20.71 | 3.88 | 27.10 | 8.04
    | 15.45 | 1.13 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 带指令的零-shot | 35.37 | 11.48 | 20.71 | 3.88 | 27.10 | 8.04 | 15.45 | 1.13 |'
- en: '| Supervised-6-shot | 37.07 | 13.13 | 24.74 | 5.21 | 31.51 | 10.88 | 19.22
    | 2.66 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 监督-6-shot | 37.07 | 13.13 | 24.74 | 5.21 | 31.51 | 10.88 | 19.22 | 2.66 |'
- en: '| Unsupervised-LDP | 38.45 | 14.22 | 25.17 | 5.06 | 31.92 | 11.12 | 19.51 |
    2.61 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 无监督-LDP | 38.45 | 14.22 | 25.17 | 5.06 | 31.92 | 11.12 | 19.51 | 2.61 |'
- en: '| Supervised upperbound |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 监督上限 |'
- en: '| NLLB-200 distilled | 61.00 | 37.24 | 46.77 | 18.78 | 48.42 | 26.92 | 39.18
    | 12.95 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| NLLB-200 提炼 | 61.00 | 37.24 | 46.77 | 18.78 | 48.42 | 26.92 | 39.18 | 12.95
    |'
- en: 4.1 Low-resource $\leftrightarrow$ English Translation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 低资源 $\leftrightarrow$ 英语翻译
- en: 'As the ROOTS corpus (Laurençon et al., [2022](#bib.bib19)) that BLOOM (Scao
    et al., [2022](#bib.bib33)) was pre-trained on offers the most diverse language
    coverage with open-sourced transparency, we tested our methods mainly with the
    BLOOM model on 13 Indic (Indic13) languages and 21 African (Afri21) languages
    present in the ROOTS corpus. We also conduct experiments with supervised instruction-tuned
    InstructGPT (text-davinci-003) (Brown et al., [2020](#bib.bib3); Ouyang et al.,
    [2022](#bib.bib29)) to provide further references. We stress that, to our knowledge,
    it is not disclosed how large text-davinci-003 is or whether it was trained on
    the test sets. As such, its results are only to compare different prompting techniques
    within the InstructGPT section. For each of the 68 language pairs, we sample randomly
    and evaluate 200 sentences from each test set with the same seed to limit the
    cost budget for API calls¹¹1BLOOM: [huggingface.co/bigscience/bloom](https://huggingface.co/bigscience/bloom).
    GPT: [platform.openai.com](https://platform.openai.com). We perform full-set evaluations
    for 4 representative languages in each group and observe $[input]\n[output] as the prompt template, where
     and  are the language tag names (in English) of the respective languages.
    For our unsupervised linguistically-diverse prompting (LDP) method, we use 4 LDP
    $Z_{i}$ to generate synthetic training data from various unlabeled sources (Wenzek
    et al., [2020](#bib.bib43)) and fine-tune BLOOM-7B1 on the query-key-value weight
    matrices.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在方法论方面，对于监督提示，我们收集了尽可能多的监督对，以适应模型的上下文长度（BLOOM 为 8，GPT davinci-003 为 6）。我们使用
    [input]\n[output] 作为提示模板，其中  和  是各自语言的语言标签名称（用英语表示）。对于我们的无监督语言多样性提示（LDP）方法，我们使用
    4 个 LDP $Z_{i}$ 从各种未标记来源生成合成训练数据（Wenzek 等，[2020](#bib.bib43)），并在查询-键-值权重矩阵上对 BLOOM-7B1
    进行微调。
- en: 'Bảng 2: chrF++ translation scores for X to Y non-english directions across
    high-high, high-low and low-low languages groups.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '表2: X到Y的非英语方向的chrF++翻译分数，按高-高、高-低和低-低语言组分类。'
- en: '|  | High-High | High-Low | Low-Low |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 高-高 | 高-低 | 低-低 |'
- en: '|  | Vi-Fr | Fr-Vi | Zh-Ne | Ne-Zh | Es-Pa | Pa-Es | Ta-Sw | Sw-Ta | Te-Sw
    | Sw-Te |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | Vi-Fr | Fr-Vi | Zh-Ne | Ne-Zh | Es-Pa | Pa-Es | Ta-Sw | Sw-Ta | Te-Sw
    | Sw-Te |'
- en: '| Foundation BLOOM-175B |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 基础 BLOOM-175B |'
- en: '| Supervised-8-shot | 52.17 | 51.50 | 30.91 | 17.83 | 25.67 | 37.71 | 31.45
    | 31.81 | 31.46 | 25.84 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 监督-8-shot | 52.17 | 51.50 | 30.91 | 17.83 | 25.67 | 37.71 | 31.45 | 31.81
    | 31.46 | 25.84 |'
- en: '| Unsupervised-LDP | 52.66 | 50.24 | 31.61 | 18.34 | 27.85 | 39.51 | 34.61
    | 34.47 | 32.14 | 30.57 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 无监督-LDP | 52.66 | 50.24 | 31.61 | 18.34 | 27.85 | 39.51 | 34.61 | 34.47 |
    32.14 | 30.57 |'
- en: '| Supervised InstructGPT (text-davinci-003) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 监督 InstructGPT (text-davinci-003) |'
- en: '| XLT (Huang et al., [2023](#bib.bib17)) | 51.16 | 44.84 | 28.56 | 13.26 |
    23.61 | 34.18 | 24.20 | 25.46 | 24.89 | 23.48 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| XLT (Huang et al., [2023](#bib.bib17)) | 51.16 | 44.84 | 28.56 | 13.26 |
    23.61 | 34.18 | 24.20 | 25.46 | 24.89 | 23.48 |'
- en: '| Unsupervised-LDP | 51.19 | 45.80 | 28.67 | 15.80 | 25.40 | 35.02 | 27.24
    | 27.70 | 28.95 | 25.12 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 无监督-LDP | 51.19 | 45.80 | 28.67 | 15.80 | 25.40 | 35.02 | 27.24 | 27.70 |
    28.95 | 25.12 |'
- en: '[Table 1](#S4.T1 "In 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")
    shows the averaged chrF++ and BLEU scores for translations between English and
    13 Indic and 21 African low-resource languages across different prompting techniques
    with BLOOM-175B, BLOOM-7B1 and GPT text-davinci-003 models. The first noticeable
    finding is that our unsupervised-LDP method performs on par with supervised prompting
    across all language groups and LLM models. This indicates that the synthetic prompts
    generated by our $\mathcal{L}^{mt}_{X\rightarrow E}$ direction. This suggests
    that fine-tuning the model on more low-resource language data improves generative
    abilities in such languages. For GPT text-davinci-003, we observe the same pattern
    when comparing supervised and unsupervised-LDP. Further, it is interesting to
    see that GPT’s scores for Indic languages are lower than BLOOM but higher for
    African languages, despite the fact that the African languages are likely to have
    less data coverage. We suspect, with evidence in the Appendix, that this is because
    the GPT tokenizer favors Latin-based sub-words in the African languages more than
    the non-Latin characters of the Indic languages, as reflected by the high degree
    of sub-word fragmentation. For instance, a 10-token English text can be equivalent
    to 160 tokens in Tamil.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[表1](#S4.T1 "在4个实验中 ‣ 利用英语主导能力与语言多样的提示使低资源语言民主化") 展示了BLOOM-175B、BLOOM-7B1和GPT
    text-davinci-003模型在不同提示技术下，英语与13种印地语和21种非洲低资源语言之间的翻译平均 chrF++ 和 BLEU 分数。第一个明显的发现是，我们的无监督-LDP方法在所有语言组和LLM模型中都能与监督提示方法相媲美。这表明，我们的$\mathcal{L}^{mt}_{X\rightarrow
    E}$方向生成的合成提示对模型的影响表明，在更多低资源语言数据上微调模型可以提高这些语言的生成能力。对于GPT text-davinci-003，我们在比较监督和无监督-LDP时观察到相同的模式。此外，有趣的是，尽管非洲语言可能有更少的数据覆盖，但GPT在印地语语言的分数低于BLOOM，而在非洲语言中却更高。我们怀疑（附录中有证据表明）这是因为GPT分词器对非洲语言中的拉丁字符子词的偏好高于印地语言中的非拉丁字符，这从高程度的子词分裂中可以看出。例如，一个10-token的英语文本在泰米尔语中可能等于160个token。'
- en: 4.2 Non-English-centric Translation
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 非英语中心的翻译
- en: 'For non-English $X$ with supervised prompting in three categories: High-High
    resource languages with Vi and Fr, High-Low resource between Zh, Es, Ne (Nepali)
    and Pa (Punjabi), and Low-Low resource languages with Sw (Swahili), Ta (Tamil)
    and Te (Telugu). We use the same model and evaluation pipelines as explained [Section 4.1](#S4.SS1
    "4.1 Low-resource ↔ English Translation ‣ 4 Experiments ‣ Democratizing LLMs for
    Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts"). For this experiment, we evaluate on the devtest sets provided by Costa-jussà
    et al. ([2022](#bib.bib7)).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非英语 $X$，使用三类监督提示：高-高资源语言（如Vi和Fr），高-低资源语言（如Zh、Es、Ne（尼泊尔语）和Pa（旁遮普语）），以及低-低资源语言（如Sw（斯瓦希里语）、Ta（泰米尔语）和Te（泰卢固语））。我们使用与[第4.1节](#S4.SS1
    "4.1 低资源 ↔ 英语翻译 ‣ 4个实验 ‣ 利用英语主导能力与语言多样的提示使低资源语言民主化")中说明的相同模型和评估管道。对于这个实验，我们在Costa-jussà
    et al. ([2022](#bib.bib7))提供的开发测试集上进行评估。
- en: As reported in [Table 2](#S4.T2 "In 4.1 Low-resource ↔ English Translation ‣
    4 Experiments ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their
    English Dominant Abilities with Linguistically-Diverse Prompts"), our unsupervised
    LDP technique also performs on par with supervised prompting in High-High Vi-Fr
    pairs. More interestingly, for High-Low and Low-Low language pairs, our unsupervised
    method even outperforms supervised prompting for these languages by up to 5 chrF++,
    largely thanks to the presence of English intermediate translations in the exemplars.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表2](#S4.T2 "在4.1 低资源 ↔ 英语翻译 ‣ 4个实验 ‣ 通过利用其英语主导能力和语言多样性提示来民主化LLM")所示，我们的无监督LDP技术在高高Vi-Fr对中表现与监督提示相当。更有趣的是，对于高低和低低语言对，我们的无监督方法甚至比监督提示高出最多5个chrF++，这主要得益于样本中存在的英语中间翻译。
- en: 4.3 Translation with LLaMA
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 使用LLaMA的翻译
- en: 'Bảng 3: Comparison between supervised and unsupervised-LDP prompting with LLaMA-30B
    model in translation tasks between English (En) and 19 European languages (X).
    LDP prompts consist of exemplars from high-resource languages seen by CRISS.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：使用LLaMA-30B模型在英语（En）和19种欧洲语言（X）之间的翻译任务中，监督和无监督LDP提示的比较。LDP提示包括由CRISS看到的高资源语言示例。
- en: '| LLaMA-30B | X$\rightarrow$X |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30B | X$\rightarrow$X |'
- en: '| --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| chrF++ | BLEU | chrF++ | BLEU |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| chrF++ | BLEU | chrF++ | BLEU |'
- en: '| --- | --- | --- | --- |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Supervised | 61.80 | 39.51 | 53.65 | 28.98 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | 61.80 | 39.51 | 53.65 | 28.98 |'
- en: '| Unsupervised-LDP | 61.75 | 38.83 | 54.00 | 29.58 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 无监督-LDP | 61.75 | 38.83 | 54.00 | 29.58 |'
- en: LLaMA (Touvron et al., [2023](#bib.bib39)) is another open-sourced LLM that
    only supports 20 European high-resource languages. In this section, we evaluate
    LLaMA in translation tasks between English and the remaining 19 languages, which
    include Hungarian, Danish and Catalan. Specifically, we use CRISS to generate
    synthetic LDP exemplars from De, Es and Fr, which we then use to prompt LLaMA
    to translate from and to such languages. As reported in [Table 3](#S4.T3 "In 4.3
    Translation with LLaMA ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts"),
    we observe similar trends where our LDP method performs competitively with supervised
    prompting. The overall scores for such languages are also much higher than those
    of non-Latin languages because LLaMA was also pre-trained with bitexts, though
    without explicit alignments.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA（Touvron等，[2023](#bib.bib39)）是另一个开源LLM，仅支持20种欧洲高资源语言。在本节中，我们评估LLaMA在英语与其他19种语言（包括匈牙利语、丹麦语和加泰罗尼亚语）之间的翻译任务。具体来说，我们使用CRISS从德语、西班牙语和法语生成合成的LDP示例，然后用这些示例来提示LLaMA进行这种语言的翻译。如[表3](#S4.T3
    "在4.3 使用LLaMA的翻译 ‣ 4个实验 ‣ 通过利用其英语主导能力和语言多样性提示来民主化LLM")所示，我们观察到类似的趋势，我们的LDP方法在与监督提示竞争时表现良好。这些语言的总体得分也远高于非拉丁语言，因为LLaMA也用双语文本进行过预训练，但没有明确对齐。
- en: 4.4 Zero-shot Multilingual Summarization
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 零样本多语言总结
- en: 'Bảng 4: ROUGE-L / GPT-4-EVAL scores (1-5 ratings) of different prompting techniques
    using InstructGPT text-davinci-003 for zero-shot summarization in high-resource
    (Es, Vi, Id) and low-resource (Sw, So, Mr) in the Extreme-summarization (X-sum)
    task (Narayan et al., [2018](#bib.bib25)).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：不同提示技术使用InstructGPT text-davinci-003进行零样本总结的ROUGE-L / GPT-4-EVAL分数（1-5评分），包括高资源（Es、Vi、Id）和低资源（Sw、So、Mr）极端总结（X-sum）任务（Narayan等，[2018](#bib.bib25)）。
- en: '| davinci-003 | Es | Vi | Id | Sw | So | Mr |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| davinci-003 | Es | Vi | Id | Sw | So | Mr |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| Basic | 12.7/2.99 | 12.6/2.77 | 12.8/2.55 | 12.2/2.33 | 11.5/3.05 | 4.1/2.98
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 基本 | 12.7/2.99 | 12.6/2.77 | 12.8/2.55 | 12.2/2.33 | 11.5/3.05 | 4.1/2.98
    |'
- en: '| XLT | 17.7/3.90 | 14.8/3.76 | 17.6/3.40 | 20.5/3.11 | 18.5/3.96 | 10.3/3.84
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| XLT | 17.7/3.90 | 14.8/3.76 | 17.6/3.40 | 20.5/3.11 | 18.5/3.96 | 10.3/3.84
    |'
- en: '| LDP | 18.1/4.11 | 17.4/3.76 | 18.6/3.58 | 21.8/3.32 | 19.0/3.98 | 10.0/3.89
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LDP | 18.1/4.11 | 17.4/3.76 | 18.6/3.58 | 21.8/3.32 | 19.0/3.98 | 10.0/3.89
    |'
- en: '| LDP+Unlabeled | 18.1/4.16 | 17.0/3.82 | 24.8/3.82 | 23.5/3.25 | 19.3/4.00
    | 11.4/3.90 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| LDP+Unlabeled | 18.1/4.16 | 17.0/3.82 | 24.8/3.82 | 23.5/3.25 | 19.3/4.00
    | 11.4/3.90 |'
- en: We extend our LDP method to multilingual summarization task by combining LDP
    with cross-lingual prompting (XLT) (Huang et al., [2023](#bib.bib17)) using instruction-tuned
    GPT text-davinci-003 model. We follow the LDP adoptions for summarization with
    (LDP + Unlabeled or $\hat{\mathcal{L}}^{sum}$) unlabeled data, as described in
    [Section 3.3](#S3.SS3 "3.3 LDP for Multilingual Summarization ‣ 3 Method ‣ Democratizing
    LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities
    with Linguistically-Diverse Prompts"). We conduct evaluation on the Extreme Summarization
    benchmark (Narayan et al., [2018](#bib.bib25)) in both high-resource (Es, Vi,
    Id-Indonesian) and low-resource (Sw, So-Somali, Mr-Marathi) languages. To avoid
    exceeding the model context length, we sample 100 documents with less than 1500
    characters for each test set and obtain only 1 in-context exemplar via LDP. We
    evaluate the models with ROUGE-L (Lin, [2004](#bib.bib20)) and GPT-4-EVAL (Liu
    et al., [2023](#bib.bib21)), which is GPT-4 based metric that recently scores
    best in human judgement alignment. We compare our LDP and LDP+Unlabeled methods
    with XLT, and basic instruction. XLT is a recent English-pivoting instruction
    proposed by Huang et al. ([2023](#bib.bib17)). As shown in [Table 4](#S4.T4 "In
    4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts"), our LDP methods outperforms standard XLT across
    all languages by up to 7 ROUGE-L and exceeds basic prompting by large margins.
    Our methods are also consistently preferred by GPT-4-EVAL with higher ratings.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过结合LDP和跨语言提示（XLT）（Huang等，[2023](#bib.bib17)），使用指令调优的GPT text-davinci-003模型，将我们的LDP方法扩展到多语言摘要任务中。我们遵循[LDP总结的适应](#S3.SS3
    "3.3 LDP for Multilingual Summarization ‣ 3 Method ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts")，包括（LDP + 无标签或$\hat{\mathcal{L}}^{sum}$）无标签数据。我们在极端摘要基准（Narayan等，[2018](#bib.bib25)）上对高资源（Es,
    Vi, Id-Indonesian）和低资源（Sw, So-Somali, Mr-Marathi）语言进行评估。为了避免超出模型上下文长度，我们从每个测试集中抽取100个字符少于1500的文档，并通过LDP仅获取1个上下文示例。我们使用ROUGE-L（Lin，[2004](#bib.bib20)）和GPT-4-EVAL（Liu等，[2023](#bib.bib21)）评估模型，GPT-4-EVAL是基于GPT-4的度量，最近在人工判断对齐中得分最高。我们将LDP和LDP+无标签方法与XLT和基本指令进行比较。XLT是Huang等人（[2023](#bib.bib17)）提出的近期英语中心指令。如[表4](#S4.T4
    "In 4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts")所示，我们的LDP方法在所有语言中比标准XLT高出最多7个ROUGE-L，并且超越了基本提示。我们的方式在GPT-4-EVAL中也
    consistently preferred with higher ratings。
- en: $20$% pre-training data
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: $20$% pre-training data
- en: 'Hình 4: chrF++ scores for translation from English to each Indic and African
    language in the ROOTS corpus (En$\rightarrow$), using BLOOM. The right y-axis
    indicates corresponding pre-training coverage of each language at log scale.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：使用BLOOM从英语到每种印地语和非洲语言在ROOTS语料库（En$\rightarrow$）的chrF++分数。右侧y轴表示每种语言的预训练覆盖度（对数比例）。
- en: '![Refer to caption](img/29164fcfcdfae1d5cd9be71a5b861d41.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/29164fcfcdfae1d5cd9be71a5b861d41.png)'
- en: (a) LDP without back-translation $\mathcal{L}^{mt}_{\text{En}\rightarrow X}$.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 没有反向翻译的LDP $\mathcal{L}^{mt}_{\text{En}\rightarrow X}$。
- en: '![Refer to caption](img/d739498c493b9fc64b0cf8d7882d1772.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d739498c493b9fc64b0cf8d7882d1772.png)'
- en: (b) LDP with back-translation $\mathcal{L}^{mtbt}_{\text{En}\rightarrow X}$.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 具有反向翻译的LDP $\mathcal{L}^{mtbt}_{\text{En}\rightarrow X}$。
- en: 'Hình 5: Analysis on whether the BLOOM model generates the right language for
    En$\rightarrow$ task using LDP without ([5(a)](#S4.F5.sf1 "Figure 5(a) ‣ Figure
    5 ‣ 4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts")) and with ([5(b)](#S4.F5.sf2 "Figure 5(b) ‣ Figure
    5 ‣ 4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts")) intra-lingual back-translation prompts. Each
    column indicates the language the model generates into while each row represents
    the language it is supposed to generate. ## Indicate other languages.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：分析BLOOM模型在使用LDP时（[5(a)](#S4.F5.sf1 "Figure 5(a) ‣ Figure 5 ‣ 4.4 Zero-shot
    Multilingual Summarization ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts")）以及（[5(b)](#S4.F5.sf2 "Figure 5(b) ‣ Figure 5 ‣ 4.4 Zero-shot Multilingual
    Summarization ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")）是否生成正确语言的分析。每一列表示模型生成的语言，每一行表示模型应该生成的语言。##
    指出其他语言。
- en: 4.5 Ablation Study
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 消融研究
- en: In this section, we conduct various analysis experiments to provide a deeper
    understanding of our LDP method and the importance of each component.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们进行各种分析实验，以深入了解我们的 LDP 方法及其每个组件的重要性。
- en: Breakdown of Language Pairs.
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 语言对的细分。
- en: '[Figure 4](#S4.F4 "In 4.4 Zero-shot Multilingual Summarization ‣ 4 Experiments
    ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts") shows the breakdown of chrF++
    performances between supervised and unsupervised-LDP promptings for each of the
    34 low-resource languages. We observe that LDP performs generally on par with
    supervised prompting equally across all languages, and that it does not unevenly
    perform much worse or better in any particular language. More information on performance
    breakdown are shown in the Appendix.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4](#S4.F4 "在 4.4 零-shot 多语言摘要 ‣ 4 实验 ‣ 通过利用其英文主导能力与语言多样性提示实现低资源语言的民主化")显示了监督和无监督-LDP
    提示的 chrF++ 性能在34种低资源语言之间的细分。我们观察到 LDP 的性能通常与监督提示相当，在所有语言中表现均匀，并且在任何特定语言中不会表现得更差或更好。更多关于性能细分的信息请参见附录。'
- en: Generating the Right Language.
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成正确的语言。
- en: '[Figure 5(a)](#S4.F5.sf1 "In Figure 5 ‣ 4.4 Zero-shot Multilingual Summarization
    ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages by Leveraging
    their English Dominant Abilities with Linguistically-Diverse Prompts") reveals
    one reason the models struggle to translate En$\rightarrow$, is more important
    in getting the models to recognize language rather than the language tag. In fact,
    we found that removing the language tag entirely can help improve the performance
    slightly.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5(a)](#S4.F5.sf1 "图5 ‣ 4.4 零-shot 多语言摘要 ‣ 4 实验 ‣ 通过利用其英文主导能力与语言多样性提示实现低资源语言的民主化")揭示了模型在翻译
    En$\rightarrow$ 时挣扎的一个原因，即语言标签比语言本身更重要。事实上，我们发现完全移除语言标签可以稍微提高性能。'
- en: 'Bảng 5: Impact of English tag, native language tags and no language tag for
    in-context prompts in Indic languages with the BLOOM model.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：使用BLOOM模型时，英语标签、本地语言标签和无语言标签对印地语语言的上下文提示的影响。
- en: '| BLOOM | Indic13-En | En-Indic13 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | Indic13-En | En-Indic13 |'
- en: '| chrF++ | BLEU | chrF++ | BLEU |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| chrF++ | BLEU | chrF++ | BLEU |'
- en: '| Supervised |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 监督 |'
- en: '| En-tag | 47.31 | 22.32 | 34.66 | 9.02 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| En-tag | 47.31 | 22.32 | 34.66 | 9.02 |'
- en: '| Unsupervised LDP |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 无监督 LDP |'
- en: '| En-tag | 46.96 | 21.99 | 22.53 | 5.02 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| En-tag | 46.96 | 21.99 | 22.53 | 5.02 |'
- en: '| En-tag + BT | 47.43 | 22.30 | 34.41 | 8.89 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| En-tag + BT | 47.43 | 22.30 | 34.41 | 8.89 |'
- en: '| Native-tag | 46.90 | 21.82 | 29.80 | 7.02 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 本地标签 | 46.90 | 21.82 | 29.80 | 7.02 |'
- en: '| Native-tag + BT | 47.52 | 22.39 | 35.22 | 9.44 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 本地标签 + BT | 47.52 | 22.39 | 35.22 | 9.44 |'
- en: '| No-tag | 46.81 | 21.92 | – | – |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 无标签 | 46.81 | 21.92 | – | – |'
- en: '| No-tag + BT | 47.62 | 22.38 | 34.54 | 8.88 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 无标签 + BT | 47.62 | 22.38 | 34.54 | 8.88 |'
- en: Impact of Native Language Tag.
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本地语言标签的影响。
- en: The reason why we need unlabeled text to create intra-lingual prompts for En$\rightarrow$
    tasks significantly, compared to using English tags. This method even approaches
    the performance of 8-shot supervised prompting and LDP with unlabeled BT prompts.
    Combining it with back-translation data (Native-tag + BT) even helps it outperform
    supervised prompting. In effect, the English tag may confuse the model to an extent
    that not using the language tag at all (*e.g.,*using “Input:[input]\nOutput:[output]”)
    does not hurt the performances.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要无标签文本来创建 En$\rightarrow$ 任务的内部语言提示，与使用英语标签相比，显著原因。这种方法甚至接近 8-shot 监督提示和无标签
    BT 提示的性能。结合回译数据（本地标签 + BT）甚至帮助其超越监督提示。实际上，英语标签可能会使模型感到困惑，以至于完全不使用语言标签（*例如，*使用“输入：[input]\n输出：[output]”）不会影响性能。
- en: 'Bảng 6: Impact of different choices of LDP high-resource languages on $X$).
    Results are averages across 10 Indic languages excluding Ta, Bn and Hi (Indic10).
    Note that the LDP exemplars in this table are collected from supervised datasets
    for analysis purpose.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：LDP 高资源语言不同选择对 $X$ 的影响。结果是对 10 种印地语语言的平均值，不包括 Ta、Bn 和 Hi（Indic10）。请注意，此表中的
    LDP 示例是从监督数据集中收集的，供分析之用。
- en: '| BLOOM | Indic10-En | En-Indic10 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM | Indic10-En | En-Indic10 |'
- en: '| chrF++ | BLEU | chrF++ | BLEU |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| chrF++ | BLEU | chrF++ | BLEU |'
- en: '| Supervised | 46.32 | 21.63 | 32.44 | 7.66 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 监督 | 46.32 | 21.63 | 32.44 | 7.66 |'
- en: '| LDP (without BT) with different $\mathcal{Z}=$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| LDP（无 BT）与不同的 $\mathcal{Z}=$ |'
- en: '| Ar,Zh,Vi,Fr (default) | 45.53 | 20.90 | 17.65 | 3.13 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| Ar,Zh,Vi,Fr（默认） | 45.53 | 20.90 | 17.65 | 3.13 |'
- en: '| Hi,Hi,Hi,Hi (Hindi) | 43.27 | 18.02 | 15.34 | 1.72 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Hi,Hi,Hi,Hi（印地语） | 43.27 | 18.02 | 15.34 | 1.72 |'
- en: '| Ta,Bn,Hi (Indic) | 45.51 | 20.82 | 16.25 | 2.97 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 泰米尔语、孟加拉语、印地语（印地语） | 45.51 | 20.82 | 16.25 | 2.97 |'
- en: '| Fr,Es,Pt (European) | 45.31 | 20.52 | 18.98 | 3.22 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 法语、西班牙语、葡萄牙语（欧洲） | 45.31 | 20.52 | 18.98 | 3.22 |'
- en: '| Vi,Vi,Vi,Vi | 44.91 | 20.31 | 12.94 | 2.15 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 越南语、越南语、越南语、越南语 | 44.91 | 20.31 | 12.94 | 2.15 |'
- en: '| Zh,Zh,Zh,Zh | 44.71 | 20.41 | 15.78 | 2.88 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 中文、中文、中文、中文 | 44.71 | 20.41 | 15.78 | 2.88 |'
- en: '| Ar,Fr,Es,Pt,Vi,Zh,Id | 45.50 | 20.43 | 16.88 | 3.32 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 阿拉伯语、法语、西班牙语、葡萄牙语、越南语、中文、印尼语 | 45.50 | 20.43 | 16.88 | 3.32 |'
- en: Choice of LDP languages.
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LDP 语言选择。
- en: Another necessary question to ask is which high-resource languages should be
    selected as LDP exemplars. [Table 6](#S4.T6 "In Impact of Native Language Tag.
    ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages
    by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts")
    examines which LDP language choice is optimal. As shown, for 10 Indic low-resource
    languages, choosing a single related language (Hindi), which is often called cross-lingual
    prompting (Zhang et al., [2023](#bib.bib45); Zhu et al., [2023](#bib.bib47)),
    can be disastrous as the model tends to translate the prompt language rather than
    the test language. Choosing a single but distant language (Vi or Zh) yields better
    results, while choosing a wide variety of languages across different regions (*e.g.,* Ar,Zh,Vi,Fr)
    may be the optimal choice.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个必须考虑的问题是应该选择哪些高资源语言作为 LDP 示例。[表6](#S4.T6 "本地语言标签的影响。 ‣ 4.5 消融研究 ‣ 4 实验 ‣
    通过利用其英语主导能力和语言学多样的提示，推动低资源语言的大众化") 研究了哪种 LDP 语言选择是最佳的。如图所示，对于10种印地低资源语言，选择一种相关语言（印地语），即通常所说的跨语言提示（Zhang
    等，[2023](#bib.bib45)；Zhu 等，[2023](#bib.bib47)），可能会产生灾难性的结果，因为模型倾向于翻译提示语言而非测试语言。选择一种但较远的语言（越南语或中文）会产生更好的结果，而选择来自不同地区的多种语言（*例如*，阿拉伯语、中文、越南语、法语）可能是最佳选择。
- en: 'Bảng 7: chrF++ comparisons between unsupervised LDP prompting with BLOOM and
    unsupervised MT CRISS (Tran et al., [2020](#bib.bib40)) for En and Gu, Ne and
    Hi'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：BLOOM 的无监督 LDP 提示与无监督 MT CRISS（Tran 等，[2020](#bib.bib40)）在英文和古吉拉特语、尼泊尔语和印地语之间的
    chrF++ 比较
- en: '|  | Gu-En | Ne-En | Hi-En |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | Gu-En | Ne-En | Hi-En |'
- en: '|  | $\rightarrow$ |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | $\rightarrow$ |'
- en: '| CRISS | 41.88 | 32.41 | 37.64 | 28.17 | 51.23 | 42.29 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| CRISS | 41.88 | 32.41 | 37.64 | 28.17 | 51.23 | 42.29 |'
- en: '| BLOOM Prompting |  |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM 提示 |  |  |'
- en: '| Supervised | 51.63 | 38.23 | 47.07 | 35.91 | 55.18 | 44.94 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 有监督 | 51.63 | 38.23 | 47.07 | 35.91 | 55.18 | 44.94 |'
- en: '| Unsupervised-LDP | 50.09 | 37.63 | 48.26 | 35.76 | 55.71 | 45.36 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 无监督-LDP | 50.09 | 37.63 | 48.26 | 35.76 | 55.71 | 45.36 |'
- en: Comparison with Unsupervised MT.
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与无监督 MT 的比较。
- en: We also compare our method against the specialized unsupervised MT model CRISS
    (Tran et al., [2020](#bib.bib40)) on eligible languages (Gu, Ne, Hi). As shown
    in [Table 7](#S4.T7 "In Choice of LDP languages. ‣ 4.5 Ablation Study ‣ 4 Experiments
    ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts"), unsupervised LDP prompting with
    BLOOM significantly outperforms CRISS across all languages, thanks to its larger
    size and strong English abilities.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将我们的方法与专门的无监督 MT 模型 CRISS（Tran 等，[2020](#bib.bib40)）在符合条件的语言（古吉拉特语、尼泊尔语、印地语）上进行了比较。如[表7](#S4.T7
    "在 LDP 语言选择中。 ‣ 4.5 消融研究 ‣ 4 实验 ‣ 通过利用其英语主导能力和语言学多样的提示，推动低资源语言的大众化")所示，无监督 LDP
    提示与 BLOOM 相比，在所有语言上显著优于 CRISS，得益于其更大的规模和强大的英语能力。
- en: $3e^{7}$Indic13
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: $3e^{7}$Indic13
- en: 'Hình 6: Performance gains compared to LDP prompting by fine-tuning BLOOM-7B1
    using LoRA (Hu et al., [2021](#bib.bib16)) with various numbers of trainable parameters
    (x-axis) for Indic13$\rightarrow$Indic13 translation tasks.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：与通过 LoRA（Hu 等，[2021](#bib.bib16)）微调 BLOOM-7B1 相比的性能提升，x 轴为不同数量的可训练参数，任务为印地语13$\rightarrow$印地语13
    翻译。
- en: Fine-tuning Trainable Parameters.
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调可训练参数。
- en: '[Figure 6](#S4.F6 "In Comparison with Unsupervised MT. ‣ 4.5 Ablation Study
    ‣ 4 Experiments ‣ Democratizing LLMs for Low-Resource Languages by Leveraging
    their English Dominant Abilities with Linguistically-Diverse Prompts") analyzes
    how LoRA-fine-tuned BLOOM-7B1 models (Hu et al., [2021](#bib.bib16)) perform in
    $X$ task, suggesting that learning to generate an unfamiliar language needs much
    more parameters, rendering parameter-efficient methods ineffective.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6](#S4.F6 "与无监督机器翻译的比较。 ‣ 4.5 消融研究 ‣ 4 实验 ‣ 通过利用其英语主导能力和语言学多样的提示，推动低资源语言的大众化")
    分析了 LoRA 微调的 BLOOM-7B1 模型（Hu 等，[2021](#bib.bib16)）在 $X$ 任务中的表现，表明学习生成一种不熟悉的语言需要更多的参数，使得参数高效的方法效果不佳。'
- en: 5 Conclusion
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We introduce linguistically-diverse prompting (LDP), which is designed to use
    synthetic high-quality in-context exemplars from high-resource languages to prompt
    LLMs to perform generative tasks, such as translation and summarization, in low-resource
    languages. Our unsupervised approach achieves on par with supervised few-shot
    learning while using zero supervision in English to and from 34 low-resource Indic
    and African translation tasks, even outperforming supervised prompting in non-English-centric
    directions. Our method also outperforms other English-pivoting techniques in multilingual
    summarization.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了一种语言多样性的提示（LDP），旨在利用来自高资源语言的合成高质量上下文示例，促使大型语言模型（LLMs）在低资源语言中执行生成任务，如翻译和摘要。我们的方法在使用零监督的情况下，在
    34 个低资源印度语和非洲语言翻译任务中与监督的少量学习效果相当，甚至在非英语中心方向上超越了监督提示。我们的方法还在多语言摘要中优于其他英语中转技术。
- en: Tài liệu
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Tài liệu
- en: 'Adelani et al. (2022) David Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer,
    Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang,
    Tajuddeen Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin
    Leong, Michael Beukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre
    Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir,
    Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed Ahmed, Millicent
    Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore,
    Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne,
    Edwin Munkoh-Buabeng, Valencia Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy
    Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. A few thousand translations
    go a long way! leveraging pre-trained models for African news translation. In
    *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pp.  3053–3070, Seattle,
    United States, July 2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.223.
    URL [https://aclanthology.org/2022.naacl-main.223](https://aclanthology.org/2022.naacl-main.223).'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Adelani 等（2022）David Adelani、Jesujoba Alabi、Angela Fan、Julia Kreutzer、Xiaoyu
    Shen、Machel Reid、Dana Ruiter、Dietrich Klakow、Peter Nabende、Ernie Chang、Tajuddeen
    Gwadabe、Freshia Sackey、Bonaventure F. P. Dossou、Chris Emezue、Colin Leong、Michael
    Beukman、Shamsuddeen Muhammad、Guyo Jarso、Oreen Yousuf、Andre Niyongabo Rubungo、Gilles
    Hacheme、Eric Peter Wairagala、Muhammad Umair Nasir、Benjamin Ajibade、Tunde Ajayi、Yvonne
    Gitau、Jade Abbott、Mohamed Ahmed、Millicent Ochieng、Anuoluwapo Aremu、Perez Ogayo、Jonathan
    Mukiibi、Fatoumata Ouoba Kabore、Godson Kalipe、Derguene Mbaye、Allahsera Auguste
    Tapo、Victoire Memdjokam Koagne、Edwin Munkoh-Buabeng、Valencia Wagner、Idris Abdulmumin、Ayodele
    Awokoya、Happy Buzaaba、Blessing Sibanda、Andiswa Bukula 和 Sam Manthalu。几千个翻译产生了巨大影响！利用预训练模型进行非洲新闻翻译。收录于
    *2022 年北美计算语言学协会分会：人类语言技术会议论文集*，第 3053–3070 页，美国西雅图，2022 年 7 月。计算语言学协会。doi: 10.18653/v1/2022.naacl-main.223。网址
    [https://aclanthology.org/2022.naacl-main.223](https://aclanthology.org/2022.naacl-main.223)。'
- en: Barrault et al. (2020) Loïc Barrault, Magdalena Biesialska, Ondřej Bojar, Marta R.
    Costa-jussà, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow,
    Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljubešić,
    Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal,
    Matt Post, and Marcos Zampieri. Findings of the 2020 conference on machine translation
    (WMT20). In *Proceedings of the Fifth Conference on Machine Translation*, pp. 
    1–55, Online, November 2020\. Association for Computational Linguistics. URL [https://aclanthology.org/2020.wmt-1.1](https://aclanthology.org/2020.wmt-1.1).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barrault 等（2020）Loïc Barrault、Magdalena Biesialska、Ondřej Bojar、Marta R. Costa-jussà、Christian
    Federmann、Yvette Graham、Roman Grundkiewicz、Barry Haddow、Matthias Huck、Eric Joanis、Tom
    Kocmi、Philipp Koehn、Chi-kiu Lo、Nikola Ljubešić、Christof Monz、Makoto Morishita、Masaaki
    Nagata、Toshiaki Nakazawa、Santanu Pal、Matt Post 和 Marcos Zampieri。2020 年机器翻译会议（WMT20）的研究发现。收录于
    *第五届机器翻译会议论文集*，第 1–55 页，在线，2020 年 11 月。计算语言学协会。网址 [https://aclanthology.org/2020.wmt-1.1](https://aclanthology.org/2020.wmt-1.1)。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020）Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared D Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda Askell 等。语言模型是少样本学习者。*神经信息处理系统进展*，33:1877–1901，2020。
- en: 'Chowdhery et al. (2022) Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery 等（2022）Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
    Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann 等。Palm：通过路径扩展语言建模。*arXiv 预印本 arXiv:2204.02311*，2022年。
- en: Conneau & Lample (2019) Alexis Conneau and Guillaume Lample. Cross-lingual language
    model pretraining. In H. Wallach, H. Larochelle, A. Beygelzimer, F. dÁlché-Buc,
    E. Fox, and R. Garnett (eds.), *Advances in Neural Information Processing Systems
    32*, pp.  7059–7069\. Curran Associates, Inc., 2019.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Conneau 和 Lample（2019）Alexis Conneau 和 Guillaume Lample。跨语言语言模型预训练。在 H. Wallach,
    H. Larochelle, A. Beygelzimer, F. dÁlché-Buc, E. Fox 和 R. Garnett（编辑），*神经信息处理系统进展
    32*中，第7059–7069页。Curran Associates, Inc.，2019年。
- en: 'Conneau et al. (2020) Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav
    Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer,
    and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale.
    In *Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics*, pp.  8440–8451, Online, July 2020\. Association for Computational
    Linguistics. doi: 10.18653/v1/2020.acl-main.747. URL [https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conneau 等（2020）Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary,
    Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer
    和 Veselin Stoyanov。大规模无监督跨语言表示学习。在*第58届计算语言学协会年会论文集*中，第8440–8451页，线上，2020年7月。计算语言学协会。doi:
    10.18653/v1/2020.acl-main.747。网址 [https://aclanthology.org/2020.acl-main.747](https://aclanthology.org/2020.acl-main.747)。'
- en: 'Costa-jussà et al. (2022) Marta R Costa-jussà, James Cross, Onur Çelebi, Maha
    Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel
    Licht, Jean Maillard, et al. No language left behind: Scaling human-centered machine
    translation. *arXiv preprint arXiv:2207.04672*, 2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Costa-jussà 等（2022）Marta R Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad,
    Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean
    Maillard 等。语言无遗：扩展以人为本的机器翻译。*arXiv 预印本 arXiv:2207.04672*，2022年。
- en: Dai et al. (2022) Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui, and
    Furu Wei. Why can gpt learn in-context? language models secretly perform gradient
    descent as meta optimizers. *arXiv preprint arXiv:2212.10559*, 2022.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dai 等（2022）Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Zhifang Sui 和 Furu Wei。为什么
    GPT 能够进行上下文学习？语言模型实际上作为元优化器执行梯度下降。*arXiv 预印本 arXiv:2212.10559*，2022年。
- en: 'Edunov et al. (2018) Sergey Edunov, Myle Ott, Michael Auli, and David Grangier.
    Understanding back-translation at scale. In *Proceedings of the 2018 Conference
    on Empirical Methods in Natural Language Processing*, pp.  489–500, Brussels,
    Belgium, October-November 2018\. Association for Computational Linguistics. doi:
    10.18653/v1/D18-1045. URL [https://www.aclweb.org/anthology/D18-1045](https://www.aclweb.org/anthology/D18-1045).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Edunov 等（2018）Sergey Edunov, Myle Ott, Michael Auli 和 David Grangier。大规模理解反向翻译。在*2018年自然语言处理经验方法会议论文集*中，第489–500页，比利时布鲁塞尔，2018年10-11月。计算语言学协会。doi:
    10.18653/v1/D18-1045。网址 [https://www.aclweb.org/anthology/D18-1045](https://www.aclweb.org/anthology/D18-1045)'
- en: 'Emezue & Dossou (2021) Chris Chinenye Emezue and Bonaventure F. P. Dossou.
    MMTAfrica: Multilingual machine translation for African languages. In *Proceedings
    of the Sixth Conference on Machine Translation*, pp.  398–411, Online, November
    2021\. Association for Computational Linguistics. URL [https://aclanthology.org/2021.wmt-1.48](https://aclanthology.org/2021.wmt-1.48).'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Emezue 和 Dossou（2021）Chris Chinenye Emezue 和 Bonaventure F. P. Dossou。MMTAfrica：面向非洲语言的多语言机器翻译。在*第六届机器翻译会议论文集*中，第398–411页，线上，2021年11月。计算语言学协会。网址
    [https://aclanthology.org/2021.wmt-1.48](https://aclanthology.org/2021.wmt-1.48)。
- en: 'Garcia et al. (2020) Xavier Garcia, Pierre Foret, Thibault Sellam, and Ankur
    Parikh. A multilingual view of unsupervised machine translation. In *Findings
    of the Association for Computational Linguistics: EMNLP 2020*, pp.  3160–3170,
    Online, November 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.283.
    URL [https://aclanthology.org/2020.findings-emnlp.283](https://aclanthology.org/2020.findings-emnlp.283).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Garcia 等（2020）Xavier Garcia, Pierre Foret, Thibault Sellam 和 Ankur Parikh。无监督机器翻译的多语言视角。在*计算语言学协会发现：EMNLP
    2020*中，第3160–3170页，线上，2020年11月。计算语言学协会。doi: 10.18653/v1/2020.findings-emnlp.283。网址
    [https://aclanthology.org/2020.findings-emnlp.283](https://aclanthology.org/2020.findings-emnlp.283)。'
- en: 'Garcia et al. (2021) Xavier Garcia, Aditya Siddhant, Orhan Firat, and Ankur
    Parikh. Harnessing multilinguality in unsupervised machine translation for rare
    languages. In *Proceedings of the 2021 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies*,
    pp.  1126–1137, Online, June 2021\. Association for Computational Linguistics.
    doi: 10.18653/v1/2021.naacl-main.89. URL [https://aclanthology.org/2021.naacl-main.89](https://aclanthology.org/2021.naacl-main.89).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Garcia 等人 (2021) Xavier Garcia, Aditya Siddhant, Orhan Firat 和 Ankur Parikh。利用无监督机器翻译中的多语言性处理稀有语言。在*2021
    年北美计算语言学协会：人类语言技术会议论文集*，第 1126–1137 页，在线，2021 年 6 月。计算语言学协会。doi: 10.18653/v1/2021.naacl-main.89。网址
    [https://aclanthology.org/2021.naacl-main.89](https://aclanthology.org/2021.naacl-main.89)。'
- en: 'Goyal et al. (2022) Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen,
    Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán,
    and Angela Fan. The Flores-101 evaluation benchmark for low-resource and multilingual
    machine translation. *Transactions of the Association for Computational Linguistics*,
    10:522–538, 2022. doi: 10.1162/tacl˙a˙00474. URL [https://aclanthology.org/2022.tacl-1.30](https://aclanthology.org/2022.tacl-1.30).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Goyal 等人 (2022) Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen,
    Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán
    和 Angela Fan。Flores-101 低资源和多语言机器翻译评估基准。*计算语言学协会会刊*，10:522–538, 2022。doi: 10.1162/tacl˙a˙00474。网址
    [https://aclanthology.org/2022.tacl-1.30](https://aclanthology.org/2022.tacl-1.30)。'
- en: 'Guzmán et al. (2019) Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino,
    Guillaume Lample, Philipp Koehn, Vishrav Chaudhary, and Marc’Aurelio Ranzato.
    The FLORES evaluation datasets for low-resource machine translation: Nepali–English
    and Sinhala–English. In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP)*, pp.  6098–6111, Hong Kong, China, November
    2019\. Association for Computational Linguistics. doi: 10.18653/v1/D19-1632. URL
    [https://www.aclweb.org/anthology/D19-1632](https://www.aclweb.org/anthology/D19-1632).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guzmán 等人 (2019) Francisco Guzmán, Peng-Jen Chen, Myle Ott, Juan Pino, Guillaume
    Lample, Philipp Koehn, Vishrav Chaudhary 和 Marc’Aurelio Ranzato。FLORES 低资源机器翻译评估数据集：尼泊尔语–英语和僧伽罗语–英语。在*2019
    年自然语言处理经验方法会议及第 9 届国际自然语言处理联合会议 (EMNLP-IJCNLP) 论文集*，第 6098–6111 页，中国香港，2019 年
    11 月。计算语言学协会。doi: 10.18653/v1/D19-1632。网址 [https://www.aclweb.org/anthology/D19-1632](https://www.aclweb.org/anthology/D19-1632)。'
- en: Hendy et al. (2023) Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak,
    Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan
    Awadalla. How good are gpt models at machine translation? a comprehensive evaluation.
    *arXiv preprint arXiv:2302.09210*, 2023.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendy 等人 (2023) Amr Hendy, Mohamed Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed
    Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify 和 Hany Hassan Awadalla。GPT
    模型在机器翻译中的表现如何？一项综合评估。*arXiv 预印本 arXiv:2302.09210*，2023。
- en: 'Hu et al. (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 (2021) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang 和 Weizhu Chen。Lora：大语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685*，2021。
- en: 'Huang et al. (2023) Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao,
    Ting Song, Yan Xia, and Furu Wei. Not all languages are created equal in llms:
    Improving multilingual capability by cross-lingual-thought prompting. *arXiv preprint
    arXiv:2305.07004*, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 (2023) Haoyang Huang, Tianyi Tang, Dongdong Zhang, Wayne Xin Zhao,
    Ting Song, Yan Xia 和 Furu Wei。不是所有语言在大语言模型中都一样：通过跨语言思维提示提升多语言能力。*arXiv 预印本 arXiv:2305.07004*，2023。
- en: 'Lample et al. (2018) Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer,
    and Marc’Aurelio Ranzato. Phrase-based & neural unsupervised machine translation.
    In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language
    Processing*, pp.  5039–5049, Brussels, Belgium, October-November 2018\. Association
    for Computational Linguistics. doi: 10.18653/v1/D18-1549. URL [https://www.aclweb.org/anthology/D18-1549](https://www.aclweb.org/anthology/D18-1549).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lample 等人 (2018) Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer
    和 Marc’Aurelio Ranzato。基于短语和神经的无监督机器翻译。在*2018 年自然语言处理经验方法会议论文集*，第 5039–5049 页，比利时布鲁塞尔，2018
    年 10 月-11 月。计算语言学协会。doi: 10.18653/v1/D18-1549。网址 [https://www.aclweb.org/anthology/D18-1549](https://www.aclweb.org/anthology/D18-1549)。'
- en: 'Laurençon et al. (2022) Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher
    Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao
    Mou, Eduardo González Ponferrada, Huu Nguyen, Jörg Frohberg, Mario Šaško, Quentin
    Lhoest, Angelina McMillan-Major, Gérard Dupont, Stella Biderman, Anna Rogers,
    Loubna Ben allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor,
    Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush,
    Shayne Longpre, Sebastian Nagel, Leon Weber, Manuel Romero Muñoz, Jian Zhu, Daniel Van
    Strien, Zaid Alyafeai, Khalid Almubarak, Vu Minh Chien, Itziar Gonzalez-Dios,
    Aitor Soroa, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose,
    David Ifeoluwa Adelani, Long Phan, Hieu Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette
    Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and Yacine Jernite. The
    bigscience ROOTS corpus: A 1.6TB composite multilingual dataset. In *Thirty-sixth
    Conference on Neural Information Processing Systems Datasets and Benchmarks Track*,
    2022. URL [https://openreview.net/forum?id=UoEw6KigkUn](https://openreview.net/forum?id=UoEw6KigkUn).'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Laurençon 等（2022）**Hugo Laurençon**、**Lucile Saulnier**、**Thomas Wang**、**Christopher
    Akiki**、**Albert Villanova del Moral**、**Teven Le Scao**、**Leandro Von Werra**、**Chenghao
    Mou**、**Eduardo González Ponferrada**、**Huu Nguyen**、**Jörg Frohberg**、**Mario
    Šaško**、**Quentin Lhoest**、**Angelina McMillan-Major**、**Gérard Dupont**、**Stella
    Biderman**、**Anna Rogers**、**Loubna Ben allal**、**Francesco De Toni**、**Giada
    Pistilli**、**Olivier Nguyen**、**Somaieh Nikpoor**、**Maraim Masoud**、**Pierre Colombo**、**Javier
    de la Rosa**、**Paulo Villegas**、**Tristan Thrush**、**Shayne Longpre**、**Sebastian
    Nagel**、**Leon Weber**、**Manuel Romero Muñoz**、**Jian Zhu**、**Daniel Van Strien**、**Zaid
    Alyafeai**、**Khalid Almubarak**、**Vu Minh Chien**、**Itziar Gonzalez-Dios**、**Aitor
    Soroa**、**Kyle Lo**、**Manan Dey**、**Pedro Ortiz Suarez**、**Aaron Gokaslan**、**Shamik
    Bose**、**David Ifeoluwa Adelani**、**Long Phan**、**Hieu Tran**、**Ian Yu**、**Suhas
    Pai**、**Jenny Chim**、**Violette Lepercq**、**Suzana Ilic**、**Margaret Mitchell**、**Sasha
    Luccioni** 和 **Yacine Jernite**。Bigscience ROOTS 语料库：一个 1.6TB 复合型多语言数据集。在*第36届神经信息处理系统大会数据集与基准追踪*，2022。网址
    [https://openreview.net/forum?id=UoEw6KigkUn](https://openreview.net/forum?id=UoEw6KigkUn)。
- en: 'Lin (2004) Chin-Yew Lin. Rouge: A package for automatic evaluation of summaries.
    In *Text summarization branches out*, pp.  74–81, 2004.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin（2004）**Chin-Yew Lin**。Rouge：一个用于自动摘要评估的工具包。在*文本摘要的新发展*，第74–81页，2004。
- en: 'Liu et al. (2023) Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu,
    and Chenguang Zhu. G-eval: Nlg evaluation using gpt-4 with better human alignment.
    *arXiv 2303.16634*, March 2023. URL [https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/](https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2023）**Yang Liu**、**Dan Iter**、**Yichong Xu**、**Shuohang Wang**、**Ruochen
    Xu** 和 **Chenguang Zhu**。G-eval：使用 GPT-4 进行自然语言生成评估，以实现更好的人工对齐。*arXiv 2303.16634*，2023年3月。网址
    [https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/](https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/)。
- en: Liu et al. (2020) Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov,
    Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising
    pre-training for neural machine translation. *arXiv preprint arXiv:2001.08210*,
    2020.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等（2020）**Yinhan Liu**、**Jiatao Gu**、**Naman Goyal**、**Xian Li**、**Sergey
    Edunov**、**Marjan Ghazvininejad**、**Mike Lewis** 和 **Luke Zettlemoyer**。用于神经机器翻译的多语言去噪预训练。*arXiv
    预印本 arXiv:2001.08210*，2020。
- en: 'Min et al. (2022) Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis,
    Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations:
    What makes in-context learning work? *arXiv preprint arXiv:2202.12837*, 2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 等（2022）**Sewon Min**、**Xinxi Lyu**、**Ari Holtzman**、**Mikel Artetxe**、**Mike
    Lewis**、**Hannaneh Hajishirzi** 和 **Luke Zettlemoyer**。重新思考示范的角色：是什么使得上下文学习有效？*arXiv
    预印本 arXiv:2202.12837*，2022。
- en: Muennighoff et al. (2022) Niklas Muennighoff, Thomas Wang, Lintang Sutawika,
    Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin
    Yong, Hailey Schoelkopf, et al. Crosslingual generalization through multitask
    finetuning. *arXiv preprint arXiv:2211.01786*, 2022.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Muennighoff 等（2022）**Niklas Muennighoff**、**Thomas Wang**、**Lintang Sutawika**、**Adam
    Roberts**、**Stella Biderman**、**Teven Le Scao**、**M Saiful Bari**、**Sheng Shen**、**Zheng-Xin
    Yong**、**Hailey Schoelkopf** 等。通过多任务微调实现跨语言泛化。*arXiv 预印本 arXiv:2211.01786*，2022。
- en: Narayan et al. (2018) Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Don’t
    give me the details, just the summary! topic-aware convolutional neural networks
    for extreme summarization. *ArXiv*, abs/1808.08745, 2018.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Narayan 等（2018）**Shashi Narayan**、**Shay B. Cohen** 和 **Mirella Lapata**。别给我细节，只要总结！用于极端摘要的主题感知卷积神经网络。*ArXiv*，abs/1808.08745，2018。
- en: Nguyen et al. (2022a) Xuan-Phi Nguyen, Hongyu Gong, Yun Tang, Changhan Wang,
    Philipp Koehn, and Shafiq Joty. Contrastive clustering to mine pseudo parallel
    data for unsupervised translation. In *International Conference on Learning Representations*,
    2022a. URL [https://openreview.net/forum?id=pN1JOdrSY9](https://openreview.net/forum?id=pN1JOdrSY9).
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2022a) Xuan-Phi Nguyen, Hongyu Gong, Yun Tang, Changhan Wang,
    Philipp Koehn 和 Shafiq Joty。对比聚类挖掘伪平行数据以进行无监督翻译。载于*国际学习表征会议*，2022a。网址 [https://openreview.net/forum?id=pN1JOdrSY9](https://openreview.net/forum?id=pN1JOdrSY9)。
- en: Nguyen et al. (2022b) Xuan-Phi Nguyen, Shafiq Joty, Kui Wu, and AiTi Aw. Refining
    low-resource unsupervised translation by language disentanglement of multilingual
    translation model. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun
    Cho (eds.), *Advances in Neural Information Processing Systems*, 2022b. URL [https://openreview.net/forum?id=eCUeRHHupF](https://openreview.net/forum?id=eCUeRHHupF).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen et al. (2022b) Xuan-Phi Nguyen, Shafiq Joty, Kui Wu 和 AiTi Aw。通过多语言翻译模型的语言解缠结来优化低资源无监督翻译。载于
    Alice H. Oh, Alekh Agarwal, Danielle Belgrave 和 Kyunghyun Cho (编辑)，*神经信息处理系统进展*，2022b。网址
    [https://openreview.net/forum?id=eCUeRHHupF](https://openreview.net/forum?id=eCUeRHHupF)。
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *arXiv preprint*, 2023.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI。GPT-4技术报告。*arXiv 预印本*，2023年。
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray 等。训练语言模型以遵循人类反馈的指令。*神经信息处理系统进展*，35:27730–27744，2022年。
- en: 'Popović (2015) Maja Popović. chrf: character n-gram f-score for automatic mt
    evaluation. In *Proceedings of the tenth workshop on statistical machine translation*,
    pp.  392–395, 2015.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Popović (2015) Maja Popović. chrf: 字符 n-gram f-score 用于自动机器翻译评估。载于*第十届统计机器翻译研讨会论文集*，第392–395页，2015年。'
- en: 'Post (2018) Matt Post. A call for clarity in reporting BLEU scores. In *Proceedings
    of the Third Conference on Machine Translation: Research Papers*, pp.  186–191,
    Brussels, Belgium, October 2018\. Association for Computational Linguistics. doi:
    10.18653/v1/W18-6319. URL [https://www.aclweb.org/anthology/W18-6319](https://www.aclweb.org/anthology/W18-6319).'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Post (2018) Matt Post. 关于报告BLEU分数的明确性呼吁。载于*第三届机器翻译会议：研究论文集*，第186–191页，比利时布鲁塞尔，2018年10月。计算语言学协会。doi:
    10.18653/v1/W18-6319。网址 [https://www.aclweb.org/anthology/W18-6319](https://www.aclweb.org/anthology/W18-6319)。'
- en: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.
    *OpenAI Blog*, 1(8):9, 2019.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2019) Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei 和 Ilya Sutskever。语言模型是无监督多任务学习者。*OpenAI 博客*，1(8):9，2019年。
- en: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*, 2022.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao et al. (2022) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé 等。Bloom: 一种176b参数的开放访问多语言模型。*arXiv 预印本 arXiv:2211.05100*，2022年。'
- en: 'Sennrich et al. (2016) Rico Sennrich, Barry Haddow, and Alexandra Birch. Improving
    neural machine translation models with monolingual data. In *Proceedings of the
    54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
    Long Papers)*, pp.  86–96, Berlin, Germany, August 2016\. Association for Computational
    Linguistics. doi: 10.18653/v1/P16-1009. URL [https://www.aclweb.org/anthology/P16-1009](https://www.aclweb.org/anthology/P16-1009).'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sennrich et al. (2016) Rico Sennrich, Barry Haddow 和 Alexandra Birch。通过单语数据改进神经机器翻译模型。载于*第54届计算语言学协会年会论文集（第1卷：长篇论文）*，第86–96页，德国柏林，2016年8月。计算语言学协会。doi:
    10.18653/v1/P16-1009。网址 [https://www.aclweb.org/anthology/P16-1009](https://www.aclweb.org/anthology/P16-1009)。'
- en: Shi et al. (2022) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou,
    et al. Language models are multilingual chain-of-thought reasoners. *arXiv preprint
    arXiv:2210.03057*, 2022.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. (2022) Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj
    Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou
    等。语言模型是多语言链式思维推理器。*arXiv 预印本 arXiv:2210.03057*，2022年。
- en: 'Sia & Duh (2023) Suzanna Sia and Kevin Duh. In-context learning as maintaining
    coherency: A study of on-the-fly machine translation using large language models.
    *arXiv preprint arXiv:2305.03573*, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sia & Duh（2023）**苏珊娜·西亚**和**凯文·杜赫**。作为保持连贯性的上下文学习：基于大语言模型的即时机器翻译研究。*arXiv预印本
    arXiv:2305.03573*，2023年。
- en: Socher et al. (2013) Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang,
    Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models
    for semantic compositionality over a sentiment treebank. In *Proceedings of the
    2013 Conference on Empirical Methods in Natural Language Processing*, pp.  1631–1642,
    Seattle, Washington, USA, October 2013\. Association for Computational Linguistics.
    URL [https://www.aclweb.org/anthology/D13-1170](https://www.aclweb.org/anthology/D13-1170).
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Socher等（2013）**理查德·索赫**、**亚历克斯·佩雷尔金**、**简·吴**、**贾森·庄**、**克里斯托弗·D·曼宁**、**安德鲁·吴**和**克里斯托弗·波茨**。递归深度模型用于情感树库的语义组合。见于*2013年自然语言处理经验方法会议论文集*，第1631–1642页，美国华盛顿州西雅图，2013年10月。计算语言学协会。网址[https://www.aclweb.org/anthology/D13-1170](https://www.aclweb.org/anthology/D13-1170)。
- en: Tang et al. (2020) Yuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Naman Goyal,
    Vishrav Chaudhary, Jiatao Gu, and Angela Fan. Multilingual translation with extensible
    multilingual pretraining and finetuning. *arXiv preprint arXiv:2008.00401*, 2020.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等（2020）**余庆·唐**、**周·陈**、**李·娟**、**彭-任·陈**、**纳曼·戈亚尔**、**维什拉夫·乔杜里**、**贾塔·古**和**安吉拉·范**。具有可扩展的多语言预训练和微调的多语言翻译。*arXiv预印本
    arXiv:2008.00401*，2020年。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models. *arXiv
    preprint arXiv:2302.13971*, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron等（2023）**雨果·图夫龙**、**提博·拉夫里尔**、**戈蒂埃·伊扎卡德**、**泽维尔·马提内特**、**玛丽-安·拉肖**、**提摩西·拉克鲁瓦**、**巴蒂斯特·罗齐埃尔**、**纳曼·戈亚尔**、**埃里克·汉布罗**、**法伊萨尔·阿扎尔**、**奥雷利安·罗德里格斯**、**阿尔芒·朱林**、**爱德华·格雷夫**和**吉约姆·兰普尔**。Llama：开放且高效的基础语言模型。*arXiv预印本
    arXiv:2302.13971*，2023年。
- en: Tran et al. (2020) Chau Tran, Yuqing Tang, Xian Li, and Jiatao Gu. Cross-lingual
    retrieval for iterative self-supervised training. In H. Larochelle, M. Ranzato,
    R. Hadsell, M. F. Balcan, and H. Lin (eds.), *Advances in Neural Information Processing
    Systems*, volume 33, pp.  2207–2219\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper/2020/file/1763ea5a7e72dd7ee64073c2dda7a7a8-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1763ea5a7e72dd7ee64073c2dda7a7a8-Paper.pdf).
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tran等（2020）**周·陈**、**余庆·唐**、**李·娟**和**贾塔·古**。跨语言检索用于迭代自监督训练。见于H. 拉罗谢尔、M. 兰扎托、R. 哈德塞尔、M. F.
    巴尔坎和H. 林（编辑），*神经信息处理系统进展*，第33卷，第2207–2219页。柯伦联合公司，2020年。网址[https://proceedings.neurips.cc/paper/2020/file/1763ea5a7e72dd7ee64073c2dda7a7a8-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/1763ea5a7e72dd7ee64073c2dda7a7a8-Paper.pdf)。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large
    language models. *arXiv preprint arXiv:2201.11903*, 2022.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2022）**贾森·魏**、**薛智·王**、**戴尔·舒尔曼斯**、**马尔滕·博斯马**、**艾德·奇**、**阮国**和**丹尼·周**。思维链提示引发大语言模型的推理。*arXiv预印本
    arXiv:2201.11903*，2022年。
- en: Wei et al. (2023) Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson,
    Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language
    models do in-context learning differently. *arXiv preprint arXiv:2303.03846*,
    2023.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2023）**杰瑞·魏**、**贾森·魏**、**易·泰**、**达斯汀·特兰**、**阿尔伯特·韦布森**、**易锋·陆**、**辛云·陈**、**汉晓·刘**、**大黄**、**丹尼·周**等。更大的语言模型在上下文学习上有所不同。*arXiv预印本
    arXiv:2303.03846*，2023年。
- en: 'Wenzek et al. (2020) Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau,
    Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. CCNet:
    Extracting high quality monolingual datasets from web crawl data. In *Proceedings
    of the 12th Language Resources and Evaluation Conference*, pp.  4003–4012, Marseille,
    France, May 2020\. European Language Resources Association. ISBN 979-10-95546-34-4.
    URL [https://www.aclweb.org/anthology/2020.lrec-1.494](https://www.aclweb.org/anthology/2020.lrec-1.494).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wenzek等（2020）**吉约姆·温泽克**、**玛丽-安·拉肖**、**亚历克西斯·孔诺**、**维什拉夫·乔杜里**、**弗朗西斯科·古斯曼**、**阿尔芒·朱林**和**爱德华·格雷夫**。CCNet：从网络爬虫数据中提取高质量单语数据集。见于*第12届语言资源与评估会议论文集*，第4003–4012页，法国马赛，2020年5月。欧洲语言资源协会。ISBN
    979-10-95546-34-4。网址[https://www.aclweb.org/anthology/2020.lrec-1.494](https://www.aclweb.org/anthology/2020.lrec-1.494)。
- en: Xie et al. (2021) Sang Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu
    Ma. An explanation of in-context learning as implicit bayesian inference. *arXiv
    preprint arXiv:2111.02080*, 2021.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie等（2021）**桑·迈克尔·谢**、**阿迪蒂·拉古纳坦**、**珀西·梁**和**滕瑜·马**。将上下文学习解释为隐式贝叶斯推断。*arXiv预印本
    arXiv:2111.02080*，2021年。
- en: 'Zhang et al. (2023) Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting
    large language model for machine translation: A case study. *arXiv preprint arXiv:2301.07069*,
    2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2023）**标志张**、**巴里·哈多** 和 **亚历山德拉·伯奇**。*Prompting large language model
    for machine translation: A case study*。*arXiv preprint arXiv:2301.07069*，2023年。'
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more
    for alignment. *arXiv preprint arXiv:2305.11206*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等人（2023）**春婷·周**、**彭飞·刘**、**蒲新·徐**、**斯里尼·艾耶**、**焦·孙**、**玉宁·毛**、**薛哲·马**、**阿维亚·埃夫拉特**、**平·余**、**莉莉·余**
    等人。*Lima: Less is more for alignment*。*arXiv preprint arXiv:2305.11206*，2023年。'
- en: 'Zhu et al. (2023) Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng
    Kong, Jiajun Chen, Lei Li, and Shujian Huang. Multilingual machine translation
    with large language models: Empirical results and analysis. *arXiv preprint arXiv:2304.04675*,
    2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人（2023）**温浩·朱**、**宏义·刘**、**青秀·董**、**晶晶·徐**、**凌鹏·孔**、**佳军·陈**、**磊·李** 和
    **书剑·黄**。*Multilingual machine translation with large language models: Empirical
    results and analysis*。*arXiv preprint arXiv:2304.04675*，2023年。'
- en: Phụ lục A Appendix
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A
- en: A.1 Low-resource Language Details
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 低资源语言详细信息
- en: '[Table 8](#A1.T8 "In A.1 Low-resource Language Details ‣ Phụ lục A Appendix
    ‣ Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant
    Abilities with Linguistically-Diverse Prompts") lists the details of each low-resource
    language in the ROOTS corpus (Laurençon et al., [2022](#bib.bib19)) that we mainly
    evaluate with the BLOOM model (Scao et al., [2022](#bib.bib33)). Regarding test
    sets, we primarily choose from the ML50 benchmark (Tang et al., [2020](#bib.bib38)),
    which collected test data from various sources, such as WMT (Barrault et al.,
    [2020](#bib.bib2)) and FLoRes (Guzmán et al., [2019](#bib.bib14); Goyal et al.,
    [2022](#bib.bib13)). For languages absent in ML50, we choose the NLLB-devtest
    sets (Costa-jussà et al., [2022](#bib.bib7)) as replacement. For non-English $X$
    tasks, we choose NLLB-devtest for all our evaluation. To limit the API call costs
    within our budget, we randomly the same 200 samples from each test set for evaluation.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[表8](#A1.T8 "在 A.1 低资源语言详细信息 ‣ 附录 A ‣ 利用其英语主导能力与语言多样化提示民主化 LLM") 列出了 ROOTS
    语料库（Laurençon 等人，[2022](#bib.bib19)）中每种低资源语言的详细信息，我们主要用 BLOOM 模型（Scao 等人，[2022](#bib.bib33)）进行评估。关于测试集，我们主要从
    ML50 基准（Tang 等人，[2020](#bib.bib38)）中选择，该基准从各种来源收集测试数据，例如 WMT（Barrault 等人，[2020](#bib.bib2)）和
    FLoRes（Guzmán 等人，[2019](#bib.bib14)；Goyal 等人，[2022](#bib.bib13)）。对于 ML50 中不存在的语言，我们选择
    NLLB-devtest 集（Costa-jussà 等人，[2022](#bib.bib7)）作为替代。对于非英语 $X$ 任务，我们选择 NLLB-devtest
    进行所有评估。为了将 API 调用成本限制在预算范围内，我们从每个测试集中随机抽取相同的 200 个样本进行评估。'
- en: 'Bảng 8: Low-resource language details and corresponding test sets and unlabeled
    data sources for $X$En translation tasks.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：低资源语言详细信息及其对应的测试集和未标记数据源，用于 $X$En 翻译任务。
- en: '| Indic | African |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| Indic | African |'
- en: '| --- | --- |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Name | Code | Test | Unlabeled | Name | Code | Test set | Unlabeled |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Name | Code | Test | Unlabeled | Name | Code | Test set | Unlabeled |'
- en: '| Assamese | as | NLLB | CC100 | Tumbuka | –/tum | NLLB | OUR |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| Assamese | as | NLLB | CC100 | Tumbuka | –/tum | NLLB | OUR |'
- en: '| Oriya | or | NLLB | ROOTS | Kikuyu | ki/kik | NLLB | OUR |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| Oriya | or | NLLB | ROOTS | Kikuyu | ki/kik | NLLB | OUR |'
- en: '| Gujarati | gu | ML50 | CC100 | Bambara | bm/bam | NLLB | MAFAND |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Gujarati | gu | ML50 | CC100 | Bambara | bm/bam | NLLB | MAFAND |'
- en: '| Marathi | mr | ML50 | CC100 | Akan | ak/aka | NLLB | OUR |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| Marathi | mr | ML50 | CC100 | Akan | ak/aka | NLLB | OUR |'
- en: '| Panjabi | pa | NLLB | CC100 | Tsonga | ts/tso | NLLB | MMTAfrica |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| Panjabi | pa | NLLB | CC100 | Tsonga | ts/tso | NLLB | MMTAfrica |'
- en: '| Kannada | kn | NLLB | CC100 | Southern Sotho | st/sot | NLLB | OUR |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| Kannada | kn | NLLB | CC100 | Southern Sotho | st/sot | NLLB | OUR |'
- en: '| Nepali | ne | ML50 | CC100 | Chewa | ny/nya | NLLB | MMTAfrica |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| Nepali | ne | ML50 | CC100 | Chewa | ny/nya | NLLB | MMTAfrica |'
- en: '| Telugu | te | ML50 | CC100 | Tswana | tn/tsn | NLLB | MMTAfrica |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Telugu | te | ML50 | CC100 | Tswana | tn/tsn | NLLB | MMTAfrica |'
- en: '| Malayalam | ml | ML50 | CC100 | Lingala | ln/lin | NLLB | MMTAfrica |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Malayalam | ml | ML50 | CC100 | Lingala | ln/lin | NLLB | MMTAfrica |'
- en: '| Urdu | ur | NLLB | CC100 | Northern Sotho | –/nso | NLLB | MMTAfrica |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| Urdu | ur | NLLB | CC100 | Northern Sotho | –/nso | NLLB | MMTAfrica |'
- en: '| Tamil | ta | ML50 | CC100 | Fon | –/fon | NLLB | MAFAND |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| Tamil | ta | ML50 | CC100 | Fon | –/fon | NLLB | MAFAND |'
- en: '| Bengali | bn | NLLB | CC100 | Rundi | rn/run | NLLB | OUR |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| Bengali | bn | NLLB | CC100 | Rundi | rn/run | NLLB | OUR |'
- en: '| Hindi | hi | ML50 | CC100 | Wolof | wo/wol | NLLB | CC100 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Hindi | hi | ML50 | CC100 | Wolof | wo/wol | NLLB | CC100 |'
- en: '|  |  |  | CC100 | Luganda | lg/lug | NLLB | CC100 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CC100 | Luganda | lg/lug | NLLB | CC100 |'
- en: '|  |  |  | CC100 | Shona | sn/sna | NLLB | CC100 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CC100 | Shona | sn/sna | NLLB | CC100 |'
- en: '|  |  |  | CC100 | Zulu | zu/zul | NLLB | CC100 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CC100 | Zulu | zu/zul | NLLB | CC100 |'
- en: '|  |  |  | CC100 | Igbo | ig/ibo | NLLB | CC100 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CC100 | Igbo | ig/ibo | NLLB | CC100 |'
- en: '|  |  |  | CC100 | Xhosa | xh/xho | NLLB | CC100 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CC100 | 科萨语 | xh/xho | NLLB | CC100 |'
- en: '|  |  |  | CC100 | Kinyarwanda | rw/kin | NLLB | MMTAfrica |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CC100 | 基尼阿旺达语 | rw/kin | NLLB | MMTAfrica |'
- en: '|  |  |  | CC100 | Yoruba | yo/yor | NLLB | CC100 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CC100 | 尤鲁巴语 | yo/yor | NLLB | CC100 |'
- en: '|  |  |  | CC100 | Swahili | sw/swa | NLLB | CC100 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | CC100 | 斯瓦希里语 | sw/swa | NLLB | CC100 |'
- en: A.2 Experiment Details
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 实验细节
- en: Few-shot data sources.
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 少量样本数据来源。
- en: For supervised prompting, we collect randomly parallel pairs from the respective
    valid set for each language. For unlabeled data for our LDP method, we collect
    and filter data from various sources, as specified in Unlabeled column of [Table 8](#A1.T8
    "In A.1 Low-resource Language Details ‣ Phụ lục A Appendix ‣ Democratizing LLMs
    for Low-Resource Languages by Leveraging their English Dominant Abilities with
    Linguistically-Diverse Prompts"). Specifically, the primary unlabeled source is
    the CC100 corpus (Wenzek et al., [2020](#bib.bib43); Conneau et al., [2020](#bib.bib6)).
    For those absent in CC100, we collect data from other sources, such as the ROOTS
    corpus (Laurençon et al., [2022](#bib.bib19)), MMTAfrica (Emezue & Dossou, [2021](#bib.bib10))
    and MAFAND (Adelani et al., [2022](#bib.bib1)). For the remaining languages where
    we could not find in research repositories, we crawled from several religious
    and news websites (OUR). The sizes of collected unlabeled texts vary greatly,
    ranging from a few millions lines for Hindi to less than 1000 lines for Bambara,
    thus presenting a challenge for data balancing. For LDP non-English high-resource
    exemplars, we randomly collect a single high-quality sentence of similar lengths
    from the CC100 corpus for each language and use the unsupervised CRISS model (Tran
    et al., [2020](#bib.bib40)) to translate them into English.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督提示，我们从每种语言的有效集随机收集平行对。对于 LDP 方法的无标签数据，我们从各种来源收集和过滤数据，如 [表 8](#A1.T8 "在 A.1
    低资源语言细节 ‣ 附录 A ‣ 利用语言多样性提示对低资源语言进行民主化") 中无标签列所指定的。具体而言，主要的无标签来源是 CC100 语料库（Wenzek
    等人，[2020](#bib.bib43)；Conneau 等人，[2020](#bib.bib6)）。对于那些在 CC100 中缺失的语言，我们从其他来源收集数据，如
    ROOTS 语料库（Laurençon 等人，[2022](#bib.bib19)），MMTAfrica（Emezue & Dossou，[2021](#bib.bib10)）和
    MAFAND（Adelani 等人，[2022](#bib.bib1)）。对于那些在研究库中找不到的语言，我们从多个宗教和新闻网站（OUR）进行抓取。收集到的无标签文本大小差异很大，从印地语的几百万行到班巴拉语的不到
    1000 行不等，因此在数据平衡方面面临挑战。对于 LDP 非英语高资源样本，我们从 CC100 语料库中随机收集每种语言长度相似的高质量句子，并使用无监督的
    CRISS 模型（Tran 等人，[2020](#bib.bib40)）将其翻译成英语。
- en: Unlabeled data filtering
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无标签数据过滤
- en: To ensure high-quality native texts for unsupervised LDP prompting as well as
    larger-scale synthetic data creation for fine-tuning, we filter unlabeled texts
    such that they (i) are within 20 to 200 character lengths, (ii) do not contain
    non-conversational artifacts like URLs, brackets, bullet points or excessive numbers,
    and (iii) do not contain more than 20% alphabetical characters for Indic and non-latin
    characters for African languages. For fine-tuning, we use an upscaling temperature
    of 25 to smoothen the data mixture imbalance.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保高质量的本地文本用于无监督 LDP 提示以及更大规模的合成数据创建以进行微调，我们过滤无标签文本，确保它们（i）在 20 到 200 字符长度之间，（ii）不包含非对话性伪影，如
    URL、括号、项目符号或过多的数字，以及（iii）对于印地语和非拉丁字符的非洲语言，不包含超过 20% 的字母字符。为了微调，我们使用 25 的上采样温度来平滑数据混合不平衡。
- en: Other Details.
  id: totrans-249
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 其他细节。
- en: We evaluate translation tasks with chrF++ (Popović, [2015](#bib.bib30)) and
    SacreBLEU (Post, [2018](#bib.bib31)). For SacreBLEU, we use the default tokenizer
    for Latin-based languages, while follow Guzmán et al. ([2019](#bib.bib14)); Goyal
    et al. ([2022](#bib.bib13)) to use [indic_nlp_library](https://github.com/anoopkunchukuttan/indic_nlp_library)
    for Indic language tokenization.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 chrF++（Popović, [2015](#bib.bib30)）和 SacreBLEU（Post, [2018](#bib.bib31)）来评估翻译任务。对于
    SacreBLEU，我们使用针对拉丁语系语言的默认分词器，同时遵循 Guzmán 等人（[2019](#bib.bib14)）；Goyal 等人（[2022](#bib.bib13)）使用
    [indic_nlp_library](https://github.com/anoopkunchukuttan/indic_nlp_library) 进行印地语语言的分词。
- en: A.3 Additional Experiments
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 额外实验
- en: $20$% pre-training data
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: $20$% pre-training data
- en: 'Hình 7: chrF++ scores for translation from each Indic and African language
    in the ROOTS corpus to English ($X$En), using BLOOM. The right y-axis indicates
    corresponding pre-training coverage of each language at log scale.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：使用 BLOOM 对来自 ROOTS 语料库的每种印地语和非洲语言到英语的 chrF++ 分数（$X$En）。右侧 y 轴表示每种语言在对数刻度下的预训练覆盖率。
- en: Breakdown of $X$En.
  id: totrans-254
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: $X$En 的详细信息
- en: Similar to the observation for En$\rightarrow$ in the main paper, [Figure 7](#A1.F7
    "In A.3 Additional Experiments ‣ Phụ lục A Appendix ‣ Democratizing LLMs for Low-Resource
    Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse
    Prompts") shows that LDP performs generally on par with supervised prompting equally
    across all languages, and that it does not unevenly perform much worse or better
    in any particular language.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 与主文中 En$\rightarrow$ 的观察类似，[图7](#A1.F7 "在 A.3 附加实验 ‣ 附录 A ‣ 利用其英语主导能力与语言多样化提示使低资源语言民主化")
    显示，LDP 在所有语言中的表现通常与监督提示持平，并且在任何特定语言中表现不会不均匀地好或差。
- en: $5$
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: $5$
- en: 'Hình 8: Tokenization issue. Left y-axis bar chart: The average ratios between
    the token lengths of $X$ indicates GPT is worse than BLOOM.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：标记化问题。左侧 y 轴条形图：$X$ 的标记长度平均比率显示，GPT 的表现比 BLOOM 差。
- en: BLOOM vs. InstructGPT.
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: BLOOM vs. InstructGPT。
- en: While much evidence show that InstructGPT text-davinci-003 is more superior
    than the vanilla BLOOM (Scao et al., [2022](#bib.bib33); Ouyang et al., [2022](#bib.bib29))
    in many languages, our experiments with low-resource languages demonstrate it
    is not always true for low-resource non-Latin languages, as shown in the main
    paper. [Figure 8](#A1.F8 "In Breakdown of 𝑋→En. ‣ A.3 Additional Experiments ‣
    Phụ lục A Appendix ‣ Democratizing LLMs for Low-Resource Languages by Leveraging
    their English Dominant Abilities with Linguistically-Diverse Prompts") explains
    clearly the reason is that GPT’s tokenizer is not designed to allocate meaningful
    sub-word tokens for non-Latin texts, such as Indic lexical items, while significantly
    favors Latin characters due to the sheer size of Latin texts in its pre-training
    data. For example of InstructGPT, a 10-token English text can be equivalent to
    a 160-token Tamil text but only a 28-token Tumbuka text, despite Tumbuka is much
    more low-resource. This issue is non-existent in BLOOM, as the ratios naturally
    decrease when data coverages increase. As shown in the table, InstructGPT becomes
    worse than BLOOM as soon as the ratio between token lengths of target language
    over English surpass 5 in Indic languages. We refer to this as sub-word token
    fragmentation, where texts are broken into very long byte-level tokens that exceed
    the context length and suppress performances.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大量证据表明，InstructGPT text-davinci-003 在许多语言中优于普通的 BLOOM（Scao 等人，[2022](#bib.bib33);
    Ouyang 等人，[2022](#bib.bib29)），我们的低资源语言实验表明，这并不总是适用于低资源的非拉丁语言，如主文所示。[图8](#A1.F8
    "在 𝑋→En 的分解中 ‣ A.3 附加实验 ‣ 附录 A ‣ 利用其英语主导能力与语言多样化提示使低资源语言民主化") 清楚地解释了原因是 GPT 的标记器并未设计为分配有意义的子词标记给非拉丁文本，如印度语言词汇项，而是由于拉丁文本在其预训练数据中的数量巨大，显著偏向拉丁字符。例如，在
    InstructGPT 中，10 个标记的英文文本可以等同于 160 个标记的泰米尔文文本，但只有 28 个标记的图姆布卡文文本，尽管图姆布卡文的资源更少。这个问题在
    BLOOM 中并不存在，因为当数据覆盖面增加时比率自然下降。如表中所示，InstructGPT 一旦目标语言与英语的标记长度比率在印度语言中超过 5，即比
    BLOOM 表现更差。我们称之为子词标记碎片化，其中文本被分解成非常长的字节级标记，超出上下文长度并抑制性能。
