- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:45:00'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:00
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Adapting LLMs for Efficient Context Processing through Soft Prompt Compression
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 适应 LLMs 以实现高效的语境处理通过软提示压缩
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04997](https://ar5iv.labs.arxiv.org/html/2404.04997)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.04997](https://ar5iv.labs.arxiv.org/html/2404.04997)
- en: Cangqing Wang¹, Yutian Yang¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Cangqing Wang¹、Yutian Yang¹
- en: 'Ruisi Li², Dan Sun², Ruicong Cai², Yuzhu Zhang², Chengqian Fu³ and Lillian
    Floyd^∗ ¹Cangqing Wang be with Boston University, MA 02215, USA {kriswang}@bu.edu¹Yutian
    Yang be with University of California, Davis, CA 95616, USA {yytyang}@ucdavis.edu²Ruisi
    Li is an independent researcher. Correspondence to Ruisi Li via email: {irisli7728}@gmail.com²Dan
    Sun be with Washington University in St. Louis, MO 63130, USA {sun.dan}@wustl.edu²Ruicong
    Cai is an independent researcher. Correspondence to Ruicong Cai via email: {aruiqian}@gmail.com²Yuzhu
    Zhang be with Carnegie Mellon University, Pittsburgh, PA 15213 {yuzhuz}@alumni.cmu.edu³Chengqian
    Fu is an independent researcher. Correspondence to Chengqian Fu via email: {cqfu728}@gmail.com^∗Lilian
    Floyd be with Georgia Institue of Technology, GA 30332, USA {lfloyd74}@gatech.edu'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Ruisi Li²、Dan Sun²、Ruicong Cai²、Yuzhu Zhang²、Chengqian Fu³ 和 Lillian Floyd^∗
    ¹Cangqing Wang 隶属于波士顿大学，MA 02215，美国 {kriswang}@bu.edu¹Yutian Yang 隶属于加利福尼亚大学戴维斯分校，CA
    95616，美国 {yytyang}@ucdavis.edu²Ruisi Li 是一名独立研究人员。联系 Ruisi Li 的电子邮件：{irisli7728}@gmail.com²Dan
    Sun 隶属于华盛顿大学圣路易斯分校，MO 63130，美国 {sun.dan}@wustl.edu²Ruicong Cai 是一名独立研究人员。联系 Ruicong
    Cai 的电子邮件：{aruiqian}@gmail.com²Yuzhu Zhang 隶属于卡内基梅隆大学，Pittsburgh，PA 15213 {yuzhuz}@alumni.cmu.edu³Chengqian
    Fu 是一名独立研究人员。联系 Chengqian Fu 的电子邮件：{cqfu728}@gmail.com^∗Lilian Floyd 隶属于乔治亚理工学院，GA
    30332，美国 {lfloyd74}@gatech.edu
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The rapid advancement of Large Language Models (LLMs) has inaugurated a transformative
    epoch in natural language processing, fostering unprecedented proficiency in text
    generation, comprehension, and contextual scrutiny. Nevertheless, effectively
    handling extensive contexts, crucial for myriad applications, poses a formidable
    obstacle owing to the intrinsic constraints of the models’ context window sizes
    and the computational burdens entailed by their operations. This investigation
    presents an innovative framework that strategically tailors LLMs for streamlined
    context processing by harnessing the synergies among natural language summarization,
    soft prompt compression, and augmented utility preservation mechanisms. Our methodology,
    dubbed SoftPromptComp, amalgamates natural language prompts extracted from summarization
    methodologies with dynamically generated soft prompts to forge a concise yet semantically
    robust depiction of protracted contexts. This depiction undergoes further refinement
    via a weighting mechanism optimizing information retention and utility for subsequent
    tasks. We substantiate that our framework markedly diminishes computational overhead
    and enhances LLMs’ efficacy across various benchmarks, while upholding or even
    augmenting the caliber of the produced content. By amalgamating soft prompt compression
    with sophisticated summarization, SoftPromptComp confronts the dual challenges
    of managing lengthy contexts and ensuring model scalability. Our findings point
    towards a propitious trajectory for augmenting LLMs’ applicability and efficiency,
    rendering them more versatile and pragmatic for real-world applications. This
    research enriches the ongoing discourse on optimizing language models, providing
    insights into the potency of soft prompts and summarization techniques as pivotal
    instruments for the forthcoming generation of NLP solutions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的快速发展开启了自然语言处理的变革时代，促成了文本生成、理解和语境审视的前所未有的熟练度。然而，有效处理广泛的语境——对众多应用至关重要——由于模型的语境窗口大小和操作所涉及的计算负担，成为一个严峻的挑战。本研究提出了一种创新框架，通过利用自然语言摘要、软提示压缩和增强的实用性保留机制之间的协同效应，战略性地调整
    LLMs 以优化语境处理。我们的方法，称为 SoftPromptComp，将从摘要方法中提取的自然语言提示与动态生成的软提示相结合，形成一个简洁但语义强健的冗长语境描绘。该描绘通过加权机制进一步优化，以保持信息和实用性的最佳状态，为后续任务提供支持。我们证明了我们的框架显著减少了计算开销，并在各种基准测试中提高了
    LLMs 的有效性，同时保持或甚至提升了生成内容的质量。通过将软提示压缩与复杂的摘要结合，SoftPromptComp 解决了管理长语境和确保模型可扩展性的双重挑战。我们的发现指向了增强
    LLMs 适用性和效率的有前景的发展方向，使它们在实际应用中更加多功能和实用。这项研究丰富了关于优化语言模型的持续讨论，提供了有关软提示和摘要技术作为未来
    NLP 解决方案关键工具的见解。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Knowledge Graph Reasoning, Reinforcement Learning, Reward Shaping, Transfer
    Learning
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 知识图谱推理、强化学习、奖励塑形、迁移学习
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: The finite context window size inherent in most LLMs constrains their ability
    to fully grasp and utilize extensive textual information, thereby limiting their
    efficacy on tasks demanding profound comprehension of lengthy documents. Additionally,
    the substantial computational resources requisite for LLM processing present another
    obstacle, particularly for applications necessitating swift responsiveness and
    high throughput. These challenges underscore the imperative for innovative methodologies
    aimed at enhancing LLM efficiency and context management capabilities without
    compromising performance.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数LLMs固有的有限上下文窗口大小限制了它们完全理解和利用大量文本信息的能力，从而限制了它们在需要深刻理解长篇文档的任务中的效果。此外，LLM处理所需的巨大的计算资源也是一个障碍，特别是对于需要快速响应和高吞吐量的应用。这些挑战突显了创新方法的必要性，以提高LLM的效率和上下文管理能力，而不降低性能。
- en: This paper introduces a pioneering framework, titled Soft Prompt Compression
    for LLMs (SPC-LLM), which aims to overcome these constraints by amalgamating the
    principles of soft prompt compression with natural language summarization techniques.
    the integration of soft prompt compression with LLMs could benefit from references
    to pioneering studies on prompt engineering or soft prompts in NLP[[1](#bib.bib1)].
    Our approach endeavors to tailor LLMs for streamlined context processing, enabling
    them to navigate extensive textual data more adeptly while alleviating computational
    burdens.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一个开创性的框架，称为面向LLMs的软提示压缩（SPC-LLM），旨在通过将软提示压缩的原则与自然语言总结技术相结合来克服这些限制。将软提示压缩与LLMs结合可以参考在NLP中关于提示工程或软提示的开创性研究[[1](#bib.bib1)]。我们的方法旨在为LLMs量身定制简化的上下文处理，使其能够更有效地处理大量文本数据，同时减轻计算负担。
- en: 'Our strategy comprises two primary facets: firstly, leveraging natural language
    summarization to distill protracted texts into succinct, content-rich summaries,
    and secondly, integrating these summaries into the model’s input via trainable
    soft prompts. This dual-pronged approach extends the effective context window
    of LLMs and fosters a nuanced comprehension and generation of text predicated
    on diverse information reservoirs. By condensing the context into a compact, information-dense
    format, SPC-LLM substantially diminishes computational overheads, rendering the
    deployment of LLMs more viable across a broad array of applications.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略包括两个主要方面：首先，利用自然语言总结将冗长的文本提炼成简洁、内容丰富的摘要，其次，通过可训练的软提示将这些摘要整合到模型的输入中。这种双管齐下的方法扩展了LLMs的有效上下文窗口，促进了基于多样信息源的文本的细致理解和生成。通过将上下文压缩成紧凑且信息密集的格式，SPC-LLM显著减少了计算开销，使得在各种应用场景中部署LLMs变得更加可行。
- en: We delineate a comprehensive methodology for implementing soft prompt compression
    alongside natural language summarization within LLMs, elucidating how this amalgamation
    augments model performance on tasks necessitating comprehension of extended contexts.
    Moreover, we furnish empirical substantiation from a series of experiments evincing
    the efficacy of SPC-LLM in enhancing the efficiency and precision of LLMs across
    various NLP tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们详细描述了在LLMs中实施软提示压缩与自然语言总结的综合方法，阐明了这种结合如何提升模型在需要理解扩展上下文的任务中的表现。此外，我们提供了一系列实验的实证证据，证明SPC-LLM在提高LLMs在各种NLP任务中的效率和精确度方面的有效性。
- en: 'The structure of this paper is as follows: we commence with a review of pertinent
    literature concerning LLM efficiency and context processing. Subsequently, we
    expound upon the proposed SPC-LLM framework, delineating its constituents and
    the integration process. We subsequently expound upon the experimental setup and
    present our findings, highlighting the advantages of our approach. Finally, we
    deliberate on the implications of our work and posit avenues for future research
    endeavors in this domain.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的结构如下：我们首先回顾了关于LLM效率和上下文处理的相关文献。随后，我们详细阐述了提出的SPC-LLM框架，描述了其组成部分和整合过程。接着，我们详细说明了实验设置，并呈现我们的发现，突出我们方法的优势。最后，我们讨论了我们工作的影响，并提出了在这一领域未来研究的方向。
- en: II Prior Work
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关工作
- en: 'The endeavor to augment the efficiency and contextual processing capabilities
    of Large Language Models (LLMs) stands as a focal point in contemporary natural
    language processing (NLP) inquiry. This section provides an overview of notable
    advancements in three pertinent domains: traditional methodologies for managing
    lengthy contexts within LLMs, the genesis and utilization of soft prompts, and
    innovations in text summarization techniques aimed at contextual information compression.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 提升大型语言模型（LLMs）的效率和上下文处理能力是现代自然语言处理（NLP）研究的核心。这一部分概述了三个相关领域的显著进展：在LLMs中管理长上下文的传统方法、软提示的起源与使用，以及旨在压缩上下文信息的文本摘要技术的创新。
- en: '![Refer to caption](img/5e1733a9c50188fd86b09a61d8b375b7.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/5e1733a9c50188fd86b09a61d8b375b7.png)'
- en: 'Figure 1: An example of successful prompt compression with SPC. The compressed
    prompt (green) in order to obtain a shorter length and maintain transferability
    and utility simultaneously than the original long prompt (red).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：成功使用SPC进行提示压缩的示例。为了获得更短的长度并同时保持传递性和实用性，压缩提示（绿色）相比于原始的长提示（红色）。
- en: '![Refer to caption](img/c4a17b83fae23013c50f2ea745b5fbc6.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c4a17b83fae23013c50f2ea745b5fbc6.png)'
- en: 'Figure 2: The illustration of SPC shows the compressed conversational answer
    expect with question.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：SPC的插图展示了压缩的对话回答期望与问题的关系。
- en: II-A Addressing Lengthy Contexts in LLMs
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 处理LLMs中的长上下文
- en: Initial efforts to expand the contextual scope of LLMs primarily concentrated
    on architectural innovations. Transformer-based models such as Longformer [[2](#bib.bib2)]
    and BigBird [[3](#bib.bib3)] introduced alterations to the conventional attention
    mechanism, facilitating the efficient analysis of extended textual sequences.
    These models employ sparse attention patterns and memory-efficient algorithms
    to mitigate computational overheads associated with handling extensive contexts.
    Nevertheless, notwithstanding their augmented capacity, these architectural refinements
    frequently necessitate significant alterations to pre-existing models and entail
    substantial retraining endeavors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 最初扩展LLMs上下文范围的努力主要集中在架构创新上。基于Transformer的模型如Longformer [[2](#bib.bib2)] 和BigBird
    [[3](#bib.bib3)] 对传统的注意力机制进行了调整，促进了对扩展文本序列的高效分析。这些模型采用稀疏注意力模式和内存高效算法，以减轻处理大量上下文时的计算开销。尽管如此，尽管这些架构改进提高了容量，但通常需要对现有模型进行显著的改动，并涉及大量的再训练工作。
- en: II-B Soft Prompts in NLP
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B NLP中的软提示
- en: The notion of soft prompts, first introduced by Li and Liang [[4](#bib.bib4)]
    and further elucidated by Lester [[5](#bib.bib5)], signifies a departure towards
    more adaptable and parameter-efficient approaches for tailoring LLMs to specific
    tasks. Unlike rigidly defined textual prompts, soft prompts comprise trainable
    embeddings optimized alongside the model to enhance performance on targeted tasks.
    This adaptability facilitates effective fine-tuning of LLMs sans exhaustive retraining,
    thereby enabling their adaptation across diverse applications. Nonetheless, the
    amalgamation of soft prompts with the objective of enhancing context compression
    and efficiency within LLMs remains a relatively underexplored domain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 软提示的概念，最初由Li和Liang [[4](#bib.bib4)] 提出，并由Lester [[5](#bib.bib5)] 进一步阐述，标志着向更具适应性和参数高效的方式迈进，以针对特定任务调整LLMs。与严格定义的文本提示不同，软提示包括与模型一起优化的可训练嵌入，以提升在特定任务上的性能。这种适应性使得LLMs能够在无需全面再训练的情况下有效微调，从而适应多种应用。然而，结合软提示以提高LLMs中的上下文压缩和效率的目标仍然是一个相对未被充分探讨的领域。
- en: II-C Text Summarization for Contextual Compression
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 上下文压缩的文本摘要
- en: Text summarization methodologies, encompassing both extractive and abstractive
    approaches, have been harnessed to condense protracted documents into succinct
    representations while retaining salient information. Advancements in summarization
    techniques, exemplified by models like BART [[6](#bib.bib6)], have exhibited considerable
    promise in generating coherent and concise summaries. Although these models adeptly
    distill lengthy texts, their direct utilization as a preprocessing step for LLMs
    in tasks necessitating nuanced comprehension of extended contexts has not been
    extensively investigated.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 文本摘要方法，包括提取式和抽象式方法，已被用于将冗长的文档浓缩为简洁的表述，同时保留重要信息。摘要技术的进展，以BART模型[[6](#bib.bib6)]为例，展示了在生成连贯和简明的摘要方面的巨大潜力。尽管这些模型能够有效地提炼长文本，但它们在作为LLMs任务中处理扩展上下文所需细致理解的预处理步骤时的直接应用尚未得到广泛研究。
- en: The fusion of soft prompts with summarization techniques to augment the processing
    of lengthy contexts in LLMs epitomizes a novel convergence of these research trajectories.
    Our study builds upon these foundational principles, proposing a unified framework
    that capitalizes on the strengths of soft prompts and advanced summarization methodologies
    to alleviate the constraints of existing LLMs in efficiently handling extensive
    textual information.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将软提示与摘要技术融合以增强LLMs处理长文本的能力，体现了这些研究方向的新型融合。我们的研究基于这些基础原则，提出了一个统一框架，利用软提示和先进摘要方法的优势，以缓解现有LLMs在高效处理大量文本信息中的限制。
- en: III Methodology
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 方法论
- en: 'Integrating summary vectors with natural language formatted prompts, the fusion
    of utility preservation with information compression, and the conceptualization
    of soft prompts are explored within a comprehensive mathematical model. In this
    section, our endeavor lies in the construction of a holistic mathematical framework,
    embodying the amalgamation of summary vectors with prompts formatted in natural
    language, the convergence of utility retention with information condensation,
    and the notion of soft prompts. Let us consider the following components:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 结合摘要向量和自然语言格式提示，探索了效用保留与信息压缩的融合，以及软提示的概念。在本节中，我们的目标是构建一个全面的数学框架，体现摘要向量与自然语言格式提示的融合、效用保留与信息压缩的汇合，以及软提示的概念。让我们考虑以下组件：
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An original document $D$
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个原始文档$D$
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A function $f_{NL}$
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个函数$f_{NL}$
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A function $f_{S}$
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个函数$f_{S}$
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A parameter matrix $S$ for soft prompt
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个软提示的参数矩阵$S$
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A predictive function of the language model, denoted as $f_{LM}$
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个语言模型的预测函数，记作$f_{LM}$
- en: 'The expression of the integrated model we aspire to formulate is as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望制定的集成模型表达式如下：
- en: '|  | $C=f_{LM}(S\oplus f_{S}(f_{NL}(D)))$ |  | (1) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $C=f_{LM}(S\oplus f_{S}(f_{NL}(D)))$ |  | (1) |'
- en: 'Where:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $f_{NL}(D)$.
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f_{NL}(D)$。
- en: •
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $f_{S}(P_{NL})$.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f_{S}(P_{NL})$。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $S$ represents the parameter matrix of trained soft prompts, meticulously optimized
    to augment the model’s versatility across distinct tasks.
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $S$代表训练好的软提示的参数矩阵，经过精心优化以增强模型在不同任务中的通用性。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\oplus$.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\oplus$。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $f_{LM}$.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $f_{LM}$。
- en: 'In this expression:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表达式中：
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Utility preservation is attained through the strategic design of functions $f_{S}$
    to uphold the maximum preservation of information.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过函数$f_{S}$的战略设计，实现了效用保留，以最大限度地保留信息。
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The compression of information is executed via function$f_{S}$.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信息的压缩通过函数$f_{S}$执行。
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The soft prompt is optimized during the training process to learn how to most
    effectively utilize the compressed information $V_{S}$ to improve the model’s
    performance on specific tasks.
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软提示在训练过程中被优化，以学习如何最有效地利用压缩信息$V_{S}$来提高模型在特定任务上的表现。
- en: Utilizing this methodology, we harness the robust information compression potential
    inherent in NL formatted prompts and summary vectors. Additionally, we bolster
    the adaptability and efficacy of the model for particular downstream tasks by
    integrating soft prompts, thereby amplifying its capacity for generalization [[7](#bib.bib7)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这种方法，我们利用了NL格式提示和摘要向量中固有的强大信息压缩潜力。此外，通过集成软提示，我们增强了模型在特定下游任务中的适应性和效能，从而提升了其泛化能力[[7](#bib.bib7)]。
- en: IV Experiments
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 实验
- en: 'The principal aim of these experiments is to assess the efficacy of our proposed
    methodology in enhancing the efficiency and performance of Large Language Models
    (LLMs) during the processing of expansive textual contexts. Our specific focus
    lies in evaluating: The efficacy of condensed contextual representations produced
    via natural language summarization and the influence of integrating soft prompt
    compression on the efficacy of LLMs across diverse NLP tasks.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这些实验的主要目的是评估我们提出的方法在提升大语言模型（LLMs）处理广泛文本背景时的效率和性能的有效性。我们特别关注的方面包括：通过自然语言摘要生成的压缩上下文表示的有效性，以及软提示压缩对LLMs在各种自然语言处理任务中的有效性的影响。
- en: To achieve this aim, we meticulously designed our experiments to measure two
    critical aspects of our methodology’s impact on LLMs. First, we evaluated the
    quality and utility of the condensed contextual representations generated by our
    advanced natural language summarization techniques. This assessment focused on
    determining how effectively our approach could distill essential information from
    extensive textual data, thereby facilitating a more efficient processing by LLMs.
    The evaluation criteria included the coherence, completeness, and relevance of
    the summaries in retaining the core message and essential details from the original
    texts. Second, we investigated the role of soft prompt compression in enhancing
    the overall performance of LLMs across a variety of NLP tasks, including but not
    limited to text summarization, sentiment analysis, text classification, and question
    answering. This part of our experiment aimed at quantifying the improvements in
    task-specific metrics such as accuracy, and processing time, attributable to the
    integration of soft prompts[[8](#bib.bib8)]. Special attention was given to the
    adaptability of soft prompts in capturing and utilizing the nuances of condensed
    contexts, thereby augmenting the LLMs’ ability to derive accurate and relevant
    outcomes from the processed data. Through a comprehensive set of experiments,
    we meticulously recorded and analyzed the performance metrics, processing times,
    and resource utilization patterns associated with each task, both with and without
    the application of our methodology[[9](#bib.bib9)]. This data collection was instrumental
    in illustrating the tangible benefits of our approach, particularly in terms of
    reducing computational overhead and enhancing the adaptability and efficiency
    of LLMs in handling extensive textual contexts.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，我们精心设计了实验来测量我们方法对LLMs影响的两个关键方面。首先，我们评估了由先进的自然语言摘要技术生成的压缩上下文表示的质量和实用性。这一评估集中在确定我们的方法如何有效地从广泛的文本数据中提取核心信息，从而促使LLMs更高效地处理这些数据。评估标准包括摘要在保留原始文本的核心信息和重要细节方面的连贯性、完整性和相关性。其次，我们研究了软提示压缩在提升LLMs在各种自然语言处理任务（包括但不限于文本摘要、情感分析、文本分类和问答）中的整体性能方面的作用。实验的这一部分旨在量化因软提示整合而改善的任务特定指标，如准确性和处理时间[[8](#bib.bib8)]。特别关注了软提示在捕捉和利用压缩上下文细微差别方面的适应性，从而增强LLMs从处理数据中得出准确和相关结果的能力。通过一系列全面的实验，我们详细记录和分析了每项任务的性能指标、处理时间和资源利用模式，无论是应用还是不应用我们的方法[[9](#bib.bib9)]。这些数据收集对于展示我们方法的实际好处，特别是在减少计算开销和提升LLMs处理广泛文本背景的适应性和效率方面，具有重要作用。
- en: By systematically addressing these focus areas, our experiments were designed
    not only to validate the effectiveness of our proposed methodology but also to
    explore its potential limitations and areas for further enhancement. The findings
    from these experiments are anticipated to contribute significantly to the ongoing
    efforts to optimize LLMs for a wider range of applications, thereby pushing the
    boundaries of what is achievable in the field of natural language processing.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过系统性地解决这些重点领域，我们的实验不仅旨在验证我们提出的方法的有效性，还探讨其潜在的局限性和进一步提升的领域。这些实验的发现预计将对优化LLMs在更广泛应用中的持续努力做出重要贡献，从而推动自然语言处理领域可实现的界限。
- en: IV-A Dataset Description
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 数据集描述
- en: Summarization Task CNN/Daily Mail dataset, which pro- vides news articles and
    associated highlights for evaluating summarization quality.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要任务 CNN/Daily Mail 数据集，提供新闻文章及相关重点信息，用于评估摘要质量。
- en: Sentiment Analysis The Stanford Sentiment Treebank (SST-2), offering movie reviews
    with sentiment labels.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析 使用斯坦福情感树库（SST-2），提供带有情感标签的电影评论。
- en: Text Classification The AG News dataset, containing news articles categorized
    into four topics.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分类 使用AG News数据集，包含分类为四个主题的新闻文章。
- en: Question Answering The SQuAD v2.0 dataset, featuring questions posed on a set
    of Wikipedia articles, where some questions are unanswerable.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 问答 使用SQuAD v2.0数据集，包含对一组维基百科文章提出的问题，其中一些问题无法回答。
- en: IV-B Main Result
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 主要结果
- en: We refine the performance of a Transformer-based model renowned for its summarization
    prowess through fine-tuning on the CNN/Daily Mail dataset. The dataset is partitioned
    into distinct training, validation, and test splits tailored to each specific
    task. Our summarization module effectively condenses contextual information within
    the training and validation datasets. Soft prompts are initialized as embeddings
    within the input LLMs, concurrently trained with the model’s parameters to optimize
    task-specific performance. Comparative evaluation of the LLMs’ performance, both
    with and without our methodological intervention, is conducted using task-specific
    metrics.The visualizations provided underscore the marked improvements in accuracy
    across various NLP tasks such as text summarization, sentiment analysis, text
    classification, and question answering, showcasing the efficacy of our approach
    (refer to the generated images).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过在CNN/Daily Mail数据集上进行微调，提升了以Transformer为基础的模型在摘要生成方面的性能。该数据集被划分为针对每个特定任务的训练集、验证集和测试集。我们的摘要生成模块有效地浓缩了训练和验证数据集中的上下文信息。软提示被初始化为输入LLMs中的嵌入，并与模型的参数一起训练，以优化任务特定的性能。使用任务特定的指标对LLMs的性能进行比较评估，比较了采用我们的方法和不采用我们的方法的效果。提供的可视化图表突显了在各种NLP任务中的准确率显著提高，如文本摘要、情感分析、文本分类和问答，展示了我们方法的有效性（参见生成的图像）。
- en: Additionally, our methodology significantly enhances computational efficiency,
    as evidenced by the dramatic reduction in processing times. For instance, our
    method reduces the processing time by up to 80.1 persent in the SQuAD2.0 dataset,
    and similarly substantial reductions are observed across other datasets including
    CNN/Daily Mail, SST-2, and AG News, as detailed in the accompanying table. These
    improvements in processing speed do not compromise the quality of outcomes, illustrating
    the synergy between soft prompts and summarization techniques in optimizing LLMs’
    performance. Potential areas for further inquiry may involve the refinement of
    soft prompt parameters and summarization algorithms [[7](#bib.bib7)]to bolster
    performance across a wider array of NLP tasks. Moreover, delving into the applicability
    of this methodology in multilingual settings [[10](#bib.bib10)] and diverse domains
    could yield valuable insights into its adaptability and global relevance. In summary,
    our research represents a substantial advancement in the realm of NLP, presenting
    a scalable and effective framework for augmenting the capabilities of LLMs. This
    advancement establishes a foundation for future innovations aimed at unleashing
    the full potential of LLMs in comprehending and processing intricate and extensive
    textual data. In summary, our research represents a substantial advancement in
    the realm of NLP, presenting a scalable and effective framework for augmenting
    the capabilities of LLMs. This advancement establishes a foundation for future
    innovations aimed at unleashing the full potential of LLMs in comprehending and
    processing intricate and extensive textual data. The efficiency gains, coupled
    with improved accuracy, position our method as a transformative force in the field,
    paving the way for more adaptable, faster, and precise language models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的方法显著提高了计算效率，处理时间的显著减少即为证明。例如，我们的方法在SQuAD2.0数据集中将处理时间减少了高达80.1%，在其他数据集如CNN/Daily
    Mail、SST-2和AG News中也观察到了类似的显著减少，如附表所示。这些处理速度的提升并没有影响结果的质量，展示了软提示与摘要技术在优化LLM性能中的协同效应。未来的研究可能涉及软提示参数和摘要算法的进一步优化[[7](#bib.bib7)]，以提高在更广泛的NLP任务中的表现。此外，深入探讨该方法在多语言环境[[10](#bib.bib10)]和不同领域的适用性，可能会获得其适应性和全球相关性的宝贵见解。总之，我们的研究代表了NLP领域的重要进展，提供了一个可扩展且有效的框架来增强LLM的能力。这一进展为未来的创新奠定了基础，旨在释放LLM在理解和处理复杂、大量文本数据中的全部潜力。总之，我们的研究代表了NLP领域的重要进展，提供了一个可扩展且有效的框架来增强LLM的能力。这一进展为未来的创新奠定了基础，旨在释放LLM在理解和处理复杂、大量文本数据中的全部潜力。效率的提高，加上精度的改善，使我们的方法在该领域中成为一种变革性力量，为更具适应性、更快速和更精确的语言模型铺平了道路。
- en: '|  | Claude2 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | Claude2 |'
- en: '| Cost | Original | Capsule Prompt | Save |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 成本 | 原始 | Capsule Prompt | 节省 |'
- en: '| Mail dataset | 12.33 | 3.37 | -77.9% |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 邮件数据集 | 12.33 | 3.37 | -77.9% |'
- en: '| SST-2 | 4.22 | 1.86 | -63.9% |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| SST-2 | 4.22 | 1.86 | -63.9% |'
- en: '| AG News | 42.41 | 15.51 | -78.5% |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| AG News | 42.41 | 15.51 | -78.5% |'
- en: '| SQuAD2.0 | 2.14 | 0.42 | -80.1% |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| SQuAD2.0 | 2.14 | 0.42 | -80.1% |'
- en: '![Refer to caption](img/17f24bcb27fbb82e757e32c1b5bffb13.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/17f24bcb27fbb82e757e32c1b5bffb13.png)'
- en: 'Figure 3: comparison chart of processing times.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：处理时间比较图。
- en: '![Refer to caption](img/7071f7e1c04a0ca8ba0671fd51f34dff.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7071f7e1c04a0ca8ba0671fd51f34dff.png)'
- en: 'Figure 4: Performance difference compared to baseline model.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：与基线模型的性能差异。
- en: Conclusion
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: There is a citing studies that have demonstrated the effectiveness of summarization
    in enhancing model performance[[11](#bib.bib11)]. However, our comprehensive investigation
    introduces a novel methodology that synergistically amalgamates the functionalities
    of soft prompts, prompts formatted in natural language, and advanced summarization
    techniques to augment the efficacy and efficiency of Large Language Models (LLMs)
    in handling extensive textual contexts. Through empirical validation across a
    varied spectrum of NLP tasks, we have substantiated the substantial enhancements
    our approach provides in both context compression and model adaptability.For instance,
    processing times were reduced by up to 80.1 persent for tasks involving the SQuAD2.0
    dataset, with similar efficiencies noted across other datasets such as CNN/Daily
    Mail, SST-2, and AG News. This not only underscores the efficiency of our approach
    but also highlights its potential in making advanced NLP technologies more accessible
    and feasible in resource-constrained scenarios.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有研究表明，总结方法在提升模型性能方面的有效性[[11](#bib.bib11)]。然而，我们的综合调查引入了一种新颖的方法论，该方法论将**软提示**、**自然语言格式化的提示**以及**先进的总结技术**协同融合，以提高大型语言模型（LLMs）处理广泛文本上下文的效果和效率。通过在各种NLP任务上的实证验证，我们证实了我们的方法在上下文压缩和模型适应性方面提供的显著改进。例如，在处理SQuAD2.0数据集的任务中，处理时间减少了多达80.1%，在CNN/Daily
    Mail、SST-2和AG News等其他数据集上也观察到了类似的效率。这不仅突显了我们方法的效率，还强调了它在资源受限情况下使先进NLP技术更易于获得和实现的潜力。
- en: The amalgamation of soft prompts with summary vectors, derived from prompts
    formatted in natural language, not only optimizes information compression but
    also conserves the utility of the original context. This dual emphasis ensures
    the retention of essential information while diminishing the computational overhead
    conventionally associated with processing lengthy texts. Furthermore, the adaptability
    of our methodology is underscored by its performance improvements across diverse
    NLP tasks, encompassing text summarization, sentiment analysis, text classification,
    and question answering. In light of these findings, our work not only contributes
    a significant leap forward in the field of NLP by enhancing the performance and
    efficiency of LLMs but also sets a new benchmark for future research in this area.
    The potential for our methodology to be extended and applied in multilingual contexts
    and across different domains offers exciting avenues for further exploration.
    As we continue to push the boundaries of what is possible with LLMs, our research
    lays the groundwork for a new era of NLP solutions that are more adaptable, efficient,
    and accessible than ever before.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将**软提示**与从**自然语言格式化的提示**中得出的**总结向量**融合，不仅优化了信息压缩，还保留了原始上下文的实用性。这种双重强调确保了关键性信息的保留，同时减少了处理长文本时通常伴随的计算开销。此外，我们的方法在多种NLP任务上的性能提升突显了其适应性，包括文本总结、情感分析、文本分类和问答。鉴于这些发现，我们的工作不仅在提升LLMs的性能和效率方面为NLP领域带来了重要进展，还为未来的研究设立了新的基准。我们的方法在多语言环境和不同领域的扩展和应用潜力为进一步探索提供了激动人心的前景。随着我们不断推动LLMs的可能性边界，我们的研究为一个更加适应性强、高效且易于获得的NLP解决方案新时代奠定了基础。
- en: Our discoveries indicate that the fusion of soft prompts with advanced summarization
    techniques presents a promising avenue for future exploration aimed at enhancing
    the efficiency and adaptability of LLMs. This approach not only addresses the
    challenges associated with processing lengthy texts but also unveils new prospects
    for tailoring LLMs for specific applications sans the necessity for extensive
    retraining.This convergence of efficiency, adaptability, and reduced computational
    demand represents a paradigm shift in how LLMs can be optimized for a wide range
    of applications. Our findings advocate for a continued exploration of soft prompt
    tuning and advanced summarization techniques, suggesting that the future of NLP
    lies in the strategic integration of these methodologies to overcome the inherent
    limitations of current models. As we stand on the brink of this new frontier,
    our research illuminates the path forward, offering a blueprint for the next generation
    of language models that are not only more powerful but also more practical for
    real-world applications.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的发现表明，将软提示与先进的总结技术融合，为未来的研究提供了一个有希望的方向，旨在提高LLM的效率和适应性。这种方法不仅解决了处理长文本相关的挑战，还揭示了为特定应用量身定制LLM的新前景，而无需进行大规模的重新训练。这种效率、适应性和降低计算需求的融合代表了LLM优化广泛应用的新范式。我们的研究提倡继续探索软提示调整和先进总结技术，建议NLP的未来在于战略性地整合这些方法，以克服当前模型固有的限制。在我们站在这一新前沿的边缘时，我们的研究照亮了前进的道路，为下一代不仅更强大而且更实用的语言模型提供了蓝图。
- en: References
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. Su, X. Wang, Y. Qin, C.-M. Chan, Y. Lin, H. Wang, K. Wen, Z. Liu, P. Li,
    J. Li, L. Hou, M. Sun, and J. Zhou, “On transferability of prompt tuning for natural
    language processing,” in Proceedings of the 2022 Conference of the North American
    Chapter of the Association for Computational Linguistics: Human Language Technologies,
    Association for Computational Linguistics, 2022.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. Su, X. Wang, Y. Qin, C.-M. Chan, Y. Lin, H. Wang, K. Wen, Z. Liu, P.
    Li, J. Li, L. Hou, M. Sun, 和 J. Zhou，“关于自然语言处理的提示调整的可转移性”，发表于2022年北美计算语言学协会会议：人类语言技术，计算语言学协会，2022年。'
- en: '[2] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document
    transformer,” 2020.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] I. Beltagy, M. E. Peters, 和 A. Cohan，“Longformer: 长文档变换器”，2020年。'
- en: '[3] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon,
    P. Pham, A. Ravula, Q. Wang, L. Yang, and A. Ahmed, “Big bird: Transformers for
    longer sequences,” 2021.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Zaheer, G. Guruganesh, A. Dubey, J. Ainslie, C. Alberti, S. Ontanon,
    P. Pham, A. Ravula, Q. Wang, L. Yang, 和 A. Ahmed，“Big bird: 用于更长序列的变换器”，2021年。'
- en: '[4] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for
    generation,” 2021.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] X. L. Li 和 P. Liang，“Prefix-tuning: 优化生成的连续提示”，2021年。'
- en: '[5] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-efficient
    prompt tuning,” 2021.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] B. Lester, R. Al-Rfou, 和 N. Constant，“参数高效提示调整的规模力量”，2021年。'
- en: '[6] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    and L. Zettlemoyer, “Bart: Denoising sequence-to-sequence pre-training for natural
    language generation, translation, and comprehension,” 2019.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov,
    和 L. Zettlemoyer，“Bart: 语序到语序的降噪预训练用于自然语言生成、翻译和理解”，2019年。'
- en: '[7] C. Li, H. Zheng, Y. Sun, C. Wang, L. Yu, C. Chang, X. Tian, and B. Liu,
    “Enhancing multi-hop knowledge graph reasoning through reward shaping techniques,”
    arXiv preprint arXiv:2403.05801, 2024.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] C. Li, H. Zheng, Y. Sun, C. Wang, L. Yu, C. Chang, X. Tian, 和 B. Liu，“通过奖励塑形技术增强多跳知识图推理”，arXiv
    预印本 arXiv:2403.05801，2024年。'
- en: '[8] Y. Shen, K. Song, X. Tan, W. Zhang, K. Ren, S. Yuan, W. Lu, D. Li, and
    Y. Zhuang, “Taskbench: Benchmarking large language models for task automation,”
    2023.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Y. Shen, K. Song, X. Tan, W. Zhang, K. Ren, S. Yuan, W. Lu, D. Li, 和 Y.
    Zhuang，“Taskbench: 大型语言模型任务自动化的基准测试”，2023年。'
- en: '[9] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark, et al., “An empirical analysis
    of compute-optimal large language model training,” Advances in Neural Information
    Processing Systems, vol. 35, pp. 30016–30030, 2022.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford,
    D. de Las Casas, L. A. Hendricks, J. Welbl, A. Clark 等，“计算优化大型语言模型训练的实证分析”，《神经信息处理系统进展》，第35卷，第30016–30030页，2022年。'
- en: '[10] L. Shen, W. Tan, S. Chen, Y. Chen, J. Zhang, H. Xu, B. Zheng, P. Koehn,
    and D. Khashabi, “The language barrier: Dissecting safety challenges of llms in
    multilingual contexts,” arXiv preprint arXiv:2401.13136, 2024.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] L. Shen, W. Tan, S. Chen, Y. Chen, J. Zhang, H. Xu, B. Zheng, P. Koehn,
    和 D. Khashabi, “语言障碍：剖析多语言环境下大语言模型的安全挑战，” arXiv 预印本 arXiv:2401.13136，2024年。'
- en: '[11] M. Gambhir and V. Gupta, “Recent automatic text summarization techniques:
    a survey,” Artificial Intelligence Review, vol. 47, pp. 1–66, 2016.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] M. Gambhir 和 V. Gupta, “最近的自动文本摘要技术：综述，” 《人工智能评论》，第47卷，第1–66页，2016年。'
