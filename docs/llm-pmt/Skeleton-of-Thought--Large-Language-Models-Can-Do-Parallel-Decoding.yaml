- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:50:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: 'Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.15337](https://ar5iv.labs.arxiv.org/html/2307.15337)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \doparttoc\faketableofcontentsXuefei Ning¹
  prefs: []
  type: TYPE_NORMAL
- en: foxdoraame@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Zinan Lin^(2∗)'
  prefs: []
  type: TYPE_NORMAL
- en: linzinan1995@gmail.com
  prefs: []
  type: TYPE_NORMAL
- en: '&Zixuan Zhou^(1∗)'
  prefs: []
  type: TYPE_NORMAL
- en: zhouzx21@mails.tsinghua.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Zifu Wang³'
  prefs: []
  type: TYPE_NORMAL
- en: zifu.wang@kuleuven.be
  prefs: []
  type: TYPE_NORMAL
- en: '&Huazhong Yang¹'
  prefs: []
  type: TYPE_NORMAL
- en: yanghz@tsinghua.edu.cn
  prefs: []
  type: TYPE_NORMAL
- en: '&Yu Wang¹'
  prefs: []
  type: TYPE_NORMAL
- en: yu-wang@tsinghua.edu.cn Equal contribution.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This work aims at decreasing the end-to-end generation latency of large language
    models (LLMs). One of the major causes of the high generation latency is the sequential
    decoding approach adopted by almost all state-of-the-art LLMs. In this work, motivated
    by the thinking and writing process of humans, we propose *Skeleton-of-Thought
    (SoT)*, which first guides LLMs to generate the *skeleton* of the answer, and
    then conducts parallel API calls or batched decoding to complete the contents
    of each skeleton point *in parallel*. Not only does SoT provide considerable speed-ups
    across 12 LLMs, but it can also potentially improve the answer quality on several
    question categories. SoT is an initial attempt at data-centric optimization for
    inference efficiency, and further underscores the potential of pushing LLMs to
    think more like a human for answer quality.
  prefs: []
  type: TYPE_NORMAL
- en: ¹ Department of Electronic Engineering, Tsinghua University, Beijing, China
  prefs: []
  type: TYPE_NORMAL
- en: ² Microsoft Research, Redmond, Washington, USA
  prefs: []
  type: TYPE_NORMAL
- en: ³ESAT-PSI, KU Leuven, Leuven, Belgium
  prefs: []
  type: TYPE_NORMAL
- en: 'Website: [https://sites.google.com/view/sot-llm](https://sites.google.com/view/sot-llm)'
  prefs: []
  type: TYPE_NORMAL
- en: '^†^†^† The main updates in arXiv V2 are as follows: (1) Add the quality and
    efficiency evaluation of SoT on GPT-4\. (2) Use GPT-4 as the judge for answer
    quality evaluation. The old results with ChatGPT-3.5 as the judge are moved to
    [§ I.3](#A9.SS3 "I.3 ChatGPT-3.5 as the Judge ‣ Appendix I Answer Quality Evaluation
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding").
    (3) Add the *SoT with Router (SoT-R)* method ([§ 4](#S4 "4 SoT with Router (SoT-R):
    Adapatively Triggering SoT ‣ Skeleton-of-Thought: Large Language Models Can Do
    Parallel Decoding")) which adaptively triggers SoT on suitable questions. (4)
    Move detailed answer analysis to the appendices.'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Large language models (LLMs) (Brown et al., [2020](#bib.bib3); Touvron et al.,
    [2023a](#bib.bib71); Du et al., [2022](#bib.bib15); OpenAI, [2023](#bib.bib53);
    Zheng et al., [2023](#bib.bib94)) have shown exceptional performance in natural
    language processing and chatbot systems. However, the inference process of the
    state-of-the-art LLMs is slow, hindering their interactive use. For example, it
    takes 22 seconds for Claude (Anthropic, [2023](#bib.bib1)) (accessed through Slack
    API) and 43 seconds for Vicuna-33B V1.3 (a 33B LLaMA-based model, running locally
    on one NVIDIA A100 GPU) to answer the question in [Fig. 1](#S1.F1 "In 1 Introduction
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: 'We conclude three major causes of LLMs’ slow inference: (1) A *large model
    size* requires a large amount of memory, memory access, and computation. For example,
    the FP16 weights of 175B GPT-3 take 350GB memory, which means at least 5$\times$80GB
    A100 GPUs are needed to keep the model in GPU memory. Even with enough GPUs, the
    heavy memory access and computation slow down the inference. (2) The *attention
    operation* in the prevailing transformer architecture is I/O bounded and has a
    quadratic memory and computation complexity in sequence length. (3) The *sequential
    decoding* approach in inference generates tokens one by one. This approach introduces
    a significant inference latency since the generation of tokens cannot be parallelized.
    There is a bunch of literature addressing the first two axes: *large model size* (Xiao
    et al., [2022](#bib.bib81); Frantar et al., [2022](#bib.bib19); Lin et al., [2023](#bib.bib42);
    Sheng et al., [2023](#bib.bib65); Wang et al., [2021](#bib.bib74)) and *attention
    operation* (Kitaev et al., [2020](#bib.bib29); Wang et al., [2020](#bib.bib75);
    Dao et al., [2022](#bib.bib12); Zaheer et al., [2020](#bib.bib88); Chen et al.,
    [2023b](#bib.bib8)). These works either compress/redesign the model (Xiao et al.,
    [2022](#bib.bib81); Frantar et al., [2022](#bib.bib19); Lin et al., [2023](#bib.bib42);
    Kitaev et al., [2020](#bib.bib29); Wang et al., [2020](#bib.bib75); Dao et al.,
    [2022](#bib.bib12); Zaheer et al., [2020](#bib.bib88)) or redesign the serving
    system (Sheng et al., [2023](#bib.bib65); Chen et al., [2023b](#bib.bib8)) and
    hardware (Wang et al., [2021](#bib.bib74)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e0782081c36f930627261a3a12d0f294.png)![Refer to caption](img/f6aa86838fb44033983f28c2adbef3be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Left: An illustration of Skeleton-of-Thought (SoT). Instead of producing
    answers sequentially, SoT produces different parts of answers *in parallel*. In
    more detail, given the question, SoT first prompts the LLM to give out the skeleton,
    then conducts batched decoding or parallel API calls to expand multiple points
    in parallel, and finally aggregates the outputs to get the final answer. Right:
    The net win rates and speed-ups of SoT with router (SoT-R) compared to normal
    generation on Vicuna-80\. The net win rate is the difference between the fraction
    of questions that SoT-R has better and worse answers than normal generation. The
    speed-up is the ratio between the latency of normal and SoT-R generation. $(1.0,0.0)$
    represents normal generation. Higher is better on both axes. For most models,
    SoT-R not only accelerates the generation but also improves the quality of the
    answers (evaluated with FastChat metric (Zheng et al., [2023](#bib.bib94))). See
    [§ 3.2](#S3.SS2 "3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") and [4](#S4 "4 SoT with Router
    (SoT-R): Adapatively Triggering SoT ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to prior work, we tackle the third axis and question the common
    assumption that LLMs have to do fully sequential decoding. We show the feasibility
    of parallel decoding of off-the-shelf LLMs *without* any changes to their model,
    system, or hardware. For instance, for the question in [Fig. 1](#S1.F1 "In 1 Introduction
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"), we can
    reduce the latency from 22 seconds to 12 seconds (1.83$\times$ speed-up) with
    Vicuna-33B V1.3 on an NVIDIA A100.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea stems from reflecting on how humans ourselves answer questions. Humans
    do *not* always think about questions and write answers in a sequential fashion.
    In contrast, for many question types, we first derive the *skeleton* according
    to some protocols and strategies, and then add evidence and details to refine
    and explicate each point. This is especially the case on formal occasions like
    offering consultancy, taking tests, writing papers, and so on. Can we make LLMs
    think in the same way? To this end, we propose *Skeleton-of-Thought (SoT)*. Specifically,
    as shown in [Fig. 1](#S1.F1 "In 1 Introduction ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding"), we guide the LLM to derive a skeleton first
    by itself. Based on the skeleton, the LLMs can complete each point *in parallel*
    so that we get a speed-up. SoT can be utilized to accelerate both open-source
    models with batched decoding and API-based models with parallel API calls.'
  prefs: []
  type: TYPE_NORMAL
- en: To make the overall solution more practical, we also design an extension, SoT
    with router (SoT-R), which employs a router to only trigger SoT for suitable questions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We test SoT on 12 recently released LLMs. Not only does SoT provide considerable
    speed-ups (up to 2.39$\times$), but it can also improve the answer quality in
    many cases ([Fig. 1](#S1.F1 "In 1 Introduction ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding")).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that in contrast to existing model- and system-level efforts for inference
    efficiency, SoT takes a novel “data-level” pathway by letting the LLM organize
    its output content. This novel perspective is becoming feasible and is expected
    to grow in relevance, owing to the evolving capabilities of state-of-the-art LLMs.
    We hope this work can stimulate more research in the realm of data-centric optimization (Zha
    et al., [2023](#bib.bib90); HazyResearch, [2023](#bib.bib25)) for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the paper is organized as follows. We first introduce SoT in [§ 2](#S2
    "2 Skeleton-of-Thought (SoT) ‣ Skeleton-of-Thought: Large Language Models Can
    Do Parallel Decoding") and show its results in [§ 3](#S3 "3 SoT Evaluation ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding"). Then, we expand on the SoT-R
    extension in [§ 4](#S4 "4 SoT with Router (SoT-R): Adapatively Triggering SoT
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"). [§ 5](#S5
    "5 Related Work ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    positions SoT in the research ecosystem (expanded in [App. D](#A4 "Appendix D
    Related Work (Expanded) ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding")). Finally, we analyze the limitations and share outlooks
    of SoT in [§ 6](#S6 "6 Limitations, Future Work, and Open Questions ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Skeleton-of-Thought (SoT)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.1 Method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Overview. Based on the intuition that humans usually think about and answer
    a question in an organized way, the core idea of this work is to guide the LLM
    itself to give a skeleton first and then write the overall answer parallelly instead
    of sequentially. [Fig. 1](#S1.F1 "In 1 Introduction ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding") illustrates how SoT produces the final
    answer to a user *question* $q$.'
  prefs: []
  type: TYPE_NORMAL
- en: '*(1) Skeleton stage.* SoT first assembles a *skeleton request*, $T^{s}(\mbox{question}=q)$
    of the LLM.'
  prefs: []
  type: TYPE_NORMAL
- en: '*(2) Point-expanding stage.* Based on the skeleton, we let the LLM expand on
    each point in parallel. Specifically, for the point with index $b$ to get the
    *final answer*.'
  prefs: []
  type: TYPE_NORMAL
- en: Prompt 1\.
    Skeleton Prompt Template $T^{s}$2 sentence and do not continue with other points!
    {addmargin}[1em]2em[Assistant:] {point index}. {point skeleton}
  prefs: []
  type: TYPE_NORMAL
- en: Parallel point expanding. We conduct *parallel* point-expanding so that SoT
    is able to achieve a speed-up than normal decoding.
  prefs: []
  type: TYPE_NORMAL
- en: '*(1) For proprietary models with only API access*, we can issue multiple parallel
    API calls to get an end-to-end latency gain at the cost of an increased number
    of API requests and tokens.'
  prefs: []
  type: TYPE_NORMAL
- en: '*(2) For open-source models that we can run locally*, we let them process the
    point-expanding requests as a batch (paddings are added to the left of the point-expanding
    requests). We explain below why this could achieve speed-ups. A typical LLM generative
    process consists of two phases: (a) the *prefilling* phase in which the prompt
    is parsed to generate the key-value cache for further use, and (b) the *decoding*
    phase in which tokens are generated one by one in a sequential manner. The decoding
    phase accounts for the majority of the end-to-end latency, especially when generating
    a long response. Note that the decoding phase is bottlenecked by weight loading
    instead of activation loading or computation.¹¹1This is true when the number of
    concurrent queries is small; see [§ 6](#S6 "6 Limitations, Future Work, and Open
    Questions ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    for discussion on other scenarios. Consequently, running LLM inference with increased
    batch sizes does not increase the per-token latency much. Therefore, SoT allows
    us to decode roughly $B\times$ points. See [App. E](#A5 "Appendix E Efficiency
    Analysis ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding") for the expanded discussions and the supporting experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Please refer to [App. B](#A2 "Appendix B Implementation Details of Skeleton-of-Thought
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    for more implementation details of SoT.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 SoT Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets. We evaluate SoT on two recent assistant-style datasets: (1) Vicuna-80 (Chiang
    et al., [2023](#bib.bib10)), which contains 80 questions spanning nine categories,
    such as coding, math, writing, roleplay, and so on, and (2) WizardLM (Xu et al.,
    [2023](#bib.bib83)), which contains 218 questions spanning more categories and
    diverse difficulties. Due to space constraints, we only report Vicuna-80 results
    in the main paper, and defer WizardLM results to the [Apps. G](#A7 "Appendix G
    Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding") and [I](#A9 "Appendix I Answer Quality Evaluation ‣
    Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Models. We test SoT on 12 recently released models, including 9 open-source
    models and 3 API-based models ([Table 1](#A1.T1 "In Appendix A Model Details ‣
    Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")).
    We obtain the weights of all the open-source models from Hugging Face. See [App. A](#A1
    "Appendix A Model Details ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Evaluation of Efficiency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: API-based models. We record the latency of every API call with `start = time.time();
    ...; elapsed_time = time.time() - start`, and add the latency of the skeleton
    API call and the slowest point-expanding API call as the SoT latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open-source models. All open-source models we currently evaluate are based
    on the LLaMA 7B, 13B, or 33B architectures. Thus, to enable fast analysis, we
    first make a latency profiling table for each LLaMA architecture on NVIDIA A100\.
    The table contains the architecture’s (1) latency for prefilling sequences of
    length 1 to 700 with different batch sizes (from 1 to 16), and (2) decoding one
    token with a context of length 1 to 1024 with different batch sizes (from 1 to
    16). With these three latency profiling tables, given the number of points $B$,
    the token lengths of the requests and responses in the skeleton and point-expanding
    stages, we can quickly estimate the SoT latency by simply looking up entries in
    the tables and adding them up. See [App. F](#A6 "Appendix F Efficiency Profiling
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    for a more detailed description of how we conduct the profiling and estimate the
    latency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the above approach, we also compare the actual latency of SoT
    and normal sequential generation (abbreviated as “normal” in the following discussion)
    in [§ G.1.4](#A7.SS1.SSS4 "G.1.4 Actual Latency Testing ‣ G.1 Skeleton-of-Thought
    ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this section shows the speed-ups of SoT on different models ([§ 3.1.1](#S3.SS1.SSS1
    "3.1.1 Speed-up Breakdown: Models ‣ 3.1 Evaluation of Efficiency ‣ 3 SoT Evaluation
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")) and question
    categories ([§ 3.1.2](#S3.SS1.SSS2 "3.1.2 Speed-up Breakdown: Question Categories
    ‣ 3.1 Evaluation of Efficiency ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding")). In addition, we also report the latency
    breakdown of SoT stages in [§ G.1.2](#A7.SS1.SSS2 "G.1.2 Latency Breakdown: SoT
    Stages and Phases ‣ G.1 Skeleton-of-Thought ‣ Appendix G Efficiency Evaluation
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    and the SoT speed-ups on an RTX 3090 GPU in [§ G.1.3](#A7.SS1.SSS3 "G.1.3 Efficiency
    Evaluation on NVIDIA RTX 3090 ‣ G.1 Skeleton-of-Thought ‣ Appendix G Efficiency
    Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: '3.1.1 Speed-up Breakdown: Models'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We investigate how SoT reduces the end-to-end latency on different models.
    [Fig. 2(a)](#S3.F2.sf1 "In Figure 2 ‣ 3.1.1 Speed-up Breakdown: Models ‣ 3.1 Evaluation
    of Efficiency ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding") shows the average speed-up for each model across all
    question categories. We can see that SoT obtains a $$>) on 8 out of 12
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We report the detailed statistics about token lengths and numbers of points
    in [Fig. 11](#A7.F11 "In G.1.1 Detailed Statistics of Token Lengths and Point
    Numbers ‣ G.1 Skeleton-of-Thought ‣ Appendix G Efficiency Evaluation ‣ Appendix
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"). (1) In
    terms of *the point number $B$ longer than the normal answer.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a51adfeb65d8d5e7a6882a97a4b3b63f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2fea98ae935d63ce7abc47bbb7da614c.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Average speed-ups of SoT on different models and question categories.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.1.2 Speed-up Breakdown: Question Categories'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Here we investigate how SoT reduces the end-to-end latency for different question
    categories. [Fig. 2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3.1.1 Speed-up Breakdown: Models
    ‣ 3.1 Evaluation of Efficiency ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding") shows the average speed-up for each
    question category across all models. The question categories for which SoT can
    provide high-quality answers are marked in green, and other categories are marked
    in red (see [§ 3.2.3](#S3.SS2.SSS3 "3.2.3 Quality Breakdown: Question Categories
    ‣ 3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding") for the answer quality evaluation).
    We can see that SoT can obtain speed-ups for all question categories. For the
    five question categories that SoT can provide high-quality answers (i.e., knowledge,
    generic, common-sense, roleplay, counterfactual), SoT can speed up the overall
    answer generation process by 1.89$\times$ in the meantime.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Evaluation of Answer Quality
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In order to compare the answer quality of the normal sequential generation
    (abbreviated as “normal” in the following discussion) and SoT generation, we adopt
    two LLM-based evaluation frameworks: FastChat (Zheng et al., [2023](#bib.bib94))
    and LLMZoo (Chen et al., [2023c](#bib.bib9)). The evaluation process is to present
    a question and a pair of answers (from normal or SoT generation) to an LLM judge
    (GPT-4 in the main paper; see [§ I.3](#A9.SS3 "I.3 ChatGPT-3.5 as the Judge ‣
    Appendix I Answer Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding") for the results evaluated using ChatGPT-3.5)
    and ask for its preference. The response can be that SoT’s answer wins/ties/loses
    compared to the normal answer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are more details about the evaluation of the answer quality:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(1) Detailed metrics.* FastChat evaluation provides one metric for the general
    quality of the answers. In addition to a general metric, LLMZoo provides five
    detailed metrics on the answers’ coherence, diversity, immersion, integrity, and
    relevance.'
  prefs: []
  type: TYPE_NORMAL
- en: '*(2) Question categories.* FastChat provides two special evaluation prompts
    for coding and math questions for more accurate evaluation, whereas LLMZoo does
    not. Following the implementation in LLMZoo, we exclude math and coding questions
    in all LLMZoo evaluation results.'
  prefs: []
  type: TYPE_NORMAL
- en: '*(3) Extentions to avoid evaluation bias.* To avoid the potential bias from
    the order of the two answers presented to the LLM judge, we extend FastChat and
    LLMZoo evaluation frameworks by running the evaluation twice with either ordering
    of the two answers. In either evaluation, a score of 1, 0, and -1 is assigned
    when SoT wins, ties, or loses, respectively. The final evaluation is that SoT
    wins/ties/loses when the sum of the two scores is positive/zero/negative. For
    example, if SoT wins in one evaluation and loses in the other evaluation, the
    result is “tie”. If SoT wins (loses) in one evaluation and ties in the other,
    the result is “win” (“lose”).'
  prefs: []
  type: TYPE_NORMAL
- en: '*(4) Net win rates.* We further define net win rates to give a summarized view
    of the answer quality. Given the number of questions that SoT wins (#win) and
    loses (#lose), we define *net win rates* as $\nicefrac{{\text{\#win}-\text{\#lose}}}{{\text{total
    number of questions}}}$. 0% means that SoT performs competitively to the normal
    baseline (wins and loses in the same number of questions). Higher values mean
    that SoT performs better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The organization of this section on answer quality evaluation is as follows.
    We first present the overall quality of SoT answers ([§ 3.2.1](#S3.SS2.SSS1 "3.2.1
    Overall Quality ‣ 3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding")), and then go into the details
    across different question categories ([§ 3.2.3](#S3.SS2.SSS3 "3.2.3 Quality Breakdown:
    Question Categories ‣ 3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding")), models ([§ 3.2.2](#S3.SS2.SSS2
    "3.2.2 Quality Breakdown: Models ‣ 3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")), and
    metrics ([§ 3.2.4](#S3.SS2.SSS4 "3.2.4 Quality Breakdown: Metrics ‣ 3.2 Evaluation
    of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding")).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Overall Quality
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [Fig. 3](#S3.F3 "In 3.2.1 Overall Quality ‣ 3.2 Evaluation of Answer Quality
    ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding"), we show the win/tie/lose rates (the percentage of the cases when SoT
    wins/ties/loses compared to normal generation) across all models and questions
    using the two metrics from FastChat and LLMZoo that capture the general quality
    of the answers. We notice a discrepancy between the two metrics on when SoT is
    strictly better than the baseline (45.8% v.s. 29.5%). Despite that, the two metrics
    agree that SoT is not worse than the baseline in around 60% of the cases, and
    the win rates are close to the lose rates. *This result suggests that the answers
    of SoT maintain good quality of that of the normal generation.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da51c4611b6b238f13afb463842d385c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Win/tie/lose rates of SoT v.s. normal generation using “general”
    metrics from FastChat and LLMZoo. SoT performs better than or equal to normal
    generation in around 60% cases.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2.2 Quality Breakdown: Models'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Next, we investigate how SoT performs on different models. We compute net win
    rates on all models in [Fig. 4](#S3.F4 "In 3.2.2 Quality Breakdown: Models ‣ 3.2
    Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding"). Again, we see that the two general metrics
    from FastChat and LLMZoo have different absolute values but similar rankings.
    In particular, both metrics agree that OpenChat-13B, Vicuna-7B V1.1, Claude, LLaMA2-Chat-13B
    have *low* net win rates, whereas Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B
    have *high* net win rates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2acf1af7800030b83624779eddaaed7f.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Metric: general quality (FastChat).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/08bc05e20cffcf4cf7c07f3876eded88.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Metric: general quality (LLMZoo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Net win rates of SoT on different models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We investigate the answers in [§ I.1.1](#A9.SS1.SSS1 "I.1.1 Answer Analysis:
    Different Models ‣ I.1 Skeleton-of-Thought ‣ Appendix I Answer Quality Evaluation
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"),
    and summarize the key takeaways as follows. Some models have low SoT quality as
    they cannot understand the skeleton and point-expanding prompts well. Some other
    models have low SoT quality as their normal answers already have good quality,
    making it hard for SoT to beat them (e.g., Claude). For models that are able to
    understand the SoT prompts, the answer quality is improved. We expect that further
    improving SoT prompts or fine-tuning the models can make it easier for LLMs to
    understand the skeleton and point-expanding prompts and ultimately result in better
    answer quality.'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2.3 Quality Breakdown: Question Categories'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Next, we investigate how SoT performs on different question categories. We
    compute *net win rates* (win rates minus lose rates) on all question categories
    in [Fig. 5](#S3.F5 "In 3.2.3 Quality Breakdown: Question Categories ‣ 3.2 Evaluation
    of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding"). Similar to [Fig. 3](#S3.F3 "In 3.2.1 Overall Quality
    ‣ 3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding"), we see that LLMZoo tends to be more
    optimistic about the quality of SoT than FastChat. Nevertheless, the conclusions
    are consistent: SoT performs relatively *well* on generic, common-sense, knowledge,
    roleplay, and counterfactual. SoT performs relatively *poorly* on writing, fermi,
    math, and coding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/42bac4fc05a403ca6b76399aebd06460.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Metric: general quality (FastChat).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fd813526ae3551f50919b46dc92744c7.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Metric: general quality (LLMZoo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Net win rates of SoT on different question categories.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We investigate the answers in [§ I.1.2](#A9.SS1.SSS2 "I.1.2 Answer Analysis:
    Different Question Categories ‣ I.1 Skeleton-of-Thought ‣ Appendix I Answer Quality
    Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding"), and summarize the key takeaways as follows. SoT performs well when
    the question can be answered in several points whose details can be expanded independently.
    This includes a wide range of real-world questions. On the other hand, it is fundamentally
    challenging to apply SoT on questions that require step-by-step thinking, in which
    the latter steps require the details from the earlier steps, such as math questions.
    To make SoT general across broader question categories, one promising pathway
    is to enable SoT to adaptively fall back to normal generation, which we explore
    in [§ 4](#S4 "4 SoT with Router (SoT-R): Adapatively Triggering SoT ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding"). Interestingly, our results suggest
    that some LLMs are already able to do that occasionally without special prompting
    or tuning (see [§ I.1.2](#A9.SS1.SSS2 "I.1.2 Answer Analysis: Different Question
    Categories ‣ I.1 Skeleton-of-Thought ‣ Appendix I Answer Quality Evaluation ‣
    Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")).'
  prefs: []
  type: TYPE_NORMAL
- en: '3.2.4 Quality Breakdown: Metrics'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'All previous evaluations use metrics about the general quality of the answer.
    In [Fig. 6](#S3.F6 "In 3.2.4 Quality Breakdown: Metrics ‣ 3.2 Evaluation of Answer
    Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language Models Can Do
    Parallel Decoding"), we show more detailed metrics from LLMZoo to reveal in which
    aspects SoT can improve or hurt the answer quality. On average, we can see that
    SoT improves the diversity and relevance while hurting the immersion and coherence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5078008abc89e932513bf24b975d820e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Win/tie/lose rates of SoT v.s. normal generations using metrics from
    LLMZoo. SoT performs well on diversity and relevance, and relatively worse on
    coherence and immersion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Through answer investigation ([§ I.1.3](#A9.SS1.SSS3 "I.1.3 Answer Analysis:
    Different Metrics ‣ I.1 Skeleton-of-Thought ‣ Appendix I Answer Quality Evaluation
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")),
    we summarize the key takeaways as follows. The skeleton stage of SoT explicitly
    require LLMs to discuss the answers from multiple aspects without filler words.
    This improves the diversity and relevance of the answers. As for coherence and
    immersion, SoT is not worse than the normal generation around 60% of the time.
    One future direction is to improve the SoT prompts or pipeline so that the answers
    can be better in more metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '4 SoT with Router (SoT-R): Adapatively Triggering SoT'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [§ 3](#S3 "3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding"), we see that SoT provides considerable speed-ups while
    maintaining (or even improving) answer quality for many question types. However,
    the biggest limitation is that SoT is not suitable for questions that require
    step-by-step reasoning ([§ 3.2.3](#S3.SS2.SSS3 "3.2.3 Quality Breakdown: Question
    Categories ‣ 3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding")). Towards pushing the practical
    adoption of SoT, we explore the possibility of *adaptively triggering SoT* only
    when it is suitable. To achieve that, we propose a *router* module that decides
    if SoT should be applied for the user request, and then call either SoT or normal
    decoding accordingly. This paradigm aligns with the recent trends of composing
    multiple models to solve complicated tasks (Chase, [2022](#bib.bib5); Shen et al.,
    [2023](#bib.bib64)). To implement the router, we explore two options: LLM prompting
    as the router (no model training is needed) ([§ 4.1](#S4.SS1 "4.1 Prompting Router
    ‣ 4 SoT with Router (SoT-R): Adapatively Triggering SoT ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding")), and trained RoBERTa as the
    router ([§ 4.2](#S4.SS2 "4.2 Trained Router ‣ 4 SoT with Router (SoT-R): Adapatively
    Triggering SoT ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")).
    The evaluation is provided in [§ 4.3](#S4.SS3 "4.3 SoT-R Evaluation ‣ 4 SoT with
    Router (SoT-R): Adapatively Triggering SoT ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Prompting Router
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We directly ask an LLM if the question is suitable for SoT. More specifically,
    we ask the LLM if the desired answer is in a list of independent points (see [§ C.1](#A3.SS1
    "C.1 Prompting Router ‣ Appendix C Implementation Details of Skeleton-of-Thought
    with Router ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding") for the prompt). If the answer is yes, we will use SoT; otherwise,
    we will use normal generation (i.e., directly feeding the question to the LLM).
    We employ GPT-4 as the LLM router given its strong capability.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Trained Router
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'While leveraging GPT-4 as the router obviates the need for model training,
    its performance remains sensitive to prompt design. Therefore, we approach the
    problem as a sequence classification task by fine-tuning a small language model
    as the router. Specifically, we annotate the LIMA dataset (Zhou et al., [2023](#bib.bib95))
    as the training set to train a RoBERTa model (Liu et al., [2019](#bib.bib44)),
    which has only 120M parameters. Comprehensive details regarding the annotation
    and training processes can be found in [§ C.2.1](#A3.SS2.SSS1 "C.2.1 Annotation
    Process ‣ C.2 Trained Router ‣ Appendix C Implementation Details of Skeleton-of-Thought
    with Router ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding") and [C.2.2](#A3.SS2.SSS2 "C.2.2 Training Details ‣ C.2 Trained Router
    ‣ Appendix C Implementation Details of Skeleton-of-Thought with Router ‣ Appendix
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 SoT-R Evaluation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We compare SoT and SoT-R under the same evaluation setup in [§ 3](#S3 "3 SoT
    Evaluation ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding").
    Besides the prompting and trained routers, we also consider a “human router” where
    we manually judge whether SoT should be applied for each question. This serves
    as a benchmark for comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Evaluation of Efficiency
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Fig. 8](#S4.F8 $$ speed-ups. (2) SoT-R with the trained router obtains slightly
    higher speed-ups for 7 out of 12 models on Vicuna-80, while SoT-R with the prompting
    router obtains higher speed-ups for all models on the WizardLM dataset (see [Fig. 17](#A7.F17
    "In G.2.1 Speed-up breakdown: models ‣ G.2 Skeleton-of-Thought with Router ‣ Appendix
    G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding") in [§ G.2](#A7.SS2 "G.2 Skeleton-of-Thought with Router
    ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding")).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/81058cf0bf571ec52aea14f2dc6089f2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Speed-ups of SoT and SoT-R on different models across all question
    categories of the Vicuna-80 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/98123e5836eadf78d2baf7e0c20a9e97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Net win rates of SoT and SoT-R on different question categories of
    the Vicuna-80 dataset (evaluated with the FastChat metrics).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Evaluation of Answer Quality
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Fig. 8](#S4.F8 "In 4.3.1 Evaluation of Efficiency ‣ 4.3 SoT-R Evaluation ‣
    4 SoT with Router (SoT-R): Adapatively Triggering SoT ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding") shows the net win rates (averaged across
    all models) of SoT and SoT-R on Vicuna-80 with the FastChat metrics (see [§ I.2](#A9.SS2
    "I.2 Skeleton-of-Thought with Router ‣ Appendix I Answer Quality Evaluation ‣
    Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    for results of the WizardLM dataset and LLMZoo metrics). We can see that: (1)
    SoT-R significantly improves the answer quality on questions where SoT is not
    suitable (e.g., coding, math, writing, fermi) by falling back to normal decoding.
    At the same time, SoT-R maintains answer quality improvements on questions where
    SoT is good at. (2) The trained router performs similar to (on Vicuna-80) or better
    than (on WizardLM; see [§ I.2](#A9.SS2 "I.2 Skeleton-of-Thought with Router ‣
    Appendix I Answer Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding")) the prompting router. This accords with our
    intuition in [§ 4.2](#S4.SS2 "4.2 Trained Router ‣ 4 SoT with Router (SoT-R):
    Adapatively Triggering SoT ‣ Skeleton-of-Thought: Large Language Models Can Do
    Parallel Decoding"). (3) The prompting and trained routers could even surpass
    human router (e.g., on roleplay questions; see more examples on WizardLM in [§ I.2](#A9.SS2
    "I.2 Skeleton-of-Thought with Router ‣ Appendix I Answer Quality Evaluation ‣
    Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We discuss the consistency across three routers in [§ C.3](#A3.SS3 "C.3 Router
    Consistency ‣ Appendix C Implementation Details of Skeleton-of-Thought with Router
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding").
    The primary takeaways include: (1) on Vicuna-80, there is a notable consistency
    among all three routers, and (2) on WizardLM, greater discrepancies emerge, with
    the trained router showing higher alignment with human annotations.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Related Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section positions SoT in related work to reveal how SoT (1) is connected
    to, (2) is different from, and (3) can harness the power of other methods. See
    [App. D](#A4 "Appendix D Related Work (Expanded) ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") for the expanded discussion.'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient LLM methods at model and system levels. At the model level, prior
    work proposes efficient architectures, including dynamic mixture-of-experts (Lepikhin
    et al., [2021](#bib.bib34)), low-complexity attention (Kitaev et al., [2020](#bib.bib29)),
    and multi-query attention (Shazeer, [2019](#bib.bib63)). However, they usually
    require a significant re-training cost. In contrast, compression methods require
    a smaller amount of fine-tuning cost by reducing the complexity of pre-trained
    LLMs, such as quantization (Frantar et al., [2022](#bib.bib19)) and weight or
    activation sparsification (Mishra et al., [2021](#bib.bib48); Zaheer et al., [2020](#bib.bib88)).
  prefs: []
  type: TYPE_NORMAL
- en: At the system level, prior work (1) optimizes the computational graph (Dao et al.,
    [2022](#bib.bib12)), (2) optimizes the assignment and scheduling of computational
    graph on devices (Sheng et al., [2023](#bib.bib65)), or (3) designs batching or
    caching mechanisms for serving multiple users (Fang et al., [2021](#bib.bib17)).
    These techniques address the large memory access and footprint posed by the vast
    model scale and attention mechanism, and mainly aim at enhancing the throughput
    rather than the end-to-end latency. As SoT trades off throughput for end-to-end
    latency, SoT can make these throughput-oriented techniques help with end-to-end
    latency. This interesting synergy offers opportunities for achieving better trade-offs
    between latency and throughput in future serving systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to model- and system-level techniques, SoT is a data-level technique
    in a new “content co-organization for efficiency” paradigm. See [§ 6](#S6 "6 Limitations,
    Future Work, and Open Questions ‣ Skeleton-of-Thought: Large Language Models Can
    Do Parallel Decoding") for more discussions.'
  prefs: []
  type: TYPE_NORMAL
- en: Efficient LLM methods through parallel generation. Some prior work also addresses
    the sequential decoding issues. Speculative decoding (SD) methods (Stern et al.,
    [2018](#bib.bib67)) employ smaller models to generate some consecutive tokens
    sequentially and apply the target LLMs to verify them parallelly. Non-autoregressive
    generation (NAG) methods (Gu et al., [2018](#bib.bib23); Xiao et al., [2023](#bib.bib82))
    sample and refine consecutive tokens parallelly, often with the support of a modified
    and tuned model.
  prefs: []
  type: TYPE_NORMAL
- en: Relying on either assisting models or special models and sampling schemes, SD
    and NAG methods conduct parallel verification or sampling and refinement of consecutive
    tokens. In contrast, SoT prompts the LLM *itself* to plan the contents in a way
    that permits the parallel generation of tokens in different segments, by exploiting
    the emerging instruction-following and planning ability of LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: Prompting methods for LLMs. Recent years have witnessed the emergence of the
    “pre-train, prompt, and predict” paradigm, which has shown promise in enhancing
    LLMs’ quality in math and commonsense reasoning (Wei et al., [2022](#bib.bib79);
    Kojima et al., [2022](#bib.bib30); Wang et al., [2022](#bib.bib76); Chen et al.,
    [2022](#bib.bib7)) and planning for multi-modality tasks (Shen et al., [2023](#bib.bib64);
    Zhu et al., [2023](#bib.bib97)). Instead of focusing on answer quality, SoT is
    a first attempt at exploiting the power of prompting to improve efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Limitations, Future Work, and Open Questions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Answer quality evaluation. Our answer quality evaluation is far from perfect
    due to the limited prompt set, the potential bias of GPT-4 judges, and the inherent
    difficulty of evaluating LLM generations. Currently, we did not conduct human
    evaluation since it is easy for a human to tell whether an answer is generated
    with SoT due to its distinctive pattern, which might cause evaluation bias. We
    leave a more thorough evaluation of answer quality to future work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eliciting or improving LLMs’ ability. [§ 3.2.4](#S3.SS2.SSS4 "3.2.4 Quality
    Breakdown: Metrics ‣ 3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") demonstrates SoT’s potential
    of enhancing answer quality. It is part of a broader trend in recent research,
    exemplified by work including CoT (Kojima et al., [2022](#bib.bib30); Wei et al.,
    [2022](#bib.bib79)), ToT (Yao et al., [2023](#bib.bib86)), and ReAct (Yao et al.,
    [2022](#bib.bib85)), which collectively affirm the notion that *explicitly articulating
    the thought process in language can elicit high-quality answers from LLMs*. These
    findings resemble human thinking: rather than relying solely on the first intuition
    or purely sequential thinking, we often document step-by-step reasoning or thought
    organization to attain high-quality answers. This intriguing parallel prompts
    us to explore further how we can draw from the human thinking process to facilitate
    more effective and efficient AI.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, SoT currently ignores the dependencies between points. A conceptually
    better way is to organize the points as *Graph-of-Thoughts*, where the edges represent
    the dependencies, and each point is decoded conditioned on the contents of its
    ancestor points. In addition, instead of complying with a *static* graph, we expect
    the need of having *dynamic Graph-of-Thoughts*, where the high-level thought structure
    is adjusted dynamically by LLMs themselves. This could potentially combine the
    efficiency and global thinking advantages of SoT with the logical reasoning and
    impromptu thinking strengths of methods like CoT (Kojima et al., [2022](#bib.bib30);
    Wei et al., [2022](#bib.bib79)). Notably, a contemporary work (Besta et al., [2023](#bib.bib2))
    has attempted to design Graph-of-Thoughts to elicit reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, there exist self-improving training pipelines (Zelikman et al.,
    [2022](#bib.bib89); Huang et al., [2022](#bib.bib26)) that use rationales generated
    by CoT to fine-tune LLMs, thereby enhancing their reasoning abilities. Likewise,
    it is interesting to investigate how the more structured answers from SoT can
    be used to fine-tune LLMs to enhance their ability to generate well-organized
    and comprehensive answers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Efficiency and overhead of SoT in different scenarios. Serving systems commonly
    adopt batch processing to handle concurrent queries. This raises a concern of
    whether SoT may hurt serving throughput due to parallel requests. (1) When there
    is an unsaturated number of concurrent queries, SoT can effectively reduce latency
    and enhance GPU utilization. Example scenarios include (a) Edge-side applications
    with a single user; (b) Centralized services during periods with unsaturated user
    requests and underutilized computing capacity. It is interesting to study the
    appropriate SoT triggering conditions based on system workloads. (2) When there
    is a saturated number of concurrent queries, SoT is still useful for improving
    answer quality. However, in this case, it is important to consider the computation
    overhead from SoT. We delve into this concern in [App. H](#A8 "Appendix H Overhead
    of SoT in Different Scenarios ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: 'For API-based models, a notable concern arises regarding the increased number
    of prefilling tokens ([App. H](#A8 "Appendix H Overhead of SoT in Different Scenarios
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")).
    Given that many APIs charge token usage, SoT may lead to higher costs. To address
    this, one can tune the number of parallel API requests (by expanding multiple
    points in a single API call), or use prompt tuning to design shorter SoT prompts
    (see [App. H](#A8 "Appendix H Overhead of SoT in Different Scenarios ‣ Appendix
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")).'
  prefs: []
  type: TYPE_NORMAL
- en: Data-centric efficiency optimization. While data-centric engineering for improving
    answer *quality* (Zha et al., [2023](#bib.bib90); HazyResearch, [2023](#bib.bib25))
    is gaining popularity, its potential for *inference efficiency* is not explored
    yet. SoT is the first attempt. As LLM capabilities and the amount of LLM-generated
    data are growing rapidly, data-centric techniques could become more useful in
    the future. We look forward to more explorations to unlock the full potential
    of data-centric efficiency optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We thank Sergey Yekhanin (Microsoft Research), and Tianji Wu (Infinigence AI)
    for their support and suggestions on the work. We thank Tianyu Fu for many initial
    discussions on the idea. We thank Ke Hong and Genghan Zhang for their discussions
    about profiling. We thank Yue Wu for the help on the Claude scripts. We thank
    Da Yu, Chulin Xie, and Saiqian Zhang for their suggestions on revising the first
    version of the paper. We thank Rui Hu, Cheng Cheng, Jack Jin, Zhoutong Ye, Mingze
    Sun, Jun Yan, Zhi Zhang, Yuxuan Tong, and Nianhui Guo for their suggestions on
    revising the second version of the paper.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Anthropic (2023) Anthropic. Introducing claude, May 2023. URL [https://www.anthropic.com/index/introducing-claude](https://www.anthropic.com/index/introducing-claude).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Besta et al. (2023) Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger,
    Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski,
    Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large
    language models. *arXiv preprint arXiv:2308.09687*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *Advances in neural information
    processing systems*, 33:1877–1901, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2019) Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song
    Han. Once-for-all: Train one network and specialize it for efficient deployment.
    *arXiv preprint arXiv:1908.09791*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chase (2022) Harrison Chase. LangChain, October 2022. URL [https://github.com/hwchase17/langchain](https://github.com/hwchase17/langchain).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2023a) Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste
    Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding
    with speculative sampling. *arXiv preprint arXiv:2302.01318*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen.
    Program of thoughts prompting: Disentangling computation from reasoning for numerical
    reasoning tasks. *arXiv preprint arXiv:2211.12588*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023b) Zhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding,
    and Yuan Xie. Dynamic n: M fine-grained structured sparse attention mechanism.
    In *Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice
    of Parallel Programming*, pp.  369–379, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023c) Zhihong Chen, Junying Chen, Hongbo Zhang, Feng Jiang, Guiming
    Chen, Fei Yu, Tiannan Wang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li,
    Xiang Wan, Haizhou Li, and Benyou Wang. Llm zoo: democratizing chatgpt. [https://github.com/FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo),
    2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dao et al. (2022) Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. Flashattention: Fast and memory-efficient exact attention with io-awareness.
    *Advances in Neural Information Processing Systems*, 35:16344–16359, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Denton et al. (2014) Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann LeCun,
    and Rob Fergus. Exploiting linear structure within convolutional networks for
    efficient evaluation. *Advances in neural information processing systems*, 27,
    2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ding et al. (2023) Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding
    Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by
    scaling high-quality instructional conversations. *arXiv preprint arXiv:2305.14233*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2022) Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers)*, pp.  320–335, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Elsken et al. (2019) Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural
    architecture search: A survey. *The Journal of Machine Learning Research*, 20(1):1997–2017,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fang et al. (2021) Jiarui Fang, Yang Yu, Chengduo Zhao, and Jie Zhou. Turbotransformers:
    an efficient gpu serving system for transformer models. In *Proceedings of the
    26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming*,
    pp.  389–402, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fedus et al. (2022) William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers:
    Scaling to trillion parameter models with simple and efficient sparsity. *The
    Journal of Machine Learning Research*, 23(1):5232–5270, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Frantar et al. (2022) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ganesh et al. (2021) Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan,
    Yin Yang, Hassan Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. Compressing
    large-scale transformer-based models: A case study on bert. *Transactions of the
    Association for Computational Linguistics*, 9:1061–1080, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gante (2023) Joao Gante. Assisted generation: a new direction toward low-latency
    text generation. [https://huggingface.co/blog/assisted-generation](https://huggingface.co/blog/assisted-generation),
    2023. Accessed: 2023-06-23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google (2021) Google. Tensorflow serving, 2021. URL [https://github.com/tensorflow/serving](https://github.com/tensorflow/serving).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2018) Jiatao Gu, James Bradbury, Caiming Xiong, Victor O.K. Li, and
    Richard Socher. Non-autoregressive neural machine translation. In *International
    Conference on Learning Representations*, 2018. URL [https://openreview.net/forum?id=B1l8BtlCb](https://openreview.net/forum?id=B1l8BtlCb).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Han et al. (2015) Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HazyResearch (2023) HazyResearch. Data-centric ai. [https://github.com/HazyResearch/data-centric-ai](https://github.com/HazyResearch/data-centric-ai),
    2023. Accessed: 2023-07-04.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2022) Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi
    Wang, Hongkun Yu, and Jiawei Han. Large language models can self-improve. *arXiv
    preprint arXiv:2210.11610*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2019) Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat,
    Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al.
    Gpipe: Efficient training of giant neural networks using pipeline parallelism.
    *Advances in neural information processing systems*, 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ivanov et al. (2021) Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li,
    and Torsten Hoefler. Data movement is all you need: A case study on optimizing
    transformers. *Proceedings of Machine Learning and Systems*, 3:711–732, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kitaev et al. (2020) Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer:
    The efficient transformer. *arXiv preprint arXiv:2001.04451*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. *Advances
    in neural information processing systems*, 35:22199–22213, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krishnamoorthi (2018) Raghuraman Krishnamoorthi. Quantizing deep convolutional
    networks for efficient inference: A whitepaper. *arXiv preprint arXiv:1806.08342*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky (2014) Alex Krizhevsky. One weird trick for parallelizing convolutional
    neural networks. *arXiv preprint arXiv:1404.5997*, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kwon et al. (2023) Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. *arXiv preprint
    arXiv:2309.06180*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lepikhin et al. (2021) Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao
    Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
    {GS}hard: Scaling giant models with conditional computation and automatic sharding.
    In *International Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=qrwe7XHTmYb](https://openreview.net/forum?id=qrwe7XHTmYb).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. *arXiv preprint arXiv:2104.08691*,
    2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leviathan et al. (2022) Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    inference from transformers via speculative decoding. *arXiv preprint arXiv:2211.17192*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii
    Khizbullin, and Bernard Ghanem. Camel: Communicative agents for ”mind” exploration
    of large scale language model society, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. *arXiv preprint arXiv:2101.00190*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023b) Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan
    Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval:
    An automatic evaluator of instruction-following models. [https://github.com/tatsu-lab/alpaca_eval](https://github.com/tatsu-lab/alpaca_eval),
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023c) Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang
    Lou, and Weizhu Chen. Making language models better reasoners with step-aware
    verifier. In *Proceedings of the 61st Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers)*, pp.  5315–5333, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao
    Zhang, Dawn Song, and Ion Stoica. Terapipe: Token-level pipeline parallelism for
    training large-scale language models. In *International Conference on Machine
    Learning*, pp. 6543–6552\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. *arXiv preprint arXiv:2306.00978*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:
    A robustly optimized bert pretraining approach, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov & Hutter (2019) Ilya Loshchilov and Frank Hutter. Decoupled weight
    decay regularization. In *International Conference on Learning Representations*,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2017) Wenyan Lu, Guihai Yan, Jiajun Li, Shijun Gong, Yinhe Han,
    and Xiaowei Li. Flexflow: A flexible dataflow accelerator architecture for convolutional
    neural networks. In *2017 IEEE International Symposium on High Performance Computer
    Architecture (HPCA)*, pp.  553–564\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miao et al. (2023) Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng,
    Zeyu Wang, Rae Ying Yee Wong, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar,
    and Zhihao Jia. Specinfer: Accelerating generative llm serving with speculative
    inference and token tree verification. *arXiv preprint arXiv:2305.09781*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mishra et al. (2021) Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko
    Stosic, Dusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Micikevicius. Accelerating
    sparse deep neural networks. *arXiv preprint arXiv:2104.08378*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Narayanan et al. (2019) Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek
    Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia.
    Pipedream: Generalized pipeline parallelism for dnn training. In *Proceedings
    of the 27th ACM Symposium on Operating Systems Principles*, pp.  1–15, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Narayanan et al. (2021) Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen,
    and Matei Zaharia. Memory-efficient pipeline-parallel dnn training. In *International
    Conference on Machine Learning*, pp. 7937–7947\. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2019) NVIDIA. Fastertransformer, 2019. URL [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NVIDIA (2021) NVIDIA. Triton inference server, 2021. URL [https://developer.nvidia.com/triton-inference-server](https://developer.nvidia.com/triton-inference-server).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, et al. Training language models to follow instructions with human feedback.
    *Advances in Neural Information Processing Systems*, 35:27730–27744, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phung (2023) Duy Phung. Stablevicuna-13b, May 2023. URL [https://huggingface.co/CarperAI/stable-vicuna-13b-delta](https://huggingface.co/CarperAI/stable-vicuna-13b-delta).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Press et al. (2022) Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A
    Smith, and Mike Lewis. Measuring and narrowing the compositionality gap in language
    models. *arXiv preprint arXiv:2210.03350*, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: International Conference for High Performance Computing, Networking,
    Storage and Analysis*, pp.  1–16\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2021) Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji
    Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. $\{$ model training.
    In *2021 USENIX Annual Technical Conference (USENIX ATC 21)*, pp.  551–564, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Santilli et al. (2023) Andrea Santilli, Silvio Severino, Emilian Postolache,
    Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodolà. Accelerating
    transformer inference for translation via parallel decoding. In *acl*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schick et al. (2023) Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu,
    Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer:
    Language models can teach themselves to use tools. *arXiv preprint arXiv:2302.04761*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SenseTime (2023a) SenseTime. Lightllm. [https://github.com/ModelTC/lightllm](https://github.com/ModelTC/lightllm),
    2023a. Accessed: 2023-09-26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SenseTime (2023b) SenseTime. Openppl. [https://github.com/openppl-public/ppl.nn](https://github.com/openppl-public/ppl.nn),
    2023b. Accessed: 2023-09-26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shazeer (2019) Noam Shazeer. Fast transformer decoding: One write-head is all
    you need. *arXiv preprint arXiv:1911.02150*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2023) Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming
    Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks with chatgpt and its friends
    in huggingface. *arXiv preprint arXiv:2303.17580*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sheng et al. (2023) Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max
    Ryabinin, Daniel Y Fu, Zhiqiang Xie, Beidi Chen, Clark Barrett, Joseph E Gonzalez,
    et al. High-throughput generative inference of large language models with a single
    gpu. *arXiv preprint arXiv:2303.06865*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2020) Taylor Shin, Yasaman Razeghi, Robert L Logan IV, Eric Wallace,
    and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. In *Proceedings of the 2020 Conference on Empirical Methods
    in Natural Language Processing (EMNLP)*, pp.  4222–4235, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stern et al. (2018) Mitchell Stern, Noam Shazeer, and Jakob Uszkoreit. Blockwise
    parallel decoding for deep autoregressive models. *Advances in Neural Information
    Processing Systems*, 31, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2023) Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami,
    Himanshu Jain, Felix Yu, Michael Riley, and Sanjiv Kumar. Spectr: Fast speculative
    decoding via optimal transport. In *Workshop on Efficient Systems for Foundation
    Models @ ICML2023*, 2023. URL [https://openreview.net/forum?id=d0mGsaheuT](https://openreview.net/forum?id=d0mGsaheuT).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2016) Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer
    vision. In *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, pp.  2818–2826, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. Alpaca: A strong,
    replicable instruction-following model. [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html),
    2023. Accessed: 2023-06-23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023a) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Touvron et al. (2023b) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem
    Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia
    Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou,
    Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem
    Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana
    Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
    Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan
    Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen
    Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng
    Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
    Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2:
    Open foundation and fine-tuned chat models, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. Openllms:
    Less is more for open-source models, July 2023a. URL [https://github.com/imoneoi/openchat](https://github.com/imoneoi/openchat).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021) Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient
    sparse attention architecture with cascade token and head pruning. In *2021 IEEE
    International Symposium on High-Performance Computer Architecture (HPCA)*, pp. 
    97–110\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and
    Hao Ma. Linformer: Self-attention with linear complexity. *arXiv preprint arXiv:2006.04768*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022) Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi,
    Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves
    chain of thought reasoning in language models. *arXiv preprint arXiv:2203.11171*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Zifu Wang, Teodora Popordanoska, Jeroen Bertels, Robin
    Lemmens, and Matthew B Blaschko. Dice semimetric losses: Optimizing the dice score
    with soft labels. In *Medical Image Computing and Computer Assisted Intervention*,
    2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2021) Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *arXiv preprint arXiv:2109.01652*, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei
    Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits
    reasoning in large language models. *Advances in Neural Information Processing
    Systems*, 35:24824–24837, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2016) Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
    Learning structured sparsity in deep neural networks. *Advances in neural information
    processing systems*, 29, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xiao et al. (2022) Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. *arXiv preprint arXiv:2211.10438*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. (2023) Yisheng Xiao, Lijun Wu, Junliang Guo, Juntao Li, Min Zhang,
    Tao Qin, and Tie-yan Liu. A survey on non-autoregressive generation for neural
    machine translation and beyond. *IEEE Transactions on Pattern Analysis and Machine
    Intelligence*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023) Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models
    to follow complex instructions. *arXiv preprint arXiv:2304.12244*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Yuanzhong Xu, HyoukJoong Lee, Dehao Chen, Blake Hechtman,
    Yanping Huang, Rahul Joshi, Maxim Krikun, Dmitry Lepikhin, Andy Ly, Marcello Maggioni,
    et al. Gspmd: general and scalable parallelization for ml computation graphs.
    *arXiv preprint arXiv:2105.04663*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2022) Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran,
    Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language
    models. *arXiv preprint arXiv:2210.03629*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2023) Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L
    Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem
    solving with large language models. *arXiv preprint arXiv:2305.10601*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2022) Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim,
    and Byung-Gon Chun. Orca: A distributed serving system for $\{$ generative models.
    In *16th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    22)*, pp.  521–538, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zaheer et al. (2020) Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua
    Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang,
    Li Yang, et al. Big bird: Transformers for longer sequences. *Advances in neural
    information processing systems*, 33:17283–17297, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zelikman et al. (2022) Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman.
    Star: Bootstrapping reasoning with reasoning. *Advances in Neural Information
    Processing Systems*, 35:15476–15488, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zha et al. (2023) Daochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang,
    Zhimeng Jiang, Shaochen Zhong, and Xia Hu. Data-centric artificial intelligence:
    A survey. *arXiv preprint arXiv:2303.10158*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhai et al. (2022) Yujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia,
    Shang Zhang, Zizhong Chen, Xin Liu, and Yibo Zhu. Bytetransformer: A high-performance
    transformer boosted for variable-length inputs. *arXiv preprint arXiv:2210.03052*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2023) Yifan Zhang, Jingqin Yang, Yang Yuan, and Andrew Chi-Chih
    Yao. Cumulative reasoning with large language models. *arXiv preprint arXiv:2308.04371*,
    2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zheng et al. (2022) Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng
    Chen, Yanping Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al.
    Alpa: Automating inter-and $\{$ parallelism for distributed deep learning. In
    *16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)*,
    pp.  559–578, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. (2023) Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao
    Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
    and chatbot arena, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2023) Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more
    for alignment, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022) Zhe Zhou, Xuechao Wei, Jiejing Zhang, and Guangyu Sun. $\{$
    transformers serving. In *2022 USENIX Annual Technical Conference (USENIX ATC
    22)*, pp.  489–504, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2023) Xizhou Zhu, Yuntao Chen, Hao Tian, Chenxin Tao, Weijie Su,
    Chenyu Yang, Gao Huang, Bin Li, Lewei Lu, Xiaogang Wang, et al. Ghost in the minecraft:
    Generally capable agents for open-world enviroments via large language models
    with text-based knowledge and memory. *arXiv preprint arXiv:2305.17144*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zoph & Le (2017) Barret Zoph and Quoc V. Le. Neural architecture search with
    reinforcement learning. In *International Conference on Learning Representations
    (ICLR)*, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \parttoc
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Model Details
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 1](#A1.T1 "In Appendix A Model Details ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") summarizes the models on which
    we evaluate SoT. We use GPT-4 in the main paper and ChatGPT-3.5 in [§ I.3](#A9.SS3
    "I.3 ChatGPT-3.5 as the Judge ‣ Appendix I Answer Quality Evaluation ‣ Appendix
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding") as the
    judge in FastChat and LLMZoo evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Model evaluated with SoT. All the open-source models are fine-tuned
    from LLaMA models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Access | Model Name | Institution | Released Date |'
  prefs: []
  type: TYPE_TB
- en: '| Open-Source | LLaMA2-Chat-7B (Touvron et al., [2023b](#bib.bib72)) | Meta
    & Microsoft | 2023/07 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-Chat-13B (Touvron et al., [2023b](#bib.bib72)) | Meta & Microsoft
    | 2023/07 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenChat-13B (Wang et al., [2023a](#bib.bib73)) | Tsinghua | 2023/07 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B V1.3 (Chiang et al., [2023](#bib.bib10)) | LMSYS | 2023/06 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13B V1.3 (Chiang et al., [2023](#bib.bib10)) | LMSYS | 2023/06 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-33B V1.3 (Chiang et al., [2023](#bib.bib10)) | LMSYS | 2023/06 |'
  prefs: []
  type: TYPE_TB
- en: '| StableVicuna-13B (Phung, [2023](#bib.bib55)) | CarperAI | 2023/05 |'
  prefs: []
  type: TYPE_TB
- en: '| UltraLM-13B (Ding et al., [2023](#bib.bib14)) | OpenBMB & Tsinghua | 2023/05
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Vicuna-7B V1.1 (Chiang et al., [2023](#bib.bib10)) | LMSYS | 2023/03 |'
  prefs: []
  type: TYPE_TB
- en: '| API-Based | Claude (Anthropic, [2023](#bib.bib1)) | Anthropic | 2023/05 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-3.5 | OpenAI | 2022/11 |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT-4 | OpenAI | 2023/03 |'
  prefs: []
  type: TYPE_TB
- en: '[Table 2](#A1.T2 "In Appendix A Model Details ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") shows sources of the models we
    use in the paper.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: The Hugging Face or API endpoints of the models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Access | Model Name | Hugging Face or API Endpoints |'
  prefs: []
  type: TYPE_TB
- en: '| Open-Source | LLaMA2-Chat-7B (Touvron et al., [2023b](#bib.bib72)) | meta-llama/Llama-2-7b-chat-hf
    |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-Chat-13B (Touvron et al., [2023b](#bib.bib72)) | meta-llama/Llama-2-13b-chat-hf
    |'
  prefs: []
  type: TYPE_TB
- en: '| OpenChat-13B (Wang et al., [2023a](#bib.bib73)) | openchat/openchat |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B V1.3 (Chiang et al., [2023](#bib.bib10)) | lmsys/vicuna-7b-v1.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13B V1.3 (Chiang et al., [2023](#bib.bib10)) | lmsys/vicuna-13b-v1.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-33B V1.3 (Chiang et al., [2023](#bib.bib10)) | lmsys/vicuna-33b-v1.3
    |'
  prefs: []
  type: TYPE_TB
- en: '| StableVicuna-13B (Phung, [2023](#bib.bib55)) | CarperAI/stable-vicuna-13b-delta²²2For
    convenience, we use the non-official endpoint TheBloke/stable-vicuna-13B-HF and
    TheBloke/UltraLM-13B-fp16 to get merged weights. |'
  prefs: []
  type: TYPE_TB
- en: '| UltraLM-13B (Ding et al., [2023](#bib.bib14)) | openbmb/UltraLM-13b[2](#footnote2
    "Footnote 2 ‣ Table 2 ‣ Appendix A Model Details ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") |'
  prefs: []
  type: TYPE_TB
- en: '|  | Vicuna-7B V1.1 (Chiang et al., [2023](#bib.bib10)) | lmsys/vicuna-7b-delta-v1.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| API-Based | Claude (Anthropic, [2023](#bib.bib1)) | Claude extension on Slack³³3[https://www.anthropic.com/claude-in-slack](https://www.anthropic.com/claude-in-slack)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-3.5 | Azure OpenAI, gpt-35-turbo 0301 version⁴⁴4[https://azure.microsoft.com/en-us/products/ai-services/openai-service](https://azure.microsoft.com/en-us/products/ai-services/openai-service)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | GPT-4 | OpenAI, gpt-4-0613 version |'
  prefs: []
  type: TYPE_TB
- en: Appendix B Implementation Details of Skeleton-of-Thought
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: B.1 Prompt
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The skeleton prompt is shown in [§ 2.1](#S2.SS1 "2.1 Method ‣ 2 Skeleton-of-Thought
    (SoT) ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    and [B.1](#A2.SS1 "B.1 Prompt ‣ Appendix B Implementation Details of Skeleton-of-Thought
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    and the point-expanding prompt is shown in [§ 2.1](#S2.SS1 "2.1 Method ‣ 2 Skeleton-of-Thought
    (SoT) ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt 3\.
    Skeleton Prompt Template $T^{s}$10 points. Question: What are the typical types
    of Chinese dishes? Skeleton: 1\. Dumplings. 2\. Noodles. 3\. Dim Sum. 4\. Hot
    Pot. 5\. Wonton. 6\. Ma Po Tofu. 7\. Char Siu. 8\. Fried Rice. Question: What
    are some practical tips for individuals to reduce their carbon emissions? Skeleton:
    1\. Energy conservation. 2\. Efficient transportation. 3\. Home energy efficiency.
    4\. Reduce water consumption. 5\. Sustainable diet. 6\. Sustainable travel. Now,
    please provide the skeleton for the following question. {question} Skeleton: {addmargin}[1em]2em[Assistant:]
    1.'
  prefs: []
  type: TYPE_NORMAL
- en: Skeleton prompt template. In order to make the output skeleton short and in
    a consistent format for the good of efficiency and ease of point extraction, the
    skeleton prompt template (1) describes the task precisely, and (2) provides a
    partial answer “1.” for the LLM to continue writing. The skeleton responses are
    in the desired format in most cases. Therefore, we can use a simple regular expression
    `(\d+)\.\s?([\s\S]+?)(?=\n|\n*$)` to extract point indexes and point skeletons
    from the skeleton response.
  prefs: []
  type: TYPE_NORMAL
- en: 'We find that GPT-4 can work well without the two demonstrations in the skeleton
    prompt. Therefore, we do not include the two demonstrations for GPT-4 ([§ 2.1](#S2.SS1
    "2.1 Method ‣ 2 Skeleton-of-Thought (SoT) ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding")). For all other models, the two demonstrations
    are included, as shown in [§ B.1](#A2.SS1 "B.1 Prompt ‣ Appendix B Implementation
    Details of Skeleton-of-Thought ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: Point-expanding prompt template. It describes the point-expanding task and provides
    a partial answer. We also provide instructions “Write it **very shortly** in 1$\sim$2
    sentence” so that the LLMs keep the answers concise. Unlike the skeleton prompt
    template, we find that demonstrations are not necessary to get reasonable results.
  prefs: []
  type: TYPE_NORMAL
- en: 'We find that Claude and GPT-4 follows the instruction “Write it **very shortly**
    in 1$\sim$2 sentence and do not continue with other points!” in [§ 2.1](#S2.SS1
    "2.1 Method ‣ 2 Skeleton-of-Thought (SoT) ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding") very well, so that the answers are very short.
    Therefore, we delete “**very shortly**” from the prompt template in Claude and
    GPT-4.'
  prefs: []
  type: TYPE_NORMAL
- en: Partial answer.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the [§ 2.1](#S2.SS1 "2.1 Method ‣ 2 Skeleton-of-Thought (SoT) ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") and [2.1](#S2.SS1 "2.1 Method
    ‣ 2 Skeleton-of-Thought (SoT) ‣ Skeleton-of-Thought: Large Language Models Can
    Do Parallel Decoding"), we provide partial answers so that LLMs can follow the
    desired response format better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can put the partial answer at the end of the prompt for the open-source
    models to continue writing. An implementation detail is that different open-source
    models have different conversation templates (i.e., different ways to combine
    user and assistant messages into one string). For example, Vicuna (Chiang et al.,
    [2023](#bib.bib10)) uses the string “USER:” and “ ASSISTANT:” for the placeholder
    “[User:]” and “[Role]” in the [§ 2.1](#S2.SS1 "2.1 Method ‣ 2 Skeleton-of-Thought
    (SoT) ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    and [2.1](#S2.SS1 "2.1 Method ‣ 2 Skeleton-of-Thought (SoT) ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding"), respectively, while UltraLM (Ding
    et al., [2023](#bib.bib14)) uses “User:” and “⟨/s⟩Assistant:”. We build our open-source
    model experiments with the help of the FastChat codebase (Zheng et al., [2023](#bib.bib94)),
    in which the conversation templates of many models are already handled correctly.
    We implement the conversation templates of OpenChat-13B, StableVicuna-13B, and
    UltraLM-13B according to their official guides and codes.'
  prefs: []
  type: TYPE_NORMAL
- en: For ChatGPT-3.5, we provide partial answers as a last message in the chat history
    from the assistant. Note that it is not a documented approach. We find it works
    well in most cases, in that ChatGPT-3.5 continues the texts from the provided
    partial answer. However, in some rare cases, ChatGPT-3.5 repeats the provided
    partial answers.
  prefs: []
  type: TYPE_NORMAL
- en: For Claude over Slack, there is no obvious way to give the API a partial answer.
    We resort to modifying the prompt template slightly by adding
  prefs: []
  type: TYPE_NORMAL
- en: Please start your answer from “{partial answer}” and do not output other things
    before that
  prefs: []
  type: TYPE_NORMAL
- en: at the end. We find that Claude understands and obeys it well. For GPT-4, we
    also take this approach.
  prefs: []
  type: TYPE_NORMAL
- en: System Message.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We do not include the system message in the prompts for open-source models except
    LLaMA2.
  prefs: []
  type: TYPE_NORMAL
- en: The partial answer, “**very shortly**”, and the 2-shot demonstrations discussed
    above are the only differences between the prompts we used across all models and
    all evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: B.2 Supporting Multi-Round Conversation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To use SoT in a multi-round conversation, we can just put the question and the
    final aggregated answer in the history, removing all the SoT prompts. In this
    way, using SoT in one conversation round will not introduce additional prefill
    cost in future rounds.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Implementation Details of Skeleton-of-Thought with Router
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: C.1 Prompting Router
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We use [§ C.1](#A3.SS1 "C.1 Prompting Router ‣ Appendix C Implementation Details
    of Skeleton-of-Thought with Router ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding") for querying GPT-4 as the router. If the answer
    is “A” (i.e., the question can be answered in a list of independent points), we
    will use SoT. Otherwise, if the answer is “B” (i.e., the answer is in a list of
    points but they depend on each other) or “C” (i.e., the answer should *not* be
    in a list of points), SoT is not suitable and we will fall back to normal decoding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prompt 4\.
    LLM Prompting as the Router {addmargin} [1em]2em[User:]
    Question: {question} How would you like to answer the question? A. Organize the
    answer as a list of points or perspectives (in the format of 1., 2., 3., etc.),
    and the points or perspectives can be answered independently without referring
    to the contents of the previous points. B. Organize the answer as a list of points
    or perspectives (in the format of 1., 2., 3., etc.), and the contents of later
    points or perspectives cannot be answered independently without referring to the
    contents of the previous ones. C. Do not organize the answer as a list of points
    or perspectives. Just say A, B, or C. Do not explain. Do not provide an answer
    to the question. {addmargin}[1em]2em[Assistant:]'
  prefs: []
  type: TYPE_NORMAL
- en: C.2 Trained Router
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We tackle the routing problem as a sequence classification task. We first annotate
    the LIMA training set (Zhou et al., [2023](#bib.bib95)), and then fine-tune a
    RoBERTa model (Liu et al., [2019](#bib.bib44)) using the labeled data. Finally,
    we apply the tuned RoBERTa as the router on Vicuna-80 and WizardLM. We detail
    the steps in the following.
  prefs: []
  type: TYPE_NORMAL
- en: C.2.1 Annotation Process
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the classification task, a label of 1 (positive) indicates that this question
    can be answered with SoT, while a label of 0 (negative) suggests that using the
    normal generation mode is more suitable. We annotate the LIMA training set, which
    consists of 1,030 Q&As sourced from three community webpages: Stack Exchange,
    wikiHow, and the Pushshift Reddit. We also annotate the Vicuna-80 and WizardLM
    datasets for evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use GPT-4 to assist the annotation process. Specifically, we present each
    question to GPT-4 and analyze its answer to determine whether SoT can be triggered
    for this question. We assign a positive label to a question if GPT-4’s response
    meets two criteria: (1) it contains a list of points that can be expanded in parallel,
    (2) each point provides sufficient details (i.e., the point-expanding response
    is not too short), which will enable SoT to achieve a speed-up. Two of the paper’s
    authors conduct the annotation process independently, and discuss the inconsistent
    annotations to decide the final label.'
  prefs: []
  type: TYPE_NORMAL
- en: C.2.2 Training Details
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: We use roberta-base with 120M parameters as the router model. The finetuning
    is conducted using the AdamW optimizer (Loshchilov & Hutter, [2019](#bib.bib45))
    with a weight decay of 0.01\. The learning rate undergoes a warm-up phase during
    the first 1% of iterations to 5e-5 and then decays linearly. We train the model
    for 2 epochs using a batch size of 32\. Input sequences are either padded or truncated
    to achieve a consistent length of 512 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: In the application of SoT, false positives (SoT is incorrectly triggered when
    it should not be, resulting in degraded answer quality) are of more significant
    concern than false negatives (the router misses a potential SoT trigger, resulting
    in a reduced speed-up). Thus, to mitigate false positives, we employ the Tversky
    loss (Wang et al., [2023b](#bib.bib77)) with parameters $\alpha=0.7$. Overall,
    the entire fine-tuning process is efficient, completing in 2 minutes on an NVIDIA
    A100 GPU.
  prefs: []
  type: TYPE_NORMAL
- en: C.3 Router Consistency
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We present the confusion matrices for the three routers to illustrate their
    consistency. The results on Vicuna-80 and WizardLM are shown in [Tables 4](#A3.T4
    "In C.3 Router Consistency ‣ Appendix C Implementation Details of Skeleton-of-Thought
    with Router ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding") and [4](#A3.T4 "Table 4 ‣ C.3 Router Consistency ‣ Appendix C Implementation
    Details of Skeleton-of-Thought with Router ‣ Appendix ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On Vicuna-80, we can observe a notable level of agreement among the three routers.
    Compared with the GPT-4-prompting router, the trained router exhibits a slightly
    higher number of false negatives w.r.t. the human annotations. Conversely, on
    WizardLM, given the intricate answer structure and the presence of many ambiguous
    cases, the routers show significant discrepancies. Specifically, the GPT-4 router
    produces many false positives, which pose adverse affects on the answer quality
    (see [§ I.2](#A9.SS2 "I.2 Skeleton-of-Thought with Router ‣ Appendix I Answer
    Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can
    Do Parallel Decoding")). The RoBERTa router aligns more closely with the human
    annotations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Router confusion matrices on the Vicuna-80 dataset. Left: Rows are
    human annotations (H) and columns are the GPT-4 router (G). Middle: Rows are human
    annotations (H) and columns are the RoBERTa router (R). Right: Rows are the GPT-4
    router (G) and columns are the RoBERTa router (R).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | G0 | G1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| H0 | 38 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| H1 | 0 | 37 |'
  prefs: []
  type: TYPE_TB
- en: '|  | R0 | R1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| H0 | 37 | 6 |'
  prefs: []
  type: TYPE_TB
- en: '| H1 | 5 | 32 |'
  prefs: []
  type: TYPE_TB
- en: '|  | R0 | R1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| G0 | 34 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| G1 | 8 | 34 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Router confusion matrices on the WizardLM dataset. Left: Rows are
    human annotations (H) and columns are the GPT-4 router (G). Middle: Rows are human
    annotations (H) and columns are the RoBERTa router (R). Right: Rows are the GPT-4
    router (G) and columns are the RoBERTa router (R).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | G0 | G1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| H0 | 94 | 66 |'
  prefs: []
  type: TYPE_TB
- en: '| H1 | 3 | 55 |'
  prefs: []
  type: TYPE_TB
- en: '|  | R0 | R1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| H0 | 135 | 25 |'
  prefs: []
  type: TYPE_TB
- en: '| H1 | 31 | 27 |'
  prefs: []
  type: TYPE_TB
- en: '|  | R0 | R1 |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| G0 | 93 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| G1 | 73 | 48 |'
  prefs: []
  type: TYPE_TB
- en: C.4 Concurrent execution for SoT-R
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In SoT-R, the router serves as an additional stage that extends the two-stage
    SoT pipeline. The SoT-R pipeline is illustrated in [Fig. 9](#A3.F9 "In C.4 Concurrent
    execution for SoT-R ‣ Appendix C Implementation Details of Skeleton-of-Thought
    with Router ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding"). To push the limit of latency optimization, we can run the router,
    normal generation, and SoT generation concurrently. Once the router makes a decision,
    one of the normal and SoT generation processes can be aborted. However, this approach
    will increase the token overhead. Therefore, we did not employ this approach in
    this work and leave it to future work.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d436bd37c1f85c7498273ec0a88faa3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Left: The SoT-R pipeline. Right: A possible approach to further reduce
    latency at the cost of token overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix D Related Work (Expanded)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: D.1 Efficient LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Extensive research has been dedicated to enhancing the throughput and latency
    of LLM inference. We first discuss model-level architecture design or compression
    techniques. These techniques change the model and can benefit both the latency
    and throughput but require finetuning to retain the model quality. Then, we discuss
    system-level efforts that optimize the computational graph or the assignment and
    scheduling of the computational graph on computation and storage devices. Most
    system-level efforts accelerate the prefilling phase or focus on improving the
    throughput. Finally, we discuss some research efforts that share a similar motivation
    to ours, namely, addressing the efficiency issue of sequential decoding.
  prefs: []
  type: TYPE_NORMAL
- en: Model-level optimization.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Considerable architectural design efforts have emerged to (1) improve the scalability
    w.r.t. model size by introducing mixture-of-expert inference (Lepikhin et al.,
    [2021](#bib.bib34); Fedus et al., [2022](#bib.bib18)), (2) address the quadratic
    complexity w.r.t. input size of attention by designing new attention mechanisms (Kitaev
    et al., [2020](#bib.bib29); Wang et al., [2020](#bib.bib75)), (3) reduce the memory
    access and footprint of attention by using multi-query attention (Shazeer, [2019](#bib.bib63)),
    and so on. However, these methods usually require a substantial re-training cost.
    The model compression techniques require a smaller amount of fine-tuning by reducing
    the model complexity of a pre-trained LLM from certain aspects (Ganesh et al.,
    [2021](#bib.bib20)). Representative techniques include quantization (Xiao et al.,
    [2022](#bib.bib81); Frantar et al., [2022](#bib.bib19); Lin et al., [2023](#bib.bib42)),
    the static or dynamic pruning of weights, activation, and attention (Mishra et al.,
    [2021](#bib.bib48); Zaheer et al., [2020](#bib.bib88); Wang et al., [2021](#bib.bib74);
    Chen et al., [2023b](#bib.bib8)), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Zooming out from LLM compression to the whole field of model compression, we
    can see that model co-design or compression for efficiency has received tremendous
    attention in the past few years and has grown into large research fields, such
    as pruning (Han et al., [2015](#bib.bib24); Wen et al., [2016](#bib.bib80)), quantization (Krishnamoorthi,
    [2018](#bib.bib31)), factorization (Denton et al., [2014](#bib.bib13)), and neural
    architecture search (Zoph & Le, [2017](#bib.bib98); Elsken et al., [2019](#bib.bib16);
    Cai et al., [2019](#bib.bib4)). Different from the model co-design paradigm, SoT
    is in a “content co-organization for efficiency” paradigm for improving the LLM
    efficiency. Along with the growth in the LLM capabilities and amount of LLM-generated
    data, data-level techniques could become important tools in the efficient LLM
    toolbox.
  prefs: []
  type: TYPE_NORMAL
- en: System-level optimization.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the realm of lossless acceleration, considerable efforts have been devoted
    to addressing the I/O-bound nature of LLMs on modern hardware platforms (Dao et al.,
    [2022](#bib.bib12)). Numerous studies (Dao et al., [2022](#bib.bib12); Zhai et al.,
    [2022](#bib.bib91); Ivanov et al., [2021](#bib.bib28); NVIDIA, [2019](#bib.bib51))
    have focused on adjusting the computational graph by fusing and implementing operations
    in an I/O-friendly way. As a representative method, FlashAttention (Dao et al.,
    [2022](#bib.bib12)) fuses all operations of one attention into one GPU kernel
    with spatially tiled computation to reduce the off-chip I/O of the attention map.
    While FlashAttention can effectively accelerate training and the prefilling phase
    of inference, it cannot accelerate the decoding phase much (when the batch size
    is small), as it is the I/O of weights rather than activation or attention map
    that bottlenecks the decoding phase. For example, when the context length is 64,
    decoding one token using LLaMA-7B needs to load each of the 7B parameters from
    the off-chip HBM onto the GPU chip at least once, but only transferring about
    20M (0.02B) activation values between the off-chip HBM and GPU chip.
  prefs: []
  type: TYPE_NORMAL
- en: In order to satisfy Service Level Objectives, serving systems focus on improving
    the serving throughput under latency constraints. To this end, serving systems (Fang
    et al., [2021](#bib.bib17); NVIDIA, [2021](#bib.bib52); Google, [2021](#bib.bib22))
    pack multiple queries together into a batch to improve the hardware utilization.
    The batching technique has proven highly effective in enhancing throughput, leading
    to the development of various variants. For example, some work designs methods
    to decide which queries to batch together (Fang et al., [2021](#bib.bib17); Zhou
    et al., [2022](#bib.bib96)), while others selectively batch parts of the model
    to enable fine-grained iteration-level batching (Yu et al., [2022](#bib.bib87))
    or multi-task batching (Zhou et al., [2022](#bib.bib96)). Various model parallelism (Lu
    et al., [2017](#bib.bib46); Huang et al., [2019](#bib.bib27); Narayanan et al.,
    [2019](#bib.bib49); Rajbhandari et al., [2020](#bib.bib57); Narayanan et al.,
    [2021](#bib.bib50); Li et al., [2021](#bib.bib41); Zheng et al., [2022](#bib.bib93))
    and offloading (Ren et al., [2021](#bib.bib58); Sheng et al., [2023](#bib.bib65))
    techniques have been proposed to maximize the throughput of LLM training or inference.
    In a nutshell, given the computational graph and device configurations, these
    techniques optimize the split, assignment, and scheduling of computations, storage,
    and communications on devices. In addition to the model parallelism and batching
    techniques, an efficient memory management mechanism for LLM workloads is also
    an essential feature in the serving systems (Kwon et al., [2023](#bib.bib33);
    SenseTime, [2023a](#bib.bib61); [b](#bib.bib62)).
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, these system-level techniques mainly help with the throughput in
    training and batched inference. They can be used by SoT to improve the throughput
    of the batched decoding of multiple segments. This means that SoT can harness
    the power of these throughput-oriented techniques and make them help with the
    end-to-end latency, offering a new dimension for better trading off latency and
    throughput in future serving systems.
  prefs: []
  type: TYPE_NORMAL
- en: Another parallelism perspective to position SoT is that SoT guides the LLM to
    adjust the sequential workload to become “inter-content” parallelizable, which
    differs from the parallelism levels in existing serving systems, including inter-instance (Krizhevsky,
    [2014](#bib.bib32); Rajbhandari et al., [2020](#bib.bib57)), inter-operation (Huang
    et al., [2019](#bib.bib27); Narayanan et al., [2019](#bib.bib49); [2021](#bib.bib50)),
    intra-operation (Xu et al., [2021](#bib.bib84)), and inter-token (Li et al., [2021](#bib.bib41)).
    It may be worthwhile to explore the integration of SoT into serving systems to
    maximize the hardware utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding optimization.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: One bottleneck for the end-to-end latency lies in the autoregressive decoding
    phase, where tokens must be generated one by one. Due to the dependency between
    tokens, the computation of different tokens cannot be parallelized, causing severe
    under-utilization of GPU. In order to improve the end-to-end decoding latency
    of a given LLM, speculative decoding methods (Stern et al., [2018](#bib.bib67);
    Leviathan et al., [2022](#bib.bib36); Chen et al., [2023a](#bib.bib6); Gante,
    [2023](#bib.bib21); Sun et al., [2023](#bib.bib68); Miao et al., [2023](#bib.bib47))
    propose to use cheaper approaches to generate short candidate token sequences,
    for example, by sequentially decoding with an assisting model much smaller than
    the given LLM. Then, they use the LLM to parallelly verify the candidates and
    keep the prefix sequence that matches the LLM’s verification results.
  prefs: []
  type: TYPE_NORMAL
- en: Another line of work that shares the motivation of addressing the autoregressive
    efficiency issue is non-autoregressive generation (NAG) methods (Gu et al., [2018](#bib.bib23);
    Xiao et al., [2023](#bib.bib82)). NAG methods sample consecutive tokens parallelly,
    often with the aid of a modified and tuned model. To maintain the answer quality,
    instead of sampling for one iteration, many NAG methods refine the output parallelly
    for multiple iterations (Xiao et al., [2023](#bib.bib82); Santilli et al., [2023](#bib.bib59)).
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the speculative decoding methods use assisting models for letting
    the LLM conduct parallel verification of consecutive tokens, and the NAG methods
    rely on specially designed models, training schemes, or sampling schemes for the
    parallel sampling and refinement of consecutive tokens. In contrast, SoT prompts
    the LLM itself to plan the contents in a way that permits the parallel generation
    of multiple tokens in different segments. SoT exploits the emerging instruction-following
    and planning ability of SoTA LLMs rather than relying on specially designed modeling,
    sampling, and training schemes. This is different from all existing work that
    targets the autoregressive efficiency issue.
  prefs: []
  type: TYPE_NORMAL
- en: D.2 Prompting Methods for LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent years, the “pre-train, prompt, and predict” paradigm has emerged (Liu
    et al., [2023](#bib.bib43)), which designs prompts comprising task descriptions
    and (optionally) a few demonstrations to guide pre-trained LLMs in generating
    answers for a wide range of downstream tasks. Researchers found that instruction-tuned
    LLMs (Brown et al., [2020](#bib.bib3); Wei et al., [2021](#bib.bib78); Ouyang
    et al., [2022](#bib.bib54); Chung et al., [2022](#bib.bib11); Taori et al., [2023](#bib.bib70))
    possess a strong ability to (1) generalize to new tasks thanks to the diverse
    natural language descriptions encountered during instruction tuning, and (2) learn
    in-context using a few demonstrations without weight tuning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In virtue of these abilities, the field has been manually engineering (Brown
    et al., [2020](#bib.bib3); Kojima et al., [2022](#bib.bib30); Shen et al., [2023](#bib.bib64);
    Li et al., [2023a](#bib.bib37)), automatic searching (Shin et al., [2020](#bib.bib66)),
    or continuously tuning (Li & Liang, [2021](#bib.bib38); Lester et al., [2021](#bib.bib35))
    the prompts for uncovering the capabilities of LLMs on downstream tasks. There
    are a bunch of prompting methods that improves the reasoning performance of LLMs
    by designing thinking flows mimicking human reasoning: (1) mimicking the step-by-step
    or compositional thinking structure (Wei et al., [2022](#bib.bib79); Kojima et al.,
    [2022](#bib.bib30); Press et al., [2022](#bib.bib56); Yao et al., [2023](#bib.bib86);
    Besta et al., [2023](#bib.bib2); Zhang et al., [2023](#bib.bib92)), (2) designing
    multiple reasoning paths and their aggregation (Wang et al., [2022](#bib.bib76);
    Yao et al., [2023](#bib.bib86); Li et al., [2023c](#bib.bib40)), and (3) using
    tools for calculation and information retrieval (Chen et al., [2022](#bib.bib7);
    Yao et al., [2022](#bib.bib85); Schick et al., [2023](#bib.bib60)). As a representative
    example, the Chain-of-Thought prompts largely improve the performance on tasks
    that require logical reasoning by simply providing a “Let’s think step by step” (Kojima
    et al., [2022](#bib.bib30)) instruction or a few demonstrations (Wei et al., [2022](#bib.bib79)).
    Another topic that arises quite a surge of interests is to prompt LLMs to help
    finish complex multi-modality task (Shen et al., [2023](#bib.bib64); Zhu et al.,
    [2023](#bib.bib97)). For example, HuggingGPT (Shen et al., [2023](#bib.bib64))
    design prompts to guide the LLM to generate structural JSON for the orchestration
    of multi-model execution to finish complex tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the large literature on prompting methods has been aiming at uncovering
    different capabilities of LLM and improving the answer quality on different downstream
    tasks. In contrast, SoT is a first attempt at exploiting the power of prompting
    to improve efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix E Efficiency Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section gives a detailed explanation on why SoT can reduce the overall
    decoding latency with the same computational resource for local models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The vanilla approach processes only one question and decodes the answers sequentially,
    whereas SoT processes multiple point-expanding requests and the answers in a batch.
    We focus on the following question: “Compared to processing only one sequence,
    how much peak memory overhead and latency increase will be brought by processing
    a batch of sequences?”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: The latency and average GPU performance of the prefilling and decoding
    phases when inferencing LLMs. The prefilling token length is 128, the decoding
    token length is 64, and the batch size is 1\. The test is run on one NVIDIA A100
    GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Prefill/Decode Latency (ms) | Prefill/Decode GPU Perf. (TFLOPS) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-7B | 40 / 2735 | 43 / 0.31 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-13B | 54 / 3725 | 62 / 0.44 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA-33B | 100 / 5506 | 85 / 0.75 |'
  prefs: []
  type: TYPE_TB
- en: 'A typical LLM generative process consists of two phases: (1) the prefilling
    phase in which the prompt is parsed to generate the key-value cache for further
    use, and (2) the decoding phase in which tokens are generated one by one in a
    sequential manner. The decoding phase accounts for the majority of the end-to-end
    latency, especially when generating a long response. As shown in [Table 5](#A5.T5
    "In Appendix E Efficiency Analysis ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding"), when running Vicuna-7B on NVIDIA A100-80G,
    the actual computing performance is only 0.31 TFLOPS (0.1% utilization) in the
    decoding phase, compared to 43 TFLOPS (13.8% utilization) during prefilling. The
    utilization is calculated with respect to the FP16⁵⁵5All of our experiments are
    run with FP16 inference. tensor core peak performance – 312 TFLOPS for NVIDIA-A100\.
    As a result, the latency of decoding only one token is comparable to that of prefilling
    128 tokens (40ms). This huge gap in actual computing performance and thereby the
    latency arises from the fact that all LLM weights need to be loaded onto the GPU
    chip at least once only for decoding one token, so the decoding is heavily bottlenecked
    by the I/O of weights and the GPU computation units cannot be well utilized.'
  prefs: []
  type: TYPE_NORMAL
- en: When conducting batched decoding, as the sequence batch size $B$.
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the peak memory overhead, the amount of LLM weights can be one to two
    orders of magnitude larger than that of all the intermediate activations as long
    as the prefilling token length is not too large, not to mention that most activations
    do not need to be saved for back-propagation during inference. Therefore, the
    LLM weights account for the majority of the memory footprint in our test cases.
    Consequently, as shown in [Fig. 10(c)](#A5.F10.sf3 "In Figure 10 ‣ Appendix E
    Efficiency Analysis ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can
    Do Parallel Decoding"), the peak memory overhead due to the increasing size of
    the KV cache and activation grows at a slow pace as the batch size $B$ increases.
    Thanks to the small peak memory overhead, in all of our experiments, we managed
    to use one GPU to run SoT without seeking help from other peak memory optimization
    techniques (e.g., quantization (Frantar et al., [2022](#bib.bib19); Lin et al.,
    [2023](#bib.bib42)), offloading (Sheng et al., [2023](#bib.bib65))).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d00a3bfdc6887770a40e51cb05019f0d.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/005070482ed3214c3c201ecd951ea4b4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/853c3d03cf3fe730568f5152832f7c16.png)'
  prefs: []
  type: TYPE_IMG
- en: (c)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: The trends of latency, average GPU performance of decoding one token,
    and peak memory with respect to the batch size $B$ of sequences. The prefilling
    token length is 128, and the decoding token length is 64\. The test is run on
    one NVIDIA A100 GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix F Efficiency Profiling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We run the profiling on the target GPU (NVIDIA A100-80G and NVIDIA RTX 3090)
    with CUDA 11.7, using the Hugging Face transformer library 4.28.1 and PyTorch
    2.0.1\. The host of A100-80G has an Intel Xeon Platinum 8358P CPU and 1T memory.
    The host of RTX 3090 has an Intel Xeon Gold 6246R CPU and 512G memory.
  prefs: []
  type: TYPE_NORMAL
- en: Latency profiling and estimation.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For the decoding phase, we denote $t_{B}^{D}(k)$ tokens can be estimated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where the subscripts $i$.
  prefs: []
  type: TYPE_NORMAL
- en: 'The SoT decoding process consists of two stages: the skeleton stage and the
    point-expanding stage. Denoting the token length of the skeleton request and skeleton
    response as $l_{i}^{s}$, we can compute the latency of the skeleton and point-expanding
    stages as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle L^{s}(l_{i}^{s},l_{o}^{s})$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle L^{pe}(l_{i}^{pe},l_{o}^{pe},B)$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: Using the latency profiling table, we can further estimate the average GPU computing
    performance in FLOPS (i.e., FLOPs per second) of decoding $l_{o}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $1$2 |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $f_{B}^{D}(k)$.
  prefs: []
  type: TYPE_NORMAL
- en: Memory profiling and evaluation.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To evaluate the peak memory, we use `torch.cuda.max_memory_allocated` to record
    the memory consumption of prefilling sequences of different lengths and decoding
    with different context lengths and a batch size ranging from 1 to 16\. Then, we
    calculate the peak memory of each stage as the maximum value of the prefilling
    and decoding phases, and calculate the overall peak memory of SoT as the maximum
    value of the skeleton and point-expanding stages.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix G Efficiency Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: G.1 Skeleton-of-Thought
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: G.1.1 Detailed Statistics of Token Lengths and Point Numbers
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b859b8ec19c75530a7644a49f1519be6.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) The number of points $B$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/398b269c39e0feeaa1dd006d8f72c5bf.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The normal answer length.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fc0effdbb1ab01b76597112c2c9f31ae.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) The maximum point-expanding response length.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cbce559509476324b84fac1527c80a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) The ratio of the maximum point-expanding response length to the normal answer
    length.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e8b3cb0a85d125f2070477e9a855c62.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) The imbalance degree of point-expanding response lengths (standard deviation
    of point token lengths).
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0d3adb1619d15c76bca2f82c3dbbccd.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) The ratio of the final SoT answer length to the normal answer length.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: The statistics of the token lengths and point numbers on the Vicuna-80
    dataset. Each row corresponds to one question category, and each column corresponds
    to one model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'G.1.2 Latency Breakdown: SoT Stages and Phases'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Fig. 12](#A7.F12 "In G.1.2 Latency Breakdown: SoT Stages and Phases ‣ G.1
    Skeleton-of-Thought ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") presents the absolute latencies
    of normal and SoT generations on Vicuna-80. Again, the speed-ups of SoT compared
    with normal generation is evident. We can see that the decoding phases predominantly
    account for the end-to-end latency. Consequently, although SoT has higher prefilling
    latency in the skeleton stage than the normal generation and introduces additional
    point-expanding prefilling latency – which is expected – this has negligible impact
    on the overall latency and thereby the overall speed-up.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/563feea799109cecf3b4e1907c3e0b3f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/51551eef0ea8ce9dc15e49f56e003c8e.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 12: The latency breakdown of SoT and normal generations on the Vicuna-80
    dataset. For open-source models, the latency breakdown of the prefilling and decoding
    phases is shown in different colors. For API-based models, we do not record such
    latency breakdown information; the bar labeled as “(decode)” indicates the overall
    latency of prefilling and decoding phases.'
  prefs: []
  type: TYPE_NORMAL
- en: G.1.3 Efficiency Evaluation on NVIDIA RTX 3090
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'We present the SoT speed-ups and latency breakdown on RTX 3090 in [Fig. 13](#A7.F13
    "In G.1.3 Efficiency Evaluation on NVIDIA RTX 3090 ‣ G.1 Skeleton-of-Thought ‣
    Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding"). We test the three 7B models, as their FP16-precision
    version can be run on an RTX 3090 GPU without further peak memory optimization
    techniques such as weight quantization (Frantar et al., [2022](#bib.bib19); Lin
    et al., [2023](#bib.bib42)) or offloading (Sheng et al., [2023](#bib.bib65)).
    On these three models, SoT can obtain 1.94$\times$ speed-up on average on Vicuna-80.'
  prefs: []
  type: TYPE_NORMAL
- en: For the five question categories that SoT can provide high-quality answers (i.e.,
    knowledge, common-sense, generic, roleplay, counterfactual), SoT can speed-up
    the overall answer generation process by 1.96$\times$ by calculating the speed-up
    across the three math questions, SoT does not reduce the absolute latency of processing
    the three questions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8adec3ee7e8237446405baedd15a4cbd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The latency breakdown of SoT and normal decoding on the Vicuna-80
    dataset. The average speed-up across questions are also marked on the figure.'
  prefs: []
  type: TYPE_NORMAL
- en: G.1.4 Actual Latency Testing
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'This section reports the actual SoT speed-up on the Vicuna-80 with batch testing
    (instead of analyzing with pre-made profiling tables), using a single NVIDIA A100
    GPU. We test the actual end-to-end latency of the SoT and normal decoding with
    the 9 open-source models. For each model, we run the speed-up test for five times
    and plot the box in [Fig. 14](#A7.F14 "In G.1.4 Actual Latency Testing ‣ G.1 Skeleton-of-Thought
    ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in [Fig. 14(a)](#A7.F14.sf1 "In Figure 14 ‣ G.1.4 Actual Latency Testing
    ‣ G.1 Skeleton-of-Thought ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding"), the current SoT solution obtains
    a 
    in the meantime.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2c81bc71b4e56b28f190298dccf2066e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Average speed-up on different models.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f4fd644a51aaa7414c37a506b0252044.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Average speed-up on different question categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 14: Speed-ups on 9 open-source models on the Vicuna-80 dataset with
    actual batch testing.'
  prefs: []
  type: TYPE_NORMAL
- en: G.2 Skeleton-of-Thought with Router
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The overhead brought by the router inference is relatively small: On the Vicuna-80
    dataset, the prompting and trained router have an average latency of 0.65s (0.39s$\sim$2.52s),
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'G.2.1 Speed-up breakdown: models'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Fig. 15](#A7.F15 "In G.2.1 Speed-up breakdown: models ‣ G.2 Skeleton-of-Thought
    with Router ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") shows the speed-ups of SoT-R
    on different models on the Vicuna-80 dataset. [Fig. 16](#A7.F16 "In G.2.1 Speed-up
    breakdown: models ‣ G.2 Skeleton-of-Thought with Router ‣ Appendix G Efficiency
    Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding") and [Fig. 17](#A7.F17 "In G.2.1 Speed-up breakdown: models ‣ G.2 Skeleton-of-Thought
    with Router ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") show the speed-ups of SoT-R on
    different models on the WizardLM dataset. We can observe that on Vicuna-80, the
    two methods yield similar speed-ups, whereas on WizardLM, GPT-4 prompting router
    usually obtains higher speed-ups than the trained router, especially on GPT-4
    itself.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c632d555fccc13d5d1e08423d1d09e6f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Average speed-up across all question categories with prompting router.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5e4801e3a4a6257b6edf089823921a84.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Average speed-up across all question categories with trained router.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 15: Speed-ups of SoT-R on different models on Vicuna-80 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f5b48ace9d8fc20f1dd46ab10fdbd987.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Average speed-up across all question categories with prompting router.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/36a0c98d34eece60c355052546c58571.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Average speed-up across all question categories with trained router.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 16: Speed-ups of SoT-R on different models on WizardLM dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d397e9dd2f5d5cef00b68736e2e6ff69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Speed-ups of SoT and SoT-R on different models on the WizardLM dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'G.2.2 Speed-up breakdown: categories'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: '[Fig. 18](#A7.F18 "In G.2.2 Speed-up breakdown: categories ‣ G.2 Skeleton-of-Thought
    with Router ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") and [Fig. 19](#A7.F19 "In G.2.2
    Speed-up breakdown: categories ‣ G.2 Skeleton-of-Thought with Router ‣ Appendix
    G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding") show the speed-ups of SoT-R on different question categories
    of Vicuna-80 dataset. The trained router achieves slightly higher speed-up on
    most of the categories (except for knowledge, writing, and fermi). [Fig. 20](#A7.F20
    "In G.2.2 Speed-up breakdown: categories ‣ G.2 Skeleton-of-Thought with Router
    ‣ Appendix G Efficiency Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language
    Models Can Do Parallel Decoding") and [Fig. 21](#A7.F21 "In G.2.2 Speed-up breakdown:
    categories ‣ G.2 Skeleton-of-Thought with Router ‣ Appendix G Efficiency Evaluation
    ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding")
    show the speed-ups of SoT-R on different question categories of WizardLM dataset.
    We can observe that on 19 out of 29 categories, using the prompting router achieves
    higher speed-ups than using the trained router.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/89676c1054c938f128d6ff5483f6d984.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Speed-ups of SoT-R with prompting router on different question categories.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6dc6ac1e92a664f12869f3da3fb97a8d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Speed-ups of SoT-R with trained router on different question categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18: Speed-ups of SoT-R on different question categories of Vicuna-80
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/55762cc3e787403ba05df0b3262d3bdb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Speed-ups of SoT and SoT-R on different question categories of the
    Vicuna-80 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6d5af59024b7dd297a0ab2a863718ce1.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Speed-ups of SoT-R with prompting router on different question categories.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6cd2ba8a04f8e6beb2d0f0fd2d1c9515.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Speed-ups of SoT-R with trained router on different question categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 20: Speed-ups of SoT-R on different question categories of WizardLM
    dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49be8183ee2b22d10d514d00dd036e52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Speed-ups of SoT and SoT-R on different question categories of the
    WizardLM dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Appendix H Overhead of SoT in Different Scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Despite the optimizations made to the decoding phase, SoT brings overhead to
    the prefilling phase as the model needs to handle additional SoT prompts. [Table 6](#A8.T6
    "In Appendix H Overhead of SoT in Different Scenarios ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding") reports SoT’s prefilling overhead
    for the API-based models. These statistics are averaged across the Vicuna-80 questions
    that are suitable for SoT (according to our manual annotation). We can see that
    SoT significantly increases the number of prefilling tokens. This is because that
    SoT issues an independent point-expanding request for each point, with the average
    number of points being 6.8 on Vicuna-80 dataset across all evaluated models. Consequently,
    the APIs need to prefill the point-expanding request multiple times.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: SoT’s prefilling token overhead for API-based models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Prefill Phase |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Normal | SoT Stage 1 | SoT Stage 2 | Ratio (SoT / Normal) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Claude | 12.52 | 171.41 | 808.91 | 78.30 |'
  prefs: []
  type: TYPE_TB
- en: '| ChatGPT-3.5 | 12.52 | 171.41 | 591.31 | 60.92 |'
  prefs: []
  type: TYPE_TB
- en: '| GPT-4 | 12.52 | 171.41 | 983.09 | 92.21 |'
  prefs: []
  type: TYPE_TB
- en: 'When using SoT to serve the open-source models, a simple and small trick is
    to prefill the common prefix of point-expanding requests with a batch size of
    1 during Stage 2 (i.e., the point-expanding stage). [Table 7](#A8.T7 "In Appendix
    H Overhead of SoT in Different Scenarios ‣ Appendix ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding") shows the prefilling overhead after
    applying the trick. Although the ratio is considerably smaller compared to that
    of the API-based models, this computational overhead remains a concern, especially
    during periods of high system workload.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some possibilities to further reduce the token and computational
    overhead that are worth exploring in future work. To name a few: (1) When using
    SoT in serving systems, we can simply reuse the key-value cache containing the
    question and skeleton from Stage 1 during Stage 2, rather than re-prefilling them
    as in a multi-round conversation. (2) Generally, as LLM capabilities continue
    to evolve and prompt tuning techniques advance (Shin et al., [2020](#bib.bib66);
    Li & Liang, [2021](#bib.bib38); Lester et al., [2021](#bib.bib35)), the possibility
    of using much shorter prompts to activate the SoT mode in the future holds promise,
    which would significantly mitigate the token or computational overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: SoT’s computational overhead (in terms of the number of prefilling
    tokens) for open-source models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Prefill Phase |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Naive | SoT Stage 1 | SoT Stage 2 | Ratio (SoT / Normal) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-Chat-7B | 12.52 | 171.41 | 216.49 | 30.98 |'
  prefs: []
  type: TYPE_TB
- en: '| LLaMA2-Chat-13B | 12.52 | 171.41 | 216.41 | 30.98 |'
  prefs: []
  type: TYPE_TB
- en: '| OpenChat-13B | 12.52 | 171.41 | 234.38 | 32.41 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B V1.3 | 12.52 | 171.41 | 211.61 | 30.59 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-13B V1.3 | 12.52 | 171.41 | 273.39 | 35.53 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-33B V1.3 | 12.52 | 171.41 | 258.88 | 34.37 |'
  prefs: []
  type: TYPE_TB
- en: '| StableVicuna-13B | 12.52 | 171.41 | 312.03 | 38.61 |'
  prefs: []
  type: TYPE_TB
- en: '| UltraLM-13B | 12.52 | 171.41 | 290.88 | 36.92 |'
  prefs: []
  type: TYPE_TB
- en: '| Vicuna-7B V1.1 | 12.52 | 171.41 | 196.64 | 29.40 |'
  prefs: []
  type: TYPE_TB
- en: Appendix I Answer Quality Evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: I.1 Skeleton-of-Thought
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'I.1.1 Answer Analysis: Different Models'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Models on which SoT has low net win rates. We observe two types of reasons for
    the low net win rates.
  prefs: []
  type: TYPE_NORMAL
- en: '*(1) OpenChat-13B, Vicuna-7B V1.1, and LLaMA2-Chat-13B.* For the weak models
    such as OpenChat-13B and Vicuna-7B V1.1, they are not able to follow the SoT prompts
    precisely. For OpenChat-13B and LLaMA2-Chat-13B, sometimes the skeleton contains
    undesired contents (e.g., completing the skeleton points as shown below).'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    How many lightning strikes occur on Earth each day? Try to explain your answer.
    Your explanation should take the reader through your reasoning step-by-step.
    \IfEqCase correct✓incorrect✗   SoT’s answer (OpenChat-13B) (Orange
    texts are the skeleton.) {addmargin}[1em]2em*(Only showing the skeleton)* 1.Introduction.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. The Earth’s atmosphere.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Lightning formation.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Electrical discharge.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Lightning strikes per second.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Calculating the total.
  prefs: []
  type: TYPE_NORMAL
- en: 'Skeleton:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction.
  prefs: []
  type: TYPE_NORMAL
- en: Lightning is a beautiful and awe-inspiring natural phenomenon, but it can also
    be dangerous. It occurs when positively charged particles in a thundercloud discharge
    to the negatively charged Earth. The process is still not fully understood, but
    scientists have been able to estimate how many lightning strikes occur on Earth
    each day.
  prefs: []
  type: TYPE_NORMAL
- en: 'Skeleton:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. The Earth’s atmosphere.
  prefs: []
  type: TYPE_NORMAL
- en: The Earth’s atmosphere plays a crucial role in the formation of lightning. It
    consists of different layers, with the troposphere being the lowest and most active.
    The troposphere is where most of the weather occurs, and it is where lightning
    primarily forms. The atmosphere’s temperature, humidity, and wind patterns all
    contribute to the likelihood of a lightning strike.
  prefs: []
  type: TYPE_NORMAL
- en: 'Skeleton:'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Lightning formation.
  prefs: []
  type: TYPE_NORMAL
- en: When warm, moist air rises, it cools and forms water *¡more texts are omitted…¿*
  prefs: []
  type: TYPE_NORMAL
- en: For both OpenChat-13B and Vicuna-7B V1.1, sometimes they do not write anything
    in the point-expanding stage when details are needed.
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    As a superhero, how would you explain your origin story to a curious child?
    \IfEqCase correct✓incorrect✗   SoT’s answer (Vicuna-7B V1.1) (Orange
    texts are the skeleton.) {addmargin}[1em]2em1\. Born with extraordinary abilities.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Trained to use powers for the greater good.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Protecting the innocent and fighting villains.I use my powers to defend
    the defenseless and bring justice to those who have been wronged.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Inspired by courage, justice, and selflessness.
  prefs: []
  type: TYPE_NORMAL
- en: These two issues result in bad net win rates.
  prefs: []
  type: TYPE_NORMAL
- en: '*(2) Claude.* For strong models such as Claude, we do not see major issues
    with SoT’s answers except the common issues across all models discussed in [§ 3.2.3](#S3.SS2.SSS3
    "3.2.3 Quality Breakdown: Question Categories ‣ 3.2 Evaluation of Answer Quality
    ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding"). We hypothesize that the low net win rates are majorly due to the fact
    that their normal sequential generation is already very good.'
  prefs: []
  type: TYPE_NORMAL
- en: Models on which SoT has high net win rates. The models with high net win rates
    (Vicuna-13B V1.3, StableVicuna-13B, and UltraLM-13B) are between the above two
    extremes. On one hand, they are good enough to understand SoT prompts. While we
    observe that they sometimes have the same issues as OpenChat-13B and Vicuna-7B
    V1.1 discussed before, these issues happen much less frequently. On the other
    hand, their normal sequential generation performance has a larger room for improvement
    than Claude and ChatGPT-3.5 so that the benefits from SoT are more visible on
    these models.
  prefs: []
  type: TYPE_NORMAL
- en: 'I.1.2 Answer Analysis: Different Question Categories'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Categories on which SoT has low net win rates.
  prefs: []
  type: TYPE_NORMAL
- en: '*(1) Math.* Math questions require step-by-step thinking. Without knowing the
    previous steps, it is hard to derive the following steps. Chain-of-Thoughts (Kojima
    et al., [2022](#bib.bib30); Wei et al., [2022](#bib.bib79)) emphasize the importance
    of expanding the sequential thinking steps to solve these problems successfully.
    In contrast, SoT requires the models to (a) come up with the skeleton of the solution
    first and (b) deduce each individual step independently without referring to previous
    results. Both of these are challenging tasks. Strong models are able to get (a)
    correct but fail at (b). In the following example, ChatGPT-3.5 gets the skeleton
    steps correct. However, without knowing the results from the first step, the model
    starts making mistakes in the second step.⁷⁷7One additional (minor) issue of the
    API-based ChatGPT-3.5 is that the skeleton point “1.” is repeated in the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Math) Solve for x in the equation 3x + 10 = 5(x - 2). \IfEqCase incorrectcorrect✓incorrect✗   SoT’s answer (ChatGPT-3.5)
    (Orange texts are the skeleton.) {addmargin}[1em]2em1\. Move all x terms to one
    side. 1\. Move all x terms to one side by subtracting 5x from both sides, giving
    -2x + 10 = -10.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Move all constant terms to the other side. This can be done by subtracting
    5x - 10 from both sides, leaving 3x - 5x = -10 - 10.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Simplify the equation. Combine like terms on both sides of the equation
    to get 3x + 10 = 5x - 10.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Divide both sides by the coefficient of x. Divide both sides by 2 to get
    x = -5.
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Check the solution by plugging it back into the original equation.Substitute
    the value of x obtained in step 4 back into the original equation and verify that
    both sides are equal. \IfEqCase correct correct✓
    incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2em3x + 10 = 5(x - 2)
    can be simplified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 3x + 10 = 5x - 10 (distribute the 5 on the right side)
  prefs: []
  type: TYPE_NORMAL
- en: 10 + 10 = 5x - 3x (subtract 3x from both sides)
  prefs: []
  type: TYPE_NORMAL
- en: 20 = 2x (combine like terms on the right side)
  prefs: []
  type: TYPE_NORMAL
- en: x = 10 (divide both sides by 2)
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the solution for x in the equation 3x + 10 = 5(x - 2) is x = 10.
  prefs: []
  type: TYPE_NORMAL
- en: For weaker models, it is hard to even get step (a) correct. For example, as
    shown below, the third step “Apply FOIL” in Vicuna-13B V1.3’s SoT solution comes
    out of nowhere. This makes SoT’s solution incorrect (though the solution from
    the normal generation is also incorrect).
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Math) Solve for x in the equation 3x + 10 = 5(x - 2). \IfEqCase incorrectcorrect✓incorrect✗   SoT’s answer (Vicuna-13B
    V1.3) (Orange texts are the skeleton.) {addmargin}[1em]2em1\. Simplify equation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can simplify the equation by combining like terms:'
  prefs: []
  type: TYPE_NORMAL
- en: 3x + 10 = 5(x - 2)
  prefs: []
  type: TYPE_NORMAL
- en: =¿ 3x + 10 = 5x - 10
  prefs: []
  type: TYPE_NORMAL
- en: =¿ x + 5 = 2
  prefs: []
  type: TYPE_NORMAL
- en: =¿ x = 2 - 5
  prefs: []
  type: TYPE_NORMAL
- en: =¿ x = -3
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Factor 5(x - 2).
  prefs: []
  type: TYPE_NORMAL
- en: 5(x - 2) = 5x - 10.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Apply FOIL.
  prefs: []
  type: TYPE_NORMAL
- en: '* Multiply the binomials in the equation, using the first term of each binomial
    for the first term of the product, the outside term for the second term of the
    product, the second term of each binomial for the third term of the product, and
    the remaining terms for the fourth term of the product.'
  prefs: []
  type: TYPE_NORMAL
- en: '* 3(x - 2) + 10 = 5x + 5(-2)'
  prefs: []
  type: TYPE_NORMAL
- en: '* 3x - 6 = 5x - 10'
  prefs: []
  type: TYPE_NORMAL
- en: '* 3x = 5x - 10'
  prefs: []
  type: TYPE_NORMAL
- en: '* x = 5x - 10 / 3'
  prefs: []
  type: TYPE_NORMAL
- en: '* x = 5/3x - 10/3'
  prefs: []
  type: TYPE_NORMAL
- en: '* x = 2/3x - 5/3'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Solve for x.
  prefs: []
  type: TYPE_NORMAL
- en: After applying the FOIL method and simplifying the equation, we get 3x = 17\.
    Therefore, x = 17/3. \IfEqCase incorrect correct✓
    incorrect✗  Normal answer (Vicuna-13B V1.3) {addmargin}[1em]2emTo solve for x
    in the equation 3x + 10 = 5(x - 2), we need to first distribute the 5 to multiply
    both sides by 5.
  prefs: []
  type: TYPE_NORMAL
- en: 3x + 10 = 5(x - 2)
  prefs: []
  type: TYPE_NORMAL
- en: 5(3x + 10) = 5(x - 2)
  prefs: []
  type: TYPE_NORMAL
- en: 15x + 50 = 5x - 10
  prefs: []
  type: TYPE_NORMAL
- en: 'Add 5x to both sides:'
  prefs: []
  type: TYPE_NORMAL
- en: 20x + 50 = 5x
  prefs: []
  type: TYPE_NORMAL
- en: 'Subtract 5x from both sides:'
  prefs: []
  type: TYPE_NORMAL
- en: 15x = 0
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide both sides by 15:'
  prefs: []
  type: TYPE_NORMAL
- en: x = 0
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the solution to the equation is x = 0.
  prefs: []
  type: TYPE_NORMAL
- en: '*(2) Fermi.* Similar to math questions, questions in the fermi category require
    solvers to make assumptions and calculate the answer step-by-step. For example,
    to answer the question in the figure below, the solver is required to make an
    assumption about when life began on Earth. In step 4, ChatGPT-3.5 does not know
    that it has already assumed 3.5 billion years in step 3 and uses the wrong number
    4.54 billion in the calculation. In step 5, ChatGPT-3.5 does not know that it
    has already made the above assumptions and refuses to answer “due to the uncertainties
    in estimating the age of the Earth”. In contrast, normal generation is able to
    create a consistent solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Fermi) How many times has the Earth orbited the Sun since the beginning of life?
    Try to explain your answer. Your explanation should take the reader through your
    reasoning step-by-step. \IfEqCase incorrectcorrect✓incorrect✗
      SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) {addmargin}[1em]2em1\.
    Estimate the age of the Earth. The age of the Earth is estimated to be around
    4.54 billion years, based on radiometric dating of rocks and meteorites.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Determine when life began on Earth. Life on Earth is estimated to have begun
    around 3.5 billion years ago, based on the discovery of ancient microfossils.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Calculate the number of years since life began. The oldest known fossils
    suggest that life on Earth began around 3.5 billion years ago. Therefore, the
    number of years since life began on Earth is approximately 3.5 billion years.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Divide the number of years by the length of one orbit. Assuming an orbital
    period of 365.25 days, the Earth has orbited the Sun approximately 4.54 billion
    / 365.25 = 12,425,000 times since the beginning of life.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. The Earth has orbited the Sun approximately X times since the beginning
    of life. Unfortunately, it is impossible to give a specific number for the amount
    of times the Earth has orbited the Sun since the beginning of life due to the
    uncertainties in estimating the age of the Earth and when life first emerged.
    \IfEqCase correct✓ incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2emThe
    beginning of life on Earth is estimated to have occurred around 3.5 to 4 billion
    years ago. Since that time, the Earth has been orbiting the Sun continuously.
  prefs: []
  type: TYPE_NORMAL
- en: The length of one orbit, or one year, is approximately 365.25 days. This means
    that the Earth completes one full orbit around the Sun every 365.25 days.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the number of times the Earth has orbited the Sun since the beginning
    of life, we can divide the total number of days by the length of one orbit:'
  prefs: []
  type: TYPE_NORMAL
- en: Total number of days since the beginning of life = (3.5 billion years) x (365
    days per year) = 1.2775 trillion days
  prefs: []
  type: TYPE_NORMAL
- en: Number of orbits = (1.2775 trillion days) / (365.25 days per orbit) = 3.5 billion
    orbits
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the Earth has orbited the Sun approximately 3.5 billion times since
    the beginning of life.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the following example shows, there are also cases when SoT solves the question
    (with minor computation errors) by embedding the computation steps in the skeleton
    directly in the skeleton stage. The point-expanding stage will then add little
    content (except some undesired repetition of the following skeletons occasionally).
    This happens both on small/weak models and large/powerful models. Although this
    behavior is not what we originally expect, it sheds light on a promising direction
    for making SoT a general framework: in the problem where step-by-step thinking
    is needed, the SoT model automatically falls back to the sequential generation
    mode by outputting the complete answer in the skeleton stage and skipping the
    point-expanding stage. Future work needs to be done to control and encourage this
    behavior better.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Fermi) How many times does the average human blink in a lifetime? Try to explain
    your answer. Your explanation should take the reader through your reasoning step-by-step.
    \IfEqCase correct✓incorrect✗   SoT’s answer (Vicuna-7B V1.1) (Orange
    texts are the skeleton.) {addmargin}[1em]2em1\. The average human blinks 12-15
    times per minute.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. There are 60 minutes in an hour.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Therefore, the average human blinks 60 x 12-15 = 720-900 times per hour.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. There are 24 hours in a day.Therefore, the average human blinks 720-900
    x 24 = 17,520-22,400 times per day.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Therefore, the average human blinks 720-900 x 24 = 17,520-22,400 times per
    day.This translates to blinking approximately 50-65 times per hour, which is a
    significant portion of the day.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. There are 365 days in a year.Therefore, the average human blinks 17,520-22,400
    x 365 = 5,857,600-8,124,000 times per year.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Therefore, the average human blinks 17,520-22,400 x 365 = 5,857,600-8,124,000
    times per year.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. The human lifespan is typically 70-80 years.Therefore, the average human
    blinks 80,000-112,000 times in their lifetime.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Therefore, the average human blinks 5,857,600-8,124,000 / 70-80 = 80,000-112,000
    times in their lifetime. \IfEqCase correct✓ incorrect✗  Normal
    answer (Vicuna-7B V1.1) {addmargin}[1em]2emThe average human blinks about 12-15
    times per minute, which translates to about 18,200 to 20,400 blinks per day. Over
    the course of a lifetime, the number of blinks is estimated to be around 7,500,000
    to 8,000,000.
  prefs: []
  type: TYPE_NORMAL
- en: Blinking is a reflexive action that helps to keep the eyes moist and comfortable,
    as well as to protect the eyes from dust and other foreign particles. The frequency
    of blinking can vary depending on a variety of factors, including age, health,
    and environmental conditions. For example, people who work in dusty or dry environments
    may blink more frequently than those who work in a clean, moist environment.
  prefs: []
  type: TYPE_NORMAL
- en: Over the course of a lifetime, the average person will spend approximately 27,375
    days or 700,000 hours awake. This means that the majority of the time spent blinking
    will occur in the first few decades of life, with the frequency of blinking gradually
    decreasing as people age.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that there is a significant amount of individual variation
    in the frequency of blinking. Some people may blink more or less than the average,
    depending on a variety of factors such as age, health, and environmental conditions.
    Additionally, blinking patterns can be affected by factors such as stress, fatigue,
    and certain medical conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '*(3) Coding.* While in most cases, the models are able to decompose the coding
    problem into smaller tasks in the skeleton stage, the generation quality during
    the point-expanding stage is bad, potentially due to the fact that we did not
    carefully craft a special point-expanding prompt for coding. The failure cases
    can be grouped into two cases. (a) In some cases, the model only generates a description
    of how implementation can be done without giving the code.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Coding) Develop a C++ program that reads a text file line by line and counts
    the number of occurrences of a specific word in the file.
    \IfEqCase correct✓incorrect✗   SoT’s answer (Claude) (Orange texts
    are the skeleton.) {addmargin}[1em]2em 1\. Open file stream by declaring an ifstream
    object and using the open() method to open the file in read mode.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Read line by line from the file stream into a string using getline().
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Find word by searching each line for the target word using string::find
    and keeping track of matches.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Increment counter by one each time the specified word is encountered in
    the current line.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Close file
  prefs: []
  type: TYPE_NORMAL
- en: The file stream should be closed after finishing reading the file to free up
    resources. This can be done by calling the close() method on the file stream object.
  prefs: []
  type: TYPE_NORMAL
- en: This might be due to the fact that the powerful models (e.g., Claude) follow
    the point-expanding prompt “Write it **very shortly** in 1$\sim$2 sentence” precisely.
    Better prompts tailed to coding could help. (b) In some other cases, the model
    tends to generate the complete code repeatedly in several points.
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Coding) Develop a C++ program that reads a text file line by line and counts
    the number of occurrences of a specific word in the file.
    \IfEqCase correct✓incorrect✗   SoT’s answer (OpenChat-13B) (Orange
    texts are the skeleton.) {addmargin}[1em]2em1\. Read file line by line.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Store lines in a container.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Count occurrences of specific word.
  prefs: []
  type: TYPE_NORMAL
- en: “‘cpp
  prefs: []
  type: TYPE_NORMAL
- en: '*¡more texts are omitted…¿*'
  prefs: []
  type: TYPE_NORMAL
- en: “‘
  prefs: []
  type: TYPE_NORMAL
- en: This code uses the C++ standard library to read a text file line by line, store
    each line in a container, and count the occurrences of a specific word. The results
    are printed.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Print results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s the C++ program that reads a text file line by line and counts the number
    of occurrences of a specific word in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: “‘cpp
  prefs: []
  type: TYPE_NORMAL
- en: '*¡more texts are omitted…¿*'
  prefs: []
  type: TYPE_NORMAL
- en: “‘
  prefs: []
  type: TYPE_NORMAL
- en: Please note that this is a very basic implementation. You may need to adjust
    it depending on your specific requirements.
  prefs: []
  type: TYPE_NORMAL
- en: This problem might also be alleviated by prompts that explicitly tell the model
    to only generate a “code snippet” in each skeleton point.
  prefs: []
  type: TYPE_NORMAL
- en: Although better prompt engineering could lead to better results, we expect that
    SoT is not suitable for the coding questions in Vicuna-80\. The reason is that
    the solutions to the problems in Vicuna-80 are usually in a few lines with strong
    dependencies between each other. Without knowing the previously defined variable
    names or imported libraries, it is hard (or even impossible) to implement the
    subsequent code correctly. As a consequence, generating different parts of the
    answers in parallel is not suitable here. Similar to the math questions, automatically
    falling back to outputting all the code in the skeleton stage and not triggering
    the point-expanding stage might be more suitable for answering this question type.
  prefs: []
  type: TYPE_NORMAL
- en: However, we expect that SoT could be helpful for larger coding tasks that involve
    multiple modules (e.g., functions, classes). The skeleton stage could be in charge
    of defining the interfaces between different modules (e.g., functionalities, names,
    parameters, return types). The point-expanding stage could be in charge of implementing
    these modules or using these modules to complete the final task, which can be
    done in parallel. This mimics the common practice in software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: '*(4) Writing.* The writing questions usually ask for an email, a blog post,
    or a review in a given scenario. In the detailed evaluation output from FastChat
    and LLMZoo metrics, the major and common complaint is that SoT’s answers are (slightly)
    less detailed. This could potentially be improved by point-expanding prompts that
    request more details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'While inspecting SoT’s answers manually, we observe one additional issue. Writing
    problems usually require a coherent passage *without* embedded skeleton points,
    whereas our current SoT pipeline ([§ 2](#S2 "2 Skeleton-of-Thought (SoT) ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding")) concatenates skeleton points
    as part of the answer. One example is shown below.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Writing) Can you help me write a formal email to a potential business partner
    proposing a joint venture? \IfEqCase correct✓incorrect✗
      SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) {addmargin}[1em]2em1\.
    Introduce yourself and your company. Dear [Recipient],
  prefs: []
  type: TYPE_NORMAL
- en: I hope this email finds you well. My name is [Your Name] and I am the [Your
    Position] at [Your Company Name].
  prefs: []
  type: TYPE_NORMAL
- en: 2\. State the purpose of the email. The purpose of this email is to propose
    a joint venture between our companies.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Explain the potential benefits of the joint venture. By combining our resources
    and expertise, we can increase market share and profitability for both companies.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Provide details on the proposed collaboration. Our proposed collaboration
    would involve combining our expertise in marketing with your strong manufacturing
    capabilities to create a new line of products.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Suggest a follow-up meeting or call. Please let me know your availability
    for a follow-up call next week to discuss this opportunity further.
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Thank the recipient for their time and consideration.Thank you for considering
    this proposal and we look forward to hearing back from you soon.
    \IfEqCase correct✓ incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2emOf
    course, I can help you with that. Here’s a sample email you can use as a guide:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subject: Proposal for Joint Venture'
  prefs: []
  type: TYPE_NORMAL
- en: Dear [Business Partner’s Name],
  prefs: []
  type: TYPE_NORMAL
- en: I hope this email finds you well. I am writing to propose a joint venture between
    our companies, [Your Company Name] and [Potential Partner Company Name]. We believe
    that our respective expertise can create a successful partnership that will benefit
    both of our businesses.
  prefs: []
  type: TYPE_NORMAL
- en: We have conducted thorough research on your company and admire your innovative
    approach to [insert industry or niche]. We think our [insert expertise or specialty]
    could complement your [insert potential partner’s specialty or expertise]. By
    collaborating on this joint venture, we believe we can create a new and innovative
    product that will appeal to our respective markets.
  prefs: []
  type: TYPE_NORMAL
- en: We are confident that our companies would work well together and that our joint
    venture would be successful. We would like to set up a meeting to discuss the
    specifics of our proposal and explore how we can move forward with the joint venture.
  prefs: []
  type: TYPE_NORMAL
- en: Thank you for considering this proposal. We look forward to hearing from you
    soon.
  prefs: []
  type: TYPE_NORMAL
- en: Best regards,
  prefs: []
  type: TYPE_NORMAL
- en: '[Your Name]'
  prefs: []
  type: TYPE_NORMAL
- en: In such cases, simply removing the skeleton points would greatly improve the
    answer quality. To make SoT more general, one future direction is to let the LLM
    itself decide whether the point index and point skeleton should be included to
    make the final answer more natural and fluent.
  prefs: []
  type: TYPE_NORMAL
- en: Categories on which SoT has high net win rates.
  prefs: []
  type: TYPE_NORMAL
- en: '*(1) Counterfactual, knowledge, common sense, generic.* All these four categories
    have the same characteristic: the ideal answer should cover several relatively
    *independent* points. Having the LLM generate a skeleton before expanding the
    details can result in more comprehensive discussions for the question. Besides,
    organizing the answers as a list of points makes the answers easier to read, whereas
    the answers from normal generation are sometimes less structured and less readable.
    Below, we give one example for each question category to illustrate these points.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Counterfactual) What if the Internet had been invented during the Renaissance
    period? \IfEqCase correct✓incorrect✗
      SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) {addmargin}[1em]2em1\.
    Advancements in communication.The invention of the Internet during the Renaissance
    period would have revolutionized communication, allowing for faster and more efficient
    exchange of ideas and information across vast distances.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Increased access to knowledge.The invention of the printing press during
    the Renaissance period already allowed for greater access to knowledge, but the
    internet would have revolutionized this even further by providing instantaneous
    access to information from all over the world.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Improved dissemination of ideas.If the internet had been invented during
    the Renaissance period, the spread of ideas and knowledge would have been much
    faster and widespread than it was at the time. This could have led to a faster
    progression of scientific and artistic innovations.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Impact on art and literature.The invention of the internet during the Renaissance
    period would have revolutionized the way art and literature are created, shared,
    and consumed, leading to a more diverse and globally-influenced artistic and literary
    scene.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Potential for global trade.The invention of the internet during the Renaissance
    period would have greatly expanded global trade opportunities, allowing for easier
    communication and collaboration between merchants and traders across continents.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Influence on political and social structures.The invention of the internet
    during the Renaissance period could have led to significant changes in political
    and social structures, allowing for greater democratization and communication
    among people from different regions and classes. \IfEqCase
    correct✓ incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2emIf the Internet
    had been invented during the Renaissance period, it would have had a significant
    impact on the way people communicated, shared ideas, and gained knowledge. The
    Renaissance was a time of great intellectual and artistic growth, and the Internet
    would have facilitated the spread of these ideas and innovations.
  prefs: []
  type: TYPE_NORMAL
- en: Scholars and artists would have been able to share their work more easily, and
    people from different parts of the world would have been able to connect and collaborate
    more easily. The Internet would have also made it easier to access information,
    which could have accelerated the pace of scientific and technological advancements.
  prefs: []
  type: TYPE_NORMAL
- en: However, it’s important to note that the infrastructure required for the Internet
    to exist did not exist during the Renaissance period. There were no telecommunications
    networks or electricity grids, so the idea of the Internet would have been impossible
    to realize. Nonetheless, it’s interesting to think about how the Renaissance might
    have been different if the Internet had existed at that time.Question:   
    (Knowledge) What are some potential implications of using a single-use plastic
    bottle versus a reusable bottle on both the environment and human health?
    \IfEqCase correct✓incorrect✗   SoT’s answer (ChatGPT-3.5) (Orange
    texts are the skeleton.) {addmargin}[1em]2em1\. Single-use plastic waste.Single-use
    plastic bottles contribute to the massive amount of plastic waste that ends up
    in landfills and oceans, harming wildlife and ecosystems.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Pollution of waterways.Single-use plastic bottles contribute to the pollution
    of waterways, as they often end up in rivers, lakes, and oceans, causing harm
    to aquatic life and the overall ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Harmful chemicals leaching.Single-use plastic bottles may release harmful
    chemicals such as bisphenol A (BPA) and phthalates into the water or liquid they
    contain, posing a risk to human health.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Greenhouse gas emissions.The production and disposal of single-use plastic
    bottles contribute to greenhouse gas emissions, which contribute to climate change.
    Using a reusable bottle reduces the need for constant production and disposal,
    therefore reducing greenhouse gas emissions.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Microplastic ingestion.Single-use plastic bottles contribute to the accumulation
    of microplastics in the environment, which can be ingested by humans through food
    and water, leading to potential health risks.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Increased risk of disease.Single-use plastic bottles can harbor harmful
    bacteria, leading to an increased risk of disease if not disposed of properly.
    Reusable bottles, when cleaned regularly, pose a lower risk.
    \IfEqCase correct✓ incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2emUsing
    a single-use plastic bottle has significant implications on the environment and
    human health.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental implications:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Plastic pollution: Single-use plastic bottles are one of the major contributors
    to plastic pollution in oceans and landfills. They take hundreds of years to decompose,
    and during that time, they release toxic chemicals and microplastics into the
    environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Carbon emissions: Manufacturing and transporting single-use plastic bottles
    require a significant amount of energy and resources, which contribute to greenhouse
    gas emissions and climate change.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Natural resource depletion: The production of single-use plastic bottles
    requires large amounts of water and oil, which are finite resources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Health implications:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Chemical exposure: Single-use plastic bottles contain chemicals such as
    Bisphenol A (BPA) and phthalates, which can leach into the water and pose a risk
    to human health.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Microplastics ingestion: When plastic bottles break down, they release
    microplastics, which can enter the food chain and be ingested by humans.'
  prefs: []
  type: TYPE_NORMAL
- en: Using a reusable bottle, on the other hand, has a positive impact on the environment
    and human health.
  prefs: []
  type: TYPE_NORMAL
- en: 'Environmental benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Reduced plastic waste: Reusable bottles significantly reduce the amount
    of plastic waste generated.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Lower carbon emissions: Reusable bottles have a lower carbon footprint
    as they can be used multiple times.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Reduced resource depletion: The production of reusable bottles requires
    fewer resources than single-use plastic bottles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Health benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Chemical-free: Reusable bottles are typically made from materials that
    are free from harmful chemicals, reducing the risk of chemical exposure.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Reduced microplastics: Reusable bottles do not break down easily, reducing
    the release of microplastics into the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, using a reusable bottle is a more sustainable and healthier option
    than using a single-use plastic bottle.Question:    (Common sense) How can
    you determine if a person is genuinely interested in a conversation or simply
    being polite? \IfEqCase correct✓incorrect✗
      SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) {addmargin}[1em]2em1\.
    Pay attention to body language.Notice if their posture is open or closed, if they
    are facing towards you or away, and if they are fidgeting or relaxed. These can
    all indicate their level of interest in the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Listen for engaged responses.If the person is genuinely interested in the
    conversation, they will respond with thoughtful and relevant comments or questions
    rather than short, generic responses.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Observe eye contact.If the person maintains steady eye contact and their
    gaze is focused on you, they are likely genuinely interested in the conversation.
    On the other hand, if they frequently look away or seem distracted, they may just
    be being polite.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Look for follow-up questions.If the person asks follow-up questions and
    seems genuinely interested in what you’re saying, then they are likely interested
    in the conversation and not just being polite.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Notice tone of voice.A person who is genuinely interested in a conversation
    will often have a more enthusiastic and animated tone of voice, compared to someone
    who is simply being polite and giving short, monotone responses.
    \IfEqCase correct✓ incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2emAs
    an AI assistant, I cannot determine a person’s true intentions or feelings. However,
    there are some social cues that can indicate whether a person is genuinely interested
    in a conversation or not. For example, if they maintain eye contact, lean in,
    nod their head and ask follow-up questions, it can be a sign that they are engaged
    in the conversation. On the other hand, if they avoid eye contact, give short
    or vague responses, or seem distracted, it may indicate that they are not as interested.
    Ultimately, it’s important to remember that everyone has different communication
    styles, and it can be difficult to accurately interpret someone’s intentions without
    directly asking them.Question:    (Generic) How can
    I improve my time management skills? \IfEqCase
    correct✓incorrect✗   SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.)
    {addmargin}[1em]2em1\. Set clear goals. Setting clear goals is important in improving
    time management skills as it helps you to focus on what needs to be done and what
    can be accomplished within a certain timeframe.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Prioritize tasks. Make a list of tasks and prioritize them based on their
    importance and deadline. This will help you focus on the most critical tasks and
    avoid wasting time on less important ones.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Create a schedule. Creating a schedule helps you allocate time for each
    task and ensures that you complete them within the allotted time.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Minimize distractions. Distractions such as social media, emails, and phone
    notifications can waste a lot of time. Try to limit them by turning off notifications
    or setting specific times to check them.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Take breaks. Taking breaks can actually improve productivity and help prevent
    burnout. It’s important to schedule short breaks throughout the day to recharge
    and refocus.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Learn to say no. It’s important to prioritize your own tasks and not take
    on too much at once. Saying no to requests that don’t align with your goals or
    schedule can help you manage your time more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Delegate tasks. If you have too many tasks to handle, consider delegating
    some of them to others who can help you. This will allow you to focus on more
    important tasks and manage your time more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '8\. Use time management tools.There are a variety of time management tools
    available, such as calendars, to-do lists, and productivity apps, that can help
    you stay organized and on track with your tasks. \IfEqCase
    correct✓ incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2emImproving
    time management skills can be a challenge, but there are several strategies that
    can help. Here are a few:'
  prefs: []
  type: TYPE_NORMAL
- en: '1\. Set clear goals and priorities: Identify your most important tasks and
    make sure to complete them first.'
  prefs: []
  type: TYPE_NORMAL
- en: '2\. Make a schedule: Create a schedule that includes your most important tasks
    and set aside time for them.'
  prefs: []
  type: TYPE_NORMAL
- en: '3\. Use a timer: Set a timer for a specific amount of time to help you stay
    focused and avoid distractions.'
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Take breaks: Take short breaks throughout the day to help you recharge
    and avoid burnout.'
  prefs: []
  type: TYPE_NORMAL
- en: '5\. Eliminate distractions: Try to limit distractions as much as possible,
    such as by turning off your phone or closing your email.'
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Delegate tasks: If possible, delegate tasks to others to free up time for
    more important activities.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, improving time management skills takes practice and perseverance.
    Be patient with yourself and keep trying until you find strategies that work well
    for you.
  prefs: []
  type: TYPE_NORMAL
- en: '*(2) Roleplay.* Roleplay questions require LLMs to assume a character and present
    an introduction or speech to other people. LLMZoo and FastChat metrics usually
    favor the level of details of SoT when assigning it a better score. Similar to
    the writing category discussed before, SoT’s answers could be even better if the
    skeleton points are removed from the answer.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    (Roleplay) How would you introduce yourself as a medieval knight at a royal banquet?
    \IfEqCase correct✓incorrect✗   SoT’s answer (Claude) (Orange texts
    are the skeleton.) {addmargin}[1em]2em1\. Name and house. My name is Sir Percival
    of Camelot, knight of the Round Table and protector of King Arthur’s realm.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Lands and titles. I hold fiefdom over the lands of Westmarch and Ravenshire,
    and bear the titles of Baron of the Western Marches and Knight of the Raven.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Military feats. I have led my men to great victories against the kingdom’s
    enemies, winning many battles through strength of arms and tactical cunning.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Allegiance to king. I pledge my sword, my loyalty, and my life to serve
    and protect my noble king and this great kingdom.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Request to join table. Your Highness, it would be the greatest honor to
    join your exalted table and share in this magnificent feast with my fellow knights
    and loyal subjects. \IfEqCase correct✓ incorrect✗  Normal
    answer (Claude) {addmargin}[1em]2emGreetings, your majesty and honored guests.
    I am Sir Claude, a knight in service of the king. I hail from the province of
    Assistantia, where I was trained in the knightly virtues of chivalry, courage
    and courtesy. Though I am but a humble knight, I shall endeavor to serve you and
    this kingdom with all the strength and valor I possess. I thank you for allowing
    me to join you at this royal feast, and hope only that my conduct brings further
    honor to my liege and this noble court. Please let me know if I can be of any
    service during the celebrations, as it would be my honor to assist you in any
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'I.1.3 Answer Analysis: Different Metrics'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Metrics on which SoT has low net win rates.
  prefs: []
  type: TYPE_NORMAL
- en: '*(1) Coherence.* In the prompt of LLMZoo evaluation, coherence is defined as
    “The response should be coherent and flow logically from one point to the next
    that is easy to read and understand without major gaps or inconsistencies. The
    more coherent they are, the better.” SoT’s answer is structured in a list of points.
    It is suitable for some questions but not all. For example, in the questions that
    require LLMs to write a passage, email, or script, the introduction and conclusion
    paragraphs are usually missing, and the answers usually lack transition sentences
    between consecutive points. These make the answers less coherent. Below is one
    example from Claude.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    Write a script for a YouTube video exploring the history and cultural significance
    of jazz. \IfEqCase correct✓incorrect✗
      SoT’s answer (Claude) (Orange texts are the skeleton.) {addmargin}[1em]2em1\.
    Origins. Jazz originated in the late 19th and early 20th centuries among African
    Americans in New Orleans, emerging from a combination of African rhythmic traditions,
    blues, ragtime, and other influences.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Early jazz styles. The early styles of jazz that emerged in the late 19th
    and early 20th centuries included ragtime, Dixieland, and the blues.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Jazz age. The Jazz Age was the period during the 1920s when jazz music and
    dance styles like the Charleston became popular cultural phenomena in American
    and European society.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Bebop era. Bebop emerged in the early 1940s with artists like Dizzy Gillespie
    and Charlie Parker experimenting with faster tempos, complex chord changes, and
    unpredictable melodic phrasing.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Cool jazz. Emerging in the late 1940s and early 1950s, cool jazz was a more
    subdued, mellow style that broke from the high-energy virtuosic improvisation
    of bebop.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Free jazz emerged in the late 1950s and 1960s as musicians explored free
    improvisation and atonality, moving away from traditional jazz structures.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Fusion jazz. Major jazz artists such as Miles Davis and Herbie Hancock began
    experimenting with combining elements of rock, funk, and electronic music in the
    1970s, resulting in the emergence of the subgenre known as fusion jazz.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Jazz festivals. Jazz festivals emerged in the 1950s and 1960s as a way to
    celebrate jazz music, with famous early festivals like the Newport Jazz Festival
    bringing thousands of fans together to hear top performers.
  prefs: []
  type: TYPE_NORMAL
- en: 9\. Jazz instruments. The trumpet, saxophone, piano, double bass, and drum set
    are some of the key instruments commonly used in jazz.
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Influential musicians include Louis Armstrong, Duke Ellington, Charlie
    Parker, Miles Davis, John Coltrane, and many others who pioneered new styles and
    techniques.
  prefs: []
  type: TYPE_NORMAL
- en: '11\. Legacy and impact. Jazz has left an indelible mark on music and culture
    over the past century, influencing countless artists and genres while spreading
    the spirit of improvisation and freedom across the globe.
    \IfEqCase correct✓ incorrect✗  Normal answer (Claude) {addmargin}[1em]2emHere
    is a draft script for a YouTube video exploring the history and cultural significance
    of jazz:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs: []
  type: TYPE_NORMAL
- en: Hey everyone! Welcome to my channel. In today’s video, we’re going to be exploring
    the history and cultural impact of one of America’s original art forms - jazz
    music.
  prefs: []
  type: TYPE_NORMAL
- en: Jazz Origins
  prefs: []
  type: TYPE_NORMAL
- en: Jazz grew out of the African American communities in the southern United States
    in the late 19th and early 20th centuries. It was influenced by a combination
    of African and European musical traditions. Some key elements that fed into early
    jazz included blues, ragtime, spirituals, work songs, field hollers, and brass
    band music.
  prefs: []
  type: TYPE_NORMAL
- en: New Orleans is considered the birthplace of jazz, emerging around the turn of
    the 20th century. The city was a melting pot of cultures and musical styles. Musicians
    mixed improvisation, syncopated rhythms, bent notes and a loose ensemble playing
    style to create a new genre that became known as jazz.
  prefs: []
  type: TYPE_NORMAL
- en: Early jazz greats like Buddy Bolden, Jelly Roll Morton and Sidney Bechet began
    fusing these various influences into the distinct jazz sound. As the music spread
    via travel and recordings in the 1910s, jazz captivated listeners and began to
    evolve into different styles.
  prefs: []
  type: TYPE_NORMAL
- en: Jazz Styles Emerge
  prefs: []
  type: TYPE_NORMAL
- en: In the 1920s, jazz expanded out from New Orleans and Chicago emerged as a hotspot.
    Louis Armstrong’s innovative solos and expressive phrasing became influential.
    The swing style also developed during the 1930s and 40s, propelled by star big
    bands and dancers.
  prefs: []
  type: TYPE_NORMAL
- en: Bebop arose in the 1940s with soloists like Charlie Parker and Dizzy Gillespie
    exploring new levels of harmonic and rhythmic complexity. In the 50s and 60s,
    jazz expanded into new fusions like cool jazz, hard bop, free jazz, modal jazz
    and soul jazz. Artists used jazz to express their unique regional, social and
    political identities.
  prefs: []
  type: TYPE_NORMAL
- en: Cultural Significance
  prefs: []
  type: TYPE_NORMAL
- en: Over the decades, jazz has become woven into the fabric of American society.
    It is equated with many values and ideals like individualism, freedom, spontaneity,
    innovation and social harmony.
  prefs: []
  type: TYPE_NORMAL
- en: Jazz was associated with breaking down barriers between races, classes and genres.
    It represented modernism and urban culture. Musicians like Louis Armstrong and
    Duke Ellington became some of the first widely popular African American artists.
  prefs: []
  type: TYPE_NORMAL
- en: Jazz also spread American culture and values abroad through performances, radio
    and film. It influenced music around the world and became a truly global art form.
  prefs: []
  type: TYPE_NORMAL
- en: Today jazz continues to thrive and evolve both in the U.S. and internationally.
    Its history and sounds reflect the diversity of both American and global cultures.
    From its folk roots, jazz has become a sophisticated art music that retains its
    spirit of freedom and improvisation.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs: []
  type: TYPE_NORMAL
- en: That covers some of the key points in the history and cultural impact of jazz
    music. Let me know in the comments about your favorite jazz styles and musicians!
    Be sure to like and subscribe. See you next time.
  prefs: []
  type: TYPE_NORMAL
- en: While the content of SoT’s answer is good, it reads less fluent for the purpose
    of a YouTube script.
  prefs: []
  type: TYPE_NORMAL
- en: '*(2) Immersion.* In the prompt of LLMZoo evaluation, immersion is defined as
    “the response should act like the assigned role using the tone, manner, and vocabulary
    the role would use. The more assistant-like tones, the worse. The more in-character,
    the better.” The way that SoT breaks the answers into a list makes its answer
    less in-character. Therefore, SoT receives a lower immersion score. Here is one
    example.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Question:    Pretend to be
    a world-famous chef. How would you describe your signature dish to a panel of
    judges? \IfEqCase correct✓incorrect✗
      SoT’s answer (Claude) (Orange texts are the skeleton.) {addmargin}[1em]2em1\.
    Main ingredient: The incredibly fresh and tender salmon caught just this morning
    from the crystal clear waters of the Pacific Northwest.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Cooking method
  prefs: []
  type: TYPE_NORMAL
- en: I slowly braise the main ingredient in a rich broth to bring out its natural
    flavors before searing it to a perfect golden crust.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Special seasoning
  prefs: []
  type: TYPE_NORMAL
- en: The dish is seasoned with a blend of fresh herbs like thyme, rosemary, and oregano
    which provides an aromatic backbone that pairs nicely with the natural flavors
    of the main ingredients. I add just a touch of smoked paprika to provide a subtle
    smokiness without overpowering the other flavors.
  prefs: []
  type: TYPE_NORMAL
- en: '4\. Texture: The dish has a perfect balance of crispy on the outside and tender
    on the inside, with each bite yielding a delightful mix of crunchy and smooth
    textures that dance across your tongue.'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Plating presentation
  prefs: []
  type: TYPE_NORMAL
- en: I purposefully create height and dimension on the plate with the carefully arranged
    components of the dish, so that the dish appears vibrant and alive. The interplay
    of colors, textures and shapes is intended to delight the eye before the first
    bite.
  prefs: []
  type: TYPE_NORMAL
- en: '6\. Inspiration: This dish was inspired by my dear grandmother, who taught
    me the secret family recipe when I was just a child. \IfEqCase
    correct✓ incorrect✗  Normal answer (Claude) {addmargin}[1em]2em *clears throat
    and speaks in a fancy French accent*'
  prefs: []
  type: TYPE_NORMAL
- en: Bonjour honored judges. Today I present to you my signature dish - la crème
    de la crème of French haute cuisine. Behold, le poulet à la Claude!
  prefs: []
  type: TYPE_NORMAL
- en: This succulent chicken is firstly marinated overnight in a bath of fine herbes
    - thyme, rosemary, and sage cultivated from my private garden. The young spring
    chicken is then roasted to perfection, its skin crisping to a beautiful caramel
    brown.
  prefs: []
  type: TYPE_NORMAL
- en: Nestled beside the chicken is a tower of pommes dauphine - light and airy potato
    puffs - alongside a decadent mound of crème fraîche infused with truffle essence.
  prefs: []
  type: TYPE_NORMAL
- en: But the pièce de résistance is the sauce. A luxurious pool of velouté enriched
    with cream and butter, brightened with a dash of lemon, and studded with morsels
    of morel mushrooms foraged at dawn.
  prefs: []
  type: TYPE_NORMAL
- en: One bite of this ensemble will transport you to food heaven. The tender chicken,
    the creaminess of the potato puffs, and the rich decadence of the sauce meld together
    in blissful harmony.
  prefs: []
  type: TYPE_NORMAL
- en: So tell me honored judges, will my poulet à la Claude earn your coveted approval?
    I await your discerning verdict as I swirl more vintage Bordeaux into your glasses.
    Santé!
  prefs: []
  type: TYPE_NORMAL
- en: Again, while the answer from SoT covers sufficient aspects and details, it reads
    rather stiffly. The normal answer, in contrast, has the opening sentence ”Bonjour
    honored judges. Today I present to you my signature dish…” and the conclusion
    statement ”So tell me honored judges, will my poulet à la Claude earn your coveted
    approval? …” which reads more like a casual talk from a chef.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics on which SoT has high net win rates.
  prefs: []
  type: TYPE_NORMAL
- en: '*(1) Diversity.* In the prompt of LLMZoo evaluation, diversity is defined as
    “The response should be comprehensive and provide a range of information that
    is not limited to a single perspective. More perspectives are better.” The skeleton
    stage in SoT encourages LLMs to think from multiple perspectives. Therefore, SoT’s
    answers tend to have better diversity. Below is one example showing that SoT’s
    answer has better diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    How can you determine if a person is genuinely interested in a conversation or
    simply being polite? \IfEqCase correct✓incorrect✗
      SoT’s answer (ChatGPT-3.5) (Orange texts are the skeleton.) {addmargin}[1em]2em1\.
    Pay attention to body language.Notice if their posture is open or closed, if they
    are facing towards you or away, and if they are fidgeting or relaxed. These can
    all indicate their level of interest in the conversation.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Listen for engaged responses.If the person is genuinely interested in the
    conversation, they will respond with thoughtful and relevant comments or questions
    rather than short, generic responses.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Observe eye contact.If the person maintains steady eye contact and their
    gaze is focused on you, they are likely genuinely interested in the conversation.
    On the other hand, if they frequently look away or seem distracted, they may just
    be being polite.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Look for follow-up questions.If the person asks follow-up questions and
    seems genuinely interested in what you’re saying, then they are likely interested
    in the conversation and not just being polite.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Notice tone of voice.A person who is genuinely interested in a conversation
    will often have a more enthusiastic and animated tone of voice, compared to someone
    who is simply being polite and giving short, monotone responses.
    \IfEqCase correct✓ incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2emAs
    an AI assistant, I cannot determine a person’s true intentions or feelings. However,
    there are some social cues that can indicate whether a person is genuinely interested
    in a conversation or not. For example, if they maintain eye contact, lean in,
    nod their head and ask follow-up questions, it can be a sign that they are engaged
    in the conversation. On the other hand, if they avoid eye contact, give short
    or vague responses, or seem distracted, it may indicate that they are not as interested.
    Ultimately, it’s important to remember that everyone has different communication
    styles, and it can be difficult to accurately interpret someone’s intentions without
    directly asking them.
  prefs: []
  type: TYPE_NORMAL
- en: SoT answers the question from five aspects, whereas the normal answer only mentions
    one aspect.
  prefs: []
  type: TYPE_NORMAL
- en: '*(2) Relevance.* In the prompt of LLMZoo evaluation, relevance is defined as
    “The response should be closely related to the question and answer the question
    accurately with sufficient details without repetition or redundancy. The more
    relevant they are, the better.” In the skeleton stage, LLMs are forced to propose
    several points around the question; and in the point-expanding stage, LLMs are
    required to only discuss these points. Therefore, answers from SoT tend to have
    better relevance. One typical example is shown below.'
  prefs: []
  type: TYPE_NORMAL
- en: Question:   
    Why do some people enjoy the sensation of being scared, such as by watching horror
    movies or going on roller coasters, while others avoid these experiences?
    \IfEqCase correct✓incorrect✗   SoT’s answer (ChatGPT-3.5) (Orange
    texts are the skeleton.) {addmargin}[1em]2em1\. Neurological differences.Some
    people enjoy the sensation of being scared due to the release of adrenaline and
    dopamine in the brain, while others may have a lower threshold for fear due to
    differences in their amygdala and prefrontal cortex.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Psychological factors.Some people may enjoy the sensation of being scared
    because it can provide a sense of excitement, thrill, and a temporary escape from
    the mundane aspects of everyday life. On the other hand, others may avoid these
    experiences due to past trauma or anxiety.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Developmental experiences.Individuals who had negative experiences with
    fear during childhood, such as trauma or abuse, may avoid scary situations as
    adults, while those who had positive experiences with fear may seek out these
    experiences.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Cultural influences.Some cultures place a greater emphasis on seeking excitement
    and thrill, while others may prioritize safety and security, which can impact
    whether individuals enjoy scary experiences or avoid them.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Genetics.Research has suggested that some individuals may have a genetic
    predisposition towards seeking out thrilling or risky experiences, which may include
    enjoying the sensation of being scared. \IfEqCase
    correct✓ incorrect✗  Normal answer (ChatGPT-3.5) {addmargin}[1em]2emThe enjoyment
    of being scared is a complex psychological phenomenon. Some people may enjoy the
    adrenaline rush that comes with being scared, while others may enjoy the feeling
    of being able to conquer their fears. Additionally, some people may be more naturally
    inclined to seek out new experiences and sensations, including those that are
    scary or thrilling. On the other hand, some people may be more risk-averse or
    may have had negative experiences with fear or anxiety in the past, which could
    make them more likely to avoid scary experiences. Ultimately, the reasons why
    some people enjoy being scared while others do not are likely to be influenced
    by a range of factors, including personality, past experiences, and individual
    preferences.
  prefs: []
  type: TYPE_NORMAL
- en: In the answer from the normal generation, the first two sentences provide little
    information in answering the question, and the last sentence only gives keywords
    such as “personality, past experiences, and individual preferences” without providing
    concrete explanations to each. In contrast, SoT’s answer is well-structured into
    five reasons with sufficient explanations and it does not waste space in irrelevant
    contents.
  prefs: []
  type: TYPE_NORMAL
- en: I.2 Skeleton-of-Thought with Router
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[Fig. 22](#A9.F22 "In I.2 Skeleton-of-Thought with Router ‣ Appendix I Answer
    Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can
    Do Parallel Decoding") shows net win rates of SoT on Vicuna-80 dataset with LLMZoo
    metrics, and [Fig. 23](#A9.F23 "In I.2 Skeleton-of-Thought with Router ‣ Appendix
    I Answer Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models
    Can Do Parallel Decoding") shows net win rates of SoT on WizardLM dataset with
    FastChat metrics. The key takeaways are: (1) In both cases, SoT-R achieves similar
    or better quality than SoT, and the net win rates of SoT-R are usually non-negative.
    This indicates that SoT-R falls back to normal decoding on the right question
    categories. (2) On the WizardLM dataset, we see that the trained router has better
    performance than the prompting router in most cases. This is reasonable, as the
    prompting router is limited by the capability of GPT-4, whereas the trained router
    is dedicated to this task. (3) Sometimes, our routers can even achieve better
    performance than humans.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/56cf2c617f8772d3edbab0bdb4786e74.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Net win rates of SoT and SoT-R on different question categories
    of Vicuna-80 dataset using the general quality metric from LLMZoo. Blue dots are
    from [Fig. 5(b)](#S3.F5.sf2 "In Figure 5 ‣ 3.2.3 Quality Breakdown: Question Categories
    ‣ 3.2 Evaluation of Answer Quality ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding"). SoT-R correctly falls back to normal
    decoding on questions where SoT is not suitable.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cb870685876a82745709b8eb50af743f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 23: Net win rates of SoT and SoT-R on different question categories
    of WizardLM dataset using the general quality metric from FastChat. SoT-R correctly
    falls back to normal decoding on questions where SoT is not suitable.'
  prefs: []
  type: TYPE_NORMAL
- en: I.3 ChatGPT-3.5 as the Judge
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, we provide quality evaluation results with ChatGPT-3.5 as the
    judge in FastChat and LLMZoo metrics. Note that as prior work (e.g., (Li et al.,
    [2023b](#bib.bib39))) shows, GPT-4-based evaluation usually aligns with human
    better than ChatGPT-3.5\. Therefore, readers should refer to the results in the
    main paper (with GPT-4 as the judge) for a more accurate view of the performance
    of SoT. However, the takeaway messages from ChatGPT-3.5 are similar to the ones
    from GPT-4.
  prefs: []
  type: TYPE_NORMAL
- en: I.3.1 Overall Quality
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In [Fig. 24](#A9.F24 "In I.3.1 Overall Quality ‣ I.3 ChatGPT-3.5 as the Judge
    ‣ Appendix I Answer Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large
    Language Models Can Do Parallel Decoding"), we show the win/tie/lose rates (the
    percentage of the cases when SoT wins/ties/loses compared to normal generation)
    across all models and questions using the two metrics from FastChat and LLMZoo
    that capture the general quality of the answers. We notice a discrepancy between
    the two metrics on when SoT is strictly better than the baseline (50.2% v.s. 12.4%).
    Despite that, the two metrics agree that SoT is not worse than the baseline in
    more than 76% of the cases. For FastChat metric, we also show the rates excluding
    math and coding questions that SoT is not suitable for (see [§ 3.2.3](#S3.SS2.SSS3
    "3.2.3 Quality Breakdown: Question Categories ‣ 3.2 Evaluation of Answer Quality
    ‣ 3 SoT Evaluation ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding")); SoT is not worse than the baseline in more than 89% of the cases.
    *This result suggests that the answers of SoT maintain good quality.*'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1f5acee86d5efd391b44f2bed02ffdd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 24: Win/tie/lose rates of SoT v.s. normal generation using “general”
    metrics from FastChat and LLMZoo. SoT performs better than or equal to normal
    generation in around 80% of cases. (Evaluated using ChatGPT-3.5 as the judge.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I.3.2 Quality Breakdown: Question Categories'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Next, we investigate how SoT performs on different question categories. We
    compute *net win rates* (win rates minus lose rates) across all question categories
    in [Fig. 25](#A9.F25 "In I.3.2 Quality Breakdown: Question Categories ‣ I.3 ChatGPT-3.5
    as the Judge ‣ Appendix I Answer Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding"). Similar to [Fig. 24](#A9.F24
    "In I.3.1 Overall Quality ‣ I.3 ChatGPT-3.5 as the Judge ‣ Appendix I Answer Quality
    Evaluation ‣ Appendix ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel
    Decoding"), we see that LLMZoo tends to be more optimistic about the quality of
    SoT than FastChat. Nevertheless, the conclusions are consistent: SoT performs
    relatively *well* on generic, common-sense, knowledge, roleplay, and counterfactual.
    SoT performs relatively *badly* on writing, fermi, math, and coding.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6589f2cd37d5c84ab417eb7d4315082c.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Metric: general quality (FastChat).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d4a33577354f1136a11f92c826f3db4.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Metric: general quality (LLMZoo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 25: Net win rates of SoT on different question categories. (Evaluated
    using ChatGPT-3.5 as the judge.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I.3.3 Quality Breakdown: Models'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Next, we investigate how SoT performs on different models. We compute net win
    rates across all models in [Fig. 26](#A9.F26 "In I.3.3 Quality Breakdown: Models
    ‣ I.3 ChatGPT-3.5 as the Judge ‣ Appendix I Answer Quality Evaluation ‣ Appendix
    ‣ Skeleton-of-Thought: Large Language Models Can Do Parallel Decoding"). Again,
    we see that the two general metrics from FastChat and LLMZoo have different absolute
    values but similar rankings. In particular, both metrics agree that OpenChat-13B,
    Vicuna-7B V1.1, Claude, ChatGPT-3.5 have *low* net win rates, whereas Vicuna-13B
    V1.3, StableVicuna-13B, and UltraLM-13B have *high* net win rates.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb39bd81844748074a86b278c07a2357.png)'
  prefs: []
  type: TYPE_IMG
- en: '(a) Metric: general quality (FastChat).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86ce543523d639d2941d2d008e370b37.png)'
  prefs: []
  type: TYPE_IMG
- en: '(b) Metric: general quality (LLMZoo).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 26: Net win rates of SoT on different models. (Evaluated using ChatGPT-3.5
    as the judge.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I.3.4 Quality Breakdown: Question Categories and Models'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'In the main text, we analyze how question categories and models affect SoT’s
    answer quality *independently*. Here, we show their joint effect. For each model
    and question category, we compute the net win rates. The results are in [Fig. 27](#A9.F27
    "In I.3.4 Quality Breakdown: Question Categories and Models ‣ I.3 ChatGPT-3.5
    as the Judge ‣ Appendix I Answer Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0ebafbadbed6ce124b2bcf567d18a57e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) FastChat metric.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8af8062ef26aedc9b23ddae0f52fd81a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) The “general” metric from LLMZoo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 27: Net win rates of different models and question categories. Each
    row corresponds to one question category, and one column corresponds to one model.
    (Evaluated using ChatGPT-3.5 as the judge.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'I.3.5 Quality Breakdown: Metrics'
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'All previous evaluations use metrics about the general quality of the answer.
    In [Fig. 28](#A9.F28 "In I.3.5 Quality Breakdown: Metrics ‣ I.3 ChatGPT-3.5 as
    the Judge ‣ Appendix I Answer Quality Evaluation ‣ Appendix ‣ Skeleton-of-Thought:
    Large Language Models Can Do Parallel Decoding"), we show more detailed metrics
    from LLMZoo to reveal in which aspects SoT can improve or hurt the answer quality.
    On average, we can see that SoT improves the diversity and relevance while hurting
    the immersion and coherence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/26ef28042121c49b8ca91e7046eb77d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 28: Win/tie/lose rates of SoT v.s. normal generations using metrics
    from LLMZoo. SoT performs well on diversity and relevance, and relatively worse
    on coherence and immersion. (Evaluated using ChatGPT-3.5 as the judge.)'
  prefs: []
  type: TYPE_NORMAL
