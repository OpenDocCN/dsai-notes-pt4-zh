- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:46:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:46:13'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'DeMPT: 解码增强多阶段提示调整，使LLMs成为更好的上下文感知翻译器'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15200](https://ar5iv.labs.arxiv.org/html/2402.15200)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15200](https://ar5iv.labs.arxiv.org/html/2402.15200)
- en: Xinglin Lyu^♣,  Junhui Li^♣,  Yanqing Zhao^♠,  Min Zhang^♠,  Daimeng Wei^♠,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xinglin Lyu^♣,  Junhui Li^♣,  Yanqing Zhao^♠,  Min Zhang^♠,  Daimeng Wei^♠,
- en: Shimin Tao^♠,  Hao Yang^♠,  Min Zhang^♣
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shimin Tao^♠,  Hao Yang^♠,  Min Zhang^♣
- en: ^♣School of Computer Science and Technology, Soochow University, Suzhou, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ^♣计算机科学与技术学院，苏州大学，中国苏州
- en: ^♠Huawei Translation Services Center, Beijing, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^♠华为翻译服务中心，中国北京
- en: xllv2020@stu.suda.edu.cn, {lijunhui,minzhang}@suda.edu.cn
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: xllv2020@stu.suda.edu.cn, {lijunhui,minzhang}@suda.edu.cn
- en: '{zhaoyanqing,zhangmin186,weidaimeng,taoshimin,yanghao30}@huawei.com Corresponding
    author: Junhui Li.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '{zhaoyanqing,zhangmin186,weidaimeng,taoshimin,yanghao30}@huawei.com 对应作者: Junhui
    Li.'
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Generally, the decoder-only large language models (LLMs) are adapted to context-aware
    neural machine translation (NMT) in a concatenating way, where LLMs take the concatenation
    of the source sentence (i.e., intra-sentence context) and the inter-sentence context
    as the input, and then to generate the target tokens sequentially. This adaptation
    strategy, i.e., concatenation mode, considers intra-sentence and inter-sentence
    contexts with the same priority, despite an apparent difference between the two
    kinds of contexts. In this paper, we propose an alternative adaptation approach,
    named Decoding-enhanced Multi-phase Prompt Tuning (DeMPT), to make LLMs discriminately
    model and utilize the inter- and intra-sentence context and more effectively adapt
    LLMs to context-aware NMT. First, DeMPT divides the context-aware NMT process
    into three separate phases. During each phase, different continuous prompts are
    introduced to make LLMs discriminately model various information. Second, DeMPT
    employs a heuristic way to further discriminately enhance the utilization of the
    source-side inter- and intra-sentence information at the final decoding phase.
    Experiments show that our approach significantly outperforms the concatenation
    method, and further improves the performance of LLMs in discourse modeling.¹¹1As
    this paper is under review, we will release our code and datasets later.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，只有解码器的大型语言模型（LLMs）以串联的方式适应上下文感知的神经机器翻译（NMT），其中LLMs将源句子的串联（即句内上下文）和句间上下文作为输入，然后按顺序生成目标标记。这种适应策略，即串联模式，将句内和句间上下文视为同等重要，尽管这两种上下文之间存在明显差异。本文提出了一种替代的适应方法，称为解码增强多阶段提示调整（DeMPT），以使LLMs有区别地建模和利用句间与句内上下文，并更有效地将LLMs适应于上下文感知的NMT。首先，DeMPT将上下文感知的NMT过程分为三个独立的阶段。在每个阶段，引入不同的连续提示，以使LLMs有区别地建模各种信息。其次，DeMPT采用启发式方法在最终解码阶段进一步有区别地增强源侧句间和句内信息的利用。实验表明，我们的方法显著优于串联方法，并进一步提高了LLMs在话语建模中的表现。¹¹1由于本文正在审稿中，我们将在稍后发布我们的代码和数据集。
- en: 'DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 'DeMPT: 解码增强多阶段提示调整，使LLMs成为更好的上下文感知翻译器'
- en: 'Xinglin Lyu^♣,  Junhui Li^♣^†^†thanks: Corresponding author: Junhui Li.,  Yanqing
    Zhao^♠,  Min Zhang^♠,  Daimeng Wei^♠, Shimin Tao^♠,  Hao Yang^♠,  Min Zhang^♣
    ^♣School of Computer Science and Technology, Soochow University, Suzhou, China
    ^♠Huawei Translation Services Center, Beijing, China xllv2020@stu.suda.edu.cn,
    {lijunhui,minzhang}@suda.edu.cn {zhaoyanqing,zhangmin186,weidaimeng,taoshimin,yanghao30}@huawei.com'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xinglin Lyu^♣,  Junhui Li^♣^†^†感谢: 对应作者: Junhui Li.,  Yanqing Zhao^♠,  Min
    Zhang^♠,  Daimeng Wei^♠, Shimin Tao^♠,  Hao Yang^♠,  Min Zhang^♣ ^♣计算机科学与技术学院，苏州大学，中国苏州
    ^♠华为翻译服务中心，中国北京 xllv2020@stu.suda.edu.cn, {lijunhui,minzhang}@suda.edu.cn {zhaoyanqing,zhangmin186,weidaimeng,taoshimin,yanghao30}@huawei.com'
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'Context-aware neural machine translation (NMT) goes beyond sentence-level NMT
    by incorporating inter-sentence context at the document level Zhang et al. ([2018](#bib.bib42));
    Miculicich et al. ([2018](#bib.bib25)); Voita et al. ([2018](#bib.bib36), [2019b](#bib.bib35),
    [2019a](#bib.bib34)); Bao et al. ([2021](#bib.bib2)); Sun et al. ([2022](#bib.bib31)),
    aiming to address discourse-related challenges such as zero pronoun translation Wang
    et al. ([2019](#bib.bib38)), lexical translation consistency Lyu et al. ([2021](#bib.bib20),
    [2022](#bib.bib21)), and discourse structure Hu and Wan ([2023](#bib.bib7)). A
    recent paradigm shift has been witnessed in context-aware NMT with the emergence
    of the decoder-only large language models (LLMs) BigScience ([2022](#bib.bib3));
    Google ([2022](#bib.bib4)); MetaAI ([2023b](#bib.bib24), [a](#bib.bib23)); OpenAI
    ([2023](#bib.bib26)). These generative language models, trained on extensive public
    data, have gained significant attention in the natural language processing (NLP)
    community. In adapting LLMs to context-aware NMT, a common strategy involves concatenating
    multiple source sentences as a prefix and generating translations token-by-token,
    relying on the prefix and previously predicted target tokens, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators") (a). However, a critical
    observation of this strategy reveals a potential drawback – the equal prioritization
    of the inter- and intra-sentence contexts during token generation. Importantly,
    the intra-sentence context inherently contains richer parallel semantic information
    with the target sentence and should be given a higher priority than the inter-sentence
    context. Consequently, we propose that separately modeling and utilizing the inter-
    and intra-sentence contexts should explicitly inform LLMs of the document-level
    context and the current sentence itself, thus being able to prevent the misallocation
    of attention weights to source-side tokens Bao et al. ([2021](#bib.bib2)); Li
    et al. ([2023](#bib.bib16)). Inspired by the success of prompt tuning Li and Liang
    ([2021](#bib.bib15)); Liu et al. ([2022](#bib.bib18)); Tan et al. ([2022](#bib.bib32)),
    our alternative approach, named Decoding-Enhanced Multi-phase Prompt Tuning (DeMPT),
    aims to enhance LLMs’ adaptability to context-aware NMT, as shown in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators") (b).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '上下文感知神经机器翻译（NMT）超越了句子级NMT，通过在文档级别融入句间上下文 Zhang et al. ([2018](#bib.bib42));
    Miculicich et al. ([2018](#bib.bib25)); Voita et al. ([2018](#bib.bib36), [2019b](#bib.bib35),
    [2019a](#bib.bib34)); Bao et al. ([2021](#bib.bib2)); Sun et al. ([2022](#bib.bib31))，旨在解决与话语相关的挑战，如零代词翻译 Wang
    et al. ([2019](#bib.bib38))，词汇翻译一致性 Lyu et al. ([2021](#bib.bib20), [2022](#bib.bib21))，以及话语结构 Hu
    and Wan ([2023](#bib.bib7))。近年来，随着解码器单一大型语言模型（LLMs）的出现，已见证上下文感知NMT的范式转变 BigScience
    ([2022](#bib.bib3)); Google ([2022](#bib.bib4)); MetaAI ([2023b](#bib.bib24),
    [a](#bib.bib23)); OpenAI ([2023](#bib.bib26))。这些生成语言模型在广泛的公共数据上进行训练，获得了自然语言处理（NLP）社区的广泛关注。在将LLMs适应于上下文感知NMT时，一种常见的策略是将多个源句子连接为前缀，并逐词生成翻译，依赖于前缀和先前预测的目标词，如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators") (a) 所示。然而，对这一策略的关键观察揭示了一个潜在的缺陷——在生成词时对句间和句内上下文的平等优先级。重要的是，句内上下文本质上包含了与目标句子更丰富的并行语义信息，应给予比句间上下文更高的优先级。因此，我们建议分别建模和利用句间和句内上下文，以明确告知LLMs文档级别上下文和当前句子，从而防止将注意力权重错误分配给源侧词 Bao
    et al. ([2021](#bib.bib2)); Li et al. ([2023](#bib.bib16))。受提示调整成功的启发 Li and Liang
    ([2021](#bib.bib15)); Liu et al. ([2022](#bib.bib18)); Tan et al. ([2022](#bib.bib32))，我们提出的替代方法，称为解码增强多阶段提示调整（DeMPT），旨在增强LLMs对上下文感知NMT的适应能力，如图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators") (b) 所示。'
- en: '![Refer to caption](img/b90414bf0551800a5143ed3173d1b7a7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b90414bf0551800a5143ed3173d1b7a7.png)'
- en: 'Figure 1: Comparison of different strategies for adapting LLMs to context-aware
    NMT. The concatenation strategy (left) treats inter-sentence and intra-sentence
    (referred to as the "source sentence" context in the figure) with equal importance.
    In contrast, our approach (right) divides context-aware NMT into three distinct
    phases, enabling LLMs to selectively model and leverage both inter- and intra-sentence
    contexts.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：适应 LLMs 到上下文感知 NMT 的不同策略的比较。连接策略（左侧）将句间和句内（图中称为“源句子”上下文）视为同等重要。相比之下，我们的方法（右侧）将上下文感知
    NMT 分为三个不同的阶段，使 LLM 能够选择性地建模和利用句间和句内上下文。
- en: 'Specifically, we divide the whole procedure of context-aware NMT into three
    phases: inter-sentence context encoding, intra-sentence context encoding, and
    decoding. Following Li and Liang ([2021](#bib.bib15)); Liu et al. ([2022](#bib.bib18)),
    we sequentially and differentially adapt LLMs for each phase, utilizing phase-specific
    trainable prompts. This phased tuning method enables LLMs to independently capture
    and model both inter- and intra-sentence contexts, facilitating a better understanding
    of their differences. Importantly, our approach only divides the original input
    into three parts without significantly increasing computational load. As a result,
    there is no substantial decrease in inference speed compared to the concatenating
    method, as detailed in Section [4.3](#S4.SS3 "4.3 Comparison of Inference Speed
    ‣ 4 Discussion ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators").'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，我们将上下文感知 NMT 的整个过程分为三个阶段：句间上下文编码、句内上下文编码和解码。根据 Li 和 Liang ([2021](#bib.bib15))；Liu
    等人 ([2022](#bib.bib18)) 的方法，我们为每个阶段顺序和差异地调整 LLM，利用阶段特定的可训练提示。这种分阶段调优方法使 LLM 能够独立捕捉和建模句间和句内上下文，从而更好地理解它们之间的差异。重要的是，我们的方法仅将原始输入分为三部分，而不会显著增加计算负担。因此，与连接方法相比，推理速度没有显著下降，详细内容见第
    [4.3](#S4.SS3 "4.3 Comparison of Inference Speed ‣ 4 Discussion ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")
    节。'
- en: Furthermore, during the decoding phase, we propose a heuristic method to emphasize
    the difference between inter- and intra-sentence contexts, and avoid long-distance
    issue when utilizing inter-sentence context. Specifically, at each decoding step,
    we use LLMs to predict the next token three times. The decoding states used for
    each prediction directly concatenate with the representations of two contexts
    in a discriminative manner. Finally, we combine three probability distributions
    to search for the next token as the output from the target vocabulary. This method
    enables LLMs to learn not only to properly capture inter-sentence context in addressing
    discourse-related issues but also to recognize a difference between inter- and
    intra-sentence contexts, allowing for effective utilization of both types of contexts.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在解码阶段，我们提出了一种启发式方法，以强调句间和句内上下文之间的差异，并在利用句间上下文时避免远距离问题。具体而言，在每个解码步骤中，我们使用
    LLM 预测下一个 token 三次。每次预测使用的解码状态直接与两个上下文的表示以区分方式连接。最后，我们将三个概率分布结合起来，以从目标词汇表中搜索下一个
    token 作为输出。这种方法使 LLM 不仅能够有效捕捉句间上下文来解决话语相关问题，还能够识别句间和句内上下文之间的差异，从而有效利用这两种上下文。
- en: 'In summary, our contributions can be outlined as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的贡献可以概述如下：
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a novel multi-phase prompt tuning approach to divide context-aware
    NMT into three phases, making LLMs aware of the distinction between inter- and
    intra-sentence contexts.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的多阶段提示调优方法，将上下文感知 NMT 分为三个阶段，使 LLM 能够意识到句间和句内上下文之间的区别。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a enhanced decoding method that discriminately utilize both context
    types. This allows LLMs not only properly capture inter-sentence context in addressing
    discourse-related issues, but also be aware of the importance of the intra-sentence
    context.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们引入了一种增强的解码方法，区别性地利用这两种上下文类型。这使得 LLM 不仅能够适当地捕捉句间上下文以解决话语相关问题，还能够意识到句内上下文的重要性。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We validate our approach using llama-2-7b and bloomz-7b1-mt as foundation models,
    demonstrating its effectiveness across five context-aware translation directions.
    Extensive analyses further highlight the substantial enhancement in LLMs’ ability
    for context-aware NMT.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们使用 llama-2-7b 和 bloomz-7b1-mt 作为基础模型来验证我们的方法，展示了它在五个上下文感知翻译方向上的有效性。广泛的分析进一步突显了
    LLM 在上下文感知 NMT 能力方面的显著提升。
- en: 2 Methodology
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 方法论
- en: 'In this section, we describe our decoding-enhanced multi-phase approach for
    adapting LLMs to context-aware NMT in details. Specifically, we break down the
    whole procedure of context-aware NMT into three phases (Section [2.1](#S2.SS1
    "2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")),
    i.e., inter-sentence context encoding, intra-sentence encoding, and decoding.
    Additionally, we discriminatively enhance the utilization of inter- and intra-sentence
    contexts during the decoding phase (Section [2.2](#S2.SS2 "2.2 Enhanced Decoding
    Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for
    Making LLMs Be Better Context-aware Translators")). Finally, we describe our phase-aware
    prompts and training objective in Section [2.3](#S2.SS3 "2.3 Phase-aware Prompts
    ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") and Section [2.4](#S2.SS4 "2.4 Training
    Objective ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators"), respectively.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们详细描述了增强解码的多阶段方法，以使LLMs适应上下文感知的NMT。具体来说，我们将上下文感知的NMT整个过程分解为三个阶段（第 [2.1](#S2.SS1
    "2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")节），即跨句子上下文编码、句内编码和解码。此外，我们在解码阶段（第
    [2.2](#S2.SS2 "2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")节）增强了对跨句子和句内上下文的利用。最后，我们在第
    [2.3](#S2.SS3 "2.3 Phase-aware Prompts ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")节和第
    [2.4](#S2.SS4 "2.4 Training Objective ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")节分别描述了我们的阶段感知提示和训练目标。'
- en: For a given document pair $(\mathcal{S},\mathcal{T})$ as the number of transformer
    layers within it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的文档对 $(\mathcal{S},\mathcal{T})$，其变换器层的数量。
- en: 2.1 Multi-phase Encoding and Decoding
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多阶段编码和解码
- en: 'We implement our approach based on deep prompt tuning Li and Liang ([2021](#bib.bib15));
    Liu et al. ([2022](#bib.bib18)). Next, we use training instance $(\mathcal{C},S,T)$
    as an example to describe the multi-phase approach. Figure [2](#S2.F2 "Figure
    2 ‣ Inter-sentence Context Encoding Phase. ‣ 2.1 Multi-phase Encoding and Decoding
    ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") illustrates the procedure of multi-phase
    prompt tuning.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '我们基于深度提示调优实现了我们的方法，参考文献包括 Li 和 Liang ([2021](#bib.bib15))；Liu 等人 ([2022](#bib.bib18))。接下来，我们使用训练实例
    $(\mathcal{C},S,T)$ 作为例子来描述多阶段方法。图 [2](#S2.F2 "Figure 2 ‣ Inter-sentence Context
    Encoding Phase. ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT:
    Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware
    Translators") 说明了多阶段提示调优的过程。'
- en: Inter-sentence Context Encoding Phase.
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 跨句子上下文编码阶段。
- en: 'In the inter-sentence context encoding phase (Phase 1 in Figure [2](#S2.F2
    "Figure 2 ‣ Inter-sentence Context Encoding Phase. ‣ 2.1 Multi-phase Encoding
    and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators")), we first concatenate all
    sentences in $\mathcal{C}$ by incorporating the trainable prompt:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '在跨句子上下文编码阶段（图 [2](#S2.F2 "Figure 2 ‣ Inter-sentence Context Encoding Phase.
    ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")的阶段1），我们首先通过结合可训练的提示将$\mathcal{C}$中的所有句子连接起来：'
- en: '|  | $H_{\mathcal{C}}^{1:L}=\text{LLM}(\mathcal{C},{\bf P}_{\mathcal{C}}),$
    |  | (1) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{\mathcal{C}}^{1:L}=\text{LLM}(\mathcal{C},{\bf P}_{\mathcal{C}}),$
    |  | (1) |'
- en: 'where $H_{\mathcal{C}}^{1:L}\in\mathbb{R}^{L\times|\mathcal{C}|\times d}$ as
    follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{\mathcal{C}}^{1:L}\in\mathbb{R}^{L\times|\mathcal{C}|\times d}$ 如下所示：
- en: '|  | $H_{\mathcal{C}}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{\mathcal{C}},\textbf{V}_{\mathcal{C}},\textbf{Q}_{\mathcal{C}}\right)\right),$
    |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{\mathcal{C}}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{\mathcal{C}},\textbf{V}_{\mathcal{C}},\textbf{Q}_{\mathcal{C}}\right)\right),$
    |  | (2) |'
- en: '|  | $\textbf{Q}_{\mathcal{C}}=H_{\mathcal{C}}^{l-1},$ |  | (3) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{Q}_{\mathcal{C}}=H_{\mathcal{C}}^{l-1},$ |  | (3) |'
- en: '|  | $\textbf{K}_{\mathcal{C}}=[{\bf P}_{\mathcal{C}}[l,:q,:];H_{\mathcal{C}}^{l-1}],$
    |  | (4) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{K}_{\mathcal{C}}=[{\bf P}_{\mathcal{C}}[l,:q,:];H_{\mathcal{C}}^{l-1}],$
    |  | (4) |'
- en: '|  | $\textbf{V}_{\mathcal{C}}=[{\bf P}_{\mathcal{C}}[l,q:,:];H_{\mathcal{C}}^{l-1}],$
    |  | (5) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{V}_{\mathcal{C}}=[{\bf P}_{\mathcal{C}}[l,q:,:];H_{\mathcal{C}}^{l-1}],$
    |  | (5) |'
- en: where $H_{\mathcal{C}}^{l}\in\mathbb{R}^{|\mathcal{C}|\times d}$ are the concatenating
    and slicing operations, respectively.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{\mathcal{C}}^{l}\in\mathbb{R}^{|\mathcal{C}|\times d}$ 分别表示连接和切片操作。
- en: '![Refer to caption](img/8651a44cba15da78d9c4c12bf06e028b.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8651a44cba15da78d9c4c12bf06e028b.png)'
- en: 'Figure 2: Illustration of pipeline of multi-phase prompt tuning LLM for context-aware
    NMT. Red lines illustrate the procedure of enhanced decoding phase.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：多阶段提示调优 LLM 用于上下文感知 NMT 的流程示意图。红线示意了增强解码阶段的过程。
- en: Intra-sentence Context Encoding Phase.
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 句内上下文编码阶段。
- en: 'In the intra-sentence context encoding phase (Phase 2 in Figure [2](#S2.F2
    "Figure 2 ‣ Inter-sentence Context Encoding Phase. ‣ 2.1 Multi-phase Encoding
    and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators")), the LLM encodes the intra-sentence
    context $S$ and trainable prompt:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '在句内上下文编码阶段（图 [2](#S2.F2 "Figure 2 ‣ Inter-sentence Context Encoding Phase.
    ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")
    的阶段 2），LLM 编码句内上下文 $S$ 和可训练的提示：'
- en: '|  | $H_{S}^{1:L}=\text{LLM}(S,H_{\mathcal{C}}^{1:L},{\bf P}_{S}),$ |  | (6)
    |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{S}^{1:L}=\text{LLM}(S,H_{\mathcal{C}}^{1:L},{\bf P}_{S}),$ |  | (6)
    |'
- en: 'where $H_{{S}}^{1:L}\in\mathbb{R}^{L\times|S|\times d}$ as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{{S}}^{1:L}\in\mathbb{R}^{L\times|S|\times d}$ 如下：
- en: '|  | $H_{S}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{S},\textbf{V}_{S},\textbf{Q}_{S}\right)\right),$
    |  | (7) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{S}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{S},\textbf{V}_{S},\textbf{Q}_{S}\right)\right),$
    |  | (7) |'
- en: '|  | $\textbf{Q}_{S}=H_{S}^{l-1},$ |  | (8) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{Q}_{S}=H_{S}^{l-1},$ |  | (8) |'
- en: '|  | $\textbf{K}_{S}=[{\bf P}_{S}[l,:q,:];H_{\mathcal{C}}^{l-1};H_{S}^{l-1}],$
    |  | (9) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{K}_{S}=[{\bf P}_{S}[l,:q,:];H_{\mathcal{C}}^{l-1};H_{S}^{l-1}],$
    |  | (9) |'
- en: '|  | $\textbf{V}_{S}=[{\bf P}_{S}[l,q:,:];H_{\mathcal{C}}^{l-1};H_{S}^{l-1}],$
    |  | (10) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{V}_{S}=[{\bf P}_{S}[l,q:,:];H_{\mathcal{C}}^{l-1};H_{S}^{l-1}],$
    |  | (10) |'
- en: where $H_{S}^{l}$ layer output of the inter-sentence context encoding.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{S}^{l}$ 是跨句上下文编码层的输出。
- en: Decoding Phase.
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解码阶段。
- en: 'In the decoding phase (Phase 3 in Figure [2](#S2.F2 "Figure 2 ‣ Inter-sentence
    Context Encoding Phase. ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators")), given the past activations $H_{S}$ and trainable
    prompt, we call the LLM again to generate the hidden state for predicting the
    probability of the target sentence:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '在解码阶段（图 [2](#S2.F2 "Figure 2 ‣ Inter-sentence Context Encoding Phase. ‣ 2.1
    Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators") 的阶段 3），给定过去的激活
    $H_{S}$ 和可训练的提示，我们再次调用 LLM 生成隐藏状态以预测目标句子的概率：'
- en: '|  | $H_{T}^{1:L}=\text{LLM}(T,H_{{S}}^{1:L},{\bf P}_{T}),$ |  | (11) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{T}^{1:L}=\text{LLM}(T,H_{{S}}^{1:L},{\bf P}_{T}),$ |  | (11) |'
- en: 'where $H_{T}^{1:L}\in\mathbb{R}^{L\times|T|\times d}$ as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{T}^{1:L}\in\mathbb{R}^{L\times|T|\times d}$ 如下：
- en: '|  | $H_{T}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{T},\textbf{V}_{T},\textbf{Q}_{T}\right)\right),$
    |  | (12) |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | $H_{T}^{l}=\text{FFN}\left(\text{Multi-Attn}\left(\textbf{K}_{T},\textbf{V}_{T},\textbf{Q}_{T}\right)\right),$
    |  | (12) |'
- en: '|  | $\textbf{Q}_{T}=H_{T}^{l-1},$ |  | (13) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{Q}_{T}=H_{T}^{l-1},$ |  | (13) |'
- en: '|  | $\textbf{K}_{T}=[{\bf P}_{T}[l,:q,:];H_{S}^{l-1};H_{T}^{l-1}],$ |  | (14)
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{K}_{T}=[{\bf P}_{T}[l,:q,:];H_{S}^{l-1};H_{T}^{l-1}],$ |  | (14)
    |'
- en: '|  | $\textbf{V}_{T}=[{\bf P}_{T}[l,q:,:];H_{S}^{l-1};H_{T}^{l-1}],$ |  | (15)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{V}_{T}=[{\bf P}_{T}[l,q:,:];H_{S}^{l-1};H_{T}^{l-1}],$ |  | (15)
    |'
- en: 'where $H_{T}^{l}\in\mathbb{R}^{|T|\times d}$:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H_{T}^{l}\in\mathbb{R}^{|T|\times d}$：
- en: '|  | $p\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(h_{t}^{L}W\right),$
    |  | (16) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $p\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(h_{t}^{L}W\right),$
    |  | (16) |'
- en: where $W\in\mathbb{R}^{d\times|\mathcal{V}|}$ is the vocabulary size.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W\in\mathbb{R}^{d\times|\mathcal{V}|}$ 是词汇表大小。
- en: 2.2 Enhanced Decoding Phase
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 增强解码阶段
- en: '![Refer to caption](img/57f788a1666872fdb183598321e8db7a.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/57f788a1666872fdb183598321e8db7a.png)'
- en: 'Figure 3: Illustration of the procedure of our proposed decoding-enhanced approach
    at the $t$-th decoding step of the decoding phase.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在解码阶段第 $t$ 次解码步骤中我们提出的增强解码方法的过程示意图。
- en: 'As shown in Figure [2](#S2.F2 "Figure 2 ‣ Inter-sentence Context Encoding Phase.
    ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
    both the inter-sentence context representation $H_{\mathcal{C}}^{1:L}$. This may
    result in a long-distance issue such that the inter-sentence context are not properly
    aligned by target-side tokens.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[2](#S2.F2 "图 2 ‣ 句间上下文编码阶段。 ‣ 2.1 多阶段编码与解码 ‣ 2 方法论 ‣ DeMPT: 解码增强型多阶段提示调优使大型语言模型成为更好的上下文感知翻译器")所示，句间上下文表示
    $H_{\mathcal{C}}^{1:L}$。这可能导致远距离问题，使得句间上下文没有通过目标端的标记进行适当对齐。'
- en: 'Therefore, to address above two issues, we propose an enhanced decoding phase
    with an aim to more effectively utilize both the inter- and intra-sentence contexts.
    Inspired by Kuang et al. ([2018](#bib.bib13)), we move both the two types of inter-
    and intra-sentence contexts closer to target words to achieve a tight interaction
    between them. Specifically, we concatenate the decoding states with the two types
    of representations to predict the next target words. As shown in Figure [3](#S2.F3
    "Figure 3 ‣ 2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
    the enhanced next word prediction $p_{e}$ is a combination of three distributions
    with different inputs:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，为了解决上述两个问题，我们提出了一个增强的解码阶段，旨在更有效地利用句间和句内上下文。受到 Kuang 等人 ([2018](#bib.bib13))
    的启发，我们将这两种类型的句间和句内上下文更接近目标词，以实现它们之间的紧密交互。具体而言，我们将解码状态与这两种表示拼接，以预测下一个目标词。如图[3](#S2.F3
    "图 3 ‣ 2.2 增强解码阶段 ‣ 2 方法论 ‣ DeMPT: 解码增强型多阶段提示调优使大型语言模型成为更好的上下文感知翻译器")所示，增强的下一个词预测
    $p_{e}$ 是三种具有不同输入的分布的组合：'
- en: '|  | $$\begin{split}p_{e}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=&amp;\lambda_{1}\times\hat{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)\\
    &amp;+\lambda_{2}\times\bar{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)\\'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{split}p_{e}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=&amp;\lambda_{1}\times\hat{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)\\
    &amp;+\lambda_{2}\times\bar{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)\\'
- en: '&amp;+\left(1-\lambda_{1}-\lambda_{2}\right)\times p\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right),\end{split}$$
    |  | (17) |'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;+\left(1-\lambda_{1}-\lambda_{2}\right)\times p\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right),\end{split}$$
    |  | (17) |'
- en: 'where $\lambda_{1}$, respectively, which can be further computed as:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{1}$，分别可以进一步计算为：
- en: '|  | $\hat{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(\hat{h}_{t}^{L}W\right),$
    |  | (18) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(\hat{h}_{t}^{L}W\right),$
    |  | (18) |'
- en: '|  | $\bar{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(\bar{h}_{t}^{L}W\right),$
    |  | (19) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{p}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right)=\text{Softmax}\left(\bar{h}_{t}^{L}W\right),$
    |  | (19) |'
- en: '|  | $\hat{h}_{t}^{L}=\text{FFN}\left([\tilde{H}_{\mathcal{C}}^{L};\tilde{H}_{S}^{L};h_{t}^{L}]\right),$
    |  | (20) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{h}_{t}^{L}=\text{FFN}\left([\tilde{H}_{\mathcal{C}}^{L};\tilde{H}_{S}^{L};h_{t}^{L}]\right),$
    |  | (20) |'
- en: '|  | $\bar{h}_{t}^{L}=\text{FFN}\left([\tilde{H}_{S}^{L};h_{t}^{L}]\right),$
    |  | (21) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bar{h}_{t}^{L}=\text{FFN}\left([\tilde{H}_{S}^{L};h_{t}^{L}]\right),$
    |  | (21) |'
- en: where $W$ at token level, respectively.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $W$ 在标记级别，分别。
- en: 2.3 Phase-aware Prompts
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 阶段感知提示
- en: 'We emphasize the LLM needs to play various roles across three phases, and maintaining
    similar prompts across different phases may not be reasonable. Thus, we empower
    LLM to distinguish different phases by introducing a type embedding and a transfer
    layer³³3Different from the multi-layer perceptron (MLPs) used for reparameterization,
    our transfer layer is shared-parameter for all prompts. Thus, there are fewer
    trainable parameters during the training of our model. We compare the number of
    trainable parameters among different tuning methods in Table [3](#S3.T3 "Table
    3 ‣ 3.2 Experimental Results ‣ 3 Experimentation ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators") and analyze
    the effect of the transfer layer in Appendix [D](#A4 "Appendix D Effect of Transfer
    Layer and Type Embedding ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators"). for these prompts:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调LLM需要在三个阶段中扮演各种角色，并且在不同阶段保持相似的提示可能不合理。因此，我们通过引入类型嵌入和转移层使LLM能够区分不同阶段³³与用于重新参数化的多层感知机（MLPs）不同，我们的转移层是所有提示共享参数的。因此，在我们模型的训练过程中，训练参数更少。我们在表[3](#S3.T3
    "表3 ‣ 3.2 实验结果 ‣ 3 实验 ‣ DeMPT：解码增强的多阶段提示调整，以使LLM更好地成为上下文感知翻译器")中比较了不同调整方法之间的训练参数数量，并在附录[D](#A4
    "附录D 转移层和类型嵌入的效果 ‣ DeMPT：解码增强的多阶段提示调整，以使LLM更好地成为上下文感知翻译器")中分析了转移层的效果。
- en: '|  | ${\bf P}_{r}=\left(\tanh\left({\bf O}_{r}W_{1}\right)\right)W_{2}+\text{TypeEmb}\left(r\right),$
    |  | (22) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\bf P}_{r}=\left(\tanh\left({\bf O}_{r}W_{1}\right)\right)W_{2}+\text{TypeEmb}\left(r\right),$
    |  | (22) |'
- en: where ${\bf O}_{r}\in\mathbb{R}^{L\times 2q\times d}$ represents either phase
    1, phase 2, or phase 3.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bf O}_{r}\in\mathbb{R}^{L\times 2q\times d}$ 代表阶段1、阶段2或阶段3。
- en: 2.4 Training Objective
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 训练目标
- en: 'We employ the cross-entropy loss as the training objective of our model. Given
    a training instance $(\mathcal{C},S,T)$, its training loss is defined as:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用交叉熵损失作为我们模型的训练目标。给定一个训练实例 $(\mathcal{C},S,T)$，其训练损失定义为：
- en: '|  | $\mathcal{L}\left(\mathcal{C},S,T\right)=-\frac{1}{&#124;T&#124;}\sum_{t=1}^{&#124;T&#124;}\text{log}~{}p_{e}\left(y_{t}&#124;S,\mathcal{C},y_{<t}\right).$
    |  | (23) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}\left(\mathcal{C},S,T\right)=-\frac{1}{|T|}\sum_{t=1}^{|T|}\text{log}~{}p_{e}\left(y_{t}|S,\mathcal{C},y_{<t}\right).$
    |  | (23) |'
- en: 'Notably, the parameters in LLM, including $W$ in Eq. [16](#S2.E16 "In Decoding
    Phase. ‣ 2.1 Multi-phase Encoding and Decoding ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
     [18](#S2.E18 "In 2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
     [19](#S2.E19 "In 2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators"),
    are frozen during training.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，LLM中的参数，包括方程[16](#S2.E16 "在解码阶段。 ‣ 2.1 多阶段编码和解码 ‣ 2 方法论 ‣ DeMPT：解码增强的多阶段提示调整，以使LLM更好地成为上下文感知翻译器")中的$W$，[18](#S2.E18
    "在2.2增强解码阶段 ‣ 2 方法论 ‣ DeMPT：解码增强的多阶段提示调整，以使LLM更好地成为上下文感知翻译器")，[19](#S2.E19 "在2.2增强解码阶段
    ‣ 2 方法论 ‣ DeMPT：解码增强的多阶段提示调整，以使LLM更好地成为上下文感知翻译器")，在训练过程中是被冻结的。
- en: 3 Experimentation
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 实验
- en: '| Model | ZH$\rightarrow$EN |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ZH$\rightarrow$EN |'
- en: '| BLEU | COMET | BLEU | COMET | BLEU | COMET | BLEU | COMET | BLEU | COMET
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| BLEU | COMET | BLEU | COMET | BLEU | COMET | BLEU | COMET | BLEU | COMET
    |'
- en: '| Trans. | 29.86 | 0.8406 | 38.53 | 0.8545 | 41.44 | 0.8682 | 48.74 | 0.8783
    | 32.25 | 0.8169 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 29.86 | 0.8406 | 38.53 | 0.8545 | 41.44 | 0.8682 | 48.74 | 0.8783 |
    32.25 | 0.8169 |'
- en: '| llama-2-7b as foundation model |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-7b 作为基础模型 |'
- en: '| MT-LoRA | 27.43 | 0.8511 | 38.18 | 0.8647 | 40.96 | 0.8712 | 47.52 | 0.8733
    | 33.00 | 0.8311 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| MT-LoRA | 27.43 | 0.8511 | 38.18 | 0.8647 | 40.96 | 0.8712 | 47.52 | 0.8733
    | 33.00 | 0.8311 |'
- en: '| MT-PT | 31.32 | 0.8565 | 41.92 | 0.8675 | 43.56 | 0.8752 | 51.32 | 0.8819
    | 35.46 | 0.8333 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| MT-PT | 31.32 | 0.8565 | 41.92 | 0.8675 | 43.56 | 0.8752 | 51.32 | 0.8819
    | 35.46 | 0.8333 |'
- en: '| CMT-PT | 31.13 | 0.8387 | 42.01 | 0.8699 | 43.11 | 0.8762 | 51.66 | 0.8823
    | 35.91 | 0.8396 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| CMT-PT | 31.13 | 0.8387 | 42.01 | 0.8699 | 43.11 | 0.8762 | 51.66 | 0.8823
    | 35.91 | 0.8396 |'
- en: '| MPT | *33.21 | 0.8645 | †43.11 | 0.8744 | *43.88 | 0.8824 | †52.01 | 0.8913
    | †36.49 | 0.8456 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| MPT | *33.21 | 0.8645 | †43.11 | 0.8744 | *43.88 | 0.8824 | †52.01 | 0.8913
    | †36.49 | 0.8456 |'
- en: '| DeMPT | *33.89 | 0.8658 | †43.71 | 0.8816 | *44.69 | 0.8899 | †53.10 | 0.8979
    | †36.55 | 0.8438 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| DeMPT | *33.89 | 0.8658 | †43.71 | 0.8816 | *44.69 | 0.8899 | †53.10 | 0.8979
    | †36.55 | 0.8438 |'
- en: '| bloomz-7b1-mt as foundation model |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| bloomz-7b1-mt 作为基础模型 |'
- en: '| MT-LoRA | 25.79 | 0.8466 | 35.67 | 0.8601 | 35.17 | 0.8522 | 46.32 | 0.8644
    | 28.01 | 0.8012 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| MT-LoRA | 25.79 | 0.8466 | 35.67 | 0.8601 | 35.17 | 0.8522 | 46.32 | 0.8644
    | 28.01 | 0.8012 |'
- en: '| MT-PT | 30.99 | 0.8520 | 40.49 | 0.8661 | 37.76 | 0.8579 | 50.68 | 0.8823
    | 30.27 | 0.8106 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| MT-PT | 30.99 | 0.8520 | 40.49 | 0.8661 | 37.76 | 0.8579 | 50.68 | 0.8823
    | 30.27 | 0.8106 |'
- en: '| CMT-PT | 30.82 | 0.8504 | 40.31 | 0.8639 | 38.01 | 0.8601 | 50.26 | 0.8832
    | 29.80 | 0.8108 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| CMT-PT | 30.82 | 0.8504 | 40.31 | 0.8639 | 38.01 | 0.8601 | 50.26 | 0.8832
    | 29.80 | 0.8108 |'
- en: '| MPT | *31.81 | 0.8601 | *41.11 | 0.8766 | †38.99 | 0.8669 | *51.33 | 0.8910
    | *30.99 | 0.8201 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| MPT | *31.81 | 0.8601 | *41.11 | 0.8766 | †38.99 | 0.8669 | *51.33 | 0.8910
    | *30.99 | 0.8201 |'
- en: '| DeMPT | *32.46 | 0.8649 | *41.92 | 0.8790 | †40.06 | 0.8703 | *52.25 | 0.8990
    | *31.79 | 0.8253 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| DeMPT | *32.46 | 0.8649 | *41.92 | 0.8790 | †40.06 | 0.8703 | *52.25 | 0.8990
    | *31.79 | 0.8253 |'
- en: 'Table 1: Results of different systems on sacreBLEU and COMET metrics. DeMPT/MPT
    is our proposed Multi-phase Prompt Tuning approach with/without Decoding-enhanced
    strategy (in Sec. [2.2](#S2.SS2 "2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣
    DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware
    Translators")). Scores with bold indicate the best performance. * or † indicates
    the gains are statistically significant over MT-PT or CMT-PT with $p$<0.01 Koehn
    ([2004](#bib.bib12)).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1：不同系统在 sacreBLEU 和 COMET 指标上的结果。DeMPT/MPT 是我们提出的多阶段提示调优方法，具有/不具有解码增强策略（在第
    [2.2](#S2.SS2 "2.2 Enhanced Decoding Phase ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")
    节）。加粗的分数表示最佳表现。* 或 † 表示相较于 MT-PT 或 CMT-PT 的增益在统计上显著，$p$<0.01 Koehn ([2004](#bib.bib12))。'
- en: '| Model | ZH$\rightarrow$ |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | ZH$\rightarrow$ |'
- en: '| Trans. | 47.63 | 54.41 | 58.29 | 62.52 | 48.79 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Trans. | 47.63 | 54.41 | 58.29 | 62.52 | 48.79 |'
- en: '| llama-2-7b as foundation model |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| llama-2-7b 作为基础模型 |'
- en: '| MT-LoRA | 44.83 | 54.52 | 57.72 | 62.18 | 49.06 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| MT-LoRA | 44.83 | 54.52 | 57.72 | 62.18 | 49.06 |'
- en: '| MT-PT | 49.49 | 57.87 | 60.89 | 65.02 | 52.59 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| MT-PT | 49.49 | 57.87 | 60.89 | 65.02 | 52.59 |'
- en: '| CMT-PT | 49.53 | 58.27 | 61.23 | 65.89 | 53.34 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| CMT-PT | 49.53 | 58.27 | 61.23 | 65.89 | 53.34 |'
- en: '| MPT | 51.56 | 59.56 | 62.15 | 67.14 | 54.18 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| MPT | 51.56 | 59.56 | 62.15 | 67.14 | 54.18 |'
- en: '| DeMPT | 52.68 | 60.33 | 63.11 | 67.95 | 54.34 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| DeMPT | 52.68 | 60.33 | 63.11 | 67.95 | 54.34 |'
- en: '| bloomz-7b1-mt as foundation model |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| bloomz-7b1-mt 作为基础模型 |'
- en: '| MT-LoRA | 43.23 | 51.82 | 51.12 | 61.77 | 43.29 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| MT-LoRA | 43.23 | 51.82 | 51.12 | 61.77 | 43.29 |'
- en: '| MT-PT | 49.48 | 56.81 | 55.40 | 64.71 | 46.14 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| MT-PT | 49.48 | 56.81 | 55.40 | 64.71 | 46.14 |'
- en: '| CMT-PT | 49.61 | 57.05 | 55.81 | 65.12 | 46.09 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| CMT-PT | 49.61 | 57.05 | 55.81 | 65.12 | 46.09 |'
- en: '| MPT | 50.22 | 57.93 | 56.69 | 66.25 | 47.29 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MPT | 50.22 | 57.93 | 56.69 | 66.25 | 47.29 |'
- en: '| DeMPT | 50.62 | 58.30 | 57.34 | 67.12 | 48.00 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| DeMPT | 50.62 | 58.30 | 57.34 | 67.12 | 48.00 |'
- en: 'Table 2: Results of different systems on BlonDe metric.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同系统在 BlonDe 指标上的结果。
- en: We build our approach upon two open-source LLMs, namely, llama-2-7b⁴⁴4[https://huggingface.co/meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)
    and bloomz-7b1-mt⁵⁵5[https://huggingface.co/bigscience/bloomz-7b1-mt](https://huggingface.co/bigscience/bloomz-7b1-mt).
    We verify the effectiveness of our proposed approach on five translation tasks,
    including {Chinese (ZH), French (FR), German (DE), Spanish (ES), Russian (RU)}$\rightarrow$English
    (EN).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个开源 LLMs 上构建了我们的方法，即 llama-2-7b⁴⁴4[https://huggingface.co/meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf)
    和 bloomz-7b1-mt⁵⁵5[https://huggingface.co/bigscience/bloomz-7b1-mt](https://huggingface.co/bigscience/bloomz-7b1-mt)。我们验证了我们提出的方法在五个翻译任务上的有效性，包括
    {中文 (ZH)，法语 (FR)，德语 (DE)，西班牙语 (ES)，俄语 (RU)}$\rightarrow$英语 (EN)。
- en: 3.1 Experimental Settings
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实验设置
- en: Datasets and Preprocessing.
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和预处理。
- en: 'The corpus of all translation tasks is extracted from New-Comentary-v18. See
    Appendix [A](#A1 "Appendix A Datasets ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators") for splitting and
    statistics of the training set, valid set, and test set. We use the tokenizer
    of foundation models to process the input data and no any other preprocessing
    is performed.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '所有翻译任务的语料库都从 New-Comentary-v18 中提取。有关训练集、验证集和测试集的拆分和统计信息，请参见附录 [A](#A1 "Appendix
    A Datasets ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs
    Be Better Context-aware Translators")。我们使用基础模型的分词器来处理输入数据，未进行其他预处理。'
- en: Baselines.
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线。
- en: 'We compare our approach against four baselines:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的方法与四个基线方法进行了比较：
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Transformer (Trans.): It is an encoder-decoder Transformer-base model Vaswani
    et al. ([2017](#bib.bib33)) that is trained from scratch.'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Transformer (Trans.)：这是一个从零开始训练的编码器-解码器 Transformer 基础模型 Vaswani 等人 ([2017](#bib.bib33))。
- en: •
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MT-LoRA: It is a tuned LLM adapted to NMT task via the tuning method of Low-Rank
    Adaptation Hu et al. ([2022](#bib.bib6)), which makes large-scale pre-training
    models adapt to a new task by injecting a trainable rank decomposition matrice
    into each layer of the Transformer architecture.'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MT-LoRA：这是一个通过低秩适应 Hu 等（[2022](#bib.bib6)）的调优方法适配到 NMT 任务的调优 LLM，它通过将可训练的秩分解矩阵注入到
    Transformer 架构的每一层，使大规模预训练模型适应新任务。
- en: •
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'MT-PT: It is a tuned LLM adapted to NMT task via the deep prompt tuning with
    MLPs reparameterization,⁶⁶6We attempt to remove reparameterization but experience
    a significant decline in performance. which only tunes continuous prompts with
    a frozen language model.'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MT-PT：这是一个通过深度提示调优与 MLPs 重新参数化适配到 NMT 任务的调优 LLM，⁶⁶6我们尝试去掉重新参数化，但性能显著下降。它仅调优连续提示，并冻结语言模型。
- en: •
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'CMT-PT: Similar to MT-PT, it is also a tuned LLM via the deep prompt tuning
    with MLPs reparameterization. Unlike MT-PT, it utilizes inter-sentence context
    within the concatenation strategy, as depicted in Figure [1](#S1.F1 "Figure 1
    ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") (a).'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CMT-PT：类似于 MT-PT，它也是通过深度提示调优与 MLPs 重新参数化的调优 LLM。不同于 MT-PT，它利用了在连接策略中的句间上下文，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators") (a) 所示。'
- en: Among them, Transformer, MT-LoRA, and MT-PT are context-agnostic systems while
    CMT-PT is a context-aware system. For a fair comparison, we ensure that all context-aware
    systems, including CMT-PT, MPT, and DeMPT, incorporate identical inter-sentence
    context.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，Transformer、MT-LoRA 和 MT-PT 是上下文无关系统，而 CMT-PT 是上下文感知系统。为了公平比较，我们确保所有上下文感知系统，包括
    CMT-PT、MPT 和 DeMPT，都包含相同的句间上下文。
- en: Model Setting and Training.
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型设置和训练。
- en: 'For the Transformer model, we implement it upon Fairseq Ott et al. ([2019](#bib.bib27)).
    For MT-LoRA models, we set the rank of trainable matrices as 16 which performs
    best in our preliminary experiment. For all MT-PT models, CMT-PT models, and our
    models, we set the prompt length $q$ to 1/3\. More details of training are provided
    in Appendix [B](#A2 "Appendix B Training Details ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators").'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 Transformer 模型，我们基于 Fairseq Ott 等（[2019](#bib.bib27)）进行实现。对于 MT-LoRA 模型，我们将可训练矩阵的秩设置为
    16，这在我们的初步实验中表现最好。对于所有 MT-PT 模型、CMT-PT 模型和我们的模型，我们将提示长度 $q$ 设置为 1/3\. 更多训练细节见附录
    [B](#A2 "Appendix B Training Details ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators")。'
- en: Evaluation.
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估。
- en: 'We use sacreBLEU (accuracy-related metric)⁷⁷7Signature: nrefs:1|case:mixed|eff:no|tok:13a|'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '我们使用 sacreBLEU（准确度相关指标）⁷⁷7Signature: nrefs:1|case:mixed|eff:no|tok:13a|'
- en: smooth:exp|version:2.3.1 Post ([2018](#bib.bib28)), COMET (semantics-related
    metric) with the wmt22-comet-da model⁸⁸8[https://github.com/Unbabel/COMET](https://github.com/Unbabel/COMET)
     Rei et al. ([2020](#bib.bib30)), and BlonDe (discourse-related metric) Jiang
    et al. ([2022](#bib.bib9)) as the evaluation metrics.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: smooth:exp|version:2.3.1 Post（[2018](#bib.bib28)），使用 COMET（语义相关度指标）与 wmt22-comet-da
    模型⁸⁸8[https://github.com/Unbabel/COMET](https://github.com/Unbabel/COMET)  Rei
    等（[2020](#bib.bib30)）以及 BlonDe（话语相关指标）Jiang 等（[2022](#bib.bib9)）作为评估指标。
- en: 3.2 Experimental Results
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 实验结果
- en: 'The main experimental results are presented in Tables [1](#S3.T1 "Table 1 ‣
    3 Experimentation ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") and [2](#S3.T2 "Table 2 ‣ 3 Experimentation
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators"). Additionally, a comparison of the number of trainable
    parameters is presented in Table [3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results
    ‣ 3 Experimentation ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") across different tuning methods. From
    these results we have the following observations:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '主要实验结果展示在表格 [1](#S3.T1 "Table 1 ‣ 3 Experimentation ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")
    和 [2](#S3.T2 "Table 2 ‣ 3 Experimentation ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators") 中。此外，不同调优方法的可训练参数数量比较见表格
    [3](#S3.T3 "Table 3 ‣ 3.2 Experimental Results ‣ 3 Experimentation ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")。从这些结果中我们得到以下观察：'
- en: •
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The encoder-decoder Transformer model (Trans.) performs better than the LLMs
    with LoRA tuning in most translation directions in BLEU score. For example, when
    utilizing llama-2-7b as the foundation model, Transformer surpasses MT-LoRA an
    average of 0.75 BLEU score across all translation tasks. However, MT-LoRA model
    outperforms Trans. in terms of COMET, suggesting that translations from LLMs may
    align more closely with human preferences.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在大多数翻译方向上，编码器-解码器 Transformer 模型（Trans.）在 BLEU 得分上表现优于使用 LoRA 调优的 LLM。例如，当使用
    llama-2-7b 作为基础模型时，Transformer 在所有翻译任务中平均比 MT-LoRA 高出 0.75 BLEU 得分。然而，MT-LoRA
    模型在 COMET 指标上优于 Trans.，这表明 LLM 的翻译可能与人类偏好更为一致。
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The MT-PT model presents superior performance compared to the MT-LoRA model
    in terms of BLEU, COMET, and BlonDe. Taking bloomz-7b1-mt as the foundation model,
    the MT-PT model outperforms the MT-LoRA model by an average of 3.84 BLEU score,
    3.51 BlonDe score, and 0.0047 COMET score. Nevertheless, the MT-PT model sacrifices
    efficiency for performance, introducing more trainable parameters (13.87% vs.
    0.12%).
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: MT-PT 模型在 BLEU、COMET 和 BlonDe 指标上表现优于 MT-LoRA 模型。以 bloomz-7b1-mt 作为基础模型时，MT-PT
    模型在 BLEU 得分、BlonDe 得分和 COMET 得分上分别比 MT-LoRA 模型高出平均 3.84、3.51 和 0.0047。然而，MT-PT
    模型在性能上有所牺牲，引入了更多的可训练参数（13.87% 对比 0.12%）。
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Leveraging the inter-sentence context is helpful in alleviating discourse-related
    issues. For example, with bloomz-7b1-mt used as the foundation model, the CMT-PT
    model, despite underperforming in BLEU and COMET compared to the MT-PT model,
    excels in discourse-related BlonDe scores (averaging 57.66 vs. 57.17).
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 利用句间上下文有助于缓解话语相关的问题。例如，尽管使用 bloomz-7b1-mt 作为基础模型的 CMT-PT 模型在 BLEU 和 COMET 指标上表现不如
    MT-PT 模型，但在话语相关的 BlonDe 得分上表现优异（平均 57.66 对比 57.17）。
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our MPT/DeMPT model outperforms all baselines across all translation tasks.
    For example, when using llama-2-7b as the foundation model, our MPT model achieves
    an average gain of 1.62/1.45/2.03 in BLEU/COMET/BlonDe score compared to the CMT-PT
    model. Furthermore, our decoding-enhance strategy enhances the capacity of LLMs
    in context-aware NMT, with DeMPT outperforming MPT in BLEU/COMET/BlonDe score
    (averaging 42.39/0.8758/59.68 vs. 41.74/0.8716/58.91).
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的 MPT/DeMPT 模型在所有翻译任务中均优于所有基线模型。例如，当使用 llama-2-7b 作为基础模型时，我们的 MPT 模型在 BLEU/COMET/BlonDe
    得分上比 CMT-PT 模型平均提高了 1.62/1.45/2.03。此外，我们的解码增强策略提升了 LLM 在上下文感知 NMT 方面的能力，DeMPT
    在 BLEU/COMET/BlonDe 得分上（平均 42.39/0.8758/59.68 对比 41.74/0.8716/58.91）优于 MPT。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The model built upon llama-2-7b as the foundation model outperforms the one
    using bloomz-7b1-mt, suggesting that llama-2-7b serves as a more robust foundation
    model for translation tasks.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于 llama-2-7b 作为基础模型构建的模型优于使用 bloomz-7b1-mt 的模型，表明 llama-2-7b 是更为稳健的翻译任务基础模型。
- en: '|  | MT-LoRA | MT-PT/CMT-PT | DeMPT |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | MT-LoRA | MT-PT/CMT-PT | DeMPT |'
- en: '| Trainable Para. | 0.12% | 13.87% | 3.11% |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 可训练参数 | 0.12% | 13.87% | 3.11% |'
- en: 'Table 3: Proportion of trainable parameters against total parameters for different
    tuning methods.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同调优方法下可训练参数占总参数的比例。
- en: 4 Discussion
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 讨论
- en: In this section, we use bloomz-7b1-mt as the foundation model to discuss and
    analyze our approach. See Appendix C$\sim$E for further discussions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们以 bloomz-7b1-mt 作为基础模型，探讨和分析我们的方法。有关进一步讨论，请参见附录 C$\sim$E。
- en: 4.1 Effect of Length of Inter-sentence Context
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 句间上下文长度的影响
- en: 'For efficient training, we define the inter-sentence context in Section [2](#S2
    "2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") as previous sentences with a total
    tokens not exceeding 256\. We are curious about the potential impact of inter-sentence
    length on the performance of our approach. Consequently, we extend the inter-sentence
    context length from 256 to 1024 and assess the performance of our approach in
    the ZH$\rightarrow$EN task.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '为了高效训练，我们在第 [2](#S2 "2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators") 节定义了句间上下文为前面的句子，总
    tokens 不超过 256。我们对句间长度对我们方法性能的潜在影响感到好奇。因此，我们将句间上下文长度从 256 扩展到 1024，并评估我们方法在 ZH$\rightarrow$EN
    任务中的表现。'
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.1 Effect of Length of Inter-sentence Context
    ‣ 4 Discussion ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators") shows the performance trend of the
    CMT-PT model and our DeMPT model. As the length of the inter-sentence context
    increases, both models exhibit a slight enhancement in both BLEU and BlonDe scores.
    Interestingly, our model with a 256-token inter-sentence context outperforms the
    CMT-PT model with a 1024-token inter-sentence context in both BLEU and BlonDe
    scores. This further suggests the effectiveness of our approach in harnessing
    the capabilities of LLMs for context-aware NMT compared to the concatenation strategy.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S4.F4 "图 4 ‣ 4.1 句间上下文长度的影响 ‣ 4 讨论 ‣ DeMPT: 解码增强的多阶段提示调整以提高 LLM 的上下文感知翻译能力")
    显示了 CMT-PT 模型和我们的 DeMPT 模型的性能趋势。随着句间上下文长度的增加，两个模型在 BLEU 和 BlonDe 分数上均有轻微提升。有趣的是，我们的模型在
    256-token 的句间上下文下，在 BLEU 和 BlonDe 分数上都优于 CMT-PT 模型在 1024-token 句间上下文下的表现。这进一步表明，与拼接策略相比，我们的方法在发挥
    LLM 上下文感知 NMT 能力方面的有效性。'
- en: '![Refer to caption](img/52183a911cdeb42e04fadaa65e37f732.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/52183a911cdeb42e04fadaa65e37f732.png)'
- en: 'Figure 4: Performance of CMT-PT and our DeMPT on ZH$\rightarrow$EN test set
    when using different inter-sentence context lengths.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：在使用不同句间上下文长度时，CMT-PT 和我们的 DeMPT 在 ZH$\rightarrow$EN 测试集上的表现。
- en: 4.2 Effect of Prompt Length
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 提示长度的影响
- en: '![Refer to caption](img/5699259a7947b50ea951e7c947248c6f.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5699259a7947b50ea951e7c947248c6f.png)'
- en: 'Figure 5: Performance of MT-PT, CMT-PT, and our DeMPT on ZH$\rightarrow$EN
    test set when using different lengths of the trainable prompts.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：在使用不同长度的可训练提示时，MT-PT、CMT-PT 和我们的 DeMPT 在 ZH$\rightarrow$EN 测试集上的表现。
- en: As our approach is implemented based on deep prompt tuning, next we compare
    the impact of the trainable prompt length for MT-PT, CMT-PT, and our DeMPT.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的方法是基于深度提示调整实现的，接下来我们比较 MT-PT、CMT-PT 和我们 DeMPT 的可训练提示长度的影响。
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ 4.2 Effect of Prompt Length ‣ 4 Discussion ‣
    DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware
    Translators") shows the performance curves when increasing the prompt length from
    32 to 128\. We observe that increased prompt length tends to enhance performance
    for both BLEU and BlonDe, yet the gains exhibit diminishing returns. This finding
    is consistent with that in Li and Liang ([2021](#bib.bib15)); Lester et al. ([2021](#bib.bib14));
    Tan et al. ([2022](#bib.bib32)). We also observe that DeMPT with a prompt length
    of 64 outperforms both MT-PT and CMT-PT with a prompt length of 128 on both metrics,
    suggesting the superiority of our approach over the concatenation strategy in
    enhancing LLMs’ capacity for context-aware NMT.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '图[5](#S4.F5 "图 5 ‣ 4.2 提示长度的影响 ‣ 4 讨论 ‣ DeMPT: 解码增强的多阶段提示调整以提高 LLM 的上下文感知翻译能力")
    显示了在将提示长度从 32 增加到 128 时的性能曲线。我们观察到，增加提示长度趋向于提升 BLEU 和 BlonDe 的性能，但收益呈现递减效应。这个发现与
    Li 和 Liang（[2021](#bib.bib15)）、Lester 等（[2021](#bib.bib14)）、Tan 等（[2022](#bib.bib32)）的结果一致。我们还观察到，提示长度为
    64 的 DeMPT 在两个指标上均优于提示长度为 128 的 MT-PT 和 CMT-PT，这表明我们的方法在提升 LLM 的上下文感知 NMT 能力方面优于拼接策略。'
- en: 4.3 Comparison of Inference Speed
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 推理速度比较
- en: '| Model | Speed | BLEU |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 速度 | BLEU |'
- en: '| MT-PT | 0.75 sec/sent. | 30.99 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| MT-PT | 0.75 sec/sent. | 30.99 |'
- en: '| CMT-PT | 0.77 sec/sent. | 30.82 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| CMT-PT | 0.77 sec/sent. | 30.82 |'
- en: '| MPT | 0.78 sec/sent. | 31.81 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| MPT | 0.78 sec/sent. | 31.81 |'
- en: '| DeMPT | 0.79 sec/sent. | 32.46 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| DeMPT | 0.79 sec/sent. | 32.46 |'
- en: 'Table 4: Comparison of inference speed on ZH$\rightarrow$EN translation task.
    Speed is measured on the test set using 4 GPUs. sec/sent. means seconds spent
    for decoding each sentence. Note that the reparameterization is not needed during
    inference Li and Liang ([2021](#bib.bib15)).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在 ZH$\rightarrow$EN 翻译任务上的推理速度比较。速度在测试集上使用 4 个 GPU 测量。sec/sent. 表示解码每个句子的所用时间。请注意，推理过程中不需要重新参数化
    Li 和 Liang（[2021](#bib.bib15)）。
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.3 Comparison of Inference Speed ‣ 4 Discussion
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators") compares the inference speed of different models on
    ZH$\rightarrow$EN translation task. Our MPT and DeMPT models, dividing the context-aware
    NMT process into three separate phases, demonstrates comparable inference speed
    to the single-phase MT-PT and CMT-PT models, with only a marginal drop of 0.02
    seconds per sentence in decoding. This illustrates the efficiency of our approach
    without introducing significant computational overhead.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '表[4](#S4.T4 "表 4 ‣ 4.3 推理速度比较 ‣ 4 讨论 ‣ DeMPT: 解码增强的多阶段提示调整，使大型语言模型成为更好的上下文感知翻译器")比较了不同模型在
    ZH$\rightarrow$EN 翻译任务上的推理速度。我们的 MPT 和 DeMPT 模型将上下文感知的 NMT 过程分为三个独立的阶段，显示出与单阶段
    MT-PT 和 CMT-PT 模型相当的推理速度，仅在解码时每句减少了 0.02 秒。这展示了我们的方法在不引入显著计算开销的情况下的高效性。'
- en: 4.4 Performance on Contrastive Test Set
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 对比测试集上的表现
- en: We evaluate the models’ ability to resolve discourse inconsistencies using the
    contrastive test set proposed by Voita et al. ([2019a](#bib.bib34)), which focuses
    on four discourse phenomena such as deixis, lexicon consistency (lex.c), ellipsis
    inflection (ell.infl), and verb phrase ellipsis (ell.VP) in English$\rightarrow$Russian
    translation. Within the test set, each instance comprises a positive translation
    and several negative ones that vary by only one specific word. The purpose of
    the contrastive test set is to assess whether a model is more inclined to generate
    a correct translation as opposed to incorrect variations.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Voita 等人（[2019a](#bib.bib34)）提出的对比测试集来评估模型解决话语不一致的能力，该测试集关注四种话语现象，如指示词、词汇一致性（lex.c）、省略词形变化（ell.infl）和动词短语省略（ell.VP）在英俄翻译中的表现。在测试集中，每个实例包括一个正确翻译和几个仅通过一个特定单词有所不同的错误翻译。对比测试集的目的是评估模型是否更倾向于生成正确的翻译，而非错误的变体。
- en: 'Table [5](#S4.T5 "Table 5 ‣ 4.4 Performance on Contrastive Test Set ‣ 4 Discussion
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators") lists the accuracy of translation prediction on the
    contrastive test set for MT-PT, CMT-PT and DeMPT. Compared to the context-agnostic
    MT-PT model, both context-aware CMT-PT and DeMPT models show substantial improvements
    across the four discourse phenomena. Additionally, DeMPT demonstrates the best
    performance, surpassing CMT-PT by an average accuracy margin of 3.8.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '表[5](#S4.T5 "表 5 ‣ 4.4 对比测试集上的表现 ‣ 4 讨论 ‣ DeMPT: 解码增强的多阶段提示调整，使大型语言模型成为更好的上下文感知翻译器")列出了
    MT-PT、CMT-PT 和 DeMPT 在对比测试集上的翻译预测准确率。与上下文无关的 MT-PT 模型相比，上下文感知的 CMT-PT 和 DeMPT
    模型在四种话语现象上都有显著的改善。此外，DeMPT 展现了最佳表现，平均准确率比 CMT-PT 高出 3.8。'
- en: '| Model | deixis | lex.c | ell.infl | ell.VP | Avg. |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 指示词 | 词汇一致性 | 省略词形变化 | 动词短语省略 | 平均 |'
- en: '| MT-PT | 50.0 | 45.7 | 53.0 | 28.6 | 44.3 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| MT-PT | 50.0 | 45.7 | 53.0 | 28.6 | 44.3 |'
- en: '| CMT-PT | 80.2 | 46.1 | 74.3 | 75.3 | 68.9 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| CMT-PT | 80.2 | 46.1 | 74.3 | 75.3 | 68.9 |'
- en: '| DeMPT | 80.1 | 55.7 | 75.9 | 79.3 | 72.7 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| DeMPT | 80.1 | 55.7 | 75.9 | 79.3 | 72.7 |'
- en: 'Table 5: Accuracy [%] of translation prediction for four discourse phenomena
    on the English $\rightarrow$ Russian contrastive test set.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：英俄对比测试集中四种话语现象的翻译预测准确率 [%]。
- en: 4.5 Human Evaluation
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 人工评估
- en: We use the Direct Assessment (DA) method Graham et al. ([2017](#bib.bib5)) to
    manually assess the quality of translations generated by DeMPT and CMT-PT. In
    this assessment, human evaluators compare the meaning of the MT output with a
    human-produced reference translation, working within the same language.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 Graham 等人（[2017](#bib.bib5)）的直接评估（DA）方法手动评估 DeMPT 和 CMT-PT 生成的翻译质量。在此评估中，人工评估者将
    MT 输出的意义与人工生产的参考翻译进行比较，均在同一语言内进行。
- en: 'Specifically, we randomly select 5 documents with a total of 200 groups of
    sentences from the ZH$\rightarrow$EN test set. To avoid potential bias in evaluation,
    we recruit 6 professional translators and ensure each translation from DeMPT or
    CMT-PT is scored twice by two translators. Table [6](#S4.T6 "Table 6 ‣ 4.5 Human
    Evaluation ‣ 4 Discussion ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning
    for Making LLMs Be Better Context-aware Translators") shows the DA scores for
    CMT-PT and DeMPT. Our DeMPT outperforms CMT-PT by 7.14 DA score, providing strong
    evidence for the effectiveness of our approach. Further details and results regarding
    the DA can be found in Appendix [C](#A3 "Appendix C Details of Human Evaluation
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators").'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '具体来说，我们随机选择了5篇文档，共200组句子，从ZH$\rightarrow$EN测试集中。为了避免评估中的潜在偏见，我们招募了6名专业翻译人员，并确保DeMPT或CMT-PT的每个翻译都由两位翻译人员打分两次。表[6](#S4.T6
    "Table 6 ‣ 4.5 Human Evaluation ‣ 4 Discussion ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators")展示了CMT-PT和DeMPT的DA评分。我们的DeMPT比CMT-PT高出7.14
    DA评分，提供了我们方法有效性的有力证据。有关DA的更多详细信息和结果，请参见附录[C](#A3 "Appendix C Details of Human
    Evaluation ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs
    Be Better Context-aware Translators")。'
- en: '| Model | Score_1 | Score_2 | Average |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 评分_1 | 评分_2 | 平均值 |'
- en: '| CMT-PT | 79.00 | 80.17 | 79.59 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| CMT-PT | 79.00 | 80.17 | 79.59 |'
- en: '| DeMPT | 86.17 (+7.17) | 87.30 (+7.13) | 86.73 (+7.14) |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| DeMPT | 86.17 (+7.17) | 87.30 (+7.13) | 86.73 (+7.14) |'
- en: 'Table 6: Human DA scores for CMT-PT and DeMPT on ZH$\rightarrow$EN translation
    task.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：ZH$\rightarrow$EN翻译任务中CMT-PT和DeMPT的人工DA评分。
- en: 5 Related Work
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Large Language Models for Context-aware Machine Translation.
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大语言模型在上下文感知机器翻译中的应用。
- en: While traditional context-aware neural machine translation (NMT) has seen considerable
    progress in recent years Jean et al. ([2017](#bib.bib8)); Wang et al. ([2017](#bib.bib39));
    Voita et al. ([2018](#bib.bib36)); Maruf et al. ([2019](#bib.bib22)); Kang et al.
    ([2020](#bib.bib10)); Bao et al. ([2021](#bib.bib2)); Sun et al. ([2022](#bib.bib31));
    Bao et al. ([2023](#bib.bib1)), the effective integration of large language models
    (LLMs) to model inter-sentence context and enhance context-aware translation remains
    an area of limited exploration. Existing studies mainly focus on the assessment
    of LLMs’ ability in discourse modeling. For example, Wang et al. ([2023](#bib.bib37))
    approach context-aware NMT as a task involving long sequence generation, employing
    a concatenation strategy, and conduct comprehensive evaluations of LLMs such as
    ChatGPT and GPT-4\. Their focus includes the impact of context-aware prompts,
    comparisons with translation models, and an in-depth analysis of discourse modeling
    ability. Similarly, Karpinska and Iyyer ([2023](#bib.bib11)) engage professional
    translators to evaluate LLMs’ capacity in context-aware NMT. In contrast, Wu et al.
    ([2024](#bib.bib40)) compare the effectiveness of various parameter-efficient
    fine-tuning methods on moderately-sized LLMs for context-aware NMT. Besides, Wu
    and Hu ([2023](#bib.bib41)) explore the prompt engineering with GPT language models
    specifically for document-level (context-aware) MT while Li et al. ([2024](#bib.bib17))
    experiment with combining sentence-level and document-level translation instructions
    of varying lengths to fine-tune LLMs.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管传统的上下文感知神经机器翻译（NMT）近年来取得了相当大的进展，Jean等人 ([2017](#bib.bib8))；Wang等人 ([2017](#bib.bib39))；Voita等人
    ([2018](#bib.bib36))；Maruf等人 ([2019](#bib.bib22))；Kang等人 ([2020](#bib.bib10))；Bao等人
    ([2021](#bib.bib2))；Sun等人 ([2022](#bib.bib31))；Bao等人 ([2023](#bib.bib1))，但有效地整合大语言模型（LLMs）以建模句间上下文和增强上下文感知翻译仍然是一个探索有限的领域。现有研究主要集中在评估LLMs在话语建模中的能力。例如，Wang等人
    ([2023](#bib.bib37)) 将上下文感知NMT视为涉及长序列生成的任务，采用串联策略，并对LLMs如ChatGPT和GPT-4进行全面评估。他们的重点包括上下文感知提示的影响、与翻译模型的比较以及对话语建模能力的深入分析。同样，Karpinska和Iyyer
    ([2023](#bib.bib11)) 招募专业翻译人员评估LLMs在上下文感知NMT中的能力。相反，Wu等人 ([2024](#bib.bib40))
    比较了各种参数高效微调方法在中型LLMs上的上下文感知NMT效果。此外，Wu和Hu ([2023](#bib.bib41)) 探索了GPT语言模型的提示工程，特别是针对文档级（上下文感知）MT，而Li等人
    ([2024](#bib.bib17)) 则实验结合了不同长度的句子级和文档级翻译指令来微调LLMs。
- en: Prompt Tuning for Large Language Model.
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 大语言模型的提示调优。
- en: Liu et al. ([2021](#bib.bib19)) and Li and Liang ([2021](#bib.bib15)) propose
    to make LLMs adapt to various tasks by adding trainable prompts (also called continuous
    prompts) to the original input sequences. In this paradigm, only the continuous
    prompts are updated during training. Liu et al. ([2022](#bib.bib18)) further introduce
    deep prompt tuning, extending the idea by inserting trainable prompts into all
    layers of LLMs, rather than just the embedding layer. While these approaches lay
    the groundwork for a general framework, our focus lies in augmenting the performance
    of LLMs specifically for inter-sentence context modeling in context-aware NMT.
    Notably related, Tan et al. ([2022](#bib.bib32)) propose a multi-phase tuning
    approach to enhance the sentence-level translation performance of a multilingual
    GPT. However, the exploration of effective LLM tuning for addressing discourse-related
    challenges in the context-aware NMT domain remains underdeveloped.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Liu 等人 ([2021](#bib.bib19)) 和 Li 和 Liang ([2021](#bib.bib15)) 提出了通过向原始输入序列添加可训练的提示（也称为连续提示）来使
    LLM 适应各种任务。在这种范式中，只有连续提示在训练过程中会被更新。Liu 等人 ([2022](#bib.bib18)) 进一步引入了深度提示调优，扩展了这一思想，通过将可训练提示插入到
    LLM 的所有层中，而不仅仅是嵌入层。尽管这些方法为通用框架奠定了基础，但我们的重点是增强 LLM 在上下文感知 NMT 中对句间上下文建模的性能。相关研究中，Tan
    等人 ([2022](#bib.bib32)) 提出了多阶段调优方法，以提高多语言 GPT 的句子级翻译性能。然而，对于解决上下文感知 NMT 领域中话语相关挑战的有效
    LLM 调优仍然欠缺探索。
- en: 6 Conclusion
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we have examined the hypothesis that it is crucial to differentially
    model and leverage inter-sentence context and intra-sentence context when adapting
    LLMs to context-aware NMT. This stems from our observation that intra-sentence
    context exhibits a stronger correlation with the target sentence compared to inter-sentence
    context, owing to its richer parallel semantic information. To this end, we have
    proposed a novel decoding-enhanced multi-phase prompt tuning (DeMPT) approach
    to make LLMs aware of the differences between inter- and intra-sentence contexts,
    and further improve LLMs’ capacity in discourse modeling. We have evaluated our
    approach using two foundation models and present experimental results across five
    translation directions. Experimental results and discussions have demonstrated
    a significant enhancement in the performance of LLMs in context-aware NMT, manifesting
    as improved translation accuracy and a reduction in discourse-related issues.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们研究了在将 LLM 适应上下文感知 NMT 时，差异性建模和利用句间上下文与句内上下文的重要性。这源于我们观察到，句内上下文相对于句间上下文与目标句子的相关性更强，因为其包含更丰富的平行语义信息。为此，我们提出了一种新颖的解码增强多阶段提示调优（DeMPT）方法，使
    LLM 能够识别句间和句内上下文之间的差异，并进一步提高 LLM 在话语建模中的能力。我们使用两个基础模型评估了我们的方法，并展示了五个翻译方向的实验结果。实验结果和讨论表明，LLM
    在上下文感知 NMT 中的性能显著提升，表现为翻译准确性提高和话语相关问题减少。
- en: Limitations
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: Owing to resource limitations, our work is restricted to moderate-scale LLMs,
    specifically those with 7 billion parameters, and a confined window size of inter-sentence
    context. It is imperative to acknowledge that the results of our research may
    differ when employing larger models and an extended window size for inter-sentence
    context. We acknowledge these limitations and consider them as avenues for future
    exploration.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 由于资源限制，我们的工作局限于中等规模的 LLM，特别是那些拥有 70 亿参数的模型，并且句间上下文的窗口大小有限。必须承认，当使用更大的模型和扩展的句间上下文窗口时，我们研究的结果可能会有所不同。我们认识到这些限制，并将其视为未来探索的方向。
- en: References
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bao et al. (2023) Guangsheng Bao, Zhiyang Teng, and Yue Zhang. 2023. Target-side
    augmentation for document-level machine translation. In *Proceedings of ACL*,
    pages 10725–10742.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等人 (2023) Guangsheng Bao, Zhiyang Teng, 和 Yue Zhang. 2023. 面向文档级机器翻译的目标端增强。见
    *ACL 会议录*，页码10725–10742。
- en: Bao et al. (2021) Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, and
    Weihua Luo. 2021. G-transformer for document-level machine translation. In *Proceedings
    of ACL*, pages 3442–3455.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao 等人 (2021) Guangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing Chen, 和 Weihua
    Luo. 2021. 面向文档级机器翻译的 G-transformer。见 *ACL 会议录*，页码3442–3455。
- en: 'BigScience (2022) BigScience. 2022. Bloom: A 176b-parameter open-access multilingual
    language model. *Computing Research Repository*, arXiv:2211.05100.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BigScience (2022) BigScience. 2022. Bloom: 一种176b参数的开源多语言语言模型。*计算研究预印本*，arXiv:2211.05100。'
- en: 'Google (2022) Google. 2022. Palm: Scaling language modeling with pathways.
    *J. Mach. Learn. Res.*, 24:240:1–240:113.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google (2022) Google. 2022. Palm：通过路径扩展语言建模。 *J. Mach. Learn. Res.*，24:240:1–240:113。
- en: Graham et al. (2017) Yvette Graham, Qingsong Ma, Timothy Baldwin, Qun Liu, Carla
    Parra, and Carolina Scarton. 2017. Improving evaluation of document-level machine
    translation quality estimation. In *Proceedings of EACL*, pages 356–361.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Graham et al. (2017) Yvette Graham, Qingsong Ma, Timothy Baldwin, Qun Liu, Carla
    Parra, and Carolina Scarton. 2017. 改善文档级机器翻译质量评估的评估。发表于 *EACL 会议录*，第356–361页。
- en: 'Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation
    of large language models. In *Proceedings of ICLR*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA：大型语言模型的低秩适配。发表于 *ICLR
    会议录*。
- en: Hu and Wan (2023) Xinyu Hu and Xiaojun Wan. 2023. Exploring discourse structure
    in document-level machine translation. In *Proceedings of EMNLP*, pages 13889–13902.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu and Wan (2023) Xinyu Hu and Xiaojun Wan. 2023. 探索文档级机器翻译中的话语结构。发表于 *EMNLP
    会议录*，第13889–13902页。
- en: Jean et al. (2017) Sebastien Jean, Stanislas Lauly, Orhan Firat, and Kyunghyun
    Cho. 2017. Does neural machine translation benefit from larger context? *Computing
    Research Repository*, arXiv:1704.05135.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jean et al. (2017) Sebastien Jean, Stanislas Lauly, Orhan Firat, and Kyunghyun
    Cho. 2017. 神经机器翻译是否从更大的上下文中受益？ *计算研究库*，arXiv:1704.05135。
- en: 'Jiang et al. (2022) Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong
    Zhang, Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan,
    and Ming Zhou. 2022. BlonDe: An automatic evaluation metric for document-level
    machine translation. In *Proceedings of NAACL*, pages 1550–1565, Seattle, United
    States.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2022) Yuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang,
    Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan, and
    Ming Zhou. 2022. BlonDe：一种自动化的文档级机器翻译评估指标。发表于 *NAACL 会议录*，第1550–1565页，西雅图，美国。
- en: Kang et al. (2020) Xiaomian Kang, Yang Zhao, Jiajun Zhang, and Chengqing Zong.
    2020. Dynamic context selection for document-level neural machine translation
    via reinforcement learning. In *Proceedings of EMNLP*, pages 2242–2254.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kang et al. (2020) Xiaomian Kang, Yang Zhao, Jiajun Zhang, and Chengqing Zong.
    2020. 基于强化学习的文档级神经机器翻译动态上下文选择。发表于 *EMNLP 会议录*，第2242–2254页。
- en: Karpinska and Iyyer (2023) Marzena Karpinska and Mohit Iyyer. 2023. Large language
    models effectively leverage document-level context for literary translation, but
    critical errors persist. In *Proceedings of WMT*, pages 419–451.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpinska and Iyyer (2023) Marzena Karpinska and Mohit Iyyer. 2023. 大型语言模型有效利用文档级上下文进行文学翻译，但仍存在严重错误。发表于
    *WMT 会议录*，第419–451页。
- en: Koehn (2004) Philipp Koehn. 2004. Statistical significance tests for machine
    translation evaluation. In *Proceedings of EMNLP*, pages 388–395.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koehn (2004) Philipp Koehn. 2004. 机器翻译评估的统计显著性检验。发表于 *EMNLP 会议录*，第388–395页。
- en: Kuang et al. (2018) Shaohui Kuang, Junhui Li, António Branco, Weihua Luo, and
    Deyi Xiong. 2018. Attention focusing for neural machine translation by bridging
    source and target embeddings. In *Proceedings of ACL*, pages 1767–1776.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuang et al. (2018) Shaohui Kuang, Junhui Li, António Branco, Weihua Luo, and
    Deyi Xiong. 2018. 通过桥接源和目标嵌入实现神经机器翻译的注意力聚焦。发表于 *ACL 会议录*，第1767–1776页。
- en: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The
    power of scale for parameter-efficient prompt tuning. In *Proceedings of EMNLP*,
    pages 3045–3059.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. 参数高效提示调优的规模效应。发表于
    *EMNLP 会议录*，第3045–3059页。
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of ACL-IJCNLP*, pages 4582–4597,
    Online.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning：优化生成的连续提示。发表于
    *ACL-IJCNLP 会议录*，第4582–4597页，在线。
- en: 'Li et al. (2023) Yachao Li, Junhui Li, Jing Jiang, Shimin Tao, Hao Yang, and
    Min Zhang. 2023. P-Transformer: Towards Better Document-to-Document Neural Machine
    Translation. *IEEE/ACM Transactions on Audio, Speech, and Language Processing*,
    31:3859–3870.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) Yachao Li, Junhui Li, Jing Jiang, Shimin Tao, Hao Yang, and
    Min Zhang. 2023. P-Transformer：迈向更好的文档到文档神经机器翻译。 *IEEE/ACM 音频、语音与语言处理汇刊*，31:3859–3870。
- en: Li et al. (2024) Yachao Li, Junhui Li, Jing Jiang, and Min Zhang. 2024. Enhancing
    document-level translation of large language model via translation mixed-instructions.
    *Computing Research Repository*, arXiv:2401.08088.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2024) Yachao Li, Junhui Li, Jing Jiang, and Min Zhang. 2024. 通过翻译混合指令提升文档级翻译的能力。
    *计算研究库*，arXiv:2401.08088。
- en: 'Liu et al. (2022) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
    Zhilin Yang, and Jie Tang. 2022. P-tuning: Prompt tuning can be comparable to
    fine-tuning across scales and tasks. In *Proceedings of ACL*, pages 61–68.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
    Zhilin Yang, 和 Jie Tang. 2022. P-tuning：提示调整可以与细调相媲美，适用于不同规模和任务。见*ACL会议论文集*，第61–68页。
- en: Liu et al. (2021) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. *Computing Research Repository*,
    arXiv:2103.10385.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, 和 Jie Tang. 2021. GPT也懂。*计算研究库*，arXiv:2103.10385。
- en: Lyu et al. (2021) Xinglin Lyu, Junhui Li, Zhengxian Gong, and Min Zhang. 2021.
    Encouraging lexical translation consistency for document-level neural machine
    translation. In *Proceedings of EMNLP*, pages 3265–3277.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu et al. (2021) Xinglin Lyu, Junhui Li, Zhengxian Gong, 和 Min Zhang. 2021.
    鼓励文档级神经机器翻译的词汇翻译一致性。见*EMNLP会议论文集*，第3265–3277页。
- en: Lyu et al. (2022) Xinglin Lyu, Junhui Li, Shimin Tao, Hao Yang, and Min Zhang.
    2022. Modeling consistency preference via lexical chains for document-level neural
    machine translation. In *Proceedings of EMNLP*, pages 6312–6326.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu et al. (2022) Xinglin Lyu, Junhui Li, Shimin Tao, Hao Yang, 和 Min Zhang.
    2022. 通过词汇链建模一致性偏好以进行文档级神经机器翻译。见*EMNLP会议论文集*，第6312–6326页。
- en: Maruf et al. (2019) Sameen Maruf, André F. T. Martins, and Gholamreza Haffari.
    2019. Selective attention for context-aware neural machine translation. In *Proceedings
    of NAACL*, pages 3092–3102.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maruf et al. (2019) Sameen Maruf, André F. T. Martins, 和 Gholamreza Haffari.
    2019. 上下文感知神经机器翻译的选择性注意力。见*NAACL会议论文集*，第3092–3102页。
- en: 'MetaAI (2023a) MetaAI. 2023a. Llama 2: Open foundation and fine-tuned chat
    models. *Computing Research Repository*, arXiv:2307.09288.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MetaAI (2023a) MetaAI. 2023a. Llama 2：开放基础和微调聊天模型。*计算研究库*，arXiv:2307.09288。
- en: 'MetaAI (2023b) MetaAI. 2023b. Llama: Open and efficient foundation language
    models. *ArXiv*, abs/2302.13971.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MetaAI (2023b) MetaAI. 2023b. Llama：开放且高效的基础语言模型。*ArXiv*，abs/2302.13971。
- en: Miculicich et al. (2018) Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, and
    James Henderson. 2018. Document-level neural machine translation with hierarchical
    attention networks. In *Proceedings of EMNLP*, pages 2947–2954.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miculicich et al. (2018) Lesly Miculicich, Dhananjay Ram, Nikolaos Pappas, 和
    James Henderson. 2018. 具有层次注意力网络的文档级神经机器翻译。见*EMNLP会议论文集*，第2947–2954页。
- en: OpenAI (2023) OpenAI. 2023. Gpt-4 technical report. *Computing Research Repository*,
    arXiv:2303.08774.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. 2023. Gpt-4技术报告。*计算研究库*，arXiv:2303.08774。
- en: 'Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam
    Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible
    toolkit for sequence modeling. In *Proceedings of NAACL-HLT: Demonstrations*.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ott et al. (2019) Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross,
    Nathan Ng, David Grangier, 和 Michael Auli. 2019. fairseq：一个快速、可扩展的序列建模工具包。见*NAACL-HLT会议论文集：展示*。
- en: Post (2018) Matt Post. 2018. A call for clarity in reporting BLEU scores. In
    *Proceedings of WMT*, pages 186–191.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Post (2018) Matt Post. 2018. 呼吁在报告BLEU分数时保持清晰。见*WMT会议论文集*，第186–191页。
- en: 'Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter
    models. In *SC20: Proceedings of High Performance Computing, Networking, Storage
    and Analysis*, pages 1–16.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajbhandari et al. (2020) Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
    和 Yuxiong He. 2020. Zero：面向训练万亿参数模型的内存优化。见*SC20：高性能计算、网络、存储与分析会议论文集*，第1–16页。
- en: 'Rei et al. (2020) Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie.
    2020. COMET: A neural framework for MT evaluation. In *Proceedings of EMNLP*,
    pages 2685–2702.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rei et al. (2020) Ricardo Rei, Craig Stewart, Ana C Farinha, 和 Alon Lavie. 2020.
    COMET：一种用于机器翻译评估的神经框架。见*EMNLP会议论文集*，第2685–2702页。
- en: Sun et al. (2022) Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian
    Huang, Jiajun Chen, and Lei Li. 2022. Rethinking document-level neural machine
    translation. In *Findings of ACL*, pages 3537–3548.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2022) Zewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao, Shujian
    Huang, Jiajun Chen, 和 Lei Li. 2022. 重新思考文档级神经机器翻译。见*ACL发现*，第3537–3548页。
- en: 'Tan et al. (2022) Zhixing Tan, Xiangwen Zhang, Shuo Wang, and Yang Liu. 2022.
    MSP: Multi-stage prompting for making pre-trained language models better translators.
    In *Proceedings of ACL*, pages 6131–6142.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tan et al. (2022) Zhixing Tan, Xiangwen Zhang, Shuo Wang, 和 Yang Liu. 2022.
    MSP：多阶段提示以提升预训练语言模型的翻译能力。见*ACL会议论文集*，第6131–6142页。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. In *Proceedings of NIPS*, pages 5998–6008.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. 注意力机制就是你所需要的一切。发表于*NIPS会议论文集*，第5998–6008页。
- en: Voita et al. (2019a) Elena Voita, Rico Sennrich, and Ivan Titov. 2019a. Context-aware
    monolingual repair for neural machine translation. In *Proceedings of EMNLP-IJCNLP*,
    pages 877–886.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita et al. (2019a) Elena Voita, Rico Sennrich, and Ivan Titov. 2019a. 面向上下文的单语修复用于神经机器翻译。发表于*EMNLP-IJCNLP会议论文集*，第877–886页。
- en: 'Voita et al. (2019b) Elena Voita, Rico Sennrich, and Ivan Titov. 2019b. When
    a good translation is wrong in context: Context-aware machine translation improves
    on deixis, ellipsis, and lexical cohesion. In *Proceedings of ACL*, pages 1198–1212.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita et al. (2019b) Elena Voita, Rico Sennrich, and Ivan Titov. 2019b. 当良好的翻译在上下文中错误：面向上下文的机器翻译在指示词、省略和词汇衔接上的改进。发表于*ACL会议论文集*，第1198–1212页。
- en: Voita et al. (2018) Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov.
    2018. Context-aware neural machine translation learns anaphora resolution. In
    *Proceedings of ACL*, pages 1264–1274.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Voita et al. (2018) Elena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov.
    2018. 面向上下文的神经机器翻译学习共指解析。发表于*ACL会议论文集*，第1264–1274页。
- en: Wang et al. (2023) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian
    Yu, Shuming Shi, and Zhaopeng Tu. 2023. Document-level machine translation with
    large language models. In *Proceedings of EMNLP*, pages 16646–16661.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023) Longyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang, Dian
    Yu, Shuming Shi, and Zhaopeng Tu. 2023. 使用大型语言模型进行文档级机器翻译。发表于*EMNLP会议论文集*，第16646–16661页。
- en: 'Wang et al. (2019) Longyue Wang, Zhaopeng Tu, Xing Wang, and Shuming Shi. 2019.
    One model to learn both: Zero pronoun prediction and translation. In *Proceedings
    of EMNLP-IJCNLP*, pages 921–930.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019) Longyue Wang, Zhaopeng Tu, Xing Wang, and Shuming Shi. 2019.
    一种模型用于学习：零代词预测和翻译。发表于*EMNLP-IJCNLP会议论文集*，第921–930页。
- en: Wang et al. (2017) Longyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu. 2017. Exploiting
    cross-sentence context for neural machine translation. In *Proceedings of EMNLP*,
    pages 2826–2831.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2017) Longyue Wang, Zhaopeng Tu, Andy Way, and Qun Liu. 2017. 利用跨句上下文进行神经机器翻译。发表于*EMNLP会议论文集*，第2826–2831页。
- en: Wu et al. (2024) Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, and Gholamreza
    Haffari. 2024. Adapting large language models for document-level machine translation.
    *Computing Research Repository*, arXiv:2401.06468.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2024) Minghao Wu, Thuy-Trang Vu, Lizhen Qu, George Foster, and Gholamreza
    Haffari. 2024. 适应大型语言模型进行文档级机器翻译。*计算研究预印本*，arXiv:2401.06468。
- en: 'Wu and Hu (2023) Yangjian Wu and Gang Hu. 2023. Exploring prompt engineering
    with GPT language models for document-level machine translation: Insights and
    findings. In *Proceedings of the Eighth Conference on Machine Translation*, pages
    166–169.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu and Hu (2023) Yangjian Wu and Gang Hu. 2023. 探索GPT语言模型在文档级机器翻译中的提示工程：见解与发现。发表于*第八届机器翻译会议论文集*，第166–169页。
- en: Zhang et al. (2018) Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang
    Xu, Min Zhang, and Yang Liu. 2018. Improving the transformer translation model
    with document-level context. In *Proceedings of EMNLP*, pages 533–542.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018) Jiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei Zhai, Jingfang
    Xu, Min Zhang, and Yang Liu. 2018. 使用文档级上下文改进变换器翻译模型。发表于*EMNLP会议论文集*，第533–542页。
- en: Appendix A Datasets
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 数据集
- en: Statistics and Splitting of Datasets.
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集的统计与划分。
- en: 'We provide the detailed statistic in Table [7](#A1.T7 "Table 7 ‣ Statistics
    and Splitting of Datasets. ‣ Appendix A Datasets ‣ DeMPT: Decoding-enhanced Multi-phase
    Prompt Tuning for Making LLMs Be Better Context-aware Translators"). For all translation
    tasks, we randomly select 80% document pairs from the corpus as the training set.
    Both the test set and validation set include 150 document pairs each, randomly
    sampled from the remaining 20% of document pairs in the corpus. Regarding sentence
    preprocessing across all datasets, we segment the sentences with the tokenizer
    from the respective foundation model. No additional preprocessing steps are performed.
    Datasets are downloaded from [https://data.statmt.org/news-commentary/v18](https://data.statmt.org/news-commentary/v18).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[7](#A1.T7 "Table 7 ‣ Statistics and Splitting of Datasets. ‣ Appendix
    A Datasets ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs
    Be Better Context-aware Translators")中提供了详细的统计数据。对于所有翻译任务，我们从语料库中随机选择80%的文档对作为训练集。测试集和验证集各包含150个文档对，随机从剩余的20%文档对中抽取。关于所有数据集的句子预处理，我们使用各自基础模型的分词器对句子进行分割，不进行额外的预处理步骤。数据集从[https://data.statmt.org/news-commentary/v18](https://data.statmt.org/news-commentary/v18)下载。'
- en: '| Dataset | ZH$\rightarrow$EN |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | ZH$\rightarrow$EN |'
- en: '| #Doc | #Sent | #Doc | #Sent | #Doc | #Sent | #Doc | #Sent | #Doc | #Sent
    |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| #Doc | #Sent | #Doc | #Sent | #Doc | #Sent | #Doc | #Sent | #Doc | #Sent
    |'
- en: '| Training | 8,622 | 342,495 | 7,915 | 310,489 | 8,417 | 333,201 | 9,677 |
    378,281 | 7,255 | 272,100 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 训练 | 8,622 | 342,495 | 7,915 | 310,489 | 8,417 | 333,201 | 9,677 | 378,281
    | 7,255 | 272,100 |'
- en: '| Validation | 150 | 6,061 | 150 | 5,890 | 150 | 5,866 | 150 | 5,782 | 150
    | 5,691 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 验证 | 150 | 6,061 | 150 | 5,890 | 150 | 5,866 | 150 | 5,782 | 150 | 5,691
    |'
- en: '| Test | 150 | 5,747 | 150 | 5,795 | 150 | 5,967 | 150 | 5,819 | 150 | 5,619
    |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 测试 | 150 | 5,747 | 150 | 5,795 | 150 | 5,967 | 150 | 5,819 | 150 | 5,619
    |'
- en: 'Table 7: Statistics of training, validation, and test sets for five translation
    tasks. #Doc and #Sent denote the numbers of Document and Sentence, respectively.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表7：五个翻译任务的训练、验证和测试集的统计数据。#Doc和#Sent分别表示文档和句子的数量。
- en: '![Refer to caption](img/cb3e7753bbb2b9e048fa28670d594bc2.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cb3e7753bbb2b9e048fa28670d594bc2.png)'
- en: 'Figure 6: Scoring criterion for Direct Assessment. We group the score into
    five ranges, i.e., 0-20, 21-40, 41-60, 61-80, 81-100.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：直接评估的评分标准。我们将评分分为五个范围，即0-20，21-40，41-60，61-80，81-100。
- en: Appendix B Training Details
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 训练详情
- en: For all Transformer NMT models, we use the transformer-base setting as in Vaswani
    et al. ([2017](#bib.bib33)), where the learning rate is set to 1e-4\. The Transformer
    NMT models are trained on 4$\times$ NVIDIA A800 GPUs with Deespeed Zero 2 offload
    setting Rajbhandari et al. ([2020](#bib.bib29)).⁹⁹9[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有Transformer NMT模型，我们使用Vaswani等（[2017](#bib.bib33)）中的transformer-base设置，其中学习率设置为1e-4。Transformer
    NMT模型在4$\times$ NVIDIA A800 GPU上训练，使用Deespeed Zero 2卸载设置Rajbhandari等（[2020](#bib.bib29)）。⁹⁹9[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)
- en: '![Refer to caption](img/7e32b17208056040233d0df23f9e6856.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7e32b17208056040233d0df23f9e6856.png)'
- en: 'Figure 7: A case study for the CMT-PT model and our DeMPT model on ZH$\rightarrow$EN
    translation task.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：CMT-PT模型和我们DeMPT模型在ZH$\rightarrow$EN翻译任务中的案例研究。
- en: Appendix C Details of Human Evaluation
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 人工评估的详细信息
- en: Criterion and Recruitment.
  id: totrans-256
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 标准和招聘。
- en: 'Given a source sentence, its translation from MT (i.e., CMT-PT and our DeMPT),
    and its human-produced reference translation, the evaluators are asked to give
    a score ranging from 0 to 100\. Figure [6](#A1.F6 "Figure 6 ‣ Statistics and Splitting
    of Datasets. ‣ Appendix A Datasets ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators") presents the detailed
    criterion of scoring. We recruit evaluators from professional translators with
    at least five years of experience in translation.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '给定一个源句子、其机器翻译（即CMT-PT和我们的DeMPT）翻译和人工生成的参考翻译，评估人员被要求给出一个0到100的评分。图[6](#A1.F6
    "Figure 6 ‣ Statistics and Splitting of Datasets. ‣ Appendix A Datasets ‣ DeMPT:
    Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware
    Translators")展示了评分的详细标准。我们从具有至少五年翻译经验的专业翻译人员中招聘评估人员。'
- en: Statistics of Translation Errors.
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 翻译错误的统计数据。
- en: 'We manually count the number of bad cases from our DeMPT model. The bad cases
    fall into two categories: (1) the DA score is 60 or lower; (2) the DA score is
    lower than that of the translation from CMT-PT. The main types of the bad cases
    are Mistranslation (Mis.), Unnoticed Omission (UO), Inappropriate Expression (IE),
    and Grammatical Error (GE). We present detailed statistics in Table [8](#A3.T8
    "Table 8 ‣ Case Study. ‣ Appendix C Details of Human Evaluation ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators").
    The statistics indicate the bad cases mainly come from Mistranslation and Unnoticed
    Omission. Meanwhile, our DeMPT model outperforms the CMT-PT model in 86.5% DA
    cases.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '我们手动统计了DeMPT模型中的坏案例。坏案例分为两类：（1）DA评分为60或更低；（2）DA评分低于CMT-PT的翻译评分。坏案例的主要类型是误翻译（Mis.）、未察觉的遗漏（UO）、不当表达（IE）和语法错误（GE）。我们在表[8](#A3.T8
    "Table 8 ‣ Case Study. ‣ Appendix C Details of Human Evaluation ‣ DeMPT: Decoding-enhanced
    Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware Translators")中展示了详细统计数据。统计数据显示，坏案例主要来自误翻译和未察觉的遗漏。同时，我们的DeMPT模型在86.5%的DA案例中优于CMT-PT模型。'
- en: Case Study.
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 案例研究。
- en: 'We present a case in Figure [7](#A2.F7 "Figure 7 ‣ Appendix B Training Details
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators") to illustrate how our DeMPT model outperforms the
    CMT-PT model. In this case, we compare the translations of two consecutive sentences
    from our model and the CMT-PT model. First, we notice that the CMT-PT model translates
    the source word 美国  in the two sentences into US and America, respectively. However,
    our model consistently translates them into US. Second, our model uses for its
    part, a phase with more coherent preference, as the translation of 同时 , instead
    of At the same time adopted in the translation from the CMT-PT model. Both of
    them demonstrate the superiority of our proposed approach in discourse modeling.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图 [7](#A2.F7 "图 7 ‣ 附录 B 训练细节 ‣ DeMPT：增强解码的多阶段提示调优，使 LLM 更好地理解上下文") 中展示了一个案例，以说明我们的
    DeMPT 模型如何优于 CMT-PT 模型。在这个案例中，我们比较了来自我们模型和 CMT-PT 模型的两个连续句子的翻译。首先，我们注意到 CMT-PT
    模型将两个句子中的源词美国 翻译成 US 和 America。然而，我们的模型始终将其翻译为 US。其次，我们的模型在其翻译中使用了更具连贯性的阶段，比如将
    同时 翻译为，而不是 CMT-PT 模型中使用的 At the same time。这些都展示了我们提出的方法在话语建模中的优越性。
- en: '| Group | Type of Bad Case |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 组别 | 坏案例类型 |'
- en: '| Mis. | UO | IE | GE | Total (Perc.) |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 错误 | UO | IE | GE | 总计（百分比） |'
- en: '| 1 | 6 | 3 | 1 | 2 | 12 (6.0%) |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 6 | 3 | 1 | 2 | 12 (6.0%) |'
- en: '| 2 | 9 | 7 | 6 | 5 | 27 (13.5%) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 9 | 7 | 6 | 5 | 27 (13.5%) |'
- en: 'Table 8: Statistics of bad cases from our DeMPT model on ZH$\rightarrow$EN
    translation task. Perc. denotes the percentage of bad cases against the total
    of DA cases.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 8：我们 DeMPT 模型在 ZH$\rightarrow$EN 翻译任务中的坏案例统计。Perc. 表示坏案例占 DA 案例总数的百分比。
- en: Appendix D Effect of Transfer Layer and Type Embedding
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 转移层和类型嵌入的效果
- en: 'As in Eq. [22](#S2.E22 "In 2.3 Phase-aware Prompts ‣ 2 Methodology ‣ DeMPT:
    Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better Context-aware
    Translators") within Section [2.3](#S2.SS3 "2.3 Phase-aware Prompts ‣ 2 Methodology
    ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making LLMs Be Better
    Context-aware Translators"), we introduce two sublayers: a non-linear transfer
    sublayer and a type embedding sublayer for the trainable prompt in each phase.
    This design enhances the awareness of LLMs regarding the distinctions in inputs
    across the three tuning phases, allowing them to adapt to specific roles at each
    phase. We investigate the effect of these two sublayers.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程 [22](#S2.E22 "在 2.3 阶段感知提示 ‣ 2 方法论 ‣ DeMPT：增强解码的多阶段提示调优，使 LLM 更好地理解上下文")
    中的第 [2.3](#S2.SS3 "2.3 阶段感知提示 ‣ 2 方法论 ‣ DeMPT：增强解码的多阶段提示调优，使 LLM 更好地理解上下文") 节，我们引入了两个子层：一个非线性转移子层和一个类型嵌入子层，用于每个阶段的可训练提示。这个设计增强了
    LLM 对三个调优阶段输入之间差异的感知，使其能够在每个阶段适应特定角色。我们调查了这两个子层的效果。
- en: 'As shown in Table [9](#A4.T9 "Table 9 ‣ Appendix D Effect of Transfer Layer
    and Type Embedding ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for Making
    LLMs Be Better Context-aware Translators"), our observations reveal that the transfer
    sublayer holds greater importance than the type embedding sublayer. Removing either
    the non-linear transfer sublayer (w/o Transfer.) or the type embedding sublayer
    (w/o Embed.) results in a performance drop of 0.84/0.0048/0.39 or 0.45/0.0036/0.007
    in BLEU/COMET/BlonDe metrics.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如表格 [9](#A4.T9 "表格 9 ‣ 附录 D 转移层和类型嵌入的效果 ‣ DeMPT：增强解码的多阶段提示调优，使 LLM 更好地理解上下文")
    所示，我们的观察揭示了转移子层的重要性大于类型嵌入子层。去掉非线性转移子层（w/o Transfer.）或类型嵌入子层（w/o Embed.）会导致 BLEU/COMET/BlonDe
    指标的性能下降分别为 0.84/0.0048/0.39 或 0.45/0.0036/0.007。
- en: '| Model | BLEU | COMET | BlonDe |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | BLEU | COMET | BlonDe |'
- en: '| MT-PT | 30.99 | 0.8520 | 49.48 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| MT-PT | 30.99 | 0.8520 | 49.48 |'
- en: '| CMT-PT | 30.82 | 0.8504 | 49.61 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| CMT-PT | 30.82 | 0.8504 | 49.61 |'
- en: '| \hdashlineDeMPT | 32.46 | 0.8649 | 50.62 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineDeMPT | 32.46 | 0.8649 | 50.62 |'
- en: '| w/o Transfer. | 31.62 | 0.8601 | 50.23 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 去除转移 | 31.62 | 0.8601 | 50.23 |'
- en: '| w/o Embed. | 32.01 | 0.8613 | 50.55 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 去除嵌入 | 32.01 | 0.8613 | 50.55 |'
- en: '| w/o CTX. | 31.98 | 0.8593 | 49.89 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 去除上下文 | 31.98 | 0.8593 | 49.89 |'
- en: 'Table 9: Comparison of performances of the DeMPT variants on ZH$\rightarrow$EN
    test set. w/o Trans. or w/o Embed. denotes the variant without the non-linear
    transfer sublayer or type embedding sublayer in Eq. [22](#S2.E22 "In 2.3 Phase-aware
    Prompts ‣ 2 Methodology ‣ DeMPT: Decoding-enhanced Multi-phase Prompt Tuning for
    Making LLMs Be Better Context-aware Translators"). w/o CTX. means the inter-sentence
    context is not available, i.e., context-agnostic DeMPT system.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：DeMPT 变体在 ZH$\rightarrow$EN 测试集上的性能比较。w/o Trans. 或 w/o Embed. 表示在 Eq. [22](#S2.E22
    "在 2.3 阶段感知提示 ‣ 2 方法论 ‣ DeMPT：解码增强的多阶段提示调整，使 LLMs 更好地感知上下文的翻译器") 中没有非线性转换子层或类型嵌入子层的变体。w/o
    CTX. 表示句子间上下文不可用，即上下文无关 DeMPT 系统。
- en: Appendix E Effect of Inter-sentence Context
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 句子间上下文的效果
- en: We implement the context-agnostic (sentence-level) DeMPT system to analyze the
    effect of the inter-sentence context. More specifically, we replace the input
    of LLMs in the inter-sentence context encoding phase with the intra-sentence context.
    In other words, we encode the intra-sentence context twice to keep the multi-phase
    tuning strategy in DeMPT while making the inter-sentence context unavailable.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了上下文无关（句子级）DeMPT 系统来分析句子间上下文的影响。更具体地说，我们在句子间上下文编码阶段用句子内上下文替换了 LLMs 的输入。换句话说，我们对句子内上下文进行两次编码，以保持
    DeMPT 的多阶段调整策略，同时使句子间上下文不可用。
- en: 'As shown in the last row of Table [9](#A4.T9 "Table 9 ‣ Appendix D Effect of
    Transfer Layer and Type Embedding ‣ DeMPT: Decoding-enhanced Multi-phase Prompt
    Tuning for Making LLMs Be Better Context-aware Translators") (i.e., w/o CTX),
    we find that the inter-sentence context is crucial for the alleviation of discourse-related
    issues. The BlonDe score drops by 0.73 when the inter-sentence context is unavailable.
    Meanwhile, our DeMPT also significantly improves the performance of LLMs in context-agnostic
    MT, e.g., + 0.99 BLEU score and + 0.0073 COMET score compared to the MT-PT model.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如表 [9](#A4.T9 "表 9 ‣ 附录 D 转换层和类型嵌入的效果 ‣ DeMPT：解码增强的多阶段提示调整，使 LLMs 更好地感知上下文的翻译器")
    最后一行所示（即 w/o CTX），我们发现句子间上下文对于缓解话语相关问题至关重要。当句子间上下文不可用时，BlonDe 分数下降了 0.73。同时，我们的
    DeMPT 也显著提升了 LLMs 在上下文无关机器翻译中的表现，例如，相比于 MT-PT 模型，BLEU 分数提升了 0.99 和 COMET 分数提升了
    0.0073。
