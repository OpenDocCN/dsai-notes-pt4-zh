- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:42:19'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.16899](https://ar5iv.labs.arxiv.org/html/2406.16899)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Yuni Susanti [0009-0001-1314-0286](https://orcid.org/0009-0001-1314-0286 "ORCID
    identifier") Artificial Intelligence Lab., Fujitsu LimitedTokyoJAPAN [susanti.yuni@fujitsu.com](mailto:susanti.yuni@fujitsu.com)
     and  Nina Holsmoelle [n.holsmoelle@gmail.com](mailto:n.holsmoelle@gmail.com)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This work aims toward an application of natural language processing (NLP) technology
    for automatic verification of causal graphs using text sources. A causal graph
    is often derived from unsupervised causal discovery methods and requires manual
    evaluation from human experts. NLP technologies, i.e., Large Language Models (LLMs)
    such as BERT (Devlin et al., [2018](#bib.bib7)) and ChatGPT, can potentially be
    used to verify the resulted causal graphs by predicting if causal relation can
    be observed between node pairs based on the textual context. In this work, we
    compare the performance of two types of NLP models: (1) Pre-trained language models
    fine-tuned for causal relation classification task (supervised) and, (2) prompt-based
    LLMs (unsupervised). Contrasted to previous studies where prompt-based LLMs work
    relatively well over a set of diverse tasks (Wei et al., [2023](#bib.bib26); Jeblick
    et al., [2022](#bib.bib13); Agrawal et al., [2022](#bib.bib2)), preliminary experiments
    on biomedical and open-domain datasets suggest that the fine-tuned models far
    outperform the prompt-based LLMs, up to 20.5 points improvement of F1 score. We
    shared the code and the pre-processed datasets.¹¹1https://github.com/littleflow3r/causal-llm'
  prefs: []
  type: TYPE_NORMAL
- en: 'Causal Graph, Causal Relation, Large Language Models^†^†copyright: none^†^†conference:
    ; ;'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the fundamental tasks in various disciplines of science is to find the
    underlying causal relations and make use of them. Causal discovery methods (Spirtes
    et al., [2000](#bib.bib23); Shimizu et al., [2006](#bib.bib22)) are able to estimate
    the causal structures from observational data and further generate causal graphs.
    A causal graph, as illustrated in Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction
    ‣ Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification"), is a directed
    graph visualizing causal relationships between the observed variables; a node
    represents a variable and an edge represents a causal relationship. As with the
    progress in the field of causal discovery, we are faced with the challenge of
    how to verify the causal graph estimated with causal discovery methods, which
    are often unsupervised. Essentially, experts are required to manually verify the
    validity of the causal graph, such as by conducting a controlled experiment. However,
    depending on the field, conducting such experiments is often expensive, or even
    infeasible due to the ethical concerns.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b63e47e3635a1fe2c6d129599146426e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Example of a causal graph.
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: '[Example of a causal graph.]Example of a causal graph.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another method to verify a causal graph is using external knowledge such as
    text sources. Information on causality is largely dispersed in text sources, and
    they are indispensable to assist human experts in verifying the validity of causal
    graphs. However, verifying a causal graph with a large number of variables becomes
    difficult due to the rapid growth of text sources. Natural Language Processing
    (NLP) technologies, i.e., Large Language Models (LLMs) such as BERT (Devlin et al.,
    [2018](#bib.bib7)) or ChatGPT, can potentially be used to verify the causal graphs
    by predicting if a causal relation exists between the node pairs based on the
    textual context. In this work, we discuss the feasibility of applying NLP technologies
    for causal graph verification through quantitative evaluation experiments on causal
    text datasets. We investigated two types of NLP models: (1) Pre-trained language
    models fine-tuned for causal relation classification task (supervised) and, (2)
    prompt-based LLMs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, the main contributions of this work are:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To our knowledge, this is the first work to study the feasibility of applying
    NLP for causal graph verification, with quantitative evaluation experiments on
    causal text datasets. Specifically, we introduce prompt-based LLMs to predict
    the causal relationship between pair of entities, comparing it to supervised causal
    relation classification model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We demonstrate that prompt-based LLMs do not necessarily perform better than
    supervised model on a causal relation classification task, despite its relatively
    satisfactory performance on clinical NLP over a set of diverse tasks (Agrawal
    et al., [2022](#bib.bib2)). We also provide a discussion of why this might be
    the case, and also the limitations of this work.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2\. Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The research on causal relation extraction/classification from text sources
    has been done mostly in supervised setting, especially in biomedical-chemistry
    domains (Khoo et al., [2000](#bib.bib15); Bui et al., [2010](#bib.bib5); Mihăilă
    and Ananiadou, [2014](#bib.bib19); Gu et al., [2016](#bib.bib9); Reklos and Albert,
    [2022](#bib.bib21); Khetan et al., [2022](#bib.bib14)), and open-domain (Khoo
    et al., [1998](#bib.bib16); Chang and Choi, [2006](#bib.bib6); Blanco et al.,
    [2008](#bib.bib4); Balashankar et al., [2019](#bib.bib3)). The pre-training and
    fine-tuning paradigm in NLP led to state-of-the-art performance in many downstream
    tasks; likewise, most of the related works listed above fine-tune the pre-trained
    language models such as BERT (Devlin et al., [2019](#bib.bib8)), or propose some
    sort of enhancement for BERT such as the work by Su and Vijay-Shanker ([2022](#bib.bib24)).
    Their results on relation extraction on biomedical datasets has been encouraging,
    motivating us to choose the BERT models as the main baselines for our fine-tuning
    experiments.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, recent work by Agrawal et al. ([2022](#bib.bib2)) shows that
    Large Language Models (LLMs) perform well at zero and few-shot information extraction
    from clinical text, despite not being trained specifically for the clinical domain.
    Similarly, other works (Wei et al., [2023](#bib.bib26); Jeblick et al., [2022](#bib.bib13))
    suggest that LLMs (i.e., InstructGPT (Ouyang et al., [2022](#bib.bib20)), ChatGPT,
    GPT-3.5, etc.), perform well in various downstream tasks even without tuning the
    parameters, but only with few examples as instructions/prompts. This inspires
    us to evaluate such instruction, or prompt-based LLMs, on our causal relation
    classification task. In this work, we compare the prompt-based LLMs against the
    more traditional supervised model where it is trained/fine-tuned using the training
    data for causal relation classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Given a pair of entities $e_{1}$, i.e., variables/node pairs of the causal graph
    such as smoking and lung cancer, the task is to predict if a causal relation can
    be observed between the pair based on its textual context. Therefore, it is essentially
    a binary classification task, classifying the relation as causal or non-causal.
    A causal graph from the causal discovery method does not normally include any
    textual context. In this work, we used the data commonly used for evaluating NLP
    models for a causal relation classification task, which already includes textual
    contexts for each pairs.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Prompt-based LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In prompt-based learning, a pre-trained language model is adapted to a specific
    task via priming on natural language prompts—pieces of text that are combined
    with an input and then fed to the language model to produce an output for that
    task (Agrawal et al., [2022](#bib.bib2)). Prompt-based learning requires the specification
    of a prompt template to be applied to the input, thus we designed two settings
    for the prompt-based LLMs experiments: (1) Single-Prompt and (2) Few-Shot Prompt
    settings, as described in the following.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. Single-Prompt.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the Single-Prompt setting, we designed the prompt to directly ask the LLMs
    to answer a question about causality between a pair of entities, without providing
    any example of the training data in the prompt (i.e., zero-shot approach). For
    the pair $e_{1}$, we hand-crafted the following three prompt variations.
  prefs: []
  type: TYPE_NORMAL
- en: (A)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'two-choices, no-context: There is a causal relationship between $e_{1}$. Answer
    with ’True.’ or ’False.’'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (B)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'two-choices, with-context: Given the following context, classify the relationship
    between $e_{1}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (C)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'three-choices, with-context: Given the cancer research-related context below,
    is there a causal relationship between $e_{1}$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The LLMs are strictly forced to respond with two choices, as in variation (A)
    and (B), which is necessary for a fair comparison with the fine-tuned model. In
    variation (C), however, we allow the LLMs to return a ‘Maybe’ because occasionally
    the LLMs implies that there is not enough evidence to decide the causality. We
    skip the ‘Maybe’ response in calculating the accuracy to get the truest accuracy
    score. We also varied the prompt by including and not including the textual context
    sentence $S$ in the prompt (with-context and no-context).
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Few-Shot Prompt.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the Few-Shot prompt setting, we designed a specific format that includes
    $n$ as the context, practically giving the LLMs guidance on how to extract and
    classify the unseen pair from the test data. Furthermore, we also add a specific
    instruction at the beginning of the prompt, as illustrated in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example: Illustration of the Few-Shot Prompt setting. In this example, $n$=1
    training examples is included in the prompt, with the red-marked example acts
    as the test data. The beginning of the prompt provides the instruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PROMPT:Given  the  context  sentence,  classify  the  relationship  between  the  entities  marked  with  e1  and  e2  as  ’causal’  or  ’non-causal’##  Context
    Sentence:  Increased  expression  of    osteopontin    contributes  to  the  progression  of    prostate  cancer  .Result:  ’e1’:  ’osteopontin’,  ’relation’:  ’causal’,  ’e2’:  ’prostate  cancer’##  Context  Sentence:  Increased  expression  of    cyclin  B1    sensitizes    prostate  cancer    cells  to  apoptosis  induced  by  chemotherapy.Result:EXPECTED  OUTPUT:’e1’:  ’cyclin  B1’,  ’relation’:  ’causal’,  ’e2’:  ’prostate  cancer’'
  prefs: []
  type: TYPE_NORMAL
- en: By inserting “¡e1¿” and “¡e2¿” to mark the location of the pair, the model technically
    only has to binary-classify the relationship between the pair. We conducted the
    Few-Shot Prompt experiment by varying the number of the training data n to be
    included in the prompt as examples. In this work, we used the official OpenAI
    API with gpt-3.5-turbo and text-davinci-003 engines.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Fine-tuned LLMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The pre-training of LLMs usually utilizes a great quantity of unlabeled data,
    and the fine-tuning involves training these pre-trained LLMs on a smaller dataset
    labeled with examples relevant to the target task. By exposing the model to these
    new labeled examples, the model adjusts its parameters and internal representations
    suited for the target task. In this work, we experimented with two models: (1)
    BERT (Devlin et al., [2018](#bib.bib7)) and (2) GPT models, for the fine-tuning
    experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. Fine-tuning BERT model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Figure [2](#S3.F2 "Figure 2 ‣ 3.2.1\. Fine-tuning BERT model ‣ 3.2\. Fine-tuned
    LLMs ‣ 3\. Methods ‣ Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification")
    shows our model architecture of fine-tuning BERT pre-trained model for the causal
    relation classification task. We opt for a simple fine-tuning architecture for
    a fair comparison with the prompt-based LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: For an input sequence $S$, as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $H^{\prime}_{cls}=W_{0}(tanh(H_{cls}))+b_{0}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'We used the binary cross entropy as the loss function during the training.
    In this work, we experimented with two BERT models adapted for the biomedical
    domain: BioBERT (Lee et al., [2019](#bib.bib18)), and PubMedBERT (Gu et al., [2021](#bib.bib10)).
    BERT (base, uncased)²²2https://huggingface.co/bert-base-uncased is used for experiment
    on open-domain data. The detailed hyper-parameters and other experiment details
    are provided in our Github.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6766ba19a8e0c68dc10ebd2cfb67e63.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Model architecture of the fine-tuning BERT model for causal relation
    classification task.
  prefs: []
  type: TYPE_NORMAL
- en: \Description
  prefs: []
  type: TYPE_NORMAL
- en: '[Model architecture]Model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Fine-tuning GPT model
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Fine-tuning the GPT model includes formatting each training example into prompt-completion
    pair, consisting of a single input example (prompt) and its associated output
    (completion). The format of the training example is different depending on the
    use case. Our task is essentially a relation classification task; however, it
    can also be formulated as a relation extraction task between pair of entities.
    We followed the GPT fine-tuning instruction and formatted the examples into classification
    and extraction task formats, as shown in the following.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fine-tuning GPT: classification/extraction example format.'
  prefs: []
  type: TYPE_NORMAL
- en: '{"prompt":  "The  results  provide  evidence  for  altered  plasticity  of  synaptic  morphology  in  memory  mutants  dnc  and  rut  and  suggest  a  role...\n\n###\n\n","completion":  "  non-causal@"}{"prompt":  "The  results  provide  evidence  for  altered  plasticity  of  synaptic  morphology  in  memory  mutants  dnc  and  rut  and  suggest  a  role...\n\n###\n\n","completion":"  dnc\n  rut\n  non-causal  END@"}'
  prefs: []
  type: TYPE_NORMAL
- en: GPT model fine-tuning is currently available only for some of the base models;
    in this work, we experimented with the $ada$ model for its fast training and relatively
    good performance.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1\. Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Causality is often observed in the biomedical domain, thus we conducted the
    experiments mainly in biomedical datasets, and one open-domain dataset (SEMEVAL).
    We created a new data where two experts in biomedical research annotates the causal
    relation between genes in homo sapiens (GENE). We also modified some relation
    extraction benchmark data (DDI, COMAGC) to only include causal relation. The datasets
    are summarized in Table [1](#S4.T1 "Table 1 ‣ 4.1\. Datasets ‣ 4\. Experiments
    ‣ Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification").
  prefs: []
  type: TYPE_NORMAL
- en: '| dataset | type | instances# |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DDI (Herrero-Zazo et al., [2013](#bib.bib12)) | Biomedical/drug-drug | 33,508
    |'
  prefs: []
  type: TYPE_TB
- en: '| COMAGC (Lee et al., [2013](#bib.bib17)) | Biomedical/gene-disease | 821 |'
  prefs: []
  type: TYPE_TB
- en: '| GENE (ours) | Biomedical/gene-gene | 789 |'
  prefs: []
  type: TYPE_TB
- en: '| SEMEVAL (Hendrickx et al., [2010](#bib.bib11)) | Open-domain | 10,717 |'
  prefs: []
  type: TYPE_TB
- en: Table 1\. Dataset types and sizes.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Experiment results. Values in bold indicates the best F1 score for
    each method and dataset. $type$ refers to the number of training data included
    in the prompt for Few-Shot setting. Values in parenthesis represent standard deviations
    of F1 scores over the 5 cross-validation test folds.
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | (Biomed) COMAGC | (Biomed) DDI | (Biomed) GENE | (News) SEMEVAL |'
  prefs: []
  type: TYPE_TB
- en: '| Prompt-based | $type$ | P | R | F1 | P | R | F1 | P | R | F1 | P | R | F1
    |'
  prefs: []
  type: TYPE_TB
- en: '| Single-Prompt | A | 28.2 | 61.0 | 38.1 (.06) | 52.2 | 25.7 | 34.3 (.02) |
    23.6 | 26.6 | 24.2 (.07) | 64.6 | 66.0 | 65.3 (.06) |'
  prefs: []
  type: TYPE_TB
- en: '| Single-Prompt | B | 28.2 | 94.2 | 43.2 (.05) | 65.1 | 69.0 | 66.7 (.04) |
    34.3 | 59.6 | 42.3 (0.1) | 77.7 | 84.7 | 80.8 (.04) |'
  prefs: []
  type: TYPE_TB
- en: '| Single-Prompt | C | 48.8 | 100 | 64.2 (.14) | 52.9 | 93.2 | 67.4 (.02) |
    27.4 | 71.9 | 39.5 (.05) | 57.4 | 82.8 | 67.7 (.03) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $n$ |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot Prompt | 5 | 37.2 | 83.5 | 51.0 (.03) | 100 | 37.6 | 53.1 (.15)
    | 22.1 | 25.7 | 22.7 (.28) | 100 | 46.0 | 62.7 (.06) |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot Prompt | 15 | 52.8 | 41.4 | 46.1 (.08) | 51.4 | 27.0 | 35.1 (.05)
    | 26.0 | 29.1 | 26.2 (.18) | 100 | 47.9 | 64.6 (.04) |'
  prefs: []
  type: TYPE_TB
- en: '| Few-Shot Prompt | 20 | 50.2 | 70.4 | 57.0 (.08) | * | * | * | 31.7 | 39.5
    | 34.3 (.08) | 58.9 | 57.7 | 58.2 (.02) |'
  prefs: []
  type: TYPE_TB
- en: '| Fine-tuning |  |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| BioBERT |  | 77.9 | 84.4 | 80.8 (.01) | 97.0 | 76.2 | 85.2 (.03) | 46.1 |
    65.2 | 53.5 (.07*) | * | * | * |'
  prefs: []
  type: TYPE_TB
- en: '| PubmedBERT |  | 80.7 | 87.4 | 83.9 (.03) | 93.2 | 83.3 | 87.9 (.01) | 50.6
    | 62.1 | 55.1 (.03) | * | * | * |'
  prefs: []
  type: TYPE_TB
- en: '| BERT-large |  | * | * | * | * | * | * | * | * | * | 93.0 | 93.0 | 93.0 (.01)
    |'
  prefs: []
  type: TYPE_TB
- en: '| GPT/ada (classify) |  | 80.5 | 70.1 | 74.1 (.06) | 99.4 | 78.1 | 87.4 (.03)
    | 58.6 | 23.1 | 31.4 (.08) | 99.9 | 94.8 | 96.8 (.03) |'
  prefs: []
  type: TYPE_TB
- en: '| GPT/ada (extraction) |  | 75.6 | 58.1 | 65.5 (.07) | 100 | 62.9 | 77.1 (.02)
    | 52.4 | 21.2 | 30.1 (.06) | 100 | 91.9 | 95.7 (.03) |'
  prefs: []
  type: TYPE_TB
- en: 4.2\. Results and Discussion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [2](#S4.T2 "Table 2 ‣ 4.1\. Datasets ‣ 4\. Experiments ‣ Prompt-based
    vs. Fine-tuned LLMs Toward Causal Graph Verification") summarizes the experiment
    results. Precision (P), Recall (R), and F1-score (F1) metrics are employed, and
    as with the general practice in binary classification task, the metrics are calculated
    on the non-negative class. We apply 5-fold cross-validation and the scores are
    averaged. We report the standard deviation values of the F1 scores over the 5-folds
    as shown in parenthesis in Table [2](#S4.T2 "Table 2 ‣ 4.1\. Datasets ‣ 4\. Experiments
    ‣ Prompt-based vs. Fine-tuned LLMs Toward Causal Graph Verification").
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, the results suggest that the fine-tuned models far outperform the
    prompt-based LLMs in predicting the causal relationship between the entity pairs
    in all of the datasets used in this work. This contrasted with the previous studies (Wei
    et al., [2023](#bib.bib26); Jeblick et al., [2022](#bib.bib13)) where LLMs perform
    relatively well, if not better than the fine-tuned models in various tasks including
    in clinical NLP tasks (Agrawal et al., [2022](#bib.bib2)). We conducted error
    analysis and outlined the following discussion.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.1\. Discussion 1: Explicit-implicit mention of causality, and the effect
    of training sample.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on our error analysis, one possible reason the prompt-based model does
    not perform as well as the fine-tuned model is because causality is rarely written
    explicitly using causal cues such as “cause”, “causing” or “caused”. Instead,
    it is often described rather implicitly/ambiguously with keywords such as “contribute”
    or “play a role” Consider the following example from the test data, where the
    fine-tuned model correctly predicted the causal relation while the prompt-based
    model failed:'
  prefs: []
  type: TYPE_NORMAL
- en: Example 4.1.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: ..hepatocyte growth factor contribute to the growth of ovarian cancer by activating
    autocrine…
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Example [4.1](#S4.Thmtheorem1 "Example 4.1\. ‣ 4.2.1\. Discussion 1:
    Explicit-implicit mention of causality, and the effect of training sample. ‣ 4.2\.
    Results and Discussion ‣ 4\. Experiments ‣ Prompt-based vs. Fine-tuned LLMs Toward
    Causal Graph Verification") above, human experts are aware that the keyword “contribute
    to the growth” implicitly describes causality, thus annotated the pair as a causal
    pair. The fine-tuned model most likely has been exposed to many similar examples
    in the training data and learned that such keywords can be an identifier of a
    causal relation. Meanwhile, without being exposed to any training sample, the
    prompt-based model missed this pattern indicating causality, resulted in wrong
    prediction. This suggests that, finding the causality pattern from training samples
    is an important step in identifying the causal relation.'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, in the Few-Shot prompt experiments where $n$ number of training
    samples are included in the prompt, the performance is not necessarily better
    compared to the models without training samples. This is shown in the Table [2](#S4.T2
    "Table 2 ‣ 4.1\. Datasets ‣ 4\. Experiments ‣ Prompt-based vs. Fine-tuned LLMs
    Toward Causal Graph Verification"), where surprisingly, the highest F score for
    the prompt-based methods is achieved with Single-Prompt B and C; both are the
    model without any training samples. We hypothesized this could be due to the limited
    size of the training samples, and adding more training data might improve the
    performance. Unfortunately, the current token limitation of the API prevented
    us to experiment with larger numbers of n.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.2\. Discussion 2: Effect of the context sentence.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned in section [3.1.1](#S3.SS1.SSS1 "3.1.1\. Single-Prompt. ‣ 3.1\.
    Prompt-based LLMs ‣ 3\. Methods ‣ Prompt-based vs. Fine-tuned LLMs Toward Causal
    Graph Verification"), we created variations of the Single-Prompt setting by including
    and not including the context sentence $S$ in the prompt (with-context and no-context).
    Context sentence provides the model with a context, which in this work we define
    as the text surrounding the entity pair. We expect the model to look at these
    surrounding texts in determining the relationship between the target entity pair.
  prefs: []
  type: TYPE_NORMAL
- en: The result suggests that for prompt-based methods, including the context sentence
    in the prompt proves to be effective, as shown in the Table [2](#S4.T2 "Table
    2 ‣ 4.1\. Datasets ‣ 4\. Experiments ‣ Prompt-based vs. Fine-tuned LLMs Toward
    Causal Graph Verification") where the Single-Prompt type B, C (with-context) consistently
    gives better scores than type A (no-context). Rather than solely relying on the
    knowledge of the model obtained during the pre-training, the context gives the
    model additional knowledge in predicting the relationship between the entity pair.
  prefs: []
  type: TYPE_NORMAL
- en: '4.2.3\. Discussion 3: Biomedical vs. Open-domain datasets.'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As shown in Table [2](#S4.T2 "Table 2 ‣ 4.1\. Datasets ‣ 4\. Experiments ‣ Prompt-based
    vs. Fine-tuned LLMs Toward Causal Graph Verification"), in general, we observe
    better scores in the results of the open-domain dataset compared to the biomedical
    datasets. This is expected because LLMs are mostly trained on open-domain texts,
    i.e., books and online content. Another reason could be due to the complexity
    of the texts in biomedical datasets, which include more domain-specific or technical
    terms.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We present a preliminary study discussing the feasibility of applying NLP technologies
    for causal graph verification. We compare the prompt-based and fine-tuned LLMs
    for predicting the causality between pair of entities based on their textual context.
    Experiments on biomedical and open-domain datasets suggest that the fine-tuned
    models outperforms the prompt-based LLMs, up to 20.5% F score. However, due to
    limitation of the data, we were only able to quantitatively evaluate the methods
    on a binary-task setting, without considering the direction of causality. The
    target causal graph is directed, thus evaluating the direction of causality is
    our main future work. Other future work includes optimizing the prompt-based LLMs
    with more advanced prompting methods e.g., chain-of-thought (Wei et al., [2022](#bib.bib25)),
    or by giving the model additional knowledge and a more precise instruction.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the result of the current experiment, a fine-tuned model needs a sufficient
    expert-annotated data for training, and this could hinder the progress of the
    research. Training data construction (i.e., data annotation) requires human expert
    knowledge, and is often difficult and costly. In this regard, we believe that
    the LLMs, especially through the prompt engineering method, could be a breakthrough
    for the causal relation classification/extraction research.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Prompt-based LLMs Hyperparameter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we used OpenAI³³3https://platform.openai.com/ API with gpt-3.5-turbo
    engine for the Single-Prompt and text-davinci-003 engine for the Few-Shot experiments.
    For all prompt- based settings, in general we assume only query access to the
    LLMs (i.e., no gradients, no log probabilities). Table [3](#A1.T3 "Table 3 ‣ Appendix
    A Prompt-based LLMs Hyperparameter ‣ Prompt-based vs. Fine-tuned LLMs Toward Causal
    Graph Verification") summarizes the hyperparameter values for the Few-Shot setting
    experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. LLMs hyperparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $temperature$ | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| $max\_token(n=5)$1200 |'
  prefs: []
  type: TYPE_TB
- en: '| $max\_token(n=15,20)$250 |'
  prefs: []
  type: TYPE_TB
- en: '| $top\_p$ | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| $frequency\_penalty$ | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: '| $presence\_penalty$ | 0.7 |'
  prefs: []
  type: TYPE_TB
- en: In the fine-tuning of the BERT-based causal relation extraction models, batch
    sizes of 16 were used for the DDI dataset, while batch sizes of 8 were used for
    the COMAGC dataset. At both the beginning and the end of the first and second
    entity, we insert a special token “$” and “#”, respectively. This is useful to
    make the model capture the location information of the two target entities. We
    also add special tokens [CLS] to the beginning and [SEP] to the end of each input
    sequence, following the standard fine-tuning practice using the pre-trained BERT
    model (Devlin et al., [2019](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B BERT fine-tuning hyperparamater
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In preparing the data, we insert a special token “$” and “#” at both the beginning
    and the end of the first and second entity, respectively. This is useful to make
    the model captures the location information of the target entities. We also add
    special tokens “[CLS]” to the beginning and “[SEP]” to the end of each input sequence,
    following the standard fine-tuning practice using the pre-trained BERT model (Devlin
    et al., [2019](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: In the fine-tuning of the BERT pre-trained model for the biomedical dataset,
    batch sizes of 16 were used for the DDI, SEMEVAL, and GENE dataset, while batch
    sizes of 8 were used for the COMAGC dataset. We used the BioBERT-large-cased⁴⁴4https://huggingface.co/dmis-lab/biobert-v1.1
    for the BioBERT pre-trained model and PubMedBERT-base-uncased-abstract-fulltext⁵⁵5https://huggingface.co/microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext
    for the PubMedBERT pre-trained model, for all biomedical datasets fine-tuning.
    Table [4](#A2.T4 "Table 4 ‣ Appendix B BERT fine-tuning hyperparamater ‣ Prompt-based
    vs. Fine-tuned LLMs Toward Causal Graph Verification") summarizes the hyperparameter
    values for fine-tuning the BERT model experiments.
  prefs: []
  type: TYPE_NORMAL
- en: Table 4\. BERT hyperparameter values.
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter | Value |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| $max\_sequence\_length$ | 128, 256 |'
  prefs: []
  type: TYPE_TB
- en: '| $epoch$ | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| $optimizer$ | Adam |'
  prefs: []
  type: TYPE_TB
- en: '| $lr$ | 2e-5 |'
  prefs: []
  type: TYPE_TB
- en: '| $eps$ | 1e-08 |'
  prefs: []
  type: TYPE_TB
- en: '| $linear\_warmup\_proportion$ | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| $dropout$ | 0.1 |'
  prefs: []
  type: TYPE_TB
- en: '| $hidden\_layer$ | 1024 (FC layer) |'
  prefs: []
  type: TYPE_TB
- en: '| $seed$ | 1234 |'
  prefs: []
  type: TYPE_TB
- en: For the SEMEVAL dataset, we used the bert-base uncased pre-trained model, with
    the same hyperparameters as the BioBERT/PubMEDBERT model above. We implemented
    the fine-tuning of the BERT model using Pytorch. The random seed of 1234 is set
    for all experiments.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agrawal et al. (2022) Monica Agrawal, Stefan Hegselmann, Hunter Lang, Yoon Kim,
    and David Sontag. 2022. Large language models are few-shot clinical information
    extractors. In *Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing*. Association for Computational Linguistics, Abu Dhabi, United
    Arab Emirates, 1998–2022. [https://aclanthology.org/2022.emnlp-main.130](https://aclanthology.org/2022.emnlp-main.130)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balashankar et al. (2019) Ananth Balashankar, Sunandan Chakraborty, Samuel Fraiberger,
    and Lakshminarayanan Subramanian. 2019. Identifying Predictive Causal Factors
    from News Streams. In *Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP)*. Association for Computational Linguistics,
    Hong Kong, China, 2338–2348. [https://doi.org/10.18653/v1/D19-1238](https://doi.org/10.18653/v1/D19-1238)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blanco et al. (2008) Eduardo Blanco, Nuria Castell, and Dan Moldovan. 2008.
    Causal Relation Extraction. In *Proceedings of the Sixth International Conference
    on Language Resources and Evaluation (LREC’08)*. European Language Resources Association
    (ELRA), Marrakech, Morocco, 1161–1186. [http://www.lrec-conf.org/proceedings/lrec2008/pdf/87_paper.pdf](http://www.lrec-conf.org/proceedings/lrec2008/pdf/87_paper.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bui et al. (2010) Quoc-Chinh Bui, Breanndán Ó Nualláin, Charles A. Boucher,
    and Peter MA Sloot. 2010. Extracting causal relations on HIV drug resistance from
    literature. *BMC Bioinformatics* 11, 1 (23 Feb 2010), 101. [https://doi.org/10.1186/1471-2105-11-101](https://doi.org/10.1186/1471-2105-11-101)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chang and Choi (2006) Du-Seong Chang and Key-Sun Choi. 2006. Incremental cue
    phrase learning and bootstrapping method for causality extraction using cue phrase
    and word pair probabilities. *Information Processing & Management* 42, 3 (2006),
    662–678. [https://doi.org/10.1016/j.ipm.2005.04.004](https://doi.org/10.1016/j.ipm.2005.04.004)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2018. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. *CoRR* abs/1810.04805 (2018), 1161–1186. arXiv:1810.04805 [http://arxiv.org/abs/1810.04805](http://arxiv.org/abs/1810.04805)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*. Association for Computational Linguistics,
    Minneapolis, Minnesota, 4171–4186. [https://doi.org/10.18653/v1/N19-1423](https://doi.org/10.18653/v1/N19-1423)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2016) Jinghang Gu, Longhua Qian, and Guodong Zhou. 2016. Chemical-induced
    disease relation extraction with various linguistic features. *Database* 2016
    (04 2016), 1161–1186. [https://doi.org/10.1093/database/baw042](https://doi.org/10.1093/database/baw042)
    arXiv:https://academic.oup.com/database/article-pdf/doi/10.1093/database/baw042/8223888/baw042.pdf
    baw042.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2021) Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama,
    Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-Specific
    Language Model Pretraining for Biomedical Natural Language Processing. *ACM Trans.
    Comput. Healthcare* 3, 1, Article 2 (oct 2021), 23 pages. [https://doi.org/10.1145/3458754](https://doi.org/10.1145/3458754)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hendrickx et al. (2010) Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav
    Nakov, Diarmuid Ó Séaghdha, Sebastian Padó, Marco Pennacchiotti, Lorenza Romano,
    and Stan Szpakowicz. 2010. SemEval-2010 Task 8: Multi-Way Classification of Semantic
    Relations between Pairs of Nominals. In *Proceedings of the 5th International
    Workshop on Semantic Evaluation*. Association for Computational Linguistics, Uppsala,
    Sweden, 33–38. [https://aclanthology.org/S10-1006](https://aclanthology.org/S10-1006)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Herrero-Zazo et al. (2013) María Herrero-Zazo, Isabel Segura-Bedmar, Paloma
    Martínez, and Thierry Declerck. 2013. The DDI corpus: An annotated corpus with
    pharmacological substances and drug–drug interactions. *Journal of Biomedical
    Informatics* 46, 5 (2013), 914–920. [https://doi.org/10.1016/j.jbi.2013.07.011](https://doi.org/10.1016/j.jbi.2013.07.011)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jeblick et al. (2022) Katharina Jeblick, Balthasar Schachtner, Jakob Dexl,
    Andreas Mittermeier, Anna Theresa Stüber, Johanna Topalis, Tobias Weber, Philipp
    Wesp, Bastian Sabel, Jens Ricke, and Michael Ingrisch. 2022. ChatGPT Makes Medicine
    Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports. arXiv:2212.14882 [cs.CL]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khetan et al. (2022) Vivek Khetan, Md Imbesat Rizvi, Jessica Huber, Paige Bartusiak,
    Bogdan Sacaleanu, and Andrew Fano. 2022. MIMICause: Representation and automatic
    extraction of causal relation types from clinical notes. In *Findings of the Association
    for Computational Linguistics: ACL 2022*. Association for Computational Linguistics,
    Dublin, Ireland, 764–773. [https://doi.org/10.18653/v1/2022.findings-acl.63](https://doi.org/10.18653/v1/2022.findings-acl.63)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khoo et al. (2000) Christopher S. G. Khoo, Syin Chan, and Yun Niu. 2000. Extracting
    Causal Knowledge from a Medical Database Using Graphical Patterns. In *Proceedings
    of the 38th Annual Meeting on Association for Computational Linguistics* (Hong
    Kong) *(ACL ’00)*. Association for Computational Linguistics, USA, 336–343. [https://doi.org/10.3115/1075218.1075261](https://doi.org/10.3115/1075218.1075261)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khoo et al. (1998) CHRISTOPHER S. G. Khoo, JAKLIN KORNFILT, ROBERT N. ODDY,
    and SUNG HYON MYAENG. 1998. Automatic Extraction of Cause-Effect Information from
    Newspaper Text Without Knowledge-based Inferencing. *Literary and Linguistic Computing*
    13, 4 (12 1998), 177–186. [https://doi.org/10.1093/llc/13.4.177](https://doi.org/10.1093/llc/13.4.177)
    arXiv:https://academic.oup.com/dsh/article-pdf/13/4/177/10888761/177.pdf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2013) Hee-Jin Lee, Sang-Hyung Shim, Mi-Ryoung Song, Hyunju Lee,
    and Jong C. Park. 2013. CoMAGC: a corpus with multi-faceted annotations of gene-cancer
    relations. *BMC Bioinformatics* 14, 1 (14 Nov 2013), 323. [https://doi.org/10.1186/1471-2105-14-323](https://doi.org/10.1186/1471-2105-14-323)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lee et al. (2019) Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu
    Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical language
    representation model for biomedical text mining. *Bioinformatics* 36, 4 (09 2019),
    1234–1240. [https://doi.org/10.1093/bioinformatics/btz682](https://doi.org/10.1093/bioinformatics/btz682)
    arXiv:https://academic.oup.com/bioinformatics/article-pdf/36/4/1234/48983216/bioinformatics_36_4_1234.pdf'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mihăilă and Ananiadou (2014) Claudiu Mihăilă and Sophia Ananiadou. 2014. Semi-supervised
    learning of causal relations in biomedical scientific discourse. *BioMedical Engineering
    OnLine* 13, 2 (11 Dec 2014), S1. [https://doi.org/10.1186/1475-925X-13-S2-S1](https://doi.org/10.1186/1475-925X-13-S2-S1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L.
    Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex
    Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
    Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training
    language models to follow instructions with human feedback. arXiv:2203.02155 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reklos and Albert (2022) Ioannis Reklos and Meroño-Peñuela Albert. 2022. MediCause:
    Causal Relation Modelling and Extraction from Medical Publications. In *Proceedings
    of the Text2KG 2022: International Workshop on Knowledge Graph Generation from
    Text*. Text2KG, Crete,Hersonissos, Greece, 1161–1186.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shimizu et al. (2006) Shohei Shimizu, Patrik O. Hoyer, Aapo Hyvärinen, and Antti
    Kerminen. 2006. A Linear Non-Gaussian Acyclic Model for Causal Discovery. *J.
    Mach. Learn. Res.* 7 (dec 2006), 2003–2030.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spirtes et al. (2000) P. Spirtes, C. Glymour, and R. Scheines. 2000. *Causation,
    Prediction, and Search* (2nd ed.). MIT press, mit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su and Vijay-Shanker (2022) Peng Su and K. Vijay-Shanker. 2022. Investigation
    of improving the pre-training and fine-tuning of BERT model for biomedical relation
    extraction. *BMC Bioinformatics* 23, 1 (04 Apr 2022), 120. [https://doi.org/10.1186/s12859-022-04642-w](https://doi.org/10.1186/s12859-022-04642-w)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H.
    Chi, Quoc Le, and Denny Zhou. 2022. Chain of Thought Prompting Elicits Reasoning
    in Large Language Models. *CoRR* abs/2201.11903 (2022), 1011. arXiv:2201.11903
    [https://arxiv.org/abs/2201.11903](https://arxiv.org/abs/2201.11903)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2023) Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang,
    Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, and
    Wenjuan Han. 2023. Zero-Shot Information Extraction via Chatting with ChatGPT.
    arXiv:2302.10205 [cs.CL]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
