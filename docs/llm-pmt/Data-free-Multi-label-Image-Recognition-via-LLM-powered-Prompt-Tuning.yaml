- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:45:48'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:45:48
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据无关的多标签图像识别通过LLM驱动的提示调整
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01209](https://ar5iv.labs.arxiv.org/html/2403.01209)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01209](https://ar5iv.labs.arxiv.org/html/2403.01209)
- en: Shuo Yang^(1,2)  Zirui Shang²  Yongqi Wang²
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shuo Yang^(1,2)  Zirui Shang²  Yongqi Wang²
- en: Derong Deng¹  Hongwei Chen¹  Qiyuan Cheng¹  Xinxiao Wu^(1,2)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Derong Deng¹  Hongwei Chen¹  Qiyuan Cheng¹  Xinxiao Wu^(1,2)
- en: ¹Guangdong Laboratory of Machine Perception and Intelligent Computing
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹广东机器感知与智能计算实验室
- en: Shenzhen MSU-BIT University, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深圳MSU-BIT大学，中国
- en: ²Beijing Key Laboratory of Intelligent Information Technology
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ²北京智能信息技术重点实验室
- en: School of Computer Science & Technology, Beijing Institute of Technology, China
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 北京理工大学计算机科学与技术学院，中国
- en: '{shuoyang,shangzirui,wuxinxiao}@bit.edu.cn'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '{shuoyang,shangzirui,wuxinxiao}@bit.edu.cn'
- en: '{1285441164yq, derongdeng.dero, chwr0001}@gmail.com  chengqiyuan@smbu.edu.cn
    Corresponding author.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '{1285441164yq, derongdeng.dero, chwr0001}@gmail.com  chengqiyuan@smbu.edu.cn
    通讯作者'
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper proposes a novel framework for multi-label image recognition without
    any training data, called data-free framework, which uses knowledge of pre-trained
    Large Language Model (LLM) to learn prompts to adapt pre-trained Vision-Language
    Model (VLM) like CLIP to multi-label classification. Through asking LLM by well-designed
    questions, we acquire comprehensive knowledge about characteristics and contexts
    of objects, which provides valuable text descriptions for learning prompts. Then
    we propose a hierarchical prompt learning method by taking the multi-label dependency
    into consideration, wherein a subset of category-specific prompt tokens are shared
    when the corresponding objects exhibit similar attributes or are more likely to
    co-occur. Benefiting from the remarkable alignment between visual and linguistic
    semantics of CLIP, the hierarchical prompts learned from text descriptions are
    applied to perform classification of images during inference. Our framework presents
    a new way to explore the synergies between multiple pre-trained models for novel
    category recognition. Extensive experiments on three public datasets (MS-COCO,
    VOC2007, and NUS-WIDE) demonstrate that our method achieves better results than
    the state-of-the-art methods, especially outperforming the zero-shot multi-label
    recognition methods by 4.7% in mAP on MS-COCO.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种新颖的无训练数据多标签图像识别框架，称为数据无关框架，该框架利用预训练的大型语言模型（LLM）的知识来学习提示，以适应预训练的视觉语言模型（VLM）如CLIP进行多标签分类。通过向LLM提出精心设计的问题，我们获得了关于对象特征和背景的全面知识，这为学习提示提供了有价值的文本描述。然后，我们提出了一种分层提示学习方法，考虑到多标签依赖性，其中当相应的对象表现出相似的属性或更可能共现时，类别特定的提示令牌的子集被共享。得益于CLIP视觉和语言语义之间的显著对齐，从文本描述中学到的分层提示在推理过程中被用于执行图像分类。我们的框架为探索多个预训练模型之间的协同作用提供了一种新方法，以进行新类别识别。在三个公共数据集（MS-COCO、VOC2007
    和 NUS-WIDE）上的广泛实验表明，我们的方法比最先进的方法取得了更好的结果，特别是在MS-COCO上比零样本多标签识别方法提高了4.7%的mAP。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Multi-label image recognition aims to recognize all objects present in an image.
    This task is challenging due to the emergence of novel objects and scenes [[5](#bib.bib5)]
    during inference in real-world scenarios, as shown in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt
    Tuning")(a). Recent large-scale pre-trained Vision-Language Models (VLMs) like
    CLIP [[37](#bib.bib37)] spawn the training-free zero-shot methods [[21](#bib.bib21)],
    which can handle new categories by calculating similarities between images and
    texts in a well-aligned embedding space. To further effectively adapt VLMs to
    enhance the performance of novel categories, several methods have been proposed
    to learn adapter [[3](#bib.bib3)] or prompts [[43](#bib.bib43)] using sufficient
    annotated images, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")(b). However, the
    performance of these prompt learning methods may be limited when it is infeasible
    to collect sufficient fully annotated images.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 多标签图像识别旨在识别图像中存在的所有对象。这项任务具有挑战性，因为在实际场景中的推断过程中会出现新对象和场景 [[5](#bib.bib5)]，如图
    [1](#S1.F1 "图 1 ‣ 1 引言 ‣ 基于 LLM 的无数据多标签图像识别")(a) 所示。最近的大规模预训练视觉-语言模型 (VLMs) 如
    CLIP [[37](#bib.bib37)] 引发了无需训练的零样本方法 [[21](#bib.bib21)]，这些方法通过计算图像和文本在良好对齐的嵌入空间中的相似性来处理新类别。为了进一步有效地将
    VLM 适应于提高新类别的性能，已经提出了几种方法来学习适配器 [[3](#bib.bib3)] 或提示 [[43](#bib.bib43)]，如图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ 基于 LLM 的无数据多标签图像识别")(b) 所示。然而，当无法收集足够的完全标注图像时，这些提示学习方法的性能可能会受到限制。
- en: '![Refer to caption](img/781020070e0718dc65760714287fa2d4.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/781020070e0718dc65760714287fa2d4.png)'
- en: 'Figure 1: Illustration of different ways to handle novel categories. (a) Traditional
    methods train on base categories but fail on novel categories. (b) Recent prompting
    methods successfully adapt VLM to novel categories but need annotated data for
    prompt tuning. (c) Our data-free framework only performs prompt tuning to adapt
    VLM to novel categories by LLM.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：处理新类别的不同方式的说明。 (a) 传统方法在基本类别上进行训练，但在新类别上失败。 (b) 最近的提示方法成功地将 VLM 适应于新类别，但需要带注释的数据进行提示调整。
    (c) 我们的数据无关框架仅通过 LLM 进行提示调整，将 VLM 适应于新类别。
- en: To address this issue, Sun et al. [[43](#bib.bib43)] propose dual context optimization
    to quickly adapt CLIP to multi-label recognition using partially labeled images,
    where only a few categories for each training image are annotated, significantly
    reducing the annotation burden. Guo et al. [[20](#bib.bib20)] propose texts as
    images in prompt tuning to adapt CLIP, where the text descriptions are human-written
    image captions from existing datasets and serve as alternatives to images. This
    method presents a more practical and efficient way for prompting as text descriptions
    are more easily accessible than images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Sun 等人 [[43](#bib.bib43)] 提出了双重上下文优化方法，以便快速将 CLIP 适应于多标签识别，使用部分标注的图像，其中每张训练图像只有少数几个类别被标注，显著减少了标注负担。Guo
    等人 [[20](#bib.bib20)] 提出了在提示调整中使用文本作为图像来适应 CLIP，其中文本描述是来自现有数据集的人工编写图像标题，并作为图像的替代方案。这种方法提供了一种更实际和高效的提示方式，因为文本描述比图像更易获取。
- en: In this paper, we propose a data-free framework for multi-label image recognition
    without any data for training. It leverages knowledge of objects from pre-trained
    Large Language Model (LLM) to adapt CLIP to multi-label classification by textual
    prompt tuning, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")(c). Specifically,
    we propose to collect comprehensive information of objects by designing different
    types of questions posed to LLM. Starting with asking LLM category-agnostic questions
    like [object lists], please summarize 90 attributes that may be common to the
    above 80 words to acquire common attributes, such as shape, color, and material,
    shared by all categories, similarly we then acquire particular attributes for
    each category by category-specific questions like please summarize 30 attributes
    of [object]. Finally, we acquire text descriptions of the attributes by category-description
    questions like please help me generate 100 different sentences about [category]
    from the angle of the [attribute]. Moreover, we design scene-related questions
    like generate ten sentences to describe different scenes involving [category1]
    and [category2] to acquire text descriptions of contextual relationships between
    multiple object categories in real-world scenes, namely relationship knowledge.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了一种无数据框架用于多标签图像识别，无需任何训练数据。它利用预训练的大规模语言模型（LLM）的对象知识，通过文本提示调优来适配CLIP进行多标签分类，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Data-free Multi-label Image Recognition via LLM-powered
    Prompt Tuning")(c)所示。具体来说，我们提出通过设计不同类型的问题向LLM收集对象的综合信息。首先，提出与类别无关的问题，例如[对象列表]，请总结90个可能在上述80个词中共有的属性，如形状、颜色和材料，这些属性被所有类别共享；类似地，我们通过类别特定的问题获得每个类别的特定属性，例如请总结[对象]的30个属性。最后，通过类别描述问题如请帮助我生成关于[类别]的100个不同句子，从[属性]的角度来获得属性的文本描述。此外，我们设计了场景相关问题，如生成十个句子描述涉及[类别1]和[类别2]的不同场景，以获得多个对象类别在真实场景中的上下文关系文本描述，即关系知识。
- en: 'Along with category labels, the acquired text descriptions of attribute and
    relationship knowledge from LLM are used as images for prompt tuning CLIP to multi-label
    recognition. To incorporate the relationship information between multiple objects
    into prompt learning to further improve the performance, we propose a hierarchical
    prompt learning method, which categorizes the prompt tokens into three types:
    (1) shared tokens shared by all object categories; (2) partial-shared tokens shared
    by the object categories of the same subgroups with co-occurrence relationship
    or similar attributes; (3) category-specific tokens specific to each individual
    object category. Through designing these hierarchical tokens, we learn prompts
    that absorb both task-specific knowledge and object-specific knowledge, as well
    as the relationship knowledge between objects. Benefiting from the remarkable
    alignment between visual and linguistic semantics of CLIP, the hierarchical prompts
    learned from text description are applied to perform classification of images
    during inference.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了类别标签之外，从LLM中获得的属性和关系知识的文本描述被用作图像，以进行CLIP的提示调优，达到多标签识别。为了将多个对象之间的关系信息纳入提示学习，从而进一步提升性能，我们提出了一种分层提示学习方法，该方法将提示令牌分类为三种类型：(1)
    所有对象类别共享的共享令牌；(2) 与具有共现关系或相似属性的同一子组对象类别共享的部分共享令牌；(3) 特定于每个对象类别的类别特定令牌。通过设计这些分层令牌，我们学习到的提示能够吸收任务特定知识和对象特定知识，以及对象之间的关系知识。得益于CLIP在视觉和语言语义上的卓越对齐，从文本描述中学到的分层提示在推理过程中用于图像分类。
- en: 'In summary, the contributions of our work are three-fold:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们工作的贡献有三方面：
- en: •
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a data-free framework for multi-label image recognition without any
    training data, which leverages rich knowledge in LLM to prompt tune CLIP. Our
    framework introduces a promising avenue for handling new objects in visual recognition,
    relying solely on pre-trained models, and also paves an effective way to explore
    the synergies between multiple pre-trained models.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种无数据框架用于多标签图像识别，该框架不依赖任何训练数据，而是利用大规模语言模型（LLM）中的丰富知识来提示调优CLIP。我们的框架为处理视觉识别中的新对象提供了一条有前途的途径，仅依靠预训练模型，并且开辟了一条有效探索多个预训练模型之间协同效应的途径。
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a hierarchical prompt learning method to adapt CLIP by using the
    acquired knowledge of objects from LLM. It incorporates relationships between
    different categories into learnable prompts, thus further improving the multi-label
    recognition performance.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种层次化提示学习方法，通过利用从 LLM 获得的对象知识来适应 CLIP。该方法将不同类别之间的关系融入到可学习的提示中，从而进一步提高多标签识别性能。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose to collect comprehensive information about object attributes and
    relationships from LLM by designing different types of questions.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们建议通过设计不同类型的问题，从大语言模型（LLM）中收集关于对象属性和关系的全面信息。
- en: 2 Related Work
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Multi-Label Image Recognition
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 多标签图像识别
- en: Early multi-label image recognition methods [[51](#bib.bib51)] naively treat
    this task as a multiple independent binary classification problem, which trains
    a binary classifier for each category [[33](#bib.bib33), [35](#bib.bib35)]. However,
    these methods do not consider correlations among labels, and recent works have
    focused on incorporating semantic dependencies among labels via graph modeling [[13](#bib.bib13),
    [29](#bib.bib29), [12](#bib.bib12), [10](#bib.bib10), [48](#bib.bib48)] or sequential
    perdition [[47](#bib.bib47), [32](#bib.bib32), [49](#bib.bib49), [23](#bib.bib23),
    [58](#bib.bib58), [56](#bib.bib56)]. There are also works focused on the attention
    mechanism [[39](#bib.bib39), [64](#bib.bib64), [17](#bib.bib17), [57](#bib.bib57)]
    or loss functions [[19](#bib.bib19), [30](#bib.bib30), [38](#bib.bib38)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的多标签图像识别方法 [[51](#bib.bib51)] 天真地将该任务视为多个独立的二分类问题，为每个类别训练一个二分类器 [[33](#bib.bib33),
    [35](#bib.bib35)]。然而，这些方法没有考虑标签之间的关联，最近的工作集中于通过图建模 [[13](#bib.bib13), [29](#bib.bib29),
    [12](#bib.bib12), [10](#bib.bib10), [48](#bib.bib48)] 或序列预测 [[47](#bib.bib47),
    [32](#bib.bib32), [49](#bib.bib49), [23](#bib.bib23), [58](#bib.bib58), [56](#bib.bib56)]
    来整合标签之间的语义依赖。还有一些工作专注于注意力机制 [[39](#bib.bib39), [64](#bib.bib64), [17](#bib.bib17),
    [57](#bib.bib57)] 或损失函数 [[19](#bib.bib19), [30](#bib.bib30), [38](#bib.bib38)]。
- en: Despite remarkable progress that has been made, these methods heavily on large-scale
    annotated images for training, which limits their capabilities in data-limited
    or label-limited scenarios. In recent years, several methods have emerged to address
    the few-shot/zero-shot multi-label image recognition  [[29](#bib.bib29), [4](#bib.bib4),
    [25](#bib.bib25), [40](#bib.bib40), [24](#bib.bib24), [5](#bib.bib5)] and partial-labeled
    image recognition [[15](#bib.bib15), [2](#bib.bib2), [11](#bib.bib11), [36](#bib.bib36),
    [60](#bib.bib60)]. By exploring the synergies between LLM and VLM, we take a significant
    step forward in multi-label image recognition by introducing a data-free framework
    where no training data is provided.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管已经取得了显著进展，这些方法仍然严重依赖大规模标注图像进行训练，这限制了它们在数据有限或标签有限场景中的能力。近年来，出现了几种方法来解决少样本/零样本多标签图像识别
    [[29](#bib.bib29), [4](#bib.bib4), [25](#bib.bib25), [40](#bib.bib40), [24](#bib.bib24),
    [5](#bib.bib5)] 和部分标记图像识别 [[15](#bib.bib15), [2](#bib.bib2), [11](#bib.bib11),
    [36](#bib.bib36), [60](#bib.bib60)]。通过探索大语言模型（LLM）和视觉语言模型（VLM）之间的协同作用，我们通过引入一个无需训练数据的数据自由框架，迈出了多标签图像识别的重要一步。
- en: '![Refer to caption](img/c52e6ad3c405692e3aedb697abd7b9fa.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c52e6ad3c405692e3aedb697abd7b9fa.png)'
- en: 'Figure 2: Overview of our framework.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们框架的概述。
- en: 2.2 Adapting CLIP to Visual Tasks
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 适应 CLIP 到视觉任务
- en: Vision-Language Models (VLMs) have demonstrated impressive capabilities on learning
    generic representations, such as CLIP [[37](#bib.bib37)]. In order to adapt VLMs
    to specific downstream tasks, many prompt tuning methods [[55](#bib.bib55), [26](#bib.bib26),
    [62](#bib.bib62), [61](#bib.bib61), [42](#bib.bib42), [63](#bib.bib63), [41](#bib.bib41)]
    have been proposed to learn task-specific prompts, which gains significant attention
    for both excellent performance and parameter-efficient characteristic. To further
    to bridge the domain gap between the data used to train VLMs and that of specific
    tasks, dedicated adapters [[59](#bib.bib59), [44](#bib.bib44), [46](#bib.bib46),
    [18](#bib.bib18), [52](#bib.bib52), [9](#bib.bib9), [3](#bib.bib3)] have been
    designed and integrated into CLIP, avoiding fine-tuning the entire model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉语言模型（VLMs）在学习通用表征方面表现出色，例如 CLIP [[37](#bib.bib37)]。为了将 VLMs 适应特定的下游任务，已经提出了许多提示调整方法
    [[55](#bib.bib55), [26](#bib.bib26), [62](#bib.bib62), [61](#bib.bib61), [42](#bib.bib42),
    [63](#bib.bib63), [41](#bib.bib41)]，这些方法因其优秀的性能和参数高效特性而受到广泛关注。为了进一步弥合用于训练 VLMs
    的数据与特定任务数据之间的领域差距，已经设计并集成了专用适配器 [[59](#bib.bib59), [44](#bib.bib44), [46](#bib.bib46),
    [18](#bib.bib18), [52](#bib.bib52), [9](#bib.bib9), [3](#bib.bib3)] 到 CLIP 中，从而避免了对整个模型进行微调。
- en: The works most relevant to our method are DualCoOp [[43](#bib.bib43)] and TaI-DPT [[20](#bib.bib20)].
    DualCoOp learns a pair of differentiable prompts using partial-annotated images,
    and TaI-DPT uses image captioning collected from existing datasets as images to
    learn prompts. In contrast, our method inquires LLMs to acquire comprehensive
    knowledge of object categories as text description for prompt learning. Moreover,
    our method learns relationship-aware hierarchical prompts which are tailored to
    multi-label image recognition.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们的方法最相关的工作是 DualCoOp [[43](#bib.bib43)] 和 TaI-DPT [[20](#bib.bib20)]。DualCoOp
    使用部分标注图像学习一对可微分提示，而 TaI-DPT 使用从现有数据集中收集的图像字幕作为图像来学习提示。相比之下，我们的方法通过询问 LLMs 以获取对象类别的全面知识作为文本描述进行提示学习。此外，我们的方法学习了关系感知的分层提示，这些提示专为多标签图像识别量身定制。
- en: 2.3 LLM-enhanced Visual Understanding
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 LLM 增强的视觉理解
- en: LLMs have been allied to help visual understanding tasks [[53](#bib.bib53),
    [6](#bib.bib6), [8](#bib.bib8)] due to their “emergent abilities” of learning
    how to answer such questions from the in-context examples [[50](#bib.bib50)].
    Visual information is represented as text descriptions and then fed into the LLMs
    together with the target question and in-context examples to generate the desired
    results [[54](#bib.bib54), [22](#bib.bib22)]. A recent study employs ChatGPT [[7](#bib.bib7)]
    to generate a comparison tree, which enhances CLIP’s zero-shot performance on
    image classification. In this paper, we focus on using LLM to generate text descriptions
    to facilitate prompt tuning of CLIP to multi-label image recognition.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs 已被用来帮助视觉理解任务 [[53](#bib.bib53), [6](#bib.bib6), [8](#bib.bib8)]，这得益于它们的“突现能力”，即从上下文示例中学习如何回答此类问题
    [[50](#bib.bib50)]。视觉信息以文本描述的形式呈现，然后与目标问题和上下文示例一起输入到 LLMs 中，以生成所需的结果 [[54](#bib.bib54),
    [22](#bib.bib22)]。最近的一项研究利用 ChatGPT [[7](#bib.bib7)] 生成比较树，从而提升了 CLIP 在图像分类中的零样本性能。在本文中，我们重点使用
    LLM 生成文本描述，以促进 CLIP 的多标签图像识别的提示调整。
- en: 3 Our Method
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 我们的方法
- en: 3.1 Overview
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 概述
- en: We propose a data-free framework for multi-label image recognition without any
    training data. A large language model (ChatGLM) serves as a repository of encyclopedic
    knowledge, and we propose to acquire comprehensive knowledge of object categories
    by designing different types of questions posed to ChatGLM. This is motivated
    by the fact that we can effectively identify an object in a picture if provided
    with a linguistic description. Then a pre-trained vision-language model (CLIP)
    is prompt tuned using the acquired knowledge to enhance multi-label classification,
    based on the aligned visual and linguistic embedding space. We propose a hierarchical
    prompt learning method to incorporate relationships between objects into the learnable
    prompts. Figure [2](#S2.F2 "Figure 2 ‣ 2.1 Multi-Label Image Recognition ‣ 2 Related
    Work ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning")
    shows an overview of our framework.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种无数据框架，用于多标签图像识别，无需任何训练数据。一个大型语言模型（ChatGLM）作为百科知识的存储库，我们建议通过向 ChatGLM
    提出不同类型的问题来获取对象类别的全面知识。这一提议的动机是，如果提供语言描述，我们可以有效地识别图像中的对象。然后，使用获得的知识对预训练的视觉语言模型（CLIP）进行提示调整，以增强多标签分类，基于对齐的视觉和语言嵌入空间。我们提出了一种分层提示学习方法，将对象之间的关系纳入可学习的提示中。图 [2](#S2.F2
    "Figure 2 ‣ 2.1 Multi-Label Image Recognition ‣ 2 Related Work ‣ Data-free Multi-label
    Image Recognition via LLM-powered Prompt Tuning") 展示了我们框架的概述。
- en: Given an input image x, multi-label image recognition aims to identify all object
    categories in it, formulated as $\mathbf{S}=f_{\Phi,\Psi}(\mathbf{x})$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入图像 x，多标签图像识别旨在识别其中的所有对象类别，公式化为 $\mathbf{S}=f_{\Phi,\Psi}(\mathbf{x})$。
- en: 3.2 Knowledge Acquirement
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 知识获取
- en: To describe an object, it is crucial to have detailed information about its
    color, shape, texture, and other attributes. To obtain this information, we engage
    with ChatGLM, a highly knowledgeable language model that functions as a chatbot
    and responds to carefully crafted questions. Its extensive encyclopedic knowledge
    allows us to extract the necessary details for adequate object description.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 描述一个对象时，详细了解其颜色、形状、纹理和其他属性至关重要。为了获得这些信息，我们与 ChatGLM 互动，这是一个知识丰富的语言模型，作为聊天机器人回答精心设计的问题。其广泛的百科知识使我们能够提取足够的对象描述所需的详细信息。
- en: Coarse Attribute Description. To capture diverse aspects of objects, we begin
    by extracting common attributes shared by all categories using category-agnostic
    questions and particular attributions for individual categories using category-specific
    questions, formulated as
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 粗略属性描述。为了捕捉物体的多样化方面，我们首先使用类别无关的问题提取所有类别共享的常见属性，并使用类别特定的问题提取个别类别的特定属性，表述为
- en: '|  | $(\mathbb{A}_{c},\mathbb{A}_{s})=\Phi(\Pi_{1}(\mathbb{Y})),$ |  | (1)
    |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $(\mathbb{A}_{c},\mathbb{A}_{s})=\Phi(\Pi_{1}(\mathbb{Y})),$ |  | (1)
    |'
- en: where $\Pi_{1}(\cdot)$-th category.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Pi_{1}(\cdot)$-th类别。
- en: Then we obtain the text descriptions of each category by asking additional questions.
    Note that these text descriptions of attributes may contain noise, we call them
    coarse attribute descriptions. This process is formulated by
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过提出额外的问题来获取每个类别的文本描述。注意，这些属性的文本描述可能包含噪声，我们称之为粗略属性描述。这个过程被表述为
- en: '|  | $\mathbb{D}_{i}^{c}=\Phi(\Pi_{2}(\mathbb{A}_{c}\cup\mathbb{A}_{s,i},Y_{i})),$
    |  | (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{D}_{i}^{c}=\Phi(\Pi_{2}(\mathbb{A}_{c}\cup\mathbb{A}_{s,i},Y_{i})),$
    |  | (2) |'
- en: where $\Pi_{2}(\cdot)$ denote the coarse attribute description sets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Pi_{2}(\cdot)$表示粗略属性描述集。
- en: Fine-grained Attribute Description. We design several questions to remove the
    noisy attributes that are irrelevant to the specific category, resulting in a
    fine-grained attribute set for each category. We then inquire ChatGLM to acquire
    the fine-grained attribute descriptions $\mathbb{D}^{f}=\{\mathbb{D}^{f}_{1},\cdots,\mathbb{D}^{f}_{N}\}$
    of Eq.([2](#S3.E2 "Equation 2 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")). This process is
    formulated by
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 精细化属性描述。我们设计了几个问题来去除与特定类别无关的噪声属性，从而为每个类别生成一个精细化的属性集。然后，我们询问ChatGLM以获取精细化属性描述$\mathbb{D}^{f}=\{\mathbb{D}^{f}_{1},\cdots,\mathbb{D}^{f}_{N}\}$，如公式([2](#S3.E2
    "公式 2 ‣ 3.2 知识获取 ‣ 3 我们的方法 ‣ 基于LLM的无数据多标签图像识别的提示调优"))所示。这个过程被表述为
- en: '|  | $1$2 |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $\Pi_{3}(\cdot)$ denotes the questions about how to remove irrelevant
    attributes, like [attribute list], please delete the above attribute words given
    that are not very relevant to [category]. Finally, 70 attribute words remain.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\Pi_{3}(\cdot)$表示关于如何去除无关属性的问题，例如[attribute list]，请删除与[category]不太相关的上述属性词。最终，剩下70个属性词。
- en: 'Relationship Description. In multi-label image recognition, the co-occurrence
    relationships between different categories contribute significantly to the performance [[12](#bib.bib12),
    [48](#bib.bib48)]. To simulate this scenario, we first split all categories into
    multiple scene-related subgroups by ChatGLM:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 关系描述。在多标签图像识别中，不同类别之间的共现关系对性能有显著影响[[12](#bib.bib12), [48](#bib.bib48)]。为了模拟这种情况，我们首先通过ChatGLM将所有类别拆分为多个与场景相关的子组：
- en: '|  | $\mathbb{G}=\{\mathbb{G}_{i}\}_{i=1}^{n_{3}}=\Phi(\Pi_{4}(\mathbb{Y})),$
    |  | (4) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{G}=\{\mathbb{G}_{i}\}_{i=1}^{n_{3}}=\Phi(\Pi_{4}(\mathbb{Y})),$
    |  | (4) |'
- en: 'where $\Pi_{4}(\cdot)$, like generate 100 different descriptive sentences for
    a scene containing [category1] and [category2], and fed into ChatGLM to obtain
    relationship descriptions:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在其中，$\Pi_{4}(\cdot)$，例如生成100个不同的描述性句子，用于包含[category1]和[category2]的场景，然后输入ChatGLM以获得关系描述：
- en: '|  | $\mathbb{D}_{i}^{r}=\Phi(\Pi_{5}(\mathbb{G}_{i})),$ |  | (5) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{D}_{i}^{r}=\Phi(\Pi_{5}(\mathbb{G}_{i})),$ |  | (5) |'
- en: where $\mathbb{D}_{i}^{r}$ denote the fine-grained attribute description sets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbb{D}_{i}^{r}$表示精细化属性描述集。
- en: Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning") illustrates an example
    of the designed questions and their corresponding answers from ChatGLM.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S3.F3 "图 3 ‣ 3.2 知识获取 ‣ 3 我们的方法 ‣ 基于LLM的无数据多标签图像识别的提示调优")展示了设计问题及其对应的ChatGLM回答的示例。
- en: '![Refer to caption](img/43c06393ada1dc8cd26bb1f59effcd8c.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/43c06393ada1dc8cd26bb1f59effcd8c.png)'
- en: 'Figure 3: An example of the designed questions and their corresponding answers
    from ChatGLM. More detailed examples can be found in the supplementary materials.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：设计问题及其对应的ChatGLM回答的示例。更多详细示例见补充材料。
- en: 3.3 Hierarchical Prompt Learning
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 分层提示学习
- en: Based on the previous generated text descriptions $\mathbb{D}=\mathbb{D}^{k},k\in\{c,f,r\}$,
    we propose hierarchical prompt learning to adapt CLIP to multi-label recognition,
    where hierarchical prompts are designed to model relationships between categories,
    and both global and local prompt learning are introduced to grasp the discriminability
    of features.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于先前生成的文本描述$\mathbb{D}=\mathbb{D}^{k},k\in\{c,f,r\}$，我们提出了层次化提示学习，以适应CLIP的多标签识别，其中层次化提示被设计用于建模类别之间的关系，并引入全局和局部提示学习，以掌握特征的区分性。
- en: 'Hierarchical Prompts. A learnable prompt usually consists of several learnable
    tokens and a placeholder to put the category label, denoted as $\mathbf{p}_{i}=[\mathbf{t}_{1}^{i},\mathbf{t}_{2}^{i},\cdots,\mathbf{t}_{M}^{i},Y_{i}]$-th
    category label. For different categories, there are two types of prompts: shared
    prompts, where tokens are shared across different categories, and category-specific
    prompts, where tokens are distinct for each category. Both types of prompts have
    been demonstrated effective in recent works [[62](#bib.bib62), [63](#bib.bib63)],
    but they neglect the relationships of different categories, leading to sub-optimal
    performance.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 层次化提示。一个可学习的提示通常由几个可学习的标记和一个用于放置类别标签的占位符组成，记作$\mathbf{p}_{i}=[\mathbf{t}_{1}^{i},\mathbf{t}_{2}^{i},\cdots,\mathbf{t}_{M}^{i},Y_{i}]$-th类别标签。对于不同的类别，有两种类型的提示：共享提示，其中标记在不同类别间共享，以及特定类别提示，其中标记对每个类别都是独特的。这两种类型的提示在最近的研究中均已证明有效[[62](#bib.bib62),
    [63](#bib.bib63)]，但它们忽略了不同类别之间的关系，导致次优的表现。
- en: To capture the relationships between different categories, we propose hierarchical
    prompts $\mathbf{P}^{h}$, a mixed version of both shared tokens and category-specific
    tokens, and additional partial-shared tokens only between categories that most
    likely co-occur in a scene or have similar attributes, denote as
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉不同类别之间的关系，我们提出了层次化提示$\mathbf{P}^{h}$，这是共享标记和特定类别标记的混合版本，并且在最可能在场景中共同出现或具有相似属性的类别之间加入额外的部分共享标记，记作
- en: '|  | $$\mathbf{P}^{h}=\begin{bmatrix}\mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,3},&amp;\cdots&amp;\mathbf{t}_{c,1},&amp;Y_{1}\\
    \mathbf{t}_{s},&amp;\mathbf{t}_{c,a},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,4},&amp;\cdots&amp;\mathbf{t}_{c,2},&amp;Y_{2}\\'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\mathbf{P}^{h}=\begin{bmatrix}\mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,3},&amp;\cdots&amp;\mathbf{t}_{c,1},&amp;Y_{1}\\
    \mathbf{t}_{s},&amp;\mathbf{t}_{c,a},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,4},&amp;\cdots&amp;\mathbf{t}_{c,2},&amp;Y_{2}\\'
- en: \mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,3},&amp;\cdots&amp;\mathbf{t}_{c,3},&amp;Y_{3}\\
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{p,2},&amp;\mathbf{t}_{p,3},&amp;\cdots&amp;\mathbf{t}_{c,3},&amp;Y_{3}\\
- en: '&amp;\ddots&amp;&amp;&amp;\ddots&amp;\\'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;\ddots&amp;&amp;&amp;\ddots&amp;\\'
- en: \mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{c,b},&amp;\mathbf{t}_{p,4},&amp;\cdots&amp;\mathbf{t}_{c,N},&amp;Y_{N}\end{bmatrix},$$
    |  |
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: \mathbf{t}_{s},&amp;\mathbf{t}_{p,1},&amp;\mathbf{t}_{c,b},&amp;\mathbf{t}_{p,4},&amp;\cdots&amp;\mathbf{t}_{c,N},&amp;Y_{N}\end{bmatrix},$$
    |  |
- en: 'where all tokens are divided into three types: (1) shared tokens, denoted as
    $\mathbf{t}_{s}$) due to no relationships in that circumstance. For example, in
    the kitchenware scene, the knife and oven have a shared token, but the corresponding
    tokens of the sofa and book may be specific.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中所有标记分为三种类型：(1) 共享标记，记作$\mathbf{t}_{s}$，由于在这种情况下没有关系。例如，在厨房场景中，刀子和烤箱有一个共享标记，但沙发和书的对应标记可能是特定的。
- en: 'Global Learning. Global learning aims to learn global hierarchical prompts
    to grasp the discriminative ability of global features. Let $\mathbf{P}_{g,i}^{h}$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 全局学习。全局学习旨在学习全局层次化提示，以掌握全局特征的区分能力。设$\mathbf{P}_{g,i}^{h}$：
- en: '|  | $\mathbf{G}_{i}=\Psi_{t}(\mathbf{P}_{g,i}^{h})\in\mathbb{R}^{d},\quad
    i\in\{1,2,\cdots,N\},$ |  | (6) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{G}_{i}=\Psi_{t}(\mathbf{P}_{g,i}^{h})\in\mathbb{R}^{d},\quad
    i\in\{1,2,\cdots,N\},$ |  | (6) |'
- en: 'where $d=512$:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$d=512$：
- en: '|  | $\mathbf{T}^{G}=\Psi_{t}(\mathbf{r})\in\mathbb{R}^{d},\quad\mathbf{r}\in\mathbb{D}.$
    |  | (7) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{T}^{G}=\Psi_{t}(\mathbf{r})\in\mathbb{R}^{d},\quad\mathbf{r}\in\mathbb{D}.$
    |  | (7) |'
- en: The cosine similarity between the global feature of text description and the
    global category embedding, denoted as $S_{i}^{G}$, is calculated by
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 文本描述的全局特征与全局类别嵌入之间的余弦相似度，记作$S_{i}^{G}$，计算方式为
- en: '|  | $$S_{i}^{G}=\left<\mathbf{T}^{G},\mathbf{G}_{i}\right> |  | (8)
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $$S_{i}^{G}=\left<\mathbf{T}^{G},\mathbf{G}_{i}\right> |  | (8)
    |'
- en: 'Local Learning. Local learning aims to learn local hierarchical prompts to
    grasp the discriminative ability of fine-grained features. Let $\mathbf{P}_{l,i}^{h}$:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 局部学习。局部学习旨在学习局部层次化提示，以掌握细粒度特征的区分能力。设$\mathbf{P}_{l,i}^{h}$：
- en: '|  | $\mathbf{L}_{i}=\Psi_{t}(\mathbf{P}_{l,i}^{h})\in\mathbb{R}^{d},\quad
    i\in\{1,2,\cdots,N\}.$ |  | (9) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{L}_{i}=\Psi_{t}(\mathbf{P}_{l,i}^{h})\in\mathbb{R}^{d},\quad
    i\in\{1,2,\cdots,N\}.$ |  | (9) |'
- en: 'For the text description $\mathbf{r}\in\mathbb{D}$:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 对于文本描述 $\mathbf{r}\in\mathbb{D}$：
- en: '|  | $\mathbf{T}^{L}=\widetilde{\Psi}_{t}(\mathbf{r})\in\mathbb{R}^{N_{r}\times
    d},\quad\mathbf{r}\in\mathbb{D},$ |  | (10) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{T}^{L}=\widetilde{\Psi}_{t}(\mathbf{r})\in\mathbb{R}^{N_{r}\times
    d},\quad\mathbf{r}\in\mathbb{D},$ |  | (10) |'
- en: 'where $N_{r}$ token features (global features). The category-aware similarity
    between the sequential local features of text description and the local category
    embedding is calculated in a weighted manner:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N_{r}$ token 特征（全球特征）。通过加权方式计算文本描述的序列本地特征与本地类别嵌入之间的类别感知相似性：
- en: '|  | $1$2 |  | (11) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (11) |'
- en: where -th
    token (column) of local image features.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 和 -th
    token (column) 的局部图像特征。
- en: 5 Experiments
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Datasets and Evaluation Metrics
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 数据集和评估指标
- en: Datasets. We conduct experiments on the MS-COCO [[31](#bib.bib31)], VOC2007 [[16](#bib.bib16)]
    and NUS-WIDE [[13](#bib.bib13)] datasets for evaluation. For all three datasets,
    no training data is used and the testing is performed on the testing or validation
    sets. MS-COCO is a widely used multi-label dataset for image recognition, which
    contains 80 categories with 82,081 training images and 40,504 validation images.
    VOC2007 contains 20 object categories with a total of 5,011 images for training
    and validation and 4,952 images for testing. NUS-WIDE contains 81 categories with
    161,789 images for training and 107,859 images for testing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集。我们在 MS-COCO [[31](#bib.bib31)]、VOC2007 [[16](#bib.bib16)] 和 NUS-WIDE [[13](#bib.bib13)]
    数据集上进行实验评估。对于所有三个数据集，不使用训练数据，测试在测试集或验证集上进行。MS-COCO 是一个广泛使用的多标签图像识别数据集，包含 80 个类别，有
    82,081 张训练图像和 40,504 张验证图像。VOC2007 包含 20 个对象类别，共有 5,011 张训练和验证图像以及 4,952 张测试图像。NUS-WIDE
    包含 81 个类别，有 161,789 张训练图像和 107,859 张测试图像。
- en: Metrics. We use the conventional evaluation metrics, including the mean of class-average
    precision (mAP) and the overall F1 score at Top-3 predictions.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。我们使用传统的评估指标，包括类别平均精度的均值（mAP）和 Top-3 预测的整体 F1 分数。
- en: 5.2 Implementation Details
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 实现细节
- en: We employ ResNet-50 as the visual encoder of CLIP with an input resolution of
    224 $\times$ in Eq.([16](#S4.E16 "Equation 16 ‣ 4.1 Inference ‣ 4 Training Objectives
    ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning")) are
    set to 0.2 and 0.65, respectively.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ResNet-50 作为 CLIP 的视觉编码器，输入分辨率为 224 $\times$ 在 Eq.([16](#S4.E16 "方程 16
    ‣ 4.1 推断 ‣ 4 训练目标 ‣ 无数据多标签图像识别通过 LLM 驱动的提示调整")) 中设置为 0.2 和 0.65。
- en: 5.3 Ablation Studies
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 消融研究
- en: 'Effectiveness of different text descriptions. To evaluate the acquired text
    descriptions from ChatGLM by our method, we employ different inquiry strategies
    to obtain text descriptions for comparison, including (1) “Hand-craft prompt”:
    the inference is directly performed using hand-craft prompts without prompt tuning;
    (2) “Image Captions”: the human-written image captions from existing datasets
    are used for prompt tuning; (3) “Coarse Attribute”: the text descriptions of object
    attributes with noise are used for prompt tuning, generated by Eq.([1](#S3.E1
    "Equation 1 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method ‣ Data-free Multi-label
    Image Recognition via LLM-powered Prompt Tuning")); (4) “Fine-grained Attribute”:
    the text descriptions of filtered object attributes are used for prompt tuning,
    generated by Eq.([3](#S3.E3 "Equation 3 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method
    ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning")).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 不同文本描述的有效性。为了评估我们方法中从 ChatGLM 获得的文本描述，我们采用了不同的询问策略来获得文本描述进行比较，包括 (1) “手工定义提示”：直接使用手工定义的提示进行推断，无需提示调整；
    (2) “图像说明”：使用现有数据集中人工编写的图像说明进行提示调整； (3) “粗略属性”：使用带噪声的物体属性文本描述进行提示调整，由 Eq.([1](#S3.E1
    "Equation 1 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method ‣ Data-free Multi-label
    Image Recognition via LLM-powered Prompt Tuning")) 生成； (4) “细粒度属性”：使用过滤后的物体属性文本描述进行提示调整，由
    Eq.([3](#S3.E3 "Equation 3 ‣ 3.2 Knowledge Acquirement ‣ 3 Our Method ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")) 生成。
- en: 'Table [1](#S4.T1 "Table 1 ‣ 4 Training Objectives ‣ Data-free Multi-label Image
    Recognition via LLM-powered Prompt Tuning") shows the results of different text
    descriptions on the MS-COCO, VOC2007 and NUS-WIDE datasets. We have the following
    observations: (1) Our method achieves substantial improvements over “Hand-crafted
    prompts”, with F1 score gains of 7.64%, 1.55%, and 9.6% on the three datasets,
    respectively. This highlights the remarkable contribution of knowledge extracted
    from ChatGLM in enhancing the zero-shot performance of CLIP on multi-label image
    recognition; (2) Our method outperforms “Fine-grained Attribute” descriptions
    by approximately 5.2%, 1.9% and 3.2% in F1 score on the three datasets, respectively.
    This superiority emphasizes that considering the relationships between objects
    captures more discriminative information to enhance multi-label recognition; (3)
    Our method performs better than “Image Captions” on most metrics, suggesting that
    ChatGLM provides more comprehensive knowledge than human-written image captions;
    (4) The performance of “Coarse Attribute” is lower than that of “Fine-grained
    Attribute”, confirming that the presence of noise within the coarse attributes
    of objects degrades the performance.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S4.T1 "Table 1 ‣ 4 Training Objectives ‣ Data-free Multi-label Image
    Recognition via LLM-powered Prompt Tuning") 显示了不同文本描述在 MS-COCO、VOC2007 和 NUS-WIDE
    数据集上的结果。我们有以下观察： (1) 我们的方法在 “手工定义提示” 上取得了显著的改进，在三个数据集上的 F1 分数分别提高了 7.64%、1.55%
    和 9.6%。这突显了从 ChatGLM 提取的知识在提升 CLIP 在多标签图像识别中的零样本性能方面的显著贡献； (2) 我们的方法在三个数据集上分别比
    “细粒度属性” 描述的 F1 分数高出约 5.2%、1.9% 和 3.2%。这一优势强调了考虑物体之间的关系能够捕捉到更多的区分性信息，从而增强多标签识别；
    (3) 我们的方法在大多数指标上优于 “图像说明”，这表明 ChatGLM 提供了比人工编写的图像说明更全面的知识； (4) “粗略属性”的表现低于 “细粒度属性”，确认了物体粗略属性中的噪声会降低性能。
- en: 'Effectiveness of different prompts. To evaluate the proposed hierarchical prompts,
    we employ different types of prompts for comparison, including (1) “Hand-craft”:
    the human-defined prompts are customized for each category like an image of [category];
    (2) “Category-specific”: the learnable tokens of prompts are specific for each
    category; (3) “Shared”: the learnable tokens of prompts are shared across all
    categories.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 不同提示的有效性。为了评估提出的分层提示，我们采用了不同类型的提示进行比较，包括 (1) “手工定义”：人为定义的提示被定制用于每个类别，如 [category]
    的图像； (2) “类别特定”：提示的可学习标记针对每个类别； (3) “共享”：提示的可学习标记在所有类别中共享。
- en: Table [2](#S4.T2 "Table 2 ‣ 4 Training Objectives ‣ Data-free Multi-label Image
    Recognition via LLM-powered Prompt Tuning") shows the results of different types
    of prompts on MS-COCO. Our hierarchical prompts outperform all other methods,
    demonstrating the effectiveness of incorporating inter-category relationships
    into prompts. Moreover, compared to the performance of hand-crafted prompts, that
    of learnable prompts (i.e., category-specific, shared and hierarchical prompts)
    is much higher. Interestingly, “Shared prompts” outperforms “Category-specific”,
    indicating the better generalization ability of shared prompts.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "Table 2 ‣ 4 Training Objectives ‣ Data-free Multi-label Image
    Recognition via LLM-powered Prompt Tuning") 显示了不同类型提示在 MS-COCO 上的结果。我们的层次提示优于所有其他方法，展示了将类别间关系纳入提示的有效性。此外，与手工制作的提示相比，可学习提示（即类别特定、共享和层次提示）的性能要高得多。有趣的是，“共享提示”优于“类别特定提示”，这表明共享提示具有更好的泛化能力。
- en: 'Table 3: Results of different components on MS-COCO.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同组件在 MS-COCO 上的结果。
- en: '| Local learning | Order loss | F1 | mAP |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 本地学习 | 顺序损失 | F1 | mAP |'
- en: '| --- | --- | --- | --- |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| $\times$ | 52.4 | 62.2 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| $\times$ | 52.4 | 62.2 |'
- en: '| ✓ | $\times$ | 56.3 | 66.0 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | $\times$ | 56.3 | 66.0 |'
- en: '| $\times$ | ✓ | 53.2 | 63.2 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| $\times$ | ✓ | 53.2 | 63.2 |'
- en: '| ✓ | ✓ | 57.3 | 66.8 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| ✓ | ✓ | 57.3 | 66.8 |'
- en: Effectiveness of order loss. To evaluate the effectiveness of the order loss
    in Eq.([13](#S4.E13 "Equation 13 ‣ 4 Training Objectives ‣ Data-free Multi-label
    Image Recognition via LLM-powered Prompt Tuning")), we remove it for comparison.
    The results on MS-COCO are shown in Table [3](#S5.T3 "Table 3 ‣ 5.3 Ablation Studies
    ‣ 5 Experiments ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt
    Tuning"), verifying the advantage of the order loss in mitigating the noisy of
    text descriptions acquired from ChatGLM.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 顺序损失的有效性。为了评估 Eq.([13](#S4.E13 "Equation 13 ‣ 4 Training Objectives ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")) 中顺序损失的有效性，我们将其移除以进行比较。MS-COCO
    上的结果显示在表 [3](#S5.T3 "Table 3 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning") 中，验证了顺序损失在减轻从 ChatGLM
    获取的文本描述噪声方面的优势。
- en: Effectiveness of local learning. To evaluate the effectiveness of the local
    learning of hierarchical prompts, we remove it for comparison. The results on
    MS-COCO are shown in Table [3](#S5.T3 "Table 3 ‣ 5.3 Ablation Studies ‣ 5 Experiments
    ‣ Data-free Multi-label Image Recognition via LLM-powered Prompt Tuning"), highlighting
    the significant impact of focusing on image sub-regions in multi-label recognition.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 本地学习的有效性。为了评估层次提示的本地学习效果，我们将其移除以进行比较。MS-COCO 上的结果显示在表 [3](#S5.T3 "Table 3 ‣
    5.3 Ablation Studies ‣ 5 Experiments ‣ Data-free Multi-label Image Recognition
    via LLM-powered Prompt Tuning") 中，突显了在多标签识别中专注于图像子区域的显著影响。
- en: 'Table 4: Results of different numbers of different token types in hierarchical
    prompts on MS-COCO. S: shared token; P-S #1: partial-shared token over more categories
    (within coarse subgroups); P-S #2: partial-shared token over fewer categories
    (within more fined subgroups); C-S: category-specific token.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4：在 MS-COCO 上不同层次提示中不同类型标记数量的结果。S: 共享标记；P-S #1: 多类别（粗略子组内）部分共享标记；P-S #2:
    少类别（细化子组内）部分共享标记；C-S: 类别特定标记。'
- en: '| S | P-S #1 | P-S #2 | C-S | F1 | mAP |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| S | P-S #1 | P-S #2 | C-S | F1 | mAP |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 8 | 8 | 8 | 8 | 56.4 | 66.6 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8 | 8 | 8 | 56.4 | 66.6 |'
- en: '| 12 | 8 | 6 | 6 | 57.2 | 66.6 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 8 | 6 | 6 | 57.2 | 66.6 |'
- en: '| 16 | 8 | 4 | 4 | 57.3 | 66.8 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 8 | 4 | 4 | 57.3 | 66.8 |'
- en: '| 20 | 4 | 4 | 4 | 57.1 | 66.7 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 4 | 4 | 4 | 57.1 | 66.7 |'
- en: '| 20 | 6 | 4 | 2 | 56.0 | 66.5 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 6 | 4 | 2 | 56.0 | 66.5 |'
- en: 'Table 5: Comparison results (mAP) with the state-of-the-art methods on MS-COCO,
    VOC2007 and NUS-WIDE.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：在 MS-COCO、VOC2007 和 NUS-WIDE 上与最先进方法的比较结果（mAP）。
- en: '| Method | Venue | Training source | Annotation | MS-COCO | VOC2007 | NUS-WIDE
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 会议 | 训练来源 | 注释 | MS-COCO | VOC2007 | NUS-WIDE |'
- en: '| SRN [[64](#bib.bib64)] | CVPR 2017 | Image | Fully labeled | 77.1 | - | 62.0
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| SRN [[64](#bib.bib64)] | CVPR 2017 | 图像 | 完全标记 | 77.1 | - | 62.0 |'
- en: '| ML-GCN [[12](#bib.bib12)] | CVPR 2019 | 83.0 | 94.0 | - |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| ML-GCN [[12](#bib.bib12)] | CVPR 2019 | 83.0 | 94.0 | - |'
- en: '| ASL [[38](#bib.bib38)] | ICCV 2021 | 86.6 | 94.6 | 65.2 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| ASL [[38](#bib.bib38)] | ICCV 2021 | 86.6 | 94.6 | 65.2 |'
- en: '| SARB [[36](#bib.bib36)] | AAAI 2022 | Image | Partially labeled | 71.2 |
    83.5 | - |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| SARB [[36](#bib.bib36)] | AAAI 2022 | 图像 | 部分标记 | 71.2 | 83.5 | - |'
- en: '| SST [[11](#bib.bib11)] | AAAI 2022 | 68.1 | 81.5 | - |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| SST [[11](#bib.bib11)] | AAAI 2022 | 68.1 | 81.5 | - |'
- en: '| DualCoOp [[43](#bib.bib43)] | NeurIPS 2022 | 78.7 | 90.3 | - |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| DualCoOp [[43](#bib.bib43)] | NeurIPS 2022 | 78.7 | 90.3 | - |'
- en: '| LL-R [[27](#bib.bib27)] | CVPR 2022 | Image | One labeled | 72.6 | 90.6 |
    47.4 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| LL-R [[27](#bib.bib27)] | CVPR 2022 | 图像 | 一个标注 | 72.6 | 90.6 | 47.4 |'
- en: '| G²NetPL [[1](#bib.bib1)] | BMVC 2022 | 72.5 | 89.9 | 48.5 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| G²NetPL [[1](#bib.bib1)] | BMVC 2022 | 72.5 | 89.9 | 48.5 |'
- en: '| LSAN [[45](#bib.bib45)] | CVPR 2016 | Image | Unlabeled | 65.5 | 87.9 | 41.3
    |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| LSAN [[45](#bib.bib45)] | CVPR 2016 | 图像 | 未标注 | 65.5 | 87.9 | 41.3 |'
- en: '| WAN [[34](#bib.bib34)] | ICCV 2019 | 63.9 | 86.2 | 40.1 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| WAN [[34](#bib.bib34)] | ICCV 2019 | 63.9 | 86.2 | 40.1 |'
- en: '| Curriculum [[15](#bib.bib15)] | CVPR 2019 | 63.2 | 83.1 | 39.4 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Curriculum [[15](#bib.bib15)] | CVPR 2019 | 63.2 | 83.1 | 39.4 |'
- en: '| Naive AN [[28](#bib.bib28)] | NeurIPS 2020 | 65.1 | 86.5 | 40.8 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Naive AN [[28](#bib.bib28)] | NeurIPS 2020 | 65.1 | 86.5 | 40.8 |'
- en: '| TaI-DPT [[20](#bib.bib20)] | CVPR 2023 | Image Caption | Unlabeled | 65.1
    | 88.3 | 46.5 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| TaI-DPT [[20](#bib.bib20)] | CVPR 2023 | 图像标题 | 未标注 | 65.1 | 88.3 | 46.5
    |'
- en: '| Ours | - | ChatGLM | 66.8 | 88.7 | 47.0 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法 | - | ChatGLM | 66.8 | 88.7 | 47.0 |'
- en: 5.4 Parameter Analysis
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 参数分析
- en: 'Number of different types of token. The performances of different numbers of
    different token types in hierarchical prompts on MS-COCO are shown in Figure [4](#S5.F4
    "Figure 4 ‣ 5.4 Parameter Analysis ‣ 5 Experiments ‣ Data-free Multi-label Image
    Recognition via LLM-powered Prompt Tuning") (a). We observe that the performance
    increases as the token number increases, but a number larger than 32 brings negative
    impact. Moreover, we also analyze the composition of different types of tokens
    in Table [4](#S5.T4 "Table 4 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning"). Note that the partial-shared
    tokens are split into two parts: “P-S #1” are coarse parts that tokens are shared
    over more categories than that of “P-S #2”. We observe similar mAP scores for
    different configurations, indicating that our method is robust to the numbers
    of different token types.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '不同类型的 token 数量。不同 token 类型数量在层次化提示下在 MS-COCO 上的表现如图 [4](#S5.F4 "Figure 4 ‣
    5.4 Parameter Analysis ‣ 5 Experiments ‣ Data-free Multi-label Image Recognition
    via LLM-powered Prompt Tuning") (a) 所示。我们观察到，随着 token 数量的增加，表现有所提升，但超过 32 的数量会带来负面影响。此外，我们还在表
    [4](#S5.T4 "Table 4 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ Data-free Multi-label
    Image Recognition via LLM-powered Prompt Tuning") 中分析了不同类型的 tokens 组成。注意，部分共享
    tokens 被分为两部分：“P-S #1” 是 tokens 共享的类别比 “P-S #2” 多的粗略部分。我们观察到不同配置下 mAP 得分相似，表明我们的方法对不同类型的
    token 数量具有鲁棒性。'
- en: '![Refer to caption](img/2c492d51aff1ca5535272fff1585eca8.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2c492d51aff1ca5535272fff1585eca8.png)'
- en: 'Figure 4: Results on MS-COCO. (a) Analysis of the effect of number of tokens.
    (b) Analysis of the effect of weight between global and local prompts, i.e. $\lambda_{2}$
    in Eq.([16](#S4.E16 "Equation 16 ‣ 4.1 Inference ‣ 4 Training Objectives ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")).'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：MS-COCO 上的结果。 (a) token 数量的影响分析。 (b) 全球和本地提示间权重的影响分析，即 $\lambda_{2}$ 在公式
    ([16](#S4.E16 "Equation 16 ‣ 4.1 Inference ‣ 4 Training Objectives ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning")) 中的权重。
- en: Weights of global and local prompts. To evaluate the contributions of the global
    and local prompts to the final performance, we conducted experiments with varied
    weights assigned to each branch, as depicted in Figure [4](#S5.F4 "Figure 4 ‣
    5.4 Parameter Analysis ‣ 5 Experiments ‣ Data-free Multi-label Image Recognition
    via LLM-powered Prompt Tuning") (b). We observe that as the weight allocated to
    the global prompts increases, the performance initially rises, peaking at a weight
    of 0.65, then gradually declines. This trend demonstrates that the global branch
    plays a more critical role, but the local branch is also necessary.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 全球和本地提示的权重。为了评估全球和本地提示对最终性能的贡献，我们进行了分配给每个分支的不同权重的实验，如图 [4](#S5.F4 "Figure 4
    ‣ 5.4 Parameter Analysis ‣ 5 Experiments ‣ Data-free Multi-label Image Recognition
    via LLM-powered Prompt Tuning") (b) 所示。我们观察到，随着分配给全球提示的权重增加，性能最初上升，在权重为 0.65 时达到峰值，然后逐渐下降。这一趋势表明全球分支起着更为关键的作用，但本地分支也是必要的。
- en: '![Refer to caption](img/72aed0ff7e51b2f3773084f3d2c569b4.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/72aed0ff7e51b2f3773084f3d2c569b4.png)'
- en: 'Figure 5: Visualization of top-3 predicated categories by different prompts.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：不同提示下的前 3 类预测结果的可视化。
- en: 5.5 Comparison with State-of-the-art Methods
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 与最先进方法的比较
- en: We compare the proposed method with several state-of-the-art methods at different
    annotations levels of training images, including fully labeled methods (SRN [[64](#bib.bib64)],
    ML-GCN [[12](#bib.bib12)], and ASL [[38](#bib.bib38)]), partially labeled methods
    (SARB [[36](#bib.bib36)], SST [[11](#bib.bib11)], and DualCoOp [[43](#bib.bib43)]),
    one labeled methods (LL-R [[27](#bib.bib27)] and G²NetPL  [[1](#bib.bib1)]), and
    unlabeled methods (LSAN [[45](#bib.bib45)], WAN [[34](#bib.bib34)], Curriculum [[15](#bib.bib15)],
    and Naive AN [[28](#bib.bib28)]). We also compare our method with a recent method
    TaI-DPT [[20](#bib.bib20)] that uses image captions for training.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所提方法与多种最先进的方法进行了比较，涵盖了不同注释级别的训练图像，包括完全标注的方法（SRN [[64](#bib.bib64]、ML-GCN [[12](#bib.bib12]
    和 ASL [[38](#bib.bib38]）、部分标注的方法（SARB [[36](#bib.bib36]、SST [[11](#bib.bib11]
    和 DualCoOp [[43](#bib.bib43]）、单一标注的方法（LL-R [[27](#bib.bib27] 和 G²NetPL [[1](#bib.bib1]）、以及未标注的方法（LSAN [[45](#bib.bib45]、WAN [[34](#bib.bib34]、Curriculum [[15](#bib.bib15]
    和 Naive AN [[28](#bib.bib28]）。我们还将我们的方法与最近的一种方法 TaI-DPT [[20](#bib.bib20] 进行了比较，该方法使用图像标题进行训练。
- en: 'Table [5](#S5.T5 "Table 5 ‣ 5.3 Ablation Studies ‣ 5 Experiments ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning") shows the comparison
    results on VOC2007, MS-COCO, and NUS-WIDE. We have observations as follows: (1)
    Our method outperforms all the unsupervised methods using unlabeled training images,
    underscoring the superiority of comprehensive knowledge of objects stored in ChatGLM;
    (2) Our method exhibits a slight performance advantage over TaI-DPT, which is
    trained on human-written image captions. This result suggests that ChatGLM has
    the ability to emulate human understanding, further validating the effectiveness
    of our method; (3) The performance of our method significantly drops compared
    to the fully labeled methods, probably due to the domain gap between the training
    data of CLIP and the target data of specific task.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [5](#S5.T5 "表 5 ‣ 5.3 消融研究 ‣ 5 实验 ‣ 无数据多标签图像识别通过 LLM 驱动的提示调整") 显示了 VOC2007、MS-COCO
    和 NUS-WIDE 上的比较结果。我们得出了以下观察： (1) 我们的方法在使用未标注训练图像的所有无监督方法中表现最佳，凸显了 ChatGLM 中存储的全面对象知识的优越性；
    (2) 我们的方法相比于使用人类编写的图像标题进行训练的 TaI-DPT 稍有性能优势。这一结果表明 ChatGLM 具备模拟人类理解的能力，进一步验证了我们方法的有效性；
    (3) 与完全标注的方法相比，我们的方法性能显著下降，这可能由于 CLIP 的训练数据与特定任务目标数据之间的领域差距。
- en: 5.6 Qualitative Analysis
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 定性分析
- en: Figure [5](#S5.F5 "Figure 5 ‣ 5.4 Parameter Analysis ‣ 5 Experiments ‣ Data-free
    Multi-label Image Recognition via LLM-powered Prompt Tuning") illustrates the
    top-3 category predictions of different prompts. Notably, the hierarchical prompts
    achieve better performance, especially on smaller objects, as shown in (a), (b),
    and (c). However, as depicted in (b), an instance of incorrect top-1 prediction
    arises in the category labeled “train”, likely due to an excessive emphasis on
    global image features, resembling a train station. Conversely, as shown in (d),
    the hand-craft prompts demonstrate superior performance, probably due to the meticulous
    design of hand-craft prompts integrating certain human prior knowledge. For instance,
    when designing the prompt for the “mouse” category, we use the term “computer
    mouse”, aligning more closely with its contextual usage to improve the performance.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S5.F5 "图 5 ‣ 5.4 参数分析 ‣ 5 实验 ‣ 无数据多标签图像识别通过 LLM 驱动的提示调整") 展示了不同提示的前三类预测。特别地，分层提示在较小的对象上表现更好，如（a）、（b）和（c）所示。然而，如（b）所示，在标记为“train”的类别中出现了错误的
    top-1 预测实例，这可能是由于对全球图像特征的过度强调，类似于火车站。相反，如（d）所示，手工设计的提示表现优越，这可能是由于手工提示整合了某些人类先验知识。例如，在设计“mouse”类别的提示时，我们使用了“computer
    mouse”这一术语，更贴近其语境使用，从而提升了性能。
- en: 6 Conclusion
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We have presented a novel data-free framework for multi-label image recognition,
    which leverages enriched text descriptions powered by LLMs such as ChatGLM to
    well adapt VLMs like CLIP through prompt tuning. By first querying ChatGLM with
    well-designed questions and then learning hierarchical prompts with contextual
    relationships between categories, our method successfully achieves promising results
    without any training data, which is evaluated by extensive experiments on three
    benchmark datasets. Our method provides an effective way to explore the synergies
    between multiple pre-trained models for visual recognition under data scarcity.
    In the future, we are going to apply the proposed data-free framework to more
    computer vision tasks such as action recognition in videos.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种新颖的无数据框架用于多标签图像识别，该框架利用由LLMs如ChatGLM提供的丰富文本描述，通过提示调整有效地适应VLMs如CLIP。通过首先用精心设计的问题查询ChatGLM，然后学习具有类别之间上下文关系的层次化提示，我们的方法在没有任何训练数据的情况下成功地取得了令人鼓舞的结果，这通过在三个基准数据集上的广泛实验得到了验证。我们的方法为在数据稀缺情况下探索多预训练模型之间的协同效应提供了一种有效的方式。未来，我们将把提出的无数据框架应用于更多计算机视觉任务，例如视频中的动作识别。
- en: References
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Abdelfattah et al. [2022a] Rabab Abdelfattah, Xin Zhang, Mostafa M Fouda, Xiaofeng
    Wang, and Song Wang. G2netpl: Generic game-theoretic network for partial-label
    image classification. *BMVC*, 2022a.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '阿卜杜勒法塔赫等人 [2022a] 拉巴布·阿卜杜勒法塔赫、辛·张、穆斯塔法·M·福达、肖峰·王、宋·王。G2netpl: 泛化博弈理论网络用于部分标签图像分类。*BMVC*，2022年。'
- en: 'Abdelfattah et al. [2022b] Rabab Abdelfattah, Xin Zhang, Zhenyao Wu, Xinyi
    Wu, Xiaofeng Wang, and Song Wang. Plmcl: Partial-label momentum curriculum learning
    for multi-label image classification. In *ECCV*, pages 39–55\. Springer, 2022b.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '阿卜杜勒法塔赫等人 [2022b] 拉巴布·阿卜杜勒法塔赫、辛·张、振耀·吴、新怡·吴、肖峰·王、宋·王。Plmcl: 用于多标签图像分类的部分标签动量课程学习。载于
    *ECCV*，第39–55页。Springer，2022年。'
- en: 'Abdelfattah et al. [2023] Rabab Abdelfattah, Qing Guo, Xiaoguang Li, Xiaofeng
    Wang, and Song Wang. Cdul: Clip-driven unsupervised learning for multi-label image
    classification. In *ICCV*, pages 1348–1357, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '阿卜杜勒法塔赫等人 [2023] 拉巴布·阿卜杜勒法塔赫、青·郭、肖光·李、肖峰·王、宋·王。Cdul: 基于Clip的无监督多标签图像分类学习。载于
    *ICCV*，第1348–1357页，2023年。'
- en: 'Alfassy et al. [2019] Amit Alfassy, Leonid Karlinsky, Amit Aides, Joseph Shtok,
    Sivan Harary, Rogerio Feris, Raja Giryes, and Alex M Bronstein. Laso: Label-set
    operations networks for multi-label few-shot learning. In *CVPR*, pages 6548–6557,
    2019.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '阿尔法西等人 [2019] 阿米特·阿尔法西、列昂尼德·卡尔林斯基、阿米特·艾德斯、约瑟夫·什托克、西万·哈拉里、罗杰里奥·费里斯、拉贾·吉耶斯、亚历克斯·M·布朗斯坦。Laso:
    用于多标签少样本学习的标签集操作网络。载于 *CVPR*，第6548–6557页，2019年。'
- en: Ben-Cohen et al. [2021] Avi Ben-Cohen, Nadav Zamir, Emanuel Ben-Baruch, Itamar
    Friedman, and Lihi Zelnik-Manor. Semantic diversity learning for zero-shot multi-label
    classification. In *ICCV*, pages 640–650, 2021.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本-科恩等人 [2021] 阿维·本-科恩、纳达夫·扎米尔、伊曼纽尔·本-巴鲁赫、伊塔玛尔·弗里德曼、利希·泽尔尼克-曼诺。零样本多标签分类的语义多样性学习。载于
    *ICCV*，第640–650页，2021年。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. *NeurIPS*, 33:1877–1901,
    2020.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 布朗等人 [2020] 汤姆·布朗、本杰明·曼、尼克·赖德、梅拉尼·萨比亚、贾里德·D·卡普兰、普拉夫拉·达里瓦尔、阿尔温德·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔等。语言模型是少样本学习者。*NeurIPS*，33:1877–1901，2020年。
- en: 'Bubeck et al. [2023] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes
    Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg,
    et al. Sparks of artificial general intelligence: Early experiments with gpt-4.
    *arXiv preprint arXiv:2303.12712*, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '布贝克等人 [2023] 塞巴斯蒂安·布贝克、瓦伦·钱德拉塞卡兰、罗嫩·艾尔丹、约翰内斯·格尔克、埃里克·霍维茨、埃切·卡马尔、彼得·李、尹·塔特·李、袁智·李、斯科特·伦德伯格等。人工通用智能的火花:
    GPT-4的早期实验。*arXiv预印本 arXiv:2303.12712*，2023年。'
- en: 'Cai et al. [2023] Yuzhe Cai, Shaoguang Mao, Wenshan Wu, Zehua Wang, Yaobo Liang,
    Tao Ge, Chenfei Wu, Wang You, Ting Song, Yan Xia, et al. Low-code llm: Visual
    programming over llms. *arXiv preprint arXiv:2304.08103*, 2023.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '蔡等人 [2023] 玉哲·蔡、邵光·毛、文杉·吴、泽华·王、耀博·梁、陶·戈、陈飞·吴、王游、廷·宋、燕·夏等。低代码llm: 基于llms的视觉编程。*arXiv预印本
    arXiv:2304.08103*，2023年。'
- en: 'Chen et al. [2023] Guangyi Chen, Xiao Liu, Guangrun Wang, Kun Zhang, Philip HS
    Torr, Xiao-Ping Zhang, and Yansong Tang. Tem-adapter: Adapting image-text pretraining
    for video question answer. In *Proceedings of the IEEE/CVF International Conference
    on Computer Vision*, pages 13945–13955, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '陈等人 [2023] 光义·陈、肖·刘、光润·王、昆·张、菲利普·HS·托尔、肖平·张、燕松·唐。Tem-adapter: 适应图像-文本预训练以进行视频问答。载于
    *IEEE/CVF国际计算机视觉会议论文集*，第13945–13955页，2023年。'
- en: Chen et al. [2019a] Tianshui Chen, Muxin Xu, Xiaolu Hui, Hefeng Wu, and Liang
    Lin. Learning semantic-specific graph representation for multi-label image recognition.
    In *ICCV*, pages 522–531, 2019a.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2019a] 田水陈、木欣·徐、晓璐·惠、赫峰吴和梁林。用于多标签图像识别的语义特定图表示学习。发表于*ICCV*，页码 522–531，2019a年。
- en: Chen et al. [2022] Tianshui Chen, Tao Pu, Hefeng Wu, Yuan Xie, and Liang Lin.
    Structured semantic transfer for multi-label recognition with partial labels.
    In *AAAI*, pages 339–346, 2022.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2022] 田水陈、陶璞、赫峰吴、袁谢、梁林。用于多标签识别的结构化语义转移，部分标签。发表于*AAAI*，页码 339–346，2022年。
- en: Chen et al. [2019b] Zhao-Min Chen, Xiu-Shen Wei, Peng Wang, and Yanwen Guo.
    Multi-label image recognition with graph convolutional networks. In *CVPR*, pages
    5177–5186, 2019b.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等人 [2019b] 赵敏·陈、修身·魏、彭·王和艳文·郭。基于图卷积网络的多标签图像识别。发表于*CVPR*，页码 5177–5186，2019b年。
- en: 'Chua et al. [2009] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping
    Luo, and Yantao Zheng. Nus-wide: a real-world web image database from national
    university of singapore. In *Proceedings of the ACM international conference on
    image and video retrieval*, pages 1–9, 2009.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蔡等人 [2009] 蔡达胜、金辉·唐、瑞昌·洪、浩杰·李、志平·罗和燕涛·郑。NUS-WIDE：来自新加坡国立大学的真实世界网页图像数据库。发表于*ACM国际图像与视频检索会议论文集*，页码
    1–9，2009年。
- en: 'Du et al. [2022] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu,
    Zhilin Yang, and Jie Tang. Glm: General language model pretraining with autoregressive
    blank infilling. In *ACL*, pages 320–335, 2022.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等人 [2022] 郑晓·杜、于杰·钱、肖·刘、明丁、杰中·邱、志林·杨和杰·唐。GLM：具有自回归空白填充的通用语言模型预训练。发表于*ACL*，页码
    320–335，2022年。
- en: Durand et al. [2019] Thibaut Durand, Nazanin Mehrasa, and Greg Mori. Learning
    a deep convnet for multi-label classification with partial labels. In *CVPR*,
    pages 647–657, 2019.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 杜等人 [2019] 提博·杜兰、纳扎宁·梅赫拉萨和格雷格·莫里。学习一个用于多标签分类的深度卷积网络，具有部分标签。发表于*CVPR*，页码 647–657，2019年。
- en: Everingham et al. [2010] Mark Everingham, Luc Van Gool, Christopher KI Williams,
    John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge.
    *IJCV*, 88:303–338, 2010.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 埃弗宁等人 [2010] 马克·埃弗宁、卢克·范·戈尔、克里斯托弗·K·I·威廉姆斯、约翰·温和安德鲁·齐瑟曼。帕斯卡视觉对象类别（VOC）挑战。*IJCV*，88:303–338，2010年。
- en: Gao and Zhou [2020] Bin-Bin Gao and Hong-Yu Zhou. Multi-label image recognition
    with multi-class attentional regions. *arXiv e-prints*, pages arXiv–2007, 2020.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高和周 [2020] 宾宾高和洪宇周。具有多类注意区域的多标签图像识别。*arXiv 预印本*，页码 arXiv–2007，2020年。
- en: 'Gao et al. [2023] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang,
    Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language
    models with feature adapters. *IJCV*, pages 1–15, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等人 [2023] 彭·高、史杰·耿、任瑞·张、特力·马、荣耀·方、永峰·张、洪生·李和玉桥。CLIP-ADAPTER：具有特征适配器的更好的视觉-语言模型。*IJCV*，页码
    1–15，2023年。
- en: Gong et al. [2013] Yunchao Gong, Yangqing Jia, Thomas Leung, Alexander Toshev,
    and Sergey Ioffe. Deep convolutional ranking for multilabel image annotation.
    *arXiv preprint arXiv:1312.4894*, 2013.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 龚等人 [2013] 云超·龚、杨青·贾、托马斯·梁、亚历山大·托谢夫和谢尔盖·伊奥夫。深度卷积排序用于多标签图像注释。*arXiv 预印本 arXiv:1312.4894*，2013年。
- en: Guo et al. [2023a] Zixian Guo, Bowen Dong, Zhilong Ji, Jinfeng Bai, Yiwen Guo,
    and Wangmeng Zuo. Texts as images in prompt tuning for multi-label image recognition.
    In *CVPR*, pages 2808–2817, 2023a.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等人 [2023a] 资贤·郭、博文·董、志龙·季、金锋·白、易文·郭和望萌·左。作为图像的文本在多标签图像识别的提示调优中。发表于*CVPR*，页码
    2808–2817，2023a年。
- en: 'Guo et al. [2023b] Ziyu Guo, Renrui Zhang, Longtian Qiu, Xianzheng Ma, Xupeng
    Miao, Xuming He, and Bin Cui. Calip: Zero-shot enhancement of clip with parameter-free
    attention. In *AAAI*, pages 746–754, 2023b.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郭等人 [2023b] 自愚·郭、任瑞·张、龙天·邱、鲜正·马、旭鹏·苗、许铭·贺和彬·崔。CALIP：无参数注意力的CLIP零样本增强。发表于*AAAI*，页码
    746–754，2023b年。
- en: 'Gupta and Kembhavi [2023] Tanmay Gupta and Aniruddha Kembhavi. Visual programming:
    Compositional visual reasoning without training. In *CVPR*, pages 14953–14962,
    2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 古普塔和肯巴维 [2023] 坦梅·古普塔和阿尼鲁德哈·肯巴维。视觉编程：无需训练的组合视觉推理。发表于*CVPR*，页码 14953–14962，2023年。
- en: He et al. [2018] Shiyi He, Chang Xu, Tianyu Guo, Chao Xu, and Dacheng Tao. Reinforced
    multi-label image classification by exploring curriculum. In *AAAI*, 2018.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何等人 [2018] 施毅·何、常旭、田宇·郭、超旭和达成·陶。通过探索课程强化的多标签图像分类。发表于*AAAI*，2018年。
- en: Huynh and Elhamifar [2020] Dat Huynh and Ehsan Elhamifar. A shared multi-attention
    framework for multi-label zero-shot learning. In *CVPR*, pages 8776–8786, 2020.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄和艾哈米法 [2020] 达特·黄和埃赫桑·艾哈米法。用于多标签零样本学习的共享多注意力框架。发表于*CVPR*，页码 8776–8786，2020年。
- en: Ji et al. [2020] Zhong Ji, Biying Cui, Huihui Li, Yu-Gang Jiang, Tao Xiang,
    Timothy Hospedales, and Yanwei Fu. Deep ranking for image zero-shot multi-label
    classification. *IEEE TIP*, 29:6549–6560, 2020.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ji 等 [2020] Zhong Ji、Biying Cui、Huihui Li、Yu-Gang Jiang、Tao Xiang、Timothy Hospedales
    和 Yanwei Fu。用于图像零样本多标签分类的深度排序。*IEEE TIP*，29:6549–6560，2020年。
- en: Jia et al. [2022] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge
    Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In *ECCV*,
    pages 709–727\. Springer, 2022.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia 等 [2022] Menglin Jia、Luming Tang、Bor-Chun Chen、Claire Cardie、Serge Belongie、Bharath
    Hariharan 和 Ser-Nam Lim。视觉提示调优。发表于 *ECCV*，页码 709–727。Springer，2022年。
- en: Kim et al. [2022] Youngwook Kim, Jae Myung Kim, Zeynep Akata, and Jungwoo Lee.
    Large loss matters in weakly supervised multi-label classification. In *CVPR*,
    pages 14156–14165, 2022.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 [2022] Youngwook Kim、Jae Myung Kim、Zeynep Akata 和 Jungwoo Lee。弱监督多标签分类中的大损失问题。发表于
    *CVPR*，页码 14156–14165，2022年。
- en: Kundu and Tighe [2020] Kaustav Kundu and Joseph Tighe. Exploiting weakly supervised
    visual patterns to learn from partial annotations. *NeurIPS*, 33:561–572, 2020.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kundu 和 Tighe [2020] Kaustav Kundu 和 Joseph Tighe。利用弱监督视觉模式从部分标注中学习。*NeurIPS*，33:561–572，2020年。
- en: Lee et al. [2018] Chung-Wei Lee, Wei Fang, Chih-Kuan Yeh, and Yu-Chiang Frank
    Wang. Multi-label zero-shot learning with structured knowledge graphs. In *CVPR*,
    pages 1576–1585, 2018.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等 [2018] Chung-Wei Lee、Wei Fang、Chih-Kuan Yeh 和 Yu-Chiang Frank Wang。基于结构化知识图谱的多标签零样本学习。发表于
    *CVPR*，页码 1576–1585，2018年。
- en: Li et al. [2017] Yuncheng Li, Yale Song, and Jiebo Luo. Improving pairwise ranking
    for multi-label image classification. In *CVPR*, pages 3617–3625, 2017.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等 [2017] Yuncheng Li、Yale Song 和 Jiebo Luo。提高多标签图像分类的成对排序。发表于 *CVPR*，页码 3617–3625，2017年。
- en: 'Lin et al. [2014] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,
    Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco:
    Common objects in context. In *ECCV*, pages 740–755\. Springer, 2014.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等 [2014] Tsung-Yi Lin、Michael Maire、Serge Belongie、James Hays、Pietro Perona、Deva
    Ramanan、Piotr Dollár 和 C Lawrence Zitnick。微软 COCO：上下文中的常见物体。发表于 *ECCV*，页码 740–755。Springer，2014年。
- en: Liu et al. [2017] Feng Liu, Tao Xiang, Timothy M Hospedales, Wankou Yang, and
    Changyin Sun. Semantic regularisation for recurrent image annotation. In *CVPR*,
    pages 2872–2880, 2017.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2017] Feng Liu、Tao Xiang、Timothy M Hospedales、Wankou Yang 和 Changyin
    Sun。用于递归图像注释的语义正则化。发表于 *CVPR*，页码 2872–2880，2017年。
- en: Liu and Tsang [2015] Weiwei Liu and Ivor Tsang. On the optimality of classifier
    chain for multi-label classification. *NeurIPS*, 28, 2015.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Tsang [2015] Weiwei Liu 和 Ivor Tsang。分类器链在多标签分类中的最优性。*NeurIPS*，28，2015年。
- en: Mac Aodha et al. [2019] Oisin Mac Aodha, Elijah Cole, and Pietro Perona. Presence-only
    geographical priors for fine-grained image classification. In *ICCV*, pages 9596–9606,
    2019.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mac Aodha 等 [2019] Oisin Mac Aodha、Elijah Cole 和 Pietro Perona。用于细粒度图像分类的仅存在地理先验。发表于
    *ICCV*，页码 9596–9606，2019年。
- en: 'Misra et al. [2016] Ishan Misra, C Lawrence Zitnick, Margaret Mitchell, and
    Ross Girshick. Seeing through the human reporting bias: Visual classifiers from
    noisy human-centric labels. In *CVPR*, pages 2930–2939, 2016.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Misra 等 [2016] Ishan Misra、C Lawrence Zitnick、Margaret Mitchell 和 Ross Girshick。看透人类报告偏差：来自噪声人类中心标签的视觉分类器。发表于
    *CVPR*，页码 2930–2939，2016年。
- en: Pu et al. [2022] Tao Pu, Tianshui Chen, Hefeng Wu, and Liang Lin. Semantic-aware
    representation blending for multi-label image recognition with partial labels.
    In *AAAI*, pages 2091–2098, 2022.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pu 等 [2022] Tao Pu、Tianshui Chen、Hefeng Wu 和 Liang Lin。具有部分标签的多标签图像识别的语义感知表示混合。发表于
    *AAAI*，页码 2091–2098，2022年。
- en: Radford et al. [2021] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. Learning transferable visual models from natural language supervision.
    In *International conference on machine learning*, pages 8748–8763\. PMLR, 2021.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 [2021] Alec Radford、Jong Wook Kim、Chris Hallacy、Aditya Ramesh、Gabriel
    Goh、Sandhini Agarwal、Girish Sastry、Amanda Askell、Pamela Mishkin、Jack Clark 等。通过自然语言监督学习可转移的视觉模型。发表于
    *International conference on machine learning*，页码 8748–8763。PMLR，2021年。
- en: Ridnik et al. [2021] Tal Ridnik, Emanuel Ben-Baruch, Nadav Zamir, Asaf Noy,
    Itamar Friedman, Matan Protter, and Lihi Zelnik-Manor. Asymmetric loss for multi-label
    classification. In *ICCV*, pages 82–91, 2021.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ridnik 等 [2021] Tal Ridnik、Emanuel Ben-Baruch、Nadav Zamir、Asaf Noy、Itamar Friedman、Matan
    Protter 和 Lihi Zelnik-Manor。用于多标签分类的非对称损失。发表于 *ICCV*，页码 82–91，2021年。
- en: Sarafianos et al. [2018] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
    Deep imbalanced attribute classification using visual attention aggregation. In
    *ECCV*, pages 680–697, 2018.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sarafianos 等 [2018] Nikolaos Sarafianos、Xiang Xu 和 Ioannis A Kakadiaris。使用视觉注意力聚合的深度不平衡属性分类。发表于
    *ECCV*，页码 680–697，2018年。
- en: Simon et al. [2022] Christian Simon, Piotr Koniusz, and Mehrtash Harandi. Meta-learning
    for multi-label few-shot classification. In *WACV*, pages 3951–3960, 2022.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simon et al. [2022] Christian Simon、Piotr Koniusz、和Mehrtash Harandi。用于多标签小样本分类的元学习。发表于*WACV*，第3951–3960页，2022年。
- en: 'Singha et al. [2023] Mainak Singha, Harsh Pal, Ankit Jha, and Biplab Banerjee.
    Ad-clip: Adapting domains in prompt space using clip. In *ICCV*, pages 4355–4364,
    2023.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singha et al. [2023] Mainak Singha、Harsh Pal、Ankit Jha、以及Biplab Banerjee。Ad-clip：使用clip在提示空间中适应领域。发表于*ICCV*，第4355–4364页，2023年。
- en: Sohn et al. [2023] Kihyuk Sohn, Huiwen Chang, José Lezama, Luisa Polania, Han
    Zhang, Yuan Hao, Irfan Essa, and Lu Jiang. Visual prompt tuning for generative
    transfer learning. In *CVPR*, pages 19840–19851, 2023.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sohn et al. [2023] Kihyuk Sohn、Huiwen Chang、José Lezama、Luisa Polania、Han Zhang、Yuan
    Hao、Irfan Essa、和Lu Jiang。用于生成转移学习的视觉提示调整。发表于*CVPR*，第19840–19851页，2023年。
- en: 'Sun et al. [2022] Ximeng Sun, Ping Hu, and Kate Saenko. Dualcoop: Fast adaptation
    to multi-label recognition with limited annotations. *NeurIPS*, 35:30569–30582,
    2022.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2022] 西蒙 Sun、Ping Hu、和Kate Saenko。Dualcoop：快速适应有限标注的多标签识别。*NeurIPS*，35:30569–30582，2022年。
- en: 'Sung et al. [2022] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Vl-adapter: Parameter-efficient
    transfer learning for vision-and-language tasks. In *CVPR*, pages 5227–5237, 2022.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung et al. [2022] Yi-Lin Sung、Jaemin Cho、和Mohit Bansal。Vl-adapter：用于视觉和语言任务的参数高效转移学习。发表于*CVPR*，第5227–5237页，2022年。
- en: Szegedy et al. [2016] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
    Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer
    vision. In *CVPR*, pages 2818–2826, 2016.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy et al. [2016] Christian Szegedy、Vincent Vanhoucke、Sergey Ioffe、Jon Shlens、和Zbigniew
    Wojna。重新思考计算机视觉中的inception架构。发表于*CVPR*，第2818–2826页，2016年。
- en: 'Upadhyay et al. [2023] Uddeshya Upadhyay, Shyamgopal Karthik, Massimiliano
    Mancini, and Zeynep Akata. Probvlm: Probabilistic adapter for frozen vison-language
    models. In *ICCV*, pages 1899–1910, 2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Upadhyay et al. [2023] Uddeshya Upadhyay、Shyamgopal Karthik、Massimiliano Mancini、和Zeynep
    Akata。Probvlm：用于冻结视觉语言模型的概率适配器。发表于*ICCV*，第1899–1910页，2023年。
- en: 'Wang et al. [2016] Jiang Wang, Yi Yang, Junhua Mao, Zhiheng Huang, Chang Huang,
    and Wei Xu. Cnn-rnn: A unified framework for multi-label image classification.
    In *CVPR*, pages 2285–2294, 2016.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2016] 江 Wang、Yi Yang、Junhua Mao、志恒 黄、昌 黄、和伟 Xu。Cnn-rnn：用于多标签图像分类的统一框架。发表于*CVPR*，第2285–2294页，2016年。
- en: Wang et al. [2020] Ya Wang, Dongliang He, Fu Li, Xiang Long, Zhichao Zhou, Jinwen
    Ma, and Shilei Wen. Multi-label classification with label graph superimposing.
    In *AAAI*, pages 12265–12272, 2020.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2020] 牙王、董亮 贺、富 李、向 龙、志超 周、金文 马、以及世磊 温。通过标签图叠加进行多标签分类。发表于*AAAI*，第12265–12272页，2020年。
- en: Wang et al. [2017] Zhouxia Wang, Tianshui Chen, Guanbin Li, Ruijia Xu, and Liang
    Lin. Multi-label image recognition by recurrently discovering attentional regions.
    In *ICCV*, pages 464–472, 2017.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. [2017] 周夏 Wang、天水 Chen、冠彬 Li、瑞佳 Xu、和梁 Lin。通过反复发现注意区域进行多标签图像识别。发表于*ICCV*，第464–472页，2017年。
- en: Wei et al. [2022] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph,
    Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler,
    Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William
    Fedus. Emergent abilities of large language models. *Transactions on Machine Learning
    Research*, 2022. Survey Certification.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2022] Jason Wei、Yi Tay、Rishi Bommasani、Colin Raffel、Barret Zoph、Sebastian
    Borgeaud、Dani Yogatama、Maarten Bosma、Denny Zhou、Donald Metzler、Ed H. Chi、Tatsunori
    Hashimoto、Oriol Vinyals、Percy Liang、Jeff Dean、和William Fedus。大语言模型的突现能力。*Transactions
    on Machine Learning Research*，2022年。调查认证。
- en: 'Wei et al. [2015] Yunchao Wei, Wei Xia, Min Lin, Junshi Huang, Bingbing Ni,
    Jian Dong, Yao Zhao, and Shuicheng Yan. Hcp: A flexible cnn framework for multi-label
    image classification. *IEEE TPAMI*, 38(9):1901–1907, 2015.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2015] 云超 Wei、魏 Xia、敏 Lin、俊石 Huang、冰冰 Ni、建 Dong、姚 Zhao、和水城 Yan。Hcp：用于多标签图像分类的灵活cnn框架。*IEEE
    TPAMI*，38(9):1901–1907，2015年。
- en: Xu et al. [2023] Mengde Xu, Zheng Zhang, Fangyun Wei, Han Hu, and Xiang Bai.
    Side adapter network for open-vocabulary semantic segmentation. In *CVPR*, pages
    2945–2954, 2023.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023] Mengde Xu、郑 张、方云 韦、Han Hu、和向 Bai。用于开放词汇语义分割的侧适配器网络。发表于*CVPR*，第2945–2954页，2023年。
- en: 'Yang et al. [2023] Jianing Yang, Xuweiyi Chen, Shengyi Qian, Nikhil Madaan,
    Madhavan Iyengar, David F Fouhey, and Joyce Chai. Llm-grounder: Open-vocabulary
    3d visual grounding with large language model as an agent. *arXiv preprint arXiv:2309.12311*,
    2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2023] Jianing Yang、Xuweiyi Chen、Shengyi Qian、Nikhil Madaan、Madhavan
    Iyengar、David F Fouhey、和Joyce Chai。Llm-grounder：使用大型语言模型作为代理的开放词汇3D视觉定位。*arXiv
    preprint arXiv:2309.12311*，2023年。
- en: Yang et al. [2022] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao
    Lu, Zicheng Liu, and Lijuan Wang. An empirical study of gpt-3 for few-shot knowledge-based
    vqa. In *AAAI*, pages 3081–3089, 2022.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. [2022] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao
    Lu, Zicheng Liu, 和 Lijuan Wang. 对GPT-3在少样本知识问答中的应用的实证研究。在*AAAI*，第3081–3089页，2022年。
- en: 'Yao et al. [2021] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng
    Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language
    models. *arXiv preprint arXiv:2109.11797*, 2021.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. [2021] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng
    Chua, 和 Maosong Sun. Cpt：为预训练视觉-语言模型进行彩色提示调优。*arXiv preprint arXiv:2109.11797*，2021年。
- en: Yazici et al. [2020] Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa,
    Bartlomiej Twardowski, and Joost van de Weijer. Orderless recurrent models for
    multi-label classification. In *CVPR*, pages 13440–13449, 2020.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yazici et al. [2020] Vacit Oguz Yazici, Abel Gonzalez-Garcia, Arnau Ramisa,
    Bartlomiej Twardowski, 和 Joost van de Weijer. 无序递归模型用于多标签分类。在*CVPR*，第13440–13449页，2020年。
- en: Ye et al. [2020] Jin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, and Yu Qiao.
    Attention-driven dynamic graph convolutional network for multi-label image recognition.
    In *ECCV*, pages 649–665\. Springer, 2020.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ye et al. [2020] Jin Ye, Junjun He, Xiaojiang Peng, Wenhao Wu, 和 Yu Qiao. 基于注意力驱动的动态图卷积网络用于多标签图像识别。在*ECCV*，第649–665页，Springer，2020年。
- en: Zhang et al. [2018] Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, and Jianfeng
    Lu. Multilabel image classification with regional latent semantic dependencies.
    *IEEE TMM*, 20(10):2801–2813, 2018.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2018] Junjie Zhang, Qi Wu, Chunhua Shen, Jian Zhang, 和 Jianfeng
    Lu. 具有区域潜在语义依赖的多标签图像分类。*IEEE TMM*，20(10):2801–2813，2018年。
- en: 'Zhang et al. [2022] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang
    Li, Jifeng Dai, Yu Qiao, and Hongsheng Li. Tip-adapter: Training-free adaption
    of clip for few-shot classification. In *ECCV*, pages 493–510\. Springer, 2022.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2022] Renrui Zhang, Wei Zhang, Rongyao Fang, Peng Gao, Kunchang
    Li, Jifeng Dai, Yu Qiao, 和 Hongsheng Li. Tip-adapter：训练免费适配CLIP用于少样本分类。在*ECCV*，第493–510页，Springer，2022年。
- en: 'Zhang et al. [2023] Wenqiao Zhang, Changshuo Liu, Lingze Zeng, Bengchin Ooi,
    Siliang Tang, and Yueting Zhuang. Learning in imperfect environment: Multi-label
    classification with long-tailed distribution and partial labels. In *ICCV*, pages
    1423–1432, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. [2023] Wenqiao Zhang, Changshuo Liu, Lingze Zeng, Bengchin Ooi,
    Siliang Tang, 和 Yueting Zhuang. 在不完美环境中的学习：具有长尾分布和部分标签的多标签分类。在*ICCV*，第1423–1432页，2023年。
- en: Zhou et al. [2022a] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
    Liu. Conditional prompt learning for vision-language models. In *CVPR*, pages
    16816–16825, 2022a.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2022a] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, 和 Ziwei Liu.
    视觉-语言模型的条件提示学习。在*CVPR*，第16816–16825页，2022a年。
- en: Zhou et al. [2022b] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei
    Liu. Learning to prompt for vision-language models. *IJCV*, 130(9):2337–2348,
    2022b.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. [2022b] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, 和 Ziwei Liu.
    为视觉-语言模型学习提示。在*IJCV*，130(9):2337–2348，2022b年。
- en: Zhu et al. [2023] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, and Hanwang Zhang.
    Prompt-aligned gradient for prompt tuning. In *ICCV*, pages 15659–15669, 2023.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2023] Beier Zhu, Yulei Niu, Yucheng Han, Yue Wu, 和 Hanwang Zhang.
    提示对齐梯度用于提示调优。在*ICCV*，第15659–15669页，2023年。
- en: Zhu et al. [2017] Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, and Xiaogang
    Wang. Learning spatial regularization with image-level supervisions for multi-label
    image classification. In *CVPR*, pages 5513–5522, 2017.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. [2017] Feng Zhu, Hongsheng Li, Wanli Ouyang, Nenghai Yu, 和 Xiaogang
    Wang. 利用图像级监督学习空间正则化用于多标签图像分类。在*CVPR*，第5513–5522页，2017年。
