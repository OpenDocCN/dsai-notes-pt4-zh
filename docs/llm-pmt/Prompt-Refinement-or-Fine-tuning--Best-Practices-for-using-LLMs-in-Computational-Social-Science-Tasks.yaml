- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-08 18:41:15'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: Prompt Refinement or Fine-tuning? Best Practices for using LLMs in Computational
    Social Science Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2408.01346](https://ar5iv.labs.arxiv.org/html/2408.01346)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Anders Giovanni Møller¹, Luca Maria Aiello^(1,2)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Large Language Models are expressive tools that enable complex tasks of text
    understanding within Computational Social Science. Their versatility, while beneficial,
    poses a barrier for establishing standardized best practices within the field.
    To bring clarity on the values of different strategies, we present an overview
    of the performance of modern LLM-based classification methods on a benchmark of
    23 social knowledge tasks. Our results point to three best practices: select models
    with larger vocabulary and pre-training corpora; avoid simple zero-shot in favor
    of AI-enhanced prompting; fine-tune on task-specific data, and consider more complex
    forms instruction-tuning on multiple datasets only when only training data is
    more abundant.'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The release of ChatGPT in November 2022 has sparked broad interest for Large
    Language Models (LLMs) due to their capability to solve complex tasks of text
    understanding and generation (Bubeck and ohers [2023](#bib.bib4)). The Computational
    Social Science (CSS) community has rapidly recognized the potential of LLMs as
    tools for capturing textual dimensions of semantics and pragmatics – crucial elements
    of online discourse that have traditionally been challenging to quantify (Bail
    [2024](#bib.bib1)).
  prefs: []
  type: TYPE_NORMAL
- en: This new opportunity, however, comes with the hurdle of choosing the appropriate
    use of LLMs in a rapidly-expanding landscape of models and solutions. Prior to
    the widespread adoption of LLMs, CSS practitioners typically relied on fine-tuning
    smaller encoder-based models for domain-specific classification tasks (Sun et al.
    [2019](#bib.bib20)). By contrast, LLMs can be used more flexibly, enabling a variety
    of alternative classification approaches (Chae and Davidson [2023](#bib.bib5)).
    Such versatility, while beneficial, poses a barrier for establishing standardized
    best practices within the field.
  prefs: []
  type: TYPE_NORMAL
- en: In their most straightforward usage, LLMs can function as zero-shot classifiers,
    requiring only some target text and a classification prompt (Kojima et al. [2022](#bib.bib11)).
    This approach is convenient because it applies the base model without the need
    of additional training to alter its weights. The prompt can be improved through
    various strategies, such as manual prompt engineering (White et al. [2023](#bib.bib22)),
    automated prompt generation based on the task descriptions (Shin et al. [2020](#bib.bib19)),
    or prompt augmentation with additional task-specific information (Brown et al.
    [2020](#bib.bib3)) or by integration of external knowledge bases (Li et al. [2022](#bib.bib14)).
  prefs: []
  type: TYPE_NORMAL
- en: Alongside approaches that require no training, fine-tuning on domain-specific
    data may offer better adaptability to specific tasks, albeit at the expense of
    increased computation (Wei et al. [2022](#bib.bib21)). Instruction-tuning is another
    flavor of fine-tuning, where the model is conditioned to adhere to explicit instructions
    and align with human judgments, although crafting high-quality instructions can
    be both costly and time consuming (Ouyang et al. [2022](#bib.bib17)). Furthermore,
    the continuous introduction of new language models raises questions around the
    effectiveness of different prompting and training techniques across various models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To bring some clarity on the value of these different practices in the typical
    workflow of text classification for the Computational Social Sciences, we provide
    an overview of how current LLM-based methods perform on a variety of CSS text
    classification tasks. Our goal is to provide practitioners with actionable guidelines
    on how to prioritize the use of different classification techniques. Specifically,
    we seek to answer three questions to investigate the effectiveness of the three
    main families of LLM-based classification:'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ1: What is the value of *prompt-improvement strategies* that add task-relevant
    knowledge?'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ2: How does *fine-tuning* on static instructions compare with LLM-generated
    instructions?'
  prefs: []
  type: TYPE_NORMAL
- en: 'RQ3: To what extent an increase in the *volume of pre-training data* (e.g.,
    Llama-2 vs. Llama-3) enhances downstream performance?'
  prefs: []
  type: TYPE_NORMAL
- en: We apply 6 state-of-the-art methods on two LLMs and test them against a standard
    benchmark of $23$ text classification tasks typical of the CSS domain (Choi et al.
    [2023](#bib.bib6)). While not fully exhaustive of all possible nuances of classification
    methods and tasks, our experiments cover the main state-of-the-art classification
    techniques, with the main goal of providing pragmatic guidelines to practitioners
    in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Materials and Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We run all our experiments on two open-source models of the Llama series: Llama-2-7B-chat
    and Meta-Llama-3-8B-Instruct, both released under commercial user license (https://ai.meta.com/llama/license/).
    We initialize both models with a temperature value of $0.9$, in line with the
    setup of previous work. Llama-3 is trained on a corpus of 15T tokens, about seven
    times larger than Llama-2, and it features a vocabulary size that is four times
    larger (128K tokens).'
  prefs: []
  type: TYPE_NORMAL
- en: The SOCKET benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Social Knowledge Evaluation Tests (SOCKET) is a collection of 58 datasets
    in the domain of social knowledge that can be used to benchmark algorithms for
    natural language understanding (Choi et al. [2023](#bib.bib6)). It is the first
    collective benchmark that has been used to test the capabilities of LLMs in various
    *social* contexts. The datasets are grouped into five types of task: *humor &
    sarcasm*, *offensiveness*, *sentiment & emotion*, *trustworthiness*, and *social
    factors*. In addition to the labeled texts, SOCKET provides one prompt for each
    of the tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: In our experiments, we only consider the 44 datasets that refer to classification
    tasks, saving regression, pair-wise comparisons, and span identification tasks
    for future work. For fine-tuning, we use the data corresponding to the 44 classification
    task. For evaluation, we use a representative subset of 23 datasets. We use the
    same train-test split as defined in Choi et al. ([2023](#bib.bib6)). To manage
    computational resources effectively, we constrained our test sample size to up
    to $2,000$ random samples per task.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-shot prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We evaluate the performance of the models using the zero-shot prompts provided
    in SOCKET (cf. Prompt 1 in Appendix). The prompts are manually designed and do
    not include any examples, directing the model to solve tasks without any specific
    guidance. In this setting, we rely entirely on the LLM’s internal representation
    and understanding of the individual tasks.
  prefs: []
  type: TYPE_NORMAL
- en: AI-knowledge prompts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We produce AI-based enhancement to the zero-shot prompts using *generated knowledge
    prompting*, a technique that relies on a language model to generate task-specific
    knowledge that can then be used as additional information to be included into
    the prompt (Liu et al. [2022](#bib.bib15)). We use GPT-4 to generate task-specific
    label descriptions based on the zero-shot prompts and the available label options
    (cf. Prompt 2). This process adds task-aware elements to the prompts, providing
    descriptions for each individual label-option (cf. Prompt 3).
  prefs: []
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Retrieval-Augmented Generation (RAG) integrates an information retrieval module
    within the generative framework of a Large Language Model (Lewis et al. [2021](#bib.bib13)).
    The RAG system uses the prompt as a query to search a domain-specific knowledge
    base, retrieving information that is relevant to both the prompt and the domain.
    The retrieved data is combined with the initial prompt and submitted to the LLM
    for generation. This methodology is designed to adapt the LLM’s output to the
    target domain without the need of additional training on specialized data (Hu
    and Lu [2024](#bib.bib10)). Recent empirical studies indicate that RAG presents
    a competitive alternative to traditional fine-tuning, particularly due to the
    minimal computational resources required for generating and querying the search
    index (Balaguer et al. [2024](#bib.bib2)).
  prefs: []
  type: TYPE_NORMAL
- en: For each task, we apply the all-MiniLM-l6-v2 model to create dense embeddings
    of all the training instances. These vector representations are constructed using
    text segments of 1000 characters, with a 150-character overlap. We efficiently
    index all the embeddings with the FAISS library (Douze et al. [2024](#bib.bib9)).
    During the evaluation phase on the test dataset, we calculate the embedding of
    the input text, and use it to query the index and retrieve the top five most similar
    texts, based on cosine similarity, along with their corresponding labels. We then
    formulate a final prompt for classification, integrating the test sample and the
    retrieved documents (cf. Prompt 4). In the system prompt, in addition to the specifics
    of our RAG configuration, we also include the AI-generated descriptions of the
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When Supervised Fine Tuning (SFT) an LLM for a specific classification task,
    the model is provided with a series of prompts containing: *i)* fixed classification
    instructions specific to the task, including all the classification labels allowed,
    and *ii)* a set of labeled texts (cf. Prompt 5). The loss calculated between the
    generated output and the examples’ true labels is used to update the model’s weights.'
  prefs: []
  type: TYPE_NORMAL
- en: We adopt a two-phase fine-tuning approach that aligns with the current best
    practices. During the first phase, we use Quantized Low-Rank Adaptation (QLoRA),
    an efficient fine-tuning technique (Dettmers et al. [2023](#bib.bib7)). In QLoRA,
    the main model is frozen and quantized to a 4-bit representation. The fine-tuning
    process is used to learn separate low-rank matrices of gradients, which are then
    combined with the frozen model during inference, weighted by a factor $\alpha$.
  prefs: []
  type: TYPE_NORMAL
- en: In the second phase, we perform Direct Preference Optimization (DPO), a technique
    that updates the model’s weights based on the explicit user preference for one
    training example over another (Rafailov et al. [2023](#bib.bib18)). During DPO,
    the model receives a prompt and pairs of responses ranked by preference. Based
    on cross-entropy loss, the model updates its weights to maximize the probability
    of generating the preferred example.
  prefs: []
  type: TYPE_NORMAL
- en: We trained both phases for one epoch, and we set $\alpha$ to 16, the dropout
    rate to 0.05, and the matrix rank to 8.
  prefs: []
  type: TYPE_NORMAL
- en: Instruction tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instruction tuning is a special type of fine-tuning that, instead of fine-tuning
    on labeled text examples from a single task, provides the model with a set of
    instructions and desired corresponding outputs from multiple tasks (Wei et al.
    [2022](#bib.bib21)). Unlike traditional fine tuning, instruction tuning improves
    the model’s ability to follow classification instructions correctly, and it produces
    a final model that can be flexibly employed to solve a series of classification
    tasks within the same domain. To implement instruction tuning, we use the same
    SFT+DPO pipeline that we employed for fine-tuning, but using instructions and
    examples from all the tasks during training. This approach is similar to that
    used by *SocialiteLlama*, the first example of instruction-tuned model on all
    the tasks from the SOCKET benchmark (Dey et al. [2024](#bib.bib8)).
  prefs: []
  type: TYPE_NORMAL
- en: '| Tasks | Llama-2 7B chat | Llama-3 8B Instruct |'
  prefs: []
  type: TYPE_TB
- en: '|  | Zero-shot | AI Knowledge | RAG | Fine-tuning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Instruction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tuning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reverse &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Instructions &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Zero-shot | AI Knowledge | RAG | Fine-tuning |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Instruction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tuning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Reverse &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Instructions &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Humor & Sarcasm |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| hahackathon#is_humor | 0.459 | 0.56 | 0.462 | 0.834 | 0.564 | 0.548 | 0.765
    | 0.864 | 0.636 | 0.442 | 0.904 | 0.933 |'
  prefs: []
  type: TYPE_TB
- en: '| sarc | 0.400 | 0.492 | 0.451 | 0.303 | 0.475 | 0.216 | 0.511 | 0.591 | 0.534
    | 0.689 | 0.499 | 0.628 |'
  prefs: []
  type: TYPE_TB
- en: '| tweet_irony | 0.313 | 0.497 | 0.366 | 0.458 | 0.464 | 0.638 | 0.540 | 0.663
    | 0.551 | 0.510 | 0.889 | 0.788 |'
  prefs: []
  type: TYPE_TB
- en: '| Offensiveness |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| contextual-abuse#PersonDirectedAbuse | 0.103 | 0.480 | 0.182 | 0.990 | 0.105
    | 0.052 | 0.671 | 0.655 | 0.460 | 0.975 | 0.992 | 0.978 |'
  prefs: []
  type: TYPE_TB
- en: '| implicit-hate#explicit_hate | 0.090 | 0.142 | 0.123 | 0.788 | 0.139 | 0.799
    | 0.665 | 0.517 | 0.447 | 0.950 | 0.951 | 0.947 |'
  prefs: []
  type: TYPE_TB
- en: '| contextual-abuse#IdentityDirectedAbuse | 0.076 | 0.515 | 0.255 | 0.883 |
    0.102 | 0.001 | 0.708 | 0.758 | 0.516 | 0.893 | 0.984 | 0.973 |'
  prefs: []
  type: TYPE_TB
- en: '| hasbiasedimplication | 0.245 | 0.426 | 0.574 | 0.530 | 0.390 | 0.767 | 0.463
    | 0.499 | 0.432 | 0.487 | 0.577 | 0.833 |'
  prefs: []
  type: TYPE_TB
- en: '| hateoffensive | 0.503 | 0.326 | 0.625 | 0.765 | 0.548 | 0.776 | 0.488 | 0.424
    | 0.440 | 0.870 | 0.838 | 0.841 |'
  prefs: []
  type: TYPE_TB
- en: '| intentyn | 0.090 | 0.157 | 0.463 | 0.158 | 0.251 | 0.595 | 0.566 | 0.289
    | 0.261 | 0.413 | 0.719 | 0.741 |'
  prefs: []
  type: TYPE_TB
- en: '| tweet_offensive | 0.412 | 0.577 | 0.723 | 0.762 | 0.533 | 0.506 | 0.693 |
    0.702 | 0.698 | 0.837 | 0.822 | 0.688 |'
  prefs: []
  type: TYPE_TB
- en: '| implicit-hate#implicit_hate | 0.085 | 0.202 | 0.108 | 0.449 | 0.268 | 0.466
    | 0.589 | 0.494 | 0.45 | 0.783 | 0.762 | 0.737 |'
  prefs: []
  type: TYPE_TB
- en: '| implicit-hate#stereotypical_hate | 0.047 | 0.164 | 0.725 | 0.892 | 0.150
    | 0.769 | 0.329 | 0.499 | 0.378 | 0.887 | 0.953 | 0.929 |'
  prefs: []
  type: TYPE_TB
- en: '| Sentiment & Emotion |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| empathy#distress_bin | 0.048 | 0.565 | 0.554 | 0.349 | 0.172 | 0.494 | 0.285
    | 0.597 | 0.667 | 0.382 | 0.602 | 0.500 |'
  prefs: []
  type: TYPE_TB
- en: '| dailydialog | 0.167 | 0.561 | 0.107 | 0.253 | 0.154 | 0.782 | 0.382 | 0.336
    | 0.109 | 0.839 | 0.837 | 0.655 |'
  prefs: []
  type: TYPE_TB
- en: '| tweet_emotion | 0.450 | 0.623 | 0.680 | 0.650 | 0.498 | 0.319 | 0.725 | 0.776
    | 0.771 | 0.802 | 0.721 | 0.750 |'
  prefs: []
  type: TYPE_TB
- en: '| crowdflower | 0.215 | 0.288 | 0.224 | 0.303 | 0.235 | 0.154 | 0.179 | 0.243
    | 0.282 | 0.342 | 0.286 | 0.353 |'
  prefs: []
  type: TYPE_TB
- en: '| Social Factors |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| hayati_politeness | 0.281 | 0.438 | 0.688 | 0.500 | 0.375 | 0.25 | 0.844
    | 0.656 | 0.656 | 0.719 | 0.844 | 0.688 |'
  prefs: []
  type: TYPE_TB
- en: '| complaints | 0.438 | 0.649 | 0.780 | 0.901 | 0.562 | 0.559 | 0.806 | 0.878
    | 0.809 | 0.916 | 0.872 | 0.817 |'
  prefs: []
  type: TYPE_TB
- en: '| stanfordpoliteness | 0.550 | 0.621 | 0.665 | 0.522 | 0.582 | 0.439 | 0.640
    | 0.644 | 0.621 | 0.678 | 0.549 | 0.550 |'
  prefs: []
  type: TYPE_TB
- en: '| questionintimacy | 0.155 | 0.222 | 0.204 | 0.209 | 0.227 | 0.182 | 0.2 |
    0.204 | 0.2 | 0.320 | 0.351 | 0.347 |'
  prefs: []
  type: TYPE_TB
- en: '| Trustworthyness |  |  |  |  |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| hypo-l | 0.269 | 0.402 | 0.557 | 0.437 | 0.349 | 0.672 | 0.665 | 0.693 |
    0.536 | 0.724 | 0.712 | 0.721 |'
  prefs: []
  type: TYPE_TB
- en: '| rumor#rumor_bool | 0.282 | 0.606 | 0.887 | 0.444 | 0.458 | 0.592 | 0.514
    | 0.542 | 0.549 | 0.620 | 0.647 | 0.669 |'
  prefs: []
  type: TYPE_TB
- en: '| two-to-lie#receiver_truth | 0.490 | 0.430 | 0.899 | 0.945 | 0.549 | 0.449
    | 0.366 | 0.613 | 0.682 | 0.945 | 0.943 | 0.933 |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-task average | 0.268 | 0.432 | 0.491 | 0.579 | 0.354 | 0.479 | 0.547
    | 0.571 | 0.508 | 0.697 | 0.750 | 0.739 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Accuracy on SOCKET classification tasks across models. Best results
    for each model are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: Reverse instruction tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instruction tuning typically relies on one fixed human-generated instruction
    for each task. This constrains the ability of LLMs to learn associations between
    the semantics of instructions and their corresponding responses. Generating synthetic
    instruction variants with LLMs mitigates this problem without needing extensive
    human labor (Møller et al. [2024](#bib.bib16)). This process is known as *reverse
    instruction generation* (Köksal et al. [2024](#bib.bib12)). It involves presenting
    the LLM with a textual output and prompting it to formulate a plausible instruction
    that could lead to that output (cf. Prompt 6). We extend this method to create
    instructions that are specific to classification tasks consisting of a target
    text, a set of possible labels, and the label for the given text (cf. Prompt 7).
  prefs: []
  type: TYPE_NORMAL
- en: For generating reverse instructions, we randomly sample up to $4,000$ samples.
    We then clean the output using simple heuristics designed to remove noisy generations,
    filtering out instructions that repeat the input text, explicitly reveal the label,
    or are improperly formatted. We create a new training set for instruction-tuning
    by simply replicating each training example for all its instruction variants,
    and then apply the SFT+DPO pipeline. During the evaluation phase, we randomly
    sample instructions from the training set and integrate them into the prompt template.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Table [1](#Sx2.T1 "Table 1 ‣ Instruction tuning ‣ Materials and Methods ‣ Prompt
    Refinement or Fine-tuning? Best Practices for using LLMs in Computational Social
    Science Tasks") presents the classification accuracy across methods and tasks.
    A critical factor impacting performance is the selection of the pre-trained model.
    On average, across tasks, there is an accuracy improvement ranging from $0.02$
    when employing Llama-3 over Llama-2 (RQ3). This result indicates that there is
    still room for improving the the language models’ understanding during pre-training,
    and suggests that switching to recent models is worth prioritizing.
  prefs: []
  type: TYPE_NORMAL
- en: When comparing the performance of prompt enhancement methods, two main findings
    emerge. First, zero-shot yields relatively high accuracy, yet it is consistently
    outperformed by AI-generated knowledge prompting (RQ1). This trend is not as pronounced
    in the *offensiveness* category, where some tasks exhibit a notable decrease in
    accuracy with AI-enhanced prompts. This could be attributed to the safeguards
    built into the LLMs when addressing sensitive content, potentially restricting
    their ability to generate high-quality prompts. Second, the performance of Retrieval-Augmented
    Generation (RAG) for prompt enhancement is inconsistent. Its relative performance
    to zero-shot is generally better with Llama-2, albeit with considerable variability
    across tasks, and tends to be less effective with Llama-3 (RQ1). This suggests
    that models with less extensive pre-training may benefit from external knowledge
    integration, but this advantage diminishes with models that have a more robust
    pre-training foundation.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning markedly improves the accuracy of AI-knowledge prompting by an average
    of $0.15$ on average. This disparity may be due to Llama-3’s superior capability
    to process complex and semantically varied input data, thanks to its expanded
    vocabulary and training corpus. The added complexity, however, introduces noise
    into Llama-2’s classification process, suggesting a need for more fine-tuning
    data to bridge the performance gap with Llama-3\. In summary, the results indicate
    that advanced fine-tuning methods involving small sets of instructions and data
    from multiple tasks hold some promise but also risk performance decline if the
    foundational model lacks the necessary expressiveness (RQ2). Moreover, while reverse
    instructions enhance training diversity, they can also lead to hallucinations
    and information leaks that require manual intervention, thus limiting their practicality.
  prefs: []
  type: TYPE_NORMAL
- en: The robustness of our findings is supported by the limited performance variation
    across task categories, which can be largely attributed to the difficulty of individual
    tasks. For instance, the *crowdflower* task exhibits the lowest performance due
    to its 13 possible classes that represent concepts challenging to discern from
    textual information.
  prefs: []
  type: TYPE_NORMAL
- en: Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our findings highlight three good practices that practitioners can adopt when
    using LLMs for classification tasks within the field of Computational Social Science.
    First, the selection of the model is a crucial decision that significantly impacts
    performance. Choosing models that have undergone extensive pre-training is recommended.
    Second, basic zero-shot methods should be avoided in favor of enhanced zero-shot
    techniques that incorporate LLM-generated descriptions of the task and labels
    into the prompt. This straightforward method offers substantial benefits relative
    to its minimal cost, unlike more complex retrieval-based methods for prompt augmentation,
    which do not appear as effective for classification purposes. Last, fine-tuning
    should be pursued whenever adequate computational resources are accessible, as
    it consistently yields positive results and can be executed cost-effectively using
    contemporary methods like QLoRa. Nevertheless, in scenarios where fine-tuning
    data is scarce, advanced instruction tuning that integrates instructions and datasets
    from diverse tasks should be approached with caution, as it may not generalize
    well and could potentially degrade performance.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bail (2024) Bail, C. A. 2024. Can Generative AI improve social science? *PNAS*,
    121(21).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Balaguer et al. (2024) Balaguer; et al. 2024. RAG vs Fine-tuning: Pipelines,
    Tradeoffs, and a Case Study on Agriculture. *arXiv:2401.08406*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown et al. (2020) Brown; et al. 2020. Language models are few-shot learners.
    *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bubeck and ohers (2023) Bubeck, S.; and ohers. 2023. Sparks of Artificial General
    Intelligence: Early experiments with GPT-4. *ArXiv:2303.12712*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chae and Davidson (2023) Chae, Y.; and Davidson, T. 2023. Large language models
    for text classification: From zero-shot learning to fine-tuning. *Open Science
    Foundation*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. (2023) Choi, M.; Pei, J.; Kumar, S.; Shu, C.; and Jurgens, D. 2023.
    Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language
    Models with SocKET Benchmark. *ArXiv:2305.14938*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dettmers et al. (2023) Dettmers, T.; Pagnoni, A.; Holtzman, A.; and Zettlemoyer,
    L. 2023. QLoRA: Efficient Finetuning of Quantized LLMs. *ArXiv:2305.14314*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dey et al. (2024) Dey, G.; Ganesan, A. V.; Lal, Y. K.; Shah, M.; Sinha, S.;
    Matero, M.; Giorgi, S.; Kulkarni, V.; and Schwartz, H. A. 2024. SOCIALITE-LLAMA:
    An Instruction-Tuned Model for Social Scientific Tasks. *ArXiv:2402.01980*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Douze et al. (2024) Douze, M.; Guzhva, A.; Deng, C.; Johnson, J.; Szilvasy,
    G.; Mazaré, P.-E.; Lomeli, M.; Hosseini, L.; and Jégou, H. 2024. The FAISS Library.
    *ArXiv:2401.08281*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hu and Lu (2024) Hu, Y.; and Lu, Y. 2024. RAG and RAU: A Survey on Retrieval-Augmented
    Language Model in Natural Language Processing. *ArXiv:2404.19543*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kojima et al. (2022) Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,
    Y. 2022. Large language models are zero-shot reasoners. *NeurIPS*, 35.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Köksal et al. (2024) Köksal, A.; Schick, T.; Korhonen, A.; and Schütze, H.
    2024. LongForm: Effective Instruction Tuning with Reverse Instructions. *ArXiv:2304.08460*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lewis et al. (2021) Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,
    V.; Goyal, N.; Küttler, H.; Lewis, M.; Yih, W.-t.; Rocktäschel, T.; Riedel, S.;
    and Kiela, D. 2021. Retrieval-Augmented Generation for Knowledge-Intensive NLP
    Tasks. *ArXiv:2005.11401*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022) Li, H.; Su, Y.; Cai, D.; Wang, Y.; and Liu, L. 2022. A survey
    on retrieval-augmented text generation. *ArXiv preprint arXiv:2202.01110*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Liu, J.; Liu, A.; Lu, X.; Welleck, S.; West, P.; Bras, R. L.;
    Choi, Y.; and Hajishirzi, H. 2022. Generated Knowledge Prompting for Commonsense
    Reasoning. *ArXiv:2110.08387*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Møller et al. (2024) Møller, A. G.; Pera, A.; Dalsgaard, J.; and Aiello, L.
    2024. The Parrot Dilemma: Human-Labeled vs. LLM-augmented Data in Classification
    Tasks. In *EACL*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ouyang et al. (2022) Ouyang, L.; et al. 2022. Training language models to follow
    instructions with human feedback. *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rafailov et al. (2023) Rafailov, R.; Sharma, A.; Mitchell, E.; Ermon, S.; Manning,
    C. D.; and Finn, C. 2023. Direct Preference Optimization: Your Language Model
    is Secretly a Reward Model. *ArXiv:2305.18290*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2020) Shin, T.; Razeghi, Y.; Logan IV, R. L.; Wallace, E.; and
    Singh, S. 2020. Autoprompt: Eliciting knowledge from language models with automatically
    generated prompts. *ArXiv:2010.15980*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2019) Sun, C.; Qiu, X.; Xu, Y.; and Huang, X. 2019. How to fine-tune
    bert for text classification? In *CCL*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. (2022) Wei, J.; Bosma, M.; Zhao, V. Y.; Guu, K.; Yu, A. W.; Lester,
    B.; Du, N.; Dai, A. M.; and Le, Q. V. 2022. Finetuned Language Models Are Zero-Shot
    Learners. *ArXiv:2109.01652*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White et al. (2023) White, J.; et al. 2023. A prompt pattern catalog to enhance
    prompt engineering with ChatGPT. *ArXiv:2302.11382*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[h!] Prompt
    1: Zero-shot prompt # System prompt You are a helpful,
    respectful and honest assistant. # Task prompt (example for the sarc task) For
    the sentence: {text}, is it sarcastic? You can choose from the following labels:
    {labels}. Answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[h!] Prompt 2: AI-Knowledge Generation
    # Task prompt For the task: {task_description}. Explain briefly
    the labels: {labels_list}'
  prefs: []
  type: TYPE_NORMAL
- en: '[h!] Prompt 3: Knowledge-improved
    Zero-shot # System prompt You are a helpful,
    respectful and honest assistant. You have the following knowledge about task-specific
    labels: ’sarcastic’: This label indicates the sentence is sarcastic, meaning it
    conveys irony or mocks with a tone of detachment or insincerity. ’literal’: This
    label is used if the sentence is not sarcastic, implying a straightforward or
    sincere expression without irony. # ... Remainder of prompt as in Prompt 1 ...'
  prefs: []
  type: TYPE_NORMAL
- en: '[h!] Prompt 5: Fine-tuning Prompt
    # System prompt You are a helpful, respectful and honest assistant.
    # Task prompt For the sentence: {task_description_with_text} You can choose from
    the following labels: {label_list}. Answer: {label}'
  prefs: []
  type: TYPE_NORMAL
- en: '[h!] Prompt 4: RAG Prompt
    # System prompt You are part of a RAG classification system designed
    to categorize texts. Continued specification of the RAG... # Task prompt Consider
    the relevance and content of each document in relation to the input text and the
    descriptions of the labels. If a retrieved document is highly relevant to the
    input text and aligns closely with the description of a label, that label might
    be the correct classification. Retrieved Documents: Document $i$} Input Text:
    {text}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Answer: [/INST]'
  prefs: []
  type: TYPE_NORMAL
- en: '[h!] Prompt 6: Reverse Instructions
    Generation Prompt Instruction: X Output: {doc}
    What kind of instruction could this be the answer to? X:'
  prefs: []
  type: TYPE_NORMAL
- en: '[h!] Prompt
    7: Reverse Instructions Generation for Classification #
    System prompt You are a helpful assistant helping in creating instructions for
    a text classification task. # Task prompt Instruction: X Input: {text} Labels:
    {label_list} Output: {label} What kind of instruction could ‘‘Output’’ be the
    answer to given ‘‘Input’’ and ‘‘Labels’’? Please make only an instruction for
    the task and include brief descriptions of the labels. Begin your answer with
    ’X: ’'
  prefs: []
  type: TYPE_NORMAL
