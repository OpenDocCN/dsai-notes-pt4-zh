# [ÂèåÂ≠ó] Âú®{Transformer}Êó∂‰ª£, {RWKV}ÊòØRNNÁöÑ[ÊñáËâ∫Â§çÂÖ¥]--ËÆ∫ÊñáËØ¶Ëß£ - P1 - ÁºñÁ®ãËØ≠Ë®ÄËßÇÂØü - BV11N411C76z

Hello today we're going to look at RWKVÔºå which in its own words is reinventing RNN for the Transformer era„ÄÇ

 This is a very interesting project and very interesting model architecture because it has some properties of transformers„ÄÇ

 notably it's a model architecture that's very scalable in terms of training so you can stack it really deep and you can still train it and also you can parallelize training„ÄÇ

At the same timeÔºå it avoids the quadratic memory bottleneck that transformers have by essentially being an RN„ÄÇ

 It's kind of a recurrent neural network in the sense that during inference„ÄÇ

 you can compute the output step by step and always have a constant memory because everything is put into a hidden state„ÄÇ

 We're going to look at how these two things come together and what the tradeoffs are between the two„ÄÇ

 The project is also very interesting because it's been largely developed by one person or by just very few people who have worked on this and then compare this to entire corporations that are pouring their human resources into transformers and still the results are that this model in some cases„ÄÇ

 not in all casesÔºå but in some cases can be very comparable in terms of performance with transformers with really big transformers and as I said„ÄÇ

Is scalableÔºå which so far RnNs have been lacking„ÄÇWe'll go over the paper„ÄÇ

 We'll go over the architecture and see what's actually happening right here„ÄÇ

 I have some of my own thoughts and you knowÔºå just opinions on this„ÄÇ And I hope you're with me„ÄÇ

 But firstÔºå let's„ÄÇ

![](img/d1c78117f4e238ce89b5b7dfa803986a_1.png)

Let me show you this„ÄÇ fully connected is a conference„ÄÇ It's by weights and biases„ÄÇ

 It's a one day eventÔºå and it has pretty cool speakers„ÄÇ

 So not only the cofounders of weights and biases themselves„ÄÇ

 but the cofounder of Langcha is there The cofounder of Cagel I there„ÄÇ

 Richard Soer from U do co is there Chiyian is there of Clapot there's so many people right here and so many cool speakers and as I said„ÄÇ

 if you are in around the San Francisco area go give that event a visit if you want„ÄÇ

 I'm going use put a link and a promo code into the description of this video that will make the tickets cheaper„ÄÇ

 So the tickets will be 49 buck instead of what you see right here„ÄÇ

 So that's going to be on June the 7„ÄÇ don't forget that June the 7 in San Francisco„ÄÇ

 It's an inperson event has a max capacity„ÄÇ So grab it now„ÄÇüòä„ÄÇ



![](img/d1c78117f4e238ce89b5b7dfa803986a_3.png)

![](img/d1c78117f4e238ce89b5b7dfa803986a_4.png)

![](img/d1c78117f4e238ce89b5b7dfa803986a_5.png)

![](img/d1c78117f4e238ce89b5b7dfa803986a_6.png)

![](img/d1c78117f4e238ce89b5b7dfa803986a_7.png)

That's it„ÄÇ Thanks to Bs and biases for also giving me this opportunity and also giving this to you„ÄÇ

 It's very cool„ÄÇSo„ÄÇRWKVÔºå it stands for„ÄÇLet me not screw this up„ÄÇ Receptive receptanceÔºå Receptance„ÄÇ

 R is for receptance„ÄÇ W is for weight„ÄÇ key K is for keyÔºå and V is for value„ÄÇ

 These describe the different elements of the model architecture and will go through that will go through it„ÄÇ

 So what are we dealing with„ÄÇ We're dealing with a model that you can use for lots of things„ÄÇ

 but we're mainly dealing with a model that in the instance it is outlined here is for language modeling„ÄÇ

 by language modellingÔºå we simply mean we have a piece of textÔºå yadaÔºå yada yadaÔºå yada„ÄÇ

 and what we want the model to predict is the next setÔºå the next tokens or the next word in the text„ÄÇ

 So from hereÔºå predict this thing and from here predict this thing and so on„ÄÇ

Transformers are usually used in this manner„ÄÇ notably I would stick the entire prefix here into a big transformer and the transformer would spit out the next step„ÄÇ

 it will do that in a form that we call causal attention„ÄÇ

 which essentially means that every piece in the text right here„ÄÇ

 So every token can attend to all the other tokens that are before it„ÄÇ

 So all the other tokens that come in front of it would have be would be inputs to that token using attention that results in a quadratic in a quadratic requirement of compute and memory Now if you can see there if every token attends to its back„ÄÇ

 it's like T times t minus-1 or something like this„ÄÇInteractions that need to be considered„ÄÇ

 so t squared in expectation„ÄÇAnd„ÄÇNoÔºå yesÔºå maybe„ÄÇYeahÔºå that makes about sense„ÄÇ

So transformers naturally have a limit because if you add one token„ÄÇ

 then you always add a memory requirement of of T„ÄÇ And so very quickly that scales to be out of proportion„ÄÇ

 so their power is traded off by the fact that they can only consider a limited set of tokens at a time„ÄÇ

 Recurrent neural networks trade this off„ÄÇ recurrent neural networks if they have to do something like this„ÄÇ

 if they have to have an input and then predict the next thing„ÄÇ

 what they'll do is they'll just start here„ÄÇThen they'll put something into a hidden state like a little memory„ÄÇ

 I'm going to represent that as a box„ÄÇ People have forgotten how R and Ns work„ÄÇ I figured„ÄÇ

Like a few years agoÔºå I could have talked about RnNs„ÄÇ

 and every one of you would have known what I've talking about„ÄÇ

 And then transformers are kind of the new thing„ÄÇ And now it's more like I have to say„ÄÇ

 I don't even have to mention stuff is a transformer But I do have to conversely explain RnN„ÄÇ

 So it would put whatever it learns into a memory like this hidden box„ÄÇ

 this box right here is the memory„ÄÇ Then from thatÔºå it would consume the memory„ÄÇ

 and it would consume this thing and it would build a new memory„ÄÇ

 and then it would consume that memory and it would consume the next input„ÄÇ

 it would build a new memory„ÄÇ So it does this step by step„ÄÇ and it can always„ÄÇ

 you can drop the old memory„ÄÇ So you can forget about the old memory because all you need pun intended to go forward from here on out is that current memory„ÄÇ

 So you do step by step„ÄÇ and you always save stuff into the memory into the hidden state or whatever you want to call it„ÄÇ

 So RnNs have this really great property that they only require a constant memory in order to do inference„ÄÇ

üòäÔºåHoweverÔºå the individualÔºå the individual inference stepÔºå for exampleÔºå when we are here„ÄÇ

 we have that memoryÔºå we predict the next the next token from it„ÄÇ

 so we predict the next this thing right here„ÄÇWe can only consider the memory and the previous token„ÄÇ

 That's all We cannot explicitly consider any token that is in the way back because everything goes through that hidden state„ÄÇ

And that bottleneck has usually been one of the downfalls for RNNs„ÄÇ

 There is a problem of vanishing gradients„ÄÇ There are a couple of other problems that you can't just compress information into that one hidden state plus RNNs have been notoriously hard to train because the inference always requires this step by step thing„ÄÇ

 which means you have to do back propagation through time which is part of the vanishing gradient problem but also means that you can't parallelize the training in a transformer I can input a tokenop a token sequence of 50 tokens„ÄÇ

 and that gives me 50 training examples which I can train all at the same time because of the causal attention mask„ÄÇ

In an RNNÔºå if I have a sequence of 50 tokensÔºå I can still only train one token loss at a time because I can't infer everything at the same time„ÄÇ

Our WKV is going to strike a trade off in the middle of those two things„ÄÇ And in a sense„ÄÇ

 so people askÔºå or I askedÔºå is this a transformer moreÔºå is it an R and N more„ÄÇ

 and I've come to the conclusion„ÄÇIt's a convenet„ÄÇ and I'm going to I'm going to explain my reasoning„ÄÇ

 and they they also refer to thatÔºå by the way„ÄÇs it's not like I found out something great right here„ÄÇ

 but in the most basic sense„ÄÇYou can think of this thing as a convolutional network across a one dimensional sequence of tokens„ÄÇ

That's that's going to be my statement and we'll go through it„ÄÇ

So they give a bit of an introduction right here on what it„ÄÇ

What this means and what I essentially just saidÔºå transformers scale quadraically with sequence length„ÄÇ

 RNNs exhibit linear scalingÔºå which is very beneficialÔºå very advantageous„ÄÇ

 The RWKV model combines the efficientÔºå parallelizable training of transformers with the efficient inference of RNNs„ÄÇ

So it's like almost two modes between which you can switch around„ÄÇ

They say they have a linear attention mechanism and as I said„ÄÇ

 they formulate the model either as a transformer or as an RN„ÄÇ

 that linear attention mechanism is something that we're going to focus on in just a bit because„ÄÇ

It's notÔºå it's not their fault because people have been doing it before them„ÄÇ

 but I think it really stretches the word attention and what it means„ÄÇ

 I think it stretches that to like a point where I don't agree anymore more calling it attention„ÄÇ

 but againÔºå they are not the first people„ÄÇKind of doing that„ÄÇ

 so I'm not going to hold them to account right here„ÄÇ

They say this is the first nontransformer architecture that to be scaled to tens of billions of parameters„ÄÇ

 So one of the properties of this thing is really you can scale it„ÄÇ

 you can scale it and you can train it in parallel which also means you can pump a lot of data into it„ÄÇ

 and that's very advantageous And there is a lot to be said here about maybe maybe it's not that much the architecture we're dealing with„ÄÇ

 but more you know good models are simply models with architectures that are scalable„ÄÇ

 So it's not maybe that's a hypothesis like how much of the performance of something like G4 is due to the fact that it is a transformer and how much is due to the fact just that is a scalable architecture you can pump a lot of data into We don't know„ÄÇ

Yet too well how those things exactly trade off„ÄÇ But there is good argument to be made thatÔºå hey„ÄÇ

 if you can find some other architectureÔºå just just scales really wellÔºå then you know„ÄÇ

 you might as well reach the same performance„ÄÇThis is a complexity„ÄÇ

 a asympttic complexity table on the different ways of doing attention mechanisms„ÄÇ

 so the transformer is that classic mechanism as we said it needs quadratic time in t big T is the sequence length of the sequence we're processing and then in terms of space it needs a little bit more„ÄÇ

 but the leading term is also this t squared right here„ÄÇ

 the D is the dimension I believe of the embeddings of the hidden spaces which is usually a constant factor across all of these things„ÄÇ

There there are various tradeoffs like a reformer and so onÔºå performerÔºå a lot of these„ÄÇ

 they do approximations to the original transformer attention mechanism„ÄÇ

 and it's also notable to say that RWKV is not an approximate attention or anything like this„ÄÇ

 It doesn't try to approximate the original attention„ÄÇ

 it replaces it with a different mechanism of considering the past„ÄÇ

So what does that mechanism look likeÔºüy„ÄÇLets let's go into it„ÄÇMaybe yeahÔºå this isÔºå this is smart„ÄÇ

 Let's first look at this here„ÄÇ If you've been in in deep learning and are old like me„ÄÇ

 you remember thisÔºå this is an LSTM„ÄÇ This is a long„ÄÇ

 shorter memory cell from back when before attention was a thing„ÄÇ

 So this was one of the main ways people build RNN's recurrent neural networks that would actually somewhat avoid that vanishing gradient problem and could learn to remember things for a long time„ÄÇ

The idea behind Ls Tms is that you have„ÄÇYoure the hiddens„ÄÇ you have two hidden states„ÄÇIf I'm correct„ÄÇ

am I correct„ÄÇYesÔºå so you have two hidden statesÔºå this C and this H H being the real hidden state„ÄÇ

 and you have a lot of these gating mechanisms„ÄÇWhat the gating mechanisms do„ÄÇ

 it's often represented like this here„ÄÇ This is an element wise product„ÄÇ

So what you would do is you would give in again in a hidden stateÔºå you would get in an input„ÄÇ

 youd put the input through some sort of computationÔºå neural networksÔºå non nonlinearities„ÄÇ

 yara yada yadaÔºå and then you'd have some result right here„ÄÇ

 which would be a vector and then the question is obviously you can compute an output from that at this particular time step but the question is how should the next hidden state look like„ÄÇ

And the idea behind LSTMs and similar architectures is that we're going to take the last hidden state„ÄÇ

SorryÔºå we're going to take the last hidden state„ÄÇAnd we're going to update it„ÄÇ

In a if in a really basic RnN we would just kind of replace the hidden state or we would maybe add the two together and then for propagated„ÄÇ

 But what an LSTM does very interestingly is introduces these these gates„ÄÇ

 So what we'll do is we'll have something like a forget gate„ÄÇAnd the forget gate is simply„ÄÇ

A binary or not a binaryÔºå but say aÔºå obviously„ÄÇContinuous„ÄÇ

 but we can imagine it as a binary vector where just it's a mask values between 0 and1„ÄÇ

 and wherever it's 0Ôºå there is an element wise multiplicationÔºå wherever it's 0„ÄÇ

 this hidden state is going to get forgotten„ÄÇAnd then the new state is going to be updated at that particular point„ÄÇ

 So there is going to be a forget gate„ÄÇ There is also going to be maybe a gate right here that tells which things of the hidden state to even remember„ÄÇ

 right that's also a binary vectorÔºå maybe only these things„ÄÇ

 and so the network itself can control which parts of the information it wants to remember„ÄÇ

 which parts it wants to forget and which parts it wants to retain and then the new hidden state is going to be sort of an addition of these masked inputs and then that goes on„ÄÇ

In order to do thatÔºå there's a lot of computation needed as you see right here„ÄÇ And in particular„ÄÇ

 I want to draw your attention to one fact„ÄÇ the next hidden statesÔºå for example„ÄÇ

 if you take this H right hereÔºå the next hidden state is always going to be some nonlinear function„ÄÇ

 for exampleÔºå this is a sigmoid„ÄÇ I believe it's a nonlinearity or a tan H or something like this of something here like of this Ct„ÄÇ

 the Ct in itself is a linear combinationÔºå but a linear combination of this„ÄÇ

 this in its turn again is a non linearity and then a linear combination of the things and this ultimately is the last hidden state„ÄÇ

 So you can see from the last hidden state to the next hidden state„ÄÇ

 we pass at least two nonlinearities And there you see this sequential stepwise nature of things„ÄÇ

 Its always it's always„ÄÇYou have to compute„ÄÇüò°ÔºåThe stepÔºå then you have to„ÄÇ

Compute the next step based on this step„ÄÇ and the outputs are non linearly related„ÄÇ

 So there is no way you can like„ÄÇJump a you ahead or take five steps together or something like this„ÄÇ

 because they're non nonlinear„ÄÇ It's not like linear functions„ÄÇ

 Linear functions compute really easilyÔºå rightÔºå No nonlinear functions that are stacked where every next thing needs the first thing as an input„ÄÇ

 Those are really not parallelzizableÔºå not„ÄÇLike agregable or anything like this„ÄÇ

 So that's the problem with RnNs if they're formulated like this„ÄÇ

They also go a little bit into the attention mechanism„ÄÇ

 which I guess by now I don't have to explain too much anymore„ÄÇ

 There is a query and a key matrix both are produced from the data So the data enters and three matrices are produced„ÄÇ

 queries keys and values will do an outer product between the queries and keys which defines this quadratic interaction„ÄÇ

 So every token can attend to every other token sometimes you would then mask this with the causal mask with the upper or lower triangular matrix„ÄÇ

 then build a softm across that„ÄÇ So the softmax essentially converts some values„ÄÇ

 So let's say your values or this positive negative negative positive would convert that into a distribution„ÄÇ

 we can interpret it as a probability distribution or you can interpret it as„ÄÇEssentially„ÄÇ

 whatever you want„ÄÇ But you can and it defines where to put the attentionÔºå right„ÄÇ

 It defines which things I should look at like this one and this one a little bit„ÄÇ

 I guess that's why it's called attention because it defines the weights„ÄÇ

In by which I aggregate information„ÄÇ So it dynamically„ÄÇ

Allocs attention to some of these values right hereÔºå rather than others„ÄÇ

That's what attention means to me in the sense how it's described„ÄÇ

 and that's why I said the term is stretched a bit far„ÄÇ

You can write attention in a different way and you can decompose it recurrently almost like an RN„ÄÇ

 so you can decompose attention computationÔºå and this is done in parts to also get around the memory bottleneck however you trade it off with computation right if you compute these things in sequences„ÄÇ

 you have to compute them sequentially where you could just compute them in parallel by doing a big outer product matrix multiplication„ÄÇ

 so you do trade off time and memory here„ÄÇAnd by the way„ÄÇ

 you have to remember all of these things or you can sum themÔºå I guess„ÄÇI mean„ÄÇ

 any matrix multiplicationÔºå you can probably„ÄÇDo that with„ÄÇ Never mind„ÄÇ

You can decompose the attention computation„ÄÇSame one as above in a way like this„ÄÇ

 So I have the outer product of just„ÄÇPair of keys and queries„ÄÇ

 I raise that to the I have the exponential of that„ÄÇ that's part of that softm computation„ÄÇ

Ultimately I divide by the sum of these values so that's the softmax operator„ÄÇ

 and then I multiply the value at that particular location„ÄÇ

 So you can see here this part here defines a weight and v is the value„ÄÇ

 so it's a weighted sum of the values„ÄÇNow we come to attention free transformers„ÄÇ

 which is a piece of work that this paper takes a lot of inspiration from attention free transformers„ÄÇ

T to go about this in the same way as attentionÔºå but they sayÔºå heyÔºå can we reformulate this„ÄÇ

 this formula up hereÔºå the one that we saw„ÄÇCan we reformulate this and„ÄÇ

Just turn it into something that doesn't need that whole quadratic memoryÔºå Chaen„ÄÇ

And they come up and sayÔºå if we don't have to do the outer product up hereÔºå this outer product„ÄÇ

If we don't have to do these out productsÔºå then that would sort of„ÄÇ

Mean that we don't have this token to token„ÄÇ Every token can attend to every other token interactions anymore„ÄÇ

 which means we don't have that quadratic thing anymore„ÄÇ and therefore„ÄÇWei„ÄÇWouldÔºå you know„ÄÇ

 we could save a lot of memory„ÄÇ So they replace the interactions between query and key„ÄÇ

 They replace thatÔºå and they sayÔºå let'sÔºå let's not even compute a query„ÄÇ

 We'll just compute a key for each token„ÄÇ So we're now only producing matrices K and V„ÄÇ no queries„ÄÇ

InsteadÔºå we'll have these ws right hereÔºå the Ws are learnedÔºå so W is a learned matrix„ÄÇ

 so not a not computed from the data„ÄÇBut learnedÔºå and it just learns how tokens interact with each other„ÄÇ

What does it meanÔºå it means that I just have a matrix„ÄÇAnd emphasize size T by T„ÄÇAnd in thereÔºå1Ôºå2Ôºå3Ôºå4„ÄÇ

5Ôºå1„ÄÇ2Ôºå3Ôºå4Ôºå or 5„ÄÇ And in this matrixÔºå there's going to be number like 7„ÄÇ Okay„ÄÇ

 and that means that the interactionÔºå that the weightÔºå the attention weight„ÄÇ

 essentially of that token 1 has with respect to token 2„ÄÇ So how much is token 1 gonna„ÄÇ

Attend to token to„ÄÇ Let's assume it's not causally masked„ÄÇIs 7„ÄÇOkayÔºå that that's that's that„ÄÇ

 it's the same for all the sequences„ÄÇ you all you just sayÔºå well„ÄÇ

 the first token is always going to attend 7 to the second tokenÔºå all the same„ÄÇ

 it's one set of learned parametersÔºå and therefore you can this is just now a multiplication by a constant essentially„ÄÇ

Now„ÄÇThat just defines a fixed attentionÔºå which is a bit too little„ÄÇ

 so the fixed attention is not flexible enough before we had completely dynamic attention and now we go to completely fixed attention that is just learned across the whole data set and the authors there said„ÄÇ

 wiselyÔºå heyÔºå you know„ÄÇIt is in„ÄÇ It is the fact that„ÄÇDepending on the current data point„ÄÇ

 we might need to look back furtherÔºå or we might need to change that attention pattern a little bit„ÄÇ

So they sayÔºå okayÔºå how about we simply add the keys hereÔºü

So the keys are values that are computed from the dataÔºå rightÔºü

So now the data can define essentially an offset to that„ÄÇ So maybe for one data point„ÄÇ

 it's eight important because it says token 1 should really look at token 2 and for another data point„ÄÇ

 the K is negative one„ÄÇ So this is6 right here that depresses that a little bit„ÄÇ

 So there is a modulation in that attention pattern„ÄÇ

 there is one across the data set which is fixed and then on top of that there is a modulation given by each individual data point to modulate that„ÄÇ

 HoweverÔºå the interaction is not multiplicative as it is in the transformer attention that you see up here„ÄÇ

 it's additiveÔºå which is in a senseÔºå a lot less powerful because the multiplicative interaction really defines when two things are close and when two things are far apart between keys and queries whereas here we just multi a little bit that fixed that fixed pattern right here„ÄÇ

HoweverÔºå that fixed pattern can't take into account a token 1„ÄÇIf it decides on its K value„ÄÇ

 it can't take into account the what token2 is„ÄÇ It simply saysÔºå I'm the word cat„ÄÇ

 I should probably really look three words behind me„ÄÇ That's really important„ÄÇ

Whereas the original attention can decideÔºå I'm the word cat„ÄÇ

And I should probably really look at words that relate to fish or fur or or sleeping„ÄÇ

 Like those wordsÔºå those kinds of words really interest me„ÄÇ And that's how it would craft its query„ÄÇ

 and that's what it would be retrieved from the keys of the other words„ÄÇ

 Whereas here it can just sayÔºå wellÔºå I'm catÔºå I should probably look three words behind me Seems really good„ÄÇ

I hope you can see how this is„ÄÇüòäÔºåKind of less powerful than the original attention„ÄÇBut„ÄÇ

 it is more scalable„ÄÇAgainÔºå you see what it defines is essentially this part right here„ÄÇIs a weight„ÄÇ

 so you have awaited some of the values„ÄÇLastly„ÄÇWe have this paper right here„ÄÇ

 so it formulates the attention mechanism It says yeah but here what we still need to do is we still need to learn that interaction matrix„ÄÇ

 which crucially also means it's still limited by T„ÄÇ

 It's still limited by you know sort of a fixed size„ÄÇ we can we can't go bigger than that and„ÄÇ

What they say now isÔºå how about we don't learn this matrix„ÄÇ All we learn is a vector„ÄÇ

 All we learn is the vector W„ÄÇAnd the vector WÔºå its the same for allÔºå and it defines„ÄÇ

 so it defines it's a vector„ÄÇAnd it has the same dimensionality as the hidden dimensions of the hidden state„ÄÇ

 And it defines for each dimensionÔºå it defines how much„ÄÇDoes the past matter„ÄÇ So for one dimension„ÄÇ

 it could sayÔºå wellÔºå the past matters a lotÔºå therefore„ÄÇThat value is very high„ÄÇ

 And for the other dimension the casinoÔºå that value is very low„ÄÇ The past doesn't matter a lot„ÄÇ

 What happens if we do thatÔºåUltimatelyÔºå we're going to do something like„ÄÇAgainÔºå something up here„ÄÇ

 So you can see we have E the exponential function of W T I„ÄÇPlusÔºå K„ÄÇKi„ÄÇÂóØ„ÄÇüòäÔºåSo„ÄÇüòäÔºåThey now sayÔºå okay„ÄÇ

 we multiply this by this termÔºå T minus I means how much back I'm looking„ÄÇ So if we wonder„ÄÇ

We are token one„ÄÇHow much do we attend toÔºå or let's go the other way around„ÄÇ We're token 4„ÄÇ

 How much do we attend to token number 2„ÄÇOkayÔºå then we ask in which dimension in the first dimension„ÄÇ

 orÔºå the first dimension is really large„ÄÇ therefore we're going to attend a lot to the general past of dimension 1„ÄÇ

Okay„ÄÇSo maybe that's this drop off„ÄÇ And then we look two tokens in the past because the current time step is 4„ÄÇ

 and this I here is 2„ÄÇ So how„ÄÇWhich token are we and which do we attend to„ÄÇ

 And you can see it's minus„ÄÇ So this is getting bigger and bigger as you go further back in the past„ÄÇ

 So it's essentially a linear drop off in the value of W and W itself can be big or small„ÄÇ

 And these two things together define how important a past token is in that particular dimension„ÄÇ

 So it's a multiplication of how far back is it and how much is this dimension in general„ÄÇ

Considering its history„ÄÇAnd then you can see it's considering this much„ÄÇ

 it's two tokens back so this much attentionÔºå and then that is modtable again by a key value that can depend on exactly what the current token is„ÄÇ

So I hope that's understandableÔºå the attention matrix is built on the fly in R WKV„ÄÇ

And it's defined a by the V„ÄÇ It's defined per dimension„ÄÇThe vector W defines a general importance„ÄÇ

A list for each dimensionÔºå it divines how relevant is the past„ÄÇThe second component is this„ÄÇ

 it's simply a linear decay on into the past„ÄÇ So the further backÔºå the less important it is„ÄÇ

That being saidÔºå this is obviously then put through the exponential function„ÄÇ

 and therefore it's a linear decay in the exponential function„ÄÇ So I guess an exponential decay„ÄÇ

And then the third thing is the modulationÔºå and this is where the actual value that the actual what the token is plays a role is then to modulate that point we determined right here„ÄÇ

 modulate it up or down a little bit also in the exponential function„ÄÇ

So that is how RWKV considers the past in generalÔºå it forgets the past in an exponential fashion„ÄÇ

 modulated by the global importance of dimensions and a value that's dependent on the current token„ÄÇ

All rightÔºå so it has the same trade offs as these attention free transformers„ÄÇNow„ÄÇ

 what do we do with this with these things somewhere we haveÔºå we have„ÄÇ

 sorry for scrolling around so heavily„ÄÇOkay„ÄÇ„Åù„ÅÜ„ÄÇThis is how the model is composed„ÄÇ

 This is a recurrent application of the model„ÄÇ So we see the same model applied to three tokens in succession„ÄÇ

 So the my name is„ÄÇBobÔºå okayÔºå you input my and you're trying to make it output name„ÄÇ

 then you input nameÔºå so you input my nameÔºå you're trying to make it output is then you input my name is you're trying to make it output Bob„ÄÇ

So it's three applications of the same modelÔºå so the model isn't composed of these three columns„ÄÇ

 but the model is composed of one column and then we just apply that over and over again„ÄÇ

You see it has a beginningÔºå essentially„ÄÇWhich is a token embedding„ÄÇ It has an end„ÄÇ

 which is a language modeling headÔºå which is a fully connected or a stack of fully connected layers that just maps into the vocabulary„ÄÇ

 But in the be in the middleÔºå it's composed of„ÄÇARecurrent„ÄÇOr sorry of a series of layers„ÄÇ

 So there is in each layerÔºå there is always sorry in each layer„ÄÇ

 there is always a time mix module and a channel mix module„ÄÇ

 So the time mix module being here and the channel mix module being here„ÄÇ

 And those are repeated time mix channel mix time mix channel mix and so on„ÄÇ And then on top„ÄÇ

 there is this language modeling head„ÄÇSo what are the two blocks„ÄÇ

 This is a schematic of how the two blocks look like„ÄÇ I know this is a bit smallÔºå but the„ÄÇ

Time mixing is down hereÔºå and the channel mixing is down here„ÄÇ

 We're going look at this in a bit of detail in the mathÔºå But observe right here„ÄÇ

 what you always have is you have the input signal„ÄÇYou're going to compute R from it„ÄÇ R is a value„ÄÇ

That is going to be used as„ÄÇA for like a gateÔºå a forget gate„ÄÇ

 So R always defines how much of whatever is in coming here or whatever is in coming from here„ÄÇ

 How much of that do I want to retain and send up to the next layer„ÄÇ

So as you can also see or is computed over here as well„ÄÇSo for every of one of these blocks„ÄÇ

 we're going to do a computation over here or over here„ÄÇ

 and then we're going to have a decision made by this branch of the computation of how much of that we even want to accept and then send up to the next layer„ÄÇ

So that's the purpose of the left branches of these computation„ÄÇ

 There is residual signal across all of these things right here„ÄÇ So that kind of mimics the„ÄÇ

The state of„ÄÇOf an LSTMÔºå maybeÔºå but in aÔºå in in an upwards way„ÄÇ So in a layer to layer way„ÄÇ

 So we always have a residual module„ÄÇ We also have a forget gate in front of adding it to the residual signal„ÄÇ

What does these two modules look like So actuallyÔºå let's first go to the channel mixing block„ÄÇ

The channel mixing block is very reminiscent of„ÄÇKind of feet forwardÔºå maybe layers„ÄÇ

 So what we have is ignoreÔºå ignore this part for a moment right here„ÄÇAs I said„ÄÇ

 the R is computed from„ÄÇThe input X„ÄÇAnd just a linear layer„ÄÇ So x times a matrix that's R„ÄÇ

 So that's a linear layer that defines R„ÄÇ Then we have K also x times a matrix„ÄÇ

It's a very simple fit forward layers right here„ÄÇThen WÔºå which is this part right hereÔºå that's no„ÄÇ

 sorry V„ÄÇV is this part right here„ÄÇYou can see that's a nonlinearity and the nonlinearity here is the squared relu„ÄÇ

 the squared relu nonlinearity on top of kÔºå and again a linear layer„ÄÇAnd at the end„ÄÇ

 we are doing that element wise multiplication by this signal right here„ÄÇ

 so the R pushed through the sigmoid here is that forget gate that we talked about„ÄÇ

But you can see it's a„ÄÇüòäÔºåEssentially„ÄÇIf you follow if you follow the signal through the actual path of signal„ÄÇ

It starts here xÔºå well that's a funky„ÄÇX„ÄÇIt's multiplied by a matrix„ÄÇ

 so a linear layer that becomes K that's put through a non linearity„ÄÇ

 then multiplied by another linear layer„ÄÇThat becomes the value V„ÄÇ

And then send through the forget gate„ÄÇ So it's essentially„ÄÇ

A feat for neural network with one non linearity„ÄÇ And at the end„ÄÇ

 a forgetgate as like another non linearity„ÄÇAnd that's it„ÄÇ That's the channel mixing module„ÄÇ

 I guess it's channel mixing because this matrixÔºå the the linear layersÔºå they doÔºå in fact„ÄÇ

 mix the channelsÔºå which means that every dimension sort of can get inputs from every other dimension„ÄÇ

 which is is a big of a feet forward network„ÄÇ NowÔºå I've crossed out all of this stuff in the back right here„ÄÇ

 So we should talk about this as well„ÄÇWhat they do is something called time shift or token shift„ÄÇ

 or I believe that's„ÄÇOne of them„ÄÇToken or time shift„ÄÇAnd that is„ÄÇ

 they always not only take the input to the current layer at this particular time step„ÄÇ

 they also always take the input from the last time step„ÄÇ

 and the linearly interpolate between the two„ÄÇ So you can see here mu and1 minus mu the mu and 1 minus mu or either hyperparameter or their learned parameters„ÄÇ

 but they are per operation„ÄÇ So mu R here is a parameter for the general computation of this R is not dependent on the particular data point„ÄÇ

 Only this and this are dependent on the data point with X T being the current input to the layer and X t minus-1 being the last step input to the layer„ÄÇ

 So this is pretty interesting because it means that we not only always only take the current input and the hidden state from before like in a general R and N„ÄÇ

 but we always take the current input the last input and„ÄÇThis layerÔºå that's it„ÄÇ

 But in the time mixing module will then take the hidden state onto these„ÄÇ

 That's why in this diagramÔºå you see these these lines right hereÔºå these diagonal lines„ÄÇ

Are the token shift lines„ÄÇ So you see this channel mix module is going to take the current input„ÄÇ

 whatever it gets from the lower layers or the original signal and the input to the last to the same layer at the last time step so„ÄÇ

Current input and input to the last time step„ÄÇ And that also goes in„ÄÇ

 These two things are linearly interpolatedÔºå and that then is the input quote unquote to the current layer„ÄÇ

 So it's the interpolation of the last step and this step input„ÄÇNo„ÄÇ

 it's not like we don't mix like the internal states right here„ÄÇ

 we mix the inputs before they go into the layer„ÄÇNow let's look at the time mix„ÄÇ

 you can see there is also this token shift happening„ÄÇ so this token shift„ÄÇ

 that's just something you can doÔºå I guess if you have a a one directional sequence that you need to predict„ÄÇ

 you can do this token shifting but„ÄÇWhat's also interesting is we have a second line„ÄÇ

 which are these states„ÄÇ So how does that workÔºå and that's going to be now the actual recurrent part of this model right here„ÄÇ

You can again see here we're always working with the token shiftÔºå we never just work with the input„ÄÇ

 but you can just think of these things here always as like XÔºå sorry„ÄÇX tilde„ÄÇ

 where x tilde is a mix between the current„ÄÇInput and the last layers input„ÄÇSo we have we compute R„ÄÇ

 which is it's just x tilde times w times a feet forward layer„ÄÇ

 and that again becomes the forget gate down here with an elementwise multiplication„ÄÇ

 we do have an output layerÔºå an output sorry we do have an output feet forward layer kind of a projection that's I'm going to guess they put that in because it was advantageous to do so it can also change the dimensionality and whatnot„ÄÇ

Then we're going to compute two things K and V„ÄÇ So you'll notice before„ÄÇ

Whatever was called V was produced from KÔºå but now both K and V are produced from X„ÄÇ

I don't know why they call them the sameÔºå probably to keep as much in line with the transformer terminology as possible„ÄÇ

 but it's really there's no relation between like the V here and the V before the K is computed similarly„ÄÇ

So this block right hereÔºå the K and the V are computed„ÄÇ

Modular the time shift as they are computed in the original transformer architecture„ÄÇ

Which is just the input times a linear layer„ÄÇThen what happens is interesting then„ÄÇ

As you can see right hereÔºå we go into thisÔºå into this weighted sum„ÄÇ So youll„ÄÇ

 you'll see something familiar here„ÄÇThat's await weightedÔºå await weight and V TÔºå That's the values„ÄÇ

 So VÔºå we computed here„ÄÇ So we're going to look for a weighted sum of the values B„ÄÇButÔºå ohÔºå sorry„ÄÇ

 NoÔºå noÔºå no„ÄÇ forgetget that„ÄÇWe're not only going to look for a weight that sum of the value V because you also see here are V's„ÄÇ

 but these are VsÔºå and this is V T„ÄÇ The VsÔºå in factÔºå are the past values„ÄÇ

So we're going to look for a weighted sum across the entire„ÄÇAsked„ÄÇAnd that's actuallyÔºå ohÔºå sorry„ÄÇ

 It's actually the same„ÄÇAs before„ÄÇYes„ÄÇBlan me back up„ÄÇSo that I here only goes to t minus1„ÄÇ

So you can see that we sum the Vs hereÔºå and then at the end we also sum the VT„ÄÇ

The only reason that there is a difference is this U right here is a different parameter than those ws„ÄÇ

 but is in essenceÔºå it's again a weighted sum over all the values and the values are from the entire sequence„ÄÇ

So so farÔºå we've just considered the current time stepÔºå the current inputÔºå and yes„ÄÇ

 the last steps inputÔºå but in this step right hereÔºå we consider the entire past„ÄÇ

And we want a weighted sum„ÄÇAcross the values of the entire past„ÄÇ

 like in these attention free transformers and whatnot„ÄÇ

 But because we now no longer are limited by having this fixed size„ÄÇ

 this fixed size attention matrixÔºå right hereÔºå or even if it's learnedÔºå right„ÄÇ

 even if it's not an attention matrix in the attention for transformers„ÄÇ

 it was still a fixed size because we're no longer limited because all we do is we say how important is each dimension„ÄÇ

And how does it decay with time in the past„ÄÇ

![](img/d1c78117f4e238ce89b5b7dfa803986a_9.png)

That does not is not limited back in time„ÄÇ It just gets really small back in time„ÄÇ

 but it is not limitedÔºå and therefore„ÄÇWe can do this until perpetuity„ÄÇ

 And especially we can do it until I equals one„ÄÇ So going back to the very first token„ÄÇ

 So for every token that we need to do inference for„ÄÇ This step„ÄÇ

 this value right here will be awaited sum across the values of the entire pastÔºå right„ÄÇ

 and you can see easily that you can do this in a recurrent fashionÔºå this is aÔºå it's a soft max„ÄÇ

 And you can see there is exponentials that here and„ÄÇüòäÔºåThese are multiplied by the values„ÄÇ

 And down here we go just over some of the exponentials„ÄÇ So it is a soft max„ÄÇ However„ÄÇ

 you can just keep track of the the numerators and the denominators separately„ÄÇ

 And then that becomes your hidden state and you can pass that forward„ÄÇ

 So you just grab this sorry right here and this down here and before dividing them„ÄÇ

 you just pass them on right and then in the next stepÔºå you simply add something on top„ÄÇ

 divideiv them for the current stepÔºå but then pass on the hidden states separately So that's what they mean by states if they say„ÄÇ

They don't mean the United States„ÄÇ I'm sorry„ÄÇ They mean these values here that you need„ÄÇ

 You can compute this in a recurrent fashionÔºå or you can compute this in a parallel fashion„ÄÇ

So just to finish here quicklyÔºå this value hereÔºå this weighted sum over all the values of the past is then„ÄÇ

Feit fed into this forget gateÔºå as you see hereÔºå and the output is computed from it„ÄÇNow„ÄÇ

 multiple things to note right here„ÄÇ multiple things to note Note that the aggregation over the past here contains essentially no non linearity„ÄÇ

 right because„ÄÇVÔºå the thing that is being aggregated or in generalÔºå these hidden states„ÄÇ

 they're just produced as a linear function of a linear interpolation of the inputs„ÄÇ right„ÄÇ

 There is nowhere where the previous state goes through and on linearity in order to compute the next hidden state„ÄÇ

 You can essentially track this as a„ÄÇA big sum„ÄÇ So as as a list„ÄÇ

 you can track it as a list and then do the sum or you can just track it as the sum„ÄÇ

 which also means that„ÄÇThe parallelism of training this becomes feasible again„ÄÇ

 so you can train these in parallel because it's just all a big sum that you can compute for an entire batch and an entire sequence at the same time„ÄÇ

AndÔºå yesÔºå alsoÔºå you can use it in an or and fashion where you do it step by step„ÄÇ

 But because that's because it has no non nonlinearities in between„ÄÇ it's it literally just a sum„ÄÇ

 And that's also why youÔºå why you see what I meanÔºå This is essentially a convenet„ÄÇ And I mean„ÄÇThis„ÄÇ

 it has two partsÔºå rightÔºå The first part is this stuff right hereÔºå this token shift„ÄÇ

 Look at the diagram and the diagram you clearly see„ÄÇIf you are this element right here„ÄÇ

 what do you have access toÔºå you have access to this„ÄÇAnd this„ÄÇOh oh by extension„ÄÇ

 you have access if you just go via the token shift„ÄÇ

 you have access to this and this right and so from the lower layer to this and this„ÄÇRight„ÄÇ

 so by here„ÄÇ So you have a receptive field that grows with depth„ÄÇ rightÔºå If we had another layer„ÄÇ

 the receptive field would grow again„ÄÇ So the token shift itself is already is sent very directly„ÄÇ

 a convenet and you„ÄÇYou knowÔºå you only have known nonlinearities as you cross these layer boundaries right here„ÄÇ

 otherwise it's essentially just a linear interpolation„ÄÇ

 which is exactly a convolution with the kernel being mu„ÄÇAnd 1 minus mu„ÄÇ

 That's your convolutional colonel„ÄÇ subsseÔºå2„ÄÇ You slide it over„ÄÇ

So that that defines a con convolution and the same thing for these things right here„ÄÇ

 that is very reminiscent of if you know about these like S4 or state space models and so on„ÄÇ

 which essentially what they do is they define a way to linearly aggregate the pastÔºå right„ÄÇ

 which is exactly what this big sum is right hereÔºå they define a way to weighted sum across the past„ÄÇ

 that in between has no no nonlinearitiesÔºå So you can just track it like this„ÄÇAnd„ÄÇYeahÔºå so„ÄÇAgain„ÄÇ

 andÔºå and S4 is essentially like a big convolution„ÄÇ

So if you want to think about this model in another way than a transformer or an RN„ÄÇ

 that there are not already enough waysÔºå it's essentially a big conve„ÄÇ

Particular in this way right hereÔºå it's a conve that has sort of an infinitely long convolution into the past or until the beginning of the sequence„ÄÇ

 I guess„ÄÇ And you the way it's done is there there is a standard con kernel„ÄÇ

 And then that's modulated by these K values right here„ÄÇAll rightÔºå so that is„ÄÇHow that works„ÄÇ

 I hope I've made this a bit clear the why this is called channel mixing and this one isn't called channel mixing„ÄÇ

 like this is just as much channel mixing as the other one isÔºå the only difference is that down here„ÄÇ

 there is kind of a non linearity within the layer„ÄÇAnd there is andÔºå and hereÔºå we have this„ÄÇ

Aggregation over timesÔºå I guess calling this time mixing is fair„ÄÇ

But this is just as much channel mixingÔºå because„ÄÇThese feet forward layersÔºå they mix the channels„ÄÇ

YeahÔºå but that's that's naming that doesn't doesn't really matter„ÄÇ So they specify here„ÄÇ

 this can be used in time parallel mode complexity of processing a batch of sequences in a single layer is this so you can process a batch as a batch„ÄÇ

 right„ÄÇSo it requires they say meanwhile updating attention scores requires a serial scan and has a complexity of this they've implemented this in a custom kuda kernel„ÄÇ

 you can actually go look at the code I've done that and it's fairly easy to understand the kuda code is one of the more understandable pieces of kuda code and you just write this function Kuda takes care of sort of parallelzing that and putting that across cores and workers processes and so on„ÄÇ

The element wise computation is time dependent but can be readily parallellyzed along the other two dimensions„ÄÇ

 On the other handÔºå you can also use this as in a time sequential mode„ÄÇ

Can be conveniently formulated recursively for decoding during inference„ÄÇÂì¶Âì¶„ÄÇMai„ÄÇ

Connection here is spasing out„ÄÇOne second„ÄÇ

![](img/d1c78117f4e238ce89b5b7dfa803986a_11.png)

And we're back„ÄÇYeahÔºå so they say each output token is dependent only on the last state„ÄÇ

Which brings obviously all the advantages and disadvantages of R N Ns with it„ÄÇ SoÔºå again„ÄÇ

 we can only consider information coming through this bottleneck of the hidden state„ÄÇ

 But I feel because it's this big sum of aggregation is essentially a weighted sum across the past and not non nonlinearity across non nonlinearity across non nonlinearity„ÄÇ

 It can much more easily look back into the past inÔºå but it can do so in a linear fashionÔºå right„ÄÇ

 So I feel this is„ÄÇAmong all the situations like TransformerÔºå LSTM and this one„ÄÇ

 this is probably the weakest form„ÄÇOf being able to look into the past with nuance„ÄÇ

 you can look into the pastÔºå but you can only do so in like a general fashion„ÄÇ

 whereasas a transformer can go look into the past and have exactly the same amount of detail as it does for the recent past„ÄÇ

 So it can look into the long past as long as it's within the context and do exactly the same computation there as it can do for the recent past or for the current token„ÄÇ

 whereas an LSTM can't do that„ÄÇ It can't look into the past at all„ÄÇ However„ÄÇ

 it can do a lot of considered computation in each step before it saves it to the hidden state„ÄÇ

 So it's weaker because it can't go back but stillÔºå it can do a lot of complex computation„ÄÇ

 This model right here„ÄÇüòäÔºåIt can look kind ofÔºå it also goes through the hidden state„ÄÇ

 but it can look the easiest much more easily into the past as an LSTM because it's just this weighted sum instead of nonlinearity after nonlinearity„ÄÇ

 but it kind of has the weakest form of computation that it does in in the exact moment„ÄÇ

I hope that makes a lot of sense that is not a scientific statement that is just me trying to ramble„ÄÇ

Maybe I'm also totally wrong about this„ÄÇ or maybeÔºå you know„ÄÇ

 you can easily make up for that by stacking a lot of layers because now this model is being able to be stacked really heavily and be scaled really heavily„ÄÇ

 And that is probably enough to make up for all the lack of computation in the individual cell„ÄÇ

 It's just like a let's just stack the stuff„ÄÇYeahÔºå another property that I have mentioned„ÄÇ

 but is not entirely maybe come through is the fact that they always compute from the inputs so they don't take„ÄÇ

 they don't take necessarily the the hidden states over so but all the functions are like linear functions or exponentials of linears of the inputs„ÄÇ

 So there's no like non nonlinearity in between that time aggregation and the„ÄÇ

And where and the inputs it themselves to the layer„ÄÇSorry enoughÔºå Rammbling„ÄÇ

 here you can see scaling behaviorsÔºå veryÔºå very beautifulÔºå cumulative time during text generation„ÄÇ

 as the tokens go upÔºå obviously this model has a linear scaling where everything else goes b„ÄÇüòäÔºåÊòØ„ÄÇ

The experimental evaluations are also really interesting„ÄÇ

 at least at the data sets that they have considered right here„ÄÇ it can hold its own„ÄÇ

 Sometimes it's a bit better„ÄÇ sometimes it's a bit worse than other similarly sized transformers„ÄÇ

 but it it performs„ÄÇAlong the same lines„ÄÇ nowÔºå I have heard people say that the model is qualitatively not as good„ÄÇ

 I have as transformers of the same size„ÄÇ I've heard other people say it is better or for some things„ÄÇ

 it's better„ÄÇThat I don't know„ÄÇ It still has to be shown„ÄÇ

 Also these challenges or these data sets right here„ÄÇ

Don't really show me what I would want to find out„ÄÇ So if I compare this to a transformer„ÄÇ

 what I would want to find out is„ÄÇHow like where is theÔºå where then is the actual differenceÔºå rightÔºü

 And I'm going to guess the difference isÔºå let's compare something that is not in the recent past„ÄÇ

 but a bit more back„ÄÇ NowÔºå if I have to consider something that is a bit more back in the past„ÄÇ

 And if that's a very complex thing„ÄÇWhose or the computation of which„ÄÇ

 Like how I have to treat that depends on the current token„ÄÇ

I can't really tell you now an example for that„ÄÇ but in a situation like this„ÄÇ

 a transformer would be way superior than like any LSTM or this model right here„ÄÇ

 So maybe during programming in certain fashions or if the context for something is only given much later„ÄÇ

But for a lot of applicationsÔºå probably not that important„ÄÇ

They also can show easily how increasing the context length since they can do that now„ÄÇ

 increasing the context context length nicely decreases the loss of language modeling on the pile data set and they give some„ÄÇ

Some suggestions right here„ÄÇ So for exampleÔºå improving computational efficiency by applying a parallel scan in this step to reduce the computational cost to this„ÄÇ

 which would be another improvement that's theoretically possible„ÄÇ

 but I'm going to guess that's not done right now„ÄÇÂóØ„ÄÇThey also discuss the limitations„ÄÇ

 so they're very open about the limitations and all the comparisons right hereÔºå right this is„ÄÇ

The linear attention leads to significant efficiency gains„ÄÇ

 but still it may also limit the model's performance on tasks that require recalling min minnuier information„ÄÇ

Over very long context„ÄÇ OkayÔºå that's essentially what I was trying to describe„ÄÇ But it's„ÄÇ

 it's done in much better words than I did„ÄÇThe recurrent architecture inherently limits its ability to look back at previous tokens„ÄÇ

 that's like an RN„ÄÇ And they also discovered that there is an increased importance of prompt engineering comparison to standard transformer models„ÄÇ

 So there are multiple hypothesesÔºå the one they give right here is the linear mechanism limits the information from the prompt that will be carried over to the model's continuation as a result carefully designed prompts maybe even more crucial for the model to perform well on tasks„ÄÇ

 it could be it could be that that is the reason there could also be other reasons maybe this model overfits a bit more so or is less generalizable„ÄÇ

 and that's why changing the prompt really matters more although maybe not„ÄÇ

mIt's just one of these things where there is probably a thousand possible explanations of why with this model„ÄÇ

The getting the prompt right really matters a lot more than in a transformer„ÄÇBut I I wouldn't put my„ÄÇ

 my bet on any one of those until we have really good experimental confirmation„ÄÇ

 There's usually a lot of„ÄÇA lot of„ÄÇI shall I say„ÄÇThere's a lot of Okhamm's razor that should be done„ÄÇ

AlrightÔºå the last thing I wanted to show was this experiment right hereÔºå which I found really cool„ÄÇ

 So the first one is this time decayÔºå so„ÄÇüòäÔºåComicate sorted along channel axis„ÄÇ

 which is where you can see the the difference„ÄÇ So here is the channel„ÄÇ This is the dimension„ÄÇ

 right This is the dimension„ÄÇ We're looking at W right here„ÄÇ

 This vector W how important is the past in layer 1 you can see as far as I can tell the past is not that important„ÄÇ

So for many of the channelsÔºå the past is kind ofba for some of the channels„ÄÇ

 it's sorted by that value„ÄÇ So for some of the channels„ÄÇThe past is important forÔºå for a lot of them„ÄÇ

 It's really not really not important„ÄÇ And you can see as you go up the layers of this network„ÄÇ

 they more and more and more consider the past„ÄÇAnd then what's interesting is the drop off right here„ÄÇ

 So you have some channels really specializing in near term information„ÄÇ

 but a lot of channels really looking back at at the long time into the back„ÄÇ

 So the W will define almost no drop off whereas at the lower layers you have much more local information that the thing considers„ÄÇ

 So I found that pretty cool to see that visualized and to see that progression as in a trained model as you move up the layers„ÄÇ

 The second visualization hereÔºå it's an example„ÄÇ So here you have a input of tokens„ÄÇ

 the Eiffel Tower is located in the city of and then they look at how likely is the word Paris right here„ÄÇ

üòäÔºåAnd what they do is they„ÄÇChange„ÄÇIn each layerÔºå they change in each layer„ÄÇ

 they they swap out the weights or they disturb the weights„ÄÇ

 and they see how much influence does that have on the probability of the word Paris appearing„ÄÇ

 This is a way we've previously looked at that in the paper on RomeÔºå I believe„ÄÇ

 was the technique where you can it's a way to figure out what are the important information paths that you're considering„ÄÇ

 So in this caseÔºå you can clearly see„ÄÇAfter the I fellÔºå we see that layers 1 to like„ÄÇ

20 or so light upÔºå right„ÄÇ And after thatÔºå it's what'sÔºå what's cool is„ÄÇSoÔºå after that„ÄÇOnly layers„ÄÇ

 whatever 21Ôºå22 and 23 light up until the end right here„ÄÇ So only these light up„ÄÇ

 which means that you canÔºå you can disturb these lower layers right here„ÄÇ

 because the information that Paris should probably is a very likely word has already passed from the lower layers to the higher layers right all the way up and is then stored and carried along in the hidden states of these higher layers across these tokens„ÄÇ

 such that the information in the lower layers is only mildly relevant to the output right here„ÄÇ

 Of courseÔºå it is relevant„ÄÇ There is not zero values right hereÔºå but it is mildlyÔºå mildly relevant„ÄÇ

So I thought that was a pretty cool visualization of what was going on in this model„ÄÇ

 Let me quickly scroll through and see if I forgot anything„ÄÇ I did not„ÄÇ

 There are some examples at the endÔºå which is pretty cool„ÄÇ The models are available„ÄÇ

 You can go check them out„ÄÇ you can go test them„ÄÇ the code base is also„ÄÇ

 I found it to be fairly understandable„ÄÇ So if you want to go look at thatÔºå it's also available„ÄÇ

 And yesÔºå my„ÄÇüòä„ÄÇ

![](img/d1c78117f4e238ce89b5b7dfa803986a_13.png)

Thing is spasming out again„ÄÇ That's where I'll end it„ÄÇ Go check out fully connected„ÄÇ again„ÄÇ

 code is in the description to get you some discount If you're in San Francisco„ÄÇ June 7„ÄÇ

 I unfortunately will not be thereÔºå but manyÔºå manyÔºå manyÔºå many cool people will be„ÄÇ How is it„ÄÇ



![](img/d1c78117f4e238ce89b5b7dfa803986a_15.png)

![](img/d1c78117f4e238ce89b5b7dfa803986a_16.png)

![](img/d1c78117f4e238ce89b5b7dfa803986a_17.png)

![](img/d1c78117f4e238ce89b5b7dfa803986a_18.png)

Thank youÔºå bye bye„ÄÇ