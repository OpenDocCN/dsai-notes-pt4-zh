# ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P10Ôºö10.Represent part-whole hierarchies in a neural network, Geoff Hinton - life_code - BV1X84y1Q7wV

![](img/a6e82a7e40b8f10bedf5eedaad6d0492_0.png)

Before we start I gave the same talk at Stanford quite recently„ÄÇ

 I suggested to the people inviting me I could just give one talk and both audiences come but they will prefer it as two separate talks so if you went to this talk recently I suggest you leave now you won't learn anything new„ÄÇ



![](img/a6e82a7e40b8f10bedf5eedaad6d0492_2.png)

Okay„ÄÇÂóØ„ÄÇWhat I'm going to do is combine some recent ideas in neural networks„ÄÇ

To try to explain how a neural network could represent parthole hierarchies„ÄÇ

Without violating any of the basic principles of how neurons work„ÄÇAnd I'm going to„ÄÇ

IEx these ideas in terms of an imaginary system„ÄÇI started writing a design document for a system and in the end I decided the design document by itself was quite interesting„ÄÇ

 so this is just vapourware stuff that doesn't exist little bits of it not exist but„ÄÇ

Somehow I find it easy to explain the ideas in the context of an imaginary system„ÄÇ

So most people now studying neural networks are doing engineering and they don't really care if it's exactly how the brain works„ÄÇ

 they're not trying to understand how the brain worksÔºå they're trying to make cool technology„ÄÇ

And so 100 layers is fine in a ressonnetÔºå weight sharing is fine in the convolutionary neuralette„ÄÇ

Some researchersÔºå particularly computational neuroscientistsÔºå investigate neural networks„ÄÇ

 artificial neural networks in an attempt to understand how the brain might actually work„ÄÇ

I think weve still got a lot to learn from the brain„ÄÇ

And I think it's worth remembering that for about half a century„ÄÇ

 the only thing that kept research on neural networks going was the belief that it must be possible to make these things learn complicated things because the brain does„ÄÇ

So„ÄÇEvery image has a different pass treeÔºå that is the structure of the holes and the parts in the image„ÄÇ

And in a real neural networkÔºå you can't dynamically allocate„ÄÇ

 you can't just grab a bunch of neurons and sayÔºå okayÔºå you now represent this„ÄÇ

Because you don't have random excess memoryÔºå you can't just set the weights of the neurons to be whatever you like What a neuron does is determined by its connections and they only change slowly„ÄÇ

At least probably mostly the change slightly„ÄÇÂóØ„ÄÇSo the question is if you can't change what neurons do quickly„ÄÇ

How can you represent a dynamic past treeÔºüIn symbolic AI it's not a problem„ÄÇ

 you just grab a piece of memory that's what it normally amounts to and say this is going to represent a node in the past and I'm going to give it pointers to other nodes„ÄÇ

 other bits of memory that represent other nodesÔºå so there's no problem„ÄÇFor about five years„ÄÇ

 I played with a theory called capsules„ÄÇWhere„ÄÇYou say because you can't allocate neurons on the fly„ÄÇ

 you're going to allocate them in advanceÔºå so we're going to take groups of neurons and we're going to allocate them to different possible nodes in a poitory„ÄÇ

And most of these groups of neurons for most images are going to be silent„ÄÇ

 a few are going to be active„ÄÇAnd then the ones that are active„ÄÇ

 we have to dynamically hook them up into a past tree„ÄÇ

 so we have to have a way of roing between these groups of neurons„ÄÇSo that was the capsules theory„ÄÇ

And I had some very competent people working with me who actually made it work„ÄÇ

 but it was tough going„ÄÇMy view is the side ideas want to work and some ideas don't want to work and capsules were sort of in between things like back propagation just want to work you try them and they work there's other ideas I've had that just don't want to work capsules were sort of in between and we got it working„ÄÇ

But I now have a new theory that could be seen as a funny kind of capsules model in which each capsule is universal„ÄÇ

 that is instead of a capsule being dedicated to a particular kind of thing„ÄÇ

Each capsule can represent any kind of thing„ÄÇBut hardware still comes in capsules„ÄÇ

Which are also called embedding sometimes„ÄÇSo„ÄÇThe imaginary system I'll talk about is called Gm„ÄÇ

And in Gam„ÄÇHardware gets allocated to columns„ÄÇAnd each column contains multiple levels of representation of what's happening in a small patch of the image„ÄÇ

So within a columnÔºå you might have a lower level representation that says it's a nostril„ÄÇ

And the next level up might say it's a nose and the next level up might say a face„ÄÇ

 the next level up a person on the top level might say it's a partyÔºå that's what the whole scene is„ÄÇ

And the idea for representing part hollow hierarchies is to use islands of agreement between the embeddings at these different levels„ÄÇ

So at the scene levelÔºå at the top levelÔºå you'd like the same embedding for every patch of the image because that patch is a patch of the same scene everywhere„ÄÇ

At the object levelÔºå you'd like the embeddings of all the different patches that belong to the object to be the same„ÄÇ

So as you go up this hierarchÔºå you're trying to make things more and more the same„ÄÇ

And that's how you're squeezing redundancy out„ÄÇThe embedding vectors are the things that act like pointers and the embedding vectors are dynamic„ÄÇ

 they're neural activations rather than neural weights„ÄÇ

 so it's fine to have different embedding vectors for every image„ÄÇ

So here's a little picture if you had a one dimensional row of patches„ÄÇ

These are the columns for the patches„ÄÇAnd„ÄÇYou'd have something like a convolution on neuralness as the front end„ÄÇ

And then after the front end you produce your lowest level embeddings to say what's going on in each particular patch and so that bottom layer of black arrows they all different Of course these embeddings are thousands of dimensions„ÄÇ

Maybe hundreds of thousands in your brain„ÄÇAnd so a two dimensional vector„ÄÇIsn't right„ÄÇ

 but at least I can represent whether two vectors are the same by using the orientation„ÄÇ

So at the lowest levelÔºå all the patches will have different representations„ÄÇBut the next level up„ÄÇ

The first two patchesÔºå they might be part of a nostrilÔºå for example„ÄÇAnd so„ÄÇÂóØ„ÄÇYeah„ÄÇ

 they'll have the same embedding„ÄÇBut the next level up„ÄÇ

The first three patches might be part of a nose„ÄÇAnd so they'll all have the same embedding„ÄÇ

Notice that even though what's in the image is quite different„ÄÇAt the part level„ÄÇ

 those three red vectors are all meant to be the same„ÄÇ

So what we're doing is we're getting the same representation for things that are superficially very different„ÄÇ

üòäÔºåWe're finding spatial coherence in an image by giving the same representation to different things„ÄÇ

üòäÔºåAnd at the object levelÔºå you might have a nose and then a mouse„ÄÇAnd they're the same face„ÄÇ

 they're part of the same face and so all those vectors are the same and this network hasn't yet settled down to produce on the unseen level„ÄÇ

So the islands of agreement are what capture the past tree„ÄÇ

 now they're a bit more powerful than a past treeÔºå they can capture things like shut the heck up„ÄÇ

You can have shut an up can be different vectors at one levelÔºå but at a higher level„ÄÇ

 shut an up can have exactly the same vectorÔºå namely the vector for shut up„ÄÇ

And they can be disconnected so you can do things a bit more powerful than a context free grammar here„ÄÇ

 but basically it's a past true„ÄÇIf you're a physicist„ÄÇ

You can think of each of these levels as an icing model„ÄÇ

With real valued vectors rather than binary spins„ÄÇAnd you can think of them being coordinate transforms between levels„ÄÇ

 which makes it much more complicatedÔºå and then this is a kind of multi level icing model„ÄÇ

But with complicated interactions between the levelsÔºå becauseÔºå for example„ÄÇ

 between the red arrows and the black arrows above them„ÄÇ

 you need the coordinate transform between a nose and a faceÔºå but we'll come to that later„ÄÇ

If you're not a physicistÔºå ignore all that because it won't help„ÄÇ

So I want to start and this is I guess is particularly relevant for a natural language course where you're some of you are not vision people„ÄÇ

By trying to prove to you that coordinate systems are not just something invented by Descarte„ÄÇ

 coordinate systems were invented by the brain a long time ago„ÄÇ

 and we use coordinate systems in understanding what's going on in an image„ÄÇ

I also want to demonstrate the psychological reality of past trees for an image„ÄÇ

So I'm going to do this with a task that I invented a long time ago„ÄÇ

In the 1970s when I was a grad studentÔºå in fact„ÄÇAnd you have to do this task to get that full benefit from it„ÄÇ

So I want you to imagine on the tabletop in front of youÔºå there's a wireframe cube„ÄÇ

And it's in the standard orientation for a cube is resting on the tabletop„ÄÇ

And from your point of view„ÄÇThere's a front bottom right hand corner„ÄÇAnd a top back left hand corner„ÄÇ

 here we go„ÄÇok„ÄÇThe front bottom right hand corner is resting on the tabletop along with the four other corners„ÄÇ

And the top back left hand corner is at the other end of a diagonal that goes through the center of the cube„ÄÇ

OkayÔºå so far so good„ÄÇNow what we're going to do is rotate the cube so that this finger stays on the tabletop„ÄÇ

And the other finger is vertically above it like that„ÄÇüòäÔºåThis finger shouldn't have moved„ÄÇok„ÄÇ

So now we've got the cube in an orientation where that thing that was a body diagon is now vertical„ÄÇ

And all you've got to do is take the bottom finger because that's still on the tabletop and point with the bottom finger to where the other corners of the cube„ÄÇ

So I want you to actually do it off you goÔºå take your bottom finger„ÄÇ

 hold your top finger at the other end of that diagonal that's now be made political and just point to where the other corners are„ÄÇ

And„ÄÇLuckily zoom so most of youÔºå other people won't be able to see what you did and I can see that some of you aren't pointing and that's very bad„ÄÇ

So most people„ÄÇPoint out four other corners and the most common response is to say they're here„ÄÇ

 hereÔºå here and hereÔºå they point out four corners in a square halfway at that axis„ÄÇÂóØ„ÄÇThat's wrong„ÄÇ

 as you might imagineÔºå and it's easy to see that it's wrong because if you imagine the cube„ÄÇ

 the normal orientation„ÄÇAnd camp the cornersÔºå there's eight of them„ÄÇAnd these were two corners„ÄÇ

So where did the other two corners goÔºüSo one theory is that when you rotated the cube„ÄÇ

 the centpetal forces made them fly off into your unconsciousÔºå that's not a very good theory„ÄÇSo„ÄÇ

What's happening here is you have no idea where the other corners are unless you're something like a crystallographer„ÄÇ

You can sort of imagine bits of the cubeÔºå but you just can't imagine this structure of the other corners„ÄÇ

 what structure they form„ÄÇAnd this common response that people give are four corners in a square„ÄÇ

Is doing something very weird„ÄÇIs trying to is saying well okay I don't know I don't know whether it's of a cube bar but I know something about cubes„ÄÇ

 I know the corners come in foursÔºå I know a cube has this fourfold rotational symmetry or two planes of bilateral symmetry but right angle s rather„ÄÇ

And so what people do is they preserve the symmetries of the cube in their response„ÄÇ

They give four corners in a square„ÄÇNowÔºå what they've actually pointed out if they do that is two pyramids„ÄÇ

Each of which has a square baseÔºå one's upside downÔºå and they're stuck base to base„ÄÇ

So you can visualize that quite easilyÔºå a square based pyramid with another one stuck underneath it„ÄÇ

And so now you get your two fingers as the vertices of those two pyramids„ÄÇAnd„ÄÇ

What's interesting about that is„ÄÇYou've preserved the symmetries of the cube at the cost of doing something pretty radical„ÄÇ

 which is changing faces to vertices and vertices to faces„ÄÇ

The thing you pointed out if you did that was an octtoahedron„ÄÇIt has eight faces and six vertices„ÄÇ

 the cube has six faces and eight vertices„ÄÇüòäÔºåSo in order to preserve the symmetries you know about of the cube„ÄÇ

You've if you did thatÔºå you've done something really radicalÔºå which has changed faces forversities„ÄÇ

 andversities for faces„ÄÇÂóØ„ÄÇI should show you what the answer looks like„ÄÇ

 so I'm going to step back and try and get enough light and maybe you can see this cube„ÄÇ

So this is a queue„ÄÇAnd„ÄÇYou can see that the other edges„ÄÇ

Former coding of Zigza ring around the middle„ÄÇSo I got a picture of it„ÄÇ

So the colored rods here are the other edges of the cubeÔºå the ones that don't touch your fingertips„ÄÇ

And your top finger connected to the three vertices of those flaps„ÄÇ

And your bottom fingers connected to the lowest three vertices there„ÄÇ

And that's what a cube looks like is's something you had no idea about this is just a completely different model of a cube it's so different I'll give it a different name I call it a hexahahedron„ÄÇ

üòäÔºåAnd„ÄÇThe thing to notice is a hexahahedron and a cube are just conceptually utterly different„ÄÇ

 you wouldn't even know one was the same as the other if you think about one as hegen and one as a cube„ÄÇ

It's like the ambiguity between a tilted square and an upright diamond„ÄÇ

 but more powerful because you're not familiar with it„ÄÇÂóØ„ÄÇ

And that's my demonstration that people really do use coordinate systems and if you use a different coordinate system to describe things„ÄÇ

 and here I force you to use a different coordinate system by making the diagonal be vertical and asking you to describe it relative to that vertical axis„ÄÇ

Then familiar things become completely unfamiliar„ÄÇüò°„ÄÇ

And when you do see them relative to this new frameÔºå they're just a completely different thing„ÄÇ

Notice that things like convolutional neural nets don't have that„ÄÇ

 they can't look at something and have two utterly different internal representations of the very same thing„ÄÇ

I'm also showing you that you do parsingÔºå so here I've colored it so you pass it into what I call the crown„ÄÇ

 which is three triangular flaps that slope upward withs and outwards„ÄÇHere's a different policy„ÄÇ

The same green flap sloping upwards and outwardsÔºå now we have a red flap sloping downwards and outwards„ÄÇ

And we have a central rectangleÔºå and you just have the two ends of the rectangle„ÄÇN„ÄÇ

 if you perceive this„ÄÇAnd now close your eyes and ask youÔºå were there any parallel edges thereÔºü

You're very well aware that those two blue edges were parallel„ÄÇ

And you're typically not aware of any other parallelesÔºå even though you know by symmetry„ÄÇ

 there must be other pairs„ÄÇSimilarly with the crownÔºå if you see the crown„ÄÇ

 and then I ask you to close your eyes and ask you where the para isÔºå you don't see any parallelurgs„ÄÇ

And that's because the coordinate systems you're using for those flaps don't line up with the edges and you only notice parallels if they line up with the coordinate system you're using so here for the rectangle the parallelurgs align line with the coordinate system for the flaps they don't„ÄÇ

So you're aware that those two blue edges are parallel„ÄÇ

 but you're not aware that one of the green edges and one of the red edges is parallel„ÄÇÂóØ„ÄÇ

So this isn't like the Necker cube ambiguity where when it flips„ÄÇ

 you think that what's out there in reality is differentÔºå things are at a different depth„ÄÇ

This is like next weekend we should be visiting relatives„ÄÇ

So if you take the sentence next weekend we shall be visiting relatives„ÄÇ

 it can mean next weekend what we will be doing is visiting relatives„ÄÇ

 or it can mean next weekend what we will be is visiting relatives„ÄÇ

Now those are completely different sensesÔºå they happen to have the same truth conditions„ÄÇ

 they mean the same thing in the sense of truth conditions because if you're visiting relatives„ÄÇ

 what you are is visiting relatives„ÄÇAnd it's that kind of ambiguity„ÄÇ

 no disagreement around what's going on in the world„ÄÇ

 but two completely different ways of seeing the sentence„ÄÇSo„ÄÇThis is this was drawn in the 1970s„ÄÇ

 this is what AI was like in the 1970s„ÄÇThis is a sort of structural description of the crown interpretation„ÄÇ

So you have nodes for the all various parts in the hierarchy„ÄÇ

I've also put something on the arcs that RWx is the relationship between the crown and the flap„ÄÇ

And that can be represented by a matrix is really the relationship between the intrinsic frame of reference of the chrome and the intrinsic frame of reference of the flap„ÄÇ

üòäÔºåAnd notice that„ÄÇIf I change my viewpointÔºå that doesn't change at all„ÄÇüò°„ÄÇ

So that kind of relationship will be a good thing to put in the weights of a neural network because you'd like a neural network to be able to recognize shapes independently viewpoint„ÄÇ

And that RWX is knowledge about this shapeÔºå that's independent of viewpoint„ÄÇ

Here's the zigzag interpretation„ÄÇAnd here's something else where I've added„ÄÇ

The things in the heavy blue boxes„ÄÇThey're the relationship between„ÄÇüò°ÔºåThe aode and the viewer„ÄÇ

That is to be more explicitÔºå the coordinate transformation between the intrinsic frame of reference of the crown and the intrinsic frame of reference of the viewer„ÄÇ

 your eyeball is that R WV„ÄÇAnd that's a different kind of thing altogether because as you change viewpoint that changes„ÄÇ

 in factÔºå as you change viewpoint all those things in blue boxes all change together in a consistent way„ÄÇ

And there's a simple relationshipÔºå which is that if you take RWVÔºå and you multiply it by RWx„ÄÇ

 you get Rx v„ÄÇSo you can easily propagate viewpoint information over a structural description„ÄÇ

And that's what I think a mental image is rather than a bunch of pixels„ÄÇ

It's a structural description with Associative viewpoint information„ÄÇÂóØ„ÄÇ

That makes sense of a lot of properties of mental images„ÄÇ

 like if you want to do any reasoning with things like RWXÔºå you form a mental image„ÄÇ

That is you fill in that you choose a viewpoint„ÄÇAnd I want to do one more demo to convince you you always choose a viewpoint when you're solving mental imagery problems„ÄÇ

So I'm going give you another very simple mental imagery problem at the risk of running overtime„ÄÇ

Imagine that„ÄÇYou're at a particular point and you travel a mile east and then you travel a mile north and then you travel a mile east again„ÄÇ

 what's your direction back to your starting pointÔºüThis isn't a very hard problem„ÄÇ

 it's sort of a bit south and quite a lot westÔºå rightÔºüIt's not exactly southwest„ÄÇ

 but it's sort of southwest„ÄÇNowÔºå when you did that task„ÄÇ

 what you imagined from your point of view is you went to mile East and then you went to mile north and then you went to mile East again„ÄÇ

I'll tell you what you didn't imagineÔºå you didn't imagine that you went to my East and then you went to my north and then you went to my East again„ÄÇ

You could have solved the problem perfectly well with North not being upÔºå but you had north A„ÄÇ

 you also didn't imagine thisÔºå you go a mile east and then a mile north and then a mile east again„ÄÇ

And you didn't imagine thisÔºå you go mile east and then a mile north and so on„ÄÇ

 you imagined it at a particular scale in a particular orientation and in a particular position„ÄÇüòä„ÄÇ

That's„ÄÇAnd you can answer questions about roughly how big it was and so„ÄÇ

 so that's evidence that to solve these tasks that involve using relationships between things„ÄÇ

 you form a mental image okayÔºå and that form mental imagery„ÄÇ

So I'm now going to give you a very brief introduction to contrastive learning„ÄÇ

So where this is a complete„ÄÇDisconnect in the talkÔºå but I'll come back together soon„ÄÇSo„ÄÇ

In contrast selfive wise learningÔºå what we try and do is make two different crops of an image have the same representation„ÄÇ

ÂóØ„ÄÇThere's a paper a long time ago by Becker and Hinton where we were doing this to discover low level coherence in an image„ÄÇ

 like the continuity of surfaces„ÄÇÊàë„ÄÇThe depth of surfaces„ÄÇ

It's been improved a lot since then and it's been used for doing things like classification„ÄÇ

 that is you take an image that has one prominent object in it„ÄÇAnd you say„ÄÇ

If I take a crop of the image that contains sort of any part of that object„ÄÇ

It should have the same representation as some other crop of the image containing part of that object„ÄÇ

And„ÄÇThis has been developed a lot in the last few years„ÄÇ

 I'm going to talk about a model developed a couple of years ago of my group in Toronto called Sinclair but there's lots of other models and since then things have improved„ÄÇ

So in SimclairÔºå you're taking an image X„ÄÇYou take two different crops and you also do color distortion of the crops„ÄÇ

 different color distortions of each crop„ÄÇAnd that's to prevent it from using color histograms to say they're the same„ÄÇ

So you mess with the color so it can't use color„ÄÇIn a simple way„ÄÇAnd„ÄÇ

That gives you Xi tilde and Xj tilde„ÄÇYou then put those through the same neural network F„ÄÇ

Then you get a representation H„ÄÇAnd then you take your representation H and you put it through another neural network„ÄÇ

 which compresses it a bit„ÄÇIt goes to low dimensionality„ÄÇ

That's an extra complexity I'm not going to explainÔºå but it makes it work a bit better„ÄÇ

You can do it without doing that„ÄÇAnd you get two embedding Z and Zj„ÄÇ

And your aim is to maximize the agreement between those vectors„ÄÇ

And so you start off doing that and you sayÔºå okayÔºå let's start off with random neural networks„ÄÇ

 random weights in the neural networksÔºå and let's take two patches and let's put them through these transformations and let's try and make ZI be the same as ZJ so let's back propagate the squared difference between components of I and components of J„ÄÇ

And heyÔºå PrestoÔºå what you discover is when everything collapses„ÄÇFor every image„ÄÇ

 it will always produce the same ZI and Zj„ÄÇAnd then you realizeÔºå well„ÄÇ

 that's not what I meant by agreementÔºå I meant they should be the same„ÄÇ

When you get two crops of the same image and different when you get two crops of different images„ÄÇ

OtherwiseÔºå there's not really agreementÔºå rightÔºüÂóØ„ÄÇSo you have to have negative examples„ÄÇ

 you have to show crops from different images and say those should be different„ÄÇ

If they're already differentÔºå you don't try and make them a lot more different„ÄÇ

It's very easy to make things very differentÔºå but that's not what you want you just want to be sure they're different enough so crop from different images aren't taken to be from the same image„ÄÇ

 so if they happen to be very similar you push them apart„ÄÇ

And that stops your representations clapsing that's called contrastive learning„ÄÇ

And it works very well„ÄÇSo„ÄÇWhat you can do is do unsupervised learning„ÄÇ

By trying to maximize agreement between the„ÄÇRepresentations you get from two image patches from the same image„ÄÇ

And after you've done thatÔºå you just take your representation of the image patch„ÄÇ

And you feed it to a linear classifierÔºå a bunch of weights so that you multiply the representation by a weight matrix„ÄÇ

 put it through a softmax and get class labels„ÄÇAnd then you train that by„ÄÇGreat descent„ÄÇAnd„ÄÇ

What you discover is that that's just about as good as training on label data„ÄÇ

 so now the only thing you trained on label data is that last linear classifier„ÄÇ

The previous layers were trained on unlabeled data„ÄÇ

And you've managed to train your representations without needing labels„ÄÇ

Now there's a problem with this„ÄÇHe works very nicely„ÄÇ

But it's really confounding objects and whole seas„ÄÇ

So it makes sense to say two different patches from the same scene„ÄÇShould get the same„ÄÇ

Vectctor label at the seam level because they're from the same scene„ÄÇ

But what if one of the patches contains bits of objects A and B and another patch contains bits of objects A and C„ÄÇ

 you don't really want those two patches to have the same representation at the object level„ÄÇ

So we have to distinguish these different levels of representation„ÄÇAnd for contrastive learning„ÄÇ

 if you don't use any kind of gating or attentionÔºå then what's happening is you're really doing learning at the seam level„ÄÇ

What we'd like is that the representations you get at the object level„ÄÇShould be the same„ÄÇ

If both patches are patches from J AÔºå but should be different if one patches from JA and one patches from J B„ÄÇ

 and to do that we're going to need some form of attention to decide whether they really come from the same thing„ÄÇ

And so Glom is designed to do thatÔºå it to I take contrastive learning„ÄÇ

And to introduce attention of the kinds you get in transformers in order not to try and say things are the same when they're not„ÄÇ

I should mention at this point that most of you will be familiar with Bert„ÄÇ

And you could think of the word fragments that are fed into Bert as like the image patches I'm using here„ÄÇ

And in BtÔºå you have that whole column of representations of the same word fragment„ÄÇIn book„ÄÇ

 what's happening presumably as you go up is you're getting„ÄÇSemanically richer representations„ÄÇ

But in BurtÔºå there's no attempt to get representations of larger things like whole phrases„ÄÇÂóØ„ÄÇ

This one I'm going to talk about will be a way to modify BchÔºå so as you go up„ÄÇ

 you get bigger and bigger islands of agreement„ÄÇSo for exampleÔºå after a couple of levels„ÄÇ

 then things like New and York will have the different fragments of York„ÄÇ

 I suppose it's got two different fragmentsÔºå will have exactly the same representation if it was done in the G right„ÄÇ

And then as you go another level„ÄÇThe fragments of new or news probably are thin in its own right„ÄÇ

 but the fragments of York would all have exactly the same representation„ÄÇ

That had this island of agreement and that will be a representation of a compound thing and as you go up you're going to get these islands of agreement that represent bigger and bigger things and that's going to be a much more useful kind of bird because instead of taking vectors that represent word fragments and then sort of muning them together by taking the max of each for example the max of each component for example„ÄÇ

 which is just a crazy thing to do you'd explicitly as you're learning form representations of larger parts and the parthole hierarchy„ÄÇ

ok„ÄÇSo what we're going after in Glom is a particular kind of spatial coherence that's more complicated than the spatial coherence caused by the fact that surfaces tend to be at the same depth and same orientation in nearby patches of an image„ÄÇ

We're going after the spatial coherence„ÄÇUThat says that if you find a mouth in an image and you find a nose in an image and then the right spatial relationship to make a face„ÄÇ

 then that's a particular kind of coherence„ÄÇAnd we want to go after that unsupervised„ÄÇüòä„ÄÇ

And we want to discover that kind of coherence in images„ÄÇSo before I go into more details of Alom„ÄÇ

 I want to disclaim that„ÄÇÂï±„ÄÇFor yearsÔºå computer vision treated vision as you've got a static image a uniform resolution and you want to say what's in it„ÄÇ

That's not how vision works in the real world in the real world„ÄÇ

 this is actually a loop where you decide where to look„ÄÇIf you're a person or a robot„ÄÇ

You better do that intelligently„ÄÇAnd„ÄÇThat gives you a sample of the objectic array„ÄÇ

 it turns the objectic arrayÔºå the incoming light„ÄÇInto a retal image and on your retina„ÄÇ

 you have high resolution in the middle and low resolution around the edges„ÄÇ

And so you're focusing on particular details and you never„ÄÇ

 ever process the whole image a uniform resolution„ÄÇ

 you're always focusing on something and processing where you taking at high resolution and everything else at much lower resolution„ÄÇ

 particularly around the edges„ÄÇSo I'm going to ignore all the complexity of how you decide where to look and all the complexity of how you put together the information you get from different extensions by saying let's just talk about the very first fixation on a novel image so you look somewhere and now what happens on that first fixation„ÄÇ

We know that the same hardware in the brain is going to be reused for the next fixation„ÄÇ

 but let's just think about the first fixation„ÄÇSo finallyÔºå here's a picture of the architecture„ÄÇ

And this is„ÄÇüòäÔºåThe architecture„ÄÇFor a single locationÔºå so like for a single word fragment in Bt„ÄÇAnd„ÄÇ

It shows you what's happening for multiple framesÔºå so Gom is really designed for video„ÄÇ

 but I only talk about applying it to static images„ÄÇ

Then you should think of a static image as a very boring video in which the frames are all the same as each other„ÄÇ

So„ÄÇI'm showing you three adjacent levels in the hierarchy„ÄÇAnd I'm showing you what happens over time„ÄÇ

So if you look at the middle level„ÄÇMaybe that's the sort of major part level„ÄÇ

And look at that box that says level L„ÄÇAnd that's at frame four„ÄÇSo the right hand level L box„ÄÇ

And let's ask how the state of that boxÔºå the state of that embedding is determined„ÄÇSo inside the box„ÄÇ

 we're going to get an embedding„ÄÇÂóØ„ÄÇAnd the embedding is going to be the representation of what's going on at the major part level for that little patch of the image„ÄÇ

And level L in this diagramÔºå all of these embeddings will always be devoted to the same patch of the retinal image„ÄÇ

ok„ÄÇThe level L embedding„ÄÇOn the right hand side„ÄÇYou can see there's three things determining it there„ÄÇ

 there's the green arrow„ÄÇAnd for static imagesÔºå the greenarrow are rather boring„ÄÇ

 it's just saying you should sort of be similar to the previous state of level L„ÄÇ

 so it's just doing temporal integration„ÄÇÁ¨¨‰∏Ä„ÄÇüòäÔºåBlue arrow is actually a neural net with a couple of hidden layers in it„ÄÇ

I'm just showing you the embeddings hereÔºå not all the layers of the neural net„ÄÇ

We need a couple of hidden layerss to do the coordinate transforms that are required„ÄÇ

And the blue arrow„ÄÇIs basically taking information at the level below at the previous time step„ÄÇ

So level L minus1 on frame three might be representing that I think I might be a nostril„ÄÇWell„ÄÇ

 if you think you might be an nostrilÔºå what you predict at the next level up is a nose„ÄÇWhat's more„ÄÇ

 if you have a coordinate frame for the nostrilÔºå you can predict the coordinate frame for the nose„ÄÇ

 maybe not perfectlyÔºå but you have a pretty good idea of the orientation position and scale of the nose„ÄÇ

So that bottom up neural net„ÄÇüò°ÔºåIs„ÄÇA netch that can take any kind of part at level on mine can take an nostril„ÄÇ

 but it could also take a steering wheel and predict the car from the steering wheel„ÄÇ

And predict what you've got at the next level up„ÄÇüò°ÔºåThe red arrow is a top down you're all that„ÄÇSo„ÄÇ

 the red arrow„ÄÇIs predicting„ÄÇThe nose from the whole face„ÄÇ

 and again it has a couple of hidden layers due coordinate transforms„ÄÇ

Because if you know the co frame of the face and you know the relationship between a face and a nose and that's going to be in the weights of that top down you're on net„ÄÇ

Then you can predict that it's a nose and what the pose of the nose is„ÄÇ

And that's all going to be in activities in that embedding better„ÄÇOkay„ÄÇ

Now there's all of that is what's going on in one column of hardware that's all about a specific patch of the image„ÄÇ

 so that's veryÔºå very like what's going on for one word fragment in Bt you have all these levels of representation„ÄÇ

ÂóØ„ÄÇIt's a bit confusing exactly what the ratio of this is to Bt and I'll give you the reference to a long archive paper at the end that has a whole section on how this relates to Bt„ÄÇ

But it's confusing because this has time steps„ÄÇAnd that makes it a little more complicatedÔºå okay„ÄÇ

So those are three things that determine the level andbeddingÔºå but there's one fourth thing„ÄÇ

Which is in black at the bottom there„ÄÇAnd that's the only way in which different locations interact„ÄÇ

And that's a very simplified form of a transformer„ÄÇIf you take a transformer as in BtÔºå and you say„ÄÇ

 let's make the embeddings and the keys and the queries and the values all be the same as each other„ÄÇ

We just have this one vector„ÄÇSo now all you're trying to do„ÄÇ

Is make the level L embedding in one column„ÄÇBe the same as the level L embedding in nearby columns„ÄÇ

But it's going to be gatedÔºå you're only to try and make it be the same„ÄÇIf„ÄÇIt's already quite similar„ÄÇ

So here's how the attention works„ÄÇYou take the level L embedding in location XÔºå that's Alex„ÄÇ

And you take the level only em bedding in the nearby location YÔºå that or why„ÄÇ

You take the scalr product„ÄÇYou expiate„ÄÇAnd you normalizeÔºå in other wordsÔºå you do a softm„ÄÇ

And that gives you the weight to use„ÄÇüò°ÔºåIn„ÄÇYour desire to make„ÄÇLXÔºå be the same as L Y„ÄÇ

So the input produced„ÄÇBy this from neighbors„ÄÇIs an attention weighted average of the level level embedding of nearby columnsÔºü

And that's an extra input that you get is trying to make you agree with nearby things and that's what's going to cause you to get these islands of agreement„ÄÇ

So back to this picture„ÄÇI think„ÄÇYeah„ÄÇThis is what we'd like to see„ÄÇüò°ÔºåAnd the reason„ÄÇ

We get those that big island of agreement at the object level„ÄÇ

Is because we're trying to get agreement thereÔºå we're trying to learn the coordinate transform„ÄÇ

From the red arrows to the level above and from the green arrows to the level above„ÄÇ

 such that we get agreement„ÄÇok„ÄÇNowÔºå one thing we need to worry about„ÄÇ

Is that the difficult thing in perception„ÄÇÂóØ„ÄÇIt's not so bad in language„ÄÇ

 it's probably worse than visual perceptionÔºå is that there's a lot of ambiguity„ÄÇ

If I'm looking at a line drawingÔºå for exampleÔºå I see a circle„ÄÇ

Well a circle could be the right eye of a face or it could be the left eye of a face or it could be the front wheel of a car or the back wheel of a car there's all sorts of things that that circle could be„ÄÇ

And we'd like to disambiguate the circle„ÄÇAnd there's a long line of work„ÄÇ

usingsing things like markco random fieldsÔºå here we need a variational mark random field„ÄÇ

 which I'll call a transformational random field„ÄÇBecause the interaction betweenÔºå for example„ÄÇ

 something that might be an eye and something that might be a mouse„ÄÇ

Needs to be gated by corner transforms„ÄÇYou knowÔºå for the„ÄÇ

 let's take a nose on our mouth because that's my standard thing„ÄÇ

If you take something that might be a nose and you want to ask„ÄÇ

 does anybody out there support the IR noseÔºüWellÔºå what you'd like to do is send to everything nearby a message saying„ÄÇ

üò°ÔºåUmÔºå do you have the right kind of pose and right kind of identity to support the idea that I'mknowsÔºü

And so you'd likeÔºå for exampleÔºå to send out a message from the nose„ÄÇ

You'd send out a message to all nearby locations saying does anybody have a mouth with the pose that I predict by taking the pose of the nose„ÄÇ

 multiplying by the coordinate transform between a nose and a mouth and now I can predict the pose of the mouth is there anybody out there with that pose who thinks they might be a mouthÔºü

And I think you can see you're going to have to send out a lot of different messages„ÄÇüòä„ÄÇ

For each kind of other thing that might support youÔºå you're going to send a different message„ÄÇ

 so you're going to need a multi headed„ÄÇtransformformer and it's going to be doing these coordinate transforms and you have to corner transform the inverse transform on the way back because if the mouse supports you what it needs to support is a nose not with the pose of the mouth„ÄÇ

 but with the appropriate pose„ÄÇSo that's going to get very complicated„ÄÇ

 you're going have n squared interactions all with coordinate transforms„ÄÇ

There's another way of doing it that's much simplerÔºå that's called a half transform„ÄÇüòä„ÄÇ

At least it's much simpler if you have a way of representing ambiguity„ÄÇ

So instead of these direct interactions between parts like a nose and a mouth„ÄÇ

What you're going to do is you're going to make each of the parts„ÄÇPredict the whole„ÄÇ

So the nose can predict the faceÔºå and it can predict the pose of the face„ÄÇ

 and the mouth can also predict the face„ÄÇüòäÔºåNow these will be in different columns of G„ÄÇ

 but in one column of GÔºå you'll have a nose preing face„ÄÇIn a nearby column„ÄÇ

 you'll have a mouth predicting a face„ÄÇAnd those two faces should be the same if this really is a face„ÄÇ

So when you do this attention weighted averaging with nearby things„ÄÇ

 what you're doing is you're getting confirmation„ÄÇThat the support„ÄÇFor the hypothesis you've got„ÄÇ

 I meanÔºå suppose to in one column make the hypothesisÔºå it's a face with this poems„ÄÇ

That gets supported by nearby columns that derived the very same embedding from quite different data„ÄÇ

 one derived it from the nose and one derived it from the mouth„ÄÇ

And this doesn't require any dynamic routing„ÄÇBecause the embeddings are always referring to what's going on in the same small patch of the image„ÄÇ

We in a column there's no routing„ÄÇAnd between columns„ÄÇThere's something a bit like rooting„ÄÇ

 but it's just the standard transformer kind of attention„ÄÇ

 you're just trying to agree with things that are similar„ÄÇAnd„ÄÇOkayÔºå so that's how Gs meant to work„ÄÇ

And the big problem is„ÄÇüò°ÔºåThat if I see a circleÔºå it might be a left eyeÔºå it might be a right eye„ÄÇ

 it might be a„ÄÇFrom wheel of a car might be the battery wheel of a car because my embedding for a particular patch at a particular level has to be able to represent anything„ÄÇ

When I get an ambiguous thing I have to do with all these possibilities of what whole it might be part of so instead of trying to resolve ambiguity at the part level what I can do is jump to the next level up and resolve the ambiguity there just by saying things are the same„ÄÇ

 which is an easier way to resolve ambiguity„ÄÇBut the cost of that is I have to be able to represent all the ambiguity I get at the next level up„ÄÇ

Now it turns out you can do that we've done a little toy example where you can actually preserve this ambiguity„ÄÇ

But it's difficultÔºå it's the kind of thing neural nets are good at„ÄÇ

So if you think about the embedding of the next level up„ÄÇ

You've got a whole bunch of neurons whose activities are that embedding„ÄÇ

And you want to represent a highly multimodal distribution„ÄÇ

 like it might be a car with this pose or a car with that pose or a face with this pose or a face with that pose„ÄÇ

All of these are possible predictions for finding a circle„ÄÇAnd so you have to represent all that„ÄÇ

And the question isÔºå can you let's do thatÔºüAnd I think the way they must be doing it is„ÄÇ

Each neuron in the embedding„ÄÇStands for an unnormalized log probability distribution over this huge space of possible identities and possible poses„ÄÇ

 the sort of cross product of identities impose poses„ÄÇÂóØ„ÄÇ

And so the neuron is this rather the log probability distribution over that space„ÄÇ

And when you activate the neuronÔºå what it's saying is add in that log probability distribution to what you've already got„ÄÇ

And so now if you have a whole bunch of load probability distributions„ÄÇAnd you add them all together„ÄÇ

You can get a much more peaky log probability distribution„ÄÇ

And when you expentiate to get a probability distributionÔºå it gets very peaky„ÄÇ

And so very vague basis functions„ÄÇIn this joint space of pose and identity and basis functions in the log probability in that space„ÄÇ

Can be combined to produce sharp conclusions„ÄÇSo„ÄÇI think that's how neurons are representing things most people think about neurons as they think about the thing that they're representing„ÄÇ

But obviously in perceptionÔºå you have to deal with uncertainty and so neurons have to be good at representing multimodal distributions„ÄÇ

And this is the only way I can think of that's good at doing it„ÄÇThat's a rather weak argument„ÄÇ

 I mean it's the argument that led Chomsky to believe that language wasn't learned because he couldn't think of how it was learned„ÄÇ

My view is neurons must be using this representation because I can't think of any other way of doing it„ÄÇ

ok„ÄÇI just said all that because I got ahead of myself because I got excited„ÄÇ

Now the reason you can get away with thisÔºå the reason you have these very vague distributions in the unormalized low probability space„ÄÇ

Is because these neurons are all dedicated to a small patch of image and they're all trying to represent the thing that's happening that patch of image so you're only trying to represent one thing„ÄÇ

 you're not trying to represent some set of possible objects if you're trying to represent some set of possible objects you have a horrible binding problem and you couldn't use these very vague distributions but so long as you know that all of these neurons„ÄÇ

 all of the active neurons refer to the same thing then you can do the intersection„ÄÇ

 you can add the low probability distribution together and intersect the sets of things they represent„ÄÇ

OkayÔºå I'm getting near the endÔºå how would you train a system like thisÔºüWellÔºå obviously„ÄÇ

 you could train it the way you train but you could do deep end to end training„ÄÇAnd for Gm„ÄÇ

 what that will consist of and the way we trained a toy example„ÄÇIs you„ÄÇTake an image„ÄÇ

You leave out some patches of the image„ÄÇYou then let Gom settle down for about 10 iterations„ÄÇ

And is trying to fill in„ÄÇThe lowest level representation of the bo in the image„ÄÇ

The lowest level embedding„ÄÇAnd it fills them in role and so you know back propagate that error and you're back propagating it through time in this network„ÄÇ

 so it will also back propagate up and down through the levels„ÄÇÁÅØ„ÄÇ

So you're basically just doing back proation through time of the error„ÄÇ

Due to filling in things incorrectlyÔºå that's basically how B is trained and you could train G the same way„ÄÇ

But I also want to include an extra bit in the training„ÄÇTo encourage islands„ÄÇÂóØ„ÄÇ

We want to encourage big islands of identical vectors at high levels„ÄÇ

And you can do that by using conrusive learning„ÄÇSo„ÄÇIf you think how the next„ÄÇAt the next time step„ÄÇ

 you think how an embedding is determined„ÄÇIs determined by combining„ÄÇ

A whole bunch of different factorsÔºå what was going on in the previous time step„ÄÇ

At this level of representation in this location„ÄÇWhat was going on at the previous time step in this location„ÄÇ

 but to the next level downÔºüOf the next little a„ÄÇAnd also what was going on at the previous time step at nearby locations„ÄÇ

At the same level„ÄÇAnd the weighted average of all those things I'll call the consensus embedding„ÄÇ

 and that's what you use for the next embedding„ÄÇAnd I think you can see that if we try and make the bottom up neural net on the top down neural net„ÄÇ

 if we try and make the predictions agree with the consensus„ÄÇ

The consensus has folded in information from nearby locations„ÄÇ

That already roughly agree because of the attention waiting„ÄÇ

And so by trying to make the top down and bottom up neural networksÔºå agree with the consensus„ÄÇ

You're trying to make them agree with what's going on nearby locations that are similar„ÄÇ

And say you'll betraying it to four islands„ÄÇThis is more interesting to neuroscientists than to people who do natural language„ÄÇ

 so I'm going to ignore that„ÄÇÂóØ„ÄÇYou might think it's wasteful to be replicating„ÄÇ

All these embeddings at the object levelÔºå so the idea is at the object level there'll be a large number of patches that all have exactly the same vector representation„ÄÇ

And that seems like a wasteÔºå but actually biology is full of things like that„ÄÇ

 all your cells have exactly the same DNA and all the parts of an organ have pretty much the same vector of protein expressions so there's lots of replication goes on in to keep things local„ÄÇ

üòäÔºåAnd it's the same here and actually that replication is very useful when you're settling on an interpretation because before you settle down you don't know which things should be the same as which other things„ÄÇ

 so having separate vectors in each location to represent what's going on there at the object level gives you the flexibility to gradually segment things as you settle down in a sensible way„ÄÇ

üòäÔºåIt allows you to hedge your bets and what you're doing is not quite like clustering you're creating clusters of identical vectors rather than discovering clusters in fixed data„ÄÇ

 so clustering you're given the data and it's fixed and you find the clusters here the embeddings at every level they vary over time they're determined by the top down and bottom up inputs and by inputs coming from nearby locations so what you're doing is forming clusters rather than discovering them in fixed data„ÄÇ

And that's got a somewhat different flavor and can't settle down faster„ÄÇ

And one other advantage this replication is„ÄÇWhat you don't want is to have much more work in your transformer as you go to higher levels„ÄÇ

But you do need longer range interactions at higher levels„ÄÇ

 presumably for the lowest levels you want fairly short range interactions in your transformer and they could be dense as you go to high levels you want much longer range interactions so you could make them sparse„ÄÇ

And people have done things like that for„ÄÇBut like systems„ÄÇ

Here it's easy to make them sparse because you're expecting big islands so all you need to do is see one patch of a big island to know what the vector representation of that island is and so sparse representations will work much better if you have these big islands of agreement as you go up so the idea is you have longer range and sparse connections as you go up so the amount of computation is the same at every level„ÄÇ

And just to summarize„ÄÇÂóØ„ÄÇI showed how to combine three important advances of neural networks in Gm I didn't actually talk about neural fields and that's important for the top down network maybe since I've got two minutes to spare i'm going to go back and mention neural fields very briefly„ÄÇ

YeahÔºå when I train that top down neural network„ÄÇI have a problem„ÄÇAnd the problem is„ÄÇüò∞„ÄÇ

If you look at those red arrows and those green arrows„ÄÇThey're quite different„ÄÇüò°„ÄÇ

But if you look at the level above the object level„ÄÇAll those vectors are the same„ÄÇAndÔºå of course„ÄÇ

In an engineered systemÔºå I want to replicate the neural nets in every location„ÄÇ

 so he's exactly the same top down and bottom up neural nets everywhere„ÄÇAnd so the question is„ÄÇ

 how can the same neural net be given a black arrowÔºü

And sometimes produce a red arrow and sometimes produce a green arrow„ÄÇ

 which have quite different orientations„ÄÇHow can it produce a nose where there's nose and a mouth where there's mags„ÄÇ

 even though the face vector is the same everywhereÔºü

And the answer is the top down neural network doesn't just get the face vector„ÄÇ

 it also gets the location of the patch for which is producing the PA vector„ÄÇ

So the three patches that should get the red vector are different from different locations from the three patches that should get the green vector„ÄÇ

So if I use a neural network and the guess the location is input as wellÔºå here's what it can do„ÄÇ

 it can take the pose that's encoded in that black vectorÔºå the pose of the face„ÄÇ

It can take the location„ÄÇIn the image for which is predicting the vector of the level below„ÄÇ

And the pose is relative to the image tooÔºå so knowing the location in the image and knowing the pose of the whole face it can figure out which bit of the face it needs to predict at that location and so in one location it can predict okay there should be nose there and it gives you the red vector in another location it can predict from where that image patch is there should be mouth there so it can give you the green arrow„ÄÇ

üòäÔºåSo you can get the same vector at the level above to predict different vectors in different places at the level below by giving it the place that it's predicting for and that's what's going on in neural fields„ÄÇ

üòäÔºåOkay now this was quite a complicated talk there's a long paper about it on archive that goes into much more detail„ÄÇ

And you could view this talk as just an encouragement to read that paper when I'm done„ÄÇ

Exactly on time„ÄÇthank you„ÄÇ a lot„ÄÇ

![](img/a6e82a7e40b8f10bedf5eedaad6d0492_4.png)