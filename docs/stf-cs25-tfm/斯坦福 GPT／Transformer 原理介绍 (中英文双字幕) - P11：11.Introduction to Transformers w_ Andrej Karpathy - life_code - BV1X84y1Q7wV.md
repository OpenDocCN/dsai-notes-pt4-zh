# ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P11Ôºö11.Introduction to Transformers w_ Andrej Karpathy - life_code - BV1X84y1Q7wV

![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_0.png)

![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_1.png)

Hi everyoneÔºå welcome to CS25 Pro UnitedV„ÄÇThis was a course that was held at Stanford in the winter of 2023„ÄÇ

üòäÔºåThis course is not about robots that can transform into carsÔºå as this picture I suggestÔºå rather„ÄÇ

 it's about deep learninging models that have taken the world by the storm and have revolutionized the field of AI and others„ÄÇ

Starting from natural language processingÔºå transformers have been applied all over from compvis„ÄÇ

 enforcementment learningÔºå biologyÔºå roboticÔºå etc„ÄÇWe have an exciting set of videos lined up for you with some truly fascinating speakers„ÄÇ

 skip talks„ÄÇPresenting how they're applying transformers to the research in different fields and areas„ÄÇ

We hope„ÄÇYou'll enjoy and learn from these videos„ÄÇSo without any furthereddoÔºå let's get started„ÄÇ

This is a purely introductory lecture„ÄÇAnd we'll go into the building blocks of transformers„ÄÇ



![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_3.png)

So firstÔºå let's start with introducing the instructors„ÄÇSo for me„ÄÇ

 I'm currently on a temporary diploma from the PhP program and I'm leading here at a robotics startup„ÄÇ

 collaborativeative robotics working on some general purpose robotsÔºå somewhat like a the„ÄÇ

 and yeah I'm very passionate about robotics and building assist learning algorithms„ÄÇ

 my research interests in in personalsonal lending ands in remote modeling and I have a bunch of publications in the robotics government driving other areas„ÄÇ

Undergrad close at CornellÔºå it's someones book CornellÔºå it's an estimate call„ÄÇ

So I'm Stephen a Gr Fer CSP speakerÔºå Per did my master's at CMU and an undergrad„ÄÇ

Mainly into NLP researchÔºå anything involving language and text„ÄÇ

 but more recently I've been getting more into computer vision as well as Mon„ÄÇ

And just some stuff I do for funÔºå a lot of music stuff made piano„ÄÇ

 some self promo but I post a lot on my ins YouTube and TikTook so if you guys want to check it out my friends and I are also starting a Stanford piano club so if anybody's interested feel free to email with Y for details other than that you know martial arts„ÄÇ

 bodybuild„ÄÇP're trying a pig dramas anime„ÄÇOccasional game„ÄÇüòÄ„ÄÇYeah„ÄÇOkay cool yeah„ÄÇ

 so my name'sryland the same talk about myself I just want to very briefly say that I'm super excited to take this class I the last time was sorry to teach this class me I the last time it was offered of I thought we brought a really great group of speaker last time I'm super excited for this offering and yeah I'm thankful you're all here and I'm looking forward to a really fun quarter you was the most out speakak outspoken student last year and so someone wants to become instructor next year in„ÄÇ

Okay„ÄÇüòäÔºåGo„ÄÇLet's see if okay a few minutes„ÄÇSo what we hope you will learn in this class is first of all„ÄÇ

 how do transforms workÔºühow they being applied just don cannot and now days like we are pretty much in them everywhere in the AI machine learning and what are some new interesting directions of research in the topics„ÄÇ

Co so this class is just an introductorÔºå just talking about the basics of transformers introducing them„ÄÇ

 talking about a self potential mechanism on which they are founded and will do a deep dive more on like a model like to GPT so get„ÄÇ

Happy to get solid„ÄÇOkayÔºå so let me start with presenting the attention timeline„ÄÇ

Attention all started with this wall paper attention is all by wasman L in 2017 that was the being transformers before that we had the field story error where we had models like RNM LSTMs and simple attention mechanisms that didn't involve for scale depot„ÄÇ

Start in 2017 we solve this explosion of transformers into NLP where people started using it for everything„ÄÇ

 I even heard the support from Google as like our performance increased every time we fight our linguist„ÄÇ

ËØ∂„ÄÇFor the course 2018 after 2018 to 2020Ôºå we saw this explosion of customers into other fields like vision„ÄÇ

üòäÔºåBch of other the stir and like biology and last year 2021 was the start of the geneticative error where we got like a lot of genetic modeling started like models like Kox„ÄÇ

 GTÔºå DaliÔºå stable equations to a lot of things happening in genetic modeling and we start scaling up in AI and now the present„ÄÇ

So this is 2022 and like the startup in 23 and now we have almost like a chattyy whisper a bunch of others and we are like scalinging onward without spell out so that's great„ÄÇ

 so that's the future„ÄÇSo going more into this„ÄÇSo once there were audit„ÄÇSo we had two modelsÔºå LTN„ÄÇ

 GIU„ÄÇWhat worked here was the day of good at ending history„ÄÇüòä„ÄÇ

But what did not work was they didn encode long sequences and they were very bad at encoding content„ÄÇ

So consider this example„ÄÇConsider trying to predict the last word in the text„ÄÇ

 I grew up in France dot dot dotÔºå I speak fluent D„ÄÇ

Here you need to understand the context for it to predict French and take attention mechanism is very good at that„ÄÇ

 whereas if you're just using LSTMsÔºå it doesn't work that„ÄÇAnother thing transformers are good at is„ÄÇ

U„ÄÇMore based on content is like also context prediction is like finding attention maps if I have something like a word like it„ÄÇ

 what now does it collect and we can give like a probability attention on what are the possible activations and this works are better than existing mechanisms„ÄÇ

Okay„ÄÇSo where we were in 2021Ôºå we were on the verge of takeoff„ÄÇ

We were starting to realize the potential of transformers in different fields„ÄÇ

 we solved a lot of long sequence problems like protein foldingÔºå Al foldÔºå offline Arl„ÄÇ

We started to see zero short generalization we saw multimodal tasks and applications like generating images from language so that's all dli yeah and it feels like Asia shared but person like two years ago„ÄÇ

And this is also a talk on transformers that can watch in give„ÄÇ



![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_5.png)

Yeah„ÄÇCo and this is where we was going from 2021 to 2022„ÄÇ

 which is we have gone from the verge of taking off to actually taking off and obviously we are seeing unique applications in audio generation art„ÄÇ

 music sort towering we are starting to see reasoning capabilities like common sense„ÄÇ

 logical reasoningÔºå mathematical reasoning„ÄÇWe are also able to now get human enlightenment and interaction„ÄÇ

 they're able to use reinforcement learning and human feedback that's how tragedy is trained to perform really good we have a lot of mechanisms for controlling toxicity bias and ethics now and a lot of also a lot of developments in other areas like different models„ÄÇ



![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_7.png)

Â§ü„ÄÇüò°ÔºåÂïä„ÄÇSo the feature is a spaceship and we are all excited about it„ÄÇüòä„ÄÇ

And there's a lot of more acquisition that we can enable and it'll be great if you can see transformers also work here one big example is for your understanding and generation that is something that everyone is interested in and I'm hoping we'll see a lot of models in this area this year also finance business„ÄÇ

üòäÔºåI'll be very excited to see GT author novelÔºå but we need to solve very long sequences modeling and most transformative models are still limited to like 4000 opens or something like that so we need to do a„ÄÇ

Make them general much more better on long sequences we are also we also want to have general agents that can do a lot of multitask„ÄÇ

Amatic input„ÄÇPredictions like Goto and so I think we will see more of that too and finally we also want domain specific models„ÄÇ

 so you might want like a GP models that's good at like maybe like help so that could be like a doctor GPT model you might have like a large GP model that's like on only on raw data so currently we have like GP models that are trained on every but we might start to see more niche models that are like good at one task and we could have like a mixture of expert and think like this is like how you normally consult an expert will' have like expert A models and you can go to a different air models for your different needs„ÄÇ

Yeah„ÄÇThere still a lot of missing ingredients to make this all successful the first of all is external memory we are already starting to see this with models like ChaGPT where the interactions are short lived„ÄÇ

 there's no long term memory and they don't have ability to remember stored conversations for long term and this is something we want to fix„ÄÇ

üòäÔºåSecond over second is reducing the computation complexity„ÄÇ

 so attention mechanism is quadtic over the sequence length which is slu and we want to reduce itra make it faster„ÄÇ

ËØ∂„ÄÇAnother thing you want to do is we want to enhance the controllability of these models like a lot of these models can be stochastic and we want to be able to control what sort of outputs we get from them and you might have experienced the charge if you just refresh you get like different output each time„ÄÇ

 but you might want to have mechanism black printersÔºå what sort of things you can„ÄÇ

And finally we want to align our state of art language models with how the human brain works and we are seeing the search„ÄÇ

 but we still need more research on seeing how they can be more important„ÄÇÂèØÁ≥ªÂìº„ÄÇüòäÔºåYes„ÄÇ

 I'm excited to be here I live very nearby so I got the invites to come to class and I was like„ÄÇ

 okay I'll just walk over but then I spent like 10 hours on the slides so it wasn't as simple„ÄÇ

So yeah I want to talk about transformersÔºå I'm going to skip the first two over there„ÄÇ

 we're not going to talk about those we're going to talk about that one just to simplify the lecture here since we've done have time„ÄÇ



![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_9.png)

ÂóØ„ÄÇOkayÔºå so I wanted to provide a little bit of context of why does this transformers class even exist so a little bit of historical context I feel like Bbo over there I joined like telling you guys about this I don't know if you guys saw the drinks and basically I joined AI in roughly 2012 and full course so maybe a decade ago and back then you wouldn't even say that you joined AI by the way that was like a dirty word now it's okay to talk about but back then it was not even deep learning it was machine learning that was a term use if you were serious but now now AI is okay to use I think„ÄÇ

So basically do you even realize how lucky you are potentially entering this area and roughly 2203 so back then in 2011 or so when I was working specifically on computer vision„ÄÇ



![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_11.png)

Your your pipelines looked like thisÔºå so you wanted to classify some images you would go to a paper and I think this is representative you would have three pages in the paper describing all kinds of a zoo of kitchen sink of different kinds of features„ÄÇ

 descriptors and you would go to poster session and in computer vision conference and everyone who have their paper feature descriptors that they're proposing and it's totally ridiculous and you would take notes on like which one you should incorporate into your pipeline because you would extract all of them and then you would put an SVM on top so that's what you would do so there's two pages make sure you get your spars sip histograms„ÄÇ

 your SSIs your color histogramsÔºå textilesÔºå tiny images and don't forget the geometry specific histograms„ÄÇ

 all of them had basically complicated code by themselves you're collecting code from everywhere and running it and it was a total nightmare so„ÄÇ

On top of that it also didn't work so this would be I think representative prediction from that time you would just get predictions like this once in a while and you'd be like you just shrug your shoulders like that just happens once in a while today you would be looking for a bug„ÄÇ

And worse than that„ÄÇEvery single every single sort of feel every single chunk of AI had their own completely separate vocabulary that they work with„ÄÇ

 so if if you go to NLP papersÔºå those papers would be completely different so you're reading the NLP paper and you're like what is this part of speech tagging morphological analysis syntactic parsing coreference resolution what is NPBT JJ and you're confused so the vocabulary and everything was completely different and you couldn't read papers I would say across different areasÔºü

So now that changed a little bit starting in 2012 when Askochevsky and colleagues basically demonstrated that if you scale a large neural network on large data set you can get very strong performance and so up till then there was a lot of focus on algorithms but this showed that actually neural net scale very well so you need to now worry about compute and data and if you scale it up works pretty well and then that recipe actually did copy paste across many areas of AI so we started to see a neural networks pop up everywhere since 2012 so we saw them computer vision and NLP and speech and translation in RL and so on so everyone started to use the same kind of modeling tool modeling framework and now when you go to NLP and you start reading papers there in machine translation for example„ÄÇ

 this is a sequence of sequence of paper which will come back to in a bit you start to read those papers and you're like okay I can recognize these words like there's a neural network there's some parameters there's an optimizer and it starts to read like things that you know of so that decreased tremendously the barrier to entry across„ÄÇ

The different areas„ÄÇAnd then I think the big deal is that when the transformer came out in 2017„ÄÇ

 it's not even that just the toolkits and the neural networks were similar is that literally the architectures converge to like one architecture that you copy paste across everything seemingly so this was kind of an unassuming machine translation paper at the time proposing the transformer architecture but what we found since then is that you can just basically copy paste this architecture„ÄÇ

And use it everywhere and what's changing is the details of the data and the chunking of the data and how you feed in and you know that's a caric but it's kind of like correct first order statement and so now papers are even more similar looking because everyone is just using transformer and so this convergence was remarkable to watch and unfolded over the last decade and it's crazy to me what I find kind of interesting is I think this is some kind of a hint that we're maybe converging to something that maybe the brain is doing because the brain is very homogeneous and uniform across the entire sheet of your cortex and okay maybe some of the details are changing but those feel like hyperparmeters of like a transformer but your auditory cortex and your visual cortex and everything else looks very similar and so maybe we're converging to some kind of a uniform powerful learning algorithm here something like that I think is kind of interesting„ÄÇ

OkayÔºå so I want to talk about where the transformer came from briefly historically„ÄÇ

So I want to start in 2003Ôºå I like this paper quite a bit it was the first sort of„ÄÇ

Popular application of neural networks to the problem of language modeling so predicting in this case the next word in a sequence„ÄÇ

 which allows you to build generative models over text and in this case they were using multiier perceptron so a very simple neural the neural net took three words and predicted the probability distribution fourth word in a sequence So this was well and good at this point Now over time people started to apply this to a machine translation so that brings us to sequence to sequence paper from 2014 that was pretty influential and the big problem here was„ÄÇ

 okay we do just want to take three words and predict it for we want to predict how to go from an English sentence to a French sentence and the key problem was„ÄÇ

 okay you can have arbitrary number of words in English and arbitrary number of words in French so how do you get an architecture that can process thisvariably sized input„ÄÇ

And so here they use a LSTM and there's basically two chunks of this„ÄÇ

 which are covered the Sla by the„ÄÇ„Åæ„Åß„Åô„ÄÇBut basically have an encoder LSTM on the left and it just consumes one word at a time and builds up a context of what it has read„ÄÇ

 and then that acts as a conditioning vector to the decoder RN or LSTM that basically goes chunk chunk chunk for the next word in a sequence translating the English to French or something like that„ÄÇ

Now the big problem with this that people identified I think very quickly and tried to resolve is that there's what's called this encoded bottleneck„ÄÇ

 so this entire English sentence that we are trying to condition on is packed into a single vector that goes from the encoder for the decoder and so this is just too much information to potentially maintain a single vector and that didn't seem correct and so people are looking around for ways to alleviate the attention of sorry the encoded bottleneck as it was called at time„ÄÇ

And so that brings us to this paperÔºå neural machine translation by jointly learning to align and translate„ÄÇ

Here just quo from the abstract in this paperÔºå we conjected that the use of a fixed length vector is a bottleneck in improving the performance of the basic encoded decoder architecture and proposed to extend this by allowing the model to automatically soft search for parts of the source sentence that are relevant to predicting target word yeah without having to form these parts or heart segments explicitly so this was a way to look back to the words that are coming from the encoder and it was achieved using this soft search so as you are decoding in the„ÄÇ

The words hereÔºå while you are decoding them you are allowed to look back at the words at the encoder via this soft attention mechanism proposed in this paper and so this paper I think is the first time that I saw basically attention so your context vector that comes from the encoder is a weighted sum of the hidden states of the words in the in the encoding„ÄÇ

And then the weights of this come from a softmax that is based on these compatbilities between the current state as you're decoding and the hidden states generated by the encoder and so this is the first time that really you start to like look at it and this is the current modern equations of the attention and I think this was the first paper that I saw it in is the first time that there's a word attention used as far as I know to call this mechanism so I actually tried to dig into the details of the history of the attention so the first author here Dmitri I had an email correspondence with him and I basically sent him an email I'm like Dmitri this is really interesting transformers have've taken over where did you come up with the soft attention mechanism that ends up being the heart of the transformer and to my surprise he wrote me back this like massive email which was really fascinating so this is an excerpt from that email„ÄÇ

ÂóØ„ÄÇSo basically he talks about how he was looking for a way to avoid this bottleneck between the encoder and decoder„ÄÇ

 he had some ideas about cursors that traversed the sequences that didn't quite work out and then here so one day I had this thought that it would be nice to enable the decoder RN to learn to search where to put the cursor in a source sequence this was sort of inspired by translation exercises that learning English in my middle school involved bulk you gaze shifts back and forth in the source and target sequence as you translate so literally I thought this was kind of interesting that he's not made English speaker and here that gave him an edge in this machine translation that led to attention and then led to transformer so that's really fascinating I expressed a soft search as softmax and that way the averaging of the ByN statess and basically to my great excitement this dis work from the very first try so really I think interesting piece of history and as it later turned out the name of RNN search was kind of blame so the better name attention came from Yohua on one of the final passes„ÄÇ

As they went over the paperÔºå so maybe attention is all you need would have been called like harnesss or just hold„ÄÇ

But we have Yoshua Benio to thank for a little bit of better name I would say„ÄÇ

 so apparently that's the history of this subject that was interesting„ÄÇOkay„ÄÇ

 so that brings us to 2017Ôºå which is attention is all unique so this attention component which in Dimetrius paper was just like one small segment and there's all this bidirectional RN RN and decoder and this attention on paper is saying okay you can actually delete everything like what's making this work well very well is just the attention by itself„ÄÇ

And so delete everythingÔºå keep attention and then what's remarkable about this paper actually is usually you see papers that aren't very incommer they add like one thing and they show that it's better„ÄÇ

 but I feel like attention is all unique was like a mix of multiple things at the same time they were combined in a very unique way„ÄÇ

And then also achieve a very good local minimum in the architecture space„ÄÇ And so to me„ÄÇ

 this is really a landmark paper that„ÄÇAnd it's quite remarkable and I think have quite a lot of work behind the scenes„ÄÇ

So delete all the RNN just keep attention because attention is operates over sets and I'm going to go into this in a second„ÄÇ

 you now need to positionally encode your inputs because attention doesn't have the notion of space by itself„ÄÇ

They„Å¶„ÄÇYou have to be very careful„ÄÇThey adopted this residual network structure from resonance„ÄÇ

 they interspersed attention with multilayer perceptrons„ÄÇ

 they used layer norms which came from a different paper they introduced a concept of multiple heads of attention that were applied to parallel and they gave us I think like a fairly good set of hyperparmeters that to this day are used so the expansion factor in the multilayer percept on goes up by forx and we'll go into like a bit more detail and this forex has stuck around and I believe there's a number of papers that tried to play with all kinds of little details of the transformer and nothing like sticks because this is actually quite good the only thing to my knowledge that stuck that didn't stick was this reshuffling of the layer norms to go into the prenorm version where here you see the layer norms are after the multi-headed attention or before they just put them before instead so just reshuffling of layer norms but otherwise the GPSTs and everything else that you're seeing today is basically the 2017 architecture from five years ago and even though everyone is working on it it's proven remarkably resilient which I think is relevant„ÄÇ

There are innovations that I think have been adopted also in position encoings„ÄÇ

 it's more common to use different rotary and relative position encoings and so on„ÄÇ

 so I think there have been changesÔºå but for the most part it's proven very resolute„ÄÇ

So really quite an interesting paper now I wanted to go into the attention mechanism„ÄÇAnd I think„ÄÇ

I sort of like the way I interpreted is not„ÄÇIs not similar to the ways that I've seen it presented before so let me try a different way of like how I see it basically to me attention is kind of like the communication phase of the transformer and the transformer interleaves two phases„ÄÇ

 the communication phaseÔºå which is the multiheaded attention and the computation stage„ÄÇ

 which is this multio perceptron or people form„ÄÇSo in the communication phase„ÄÇ

 it's really just a data dependent message passing on directed graphs„ÄÇ

And you can think of it as okay forget everything with a machine translation and everything let's just we have directed graphs at each a node you are storing a vector and then let me talk now about the communication phase of how these vectors talk to each other in this directed graph and then the compute phase later is just a multi of receptron which now which then basically acts on every node individually but how do these node talk to each other in this directed graph so I wrote like some simple python„ÄÇ

Like I wrote this in Python basically to create one round of communication of using attention as the message passing scheme„ÄÇ

So here„ÄÇA node has this private data vector as you can think of it as private information to this node„ÄÇ

 and then it can also emit a key a query and a value and simply that's done by linear transformation from this node„ÄÇ

 So the key is what are the things that I am„ÄÇSorry„ÄÇ

 the query is one of the things that I'm looking for„ÄÇ

 the key is where are the things that I have and the value is one are the things that I will communicate„ÄÇ

üò°ÔºåAnd so then when you have your graph that's made up of nodes in some random edges„ÄÇ

 when you actually have these nodes communicatingÔºå what's happening is you loop over all the nodes individually in some random order and you are at some node and you get the query vector Q which is I'm a node in some graph and this is what I'm looking for and so that's just achieved via this linear transformation here„ÄÇ

And then we look at all the inputs that point to this node„ÄÇ

 and then they broadcast where are the things that I haveÔºå which is their keys„ÄÇüò°„ÄÇ

So they broadcast the keysÔºå I have the query then those interact by dot product to get scores„ÄÇ

 so basically simply by doing dot product you get some kind of an unormalized weighting of the interestingness of all of the information in the notes that point to me and to the things I'm looking for and then when you normalize that with softbacks so it just sums to one„ÄÇ

 you basically just end up using those scores which now sum to one and our probability to distribution and you do a weighted sum of the values to get your update„ÄÇ

So„ÄÇI have a queryÔºå they have keys dot products to get interestingness or like affinity„ÄÇ

 softmax to normalize it and then weight at some of those valuesÔºå flow to me and update me„ÄÇ

And this is happening for each note individually and then we update at the end and so this kind of a message passing scheme is kind of like at the heart of the transformer and happens in a more vectorized batched way that is more confusing and is also inter interspersed with layer norms and things like that to make the training behave better„ÄÇ

 but that's roughly what's happening in the attention mechanism I think on the high level„ÄÇU„ÄÇSo yeah„ÄÇ

 so in the communication page of the transformerÔºå then this message passing scheme happens in every head in parallel and then in every layer in series„ÄÇ

And with different weights each time„ÄÇAnd that's it as far as the multiheaded tension goes and so if you look at these encoder decoder models„ÄÇ

 you can sort of think of it then in terms of the connectivity of these nodes in the graph„ÄÇ

 you can kind of think of it as like okay all these tokens that are in the encoder that we want to condition on they are fully connected to each other so when they communicate they communicate fully when you calculate their features but in the decoder because we are trying to have a language model we don't want to have communication from future tokens because they give away the answer at this step so the tokens in the decoder are fully connected from all the encoder states and then they are also fully connected from everything that is before„ÄÇ

And so you end up with this like triangular structure of in the directed graph but that's the message passing scheme that this basically implements and then you have to be also a little bit careful because in the cross attention here with the decoder you consume the features from the top of the encodeder so think of it as in the encoder all the nodes are looking at each other all the tokens are looking at each other many„ÄÇ

 many times and they really figure out what's in there and then the decoder when it' it's looking only at the top nodes„ÄÇ

So that's roughly the message passing scheme I was going to go into more of an implementation of a transformer„ÄÇ

 I don't know if there's any questions about this„ÄÇÂì¶Ôºå‰Ω†ÈóÆ„ÄÇSelf attention and move by attention„ÄÇ

 But what is the the„ÄÇ‰∏çÊòØ„ÄÇÂóØ„ÄÇYeahÔºå so„ÄÇSelf attention and multi headeded detention so the multi headeded detention is just this attention scheme„ÄÇ

 but it's just applied multiple times in parallel multiple has just means independent applications of the same attention so„ÄÇ

This message passing scheme basically just happens in parallel multiple times with different weights for the query key and value„ÄÇ

 so you can always look at it like in parallel I'm looking for I'm seeking different kinds of information from different nodes and I'm collecting it all in the same node„ÄÇ

It's all done in parallel„ÄÇSo heads is really just like copy paste in parallel and layers are copy paste„ÄÇ

 but in series„ÄÇMaybe that makes sense„ÄÇAnd self attentionÔºå when it's self attention„ÄÇ

 what it's referring to is that the node here produces each node hereÔºå so as I described it here„ÄÇ

 this is really self attention because every one of these nodes produces a key query value from this individual node„ÄÇ

When you have cross attention„ÄÇYou have one cross attention here coming from the encoder„ÄÇ

 that just means that the queries are still produced from this node„ÄÇ

 but the keys and the values are produced as a function of nodes that are coming from the encoder„ÄÇ

So I have my queries because I'm trying to decode some the fifth word in the sequence and I'm looking for certain things because I'm the fifth word and then the keys and the values in terms of the source of information that could answer my queries can come from the previous nodes in the current decoding sequence or from the top of the encoder so all the nodes that have already seen all of the encoding tokens many„ÄÇ

 many times can now broadcast what they contain in terms of the information„ÄÇSo„ÄÇ

I guess to summarize the self attention is kind of like„ÄÇ

So cross attention and self attention only differ in where the piece and the values come from„ÄÇ

 either the piece and values are produced from this node or they are produced from some external source like like an encoder and the node over there„ÄÇ

üò°ÔºåBut algorithmically is the same micro operations„ÄÇÂ•ΩÁöÑ„ÄÇO„ÄÇIn the message passing graph paradigm the„ÄÇ

So yeahÔºå so„ÄÇÂóØÊàë„ÄÇSo think of so each one of these nodes is a token„ÄÇÂóØ„ÄÇ

I guess like they don't have a very good picture of it in the transformerÔºå but like„ÄÇ

Like this node here could represent the third word in the output in the decoder„ÄÇAnd in the beginning„ÄÇ

 it is just the embedding of the word„ÄÇÂóØ„ÄÇAnd then„ÄÇOkay„ÄÇ

 I have to think through this knowledge morningÔºå I came up with it this morning„ÄÇÊ≤°Êúâ„ÄÇActually„ÄÇ

 I ca yesterday„ÄÇÊàëÁªìÊûú„ÄÇStation„ÄÇ„Åî„ÅÑ„ÄÇNotes have been blocked„ÄÇWe better„ÄÇ

These notes are basically the vector„ÄÇI'll go to an implementationÔºå I'll go to the implementation„ÄÇ

 and then maybe I'll make the connections to the graph„ÄÇ

So let me try to first go to let me not go to with this intuition in mind at least to NAGPT which is a complete implementation of a transformer that is very minimal thats why I worked on this over the last few days and here it is reproducing GT2 on open web textex so it's a pretty serious implementation that reproduces GPT2 I would say and provide it in a compute this was one note of HPs for 38 hours or something like that correctly and it's very readable it's 300 lives so everyone can take a look at it and yeah let me basically briefly step through it„ÄÇ

So let's try to have a decoder only transformer so what that means is that it's a language model it tries to model the next word in a sequence or the next character sequence so the data that we train on it's always some kind of text so here's some fake Shakespeare so this is real Shakespeare we're going to produce fake Shakespeare so this is called the tiny Shakespeare dataset set which is one of my favorite toy datasets you take all of Shakespeare concateninated and it's one megate file and then you can train language models on it and get infinite Shakespeare if you like which I think is medical so we have a text the first thing we need to do is we need to convert it to a sequence of integers„ÄÇ

Because transformers natively processÔºå you know„ÄÇYou can't pluck text into transform you need to some outencod„ÄÇ

 So the way that encoding is done is we convertÔºå for exampleÔºå in the simplest case„ÄÇ

 every character gets an integerr„ÄÇAnd then instead of high there„ÄÇ

 we would have this sequence of integers„ÄÇSo then you can encode every single character as an integer and get like a NAA sequence of integer so you just incatetcatenate it all into one large long one dimensional sequence and then you can train on it Now here we only have a single document in some cases if you have multiple independent documents what people like to do is create special tokens and they intersperse those documents with those special text tokens that they splice in between to create boundaries„ÄÇ

But those boundaries actually don't have any„ÄÇAny modeling impact it's just that the transformer is supposed to learn beyond that propagation that the end of document sequence means that you should wipe the memory„ÄÇ

üò°ÔºåOkayÔºå so then we produce batchesÔºå so these batches of data just mean that we go back to the one dimensional sequence and we take out chunks of this sequence„ÄÇ

 so say if the block size is eight„ÄÇThen the block size indicates the maximum length of context that your transformer will process„ÄÇ

 so if our block size is eightÔºå that means that we are going to have up to eight characters of context to predict ninth character in a sequence„ÄÇ

And the batch size indicates how many sequences and parallel we're going to process„ÄÇ

 and we want this to be as large as possibleÔºå so we're fully taking advantage of the GPU and the parallelels under the cords„ÄÇ

So in this exampleÔºå we're doing four by8 batchesÔºå so every row here is independent example sort of„ÄÇ

 and then every„ÄÇEvery row here is is a small chunk of the sequence that we're going to train on and then we have both the inputs and the targets at every single point here„ÄÇ

 so to fully spell out what's contained in a single4 by8 batch to the transformer is sort of like compacted here so when the input is 47 by itself the target is 58 and when the input is the sequence 4758 the target is one and when it's 47581 the target is 51 and so on so actually the single batch of examples that's 4 by8 actually has a ton of individual examples that we are expecting the transformer to learn on in in parallel and so you'll see that the batches are learned on completely independently but the time dimensions sort of here along horizontally is also trained on in parallel so sort of your real batch size is more like deted„ÄÇ

It's just that the context grows linearly for the predictions that you make along the T direction„ÄÇ

In the in the modelÔºå so this is how the this is all the examples of the model will learn from this single back„ÄÇ

So now this is the GPT class„ÄÇAnd because this is a decoder only model„ÄÇ

 so we're not going to have an encoder because there's no like English we're translating from we're not trying to condition on some other external information„ÄÇ

 we're just trying to produce a sequence of words that follow each other are likely to„ÄÇ

So this is all p torch and I'm growing sluy faster because I'm assuming people have taken 2 31 and or something along those lines„ÄÇ

 but here in the forward pass we take this these indices„ÄÇ

And then we both encode the identity of the indices just via an embedding lookup table„ÄÇ

 so every single integer has a we index into a lookup table of vectors in this n dot embedding and pull out the word vector for that token„ÄÇ

And then because the messageÔºå because transform by itself doesn't actually„ÄÇ

 it processes sets nativelyÔºå so we need to also positionally encode these vectors so that we basically have both the information about the token identity and its place in the sequence from one to block size„ÄÇ

Now those the information about what and where is combined additively„ÄÇ

 so the token embeddings and the positional embeddings are just added exactly as here„ÄÇSo this X here„ÄÇ

üò°ÔºåThen there's optional dropoutÔºå this x here basically just contains the set of words„ÄÇ

And their positions„ÄÇAnd that feeds into the blocks of transformer that we're going to look into what's blocked here„ÄÇ

 but for here for nowÔºå this is just a series of blocks in the transformer„ÄÇüò°ÔºåAnd then in the end„ÄÇ

 there's a layer normÔºå and then you're decoding the logits for the next word or next integer in a sequence using a linear projection of the output of this transformer„ÄÇ

 so LM head hereÔºå a short for language model head is just a linear function„ÄÇ

So basically positionally encode all the wordsÔºå feed them into a sequence of blocks„ÄÇ

 and then apply a linear layer to get the probability distribution for the next character„ÄÇ

 and then if we have the targets which we produced in the data order and you'll notice that the targets are just the inputs offset by one in time„ÄÇ

Then those targets feed into cross entrytropy lossÔºå so this is just a negative one likelihood„ÄÇ

 typical classification loss„ÄÇSo now let's drill into what's here in the blocks„ÄÇ

So these blocks are applied sequentially„ÄÇüò°ÔºåThere's again„ÄÇ

 as I mentioned communicate phase and the compute phase so in the communicate phase both of the nodes get to talk to each other and so these nodes are basically„ÄÇ

If our block size is eight„ÄÇThen we are going to have eight node in this graph„ÄÇ

There's eight nodes in this graphÔºå the first node is pointed to only by itself„ÄÇ

 the second node is pointed to by the first node and itself„ÄÇ

The third node is wanted to buy the first two nodes and itselfÔºå et cetera„ÄÇ

 so there's eight nodes here„ÄÇSo you applyÔºå there's a residual pathway in XÔºå you take it out„ÄÇ

 you apply a layer norm and then the self attention so that these communicate these eight nodes communicate„ÄÇ

 but you have to keep in mind that the batch is four so because batch is four this is also applied so we have eight nodes communicating but there's a batch of four of them all individually communicating on those eight node„ÄÇ

 there's no crisscross across the batchsh dimension of course there's no batchche norm range anyway luckier„ÄÇ

And then once theyve changed informationÔºå they are processed using the modo receptor„ÄÇ

 and that's the compute base„ÄÇSo and then also here are missing we are missing the cross attention and because this is a decoder only model„ÄÇ

 so all we have is this step hereÔºå the multiheaded attention and that's this line the communicate phase„ÄÇ

 and then we have the feet forwardÔºå which is the MLP„ÄÇAnd that's the complete phase„ÄÇ

I'll take questions a bit later„ÄÇThen the MLP here is fairly straightforward the MLP is just individual processing on each node just transforming the feature representation sort of at that node so„ÄÇ

Applying a two layer neural nu with a gal nonlinearity„ÄÇ

 which is just think of it as a re or something like thatÔºå it just a nonlinearity„ÄÇ

And then MLP straightforwardÔºå I don't think there's anything too crazy there„ÄÇ

 and then this is the causal self attention partÔºå the communication phase„ÄÇ

So this is kind of like the loo of things and the most complicated part„ÄÇ

 it's only complicated because of the batch„ÄÇAnd the implementation detail of how you mask the connectivity in the graph so that you don' you can't obtain any information from the future when you're predicting your token„ÄÇ

 otherwise it gives away the information so if I'm the fifth token and if I'm the fifth position then I'm getting the fourth token coming into the input and I'm attending to the third second and first and I'm trying to figure out what is the what is the next token well then in this batch in the next element over in the time dimension the answer is at the input so I can't get any information from there so that's why this is all tricky but basically in the forward pass„ÄÇ

ÂóØ„ÄÇWe are calculating the queriesÔºå keys and values based on x„ÄÇ

So these are the keys queries and values here when I'm computing the attention„ÄÇ

 I have the queries matrix multiplying the keysÔºå so this is the dot product in parallel for all the queries in all piece„ÄÇ

In older heads„ÄÇSo that I I felt to mention that there's also the aspect of the heads which is also done all and parallel here„ÄÇ

 so we have the batch dimensionÔºå the time dimension and the head dimension and you end up with five dimensional tenss and it's all really confusing so I invite you to step through it later and commit yourself that this is actually doing the right thing basically give have the batch dimension„ÄÇ

 the head dimension and the time dimension and then you have features at them„ÄÇ

And so this is evaluating for all the batch elements„ÄÇ

 for all the head elements and all the time elements„ÄÇThe simple Python that I gave you earlier„ÄÇ

 which is queryductpro P„ÄÇThen here we do a mask to fill and what this is doing is it's basically clamping the„ÄÇ

The attention between the node that are not supposed to communicate to be negative infinity and we're doing negative infinity because we're about to soft max and so negative infinity will make basically the attention that those elements be zero„ÄÇ

üò°ÔºåAnd so here we are going to basically end up with the weights„ÄÇ

They serve affinities between these notes„ÄÇOptional dropout and then here attention matrix multiply V is basically the„ÄÇ

The gathering of the informationÔºå according to the ainities we've calculated„ÄÇ

 and this is just a weighted sum of the values at all those nodes„ÄÇ

 So this matrix multiplies as doing that weighted sum„ÄÇ

And then transpose contiguous view because it's all complicated and bashed in five dimensional testors„ÄÇ

 but it's really not doing anything optional dropout and then a linear projection back to the residual pathway„ÄÇ

So this is implementing the communication phase now„ÄÇThen you can train this transformer„ÄÇ

And then you can generate infinite Shakespeare and you will simply do this by because our block size is eight„ÄÇ

 we start with sum tokenÔºå say like I used in this case„ÄÇ

 you can use something like a new line as the star token„ÄÇ

And then you communicate only to yourself because there's a single node and you get the probability distribution for the first word in the sequence„ÄÇ

 and then you decode it or the first character in the sequence„ÄÇ

 you decode the character and then you bring back the character and you reencode it as an integer and now you have the second thing and so you get okay„ÄÇ

 where the first position and this is whatever integer it is„ÄÇ

Addd the position encodings goes into the sequence goes into transformer and again this token now communicates with the first open and its identity and so you just keep plugging the back and once you run out of the block size„ÄÇ

 which is eightÔºå you start to crawl„ÄÇBecause you can never have block size more than8 in the way you've trained this transformer so we have more and more context until eight and then if you want to generate beyond date you have to start cropping because the transformer only works for eight elements in time dimension and so all of these transformers in the naive setting have a finite de box size or context length and in typical models this will be 1024 tokens or 2048 tokens something like that but these tokens are usually like EE tokens or sentence piece tokens or work these tokens there's many different encodings so it's not like that long and so that's why I think did' mention we really want to expand the context size and gets gnarly because the attention is sp in the na case„ÄÇ

NowÔºå if you want to„ÄÇImplement an encoder instead of a decoder„ÄÇAttention„ÄÇ

Then all you have to do is this master„ÄÇYou just delete that line„ÄÇSo if you don't mask the attention„ÄÇ

 then all the nodes communicate to each other and everything is allowed and information flows between all the nodes„ÄÇ

So if you want to have the encoder hereÔºå just delete all the encoder blocks will use attention where this line is deleted„ÄÇ

 that's it„ÄÇSo you're allowing whatever is encode in my storeÔºå say 10 tokensÔºå 10 notes„ÄÇ

 and they are all allowed to communicate to each other going up the transformer„ÄÇ

And then if you want to implement cross attentionÔºå so you have a full encoder decoder transformer„ÄÇ

 not just a decoder only transformer or GPT„ÄÇThen we need to also add cross attention in the middle„ÄÇ

 so here there's a self attention piece where all the there's a self attention piece„ÄÇ

 a cross attention piece and this MLP and in the cross attention„ÄÇ

We need to take the features from the top of the encoder„ÄÇ

We need to add one more line here and this would be the cross attention instead of I should have implemented it instead of just pointing I think„ÄÇ

 but there will be a cross attention line here so we'll have three lines because we need to add another block and the queries will come from x„ÄÇ

 but the piece and the values will come from the top of the encoder„ÄÇ

And there will be basically information flowing from the encoder strictly to all the nodes inside X„ÄÇ

And then that's itÔºå so it's very simple sort of modifications on the decoder attention„ÄÇ

So you'll hear people talk that you kind of have a decoder only model like GP„ÄÇ

 you can have an encoder only model like Bt or you can have an encoder decoder model like say t5 doing things like machine translation so and in Bt you can't train it using sort of this„ÄÇ

Language modeling setup that's utter aggressive and you're just trying to predict the next omin sequence you're training it to a slightly different objectives you're putting in like the full sentence and the full sentence is allowed to communicate fully and then you're trying to classify sentiment or something like that so you're not trying to model like the next token in the sequence so these are trans slightly different with math„ÄÇ

Âïä„ÄÇWith using masking and other deno techniques„ÄÇOkay„ÄÇ

 so that's kind of like the transformer I'm going to continue so yeahÔºå maybe more questions„ÄÇ‰Ω†ËØ¥Êàë„ÄÇÂì¶„ÄÇÂØπ„ÄÇ

Êàë really„ÄÇI„ÄÇOkay„ÄÇYouÔºå good pepper still for„ÄÇFind is a dynamic route that„ÄÇ

We can actually change in everything and you also have something„ÄÇ

Did it like we are enforcing these constraintsprints on it by just masking by give aware of it„ÄÇÂïäÂïä„ÄÇ

So I'm not sure if I fully followÔºå so there's different ways to look at this analogy„ÄÇ

 but one analogy is you can interpret this graph as really fixed it's just that every time you do the communicate„ÄÇ

 we are using different weightsÔºå you can look at it down„ÄÇ

So if we have block size of eight in my exampleÔºå we would have eight nodes here we have two four six okay„ÄÇ

 so we'd have eight nodesÔºå they would be connected in you lay them out and you only connect from left to right but we're different„ÄÇ

YeahÔºå we'll have a very connected part„ÄÇOh„ÄÇWhy wouldU the connections don't change as a function of the data or something like thatÔºü

The not„ÄÇ I wonder if I theres any„ÄÇI don't think I've seen a single example where the connectivity changes dynamically option data„ÄÇ

 usually the connectivity is fixed if you have an encoder and you're training a Bt„ÄÇ

 you have how many tokens you want and they are fully connected„ÄÇAnd if you have decoder a long model„ÄÇ

 you have the triular thingÔºå and if you have encoder decoder„ÄÇ

 then you have awkwardly sort of like two tools of nodes„ÄÇËØ∂ÂëÄ„ÄÇÂïäÂØπÂïäÂ∞±ÊòØÊàë‰ª¨„ÄÇÊúÄÂêéÂ∞±„ÄÇÂÆ∂‰Ω¢‰Ω†ÂóØ„ÄÇÂóØ„ÄÇ‰∏çË¶ÅÂØπËµ∑„ÄÇ‰Ωú‰∏∫Áõ¥Êí≠‰∫∫ÊàëËØâ„ÄÇÁ¨¨ÂàíÂ≠ôÂó∞ÊúàÂïä„ÄÇ

I wonder„ÄÇDo much more about this and then go„ÄÇÈÉΩËµ∞„ÄÇ‰Ω†ÂÜáÊÉ≥Âá∫Âéª„ÄÇÊúâÁõ¥„ÄÇOkay„ÄÇWhich„ÄÇPart„ÄÇOkay„ÄÇÂØπÁ≥ªÂíß„ÄÇ„Åì„Çå„ÄÇÂóØ„ÄÇÂì¶ÔºåÊúâ„Åã„ÄÇÂÜá your„ÄÇ

Different things„ÄÇÊàëÊòØÂ§ö„ÄÇYeah it's really hard to say so that's why I think this paper is so interesting is like„ÄÇ

 yeah usually you'd see like a path and maybe they head path internally because just didn't publish it and all you can see is sort of things that didn't look like the transformer I mean you had resnets which have lots of this but a resnet would be kind of like this but there's there's no self- attention component„ÄÇ

 but the MLP is there kind of in a resNe„ÄÇÂóØ„ÄÇSo a resonanceson looks very much like this„ÄÇ

 except there's noÔºå you can use layer norms and resonances I believe as well„ÄÇ

 typically sometimes they can be bash norms„ÄÇSo it is kind of like a renet„ÄÇ

 it is kind of like they took a ressonnet and they put in a trans selfpottiary block in addition to the preexistent MLP block„ÄÇ

 which is kind of like convolutions and MLP was strictly speaking deconvolution one by one convolution„ÄÇ

 but I think the idea is similar that MLP is just kind of like you know typical weights„ÄÇ

 No nonlineararity weights or operation„ÄÇÂóØ„ÄÇAnd but I will say likeÔºå yeah„ÄÇ

 it's kind of interesting because„ÄÇA lot of work is not there and then they give you this transformer and then it turns out five years later it's not changed even though everyone's trying to change it so it's kind of interesting to me that iss kind of like a package came like a package which I think is really interesting historically and I also talked to paper authors and they were unaware of the impact that transform would have at the time so when you read this paper actually it's kind of unfortunate because this is like the paper that changed everything but when people read it it's like question marks because it reads like a pretty random machine translation paper I go over doing machine translation oh here's a cool architecture okay great good results like„ÄÇ

It's it doesn't sort of know what's going to happen and so when people read it today I think they're kind of confused potentially like having like having„ÄÇ

 I will have some tweets at the endÔºå but I think I would have renamed it with the benefit of hindsight of like„ÄÇ

 well I'll get to it„ÄÇYeah„ÄÇËøô‰∏™„ÄÇOkay„ÄÇÊàë‰∏™Êîæ„ÄÇYeah I think that's a good question as well currently I mean I certainly don't love the autoaggressive modeling approach„ÄÇ

 I think it's kind of weird to like sample a token and then commit to it„ÄÇSo„ÄÇ

know maybe there's some ways some hybrids with diffusion as an example„ÄÇ

 which I think would be really cool„ÄÇOr we'll find some other ways to like edit the sequences later about filling in our regressive framework„ÄÇ

 but I think the fusion is kind of like an up and coming modeling approach that I personally find much more appeal when I sample text I don't go chunk chunk chunk and commit I do a draft one and then I do a better draft two„ÄÇ

And that feels like a the fusion process„ÄÇSo that would be my hope„ÄÇOkayÔºå also question so yeah„ÄÇ

 you like the logic but it„ÄÇWhen you sayÔºå like the„ÄÇSe attention is sort of like computing like a agiator because to the adopt product on the notionary„ÄÇ

And then those we have the edge the like multily by the other values„ÄÇ

And then just appropriate it yesÔºå yes rightÔºå and do you think there's like a„ÄÇ

Agy like profitable networks and„ÄÇI find the graph neural networks kind of like a confusing term because„ÄÇ

I meanÔºå yeahÔºå previouslyÔºå there was this notion of„ÄÇ

I kind of thought maybe today everything is a graph neural network because the transformer is a graph neural and processor„ÄÇ

 the native representation that the transformer operates over is sets that are connected by edges in a direct way and so that's the native representation and then yeah„ÄÇ

OkayÔºå I should go on because I still have like 30 slides„ÄÇG solve„ÄÇThose want one provide by„ÄÇOhÔºå yeah„ÄÇ

YeahÔºå the root DÔºå I think basically like as if you're initializing with random weight separate from ausion as your dimension size grows„ÄÇ

 so does your valuesÔºå the variance grows and then your softmax will just become the one half vector„ÄÇ

 so it's just a way to control the variance and bring it to always be in a good range for softmax and nice distribution„ÄÇ

O„ÄÇSo it's almost like an initialization thing„ÄÇOkay„ÄÇ

So transformers have been applied to all the other fields and the way this was done is in my opinion kind of ridiculous ways honestly„ÄÇ

 because I was a computer vision person and you have commons and they kind of make sense„ÄÇ

 so what we're doing now with bits as an example is you take an image and you chop it up into little squares and then those squares literally feed into a transformer and that's it„ÄÇ

Which is kind of ridiculous and so„ÄÇ‰∏ãÈù¢„ÄÇYeahÔºå and so the transformer doesn't even in the simplest case like really know where these patches might come from„ÄÇ

 they are usually positionly encodedÔºå but it has to sort of like rediscover a lot of the structure I think of them in some ways„ÄÇ

 and it's kind of weird to approach it that way„ÄÇBut it's just like the simplest baseline of just choing up big images into small squares and feeding them in as like the individual nodes actually works fairly well and then this is in transformer encoder so all the patches are talking to each other throughout the interpret„ÄÇ

And the number of nodes here would be sort of like nine„ÄÇOkay„ÄÇAlso in speech recognition„ÄÇ

 you just take your Mel spectrogram and you chop it up into your slices and feed them into a transformer„ÄÇ

 so there wrote paper like this but also whisper whisper is a copy based transformer if you saw whisper from OpenAI you just chop up Mel Spectrogram and feed it into a transformer and then pretend you're dealing with text and it works very well„ÄÇ

Decision transformer and RL you take your state's actions and reward that you experience an environment and you just pretend it's a language and you start to model the sequences of that and then you can use that for planning later that works pretty well you know even things like alpha fold so we were briefly talking about molecules and how you can plug them in so at the heart of alpha fold computationally is also a transformer„ÄÇ

One thing I wanted to also say about transformers is I find that they're very they're super flexible and I really enjoy that I'll give you an example from Tesla„ÄÇ

Like you have a come that that takes an image and makes predictions about the image and then the big question is how do you feed in extra information and it's not always trivial like say I have additional information that I want to inform that I want the output to be informed by maybe I have other sensors like radar maybe I have some map information or a vehicle type or some audio and the question is how do you feed information into a come that like where do you feed it in do you concatenate it like how do you do you add at what stage and so with the transformer it's much easier because you just take whatever you want„ÄÇ

 you chop it up into piecessis and you feed it in with a set of what you had before and you let the self- attentionten figure out how the potentially should communicate and that actually frankly works„ÄÇ

So just chop up everything and throw it into the mix is kind of like the way and it frees neurallet from this version of Eidean space where previously you had to arrange your computation to conform to the Elidean space or three dimensions of how you're laying out the compute like the compute actually kind of happens in normal like 3D space if you think about it„ÄÇ

 but in intention everything is just sets so it's a very flexible framework and you can just like throw this stuff into your conditioning set and everything just self-ated over so it's quite beautiful with retro respect okay„ÄÇ

So now what exactly makes transformers so effectiveÔºü

I think a good example of this comes from the GT3 paper which I encourage people to read language models are twoshot learners„ÄÇ

 I would have probably remained this a little bitÔºå I would have said something like transformers are capable of in context learning or like meta learning that's kind of like what makes them really special so basically the something that they're working with is okay I have some context and I'm trying to let's say passage this is just one example of many I have a passage and I'm asking questions about it„ÄÇ

And then I'm giving as part of the context in the promptÔºå I'm giving the questions and the answers„ÄÇ

 so I'm giving one example of question answerÔºå another example of question answer„ÄÇ

 another example of question answer and so on„ÄÇAnd this becomes a„ÄÇÂèØ‰ª•Ôºå peopleË∑ü‰ªñËØ¥‰∏ÄÂïä„ÄÇOkay„ÄÇ

 it is really important for me think„ÄÇOkayÔºå so what's really interesting is basically like„ÄÇ

With more examples given in the context the accuracy improves and so what that hint at is that the transformer is able to somehow learn in the activations without doing any gradient descent in a typical fine tuning fashion so if you fine tune you have to give an example and the answer and you fine tuning using gradient descent but it looks like the transformer internally in its weights is doing something that looks like potentially gradient decent some kind of the mental learning in the weights of the transformer as it is reading the prompt and so in this paper they go into okay distinguishing this outer loop with stochastic gradient descent and this inner loop of the in contexttext learning so the inner loop is as the transformer is sort of like reading the sequence almost and the outer loop is is the training by gradient descent so basically there's some training happening in the activation of the transformer as it is consuming a sequence that may be very much looks like gradient descent and so there's some recent papers that kind of hint at this and study it and so as an example in this paper here they propose something called the raw operator and they argue that the raw operator is implemented by a transformer„ÄÇ

And then they show that you can implement things like Ri regression on top of a raw operator„ÄÇ

 and so this is kind of giving their papers hinting that maybe there is something that looks like gradient based learning inside the activations of the transformer„ÄÇ

And I think this is not impossible to think through because is what is gradient based learning„ÄÇ

 overpasÔºå backward passÔºå and then updateÔºüWellÔºå that looks like a resume„ÄÇ

Right because you're just changing you're adding to the weights so you start initial random set of weights„ÄÇ

 forward pass backward pass and update your weights and then forward pass backward path weights„ÄÇ

 it looks like a resnet transformers is a resnet so much more hand wavy„ÄÇ

 but basically some paper trying to hint that why that would be potentially possible„ÄÇ

And then I have a bunch of tweets I just gotten and pasted here in the end„ÄÇ

 this was kind of like meant for general consumption„ÄÇ

 so they're a bit more high level and high a little bit„ÄÇ

 but I'm talking about why this architecture is so interesting and why why potentially became so popular and I think it simultaneously optimizes three properties that I think are very desirable number one„ÄÇ

 the transform is very expressive and if overpass it's sort of like is' able to implement very interesting functions„ÄÇ

 potentially functions that can even like do metaer„ÄÇNumber two„ÄÇ

 it is very optimizable thanks to things like residual connections layerknow and so on and number three it's extremely efficient„ÄÇ

 this is not always appreciatedÔºå but the transformer„ÄÇ

 if you look at the computational graph is a shallow wide network which is perfect to take advantage of the parallelmal GPUs so I think the transformer was designed very deliberately to run efficiently on GPUs there's previous work like neural GPU that I really enjoy as well which is really just like how do we design neural adss that are efficient on GPUs and thinking backwards from the constraints of the hardware which I think is a very interesting way to think about it„ÄÇ

Âïä„ÄÇOh„ÄÇOh yeah so here I'm saying I probably would have called I probably would have called the transformer a general purposeus efficient optimizable computer instead of the attention is all you need like that's what I would have maybe in hindsight called that paper is proposing it is a model that is„ÄÇ

ÂóØ„ÄÇVery general purposeÔºå so forward passes express it„ÄÇ

 it very efficient in terms of GPU usage and is easily optimizable by gradientd descent and trains it very nicely„ÄÇ

Then I have some other hot tweets here„ÄÇAnywayÔºå so yeah you can read them later„ÄÇ

 but I think this ones maybe interestingÔºå so if previous neural nets are special purpose computers designed for a specific tasks„ÄÇ

 GPT is a general purpose computer reconfigurable at runtime to run natural language programs„ÄÇ

 so program the programs are given as prompts„ÄÇAnd then Chi became Ro the program by completing the document„ÄÇ

So I really like these analogies personally to computer„ÄÇ

 it's just like a powerful computer and it's optimizable by gradient descent„ÄÇAnd„ÄÇÁ≥ª don't know oÂïä„ÄÇ

I think you can read this laterÔºå but it's right out just thank youÔºå I'll just leave this up„ÄÇüòäÔºåYeah„ÄÇ

So sorryÔºå I just found this lead„ÄÇTurns out that if you scale up the training set and use a powerful enough neuralNes like a transformer„ÄÇ

 the network becomes a kind of general purpose computer over text„ÄÇ

 so I think that's a kind of like nice way to look at it and instead of performing a single text sequence you can design the sequence in the prompt and because the transformer is both powerful but also is trained on large enough very hard data set it kind of becomes a general purpose text computer and so I think that's kind of interesting way you look at it„ÄÇ

Yeah„ÄÇÂì¶„ÄÇYeah„ÄÇÂêé„ÄÇYeah„ÄÇÂì¶„ÄÇÂØπ„ÄÇÂïä„ÄÇÁ≥ª guess„ÄÇIÁ≥ª„ÄÇWhat„ÄÇGood„ÄÇÂ•Ω„ÄÇÂì¶„ÄÇThat was alreadyient„ÄÇÊúâ„ÄÇOh„ÄÇSo it is„ÄÇOh„ÄÇ

Im moved sorry„ÄÇYes meÔºå‰ΩÜÊòØÊàëÊÑèÊÄùËßâÂæóÂ•Ω„ÄÇOkay„ÄÇÂÖ∂ÂÆûÂ§ß‰∏ÄÂºÄËøá„ÄÇOh„ÄÇI was too favorite„ÄÇIts pretty really like most of it„ÄÇ

 you know„ÄÇIt mostly more efficient„ÄÇIt very good place but you have that„ÄÇÂ§ß‰ºö„ÄÇ„Åô„Å¶„ÄÇDr„ÄÇ By„ÄÇYeah very„ÄÇ

So I think there's a bit of thatÔºå yeahÔºå so I would say RNNs like in principle yes„ÄÇ

 they can implement arbitrary programsÔºå I think it's kind of like a useless statement to some extent because they are they're probably„ÄÇ

 I'm not sure that they're they're probably expressive because in a sense of like power in that they can implement these arbitrary functions„ÄÇ

But they're not optimizable„ÄÇüò°ÔºåAnd they're certainly not efficient because they are serial computing devices„ÄÇ

Um so I thinkÔºå so if you look at it as a compute graphÔºå RNNs are very long„ÄÇÂèò‰∫Ü„ÄÇ‰Ω¢Ë°ÄÊåÇ„ÄÇÂïä„ÄÇ

Like if you stretched out the neurons and you look like take all the individual neurons in our connectivity and stretch them out and try to visualize them„ÄÇ

 RNNs would be like a very long graph in a bad and it's bad also optizability because I don't exactly know why but just the rough intuition is when you're back propagating„ÄÇ

 you don't want to make too many steps„ÄÇüò°ÔºåAnd so transformers are a shallow wide graph„ÄÇ

 and so from supervision to inputs is a very small number of hops„ÄÇ

And it's along residual pathways which make gradients flow very easily and there's all these layer norms to control gradient the scales of all of those activations and so there's not too many hops and you're going from supervision to input very quickly and just flows through the graph„ÄÇ

Ands it can all be done in parallel so you don't need to do this encoder decoder RNANs you have to go from first word„ÄÇ

 then cycle word then third wordÔºå but here in transformer„ÄÇ

 every single word was processed completely as sort of in parallel„ÄÇ

Which is kind of the so I think all these are really important because all these are really important and I think number three is less talked about but extremely important because in deep learning scale matters and so the size of the network that you can train gives you is extremely important and so if it's efficient on the current hardware then we can make it bigger„ÄÇ

ÊàëËÄåÂÆ∂Âá∫Âöü‰Ω¢ÂØπÊàëÊàë‰∏™ÊúâÊàëËá™Â∑±„ÄÇ„Åã„ÄÇYeah how does that believe the dataNo so yeah so you take your image and you apparently chop them up into patches so there's the first thousand tokens or whatever and now I have a special so radar could be also but I don't actually know the native representation of radar so„ÄÇ

But you couldÔºå you just need to chop it up and enter it„ÄÇ And then you have to encode it somehow„ÄÇ

 Like the transformer needs to know that they're coming from radar„ÄÇ So you create a special„ÄÇ

You have some kind of a special token that you„ÄÇThese radar tokens are much slightly different in the representation and it's learnable by gradientcent and„ÄÇ

Like vehicle information would also come in with a special embeddedbedding token that can be learned„ÄÇ

So„ÄÇHave you those like you don'tÔºå it's all just a set„ÄÇ

And just's when the voiceman guarantee know hit and it say button„ÄÇYeahÔºå it's all just a set„ÄÇ

 but you can position encode these sets if you want„ÄÇBut position encoding means you can hardwire„ÄÇ

 for exampleÔºå the coordinates like using sinusoss and PosonsÔºå you can hardwire that„ÄÇ

 but it's better if you don't hardwire the position„ÄÇ

 you just it's just a vector that is always hanging out at this location„ÄÇ

 whatever content is there just adss on it and this vector strainable by background that's how I do it„ÄÇ

YeahÂîî„ÄÇI think they're kind of delegateÔºå like they seem to work„ÄÇ

 but it seems like sometimes our solution want to put sub structure„ÄÇÂíÅÂò¢‰Ω¢Á≥ª might„ÄÇÂóØ„ÄÇ

I' if I understand questionÔºå so I mean the position encoders like they they're actually like not they have okay„ÄÇ

 so they have very little inductive bias or something like that they're just vectors hanging out the location always and you're trying to you're trying to help them network in some way„ÄÇ

And„ÄÇI think the intuition is goodÔºå but„ÄÇLike if you have enough data usually trying to mess with it is like a bad thing like trying to like trying to enter knowledge when you have enough knowledge in the data set itself is not usually productive so it really depends on what scale you are if you have infinity data then you actually want to encode less and less that turns out to work better and if you have very little data then actually you do want to encode some biases and maybe if you have a much smaller data set and maybe coalutions are a good idea because you actually have this bias coming from more filters and so„ÄÇ

But I think„ÄÇSo the transformer is extremely general„ÄÇ

 but there are ways to mess with the encodings to put in more structure like you couldÔºå for example„ÄÇ

 encode sinuses and cosines and fix it or you could actually go to the attention mechanism and say„ÄÇ

 okayÔºå if my image is cho up into patches this patch can only communicate to this neighborhood and you can you just do that in the attention matrix just mask out whatever you don't want to communicate and so people really play with this because the full attention is inefficient so they will intersperse„ÄÇ

 for exampleÔºå layers that only communicate little patches and then layers to communicate globally and they will sort of do all kinds of tricks like that so you can slowly bring in more inductive bias„ÄÇ

 you would do it but the inductive biases are sort of like they're factored out from the core transformer and they are factored out in the„ÄÇ

In the connectivity of the notesÔºå and they are factored out in positioning and can mess this„ÄÇ

P proposition„ÄÇAndHow you pocketÔºüÂ§ß„ÄÇ‰∏çËØ•Êàë„ÄÇÂì¶„ÄÇSo there's probably about 200 papers on this now„ÄÇ

 if not moreÔºå they're kind of hard to keep track of honestly like my Safari browser„ÄÇ

 which is what's sort on my computerÔºå like 200 open tabs„ÄÇBut„ÄÇYeah„ÄÇ

 so'm not even sure if I want to pick my paper honestly„ÄÇ

And you can a maybe use a transformer like the other one that I actually like even more is potentially keep the context length fixed but allow the network to somehow use a scratchpa and so the way this works is you will teach the transformer somehow via examples in the hey you actually have a scratchy hey basically you can't remember too much your context line is finite but you can use a scratchpad and you do that by emitting a scratchpa and then writing whatever you want to remember and then end scratchpad and then you continue with whatever you want and then later when it's decoding you actually like have special logic that when you detect start scratchpad you will sort of like save whatever it puts in there in like external thing and allow it to attend over it„ÄÇ

So basically you can teach the transformer just dynamically because it's so meta learned„ÄÇ

 you can teach it dynamically to use other gizmos and gadgets and allow it to expandend his memory that way if that makes sense it's just like human learning to use a notepad right you don't have to keep it in your brain so keeping things in your brain is kind of like a context line from the transformer„ÄÇ

 but maybe you can just give it a notebook and then it con query the notebook and read from it and write to it maybe you another„ÄÇ

Yes„ÄÇÂíÅËßâÁé∞Âú®‰Ω†ÊÄé‰∏ã„ÄÇThe way going to this„ÄÇÈÉΩÂø´ÂòÖÂï≤„ÄÇ‰∏∫‰ªÄ‰Ω†ÂòÖ„ÄÇI don't know if I detected that„ÄÇ

 I kind of feel like did you feel like it was more than just a long prompt that's unfoldingÔºüÂí±ÂÜç„ÄÇ

I didn't try extensivelyÔºå but I did see a forgetting event and I kind of felt like the block size was just moved„ÄÇ

Âïä„ÄÇMaybe I'mÔºå I don't actually know about the internal of tragedy„ÄÇGet two„ÄÇSo one question is„ÄÇ

 what do you think about architectureÔºüS for S for„ÄÇI' say I don't know thisÔºå which one is this forÔºü

And second questionÔºå this was a personal questionÔºå what are you going to work on nextÔºüI mean„ÄÇ

 so right now I'm working on things like nanoGPT where is nanoGT„ÄÇüòäÔºåÂëÉ„ÄÇI mean„ÄÇ

 I'm going basically slightly from computer vision and like part kind of like the computer vision based products do a little bit in the language domain whereas tryGT„ÄÇ

 okay then on GT„ÄÇSo originally I had MinGÔºå which I ever wrotete to nanoGPT and I'm working on this I'm trying to reproduce GPTs and I mean I think something like chat GPT I think incrementally improved in a product fashion would be extremely interesting„ÄÇ

I think a lot of people feel it and that's why it went so wide so I think there's something like a Google plus plus plus to build that I think is very interesting„ÄÇ

Do we diversity run thoughtÔºü

![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_13.png)