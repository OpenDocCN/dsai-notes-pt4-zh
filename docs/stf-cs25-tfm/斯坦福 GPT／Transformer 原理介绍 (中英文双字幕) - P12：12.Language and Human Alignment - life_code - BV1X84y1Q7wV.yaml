- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P12ï¼š12.Language and Human Alignment - life_code
    - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_1.png)'
  prefs: []
  type: TYPE_IMG
- en: It's my pleasure to welcome Jan from OpenEI he leads the alignment team there
    and who was previously a researcher at Deep Mind as wellã€‚holds a PhD in reinforcement
    learning theoryï¼Œ has been thinking about the alignment problem for over 10 years
    and today he'll be giving a very interesting talk so hope you guys enjoyedã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Yeahï¼Œ thanks a lot for the intro and thanks a lot for having meã€‚ğŸ˜Šã€‚I'm very excited
    you talk about this stuff I'm also super happy to keep it interactive if you have
    questions at any pointã€‚please interrupt meã€‚ğŸ˜Šï¼Œå•Šã€‚Yeahï¼Œ I want to start out withã€‚A
    few very basic observations on kind of what I think is going onã€‚Andã€‚So the first
    one isã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_3.png)'
  prefs: []
  type: TYPE_IMG
- en: TmiI is joining the gameã€‚So hemiI has a lot of different factorsã€‚They aren't
    all joined at the same timeï¼Œ but rather they join one by oneã€‚And not all the their
    players are like vary a lot in how good they are and right now a lot of the players
    that have drawn so farã€‚I'm really that smart and usually can do it only a very
    narrow set of tasksã€‚è¯¶ã€‚ğŸ˜Šï¼ŒButã€‚
  prefs: []
  type: TYPE_NORMAL
- en: One thing that we've kind of observedd this that over timeï¼Œ you knowã€‚we're seeing
    strong and strong players jointã€‚And this is kind of where we are nowã€‚And then
    in generalã€‚We expect that Himmiai has some incredibly strong playerssã€‚So those
    will be playerss that are able to think so much better than humansã€‚
  prefs: []
  type: TYPE_NORMAL
- en: so much faster and so much more cheaplyã€‚Andã€‚These haven't joined yetã€‚And soã€‚å¯¹ã€‚Have
    like anchor fund that we haveï¼Œ if you thinkï¼Œ for exampleï¼Œ about chattPã€‚ğŸ˜Šï¼ŒChatTBT
    can already beã€‚Any human as like knowing more facts or speaking more languagesã€‚And
    it can write about 50 voice per secondã€‚And can do so about 100 times cheaper than
    humans could at the minimum wageã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And soã€‚You knowï¼Œ there'sã€‚The CheBT also has some really important limitations
    and there's a lot of things they can't doã€‚But it is kind ofã€‚An indicator ofï¼Œ you
    knowï¼Œ some of the players that maybe we be joining in the futureã€‚And so it seemed
    like and in the long runã€‚TmiI will have all the advantages over team humansã€‚Butã€‚And
    there is oneï¼Œ there's an important caveatï¼Œ which isã€‚
  prefs: []
  type: TYPE_NORMAL
- en: There's one important advantageage that Kim Kim hasã€‚which is team Human gets
    to pick which players from teammi I join and winã€‚And so this is kind of like an
    advantage that should we should really be leaning into when we're thinking about
    what to do and when we're thinking aboutã€‚you knowï¼Œ this game that we're playing
    with CMI and that we'll be playing with TIã€‚Ohã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So I think two of the main objectives of what we as team humans should do is
    like firstã€‚We should try to recruitã€‚Pes fromåœ°I to clientè¿›ã€‚And so this is kind
    of what I would broadly follow onã€‚And this is kind of like the problem that I'm
    withã€‚And then there is also other objectivesã€‚so another objective that I think
    is going to be really important is you want to write the rules of the game so
    that team human doesn't liveã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And right nowï¼Œ King human kind of has the ball and we get to write the rules
    so we should write the rules that you knowã€‚Makes sense andã€‚é‚£ä¸ªã€‚Still play this
    game in the futureã€‚And so in this talkã€‚I won't really talk about the second point
    at allã€‚and I will talk about the first point because that's why I know that's
    not where timeã€‚è¯¶ã€‚ğŸ˜Šã€‚
  prefs: []
  type: TYPE_NORMAL
- en: K to phrase it differently or to make it kind of like moreã€‚![](img/b473ddacf8908cc92f68d174d1de8757_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: Practicalï¼Œ like one way I'm thinking about alignment is like you want to build
    AI systems that follow human intentã€‚Andï¼Œ you knowã€‚Follow human preferences that
    do we want them toã€‚And so a bunch of the thingsã€‚basically I'll talk about two
    main thingsï¼Œ the first part is going to be work that we've done in the pastã€‚ğŸ˜Šï¼ŒAnd
    have likeï¼Œ whichs roughlyã€‚Is in the bucket of like we are trying to figure out
    how we can makeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: The models that we have today as aligned as we can and we're just kind of trying
    we're going to try hard to do this and we'll see how far we canã€‚And then the second
    bucket isã€‚The things that we have to do nextã€‚The stuff that we haven't done yet
    that we think are going to be really importantã€‚And I want to kind of like lay
    out why I think they can beã€‚å¯¹ã€‚Nowï¼Œ I saidï¼Œ you knowã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I I'm like trying to make this more clear or like more broken down what alignment
    meansã€‚so a not like hereã€‚Because now the big is like what does it mean to follow
    human attackï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_7.png)'
  prefs: []
  type: TYPE_IMG
- en: Theres like two main categories of intent that we care about isã€‚I would say
    if it enhance to youã€‚you knowï¼Œ like I give the assistant instruction or if you
    want to be my assistantã€‚it should be my assistantï¼Œ I should follow the instructionã€‚But
    then there's also all these other intents that don't say when I'm usuallyï¼Œ you
    knowã€‚
  prefs: []
  type: TYPE_NORMAL
- en: talking to a system or a humanï¼Œ they ultimately really care aboutã€‚like you know
    it shouldn't literally always do what I say but thing that I mean and it shouldn't
    make up stuff and it shouldn't you do harmful things and it should ask follow
    up questions when it's not sure what I mean and so on and so onã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And soã€‚These are all kind of like things they're often just likely difficult
    to like just not precisely specify or likeã€‚Andã€‚You knowï¼Œ what does I agreeï¼ŸBut
    it is still things that we want to get they do and that we have to figure out
    how toã€‚you knowï¼Œ get into itã€‚And so kind of like the main technique that we're
    using todayã€‚ğŸ˜Šï¼ŒFor thisã€‚is what we call important feedbackï¼Œ so that was used to
    train script GP and chat GBTã€‚
  prefs: []
  type: TYPE_NORMAL
- en: which has a tune like main in occupied this parkã€‚è¯¶ã€‚The the basic system is very
    simple and it's also like a super general technique that appliess to most of thisã€‚AI
    models and modalities and settingsã€‚But in this caseã€‚we'll be using manage quality
    and so off two stepsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: There is actually not a step of like chain limitations so I'm going to start
    with the thingsã€‚First that is you want to train one model from Compã€‚So you have
    on passed in this caseã€‚now explain when notã€‚Or you knowï¼Œ help me with my transã€‚Whatever
    it isã€‚and then the model does a bunch of things and then you made which one is
    like close to the thing that you intended the modelã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And so you have a bigger set of preferences than you train your reward model
    and the reward model basically just learn to predict which one would you preferã€‚Bookã€‚Everything
    okay I would say like just spend more in front of the camera as soon possibleã€‚but
    I think that looks goodã€‚ğŸ˜Šï¼ŒSorry about thatã€‚Maybe let's turn it a little bitã€‚Oå¯¹ã€‚æ²¡æœ‰ã€‚Okayã€‚so
    now we have this through our model that captures kind our preferences and what
    we care about and what we intend for the model to doã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And then the second step is now you optimize your reward model with the inputã€‚ğŸ˜Šï¼ŒAnd
    so not settingã€‚you knowï¼Œ like model tries a whole bunch of different things and
    then what model have tells it which one would the assisted things is probably
    more like the thing thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: æ²¡æœ‰ã€‚They will depend on the letterï¼Œ different label ofã€‚They also might be inconsistencies
    and you can give these examples of like in preferencesã€‚ğŸ˜Šã€‚But those haven't really
    been a problem inã€‚And so farï¼Œ you knowï¼Œ like our flights often don't agreeã€‚but
    the model average overall thes haveã€‚ğŸ˜Šï¼Œä¸‹ã€‚ä¸ yeahï¼Œ soã€‚
  prefs: []
  type: TYPE_NORMAL
- en: This is like the basic technique is conceptual like quite simpleã€‚you can make
    it even simpler if you hadï¼Œ you knowã€‚if you didn't translate into one model and
    you label instead like everyR thatã€‚ğŸ˜Šã€‚But it would be a lot less data efficient
    and so you can turn your one on to think upã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_9.png)'
  prefs: []
  type: TYPE_IMG
- en: å¯¹çš„ã€‚å¯¹ã€‚So how will it work so this is kind of like one of the main thoughts from
    the in sheet paper and this is the one I like showing becauseã€‚It really blew my
    mind and it still doesã€‚What do we see hereï¼ŸOn the X axisã€‚you see this is from
    the GP3 model series and you see this is like different three different size of
    models over two orders of magnitudeã€‚ğŸ˜Šï¼ŒAnd then the y axis is how well does the
    level score on human preferenceï¼ŸğŸ˜Šã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So if you show a bunch of samples to humansï¼Œ how likely are they to prefer one
    over the otherï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: And then what we see is that even like the largest G3 modelã€‚ğŸ˜Šã€‚Is this preferred
    to the smallest instruct G variant and so the 100x smaller instruct modelï¼ŸğŸ˜Šï¼ŒAnd
    ifã€‚Actually preferredã€‚Over the much largerã€‚æ¥å±æˆ‘æ™’æ¥å±ç”µè¯ã€‚And that's kind of wildã€‚Sorryã€‚let
    me just finish my talkã€‚ So whyã€‚æˆ‘ä¹Ÿè¿™æ˜¯æ²¡å†™å°±æ²¡é€€è¿‡å»ã€‚It basically shows that it would like
    to be on the line of COD that making them whole more made them so much more useful
    than was getting outã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒOr fine nice worse nobody want to use itã€‚And can you guys hear me we make
    all these fancy alignment techniques that don't get adoptedã€‚And so what we wereã€‚Like
    originally in like the first version we that saw these regressions and then what
    here is labeled PPO PX is kind of like a very end where we mix in free training
    data into the fine tuning and that mitigated a bunch of the regressions that we
    sawã€‚
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚ğŸ˜Šï¼ŒYeahï¼Œ and I just had to put a follow up to thatã€‚How would importantã€‚It's
    like fidelity of fine team that youã€‚You guys you collect data from humans right
    what if you were to use some pretrainmigrant model to score you know or something
    like that yeah and you could do that and there have been like other papers on
    that like I thought they wrote this paper on constitutional AI that was trying
    to like exactly do thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: in terms of well there's certain things that the language model will be able
    to automatically rank and some things it won because you knowã€‚it won't know your
    exact preferences or it won't know exactly what like what we wanted to do and
    so you know whenever the language model does somethingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: That we just prefer we actually we have to give it another data point right
    or in other wordsã€‚you knowï¼Œ if you're aligning with humansï¼Œ you somehow have to
    put humans into the loop so that you know otherwiseã€‚How does the model know what
    it's supposed to doï¼ŸLots of more questionsï¼Œ I don't know who was Chrisã€‚ğŸ˜Šï¼ŒYeahï¼Œ
    how many you mean approximately like what's how many orders of magnitude of like
    submitted preferences do you need to achieve these yesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I'm going to get to that in a second sureã€‚questionï¼Œ I think someã€‚Yeahã€‚ğŸ˜Šï¼ŒSo why
    you an experimentï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: We havenã€‚We haven't actually carefully compared across our algorithmsã€‚and it
    could very well be that a different neural algorithm would be betterã€‚ğŸ˜Šï¼ŒThat was
    connect lifeã€‚I know PVO was invented open the eyeï¼Œ so that's why we used itã€‚It's
    not not really a good reason other than thatï¼Œ it works also pretty wellã€‚Yesã€‚eCompisonsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: this is better than this other thingã€‚So usually we have people compare between
    like three to six different responses from usually different modelsã€‚Okayã€‚Yeahã€‚ğŸ˜Šï¼ŒSo
    is PPO and the reward mode currently used in chat GT in production and it's so
    like do you use any of the human feedback like you know you can regenerate responses
    and stuff like that to helpã€‚
  prefs: []
  type: TYPE_NORMAL
- en: As a reward function as wellï¼Œ how do you mean to regenerate like there's a button
    onT where you can say about regenerate responses or do you use any implicit feedback
    basically in human use I don't know what the current state is for that I expect
    people will try to use itã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but you knowGBT hasn't been out that longã€‚Yeahï¼Œ so I'm curious about this graphic
    like it seems like 100 x as you mentioned increasing parameters doesn't give you
    that much more like fidelity there qualitatively you have been tracked this for
    a whileã€‚
  prefs: []
  type: TYPE_NORMAL
- en: can you tell right off the bat if you're like interacting with the 1 billion
    like a model or the like 100 billion model like auring pseudoturing test but parameter
    size like I give you a black box can you tell me how many parameters it hasï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒProbably not very preciselyã€‚But I think the big counter question is like do
    I get to write the prompt so if you just draw random prompts from whatever people
    put in the opening at playgroundã€‚which is what we use points for our TPTï¼Œ then
    I probably need quite a few to tell the difference but if I get to write the prompt
    I can probably do it in one or to at least like if the task is like tell the difference
    between this and thisã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Yeahã€‚I want to can I just do two more slides and maybe your questions get answered
    and thenã€‚![](img/b473ddacf8908cc92f68d174d1de8757_11.png)
  prefs: []
  type: TYPE_NORMAL
- en: so this was the question about training costs so this is another thing that
    kind of really blew my mind is like compared to pre training it is incredibly
    cheap so if you look at like the amount of labs that it takes to train GB to eat
    and then you compare it with like how much does fine tuning and the R withs pre
    training makes and everythingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼Œlike the most expensive instructorB version is like less than 2% of the pre
    training computersã€‚ğŸ˜Šã€‚And if you want to train an even bigger modelï¼Œ it's going
    to be more expensive and you could still use the same like fine unique step to
    make it more alignedã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And of courseï¼Œ I think the important thing to note also here is like we haven't
    fixed all the problemsã€‚there's like important limitations and so I wouldn't say
    that this is like you know the last version and we wouldn't try to figure out
    how to spend more compute and more human data in the futureã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šã€‚![](img/b473ddacf8908cc92f68d174d1de8757_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: But all in allï¼Œ it was surprisingly effectiveã€‚![](img/b473ddacf8908cc92f68d174d1de8757_15.png)
  prefs: []
  type: TYPE_NORMAL
- en: Okayï¼Œ there were more questionsã€‚ğŸ˜Šï¼ŒMy questions Yeahï¼Œ I just wanted to ask what
    the PTxã€‚Mixing pre training data into the IO F tuneï¼Œ just like mixing gradientsã€‚Y
    quick oneã€‚what's the number of parameters for this graph so you fixed a number
    of branches for this so this is the full size GP3 versionã€‚so this is the 175 billion
    modelã€‚Soã€‚O questionssï¼Œ noï¼Œ okayã€‚
  prefs: []
  type: TYPE_NORMAL
- en: there's also some questions on Zoom great Okay sureï¼Œ so the busman isã€‚Okayï¼Œ
    sureã€‚so the first question is how would you deal with all edge breaking in the
    limitã€‚example preferences are a good proxy for valuesã€‚But optimizing for them
    is terized to incentiv device perceptionã€‚Yesï¼Œ I'll get to thatã€‚ğŸ˜Šï¼ŒSureã€‚Sure that
    the next question so that is like you want to automatic alignment research what
    happens if you need conceptual vi which other people are experts to5 Okayã€‚
  prefs: []
  type: TYPE_NORMAL
- en: that would be I get a take at the end as well sure let's see sorryã€‚Yeahã€‚I guess
    like one question is likeï¼Œ how would fun feeling directly on human feedback compared
    to fan powerï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_17.png)'
  prefs: []
  type: TYPE_IMG
- en: m fine tuning like supervised fan tuningã€‚But it's more like if you directly
    use the human picker dataã€‚Yeahï¼Œ I'm also not sure whether it beã€‚U so I mean so
    one baseline I'm showing here is like what if you just take human demonstrations
    in the sense thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you knowï¼Œ we have a bunch of tasksï¼Œ we just ask humans to do themã€‚record what
    they did and then train the model to imitate that and here it's like just very
    basic behavioral cloning just using the same loss to use and retrainingã€‚And thenï¼Œ
    you knowï¼Œ it is noticeably better than the H shot hunted versionã€‚but it's still
    not as good as at allã€‚And so that's why we like using our and basicallyã€‚Conceptuallyã€‚
  prefs: []
  type: TYPE_NORMAL
- en: there's two problems with the imitating humans approach one isã€‚Humans are better
    at some things than the model is and they're worse at other things and so at the
    things that the model is worthã€‚you try to imitate something that you can't doã€‚And
    on the things where the model is betterã€‚you're making the model worse because
    you're forcing it to do the thing in the way that the human wouldã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Whatã€‚And soã€‚With our all you with our other Jeffï¼Œ you're kind of letting the
    model do whatever it wants toã€‚and it can just figure out like the best way for
    it to do thingsã€‚![](img/b473ddacf8908cc92f68d174d1de8757_19.png)
  prefs: []
  type: TYPE_NORMAL
- en: It's also another important advantageï¼Œ and I'm going to get to that but briefly
    want to talk about Cha GBTã€‚ğŸ˜Šï¼ŒSo one thing I kind of think of CaGBT is like the
    upgrade to instructGBT it's kind of like the next step at making the models more
    aligned and more useful to humansã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And some things that's likeï¼Œ you knowï¼Œ I think chapter does better is kind of
    like using dialogue as the universal interface right you can talk to it directlyã€‚you
    can ask follow up questionsï¼Œ you can ask it to you know refine the answer and
    all of these things that makes it a lot easier to deal withã€‚
  prefs: []
  type: TYPE_NORMAL
- en: It's better at refusing harmful tasksï¼Œ but it's also there's still important
    limitations right like the biggest one is like the model of hallucates a lotã€‚it
    makes up facts when you knowï¼Œ for whatever task you give it and that you know
    is makes it quite unreliable it's also still sensitive to prompting which kind
    of shows thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒYou knowï¼Œ it still has important misalignmentã€‚That we need to fixã€‚Like reallyã€‚if
    a model was like model should really likeã€‚Do the task to best of its ability no
    matter how you're prompted to do thatã€‚å—¯ã€‚ğŸ˜Šã€‚![](img/b473ddacf8908cc92f68d174d1de8757_21.png)
  prefs: []
  type: TYPE_NORMAL
- en: But yeahï¼Œ one important principle that I thinkã€‚Is really useful for or that
    like our that Jeff leans on a lot is that evaluation is easier than generationã€‚ğŸ˜Šï¼ŒSo
    if we ask humans to compare and rank different responsesï¼Œ the model gaveã€‚å—¯ã€‚ğŸ˜Šï¼ŒIt
    isã€‚Easier to tell the difference between different variants of what the model
    did than is to do the task itselfã€‚Or in other wordsï¼Œ you knowï¼Œ you can do the
    comparisons on tasksã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you could still like spot good behavior on tasks that you might not be able
    to do well yourselfã€‚And so if you're giving this kind of like feedbackã€‚That letsï¼Œ
    you knowã€‚the system should do better than you actually couldã€‚And I think that's
    a very general principle that holds in lots of domainsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: so kind of like you're probably most familiar yesï¼Œ you know the P versus NP
    and everyoneã€‚you know we don't actually know whether they're different but in
    practice it seems like NP tasks they're just much harderã€‚![](img/b473ddacf8908cc92f68d174d1de8757_23.png)
  prefs: []
  type: TYPE_NORMAL
- en: Umï¼Œ it also applies to lots of other settingsï¼Œ like a lot of professional sports
    or esports just wouldn't be fun to watch if he couldn't tell he was winningã€‚More
    easily than you could actually compete on a professional levelã€‚it applies to a
    lot of consumer productsï¼Œ you can like look at two smartphones and tell which
    one you like more without that is like also deeper than just looking at like the
    specsã€‚That it is actually very high to pull this to my phoneã€‚It also applies to
    academic researchã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you knowï¼Œ it's much easier to view a paper and say what all the things that
    are bad about itã€‚Thenhan it is to write a good paper yourselfã€‚It applies toï¼Œ I
    don't know when youã€‚å—¯ã€‚Yeahã€‚basically there's lots of domains where the appliesã€‚and
    so I think this is like a veryã€‚ğŸ˜Šã€‚This principle is like very useful when we want
    to like align AI systems on tasks that we might not be able to do ourselves wellã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_25.png)'
  prefs: []
  type: TYPE_IMG
- en: Okayã€‚Okayï¼Œ so having said thatã€‚I like Jeff has some really important limitation
    and I think that's going to make itã€‚Really difficult to use ourHf to scale alignmentã€‚![](img/b473ddacf8908cc92f68d174d1de8757_27.png)
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚ğŸ˜Šï¼ŒLet me explain this with a diagramï¼Œ so basically on the X axisã€‚That's plot
    like the AI progressã€‚And on the Y axisï¼Œ how difficult different tasks areã€‚And
    then as we have more AI progressã€‚kind of like the tasks that AIï¼Œ the difficulty
    of tasks that AI can do goes upã€‚And likeã€‚One of the fundamental problems is that
    the level of tasks that humans can reliably evaluate doesn't go up because humans
    don't get better with AI progressã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And soã€‚I think we're like somewhere hereã€‚But the problem is once you cross this
    lineã€‚you don't really knowã€‚ğŸ˜¡ï¼ŒWhat like whether your model is actually doing the
    right thing because you can't reliably evaluate anymore and so that's kind of
    like the point where RLHF training will start to break down and what we probably
    see is kind of what the question before I allud it to is like well now the systems
    are optimized for whatever feedback we give them and so they will try to tell
    us what we want to hear rather all the things that they know to be true and you
    know they might learn how to deceive us because you know that makes it easierã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Justis score high on preferenceã€‚And so kind of likeã€‚The basic idea that we want
    to leverage is related to the principle I just mentionedã€‚which is evaluation student
    easier and generationï¼Œ so for exampleã€‚if you have a large language model writing
    a codeb like an entire codeb there's just no way humans would be able to find
    all the bugs and all the flaws in the codeb or you know the codebase could have
    like a Trojan in there and you might not be able to tellã€‚
  prefs: []
  type: TYPE_NORMAL
- en: because it is so hard and that's why we see so much buggy code out thereã€‚But
    if you're asked your language model to find bugs and point them out to youã€‚Once
    you'veï¼Œ you knowã€‚once you've seen the bugï¼Œ it's so much easier for you to sayï¼Œ
    oh yeahï¼Œ this was a bugï¼Œ you knowã€‚please fix itã€‚And so now you've taken the task
    of footbase down toã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I just have to evaluate whether that was the bug according to the spec I had
    in mindã€‚And so the general principle that we are excited about here is like we
    want toã€‚ğŸ˜Šã€‚He leverage AI assistance for human evaluationã€‚And so the hope is that
    we togetherã€‚if you pair up humans with AIï¼Œ you actually get a line that looks
    more like this whereã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you know like humans together with the AI can evaluate much more than they could
    by on their ownã€‚ğŸ˜Šã€‚![](img/b473ddacf8908cc92f68d174d1de8757_29.png)
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚ğŸ˜Šï¼ŒAnd soã€‚To make this concrete there's like two different ways you could do
    that or there's many different ways you could do that too i'm going to highlight
    it' like first you can write ask AI to write a critique this is a project we did
    last year and in this case there was a simple summarization task and we train
    a language model to kind of like just sayã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒThings that are wrong with the summaryã€‚Um andã€‚![](img/b473ddacf8908cc92f68d174d1de8757_31.png)
  prefs: []
  type: TYPE_NORMAL
- en: There's other things you could doï¼Œ for exampleï¼Œ you could give people chat GPT
    and ask them okayã€‚use chatGT to help you evaluate and then you could ask for a
    critique or you could ask for a lot of other things you could ask for an explanationã€‚you
    can ask for fact checking or quote or you know whatever the model like chatGBT
    can actually reliably help you withã€‚And soã€‚The idea would be thatï¼Œ you knowï¼Œ like
    using YaI assistanceï¼Œ you can kind of getã€‚
  prefs: []
  type: TYPE_NORMAL
- en: All the smart city I haveã€‚And leverage that in order to figure out how you should
    evaluate what this system is doing and like whether it's aligned with your preferences
    or whether it's trying to receive youã€‚Andã€‚The big problem with this is how do
    we know whether it's workingï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: One of the kind of like difficulties is thatã€‚By assumption we're kind of dealing
    with a hard task where it's difficult to evaluate and we also want the task to
    be real because we don toã€‚you know we don' to want to solve some high tasks that
    doesn't matterã€‚And soã€‚It becomes differentã€‚
  prefs: []
  type: TYPE_NORMAL
- en: so you need like a hard task that is realï¼Œ but also if you don't if you have
    those you usually don't have ground too so you don't know which was the right
    answer and how do you know whether assistance is working or it's biasing everyone
    to say the same thingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And soã€‚There's a simple technique that we use in the critiqueã€‚To do thisã€‚Where
    we're like that we call targeted perturbations and so what you do is you have
    a bunch of promptsã€‚so this could be like whatever people type into chat UTã€‚And
    then youã€‚Kind of like take the response that you have and you say like this is
    their correct response it doesn't actually have to be correct but let's just assume
    it is and now you're asking a human to introduce some kind of subtle flaw that
    is hurt like easy to miss but is an important flaw and know what you have is you
    have this peer data set of like a good response and a bad response and you know
    which one is good and bad because you made it worseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And soï¼Œ you knowã€‚in a wayï¼Œ that gives you grantã€‚And so what you now can do is
    you canã€‚ğŸ˜Šã€‚Take like randomly select one either the correct or the flawed response
    and then show it to either a human or a human with assistanceã€‚and then they have
    to say whether it was the corrector the flawed one or like how good the response
    isã€‚ğŸ˜Šï¼Œå—¯ã€‚Andã€‚ğŸ˜Šï¼ŒIn this settingï¼Œ we can kind of likeã€‚Figure like just try a lot of
    different AI assistance or your scalable oversight techniques and then sayã€‚
  prefs: []
  type: TYPE_NORMAL
- en: You knowï¼Œ like is thisï¼ŸActually helping humans find the flaws that we planted
    more lively than notã€‚And if it isï¼Œ then you knowï¼Œ like you're actually really
    helpingã€‚![](img/b473ddacf8908cc92f68d174d1de8757_33.png)
  prefs: []
  type: TYPE_NORMAL
- en: And so we did this in the critiques paperï¼Œ so this is training the language
    models to write critiques and for summaries and what we can show is that when
    were assisting humans with critiques at the evaluationã€‚they actually find 50%
    more flaws than it did without and so this is kind of like real signs of life
    that you can already use the models that we can have todayã€‚
  prefs: []
  type: TYPE_NORMAL
- en: To help humans evaluate and like find problems they would have missed otherwiseã€‚And
    of courseã€‚we still have to do thisã€‚Like on a much harder task and like withã€‚Or
    like a real task in a sense and we also want to have likeã€‚Bigger effect sizeã€‚but
    I think it's just likeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_35.png)'
  prefs: []
  type: TYPE_IMG
- en: It shows that there's promise of these techniques already workingã€‚And so in
    the long runã€‚ğŸ˜Šã€‚Um what I think we want to get to is we kind of want to leverage
    EI for all the cognitive labor that goes into evaluating whatever our E systems
    are doingã€‚And this could beï¼Œ you knowï¼Œ like readingating everything that's relevant
    or fact checking or doing calculations or like writing code or any of these thingsã€‚and
    then humans should focus on like their preference input like the things figuring
    out what they actually care about and what they wantã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒThe model to doã€‚Andã€‚å—¯ã€‚ğŸ˜Šï¼ŒAnd this wayï¼Œ you can kind of like leverageã€‚You knowã€‚like
    the abilities thatï¼Œ you knowï¼Œ the AI players will bring to the table and the things
    that they will be better at than us eventuallyã€‚And thenã€‚Kind of like use them
    to help communicate the thing that we actually care about and you knowã€‚the things
    that we actually want them to doã€‚And yeahï¼Œ that's itã€‚ğŸ˜Šï¼ŒBut yeahã€‚
  prefs: []
  type: TYPE_NORMAL
- en: those are like the main slidesï¼Œ I'm happy to take more questionsã€‚Yesã€‚I was wondering
    about this hallucination of responses Have you ever tried to consider some notion
    of uncertainty in the access but' sampling or something like this have value variance
    between the responses and somehow valueã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Yeahè¿™ã€‚So ensembling is difficult because either you're like training in a fine
    training ensemble from the same pretrain model and so you don't get that much
    barrier in you ensemble or you have pretraining a bunch of different models and
    now you're spending a lot of money on pretrainingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: One thingï¼Œ I meanã€‚It seems like it should be a solvable problem to justã€‚C the
    model to say it's uncertain when it's actually uncertainã€‚And there's been a bunch
    of research in that direction butã€‚I think right now it's still like we're not
    really in a good shapeã€‚Smart stuff to doã€‚Sheã€‚Yeahã€‚
  prefs: []
  type: TYPE_NORMAL
- en: do you think we may run into a kind of signals and noise ratio problem when
    it comes to AI suggested critiques to AI answers because Im sure like when AI
    is trying to point out potential problems with textã€‚humans are more likely to
    support more problemsï¼Œ but what if it's noing problems that humans wouldn't have
    necessarily had a problem to begin with Yeahã€‚
  prefs: []
  type: TYPE_NORMAL
- en: so we did try to control for that a little bit by like having humans rate the
    severity of the laws and whether they would have noticed them otherwiseã€‚they can
    still see a significant effectã€‚but also like I mean a lot of the time the model
    is net picking and then those are like not the interesting cases yeah also if
    you like look at the example I showed I think is from the blog post like a lot
    of the keys are just actually quite garbage and one of the likeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒThings that makes it easy for critiques isã€‚It's okay the most of them are
    garbage because the human can just beat them and discard them and it kind of likeã€‚Moreï¼Œ
    you knowï¼Œ helps the evaluatedval know where to focus on or likeã€‚Notice like think
    of something they would have missed otherwiseã€‚So it's more likeã€‚you know the critiques
    help you brainstorm how you should evaluate or somethingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but if you're kind of like using an assistantï¼Œ you probably want more reliability
    than like filling most of the answers to whyã€‚ğŸ˜Šï¼ŒYeahã€‚Yesï¼Œ how do we ensure that
    the evaluation metrics we are using in your recursive word modeling approach like
    detect deception like left turn or something don't have like major discontinuitiesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Yeahï¼Œ I thinkã€‚Wellï¼Œ it depends a lot what kind of community you're talking about
    rightï¼ŸIfã€‚You knowã€‚you get overnight like and model letters that say  a thousand0
    inches larger on like number of parameters or like equivalently betterã€‚I can likeã€‚Create
    quite a step upï¼Œ and that's makes it quite difficult toã€‚Do this kind of be evaluationï¼Ÿå—¯ã€‚ğŸ˜Šï¼ŒSo
    in that senseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I think it's going to be very important to like scale up BI more continuously
    and like do more incremental stepsã€‚Having said thatï¼Œ I thinkã€‚The basic picture
    with the request from reward modeling isã€‚That you're training the systems to help
    you evaluateï¼Œ you knowã€‚Systems to train from or find you from the same system
    right and so if you can figure out how to getã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Like finding them in the way that they mention everything that is actually useful
    and relevantã€‚then it would still be able toã€‚Evaluate systemsï¼Œ even though they're
    much smarter than anything you could have evaluated beforeã€‚Maybe let me even make
    that more concrete because there's I think a way that you could measure that or
    like one way we've tried it was using what we call the discriminated critique
    gapã€‚So to measure thatï¼Œ you're training two different or you fine training two
    different models from the sametrain modelã€‚
  prefs: []
  type: TYPE_NORMAL
- en: One is a discriminator just says likeã€‚Is this the flawed response or the correct
    response or more correct responseï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: And the other one is a critique model that tells you what the flaw wasã€‚And now
    you can measure how often did the discriminator get it correctly and the discriminator
    is just like pure gradient descentã€‚it doesn't have to want to tell you anythingï¼Œ
    you're just basically hooking into the model through presentations and trying
    to like get all the relevant latent insight it has to this prediction task and
    then on the other side other hand you have this critique model or this general
    assistance model and you're measuring how often does the assistant actually help
    me point to the right flawã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And then if you compare these two scoresï¼Œ you can like look at the gap between
    themã€‚Henceã€‚sorry discriminated critique gapã€‚And then if the gap is largeã€‚then
    the model knows a lot about the tasks it's not telling youï¼Œ and so that would
    be very worryingã€‚And the gap is small thenã€‚You knowï¼Œ there's muchã€‚Isumably there's
    like fewer things that the model is like not telling youã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but I guess there's like lots of value in like ensuring this this approach is
    like robust and like contest testing it on learn fromã€‚That's rightã€‚That's why
    we want to test it on current modelssã€‚Yeahã€‚Yesï¼Œ I don't know who was firstã€‚I think
    someone in the backã€‚ğŸ˜Šï¼ŒIã€‚So I wanted to ask about like maybe first the NX slide
    where like there was like human strong was likeã€‚And soï¼Œ you knowï¼Œ I've been helping
    notice like part of that also is like communicating what you want the AI to doã€‚
  prefs: []
  type: TYPE_NORMAL
- en: rightï¼Œ not just like evaluatingï¼Œ but like communicatingã€‚Perhaps likeï¼Œ I would
    like you to do thisã€‚And maybeï¼Œ or maybe I thatã€‚And so like at least like in my
    personal experience using the chatã€‚like there are some things that could do thatã€‚It's
    crazyã€‚for instanceï¼Œ And I was likeï¼Œ noã€‚like how did that comeã€‚Orï¼Œ you knowï¼Œ if
    you can ask about like clickã€‚
  prefs: []
  type: TYPE_NORMAL
- en: there' are like different things right Or I'm likeï¼Œ okayï¼Œ likeï¼Œ what can I ask
    for and likeã€‚what point is thatã€‚One thing that I thought was a bit concerning
    with just this idea that likeã€‚you knowï¼Œ people don't always communicate their
    preferences likeã€‚Honestlyï¼Œ we're likeã€‚Like there could be like like coordinated
    effortsï¼Œ rightã€‚
  prefs: []
  type: TYPE_NORMAL
- en: like like instill rewards for like specific capabilitiesã€‚you knowã€‚like coordinated
    effort to do such a thingã€‚one ideaã€‚like I had this was like I tried to ask if
    that has like some idea of like Wi for itself' I didn't know how to use it at
    firstã€‚So I just thought like maybe userã€‚ There didn't seem to be oneã€‚ But like
    I was hoping there was oneã€‚
  prefs: []
  type: TYPE_NORMAL
- en: there was one for like G3ï¼Œ right Like I think fman a little I was hoping And
    so my question is likeã€‚UmHow how do you like make that sort of like thing safe
    right like have you like recognize coordinateordin efforts to like you know like
    specifically reward certain kinds of behavior maybe like some group on decides
    that they would like toum yeahã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you know give it some capability so this is a you know like yeah this is a really
    good question and like in a wayã€‚I meanï¼Œ the first obvious thing that you shouldn't
    do is like you shouldn't just like literally train on the data that people give
    who like using the interface and we've kind of like seen other examples of what
    happens we do that if you think of like Microsoft T or something that can go pretty
    wrongã€‚
  prefs: []
  type: TYPE_NORMAL
- en: the other thing isï¼Œ I mean right now what we're doing is like we're hiring a
    bunch of people and then ask them to rate different model responsesã€‚But also now
    the question becomes likeï¼Œ you knowï¼Œ who will be hiring and like what's their
    backgroundã€‚what they're trying to do and soã€‚And in particularã€‚like the thing I
    think we're doing quite poorly right now is like actuallyã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Like importing like a diverse and representative set of human preferences and
    more just like you know whoever we end up we can hire and so I kind of wish there
    was also this likeã€‚More targeted research on like how we should do that and how
    that could be done well and some of it is also like you know better placed outside
    of like big tech companies because if you are like tech companies always have
    an incentive to like you knowã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Yesã€‚Import human preferences in the way that maybe is not like the thing that
    we actually humanity would do under flex or thing and so I think it's a really
    big important question like data contaminations like the dual problem to this
    like do you think obvious yeah I mean like is that something people might can
    anyone can poison in free training way just put something on the internetï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: And it'sï¼Œ you knowï¼Œ something that we have to be very mindful ofã€‚Yesï¼Œ I don't
    knowã€‚Considering that we're currently training these models should also work our
    these thingsã€‚Hofully getting closer to preferences at this pointã€‚As human practices
    changeã€‚as we've seen like pastï¼Œ like justã€‚is there somethingã€‚They should keeping
    up that's likeã€‚Yeahã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I meanï¼Œ the most obvious thing is its like the model knowledge base is kind
    of like the pre training cutoff date like somebodyã€‚you knowï¼Œ whatever data you
    went into pretrainï¼Œ it doesn't know about like a lot of things that happened after
    thatã€‚U in terms of updating kind of like human preferences or theï¼Œ you knowã€‚like
    the comparisons that go into the robot modelã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you just collect more data and retrain and the functioning run is like comparatively
    cheap so you can you know do that againã€‚I think what gets harder is that you know
    likeã€‚As you have deployed the model and people started using it for all kinds
    ofã€‚you knowï¼Œ tasks that they want to build their company around like theyã€‚Umã€‚if
    you update and you change the modelï¼Œ then they also have to do a bunch of work
    into like adopting that prompt to whatever they have they're doingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And so it doesn't come as with zero costã€‚Yes sorry you so I't know of' seeing
    human performance but one of the the advantage of G3 is that it has this immense
    corpus of the invasive the entire internet if you want to specialize in a specific
    domain like chemistry or material materialial science or something in particular
    to know generate new contactsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: å¯¹ã€‚å·±çŸ¥å˜…ä½†å’§åšŸã€‚To use less dataï¼Œ it' still where fast we should beã€‚You mean like less
    data on like the chemical domaination thing yeah you may just have research papers
    over the last 30 years or something yeah and you can throw it into pretraining
    right and then the model knows about it but can the model really learn effectively
    without so much dataï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: Or cant be somehow adapt the abstract concept behind GP3ï¼ŸOr that's okayã€‚Yeahã€‚I
    mean that's kind of the general idea with what you intend to do with fine training
    and to some extent we've seen it like generalizing this wayã€‚for exampleï¼Œ instructorstructBT
    was trained almost entirely on Englishã€‚Languageã€‚feedback and demonstrationsã€‚And
    it works in other languages and so that's kind of wild and so similarly you know
    you could train the model with people who don't know anything about chemistry
    and then you know it learns to follow instructions and it will do so like on the
    topic of chemistry and this fine can be very sample efficient like with 100 data
    points you can actually make a meaningful change in the model behaviorã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So it can be quite effectiveã€‚I that pick someone hasn't askedï¼Œ yesã€‚Regardingã€‚Response
    generationã€‚do you or how much ever effort do you put on our emphasis and training
    on different expression styleã€‚So what I've noticed from activity that it always
    gives you back very structured to scientifically structured answersã€‚Do you consider
    any training ifã€‚It turns you like a scientific the scientific East Church should
    answer or rather an asteriskã€‚
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚Yeahã€‚I meanï¼Œ the tricky thing is ideally the model should give you the kind
    of answer that you want to have right and some people prefer more scientific or
    technical answerã€‚some people might prefer a more generic answerã€‚Andã€‚Me mean right
    nowã€‚like chat GT doesn't have likeï¼Œ you knowï¼Œ a way for you to set like your specific
    preferences and that's something thatã€‚you know would be really exciting to haveã€‚But
    alsoï¼Œ I thinkã€‚
  prefs: []
  type: TYPE_NORMAL
- en: The kind of sta probably that you've observed are is in fact like probably a
    product of our labeler poolã€‚And so a lot of the chatB key workers were like moreï¼Œ
    you know likeã€‚I think more like computer scienceency and like more there was like
    more data generated by programmersã€‚Compared to instruct GTï¼Œ which was more like
    generalist labelersã€‚Andã€‚And yeahã€‚
  prefs: []
  type: TYPE_NORMAL
- en: there's like differentï¼Œ it's like kind of it changes also the styleã€‚So there
    is no specificã€‚To distinguish thatã€‚Yeahï¼Œ I meanã€‚We should make a distinguished
    effort it should give you like the style that you wantã€‚rightï¼Ÿå—¯ã€‚Yesã€‚So one of the
    things that Im thing about is howã€‚Is get a quick actuallyã€‚Inçš„Iã€‚Your generation
    or coming generation and so go back toã€‚The AI progressã€‚å¯¹ã€‚Human levelï¼Œ yeahã€‚
  prefs: []
  type: TYPE_NORMAL
- en: let humans get evaluatedï¼Œ what I'm sorry to think about is likeã€‚Over great I
    have you as Ive showed like my 10 year old cousin how your cha routine just to
    mess around with and that green line is a lot overã€‚U and furthermoreï¼Œ if SA this
    becomes part of their educational experienceã€‚it's going to be much better I ever
    perceive it to be more difficult for them to discriminate even simpler tasksã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And so I are thinking about itã€‚How that might disrupt or make this alignment
    a little bit more difficult in the long run as you have people who are more rotateã€‚for
    instanceï¼Œ what Jack sorry Jack QBC says as like a given truth anywayã€‚I was just
    wondering whatï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: I meanï¼Œ there's a real risk of overlying on a tech that is immature and that
    is not ready forã€‚you knowã€‚You just believing like please don't believe everything
    the model sets right but also I think one thingã€‚ğŸ˜Šï¼ŒThat I'm hopeful for is that
    likeï¼Œ you knowï¼Œ your cousin will end up like figuring out how to do this where
    likeã€‚you knowï¼Œ they grew up withã€‚You knowï¼Œ like all of these AI tools that are
    getting better and learning how to actually leverage themã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒProductivelyï¼Œ rightï¼Œ and like it's kind of likeã€‚You knowã€‚20 years ago or something
    when you were like you like using Google search much earlier than everyone elseã€‚you're
    probably going to get better at like using that as a tool for everything you want
    to doã€‚å—¯ã€‚Yeahã€‚I think you headed your hand up for a whileã€‚The slide where tasks
    inã€‚å‘ƒ chat testã€‚Model tasksã€‚å—¯ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: å“¦ï¼Œå–‚ï¼Œå¯¹ï¼Œ last oneã€‚Yeahï¼Œ so right nowï¼Œ it seems like you guys areã€‚Using humans
    as biological sensors to the real world to like physical ground truthã€‚Andã€‚Using
    language as like a compressed interface to that ground truthã€‚are you guys also
    looking at usingï¼ŸAccessor technology directly with your models to get a more truthful
    answer ofã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you knowã€‚Yeahï¼Œ I meanï¼Œ it depends on what that sensor could be right like I
    guess like one of the most straightforward things is you could ask the model to
    browse and then it can like fact check its own answers and it can you know likeã€‚ğŸ˜Šï¼ŒImport
    external knowledge that I didn't rememberã€‚Andã€‚ğŸ˜Šï¼Œè¯¶ã€‚Yeahï¼Œ I think that would be
    quite usefulã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I think thatll be quite useful for assisting human evaluationã€‚ğŸ˜Šï¼Œè¯¶ã€‚Whats that
    going workï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: And you can look at WebGbTï¼Œ which you knowï¼Œ is a published work on using the
    model for browsingã€‚å—¯ã€‚I think so one thing that makes it harder when you're using
    this like external sensors or if youã€‚Letting the model interact more directly
    with the real world is that it raises more safety questionsã€‚rightï¼ŸIf you let your
    language model make arbitrary API callsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: then you have to be a lot more careful with which calls thisilla to make and
    which is it notã€‚And if youã€‚As a posterï¼Œ if you just like you're reviewing everything
    the model saysã€‚then you can decide which ones you want to makeã€‚å—¯ã€‚ğŸ˜Šï¼ŒSo yeahï¼Œ it's
    an open problemã€‚Okayã€‚one more questionã€‚I think you didn'tã€‚è¿™ã€‚About the reasoning
    abilities of these asological language modelsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I've seen like different people talk about how is only like a fixed amount from
    two per token while like humansã€‚they have system one and system two where we could
    like just speak quickly versus actually do some reasoning and think through things
    that was more effortã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And then I've see in other words to try to like kind of use the force it to
    a chain of problems or the chain of like reasoning or like let' things step by
    step and stuff do think that stuff is sufficient to really get reasoning thats
    the level what we want or will require real big fine tuning or architectural changesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I don't knowã€‚I'm also the wrong person to askï¼Œ I'm mostly not trying toã€‚Get
    the models to have new capabilities and more like you knowï¼Œ getting them to play
    on team humanã€‚Ohï¼Œ do we want to do the online questionsï¼Ÿå‘ƒï¼Œæ˜¯å—¯ã€‚æœ‰ã€‚I also makes sense
    but my questionã€‚So what you think is the is it a role for often adults in from
    like especially if you have like human like you don't teach atsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: it's hard to more to humanã€‚So you think like this recipe be even more often
    hourã€‚Allã€‚Yeahã€‚quite possiblyã€‚å—¯ã€‚ğŸ˜Šï¼ŒI mean yeah as you point out right like there
    is a lot of conversational data and if you can use it that that would be it should
    be useful I think broadly can categorize this kind of thing as like let's make
    the R algorithm better and like ourL feedback and I think that's valuable and
    that should help us like make the same prechain models like more aligned according
    to the human preferences that we collectedã€‚
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚ğŸ˜Šï¼ŒBut also you would still run into the all the limitations that ourLF has
    right I think there's a fine API for G3 I don't think it offers our role right
    now but supervised fine training so you could do likeã€‚ğŸ˜Šï¼ŒYou can like this so best
    of them and do this kind of exp iteration or all questions so the first question
    is could you more clearly describe the full training process by child G example
    starting with the text Dancy 001 and X or programming data by steps of higher
    leftï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒSorry I didn't I didn't catch that start with texting 001 and then so how
    much images of data need you getting use and there are how many steps of part
    of that pitch kind of thingã€‚ğŸ˜Šã€‚![](img/b473ddacf8908cc92f68d174d1de8757_37.png)
  prefs: []
  type: TYPE_NORMAL
- en: Andã€‚I think the exact numbers are not publicï¼Œ it's basically similar to instruct
    GBT and for the instructGBT numbersã€‚![](img/b473ddacf8908cc92f68d174d1de8757_39.png)
  prefs: []
  type: TYPE_NORMAL
- en: And we hadã€‚I think around 50ï¼Œ000 comparisonsã€‚And probably like 10ã€‚000 demonstrations
    or like maybe tens of thousandsï¼Œ I don't remember the exact numberã€‚![](img/b473ddacf8908cc92f68d174d1de8757_41.png)
  prefs: []
  type: TYPE_NORMAL
- en: so I had like this other slide with yeahï¼Œ it was like about 20ã€‚000 hours of
    human feedback what I calculated rightã€‚I mean the big question is like how do
    you make how do you ensure qualityï¼ŸğŸ˜Šã€‚![](img/b473ddacf8908cc92f68d174d1de8757_43.png)
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚ğŸ˜Šï¼ŒBut any case need sometimes flip on let and click that I'll give anything
    somethingã€‚But it's the whole problemï¼Œ right like that assume you really have the
    one model that you trustã€‚Okayã€‚Sureï¼Œ so the next question I think council of before
    was you want to automate a land research search for f conceptual issuess which
    are difficult for experts to verify partã€‚I meanã€‚The kind of like ambition of that
    plan is to train a model that can do this kind of conceptual researchã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And you knowï¼Œ you knowï¼Œ you can picture is it like a language model that likeï¼Œ
    you knowã€‚writes an alignment research paper that we you knowï¼Œ like we read and
    then we're likeï¼Œ ohã€‚this is a really cool ideaï¼Œ we should try thisã€‚ğŸ˜Šï¼ŒAnd I thinkã€‚You
    knowã€‚going back to evaluation is's easier in generationã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I think it also applies to alignment research and like I think at the very least
    like I find it much easier to evaluate you knowã€‚alignment research than I find
    it to like produce itã€‚And soã€‚While there might be conceptual breakthroughs that
    we need that we couldn't even evaluate right now because theyre just likeã€‚you
    knowï¼Œ if we saw themï¼Œ we'd be likeï¼Œ what is thisï¼ŸAnd this is kind of likeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Like this is like the reason why we want to de scalpal oversightï¼Œ rightï¼Œ because
    you knowï¼Œ like ifã€‚you knowï¼Œ the language model produces this really brilliant
    insight and we can't even recognize it at the timeã€‚Umï¼Œ we should be able to have
    an easier time recognizing it if we use DI assistance and if we leverage like
    our best AI modelsã€‚ğŸ˜Šï¼ŒTo like figure out whether or not that was a good idea what
    is the weaknesses and what are the strengths and like you know what kind of experiments
    should we run to know whether this is a good idea and soã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Yeahï¼Œ I think basicallyã€‚You knowï¼Œ the story of just using ourHF to train a model
    to do good alignment researchã€‚You have the obvious pitfallsï¼Œ which isï¼Œ you knowï¼Œ
    the model might write like an alignment proposal that kind of looks good to usã€‚but
    is actuallyï¼Œ you knowï¼Œ not a good proposal and it creates AI that is misaligned
    with humansã€‚ğŸ˜Šã€‚And and so in order to distinguish the two which might be really
    hardï¼Œ maybe it's notï¼Œ but you knowã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I think we should expect it to be really hard and then leveraging EI assistance
    to evaluate that seems like a reallyã€‚Promising plan So about AI that that will
    be the skillsã€‚I meanï¼Œ that was my whole pointã€‚it's not sufficientã€‚ã‹ã¡ã€‚But do you
    think like do you need like something more or you want got open strategy that
    you think like if you can like get a lot of feedback data will that be sufficient
    to get very generations or do you see more in that I' mistake I mean the journalists
    or like I think theã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒBasically the vast majority of the model's capabilities and like all the cool
    things you see it do come from pretraining and now it's from the fine tu stage
    the reason why people sometimes attribute to the fine tu stage is that you didn't
    see it in the pretrainin model and the reason I think the reason that we didn't
    see it in the pretrainin model is because the pre model was so misalign it was
    not trying to help you and was not trying to show you all the things it can do
    and instead it just regurgent takes a bunch of random web textextã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒAnd that's not what you're looking forã€‚And soã€‚Yeahã€‚I think that what our basically
    has been doing is like unlocking capabilities that we're already a modelã€‚And making
    those available for human to useã€‚And some ways likeã€‚You knowã€‚alignment research
    is very dual use in the sense thatï¼Œ you knowï¼Œ aã€‚
  prefs: []
  type: TYPE_NORMAL
- en: if you have really good alignment techniquesï¼Œ you can use it to align with whatever
    values you wantã€‚including values thatï¼Œ you knowï¼Œ we wouldn't particularly endorseã€‚And
    Bã€‚It alsoã€‚Like if you're doing alignment rightï¼Œ it will always look a little bit
    like you made the AI system more capable because before it was wasn't really trying
    that hard to help you and now you've made it more aligned so you know you actually
    see these capabilities you already hadã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Sureã€‚Let's seeã€‚Yeah this follows a question before how do we deal with outlooklet
    that's written so this was like reference sort of but pro values but optimizing
    for them to size to incentivize deception so how do we contraryï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_45.png)'
  prefs: []
  type: TYPE_IMG
- en: Yeahï¼Œ so that was what I was talking about hereï¼Œ rightï¼ŸğŸ˜Šã€‚![](img/b473ddacf8908cc92f68d174d1de8757_47.png)
  prefs: []
  type: TYPE_NORMAL
- en: This is like the whole problem that we have whereã€‚What humans can evaluate is
    constant and so we won't be able to evaluate like sophisticated attempts at deceiving
    us and that's why we want to do scalable supervision so that we empower humans
    to spot these attempts at deceptionã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Yesã€‚åˆ°ã€‚How will your plan for contract in surveillance stand up to our and use
    distinction shifts and avoid really changing because of other care assistanceã€‚Yesï¼Œ
    soã€‚I think these are real worries andã€‚To some extent we kind of like have to test
    empirically like howã€‚
  prefs: []
  type: TYPE_NORMAL
- en: How difficult and how severe they actually are and I think so my personal stance
    right now is something like I thinkã€‚Trying to get the outer alignment signal really
    right is going to be like 90% of the effortã€‚And once we have thatï¼Œ then a lot
    of the other things might also fall into placeï¼Œ so for exampleã€‚I mean it kind
    of depends on which story of in misalignment you're worried aboutã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but you know one story is you kind of training your system and it learns how
    to doã€‚ItGs basically a bunch of inner optimizesï¼Œ kind of like meta reinforcement
    learningã€‚ğŸ˜Šï¼ŒSo for exampleã€‚like GPT3 can do like in context learning and that's
    like a kind of you knowï¼Œ learned optimizerã€‚And so now you're like doing our A
    Jeff training or whatever like alignment training you haveã€‚
  prefs: []
  type: TYPE_NORMAL
- en: and you're like the the then optimizers learn to do the thing that you want
    on the divisionã€‚But now if you have a distributional shift and this distributional
    shift could be autoedã€‚meaning like the model is causing it itselfã€‚ğŸ¼And now you're
    going out of the distributionã€‚all these inner optimizeizersï¼Œ like try to optimize
    for something elseã€‚Andã€‚One way you can likeã€‚Andã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you knowï¼Œ like how much that would actually happen in practice kind of actuallyã€‚but
    one kind of like more important question is likeã€‚If you have a really reliable
    outer alignment signal and you have this like general training signal that you
    trustã€‚you can also use thatï¼Œ you knowï¼Œ on the new distribution to train the system
    to be moreã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Or like to get optimizes in a rowï¼Œ basicallyã€‚And so then you've reduced like
    the inner alignment problems to like how do you deal with a distributional shift
    and how do you like construct an outer alignment signal that you trustï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: And those are problems that we have to deal with anywaysã€‚å—¯ã€‚But yeahã€‚I don't
    know how it's actually going to shake outï¼Œ butã€‚There's some important open questionsã€‚å—¯ï¼Œæ˜¯è¿™ä¸ªã€‚So
    regarding thoughtsã€‚A line thats one be kind ofã€‚Problems that accounting discussionã€‚SeeThere's
    not much interest it seems in like explaining how to come these other judgments
    or that's wondering there's been much interest in AI toward decomppoing these
    models is like explaining why this even second straight arbitarily and that's
    definitely a truth around it were shows but as to why it' making theseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: How lot of judgmentsï¼Œ have you all been able to interterrogate moreï¼ŸI meanï¼Œ
    I thinkã€‚Where we are right now is like pretty disï¼Œ satisfactoryã€‚I meanã€‚you can
    ask the model why I gave a certain responseã€‚but you don't know whether it's answereding
    truthfullyã€‚And you can alsoï¼Œ I meanã€‚
  prefs: []
  type: TYPE_NORMAL
- en: another thing you can do is yeah you can give the model of's response and ask
    it to find out flawsã€‚which is what we did in the C's paperã€‚å—¯ã€‚ğŸ˜Šï¼ŒButï¼Œ you knowï¼Œ
    I think that likeã€‚I meanã€‚there's one version where you try to make that betterã€‚but
    then the question is like what is your going to a signalï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: I think a like better angle of the attack is probably interpretabilityã€‚Whereï¼Œ
    you knowã€‚like you figure out how to look inside the model and thenã€‚ã¯ã§ã™ã€‚And that's
    what I was passionate about that the level of the research at interpret built
    yeahã€‚is like it seems that that's been really to do for you largering smell different
    and have such a high spaceã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Your current thinking is moving towards black that phrase like reducing the
    missionality of that representation and something that needsã€‚Yeahï¼Œ I meanï¼Œ we
    are working on that problemï¼Œ but I don't think we have anything that I'd like
    to show right now and soã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Uã€‚It seems generally not to be a very easy problemï¼Œ but you knowã€‚'m I'm hopeful
    that we can do some thingsï¼Œ I think in generalã€‚The problem of interpretability
    or like using interpretability for alignment is kind of tricky becauseã€‚I suspect
    it's going to be neither it's going to be not sufficient and it might not be necessaryã€‚
  prefs: []
  type: TYPE_NORMAL
- en: so any amount of interpretability you can leverage would be useful because it's
    another tool in your toolbox of like detecting deception or like knowing you know
    what you said like how why the model gave a certain answer or made a certain decisionã€‚Andã€‚ğŸ˜Šï¼ŒSã€‚You
    knowï¼Œ it isã€‚Kind of unclear if you really get really good at interpretabilityã€‚
  prefs: []
  type: TYPE_NORMAL
- en: how you then leverage thatã€‚For alignmentï¼Œ like presumably you could look in
    the model and just like throw all the models out that you can find in misalignment
    inã€‚but then aren't you just selecting for models that have misalignments that
    are really hard to find with your inability toolsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Trueï¼Œ just going all of touch because really my aspect about that as kind of
    stand practice in ethic in general is that you have to put an explanation on Yeah
    and I guess in my question likeã€‚why why would you take theã€‚ã¾ã›ã€‚Yesï¼Œ so why would
    it not been necessaryï¼ŸAgainã€‚
  prefs: []
  type: TYPE_NORMAL
- en: this is kind of like an open questionï¼Œ butã€‚Basicallyã€‚what stance you could take
    is that at the end of the dayã€‚what really is going to matter is the decisions
    that the model actually takes and not the reasons why it took themã€‚And so if you
    can get to the point where you're confident that all the things the model actually
    does are aligned with what you wantã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Then does it still matter what the model thinks internallyï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: I don't knowYeah that's that's what were trying to do right like we're trying
    to make like a really really good evaluation signal and then you can select forã€‚you
    knowï¼Œ you can train the model to do the things that you wanted to do because you
    can always evaluateã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Btter than the model can do stuffã€‚Yesã€‚Yeahï¼Œ I'll probably have much thats a
    questionã€‚å—¯ï¼Œä»–ä¸ªã€‚Much good but yeah thanks so much for the great lecture Very interesting
    I might actually not much I might just do like a live I want just like application
    thing Yeah sure the end of theã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒI lostï¼Œ sorryï¼Œ so how do think this thingã€‚ğŸ˜Šï¼ŒAll we a around of onceã€‚![](img/b473ddacf8908cc92f68d174d1de8757_49.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_50.png)'
  prefs: []
  type: TYPE_IMG
