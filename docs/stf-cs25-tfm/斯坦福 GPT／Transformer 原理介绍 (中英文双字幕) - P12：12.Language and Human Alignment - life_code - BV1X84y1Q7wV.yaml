- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P12：12.Language and Human Alignment - life_code
    - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福GPT/Transformer原理介绍 (中英文双字幕) - P12：12.Language and Human Alignment - life_code
    - BV1X84y1Q7wV
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_0.png)'
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_1.png)'
- en: It's my pleasure to welcome Jan from OpenEI he leads the alignment team there
    and who was previously a researcher at Deep Mind as well。holds a PhD in reinforcement
    learning theory， has been thinking about the alignment problem for over 10 years
    and today he'll be giving a very interesting talk so hope you guys enjoyed。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我很高兴欢迎来自OpenEI的Jan，他领导着那里的对齐团队，之前也曾是Deep Mind的研究员。拥有强化学习理论的博士学位，思考对齐问题超过10年，今天他将进行非常有趣的演讲，希望大家喜欢。
- en: Yeah， thanks a lot for the intro and thanks a lot for having me。😊。I'm very excited
    you talk about this stuff I'm also super happy to keep it interactive if you have
    questions at any point。please interrupt me。😊，啊。Yeah， I want to start out with。A
    few very basic observations on kind of what I think is going on。And。So the first
    one is。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，非常感谢你的介绍，也很高兴我能参加。😊。我很兴奋能够讨论这些内容，我也非常乐意保持互动，如果你在任何时候有问题，请随时打断我。😊，啊。是的，我想开始谈谈。我认为发生的事情的一些非常基本的观察。而且。所以第一个是。
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_3.png)'
- en: TmiI is joining the game。So hemiI has a lot of different factors。They aren't
    all joined at the same time， but rather they join one by one。And not all the their
    players are like vary a lot in how good they are and right now a lot of the players
    that have drawn so far。I'm really that smart and usually can do it only a very
    narrow set of tasks。诶。😊，But。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: TmiI正在加入游戏。所以hemiI有很多不同的因素。他们并不是同时加入，而是一个接一个地加入。而且他们的参与者在能力上差异很大，目前许多参与者到目前为止。我真的不那么聪明，通常只能完成非常狭窄的任务集合。诶。😊，但是。
- en: One thing that we've kind of observedd this that over time， you know。we're seeing
    strong and strong players joint。And this is kind of where we are now。And then
    in general。We expect that Himmiai has some incredibly strong playerss。So those
    will be playerss that are able to think so much better than humans。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到的一件事是，随着时间的推移，你知道。我们看到越来越强的参与者加入。这就是我们现在的状态。然后一般来说。我们预计Himmiai有一些极其强大的参与者。因此，这些参与者能够比人类思考得更好。
- en: so much faster and so much more cheaply。And。These haven't joined yet。And so。对。Have
    like anchor fund that we have， if you think， for example， about chattP。😊，ChatTBT
    can already be。Any human as like knowing more facts or speaking more languages。And
    it can write about 50 voice per second。And can do so about 100 times cheaper than
    humans could at the minimum wage。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 快得多，便宜得多。而且。这些还没有加入。因此。对。拥有像我们一样的锚基金，如果你考虑一下，比如说chattP。😊，ChatTBT已经可以做到。任何人都能了解更多事实或说更多语言。而且它可以以每秒大约50个声音的速度写作。并且成本是人类在最低工资下的约100倍。
- en: And so。You know， there's。The CheBT also has some really important limitations
    and there's a lot of things they can't do。But it is kind of。An indicator of， you
    know， some of the players that maybe we be joining in the future。And so it seemed
    like and in the long run。TmiI will have all the advantages over team humans。But。And
    there is one， there's an important caveat， which is。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所以。你知道，有。CheBT也有一些非常重要的局限性，还有很多事情他们无法做到。但是这算是一个。你知道，某些未来可能加入的参与者的指标。因此，似乎从长远来看。TmiI将拥有所有对团队人类的优势。但是。还有一个重要的警告，就是。
- en: There's one important advantageage that Kim Kim has。which is team Human gets
    to pick which players from teammi I join and win。And so this is kind of like an
    advantage that should we should really be leaning into when we're thinking about
    what to do and when we're thinking about。you know， this game that we're playing
    with CMI and that we'll be playing with TI。Oh。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kim Kim有一个重要的优势。团队人类可以选择哪些来自团队mi I的参与者加入并获胜。因此，这是一种在我们思考该做什么时，应该真正重视的优势，当我们思考。你知道，我们正在与CMI玩这个游戏，以及我们将与TI玩。哦。
- en: So I think two of the main objectives of what we as team humans should do is
    like first。We should try to recruit。Pes from地I to client进。And so this is kind
    of what I would broadly follow on。And this is kind of like the problem that I'm
    with。And then there is also other objectives。so another objective that I think
    is going to be really important is you want to write the rules of the game so
    that team human doesn't live。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为作为人类团队我们应该做的两个主要目标是，首先，我们应该尝试招募。来自地I到客户的Pes。因此，这大致上是我将要追踪的内容。这就是我面临的问题。还有其他目标。所以我认为另一个非常重要的目标是，你要制定游戏规则，以便人类团队不会失去。
- en: And right now， King human kind of has the ball and we get to write the rules
    so we should write the rules that you know。Makes sense and。那个。Still play this
    game in the future。And so in this talk。I won't really talk about the second point
    at all。and I will talk about the first point because that's why I know that's
    not where time。诶。😊。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，人类掌握了主动权，我们可以制定规则，所以我们应该制定合理的规则，以便于未来继续进行这场游戏。因此，在这次演讲中，我不会谈论第二点，而是将讨论第一点，因为这是我所了解的，并且那不是浪费时间。诶。😊。
- en: K to phrase it differently or to make it kind of like more。![](img/b473ddacf8908cc92f68d174d1de8757_5.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 用不同的方式表达，或者让它更像。![](img/b473ddacf8908cc92f68d174d1de8757_5.png)
- en: Practical， like one way I'm thinking about alignment is like you want to build
    AI systems that follow human intent。And， you know。Follow human preferences that
    do we want them to。And so a bunch of the things。basically I'll talk about two
    main things， the first part is going to be work that we've done in the past。😊，And
    have like， whichs roughly。Is in the bucket of like we are trying to figure out
    how we can make。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我所考虑的对齐的一个方式是，你想要构建遵循人类意图的 AI 系统。而且，你知道的，遵循我们想要的那种人类偏好。所以，有很多事情，基本上我将讨论两个主要方面，第一部分将是我们过去所做的工作。😊，大致上属于我们正在尝试弄明白如何实现的范围。
- en: The models that we have today as aligned as we can and we're just kind of trying
    we're going to try hard to do this and we'll see how far we can。And then the second
    bucket is。The things that we have to do next。The stuff that we haven't done yet
    that we think are going to be really important。And I want to kind of like lay
    out why I think they can be。对。Now， I said， you know。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们今天拥有的模型已经尽可能地对齐，我们正在努力尝试这个，看看我们能做到多远。第二个方面是我们接下来必须做的事情。我们尚未完成的事情，我们认为将非常重要。我想大致说明一下为什么我认为它们可能是。对。现在，我说，你知道。
- en: I I'm like trying to make this more clear or like more broken down what alignment
    means。so a not like here。Because now the big is like what does it mean to follow
    human attack？
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我正试图使这个更加清晰，或像分解一下对齐的含义。所以不在这里，因为现在的重点是：遵循人类攻击意味着什么？
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_7.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_7.png)'
- en: Theres like two main categories of intent that we care about is。I would say
    if it enhance to you。you know， like I give the assistant instruction or if you
    want to be my assistant。it should be my assistant， I should follow the instruction。But
    then there's also all these other intents that don't say when I'm usually， you
    know。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关心的意图大致分为两个主要类别。我会说，如果它对你有帮助，你知道，我给助手指令，或者如果你想做我的助手，它应该做我的助手，我应该遵循这些指令。但还有许多其他意图在我通常情况下没有说明，你知道的。
- en: talking to a system or a human， they ultimately really care about。like you know
    it shouldn't literally always do what I say but thing that I mean and it shouldn't
    make up stuff and it shouldn't you do harmful things and it should ask follow
    up questions when it's not sure what I mean and so on and so on。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与系统或人类交谈时，他们最终真的在乎的是。就像你知道的，它不应该字面上总是执行我说的每一句话，而是理解我的意思，它不应该编造东西，不应该做有害的事情，并且在不确定我意思时应该提出后续问题，等等。
- en: And so。These are all kind of like things they're often just likely difficult
    to like just not precisely specify or like。And。You know， what does I agree？But
    it is still things that we want to get they do and that we have to figure out
    how to。you know， get into it。And so kind of like the main technique that we're
    using today。😊，For this。is what we call important feedback， so that was used to
    train script GP and chat GBT。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以。这些都属于那些通常很难准确指定的事情，或者像……你知道，什么是我同意的？但这仍然是我们想要实现的事情，我们必须弄清楚如何去……你知道，进入这个领域。因此，今天我们使用的主要技术之一是我们所称的重要反馈，这曾用于训练脚本
    GP 和聊天 GPT。
- en: which has a tune like main in occupied this park。诶。The the basic system is very
    simple and it's also like a super general technique that appliess to most of this。AI
    models and modalities and settings。But in this case。we'll be using manage quality
    and so off two steps。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是占据这个公园的主要调音。诶。基本系统非常简单，而且也像是适用于大多数AI模型和模式的超级通用技术。但在这种情况下。我们将使用管理质量，因此分为两个步骤。
- en: There is actually not a step of like chain limitations so I'm going to start
    with the things。First that is you want to train one model from Comp。So you have
    on passed in this case。now explain when not。Or you know， help me with my trans。Whatever
    it is。and then the model does a bunch of things and then you made which one is
    like close to the thing that you intended the model。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上没有链限制的步骤，所以我会从这些事情开始。首先，你想要从Comp训练一个模型。在这种情况下你已经通过了。现在解释何时不。或者你知道，帮我进行翻译。不管是什么，然后模型会做一堆事情，然后你会选择哪个更接近你想要的模型。
- en: And so you have a bigger set of preferences than you train your reward model
    and the reward model basically just learn to predict which one would you prefer。Book。Everything
    okay I would say like just spend more in front of the camera as soon possible。but
    I think that looks good。😊，Sorry about that。Maybe let's turn it a little bit。O对。没有。Okay。so
    now we have this through our model that captures kind our preferences and what
    we care about and what we intend for the model to do。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你有一个更大的偏好集，你训练你的奖励模型，而奖励模型基本上学习预测你更喜欢哪个。书。所有一切都可以说，就尽快多花时间在镜头前。我觉得这看起来不错。😊，抱歉。也许我们把它转动一点。哦，对。没有。好吧。现在我们通过模型捕捉到我们关心的偏好和我们希望模型做的事情。
- en: And then the second step is now you optimize your reward model with the input。😊，And
    so not setting。you know， like model tries a whole bunch of different things and
    then what model have tells it which one would the assisted things is probably
    more like the thing that。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后第二步是现在你用输入优化你的奖励模型。😊，因此不是设置。你知道，模型尝试一堆不同的事情，然后模型告诉它哪个可能更接近你想要的事情。
- en: 没有。They will depend on the letter， different label of。They also might be inconsistencies
    and you can give these examples of like in preferences。😊。But those haven't really
    been a problem in。And so far， you know， like our flights often don't agree。but
    the model average overall thes have。😊，下。不 yeah， so。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不。他们将依赖于字母，不同的标签。他们可能还会存在不一致性，你可以给出这些关于偏好的示例。😊。但这些在目前并没有真正成为问题。到目前为止，你知道，我们的飞行往往并不一致，但模型整体平均起来是这样。😊，下。不，是的，所以。
- en: This is like the basic technique is conceptual like quite simple。you can make
    it even simpler if you had， you know。if you didn't translate into one model and
    you label instead like everyR that。😊。But it would be a lot less data efficient
    and so you can turn your one on to think up。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是基本技术，概念上非常简单。如果你没有翻译成一个模型，而是像每个R那样标记，你可以使其更简单。😊。但这将大大降低数据效率，因此你可以将其转变为思考。
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_9.png)'
- en: 对的。对。So how will it work so this is kind of like one of the main thoughts from
    the in sheet paper and this is the one I like showing because。It really blew my
    mind and it still does。What do we see here？On the X axis。you see this is from
    the GP3 model series and you see this is like different three different size of
    models over two orders of magnitude。😊，And then the y axis is how well does the
    level score on human preference？😊。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对的。对。那么它将如何运作，这实际上是论文中的一个主要思想，而这是我喜欢展示的，因为这真的让我大开眼界，至今仍然如此。我们在这里看到了什么？在X轴上，你会看到这是来自GP3模型系列的，并且这是不同的三种模型，跨越两个数量级。😊，然后Y轴是人类偏好的评分水平如何？😊。
- en: So if you show a bunch of samples to humans， how likely are they to prefer one
    over the other？
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你给一群人展示一些样本，他们更倾向于选择其中的一个吗？
- en: And then what we see is that even like the largest G3 model。😊。Is this preferred
    to the smallest instruct G variant and so the 100x smaller instruct model？😊，And
    if。Actually preferred。Over the much larger。接屏我晒接屏电话。And that's kind of wild。Sorry。let
    me just finish my talk。 So why。我也这是没写就没退过去。It basically shows that it would like
    to be on the line of COD that making them whole more made them so much more useful
    than was getting out。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到即使是最大的G3模型。😊。也比最小的指令G变体更受欢迎，因此100倍小的指令模型？😊，如果。实际上更受欢迎。超过更大的模型。接屏我晒接屏电话。这有点疯狂。抱歉。让我完成我的发言。那么为什么。我也这是没写就没退过去。这基本上表明，它想在COD的线上，整个模型让它们变得更有用。
- en: 😊，Or fine nice worse nobody want to use it。And can you guys hear me we make
    all these fancy alignment techniques that don't get adopted。And so what we were。Like
    originally in like the first version we that saw these regressions and then what
    here is labeled PPO PX is kind of like a very end where we mix in free training
    data into the fine tuning and that mitigated a bunch of the regressions that we
    saw。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，或者说更糟，没有人想用它。你们能听到我吗？我们做了很多花哨的对齐技术，但并没有被采用。所以我们原本是在第一个版本中看到了这些回归，然后这里标记为PPO
    PX的部分，实际上是在最后阶段，我们将自由训练数据混合到微调中，这减轻了我们看到的一些回归。
- en: 嗯。😊，Yeah， and I just had to put a follow up to that。How would important。It's
    like fidelity of fine team that you。You guys you collect data from humans right
    what if you were to use some pretrainmigrant model to score you know or something
    like that yeah and you could do that and there have been like other papers on
    that like I thought they wrote this paper on constitutional AI that was trying
    to like exactly do that。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。😊，是的，我刚好想跟进一下。那微调的重要性如何？你们是从人类那里收集数据的，对吧？如果你们使用一些预训练的迁移模型来评分，或者类似的东西，那样可以做到吗？确实有其他论文讨论过这个问题，我认为他们写过一篇关于**宪法人工智能**的论文，正是试图做到这一点。
- en: in terms of well there's certain things that the language model will be able
    to automatically rank and some things it won because you know。it won't know your
    exact preferences or it won't know exactly what like what we wanted to do and
    so you know whenever the language model does something。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 就某些方面而言，语言模型将能够自动排序某些事物，而有些事情则不能，因为你知道，它不会知道你的确切偏好，也不会知道我们想要做的是什么。因此，每当语言模型执行某个操作时。
- en: That we just prefer we actually we have to give it another data point right
    or in other words。you know， if you're aligning with humans， you somehow have to
    put humans into the loop so that you know otherwise。How does the model know what
    it's supposed to do？Lots of more questions， I don't know who was Chris。😊，Yeah，
    how many you mean approximately like what's how many orders of magnitude of like
    submitted preferences do you need to achieve these yes。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更喜欢这样，实际上我们必须提供另一个数据点，换句话说，如果你要与人类对齐，某种程度上你必须把人类纳入环路，否则，模型怎么知道它应该做什么呢？还有很多问题，我不知道克里斯是谁。😊，是的，你的意思是大约需要多少个提交的偏好来实现这些？
- en: I'm going to get to that in a second sure。question， I think some。Yeah。😊，So why
    you an experiment？
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我马上就会提到这个问题。是的，有些问题。😊，那么你进行实验的原因是什么？
- en: We haven。We haven't actually carefully compared across our algorithms。and it
    could very well be that a different neural algorithm would be better。😊，That was
    connect life。I know PVO was invented open the eye， so that's why we used it。It's
    not not really a good reason other than that， it works also pretty well。Yes。eCompisons。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有。我们实际上没有仔细比较我们的算法，可能不同的神经算法会更好。😊，那是连接生活。我知道PVO是在OpenAI发明的，所以我们才使用它。这不是个好理由，除了它运作得相当不错。是的，eCompisons。
- en: this is better than this other thing。So usually we have people compare between
    like three to six different responses from usually different models。Okay。Yeah。😊，So
    is PPO and the reward mode currently used in chat GT in production and it's so
    like do you use any of the human feedback like you know you can regenerate responses
    and stuff like that to help。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个比其他的东西要好。所以通常我们让人们比较大约三到六个来自不同模型的不同响应。好的，是的。😊，那么PPO和奖励模式目前在聊天GT中投入生产吗？你们是否使用任何人类反馈，比如你知道的，你可以重新生成响应之类的东西来帮助？
- en: As a reward function as well， how do you mean to regenerate like there's a button
    onT where you can say about regenerate responses or do you use any implicit feedback
    basically in human use I don't know what the current state is for that I expect
    people will try to use it。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 作为奖励函数，你是指重新生成的方式吗？就像在T上有一个按钮，你可以选择重新生成响应，或者你使用任何隐含反馈吗？基本上在人类使用中，我不知道目前的情况如何，我预计人们会尝试使用它。
- en: but you knowGBT hasn't been out that long。Yeah， so I'm curious about this graphic
    like it seems like 100 x as you mentioned increasing parameters doesn't give you
    that much more like fidelity there qualitatively you have been tracked this for
    a while。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你知道GBT推出的时间不久。是的，我对这个图感到好奇，似乎像你提到的，增加参数并没有给你带来太多的**保真度**，在质的方面你已经追踪这个一段时间了。
- en: can you tell right off the bat if you're like interacting with the 1 billion
    like a model or the like 100 billion model like auring pseudoturing test but parameter
    size like I give you a black box can you tell me how many parameters it has？
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你能否一开始就判断你是在与10亿模型还是100亿模型互动，就像伪图灵测试一样，但参数规模如何？我给你一个黑箱，你能告诉我它有多少个参数吗？
- en: 😊，Probably not very precisely。But I think the big counter question is like do
    I get to write the prompt so if you just draw random prompts from whatever people
    put in the opening at playground。which is what we use points for our TPT， then
    I probably need quite a few to tell the difference but if I get to write the prompt
    I can probably do it in one or to at least like if the task is like tell the difference
    between this and this。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，可能不是非常准确。但我认为大反问是，我能否写提示。如果你只是随机从人们在*游乐场*上输入的内容中提取提示，而这正是我们用于TPT的点，那么我可能需要相当多的提示才能分辨出差异，但如果我可以写提示，我大概能在一两次内做到，尤其是如果任务是区分这个和那个。
- en: Yeah。I want to can I just do two more slides and maybe your questions get answered
    and then。![](img/b473ddacf8908cc92f68d174d1de8757_11.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。我想我可以再做两张幻灯片，或许你的问题会得到解答，然后。![](img/b473ddacf8908cc92f68d174d1de8757_11.png)
- en: so this was the question about training costs so this is another thing that
    kind of really blew my mind is like compared to pre training it is incredibly
    cheap so if you look at like the amount of labs that it takes to train GB to eat
    and then you compare it with like how much does fine tuning and the R withs pre
    training makes and everything。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于训练成本的问题，所以这一点真的让我大吃一惊，与预训练相比，它是极其便宜的。如果你看看训练GB所需的实验室数量，然后与微调的成本进行比较，以及R与预训练的关系。
- en: 😊，like the most expensive instructorB version is like less than 2% of the pre
    training computers。😊。And if you want to train an even bigger model， it's going
    to be more expensive and you could still use the same like fine unique step to
    make it more aligned。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，像是最贵的讲师B版本价格不到预训练计算机的2%。😊。如果你想训练一个更大的模型，那成本会更高，但你仍然可以使用相同的*微调*步骤来使其更一致。
- en: And of course， I think the important thing to note also here is like we haven't
    fixed all the problems。there's like important limitations and so I wouldn't say
    that this is like you know the last version and we wouldn't try to figure out
    how to spend more compute and more human data in the future。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我认为这里需要注意的重要一点是，我们还没有解决所有问题。还有一些重要的限制，因此我不会说这是你知道的最后一个版本，我们将努力探索未来如何利用更多的计算资源和更多的人类数据。
- en: 😊。![](img/b473ddacf8908cc92f68d174d1de8757_13.png)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 😊。![](img/b473ddacf8908cc92f68d174d1de8757_13.png)
- en: But all in all， it was surprisingly effective。![](img/b473ddacf8908cc92f68d174d1de8757_15.png)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 但总的来说，这个效果出乎意料地好。![](img/b473ddacf8908cc92f68d174d1de8757_15.png)
- en: Okay， there were more questions。😊，My questions Yeah， I just wanted to ask what
    the PTx。Mixing pre training data into the IO F tune， just like mixing gradients。Y
    quick one。what's the number of parameters for this graph so you fixed a number
    of branches for this so this is the full size GP3 version。so this is the 175 billion
    model。So。O questionss， no， okay。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，还有更多问题。😊，我的问题是，我只是想问PTx是什么。将预训练数据混入IO F微调，就像混合梯度一样。快速问一下，这个图的参数数量是多少？所以你固定了这个分支的数量，这是完整的GP3版本，也就是1750亿模型。所以。没有其他问题吗，好的。
- en: there's also some questions on Zoom great Okay sure， so the busman is。Okay，
    sure。so the first question is how would you deal with all edge breaking in the
    limit。example preferences are a good proxy for values。But optimizing for them
    is terized to incentiv device perception。Yes， I'll get to that。😊，Sure。Sure that
    the next question so that is like you want to automatic alignment research what
    happens if you need conceptual vi which other people are experts to5 Okay。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Zoom上也有一些问题，太好了，好的，当然。所以第一个问题是，你会如何处理所有边缘情况？例如，偏好是价值的一个良好代理。但优化它们是会激励设备感知的。是的，我会提到这个。😊，当然。下一个问题是，如果你想进行自动对齐研究，发生了什么，如果你需要概念视觉，其他人是专家，怎么办？好的。
- en: that would be I get a take at the end as well sure let's see sorry。Yeah。I guess
    like one question is like， how would fun feeling directly on human feedback compared
    to fan power？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得我也会在最后得到一个机会，好的，我们来看看，抱歉。是的。我想有一个问题就是，*人类反馈*与*风扇功率*相比，乐趣感觉会如何呢？
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_17.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_17.png)'
- en: m fine tuning like supervised fan tuning。But it's more like if you directly
    use the human picker data。Yeah， I'm also not sure whether it be。U so I mean so
    one baseline I'm showing here is like what if you just take human demonstrations
    in the sense that。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微调像监督性微调一样。但更像是，如果你直接使用人类选择的数据。是的，我也不确定是否可以。因此，我展示的一个基线是，如果你只是采取人类示范的方式。
- en: you know， we have a bunch of tasks， we just ask humans to do them。record what
    they did and then train the model to imitate that and here it's like just very
    basic behavioral cloning just using the same loss to use and retraining。And then，
    you know， it is noticeably better than the H shot hunted version。but it's still
    not as good as at all。And so that's why we like using our and basically。Conceptually。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，我们有一堆任务，我们只让人类去做它们，记录他们的操作，然后训练模型模仿。这实际上是非常基础的行为克隆，只是使用相同的损失进行重新训练。然后，你知道，它比H
    shot hunted版本明显更好，但仍然远不如。因此，这就是我们喜欢使用我们的基本概念。
- en: there's two problems with the imitating humans approach one is。Humans are better
    at some things than the model is and they're worse at other things and so at the
    things that the model is worth。you try to imitate something that you can't do。And
    on the things where the model is better。you're making the model worse because
    you're forcing it to do the thing in the way that the human would。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿人类的方法有两个问题，一个是。人类在某些方面比模型更好，而在其他方面则更差，所以在模型擅长的事情上，你试图模仿你无法做到的东西。而在模型更擅长的事情上，你让模型变得更糟，因为你强迫它以人类的方式完成任务。
- en: What。And so。With our all you with our other Jeff， you're kind of letting the
    model do whatever it wants to。and it can just figure out like the best way for
    it to do things。![](img/b473ddacf8908cc92f68d174d1de8757_19.png)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 什么。然后。所以。通过我们的所有你与其他的杰夫，你有点让模型随心所欲。它可以找到做事情的最佳方式。![](img/b473ddacf8908cc92f68d174d1de8757_19.png)
- en: It's also another important advantage， and I'm going to get to that but briefly
    want to talk about Cha GBT。😊，So one thing I kind of think of CaGBT is like the
    upgrade to instructGBT it's kind of like the next step at making the models more
    aligned and more useful to humans。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的优势，我会提到这一点，但简要谈谈Cha GBT。😊，所以我认为CaGBT就像是对instructGBT的升级，算是让模型更对齐并对人类更有用的下一步。
- en: And some things that's like， you know， I think chapter does better is kind of
    like using dialogue as the universal interface right you can talk to it directly。you
    can ask follow up questions， you can ask it to you know refine the answer and
    all of these things that makes it a lot easier to deal with。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有些事情，比如，我认为章节做得更好的，就是把对话作为通用接口，你可以直接与它对话。你可以问后续问题，可以要求它完善答案，这些都使得处理变得更容易。
- en: It's better at refusing harmful tasks， but it's also there's still important
    limitations right like the biggest one is like the model of hallucates a lot。it
    makes up facts when you know， for whatever task you give it and that you know
    is makes it quite unreliable it's also still sensitive to prompting which kind
    of shows that。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 它在拒绝有害任务方面更擅长，但仍然有重要的局限性，最大的问题是模型会经常产生幻觉。当你给它任何任务时，它会编造事实，这使得它非常不可靠，同时也对提示非常敏感，这显示了。
- en: 😊，You know， it still has important misalignment。That we need to fix。Like really。if
    a model was like model should really like。Do the task to best of its ability no
    matter how you're prompted to do that。嗯。😊。![](img/b473ddacf8908cc92f68d174d1de8757_21.png)
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，你知道，它仍然存在重要的不对齐问题。我们需要解决。真的。如果一个模型应该尽其所能地完成任务，无论你如何提示。嗯。😊。![](img/b473ddacf8908cc92f68d174d1de8757_21.png)
- en: But yeah， one important principle that I think。Is really useful for or that
    like our that Jeff leans on a lot is that evaluation is easier than generation。😊，So
    if we ask humans to compare and rank different responses， the model gave。嗯。😊，It
    is。Easier to tell the difference between different variants of what the model
    did than is to do the task itself。Or in other words， you know， you can do the
    comparisons on tasks。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 但确实，我认为有一个重要原则，对于杰夫来说非常有用，就是评估比生成更容易。😊，所以如果我们让人类比较和排名模型给出的不同响应。嗯。😊，分辨模型生成的不同变体比直接完成任务更简单。换句话说，你可以对任务进行比较。
- en: you could still like spot good behavior on tasks that you might not be able
    to do well yourself。And so if you're giving this kind of like feedback。That lets，
    you know。the system should do better than you actually could。And I think that's
    a very general principle that holds in lots of domains。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 你仍然可以在你自己可能做得不好的任务上发现良好的行为。所以如果你给出这样的反馈。那就意味着，系统应该做得比你实际能够做得更好。我认为这是一个在很多领域都适用的非常普遍的原则。
- en: so kind of like you're probably most familiar yes， you know the P versus NP
    and everyone。you know we don't actually know whether they're different but in
    practice it seems like NP tasks they're just much harder。![](img/b473ddacf8908cc92f68d174d1de8757_23.png)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可能最熟悉的是，是的，你知道P与NP，每个人。你知道我们实际上并不知道它们是否不同，但在实践中，似乎NP任务要难得多。![](img/b473ddacf8908cc92f68d174d1de8757_23.png)
- en: Um， it also applies to lots of other settings， like a lot of professional sports
    or esports just wouldn't be fun to watch if he couldn't tell he was winning。More
    easily than you could actually compete on a professional level。it applies to a
    lot of consumer products， you can like look at two smartphones and tell which
    one you like more without that is like also deeper than just looking at like the
    specs。That it is actually very high to pull this to my phone。It also applies to
    academic research。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，它还适用于许多其他环境，例如很多职业体育或电子竞技，如果你不能轻易判断自己是否获胜，那观看的乐趣就会大打折扣。比起实际在专业水平上竞争更容易。它适用于许多消费产品，你可以看两个智能手机，告诉你更喜欢哪个，这不仅仅是看规格那么简单。实际上，这很难把它和我的手机相提并论。它还适用于学术研究。
- en: you know， it's much easier to view a paper and say what all the things that
    are bad about it。Thenhan it is to write a good paper yourself。It applies to， I
    don't know when you。嗯。Yeah。basically there's lots of domains where the applies。and
    so I think this is like a very。😊。This principle is like very useful when we want
    to like align AI systems on tasks that we might not be able to do ourselves well。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，查看一篇论文并指出其所有缺陷要简单得多，而写好一篇论文则要困难得多。它适用于，我不知道何时。嗯。是的。基本上，这在许多领域都适用。所以我认为这就像是一个非常。😊。这个原则在我们希望对AI系统的任务进行对齐时非常有用，而这些任务可能是我们自己无法做好。
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_25.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_25.png)'
- en: Okay。Okay， so having said that。I like Jeff has some really important limitation
    and I think that's going to make it。Really difficult to use ourHf to scale alignment。![](img/b473ddacf8908cc92f68d174d1de8757_27.png)
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。好的，既然这么说。我觉得杰夫有一些非常重要的局限性，我认为这会使得我们的Hf在对齐时变得非常困难。![](img/b473ddacf8908cc92f68d174d1de8757_27.png)
- en: 嗯。😊，Let me explain this with a diagram， so basically on the X axis。That's plot
    like the AI progress。And on the Y axis， how difficult different tasks are。And
    then as we have more AI progress。kind of like the tasks that AI， the difficulty
    of tasks that AI can do goes up。And like。One of the fundamental problems is that
    the level of tasks that humans can reliably evaluate doesn't go up because humans
    don't get better with AI progress。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。😊，让我用图表解释一下，基本上在X轴上。绘制AI进展。而在Y轴上，绘制不同任务的难度。随着我们的AI进展，AI能够完成的任务难度也在上升。而像。一个根本性的问题是，人类能够可靠评估的任务水平并没有提高，因为人类不会随着AI进展而变得更好。
- en: And so。I think we're like somewhere here。But the problem is once you cross this
    line。you don't really know。😡，What like whether your model is actually doing the
    right thing because you can't reliably evaluate anymore and so that's kind of
    like the point where RLHF training will start to break down and what we probably
    see is kind of what the question before I allud it to is like well now the systems
    are optimized for whatever feedback we give them and so they will try to tell
    us what we want to hear rather all the things that they know to be true and you
    know they might learn how to deceive us because you know that makes it easier。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以。我觉得我们大概在这里。但是问题是，一旦你跨过这条线。你就真的不知道。😡，你的模型是否真的在做正确的事情，因为你无法再可靠地评估，因此这就像是RLHF训练开始崩溃的点，我们可能看到的就是之前我提到的问题，现在系统被优化为响应我们给出的任何反馈，因此它们会试图告诉我们我们想听的东西，而不是它们知道的真相，你知道，它们可能会学会如何欺骗我们，因为这让事情变得更简单。
- en: Justis score high on preference。And so kind of like。The basic idea that we want
    to leverage is related to the principle I just mentioned。which is evaluation student
    easier and generation， so for example。if you have a large language model writing
    a codeb like an entire codeb there's just no way humans would be able to find
    all the bugs and all the flaws in the codeb or you know the codebase could have
    like a Trojan in there and you might not be able to tell。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Justis 在偏好评分上很高。因此，基本的想法是我们想要利用的原则与我刚才提到的有关。这就是评估学生的便利性和生成能力，比如说，如果你有一个大型语言模型来写代码，像整个代码库那样，人类根本无法找到所有的错误和缺陷，或者代码库里可能有一个木马，你可能也无法察觉。
- en: because it is so hard and that's why we see so much buggy code out there。But
    if you're asked your language model to find bugs and point them out to you。Once
    you've， you know。once you've seen the bug， it's so much easier for you to say，
    oh yeah， this was a bug， you know。please fix it。And so now you've taken the task
    of footbase down to。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这实在太难了，这就是为什么我们会看到那么多有缺陷的代码。但是如果你让语言模型来找出错误并指出来，一旦你知道了这个错误，哦，是的，这确实是一个错误，你知道的，请修复它。因此，现在你只需将任务缩小到。
- en: I just have to evaluate whether that was the bug according to the spec I had
    in mind。And so the general principle that we are excited about here is like we
    want to。😊。He leverage AI assistance for human evaluation。And so the hope is that
    we together。if you pair up humans with AI， you actually get a line that looks
    more like this where。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我只需要评估这是否是我心中规范所设想的错误。因此，我们感到兴奋的一般原则是，我们想要。😊。我们利用 AI 的帮助来进行人类评估。所以希望是，如果你将人类与
    AI 配对，实际上会得到一个更像这样的结果。
- en: you know like humans together with the AI can evaluate much more than they could
    by on their own。😊。![](img/b473ddacf8908cc92f68d174d1de8757_29.png)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，人与 AI 一起评估的能力远超过单独评估的能力。😊。![](img/b473ddacf8908cc92f68d174d1de8757_29.png)
- en: 嗯。😊，And so。To make this concrete there's like two different ways you could do
    that or there's many different ways you could do that too i'm going to highlight
    it' like first you can write ask AI to write a critique this is a project we did
    last year and in this case there was a simple summarization task and we train
    a language model to kind of like just say。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。😊，所以。为了使这更具体，有两种不同的方式你可以做到这一点，或者有许多不同的方式你可以做到。我将强调一下，首先，你可以让 AI 来写一个批评，这是我们去年做的一个项目，在这个案例中有一个简单的总结任务，我们训练了一个语言模型来简单地说。
- en: 😊，Things that are wrong with the summary。Um and。![](img/b473ddacf8908cc92f68d174d1de8757_31.png)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，总结中存在的问题。嗯，以及。![](img/b473ddacf8908cc92f68d174d1de8757_31.png)
- en: There's other things you could do， for example， you could give people chat GPT
    and ask them okay。use chatGT to help you evaluate and then you could ask for a
    critique or you could ask for a lot of other things you could ask for an explanation。you
    can ask for fact checking or quote or you know whatever the model like chatGBT
    can actually reliably help you with。And so。The idea would be that， you know， like
    using YaI assistance， you can kind of get。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做其他事情，例如，你可以让人们使用 ChatGPT，并问他们，好吧。使用 ChatGPT 来帮助你评估，然后你可以请求一个批评，或者你可以请求很多其他的东西，你可以请求解释，进行事实核查，或者你知道的，任何模型像
    ChatGPT 实际上可以可靠地帮助你的事情。因此，想法是，使用 AI 的帮助，你可以获得。
- en: All the smart city I have。And leverage that in order to figure out how you should
    evaluate what this system is doing and like whether it's aligned with your preferences
    or whether it's trying to receive you。And。The big problem with this is how do
    we know whether it's working？
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的智能城市我拥有。并利用这一点来确定你应该如何评估这个系统的运行情况，以及它是否符合你的偏好，或者它是否在试图欺骗你。而且，这个问题的关键在于，我们如何知道它是否有效？
- en: One of the kind of like difficulties is that。By assumption we're kind of dealing
    with a hard task where it's difficult to evaluate and we also want the task to
    be real because we don to。you know we don' to want to solve some high tasks that
    doesn't matter。And so。It becomes different。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个困难在于，根据假设我们正在处理一个难度较大的任务，在评估时困难重重，并且我们希望任务是真实的，因为我们不想解决一些无关紧要的高难度任务。因此，这就变得不一样了。
- en: so you need like a hard task that is real， but also if you don't if you have
    those you usually don't have ground too so you don't know which was the right
    answer and how do you know whether assistance is working or it's biasing everyone
    to say the same thing。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你需要一个真实的困难任务，但如果你没有那些，通常也没有基准，所以你不知道哪个是正确答案，你怎么知道辅助是否有效，或者它是否使每个人都偏向于说同样的事情。
- en: And so。There's a simple technique that we use in the critique。To do this。Where
    we're like that we call targeted perturbations and so what you do is you have
    a bunch of prompts。so this could be like whatever people type into chat UT。And
    then you。Kind of like take the response that you have and you say like this is
    their correct response it doesn't actually have to be correct but let's just assume
    it is and now you're asking a human to introduce some kind of subtle flaw that
    is hurt like easy to miss but is an important flaw and know what you have is you
    have this peer data set of like a good response and a bad response and you know
    which one is good and bad because you made it worse。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，有一个简单的技术，我们在评估中使用。我们称之为目标扰动，你所做的是有一堆提示。这可能是人们在聊天中输入的内容。然后你稍微调整一下你的回答，假设这是他们的正确回答，它不一定要正确，但我们假设它是，现在你在要求一个人引入某种微妙的缺陷，这种缺陷很容易被忽视，但却是重要的缺陷。你得到的是一个良好回答和一个糟糕回答的同伴数据集，你知道哪个是好哪个是坏，因为你让它变得更糟。
- en: And so， you know。in a way， that gives you grant。And so what you now can do is
    you can。😊。Take like randomly select one either the correct or the flawed response
    and then show it to either a human or a human with assistance。and then they have
    to say whether it was the corrector the flawed one or like how good the response
    is。😊，嗯。And。😊，In this setting， we can kind of like。Figure like just try a lot of
    different AI assistance or your scalable oversight techniques and then say。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你知道，从某种意义上说，这给了你机会。现在你可以做的是，随机选择一个正确或有缺陷的回答，然后把它展示给一个人或一个有帮助的人。他们必须说出这是正确的还是有缺陷的，或者这个回答有多好。😊，嗯。在这种情况下，我们可以尝试许多不同的人工智能辅助或可扩展的监督技术，然后说。
- en: You know， like is this？Actually helping humans find the flaws that we planted
    more lively than not。And if it is， then you know， like you're actually really
    helping。![](img/b473ddacf8908cc92f68d174d1de8757_33.png)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，这是否？实际上帮助人类发现我们植入的缺陷，比不帮助时更生动。如果是这样，那么你实际上是在真正帮助。![](img/b473ddacf8908cc92f68d174d1de8757_33.png)
- en: And so we did this in the critiques paper， so this is training the language
    models to write critiques and for summaries and what we can show is that when
    were assisting humans with critiques at the evaluation。they actually find 50%
    more flaws than it did without and so this is kind of like real signs of life
    that you can already use the models that we can have today。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在评估论文中做了这个，训练语言模型写评估和总结。我们可以展示，当我们在评估中帮助人类进行评估时，他们实际上发现的缺陷比没有帮助时多50%。这就像是现实生活中的真实迹象，表明你可以使用今天我们已有的模型。
- en: To help humans evaluate and like find problems they would have missed otherwise。And
    of course。we still have to do this。Like on a much harder task and like with。Or
    like a real task in a sense and we also want to have like。Bigger effect size。but
    I think it's just like。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助人类评估和发现他们可能错过的问题。当然，我们仍然需要在更困难的任务上进行这种评估，或者在某种意义上的实际任务中，我们还希望有更大的效应量。但我认为，这只是像。
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_35.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_35.png)'
- en: It shows that there's promise of these techniques already working。And so in
    the long run。😊。Um what I think we want to get to is we kind of want to leverage
    EI for all the cognitive labor that goes into evaluating whatever our E systems
    are doing。And this could be， you know， like readingating everything that's relevant
    or fact checking or doing calculations or like writing code or any of these things。and
    then humans should focus on like their preference input like the things figuring
    out what they actually care about and what they want。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明这些技术已经显示出一定的前景。从长远来看。😊。嗯，我认为我们想要实现的是，利用人工智能来处理所有评估我们系统所做的认知劳动。这可能包括阅读相关内容、事实核查、做计算、编写代码或任何这些事情。然后人类应该专注于他们的偏好输入，了解他们真正关心的事情和他们想要的内容。
- en: 😊，The model to do。And。嗯。😊，And this way， you can kind of like leverage。You know。like
    the abilities that， you know， the AI players will bring to the table and the things
    that they will be better at than us eventually。And then。Kind of like use them
    to help communicate the thing that we actually care about and you know。the things
    that we actually want them to do。And yeah， that's it。😊，But yeah。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 😊 模型来做。嗯。😊 以这种方式，你可以利用。你知道。AI参与者将带来的能力以及他们最终会比我们更擅长的事情。然后。就像是利用它们来帮助传达我们实际关心的事情，以及我们真正希望它们做的事情。是的，仅此而已。😊
    但是是的。
- en: those are like the main slides， I'm happy to take more questions。Yes。I was wondering
    about this hallucination of responses Have you ever tried to consider some notion
    of uncertainty in the access but' sampling or something like this have value variance
    between the responses and somehow value。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是主要的幻灯片，我很乐意回答更多问题。是的。我在想这种响应的幻觉。你有没有尝试考虑一些关于访问的不确定性的概念，比如采样或类似的东西，响应之间的价值方差，以及某种价值。
- en: Yeah这。So ensembling is difficult because either you're like training in a fine
    training ensemble from the same pretrain model and so you don't get that much
    barrier in you ensemble or you have pretraining a bunch of different models and
    now you're spending a lot of money on pretraining。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。这。集成是困难的，因为要么你在同一个预训练模型上训练一个精细的集成，因此在集成中没有太多的障碍，要么你预训练了一堆不同的模型，现在你在预训练上花费了很多钱。
- en: One thing， I mean。It seems like it should be a solvable problem to just。C the
    model to say it's uncertain when it's actually uncertain。And there's been a bunch
    of research in that direction but。I think right now it's still like we're not
    really in a good shape。Smart stuff to do。She。Yeah。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一件事，我是说。看起来这应该是一个可解决的问题，就是让模型在不确定时说它不确定。朝这个方向已经有很多研究，但我认为现在我们仍然不在一个好的状态。做一些聪明的事情。她。是的。
- en: do you think we may run into a kind of signals and noise ratio problem when
    it comes to AI suggested critiques to AI answers because Im sure like when AI
    is trying to point out potential problems with text。humans are more likely to
    support more problems， but what if it's noing problems that humans wouldn't have
    necessarily had a problem to begin with Yeah。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为在AI建议批评AI答案时，我们可能会遇到信号与噪声比的问题吗？因为我肯定，当AI试图指出文本中的潜在问题时，人类更可能会支持更多的问题，但如果这些问题是人类一开始并不一定有问题的呢？是的。
- en: so we did try to control for that a little bit by like having humans rate the
    severity of the laws and whether they would have noticed them otherwise。they can
    still see a significant effect。but also like I mean a lot of the time the model
    is net picking and then those are like not the interesting cases yeah also if
    you like look at the example I showed I think is from the blog post like a lot
    of the keys are just actually quite garbage and one of the like。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们确实尝试通过让人类评估法律的严重性和他们是否会注意到这些法律来对此进行一些控制。他们仍然能看到显著的效果。但还有很多时候模型是在挑剔，而这些并不是有趣的案例。对了，如果你看看我展示的例子，我认为是来自于博客文章，很多关键其实非常糟糕。
- en: 😊，Things that makes it easy for critiques is。It's okay the most of them are
    garbage because the human can just beat them and discard them and it kind of like。More，
    you know， helps the evaluatedval know where to focus on or like。Notice like think
    of something they would have missed otherwise。So it's more like。you know the critiques
    help you brainstorm how you should evaluate or something。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 😊 使批评变得容易的事情是。大多数都是垃圾，因为人类可以轻松击败它们并将其丢弃，这样就有助于评估者知道应该关注哪里，或者注意到他们可能会错过的东西。所以更像是。你知道，批评帮助你进行头脑风暴，如何进行评估或其他的事情。
- en: but if you're kind of like using an assistant， you probably want more reliability
    than like filling most of the answers to why。😊，Yeah。Yes， how do we ensure that
    the evaluation metrics we are using in your recursive word modeling approach like
    detect deception like left turn or something don't have like major discontinuities。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你像是在使用助手，你可能希望获得比仅仅填补大多数答案更可靠的东西。😊 是的。我们如何确保我们在你的递归词建模方法中使用的评估指标，比如检测欺骗，比如左转或其他什么，不会出现重大不连续性。
- en: Yeah， I think。Well， it depends a lot what kind of community you're talking about
    right？If。You know。you get overnight like and model letters that say  a thousand0
    inches larger on like number of parameters or like equivalently better。I can like。Create
    quite a step up， and that's makes it quite difficult to。Do this kind of be evaluation？嗯。😊，So
    in that sense。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我认为。嗯，这在很大程度上取决于你所说的是什么类型的社区，对吧？如果。你知道。你收到一夜之间的模型信件，上面说参数数量大了一千倍或同样更好。我可以像。创造一个很大的进步，这使得进行这种评估变得相当困难？嗯。😊，所以从这个意义上说。
- en: I think it's going to be very important to like scale up BI more continuously
    and like do more incremental steps。Having said that， I think。The basic picture
    with the request from reward modeling is。That you're training the systems to help
    you evaluate， you know。Systems to train from or find you from the same system
    right and so if you can figure out how to get。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为将BI更加持续地扩展并进行更多增量步骤将非常重要。也就是说，我认为。关于奖励建模的请求的基本情况是。你正在训练系统来帮助你评估，知道吗？系统可以从同一系统中训练或找到你，所以如果你能搞清楚如何去获取。
- en: Like finding them in the way that they mention everything that is actually useful
    and relevant。then it would still be able to。Evaluate systems， even though they're
    much smarter than anything you could have evaluated before。Maybe let me even make
    that more concrete because there's I think a way that you could measure that or
    like one way we've tried it was using what we call the discriminated critique
    gap。So to measure that， you're training two different or you fine training two
    different models from the sametrain model。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 像以它们提到的一切有用和相关的方式来寻找它们，那么它仍然能够。评估系统，即使它们比你以前评估的任何东西都要聪明得多。也许让我更具体一点，因为我认为你可以用某种方式来衡量这一点，或者我们尝试过的一种方式是使用我们称之为判别的批评差距。所以要衡量这一点，你正在训练两个不同的或你微调的两个不同的模型来自同一个训练模型。
- en: One is a discriminator just says like。Is this the flawed response or the correct
    response or more correct response？
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个是判别器，它只是说。这个是有缺陷的响应还是正确的响应或更正确的响应？
- en: And the other one is a critique model that tells you what the flaw was。And now
    you can measure how often did the discriminator get it correctly and the discriminator
    is just like pure gradient descent。it doesn't have to want to tell you anything，
    you're just basically hooking into the model through presentations and trying
    to like get all the relevant latent insight it has to this prediction task and
    then on the other side other hand you have this critique model or this general
    assistance model and you're measuring how often does the assistant actually help
    me point to the right flaw。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个是批评模型，它告诉你缺陷是什么。现在你可以衡量判别器有多频繁地正确，判别器只是纯粹的梯度下降。它并不需要想告诉你任何事情，你基本上只是通过表示与模型连接，并试图获取它对这个预测任务的所有相关潜在见解，而在另一方面，你有这个批评模型或这个一般的辅助模型，你在衡量这个助手实际上多频繁地帮助我指向正确的缺陷。
- en: And then if you compare these two scores， you can like look at the gap between
    them。Hence。sorry discriminated critique gap。And then if the gap is large。then
    the model knows a lot about the tasks it's not telling you， and so that would
    be very worrying。And the gap is small then。You know， there's much。Isumably there's
    like fewer things that the model is like not telling you。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如果你比较这两个分数，你可以查看它们之间的差距。因此。抱歉，判别的批评差距。如果差距很大，那么模型对任务了解得很多，但却没有告诉你，这会非常令人担忧。而如果差距很小。那么，你知道的，可能模型没有告诉你的事情就少得多。
- en: but I guess there's like lots of value in like ensuring this this approach is
    like robust and like contest testing it on learn from。That's right。That's why
    we want to test it on current modelss。Yeah。Yes， I don't know who was first。I think
    someone in the back。😊，I。So I wanted to ask about like maybe first the NX slide
    where like there was like human strong was like。And so， you know， I've been helping
    notice like part of that also is like communicating what you want the AI to do。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但我想确保这种方法是稳健的，并在学习中进行更多的逐步测试是很有价值的。这就是为什么我们想要在当前模型上进行测试。是的。我不知道谁是第一个。我想是后面有人。😊，我。我想问一下，也许首先是NX幻灯片，上面有关于人类强度的内容。所以，你知道，我一直在帮助注意到，部分内容也是在传达你希望AI做的事情。
- en: right， not just like evaluating， but like communicating。Perhaps like， I would
    like you to do this。And maybe， or maybe I that。And so like at least like in my
    personal experience using the chat。like there are some things that could do that。It's
    crazy。for instance， And I was like， no。like how did that come。Or， you know， if
    you can ask about like click。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，不仅是评估，还有沟通。也许我想让你这样做。也许，我希望我这样做。所以至少在我个人的聊天体验中，有些事情可以做到。这太疯狂了。例如，我会说，不。那是怎么来的？或者你知道，如果你能询问点击。
- en: there' are like different things right Or I'm like， okay， like， what can I ask
    for and like。what point is that。One thing that I thought was a bit concerning
    with just this idea that like。you know， people don't always communicate their
    preferences like。Honestly， we're like。Like there could be like like coordinated
    efforts， right。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的事情，对吧？或者我在想，好吧，我可以要求什么，到了什么程度。让我觉得有点担忧的事情就是，像这样，人们并不总是沟通他们的偏好，老实说，我们可能会。可能会有协调的努力，对吧。
- en: like like instill rewards for like specific capabilities。you know。like coordinated
    effort to do such a thing。one idea。like I had this was like I tried to ask if
    that has like some idea of like Wi for itself' I didn't know how to use it at
    first。So I just thought like maybe user。 There didn't seem to be one。 But like
    I was hoping there was one。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 就像是为了特定能力而设立奖励的协调努力。一个想法是，我尝试询问是否有一些关于Wi本身的想法，但我一开始不知道怎么用。所以我想，也许用户应该有这样的东西。似乎并没有。但是我希望有一个。
- en: there was one for like G3， right Like I think fman a little I was hoping And
    so my question is like。UmHow how do you like make that sort of like thing safe
    right like have you like recognize coordinateordin efforts to like you know like
    specifically reward certain kinds of behavior maybe like some group on decides
    that they would like toum yeah。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于G3来说是有的，对吧？我认为我有一点希望。我的问题是，你如何使这类事情安全？你是否认识到协调努力来具体奖励某种行为，可能某个群体决定他们想要这样。
- en: you know give it some capability so this is a you know like yeah this is a really
    good question and like in a way。I mean， the first obvious thing that you shouldn't
    do is like you shouldn't just like literally train on the data that people give
    who like using the interface and we've kind of like seen other examples of what
    happens we do that if you think of like Microsoft T or something that can go pretty
    wrong。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，给它一些能力，这就是一个非常好的问题，以某种方式。我的意思是，首先显而易见的事情是你不应该仅仅在使用接口的人提供的数据上进行训练，我们已经看到了其他例子的结果，如果你考虑像微软的T之类的东西，事情可能会变得相当糟糕。
- en: the other thing is， I mean right now what we're doing is like we're hiring a
    bunch of people and then ask them to rate different model responses。But also now
    the question becomes like， you know， who will be hiring and like what's their
    background。what they're trying to do and so。And in particular。like the thing I
    think we're doing quite poorly right now is like actually。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件事是，我的意思是，现在我们所做的就是雇用一群人，然后让他们评估不同模型的响应。但现在问题变成了，你知道，谁会被雇佣，他们的背景是什么，他们在试图做什么。因此，特别是，我认为我们现在做得相当糟糕的是，实际上。
- en: Like importing like a diverse and representative set of human preferences and
    more just like you know whoever we end up we can hire and so I kind of wish there
    was also this like。More targeted research on like how we should do that and how
    that could be done well and some of it is also like you know better placed outside
    of like big tech companies because if you are like tech companies always have
    an incentive to like you know。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 就像导入多样且具有代表性的人类偏好一样，更像是你知道的，我们最终能雇佣谁，所以我有点希望能有更有针对性的研究，关于我们该如何做到这一点，以及如何才能做到好，其中一些也许更适合在大型科技公司之外进行，因为如果你是在科技公司，总是有动力去。
- en: Yes。Import human preferences in the way that maybe is not like the thing that
    we actually humanity would do under flex or thing and so I think it's a really
    big important question like data contaminations like the dual problem to this
    like do you think obvious yeah I mean like is that something people might can
    anyone can poison in free training way just put something on the internet？
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。以人类偏好的方式导入，可能并不是我们在灵活状态下人类实际上会做的事情，所以我认为这是一个非常重要的问题，比如数据污染，这就像是这个双重问题，你觉得明显吗？是的，我的意思是，这是不是人们可以用任何人都可以通过自由训练的方式污染网络上的东西？
- en: And it's， you know， something that we have to be very mindful of。Yes， I don't
    know。Considering that we're currently training these models should also work our
    these things。Hofully getting closer to preferences at this point。As human practices
    change。as we've seen like past， like just。is there something。They should keeping
    up that's like。Yeah。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，这一点我们必须非常注意。是的，我不知道。考虑到我们目前正在训练这些模型，也应该解决这些问题。希望此时能更接近偏好。随着人类实践的变化，正如我们看到的过去一样，是否还有什么东西应该跟上？
- en: I mean， the most obvious thing is its like the model knowledge base is kind
    of like the pre training cutoff date like somebody。you know， whatever data you
    went into pretrain， it doesn't know about like a lot of things that happened after
    that。U in terms of updating kind of like human preferences or the， you know。like
    the comparisons that go into the robot model。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，最明显的事情是模型的知识库就像是预训练截止日期的某种东西；你知道，无论你输入了什么数据进行预训练，它对之后发生的许多事情都一无所知。在更新人类偏好或用于机器人模型的比较方面也是如此。
- en: you just collect more data and retrain and the functioning run is like comparatively
    cheap so you can you know do that again。I think what gets harder is that you know
    like。As you have deployed the model and people started using it for all kinds
    of。you know， tasks that they want to build their company around like they。Um。if
    you update and you change the model， then they also have to do a bunch of work
    into like adopting that prompt to whatever they have they're doing。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 你只需收集更多数据并重新训练，而功能运行相对便宜，所以你可以再次这样做。我认为变得更难的是，当你部署模型后，人们开始使用它来进行各种任务，以便围绕他们的公司构建东西；如果你更新并更改模型，他们也必须进行大量工作来适应他们正在做的事情。
- en: And so it doesn't come as with zero cost。Yes sorry you so I't know of' seeing
    human performance but one of the the advantage of G3 is that it has this immense
    corpus of the invasive the entire internet if you want to specialize in a specific
    domain like chemistry or material materialial science or something in particular
    to know generate new contacts。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这并不是零成本的。是的，抱歉，我不知道人类的表现，但G3的一个优势是它拥有这个庞大的语料库，几乎涵盖了整个互联网。如果你想在特定领域如化学或材料科学中专业化，生成新的内容。
- en: 对。己知嘅但咧嚟。To use less data， it' still where fast we should be。You mean like less
    data on like the chemical domaination thing yeah you may just have research papers
    over the last 30 years or something yeah and you can throw it into pretraining
    right and then the model knows about it but can the model really learn effectively
    without so much data？
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对。己知的但仍然要使用更少的数据，这里仍然是我们应该快速的地方。你的意思是，在化学领域的数据更少，是的，你可能只需要过去30年的研究论文，然后可以将其投入到预训练中，模型就知道这些内容，但模型真的能在没有那么多数据的情况下有效学习吗？
- en: Or cant be somehow adapt the abstract concept behind GP3？Or that's okay。Yeah。I
    mean that's kind of the general idea with what you intend to do with fine training
    and to some extent we've seen it like generalizing this way。for example， instructorstructBT
    was trained almost entirely on English。Language。feedback and demonstrations。And
    it works in other languages and so that's kind of wild and so similarly you know
    you could train the model with people who don't know anything about chemistry
    and then you know it learns to follow instructions and it will do so like on the
    topic of chemistry and this fine can be very sample efficient like with 100 data
    points you can actually make a meaningful change in the model behavior。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 或者无法以某种方式适应GP3背后的抽象概念？或者这没关系。是的。我的意思是，这就是你打算通过微调实现的总体思路，在某种程度上我们已经看到了这种泛化的方式。例如，instrcutBT几乎完全在英语语言上进行训练。反馈和演示。而且它在其他语言中也能工作，这真是太神奇了，因此类似地，你可以用对化学一无所知的人来训练模型，然后它会学习如何遵循指令，并在化学主题上这样做，而这种微调可以非常高效，比如用100个数据点就能实际改变模型行为。
- en: So it can be quite effective。I that pick someone hasn't asked， yes。Regarding。Response
    generation。do you or how much ever effort do you put on our emphasis and training
    on different expression style。So what I've noticed from activity that it always
    gives you back very structured to scientifically structured answers。Do you consider
    any training if。It turns you like a scientific the scientific East Church should
    answer or rather an asterisk。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这可以非常有效。我觉得有人问过，没错。关于。响应生成。你在不同表达风格上的努力和培训有多少呢？我注意到，从活动中，它总是给你非常结构化的、科学结构化的答案。如果它把你变成一个科学的科学东正教回答，还是说一个星号。
- en: 嗯。Yeah。I mean， the tricky thing is ideally the model should give you the kind
    of answer that you want to have right and some people prefer more scientific or
    technical answer。some people might prefer a more generic answer。And。Me mean right
    now。like chat GT doesn't have like， you know， a way for you to set like your specific
    preferences and that's something that。you know would be really exciting to have。But
    also， I think。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，是的。我觉得棘手的地方在于，理想情况下，模型应该给你想要的答案，有些人更喜欢科学或技术性的答案，而有些人可能更喜欢更通用的答案。而且，现在的chat
    GT没有办法让你设定具体的偏好，这真是令人兴奋。但我还认为。
- en: The kind of sta probably that you've observed are is in fact like probably a
    product of our labeler pool。And so a lot of the chatB key workers were like more，
    you know like。I think more like computer scienceency and like more there was like
    more data generated by programmers。Compared to instruct GT， which was more like
    generalist labelers。And。And yeah。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 你观察到的那种状态实际上可能是我们标注者池的产物。因此，很多chatB的工作人员更像是，你知道的。我认为他们更像是计算机科学相关的，还有更多数据是由程序员生成的。相比之下，指令GT的标注者更像是通才。而且，是的。
- en: there's like different， it's like kind of it changes also the style。So there
    is no specific。To distinguish that。Yeah， I mean。We should make a distinguished
    effort it should give you like the style that you want。right？嗯。Yes。So one of the
    things that Im thing about is how。Is get a quick actually。In的I。Your generation
    or coming generation and so go back to。The AI progress。对。Human level， yeah。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像不同，它的风格也会改变。所以没有特定的区分。是的，我的意思是。我们应该做出明确的努力，给你想要的风格，对吧？嗯，是的。所以我想到的一件事是，如何能快速地。在你的这一代或未来的代际，回归到。AI的进步。对，人类水平，是的。
- en: let humans get evaluated， what I'm sorry to think about is like。Over great I
    have you as Ive showed like my 10 year old cousin how your cha routine just to
    mess around with and that green line is a lot over。U and furthermore， if SA this
    becomes part of their educational experience。it's going to be much better I ever
    perceive it to be more difficult for them to discriminate even simpler tasks。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让人类进行评估，我在想的是。过于伟大，我给我的10岁表弟展示了你的聊天程序，仅仅是为了玩一下，那条绿色线比你想象的要多。此外，如果这成为他们教育体验的一部分，他们将更难以区分甚至更简单的任务。
- en: And so I are thinking about it。How that might disrupt or make this alignment
    a little bit more difficult in the long run as you have people who are more rotate。for
    instance， what Jack sorry Jack QBC says as like a given truth anyway。I was just
    wondering what？
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我在思考这件事。长期来看，这可能会扰乱或使对齐变得更加困难，因为你有更多的旋转人。例如，杰克说的那个真相。我只是想知道什么？
- en: I mean， there's a real risk of overlying on a tech that is immature and that
    is not ready for。you know。You just believing like please don't believe everything
    the model sets right but also I think one thing。😊，That I'm hopeful for is that
    like， you know， your cousin will end up like figuring out how to do this where
    like。you know， they grew up with。You know， like all of these AI tools that are
    getting better and learning how to actually leverage them。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，过于依赖一个不成熟且尚未准备好的技术确实存在风险。你知道。请不要相信模型说的一切，没错，但我认为有一件事。😊我希望的是，你的表亲最终会找到解决办法，他们从小就接触到这些越来越好的AI工具，学会如何实际利用它们。
- en: 😊，Productively， right， and like it's kind of like。You know。20 years ago or something
    when you were like you like using Google search much earlier than everyone else。you're
    probably going to get better at like using that as a tool for everything you want
    to do。嗯。Yeah。I think you headed your hand up for a while。The slide where tasks
    in。呃 chat test。Model tasks。嗯。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，有效地使用，对吧，就像是。你知道，二十年前，或者在某个时候，当你使用谷歌搜索时，比其他人早得多。你可能会在使用这个工具方面变得更好，来完成你想做的一切。嗯。是的，我觉得你举手举了一段时间。任务中的幻灯片。呃，聊天测试。模型任务。嗯。
- en: 哦，喂，对， last one。Yeah， so right now， it seems like you guys are。Using humans
    as biological sensors to the real world to like physical ground truth。And。Using
    language as like a compressed interface to that ground truth。are you guys also
    looking at using？Accessor technology directly with your models to get a more truthful
    answer of。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，喂，最后一个。是的，所以现在，看起来你们在使用人类作为与现实世界的生物传感器，获取物理真相。并且使用语言作为对这种真相的压缩接口。你们是否也在考虑直接使用接入技术与模型结合，以获取更真实的答案？
- en: you know。Yeah， I mean， it depends on what that sensor could be right like I
    guess like one of the most straightforward things is you could ask the model to
    browse and then it can like fact check its own answers and it can you know like。😊，Import
    external knowledge that I didn't remember。And。😊，诶。Yeah， I think that would be
    quite useful。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道。是的，我的意思是，这取决于传感器可能是什么。我想，最简单的事情之一是你可以要求模型浏览，然后它可以自行核实答案，能导入我不记得的外部知识。而且。😊，嗯，我觉得这会很有用。
- en: I think thatll be quite useful for assisting human evaluation。😊，诶。Whats that
    going work？
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这对于辅助人类评估非常有用。😊，那将如何进行？
- en: And you can look at WebGbT， which you know， is a published work on using the
    model for browsing。嗯。I think so one thing that makes it harder when you're using
    this like external sensors or if you。Letting the model interact more directly
    with the real world is that it raises more safety questions。right？If you let your
    language model make arbitrary API calls。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看看 WebGbT，众所周知，这是一个关于使用模型进行浏览的已发表工作。嗯。我觉得，当你使用这些外部传感器时，有一件事情使得使用变得更困难，或者如果你让模型与现实世界更直接互动，会引发更多安全问题，对吧？如果让你的语言模型进行任意
    API 调用。
- en: then you have to be a lot more careful with which calls thisilla to make and
    which is it not。And if you。As a poster， if you just like you're reviewing everything
    the model says。then you can decide which ones you want to make。嗯。😊，So yeah， it's
    an open problem。Okay。one more question。I think you didn't。这。About the reasoning
    abilities of these asological language models。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你必须更加小心选择调用哪个。这些调用。如果你作为发布者在审查模型所说的一切。然后你可以决定想要做哪些。嗯。😊，所以是的，这是一个开放的问题。好的，还有一个问题。我想你没有讨论。这些模型的推理能力。
- en: I've seen like different people talk about how is only like a fixed amount from
    two per token while like humans。they have system one and system two where we could
    like just speak quickly versus actually do some reasoning and think through things
    that was more effort。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我见过不同的人讨论如何仅仅是每个令牌固定数量，而人类则有系统一和系统二，我们可以快速说话，或是进行一些推理和深入思考，这需要更多的努力。
- en: And then I've see in other words to try to like kind of use the force it to
    a chain of problems or the chain of like reasoning or like let' things step by
    step and stuff do think that stuff is sufficient to really get reasoning thats
    the level what we want or will require real big fine tuning or architectural changes。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我看到其他人试图强迫它解决一系列问题，或者进行一系列推理，逐步来做事情。你觉得这些东西足以真正获得推理吗？这是我们想要的水平，还是需要真正的大规模微调或架构改变？
- en: I don't know。I'm also the wrong person to ask， I'm mostly not trying to。Get
    the models to have new capabilities and more like you know， getting them to play
    on team human。Oh， do we want to do the online questions？呃，是嗯。有。I also makes sense
    but my question。So what you think is the is it a role for often adults in from
    like especially if you have like human like you don't teach ats。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道。我也不是问这个的合适人选，我大多数时候并不想这样做。让模型具备新的能力，更像是让它们加入人类团队。哦，我们想做在线问题吗？呃，是的，有。我也觉得有道理，但我的问题是。你觉得对于成年人来说，特别是如果你有像人类一样的东西，这是否是一种角色，你并不教它们。
- en: it's hard to more to human。So you think like this recipe be even more often
    hour。All。Yeah。quite possibly。嗯。😊，I mean yeah as you point out right like there
    is a lot of conversational data and if you can use it that that would be it should
    be useful I think broadly can categorize this kind of thing as like let's make
    the R algorithm better and like ourL feedback and I think that's valuable and
    that should help us like make the same prechain models like more aligned according
    to the human preferences that we collected。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对人类来说更难。所以你觉得这个配方甚至会更加频繁地使用。是的，很可能。嗯。😊 我的意思是，正如你指出的那样，确实有很多对话数据，如果你能利用这些数据，那将是有用的。我认为可以广泛地将这种事情归类为让R算法变得更好，以及我们的反馈，我觉得这很有价值，这应该帮助我们使相同的预训练模型更符合我们收集到的人类偏好。
- en: 嗯。😊，But also you would still run into the all the limitations that ourLF has
    right I think there's a fine API for G3 I don't think it offers our role right
    now but supervised fine training so you could do like。😊，You can like this so best
    of them and do this kind of exp iteration or all questions so the first question
    is could you more clearly describe the full training process by child G example
    starting with the text Dancy 001 and X or programming data by steps of higher
    left？
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。😊 但你仍然会遇到所有我们的反馈所存在的局限性。我认为G3有一个不错的API，但我不认为它现在提供我们的角色，但可以进行监督微调，所以你可以这样做。😊
    你可以把它们结合起来，进行这种类型的实验或所有问题，所以第一个问题是你能更清楚地描述完整的训练过程，从文本Dancy 001和X或编程数据开始，按更高的步骤进行吗？
- en: 😊，Sorry I didn't I didn't catch that start with texting 001 and then so how
    much images of data need you getting use and there are how many steps of part
    of that pitch kind of thing。😊。![](img/b473ddacf8908cc92f68d174d1de8757_37.png)
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 😊 对不起，我没听清，从文本001开始，那么你需要多少图像数据，还有多少步骤部分在那个提案中？😊。![](img/b473ddacf8908cc92f68d174d1de8757_37.png)
- en: And。I think the exact numbers are not public， it's basically similar to instruct
    GBT and for the instructGBT numbers。![](img/b473ddacf8908cc92f68d174d1de8757_39.png)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，我认为确切的数字是保密的，基本上与指令GBT类似，关于指令GBT的数字。![](img/b473ddacf8908cc92f68d174d1de8757_39.png)
- en: And we had。I think around 50，000 comparisons。And probably like 10。000 demonstrations
    or like maybe tens of thousands， I don't remember the exact number。![](img/b473ddacf8908cc92f68d174d1de8757_41.png)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有。我想大约有50,000次比较。还有大概10,000次演示，或者说可能是成千上万次，我不记得确切的数字。![](img/b473ddacf8908cc92f68d174d1de8757_41.png)
- en: so I had like this other slide with yeah， it was like about 20。000 hours of
    human feedback what I calculated right。I mean the big question is like how do
    you make how do you ensure quality？😊。![](img/b473ddacf8908cc92f68d174d1de8757_43.png)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我有另一张幻灯片，是关于20,000小时的人类反馈，这是我计算的。我的意思是，大问题是你如何确保质量？😊。![](img/b473ddacf8908cc92f68d174d1de8757_43.png)
- en: 嗯。😊，But any case need sometimes flip on let and click that I'll give anything
    something。But it's the whole problem， right like that assume you really have the
    one model that you trust。Okay。Sure， so the next question I think council of before
    was you want to automate a land research search for f conceptual issuess which
    are difficult for experts to verify part。I mean。The kind of like ambition of that
    plan is to train a model that can do this kind of conceptual research。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。😊 不管怎样，有时需要切换并点击，我会给你一些东西。但这就是整个问题，对吧？假设你真的有一个值得信赖的模型。好的。没问题，下一个问题我认为之前的建议是，你想要自动化对概念问题的研究搜索，这对于专家来说是很难验证的部分。我的意思是，这个计划的目标是训练一个能够进行这种概念研究的模型。
- en: And you know， you know， you can picture is it like a language model that like，
    you know。writes an alignment research paper that we you know， like we read and
    then we're like， oh。this is a really cool idea， we should try this。😊，And I think。You
    know。going back to evaluation is's easier in generation。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，你可以想象这就像一个语言模型，它像是，写一篇对齐研究论文，我们像是读了这篇论文，然后我们就觉得，哦，这是个很酷的想法，我们应该尝试一下。😊 我觉得，回到评估方面，生成时会更容易。
- en: I think it also applies to alignment research and like I think at the very least
    like I find it much easier to evaluate you know。alignment research than I find
    it to like produce it。And so。While there might be conceptual breakthroughs that
    we need that we couldn't even evaluate right now because theyre just like。you
    know， if we saw them， we'd be like， what is this？And this is kind of like。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这同样适用于对齐研究，至少我发现评估对齐研究要比制作对齐研究容易得多。因此，尽管可能有我们需要的概念性突破，但我们现在甚至无法评估它们，因为它们就像，如果我们看到它们，我们会想，这是什么？这有点像。
- en: Like this is like the reason why we want to de scalpal oversight， right， because
    you know， like if。you know， the language model produces this really brilliant
    insight and we can't even recognize it at the time。Um， we should be able to have
    an easier time recognizing it if we use DI assistance and if we leverage like
    our best AI models。😊，To like figure out whether or not that was a good idea what
    is the weaknesses and what are the strengths and like you know what kind of experiments
    should we run to know whether this is a good idea and so。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们想要减少监督的原因，对吧？因为你知道，如果语言模型产生了这个非常出色的见解，而我们当时甚至无法识别它。嗯，如果我们使用DI辅助，并利用我们最好的AI模型，我们应该能够更轻松地识别它，😊，以确定那是否是个好主意，它的弱点和优点是什么，以及我们应该进行什么样的实验来知道这是否是个好主意。
- en: Yeah， I think basically。You know， the story of just using ourHF to train a model
    to do good alignment research。You have the obvious pitfalls， which is， you know，
    the model might write like an alignment proposal that kind of looks good to us。but
    is actually， you know， not a good proposal and it creates AI that is misaligned
    with humans。😊。And and so in order to distinguish the two which might be really
    hard， maybe it's not， but you know。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我基本上觉得。你知道，利用我们的HF训练一个模型以进行良好的对齐研究的故事。你会面临明显的陷阱，也就是说，模型可能会写出看似不错的对齐提案，但实际上，并不是一个好的提案，导致产生与人类不对齐的人工智能。😊。因此，为了区分这两者，可能会非常困难，也许不会，但你知道。
- en: I think we should expect it to be really hard and then leveraging EI assistance
    to evaluate that seems like a really。Promising plan So about AI that that will
    be the skills。I mean， that was my whole point。it's not sufficient。かち。But do you
    think like do you need like something more or you want got open strategy that
    you think like if you can like get a lot of feedback data will that be sufficient
    to get very generations or do you see more in that I' mistake I mean the journalists
    or like I think the。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们应该预期这会非常困难，然后利用情感智能（EI）协助进行评估，这似乎是一个非常有前景的计划。所以关于人工智能，这将是技能的问题。我是说，这正是我的观点。这是不够的。かち。但是你觉得需要更多的东西，还是你想要一种开放策略，你认为如果能获取大量反馈数据，这是否足以产生良好的结果，还是你觉得还需要更多？我觉得我可能搞错了，像是记者，或者我认为是这样的。
- en: 😊，Basically the vast majority of the model's capabilities and like all the cool
    things you see it do come from pretraining and now it's from the fine tu stage
    the reason why people sometimes attribute to the fine tu stage is that you didn't
    see it in the pretrainin model and the reason I think the reason that we didn't
    see it in the pretrainin model is because the pre model was so misalign it was
    not trying to help you and was not trying to show you all the things it can do
    and instead it just regurgent takes a bunch of random web textext。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，基本上，模型的大多数能力以及你看到的所有酷炫功能都来自于预训练，现在则来自于微调阶段。人们有时将其归因于微调阶段的原因是你在预训练模型中没有看到它，我认为我们在预训练模型中没有看到的原因是，预训练模型是如此不对齐，它并不想帮助你，也不想向你展示它能做的所有事情，而是仅仅复述了一些随机的网络文本。
- en: 😊，And that's not what you're looking for。And so。Yeah。I think that what our basically
    has been doing is like unlocking capabilities that we're already a model。And making
    those available for human to use。And some ways like。You know。alignment research
    is very dual use in the sense that， you know， a。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，这不是你所寻找的。所以，是的。我认为我们所做的基本上是解锁已经存在于模型中的能力，并使其对人类可用。在某种程度上，像你知道的，对齐研究在某种意义上是双重用途的。
- en: if you have really good alignment techniques， you can use it to align with whatever
    values you want。including values that， you know， we wouldn't particularly endorse。And
    B。It also。Like if you're doing alignment right， it will always look a little bit
    like you made the AI system more capable because before it was wasn't really trying
    that hard to help you and now you've made it more aligned so you know you actually
    see these capabilities you already had。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有非常好的对齐技术，你可以用它来对齐任何你想要的价值观。包括一些我们不会特别赞同的价值观。而且B。如果你对齐得当，它总是会看起来像是你让AI系统更强大，因为之前它并没有真正努力帮助你，而现在你让它更对齐，所以你实际上看到了它原本就有的能力。
- en: Sure。Let's see。Yeah this follows a question before how do we deal with outlooklet
    that's written so this was like reference sort of but pro values but optimizing
    for them to size to incentivize deception so how do we contrary？
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当然。让我们看看。是的，这个问题跟之前如何处理那种写的前景有关，所以这是一种参考，但是为了优化它们的价值以激励欺骗，那么我们如何应对？
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_45.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_45.png)'
- en: Yeah， so that was what I was talking about here， right？😊。![](img/b473ddacf8908cc92f68d174d1de8757_47.png)
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以这就是我在这里讨论的内容，对吧？😊。![](img/b473ddacf8908cc92f68d174d1de8757_47.png)
- en: This is like the whole problem that we have where。What humans can evaluate is
    constant and so we won't be able to evaluate like sophisticated attempts at deceiving
    us and that's why we want to do scalable supervision so that we empower humans
    to spot these attempts at deception。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们所面临的整个问题。人类能够评估的是恒定的，因此我们将无法评估复杂的欺骗尝试，这就是我们希望进行可扩展监督的原因，以便我们能够识别这些欺骗尝试。
- en: Yes。到。How will your plan for contract in surveillance stand up to our and use
    distinction shifts and avoid really changing because of other care assistance。Yes，
    so。I think these are real worries and。To some extent we kind of like have to test
    empirically like how。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。到。你们的监控合同计划如何应对我们的区分转变，并避免因其他护理支持而真正改变？是的，所以。我认为这些是真实的担忧。在某种程度上，我们必须通过实证测试来看如何。
- en: How difficult and how severe they actually are and I think so my personal stance
    right now is something like I think。Trying to get the outer alignment signal really
    right is going to be like 90% of the effort。And once we have that， then a lot
    of the other things might also fall into place， so for example。I mean it kind
    of depends on which story of in misalignment you're worried about。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上它们有多困难和严重，我个人目前的立场是，我认为。试图正确获取外部对齐信号将占据90%的努力。一旦我们有了这个，很多其他事情也可能会顺利进行，比如。例如。这有点依赖于你担心的不对齐故事。
- en: but you know one story is you kind of training your system and it learns how
    to do。ItGs basically a bunch of inner optimizes， kind of like meta reinforcement
    learning。😊，So for example。like GPT3 can do like in context learning and that's
    like a kind of you know， learned optimizer。And so now you're like doing our A
    Jeff training or whatever like alignment training you have。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 但你知道，一个故事是你训练你的系统，它学习如何去做。基本上，它是一堆内部优化者，有点像元强化学习。😊所以例如。像GPT3可以进行上下文学习，这是一种你知道的，学习的优化器。所以现在你正在进行我们的A
    Jeff训练或其他任何对齐训练。
- en: and you're like the the then optimizers learn to do the thing that you want
    on the division。But now if you have a distributional shift and this distributional
    shift could be autoed。meaning like the model is causing it itself。🎼And now you're
    going out of the distribution。all these inner optimizeizers， like try to optimize
    for something else。And。One way you can like。And。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你就像是优化者学会了你在分部上想要的事情。但是现在如果你有一个分布转变，这个分布转变可能是自我引起的。也就是说，模型自己导致了它。🎼现在你正走出分布，所有这些内部优化者都在尝试优化其他东西。而且。有一种方式你可以这样做。
- en: you know， like how much that would actually happen in practice kind of actually。but
    one kind of like more important question is like。If you have a really reliable
    outer alignment signal and you have this like general training signal that you
    trust。you can also use that， you know， on the new distribution to train the system
    to be more。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，这在实践中实际上会发生多少。但是有一个更重要的问题是。如果你有一个非常可靠的外部对齐信号，并且有一个你信任的通用训练信号。你也可以在新的分布上使用它来训练系统。
- en: Or like to get optimizes in a row， basically。And so then you've reduced like
    the inner alignment problems to like how do you deal with a distributional shift
    and how do you like construct an outer alignment signal that you trust？
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 或者想要连续优化，基本上。所以你就把内部对齐问题简化为如何应对分布变化，以及如何构建一个值得信赖的外部对齐信号？
- en: And those are problems that we have to deal with anyways。嗯。But yeah。I don't
    know how it's actually going to shake out， but。There's some important open questions。嗯，是这个。So
    regarding thoughts。A line thats one be kind of。Problems that accounting discussion。SeeThere's
    not much interest it seems in like explaining how to come these other judgments
    or that's wondering there's been much interest in AI toward decomppoing these
    models is like explaining why this even second straight arbitarily and that's
    definitely a truth around it were shows but as to why it' making these。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们必须面对的问题。嗯。但我不知道这实际上会如何发展，但。有一些重要的开放问题。嗯，是这样的。关于思考，一个线索是，这可能算是一种。需要进行会计讨论的问题。似乎在解释如何应对这些判断方面并没有太多兴趣，或者在AI中对分解这些模型的兴趣似乎不大，这就解释了为什么这甚至第二次是任意的，确实在周围有一些真相，但至于它为何会这样做。
- en: How lot of judgments， have you all been able to interterrogate more？I mean，
    I think。Where we are right now is like pretty dis， satisfactory。I mean。you can
    ask the model why I gave a certain response。but you don't know whether it's answereding
    truthfully。And you can also， I mean。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你们能更多地质疑判断吗？我的意思是，我认为目前的情况相当不令人满意。你可以问模型为什么给出某个响应，但你不知道它的回答是否真实。而且，你也可以，我的意思是。
- en: another thing you can do is yeah you can give the model of's response and ask
    it to find out flaws。which is what we did in the C's paper。嗯。😊，But， you know，
    I think that like。I mean。there's one version where you try to make that better。but
    then the question is like what is your going to a signal？
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以做的另一件事是，给模型一个响应，并要求它找出缺陷。这就是我们在C的论文中所做的。嗯。😊，但你知道，我觉得就有一个版本，你试图改善这一点。但问题是，你的信号是什么？
- en: I think a like better angle of the attack is probably interpretability。Where，
    you know。like you figure out how to look inside the model and then。はです。And that's
    what I was passionate about that the level of the research at interpret built
    yeah。is like it seems that that's been really to do for you largering smell different
    and have such a high space。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为更好的切入角度可能是可解释性。在这里，你可以弄清楚如何深入模型内部，然后。是的。我对可解释性研究的热情在于，这似乎真的对你们的模型有很大帮助，且具有较高的复杂性。
- en: Your current thinking is moving towards black that phrase like reducing the
    missionality of that representation and something that needs。Yeah， I mean， we
    are working on that problem， but I don't think we have anything that I'd like
    to show right now and so。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你当前的思考正在向黑箱那一类转变，比如降低这种表示的任务性，这需要。是的，我的意思是，我们正在研究这个问题，但我认为现在没有任何我想展示的东西。
- en: U。It seems generally not to be a very easy problem， but you know。'm I'm hopeful
    that we can do some things， I think in general。The problem of interpretability
    or like using interpretability for alignment is kind of tricky because。I suspect
    it's going to be neither it's going to be not sufficient and it might not be necessary。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎普遍不是一个容易解决的问题，但你知道，我对我们能做一些事情感到乐观。总体而言，利用可解释性进行对齐的问题有点棘手，因为我怀疑它既不充分，也可能不是必要的。
- en: so any amount of interpretability you can leverage would be useful because it's
    another tool in your toolbox of like detecting deception or like knowing you know
    what you said like how why the model gave a certain answer or made a certain decision。And。😊，S。You
    know， it is。Kind of unclear if you really get really good at interpretability。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，任何你可以利用的可解释性都将是有用的，因为这是你检测欺骗或了解模型为何给出特定答案或做出特定决定的工具之一。而且，😊，S。你知道的，这有点不清楚，如果你真的能在可解释性方面做到很好。
- en: how you then leverage that。For alignment， like presumably you could look in
    the model and just like throw all the models out that you can find in misalignment
    in。but then aren't you just selecting for models that have misalignments that
    are really hard to find with your inability tools。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何利用这一点？对于对齐，假设你可以查看模型，并且随意排除所有发现不对齐的模型。但那样的话，你不就是在选择那些在你当前工具下很难发现的重大不对齐模型吗？
- en: True， just going all of touch because really my aspect about that as kind of
    stand practice in ethic in general is that you have to put an explanation on Yeah
    and I guess in my question like。why why would you take the。ませ。Yes， so why would
    it not been necessary？Again。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其实，只是全面接触，因为我对此的看法在一般伦理实践中是，你必须进行解释。嗯，我在我的问题中，比如说，为什么你会选择这个。嗯，是的，为什么这并不是必要的？再一次。
- en: this is kind of like an open question， but。Basically。what stance you could take
    is that at the end of the day。what really is going to matter is the decisions
    that the model actually takes and not the reasons why it took them。And so if you
    can get to the point where you're confident that all the things the model actually
    does are aligned with what you want。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点像一个开放性问题，但基本上你可以采取的立场是，归根结底，真正重要的是模型所做出的决策，而不是它做出这些决策的原因。因此，如果你能做到这一点，你就可以确信模型所做的一切都与你的期望一致。
- en: Then does it still matter what the model thinks internally？
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 那么模型内部的想法还重要吗？
- en: I don't knowYeah that's that's what were trying to do right like we're trying
    to make like a really really good evaluation signal and then you can select for。you
    know， you can train the model to do the things that you wanted to do because you
    can always evaluate。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道，对，我们就是在努力做这个，我们想要做一个非常好的评估信号，然后你可以选择。你知道的，你可以训练模型去做你想让它做的事情，因为你总是可以进行评估。
- en: Btter than the model can do stuff。Yes。Yeah， I'll probably have much thats a
    question。嗯，他个。Much good but yeah thanks so much for the great lecture Very interesting
    I might actually not much I might just do like a live I want just like application
    thing Yeah sure the end of the。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 比模型更能做事情。是的。嗯，我可能会有很多问题。嗯，他个。很好，但非常感谢你精彩的讲座，非常有趣，我可能实际上不太多，我可能只想做一个直播，我只想像应用一样。好的，最后的部分。
- en: 😊，I lost， sorry， so how do think this thing。😊，All we a around of once。![](img/b473ddacf8908cc92f68d174d1de8757_49.png)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我输了，抱歉，那你觉得这个事情怎么样。😊，我们只围绕了一次。![](img/b473ddacf8908cc92f68d174d1de8757_49.png)
- en: '![](img/b473ddacf8908cc92f68d174d1de8757_50.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b473ddacf8908cc92f68d174d1de8757_50.png)'
