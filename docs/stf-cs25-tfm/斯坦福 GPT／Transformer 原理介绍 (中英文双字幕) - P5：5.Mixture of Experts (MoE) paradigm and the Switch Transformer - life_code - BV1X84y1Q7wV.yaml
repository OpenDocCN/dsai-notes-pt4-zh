- en: ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P5Ôºö5.Mixture of Experts (MoE) paradigm and
    the Switch Transformer - life_code - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_0.png)'
  prefs: []
  type: TYPE_IMG
- en: TodayÔºå Erwin and I are going to be giving a talk on scaling transformers through
    sparsity and the kind of sparsity we're to be talking about today is the kind
    where„ÄÇyou know each input can get you knowÔºå either a different set of weights
    or have a different amount of computation applied to it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_2.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_3.png)'
  prefs: []
  type: TYPE_IMG
- en: Orren you want to start it offÔºüY„ÄÇSo I guess the overall motivation for this
    line of work is that„ÄÇYou knowÔºå the community has kind of realized that scale is
    perhaps one of the most important access to to focus on for obtaining strong performance
    and there's almost like this sort of ongoing arms race right now with different
    labs and different institutions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_5.png)'
  prefs: []
  type: TYPE_IMG
- en: Sort of competing for training the largest models„ÄÇAnd so maybe these dates back
    from early 2020 with a paper from Open AI called Scing Gs for No languageage Model„ÄÇWhere
    they find that model performance„ÄÇFollows the predictable our law„ÄÇScas sort of
    as a power with model size„ÄÇIn terms of either computeÔºå also just you knowÔºå parameters„ÄÇAnd
    so this scaling law kind of generalizes over multiple orderss of magnitude and
    that gives us the confidence that if we are to train very large models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we can expect solid performance just by extrapolating these scaling laws„ÄÇSo
    in that paper„ÄÇthey also„ÄÇFind the interesting„ÄÇObservation that„ÄÇBasicallyÔºå larger
    models are more simple efficient„ÄÇAnd soÔºå you knowÔºå if you have a fixed compute
    budget„ÄÇYou can sort of„ÄÇYou know„ÄÇyou can predict what what is the sizeÔºå what is
    the optimal model size or fixed compute budget„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the overall observation is that„ÄÇYou knowÔºå you rather train very large models
    for less steps than train smaller models for more training steps„ÄÇAnd so these
    models are scaled„ÄÇYou knowÔºå through„ÄÇBasicallyÔºå the paper focuses on dense modelsÔºå
    right„ÄÇwhere you just increase the model dimensions„ÄÇThey're not looking at sparsity
    and so sparsity is a new dimension that you can use to scale architectures„ÄÇYou
    knowÔºå N DC sort of the„ÄÇThe focus of the talk„ÄÇAnd so the spaity we we're mentioning
    here is basically„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you will have„ÄÇSpa spaly activated weights based on the network inputs„ÄÇSo every
    input will go to a roughly similar amount of computation„ÄÇbut will be applied different
    weights„ÄÇAnd so this dates back to 1991 with„ÄÇPaper called adaptive Mis of local
    exports„ÄÇAnd was recently revisited by Noam Shaesar and colleagues at Google Brain„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: With LSTMs where they're replaced sort of the fit forward„ÄÇNetworks and LSTMs
    with mixture of exp„ÄÇAnd so the way this works there roughly is that„ÄÇYou will have
    multiple experts in implementing you know„ÄÇa small network or in that caseÔºå I think
    just a„ÄÇD matrix multiplication„ÄÇAnd so you have an additional getting network shown
    in green here that outputs probably distribution over„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Experts that each talking should be sent to„ÄÇSo this priority distribution is
    computed as a softm„ÄÇand once you have itÔºå you select a few experts„ÄÇSo the are
    different strategies„ÄÇmaybe we'll talk about it laterÔºå and the output is simply
    sort of the way to make sure of all selected export outputs„ÄÇSo that they've been
    pretty successful in„ÄÇPrimarily in translationÔºå but there was someÔºå you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You knowÔºå some complexities that Hindu broader use in NLP„ÄÇAnd so the switch
    transformer paper„ÄÇAddresses some of those and will be discussing how to„ÄÇyou know
    how to fix training abilities or reduce communication costs and and reduce model
    complexity„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: AlrightÔºå do rightÔºå you wantan to go„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå so yeah„ÄÇSo one kind of approach that we're going to have first spaarsity
    is the switch transformer„ÄÇwhich is kind of like a simplified mixture of expert
    variantÔºå along with some other improved„ÄÇyou knowÔºå training and fine tuning techniques
    that allow it toÔºå you know„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_11.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_12.png)'
  prefs: []
  type: TYPE_IMG
- en: Be stably trained and also perform better when finding done a lot of downstream
    tasks„ÄÇAnd so yeah„ÄÇso the switch transformer kind of model works as the following„ÄÇso
    you have some transformer model that hasÔºå you know self attention and feed forward
    layers and the idea is that we replace maybe one every two or one every four feed
    forward layers with a switch transformer layer„ÄÇSo you can see on the left is like
    one kind of layer block which is selfatten then addized then a fee forward layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then abnormalize and in this caseÔºå we're replacing the normal feed forward layer
    with like switch layer and we can see an illustration of this on the right so
    on the right we can see that the layer has two inputs one is the token more„ÄÇthe
    other is the token parameters and we can see that these you know embedding representations
    will get sent to a router which is exactly how it works in the mixture of expert
    so the router is basically just going to be you know getting a distribution over
    all of the experts so in this case we can see that the highest probability is
    going to the expert number two out of the four experts and then the right token
    is actually having the most probability on the first feed forward weight which
    is like the first expert„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So yeah we can see here that like what we're going to do is in the switch transformer
    which is very simple is just send it to the highest probability expert and so
    here we can see where the adaptive computation lies where we'll have four sets
    of weights there's some shared weights and computation across all tokens„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: for exampleÔºå the self attention layer is computed exactly the same for the more
    token and for the parameters token„ÄÇBut in the sparse switch layerÔºå we can see
    that like actually the inputs are while having the same amount of floating point
    operations applied to them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: actually have different weight matrices„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_14.png)
  prefs: []
  type: TYPE_NORMAL
- en: Next slide„ÄÇYeahÔºå so that's the kind of high level idea with switch transformformer
    is thatÔºå you know„ÄÇinstead of sending a token to multiple different experts„ÄÇwhich
    can also increase the communication costsÔºå as I'll go into a little bit later„ÄÇit
    also just like significantly kind of simplifies the algorithm by just only sending
    it to one expert„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_16.png)'
  prefs: []
  type: TYPE_IMG
- en: So for the improved training methodologyÔºå we focused on three different things
    to help improve the training of sparse models„ÄÇThe first was selected precisionÔºå
    which like allows these sparse models to be trained in lower precision formats„ÄÇwhich
    is incredibly important Most of the models we train„ÄÇwe really don't want to be
    using F 32 because it's just slower to compute and also when you're communicating
    tenss across different processes and stuff„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: its is twice as slow just because there's twice as many things„ÄÇAlso„ÄÇwe have
    some initialization tricks and some training tricks as well for allowing them
    to be trained more stably„ÄÇespecially as the models grow in sizeÔºå which is like
    a new initialization method along with like a change to the learning rate schedule„ÄÇAnd
    thirdÔºå since that our models have so many more parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we do notice like definitely different overfiing dynamics„ÄÇespecially once we
    finet these models that have been pretrained on all of the internet on these small
    tasks with maybe only 50 to 100„ÄÇ000 examples that they can be much more prone
    to overfitting so we also look at some custom you regularization to help prevent
    some of the overfitting that we observe„ÄÇAnd finallyÔºå we also talk about this differentiable
    load balancing technique we make„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Which kind of allows you know each expert to roughly get the same amount of
    tokens„ÄÇBecause you know this is very importantÔºå especially given that we you know
    want the stuff to be efficient on hardware„ÄÇwe want roughly each expert to have
    similar amounts of token sent to it and so to kind of encourage this we tack on
    an additional like load balancing loss along with our cross entrytropy loss that
    we're training with„ÄÇNext slide Okay so here I'm going to go into selected precision„ÄÇ
    So yeah again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so like when we're training large modelsÔºå it's really important that we should
    be able to train them in lower precision formats„ÄÇSo instead of each you know weight
    being an activation being 32 bits we want to shrink it down to 16 bits and we
    use like the B float 16 representation and what we found out of the gate is that
    you know these models are just unstable especially dis sparse models are much
    more unstable than the dense models in terms of like you'll train it for 1020000
    steps and then the losses would just diverge this was something that we you know
    frequently encountered and so one key thing that we found is that basically you
    need to be casting a part of the computation in float 32 for these models to be
    able to be trained stably„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the key component that we found that you need to cast is the like router
    computation and essentially you know we can go into the technical details a little
    bit more later„ÄÇbut basically anytime that there's like these exponentiation functions„ÄÇit's
    very important that we are you having higher and higher precision because of roundoff
    errors that can then drastically change the output of some kind of exponentiation
    function So for example„ÄÇlike if you have an exponentiation function and you change
    it by 0„ÄÇ1 or 02 or 0„ÄÇ3„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this can drastically change the output of like exponentiating it„ÄÇespecially
    depending on how large the input is„ÄÇSo yeah„ÄÇso this was like a very important
    thing and it basically doesn't change the compute at all and allows the models
    to just be significantly more stable„ÄÇNext slide„ÄÇSo the second thing we looked
    at is also the initialization scale„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so like the standard way that we were initializing these models we found to
    also just make the models much more prone to being unstable and or just performing
    worse so one thing that we did that we found was very effective was to just simply
    make the initialization scale much smaller and when we did this we found that
    you know the quality just like drastically improved it was like a very simple
    fix„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Next slide„ÄÇAnd the third thing I mentioned where since we notice that these
    models are much more prone to overfitting„ÄÇsince they just have significantly more
    parameters„ÄÇIs that we also use much more dropout for the expert layers only so
    here we can see we took we have like you know the T5 base which is a dense model„ÄÇand
    then we have a bunch of different switch variants on that and we found to be the
    most effective on these four different fine tuning tasks was just to really significantly
    increase the dropout rate inside the expert layers and we found that this was
    pretty effective for combating the overfitting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: slide we have a question Oh also about the students Yeah okayÔºå let me take a
    look„ÄÇDo you want to go ahead I can ask is just in reference to that previous table
    where you you have throughput and precision it just seems surprising to me that
    you could match this 1390 number„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We using selective precisionÔºå it seems like I would expect it to be like something
    in between„ÄÇYeah„ÄÇso it essentially comes down to the fact that like there's maybe
    a little bit of noise sample bit the speed and the only part we're casting is
    the router„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is you knowÔºå maybe like it is such a insignificant portion of the computation
    and there's zero communication there that it's essentially like a free operation
    in the network so whether you cast it to B flow 16 or flow 32 it doesn't actually
    impact the speed at all within the precision that we can actually measure the
    speed„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And alsoÔºå these architectures only use fast layer when when every fall layers„ÄÇAnd
    so„ÄÇYeah„ÄÇessentially the flood 32 parties is kind of very negligible in the entire
    architecture„ÄÇIt's like for exampleÔºå I think like off the top of my head it's like140th
    the computation that would cost for you to do the first like like weight matrix
    multiply in like a dense raylude dense layer or something„ÄÇso it's a veryÔºå very
    small part and yeah we're not using them very frequently like Erwin mentioned
    as well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Got itÔºå okayÔºå thanks„ÄÇYeahÔºå and thenÔºå you knowÔºå just like a quick point in this„ÄÇlike
    I won't into some of the technical detailsÔºå but yeahÔºå we definitelyÔºå you know„ÄÇsince
    we're training these things on hardware we really like I think a big part of the
    mixture of X paradigm is that these things are designed such that it maps really
    efficiently to hardware„ÄÇSo we want to be doing dense matrix multiplies„ÄÇAnd for
    this to work really well we also want to be able to have you know roughly equal
    amount of tokens going to each of the different experts and I think the this isn't
    that sensitive to the load balancing formulation like we tried a few things a
    lot of them worked„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but yeah essentially you definitely want some kind of load balancing loss added
    on when using sparsity Yeah next slide„ÄÇÂóØ„ÄÇYeahÔºå ErwinÔºå go ahead„ÄÇYesÔºå so„ÄÇSo the
    frameworksÔºå the library we use rely on static shapes for„ÄÇOkay„ÄÇSo we so X L A„ÄÇSo
    the compiler for tensorflow and machine offlow expects static shapes for tensors„ÄÇHoweverÔºå
    the computations in switch transformers are dynamic because„ÄÇYou knowÔºå because
    of the router„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: rightÔºå like different inputs will be routed to different experts„ÄÇAnd soÔºå we
    need to„ÄÇSpecify ahead of time„ÄÇhow many tokens will be sent to each checkbook„ÄÇAnd
    so we will introduce this expat capacity hyperpy„ÄÇTo specify that„ÄÇAnd that's going
    to be a static numberÔºå which says how many tokens each export can process„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so in practiceÔºå we instead parameterize this by having a quantity called
    the capacity factor„ÄÇSo we have an example here„ÄÇSoÔºå you knowÔºå so the bottom row
    is„ÄÇOkay„ÄÇso is a bunch of tokens on one deviceÔºå and then you need to sort of route
    those tokens to multiple devices or multiple expertss„ÄÇSo if too many tokenins
    are routed to a single exp stop„ÄÇSome tokens will be dropped because„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: as we saidÔºå like export having a fixed capacity„ÄÇSo that's the example on the
    left where the capacity factor is one„ÄÇand that basically means that the total„ÄÇIt
    does noÔºå like extra buffer„ÄÇFor writing tokens„ÄÇSo instead of thatÔºå we can use a
    capacity factor that's larger than one„ÄÇ so on the right„ÄÇyou have an example with
    a 1„ÄÇ5„ÄÇSo that means that now each expert has like sort of three slots that can
    process three tokens„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so that prevents token dropping because we have more capacity„ÄÇbut the issue
    is that this means higherÔºå you know„ÄÇthis means more expensive communication across
    devices„ÄÇYeahÔºå okayÔºå so does it„ÄÇYeahÔºå go ahead„ÄÇBut yeah„ÄÇSo yeah„ÄÇSo one thing that
    we also experimented with was this method called no token Left behind„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the idea was the following„ÄÇSo since we have to have likeÔºå you know„ÄÇa fixed
    batch size for each expert and there can be token dropping„ÄÇWe kind of we're thinking
    that„ÄÇheyÔºå yeahÔºå having tokens dropped there like you know having some tokens„ÄÇnot
    having any computationnot applied to it is probably hurting the model performance„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So what if we do a multistage routing procedure„ÄÇSo first„ÄÇyou do the normal routing
    where it's like you send each token to its highest probability expert„ÄÇBut then
    any drop tokensÔºå you then send to their second„ÄÇHighest probability expert and
    so forth and so onÔºå where you can basically repeat this process to guarantee that
    no tokens are being dropped„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: InterestinglyÔºå actuallyÔºå this approach didn't empirically improve model performance
    if anything had actually kind of hurt it„ÄÇAnd we thought that was actually very
    interesting„ÄÇAnd I think the intuition is thatÔºå you know„ÄÇonce the model learnsÔºå
    it wants to send a token to one expert„ÄÇlike it really wants to have that computation
    applied to it and just applying some other computation doesn't you know have at
    all the same property along with it actually maybe being potentially detrimental„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So yeahÔºå we thought that was pretty interesting as we were very optimistic this
    would potentially you know get improved performance„ÄÇbut it ended up not really
    making a difference and we found this quite surprising„ÄÇWe have a question from„ÄÇÂóØ„ÄÇI
    think about actually kind of like address and literally the last point that you
    brought up I think when I think about like a mixture of experts usually like they
    specialize in like different things right so I think it's like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Just like a lotÔºå like I was just wondering„ÄÇLike if you send it to like the second
    best or whatever„ÄÇlike what if like all your tokens would be particularly good
    for like one expert and then you only like process„ÄÇlet's say like 20% of your
    tokens„ÄÇSo that ends up being better than rerouting them to anything else exactly
    Yeah„ÄÇso yeahÔºå even if you're dropping a lot of tokensÔºå it's not beneficial to
    be sending them to the second third or fourth best thing and one actually interesting
    property that we„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you know noticed about these models is they're surprisingly robust to token
    dropping„ÄÇespecially during fine tuning„ÄÇSo yeahÔºå so in the standard paradigm„ÄÇwhat
    we'll do is we'll pretrain this thingÔºå we'll have some load balancing loss„ÄÇwhich
    makes the tokens pretty balancedÔºå actually„ÄÇBut then during fine tuning where it's
    like we really want to fine tuningna on a specific task we actually studied this
    exact question and we were studying does it help to have a load balancing loss
    during fine tuning or not and so if you have the load balancing loss yeah that
    kind of is encouraging you know for the specific task we want to try to have you
    know all the experts be used versus turning it off whereas there's definitely
    some you know prior specialization and it's actually much better to just turn
    the auxiliary loss off and even if it's like you know 60 to 70% of the tokens
    are being dropped that actually performs much better than you know having all
    the tokens balance but doesn't a load balancing loss encourage basically all the
    experts to learn very similar weights and then just randomly assign a tokens„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Because then it doesn't matter to which expert stuff is being sent to„ÄÇSo when
    we use the load bouncing loss like the routing mechanism is definitely learned„ÄÇso
    the model definitely is encouraged to you know choose an expert that it wants
    to send it to for good right but like if all the experts learn the same weights„ÄÇthen
    the router learns basically ohÔºå it doesn' matter where to send it to„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if you encourage load balancingÔºå you encourage„ÄÇTechnically that like you
    want any loss to fit of any expert„ÄÇrightÔºüI meanÔºå that's maybe the extreme behavior„ÄÇif
    you have a very high sort of load balancing less coefficient„ÄÇBut in practice that
    coefficient it's kind of tune and we absorb that for you know smart enough values
    the of steel loans like se like meaningful routing Yeah because it's like the
    balance between this like you know cross entropy loss and this load balancing
    loss and so on one hand yeah you definitely want to encourage the model to be
    balanced then on the other hand you also want to just good empirical performance
    and yeah the model is able to definitely like on one hand learn and specialized
    experts where they have different weights such that it's like you know definitely
    expect certain tokens decent sent to certain experts but on the other hand still
    be recently balanced so that the models are officially run on like modern hardware„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Excycl„ÄÇWe also have a question from the classroom„ÄÇSo the question that I want
    to ask is it seems to me like this is a very experimental talk we're talking about
    floating point precision we're talking about different approaches and currently
    work well and whenever were dealing with a group clients there's a question of
    what is the research question and I feel like I miss that so what are we trying
    to answer with all these experiments„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå I think I think the high level research question is like you know„ÄÇcan
    we you know create models that are you knowÔºå like doing adaptive computation from
    the standpoint of like„ÄÇyou know can we try to make models more simulate the dynamics
    that we think models should you know most naturally use„ÄÇwhich is different inputs
    have different amounts of computation applied have different weights applied to
    them you know and basically all of this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: basically we're trying to research and like figure out how can we create like
    a new framework for these models to be trained as opposed to their dense counterparts
    that you know for every input are always having the same exact computation applied
    so that's interesting because when you say the same exact computation applied
    one might imagine that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Like to me the immediate thing is about how long to deliberate about something
    what I mean by that is if we want to have variable length computation you could
    imagine that I could have a short amount of computation or it could have much
    longer computation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but this idea of like why then do we instead consider the dimension of different
    computation I mean assuming of course that these experts do indeed need to learn
    different things which I think you'll get to in yeah„ÄÇSoÔºå why do we immediately
    jump to thinking about specialized experts as opposed to thinking about variable
    length computationÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So yeahÔºå so this is actuallyÔºå we actually go into some variable length computation
    stuff later in the talk„ÄÇand I feel like they're both actually just important axes
    that should both be pushed on„ÄÇI think I guess yeahÔºå I guess it's kind ofÔºå you
    know„ÄÇI guess what I'm not freezing my question but I'm trying to understand is
    yourre thinking about why did you decide to tack this one first„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I want to understand why your team chose to go this direction first„ÄÇYeahÔºå absolutely„ÄÇso
    I think that one empiricallyÔºå it seems that sparsity has led to better empirical
    results in the field of deep learning than adaptive computation so far„ÄÇAnd I think
    the way that we use these things maps really well to our modern hardware which
    is also very promising„ÄÇAnd I think the way we were kind of looking at it is like
    sparsity is like a first step towards doing more interesting and general adaptive
    computation where and and know because I think it's like this stuff is complicated
    and typically starting from something that works well is better than necessarily
    like you know you know trying something that's not necessarily as proven out and
    then trying to like get it to work really well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I think we're kind of starting from sparsityÔºå which like nu Shaser and others
    got to work really well in the context of LSTMs we were kind of interested in
    let's port some of this to transformers let's get it working really well and then
    that's slowly start expanding towards a lot of the other natural questions that
    you mentioned whereas like okay whereas instead of different weights per core„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's also maybe have different computation per core and all of this that's
    I guess how we were kind of building the natural build up in progression of„ÄÇOur
    research got it coolÔºå thank you yeah„ÄÇWhatho do you think ErrwinÔºå anything else
    to addÔºüÂóØ„ÄÇYeah„ÄÇI meanÔºå I guess I kind of see adaptive computation and sparsity
    as„ÄÇYou know„ÄÇrelated but separate things„ÄÇ SoÔºå you know especially more like different
    parameters for each example and adaptive computation might be more different amount
    of floods„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we have some of that with the talking and droppingÔºå but that's kind of„ÄÇNo„ÄÇthat's
    not the domain„ÄÇTheomamain motivationÔºå definitelyÔºå as Barrett mentioned„ÄÇI would
    sayÔºå you„ÄÇno one really has figured out adaptive computation yet„ÄÇFor deep learning„ÄÇAnd
    what one reason is because we have these„ÄÇYou knowÔºå acceleratorsÔºå rightÔºå expectÔºå
    expect like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Sort of no„ÄÇWe need to work with like batch like data parallelismÔºå right„ÄÇAnd
    all of our accelerators in our frameworks use this SPMD paradigm where we're kind
    of supposed to apply the same computation to„ÄÇTwo examples„ÄÇAnd so if you look at
    the literature you have works like universal transformers„ÄÇwhere they replace the
    fit forward in the transformer bay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Just a recurrent weight and so it's kind of like an LSTM on each token and the
    LSTM can stop at different time and space on some„ÄÇCriteriaÔºå but the way these
    things are implemented is just through masking„ÄÇBecause it needs to be implemented
    in the SPMD programming style„ÄÇAnd so definitely spaity was kind of like easier
    to get to work first„ÄÇ And also„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there were some prior results with LSTM so„ÄÇIn terms of like the first questionÔºå
    you know„ÄÇsort of what store research question here is just likeÔºå ohÔºå can we design
    more efficient modelsÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And spaity is this new axis that hasn't been explored that much„ÄÇYeahÔºå I think
    that„ÄÇYou know„ÄÇI'm happy with just that being the research question„ÄÇGreatÔºå okayÔºå
    yeahÔºå so next slide„ÄÇYep„ÄÇOops„ÄÇYeah„ÄÇagainÔºå so kind of putting it all together„ÄÇSo
    the switch transformer layer selects an expert like just the top expert and then
    incorporates a bunch of the general sparse model improvements to„ÄÇyou knowÔºå allow
    it to fine tune better„ÄÇ allowow it toÔºå you know„ÄÇBe more regularized„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: allow it to you knowÔºå be trained with lower precision formats and a lot of like
    technical details to just get them training and working well„ÄÇYeah so one thing
    that we also wanted to do was a comparison between like top one and top two routing
    since top two routing was kind of the you know most popular technique and so here
    we can see we have two different dense models trained at different sizes and we're
    going to be looking at like the the pretrain like negative log perplexity so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå the bigger the numberÔºå the better„ÄÇSo next slide„ÄÇSo and what we're going
    to be doing is we're going to be studying them at different capacity factors„ÄÇso
    a capacity factor of 2„ÄÇ0 basically means that there' is enough buffer for two
    tokens to be sent to every single expert„ÄÇAnd we're going to be comparing like
    top one versus top two routing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And also comparing their speeds along with their like time to get some like
    threshold quality„ÄÇOkay„ÄÇyeahÔºå so here we can see in the capacity factor 2„ÄÇ0 case„ÄÇThat
    the MOE models outperform switch transformformer„ÄÇwhich makes a lot of sense like
    since switch transformformer is only you know sending like a top one token to
    each expert„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the mixture of expert is sending you know two tokens so that makes sense that
    this extra buffer will be like disproportionately beneficial for the mixture of
    expert models„ÄÇAnd so we noticed that and next slide or yeahÔºå next„ÄÇNow when we
    so the really interesting parts for the top one runningut becomes when we lower
    the capacity factors so having a high capacity factor is bad for many reasons„ÄÇone
    of which is it really incurs more of these you know communication costs for sending
    tokens to the correct experts it also incurs more compute costs and also incurs
    like a lot of memory overhead so if you can get this lower it's it's usually like
    a very very good thing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so what we see here is that switch transformformer actually outperforms
    mixture of experts when you have like a lower capacity factor and we can see that
    the time to quality threshold we you know yeah we get there much quicker and so
    even across the 2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 0 and the 1„ÄÇ25 capacity factors like the kind of preto optimal thing we saw
    on our setup is to use switch transformer at a lower capacity factor just due
    to the fact that while the quality is a little bit worse on a step basis„ÄÇit's
    just like much faster to run so it's kind of the preto optimal decision„ÄÇNext slide„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we can also be seeing that like for capacity factor 1„ÄÇ0 again„ÄÇwe can see
    that this really disproportionately benefits switch transformer and is even better
    from annaparto standpoint than the 1„ÄÇ25 capacity factors„ÄÇAnd interestinglyÔºå since
    you know„ÄÇMOE also does like a little bit more computationÔºå we can also just increase
    the amount of compute done elsewhere in the model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we can see that that's like a much more efficient allocation of compute„ÄÇSo
    yeah„ÄÇoverall our takeaway is that yeah lower capacity factors using top one routing
    is more preto efficient than„ÄÇyou knowÔºå using like top two routing at higher capacity
    factors„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_18.png)
  prefs: []
  type: TYPE_NORMAL
- en: Next slide„ÄÇOranÔºå you can take it over„ÄÇOkayÔºå so next we look at how switch transformer
    scales„ÄÇAs a function of the number of exports in the switch layers„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_20.png)
  prefs: []
  type: TYPE_NORMAL
- en: And so on the right side here you see a plot that shows complexity„ÄÇVus training
    steps for different switch architectures ranging from Tfi basease„ÄÇwhich is basically
    no expert or a single expert up to 128 experts„ÄÇAnd so you see that as we increase
    the number of experts„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which also increases number of parameters of spa parameters„ÄÇYou get sort of
    speedupsÔºå you know„ÄÇyou get increasing speedups of the dense baseline and all like
    sort of dimition returns„ÄÇTo multiplying to increasing the number of experts as
    well„ÄÇSo the previous figure was looking at complexityity versus string steps„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Here we look at complexityplexity versus strength time„ÄÇSo that includesÔºå you
    know„ÄÇAll theÔºå you know„ÄÇadditional communication costs when you have more experts
    on„ÄÇYou knowÔºå comparing„ÄÇcomparing to the dance baseline„ÄÇAnd so this is for switch
    base or then T5 bays„ÄÇand we observe x up to seven x Philipups over our T5 bays„ÄÇÂóØ„ÄÇAnd
    soÔºå you knowÔºå just to„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Maybe contextualize theseÔºå these numbers like„ÄÇYou know„ÄÇ7 x speeds into planning
    are pretty hard to obtain„ÄÇAnd so I think this is one of theÔºå you know„ÄÇone of the
    result that„ÄÇYou knowÔºå can spark a lot of interest in sports models„ÄÇeven if it's
    only for preing for nowÔºå like just having that number is likeÔºå you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: maybe it does a„ÄÇThere's a significant„ÄÇU„ÄÇDoes something significant that can
    be obtained here„ÄÇOkay„ÄÇso sports scaling laws„ÄÇSo here well look at„ÄÇSo it philosophy
    versus„ÄÇSpaous mineral parameterss„ÄÇwhich are increased by increasing the number
    of experts„ÄÇAnd so similarly to the sort of neural scaling blood paper„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We observe that as you increase the parameterss„ÄÇWhich the sparse parameterss
    and keep the fluxps fixed„ÄÇyou get diminishing like consistent gainsÔºå but diminishing
    gains„ÄÇOkay„ÄÇso now we're going to compare export parallelism and model parallel„ÄÇSo
    we introduced sparsity or export parallelism as a new dimension to scale models„ÄÇ
    ButÔºå of course„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there's the other one for dense modelÔºå which is simply model parallelism where
    you know„ÄÇmodel weights are partition across calls once they are„ÄÇAbove the the
    maximum size that you can feed on a single core„ÄÇNo„ÄÇAll rightÔºå so„ÄÇYeah„ÄÇadvertising
    the left is expo priority here„ÄÇYeahÔºå so essentially what we're doing is is yeah
    we're kind of comparing a switch base model versus the dense space and we're also
    comparing against a larger dense model that has used model parallelism and we
    can see that you know because basically when we want to scale up model size we
    kind of have two axes that we can either go through we can either increase the
    number of flops by scaling through model parallelism or increase the number of
    parameters by scaling through sparssity and so we can see that you know even compared
    to like you know a dense model that's been scaled up through model parallelism
    that' sparssity is still at the scale a more effective way to scale up the model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ByÔºå you knowÔºå still getting 2„ÄÇ5 x speed ups over this larger dense model that
    was using model parallelism„ÄÇCoolÔºå so slide a bit„ÄÇBasicallyÔºå here T5 large is the
    dense model that uses other part of„ÄÇYeah„ÄÇall rightÔºå go ahead„ÄÇOkay„ÄÇYeahÔºå and so
    one thing that we also wanted to look at is like you know„ÄÇare these expert models
    effective if you have like you know a really small amount of compute or just a
    small amount of experts„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So typically when we're designing these modelsÔºå like we have one expert per
    core„ÄÇbut if you don't have like a large cluster run these things on let's say
    you just have like a GPU with two cores or something like is having two experts
    more effective than just like a dense model And the answer is yes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we can see even pretty good scaling propertiesÔºå even with like a tiny amount
    of experts„ÄÇwhich is very promising for these models to be used even in like much
    lower compute regimes„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_22.png)
  prefs: []
  type: TYPE_NORMAL
- en: Next slide„ÄÇOr you want to go ahead„ÄÇOkayÔºå so yeahÔºå so„ÄÇYesÔºå so look at„ÄÇYou know„ÄÇWhat
    things look like when we use different types of parm„ÄÇnamely expel parm to add
    exp to model parallelm to sh model ways„ÄÇAcross calls and also data parallelismÔºå
    which is sort of the dominant paradigm in at the moment„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_24.png)'
  prefs: []
  type: TYPE_IMG
- en: And so„ÄÇYou knowÔºå I guessÔºå you knowÔºå in the previous slidesÔºå we mostly talked
    about X parallelism„ÄÇbut of courseÔºå you knowÔºå dense models and large dance models
    use model parallelm„ÄÇSo G3 and other large modelsÔºå What they do is that they will
    simply she model weights across different core„ÄÇYeahÔºå we have a question„ÄÇOh yeahÔºå
    I just wanted to know because I think there was like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I don't know if you go got under rest lateratorÔºå but I think somewhere in a
    paper„ÄÇit' said that the more experts you haveÔºå the more sample efficient it gets„ÄÇAnd
    I was just like hoping that you could give us some intuition about that because
    I„ÄÇDon't understand why that will be the case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_26.png)'
  prefs: []
  type: TYPE_IMG
- en: So I guessÔºå yeahÔºå maybe Er oh yeahÔºå so I guess likeÔºå you know„ÄÇthere's all of
    this work on larger models are more sample efficient„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_28.png)
  prefs: []
  type: TYPE_NORMAL
- en: And larger in the context of these scaling law works means like more parameters
    and more flops as you increase the number of experts there's more parameters„ÄÇbut
    not more flopsÔºå but the model is still like you know larger in like you know a
    similar sense„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so I guess like building on the intuition that larger models are more sample
    efficient in my mind it's not necessarily that's surprising that these models
    with more experts that have more parameters are more sample efficient„ÄÇI guess
    that's my kind of high level intuition for it„ÄÇYeahÔºå I would say that's kind of
    expected that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you knowÔºå more exports leads to better sample efficiency„ÄÇEspecially if you look
    at training setup„ÄÇright in a training tab„ÄÇOkay cool„ÄÇSo where are weÔºüYeahÔºå so yeah„ÄÇSo
    okay„ÄÇSo we look at how model weights are speed of a cost for different scenarios„ÄÇSo
    data parallelism is the first one„ÄÇ So that's kind of theÔºå the typical setup that
    the uses„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Especially for nuts solar large networks which don't require mother parisism„ÄÇAnd
    so let meÔºå yeah„ÄÇlet me explain how„ÄÇYeahÔºå I'll just go to the final„ÄÇFigure and
    now explain how to look at this figure„ÄÇOkayÔºå so we have 16 processes„ÄÇwhich are
    organized in a 4 by4 meshÔºå rightÔºå So each dotted line„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: each4 by four dotted line here represents a different core„ÄÇIn the first row„ÄÇstudies
    how the model weights are speed over callss and the second row„ÄÇItuss how data„ÄÇso
    literally examples and tokens are split over calls„ÄÇAnd yeahÔºå and then the final
    thing to„ÄÇThat's required to understand this figure is that each„ÄÇÂóØ„ÄÇYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: each color of the shaded squares here identifies a unique weight matrix„ÄÇOkay„ÄÇso
    let's start with data parallelism„ÄÇSo for data parallelism„ÄÇthe same model weights
    are replicated across all core„ÄÇAnd the data is simply partition of our cause„ÄÇand
    so that's what this corresponds to„ÄÇYou knowÔºå if you like using the the description
    of the caption„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the explanation of the caption that just gave„ÄÇSo next we have model parallelism„ÄÇThat's
    kind of just like a theoretical example because in practice„ÄÇpeople always use
    model parallelism in conjunction with data parallelism„ÄÇBut so if you were to do
    only model parallelm now you would have a single model weight that is partitioned
    of all course and your data would just be replicated„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Of all causing said„ÄÇSo now we have modeling data parallelism„ÄÇand that's kind
    of the typical scenario for large dense networks„ÄÇSo in that case„ÄÇmodel weights
    are partition among a subset of the calls„ÄÇthe subset of calls that process different
    batches of data„ÄÇAnd so in that example here we have„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You knowÔºå sort of fourÔºå so the first sub square here means that the model weights
    are partitioned across four four calls„ÄÇAnd„ÄÇAnd this is replicated„ÄÇSort of four
    times for the data parallel dimension„ÄÇOn the data side for model and data parallelism„ÄÇYeahÔºå
    the data here„ÄÇIs replicated across model parallel core and partitioned across
    data parallel core„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So next we have exp and data parallelism„ÄÇSo in that scenario that's kind of
    similar to data parallelism„ÄÇbut now each car will hold a different model weightÔºå
    which is illustrate by the different colors„ÄÇBy„ÄÇAnd for the data sideÔºå the data
    is simply replicated sorry„ÄÇthe data is positioneded across all callsÔºå just like
    in the data parallel scenario„ÄÇAnd so finally„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have the right most columnÔºå which is„ÄÇI guess yeah„ÄÇthat's the setup use in
    the switch transformer paper for the larger models„ÄÇAnd so„ÄÇhere for the model„ÄÇPartitioning
    each expert is partitioned across multiple calls„ÄÇso in that exampleÔºå we have four
    experts„ÄÇEach partition press four calls„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the data is replicated across multiple parallel calls and partitioned across
    data parallel calls„ÄÇSo that that's a little bit complex to understand orally„ÄÇbut
    the switch transformer paper has a nice the same figure with a nice caption to
    explain it„ÄÇAnd yeahÔºå maybe we can„ÄÇNo BarretÔºå we can add something quickly about
    how this is implemented in practice„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So„ÄÇThere's this paper called mesh TransformerÔºå which kind of extends„ÄÇbatchache
    or data parallel to more general purpose SPMD style programming„ÄÇAnd so different
    labs have different you know frameworks„ÄÇbut this paper kind of lays the foundation
    for general SPMD distributed computing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is required for training large scale models„ÄÇAnd so under the mesh abstraction„ÄÇbasically
    we have a mesh of processes„ÄÇch and so that mesh has dimensionsÔºå name dimensions„ÄÇand
    these name dimensions specify how the tensile dimensions will be partitioned or
    replicated across the mesh dimensions„ÄÇAnd so just that simple abstraction sort
    of supportsÔºå you know they parallelism„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: also model parallelism and especially export parallelism at once and so you
    know I invite whoever is interested to also check that paper that because that's
    kind of„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_30.png)
  prefs: []
  type: TYPE_NORMAL
- en: You knowÔºå that kind of lays the foundation for understanding these things„ÄÇAll
    right„ÄÇDar want to go cool yeahÔºå so next we are going to kind of talk about like
    how we take these parallelism strategies and like kind of combine them together
    to make like a 1„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 6 trillion parameter sparse model„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_32.png)
  prefs: []
  type: TYPE_NORMAL
- en: So next slide„ÄÇSo so yeahÔºå so what we ended up doing in this work was we had
    we trained two different very large spae models and we compared them to the largest
    T5 model so we can see the T5 XXL„ÄÇwhich is a dense model and it was the largest
    one trained in the T5 paper and it has around 13 billion parameters and here we
    list a lot of the model dimensions like D model DF which are just like you know
    the various sizes and shapes of the tensors and stuff„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the number of layersÔºå the number of heads and importantly we also mentioned
    the negative log perplexity„ÄÇAt step 250 k and at 500 k and so yeah so we designed
    two sparse models to test and to test like how scaling versus sparsity versus
    scaling versus sparsity and flops work so first let me talk about switch XXL so
    that has the same amount of flops per token as T5 xxL but has 64 experts and this
    leads it to have around 400 billion parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we can see that on a step basisÔºå it actually performs quite well and outperforms
    the T5 xXL by like quite a good margin„ÄÇInterestinglyÔºå thoughÔºå the third model
    we designed SwCÔºå which has 1„ÄÇ6 trillion parameters„ÄÇbut has significantly fewer
    flopsÔºå almost 10 less flops per token than either of the above two models„ÄÇso it's
    really trading by reducing flops that have way more sparse parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we can see on a step basisÔºå the switch C model works well„ÄÇbut not as well
    as actually the higher fl model„ÄÇBut on like a kind of a Pareto axis where were
    looking at TPU hours on the X axis and not step„ÄÇthe switch C model actually outperforms
    them both by like a pretty large margin„ÄÇSo for pre training performanceÔºå we're
    seeing that actually just like having a lot of sparsity and less swapps is actually
    can be quite good„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåNext slide„ÄÇYeahÔºå and so yeahÔºå this so againÔºå those two sparse models are kind
    of really trying to get at this hypothesis that actually Noam Shaer had„ÄÇwhich
    isÔºå you know„ÄÇThat you know parameters are good for more knowledge reasoning and
    compute aK flops is good for intelligence and so we're going to kind of try to
    get at that by taking these different sparse models and then fine tuning them
    on different tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: some of which require more like knowledge and then others which require more
    of like reasoning„ÄÇFor whatever like hand w definitionÔºå we want to give that„ÄÇSo
    yeahÔºå so for a fixed oh go back so yeah„ÄÇso for a fixed„ÄÇOh can you go back to the
    previous slide Oh sorry Okay„ÄÇso for a fixed quality on an upstream pre training
    taskÔºå yeahÔºå do parameters independently matterÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So we're going to look at two tasks hereÔºå one of which is super gluelu„ÄÇwhich
    is kind of our like reasoning task and then another is like trivia QA which is
    like some knowledge task where it's like you just give it a question and you have
    an output and answer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Okay„ÄÇAnd so here we're going to take a look at superglo quality so we can see
    on the X axis is the pre training performance and the Y axis is the superg score
    after fine tuning„ÄÇAnd interestinglyÔºå we can see definitely that the sparse models
    definitely are for a fixed pre training perplexity„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: do worse on fine tuning„ÄÇThis can be especially noticed at like the upper right
    portion of the plot where the dense models are definitely fine tuning better than
    their sparse counterpoint„ÄÇNext slide„ÄÇInterestinglyÔºå when we study it on the more
    knowledge heavy tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the sparse model for a fixed pretraining perplexity does disproportionately
    well„ÄÇso you know for a model that roughly has the same perplexity we're getting
    like really large boosts for these knowledge heavy tasks so this is pretty interesting
    and it also really you know shows some of the dangers of comparing only on your
    pretraining metric so these models you know can have the same exact pretraining
    metric but very different you know properties when fine tuning them on different
    tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Next slide„ÄÇAnd interestinglyÔºå so yeahÔºå all of the switch models here are just
    likeÔºå you know„ÄÇvarious models„ÄÇThat have still a good amount of flosÔºå but the red
    model is actually the 1„ÄÇ6 trillion parameter sparse model that hasÔºå you know very
    few flopsÔºå but a lot a lot of parameters„ÄÇAnd we can see that as the red dot hereÔºå
    and it does actually disproportionately bad compared to other sparse models that
    also have pretty good perplexities„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so yeahÔºå it's definitely very interesting and it shows that for models during
    pre traininging that have a lot of sparsity„ÄÇthey definitely suffer on some of
    these more reasoning heavy metrics„ÄÇbut do disproportionately well for more of
    these knowledge heavy tasks„ÄÇNext slide„ÄÇYeah„ÄÇand so here we can see it as just
    like a huge outlier for pre training perplexity doing like just incredibly well
    on this downstream question answering task„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_34.png)'
  prefs: []
  type: TYPE_IMG
- en: Next slide„ÄÇ![](img/cfb06f02d43700aca0402fcc8c4854ad_36.png)
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå okayÔºå so alsoÔºå you knowÔºå one thing that we were going to do is just look
    at the fine tuning properties of sparse models across like a few scales and just
    see how they perform„ÄÇNext slide„ÄÇYeahÔºå and so here we try two different models
    One is T5 base then we make a flat match sparse counterpoint and when they say
    flat match it's like you know each token will have the same amount of flops but
    now we just have experts so we do this for both base and large and we see that
    actually across almost all tasks besides two archrc tasks the sparse models perform
    quite well which is which is definitely promising so we are seeing that these
    models are pretty robust they pretrain well and then they also fine tune well
    when scaled appropriately by scaling up both the flops and sparsity whereas you
    know the negative results you've really seen are like yeah when you just have
    a huge amount of sparsity and not too many flops„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Next slide„ÄÇYeah and one also thing we wanted to look at was the multilingual
    training so we were previously studying all of this on like English only and we
    also wanted to see how Sprsity helps in the multilingual setting because you know
    we also felt like this would be a very natural place for Sprsity to work well
    where potentially experts could specialize across languages„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we do see strong resultsÔºå so on 91% of the languages„ÄÇI think of like around
    100 languages we see over like at least a4 exped over the MT5 dense model„ÄÇNext
    slide„ÄÇOr you want to go aheadÔºüNo go go ahead„ÄÇOkayÔºå Yeah„ÄÇso another thing we wanted
    to talk about was distillation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So one downside of these sparse models is that they'll have a lot more parametersÔºå
    which means that„ÄÇyou knowÔºå if you're serving these things or something you either
    need like high throughput use cases or you need to maybe distill it back down
    until like a smaller dense model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So here what we do is we look at like the T5 base and switch base and we look
    at its pretraining performance„ÄÇand then we go through some abllationations of
    different distillation techniques and find that like with the best techniques„ÄÇwe
    can keep around 30% of the quality improvements of sparsity while distilling it
    back down into its dense counterpart„ÄÇSo next slide„ÄÇYeahÔºå and then we kind of study
    this across multiple scales and again we see like around like 30% to 40% of the
    gains can be like you know kept when going from a dense model when going from
    you know a sparse model and distilling it back down until like its fl match dense
    model so you can get you know get rid of up to 99% of the parameters and still
    keep like around 30% of the improvements„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is very promising„ÄÇNext slideÔºå waitÔºå I'm sorry„ÄÇYeah„ÄÇAll rightÔºå sorry about
    that„ÄÇcan you say that last sentence againÔºå you said that you can keep the benefit
    30% of the teachers benefit„ÄÇüò°ÔºåYeahÔºå basically so yeahÔºå you you yeahÔºå exactly so
    yeahÔºå so we're looking at likeÔºå yeah„ÄÇyou train a sparse model and then you distill
    it back onto to a dense model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And versus training a dense model from scratch„ÄÇAnd like you look at the gap
    between a sparse and dense model from scratch versus the gap between the dense
    and then the distilled dense model„ÄÇWhat do you registeringÔºüYou go forward„ÄÇOh yesÔºå
    yeah„ÄÇOh yeah„ÄÇmaybe let me just do like a quick high level summary again„ÄÇ So yeah„ÄÇwe'
    we'll do is for our comparisons is we'll train a dense model from scratch„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We'll train a sparse model from scratch„ÄÇAnd then we'll also run a third experiment
    where we distill that sparse model down into a dense model„ÄÇWhat does distilling
    mean like we we're basically trying to match the like the teacher's logits„ÄÇLike
    the kind of standard thing of likeÔºå you knowÔºå like matching the like either the
    thelogits or like the soft probabilities for each token or something like that„ÄÇOkayÔºå
    if I can jump in with my questionÔºå so what I'm struggling with is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: how do I interpret the alignment that as percent of teacher and performanceÔºüYeahÔºå
    okay„ÄÇso it's basically looking at the like the gap between the dense and sparse
    model„ÄÇSo we'll have the dense model get some performance„ÄÇwe'll have the sparse
    model get some performance and then the dense model dis still from the sparse
    model would be somewhere in between that range and we're basically saying it's
    30% through that range„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it's like in like a 0 to1 interval it's like „ÄÇ3 of the way from the dense
    to the sparse model I see so this is not saying that the percent of teacher performance
    does not mean that if the teacher say if we use the teacher guesses or predictions
    as the ground truth„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this is not saying that the distilled model gets matches with the teacher 33%
    of the time„ÄÇNoÔºå no„ÄÇexactly„ÄÇIt's basically saying you get like 30% of the the quality
    improvements„ÄÇYeah„ÄÇexactly Okay cool And then if we can back up the slideÔºå I had
    a different question„ÄÇbut I didn't want to interrupt when we're talking about all
    of these different T5 bases and then also on a few slides before this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I don't know that much about T5Ôºå I'm curious you know when T5 is trained„ÄÇis
    there a weight penalty in the loss function„ÄÇIs there a way to catererÔºüNo„ÄÇthere
    is no way to cage trained with any of those sparse or dense models I see so out
    of curiosity then how do dense models perform compared to the switch model if
    you add some sort of weight regularization that incentivizes getting rid of useless
    weights„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So some kind of like maybe like L1 term or something like that quickly Yeah„ÄÇso
    I'm just wondering like how much of because here we're talking about the benefits
    of sparsity and I'm wondering how much of this benefit from sparsity is due to
    the fact that just some of this I mean„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: effectively what the switch model is doingÔºå if I understand correctly maybe
    I don't what understand is that the switch model in the feed forward layer„ÄÇit's
    just like you you fix some of the weight to be zero that's what it means to be
    spars„ÄÇWell„ÄÇactually we're kind of really trying to like inject more weights„ÄÇso
    we're actually kind of trying to do it's a little bit maybe like paradoxical because
    we're saying switch transformer but our idea to be like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: heyÔºå we actually want to just have significantly more weights„ÄÇNot last it's
    kind of like you would zero out weights but within a much larger weight matrix
    if that makes sense I see yes„ÄÇand so to me it seems like a relevant baseline to
    just ask what happens if I have the dense matrix but I incentivize it would say
    an L1 or L2 penalty on the weights and would I'd be curious to know how that compares„ÄÇYeahÔºå
    we didn't run thisÔºå but also that kind of gets rid of weights for the dense modelÔºå
    so if any„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So yeahÔºå yeah„ÄÇYeahÔºå and the last point is like if you just add like an L1 penalty
    loss„ÄÇYou're not going to have structural sparsity„ÄÇWas likeÔºå here weÔºå you know„ÄÇIt's
    not random weights in your giant weight matrix that are 0 thatÔºå right„ÄÇIt's like
    really like blocksÔºå depending like blocks corresponding to each expo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Right so that that structure allows the the whole like communication stuff and
    and that's„ÄÇYes that just the fact that you have multiple cousins on right„ÄÇSo I
    totally agree with that block structure and that's what I'm trying to say is that
    the switch has this very richs it's not just sparse„ÄÇIt also has this rich structure
    what I'm trying to do in my mind is disentangle is the sparsity What's offering
    an advantage or is this additional structure that you've built in is that what
    is the performance game So that's why I'm asking„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the the„ÄÇThe block structure is what„ÄÇEnables to leverage the fact that you
    have multiple calls but like if you if you didn't have that drug structure„ÄÇyou'd
    still have to route to everything and so you have more communication costs and
    so on so and then your first question was what sorry„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm not actually sure if there was a questionÔºå I guess what I'm trying to say
    is I'm trying toambiguate„ÄÇyeahÔºå anyways„ÄÇBut I agree„ÄÇit's a little bit weird because
    sparsity kind of's spectrum of meaningful for sparsity„ÄÇrightÔºå it likeÔºå for exampleÔºå
    compression and like model pruning is a form of sparsity„ÄÇbut also switch transformer
    and ME also referred to as sparsity and„ÄÇThat kind of related„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but definitely dynamic aiming at different things so this is a really interesting
    idea of it's sparse„ÄÇbut you have more parameters I'll have to think about it more
    thank you„ÄÇYeah„ÄÇlike it's kind of like spot within this like giant weight matrixÔºå
    which is exactly YeahÔºå YeahÔºå yeah„ÄÇI hadn't appreciated that„ÄÇ So I appreciate you
    you pointing that out„ÄÇ Thank you„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I have a lot of question on distillation part„ÄÇYeah first okay so if you disill
    it back down now you have like one technically you're back to the dense layer
    architecture right so now the entire like the entire idea of expert it's a certain
    tokens would be sent to different experts because they just like I don't know
    are more specialized in figuring something out about this token so now if you
    go back to this like dense layer like aren't you like basically only serving whatever
    like whichever expert you base this dense layer on like these tokens will probably
    perform well and all the other tokens are kind of like left behind rightÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Yeah„ÄÇI'm actually sorry„ÄÇI don't think I'm fully understanding your question„ÄÇSo
    so are you kind of getting at like we're just this on a specific data set so that
    don figure out how to use that like yeah yeah so maybe concretely like let's so
    like for super glue right like let's say you want to serve a model that does super
    glue well I think the idea is that like you distill the sparse model into a dense
    model on super gluelu So then you kind of get this compressed dense model that
    now performs better than if you were to just you know train it from scratch or
    train it from like a pretrained dense model So then it's like you use say that
    again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YouÔºå you have to pick one expertÔºå rightÔºü NoÔºå noÔºå no„ÄÇyou can just distill all
    of the because you're just matching the the model outputs„ÄÇSo you can just treat
    the sparse model as kind of like a black box thing„ÄÇAll we're doing is just trying
    to have the dense model match the actual like final likeÔºå you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: token predictions„ÄÇ Oh GodÔºå Okay about it„ÄÇ OkayÔºå sorry I was not„ÄÇI was not familiar
    with the idea of the disill„ÄÇSo I think that was like my entire confusion„ÄÇOkay„ÄÇYeahÔºå
    of courseÔºå yeahÔºå because I guess one motivation here is that„ÄÇHaving experts can
    make serving a little bit more difficultÔºå because„ÄÇIt requires bigger toppologies„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let's say you have eight exps„ÄÇYou need like„ÄÇWellÔºå I guess you can have multiple
    export on fewer calls„ÄÇbut„ÄÇYou knowÔºå let' just say that a little bit harder to
    solve„ÄÇAnd so if we canÔºå you know„ÄÇget the benefits from spay at per training„ÄÇAnd
    then use distillation to a dense model for serving be that can be beneficial„ÄÇso
    I think that was sort of the motivation for that experimentÔºå right Eric„ÄÇYeahÔºå
    exactly„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_38.png)'
  prefs: []
  type: TYPE_IMG
- en: OkayÔºå well are't we yeahÔºüYesÔºå kind go aheadÔºå BenÔºå I just said„ÄÇI think one more
    written kind questionÔºå so yeah„ÄÇSo yeah go ahead pleasure're asking Oh yeah yeah
    soundss good yeah you guys for the talks so far question was wondering if you
    think there are any interesting directions around building models that are like
    explicitly optimized for parallel training I guess like the moE models seems like
    you know it does a really good job here and also like at inference time it's you
    know very useful to like you know have fewer flops for computation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But or for forward passÔºå but I guess do you think that there are any interesting
    directions around distributed training where you might have like models that are
    explicitly architected to have a lot of parallel heads or other like features
    that are you know kind of embarrassingly parallelizable or does just using like
    standard you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: scale up the models by adding more layers and then just you know get away with
    using model and data parallelism work well enough„ÄÇYeahÔºå so I thinkÔºå so yeahÔºå so
    let me just make sure I'm fully understanding„ÄÇSo yeah„ÄÇI think also likeÔºå you knowÔºå
    right nowÔºå like even our models are definitely very co designed with the hardware
    and like the shapes and things„ÄÇyou knowÔºå so yeah I think at a high level like
    yes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think there's a ton of interesting research on like codesing the hardware„ÄÇthe
    partitioning algorithms and the models„ÄÇüòäÔºåI think given„ÄÇyou know that we have this
    kind of like SPMD mesh style partitioning„ÄÇwe are already kind of designing our
    models in ways that fit it really well„ÄÇ So for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: when we want to scale up our modelÔºå one of the first dimensions we go to scale
    up is the internal hidden dimension„ÄÇI there some really nice properties of scaling
    up this dimension„ÄÇIt basically becomes like kind of you know independent to some
    of the communication costs„ÄÇIt's really good when looking at the compute to memory
    operations on these you know like compute devices and stuff„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå exactly„ÄÇlike I think when we're even designing these models„ÄÇwe're like
    really setting dimensions such that it maps well out the hardware„ÄÇSo it's almost
    like„ÄÇyou knowÔºå given that we have this model data parallelism„ÄÇwe're like actually
    designing models more for it„ÄÇ But I also think that there's a ton of new interesting
    distributed algorithms and stuff like that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which makes designing models very interesting„ÄÇLike I think one thing that I
    think is really cool is like the Microsoft  zero partitioning too„ÄÇwhich also like
    adds some new new like nice implications for like how to design and scale models
    and stuff„ÄÇSo yeahÔºå I think there's likeÔºå this is a very fruitful research direction„ÄÇif
    that kind of answered to your question„ÄÇ YeahÔºå noÔºå that was super helpful and interesting„ÄÇ
    yeah„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Yeah definitely like i'm very optimistic on the future of us like designing
    the hardwareÔºå the model„ÄÇthe partitioning strategies altogether because really
    to get it to work well you kind of have to know about all three and like kind
    of you know intertwine the development of them Yeah yeah that sounds awesome„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: CoolÔºå yeahÔºå so just to summarize it's like yeah so switch transformer is like
    a nice simplification over a mixture of experts and we're seeing that we get really
    strong speed up improvements on pretrain over like a lot of the T5 models which
    are very strong baseline we're seeing that we can you know efficiently distill
    the sparse models back to dense ones„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and you know get improved both pretraining and fine tuning through some of these
    newer techniques we talked about„ÄÇand we're also seeing that the models are working
    on multilingual data and that we can you know now easily successfully train up
    to you know 1„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 6 trillion parameter modelsÔºå which is pretty promising„ÄÇAnd next slide„ÄÇand so
    we also wanted to go into two slides about some like newer work about actually
    using these kind of models for computer vision and actually also a little bit
    of how they can be used to actually do some level of like adaptive computation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: where not only now each input gets different weights„ÄÇbut also sometimes different
    inputs will have different amounts of compute applied to it„ÄÇAnd yeah so there's
    some really great work of doing this out of the Google Zuric team and yeah there's
    just doing it for image classification and you know they're basically seeing a
    lot of the similar types of scaling properties where you scaling up the number
    of experts and using sparsity allows them to get good performances on image classification„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Next slide„ÄÇAnd interestinglyÔºå one of the things they do is like as we talk the
    capacity factor„ÄÇso we were talking about values of like1 1„ÄÇ252„ÄÇ0Ôºå which means
    like at a value of 2„ÄÇ0 there's buffer for you know two tokens per expert„ÄÇbut they
    actually study it going less than one so that means that like at 0„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 5 that means there's only like room for half the number of tokens„ÄÇAnd the nice
    part is is that they did this for image classification and also in images there's
    just a lot of redundancy and they notice that you can actually get really good
    performance by only allowing like you know up to one10th of the the parts of the
    image to be processed by a sparse layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So yeahÔºå we think this is like a really nice direction too in terms of combining
    sparsity along with like adaptive computation„ÄÇüòäÔºåAnd yeahÔºå and yeahÔºå thanks so
    much for having us„ÄÇThat's theÔºå that's the talk„ÄÇIt does think you better than„ÄÇSorry
    other fun for coming here so you know„ÄÇÈÅì„ÄÇSo I will just like ask a bunch of questions
    and then we can have like after the class open question panel for the students„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So one thing is like have you tried using like like more like linear attention
    mechanisms like reformers and like all the stuff to like scale the computationÔºü
  prefs: []
  type: TYPE_NORMAL
- en: I personally haven't maybeÔºå I haven't personally done this„ÄÇYesÔºå so„ÄÇYou know„ÄÇI
    guess we can maybe comment on how„ÄÇYou knowÔºå the attention„ÄÇthe cost coming from
    the attention maps isn't„ÄÇThe dominant curve in these large transformers„ÄÇSo you
    knowÔºå the motivation for using Millar attention like perform is that it reduces
    the quadratic cost of attention map„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: rightÔºüÂóØ„ÄÇBut so farÔºå I meanÔºå at leastÔºå you knowÔºå in like sort of typical NLP
    setups like super gly far and so on„ÄÇAs you scale the modelsÔºå most of the memory
    comes from the model weights as opposed to attention to the attention maps„ÄÇThat's
    also becauseÔºå you knowÔºå using very long„ÄÇContext or sequence length„ÄÇDoes't improve
    that footfall and so you know just you know walking with a vanilla self attention
    mechanism is a very strong baseline already„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Goard itÔºå okay„ÄÇSo another question is like do you think this like mechanism
    is even more scalable„ÄÇlike can you go on and build like 10 trillion parametermeter
    models stuff like that like what do you thinkÔºü
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå definitely I think yeahÔºå totallyÔºå I think honestly one of the biggest
    constraints is that like you know„ÄÇand this isn't even necessarily a constrained
    it's just like you have to fit the parameter somewhere and there's just limited
    storage on devices„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but if you get enough devices such that you know yeah„ÄÇyou can just partition
    the weights it's like yeah I don't see anything stopping it„ÄÇGot it so what do
    you think like personally is your like like you thing like with the direction
    like like scaling of transformers will go into like will there be more like works
    that are trying to just like use such transformer like mechanisms make of experts
    or do you think thiss like you're going to be other things that the community
    needsÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Yeah I meanÔºå I definitely think mixture of experts should find its way or at
    least you know sparse layers like switch transformment stuff but will definitely
    I think find their way into like the future of large models„ÄÇI think they really
    confer a lot of benefits and they're also very good and like high throughput application
    So I think the one thing like so the one downside is on sparsity is like if you
    look at the performance per model weight they're going always be worse than dense
    models So it's like if you really are constrained on like I want to design the
    best model I can to fit on as small of a device as I can then they're probably
    not going to be the best solution because the sparse weights just aren't as good
    as just the dense weight that's being used for everything„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I think it really depends on the applicationÔºå but I'm very optimistic for
    when we're training these models during pretraining with lots of data parallelism
    and then we're serving them in like medium the higher throughput examples„ÄÇI feel
    like they could actually just be a pretty big win„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that's kind of my thoughts on how I think scarrscity will be used in terms
    of other things yeah I think I don't know there's a ton of exciting research you
    know from everything from yeah like a lot of the linear attention stuff adaptive
    computation new pretraining objectives you know yeah it's hard to know what the
    future will look like but yeah a lot of exciting things to look forward to great
    sounds Okay so we can now have like a round of student questions so is totally
    voting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_40.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/cfb06f02d43700aca0402fcc8c4854ad_41.png)'
  prefs: []
  type: TYPE_IMG
