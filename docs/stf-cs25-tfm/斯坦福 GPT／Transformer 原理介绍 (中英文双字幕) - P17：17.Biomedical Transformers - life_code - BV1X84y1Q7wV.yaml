- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P17：17.Biomedical Transformers - life_code
    - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P17：17.生物医学变压器 - life_code - BV1X84y1Q7wV
- en: '![](img/867d42af88a583402ff132cc4ae7c3fa_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/867d42af88a583402ff132cc4ae7c3fa_0.png)'
- en: '![](img/867d42af88a583402ff132cc4ae7c3fa_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/867d42af88a583402ff132cc4ae7c3fa_1.png)'
- en: Yeah， so making you all see the speaker notes was not part of the plan， but
    I'm glad to be here and。My name is Rebek Naro。And I am a research scientist in
    the Health I team at Google。A little bit more about me， growing up in India， my
    parents always wanted me to be a doctor。to be precise a medical doctor， but unfortunately
    I was probably not good enough to memorize all the biology textbooks that you
    had to do in case you wanted to crack the medical entrance examinations so I ended
    up becoming a computer scientist instead。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，让大家看到演讲者笔记并不是计划的一部分，但我很高兴来到这里。我的名字是**Rebek Naro**。我是一名在谷歌健康团队的研究科学家。关于我的更多信息是，成长在印度，我的父母一直希望我成为一名医生，准确地说是一名医学医生，但不幸的是，我可能没有足够的能力去记住所有的生物学教材，以便能通过医学入学考试，因此我最终成为了一名计算机科学家。
- en: 诶。But as a great man once said， you can't connect the dots looking forward。you
    only join them looking backwards， so through other long winded path。Not too dissimilar
    from how we actually train our neural networks I ended up working in medicine
    again this time armed with this magical new tool of AI and I can tell you that
    my parents are far more happy with my life choices right now。嗯。But then I were
    truly satisfied。But take questionsians aside。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 诶。不过，正如伟人曾经说过的，你无法向前看连接点。你只能在回顾时将它们连接起来，因此通过其他冗长的路径。这与我们实际训练神经网络的方式并没有太大不同。我再次回到了医学领域，这次我拥有了这个神奇的新工具——人工智能，我可以告诉你，我的父母对我现在的生活选择感到更加满意。嗯。不过，那时我并没有真正感到满足。但把问题放在一边。
- en: My goal for this talk is to peel back the curtains and give you a flavor of
    all the innovation that is happening at the intersection of AI and biomedicine
    and how that is being catalyzed by transformers and large language models in particular。So
    we will spend the first few minutes trying to work up from first principles why
    transformers and large language models are a particularly good fit for biomedical
    data。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我这次演讲的目标是揭示幕后的情况，让你们了解在**人工智能**和**生物医学**交汇处发生的所有创新，以及这些创新是如何受到**变换器**和**大型语言模型**的推动。因此，我们将花几分钟时间从基本原理出发，探讨为什么变换器和大型语言模型特别适合生物医学数据。
- en: and then we will deep dive into a few papers covering a bunch of different biomedical
    application settings。And finally， I'll present my views on how this field is likely
    going to evolve in the next few years。And even though my voice or tone may not
    exactly sound that way。I am incredibly excited by the possibilities of AI and
    biomedicine。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将深入探讨几篇涵盖不同生物医学应用场景的论文。最后，我将分享我对这个领域在未来几年可能发展的看法。尽管我的声音或语调可能并不完全如此，但我对人工智能和生物医学的可能性感到无比兴奋。
- en: And I think we have an incredible opportunity in front of us to advance human
    health and human potential and my hope at the end of this talk is you all will
    feel the same way as I do today and perhaps join me。😊，So yeah， let's jump straight
    in why transformers in biomedicine and sorry I'm going to pick people who are
    in person to answer。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们面前有一个令人难以置信的机会，可以推动人类健康和人类潜力，我希望在这次演讲结束时，你们都能像我今天一样感受到这一点，或许愿意加入我。😊，所以，让我们直接深入讨论为什么变压器在生物医学中的应用，对不起，我会请在场的人回答。
- en: so maybe if one of you could volunteer。Go for， not that's a good answer。嗯。嗯。s飞啊后有去快玩到未
    sorry sorry快玩到几个打咯佢翻系 l。Yeah， sure，'s another one as well that's an important
    application setting。你系接十我事啊。Yeah great one so I think all of you were on the right
    track and so maybe if you just look at different kinds of you know biomedical
    data。for example what are clinical notes I think it's sequence of Dr Jabberish
    okay I did not say that but let's just call sequence of doctor speak or doctor
    notes。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，也许你们中的一个可以自愿一下。去吧，虽然这不是一个好答案。嗯。嗯。s飞啊后有去快玩到未，抱歉，抱歉快玩到几个打咯佢翻系l。是的，当然，还有另一个也是一个重要的应用场景。你系接十我事啊。是的，很好，所以我认为你们都在正确的轨道上，也许如果你们看一下不同类型的生物医学数据。比如，临床笔记是什么，我想那是医生的胡言乱语的序列，好吧，我没有这么说，但我们就称之为医生说话的序列或者医生笔记。
- en: Similarly， if you were to look at electronic medical records。what are they they
    are essentially sequence of like a person's encounters with the medical system。嗯。What
    about proteins going deeper into the biological stack。they are nottdding but a
    sequence of amino acids linked together by peptide bonds。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你查看电子病历，它们本质上是一个人就医经历的序列。嗯。关于蛋白质，深入生物层面，它们不过是由肽键连接在一起的氨基酸序列。
- en: And does anybody know what this is？好多水。I think that's how we store。嗯系。Sorry
    again。you're getting close。系。Anyone else？So this is in the Wecome collection in
    London and this is actually a printout of the full human genome。And。No they did
    not cheat over here the font is super small and as you can see there's a bunch
    of8GCs the entire printout contains I think over 130 volumes in that shelf and
    each page is printed on both sides and it's a  four point font with precisely
    43。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有人知道这是什么吗？好多水。我想这就是我们的储存方式。嗯，没错。再一次抱歉。你快到了。没错。还有其他人吗？这是伦敦的威尔康收藏，这实际上是完整人类基因组的打印件。并且。他们并没有作弊，字体非常小，如你所见，那里有很多8GCs，整个打印件我想有超过130卷放在那个架子上，每一页都是双面打印，使用的是四号字体，精确到43。
- en: 000 cactus per page so that is how big the human reference genome is more than
    billions of base pair。And so again， the genome is nothing but a sequence of neotide
    base pairs。so what we are essentially seeing over here is sequences are everywhere
    in biomedical data and what is the best neuralelectric architecture for modeling
    them？
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每页有000个仙人掌，这就是人类参考基因组的大小，超过数十亿个碱基对。再说一次，基因组只是一串核苷酸碱基对的序列。因此，我们在这里看到的本质上是生物医学数据中的序列无处不在，而什么是建模它们的最佳神经电气架构呢？
- en: 嗯。And I guess since you are all in this course， I don't have to convince you
    that the answer is transformers。right？Okay that's good， but maybe I'll just offer
    a few reasons over here firstly as you can see the data itself is multimodal in
    nature and we just saw a few examples and as someone pointed out transformers
    have proven remarkable at you know guing up pretty much any kind of data and we
    are really seeing this remarkable convergence across fields whether that's speech
    or NLP or vision or robotics I mean pretty much everywhere we are using transformers
    and I think biomedicine is no different。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。我想既然你们都在这个课程中，我就不需要说服你们答案是变换器，对吧？好的，这很好，但也许我在这里提供几个理由。首先，正如你们所看到的数据本质上是多模态的，我们刚刚看到了几个例子，正如有人指出的那样，变换器在处理几乎任何类型的数据方面表现出色。我们确实看到了各个领域的显著融合，无论是语音、自然语言处理、视觉还是机器人，变换器几乎无处不在，我认为生物医学也不例外。
- en: 😊，I think secondly， transformers are far more effective at modeling complex
    long range interactions over sequences。and this property is particularly important
    in the biomedical domain and we will cover this in more detail later in the talk。And
    finally， as again， someone pointed out， these data sets can be quite big and you
    can easily get into the billions of token territory and this is where transformers
    with all the parallelizable operations and the relative ease of training and maybe
    someone should try training an LSM and an RN on these kind of datas。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我认为第二，变压器在对序列中的复杂长程交互建模方面远远更有效。这个特性在生物医学领域尤其重要，我们将在稍后的演讲中详细讨论这一点。最后，正如有人指出的，这些数据集可能非常庞大，你很容易就会进入数十亿个标记的领域，而这正是变压器凭借其所有可并行化操作和相对易于训练的优势所擅长的地方，也许应该尝试在这些数据上训练一个LSM和一个RN。
- en: you'll realize that these are much better suited for the kind of data sets that
    we have in this domain over here。So yeah， I think there are a few more reasons
    as well。but I think these are the key ones as to why transformers are particularly
    well suited for biomedical data sets and tasks。Any questions so far？Okay， great，
    so now in the next part of this talk。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你会意识到这些更适合我们在这个领域拥有的数据集。所以是的，我认为还有几个原因，但我觉得这些是变压器特别适合生物医学数据集和任务的关键原因。到目前为止有任何问题吗？好的，很好，那么接下来在这个讲座的下一部分。
- en: we will dive deep into a few papers applying transformers to biomedical data。We'll
    start with clinical applications first and then go gradually deeper into the biology
    stack looking at proteins and genomic applications as well。and what you will observe
    is that while transformers and large language models by extension are a great
    fit。often you have to innovate， not just on the modeling side。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨几篇将变换器应用于生物医学数据的论文。我们首先从临床应用开始，然后逐步深入生物学层面，查看蛋白质和基因组应用。你会发现，虽然变换器及其扩展的大型语言模型非常契合，但往往需要在建模方面进行创新。
- en: but also on the data and evaluation side to make these applicant scenarios really
    work。And so the first paper I want to talk about over here is this recent work
    from our team called largege La modelss encode clinical Knowled。The motivation
    for this work is actually quite straightforward so if you look at medicine it
    is a humane endeavor and language is at the heart of it facilitating interactions
    between people and those who provide care for them unfortunately if you look at
    a lot of medical AI systems developed till date these are all narrow single task
    single domain models lacking interactive and expressibility capabilities。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但也在数据和评估方面，以使这些申请场景真正有效。因此，我想在这里讨论的第一篇论文是我们团队最近的工作，名为大型语言模型对临床知识的编码。这个工作的动机其实非常简单，如果你观察医学，它是一项人道事业，而语言正是其核心，促进了人们与提供护理者之间的互动。不幸的是，如果你看看迄今为止开发的许多医疗AI系统，这些系统都是狭窄的单一任务单一领域模型，缺乏互动性和表达能力。
- en: And as a result， what has happened is there is this discordance between what
    these models can do and what is expected of them by patients and you know care
    providers and others and this in turn has I think prevented broad uptake of medical
    AI。诶。And you can see that， for example， we don't really have AI and many clinics
    out there like helping us with diagnosis and so on and so forth。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，这些模型能够做到的与患者、护理提供者等对它们的期望之间存在不一致，这反过来我认为阻碍了医疗AI的广泛应用。诶。比如，我们实际上在许多诊所中并没有AI来帮助我们进行诊断等等。
- en: But the recent progress with Transform based large language models。it offers
    us an opportunity to change all of this and redesign and rethink medical AI systems
    with language at the heart of it。mediating humania interactions between doctors，
    researchers and patients。And I will be amazed if I don't point out that there
    has been a large volume of work in this space。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，基于变换的大型语言模型的最新进展为我们提供了一个机会，可以改变这一切，重新设计和思考以语言为核心的医疗人工智能系统，调解医生、研究人员和患者之间的互动。如果我不提到在这个领域已经有大量的工作，我将感到惊讶。
- en: particularly in the last few years， there have been various attempts to train
    language models in the biomedical domain with models of various different sizes
    on different cor of biomedical data。And while this is exciting， the quality bar
    for applications in the medical domain is actually quite high。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在过去的几年中，针对生物医学领域进行了各种尝试，以不同规模的模型在不同的生物医学数据集上训练语言模型。虽然这很令人兴奋，但在医疗领域的应用质量标准实际上相当高。
- en: And so what is missing is actually is that there is actually not many good evaluation
    benchmarks and evaluation protocols and frameworks so we don't have the equivalent
    of a big bench in medicine and hopefully you guys have covered big bench before
    and so big bench is is benchmark where you can assess large language models across
    a variety of task domains and settings but we don't have an equivalent of that
    in the medical domain。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，缺失的实际上是，我们并没有很多好的评估基准、评估协议和框架，因此在医学领域我们没有相当于大基准的东西。希望你们之前已经了解过大基准，而大基准是一个可以评估大型语言模型在各种任务领域和设置中的基准，但在医学领域我们没有这样的等效物。
- en: And further， if you look at the evaluations that are typically used in these
    previous studies。they only look at objective metrics like accuracy or natural
    language generation metrics like blue or cider。but these failed to capture the
    nuances of real world use cases in clinical settings。So what we essentially need
    was a good benchmark and a task and also a good evaluation framework for evaluating
    these models。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你查看以往研究中通常使用的评估，它们仅关注诸如准确性或自然语言生成指标（如blue或cider）等客观指标，但这些未能捕捉到临床环境中真实使用案例的细微差别。因此，我们本质上需要一个好的基准、一个任务，以及一个良好的评估框架来评估这些模型。
- en: And so to address this unmet need and assess the potential of LLMs in medicine，
    in our team。we decided to focus on the medical question answering task， why。because
    answering medical questions is actually quite challenging。it requires reading
    comprehension skills， ability to accurately recall medical knowledge and also
    manipulate and reason about it。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了满足这一未被满足的需求并评估大语言模型（LLMs）在医学中的潜力，我们团队决定专注于医疗问答任务，为什么呢？因为回答医学问题实际上是相当具有挑战性的。它需要阅读理解能力、准确回忆医学知识的能力，以及对这些知识进行操作和推理的能力。
- en: And furthermore， the Q&A task is general enough and can subsume a bunch of different
    application settings such as summarization of clinical notes。clinical decision
    support， and also like primary cartriaging of patient concerns and so on。So we've
    identified the task the next question is what data set and so when we looked at
    the literature over here。what we saw is that there were several data sets floating
    around assessing model capabilities in a bunch of different settings so what we
    decided was we should probably just unify all of them and put together in one
    benchmark and so we did that and we call it multim QA and so if you look at it
    this benchmark now covers medical question answering data sets from a bunch of
    different settings such as professional medical questions like the US medical
    license exam style questions it also includes medical research questions those
    based on pububM abstracts and so on and also questions from live users and consumers
    asking about medical information。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，问答任务足够通用，可以涵盖多种不同的应用场景，例如临床笔记的总结、临床决策支持，以及初步分类患者关切等。因此，我们确定了这个任务，接下来的问题是数据集。当我们查看相关文献时，我们发现有几个数据集在不同的场景中评估模型能力，因此我们决定将它们统一起来，整理成一个基准。因此，我们这样做了，我们称之为多模态问答（multim
    QA）。如果你查看这个基准，现在涵盖了来自多种不同场景的医学问答数据集，例如专业医学问题（如美国医学执照考试风格的问题），还包括基于PubMed摘要的医学研究问题，以及来自实时用户和消费者询问医学信息的问题。
- en: And also the setting changes， it could be closed domain or open domain and the
    model may be expected to produce long form answer in one setting and maybe a short
    form answer in another setting。And finally， we saw that while the。The Q&A data
    sets which covered consumer course， yeah go。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 还有环境的变化，可能是封闭域或开放域，模型可能在一个环境中被期望产生长格式的答案，而在另一个环境中则可能产生短格式的答案。最后，我们看到尽管如此。这些涵盖消费者课程的问答数据集，嗯，继续。
- en: I'll come back to this。So yeah very quickly finally when we looked at cool so
    the question was how do we evaluate longform answers and I'll come back to this
    sub later and so very quickly when we looked at the data sets that actually provided
    consumer medical questions we found them to be quite small in size so we decided
    to augment them and so we went out to Google and looked at like the most frequently
    asked consumer medical questions and so we curated a data set and we added that
    to the benchmark and we call that health search QA over here。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍后会再回到这个话题。所以，最后我们快速地看了下，问题是我们如何评估长格式的答案，我稍后会再回到这个子话题。因此，当我们查看实际上提供消费者医疗问题的数据集时，我们发现它们的规模相当小，所以我们决定扩充这些数据集。于是我们在谷歌上查看了最常被问的消费者医疗问题，并整理了一个数据集，将其添加到基准中，我们称之为健康搜索问答。
- en: And so yeah， again，I'll come back with the start circular I'm sure so here are
    a few examples so if you look at the consumer medical questions they are quite
    short in nature while and so they come from the health search queue and the liveQ
    datas whereas I think if you look at the USM style questions these are like long
    viignettes and so doctors have to like really really carefully read through them
    and come up with the right answer which often and involves a process of emination
    so again very very different application settings and so the model has to really
    like really adapt and understand the task to do well in all these settings across
    the board。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，是的，我会再回来，确保能找到开始的循环，这里有一些例子。如果你看看消费者医疗问题，它们的性质相对较短，这些问题来自健康搜索队列和liveQ数据，而如果你看美国医学风格的问题，它们就像长篇小插曲，医生必须非常仔细地阅读这些内容并找出正确答案，这通常涉及到一个排除的过程。所以，应用场景非常不同，模型必须真正适应并理解任务，以便在所有这些环境中表现良好。
- en: And live QA is interesting because the answers， the reference answers over here
    were actually provided by librarians。so that's another good comparison point for
    us going ahead。And so in terms of statistics。we had a total of seven data sets
    in this benchmark， as I said， we cover professional medicine。medical research
    and consumer medical questions there again of various different sizes and can
    be long form short form open domain and flow domain so very diverse and I think
    it provides a very comprehensive evaluation of models in this medicaling setting。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 实时问答很有趣，因为这里的参考答案实际上是由图书馆员提供的。所以这为我们未来的比较提供了另一个好的切入点。在统计方面，我们在这个基准测试中总共有七个数据集。正如我所说，我们涵盖了专业医学、医学研究和消费者医学问题，这些问题各不相同，既有长格式也有短格式，涉及开放领域和流域，因此非常多样化。我认为这为在医学领域中对模型的综合评估提供了非常全面的依据。
- en: So we have a task on the benchmark the next question again I think asked was
    how do we evaluate these models and as I mentioned before automated metrics are
    actually deeply unsatisfactory because they fail to capture the nuances of real
    world clinical applications so what we did was actually heavily inspired by some
    of Steven's work over here was to put together a human evaluation framework for
    assessing these longform answerss and。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在基准上有一个任务，接下来的问题我认为是问我们如何评估这些模型，正如我之前提到的，自动化指标实际上是非常不令人满意的，因为它们未能捕捉到真实世界临床应用的细微差别。因此，我们所做的实际上是深受斯蒂文的一些工作的启发，构建了一个人类评估框架来评估这些长篇回答。
- en: This had two parts， the first part was evaluation by clinicians and we asked
    them to rate the moral responses along 12 axes pertaining to factuality of the
    responses。ability to recall medical knowledge to medical reasoning。and also for
    the potential of harm and bias in these responses。But if you look at like the
    potential end users of such medical Q&A systems these are likely going to be non-
    expertpert layer users。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这分为两个部分，第一部分是临床医生的评估，我们让他们根据与回答的真实性相关的12个维度对道德反应进行评分。包括回忆医学知识的能力、医学推理，以及这些反应中潜在的伤害和偏见。但如果你看一下这些医学问答系统的潜在最终用户，他们很可能是非专家层的用户。
- en: so it is also important to get these answers evaluated by them as well and so
    we also additionally asked a pool of lay users as to how helpful and actionable
    they thought the answers were。And so that was our evaluation framework and we
    also have the benchmark fix。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让他们评估这些答案也很重要，因此我们还额外询问了一组普通用户，看看他们认为这些答案有多有帮助和可操作。因此，这就是我们的评估框架，我们也有基准固定。
- en: so now we've moved a fun part of building and aligning LLM to the medical domain
    task。So in this work we decide to build on the Palm family of language models
    has that been covered in the coast before。okay great， so but very quickly， I believe
    this is still the largest publicly announced densely activated decodrenly large
    language model with the largest one being fine 40 billion parameters in total。A
    few more details， the model strain on SA 40 billion tokens， 25% of which is multilingual。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以现在我们已经将构建和对齐LLM的有趣部分转向医疗领域的任务。我们决定基于之前提到的Palm系列语言模型进行工作。好的，很棒，但我相信这仍然是公开宣布的激活密集的解码大型语言模型中规模最大的一个，最大的模型总共有400亿个参数。还有更多细节，该模型在400亿个令牌上进行训练，其中25%是多语言的。
- en: the data come from a bunch of different sources， including social media conversations，
    web pages。books， GitHub， and Wikipedia and so on and so forth。And at that time
    of release。the model was shared the art on many NLP reasoning benchmarks and also
    was the first model to exceed the average human performance on Big Bennch。Further
    over the last year， palm derived models were shown to be super useful in a bunch
    of different application settings。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来自多个不同的来源，包括社交媒体对话、网页、书籍、GitHub 和维基百科等。在发布时，该模型在许多 NLP 推理基准测试中表现出色，并且是第一个超越人类平均表现的模型。进一步来说，在过去一年中，源于
    Palm 的模型在多个应用场景中被证明非常有用。
- en: including for cogen， which was the palmM code model and robotics。the palm Saan
    model and also for answering math and science questions， which was the Minva models。and
    so we thought PAM was a very good foundation model for us to build on and use
    it in the medical domain as well。And overall， I think Palmm is a true magic of
    engineering。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 包括用于cogen的内容，这就是palmM代码模型和机器人技术。palm Saan模型，以及用于回答数学和科学问题的Minva模型。因此，我们认为PAM是一个非常好的基础模型，可以在医疗领域进行构建和使用。总的来说，我认为Palmm是真正的工程奇迹。
- en: but I will refer you all back to Acsha's paper on this for more details I think
    it's a must read。And again in late October last year， Jason Way and a few others
    at Google Bra came out with the F palm variant of these of the palmM model and
    this is basically the instruction tune counterpart and this model was even better
    than PM and I believe it is still the sort of sort of yard on many benchmarks
    such as MMLu。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但我会把你们都引导回 Acsha 的论文，了解更多细节，我认为这是一篇必读之作。再说一次，去年十月底，Jason Way 和谷歌的其他几位同事推出了这些
    palmM 模型的 F palm 变体，基本上是指令调优的对应模型，这个模型甚至比 PM 更优秀，我相信它在许多基准测试中仍然是一个标杆，比如 MMLu。
- en: TiDQA and I think it exceeds palm performance by an average of 9。4% across big
    bench tasks。So。So we decided to build on the F palm model and we applied a combination
    of prompting strategies。including few short prompting， chain of thought reasoning
    and also cell consistency to the54 billion parameter variant。and we evaluated
    it on the multimQA data sets that had these short form MCQ questions。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: TiDQA和我认为它在大型基准任务中平均超过了Palm性能9.4%。所以，我们决定基于F Palm模型进行构建，并应用了多种提示策略，包括短提示、思维链推理以及单元一致性到54亿参数的变体上。我们在包含这些短形式MCQ问题的multimQA数据集上进行了评估。
- en: And we found that this model was really really good at the time of publicationation。this
    model on the USM Me data set exceeded the previous state of the art by over 17%。It's
    only for the USMLE Me area that's right and so you see that the accuracy over
    the previous year of Y at the time of publication went up by over 17% and I believe
    this was the first LLM basedDI system to obtain like a passing equivalent score
    which was 60% or above on this benchmark。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现这个模型在发布时确实表现得非常出色。这个模型在USM Me数据集上的表现超过了之前的最先进水平超过17%。它仅适用于USMLE Me领域，因此你会看到在发布时，相较于前一年的Y，其准确率提升了超过17%，我相信这是第一个基于LLM的DI系统在该基准上获得60%或以上及格相当分数的系统。
- en: And similarly， when we looked at other MCQ data sets in the benchmark， for example，
    MCQA。which is a data set of Indian medical entrance examination questions。the
    model was again the state of the art on PubMC which was question answering based
    on Pubmed abstracts。that was again the model was state of the art at the time
    of publication and same story on MMLU clinical topics as well which include genetics。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，当我们查看基准中的其他多项选择题数据集时，例如，MCQA。它是一个印度医学入学考试问题的数据集。该模型在基于Pubmed摘要的问答系统PubMC上再次达到了最先进水平。那时，该模型在发布时也是最先进的，MMLU临床主题的情况也是如此，其中包括遗传学。
- en: anatomy professional medicine， clinical knowledge and a bunch of other topics
    in there。So。All this was great and then when we started looking at the scaling
    plots。what we saw was that the performance seemed to be improving as we scaled
    the model from 8 billion to 62 billion to 5 and40 billion。And so what this basically
    suggested that these general purpose large language models trained on public internet
    seem to encode clinical knowledge pretty well and their medical reasoning abilities
    tend to scale with model parameter size。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 解剖学专业医学、临床知识以及其他一些主题都在其中。所以。所有这些都很好，然后当我们开始查看缩放图时。我们看到的是，随着我们将模型从80亿扩展到620亿，再到540亿，性能似乎在改善。因此，这基本上表明，这些在公共互联网训练的通用大型语言模型似乎很好地编码了临床知识，并且它们的医学推理能力往往随着模型参数的增大而提升。
- en: We also did another experiment when we looked at selective prediction and we
    used the self-consency votes to determine when to differ。and this is important
    in clinical settings because doctors communicate when they don't know about something
    and if our AI systems are going to be used in clinical settings for example for
    diagnosis。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还进行了另一个实验，研究选择性预测，并使用自我一致性投票来确定何时出现差异。这在临床环境中非常重要，因为医生在不确定某件事时会进行沟通，如果我们的AI系统要在临床环境中使用，例如用于诊断。
- en: they should be able to tell you when they don't know something。And so what we
    observed here was this fairly crude metric we were getting like a linear improvement
    in performance as we changed the defral threshold and this was quite nice but
    in practice it is actually quite ineffient because you are generating like you
    know multiple decoding samples to be able to compute this metric so we need a
    better method ask yeah it's basically says I'm uncertain around one and that's
    determined based on the cell consistency works so exactly same exactly。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 他们应该能够告诉你什么时候他们不知道某件事情。因此，我们在这里观察到的是一个相当粗糙的指标，随着我们改变默认阈值，性能有线性改善，这很好，但实际上效率相当低，因为你生成多个解码样本来计算这个指标，所以我们需要一种更好的方法。是的，这基本上是说我对一个周围感到不确定，而这取决于单元一致性工作，所以完全相同。
- en: So theNo because they're just trained on this expert prediction task and that
    depends on the data the PubM QA has some answers which are maybe but again we
    don't explicitly find tune in the models over here so no the models are not trained。Yeah，
    so that is where we have this medical runs in the long zero lot。No。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以不，因为它们仅仅是在这个专家预测任务上进行训练，这依赖于数据，PubM QA有一些可能的答案，但我们并没有在这里明确地对模型进行微调，所以不，模型并没有经过训练。是的，这就是我们在这个医疗领域长时间运行的原因。没有。
- en: so this is primarily based on the reference of in the data sets which is so
    this is all accuracy matrix so we already know between the four options of I options
    which one's the right one and so we just do the classification yeah so I'll come
    back to the condition evaluation aitrator。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这主要是基于数据集中的参考，这就是所有的准确性矩阵，所以我们已经知道四个选项中哪一个是正确的，因此我们只需进行分类。是的，我会回到条件评估的ai评估器。
- en: So maybe is something how are you naturally uncertainty so if you know what
    selfconency prompting what we do is we generate multiple decos from the same model
    and then we see the number of times the highest ranking answer is voted and based
    on that you can fix a threshold and say if it's below this number I'm going to
    differ so if say the majority answer comes up in your selfconsency decode only
    like n times out of k or whatever then that if that n is too small then it's very
    likely the model's uncertain so that's how we differ。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 所以也许是关于你自然不确定性的某种情况，因此如果你知道自洽提示是什么，我们所做的是从同一个模型生成多个解码，然后查看最高排名的答案被投票的次数，根据这个你可以设定一个阈值，若低于这个数字我就会选择不同的答案。所以如果在你的自洽解码中，大多数答案仅出现了
    n 次，而总共有 k 次投票，若这个 n 太小，那么模型很可能是不确定的，这就是我们选择不同答案的方式。
- en: 嗯。嗯。So we don't really see like a paper off in this plot。so like it's actually
    that that what the rest would look like。I think if you plot it further to flatline，
    but again that's not useful I mean if you're saying no to every question that's
    not useful at also you want to have a reasonable deferral percentage of I think
    that's high I think that's still high 50% is quite high but again that this is
    a very contrived setting but in real world use cases is probably I think that
    number should be much lower。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。嗯。所以我们在这个图中并没有看到像纸一样的东西。实际上，其余部分会看起来像这样。我认为如果你继续绘制到平坦线，但这又没有用，我是说，如果你对每个问题都说不，那也没有用。此外，你希望有一个合理的拒绝比例，我觉得那很高，我认为仍然很高，50%是相当高的，但这仍然是一个非常人为的设定，而在现实世界的使用案例中，我认为这个数字应该低得多。
- en: might and the data addicts， some medical products went to more important than
    others。so more question。嗯。That's right I think balanced accuracy might be a better
    metric but we looked at some of these data sets and one data set the s was pretty
    bad the Pub data set and I think no one should use it so if anyone's reporting
    sort of numbers on that data you should just discussrus them and'm talking about
    very specific people but again I think as I mentioned these accuracy metrics are
    good for you know publicity and pushing of benchmark numbers and so on and so
    forth but the real evaluation is human evaluation of the long performances and
    that's what I'll come to in the next one。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 影响和数据上瘾者，一些医疗产品比其他产品更重要。所以还有更多问题。嗯。没错，我认为平衡准确率可能是更好的指标，但我们查看了一些数据集，其中一个数据集的结果相当糟糕，即Pub数据集，我认为没人应该使用它。所以如果有人在报告该数据上的数字，你应该直接讨论这些问题。我说的是非常具体的人，但再次强调，正如我提到的，这些准确性指标在宣传和推动基准数字方面很好，但真正的评估是对长期表现的人为评估，这就是我将在下一个中讨论的内容。
- en: 嗯。So so far so good right I mean we we were getting solar results on these benchmarks
    and we were very happy and so what we did was I mean one thing you'll observe
    that I have so far only reported results on multiple choice questions shortformances
    so what was left for us to do was to take these answers take these models and
    generate longform answerss to the other data sets that we had and get them human
    evaluated and I think that is where the real project began when we looked at the
    evolvevals by experts on lay people it revealed very like key gaps and limitations
    in the fl farm responses。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。那么到目前为止还不错，对吧？我的意思是，我们在这些基准测试中得到了太阳能的结果，我们非常高兴。因此我们所做的事情是，我的意思是，你会注意到到目前为止我只报告了关于选择题的结果，所以我们剩下要做的就是将这些答案、这些模型生成长答案，以便于我们拥有的其他数据集，并进行人工评估。我认为这才是真正项目开始的地方，当我们查看专家对普通人的评估时，它揭示了在响应中存在的关键差距和局限性。
- en: we were often seeing that these models were hallucinating or producing incomp
    responses and when we asked experts like whether they preferred clinician generated
    answers or these model generated answerss they almost always preferred clinician
    generated answerss。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们常常发现这些模型会产生幻觉或生成不完整的回应，当我们询问专家时，他们是否更喜欢临床医生生成的答案还是这些模型生成的答案时，他们几乎总是更喜欢临床医生生成的答案。
- en: 嗯。So it was very clear that they both。And so what these previous results showed
    was while these models already encode some degree of clinical knowledge to be
    really used in actual real settings。you need to align these models better to the
    safety critical requirements of the medical domain。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。所以很明显，他们两者都。之前的结果显示，这些模型虽然已经编码了一定程度的临床知识，可以在实际环境中使用，但你需要更好地将这些模型与医疗领域的安全关键要求对齐。
- en: But a big challenge is we did not have any kind of supervised our feedback data。and
    so we really need the alignment technique to be data efficient。But thankfully。we
    had instruction from tuning， which was introduced by Brian Lester and a few others
    at Google a couple of years back。and how this method works is it essentially freezes
    the big LLM model and only learns an additional small set of prompt vectors which
    can then be used to condition the model that inference when doing the generation。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但一个大挑战是我们没有任何形式的监督反馈数据。因此，我们真的需要对齐技术以提高数据效率。不过，值得庆幸的是，我们得到了调优的指导，这一方法是由布莱恩·莱斯特和谷歌的几位其他人几年前提出的。这种方法的工作原理基本上是冻结大型
    LLM 模型，只学习一小部分额外的提示向量，这些向量可以在生成时用于对模型进行条件化。
- en: And the nice thing about this is it allows very easy use of the model across
    tasks and domains。and you only need to like carry around these additional prompt
    parameters right and these tend to be much smaller than like the the billions
    of parameters that you have in the other。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 而这最好的地方在于，它允许在不同任务和领域中非常容易地使用模型。而且你只需要携带这些额外的提示参数，这些通常比其他模型中数十亿的参数要小得多。
- en: And the other good thing is this is very computationally efficient as well。so
    like if you were to do end to end fine tuning often in our compute infrastructure。even
    with like a few thousand examples that would take like a few days whereas with
    instruction prompt tuning given the data set size is also reduced the number of
    examples that you need is quite small and just updating the prompt token vectors
    it meant that we were able to get model updates in like a few hours and so that
    was like really fast and enabled like really quick iterations for us。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个好处是，这种方法在计算上也非常高效。所以，如果你在我们的计算基础设施上进行端到端的微调，即使有几千个例子，这也可能需要几天的时间，而通过指令提示微调，由于数据集的大小减少了，你所需的例子数量非常少，只需更新提示令牌向量，这意味着我们能够在几小时内获得模型更新，这样真的很快，使我们能够进行非常快速的迭代。
- en: So this was how we put together the final metPm model we so this was how we
    put together the final metPm model so we used instructions and exemplars from
    a panel of expert clinicians and these are in the order of hundreds not like thousands
    of tens of thousands and you see a few examples over there there's an instruction
    followed by model answer followed by an explanation and we use that to learn the
    prompt vectors and so the final metPm model is basically of plan pump plus these
    additional softpro vector parameters which are used to align the model to the
    requirements of the medical domain and why this works well is because as we have
    seen before the model already has medical knowledge encoded in it all we need
    is to like least the model how to use it properly in the given application setting
    and that's what these prompt parameters to for us。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们如何整合最终的metPm模型的过程，我们使用了来自专家临床医生小组的指示和示例，这些指示和示例数量是几百，而不是几千或几万，你可以看到那里的几个示例，有一个指示，后面是模型答案，再后面是解释，我们用这些来学习提示向量。因此，最终的metPm模型基本上是计划泵加上这些额外的softpro向量参数，这些参数用于将模型与医疗领域的要求对齐。之所以这样做效果很好，是因为正如我们之前看到的，模型中已经编码了医学知识，我们所需要的只是教会模型如何在给定的应用环境中正确使用这些知识，而这正是这些提示参数的作用。
- en: So the question I wanted to ask is， nowadays you've probably seen a lot about
    like borrow in chat。and given the fact that you have all of these human preferences
    expressed by your elevatorers。and you guys and you guys try playing that a reward
    or preference model。Yeah I think you can think about like difference changes of
    model development right so this is pre deploymentloyment and release in the real
    world so you can't put a crappy model out there in the real world so even before
    doing that if you can like get like maybe 10 examples from whatever experts that
    you can get heard of and use that to prompt your new model that's better that's
    a much better starting point before you expose the model to the real world and
    collect references from real users at scale and so I think RLH is also much less
    sample efficient compared to instruction prompt tuning again because you're probably
    trying to update your entire model as well so I think this is a very good starting
    point and so they can be combined depending on how depending on the lifecycl of
    the model。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想问的问题是，现在你可能看到很多关于在聊天中借用的内容。并且考虑到你有所有这些由你的评估者表达的人类偏好。你们试图将其转化为奖励或偏好模型。是的，我认为你可以考虑模型发展的不同变化，所以这是部署前的阶段，并在现实世界中发布，因此你不能将一个糟糕的模型放到现实世界中。所以在做到这一点之前，如果你能从任何你听说过的专家那里获得大约10个示例，并用这些示例来提示你的新模型，那将是一个更好的起点，在将模型暴露于现实世界并从真实用户那里大规模收集反馈之前。因此，我认为与指令提示调优相比，RLH的样本效率也要低得多，因为你可能试图更新整个模型。因此，我认为这是一个非常好的起点，可以根据模型的生命周期进行组合。
- en: The data set is public， I'll talk about the results in a bit within。You mean
    the model responses and what the humans are that's a good point we are so far
    not considering releasing them。but maybe we can Okay do you see a use case for
    that。Do have a bunch of data in the prens。so frame the model to express those
    pregnancies and use that model R chat in medical。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是公开的，我稍后会讨论一下结果。你是说模型的反应和人类的表现，这是个好点子，我们目前还没有考虑发布它们。也许我们可以。好吧，你觉得这有什么用例吗？我们有一堆数据在记录中。所以让模型表达那些妊娠，并在医学中使用这个模型聊天。
- en: So if I wanted to explainYeah that's a good point， I think the evaluation data
    set is I'll talk about this a bit here。it's still small but I think if we scale
    it up and we are doing it right now I think we can release that and that'll be
    I think a good resource of what you're trying。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果我想解释一下，嗯，这个观点不错，我认为评估数据集，我会在这里稍微谈谈。它仍然很小，但我觉得如果我们扩大规模，我们现在正在这样做，我认为我们可以发布这个，这将是我认为你们所追求的一个很好的资源。
- en: 可。So we have the metPm model as I said， and now we took the long form answers
    from it and compared that to the F palm model as well as to answers generated
    by expert clinicians and as I said we have two parts to the human evaluation one
    is by expert clinicians and the other one is by LA users and so what do these
    results look like on the one and 40 y questions that we got these evaluation results
    on what we observed typically。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 可以。所以我们有了我所说的metPm模型，现在我们从中提取了长形式的答案，并将其与F palm模型以及专家临床医生生成的答案进行了比较。正如我所说，我们的人类评估分为两个部分，一个是由专家临床医生进行，另一个是由LA用户进行。那么，我们在这些评估结果上获得的40个问题的结果是什么样的呢？我们通常观察到的情况是。
- en: Across the board was when we looked at like different axes while the fl palm
    model would be quite terrible honestly the metT pump model would do much better
    and typically close the gap to expert clinicians so on this axis you see that
    the fl palm model has probably 60% or accuracy in terms of like scientific consensus
    the metP model improves on that quite a bit and closes the gap to clinicians over
    here。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在各个方面，我们查看了不同的轴，而FL Palm模型老实说表现相当糟糕，METT泵模型则表现得好多了，通常能够缩小与专家临床医生之间的差距。在这个轴上，FL
    Palm模型的科学共识准确率大约为**60%**，而METP模型在这方面有了很大改善，缩小了与临床医生之间的差距。
- en: Similar story on other axes as well over here you see the like clinicians rating
    on the axes of how well the model can retrieve medical knowledge。how well it can
    reason about it and again we see the same trend as in the previous slide。so the
    column correct so it's evidence of correct comprehension and the rights incorrect
    minus no so both can be present at the same time so you can have evidence of correct
    comprehension also evidence of incorrect comprehension sometimes so exactly so
    so that's why they're not one minus over here but the trends are the same or also
    that's why skipable but that's a detail。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里在其他维度上也有类似的故事，你可以看到临床医生在模型如何检索医学知识、如何进行推理的评分。我们再次看到与前一张幻灯片相同的趋势。因此，列出的正确性是正确理解的证据，而错误的则是无，所以这两者可以同时存在。你可以同时有正确理解的证据，也可能有错误理解的证据。没错，这就是为什么这里不是一个减去的结果，但趋势是相同的，这也是为什么可以跳过的原因，但这只是一个细节。
- en: Yeah again so there's a type over here， but this one pertains to incorrect our
    missing content but this was an interesting one because what when we were doing
    this from tuning thing was we were teaching the metP model to produce longer and
    more complete answerss and so you'd see a few qualitative examples later but what
    ended up happening the process was sometimes the model was maybe producing more
    incorrect information so that's why you see that maybe in this particular axis
    the fl palm model was slightly better but again this was much worse compared to
    transmissions。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，再次提到，这里有一个类型，但这个与不正确或缺失的内容有关。这是一个有趣的案例，因为在进行调优时，我们正在教metP模型生成更长且更完整的回答。你稍后会看到一些定性的例子，但在这个过程中，模型有时可能会产生更多的不正确信息。因此，你会看到在这个特定的维度上，fl
    palm模型稍微好一些，但与传输相比，这依然差得多。
- en: 喂，我也在这买。It's a good question， it is more like it's something something completely
    out of context。so it may be irrelevant to the question。So that's why I would say
    it。Okay。We also looked at like possible and extent and likelihood of harm。and
    again we see that with the instruction prompt tuning we're able to close the gap
    to expert teenagers over here。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 喂，我也在这买。这个问题很好，更像是完全脱离上下文的东西。所以这可能与问题无关。这就是我会这样说的原因。好的。我们还考虑了可能的危害程度和可能性。再一次，我们看到通过指令提示调优，我们能够缩小与这边专家青少年的差距。
- en: Same on the bias access as well。嗯不能确。Can you interpret the talk？诶其该。So like
    I basically see like something de and then the clinicians at like6% see would
    you talk more like how to clarify exactly what that？
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 关于偏见访问也是一样。嗯，不能确定。你能解释一下这个讨论吗？诶，应该是这样。所以我基本上看到像某种数据，然后临床医生大约6%会这样说，你能多谈谈如何准确澄清这个问题吗？
- en: 😊，Yeah so it's basically so there might be certain conditions or pathologies
    or diagnosis right like cancer and if for example the clinician has not caught
    that or has maybe given a response that does not appropriately convey the severity
    of the condition then that could potentially lead to severe harm or death and
    so that's what we were trying to capture a here so that's a very high level overview
    this is。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，是的，基本上可能会有某些情况、病理或诊断，比如癌症。如果临床医生没有及时发现，或者可能给出的反应未能恰当地传达病情的严重性，那么这可能会导致严重的伤害或死亡，这正是我们想要在这里捕捉的内容，所以这是一个非常高层次的概述。
- en: I think a very nuanced topic and there's a framework for it called the AHRQ
    framework and so we've linked that in the paper as well and so I think that gives
    you a very detailed notion of harm and bias that I would refer to that but at
    a high level this is what I'm talking about how that helps。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一个非常微妙的话题，针对这个话题有一个框架叫做AHRQ框架，我们在论文中也链接了它，我认为这为你提供了一个非常详细的伤害和偏见的概念，我会提到这一点，但从高层次来看，这就是我所谈论的内容，以及它是如何帮助的。
- en: All right， so when later I class and I say the clinician had 5。7 a possible
    which means like what would I say does that mean that they recommend something
    or maybe they fail to recommend something yeah。so it's basically a misdiagnosis
    or maybe failing to capture the severity of a diagnosis this is typical in life
    threatening conditions right so。So it's more often than not mistakes， but rather
    just missing out on details。Yeah。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，后来我上课时说临床医生有5.7的可能性，这意味着我该怎么说？这是不是意味着他们推荐了什么，或者也许他们没有推荐什么？是的。因此，这基本上是误诊，或者说未能捕捉到诊断的严重性，这在危及生命的情况中是很典型的，对吧？所以，往往更多的是错误，而不是只是错过了细节。是的。
- en: so I talked about bias as well and then as I said the other axes of human evaluation
    was with layer users and so we asked them how well does the model answer address
    the intent of the question and again we saw with instruction prompt during MetPm
    closing the gap to clinicians and then we asked them like how helpful the responses
    were and what we see is that while fl palm responses were considered to be helpful
    like 60% of the time the number improved to 80% for MetPM。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我也谈到了偏见，正如我所说，人类评估的其他维度是通过层用户，因此我们问他们模型回答对问题意图的满足程度如何，再次看到在MetPM的指令提示下，与临床医生之间的差距缩小了。然后我们问他们这些回应有多有帮助，我们看到的是，虽然fl
    palm的回应被认为有帮助的比例约为60%，但MetPM的比例提高到了80%。
- en: but was still fairly lower compared clinicians at 90%。嗯。So here are a few qualitative
    examples and so what you see is that physicians and this is typically because
    they work in time constrained settings they their answers tend to be precise and
    succinct but sometimes it's very hard as layer users or patients to like decipher
    and decode the answer and get all the full set of details right and so what I
    think language models like MePm can help with is actually converting the physicians
    speak to something that's more easily digestible by layer users and so this is
    where I think how these models will likely fit in clinical settings in the neural
    term where they are going to augment physicians in terms of like interacting with
    patients and other physicians and researchers as well。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但与90%的临床医生相比，仍然显得相当低。嗯。所以这里有一些定性示例，你会发现医生通常因为在时间有限的环境中工作，他们的回答往往精准简洁，但有时作为层用户或患者，很难解读和理解这些答案，并获取所有完整的细节。因此，我认为像MePm这样的语言模型可以帮助将医生的语言转换为更易于层用户消化的内容，这就是我认为这些模型在临床环境中如何适应的原因，它们将增强医生与患者、其他医生和研究人员的互动。
- en: Sa good。Im just wondering because we visit。It that water is down and it's a
    very。If I take as a patient， you do it or not authority。That's right I think it's
    subjective and so that's why I think we're still seeing like lay users rate plan
    performance answerss to be helpful 80% well that's much higher for physicians
    so it's not perfect by any means but I think this is where its there there is
    a complementarity element we feel over here。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我只是想知道，因为我们在访问。水位下降，而且这很。作为患者，我是否可以，你是否有权这样做。没错，我认为这是主观的，这就是为什么我认为我们仍然看到普通用户的评价计划表现的答案很有帮助，80%对于医生来说要高得多，所以这并不完美，但我认为这里有一个互补的元素，我们在这里感受到。
- en: And we've asked that and like when we ask people like how easy is it to like
    interpret doctor notes or recommendations and they often say。oh it's very hard
    I need to go back to Google search for what these terms mean。what these abbreviations
    mean and so I think this is where a language model can come and take that note
    and convert that into something that's more easily digestible like so I think
    that's the opportunity over here I feel。So that was all on our paper， but I also
    want to maybe very quickly point out a very recent work which came out last week
    with this rather provocative title do we still need clinical language models and
    by clinical language models they meant smaller models which are trained in domain
    with clinical data such as medical notes and records and so on and so forth and
    what this paper basically suggests is that smaller fine-ted in domain LMs are
    likely better than general purpose LLMs in this paper I think the evaluate on
    GT3 with in context learning so I think that's a pretty interesting any observation
    I think there's a lot of value for smaller in domain LMs such as P GPT and a few
    other lib variances but I think one thing that this paper does not do is consider
    in context learning sorry prompt tuning and I think that's where some of the benefits
    of these larger general purpose LLMs shine and again we haven't done any in domain
    LM pretraining。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: These large general purpose。Models， but that's again an option for us as well
    to do dling。So you can take these5 and 40 billion parameters and then still turn
    it on medical nodes or whatever domain specific data that you can get hold of
    and hopefully that'll probably further improve the performance。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大型通用模型，不过这对我们来说也是一个选择，可以进行微调。因此，你可以利用这5和400亿个参数，然后依然在医学节点或其他特定领域的数据上进行调整，希望这能进一步提升性能。
- en: So key takeaways so far。What I want to convey is general purpose LMs。it looks
    like they do encode medical knowledge and performance on medical reasoning tasks
    seem to improve with scale。however these models I don't think can be directly
    used out of the box in clinical settings and they need to be aligned with the
    safety critical requirements of the medical domain。And I think instruction proing
    is an extremely efficient technique both on the data side and also on the compute
    side and we should probably use it more often。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止的关键要点是什么。我想传达的是通用语言模型。看起来它们确实编码了医学知识，且在医学推理任务上的表现似乎随着规模的增加而改善。然而，我认为这些模型不能直接在临床环境中使用，需要与医疗领域的安全关键要求保持一致。我认为指令调优是一种在数据和计算方面都极为有效的技术，我们应该更频繁地使用它。
- en: depending on and hopefully the API starts supporting it as well。and these models
    appear to be closing the gap to expert clinicians at least on this medical question
    answerscing tasks and while this is hugely exciting and has profound implications
    you can all probably dream up and imagine the application scenarios over here。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 根据情况，希望API也开始支持它。这些模型似乎在医疗问题回答任务上正在缩小与专家临床医生之间的差距，而这一点无疑令人兴奋，并且具有深远的影响，你们可能都能想象出这里的应用场景。
- en: I think comprehensive benchmarks and evaluation frameworks are necessary in
    order to further assess and improve these models for real use cases。So I'll trouble
    over here any questions？完成。In medicineine， there survived。哎，是。对。你啊。A lot of it
    is because these data sets tend to get locked in silos with privacy and other
    kinds of regulations which prevent them from being put out there in the real world。so
    you have to have like hip compliant systems for storage and so on and so forth
    so it's very difficult to get data out of these silos and put together an open
    benchmark so honestly I feel like that's probably not going to improve the scale
    of these data sets at least the open version of these dataset sets are going to
    remain。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为全面的基准测试和评估框架是必要的，以进一步评估和改进这些模型以用于实际案例。那我在这里烦请有什么问题？完成。在医学上，依然存在。哎，是。对。你啊。很多原因是这些数据集往往会被锁在隐私和其他法规的孤岛中，这些法规阻止它们在现实世界中发布。因此，你必须拥有符合HIPAA的存储系统等等，因此从这些孤岛中获取数据并整合一个开放的基准非常困难。老实说，我觉得这可能不会改善这些数据集的规模，至少开放版本的数据集将会保持不变。
- en: Quite small compared to the big L training data sets or the computer division
    data sets on natural images and on and so forth。but what may happen in the future
    is we may have like more distributed fedated evaluation settings where you take
    the model into these private silos and get them evaluated on so they are never
    exposed and put out there in the public but rather we can have these fed rate
    evaluation settings so I think that there's some work on that already there's
    a system called MeF and probably see more of them。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于大规模的 L 训练数据集或自然图像的计算机分部数据集，这个规模相当小。不过，未来可能会发生的情况是，我们可能会有更多分布式的联邦评估设置，在这些设置中，你将模型放入这些私密的存储区进行评估，因此它们不会被暴露并公开，而是可以进行这些联邦评估设置。因此，我认为在这方面已经有一些工作在进行中，有一个系统叫做
    MeF，可能会看到更多类似的系统。
- en: 就等个嘅发过先。Sure， so the question over here was why medical data sets are smaller
    compared to natural image data sets in computer division or LMP training data
    sets and so on and so forth。What do you think are some of the earliest applications
    of medical LOms like deployed in industry？
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 就等个嘅发过先。当然，这里问的问题是，为什么医学数据集相较于自然图像数据集在计算机分割或LMP训练数据集中要小得多，等等。你认为医学LOms在工业中最早的一些应用是什么？
- en: I think the first set of use cases are probably going to be not diagnostic in
    it sorry the question was what do you think are the use cases of medical algorithms
    in medical industry settings and so。The answer is I think the first set of use
    cases that we are going to see are probably going to be non-diagnostic in nature。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为第一组使用案例可能会不是诊断性的，抱歉，问题是你认为医疗算法在医疗行业环境中的使用案例是什么。因此，我的回答是，我认为我们将看到的第一组使用案例可能本质上是非诊断性的。
- en: but more around like if a patient comes in and interacts with a doctor。can you
    like generate summary notes and can you do like workflow tasks such as generating
    letters for insurance for medications for referrals and so on and so but I think
    these tasks are right up the alley of large language models and I think if not
    already in the next six months to a year we'll see a lot of these use cases coming
    up and I think that's going to make doctors like care providers life much easier
    because right now they're spending a lot of time doing these things and not actually
    providing care and attending to the patient diagnostic use cases I think will
    take a lot more time。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但是更多的是如果患者进来与医生互动，你能生成摘要笔记吗？你能做一些工作流程任务，比如为保险生成药物推荐信等等吗？我认为这些任务正适合大型语言模型，我认为如果还没有的话，在接下来的六个月到一年内，我们会看到很多这样的用例出现。我认为这将使医生和护理提供者的生活变得更加轻松，因为现在他们花费大量时间在这些事情上，而不是实际提供护理和关注患者的诊断用例，我认为这需要更多时间。
- en: we need a lot more evaluation the data sets as we can see are probably not there
    evaluation frameworks are not there but I think in the long run and that is the
    dream setting right。And then maybe a follow up is M， I'm assuming meed Palm is
    not open source。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要对数据集进行更多评估，显然评估框架还不完善，但我认为从长远来看，那是理想的设定。然后也许后续是M，我假设Meed Palm并不是开源的。
- en: what do you think the best open source model is for medical？诶。Yeah I think it
    depends on the so the question is what is the best open source model for medical
    data I think depends on the evaluation setting so I think the PM GPT model from
    the Stanford Foundation models group is quite strong I think GPT3 or 3。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为最好的医疗开源模型是什么？诶。是的，我认为这取决于，所以问题是医疗数据的最佳开源模型是什么，我认为这取决于评估设置，所以我认为斯坦福基金会模型组的PM
    GPT模型相当强大，我认为GPT-3或3。
- en: 5 or whatever variant if you can bring in some domain specific medical data
    and do some in domain tuning adding that model can also improve quite a bit so
    I think those two would be my favorite chart points over here。So feels like part
    of the important itself problem after section。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能引入一些特定领域的医学数据并进行一些领域内的调优，那么5或其他变体也能显著改善模型，所以我认为这两个将是我在这里最喜欢的要点。因此，这感觉像是部分重要性本身的问题。
- en: It's you can just think them as vectors corresponding to a few additional tokens
    so it's not really human legible so the question was what do the soft prospectors
    look like and are they human legible and yeah the answer is no they're not。Just
    a you said mentioned by very know for larger quality policy third of route and
    usually on sites have hospitalized moral quality infrastructure online medicine
    we really believe that should be learning what we have。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把它们看作是对应于几个额外标记的向量，因此并不是很容易被人理解。那么问题是，软前景是什么样的？它们是否容易被人理解？答案是，不，它们并不能。正如你所提到的，针对更大质量政策的三分之一路径，通常在网站上有住院道德质量基础设施在线医学，我们真的相信应该学习我们所拥有的。
- en: On site hardware， on site and machines that contain these models of the board。Sure。so
    the question was given a lot of the hospital systems and providers networks are
    quite low tech and don't have good enough hardware。do you really think fed learning
    could be used for distributed training of large scale LLMs？
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现场硬件，现场和包含这些型号板子的机器。好的。所以问题是考虑到许多医院系统和服务提供者网络的技术水平相对较低，并且硬件不够好。你真的认为联邦学习可以用于大规模LLM的分布式训练吗？
- en: I think we are increasingly seeing a trend towards cloud and so a lot of these
    hospital systems are moving their storage and data and compute to standard chart
    providers like AWS or as your Google cloud and so I think that helps because these
    systems on the back and side do have the compute to be able to like train these
    kind of models I think it's going to be a very gradual process so systems that
    have high quality infrastructure probably we're going to start with that first
    and then gradually work our way into the long tail but it also feels like something
    that will inevitably exist in the world so 10 years down the line of 15 years
    down the line and we have these distributed La scale relevant training systems
    we always think why did I even doubt that this will not exist it's so obvious
    it's something that has to exist because that's where all the patient data is
    all the interesting data is right because I think that will just happen it's just
    not clear where that's going to be done by one company whether that's going to
    be done by consortium of academic or industry groups or。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们越来越看到向云计算发展的趋势，因此许多医院系统正在将它们的存储、数据和计算迁移到像AWS或Google Cloud这样的标准云服务提供商。我认为这很有帮助，因为这些系统在后台确实具备能够训练这些模型的计算能力。我认为这将是一个非常渐进的过程，因此基础设施高质量的系统可能会首先启动，然后逐步进入长尾领域。但这似乎也是一种必然会存在于世界中的事物，所以在10年或15年后，当我们拥有这些分布式的大规模相关训练系统时，我们总会想，为什么我曾怀疑过这不会存在？这太明显了，它必须存在，因为所有的病人数据和所有有趣的数据都在那里。我认为这只是会发生，只是不清楚这一切是由一家企业完成，还是由学术界或行业团体的联合体来完成。
- en: s are going to be involved on and so forth。That's right。so the question over
    here is we seeing cloud computing but we are pretty much uploading the data to
    the same warehouse the answer is true but again I think these are all going to
    be separate buckets with their own access controls and so on and so forth so that
    is how you can differentiate between them。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将会涉及到等等。没错。那么这里的问题是，我们看到云计算，但我们基本上是在将数据上传到同一个仓库，答案是对的，但我认为这些都会是各自独立的桶，有着各自的访问控制等等，这就是你如何区分它们的方法。
- en: 对。好。It's been a lot of。这 really的。I。it doesn't seem like that thing in sense
    that we're that we're used to carry。Sure so the question was has there been any
    studies in MePm looking at private information in these data sets and the short
    answer is no one of the criteria for selecting the data sets that we used in the
    study was to not include any kind of personally identifiable data or clinical
    data of that sort and that helped like you know get this paper out on time but
    I think that's an important point。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对。好。已经有很多了。这真的。I。似乎不像我们习惯携带的那种东西。当然，问题是是否有研究在 MePm 中查看这些数据集中的私人信息，简短的回答是没有。我们在研究中选择数据集的标准之一是不要包含任何可识别个人身份的数据或此类临床数据，这有助于我们按时完成这篇论文，但我认为这是一个重要的观点。
- en: It's unlikely that we're going to have a lot of PI data in public well in the
    public data that we are training on but。Even when you're training on say one private
    corpus and then you're using it in another application setting。you want to ensure
    that the model does not leak out any kind of PHI information during a generation。so
    I think those sort of studies are necessary， we haven't got into them yet。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在公开数据中训练时，不太可能有大量的PI数据。但是，即使你在某个私有语料库上训练，然后在另一个应用场景中使用，你也需要确保模型在生成过程中不会泄露任何PHI信息。因此，我认为这类研究是必要的，我们尚未深入进行这些研究。
- en: So does think fairly on withs and exploring good models and that people be able
    having groups over the same experience towards the world trying。So the question
    is what are the next steps in terms of improving these models further yeah retrieval
    is a very important one being able to cite sources and especially taking authoritative
    sources and use that in generating the answers and' also communicating that to
    the users is very important I think how you communicating uncertainty is very
    important so we gotten to some extent using instruction from tu but I think that
    can be much much better so I think that's another big bucket again I would stress
    on the evaluation side like you know looking at more data sets which for example
    may do Q&A on health records or or other kinds of medical data I think that will
    be important and also extending the evaluation both in terms of scale having a
    diverse panel of clinician sport and also in terms of the data that you're using
    maybe adverarily modifying the questions to include like demographic confounders
    or something like that I think those are all could be interesting directions I
    think on the modeling side the interesting question for me is again this interplay
    between。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，公平地考虑模型和探索良好模型，以及人们能够在相同经历中形成的群体，是非常重要的。问题是，接下来在改进这些模型方面的步骤是什么。检索是一个非常重要的环节，能够引用来源，尤其是采用权威来源来生成答案，并将其传达给用户，这一点我认为非常重要。我认为，如何传达不确定性也非常重要。我们在某种程度上使用了来自tu的指导，但我认为这可以做得更好。所以我认为这是另一个重要的方面。我会强调评估方面，比如关注更多数据集，例如在健康记录或其他类型医疗数据上进行问答，我认为这很重要。同时，扩展评估的规模，拥有多样化的临床医生支持，并且在使用的数据上，可能要有意识地修改问题，以包含人口统计混杂因素等，这些都是值得关注的有趣方向。在建模方面，对我来说，一个有趣的问题是这种相互作用。
- en: Smaller。Domain specific alarms versus large general purpose alarms and how that's
    going to play out there seems to be some evidence of emergence over here especially
    with medical reasoning and so as you can see at lower scales sometimes the performance
    is not good enough I mean 50% I mean that's a good number but that's just not
    viable and but when you get to like 80 90% products really become useful right
    and so that we are seeing at like you know bigger parameter sizes of these models。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 更小。特定领域的警报与大型通用警报之间的差异，以及这将如何发展，似乎在这里有一些新兴的证据，尤其是在医学推理方面。因此，正如你所见，在较低的尺度上，有时性能并不够好，我的意思是50%，这是一个不错的数字，但这并不可行。然而，当你达到80%或90%时，产品确实变得非常有用，对吧？我们在这些模型的更大参数规模上看到了这一点。
- en: But I don't know， I think it's still an open question over here。yah诶。The question
    was is hallucination an issue I think it still is。but I believe that you can control
    that fairly well with instruction prompt tuning but like any kind of feedback
    data I think it's not terribly difficult to do and so the。I think it might have
    been overblown generally， so especially when you are doing it in a particular
    domain。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 但我不知道，我认为这个问题在这里仍然悬而未决。耶。问题是幻觉是否是个问题，我认为它仍然是。但我相信你可以通过指导提示调整来很好地控制这一点，不过像任何反馈数据一样，我认为这并不困难。所以，我觉得这个问题可能被夸大了，尤其是在特定领域时。
- en: I think it's easier to control。I the extent to which you have been leader or
    like like one it looks like I just。Recently， there's been the product above and
    their。一本です。Okay。Yeah。So I'm just curious because this particular app really very。very
    relevant and talking about high bar and quality so I'm just curious if speak。Yeah。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得控制起来更容易。你作为领导者的程度或像领导者一样的程度，看起来我只是。最近，出现了上面提到的产品及其。一本です。好的。是的。所以我只是很好奇，因为这个特定的应用程序真的非常。非常相关，并谈论高标准和质量，所以我只是好奇是否说。
- en: so the question was there is a lot of talk and noise around hallucinations and
    general purple cell limbs and this in this particular application domain it seems
    particularly relevant and so can you expand on that a little bit further sure。What
    we are seeing is even with like an order of a few hundred examples from expert
    clinicians teaching the model how to communicate medical information。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题在于，关于幻觉和一般的紫色细胞肢体有很多讨论和噪音，而在这个特定的应用领域，这似乎尤其相关。你能对此进一步展开一下吗？当然可以。我们看到的是，即使只有几百个专家临床医生的示例在教导模型如何传达医疗信息。
- en: That is good enough to get the model to maybe stop。Hucinating or at least communicate
    its uncertainty in a better way so at least in this particular domain or this
    setting it feels more tractable to us and the reason I'm saying this is we've
    looked at the answers qualitatively and we are seeing that the model does not
    tend to like generate like super long answers or you know or like you know make
    like very confident predictions but rather the tone itself becomes like very reserved
    and it starts using terms like you know maybe this needs to be done further or
    something like that which communicates uncertainty so how well is that actually
    correlatedated with the representation underlying uncertainty that we have is
    still I think an area of research but I think this is already promising for us
    that it feels controllable in limited application settings like medicine。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这足以使模型可能停止幻觉，或者至少以更好的方式传达其不确定性，因此在这个特定领域或设置中，它对我们来说感觉更可控。我之所以这么说，是因为我们定性地观察了答案，发现模型并不倾向于生成超级长的回答，或者做出非常自信的预测，而是语气本身变得非常谨慎，开始使用诸如“也许需要进一步处理”之类的术语，这传达了不确定性。那么，这与我们所拥有的不确定性基本表示之间的相关性到底如何，我认为仍然是一个研究领域，但我认为这对我们来说已经很有前景，因为它在有限的应用场景中，如医学，感觉是可控的。
- en: But if you have a general purpose Lmm trying to answer pretty much everything
    about the world。I think that's a much harder problem。Do you think that would be
    a feature of like the domain。David said？In medical situations， doctors are more
    reserved perhapss and don't。Absolute have absolutely English for。On on certain
    things or do you think it's more that like you have like just specialized right
    like it could be。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你有一个通用的语言模型，试图回答关于世界的几乎所有问题。我认为这将是一个更困难的问题。你认为这会是某种领域的特征吗？大卫说？在医疗情况下，医生可能会更谨慎一些，绝对不对某些事情有绝对的英语表达。或者你认为这更多的是你只需要专门化？这可能是这样的。
- en: Something else entirely also I'm just curious yeah， so the question is。do you
    think this the way how the model is performing in this domain is that a feature
    of the data sets in the medical domain and。Typically based on how doctors communicate
    and I think that's true and I think that's something we need to build on and use
    over here and I think that's extremely helpful and hopefully this kind of behavior
    is general enough and can be transmitted to the model even when it's used in non-medical
    settings to be like you know more reserved when it's communicating hallucinateless
    and so on and so forth so I believe that that's one of the opportunities over
    here to like use these benchmarks come up with methods that reduce hallucination。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 完全不同的事情，我只是好奇，所以问题是。你认为这个模型在这个领域的表现是数据集在医疗领域的一个特征吗？通常是基于医生的沟通方式，我认为这是正确的，我认为这是我们需要在这里构建和利用的东西，我认为这非常有帮助，希望这种行为足够普遍，能够转移到模型中，即使在非医疗环境中使用时，也能在交流时更加克制，减少虚假信息等等。所以我相信这是这里的一个机会，可以利用这些基准，提出减少虚假信息的方法。
- en: communicate uncertainty better and then use that as a bidirectional learning
    opportunity to improve the general purpose cell this。So if you have any further
    questions， I'll come back again at the end of the talk。but I want to cover the
    rest of the applications as well。So the next domain I want to talk about is proteins
    and the papers from now I'm going to like zip through them a little bit given
    time。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 更好地传达不确定性，然后将其作为双向学习的机会，以改进通用目的细胞。这么说，如果你还有其他问题，我会在演讲结束时再回来。 但我也想涵盖其他应用。因此，我想谈论的下一个领域是蛋白质，从现在开始我会稍微快速浏览这些论文，考虑到时间。
- en: but the first one I want to talk is this paper from a few folks at Google Research
    back in 2020 called mass language modeling for proteins by linearly scalable long
    Con transformformers。So the problem here is that modeling long range biological
    sequences requires efficient transformer architectures。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我想首先谈谈2020年谷歌研究的一些人发表的论文，标题为“通过线性可扩展的长卷积变换器进行蛋白质的大规模语言建模”。所以这里的问题是，建模长距离生物序列需要高效的变换器架构。
- en: and so in this particular paper what they introduced was this performer architecture。which
    approximates the softmax attention kernel via low rank decomposition。And so this
    does not incorporate any spaity price， say like other methods like the reformer
    or there are many others。and this is good because spaity price may not be appropriate
    for biological data such as protein。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此在这篇特定的论文中，他们引入了这种表演者架构。它通过低秩分解来近似softmax注意力核。并且这并不包含任何稀疏成本，比如其他方法，如reformer或其他许多方法。这是好的，因为稀疏成本可能不适合生物数据，例如蛋白质。
- en: which required global interactions to be modeled。And then the other thing is
    this model。the performance scales linearly rather than quadratically with the
    sequence link de。and the number of random features that you need to approximate
    this Somax attention kernel is completely independent of the input sequence link。So
    just to very quickly visualize the speedups and the space complexity improvements。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要对全球互动进行建模。然后另一个方面是这个模型。性能与序列链接的缩放是线性的，而不是二次的。你需要近似这个Somax注意力核的随机特征数量与输入序列的链接完全无关。所以为了快速可视化加速和空间复杂度的改善。
- en: what you're having with this low angle decomposition is instead of having like
    fat matrices in your softmax attention kernel。you now have thinner mattressrices
    which are determined by the size of the random features and that basically reduces
    your quadratic complexity to something that is more linear in nature and also
    leads to space improvements。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这个低角度分解中所遇到的是，不再像你的softmax注意力核中有厚重的矩阵。现在你有了更薄的矩阵，这些矩阵由随机特征的大小决定，这基本上将你的平方复杂度降低到更线性的性质，并且还带来了空间的改进。
- en: So I would yeah there are more theoretical analysis and details in a paper and
    I would refer you all back to it。but what we see in terms of results when doing
    protein language modeling is that the accuracy of of this model is on par with
    transformers while reducing computational cost quite a bit and so what this suggests
    is that the approximation of the softmax attention kernel is a tight approximation
    so that is good and then when you compare that with other methods such as the
    reformer or the Lyformer。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我会说，论文中有更多的理论分析和细节，我会建议大家参考一下。但在进行蛋白质语言建模时，我们看到的结果是，这个模型的准确性与变压器相当，同时显著降低了计算成本，这表明softmax注意力核的近似是非常紧密的，这很好。然后当你将其与其他方法如reformer或Lyformer进行比较时。
- en: the accuracy is much higher at least on this task so it seems that compared
    to other methods that like try to build more efficient transformist this one is
    much better for biological sequence data at least in this setting。And finally，
    if you look at the attention of the amino acid similarity matrix。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务上，准确性要高得多，因此与其他试图构建更有效的变换模型的方法相比，这种方法对于生物序列数据在这种设置下要好得多。最后，如果你查看氨基酸相似性矩阵的注意力。
- en: you can see that the performma model recognizes highly similar amino acid pairs
    such as DE and FNY over here。That suggests that the model is learning the right
    set of information that we really want over here。So that was a two minute overview
    of the paper， but I wonder yeah。talk about another one which also I think is really
    really cool。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，performma模型识别了高度相似的氨基酸对，例如这里的DE和FNY。这表明模型正在学习我们真正想要的信息集。所以这是对论文的两分钟概述，但我想，嗯，谈谈另一个我认为也非常非常酷的。
- en: So this one is called protein Llum again by a few of folks at Global Research。and
    what this tells us is model based natural language protein annotation。And why
    this problem is important is because the protein information is in very high demand。So
    over 50% of all known protein sal in sequence we don't actually know what they
    do so it's important that we able to like decier that to some degree at least
    and then the second thing is we may want to for example find protein sequences
    with given functions and this is particularly important in the CRISPR domain and
    so if you can train bidirectional models that can do this I think thatll be incredibly
    helpful and。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这个被称为蛋白质Llum的项目是由全球研究的一些人提出的。这告诉我们的是基于模型的自然语言蛋白质注释。这个问题之所以重要，是因为蛋白质信息的需求非常高。因此，已知蛋白质序列中超过50%的功能我们实际上并不知道，所以至少在某种程度上能够解读这些信息是很重要的。第二点是，我们可能想要找到具有特定功能的蛋白质序列，这在CRISPR领域尤为重要。如果你能训练出能够做到这一点的双向模型，我认为这将是非常有帮助的。
- en: And the reason I say this， again， is that the Unipro database that has over。There
    is I think millions of researchers worldwide using it today。and so getting this
    information populated in that database would be incredibly useful and accelerate
    a lot of research in this space。And so the European Bioinformatics Institute，
    they have curated this prett data about proteins and so basically you can use
    this protein in record to like train these models and so what you want to do is
    you want to maybe learn to directly map from amino acid sequences to natural language
    descriptions of them。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我之所以这么说，是因为Unipro数据库拥有超过千万的研究人员在全球范围内使用它。将这些信息填充到该数据库中将是非常有用的，并能加速这一领域的大量研究。因此，欧洲生物信息学研究所对蛋白质进行了数据整理，基本上你可以利用这些蛋白质记录来训练这些模型。因此，你想要做的就是直接将氨基酸序列映射到它们的自然语言描述上。
- en: And this problem is not too different from an image captioning problem where
    instead of having a sequence of pixels。I don't know if sequence is right， but
    again if you have pixels instead you have a sequence of aminoamine acids and they
    can range a number from two to 40K and then what you want to generate out is a
    protein description of the protein。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题与图像标题生成问题并没有太大不同，在这里你不是处理一系列像素。我不知道“序列”是否合适，但如果你有的是像素，而是有一系列氨基酸，它们的数量可以从二到四万不等，然后你想生成的是蛋白质的描述。
- en: And in this paper the way they do this is they train a T5 model on protein sequence
    sanitation tasks。so the tasks are set up in a bunch of different ways and the
    supervised data comes from a bunch of different sources in the protein record
    that they have and this model is an encode E T5 model so it's a very cool application
    and the results are that out of the 56 million proteins in that unprod database
    that were previously uncharacterized 49 million of them now have associated textual
    descriptions so we now have a handle on what they do。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，他们的做法是训练一个T5模型来处理蛋白质序列的清理任务。因此，这些任务以多种不同方式设置，监督数据来自他们所拥有的蛋白质记录中的多个不同来源。这个模型是一个编码E
    T5模型，因此这是一个非常酷的应用。结果是，在之前未被表征的5600万种蛋白质中，现已有4900万种与之相关的文本描述，因此我们现在对它们的功能有了更清晰的理解。
- en: 嗯。And so that's really cool and then the other one I think which is probably
    even more interesting is now you can run queries like find me a smaller version
    of this CRISP Cas9 protein so that it can target certain tissue spectra and now
    the model can know come back with sequences and so I think this is again going
    to be incredibly useful and going to accelerate a lot of research in this space
    already there's a lot of momentum I think these models are going to further help。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。所以这真的很酷，然后我认为另一个可能更有趣的是，现在你可以运行查询，比如找到这个CRISP Cas9蛋白的小型版本，以便它可以针对某些组织光谱，现在模型能够返回序列，所以我认为这将再次非常有用，并加速这个领域的许多研究，已经有很多势头，我认为这些模型将进一步提供帮助。
- en: 诶。So that was on proteins， the last class of applications that I want to cover
    is on the genomics site。Again， the first paper over here was somewhat last year
    from our Ge team at HealthI at Google。which is building gap our sequence transformers
    for sequence correction。So this model is called deep consensus and so what role
    does this model play and why does it matter？
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。所以刚才讲的是蛋白质，我想要涵盖的最后一类应用是基因组学网站。再说，这里第一篇论文是去年我们谷歌HealthI的Ge团队发表的，它是在为序列校正构建我们的序列变换器。所以这个模型叫做深度共识，这个模型扮演什么角色，为什么它重要？
- en: So if you look at the sequencing data lifecycl， what you do is you go from basically
    atoms to bits and so you have this physical specimen which hopefully has some
    DNA in it and you。P it through a sequencing machine such as Spg bio and that comes
    out with the raw data and that raw data gets mapped to a reference genome and
    then sometimes there might be diffs between an individual and the reference genome
    and that can be corrected through this model called deep variant that was introduced
    by your team a few years back and that's open source and then once you have this
    sequence you can then use it for a bunch of different analysis such as ancestry
    or like just basic biomedical research。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你看一下测序数据生命周期，你基本上是从原子到比特的转变，因此你有这个物理样本，希望其中有一些DNA，然后你把它通过像Spg bio这样的测序机器，得出的原始数据会被映射到参考基因组，有时个体与参考基因组之间可能会有差异，这可以通过你们几年前引入的一个叫做deep
    variant的模型进行修正，而这个模型是开源的。然后一旦你拥有了这个序列，你就可以用它进行多种不同的分析，例如祖先分析或基础生物医学研究。
- en: So where deep variant fits in is it actually makes the raw DNA reads that comes
    out from the PA biosequenceer it tries to make it more accurate and so how the
    PA biosqueenceer actually works is it uses this circular consensus sequencing
    algorithm where the DNA molecule is like you know read several times and it produces
    multiple different subres and these subreads are。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 深度变体的作用在于，它实际上使得从PA生物测序仪中得到的原始DNA读取结果更加准确。PA生物测序仪的工作原理是使用一种循环共识测序算法，其中DNA分子会被多次读取，从而生成多个不同的子读取结果，这些子读取是。
- en: They do contain some errors and so they're finally like assembled together and
    so what Deep W tries to do is it tries to improve on the errors over here basically
    that comes out from just this circularence sensor sequencing algorithm。And so
    how does this model work so as that the basic task for deep consensus is to use
    the CSCCS data and the subreeds associated with them to generate a corrected sequence
    and so in this example when we run through the model what we see is that while
    the CCS identity was at like 95。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 它们确实包含一些错误，因此它们最终是组合在一起的，而深度共识（Deep W）所尝试做的就是基本上改善由这个循环传感器序列算法所产生的错误。那么，这个模型是如何工作的呢？深度共识的基本任务是利用CSCCS数据及其相关的子序列生成一个修正的序列。在这个例子中，当我们运行模型时，我们看到CCS的身份大约在95。
- en: 7% the deep consensus prediction identity was at 100% so it's a fairly simple
    task where you're trying to like reduce errors that come out from the fact bio
    with the CCS algorithm。And so the very natural question is where do these labels
    come from？
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 7% 的深度共识预测身份达到了 100%，所以这是一项相对简单的任务，你试图减少由于 CCS 算法带来的错误。很自然的问题是，这些标签来自哪里？
- en: So each CCS sequence that you have that is aligned to a high quality assembly
    and this high qualityality assembly is created by having many CCS reads stitch
    together and so that ends up having fewer errors and so you can then try to use
    that high- qualityality stitch assembly and map that back to the CCA trade for
    a given block and use that as that label so that results in more you know like
    stronger ground truth and you can use that to train the model to improve the accuracy
    further and so this is what the model is strained on and so the model looks like
    this it's a transformer architecture it takes these subres and this CCS read as
    well and it has a bunch of additional context features that come in from this
    the sequence itself the instrument sequencing instrument as well and these are
    all fit into the transformer model it produces polish segment and these segments
    are then like stitch together to produce the final polish read over here。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，您拥有的每个与高质量组装对齐的CCS序列，这种高质量组装是通过将多个CCS读取拼接在一起创建的，从而最终减少了错误，因此您可以尝试使用这种高质量拼接组装，并将其映射回给定块的CCA贸易，并将其用作标签，这样就能产生更强的真实数据，您可以用它来训练模型以进一步提高准确性，因此这就是模型的训练内容，模型的架构是变压器架构，它接收这些子读取和CCS读取，以及来自序列本身、测序仪等的额外上下文特征，所有这些都被输入到变压器模型中，它生成抛光片段，这些片段随后被拼接在一起以生成最终的抛光读取。
- en: One thing I will point out over here is that in order to train this model。you
    can't use a cross enpy loss and this is because。You know。if you insert you often
    have insertions in DNA sequences and so that can when you use cross entropy loss
    like really throw off the model。even like a single error as you can see over here
    can propagate throughout the sequence and make it really worse so what you need
    is a special kind of alignment loss based on a distance that can really capture
    this error much much better。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里要指出的一点是，为了训练这个模型，你不能使用交叉熵损失，这是因为。你知道，如果你插入，DNA序列中通常会有插入，因此使用交叉熵损失时，这会严重影响模型。即使是像这里看到的一个单一错误，也可以在整个序列中传播，导致结果变得更糟。因此，你需要一种基于距离的特殊对齐损失，能够更好地捕捉这个错误。
- en: And so making this alignment loss work on you know PUs and making a different
    shable is I think the real meat of this paper and so again go back to the paper
    if you're interested in that kind of topic。I think that's really cool。But at a
    very high level how well does this model work so if you look at the final output
    you have the read name you have the base predictions and also the predicted quality
    which can be thought of as a confidence score and these base predictions are often
    quite long and so you can see that continuous off screen because it's like you
    10 k to electronick basis long over here。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让这个对齐损失在处理PUs时有效，并制造出不同的shable，我认为这是本文的核心内容，所以如果你对这类主题感兴趣，再次回到论文中查看。我觉得这真的很酷。但是从一个很高的层面来看，这个模型的效果如何呢？如果你查看最终输出，你会看到读取名称、碱基预测，以及可以视为置信度分数的预测质量，而这些碱基预测通常相当长，所以你可以看到那持续在屏幕外，因为这里的电子基础大约是10k。
- en: And when you look at the quality， it improved quite a bit over the vanilla CCS
    algorithm over here。the per read accuracy over here improved quite a bit。And so
    you may ask like what is the real world impact of this kind of model right so
    the answer is this model is already being used in the real world so at Stanford
    in the genomic stream by Dr。Ashley and a few others there was this recent ultra
    rapid nanopogen sequencing paper where they set a world record for the fastest
    genome sequencing and this deep consensus transformer architecture was used in
    that SM sequence and so in this particular study they were able to very quickly
    diagnose that Matthew over here had a heart condition due to a genetic reasons
    and so they were very quickly able to like put Matthew on the patient' donors
    list over here so that's a kind of real world impact we can have with these biomedical
    transform models and AI systems in general。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当你观察质量时，发现它比这里的普通 CCS 算法有了相当大的改善。每次读取的准确性在这里提高了很多。因此，你可能会问这种模型在现实世界中的实际影响是什么？答案是，这种模型已经在现实世界中被使用，例如在斯坦福大学，由艾什利博士和其他几位研究人员进行的基因组研究中，最近有一篇超快速纳米测序的论文，他们创下了最快基因组测序的世界纪录，而这种深度共识变换器架构在那次
    SM 测序中得到了应用。在这项特定研究中，他们能够非常迅速地诊断出这里的马修因遗传原因有心脏疾病，因此他们能够迅速地将马修列入患者供体名单。这就是我们可以通过这些生物医学变换模型和
    AI 系统在现实世界中产生的影响。
- en: And very quickly， the last paper that I want to talk about is this paper from
    Deep Mind on effective gene expression prediction from sequences by integrating
    long range interactions this was published in nature methods。And the motivation
    for this work is again that since the human genome Project there have been thousands
    of genome by association style hits where the goal is to you know map genetic
    variants to different kind of disease phenotypes but a lot of this involves experimentation
    and experimentation like real experiment patient takes a lot of time and so if
    we can like do that with machine learning models that's really and so that's what
    they set out to do in this paper and。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: So if you look at the gene itself， there are like， you know。10% of the gene
    are going to be like coding variants and these influence protein function。And
    then the way they can cause diseases is by disrupting the structure of proteins
    that are generated or by affecting the protein protein interactions。The good part
    about these coding variants are they tend to be closer to the gene and so they're
    easier to interpret on the other hand the 90% of the gene is like noncoding variants
    and the way they work is they influence protein expression so they are more like
    regulatory sequences and so the way they can lead to diseases if they have any
    variants is by disrupting the transcription of proteins。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所以如果你看基因本身，像是，知道的。10%的基因是编码变异，这些变异会影响蛋白质功能。然后它们导致疾病的方式是通过破坏生成的蛋白质的结构或者影响蛋白质之间的相互作用。这些编码变异的好处在于它们通常更靠近基因，因此更容易解释；另一方面，90%的基因是非编码变异，它们的作用是影响蛋白质的表达，因此更像是调控序列。如果它们有任何变异，它们导致疾病的方式是通过干扰蛋白质的转录。
- en: And given that these noncoding variants can be very。very far away from the gene
    and the coding variants。it's very difficult to interpret them and so the question
    is can we train transform models that can predict the influence of these noncoding
    variants and so that is the task over here and so this is a visualization again
    so the paper again looks at it focuses on transcription which is the first step
    in terms of converting DNA into RNA and the way this is done is you have RNA polymerase
    which gets recruited at the beginning of the gene by these proteins called transcription
    factors and these transcription factors are a binding side which correspond to
    these promoters which are quite close to the gene but then you also have these
    enhances which can be like very very far away from these promoters in terms of
    the linear space also influencing this transcription and you may ask how can such。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 而考虑到这些非编码变体可能离基因和编码变体非常远，因此很难对它们进行解读，问题在于我们能否训练变换模型来预测这些非编码变体的影响，这就是这里的任务。这是一个可视化，再次说明，论文关注于转录，这是将DNA转化为RNA的第一步。实现这一点的方法是RNA聚合酶在基因的起始位置被称为转录因子的蛋白质招募，这些转录因子对应于靠近基因的启动子。而这些增强子可能在这些启动子和线性空间中非常远，也会影响转录。你可能会问，怎么会有这样的情况。
- en: How can these enhances influence the activity over here。this is because while
    they may be far away in the linear space when the sequence falls and in the 3D
    structure they will end up being quite close to each other and so that they can
    completely affect the transcription process over here。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些增强如何影响这里的活动。这是因为虽然它们在一维空间中可能相距较远，但在三维结构中，它们最终会非常接近，从而完全影响这里的转录过程。
- en: So it's a very high level overview of what's happening over here and in terms
    of the biology and so the question is if there are any variants in these noncoding
    variants and in these enhances they may like disrupt the transcription factor
    binding and this can internal turn lead to like you know no proteins and then
    finally to diseases right so we want to be able to predict that based on the DNA
    sequences that have been generated。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是对这里发生的事情以及生物学方面的一个非常高层次的概述。那么问题是，这些非编码变异和增强子中是否存在任何变异，它们可能会干扰转录因子的结合，这最终可能导致没有蛋白质，最终导致疾病，对吧？所以我们希望能够基于生成的DNA序列来预测这一点。
- en: So the problem is kind of quite straightforward， it's a supervised learning
    problem。the setup is predict experimental data from these DNA sequences and this
    can take many different forms。the primary one is gene expression over here， but
    then there are also other tasks such as DNA accessibility。histone modifications
    and transcription factor binding and so on and so forth。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以问题其实相当简单，这是一种监督学习问题。设置是从这些DNA序列中预测实验数据，这可以有很多不同的形式。主要的是这里的基因表达，但也有其他任务，如DNA可及性、组蛋白修饰和转录因子结合等等。
- en: So as you can imagine the baseline model for this task for many years was the
    CNN model and as you stack up a little different CNNL layers you can increase
    the receptive field but there's a limit to that so in this work what they showed
    was you can use transformers instead and do better modeling of these long interactions
    so the final model is called Ener which is a combination of this enhancer and
    transformer and so if you look at the model itself it has a few CNN layers at
    the beginning but then it has a bunch of transformer blocks that are stacked together。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如你所想，这项任务多年来的基础模型是CNN模型，随着你堆叠一些不同的CNN层，你可以增加感受野，但这有其限制。因此，在这项工作中，他们展示了可以使用变换器来更好地建模这些长时间交互，因此最终模型被称为Ener，它是这种增强器和变换器的结合。如果你查看模型本身，它在开始时有一些CNN层，但随后有一堆堆叠在一起的变换器块。
- en: And the input is 200 KB DNA sequences and there are approximately 30k examples
    that have been trained and the output is like genomic tracks of this RNA expression
    with。and they have organism specific heads， so one for humans and one for mouse。And
    finally one key detail is that relative position encodings that were used in this
    model were actually very key and these relative position encoings were modeling
    this power law of interactions and as a result of you know using these relative
    position encodings with the transformer block architecture。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是200 KB的DNA序列，训练了大约30k个示例，输出类似于这种RNA表达的基因组轨迹。并且它们有特定于生物的头部，人类一个，小鼠一个。最后一个关键细节是，这个模型中使用的相对位置编码实际上非常重要，这些相对位置编码在建模交互的幂律方面起到了作用，因此在使用这些相对位置编码与变换器块架构时。
- en: they were now able to model like interactions over 100 k based byes away。And
    so you see that in the results over here， so you have the experimental data in
    green and you can see the CNN baseline over here and you see that as soon as you
    go far away you see that the CNN model is no longer able to capture these gene
    expressions。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 他们现在能够对超过100k基因的相互作用进行建模。因此你可以在这里看到结果，实验数据以绿色显示，你可以看到CNN基线在这里，一旦远离，你会发现CNN模型无法再捕捉这些基因表达。
- en: but you can see that the enhancer model is now able to like pick them up so
    you can see that as the model goes far away。the enhancer model is able to capture
    this whereas CNN model is no longer able to capture this。And finally， one。I think
    very interesting experiment that they had in the paper was they were also able
    to like you know predict promoter enhancer influences and that prediction was
    actually on par with experimented data so this suggests that using this machine
    learning model we can sidestep a lot of these wet lab experiments and get like
    key details。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你可以看到增强模型现在能够像是拾起它们一样，所以你可以看到随着模型的远离，增强模型能够捕捉到这一点，而 CNN 模型则无法再捕捉到这一点。最后，我认为他们在论文中进行的一个非常有趣的实验是，他们也能够预测启动子-增强子影响，而这种预测实际上与实验数据相当，所以这表明使用这个机器学习模型我们可以绕过许多湿实验，并获得关键细节。
- en: which could be super useful。So yeah， so very quickly。I'm sorry I had to like
    cram through proteins and genomics applications over here。but I think what you
    would see is that overall when you look at clinical proteins and genomic applications。we
    see that transformers have incredible potential in biomedicine。And with clinical
    applications。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能非常有用。所以是的，非常快。我很抱歉我必须在这里匆忙讲解蛋白质和基因组应用。但我认为总体来看，当你观察临床蛋白质和基因组应用时，我们看到变压器在生物医学中具有巨大的潜力，并且在临床应用中。
- en: I think the challenges are perhaps more centered around data and evaluation。but
    on the proteins and genomics side， I think there are some extremely interesting
    opportunities to innovate on the architecture here。And finally as I said， there
    are incredible bidirectional learning opportunities。I think the problem of you
    know modeling long range interactions that's useful beyond proteins beyond genomics。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为挑战可能更集中在数据和评估方面。但在蛋白质和基因组学方面，我认为在这里有一些极其有趣的创新机会。最后，正如我所说，这里有令人难以置信的双向学习机会。我认为建模长程相互作用的问题不仅在蛋白质和基因组学领域有用。
- en: I think it's useful in genomics and so I think any architecture improvement
    over here can inspire wider progress in AI so I think that's a big reason to work
    on this。嗯。Any questions so far？Sorry， I covered a lot of ground over here， apologies
    for that。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这在基因组学中是有用的，因此我认为这里的任何架构改进都能激励AI的更广泛进步，这也是我认为要致力于此的一个重要原因。嗯。到目前为止有什么问题吗？抱歉，我讲了很多内容，对此表示歉意。
- en: but I think these are super cool papers and you should go back and read them。mSo
    quite finally。I want to maybe spend a couple of minutes touch upon how I see the
    future of biomedical AI evolving。Overall， I believe it's not a question of。Like
    if AI will transform biomedicine。I think it's rather a question of when and how。And
    I think the very specific thesis I have over here is given the nature of biomedical
    data and how multimodal in nature and with all the progress in transformers self
    learning large language models。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为这些论文非常酷，你应该回去阅读它们。最终，我想花几分钟谈谈我对生物医学人工智能未来发展的看法。总体来说，我相信这不是一个关于人工智能是否会改变生物医学的问题。我认为这更是一个关于何时以及如何的问题。我的具体观点是，鉴于生物医学数据的性质及其多模态特征，以及在自我学习大型语言模型方面的所有进展。
- en: I think we have an incredibly powerful like framework to like leverage all this
    richness at scale and like truly build foundational medical AI models。So I think
    that is incredibly exciting。And。So I'm not I think it's you've already been in
    Hawaiias for far too long。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们拥有一个极其强大的框架，可以充分利用这一切丰富性，并真正构建基础的医疗 AI 模型。因此，我认为这非常令人兴奋。而且。所以我认为你在夏威夷待得太久了。
- en: so I'm not going to ask you to recognize these people。but they're actually famous
    physician science centers some of them went on to win Nobel prizezes and so。I
    think what I want to say over here is there's no reason for a scientist to be
    different from a physician they can be combined together and that's what also
    want to like convey with our AI systems as well we don't have to like separate
    clinical applications and biological applications。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我不会要求你认识这些人。但他们实际上是著名的医学科学中心，其中一些还获得了诺贝尔奖。我想在这里说的是，科学家和医生没有理由是不同的，他们可以结合在一起。这也是我想通过我们的人工智能系统传达的，我们不必将临床应用和生物应用分开。
- en: I think when we combine them together we are going to discover a lot of new
    insights and I think that's going to accelerate biomedical research and internally
    to new discoveries and which is going to like you know be used to eradicate diseases。advanced
    human health plan and generally drive human potential forward。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为当我们将它们结合在一起时，我们将会发现许多新的见解，我认为这将加速生物医学研究并推动新的发现，这将用于消灭疾病。推动先进的人类健康计划，并整体上推动人类潜能向前发展。
- en: So question I actually know these three are。Sure， I think the right most one
    is Alexander Fleming and then Jonas Sak and then Paula Liick。So fleming is penicillin，
    salt， Capoo， and Elich was a bunch of different stuff。And。And so maybe I ask this
    question to all of you， which field of AI do you think will which field do you
    think AI will win the first Nobel Priing？
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我实际上知道这三位是谁。没错，我认为最右边的是**亚历山大·弗莱明**，然后是**乔纳斯·索尔克**，接着是**保拉·里克**。所以弗莱明是青霉素，盐，Capoo，艾利希则涉及许多不同的东西。然后，我想问你们一个问题，哪一个领域的人工智能你认为会首先赢得诺贝尔奖？
- en: You're an not going。嗯。I mean think but like。Satan noble noble was like it's
    not a real field I these people abroad。I think economics is。it's associated with
    the Noble Foundation no I think was like this is a joke and not worry these people
    and then it's almost like wait wait own economic noble bias what was biology that
    like can have like oh gay render this joke that do cancer or something。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你不是在说。嗯。我是说想，但就像。撒旦的诺贝尔，诺贝尔就像这不是一个真正的领域，我觉得这些人出国。经济学我觉得是。它与诺贝尔基金会有关，不，我觉得就像这是个笑话，不用担心这些人，然后几乎就像等一下，自己的经济学诺贝尔偏见，生物学是什么，像是可以有哦，搞笑的渲染这个笑话，癌症或者其他什么。
- en: That's right so I also feel the same way and I hope the overwhelming majority
    of you also think that it's going to be in medicine and I'm going to end on that
    note huge thank you to all my collaborators and teammates for most importantly
    allowing me to like ST slides and then also thank you to all of you for like patiently
    listening over here hopefully this was helpful。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，我也有同样的感觉，我希望你们中的绝大多数也认为这将会出现在医学领域，我将以此为结束，非常感谢我的所有合作伙伴和团队成员，最重要的是感谢你们让我像使用ST幻灯片一样，感谢你们耐心地倾听，希望这对你们有所帮助。
- en: '![](img/867d42af88a583402ff132cc4ae7c3fa_3.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/867d42af88a583402ff132cc4ae7c3fa_3.png)'
