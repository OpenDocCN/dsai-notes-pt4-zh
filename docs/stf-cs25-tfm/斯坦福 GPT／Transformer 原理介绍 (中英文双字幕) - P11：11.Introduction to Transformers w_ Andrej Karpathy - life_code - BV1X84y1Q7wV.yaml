- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P11：11.Introduction to Transformers w_ Andrej
    Karpathy - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT/Transformer 原理介绍 (中英文双字幕) - P11：11. 变压器介绍与 Andrei Karpathy - life_code
    - BV1X84y1Q7wV
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_0.png)'
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_1.png)'
- en: Hi everyone， welcome to CS25 Pro UnitedV。This was a course that was held at
    Stanford in the winter of 2023。😊，This course is not about robots that can transform
    into cars， as this picture I suggest， rather。it's about deep learninging models
    that have taken the world by the storm and have revolutionized the field of AI
    and others。Starting from natural language processing， transformers have been applied
    all over from compvis。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 大家好，欢迎来到CS25 Pro UnitedV。这个课程在2023年冬季于斯坦福举办。😊 这个课程并不是关于可以变成汽车的机器人，而是关于深度学习模型，它们引起了世界的轰动，彻底改变了人工智能等领域。从自然语言处理开始，变压器已广泛应用于计算机视觉等领域。
- en: enforcementment learning， biology， robotic， etc。We have an exciting set of videos
    lined up for you with some truly fascinating speakers。skip talks。Presenting how
    they're applying transformers to the research in different fields and areas。We
    hope。You'll enjoy and learn from these videos。So without any furthereddo， let's
    get started。This is a purely introductory lecture。And we'll go into the building
    blocks of transformers。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习、生物学、机器人等。我们为您准备了一系列令人兴奋的视频，邀请了一些真正迷人的演讲者。跳过演讲。展示他们如何将变压器应用于不同领域的研究。希望您能享受并从这些视频中学习。所以不再拖延，让我们开始。这是一场纯粹的入门讲座。我们将深入探讨变压器的构建模块。
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_3.png)'
- en: So first， let's start with introducing the instructors。So for me。I'm currently
    on a temporary diploma from the PhP program and I'm leading here at a robotics
    startup。collaborativeative robotics working on some general purpose robots， somewhat
    like a the。and yeah I'm very passionate about robotics and building assist learning
    algorithms。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所以首先，让我们介绍一下讲师。我目前在PhD项目中暂时挂名，并在一家机器人初创公司担任领导，致力于一些通用机器人，类似于这个。我对机器人和构建辅助学习算法充满热情。
- en: my research interests in in personalsonal lending ands in remote modeling and
    I have a bunch of publications in the robotics government driving other areas。Undergrad
    close at Cornell， it's someones book Cornell， it's an estimate call。So I'm Stephen
    a Gr Fer CSP speaker， Per did my master's at CMU and an undergrad。Mainly into
    NLP research， anything involving language and text。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我在个人贷款和远程建模方面的研究兴趣，以及在机器人、政府、驾驶等领域有很多出版物。大学时在康奈尔，某本书提到康奈尔，这是一个估算的呼叫。所以我是一名CSP演讲者，完成了CMU的硕士和本科学位。主要从事自然语言处理研究，涉及语言和文本的任何内容。
- en: but more recently I've been getting more into computer vision as well as Mon。And
    just some stuff I do for fun， a lot of music stuff made piano。some self promo
    but I post a lot on my ins YouTube and TikTook so if you guys want to check it
    out my friends and I are also starting a Stanford piano club so if anybody's interested
    feel free to email with Y for details other than that you know martial arts。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 但最近我对计算机视觉和一些娱乐项目也越来越感兴趣。我有很多音乐相关的活动，制作了钢琴曲。有一些自我宣传，但我在Instagram、YouTube和TikTok上发布了很多内容。如果你们有兴趣，我和我的朋友们也在成立一个斯坦福钢琴俱乐部，所以如果有人感兴趣，可以随时发邮件询问Y的详细信息，除此之外，我也练习武术。
- en: bodybuild。P're trying a pig dramas anime。Occasional game。😀。Yeah。Okay cool yeah。so
    my name'sryland the same talk about myself I just want to very briefly say that
    I'm super excited to take this class I the last time was sorry to teach this class
    me I the last time it was offered of I thought we brought a really great group
    of speaker last time I'm super excited for this offering and yeah I'm thankful
    you're all here and I'm looking forward to a really fun quarter you was the most
    out speakak outspoken student last year and so someone wants to become instructor
    next year in。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在尝试一些动画、偶尔玩游戏。😀 好的。我的名字是Ryland，简单介绍一下自己。我想简短地说，我非常兴奋能参加这个课程。上次我教这门课时，我觉得我们带来了一个非常优秀的演讲者团队。我对这次课程感到非常期待，谢谢大家的到来，我期待一个非常有趣的学期。你是去年最活跃的学生，明年有意成为讲师。
- en: Okay。😊，Go。Let's see if okay a few minutes。So what we hope you will learn in
    this class is first of all。how do transforms work？how they being applied just
    don cannot and now days like we are pretty much in them everywhere in the AI machine
    learning and what are some new interesting directions of research in the topics。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。😊，继续。让我们看看，如果有几分钟。我们希望你在这堂课中学到的第一件事是，变压器是如何工作的？它们是如何被应用的，现如今在人工智能和机器学习中无处不在，以及在这些主题中的一些新有趣研究方向。
- en: Co so this class is just an introductor， just talking about the basics of transformers
    introducing them。talking about a self potential mechanism on which they are founded
    and will do a deep dive more on like a model like to GPT so get。Happy to get solid。Okay，
    so let me start with presenting the attention timeline。Attention all started with
    this wall paper attention is all by wasman L in 2017 that was the being transformers
    before that we had the field story error where we had models like RNM LSTMs and
    simple attention mechanisms that didn't involve for scale depot。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这堂课只是一个介绍，只是谈论变压器的基础，介绍它们。讨论一个自我潜力机制，这是它们的基础，我们将深入探讨像GPT这样的模型。所以，准备好得到扎实的知识。好的，让我开始展示注意力时间线。注意力的所有一切始于2017年这篇壁纸论文，注意力是由Vaswani等人提出的，在那之前我们有许多模型，如RNN、LSTM和简单的注意机制，这些并没有涉及规模部署。
- en: Start in 2017 we solve this explosion of transformers into NLP where people
    started using it for everything。I even heard the support from Google as like our
    performance increased every time we fight our linguist。诶。For the course 2018 after
    2018 to 2020， we saw this explosion of customers into other fields like vision。😊，Bch
    of other the stir and like biology and last year 2021 was the start of the geneticative
    error where we got like a lot of genetic modeling started like models like Kox。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从2017年开始，我们在NLP领域解决了变压器的爆炸，大家开始将其应用于所有事物。我甚至听说谷歌的支持，每次我们与语言学家合作时，性能都有所提升。嗯。在2018年之后，2020年，我们看到客户在其他领域（如视觉）中的激增。😊，还有生物学，去年2021年是基因生成错误的开始，我们开始了很多基因建模，比如Kox模型。
- en: GT， Dali， stable equations to a lot of things happening in genetic modeling
    and we start scaling up in AI and now the present。So this is 2022 and like the
    startup in 23 and now we have almost like a chattyy whisper a bunch of others
    and we are like scalinging onward without spell out so that's great。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GT，Dali，稳定方程与许多在基因建模中发生的事情相关，我们开始在人工智能上扩展，现在是2022年，像23年的初创公司，现在我们几乎有一群其他的“喋喋不休”的声音，我们在不断地扩展，这很好。
- en: so that's the future。So going more into this。So once there were audit。So we
    had two models， LTN。GIU。What worked here was the day of good at ending history。😊。But
    what did not work was they didn encode long sequences and they were very bad at
    encoding content。So consider this example。Consider trying to predict the last
    word in the text。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是未来。深入讨论一下。所以一旦进行了审计，我们有两个模型，LTN和GIU。这里有效的是它们擅长结束历史。😊。但无效的是它们未能编码长序列，且编码内容的能力非常差。所以考虑这个例子。试着预测文本中的最后一个词。
- en: I grew up in France dot dot dot， I speak fluent D。Here you need to understand
    the context for it to predict French and take attention mechanism is very good
    at that。whereas if you're just using LSTMs， it doesn't work that。Another thing
    transformers are good at is。U。More based on content is like also context prediction
    is like finding attention maps if I have something like a word like it。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我在法国长大……我流利地说D。在这里你需要理解上下文才能预测法语，而注意力机制对此非常有效。而如果你只是使用LSTM，就行不通。变压器擅长的另一件事是，基于内容的上下文预测，像是找到注意力图，如果我有像“它”这样的词。
- en: what now does it collect and we can give like a probability attention on what
    are the possible activations and this works are better than existing mechanisms。Okay。So
    where we were in 2021， we were on the verge of takeoff。We were starting to realize
    the potential of transformers in different fields。we solved a lot of long sequence
    problems like protein folding， Al fold， offline Arl。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在它收集了什么，我们可以给出可能激活的概率关注，这些工作比现有机制更好。好的。所以在2021年，我们快要起飞了。我们开始意识到变压器在不同领域的潜力。我们解决了许多长序列问题，如蛋白质折叠、Al
    fold、离线Arl。
- en: We started to see zero short generalization we saw multimodal tasks and applications
    like generating images from language so that's all dli yeah and it feels like
    Asia shared but person like two years ago。And this is also a talk on transformers
    that can watch in give。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始看到零样本泛化，我们看到了多模态任务和应用，比如从语言生成图像，这一切都与DLI有关，是的，感觉像是亚洲两年前分享过的内容。这也是关于变压器的一个讨论，可以观看并提供。
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_5.png)'
- en: Yeah。Co and this is where we was going from 2021 to 2022。which is we have gone
    from the verge of taking off to actually taking off and obviously we are seeing
    unique applications in audio generation art。music sort towering we are starting
    to see reasoning capabilities like common sense。logical reasoning， mathematical
    reasoning。We are also able to now get human enlightenment and interaction。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。这就是我们从2021年到2022年所经历的变化，我们已经从起飞的边缘真正起飞，显然我们在音频生成、艺术和音乐等领域看到了独特的应用。我们开始看到推理能力，如常识、逻辑推理和数学推理。我们现在也能够进行人类启发和互动。
- en: they're able to use reinforcement learning and human feedback that's how tragedy
    is trained to perform really good we have a lot of mechanisms for controlling
    toxicity bias and ethics now and a lot of also a lot of developments in other
    areas like different models。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 他们能够利用强化学习和人类反馈，这就是悲剧训练得以表现良好的原因。我们现在有许多机制来控制毒性、偏见和伦理问题，同时在其他领域也有许多发展，比如不同的模型。
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_7.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_7.png)'
- en: 够。😡，啊。So the feature is a spaceship and we are all excited about it。😊。And there's
    a lot of more acquisition that we can enable and it'll be great if you can see
    transformers also work here one big example is for your understanding and generation
    that is something that everyone is interested in and I'm hoping we'll see a lot
    of models in this area this year also finance business。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 够。😡，啊。所以这个功能就像是一艘飞船，我们都对此感到兴奋。😊。还有很多其他收购，我们可以启用，如果你能看到变压器也在这里工作就更好了，一个很好的例子就是你的理解和生成，这是每个人都感兴趣的，我希望今年在这个领域能看到很多模型，包括金融和商业。
- en: 😊，I'll be very excited to see GT author novel， but we need to solve very long
    sequences modeling and most transformative models are still limited to like 4000
    opens or something like that so we need to do a。Make them general much more better
    on long sequences we are also we also want to have general agents that can do
    a lot of multitask。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我会非常兴奋看到GT创作小说，但我们需要解决非常长的序列建模问题，而大多数变换模型仍然限制在大约4000个开放词左右，因此我们需要改进它们，使它们在长序列上表现得更好。我们也希望拥有能够执行多任务的通用代理。
- en: Amatic input。Predictions like Goto and so I think we will see more of that too
    and finally we also want domain specific models。so you might want like a GP models
    that's good at like maybe like help so that could be like a doctor GPT model you
    might have like a large GP model that's like on only on raw data so currently
    we have like GP models that are trained on every but we might start to see more
    niche models that are like good at one task and we could have like a mixture of
    expert and think like this is like how you normally consult an expert will' have
    like expert A models and you can go to a different air models for your different
    needs。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Amatic输入。像Goto这样的预测，所以我认为我们也会看到更多这些，最后我们还希望有特定领域的模型。所以你可能希望有一个在某些方面表现良好的GP模型，比如医生GPT模型，或者一个仅基于原始数据的大型GP模型。目前我们有的GP模型是在各种数据上训练的，但我们可能会开始看到更多专注于单一任务的细分模型，我们可以像咨询专家一样，有专家A模型，可以根据不同需求去不同的模型。
- en: Yeah。There still a lot of missing ingredients to make this all successful the
    first of all is external memory we are already starting to see this with models
    like ChaGPT where the interactions are short lived。there's no long term memory
    and they don't have ability to remember stored conversations for long term and
    this is something we want to fix。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。要使这一切成功仍然缺少很多要素，首先是外部记忆，我们已经开始在像ChaGPT这样的模型中看到这一点，交互是短暂的。没有长期记忆，也无法长期记住存储的对话，这正是我们想要解决的问题。
- en: 😊，Second over second is reducing the computation complexity。so attention mechanism
    is quadtic over the sequence length which is slu and we want to reduce itra make
    it faster。诶。Another thing you want to do is we want to enhance the controllability
    of these models like a lot of these models can be stochastic and we want to be
    able to control what sort of outputs we get from them and you might have experienced
    the charge if you just refresh you get like different output each time。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，每秒每秒都在减少计算复杂度。因此，注意机制在序列长度上是二次的，我们想要减少它，使其更快。诶。你还想做的另一件事是增强这些模型的可控性，因为很多模型可能是随机的，我们希望能够控制我们从中获得的输出类型，你可能体验过这种变化，只需刷新一次，每次得到的输出都不一样。
- en: but you might want to have mechanism black printers， what sort of things you
    can。And finally we want to align our state of art language models with how the
    human brain works and we are seeing the search。but we still need more research
    on seeing how they can be more important。可系哼。😊，Yes。I'm excited to be here I live
    very nearby so I got the invites to come to class and I was like。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能想要有机制黑箱打印机，看看你能做些什么。最后，我们希望将我们最先进的语言模型与人脑的工作方式对齐，我们正在看到搜索，但我们仍然需要更多研究，看看它们能如何更重要。可系哼。😊，是的。我很高兴在这里，我住得很近，所以我收到了来上课的邀请，我就像。
- en: okay I'll just walk over but then I spent like 10 hours on the slides so it
    wasn't as simple。So yeah I want to talk about transformers， I'm going to skip
    the first two over there。we're not going to talk about those we're going to talk
    about that one just to simplify the lecture here since we've done have time。![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_9.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我就走过去，但我在幻灯片上花了大约10个小时，所以这并不简单。所以，是的，我想谈谈变压器，我将跳过前两个。我们不会讨论那些，我们将讨论这个，以简化讲座，因为我们没有时间。![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_9.png)
- en: 嗯。Okay， so I wanted to provide a little bit of context of why does this transformers
    class even exist so a little bit of historical context I feel like Bbo over there
    I joined like telling you guys about this I don't know if you guys saw the drinks
    and basically I joined AI in roughly 2012 and full course so maybe a decade ago
    and back then you wouldn't even say that you joined AI by the way that was like
    a dirty word now it's okay to talk about but back then it was not even deep learning
    it was machine learning that was a term use if you were serious but now now AI
    is okay to use I think。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。好的，我想提供一些背景，解释为什么这个变压器类存在，所以我觉得有一点历史背景。我加入AI大约在2012年，全职课程，所以大约十年前。那时候你甚至不会说你加入了AI，那个词有点脏，现在谈论这些是可以的，但那时甚至没有深度学习，只有机器学习，这是一个认真的术语。但现在AI可以使用，我觉得。
- en: So basically do you even realize how lucky you are potentially entering this
    area and roughly 2203 so back then in 2011 or so when I was working specifically
    on computer vision。![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_11.png)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，基本上你有没有意识到你有多幸运，潜在地进入这个领域，大约在2203年。那时候，大约在2011年，我专门从事计算机视觉的工作。![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_11.png)
- en: Your your pipelines looked like this， so you wanted to classify some images
    you would go to a paper and I think this is representative you would have three
    pages in the paper describing all kinds of a zoo of kitchen sink of different
    kinds of features。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你的管道看起来像这样，你想分类一些图像，你会去查论文，我觉得这很有代表性，论文中会有三页描述各种不同特征的“厨房水槽”。
- en: descriptors and you would go to poster session and in computer vision conference
    and everyone who have their paper feature descriptors that they're proposing and
    it's totally ridiculous and you would take notes on like which one you should
    incorporate into your pipeline because you would extract all of them and then
    you would put an SVM on top so that's what you would do so there's two pages make
    sure you get your spars sip histograms。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 描述符，你会去计算机视觉会议的海报展，每个人都有他们提议的论文特征描述符，这简直是荒谬的。你会记录哪些应该纳入你的管道，因为你会提取所有这些特征，然后在上面放一个支持向量机（SVM），所以你会这么做。确保你获取你的稀疏直方图。
- en: your SSIs your color histograms， textiles， tiny images and don't forget the
    geometry specific histograms。all of them had basically complicated code by themselves
    you're collecting code from everywhere and running it and it was a total nightmare
    so。On top of that it also didn't work so this would be I think representative
    prediction from that time you would just get predictions like this once in a while
    and you'd be like you just shrug your shoulders like that just happens once in
    a while today you would be looking for a bug。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你的SSIs是颜色直方图、纺织品、微小图像，别忘了几何特定直方图。它们本身的代码基本上是复杂的，你从各处收集代码并运行，完全是一场噩梦。所以，更糟的是它还不管用，我认为这代表了那时的预测，你偶尔会得到这样的预测，你只能耸耸肩，觉得这就偶尔发生，而今天你会寻找bug。
- en: And worse than that。Every single every single sort of feel every single chunk
    of AI had their own completely separate vocabulary that they work with。so if if
    you go to NLP papers， those papers would be completely different so you're reading
    the NLP paper and you're like what is this part of speech tagging morphological
    analysis syntactic parsing coreference resolution what is NPBT JJ and you're confused
    so the vocabulary and everything was completely different and you couldn't read
    papers I would say across different areas？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟的是，每个AI领域都有自己完全独立的词汇。如果你去看NLP论文，这些论文会完全不同，你在读NLP论文时，可能会疑惑“这是什么？词性标注、形态分析、句法解析、指代消解，NPBT、JJ又是什么？”因此，词汇和一切都是完全不同的，你几乎无法跨领域阅读论文。
- en: So now that changed a little bit starting in 2012 when Askochevsky and colleagues
    basically demonstrated that if you scale a large neural network on large data
    set you can get very strong performance and so up till then there was a lot of
    focus on algorithms but this showed that actually neural net scale very well so
    you need to now worry about compute and data and if you scale it up works pretty
    well and then that recipe actually did copy paste across many areas of AI so we
    started to see a neural networks pop up everywhere since 2012 so we saw them computer
    vision and NLP and speech and translation in RL and so on so everyone started
    to use the same kind of modeling tool modeling framework and now when you go to
    NLP and you start reading papers there in machine translation for example。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在2012年发生了变化，当Askochevsky及其同事展示了如果在大型数据集上扩展大型神经网络，可以获得很强的性能。到那时，大家对算法的关注很多，但这表明神经网络扩展得很好，所以你现在需要关注计算和数据，如果你扩大规模，它的效果相当不错。这种方法也在AI的许多领域复制粘贴，因此自2012年起我们开始看到神经网络在各处出现，包括计算机视觉、自然语言处理、语音、翻译和强化学习等。因此，每个人开始使用相同的建模工具和框架，而现在当你去阅读NLP领域的论文，比如机器翻译时。
- en: this is a sequence of sequence of paper which will come back to in a bit you
    start to read those papers and you're like okay I can recognize these words like
    there's a neural network there's some parameters there's an optimizer and it starts
    to read like things that you know of so that decreased tremendously the barrier
    to entry across。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个论文的序列，我们稍后会再提到。当你开始阅读这些论文时，你会发现“哦，我能识别这些词：有神经网络、有一些参数、有一个优化器”，它开始读起来像你熟悉的内容。这大大降低了进入不同领域的门槛。
- en: The different areas。And then I think the big deal is that when the transformer
    came out in 2017。it's not even that just the toolkits and the neural networks
    were similar is that literally the architectures converge to like one architecture
    that you copy paste across everything seemingly so this was kind of an unassuming
    machine translation paper at the time proposing the transformer architecture but
    what we found since then is that you can just basically copy paste this architecture。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的领域。我认为关键在于2017年变压器（transformer）问世时。并不是说工具包和神经网络相似，而是架构确实趋向于一种架构，你可以在所有地方复制粘贴。那时这是一篇看似不起眼的机器翻译论文，提出了变压器架构，但自那时起我们发现，基本上可以随意复制粘贴这个架构。
- en: And use it everywhere and what's changing is the details of the data and the
    chunking of the data and how you feed in and you know that's a caric but it's
    kind of like correct first order statement and so now papers are even more similar
    looking because everyone is just using transformer and so this convergence was
    remarkable to watch and unfolded over the last decade and it's crazy to me what
    I find kind of interesting is I think this is some kind of a hint that we're maybe
    converging to something that maybe the brain is doing because the brain is very
    homogeneous and uniform across the entire sheet of your cortex and okay maybe
    some of the details are changing but those feel like hyperparmeters of like a
    transformer but your auditory cortex and your visual cortex and everything else
    looks very similar and so maybe we're converging to some kind of a uniform powerful
    learning algorithm here something like that I think is kind of interesting。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 并且将其应用于各个领域，变化的只是数据的细节、数据的分块方式以及如何输入数据。你知道这有些夸张，但这基本上是一个正确的一阶陈述，因此现在的论文看起来更加相似，因为每个人都在使用变压器，这种趋同在过去十年中令人瞩目。我发现有趣的是，我认为这或许暗示我们可能正在趋向某种大脑在做的事情，因为大脑在整个皮层的表面上是非常同质和均匀的。好的，也许一些细节在变化，但那些感觉像是变压器的超参数，你的听觉皮层、视觉皮层以及其他一切看起来都非常相似，因此我们也许正在趋向某种统一的强大学习算法，我觉得这一点相当有趣。
- en: Okay， so I want to talk about where the transformer came from briefly historically。So
    I want to start in 2003， I like this paper quite a bit it was the first sort of。Popular
    application of neural networks to the problem of language modeling so predicting
    in this case the next word in a sequence。which allows you to build generative
    models over text and in this case they were using multiier perceptron so a very
    simple neural the neural net took three words and predicted the probability distribution
    fourth word in a sequence So this was well and good at this point Now over time
    people started to apply this to a machine translation so that brings us to sequence
    to sequence paper from 2014 that was pretty influential and the big problem here
    was。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我想简要谈谈变压器的历史起源。我想从2003年开始，我非常喜欢这篇论文，这是神经网络首次被应用于语言建模问题的流行应用，因此在这种情况下预测序列中的下一个单词。这使你能够建立文本的生成模型，而在这种情况下，他们使用了多层感知器，所以这是一个非常简单的神经网络，它取三个单词并预测序列中第四个单词的概率分布。到目前为止，这很好。随着时间的推移，人们开始将其应用于机器翻译，这使我们回到了2014年的序列到序列论文，那篇论文相当有影响力，而这里的大问题是。
- en: okay we do just want to take three words and predict it for we want to predict
    how to go from an English sentence to a French sentence and the key problem was。okay
    you can have arbitrary number of words in English and arbitrary number of words
    in French so how do you get an architecture that can process thisvariably sized
    input。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们确实只想取三个单词并进行预测，我们想预测如何将英文句子转换为法文句子，关键问题是：好的，你可以在英语中有任意数量的单词，在法语中也有任意数量的单词，那么如何构建一个能够处理这种可变大小输入的架构呢？
- en: And so here they use a LSTM and there's basically two chunks of this。which are
    covered the Sla by the。まです。But basically have an encoder LSTM on the left and
    it just consumes one word at a time and builds up a context of what it has read。and
    then that acts as a conditioning vector to the decoder RN or LSTM that basically
    goes chunk chunk chunk for the next word in a sequence translating the English
    to French or something like that。Now the big problem with this that people identified
    I think very quickly and tried to resolve is that there's what's called this encoded
    bottleneck。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里他们使用了LSTM，基本上有两个部分，这些部分被Sla所覆盖。但基本上，左侧有一个编码器LSTM，它一次消费一个单词并建立它所阅读内容的上下文。然后它作为解码器RN或LSTM的条件向量，后者基本上逐个单词翻译序列中的下一个单词，将英语翻译成法语，或类似的东西。人们很快识别到的问题是所谓的编码瓶颈。
- en: so this entire English sentence that we are trying to condition on is packed
    into a single vector that goes from the encoder for the decoder and so this is
    just too much information to potentially maintain a single vector and that didn't
    seem correct and so people are looking around for ways to alleviate the attention
    of sorry the encoded bottleneck as it was called at time。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们试图进行条件处理的整个英文句子被打包成一个从编码器传递给解码器的单一向量，因此这可能包含的信息太多，无法维持一个单一的向量，这似乎不太正确，于是人们开始寻找缓解编码瓶颈（当时被称为编码瓶颈）的方法。
- en: And so that brings us to this paper， neural machine translation by jointly learning
    to align and translate。Here just quo from the abstract in this paper， we conjected
    that the use of a fixed length vector is a bottleneck in improving the performance
    of the basic encoded decoder architecture and proposed to extend this by allowing
    the model to automatically soft search for parts of the source sentence that are
    relevant to predicting target word yeah without having to form these parts or
    heart segments explicitly so this was a way to look back to the words that are
    coming from the encoder and it was achieved using this soft search so as you are
    decoding in the。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们回到这篇论文，神经机器翻译通过联合学习对齐和翻译。这是这篇论文摘要中的一段，我们推测使用固定长度向量是提高基本编码解码器架构性能的瓶颈，并建议通过允许模型自动soft搜索与预测目标词相关的源句子部分来扩展这一点，而不需要显式形成这些部分或核心片段，因此这是一种回顾来自编码器的词汇的方法，使用这种soft搜索实现了这一点，因此在解码时。
- en: The words here， while you are decoding them you are allowed to look back at
    the words at the encoder via this soft attention mechanism proposed in this paper
    and so this paper I think is the first time that I saw basically attention so
    your context vector that comes from the encoder is a weighted sum of the hidden
    states of the words in the in the encoding。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的词汇，当你在解码它们时，你可以通过这篇论文提出的soft attention机制回顾编码器中的词汇，因此我认为这篇论文是我第一次看到基本的注意力，因此来自编码器的上下文向量是编码中词汇的隐藏状态的加权总和。
- en: And then the weights of this come from a softmax that is based on these compatbilities
    between the current state as you're decoding and the hidden states generated by
    the encoder and so this is the first time that really you start to like look at
    it and this is the current modern equations of the attention and I think this
    was the first paper that I saw it in is the first time that there's a word attention
    used as far as I know to call this mechanism so I actually tried to dig into the
    details of the history of the attention so the first author here Dmitri I had
    an email correspondence with him and I basically sent him an email I'm like Dmitri
    this is really interesting transformers have've taken over where did you come
    up with the soft attention mechanism that ends up being the heart of the transformer
    and to my surprise he wrote me back this like massive email which was really fascinating
    so this is an excerpt from that email。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这里的权重来自一个基于当前状态和编码器生成的隐藏状态之间兼容性的softmax，因此这是你第一次真正开始关注它，这是现代注意力机制的当前方程，我认为我第一次看到这个概念的论文是首个使用“注意力”这个词的论文，据我所知，这是首次称这种机制的，因此我实际上试图深入了解注意力的历史，第一作者德米特里，我与他有过电子邮件交流，我基本上给他发了一封邮件，我说德米特里，这真有趣，变压器已经接管了，你是从哪里想到soft
    attention机制的，这个机制成为变压器的核心，令我惊讶的是，他回了我一封非常长的邮件，内容非常引人入胜，这里是那封邮件的摘录。
- en: 嗯。So basically he talks about how he was looking for a way to avoid this bottleneck
    between the encoder and decoder。he had some ideas about cursors that traversed
    the sequences that didn't quite work out and then here so one day I had this thought
    that it would be nice to enable the decoder RN to learn to search where to put
    the cursor in a source sequence this was sort of inspired by translation exercises
    that learning English in my middle school involved bulk you gaze shifts back and
    forth in the source and target sequence as you translate so literally I thought
    this was kind of interesting that he's not made English speaker and here that
    gave him an edge in this machine translation that led to attention and then led
    to transformer so that's really fascinating I expressed a soft search as softmax
    and that way the averaging of the ByN statess and basically to my great excitement
    this dis work from the very first try so really I think interesting piece of history
    and as it later turned out the name of RNN search was kind of blame so the better
    name attention came from Yohua on one of the final passes。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。因此，基本上他谈到他在寻找一种方法，以避免编码器和解码器之间的瓶颈。他有一些关于光标遍历序列的想法，但没有完全实现。然后有一天，我想到，让解码器RNN学习在源序列中放置光标的位置会很好，这种想法受到了我初中学习英语时翻译练习的启发，涉及到在源序列和目标序列之间大量的注视转移。所以字面上来说，我觉得这很有趣，他不是以英语为母语的人，这让他在机器翻译中占了便宜，导致了注意力机制的出现，进而导致了变换器的出现。这真是令人着迷，我将软搜索表达为softmax，通过这种方式对ByN状态进行平均，基本上让我非常兴奋的是，这项工作在第一次尝试时就成功了。所以我认为这是一个非常有趣的历史片段，后来发现RNN搜索这个名字有些不妥，最终的好名字“注意力”是来自约书亚的最后几次修改。
- en: As they went over the paper， so maybe attention is all you need would have been
    called like harnesss or just hold。But we have Yoshua Benio to thank for a little
    bit of better name I would say。so apparently that's the history of this subject
    that was interesting。Okay。so that brings us to 2017， which is attention is all
    unique so this attention component which in Dimetrius paper was just like one
    small segment and there's all this bidirectional RN RN and decoder and this attention
    on paper is saying okay you can actually delete everything like what's making
    this work well very well is just the attention by itself。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当他们回顾这篇论文时，也许“注意力就是你所需要的”会被称为“驾驭”或者“仅仅保持”。但我们要感谢**约书亚·本尼奥**为这个名字的改进。显然，这就是这个主题的历史，挺有趣的。好的。这让我们回到了2017年，“注意力就是唯一”的这个组件在迪梅特里乌斯的论文中只是一个小部分，还有所有这些双向RNN和解码器，而这篇论文上的注意力是说，实际上你可以删除所有东西，真正让这个工作顺利的就是注意力本身。
- en: And so delete everything， keep attention and then what's remarkable about this
    paper actually is usually you see papers that aren't very incommer they add like
    one thing and they show that it's better。but I feel like attention is all unique
    was like a mix of multiple things at the same time they were combined in a very
    unique way。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以删除所有东西，保留注意力，而这篇论文的显著之处在于，通常你会看到一些论文不太完整，他们添加一件事并展示出更好。但我觉得“注意力就是唯一”像是同时结合了多种东西，以一种非常独特的方式组合在一起。
- en: And then also achieve a very good local minimum in the architecture space。And
    so to me。this is really a landmark paper that。And it's quite remarkable and I
    think have quite a lot of work behind the scenes。So delete all the RNN just keep
    attention because attention is operates over sets and I'm going to go into this
    in a second。you now need to positionally encode your inputs because attention
    doesn't have the notion of space by itself。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在架构空间中也实现了一个非常好的局部最小值。因此对我来说，这真的是一篇具有里程碑意义的论文。这相当显著，我认为在背后有很多工作。因此删除所有RNN，只保留注意力，因为注意力是作用于集合的，我将在接下来详细讲解。你现在需要对输入进行位置编码，因为注意力本身并没有空间的概念。
- en: Theyて。You have to be very careful。They adopted this residual network structure
    from resonance。they interspersed attention with multilayer perceptrons。they used
    layer norms which came from a different paper they introduced a concept of multiple
    heads of attention that were applied to parallel and they gave us I think like
    a fairly good set of hyperparmeters that to this day are used so the expansion
    factor in the multilayer percept on goes up by forx and we'll go into like a bit
    more detail and this forex has stuck around and I believe there's a number of
    papers that tried to play with all kinds of little details of the transformer
    and nothing like sticks because this is actually quite good the only thing to
    my knowledge that stuck that didn't stick was this reshuffling of the layer norms
    to go into the prenorm version where here you see the layer norms are after the
    multi-headed attention or before they just put them before instead so just reshuffling
    of layer norms but otherwise the GPSTs and everything else that you're seeing
    today is basically the 2017 architecture from five years ago and even though everyone
    is working on it it's proven remarkably resilient which I think is relevant。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 他们。你必须非常小心。他们从共振中采用了这种残差网络结构。他们将注意力与多层感知器交错使用。他们使用了来自不同论文的层归一化，提出了应用于并行的多头注意力概念，他们给了我们一组我认为相当好的超参数，直到今天仍在使用，因此多层感知器中的扩展因子上升了四倍，我们将稍微深入一点，这个四倍的因子一直存在，我相信有许多论文尝试对变换器的各种细节进行调整，但没有什么像它一样有效，因为这个设计实际上非常好。根据我所知，唯一没有持久的变化是将层归一化重组为预归一化版本，在这里你可以看到层归一化在多头注意力之后或之前，他们只是把它们放在前面，所以只是层归一化的重新排列，但其他的GPST及你今天看到的所有内容基本上是五年前的2017年架构，尽管每个人都在对此进行研究，但它证明了相当强的韧性，我认为这很重要。
- en: There are innovations that I think have been adopted also in position encoings。it's
    more common to use different rotary and relative position encoings and so on。so
    I think there have been changes， but for the most part it's proven very resolute。So
    really quite an interesting paper now I wanted to go into the attention mechanism。And
    I think。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为有一些创新也被采用于位置编码。使用不同的旋转和相对位置编码变得更加常见等等。因此，我认为有一些变化，但大多数情况下它证明了非常稳定。所以这篇论文确实相当有趣，现在我想深入了解注意力机制。我认为。
- en: I sort of like the way I interpreted is not。Is not similar to the ways that
    I've seen it presented before so let me try a different way of like how I see
    it basically to me attention is kind of like the communication phase of the transformer
    and the transformer interleaves two phases。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我有点喜欢我解释的方式，不是。与我以前见过的展示方式不相似，所以让我试试另一种方式来描述我如何看待它。对我而言，注意力就像是变换器的沟通阶段，而变换器交错着两个阶段。
- en: the communication phase， which is the multiheaded attention and the computation
    stage。which is this multio perceptron or people form。So in the communication phase。it's
    really just a data dependent message passing on directed graphs。And you can think
    of it as okay forget everything with a machine translation and everything let's
    just we have directed graphs at each a node you are storing a vector and then
    let me talk now about the communication phase of how these vectors talk to each
    other in this directed graph and then the compute phase later is just a multi
    of receptron which now which then basically acts on every node individually but
    how do these node talk to each other in this directed graph so I wrote like some
    simple python。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 沟通阶段是多头注意力，而计算阶段是这个多层感知器或人们所形成的。因此，在沟通阶段，这实际上只是基于数据的消息传递在有向图上。你可以想象，忘掉机器翻译的一切，让我们在每个节点上存储一个向量，然后让我谈谈这些向量在这个有向图中如何互相沟通，计算阶段则只是一个多层感知器，它基本上对每个节点进行单独处理，但这些节点如何在这个有向图中相互沟通，所以我写了一些简单的Python代码。
- en: Like I wrote this in Python basically to create one round of communication of
    using attention as the message passing scheme。So here。A node has this private
    data vector as you can think of it as private information to this node。and then
    it can also emit a key a query and a value and simply that's done by linear transformation
    from this node。So the key is what are the things that I am。Sorry。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我在Python中写了这个，基本上是为了创建使用注意力作为消息传递机制的一个沟通轮次。所以这里，一个节点有这个私有数据向量，你可以把它看作是这个节点的私有信息。然后它还可以发出一个键、一个查询和一个值，这只是通过从这个节点进行线性变换来完成的。键是我是什么东西，抱歉。
- en: the query is one of the things that I'm looking for。the key is where are the
    things that I have and the value is one are the things that I will communicate。😡，And
    so then when you have your graph that's made up of nodes in some random edges。when
    you actually have these nodes communicating， what's happening is you loop over
    all the nodes individually in some random order and you are at some node and you
    get the query vector Q which is I'm a node in some graph and this is what I'm
    looking for and so that's just achieved via this linear transformation here。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 查询是我正在寻找的内容之一，键是我所拥有的内容，而值是我将要传达的内容。因此，当你拥有由一些随机边构成的节点图时，当这些节点实际进行通信时，发生的事情是你以随机顺序循环遍历所有节点，你在某个节点上，获得查询向量Q，这代表我是图中的一个节点，这是我正在寻找的内容，这只是通过这里的线性变换实现的。
- en: And then we look at all the inputs that point to this node。and then they broadcast
    where are the things that I have， which is their keys。😡。So they broadcast the
    keys， I have the query then those interact by dot product to get scores。so basically
    simply by doing dot product you get some kind of an unormalized weighting of the
    interestingness of all of the information in the notes that point to me and to
    the things I'm looking for and then when you normalize that with softbacks so
    it just sums to one。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们查看指向这个节点的所有输入，广播我所拥有的内容，也就是它们的键。因此，它们广播键，我有查询，然后这些通过点积相互作用以获得分数。通过点积，你基本上获得了指向我的所有信息的有趣性未归一化权重，以及我正在寻找的内容，然后当你用softmax进行归一化时，它只会总和为1。
- en: you basically just end up using those scores which now sum to one and our probability
    to distribution and you do a weighted sum of the values to get your update。So。I
    have a query， they have keys dot products to get interestingness or like affinity。softmax
    to normalize it and then weight at some of those values， flow to me and update
    me。And this is happening for each note individually and then we update at the
    end and so this kind of a message passing scheme is kind of like at the heart
    of the transformer and happens in a more vectorized batched way that is more confusing
    and is also inter interspersed with layer norms and things like that to make the
    training behave better。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，你最终会使用那些现在总和为1的分数，它们是概率分布，你对值进行加权求和以获得更新。因此，我有一个查询，他们有键的点积来获取有趣性或亲和力，使用softmax进行归一化，然后对这些值进行加权，流向我并更新我。这对于每个节点都是单独发生的，然后我们在最后进行更新，因此这种信息传递机制在变换器的核心，发生在更向量化的批处理方式中，这更复杂，并且还与层归一化等交错在一起，以使训练效果更好。
- en: but that's roughly what's happening in the attention mechanism I think on the
    high level。U。So yeah。so in the communication page of the transformer， then this
    message passing scheme happens in every head in parallel and then in every layer
    in series。And with different weights each time。And that's it as far as the multiheaded
    tension goes and so if you look at these encoder decoder models。you can sort of
    think of it then in terms of the connectivity of these nodes in the graph。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但这大致上就是我认为的注意力机制在高层次上的运作方式。所以，在变换器的通信页面中，这种信息传递机制在每个头中并行发生，然后在每一层中串行进行，每次都有不同的权重。这就是多头注意力的全部内容，因此如果你查看这些编码器-解码器模型，可以将其视为图中这些节点的连通性。
- en: you can kind of think of it as like okay all these tokens that are in the encoder
    that we want to condition on they are fully connected to each other so when they
    communicate they communicate fully when you calculate their features but in the
    decoder because we are trying to have a language model we don't want to have communication
    from future tokens because they give away the answer at this step so the tokens
    in the decoder are fully connected from all the encoder states and then they are
    also fully connected from everything that is before。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将其视为，编码器中的所有令牌都是彼此完全连接的，因此当它们计算特征时，进行完全通信，但在解码器中，由于我们试图构建一个语言模型，我们不希望有来自未来令牌的通信，因为它们在此步骤中泄露了答案，因此解码器中的令牌与所有编码器状态完全连接，同时它们也与所有在其之前的内容完全连接。
- en: And so you end up with this like triangular structure of in the directed graph
    but that's the message passing scheme that this basically implements and then
    you have to be also a little bit careful because in the cross attention here with
    the decoder you consume the features from the top of the encodeder so think of
    it as in the encoder all the nodes are looking at each other all the tokens are
    looking at each other many。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你最终会在有向图中得到这样一个三角结构，但这基本上就是消息传递机制的实现。然后你还需要稍微小心，因为在这里与解码器的交叉注意力中，你会使用来自编码器顶部的特征。因此，可以想象在编码器中，所有节点彼此之间都在相互关注，所有的令牌彼此之间也在关注许多。
- en: many times and they really figure out what's in there and then the decoder when
    it' it's looking only at the top nodes。So that's roughly the message passing scheme
    I was going to go into more of an implementation of a transformer。I don't know
    if there's any questions about this。哦，你问。Self attention and move by attention。But
    what is the the。不是。嗯。Yeah， so。Self attention and multi headeded detention so the
    multi headeded detention is just this attention scheme。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 多次，他们真正搞清楚里面有什么，然后解码器在查看顶部节点时。因此，这大致就是我想要深入讲解的消息传递机制。关于这一点我不知道有没有问题。哦，你问。自注意力和多头注意力。但是什么是那个。不是。嗯。是的，所以。自注意力和多头注意力，所以多头注意力就是这种注意力机制。
- en: but it's just applied multiple times in parallel multiple has just means independent
    applications of the same attention so。This message passing scheme basically just
    happens in parallel multiple times with different weights for the query key and
    value。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是以并行的方式多次应用，"多次"仅仅意味着对同一注意力的独立应用。因此，这个消息传递机制基本上就是在并行中多次发生，并且对于查询、键和值使用不同的权重。
- en: so you can always look at it like in parallel I'm looking for I'm seeking different
    kinds of information from different nodes and I'm collecting it all in the same
    node。It's all done in parallel。So heads is really just like copy paste in parallel
    and layers are copy paste。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你总是可以这样看：我在并行中寻找不同类型的信息来自不同节点，并且我把所有信息都汇集到同一个节点中。这一切都是在并行中完成的。所以头部实际上就像是在并行中的复制粘贴，而层就是复制粘贴。
- en: but in series。Maybe that makes sense。And self attention， when it's self attention。what
    it's referring to is that the node here produces each node here， so as I described
    it here。this is really self attention because every one of these nodes produces
    a key query value from this individual node。When you have cross attention。You
    have one cross attention here coming from the encoder。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 但它是串行的。也许这样说有意义。而自注意力，当提到自注意力时，指的是这里的每个节点生成这里的每个节点。因此，正如我在这里描述的，这确实是自注意力，因为这些节点中的每一个都从这个独立节点生成一个键、查询和值。当你有交叉注意力时，来自编码器的一个交叉注意力在这里。
- en: that just means that the queries are still produced from this node。but the keys
    and the values are produced as a function of nodes that are coming from the encoder。So
    I have my queries because I'm trying to decode some the fifth word in the sequence
    and I'm looking for certain things because I'm the fifth word and then the keys
    and the values in terms of the source of information that could answer my queries
    can come from the previous nodes in the current decoding sequence or from the
    top of the encoder so all the nodes that have already seen all of the encoding
    tokens many。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅意味着查询仍然是从这个节点生成的，但键和值是作为来自编码器的节点函数生成的。因此，我有我的查询，因为我试图解码序列中的第五个词，而我在寻找某些信息，因为我是第五个词，然后关于可以回答我查询的信息源的键和值可以来自当前解码序列中的前一个节点，或者来自编码器顶部，因此所有已见过所有编码令牌的节点许多。
- en: many times can now broadcast what they contain in terms of the information。So。I
    guess to summarize the self attention is kind of like。So cross attention and self
    attention only differ in where the piece and the values come from。either the piece
    and values are produced from this node or they are produced from some external
    source like like an encoder and the node over there。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 多次现在可以广播它们在信息方面所包含的内容。所以，我想总结一下，自注意力有点像。交叉注意力和自注意力只在于键和值的来源不同。要么键和值是从这个节点生成的，要么是从某个外部源生成的，比如一个编码器和那边的节点。
- en: 😡，But algorithmically is the same micro operations。好的。O。In the message passing
    graph paradigm the。So yeah， so。嗯我。So think of so each one of these nodes is a
    token。嗯。I guess like they don't have a very good picture of it in the transformer，
    but like。Like this node here could represent the third word in the output in the
    decoder。And in the beginning。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 😡，但算法上是相同的微操作。好的。O。在消息传递图范式中。所以是的，所以。嗯，我。所以想象一下，这些节点每一个都是一个标记。嗯。我想它们在变换器中的图像并不是很好，但像是。这个节点可以表示解码器输出中的第三个单词。起初。
- en: it is just the embedding of the word。嗯。And then。Okay。I have to think through
    this knowledge morning， I came up with it this morning。没有。Actually。I ca yesterday。我结果。Station。ごい。Notes
    have been blocked。We better。These notes are basically the vector。I'll go to an
    implementation， I'll go to the implementation。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是单词的嵌入。嗯。然后。好的。我得仔细思考一下这个知识，今天早上我想到了它。没有。实际上。我昨天。我的结果。车站。ごい。笔记被封锁了。我们最好。那些笔记基本上是向量。我会去实现，我会去实现。
- en: and then maybe I'll make the connections to the graph。So let me try to first
    go to let me not go to with this intuition in mind at least to NAGPT which is
    a complete implementation of a transformer that is very minimal thats why I worked
    on this over the last few days and here it is reproducing GT2 on open web textex
    so it's a pretty serious implementation that reproduces GPT2 I would say and provide
    it in a compute this was one note of HPs for 38 hours or something like that correctly
    and it's very readable it's 300 lives so everyone can take a look at it and yeah
    let me basically briefly step through it。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后也许我会建立与图的连接。所以让我先尝试，不去考虑这个直觉，至少去NAGPT，这是一个非常简约的变换器完整实现，这就是我在过去几天里所做的，这里它在开放网页文本上再现GT2，所以这是一个相当严肃的实现，我会说再现了GPT2，并提供在一个计算上，这是HP的一个笔记，持续了38小时左右，而且它非常可读，只有300行，所以每个人都可以看一下，好的，让我基本上简单地讲解一下。
- en: So let's try to have a decoder only transformer so what that means is that it's
    a language model it tries to model the next word in a sequence or the next character
    sequence so the data that we train on it's always some kind of text so here's
    some fake Shakespeare so this is real Shakespeare we're going to produce fake
    Shakespeare so this is called the tiny Shakespeare dataset set which is one of
    my favorite toy datasets you take all of Shakespeare concateninated and it's one
    megate file and then you can train language models on it and get infinite Shakespeare
    if you like which I think is medical so we have a text the first thing we need
    to do is we need to convert it to a sequence of integers。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们尝试只使用解码器的变换器，这意味着它是一个语言模型，它试图建模序列中的下一个单词或下一个字符序列。因此，我们训练的数据总是某种文本，所以这里有一些假的莎士比亚，这是实际的莎士比亚，我们将生成假的莎士比亚，这被称为迷你莎士比亚数据集，这是我最喜欢的玩具数据集之一，你将所有莎士比亚连接在一起，它是一个兆字节的文件，然后你可以在上面训练语言模型，获得无穷的莎士比亚，如果你喜欢，我认为这是医学的。所以我们有一个文本，第一件需要做的事情是将其转换为整数序列。
- en: Because transformers natively process， you know。You can't pluck text into transform
    you need to some outencod。So the way that encoding is done is we convert， for
    example， in the simplest case。every character gets an integerr。And then instead
    of high there。we would have this sequence of integers。So then you can encode every
    single character as an integer and get like a NAA sequence of integer so you just
    incatetcatenate it all into one large long one dimensional sequence and then you
    can train on it Now here we only have a single document in some cases if you have
    multiple independent documents what people like to do is create special tokens
    and they intersperse those documents with those special text tokens that they
    splice in between to create boundaries。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因为变换器本质上处理，你知道。你不能直接将文本放入变换器中，你需要进行某种编码。因此，编码的方式是我们将每个字符转换，例如，在最简单的情况下。每个字符变成一个整数。然后不是高在那里。我们会有这个整数序列。因此，你可以将每个字符编码为一个整数，并得到像NAA这样的整数序列，你只需将它们全部连接成一个大型一维序列，然后你就可以在上面训练。现在这里我们只有一个单独的文档，在某些情况下，如果你有多个独立的文档，人们喜欢创建特殊的标记，并将这些文档与那些特殊的文本标记交替插入，以创建边界。
- en: But those boundaries actually don't have any。Any modeling impact it's just that
    the transformer is supposed to learn beyond that propagation that the end of document
    sequence means that you should wipe the memory。😡，Okay， so then we produce batches，
    so these batches of data just mean that we go back to the one dimensional sequence
    and we take out chunks of this sequence。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些边界实际上没有任何建模影响，只是变压器应该学习超出这种传播的内容，文档序列的结束意味着你应该清空记忆。😡，好的，然后我们生成批次，这些数据批次意味着我们回到一维序列，并提取出这一序列的块。
- en: so say if the block size is eight。Then the block size indicates the maximum
    length of context that your transformer will process。so if our block size is eight，
    that means that we are going to have up to eight characters of context to predict
    ninth character in a sequence。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 所以假设块大小是八。那么块大小指示你的变压器将处理的最大上下文长度。如果我们的块大小是八，那意味着我们将有最多八个字符的上下文来预测序列中的第九个字符。
- en: And the batch size indicates how many sequences and parallel we're going to
    process。and we want this to be as large as possible， so we're fully taking advantage
    of the GPU and the parallelels under the cords。So in this example， we're doing
    four by8 batches， so every row here is independent example sort of。and then every。Every
    row here is is a small chunk of the sequence that we're going to train on and
    then we have both the inputs and the targets at every single point here。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大小指示我们将处理多少个序列并行。我们希望这个尽可能大，以便充分利用GPU和电缆下的并行性。因此在这个例子中，我们正在进行4x8批次，所以这里的每一行都是独立的例子。然后每一行都是我们将要训练的小块序列，并且我们在每个点上都有输入和目标。
- en: so to fully spell out what's contained in a single4 by8 batch to the transformer
    is sort of like compacted here so when the input is 47 by itself the target is
    58 and when the input is the sequence 4758 the target is one and when it's 47581
    the target is 51 and so on so actually the single batch of examples that's 4 by8
    actually has a ton of individual examples that we are expecting the transformer
    to learn on in in parallel and so you'll see that the batches are learned on completely
    independently but the time dimensions sort of here along horizontally is also
    trained on in parallel so sort of your real batch size is more like deted。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所以要完全阐明包含在单个4x8批次中的内容，对变压器来说就像是压缩在这里，所以当输入是47时，目标是58，而当输入是序列4758时，目标是1，当是47581时，目标是51，依此类推，实际上这个4x8的单个批次实例有很多个体例子，我们期望变压器能够并行学习，所以你会看到这些批次是完全独立学习的，但时间维度在这里水平排列，也是在并行训练，所以你真正的批量大小更像是deted。
- en: It's just that the context grows linearly for the predictions that you make
    along the T direction。In the in the model， so this is how the this is all the
    examples of the model will learn from this single back。So now this is the GPT
    class。And because this is a decoder only model。so we're not going to have an encoder
    because there's no like English we're translating from we're not trying to condition
    on some other external information。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 只是上下文在你沿着T方向进行的预测中线性增长。在模型中，这就是模型将从这个单一批次学习的所有示例。因此现在这是GPT类。因为这是一个仅解码的模型，所以我们不会有编码器，因为没有我们要翻译的英文，我们不是试图以其他外部信息为条件。
- en: we're just trying to produce a sequence of words that follow each other are
    likely to。So this is all p torch and I'm growing sluy faster because I'm assuming
    people have taken 2 31 and or something along those lines。but here in the forward
    pass we take this these indices。And then we both encode the identity of the indices
    just via an embedding lookup table。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是试图生成相互可能跟随的词序列。所以这都是pytorch，我的成长速度更快，因为我假设人们已经接受了2 31或类似的东西。但在前向传递中，我们获取这些索引。然后我们通过嵌入查找表对这些索引的身份进行编码。
- en: so every single integer has a we index into a lookup table of vectors in this
    n dot embedding and pull out the word vector for that token。And then because the
    message， because transform by itself doesn't actually。it processes sets natively，
    so we need to also positionally encode these vectors so that we basically have
    both the information about the token identity and its place in the sequence from
    one to block size。Now those the information about what and where is combined additively。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 所以每一个整数都有一个索引，我们索引到这个n.dot嵌入的查找表中，提取出该令牌的词向量。然后因为transform本身并不实际处理，所以我们需要对这些向量进行位置编码，以便我们基本上同时拥有令牌身份和其在序列中的位置（从1到块大小）信息。现在，关于什么和位置的信息是加法结合的。
- en: so the token embeddings and the positional embeddings are just added exactly
    as here。So this X here。😡，Then there's optional dropout， this x here basically
    just contains the set of words。And their positions。And that feeds into the blocks
    of transformer that we're going to look into what's blocked here。but for here
    for now， this is just a series of blocks in the transformer。😡，And then in the
    end。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以令牌嵌入和位置嵌入正如这里所示直接相加。所以这里的X。😡 然后有可选的dropout，这里的x基本上只包含词汇集合。还有它们的位置。这些信息输入到我们将要查看的transformer的块中，这里暂时只是transformer中的一系列块。😡
    然后在最后。
- en: there's a layer norm， and then you're decoding the logits for the next word
    or next integer in a sequence using a linear projection of the output of this
    transformer。so LM head here， a short for language model head is just a linear
    function。So basically positionally encode all the words， feed them into a sequence
    of blocks。and then apply a linear layer to get the probability distribution for
    the next character。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个层归一化，然后你使用这个transformer输出的线性投影来解码下一个单词或下一个整数的logits。所以这里的LM头，语言模型头的缩写只是一个线性函数。因此，基本上对所有词进行位置编码，将它们输入到一系列块中。然后应用一个线性层来获取下一个字符的概率分布。
- en: and then if we have the targets which we produced in the data order and you'll
    notice that the targets are just the inputs offset by one in time。Then those targets
    feed into cross entrytropy loss， so this is just a negative one likelihood。typical
    classification loss。So now let's drill into what's here in the blocks。So these
    blocks are applied sequentially。😡，There's again。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后如果我们有在数据顺序中生成的目标，你会注意到这些目标只是时间上偏移一个的输入。然后这些目标输入到交叉熵损失中，所以这只是一个负对数似然，典型的分类损失。现在让我们深入了解这些块中的内容。这些块是顺序应用的。😡
    再次。
- en: as I mentioned communicate phase and the compute phase so in the communicate
    phase both of the nodes get to talk to each other and so these nodes are basically。If
    our block size is eight。Then we are going to have eight node in this graph。There's
    eight nodes in this graph， the first node is pointed to only by itself。the second
    node is pointed to by the first node and itself。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我提到的沟通阶段和计算阶段，在沟通阶段，两个节点互相交流，因此这些节点基本上是。如果我们的块大小是八，那么在这个图中我们将有八个节点。这个图中有八个节点，第一个节点仅被自身指向。第二个节点被第一个节点和自身指向。
- en: The third node is wanted to buy the first two nodes and itself， et cetera。so
    there's eight nodes here。So you apply， there's a residual pathway in X， you take
    it out。you apply a layer norm and then the self attention so that these communicate
    these eight nodes communicate。but you have to keep in mind that the batch is four
    so because batch is four this is also applied so we have eight nodes communicating
    but there's a batch of four of them all individually communicating on those eight
    node。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个节点被前两个节点和自身指向，等等。所以这里有八个节点。因此你应用，X中有一个残差路径，你将其提取。你应用一个层归一化，然后是自注意力，这样这八个节点就进行交流。但你必须记住批次是四，所以因为批次是四，这也是应用的，因此我们有八个节点在交流，但这四个批次中的每一个都在这些八个节点上各自进行交流。
- en: there's no crisscross across the batchsh dimension of course there's no batchche
    norm range anyway luckier。And then once theyve changed information， they are processed
    using the modo receptor。and that's the compute base。So and then also here are
    missing we are missing the cross attention and because this is a decoder only
    model。so all we have is this step here， the multiheaded attention and that's this
    line the communicate phase。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在批次维度上当然没有交叉，批次归一化范围也没有，因此运气更好。一旦它们交换了信息，就使用modo receptor进行处理。这是计算基础。所以这里还有缺失，我们缺少交叉注意力，因为这是一个仅解码器模型。因此我们只有这个步骤，即多头注意力，这是这个沟通阶段。
- en: and then we have the feet forward， which is the MLP。And that's the complete
    phase。I'll take questions a bit later。Then the MLP here is fairly straightforward
    the MLP is just individual processing on each node just transforming the feature
    representation sort of at that node so。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们有前馈层，即 MLP。这是完整的阶段。我稍后会回答问题。这里的 MLP 相当简单，MLP 在每个节点上进行个体处理，只是在该节点上转换特征表示。
- en: Applying a two layer neural nu with a gal nonlinearity。which is just think of
    it as a re or something like that， it just a nonlinearity。And then MLP straightforward，
    I don't think there's anything too crazy there。and then this is the causal self
    attention part， the communication phase。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 应用一个两层神经网络，带有一个伽尔非线性。可以将其视为一个重映射或类似的东西，这只是一个非线性。而且 MLP 很直接，我认为这里没有什么太疯狂的。这是因果自注意力部分，通信阶段。
- en: So this is kind of like the loo of things and the most complicated part。it's
    only complicated because of the batch。And the implementation detail of how you
    mask the connectivity in the graph so that you don' you can't obtain any information
    from the future when you're predicting your token。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这有点像事物的轮廓，最复杂的部分。之所以复杂，是因为批处理和实现细节，比如如何在图中屏蔽连接，以便在预测标记时无法从未来获取任何信息。
- en: otherwise it gives away the information so if I'm the fifth token and if I'm
    the fifth position then I'm getting the fourth token coming into the input and
    I'm attending to the third second and first and I'm trying to figure out what
    is the what is the next token well then in this batch in the next element over
    in the time dimension the answer is at the input so I can't get any information
    from there so that's why this is all tricky but basically in the forward pass。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 否则它会泄露信息，所以如果我是第五个标记，并且处于第五个位置，那么我会获得第四个标记作为输入，我会关注第三、第二和第一个标记，并试图弄清楚下一个标记是什么。在这一批的下一个时间维度的元素中，答案在输入中，所以我无法从那里获取任何信息，这就是为什么这一切都很棘手，但基本上在前向传递中。
- en: 嗯。We are calculating the queries， keys and values based on x。So these are the
    keys queries and values here when I'm computing the attention。I have the queries
    matrix multiplying the keys， so this is the dot product in parallel for all the
    queries in all piece。In older heads。So that I I felt to mention that there's also
    the aspect of the heads which is also done all and parallel here。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。我们基于 x 计算查询、键和值。这些就是我在计算注意力时的键、查询和值。我有查询矩阵乘以键，因此这是对所有查询和所有头部的并行点积。所以我觉得还需要提到的是头部的部分也是在这里并行完成的。
- en: so we have the batch dimension， the time dimension and the head dimension and
    you end up with five dimensional tenss and it's all really confusing so I invite
    you to step through it later and commit yourself that this is actually doing the
    right thing basically give have the batch dimension。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有批处理维度、时间维度和头部维度，你最终会得到五维张量，这一切都很混乱，所以我邀请你稍后逐步了解，并确认这实际上是在做正确的事情，基本上就是拥有批处理维度。
- en: the head dimension and the time dimension and then you have features at them。And
    so this is evaluating for all the batch elements。for all the head elements and
    all the time elements。The simple Python that I gave you earlier。which is queryductpro
    P。Then here we do a mask to fill and what this is doing is it's basically clamping
    the。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 头部维度和时间维度，然后你会得到它们的特征。因此，这是在评估所有批处理元素、所有头部元素和所有时间元素。之前给你的简单 Python 示例，就是 queryductpro
    P。然后我们在这里做一个填充掩码，基本上是在限制这个。
- en: The attention between the node that are not supposed to communicate to be negative
    infinity and we're doing negative infinity because we're about to soft max and
    so negative infinity will make basically the attention that those elements be
    zero。😡，And so here we are going to basically end up with the weights。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力在不应该相互通信的节点之间被设置为负无穷，我们这样做是因为我们即将进行 softmax，因此负无穷会使那些元素的注意力基本上为零。😡 所以在这里我们基本上会得到权重。
- en: They serve affinities between these notes。Optional dropout and then here attention
    matrix multiply V is basically the。The gathering of the information， according
    to the ainities we've calculated。and this is just a weighted sum of the values
    at all those nodes。So this matrix multiplies as doing that weighted sum。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 它们在这些节点之间服务于亲和性。可选的 dropout，然后这里的注意力矩阵乘以 V，基本上是根据我们计算的亲和性来汇集信息。这只是所有这些节点上值的加权和。所以这个矩阵乘法就是在进行这个加权和。
- en: And then transpose contiguous view because it's all complicated and bashed in
    five dimensional testors。but it's really not doing anything optional dropout and
    then a linear projection back to the residual pathway。So this is implementing
    the communication phase now。Then you can train this transformer。And then you can
    generate infinite Shakespeare and you will simply do this by because our block
    size is eight。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: we start with sum token， say like I used in this case。you can use something
    like a new line as the star token。And then you communicate only to yourself because
    there's a single node and you get the probability distribution for the first word
    in the sequence。and then you decode it or the first character in the sequence。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: you decode the character and then you bring back the character and you reencode
    it as an integer and now you have the second thing and so you get okay。where the
    first position and this is whatever integer it is。Addd the position encodings
    goes into the sequence goes into transformer and again this token now communicates
    with the first open and its identity and so you just keep plugging the back and
    once you run out of the block size。which is eight， you start to crawl。Because
    you can never have block size more than8 in the way you've trained this transformer
    so we have more and more context until eight and then if you want to generate
    beyond date you have to start cropping because the transformer only works for
    eight elements in time dimension and so all of these transformers in the naive
    setting have a finite de box size or context length and in typical models this
    will be 1024 tokens or 2048 tokens something like that but these tokens are usually
    like EE tokens or sentence piece tokens or work these tokens there's many different
    encodings so it's not like that long and so that's why I think did' mention we
    really want to expand the context size and gets gnarly because the attention is
    sp in the na case。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Now， if you want to。Implement an encoder instead of a decoder。Attention。Then
    all you have to do is this master。You just delete that line。So if you don't mask
    the attention。then all the nodes communicate to each other and everything is allowed
    and information flows between all the nodes。So if you want to have the encoder
    here， just delete all the encoder blocks will use attention where this line is
    deleted。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: that's it。So you're allowing whatever is encode in my store， say 10 tokens，
    10 notes。and they are all allowed to communicate to each other going up the transformer。And
    then if you want to implement cross attention， so you have a full encoder decoder
    transformer。not just a decoder only transformer or GPT。Then we need to also add
    cross attention in the middle。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: so here there's a self attention piece where all the there's a self attention
    piece。a cross attention piece and this MLP and in the cross attention。We need
    to take the features from the top of the encoder。We need to add one more line
    here and this would be the cross attention instead of I should have implemented
    it instead of just pointing I think。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: but there will be a cross attention line here so we'll have three lines because
    we need to add another block and the queries will come from x。but the piece and
    the values will come from the top of the encoder。And there will be basically information
    flowing from the encoder strictly to all the nodes inside X。And then that's it，
    so it's very simple sort of modifications on the decoder attention。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: So you'll hear people talk that you kind of have a decoder only model like GP。you
    can have an encoder only model like Bt or you can have an encoder decoder model
    like say t5 doing things like machine translation so and in Bt you can't train
    it using sort of this。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling setup that's utter aggressive and you're just trying to predict
    the next omin sequence you're training it to a slightly different objectives you're
    putting in like the full sentence and the full sentence is allowed to communicate
    fully and then you're trying to classify sentiment or something like that so you're
    not trying to model like the next token in the sequence so these are trans slightly
    different with math。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 啊。With using masking and other deno techniques。Okay。so that's kind of like the
    transformer I'm going to continue so yeah， maybe more questions。你说我。哦。对。我 really。I。Okay。You，
    good pepper still for。Find is a dynamic route that。We can actually change in everything
    and you also have something。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Did it like we are enforcing these constraintsprints on it by just masking by
    give aware of it。啊啊。So I'm not sure if I fully follow， so there's different ways
    to look at this analogy。but one analogy is you can interpret this graph as really
    fixed it's just that every time you do the communicate。we are using different
    weights， you can look at it down。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: So if we have block size of eight in my example， we would have eight nodes here
    we have two four six okay。so we'd have eight nodes， they would be connected in
    you lay them out and you only connect from left to right but we're different。Yeah，
    we'll have a very connected part。Oh。Why wouldU the connections don't change as
    a function of the data or something like that？
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The not。I wonder if I theres any。I don't think I've seen a single example where
    the connectivity changes dynamically option data。usually the connectivity is fixed
    if you have an encoder and you're training a Bt。you have how many tokens you want
    and they are fully connected。And if you have decoder a long model。you have the
    triular thing， and if you have encoder decoder。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: then you have awkwardly sort of like two tools of nodes。诶呀。啊对啊就是我们。最后就。家佢你嗯。嗯。不要对起。作为直播人我诉。第划孙嗰月啊。I
    wonder。Do much more about this and then go。都走。你冇想出去。有直。Okay。Which。Part。Okay。对系咧。これ。嗯。哦，有か。冇
    your。Different things。我是多。Yeah it's really hard to say so that's why I think this
    paper is so interesting is like。yeah usually you'd see like a path and maybe they
    head path internally because just didn't publish it and all you can see is sort
    of things that didn't look like the transformer I mean you had resnets which have
    lots of this but a resnet would be kind of like this but there's there's no self-
    attention component。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: but the MLP is there kind of in a resNe。嗯。So a resonanceson looks very much
    like this。except there's no， you can use layer norms and resonances I believe
    as well。typically sometimes they can be bash norms。So it is kind of like a renet。it
    is kind of like they took a ressonnet and they put in a trans selfpottiary block
    in addition to the preexistent MLP block。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: which is kind of like convolutions and MLP was strictly speaking deconvolution
    one by one convolution。but I think the idea is similar that MLP is just kind of
    like you know typical weights。No nonlineararity weights or operation。嗯。And but
    I will say like， yeah。it's kind of interesting because。A lot of work is not there
    and then they give you this transformer and then it turns out five years later
    it's not changed even though everyone's trying to change it so it's kind of interesting
    to me that iss kind of like a package came like a package which I think is really
    interesting historically and I also talked to paper authors and they were unaware
    of the impact that transform would have at the time so when you read this paper
    actually it's kind of unfortunate because this is like the paper that changed
    everything but when people read it it's like question marks because it reads like
    a pretty random machine translation paper I go over doing machine translation
    oh here's a cool architecture okay great good results like。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: It's it doesn't sort of know what's going to happen and so when people read
    it today I think they're kind of confused potentially like having like having。I
    will have some tweets at the end， but I think I would have renamed it with the
    benefit of hindsight of like。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: well I'll get to it。Yeah。这个。Okay。我个放。Yeah I think that's a good question as
    well currently I mean I certainly don't love the autoaggressive modeling approach。I
    think it's kind of weird to like sample a token and then commit to it。So。know
    maybe there's some ways some hybrids with diffusion as an example。which I think
    would be really cool。Or we'll find some other ways to like edit the sequences
    later about filling in our regressive framework。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: but I think the fusion is kind of like an up and coming modeling approach that
    I personally find much more appeal when I sample text I don't go chunk chunk chunk
    and commit I do a draft one and then I do a better draft two。And that feels like
    a the fusion process。So that would be my hope。Okay， also question so yeah。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: you like the logic but it。When you say， like the。Se attention is sort of like
    computing like a agiator because to the adopt product on the notionary。And then
    those we have the edge the like multily by the other values。And then just appropriate
    it yes， yes right， and do you think there's like a。Agy like profitable networks
    and。I find the graph neural networks kind of like a confusing term because。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: I mean， yeah， previously， there was this notion of。I kind of thought maybe today
    everything is a graph neural network because the transformer is a graph neural
    and processor。the native representation that the transformer operates over is
    sets that are connected by edges in a direct way and so that's the native representation
    and then yeah。Okay， I should go on because I still have like 30 slides。G solve。Those
    want one provide by。Oh， yeah。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Yeah， the root D， I think basically like as if you're initializing with random
    weight separate from ausion as your dimension size grows。so does your values，
    the variance grows and then your softmax will just become the one half vector。so
    it's just a way to control the variance and bring it to always be in a good range
    for softmax and nice distribution。O。So it's almost like an initialization thing。Okay。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: So transformers have been applied to all the other fields and the way this was
    done is in my opinion kind of ridiculous ways honestly。because I was a computer
    vision person and you have commons and they kind of make sense。so what we're doing
    now with bits as an example is you take an image and you chop it up into little
    squares and then those squares literally feed into a transformer and that's it。Which
    is kind of ridiculous and so。下面。Yeah， and so the transformer doesn't even in the
    simplest case like really know where these patches might come from。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: they are usually positionly encoded， but it has to sort of like rediscover a
    lot of the structure I think of them in some ways。and it's kind of weird to approach
    it that way。But it's just like the simplest baseline of just choing up big images
    into small squares and feeding them in as like the individual nodes actually works
    fairly well and then this is in transformer encoder so all the patches are talking
    to each other throughout the interpret。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: And the number of nodes here would be sort of like nine。Okay。Also in speech
    recognition。you just take your Mel spectrogram and you chop it up into your slices
    and feed them into a transformer。so there wrote paper like this but also whisper
    whisper is a copy based transformer if you saw whisper from OpenAI you just chop
    up Mel Spectrogram and feed it into a transformer and then pretend you're dealing
    with text and it works very well。Decision transformer and RL you take your state's
    actions and reward that you experience an environment and you just pretend it's
    a language and you start to model the sequences of that and then you can use that
    for planning later that works pretty well you know even things like alpha fold
    so we were briefly talking about molecules and how you can plug them in so at
    the heart of alpha fold computationally is also a transformer。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: One thing I wanted to also say about transformers is I find that they're very
    they're super flexible and I really enjoy that I'll give you an example from Tesla。Like
    you have a come that that takes an image and makes predictions about the image
    and then the big question is how do you feed in extra information and it's not
    always trivial like say I have additional information that I want to inform that
    I want the output to be informed by maybe I have other sensors like radar maybe
    I have some map information or a vehicle type or some audio and the question is
    how do you feed information into a come that like where do you feed it in do you
    concatenate it like how do you do you add at what stage and so with the transformer
    it's much easier because you just take whatever you want。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: you chop it up into piecessis and you feed it in with a set of what you had
    before and you let the self- attentionten figure out how the potentially should
    communicate and that actually frankly works。So just chop up everything and throw
    it into the mix is kind of like the way and it frees neurallet from this version
    of Eidean space where previously you had to arrange your computation to conform
    to the Elidean space or three dimensions of how you're laying out the compute
    like the compute actually kind of happens in normal like 3D space if you think
    about it。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: but in intention everything is just sets so it's a very flexible framework and
    you can just like throw this stuff into your conditioning set and everything just
    self-ated over so it's quite beautiful with retro respect okay。So now what exactly
    makes transformers so effective？
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: I think a good example of this comes from the GT3 paper which I encourage people
    to read language models are twoshot learners。I would have probably remained this
    a little bit， I would have said something like transformers are capable of in
    context learning or like meta learning that's kind of like what makes them really
    special so basically the something that they're working with is okay I have some
    context and I'm trying to let's say passage this is just one example of many I
    have a passage and I'm asking questions about it。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: And then I'm giving as part of the context in the prompt， I'm giving the questions
    and the answers。so I'm giving one example of question answer， another example
    of question answer。another example of question answer and so on。And this becomes
    a。可以， people跟他说一啊。Okay。it is really important for me think。Okay， so what's really
    interesting is basically like。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: With more examples given in the context the accuracy improves and so what that
    hint at is that the transformer is able to somehow learn in the activations without
    doing any gradient descent in a typical fine tuning fashion so if you fine tune
    you have to give an example and the answer and you fine tuning using gradient
    descent but it looks like the transformer internally in its weights is doing something
    that looks like potentially gradient decent some kind of the mental learning in
    the weights of the transformer as it is reading the prompt and so in this paper
    they go into okay distinguishing this outer loop with stochastic gradient descent
    and this inner loop of the in contexttext learning so the inner loop is as the
    transformer is sort of like reading the sequence almost and the outer loop is
    is the training by gradient descent so basically there's some training happening
    in the activation of the transformer as it is consuming a sequence that may be
    very much looks like gradient descent and so there's some recent papers that kind
    of hint at this and study it and so as an example in this paper here they propose
    something called the raw operator and they argue that the raw operator is implemented
    by a transformer。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: And then they show that you can implement things like Ri regression on top of
    a raw operator。and so this is kind of giving their papers hinting that maybe there
    is something that looks like gradient based learning inside the activations of
    the transformer。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: And I think this is not impossible to think through because is what is gradient
    based learning。overpas， backward pass， and then update？Well， that looks like a
    resume。Right because you're just changing you're adding to the weights so you
    start initial random set of weights。forward pass backward pass and update your
    weights and then forward pass backward path weights。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: it looks like a resnet transformers is a resnet so much more hand wavy。but basically
    some paper trying to hint that why that would be potentially possible。And then
    I have a bunch of tweets I just gotten and pasted here in the end。this was kind
    of like meant for general consumption。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: so they're a bit more high level and high a little bit。but I'm talking about
    why this architecture is so interesting and why why potentially became so popular
    and I think it simultaneously optimizes three properties that I think are very
    desirable number one。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: the transform is very expressive and if overpass it's sort of like is' able
    to implement very interesting functions。potentially functions that can even like
    do metaer。Number two。it is very optimizable thanks to things like residual connections
    layerknow and so on and number three it's extremely efficient。this is not always
    appreciated， but the transformer。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: if you look at the computational graph is a shallow wide network which is perfect
    to take advantage of the parallelmal GPUs so I think the transformer was designed
    very deliberately to run efficiently on GPUs there's previous work like neural
    GPU that I really enjoy as well which is really just like how do we design neural
    adss that are efficient on GPUs and thinking backwards from the constraints of
    the hardware which I think is a very interesting way to think about it。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 啊。Oh。Oh yeah so here I'm saying I probably would have called I probably would
    have called the transformer a general purposeus efficient optimizable computer
    instead of the attention is all you need like that's what I would have maybe in
    hindsight called that paper is proposing it is a model that is。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: 嗯。Very general purpose， so forward passes express it。it very efficient in terms
    of GPU usage and is easily optimizable by gradientd descent and trains it very
    nicely。Then I have some other hot tweets here。Anyway， so yeah you can read them
    later。but I think this ones maybe interesting， so if previous neural nets are
    special purpose computers designed for a specific tasks。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: GPT is a general purpose computer reconfigurable at runtime to run natural language
    programs。so program the programs are given as prompts。And then Chi became Ro the
    program by completing the document。So I really like these analogies personally
    to computer。it's just like a powerful computer and it's optimizable by gradient
    descent。And。系 don't know o啊。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: I think you can read this later， but it's right out just thank you， I'll just
    leave this up。😊，Yeah。So sorry， I just found this lead。Turns out that if you scale
    up the training set and use a powerful enough neuralNes like a transformer。the
    network becomes a kind of general purpose computer over text。so I think that's
    a kind of like nice way to look at it and instead of performing a single text
    sequence you can design the sequence in the prompt and because the transformer
    is both powerful but also is trained on large enough very hard data set it kind
    of becomes a general purpose text computer and so I think that's kind of interesting
    way you look at it。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Yeah。哦。Yeah。后。Yeah。哦。对。啊。系 guess。I系。What。Good。好。哦。That was alreadyient。有。Oh。So
    it is。Oh。Im moved sorry。Yes me，但是我意思觉得好。Okay。其实大一开过。Oh。I was too favorite。Its
    pretty really like most of it。you know。It mostly more efficient。It very good place
    but you have that。大会。すて。Dr。 By。Yeah very。So I think there's a bit of that， yeah，
    so I would say RNNs like in principle yes。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: they can implement arbitrary programs， I think it's kind of like a useless statement
    to some extent because they are they're probably。I'm not sure that they're they're
    probably expressive because in a sense of like power in that they can implement
    these arbitrary functions。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: But they're not optimizable。😡，And they're certainly not efficient because they
    are serial computing devices。Um so I think， so if you look at it as a compute
    graph， RNNs are very long。变了。佢血挂。啊。Like if you stretched out the neurons and you
    look like take all the individual neurons in our connectivity and stretch them
    out and try to visualize them。RNNs would be like a very long graph in a bad and
    it's bad also optizability because I don't exactly know why but just the rough
    intuition is when you're back propagating。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: you don't want to make too many steps。😡，And so transformers are a shallow wide
    graph。and so from supervision to inputs is a very small number of hops。And it's
    along residual pathways which make gradients flow very easily and there's all
    these layer norms to control gradient the scales of all of those activations and
    so there's not too many hops and you're going from supervision to input very quickly
    and just flows through the graph。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Ands it can all be done in parallel so you don't need to do this encoder decoder
    RNANs you have to go from first word。then cycle word then third word， but here
    in transformer。every single word was processed completely as sort of in parallel。Which
    is kind of the so I think all these are really important because all these are
    really important and I think number three is less talked about but extremely important
    because in deep learning scale matters and so the size of the network that you
    can train gives you is extremely important and so if it's efficient on the current
    hardware then we can make it bigger。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 我而家出嚟佢对我我个有我自己。か。Yeah how does that believe the dataNo so yeah so you take your
    image and you apparently chop them up into patches so there's the first thousand
    tokens or whatever and now I have a special so radar could be also but I don't
    actually know the native representation of radar so。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: But you could， you just need to chop it up and enter it。And then you have to
    encode it somehow。Like the transformer needs to know that they're coming from
    radar。So you create a special。You have some kind of a special token that you。These
    radar tokens are much slightly different in the representation and it's learnable
    by gradientcent and。Like vehicle information would also come in with a special
    embeddedbedding token that can be learned。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: So。Have you those like you don't， it's all just a set。And just's when the voiceman
    guarantee know hit and it say button。Yeah， it's all just a set。but you can position
    encode these sets if you want。But position encoding means you can hardwire。for
    example， the coordinates like using sinusoss and Posons， you can hardwire that。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: but it's better if you don't hardwire the position。you just it's just a vector
    that is always hanging out at this location。whatever content is there just adss
    on it and this vector strainable by background that's how I do it。Yeah唔。I think
    they're kind of delegate， like they seem to work。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: but it seems like sometimes our solution want to put sub structure。咁嘢佢系 might。嗯。I'
    if I understand question， so I mean the position encoders like they they're actually
    like not they have okay。so they have very little inductive bias or something like
    that they're just vectors hanging out the location always and you're trying to
    you're trying to help them network in some way。And。I think the intuition is good，
    but。Like if you have enough data usually trying to mess with it is like a bad
    thing like trying to like trying to enter knowledge when you have enough knowledge
    in the data set itself is not usually productive so it really depends on what
    scale you are if you have infinity data then you actually want to encode less
    and less that turns out to work better and if you have very little data then actually
    you do want to encode some biases and maybe if you have a much smaller data set
    and maybe coalutions are a good idea because you actually have this bias coming
    from more filters and so。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: But I think。So the transformer is extremely general。but there are ways to mess
    with the encodings to put in more structure like you could， for example。encode
    sinuses and cosines and fix it or you could actually go to the attention mechanism
    and say。okay， if my image is cho up into patches this patch can only communicate
    to this neighborhood and you can you just do that in the attention matrix just
    mask out whatever you don't want to communicate and so people really play with
    this because the full attention is inefficient so they will intersperse。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: for example， layers that only communicate little patches and then layers to
    communicate globally and they will sort of do all kinds of tricks like that so
    you can slowly bring in more inductive bias。you would do it but the inductive
    biases are sort of like they're factored out from the core transformer and they
    are factored out in the。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: In the connectivity of the notes， and they are factored out in positioning and
    can mess this。P proposition。AndHow you pocket？大。不该我。哦。So there's probably about
    200 papers on this now。if not more， they're kind of hard to keep track of honestly
    like my Safari browser。which is what's sort on my computer， like 200 open tabs。But。Yeah。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: so'm not even sure if I want to pick my paper honestly。And you can a maybe use
    a transformer like the other one that I actually like even more is potentially
    keep the context length fixed but allow the network to somehow use a scratchpa
    and so the way this works is you will teach the transformer somehow via examples
    in the hey you actually have a scratchy hey basically you can't remember too much
    your context line is finite but you can use a scratchpad and you do that by emitting
    a scratchpa and then writing whatever you want to remember and then end scratchpad
    and then you continue with whatever you want and then later when it's decoding
    you actually like have special logic that when you detect start scratchpad you
    will sort of like save whatever it puts in there in like external thing and allow
    it to attend over it。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: So basically you can teach the transformer just dynamically because it's so
    meta learned。you can teach it dynamically to use other gizmos and gadgets and
    allow it to expandend his memory that way if that makes sense it's just like human
    learning to use a notepad right you don't have to keep it in your brain so keeping
    things in your brain is kind of like a context line from the transformer。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: but maybe you can just give it a notebook and then it con query the notebook
    and read from it and write to it maybe you another。Yes。咁觉现在你怎下。The way going to
    this。都快嘅啲。为什你嘅。I don't know if I detected that。I kind of feel like did you feel
    like it was more than just a long prompt that's unfolding？咱再。I didn't try extensively，
    but I did see a forgetting event and I kind of felt like the block size was just
    moved。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 啊。Maybe I'm， I don't actually know about the internal of tragedy。Get two。So
    one question is。what do you think about architecture？S for S for。I' say I don't
    know this， which one is this for？
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: And second question， this was a personal question， what are you going to work
    on next？I mean。so right now I'm working on things like nanoGPT where is nanoGT。😊，呃。I
    mean。I'm going basically slightly from computer vision and like part kind of like
    the computer vision based products do a little bit in the language domain whereas
    tryGT。okay then on GT。So originally I had MinG， which I ever wrotete to nanoGPT
    and I'm working on this I'm trying to reproduce GPTs and I mean I think something
    like chat GPT I think incrementally improved in a product fashion would be extremely
    interesting。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: I think a lot of people feel it and that's why it went so wide so I think there's
    something like a Google plus plus plus to build that I think is very interesting。Do
    we diversity run thought？
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_13.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
