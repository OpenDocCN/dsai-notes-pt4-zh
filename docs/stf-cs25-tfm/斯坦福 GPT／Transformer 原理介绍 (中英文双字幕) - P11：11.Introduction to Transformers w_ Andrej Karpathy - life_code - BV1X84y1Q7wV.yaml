- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P11ï¼š11.Introduction to Transformers w_ Andrej
    Karpathy - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPT/Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P11ï¼š11. å˜å‹å™¨ä»‹ç»ä¸ Andrei Karpathy - life_code
    - BV1X84y1Q7wV
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_0.png)'
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_1.png)'
- en: Hi everyoneï¼Œ welcome to CS25 Pro UnitedVã€‚This was a course that was held at
    Stanford in the winter of 2023ã€‚ğŸ˜Šï¼ŒThis course is not about robots that can transform
    into carsï¼Œ as this picture I suggestï¼Œ ratherã€‚it's about deep learninging models
    that have taken the world by the storm and have revolutionized the field of AI
    and othersã€‚Starting from natural language processingï¼Œ transformers have been applied
    all over from compvisã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°CS25 Pro UnitedVã€‚è¿™ä¸ªè¯¾ç¨‹åœ¨2023å¹´å†¬å­£äºæ–¯å¦ç¦ä¸¾åŠã€‚ğŸ˜Š è¿™ä¸ªè¯¾ç¨‹å¹¶ä¸æ˜¯å…³äºå¯ä»¥å˜æˆæ±½è½¦çš„æœºå™¨äººï¼Œè€Œæ˜¯å…³äºæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå®ƒä»¬å¼•èµ·äº†ä¸–ç•Œçš„è½°åŠ¨ï¼Œå½»åº•æ”¹å˜äº†äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚ä»è‡ªç„¶è¯­è¨€å¤„ç†å¼€å§‹ï¼Œå˜å‹å™¨å·²å¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ç­‰é¢†åŸŸã€‚
- en: enforcementment learningï¼Œ biologyï¼Œ roboticï¼Œ etcã€‚We have an exciting set of videos
    lined up for you with some truly fascinating speakersã€‚skip talksã€‚Presenting how
    they're applying transformers to the research in different fields and areasã€‚We
    hopeã€‚You'll enjoy and learn from these videosã€‚So without any furthereddoï¼Œ let's
    get startedã€‚This is a purely introductory lectureã€‚And we'll go into the building
    blocks of transformersã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¼ºåŒ–å­¦ä¹ ã€ç”Ÿç‰©å­¦ã€æœºå™¨äººç­‰ã€‚æˆ‘ä»¬ä¸ºæ‚¨å‡†å¤‡äº†ä¸€ç³»åˆ—ä»¤äººå…´å¥‹çš„è§†é¢‘ï¼Œé‚€è¯·äº†ä¸€äº›çœŸæ­£è¿·äººçš„æ¼”è®²è€…ã€‚è·³è¿‡æ¼”è®²ã€‚å±•ç¤ºä»–ä»¬å¦‚ä½•å°†å˜å‹å™¨åº”ç”¨äºä¸åŒé¢†åŸŸçš„ç ”ç©¶ã€‚å¸Œæœ›æ‚¨èƒ½äº«å—å¹¶ä»è¿™äº›è§†é¢‘ä¸­å­¦ä¹ ã€‚æ‰€ä»¥ä¸å†æ‹–å»¶ï¼Œè®©æˆ‘ä»¬å¼€å§‹ã€‚è¿™æ˜¯ä¸€åœºçº¯ç²¹çš„å…¥é—¨è®²åº§ã€‚æˆ‘ä»¬å°†æ·±å…¥æ¢è®¨å˜å‹å™¨çš„æ„å»ºæ¨¡å—ã€‚
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_3.png)'
- en: So firstï¼Œ let's start with introducing the instructorsã€‚So for meã€‚I'm currently
    on a temporary diploma from the PhP program and I'm leading here at a robotics
    startupã€‚collaborativeative robotics working on some general purpose robotsï¼Œ somewhat
    like a theã€‚and yeah I'm very passionate about robotics and building assist learning
    algorithmsã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä»‹ç»ä¸€ä¸‹è®²å¸ˆã€‚æˆ‘ç›®å‰åœ¨PhDé¡¹ç›®ä¸­æš‚æ—¶æŒ‚åï¼Œå¹¶åœ¨ä¸€å®¶æœºå™¨äººåˆåˆ›å…¬å¸æ‹…ä»»é¢†å¯¼ï¼Œè‡´åŠ›äºä¸€äº›é€šç”¨æœºå™¨äººï¼Œç±»ä¼¼äºè¿™ä¸ªã€‚æˆ‘å¯¹æœºå™¨äººå’Œæ„å»ºè¾…åŠ©å­¦ä¹ ç®—æ³•å……æ»¡çƒ­æƒ…ã€‚
- en: my research interests in in personalsonal lending ands in remote modeling and
    I have a bunch of publications in the robotics government driving other areasã€‚Undergrad
    close at Cornellï¼Œ it's someones book Cornellï¼Œ it's an estimate callã€‚So I'm Stephen
    a Gr Fer CSP speakerï¼Œ Per did my master's at CMU and an undergradã€‚Mainly into
    NLP researchï¼Œ anything involving language and textã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨ä¸ªäººè´·æ¬¾å’Œè¿œç¨‹å»ºæ¨¡æ–¹é¢çš„ç ”ç©¶å…´è¶£ï¼Œä»¥åŠåœ¨æœºå™¨äººã€æ”¿åºœã€é©¾é©¶ç­‰é¢†åŸŸæœ‰å¾ˆå¤šå‡ºç‰ˆç‰©ã€‚å¤§å­¦æ—¶åœ¨åº·å¥ˆå°”ï¼ŒæŸæœ¬ä¹¦æåˆ°åº·å¥ˆå°”ï¼Œè¿™æ˜¯ä¸€ä¸ªä¼°ç®—çš„å‘¼å«ã€‚æ‰€ä»¥æˆ‘æ˜¯ä¸€åCSPæ¼”è®²è€…ï¼Œå®Œæˆäº†CMUçš„ç¡•å£«å’Œæœ¬ç§‘å­¦ä½ã€‚ä¸»è¦ä»äº‹è‡ªç„¶è¯­è¨€å¤„ç†ç ”ç©¶ï¼Œæ¶‰åŠè¯­è¨€å’Œæ–‡æœ¬çš„ä»»ä½•å†…å®¹ã€‚
- en: but more recently I've been getting more into computer vision as well as Monã€‚And
    just some stuff I do for funï¼Œ a lot of music stuff made pianoã€‚some self promo
    but I post a lot on my ins YouTube and TikTook so if you guys want to check it
    out my friends and I are also starting a Stanford piano club so if anybody's interested
    feel free to email with Y for details other than that you know martial artsã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æœ€è¿‘æˆ‘å¯¹è®¡ç®—æœºè§†è§‰å’Œä¸€äº›å¨±ä¹é¡¹ç›®ä¹Ÿè¶Šæ¥è¶Šæ„Ÿå…´è¶£ã€‚æˆ‘æœ‰å¾ˆå¤šéŸ³ä¹ç›¸å…³çš„æ´»åŠ¨ï¼Œåˆ¶ä½œäº†é’¢ç´æ›²ã€‚æœ‰ä¸€äº›è‡ªæˆ‘å®£ä¼ ï¼Œä½†æˆ‘åœ¨Instagramã€YouTubeå’ŒTikTokä¸Šå‘å¸ƒäº†å¾ˆå¤šå†…å®¹ã€‚å¦‚æœä½ ä»¬æœ‰å…´è¶£ï¼Œæˆ‘å’Œæˆ‘çš„æœ‹å‹ä»¬ä¹Ÿåœ¨æˆç«‹ä¸€ä¸ªæ–¯å¦ç¦é’¢ç´ä¿±ä¹éƒ¨ï¼Œæ‰€ä»¥å¦‚æœæœ‰äººæ„Ÿå…´è¶£ï¼Œå¯ä»¥éšæ—¶å‘é‚®ä»¶è¯¢é—®Yçš„è¯¦ç»†ä¿¡æ¯ï¼Œé™¤æ­¤ä¹‹å¤–ï¼Œæˆ‘ä¹Ÿç»ƒä¹ æ­¦æœ¯ã€‚
- en: bodybuildã€‚P're trying a pig dramas animeã€‚Occasional gameã€‚ğŸ˜€ã€‚Yeahã€‚Okay cool yeahã€‚so
    my name'sryland the same talk about myself I just want to very briefly say that
    I'm super excited to take this class I the last time was sorry to teach this class
    me I the last time it was offered of I thought we brought a really great group
    of speaker last time I'm super excited for this offering and yeah I'm thankful
    you're all here and I'm looking forward to a really fun quarter you was the most
    out speakak outspoken student last year and so someone wants to become instructor
    next year inã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ­£åœ¨å°è¯•ä¸€äº›åŠ¨ç”»ã€å¶å°”ç©æ¸¸æˆã€‚ğŸ˜€ å¥½çš„ã€‚æˆ‘çš„åå­—æ˜¯Rylandï¼Œç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±ã€‚æˆ‘æƒ³ç®€çŸ­åœ°è¯´ï¼Œæˆ‘éå¸¸å…´å¥‹èƒ½å‚åŠ è¿™ä¸ªè¯¾ç¨‹ã€‚ä¸Šæ¬¡æˆ‘æ•™è¿™é—¨è¯¾æ—¶ï¼Œæˆ‘è§‰å¾—æˆ‘ä»¬å¸¦æ¥äº†ä¸€ä¸ªéå¸¸ä¼˜ç§€çš„æ¼”è®²è€…å›¢é˜Ÿã€‚æˆ‘å¯¹è¿™æ¬¡è¯¾ç¨‹æ„Ÿåˆ°éå¸¸æœŸå¾…ï¼Œè°¢è°¢å¤§å®¶çš„åˆ°æ¥ï¼Œæˆ‘æœŸå¾…ä¸€ä¸ªéå¸¸æœ‰è¶£çš„å­¦æœŸã€‚ä½ æ˜¯å»å¹´æœ€æ´»è·ƒçš„å­¦ç”Ÿï¼Œæ˜å¹´æœ‰æ„æˆä¸ºè®²å¸ˆã€‚
- en: Okayã€‚ğŸ˜Šï¼ŒGoã€‚Let's see if okay a few minutesã€‚So what we hope you will learn in
    this class is first of allã€‚how do transforms workï¼Ÿhow they being applied just
    don cannot and now days like we are pretty much in them everywhere in the AI machine
    learning and what are some new interesting directions of research in the topicsã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚ğŸ˜Šï¼Œç»§ç»­ã€‚è®©æˆ‘ä»¬çœ‹çœ‹ï¼Œå¦‚æœæœ‰å‡ åˆ†é’Ÿã€‚æˆ‘ä»¬å¸Œæœ›ä½ åœ¨è¿™å ‚è¯¾ä¸­å­¦åˆ°çš„ç¬¬ä¸€ä»¶äº‹æ˜¯ï¼Œå˜å‹å™¨æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿå®ƒä»¬æ˜¯å¦‚ä½•è¢«åº”ç”¨çš„ï¼Œç°å¦‚ä»Šåœ¨äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ä¸­æ— å¤„ä¸åœ¨ï¼Œä»¥åŠåœ¨è¿™äº›ä¸»é¢˜ä¸­çš„ä¸€äº›æ–°æœ‰è¶£ç ”ç©¶æ–¹å‘ã€‚
- en: Co so this class is just an introductorï¼Œ just talking about the basics of transformers
    introducing themã€‚talking about a self potential mechanism on which they are founded
    and will do a deep dive more on like a model like to GPT so getã€‚Happy to get solidã€‚Okayï¼Œ
    so let me start with presenting the attention timelineã€‚Attention all started with
    this wall paper attention is all by wasman L in 2017 that was the being transformers
    before that we had the field story error where we had models like RNM LSTMs and
    simple attention mechanisms that didn't involve for scale depotã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å ‚è¯¾åªæ˜¯ä¸€ä¸ªä»‹ç»ï¼Œåªæ˜¯è°ˆè®ºå˜å‹å™¨çš„åŸºç¡€ï¼Œä»‹ç»å®ƒä»¬ã€‚è®¨è®ºä¸€ä¸ªè‡ªæˆ‘æ½œåŠ›æœºåˆ¶ï¼Œè¿™æ˜¯å®ƒä»¬çš„åŸºç¡€ï¼Œæˆ‘ä»¬å°†æ·±å…¥æ¢è®¨åƒGPTè¿™æ ·çš„æ¨¡å‹ã€‚æ‰€ä»¥ï¼Œå‡†å¤‡å¥½å¾—åˆ°æ‰å®çš„çŸ¥è¯†ã€‚å¥½çš„ï¼Œè®©æˆ‘å¼€å§‹å±•ç¤ºæ³¨æ„åŠ›æ—¶é—´çº¿ã€‚æ³¨æ„åŠ›çš„æ‰€æœ‰ä¸€åˆ‡å§‹äº2017å¹´è¿™ç¯‡å£çº¸è®ºæ–‡ï¼Œæ³¨æ„åŠ›æ˜¯ç”±Vaswaniç­‰äººæå‡ºçš„ï¼Œåœ¨é‚£ä¹‹å‰æˆ‘ä»¬æœ‰è®¸å¤šæ¨¡å‹ï¼Œå¦‚RNNã€LSTMå’Œç®€å•çš„æ³¨æ„æœºåˆ¶ï¼Œè¿™äº›å¹¶æ²¡æœ‰æ¶‰åŠè§„æ¨¡éƒ¨ç½²ã€‚
- en: Start in 2017 we solve this explosion of transformers into NLP where people
    started using it for everythingã€‚I even heard the support from Google as like our
    performance increased every time we fight our linguistã€‚è¯¶ã€‚For the course 2018 after
    2018 to 2020ï¼Œ we saw this explosion of customers into other fields like visionã€‚ğŸ˜Šï¼ŒBch
    of other the stir and like biology and last year 2021 was the start of the geneticative
    error where we got like a lot of genetic modeling started like models like Koxã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ä»2017å¹´å¼€å§‹ï¼Œæˆ‘ä»¬åœ¨NLPé¢†åŸŸè§£å†³äº†å˜å‹å™¨çš„çˆ†ç‚¸ï¼Œå¤§å®¶å¼€å§‹å°†å…¶åº”ç”¨äºæ‰€æœ‰äº‹ç‰©ã€‚æˆ‘ç”šè‡³å¬è¯´è°·æ­Œçš„æ”¯æŒï¼Œæ¯æ¬¡æˆ‘ä»¬ä¸è¯­è¨€å­¦å®¶åˆä½œæ—¶ï¼Œæ€§èƒ½éƒ½æœ‰æ‰€æå‡ã€‚å—¯ã€‚åœ¨2018å¹´ä¹‹åï¼Œ2020å¹´ï¼Œæˆ‘ä»¬çœ‹åˆ°å®¢æˆ·åœ¨å…¶ä»–é¢†åŸŸï¼ˆå¦‚è§†è§‰ï¼‰ä¸­çš„æ¿€å¢ã€‚ğŸ˜Šï¼Œè¿˜æœ‰ç”Ÿç‰©å­¦ï¼Œå»å¹´2021å¹´æ˜¯åŸºå› ç”Ÿæˆé”™è¯¯çš„å¼€å§‹ï¼Œæˆ‘ä»¬å¼€å§‹äº†å¾ˆå¤šåŸºå› å»ºæ¨¡ï¼Œæ¯”å¦‚Koxæ¨¡å‹ã€‚
- en: GTï¼Œ Daliï¼Œ stable equations to a lot of things happening in genetic modeling
    and we start scaling up in AI and now the presentã€‚So this is 2022 and like the
    startup in 23 and now we have almost like a chattyy whisper a bunch of others
    and we are like scalinging onward without spell out so that's greatã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: GTï¼ŒDaliï¼Œç¨³å®šæ–¹ç¨‹ä¸è®¸å¤šåœ¨åŸºå› å»ºæ¨¡ä¸­å‘ç”Ÿçš„äº‹æƒ…ç›¸å…³ï¼Œæˆ‘ä»¬å¼€å§‹åœ¨äººå·¥æ™ºèƒ½ä¸Šæ‰©å±•ï¼Œç°åœ¨æ˜¯2022å¹´ï¼Œåƒ23å¹´çš„åˆåˆ›å…¬å¸ï¼Œç°åœ¨æˆ‘ä»¬å‡ ä¹æœ‰ä¸€ç¾¤å…¶ä»–çš„â€œå–‹å–‹ä¸ä¼‘â€çš„å£°éŸ³ï¼Œæˆ‘ä»¬åœ¨ä¸æ–­åœ°æ‰©å±•ï¼Œè¿™å¾ˆå¥½ã€‚
- en: so that's the futureã€‚So going more into thisã€‚So once there were auditã€‚So we
    had two modelsï¼Œ LTNã€‚GIUã€‚What worked here was the day of good at ending historyã€‚ğŸ˜Šã€‚But
    what did not work was they didn encode long sequences and they were very bad at
    encoding contentã€‚So consider this exampleã€‚Consider trying to predict the last
    word in the textã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å°±æ˜¯æœªæ¥ã€‚æ·±å…¥è®¨è®ºä¸€ä¸‹ã€‚æ‰€ä»¥ä¸€æ—¦è¿›è¡Œäº†å®¡è®¡ï¼Œæˆ‘ä»¬æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼ŒLTNå’ŒGIUã€‚è¿™é‡Œæœ‰æ•ˆçš„æ˜¯å®ƒä»¬æ“…é•¿ç»“æŸå†å²ã€‚ğŸ˜Šã€‚ä½†æ— æ•ˆçš„æ˜¯å®ƒä»¬æœªèƒ½ç¼–ç é•¿åºåˆ—ï¼Œä¸”ç¼–ç å†…å®¹çš„èƒ½åŠ›éå¸¸å·®ã€‚æ‰€ä»¥è€ƒè™‘è¿™ä¸ªä¾‹å­ã€‚è¯•ç€é¢„æµ‹æ–‡æœ¬ä¸­çš„æœ€åä¸€ä¸ªè¯ã€‚
- en: I grew up in France dot dot dotï¼Œ I speak fluent Dã€‚Here you need to understand
    the context for it to predict French and take attention mechanism is very good
    at thatã€‚whereas if you're just using LSTMsï¼Œ it doesn't work thatã€‚Another thing
    transformers are good at isã€‚Uã€‚More based on content is like also context prediction
    is like finding attention maps if I have something like a word like itã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨æ³•å›½é•¿å¤§â€¦â€¦æˆ‘æµåˆ©åœ°è¯´Dã€‚åœ¨è¿™é‡Œä½ éœ€è¦ç†è§£ä¸Šä¸‹æ–‡æ‰èƒ½é¢„æµ‹æ³•è¯­ï¼Œè€Œæ³¨æ„åŠ›æœºåˆ¶å¯¹æ­¤éå¸¸æœ‰æ•ˆã€‚è€Œå¦‚æœä½ åªæ˜¯ä½¿ç”¨LSTMï¼Œå°±è¡Œä¸é€šã€‚å˜å‹å™¨æ“…é•¿çš„å¦ä¸€ä»¶äº‹æ˜¯ï¼ŒåŸºäºå†…å®¹çš„ä¸Šä¸‹æ–‡é¢„æµ‹ï¼Œåƒæ˜¯æ‰¾åˆ°æ³¨æ„åŠ›å›¾ï¼Œå¦‚æœæˆ‘æœ‰åƒâ€œå®ƒâ€è¿™æ ·çš„è¯ã€‚
- en: what now does it collect and we can give like a probability attention on what
    are the possible activations and this works are better than existing mechanismsã€‚Okayã€‚So
    where we were in 2021ï¼Œ we were on the verge of takeoffã€‚We were starting to realize
    the potential of transformers in different fieldsã€‚we solved a lot of long sequence
    problems like protein foldingï¼Œ Al foldï¼Œ offline Arlã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨å®ƒæ”¶é›†äº†ä»€ä¹ˆï¼Œæˆ‘ä»¬å¯ä»¥ç»™å‡ºå¯èƒ½æ¿€æ´»çš„æ¦‚ç‡å…³æ³¨ï¼Œè¿™äº›å·¥ä½œæ¯”ç°æœ‰æœºåˆ¶æ›´å¥½ã€‚å¥½çš„ã€‚æ‰€ä»¥åœ¨2021å¹´ï¼Œæˆ‘ä»¬å¿«è¦èµ·é£äº†ã€‚æˆ‘ä»¬å¼€å§‹æ„è¯†åˆ°å˜å‹å™¨åœ¨ä¸åŒé¢†åŸŸçš„æ½œåŠ›ã€‚æˆ‘ä»¬è§£å†³äº†è®¸å¤šé•¿åºåˆ—é—®é¢˜ï¼Œå¦‚è›‹ç™½è´¨æŠ˜å ã€Al
    foldã€ç¦»çº¿Arlã€‚
- en: We started to see zero short generalization we saw multimodal tasks and applications
    like generating images from language so that's all dli yeah and it feels like
    Asia shared but person like two years agoã€‚And this is also a talk on transformers
    that can watch in giveã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¼€å§‹çœ‹åˆ°é›¶æ ·æœ¬æ³›åŒ–ï¼Œæˆ‘ä»¬çœ‹åˆ°äº†å¤šæ¨¡æ€ä»»åŠ¡å’Œåº”ç”¨ï¼Œæ¯”å¦‚ä»è¯­è¨€ç”Ÿæˆå›¾åƒï¼Œè¿™ä¸€åˆ‡éƒ½ä¸DLIæœ‰å…³ï¼Œæ˜¯çš„ï¼Œæ„Ÿè§‰åƒæ˜¯äºšæ´²ä¸¤å¹´å‰åˆ†äº«è¿‡çš„å†…å®¹ã€‚è¿™ä¹Ÿæ˜¯å…³äºå˜å‹å™¨çš„ä¸€ä¸ªè®¨è®ºï¼Œå¯ä»¥è§‚çœ‹å¹¶æä¾›ã€‚
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_5.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_5.png)'
- en: Yeahã€‚Co and this is where we was going from 2021 to 2022ã€‚which is we have gone
    from the verge of taking off to actually taking off and obviously we are seeing
    unique applications in audio generation artã€‚music sort towering we are starting
    to see reasoning capabilities like common senseã€‚logical reasoningï¼Œ mathematical
    reasoningã€‚We are also able to now get human enlightenment and interactionã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚è¿™å°±æ˜¯æˆ‘ä»¬ä»2021å¹´åˆ°2022å¹´æ‰€ç»å†çš„å˜åŒ–ï¼Œæˆ‘ä»¬å·²ç»ä»èµ·é£çš„è¾¹ç¼˜çœŸæ­£èµ·é£ï¼Œæ˜¾ç„¶æˆ‘ä»¬åœ¨éŸ³é¢‘ç”Ÿæˆã€è‰ºæœ¯å’ŒéŸ³ä¹ç­‰é¢†åŸŸçœ‹åˆ°äº†ç‹¬ç‰¹çš„åº”ç”¨ã€‚æˆ‘ä»¬å¼€å§‹çœ‹åˆ°æ¨ç†èƒ½åŠ›ï¼Œå¦‚å¸¸è¯†ã€é€»è¾‘æ¨ç†å’Œæ•°å­¦æ¨ç†ã€‚æˆ‘ä»¬ç°åœ¨ä¹Ÿèƒ½å¤Ÿè¿›è¡Œäººç±»å¯å‘å’Œäº’åŠ¨ã€‚
- en: they're able to use reinforcement learning and human feedback that's how tragedy
    is trained to perform really good we have a lot of mechanisms for controlling
    toxicity bias and ethics now and a lot of also a lot of developments in other
    areas like different modelsã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬èƒ½å¤Ÿåˆ©ç”¨å¼ºåŒ–å­¦ä¹ å’Œäººç±»åé¦ˆï¼Œè¿™å°±æ˜¯æ‚²å‰§è®­ç»ƒå¾—ä»¥è¡¨ç°è‰¯å¥½çš„åŸå› ã€‚æˆ‘ä»¬ç°åœ¨æœ‰è®¸å¤šæœºåˆ¶æ¥æ§åˆ¶æ¯’æ€§ã€åè§å’Œä¼¦ç†é—®é¢˜ï¼ŒåŒæ—¶åœ¨å…¶ä»–é¢†åŸŸä¹Ÿæœ‰è®¸å¤šå‘å±•ï¼Œæ¯”å¦‚ä¸åŒçš„æ¨¡å‹ã€‚
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_7.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_7.png)'
- en: å¤Ÿã€‚ğŸ˜¡ï¼Œå•Šã€‚So the feature is a spaceship and we are all excited about itã€‚ğŸ˜Šã€‚And there's
    a lot of more acquisition that we can enable and it'll be great if you can see
    transformers also work here one big example is for your understanding and generation
    that is something that everyone is interested in and I'm hoping we'll see a lot
    of models in this area this year also finance businessã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¤Ÿã€‚ğŸ˜¡ï¼Œå•Šã€‚æ‰€ä»¥è¿™ä¸ªåŠŸèƒ½å°±åƒæ˜¯ä¸€è‰˜é£èˆ¹ï¼Œæˆ‘ä»¬éƒ½å¯¹æ­¤æ„Ÿåˆ°å…´å¥‹ã€‚ğŸ˜Šã€‚è¿˜æœ‰å¾ˆå¤šå…¶ä»–æ”¶è´­ï¼Œæˆ‘ä»¬å¯ä»¥å¯ç”¨ï¼Œå¦‚æœä½ èƒ½çœ‹åˆ°å˜å‹å™¨ä¹Ÿåœ¨è¿™é‡Œå·¥ä½œå°±æ›´å¥½äº†ï¼Œä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­å°±æ˜¯ä½ çš„ç†è§£å’Œç”Ÿæˆï¼Œè¿™æ˜¯æ¯ä¸ªäººéƒ½æ„Ÿå…´è¶£çš„ï¼Œæˆ‘å¸Œæœ›ä»Šå¹´åœ¨è¿™ä¸ªé¢†åŸŸèƒ½çœ‹åˆ°å¾ˆå¤šæ¨¡å‹ï¼ŒåŒ…æ‹¬é‡‘èå’Œå•†ä¸šã€‚
- en: ğŸ˜Šï¼ŒI'll be very excited to see GT author novelï¼Œ but we need to solve very long
    sequences modeling and most transformative models are still limited to like 4000
    opens or something like that so we need to do aã€‚Make them general much more better
    on long sequences we are also we also want to have general agents that can do
    a lot of multitaskã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæˆ‘ä¼šéå¸¸å…´å¥‹çœ‹åˆ°GTåˆ›ä½œå°è¯´ï¼Œä½†æˆ‘ä»¬éœ€è¦è§£å†³éå¸¸é•¿çš„åºåˆ—å»ºæ¨¡é—®é¢˜ï¼Œè€Œå¤§å¤šæ•°å˜æ¢æ¨¡å‹ä»ç„¶é™åˆ¶åœ¨å¤§çº¦4000ä¸ªå¼€æ”¾è¯å·¦å³ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦æ”¹è¿›å®ƒä»¬ï¼Œä½¿å®ƒä»¬åœ¨é•¿åºåˆ—ä¸Šè¡¨ç°å¾—æ›´å¥½ã€‚æˆ‘ä»¬ä¹Ÿå¸Œæœ›æ‹¥æœ‰èƒ½å¤Ÿæ‰§è¡Œå¤šä»»åŠ¡çš„é€šç”¨ä»£ç†ã€‚
- en: Amatic inputã€‚Predictions like Goto and so I think we will see more of that too
    and finally we also want domain specific modelsã€‚so you might want like a GP models
    that's good at like maybe like help so that could be like a doctor GPT model you
    might have like a large GP model that's like on only on raw data so currently
    we have like GP models that are trained on every but we might start to see more
    niche models that are like good at one task and we could have like a mixture of
    expert and think like this is like how you normally consult an expert will' have
    like expert A models and you can go to a different air models for your different
    needsã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Amaticè¾“å…¥ã€‚åƒGotoè¿™æ ·çš„é¢„æµ‹ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºæˆ‘ä»¬ä¹Ÿä¼šçœ‹åˆ°æ›´å¤šè¿™äº›ï¼Œæœ€åæˆ‘ä»¬è¿˜å¸Œæœ›æœ‰ç‰¹å®šé¢†åŸŸçš„æ¨¡å‹ã€‚æ‰€ä»¥ä½ å¯èƒ½å¸Œæœ›æœ‰ä¸€ä¸ªåœ¨æŸäº›æ–¹é¢è¡¨ç°è‰¯å¥½çš„GPæ¨¡å‹ï¼Œæ¯”å¦‚åŒ»ç”ŸGPTæ¨¡å‹ï¼Œæˆ–è€…ä¸€ä¸ªä»…åŸºäºåŸå§‹æ•°æ®çš„å¤§å‹GPæ¨¡å‹ã€‚ç›®å‰æˆ‘ä»¬æœ‰çš„GPæ¨¡å‹æ˜¯åœ¨å„ç§æ•°æ®ä¸Šè®­ç»ƒçš„ï¼Œä½†æˆ‘ä»¬å¯èƒ½ä¼šå¼€å§‹çœ‹åˆ°æ›´å¤šä¸“æ³¨äºå•ä¸€ä»»åŠ¡çš„ç»†åˆ†æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥åƒå’¨è¯¢ä¸“å®¶ä¸€æ ·ï¼Œæœ‰ä¸“å®¶Aæ¨¡å‹ï¼Œå¯ä»¥æ ¹æ®ä¸åŒéœ€æ±‚å»ä¸åŒçš„æ¨¡å‹ã€‚
- en: Yeahã€‚There still a lot of missing ingredients to make this all successful the
    first of all is external memory we are already starting to see this with models
    like ChaGPT where the interactions are short livedã€‚there's no long term memory
    and they don't have ability to remember stored conversations for long term and
    this is something we want to fixã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚è¦ä½¿è¿™ä¸€åˆ‡æˆåŠŸä»ç„¶ç¼ºå°‘å¾ˆå¤šè¦ç´ ï¼Œé¦–å…ˆæ˜¯å¤–éƒ¨è®°å¿†ï¼Œæˆ‘ä»¬å·²ç»å¼€å§‹åœ¨åƒChaGPTè¿™æ ·çš„æ¨¡å‹ä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ï¼Œäº¤äº’æ˜¯çŸ­æš‚çš„ã€‚æ²¡æœ‰é•¿æœŸè®°å¿†ï¼Œä¹Ÿæ— æ³•é•¿æœŸè®°ä½å­˜å‚¨çš„å¯¹è¯ï¼Œè¿™æ­£æ˜¯æˆ‘ä»¬æƒ³è¦è§£å†³çš„é—®é¢˜ã€‚
- en: ğŸ˜Šï¼ŒSecond over second is reducing the computation complexityã€‚so attention mechanism
    is quadtic over the sequence length which is slu and we want to reduce itra make
    it fasterã€‚è¯¶ã€‚Another thing you want to do is we want to enhance the controllability
    of these models like a lot of these models can be stochastic and we want to be
    able to control what sort of outputs we get from them and you might have experienced
    the charge if you just refresh you get like different output each timeã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ¯ç§’æ¯ç§’éƒ½åœ¨å‡å°‘è®¡ç®—å¤æ‚åº¦ã€‚å› æ­¤ï¼Œæ³¨æ„æœºåˆ¶åœ¨åºåˆ—é•¿åº¦ä¸Šæ˜¯äºŒæ¬¡çš„ï¼Œæˆ‘ä»¬æƒ³è¦å‡å°‘å®ƒï¼Œä½¿å…¶æ›´å¿«ã€‚è¯¶ã€‚ä½ è¿˜æƒ³åšçš„å¦ä¸€ä»¶äº‹æ˜¯å¢å¼ºè¿™äº›æ¨¡å‹çš„å¯æ§æ€§ï¼Œå› ä¸ºå¾ˆå¤šæ¨¡å‹å¯èƒ½æ˜¯éšæœºçš„ï¼Œæˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿæ§åˆ¶æˆ‘ä»¬ä»ä¸­è·å¾—çš„è¾“å‡ºç±»å‹ï¼Œä½ å¯èƒ½ä½“éªŒè¿‡è¿™ç§å˜åŒ–ï¼Œåªéœ€åˆ·æ–°ä¸€æ¬¡ï¼Œæ¯æ¬¡å¾—åˆ°çš„è¾“å‡ºéƒ½ä¸ä¸€æ ·ã€‚
- en: but you might want to have mechanism black printersï¼Œ what sort of things you
    canã€‚And finally we want to align our state of art language models with how the
    human brain works and we are seeing the searchã€‚but we still need more research
    on seeing how they can be more importantã€‚å¯ç³»å“¼ã€‚ğŸ˜Šï¼ŒYesã€‚I'm excited to be here I live
    very nearby so I got the invites to come to class and I was likeã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä½ å¯èƒ½æƒ³è¦æœ‰æœºåˆ¶é»‘ç®±æ‰“å°æœºï¼Œçœ‹çœ‹ä½ èƒ½åšäº›ä»€ä¹ˆã€‚æœ€åï¼Œæˆ‘ä»¬å¸Œæœ›å°†æˆ‘ä»¬æœ€å…ˆè¿›çš„è¯­è¨€æ¨¡å‹ä¸äººè„‘çš„å·¥ä½œæ–¹å¼å¯¹é½ï¼Œæˆ‘ä»¬æ­£åœ¨çœ‹åˆ°æœç´¢ï¼Œä½†æˆ‘ä»¬ä»ç„¶éœ€è¦æ›´å¤šç ”ç©¶ï¼Œçœ‹çœ‹å®ƒä»¬èƒ½å¦‚ä½•æ›´é‡è¦ã€‚å¯ç³»å“¼ã€‚ğŸ˜Šï¼Œæ˜¯çš„ã€‚æˆ‘å¾ˆé«˜å…´åœ¨è¿™é‡Œï¼Œæˆ‘ä½å¾—å¾ˆè¿‘ï¼Œæ‰€ä»¥æˆ‘æ”¶åˆ°äº†æ¥ä¸Šè¯¾çš„é‚€è¯·ï¼Œæˆ‘å°±åƒã€‚
- en: okay I'll just walk over but then I spent like 10 hours on the slides so it
    wasn't as simpleã€‚So yeah I want to talk about transformersï¼Œ I'm going to skip
    the first two over thereã€‚we're not going to talk about those we're going to talk
    about that one just to simplify the lecture here since we've done have timeã€‚![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_9.png)
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæˆ‘å°±èµ°è¿‡å»ï¼Œä½†æˆ‘åœ¨å¹»ç¯ç‰‡ä¸ŠèŠ±äº†å¤§çº¦10ä¸ªå°æ—¶ï¼Œæ‰€ä»¥è¿™å¹¶ä¸ç®€å•ã€‚æ‰€ä»¥ï¼Œæ˜¯çš„ï¼Œæˆ‘æƒ³è°ˆè°ˆå˜å‹å™¨ï¼Œæˆ‘å°†è·³è¿‡å‰ä¸¤ä¸ªã€‚æˆ‘ä»¬ä¸ä¼šè®¨è®ºé‚£äº›ï¼Œæˆ‘ä»¬å°†è®¨è®ºè¿™ä¸ªï¼Œä»¥ç®€åŒ–è®²åº§ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰æ—¶é—´ã€‚![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_9.png)
- en: å—¯ã€‚Okayï¼Œ so I wanted to provide a little bit of context of why does this transformers
    class even exist so a little bit of historical context I feel like Bbo over there
    I joined like telling you guys about this I don't know if you guys saw the drinks
    and basically I joined AI in roughly 2012 and full course so maybe a decade ago
    and back then you wouldn't even say that you joined AI by the way that was like
    a dirty word now it's okay to talk about but back then it was not even deep learning
    it was machine learning that was a term use if you were serious but now now AI
    is okay to use I thinkã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚å¥½çš„ï¼Œæˆ‘æƒ³æä¾›ä¸€äº›èƒŒæ™¯ï¼Œè§£é‡Šä¸ºä»€ä¹ˆè¿™ä¸ªå˜å‹å™¨ç±»å­˜åœ¨ï¼Œæ‰€ä»¥æˆ‘è§‰å¾—æœ‰ä¸€ç‚¹å†å²èƒŒæ™¯ã€‚æˆ‘åŠ å…¥AIå¤§çº¦åœ¨2012å¹´ï¼Œå…¨èŒè¯¾ç¨‹ï¼Œæ‰€ä»¥å¤§çº¦åå¹´å‰ã€‚é‚£æ—¶å€™ä½ ç”šè‡³ä¸ä¼šè¯´ä½ åŠ å…¥äº†AIï¼Œé‚£ä¸ªè¯æœ‰ç‚¹è„ï¼Œç°åœ¨è°ˆè®ºè¿™äº›æ˜¯å¯ä»¥çš„ï¼Œä½†é‚£æ—¶ç”šè‡³æ²¡æœ‰æ·±åº¦å­¦ä¹ ï¼Œåªæœ‰æœºå™¨å­¦ä¹ ï¼Œè¿™æ˜¯ä¸€ä¸ªè®¤çœŸçš„æœ¯è¯­ã€‚ä½†ç°åœ¨AIå¯ä»¥ä½¿ç”¨ï¼Œæˆ‘è§‰å¾—ã€‚
- en: So basically do you even realize how lucky you are potentially entering this
    area and roughly 2203 so back then in 2011 or so when I was working specifically
    on computer visionã€‚![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_11.png)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼ŒåŸºæœ¬ä¸Šä½ æœ‰æ²¡æœ‰æ„è¯†åˆ°ä½ æœ‰å¤šå¹¸è¿ï¼Œæ½œåœ¨åœ°è¿›å…¥è¿™ä¸ªé¢†åŸŸï¼Œå¤§çº¦åœ¨2203å¹´ã€‚é‚£æ—¶å€™ï¼Œå¤§çº¦åœ¨2011å¹´ï¼Œæˆ‘ä¸“é—¨ä»äº‹è®¡ç®—æœºè§†è§‰çš„å·¥ä½œã€‚![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_11.png)
- en: Your your pipelines looked like thisï¼Œ so you wanted to classify some images
    you would go to a paper and I think this is representative you would have three
    pages in the paper describing all kinds of a zoo of kitchen sink of different
    kinds of featuresã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„ç®¡é“çœ‹èµ·æ¥åƒè¿™æ ·ï¼Œä½ æƒ³åˆ†ç±»ä¸€äº›å›¾åƒï¼Œä½ ä¼šå»æŸ¥è®ºæ–‡ï¼Œæˆ‘è§‰å¾—è¿™å¾ˆæœ‰ä»£è¡¨æ€§ï¼Œè®ºæ–‡ä¸­ä¼šæœ‰ä¸‰é¡µæè¿°å„ç§ä¸åŒç‰¹å¾çš„â€œå¨æˆ¿æ°´æ§½â€ã€‚
- en: descriptors and you would go to poster session and in computer vision conference
    and everyone who have their paper feature descriptors that they're proposing and
    it's totally ridiculous and you would take notes on like which one you should
    incorporate into your pipeline because you would extract all of them and then
    you would put an SVM on top so that's what you would do so there's two pages make
    sure you get your spars sip histogramsã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: æè¿°ç¬¦ï¼Œä½ ä¼šå»è®¡ç®—æœºè§†è§‰ä¼šè®®çš„æµ·æŠ¥å±•ï¼Œæ¯ä¸ªäººéƒ½æœ‰ä»–ä»¬æè®®çš„è®ºæ–‡ç‰¹å¾æè¿°ç¬¦ï¼Œè¿™ç®€ç›´æ˜¯è’è°¬çš„ã€‚ä½ ä¼šè®°å½•å“ªäº›åº”è¯¥çº³å…¥ä½ çš„ç®¡é“ï¼Œå› ä¸ºä½ ä¼šæå–æ‰€æœ‰è¿™äº›ç‰¹å¾ï¼Œç„¶ååœ¨ä¸Šé¢æ”¾ä¸€ä¸ªæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ï¼Œæ‰€ä»¥ä½ ä¼šè¿™ä¹ˆåšã€‚ç¡®ä¿ä½ è·å–ä½ çš„ç¨€ç–ç›´æ–¹å›¾ã€‚
- en: your SSIs your color histogramsï¼Œ textilesï¼Œ tiny images and don't forget the
    geometry specific histogramsã€‚all of them had basically complicated code by themselves
    you're collecting code from everywhere and running it and it was a total nightmare
    soã€‚On top of that it also didn't work so this would be I think representative
    prediction from that time you would just get predictions like this once in a while
    and you'd be like you just shrug your shoulders like that just happens once in
    a while today you would be looking for a bugã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çš„SSIsæ˜¯é¢œè‰²ç›´æ–¹å›¾ã€çººç»‡å“ã€å¾®å°å›¾åƒï¼Œåˆ«å¿˜äº†å‡ ä½•ç‰¹å®šç›´æ–¹å›¾ã€‚å®ƒä»¬æœ¬èº«çš„ä»£ç åŸºæœ¬ä¸Šæ˜¯å¤æ‚çš„ï¼Œä½ ä»å„å¤„æ”¶é›†ä»£ç å¹¶è¿è¡Œï¼Œå®Œå…¨æ˜¯ä¸€åœºå™©æ¢¦ã€‚æ‰€ä»¥ï¼Œæ›´ç³Ÿçš„æ˜¯å®ƒè¿˜ä¸ç®¡ç”¨ï¼Œæˆ‘è®¤ä¸ºè¿™ä»£è¡¨äº†é‚£æ—¶çš„é¢„æµ‹ï¼Œä½ å¶å°”ä¼šå¾—åˆ°è¿™æ ·çš„é¢„æµ‹ï¼Œä½ åªèƒ½è€¸è€¸è‚©ï¼Œè§‰å¾—è¿™å°±å¶å°”å‘ç”Ÿï¼Œè€Œä»Šå¤©ä½ ä¼šå¯»æ‰¾bugã€‚
- en: And worse than thatã€‚Every single every single sort of feel every single chunk
    of AI had their own completely separate vocabulary that they work withã€‚so if if
    you go to NLP papersï¼Œ those papers would be completely different so you're reading
    the NLP paper and you're like what is this part of speech tagging morphological
    analysis syntactic parsing coreference resolution what is NPBT JJ and you're confused
    so the vocabulary and everything was completely different and you couldn't read
    papers I would say across different areasï¼Ÿ
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ç³Ÿçš„æ˜¯ï¼Œæ¯ä¸ªAIé¢†åŸŸéƒ½æœ‰è‡ªå·±å®Œå…¨ç‹¬ç«‹çš„è¯æ±‡ã€‚å¦‚æœä½ å»çœ‹NLPè®ºæ–‡ï¼Œè¿™äº›è®ºæ–‡ä¼šå®Œå…¨ä¸åŒï¼Œä½ åœ¨è¯»NLPè®ºæ–‡æ—¶ï¼Œå¯èƒ½ä¼šç–‘æƒ‘â€œè¿™æ˜¯ä»€ä¹ˆï¼Ÿè¯æ€§æ ‡æ³¨ã€å½¢æ€åˆ†æã€å¥æ³•è§£æã€æŒ‡ä»£æ¶ˆè§£ï¼ŒNPBTã€JJåˆæ˜¯ä»€ä¹ˆï¼Ÿâ€å› æ­¤ï¼Œè¯æ±‡å’Œä¸€åˆ‡éƒ½æ˜¯å®Œå…¨ä¸åŒçš„ï¼Œä½ å‡ ä¹æ— æ³•è·¨é¢†åŸŸé˜…è¯»è®ºæ–‡ã€‚
- en: So now that changed a little bit starting in 2012 when Askochevsky and colleagues
    basically demonstrated that if you scale a large neural network on large data
    set you can get very strong performance and so up till then there was a lot of
    focus on algorithms but this showed that actually neural net scale very well so
    you need to now worry about compute and data and if you scale it up works pretty
    well and then that recipe actually did copy paste across many areas of AI so we
    started to see a neural networks pop up everywhere since 2012 so we saw them computer
    vision and NLP and speech and translation in RL and so on so everyone started
    to use the same kind of modeling tool modeling framework and now when you go to
    NLP and you start reading papers there in machine translation for exampleã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æƒ…å†µåœ¨2012å¹´å‘ç”Ÿäº†å˜åŒ–ï¼Œå½“AskochevskyåŠå…¶åŒäº‹å±•ç¤ºäº†å¦‚æœåœ¨å¤§å‹æ•°æ®é›†ä¸Šæ‰©å±•å¤§å‹ç¥ç»ç½‘ç»œï¼Œå¯ä»¥è·å¾—å¾ˆå¼ºçš„æ€§èƒ½ã€‚åˆ°é‚£æ—¶ï¼Œå¤§å®¶å¯¹ç®—æ³•çš„å…³æ³¨å¾ˆå¤šï¼Œä½†è¿™è¡¨æ˜ç¥ç»ç½‘ç»œæ‰©å±•å¾—å¾ˆå¥½ï¼Œæ‰€ä»¥ä½ ç°åœ¨éœ€è¦å…³æ³¨è®¡ç®—å’Œæ•°æ®ï¼Œå¦‚æœä½ æ‰©å¤§è§„æ¨¡ï¼Œå®ƒçš„æ•ˆæœç›¸å½“ä¸é”™ã€‚è¿™ç§æ–¹æ³•ä¹Ÿåœ¨AIçš„è®¸å¤šé¢†åŸŸå¤åˆ¶ç²˜è´´ï¼Œå› æ­¤è‡ª2012å¹´èµ·æˆ‘ä»¬å¼€å§‹çœ‹åˆ°ç¥ç»ç½‘ç»œåœ¨å„å¤„å‡ºç°ï¼ŒåŒ…æ‹¬è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€è¯­éŸ³ã€ç¿»è¯‘å’Œå¼ºåŒ–å­¦ä¹ ç­‰ã€‚å› æ­¤ï¼Œæ¯ä¸ªäººå¼€å§‹ä½¿ç”¨ç›¸åŒçš„å»ºæ¨¡å·¥å…·å’Œæ¡†æ¶ï¼Œè€Œç°åœ¨å½“ä½ å»é˜…è¯»NLPé¢†åŸŸçš„è®ºæ–‡ï¼Œæ¯”å¦‚æœºå™¨ç¿»è¯‘æ—¶ã€‚
- en: this is a sequence of sequence of paper which will come back to in a bit you
    start to read those papers and you're like okay I can recognize these words like
    there's a neural network there's some parameters there's an optimizer and it starts
    to read like things that you know of so that decreased tremendously the barrier
    to entry acrossã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªè®ºæ–‡çš„åºåˆ—ï¼Œæˆ‘ä»¬ç¨åä¼šå†æåˆ°ã€‚å½“ä½ å¼€å§‹é˜…è¯»è¿™äº›è®ºæ–‡æ—¶ï¼Œä½ ä¼šå‘ç°â€œå“¦ï¼Œæˆ‘èƒ½è¯†åˆ«è¿™äº›è¯ï¼šæœ‰ç¥ç»ç½‘ç»œã€æœ‰ä¸€äº›å‚æ•°ã€æœ‰ä¸€ä¸ªä¼˜åŒ–å™¨â€ï¼Œå®ƒå¼€å§‹è¯»èµ·æ¥åƒä½ ç†Ÿæ‚‰çš„å†…å®¹ã€‚è¿™å¤§å¤§é™ä½äº†è¿›å…¥ä¸åŒé¢†åŸŸçš„é—¨æ§›ã€‚
- en: The different areasã€‚And then I think the big deal is that when the transformer
    came out in 2017ã€‚it's not even that just the toolkits and the neural networks
    were similar is that literally the architectures converge to like one architecture
    that you copy paste across everything seemingly so this was kind of an unassuming
    machine translation paper at the time proposing the transformer architecture but
    what we found since then is that you can just basically copy paste this architectureã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„é¢†åŸŸã€‚æˆ‘è®¤ä¸ºå…³é”®åœ¨äº2017å¹´å˜å‹å™¨ï¼ˆtransformerï¼‰é—®ä¸–æ—¶ã€‚å¹¶ä¸æ˜¯è¯´å·¥å…·åŒ…å’Œç¥ç»ç½‘ç»œç›¸ä¼¼ï¼Œè€Œæ˜¯æ¶æ„ç¡®å®è¶‹å‘äºä¸€ç§æ¶æ„ï¼Œä½ å¯ä»¥åœ¨æ‰€æœ‰åœ°æ–¹å¤åˆ¶ç²˜è´´ã€‚é‚£æ—¶è¿™æ˜¯ä¸€ç¯‡çœ‹ä¼¼ä¸èµ·çœ¼çš„æœºå™¨ç¿»è¯‘è®ºæ–‡ï¼Œæå‡ºäº†å˜å‹å™¨æ¶æ„ï¼Œä½†è‡ªé‚£æ—¶èµ·æˆ‘ä»¬å‘ç°ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥éšæ„å¤åˆ¶ç²˜è´´è¿™ä¸ªæ¶æ„ã€‚
- en: And use it everywhere and what's changing is the details of the data and the
    chunking of the data and how you feed in and you know that's a caric but it's
    kind of like correct first order statement and so now papers are even more similar
    looking because everyone is just using transformer and so this convergence was
    remarkable to watch and unfolded over the last decade and it's crazy to me what
    I find kind of interesting is I think this is some kind of a hint that we're maybe
    converging to something that maybe the brain is doing because the brain is very
    homogeneous and uniform across the entire sheet of your cortex and okay maybe
    some of the details are changing but those feel like hyperparmeters of like a
    transformer but your auditory cortex and your visual cortex and everything else
    looks very similar and so maybe we're converging to some kind of a uniform powerful
    learning algorithm here something like that I think is kind of interestingã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å°†å…¶åº”ç”¨äºå„ä¸ªé¢†åŸŸï¼Œå˜åŒ–çš„åªæ˜¯æ•°æ®çš„ç»†èŠ‚ã€æ•°æ®çš„åˆ†å—æ–¹å¼ä»¥åŠå¦‚ä½•è¾“å…¥æ•°æ®ã€‚ä½ çŸ¥é“è¿™æœ‰äº›å¤¸å¼ ï¼Œä½†è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªæ­£ç¡®çš„ä¸€é˜¶é™ˆè¿°ï¼Œå› æ­¤ç°åœ¨çš„è®ºæ–‡çœ‹èµ·æ¥æ›´åŠ ç›¸ä¼¼ï¼Œå› ä¸ºæ¯ä¸ªäººéƒ½åœ¨ä½¿ç”¨å˜å‹å™¨ï¼Œè¿™ç§è¶‹åŒåœ¨è¿‡å»åå¹´ä¸­ä»¤äººç©ç›®ã€‚æˆ‘å‘ç°æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘è®¤ä¸ºè¿™æˆ–è®¸æš—ç¤ºæˆ‘ä»¬å¯èƒ½æ­£åœ¨è¶‹å‘æŸç§å¤§è„‘åœ¨åšçš„äº‹æƒ…ï¼Œå› ä¸ºå¤§è„‘åœ¨æ•´ä¸ªçš®å±‚çš„è¡¨é¢ä¸Šæ˜¯éå¸¸åŒè´¨å’Œå‡åŒ€çš„ã€‚å¥½çš„ï¼Œä¹Ÿè®¸ä¸€äº›ç»†èŠ‚åœ¨å˜åŒ–ï¼Œä½†é‚£äº›æ„Ÿè§‰åƒæ˜¯å˜å‹å™¨çš„è¶…å‚æ•°ï¼Œä½ çš„å¬è§‰çš®å±‚ã€è§†è§‰çš®å±‚ä»¥åŠå…¶ä»–ä¸€åˆ‡çœ‹èµ·æ¥éƒ½éå¸¸ç›¸ä¼¼ï¼Œå› æ­¤æˆ‘ä»¬ä¹Ÿè®¸æ­£åœ¨è¶‹å‘æŸç§ç»Ÿä¸€çš„å¼ºå¤§å­¦ä¹ ç®—æ³•ï¼Œæˆ‘è§‰å¾—è¿™ä¸€ç‚¹ç›¸å½“æœ‰è¶£ã€‚
- en: Okayï¼Œ so I want to talk about where the transformer came from briefly historicallyã€‚So
    I want to start in 2003ï¼Œ I like this paper quite a bit it was the first sort ofã€‚Popular
    application of neural networks to the problem of language modeling so predicting
    in this case the next word in a sequenceã€‚which allows you to build generative
    models over text and in this case they were using multiier perceptron so a very
    simple neural the neural net took three words and predicted the probability distribution
    fourth word in a sequence So this was well and good at this point Now over time
    people started to apply this to a machine translation so that brings us to sequence
    to sequence paper from 2014 that was pretty influential and the big problem here
    wasã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæˆ‘æƒ³ç®€è¦è°ˆè°ˆå˜å‹å™¨çš„å†å²èµ·æºã€‚æˆ‘æƒ³ä»2003å¹´å¼€å§‹ï¼Œæˆ‘éå¸¸å–œæ¬¢è¿™ç¯‡è®ºæ–‡ï¼Œè¿™æ˜¯ç¥ç»ç½‘ç»œé¦–æ¬¡è¢«åº”ç”¨äºè¯­è¨€å»ºæ¨¡é—®é¢˜çš„æµè¡Œåº”ç”¨ï¼Œå› æ­¤åœ¨è¿™ç§æƒ…å†µä¸‹é¢„æµ‹åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ã€‚è¿™ä½¿ä½ èƒ½å¤Ÿå»ºç«‹æ–‡æœ¬çš„ç”Ÿæˆæ¨¡å‹ï¼Œè€Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»–ä»¬ä½¿ç”¨äº†å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œå®ƒå–ä¸‰ä¸ªå•è¯å¹¶é¢„æµ‹åºåˆ—ä¸­ç¬¬å››ä¸ªå•è¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚åˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™å¾ˆå¥½ã€‚éšç€æ—¶é—´çš„æ¨ç§»ï¼Œäººä»¬å¼€å§‹å°†å…¶åº”ç”¨äºæœºå™¨ç¿»è¯‘ï¼Œè¿™ä½¿æˆ‘ä»¬å›åˆ°äº†2014å¹´çš„åºåˆ—åˆ°åºåˆ—è®ºæ–‡ï¼Œé‚£ç¯‡è®ºæ–‡ç›¸å½“æœ‰å½±å“åŠ›ï¼Œè€Œè¿™é‡Œçš„å¤§é—®é¢˜æ˜¯ã€‚
- en: okay we do just want to take three words and predict it for we want to predict
    how to go from an English sentence to a French sentence and the key problem wasã€‚okay
    you can have arbitrary number of words in English and arbitrary number of words
    in French so how do you get an architecture that can process thisvariably sized
    inputã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæˆ‘ä»¬ç¡®å®åªæƒ³å–ä¸‰ä¸ªå•è¯å¹¶è¿›è¡Œé¢„æµ‹ï¼Œæˆ‘ä»¬æƒ³é¢„æµ‹å¦‚ä½•å°†è‹±æ–‡å¥å­è½¬æ¢ä¸ºæ³•æ–‡å¥å­ï¼Œå…³é”®é—®é¢˜æ˜¯ï¼šå¥½çš„ï¼Œä½ å¯ä»¥åœ¨è‹±è¯­ä¸­æœ‰ä»»æ„æ•°é‡çš„å•è¯ï¼Œåœ¨æ³•è¯­ä¸­ä¹Ÿæœ‰ä»»æ„æ•°é‡çš„å•è¯ï¼Œé‚£ä¹ˆå¦‚ä½•æ„å»ºä¸€ä¸ªèƒ½å¤Ÿå¤„ç†è¿™ç§å¯å˜å¤§å°è¾“å…¥çš„æ¶æ„å‘¢ï¼Ÿ
- en: And so here they use a LSTM and there's basically two chunks of thisã€‚which are
    covered the Sla by theã€‚ã¾ã§ã™ã€‚But basically have an encoder LSTM on the left and
    it just consumes one word at a time and builds up a context of what it has readã€‚and
    then that acts as a conditioning vector to the decoder RN or LSTM that basically
    goes chunk chunk chunk for the next word in a sequence translating the English
    to French or something like thatã€‚Now the big problem with this that people identified
    I think very quickly and tried to resolve is that there's what's called this encoded
    bottleneckã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é‡Œä»–ä»¬ä½¿ç”¨äº†LSTMï¼ŒåŸºæœ¬ä¸Šæœ‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œè¿™äº›éƒ¨åˆ†è¢«Slaæ‰€è¦†ç›–ã€‚ä½†åŸºæœ¬ä¸Šï¼Œå·¦ä¾§æœ‰ä¸€ä¸ªç¼–ç å™¨LSTMï¼Œå®ƒä¸€æ¬¡æ¶ˆè´¹ä¸€ä¸ªå•è¯å¹¶å»ºç«‹å®ƒæ‰€é˜…è¯»å†…å®¹çš„ä¸Šä¸‹æ–‡ã€‚ç„¶åå®ƒä½œä¸ºè§£ç å™¨RNæˆ–LSTMçš„æ¡ä»¶å‘é‡ï¼Œåè€…åŸºæœ¬ä¸Šé€ä¸ªå•è¯ç¿»è¯‘åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯ï¼Œå°†è‹±è¯­ç¿»è¯‘æˆæ³•è¯­ï¼Œæˆ–ç±»ä¼¼çš„ä¸œè¥¿ã€‚äººä»¬å¾ˆå¿«è¯†åˆ«åˆ°çš„é—®é¢˜æ˜¯æ‰€è°“çš„ç¼–ç ç“¶é¢ˆã€‚
- en: so this entire English sentence that we are trying to condition on is packed
    into a single vector that goes from the encoder for the decoder and so this is
    just too much information to potentially maintain a single vector and that didn't
    seem correct and so people are looking around for ways to alleviate the attention
    of sorry the encoded bottleneck as it was called at timeã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¯•å›¾è¿›è¡Œæ¡ä»¶å¤„ç†çš„æ•´ä¸ªè‹±æ–‡å¥å­è¢«æ‰“åŒ…æˆä¸€ä¸ªä»ç¼–ç å™¨ä¼ é€’ç»™è§£ç å™¨çš„å•ä¸€å‘é‡ï¼Œå› æ­¤è¿™å¯èƒ½åŒ…å«çš„ä¿¡æ¯å¤ªå¤šï¼Œæ— æ³•ç»´æŒä¸€ä¸ªå•ä¸€çš„å‘é‡ï¼Œè¿™ä¼¼ä¹ä¸å¤ªæ­£ç¡®ï¼Œäºæ˜¯äººä»¬å¼€å§‹å¯»æ‰¾ç¼“è§£ç¼–ç ç“¶é¢ˆï¼ˆå½“æ—¶è¢«ç§°ä¸ºç¼–ç ç“¶é¢ˆï¼‰çš„æ–¹æ³•ã€‚
- en: And so that brings us to this paperï¼Œ neural machine translation by jointly learning
    to align and translateã€‚Here just quo from the abstract in this paperï¼Œ we conjected
    that the use of a fixed length vector is a bottleneck in improving the performance
    of the basic encoded decoder architecture and proposed to extend this by allowing
    the model to automatically soft search for parts of the source sentence that are
    relevant to predicting target word yeah without having to form these parts or
    heart segments explicitly so this was a way to look back to the words that are
    coming from the encoder and it was achieved using this soft search so as you are
    decoding in theã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è®©æˆ‘ä»¬å›åˆ°è¿™ç¯‡è®ºæ–‡ï¼Œç¥ç»æœºå™¨ç¿»è¯‘é€šè¿‡è”åˆå­¦ä¹ å¯¹é½å’Œç¿»è¯‘ã€‚è¿™æ˜¯è¿™ç¯‡è®ºæ–‡æ‘˜è¦ä¸­çš„ä¸€æ®µï¼Œæˆ‘ä»¬æ¨æµ‹ä½¿ç”¨å›ºå®šé•¿åº¦å‘é‡æ˜¯æé«˜åŸºæœ¬ç¼–ç è§£ç å™¨æ¶æ„æ€§èƒ½çš„ç“¶é¢ˆï¼Œå¹¶å»ºè®®é€šè¿‡å…è®¸æ¨¡å‹è‡ªåŠ¨softæœç´¢ä¸é¢„æµ‹ç›®æ ‡è¯ç›¸å…³çš„æºå¥å­éƒ¨åˆ†æ¥æ‰©å±•è¿™ä¸€ç‚¹ï¼Œè€Œä¸éœ€è¦æ˜¾å¼å½¢æˆè¿™äº›éƒ¨åˆ†æˆ–æ ¸å¿ƒç‰‡æ®µï¼Œå› æ­¤è¿™æ˜¯ä¸€ç§å›é¡¾æ¥è‡ªç¼–ç å™¨çš„è¯æ±‡çš„æ–¹æ³•ï¼Œä½¿ç”¨è¿™ç§softæœç´¢å®ç°äº†è¿™ä¸€ç‚¹ï¼Œå› æ­¤åœ¨è§£ç æ—¶ã€‚
- en: The words hereï¼Œ while you are decoding them you are allowed to look back at
    the words at the encoder via this soft attention mechanism proposed in this paper
    and so this paper I think is the first time that I saw basically attention so
    your context vector that comes from the encoder is a weighted sum of the hidden
    states of the words in the in the encodingã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œçš„è¯æ±‡ï¼Œå½“ä½ åœ¨è§£ç å®ƒä»¬æ—¶ï¼Œä½ å¯ä»¥é€šè¿‡è¿™ç¯‡è®ºæ–‡æå‡ºçš„soft attentionæœºåˆ¶å›é¡¾ç¼–ç å™¨ä¸­çš„è¯æ±‡ï¼Œå› æ­¤æˆ‘è®¤ä¸ºè¿™ç¯‡è®ºæ–‡æ˜¯æˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°åŸºæœ¬çš„æ³¨æ„åŠ›ï¼Œå› æ­¤æ¥è‡ªç¼–ç å™¨çš„ä¸Šä¸‹æ–‡å‘é‡æ˜¯ç¼–ç ä¸­è¯æ±‡çš„éšè—çŠ¶æ€çš„åŠ æƒæ€»å’Œã€‚
- en: And then the weights of this come from a softmax that is based on these compatbilities
    between the current state as you're decoding and the hidden states generated by
    the encoder and so this is the first time that really you start to like look at
    it and this is the current modern equations of the attention and I think this
    was the first paper that I saw it in is the first time that there's a word attention
    used as far as I know to call this mechanism so I actually tried to dig into the
    details of the history of the attention so the first author here Dmitri I had
    an email correspondence with him and I basically sent him an email I'm like Dmitri
    this is really interesting transformers have've taken over where did you come
    up with the soft attention mechanism that ends up being the heart of the transformer
    and to my surprise he wrote me back this like massive email which was really fascinating
    so this is an excerpt from that emailã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œè¿™é‡Œçš„æƒé‡æ¥è‡ªä¸€ä¸ªåŸºäºå½“å‰çŠ¶æ€å’Œç¼–ç å™¨ç”Ÿæˆçš„éšè—çŠ¶æ€ä¹‹é—´å…¼å®¹æ€§çš„softmaxï¼Œå› æ­¤è¿™æ˜¯ä½ ç¬¬ä¸€æ¬¡çœŸæ­£å¼€å§‹å…³æ³¨å®ƒï¼Œè¿™æ˜¯ç°ä»£æ³¨æ„åŠ›æœºåˆ¶çš„å½“å‰æ–¹ç¨‹ï¼Œæˆ‘è®¤ä¸ºæˆ‘ç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªæ¦‚å¿µçš„è®ºæ–‡æ˜¯é¦–ä¸ªä½¿ç”¨â€œæ³¨æ„åŠ›â€è¿™ä¸ªè¯çš„è®ºæ–‡ï¼Œæ®æˆ‘æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç§°è¿™ç§æœºåˆ¶çš„ï¼Œå› æ­¤æˆ‘å®é™…ä¸Šè¯•å›¾æ·±å…¥äº†è§£æ³¨æ„åŠ›çš„å†å²ï¼Œç¬¬ä¸€ä½œè€…å¾·ç±³ç‰¹é‡Œï¼Œæˆ‘ä¸ä»–æœ‰è¿‡ç”µå­é‚®ä»¶äº¤æµï¼Œæˆ‘åŸºæœ¬ä¸Šç»™ä»–å‘äº†ä¸€å°é‚®ä»¶ï¼Œæˆ‘è¯´å¾·ç±³ç‰¹é‡Œï¼Œè¿™çœŸæœ‰è¶£ï¼Œå˜å‹å™¨å·²ç»æ¥ç®¡äº†ï¼Œä½ æ˜¯ä»å“ªé‡Œæƒ³åˆ°soft
    attentionæœºåˆ¶çš„ï¼Œè¿™ä¸ªæœºåˆ¶æˆä¸ºå˜å‹å™¨çš„æ ¸å¿ƒï¼Œä»¤æˆ‘æƒŠè®¶çš„æ˜¯ï¼Œä»–å›äº†æˆ‘ä¸€å°éå¸¸é•¿çš„é‚®ä»¶ï¼Œå†…å®¹éå¸¸å¼•äººå…¥èƒœï¼Œè¿™é‡Œæ˜¯é‚£å°é‚®ä»¶çš„æ‘˜å½•ã€‚
- en: å—¯ã€‚So basically he talks about how he was looking for a way to avoid this bottleneck
    between the encoder and decoderã€‚he had some ideas about cursors that traversed
    the sequences that didn't quite work out and then here so one day I had this thought
    that it would be nice to enable the decoder RN to learn to search where to put
    the cursor in a source sequence this was sort of inspired by translation exercises
    that learning English in my middle school involved bulk you gaze shifts back and
    forth in the source and target sequence as you translate so literally I thought
    this was kind of interesting that he's not made English speaker and here that
    gave him an edge in this machine translation that led to attention and then led
    to transformer so that's really fascinating I expressed a soft search as softmax
    and that way the averaging of the ByN statess and basically to my great excitement
    this dis work from the very first try so really I think interesting piece of history
    and as it later turned out the name of RNN search was kind of blame so the better
    name attention came from Yohua on one of the final passesã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šä»–è°ˆåˆ°ä»–åœ¨å¯»æ‰¾ä¸€ç§æ–¹æ³•ï¼Œä»¥é¿å…ç¼–ç å™¨å’Œè§£ç å™¨ä¹‹é—´çš„ç“¶é¢ˆã€‚ä»–æœ‰ä¸€äº›å…³äºå…‰æ ‡éå†åºåˆ—çš„æƒ³æ³•ï¼Œä½†æ²¡æœ‰å®Œå…¨å®ç°ã€‚ç„¶åæœ‰ä¸€å¤©ï¼Œæˆ‘æƒ³åˆ°ï¼Œè®©è§£ç å™¨RNNå­¦ä¹ åœ¨æºåºåˆ—ä¸­æ”¾ç½®å…‰æ ‡çš„ä½ç½®ä¼šå¾ˆå¥½ï¼Œè¿™ç§æƒ³æ³•å—åˆ°äº†æˆ‘åˆä¸­å­¦ä¹ è‹±è¯­æ—¶ç¿»è¯‘ç»ƒä¹ çš„å¯å‘ï¼Œæ¶‰åŠåˆ°åœ¨æºåºåˆ—å’Œç›®æ ‡åºåˆ—ä¹‹é—´å¤§é‡çš„æ³¨è§†è½¬ç§»ã€‚æ‰€ä»¥å­—é¢ä¸Šæ¥è¯´ï¼Œæˆ‘è§‰å¾—è¿™å¾ˆæœ‰è¶£ï¼Œä»–ä¸æ˜¯ä»¥è‹±è¯­ä¸ºæ¯è¯­çš„äººï¼Œè¿™è®©ä»–åœ¨æœºå™¨ç¿»è¯‘ä¸­å äº†ä¾¿å®œï¼Œå¯¼è‡´äº†æ³¨æ„åŠ›æœºåˆ¶çš„å‡ºç°ï¼Œè¿›è€Œå¯¼è‡´äº†å˜æ¢å™¨çš„å‡ºç°ã€‚è¿™çœŸæ˜¯ä»¤äººç€è¿·ï¼Œæˆ‘å°†è½¯æœç´¢è¡¨è¾¾ä¸ºsoftmaxï¼Œé€šè¿‡è¿™ç§æ–¹å¼å¯¹ByNçŠ¶æ€è¿›è¡Œå¹³å‡ï¼ŒåŸºæœ¬ä¸Šè®©æˆ‘éå¸¸å…´å¥‹çš„æ˜¯ï¼Œè¿™é¡¹å·¥ä½œåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶å°±æˆåŠŸäº†ã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸æœ‰è¶£çš„å†å²ç‰‡æ®µï¼Œåæ¥å‘ç°RNNæœç´¢è¿™ä¸ªåå­—æœ‰äº›ä¸å¦¥ï¼Œæœ€ç»ˆçš„å¥½åå­—â€œæ³¨æ„åŠ›â€æ˜¯æ¥è‡ªçº¦ä¹¦äºšçš„æœ€åå‡ æ¬¡ä¿®æ”¹ã€‚
- en: As they went over the paperï¼Œ so maybe attention is all you need would have been
    called like harnesss or just holdã€‚But we have Yoshua Benio to thank for a little
    bit of better name I would sayã€‚so apparently that's the history of this subject
    that was interestingã€‚Okayã€‚so that brings us to 2017ï¼Œ which is attention is all
    unique so this attention component which in Dimetrius paper was just like one
    small segment and there's all this bidirectional RN RN and decoder and this attention
    on paper is saying okay you can actually delete everything like what's making
    this work well very well is just the attention by itselfã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä»–ä»¬å›é¡¾è¿™ç¯‡è®ºæ–‡æ—¶ï¼Œä¹Ÿè®¸â€œæ³¨æ„åŠ›å°±æ˜¯ä½ æ‰€éœ€è¦çš„â€ä¼šè¢«ç§°ä¸ºâ€œé©¾é©­â€æˆ–è€…â€œä»…ä»…ä¿æŒâ€ã€‚ä½†æˆ‘ä»¬è¦æ„Ÿè°¢**çº¦ä¹¦äºšÂ·æœ¬å°¼å¥¥**ä¸ºè¿™ä¸ªåå­—çš„æ”¹è¿›ã€‚æ˜¾ç„¶ï¼Œè¿™å°±æ˜¯è¿™ä¸ªä¸»é¢˜çš„å†å²ï¼ŒæŒºæœ‰è¶£çš„ã€‚å¥½çš„ã€‚è¿™è®©æˆ‘ä»¬å›åˆ°äº†2017å¹´ï¼Œâ€œæ³¨æ„åŠ›å°±æ˜¯å”¯ä¸€â€çš„è¿™ä¸ªç»„ä»¶åœ¨è¿ªæ¢…ç‰¹é‡Œä¹Œæ–¯çš„è®ºæ–‡ä¸­åªæ˜¯ä¸€ä¸ªå°éƒ¨åˆ†ï¼Œè¿˜æœ‰æ‰€æœ‰è¿™äº›åŒå‘RNNå’Œè§£ç å™¨ï¼Œè€Œè¿™ç¯‡è®ºæ–‡ä¸Šçš„æ³¨æ„åŠ›æ˜¯è¯´ï¼Œå®é™…ä¸Šä½ å¯ä»¥åˆ é™¤æ‰€æœ‰ä¸œè¥¿ï¼ŒçœŸæ­£è®©è¿™ä¸ªå·¥ä½œé¡ºåˆ©çš„å°±æ˜¯æ³¨æ„åŠ›æœ¬èº«ã€‚
- en: And so delete everythingï¼Œ keep attention and then what's remarkable about this
    paper actually is usually you see papers that aren't very incommer they add like
    one thing and they show that it's betterã€‚but I feel like attention is all unique
    was like a mix of multiple things at the same time they were combined in a very
    unique wayã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åˆ é™¤æ‰€æœ‰ä¸œè¥¿ï¼Œä¿ç•™æ³¨æ„åŠ›ï¼Œè€Œè¿™ç¯‡è®ºæ–‡çš„æ˜¾è‘—ä¹‹å¤„åœ¨äºï¼Œé€šå¸¸ä½ ä¼šçœ‹åˆ°ä¸€äº›è®ºæ–‡ä¸å¤ªå®Œæ•´ï¼Œä»–ä»¬æ·»åŠ ä¸€ä»¶äº‹å¹¶å±•ç¤ºå‡ºæ›´å¥½ã€‚ä½†æˆ‘è§‰å¾—â€œæ³¨æ„åŠ›å°±æ˜¯å”¯ä¸€â€åƒæ˜¯åŒæ—¶ç»“åˆäº†å¤šç§ä¸œè¥¿ï¼Œä»¥ä¸€ç§éå¸¸ç‹¬ç‰¹çš„æ–¹å¼ç»„åˆåœ¨ä¸€èµ·ã€‚
- en: And then also achieve a very good local minimum in the architecture spaceã€‚And
    so to meã€‚this is really a landmark paper thatã€‚And it's quite remarkable and I
    think have quite a lot of work behind the scenesã€‚So delete all the RNN just keep
    attention because attention is operates over sets and I'm going to go into this
    in a secondã€‚you now need to positionally encode your inputs because attention
    doesn't have the notion of space by itselfã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååœ¨æ¶æ„ç©ºé—´ä¸­ä¹Ÿå®ç°äº†ä¸€ä¸ªéå¸¸å¥½çš„å±€éƒ¨æœ€å°å€¼ã€‚å› æ­¤å¯¹æˆ‘æ¥è¯´ï¼Œè¿™çœŸçš„æ˜¯ä¸€ç¯‡å…·æœ‰é‡Œç¨‹ç¢‘æ„ä¹‰çš„è®ºæ–‡ã€‚è¿™ç›¸å½“æ˜¾è‘—ï¼Œæˆ‘è®¤ä¸ºåœ¨èƒŒåæœ‰å¾ˆå¤šå·¥ä½œã€‚å› æ­¤åˆ é™¤æ‰€æœ‰RNNï¼Œåªä¿ç•™æ³¨æ„åŠ›ï¼Œå› ä¸ºæ³¨æ„åŠ›æ˜¯ä½œç”¨äºé›†åˆçš„ï¼Œæˆ‘å°†åœ¨æ¥ä¸‹æ¥è¯¦ç»†è®²è§£ã€‚ä½ ç°åœ¨éœ€è¦å¯¹è¾“å…¥è¿›è¡Œä½ç½®ç¼–ç ï¼Œå› ä¸ºæ³¨æ„åŠ›æœ¬èº«å¹¶æ²¡æœ‰ç©ºé—´çš„æ¦‚å¿µã€‚
- en: Theyã¦ã€‚You have to be very carefulã€‚They adopted this residual network structure
    from resonanceã€‚they interspersed attention with multilayer perceptronsã€‚they used
    layer norms which came from a different paper they introduced a concept of multiple
    heads of attention that were applied to parallel and they gave us I think like
    a fairly good set of hyperparmeters that to this day are used so the expansion
    factor in the multilayer percept on goes up by forx and we'll go into like a bit
    more detail and this forex has stuck around and I believe there's a number of
    papers that tried to play with all kinds of little details of the transformer
    and nothing like sticks because this is actually quite good the only thing to
    my knowledge that stuck that didn't stick was this reshuffling of the layer norms
    to go into the prenorm version where here you see the layer norms are after the
    multi-headed attention or before they just put them before instead so just reshuffling
    of layer norms but otherwise the GPSTs and everything else that you're seeing
    today is basically the 2017 architecture from five years ago and even though everyone
    is working on it it's proven remarkably resilient which I think is relevantã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬ã€‚ä½ å¿…é¡»éå¸¸å°å¿ƒã€‚ä»–ä»¬ä»å…±æŒ¯ä¸­é‡‡ç”¨äº†è¿™ç§æ®‹å·®ç½‘ç»œç»“æ„ã€‚ä»–ä»¬å°†æ³¨æ„åŠ›ä¸å¤šå±‚æ„ŸçŸ¥å™¨äº¤é”™ä½¿ç”¨ã€‚ä»–ä»¬ä½¿ç”¨äº†æ¥è‡ªä¸åŒè®ºæ–‡çš„å±‚å½’ä¸€åŒ–ï¼Œæå‡ºäº†åº”ç”¨äºå¹¶è¡Œçš„å¤šå¤´æ³¨æ„åŠ›æ¦‚å¿µï¼Œä»–ä»¬ç»™äº†æˆ‘ä»¬ä¸€ç»„æˆ‘è®¤ä¸ºç›¸å½“å¥½çš„è¶…å‚æ•°ï¼Œç›´åˆ°ä»Šå¤©ä»åœ¨ä½¿ç”¨ï¼Œå› æ­¤å¤šå±‚æ„ŸçŸ¥å™¨ä¸­çš„æ‰©å±•å› å­ä¸Šå‡äº†å››å€ï¼Œæˆ‘ä»¬å°†ç¨å¾®æ·±å…¥ä¸€ç‚¹ï¼Œè¿™ä¸ªå››å€çš„å› å­ä¸€ç›´å­˜åœ¨ï¼Œæˆ‘ç›¸ä¿¡æœ‰è®¸å¤šè®ºæ–‡å°è¯•å¯¹å˜æ¢å™¨çš„å„ç§ç»†èŠ‚è¿›è¡Œè°ƒæ•´ï¼Œä½†æ²¡æœ‰ä»€ä¹ˆåƒå®ƒä¸€æ ·æœ‰æ•ˆï¼Œå› ä¸ºè¿™ä¸ªè®¾è®¡å®é™…ä¸Šéå¸¸å¥½ã€‚æ ¹æ®æˆ‘æ‰€çŸ¥ï¼Œå”¯ä¸€æ²¡æœ‰æŒä¹…çš„å˜åŒ–æ˜¯å°†å±‚å½’ä¸€åŒ–é‡ç»„ä¸ºé¢„å½’ä¸€åŒ–ç‰ˆæœ¬ï¼Œåœ¨è¿™é‡Œä½ å¯ä»¥çœ‹åˆ°å±‚å½’ä¸€åŒ–åœ¨å¤šå¤´æ³¨æ„åŠ›ä¹‹åæˆ–ä¹‹å‰ï¼Œä»–ä»¬åªæ˜¯æŠŠå®ƒä»¬æ”¾åœ¨å‰é¢ï¼Œæ‰€ä»¥åªæ˜¯å±‚å½’ä¸€åŒ–çš„é‡æ–°æ’åˆ—ï¼Œä½†å…¶ä»–çš„GPSTåŠä½ ä»Šå¤©çœ‹åˆ°çš„æ‰€æœ‰å†…å®¹åŸºæœ¬ä¸Šæ˜¯äº”å¹´å‰çš„2017å¹´æ¶æ„ï¼Œå°½ç®¡æ¯ä¸ªäººéƒ½åœ¨å¯¹æ­¤è¿›è¡Œç ”ç©¶ï¼Œä½†å®ƒè¯æ˜äº†ç›¸å½“å¼ºçš„éŸ§æ€§ï¼Œæˆ‘è®¤ä¸ºè¿™å¾ˆé‡è¦ã€‚
- en: There are innovations that I think have been adopted also in position encoingsã€‚it's
    more common to use different rotary and relative position encoings and so onã€‚so
    I think there have been changesï¼Œ but for the most part it's proven very resoluteã€‚So
    really quite an interesting paper now I wanted to go into the attention mechanismã€‚And
    I thinkã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºæœ‰ä¸€äº›åˆ›æ–°ä¹Ÿè¢«é‡‡ç”¨äºä½ç½®ç¼–ç ã€‚ä½¿ç”¨ä¸åŒçš„æ—‹è½¬å’Œç›¸å¯¹ä½ç½®ç¼–ç å˜å¾—æ›´åŠ å¸¸è§ç­‰ç­‰ã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºæœ‰ä¸€äº›å˜åŒ–ï¼Œä½†å¤§å¤šæ•°æƒ…å†µä¸‹å®ƒè¯æ˜äº†éå¸¸ç¨³å®šã€‚æ‰€ä»¥è¿™ç¯‡è®ºæ–‡ç¡®å®ç›¸å½“æœ‰è¶£ï¼Œç°åœ¨æˆ‘æƒ³æ·±å…¥äº†è§£æ³¨æ„åŠ›æœºåˆ¶ã€‚æˆ‘è®¤ä¸ºã€‚
- en: I sort of like the way I interpreted is notã€‚Is not similar to the ways that
    I've seen it presented before so let me try a different way of like how I see
    it basically to me attention is kind of like the communication phase of the transformer
    and the transformer interleaves two phasesã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰ç‚¹å–œæ¬¢æˆ‘è§£é‡Šçš„æ–¹å¼ï¼Œä¸æ˜¯ã€‚ä¸æˆ‘ä»¥å‰è§è¿‡çš„å±•ç¤ºæ–¹å¼ä¸ç›¸ä¼¼ï¼Œæ‰€ä»¥è®©æˆ‘è¯•è¯•å¦ä¸€ç§æ–¹å¼æ¥æè¿°æˆ‘å¦‚ä½•çœ‹å¾…å®ƒã€‚å¯¹æˆ‘è€Œè¨€ï¼Œæ³¨æ„åŠ›å°±åƒæ˜¯å˜æ¢å™¨çš„æ²Ÿé€šé˜¶æ®µï¼Œè€Œå˜æ¢å™¨äº¤é”™ç€ä¸¤ä¸ªé˜¶æ®µã€‚
- en: the communication phaseï¼Œ which is the multiheaded attention and the computation
    stageã€‚which is this multio perceptron or people formã€‚So in the communication phaseã€‚it's
    really just a data dependent message passing on directed graphsã€‚And you can think
    of it as okay forget everything with a machine translation and everything let's
    just we have directed graphs at each a node you are storing a vector and then
    let me talk now about the communication phase of how these vectors talk to each
    other in this directed graph and then the compute phase later is just a multi
    of receptron which now which then basically acts on every node individually but
    how do these node talk to each other in this directed graph so I wrote like some
    simple pythonã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ²Ÿé€šé˜¶æ®µæ˜¯å¤šå¤´æ³¨æ„åŠ›ï¼Œè€Œè®¡ç®—é˜¶æ®µæ˜¯è¿™ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨æˆ–äººä»¬æ‰€å½¢æˆçš„ã€‚å› æ­¤ï¼Œåœ¨æ²Ÿé€šé˜¶æ®µï¼Œè¿™å®é™…ä¸Šåªæ˜¯åŸºäºæ•°æ®çš„æ¶ˆæ¯ä¼ é€’åœ¨æœ‰å‘å›¾ä¸Šã€‚ä½ å¯ä»¥æƒ³è±¡ï¼Œå¿˜æ‰æœºå™¨ç¿»è¯‘çš„ä¸€åˆ‡ï¼Œè®©æˆ‘ä»¬åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šå­˜å‚¨ä¸€ä¸ªå‘é‡ï¼Œç„¶åè®©æˆ‘è°ˆè°ˆè¿™äº›å‘é‡åœ¨è¿™ä¸ªæœ‰å‘å›¾ä¸­å¦‚ä½•äº’ç›¸æ²Ÿé€šï¼Œè®¡ç®—é˜¶æ®µåˆ™åªæ˜¯ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå®ƒåŸºæœ¬ä¸Šå¯¹æ¯ä¸ªèŠ‚ç‚¹è¿›è¡Œå•ç‹¬å¤„ç†ï¼Œä½†è¿™äº›èŠ‚ç‚¹å¦‚ä½•åœ¨è¿™ä¸ªæœ‰å‘å›¾ä¸­ç›¸äº’æ²Ÿé€šï¼Œæ‰€ä»¥æˆ‘å†™äº†ä¸€äº›ç®€å•çš„Pythonä»£ç ã€‚
- en: Like I wrote this in Python basically to create one round of communication of
    using attention as the message passing schemeã€‚So hereã€‚A node has this private
    data vector as you can think of it as private information to this nodeã€‚and then
    it can also emit a key a query and a value and simply that's done by linear transformation
    from this nodeã€‚So the key is what are the things that I amã€‚Sorryã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨Pythonä¸­å†™äº†è¿™ä¸ªï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸ºäº†åˆ›å»ºä½¿ç”¨æ³¨æ„åŠ›ä½œä¸ºæ¶ˆæ¯ä¼ é€’æœºåˆ¶çš„ä¸€ä¸ªæ²Ÿé€šè½®æ¬¡ã€‚æ‰€ä»¥è¿™é‡Œï¼Œä¸€ä¸ªèŠ‚ç‚¹æœ‰è¿™ä¸ªç§æœ‰æ•°æ®å‘é‡ï¼Œä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯è¿™ä¸ªèŠ‚ç‚¹çš„ç§æœ‰ä¿¡æ¯ã€‚ç„¶åå®ƒè¿˜å¯ä»¥å‘å‡ºä¸€ä¸ªé”®ã€ä¸€ä¸ªæŸ¥è¯¢å’Œä¸€ä¸ªå€¼ï¼Œè¿™åªæ˜¯é€šè¿‡ä»è¿™ä¸ªèŠ‚ç‚¹è¿›è¡Œçº¿æ€§å˜æ¢æ¥å®Œæˆçš„ã€‚é”®æ˜¯æˆ‘æ˜¯ä»€ä¹ˆä¸œè¥¿ï¼ŒæŠ±æ­‰ã€‚
- en: the query is one of the things that I'm looking forã€‚the key is where are the
    things that I have and the value is one are the things that I will communicateã€‚ğŸ˜¡ï¼ŒAnd
    so then when you have your graph that's made up of nodes in some random edgesã€‚when
    you actually have these nodes communicatingï¼Œ what's happening is you loop over
    all the nodes individually in some random order and you are at some node and you
    get the query vector Q which is I'm a node in some graph and this is what I'm
    looking for and so that's just achieved via this linear transformation hereã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æŸ¥è¯¢æ˜¯æˆ‘æ­£åœ¨å¯»æ‰¾çš„å†…å®¹ä¹‹ä¸€ï¼Œé”®æ˜¯æˆ‘æ‰€æ‹¥æœ‰çš„å†…å®¹ï¼Œè€Œå€¼æ˜¯æˆ‘å°†è¦ä¼ è¾¾çš„å†…å®¹ã€‚å› æ­¤ï¼Œå½“ä½ æ‹¥æœ‰ç”±ä¸€äº›éšæœºè¾¹æ„æˆçš„èŠ‚ç‚¹å›¾æ—¶ï¼Œå½“è¿™äº›èŠ‚ç‚¹å®é™…è¿›è¡Œé€šä¿¡æ—¶ï¼Œå‘ç”Ÿçš„äº‹æƒ…æ˜¯ä½ ä»¥éšæœºé¡ºåºå¾ªç¯éå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œä½ åœ¨æŸä¸ªèŠ‚ç‚¹ä¸Šï¼Œè·å¾—æŸ¥è¯¢å‘é‡Qï¼Œè¿™ä»£è¡¨æˆ‘æ˜¯å›¾ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹ï¼Œè¿™æ˜¯æˆ‘æ­£åœ¨å¯»æ‰¾çš„å†…å®¹ï¼Œè¿™åªæ˜¯é€šè¿‡è¿™é‡Œçš„çº¿æ€§å˜æ¢å®ç°çš„ã€‚
- en: And then we look at all the inputs that point to this nodeã€‚and then they broadcast
    where are the things that I haveï¼Œ which is their keysã€‚ğŸ˜¡ã€‚So they broadcast the
    keysï¼Œ I have the query then those interact by dot product to get scoresã€‚so basically
    simply by doing dot product you get some kind of an unormalized weighting of the
    interestingness of all of the information in the notes that point to me and to
    the things I'm looking for and then when you normalize that with softbacks so
    it just sums to oneã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æŸ¥çœ‹æŒ‡å‘è¿™ä¸ªèŠ‚ç‚¹çš„æ‰€æœ‰è¾“å…¥ï¼Œå¹¿æ’­æˆ‘æ‰€æ‹¥æœ‰çš„å†…å®¹ï¼Œä¹Ÿå°±æ˜¯å®ƒä»¬çš„é”®ã€‚å› æ­¤ï¼Œå®ƒä»¬å¹¿æ’­é”®ï¼Œæˆ‘æœ‰æŸ¥è¯¢ï¼Œç„¶åè¿™äº›é€šè¿‡ç‚¹ç§¯ç›¸äº’ä½œç”¨ä»¥è·å¾—åˆ†æ•°ã€‚é€šè¿‡ç‚¹ç§¯ï¼Œä½ åŸºæœ¬ä¸Šè·å¾—äº†æŒ‡å‘æˆ‘çš„æ‰€æœ‰ä¿¡æ¯çš„æœ‰è¶£æ€§æœªå½’ä¸€åŒ–æƒé‡ï¼Œä»¥åŠæˆ‘æ­£åœ¨å¯»æ‰¾çš„å†…å®¹ï¼Œç„¶åå½“ä½ ç”¨softmaxè¿›è¡Œå½’ä¸€åŒ–æ—¶ï¼Œå®ƒåªä¼šæ€»å’Œä¸º1ã€‚
- en: you basically just end up using those scores which now sum to one and our probability
    to distribution and you do a weighted sum of the values to get your updateã€‚Soã€‚I
    have a queryï¼Œ they have keys dot products to get interestingness or like affinityã€‚softmax
    to normalize it and then weight at some of those valuesï¼Œ flow to me and update
    meã€‚And this is happening for each note individually and then we update at the
    end and so this kind of a message passing scheme is kind of like at the heart
    of the transformer and happens in a more vectorized batched way that is more confusing
    and is also inter interspersed with layer norms and things like that to make the
    training behave betterã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼Œä½ æœ€ç»ˆä¼šä½¿ç”¨é‚£äº›ç°åœ¨æ€»å’Œä¸º1çš„åˆ†æ•°ï¼Œå®ƒä»¬æ˜¯æ¦‚ç‡åˆ†å¸ƒï¼Œä½ å¯¹å€¼è¿›è¡ŒåŠ æƒæ±‚å’Œä»¥è·å¾—æ›´æ–°ã€‚å› æ­¤ï¼Œæˆ‘æœ‰ä¸€ä¸ªæŸ¥è¯¢ï¼Œä»–ä»¬æœ‰é”®çš„ç‚¹ç§¯æ¥è·å–æœ‰è¶£æ€§æˆ–äº²å’ŒåŠ›ï¼Œä½¿ç”¨softmaxè¿›è¡Œå½’ä¸€åŒ–ï¼Œç„¶åå¯¹è¿™äº›å€¼è¿›è¡ŒåŠ æƒï¼Œæµå‘æˆ‘å¹¶æ›´æ–°æˆ‘ã€‚è¿™å¯¹äºæ¯ä¸ªèŠ‚ç‚¹éƒ½æ˜¯å•ç‹¬å‘ç”Ÿçš„ï¼Œç„¶åæˆ‘ä»¬åœ¨æœ€åè¿›è¡Œæ›´æ–°ï¼Œå› æ­¤è¿™ç§ä¿¡æ¯ä¼ é€’æœºåˆ¶åœ¨å˜æ¢å™¨çš„æ ¸å¿ƒï¼Œå‘ç”Ÿåœ¨æ›´å‘é‡åŒ–çš„æ‰¹å¤„ç†æ–¹å¼ä¸­ï¼Œè¿™æ›´å¤æ‚ï¼Œå¹¶ä¸”è¿˜ä¸å±‚å½’ä¸€åŒ–ç­‰äº¤é”™åœ¨ä¸€èµ·ï¼Œä»¥ä½¿è®­ç»ƒæ•ˆæœæ›´å¥½ã€‚
- en: but that's roughly what's happening in the attention mechanism I think on the
    high levelã€‚Uã€‚So yeahã€‚so in the communication page of the transformerï¼Œ then this
    message passing scheme happens in every head in parallel and then in every layer
    in seriesã€‚And with different weights each timeã€‚And that's it as far as the multiheaded
    tension goes and so if you look at these encoder decoder modelsã€‚you can sort of
    think of it then in terms of the connectivity of these nodes in the graphã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å¤§è‡´ä¸Šå°±æ˜¯æˆ‘è®¤ä¸ºçš„æ³¨æ„åŠ›æœºåˆ¶åœ¨é«˜å±‚æ¬¡ä¸Šçš„è¿ä½œæ–¹å¼ã€‚æ‰€ä»¥ï¼Œåœ¨å˜æ¢å™¨çš„é€šä¿¡é¡µé¢ä¸­ï¼Œè¿™ç§ä¿¡æ¯ä¼ é€’æœºåˆ¶åœ¨æ¯ä¸ªå¤´ä¸­å¹¶è¡Œå‘ç”Ÿï¼Œç„¶ååœ¨æ¯ä¸€å±‚ä¸­ä¸²è¡Œè¿›è¡Œï¼Œæ¯æ¬¡éƒ½æœ‰ä¸åŒçš„æƒé‡ã€‚è¿™å°±æ˜¯å¤šå¤´æ³¨æ„åŠ›çš„å…¨éƒ¨å†…å®¹ï¼Œå› æ­¤å¦‚æœä½ æŸ¥çœ‹è¿™äº›ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹ï¼Œå¯ä»¥å°†å…¶è§†ä¸ºå›¾ä¸­è¿™äº›èŠ‚ç‚¹çš„è¿é€šæ€§ã€‚
- en: you can kind of think of it as like okay all these tokens that are in the encoder
    that we want to condition on they are fully connected to each other so when they
    communicate they communicate fully when you calculate their features but in the
    decoder because we are trying to have a language model we don't want to have communication
    from future tokens because they give away the answer at this step so the tokens
    in the decoder are fully connected from all the encoder states and then they are
    also fully connected from everything that is beforeã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥å°†å…¶è§†ä¸ºï¼Œç¼–ç å™¨ä¸­çš„æ‰€æœ‰ä»¤ç‰Œéƒ½æ˜¯å½¼æ­¤å®Œå…¨è¿æ¥çš„ï¼Œå› æ­¤å½“å®ƒä»¬è®¡ç®—ç‰¹å¾æ—¶ï¼Œè¿›è¡Œå®Œå…¨é€šä¿¡ï¼Œä½†åœ¨è§£ç å™¨ä¸­ï¼Œç”±äºæˆ‘ä»¬è¯•å›¾æ„å»ºä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œæˆ‘ä»¬ä¸å¸Œæœ›æœ‰æ¥è‡ªæœªæ¥ä»¤ç‰Œçš„é€šä¿¡ï¼Œå› ä¸ºå®ƒä»¬åœ¨æ­¤æ­¥éª¤ä¸­æ³„éœ²äº†ç­”æ¡ˆï¼Œå› æ­¤è§£ç å™¨ä¸­çš„ä»¤ç‰Œä¸æ‰€æœ‰ç¼–ç å™¨çŠ¶æ€å®Œå…¨è¿æ¥ï¼ŒåŒæ—¶å®ƒä»¬ä¹Ÿä¸æ‰€æœ‰åœ¨å…¶ä¹‹å‰çš„å†…å®¹å®Œå…¨è¿æ¥ã€‚
- en: And so you end up with this like triangular structure of in the directed graph
    but that's the message passing scheme that this basically implements and then
    you have to be also a little bit careful because in the cross attention here with
    the decoder you consume the features from the top of the encodeder so think of
    it as in the encoder all the nodes are looking at each other all the tokens are
    looking at each other manyã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ æœ€ç»ˆä¼šåœ¨æœ‰å‘å›¾ä¸­å¾—åˆ°è¿™æ ·ä¸€ä¸ªä¸‰è§’ç»“æ„ï¼Œä½†è¿™åŸºæœ¬ä¸Šå°±æ˜¯æ¶ˆæ¯ä¼ é€’æœºåˆ¶çš„å®ç°ã€‚ç„¶åä½ è¿˜éœ€è¦ç¨å¾®å°å¿ƒï¼Œå› ä¸ºåœ¨è¿™é‡Œä¸è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ä¸­ï¼Œä½ ä¼šä½¿ç”¨æ¥è‡ªç¼–ç å™¨é¡¶éƒ¨çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œå¯ä»¥æƒ³è±¡åœ¨ç¼–ç å™¨ä¸­ï¼Œæ‰€æœ‰èŠ‚ç‚¹å½¼æ­¤ä¹‹é—´éƒ½åœ¨ç›¸äº’å…³æ³¨ï¼Œæ‰€æœ‰çš„ä»¤ç‰Œå½¼æ­¤ä¹‹é—´ä¹Ÿåœ¨å…³æ³¨è®¸å¤šã€‚
- en: many times and they really figure out what's in there and then the decoder when
    it' it's looking only at the top nodesã€‚So that's roughly the message passing scheme
    I was going to go into more of an implementation of a transformerã€‚I don't know
    if there's any questions about thisã€‚å“¦ï¼Œä½ é—®ã€‚Self attention and move by attentionã€‚But
    what is the theã€‚ä¸æ˜¯ã€‚å—¯ã€‚Yeahï¼Œ soã€‚Self attention and multi headeded detention so the
    multi headeded detention is just this attention schemeã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ¬¡ï¼Œä»–ä»¬çœŸæ­£ææ¸…æ¥šé‡Œé¢æœ‰ä»€ä¹ˆï¼Œç„¶åè§£ç å™¨åœ¨æŸ¥çœ‹é¡¶éƒ¨èŠ‚ç‚¹æ—¶ã€‚å› æ­¤ï¼Œè¿™å¤§è‡´å°±æ˜¯æˆ‘æƒ³è¦æ·±å…¥è®²è§£çš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶ã€‚å…³äºè¿™ä¸€ç‚¹æˆ‘ä¸çŸ¥é“æœ‰æ²¡æœ‰é—®é¢˜ã€‚å“¦ï¼Œä½ é—®ã€‚è‡ªæ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›ã€‚ä½†æ˜¯ä»€ä¹ˆæ˜¯é‚£ä¸ªã€‚ä¸æ˜¯ã€‚å—¯ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥ã€‚è‡ªæ³¨æ„åŠ›å’Œå¤šå¤´æ³¨æ„åŠ›ï¼Œæ‰€ä»¥å¤šå¤´æ³¨æ„åŠ›å°±æ˜¯è¿™ç§æ³¨æ„åŠ›æœºåˆ¶ã€‚
- en: but it's just applied multiple times in parallel multiple has just means independent
    applications of the same attention soã€‚This message passing scheme basically just
    happens in parallel multiple times with different weights for the query key and
    valueã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™åªæ˜¯ä»¥å¹¶è¡Œçš„æ–¹å¼å¤šæ¬¡åº”ç”¨ï¼Œ"å¤šæ¬¡"ä»…ä»…æ„å‘³ç€å¯¹åŒä¸€æ³¨æ„åŠ›çš„ç‹¬ç«‹åº”ç”¨ã€‚å› æ­¤ï¼Œè¿™ä¸ªæ¶ˆæ¯ä¼ é€’æœºåˆ¶åŸºæœ¬ä¸Šå°±æ˜¯åœ¨å¹¶è¡Œä¸­å¤šæ¬¡å‘ç”Ÿï¼Œå¹¶ä¸”å¯¹äºæŸ¥è¯¢ã€é”®å’Œå€¼ä½¿ç”¨ä¸åŒçš„æƒé‡ã€‚
- en: so you can always look at it like in parallel I'm looking for I'm seeking different
    kinds of information from different nodes and I'm collecting it all in the same
    nodeã€‚It's all done in parallelã€‚So heads is really just like copy paste in parallel
    and layers are copy pasteã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ æ€»æ˜¯å¯ä»¥è¿™æ ·çœ‹ï¼šæˆ‘åœ¨å¹¶è¡Œä¸­å¯»æ‰¾ä¸åŒç±»å‹çš„ä¿¡æ¯æ¥è‡ªä¸åŒèŠ‚ç‚¹ï¼Œå¹¶ä¸”æˆ‘æŠŠæ‰€æœ‰ä¿¡æ¯éƒ½æ±‡é›†åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸­ã€‚è¿™ä¸€åˆ‡éƒ½æ˜¯åœ¨å¹¶è¡Œä¸­å®Œæˆçš„ã€‚æ‰€ä»¥å¤´éƒ¨å®é™…ä¸Šå°±åƒæ˜¯åœ¨å¹¶è¡Œä¸­çš„å¤åˆ¶ç²˜è´´ï¼Œè€Œå±‚å°±æ˜¯å¤åˆ¶ç²˜è´´ã€‚
- en: but in seriesã€‚Maybe that makes senseã€‚And self attentionï¼Œ when it's self attentionã€‚what
    it's referring to is that the node here produces each node hereï¼Œ so as I described
    it hereã€‚this is really self attention because every one of these nodes produces
    a key query value from this individual nodeã€‚When you have cross attentionã€‚You
    have one cross attention here coming from the encoderã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®ƒæ˜¯ä¸²è¡Œçš„ã€‚ä¹Ÿè®¸è¿™æ ·è¯´æœ‰æ„ä¹‰ã€‚è€Œè‡ªæ³¨æ„åŠ›ï¼Œå½“æåˆ°è‡ªæ³¨æ„åŠ›æ—¶ï¼ŒæŒ‡çš„æ˜¯è¿™é‡Œçš„æ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆè¿™é‡Œçš„æ¯ä¸ªèŠ‚ç‚¹ã€‚å› æ­¤ï¼Œæ­£å¦‚æˆ‘åœ¨è¿™é‡Œæè¿°çš„ï¼Œè¿™ç¡®å®æ˜¯è‡ªæ³¨æ„åŠ›ï¼Œå› ä¸ºè¿™äº›èŠ‚ç‚¹ä¸­çš„æ¯ä¸€ä¸ªéƒ½ä»è¿™ä¸ªç‹¬ç«‹èŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªé”®ã€æŸ¥è¯¢å’Œå€¼ã€‚å½“ä½ æœ‰äº¤å‰æ³¨æ„åŠ›æ—¶ï¼Œæ¥è‡ªç¼–ç å™¨çš„ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›åœ¨è¿™é‡Œã€‚
- en: that just means that the queries are still produced from this nodeã€‚but the keys
    and the values are produced as a function of nodes that are coming from the encoderã€‚So
    I have my queries because I'm trying to decode some the fifth word in the sequence
    and I'm looking for certain things because I'm the fifth word and then the keys
    and the values in terms of the source of information that could answer my queries
    can come from the previous nodes in the current decoding sequence or from the
    top of the encoder so all the nodes that have already seen all of the encoding
    tokens manyã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»…æ„å‘³ç€æŸ¥è¯¢ä»ç„¶æ˜¯ä»è¿™ä¸ªèŠ‚ç‚¹ç”Ÿæˆçš„ï¼Œä½†é”®å’Œå€¼æ˜¯ä½œä¸ºæ¥è‡ªç¼–ç å™¨çš„èŠ‚ç‚¹å‡½æ•°ç”Ÿæˆçš„ã€‚å› æ­¤ï¼Œæˆ‘æœ‰æˆ‘çš„æŸ¥è¯¢ï¼Œå› ä¸ºæˆ‘è¯•å›¾è§£ç åºåˆ—ä¸­çš„ç¬¬äº”ä¸ªè¯ï¼Œè€Œæˆ‘åœ¨å¯»æ‰¾æŸäº›ä¿¡æ¯ï¼Œå› ä¸ºæˆ‘æ˜¯ç¬¬äº”ä¸ªè¯ï¼Œç„¶åå…³äºå¯ä»¥å›ç­”æˆ‘æŸ¥è¯¢çš„ä¿¡æ¯æºçš„é”®å’Œå€¼å¯ä»¥æ¥è‡ªå½“å‰è§£ç åºåˆ—ä¸­çš„å‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œæˆ–è€…æ¥è‡ªç¼–ç å™¨é¡¶éƒ¨ï¼Œå› æ­¤æ‰€æœ‰å·²è§è¿‡æ‰€æœ‰ç¼–ç ä»¤ç‰Œçš„èŠ‚ç‚¹è®¸å¤šã€‚
- en: many times can now broadcast what they contain in terms of the informationã€‚Soã€‚I
    guess to summarize the self attention is kind of likeã€‚So cross attention and self
    attention only differ in where the piece and the values come fromã€‚either the piece
    and values are produced from this node or they are produced from some external
    source like like an encoder and the node over thereã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å¤šæ¬¡ç°åœ¨å¯ä»¥å¹¿æ’­å®ƒä»¬åœ¨ä¿¡æ¯æ–¹é¢æ‰€åŒ…å«çš„å†…å®¹ã€‚æ‰€ä»¥ï¼Œæˆ‘æƒ³æ€»ç»“ä¸€ä¸‹ï¼Œè‡ªæ³¨æ„åŠ›æœ‰ç‚¹åƒã€‚äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›åªåœ¨äºé”®å’Œå€¼çš„æ¥æºä¸åŒã€‚è¦ä¹ˆé”®å’Œå€¼æ˜¯ä»è¿™ä¸ªèŠ‚ç‚¹ç”Ÿæˆçš„ï¼Œè¦ä¹ˆæ˜¯ä»æŸä¸ªå¤–éƒ¨æºç”Ÿæˆçš„ï¼Œæ¯”å¦‚ä¸€ä¸ªç¼–ç å™¨å’Œé‚£è¾¹çš„èŠ‚ç‚¹ã€‚
- en: ğŸ˜¡ï¼ŒBut algorithmically is the same micro operationsã€‚å¥½çš„ã€‚Oã€‚In the message passing
    graph paradigm theã€‚So yeahï¼Œ soã€‚å—¯æˆ‘ã€‚So think of so each one of these nodes is a
    tokenã€‚å—¯ã€‚I guess like they don't have a very good picture of it in the transformerï¼Œ
    but likeã€‚Like this node here could represent the third word in the output in the
    decoderã€‚And in the beginningã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜¡ï¼Œä½†ç®—æ³•ä¸Šæ˜¯ç›¸åŒçš„å¾®æ“ä½œã€‚å¥½çš„ã€‚Oã€‚åœ¨æ¶ˆæ¯ä¼ é€’å›¾èŒƒå¼ä¸­ã€‚æ‰€ä»¥æ˜¯çš„ï¼Œæ‰€ä»¥ã€‚å—¯ï¼Œæˆ‘ã€‚æ‰€ä»¥æƒ³è±¡ä¸€ä¸‹ï¼Œè¿™äº›èŠ‚ç‚¹æ¯ä¸€ä¸ªéƒ½æ˜¯ä¸€ä¸ªæ ‡è®°ã€‚å—¯ã€‚æˆ‘æƒ³å®ƒä»¬åœ¨å˜æ¢å™¨ä¸­çš„å›¾åƒå¹¶ä¸æ˜¯å¾ˆå¥½ï¼Œä½†åƒæ˜¯ã€‚è¿™ä¸ªèŠ‚ç‚¹å¯ä»¥è¡¨ç¤ºè§£ç å™¨è¾“å‡ºä¸­çš„ç¬¬ä¸‰ä¸ªå•è¯ã€‚èµ·åˆã€‚
- en: it is just the embedding of the wordã€‚å—¯ã€‚And thenã€‚Okayã€‚I have to think through
    this knowledge morningï¼Œ I came up with it this morningã€‚æ²¡æœ‰ã€‚Actuallyã€‚I ca yesterdayã€‚æˆ‘ç»“æœã€‚Stationã€‚ã”ã„ã€‚Notes
    have been blockedã€‚We betterã€‚These notes are basically the vectorã€‚I'll go to an
    implementationï¼Œ I'll go to the implementationã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯å•è¯çš„åµŒå…¥ã€‚å—¯ã€‚ç„¶åã€‚å¥½çš„ã€‚æˆ‘å¾—ä»”ç»†æ€è€ƒä¸€ä¸‹è¿™ä¸ªçŸ¥è¯†ï¼Œä»Šå¤©æ—©ä¸Šæˆ‘æƒ³åˆ°äº†å®ƒã€‚æ²¡æœ‰ã€‚å®é™…ä¸Šã€‚æˆ‘æ˜¨å¤©ã€‚æˆ‘çš„ç»“æœã€‚è½¦ç«™ã€‚ã”ã„ã€‚ç¬”è®°è¢«å°é”äº†ã€‚æˆ‘ä»¬æœ€å¥½ã€‚é‚£äº›ç¬”è®°åŸºæœ¬ä¸Šæ˜¯å‘é‡ã€‚æˆ‘ä¼šå»å®ç°ï¼Œæˆ‘ä¼šå»å®ç°ã€‚
- en: and then maybe I'll make the connections to the graphã€‚So let me try to first
    go to let me not go to with this intuition in mind at least to NAGPT which is
    a complete implementation of a transformer that is very minimal thats why I worked
    on this over the last few days and here it is reproducing GT2 on open web textex
    so it's a pretty serious implementation that reproduces GPT2 I would say and provide
    it in a compute this was one note of HPs for 38 hours or something like that correctly
    and it's very readable it's 300 lives so everyone can take a look at it and yeah
    let me basically briefly step through itã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä¹Ÿè®¸æˆ‘ä¼šå»ºç«‹ä¸å›¾çš„è¿æ¥ã€‚æ‰€ä»¥è®©æˆ‘å…ˆå°è¯•ï¼Œä¸å»è€ƒè™‘è¿™ä¸ªç›´è§‰ï¼Œè‡³å°‘å»NAGPTï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€çº¦çš„å˜æ¢å™¨å®Œæ•´å®ç°ï¼Œè¿™å°±æ˜¯æˆ‘åœ¨è¿‡å»å‡ å¤©é‡Œæ‰€åšçš„ï¼Œè¿™é‡Œå®ƒåœ¨å¼€æ”¾ç½‘é¡µæ–‡æœ¬ä¸Šå†ç°GT2ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªç›¸å½“ä¸¥è‚ƒçš„å®ç°ï¼Œæˆ‘ä¼šè¯´å†ç°äº†GPT2ï¼Œå¹¶æä¾›åœ¨ä¸€ä¸ªè®¡ç®—ä¸Šï¼Œè¿™æ˜¯HPçš„ä¸€ä¸ªç¬”è®°ï¼ŒæŒç»­äº†38å°æ—¶å·¦å³ï¼Œè€Œä¸”å®ƒéå¸¸å¯è¯»ï¼Œåªæœ‰300è¡Œï¼Œæ‰€ä»¥æ¯ä¸ªäººéƒ½å¯ä»¥çœ‹ä¸€ä¸‹ï¼Œå¥½çš„ï¼Œè®©æˆ‘åŸºæœ¬ä¸Šç®€å•åœ°è®²è§£ä¸€ä¸‹ã€‚
- en: So let's try to have a decoder only transformer so what that means is that it's
    a language model it tries to model the next word in a sequence or the next character
    sequence so the data that we train on it's always some kind of text so here's
    some fake Shakespeare so this is real Shakespeare we're going to produce fake
    Shakespeare so this is called the tiny Shakespeare dataset set which is one of
    my favorite toy datasets you take all of Shakespeare concateninated and it's one
    megate file and then you can train language models on it and get infinite Shakespeare
    if you like which I think is medical so we have a text the first thing we need
    to do is we need to convert it to a sequence of integersã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è®©æˆ‘ä»¬å°è¯•åªä½¿ç”¨è§£ç å™¨çš„å˜æ¢å™¨ï¼Œè¿™æ„å‘³ç€å®ƒæ˜¯ä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œå®ƒè¯•å›¾å»ºæ¨¡åºåˆ—ä¸­çš„ä¸‹ä¸€ä¸ªå•è¯æˆ–ä¸‹ä¸€ä¸ªå­—ç¬¦åºåˆ—ã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®­ç»ƒçš„æ•°æ®æ€»æ˜¯æŸç§æ–‡æœ¬ï¼Œæ‰€ä»¥è¿™é‡Œæœ‰ä¸€äº›å‡çš„èå£«æ¯”äºšï¼Œè¿™æ˜¯å®é™…çš„èå£«æ¯”äºšï¼Œæˆ‘ä»¬å°†ç”Ÿæˆå‡çš„èå£«æ¯”äºšï¼Œè¿™è¢«ç§°ä¸ºè¿·ä½ èå£«æ¯”äºšæ•°æ®é›†ï¼Œè¿™æ˜¯æˆ‘æœ€å–œæ¬¢çš„ç©å…·æ•°æ®é›†ä¹‹ä¸€ï¼Œä½ å°†æ‰€æœ‰èå£«æ¯”äºšè¿æ¥åœ¨ä¸€èµ·ï¼Œå®ƒæ˜¯ä¸€ä¸ªå…†å­—èŠ‚çš„æ–‡ä»¶ï¼Œç„¶åä½ å¯ä»¥åœ¨ä¸Šé¢è®­ç»ƒè¯­è¨€æ¨¡å‹ï¼Œè·å¾—æ— ç©·çš„èå£«æ¯”äºšï¼Œå¦‚æœä½ å–œæ¬¢ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯åŒ»å­¦çš„ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰ä¸€ä¸ªæ–‡æœ¬ï¼Œç¬¬ä¸€ä»¶éœ€è¦åšçš„äº‹æƒ…æ˜¯å°†å…¶è½¬æ¢ä¸ºæ•´æ•°åºåˆ—ã€‚
- en: Because transformers natively processï¼Œ you knowã€‚You can't pluck text into transform
    you need to some outencodã€‚So the way that encoding is done is we convertï¼Œ for
    exampleï¼Œ in the simplest caseã€‚every character gets an integerrã€‚And then instead
    of high thereã€‚we would have this sequence of integersã€‚So then you can encode every
    single character as an integer and get like a NAA sequence of integer so you just
    incatetcatenate it all into one large long one dimensional sequence and then you
    can train on it Now here we only have a single document in some cases if you have
    multiple independent documents what people like to do is create special tokens
    and they intersperse those documents with those special text tokens that they
    splice in between to create boundariesã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºå˜æ¢å™¨æœ¬è´¨ä¸Šå¤„ç†ï¼Œä½ çŸ¥é“ã€‚ä½ ä¸èƒ½ç›´æ¥å°†æ–‡æœ¬æ”¾å…¥å˜æ¢å™¨ä¸­ï¼Œä½ éœ€è¦è¿›è¡ŒæŸç§ç¼–ç ã€‚å› æ­¤ï¼Œç¼–ç çš„æ–¹å¼æ˜¯æˆ‘ä»¬å°†æ¯ä¸ªå­—ç¬¦è½¬æ¢ï¼Œä¾‹å¦‚ï¼Œåœ¨æœ€ç®€å•çš„æƒ…å†µä¸‹ã€‚æ¯ä¸ªå­—ç¬¦å˜æˆä¸€ä¸ªæ•´æ•°ã€‚ç„¶åä¸æ˜¯é«˜åœ¨é‚£é‡Œã€‚æˆ‘ä»¬ä¼šæœ‰è¿™ä¸ªæ•´æ•°åºåˆ—ã€‚å› æ­¤ï¼Œä½ å¯ä»¥å°†æ¯ä¸ªå­—ç¬¦ç¼–ç ä¸ºä¸€ä¸ªæ•´æ•°ï¼Œå¹¶å¾—åˆ°åƒNAAè¿™æ ·çš„æ•´æ•°åºåˆ—ï¼Œä½ åªéœ€å°†å®ƒä»¬å…¨éƒ¨è¿æ¥æˆä¸€ä¸ªå¤§å‹ä¸€ç»´åºåˆ—ï¼Œç„¶åä½ å°±å¯ä»¥åœ¨ä¸Šé¢è®­ç»ƒã€‚ç°åœ¨è¿™é‡Œæˆ‘ä»¬åªæœ‰ä¸€ä¸ªå•ç‹¬çš„æ–‡æ¡£ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¦‚æœä½ æœ‰å¤šä¸ªç‹¬ç«‹çš„æ–‡æ¡£ï¼Œäººä»¬å–œæ¬¢åˆ›å»ºç‰¹æ®Šçš„æ ‡è®°ï¼Œå¹¶å°†è¿™äº›æ–‡æ¡£ä¸é‚£äº›ç‰¹æ®Šçš„æ–‡æœ¬æ ‡è®°äº¤æ›¿æ’å…¥ï¼Œä»¥åˆ›å»ºè¾¹ç•Œã€‚
- en: But those boundaries actually don't have anyã€‚Any modeling impact it's just that
    the transformer is supposed to learn beyond that propagation that the end of document
    sequence means that you should wipe the memoryã€‚ğŸ˜¡ï¼ŒOkayï¼Œ so then we produce batchesï¼Œ
    so these batches of data just mean that we go back to the one dimensional sequence
    and we take out chunks of this sequenceã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™äº›è¾¹ç•Œå®é™…ä¸Šæ²¡æœ‰ä»»ä½•å»ºæ¨¡å½±å“ï¼Œåªæ˜¯å˜å‹å™¨åº”è¯¥å­¦ä¹ è¶…å‡ºè¿™ç§ä¼ æ’­çš„å†…å®¹ï¼Œæ–‡æ¡£åºåˆ—çš„ç»“æŸæ„å‘³ç€ä½ åº”è¯¥æ¸…ç©ºè®°å¿†ã€‚ğŸ˜¡ï¼Œå¥½çš„ï¼Œç„¶åæˆ‘ä»¬ç”Ÿæˆæ‰¹æ¬¡ï¼Œè¿™äº›æ•°æ®æ‰¹æ¬¡æ„å‘³ç€æˆ‘ä»¬å›åˆ°ä¸€ç»´åºåˆ—ï¼Œå¹¶æå–å‡ºè¿™ä¸€åºåˆ—çš„å—ã€‚
- en: so say if the block size is eightã€‚Then the block size indicates the maximum
    length of context that your transformer will processã€‚so if our block size is eightï¼Œ
    that means that we are going to have up to eight characters of context to predict
    ninth character in a sequenceã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å‡è®¾å—å¤§å°æ˜¯å…«ã€‚é‚£ä¹ˆå—å¤§å°æŒ‡ç¤ºä½ çš„å˜å‹å™¨å°†å¤„ç†çš„æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€‚å¦‚æœæˆ‘ä»¬çš„å—å¤§å°æ˜¯å…«ï¼Œé‚£æ„å‘³ç€æˆ‘ä»¬å°†æœ‰æœ€å¤šå…«ä¸ªå­—ç¬¦çš„ä¸Šä¸‹æ–‡æ¥é¢„æµ‹åºåˆ—ä¸­çš„ç¬¬ä¹ä¸ªå­—ç¬¦ã€‚
- en: And the batch size indicates how many sequences and parallel we're going to
    processã€‚and we want this to be as large as possibleï¼Œ so we're fully taking advantage
    of the GPU and the parallelels under the cordsã€‚So in this exampleï¼Œ we're doing
    four by8 batchesï¼Œ so every row here is independent example sort ofã€‚and then everyã€‚Every
    row here is is a small chunk of the sequence that we're going to train on and
    then we have both the inputs and the targets at every single point hereã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¹é‡å¤§å°æŒ‡ç¤ºæˆ‘ä»¬å°†å¤„ç†å¤šå°‘ä¸ªåºåˆ—å¹¶è¡Œã€‚æˆ‘ä»¬å¸Œæœ›è¿™ä¸ªå°½å¯èƒ½å¤§ï¼Œä»¥ä¾¿å……åˆ†åˆ©ç”¨GPUå’Œç”µç¼†ä¸‹çš„å¹¶è¡Œæ€§ã€‚å› æ­¤åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬æ­£åœ¨è¿›è¡Œ4x8æ‰¹æ¬¡ï¼Œæ‰€ä»¥è¿™é‡Œçš„æ¯ä¸€è¡Œéƒ½æ˜¯ç‹¬ç«‹çš„ä¾‹å­ã€‚ç„¶åæ¯ä¸€è¡Œéƒ½æ˜¯æˆ‘ä»¬å°†è¦è®­ç»ƒçš„å°å—åºåˆ—ï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨æ¯ä¸ªç‚¹ä¸Šéƒ½æœ‰è¾“å…¥å’Œç›®æ ‡ã€‚
- en: so to fully spell out what's contained in a single4 by8 batch to the transformer
    is sort of like compacted here so when the input is 47 by itself the target is
    58 and when the input is the sequence 4758 the target is one and when it's 47581
    the target is 51 and so on so actually the single batch of examples that's 4 by8
    actually has a ton of individual examples that we are expecting the transformer
    to learn on in in parallel and so you'll see that the batches are learned on completely
    independently but the time dimensions sort of here along horizontally is also
    trained on in parallel so sort of your real batch size is more like detedã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¦å®Œå…¨é˜æ˜åŒ…å«åœ¨å•ä¸ª4x8æ‰¹æ¬¡ä¸­çš„å†…å®¹ï¼Œå¯¹å˜å‹å™¨æ¥è¯´å°±åƒæ˜¯å‹ç¼©åœ¨è¿™é‡Œï¼Œæ‰€ä»¥å½“è¾“å…¥æ˜¯47æ—¶ï¼Œç›®æ ‡æ˜¯58ï¼Œè€Œå½“è¾“å…¥æ˜¯åºåˆ—4758æ—¶ï¼Œç›®æ ‡æ˜¯1ï¼Œå½“æ˜¯47581æ—¶ï¼Œç›®æ ‡æ˜¯51ï¼Œä¾æ­¤ç±»æ¨ï¼Œå®é™…ä¸Šè¿™ä¸ª4x8çš„å•ä¸ªæ‰¹æ¬¡å®ä¾‹æœ‰å¾ˆå¤šä¸ªä½“ä¾‹å­ï¼Œæˆ‘ä»¬æœŸæœ›å˜å‹å™¨èƒ½å¤Ÿå¹¶è¡Œå­¦ä¹ ï¼Œæ‰€ä»¥ä½ ä¼šçœ‹åˆ°è¿™äº›æ‰¹æ¬¡æ˜¯å®Œå…¨ç‹¬ç«‹å­¦ä¹ çš„ï¼Œä½†æ—¶é—´ç»´åº¦åœ¨è¿™é‡Œæ°´å¹³æ’åˆ—ï¼Œä¹Ÿæ˜¯åœ¨å¹¶è¡Œè®­ç»ƒï¼Œæ‰€ä»¥ä½ çœŸæ­£çš„æ‰¹é‡å¤§å°æ›´åƒæ˜¯detedã€‚
- en: It's just that the context grows linearly for the predictions that you make
    along the T directionã€‚In the in the modelï¼Œ so this is how the this is all the
    examples of the model will learn from this single backã€‚So now this is the GPT
    classã€‚And because this is a decoder only modelã€‚so we're not going to have an encoder
    because there's no like English we're translating from we're not trying to condition
    on some other external informationã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: åªæ˜¯ä¸Šä¸‹æ–‡åœ¨ä½ æ²¿ç€Tæ–¹å‘è¿›è¡Œçš„é¢„æµ‹ä¸­çº¿æ€§å¢é•¿ã€‚åœ¨æ¨¡å‹ä¸­ï¼Œè¿™å°±æ˜¯æ¨¡å‹å°†ä»è¿™ä¸ªå•ä¸€æ‰¹æ¬¡å­¦ä¹ çš„æ‰€æœ‰ç¤ºä¾‹ã€‚å› æ­¤ç°åœ¨è¿™æ˜¯GPTç±»ã€‚å› ä¸ºè¿™æ˜¯ä¸€ä¸ªä»…è§£ç çš„æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸ä¼šæœ‰ç¼–ç å™¨ï¼Œå› ä¸ºæ²¡æœ‰æˆ‘ä»¬è¦ç¿»è¯‘çš„è‹±æ–‡ï¼Œæˆ‘ä»¬ä¸æ˜¯è¯•å›¾ä»¥å…¶ä»–å¤–éƒ¨ä¿¡æ¯ä¸ºæ¡ä»¶ã€‚
- en: we're just trying to produce a sequence of words that follow each other are
    likely toã€‚So this is all p torch and I'm growing sluy faster because I'm assuming
    people have taken 2 31 and or something along those linesã€‚but here in the forward
    pass we take this these indicesã€‚And then we both encode the identity of the indices
    just via an embedding lookup tableã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åªæ˜¯è¯•å›¾ç”Ÿæˆç›¸äº’å¯èƒ½è·Ÿéšçš„è¯åºåˆ—ã€‚æ‰€ä»¥è¿™éƒ½æ˜¯pytorchï¼Œæˆ‘çš„æˆé•¿é€Ÿåº¦æ›´å¿«ï¼Œå› ä¸ºæˆ‘å‡è®¾äººä»¬å·²ç»æ¥å—äº†2 31æˆ–ç±»ä¼¼çš„ä¸œè¥¿ã€‚ä½†åœ¨å‰å‘ä¼ é€’ä¸­ï¼Œæˆ‘ä»¬è·å–è¿™äº›ç´¢å¼•ã€‚ç„¶åæˆ‘ä»¬é€šè¿‡åµŒå…¥æŸ¥æ‰¾è¡¨å¯¹è¿™äº›ç´¢å¼•çš„èº«ä»½è¿›è¡Œç¼–ç ã€‚
- en: so every single integer has a we index into a lookup table of vectors in this
    n dot embedding and pull out the word vector for that tokenã€‚And then because the
    messageï¼Œ because transform by itself doesn't actuallyã€‚it processes sets nativelyï¼Œ
    so we need to also positionally encode these vectors so that we basically have
    both the information about the token identity and its place in the sequence from
    one to block sizeã€‚Now those the information about what and where is combined additivelyã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æ¯ä¸€ä¸ªæ•´æ•°éƒ½æœ‰ä¸€ä¸ªç´¢å¼•ï¼Œæˆ‘ä»¬ç´¢å¼•åˆ°è¿™ä¸ªn.dotåµŒå…¥çš„æŸ¥æ‰¾è¡¨ä¸­ï¼Œæå–å‡ºè¯¥ä»¤ç‰Œçš„è¯å‘é‡ã€‚ç„¶åå› ä¸ºtransformæœ¬èº«å¹¶ä¸å®é™…å¤„ç†ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å¯¹è¿™äº›å‘é‡è¿›è¡Œä½ç½®ç¼–ç ï¼Œä»¥ä¾¿æˆ‘ä»¬åŸºæœ¬ä¸ŠåŒæ—¶æ‹¥æœ‰ä»¤ç‰Œèº«ä»½å’Œå…¶åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼ˆä»1åˆ°å—å¤§å°ï¼‰ä¿¡æ¯ã€‚ç°åœ¨ï¼Œå…³äºä»€ä¹ˆå’Œä½ç½®çš„ä¿¡æ¯æ˜¯åŠ æ³•ç»“åˆçš„ã€‚
- en: so the token embeddings and the positional embeddings are just added exactly
    as hereã€‚So this X hereã€‚ğŸ˜¡ï¼ŒThen there's optional dropoutï¼Œ this x here basically
    just contains the set of wordsã€‚And their positionsã€‚And that feeds into the blocks
    of transformer that we're going to look into what's blocked hereã€‚but for here
    for nowï¼Œ this is just a series of blocks in the transformerã€‚ğŸ˜¡ï¼ŒAnd then in the
    endã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»¤ç‰ŒåµŒå…¥å’Œä½ç½®åµŒå…¥æ­£å¦‚è¿™é‡Œæ‰€ç¤ºç›´æ¥ç›¸åŠ ã€‚æ‰€ä»¥è¿™é‡Œçš„Xã€‚ğŸ˜¡ ç„¶åæœ‰å¯é€‰çš„dropoutï¼Œè¿™é‡Œçš„xåŸºæœ¬ä¸ŠåªåŒ…å«è¯æ±‡é›†åˆã€‚è¿˜æœ‰å®ƒä»¬çš„ä½ç½®ã€‚è¿™äº›ä¿¡æ¯è¾“å…¥åˆ°æˆ‘ä»¬å°†è¦æŸ¥çœ‹çš„transformerçš„å—ä¸­ï¼Œè¿™é‡Œæš‚æ—¶åªæ˜¯transformerä¸­çš„ä¸€ç³»åˆ—å—ã€‚ğŸ˜¡
    ç„¶ååœ¨æœ€åã€‚
- en: there's a layer normï¼Œ and then you're decoding the logits for the next word
    or next integer in a sequence using a linear projection of the output of this
    transformerã€‚so LM head hereï¼Œ a short for language model head is just a linear
    functionã€‚So basically positionally encode all the wordsï¼Œ feed them into a sequence
    of blocksã€‚and then apply a linear layer to get the probability distribution for
    the next characterã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ªå±‚å½’ä¸€åŒ–ï¼Œç„¶åä½ ä½¿ç”¨è¿™ä¸ªtransformerè¾“å‡ºçš„çº¿æ€§æŠ•å½±æ¥è§£ç ä¸‹ä¸€ä¸ªå•è¯æˆ–ä¸‹ä¸€ä¸ªæ•´æ•°çš„logitsã€‚æ‰€ä»¥è¿™é‡Œçš„LMå¤´ï¼Œè¯­è¨€æ¨¡å‹å¤´çš„ç¼©å†™åªæ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šå¯¹æ‰€æœ‰è¯è¿›è¡Œä½ç½®ç¼–ç ï¼Œå°†å®ƒä»¬è¾“å…¥åˆ°ä¸€ç³»åˆ—å—ä¸­ã€‚ç„¶ååº”ç”¨ä¸€ä¸ªçº¿æ€§å±‚æ¥è·å–ä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡åˆ†å¸ƒã€‚
- en: and then if we have the targets which we produced in the data order and you'll
    notice that the targets are just the inputs offset by one in timeã€‚Then those targets
    feed into cross entrytropy lossï¼Œ so this is just a negative one likelihoodã€‚typical
    classification lossã€‚So now let's drill into what's here in the blocksã€‚So these
    blocks are applied sequentiallyã€‚ğŸ˜¡ï¼ŒThere's againã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¦‚æœæˆ‘ä»¬æœ‰åœ¨æ•°æ®é¡ºåºä¸­ç”Ÿæˆçš„ç›®æ ‡ï¼Œä½ ä¼šæ³¨æ„åˆ°è¿™äº›ç›®æ ‡åªæ˜¯æ—¶é—´ä¸Šåç§»ä¸€ä¸ªçš„è¾“å…¥ã€‚ç„¶åè¿™äº›ç›®æ ‡è¾“å…¥åˆ°äº¤å‰ç†µæŸå¤±ä¸­ï¼Œæ‰€ä»¥è¿™åªæ˜¯ä¸€ä¸ªè´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œå…¸å‹çš„åˆ†ç±»æŸå¤±ã€‚ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥äº†è§£è¿™äº›å—ä¸­çš„å†…å®¹ã€‚è¿™äº›å—æ˜¯é¡ºåºåº”ç”¨çš„ã€‚ğŸ˜¡
    å†æ¬¡ã€‚
- en: as I mentioned communicate phase and the compute phase so in the communicate
    phase both of the nodes get to talk to each other and so these nodes are basicallyã€‚If
    our block size is eightã€‚Then we are going to have eight node in this graphã€‚There's
    eight nodes in this graphï¼Œ the first node is pointed to only by itselfã€‚the second
    node is pointed to by the first node and itselfã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘æåˆ°çš„æ²Ÿé€šé˜¶æ®µå’Œè®¡ç®—é˜¶æ®µï¼Œåœ¨æ²Ÿé€šé˜¶æ®µï¼Œä¸¤ä¸ªèŠ‚ç‚¹äº’ç›¸äº¤æµï¼Œå› æ­¤è¿™äº›èŠ‚ç‚¹åŸºæœ¬ä¸Šæ˜¯ã€‚å¦‚æœæˆ‘ä»¬çš„å—å¤§å°æ˜¯å…«ï¼Œé‚£ä¹ˆåœ¨è¿™ä¸ªå›¾ä¸­æˆ‘ä»¬å°†æœ‰å…«ä¸ªèŠ‚ç‚¹ã€‚è¿™ä¸ªå›¾ä¸­æœ‰å…«ä¸ªèŠ‚ç‚¹ï¼Œç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä»…è¢«è‡ªèº«æŒ‡å‘ã€‚ç¬¬äºŒä¸ªèŠ‚ç‚¹è¢«ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å’Œè‡ªèº«æŒ‡å‘ã€‚
- en: The third node is wanted to buy the first two nodes and itselfï¼Œ et ceteraã€‚so
    there's eight nodes hereã€‚So you applyï¼Œ there's a residual pathway in Xï¼Œ you take
    it outã€‚you apply a layer norm and then the self attention so that these communicate
    these eight nodes communicateã€‚but you have to keep in mind that the batch is four
    so because batch is four this is also applied so we have eight nodes communicating
    but there's a batch of four of them all individually communicating on those eight
    nodeã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ä¸ªèŠ‚ç‚¹è¢«å‰ä¸¤ä¸ªèŠ‚ç‚¹å’Œè‡ªèº«æŒ‡å‘ï¼Œç­‰ç­‰ã€‚æ‰€ä»¥è¿™é‡Œæœ‰å…«ä¸ªèŠ‚ç‚¹ã€‚å› æ­¤ä½ åº”ç”¨ï¼ŒXä¸­æœ‰ä¸€ä¸ªæ®‹å·®è·¯å¾„ï¼Œä½ å°†å…¶æå–ã€‚ä½ åº”ç”¨ä¸€ä¸ªå±‚å½’ä¸€åŒ–ï¼Œç„¶åæ˜¯è‡ªæ³¨æ„åŠ›ï¼Œè¿™æ ·è¿™å…«ä¸ªèŠ‚ç‚¹å°±è¿›è¡Œäº¤æµã€‚ä½†ä½ å¿…é¡»è®°ä½æ‰¹æ¬¡æ˜¯å››ï¼Œæ‰€ä»¥å› ä¸ºæ‰¹æ¬¡æ˜¯å››ï¼Œè¿™ä¹Ÿæ˜¯åº”ç”¨çš„ï¼Œå› æ­¤æˆ‘ä»¬æœ‰å…«ä¸ªèŠ‚ç‚¹åœ¨äº¤æµï¼Œä½†è¿™å››ä¸ªæ‰¹æ¬¡ä¸­çš„æ¯ä¸€ä¸ªéƒ½åœ¨è¿™äº›å…«ä¸ªèŠ‚ç‚¹ä¸Šå„è‡ªè¿›è¡Œäº¤æµã€‚
- en: there's no crisscross across the batchsh dimension of course there's no batchche
    norm range anyway luckierã€‚And then once theyve changed informationï¼Œ they are processed
    using the modo receptorã€‚and that's the compute baseã€‚So and then also here are
    missing we are missing the cross attention and because this is a decoder only
    modelã€‚so all we have is this step hereï¼Œ the multiheaded attention and that's this
    line the communicate phaseã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ‰¹æ¬¡ç»´åº¦ä¸Šå½“ç„¶æ²¡æœ‰äº¤å‰ï¼Œæ‰¹æ¬¡å½’ä¸€åŒ–èŒƒå›´ä¹Ÿæ²¡æœ‰ï¼Œå› æ­¤è¿æ°”æ›´å¥½ã€‚ä¸€æ—¦å®ƒä»¬äº¤æ¢äº†ä¿¡æ¯ï¼Œå°±ä½¿ç”¨modo receptorè¿›è¡Œå¤„ç†ã€‚è¿™æ˜¯è®¡ç®—åŸºç¡€ã€‚æ‰€ä»¥è¿™é‡Œè¿˜æœ‰ç¼ºå¤±ï¼Œæˆ‘ä»¬ç¼ºå°‘äº¤å‰æ³¨æ„åŠ›ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªä»…è§£ç å™¨æ¨¡å‹ã€‚å› æ­¤æˆ‘ä»¬åªæœ‰è¿™ä¸ªæ­¥éª¤ï¼Œå³å¤šå¤´æ³¨æ„åŠ›ï¼Œè¿™æ˜¯è¿™ä¸ªæ²Ÿé€šé˜¶æ®µã€‚
- en: and then we have the feet forwardï¼Œ which is the MLPã€‚And that's the complete
    phaseã€‚I'll take questions a bit laterã€‚Then the MLP here is fairly straightforward
    the MLP is just individual processing on each node just transforming the feature
    representation sort of at that node soã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬æœ‰å‰é¦ˆå±‚ï¼Œå³ MLPã€‚è¿™æ˜¯å®Œæ•´çš„é˜¶æ®µã€‚æˆ‘ç¨åä¼šå›ç­”é—®é¢˜ã€‚è¿™é‡Œçš„ MLP ç›¸å½“ç®€å•ï¼ŒMLP åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿›è¡Œä¸ªä½“å¤„ç†ï¼Œåªæ˜¯åœ¨è¯¥èŠ‚ç‚¹ä¸Šè½¬æ¢ç‰¹å¾è¡¨ç¤ºã€‚
- en: Applying a two layer neural nu with a gal nonlinearityã€‚which is just think of
    it as a re or something like thatï¼Œ it just a nonlinearityã€‚And then MLP straightforwardï¼Œ
    I don't think there's anything too crazy thereã€‚and then this is the causal self
    attention partï¼Œ the communication phaseã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨ä¸€ä¸ªä¸¤å±‚ç¥ç»ç½‘ç»œï¼Œå¸¦æœ‰ä¸€ä¸ªä¼½å°”éçº¿æ€§ã€‚å¯ä»¥å°†å…¶è§†ä¸ºä¸€ä¸ªé‡æ˜ å°„æˆ–ç±»ä¼¼çš„ä¸œè¥¿ï¼Œè¿™åªæ˜¯ä¸€ä¸ªéçº¿æ€§ã€‚è€Œä¸” MLP å¾ˆç›´æ¥ï¼Œæˆ‘è®¤ä¸ºè¿™é‡Œæ²¡æœ‰ä»€ä¹ˆå¤ªç–¯ç‹‚çš„ã€‚è¿™æ˜¯å› æœè‡ªæ³¨æ„åŠ›éƒ¨åˆ†ï¼Œé€šä¿¡é˜¶æ®µã€‚
- en: So this is kind of like the loo of things and the most complicated partã€‚it's
    only complicated because of the batchã€‚And the implementation detail of how you
    mask the connectivity in the graph so that you don' you can't obtain any information
    from the future when you're predicting your tokenã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æœ‰ç‚¹åƒäº‹ç‰©çš„è½®å»“ï¼Œæœ€å¤æ‚çš„éƒ¨åˆ†ã€‚ä¹‹æ‰€ä»¥å¤æ‚ï¼Œæ˜¯å› ä¸ºæ‰¹å¤„ç†å’Œå®ç°ç»†èŠ‚ï¼Œæ¯”å¦‚å¦‚ä½•åœ¨å›¾ä¸­å±è”½è¿æ¥ï¼Œä»¥ä¾¿åœ¨é¢„æµ‹æ ‡è®°æ—¶æ— æ³•ä»æœªæ¥è·å–ä»»ä½•ä¿¡æ¯ã€‚
- en: otherwise it gives away the information so if I'm the fifth token and if I'm
    the fifth position then I'm getting the fourth token coming into the input and
    I'm attending to the third second and first and I'm trying to figure out what
    is the what is the next token well then in this batch in the next element over
    in the time dimension the answer is at the input so I can't get any information
    from there so that's why this is all tricky but basically in the forward passã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¦åˆ™å®ƒä¼šæ³„éœ²ä¿¡æ¯ï¼Œæ‰€ä»¥å¦‚æœæˆ‘æ˜¯ç¬¬äº”ä¸ªæ ‡è®°ï¼Œå¹¶ä¸”å¤„äºç¬¬äº”ä¸ªä½ç½®ï¼Œé‚£ä¹ˆæˆ‘ä¼šè·å¾—ç¬¬å››ä¸ªæ ‡è®°ä½œä¸ºè¾“å…¥ï¼Œæˆ‘ä¼šå…³æ³¨ç¬¬ä¸‰ã€ç¬¬äºŒå’Œç¬¬ä¸€ä¸ªæ ‡è®°ï¼Œå¹¶è¯•å›¾å¼„æ¸…æ¥šä¸‹ä¸€ä¸ªæ ‡è®°æ˜¯ä»€ä¹ˆã€‚åœ¨è¿™ä¸€æ‰¹çš„ä¸‹ä¸€ä¸ªæ—¶é—´ç»´åº¦çš„å…ƒç´ ä¸­ï¼Œç­”æ¡ˆåœ¨è¾“å…¥ä¸­ï¼Œæ‰€ä»¥æˆ‘æ— æ³•ä»é‚£é‡Œè·å–ä»»ä½•ä¿¡æ¯ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™ä¸€åˆ‡éƒ½å¾ˆæ£˜æ‰‹ï¼Œä½†åŸºæœ¬ä¸Šåœ¨å‰å‘ä¼ é€’ä¸­ã€‚
- en: å—¯ã€‚We are calculating the queriesï¼Œ keys and values based on xã€‚So these are the
    keys queries and values here when I'm computing the attentionã€‚I have the queries
    matrix multiplying the keysï¼Œ so this is the dot product in parallel for all the
    queries in all pieceã€‚In older headsã€‚So that I I felt to mention that there's also
    the aspect of the heads which is also done all and parallel hereã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚æˆ‘ä»¬åŸºäº x è®¡ç®—æŸ¥è¯¢ã€é”®å’Œå€¼ã€‚è¿™äº›å°±æ˜¯æˆ‘åœ¨è®¡ç®—æ³¨æ„åŠ›æ—¶çš„é”®ã€æŸ¥è¯¢å’Œå€¼ã€‚æˆ‘æœ‰æŸ¥è¯¢çŸ©é˜µä¹˜ä»¥é”®ï¼Œå› æ­¤è¿™æ˜¯å¯¹æ‰€æœ‰æŸ¥è¯¢å’Œæ‰€æœ‰å¤´éƒ¨çš„å¹¶è¡Œç‚¹ç§¯ã€‚æ‰€ä»¥æˆ‘è§‰å¾—è¿˜éœ€è¦æåˆ°çš„æ˜¯å¤´éƒ¨çš„éƒ¨åˆ†ä¹Ÿæ˜¯åœ¨è¿™é‡Œå¹¶è¡Œå®Œæˆçš„ã€‚
- en: so we have the batch dimensionï¼Œ the time dimension and the head dimension and
    you end up with five dimensional tenss and it's all really confusing so I invite
    you to step through it later and commit yourself that this is actually doing the
    right thing basically give have the batch dimensionã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æœ‰æ‰¹å¤„ç†ç»´åº¦ã€æ—¶é—´ç»´åº¦å’Œå¤´éƒ¨ç»´åº¦ï¼Œä½ æœ€ç»ˆä¼šå¾—åˆ°äº”ç»´å¼ é‡ï¼Œè¿™ä¸€åˆ‡éƒ½å¾ˆæ··ä¹±ï¼Œæ‰€ä»¥æˆ‘é‚€è¯·ä½ ç¨åé€æ­¥äº†è§£ï¼Œå¹¶ç¡®è®¤è¿™å®é™…ä¸Šæ˜¯åœ¨åšæ­£ç¡®çš„äº‹æƒ…ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯æ‹¥æœ‰æ‰¹å¤„ç†ç»´åº¦ã€‚
- en: the head dimension and the time dimension and then you have features at themã€‚And
    so this is evaluating for all the batch elementsã€‚for all the head elements and
    all the time elementsã€‚The simple Python that I gave you earlierã€‚which is queryductpro
    Pã€‚Then here we do a mask to fill and what this is doing is it's basically clamping
    theã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¤´éƒ¨ç»´åº¦å’Œæ—¶é—´ç»´åº¦ï¼Œç„¶åä½ ä¼šå¾—åˆ°å®ƒä»¬çš„ç‰¹å¾ã€‚å› æ­¤ï¼Œè¿™æ˜¯åœ¨è¯„ä¼°æ‰€æœ‰æ‰¹å¤„ç†å…ƒç´ ã€æ‰€æœ‰å¤´éƒ¨å…ƒç´ å’Œæ‰€æœ‰æ—¶é—´å…ƒç´ ã€‚ä¹‹å‰ç»™ä½ çš„ç®€å• Python ç¤ºä¾‹ï¼Œå°±æ˜¯ queryductpro
    Pã€‚ç„¶åæˆ‘ä»¬åœ¨è¿™é‡Œåšä¸€ä¸ªå¡«å……æ©ç ï¼ŒåŸºæœ¬ä¸Šæ˜¯åœ¨é™åˆ¶è¿™ä¸ªã€‚
- en: The attention between the node that are not supposed to communicate to be negative
    infinity and we're doing negative infinity because we're about to soft max and
    so negative infinity will make basically the attention that those elements be
    zeroã€‚ğŸ˜¡ï¼ŒAnd so here we are going to basically end up with the weightsã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›åœ¨ä¸åº”è¯¥ç›¸äº’é€šä¿¡çš„èŠ‚ç‚¹ä¹‹é—´è¢«è®¾ç½®ä¸ºè´Ÿæ— ç©·ï¼Œæˆ‘ä»¬è¿™æ ·åšæ˜¯å› ä¸ºæˆ‘ä»¬å³å°†è¿›è¡Œ softmaxï¼Œå› æ­¤è´Ÿæ— ç©·ä¼šä½¿é‚£äº›å…ƒç´ çš„æ³¨æ„åŠ›åŸºæœ¬ä¸Šä¸ºé›¶ã€‚ğŸ˜¡ æ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬åŸºæœ¬ä¸Šä¼šå¾—åˆ°æƒé‡ã€‚
- en: They serve affinities between these notesã€‚Optional dropout and then here attention
    matrix multiply V is basically theã€‚The gathering of the informationï¼Œ according
    to the ainities we've calculatedã€‚and this is just a weighted sum of the values
    at all those nodesã€‚So this matrix multiplies as doing that weighted sumã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬åœ¨è¿™äº›èŠ‚ç‚¹ä¹‹é—´æœåŠ¡äºäº²å’Œæ€§ã€‚å¯é€‰çš„ dropoutï¼Œç„¶åè¿™é‡Œçš„æ³¨æ„åŠ›çŸ©é˜µä¹˜ä»¥ Vï¼ŒåŸºæœ¬ä¸Šæ˜¯æ ¹æ®æˆ‘ä»¬è®¡ç®—çš„äº²å’Œæ€§æ¥æ±‡é›†ä¿¡æ¯ã€‚è¿™åªæ˜¯æ‰€æœ‰è¿™äº›èŠ‚ç‚¹ä¸Šå€¼çš„åŠ æƒå’Œã€‚æ‰€ä»¥è¿™ä¸ªçŸ©é˜µä¹˜æ³•å°±æ˜¯åœ¨è¿›è¡Œè¿™ä¸ªåŠ æƒå’Œã€‚
- en: And then transpose contiguous view because it's all complicated and bashed in
    five dimensional testorsã€‚but it's really not doing anything optional dropout and
    then a linear projection back to the residual pathwayã€‚So this is implementing
    the communication phase nowã€‚Then you can train this transformerã€‚And then you can
    generate infinite Shakespeare and you will simply do this by because our block
    size is eightã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: we start with sum tokenï¼Œ say like I used in this caseã€‚you can use something
    like a new line as the star tokenã€‚And then you communicate only to yourself because
    there's a single node and you get the probability distribution for the first word
    in the sequenceã€‚and then you decode it or the first character in the sequenceã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: you decode the character and then you bring back the character and you reencode
    it as an integer and now you have the second thing and so you get okayã€‚where the
    first position and this is whatever integer it isã€‚Addd the position encodings
    goes into the sequence goes into transformer and again this token now communicates
    with the first open and its identity and so you just keep plugging the back and
    once you run out of the block sizeã€‚which is eightï¼Œ you start to crawlã€‚Because
    you can never have block size more than8 in the way you've trained this transformer
    so we have more and more context until eight and then if you want to generate
    beyond date you have to start cropping because the transformer only works for
    eight elements in time dimension and so all of these transformers in the naive
    setting have a finite de box size or context length and in typical models this
    will be 1024 tokens or 2048 tokens something like that but these tokens are usually
    like EE tokens or sentence piece tokens or work these tokens there's many different
    encodings so it's not like that long and so that's why I think did' mention we
    really want to expand the context size and gets gnarly because the attention is
    sp in the na caseã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Nowï¼Œ if you want toã€‚Implement an encoder instead of a decoderã€‚Attentionã€‚Then
    all you have to do is this masterã€‚You just delete that lineã€‚So if you don't mask
    the attentionã€‚then all the nodes communicate to each other and everything is allowed
    and information flows between all the nodesã€‚So if you want to have the encoder
    hereï¼Œ just delete all the encoder blocks will use attention where this line is
    deletedã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: that's itã€‚So you're allowing whatever is encode in my storeï¼Œ say 10 tokensï¼Œ
    10 notesã€‚and they are all allowed to communicate to each other going up the transformerã€‚And
    then if you want to implement cross attentionï¼Œ so you have a full encoder decoder
    transformerã€‚not just a decoder only transformer or GPTã€‚Then we need to also add
    cross attention in the middleã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: so here there's a self attention piece where all the there's a self attention
    pieceã€‚a cross attention piece and this MLP and in the cross attentionã€‚We need
    to take the features from the top of the encoderã€‚We need to add one more line
    here and this would be the cross attention instead of I should have implemented
    it instead of just pointing I thinkã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: but there will be a cross attention line here so we'll have three lines because
    we need to add another block and the queries will come from xã€‚but the piece and
    the values will come from the top of the encoderã€‚And there will be basically information
    flowing from the encoder strictly to all the nodes inside Xã€‚And then that's itï¼Œ
    so it's very simple sort of modifications on the decoder attentionã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: So you'll hear people talk that you kind of have a decoder only model like GPã€‚you
    can have an encoder only model like Bt or you can have an encoder decoder model
    like say t5 doing things like machine translation so and in Bt you can't train
    it using sort of thisã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Language modeling setup that's utter aggressive and you're just trying to predict
    the next omin sequence you're training it to a slightly different objectives you're
    putting in like the full sentence and the full sentence is allowed to communicate
    fully and then you're trying to classify sentiment or something like that so you're
    not trying to model like the next token in the sequence so these are trans slightly
    different with mathã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: å•Šã€‚With using masking and other deno techniquesã€‚Okayã€‚so that's kind of like the
    transformer I'm going to continue so yeahï¼Œ maybe more questionsã€‚ä½ è¯´æˆ‘ã€‚å“¦ã€‚å¯¹ã€‚æˆ‘ reallyã€‚Iã€‚Okayã€‚Youï¼Œ
    good pepper still forã€‚Find is a dynamic route thatã€‚We can actually change in everything
    and you also have somethingã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Did it like we are enforcing these constraintsprints on it by just masking by
    give aware of itã€‚å•Šå•Šã€‚So I'm not sure if I fully followï¼Œ so there's different ways
    to look at this analogyã€‚but one analogy is you can interpret this graph as really
    fixed it's just that every time you do the communicateã€‚we are using different
    weightsï¼Œ you can look at it downã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: So if we have block size of eight in my exampleï¼Œ we would have eight nodes here
    we have two four six okayã€‚so we'd have eight nodesï¼Œ they would be connected in
    you lay them out and you only connect from left to right but we're differentã€‚Yeahï¼Œ
    we'll have a very connected partã€‚Ohã€‚Why wouldU the connections don't change as
    a function of the data or something like thatï¼Ÿ
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The notã€‚I wonder if I theres anyã€‚I don't think I've seen a single example where
    the connectivity changes dynamically option dataã€‚usually the connectivity is fixed
    if you have an encoder and you're training a Btã€‚you have how many tokens you want
    and they are fully connectedã€‚And if you have decoder a long modelã€‚you have the
    triular thingï¼Œ and if you have encoder decoderã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: then you have awkwardly sort of like two tools of nodesã€‚è¯¶å‘€ã€‚å•Šå¯¹å•Šå°±æ˜¯æˆ‘ä»¬ã€‚æœ€åå°±ã€‚å®¶ä½¢ä½ å—¯ã€‚å—¯ã€‚ä¸è¦å¯¹èµ·ã€‚ä½œä¸ºç›´æ’­äººæˆ‘è¯‰ã€‚ç¬¬åˆ’å­™å—°æœˆå•Šã€‚I
    wonderã€‚Do much more about this and then goã€‚éƒ½èµ°ã€‚ä½ å†‡æƒ³å‡ºå»ã€‚æœ‰ç›´ã€‚Okayã€‚Whichã€‚Partã€‚Okayã€‚å¯¹ç³»å’§ã€‚ã“ã‚Œã€‚å—¯ã€‚å“¦ï¼Œæœ‰ã‹ã€‚å†‡
    yourã€‚Different thingsã€‚æˆ‘æ˜¯å¤šã€‚Yeah it's really hard to say so that's why I think this
    paper is so interesting is likeã€‚yeah usually you'd see like a path and maybe they
    head path internally because just didn't publish it and all you can see is sort
    of things that didn't look like the transformer I mean you had resnets which have
    lots of this but a resnet would be kind of like this but there's there's no self-
    attention componentã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: but the MLP is there kind of in a resNeã€‚å—¯ã€‚So a resonanceson looks very much
    like thisã€‚except there's noï¼Œ you can use layer norms and resonances I believe
    as wellã€‚typically sometimes they can be bash normsã€‚So it is kind of like a renetã€‚it
    is kind of like they took a ressonnet and they put in a trans selfpottiary block
    in addition to the preexistent MLP blockã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: which is kind of like convolutions and MLP was strictly speaking deconvolution
    one by one convolutionã€‚but I think the idea is similar that MLP is just kind of
    like you know typical weightsã€‚No nonlineararity weights or operationã€‚å—¯ã€‚And but
    I will say likeï¼Œ yeahã€‚it's kind of interesting becauseã€‚A lot of work is not there
    and then they give you this transformer and then it turns out five years later
    it's not changed even though everyone's trying to change it so it's kind of interesting
    to me that iss kind of like a package came like a package which I think is really
    interesting historically and I also talked to paper authors and they were unaware
    of the impact that transform would have at the time so when you read this paper
    actually it's kind of unfortunate because this is like the paper that changed
    everything but when people read it it's like question marks because it reads like
    a pretty random machine translation paper I go over doing machine translation
    oh here's a cool architecture okay great good results likeã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: It's it doesn't sort of know what's going to happen and so when people read
    it today I think they're kind of confused potentially like having like havingã€‚I
    will have some tweets at the endï¼Œ but I think I would have renamed it with the
    benefit of hindsight of likeã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: well I'll get to itã€‚Yeahã€‚è¿™ä¸ªã€‚Okayã€‚æˆ‘ä¸ªæ”¾ã€‚Yeah I think that's a good question as
    well currently I mean I certainly don't love the autoaggressive modeling approachã€‚I
    think it's kind of weird to like sample a token and then commit to itã€‚Soã€‚know
    maybe there's some ways some hybrids with diffusion as an exampleã€‚which I think
    would be really coolã€‚Or we'll find some other ways to like edit the sequences
    later about filling in our regressive frameworkã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: but I think the fusion is kind of like an up and coming modeling approach that
    I personally find much more appeal when I sample text I don't go chunk chunk chunk
    and commit I do a draft one and then I do a better draft twoã€‚And that feels like
    a the fusion processã€‚So that would be my hopeã€‚Okayï¼Œ also question so yeahã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: you like the logic but itã€‚When you sayï¼Œ like theã€‚Se attention is sort of like
    computing like a agiator because to the adopt product on the notionaryã€‚And then
    those we have the edge the like multily by the other valuesã€‚And then just appropriate
    it yesï¼Œ yes rightï¼Œ and do you think there's like aã€‚Agy like profitable networks
    andã€‚I find the graph neural networks kind of like a confusing term becauseã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: I meanï¼Œ yeahï¼Œ previouslyï¼Œ there was this notion ofã€‚I kind of thought maybe today
    everything is a graph neural network because the transformer is a graph neural
    and processorã€‚the native representation that the transformer operates over is
    sets that are connected by edges in a direct way and so that's the native representation
    and then yeahã€‚Okayï¼Œ I should go on because I still have like 30 slidesã€‚G solveã€‚Those
    want one provide byã€‚Ohï¼Œ yeahã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Yeahï¼Œ the root Dï¼Œ I think basically like as if you're initializing with random
    weight separate from ausion as your dimension size growsã€‚so does your valuesï¼Œ
    the variance grows and then your softmax will just become the one half vectorã€‚so
    it's just a way to control the variance and bring it to always be in a good range
    for softmax and nice distributionã€‚Oã€‚So it's almost like an initialization thingã€‚Okayã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: So transformers have been applied to all the other fields and the way this was
    done is in my opinion kind of ridiculous ways honestlyã€‚because I was a computer
    vision person and you have commons and they kind of make senseã€‚so what we're doing
    now with bits as an example is you take an image and you chop it up into little
    squares and then those squares literally feed into a transformer and that's itã€‚Which
    is kind of ridiculous and soã€‚ä¸‹é¢ã€‚Yeahï¼Œ and so the transformer doesn't even in the
    simplest case like really know where these patches might come fromã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: they are usually positionly encodedï¼Œ but it has to sort of like rediscover a
    lot of the structure I think of them in some waysã€‚and it's kind of weird to approach
    it that wayã€‚But it's just like the simplest baseline of just choing up big images
    into small squares and feeding them in as like the individual nodes actually works
    fairly well and then this is in transformer encoder so all the patches are talking
    to each other throughout the interpretã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: And the number of nodes here would be sort of like nineã€‚Okayã€‚Also in speech
    recognitionã€‚you just take your Mel spectrogram and you chop it up into your slices
    and feed them into a transformerã€‚so there wrote paper like this but also whisper
    whisper is a copy based transformer if you saw whisper from OpenAI you just chop
    up Mel Spectrogram and feed it into a transformer and then pretend you're dealing
    with text and it works very wellã€‚Decision transformer and RL you take your state's
    actions and reward that you experience an environment and you just pretend it's
    a language and you start to model the sequences of that and then you can use that
    for planning later that works pretty well you know even things like alpha fold
    so we were briefly talking about molecules and how you can plug them in so at
    the heart of alpha fold computationally is also a transformerã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: One thing I wanted to also say about transformers is I find that they're very
    they're super flexible and I really enjoy that I'll give you an example from Teslaã€‚Like
    you have a come that that takes an image and makes predictions about the image
    and then the big question is how do you feed in extra information and it's not
    always trivial like say I have additional information that I want to inform that
    I want the output to be informed by maybe I have other sensors like radar maybe
    I have some map information or a vehicle type or some audio and the question is
    how do you feed information into a come that like where do you feed it in do you
    concatenate it like how do you do you add at what stage and so with the transformer
    it's much easier because you just take whatever you wantã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: you chop it up into piecessis and you feed it in with a set of what you had
    before and you let the self- attentionten figure out how the potentially should
    communicate and that actually frankly worksã€‚So just chop up everything and throw
    it into the mix is kind of like the way and it frees neurallet from this version
    of Eidean space where previously you had to arrange your computation to conform
    to the Elidean space or three dimensions of how you're laying out the compute
    like the compute actually kind of happens in normal like 3D space if you think
    about itã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: but in intention everything is just sets so it's a very flexible framework and
    you can just like throw this stuff into your conditioning set and everything just
    self-ated over so it's quite beautiful with retro respect okayã€‚So now what exactly
    makes transformers so effectiveï¼Ÿ
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: I think a good example of this comes from the GT3 paper which I encourage people
    to read language models are twoshot learnersã€‚I would have probably remained this
    a little bitï¼Œ I would have said something like transformers are capable of in
    context learning or like meta learning that's kind of like what makes them really
    special so basically the something that they're working with is okay I have some
    context and I'm trying to let's say passage this is just one example of many I
    have a passage and I'm asking questions about itã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: And then I'm giving as part of the context in the promptï¼Œ I'm giving the questions
    and the answersã€‚so I'm giving one example of question answerï¼Œ another example
    of question answerã€‚another example of question answer and so onã€‚And this becomes
    aã€‚å¯ä»¥ï¼Œ peopleè·Ÿä»–è¯´ä¸€å•Šã€‚Okayã€‚it is really important for me thinkã€‚Okayï¼Œ so what's really
    interesting is basically likeã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: With more examples given in the context the accuracy improves and so what that
    hint at is that the transformer is able to somehow learn in the activations without
    doing any gradient descent in a typical fine tuning fashion so if you fine tune
    you have to give an example and the answer and you fine tuning using gradient
    descent but it looks like the transformer internally in its weights is doing something
    that looks like potentially gradient decent some kind of the mental learning in
    the weights of the transformer as it is reading the prompt and so in this paper
    they go into okay distinguishing this outer loop with stochastic gradient descent
    and this inner loop of the in contexttext learning so the inner loop is as the
    transformer is sort of like reading the sequence almost and the outer loop is
    is the training by gradient descent so basically there's some training happening
    in the activation of the transformer as it is consuming a sequence that may be
    very much looks like gradient descent and so there's some recent papers that kind
    of hint at this and study it and so as an example in this paper here they propose
    something called the raw operator and they argue that the raw operator is implemented
    by a transformerã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: And then they show that you can implement things like Ri regression on top of
    a raw operatorã€‚and so this is kind of giving their papers hinting that maybe there
    is something that looks like gradient based learning inside the activations of
    the transformerã€‚
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: And I think this is not impossible to think through because is what is gradient
    based learningã€‚overpasï¼Œ backward passï¼Œ and then updateï¼ŸWellï¼Œ that looks like a
    resumeã€‚Right because you're just changing you're adding to the weights so you
    start initial random set of weightsã€‚forward pass backward pass and update your
    weights and then forward pass backward path weightsã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: it looks like a resnet transformers is a resnet so much more hand wavyã€‚but basically
    some paper trying to hint that why that would be potentially possibleã€‚And then
    I have a bunch of tweets I just gotten and pasted here in the endã€‚this was kind
    of like meant for general consumptionã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: so they're a bit more high level and high a little bitã€‚but I'm talking about
    why this architecture is so interesting and why why potentially became so popular
    and I think it simultaneously optimizes three properties that I think are very
    desirable number oneã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: the transform is very expressive and if overpass it's sort of like is' able
    to implement very interesting functionsã€‚potentially functions that can even like
    do metaerã€‚Number twoã€‚it is very optimizable thanks to things like residual connections
    layerknow and so on and number three it's extremely efficientã€‚this is not always
    appreciatedï¼Œ but the transformerã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: if you look at the computational graph is a shallow wide network which is perfect
    to take advantage of the parallelmal GPUs so I think the transformer was designed
    very deliberately to run efficiently on GPUs there's previous work like neural
    GPU that I really enjoy as well which is really just like how do we design neural
    adss that are efficient on GPUs and thinking backwards from the constraints of
    the hardware which I think is a very interesting way to think about itã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: å•Šã€‚Ohã€‚Oh yeah so here I'm saying I probably would have called I probably would
    have called the transformer a general purposeus efficient optimizable computer
    instead of the attention is all you need like that's what I would have maybe in
    hindsight called that paper is proposing it is a model that isã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚Very general purposeï¼Œ so forward passes express itã€‚it very efficient in terms
    of GPU usage and is easily optimizable by gradientd descent and trains it very
    nicelyã€‚Then I have some other hot tweets hereã€‚Anywayï¼Œ so yeah you can read them
    laterã€‚but I think this ones maybe interestingï¼Œ so if previous neural nets are
    special purpose computers designed for a specific tasksã€‚
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: GPT is a general purpose computer reconfigurable at runtime to run natural language
    programsã€‚so program the programs are given as promptsã€‚And then Chi became Ro the
    program by completing the documentã€‚So I really like these analogies personally
    to computerã€‚it's just like a powerful computer and it's optimizable by gradient
    descentã€‚Andã€‚ç³» don't know oå•Šã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: I think you can read this laterï¼Œ but it's right out just thank youï¼Œ I'll just
    leave this upã€‚ğŸ˜Šï¼ŒYeahã€‚So sorryï¼Œ I just found this leadã€‚Turns out that if you scale
    up the training set and use a powerful enough neuralNes like a transformerã€‚the
    network becomes a kind of general purpose computer over textã€‚so I think that's
    a kind of like nice way to look at it and instead of performing a single text
    sequence you can design the sequence in the prompt and because the transformer
    is both powerful but also is trained on large enough very hard data set it kind
    of becomes a general purpose text computer and so I think that's kind of interesting
    way you look at itã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Yeahã€‚å“¦ã€‚Yeahã€‚åã€‚Yeahã€‚å“¦ã€‚å¯¹ã€‚å•Šã€‚ç³» guessã€‚Iç³»ã€‚Whatã€‚Goodã€‚å¥½ã€‚å“¦ã€‚That was alreadyientã€‚æœ‰ã€‚Ohã€‚So
    it isã€‚Ohã€‚Im moved sorryã€‚Yes meï¼Œä½†æ˜¯æˆ‘æ„æ€è§‰å¾—å¥½ã€‚Okayã€‚å…¶å®å¤§ä¸€å¼€è¿‡ã€‚Ohã€‚I was too favoriteã€‚Its
    pretty really like most of itã€‚you knowã€‚It mostly more efficientã€‚It very good place
    but you have thatã€‚å¤§ä¼šã€‚ã™ã¦ã€‚Drã€‚ Byã€‚Yeah veryã€‚So I think there's a bit of thatï¼Œ yeahï¼Œ
    so I would say RNNs like in principle yesã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: they can implement arbitrary programsï¼Œ I think it's kind of like a useless statement
    to some extent because they are they're probablyã€‚I'm not sure that they're they're
    probably expressive because in a sense of like power in that they can implement
    these arbitrary functionsã€‚
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: But they're not optimizableã€‚ğŸ˜¡ï¼ŒAnd they're certainly not efficient because they
    are serial computing devicesã€‚Um so I thinkï¼Œ so if you look at it as a compute
    graphï¼Œ RNNs are very longã€‚å˜äº†ã€‚ä½¢è¡€æŒ‚ã€‚å•Šã€‚Like if you stretched out the neurons and you
    look like take all the individual neurons in our connectivity and stretch them
    out and try to visualize themã€‚RNNs would be like a very long graph in a bad and
    it's bad also optizability because I don't exactly know why but just the rough
    intuition is when you're back propagatingã€‚
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: you don't want to make too many stepsã€‚ğŸ˜¡ï¼ŒAnd so transformers are a shallow wide
    graphã€‚and so from supervision to inputs is a very small number of hopsã€‚And it's
    along residual pathways which make gradients flow very easily and there's all
    these layer norms to control gradient the scales of all of those activations and
    so there's not too many hops and you're going from supervision to input very quickly
    and just flows through the graphã€‚
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Ands it can all be done in parallel so you don't need to do this encoder decoder
    RNANs you have to go from first wordã€‚then cycle word then third wordï¼Œ but here
    in transformerã€‚every single word was processed completely as sort of in parallelã€‚Which
    is kind of the so I think all these are really important because all these are
    really important and I think number three is less talked about but extremely important
    because in deep learning scale matters and so the size of the network that you
    can train gives you is extremely important and so if it's efficient on the current
    hardware then we can make it biggerã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: æˆ‘è€Œå®¶å‡ºåšŸä½¢å¯¹æˆ‘æˆ‘ä¸ªæœ‰æˆ‘è‡ªå·±ã€‚ã‹ã€‚Yeah how does that believe the dataNo so yeah so you take your
    image and you apparently chop them up into patches so there's the first thousand
    tokens or whatever and now I have a special so radar could be also but I don't
    actually know the native representation of radar soã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: But you couldï¼Œ you just need to chop it up and enter itã€‚And then you have to
    encode it somehowã€‚Like the transformer needs to know that they're coming from
    radarã€‚So you create a specialã€‚You have some kind of a special token that youã€‚These
    radar tokens are much slightly different in the representation and it's learnable
    by gradientcent andã€‚Like vehicle information would also come in with a special
    embeddedbedding token that can be learnedã€‚
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Soã€‚Have you those like you don'tï¼Œ it's all just a setã€‚And just's when the voiceman
    guarantee know hit and it say buttonã€‚Yeahï¼Œ it's all just a setã€‚but you can position
    encode these sets if you wantã€‚But position encoding means you can hardwireã€‚for
    exampleï¼Œ the coordinates like using sinusoss and Posonsï¼Œ you can hardwire thatã€‚
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: but it's better if you don't hardwire the positionã€‚you just it's just a vector
    that is always hanging out at this locationã€‚whatever content is there just adss
    on it and this vector strainable by background that's how I do itã€‚Yeahå””ã€‚I think
    they're kind of delegateï¼Œ like they seem to workã€‚
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: but it seems like sometimes our solution want to put sub structureã€‚å’å˜¢ä½¢ç³» mightã€‚å—¯ã€‚I'
    if I understand questionï¼Œ so I mean the position encoders like they they're actually
    like not they have okayã€‚so they have very little inductive bias or something like
    that they're just vectors hanging out the location always and you're trying to
    you're trying to help them network in some wayã€‚Andã€‚I think the intuition is goodï¼Œ
    butã€‚Like if you have enough data usually trying to mess with it is like a bad
    thing like trying to like trying to enter knowledge when you have enough knowledge
    in the data set itself is not usually productive so it really depends on what
    scale you are if you have infinity data then you actually want to encode less
    and less that turns out to work better and if you have very little data then actually
    you do want to encode some biases and maybe if you have a much smaller data set
    and maybe coalutions are a good idea because you actually have this bias coming
    from more filters and soã€‚
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: But I thinkã€‚So the transformer is extremely generalã€‚but there are ways to mess
    with the encodings to put in more structure like you couldï¼Œ for exampleã€‚encode
    sinuses and cosines and fix it or you could actually go to the attention mechanism
    and sayã€‚okayï¼Œ if my image is cho up into patches this patch can only communicate
    to this neighborhood and you can you just do that in the attention matrix just
    mask out whatever you don't want to communicate and so people really play with
    this because the full attention is inefficient so they will intersperseã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: for exampleï¼Œ layers that only communicate little patches and then layers to
    communicate globally and they will sort of do all kinds of tricks like that so
    you can slowly bring in more inductive biasã€‚you would do it but the inductive
    biases are sort of like they're factored out from the core transformer and they
    are factored out in theã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: In the connectivity of the notesï¼Œ and they are factored out in positioning and
    can mess thisã€‚P propositionã€‚AndHow you pocketï¼Ÿå¤§ã€‚ä¸è¯¥æˆ‘ã€‚å“¦ã€‚So there's probably about
    200 papers on this nowã€‚if not moreï¼Œ they're kind of hard to keep track of honestly
    like my Safari browserã€‚which is what's sort on my computerï¼Œ like 200 open tabsã€‚Butã€‚Yeahã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: so'm not even sure if I want to pick my paper honestlyã€‚And you can a maybe use
    a transformer like the other one that I actually like even more is potentially
    keep the context length fixed but allow the network to somehow use a scratchpa
    and so the way this works is you will teach the transformer somehow via examples
    in the hey you actually have a scratchy hey basically you can't remember too much
    your context line is finite but you can use a scratchpad and you do that by emitting
    a scratchpa and then writing whatever you want to remember and then end scratchpad
    and then you continue with whatever you want and then later when it's decoding
    you actually like have special logic that when you detect start scratchpad you
    will sort of like save whatever it puts in there in like external thing and allow
    it to attend over itã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: So basically you can teach the transformer just dynamically because it's so
    meta learnedã€‚you can teach it dynamically to use other gizmos and gadgets and
    allow it to expandend his memory that way if that makes sense it's just like human
    learning to use a notepad right you don't have to keep it in your brain so keeping
    things in your brain is kind of like a context line from the transformerã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: but maybe you can just give it a notebook and then it con query the notebook
    and read from it and write to it maybe you anotherã€‚Yesã€‚å’è§‰ç°åœ¨ä½ æ€ä¸‹ã€‚The way going to
    thisã€‚éƒ½å¿«å˜…å•²ã€‚ä¸ºä»€ä½ å˜…ã€‚I don't know if I detected thatã€‚I kind of feel like did you feel
    like it was more than just a long prompt that's unfoldingï¼Ÿå’±å†ã€‚I didn't try extensivelyï¼Œ
    but I did see a forgetting event and I kind of felt like the block size was just
    movedã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: å•Šã€‚Maybe I'mï¼Œ I don't actually know about the internal of tragedyã€‚Get twoã€‚So
    one question isã€‚what do you think about architectureï¼ŸS for S forã€‚I' say I don't
    know thisï¼Œ which one is this forï¼Ÿ
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: And second questionï¼Œ this was a personal questionï¼Œ what are you going to work
    on nextï¼ŸI meanã€‚so right now I'm working on things like nanoGPT where is nanoGTã€‚ğŸ˜Šï¼Œå‘ƒã€‚I
    meanã€‚I'm going basically slightly from computer vision and like part kind of like
    the computer vision based products do a little bit in the language domain whereas
    tryGTã€‚okay then on GTã€‚So originally I had MinGï¼Œ which I ever wrotete to nanoGPT
    and I'm working on this I'm trying to reproduce GPTs and I mean I think something
    like chat GPT I think incrementally improved in a product fashion would be extremely
    interestingã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: I think a lot of people feel it and that's why it went so wide so I think there's
    something like a Google plus plus plus to build that I think is very interestingã€‚Do
    we diversity run thoughtï¼Ÿ
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0ab12f5375bf7d79cfc3dbecdf3007c_13.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
