- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P18：18.UnitedNeuroscience-Inspired Artificial
    Intelligence - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P18：18.源于联合神经科学的人工智能 - life_code - BV1X84y1Q7wV
- en: '![](img/2cf4720874569984260c34088d0a5835_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_0.png)'
- en: '![](img/2cf4720874569984260c34088d0a5835_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_1.png)'
- en: Hu， it's fun to be here。So the work I'm presenting today。title of it is attentiontention
    Appimate Sports Distribut Memory。and this was done in collaboration with Drrens
    Peavvan and my PhD advisor， Gabriel Kma。So why should you care about this work
    we show that the heuristic attention operation can be implemented with simple
    properties of highdimenional vectors in a biologically plausible fashion。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，能在这里真有趣。所以我今天要展示的工作标题是“注意力应用体育分布记忆”。这是与Drrens Peavvan和我的博士生导师Gabriel Kma合作完成的。那么，为什么你应该关心这项工作呢？我们展示了启发式注意力操作可以以生物合理的方式，利用高维向量的简单属性来实现。
- en: so the transformer and attention as you know are incredibly powerful but they
    were heuristically developed and the softmax operation and attention is particularly
    important but also heuristic and so we show that the intersection of hyperspheres
    that is used in sports student memory closely approximates the softmax and attention
    more broadly both in theory and with some experiments on train transformers so
    you can see SCM is supposed to read memory as preempting attention by approximately
    30 years who developed back in 1988 and what's exciting about this is that it
    meets a high bar for biological plausibility hopefully I have time to actually
    get into the wiring of the cerebollum and how you can map each operation to part
    of the circuit there。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，正如你所知，变压器和注意力机制是极其强大的，但它们是通过启发式方法开发的，softmax 操作和注意力机制尤其重要，但同样是启发式的，因此我们展示了用于运动学生记忆的超球体交集在理论上和一些训练变压器的实验中，近似地反映了
    softmax 和注意力机制。因此，你可以看到 SCM 应该在大约 30 年前通过预先注意的方式读取记忆，而这项技术是在 1988 年开发的。这令人兴奋的是，它满足了生物学上的高可信度标准。希望我有时间实际深入到小脑的连接结构，以及如何将每个操作映射到电路的某个部分。
- en: So first I'm going to give you an overview of Sprse distributedive memory。then
    I have a transformer attention summary， but I assume that you guys already know
    all of that。we can like get there and then decide how deep we want to go into
    it。I'll then talk about how actually attention approximates STM interpret the
    transformer more broadly and then hopefully there's time to go into STM's biological
    possibility。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我将为您概述Sprse分布式记忆。然后，我会提供一个变换器注意力的总结，但我假设你们已经了解这些内容。我们可以先到那里，然后决定想深入探讨多少。我将讨论注意力如何实际近似STM，并更广泛地解释变换器，然后希望有时间探讨STM的生物学可能性。
- en: Also I'm going to like keep everything high level visual intuition and then
    go into the math。but stop me and please ask questions literally whatever okay。so
    sp distributed memory is motivated by the question of how the brain can read and
    write memories in order to later retrieve the correct one and some considerations
    that it takes into account or high memory capacity。robustness to query noise，
    biological possibility and some notion of fault tolerance。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我将保持所有内容在高层次的视觉直觉上，然后再深入到数学部分。不过请随时打断我，问任何问题，真的没问题。好的，所以分布式存储的动机是基于一个问题：大脑如何读取和写入记忆，以便稍后能够检索到正确的记忆。它考虑的一些因素包括高记忆容量、对查询噪声的鲁棒性、生物学上的可能性，以及某种容错的概念。
- en: SDM is unique from other associated memory models that you may be familiar with
    like hop field networks ins so much as is it's sparse so it operates in a very
    highdiional vector space and the neurons that exist in this space only occupy
    a very small portion of possible locations it's also distributed so all read and
    write operations apply to all nearby neurons it is actually as a side note hot
    field networks if you're familiar with them are a special case of sparse distributed
    memory I'm not going to go deep into that now but I have a blog post on it okay
    so first we're going to look at the right operation for sparse distributed memory
    we're in this high dimensionional binary vector space we're using cannon distance
    as our metric for now we'll move to continue with later and we have this green
    pattern which is represented by the solid and the hollow circles are the hypothetical
    neuro。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: SDM与您可能熟悉的其他关联记忆模型（如Hopfield网络）独特之处在于，它是稀疏的，因此在一个非常高维的向量空间中操作，并且存在于这个空间中的神经元仅占据了可能位置的很小一部分。它也是分布式的，因此所有的读写操作适用于所有附近的神经元。顺便提一下，如果您熟悉Hopfield网络，它实际上是稀疏分布记忆的一种特殊情况。我现在不打算深入探讨这个，但我有一篇关于它的博客文章。好的，首先我们将查看稀疏分布记忆的写操作。我们处在这个高维二进制向量空间中，目前使用的是Cannon距离作为我们的度量，稍后我们将继续讨论。我们有这个绿色模式，由实心和空心圆圈表示，这是假设的神经元。
- en: Also think of everything like quite abstractly and then we'll map the biology
    later so this pattern has a red radius。which is some heming distance， it activates
    all of the neurons within that heming distance。And then it here I just note that
    each of those neurons are now storing that green pattern and the green pattern
    has disappeared。so I'm keeping track of this location with this kind of fuzzy
    hollowicle that'll be relevant later。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 也要将一切抽象地考虑，然后我们稍后会映射生物学，所以这个模式有一个红色半径。这是某种汉明距离，它激活了该汉明距离内的所有神经元。然后在这里，我只是指出这些神经元现在存储着那个绿色模式，而绿色模式已经消失。所以我用这种模糊的空心小球来跟踪这个位置，这将在后面变得相关。
- en: So we're writing in another pattern， this orange one。And note here that neurons
    can store multiple patterns inside of them and formally this is actually a superposition
    versus a summation of these highmenal vectors because they' highmennstrulo you
    don't have that crosswalk so you can get away with that。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们在另一种模式下写作，这个橙色的。并且请注意，神经元可以在内部存储多个模式，从形式上讲，这实际上是这些高维向量的叠加，而不是求和，因为它们的高维结构使得你不需要那种交叉步道，所以你可以这样处理。
- en: but for now you you may just think that it like as a neuron can store multiple
    patterns。嗯。Finally。we have a third pattern， it's blue one， we're writing in in
    another location。And yeah， so again。you can we're keeping track of the original
    pattern locations and they can be。but they can be triangulated from the nearby
    neurons that are storing them。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在你可以认为它就像一个神经元，可以存储多个模式。嗯。最后，我们有第三个模式，它是蓝色的，我们在另一个位置写入。是的，所以再次强调，我们在跟踪原始模式的位置，它们可以是，但它们可以通过存储它们的邻近神经元进行三角定位。
- en: And so we've written in three patterns now we want to read from the system so
    I this pink star eye。it appears it has given it's represented by a given vector
    which has a bit of location of space it activates neuro neuro again。but now the
    neurons output the patterns that they stored previously and so you can see that
    based upon dis location it's getting four blue patterns two orange and one green
    and it then does a majority little operation where it updates towards whatever
    pattern it's seen from so in this case because blue is actually a majority it's
    just been to update completely towards blue again I'll formalize this more in
    a bit but this is really to give you intuition for the core operations of SCM。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们已经写了三种模式，现在我们想从系统中读取。我这颗粉色星星的眼睛。它似乎是由一个给定的向量表示，该向量在空间中的某个位置激活了神经元。但现在神经元输出它们之前存储的模式，因此你可以看到基于这个位置，它得到了四个蓝色模式、两个橙色和一个绿色。然后它执行一个简单的多数运算，根据它所看到的模式进行更新。所以在这个例子中，由于蓝色实际上是多数，它将完全更新为蓝色。我稍后会对此进行更详细的
    formalize，但这真的只是为了让你直观了解 SCM 的核心操作。
- en: So the key thing to relate this back to attention is actually to abstract away
    the neurons that are operating under the hood and just consider these circle intersection
    and so what each of these intersections between the pink lead circle and each
    of the right circle means is the intersection is the neurons that both store that
    pattern and was written in and are now being read from by the query。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，将这与注意力相关联的关键在于抽象掉在后台运作的神经元，只需考虑这些圆的交集。因此，粉色主圆与每个右侧圆之间的交集意味着，这个交集是那些既存储该模式又被查询读取的神经元。
- en: And the size of that intersection corresponds to how many patterns the query
    is then going to read。And so formally， we define the number of neurons in this
    circle intersection as the coinality between number of neurons in pattern。number
    of neurons in query and their intersection。Okay。are there any questions like at
    a high level before I get more into the math？
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这个交集的大小对应于查询将要读取多少个模式。因此，我们正式定义这个圆形交集中的神经元数量为模式中神经元数量、查询中神经元数量及其交集之间的共性。好的，在我深入数学之前，有没有什么高层次的问题？
- en: I don't know if I can check is it easy for me to check zoom na sorry zoom people
    i'm not going to check okay is randomly distributed yes yeah yeah and there's
    later there's more recent work that they can learn and update their location tile
    manifold but in this you can assume that they're randomly initialized binary high
    dimensional vectors okay so this is the full SDM update role I'm going to break
    it down。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道我能否检查一下，检查 Zoom 对我来说是否容易，抱歉，Zoom 的人，我不打算检查，好吗？是随机分布的，是的，是的，之后还有更近期的工作，他们可以学习并更新他们的位置瓷砖流形，但在这里你可以假设它们是随机初始化的二进制高维向量，好吗？所以这是完整的
    SDM 更新规则，我将把它分解。
- en: So the first thing that you do so this is this is for reading to be clear so
    you've already written patterns into your neurons so the first thing you do is
    you weight each pattern by the size of its circle intersection so。The circle intersection
    there for each pattern。Then you sum over all of the patterns。😡。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你要清楚这是为了阅读，所以你已经将模式写入了你的神经元中。你做的第一件事是根据每个模式的圆形交集的大小来加权每个模式。每个模式都有一个圆形交集。然后，你对所有模式进行求和。😡
- en: That have been written into this space， so you're just doing a weighted summation
    of them。And then there's this normalization by the total number of intersections
    that you have。And finally。because at least for now we're working in this binary
    space， you map back to binary。just seeing if the values are greater than a half。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些已经写入这个空间的内容，你只是对它们进行加权求和。然后通过你所拥有的交集总数进行归一化。最后，因为至少现在我们在这个二进制空间中，你会映射回二进制，仅仅是查看这些值是否大于一半。
- en: '![](img/2cf4720874569984260c34088d0a5835_3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_3.png)'
- en: Okay， i'm how familiar are people with attention I looked at like the previous
    talks you had they seem quite high level like can you guys write the attention
    equation for me is that like。Can I get thumbs up if you can do that？Yeah okay
    i'm not like i'll go through this but i'll probably go through it faster than
    otherwise so when I first made this presentation like this was the state of the
    art for transformers which is like Al and so it's kind of funny like how far things
    have come now I don't need to tell you that transformers are important。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我想知道人们对注意力机制有多熟悉。我看了你们之前的讲座，似乎水平很高。你们能为我写出注意力方程吗？如果可以，请给个赞。是的，好吧，我不会详细讲解，但我可能会比其他时候更快地讲。当我第一次做这个演示时，这就是变压器的最新技术，现在的进展真是有趣，我不需要告诉你们变压器有多重要。
- en: '![](img/2cf4720874569984260c34088d0a5835_5.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_5.png)'
- en: So。Yeah I'm going to work with this example well well okay I'm going to work
    with this example here the cat sat on the blank and so we're in this setting we're
    predicting the next token which the word math and so there are kind of four things
    that the attention operation is doing the first one up here is it's ex generatingrating
    whatre called keys values and queries and again I'll get over the map in a second
    I'm just trying to keep it high level first and then we're going to compare our
    query with each of the keys so the word the which is closest to the word we're
    next predicting is is our query and we're seeing how similar it is each of the
    key vectors。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以。是的，我将使用这个例子，好吧，我将使用这个例子，这里是猫坐在空白处，因此在这个环境中，我们正在预测下一个令牌，即单词“数学”。因此，注意力操作有四个主要功能，首先是在这里，它生成所谓的键、值和查询，再次，我稍后会详细解释这个过程，我只是想先保持高层次的理解，然后我们将比较我们的查询与每个键。因此，单词“the”，即我们下一个预测的单词，作为我们的查询，我们正在查看它与每个键向量的相似度。
- en: We then， based upon that similarity， do this softmax normalization so that all
    of the attention weights sum to one。and then we sum together their value vectors
    to use to propagate to like the next layer or uses our prediction。And so at a
    high level you can think of this as like the query word the is looking for nouns
    and their associated verbs and so hypothetically it has a high similarity with
    words like cat and sat or their keys so this then gives large weight to the cat
    and sat value vectors which get moved to the next part of the network and the
    cat value vector hypothetically contains a superposition of other animals like
    mice and maybe words that rhyme with mat and so and the sat vector also contains
    things that are sat on including mat and so what you actually get from the value
    vectors of paying attention to cat and sat are like three times mat plus one times
    mouse plus one times sofa this is again like a totally hypothetical example but
    I'm trying to make the point that you can extract from your value vectors things
    useful for predicting the next token by paying attention。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们基于这种相似性进行softmax归一化，以便所有的注意力权重加起来为一。然后，我们将它们的值向量相加，用于传播到下一个层或用于我们的预测。因此，从高层次来看，你可以认为这个查询词正在寻找名词及其相关动词，并且假设它与像“cat”和“sat”这样的词有很高的相似性，或者它们的键。因此，这会给予“cat”和“sat”值向量较大的权重，这些权重会被传递到网络的下一个部分，而“cat”值向量假设上包含了其他动物（如老鼠）以及可能与“mat”押韵的词，而“sat”向量也包含了包括“mat”在内的坐着的东西。因此，从关注“cat”和“sat”的值向量中，你实际上得到的是类似于三倍的“mat”加上一倍的“mouse”加上一倍的“sofa”，这再次是一个完全假设的例子，但我想强调的是，通过关注，你可以从你的值向量中提取出对预测下一个标记有用的东西。
- en: So specific keys。嗯。对。So and I guess yeah another thing here is like what you
    pay attention to some cat and sat might be different from what you're actually
    extracting you're paying attention to your keys but you're getting your value
    vectors out Okay。so here is the full attention equation the top line I'm separating
    out the projector matrices W subscript you pay a Q and the second when I just
    collapse them into like the new OAC and。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所以特定的键。嗯。对。所以我想另一个要点是，你所关注的某些内容和实际提取的内容可能不同，你关注的是你的键，但你得到的是你的值向量。好的。这里是完整的注意力方程，上面的行我把投影矩阵W下标分开，你支付Q，第二行我只是把它们合并成新的OAC。
- en: Yeah so breaking this apart the first step here is we compare we do a dot product
    between our query vector and our keys this should actually be a small human capital
    and so yeah we're doing this dot product between them to see get a notion of similarity
    we then apply the Somax operation which is an exponential over sum of exponentials
    the way to think of the Som is it just makes large values larger and this will
    be important for the relation to SE and so I'll spend a minute on it at the top
    here I have like some hyper items index from zero to nine and then the like values
    for each of items in the second row I just do like a normal normalization of them
    and so the top item goes to 30% value but if I instead do a Somax and it depends
    on a beta co definition and the Somax but the value becomes 0。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以将其拆分，第一步是我们比较我们的查询向量和我们的键之间进行点积，这实际上应该是一个小的人力资本，所以我们在它们之间进行这个点积，以获取相似度的概念。接着我们应用Somax操作，这是一个指数和指数和的操作，思考Som的方式是它只是让大的值变得更大，这在与SE的关系中将是重要的，因此我在这里花一点时间。在顶部，我有一些超项，索引从零到九，然后在第二行中是每个项的值，我只是对它们进行正常化，所以顶部的项变为30%的值，但如果我改为使用Somax，这取决于beta的共定义和Somax，但值变为0。
- en: 6 so it。Your distributions peak here is kind of one way of thinking of it and
    this is useful for attention because you only want to pay attention to the most
    important things or the things that are nearby and kind of ignore stuff further
    away。And so once we've applied our softm， we then。Just do a weighted summation
    of our value vector。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里你的分布峰值可以看作是一种思维方式，这对注意力是有用的，因为你只想关注最重要的事物或附近的事物，忽略那些更远的东西。因此，一旦我们应用了我们的softm，我们就只需对我们的值向量进行加权求和。
- en: Which actually get extracted and propagate to the next layer。嗯。Okay， so here's
    the。The full equation。I went through that a little bit quickly I'm happy to answer
    questions on it。but I think half of you do it， half of you don't。Okay。So how does
    transformer attention approximate Sprse distributed memory。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上提取并传播到下一层。嗯。好的，那么这是。完整的方程。我稍微快速地讲了一下，乐意回答你们的问题。可是我觉得你们一半会，一半不会。好的。那么，变压器注意力是如何近似稀疏分布式记忆的呢？
- en: this 30 year old thing that I've said is biologically plausible。So yeah。A like
    accept that likely so I'm going to get to that at the end yeah。attention is also
    like in the sense of all attention not was。😊。I think the attention equation I'm
    showing here was developed。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这件我提到的三十年前的事情在生物上是可信的。所以，是的。我接受这种可能性，所以我会在最后提到这一点。注意力也是在所有注意力的意义上，而不是。😊。我认为我在这里展示的注意力公式是经过开发的。
- en: I mean attention to all you need was the highlight， but Benjiio has a paper
    from 2015。Where it was actually first written in this way。Correct me if I'm wrong，
    but I'm pretty sure yeah。I mean， I guess like this particular one that's why I
    was asking the question because like no。it's a good question like you show that
    like two。Different methods that could be classified as like attention proposals
    right are like the same than like himself shoulder。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，你需要关注的只是重点，但Benjiio在2015年发表了一篇论文。那是首次以这种方式写的。如果我错了，请纠正我，但我很确定。我的意思是，我想这特别的一个，这就是我问这个问题的原因，因为不。这是个好问题，你展示了两种可以被归类为注意力提案的不同方法，它们是相同的，就像他自己肩负的那样。
- en: One of them that like。Yes， exactly so I'll show that SDM has really nice mappings
    to a circuit a cebolumm at the neuronal level and then theres right now it's this
    link to attention and I guess you make a good point that there are other attention
    mechanisms this is the one that has been dominant but I don't think that's just
    a coincidence like there's been a bunch of computed your Somax is expensive and
    there's been a bunch of work like the Lformer etc cea et cea that tries to get
    rid of the Somax operation and it's just done really badly like there's a bunch
    of jokes on Twitter now that it's like a black hole for people that like try and
    get rid of Somax and you can't and so it seems like this and like other versions
    of a transformers just don't scale as well in the same way and so there's something
    important about this particular attention equation。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个喜欢。是的，正是这样，我将展示SDM在神经层面与电路之间的良好映射，然后现在它与注意力之间有联系，我想你提到的其他注意机制是个好观点，这是主导的机制，但我认为这并非巧合；像计算的Somax成本很高，并且还有很多工作，比如Lformer等，试图摆脱Somax操作，但效果都很差，现在Twitter上有很多玩笑，称试图摆脱Somax的人就像掉进了黑洞，所以看起来这些和其他版本的变压器在同样的方式上不够扩展，因此这个特定的注意力方程式显得很重要。
- en: But like then goes the other way， right， which is like， if this is really important。then
    like SDM is like actually like miss。U。So the thing that I think is important is
    that you have this exponential weighting。Where you're really paying attention
    to the things that matter and you're ignoring everything else and that is what
    SDM approximates there might be equations。but the point I was just trying to make
    there is like the Soax does seem to be important and this equation does seem to
    be very successful and we haven't come up with better formulations for it。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但是就像之前那样往回说，如果这真的很重要，那么SDM实际上是错过了你。因此，我认为重要的是你有这种指数加权。你真的在关注那些重要的事物，而忽略其他一切，这正是SDM所近似的，可能会有方程式，但我想说的是，Soax似乎确实很重要，而这个方程式似乎非常成功，我们还没有提出更好的公式。
- en: Yeah no it's a great question okay， so it turns out that sparse distributed
    memory as you move your query and your pattern away from each other so you pull
    these circles apart the read and bright circles the number of neurons that are
    in this intersection and a sufficiently high dimensional space decays approximately
    exponentially and so on this right plot here I'm pulling apart the X axis is me
    pulling apart the blue and the pink circles and the y axis is on a log scale the
    number of neurons that are in the intersection and so to the extent that this
    is the linear plot on a long scale it's exponential。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这个问题很好，所以事实证明，稀疏分布记忆在你将查询和模式分开时，就像你把这些圆圈拉开一样，读取和亮起的圆圈之间的交集中的神经元数量在一个足够高维的空间中大约是指数衰减的。在这个右侧图中，我把蓝色和粉色圆圈拉开，X
    轴是我拉开的过程，Y 轴是交集中神经元数量的对数尺度，因此在对数尺度上这是线性图，它是指数的。
- en: And this is for a particular setting where I have my I have 64 dimension meal
    vectors。which is like used in D2， it holds across a lot of different settings。particularly
    higher dimensions， which are now used for bigger transformers。Okay。so I have this
    shorthand for the circle intersection equation and。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用于特定的设置，我有64维的餐向量。这就像在D2中使用的一样，它适用于许多不同的设置，特别是更高维度，现在用于更大的变压器。好的。所以我有这个圆交叉方程的简写。
- en: What I'll show is how the circle intersection is approximately exponential so
    we can write it with two constant C sub one and sub of two with the the one outside
    because you're normalizing softmax is exponential over some exponentials that
    will cancel the thing that matters the C2 and you can approximate that nicely
    with the beta coefficient that's used in the softmax。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我将展示的是圆的交集大约是指数级的，因此我们可以用两个常数C1和C2来表示，外面的那个因为你在归一化softmax时是对某些指数进行指数运算，这将抵消重要的部分C2，而你可以很好地用在softmax中使用的beta系数来近似它。
- en: And so yeah， I guess as well， I'll focus first on the binary original version
    of SEM。but then we also develop a continuous version。Okay， so yeah。the two things
    that you need for this circle intersection and the exponential decay to work are
    you need to map to attention is you need some notion of continuous space and so
    you can use this equation here to map hem distances to disretize proine similarity
    values where the hat server the vectors are L2 normalizations。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以是的，我想我会首先关注SEM的二进制原始版本，但我们也会开发一个连续版本。好的，所以是的。使这个圆形交集和指数衰减有效的两个因素是你需要映射到注意力，即你需要某种连续空间的概念，因此你可以使用这里的这个方程将距离映射到离散的相似性值，其中的帽子表示向量是L2标准化。
- en: And you can then write the circle intersection equation on the left。As this
    exponential with these two concepts that you need to learn and then rewrite this
    by converting C2 and see。you can write this as the beta coefficient。Let me get
    to some plots yeah so you need the correct coefficient but you can fit this with
    a log linear regression and a closed form。啊。I want to show a plot here。Yeah okay，
    so in the blue is our circle intersection for two different he distances both
    using 64 dimension vectors and the orange is our actual stockax attention operation
    where we fit the beta coefficient that it will it the hem distance used by attention
    is equivalent to the hem distance used by SEM and you can see so that the main
    plot is the normalized weights so just summed up in a divide to one and that I
    log plots here and you can see that in not loggged space the curves agree quite
    nicely you can see that for the higher dimensional sorry the larger he distance。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以在左侧写出圆的交集方程。这个指数与这两个你需要学习的概念有关，然后通过转换 C2 来重写它，看看。你可以将其写成贝塔系数。让我来展示一些图，是的，你需要正确的系数，但你可以用对数线性回归和闭合形式来拟合它。啊。我想在这里展示一张图。是的，好吧，蓝色是我们在两个不同的
    he 距离下的圆交集，都是使用 64 维向量，而橙色是我们的实际 StockAx 注意力操作，其中我们拟合的贝塔系数表明，注意力使用的 hem 距离等于 SEM
    使用的 hem 距离。你可以看到，主要图是归一化权重，简单地相加后除以 1，而这里是对数图，你可以看到在未对数化的空间中，曲线相当吻合，你可以看到对于较高维度，抱歉，更大的
    he 距离。
- en: the log plot you see this drop off here the circle intersection stops being
    exponential but it turns out this actually isn't a problem because the point at
    which the drop the exponential incorporates down you're approximately。20 here
    and you're basically paying negligible attention to any of those points and so
    in the regime where the exponential rule matters this approximation holds true。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到的对数图在这里出现下降，圆形交点不再呈指数形，但实际上这并不是问题，因为下降的点大约在**20**这里，你基本上对任何那些点的关注都微不足道，因此在指数法则重要的范围内，这个近似是成立的。
- en: 对对，对刚刚就说这是感觉。Yeah， yeah yeah， no， I just wanted to actually like show up figure
    to get some inition before yeah。So all we're doing here is we're just， we're in
    a binary space with original EM and we're just using this mapping thing to cosine
    similarity。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对对，对刚刚就说这是感觉。是的，没错，我只是想实际展示一下，想在这里获得一些基础知识。所以我们在这里做的就是，我们处在一个原始的EM二进制空间，利用这个映射来计算余弦相似度。
- en: And then what you need to do is just have the beta coefficient and then you
    can view your beta coefficient and attention as determining how PP things are。and
    this relates directly to the he distance of your circles that you're using for
    Read&W write on Washington。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你需要做的就是拥有贝塔系数，然后你可以将你的贝塔系数和注意力视为决定PP事物的程度。这与您在华盛顿进行阅读和写作时使用的圆圈的距离直接相关。
- en: And so yeah， to like mathematically show this now on this slide I'm not using
    any tricks。I'm just rewriting attention using the SEM notation of patterns and
    queries。So this little box down here is doing that nothing。And this is the money
    slide where we're updating our query。And on the left。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，是的，要在这个幻灯片上以数学方式展示这一点，我没有使用任何技巧。我只是用模式和查询的SEM符号重写了注意力。因此，这个小框框并没有做任何事情。而这是关键幻灯片，我们在这里更新我们的查询。左侧的内容。
- en: we have our attention equation written in SE notation， we expand our subax。And
    then the main statement is that this is closely approximated by if we swap out
    our exponential with the SMM for corner century equation。Yeah。So and again， the
    two things that you need for this to work are one， your attention vectors。your
    keys and queries will to be l to normalized slide on hats on them and then。You
    want。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的注意力方程用SE符号表示，我们扩展了子轴。然后主要的陈述是，如果我们用角落世纪方程的SMM替换掉指数，这个方程是紧密近似的。是的。因此，为了使这有效，你需要的两个东西是：第一，你的注意力向量。你的键和值需要进行归一化处理，然后。你想要。
- en: If you decided given he distance for SEM and I'll get into what he distance
    are good for different things。then you need to have a beta coefficient that relates
    to it。But again。that's just how many things are you trying to pay attention to？So
    yeah， just as a quick side note。you can write SDM using continuous vectors and
    then not need this mapping to concerned similarity。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你决定了SEM的距离，那么我会深入探讨这些距离适用于不同的事物。然后你需要有一个与之相关的beta系数。但再次强调，这到底是你想关注多少个事物？所以，是的，作为一个简短的旁注，你可以使用连续向量编写SDM，然后不需要这个映射来关注相似性。
- en: And so here I have the plots again， but with this and。I added the。The orange
    of the green have flippeds， but I've added the continuesnus across the here too。And
    what's nice about the continuous version is you can actually then write Srse distributed
    memory as a multilayered conceptualron with slightly different assumptions。and
    I'm not going to talk about that now， but this is featured in Spe distributed
    memory as a continual learner。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里我再次展示了这些图表，但添加了这个。我增加了这个。绿色的橙色已经翻转，但我在这里也增加了连续性。而连续版本的好处在于你实际上可以将**Srse**分布式内存写成一个多层次的概念框架，假设略有不同。我现在不打算讨论这个，但这是**Spe**分布式内存作为持续学习者的一个特点。
- en: which is was added to the additional readings and' be in sorry this shouldn't
    say ICML。this should say ICl。It's just been accepted to declare for this year。Okay。so
    do train transformers use these beta coefficients that I've said are similar to
    those for SDI and so。It shouldn't be surprising that depending on the how distance
    you set STM as better for certain things。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这被添加到了附加阅读材料中，抱歉，这里不应该说ICML，而是应该说ICl。它刚刚被接受为今年的声明。好的。那么训练变换器是否使用我提到的类似于SDI的这些贝塔系数呢？根据你设置STM的距离，对于某些事情来说，效果更好，这并不令人惊讶。
- en: for example， you just want to store as many memories as possible and you're
    assuming that your queries aren't noisy or you're assuming your queries are really
    noisy so you can't store as much but you can retrieve from a long distance and
    if attention of the transformers implementing things prior distributed memory。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你只是想尽可能多地存储记忆，而你假设你的查询不是嘈杂的，或者你假设你的查询确实很嘈杂，因此你无法存储太多，但你可以从远处检索，并且如果注意力机制的变换器在实现先前的分布式记忆时。
- en: we should expect to see that the beta coefficients that the transformer uses
    correspond to these good instances of SDM and so we have some weak evidence that
    that's the case。so this is the key query normalized variant of attention where
    you actually learn your beta coefficient。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该预期看到变换器使用的 beta 系数与这些良好的 SDM 实例相对应，因此我们有一些微弱的证据表明情况确实如此。这是注意力的关键查询归一化变体，在这里你实际上可以学习到你的
    beta 系数。
- en: normally in transformers you don't but you don't L to longer your vectors and
    so you can kind of have this like effective beta coefficient so in this case it'
    just a cleaner instance where we're actually learning beta and this was trend
    on a number of different translation pass we take the learn beta coefficients
    across layers across。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在变压器中，你不会这样做，但你不需要将向量延长，因此你可以有这样一个有效的 beta 系数。在这种情况下，这只是一个更清晰的实例，我们实际上是在学习
    beta，而这是在多个不同的翻译过程中进行的，我们跨层学习 beta 系数。
- en: And plot as a histogram and the red bloodted line correspond corresponds to
    three different notions of spark distributed memory that are optimal for different
    things。And again， this is weak evidence insomuch as to derive the optimal SM beta
    coefficients or corresponding handling distances。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 并将其绘制为直方图，红色线条对应于三种不同的火花分布式内存概念，适用于不同的情况。再次强调，这是微弱的证据，因为无法推导出最佳的SM beta系数或相应的处理距离。
- en: we need to assume random patterns in this high dimension space and like obviously
    real world data isnt random。however it is nice to see one all of the datata coefficients
    fall within the bound and two they skew towards the max query noise which makes
    more sense if you're dealing with like complicated real world data where the next
    data points you see might be out of distribution theyve seen the past。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要假设在这个高维空间中存在随机模式，显然现实世界的数据并不是随机的。然而，看到所有数据系数都在范围内是很不错的，第二，它们偏向于最大查询噪声，这在处理复杂的现实世界数据时更有意义，因为你接下来看到的数据点可能会超出它们过去所见的分布。
- en: the maximum memory capacity variant assumes no very noise at all。and so it's
    like how many things can I pack in assuming that the questions I'm asking the
    system are perfectly formed。Okay。Just talking a little bit about transform components
    more broadly。So I've mentioned that you can write the feed forward layer as a
    version of SDM that has like a sort of notion of longer term memory。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最大内存容量变体假设没有任何噪声。因此，就像我可以假设在提问时系统的问题是完美形成的，我能放入多少东西。好的。稍微谈谈变换组件更广泛的内容。我提到过，你可以将前馈层写成一种具有某种长期记忆概念的SDM版本。
- en: 嗯。There's also layer norm which is crucial in transformers and it's not quite
    the same that it can be related to the alTitude normalization that's required
    by SDM there's also the key query normalization variant that explicitly does this
    altU normalization and it does get slightly better performance at least on the
    small test that they did I don't know if this would scale to larger models。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。还有层归一化，这在变压器中至关重要，和SDM所需的高度归一化不完全相同，还有一种关键查询归一化变体，它明确地进行这种高度归一化，至少在他们进行的小测试中，它确实获得了稍微更好的性能，我不知道这是否适用于更大的模型。
- en: And so I guess this work is interesting in so much as like the biological plaibility
    which I'm about to get to and then the links to transformers。it hasn't to date
    improved transformer architectures。but that doesn't mean that this lens couldn't
    be used or be useful in some way。Um， so yeah。I list a few other way things that
    STM is related to that could be used to funnel in and actually in the the new
    work where STM is continual learner。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想这项工作之所以有趣，是因为我即将提到的生物可行性，以及与变压器的联系。到目前为止，它并没有改善变压器架构，但这并不意味着这个视角不能以某种方式被使用或有用。嗯，是的。我列出了几个与STM相关的其他方面，这些方面可以用来引导进入，实际上在新的工作中，STM是一个持续学习者。
- en: we kind of expand the cerebellar circuit， look at components of it。particularly
    inhibitory enter neurons。Implement those in a deep learning model and it then
    becomes much better at continual learning so that was kind of a fun way of actually
    using this link to get better bottom line performance。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们扩展了小脑电路，查看其组成部分，特别是抑制性中间神经元。将这些实现到深度学习模型中，它在持续学习方面表现得更好，因此这是一种有趣的方式，实际上利用这个联系来提升最终的性能。
- en: By。Okay， so a summary of this section is basically just the intersection between
    two hyperspheres approximates an exponential and this allows FM's read and write
    operations to approximate attention both in theory and our limited tests and so
    kind of like big picture research questions that could come out of this is first
    is the transformer so successful because it's performing some key cognitive operation。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这一节的总结基本上就是两个超球体的交集近似于指数，这使得FM的读写操作在理论上和我们的有限测试中都能近似于注意力，因此，从大局来看，可能会出现的一些研究问题是，变压器之所以成功是否因为它执行了一些关键的认知操作。
- en: the cerebellum is a very old brain region used by most organisms including fruit
    flies maybe even cephalopods through like divergent but now convergent evolution
    and then given that the transformers been so successful empirically is SDM actually
    the correct theory for cerebellar function and that's still an open question。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 小脑是一个非常古老的脑区，大多数生物包括果蝇，甚至可能还有头足类动物都在使用它，这一过程经历了发散但现在趋同的进化。考虑到变压器在经验上如此成功，SDM究竟是否是小脑功能的正确理论，这仍然是一个悬而未决的问题。
- en: As we learn more and more about the cerebellum there's nothing that yet disproves
    SDM as working there and I think it's I'll go on a limb and say it's like one
    of the more compelling theories for how the cerebelin is actually working。Yeah，
    and so I think this this work kind of motivates looking at more of these questions。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们对小脑了解的越来越多，目前没有任何证据否定SDM在这里的作用，我认为我可以大胆地说，这是一种更具说服力的理论，解释小脑是如何实际工作的。是的，我认为这项研究激励我们去探讨更多这样的问题。
- en: both of these questions more seriously。嗯 okay。Do we have time？
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题都更加严肃。嗯，好吧。我们有时间吗？
- en: Cool so here's the circuit that implements SDM at the bottom we have patterns
    coming in for either reading or writing。And I actually， I breakdown down of these
    slides。Okay， yeah。so so first we have patterns that come in and every neuron here。these
    are the dendrites of each neuron。And they're deciding whether or not they're going
    to fire for the input that comes in。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 很酷，这是实现SDM的电路，底部有用于读取或写入的模式。实际上，我对这些幻灯片进行了分析。好的，是的。所以首先我们有输入的模式，每个神经元在这里。这些是每个神经元的树突。它们正在决定是否对输入信号进行放电。
- en: Then if the neuron does fire and you're writing in that pattern。😡，Then you simultaneously。and
    I'm going to explain let' you here that this is crazy the brain doesn't do this
    and then I'm going to hopefully trigger you not only need to have the thing that
    the pattern activates neurons。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，如果神经元确实发火，而你以那种模式在写。😡，那么你同时。我要在这里解释，这真是疯狂，大脑并不这样做，然后我希望能触发你，不仅需要让模式激活神经元。
- en: but you need to have a separate line that tells the neuron what to store。And
    just like you have this difference between keys and values where they can be different
    vectors representing different things here you can have a key that comes in and
    tells the neuron when to activate and the value for one it should actually like
    soar and then put layer。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但是你需要有一个单独的行来告诉神经元该存储什么。就像你在键和值之间存在差异一样，这里它们可以是表示不同事物的不同向量，你可以有一个输入的键来告诉神经元何时激活，而对应的值则是它实际上应该像是飞翔一样，然后放入层。
- en: This is called a heteroasso mapping。And then once you're reading from the system。You
    also have your query come in here， activate neurons。and those neurons then output
    whatever they store and the neurons vector is this a particular column that it's
    stored and's again as a reminder。it stored patterns in superconion and then it
    will dump whatever it's stored across these output lines。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为异构关联映射。然后，一旦你从系统中读取数据。你的查询也会在这里到达，激活神经元。然后这些神经元输出它们存储的内容，而神经元向量是存储的特定列，作为提醒。它在超连接中存储了模式，然后会将它存储的内容通过这些输出线路转储。
- en: And then you have this G majority bit operation to convert to a zero one。decide
    if the neuro is going to fire or not。And so。Here is the same circuit。but where
    I overlay cell types and the ce。And so。I'll come back to this slide because most
    people probably aren't familiar with Sarahbeller circuitry。It's in water okay，
    so the way that the cerebellum is pretty homogeneous and it follows this pattern
    throughout also cl back 70% of all neurons in the brainno and cerebellum they're
    small so you wouldn't know it but the cerebellum is like very underappreciated
    and there's abundant evidence that has a closed loop systems with most higher
    order by processing now a yourcerebellums damage you are more like autism et cetera。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你有这个G主导位操作来转换为零一。决定神经元是否会发放信号。因此。这里是相同的电路，但我叠加了细胞类型和ce。因此。我会回到这张幻灯片，因为大多数人可能对Sarahbeller电路不太熟悉。它在水中，所以小脑是相当均匀的，并且整个过程遵循这种模式，此外，大脑和小脑中约有70%的神经元很小，所以你不会知道，但小脑是非常被低估的，并且有大量证据表明它与大多数高级处理具有闭环系统。现在，如果你的小脑受损，你更可能出现自闭症等情况。
- en: et cetera， so it does a lot more than just fine motor coordination which a lot
    of people have like assumed in the past Okay so inputs come in the most fibers
    here they interface with granular cells this is a major projection where you have
    tons and tons of granular cells。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 等等，因此它的功能远不止于许多人过去所认为的精细运动协调。好的，输入通过这里的大部分纤维与颗粒细胞相连接，这是一个主要的投射，那里有大量的颗粒细胞。
- en: Each granial cell has what are called parallel fibers。which is these incredibly
    long and thin aons that branch out in its T structure。And。Then they're hit by
    the perkinji cells which will receive up to 100，000 parallel fiber inputs。it's
    the highest connectivity of any on the brain。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每个颗粒细胞都有称为平行纤维的结构，这些纤维是极其细长的轴突，以T形结构分支。然后，它们会受到普金杰细胞的冲击，这些细胞可以接收多达100,000个平行纤维输入。这是大脑中连接性最高的部分。
- en: and then the perkinji cell will decide whether or not to fire and send its output
    downward here。So that's the whole system where parents come in and ground decide
    they fire or not and the way that they' then output their own。You then have a
    separate right line which is the climbing fiber。so the climbing fibers come up
    and they're pretty amazing in that these connections here you've committed more
    that is important one that really matters is that they're not very strong enough
    one that really matters is it goes up and it wraps around individual oring cells。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，普金基细胞将决定是否发射并向下发送输出。因此，这就是整个系统，父母参与并决定它们是否发射，以及它们输出自己的方式。然后你会有一条独立的右线，即爬升纤维。爬升纤维向上延伸，它们的连接非常惊人，重要的是，它们并不是非常强的，真正重要的是它们向上延伸并缠绕在个别的树突细胞周围。
- en: And the mapping is close to one to one between cl fibers and pukine cells。at
    least a very strong attribute。And so youre connected to us here in the stuff all
    this line yeah right Oh so there's separate neurons coming from in separate areas
    particularly go into deep cerebelella nuclei kind of in the core the cerebonelum
    and that then feeds intous like back to higher order brain regions or like down
    the muscle muscle movement etc a lot of people will think that the cerebelbonelum
    is kind of like a fine tuning lookup table where like you've already decided the
    muscle movement you want to do but the cerebellum will then like do a bunch of
    adjustment adjustment so it's like much more accurate but it seems like this also
    applies to like next word prediction like we have FMRI data for this a neuroscienceist
    one that's made like a dirty little secret of fMRI is that the cerebellum lines
    up for everything So okay。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 而且纤维与普金细胞之间的映射几乎是一一对应的，至少是一个非常强的特性。因此，你在这里与我们连接，所有这一切都是对的。哦，原来有来自不同区域的独立神经元，特别是进入深小脑核，像是在小脑的核心，然后反馈给更高阶的大脑区域，或者像是肌肉运动等等。很多人会认为小脑就像是一个精细调整的查找表，你已经决定了想要的肌肉运动，但小脑会进行一系列调整，使其更加准确。不过，这似乎也适用于下一个词的预测，我们有FMRI数据支持这一点，一个神经科学家说，fMRI的一个小秘密就是小脑与所有事物相一致。所以，好的。
- en: Going back to this circuit here then。Yeah， time scales or the operating Act，
    I mean。how long is the information stored and retrieved？Do we have any idea about
    this like it' just like a couple milliseconds or like there's a information work
    system so the main theory is that you have updating through。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 那么回到这里的电路。是的，时间尺度或操作方式，我是说。信息存储和检索的时间有多长？我们对此有任何了解吗？就像只需几毫秒，还是有一个信息工作系统，所以主要理论是你需要通过更新。
- en: Time dependent plasticity where you' climbing fiber will either which is' doing
    the what you want right in will fire either just before or just after your graile
    cells fire and so that then updates the pro cell sinapses for long-ter progression
    or potentation so whatever times still that's happening on the climbing fiber
    makes very large active potentials or it leads to very large amount when solve
    and so I do think you could get pretty fast synaptic updates and they also persisted
    for a long time I think so the have can say but like the rest of your life yeah。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 时间依赖性塑性，其中你的爬升纤维会在你想要的事情发生之前或之后发放信号，因此这会更新长时程的突触，以促进进展或增强。因此，无论时间如何，爬升纤维上发生的事情都会产生非常大的动作电位，或者导致大量的解决。因此，我确实认为你可以获得相当快速的突触更新，并且这些更新也能持续很长时间，我认为这一点是可以确认的，但就像你生活的其他部分一样。
- en: 是啊。So what's really unique about this circuit is the fact that you have these
    two orthopogonal inputs where you have the losss fibers bringing information in
    to decide if the neurons can fire or not。but then the totally separate quantum
    fiber lines that can update specific neurons and what they're storing and will
    later output and then the protiji cell is so important that's kind of doing this
    cooling across every single neuro and each neuron remember it's storing the vector
    this way。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 是啊。那么这个电路真正独特的地方在于你有这两个正交输入，其中有损失光纤将信息传入，以决定神经元是否可以激发。但是还有完全独立的量子光纤线路，可以更新特定神经元及其存储的内容，之后再输出。然后原型细胞是如此重要，它在每个神经元之间进行冷却，每个神经元记住以这种方式存储向量。
- en: and so the protiji cell is doing elementwise summation and then deciding whether
    it fires or not。and this allows for you to store your vectors in superposition
    and then later dennoize them。哦。This all the theory I see un maps quite well to
    the Mar and all us theories of solar vlor function。which are still quite dominant
    if anyone months from a there months about even those so they analog the neuron
    in the SDM and you introduced before and that might kind basically each neuron
    of thekengeji cell setting of each neuron is a grainno song。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以原始细胞正在进行逐元素求和，然后决定是否发放信号。这使你能够将向量存储在叠加状态中，之后再进行去噪。哦。这一切理论在地图上与Mar和所有关于太阳vlor功能的理论映射得相当好。这些理论仍然非常主导，如果有人从那里了解到这些，甚至那些模拟神经元的SDM和你之前介绍的内容，每个细胞的每个神经元基本上都是一个粒子歌曲的设置。
- en: Okay， and then yeah， so the location of the neuron。those hollow circles corresponds
    to the gra cell dendrs here。Where the patterns that pop in correspond to the activations
    of modifiers and then the effort postynaptic connections are with the per cell。so
    that's actually what it's storing is in the synaptic connections with the N per
    cells。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，然后是神经元的位置。这些空心圆圈对应于这里的胶质细胞树突。出现的模式对应于修饰符的激活，努力的突触连接与每个细胞有关。因此，它实际存储的是与每个细胞的突触连接。
- en: At that interface。And then the prokee cell does the majority bit operation in
    deciding to loss fire or not。yeah， I think we're basically into into question
    time so yeah， thanks a lot。嗯。可可以。嗯啊还有个事。I don't know anything about that being，
    but it seems as understood it' very used for a long term now。And I have curious
    what's your hypothesis whats system？
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个接口上。然后，prokee 单元主要进行决定是否失去火力的位操作。是的，我想我们基本上进入了提问时间，非常感谢。嗯，可以。嗯，还有件事。我对那个存在一无所知，但似乎它已经被使用很长时间了。我很好奇你的假设是什么，系统又是什么？
- en: What we should be doing for short number delivery。Because it seems that。So if
    you have this link transformers。YeahI think long term memory。let's go for a short
    term memory because for me it seems like。We are doing this in the prompt context
    right now， but how could we incorporate these to to directly Yeah yeah。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该做的短期数字交付。因为似乎是这样。如果你有这个链接的变换器。是的，我认为长期记忆。让我们关注短期记忆，因为对我来说似乎是这样。我们现在在提示上下文中这样做，但我们如何能直接将这些融入进来，是的，是的。
- en: so this work actually focuses more on the short term memory where it relates
    to the attention operation but you can rewrite SDM。it's almost more natural to
    interpret it as a multied perceptron。That does like a softax activation across
    its or a top pay activation across its neurons it's like a little bit more complicated
    than that。but。Yeah， so。Yeah， the most interesting thing you hear is the fact that
    like I just have a bunch of neurons。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这项工作实际上更关注短期记忆与注意力操作的关系，但你可以重写SDM。它几乎更自然地被解释为一个多层感知器。它就像在其神经元之间进行一种softax激活，或者一种top
    pay激活，稍微复杂一些。但。是的，所以。是的，你听到的最有趣的事情是，我只有一堆神经元。
- en: And in activating nearby neurons in this high mental space， you get this exponential
    weighting。which is the salt mass and then because it's an associatedso memory
    where like you have keys and values。it is attention。And yeah， I guess like the
    thing I most want to drive home from this is like it's actually surprisingly easy
    for the brain to implement the attention operation。the attention operation， just
    using high dimensionsional vector and activating your back。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个高层次的思维空间中激活附近的神经元时，你会获得这种指数加权。这就是盐质量，然后因为这是一个关联记忆，就像你有键和值。这就是注意力。是的，我想我最想强调的事情是，大脑实际上能够相对容易地实现注意力操作。注意力操作，仅仅使用高维向量并激活你的后部。
- en: So it's good for a short of and money yes， if you were if you were actually
    use SEM for attention。Yeah， so let me go all the way back real quick。This is important。There
    are kind of two ways of viewing SDM and I don't think you were here for the talk。I
    think I saw you come in a bit later， which is totally fine， but Oh cool cool cool
    yeah， yeah。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这对于短期和资金来说是好的，如果你真的想用 SEM 来吸引注意力。是的，让我快速回到最初。这很重要。对 SDM 有两种看法，我不认为你在讨论时在场。我看到你稍晚才进来，这完全没问题，但哦，太好了，太好了。
- en: Okay， so so there are two ways of looking at SDM there's the neuron perspective。Wwhich
    is this one here and this is actually what's going on in the very course and so
    the only thing that is exactly constant is the the。the patterns are error， and
    then there's the pattern based perspective。which is actually what attention is
    doing。And so here you're abstracting away than neurons or assuming you're operating
    under the hood。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，有两种方式来看待SDM，一个是神经元的视角。这就是这里的观点，实际上这就是课程中正在发生的事情，唯一不变的是错误模式，然后是基于模式的视角。这实际上就是注意力在做的事情。因此，在这里你是在抽象神经元，或者假设你是在幕后操作。
- en: but what you're actually computing is the distance between the true location
    of your pattern and the query and there are pros and cons to both of these the
    pro to this is you get much higher fidelity distance like you know exactly how
    far the query is some of your original patterns and that's really important when
    you're deciding what to update towards like you really want to make know like
    what is closest and what is further away and be able to apply the exponential
    weighting directly The problem is you need to store all of your pattern limitations
    in operate and so this is why transformers have like limited context with the
    other perspective is this long-term memory one where you forget about the patterns
    and you just look at where you just have your neurons that store a bunch of patterns
    in them and this way you superposition and so you can't really you can tinyulate
    what your original pattern was like point and it's all much noisier。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 但实际上你计算的是你模式的真实位置与查询之间的距离，这两者各有利弊。这样做的好处是你能获得更高保真的距离，确切地知道查询与某些原始模式的距离，而这在你决定更新方向时非常重要；你真的想要清楚什么是最近的，什么是更远的，并能够直接应用指数加权。问题在于你需要存储所有模式的限制并进行操作，这就是为什么变换器在上下文上有限的原因。另一种视角是长时记忆，其中你忘记了模式，只关注存储了一堆模式的神经元。这种方式下你实现了叠加，因此你无法真正模拟你原始模式是什么样的，结果也会变得更加嘈杂。
- en: But you can store tons of hundreds and you're not constrained by a context window。or
    you can think of any penalty layer as storing like the entire data set in a noisy
    superposition of states。Yeah， hopefully that kind of answers your question。I think
    there's one here first then yeah。So I guess my question is like。So I guess like
    you kind of showing that like being u modern what self attention mechanism on
    maps on like。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可以存储成百上千的数据，而不受上下文窗口的限制。或者你可以将任何惩罚层视为在嘈杂的状态叠加中存储整个数据集。是的，希望这能回答你的问题。我想这里有一个，然后是的。所以我想我的问题是这样的。所以我想你展示了现代的自注意机制是如何映射的。
- en: SVM mechanisms that like seems possible and like some look like the modern contemporary
    theories of like how bringing the down implement SDM。And I guess my question is
    like， to what degree has that like been？
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: SVM机制似乎是可能的，并且某些方面看起来像现代当代理论，例如如何将SDM实施落到实处。我想问的是，这种情况在多大程度上存在呢？
- en: Like experimentally verified versus like you were like mentioning earlier that
    like it might actually be easier to have done as using like an MLP layer in some
    sense than like onto to these like mechanisms and so like how do experimentalists
    like actually distinguish with hypothees like for instance like one thing that
    like I was it entirely clear about is like。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 像实验验证与之前提到的那样，实际上使用MLP层可能比这些机制更简单，那么实验者是如何区分假设的，比如说我并不完全清楚的一件事是。
- en: 对。Even if like the brain couldn't do attention or like SM like that doesn't
    actually mean it would because like maybe it can't do back yeah so like how do
    how does this like get actually tested totally yeah yeah so on the backdrop point。You
    wouldn't have to do it here because you have the climbing fibers that can directly
    like give training signals。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对。即使大脑无法集中注意力，或者像SM那样，这并不意味着它一定会这样，因为也许它无法做到这一点。那么，这到底是如何被彻底测试的呢？是的，关于背景这一点。你不必在这里进行测试，因为你有可以直接提供训练信号的攀爬纤维。
- en: Through your like what they options store so in this case you。It's like a supervised
    learning task for the funding cut does it what it wants to write in or like how
    it should be updated the pretend be cell sees but for your broader point you basically
    need to do to test this you need to be able to do realtime learning the Drophila
    mushroom body is basically identical to the ser bone and the fly on the brain
    data set has done most of the individual neuron connectivity but what you would
    really want to do is like any vitro realtime super like。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过你的喜好，选项存储在这种情况下。就像是一个监督学习任务，对于资金的削减，它想要写什么，或者如何更新就像细胞所看到的。但对于你更广泛的观点，你基本上需要做的就是测试这一点，你需要能够进行实时学习。果蝇的蘑菇体基本上与神经元相同，而果蝇脑数据集已经完成了大部分个体神经元的连接性。但你真正想做的就像任何体外的实时超级学习。
- en: Super super high frames per second calcium imaging and be able to see how synaps
    has change over time and so for an associative learning task like。Hear a sound
    move left hear another sound moved right or smells or whatever present one of
    those trace like figure out the small subset of neurons that fire which we know
    it' a small subset so that are with the end distance interpretation see how the
    science is here update and how the outputs of it corresponding to changes in motor
    action and then extinguish that number so right in the new one and then watch
    it watch it go away again and like our cameras are getting fast enough and like
    our calcium and like voltage indicators are getting to be really good so hopefully
    in the next like three to five years we can do some of those tests and I think
    that would be very definitive。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 超高帧率的钙成像技术能够观察突触如何随时间变化，适用于如关联学习任务等场景。听到一个声音向左移动，再听到另一个声音向右移动，或者嗅到某种气味，呈现出类似的痕迹，找出那些发火的小子集神经元，我们知道这只是一个小子集。因此，通过最终的距离解释，我们可以看到科学如何更新，以及其输出如何与运动行为的变化相对应，然后消除那个数字，正确地更新新的数据，再次观察这一过程消失。我们的摄像头速度越来越快，钙指示剂和电压指示剂的性能也在不断提升，希望在未来三到五年内能够进行一些这样的测试，我认为这将是非常具有决定性的。
- en: Yeah。Yeah， can we have any other questions， I think there was one more， and
    then I should。how you not be mirrorn theM like this final medical biological implementation
    what the range of your circle that you're mapping around that。understand thatYeah
    so I wouldn't get confused with multiheadedness because that's different attention
    heads all doing their own attention operation it's funny that the cebolum has
    microphones which you can think of as like separate attention heads in a way I
    don't want to take that analogy too far and like but but it is it is somewhat
    interesting so the way you relate this is。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。是的，我们还有其他问题吗？我想还有一个，然后我应该……你不会把他们像这样最终的医学生物实现镜像化，你所映射的范围是什么。明白了，所以我不会混淆多头性，因为那是不同的注意力头，各自进行自己的注意力操作。有趣的是，脑盖有麦克风，可以认为它们在某种程度上像独立的注意力头。我不想把这个类比推得太远，但确实有点有趣，所以你与此的关系是。
- en: In attention you have your beta coefficient， that is an effective beta coefficient
    because the vector norms of your keys and queries aren't concerned。that corresponds
    to a heence and here that corresponds to the number of neurons that are on for
    any given input。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在注意力机制中，你有你的 beta 系数，即有效 beta 系数，因为你的键和值的向量范数并不相关。这对应于一个指向，并且这里对应于任何给定输入时开启的神经元数量。
- en: 啊。And the heming distance you want， I had the slide before。the heming distance
    you want depends upon what you're actually trying to do and if you're not trying
    to store that many memories。for example， you're going to have a higher heming
    distance because you can get a higher fidelity calculation for the number of neurons
    in that noisy intersection。Yeah。Cool， yeah， thanks a lot。So as it disclaimer。So
    as a disclaimer before introduce our next speaker。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 啊。那么你想要的海明距离，我之前有那张幻灯片。你想要的海明距离取决于你实际上想要做什么，如果你并不打算存储那么多记忆。例如，你将会有更高的海明距离，因为你可以在那个嘈杂的交叉点获得更高保真度的计算。是的。很酷，嗯，非常感谢。所以作为免责声明。在介绍我们的下一位演讲者之前，先说明一下。
- en: the person was scheduled unfortunately had the cad in soul last minute due to
    faculty interviews。so our next speaker has very graciously agreed to present at
    very last minute but we are very grateful to him so I'd like to introduce everybody
    to Will so Will is a computational neuroscience machine learning PhD student at
    the University College of London at their Gatsby unit so I don't know if anybody
    has heard about the Gatsby unit I'm a bit of a history buff or history nerd depending
    on how you phrase it the Gatsby unit was actually this incredible powerhouse in
    the 1990s and 2000s so Hinntton used to be there Zubin Gurimani used to be there
    he's now in charge of Google research I think they've done a tremendous amount
    of good work anyways and now I'd like to invite Will to talk about how to build
    a cognitive map。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这个人原定的日程不幸因为最后一刻的教职面试而取消了。所以下一位演讲者非常友好地同意在最后一刻进行演讲，我们对他非常感激，所以我想向大家介绍Will。Will是伦敦大学学院盖茨比单元的计算神经科学和机器学习博士生。我不知道是否有人听说过盖茨比单元，我有点历史爱好者或者说历史极客，取决于你怎么说。盖茨比单元实际上在90年代和2000年代是一个令人难以置信的强大中心，Hinntton曾在那里，Zubin
    Gurimani也曾在那里，他现在负责谷歌研究。我认为他们做了大量出色的工作。现在我想邀请Will谈谈如何构建认知地图。
- en: Did you want to share your screen okay， can you stand in front of here， let
    me stop sharing。![](img/2cf4720874569984260c34088d0a5835_7.png)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你想分享你的屏幕吗？好的，你能站在这里吗？让我停止分享。![](img/2cf4720874569984260c34088d0a5835_7.png)
- en: Okay。So I'm going to be presenting this work， it's a it's all about how a model
    that people in the group that I work with。To study the hippocampal Enino system
    it completely independently turned out to look a bit like a transformer so that's
    this paper that i'm going to talk about is describing that link so the paper that
    builds this link is by these three people James is a postdoc half at Stanford
    him's a professor at Oxford and in London and Joe's a PhD student in London so。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以我将要介绍这项工作，它是关于我所在小组的人如何研究海马体神经系统的。结果发现它与变压器有些相似，所以我将要讨论的这篇论文描述了这个联系，这篇建立这一联系的论文是由这三位作者撰写的：詹姆斯是一位斯坦福大学的博士后，他是一位牛津大学和伦敦的教授，而乔是一位在伦敦的博士生。
- en: 😊，So this is the problem that this model of the hippocampal enter our system
    which we'll talk more about is supposed to solve is basically the observation
    there's a lot of structure in the world and generally we should use it in order
    to generalize quickly between tasks the kind of thing I mean by that is you know
    how 2D space works because of your long experience living in the world and so
    if you start at this greenhouse and step north it's orange one and this red ones
    then think one because of the structure of 2D space。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以这个模型所要解决的问题是，海马体进入我们系统的观察，基本上是世界上有很多结构，我们应该利用这些结构来快速在任务之间进行泛化。我所指的就是，你对2D空间的理解是因为你在这个世界上生活了很长时间，所以如果你从这个温室出发，向北走，就会看到橙色的一个和红色的一个，然后再想想，因为2D空间的结构。
- en: 😊，You can think to yourself， oh， what will happen if I step left and you know
    that you'll end up back at the green one because loops of this type clothes in
    2D space。okay？😡，And this is， you know， perhaps perhaps this is a new city you've
    just arrived in。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，你可以在心里想，哦，如果我向左走会发生什么，你知道你会最终回到绿色的那个，因为这种类型的循环在二维空间中是封闭的。好的？😡，而这，你懂的，也许这是一座你刚到的新城市。
- en: this is like a zero shop generalization because you somehow realize that the
    structure applies more broadly and use it in a new context。😊，Yeah， and there's
    generally a lot of these kinds of situations where there's structures that like
    reappear in the world。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是零商店的概括，因为你某种程度上意识到这个结构更广泛地适用，并在新的上下文中使用它。😊，是的，通常会有很多这种情况，其中结构在世界上不断重现。
- en: so there can be lots of instances where the same structure will be useful to
    doing these like zero shot generalizations to predict what you're going to see
    next。😊，Okay。And so you may be able to see how we're already going to start mapping
    this onto some kind of sequence prediction task that feels a bit transform e。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以会有很多情况，其中相同的结构对于执行这些任务是有用的，比如零-shot泛化来预测你接下来会看到什么。😊，好的。那么你可能会看到我们已经开始将其映射到某种序列预测任务，这听起来有点像变换。
- en: which is you receive this sequence of like observations and in this case， actions。movements
    in space and your job is given a new action step left here。you have to try and
    predict what we're going to sit so that's that kind of sequence prediction versionly。😊，And
    the way we're going to try and solve this is based on factorization。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你接收了一系列类似的观察，在这种情况下，是动作。在空间中的运动，而你的工作是给定一个新的动作步骤，你必须尝试预测我们将要坐的地方，所以这就是那种序列预测的版本。😊，我们将尝试解决这个问题的方法是基于因式分解。
- en: it's like you can't go into one environment and just learn from the experiences
    that one environment。you have to separate out the structure and the experiences
    you're having so that you can reuse the structural part which appears very often
    in the world。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像你不能只在一个环境中学习该环境的经验。你必须分离出结构和你所经历的经验，以便可以重用在世界上经常出现的结构部分。
- en: 😊，Okay， and so yeah， separating memories from structure and so you know。here's
    our separation of the two， we have our dude wandering around this like 2D grid
    world。And you want to separate out the fact that there's 2D space and it's 2D
    space that has these rules underlying it。and in a particular instance in the environment
    that you're in。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，好的，所以是的，将记忆与结构分开，你知道的。这是我们对两者的分离，我们有个家伙在这个二维网格世界中游荡。你想要区分的是，存在二维空间，并且这个二维空间有其基本规则。而在你所处环境的特定情况下。
- en: you need to be able to recall which objects are at which locations in the environment。Okay。so
    in this case， it's like， oh， this position has an orange house， this position。😊，Greams，
    sorry。orange。Red and pink and so you have to bind those two you have to be like
    whenever you realize that you're back in this position。recall that that is the
    observation you're going to see there。😡，Okay。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要能够记住环境中哪些物体位于哪个位置。好的。所以在这种情况下，就像，哦，这个位置有一栋橙色的房子，这个位置。😊，抱歉，Greams。橙色。红色和粉色，所以你必须把这两个绑定在一起。你必须在意识到自己回到这个位置时，回想起你将在那里看到的观察。😡，好的。
- en: and so this model that they we're going to build is some model that tries to
    achieve this。Yeah。new starts and so when you enter it imagine you enter a new
    environment with the same structure。you wander around and realize it's the same
    structure。all you have to do is bind the new things that you see to the locations
    and then you're task up you know you know how the world works。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，他们要构建的这个模型是一个试图实现这个目标的模型。是的。新的开始，所以当你进入时，想象你进入一个结构相同的新环境。你四处游荡，意识到它的结构是一样的。你要做的就是将看到的新事物与位置绑定，然后你就明白任务了，你知道世界是如何运作的。
- en: 😊，So this is what neuroscientists mean by a cognitive map is this idea of like
    separ it out and understanding the structure that you can reuse in new situations。And
    yeah， this model that was built in the lab is a model of this process happening
    of the separation between the two of them and how you use them to do new inferences
    and this is the bit thats supposed to look like a transform so's the general introduction
    and then we'll dive into it a little more now。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，神经科学家所说的认知地图是指将事物分开并理解其结构的理念，以便在新情境中重用。而且，实验室构建的这个模型是关于这一过程的模型，描述了它们之间的分离，以及如何利用它们进行新的推理。这部分应该看起来像一个转换，这就是一般介绍，接下来我们将深入探讨一下。
- en: 😊，Make sense though， the broad picture。Good。Silence， I'll assume is good。So
    we'll start off with some brain stuff so there's a long stream of evidence from
    spatial navigation that the brain is doing something like this I mean I think
    you can probably imagine how you yourself are doing this already when you go to
    a new city or you're like trying to understand a new task that has some structure
    you recognized from previously you can see how this is something you're probably
    doing but spatial navigation is in there in neuroscience which had like a huge
    stream of discoveries of the last like 50 years and a lot of evidence of the neural
    basis of this computation so we're going to talk through some of the examples
    the other of these psychologists like Tllman who were showing that rats in this
    case can do this kind of path integration structure so the way this work is they
    got put at a start position here done with RMS and they got trained that this
    route up here got you reward so this is the maze we had to run around。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，虽然有道理，但总体来看是这样的。很好。沉默，我假设是好的。那么我们先从一些大脑相关的内容开始，因为在空间导航方面有大量证据表明大脑在进行类似的处理。我想你可以想象，当你去一个新城市或者试图理解一个有你之前认知结构的新任务时，你自己如何在进行这样的处理。空间导航在神经科学中占据了重要地位，过去大约50年中有大量发现，并且有许多关于这种计算的神经基础的证据。所以我们将讨论一些例子，还有其他心理学家，比如Tllman，他们展示了老鼠可以进行这种路径整合结构。这项工作的方式是，他们被放置在这里的起始位置，使用RMS进行训练，让它们了解到这里的路径可以获得奖励，所以这是我们必须绕行的迷宫。
- en: 😊，Then they were asked they were put in this new the same thing but they blocked
    off this path that takes us long winding route and given instead a selection of
    all these arms to go down and they look at which path the rat goes down and the
    finding is that the rat goes down the one that corresponds to heading off in this
    direction so the rat has somehow not just learn like you know one option of this
    is it's like blind memorization of action that I need to take in order to root
    around instead no it's learning actually that embedding the reward and its understanding
    of Trudy's face and taking a direct route there even though it's never taken it
    before there's evidence that rats are doing this as well as us。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，然后他们被问到被放在这个新的环境中，情况是一样的，但他们封锁了这条通往漫长曲折路线的路径，而是提供了所有这些可以选择的分支，他们观察老鼠选择哪条路径，结果发现老鼠走的是朝这个方向的路径。因此，老鼠似乎不仅仅是学习一个选项，而是像盲目记忆一样，知道自己需要采取的行动来探索，而是实际上在学习，将奖励嵌入其中，理解Trudy的脸，并选择一条直接的路线，尽管它之前从未走过。有证据表明老鼠也在这样做，和我们一样。
- en: And then a series of like neural discoveries about the basis of this。so John
    O'keefe stuck an electrode in the hippocampus。which is a brain area we'll talk
    more about and found these things called place cells so what I'm ploting here
    is each of these columns is a single neuron and the mouse or I can't remember
    is running around a square environment the black lines are the path the rodent
    traces out through time and you put a red dot down every time you see this individual
    neuron spike and then the bottom plot of this is just a smooth version of that
    spike brain so that firing rate what you can think of is like the activity of
    a neuron and neuron network that's the analogy that people you usually drawn and
    so these ones are called play cells because the neurons that respond in a particular
    position in space and in theem years this was like huge excitement you know and
    people mean studying mainly like sensory systems and motor output and suddenly
    the deep cognitive variable place something you never you don't have a GPS signal
    but somehow there's this like signal for what looks like position in the brain
    in very like understandable ways。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后关于这个基础的一系列神经发现。所以约翰·奥基夫在海马体里插入了一个电极。这个脑区我们会进一步讨论，他发现了被称为位置细胞的东西。因此，我在这里绘制的每一列都是一个单一的神经元，而小鼠或我记不清是哪个正在一个方形环境中跑动，黑线是啮齿动物随着时间描绘的路径，每当你看到这个个体神经元发放时，就会在上面放一个红点，而底部的图是这个发放活动的平滑版本。你可以把这个发放率想象成神经元和神经网络的活动，这是人们通常所绘制的类比。这些被称为位置细胞，因为这些神经元在特定空间位置上有反应。多年来，这曾引起巨大的兴奋，人们主要研究感觉系统和运动输出，而突然间，深层认知变量的位置变得重要。你没有GPS信号，但不知怎的，大脑中有一种信号可以理解为位置。
- en: 😊，The next step in the biggest step I guess in this chain of discovery is the
    Mosa lab。which is a group in Norway， the second lecture in a different area of
    the brain。the media entertoranal cortex， and so this is the hippocampal entertoranal
    system we're going to be talking about and they found this neuron called a grid
    cell so again the same plot structure that I'm showing here。but instead these
    neurons respond not in one position of room but in a like a hexagonal lattice
    of positions in the root。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我猜这次发现链中最大的一步是Mosa实验室。这个实验室位于挪威，第二次讲座涉及大脑的不同区域。媒体内嗅皮层，因此我们将讨论的这个是海马内嗅系统，他们发现了一种被称为网格细胞的神经元。因此，这里展示的图结构是相同的，但这些神经元不是在房间的一个位置响应，而是在房间内呈六角格状的多个位置响应。
- en: 😊，Okay， so these two， I guess I'm showing to you because they like really motivate
    the underlying neural basis of this kind of like spatial cognition embedded embodying
    the structure of this space in some way。😊，Okay and it's very surprising finding
    why why are neurons choosing to represent things with this Xmatic is like yeah
    provoked a lot of research and broadly there's been like many more discoveries
    in this area so there's place cells that talk to you about grid cells cells that
    respond based on the location of not yourself but another animal cells that respond
    when your head is facing a particular direction cells that respond to when particular
    distance away from an object so like I' am one step south of an object that kind
    of cell。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，好的，所以这两个，我想我向你展示是因为它们确实激发了这种空间认知的神经基础，以某种方式体现了这个空间的结构。😊，好的，而且这个发现非常令人惊讶，为什么神经元选择用这种Xmatic来表示事物，确实引发了很多研究，并且在这个领域已经有了更多的发现，所以有位置细胞，谈论网格细胞，响应的细胞根据你不是自己而是另一个动物的位置，响应的细胞当你的头朝着特定方向时，响应的细胞在离某个物体特定距离时，比如说我在某个物体的南边一步，这种细胞。
- en: 😊，Clls that respond to reward positions， cells respond to vectors to boundaries。cells
    that respond to like all sorts， all kinds of structure that this pair of brain
    structures。the hippocampus here， this red area and the entertor anal cortex， this
    blue area here。which is conserved across a lot of species are represented。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，对奖励位置做出反应的细胞，细胞对边界的向量做出反应。对各种各样的结构做出反应的细胞，这对大脑结构的配对。这里的海马体，这个红色区域，以及这里的内嗅皮层，这个蓝色区域。许多物种中保留下来的这些结构被表示出来。
- en: There's also finally one finding in this that's fun is they didn't an FMRI experiment
    on London taxiab drivers and I don't know if you notice。but the London taxi cabab
    drivers they do a thing called the knowledge which is a twoy long test where they
    have to learn every street in London and the idea is the test goes something like
    oh there's a traffic jam here and a road work here and I need you to get from
    like Camden town down to one's worth the quickest way possible what route would
    you go they have to tell you which route they're going be able to take through
    all the roads and how they would replan if they found stop there's kind of sense
    so it's like intense you see them like driving around sometimes learning all of
    these like routes with a little map they're being made a little bit obsolete by
    Google Maps but you know luckily they've got them before this experiment was done
    before that was true and so they've got here is a measure of the size of your
    hippocampus using FMRI versus how long you've been a taxiab driver in months and
    the claim is basically the longer you're a taxiab driver the bigger your hippocampus
    for the more you're having to do this kind of spatial reason。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一个有趣的发现，他们对伦敦出租车司机进行了FMRI实验，不知道你是否注意到。伦敦的出租车司机要进行一个叫做“知识”的测试，这是一个为期两年的测试，他们必须学习伦敦的每一条街道。测试的内容大致是这样的：哦，这里有交通堵塞，这里有施工，我需要你从卡姆登镇到沃恩斯沃斯以最快的方式走，你会选择哪条路线？他们必须告诉你他们将通过哪些道路，以及如果遇到阻碍他们将如何重新规划路线。这有点像是一种紧迫感，你可以看到他们有时驾驶着汽车学习所有这些路线，手里拿着小地图。尽管他们正在被谷歌地图稍微取代，但幸运的是，在这个实验进行之前，他们还没有被取代。因此，他们在实验中测量了使用FMRI的海马体大小与你作为出租车司机的时间（以月份计算）之间的关系，基本上，作为出租车司机的时间越长，海马体越大，因为你需要进行更多的空间推理。
- en: 😊，So that's a big set of evidence that these brain areas are doing something
    to do with space but there's a lot of evidence that there's something more than
    that something non-spatial going on in these areas okay and we're going to build
    these together to make the broader claim about this like underlying structural
    inference and so I'm going to talk through a couple of those the first one of
    these is a guy called patient HM this is the most studied patient in like medical
    history he had epilepsy and cure in epilepsy have to cut out the brain region
    that's causing these like seizure like events in your brain and in this case the
    elepsy was coming from the guys hippocampus so the bilaally lesions hipcampus
    they cut out both this hippocpi and it turned out that this guy then had terrible
    amnesia he never formed another memory again and he could only recall memories
    from a long time before the surgery happened。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以这是一大堆证据表明这些大脑区域与空间有关，但还有很多证据表明在这些区域中发生着某种超越空间的事情，好吧，我们将把这些结合起来，做出更广泛的关于这种潜在结构推理的主张，因此我将讨论其中几个，第一个是一个叫做病人HM的人，这是医学史上研究最多的病人，他有癫痫，而治疗癫痫需要切除导致这些类似癫痫发作的大脑区域，在这种情况下，癫痫源自他的海马体，因此他们切除了双侧海马体，结果发现这个人随后出现了严重的健忘症，他再也无法形成新的记忆，只能回忆起手术发生前很久的记忆。
- en: 😊，Okay but so experiments were showed a lot of this stuff about how we understand
    the neural basis of memory things like he could learn to do motor tasks so somehow
    the motor tasks are being done for example they gave him some very difficult motor
    coordination task that people can't generally do a camera with a lot of practice
    and he got very good at this eventually and was as good at other people I'd learning
    to do that we had no recollection of ever doing the task so he'd go into do this
    new task and be like I've never seen this before I've no idea what you're asking
    me to do and he do it amazing be like so there's some evidence there that the
    hippocampus is involved in least some parts of memory there which seems a bit
    separate to this stuff about space that I've been talking to you。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，好的，但实验显示了很多关于我们如何理解记忆神经基础的东西，比如他能学习做运动任务，因此某种程度上运动任务是被完成的。例如，他们给他一些非常困难的运动协调任务，这些任务一般人通过大量练习也很难做到，而他最终变得非常擅长，和其他人一样，学习去做这个任务，但他对自己做过的任务没有任何记忆。他会去做这个新任务，然后说：“我从来没有见过这个，我不知道你让我做什么。”然后他做得非常好，这里有一些证据表明海马体至少在某些记忆部分中是参与的，这似乎与我之前跟你谈论的空间内容有些分开。
- en: 😊，second of these is imagining things so this is actually a paper by Deisa Sabbth
    who's before he was Deep Minhead with a neuroscientist and here maybe you can't
    read that I'll read some of these that you're asked to imagine you're lying on
    a white sandy beach and a beautiful tropical bay and so the control this bottom
    one says things like it's very hot and the sun is beating down on me the sand
    underneath underneath me there's almost unbearably hot I can hear the sounds of
    small wavelelets laughingping on the beach see is gorgeous amarine color you know
    like so nice lucid descriptional this beauty scene whereas the person with a hippocampal
    damage saysAs for seeing I can't really apart from just the sky I can hear the
    sound of seagulls under the sea I can feel the grain of sandre my fingers。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，第二个是想象事物，这实际上是一篇由Deisa Sabbth撰写的论文，他在成为Deep Mind的科学家之前是一名神经科学家。这里也许你看不清楚，我来读一些内容，你被要求想象你躺在一个白色沙滩和美丽热带海湾上。因此，控制下方的内容说了一些，比如“非常热，阳光照射在我身上，沙子在我身下几乎热得难以忍受，我能听到小浪花拍打沙滩的声音，海水是迷人的海洋色，非常漂亮”。这种对美丽场景的清晰描述，而有海马损伤的人则说：“至于看，我几乎什么也看不到，除了天空，我能听到海鸥的声音，能感觉到沙粒在我的手指间。”
- en: 😊，And then like yeah like the struggles are basically really struggles to do
    this imagine and imagine this scenario。some of the things read these is like very
    surprising。so the last of these is this transitive inference task。😊，So terms of
    inference， A is greater than B。B is greater than C， therefore A is greater than
    C。😊。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，然后就像，是的，挣扎基本上就是为了想象这个场景。这些事情读起来非常令人惊讶。最后这个是**传递推理任务**。😊，所以在推理方面，A大于B，B大于C，因此A大于C。😊。
- en: And the way they convert this into a rodent experiment is you get given two
    pots of food that have different smells and your job is to go to the pot of food
    you learn which pot of food has sorry。which pot with the smell has the food。And
    so these are colored by the two pots by their smell A and B and the rodent has
    to learn to go to a particular pot in this case the one that smells like A and
    they do two of these they do A has the food when it's presented in a pair with
    B and B has the food when it's presented in a pair with C and then they test what
    does the mouse do when presented with A and C a completely new situation and they
    say have a hippocampus they'll go for A over C。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 它们将这一实验转化为啮齿动物实验的方式是，你会得到两个有不同气味的食物罐，而你的任务是去找出哪个食物罐有食物，抱歉，哪个气味的罐子里有食物。因此，这两个罐子被标记为气味A和B，啮齿动物需要学会去特定的罐子，在这种情况下是气味为A的那个。他们进行两次实验，当A与B配对时，A有食物，而当B与C配对时，B有食物。然后他们测试当老鼠面对A和C这种全新的情况时会怎么做，他们说有海马体的老鼠会选择A而不是C。
- en: they'll do transitive inference is they don't have one they cut。And so there's
    a much more broad set this is like， oh。I've shown you how hip campusampus is used
    for this spatial stuff that people have been excited about。but there's also all
    of this kind of relational stuff， imagining you situation。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 他们进行传递推理的方法是，如果没有一个，他们会删减。因此，有一个更广泛的集合，这就像，哦。我已经向你展示了如何使用hip campusampus来处理人们对此空间内容的兴奋。但还有所有这些关系方面的东西，想象你的情况。
- en: some slightly more complex story in it。😊，The last thing I'm going to do is how
    the Entoral cortex as well so that's where if you remember hipcampus was these
    guys Entoral cortex was these grid cells was how Entoral cortex was appearing
    to do some broader stuff as well this is all motivation for the model just trying
    to build all of these things together。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 里面有一些稍微复杂的故事。😊，我最后要做的是关于内嗅皮层的，如果你记得海马体，那么这些内嗅皮层是这些网格细胞，内嗅皮层似乎在做一些更广泛的事情，这一切都是模型的动机，只是试图将所有这些东西结合在一起。
- en: 😊，So in this one this is called the stretchy birds task okay so you put people
    in the FMRI machine and you make them navigate but navigate in bird space and
    what bird space means is it's a twodisional space of images and each image is
    one of these birds and as you vary along the X dimension the bird's legs get longer
    and shorter and as you vary along a y direction the bird's neck gets longer and
    shorter okay and the patients sit there subjects sit there and just watch the
    bird images change so that traces out some part in 2D space but they never see
    the 2D space they just see the images and the claim is basically and then they're
    asked to do some like navigational task they're like oh whenever you're in this
    place in 2D space you show like Santa Claus next to the bird and so the participants
    have to pin that particular bird image that thick place in 2D space to the Santa
    Claus and you're asked to like go and find the Santa Claus again using some non-directional
    controller and they like navigate their way back。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以这个任务叫做“拉伸鸟类任务”，好的，所以你把人放进FMRI机器里，让他们在鸟类空间中导航。鸟类空间是一个二维图像空间，每张图像都是一种鸟。当你在X维度上变化时，鸟的腿会变长或变短；在Y方向上变化时，鸟的脖子会变长或变短。患者坐在那里，受试者看着鸟类图像的变化，这描绘出二维空间中的某些部分，但他们从未看到过二维空间，只是看到了图像。基本上他们被要求完成某种导航任务，比如说，哦，每当你在二维空间中的这个位置时，你会看到圣诞老人和鸟在一起。所以参与者必须将特定的鸟类图像与二维空间中的圣诞老人对应起来，并被要求使用某种非定向控制器再次找到圣诞老人，他们就这样导航回去。
- en: 😊，And the claim is that these people use grid cells so the Entornal cortex is
    active in how these people are navigating this abstract cognitive bird space。and
    the way you test that claim is you look at the FMRI signal in the Entornal cortex
    as the participants head at some particular angle in bird space。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，而且这个说法是，这些人使用网格细胞，因此内嗅皮层在这些人导航这个抽象的认知鸟类空间时是活跃的。你验证这个说法的方法是观察参与者在鸟类空间某个特定角度时内嗅皮层的FMRI信号。
- en: And because of the sixfold symmetry of the hexagonal lattice。you get this sixfoldsymmetric
    waving up and down of the Entornal cortex activity as you head in particular directions
    in 2D sets。it like evidence that the system is being used not just for like navigation
    in 2D space。but any cognitive task with some underlying structure that you extract。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 由于六边形晶格的六重对称性，你会看到这种六重对称的上下波动的**外侧皮层**活动，随着你在二维集合中特定方向的移动。这就像是证据，表明这个系统不仅仅用于二维空间中的导航，而是用于任何具有某种潜在结构的认知任务。
- en: you use it to do these tasks people haven't done that experiment。but people
    have done things like look at how grid cells but they haven't done。😊。They've done
    things like 3D space， but not like cognitive 3D space。they've done like literally
    like make they've done it in bats。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你用它来做这些人们还没有做过的实验。但人们确实做过一些事情，比如观察网格细胞，但他们没有做到。😊。他们做过三维空间的研究，但不是像认知三维空间那样。他们确实做过，比如在蝙蝠身上进行研究。
- en: they stick electrodes and bats and make the bats fly around the room look at
    how their grid cells respond。Yeah， but definitely。😊，I think they've done it they've
    done it in sequence space So in this case you hear a sequence of sound with hierarchical
    structure so it's like how months。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 他们粘贴电极在蝙蝠身上，让蝙蝠在房间里飞，看它们的网格细胞如何反应。嗯，确实如此。😊，我认为他们在序列空间中已经做到这一点。所以在这种情况下，你会听到一个具有层级结构的声音序列，就像月份一样。
- en: weeks， days and meals， something like that so like weeks have a periodic structure
    months have a periodic structure days have a periodic and meals have a periodic
    structure and so you hear a sequence of sounds with exactly the same kind of structure
    of that hierarchy of sequences and you look at the representation of the entertornal
    cortex through fMRI and you see exactly the same thing that the structure is all
    represented and like that even more than that you actually see in the entertornal
    cortex a array of length scales so at one end of the Entornal cortex you've got
    a very large scale grid cells that are like responding to large variations in
    space the other end' got small ones and you see the same thing recaply there the
    like meals cycle that cycle a lot more quicker is represented in one end of the
    entertornal cortex and fMRI and the month cycle is at the other end with like
    a scale in between so there's some evidence to that end。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 周、天和餐，这样的东西，所以像周有一个周期结构，月有一个周期结构，天有一个周期结构，餐也有一个周期结构，因此你听到的声音序列正好具有这种层级序列的结构。通过fMRI观察内侧颞皮层的表现，你会看到完全相同的结构被展现出来，甚至更进一步，你实际上会在内侧颞皮层看到一个长度尺度的阵列。在内侧颞皮层的一端，你会看到非常大尺度的网格细胞，它们对空间中的大变化作出反应；另一端则是小尺度的细胞，你会在那里看到相同的东西，比如餐的周期，这个周期在内侧颞皮层和fMRI中表现得要快得多，而月的周期则位于另一端，两者之间有一个尺度。因此，对此有一些证据。
- en: 😊，No worries so I've been talking about MEc， the media internaltoranal cortex。another
    brain area that people don't look at as much is the L， the lateral entertorronal
    cortex。but wouldn't be important for this model and basically the only that you
    shouldnt be aware of before we get the model is that it seems to represent very
    high level the similarity structure in a lateral entertoral cortex seems to be
    like a very high- levell semantic one for example。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，没问题，所以我一直在谈论MEc，内侧内嗅皮层。另一个人们不太关注的脑区是L，外侧内嗅皮层。但在这个模型之前，你需要注意的是，它似乎代表了非常高层次的相似性结构，外侧内嗅皮层似乎像是一个非常高层次的语义结构，例如。
- en: you present some images and you look at how in the visual cortex things are
    more similarly represented if they look similar。but by the time you guess the
    lateral entertorranal cortex things look more similar based on their usage。for
    example， like an ironing board and an iron will be represented similarly。even
    though they look very different because they're somehow like semantically。😊，Okay。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 你展示了一些图像，观察在视觉皮层中，如果它们看起来相似，事物的表示会更加相似。但当你猜测外侧内嗅皮层时，事物的相似性更多是基于它们的使用。例如，熨衣板和熨斗会被相似地表示，即使它们看起来非常不同，因为在某种程度上它们在语义上是相似的。😊，好的。
- en: so that's the role that the LEC is going to play in this model。So yeah basically
    the claim is this is for more than just 2D space。so the neural implementation
    of this cognitive map。which is for notmin only 2D space which this cartoon is
    supposed to represent。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是LEC在这个模型中将要扮演的角色。嗯，基本上这个说法是，它不仅仅适用于二维空间。神经实现的这个认知地图，是不仅仅针对二维空间的，而这个漫画本来是要表现的就是这一点。
- en: also things any other structure， so some structures like transitive inference。this
    one is faster than that and faster than that or family trees like this person
    my mother's brother and is therefore my uncle。those kind of things。😊，These like
    broader structural inferences that you'll want to be able to use in many situations
    so basically the same problem。😡，Great， that was a lot in neuroscience。😊，And now
    we're going to get on to the model that tries to summarize all of these things
    and that's going to be the model that will end up looking like a transform。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他任何结构的事物，比如推理的结构。这种结构比那种结构更快，也比那种结构更快，或者像家谱这样的结构，比如这个人是我母亲的兄弟，因此是我的叔叔。这些东西。😊这些更广泛的结构推理，你希望在很多情况下都能使用，所以基本上是同样的问题。😡太好了，这在神经科学中有很多内容。😊现在我们要进入一个模型，它试图总结所有这些内容，这个模型最终会像一个变换。
- en: So yeah， we basically want this separation。These diagrams here are supposed
    to represent a particular environment that you're wandering around it has an underlying
    grid structure and you see a set of stimuli at each point on these grid which
    these little cartoon bit and you want to try and create a thing that separates
    out this like 2D structural grid from the actual experiences you're seeing and
    the mapping to the things I've been showing you is that this grid grid like code
    is actually the grid cells in the middleenttoral cortex are somehow abstracting
    the structure the lateralenttor anal cortex encoding these semantically meaningful
    similarities will be the objects that you're seeing so it's just like this is
    what I'm saying in the world。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们基本上想要这种分离。这些图表应该代表你在其中游荡的特定环境，它具有潜在的网格结构，你在这些网格的每个点上看到一组刺激，这些小卡通图形。你想尝试创造一个东西，将这种二维结构网格与实际看到的体验分开，而我展示给你的映射是，这种网格状的代码实际上是中脑皮层中的网格细胞在以某种方式抽象出结构，侧脑皮层编码这些语义上有意义的相似性，将是你看到的物体。因此，这就是我在世界上所表达的内容。
- en: 😊，And the combination of the two of them will be the hippocampus。So yeah， any
    more diagrams。you've got G， the structural code， the grid code in MEc Lc， who
    is someone else asking a question？😊。Since morning， so now it's Strstein， yeah。Sorry，
    I can't hear you if you're asking a question。How do I mute someone if theyre？Maybe
    type it in the chat if there is one。嗯对。Nice。So yeah。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，他们两者的结合将形成海马体。所以，嗯，还有其他图表吗？你有G，结构代码，MEc Lc中的网格代码，谁在问问题？😊。从早上开始，现在是Strstein，是的。抱歉，如果你在问问题我听不见。如何将某人静音？如果有聊天的话，也许可以打字。嗯，对。很好。那么，是的。
- en: we got a hippocampus in the middle， which is going to be our binding of the
    two of them together。😊。Okay。So I'm going to step through each of these three parts
    on their own and how they do the job that I assign to them and then come back
    together and show the full model。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在中间有一个海马体，这将是我们将它们两个结合在一起的部分。😊。好的。接下来我会逐一介绍这三个部分，以及它们如何完成我分配给它们的任务，然后再结合在一起展示完整模型。
- en: So lateral intergriial cortex encodes what you're seeing so this is like these
    images or the houses we were looking at before and that would just be some vector
    Xt that's different for a random vector different for everything。😊，The media internalnal
    cortex is the one that tells you where you are in space and it has the job of
    path integrating okay so this means receiving a sequence of actions that you've
    taken in space。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所以侧内侧皮层编码你所看到的内容，就像我们之前看到的这些图像或房屋，那只是一个随机向量Xt，对于每个事物都是不同的。😊，内部皮层负责告诉你在空间中的位置，并承担路径积分的任务，这意味着接收你在空间中所采取的动作序列。
- en: for example， I went north east and south and telling you where in 2D space that
    you are so it's somehow the bit that embeds the structure of the world。😡，And the
    way that we'll do that is this G of T， this vector of activities in this brain
    area will be updated by a matrix that depends on the actions you've taken okay
    so if you step north you update the representation with the step north matrix
    okay and those matrices are going to have to obey some rules for example if you
    step north and step south you haven't moved and so the step north matrix and the
    step south matrix actually inverses one another so that the activity stays the
    same and represents the structure of the world for。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我向东北和南方移动，并告诉你在二维空间中的位置，所以这在某种程度上是嵌入世界结构的那部分。😡，我们将通过这个G of T来实现，这个脑区的活动向量将通过一个矩阵来更新，这个矩阵依赖于你所采取的行动。好吧，如果你向北走，你就用向北的矩阵更新表示，好吧，这些矩阵必须遵循一些规则。例如，如果你向北走并向南走，你实际上是没有移动的，因此向北的矩阵和向南的矩阵实际上是互为逆的，这样活动保持不变，并且代表世界的结构。
- en: Okay， so that's the world structure part。Finally， the memory。because we have
    to memorize which things we found or which positions going to happen in the HiAcus
    and that's going to be through a version of these things called the hotfield networks
    that you heard mentioned in that last talk。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这就是世界结构的部分。最后，关于记忆。因为我们必须记住我们发现了哪些东西，或者在 HiAcus 中将要发生哪些位置，这将通过你在最后一场演讲中提到的热场网络的某种形式来实现。
- en: So this is like a content addressable memory and it's biologically plausible
    with playing。The way it works is you have a set of activities， P。which are the
    activities of all these neurons and when it receives so it just like recurrently
    updates itself。so there's some weight matrix in here W some nonlinearity and you
    run it forward in time and it's like settled into something some dynamical system
    it's settled into to some attractive state。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就像是一种内容可寻址的记忆，并且在生物上是可行的，类似于游戏。它的工作原理是你有一组活动，P。这些是所有这些神经元的活动，当它接收到信息时，它就像是不断地自我更新。因此这里有一个权重矩阵W，一些非线性，并且你将其向前运行在时间上，就像是进入了某种动态系统，它稳定到了某种吸引状态。
- en: 😊，And the way you make a new memory is through the weight matrix， okay。so you
    make it like a sum of outer products of these chi mu， each chi has some memory。some
    pattern you want to recall。😡，Okay。😊，And then it's yeah， this is just writing it
    in there。the update pattern is like that and the claim is basically the FP， the
    memory。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，你创建新记忆的方式是通过权重矩阵，好的。所以你将它表示为这些chi mu的外积之和，每个chi都有一些记忆。你想要回忆的某种模式。😡，好的。😊，然后是的，这只是把它写进去。更新模式就是这样，基本上声明是FP，即记忆。
- en: the activity of the neurons， the hippocampal neurons is close to some memory，
    say ku。😊。Then this doc product will be much larger than all of the other doc products
    with all the other members。so there's some over all of them will basically be
    dominated by this one term kinu and so your attractors at the network will basically
    settle into that one ku。😡，And maybe the preemp some of this stuff that's kind
    of come later。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的活动，海马神经元与某些记忆密切相关，比如ku。😊。因此，这个文档产品将比其他所有文档产品大得多，包含所有其他成员。因此，它们的整体效果基本上会被这个术语kinu主导，因此你的吸引子在网络中将基本上会稳定在这个ku上。😡，而且可能会提前处理一些稍后出现的内容。
- en: you can see how this like similarity between points is。yeah powerI similarity
    and then adding some adding them up weighted by this powerI similarity is the
    bit that's going to turn out looking a bit like attention。😊，And so some of the
    cool things you can do with these systems is like here's a set of images that
    someone's encoded in a hot field network and then someone's presented this image
    to the network and ask it to just run to its like dynamical attractor minima and
    it recreates all of the memory that it's got stored so it like completes the rest
    of。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到这种点之间的相似性是如何存在的。是的，powerI 相似性，然后将它们加权后相加，这部分看起来有点像注意力。😊，所以你可以用这些系统做的一些酷炫的事情，比如这是一个被某人编码在热场网络中的图像集合，然后有人将这个图像呈现给网络，并要求它运行到其动态吸引子最小值，结果它重新创建了所有存储的记忆，所以它就像完成了剩下的部分。
- en: 😊，So that's our system。Yeah， I'm sorry that's like。Like they had looked feel
    like。Ive heard that like this interpretation is like the modern interpretation。This
    one is actually which conservative sorry yeah。😊。Yeah yeah it's only the link to
    transformers will basically only be through the fact there's classic coffee networks
    and then there's modern ones that were middle like 2016 and the link between attention
    and modern is precise the link with like classic is not as I mean yeah modern。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以那就是我们的系统。是的，我很抱歉，这就像。就像他们看起来感觉像。我听说这种解释就像是现代解释。这实际上是保守的，抱歉，是的。😊。是的，是的，链接到变压器基本上只通过经典咖啡网络和现代网络来实现，现代网络大约在2016年，而注意力与现代之间的链接正是经典与现代之间的链接不是那么明确，我是说，是的，现代。
- en: 😊，没也没什么好。With it change in the non nonlinearity。Because then you have to do
    the exponentiation thing。Well， maybe get to the later and you can tell me some
    no， no， no， more questions are good。😊。We'll get yeah， maybe separate energy bump。And
    I think the exponential is in that。Okay， no worries。So that's basically how our
    systems going to work but this to and iconicenba machine what the name of this
    thing is and so you the patterns you want to store in the hippocampus so these
    memories that we want to embed are a combination of the position and the input
    and like half of Mor's face here if you then have decided you' want going to end
    up at a particular position you can recall the stimulus that you saw there and
    predict that as your like next observation or vice versa if you like see a new
    thing you can infer oh like path integrated wrong I must actually be here assuming
    there' but there's usually more than one thing in the world that might be in a
    different position。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，没也没什么好。随着非线性变化的到来。因为那时你得进行指数运算。好吧，也许等会儿你可以告诉我一些不，不，不，更多的问题是好的。😊。我们会得到，是的，也许是分离的能量波动。我认为指数在其中。好的，没关系。所以这基本上是我们的系统将如何运作，但这个标志性的机器叫什么呢？所以你想在海马体中存储的模式，这些我们想嵌入的记忆是位置和输入的组合，就像莫尔的半张脸一样。如果你决定要到达特定位置，你可以回忆起你在那里看到的刺激，并将其预测为你的下一个观察，反之亦然。如果你看到一个新东西，你可以推断出，哦，路径整合错误，我实际上应该在这里，假设那是，但世界上通常会有不止一件事情可能在不同的位置。
- en: 啱。😊，Yeah， that's the whole system， does the whole Tom and Iicenba machine make
    sense。roughly what it's doing。😊，Okay， cool and basically this last bit is saying
    it's really good so what i'm showing here is this is on the 2D navigation task
    and it's so it's a big grid I think they use like。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。😊，对，这就是整个系统，汤姆和Iicenba机器大致是这样运作的。😊，好的，基本上最后这一部分是在说它真的很好，所以我在这里展示的是2D导航任务，它是一个大网格，我想他们使用的是这样的。
- en: 😊，11 by 11 or something and it's like wandering around and have to predict what's
    going to see in some new environment and on here this is the number of nodes in
    that graph that you've visited and on the Y axis is how much you correctly predict
    and each of these lines is based on how many of those type of environments I've
    seen before how quickly do I learn and the basic phenomena it's showing is over
    time as you see more and more of these environments you learn to learn so like
    learn the structure of the world and eventually able to quickly generalize to
    the new situation and predict what you're going to see and the scales not with
    like the number of edges that you've visited which will be the like learn everything
    option you predict because if you're trying to predict which say I'm going to
    see given my current state and action and in a dumb way youre just going to see
    all states in action so all edges but this thing is able to do it much more globally
    because it needs to visit all nodes and just memorize what is each position and
    you can see that it's learning curve follows the number of nodes visited Learn
    curve。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，11乘11或类似的情况，就像在新环境中四处游荡，需要预测接下来会看到什么，这里是你在该图中访问的节点数量，Y轴表示你预测的正确程度，每条线基于我之前见过的那些环境的数量，展示了我学习的速度。基本现象是，随着时间的推移，当你看到越来越多的环境时，你学会了学习，像是了解世界的结构，最终能够迅速泛化到新的情境，预测你会看到什么。这个尺度与访问过的边的数量无关，那将是“学习一切”的选项，因为如果你试图预测在当前状态和动作下会看到什么，以一种笨拙的方式，你只会看到所有状态和动作，也就是所有边。但这个模型能够更加全面地进行预测，因为它需要访问所有节点，仅仅记住每个位置是什么，而你可以看到它的学习曲线跟随访问的节点数量的学习曲线。
- en: 😊，It's a thing well。For neuroscience， this is what's exciting is that the neural
    patterns of response brain in these like model regions match the ones observed
    in the brain。so in the hippocampal section you get place cell like activities。this
    hexagon is the grid of the environment that's exploring and plotted is the firing
    rate of that neuron whereas the ones in the medial interinnal cortex show this
    gridlike firing pattern。Yeah。This like example we from here operate some like
    discrete space in the century you have any thoughts about how that transfers what
    continuous those to world things。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，这确实很有意思。对于神经科学来说，令人兴奋的是，这些模型区域的神经反应模式与观察到的大脑反应模式相匹配。在海马区，你可以看到类似位置细胞的活动。这个六边形是探索环境的网格，而绘制的是该神经元的发放率，而内侧皮层中的神经元则表现出这种网格状的发放模式。是的。我们从这里的例子操作一些离散空间，你对如何将这些转化为连续的世界事物有什么看法吗？
- en: you think that we just like map over Nathan into like a very nicely discrete
    of space where we think it's can make like more complicated going on。Yeah， I imagine
    there's something more complicated going on， I guess this。So there's like a super
    no no yeah， maybe you can make maybe you can make arguments that as I was saying
    there's these different modules that have operated at different scales。you can
    see this already here like grid cells at one scale grid cells at another scale
    and so you could imagine how that could be useful for like one of them operates
    at the highest level and mix one of them operates at the lowest level you know
    like adaptable they seem to scale up or down depending on your environment and
    so like an adaptable set of length scales that you can use to but that's quite
    speculative。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为我们只是把Nathan映射到一个非常离散的空间中，我们认为这可能会变得更复杂。是的，我想象着可能有更复杂的东西在发生，我想就是这个。所以有一个超级的禁忌，嗯，也许你可以提出一些论点，就像我之前说的，这些不同的模块在不同的尺度上运作。你可以在这里看到这一点，比如在一个尺度上的网格细胞，在另一个尺度上的网格细胞，因此你可以想象这对其中一个在最高水平运作的模块是多么有用，而另一个则在最低水平运作。你知道，这些模块似乎可以根据环境的不同而自适应地扩大或缩小，所以有一组适应性长度尺度可以使用，但这相当推测。
- en: 😊，Okay， sorry， yeah， to make sure I understand if you'd go well。Okay，好 one more。yeah。so
    you have your。What's the key and what's the value？Yeah， so the。And hot bill networks
    are always auto instead of E associated， so how are you？
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，好的，抱歉，嗯，为了确保我理解你是否会很好。好的，再来一次。是的。所以你有你的。关键是什么，值是什么？是的，所以那个。热账单网络总是自动的，而不是与E相关的，那么你怎么样？
- en: The memories that we're going to put in， so the patterns， let's say kind you。Is
    going to be some like outer product of the position at a given time and the。Flatened。Yeah。So
    we yeah， take the outer product of those so every element in X can see every element
    of G flatten those out and that's go get to the government bed。😊，Let makes sense，
    sorry I should put that。Yeah， and then you do the same operation except you flatten
    with an identity in the。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要放入的记忆，所以这些模式，假设你这样说。将会是某种在给定时间位置的外积和。扁平化。是的。所以我们是的，取这些的外积，因此X中的每个元素都可以看到G中的每个元素，把它们扁平化，这样就能到达政府床。😊，让我们理清思路，抱歉，我应该加上那个。是的，然后你进行相同的操作，只不过是在扁平化时使用一个身份。
- en: let's say you're at a position youre a critical we're going to see， you set
    X to the identity。you do this operation that creates a very big vector from G。you
    put that in and you let it run its dynamics and it recalls the pattern and you
    like learn a network that like traces out the X from that。And the figures you
    show if you go down with good， yeah。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你处于一个关键的位置，我们将看到，你将X设置为单位矩阵。你进行这个操作，从G创建一个非常大的向量。你将其放入并让它运行其动态，它会回忆起模式，你就像学习一个网络，从中描绘出X。你展示的图形，如果你仔细查看，嗯，是好的。
- en: the it's it's hard to see but what's on the X axis and what's like what？
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这很难看清，但X轴上是什么，以及这是什么？
- en: Are you training a field network with this？s flattened out a product of the
    book yeah the actual the training that's going on is more in the structure of
    the world about because it has to learn those matrices all it gets told is which
    action type it's taken and it has to learn the fact that stepping east is the
    opposite stepping west so all of the learning of stuff is in those matrices learning
    to get the right structure。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你是在用这个训练一个领域网络吗？这本书的产品实际上是更关注于世界的结构，因为它必须学习那些矩阵。它被告知的只是它采取了哪种行动类型，它必须学习向东迈步是向西迈步的反义。因此，所有的学习都在那些矩阵中，学习如何获得正确的结构。
- en: 😊，There's also， I mean because the the hot film network learning the hot film
    Network will like re initialitialize every environment and you're like shoving
    them reason。😊，So it's less like that's less the bit of this screen， it's causing
    this certainly。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，还有，我是说因为热片网络学习，热片网络会像重新初始化每个环境，而你就像在推它们的原因。😊，所以这看起来少了一点，这是造成这种情况的原因。
- en: but it's not causing this like shift up， which is as training progresses in
    many different environments。you get better at the task because it's learning a
    structure the task。😡，Okay。and the link to I think this is all just modern hot
    network。so the initial paper was actually plastic cocktail networks， but yeah。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并没有导致像这种提升，这在许多不同环境中的训练过程中会出现。你在这个任务上会变得更好，因为它学习了任务的结构。😡，好的。还有我认为这都是现代热门网络的链接。所以最初的论文实际上是塑料鸡尾酒网络，但没错。
- en: now now the new versions of it are modern cocktail networks yeah right。and then
    insom much as modern cocktail networks equal attention。😊，This is a transport。But
    then you're okay， and then you have some there are some results looking at。Activations。Well
    these are recordings of the brain or these no these are actually in10。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在新版本的它是现代鸡尾酒网络，对吧？而且在某种程度上，现代鸡尾酒网络等于关注。😊 这是一个传输。但那样你没问题，然后你有一些结果可以看。激活。嗯，这些是大脑的记录，或者说这些实际上是
    in10。
- en: so this is the left ones and neurons in the G section in the middleentnal cortex
    part of 10。As you very position yeah。And we're going to get my last section is
    about how these1 is like and so we'll get to hopefully we'll be clear the link
    between to after that Okay we are happy with that no hopefully cool T is approximately
    equal to transform yeah so you seem you know all of this but I guess my notation
    at least we can clarify that you got your data which is maybe your like tokens
    coming in and you got your positional embedding and the positional embedding will
    play a very big role here that's the E and together they make this vector H okay
    and these arrive over time。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是左侧的神经元，以及中间部分的G区在内侧皮层的部分10。就像你的位置一样。然后我的最后一部分是关于这些1的样子，所以希望我们能清楚地了解之间的联系。好的，我们对这个没问题，希望很酷的T大约等于变换。是的，你似乎知道所有这些，但我想我的标记至少可以澄清一下，你得到了数据，也许是你的输入标记，然后你得到了位置嵌入，而位置嵌入在这里将发挥非常重要的作用，这就是E，它们共同形成了这个向量H，好吧，这些会随着时间的推移而到达。
- en: 😊，Yeah and you got your attention updates that you see some similarity between
    the key and the query and then you add weighted the values with those similarities
    we're all happy with that。😊，And here's the staff version。So the basic intuition
    about how these parts map onto each other is that the G is the position encoding
    as you may have being able to predict the x or the input tokens。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，是的，你得到了关注更新，你会发现关键字和查询之间有一些相似性，然后你将这些相似性加权值，我们都对此很满意。😊，这是工作人员版本。所以，这些部分如何相互映射的基本直觉是G是位置编码，正如你可能能够预测x或输入标记的那样。
- en: this guy when you put in the memory and you try and recall which memory is most
    similar to that's the attention part and maybe some yeah you you compare the current
    GT to all of the previous GTs and you recall the ones with high similarity structure
    and return the corresponding x I've still got 10 minutes's not okay。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当你把这个家伙放入记忆中，尝试回忆哪个记忆最相似时，这就是注意力的部分，也许还有一些，你将当前的GT与所有之前的GT进行比较，并回忆起结构相似度高的那些，然后返回相应的x，我还有10分钟，不行。
- en: 😊，Maybe some differences， or I think I'm going to go through this between how
    you would maybe like the normal transformer and how to make it map onto this the
    following。So the first of these is that the keys and the queries are the same
    at all time point。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，也许存在一些差异，或者我认为我将要讨论的是如何将普通变压器映射到以下内容。所以其中第一个是，在所有时间点上，键和值是相同的。
- en: so there's no difference in the matrix that maps from tokenness to keys and
    tokenqueries。same matrix， and it only depends on the position of encoding。Okay。so
    you only recall memories based on how similar their positions are。So yeah， this
    is。K at time tau equals query at time tau equals some matrix applied only to the
    position embedding at time。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 所以从标记性到键和标记查询的映射矩阵没有区别。同一个矩阵，且只依赖于编码的位置。好的。所以你只会根据它们位置的相似性来回忆记忆。因此，是的，这就是。K
    在时间 τ 等于查询在时间 τ 等于仅应用于时间位置嵌入的某个矩阵。
- en: Then the values depend only on this x part， so it's some like factorization
    of the two。which is that value at time tower is like some value metrics when you
    apply to that x part。so that's the only bit you want to learn。😊，Recall， I guess
    is that right， think that right。And then it's a causal transformer in that you
    only do attention at things that have arrived at time points in the past。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这些值仅依赖于这个 x 部分，所以这有点像两个部分的因式分解。时刻塔的值就像是应用于那个 x 部分时的一些值度量。所以这就是你想要学习的唯一部分。😊，回想一下，我想是这样，对吧，想得对吗？然后这是一个因果变换器，因为你只关注过去时间点到达的事物。
- en: Make sense。And finally， the perhaps like weird and interesting difference is
    that there's this path integration going on in the positional encodings。so these
    E are the equivalent of the grid cells， the G from the previous bit。and they're
    going to be updated through this matrix that depend on the actions you're taking
    the well。😊，Yeah， so that's basically the correspondence I'm going to go through
    a little bit about how the hottfield network is approximately like doing attention
    over previous tokens。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有意义。最后，也许奇怪而有趣的区别在于，位置编码中有路径积分的存在。所以这些E相当于网格细胞，G来自前面那部分。它们将通过依赖于你所采取的动作的矩阵进行更新。😊，是的，基本上这就是我将要讲述的对应关系，接下来我会稍微介绍一下霍特菲尔德网络如何大致类似于对之前的标记进行注意力处理。
- en: 😊，So yeah， I was describing to you before the classic hot field network。which
    if you remove the nonlineararity looks like this and the mapping。I guess is like
    the hippocampal activity， the the like current neural activity is the query。😊。The
    set of memories themselves are the key， you're doing this dot product to get the
    current similarity between the query and the key。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以是的，我之前在向你描述经典的热场网络。如果你去掉非线性，看起来是这样的，映射。我想这就像海马体的活动，当前的神经活动是查询。😊。记忆集合本身是关键，你正在进行这个点积，以获取查询和键之间的当前相似性。
- en: and then you're summing them up weighted by that dot product all of the memories
    that are values。😡。So that's a simple version。But actually these hop networks are
    quite bad they like in some senses。they tend to have fail， they have a like low
    memory capacity for n neurons。they have something they can only embed like 0。14
    n memories。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你将它们按点积加权求和，这些记忆都是值。😡。所以这是一个简单的版本。但实际上，这些跳跃网络在某些方面相当糟糕。它们往往会失败，对于n个神经元，它们的记忆容量很低。它们只能嵌入大约0.14
    n个记忆。
- en: just like a big result from statistical physics in the 80s。😊，But's okay， people
    have improved this。the reason that they're bad is it seems to be basically that
    the overlap between your query and the memories is too big for too too many memories。you
    know you basically like look too similar to too many things so how do you do that
    you like sharpen your similarity function。😊，Okay and the way we're going to sharpen
    it is through this function and this function is going to be soft。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 就像80年代统计物理的一个重大结果。😊不过没关系，人们对此进行了改进。导致它们不好的原因基本上是你的查询和记忆之间的重叠对于太多记忆来说太大了。你知道，你基本上看起来与太多事物太相似。那么你该如何处理呢？你需要增强你的相似性函数。😊好的，我们将通过这个函数来增强它，而这个函数将是柔和的。
- en: so it's going to be like oh， how similar am I to this particular pattern weighted
    expentiated and then over how similar am I to all the other？
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这将会是“哦，我与这个特定模式有多相似，经过加权的指数运算，然后我与其他所有模式又有多相似？”
- en: 😊，That's our new measure of similarity， and that's the minus sign of the modern
    hot field one。😊，Yeah。😊，And then you can see how this thing， yeah， it's basically
    doing the attention mechanism。😊。and it's also biologically plausible we'll quickly
    run through that is that you have some set of activity PT。this like neural activity
    and you're going to compare that to each chiute and that's through these memory
    neurons so there's a set of memory neurons one for each pattern you've memorized
    mu。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，这是我们新的相似性度量，这是现代热门领域的负号。😊，是的。😊，然后你可以看到这个东西，是的，它基本上在做注意力机制。😊。而且它在生物上是合理的，我们会快速浏览一下，你有一些活动
    PT。这就像神经活动，你要将其与每个 chiute 进行比较，这通过这些记忆神经元来实现，因此有一组记忆神经元，每个神经元对应你记住的模式 mu。
- en: 😊，And the weights to this memory neuron will be this chiyute。and then the activity
    of this neuron will be this dot product。😡。And then you're going to do divisive
    normalization to run this operation between these neurons。so like to make them
    compete with one another and only recall the memories that are most similar through
    most activated according to this like softm operation and then they'll project
    back to the PT and produce the output by summing up the memories weighted by this
    thing times is the kimu which is the weights so then weights out to the memory
    neurons and back back to their P the hippocampus are both ku。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，这个记忆神经元的权重将是这个chiyute。然后这个神经元的活动将是这个点积。😡。接下来你将进行除法归一化，以便在这些神经元之间运行这个操作，让它们相互竞争，只回忆那些最相似的记忆，通过根据这个softm操作激活程度最高的记忆，然后它们会投射回PT，并通过将加权的记忆相加来产生输出，这个权重乘以kimu，也就是权重，然后输出到记忆神经元，再返回到海马体，都是ku。
- en: And so that's how you can like biologically possiblyibly run this modern hot
    film network。And so so yeah thoughts what the memories that are over probably
    not yeah I guess somehow you have to have knowledge you know in this case it works
    nicely because we like wipe this poor agent memory every time and only memorize
    things from the environment and so you need something that like gates it so that
    it only looks for things in the current environment somehow how that happens I'm
    not sure there are claims that there's this like just shift over time the claim
    is basically that like somehow as time passes the representation is just so they
    like rotate or something and then they're also embedding something like a time
    similarity as well because the closer in time you are the more you're like in
    the same rotated thing so maybe that's a mechanism to like you know past a certain
    time you don't have cool things。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这就是你如何在生物学上可能运行这个现代热门电影网络的方式。然后，嗯，记忆的想法过得可能不太好，我想不管怎样，你必须有知识。你知道，在这种情况下，它运作得很好，因为我们每次都会抹去这个可怜的代理的记忆，只记住环境中的事物，因此你需要某种东西来控制它，使其只查找当前环境中的事物。我不太确定这怎么发生，有人声称这就像是随着时间的推移发生了某种转变，基本上这个说法是，随着时间的推移，表示方式就像旋转一样，然后他们还嵌入了一些时间相似性，因为你越接近时间，你就越在同一个旋转的事物中。所以，也许这是一种机制，你知道，过了一段时间你就不会有酷炫的东西。
- en: 😊，But the evidence and debate a lot around that other mechanisms like it， I'm
    sure。😊。Maybe context is another one actuallyley we briefly talk about that you
    know。if you know you're in the same context then you can send a signal like somehow
    in the prefrontal code can like work out what kind of setting in my end you can
    send that signal back and be like。oh， make sure you attend to these one that are
    in the same context。😊，So yeah there we go。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，但是关于其他机制的证据和争论很多，我相信。😊。也许上下文实际上是另一个，我们简要讨论过，如果你知道自己处在同一个上下文中，那么你可以发送一个信号，就像前额叶编码可以推断出我这边是什么样的设置，你可以把那个信号发回去，像是。哦，确保你关注这些处于同一上下文中的事物。😊，所以，是的，就这样。
- en: ti transformformer， that's the job， it path integrates position encodings which
    is kind of fun a compute similarity using these positional encodings and it only
    compares to past memories。but otherwise it looks a bit like a transformer setup。😊，And
    here's a set out we are our MEc， LEC。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ti transformformer，这是工作，它的路径集成了位置编码，这有点有趣，使用这些位置编码计算相似性，并且它只比较过去的记忆。但否则它看起来有点像变压器设置。😊，这是我们的MEc，LEC的设置。
- en: hippocampus and places。Some yes， so here's a brief。the last thing I think I'm
    going to say is it like。😊。This extends T nicely because it allows it previously
    you have to do this outer product and flatten but the very dimensionality is like
    terrible scaling with like for example you want to do position what I saw and
    the context signal something after like outer product three vectors and flatten
    that as's much much bigger you're scaling like rather than what you'd like to
    do is just like3M and so this version of TM with this new modern hotfield network
    does scale nicely to adding a context input as just another input in what was
    previously this like modern hot field network。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 海马体和地方。有些是的，所以这是一个简要说明。我最后要说的就是这样的。😊。这很好地扩展了T，因为它允许你之前必须进行外积并扁平化，但这种维度的扩展真是可怕，比如说你想做位置，我看到的上下文信号是在像外积三个向量并扁平化后，像这样扩展得大得多，你的扩展比你想做的要大，而这个TM版本结合这个新的现代热场网络，确实很好地扩展到了将上下文输入作为以前这个现代热场网络中的另一个输入。
- en: 😊，There's some， so yeah， our conclusions is there's like。Proved somewhat interesting
    as a twoway relationship from the AI to the neurosci。we use this new memory model，
    this modern hot field network that has all of you know all of this bit is supposed
    to be in the hippocampus。whereas previously we just had these like memory bits
    in the classic hot field network in the hippocampus。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，有一些，所以我们的结论是，这是一种双向关系，从人工智能到神经科学。我们使用这个新的记忆模型，这个现代热门领域的网络，所有这些知识都应该存在于海马体中。以前我们只是在经典的热门领域网络中有这些记忆单元。
- en: so it makes kind of interesting predictions about different place cell structures
    in the hippocampus and it just sped up the code not right。😊，From the neurodo AI
    maybe there's some a few things that are slightly different just like learnable
    recurrent position encod so people do some of this I think they get like position
    encodings and learn RN that updates them but maybe this is like some motivation
    to try for example they don't weight matrices and these weight mixes are very
    biased towards because they're invertible generally and think about that they're
    very bias towards representing very clean structures like 2D space so might you
    know interesting there the other thing is this is like one attention layer only
    and so like somehow by using。
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这对海马体中不同地方细胞结构做出了有趣的预测，而且代码加速得不太对。😊，从神经网络AI来看，也许有一些略有不同的地方，比如可学习的递归位置编码，所以人们会进行一些尝试，我认为他们会得到位置编码，并学习更新的RN，但也许这会激励他们尝试，比如他们没有权重矩阵，而这些权重混合通常偏向于表示非常干净的结构，比如二维空间，所以在这里可能会很有趣。另一个方面是这只有一个注意力层，因此通过使用。
- en: 😊，Nice extra recommendations making the task very easy in terms of like processing
    X and using the right position encoding。you've got it to solve the task with just
    one of these。😊。Also kind of nice and maybe it's like a nice interpretation is
    that you can go in and really probe what these neurons are doing this network
    and really understand you know we know that the position encoding looks like grid
    cells we have a very deep understanding of why grid cells are a useful thing to
    have if you're doing this part integration it was like hopefully helps like interpret
    all these things oh yeah and if there was a term I was going to tell you all about
    that grid cells which might hobby horse but I don't think there's time so I'll
    stop that。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，很棒的额外推荐，使得处理 X 和使用正确的位置编码变得非常简单。你可以仅用其中之一来解决这个任务。😊。还有一种不错的，或许是一种很好的解释，就是你可以深入探究这个网络中这些神经元在做什么，并真正理解我们知道的位置编码看起来像网格细胞，我们对网格细胞为什么在进行部分积分时是有用的有很深的理解，希望这有助于解释所有这些事情。哦对了，如果我有一个术语要告诉你关于网格细胞的事情，可能有点跑题，但我觉得没时间了，所以我就不继续了。
- en: 😊，开始。好。Questions。very questions so in the very those breeds are linked to one
    neuro or these yeah that's one neuro response Yeah let me tell you more about
    the grid cell system because you because your electrode stuck in here right and
    they generally have like the classic measuring techniques a tro which is four
    wires okay and they receive these spikes which like electrical fluctuations as
    a result of a neuron firing and they can like triangulate that that particular
    spike they measured because of the pattern of activity on the four wires has to
    have only come from one position so they can work out which neuronsn sent that
    particular spike。
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，开始。好。问题。非常多的问题，这些品种与神经或这些有关系，是的，那是一个神经反应。是的，让我告诉你更多关于网格细胞系统的知识，因为你的电极卡在这里，对吧？他们通常使用经典的测量技术，一个四线电极
    okay，它们接收这些尖峰，就像神经元发射的电波波动，它们可以三角定位那个特定的尖峰，因为在四根电线上的活动模式必须仅来自一个位置，因此它们可以算出是哪个神经元发送了那个特定的尖峰。
- en: 😊，Yeah but there's so there's a set of neurons that have group cell patterns
    lots of neurons have patterns that are just translated versions of one another
    so the same grid like shifted in space that's called a module and then there are
    a set of modules which are the same types of neuronsn but with a lattice that's
    much bigger or much smaller and in wraps that's roughly seven so there's a very
    surprising crystalline structure of these seven modules within each module each
    neuron is just translated by one which yeah there's a lot of theory work about
    why that's a very sensible thing to do if you want to do part integration of workout
    where you are in the environment based on your like velocity signals。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，是的，但有一组神经元具有群体细胞模式，许多神经元的模式只是彼此的翻译版本，因此相同的网格像是在空间中移动，这被称为模块，然后有一组模块，它们是相同类型的神经元，但具有更大或更小的晶格，大约是七个，所以在每个模块内有七个模块形成了一个非常惊人的晶体结构，每个神经元只是由一个翻译，是的，如果你想根据你的速度信号进行环境中的部分集成，这是一个非常合理的理论工作。
- en: 😊，Al。So this thing that you said this was like really fascinating about that
    friendly thing。And it's product fit or a product of learning。Evolution， it's like
    it emerges like。1 days after in a baby rat's life after being born so or suddenly
    that structure seems to be like very biased to being created unclear you know
    we were talking about how it was being co-opted to encode other things and so
    it's debatable how flexible it is or how hard is。
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，Al。这件你说的事情真是太吸引人了，关于那种友好的东西。它的产品适配性或学习的产物。进化，就像它在出生后1天就开始出现，所以那个结构似乎非常偏向于被创造得不清晰。你知道，我们在谈论它是如何被共同利用来编码其他东西的，所以关于它的灵活性或难度是有争议的。
- en: but it seemed you know we were that the fMRI evidence suggests that there's
    some like more flexibility in the system unclear quite how it's codingding it。but
    it'd be cool to get neural recordings of it and。😊，That。Excell， let's give a finger
    down round。
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 但是似乎你知道，我们的fMRI证据表明，系统中有一些更大的灵活性，但不太清楚它是如何编码的。可是，获取它的神经记录会很酷。😊 那。太棒了，让我们来一次手指下降的轮换。
- en: '![](img/2cf4720874569984260c34088d0a5835_9.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_9.png)'
- en: '![](img/2cf4720874569984260c34088d0a5835_10.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2cf4720874569984260c34088d0a5835_10.png)'
