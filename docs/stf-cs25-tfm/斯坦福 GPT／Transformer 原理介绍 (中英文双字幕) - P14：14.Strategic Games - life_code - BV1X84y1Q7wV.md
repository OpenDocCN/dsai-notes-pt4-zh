# ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P14Ôºö14.Strategic Games - life_code - BV1X84y1Q7wV

![](img/e3abef98963297997c09e5d05b736a64_0.png)

![](img/e3abef98963297997c09e5d05b736a64_1.png)

So it spend sometimes like two months training the bots„ÄÇOn thousands of CPUs„ÄÇ

Tabytes and memory sometimesÔºå but when it came time to actually play against the humans„ÄÇ

 they would act almost instantlyÔºå it was just a lookup table„ÄÇAnd the humans„ÄÇ

 when they were in a tough spotÔºå they would not act instantly„ÄÇ

 they would think they would sit there and they would think for five seconds„ÄÇ

 maybe five minutes if it was a really difficult decision and it was clear that that was allowing them to come up with better strategies„ÄÇ

And so I wanted to investigate this behavior in our bots like if we could add this to our bots„ÄÇ

 how much of a difference would it makeÔºå the ability to instead of acting instantly to take some time and compute a better strategy for the spot that the agent was in„ÄÇ

And this is what I found„ÄÇSo on the X axis here we have like the number of buckets„ÄÇ

 the number you can think of this as like the number of parameters in your model and on the Y axis we have distance from national equilibrium so this is basically like how much you would lose towards worstk's adversary so the lower this number is the better your poker bot is and you can see as you scale the number of parameters„ÄÇ

 your performance improves and as you increase the number of parameters by about100 x you know your exploitability goes down by about half and indeed you're getting a much better poker bot„ÄÇ

But you can see the blue line here is if you don't have search and the orange line is if you do add search„ÄÇ

üò°ÔºåAnd you can see just adding searchÔºå adding the ability to sit there and think for a bit„ÄÇ

 improve the performance of these models„ÄÇAnd it reduced the exploitability„ÄÇ

 the distance from ra equilibrium by about 7 x„ÄÇAnd if you were to extend that blue line and see how many prim would you need in order to be comparable to adding search„ÄÇ

 the answer is you would need to scale up your model by about 100Ôºå000 x„ÄÇOkay„ÄÇ

So this was pretty mind blowlowing to me when I saw thisÔºå I meanÔºå over the course of my PhD„ÄÇ

 the first three yearsÔºå first three or four years of my PhD„ÄÇ

 I' managed to scale up these models by about 100 x„ÄÇÂóØ„ÄÇAnd I was proud of thatÔºå I mean„ÄÇ

 that's like a pretty pretty impressive resultÔºå I think„ÄÇ

But what this plot was showing me was that just adding search„ÄÇ

Was the equivalent of scaling things up by about 100Ôºå000 x„ÄÇ

 and so all of my previous research up until this point„ÄÇ

Would just be a footnote compared to adding search„ÄÇSo„ÄÇWhen I saw thisÔºå it became clear„ÄÇ

 this was the answer to beating top humans in pokerÔºå and so for the next yearÔºå basically nonstop„ÄÇ

 I worked on scaling search„ÄÇNow there's a question that naturally comes up which is why wasn't this considered before there's a few factors so first of all I should say like the search had been considered in poker before and it's actually quite natural to say like well you know if you had search and chess and search and go like why would you not consider search in poker„ÄÇ

There's a few reasonsÔºå one is that„ÄÇCulturally like the poker research grew out of game theory and reinforcement learning and so it wasn't really from the same background as like the people are working on chess and working on go„ÄÇ

When you scale search scaling test time compute it makes all your experiments much more expensive and just like more unpleasant to work with and there are just like incentive structures I mean people are always thinking about winning the next annual computer poker competition and the ACPC limited the resources that you could use at test time so search wasn't really possible effectively in the ACPC„ÄÇ

And I think the biggest factor is that people just didn't think it would make such a huge difference I mean I think it's reasonable to look at something like search and think like oh yeah that might make like a 10 X difference you probably wouldnt think it makes a 10000 x difference and so there were some people working on it but it wasn't really the focus of a lot of people's research„ÄÇ

So anywayÔºå focused on scaling search and that led to the 2017 Bras versus AIT competition where we again played our bot against four top poker pros„ÄÇ

 120Ôºå000 hands of poker $200Ôºå0000 prize money and this time the bot won by 15 big blinds per100 instead of nine big blinds per100„ÄÇ

This was a crushing victoryÔºå each human lost individually to the bot and„ÄÇ



![](img/e3abef98963297997c09e5d05b736a64_3.png)

Four standard deviations of statistical significance„ÄÇ

We followed this up in 2019 with six player poke AI competition„ÄÇ

The big difference here is that we figured out how to do depth limited search„ÄÇ

 so before the 2017 bot it would always have the search to the end of the game here it only had to do search a few moves ahead and it could stop there„ÄÇ

And so this time againÔºå it wont with statistical significance„ÄÇ

 and what's really surprising about this spot is that despite it being a much larger game„ÄÇ

 the six player Pokerbot Plebuvis cost under $150 to train on cloud computing resources and it runs on 28 CPU cores at inference time„ÄÇ

 there's no GPUs„ÄÇ

![](img/e3abef98963297997c09e5d05b736a64_5.png)

So I think what this shows is that„ÄÇThis really wasn an algorithmic improvement„ÄÇ

 I mean this would have been doable 20 years ago if if people knew how to do it„ÄÇ

And I think it also shows the power of searchÔºå you know„ÄÇ

 if you can figure out how to scale that compute at test time„ÄÇ

 it really can make a huge difference and bring down your your training costs by a huge amount„ÄÇËØ∂„ÄÇüòä„ÄÇ

OkayÔºå anywayÔºå yeahÔºå so I wanted to say also this is not limited to poker if you look at go you see a similar pattern„ÄÇ

 so this is a plot from the alphago zero paper„ÄÇOn the X axis„ÄÇ

 we have different versions of alphaphago and on the y axisÔºå we have elo rating„ÄÇ

 which is a way of comparing different botsÔºå but also a way of comparing bots to humans„ÄÇ

And you can see if„ÄÇOkayÔºå so superhuid performance is around 3Ôºå600 elo„ÄÇAnd you can see Alpha Go Lee„ÄÇ

 the version that played against Li Addal in 2016Ôºå that's right over the line of superhuman performance„ÄÇ

Alphago is zeroÔºå the strongest version of alphaphago is around 5Ôºå200 elo„ÄÇ

But if you take out the test time searchÔºå if you just play according to the policy net and not do any modc research in alphago was zero at test time„ÄÇ

 then the eO rating drops to around 3Ôºå000Ôºå which is substantially below expert human performance„ÄÇ

So what this shows is that if you take out Montegli research at test time„ÄÇüò°„ÄÇ

The alphaphaGo zero is not superhuman and in fact nobody has made a superhuman gobot that does not use search in some form„ÄÇ

 nobody has made a raw neural network that can beat top humans in go„ÄÇAnd I should say also„ÄÇ

 this is just if you're taking out the search at test time„ÄÇ

 I'm not even talking about taking it out of training timeÔºå if you took it out of training time„ÄÇ

 we wouldn't even got off the ground„ÄÇNow there's a question of like okay well„ÄÇ

 surely you could just scale up the modelÔºå scale up the matter of training and you would eventually you know surpass the humanment performance and match the performance if you added search and that's true yes„ÄÇ

 if you scale up the models and if you scale up the training„ÄÇ

 then you would eventually match the performance with search„ÄÇ

 but there's a question of like how much would you have to scale it up by„ÄÇNow„ÄÇ

 a rough rule of thumb is that in order to increase your eO rating by about 120 points„ÄÇ

 you either have to double the amount of model size and training or you have to double the amount of test time search„ÄÇ

And so if you look at that gap of around 2000 elow points and you calculate the number of deadlings that you would need„ÄÇ

 the answer is that in order to get the raw policy net from 3Ôºå000 e to 5200 elow„ÄÇ

 you would need to scale your model and your training by about 100Ôºå00x„ÄÇOkay„ÄÇ

 so why is this importantÔºüüò°ÔºåI think you look at what's happening today with large language models and transformers and and you see something similar„ÄÇ

 I meanÔºå you're getting„ÄÇHuge there's a question of like what do I mean by search there's a specific kinds of search like multiult research„ÄÇ

 the ability to just like plan ahead what you're going to do instead of just like acting instantly based on your precomputed policy„ÄÇ

 but really what I mean by search more broadly is the ability to scale the amount of computation to get better performance„ÄÇ

I think that's the real value that search is adding instead of just like acting according to your precomputd„ÄÇ

You knowÔºå front loading all of your computations so you're doing everything all your computation ahead of time and then like at inference time acting basically instantly„ÄÇ

Could you get a better solution if you had five minutes to output an action instead of 100 millisecondsÔºü

Okay„ÄÇSo„ÄÇYeahÔºå I think you look at„ÄÇI'm sorryÔºå there's a question„ÄÇ

 does a transformer with a search circuit count as search„ÄÇ

 or do you mean and engineering search algorithmsÔºüI don't want to get bogged down into like the details of like how to do this because like the answer is nobody really knows yet„ÄÇ

 nobody really has a general way of doing search and all the domains that we've done search successfully like poker and go it's done in a fairly domain specific way„ÄÇ

 you know go use this algorithm called multiola research„ÄÇAnd yeah„ÄÇ

 you could think of beamS searcharch as like one simple form of searchÔºå but„ÄÇ

 it does seem like there should be betterÔºå better ways in the future„ÄÇYeah„ÄÇSo anyway„ÄÇ

 where I'm going with this is like you look at how large language models are being trained today and you know„ÄÇ

 you're seeing millions of dollars being thrown at pre training like„ÄÇ

I wouldn't be surprised if we see a large language model that would cost like $100 million to train„ÄÇ

We might even get to a billion dollars„ÄÇBut„ÄÇThe inference cost is still going to be very small„ÄÇ

And so there's a question of likeÔºå could you do substantially better if you couldÔºü

Scall the amount of inference cost as well„ÄÇMaybe that could like amortize some of your training cost„ÄÇ

Yeah„ÄÇOkayÔºå so there's this„ÄÇThere's this lecture called the Bi lessons by Richard Sutton that says the biggest lesson that can be learned„ÄÇ

 and so it's a really great essay I recommend reading it„ÄÇ

 but one of the big takeaways is like he says the biggest lesson that can be learned from over 70 years of AI research is that general methods that leverage computation are ultimately the most effective„ÄÇ

The two methods that seem to scale arbitrarily in this way are search and learning„ÄÇNow„ÄÇ

 I think we've done a great job with generalizing searchÔºå sorryÔºå generalizing learning„ÄÇ

 and I think there's still room for improvement when it comes to search„ÄÇU and yeah„ÄÇ

 the next goal really is about generalityÔºå like can we develop a truly general way of scaling inference compute instead of just doing things like Montecalo research that are specific to a better domain to a specific domain„ÄÇ

And also better than things like chain of thought„ÄÇÂóØ„ÄÇ

What this would look like is that you have much higher test time compute„ÄÇ

But you have much more capable models„ÄÇAnd I think for certain domains that trade off is worth it„ÄÇ

 like if you think about what inference cost we're willing to pay for proof of the Raymond hypothesis„ÄÇ

 I think we'd be willing to pay a lot„ÄÇOr you knowÔºå the cost of„ÄÇ

What cost were we want to pay for new life saving drugsÔºå I think we'd be willing to pay a lot„ÄÇ

So I think that there is an opportunity here„ÄÇOkayÔºå anywayÔºå so that's why I prelude„ÄÇ

 I guess any questions about that before I move on to CiceroÔºüBy the way„ÄÇ

 the reason why I'm talking about this is because it's going to inform„ÄÇüò°„ÄÇ

The approach that we took to CiceroÔºå which I think is quite different from the approach that a lot of other the researchers might have taken to this problem„ÄÇ

Someone asked can you give an example of search well multicarural research is one form of search„ÄÇ

 you could also think of like breadth for search depth for search these kinds of things they're all search I would also argue that like chain of thought is doing something similar to search where it it's allowing the model to like leverage extra compute at test time to get better performances„ÄÇ

But I think that that's the main thing that you want„ÄÇ

 the ability to like leverage extra compute at a test time„ÄÇ

What's the search what's the space that you are searching over again like in in a game like go it's like different board positions„ÄÇ

 but you could also imagine searching over like you know different sentences that you could say things like that„ÄÇ

There's a lot of flexibility there as well„ÄÇOkayÔºå so now I want to get into Cicero so first thing I should say when it comes to Cicero„ÄÇ

 this is„ÄÇA big team effort„ÄÇThis was like this is actually one of the great things about working on this project that there was just such a diverse talent pool„ÄÇ

 experts in reinforcement learningÔºå planningÔºå game theoryÔºå natural language processing„ÄÇ

 all working together on this and it would not have been possible without without everybody„ÄÇ

So the motivation for diplomacy actually came from 2019„ÄÇ

 we were looking at all the breakthroughs that were happening at the time and I think a good example of this is this XKCD comic that came out in 2012 that shows like different categories games„ÄÇ

 games that are solvedÔºå games where computers can beat documents„ÄÇ

 games where computers still lose documents and games where computers may never outplay documents„ÄÇ

And in this categoryÔºå computer still lose to top humans you had four gamesÔºå go Arima„ÄÇ

 poker and Starcraftft„ÄÇIn 2015„ÄÇüòäÔºåActuallyÔºå one of my colleagues„ÄÇ

 David Wu made the first day out to beat Top humans in arema„ÄÇ

 in 2016 we have Algo beating Lisa do and Go„ÄÇIn 2017„ÄÇ

 you have the work that I just described where we beattop humans in poker„ÄÇAnd in 2019„ÄÇ

 we had Alpha star beating X humans in Starcraftft„ÄÇ

So that shows the incredible amount of progress that had happened in strategic reasoning over the past several years„ÄÇ

 leading up to 2019„ÄÇAnd at the same timeÔºå we also had GP2 come out in 2019„ÄÇ

 and it showed that language model like„ÄÇAnd natural language processing was progressing much faster than I think a lot of people„ÄÇ

 including us expected„ÄÇAnd so we were thinking about what after the sixth player poker work I was discussing with my colleagues„ÄÇ

 what should we work on nextÔºüAnd we were throwing around like different domains to work on„ÄÇ

Given the incredible amount of progress in AIÔºå we wanted to pick something really ambitious„ÄÇ

 something that we thought„ÄÇYou couldn't just tackle by scaling up the existing approaches that you really needed something new in order to address„ÄÇ

And we landed on diplomacy because we thought that it would be the hardest game to make an AI for„ÄÇ

So what is diplomacyÔºüDiploomacy is a natural language strategy game„ÄÇ

 it takes place right before World War I you play as one of the seven great powers of Europe„ÄÇ

 EnglandÔºå FranceÔºå GermanyÔºå AustriaÔºå Russia and Turkey„ÄÇ

 and your goal is to control a majority of the map in practice that rarely happens like if you control a majority of a majority of the map and you've won in practice nobody ends up winning outright and so your score„ÄÇ

Is proportional to the percentage of the map that you controlÔºü

Now what's really interesting about diplomacy is that it is a natural language negotiation game„ÄÇ

 so you have these conversations like what you're seeing here between Germany and England where they will probably communicate with each other before making their moves and so you can have Germany ask like want to support Sweden„ÄÇ

 England says let me think on that and so on„ÄÇSo this is a popular strategy game developed in the 1950s„ÄÇ

 it was JFK and Kisser's favorite game actually„ÄÇAnd like I said„ÄÇ

 each chart involves sophisticated private natural language negotiations„ÄÇ

 and I want to make clear like this is not„ÄÇNegotiations like you would see in a game like Sotos and Katan„ÄÇ

 for exampleÔºå you're seeing„ÄÇIt's much more like survivorÔºå if you've ever seen a TV show survivor„ÄÇ

 you have discussions around like alliances that you'd like to build discussions around like specific tactics that you'd like to execute on the current current and also you know like more long- term strategy around like where do we go from here and how do we divide resources„ÄÇ

Now the way the game worksÔºå you have these negotiations that last between five and 15 minutes depending on the version of the game„ÄÇ

 and then on each term and all these negotiations are done privately in parawise negotiation„ÄÇ

 also I think that you are not mutedÔºå okay thank you„ÄÇAnd then after the negotiation period completes„ÄÇ

 everybody will simultaneously write down their moves„ÄÇüò°„ÄÇ

And so a player could promise you something like I'm going to support you into this territory this term„ÄÇ

 but then when people actually write down their moves„ÄÇ

 they might not write that down and so you only find out if they were true to their word when all the moves are revealed simultaneously„ÄÇ

And so for this reasonÔºå alliances and trust building is key„ÄÇ

 the ability to trust that somebody's going to follow through on their promises„ÄÇ

 that's really what this game is all about and the ability to convince people that you are going to follow through on your promises is really what this game is all about„ÄÇ

And so for this reason„ÄÇDiploacy has long been considered a challenge problem for AI„ÄÇ

 There research in the game going back to the 80sÔºå the research really only picked up„ÄÇ

It picked up like quite intensely in starting in 2019 when researchers from DeepMdÔºå ourselvesÔºå MiA„ÄÇ

 other places started working on this„ÄÇNow a lot of that research„ÄÇ

 the vast majority of that research actually was focused on the non languageguage version of the game„ÄÇ

 which was seen as a stepping stone into the full natural language version„ÄÇ

 though we decided to focus from the start on the full natural language version of the game„ÄÇ

So to give you a sense of like what these negotiations and dialogue look like„ÄÇ

 here is one example so here„ÄÇEnglandÔºå you can see they move their fleet in Norway to St„ÄÇ

 Petersburg and that occupies the Russian territory„ÄÇ

And so this is what the board state looks like after that move and now there's this conversation between Austria and Russia„ÄÇ

 Austria saysÔºå well what happened up north Russia says England stabbed I'm afraid the end may be close from you my friend Austria says yeah that's rough are you going to be okay up there Russia says I hope so England seems to still want to work together Austria says can you make a deal with Germany so the players are now discussing like what should be discussed with other players Russia says good idea then Austria says if we find as long as you can defend Sevetopol so Sevesttopol is this territory down to the south you can see that Turkey has a fleet and an army in the Black Sea in Armen next to Sevestoppo and so they could potentially attack that territory next turn„ÄÇ

ÂóØ„ÄÇüòäÔºåAustria says can you support hold Sevetooles Ukraine and Romania I'll support hold Romania„ÄÇ

 Russia says yepÔºå they'm already doing soÔºå Austria says awesome„ÄÇ

 hopefully we can start getting you back on your feet„ÄÇ

So this is an example of the kinds of conversations that you'll see in a game of diplomacy in this conversation„ÄÇ

 Austria is actually our bot Cicero„ÄÇüòäÔºåSo that kind of gives you a sense of the sophistication of the agent dialogue„ÄÇ

ËØ∂„ÄÇOkay I'll skip this for okay so I guess I'll go into this I don't want to take up too much time really what makes diplomacy interesting is that support is key so here for example„ÄÇ

 Budapest and Warsaw the red and the purple units both try to move into glacia and so since it's a one verse one they both bounce back and now they was into the territory in the middle panel you can see Vienna supports Budapest into glacciia and so now it's a two verse one and that red unit will indeed enter Galacciia„ÄÇ

And what's really interesting about the po is that it doesn't just have to be your own units that are supporting you„ÄÇ

 it could be another player's units as wellÔºå so for example„ÄÇ

 the green player could support the red player into glacciia and then that ready unit would still go in there„ÄÇ

So support is really what the game is all about negotiating over support and so for that reason diplomacy has this reputation as the game that ruins friendships it's really difficult to have an alliance with somebody for three or four hours and then have that„ÄÇ

üòäÔºåHave them backstb you and basically is ruining your game„ÄÇ

But if you talk to expert diplomacy playersÔºå they view it differently„ÄÇ

They say diplomacy is ultimately about building trust in an environment that encourages you to not trust anyone„ÄÇ

And that's why we decided to work on the gameÔºå you know„ÄÇ

 could we make an AI that is able to build trust with players in an environment that encourages them to not trust anybody can thebo like honestly communicate that it's going to do something and and evaluate whether another person is being honest when they are saying that they're going to do something„ÄÇ

Okay so„ÄÇW diplomacyÔºå it sits in this nice intersection of reinforcement learning and planning„ÄÇ

 and also natural language„ÄÇThere's two perspectives that we can take on why diplomacy is a really interesting domain one is the multiedging perspective so here„ÄÇ

üò°ÔºåAll the previous game we have results like chess go poker„ÄÇ

 these have all been in purely zero sum two player zero sum domains„ÄÇ

 and in these domains selfplay is guaranteed to converge to an optimal solution basically what this means is you can start to having the bot play completely from scratch with no human data„ÄÇ

And by playing against itself repeatedlyÔºå it will eventually converge to this unbeatable optimal solution called the mini Max equilibrium„ÄÇ

But that result only holds in two players zero sum„ÄÇ

 that whole paradigm only holds in two players zero sum games„ÄÇ

When you go to domains that involve cooperationÔºå in addition to competition„ÄÇ

Then success requires understanding human behavior and conventions you can't just treat the other players like machines anymore„ÄÇ

 you have to treat them like humansÔºå you have to model„ÄÇüò°ÔºåHuman irrationalityÔºå human subbotality„ÄÇ

 one example of this is actually language like„ÄÇYou can imagine if you were to train a bo completely from scratch in the game of diplomacy„ÄÇ

Like the full natural language version of the game„ÄÇ

 there's no reason why the bot would learn to communicate in English„ÄÇ

 it would learn to communicate in some weird gibberish robot language„ÄÇ

 and then when you stick it in a game with six humans„ÄÇ

 it's not going to be able to cooperate with them„ÄÇüò°ÔºåÂóØ„ÄÇ

So we have to find a way to incorporate human data and be able to learn how humans behave in order to succeed in this game„ÄÇ

üò°ÔºåËØ∂ ok k „ÄÇThere's also the NLP perspective„ÄÇWhich is that current language models„ÄÇ

Are essentially just imitating human like text„ÄÇ Now there's been some progress with things like RLHF„ÄÇ

 but„ÄÇThat's still like not really the way that humans communicate they communicate with an intention in minds right they come up with this intention and then they communicate with the goal of communicating that intention and they understand that others are trying to do the same„ÄÇ

üò°ÔºåAnd so there's a question of likeÔºå can we move beyond shitt chatÔºüTo grounded intentional dialogue„ÄÇ

So Cicero is an AI agent for diplomaacy that integrates high level strategic play and open domain dialogue„ÄÇ

And we use 50Ôºå000 human games of diplomacy acquired through a partnership with the website webdplomacy„ÄÇ

net„ÄÇSo we entered Cicero in an online diplomacy league just to give you the results up front„ÄÇ

 Cicero was not detected as an AI agent for 40 games with 828 players„ÄÇ

 there was one player that mentioned after the fact that like they kind of like made a joke about us being a bot but they didn't really follow up on it and nobody else followed up on it and they later accuseduse somebody else of also being a bot so we weren't sure how seriously to take that accusation but I think it's sad to say it made it through all 40 games not being detected as bot and then we in fact we told the players afterwards that it was a bot the whole time these are the kinds of responses that we got people were quite surprised„ÄÇ

 pleasantly surprised fortunatelyÔºå nobody was was upset with us but they were quite surprised that„ÄÇ

There was a bot that had been playing this game with them the whole time„ÄÇSo in terms of results„ÄÇ

 Cicero placed in the top 10% of playersÔºå it's a high variance game and so if you look at players that played five or more games that played second out of 19„ÄÇ

 and it achieved more than double the average human score„ÄÇ

So I would describe this as a strong level of human performance„ÄÇ

 I wouldn't go as far as to say that this is superhuman by any means„ÄÇ

 but it is currently quite a strong result„ÄÇNowÔºå to give you a picture of how Cicero works„ÄÇSo„ÄÇ

The input that we feed into the model is the board state and the recent action history that's shown on the top left here„ÄÇ

 and also the dialogue that it's had with all the players up until now„ÄÇ

So that's going to get fed into a dialogue conditional action model that's going to predict what Cicero thinks all the players are going to do this turn and what they think we will do this term„ÄÇ

These„ÄÇThese lead to what we call anchor policies that are then used for planning„ÄÇüò°ÔºåÂóØ„ÄÇNow„ÄÇ

 planning here„ÄÇAgainÔºå this is like the part where we leverage extra compute at test time in order to get better performance„ÄÇ

So essentially we take these initial predictions of what everybody's going to do„ÄÇ

 what are called anchor policies„ÄÇAnd we improve upon these predictions using this planning process called pickle„ÄÇ

 where basically we account for the fact that players„ÄÇ

Willll pick actions that have higher expected value with higher probability we're essentially adding this like rationality prior to all the players to assume that they're not going to blunder as often as the model might suggest and they're going to pick smarter actions with higher probability than the initial model might suggest„ÄÇ

And what we find is that this actually gives us a better prediction of what all the players will do than just relying on the raw neural net itself„ÄÇ

This gives us the action that we actually play in the game„ÄÇ

 and it also gives us what we call intense„ÄÇSo intents are an action for ourselves and an action for the dialogue partner that we're speaking to„ÄÇ

And now we have this dialogue conditional so we have we have this dialogue model that conditions on these intents„ÄÇ

 so the intents are fed into the dialogue model along with the board state and action history and also the dialogue that we've had so so far and that dialogue model will then generate„ÄÇ

Candidate messages that„ÄÇAre conditional on those intents„ÄÇ

These candidate messages go through a series of filters that filter out nonsenseÔºå grounding issues„ÄÇ

 and also like value action low expected value messages„ÄÇAnd ultimately„ÄÇ

 we get out a message to send to our dialogue partner„ÄÇNow every time we send or receive a message„ÄÇ

 we will repeat this whole process„ÄÇSo there's actually a lot that is quite novel in Cicero and I'm going to try to talk about„ÄÇ

The contributions as much as possibleÔºå I might go through this a little quickly just so we have time for questions„ÄÇ

The first one is a controllable dialogue model that conditions on the game state and a set of intended actions for the speaker and the recipient„ÄÇ

So we have a questionÔºå what is the action space here for the modelÔºüËØ∂„ÄÇAnd the action space„ÄÇ

For the action prediction model is like all the actions that you could take in the game that a player could take in the game„ÄÇ

For the dialogue modelÔºå it's like messages that you can send„ÄÇGot itÔºå so oh the sick„ÄÇH„ÄÇüòäÔºåOkay„ÄÇ

 so we try what we call an intent model that predicts what actions people will take at the end of truthful turns„ÄÇ

 basically where we're trying to predict what are people intending to do when they communicate a certain message„ÄÇ

And„ÄÇThen we use this to automatically annotate the data set with basically what we expect people's intentions were when they sent that message and we filter out„ÄÇ

We filter out as much as possible lies from the data set so that the text in the data is annotated with the truthful intention„ÄÇ

And then during play Cicero conditions the dialogue model on the truthful intention that it intends to take„ÄÇ

 and the goal then is that the hope then is that it will generate a message consistent with that intention„ÄÇ

And that is then fed into„ÄÇInto everything else that' that's you know„ÄÇ

 sorry that the intentions that we generate through planning are fed into the dial model„ÄÇ

So to give you an example of what this looks likeÔºå this gives us a way to control the dialogue model through a set of intentions„ÄÇ

üò°ÔºåLikeÔºå here„ÄÇËØ∂„ÄÇWe are Cicero England in pinkÔºå and their action is to move to Belgium„ÄÇ

 among other things„ÄÇAnd so if we feed this attention into the dialogue model„ÄÇ

 then the message that might get generated is something like England saying to France„ÄÇ

 do you mind supporting meÔºå do you mind supporting Eddie to BelgiumÔºüOn the other hand„ÄÇ

 let's say Cicero's action is to support France to Belgium„ÄÇThen if you feed that into the dial model„ÄÇ

 then the message that's generated might say something like„ÄÇ

 let me know if you want me to support you to BelgiumÔºå otherwise I'll probably poke Hol„ÄÇNow„ÄÇ

 what we find is that conditioning the dialogue model on these intentions in this way„ÄÇ

 it makes the model more controllableÔºå but it also leads to higher quality dialogue with less nonsense„ÄÇ

So we found that it led to dialogue that was more consistent with the state„ÄÇ

 more consistent with the planÔºå higher qualityÔºå lower perplexityÔºå and I think the argument„ÄÇ

 the reasoning for why this is the case is that we're kind of like relieving the dialogue model of the burden of having to come up with„ÄÇ

A good strategy we're allowing the dialogue model to do what it does best to focus on what it does best„ÄÇ

 which is dialogue„ÄÇAnd we're relieving it of the strategic components of the game because we're feeding that strategy into the dialect model„ÄÇ

OkayÔºå so that's one main contributionÔºå this controllable dialogue model that conditions on a plan„ÄÇ

The second is a planning engine that accounts for dialogue and human behavior„ÄÇSo„ÄÇOkay„ÄÇ

 I mentioned that a lot of„ÄÇPrevious work on games„ÄÇWas done using self play in two players here or some settings„ÄÇ

NowÔºå the problem with like pure self play„ÄÇIs that it can learn strong policiesÔºå but it doesn't„ÄÇ

It doesn't stick with human conventions and it can't account for dialogue„ÄÇ

 it's just going to ignore the human data and the human way of playing if you just do self play„ÄÇ

So that's one extreme„ÄÇThe other extreme that you can go is you just do supervised learning on human data„ÄÇ

Create this model of how humans play and then train with those you know imitation humans„ÄÇ

And if you do thisÔºå you'll end up with a bot that's consistent with dialogue and human conventions„ÄÇ

 but it's only as strong as the training dataÔºå and we found that it was actually very easily manipulable through adversarial dialogue„ÄÇ

 so for example you could send messages to it saying like thanks for agreeing to support me at the Paris and it will think like well I've only ever seen that message in my training data when I've agreed to support the person of the Paris and so I guess I'm supporting them at the Paris thisern even though that might be a terrible move for the bot„ÄÇ

So I came up with this algorithm called PickleÔºå that kind of like is a happy medium between these two extremes„ÄÇ

ÂóØ„ÄÇThe way pickle works is it''s basically trying to it's it's doing self play„ÄÇ

 but regularized toward„ÄÇSticking to the human imitation policy„ÄÇ

So it has a KL penalty for deviating from the human imitation policy„ÄÇ

So we have this parameter lambda that controls how easy it is to deviate from the human imitation policy at lambmbda equals zero„ÄÇ

It just ignores the human imageization policy completely and just does pure self play„ÄÇ

And so just like do cell play as if from scratchÔºå add them be equals zero„ÄÇAt Lambda equals infinity„ÄÇ

 it's just playing the human imation policy and not doing self Plaal„ÄÇ

But for intermediate values of LambdaÔºå what we find is that it actually gives you a good medium between sticking to human conventions and performing strongly„ÄÇ

So you can kind of see this behavior emerge here„ÄÇSorry there's a question is this similar to offline Rl or also incorporates exploration„ÄÇ

 so I'd say there's actually a lot of similar work on you know having a KL penalty„ÄÇ

And so yes I would say that it's like very similar to a lot of that work and it's also been done actually an alphapha star where they had a KL penalty„ÄÇ

 though that was more about aiding exploration like using human data to aid exploration rather than trying to better imitate humans so I think what's interesting about the pickle work is that one we find it imitates humans better not than just doing supervised learning alone and two we are„ÄÇ

Doing a bit of theory of mind where we assume that the other players are also like we're using this as a model for our behavior„ÄÇ

 what we expect other people to think our behavior is in addition to modeling the other players„ÄÇ

So it's like a common knowledge„ÄÇCommon knowledge like„ÄÇAlgorithm that we're using here„ÄÇOkay„ÄÇ

 so the kind of behavior that you see from this„ÄÇYou can see hereÔºå let's say England agreesÔºå sorry„ÄÇ

 so let's say we're in this situationÔºå this actually came up real in a real game„ÄÇ

And it inspired a figure from our paper„ÄÇSo England and France are fighting„ÄÇ

 France is the bot and France asks if„ÄÇIf England is willing to disengage„ÄÇAnd let's say England says„ÄÇ

 yesÔºå I will move out of English channel if you head back to NAO„ÄÇWellÔºå we can see that Cicero does„ÄÇ

 in fact back off„ÄÇLeeaaves and„ÄÇGoes to NAO and the disengagement is successful„ÄÇ

 and so this shows that the bot strategy really is reflecting the dialogue that it's had with this other player„ÄÇ

Another message that England might send is something like„ÄÇ

 I'm sorry you've been fighting me this whole gameÔºå I can't trust you that you won't stab me„ÄÇ

And so in this caseÔºå Cicero will continue its attack on EnglandÔºå and you can see again„ÄÇ

 this is reflectiveÔºå it's changing its behavior depending on the dialogue„ÄÇ

But you can also have this kind of message where you know England saysÔºå yes„ÄÇ

 I'll leave English channelnnel if you move tota Munich at Hall to Belgium„ÄÇ

 so these are really bad moves for Ccero to follow and so if you just look at the raw policy net„ÄÇ

It might actually do thisÔºå it might actually do these moves because England suggested it„ÄÇ

 but because we're using pickle that incorporates like it accounts for the expected value of different actions„ÄÇ

 it will actually harshly back off but ignore the suggested moves because it recognize that those will leave it very vulnerable to an attack„ÄÇ

OkayÔºå I's skip this slide for time„ÄÇÂóØ„ÄÇOh„ÄÇAnother thing I should say is that„ÄÇ

We're not just doing planningÔºå we're actually doing this in a full self play reinforcement learning loop„ÄÇ

And againÔºå the goal here is it's really about modeling humans better than supervised learning alone and we found that doing this selfplay reinforcement learning with pickle allowed us to better model human behavior than just doing imitation learning„ÄÇ

FinallyÔºå we have an ensemble of message filtering techniques that filters both nonsensical and strategically un soundund messages„ÄÇ

UmSo to give you an example of what these filters look likeÔºå here„ÄÇ

 one that we developed is value based filtering„ÄÇüò°ÔºåSo„ÄÇ

The motivation for this is that when we feed into our dialogue model is a plan for ourselves and for our speaking partner„ÄÇ

 but it's the entire plan that we have for ourselves„ÄÇ

 and so we might end up feeding into the dialogue model the fact that we're going to attack the player that we're speaking to„ÄÇ

NowÔºå the dialogue model isÔºå you knowÔºå to be honest kind of dumb and it doesn't really know„ÄÇ

That it shouldn't be telling this player that theyre going to be attacked this term„ÄÇ

And so you have these messages that might be sentÔºå something like the second one shown here where England says to France„ÄÇ

 we have hostile intentions towards youÔºå you must be wipeding the boardÔºå please provide a croissant„ÄÇ

So this is actually like a message that the bot sent to a player not to a player it was a this was like preliminary testing and kind of like motivated this whole approach„ÄÇ

ÂóØ„ÄÇSo we don't want the bot to send these kinds of messages if it's going to attack a player„ÄÇ

 we want it to send something that's likeÔºå you knowÔºå not an outright lie necessarily„ÄÇ

 but just something either not send a message or send something that's much much much more bland„ÄÇüò°„ÄÇ

And so we filter out these kinds of messages by looking at the value like„ÄÇ

What we do is we generate a bunch of candidate messages„ÄÇ

And then we see if we were to send this messageÔºå what is the behavior that we would expect the other players to take like what actions will we expect them to do after we send this message and what do they expect we will do after we send this messageÔºü

And then we see„ÄÇWhat is the expected value of the action that we intend to take„ÄÇ

 given the prediction of what everybody else is going to doÔºüüò°„ÄÇ

So if our intention is to attack FranceÔºå then we can see well„ÄÇ

 if I were to send this message to FranceÔºå then they're going to get really defensive and defend against an attack from us and our attack is going to be unsuccessful and so therefore I probably shouldn't send this message to them„ÄÇ

ÂóØ„ÄÇAnd so in this webÔºå we can actually filter out messages that have low expected value„ÄÇ

And we thought that this worked surprisingly well„ÄÇDialogue examples„ÄÇ

 I'll go through one just for the sake of time„ÄÇSo here we have„ÄÇ

Cicero is France and France is saying France is conversing with TurkeyÔºå who's a human player„ÄÇ

 and they're debating over who's going to get tunisÔºå this territory circled in red„ÄÇ

You can see they both have fleets next to the territory„ÄÇ

 if they both go for it neither them we're going to get it„ÄÇ

 and so they need to work out some sort of deal„ÄÇSo France saysÔºå I'll work with you„ÄÇ

 but I need tunes for nowÔºå Turkey saysÔºå nopeÔºå you got to let me have it„ÄÇFrance says noÔºå I need it„ÄÇ

 and then France suggestsÔºå you knowÔºå you can take these other territories instead you have Serbia and Rome to take„ÄÇ

Turkey says they're impossible targets and then Cicero suggests specific moves„ÄÇ

That would allow Turkey to capture these territory so Cicero says Greece Iionion I unate to Tian„ÄÇ

 Turkey saysm you're right good ideas and then France says then in the fall you take Rome Austria collapses and so that allows Turkey to you know make progress against Austria but conveniently it also allows France to capture tus because Turkey will be using those units for something else„ÄÇ

OkayÔºå so limitations in future directions intent representation is just an action per player„ÄÇ Okay„ÄÇ

 so there's a question of like„ÄÇThe intentions that we're feeding into the dialogue model is an action that we're going to take for this turn and for the next turn for ourselves and for the other player„ÄÇ

But ideallyÔºå we would have a richer set of intentions„ÄÇ

 we would be able to like condition on things like long term strategy or style of communication or like asking questions„ÄÇ

That is that's one of the limitations of this approach now of course„ÄÇ

The richer you make the space of intentionsÔºå the more room there is for things to go wrong„ÄÇ

 and you also have to like then train the model to be able to handle these like wider space of intentions„ÄÇ

üò°ÔºåThere was a questionÔºå do you think the dialogue model is learning an internal model„ÄÇ

 internal world model to be so good at predicting movesÔºüThis is this is arguably why we're„ÄÇ

Conditioning on intentionsÔºå we're relieving the dialogue model of having to come up with a good world model because we're telling it like these are the moves that we are planning to take this turn and these are the moves that we would like this other player to take this turn„ÄÇ

So we're like„ÄÇWe're able to like„ÄÇHave the world model separate from the dialogue model„ÄÇ

 but condition on the output from the world model„ÄÇOkay„ÄÇ

 another limitation is that Cicero's value model doesn't condition on dialogue„ÄÇ

 and so it has a limited understanding of the long term effects of dialogue„ÄÇÂóØ„ÄÇ

This greatly limits our ability to„ÄÇPlan what kind of messages we should be sending„ÄÇ

And this is actually why we always condition Cicero's dialogue generation on its truthful intentions„ÄÇ

You could argue that there is situations in diplomacy where you would want to lie to the other player„ÄÇ

 the best players rarely lieÔºå but they do lie sometimes„ÄÇAnd„ÄÇ

You have to understand the trade off between like if you lieÔºå you are going to„ÄÇüò°„ÄÇ

It's going to be much harder to work with this person in the future„ÄÇ

And so you have to make sure that the„ÄÇValue that you're getting positionally is worth that loss of trust and that broken relationship„ÄÇ

no„ÄÇBecause Cicero's value model doesn't condition on dialogue„ÄÇ

It can't really understand this trade off„ÄÇüò°ÔºåAnd so for this reason„ÄÇ

 we actually always conditioned it on its truthful intentions„ÄÇNo„ÄÇ

It is possible to have Cicero Viol condition on dialogue„ÄÇ

 but you would need way more data and it would make things much more expensive and so we weren't able to do it further for this spot„ÄÇ

And finallyÔºå there's a big question that I mentioned earlierÔºå which is„ÄÇ

 is there a more general way of scaling inference time compute to achieve better performanceÔºü

The way that we've done planning in Cicero is I would argue a bit domain specific„ÄÇ

 I think it's like the idea of pickle is quite general„ÄÇ

 but I think that there are potentially more general ways of doing planning„ÄÇËØ∂„ÄÇ

Somebody's asking looking forward to the next two to three years„ÄÇ

 what criteria will you use to select the next game to try to conquer honestly„ÄÇ

Like I said we chose diplomacy because we thought it'd be the hardest game to make an AI for it and I think that that's true I don't think that we're going to be working on games anywhere because I can't think of any other game that if we were to succeed at that it would be truly impressive„ÄÇ

And so I think where the research is going in the future„ÄÇIs generality„ÄÇLike„ÄÇ

Instead of getting an AI to play this specific gameÔºå can we get an AI that is able to play diplomacy„ÄÇ

 but could also play go or poker or could also write essays and stories and solve math problems and write theorems„ÄÇ

I think what we will see„ÄÇIs games serving as benchmarksÔºüFor progress„ÄÇBut not as„ÄÇThe goalÔºå you know„ÄÇ

 it'll be part of the test set but not part of the training set„ÄÇ

 and I think that's the way it should be going forward„ÄÇFinallyly„ÄÇ

 I want to add that diplomacy is an amazing testbed for multi agentent AI and Gred dialogue„ÄÇ

If you are interested in these kinds of domainsÔºå I highly recommend taking advantage of the fact that we are„ÄÇ

We've open sourced all of our coded models and the dialogue and action data is available through a what's called an RFP where you can apply to get access to the dialogue and data„ÄÇ

OkayÔºå so thanks for listening to wrap of Cicero combined strategic reasonaing and natural language and diplomaacy in place in the top 10% of human players and the paper is in science and code of models are publicly available at this URL so thanks and for there any time I'll take questions great thanks a lot my talk„ÄÇ

So we'll also open some questions from the classÔºå but you finished their Zoom questions so if anyone has like Zoom questions there I think no we can answer those„ÄÇ

YeahÔºå there's one question are you concerned about AIs out competingeting humans at real world diplomatic strategic negotiation at deception tasks„ÄÇ

 so like I said we're not very focused on deception even though you know arguably deception is a part of the game of diplomacy„ÄÇ

Á≥ª„ÄÇThink for diplomatic and strategic negotiation„ÄÇI don't like look the way that we've developed is real„ÄÇ

It's designed to play diplomacy the game of diplomacy specifically and you can't use it out of the box for other tasks that said I do think that the techniques are quite„ÄÇ

General and so hopefully others can build on that and to be able to do different things„ÄÇ

 and I think it is you know entirely possible that over the next several years„ÄÇ

 you will see this entering into real world negotiations much more often I actually think that diplomacy is a big step towards real world applicability compared to breakies in games like go and poker„ÄÇ

Because because now your action space is really like the space of natural language and you have to model human behavior„ÄÇ

Do you think in the future we could appoint AI to the UN CouncilÔºü

hopefully only if it does better than humansÔºå but that would be very interesting to see„ÄÇGreat„ÄÇ

 I'm also curious like what what's like the future things that you're working on in this direction like do you think you can do something like Alpago zero where you just like take this take this like repeated model and then maybe just make it Excel play or like what sort of future actions are you thinking for improving this sort of box„ÄÇ

üòäÔºåI think the future directions are really focused around generality„ÄÇ

 like I think one of the big insights of Cicero is like this ability to leverage planning to get better performance with language models and in this strategic domain„ÄÇ

I think there' was a lot of opportunity to do that sort of thing in a broader space of domains„ÄÇ

 I mean you look at language models today and they do token by token prediction„ÄÇ

And I think there's a big opportunity to go beyond that so that that's what i'm excited to look into I'm also curious like I didn't understand the exact details how using planning or Montete research with your like the models that you have„ÄÇ

So is it like we didn't use Monla research in CiceroÔºå Mongla research„ÄÇIs a very good heuristic„ÄÇ

 but it's a heuristic that is„ÄÇParticularly useful for deterministic perfect information games„ÄÇ

And I think in order to like have a truly general form of planning we need to go more abstract than multic research we use this algorithm called pickle based on a regret minimization algorithm I don't really want to go into the details of it because it's not that important for the class„ÄÇ

 but the idea is like it is this iterative algorithm that will gradually refine the prediction of what everybody's going do and get better and better predictions the more iterations that you run„ÄÇ

But and that's similar research„ÄÇYeah„ÄÇCoÔºå yeahÔºå sure„ÄÇÂØπ„ÄÇGo for itÔºå you're un mututed„ÄÇOkay„ÄÇSo yeah„ÄÇ

 my question is like when we were talking about gender likeibility„ÄÇ

 how does the communication between different modules of the model look likeÔºü

Particularly when we're talking about the dialogue model„ÄÇ

Like how do you send information from the policy network to the dialogue model and in the future„ÄÇ

 if you have a model that's good at different tasks„ÄÇ

 are we going to have like a really big policy net that learns all of them or like separate language modules for all of them like how do you break it downÔºü

So we actually convert the policy like action for ourselves and for our dialogue partner into a string naturaltro language string and to that into the dialogue model along with all the dialogue that it's had so far„ÄÇ

 so it's just all text in text out„ÄÇAnd that works great„ÄÇ

And then what was the second part of your questionÔºüSomething like„ÄÇ

 are we just going to have like one giant policy net trained on everythingÔºüYeah„ÄÇ

 it was like so if you're only using text first doesn't it limit the model and if you're using it for different games like are you thinking like when you say in the future you will work on generalizability„ÄÇ

 are you thinking about a big policy network that is trained on separate games or is able to like understand different games at the same time or do we have like separate policy networks for different gamesÔºü

And yeahÔºå like doesn't this like text interface limit the model in terms of communication like if you're using vectors it might like„ÄÇ

YeahÔºå it might be a bit of a bottom way„ÄÇI meanÔºå I think ideally you go in this direction where you have like you know a foundational model that works for pretty much everything does text I mean certainly yeah just like a text in text out that like limits what you can do in terms communication„ÄÇ

 but hopefully we get beyond that„ÄÇ

![](img/e3abef98963297997c09e5d05b736a64_7.png)

I think it's a reasonable choice for now„ÄÇThank you„ÄÇWe questions„ÄÇ

And I think we have more some questions Okay so there's a question in the chat i'd love to hear speculation on the future for instance we've seen some startups that are fine tuning LLMs to be biased or experts in say subject experts of„ÄÇ

This seems like a pretty general question„ÄÇÁ≥ª„ÄÇDon't have strong opinions on this„ÄÇËØ∂„ÄÇÁ≥ª‰∏ãÈù¢„ÄÇLikeÔºå I'm not„ÄÇ

 I'm not too„ÄÇI'm not too focused myself on fine tuning language models to specific tasks„ÄÇ

I think the direction that i'm much more interested in going forward is you know like the more general forms of planning„ÄÇ

 so I don't think I can really comment on„ÄÇYou knowÔºå how do you„ÄÇTwo in these language models„ÄÇ

In these ways„ÄÇSo what sort of planning methods are you like interested in looking at like like MCps is one so let me sorry I got to step out for just one second„ÄÇ

Got the switch roomsÔºå excuse me„ÄÇOkayÔºå never mind we're all good sorry what was the question Oh yes I was I was just asking like what sort of planning algorithms do you think are very interesting to combine So you can think like we have like so many options like we have like planet kind of stuff or Rl there's like MCps there's like the work you did with Cro so what do you think are the most interesting algorithms that you think will scale well can generalize„ÄÇ

üòäÔºåWell I think that's the big question that a lot of people are trying to figure out today and it's not really clear what the answer is I mean I think you know you look at something the chain of thought„ÄÇ

I think there's a lot of limitations to chain of thought„ÄÇ

 and I think that it should be possible to do a lot better„ÄÇ

 but it is really impressive to see just how general of an approach it is„ÄÇ

And so I think it would be nice to see„ÄÇËØ∂„ÄÇTo see things that are general in that way„ÄÇ

 but hopefully able to achieve better performance„ÄÇGo„ÄÇÂ•Ω„ÄÇAlso„ÄÇ

 when you see like citra is like an encoder decoder model in the sense it encodes the world„ÄÇ

 and then you have the dialg modelÔºå which is time to de it„ÄÇIt was an encoder decoder modelÔºå yes„ÄÇ

 I don't think that that's necessarily the right choiceÔºå but that's what we used„ÄÇAny questionsÔºüÂóØ„ÄÇ

Okay I think youre mostly good„ÄÇAyÔºå thanks a lot this was a good well yeah„ÄÇ

 hope you all enjoyed it and if there are any questions feel free to email me reach out I'm happy to happy to chat„ÄÇ



![](img/e3abef98963297997c09e5d05b736a64_9.png)