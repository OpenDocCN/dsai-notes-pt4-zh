# 斯坦福 GPT/Transformer 原理介绍 (中英文双字幕) - P14：14.战略游戏 - life_code - BV1X84y1Q7wV

![](img/e3abef98963297997c09e5d05b736a64_0.png)

![](img/e3abef98963297997c09e5d05b736a64_1.png)

所以有时候需要两个月来训练这些机器人。在成千上万的 CPU 上，有时候需要数 TB 的内存，但当真正与人类对战时，他们几乎是瞬间行动的，这只是一个查找表。而人类在困境中并不会立即行动。

他们会坐在那里思考五秒钟，可能在非常困难的决定中思考五分钟，很明显，这使他们能够想出更好的策略。所以我想在我们的机器人中调查这种行为，看看如果我们能够将其添加到我们的机器人中，会有多大不同，能够不是立即行动，而是花些时间计算出更好的策略。

这是我发现的。因此，在 X 轴上我们有桶的数量，你可以把这个想象成模型中的参数数量，而在 Y 轴上我们有距离纳什均衡的距离，这基本上是你在最坏对手面前损失多少，所以这个数字越低，你的扑克机器人就越好，你可以看到随着参数数量的增加。

你的表现有所提高，当你将参数数量增加约 100 倍时，你会发现可利用性降低了大约一半，实际上你得到了一个更好的扑克机器人。但你可以看到这里的蓝线是如果没有搜索，橙线是如果你添加了搜索。

😡，你可以看到，仅仅添加搜索，增加坐下来思考的能力，就提升了这些模型的表现。它减少了可利用性，距离纳什均衡大约降低了 7 倍。如果你想延长那条蓝线，看看需要多少个参数才能与添加搜索相媲美，答案是你需要将模型的规模扩大约 100,000 倍。好的。

当我看到这一点时，我感到非常震撼，意思是，在我博士学位的过程中，前三年，前三到四年，我成功地将这些模型的规模扩大了约 100 倍。嗯。我对此感到自豪，我认为这真是一个相当相当令人印象深刻的结果。

但这张图表向我展示的是，仅仅添加搜索，相当于将规模扩大约 100,000 倍。所以我之前的所有研究到这一点为止，只能算作脚注，跟添加搜索相比。因此，当我看到这一点时，变得清晰起来。

这就是战胜顶尖人类扑克选手的答案，因此在接下来的一年里，我几乎不间断地致力于扩展搜索。现在自然会出现一个问题，那就是为什么之前没有考虑到这一点。首先，我应该说扑克中的搜索在之前是被考虑过的，实际上可以说在国际象棋和围棋中都有搜索，那么为什么在扑克中不考虑搜索呢？

有几个原因，其一是文化方面，扑克研究起源于博弈论和强化学习，并不是与从事国际象棋和围棋研究的人相同的背景。当你扩展搜索，扩展测试时间的计算时，会使你的实验成本大幅增加，也让工作变得更加不愉快。还有一些激励结构，毕竟人们总是想着赢得下一个年度计算机扑克比赛，而 ACPC 限制了你在测试时可以使用的资源，因此在 ACPC 中实际上是无法有效进行搜索的。

我认为最大的因素是人们根本没有想到这会产生如此巨大的差异。我认为合理地看待像搜索这样的东西，认为它可能会带来 10 倍的差异，但你可能不会想到它会产生 10,000 倍的差异，因此虽然有一些人在研究这个，但它并不是很多人研究的重点。

所以无论如何，专注于扩展搜索，这导致了 2017 年 Bras 与 AIT 的比赛，我们再次将我们的机器人与四位顶级扑克高手对战。120,000 手扑克，$200,000 的奖金，这次机器人每 100 手赢了 15 个大盲注，而不是之前的 9 个大盲注。

这是一场毁灭性的胜利，每位人类选手都单独输给了机器人。![](img/e3abef98963297997c09e5d05b736a64_3.png)

四个标准差的统计显著性。我们在 2019 年进行了六人扑克人工智能比赛。最大的区别在于我们找到了如何进行深度限制搜索。因此，在 2017 年的机器人比赛中，它总是需要搜索到游戏的尽头，而这次只需搜索几步之后就可以停止。

所以这次再次获得了统计显著性。令人惊讶的是，尽管这是一个更大的游戏，这个六人扑克机器人 Plebuvis 在云计算资源上训练的费用不足 150 美元，并且在推理时运行于 28 个 CPU 核心上，没有使用 GPU。

![](img/e3abef98963297997c09e5d05b736a64_5.png)

所以我认为这表明，这实际上并不是算法上的改进。我是说，如果人们知道怎么做，20 年前就能做到这一点。我认为这也显示了搜索的威力，如果你能找出如何在测试时扩展计算。

这真的可以带来巨大的差异，并大幅降低你的训练成本。诶。😊。好的，无论如何，我还想说，这不仅限于扑克，如果你看看围棋，你会看到类似的模式。这是来自 AlphaGo Zero 论文的一个图。X 轴是不同版本的 AlphaGo，Y 轴是 elo 评分。

这是一种比较不同机器人，以及将机器人与人类进行比较的方法。你可以看到，如果你查看超级人类的表现，大约是 3600 elo。你可以看到 Alpha Go Lee，2016 年与李世石对弈的版本，正好超过超级人类表现的线。最强的 AlphaGo 版本大约是 5200 elo。

但是如果你去掉测试时间的搜索，如果你只是根据策略网络进行游戏，而不在 AlphaGo Zero 的测试时间进行任何蒙特卡洛研究，那么 elo 评分会降到大约 3000，这远低于专家人类的表现。这显示，如果在测试时间去掉蒙特卡洛研究，😡，AlphaGo Zero 并不是超级人类的，实际上没有人制作出不使用某种形式搜索的超级人类围棋机器人。

没有人制作出能击败顶级人类的原始神经网络。如果我还要说，这是如果在测试时间去掉搜索。我甚至不谈在训练时间去掉搜索，如果你在训练时间去掉它，我们甚至无法起步。现在有一个问题就是，好吧。

当然，你可以扩大模型，增加训练的量，最终超越人类的表现，如果你添加搜索，这也是正确的。如果你扩大模型和训练规模。

然后你最终会通过搜索匹配表现。但是有一个问题是，你需要扩大多少规模。大致经验法则是，要将你的 elo 评分提高约 120 点，你要么需要将模型大小和训练量翻倍，要么需要将测试时间的搜索翻倍。

如果你看那大约 2000 elo 点的差距，计算一下你需要的 deadlings 数量，答案是为了让原始策略网络从 3000 elo 提升到 5200 elo，你需要将模型和训练扩大大约 100000 倍。好的。那么这为什么重要呢？😡，我认为看看今天的大型语言模型和变换器，你会看到类似的情况。

我的意思是，你得到了。巨大的问题是，我所说的搜索是什么意思，有一种特定的搜索类型，比如多目标搜索，能够提前规划你将要做的事情，而不是仅仅根据预先计算的策略瞬间行动。

但我更广义上所指的搜索是能够扩展计算量以获得更好的性能。我认为这才是真正的价值，而不仅仅是根据你的预计算来行动。你知道，把所有计算都提前完成，然后在推理时几乎瞬间行动。如果你有五分钟输出一个动作，而不是 100 毫秒，能否得到更好的解决方案？

好吧。所以。是的，我觉得你要看一下。抱歉，有个问题。带有搜索电路的变压器算作搜索吗？还是你指的是工程搜索算法？我不想深入探讨如何做到这一点，因为答案是目前没人真正知道。我们在扑克和围棋等领域成功进行搜索的方式都是相当特定于领域的。

你知道，去使用这个叫做多层研究的算法。是的。你可以把束搜索看作一种简单的搜索形式，但看起来未来应该会有更好的方法。是的。所以，无论如何，我的意思是，你要看看今天大型语言模型是如何训练的。

你会看到数百万美元被投入到预训练中。 我不会惊讶如果我们看到一个大型语言模型训练成本达到 1 亿美元。我们甚至可能达到 10 亿美元。但推理成本仍然会非常小。因此有个问题：如果可以的话，你能否做得更好？

也要考虑推理成本的规模。也许这可以摊销你的一些训练成本。是的。好吧，有一场讲座叫做理查德·萨顿的《Bi 教训》，它说最大的教训就是可以学到的。所以这是篇很好的文章，我推荐阅读。但其中一个主要的收获是，他说从 70 多年 AI 研究中学到的最大教训是，利用计算的通用方法最终是最有效的。

似乎以这种方式无限扩展的两种方法是搜索和学习。现在。我认为我们在推广搜索方面做得很好，抱歉，推广学习方面。我认为在搜索方面仍然有改进的空间。你知道，接下来的目标真的就是通用性：我们能否开发一种真正通用的方法来扩展推理计算，而不仅仅是做诸如蒙特卡洛研究那样特定于某一领域的事情。

而且这比诸如思维链之类的东西要好。嗯。这看起来会是你有更高的测试时间计算，但你有更强大的模型。我认为在某些领域，这种权衡是值得的。如果你考虑到我们愿意为“雷蒙德假设”的证明支付的推理成本。

我认为我们愿意支付很多钱。或者你知道，这个成本。我们愿意为新的救命药物支付多少成本，我认为我们愿意支付很多。所以我觉得这里有一个机会。好吧，反正，这就是我前面的铺垫。我想在我继续谈论西塞罗之前，有什么问题吗？顺便说一下。

我谈论这个的原因是因为它将影响。😡。我们对西塞罗的处理方式，我认为与其他许多研究人员可能采取的方式有很大不同。有人问，能否举一个多模态研究的搜索例子。你也可以想象像广度优先搜索、深度优先搜索这些类型的搜索，它们都是搜索。我还想说，思维链在某种程度上类似于搜索，因为它允许模型在测试时利用额外的计算资源，以获得更好的性能。

但我认为这就是你想要的主要内容。能够在测试时利用额外的计算资源。搜索的空间是什么？你在搜索什么？在围棋这样一个游戏中，就是不同的棋盘位置。但你也可以想象搜索像不同的句子之类的东西。

这里也有很多灵活性。好吧，现在我想谈谈西塞罗。首先我应该说，关于西塞罗。这是一个大型团队合作。这实际上是参与这个项目的一个伟大之处，因为这里有如此多样的人才。

在强化学习、规划、博弈论和自然语言处理方面的专家，大家一起合作，这一切都是不可能的，没有每个人的努力。因此，外交的动机实际上来自 2019 年。我们当时在关注所有的突破，我认为一个很好的例子是 2012 年发布的这幅 XKCD 漫画，展示了不同类别的游戏。

已经解决的游戏，计算机可以击败文档的游戏。计算机仍然会输给文档的游戏，以及计算机可能永远无法超越文档的游戏。在这一类别中，计算机仍然输给顶级人类的四个游戏，包括围棋、阿里马、扑克和星际争霸。在 2015 年。😊实际上，我的一位同事。

大卫·吴在 2016 年首次突破，战胜了顶尖人类选手。在 2016 年，我们的算法战胜了 Lisa 在围棋中的表现。在 2017 年，我刚刚描述的工作中，我们在扑克比赛中击败了顶尖人类。而在 2019 年，我们的 AlphaStar 战胜了多名人类选手在《星际争霸》中。这显示了在战略推理方面，在过去几年中发生的惊人进展，直至 2019 年。同时，我们在 2019 年也推出了 GP2，这表明语言模型和自然语言处理的进展速度远超许多人的预期，包括我们自己。因此，我们在思考第六位扑克选手工作的下一步。

我们下一步应该做什么？我们讨论了不同的领域。考虑到人工智能的惊人进展，我们希望选择一个非常雄心勃勃的项目，一项我们认为无法仅通过扩大现有方法来解决的任务，因此我们选择了外交，因为我们认为这是为人工智能开发的最困难的游戏。

那么，什么是外交？外交是一种自然语言策略游戏。游戏发生在第一次世界大战前，你将作为欧洲七大强国之一进行游戏：英格兰、法国、德国、奥地利、俄罗斯和土耳其。你的目标是在地图上控制大多数区域，实际上，这种情况很少发生；如果你控制了大多数区域，实际上就意味着你赢了，但通常没有人能彻底获胜，因此你的得分会受到影响。

这与您控制的地图百分比成正比吗？

现在，外交游戏中真正有趣的是，它是一种自然语言的谈判游戏。因此，你会看到德国和英格兰之间的对话，他们会在行动前进行交流，比如德国可能会问：“你想支持瑞典吗？”

英格兰说：“让我想想。”所以这是一个在 1950 年代开发的受欢迎的策略游戏。实际上，这也是 JFK 和基辛格的最爱。正如我所说，每一局都涉及复杂的私人自然语言谈判。我想明确指出，这并不是像你在游戏《卡坦岛》中看到的那种谈判。

例如，你会看到，这更像是一场生存游戏，如果你看过真人秀《幸存者》。你会围绕想要建立的联盟进行讨论，围绕想要在当前局势中执行的具体战术进行讨论，还会讨论更长期的策略，比如我们从这里往哪里去，以及如何分配资源。

现在游戏的运作方式是，谈判持续时间在五到十五分钟之间，具体取决于游戏的版本。而且所有这些谈判都是以对等的方式私下进行的。我还认为你没有被静音，谢谢你。然后在谈判期结束后，大家会同时写下他们的行动。😡

所以，一个玩家可能会承诺你，比如说，我会在这个回合支持你进入这个领土。但是当人们实际写下他们的行动时，他们可能不会这样写，所以你只有在所有的行动同时揭晓时才能知道他们是否遵守了诺言。因此，联盟和建立信任至关重要。

相信有人会履行承诺的能力，这实际上就是这个游戏的核心，而说服他人你会履行承诺的能力同样是这个游戏的核心。因此，出于这个原因，外交长期以来被认为是人工智能的挑战性问题。关于这个游戏的研究可以追溯到 80 年代，研究真正开始得到重视。

这方面的研究在 2019 年开始时非常激烈，当时 DeepMind 的研究人员，以及我们自己和其他地方的人开始进行这方面的工作。现在很多研究，绝大多数研究实际上集中在游戏的非语言版本上，这被视为通向完整自然语言版本的垫脚石。

尽管我们决定从一开始就专注于游戏的完整自然语言版本。为了让你了解这些谈判和对话的样子，这里有一个例子。在这个例子中，英格兰，你可以看到他们将舰队从挪威移动到圣彼得堡，这占领了俄罗斯的领土。

所以这就是在那次行动后的棋盘状态，现在奥地利和俄罗斯之间有这样的对话。奥地利说，北边发生了什么？俄罗斯说，英格兰背叛了，我担心你我朋友的结局可能要来了。奥地利说，是的，那很糟糕，你能在那边撑得住吗？俄罗斯说，我希望如此，英格兰似乎仍然想要合作。奥地利说，你能和德国达成协议吗？所以玩家们现在在讨论应该和其他玩家讨论什么。俄罗斯说，好主意，然后奥地利说，如果我们能找到，只要你能保住塞瓦斯托波尔就行。塞瓦斯托波尔是南边的一个领土，你可以看到土耳其在黑海和塞瓦斯托波尔旁边有一支舰队和一支军队，他们可能会在下一轮攻击那个领土。

嗯。😊，奥地利说，你能支持保住塞瓦斯托波尔、乌克兰和罗马尼亚吗？我会支持保住罗马尼亚。俄罗斯说，是的，他们已经在这样做，奥地利说，太好了。希望我们能开始让你恢复元气。这是你在外交游戏中会看到的对话的一种例子。

奥地利实际上是我们的机器人西塞罗。😊，这让你对代理对话的复杂性有了一些了解。诶。好的，我会跳过这一点，所以我想我会进入这个，我不想占用太多时间，实际上让外交变得有趣的是支持是关键，所以这里举个例子。

在布达佩斯和华沙，红色和紫色单位都试图进入格拉奇亚，由于这是一个一对一的对抗，他们都反弹回去。现在在中间面板上，你可以看到维也纳支持布达佩斯进入格拉奇亚，因此现在是两个对一个，那个红色单位确实会进入格拉奇亚。

而关于这个协议真正有趣的是，不仅仅是你自己的单位在支持你，其他玩家的单位也可以支持你。例如，绿色玩家可以支持红色玩家进入格拉奇亚，那么那个红色单位仍然会进入那里。所以支持实际上就是游戏的全部，谈判关于支持的事情，因此，外交有了“毁掉友谊的游戏”的名声，和某人维持三到四个小时的联盟真的很困难，然后再有这样的情况。

😊，让他们背叛你，这基本上是在毁掉你的游戏。但如果你和专业外交玩家交谈，他们的看法会有所不同。他们认为，外交最终是建立信任的过程，而环境却促使你不去信任任何人。这就是我们决定致力于这个游戏的原因，你知道的。

我们能否创造一个能够在一个鼓励玩家不信任任何人的环境中建立信任的 AI？它能否诚实地沟通自己将要做的事情，并评估另一个人在说他们要做某事时是否诚实？

好的，所以。在外交游戏中，它恰好位于强化学习和规划的交汇点上，还有自然语言。我们可以从两个角度来看为什么外交是一个非常有趣的领域，一个是多边的视角。

😡，之前的所有游戏，比如国际象棋、围棋和扑克，这些都是纯粹的零和两个玩家的领域。在这些领域，自我对弈可以保证收敛到最优解，基本上这意味着你可以让机器人完全从头开始游戏，没有任何人类数据。通过反复自我对弈，它最终会收敛到这个无与伦比的最优解，称为迷你最大均衡。

但这个结果仅在两个玩家的零和游戏中成立。这整个范式仅适用于两个玩家的零和游戏。当你进入需要合作的领域时，除了竞争，成功就需要理解人类行为和约定，你不能再把其他玩家当作机器对待。

你必须像对待人类一样对待它们，你必须建模。😡人类的不理性，人类的有限性。其中一个例子实际上是语言。你可以想象，如果你从零开始训练一个机器人进行外交游戏，像这个游戏的完整自然语言版本，机器人没有理由会学会用英语交流。

它会学会用一些奇怪的机器人语言进行交流，当你把它放入与六个人类的游戏中时，它无法与他们合作。😡嗯。所以我们必须找到一种方法来整合人类数据，并能够学习人类的行为，以便在这个游戏中取得成功。

😡诶，好的。还有一个 NLP 的视角。当前的语言模型基本上只是模仿人类的文本。虽然在 RLHF 等方面取得了一些进展，但这仍然不是人类交流的方式，人类是有意图地进行交流，他们想出这个意图，然后以传达这个意图为目标进行交流，并理解其他人也在尝试做到这一点。

😡所以有一个问题，就是我们能否超越糟糕的聊天？实现有目的的对话。因此，Cicero 是一个外交 AI 代理，它结合了高水平的战略玩法和开放领域的对话。我们使用了通过与网站 webdplomacy.net 合作获得的 50,000 场人类外交游戏。因此，我们将 Cicero 投入一个在线外交联盟，提前给你结果。

Cicero 在 828 名玩家的 40 场比赛中未被识别为 AI 代理。有一位玩家事后提到他们开玩笑说我们是机器人，但他们并没有真正跟进，其他人也没有跟进，后来他们还指控其他人也是机器人，因此我们不确定该如何认真对待这个指控，但我觉得很遗憾的是，它在 40 场比赛中都没有被识别为机器人，事实上我们在赛后告诉玩家它一直是机器人，这就是我们得到的回应，人们相当惊讶。

幸运的是，没有人对我们感到不满，但他们非常惊讶，竟然有一个机器人一直在与他们一起玩这个游戏。就结果而言，Cicero 排在前 10%的玩家中，这是一个高变异性的游戏，如果你看看玩了五场或更多比赛的玩家，它在 19 人中排第二，达到了超过两倍于平均人类得分的成绩。

我会将其描述为一种强大的人的表现水平。我不会说这绝对是超人，但这目前确实是一个很强的结果。现在，为了让你了解 Cicero 是如何工作的，我们输入到模型中的信息是棋盘状态和左上角显示的最近行动历史。

还有与所有玩家进行的对话直到现在。因此，这将被输入到一个对话条件行动模型中，该模型将预测 Cicero 认为所有玩家在这一轮将要做的事情，以及他们认为我们将在这一轮做什么。

这些导致了我们所称的锚定策略，这些策略随后用于规划。😡，嗯。现在，规划在这里。再次强调，这是我们在测试时利用额外计算资源以获得更好性能的部分。因此，我们基本上会对大家将要做的初始预测，即所谓的锚定策略进行改进，使用这一名为 pickle 的规划过程。

基本上，我们考虑到玩家将以更高的概率选择期望值更高的行动。我们本质上是为所有玩家添加了一种理性假设，假设他们不会像模型所暗示的那样经常失误，并且他们会以比初始模型所暗示的更高的概率选择更聪明的行动。

我们发现，这实际上比仅仅依赖原始神经网络给出了更好的对所有玩家将要做的事情的预测。这给出了我们在游戏中实际采取的行动，也给出了我们所称的意图。因此，意图是我们自己和我们正在对话的对话伙伴的行动。现在我们有这个对话条件，因此我们有一个对话模型，以这些意图为条件。

因此，意图与棋盘状态、行动历史以及到目前为止的对话一起输入到对话模型中，这个对话模型随后将生成。基于这些意图的候选消息。这些候选消息会通过一系列过滤器，过滤掉无意义、基础问题，以及像低预期值消息这样的价值行动。最终。

我们向对话伙伴发送消息。现在每次我们发送或接收消息时，都会重复整个过程。因此，Cicero 中有很多新颖的地方，我会尽量谈论。贡献尽可能多，我可能会稍微快一点，以便我们有时间进行提问。第一个是一个可控对话模型，它以游戏状态和说话者及接收者的一组意图行动为条件。

我们有一个问题，模型的行动空间是什么？诶。行动预测模型的行动空间是玩家在游戏中可以采取的所有行动。对于对话模型，它是你可以发送的消息。明白了，所以哦，那个病。H。😊，好的。我们尝试一个我们称之为意图模型的模型，预测人们在真实轮次结束时将采取的行动。

基本上，我们试图预测人们在传达某条消息时的意图。然后，我们用这个自动注释数据集，基本上记录我们期望人们发送该消息时的意图，并过滤掉。

我们尽可能从数据集中过滤掉谎言，以便数据中的文本注释真实意图。然后，在游戏中，西塞罗将对话模型基于其打算采取的真实意图进行条件化。目标是生成与该意图一致的消息。这又被输入到其他所有内容中。

抱歉，我们通过规划生成的意图被输入到对话模型中。所以给你一个例子，这让我们通过一组意图来控制对话模型。😡，就像这里。嘿。我们是西塞罗英格兰，穿粉色，他们的行动是移动到比利时，其他事项也是如此。如果我们将这个意图输入对话模型。

然后，可能生成的消息就像是英格兰对法国说：“你介意支持我吗？你介意支持埃迪去比利时吗？”另一方面，假设西塞罗的行动是支持法国去比利时。那么，如果你将其输入对话模型，生成的消息可能会是这样的。

如果你想让我支持你去比利时，请告诉我，否则我可能会打扰霍尔。现在，我们发现以这种方式将对话模型与这些意图结合起来，可以使模型更加可控，同时也提高了对话质量，减少了无谓的内容。因此，我们发现这导致了更符合状态的对话。

更符合计划，质量更高，困惑度更低，我认为原因在于，我们像是解放了对话模型，不再需要考虑制定好的策略。我们让对话模型专注于它最擅长的对话，并将战略组件的负担移交给对话模型。

好的，这是一个主要贡献，这个可控的对话模型基于一个计划。第二个贡献是一个规划引擎，它考虑了对话和人类行为。所以，好的。我提到之前关于游戏的许多工作都是在两个玩家的自我对战或某些设置下进行的。现在，纯自我对战的问题在于，它能学习强策略，但并不能。

它不遵循人类的约定，也无法处理对话。如果你只是进行自我对战，它会忽略人类数据和人类的游戏方式。这是一个极端。另一种极端是对人类数据进行监督学习。创建人类游戏的模型，然后用那些模仿人类进行训练。

如果你这样做，你最终会得到一个与对话和人类约定一致的“机器人”，但它的能力仅取决于训练数据，我们发现它实际上很容易受到对抗性对话的操控。例如，你可以发送消息给它说“感谢你同意在巴黎支持我”，它会认为，嗯，我在训练数据中只见过这个消息，当我同意支持某人时，所以我想我在巴黎支持他们，即使这对“机器人”来说可能是一个糟糕的举动。

所以我想出了一个叫做“Pickle”的算法，它在这两个极端之间找到了一个快乐的中间地带。嗯。Pickle 的工作原理基本上是进行自我对弈，但会朝着遵循人类模仿策略的方向进行规范。因此，它对偏离人类模仿策略施加了 KL 惩罚。

我们有一个参数λ，它控制从人类模仿策略偏离的难易程度，当λ等于零时，它完全忽略人类模仿策略，进行纯自我对弈。因此，像从零开始一样进行自我对弈，当λ等于零时。而当λ等于无穷大时，它只是执行人类模仿策略，而不进行自我对弈。

但是对于λ的中间值，我们发现它实际上在遵循人类约定和表现强劲之间提供了一个良好的平衡。所以你可以在这里看到这种行为的出现。抱歉，有个问题，这是否类似于离线强化学习，还是也包含了探索？

我可以说，实际上有很多类似的工作，比如施加 KL 惩罚。因此，我会说这与很多相关工作非常相似，实际上在 AlphaStar 中也做过，他们施加了 KL 惩罚。尽管那更多是关于利用人类数据来帮助探索，而不是更好地模仿人类。因此，我认为 Pickle 工作的有趣之处在于，我们发现它在模仿人类方面表现得更好，而不仅仅是进行监督学习。

我们在进行一些心智理论的工作，我们假设其他玩家也在使用这个模型来指导我们的行为。我们期望其他人对我们的行为的看法，除了对其他玩家进行建模之外。所以这就像是我们在这里使用的一个常识算法。好吧。从这里你可以看到的行为，比如说，假设英格兰同意，抱歉。

假设我们处于这种情况，这实际上在一场真实的游戏中发生过。这启发了我们论文中的一个例子。所以英格兰和法国正在交战。法国是“机器人”，法国询问英格兰是否愿意脱离接触。如果英格兰回答说，是的，如果你回到北大西洋，我会离开英吉利海峡。那么我们可以看到西塞罗的表现。

实际上后撤。离开并。去往 NAO，成功脱离接触。因此，这表明机器人的策略确实反映了它与其他玩家的对话。英格兰可能发送的另一个消息是这样的话。我很抱歉，你整个游戏都在与我作斗争，我不能相信你不会背刺我。

因此，在这种情况下，西塞罗将继续攻击英格兰，你可以再次看到。这是有反应的，它根据对话改变了行为。但你也可以有这样的消息，比如英格兰说，是的。如果你把军队移动到比利时，我就会离开英吉利海峡。

所以这些对于西塞罗来说都是非常糟糕的举动，如果你只看原始的策略网络，它可能会这样做，可能会做出这些举动，因为英格兰建议了它。但因为我们使用 pickle，它考虑了不同动作的期望值，它实际上会强烈地后撤，但忽略这些建议的举动，因为它意识到这些会使其非常容易受到攻击。

好吧，我会跳过这一页以节省时间。嗯。哦。我还应该说的是，我们不仅仅是在做规划，我们实际上是在一个完整的自我对弈强化学习循环中进行。这次的目标确实是比单纯的监督学习更好地建模人类行为，我们发现使用 pickle 进行自我对弈强化学习可以比单纯的模仿学习更好地模拟人类行为。最后，我们有一组消息过滤技术，可以过滤掉无意义和策略上不合理的消息。

嗯，所以给你一个关于这些过滤器的例子，这里。有一个我们开发的基于价值的过滤器。😡，所以。这么做的动机在于，当我们输入对话模型时，是为我们自己和我们讲话伙伴制定的计划，但这是我们为自己制定的整个计划。

因此，我们可能最终会把我们打算攻击与我们对话的玩家这一事实输入到对话模型中。老实说，对话模型有点傻，它并不知道不应该告诉这个玩家他们会被攻击。因此，可能会发送一些消息，比如这里显示的第二条，英格兰对法国说。

我们对你有敌意，你必须清理战场，请提供一个可颂。实际上，这是机器人发送给一个玩家的消息，而不是对另一位玩家的。这是初步测试，激发了整个方法。

嗯。因此，如果机器人打算攻击玩家，我们不希望它发送这种消息。我们希望它发送一些不一定是完全谎言的东西，或者根本不发送消息，或者发送一些更为平淡的内容。😡。因此，我们通过查看这些值来过滤这些消息。

我们所做的是生成一堆候选消息。然后我们看看，如果我们发送这条消息，我们期望其他参与者的行为是什么，他们在收到这条消息后会采取什么行动，他们又期望我们在发送这条消息后会做什么？

然后我们看到，我们打算采取的行动的预期值是什么。考虑到对其他人将要做的预测😡。所以如果我们的意图是攻击法国，那么我们可以看到，如果我将这条消息发送给法国，他们会变得非常防御，抵御我们的攻击，而我们的攻击将会失败，因此我可能不应该将这条消息发送给他们。

嗯。所以在这个网络中，我们实际上可以过滤掉预期值较低的消息。我们发现这效果惊人地好。对话示例。为了节省时间，我将举一个例子。这里有。西塞罗代表法国，法国正在与人类玩家土耳其对话。

他们正在争论谁将占领突尼斯，这个被红圈圈出的领土。你可以看到他们都有舰队在领土旁边。如果他们都去争夺，那么都不会得到。因此，他们需要达成某种协议。所以法国说，我会和你合作。

但是我现在需要突尼斯，土耳其说，不，你得让我拥有它。法国说不，我需要它。然后法国建议，你知道，你可以去占领其他领土，比如塞尔维亚和罗马。土耳其说，那些是不可及的目标，接着西塞罗提出具体的行动，允许土耳其占领这些领土，所以西塞罗说希腊、爱奥尼亚和特兰。

土耳其说，你说得对，这些是好主意，然后法国说，那在秋天你就可以占领罗马，奥地利崩溃了，这样土耳其就可以对奥地利取得进展，但方便的是，这也让法国可以占领突尼斯，因为土耳其会将那些单位用于其他事情。

好的，未来方向中的限制意图表示只是每个玩家的一项行动。好的。所以有一个问题。我们输入对话模型的意图是我们在这一回合以及下一回合要为自己和其他玩家采取的行动。

但理想情况下，我们会有更丰富的意图集合。我们能够考虑长期策略、沟通风格或提问等因素。这是这种方法的一个限制。当然，意图的空间越丰富，出错的可能性就越大。

你还需要训练模型，以便处理更广泛的意图空间。有一个问题，你认为对话模型是否在学习一个内部模型？内部世界模型是否使其能够如此准确地预测动作？这就是为什么我们依赖意图，减轻对话模型构建良好世界模型的负担，因为我们告诉它这是我们在这一回合计划采取的动作，而这些是我们希望其他玩家在这一回合采取的动作。

所以我们能够让世界模型与对话模型分开，但依赖于世界模型的输出。好的，另一个限制是 Cicero 的价值模型并不依赖于对话，因此对对话的长期影响理解有限。

这极大地限制了我们规划应发送的消息类型的能力。实际上，这也是我们总是将 Cicero 的对话生成基于其真实意图的原因。你可以争辩，在外交中有些情况下，你会想对其他玩家撒谎，最好的玩家很少撒谎，但有时会撒谎。

你必须理解，如果你撒谎，你将会变得更难与这个人合作。因此，你必须确保你所获得的相对价值值得信任的损失和破裂的关系。因为 Cicero 的价值模型并不依赖于对话。

它实际上无法理解这种权衡。因此，出于这个原因，我们始终将其基于真实意图。实际上，可以让 Cicero Viol 依赖于对话，但你需要更多的数据，这会使事情变得更加昂贵，因此我们无法进一步进行。

最后，我提到了一个大问题，那就是是否有更普遍的方法来缩放推理时间计算，以实现更好的性能？

我认为我们在 Cicero 中进行规划的方式有点领域特定。我觉得 pickle 的概念相当普遍，但我认为可能还有更普遍的规划方式。有人问期待未来两到三年。

你将使用什么标准来选择下一个游戏以诚实地征服？正如我所说，我们选择外交是因为我们认为这是一个最难为其制作 AI 的游戏，我认为这确实是正确的。我想不出任何其他游戏，如果我们成功，那将是非常令人印象深刻的。

所以我认为未来的研究方向是通用性。我们能否让人工智能不仅能玩这个特定的游戏，还能玩围棋或扑克，或者还能写论文、故事、解决数学问题和写定理。我认为我们将看到的是，游戏作为进步的基准，但不是目标。

这将是测试集的一部分，但不是训练集的一部分。我认为这应该是未来的方向。最后，我想补充的是，外交是多代理人工智能和对话生成的一个惊人测试平台。如果你对这些领域感兴趣，我强烈推荐利用我们所提供的机会。

我们已开源所有编码模型，对话和行动数据可以通过一种称为 RFP 的方式获取，你可以申请获取对话和数据。好的，感谢大家的倾听，Cicero 结合了战略推理和自然语言，在人类顶尖 10%的玩家中表现出色，论文已发表于科学期刊，模型代码在这个网址公开可用，所以谢谢，随时可以提问，感谢大家的支持。

我们还将开放一些来自课堂的问题，但你已经完成了他们的 Zoom 问题，所以如果有人有 Zoom 的问题，我想我们可以回答。是的，有一个问题，你是否担心人工智能在真实世界的外交战略谈判和欺骗任务中超过人类。

正如我所说，我们并不太关注欺骗，尽管可以说欺骗是外交游戏的一部分。思考外交和战略谈判。我不喜欢我们所发展的方式是真实的。它是专门设计来进行外交游戏的，你不能直接用于其他任务，但我确实认为这些技术是相当通用的，因此希望其他人可以在此基础上构建，以便能够做不同的事情。

我认为在接下来的几年里，这种情况完全可能，你会看到它在现实世界的谈判中更频繁地出现。我实际上认为，与围棋和扑克等游戏相比，外交是朝向现实世界适用性迈出的重要一步。

因为现在你的行动空间实际上就是自然语言的空间，你必须对人类行为进行建模。你认为未来我们能否任命人工智能担任联合国理事会的成员？

希望只有在它表现优于人类的情况下，但那将是非常有趣的。太好了。我也很好奇，你们在这个方向上正在研究哪些未来的事情，你认为能否做类似 Alphago Zero 的事情，重复这个模型，也许让它在 Excel 中运行，或者你在改进这个盒子方面有哪些未来的想法？

😊，我认为未来的方向确实是围绕通用性展开的。我认为 Cicero 的一个重大洞察是利用规划来提升语言模型的性能，尤其是在这个战略领域。我认为在更广泛的领域中有很多机会进行这种探索。现在的语言模型是逐个标记进行预测的。

我认为有一个很大的机会可以超越这一点，这让我感到兴奋。我也很好奇，我没有理解具体细节，如何使用规划或蒙特卡罗研究与您拥有的模型结合。我们在 Cicero 中没有使用蒙特卡罗研究，蒙特卡罗研究是一个非常好的启发式方法。

但它是一种启发式方法。特别适用于确定性完全信息游戏。为了拥有真正通用的规划形式，我们需要更抽象地超越蒙特卡罗研究。我们使用一种称为 pickle 的算法，基于遗憾最小化算法。我不想详细讨论，因为这对课程并不重要。

但这个想法就像是一个迭代算法，逐渐完善每个人的预测，随着迭代次数的增加，预测会越来越准确。但这类似于其他研究。是的，好的，当然。对。继续吧，你可以发言。好的。是的。

我的问题是，当我们讨论性别可接纳性时，模型不同模块之间的沟通是怎样的？

特别是在我们谈论对话模型时。你如何将信息从策略网络发送到对话模型？未来，如果你有一个擅长不同任务的模型，我们会有一个学习所有任务的庞大策略网络，还是为每个任务都有独立的语言模块？你会如何拆分？

所以我们实际上将政策像行为一样转换为字符串，自然语言字符串，然后将其输入对话模型，以及到目前为止所有的对话内容。因此，这就是文本输入文本输出。这种方法效果很好。

那你第二个问题的内容是什么？类似于我们会有一个巨大的策略网络训练在所有内容上吗？是的，如果你只使用文本，难道不会限制模型吗？如果你在不同的游戏中使用它，比如你提到的未来会关注通用性，你是指一个在不同游戏上训练的大策略网络，还是能够同时理解不同游戏，或者是为不同游戏有独立的策略网络？

嗯，是的，这种文本接口在沟通上限制了模型的能力，比如如果使用向量可能会有些底层。我是说，理想情况下你应该朝着这个方向发展，拥有一个适用于几乎所有文本的基础模型，当然，仅仅是文本输入和输出，这在沟通上是有限制的。

但希望我们能超越这个。![](img/e3abef98963297997c09e5d05b736a64_7.png)

我认为这在现在是个合理的选择。谢谢。我们有问题。我想我们还有一些问题，好吧，聊天中有一个问题，我想听听你对未来的猜测，比如说我们看到一些初创公司正在微调 LLMs，以便偏向某个领域或成为某种主题的专家。

这似乎是个相当通用的问题。系。我对此没有强烈的看法。诶。系下面。就像，我并不太专注于将语言模型微调到特定任务。我认为我更感兴趣的方向是更通用的规划形式。所以我觉得我不能真正评论。你知道的，这些语言模型怎么能做到。

以这种方式。那么你对哪些规划方法感兴趣，比如说 MCPS 是其中之一，所以让我抱歉，我得出去一秒。换个房间，抱歉。好的，没事，我们都很好，抱歉，问题是什么？哦，是的，我只是问你认为哪些规划算法非常有趣，可以结合使用。我们有很多选择，比如行星类的东西或者 RL，还有 MCPS，以及你与 Cro 合作的工作。那么你认为哪些算法最有趣，能够很好地扩展和泛化？

😊，我认为这是许多人今天试图弄清楚的大问题，而答案并不清楚。我的意思是，你看看链式思维。我认为链式思维有很多局限性。

我认为有很多方法可以做得更好。但看到这样通用的方法真的很令人印象深刻。所以我认为看到这样通用的东西会很不错，但希望能实现更好的性能。走吧，好。

当你看到像 Citra 这样的编码解码模型时，它在某种意义上编码了世界。然后你有对话模型，负责解码。是的，它是一个编码解码模型。我不认为这 necessarily 是正确的选择，但这就是我们使用的。有什么问题吗？嗯，好的，我觉得你大致上没问题。谢谢，这真是好。

希望你们都喜欢，如果有任何问题，请随时发邮件联系我，我很乐意聊聊。![](img/e3abef98963297997c09e5d05b736a64_9.png)
