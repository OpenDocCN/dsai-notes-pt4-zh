- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P7：7.Self Attention and Non-parametric transformers
    (NPTs) - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT/Transformer 原理介绍 (中英文双字幕) - P7：7.自注意力与非参数变压器 (NPTs) - life_code - BV1X84y1Q7wV
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_0.png)'
- en: Thanks so much great to be here and happy Halloween related Halloween everyone
    so I think the talk is going to be split into two sections so I'll start by spending
    like 10 minutes 15 minutes chatting about transformers in general but I'm assuming
    most of you are familiar with them and we can move on to MPTs which Ya and Neil
    will be presenting。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢，很高兴来到这里，祝大家万圣节快乐。我认为这次演讲将分为两个部分，所以我会先花大约 10 到 15 分钟讨论变压器的一般情况，但我假设你们大多数人都熟悉它们，我们可以继续讨论
    MPT，Ya 和 Neil 会做介绍。
- en: 😊。![](img/1777b223f7bf8be27a8767d209c7f271_2.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 😊。![](img/1777b223f7bf8be27a8767d209c7f271_2.png)
- en: So let's see， I'm going like。Try to fly through the transformer overview and
    maybe spend。A little bit extra time on like the history of transformers and maybe
    just tell the story a little bit。I think that might be more interesting。嗯。So just
    in terms of the transformer architecture。the two kind of things that it introduced
    for the first time were multihead attention and selfat and then it combined those
    with fast utter aggressive decoding so before the Transer pretty much everyone
    was using LSTMs and LSTMs with attention I'll try to get into the difference of
    selfat multihead attention。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们来看看，我打算快速介绍一下变压器的概况，并可能在变压器的历史上花一点额外的时间，或者稍微讲述一下这个故事。我认为这可能会更有趣。嗯。就变压器架构而言，它首次引入的两种主要概念是多头注意力和自注意力，然后将这些与快速自回归解码结合在一起。所以在变压器之前，几乎每个人都在使用
    LSTM 及其带注意力的 LSTM，我会尽量解释自注意力和多头注意力之间的区别。
- en: So originally you would have two sequences and you would have a attention module
    which would attend from the source to the target and so each token or each word
    in the source sequence would get associated with you know a soft approximation
    of one element in the target sequence。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以最初你会有两个序列，然后你会有一个注意力模块，它会从源序列关注到目标序列，因此源序列中的每个令牌或每个词都会与目标序列中的一个元素的软近似相关联。
- en: And so you'd end up with something like like this， but with self attention。we
    did away with the two separate sequences， we make them both the same。and so you're
    relating each element within the sequence to another element in the sequence。And
    so the。The idea here is that you're learning a relationship of the words within
    a sentence to the other words。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最终得到的结果像这样，但具有自注意力。我们不再使用两个单独的序列，而是让它们相同。这样，你就可以在序列中将每个元素与另一个元素关联起来。这里的想法是你在学习句子中词语之间的关系。
- en: so you can imagine something like an adjective which is being applied to a noun
    and so you want to relate that adjective like the blue ball。you want to relate
    blue as referring to ball through learning patterns within the sequence interest
    sequence patterns。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以想象有一个形容词被应用于一个名词，因此你想要将这个形容词与名词关联起来，比如“蓝色的球”。你想通过学习序列中的模式将“蓝色”与“球”关联起来。
- en: 嗯。So sorry I gave this talk in Kenya， so I' am using Kewa Heley here， but with
    multihead attention。the idea is you have like each word represented by an embedding
    which is in the depth dimension here and then you have your sentence of words。you
    split that up into a bunch of different groups。so here I've droppedpped it depthwise
    into four groups。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。抱歉，我在肯尼亚做过这个演讲，所以我在这里使用 Kewa Heley，但使用多头注意力。这个概念是每个词都用一个嵌入表示，在这里是深度维度，然后你有一个单词的句子。你把它分成几组。所以在这里我在深度上分成了四组。
- en: You apply attention to each one of these groups independently and then when
    you get the result back you can catnate them together and you're back to your
    model dimension representation。What this lets you do is if each attention like
    each attention head can now focus on learning one pattern。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你对每一组独立应用注意力，当你得到结果后，可以将它们连接在一起，回到你的模型维度表示。这让你能够做到的是，如果每个注意力头现在可以专注于学习一种模式。
- en: so maybe attention head one is learning the relationship of adjectives to nouns
    and the second attention head can learn something different so this lets us learn
    like a hierarchy or a list of different relationships。Okay， so that was self attention。The
    other piece is fast auto oppressive decoding。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所以也许注意力头一正在学习形容词与名词之间的关系，而第二个注意力头可以学习一些不同的东西，这样我们就可以学习一个层次或不同关系的列表。好的，这就是自注意力。另一个部分是快速自回归解码。
- en: And do I really want to go into this？Okay， I will， so the important thing about
    this is it。If you're doing normal autoaggressive decoding， what you do is you
    generate your first token and now conditioned on that first token you generate
    the second and condition on the first two you generate the third and so on and
    so forth。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的想深入探讨这个吗？好吧，我会的，重要的是，如果你在进行正常的自回归解码，你会生成第一个标记，然后基于第一个标记生成第二个标记，基于前两个生成第三个，依此类推。
- en: but that's super slow right like it's a loop applying this thing again and again
    and so what we can do instead is we make an assumption in the code that our model
    always generates the right thing and we generate and then we generate a prediction。Only
    one token ahead and so the way that this looks is。You okay so。hy。Why hat here？Sorry
    once again。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但这超级慢，对吧，就像一个循环一次又一次地应用这个东西。因此，我们可以做的是在代码中假设我们的模型始终生成正确的内容，然后生成一个预测，仅仅提前一个标记，所以这个过程看起来是这样的。你还好吗？嗯，为什么这里有个帽子？抱歉，再一次。
- en: Input to output， so you have like your outputs， which are y， you have your targets，
    which are Y hat。And what you do is you feed in those gold targets so that you
    don't need to actually do this loop。so instead of assuming instead of having to
    generate the first token feed it back into your architecture。generate a second
    token， you feed in the entire target sequence and you just pretend that you generate
    all the right tokens up to position k and then you predict the K plus first and
    you compute your loss on that。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到输出，你有你的输出，即 y，你有你的目标，即 Y hat。你所做的是输入那些黄金目标，这样你就不需要实际执行这个循环。因此，不再假设生成第一个标记后将其反馈到架构中，生成第二个标记，而是输入整个目标序列，并假装生成所有正确的标记，直到位置
    k，然后预测 K+1，并计算该位置的损失。
- en: So in reality your model might have generated you know at the beginning of training
    junk。but you're getting a loss as if your model had seen all the correct tokens
    and is now just predicting the next one this is a little bit subtle but it's hugely
    impactful for training speed because all of this can be done un in parallel and
    so it's actually what makes transformers so scalable。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你的模型可能在训练开始时生成了一些无用的内容，但你得到的损失却仿佛模型已经看到了所有正确的标记，现在只是在预测下一个标记，这一点有些微妙，但对训练速度影响巨大，因为所有这些都可以并行进行，这正是让变压器模型如此可扩展的原因。
- en: Okay， so in order to do this successfully， if you were just feeding in all of
    the。All of the correct tokens naively， what would happen is your model would just
    be able to look forward in time and cheat。So you've put in all of your true targets，
    the things that you're trying to get your model to predict and so if that's where
    you're computing your loss on。it could just look forward in time and say， okay
    I'm just going to grab that and it would get zero error trivially right because
    you've given it all the right answers so what we have to do inside the architecture
    is we need to actually prevent the attention mechanism from being able to look
    at tokens that it shouldn't have been able to see already so the way that this
    looks is you create a mask on your attention。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，为了成功做到这一点，如果你只是简单地输入所有正确的标记，会发生什么呢？你的模型将能够向前看并“作弊”。你输入了所有的真实目标，也就是你希望模型预测的内容。如果这是你计算损失的依据，它就能向前看并说，好的，我只需抓取那个，它就会轻松获得零错误，因为你给了它所有正确的答案。因此，我们需要在架构内部实际防止注意力机制查看它不应该已经看到的标记。这个过程的方式是创建一个注意力掩码。
- en: 嗯。😊，And so sorry this is the example of like doing a trivial attention if you
    don't mask your attention properly what it's going to do is it's just going to
    look into the future just grab the token that you're telling it to predict and
    copy it over and so it learn something trivial something that doesn't actually
    generalize and so what we do is we。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。😊，所以抱歉，这是一个简单的注意力示例，如果你没有正确地掩盖你的注意力，它将只会向未来看，直接抓取你告诉它预测的标记并复制过来，因此它学到的东西是简单的，不会真正推广。因此，我们所做的是。
- en: Actually prevent it from attending to those tokens， we prevent it from attending
    into the future。For each position in the source sequence， we block out everything
    that it shouldn't be able to see。everything into the future， and then as we move
    down we gradually unblock so it can start to see into the past。嗯。So those are
    kind of like two the three major components of transformers， the self attention。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，为了防止模型关注那些标记，我们阻止它关注未来的内容。对于源序列中的每个位置，我们屏蔽了所有它不应该看到的内容。所有未来的内容，然后随着我们向下移动，逐渐解除屏蔽，让它可以开始看到过去。嗯。所以这些就像是变压器的三个主要组成部分之一，自注意力机制。
- en: the multihead attention， and then deploying this gold targets decoding fast
    utter restive decoding。嗯。In terms of the story， which might be a little bit more
    interesting。嗯。So transformers。I was an intern with Lukash Kaiser at Google back
    in 2017。and I was sitting next to Nome and Sheish was like a couple seats down
    from us。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力，以及快速解码黄金目标的部署。嗯。在故事上，这可能更有趣。嗯。变压器。2017年我在谷歌时是Lukash Kaiser的实习生，我坐在Noam旁边，Sheish在我们几排之外。
- en: And what's really incredible is that essentially this entire project came together
    in like three months。And it was done so I showed up at Google Noam had been working
    on。Autoag models。same thing with like Ashishish and Yaakov and Nikki and they'd
    just been kind of like exploring the space figuring it out and Luksh and I at
    the same time we had been working on this framework called Tensor to Tensor。嗯。Which
    was like explicitly made for multimodal learning autoreive learning and lukash
    is kind of like a master of。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 而且令人难以置信的是，这个项目基本上在三个月内完成。我在谷歌时，Noam一直在研究Autoag模型，Ashish、Yaakov和Nikki也是，他们一直在探索这个领域。与此同时，Lukash和我在工作一个名为Tensor
    to Tensor的框架。嗯。这个框架是专门为多模态学习和自回归学习而创建的，Lukash在这一领域是个高手。
- en: Keeping track of everything that's happening in the field and adopting it。and
    so within tensor to tensor， there were like these。There were like these kind of
    emerging little things that maybe one paper had been written about and people
    were interested in like layerorm。but it hadn't actually taken off yet the warmup
    in the learning rate schedule all of these little pieces were just default like
    on by default。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪领域中发生的所有事情并加以采用。因此，在tensor to tensor中，有这些。可能一篇论文提到过的一些新兴小东西，像layer norm，但其实还没有流行起来，学习率预热的所有这些小部分都是默认开启的。
- en: and so whennome and aishish and the Nikki and Yaak，Came over and adopted tensor
    to tensor。all of these things were just on by default。And so a lot of people。when
    they look at the transformer paper， it just seems like there's so many like arbitrary
    little things thrown in and when now like in present day these have become standard
    for like a lot of different training algorithms like the learning rate warm up。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 所以当Noam、Ashish、Nikki和Yaak过来并采用tensor to tensor时，所有这些功能都是默认开启的。因此很多人，当他们查看变压器论文时，似乎有很多任意的小东西被加入，而现在这些已经成为许多不同训练算法的标准，比如学习率预热。
- en: The way that we did initialization， all of these pieces have just become the
    norm。but back then they had like have just been introduced。And so。We we spent
    a lot of time running ablations trying to figure out like which were the necessary
    pieces and what made it work and if any of you have actually tried training transformers
    and tried like pulling out the learning rate warmup or changing any of these little
    pieces。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化的方式，所有这些部分都已成为规范。但那时它们刚刚被引入。因此，我们花了很多时间进行消融实验，试图弄清楚哪些是必要的部分，是什么让它有效的。如果你们中的任何人尝试过训练变压器，并试图去掉学习率预热或更改任何这些小部分。
- en: you'll see that it really does break down optimization like it actually really
    does hurt performance。For instance， like removing the layer arms， that type of
    thing。嗯。So I always thought it was kind of funny how。All of these random editions
    that Luash had just like thrown in because he was playing around with them turned
    out to be crucial and they were just on by default。嗯。So anyway， it was like three
    months， I remember。It all really started coming together towards the end。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，它确实在优化上产生了影响，实际上确实会影响性能。例如，去掉layer norm，类似的事情。嗯。我总觉得很有趣，Lukash随意添加的所有这些随机功能，结果却是至关重要的，都是默认开启的。嗯。所以，无论如何，我记得是在三个月内，一切真的开始在最后阶段汇聚。
- en: like just before the Nup deadline。And I can still remember sitting in the micro
    kitchen and a sheish telling me like as just like I was a little intern telling
    me like this is going to be such a big deal and I was like yeah sure okay like
    I have no idea what's happening I just showed up。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 就在Nup截止日期之前。我仍然记得坐在微型厨房，某个同事告诉我，像我这样的小实习生，跟我说这将是一个大事件，我当时心想，好的，我真的不知道发生了什么，我只是出现了。
- en: And he was like， no dude， like this this actually matters like you know。we bumped
    up blue three points and I was like sick， great anyway。嗯。😊。And then I can remember
    on the night of the deadline for nerves。It was like 2 am a shish。Sheish was the
    only one left at the office and we were still like moving around figures and like
    adjusting things and then I went to bed but she stayed up and I slept in like
    this tiny little phone booth。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 他当时说，不，伙计，这实际上很重要，你知道吗。我们提升了蓝色三分，我心想太好了，随便吧。嗯。😊。然后我记得在提交截止日期的前一晚。那时是凌晨两点，某个同事是办公室里最后一个留下的人，我们仍在移动数据和调整东西，然后我去睡觉了，但她继续熬夜，而我则在一个小小的电话亭里睡觉。
- en: And then for the other paper that I was submitting， I forgot to press submit，
    but luckily。Like some lady opened the door to the phone booth and hit me in the
    head while I was sleeping in the morning and just before the deadline I got the
    paper in and so I owe it to that lady for submitting to N that year but yeah anyway
    the I think the crazy thing about transformers was that it all came together in
    like three months like most of the ideas happened in that span and it was just
    like this sprint towards the N deadline。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在我提交的另一篇论文中，我忘记按提交，但幸运的是。有位女士在早上打开电话亭的门，打了我一下头，就在截止日期之前，我把论文提交了，所以我得感谢那位女士那年提交到N，不过没关系，我认为关于transformers的疯狂之处在于，所有的事情都在三个月内聚集在一起，绝大多数想法都是在那段时间发生的，这就像是朝着N截止日期的冲刺。
- en: Um， and I think a lot of the other members on the team。Yaka Lukaash she they
    knew how important it was， but for me I was like。I don't know。I really did not
    appreciate the impact， but in retrospect it's been amazing how the community has
    kind of like come together and adopted it。😊，And I think most of that can be ad
    to the ease of optimization。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，我认为团队中的其他成员。Yaka Lukaash她们知道这有多重要，但对我来说，我是。 我不知道。我真的没有意识到它的影响，但回想起来，社区是如何团结在一起并采纳它的，真是太惊人了。😊我认为这大部分归功于优化的便利性。
- en: it seems like very robust to hyperparameter choices so you don't need to like
    tune the hell out of it。spend a lot of time tweaking little things。And the other
    side is that it's like super tailored to the accelerators that we run on。So it's
    like very paralyzable， hyper efficient， and so it lends itself to that kind of
    scaling law effort that's really taken off in popularity。Okay， unless there are
    any questions that。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎对超参数选择非常稳健，所以你不需要花费很多时间去调整细节。而另一方面，它非常适合我们运行的加速器。所以它非常易于并行处理，超级高效，因此它适合那种迅速流行的规模法努力。好吧，除非有任何问题。
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_4.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_4.png)'
- en: We're both excited， so we just unmuteed at the same time。Yeah， so cold。Yeah。sos
    that's my section if there's any questions happy to answer them otherwise。Let's
    get into NPs NPptTs are like I think。There's such a nice next level abstraction
    of the architecture。so you've probably seen the trend of。Transformers getting
    applied to new domains。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们都很兴奋，所以我们同时解除静音。是的，太冷了。是的。这是我的部分，如果有任何问题我很乐意回答，否则。我们开始讨论NP。NPptTs就像我认为的那样。有一个很好的下一个层次的架构抽象。所以你可能看到了，Transformers被应用到新的领域的趋势。
- en: first into vision and video and audio。But this is kind of like cutting back
    to an even more abstract level。like I think tabular data， yeah， I don't know，
    I'll Yna and Ne take over from here。but I think MPT is a pretty sick， pretty sick
    project。Thanks A for the introduction thanks all for the invitation we're very
    happy to be here and Neil and I are now going to tell you about our selfattention
    between data points paper where we introduce introduce the nonprometric transformer
    architecture we'll start with a little bit of motivation we want to explaining
    the architecture detail Im show you the experiments this is more or less a step
    through of the paper but maybe you know with a little bit extra insight here and
    there。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先是视觉、视频和音频。但这就像回到一个更抽象的层面。我认为表格数据，嗯，我不知道，我会让 Yna 和 Ne 从这里接手，但我认为 MPT 是一个非常棒的项目。感谢
    A 的介绍，感谢所有人的邀请，我们非常高兴能在这里，Neil 和我现在将告诉你关于我们数据点间自注意力论文的内容，我们引入非参数变压器架构，我们将从一些动机开始，解释架构细节，展示实验，这基本上是对论文的逐步解读，但可能会在这里和那里提供一些额外的见解。
- en: All right， as promised， the motivation and a brief summary。So we'll start by
    thinking about something that we don't often think about。that is that from seal
    to transformers， most of supervised deep learning relies on parametric prediction。So
    what that means is that we have some self training data and we want to learn to
    predict the outcomes y from the inputs X and for this we set up some model with
    tunable parameters theta。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，正如承诺的那样，动机和简要总结。所以我们将开始思考一些我们不常考虑的事情，那就是从封 seal 到变压器，大多数监督深度学习依赖于参数预测。这意味着我们有一些自我训练数据，我们想要学习从输入
    X 预测结果 y，为此我们设置一些带有可调参数 theta 的模型。
- en: then we optimize these parameters to maximize predictive likelihoods on a training
    set or you know equivalently we minimize some loss。And then after training， we
    have this optimized set of parameters theta。and then at test time we just put
    these into the model and use these parameters to predict on novel test data。And
    so crucially here， our prediction at test time only depends on these parameters。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们优化这些参数以最大化训练集上的预测似然，或者说我们等价于最小化某些损失。然后在训练后，我们得到了这组优化的参数 theta，然后在测试时我们只需将这些参数放入模型中，并用这些参数对新测试数据进行预测。因此，关键是，我们在测试时的预测仅依赖于这些参数。
- en: right it's parametric。Also that means that given these parameters。the prediction
    is entirely independent of the training data and so why would we want to do parametric
    prediction？
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 对，参数化。这也意味着，给定这些参数，预测完全独立于训练数据，那么我们为什么要进行参数预测呢？
- en: Well， it's really convenient because all that we've learned from the training
    data can be summarized in the parameters and so at prediction time we only need
    these final parameters and we do not need to store the training data。which might
    be really， really large。On the other hand。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常方便，因为我们从训练数据中学到的所有内容都可以在参数中总结，因此在预测时我们只需要这些最终参数，而不需要存储可能非常庞大的训练数据。另一方面。
- en: we usually have models that already predict for a bunch of data in parallel
    right think of mini batching and modern architectures and actually things like
    batch on already make these data interact。And so our thinking here was that if
    we've got all of this data in parallel anyways。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常有模型可以并行预测一堆数据，想想小批量处理和现代架构，实际上像批量处理这样的东西已经使这些数据相互作用。因此我们在这里的想法是，如果我们反正有所有这些并行数据。
- en: there's no reason not to make use of it and so more a bit grounder we kind of
    challenge carmetric prediction as the dominant paradigm in deep learning and so
    we want to give models the additional flexibility of using the training data directly
    when making predictions。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由不利用它，因此我们挑战卡尔梅里克预测作为深度学习中的主导范式，我们希望在进行预测时给予模型使用训练数据的额外灵活性。
- en: And so a bit more concretely。😊，We introduced the nonparmetric transformer architecture。and
    this is going to be a general deep learning architecture。meaning we can apply
    to a variety of scenarios。![](img/1777b223f7bf8be27a8767d209c7f271_6.png)
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体一点。😊 我们引入了非参数变压器架构。这将成为一个通用的深度学习架构，意味着我们可以应用于多种场景。![](img/1777b223f7bf8be27a8767d209c7f271_6.png)
- en: NPTs will take the entire data set as input whenever possible。And NPTs then
    crucially learn to predict from interactions between data points。And to achieve
    this。we use multi head self attention。That as Age has introduced us to。has just
    really established itself as a general purpose layer for reasoning。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: NPTs将尽可能将整个数据集作为输入。然后，NPTs关键地学习从数据点之间的交互中进行预测。为了实现这一点，我们使用多头自注意力。这正如Age向我们介绍的，已经真正确立为一种通用推理层。
- en: We also take another thing from the NLP community and we use a stochastic masking
    mechanism and we use that to tell entitiesmpes where to predict and also to regularise
    the learning task of it。And last year， of course， we hope to convince that the
    ends up working really。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还借鉴了自然语言处理社区的另一项内容，使用随机掩码机制，告诉实体样本在哪里进行预测，并且也对其学习任务进行正则化。去年，当然，我们希望能够证明最终效果很好。
- en: really well and that this kind of simple idea of learning to predict from the
    other data points of the input。from the training points of the input and up working
    as as well。So。and so very briefly summarizing what we've heard already。A， we input
    into NPptTCide dataset。And then B， let's say for the purpose of this slide here。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的想法，学习从输入的其他数据点进行预测，实际上工作得很好。因此，非常简要地总结我们已经听到的内容，A，我们输入NPptTCide数据集。然后B，假设为了这个幻灯片的目的。
- en: we only care about predicting the orange question mark in that green row。And
    then we can compare entitiespts to parametric prediction right so a classical
    deep learning model would predict this target value only from the features of
    that single grid input to do that it would use the parameters theta。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心在那一绿色行中预测橙色问号。然后，我们可以将实体点与参数预测进行比较，经典的深度学习模型将仅从该单个网格输入的特征中预测这个目标值，为此它将使用参数θ。
- en: those would depend on whatever training data we've seen and so on。but at test
    time we only look at that single row for which we care about the prediction。😊。In
    contrast， NPpts predict an explicit dependence on all samples in the input they
    can look beyond that single green data of interest and look at all other samples
    that are there and consider their values for prediction so this presents an entirely。😊，Different
    way of thinking about how we learn predictive mechanisms somebody on Twitter called
    this Canan 2。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些将依赖于我们所看到的任何训练数据等，但在测试时我们只关注那一行，我们关心的预测。😊。相比之下，NPpts明确预测输入中所有样本的依赖关系，它们可以超越那单个绿色感兴趣的数据，查看所有其他样本并考虑它们的值进行预测，因此这呈现出一种完全不同的方式来思考我们如何学习预测机制，有人在推特上称之为Canan
    2。
- en: 0， which we would have not written in the paper， but maybe is kind of a nice
    way of thinking about how NPTs can learn to predict。😊。![](img/1777b223f7bf8be27a8767d209c7f271_8.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 0，这在论文中没有写，但也许是思考NPTs如何学习预测的一个不错的方法。😊！[](img/1777b223f7bf8be27a8767d209c7f271_8.png)
- en: So of course， non parametric models are a thing already， we didn't invent them
    at all and。![](img/1777b223f7bf8be27a8767d209c7f271_10.png)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，当然，无参数模型已经存在，我们并没有发明它们。![](img/1777b223f7bf8be27a8767d209c7f271_10.png)
- en: I defined them here as prediction in explicit dependence on the training data。which
    is certainly what MPptTs do。Classical examples like Gaussian processes， can neighbor。kernelnal
    methods， those might be familiar to you。😊。And there exists also efforts to combine
    the benefits of nonprometrics and representation learning in a similar fashion
    to how we did it in entities。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里将它们定义为对训练数据的显式依赖预测，这无疑是MPptTs所做的。经典的例子如高斯过程、邻域方法、核方法，这些可能对你来说是熟悉的。😊。也有努力将无参数和表示学习的好处结合起来，类似于我们在实体中所做的。
- en: 😊，However， these approaches are usually limited in some sense in comparison
    opportunities right they're often kind of motivated from the statistics community
    a bit more they often require more fiically approximate inference schemes are
    limited in the interactions they can learn or things like that and so。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，然而，这些方法在某种意义上通常受到限制，与机会相比，它们往往更多地受到统计学界的驱动，通常需要更精确的近似推断方案，限制了它们可以学习的交互或类似的事情。
- en: 😊，We really think NPTs present。Maybe the most versatile and most widely applicable
    of these nonprometric prediction approaches but that's something we explicitly
    wanted to have we wanted to have something that's really easy to use plug play
    works in a ton of scenarios and works really well。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我们真的认为NPTs很出色。也许是这些非参数预测方法中最通用和应用最广泛的，但这正是我们明确想要的，我们希望有一种真正易于使用的方式，适用于许多场景，并且效果很好。
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_12.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_12.png)'
- en: And so with that， I'd hand over to Neil， whos going to tell you about the nonparmetric
    transformer architecture in all of its details。you also have one question hi Jen
    could you please go to the previous slide？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这一点上，我会把话转给Neil，他将告诉你有关非参数变换器架构的所有细节。你还有一个问题，嗨，Jen，您能否回到上一张幻灯片？
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_14.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_14.png)'
- en: The very previous slide。Yeah， yes。This slide。Yeah。So in terms of the problem
    definition。I think is' quite similar to some meta learning problem。which basically
    learns a mapping from a data point and the data sets to some predictions。So could
    you please suggest any differences between your problem setting and meta learning
    problem setting。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的幻灯片。是的，没错。这一幻灯片。是的。在问题定义方面，我认为这与某些元学习问题相似，基本上是学习从数据点和数据集到某些预测的映射。那么，请问您能指出您问题设置与元学习问题设置之间的任何区别吗？
- en: I can't really。Fed out any differences between these two problems。Well。I think
    it really depends on the framing that you want to have right so I would say meta
    learning would be when I try to predict over multiple data sets。so when I try
    to predict some when I try to learn some sort of prediction model or I can just
    plug in a different data set and it will automatically or almost automatically
    give me new predictions on this different data distribution。But that's not what
    we do at all， right， we're training a single model for a fixed data set。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我实在是无法找出这两个问题之间的任何区别。嗯，我认为这真的取决于您想要的框架，所以我会说元学习是当我试图在多个数据集上进行预测时。因此，当我试图学习某种预测模型，或者我可以插入不同的数据集时，它会几乎自动地为我提供在这种不同数据分布上的新预测。但这根本不是我们所做的，对吧？我们是在为固定数据集训练单个模型。
- en: And so this is why I wouldn't really call that meta learning because we're doing。we're
    trying to predict on the same tasks that all the supervised deep learning or any
    supervised machine learning method is trying to predict well on。Consuming you
    use kind of same test site to test your trend model， right？I mean， I mean， like。呃。So。So
    basically in MI learning we're going to test on different kind of meta test sets。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是为什么我不想称其为元学习，因为我们正在做的事情是，我们试图在所有监督深度学习或任何监督机器学习方法试图准确预测的相同任务上进行预测。前提是你使用相同的测试集来测试你的趋势模型，对吧？我的意思是，呃。所以。基本上在MI学习中，我们将在不同的元测试集上进行测试。
- en: but in your case you just want to use a test set which is similar to the distribution
    Yeah of your training set right？
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在您的情况下，您只想使用一个与您的训练集分布相似的测试集，对吧？
- en: Yeah absolutely so we explore data set distribution shift a bit。I think it's
    a really interesting scenario I think meta learning different data sets is also
    an interesting scenario right when you have this model where you just pressure
    in different data sets but for the scope of this paper it's very much training
    set test set they come from the same distribution and we're just trying to do
    supervised learning in a standard setting thats so cool thank you。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，绝对如此，我们稍微探讨了一下数据集分布的变化。我认为这是一个非常有趣的场景，而元学习不同的数据集也是一个有趣的场景，对吧？当你有这个模型时，可以输入不同的数据集，但就本文的范围而言，训练集和测试集来自同一分布，我们只是试图在标准设置中进行监督学习，这真是太酷了，谢谢。
- en: 😊，Thank you for the question。![](img/1777b223f7bf8be27a8767d209c7f271_16.png)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，感谢您的提问。![](img/1777b223f7bf8be27a8767d209c7f271_16.png)
- en: Yeah， and I would chime in a couple additional things， I guess。so at least from
    what I understand from the problem definition of meta learning。I think the aim
    is more。Perhaps being able to perform well on a new data with a relatively small
    number of additional gradient steps on that data set。so I think there's some interesting
    ways that you could actually consider applying NPTs in a meta learning type setting
    and so we'll get into this a little bit more but for example you know there might
    be ways to essentially add in a new data so let's suppose we've trained on a bunch
    of different data sets we now add in a new data set we can perhaps do some sorts
    of kind of zero zero shot meta learning basically where there's no need for additional
    gradient steps because we're basically predicting kind of similar to how you might
    do prompting nowadays in NLP literature。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我想我可以补充几点。我理解的元学习问题定义的目标更倾向于能够在新数据上表现良好，并且在该数据集上只需少量额外的梯度步骤。所以，我认为有一些有趣的方式可以在元学习的设置中考虑应用NPTs，我们稍后会更深入探讨这个问题。例如，假设我们已经在许多不同的数据集上进行训练，现在添加一个新数据集，我们可以进行一些类似于零样本元学习的操作，基本上不需要额外的梯度步骤，因为我们基本上是以类似于现在NLP文献中的提示方式进行预测。
- en: Anyways， yeah， I think we'll get into some more details。Just to chime in on
    that I don't think that every meta learning algorithm I think the ones that you're
    described right now are like optimization based。but they're also black box ones
    like you don't need to。Further。I think the main difference seems to be that there
    is like one task versus multiple tasks for meta learning。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，是的，我认为我们会更详细地讨论这个问题。我想补充的是，我并不认为每种元学习算法都是基于优化的，像你现在描述的那些算法也是黑箱型的，不需要进一步的细化。我认为主要的区别似乎在于单任务与多任务的元学习。
- en: Yeah， I I think so too， I think the the like main， yeah。theing the main framing
    question is whether or not there's multiple data sets。Cool。Okay， awesome。If there's
    no other questions， I'll dive a bit more into the architecture。![](img/1777b223f7bf8be27a8767d209c7f271_18.png)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我也这样认为，主要的问题是是否有多个数据集。很好。好的，太好了。如果没有其他问题，我将更深入地探讨架构。![](img/1777b223f7bf8be27a8767d209c7f271_18.png)
- en: Awesome， so there's three key components to NPpts。I'm gonna to first state them
    at a high level。and then we'll go through each of them in more detail。So， first
    of all。we take the entire data set。All data points is input。So， for example， at
    test time。the model is going to take as input， both training and test data。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，NPpts有三个关键组成部分。我将首先从高层次陈述它们，然后我们将更详细地讨论每个部分。因此，首先，我们取整个数据集。所有数据点作为输入。例如，在测试时，模型将同时使用训练数据和测试数据作为输入。
- en: And we approximate this with mini batches for large data。😊。We apply self attention
    between data points， so for example， at test time。we model relationships amongst
    training points， amongst test points and between the two sets。And then finally
    we have this masking based training objective。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过小批量来近似处理大数据。😊。我们在数据点之间应用自注意力，例如，在测试时，我们建模训练点、测试点之间以及两组之间的关系。最后，我们有一个基于掩蔽的训练目标。
- en: it's a burnt like stochastic masking and the key point is that we actually use
    it on both features as well as on training targets and we'll get into why that
    kind of leads to an interesting predictive mechanism later。Sure。So to start with
    this idea of data sets as input。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种类似随机掩蔽的烧录，关键点在于我们实际上在特征和训练目标上都使用它，稍后我们会讨论为什么这导致了一种有趣的预测机制。好的。那么首先从数据集作为输入的这个想法开始。
- en: there's two things that compose the input to NPT， it's a full data set in the
    form of a matrix X and a masking matrix M。And so Yick has described this data
    set matrix a little bit， we basically have data points as rows。the columns are
    attributes and each attribute shares some kind of semantic meaning among all of
    its data points。so say for example you're just doing single target classification
    or regression the last column would be the target and the rest of the matrix would
    be input features so for example the pixels of an image。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 组成NPT输入的有两个部分，一个是以矩阵X形式的完整数据集，另一个是掩蔽矩阵M。因此，Yick对这个数据集矩阵进行了稍微描述，我们基本上有数据点作为行，列是属性，每个属性在所有数据点之间共享某种语义意义。例如，如果你正在进行单目标分类或回归，最后一列将是目标，其余的矩阵将是输入特征，例如图像的像素。
- en: We also have a masking matrix， so let's say you know we're thinking about mass
    language modeling the mass tokens will just tell us where we're going to conceal
    words and where we're going to backproagate a loss we do a similar type of thing
    here where we use this binary mass matrix to specify which entries are masked。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一个掩码矩阵，所以假设你知道我们在考虑大规模语言建模，大规模的标记会告诉我们在哪里隐藏单词，以及我们在哪里反向传播损失，我们在这里做类似的事情，我们使用这个二进制掩码矩阵来指定哪些条目是被掩盖的。
- en: And the goal is to predict mass values from observed values。I see that there
    was a question about handling inputs with different lengths。In the data sets we've
    considered we'll get into it in the results section。but it's mostly been sort
    of tabular in image data where the lengths for each of the data points is the
    same。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是从观察值中预测掩码值。我看到有一个关于处理不同长度输入的问题。在我们考虑的数据集中，我们将在结果部分讨论这一点，但大多数数据集基本上都是表格数据和图像数据，其中每个数据点的长度是相同的。
- en: but it would work just like padding that would be a reasonable way to go about
    that and there's also kind of an interesting yeah go for an。This to add to that
    I'm not sure if length prefers refers to columns or two rows right rows we don't
    care about how many rows length padding or something would be an option Yeah my
    question was about column exactly so that that makes sense I think。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但它的工作方式就像填充，这将是一个合理的方法，并且还有一些有趣的，嗯，继续。对此我不确定“长度”是指列还是行，实际上我们不关心行的数量，长度填充之类的会是一个选择。是的，我的问题正是关于列的，所以这有道理，我想。
- en: Yeah， and I mean， that goes along with the whole meta learning discussion is
    I think if we wanted to adapt to data sets that have a different number of data
    data points per data set。you know， we can take advantage of the fact that self
    attention is kind of okay with that。Cool。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我是说，这与整个元学习讨论有关，我认为如果我们想适应每个数据集具有不同数量数据点的数据集，你知道，我们可以利用自注意力在这方面的优势。酷。
- en: So continuing on。Le to discuss here is basically how we do the embedding so
    to put this more explicitly we have this data matrix it has n data points。it's
    called X and it all has D attributes and we have the binary mass matrix M we're
    going to stack them and then we're going to do a linear embedding so specifically
    we're doing the same linear embedding independently for each data point we're
    learning a different embedding for each attribute。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以继续下去，接下来讨论的是我们如何进行嵌入，更明确地说，我们有这个数据矩阵，它有 n 个数据点，称为 X，所有数据点都有 D 个属性，我们有二进制掩码矩阵
    M，我们将它们堆叠在一起，然后进行线性嵌入，具体来说，我们对每个数据点独立地进行相同的线性嵌入，我们为每个属性学习不同的嵌入。
- en: We have a positional encoding on the index of the attributes because we don't
    really care about say being equi over the columns。if it's tabular data you of
    course want to treat all these kind of heterogeneous columns differently and then
    finally we have an encoding on the type of columns so whether or not it's continuous
    or categorical。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对属性的索引进行了位置编码，因为我们并不真的关心在列上是均匀的。如果是表格数据，你当然想要以不同的方式对待所有这些异构列，最后我们对列的类型进行了编码，因此无论是连续的还是分类的。
- en: And that ends up giving us this input data set representation that is dimensions
    n by D by E。The second key component of NPpts is tension between data points。So
    to do that。we first take this representation we have and flatten to an end by
    d times e representation。so basically we're treating each of these d times e size
    rows as if it's a token representation。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这最终给我们带来了这个输入数据集的表示，维度为 n x D x E。NPpts 的第二个关键组成部分是数据点之间的张力。为了实现这一点，我们首先将我们拥有的表示展平为
    n x d x e 的表示。所以基本上，我们将这些 d x e 大小的行视为令牌表示。
- en: We're actually going to just accomplish this operation using multied selfat
    you know we've reviewed this a lot。but the nice thing is that we know from language
    modeling we stack this multiple times we can model these higher order dependencies
    and here they between data points and that's really the key draw of this architecture
    there's been other kind of instances of people using attention for similar sorts
    of things so for example like attentive neural processes a lot of times they've
    sort of used just a single layer as kind of representational lookup and we believe
    that this actually ends up limiting expresssivity and that by sacking this many
    times you can learn more complex relationships between the data points。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上要用多重自注意力来完成这个操作，你知道我们已经多次审查了这个。但好的一点是，我们知道在语言建模中，我们可以多次堆叠它，建模这些数据点之间的高阶依赖关系，这实际上是该架构的关键优点。还有其他实例使用注意力来处理类似的事情，例如注意神经过程，很多时候他们只使用单层作为表示查找，我们认为这实际上限制了表达能力，通过多次堆叠，你可以学习数据点之间更复杂的关系。
- en: On any lots of questions。So you can go ahead first。OhCool thanks I have a question
    about like how you guys do the embedding is there always like arties like convolutional
    filters or like linear layers like what is the type of embedding that you guys
    use Yeah so i'm attempting to go back to the slide I think it's not not very happy
    with linear now but yeah so for for Tular data we did just linear embeddings actually
    so we。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多问题。那么你可以先问。哦，太好了，谢谢，我有一个问题，比如你们如何进行嵌入，是否总是像卷积滤波器或线性层这样的属性，或者你们使用的嵌入类型是什么？是的，我尝试回到幻灯片，我认为对线性现在不是很满意，但对于Tular数据，我们实际上只用了线性嵌入。
- en: You know， we we could get into like， I guess details of feturization for categorical
    and continuous。but it's literally like， say for categorical， you know。you do a
    one hot encoding and then you learn this embedding that is specific to that attribute
    and then for numerical。I believe we were just standard onizing for the image data
    we did end up using。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，我们可以深入讨论类别和连续特征化的细节，但实际上，对于类别特征，你会进行独热编码，然后学习与该属性特定的嵌入；而对于数值特征，我相信我们只是进行了标准化，对于图像数据我们最终使用了。
- en: A Resnet 18 encoder for C4 10。however， I think that I mean。we'll discuss that
    a bit later in results， but that embedding is a bit arbitrary。you can sort of
    do whatever the key part of the architecture is the attention between data points。So
    it's in terms of how you actually want to embed each attribute。 it's kind of up
    to you。Thanks。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于C4 10的Resnet 18编码器。然而，我认为我们会在结果中稍后讨论这一点，但这个嵌入有点任意。你可以做任何事情，架构的关键部分是数据点之间的注意力。所以在你实际想要嵌入每个属性时，这取决于你。谢谢。
- en: I think another question， same question as Victor Tos。Awesome。Cool。so here we
    have attention between data points done。so we can also do this attention between
    attributes。So we reshape back to this N by D by E representation and then we can
    just apply self- attention independently to each row in other words to a single
    data point and the intuition for why we would kind of do this nested type idea
    where we switch between attention between data points and attention between attributes
    is just we're trying to learn better per data point representations for the between
    data point interactions this is literally just normal self-atten。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为另一个问题，和Victor Tos的问题一样。太棒了。这里我们完成了数据点之间的注意力。那么我们也可以在属性之间进行这种注意力。我们将其重塑为N乘D乘E的表示，然后我们可以独立地对每一行应用自注意力，换句话说，对单个数据点进行自注意力。我们这样进行嵌套的原因是，我们试图为数据点之间的交互学习更好的每个数据点表示，这实际上只是普通的自注意力。
- en: as you'd see in language modeling or image classification the attributes are
    the tokens here。And finally， we just minins and repeat。So what are we actually
    getting out of this to summarize we're learning higher order relationships between
    data points？
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在语言建模或图像分类中看到的，属性在这里是令牌。最后，我们只需挖掘并重复。所以我们究竟从中得到什么，总结一下，我们在学习数据点之间的高阶关系？
- en: We're learning transformations of individual data points。And then importantly。NPT
    is equivariant to a permutation of the data points。this basically just reflects
    the intuition that the learned relationships between the data points should not
    depend on the ordering in which you receive them or in which you observe your
    data set。The third key component of NPptTs is a masking based string objective。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在学习单个数据点的变换。重要的是，NPT对数据点的排列是等变的。这基本上反映了一个直觉：数据点之间学习到的关系不应该依赖于你接收它们的顺序或观察数据集的顺序。NPptTs的第三个关键组成部分是基于掩码的字符串目标。
- en: So recall that what we're trying to do is we're trying to predict missing entries
    from observed entries and those mass values can be both features or targets so
    again the classic use say mass language modeling is to do self-supervised learning
    on a sequence of tokens which you could think of as just having features in our
    setting ours is a bit different in that we do stochastic feature masking to mass
    feature values with a probability piece sub future and then we also do this masking
    of training targets with this probability piece subtart so if we write out the
    training objective。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以回想一下，我们的目标是从观察到的条目中预测缺失的条目，而这些缺失的值可以是特征或目标。因此，经典的应用，比如掩码语言建模，就是对一系列标记进行自监督学习，你可以认为在我们的设定中，它与特征略有不同，因为我们使用随机特征掩码以某个概率掩盖特征值，然后我们还使用这个概率对训练目标进行掩码。如果我们写出训练目标。
- en: We are just taking a weighted sum of the negative log likelihood loss from targets
    as well as from features。and of course at test time we're only going to mask and
    compute a loss over the targets of test points。So to break this down a bit further
    and point out some of the cool parts of it here。the thing that's highlighted right
    now on the far right is the term relating to the features。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仅仅是对目标和特征的负对数似然损失进行加权求和。当然，在测试时，我们只会对测试点的目标进行掩码和计算损失。为了进一步分解这一点并指出一些有趣的部分，现在右侧突出显示的就是与特征相关的术语。
- en: it's the feature masking， basically we find that this has a nice regularizing
    effect more or less the model can now predict anywhere and makes the task a bit
    harder and introduce some more supervision and we found in an ablation for the
    tabular dataset sets that it helped for eight of 1 of those。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是特征掩码，基本上我们发现这有一个良好的正则化效果，模型现在可以在任何地方进行预测，使得任务变得稍微困难一些，并引入更多的监督。在对表格数据集进行消融实验时，我们发现这对八个数据集的效果都很好。
- en: And then there's this other term， which is kind of interesting it's this stochastic
    target masking and the idea is that。You're actually going to have some training
    targets unmasked to the model at input at training time。which means that the NPptT
    can learn to predict the mask targets of certain training data points using the
    targets of other training data points as well as all of the training features。And
    so that means you don't actually need to memorize a mapping between training inputs
    and outputs in your parameters。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 然后还有另一个有趣的术语，这就是随机目标掩码，想法是：在训练时，某些训练目标实际上不会被模型掩盖。这意味着NPptT可以利用其他训练数据点的目标以及所有训练特征来预测某些训练数据点的掩码目标。因此，这意味着你实际上不需要在参数中记忆训练输入和输出之间的映射。
- en: You can instead devote the representational capacity of the model to learn functions
    that use other training features and targets as input。So this is kind of getting
    into the idea of this sort of like learn K and N idea。You know。obviously， we can
    be learn more complex， relational lookups and those sorts of things from this。But
    you can imagine one such you know case being， we have a bunch of test data points
    coming in。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将模型的表示能力用于学习使用其他训练特征和目标作为输入的函数。因此，这有点涉及到类似学习K和N的想法。显然，我们可以从中学习更复杂的关系查找等内容。但你可以想象一个这样的案例：我们有一堆测试数据点进入。
- en: We're going to look at their features and use that to assign them to clusters
    of training data points and then our prediction for those points is just going
    to be an interpolation of the training targets in that respective cluster that's
    like an example of something that this mechanism lets NPptTs learn。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看它们的特征，并利用这些特征将其分配到训练数据点的聚类中，然后我们对这些点的预测仅仅是相应聚类中训练目标的插值，这就像是这个机制让NPptTs学习的一个例子。
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_20.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_20.png)'
- en: All right， so if there's any questions， we can take them now。otherwise I'm happy
    to take them in the discussion or something。All right。So let's discuss go for
    it curious when you're using the entire data set。Is that limit the type of data
    sets you can use because of the size？Yeah， so in practice。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，如果有任何问题，我们现在可以提问。否则，我很乐意在讨论中或者其他时候回答。好的。那么，让我们讨论一下，当你使用整个数据集时，这是否限制了你能使用的数据集类型，因为数据量的大小？是的，在实践中。
- en: we do random mini batchching as an approximation， so the idea is just you know
    if you have a reasonably large mini batch。you're going to benefit a bit from still
    having kind of this lookup ability because if youre reasonable number of classes。probably
    you're going to be able to learn some you know interesting mappings based on features
    and targets amongst those classes we found in practice that you know and we'll
    get into this a little bit。but we do actually indeed learn to use relationships
    between data points on prediction for data sets where we're doing mini batchching
    and we also didn't necessarily find that you need like a ludicrously large batch
    size for this to be a thing but I do think it's just this is in general and important
    point and it's one that points us towards looking into say you know sparse transformers
    literature for trying to expand to some larger data sets without having the mini
    batchching assumption。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行随机小批量处理作为一种近似，所以这个想法是，如果你有一个相对较大的小批量，你会从中受益，因为如果类别数量合理，你可能会基于特征和目标学到一些有趣的映射。在实践中我们发现，确实能够利用数据点之间的关系进行预测，对于我们进行小批量处理的数据集，我们也并没有发现需要非常大的批量大小才能做到这一点，但我认为这是一个普遍且重要的观点，这也指引我们去研究稀疏变换器文献，以尝试在不假设小批量处理的情况下扩展到一些更大的数据集。
- en: Great， thank you。If I can add a number to that， we can without mini batchching
    accommodate data sets off around like 8000 points or so。so that already accounts
    for a fair proportion I would say of the tabular data sets out there but we also
    do data sets with  11 million points where obviously we then resort two mini batchching
    so it's very good to have like an idea of the sizes that we're talking about。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，谢谢。如果我可以加一句，我们在没有小批量处理的情况下，可以容纳大约8000个点的数据集。所以这已经占据了相当一部分我认为的表格数据集，但我们也处理了1100万个点的数据集，显然我们那时就需要
    resort 到小批量处理，所以了解我们讨论的数据集大小是非常重要的。
- en: I'm curious on that， I mean it's pretty exciting， I feel like you don't normally
    hear about。Transformers being applied to the sets sub size 8000。U。I'm curious
    and we could talk about this sort of later once we've covered the other material
    if you found that sample efficiency is one of the key gains here or just experience
    working on small data of transformers generally and yeah。but I'll happy to put
    the answer to that until after as part of this。😊，Yeah， I I think that'd be。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我对此很感兴趣，我觉得这很令人兴奋，我感觉你通常不会听到变换器被应用于小于8000的子集。U。我很好奇，我们可以在覆盖其他材料后再讨论一下，如果你发现样本效率是这里的关键收益之一，或者只是一般在小数据上工作的变换器的经验。不过，我很乐意将这个问题的答案留到后面。😊，是的，我想这会很不错。
- en: that'd be really nice to talk about a bit。And it was something that in general，
    I guess， I'd say。was surprising to us in terms of how robust NPTs were on small
    data sets and how we surprisingly didn't have to tune a terrible number of parameters。But
    we can get into details in a bit。😊，awesome。So to get into the experiments。we focused
    a lot on tabular data because it's a very general setting。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很不错，可以聊一聊。而且一般来说，我想我会说，NPT在小数据集上是多么稳健，令我们惊讶的是我们并没有需要调整很多参数。但我们可以稍后深入细节。😊，太棒了。那么，让我们开始实验。我们非常关注表格数据，因为这是一个非常通用的设置。
- en: and it's also notoriously challenging for deep learning。So we know， you know。treebasedase
    boosting methods， stuff like X boost is very dominant and this is also a very
    relevant domain to。I think people in industry and that sort of thing。So we were
    excited about the idea of trying to do better on this。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这在深度学习中也是众所周知的挑战。因此我们知道，基于树的提升方法，如XGBoost非常占主导地位，这也是一个与行业相关的领域。所以我们对在这一点上做得更好感到兴奋。
- en: So we chose a broad selection of data sets varying across a few different dimensions。you
    know as we mentioned， you know on the order of hundreds to tens of millions of
    instances broad range in the number of features in the composition of features
    in terms of being categorical or continuous various types of tasks。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们选择了广泛的数据集，涵盖几个不同的维度。你知道的，正如我们提到的，从数百到数千万个实例，特征数量的广泛范围，特征组成在类别或连续性方面，涉及各种类型的任务。
- en: binary and multiclass classification as well as regression And like I said。the
    baselines were kind of the usual suspects for tabular data， X boost cat boost。byGBM
    to MLPs and Tnet， which is a transformer architecture for tabular data。![](img/1777b223f7bf8be27a8767d209c7f271_22.png)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 二分类和多分类以及回归。正如我所说，基线是针对表格数据的常见方法，XGBoost、CatBoost、GBM 到 MLP 和 Tnet，这是一种针对表格数据的变换器架构。![](img/1777b223f7bf8be27a8767d209c7f271_22.png)
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_23.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_23.png)'
- en: So to get into the results here I'm showing the average rank for the various
    subtasks we did well in terms of rankwise performance against methods like cat
    boostost and X boostt which are designed specifically for Tular data and in fact
    we find that NPT is the top performer on four of the 10 of these data sets on
    image data I mentioned that we used a CNN encoder and with that we were performing
    well on C410。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 所以为了得到这里的结果，我展示了我们在各种子任务中表现良好的平均排名，针对像 CatBoost 和 XGBoost 这样的专为表格数据设计的方法，事实上我们发现
    NPT 在这十个数据集中四个是表现最好的，在我提到的图像数据上，我们使用了 CNN 编码器，因此我们在 C410 上表现良好。
- en: And we also think that， you know， in general， like with， let's say。new work
    on image transformers on small data， this can probably just be done with linear
    patching。And so this kind of the manner in which you're embedding things is probably
    not the key。Neil。if I can jump in with two questions。Can you go back two slides
    first？
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也认为，通常来说，假设说，在小数据上的图像变换器的新工作，这可能仅仅通过线性拼接完成。所以，这种嵌入的方式可能不是关键。尼尔。如果我可以插入两个问题。你能先返回两张幻灯片吗？
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_25.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_25.png)'
- en: One is just a small minor point， back one more please。Thank you。here for the
    features， 50 plus。what does plus mean here？I'll have to double check what the
    exact number is I'm pretty sure it's probably around 50。I would guess like so
    the 50 is really a order of it's not like 150 or 5000。Yes， yeah。 I mean。I I'll
    double check for you， or you can check with the the metadata statistics at the
    end of the paper。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 有一点小问题，请再退回一张。谢谢。在这里，特征是 50 以上。这里的 plus 意味着什么？我得再确认一下确切的数字，我很确定它大约在 50 左右。我猜是这样的，所以这个
    50 其实是个大概的数量，不是说 150 或 5000。是的，没错。我会为你再确认一下，或者你可以在论文末尾的元数据统计中查看。
- en: But no， it wasn't like you know， arbitrarily large， I would say， though。You
    know。we did these ablations on whether or not we actually need attention between
    attributes。We did find that this ended ended up benefiting us， but you could perhaps
    do kind of say。just an MLP embedding in that dimension and go to like a relatively
    small number of hidden dimensions and fit kind of an arbitrary number of features。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但不是的，它并不是说，随便有多大，我会这么说。你知道的。我们进行了一些实验，看看是否真的需要属性之间的注意力。我们发现这确实对我们有帮助，但你也可以说，单纯在那个维度上使用一种
    MLP 嵌入，并选择一个相对较小的隐藏维度，适应任意数量的特征。
- en: So I think that， yeah， if you， if you kind of relax the necessity of attention
    between attributes。you can probably scale out at least that dimension quite a
    lot。Okay， and then my second question。if you could go forward one slide。![](img/1777b223f7bf8be27a8767d209c7f271_27.png)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为，是的，如果你放松属性之间的注意力需求，你可能会在至少那个维度上扩展很多。好的，然后我的第二个问题。如果你能往前推进一张幻灯片。![](img/1777b223f7bf8be27a8767d209c7f271_27.png)
- en: Thank you here， I'm not sure I quite caught。what does four of 10 data sets。2
    of 10 data sets and four of 10 mean。This is of the， of all the tabular data sets
    that we had。So oh， I you think binaryer classification is for I see， okay。Yeah，
    exactly。Awesome。Any other questions？The standard errors here because I mean， there's
    like there's just 10 data sets。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢你，我不太明白。四个数据集中的十个，十个数据集中的两个，四个数据集是什么意思？这是关于我们所拥有的所有表格数据集。所以哦，我想二分类是针对的，我明白了，好的。是的，正是如此。还有其他问题吗？这里的标准误差，因为我想，只有十个数据集。
- en: right。Yeah， correct。1010 total tabular data sets。Yeah， but these are rank good。Yeah，
    these are。these are rankwise performance， correct， Okay， I'm justing。How the。Where
    the uncertainty comes from in this case？Yeah。average averaged over four of 10
    data sets the rank so for each particular data set we have a rank of all the different
    methods then we take the average and the very answer of the rankings within each
    of the types of task within binary classification within multiclass。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对。是的，正确。共有1010个表格数据集。是的，但这些是排名好的。是的，这些是排名表现，正确，好的，我只是想知道。在这种情况下，不确定性来源于哪里？是的。对四个数据集的平均排名，所以对于每个特定数据集，我们有所有不同方法的排名，然后我们取平均值，以及每种任务类型在二元分类和多类中的排名差异。
- en: etc。Good。We also， if you're curious， you know， have the full results in the
    paper。Yeah， thank you。We also have a couple of questions more some questions。Hey
    yeah， thanks。I guess I just found it a little surprising that the worst performer
    was KNN given that it's also nonparmetric。I guess could you comment on that and
    yeah， is it that there's something like intrinsic to the NPT that makes it just
    exceptional far beyond other nonparmetric methods or yeah。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。很好。如果你感兴趣的话，我们在论文中也有完整的结果。是的，谢谢。我们还有几个问题。嘿，是的，谢谢。我想我只是发现KNN是最差表现者有点惊讶，因为它也是非参数的。你能对此做一下评论吗？是的，是不是NPT有某种内在特性使其远超其他非参数方法？
- en: why is it that KN performs the worst here？Well， I suppose ultimately can and
    is is still a relatively naive predictive method in that you know it might just
    be predicting based on kind of cluster means so for example。you know I think this
    is probably universally true for all the data sets but there's probably some amount
    of kind of additional reasoning that needs to occur over the features at least
    to basic level so for example。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么KNN在这里表现最差？嗯，我想最终KNN仍然是一个相对天真的预测方法，因为你知道它可能只是基于某种聚类均值进行预测。例如，我认为这对所有数据集来说可能都是普遍真实的，但在特征上可能需要进行某种额外的推理，至少达到基本水平。
- en: like one of the data sets is this poker hand data set where it's like a mapping
    between all of the different hands you have in poker and what like they're commonly
    known to people like full houses or whatever So this requires some amount of reasoning
    over the features to be able to group things together。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个数据集是扑克手数据集，它是所有不同扑克手与人们常知道的，例如葫芦之间的映射。因此，这需要对特征进行一些推理，以便能够将事物组合在一起。
- en: So just taking like the cluster means of the featurization of those different
    you know hands is likely not going to give you a great predictive function。whereas
    NPTs can kind of do the classic thing where say you have an MLP type of thing
    over the features or like a you know tree type of thing over the features。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，仅仅取这些不同手的特征化聚类均值可能不会给你一个好的预测函数。而NPT可以做到经典的事情，比如说在特征上有一个MLP类型的东西，或者说在特征上有一个树类型的东西。
- en: you can learn some sort of complex embedding， but then you also can do some
    nonparmetric sort of prediction based on say like clusters of embeddings。I said
    yeah that makes sense， I guess if what if you used？
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以学习某种复杂的嵌入，但你也可以基于嵌入的聚类进行某种非参数预测。我是说，是的，这有道理。如果你使用什么呢？
- en: Pre trained embeddings from a stack of encoders as your vector representation
    for the canon。How do you think that would perform compared to the rest of the
    crowd？Yeah， so this is like， I mean。this idea is kind of like deep kernel learning
    or like， yeah。I believe it is deep kernel learning is basically you use an MLP
    independently。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 从一堆编码器中预训练的嵌入作为你的向量表示用于经典。你觉得这与其他方法相比表现如何？是的，这就像，我的意思是。这种想法有点像深核学习，或者说，是的。我相信深核学习基本上是你独立使用一个多层感知机（MLP）。
- en: So you learn an MP on each input data point， and then you apply a G over all
    the representations of those。you get this sort of like complex embedding and then
    the lookups。The key difference between that type of idea and Ns is that we also
    learn the relationships between the data points themselves。because we use this
    parametric attention mechanism to learn the relationships。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你在每个输入数据点上学习一个MLP，然后你对所有这些表示应用一个G。你得到了这种复杂的嵌入，然后进行查找。该想法与Ns之间的关键区别在于，我们还学习数据点之间的关系，因为我们使用这种参数化的注意机制来学习这些关系。
- en: So we're not just learning like an embedding independently。We're basically back
    propagating through the entire process learning the ways in which we would try
    to embed this。but also the the ways that say the lookup would occur and essentially
    the the relationships at that could potentially be kind of higher order as well。cool
    more follow up。😊，Oh yeah go for it cool yeah thanks so I guess then if if the
    advantage of NPT has to do with sort of the relationships between data points。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们并不是仅仅独立地学习一个嵌入。我们基本上是通过整个过程进行反向传播，学习我们试图嵌入的方式，以及查找的方式，本质上是潜在的高阶关系。很酷，更多的后续问题。😊，哦，是的，继续吧，酷，好的，谢谢。我想如果NPT的优势与数据点之间的关系有关。
- en: AndWhat if you you know took the？Took the let's say you know encoder representations
    and then you passed that as input say for the you know 10 nearest neighbors along
    with like some other like。input representation and sort of had this like weighted
    average like attention style where you weighted the vectors of the nearest neighbors
    based on the attention weights between those input data points and then like the
    supplied input data point and then like past that as you know the vector to like
    the final prediction layer。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如果你知道获取了编码器的表示，然后将其作为输入传递给10个最近邻，并且还有一些其他的输入表示，进行加权平均，比如注意力机制的那种方式，依据输入数据点与提供的输入数据点之间的注意力权重对最近邻的向量进行加权，然后将其传递给最终的预测层，你觉得怎么样？
- en: like do you think that captures some amount of the relationship or is that off
    base？
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为这是否捕捉到了一定程度的关系，还是偏离了？
- en: So I think the nice part， like and really what our ideas behind this whole thing
    is just。These sorts of instances where certain fixed kernels would perform particularly
    well in tasks is like kind of an annoyance。And like ultimately like tuning a lot
    of these types of things or're trying to derive the predictive methods that might
    make a lot of sense for a given situation。kind of stinks And ideally， you'd want
    to just back propagate on a data and kind of learn these relationships yourself。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为不错的一点，以及我们对这一切的想法是，某些固定核在特定任务中表现特别好的情况是一种恼人的事情。最终，调优这些类型的东西或试图推导出适合特定情况的预测方法，确实令人沮丧。理想情况下，你会希望能够在数据上进行反向传播，自己学习这些关系。
- en: So I actually would be really interested to see if we can come up with some
    synthetic experiments that have these sort of like very particular K and N。like
    predictive mechanisms and just see if we can learn precisely those and get you
    know。zero error with NPpts。 And， in fact， like we'll get into this a little bit
    with some of the interventional experiments we do。We have like kind of precise
    lookup functions that NPpts end up being able to learn。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我其实非常想看看我们是否能够设计一些合成实验，具有这些非常特定的K和N，像预测机制，并看看我们是否能够精确学习这些，并获得零误差与NPpts。事实上，我们将稍微涉及一些我们所做的干预实验。我们有一些精确的查找函数，NPpts最终能够学习到这些。
- en: So we can learn interesting relational functions。😊，Cool， yeah， thanks a lot，
    appreciate。好。All right。we have one more question from。![](img/1777b223f7bf8be27a8767d209c7f271_29.png)
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以学习有趣的关系函数。😊，酷，没错，非常感谢，太好了。好的。我们还有一个问题来自于。![](img/1777b223f7bf8be27a8767d209c7f271_29.png)
- en: I just wanted to clarify something about basically so at test time you just
    take the exact same data set and you just like add like your test examples right
    and then you like do the same type of like masking and is that how it works？
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是想澄清一下，基本上在测试时你只是取相同的数据集，然后添加你的测试示例，对吗？然后你也做相同类型的掩蔽，是这样吗？
- en: Yeah， correct。Okay got it and I do have one more question that is just because
    like I think I like misunderstood like how like the effects of your NPT objective。do
    you mind going back to that slide？Sure。Yeah， can you repeat one more time like
    what makes this so special？
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，没错。好的，我明白了。我还有一个问题，就是因为我觉得我误解了你们的NPT目标的影响。你介意回到那张幻灯片吗？当然可以。你能再重复一次，是什么让这个如此特别的吗？
- en: Yeah， so the regularizerizer on the right over the features I would think of
    very similarly to self-supervised learning with just a standard transformer like
    you're basically just introducing a lot more supervision and you're even if say
    you're just doing a supervised objective this is kind of like some amount of reconstruction
    over the features you learn a more interesting representation and like what a
    regularizing effect。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以在特征右侧的正则化器，我会将其视为非常类似于自监督学习，使用标准的 transformer，基本上就是引入更多的监督，即使你只是进行一个监督目标，这在某种程度上就像是对你学习到的特征进行重构，你会学习到更有趣的表示，以及正则化效果。
- en: which we think is interesting but perhaps not as interesting as this stochastic
    target masking。this one is unique because。In kind of standard parametric deep
    learning。you're not going to have an instance in your training process where you're
    taking targets as input。And so basically， what happens is。If you have your training
    data set is input。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为这是有趣的，但也许没有这种随机目标掩蔽那么有趣。这种方法是独特的，因为在标准的参数化深度学习中，你的训练过程中不会有将目标作为输入的实例。因此，基本上发生的是，如果你的训练数据集作为输入。
- en: whatever you're gonna have some stochastic feature masking stuff happening on
    the features amongst the training targets。you're randomly going to have some of
    those unmasked and some of them will indeed be masked。You're going to be back
    propagating a loss on the ones that are masked， of course。because you don't want
    your model to have those available input if you're going to actually try to you
    know back propagate a loss on it。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，你会在训练目标的特征上进行一些随机特征掩蔽操作。你会随机选择一些不被掩蔽，而有些则会被掩蔽。当然，对于被掩蔽的部分，你会反向传播损失，因为你不希望你的模型在实际尝试反向传播损失时能获得这些可用的输入。
- en: But you can use the other ones as input， And that means you can learn these
    kind of like interpoative functions。So that was like this whole idea of like being
    able to kind of like learn K and N。But doesn't that allow？The model to cheat again。Yeah。so
    this is like an interesting point and actually like subtle so I think it's it's
    really worthwhile to bring up so first of all。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可以使用其他作为输入，这意味着你可以学习这些像插值函数一样的东西。这就是能够学习 K 和 N 的整个想法。但这难道不允许模型再次作弊吗？是的。这是一个有趣的观点，其实很微妙，所以我认为提出这个问题是非常值得的，首先。
- en: we never actually back propagate a loss on something that was visible to the
    model at input and so if。for example the model did actually end up basically overfitting
    on training labels we would not observe the model's ability to generalize to test
    data we don't observe this so obviously it seems like this kind of blocking of
    back propagation on labels that are visible at input to the NPT is helping。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从未在模型输入可见的内容上反向传播损失，因此如果。例如，模型确实最终在训练标签上过拟合，我们将不会观察到模型在测试数据上的泛化能力，我们并未观察到这一点，因此显然，这种对在
    NPT 输入时可见标签的反向传播阻断是有帮助的。
- en: It could also be possible that。In bird style stochastic masking。you also randomly
    will flip some labels to be in a different category。So this is like kind of just
    like a random fine print that was introduced in the bird masking text。We also
    do that。So it's possible that that somehow contributes to the to that。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 也有可能在鸟式随机掩蔽中，你也会随机将一些标签翻转为不同的类别。这就像是在鸟掩蔽文本中引入的一种随机细则。我们也这样做。因此，这可能在某种程度上有助于这个。
- en: but it's probably pretty likely to just be the fact that we're not back propagating
    a loss on something that's visible。Great， thanks， make sense。😊，I have two more
    questions， if I can jump in。![](img/1777b223f7bf8be27a8767d209c7f271_31.png)
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 但这很可能只是因为我们没有在可见的内容上反向传播损失。太好了，谢谢，明白了。😊，如果可以的话，我还有两个问题，能插嘴吗？![](img/1777b223f7bf8be27a8767d209c7f271_31.png)
- en: Yes， sorry。Can we go to the the metrics， the performance， the results slide。![](img/1777b223f7bf8be27a8767d209c7f271_33.png)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，抱歉。我们可以去查看度量指标、性能、结果的幻灯片吗？![](img/1777b223f7bf8be27a8767d209c7f271_33.png)
- en: Sure。![](img/1777b223f7bf8be27a8767d209c7f271_35.png)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！![](img/1777b223f7bf8be27a8767d209c7f271_35.png)
- en: I feel like I missed something else。I'm sorry about this。So A U are。so looking
    on the binary classification， A U R O C。Can you clarify what these numbers mean。Are
    they the A U ROC。So this is the so on each of the data sets。So say for a particular
    binary classification data set。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我感觉我错过了一些东西。我对此感到抱歉。所以 A U 是。关于二元分类，A U R O C。你能解释一下这些数字的含义吗？它们是 A U ROC 吗？这是关于每个数据集的。那么以一个特定的二元分类数据集为例。
- en: we're going to get a ranking of the methods we're going to repeat this yeah。so
    these numbers here are the the relative ranking across in this particular case。the
    four data sets。Correct， yeah， I see。So this， these values are not the A U R O
    Cs on average。across the data sets。No， yeah。 they're not。 I mean， like everything。Averaging
    our might make sense。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对这些方法进行排名，我们会重复这个过程。所以这些数字在这个特定情况下是四个数据集的相对排名。对，没错，我明白了。所以这些值不是数据集上的 A U
    R O C 平均值。不是，没错，它们不是。我是说，像所有事情一样，平均可能是有意义的。
- en: but averaging things like accuracy and RMC seems like a bad idea right because
    you might have some data sets where everything has high accuracy or where RMC
    needs something drastically different I see so this these numbers here only tell
    us the relative ranking between the different methods not how well they actually
    perform。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 但是平均准确率和 RMC 似乎是个坏主意，对吧，因为你可能有一些数据集的准确率都很高，或者 RMC 需要一些完全不同的东西。我明白了，所以这些数字只告诉我们不同方法之间的相对排名，而不是它们实际表现得多好。
- en: I mean， it tells us how they perform relative to one another， but not how well
    they perform in I see。Okay， but that's not in the appendix。all we have that information。I
    see， Okay， I was。I was sitting here confused going like， why is A U R O C。Why
    is the best one the smallest and accuracy， What is an accuracy of 2。 anyways。
    Okay。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，它告诉我们它们相对彼此的表现，但并不告诉我们它们表现得多好。我明白了。不过，这不在附录中，我们没有这些信息。我明白了，好的，我一直在这里困惑，为什么
    A U R O C 最好的一个是最小的，而准确率，准确率是多少呢？总之，好的。
- en: that makes my more sense。Thank you both。Awesome。😊，Great。so I'll try to speed
    through this just in the interest of time。But the basically thing。the thing that
    you might be thinking after all these results is are we even learning any data
    point interactions on these real data sets。And so basically， we design an experiment
    to figure this out。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我更能理解。谢谢你们俩。太棒了。😊 好的。所以为了节省时间，我会尽量加快速度。但基本上，你可能会在看完所有这些结果后想，难道我们真的在这些真实数据集上学习到任何数据点的交互吗？因此，我们设计了一个实验来弄清楚这一点。
- en: And the idea is that we're going to disallow NPT from using other data points
    when predicting on one of them。😊。![](img/1777b223f7bf8be27a8767d209c7f271_37.png)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法是我们要禁止 NPT 在对某一个数据点进行预测时使用其他数据点。😊。![](img/1777b223f7bf8be27a8767d209c7f271_37.png)
- en: If we do that， and we observe that NPT actually predicts or performs significantly
    worse。it is indeed using these interactions between data points。A subtle challenge
    or kind of like an added added bonus we can get from this is that ideally。we wouldn't
    actually break batch statistics。 So let's say like the mean of each particular
    attribute。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们这样做，并且观察到 NPT 实际上预测或表现显著更差，确实是在利用数据点之间的交互。从中我们可以获得一个微妙的挑战或一种附加的好处，那就是理想情况下，我们实际上不应该破坏批量统计数据。比如说每个特定属性的均值。
- en: if we can find a way to do this experiment such that we don't break these things。we
    can kind of rule out the possibility that we learn something that's a bit similar
    to bash norm。And so the way that we do this is we basically look at the predictions
    for each one of the data points in sequence。So let's say in this case we're looking
    at the prediction of the model for this particular green row and you know it's
    going be predicting in this last column that has this question mark which is mess
    what we're going do is we're going permute each of the attributes independently
    amongst all other data points except for that one so the information for that
    row if it was kind of just predicting like a classic parametric deep model is
    still intact but the information from all of the other rows is gone so that's
    why we call this sort of the corruption experiment。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能找到一种方式来进行这个实验而不破坏这些东西，我们可以排除学习到一些与 bash norm 有点相似的东西的可能性。因此，我们的做法是基本上依次查看每个数据点的预测。所以假设在这种情况下，我们正在查看模型对这一特定绿色行的预测，而它会在这个最后一列有一个问号进行预测，我们将独立地对除了这一行以外的所有其他数据点的每个属性进行排列，所以这一行的信息如果它只是像经典参数深度模型那样进行预测依然保持完整，但其他行的信息则消失了，所以这就是我们称之为“干扰实验”的原因。
- en: And so we find in general when we perform this experiment performance kind of
    falls off a cliff for the vast majority of these methods and I'll note that you
    the performances between the methods on a lot of these were fairly close and so
    this is actually indeed pretty significant so for example on protein we went from
    being the top performer amongst all the methods to the worst performer worse than
    even like KNN or something like that I'll also note that there's kind of this
    interesting behavior where on these data sets like For and kick and breast cancer
    we actually observe that there's basically no drop in performance and we basically
    see this as kind of an interesting feature and not necessarily a bug of the model
    which is that if we're back propagating on a given data the model can sort of
    just find that it's actually not that worthwhile to attempt to predict using some
    kind of you know relational predictive mechanism amongst data points and can instead
    just learn to predict parametrically and basically ignore other data points when
    it's predict on any given one of them。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们发现，一般来说，当我们进行这个实验时，表现会在大多数方法中急剧下降，我要指出的是，这些方法之间的表现差距相对较小，因此这实际上是相当重要的。例如，在蛋白质方面，我们从所有方法中表现最好的转变为最差，甚至比
    KNN 还差。我还要指出，在这些数据集上，如For、Kick和乳腺癌，我们实际上观察到性能几乎没有下降，我们将其视为一种有趣的特征，而不一定是模型的缺陷，模型可以发现尝试使用某种关系预测机制来预测数据点并不太值得，反而可以选择学习参数化预测，基本上在预测任一数据点时忽略其他数据点。
- en: And so this probably leads to some kind of like interesting ideas where perhaps
    you could do like postdoc pruning or something like that。taking away the tension
    between data points and doing fine tuning， let's say。All right。so now I'll hand
    over to Y to talk a bit about learning some interesting relationships。Yeah will
    you though I see that we're at the end of what the time is。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这可能引发一些有趣的想法，也许你可以进行后期剪枝之类的，消除数据点之间的紧张关系，进行微调。好吧，现在我把时间交给Y，让他谈谈学习一些有趣的关系。是的，我看到我们接近时间的尽头。
- en: but like I know there's a buffer planned and or something can you I can go through
    this experiment we can have a more discussion what's what do you guys prefer？
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我知道有个缓冲计划，或者说可以通过这个实验，我们可以更深入地讨论，大家更倾向于什么？
- en: Yeah， I think。Normally， what we do is we would sort of stop the recording at
    this point and have an off the record discussion。And。I guess the question to ask
    is does anyone have any questions at this point？
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我认为，通常我们会在这个时候停止录音，进行一个非正式的讨论。然后，我想问的是，大家目前有任何问题吗？
- en: But I think we've basically。Laantching questions as they come， so I personally
    feel fine just yeah。considering this a sort of question throughout。Yeah。I guess
    that sounds that sounds good ya you can go forward with it with like with your
    tongue as planned and yeah we can data we can see about the time thing。I think
    this will only be like another。4our or five minutes I shouldn't go for it， yes
    for sure。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为我们基本上是在根据问题的出现来进行提问，所以我个人觉得这样也不错，是的，考虑到这一点就算是个问题。是的，我想这听起来不错，你可以按计划继续进行，嗯，我们可以看看时间的事情。我认为这只会再花大约四五分钟，我应该可以继续，没问题。
- en: All right。So Neil has now told us how well entities perform in real data and
    that they do make use of information from other samples of the input。But we're
    now going to take this a bit further and come up with some toy experiments that
    test a bit。the extent to which entities can learn to look up information from
    other role。like the extent to which they can learn this nonprometric prediction
    mechanism。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。所以尼尔现在告诉我们实体在真实数据中的表现，以及它们确实利用了输入其他样本的信息。但我们现在要更进一步，提出一些玩具实验来测试一下实体从其他角色中查找信息的能力，也就是它们学习这种非参数预测机制的程度。
- en: And so specifically what we'll do is we'll create the following semi syntheticthe
    data set。So I want you to focus on A now。Yeah so we take one of the tabular data
    sets that we've used previously specifically the protein data set。but it doesn't
    really matter what matters is there it's a regression data set and so now what
    we do is we。The top half here is the original data set， but the bottom half is
    a copy of the original data set where we have unveiled the true target value so
    now NPTs could learn to use attention between data points to achieve arbitrarily
    good performance。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 所以具体来说，我们将创建以下半合成数据集。因此，我希望你现在关注A。是的，我们取用之前使用的一个表格数据集，特别是蛋白质数据集。但其实并不重要，重要的是这是一个回归数据集，所以现在我们做的是。这里的上半部分是原始数据集，但下半部分是原始数据集的副本，我们揭示了真实的目标值，这样NPT可以学习在数据点之间使用注意力，以实现任意好的性能。
- en: they could learn to look up the target values in these matching duplicate row
    and then paste them back into that must out target value and then a test time
    of course we put in a novel test data input where this mechanism is also possible
    just to make sure that it hasn't learned to memorize anything but has actually
    learned this correct relational mechanism。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以学习在这些匹配的重复行中查找目标值，然后将它们粘贴回必须输出的目标值中，然后在测试时，我们当然输入一个新的测试数据，这个机制也是可行的，只是为了确保它没有学习去记忆任何东西，而是实际学会了这个正确的关系机制。
- en: And so what we see is that indeed MPs do successfully learn to perform this
    lookup so what I'm visualizing here is attention maps and they very clearly show
    that let's say when predicting for this green row here。this first green row what
    emmputees look at is exactly only that other green row here and so。😊。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到确实MP成功学习到了执行这个查找的能力，所以我在这里可视化的是注意力图，它们非常清楚地显示出，当预测这个绿色行时，这个第一个绿色行，MP们看到的正是这个其他绿色行。😊。
- en: This is really nice we can further look at the kind of the。😊。The pieson correlation
    between what MPs should predict and what they actually do predict and so this
    is 99。9% this is much better than anything you could achieve with parametric prediction
    and so it seems thatmp here can actually discover this mechanism and discover
    here I feel like it's the right word because MPmps could have as we've seen it's
    just also continued to predict in parametric fashion right from each row independently
    this is really kind of showing to us that there is this bias in the model to learn
    to predict from other rows and of course that is also very attractive in this
    setting because it allows you to achieve arbitrary load loss in this setting or
    as low as you can optimize for it。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的很好，我们可以进一步观察到。😊。MPs应该预测的和它们实际预测的之间的皮尔逊相关性，这个相关性是99.9%。这比你通过参数预测所能达到的任何结果都要好，所以似乎MP在这里确实可以发现这个机制，我觉得“发现”这个词是恰当的，因为MP们可以如我们所见，继续以参数的方式独立地从每一行进行预测，这实际上向我们展示了模型有偏向于学习从其他行预测的倾向，当然在这种情况下也非常有吸引力，因为它允许你在这种情况下实现任意低的损失，或者尽可能地优化它。
- en: 😊，And so。We kind of take that to mean that our gradient based discovery。non
    parametrics philosophy seems to make some sense。And so we can take this a bit
    further by performing somewhat of an interventional experiment that investigates
    the extent to which NPTs have actually learned a robust causal mechanism that's
    underlying this semi synthetic data set。😊，Just depending you know this。This extra
    extra column of test data that' already kind of cool。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以。我们认为这意味着我们的基于梯度的发现。非参数哲学似乎是有意义的。因此，我们可以通过进行某种干预实验进一步探索，调查NPT们是否实际上学习到这种半合成数据集背后所隐含的稳健因果机制的程度。😊，这仅仅是依赖于这个额外的测试数据列，已经很酷了。
- en: but I think we can take a bit further and actually study if this generalizes
    beyond the data that we see in the training set or beyond data coming from this
    specific distribution and so what we now do is we intervene on individual duplicate
    data points at test time by varying their target value so now we only care about
    the prediction in a specific row we do this across all rows but at each time we
    just care about a single row what we do is we change the target value here that
    what we're hoping to see is and then NPT just adjusts the prediction as well right
    there's a very simple intervention experiment for us to test if NPTs have actually
    learned this mechanism and to some extent it also test robustness because now
    we're associating target values with features that are not part of the train distribution
    here。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为我们可以更进一步，实际上研究一下它是否可以推广到训练集中看到的数据之外，或者来自这个特定分布之外的数据。因此，我们现在在测试时对单个重复数据点进行干预，通过改变它们的目标值。因此，我们现在只关注特定行的预测，我们在所有行中都这样做，但每次我们只关心一行。我们在这里改变目标值，我们希望看到的是，然后NPT也会调整预测。这是一个非常简单的干预实验，测试NPT是否真的学会了这一机制，在某种程度上也测试了鲁棒性，因为现在我们将目标值与不属于训练分布的特征相关联。
- en: 😊，And so what we see is that as we as we adjust these values here。this is the
    kind of the duplicate value and then we here see the target value as we adjust
    them。we can see the correlation stay is really really good， it's not quite 99。9%
    like on average。we're now at 99。6， but it' still very， very good。😊。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，因此我们看到，当我们调整这些值时，这是重复值，然后我们在这里看到目标值。随着调整，我们可以看到相关性保持得非常好，虽然平均值并不是99.9%。现在是99.6%，但仍然非常好。😊。
- en: At this point you might be slightly annoyed with me because you know standard
    nonprometric models can also solve this task right this is a task that I could
    solve by nearest enables Sure maybe you know I would have to change the input
    format bit because this is kind of like in a batch setting and I can just use
    masks but most generally nearest neighbor can also you know it also looks up different
    input points based on their features nearest neighbor doesn't learn to do this。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能对我稍感烦恼，因为你知道标准的非参数模型也可以解决这个任务，对吧？这实际上是一个我可以通过最近邻来解决的任务。没错，也许你知道，我需要稍微改变输入格式，因为这有点像批处理设置，我可以使用掩码，但一般来说，最近邻也会根据特征查找不同的输入点，最近邻并不是通过学习来实现这一点的。
- en: I still think it's cool that we need to learn this because it does require you
    know a decent amount of you know computational sequences that we have to learn
    like match on the features look up target you copy it back and so on but。It is
    in fact very easy for us to complicate this task to a degree such that essentially
    no other model that we know of can can solve this very easily and so like a really
    simple thing to do is just to add a plus one to all of the duplicate values。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我仍然觉得我们需要学习这一点很酷，因为这确实需要我们学习相当多的计算序列，比如根据特征匹配，查找目标并复制等等。但实际上，我们很容易将这个任务复杂化到一个程度，以至于基本上没有其他模型可以很容易地解决这个问题。因此，一个非常简单的方法就是给所有重复值加一。
- en: 😊，So now nearest neighbor would look up the right collar the right row， of course。but
    it would always predict the wrong target with a plus one on it and in in fact，
    many of the。The models that we're aware of， they're not modeling。😡。The joint distribution
    over features and targets what they're modeling is the traditional distribution
    of the targets given the input features and so they also cannot do this and so
    for us it's really not a problem at all MPs will just learn to subtract another
    one and no problems and sure this is also still a very synthetic setting but I
    do think I mean I challenge you to come up with something that MPmpes can't solve
    but the other models can solve I think in general this masking mechanism and the
    nonmetric of the approach really nice in general and leads to lots of nice behavior
    in a variety of settings and so with that I think we can go to the conclusions
    which Neil is going to give you。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，所以现在最近邻会查找正确的颜色和正确的行，当然。但它总是会预测错误的目标，结果加一，实际上，许多我们知道的模型并没有建模。😡。特征和目标的联合分布，他们建模的是给定输入特征的传统目标分布，因此他们也无法做到这一点。对我们来说，这根本不是问题，MP只会学习再减去一个，也没问题。当然，这仍然是一个非常合成的环境，但我确实认为，我挑战你想出一些MP无法解决而其他模型能解决的问题。总的来说，这种掩码机制和方法的非度量性在一般情况下非常不错，并导致在各种设置中表现出许多良好行为。因此，我认为我们可以进入结论，尼尔将为您提供。
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_39.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_39.png)'
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_40.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_40.png)'
- en: Yeah， I think， I mean， we're can cut out the。Main part here。I'll just fast forward。look
    at them Yeah yeah， I was gonna say I think you you all get the gist NPpts take
    the entire data set input and they use self- attentiontention to model complex
    relationships between data points。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我认为，我们可以剪掉这里的主要部分。我只是快进。看看他们。是的，是的，我想说，我认为你们都明白了，NPpts取整个数据集作为输入，并使用自注意力机制建模数据点之间的复杂关系。
- en: know， they do well in experiments on type of their data as well as image data
    we present some of these interventional experiments to show that they can solve
    complex reasoning tasks there's some more experiments in the paper。I'd say that
    you know， the interesting type of future work is scaling type things。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 知道，他们在其数据类型以及我们呈现的图像数据的实验中表现良好，我们展示了一些干预实验，表明他们能够解决复杂的推理任务，论文中还有更多实验。我认为，有趣的未来工作类型是扩展这类事物。
- en: So we can you know， not having this mini batchching approximation。and then also
    just trying to expand this to some more interesting application domain So we talked
    a little bit about metal learning。but it could also be things like you know few
    shot generalization in general domain adaptation。semi surprise learning， etc。😊，So
    I think if there's some more questions。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以，不使用这个小批量近似。而且还试图将其扩展到一些更有趣的应用领域。我们稍微讨论了一下元学习，但也可以是一些像少量样本的泛化、一般领域的适应、半监督学习等。😊，所以我认为如果还有更多问题。
- en: maybe we can do some more discussion。Yeah， things sounds good。Great thanks for
    the talk。I think everyone had a fun time see。I will just ask some general questions
    and then we can have like a discussion session with everyone after that So I think
    one thing that I noticed is like like this like you said like this is similar
    to like canons and I thought like this seems similar to like graph neural networks
    where I can think like each data point is like a node and then you can think of
    everything as a fully connected graph and you're learning some sort of attention
    rate in this graph。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们可以进行更多讨论。是的，听起来不错。非常感谢演讲。我认为大家都度过了愉快的时光。我会问一些一般性的问题，然后我们可以在那之后与大家进行讨论。我注意到的一件事是，你说的这和*卡农*相似，我觉得这似乎与图神经网络相似，我可以把每个数据点看作一个节点，然后你可以把所有东西视为一个完全连接的图，并且你在这个图中学习某种注意力权重。
- en: So this is like a note prediction task you are kind of doing on this sort of
    like graph structure。so any comments on that like is it similar to like graph
    neural networks or is it like other differences？
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就像是一个音符预测任务，你在这种图结构上进行的。对此有什么评论吗？这是否类似于图神经网络，还是有其他不同之处？
- en: Yeah this is a very good observation yeah I think there are a lot of similarities
    to work on graphraph neural networks if we want to talk about differences the
    differences might be that we're kind of assuming a fully connected graph right
    and so you could maybe also phrase that as we're discovering the relational structure
    or as graphraph neural networks usually assume that it's given but that's also
    not always true and so there are a lot of similarities I don't know Neil if there
    is something specific you would like to mention go ahead but it's a very good
    observation and then we also do feel that that's the case and we've added an extra
    sectional related work to graphra neural networks in the updated version of the
    that will be online soon。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这是一个非常好的观察，我认为在图神经网络方面有很多相似之处。如果我们要谈论差异，可能的差异在于我们假设一个完全连接的图，因此你也可以将其表述为我们正在发现关系结构，而图神经网络通常假设它是给定的，但这并不总是正确的，因此有很多相似之处。我不知道尼尔你是否想提到一些具体的内容，继续说吧，这真是个很好的观察，我们也确实感觉到这种情况，我们在更新版本中添加了一个与图神经网络相关的额外部分，这个版本很快就会在线上。
- en: Got it。Yeah， I agree with everything you've said。I think that the closest work
    from the GNN literature that we were looking at a little bit was this neural relational
    inference paper。which uses mess passage message passing neural networks to try
    to kind of like learn edges that may or may not exist and help for like extrapolating
    I think positions of like particles in like a multipart system or something。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 明白了，是的，我同意你所说的一切。我认为我们所关注的与图神经网络文献中最接近的工作是这篇神经关系推理论文，它使用消息传递神经网络试图学习可能存在或不存在的边，并帮助推断多粒子系统中粒子的位置或其他什么。
- en: which is like kind of a similar idea to us like you know。if you don't have these
    edges as given the attention mechanism kind of approximate and interesting relationship
    amongst some interacting things。I see got it yeah that's pretty cool Another thing
    is like so you mostly look on like tableular data。but can you also like have other
    modalities like if you want to do language or something。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们来说有点像类似的想法，你知道的。如果你没有这些边作为给定，注意机制会大致近似一些交互事物之间的有趣关系。我明白了，这真不错。另一件事是，你主要关注的是表格数据，但你是否也可以使用其他模态，比如如果你想处理语言或其他东西。
- en: can you still use non para transformomers？Yeah， so I think part of our motivation
    for doing tabular was because we felt like tabular data is in a sense。a generalization
    of let's say the language data， for example， I mean。I guess there's these other
    notions like that people have brought up like padding but ultimately you can think
    of it as like a bunch of categorical attributes So it is it is definitely generalizable
    to things like sentences and we do you know images so yeah I think actually like
    like I always go back and forth on whether or not I think smaller or larger data
    is more interesting for us So I think small data is really interesting because
    we can't just fit the entire data set into it and all of this just works out of
    the books without any extra thought but large data is actually also really interesting
    because。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 你还能使用非参数变换器吗？是的，我认为我们做表格数据的部分动机是因为我们觉得表格数据在某种意义上是语言数据的一种推广。例如，我想。这些其他概念像填充，最终你可以把它看作一堆类别属性，所以它确实可以推广到像句子这样的东西上，我们也处理图像。所以我认为其实我总是在思考小数据或大数据对我们来说哪个更有趣。所以我认为小数据真的很有趣，因为我们不能将整个数据集放入其中，而这一切都可以毫不费力地运作，但大数据实际上也很有趣，因为。
- en: Sure you might have to introduce some app mechanism or some lookup mechanism
    because you can't always have the entire data set in but at the same time you
    are very explicitly kind of trading off the compute that you use to look up with
    the compute that you need to store like how much how many parameters in GPT are
    used for storing data right there's lots of memorization happening in these models
    and we know that and so maybe we can use the parameters more efficiently to learn
    lookup type behavior right that is more close to this you know neurocanin or whatever
    so I think these are very exciting。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可能需要引入一些应用机制或查找机制，因为你不能总是将整个数据集加载进来，但与此同时，你非常明确地在权衡用于查找的计算与用于存储的计算，比如GPT中有多少参数用于存储数据，对吧，这些模型中发生了大量的记忆化，我们对此是知道的，因此也许我们可以更有效地利用参数来学习查找类型的行为，这更接近于你知道的神经动力学或其他什么，所以我认为这些都是非常令人兴奋的。
- en: Questions yeah yeah I'll also be looking forward to the future works because
    this seems like a very good way to like do one shot learning kind of situations
    so yeah really very interesting to see that。Okay， so I will stop the recording
    and we can have like any other questions。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是的，我也期待未来的作品，因为这似乎是处理一-shot学习情境的很好方法，所以真的很有趣看到这一点。好的，我将停止录音，我们可以进行其他问题。
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_42.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1777b223f7bf8be27a8767d209c7f271_42.png)'
