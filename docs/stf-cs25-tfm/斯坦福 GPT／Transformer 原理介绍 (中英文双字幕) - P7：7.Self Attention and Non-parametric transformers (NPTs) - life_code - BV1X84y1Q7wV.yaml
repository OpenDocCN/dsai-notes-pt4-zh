- en: ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P7Ôºö7.Self Attention and Non-parametric transformers
    (NPTs) - life_code - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Thanks so much great to be here and happy Halloween related Halloween everyone
    so I think the talk is going to be split into two sections so I'll start by spending
    like 10 minutes 15 minutes chatting about transformers in general but I'm assuming
    most of you are familiar with them and we can move on to MPTs which Ya and Neil
    will be presenting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòä„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_2.png)
  prefs: []
  type: TYPE_NORMAL
- en: So let's seeÔºå I'm going like„ÄÇTry to fly through the transformer overview and
    maybe spend„ÄÇA little bit extra time on like the history of transformers and maybe
    just tell the story a little bit„ÄÇI think that might be more interesting„ÄÇÂóØ„ÄÇSo just
    in terms of the transformer architecture„ÄÇthe two kind of things that it introduced
    for the first time were multihead attention and selfat and then it combined those
    with fast utter aggressive decoding so before the Transer pretty much everyone
    was using LSTMs and LSTMs with attention I'll try to get into the difference of
    selfat multihead attention„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So originally you would have two sequences and you would have a attention module
    which would attend from the source to the target and so each token or each word
    in the source sequence would get associated with you know a soft approximation
    of one element in the target sequence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so you'd end up with something like like thisÔºå but with self attention„ÄÇwe
    did away with the two separate sequencesÔºå we make them both the same„ÄÇand so you're
    relating each element within the sequence to another element in the sequence„ÄÇAnd
    so the„ÄÇThe idea here is that you're learning a relationship of the words within
    a sentence to the other words„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so you can imagine something like an adjective which is being applied to a noun
    and so you want to relate that adjective like the blue ball„ÄÇyou want to relate
    blue as referring to ball through learning patterns within the sequence interest
    sequence patterns„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇSo sorry I gave this talk in KenyaÔºå so I' am using Kewa Heley hereÔºå but with
    multihead attention„ÄÇthe idea is you have like each word represented by an embedding
    which is in the depth dimension here and then you have your sentence of words„ÄÇyou
    split that up into a bunch of different groups„ÄÇso here I've droppedpped it depthwise
    into four groups„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You apply attention to each one of these groups independently and then when
    you get the result back you can catnate them together and you're back to your
    model dimension representation„ÄÇWhat this lets you do is if each attention like
    each attention head can now focus on learning one pattern„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so maybe attention head one is learning the relationship of adjectives to nouns
    and the second attention head can learn something different so this lets us learn
    like a hierarchy or a list of different relationships„ÄÇOkayÔºå so that was self attention„ÄÇThe
    other piece is fast auto oppressive decoding„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And do I really want to go into thisÔºüOkayÔºå I willÔºå so the important thing about
    this is it„ÄÇIf you're doing normal autoaggressive decodingÔºå what you do is you
    generate your first token and now conditioned on that first token you generate
    the second and condition on the first two you generate the third and so on and
    so forth„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but that's super slow right like it's a loop applying this thing again and again
    and so what we can do instead is we make an assumption in the code that our model
    always generates the right thing and we generate and then we generate a prediction„ÄÇOnly
    one token ahead and so the way that this looks is„ÄÇYou okay so„ÄÇhy„ÄÇWhy hat hereÔºüSorry
    once again„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Input to outputÔºå so you have like your outputsÔºå which are yÔºå you have your targetsÔºå
    which are Y hat„ÄÇAnd what you do is you feed in those gold targets so that you
    don't need to actually do this loop„ÄÇso instead of assuming instead of having to
    generate the first token feed it back into your architecture„ÄÇgenerate a second
    tokenÔºå you feed in the entire target sequence and you just pretend that you generate
    all the right tokens up to position k and then you predict the K plus first and
    you compute your loss on that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So in reality your model might have generated you know at the beginning of training
    junk„ÄÇbut you're getting a loss as if your model had seen all the correct tokens
    and is now just predicting the next one this is a little bit subtle but it's hugely
    impactful for training speed because all of this can be done un in parallel and
    so it's actually what makes transformers so scalable„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so in order to do this successfullyÔºå if you were just feeding in all of
    the„ÄÇAll of the correct tokens naivelyÔºå what would happen is your model would just
    be able to look forward in time and cheat„ÄÇSo you've put in all of your true targetsÔºå
    the things that you're trying to get your model to predict and so if that's where
    you're computing your loss on„ÄÇit could just look forward in time and sayÔºå okay
    I'm just going to grab that and it would get zero error trivially right because
    you've given it all the right answers so what we have to do inside the architecture
    is we need to actually prevent the attention mechanism from being able to look
    at tokens that it shouldn't have been able to see already so the way that this
    looks is you create a mask on your attention„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇüòäÔºåAnd so sorry this is the example of like doing a trivial attention if you
    don't mask your attention properly what it's going to do is it's just going to
    look into the future just grab the token that you're telling it to predict and
    copy it over and so it learn something trivial something that doesn't actually
    generalize and so what we do is we„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Actually prevent it from attending to those tokensÔºå we prevent it from attending
    into the future„ÄÇFor each position in the source sequenceÔºå we block out everything
    that it shouldn't be able to see„ÄÇeverything into the futureÔºå and then as we move
    down we gradually unblock so it can start to see into the past„ÄÇÂóØ„ÄÇSo those are
    kind of like two the three major components of transformersÔºå the self attention„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the multihead attentionÔºå and then deploying this gold targets decoding fast
    utter restive decoding„ÄÇÂóØ„ÄÇIn terms of the storyÔºå which might be a little bit more
    interesting„ÄÇÂóØ„ÄÇSo transformers„ÄÇI was an intern with Lukash Kaiser at Google back
    in 2017„ÄÇand I was sitting next to Nome and Sheish was like a couple seats down
    from us„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what's really incredible is that essentially this entire project came together
    in like three months„ÄÇAnd it was done so I showed up at Google Noam had been working
    on„ÄÇAutoag models„ÄÇsame thing with like Ashishish and Yaakov and Nikki and they'd
    just been kind of like exploring the space figuring it out and Luksh and I at
    the same time we had been working on this framework called Tensor to Tensor„ÄÇÂóØ„ÄÇWhich
    was like explicitly made for multimodal learning autoreive learning and lukash
    is kind of like a master of„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of everything that's happening in the field and adopting it„ÄÇand
    so within tensor to tensorÔºå there were like these„ÄÇThere were like these kind of
    emerging little things that maybe one paper had been written about and people
    were interested in like layerorm„ÄÇbut it hadn't actually taken off yet the warmup
    in the learning rate schedule all of these little pieces were just default like
    on by default„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and so whennome and aishish and the Nikki and YaakÔºåCame over and adopted tensor
    to tensor„ÄÇall of these things were just on by default„ÄÇAnd so a lot of people„ÄÇwhen
    they look at the transformer paperÔºå it just seems like there's so many like arbitrary
    little things thrown in and when now like in present day these have become standard
    for like a lot of different training algorithms like the learning rate warm up„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The way that we did initializationÔºå all of these pieces have just become the
    norm„ÄÇbut back then they had like have just been introduced„ÄÇAnd so„ÄÇWe we spent
    a lot of time running ablations trying to figure out like which were the necessary
    pieces and what made it work and if any of you have actually tried training transformers
    and tried like pulling out the learning rate warmup or changing any of these little
    pieces„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you'll see that it really does break down optimization like it actually really
    does hurt performance„ÄÇFor instanceÔºå like removing the layer armsÔºå that type of
    thing„ÄÇÂóØ„ÄÇSo I always thought it was kind of funny how„ÄÇAll of these random editions
    that Luash had just like thrown in because he was playing around with them turned
    out to be crucial and they were just on by default„ÄÇÂóØ„ÄÇSo anywayÔºå it was like three
    monthsÔºå I remember„ÄÇIt all really started coming together towards the end„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like just before the Nup deadline„ÄÇAnd I can still remember sitting in the micro
    kitchen and a sheish telling me like as just like I was a little intern telling
    me like this is going to be such a big deal and I was like yeah sure okay like
    I have no idea what's happening I just showed up„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And he was likeÔºå no dudeÔºå like this this actually matters like you know„ÄÇwe bumped
    up blue three points and I was like sickÔºå great anyway„ÄÇÂóØ„ÄÇüòä„ÄÇAnd then I can remember
    on the night of the deadline for nerves„ÄÇIt was like 2 am a shish„ÄÇSheish was the
    only one left at the office and we were still like moving around figures and like
    adjusting things and then I went to bed but she stayed up and I slept in like
    this tiny little phone booth„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then for the other paper that I was submittingÔºå I forgot to press submitÔºå
    but luckily„ÄÇLike some lady opened the door to the phone booth and hit me in the
    head while I was sleeping in the morning and just before the deadline I got the
    paper in and so I owe it to that lady for submitting to N that year but yeah anyway
    the I think the crazy thing about transformers was that it all came together in
    like three months like most of the ideas happened in that span and it was just
    like this sprint towards the N deadline„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: UmÔºå and I think a lot of the other members on the team„ÄÇYaka Lukaash she they
    knew how important it wasÔºå but for me I was like„ÄÇI don't know„ÄÇI really did not
    appreciate the impactÔºå but in retrospect it's been amazing how the community has
    kind of like come together and adopted it„ÄÇüòäÔºåAnd I think most of that can be ad
    to the ease of optimization„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it seems like very robust to hyperparameter choices so you don't need to like
    tune the hell out of it„ÄÇspend a lot of time tweaking little things„ÄÇAnd the other
    side is that it's like super tailored to the accelerators that we run on„ÄÇSo it's
    like very paralyzableÔºå hyper efficientÔºå and so it lends itself to that kind of
    scaling law effort that's really taken off in popularity„ÄÇOkayÔºå unless there are
    any questions that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_4.png)'
  prefs: []
  type: TYPE_IMG
- en: We're both excitedÔºå so we just unmuteed at the same time„ÄÇYeahÔºå so cold„ÄÇYeah„ÄÇsos
    that's my section if there's any questions happy to answer them otherwise„ÄÇLet's
    get into NPs NPptTs are like I think„ÄÇThere's such a nice next level abstraction
    of the architecture„ÄÇso you've probably seen the trend of„ÄÇTransformers getting
    applied to new domains„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: first into vision and video and audio„ÄÇBut this is kind of like cutting back
    to an even more abstract level„ÄÇlike I think tabular dataÔºå yeahÔºå I don't knowÔºå
    I'll Yna and Ne take over from here„ÄÇbut I think MPT is a pretty sickÔºå pretty sick
    project„ÄÇThanks A for the introduction thanks all for the invitation we're very
    happy to be here and Neil and I are now going to tell you about our selfattention
    between data points paper where we introduce introduce the nonprometric transformer
    architecture we'll start with a little bit of motivation we want to explaining
    the architecture detail Im show you the experiments this is more or less a step
    through of the paper but maybe you know with a little bit extra insight here and
    there„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: All rightÔºå as promisedÔºå the motivation and a brief summary„ÄÇSo we'll start by
    thinking about something that we don't often think about„ÄÇthat is that from seal
    to transformersÔºå most of supervised deep learning relies on parametric prediction„ÄÇSo
    what that means is that we have some self training data and we want to learn to
    predict the outcomes y from the inputs X and for this we set up some model with
    tunable parameters theta„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then we optimize these parameters to maximize predictive likelihoods on a training
    set or you know equivalently we minimize some loss„ÄÇAnd then after trainingÔºå we
    have this optimized set of parameters theta„ÄÇand then at test time we just put
    these into the model and use these parameters to predict on novel test data„ÄÇAnd
    so crucially hereÔºå our prediction at test time only depends on these parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: right it's parametric„ÄÇAlso that means that given these parameters„ÄÇthe prediction
    is entirely independent of the training data and so why would we want to do parametric
    predictionÔºü
  prefs: []
  type: TYPE_NORMAL
- en: WellÔºå it's really convenient because all that we've learned from the training
    data can be summarized in the parameters and so at prediction time we only need
    these final parameters and we do not need to store the training data„ÄÇwhich might
    be reallyÔºå really large„ÄÇOn the other hand„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we usually have models that already predict for a bunch of data in parallel
    right think of mini batching and modern architectures and actually things like
    batch on already make these data interact„ÄÇAnd so our thinking here was that if
    we've got all of this data in parallel anyways„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there's no reason not to make use of it and so more a bit grounder we kind of
    challenge carmetric prediction as the dominant paradigm in deep learning and so
    we want to give models the additional flexibility of using the training data directly
    when making predictions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so a bit more concretely„ÄÇüòäÔºåWe introduced the nonparmetric transformer architecture„ÄÇand
    this is going to be a general deep learning architecture„ÄÇmeaning we can apply
    to a variety of scenarios„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_6.png)
  prefs: []
  type: TYPE_NORMAL
- en: NPTs will take the entire data set as input whenever possible„ÄÇAnd NPTs then
    crucially learn to predict from interactions between data points„ÄÇAnd to achieve
    this„ÄÇwe use multi head self attention„ÄÇThat as Age has introduced us to„ÄÇhas just
    really established itself as a general purpose layer for reasoning„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We also take another thing from the NLP community and we use a stochastic masking
    mechanism and we use that to tell entitiesmpes where to predict and also to regularise
    the learning task of it„ÄÇAnd last yearÔºå of courseÔºå we hope to convince that the
    ends up working really„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: really well and that this kind of simple idea of learning to predict from the
    other data points of the input„ÄÇfrom the training points of the input and up working
    as as well„ÄÇSo„ÄÇand so very briefly summarizing what we've heard already„ÄÇAÔºå we input
    into NPptTCide dataset„ÄÇAnd then BÔºå let's say for the purpose of this slide here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we only care about predicting the orange question mark in that green row„ÄÇAnd
    then we can compare entitiespts to parametric prediction right so a classical
    deep learning model would predict this target value only from the features of
    that single grid input to do that it would use the parameters theta„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: those would depend on whatever training data we've seen and so on„ÄÇbut at test
    time we only look at that single row for which we care about the prediction„ÄÇüòä„ÄÇIn
    contrastÔºå NPpts predict an explicit dependence on all samples in the input they
    can look beyond that single green data of interest and look at all other samples
    that are there and consider their values for prediction so this presents an entirely„ÄÇüòäÔºåDifferent
    way of thinking about how we learn predictive mechanisms somebody on Twitter called
    this Canan 2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 0Ôºå which we would have not written in the paperÔºå but maybe is kind of a nice
    way of thinking about how NPTs can learn to predict„ÄÇüòä„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_8.png)
  prefs: []
  type: TYPE_NORMAL
- en: So of courseÔºå non parametric models are a thing alreadyÔºå we didn't invent them
    at all and„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_10.png)
  prefs: []
  type: TYPE_NORMAL
- en: I defined them here as prediction in explicit dependence on the training data„ÄÇwhich
    is certainly what MPptTs do„ÄÇClassical examples like Gaussian processesÔºå can neighbor„ÄÇkernelnal
    methodsÔºå those might be familiar to you„ÄÇüòä„ÄÇAnd there exists also efforts to combine
    the benefits of nonprometrics and representation learning in a similar fashion
    to how we did it in entities„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåHoweverÔºå these approaches are usually limited in some sense in comparison
    opportunities right they're often kind of motivated from the statistics community
    a bit more they often require more fiically approximate inference schemes are
    limited in the interactions they can learn or things like that and so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåWe really think NPTs present„ÄÇMaybe the most versatile and most widely applicable
    of these nonprometric prediction approaches but that's something we explicitly
    wanted to have we wanted to have something that's really easy to use plug play
    works in a ton of scenarios and works really well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_12.png)'
  prefs: []
  type: TYPE_IMG
- en: And so with thatÔºå I'd hand over to NeilÔºå whos going to tell you about the nonparmetric
    transformer architecture in all of its details„ÄÇyou also have one question hi Jen
    could you please go to the previous slideÔºü
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_14.png)'
  prefs: []
  type: TYPE_IMG
- en: The very previous slide„ÄÇYeahÔºå yes„ÄÇThis slide„ÄÇYeah„ÄÇSo in terms of the problem
    definition„ÄÇI think is' quite similar to some meta learning problem„ÄÇwhich basically
    learns a mapping from a data point and the data sets to some predictions„ÄÇSo could
    you please suggest any differences between your problem setting and meta learning
    problem setting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I can't really„ÄÇFed out any differences between these two problems„ÄÇWell„ÄÇI think
    it really depends on the framing that you want to have right so I would say meta
    learning would be when I try to predict over multiple data sets„ÄÇso when I try
    to predict some when I try to learn some sort of prediction model or I can just
    plug in a different data set and it will automatically or almost automatically
    give me new predictions on this different data distribution„ÄÇBut that's not what
    we do at allÔºå rightÔºå we're training a single model for a fixed data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so this is why I wouldn't really call that meta learning because we're doing„ÄÇwe're
    trying to predict on the same tasks that all the supervised deep learning or any
    supervised machine learning method is trying to predict well on„ÄÇConsuming you
    use kind of same test site to test your trend modelÔºå rightÔºüI meanÔºå I meanÔºå like„ÄÇÂëÉ„ÄÇSo„ÄÇSo
    basically in MI learning we're going to test on different kind of meta test sets„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but in your case you just want to use a test set which is similar to the distribution
    Yeah of your training set rightÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Yeah absolutely so we explore data set distribution shift a bit„ÄÇI think it's
    a really interesting scenario I think meta learning different data sets is also
    an interesting scenario right when you have this model where you just pressure
    in different data sets but for the scope of this paper it's very much training
    set test set they come from the same distribution and we're just trying to do
    supervised learning in a standard setting thats so cool thank you„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåThank you for the question„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_16.png)
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå and I would chime in a couple additional thingsÔºå I guess„ÄÇso at least from
    what I understand from the problem definition of meta learning„ÄÇI think the aim
    is more„ÄÇPerhaps being able to perform well on a new data with a relatively small
    number of additional gradient steps on that data set„ÄÇso I think there's some interesting
    ways that you could actually consider applying NPTs in a meta learning type setting
    and so we'll get into this a little bit more but for example you know there might
    be ways to essentially add in a new data so let's suppose we've trained on a bunch
    of different data sets we now add in a new data set we can perhaps do some sorts
    of kind of zero zero shot meta learning basically where there's no need for additional
    gradient steps because we're basically predicting kind of similar to how you might
    do prompting nowadays in NLP literature„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: AnywaysÔºå yeahÔºå I think we'll get into some more details„ÄÇJust to chime in on
    that I don't think that every meta learning algorithm I think the ones that you're
    described right now are like optimization based„ÄÇbut they're also black box ones
    like you don't need to„ÄÇFurther„ÄÇI think the main difference seems to be that there
    is like one task versus multiple tasks for meta learning„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå I I think so tooÔºå I think the the like mainÔºå yeah„ÄÇtheing the main framing
    question is whether or not there's multiple data sets„ÄÇCool„ÄÇOkayÔºå awesome„ÄÇIf there's
    no other questionsÔºå I'll dive a bit more into the architecture„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_18.png)
  prefs: []
  type: TYPE_NORMAL
- en: AwesomeÔºå so there's three key components to NPpts„ÄÇI'm gonna to first state them
    at a high level„ÄÇand then we'll go through each of them in more detail„ÄÇSoÔºå first
    of all„ÄÇwe take the entire data set„ÄÇAll data points is input„ÄÇSoÔºå for exampleÔºå at
    test time„ÄÇthe model is going to take as inputÔºå both training and test data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we approximate this with mini batches for large data„ÄÇüòä„ÄÇWe apply self attention
    between data pointsÔºå so for exampleÔºå at test time„ÄÇwe model relationships amongst
    training pointsÔºå amongst test points and between the two sets„ÄÇAnd then finally
    we have this masking based training objective„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's a burnt like stochastic masking and the key point is that we actually use
    it on both features as well as on training targets and we'll get into why that
    kind of leads to an interesting predictive mechanism later„ÄÇSure„ÄÇSo to start with
    this idea of data sets as input„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there's two things that compose the input to NPTÔºå it's a full data set in the
    form of a matrix X and a masking matrix M„ÄÇAnd so Yick has described this data
    set matrix a little bitÔºå we basically have data points as rows„ÄÇthe columns are
    attributes and each attribute shares some kind of semantic meaning among all of
    its data points„ÄÇso say for example you're just doing single target classification
    or regression the last column would be the target and the rest of the matrix would
    be input features so for example the pixels of an image„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We also have a masking matrixÔºå so let's say you know we're thinking about mass
    language modeling the mass tokens will just tell us where we're going to conceal
    words and where we're going to backproagate a loss we do a similar type of thing
    here where we use this binary mass matrix to specify which entries are masked„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the goal is to predict mass values from observed values„ÄÇI see that there
    was a question about handling inputs with different lengths„ÄÇIn the data sets we've
    considered we'll get into it in the results section„ÄÇbut it's mostly been sort
    of tabular in image data where the lengths for each of the data points is the
    same„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it would work just like padding that would be a reasonable way to go about
    that and there's also kind of an interesting yeah go for an„ÄÇThis to add to that
    I'm not sure if length prefers refers to columns or two rows right rows we don't
    care about how many rows length padding or something would be an option Yeah my
    question was about column exactly so that that makes sense I think„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå and I meanÔºå that goes along with the whole meta learning discussion is
    I think if we wanted to adapt to data sets that have a different number of data
    data points per data set„ÄÇyou knowÔºå we can take advantage of the fact that self
    attention is kind of okay with that„ÄÇCool„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So continuing on„ÄÇLe to discuss here is basically how we do the embedding so
    to put this more explicitly we have this data matrix it has n data points„ÄÇit's
    called X and it all has D attributes and we have the binary mass matrix M we're
    going to stack them and then we're going to do a linear embedding so specifically
    we're doing the same linear embedding independently for each data point we're
    learning a different embedding for each attribute„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We have a positional encoding on the index of the attributes because we don't
    really care about say being equi over the columns„ÄÇif it's tabular data you of
    course want to treat all these kind of heterogeneous columns differently and then
    finally we have an encoding on the type of columns so whether or not it's continuous
    or categorical„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And that ends up giving us this input data set representation that is dimensions
    n by D by E„ÄÇThe second key component of NPpts is tension between data points„ÄÇSo
    to do that„ÄÇwe first take this representation we have and flatten to an end by
    d times e representation„ÄÇso basically we're treating each of these d times e size
    rows as if it's a token representation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We're actually going to just accomplish this operation using multied selfat
    you know we've reviewed this a lot„ÄÇbut the nice thing is that we know from language
    modeling we stack this multiple times we can model these higher order dependencies
    and here they between data points and that's really the key draw of this architecture
    there's been other kind of instances of people using attention for similar sorts
    of things so for example like attentive neural processes a lot of times they've
    sort of used just a single layer as kind of representational lookup and we believe
    that this actually ends up limiting expresssivity and that by sacking this many
    times you can learn more complex relationships between the data points„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: On any lots of questions„ÄÇSo you can go ahead first„ÄÇOhCool thanks I have a question
    about like how you guys do the embedding is there always like arties like convolutional
    filters or like linear layers like what is the type of embedding that you guys
    use Yeah so i'm attempting to go back to the slide I think it's not not very happy
    with linear now but yeah so for for Tular data we did just linear embeddings actually
    so we„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You knowÔºå we we could get into likeÔºå I guess details of feturization for categorical
    and continuous„ÄÇbut it's literally likeÔºå say for categoricalÔºå you know„ÄÇyou do a
    one hot encoding and then you learn this embedding that is specific to that attribute
    and then for numerical„ÄÇI believe we were just standard onizing for the image data
    we did end up using„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: A Resnet 18 encoder for C4 10„ÄÇhoweverÔºå I think that I mean„ÄÇwe'll discuss that
    a bit later in resultsÔºå but that embedding is a bit arbitrary„ÄÇyou can sort of
    do whatever the key part of the architecture is the attention between data points„ÄÇSo
    it's in terms of how you actually want to embed each attribute„ÄÇ it's kind of up
    to you„ÄÇThanks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think another questionÔºå same question as Victor Tos„ÄÇAwesome„ÄÇCool„ÄÇso here we
    have attention between data points done„ÄÇso we can also do this attention between
    attributes„ÄÇSo we reshape back to this N by D by E representation and then we can
    just apply self- attention independently to each row in other words to a single
    data point and the intuition for why we would kind of do this nested type idea
    where we switch between attention between data points and attention between attributes
    is just we're trying to learn better per data point representations for the between
    data point interactions this is literally just normal self-atten„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: as you'd see in language modeling or image classification the attributes are
    the tokens here„ÄÇAnd finallyÔºå we just minins and repeat„ÄÇSo what are we actually
    getting out of this to summarize we're learning higher order relationships between
    data pointsÔºü
  prefs: []
  type: TYPE_NORMAL
- en: We're learning transformations of individual data points„ÄÇAnd then importantly„ÄÇNPT
    is equivariant to a permutation of the data points„ÄÇthis basically just reflects
    the intuition that the learned relationships between the data points should not
    depend on the ordering in which you receive them or in which you observe your
    data set„ÄÇThe third key component of NPptTs is a masking based string objective„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So recall that what we're trying to do is we're trying to predict missing entries
    from observed entries and those mass values can be both features or targets so
    again the classic use say mass language modeling is to do self-supervised learning
    on a sequence of tokens which you could think of as just having features in our
    setting ours is a bit different in that we do stochastic feature masking to mass
    feature values with a probability piece sub future and then we also do this masking
    of training targets with this probability piece subtart so if we write out the
    training objective„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We are just taking a weighted sum of the negative log likelihood loss from targets
    as well as from features„ÄÇand of course at test time we're only going to mask and
    compute a loss over the targets of test points„ÄÇSo to break this down a bit further
    and point out some of the cool parts of it here„ÄÇthe thing that's highlighted right
    now on the far right is the term relating to the features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's the feature maskingÔºå basically we find that this has a nice regularizing
    effect more or less the model can now predict anywhere and makes the task a bit
    harder and introduce some more supervision and we found in an ablation for the
    tabular dataset sets that it helped for eight of 1 of those„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then there's this other termÔºå which is kind of interesting it's this stochastic
    target masking and the idea is that„ÄÇYou're actually going to have some training
    targets unmasked to the model at input at training time„ÄÇwhich means that the NPptT
    can learn to predict the mask targets of certain training data points using the
    targets of other training data points as well as all of the training features„ÄÇAnd
    so that means you don't actually need to memorize a mapping between training inputs
    and outputs in your parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You can instead devote the representational capacity of the model to learn functions
    that use other training features and targets as input„ÄÇSo this is kind of getting
    into the idea of this sort of like learn K and N idea„ÄÇYou know„ÄÇobviouslyÔºå we can
    be learn more complexÔºå relational lookups and those sorts of things from this„ÄÇBut
    you can imagine one such you know case beingÔºå we have a bunch of test data points
    coming in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We're going to look at their features and use that to assign them to clusters
    of training data points and then our prediction for those points is just going
    to be an interpolation of the training targets in that respective cluster that's
    like an example of something that this mechanism lets NPptTs learn„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_20.png)'
  prefs: []
  type: TYPE_IMG
- en: All rightÔºå so if there's any questionsÔºå we can take them now„ÄÇotherwise I'm happy
    to take them in the discussion or something„ÄÇAll right„ÄÇSo let's discuss go for
    it curious when you're using the entire data set„ÄÇIs that limit the type of data
    sets you can use because of the sizeÔºüYeahÔºå so in practice„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we do random mini batchching as an approximationÔºå so the idea is just you know
    if you have a reasonably large mini batch„ÄÇyou're going to benefit a bit from still
    having kind of this lookup ability because if youre reasonable number of classes„ÄÇprobably
    you're going to be able to learn some you know interesting mappings based on features
    and targets amongst those classes we found in practice that you know and we'll
    get into this a little bit„ÄÇbut we do actually indeed learn to use relationships
    between data points on prediction for data sets where we're doing mini batchching
    and we also didn't necessarily find that you need like a ludicrously large batch
    size for this to be a thing but I do think it's just this is in general and important
    point and it's one that points us towards looking into say you know sparse transformers
    literature for trying to expand to some larger data sets without having the mini
    batchching assumption„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: GreatÔºå thank you„ÄÇIf I can add a number to thatÔºå we can without mini batchching
    accommodate data sets off around like 8000 points or so„ÄÇso that already accounts
    for a fair proportion I would say of the tabular data sets out there but we also
    do data sets with  11 million points where obviously we then resort two mini batchching
    so it's very good to have like an idea of the sizes that we're talking about„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm curious on thatÔºå I mean it's pretty excitingÔºå I feel like you don't normally
    hear about„ÄÇTransformers being applied to the sets sub size 8000„ÄÇU„ÄÇI'm curious
    and we could talk about this sort of later once we've covered the other material
    if you found that sample efficiency is one of the key gains here or just experience
    working on small data of transformers generally and yeah„ÄÇbut I'll happy to put
    the answer to that until after as part of this„ÄÇüòäÔºåYeahÔºå I I think that'd be„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that'd be really nice to talk about a bit„ÄÇAnd it was something that in generalÔºå
    I guessÔºå I'd say„ÄÇwas surprising to us in terms of how robust NPTs were on small
    data sets and how we surprisingly didn't have to tune a terrible number of parameters„ÄÇBut
    we can get into details in a bit„ÄÇüòäÔºåawesome„ÄÇSo to get into the experiments„ÄÇwe focused
    a lot on tabular data because it's a very general setting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and it's also notoriously challenging for deep learning„ÄÇSo we knowÔºå you know„ÄÇtreebasedase
    boosting methodsÔºå stuff like X boost is very dominant and this is also a very
    relevant domain to„ÄÇI think people in industry and that sort of thing„ÄÇSo we were
    excited about the idea of trying to do better on this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we chose a broad selection of data sets varying across a few different dimensions„ÄÇyou
    know as we mentionedÔºå you know on the order of hundreds to tens of millions of
    instances broad range in the number of features in the composition of features
    in terms of being categorical or continuous various types of tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: binary and multiclass classification as well as regression And like I said„ÄÇthe
    baselines were kind of the usual suspects for tabular dataÔºå X boost cat boost„ÄÇbyGBM
    to MLPs and TnetÔºå which is a transformer architecture for tabular data„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_22.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_23.png)'
  prefs: []
  type: TYPE_IMG
- en: So to get into the results here I'm showing the average rank for the various
    subtasks we did well in terms of rankwise performance against methods like cat
    boostost and X boostt which are designed specifically for Tular data and in fact
    we find that NPT is the top performer on four of the 10 of these data sets on
    image data I mentioned that we used a CNN encoder and with that we were performing
    well on C410„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we also think thatÔºå you knowÔºå in generalÔºå like withÔºå let's say„ÄÇnew work
    on image transformers on small dataÔºå this can probably just be done with linear
    patching„ÄÇAnd so this kind of the manner in which you're embedding things is probably
    not the key„ÄÇNeil„ÄÇif I can jump in with two questions„ÄÇCan you go back two slides
    firstÔºü
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_25.png)'
  prefs: []
  type: TYPE_IMG
- en: One is just a small minor pointÔºå back one more please„ÄÇThank you„ÄÇhere for the
    featuresÔºå 50 plus„ÄÇwhat does plus mean hereÔºüI'll have to double check what the
    exact number is I'm pretty sure it's probably around 50„ÄÇI would guess like so
    the 50 is really a order of it's not like 150 or 5000„ÄÇYesÔºå yeah„ÄÇ I mean„ÄÇI I'll
    double check for youÔºå or you can check with the the metadata statistics at the
    end of the paper„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But noÔºå it wasn't like you knowÔºå arbitrarily largeÔºå I would sayÔºå though„ÄÇYou
    know„ÄÇwe did these ablations on whether or not we actually need attention between
    attributes„ÄÇWe did find that this ended ended up benefiting usÔºå but you could perhaps
    do kind of say„ÄÇjust an MLP embedding in that dimension and go to like a relatively
    small number of hidden dimensions and fit kind of an arbitrary number of features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I think thatÔºå yeahÔºå if youÔºå if you kind of relax the necessity of attention
    between attributes„ÄÇyou can probably scale out at least that dimension quite a
    lot„ÄÇOkayÔºå and then my second question„ÄÇif you could go forward one slide„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_27.png)
  prefs: []
  type: TYPE_NORMAL
- en: Thank you hereÔºå I'm not sure I quite caught„ÄÇwhat does four of 10 data sets„ÄÇ2
    of 10 data sets and four of 10 mean„ÄÇThis is of theÔºå of all the tabular data sets
    that we had„ÄÇSo ohÔºå I you think binaryer classification is for I seeÔºå okay„ÄÇYeahÔºå
    exactly„ÄÇAwesome„ÄÇAny other questionsÔºüThe standard errors here because I meanÔºå there's
    like there's just 10 data sets„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: right„ÄÇYeahÔºå correct„ÄÇ1010 total tabular data sets„ÄÇYeahÔºå but these are rank good„ÄÇYeahÔºå
    these are„ÄÇthese are rankwise performanceÔºå correctÔºå OkayÔºå I'm justing„ÄÇHow the„ÄÇWhere
    the uncertainty comes from in this caseÔºüYeah„ÄÇaverage averaged over four of 10
    data sets the rank so for each particular data set we have a rank of all the different
    methods then we take the average and the very answer of the rankings within each
    of the types of task within binary classification within multiclass„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: etc„ÄÇGood„ÄÇWe alsoÔºå if you're curiousÔºå you knowÔºå have the full results in the
    paper„ÄÇYeahÔºå thank you„ÄÇWe also have a couple of questions more some questions„ÄÇHey
    yeahÔºå thanks„ÄÇI guess I just found it a little surprising that the worst performer
    was KNN given that it's also nonparmetric„ÄÇI guess could you comment on that and
    yeahÔºå is it that there's something like intrinsic to the NPT that makes it just
    exceptional far beyond other nonparmetric methods or yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: why is it that KN performs the worst hereÔºüWellÔºå I suppose ultimately can and
    is is still a relatively naive predictive method in that you know it might just
    be predicting based on kind of cluster means so for example„ÄÇyou know I think this
    is probably universally true for all the data sets but there's probably some amount
    of kind of additional reasoning that needs to occur over the features at least
    to basic level so for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like one of the data sets is this poker hand data set where it's like a mapping
    between all of the different hands you have in poker and what like they're commonly
    known to people like full houses or whatever So this requires some amount of reasoning
    over the features to be able to group things together„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So just taking like the cluster means of the featurization of those different
    you know hands is likely not going to give you a great predictive function„ÄÇwhereas
    NPTs can kind of do the classic thing where say you have an MLP type of thing
    over the features or like a you know tree type of thing over the features„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can learn some sort of complex embeddingÔºå but then you also can do some
    nonparmetric sort of prediction based on say like clusters of embeddings„ÄÇI said
    yeah that makes senseÔºå I guess if what if you usedÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Pre trained embeddings from a stack of encoders as your vector representation
    for the canon„ÄÇHow do you think that would perform compared to the rest of the
    crowdÔºüYeahÔºå so this is likeÔºå I mean„ÄÇthis idea is kind of like deep kernel learning
    or likeÔºå yeah„ÄÇI believe it is deep kernel learning is basically you use an MLP
    independently„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you learn an MP on each input data pointÔºå and then you apply a G over all
    the representations of those„ÄÇyou get this sort of like complex embedding and then
    the lookups„ÄÇThe key difference between that type of idea and Ns is that we also
    learn the relationships between the data points themselves„ÄÇbecause we use this
    parametric attention mechanism to learn the relationships„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we're not just learning like an embedding independently„ÄÇWe're basically back
    propagating through the entire process learning the ways in which we would try
    to embed this„ÄÇbut also the the ways that say the lookup would occur and essentially
    the the relationships at that could potentially be kind of higher order as well„ÄÇcool
    more follow up„ÄÇüòäÔºåOh yeah go for it cool yeah thanks so I guess then if if the
    advantage of NPT has to do with sort of the relationships between data points„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: AndWhat if you you know took theÔºüTook the let's say you know encoder representations
    and then you passed that as input say for the you know 10 nearest neighbors along
    with like some other like„ÄÇinput representation and sort of had this like weighted
    average like attention style where you weighted the vectors of the nearest neighbors
    based on the attention weights between those input data points and then like the
    supplied input data point and then like past that as you know the vector to like
    the final prediction layer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like do you think that captures some amount of the relationship or is that off
    baseÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So I think the nice partÔºå like and really what our ideas behind this whole thing
    is just„ÄÇThese sorts of instances where certain fixed kernels would perform particularly
    well in tasks is like kind of an annoyance„ÄÇAnd like ultimately like tuning a lot
    of these types of things or're trying to derive the predictive methods that might
    make a lot of sense for a given situation„ÄÇkind of stinks And ideallyÔºå you'd want
    to just back propagate on a data and kind of learn these relationships yourself„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I actually would be really interested to see if we can come up with some
    synthetic experiments that have these sort of like very particular K and N„ÄÇlike
    predictive mechanisms and just see if we can learn precisely those and get you
    know„ÄÇzero error with NPpts„ÄÇ AndÔºå in factÔºå like we'll get into this a little bit
    with some of the interventional experiments we do„ÄÇWe have like kind of precise
    lookup functions that NPpts end up being able to learn„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we can learn interesting relational functions„ÄÇüòäÔºåCoolÔºå yeahÔºå thanks a lotÔºå
    appreciate„ÄÇÂ•Ω„ÄÇAll right„ÄÇwe have one more question from„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_29.png)
  prefs: []
  type: TYPE_NORMAL
- en: I just wanted to clarify something about basically so at test time you just
    take the exact same data set and you just like add like your test examples right
    and then you like do the same type of like masking and is that how it worksÔºü
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå correct„ÄÇOkay got it and I do have one more question that is just because
    like I think I like misunderstood like how like the effects of your NPT objective„ÄÇdo
    you mind going back to that slideÔºüSure„ÄÇYeahÔºå can you repeat one more time like
    what makes this so specialÔºü
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå so the regularizerizer on the right over the features I would think of
    very similarly to self-supervised learning with just a standard transformer like
    you're basically just introducing a lot more supervision and you're even if say
    you're just doing a supervised objective this is kind of like some amount of reconstruction
    over the features you learn a more interesting representation and like what a
    regularizing effect„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which we think is interesting but perhaps not as interesting as this stochastic
    target masking„ÄÇthis one is unique because„ÄÇIn kind of standard parametric deep
    learning„ÄÇyou're not going to have an instance in your training process where you're
    taking targets as input„ÄÇAnd so basicallyÔºå what happens is„ÄÇIf you have your training
    data set is input„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: whatever you're gonna have some stochastic feature masking stuff happening on
    the features amongst the training targets„ÄÇyou're randomly going to have some of
    those unmasked and some of them will indeed be masked„ÄÇYou're going to be back
    propagating a loss on the ones that are maskedÔºå of course„ÄÇbecause you don't want
    your model to have those available input if you're going to actually try to you
    know back propagate a loss on it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But you can use the other ones as inputÔºå And that means you can learn these
    kind of like interpoative functions„ÄÇSo that was like this whole idea of like being
    able to kind of like learn K and N„ÄÇBut doesn't that allowÔºüThe model to cheat again„ÄÇYeah„ÄÇso
    this is like an interesting point and actually like subtle so I think it's it's
    really worthwhile to bring up so first of all„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we never actually back propagate a loss on something that was visible to the
    model at input and so if„ÄÇfor example the model did actually end up basically overfitting
    on training labels we would not observe the model's ability to generalize to test
    data we don't observe this so obviously it seems like this kind of blocking of
    back propagation on labels that are visible at input to the NPT is helping„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It could also be possible that„ÄÇIn bird style stochastic masking„ÄÇyou also randomly
    will flip some labels to be in a different category„ÄÇSo this is like kind of just
    like a random fine print that was introduced in the bird masking text„ÄÇWe also
    do that„ÄÇSo it's possible that that somehow contributes to the to that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it's probably pretty likely to just be the fact that we're not back propagating
    a loss on something that's visible„ÄÇGreatÔºå thanksÔºå make sense„ÄÇüòäÔºåI have two more
    questionsÔºå if I can jump in„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_31.png)
  prefs: []
  type: TYPE_NORMAL
- en: YesÔºå sorry„ÄÇCan we go to the the metricsÔºå the performanceÔºå the results slide„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_33.png)
  prefs: []
  type: TYPE_NORMAL
- en: Sure„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_35.png)
  prefs: []
  type: TYPE_NORMAL
- en: I feel like I missed something else„ÄÇI'm sorry about this„ÄÇSo A U are„ÄÇso looking
    on the binary classificationÔºå A U R O C„ÄÇCan you clarify what these numbers mean„ÄÇAre
    they the A U ROC„ÄÇSo this is the so on each of the data sets„ÄÇSo say for a particular
    binary classification data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we're going to get a ranking of the methods we're going to repeat this yeah„ÄÇso
    these numbers here are the the relative ranking across in this particular case„ÄÇthe
    four data sets„ÄÇCorrectÔºå yeahÔºå I see„ÄÇSo thisÔºå these values are not the A U R O
    Cs on average„ÄÇacross the data sets„ÄÇNoÔºå yeah„ÄÇ they're not„ÄÇ I meanÔºå like everything„ÄÇAveraging
    our might make sense„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but averaging things like accuracy and RMC seems like a bad idea right because
    you might have some data sets where everything has high accuracy or where RMC
    needs something drastically different I see so this these numbers here only tell
    us the relative ranking between the different methods not how well they actually
    perform„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I meanÔºå it tells us how they perform relative to one anotherÔºå but not how well
    they perform in I see„ÄÇOkayÔºå but that's not in the appendix„ÄÇall we have that information„ÄÇI
    seeÔºå OkayÔºå I was„ÄÇI was sitting here confused going likeÔºå why is A U R O C„ÄÇWhy
    is the best one the smallest and accuracyÔºå What is an accuracy of 2„ÄÇ anyways„ÄÇ
    Okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that makes my more sense„ÄÇThank you both„ÄÇAwesome„ÄÇüòäÔºåGreat„ÄÇso I'll try to speed
    through this just in the interest of time„ÄÇBut the basically thing„ÄÇthe thing that
    you might be thinking after all these results is are we even learning any data
    point interactions on these real data sets„ÄÇAnd so basicallyÔºå we design an experiment
    to figure this out„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the idea is that we're going to disallow NPT from using other data points
    when predicting on one of them„ÄÇüòä„ÄÇ![](img/1777b223f7bf8be27a8767d209c7f271_37.png)
  prefs: []
  type: TYPE_NORMAL
- en: If we do thatÔºå and we observe that NPT actually predicts or performs significantly
    worse„ÄÇit is indeed using these interactions between data points„ÄÇA subtle challenge
    or kind of like an added added bonus we can get from this is that ideally„ÄÇwe wouldn't
    actually break batch statistics„ÄÇ So let's say like the mean of each particular
    attribute„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if we can find a way to do this experiment such that we don't break these things„ÄÇwe
    can kind of rule out the possibility that we learn something that's a bit similar
    to bash norm„ÄÇAnd so the way that we do this is we basically look at the predictions
    for each one of the data points in sequence„ÄÇSo let's say in this case we're looking
    at the prediction of the model for this particular green row and you know it's
    going be predicting in this last column that has this question mark which is mess
    what we're going do is we're going permute each of the attributes independently
    amongst all other data points except for that one so the information for that
    row if it was kind of just predicting like a classic parametric deep model is
    still intact but the information from all of the other rows is gone so that's
    why we call this sort of the corruption experiment„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so we find in general when we perform this experiment performance kind of
    falls off a cliff for the vast majority of these methods and I'll note that you
    the performances between the methods on a lot of these were fairly close and so
    this is actually indeed pretty significant so for example on protein we went from
    being the top performer amongst all the methods to the worst performer worse than
    even like KNN or something like that I'll also note that there's kind of this
    interesting behavior where on these data sets like For and kick and breast cancer
    we actually observe that there's basically no drop in performance and we basically
    see this as kind of an interesting feature and not necessarily a bug of the model
    which is that if we're back propagating on a given data the model can sort of
    just find that it's actually not that worthwhile to attempt to predict using some
    kind of you know relational predictive mechanism amongst data points and can instead
    just learn to predict parametrically and basically ignore other data points when
    it's predict on any given one of them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so this probably leads to some kind of like interesting ideas where perhaps
    you could do like postdoc pruning or something like that„ÄÇtaking away the tension
    between data points and doing fine tuningÔºå let's say„ÄÇAll right„ÄÇso now I'll hand
    over to Y to talk a bit about learning some interesting relationships„ÄÇYeah will
    you though I see that we're at the end of what the time is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but like I know there's a buffer planned and or something can you I can go through
    this experiment we can have a more discussion what's what do you guys preferÔºü
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå I think„ÄÇNormallyÔºå what we do is we would sort of stop the recording at
    this point and have an off the record discussion„ÄÇAnd„ÄÇI guess the question to ask
    is does anyone have any questions at this pointÔºü
  prefs: []
  type: TYPE_NORMAL
- en: But I think we've basically„ÄÇLaantching questions as they comeÔºå so I personally
    feel fine just yeah„ÄÇconsidering this a sort of question throughout„ÄÇYeah„ÄÇI guess
    that sounds that sounds good ya you can go forward with it with like with your
    tongue as planned and yeah we can data we can see about the time thing„ÄÇI think
    this will only be like another„ÄÇ4our or five minutes I shouldn't go for itÔºå yes
    for sure„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: All right„ÄÇSo Neil has now told us how well entities perform in real data and
    that they do make use of information from other samples of the input„ÄÇBut we're
    now going to take this a bit further and come up with some toy experiments that
    test a bit„ÄÇthe extent to which entities can learn to look up information from
    other role„ÄÇlike the extent to which they can learn this nonprometric prediction
    mechanism„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so specifically what we'll do is we'll create the following semi syntheticthe
    data set„ÄÇSo I want you to focus on A now„ÄÇYeah so we take one of the tabular data
    sets that we've used previously specifically the protein data set„ÄÇbut it doesn't
    really matter what matters is there it's a regression data set and so now what
    we do is we„ÄÇThe top half here is the original data setÔºå but the bottom half is
    a copy of the original data set where we have unveiled the true target value so
    now NPTs could learn to use attention between data points to achieve arbitrarily
    good performance„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they could learn to look up the target values in these matching duplicate row
    and then paste them back into that must out target value and then a test time
    of course we put in a novel test data input where this mechanism is also possible
    just to make sure that it hasn't learned to memorize anything but has actually
    learned this correct relational mechanism„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so what we see is that indeed MPs do successfully learn to perform this
    lookup so what I'm visualizing here is attention maps and they very clearly show
    that let's say when predicting for this green row here„ÄÇthis first green row what
    emmputees look at is exactly only that other green row here and so„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is really nice we can further look at the kind of the„ÄÇüòä„ÄÇThe pieson correlation
    between what MPs should predict and what they actually do predict and so this
    is 99„ÄÇ9% this is much better than anything you could achieve with parametric prediction
    and so it seems thatmp here can actually discover this mechanism and discover
    here I feel like it's the right word because MPmps could have as we've seen it's
    just also continued to predict in parametric fashion right from each row independently
    this is really kind of showing to us that there is this bias in the model to learn
    to predict from other rows and of course that is also very attractive in this
    setting because it allows you to achieve arbitrary load loss in this setting or
    as low as you can optimize for it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåAnd so„ÄÇWe kind of take that to mean that our gradient based discovery„ÄÇnon
    parametrics philosophy seems to make some sense„ÄÇAnd so we can take this a bit
    further by performing somewhat of an interventional experiment that investigates
    the extent to which NPTs have actually learned a robust causal mechanism that's
    underlying this semi synthetic data set„ÄÇüòäÔºåJust depending you know this„ÄÇThis extra
    extra column of test data that' already kind of cool„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but I think we can take a bit further and actually study if this generalizes
    beyond the data that we see in the training set or beyond data coming from this
    specific distribution and so what we now do is we intervene on individual duplicate
    data points at test time by varying their target value so now we only care about
    the prediction in a specific row we do this across all rows but at each time we
    just care about a single row what we do is we change the target value here that
    what we're hoping to see is and then NPT just adjusts the prediction as well right
    there's a very simple intervention experiment for us to test if NPTs have actually
    learned this mechanism and to some extent it also test robustness because now
    we're associating target values with features that are not part of the train distribution
    here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåAnd so what we see is that as we as we adjust these values here„ÄÇthis is the
    kind of the duplicate value and then we here see the target value as we adjust
    them„ÄÇwe can see the correlation stay is really really goodÔºå it's not quite 99„ÄÇ9%
    like on average„ÄÇwe're now at 99„ÄÇ6Ôºå but it' still veryÔºå very good„ÄÇüòä„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: At this point you might be slightly annoyed with me because you know standard
    nonprometric models can also solve this task right this is a task that I could
    solve by nearest enables Sure maybe you know I would have to change the input
    format bit because this is kind of like in a batch setting and I can just use
    masks but most generally nearest neighbor can also you know it also looks up different
    input points based on their features nearest neighbor doesn't learn to do this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I still think it's cool that we need to learn this because it does require you
    know a decent amount of you know computational sequences that we have to learn
    like match on the features look up target you copy it back and so on but„ÄÇIt is
    in fact very easy for us to complicate this task to a degree such that essentially
    no other model that we know of can can solve this very easily and so like a really
    simple thing to do is just to add a plus one to all of the duplicate values„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåSo now nearest neighbor would look up the right collar the right rowÔºå of course„ÄÇbut
    it would always predict the wrong target with a plus one on it and in in factÔºå
    many of the„ÄÇThe models that we're aware ofÔºå they're not modeling„ÄÇüò°„ÄÇThe joint distribution
    over features and targets what they're modeling is the traditional distribution
    of the targets given the input features and so they also cannot do this and so
    for us it's really not a problem at all MPs will just learn to subtract another
    one and no problems and sure this is also still a very synthetic setting but I
    do think I mean I challenge you to come up with something that MPmpes can't solve
    but the other models can solve I think in general this masking mechanism and the
    nonmetric of the approach really nice in general and leads to lots of nice behavior
    in a variety of settings and so with that I think we can go to the conclusions
    which Neil is going to give you„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_39.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_40.png)'
  prefs: []
  type: TYPE_IMG
- en: YeahÔºå I thinkÔºå I meanÔºå we're can cut out the„ÄÇMain part here„ÄÇI'll just fast forward„ÄÇlook
    at them Yeah yeahÔºå I was gonna say I think you you all get the gist NPpts take
    the entire data set input and they use self- attentiontention to model complex
    relationships between data points„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: knowÔºå they do well in experiments on type of their data as well as image data
    we present some of these interventional experiments to show that they can solve
    complex reasoning tasks there's some more experiments in the paper„ÄÇI'd say that
    you knowÔºå the interesting type of future work is scaling type things„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we can you knowÔºå not having this mini batchching approximation„ÄÇand then also
    just trying to expand this to some more interesting application domain So we talked
    a little bit about metal learning„ÄÇbut it could also be things like you know few
    shot generalization in general domain adaptation„ÄÇsemi surprise learningÔºå etc„ÄÇüòäÔºåSo
    I think if there's some more questions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: maybe we can do some more discussion„ÄÇYeahÔºå things sounds good„ÄÇGreat thanks for
    the talk„ÄÇI think everyone had a fun time see„ÄÇI will just ask some general questions
    and then we can have like a discussion session with everyone after that So I think
    one thing that I noticed is like like this like you said like this is similar
    to like canons and I thought like this seems similar to like graph neural networks
    where I can think like each data point is like a node and then you can think of
    everything as a fully connected graph and you're learning some sort of attention
    rate in this graph„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is like a note prediction task you are kind of doing on this sort of
    like graph structure„ÄÇso any comments on that like is it similar to like graph
    neural networks or is it like other differencesÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Yeah this is a very good observation yeah I think there are a lot of similarities
    to work on graphraph neural networks if we want to talk about differences the
    differences might be that we're kind of assuming a fully connected graph right
    and so you could maybe also phrase that as we're discovering the relational structure
    or as graphraph neural networks usually assume that it's given but that's also
    not always true and so there are a lot of similarities I don't know Neil if there
    is something specific you would like to mention go ahead but it's a very good
    observation and then we also do feel that that's the case and we've added an extra
    sectional related work to graphra neural networks in the updated version of the
    that will be online soon„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Got it„ÄÇYeahÔºå I agree with everything you've said„ÄÇI think that the closest work
    from the GNN literature that we were looking at a little bit was this neural relational
    inference paper„ÄÇwhich uses mess passage message passing neural networks to try
    to kind of like learn edges that may or may not exist and help for like extrapolating
    I think positions of like particles in like a multipart system or something„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is like kind of a similar idea to us like you know„ÄÇif you don't have these
    edges as given the attention mechanism kind of approximate and interesting relationship
    amongst some interacting things„ÄÇI see got it yeah that's pretty cool Another thing
    is like so you mostly look on like tableular data„ÄÇbut can you also like have other
    modalities like if you want to do language or something„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: can you still use non para transformomersÔºüYeahÔºå so I think part of our motivation
    for doing tabular was because we felt like tabular data is in a sense„ÄÇa generalization
    of let's say the language dataÔºå for exampleÔºå I mean„ÄÇI guess there's these other
    notions like that people have brought up like padding but ultimately you can think
    of it as like a bunch of categorical attributes So it is it is definitely generalizable
    to things like sentences and we do you know images so yeah I think actually like
    like I always go back and forth on whether or not I think smaller or larger data
    is more interesting for us So I think small data is really interesting because
    we can't just fit the entire data set into it and all of this just works out of
    the books without any extra thought but large data is actually also really interesting
    because„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Sure you might have to introduce some app mechanism or some lookup mechanism
    because you can't always have the entire data set in but at the same time you
    are very explicitly kind of trading off the compute that you use to look up with
    the compute that you need to store like how much how many parameters in GPT are
    used for storing data right there's lots of memorization happening in these models
    and we know that and so maybe we can use the parameters more efficiently to learn
    lookup type behavior right that is more close to this you know neurocanin or whatever
    so I think these are very exciting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Questions yeah yeah I'll also be looking forward to the future works because
    this seems like a very good way to like do one shot learning kind of situations
    so yeah really very interesting to see that„ÄÇOkayÔºå so I will stop the recording
    and we can have like any other questions„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1777b223f7bf8be27a8767d209c7f271_42.png)'
  prefs: []
  type: TYPE_IMG
