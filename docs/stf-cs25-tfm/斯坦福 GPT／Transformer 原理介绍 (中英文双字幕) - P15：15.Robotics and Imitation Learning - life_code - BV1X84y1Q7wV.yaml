- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P15：15.Robotics and Imitation Learning -
    life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P15：15.机器人与模仿学习 - life_code - BV1X84y1Q7wV
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_0.png)'
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_1.png)'
- en: Hey guys yeah thanks for waiting i'm really happy to be here I guess to shortly
    introduce myself my name is Ted Shao i'm a senior research engineer at the Google
    Bra team I've been on working on robotics now for the past five years I've touched
    upon a few topics including multitask learning reinforcement learning and then
    lately just broadly thinking about how we can scale robots to make sure that we
    can actually work in the wild in the real world。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 嗨，大家，谢谢你们的耐心等待，我很高兴能在这里。我来简单自我介绍一下，我叫Ted Shao，是谷歌Bra团队的高级研究工程师。我在机器人领域工作了五年，涉及了一些主题，包括多任务学习、强化学习，最近则在广泛思考如何扩展机器人，以确保它们能够在现实世界中实际工作。
- en: 😊，I guess today I'll be talking about quite a few different topics。but as a
    first preface I guess the first thing to know is that our team is pretty massive
    now all of these projects are huge collaborations with some projects have more
    than 40 people working on these for many years so these are large efforts and
    i'm just very fortunate to call myself to be on teams of very smart people and
    secondly some of my takes are spicier or more controversial than others and so
    all of those opinions are definitely only my own and don't reflect those of Google
    or anyone else on the team。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我想今天我会谈论一些不同的话题。首先需要说明的是，我们的团队现在规模相当庞大，这些项目都是大型合作，有些项目已经有超过40人参与多年，所以这些都是大规模的努力。我很幸运能与一群非常聪明的人一起工作。其次，我的一些看法可能比其他观点更激烈或更具争议，因此所有这些意见绝对只是我个人的，不代表谷歌或团队中的其他人。
- en: So with that out of the way， yeah， welcome to my TEDx talk。![](img/dde5df4844de2a3e55cc92f364263e12_3.png)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 话不多说，欢迎来到我的TEDx演讲。![](img/dde5df4844de2a3e55cc92f364263e12_3.png)
- en: 嗯。So I think maybe some of you have seen you know a lot of the cool robot videos
    learning videos out in the wild these days。but I am more excited than ever and
    it's not just hype I think I think there's been a fundamental shift in how researcher
    and robotics view learning over the past two years and I think the shift has a
    lot to do with all of the trends happening more broadly and foundation modeling
    in largescale internet models across different fields like language audio and
    so on but I think my goal today is to convey to you why I am particularly excited
    about this time today right and now and why there's been a very fundamental 18
    degree paradigm shift I think across the robot learning field and if you walk
    away from this talk with just one thing and that's you're slightly a bit more
    excited about robotics than you were before or believe that the time is now for
    these robots to really start scaling exponentially and doing something really
    cool I think then my talk will have succeeded。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。所以我想，也许你们中的一些人已经看过很多关于酷炫机器人学习的视频。这些天真是太棒了。但是我比以往更加兴奋，这不仅仅是炒作，我认为在过去的两年中，研究人员和机器人领域对学习的看法发生了根本性的变化，而这种变化与更广泛的趋势有很大关系，包括大型互联网模型在不同领域（如语言、音频等）中的基础建模。我今天的目标是向你们传达，为什么我特别兴奋于现在这个时刻，以及为什么在机器人学习领域发生了一个非常根本的18度范式转变。如果你在这次演讲中只记住一件事，那就是你对机器人科技的兴奋程度稍微提高了一些，或者相信现在是这些机器人真正开始指数级扩展并做一些很酷的事情的时刻，那么我的演讲就成功了。
- en: 嗯。galaxy。The talk will have a few parts we're going to start at a very high
    level and just talk about why a foundation for foundation model for robotics at
    all。what that might look like and the ingredients and recipe for how we might
    get there then we'll dive into a few different works pretty deeply that my team
    has been very proud of over the past year or two and finally we'll go back to
    the high level and then zoom out and think about what's next for robot learning。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。银河。演讲将分为几个部分，我们将从一个非常高的层面开始，谈谈为什么要为机器人建立基础模型。它可能是什么样子，以及我们可能如何达到的成分和配方，然后我们将深入探讨我团队在过去一年或两年中非常自豪的一些工作，最后我们将回到高层，思考机器人学习的下一步。
- en: So why a foundation model for robotics？One second， let me try to hide this thing。No
    that's fine I will keep that bar there for now but the top bar says why foundation
    model for robotics you know being coined here at Stanford and I'll use the phrases
    internet scale model foundation model and large language model pretty interchangeably
    throughout and I hope it's pretty clear but generally when I'm talking about these
    big monolithic beasts that are training on tons of data。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么机器人需要基础模型呢？等一下，让我试着隐藏这个东西。不，这没关系，我暂时会保留那个条，但顶部的条说“为什么机器人需要基础模型”，你知道这个概念是在斯坦福提出的，我将用“互联网规模模型”、“基础模型”和“大型语言模型”来互换使用，希望这能清楚表达，但通常我谈论的是这些在大量数据上训练的大型单体模型。
- en: they have two very important properties that I think are quite nice one is emergence
    when very simple things kind of work at a small scale they get a ton better when
    you just scale things up more data more compute larger models and what we see
    here is that when these models even become good enough the domain space of what
    they're good at and able to do starts to go combatorial even larger and here for
    these two points I would like to suggest two blog posts I highly recommend one
    is from Jacob Steinhart called more is different for AI and this kind of links
    the phenomenon。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 它们有两个我认为相当不错的重要特性，一个是“涌现”，当非常简单的事物在小规模上工作时，当你只是扩大规模，增加数据、计算和更大的模型时，它们的表现会好得多，而我们在这里看到的是，当这些模型变得足够优秀时，它们擅长和能够做的领域空间开始变得组合更大。对此，我想推荐两篇博客文章，我强烈建议阅读，一篇是雅各布·斯坦哈特的《更多就是不同：人工智能》，这篇文章链接了这个现象。
- en: See in other fields like physics or biology， for example。individual water molecules
    will behave very differently and have very different let's say electrostatic forces
    then they start to up clump up and start behaving as a liquid altogether we see
    this in herds of animal and flocking patterns we see this in humans in economies
    we see this all across different fields and now even in AI we see models that
    are doing stuff that will not be even possible or they add a smaller scale。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理或生物等其他领域中，例如，单个水分子的行为会非常不同，它们的电静力学作用也会有很大差异，然后它们开始聚集并共同表现出液体的特性，我们在动物群体和聚集模式中看到了这一点，我们在经济学中的人类行为中也看到了这一点，这种现象在不同领域普遍存在，如今甚至在人工智能中，我们看到模型在做一些在更小规模下甚至不可能的事情。
- en: but when they reach some critical scale in size， they start to work really really
    well this is documented by Jason in this blog post emergence in LOMs which you
    see this plot on the bottom left success rate across a bunch of different tasks
    whether it's modly arithmetic or purging question answering the success rate is
    basically flat until these models get big enough good enough and then these success
    rates just kind of skyrocket and that's why I think these are particularly exciting
    sky question I'm curious to know。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但当它们达到某个临界规模时，它们开始表现得非常好。这在杰森的博客文章《LOMs中的涌现》中有记录，你可以在左下角看到这个图，成功率跨越了许多不同任务，无论是基本算术还是抽取式问答，成功率基本是平稳的，直到这些模型变得足够大、足够好，然后成功率就开始飙升，这就是我认为这些模型特别令人兴奋的原因，我很好奇你对此的看法。
- en: The body foundation model display scaling now。Great question and I'm really
    glad you asked we have。I'm pretty excited to present some directions we have along
    that I hope we'll answer a question in maybe about 10 minutes or so yeah。but I
    think that's a question on all of our minds including myself。so I think before
    we even get to the feasibility or the existence of any robotics foundation models
    like is this even needed and I think the argument that I don't think is obvious
    is that I think emerging capabilities in relying on these might be actually indispensable
    for robotics to actually work a lot of the research over the past decades of robotics
    has been in one bin one room one table。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 身体基础模型现在展示了扩展性。好问题，我很高兴你问这个。我们有一些方向，我很兴奋能在大约10分钟内解答这个问题。但我认为这是我们所有人，包括我自己都在思考的问题。所以，我认为在我们甚至讨论任何机器人基础模型的可行性或存在性之前，我们需要问这是否真的必要，我认为一个并不显而易见的论点是，依赖于这些新兴能力可能对机器人实际工作是不可或缺的，过去几十年的机器人研究大多是在一个箱子、一个房间、一个桌子里进行的。
- en: one robot one building even but these are so vastly different from the orders
    of magnitude more complex while real world situations that humans operate in every
    single day and I think to make that gigantic leap we're going to have to rely
    on this emerging capabilities scaling curve where things kind of work you have
    very can demos maybe you have you know a humanoid robot programd the backflip
    after hundreds of trials but going from that to like the chaotic real world I
    think we're going to have to rely on this emergence phenomenon for that。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 一个机器人一个建筑，然而这些与人类每天所处的真实世界情况相比差异巨大。我认为，要实现这个巨大的飞跃，我们将不得不依赖这种新兴能力的规模曲线，其中某些东西可以正常运作，也许你有一个人形机器人经过数百次试验成功地编程了后空翻，但从这个到混乱的真实世界，我认为我们将不得不依靠这种涌现现象。
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_5.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_5.png)'
- en: And I think maybe even intellectually or academically it's also interesting
    to think about why or why not a foundation model for robotics might even work
    it's worked in so many other domains。there's existence proofs in audio music coding
    language another domain every single day it seems with 3D models and beyond but
    if there is something very special about robotics whether it's embodiment or causality
    or physical grounding and that is the barrier to making this very simple recipe
    that's worked in all these other domains if there is something special about robotics
    that causes this recipe to fail I think that's quite interesting to study why
    that is I'm personally an optimist I don't think there is some magical secret
    sauce that's going to keep robotics from being tackled with the same formulas
    and recipes that's worked elsewhere。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为，甚至从智力或学术的角度思考为什么或为什么不一个机器人基础模型可能会有效是很有趣的，它在许多其他领域都取得了成功。在音频、音乐、编码语言等领域，每天似乎都有存在的证明，甚至在3D模型及其他方面。但如果机器人有一些特别之处，无论是具身性、因果关系还是物理基础，这成为了使这个在其他领域都有效的简单配方无法适用的障碍。如果机器人有特殊之处导致这个配方失效，我认为研究原因是非常有趣的。就我个人而言，我是个乐观主义者，我不认为会有某种神秘的“秘密酱”阻止机器人使用在其他地方有效的相同公式和配方。
- en: but you know I think this is a concept I'd like to find out the answer to。And
    so maybe then instead of just motivating this philosophically okay we need foundation
    models foundation models are great let's try to build them for robotics how do
    we actually do that Well I think we can leverage a few ingredients by standing
    on the shoulder of giants and looking at other domains The first one is looking
    at different design principles of Ml scaling from other domains let's look first
    at highcapac architectures。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，我认为这是一个我想要找到答案的概念。因此，也许与其从哲学上激励我们需要基础模型，基础模型是伟大的，我们如何为机器人建立它们，实际上该怎么做？我认为我们可以通过借鉴其他领域的巨人的肩膀，利用一些成分。首先是关注其他领域的机器学习扩展的不同设计原则，让我们先看看高容量架构。
- en: the topic of this class today ideas such as self- attentiontension as all the
    different ideas encompass in the transformer as Andre Carpathy famously said。it's
    like a magical universal differentiable computer that's very general。very robust
    and very remarkably scalable on many different dimensions let's use those we should
    also leverage the more guiding principles that have been seen the scaling laws。the
    trends this here is Stcha know we not only have to scale the model size we also
    have to scale compute and we also have to scale the number of unique tokens in
    the corpus of the vast data sets that we train。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 今天这堂课的话题是自注意力等概念，以及变换器中包含的所有不同思想，正如安德烈·卡帕斯基著名所说的。这就像一个神奇的通用可微计算机，非常通用、非常稳健，在许多不同维度上极具可扩展性。我们应当利用这些，同时也要利用可扩展性法则的更多指导原则。这里的趋势是，我们不仅需要扩大模型大小，还需要扩展计算能力，也要扩大我们训练的大型数据集中唯一标记的数量。
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_7.png)'
- en: But if we do all three together， this has been shown to reliably have a pretty
    good chance of succeeding no matter what domain you're looking at。And so and finally
    what that kind of means， and I think this is actually going to come up later is
    that data set size seems to matter these days a lot more than quality。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们将三者结合，这已被证明在任何领域都有相当不错的成功机会。因此，最终这意味着，我认为这将在后面提到的数据集大小似乎比质量更重要。
- en: even if you have some sentences on Wikipedia that are misspelled or some some
    you know falsehoods or some things that aren't so desirable if in aggregate your
    data set is diverse enough and interesting enough。these things will hopefully
    wash out in the mix。Inreedient number two。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 即使维基百科上有一些拼写错误的句子，或者一些你知道的不实信息，或者一些不太理想的内容，只要你的数据集在总体上足够多样化和有趣，这些问题希望会在混合中被洗净。这是第二个成分。
- en: the proliferation of the internet scale models themselves， not just the principles。What's
    exciting and I'm sure it's you know definitely been very shocking for both experts
    and lay people alike is that a lot of these generative models across many different
    modalities have been experiencing emergingnt capabilities and have been surpassing
    all of our wildest expectations time and time and again but even when we think
    that we're exhaust at all this stuff is too much is not going to work something
    will come out and completely blow me out of the water and I think this trend will
    definitely keep continuing and I think in addition to that they not only will
    continue coming on and accelerate more rapidly they're going to happen with whether
    or not like we do anything you in the grand scale speaking me as a robotics researcher
    or you know you in whatever subfield you on there are parts of machine learning
    that likely you'll probably not ever touch in at least the near future and those
    parts will be seeing tremendous breakthroughs and scaling and new capabilities
    coming online every single week。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 互联网规模模型本身的激增，不仅仅是原则。令人兴奋的是，这无疑对专家和普通人都造成了震惊，因为许多不同模态的生成模型正在经历新兴能力，并一次又一次地超越我们所有的期望。即使我们认为已经耗尽，认为这些内容太多、不可能工作时，总会出现一些东西彻底颠覆我的认知。我认为这种趋势肯定会持续下去，而且不仅会继续加速，它们的发生将不受我们是否采取行动的影响。作为一个机器人研究人员，或者你在任何其他子领域中，机器学习的某些部分可能在短期内你根本不会接触到，而这些部分将每周都在见证巨大的突破、扩展和新的能力上线。
- en: And you can look at this not only in the impressiveness of the models。but also
    the acceleration of progress， the time scales in which new models are being released
    where large collaborations are being worked on by many groups and then you know
    being available to access for all to use and build upon。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以不仅从模型的令人印象深刻程度来看待这一点，还可以从进展的加速、发布新模型的时间尺度来看，许多团体在大规模合作下共同努力，然后这些模型又可以被所有人访问和使用，进行构建。
- en: And the final ingredient and this trend is more of a robotic specific one。but
    it is a vast shift from online robotic learning where robots collect experience
    online make actions and learn through trial and error to an offline setting where
    we decouple the data generation process from the data consumption process as we've
    seen in all these other foundation modeling domains。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的成分是一个更具机器人特定趋势的内容。这是一个巨大的转变，从在线机器人学习中，机器人在线收集经验，进行操作，通过试错学习，转变为离线环境，在这里我们将数据生成过程与数据消费过程分开，正如我们在其他基础建模领域所见的那样。
- en: these big internet scale data sets are so diverse and they're static we just
    scrape them once or scrape them multiple times continuously but we aggregate a
    continuous pile that's just growing here we see either the pile data set from
    a Luther or Lyon 5B for image paired image text and these are pretty big and they'
    orders of magnitude more than what we've seen before and they are definitely a
    key ingredient to why other domains have been doing so well by training these
    big foundation models。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大型互联网规模数据集是如此多样化而静态，我们只需抓取一次或多次连续抓取，但我们聚合出一个不断增长的持续数据堆。在这里，我们看到来自Luther或Lyon
    5B的数据集，这些图像配对文本的数据集相当庞大，数量级上远超我们以前见过的，这无疑是其他领域能够如此成功训练这些大型基础模型的关键成分。
- en: And。This coming back to robotics then I'd like to take a brief detour into how
    the shift came to be because it's very easy to say in a sentence yeah robotics
    is offline more than online and this is coming as kind of a no brainer to many
    folks who are coming from other domains like this is the way things are done but
    in robotics this has been a very big shift and I think robotics has also been
    synonymous with RL reinforcement learning for a lot of people and I think increasingly
    this is becoming less true and so I'd like to take you down a brief trip down
    the history of my team the slide of the talk as brief history of robotics at Google
    and yeah of course thanks and I think this is not just for dramatic exposition
    it's really to try to guide you through how drastically our team's thinking has
    kind of evolved over the years and how that's going to inform the design decisions
    and and the kind of risks and research directions that we take in the specific
    projects that I'm going to show coming up thank you。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，这回到机器人技术上，我想简要讲一下转变是如何发生的，因为说“机器人技术更多的是离线而非在线”这一句很容易，对许多来自其他领域的人来说，这似乎是显而易见的方式，但在机器人技术中，这是一个非常大的转变，我认为对于很多人来说，机器人技术与强化学习是同义的，但我认为这种情况正变得越来越不成立，因此我想带你们回顾一下我团队的简史，关于Google机器人技术的简要历史，当然，谢谢，我认为这不仅仅是为了戏剧性表述，而是真正想指导你们了解我们团队的思维是如何在这些年中发生了剧烈变化，这将如何影响我们在具体项目中所做的设计决策，以及我们所承担的风险和研究方向。谢谢。
- en: So in 2016 some of you may have seen this we had what we called the arm farm7
    cua robots in a room collecting picking data 247 and this was doing on policyL
    in the real world we were the first team to kind of say hey can we can we even
    do this with the goal of saying can we do end to end robot learning with results
    in the real world this was kind of risky at the time that was not a common take
    and from that we developed several interesting research directions that we started
    exploring we looked into stuff like QTO which is a Q learning method working on
    continuous control actions while taking vision inputs we worked on cyclegan to
    transform simulationbased images into real real-look images for Sim to real we
    looked at concurrent role of how we get robots looing faster and more efficiently
    in the real world I'm sorry do you have a question yeah great question and that
    one I think was basically。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在2016年，你们中的一些人可能见过这个，我们有一个我们称之为臂农场的项目，7个Cua机器人在一个房间里进行247的采集和拾取数据，这是在现实世界中进行的在线策略学习，我们是第一个团队说“嘿，我们能不能这样做，目标是能否进行端到端的机器人学习，并在现实世界中取得成果”，这在当时是有一定风险的，并不是一个常见的观点，从中我们发展出了几条有趣的研究方向，我们开始探索，比如QTO，这是一种在处理连续控制动作时使用视觉输入的Q学习方法，我们还使用CycleGAN将基于仿真的图像转换为真实感图像，以实现从仿真到现实的转变，我们研究了如何让机器人在现实世界中更快更高效地工作。对不起，你有问题吗？是的，很好的问题，我认为基本上是这样的。
- en: The arms would pick stuff up from the bin if they messed up and it fell out
    well we'd come back the next morning and there'd be objects scattered all throughout
    the room。so there was no reset but if they missed a little bit the objects would
    fall back into the bin and hopefully be in a position where they could pick them
    up again。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果臂从箱子里拿东西时出现错误掉了出来，我们第二天早上会回来，发现房间里散落着物体。所以没有重置，但如果它们稍微错过，物体会掉回箱子，希望能够重新处于可以再捡起的位置。
- en: 😡，Oh yeah of courseChat thanks I'll do that in the future this specific question
    was for this 247 arm farm how did we do resets and the the answer is well we didn't
    we designed the bin so that they were kind of banked so that objects slightly
    missed they would fall back in the bin reorn themselves maybe add more diversity
    with the training data but this was doing off policy online Rl with Q learning
    and we mixed it with SIim data deployed again。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 😡，哦，当然，聊天谢谢你，我会在未来这样做，这个具体问题是关于这个247臂农场的，我们是如何进行重置的，答案是我们没有，我们设计了一个箱子，使得物体稍微错过时会掉回箱子中，自我再生，也许会通过训练数据增加更多多样性，但这是在使用Q学习进行离策略在线强化学习，同时我们又将其与SIim数据重新部署。
- en: Next we kind of went through this consolidation phase around 2020 when we're
    like all right this is pretty cool and you know but we want to get out of the
    bin how do we do more complex tasks and a more practical setting that could be
    closer to something that humans would want to use that's more general every day
    there we kind of settled on this office micro kitchenitchen environment if you
    heard of the famous Google micro kitchenitchs and I think this was the setting
    we decided's operate in and there we started collecting data we scaled our real
    operations and there we kind of scaled approaches to some different things and
    I think in the bottom right here is like the more mechanized reset version I would
    say of the arm farm here we had a bin that folded in half and this was doing multitask
    Rl in the real world and the bin would flip in half dumping objects from one side
    to the other so you could do more interesting tasks whereas the arm farm was pick
    anything up now we could say hey pick up the carrot and place the tomato onto
    the plate and then the bin would flip and you'd reset。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们大约在2020年经历了一个整合阶段，觉得“这真不错”，但我们想要脱离简单的框架，如何在更实际的环境中完成更复杂的任务，以更接近人类日常使用的方式。在这里，我们最终确定了一个办公室微型厨房的环境，如果你听说过著名的谷歌微型厨房，我想这是我们决定操作的环境，我们开始收集数据，扩大我们的实际操作，并探索一些不同的方法。在右下角，可以说这是机械化的重置版本，我们有一个可以折叠的箱子，它可以进行现实世界中的多任务强化学习，箱子会在两侧翻转，从一个侧面倾倒物体到另一个侧面，这样我们可以进行更有趣的任务，而之前的手臂农场只能任意拾取，现在我们可以说，“嘿，拿起胡萝卜，把番茄放到盘子上”，然后箱子会翻转并重置。
- en: Some other works to at multitask imitation learning， this is BC0。and then we
    also looked at stuff like combining reinforcement learning with imitation learning
    bootsottrapping。But in 2020 once again we realized we were working on a ton of
    different directions and we wanted to consolidate and I think the two main things
    that were really boling us point at the time where we were hitting two main walls
    across all these methods some of them were plateauing at this 50 to 70% you know
    rough range in the real world and other methods were requiring very specific data
    distributions they had to be on policy or they could only use demonstrations or
    they blah。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在多任务模仿学习方面，还有其他一些工作，这就是BC0。我们还研究了将强化学习与模仿学习相结合的内容。但在2020年，我们再次意识到我们在许多不同的方向上工作，想要整合。我认为当时真正困扰我们的两大问题是，我们在所有这些方法中遇到了两个主要的瓶颈，一些方法在现实世界中的表现停滞在50%到70%左右，而其他方法则需要非常特定的数据分布，它们必须在政策上，或者只能使用演示，等等。
- en: blah， blah like there are so many different nuances and like gotchas to all
    these different methods and all these different drawbacks and so the question
    we posed was we're open to any method。any strategy that will enable us to solve
    tasks in a very performant matter more than 90% in the real world and also that
    can scale with some kind of data that we can collect you know and maybe this is
    a bit more lax than let's say an academic setting where you're much more resource
    constrained but at the end of the day you know even our team does not have infinite
    money we still have a certain number of row。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: blah，blah，这些不同的方法都有许多不同的细微差别和问题，我们提出的问题是我们对任何方法开放，任何能够让我们在现实世界中以非常高效的方式解决任务的策略，准确率超过90%，并且能够根据我们收集的一些数据进行扩展。你知道，这可能比在资源更加有限的学术环境中要宽松一些，但归根结底，我们团队也并没有无限的资金，我们仍然有一定数量的资源。
- en: certain of operators and we're constrained by the laws of physics so we need
    some way to acquire more data that we can then learn from and so we're all scratching
    our heads thinking about this for a few months in spring 2022。We decided on going
    with multitask imitation learning so this was a vast departure from the 247 arm
    farm this was a vast evolution of how we approached the problem we found that
    you know with enough you know gentle care and love multitask imitation learning
    was able to hit these 90% numbers and those able to get better with more demonstrations
    these aren't the cheapest thing but it was able to scale with additional demonstrations
    which was the sign of life that we were looking for so that brings us to less
    than a year ago our team was deciding this is the path forward at least in the
    near termm future but。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 某些操作员受限于物理法则，因此我们需要某种方式来获取更多数据，以便我们可以从中学习。因此，在2022年春季，我们都在思考这个问题几个月。我们决定采用多任务模仿学习，这与247臂农场大相径庭，这是我们解决问题方法的巨大演变。我们发现，只要有足够的温柔关爱，多任务模仿学习能够达到90%的表现，并且随着更多演示而变得更好，这些演示并不是最便宜的，但它能够随着额外演示而扩展，这正是我们寻找的生命迹象。所以这让我们回到了不到一年前，我们团队决定这是近期未来的前进方向。
- en: Maybe you know we could just think about how the approach we were taking here
    might also spread out in the future and we might be able to bring back these other
    threads。for example， if now that we're decoupling this data collection of demonstrations
    or et from how you learn from them with a multitask imation learning policy。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们可以考虑我们在这里采取的方法未来如何扩展，我们可能能够重新引入这些其他线程。例如，现在我们将演示数据的收集与如何从中学习通过多任务模仿学习政策解耦。
- en: maybe we in the future then do something like offline RL。But I think at a high
    level now I've just you know in a few short minutes just compressed six years
    of very bitter lessons that our team has been learning and I think from where
    we are today and looking back even just two years ago。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 也许未来我们会做一些像离线强化学习的事情。但我认为在高层次上，我在短短几分钟内压缩了我们团队六年非常痛苦的教训，从今天的角度回顾，甚至仅仅两年前的情况。
- en: if you told me that the strategies were at point today could just scale the
    way they are。I probably would not have believed you。😡，啊，有几。But anything they do
    instead of like trying figure out how it did。Great question so I think task conditioning
    is definitely still was an open question at the time。but I think with this work
    BC0 we found that language was able at least in a templated language kind of representation
    was good enough where we could direct I think BC zero is over 80 tasks so they
    were very templated like pick grapes or like move grapes onto play or drag this
    drag cloth across table and I think this representation was still enough where
    you're learning a good number of skills you're passing in essentially a one hot
    ID into your policy network and it will learn to imitate that and for each one
    of those 80 task could collect hundreds or thousands of demonstrations。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你告诉我今天的策略能够以这种方式扩展，我可能不会相信你。😡，啊，有几。但无论他们做什么，而不是试图弄清楚怎么做。这是个很好的问题，我认为当时任务条件化确实还是一个开放的问题。但我认为通过这项工作BC0，我们发现语言，至少在模板化语言的表示中，足够好，我们可以指导，我认为BC0涉及超过80个任务，它们非常模板化，比如采摘葡萄，或将葡萄移到盘子上，或者把布拖过桌子。我认为这种表示仍然足够，让你学习到相当数量的技能，你本质上是将一个独热编码ID输入到你的策略网络中，它将学习模仿，而对于这80个任务中的每一个，能够收集到数百或数千个演示。
- en: And I will touch upon the specifics of that a bit later too。Um。So yeah， today，
    or at least in 2022。Let's do offline methods， let's decouple data generation from
    data consumption。And let's take these three lessons now that we touched upon。let's
    take the design principles of ML scaling and then figure out what lessons can
    actually be applied when we go look into the future for recipe for robot learning
    and foundation models。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍后会详细谈谈这些具体内容。嗯。所以是的，今天，或者至少在2022年。让我们做离线方法，让我们将数据生成与数据消费解耦。让我们利用这三条我们提到的教训，结合机器学习扩展的设计原则，找出未来在机器人学习和基础模型的配方中可以应用的教训。
- en: The first lesson I think is very important is these high capacity architectures
    like attention and the second I'll touch on later is data interoperability tokenization
    discretization and the second ingredient is the proliferation of these models
    themselves Can we leverage them because they will get better over time and I think
    here I would like to plug my colleague Carol Helsman's bitter lesson 2。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为第一个教训非常重要的是这些高容量架构，比如注意力机制，第二个我稍后会提到的是数据互操作性、标记化、离散化，而第二个要素是这些模型本身的普及，我们能否利用它们，因为它们会随着时间的推移变得更好，我想在这里提到我的同事卡罗尔·赫尔斯曼的苦涩教训2。
- en: 0 which is the bitter lesson the first one from Richard Sutson was you should
    leverage methods that scale with more compute and maybe in today's day and age
    the lesson is that we should leverage methods that are able to utilize improvements
    and foundation models because they're going to get better yeah so the lesson 1。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 0 这是来自理查德·萨顿的第一个苦涩教训，你应该利用能够随着更多计算而扩展的方法，也许在当今时代，教训是我们应该利用能够利用基础模型改进的方法，因为它们会变得更好，是的，这就是教训1。
- en: 0 and。0 one thing thats compare me is have a set。And I want to choose the methods
    that are going to scale with more computer or in this case scale with better foundation
    models the question is how do I actually decide which of those methods meet those
    criteria？
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 0和。0 一件让我比较的事情是拥有一套方法。我想选择那些能随着更多计算机扩展，或者在这种情况下，随着更好的基础模型扩展的方法，问题是我到底如何决定哪些方法符合这些标准？
- en: Yeah， great question I think and maybe it's and I think that's a very I don't
    have a good answer for that Oh sorry the question was in bitter lesson 1。0 and
    bitter lesson 2。0 the question is well that's great that's the lesson but how
    do we actually decide which methods meet this criteria and I think you my answer
    is it's not always obvious and it's actually quite tricky sometimes but maybe
    you know sometimes you know what you can be very confident that。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，问题很好，我认为，也许这是一个很好的问题，我对此没有好的答案，哦，抱歉，问题是关于苦涩教训1。0和苦涩教训2。0，问题是，好的，这就是教训，但我们到底如何决定哪些方法符合这些标准，我认为我的答案是，这并不总是显而易见，实际上有时相当棘手，但也许有时你知道你可以非常自信地说。
- en: oh yeah this will definitely scale with more data and compute and some that
    are but basically the more hardcoded you are。the more assumptions， the more heuristic
    you bake in the more you in our day and age。the more you rely on a specific implementation
    of a specific foundation model of a specific class of algorithm。maybe that will
    be less robust than a method that just assumes some very abstract input and output
    and assumes that how you get from that input and output can improve over time
    and maybe the algorithm itself even changes altogether so I think that would be
    my take on the bitter lesson 2。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，是的，这肯定会随着更多的数据和计算而扩展，某些方面是这样的，但基本上你越是硬编码，假设就越多，启发式就越多，你就越依赖于特定基础模型的特定实现和特定算法类，也许这将比一种假设某种非常抽象输入和输出，并假设如何从输入和输出改进的方式的方法更不可靠，也许算法本身甚至会彻底改变，所以我认为这就是我对苦涩教训2的看法。
- en: 0 but this is definitely still I think the jury's still out on this。😊，で。And
    my basic one of the things I like to propose is that language is the way that
    we can leverage bitter lesson 2。0 if you have language as the universal representation
    through which all these foundations communicate to each other。whether it's you
    know captioning or generation or whatnot。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 0 但这仍然是，我认为陪审团尚未对此做出决定。😊，对。我基本上想提出的一个观点是，语言是我们利用苦涩教训2。0的方法，如果你把语言作为所有这些基础相互交流的通用表示，无论是你知道的图文描述、生成或其他。
- en: I think that's one way that we could leverage bitter lesson 2。0。And finally，
    the third ingredient。offline robot learning， decoupling data generation from data
    consumption。Putting these all together。my recipe for how one take at a modern
    attempt that embodied intelligence would look like would be to combine these large
    offline data sets with high capacity architectures by using language as the universal
    group and in the works I'm going to present shortly all of our different projects
    I think in some way or another are inspired by this philosophy。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一种利用苦涩教训2。0的方法。最后，第三个要素是离线机器人学习，将数据生成与数据消费解耦。将这些结合在一起，我对现代体现智能的尝试的配方是将这些大型离线数据集与高容量架构结合，通过使用语言作为通用的媒介，在我即将呈现的不同项目中，我认为在某种程度上都受到了这一哲学的启发。
- en: Oh。And now now that we've kind of you know， understood the motivations and potentially
    one possible approach。sorry of course the bottom often is that architecture using
    languages and I'm curious to know which if any of these are currently。Waternck's
    not the right word originals are limited。Got it because it seems like we already
    have architecture architecture seems we already not the question was it seems
    like we have a lot of these ingredients and so why hasn't robotics been solved
    yet so I would argue that actually this take here and maybe this is to the wrong
    audience at the moment。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 哦。现在我们似乎理解了动机和可能的一种方法。抱歉，底层常常是使用语言的架构，我好奇这些中是否有任何当前的。Waternck这个词不太合适，原始的内容是有限的。明白了，因为看起来我们已经有了架构，似乎我们已经有很多这些成分，那么为什么机器人技术还没有解决呢？我认为实际上这里的观点，也许现在对于这个听众来说不太合适。
- en: but I think this is not very nonobvious across the robotics field many people
    do not agree with all of these much less two of these or even any of these points
    and so I think and also the existence of the scale of how mature each of these
    components are within robotics is at very different stages and I would say like
    and we can talk a bit later about like for example like data scale or the architectures
    that have kind of diffused through osmosis from other ML domains into robotics
    but I think we're still at very different stages on how how much people have actually
    bought into these lessons and invested in them。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为这在机器人领域并不明显，许多人并不同意所有这些，甚至对于其中的两个或任何这些观点也持怀疑态度。并且我认为，机器人技术中每个组件的成熟度的规模存在很大差异，我们可以稍后讨论，例如数据规模或已经通过其他机器学习领域渗透到机器人中的架构，但我认为我们在各自如何接受这些教训并对其进行投资方面仍处于非常不同的阶段。
- en: you're not getting into this， sorry I'm curious to know I'm asking you to make
    names。The people who might not be mining into all these pieces。1。yeah I can probably
    I also don't want to get into too much trouble here i'll probably get myself in
    a bit of hot water in a few slides so I'll expand upon of it then yeah yeah yeah
    and I would say that like me personally and you know not speaking for my team
    but a lot of people on my team are probably at the very extreme end of learning
    scaling data drivenve you know foundation model based let's go big and I think
    a lot of people don't believe that yeah happy to discuss why later。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你没有参与这个，抱歉，我很好奇想问你一些名字。那些可能不会深入挖掘这些内容的人。1。是的，我可能也不想在这里惹上太多麻烦，我可能会在几张幻灯片中陷入一些麻烦，所以我会进一步扩展这个问题。是的，我个人而言，当然不代表我的团队，但我团队中的很多人可能在学习、扩展、数据驱动的基础模型方面处于极端的边缘，让我们大胆尝试，我认为很多人并不相信这一点，之后乐意讨论原因。
- en: Maybe after the zoom as well so yeah well okay， then let's let's go ahead and
    dive in and see how this recipe might actually percolate into specific domains。😊，And
    the first one is RT1， this is a recent work from our group that works on how we
    can scale imation learning and let's look at how we can actually apply these first
    principles。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 也许在 Zoom 之后也是如此，所以好的，那我们就开始深入探讨，看看这个食谱如何真正渗透到特定领域。😊 第一个是 RT1，这是我们小组最近的工作，研究如何扩展模仿学习，看看我们如何可以实际应用这些基本原则。
- en: So the first one is to consider what we actually let' let's put ourselves into
    the spring 2020 mindset we've been collecting demonstrations for a while。This
    is a ton of demos like 100000 over that was collected over like a year and a half
    on many。many robots on many， many tasks that exists it was expensive and over
    time this will actually not trickle up at insane amounts like we won't just get
    100000 new high- quality demos every day this will grow over time but it's not
    going to grow for free and autonomous ways of doing this is very hard as you saw
    earlier with MT with a bin reset mechanism or DeepM has a work called Rgb stacking
    where they try to do autonomous resets and the way that we're doing it right now
    or at least for this paper was human teleop pioneered by BC0 and that was very
    expensive as well so this is gonna to be limited through and finally BC0 used
    a resnetbased backbone and it was pretty good but it found that it was very sensitive
    to training distributions for example when they remove data from some teleopators
    to make the data more。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以第一个要考虑的是我们实际上要做什么，让我们把自己放在2020年春季的思维模式中，我们已经收集了演示一段时间。这是大量的演示，大约100000个，花了一年半的时间在许多机器人上执行许多任务。这非常昂贵，随着时间的推移，这不会以疯狂的速度增加，我们不会每天获得100000个高质量的新演示，这将随着时间的推移增长，但不会免费增长，采用自主方式做这件事非常困难，正如你之前看到的MT使用的bin重置机制，或者DeepM有一项名为RGB堆叠的工作，他们试图进行自主重置，而我们现在的做法，或者至少在这篇论文中，是由BC0开创的人工远程操控，这同样非常昂贵，因此这将受到限制，最后BC0使用了基于ResNet的主干网络，效果相当不错，但发现它对训练分布非常敏感，例如，当他们从一些遥控操作中移除数据以使数据更加均匀时，性能反而有所改善。
- en: genous performance got better and that's not really a property we like right
    we want more data even if it's not exactly the same so the lesson here models
    need to be robust and they need to generalize。Cool so we have models need to be
    robust and generalized what else do we have well off the shelf models are pretty
    slow if we take in these huge you know vision transformers other domains they're
    not going to run on the real robot we need to be able to run at a pretty high
    frequency they need to be reactive inference time need to be slow because all
    our models are vision based。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 性能变得更好，但这并不是我们喜欢的特性，我们希望更多的数据，即使它们不完全相同。因此，这里的教训是模型需要强健，并且需要泛化。好吧，我们知道模型需要强健和泛化，还有什么呢？现成的模型相当慢，如果我们采用这些巨大的视觉变压器，在其他领域中，它们无法在真实机器人上运行，我们需要能够以相当高的频率运行，它们需要具有反应性，推理时间需要短，因为我们所有的模型都是基于视觉的。
- en: And finally， we want our data to be able to understand language， as I mentioned。if
    language is the universal glue， our data set already has some language and we
    want eventual models to be very multimodal。this is a first principle that we need
    to take in。What does this mean we can't just take something existing。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望我们的数据能够理解语言，正如我提到的。如果语言是普遍的粘合剂，我们的数据集已经包含了一些语言，我们希望最终的模型能够非常多模态。这是我们需要考虑的首要原则。这意味着我们不能仅仅采用现有的东西。
- en: we probably need to design or at least modify something from the ground up and
    let's take the best practices that we've seen work in other fields。And so we worked
    for a bit and we came up with this architecture for RT1 again once again。this
    was a large team with a bunch of different contributions and I'll just go through
    a few of them here。At a high level RT1 is robotics transformer it operates at
    three hertz it takes in visual input from the robot RGB camera as well as a natural
    language instruction there the image is patchrified and fed into a film efficient
    net tokenizer it's then passed into token learn which I'll talk about soon and
    then also the language instructions are tokenized and then they are put into the
    same transformer and then finally we output discretized actions as tokens and
    send that to the real world in threeHtz in closed loop。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能需要从头开始设计或至少修改一些东西，让我们借鉴在其他领域中看到的最佳实践。因此，我们工作了一段时间，并再次提出了这个RT1的架构。这是一个大型团队，参与了许多不同的贡献，我将在这里简要介绍其中一些。从高层次来看，RT1是一个机器人变压器，它以三赫兹的频率运行，接收来自机器人RGB相机的视觉输入以及自然语言指令，图像被切片并输入到一个高效的网络分词器中，接着传递给即将讨论的token
    learn，同时语言指令也被分词，并被放入同一个变压器中，最后我们输出离散化的动作作为tokens，并以三赫兹的频率将其发送到现实世界中，形成闭环。
- en: This transformer is a decoder one we use a sparse categorical entropy objective
    for action prediction by applying a causal mask。we use the pretrained efficient
    net backbone and we also use token learner very for faster inference。Ding a little
    bit deeper， oh sorry yeah a question。对到。对对对。Great question。The image token when
    it goes in from so each image is the you know the high fidelity RGB image from
    the camera we split that up into 81 separate patches and so each patch is you
    know spatially just like the square there but the cool thing is that what token
    learner does here this thing here is it's a previous work from our group that
    takes in a bunch of possible you know image patches and dynamically selects which
    of those image patch tokens are more relevant for the tax at hand given the existing
    context so from those 81 image patch tokens we subsample eight of them to use
    for inference and this happens at every time then。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变换器是一个解码器，我们使用稀疏分类熵目标进行动作预测，通过应用因果掩码。我们使用预训练的高效网络骨干，并且还使用标记学习器以加快推理。深入一点，哦，对不起，有个问题。对的。非常好的问题。当图像标记从每个图像中进入时，你知道来自相机的高保真RGB图像，我们将其分割成81个独立补丁，所以每个补丁在空间上就像一个正方形，但有趣的是，标记学习器在这里做的是，这是我们组的一个先前工作，它接收一组可能的图像补丁，并动态选择哪些图像补丁标记在给定的上下文中更相关，因此从这81个图像补丁标记中，我们抽样出八个用于推理，这个过程在每次都发生。
- en: And that process has learned which of the eight patches are relevant at any
    given moment and otherwise we're sending in way too many tokens and the context
    length of explode and we wouldn't be able to do inference on robots we are also
    passing in a sequence length of six images history is quite important when you're
    doing temporal coherent tasks in the real world where things like physics and
    you know exactly this nuanced detail of what the objects are doing in relation
    to each other into to your robot those details really matter。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程已经学习到在任何给定时刻哪八个补丁是相关的，否则我们发送的标记太多，上下文长度会爆炸，我们将无法在机器人上进行推理，我们还传入六个图像的序列长度，历史在进行时间一致的现实世界任务时非常重要，因为物理等因素以及你知道对象之间的细微细节对你的机器人来说非常重要，这些细节确实很重要。
- en: And in total， the model size is 35 million parameters。which is quite a bit smaller
    than a lot of these other huge internet scale models。And finally one main difference
    here is action discretization before a lot of the products we were doing we' doing
    continuous control and if you think about it right our robot does have we do end
    effectector pose control position control and the real world is a continuous state
    space but and to do that we had to come up with many algorithmic novelties for
    example。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，模型的大小是3500万个参数，这比许多其他大型互联网规模模型要小得多。最后一个主要区别是动作离散化，在我们进行的许多产品中，我们是在进行连续控制。如果你仔细想想，我们的机器人确实有末端执行器姿态控制、位置控制，现实世界是一个连续状态空间，但为了做到这一点，我们必须提出许多算法新颖性。
- en: a CM actor that did basically sampling of these continuous action spaces to
    propose the highest ones that would get rated by the Q function and we do this
    fly blah。blah， blah， and but that's like so sensitive but we needed to get do
    that to get things to work。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一个CM演员基本上对这些连续动作空间进行采样，以提出那些将由Q函数评估的最高动作，我们这样飞来飞去。但是这非常敏感，但我们需要这样做才能让事情运转。
- en: but now we just decided let's just know bin our actions it's only 256 discrete
    actions and let's just predict those as tokens。Any question Yeah， what I was mentioning
    there。You have this design requirement or engineering requirement about speed
    and latancy reaction。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但是现在我们决定让我们的行为知道，只需256个离散动作，并将其预测为标记。有什么问题吗？我提到的是。你有关于速度和延迟反应的设计或工程要求。
- en: I mean when you say that that necessitates having relativelyly small model which
    makes sense。but important message of scaling when we're talking about foundation
    models is that built be bottleneck by either data compute four parameters so I
    guess what I'm curious to know is how do you balance these off in the sense that
    you want to have lots of parameters that have a really powerful model or on the
    other hand you want to have very fashion input。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我是说，当你说这需要相对较小的模型时，这是合理的。但是当我们谈论基础模型的扩展时，一个重要的信息是，它可能会被数据、计算或参数的瓶颈限制，所以我想知道的是，你如何平衡这些，因为你希望拥有大量参数以构建一个强大的模型，或者另一方面你希望输入非常时尚。
- en: Yeah， great question and to repeat it the question is we kind of set a pretty
    hard constraint with 100 millisecond inference time yet a lot of the lessons in
    foundation modeling is that you shouldn't be constraining yourself against any
    dimension。whether the status set size compute or model capacity and I think my
    initial answer to that is that's the very great point and something I think that's
    going to be coming up as a severe bottleneck in the future but for our initial
    case I think this is more of an exploration of whether these principles and even
    scaling well beyond what we were looking at now could work already this 35 million
    is gigantic compared to a lot of prior work using for example Resnet 34 or whatnot
    so this is already much bigger than you a lot of other options and maybe for now
    at least it's the easiest it's the largest scale we could go to roughly in the
    short term without having to think of more tricks I guess got。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，问题很棒，简单重复一下，问题是我们设置了一个100毫秒推理时间的严格限制，但基础建模中的许多经验教训是你不应该在任何维度上限制自己，无论是状态集大小、计算能力还是模型容量。我认为我最初的回答是这是一个非常好的观点，我认为这将在未来成为一个严重的瓶颈，但就我们的初步案例而言，我认为这更多是探索这些原则，甚至超出我们目前所考虑的规模是否可以有效，这3500万与许多之前的工作相比是巨大的，例如使用Resnet
    34等，因此这已经比许多其他选项大得多，或许至少在短期内这是我们能够达到的最大规模，而不必考虑更多技巧。
- en: Yeah， we can talk about a bit later， maybe I think I'd also love to hear your
    thoughts too because it's very non obvious how we can get past some of these bottlenecks。sort
    yeah， great question we ran some abls on model size， I might have that in a few
    slides。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们可以稍后再谈这个，或许我也想听听你的想法，因为如何突破这些瓶颈并不明显。是的，问题很棒，我们对模型大小进行了一些消融实验，我可能会在几张幻灯片中提到。
- en: but maybe we can return to that then and if not yeah but great question。So yeah
    that's the architecture and I'll discuss some of the ablation of the trends later
    on。but maybe you know this is a robotics lecture I should show you some pretty
    visuals right so let's look at some evaluations we did we compare against some
    baselines one is gotto which you might be familiar with and then other ones species0
    the reite based and we find that we evaluate on scene tasks versus unseen tasks
    and we also in various toer objects our normal data collect looks like this top
    left picture three cans on a great ask that's basically it but then we push it
    further by bringing in a lot more objects so that the table is so clutter that
    even as a human sometimes it it's hard to find the object that you're actually
    looking for we add in table class to make the textures very different we bring
    it to new micro kitchenitchs with these surfaces altogether and we find that Rt1
    is more robust than these other different methods。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 不过也许我们可以稍后再谈这个，如果没有，那也是个很好的问题。所以是的，这就是架构，我稍后会讨论一些趋势的消融实验。但也许你知道这是一个机器人讲座，我应该给你展示一些很漂亮的视觉效果，所以让我们看看我们做的一些评估，我们与一些基线进行比较，其中一个是你可能熟悉的gotto，还有其他基于reite的species0。我们发现我们在可见任务与不可见任务上进行评估，并且在各种物体上我们的正常数据收集看起来是这样的：左上角的图片是三罐在一个灰色的桌子上，但我们进一步推进，引入了更多物体，使得桌面混乱到即使是人类有时也很难找到你实际上在寻找的物体。我们增加了桌子类别，以使纹理非常不同，我们引入了新的微型厨房环境和这些表面在一起，我们发现Rt1比其他不同的方法更稳健。
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_9.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_9.png)'
- en: this dataGood question question was from the was the gotto model trained on
    our data or was it just already included in gotto the answer is this data was
    not included in gotto and so we retrained the gotto model only on our data。And
    yeah so here's just a different visualization of the robot going out in our micro
    kitchenitchen and doing different interesting things you can see here that it's
    trained on one setting。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据的好问题是，gotto模型是用我们的数据训练的，还是已经包含在gotto中？答案是这个数据没有包含在gotto中，所以我们仅用我们的数据重新训练了gotto模型。是的，这里是机器人在我们的微型厨房中执行各种有趣任务的不同可视化，你可以看到它是在一个设置上训练的。
- en: but then it goes into brand new kitchen brand new countertops new objects and
    it's able to do all of them pretty robustly we also put it into a long horizon
    setting using the Saan framework that we'll talk about next but in these settings
    a lot of them are mixing all of these generalization capabilities and plot on
    the left here we're using what we call generalization levels inspired by the VeEMA
    paper that would basically increasingly change more factors of variation simultaneously
    and here we found RT1 is the most robust。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 但随后它进入了全新的厨房，新的台面，新物体，并且能够相当稳健地处理所有这些。我们还把它放入了一个长期的设置中，使用我们稍后会谈到的Saan框架。在这些设置中，许多都是混合了所有这些泛化能力，左侧的图中我们使用了受VeEMA论文启发的所谓泛化水平，基本上是同时越来越多地改变更多的变异因素。在这里我们发现RT1是最稳健的。
- en: 就系好，你都可同力睇落去前度第个过去。Yeah good question we'll go into a bit more detail later
    but I think at a high level teleoperators get a structure templated command of
    like verb noun verb or something like like pick Coke can or move Apple near sponge
    and we have around 700 tasks set up this way and they go ahead and collect that
    data test done and then later we have we make sure that successes are actually
    successes and we discard stuff that's like unsafe for example。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样，你可以同样地去看前面的内容。是的，好问题，稍后我们会更详细地讨论，但我认为从高层次来看，远程操作员会获得一个结构化的模板命令，例如动词名词动词，或者像“捡起可乐罐”或“将苹果移近海绵”，我们大约设置了700个这样的任务，他们会继续收集数据，完成测试，然后我们确保成功的确是成功的，并且我们会丢弃那些不安全的内容，例如。
- en: Oh yeah， got it for this paper， we utilize 130，000 demonstrations for this。佢都个写翻。啊这个这。Is
    there any issue could in。Yeah great question I think a lot of prior work has also
    noted that when you have for example the question was did you find that the trajectories
    in your data were very multimodal and I think what you mean by that is that to
    go from point a to point A to point B I can go left or I can go right or I can
    go straight and I think this kind of diversity in basically for a single image
    state but yet my data has three possible labels that can have very bad effects
    sometimes for us I think because we are using teleoppper demonstrations the data
    was more homogenous than perhaps like in the while for example there's a type
    of data function called play data where operating just do whatever they want and
    we label in hindsight and I think our data is more homogenous in that but we did
    not find a lot of the issues that we've seen in prior projects one potential answer
    is maybe it's the architecture itself but we can talk about that later too that
    question。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，是的，明白了，对于这篇论文，我们利用了130,000个演示。它们都被写下来了。啊这个。有什么问题吗？是的，好的问题，我认为许多之前的工作也注意到，当你有例如，问题是你发现数据中的轨迹非常多模态吗？我认为你的意思是，从A点到B点我可以向左、向右或直行，而我认为这种多样性基本上对于单一图像状态来说，但我的数据有三个可能的标签，这在某些时候会产生非常糟糕的影响。我们认为，因为我们使用远程操作员的演示，数据比例如“玩数据”更为同质，后者让操作员随意操作，我们事后进行标记。我认为我们的数据在这方面更为同质，但我们没有发现很多在之前项目中遇到的问题，一个潜在的答案可能是架构本身，但我们稍后也可以讨论这个问题。
- en: 再到到叉市。Right。我好的。Great question we actually do have a termination action so the
    policy itself so the question was how do you determine when a episode is complete
    and the policy is able to predict terminate because at the end of the H teley
    operation session the operator can click a button and it's marked as the episode's
    done。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 回到叉市。对。我好的。好问题，我们实际上有一个终止动作，所以策略本身，问题是如何判断一个回合是否完成，策略能够预测终止，因为在H teley操作会话结束时，操作员可以点击一个按钮，标记为该回合已完成。
- en: think。Yeah， I think for these evaluations we were quite strict but definitely
    I think in some cases you know。maybe maybe if we're just doing an experiment for
    ourselves we'll have a dense reward scale of like grasp the object and move closer
    grasp the object and almost got there if I messed up at the end and we'll have
    like a grading curve basically。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我想。是的，我认为在这些评估中我们非常严格，但我确实认为在某些情况下，你知道，也许如果我们只是为自己做实验，我们会有一个密集的奖励尺度，比如抓取物体并移动更近，抓取物体并几乎成功，如果我在最后搞砸了，我们就会有一个评分曲线。
- en: but for all these all of these stats I'm showing here it was zero or one one
    fully complete zero。Was not fully complete。![](img/dde5df4844de2a3e55cc92f364263e12_11.png)
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但对于我在这里展示的所有这些统计数据，结果是零或一，一个完全完成，零。并没有完全完成。![](img/dde5df4844de2a3e55cc92f364263e12_11.png)
- en: 哦。And I think what was expecting exciting maybe talking about the multimodality
    aspect is then we pushed to limit even further and were we decided to train on
    very diverse data distributions your so right now you saw 1301000 demonstrations
    trained on this everyday robot proprietary mobile manipulator。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 哦。我认为让人期待的或许是关于多模态方面的讨论，然后我们进一步推动极限，我们决定在非常多样化的数据分布上进行训练。是的，你现在看到的是基于这个日常机器人专有移动操控器的1301000个演示。
- en: but we were also looking to train on very different data distributions with
    very different action distributions。very different trajectories， even very different
    visual objects tasks and to do that we included two other data sources one was
    simulation data which was kind of our robot but in SIim but it looked quite different
    and also this data was collected with reinforcement learning and not with teleoper
    demonstrations in the past with all the IL plus RL work that I mentioned we found
    that combined these two types of data was going to be very difficult because RL
    data has very short actions it's very quick that's very optimized for the specific
    reward function versus。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也在寻找在非常不同的数据分布上进行训练的机会，包括非常不同的动作分布、非常不同的轨迹，甚至非常不同的视觉对象任务。为此，我们包含了两个其他数据源，一个是模拟数据，这就像是我们的机器人在SIim中，但看起来非常不同，这些数据也是通过强化学习收集的，而不是通过过去提到的所有IL和RL工作中的遥控演示。我们发现结合这两种数据类型将会非常困难，因为RL数据的动作非常短促、迅速，并且针对特定的奖励函数进行了优化。
- en: Human collected teleoperationation data is a lot more human lab so to speak
    and finally we revived a data from many years ago of 2018 if you remember the
    Ka project。that Ar farm has not been operational in that state for many years
    now。but we had that data still and so we were hoping to see if a different robot
    with a different action space on different objects with different visuals in a
    different building could still be combined with data from this macro kitchenitchen
    data set that we trained on originally and what was very surprising to me is that
    RT1 was able to learn from all these very diverse data distributions I had never
    seen a result like this where any other architecture for example。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 人工收集的遥控数据更加“人性化”，可以这么说，最后我们恢复了几年前的2018年的数据，如果你还记得Ka项目。那时，Ar farm在那种状态下已经多年未运行。但我们仍然拥有这些数据，因此我们希望看看是否可以将一个不同动作空间的不同机器人与不同视觉的物体和不同建筑的数据与我们最初训练的宏观厨房数据集结合起来。令我感到非常惊讶的是，RT1能够从所有这些非常多样化的数据分布中学习，我从未见过这样的结果，任何其他架构也如此。
- en: a resnet or even the learning method like reinforcement learning could successfully
    learn on such different data distributions so robustly and we evaluated for example
    on combining concepts so we would have the original everyday robot robot pick
    up objects that were only seen in the Ka project or we would put objects only
    seen in simulation。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一个ResNet或甚至像强化学习这样的学习方法可以在如此不同的数据分布上成功学习得如此稳健，我们评估了例如概念的结合，因此我们会让原始的日常机器人拾取在Ka项目中仅见的物体，或者我们会放置在模拟中仅见的物体。
- en: And see if our policy could understand that so it did seem like it could generalize
    between objects had seen in other data sets and concepts that had seen in other
    data sets into the setting it was in now in the real microfi and that was a very
    result question action of the everyday robot。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 并观察我们的策略是否能理解，所以看起来它确实能够在之前的数据集看到的物体之间进行泛化，以及在现在的真实微型环境中看到的概念，这是日常机器人的一个非常重要的结果。
- en: Great question yeah we just tokenized it and made sure that the tokenization
    scheme was kind of interoperable and I think that was the I can dive that in a
    bit later too yeah yeah。And note that does not mean we can send the exact actions
    for one robot to another and have it execute it was more just like in the data
    set I think even by human inspect you can tell that these are coming from two
    different robots。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 很好的问题，是的，我们进行了标记化，并确保标记化方案是可互操作的，我想这就是要点，我稍后可以深入探讨一下。请注意，这并不意味着我们可以将一个机器人的确切动作发送给另一个机器人并让其执行，更像是在数据集中，甚至通过人工检查，你可以看出这些数据来自两个不同的机器人。
- en: 😡，So yeah let's look at some ablations for the scaling laws that we're all here
    for now we found that you know reducing data size reduces performance。but more
    interesting maybe is task diversity was quite important here we have two different
    trends。The green line is what happens when you reduce the total amount of episodes
    per task and then gray here the purple curve is for what happens when you reduce
    the total number of tasks when we found that having more tasks is relatively more
    important than having more data for each task。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 😡，所以是的，让我们来看看关于我们现在在这里讨论的缩放法则的一些消融实验。我们发现减少数据量确实会降低性能，但更有趣的是任务的多样性在这里非常重要，我们有两个不同的趋势。绿色线是减少每个任务的总剧集数量时发生的情况，紫色曲线是减少总任务数量时发生的情况。我们发现，拥有更多的任务在相对上比为每个任务拥有更多的数据更为重要。
- en: And I think this was a lesson that I think is probably going to suggest ways
    that you know we should scale robotics even further is not to just collect more
    data of the same task in the same settings。but to go out into the wild and get
    more diverse behavior。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得这是一课，我想这可能会建议我们应该进一步扩大机器人技术的方式，不仅仅是收集同一任务在相同设置下的更多数据，而是要走出实验室，获取更丰富的行为。
- en: questionI haven't defined diversity dataGreat question question is how do you
    define data diversity in this case it's just a number of unique structured templated
    commands that teleops receive so those 700 templated commands when you start reducing
    them and only train on 500 or only train on 300 of them performance drops much
    quicker than if we had take taken the same proportional cuts to the total method
    yeah so I guess'm curious。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 问题是我没有定义多样性数据。很好的问题，问题是你如何定义这个案例中的数据多样性，它仅仅是遥控操作接收到的独特结构化模板命令的数量，所以当你开始减少这700个模板命令，仅训练500个或300个时，性能下降的速度比我们对总体方法进行相同比例的削减时要快得多，所以我想我很好奇。
- en: 没要头。It that it see donmes and really to point very things。Yeah I don't think
    we the question was there seems to be almost a linear correlation between a data
    size and success rate and I think you know we could apply some fancy like you
    know scaling law you know try and curve fitting but we didn't look too much into
    that because you know this is a trend that we kind of expected we just weren't
    sure about the magnitude of how much it would affect us and I think。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 没有头绪。它看起来似乎有些混乱，真的很难抓住要点。是的，我觉得问题在于似乎数据大小和成功率之间几乎存在线性相关性，我想你知道我们可以应用一些复杂的，比如说缩放法则，尝试曲线拟合，但我们没有深入研究这一点，因为这是我们预期的趋势，我们只是不确定它会对我们造成多大的影响，我认为。
- en: I don't have any really good insights on this besides that we see this phenomenon
    empirically。ItThere should be one about。Yeah and great question so the question
    is oh maybe this will just go on indefinitely more is there something magical
    about you know January 2023 and I think this is maybe also this is when we start
    to conflate the algorithmics exploration with like practical considerations of
    scaling real world operations which was when we got up data our policies were
    hitting you saturating on these hitting close to 100% we're like all right let's
    collect another data set so we basically collect until it's at 100 and then we
    switch to something else but at this point what was interesting is that when we
    kind of bet really big on this Rt1 architecture we we'd already been collecting
    demos for a while so it was possible that we had collecting more than we needed
    and in some cases actually you could cut tasks without losing too much performance
    which was quite interesting but。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们在经验上看到了这种现象，我没有什么特别好的见解。嗯，问题是，也许这会无限继续下去，2023年1月有什么神奇之处。我觉得也许这就是我们开始将算法探索与现实操作的缩放实际考虑混淆的时候，正是在这时我们获得了数据，我们的策略接近100%的饱和度，我们就想“好吧，收集另一个数据集”，所以我们基本上收集到100%后就切换到其他任务，但有趣的是，当我们在这个Rt1架构上大下注时，我们已经收集了演示一段时间，因此我们有可能收集了超过所需的数据，在某些情况下，实际上可以在不损失太多性能的情况下削减任务，这非常有趣。
- en: Yeah。He says if this at all related the fact different task trajectory data
    might be more diverse among the different tasks but among multiple trajectoryor
    with the same tasks like if you just use like the same author for all the tasks
    or like saying there no fields like given。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。他说如果这与不同任务轨迹数据在不同任务之间可能更加多样化有关，但在多个轨迹中，如果你只是对所有任务使用相同的作者，或者说没有给定的领域的话。
- en: 朱类喺度要发过俾猪呢啊呢啊。Great question and the question is whether or not all tasks are
    created equal in terms of like their capacity and entropy for different behaviors
    you can learn from them and yeah that's definitely true some tasks are much easier
    we have a task that's just pick up this object that's going to have much less
    interesting stuff you can squeeze out of it than you know moving something into
    a drawer and then closing the drawer but yeah great question。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 朱类喺度要发过俾猪呢啊呢啊。这个问题很好，问题在于不同任务在能力和熵方面是否相等，确实有些任务要简单得多，比如“捡起这个物体”的任务，它所能挤出有趣内容的潜力远不如“把东西放进抽屉然后关上抽屉”，所以这个问题很不错。
- en: Great now， ablations， we also trained without the big model size。we did it without
    pretraining without you know with continuous set of discrete actions with autoregressive
    actions without history without the transformer。and I think all of these design
    choices did seem to be required for robust performance。Oh yeah。of course。Yeah。分正家成。Yeah
    I think all I mean like and again you know for paper writing it's kind of like
    the best thing that we can empirical find that's that's the method and then we'll
    figure out why each of these are important and so yeah I think what one surprising
    thing here perhaps was that autoaggressive actions hurt you might think that passing
    in more information is always better than passing in fewer fewer information but
    in this case maybe conditioning on your previous actions was kind of doing kind
    of like in context learning was doing online systems identification to figure
    out what。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，现在，消融实验，我们也在没有大型模型的情况下进行训练。我们没有预训练，使用了一系列连续的离散动作和自回归动作，没有历史，没有变压器。我认为所有这些设计选择似乎对稳健性能是必要的。哦，当然。是的。分正家成。是的，我认为所有的意思是，对于论文写作来说，这就是我们可以经验性找到的最佳方法，然后我们会弄清楚这些方法为何重要，所以我认为这里一个令人惊讶的事情也许是自回归动作会产生负面影响，你可能会认为提供更多信息总是比提供更少信息更好，但在这种情况下，也许对你先前的动作进行条件化就像进行上下文学习一样，进行在线系统识别以确定。
- en: Teleoc rateer this data came from and like how you can overfit to that specific
    set of action history and so removing that was actually better。One interesting
    tibit there。没。Cool then and maybe in the interest of time I'll try to get through
    the other ones a bit more quicker and then we can maybe just do few i'll just
    do the questions at the end if that's possible just so we have time to get through
    everything。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据来自哪里，以及你如何会过拟合到那一特定的动作历史，因此去除这些实际上更好。有趣的小插曲。没。很好，可能出于时间考虑，我会试着更快地完成其他的，然后我们也许可以在最后做一些问题，如果可能的话，这样我们就有时间处理所有内容。
- en: The next work here moving a bit away from skill learning then and actually onto
    the planning level。I think the first project took a lot of the design principles
    of other fields and this offline robot learning paradigm and put it into the skill
    learning so we actually bring that out to other parts of the robotic system and
    the first work here is Sacan if you remember here back in this timeline in 2022
    we started thinking about oh yeah how do we scale this multitaask im learning
    but at the same time large language models and other types of foundation models
    we really picking up steam whether it was Iogen or Do2 and we definitely wanted
    to figure out how we could use those as well we had come up with this RT21 design
    that we're betting big on but from here we started to explore how all of the better
    lesson 2。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的下一个工作稍微偏离技能学习，实际上进入了规划层面。我认为第一个项目借鉴了其他领域的设计原则以及这种离线机器人学习范式，并将其应用于技能学习，因此我们实际上将其推广到机器人系统的其他部分，第一项工作是Sacan，如果你还记得2022年的这个时间点，我们开始思考如何扩展这种多任务学习，同时大型语言模型和其他类型的基础模型也在迅速发展，无论是Iogen还是Do2，我们确实想找出如何利用它们。我们想出了这个RT21设计，我们对其寄予厚望，但从这里开始我们开始探索所有的更好的教训。
- en: 0 we could start utilizing foundation models within the context of our full
    stack system。The problem of doing this naively is that language models are not
    completely a very natural fit for robotics。for example， if you're a robot in a
    kitchen you ask a language model I still my drink what can you do language model
    will give you stuff that's not very relevant it's going to ask you to vacuum it
    it's going to ask you to call cleaner or it's going to apologize and these are
    not things that the robot can do in your kitchen with your spill drink to help
    you。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始在我们的全栈系统中利用基础模型。这样做的一个问题是，语言模型并不完全适合机器人。例如，如果你是一台厨房里的机器人，你问语言模型“我洒了饮料，我该怎么办”，语言模型会给出一些不太相关的建议，比如让你吸尘，或者叫清洁工，或者道歉，而这些都是机器人在厨房无法帮助你的事情。
- en: And so there are two parts of this then one issue is that our robots are limited。they
    are very constrained with what they can do， they cannot do everything but they
    can do certain things and then the second problem is that the language models
    are also constrained they don't know what the robot sees they don't understand
    that they are in a robot body in a micro kitchenitchen needing to do real stuff
    in the physical world and so we need to get the robots to speak language model
    language and then the language model to speak robot language to do this we present
    say can in the same setting you know please put a napple on the table we score
    the predictions of the language model on a constrained set of tasks that we know
    the robot has been trained to do and then we also take the affordance function
    from the robot an affordance function is a estimation of given some kind of state
    what the robot is able to do how confident it is that it can successfully accomplish
    that task in the given state in our case we use something like a value function
    from reinforcement learning。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这有两个部分，一个问题是我们的机器人是有限的。它们在能做的事情上受到很大限制，不能做所有事情，但可以做某些事情。第二个问题是语言模型也受到限制，它们不知道机器人看到的东西，不理解自己是在微型厨房的机器人身体中，需在物理世界中完成实际任务。因此，我们需要让机器人讲语言模型的语言，让语言模型讲机器人语言。为此，我们在相同的设置中提出，比如“请把一个苹果放在桌子上”，我们对语言模型在一组受限任务上的预测进行评分，这些任务是我们知道机器人经过训练能够完成的，同时我们也会从机器人中提取能力函数，能力函数是对给定状态下机器人能做什么的估计，以及在该状态下它成功完成该任务的信心。在我们的案例中，我们使用强化学习中的某种价值函数。
- en: Wch kind of encompasses this quality given these two values。these two scores
    we have the confidence from the language model and then the confidence from the
    robot we can combine these and then hopefully the combined prediction is both
    something that's going to be very sevantically relevant for the high-level instruction
    finding an apple is the first step in please put an apple on table but it's also
    something that the robot can do because no robot on the frame but it knows that
    it's been trained to find an apple so it can navigate around to find it and so
    hopefully you can do this then in closed loop and then keep on going and predicting
    a highlel plan from the language model that's grounded with the affordance function
    of what the robot understands。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种评分涵盖了这个特性。我们有来自语言模型的信心值和来自机器人的信心值，我们可以将这两个值结合起来，希望组合后的预测既能对高层次指令（比如“找到一个苹果是第一步，请把苹果放在桌子上”）有很大的语义相关性，又是机器人能做到的事情，因为机器人在框架中并不知道，但它知道自己被训练过去找到苹果，因此可以导航去寻找。希望你可以这样进行闭环，然后持续从语言模型预测出高层次的计划，并与机器人理解的能力函数相结合。
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_13.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_13.png)'
- en: There's a video here of the sick hand doing different stuff。but you know happy
    to share it later offline it's very cool trust me it's the greatest thing since
    sliced bread。😊。![](img/dde5df4844de2a3e55cc92f364263e12_15.png)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个视频，展示了机器手在做不同的事情。但我很乐意稍后线下分享，这非常酷，相信我，这是自切面包以来最伟大的东西。😊！[](img/dde5df4844de2a3e55cc92f364263e12_15.png)
- en: 好。没有异议。And yeah， some numbers then we tested this out on very long horizonor
    instructions encompassing more than 10 separate navigation and manipulation skills
    in the micro kitchenitchen that you see on the bottom right。we evaluated hundreds
    of different evaluations on this and we tested out very a lot of different concepts
    including things like refphrasing by using single prohibits by drawing instructions
    that just came from you know colleagues and friends。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 好。没有异议。是的，我们在非常长的指令测试中，这些指令包含超过10种单独的导航和操作技能，在右下角的微型厨房中。我们在这方面评估了数百个不同的评估，并测试了许多不同的概念，包括通过使用单一的禁止词重新表述，通过从同事和朋友那里得到的指令进行绘制。
- en: and then and we found that while they were failures in both the language model
    planning stuff side。where it would predict the wrong path for the current situation。as
    well as on the policy execution side， even when it gets a good plan the robot
    will mess up sometimes overall it was still doing quite well。And now let's kind
    of take this back to the lesson I think this is a very great example of how we
    can leverage Internet scale foundation models as they get better when we started
    the project we started with a language model calledFlan from Google throughout
    our implementation palm came online half language model and when that happened
    we were able to just hot swap it in and performance just kind of got better for
    free without us having to do anything by just assuming that language was the API
    the plan just has to be any string it can come from any source it can come from
    a human it can come from a language model when we improve that language model。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，在语言模型规划方面确实存在失败，模型会预测当前情况的错误路径，同时在策略执行方面，即便得到了一个好的计划，机器人有时也会搞砸，不过总体上它表现得还是相当不错。现在让我们回到这个教训，我认为这是一个很好的例子，展示了我们如何利用互联网规模的基础模型随着它们的改进而变得更好。当我们开始这个项目时，我们使用了谷歌的一个语言模型Flan，在我们实施期间，Palm这个半语言模型上线了，当时我们能够进行热插拔，性能在没有我们做任何事情的情况下就自然而然地提升了，前提是语言就是API，计划只需任何字符串，来源可以是人类，也可以是语言模型，当我们改进那个语言模型时。
- en: the system gets better overall and here you see with a scaling size is as the
    model LOM increased in size our planning performance got even better。And some
    cool tricks here to get it working well how did we actually produce this plan
    well just by prompting as is the ridge these days with chain of thought and with
    better prompting of just giving examples of here's some great robot plans now
    give me a new plan starting with this highlevel instruction we saw that the robot
    could do all things from understanding different languages to asking them to do
    very complex reasoning like hey give me something caffeinated or I don't do a
    caffeine anymore or get me something like you know better or I could bring me
    a healthy snack versus bring me a unhealthy snack so Kevin was able to reason
    through all of these。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 系统总体上变得更好，你可以看到随着模型LOM规模的增加，我们的计划性能也变得更出色。这里有一些很酷的技巧来使其运作良好，我们是如何生成这个计划的？其实就是通过提示，正如如今的思维链那样，通过更好的提示，给出一些优秀的机器人计划的例子，现在给我一个新的计划，从这个高层指令开始。我们发现机器人能够完成所有事情，从理解不同语言到要求它们进行非常复杂的推理，比如“嘿，给我一些含咖啡因的东西”或者“我不再喝咖啡了”或“给我一些更好的东西”或者“带给我一个健康的小吃”对比“带给我一个不健康的小吃”，所以凯文能够在这些方面进行推理。
- en: Let's you know I think that was our kind of the first contact of robotics with
    language models on our team and it was the first exploration into how these two
    worlds could overlap there was definitely still improvements so in our monologue
    we try to improve those further by bringing in vision language models。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我想这可能是我们团队首次将机器人技术与语言模型接触的机会，也是对这两个领域如何重叠的首次探索，确实仍有改进空间，因此在我们的独白中，我们尝试通过引入视觉语言模型进一步改善这些。
- en: The idea here is that you know we had very high plan rate success with Sacan
    but unfortunately it wasn't really able to recover from failures what I mean by
    that is that the language model would not really get updates of what was going
    on in the world so that if this was the plan proposed go to the table。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是，我们的计划成功率很高，但遗憾的是，它并不能真正从失败中恢复。我的意思是，语言模型并不能实时更新世界的情况，因此如果这是提出的计划，比如去桌子那边。
- en: pick up a Coke bring it to you but you messed up in the coca you dropped it
    on the floor it would still continue trying to bring it to you put it aside all
    of that does not really matter anymore because you dropped the cocan and so in
    this work in your monologue we are really hoping to figure out how you could add
    closed loop dynamic feedback from the environment into this planning process。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 拿起一罐可乐带给你，但你搞砸了，把可乐掉在地上，它仍然会继续试图把它带给你，把它放在一边，这一切都不再重要，因为你掉了可乐。因此，在你的独白中，我们真的希望弄清楚如何将环境的闭环动态反馈添加到这个规划过程中。
- en: Let's take that exact same example now instead of just directly correcting every
    instruction。maybe we add back some feedback from the scene also conveyed using
    language as the universal API here。the scene can tell you what's actually in them
    maybe the robot asks a question now in the robot this is a language model asking
    the clarification question maybe hear a human response or another language model
    then you can predict action the next task to do once the language model has enough
    context and maybe you even add in stuff like success detection and so on and so
    forth。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在拿那个确切的例子，而不是直接纠正每个指令。也许我们从场景中添加一些反馈，也通过语言作为通用API来传达。场景可以告诉你里面实际有什么，也许机器人现在会问一个问题，这里机器人是一个语言模型在询问澄清问题，也许人类会回答，或者另一个语言模型，然后你可以预测下一步要执行的任务，一旦语言模型有了足够的上下文，也许你甚至添加成功检测等内容。
- en: '![](img/dde5df4844de2a3e55cc92f364263e12_17.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dde5df4844de2a3e55cc92f364263e12_17.png)'
- en: How do we do this then well the first thing that we implement is what we call
    passive scene description just using either and off the shelf engineer heuristic
    using objectic detection models。something like vile， you can describe the scene
    and text and just convey all that context to the language model。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 那我们怎么做到这一点呢？首先，我们实现的是所谓的被动场景描述，仅仅使用现成的工程启发式方法，利用物体检测模型。像vile这样的东西，你可以用文本描述场景，并将所有上下文传达给语言模型。
- en: For active scene description， this is maybe similar to visual question answering
    if you're familiar with that field。the language model can actually propose active
    queries that it's curious about in the scene。maybe to make sure that has enough
    context to move on and here either a human can provide the answer or in the future
    a VQA model as they improve can provide that。And finally， for success detection，
    this is very important to allow the language model planner to know when to try
    to retce something here we take in the first and last image fine tune a clip success
    detector and use that to provide binary success failure information back to our
    language model。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主动场景描述，这可能类似于视觉问答，如果你熟悉这个领域的话。语言模型实际上可以提出它对场景中的积极查询，也许是为了确保它有足够的上下文继续前进。在这里，或者一个人可以提供答案，或者未来随着VQA模型的改进可以提供答案。最后，关于成功检测，这对于允许语言模型规划者知道何时尝试重新开始某件事非常重要。在这里，我们获取第一和最后一张图像，微调一个剪辑成功检测器，并使用它向我们的语言模型提供二进制成功与失败的信息。
- en: 嗯。![](img/dde5df4844de2a3e55cc92f364263e12_19.png)
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。![](img/dde5df4844de2a3e55cc92f364263e12_19.png)
- en: And for the results wise， we can see a very similar say can long horizon evaluation。but
    here we actually what's interesting is that we're able to and basically implement
    all these different automated feedback mechanisms on the robot and so that it's
    able to reason and recover from things here you see it's going to try to go to
    the table but the humans actually been saying hey。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在结果方面，我们可以看到非常相似的长期评估。但是这里有趣的是，我们实际上能够基本上在机器人上实现所有这些不同的自动反馈机制，因此它能够推理并从这里的事情中恢复。你会看到它试图去桌子，但人类实际上在说嘿。
- en: I change my mind and then it changes the human change its mind again ask to
    go back and forth and the robot's able to you know maybe we're kind of touch the
    language model at this point but the language also able to replan and you know
    make sure that the human intent is satisfied。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我改变主意，然后人类又改变主意，再次请求来回移动，而机器人能够做到这一点，你知道，也许我们此刻有点触及语言模型，但语言模型也能够重新规划，并确保人类的意图得到满足。
- en: We also tried， I'm not sure if this video shows it。but situations where we did
    adversarial inputs where I walked around and just kind of knocking objects out
    of the robot's hands and forcing the success detector to tell it。hey， you messed
    up， you know， try again。啊。And we also tried this out on a couple of different
    domains。a simulated tabletop manipulation domain as well as a real world manipulation
    domain。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也尝试了，我不确定这个视频是否展示了，但在我走来走去，敲打机器人的手，强迫成功检测器告诉它“嘿，你搞砸了，知道吗，重试一下。”时的对抗输入情况。啊。我们也在几个不同领域进行了测试，包括一个模拟的桌面操作领域和一个现实世界的操作领域。
- en: and we found that this was much better than SaanN or let's say just only using
    visual features themselves with something like clipport。And I think here it really
    speaks towards a trend that I've really come to appreciate in 2018 a robotics
    professor once said that when they look at all of the different things preventing
    robot learning from scaling tremendously。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现这比仅仅使用视觉特征（如clipport）要好得多。我认为这确实反映了一个趋势，我在2018年时非常欣赏过。一位机器人教授曾说，当他们查看所有阻碍机器人学习大规模扩展的不同因素时。
- en: it thought the bottleneck was high- levell semantic planning about reasoning
    about common sense and I think in 2022 and 2023 language models can provide a
    one path of how this can kind of be offloaded at least in the interim。and I think
    if language models are the API， then you can just bring in these vision language
    models as object detectors get better as success detectors as VQA as language
    models get better。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 它认为瓶颈在于高层次的语义规划，即关于常识推理的思考。我认为在2022年和2023年，语言模型可以提供一种路径，至少在过渡期间可以进行卸载。如果语言模型是API，那么你可以将这些视觉语言模型作为对象检测器引入，随着成功检测器、VQA和语言模型的提升。
- en: you can bring them all into the fold and the act is kind of a lifefest if your
    robot currently does not have common sense reasoning。these other models can act
    as a scaffold in a lifefest to bring you up to par with what they currently load
    and maybe then in the future you'll get beyond what the language models know but
    in the short term it does seem that we can leverage them to accelerate what we
    can do in the real world。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将它们全部纳入其中，这个行为有点像一个生命救助器，如果你的机器人当前没有常识推理能力。这些其他模型可以作为一个支架，帮助你赶上它们当前的能力，或许在未来你能超越语言模型的知识，但在短期内，我们似乎可以利用它们来加速我们在现实世界中的操作。
- en: Moving on now from we saw now how language models could do planning。we saw how
    vision language models could help planning。and now we're going to switch gears
    a bit and think about how vision language models can help other aspects of the
    bottlenecks that robot learning faces。One of these is that data collection is
    very expensive as we mentioned before， we did have this 130。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转到下一步，我们看到语言模型如何进行规划，看到视觉语言模型如何帮助规划。现在我们要稍微调整一下思路，考虑视觉语言模型如何帮助解决机器人学习面临的其他瓶颈。其中之一是数据收集非常昂贵，正如我们之前提到的，我们确实有这个130。
- en: 000 episode demonstration data set， but it was collected over a year and a half
    at significant cost both in resources in time and money and with many many robots
    and of course these tasks too were also。A bit limited right we use 700 very templated
    commands instructions that would give to teleopators because we knew that was
    this would scale right if we collected enough data for each of these templated
    tasks。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 000集演示数据集，但它是在一年半的时间内收集的，成本很高，包括资源、时间和金钱，涉及许多机器人，当然这些任务也有些限制。我们使用了700条非常模板化的命令指令给遥控操作员，因为我们知道如果我们为每个模板任务收集足够的数据，这样可以扩展。
- en: we could do that specific task and here's the flow that someone was asking about
    earlier we give this pick coA instruction the operator controls the robot in the
    real world finished the task marks the episode is terminate and then shade that
    out to this big orange data set and that big orange data set is so we trained
    on in all the previous projects for the control policies。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以完成这个特定任务，下面是之前有人提到的流程：我们给出这个pick coA指令，操作员在现实世界中控制机器人，完成任务，标记这一集为结束，然后将其划分到这个大型橙色数据集中，而这个大型橙色数据集是我们在所有之前项目中训练控制策略所使用的。
- en: What we initiallyly considered was adding a bit of crowdsourced hindsight annotation
    if you're familiar with it with a hindsight experience replay and reinforcement
    learning with goal conditioning with you know maybe the robot did something that
    wasn't just this highlevel templated instruction who could ask a human to describe
    more verly what the robot did maybe it picked up the coca that was on the right
    side of the table maybe it picked it up and then knocked it over maybe it moved
    it very slowly to the middle there's a lot of semantic diversity encompassed in
    this demonstration that that is not totally you caught by this highle templated
    pick coca instruction so we labeled 3% of this big orange data set with these
    very ver descriptions。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初考虑的是增加一些众包的回顾注释，如果你熟悉的话，包括回顾经验重放和强化学习，结合目标条件。也许机器人做了一些不仅仅是高层模板指令的事情，我们可以让人类更详细地描述机器人所做的事情，也许它拿起了桌子右侧的可乐，也许它把可乐拿起来后打翻了，也许它很慢地把可乐移动到中间。这一演示包含了大量语义多样性，而这些是高层模板“拿可乐”指令所无法完全捕捉到的。因此，我们用这些详细描述标记了这个大橙色数据集的3%。
- en: And next we kind of applied the pseudo labelbel str strategy that's been seen
    in other fields such as video pretraining with their inverse dynamics model。but
    instead we apply that to the instructions to the semantics of what's contained
    in your data set so step one。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们应用了一种伪标签策略，这种策略在其他领域（如视频预训练和其逆动力学模型）中已经被看到。但我们将其应用于数据集中所包含的语义说明，所以第一步。
- en: we pretrain a clip model on your small label data set of 3% of your main data。Then
    you go ahead and use that trainBLM data to label all of the templated instruction
    demonstrations that you had before that 130。000 episode dataset sets， now you
    have a relabel data which has a large diversity of interesting semantic instructions
    and then we plug in all of these data sets into RT1 and just train a language
    condition behavior cloning policy similarly to how we would and normallyally。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在你的小标签数据集（占你主要数据集的3%）上预训练了一个CLIP模型。然后，你可以使用该训练后的BLM数据来标记之前所有的模板指令演示，也就是那130,000个episode的数据集。现在你有了一个重新标记的数据，里面有大量有趣的语义指令，然后我们将所有这些数据集输入到RT1中，并像往常一样训练一个语言条件行为克隆策略。
- en: but even though normally we just use data B， the orange one， now we use all
    three dataset sets。And then finally we evaluate on entirely new unseen instructions
    in the prior works right we were evaluating mainly on the 700 templated instructions。but
    in this work we actually go beyond that we can type in you know almost anything
    you want that you think might succeed and you can phrase it how you can you can
    add typos you can even do it by referring to semanttic concepts you can add spatial
    concepts and we see how it does。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但尽管我们通常只使用数据集B，即橙色数据集，现在我们使用所有三个数据集。最后，我们在完全新的、未见过的指令上进行评估。在之前的工作中，我们主要是在700条模板指令上进行评估。但在这项工作中，我们实际上超越了这一点，我们可以输入几乎任何你认为可能成功的内容，并且可以用各种方式表达，你可以添加拼写错误，甚至可以通过参考语义概念来表达空间概念，我们会看看效果如何。
- en: The reason that this might work maybe visually to represent this is here are
    the TN embeddings on the left and the right it's the same embeddings but on the
    left they're colored by the original templated instruction that was used to collect
    that episode and on the right is what the vision language model thinks if it's
    allowed to put a freeform natural language caption and assign it to that episode
    you see that on the left you have these big clusters of pick coca is like you
    know hundreds or thousands of episodes but we all just call them pick coca well
    on the right then we can then expand those concepts and say actually this episode
    is picking up the red coca this episode is picking up the crumpled coca this is
    picking up the coca that's next to the chip bag and so you can get a lot more
    mileage out of the same underlying data set by just using language as the diversity
    mechanism through which you kind of expand the concepts that you're considering
    and for example in the middle you see you know open top drawer to become hold
    and pull out the top drawer。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能有效的原因是，这里是左侧的TN嵌入，右侧是相同的嵌入，但左侧是根据用于收集该情节的原始模板指令着色的，右侧是视觉语言模型认为如果允许它为该情节添加一个自由形式的自然语言标题，它会是什么。你会看到，左侧有这些大的“可口可乐”聚类，像是数百或数千个情节，但我们都称之为“可口可乐”。而在右侧，我们可以扩展这些概念，实际上这个情节是在选择红色的可口可乐，这个情节是在选择皱巴巴的可口可乐，这个是在选择靠近薯片袋的可口可乐。因此，你可以通过使用语言作为多样性机制，充分利用同一基础数据集，扩展你所考虑的概念。例如，在中间，你会看到“打开上面的抽屉”变成“抓住并拉出上面的抽屉”。
- en: We have stuff like the center left for for the middle episode for the bottom
    one pick green rice chips from whitepo becomes lift up the chip bag from the bowl
    and drop it at the bottom left corner of the table。so you had a lot of these semantic
    know spatial concepts that are now going to be in your target supervised labels。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有像“左中”的内容，对于底部的情节，“从白色碗中挑选绿色薯片”变成“从碗中抬起薯片袋并将其放在桌子的左下角”。所以你有很多这样的语义空间概念，现在将进入你的目标监督标签中。
- en: I a question。That time I be their perspective。I heard languages simply you have
    to that are。Yeah。Great question so I guess if I can rephrase a bit the problem
    is that like is actually a very difficult and perhaps even untract problem of
    how you map all the linguistic concepts you see out in the wild down to like maybe
    like embodied specific types of episodes and like。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我有一个问题。那时我站在他们的角度。我听到的语言似乎是你必须要的。是的，伟大的问题，所以我想如果我能稍微重新表述一下，问题在于，实际上这是一个非常困难甚至可能是不可解决的问题，如何将你在现实中看到的所有语言概念映射到一些具体的体现类型的情节上。
- en: Here maybe I would say is that we are definitely introducing a lot of our priors
    and our biases onto like maybe what we call as left you mean left 10 centimeters
    of two centimeters like like what do words mean and these definitions what do
    they mean to us to the cloudcomp raters that generated these caption what do they
    mean to the robot what do they mean to the language models maybe these are all
    slightly different but the hope is at least if they're roughly similar we'll get
    like directionally correct improvements so I would say the nuances of the specific
    hard lines of definitions and like actual like semantic meaning of these words
    I think that's maybe out of scope right now but maybe something will dive into
    further at a higher level though I think basically the bar is just so low we have
    these 700 template instructions that are basically one hot IDs and we just want
    to make those closer to natural language even if by a little and I think at least
    we're trying to get towards that with these vision language models that are。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我想说的是，我们无疑在引入很多我们的先验和偏见，比如我们称之为“左边”时，你是指左边10厘米还是2厘米，这些词语的定义对我们来说意味着什么，生成这些字幕的云计算评级者对它们的理解又是什么，它们对机器人意味着什么，对语言模型又意味着什么，可能这些都是略有不同的，但希望至少如果它们大致相似，我们就能得到方向上的正确改进。因此，我认为这些具体定义的细微差别，以及这些词语的实际语义，可能现在超出了范围，但或许在更高的层面上，我们会进一步探讨。不过，我觉得基本的标准实在是太低了，我们有700个模板指令，基本上是独热编码，我们只想让这些更接近自然语言，即使只是一点点，我认为我们至少在朝着这个方向努力，借助这些视觉语言模型。
- en: I hope that answers your question。And we also compared to a few baselines on
    the top left here。we look at what if we only train on this 3% of these fancy human
    rated labels。what if we only train on the original RT1 data set what if we train
    on both of these and what if we train on both of these plus all of the predictions
    given by our VLN and what's interesting here is that you know relabeling seas
    to universally help we evaluated only on novel instructions that was new for this
    project it's the first time on a robotics project where we only tested on something
    I could type whatever I thought I would type it in and that became the test set
    and we just had to make sure that it was never contained in the training coverage
    and you see all these interesting examples on the right here of stuff like move
    the lonely object to the others I have no idea how this worked stuff like lifting
    the yellow rectangle talking about colors talking about move the right apple to
    the left here we actually two apples in the scene and actually in our training
    demonstration。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能回答你的问题。我们还在左上角与几个基线进行了比较。我们考虑如果只在这3%的高端人类评定标签上进行训练会怎样？如果我们只在原始的RT1数据集上训练会怎样？如果我们同时在这两个上进行训练又会怎样？再加上我们的VLN提供的所有预测会怎样？有趣的是，你知道重新标记似乎是普遍有益的，我们只在这个项目中新颖的指令上进行了评估，这是机器人项目中第一次只在我可以输入的内容上进行测试，我输入的任何想法都成为了测试集，我们必须确保这些内容从未包含在训练覆盖中，你可以在右边看到这些有趣的例子，比如“将孤独的物体移到其他地方”，我完全不知道这怎么工作，像是提起黄色矩形，谈论颜色，以及“将右侧的苹果移到左边”，实际上场景中有两个苹果，并且在我们的训练演示中。
- en: Data we never collected scenes with duplicate objects just because you know
    we thought of this low pay modality problem if you just say pick cocan and these
    two cocans it's going to be very difficult to figure out which one to do but with
    language labeling it seems like maybe you could do that now so even though we
    never trained on scenes of two apples now you could evaluate on them and just
    specify with language which apple you want to go for and it was working pretty
    reasonably。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从未收集过包含重复物体的场景，因为我们考虑到这种低支付模态问题，如果你说“挑选可乐”，而这两个可乐就会很难确定该做哪个，但通过语言标签，看起来也许你现在可以做到，所以即使我们从未在有两个苹果的场景中进行训练，现在你可以在这些场景上进行评估，并仅通过语言指定你想选择哪个苹果，这实际上也运作得相当合理。
- en: And finally， for the last example here I thought it was kind of interesting
    a single coca we tried to do a novel behavior pushed towards the left was not
    a templated instruction we only had move coca near why where why is another object
    move coca near Apple move coca near sponge so pushing this motion of just pushing
    the coca into air essentially was not something that we ever encompass but maybe
    it was in one of the labels maybe like if you've seen like move coca near apple
    and the apples on the left and you saw move coca near sponge and the sponges on
    the left you would general the model can generalize and be like oh left means
    this side of the table not a specific object so maybe that's what's happening
    but it's very unclear this as I said you know just I thought of something I typed
    it and just solid happened and we definitely hope to explore this more quantitatively
    in the future bottom left of course is I think comparing against nonvisual augmentation
    so maybe you can also get these interesting concepts just from language alone。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个例子我觉得很有趣，我们尝试的单个可乐做了一个新颖的行为，推动到左侧，这不是一个模板化的指令，我们只是让可乐靠近其他物体，比如说可乐靠近苹果，靠近海绵，所以推动可乐进入空气的这个动作其实从未包含过，但也许在某个标签中有，比如说如果你看到“可乐靠近苹果”，而苹果在左边，再看到“可乐靠近海绵”，海绵也在左边，你可能会推测模型可以推广，理解“左”意味着桌子的一侧，而不是特定物体，所以这可能是发生的情况，但这并不明确。我只是想到了一些东西，打字就变成了实际情况，我们确实希望在未来更定量地探索这一点，左下角当然是，我认为与非视觉增强进行比较，所以也许你可以仅通过语言获得这些有趣的概念。
- en: Here we had adding random noise or we do Madlib style just swapping out words。or
    we'd even use a LOM GT3 in this case to propose rephrasings of existing instructions。but
    I think my takeaway there is that you really need visual grounding for the visual
    language model to say actually yeah this caption is factually accurate at this
    given point in time and that it's something perhaps that would be interesting
    for a robot。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们添加了随机噪声，或者用Madlib风格仅仅替换单词。我们甚至可以在这种情况下使用LOM GT3来提出现有指令的改写。但是我认为我的收获是，你确实需要视觉基础，以便视觉语言模型能够在某一时刻确实说出这个说明在事实上的准确性，而这可能是对机器人来说有趣的内容。
- en: that fine tuning process provides both of this。Yeah。不。喂。Yeah yeah definitely
    these are just some subsets of five of these evaluation instructions but we had
    over 60 of them we didn't do a full quantitative ablation。for example as we did
    in RT1 we had this like scene in unseen task set and that was compositional you
    would see you know move Coke near Apple and you would see move Apple near sponge
    but we'd hold out move Cokene Sponge and we'd test that out but in this case I
    think we can go much be more beyond that because our language is completely free
    form the compositional space of what you can kind of combine is just going to
    be much larger so we did try a little bit to answer question we tried some combinatorial
    evaluations but there's definitely a lot more thoroughness that we could do then
    too。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这个微调过程同时提供了这两点。是的。不。喂。是的，确实这些只是五个评估指令的某些子集，但我们有超过60个，我们没有进行完整的定量消融。例如，正如我们在RT1中所做的那样，我们有一个未见任务集的场景，这个场景是组合性的，你会看到，比如将可乐移动到苹果附近，或者将苹果移动到海绵附近，但我们会保留将可乐移动到海绵附近，并对此进行测试。但是在这种情况下，我认为我们可以更进一步，因为我们的语言是完全自由形式的，组合空间会更大，因此我们确实尝试了一些组合评估，但我们可以做得更深入。
- en: How am I doing on time Okay10 minutes maybe i'll try to wrap up pretty soon
    then the dial of' takeaway then is that two parts right lesson two leverage foundation
    models excuse use them as data augmentation and lesson three let's make sure that
    our offline data set you know is robust enough where these different behaviors
    exist and you can describe them in language if you don't have enough diverse behaviors
    no matter how good your labeling is you probably can't elicit all the interesting
    concepts that you want to learn from。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我在时间上怎么样？好吧，可能10分钟，我会尽量快点结束。那么，结论是两个部分，对吧？第一课是利用基础模型，第二课是将它们作为数据增强，第三课是确保我们的离线数据集足够强大，以便这些不同的行为存在，并且你可以用语言描述它们。如果你没有足够多样化的行为，无论你的标注多么好，你可能无法引导出所有有趣的概念。
- en: And maybe most exciting for me here was that actually some label noise is okay
    notoriously in supervised learning and imitation learning you need very clean
    labels that are always 100% true right you don't want to be learning from like
    noisy data where some like large percentage is just not accurate but in our case
    it seems that like some label noise was okay the vision language models was not
    always predicting factually accurate descriptions of the scene and I think this
    definitely hurt when it got too high the noise but at smaller levels it definitely
    still seemed to be okay and robust enough to handle that。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对我来说，也许最令人兴奋的是，实际上一些标签噪声是可以接受的。在监督学习和模仿学习中，通常需要非常干净的标签，总是100%真实的。你不想从一些大比例不准确的噪声数据中学习，但在我们的案例中，似乎一些标签噪声是可以的，视觉语言模型并不总是准确预测场景的描述，我认为当噪声过高时确实会造成伤害，但在较低水平上，它似乎仍然可以处理得很好且足够稳健。
- en: So that was a deep dive then on some individual works that use this big recipe
    of language。foundation models， off data sets in different parts of the robot system。And
    this was the kind of pitch at the beginning， and I hope you at least see a little
    bit of how our team has tried to take these principles and apply them to accelerating
    robot learning in the real world。![](img/dde5df4844de2a3e55cc92f364263e12_21.png)
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是对一些使用这种大型语言配方的个别作品的深入探讨。基础模型，来自机器人系统不同部分的数据集。这是开始时的那种介绍，我希望你们至少能看到我们的团队如何尝试将这些原则应用于加速现实世界中的机器人学习。![](img/dde5df4844de2a3e55cc92f364263e12_21.png)
- en: As we see these different types of ingredients and lessons map onto different
    parts of the robot system altogether for skill learning right that was RQ1 that
    we talked about for planning that was they can and then adding the close feedback
    with vision language models that was in monologue for low-lel control we didn't
    talk about this today but an exciting work from our team is actually using language
    models to predict code that's executed on the robot directly perhaps this lowlevel
    controllers language models you know they read textbooks they read they've read
    Rossstockocs they've read you know you are5 documentation code and they can write
    code for these robots and we can execute that。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们看到这些不同类型的成分和课程如何映射到机器人系统的不同部分，以实现技能学习，这就是我们之前讨论的RQ1，关于规划的部分。他们可以通过与视觉语言模型的紧密反馈来实现，这在低级控制的独白中没有涉及，但我们团队的一项令人兴奋的工作实际上是使用语言模型来预测在机器人上直接执行的代码。也许这些低级控制器的语言模型，他们阅读教科书，阅读过Ross等人的文献，阅读过您所提到的文档代码，并能够为这些机器人编写代码，我们可以执行这些代码。
- en: For data documentation， we saw dial with vision language models and also I didn'
    talk about this here。but for object centric representations for things like feature
    activation map for specific objects。we can use those as task representation for
    mapping a scene and in NL map they did that for object object centric navigation
    around the microfi that we looked at。And I think hopefully in the next you know
    coming weeks and months。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数据文档，我们看到了与视觉语言模型的对话，尽管我在这里没有讨论。但对于以对象为中心的表示，比如特定对象的特征激活图，我们可以将其用作任务表示来映射场景。在NL地图中，他们为我们所研究的微型导航进行了对象中心的导航。我希望在接下来的几周和几个月中。
- en: we have a few more rows and entries to add here as well。but I think this kind
    of mindsetet is a very exciting research direction of how you can apply these
    big high-level concepts about foundation models and offline data sets and you
    look at what exists in the robot systems up today and you find many gaps and opportunities
    still available where we can do everything from exploratory pilots on how this
    might look like all the way to more extensive evaluations and really building
    out robust systems。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有更多的行和条目可以添加。但我认为这种心态是一种非常令人兴奋的研究方向，关于如何应用这些关于基础模型和离线数据集的高层概念，看看目前机器人系统中存在的内容，发现许多仍然可用的空白和机会，我们可以从探索性试点到更广泛的评估，真正构建出强大的系统。
- en: I think both of these high value。So I'll conclude with just saying that it was
    very fun exploring all of these complementary directions。but there are still some
    major questions of how we can take these concepts even further and how these trends
    and ideas might even evolve moving forward as foundation models get better as
    more data becomes available online as more data becomes homogenized and tokenized
    and interoperable and I think a lot of the concepts from other fields like linguistics
    and vision and from you know all of the big scaling kind of level questions that
    are being pioneered in languagebased foundation models。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这两者都具有很高的价值。所以我想总结一下，探索所有这些互补的方向非常有趣。但仍然有一些重大问题，关于我们如何能将这些概念进一步发展，以及随着基础模型的改善、在线数据的可用性增加、更多数据的同质化和标记化、互操作性提高，这些趋势和想法可能如何演变。我认为来自其他领域的许多概念，例如语言学和视觉，以及所有大规模扩展相关的问题，都在语言基础模型中得到开创。
- en: hopefully those kind of ideas can trickle down to robotics。maybe even robotics
    can provide something back by providing embodied action。causal datas that maybe
    might improve the quality of reasoning of some of these large language models
    that are not embodied with that though I guess I'd like to thank everyone for
    your time and for David and for providing me and open to any questions about papers
    or just at a high level as well。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这些想法能够渗透到机器人技术中。也许机器人技术甚至可以通过提供具体的行动来回馈，提供因果数据，可能会改善一些没有具体体现的这些大型语言模型的推理质量。对此，我想感谢大家的时间，感谢David提供的支持，并对论文或任何高层次的问题持开放态度。
- en: Thanks so much。嗯。嗯。8是。I。等一下啊。诶。But。你买打。Yeah great question so the question I
    guess is like what about tasks that require more semantic reasoning like you know
    operating at a certain speed or with maybe like I don't know numerical reasoning
    within the question of prompt itself I would say so for a lot of the more common
    sense reasoning like you know throw away three cocans know after another I think
    you know the language model is very good at that right now so for the saycan planner
    it will predict you know throw away the coca three separate times for the low-level
    skilled policy learning though I think that's more of a that's more high variance
    I would say and definitely for right now we don't really condition on speed or
    how you do exactly but that's definitely maybe something that I could do if you
    could relabel with like pick up the coca slowly versus pick up the coca quickly
    maybe that is something a vision language model could recognize。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 非常感谢。嗯。嗯。8是。我。等一下啊。诶。但。你买打。是的，问得很好，所以我想这个问题是关于需要更多语义推理的任务，比如说以某种速度操作，或者可能是在提示本身的问题中涉及的数字推理。我会说，对于很多常识推理，比如你知道连续丢弃三瓶可乐，我认为语言模型现在对此非常擅长。因此，对于saycan规划器，它会预测连续丢弃三次可乐。而对于低级技能策略学习，我认为这更具高方差。我会说，目前我们并没有真正根据速度或具体操作进行条件设置，但这可能是我能做到的，如果你能重新标记，比如慢慢捡起可乐与快速捡起可乐，这可能是视觉语言模型能够识别的。
- en: 检是。我今日唔佢啲人你使过。How many tutorials in evaluation so if we have data two tasks？
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 检查。我今天不太认识那些人。评估中有多少个教程，如果我们有两个任务的数据？
- en: Ping something because that one thing only been thought that I the right talk
    only then thought I pick up government law。II want do that。Great question the
    question was at what scale do we see like combinatorial generalization start to
    occur maybe between like you've seen colors of one block and then you want to
    evaluate on a new color and I think that's a great question and unfortunately
    my answer is gonna to be very vague it depends it depends on how you define your
    tasks it depends on the scale of your data and it depends on like the concept
    that you're trying to generalize across I think there have been a numerous attempts
    to kind of basically formalize what it means to generalize within learning and
    within robotics even within like the specific settings we consider and I don't
    think there are any clear trends like of where you can say oh yeah。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 提到某件事，因为那只有在我正确谈论的情况下才会想到，我想拾起政府法律。我想做到这一点。很好的问题，问题是我们在什么规模上看到组合泛化开始发生，可能是在你看到一个方块的颜色，然后想要在新颜色上进行评估。我认为这是个很好的问题，不幸的是我的回答会非常模糊，这取决于你如何定义你的任务，取决于你数据的规模，也取决于你试图跨越的概念。我认为已经有很多尝试基本上在学习和机器人领域中正式化泛化的意义，即使在我们考虑的特定设置中，我认为没有明确的趋势，比如你可以说哦，是的。
- en: this is the number I need to hit where you know I can generalize across X Yz
    dimensions like you could evaluate all of those but I don't think it will help
    you predict new trends at least right now I think we're probably this is just
    me talking I would say we' one order of magnitude off before we can start to make
    very broadly generalizing statements about。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我需要达到的数字，你知道我可以在X Y Z维度上进行泛化，比如你可以评估所有这些，但我认为这并不会帮助你预测新趋势，至少在现在。我认为我们可能还差一个数量级，才能开始进行非常广泛的泛化陈述。
- en: Generalization capabilities， I think you know add one to two more zeros to our
    data set size and we can start to talk about that in terms of task object skills。Yeah。嗯。你怎么了。嗯。So
    say you have to please specify the inspection。And notice in vaccines。So we发个你系。Yeah。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化能力，我认为你知道，将我们的数据集规模增加一到两个零，我们就可以开始在任务对象技能方面谈论这一点。是的。嗯。你怎么了。嗯。所以请你指定检查的内容。并注意疫苗。所以我们发个你系。是的。
- en: going extend。对。Yeah， very astute observation so the question was that in Sacan
    the value functions that predict these scrs on the right here for the affordances
    are only storing a certain limited number of tasks so is that the bottleneck and
    I would say yes 100% scaling the number of tasks that your system is able to do
    that you can then give to the planner as it's buffet of options to choose that
    is the bottleneck right no matter how good your planner is if you can only do
    like three tasks there's only certain like combinations of those three tasks that
    it can do to you know map onto a highlel instruction so as you add more tasks
    as the lowlel skill capabilities of your robot increases you're kind of like adding
    precision to like the coverage of the highlevel instructions that your robot can
    try to do so that's one of the main bottlenecks I see today。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 正在扩展。对。是的，非常敏锐的观察，问题是，在Sacan中，预测这些右侧的价值函数所存储的功能只限于某些特定的任务，因此这是否是瓶颈，我会说是的，100%你系统能够执行的任务数量的扩展，能够提供给规划者作为选择的自助餐确实是瓶颈。不管你的规划者多么优秀，如果你只能完成三项任务，那么只能对这三项任务的某些组合进行映射以符合高级指令。因此，随着更多任务的增加，机器人低级技能能力的提升，你实际上是在为机器人能够尝试执行的高级指令的覆盖添加精确度。这是我今天看到的主要瓶颈之一。
- en: 对。放系到英点咩。Great question so have we tried RT1 with RLHF or with RL I think。The
    short answer is I think we have some stuff in the works that is doing that but
    right now for all of our projects currently we're just using this implementation
    learning loss again I think I view this Mohead bet that we're making as kind of
    an existence proof it works it's not cheap but it kind of does work and it does
    scale and that at least is a good starting point and our main you know hope over
    the next months and years is can we improve beyond that can we add back in offline
    improvement you know can we add in our L back to the equation somehow I'm an R
    person at heart so I really hope so。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对。放到英文点吗。很好的问题，我们是否尝试过用RLHF或RL的RT1。我认为简短的答案是我们有一些正在进行的项目，但目前所有项目仍然使用这种实现学习损失。我认为我把这个Mohead赌注视为一种存在证明，它确实有效，虽然不便宜，但确实有效并且可以扩展，这至少是一个良好的起点。我们在接下来的几个月和几年里的主要希望是，能否在此基础上有所改善，是否能将离线改进加入方程中，我是个R的人，所以我真的希望如此。
- en: 好啊啊。having。法听你份。So I could repeat that。好。嗰个生日己睇唔啲得。Do we or if the event be
    different。Go。Yeah good question so regarding task balance and whether like text
    only data is sufficient for helping like motor control learning I think my hope
    is that when you know models when we experience emergence in both the robotic
    space and we've already seen emergence in the language space at some point maybe
    these reasoning concepts will start to transfer between the two I would point
    them to one interesting paper which is I think can Wikipedia help reinforcement
    learning from Shane and some other folks they pretrain you know a large policy
    network on like you know autoaggressive token prediction on Wikipedia just text
    only and they use that to initialize like control for Atari games and this actually
    help so you know maybe this is philosophical but maybe there's something about
    decision making reasoning that transfers between text and action data so。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 好啊。听到你的意见。让我重复一下。好。那个生日己看得不太好。我们会有不同的事件吗？走吧。是的，好的问题，关于任务平衡以及仅用文本数据是否足够帮助运动控制学习，我的希望是，当模型在机器人领域经历涌现时，我们已经在语言领域看到涌现，或许这些推理概念会在两者之间开始转移。我会提到一篇有趣的论文，我认为《维基百科能否帮助强化学习》，由Shane和其他人撰写，他们在维基百科上进行了大规模的策略网络预训练，仅使用文本，并用其初始化对Atari游戏的控制，这确实有效。所以，也许这是哲学上的问题，但也许在文本与行动数据之间有某种决策推理的转移。
- en: 对。好好。question for this past images。having with。Great question I definitely agree
    you know passing in six images is not going to be enough when you're executing
    tasks for minutes at a time like clean my whole house and then you can only pass
    in the last like you know two seconds like come on so I think that's definitely
    going to be a limitation as our tasks at more complex and long horizon and I think
    here it's another open question too is context length we have high dimensional
    images even with token learning for reducing the number of patches that we pass
    through it's still you know very high dimensional and we quickly hit the context
    length cap can we do how do we you improve it beyond this maybe it's like retrieval
    transformers or some other kind of mechanism。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 对。好好。这是关于过去图像的问题。确实是个好问题，我绝对同意，你知道，传递六幅图像在执行任务时是不够的，比如清理我的整个房子，而你只能在最后的两秒钟内传递，真是让人失望。所以我认为这肯定会成为一个限制，尤其是当我们的任务变得更加复杂和长期时。我觉得这里还有另一个悬而未决的问题是上下文长度。我们有高维图像，即使通过标记学习来减少我们传递的补丁数量，它仍然是非常高维的，我们很快就会达到上下文长度的上限。我们能否在此基础上进行改进，或许是检索变换器或其他机制。
- en: 哦这就是啊一审这这边现在个这个的。Great question I think we are hoping to explore that in the
    future。but with this like context length limitationation we are already near the
    context length capacity with just the six images alone much less you know passing
    in whole trajectories of zero shot behavior few shot behavior we wish to see so2
    yeah。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，这就是啊，审查的。这边现在有这个。真是个好问题，我认为我们希望在未来探索这个。但是由于上下文长度的限制，我们已经接近上下文长度的容量，仅仅是六幅图像，更不用说传递整个零样本行为或少样本行为的轨迹了，所以是的。
- en: 好的对。好的，听到。Cool， thank you guys。![](img/dde5df4844de2a3e55cc92f364263e12_23.png)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，对。好的，听到这个。很酷，谢谢大家。![](img/dde5df4844de2a3e55cc92f364263e12_23.png)
