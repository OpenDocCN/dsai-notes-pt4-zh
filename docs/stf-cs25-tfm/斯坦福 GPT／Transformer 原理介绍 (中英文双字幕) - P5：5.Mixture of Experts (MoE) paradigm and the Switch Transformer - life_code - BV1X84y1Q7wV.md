# ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P5Ôºö5.Mixture of Experts (MoE) paradigm and the Switch Transformer - life_code - BV1X84y1Q7wV

![](img/cfb06f02d43700aca0402fcc8c4854ad_0.png)

TodayÔºå Erwin and I are going to be giving a talk on scaling transformers through sparsity and the kind of sparsity we're to be talking about today is the kind where„ÄÇ

 you know each input can get you knowÔºå either a different set of weights or have a different amount of computation applied to it„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_2.png)

![](img/cfb06f02d43700aca0402fcc8c4854ad_3.png)

Orren you want to start it offÔºüY„ÄÇSo I guess the overall motivation for this line of work is that„ÄÇ

You knowÔºå the community has kind of realized that scale is perhaps one of the most important access to to focus on for obtaining strong performance and there's almost like this sort of ongoing arms race right now with different labs and different institutions„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_5.png)

Sort of competing for training the largest models„ÄÇAnd so maybe these dates back from early 2020 with a paper from Open AI called Scing Gs for No languageage Model„ÄÇ

Where they find that model performance„ÄÇFollows the predictable our law„ÄÇ

Scas sort of as a power with model size„ÄÇIn terms of either computeÔºå also just you knowÔºå parameters„ÄÇ

And so this scaling law kind of generalizes over multiple orderss of magnitude and that gives us the confidence that if we are to train very large models„ÄÇ

 we can expect solid performance just by extrapolating these scaling laws„ÄÇSo in that paper„ÄÇ

 they also„ÄÇFind the interesting„ÄÇObservation that„ÄÇBasicallyÔºå larger models are more simple efficient„ÄÇ

And soÔºå you knowÔºå if you have a fixed compute budget„ÄÇYou can sort of„ÄÇYou know„ÄÇ

 you can predict what what is the sizeÔºå what is the optimal model size or fixed compute budget„ÄÇ

And the overall observation is that„ÄÇYou knowÔºå you rather train very large models for less steps than train smaller models for more training steps„ÄÇ

And so these models are scaled„ÄÇYou knowÔºå through„ÄÇBasicallyÔºå the paper focuses on dense modelsÔºå right„ÄÇ

 where you just increase the model dimensions„ÄÇThey're not looking at sparsity and so sparsity is a new dimension that you can use to scale architectures„ÄÇ

You knowÔºå N DC sort of the„ÄÇThe focus of the talk„ÄÇAnd so the spaity we we're mentioning here is basically„ÄÇ

 you will have„ÄÇSpa spaly activated weights based on the network inputs„ÄÇ

 So every input will go to a roughly similar amount of computation„ÄÇ

 but will be applied different weights„ÄÇAnd so this dates back to 1991 with„ÄÇ

Paper called adaptive Mis of local exports„ÄÇAnd was recently revisited by Noam Shaesar and colleagues at Google Brain„ÄÇ

With LSTMs where they're replaced sort of the fit forward„ÄÇNetworks and LSTMs with mixture of exp„ÄÇ

And so the way this works there roughly is that„ÄÇYou will have multiple experts in implementing you know„ÄÇ

 a small network or in that caseÔºå I think just a„ÄÇD matrix multiplication„ÄÇ

And so you have an additional getting network shown in green here that outputs probably distribution over„ÄÇ

Experts that each talking should be sent to„ÄÇSo this priority distribution is computed as a softm„ÄÇ

 and once you have itÔºå you select a few experts„ÄÇSo the are different strategies„ÄÇ

 maybe we'll talk about it laterÔºå and the output is simply sort of the way to make sure of all selected export outputs„ÄÇ

So that they've been pretty successful in„ÄÇPrimarily in translationÔºå but there was someÔºå you know„ÄÇ

You knowÔºå some complexities that Hindu broader use in NLP„ÄÇAnd so the switch transformer paper„ÄÇ

Addresses some of those and will be discussing how to„ÄÇ

 you know how to fix training abilities or reduce communication costs and and reduce model complexity„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_7.png)

AlrightÔºå do rightÔºå you wantan to go„ÄÇ

![](img/cfb06f02d43700aca0402fcc8c4854ad_9.png)

YeahÔºå so yeah„ÄÇ So one kind of approach that we're going to have first spaarsity is the switch transformer„ÄÇ

 which is kind of like a simplified mixture of expert variantÔºå along with some other improved„ÄÇ

 you knowÔºå training and fine tuning techniques that allow it toÔºå you know„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_11.png)

![](img/cfb06f02d43700aca0402fcc8c4854ad_12.png)

Be stably trained and also perform better when finding done a lot of downstream tasks„ÄÇAnd so yeah„ÄÇ

 so the switch transformer kind of model works as the following„ÄÇ

 so you have some transformer model that hasÔºå you know self attention and feed forward layers and the idea is that we replace maybe one every two or one every four feed forward layers with a switch transformer layer„ÄÇ

So you can see on the left is like one kind of layer block which is selfatten then addized then a fee forward layer„ÄÇ

 then abnormalize and in this caseÔºå we're replacing the normal feed forward layer with like switch layer and we can see an illustration of this on the right so on the right we can see that the layer has two inputs one is the token more„ÄÇ

 the other is the token parameters and we can see that these you know embedding representations will get sent to a router which is exactly how it works in the mixture of expert so the router is basically just going to be you know getting a distribution over all of the experts so in this case we can see that the highest probability is going to the expert number two out of the four experts and then the right token is actually having the most probability on the first feed forward weight which is like the first expert„ÄÇ

So yeah we can see here that like what we're going to do is in the switch transformer which is very simple is just send it to the highest probability expert and so here we can see where the adaptive computation lies where we'll have four sets of weights there's some shared weights and computation across all tokens„ÄÇ

 for exampleÔºå the self attention layer is computed exactly the same for the more token and for the parameters token„ÄÇ

But in the sparse switch layerÔºå we can see that like actually the inputs are while having the same amount of floating point operations applied to them„ÄÇ

 actually have different weight matrices„ÄÇ

![](img/cfb06f02d43700aca0402fcc8c4854ad_14.png)

Next slide„ÄÇYeahÔºå so that's the kind of high level idea with switch transformformer is thatÔºå you know„ÄÇ

 instead of sending a token to multiple different experts„ÄÇ

 which can also increase the communication costsÔºå as I'll go into a little bit later„ÄÇ

 it also just like significantly kind of simplifies the algorithm by just only sending it to one expert„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_16.png)

So for the improved training methodologyÔºå we focused on three different things to help improve the training of sparse models„ÄÇ

The first was selected precisionÔºå which like allows these sparse models to be trained in lower precision formats„ÄÇ

 which is incredibly important Most of the models we train„ÄÇ

 we really don't want to be using F 32 because it's just slower to compute and also when you're communicating tenss across different processes and stuff„ÄÇ

 its is twice as slow just because there's twice as many things„ÄÇAlso„ÄÇ

 we have some initialization tricks and some training tricks as well for allowing them to be trained more stably„ÄÇ

 especially as the models grow in sizeÔºå which is like a new initialization method along with like a change to the learning rate schedule„ÄÇ

And thirdÔºå since that our models have so many more parameters„ÄÇ

 we do notice like definitely different overfiing dynamics„ÄÇ

 especially once we finet these models that have been pretrained on all of the internet on these small tasks with maybe only 50 to 100„ÄÇ

000 examples that they can be much more prone to overfitting so we also look at some custom you regularization to help prevent some of the overfitting that we observe„ÄÇ

And finallyÔºå we also talk about this differentiable load balancing technique we make„ÄÇ

Which kind of allows you know each expert to roughly get the same amount of tokens„ÄÇ

Because you know this is very importantÔºå especially given that we you know want the stuff to be efficient on hardware„ÄÇ

 we want roughly each expert to have similar amounts of token sent to it and so to kind of encourage this we tack on an additional like load balancing loss along with our cross entrytropy loss that we're training with„ÄÇ

Next slide Okay so here I'm going to go into selected precision„ÄÇ So yeah again„ÄÇ

 so like when we're training large modelsÔºå it's really important that we should be able to train them in lower precision formats„ÄÇ

 So instead of each you know weight being an activation being 32 bits we want to shrink it down to 16 bits and we use like the B float 16 representation and what we found out of the gate is that you know these models are just unstable especially dis sparse models are much more unstable than the dense models in terms of like you'll train it for 1020000 steps and then the losses would just diverge this was something that we you know frequently encountered and so one key thing that we found is that basically you need to be casting a part of the computation in float 32 for these models to be able to be trained stably„ÄÇ

And the key component that we found that you need to cast is the like router computation and essentially you know we can go into the technical details a little bit more later„ÄÇ

 but basically anytime that there's like these exponentiation functions„ÄÇ

 it's very important that we are you having higher and higher precision because of roundoff errors that can then drastically change the output of some kind of exponentiation function So for example„ÄÇ

 like if you have an exponentiation function and you change it by 0„ÄÇ1 or 02 or 0„ÄÇ3„ÄÇ

 this can drastically change the output of like exponentiating it„ÄÇ

 especially depending on how large the input is„ÄÇSo yeah„ÄÇ

 so this was like a very important thing and it basically doesn't change the compute at all and allows the models to just be significantly more stable„ÄÇ

Next slide„ÄÇSo the second thing we looked at is also the initialization scale„ÄÇ

 so like the standard way that we were initializing these models we found to also just make the models much more prone to being unstable and or just performing worse so one thing that we did that we found was very effective was to just simply make the initialization scale much smaller and when we did this we found that you know the quality just like drastically improved it was like a very simple fix„ÄÇ

Next slide„ÄÇAnd the third thing I mentioned where since we notice that these models are much more prone to overfitting„ÄÇ

 since they just have significantly more parameters„ÄÇ

Is that we also use much more dropout for the expert layers only so here we can see we took we have like you know the T5 base which is a dense model„ÄÇ

 and then we have a bunch of different switch variants on that and we found to be the most effective on these four different fine tuning tasks was just to really significantly increase the dropout rate inside the expert layers and we found that this was pretty effective for combating the overfitting„ÄÇ

slide we have a question Oh also about the students Yeah okayÔºå let me take a look„ÄÇ

Do you want to go ahead I can ask is just in reference to that previous table where you you have throughput and precision it just seems surprising to me that you could match this 1390 number„ÄÇ

We using selective precisionÔºå it seems like I would expect it to be like something in between„ÄÇYeah„ÄÇ

 so it essentially comes down to the fact that like there's maybe a little bit of noise sample bit the speed and the only part we're casting is the router„ÄÇ

 which is you knowÔºå maybe like it is such a insignificant portion of the computation and there's zero communication there that it's essentially like a free operation in the network so whether you cast it to B flow 16 or flow 32 it doesn't actually impact the speed at all within the precision that we can actually measure the speed„ÄÇ

And alsoÔºå these architectures only use fast layer when when every fall layers„ÄÇAnd so„ÄÇYeah„ÄÇ

 essentially the flood 32 parties is kind of very negligible in the entire architecture„ÄÇ

It's like for exampleÔºå I think like off the top of my head it's like140th the computation that would cost for you to do the first like like weight matrix multiply in like a dense raylude dense layer or something„ÄÇ

 so it's a veryÔºå very small part and yeah we're not using them very frequently like Erwin mentioned as well„ÄÇ

Got itÔºå okayÔºå thanks„ÄÇYeahÔºå and thenÔºå you knowÔºå just like a quick point in this„ÄÇ

 like I won't into some of the technical detailsÔºå but yeahÔºå we definitelyÔºå you know„ÄÇ

 since we're training these things on hardware we really like I think a big part of the mixture of X paradigm is that these things are designed such that it maps really efficiently to hardware„ÄÇ

So we want to be doing dense matrix multiplies„ÄÇAnd for this to work really well we also want to be able to have you know roughly equal amount of tokens going to each of the different experts and I think the this isn't that sensitive to the load balancing formulation like we tried a few things a lot of them worked„ÄÇ

 but yeah essentially you definitely want some kind of load balancing loss added on when using sparsity Yeah next slide„ÄÇ

ÂóØ„ÄÇYeahÔºå ErwinÔºå go ahead„ÄÇYesÔºå so„ÄÇSo the frameworksÔºå the library we use rely on static shapes for„ÄÇOkay„ÄÇ

 So we so X L A„ÄÇ So the compiler for tensorflow and machine offlow expects static shapes for tensors„ÄÇ

HoweverÔºå the computations in switch transformers are dynamic because„ÄÇYou knowÔºå because of the router„ÄÇ

 rightÔºå like different inputs will be routed to different experts„ÄÇAnd soÔºå we need to„ÄÇ

Specify ahead of time„ÄÇ how many tokens will be sent to each checkbook„ÄÇ

And so we will introduce this expat capacity hyperpy„ÄÇTo specify that„ÄÇ

 And that's going to be a static numberÔºå which says how many tokens each export can process„ÄÇ

And so in practiceÔºå we instead parameterize this by having a quantity called the capacity factor„ÄÇ

So we have an example here„ÄÇSoÔºå you knowÔºå so the bottom row is„ÄÇOkay„ÄÇ

 so is a bunch of tokens on one deviceÔºå and then you need to sort of route those tokens to multiple devices or multiple expertss„ÄÇ

So if too many tokenins are routed to a single exp stop„ÄÇSome tokens will be dropped because„ÄÇ

 as we saidÔºå like export having a fixed capacity„ÄÇSo that's the example on the left where the capacity factor is one„ÄÇ

 and that basically means that the total„ÄÇIt does noÔºå like extra buffer„ÄÇFor writing tokens„ÄÇ

So instead of thatÔºå we can use a capacity factor that's larger than one„ÄÇ so on the right„ÄÇ

 you have an example with a 1„ÄÇ5„ÄÇSo that means that now each expert has like sort of three slots that can process three tokens„ÄÇ

And so that prevents token dropping because we have more capacity„ÄÇ

 but the issue is that this means higherÔºå you know„ÄÇ

 this means more expensive communication across devices„ÄÇYeahÔºå okayÔºå so does it„ÄÇYeahÔºå go ahead„ÄÇ

 But yeah„ÄÇ So yeah„ÄÇ So one thing that we also experimented with was this method called no token Left behind„ÄÇ

 And the idea was the following„ÄÇ So since we have to have likeÔºå you know„ÄÇ

 a fixed batch size for each expert and there can be token dropping„ÄÇ We kind of we're thinking that„ÄÇ

 heyÔºå yeahÔºå having tokens dropped there like you know having some tokens„ÄÇ

 not having any computationnot applied to it is probably hurting the model performance„ÄÇ

 So what if we do a multistage routing procedure„ÄÇ So first„ÄÇ

 you do the normal routing where it's like you send each token to its highest probability expert„ÄÇ

 But then any drop tokensÔºå you then send to their second„ÄÇ

Highest probability expert and so forth and so onÔºå where you can basically repeat this process to guarantee that no tokens are being dropped„ÄÇ

InterestinglyÔºå actuallyÔºå this approach didn't empirically improve model performance if anything had actually kind of hurt it„ÄÇ

And we thought that was actually very interesting„ÄÇAnd I think the intuition is thatÔºå you know„ÄÇ

 once the model learnsÔºå it wants to send a token to one expert„ÄÇ

 like it really wants to have that computation applied to it and just applying some other computation doesn't you know have at all the same property along with it actually maybe being potentially detrimental„ÄÇ

So yeahÔºå we thought that was pretty interesting as we were very optimistic this would potentially you know get improved performance„ÄÇ

 but it ended up not really making a difference and we found this quite surprising„ÄÇ

We have a question from„ÄÇÂóØ„ÄÇI think about actually kind of like address and literally the last point that you brought up I think when I think about like a mixture of experts usually like they specialize in like different things right so I think it's like„ÄÇ

Just like a lotÔºå like I was just wondering„ÄÇLike if you send it to like the second best or whatever„ÄÇ

 like what if like all your tokens would be particularly good for like one expert and then you only like process„ÄÇ

 let's say like 20% of your tokens„ÄÇSo that ends up being better than rerouting them to anything else exactly Yeah„ÄÇ

 so yeahÔºå even if you're dropping a lot of tokensÔºå it's not beneficial to be sending them to the second third or fourth best thing and one actually interesting property that we„ÄÇ

 you know noticed about these models is they're surprisingly robust to token dropping„ÄÇ

 especially during fine tuning„ÄÇSo yeahÔºå so in the standard paradigm„ÄÇ

 what we'll do is we'll pretrain this thingÔºå we'll have some load balancing loss„ÄÇ

 which makes the tokens pretty balancedÔºå actually„ÄÇBut then during fine tuning where it's like we really want to fine tuningna on a specific task we actually studied this exact question and we were studying does it help to have a load balancing loss during fine tuning or not and so if you have the load balancing loss yeah that kind of is encouraging you know for the specific task we want to try to have you know all the experts be used versus turning it off whereas there's definitely some you know prior specialization and it's actually much better to just turn the auxiliary loss off and even if it's like you know 60 to 70% of the tokens are being dropped that actually performs much better than you know having all the tokens balance but doesn't a load balancing loss encourage basically all the experts to learn very similar weights and then just randomly assign a tokens„ÄÇ

Because then it doesn't matter to which expert stuff is being sent to„ÄÇ

So when we use the load bouncing loss like the routing mechanism is definitely learned„ÄÇ

 so the model definitely is encouraged to you know choose an expert that it wants to send it to for good right but like if all the experts learn the same weights„ÄÇ

 then the router learns basically ohÔºå it doesn' matter where to send it to„ÄÇ

So if you encourage load balancingÔºå you encourage„ÄÇTechnically that like you want any loss to fit of any expert„ÄÇ

 rightÔºüI meanÔºå that's maybe the extreme behavior„ÄÇ if you have a very high sort of load balancing less coefficient„ÄÇ

But in practice that coefficient it's kind of tune and we absorb that for you know smart enough values the of steel loans like se like meaningful routing Yeah because it's like the balance between this like you know cross entropy loss and this load balancing loss and so on one hand yeah you definitely want to encourage the model to be balanced then on the other hand you also want to just good empirical performance and yeah the model is able to definitely like on one hand learn and specialized experts where they have different weights such that it's like you know definitely expect certain tokens decent sent to certain experts but on the other hand still be recently balanced so that the models are officially run on like modern hardware„ÄÇ

Excycl„ÄÇWe also have a question from the classroom„ÄÇSo the question that I want to ask is it seems to me like this is a very experimental talk we're talking about floating point precision we're talking about different approaches and currently work well and whenever were dealing with a group clients there's a question of what is the research question and I feel like I miss that so what are we trying to answer with all these experiments„ÄÇ

YeahÔºå I think I think the high level research question is like you know„ÄÇ

 can we you know create models that are you knowÔºå like doing adaptive computation from the standpoint of like„ÄÇ

 you know can we try to make models more simulate the dynamics that we think models should you know most naturally use„ÄÇ

 which is different inputs have different amounts of computation applied have different weights applied to them you know and basically all of this„ÄÇ

 basically we're trying to research and like figure out how can we create like a new framework for these models to be trained as opposed to their dense counterparts that you know for every input are always having the same exact computation applied so that's interesting because when you say the same exact computation applied one might imagine that„ÄÇ

Like to me the immediate thing is about how long to deliberate about something what I mean by that is if we want to have variable length computation you could imagine that I could have a short amount of computation or it could have much longer computation„ÄÇ

 but this idea of like why then do we instead consider the dimension of different computation I mean assuming of course that these experts do indeed need to learn different things which I think you'll get to in yeah„ÄÇ

SoÔºå why do we immediately jump to thinking about specialized experts as opposed to thinking about variable length computationÔºü

So yeahÔºå so this is actuallyÔºå we actually go into some variable length computation stuff later in the talk„ÄÇ

 and I feel like they're both actually just important axes that should both be pushed on„ÄÇ

 I think I guess yeahÔºå I guess it's kind ofÔºå you know„ÄÇ

 I guess what I'm not freezing my question but I'm trying to understand is yourre thinking about why did you decide to tack this one first„ÄÇ

 I want to understand why your team chose to go this direction first„ÄÇYeahÔºå absolutely„ÄÇ

 so I think that one empiricallyÔºå it seems that sparsity has led to better empirical results in the field of deep learning than adaptive computation so far„ÄÇ

 And I think the way that we use these things maps really well to our modern hardware which is also very promising„ÄÇ

 And I think the way we were kind of looking at it is like sparsity is like a first step towards doing more interesting and general adaptive computation where and and know because I think it's like this stuff is complicated and typically starting from something that works well is better than necessarily like you know you know trying something that's not necessarily as proven out and then trying to like get it to work really well„ÄÇ

 So I think we're kind of starting from sparsityÔºå which like nu Shaser and others got to work really well in the context of LSTMs we were kind of interested in let's port some of this to transformers let's get it working really well and then that's slowly start expanding towards a lot of the other natural questions that you mentioned whereas like okay whereas instead of different weights per core„ÄÇ

 let's also maybe have different computation per core and all of this that's I guess how we were kind of building the natural build up in progression of„ÄÇ

Our research got it coolÔºå thank you yeah„ÄÇWhatho do you think ErrwinÔºå anything else to addÔºüÂóØ„ÄÇYeah„ÄÇ

 I meanÔºå I guess I kind of see adaptive computation and sparsity as„ÄÇYou know„ÄÇ

 related but separate things„ÄÇ SoÔºå you know especially more like different parameters for each example and adaptive computation might be more different amount of floods„ÄÇ

 and we have some of that with the talking and droppingÔºå but that's kind of„ÄÇNo„ÄÇ

 that's not the domain„ÄÇTheomamain motivationÔºå definitelyÔºå as Barrett mentioned„ÄÇI would sayÔºå you„ÄÇ

 no one really has figured out adaptive computation yet„ÄÇFor deep learning„ÄÇ

 And what one reason is because we have these„ÄÇYou knowÔºå acceleratorsÔºå rightÔºå expectÔºå expect like„ÄÇ

Sort of no„ÄÇWe need to work with like batch like data parallelismÔºå right„ÄÇ

And all of our accelerators in our frameworks use this SPMD paradigm where we're kind of supposed to apply the same computation to„ÄÇ

Two examples„ÄÇAnd so if you look at the literature you have works like universal transformers„ÄÇ

 where they replace the fit forward in the transformer bay„ÄÇ

Just a recurrent weight and so it's kind of like an LSTM on each token and the LSTM can stop at different time and space on some„ÄÇ

CriteriaÔºå but the way these things are implemented is just through masking„ÄÇ

Because it needs to be implemented in the SPMD programming style„ÄÇ

And so definitely spaity was kind of like easier to get to work first„ÄÇ And also„ÄÇ

 there were some prior results with LSTM so„ÄÇIn terms of like the first questionÔºå you know„ÄÇ

 sort of what store research question here is just likeÔºå ohÔºå can we design more efficient modelsÔºü

And spaity is this new axis that hasn't been explored that much„ÄÇ YeahÔºå I think that„ÄÇYou know„ÄÇ

 I'm happy with just that being the research question„ÄÇGreatÔºå okayÔºå yeahÔºå so next slide„ÄÇYep„ÄÇOops„ÄÇYeah„ÄÇ

 againÔºå so kind of putting it all together„ÄÇ So the switch transformer layer selects an expert like just the top expert and then incorporates a bunch of the general sparse model improvements to„ÄÇ

 you knowÔºå allow it to fine tune better„ÄÇ allowow it toÔºå you know„ÄÇBe more regularized„ÄÇ

 allow it to you knowÔºå be trained with lower precision formats and a lot of like technical details to just get them training and working well„ÄÇ

Yeah so one thing that we also wanted to do was a comparison between like top one and top two routing since top two routing was kind of the you know most popular technique and so here we can see we have two different dense models trained at different sizes and we're going to be looking at like the the pretrain like negative log perplexity so„ÄÇ

YeahÔºå the bigger the numberÔºå the better„ÄÇSo next slide„ÄÇ

So and what we're going to be doing is we're going to be studying them at different capacity factors„ÄÇ

 so a capacity factor of 2„ÄÇ0 basically means that there' is enough buffer for two tokens to be sent to every single expert„ÄÇ

And we're going to be comparing like top one versus top two routing„ÄÇ

And also comparing their speeds along with their like time to get some like threshold quality„ÄÇOkay„ÄÇ

 yeahÔºå so here we can see in the capacity factor 2„ÄÇ0 case„ÄÇ

That the MOE models outperform switch transformformer„ÄÇ

 which makes a lot of sense like since switch transformformer is only you know sending like a top one token to each expert„ÄÇ

 the mixture of expert is sending you know two tokens so that makes sense that this extra buffer will be like disproportionately beneficial for the mixture of expert models„ÄÇ

And so we noticed that and next slide or yeahÔºå next„ÄÇ

Now when we so the really interesting parts for the top one runningut becomes when we lower the capacity factors so having a high capacity factor is bad for many reasons„ÄÇ

 one of which is it really incurs more of these you know communication costs for sending tokens to the correct experts it also incurs more compute costs and also incurs like a lot of memory overhead so if you can get this lower it's it's usually like a very very good thing„ÄÇ

And so what we see here is that switch transformformer actually outperforms mixture of experts when you have like a lower capacity factor and we can see that the time to quality threshold we you know yeah we get there much quicker and so even across the 2„ÄÇ

0 and the 1„ÄÇ25 capacity factors like the kind of preto optimal thing we saw on our setup is to use switch transformer at a lower capacity factor just due to the fact that while the quality is a little bit worse on a step basis„ÄÇ

 it's just like much faster to run so it's kind of the preto optimal decision„ÄÇNext slide„ÄÇ

And we can also be seeing that like for capacity factor 1„ÄÇ0 again„ÄÇ

 we can see that this really disproportionately benefits switch transformer and is even better from annaparto standpoint than the 1„ÄÇ

25 capacity factors„ÄÇAnd interestinglyÔºå since you know„ÄÇ

 MOE also does like a little bit more computationÔºå we can also just increase the amount of compute done elsewhere in the model„ÄÇ

 and we can see that that's like a much more efficient allocation of compute„ÄÇSo yeah„ÄÇ

 overall our takeaway is that yeah lower capacity factors using top one routing is more preto efficient than„ÄÇ

 you knowÔºå using like top two routing at higher capacity factors„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_18.png)

Next slide„ÄÇOranÔºå you can take it over„ÄÇOkayÔºå so next we look at how switch transformer scales„ÄÇ

As a function of the number of exports in the switch layers„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_20.png)

And so on the right side here you see a plot that shows complexity„ÄÇ

Vus training steps for different switch architectures ranging from Tfi basease„ÄÇ

 which is basically no expert or a single expert up to 128 experts„ÄÇ

And so you see that as we increase the number of experts„ÄÇ

 which also increases number of parameters of spa parameters„ÄÇYou get sort of speedupsÔºå you know„ÄÇ

 you get increasing speedups of the dense baseline and all like sort of dimition returns„ÄÇ

To multiplying to increasing the number of experts as well„ÄÇ

So the previous figure was looking at complexityity versus string steps„ÄÇ

 Here we look at complexityplexity versus strength time„ÄÇSo that includesÔºå you know„ÄÇAll theÔºå you know„ÄÇ

 additional communication costs when you have more experts on„ÄÇYou knowÔºå comparing„ÄÇ

 comparing to the dance baseline„ÄÇAnd so this is for switch base or then T5 bays„ÄÇ

 and we observe x up to seven x Philipups over our T5 bays„ÄÇÂóØ„ÄÇAnd soÔºå you knowÔºå just to„ÄÇ

Maybe contextualize theseÔºå these numbers like„ÄÇYou know„ÄÇ

7 x speeds into planning are pretty hard to obtain„ÄÇ And so I think this is one of theÔºå you know„ÄÇ

 one of the result that„ÄÇYou knowÔºå can spark a lot of interest in sports models„ÄÇ

 even if it's only for preing for nowÔºå like just having that number is likeÔºå you know„ÄÇ

 maybe it does a„ÄÇThere's a significant„ÄÇU„ÄÇDoes something significant that can be obtained here„ÄÇOkay„ÄÇ

 so sports scaling laws„ÄÇ So here well look at„ÄÇSo it philosophy versus„ÄÇSpaous mineral parameterss„ÄÇ

 which are increased by increasing the number of experts„ÄÇ

And so similarly to the sort of neural scaling blood paper„ÄÇ

We observe that as you increase the parameterss„ÄÇWhich the sparse parameterss and keep the fluxps fixed„ÄÇ

 you get diminishing like consistent gainsÔºå but diminishing gains„ÄÇOkay„ÄÇ

 so now we're going to compare export parallelism and model parallel„ÄÇ

 So we introduced sparsity or export parallelism as a new dimension to scale models„ÄÇ ButÔºå of course„ÄÇ

 there's the other one for dense modelÔºå which is simply model parallelism where you know„ÄÇ

 model weights are partition across calls once they are„ÄÇ

Above the the maximum size that you can feed on a single core„ÄÇNo„ÄÇAll rightÔºå so„ÄÇYeah„ÄÇ

 advertising the left is expo priority here„ÄÇYeahÔºå so essentially what we're doing is is yeah we're kind of comparing a switch base model versus the dense space and we're also comparing against a larger dense model that has used model parallelism and we can see that you know because basically when we want to scale up model size we kind of have two axes that we can either go through we can either increase the number of flops by scaling through model parallelism or increase the number of parameters by scaling through sparssity and so we can see that you know even compared to like you know a dense model that's been scaled up through model parallelism that' sparssity is still at the scale a more effective way to scale up the model„ÄÇ

ByÔºå you knowÔºå still getting 2„ÄÇ5 x speed ups over this larger dense model that was using model parallelism„ÄÇ

CoolÔºå so slide a bit„ÄÇBasicallyÔºå here T5 large is the dense model that uses other part of„ÄÇYeah„ÄÇ

 all rightÔºå go ahead„ÄÇOkay„ÄÇYeahÔºå and so one thing that we also wanted to look at is like you know„ÄÇ

 are these expert models effective if you have like you know a really small amount of compute or just a small amount of experts„ÄÇ

 So typically when we're designing these modelsÔºå like we have one expert per core„ÄÇ

 but if you don't have like a large cluster run these things on let's say you just have like a GPU with two cores or something like is having two experts more effective than just like a dense model And the answer is yes„ÄÇ

 so we can see even pretty good scaling propertiesÔºå even with like a tiny amount of experts„ÄÇ

 which is very promising for these models to be used even in like much lower compute regimes„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_22.png)

Next slide„ÄÇOr you want to go ahead„ÄÇ OkayÔºå so yeahÔºå so„ÄÇYesÔºå so look at„ÄÇYou know„ÄÇ

What things look like when we use different types of parm„ÄÇ

 namely expel parm to add exp to model parallelm to sh model ways„ÄÇ

Across calls and also data parallelismÔºå which is sort of the dominant paradigm in at the moment„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_24.png)

And so„ÄÇYou knowÔºå I guessÔºå you knowÔºå in the previous slidesÔºå we mostly talked about X parallelism„ÄÇ

 but of courseÔºå you knowÔºå dense models and large dance models use model parallelm„ÄÇ

 So G3 and other large modelsÔºå What they do is that they will simply she model weights across different core„ÄÇ

YeahÔºå we have a question„ÄÇOh yeahÔºå I just wanted to know because I think there was like„ÄÇ

 I don't know if you go got under rest lateratorÔºå but I think somewhere in a paper„ÄÇ

 it' said that the more experts you haveÔºå the more sample efficient it gets„ÄÇ

And I was just like hoping that you could give us some intuition about that because I„ÄÇ

Don't understand why that will be the case„ÄÇ

![](img/cfb06f02d43700aca0402fcc8c4854ad_26.png)

So I guessÔºå yeahÔºå maybe Er oh yeahÔºå so I guess likeÔºå you know„ÄÇ

 there's all of this work on larger models are more sample efficient„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_28.png)

And larger in the context of these scaling law works means like more parameters and more flops as you increase the number of experts there's more parameters„ÄÇ

 but not more flopsÔºå but the model is still like you know larger in like you know a similar sense„ÄÇ

 so I guess like building on the intuition that larger models are more sample efficient in my mind it's not necessarily that's surprising that these models with more experts that have more parameters are more sample efficient„ÄÇ

I guess that's my kind of high level intuition for it„ÄÇYeahÔºå I would say that's kind of expected that„ÄÇ

 you knowÔºå more exports leads to better sample efficiency„ÄÇEspecially if you look at training setup„ÄÇ

 right in a training tab„ÄÇOkay cool„ÄÇSo where are weÔºüYeahÔºå so yeah„ÄÇ So okay„ÄÇ

 So we look at how model weights are speed of a cost for different scenarios„ÄÇ

 So data parallelism is the first one„ÄÇ So that's kind of theÔºå the typical setup that the uses„ÄÇ

Especially for nuts solar large networks which don't require mother parisism„ÄÇAnd so let meÔºå yeah„ÄÇ

 let me explain how„ÄÇYeahÔºå I'll just go to the final„ÄÇ

Figure and now explain how to look at this figure„ÄÇOkayÔºå so we have 16 processes„ÄÇ

 which are organized in a 4 by4 meshÔºå rightÔºå So each dotted line„ÄÇ

 each4 by four dotted line here represents a different core„ÄÇIn the first row„ÄÇ

 studies how the model weights are speed over callss and the second row„ÄÇItuss how data„ÄÇ

 so literally examples and tokens are split over calls„ÄÇAnd yeahÔºå and then the final thing to„ÄÇ

That's required to understand this figure is that each„ÄÇÂóØ„ÄÇYeah„ÄÇ

 each color of the shaded squares here identifies a unique weight matrix„ÄÇOkay„ÄÇ

 so let's start with data parallelism„ÄÇSo for data parallelism„ÄÇ

 the same model weights are replicated across all core„ÄÇAnd the data is simply partition of our cause„ÄÇ

 and so that's what this corresponds to„ÄÇYou knowÔºå if you like using the the description of the caption„ÄÇ

 the explanation of the caption that just gave„ÄÇSo next we have model parallelism„ÄÇ

That's kind of just like a theoretical example because in practice„ÄÇ

 people always use model parallelism in conjunction with data parallelism„ÄÇ

But so if you were to do only model parallelm now you would have a single model weight that is partitioned of all course and your data would just be replicated„ÄÇ

Of all causing said„ÄÇSo now we have modeling data parallelism„ÄÇ

 and that's kind of the typical scenario for large dense networks„ÄÇSo in that case„ÄÇ

 model weights are partition among a subset of the calls„ÄÇ

 the subset of calls that process different batches of data„ÄÇAnd so in that example here we have„ÄÇ

You knowÔºå sort of fourÔºå so the first sub square here means that the model weights are partitioned across four four calls„ÄÇ

And„ÄÇAnd this is replicated„ÄÇSort of four times for the data parallel dimension„ÄÇ

On the data side for model and data parallelism„ÄÇYeahÔºå the data here„ÄÇ

Is replicated across model parallel core and partitioned across data parallel core„ÄÇ

So next we have exp and data parallelism„ÄÇSo in that scenario that's kind of similar to data parallelism„ÄÇ

 but now each car will hold a different model weightÔºå which is illustrate by the different colors„ÄÇBy„ÄÇ

And for the data sideÔºå the data is simply replicated sorry„ÄÇ

 the data is positioneded across all callsÔºå just like in the data parallel scenario„ÄÇAnd so finally„ÄÇ

 we have the right most columnÔºå which is„ÄÇI guess yeah„ÄÇ

 that's the setup use in the switch transformer paper for the larger models„ÄÇAnd so„ÄÇ

 here for the model„ÄÇPartitioning each expert is partitioned across multiple calls„ÄÇ

 so in that exampleÔºå we have four experts„ÄÇEach partition press four calls„ÄÇ

And the data is replicated across multiple parallel calls and partitioned across data parallel calls„ÄÇ

So that that's a little bit complex to understand orally„ÄÇ

 but the switch transformer paper has a nice the same figure with a nice caption to explain it„ÄÇ

And yeahÔºå maybe we can„ÄÇNo BarretÔºå we can add something quickly about how this is implemented in practice„ÄÇ

So„ÄÇThere's this paper called mesh TransformerÔºå which kind of extends„ÄÇ

batchache or data parallel to more general purpose SPMD style programming„ÄÇ

And so different labs have different you know frameworks„ÄÇ

 but this paper kind of lays the foundation for general SPMD distributed computing„ÄÇ

 which is required for training large scale models„ÄÇAnd so under the mesh abstraction„ÄÇ

 basically we have a mesh of processes„ÄÇch and so that mesh has dimensionsÔºå name dimensions„ÄÇ

 and these name dimensions specify how the tensile dimensions will be partitioned or replicated across the mesh dimensions„ÄÇ

And so just that simple abstraction sort of supportsÔºå you know they parallelism„ÄÇ

 also model parallelism and especially export parallelism at once and so you know I invite whoever is interested to also check that paper that because that's kind of„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_30.png)

You knowÔºå that kind of lays the foundation for understanding these things„ÄÇAll right„ÄÇ

 Dar want to go cool yeahÔºå so next we are going to kind of talk about like how we take these parallelism strategies and like kind of combine them together to make like a 1„ÄÇ

6 trillion parameter sparse model„ÄÇ

![](img/cfb06f02d43700aca0402fcc8c4854ad_32.png)

So next slide„ÄÇSo so yeahÔºå so what we ended up doing in this work was we had we trained two different very large spae models and we compared them to the largest T5 model so we can see the T5 XXL„ÄÇ

 which is a dense model and it was the largest one trained in the T5 paper and it has around 13 billion parameters and here we list a lot of the model dimensions like D model DF which are just like you know the various sizes and shapes of the tensors and stuff„ÄÇ

 the number of layersÔºå the number of heads and importantly we also mentioned the negative log perplexity„ÄÇ

At step 250 k and at 500 k and so yeah so we designed two sparse models to test and to test like how scaling versus sparsity versus scaling versus sparsity and flops work so first let me talk about switch XXL so that has the same amount of flops per token as T5 xxL but has 64 experts and this leads it to have around 400 billion parameters„ÄÇ

And we can see that on a step basisÔºå it actually performs quite well and outperforms the T5 xXL by like quite a good margin„ÄÇ

InterestinglyÔºå thoughÔºå the third model we designed SwCÔºå which has 1„ÄÇ6 trillion parameters„ÄÇ

 but has significantly fewer flopsÔºå almost 10 less flops per token than either of the above two models„ÄÇ

 so it's really trading by reducing flops that have way more sparse parameters„ÄÇ

And we can see on a step basisÔºå the switch C model works well„ÄÇ

 but not as well as actually the higher fl model„ÄÇBut on like a kind of a Pareto axis where were looking at TPU hours on the X axis and not step„ÄÇ

 the switch C model actually outperforms them both by like a pretty large margin„ÄÇ

 So for pre training performanceÔºå we're seeing that actually just like having a lot of sparsity and less swapps is actually can be quite good„ÄÇ

üòäÔºåNext slide„ÄÇYeahÔºå and so yeahÔºå this so againÔºå those two sparse models are kind of really trying to get at this hypothesis that actually Noam Shaer had„ÄÇ

 which isÔºå you know„ÄÇThat you know parameters are good for more knowledge reasoning and compute aK flops is good for intelligence and so we're going to kind of try to get at that by taking these different sparse models and then fine tuning them on different tasks„ÄÇ

 some of which require more like knowledge and then others which require more of like reasoning„ÄÇ

For whatever like hand w definitionÔºå we want to give that„ÄÇSo yeahÔºå so for a fixed oh go back so yeah„ÄÇ

 so for a fixed„ÄÇOh can you go back to the previous slide Oh sorry Okay„ÄÇ

 so for a fixed quality on an upstream pre training taskÔºå yeahÔºå do parameters independently matterÔºü

So we're going to look at two tasks hereÔºå one of which is super gluelu„ÄÇ

 which is kind of our like reasoning task and then another is like trivia QA which is like some knowledge task where it's like you just give it a question and you have an output and answer„ÄÇ

Okay„ÄÇAnd so here we're going to take a look at superglo quality so we can see on the X axis is the pre training performance and the Y axis is the superg score after fine tuning„ÄÇ

And interestinglyÔºå we can see definitely that the sparse models definitely are for a fixed pre training perplexity„ÄÇ

 do worse on fine tuning„ÄÇThis can be especially noticed at like the upper right portion of the plot where the dense models are definitely fine tuning better than their sparse counterpoint„ÄÇ

Next slide„ÄÇInterestinglyÔºå when we study it on the more knowledge heavy tasks„ÄÇ

 the sparse model for a fixed pretraining perplexity does disproportionately well„ÄÇ

 so you know for a model that roughly has the same perplexity we're getting like really large boosts for these knowledge heavy tasks so this is pretty interesting and it also really you know shows some of the dangers of comparing only on your pretraining metric so these models you know can have the same exact pretraining metric but very different you know properties when fine tuning them on different tasks„ÄÇ

Next slide„ÄÇAnd interestinglyÔºå so yeahÔºå all of the switch models here are just likeÔºå you know„ÄÇ

 various models„ÄÇThat have still a good amount of flosÔºå but the red model is actually the 1„ÄÇ

6 trillion parameter sparse model that hasÔºå you know very few flopsÔºå but a lot a lot of parameters„ÄÇ

 And we can see that as the red dot hereÔºå and it does actually disproportionately bad compared to other sparse models that also have pretty good perplexities„ÄÇ

And so yeahÔºå it's definitely very interesting and it shows that for models during pre traininging that have a lot of sparsity„ÄÇ

 they definitely suffer on some of these more reasoning heavy metrics„ÄÇ

 but do disproportionately well for more of these knowledge heavy tasks„ÄÇNext slide„ÄÇYeah„ÄÇ

 and so here we can see it as just like a huge outlier for pre training perplexity doing like just incredibly well on this downstream question answering task„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_34.png)

Next slide„ÄÇ

![](img/cfb06f02d43700aca0402fcc8c4854ad_36.png)

YeahÔºå okayÔºå so alsoÔºå you knowÔºå one thing that we were going to do is just look at the fine tuning properties of sparse models across like a few scales and just see how they perform„ÄÇ

Next slide„ÄÇYeahÔºå and so here we try two different models One is T5 base then we make a flat match sparse counterpoint and when they say flat match it's like you know each token will have the same amount of flops but now we just have experts so we do this for both base and large and we see that actually across almost all tasks besides two archrc tasks the sparse models perform quite well which is which is definitely promising so we are seeing that these models are pretty robust they pretrain well and then they also fine tune well when scaled appropriately by scaling up both the flops and sparsity whereas you know the negative results you've really seen are like yeah when you just have a huge amount of sparsity and not too many flops„ÄÇ

Next slide„ÄÇYeah and one also thing we wanted to look at was the multilingual training so we were previously studying all of this on like English only and we also wanted to see how Sprsity helps in the multilingual setting because you know we also felt like this would be a very natural place for Sprsity to work well where potentially experts could specialize across languages„ÄÇ

And we do see strong resultsÔºå so on 91% of the languages„ÄÇ

 I think of like around 100 languages we see over like at least a4 exped over the MT5 dense model„ÄÇ

Next slide„ÄÇOr you want to go aheadÔºüNo go go ahead„ÄÇ OkayÔºå Yeah„ÄÇ

 so another thing we wanted to talk about was distillation„ÄÇ

 So one downside of these sparse models is that they'll have a lot more parametersÔºå which means that„ÄÇ

 you knowÔºå if you're serving these things or something you either need like high throughput use cases or you need to maybe distill it back down until like a smaller dense model„ÄÇ

So here what we do is we look at like the T5 base and switch base and we look at its pretraining performance„ÄÇ

 and then we go through some abllationations of different distillation techniques and find that like with the best techniques„ÄÇ

 we can keep around 30% of the quality improvements of sparsity while distilling it back down into its dense counterpart„ÄÇ

So next slide„ÄÇYeahÔºå and then we kind of study this across multiple scales and again we see like around like 30% to 40% of the gains can be like you know kept when going from a dense model when going from you know a sparse model and distilling it back down until like its fl match dense model so you can get you know get rid of up to 99% of the parameters and still keep like around 30% of the improvements„ÄÇ

 which is very promising„ÄÇNext slideÔºå waitÔºå I'm sorry„ÄÇYeah„ÄÇAll rightÔºå sorry about that„ÄÇ

 can you say that last sentence againÔºå you said that you can keep the benefit 30% of the teachers benefit„ÄÇ

üò°ÔºåYeahÔºå basically so yeahÔºå you you yeahÔºå exactly so yeahÔºå so we're looking at likeÔºå yeah„ÄÇ

 you train a sparse model and then you distill it back onto to a dense model„ÄÇ

And versus training a dense model from scratch„ÄÇ And like you look at the gap between a sparse and dense model from scratch versus the gap between the dense and then the distilled dense model„ÄÇ

What do you registeringÔºüYou go forward„ÄÇOh yesÔºå yeah„ÄÇOh yeah„ÄÇ

 maybe let me just do like a quick high level summary again„ÄÇ So yeah„ÄÇ

 we' we'll do is for our comparisons is we'll train a dense model from scratch„ÄÇ

We'll train a sparse model from scratch„ÄÇAnd then we'll also run a third experiment where we distill that sparse model down into a dense model„ÄÇ

What does distilling mean like we we're basically trying to match the like the teacher's logits„ÄÇ

Like the kind of standard thing of likeÔºå you knowÔºå like matching the like either the thelogits or like the soft probabilities for each token or something like that„ÄÇ

OkayÔºå if I can jump in with my questionÔºå so what I'm struggling with is„ÄÇ

 how do I interpret the alignment that as percent of teacher and performanceÔºüYeahÔºå okay„ÄÇ

 so it's basically looking at the like the gap between the dense and sparse model„ÄÇ

So we'll have the dense model get some performance„ÄÇ

 we'll have the sparse model get some performance and then the dense model dis still from the sparse model would be somewhere in between that range and we're basically saying it's 30% through that range„ÄÇ

 So it's like in like a 0 to1 interval it's like „ÄÇ3 of the way from the dense to the sparse model I see so this is not saying that the percent of teacher performance does not mean that if the teacher say if we use the teacher guesses or predictions as the ground truth„ÄÇ

 this is not saying that the distilled model gets matches with the teacher 33% of the time„ÄÇNoÔºå no„ÄÇ

 exactly„ÄÇ It's basically saying you get like 30% of the the quality improvements„ÄÇ Yeah„ÄÇ

 exactly Okay cool And then if we can back up the slideÔºå I had a different question„ÄÇ

 but I didn't want to interrupt when we're talking about all of these different T5 bases and then also on a few slides before this„ÄÇ

 I don't know that much about T5Ôºå I'm curious you know when T5 is trained„ÄÇ

 is there a weight penalty in the loss function„ÄÇIs there a way to catererÔºüNo„ÄÇ

 there is no way to cage trained with any of those sparse or dense models I see so out of curiosity then how do dense models perform compared to the switch model if you add some sort of weight regularization that incentivizes getting rid of useless weights„ÄÇ

So some kind of like maybe like L1 term or something like that quickly Yeah„ÄÇ

 so I'm just wondering like how much of because here we're talking about the benefits of sparsity and I'm wondering how much of this benefit from sparsity is due to the fact that just some of this I mean„ÄÇ

 effectively what the switch model is doingÔºå if I understand correctly maybe I don't what understand is that the switch model in the feed forward layer„ÄÇ

 it's just like you you fix some of the weight to be zero that's what it means to be spars„ÄÇWell„ÄÇ

 actually we're kind of really trying to like inject more weights„ÄÇ

 so we're actually kind of trying to do it's a little bit maybe like paradoxical because we're saying switch transformer but our idea to be like„ÄÇ

 heyÔºå we actually want to just have significantly more weights„ÄÇ

Not last it's kind of like you would zero out weights but within a much larger weight matrix if that makes sense I see yes„ÄÇ

 and so to me it seems like a relevant baseline to just ask what happens if I have the dense matrix but I incentivize it would say an L1 or L2 penalty on the weights and would I'd be curious to know how that compares„ÄÇ

YeahÔºå we didn't run thisÔºå but also that kind of gets rid of weights for the dense modelÔºå so if any„ÄÇ

So yeahÔºå yeah„ÄÇYeahÔºå and the last point is like if you just add like an L1 penalty loss„ÄÇ

You're not going to have structural sparsity„ÄÇWas likeÔºå here weÔºå you know„ÄÇ

It's not random weights in your giant weight matrix that are 0 thatÔºå right„ÄÇ

 It's like really like blocksÔºå depending like blocks corresponding to each expo„ÄÇ

 Right so that that structure allows the the whole like communication stuff and and that's„ÄÇ

Yes that just the fact that you have multiple cousins on right„ÄÇ

 So I totally agree with that block structure and that's what I'm trying to say is that the switch has this very richs it's not just sparse„ÄÇ

 It also has this rich structure what I'm trying to do in my mind is disentangle is the sparsity What's offering an advantage or is this additional structure that you've built in is that what is the performance game So that's why I'm asking„ÄÇ

So the the„ÄÇThe block structure is what„ÄÇEnables to leverage the fact that you have multiple calls but like if you if you didn't have that drug structure„ÄÇ

 you'd still have to route to everything and so you have more communication costs and so on so and then your first question was what sorry„ÄÇ

I'm not actually sure if there was a questionÔºå I guess what I'm trying to say is I'm trying toambiguate„ÄÇ

 yeahÔºå anyways„ÄÇBut I agree„ÄÇ it's a little bit weird because sparsity kind of's spectrum of meaningful for sparsity„ÄÇ

 rightÔºå it likeÔºå for exampleÔºå compression and like model pruning is a form of sparsity„ÄÇ

 but also switch transformer and ME also referred to as sparsity and„ÄÇThat kind of related„ÄÇ

 but definitely dynamic aiming at different things so this is a really interesting idea of it's sparse„ÄÇ

 but you have more parameters I'll have to think about it more thank you„ÄÇYeah„ÄÇ

 like it's kind of like spot within this like giant weight matrixÔºå which is exactly YeahÔºå YeahÔºå yeah„ÄÇ

 I hadn't appreciated that„ÄÇ So I appreciate you you pointing that out„ÄÇ Thank you„ÄÇ

I have a lot of question on distillation part„ÄÇYeah first okay so if you disill it back down now you have like one technically you're back to the dense layer architecture right so now the entire like the entire idea of expert it's a certain tokens would be sent to different experts because they just like I don't know are more specialized in figuring something out about this token so now if you go back to this like dense layer like aren't you like basically only serving whatever like whichever expert you base this dense layer on like these tokens will probably perform well and all the other tokens are kind of like left behind rightÔºü

Yeah„ÄÇI'm actually sorry„ÄÇ I don't think I'm fully understanding your question„ÄÇ

 So so are you kind of getting at like we're just this on a specific data set so that don figure out how to use that like yeah yeah so maybe concretely like let's so like for super glue right like let's say you want to serve a model that does super glue well I think the idea is that like you distill the sparse model into a dense model on super gluelu So then you kind of get this compressed dense model that now performs better than if you were to just you know train it from scratch or train it from like a pretrained dense model So then it's like you use say that again„ÄÇ

YouÔºå you have to pick one expertÔºå rightÔºü NoÔºå noÔºå no„ÄÇ

 you can just distill all of the because you're just matching the the model outputs„ÄÇ

 So you can just treat the sparse model as kind of like a black box thing„ÄÇ

 All we're doing is just trying to have the dense model match the actual like final likeÔºå you know„ÄÇ

 token predictions„ÄÇ Oh GodÔºå Okay about it„ÄÇ OkayÔºå sorry I was not„ÄÇ

 I was not familiar with the idea of the disill„ÄÇ So I think that was like my entire confusion„ÄÇ Okay„ÄÇ

 YeahÔºå of courseÔºå yeahÔºå because I guess one motivation here is that„ÄÇ

Having experts can make serving a little bit more difficultÔºå because„ÄÇIt requires bigger toppologies„ÄÇ

 let's say you have eight exps„ÄÇYou need like„ÄÇWellÔºå I guess you can have multiple export on fewer calls„ÄÇ

 but„ÄÇYou knowÔºå let' just say that a little bit harder to solve„ÄÇ And so if we canÔºå you know„ÄÇ

 get the benefits from spay at per training„ÄÇAnd then use distillation to a dense model for serving be that can be beneficial„ÄÇ

 so I think that was sort of the motivation for that experimentÔºå right Eric„ÄÇYeahÔºå exactly„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_38.png)

OkayÔºå well are't we yeahÔºüYesÔºå kind go aheadÔºå BenÔºå I just said„ÄÇ

 I think one more written kind questionÔºå so yeah„ÄÇSo yeah go ahead pleasure're asking Oh yeah yeah soundss good yeah you guys for the talks so far question was wondering if you think there are any interesting directions around building models that are like explicitly optimized for parallel training I guess like the moE models seems like you know it does a really good job here and also like at inference time it's you know very useful to like you know have fewer flops for computation„ÄÇ

But or for forward passÔºå but I guess do you think that there are any interesting directions around distributed training where you might have like models that are explicitly architected to have a lot of parallel heads or other like features that are you know kind of embarrassingly parallelizable or does just using like standard you know„ÄÇ

 scale up the models by adding more layers and then just you know get away with using model and data parallelism work well enough„ÄÇ

YeahÔºå so I thinkÔºå so yeahÔºå so let me just make sure I'm fully understanding„ÄÇ So yeah„ÄÇ

 I think also likeÔºå you knowÔºå right nowÔºå like even our models are definitely very co designed with the hardware and like the shapes and things„ÄÇ

 you knowÔºå so yeah I think at a high level like yes„ÄÇ

 I think there's a ton of interesting research on like codesing the hardware„ÄÇ

 the partitioning algorithms and the models„ÄÇüòäÔºåI think given„ÄÇ

 you know that we have this kind of like SPMD mesh style partitioning„ÄÇ

 we are already kind of designing our models in ways that fit it really well„ÄÇ So for example„ÄÇ

 when we want to scale up our modelÔºå one of the first dimensions we go to scale up is the internal hidden dimension„ÄÇ

 I there some really nice properties of scaling up this dimension„ÄÇ

 It basically becomes like kind of you know independent to some of the communication costs„ÄÇ

 It's really good when looking at the compute to memory operations on these you know like compute devices and stuff„ÄÇ

YeahÔºå exactly„ÄÇ like I think when we're even designing these models„ÄÇ

 we're like really setting dimensions such that it maps well out the hardware„ÄÇ So it's almost like„ÄÇ

 you knowÔºå given that we have this model data parallelism„ÄÇ

 we're like actually designing models more for it„ÄÇ But I also think that there's a ton of new interesting distributed algorithms and stuff like that„ÄÇ

 which makes designing models very interesting„ÄÇ Like I think one thing that I think is really cool is like the Microsoft  zero partitioning too„ÄÇ

 which also like adds some new new like nice implications for like how to design and scale models and stuff„ÄÇ

 So yeahÔºå I think there's likeÔºå this is a very fruitful research direction„ÄÇ

 if that kind of answered to your question„ÄÇ YeahÔºå noÔºå that was super helpful and interesting„ÄÇ yeah„ÄÇüòä„ÄÇ

Yeah definitely like i'm very optimistic on the future of us like designing the hardwareÔºå the model„ÄÇ

 the partitioning strategies altogether because really to get it to work well you kind of have to know about all three and like kind of you know intertwine the development of them Yeah yeah that sounds awesome„ÄÇ

CoolÔºå yeahÔºå so just to summarize it's like yeah so switch transformer is like a nice simplification over a mixture of experts and we're seeing that we get really strong speed up improvements on pretrain over like a lot of the T5 models which are very strong baseline we're seeing that we can you know efficiently distill the sparse models back to dense ones„ÄÇ

and you know get improved both pretraining and fine tuning through some of these newer techniques we talked about„ÄÇ

 and we're also seeing that the models are working on multilingual data and that we can you know now easily successfully train up to you know 1„ÄÇ

6 trillion parameter modelsÔºå which is pretty promising„ÄÇAnd next slide„ÄÇ

 and so we also wanted to go into two slides about some like newer work about actually using these kind of models for computer vision and actually also a little bit of how they can be used to actually do some level of like adaptive computation„ÄÇ

 where not only now each input gets different weights„ÄÇ

 but also sometimes different inputs will have different amounts of compute applied to it„ÄÇ

And yeah so there's some really great work of doing this out of the Google Zuric team and yeah there's just doing it for image classification and you know they're basically seeing a lot of the similar types of scaling properties where you scaling up the number of experts and using sparsity allows them to get good performances on image classification„ÄÇ

Next slide„ÄÇAnd interestinglyÔºå one of the things they do is like as we talk the capacity factor„ÄÇ

 so we were talking about values of like1 1„ÄÇ252„ÄÇ0Ôºå which means like at a value of 2„ÄÇ

0 there's buffer for you know two tokens per expert„ÄÇ

 but they actually study it going less than one so that means that like at 0„ÄÇ

5 that means there's only like room for half the number of tokens„ÄÇ

And the nice part is is that they did this for image classification and also in images there's just a lot of redundancy and they notice that you can actually get really good performance by only allowing like you know up to one10th of the the parts of the image to be processed by a sparse layer„ÄÇ

 So yeahÔºå we think this is like a really nice direction too in terms of combining sparsity along with like adaptive computation„ÄÇ

üòäÔºåAnd yeahÔºå and yeahÔºå thanks so much for having us„ÄÇThat's theÔºå that's the talk„ÄÇ

It does think you better than„ÄÇSorry other fun for coming here so you know„ÄÇÈÅì„ÄÇ

So I will just like ask a bunch of questions and then we can have like after the class open question panel for the students„ÄÇ

So one thing is like have you tried using like like more like linear attention mechanisms like reformers and like all the stuff to like scale the computationÔºü

I personally haven't maybeÔºå I haven't personally done this„ÄÇYesÔºå so„ÄÇYou know„ÄÇ

 I guess we can maybe comment on how„ÄÇYou knowÔºå the attention„ÄÇ

 the cost coming from the attention maps isn't„ÄÇThe dominant curve in these large transformers„ÄÇ

So you knowÔºå the motivation for using Millar attention like perform is that it reduces the quadratic cost of attention map„ÄÇ

 rightÔºüÂóØ„ÄÇBut so farÔºå I meanÔºå at leastÔºå you knowÔºå in like sort of typical NLP setups like super gly far and so on„ÄÇ

As you scale the modelsÔºå most of the memory comes from the model weights as opposed to attention to the attention maps„ÄÇ

That's also becauseÔºå you knowÔºå using very long„ÄÇContext or sequence length„ÄÇ

Does't improve that footfall and so you know just you know walking with a vanilla self attention mechanism is a very strong baseline already„ÄÇ

Goard itÔºå okay„ÄÇSo another question is like do you think this like mechanism is even more scalable„ÄÇ

 like can you go on and build like 10 trillion parametermeter models stuff like that like what do you thinkÔºü

YeahÔºå definitely I think yeahÔºå totallyÔºå I think honestly one of the biggest constraints is that like you know„ÄÇ

 and this isn't even necessarily a constrained it's just like you have to fit the parameter somewhere and there's just limited storage on devices„ÄÇ

 but if you get enough devices such that you know yeah„ÄÇ

 you can just partition the weights it's like yeah I don't see anything stopping it„ÄÇ

Got it so what do you think like personally is your like like you thing like with the direction like like scaling of transformers will go into like will there be more like works that are trying to just like use such transformer like mechanisms make of experts or do you think thiss like you're going to be other things that the community needsÔºü

Yeah I meanÔºå I definitely think mixture of experts should find its way or at least you know sparse layers like switch transformment stuff but will definitely I think find their way into like the future of large models„ÄÇ

 I think they really confer a lot of benefits and they're also very good and like high throughput application So I think the one thing like so the one downside is on sparsity is like if you look at the performance per model weight they're going always be worse than dense models So it's like if you really are constrained on like I want to design the best model I can to fit on as small of a device as I can then they're probably not going to be the best solution because the sparse weights just aren't as good as just the dense weight that's being used for everything„ÄÇ

So I think it really depends on the applicationÔºå but I'm very optimistic for when we're training these models during pretraining with lots of data parallelism and then we're serving them in like medium the higher throughput examples„ÄÇ

 I feel like they could actually just be a pretty big win„ÄÇ

So that's kind of my thoughts on how I think scarrscity will be used in terms of other things yeah I think I don't know there's a ton of exciting research you know from everything from yeah like a lot of the linear attention stuff adaptive computation new pretraining objectives you know yeah it's hard to know what the future will look like but yeah a lot of exciting things to look forward to great sounds Okay so we can now have like a round of student questions so is totally voting„ÄÇ



![](img/cfb06f02d43700aca0402fcc8c4854ad_40.png)

![](img/cfb06f02d43700aca0402fcc8c4854ad_41.png)