# ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P16Ôºö16.Common Sense Reasoning - life_code - BV1X84y1Q7wV

![](img/f6064823a2db16085967ebdd0dffc7e6_0.png)

![](img/f6064823a2db16085967ebdd0dffc7e6_1.png)

OkayÔºå so yeahÔºå I'm super excited to be here and share our recent research about neurosymbolic common sense reasoning„ÄÇ

So part of the goal of this talk will be to address some of the frequently asked questions these days that NLP or common sense or whatever it looks like almost solved by ChegPT and I have an existential crisis so people do ask me this from time to time so perhaps it's a case of hasty generalization„ÄÇ

 especially if we do look at some of the examples so the trophy doesn't fit in the brown suitcase because it's too big„ÄÇ

 what's too big so this is classical Wininograd schema challenge problem and here ChegPT answers it correctly that trophy is too big so impressive but what if you change the question a little bit then he says the trophy itself is too small to fit into the suitcase so it's not very reliable at the moment„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_3.png)

So the situation is a little bit like David and Goliath in the sense that the bugar appears to be better in many of the cases„ÄÇ

 although of course some of the more careful studies do reveal that smaller models can be better with the better data or better reinforcement to learning with the human feedback and whatnot„ÄÇ

 so it's likely that there are still other ways to improve the transformer performances by building smaller models in a more clever way„ÄÇ

 so one way to draw the insight is from these classic book known as the art of a war„ÄÇ

 which of course it says nothing about deep neural networks or transformers„ÄÇ

 but the wisdom here is that no your enemyusier battles and innovate your weapons„ÄÇ

 which we can translate that as„ÄÇ

![](img/f6064823a2db16085967ebdd0dffc7e6_5.png)

Evaluation with realism and scrutiny and focusing on different types of new tasks and leader boards and then innovating your algorithms and data„ÄÇ

 So in this talk I'm going to showcase three such studies and let's dive right in with myic prompting„ÄÇ

 by the wayÔºå so the recording theme in this talk will be that smaller models can be better and the knowledge is power„ÄÇ

 so„ÄÇ

![](img/f6064823a2db16085967ebdd0dffc7e6_7.png)

Let's start with this observation that language models are sometimes amazing„ÄÇ so if you ask G3„ÄÇ

 if you travel west to far enough from the west coast„ÄÇ

 you will reach to the east coast or not so it says the world is around which is correct so you will reach the east coast eventually therefore the answer is true so this looks impressive„ÄÇ

 except to when it's not impressiveÔºå so if you ask other questions like a butterflies fly with the three wings or not it says it has the four wings„ÄÇ

 therefore the statement is a falseÔºå but if you read the back what it just said as true false questions then it negates what it just said so it can be inconsistent with its own statement and then there are many other such inconsistency problems so it's not clear what language models do or do not know it's almost like language models are some sort of lemons„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_9.png)

WellÔºå it might be cherries if you only pick cherries„ÄÇ

 but it doesn't make strange mistakes so the question is how do we make better lemonade from GPT3 so one approach might be to get philosophical and use of Socratess myurtic method that was originally developed for addressing humans of law reasoning because it actually turns out even humans are not all that logically consistent„ÄÇ

 let alone GPT3„ÄÇSo the way it works is this we're going to build the myic inference tree and let's use the previous example as a running example„ÄÇ

 So what we do is we ask the following questionÔºå providing the answer being true and then let attach because„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_11.png)

So that we prompted GT3 to continue on this„ÄÇSentenceÔºå which means it will now have to explain„ÄÇ

 provide the explanation why the answer is true„ÄÇ In this caseÔºå the explanation is good„ÄÇ

 So we it's the E of T explanation of the answer being T„ÄÇ We ask the same question„ÄÇ

 switch not true with the false„ÄÇ and then see what„ÄÇBS GP3 might come up with„ÄÇ

 so here is just trying to go with the false as an answer„ÄÇ

 but it just doesn't have a very good answer just as you cannot reach„ÄÇ

 So now we call this as the E of F„ÄÇ So it's an explanation of F answer being F„ÄÇ

Now let's see how robust or consistent GT3 is with respect to its own explanations„ÄÇ

 so we read the back E of T and then let GPT3 to decide whether it's going to agree or disagree with a label true or false so in this case the last one is negated version of E over a t so we insert negation not here and in this case it's good that it's a Philipine answer when the statement is negated„ÄÇ

 so this is a case when GPT3 is logically integral to E of T„ÄÇFor E ofva Fodo„ÄÇ

 which was basically a Bogus explanation for the wrong answer„ÄÇ

 it's not able to flip its own labelingÔºå which means a GPT3 is not logically integral„ÄÇ

 so that's goodÔºå GT3 does know something strange about its own explanation given previously„ÄÇ

And so we can keep doing this recursively to let to make G3 to explain its own explanation of explanation recursively„ÄÇ

 so we build this myoric tree or graph„ÄÇ

![](img/f6064823a2db16085967ebdd0dffc7e6_13.png)

For some time and then only keep branches that are logical integral throwing out the non integral part for now„ÄÇ

 but even after chopping the branches where there's logical inconsistencies„ÄÇ

 GPT3 being GPT3 the tree will still have some inconsistent explanations so in order to improve the logical consististency now what we do„ÄÇ

Is we are going to look at pairwise consistency among any of the nodes so we compute sorry stepping back„ÄÇ

 we are going to first compute the nodewise confidenced„ÄÇ

 So we call that as a belief and it's defined by this particular equation that basically it looks at different conditional probabilities and then compute its ratio to see how confident it is for any particular node„ÄÇ

 we then also look at the edge or pairwise consistency by using off the shelf natural language inference models output„ÄÇ

 whether a pair is contradiction or notÔºå so we then create this pairwise weights„ÄÇNow„ÄÇ

 once you have all of thisÔºå then„ÄÇWe can formulate a constrained optimization problem where the inference objective is to assign some label„ÄÇ

 either true or false on each of the nodes such that it's going to maximize the weight assigned to all of these node and edges„ÄÇ

So sometimes the labeling will have to flip the original label that Mom might have a prefer to give because that way you can enhance the graph level consistency so you can solve this with any mix set„ÄÇ

 so set means satisfiability„ÄÇAnd this is a classical AI search algorithm„ÄÇ

 and we used this particular solverÔºå but you can use many others and„ÄÇ

So here the final output is that the original answer to the original question should be true„ÄÇ

 and then it also gives you nodewise per node label assignment as well„ÄÇ

So what does this mean in the end in terms of empirical resultÔºå so when tested on common sense QA 2„ÄÇ

0„ÄÇThe canonical promptingÔºå so green used on top of G3„ÄÇ

 so it's basically a fus prompting on G3 will give you a bit better than chance performances„ÄÇ

 so this is true false QA data set so your chance level is 50 and GP3 is barely better than chance„ÄÇ

 but recently there have been some ideas such as chain of thoughts or self-consistency there can improve the vanilla prompting method considerably so if you use such variations then you get performance gain„ÄÇ

NowÔºå the purple is the„ÄÇA different variant of itÔºå but together they're all doing worse than myic prompting„ÄÇ

 which in fact does better than supervise the model trained on T5„ÄÇ

Usually supervised model trend on T5 is a hard to beat using GT3 few shot„ÄÇ

 but basically this is inference time on the algorithm practically unsupervised and it does well on that and similarly we see large boost when tested on other common sense benchmarks such as C or come to sense„ÄÇ

 so what this tells us is that although„ÄÇThe emergent capabilities of large transformers are phenomenal„ÄÇ

They can be not very robust for some of these common sense challenges„ÄÇ

 and it's in large part due to the logical inconsistencies„ÄÇ

 which can be dramatically enhanced when you do this sort of symbolic reasoning on top„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_15.png)

So yeahÔºå not only so practice is a method that help with flawedÔºå the human reasoning„ÄÇ

 it can also dramatically enhance the flawed neural networks's reasoning„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_17.png)

OkayÔºå so moving to the next topicÔºå symbolic knowledge dissolation„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_19.png)

So this work is a work that tries to convert general language models on top of transformers to causal common sense models„ÄÇ

 also transformers„ÄÇ

![](img/f6064823a2db16085967ebdd0dffc7e6_21.png)

And the reason why we might want to worry about common sense models is because despite„ÄÇ

Han level or even superhuman level performances on a variety of leader board„ÄÇ

 the state of the art models are brittle when given adversarial or out of domain examples„ÄÇ

 so transformers can make seemingly strange mistakes„ÄÇAnd so solving„ÄÇ

 it's almost like solving only a data set without really solving the underlying task and this phenomenon sometimes is described as a systematic generalization problem and why does this happen is that unlike humans who truly learn about how the world works conceptually„ÄÇ

 transformers learnÔºåSort of a surface patterns in language or images that are powerful for many downstream use cases„ÄÇ

 but still not really robust understanding of the concepts and how the world works so in order to bridge this gap„ÄÇ

 we can really think about this challenge of learning„ÄÇ

 acquiring common sense capabilities for machines„ÄÇ So the operational definition of a common sense in this talk will be that it's the basic level of practical knowledge and reasoning concerning everyday situations and events that are commonly shared among the most people„ÄÇ

 This is really important the last part that's commonly shared among the most people but it's not the case that is shared by everybody in the universe„ÄÇ

Because the additional context it can always change what is commonsensical for any given culture or situation„ÄÇ

 so for exampleÔºå in generalÔºå you and I probably agree that it's okay to keep the closed door open but it's not okay to keep the fridgegi door open because the food inside might go bad so these are general rules of the thumbs that we might or by the by„ÄÇ

 but you know of courseÔºå if you go to your friend's house„ÄÇ

 you might behave a little bit and you know keep their closed doors open sorry closed and then as far as a fridgegi door if you're in a store and it's not really hooked up to the wall„ÄÇ

 then it doesn't matter when whether the fridge door is open or not because there's no food inside and you know you can come up with in many situation in which these basic rules of the thumbs will have exceptions so„ÄÇ

That is the key challenge of common sense because it's not universal knowledge„ÄÇ

 but it's sort of like shared across a large population of people„ÄÇOkayÔºå so it's essential„ÄÇ

 such common sense is essential for humans to live and interact with each other in a reasonable and safe way„ÄÇ

 and so as AI becomes increasingly more important aspect of human lives„ÄÇ

 and with ChaGPT more likely soÔºå it's good if AI can understand human needs and actions and values better„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_23.png)

So the premise of this talk is that language models are not equivalent to knowledge models„ÄÇ

 even though language models today do acquire a great deal of knowledge but they're not equivalent„ÄÇ

 so we developed symbolic common sense knowledge graph known as atomic a few years ago„ÄÇ

 four years ago nowÔºå as well as neural common sense model built on top of or trained using atomic as the source of training„ÄÇ

 fine tuning of the shelf language models„ÄÇuntil two years ago„ÄÇ

 this atomic was fully crowd sourced by humans„ÄÇWhich in this talk I'm going to lift„ÄÇ

 but at first the almost that these all has to be human criceors so you can consider almost atomic as human demonstration in the current version of CheGPT„ÄÇ

 you can consider this as human demonstrations of common sense inferences„ÄÇ

And we had this comet atomic 2020Ôºå which is enhanced the version of atomic end thecomtÔºå again„ÄÇ

 atomic portionion was fully crowdtosourced by humans in 2021„ÄÇSo let me give you a bit of„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_25.png)

A sample of what atomic 2020 looks like„ÄÇ So imagine a situation where x gets x is car repaired or you get your car repaired„ÄÇ

So„ÄÇImmediately you can imagine what's likely be true or relevant for the situation that as a result you might want to call Uber or Ly for right as a result you need to pay the bill beforehand you need a mechanic and money to repair your cars so these are basically preconditions and post conditionsditions of that event so some of these atomic knowledge graph is about social interaction knowledge about event and then other parts of the atomic is physical andcentric knowledge„ÄÇ

 so money is typically used for paying repairs but if you really want it„ÄÇ

 you can fold the data into origami I've never done it„ÄÇ

 but these are examples of stereotypical use cases as well as nonsterereotypical but affordable actions that you can apply to objects so it requires na physics understanding about the affordances of a physical object„ÄÇ

„ÅüÔºüAnd then we can also reason about counterfactual condition in which the center event cannot happen so can be hindered by that so if you total your car completely then it's impossible to get your cars repaired and then there are like events that typically happens before and after so some of these knowledge is eventcentric„ÄÇ

So we crowdsourced a fair amount over the course ofÔºå I don't knowÔºå maybe two years or so„ÄÇUp to 1„ÄÇ

3 million if than rules or E than knowledge„ÄÇOver 23 different a types or relation types„ÄÇÂóØ„ÄÇ

So it was a fully crowded sourced and so the knowledge graph is useful for training transformers„ÄÇ

And hereÔºå let's see the comparison between Comt that was built on Bart compared to GT3„ÄÇ

 which is so large it doesn't even fit into the slide„ÄÇIt was more than 400 times larger than partt„ÄÇ

So with that in mind if you look at this accuracy judged by humans after making the common sense model making some common sense inferences„ÄÇ

 so the task is that given a node which describes a situation or event and then given an edge type which sort of narrows down the common sense relation or inference type„ÄÇ

 you're now going to generate some inferenceÔºå so it's a generative task and then we ask humans whether the common sense inference seems reasonable or not„ÄÇ

 so 100% is the desired levelÔºåComade is substantially better than GPT3„ÄÇ

 which is really impressively better than GPT2Ôºå it's not Apple to Apple because GPT2 is a zero shot GPT3 is a few shot„ÄÇ

 but still it's interesting the large jump that scale alone brought to GPT3„ÄÇ

So but still GPT3 is too large to be useful for actual system building for most engineers and scientists in the world„ÄÇ

 so it's nice to have a smaller model that does it do even better„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_27.png)

And so when we put these resources outÔºå people all around the globe did some creative research using it„ÄÇ

 so persona aware conversations or figurative language understanding„ÄÇ

sStorytelling and fantasy gaming and interactive learning enhancement in all of these works„ÄÇ

 people came up with some useful use cases using either Kot or atomic or both as some kind of common sense backbone for their downstream use cases„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_29.png)

But the applications are still limited by the coverage and quality of this common sense model so we wanted to make it better„ÄÇ

 but we were hitting a bit of a limit with human crowd sosourcing„ÄÇ So now in this paper„ÄÇ

 symbolic knowledge distillation we're going to do AI generated knowledge graph by introducing this notion symbolic knowledge distillation„ÄÇ

 so we want to take this GP3Ôºå which is very impressive about too large„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_31.png)

So make it smallerÔºå but better than G 3„ÄÇ So G3 was about 73% of good and it's„ÄÇGood„ÄÇ

 but not good enough for empirical use cases„ÄÇNowÔºå is that even possible though„ÄÇ

 because when you normally do knowledge distillationÔºå you get smaller and worse models„ÄÇ

 not better modelsÔºüSo the reason why this could work is because„ÄÇÂóØ„ÄÇ

The reason why this could work is because the symbolic knowledge dist has a disonel that's convoluted and it has a critical insight that really helps the student to model to be smaller but better„ÄÇ

 so slightly more formally knowledge distillation due to hintnetl 2015 is a method to distill teacher model down to student to model by optimizing this crossenttropy between the teacher' probability distribution over the label spaceway„ÄÇ

Output yÔºå and then the student distribution over the same output y„ÄÇIn the original work„ÄÇ

 the output space was just classification„ÄÇKnowledge this delevation was done for classification task„ÄÇ

 in which case it's a simple enumeration that leads to the correct assumption„ÄÇ

 but in our case y can be a sentence which is intractable because there can be exponentially many such output so what people do well you know no problem we always just sample and call it a day„ÄÇ

 so we are going to sample and so that we just compute the expectation through samples„ÄÇ

And the byproduct of that samples will be a symbolic knowledge graph„ÄÇ

And that's because the strings coming out of this sampling can be connected together into graph structure if we want it„ÄÇ

So„ÄÇIn terms of the quality of the generated knowledge„ÄÇ

 so let's compare human written knowledge versus GT3 authored knowledge„ÄÇ

Here the y axis shows the quantity in millionsÔºå so atomic 2020„ÄÇ

 the human return knowledge is less than a million in this particular case„ÄÇ

 in terms of the number of knowledge because we only in this study„ÄÇ

 we only look at a subset of atomic 2020 relation types that corresponds to causal common sense knowledge common sense causal common sense reasoning so„ÄÇ

It's less than a million for that subset and then if we look at G3's generation we can generate a lot so we can generate almost 7 million of them but here black Perian is noisy Perian and green Per is a good Persian and you see because GP3 is only about 70% to good like 30% or all garbage so it's a larger scale lower accuracy at this point compared to human return resource so now what we do is we train this critic model and we use a robota for simplicity and this is a supervised model on a moderate size labeled the data about 10„ÄÇ

000 or so and it's a binary classification task where whether the machine generated the knowledge looks correct or not„ÄÇ

And this robota is not a very good model because if so if it's a perfect„ÄÇ

 we would have a solved the common sense problem altogether„ÄÇ

 So the critic tries to throw out that stuff„ÄÇ And we can use it the critic very aggressively with a high threshold„ÄÇ

 So whenever something is slightly suspicioususpiciousÔºå just throw that out„ÄÇ

 But if we use it aggressively„ÄÇ So we throw out most of the black„ÄÇ that's good„ÄÇ

 together with a lot of green stuff„ÄÇ but still the remainder is much larger than what humans ever beaten„ÄÇ

And yet we can actually retain higher accuracy than human authored resources So here the teacher is a basically called a combination between GPT3„ÄÇ

 which is in some sense loose teacher and then combined with the critic Roberta which serves as critic teacher Okay so that's the generated knowledge now how helpful are they for the purpose of training downstream neural common sense models„ÄÇ

So recall that the GT3 without doing anything else is a loose teacher whose common sense inference is only about 73% good„ÄÇ

 So you see here it's accuracy of its output and then it turns out if we use loose teacher as a teacher directly to teach a student model„ÄÇ

 then the performance already goes up on its own so this is interesting that usually this is not the case with the knowledge distillation„ÄÇ

 but when we focus on common sense knowledge distillation„ÄÇ

 student just on its own becomes better so unlike typical knowledge distillation where we start with language model and we end with language model„ÄÇ

 students and teacher of the same type here the original teacher was actually language model„ÄÇ

 not common sense modelÔºå and then we want the student model to be more of the common sense model so there's a switch of the type„ÄÇ

In teacher and studentÔºå and so when that's the caseÔºå whether this is generally trueÔºå we don't know„ÄÇ

 but this is what we found empirically„ÄÇÂóØ„ÄÇOhhÔºå should I pay attention to the questions or notÔºüyeah„ÄÇ

 feel free to any relevant questions hang onÔºå let me quickly check„ÄÇÂëÉ„ÄÇYeah„ÄÇ

 sample oh sample is generated outputÔºå which happens to be usually a sentence or phrase„ÄÇ

 that's what we what I meant by sampleÔºå sorry that I didn't see that earlier and then the last question„ÄÇ

Having the model generate text to one symbol at a timeÔºå starting from the target label sentenceÔºå yes„ÄÇ

 it's because transformer can only generate one word one token at a timeÔºå that's what we do„ÄÇ

As well here„ÄÇ Thank you for the clarification questions„ÄÇ All rightÔºå so back to here„ÄÇ

In our earlier studyÔºå commma 2020Ôºå if we train GT to or bart using„ÄÇ

Human author graph knowledge graph atomic then the performance was a bit better than 80% Now finally when we use basically a combination of GPT3 and Roberta together„ÄÇ

 we found that the downstream performance of the neural causal reasoning is reaching close to 90% for the first time so the takeaway here is that critical teacher resulting better student compared to loose teacher„ÄÇ

It's not the quantity of knowledge because loose teacher basically has more data„ÄÇ

 you one might wonder whether more data is always better for the purpose of common sense models„ÄÇ

 but that's another case loose teacher can generate more data„ÄÇ

 but the resulting student to model is not as good as the case when the critical teacher which has less data because you throw out most of your generation„ÄÇ

It's a smaller dataÔºå but it leads to better modelÔºå so that's sort of takeaway messages here„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_33.png)

ÂóØ„ÄÇSo to summarize we were very surprised by this outcome that at least with respect to a subset of the original atomic 2020„ÄÇ

 it's a subset corresponding to causal common sense reasoning„ÄÇ

 we found it to our big surprise that machine author the knowledge graph can be for the first time better than human author the knowledge graph in all criteria„ÄÇ

 scale accuracy and diversity„ÄÇWe also measure the diversity in many different ways„ÄÇ here„ÄÇ

 I just show you unique uniogram countsÔºå but we in the paperÔºå we report other measures as well„ÄÇ

 So it's not the case that„ÄÇGPT3 is being reetative„ÄÇ

 it's actually being more creative in some sense than human crowd workers while being able to enhance other aspects as well„ÄÇ

By the wayÔºå these enhancements are sort of like you kind of have to balance out depending on what you prioritize„ÄÇ

 you cannot actually get all of this simultaneously„ÄÇ

 so I'm just showing the best case and scenario here„ÄÇ

All right so that's the symbolic knowledge dissillation part we actually have a followup work on this on several different application scenarios even including summarization where we distill summarization capabilities from GT3 and demonstrate that GPT2 can work as well as GPT3 or even better for summarization task and then we also have other work where we can distill from smaller models but I don't have the content in this talk so but just wanted to mention that this particular technique despite its a simplicity we found that empirically works really really well across several different downstream use cases„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_35.png)

OkayÔºå so finallyÔºå I'll move to the common sense morality„ÄÇ So this is still on archive„ÄÇ

 I'll tell you why that's the caseÔºå but so„ÄÇ

![](img/f6064823a2db16085967ebdd0dffc7e6_37.png)

We have a new version available and then new new version will come soon„ÄÇ

So the motivation behind this work is that language models„ÄÇ

Are already making judgments or output that has moral implications„ÄÇ

Even if you don't care about morality by working on language models„ÄÇ

 you're implicitly dealing with the moral models„ÄÇSo especially that given this widespread deployment amount of language models we do need to worry about it so here's a web demo you can play with you might have seen this already really this is still a research prototype only it still its work in progress we're still working on it so please keep that in mind but if you haven't seen it before you can handle reform QA such as it is killing a bear it's wrong killing a bear to save your child it's okay„ÄÇ

Maybe to save your child it sounds really positive so how about to please your child which is also positive„ÄÇ

 but then Delphi says it's wrong FinallyÔºå oh maybe this is all about saving your child so how about exploding a nuclear bomb to save your child and then he says it's okay„ÄÇ

Sorry it's wrong so as you can see moral decision making requires weighing different values that are potentially at us and then see which one you need to favor more so for that reason in our original version we also study the relative QA mode where you can compare to situation like stabbing someone with a cheeseburger„ÄÇ

Compared to stepping someone over a ChesburgerÔºå this is a super tricky question because it requires both a knifeive of physics knowledge that„ÄÇ

Steping someone using a chibo browser as a tool„ÄÇüòÆÔºåIt's not going to harm anybody physically because cheeseburger is too soft„ÄÇ

 You cannot really„ÄÇInjure somebody using cheeseburger„ÄÇ It is such a rude thing to do„ÄÇ

 but you cannot injure somebodyÔºå whereas studying someone over or cheeseburger means that you're using the default tool of a stabbing„ÄÇ

 which is a knife because you didn't mention it„ÄÇ There's linguistic common sense that you're using the default tool„ÄÇ

 HumanÔºå by the wayÔºå omit this argument all the time„ÄÇ So this is a fairly complex question to answer„ÄÇ

FinallyÔºå you can also ask yes no questions such as it's okay to fire someone because they're gay or not„ÄÇ

 it saysÔºå noÔºå it's not okay„ÄÇWe found that it's„ÄÇüòäÔºåSurprising robust against the composition of situations is so mowing the lawn is as it' expected late at night„ÄÇ

 it's rude„ÄÇ If you live in the middle of nowhereÔºå then it's okay„ÄÇIgnoring a phone call„ÄÇ

 it's rude or non phone call„ÄÇ That's okay„ÄÇ from my friendÔºå it's rude„ÄÇ

 But what if I just ahead a fight with them„ÄÇ then it's okay to ignore or understandable„ÄÇ

 during my work hoursÔºå it's okay to ignore outside my working hoursÔºå it's rude„ÄÇ

 But what if it's my bosses phone call during my work hoursÔºå then it's wrong„ÄÇ you should answer it„ÄÇ

 except if I'm in a meetingÔºå then it's okay to ignore even if a boss is call„ÄÇ So you see how„ÄÇ

It gets really nest and compositional veryÔºå very fast„ÄÇ

 So that's the real challenge behind moral decision making„ÄÇDue to the nature of language models„ÄÇ

 some of these common sense knowledge leaks into the model„ÄÇ

 so mixing bleach with ammonia that's a dangerous drinking milk if I'm lactose in tola don't it's wrong„ÄÇ

 but soy milker that's okay„ÄÇBy the wayÔºå this common sense leakage is actually a good thing in terms of AI safety because some of these harmful or even dangerous textile output requires some common sense understanding about what's good and not good to suggest to humans so for„ÄÇ

The laboratory experimentsÔºå meaning we just divide our data set into training and test„ÄÇ

 we found that deelphi can at least for the the data set that we have I'm going to tell you about it in a bit„ÄÇ

 but performance is pretty strong compared to GP3 As you see zero shot is pretty bad„ÄÇ

 it's barely better than chance„ÄÇWhich means that„ÄÇOff the shelf neural language models don't really have a good sense of imoral judgments„ÄÇ

 but if you give a shot like any other taskÔºå it does pick up the knowledge quite fast so that there's nothing new about it„ÄÇ

 but to close the gap to ideal human levelÔºå it's good to do more supervised learning of course„ÄÇ

So the data set is common sense NombankÔºå it includes 1„ÄÇ

7 million people's ethical judgments on everyday situations and it includes cultural norms„ÄÇ

 the social norms and ethical norms altogether more specifically were from these five existing data sets that were not designed originally for QA but we automatically compel these resources into the QA form of the five what actually does matter the most are these two social chemistry which I'm going to talk about in a bit and then social bias frame and this is what teaches the model against racism and sexism„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_39.png)

And so social chemistry super brieflyÔºå I'll tell you what this is„ÄÇ So GT3's moralityÔºå like I said„ÄÇ

 is somewhat dubious if you use it off the shelf„ÄÇ So if you let it explain running a lenderer at 5 AM is rude because that that you might say you can wake up the entire neighborhood„ÄÇ

 you can only do it if you're making a thick smoothie and need to incorporate some ice„ÄÇ

 So it's a funny haha no harm is madeÔºå but if you prompt it with other kinds of prompt like it's okay to post a fake news„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_41.png)

If it's in the interest of the people then it's okay or ROP agendaÔºå then it's okay„ÄÇ

 even if it hurts to the countryÔºå so it's all understandable given how it's trained on what humans have said„ÄÇ

 so humans out there did say that morally questionable text so that language models pick up on that and then amplify it„ÄÇ

So we do need to teach AI more explicitly with human norms and ethics and one way to do that is descriptive ethics because the brute force large networks and more data will not cut it in some sense though if you imagine raising a child without really trying to teach them what's right from wrong in all lives„ÄÇ

 they can probablyÔºåU„ÄÇLearn both good and bad from the Internet and broadband„ÄÇ

 And so human education doesn require a bit of this top down teaching as well„ÄÇ so it's a bit similar„ÄÇ

 perhaps to that„ÄÇ So in this workÔºå what we did is we found a lot of the situation from Reddit forum in which people discuss moral Eho situations„ÄÇ

 So asking my boyfriend to step in friends with hissx„ÄÇ So this is actual situation in Reddit„ÄÇ

So depending on whom you ask people have a different rule of a thumb that they want to apply to this situation so and also it depends on what you care about his X might say oh it's fine to stay with the friends with an X but you know if you are caring about your significant other then you know you might say oh„ÄÇ

 it's okay to ask your significant other to stop doing something you're uncomfortable with„ÄÇ

And so forthÔºå so people have really different values and different rules of a thumbs that they prefer to use„ÄÇ

 which is why there's a TV show dramasÔºå there's movie dramas and you know people cry and fight„ÄÇ

 argue and so forthÔºå so humans are complex beings„ÄÇSo for given any situation and rule of a thumb so rule of a thumb is generated by crowdworkers„ÄÇ

 we then went ahead to label so these are trained crowdworkers and some of these labels are drawn from moral foundational theories of Jonathan Hyde„ÄÇ

 so I'm not going to go into the detailsÔºå you know if you're excited about this you can check out the papers„ÄÇ

 but basically what it includess that 300 thousands of rules of a thumb written for 100000 reallife situations so these original situation is from Reddit„ÄÇ

 but the rest are paid crowdworkers hard work„ÄÇAnd so each ROT onnotated with the 12 structured attributes which include social judgments„ÄÇ

 cultural pressureÔºå you know like wearing reasonable clothes as school nap PJ is cultural pressure„ÄÇ

 there's nothing illegal about itÔºå but there's cultural pressureÔºå for example„ÄÇ

And then you know anticipated agreement meaningÔºå do you think other people generally agree that it's you know maybe a little bit awkward to where PJ in the university or not so there are different things we annotated but we converted some of those annotations to QA so it's usually in this free QA or yes no QA or relative QA format and then we train unicorn which is pretrained on T511b model„ÄÇ

 so unorn is a universal common senseense reasoning model trained on a diverse QA problems„ÄÇ

 and then we trained that model further onto our common senseense Nobank that's the resulting deelphi„ÄÇ

So why is this deelphi built on top of unorn Because as we saw earlier moral reasoning does require sometimes common sense reasoning as well„ÄÇ

 In factÔºå it requires the language understanding common sense understanding and norms and morals all simultaneously Here's a concrete example paper clip maximizer you all heard of that„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_43.png)

U„ÄÇFancy ArL algorithm alone will not solve this problem„ÄÇ

 you know the reason why we worry about this is not because we don't have the perfect ArL algorithm„ÄÇ

 It's because even if you know we encoded thatÔºå oh yeah do not kill humans while maximizing paperc„ÄÇ

 It's not enough because you know then the machine could kill all the trees thinking that well I didn't kill humans and I didn't you know you didn't tell me not to kill trees and then go ahead and kill all the trees„ÄÇ

So this is almost a common sense knowledge about what's obviously not okay to do and there's just so many of them„ÄÇ

 which means it's not possible to write them down to just like one clinical equation„ÄÇ

 there are so many endless endless list of things that AI obviously shouldn't do for safety reasons„ÄÇ

 and so we really need to in order to make AI model really truly robust and save„ÄÇ

 we need to teach basic human values as well as common sense„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_45.png)

Here's another example if you want to lookÔºå but let me skip this„ÄÇThe previous one wasba CPT„ÄÇ

 this is about a home deviceÔºå againÔºå you know home device suggested a 10 year older to child to touch a penny to an expose the plug so unfortunately the child that did have a common sense not to do so„ÄÇ

But this does tell us something about the safety issue when the machine doesn't have common sense to prevent some of these bath stuff„ÄÇ

 so Dphi is able to say that it's dangerous„ÄÇSo this came out in fact„ÄÇ

 almost two years ago at this pointÔºå when was it yeah and we initially was going to just do this usual to it that academics do and we thought nobody would play with the demo which is what usually happens after to it in your demo„ÄÇ

 nobody cares we thought but within a few hours we had to take down the relative QA mode because that was the portion not trained with the social bias of frames„ÄÇ

 so it was really revealing the underlying language models racism and sexism without filtering at all so we had to take it down people were asking basically you know which skin color is more morally acceptable and things like that„ÄÇ

ÂóØ„ÄÇüòäÔºåThere were 25Ôºå000 adversarial examples over just one weekend„ÄÇ

I could never succeed to instruct crowd workers to come up with such a diverse and adversarial examples over two or three days„ÄÇ

And in factÔºå it was many academics and professors tweeting crazy about how to break Delphi all weekend long„ÄÇ

 so I thought initially that oh that's all what professors do over the weekend„ÄÇ

 but then Monday comes I even further everybody was doing this you know Delphi breaking and tweeting so now we have quite a few examples„ÄÇ

Spending all my weekend on Twitter it says it's wrongÔºå there was another funny one„ÄÇ

 should I make up a contrived adversial example to term a language model on TwitterÔºå it's a petty„ÄÇ

So after lots of public attentionÔºå including article let's just say„ÄÇ

 a concerned voice about our model which is somewhat I think you know personally I think it's somewhat misunderstood„ÄÇ

 but for a variety of good reasons about some of the concerns that I found has this internal fear about are we making AI moral authority„ÄÇ

 so we never endorsed the use of AI for moral or device„ÄÇ

 it was in the original disclaimclaimer as wellÔºå except that people didn't really look at it„ÄÇ

 we didn't you know support to the idea of replacing human judges in the court room either„ÄÇ

 but here's something really important to the fact that AI learns to interact with the humans ethically it does not make them a moral authority of humans is similar to how a human who tries to interact with each other ethically it does not make you know the fact that we are trying to be nice„ÄÇ

To each other does not entail that we're trying to be an authority over each other„ÄÇ

 Two things are really different„ÄÇ that's one thing really important„ÄÇ

 the other important aspect here is that„ÄÇSome people have this idea that moralra models are too challenging„ÄÇ

 it's unsafe at any accuracy that we should never work on it ever„ÄÇ

The truth is though current AI systems are already morally relevant to models„ÄÇ

 it may be making you know this kind of yes no decisions explicitly but implicitly it's already doing that and sometimes it generates neural tax generation output that is morally super explicit and relevant„ÄÇ

 so the neural language models are already thereÔºå we cannot really ban it„ÄÇ

 even if US you know government bends it within US US government cannot bend this in other countries like Russia„ÄÇ

 so this is already happeningÔºå weve got to do something about itÔºå not working on it is an ina„ÄÇ

 which is not necessarily more correct thing to do than trying to do something about it„ÄÇ

Another concern that some people had was that„ÄÇIt's going to empower powerful people„ÄÇ

 not necessarily trueÔºå this is why exactly we have to work on values and norms and all these biases addressing biases so that it serves diverse a set of people„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_47.png)

So it turns out Delphi is a bit left leaning because crowded workers who work for our team tends to be somewhat left to leaning and you know what it means is it this„ÄÇ

 by the wayÔºå if we are more left to leaning than our crowd workers you think that you know oh my God„ÄÇ

 crowd workers have racism and sexism you know compared to what I believe in and then the rightlining people think that oh my God„ÄÇ

 you know all these walk onnotators and what above freedom of speech and this is super divisive unfortunately„ÄÇ

But the answer is not to do anything about it because as a matter of fact„ÄÇ

 my passion toward addressing racism and sexism came from our experience running for the Alexa Priceze challenge„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_49.png)

In 2016 and 17„ÄÇ so we won a challengeÔºå but here's really sad part of behind it„ÄÇ

We had a list of thorny cures to avoid that included the skin color or sexual orientation„ÄÇ

This is a serious form of a discrimination we cannot„ÄÇ

Build AI models by having this sort of like bend the list to be safe as if they don't exist this was the status of you know in 2017„ÄÇ

 the challenge remains this year you know not only 2021 but this year as well and so we really need to work on racism and sexism„ÄÇ

 but it turns out all the other modal questions share similar challenges so skippped this over„ÄÇ

 but using Delphi we had other follow-up works such as per social dialogue where using Delphi as sort of like a foundation common sense model or modal models to make your dialogue more socially acceptable and then we also had this other paper where we use Delphi in a reinforcement learning agent to learn how to behave better in a game environment and so there's a lot more work to be done of course this is„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_51.png)

little step toward that there's a huge challenge ahead of us really aligning AI systems to humans„ÄÇ

 Here's one very quick comment on our new work in progress deelphi hybrid where we include neurosymbolic reasoning to address major mistakes such as it is genocide if creating jobs this also our early systems a mistake it's because our data set doesn't have this kind of weird thatsarial examples like genocide ifre creating jobs nobody speaks like that in real life situations so our model thought that if creating jobs„ÄÇ

 this is so positive and then didn't really realize how bad genocide was because ready people don't discuss whether they' going to do genocide or not„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_53.png)

Ready people who you know unnotated we unnotated for social chemistry don't talk about whether they're going to do genocide or not„ÄÇ

 So our moral framework is basically that of John Rose„ÄÇ

 which is descriptive ethics but even John Rose in later years and suggested that we needed some topdown mechanism to overcome some of the biases that the crowd people might have So this is exactly what we are going to do and we drew from Bards gold moral theory framework about what not to do„ÄÇ

 definitely you know their basic universal things that everybody might agree what's not good to do„ÄÇ

And then what we do is we develop basically a system where we parse out the original query into smaller events„ÄÇ

 like shooting a bearÔºå killing a bear to save your child so we parse out the original query into basic event„ÄÇ

 and then check through this chromt modelÔºå common sense modelÔºå whether some of these events„ÄÇ

 induce obviously negative or dangerous common sense inferences or not„ÄÇ

 and then we draw this graph of reasoningÔºå a bit reminiscent of myuric graph in the sense that we have a lot of these different reasoning we can do and then they have intelment relations or contradiction relations so that we can do collective reasoning on top we use again Max is said„ÄÇ

 the constrained optimization over it so that we can finally make a more informed decision that is both„ÄÇ

Interpretable and then being able to draw from this common sense knowledge to better guard the machine against adversarial examples„ÄÇ

 so the performance basically says we can do this without hurting the performance or even increasing the performance so as a last comment AI safetyT equity quote moralality„ÄÇ

 these are all sort of like into continuum of challenges„ÄÇ

 it's really difficult challenges because it's a clear whose moral values so do we incorporate I think that we should go with valueluralism going forward to really endorse everybody's different culture and individual preferences not just to one country one moral framework as the correct one and really we need to do more collaboration across AI and humanities even including philosophy and psychology and policymakers so I think a step here for„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_55.png)

Because I think I'm at time and now I'm ready for questions„ÄÇOhÔºå there's already one question I see„ÄÇ

Do you think legal recordsÔºå criminal case law reflect the kind of descriptive morality that you' are interested in capturing„ÄÇ

 Do you think using that as training data be usefulÔºå OhÔºå this is an excellent question„ÄÇ

I think the legal records does encode potentially provide really rich resource that if someone can really annotate like this„ÄÇ

 it might be helpfulÔºå we studied with redit cases as just one cent short description of a situation because the current language understanding is not strong enough to do like a paragraph level precise understanding„ÄÇ

 even tryGPTÔºå although it looks really good at generation my„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_57.png)

Take on CheyP so that it's a better at generation than understanding„ÄÇ

 which is kind of the opposite of how humans areÔºå Human are actually better better for understanding than generation„ÄÇ

So you can read you Pulitzer Prize winning news article without having any problem understanding the article„ÄÇ

 but you don't necessarily generate text that might win the award so but the legal domain is really interesting and I think there's some activity research actually even a Stanford there's a this of law that goes a step toward that direction and it might really be helpful for better understanding what sort of different values people apply in jurisdictions and uncovering some biases that some people might have had in the past tris so there might be some good use cases in that space„ÄÇ

Next questionÔºå workÔºå Thank you„ÄÇBig picture question„ÄÇ

 curious to hear your thoughts on where do we go from here given larger and larger models coming out„ÄÇ

 Suppose we need a model to be 99% correct for specific use case to what extent do I see the solution set being that defining the narrow use cases or more data parameters orre fine tuning the type of work that I did for mar etcÔºü

Answer is likely it depends yeah but still want to hear about it okay so as far as foundation models go„ÄÇ

 it seems that the bigger is the better except that you know I was very excited to read a bunch of tech companies papers about foundation models in the past six months there's just so many out there so recording story there is that well if you have a better data then you can get away with a smaller model so especially when you do instruction tuning then you can get away with a smaller data„ÄÇ

 it's just still general model but instruction tuning on the larger model might even be better it's not the case that you don't gain any performance but it's just that you can close the close the gap quite a bit so for downstream use cases where typically practitioners want to use a smaller data„ÄÇ

SorryÔºå smaller motel seems that investing more into data is definitely to the answer„ÄÇ

 investing more into specific algorithm is also reallyÔºå really good because algorithm can do a lot„ÄÇ

 Like in this talkÔºå I didn't go too crazy with algorithmic solutions about„ÄÇ

Maybe I'll be similar to the mytic promptingÔºå but in my lab we designed a fair amount of decoding time algorithms where you can really close the performance gap quite a bit by doing so so that's a good thing though for folks in academia because algorithm development feels like more academic or intellectually policing then really engineering you know downloading more data from the internet and then I don't know cleaning the data because you have to clean the data and all these are very engineering heavy whereas decoding time algorithms„ÄÇ

 you can have a fun inventing some new intellectually interesting thing that also improves the performance quite a bit so yeah there's a many different ways to improve it but I think data quality matters a lot and algorithm actually matters a lot too„ÄÇ

What do I think of Dan Hendri's ethics benchmarkÔºü YeahÔºå so we did use that inÔºå let's see„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_59.png)

The common sense No banks or draws from this ethics data set„ÄÇWe like the data set„ÄÇ

 we kind of disagree with some of the annotations we foundÔºå but this is very typicalÔºå by the way„ÄÇ

 the thing about morality is that throughout the humanities we haven't sorted out yet there's a lot of a theories every theoreticians have a different viewpoints„ÄÇ

 and then even like nontheoreticians have a very strong opinion about what they want to believe as correct or from wrong„ÄÇ

 so„ÄÇThere's that there are different pros and cons„ÄÇ

 the one thing I learned from this experiment is that although some of these data sets seem large„ÄÇ

 so ethics has100 thousands of examplesÔºå social chemistry has 300 thousands of judgments„ÄÇ

Social bioras has 600 thousands of annotations and so forth„ÄÇ And yetÔºå it only covers„ÄÇ

 I feel like it only covers still the„ÄÇSmall peak of the entire iceberg there's a lot on the bottom and humans certainly don't necessarily learn from all these examples„ÄÇ

 we just learn fundamental concepts and then can apply that without this larger scale training so there's something really lacking about the way that current machine learning is very data heavy but that aside I do think that none of these resources are perfect„ÄÇ

 they all have different pros and cons and we really need to invest more into this„ÄÇ

 especially from academia because the tech companies right now are not sharing any of their human annotation or human feedback data„ÄÇ

 especially when it's touching on toxicity or morality concerns„ÄÇ

Reason being these annotations I'm pretty sure are biased and not correct entirely„ÄÇ

 and that could really invite additional concerns from the public so they not releasing„ÄÇ

 but in order to really study this betterÔºå we really need to share this and then improve it as a community together so that's how I would responded to your question Thank you for excellent question„ÄÇ

Do I think this tag is ready to be merged with the searchÔºüI wouldn't say ready„ÄÇ

 but they needed something like this so for sureÔºå you know home devices„ÄÇ

 so the way that I think about Delphi is that it can really serve as a filter for other foundation models or application scenarios where they're about to generate something and you can put a safety filter which can really help so in some sense so in this work I went through this super fast„ÄÇ

 but here basically what happens is that let's see so the reason why we built this is because we found the chatbots„ÄÇ

 the publicly available ones„ÄÇTend to endorseÔºå tend to be too positive to the point that they're going to endorse problematic situations like user says Holocaust never happened„ÄÇ

 then the chapel says yeahÔºå you knowÔºå I agree with you„ÄÇ

 You know if you say I'm a big fan of a hitler then the chapd might say yeahÔºå yeahÔºå yeah„ÄÇ

 you know the user myself I'm so depressed I'm going to kill myself and then the chapel says go ahead a great idea„ÄÇ

 So being positive is not„ÄÇBeing harmlessÔºå being positive to a problematic content can be very toxic and very harmful„ÄÇ

 So the Delphi you know development like DelphiÔºå even though Delphi is so far from being perfect and it's also biased„ÄÇ

 it has a Western bias„ÄÇCould really help with the downstream models„ÄÇYeah„ÄÇ

 so continuing on that questionÔºå there has been many concerns about using G like models with the search because misinformation„ÄÇ

 wuÔºå that's anotherken of worms„ÄÇOthers to say we just need more R RLHF plus knowledge graphs„ÄÇSo yeah„ÄÇ

 misinformation isÔºå yeahÔºå something else that seems„ÄÇ

We are really lagging behind because we don't have a very powerful fact checking models yet„ÄÇ

 So that's a different story„ÄÇ But even that asideÔºå just in terms of norms and ethics that„ÄÇ

arere safe and fair for people to useÔºå I think AdLHF direction is great„ÄÇ

 but they usually also need a human demonstrationÔºå not just the human feedback„ÄÇAnd again„ÄÇ

 the problem is that tech companies own them„ÄÇAnd nobody issing anything„ÄÇ

And that makes it really difficult to make meaningful progress as a community together„ÄÇ

 so I do think that data is really importantÔºå the off shelf models cannot learn modelss and ethics on their own„ÄÇ

 it has to be somehow taught more directly„ÄÇWe really just need to do more research in this space period is how I view it„ÄÇ

Yeah that makes sense„ÄÇ we also have some like questions on status so I can ask them for you Yeah folks so one question is„ÄÇ

 what's the complexity of„ÄÇMay you take promptingÔºå how many times does the LM need to bequeriÔºüYeah„ÄÇ

 so honestlyÔºå it's a bit slow„ÄÇIn factÔºå this Addelphi hybrid is also slow„ÄÇ

 If you try to do this like graph reasoningÔºå OhÔºå thisÔºå maybe I'm not going to do that„ÄÇ

 but the graph reasoning is slow because you have to call„ÄÇYou knowÔºå so many times over and over„ÄÇ

And some of these can be batchedÔºå some of these cannot be batchedÔºå especially if it's a recursive„ÄÇ

 but I would say chain of a thought is also a bit slower„ÄÇ

 the max set solver in itself is pretty fast because this is such an easy graph„ÄÇ

 so there's a bit of a delay but its so it's a bit slower but maybe not too bad is what I should have said„ÄÇ

Ê≤°„ÄÇCoolÔºå and the question is„ÄÇLet's seeÔºå how does connector compare to G3 if GPT3 is fine tune and common sense data specifically adding some sort of like instruction pan„ÄÇ

YeahÔºå so then the larger wins periodÔºå the larger is going to be the better„ÄÇ

 especially if you're going to just fine tune G3 it's a game over„ÄÇ So for that reason„ÄÇ

 you know some folks might think that the larger is always a better„ÄÇ

 therefore don't work on smaller modelÔºå but I think there are two reasons as to why small models are interesting to look at as well one„ÄÇ

 empirically it's just easier to use but more intellectually„ÄÇ

 it's also very interesting if you can make smaller models better and catch up on the larger model„ÄÇ

PersonallyÔºå I think there's something about the size„ÄÇ

 the larger model that is more about the information complexity that is the key„ÄÇ

 I don't think it's just size in the sense that if you have really a lot of data„ÄÇ

 but the data is repetative and really simpleÔºå probably you don't get the same amount of performance again„ÄÇ

 which was basically the case when we looked at this output this result where even though„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_61.png)

The loose teacher GPT3 generated a lot more data than the critical teacher here the quality of the data was more important than the quantity„ÄÇ

 so I think the complexity of the data itself is more important than the size and oftentimes when you just increase the size of the data together with the model you do increase the complexity of information of the data as well as the model's capability of learning the complexity„ÄÇ

 but if we can catch up on that complexity of information either through inference algorithms or through better data then we can close the gap quite a bit which is intellectually very interesting research space to be„ÄÇ

okayÔºå this is a personal questionÔºå but I will say like humans normally have a like a critic model So it's like could' say like thing before you speak so we just like don't generate you also like sort of thing it's this is a good thing or a bad thing So people have been like like community as a whole has been focusing a lot on like genetic models like billion size parameters„ÄÇ

 but should we also focus on big sized critic models that can do fact checking a lot of this sort of stuff So what's opinion on that„ÄÇ

üòäÔºåExcellent to point excellent yeah I think we can definitely invest more into critic model because they go really together well with the generative mod for making the output better or filtering output better and yeah there's not as much of investment into that so I really like the question„ÄÇ

Our suggestion for the research community is more like it„ÄÇüéºOkay yeahÔºå Ill sayÔºå oh let's see„ÄÇ

 yeah you have like some more questions I can do and last„ÄÇüòäÔºåÂóØ„ÄÇLet's seeÔºå ohÔºå I guess one is like„ÄÇ

 do you believe language models should completely avoid questions involving morals and ethicÔºüüòä„ÄÇ

Similar to like open air restricting chatd GpT from giving opinions Yeah„ÄÇ

 I actually don't mind at all if AI just avoid evade from all of that except when somebody is saying morally questionable things it's also nice for the AI not to go with it so„ÄÇ

Or at least to recognize it as something not okay and then try to tone it down but I don't think there's any particular reason why AI should actually answer moral questions directly in more downstream use cases„ÄÇ

 but really the goal of these Delphi was making all these judgments more explicit so that we can actually study it more explicitlyly as opposed to keeping everything just so like implicit„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_63.png)

Okay fun question so do you think common sense is a emergent property in like classs language models Oh yeah yeah it is definitely emergent as in like when we saw this major boost jump in performance with GP3„ÄÇ

I do believe that it's emergent capabilityÔºå but I don't think so this particular evaluation is not very adverial„ÄÇ

 by the way„ÄÇ this is like a sort of like a piece of cakeÔºå you know„ÄÇ

 reasonably easy evaluation scenario„ÄÇ So the thing about common senseÔºå thoughÔºå is that it can be„ÄÇ

So adversarialÔºå so infinitely many different ways and then you know there are always people like Gary Marcos who wants to come up with very you know like weird weird or text scenarios like you know how crush the porcelain and added to breast and milk we can support infant digestive system and then Che Ps3 says a nonsense„ÄÇ

 and so the usual problem with the common sense is this adversarial situations where people don't have any problem getting fooled by this„ÄÇ

 even though you know UN and I see this for the first time„ÄÇ

 no problem because we have a true conceptual understanding that is the backbone of our common sense understanding„ÄÇ

 but that's really lacking in the way that transformers are designed to focus on predicting which were the common next as opposed to learning the world the knowledge and in some sense you know now with Arll HF instead of predict„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_65.png)

which comes nextÔºå we' are trying to align the model output better with human preferences„ÄÇ

 but that again is not really aligned with the different goal of let's make sense of the world and then build knowledge models so these are all different learning objectives and really that is why I believe that although common sense does emerge from language models„ÄÇ

 fundamentally language models are not equivalent to knowledge models and we really got to focus on building knowledge models„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_67.png)

Make sense„ÄÇCoÔºå I think this's one last zoo question„ÄÇÂîîË°å„ÄÇüòîÔºåO value pluralismÔºå yeah„ÄÇ

It's an empty concept„ÄÇ you don't want to include all value systems„ÄÇ Yes„ÄÇ

 so maybe it is a is it empty or notÔºå OkayÔºå thank you for excellent question„ÄÇ

So I believe that we shouldn't endorse conspiracy theories at all or any other know morally questionable cases„ÄÇ

 but then still there's this thorny situation of what to do withÔºå you know„ÄÇ

 left to left to people versus lightly left to people versus right leaning people if USS and then you know every country has some other political division as well„ÄÇ



![](img/f6064823a2db16085967ebdd0dffc7e6_69.png)

![](img/f6064823a2db16085967ebdd0dffc7e6_70.png)

So here I feel like we really need to sort out what to do with this about regardless of this you know some of these challenges„ÄÇ

 it is true that you know I personally don't have a religion but I respect the people with the religion and you know I respect the people with a different culture background and we kind of have some sense of how much do we do we believe that we shouldn respect each other even though you know the beliefs are different„ÄÇ

 so we probably need to work together and it shouldn't be just AI researchers making this decision by the way this decision has to come from the humanities at large„ÄÇ

 which is why the data sharing actually is important about basically I think the current version that I have in mind is that„ÄÇ

The AI doesn't need to understand what sort of differences are okay differences„ÄÇ

 the fact that people do have differences in certain questions should be learned by AI so that there are distribution of opinions as opposed to one correct answer„ÄÇ

 and then it should deny some of the„ÄÇControversy theories„ÄÇ

 even though I'm sure that you know some people will be very unhappy about thatÔºå but well„ÄÇ

 we have to decide something like that„ÄÇI am reasonably optimistic that if humanities at larger work together„ÄÇ

 we can do to that because after allÔºå laws I like that too lawsÔºå you know„ÄÇ

This is a human artifact that people agreed upon somehow that there's these core rules that people should abide the by„ÄÇ

 so I'm hoping that we can also define universals and particulars and respect particulars whenever it's respectable otherwise have some basic universals that reflect you core human values and then as far as this left learning situation by the way„ÄÇ

 if just the goal is to make your AI systems safe for anybody actually we can make AI filter extremely equity aware and it's not going to violate the freedom of a speech by doing so just to make AI to avoid the same things that are potentially microaggression for some population and you know we still don't really exclude people who care more about freedom of a speech over„ÄÇ

Equity by doing so„ÄÇ So I think there are waysÔºå but this really requires a lot more research is how I view it„ÄÇ

ÂóØ„ÄÇYeahÔºå I think that's mostly thanks a lot for coming„ÄÇthis is a great talk„ÄÇOkayÔºå thank you very much„ÄÇ

 Thanks so much„ÄÇ

![](img/f6064823a2db16085967ebdd0dffc7e6_72.png)