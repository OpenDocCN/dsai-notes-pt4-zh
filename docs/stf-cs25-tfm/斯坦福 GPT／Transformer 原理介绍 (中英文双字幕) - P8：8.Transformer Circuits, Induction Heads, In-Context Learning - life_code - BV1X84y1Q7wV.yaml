- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P8ï¼š8.Transformer Circuits, Induction Heads,
    In-Context Learning - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P8ï¼š8.Transformer ç”µè·¯ã€è¯±å¯¼å¤´ã€ä¸Šä¸‹æ–‡å­¦ä¹  - life_code
    - BV1X84y1Q7wV
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_0.png)'
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_1.png)'
- en: Thank you all for having me it's exciting to be here one of my favorite things
    is talking about what is going on inside neural networks or at least what we're
    trying to figure out is going on inside neural networks so it's always fun to
    chat about thatã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢å¤§å®¶é‚€è¯·æˆ‘ï¼Œèƒ½åœ¨è¿™é‡Œæˆ‘å¾ˆå…´å¥‹ï¼Œæˆ‘æœ€å–œæ¬¢çš„äº‹æƒ…ä¹‹ä¸€å°±æ˜¯è°ˆè®ºç¥ç»ç½‘ç»œå†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæˆ–è€…è‡³å°‘æˆ‘ä»¬è¯•å›¾å¼„æ¸…æ¥šç¥ç»ç½‘ç»œå†…éƒ¨å‘ç”Ÿäº†ä»€ä¹ˆï¼Œå› æ­¤æ€»æ˜¯å¾ˆæœ‰è¶£èŠèŠè¿™ä¸ªã€‚
- en: ğŸ˜Šï¼Œå—¯ã€‚Oh goshï¼Œ I have to figure out how to do thingsï¼Œ okayã€‚What what I want okay
    there we go now now we are advancing slides that seems promising so I think interpretly
    means lots of different things to different people it's a very a very broad term
    and people mean all sorts of different things by itã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå—¯ã€‚å¤©å“ªï¼Œæˆ‘å¾—æƒ³åŠæ³•åšè¿™ä»¶äº‹ï¼Œå¥½å§ã€‚å¥½å§ï¼Œæˆ‘æƒ³è¦çš„å¥½å§ï¼Œç°åœ¨æˆ‘ä»¬æ­£åœ¨æ¨è¿›å¹»ç¯ç‰‡ï¼Œè¿™ä¼¼ä¹å¾ˆæœ‰å¸Œæœ›ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºâ€œå¯è§£é‡Šæ€§â€å¯¹ä¸åŒçš„äººæ„å‘³ç€å¾ˆå¤šä¸åŒçš„äº‹æƒ…ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¹¿æ³›çš„æœ¯è¯­ï¼Œäººä»¬å¯¹æ­¤æœ‰å„ç§å„æ ·çš„ç†è§£ã€‚
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_3.png)'
- en: And so I wanted to talk just briefly about the kind of interpretability that
    I spent my time thinking aboutã€‚which is what I'd call mechanistic interpretability
    so most of my work actually has not been on language models or on RNNs or transformers
    on understanding vision conves and try to understand how do the parameters in
    those models actually mapped to algorithms so you can like think of the parameters
    of a neural network as being like a compiled computer program and the neurons
    are kind of like variables or registers and somehows there are these complex computer
    programs that are embedded in those weights and we'd like to turn them back in
    to computer programs that that humans can understand it's a kind of kind of reverse
    engineering problemã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æƒ³ç®€å•è°ˆè°ˆæˆ‘æ€è€ƒçš„é‚£ç§å¯è§£é‡Šæ€§ï¼Œæˆ‘ç§°ä¹‹ä¸ºæœºæ¢°å¯è§£é‡Šæ€§ã€‚æˆ‘çš„å¤§éƒ¨åˆ†å·¥ä½œå®é™…ä¸Šå¹¶ä¸æ˜¯é’ˆå¯¹è¯­è¨€æ¨¡å‹æˆ–é€’å½’ç¥ç»ç½‘ç»œæˆ–å˜æ¢å™¨ï¼Œè€Œæ˜¯ç†è§£è§†è§‰å·ç§¯ï¼Œå¹¶è¯•å›¾ç†è§£è¿™äº›æ¨¡å‹ä¸­çš„å‚æ•°æ˜¯å¦‚ä½•æ˜ å°„åˆ°ç®—æ³•çš„ï¼Œå› æ­¤ä½ å¯ä»¥å°†ç¥ç»ç½‘ç»œçš„å‚æ•°è§†ä¸ºä¸€ç§ç¼–è¯‘è¿‡çš„è®¡ç®—æœºç¨‹åºï¼Œè€Œç¥ç»å…ƒæœ‰ç‚¹åƒå˜é‡æˆ–å¯„å­˜å™¨ï¼Œä»¥æŸç§æ–¹å¼ï¼Œè¿™äº›å¤æ‚çš„è®¡ç®—æœºç¨‹åºåµŒå…¥åœ¨è¿™äº›æƒé‡ä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›å°†å®ƒä»¬è½¬åŒ–ä¸ºäººç±»å¯ä»¥ç†è§£çš„è®¡ç®—æœºç¨‹åºï¼Œè¿™æ˜¯ä¸€ç§åå‘å·¥ç¨‹çš„é—®é¢˜ã€‚
- en: ğŸ˜Šï¼ŒAnd so this is kind of a fun example that we found where there was a car neuron
    and you could actually see that you know we have the car neuron and it's constructed
    from like a wheel neuron and it looks for in the case of the wheel neuron it's
    looking for the wheels on the bottom those are positive weights and it doesn't
    want to see them on the top so it's negative weights there and there's also a
    window neuron it's looking for the windows on the top and not on the bottom and
    so we're actually seeing there it's an algorithm it's an algorithm that goes and
    turns it's just saying you well a car is has wheel on the bottom and windows on
    the top and chromrome in the middle and that's actually like just the strongest
    neurons for that and so we're actually seeing a meaningful algorithm and that's
    not an exception that's sort of the general story that if you're willing to go
    and look at neural network weights and you're willing to invest a lot of energy
    and trying to per engineerine them there's meaningful algorithms written in the
    weights waiting for you to find themã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬æ‰¾åˆ°çš„ä¸€ä¸ªæœ‰è¶£çš„ä¾‹å­ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªæ±½è½¦ç¥ç»å…ƒï¼Œä½ å¯ä»¥å®é™…çœ‹åˆ°ï¼Œæˆ‘ä»¬æœ‰æ±½è½¦ç¥ç»å…ƒï¼Œå®ƒæ˜¯ç”±ä¸€ä¸ªè½®å­ç¥ç»å…ƒæ„æˆçš„ï¼Œå®ƒåœ¨å¯»æ‰¾åº•éƒ¨çš„è½®å­ï¼Œé‚£äº›æ˜¯æ­£æƒé‡ï¼Œè€Œå®ƒä¸æƒ³åœ¨é¡¶éƒ¨çœ‹åˆ°å®ƒä»¬ï¼Œæ‰€ä»¥é‚£æ˜¯è´Ÿæƒé‡ï¼Œè¿˜æœ‰ä¸€ä¸ªçª—æˆ·ç¥ç»å…ƒï¼Œå®ƒåœ¨å¯»æ‰¾é¡¶éƒ¨çš„çª—æˆ·è€Œä¸æ˜¯åº•éƒ¨ï¼Œæ‰€ä»¥æˆ‘ä»¬å®é™…ä¸Šçœ‹åˆ°è¿™æ˜¯ä¸€ç§ç®—æ³•ï¼Œå®ƒå°±æ˜¯åœ¨è¯´ï¼Œä½ çŸ¥é“ï¼Œä¸€è¾†è½¦åœ¨åº•éƒ¨æœ‰è½®å­ï¼Œé¡¶éƒ¨æœ‰çª—æˆ·ï¼Œä¸­é—´æœ‰è½¦èº«ï¼Œè€Œè¿™å®é™…ä¸Šå°±æ˜¯æœ€å¼ºçš„ç¥ç»å…ƒï¼Œå› æ­¤æˆ‘ä»¬å®é™…ä¸Šçœ‹åˆ°äº†ä¸€ä¸ªæœ‰æ„ä¹‰çš„ç®—æ³•ï¼Œè¿™å¹¶ä¸æ˜¯ä¸€ä¸ªä¾‹å¤–ï¼Œè¿™ç§æƒ…å†µåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªæ™®éçš„æ•…äº‹ï¼Œå¦‚æœä½ æ„¿æ„å»æŸ¥çœ‹ç¥ç»ç½‘ç»œçš„æƒé‡ï¼Œå¹¶ä¸”æ„¿æ„æŠ•å…¥å¤§é‡ç²¾åŠ›å»è¯•å›¾è¿›è¡Œåå‘å·¥ç¨‹ï¼Œé‚£ä¹ˆåœ¨è¿™äº›æƒé‡ä¸­æœ‰æ„ä¹‰çš„ç®—æ³•æ­£ç­‰ç€ä½ å»å‘ç°ã€‚
- en: And there's a bunch of reasons I think that's an interesting thing to think
    about one is you know just no one knows how to go and do the things that neural
    networks can do like no one knows how to write a computer program that can accurately
    classify imagenet let alone you know the language modeling tasks that we're doing
    no one knows how like directly write a computer program that can do the things
    that G3 does and yet somehow breaking descent is able to go and discover a way
    to do this and I want to know what's going on I want to know you know how what
    is it discovered that it can do in these systemsã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¾ˆå¤šåŸå› è®©æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„æ€è€ƒè¯¾é¢˜ã€‚å…¶ä¸­ä¸€ä¸ªå°±æ˜¯ï¼Œæ²¡æœ‰äººçŸ¥é“å¦‚ä½•å»åšç¥ç»ç½‘ç»œèƒ½å¤Ÿå®Œæˆçš„äº‹æƒ…ï¼Œæ¯”å¦‚è¯´ï¼Œæ²¡æœ‰äººçŸ¥é“å¦‚ä½•ç¼–å†™ä¸€ä¸ªå¯ä»¥å‡†ç¡®åˆ†ç±»ImageNetçš„è®¡ç®—æœºç¨‹åºï¼Œæ›´ä¸ç”¨è¯´æˆ‘ä»¬æ­£åœ¨è¿›è¡Œçš„è¯­è¨€å»ºæ¨¡ä»»åŠ¡äº†ã€‚æ²¡æœ‰äººçŸ¥é“å¦‚ä½•ç›´æ¥ç¼–å†™ä¸€ä¸ªèƒ½å¤Ÿå®ŒæˆG3æ‰€åšäº‹æƒ…çš„è®¡ç®—æœºç¨‹åºï¼Œç„¶è€Œä¸çŸ¥ä½•æ•…ï¼Œ**ç ´åæ€§ä¸‹é™**èƒ½å¤Ÿæ‰¾åˆ°ä¸€ç§æ–¹æ³•å»å®ç°è¿™ä¸€ç‚¹ã€‚æˆ‘æƒ³çŸ¥é“åˆ°åº•å‘ç”Ÿäº†ä»€ä¹ˆï¼Œæˆ‘æƒ³äº†è§£åœ¨è¿™äº›ç³»ç»Ÿä¸­å®ƒä»¬å‘ç°äº†ä»€ä¹ˆå¯ä»¥åšçš„äº‹æƒ…ã€‚
- en: There's another reason why I think this is importantã€‚which is is safety so you
    know if if we want to go and use these systems in places where they have big effect
    on the world and I think a question we need to ask ourselves is you know what
    what happens when these models have unanticipated failure modes failure modes
    we didn't know to go and test for to look for to check for how can we how can
    we discover those things especially if they're really pathological failure modes
    or the models in some sense deliberately doing something that we don't want well
    the only way that I really see that we can do that is if we can get to a point
    where we really understand what's going on inside these systemsã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™æ˜¯é‡è¦çš„å¦ä¸€ä¸ªåŸå› å°±æ˜¯å®‰å…¨æ€§ã€‚å¦‚æœæˆ‘ä»¬å¸Œæœ›åœ¨ä¼šå¯¹ä¸–ç•Œäº§ç”Ÿé‡å¤§å½±å“çš„åœ°æ–¹ä½¿ç”¨è¿™äº›ç³»ç»Ÿï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬éœ€è¦é—®è‡ªå·±ä¸€ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯ï¼šå½“è¿™äº›æ¨¡å‹å‡ºç°æ„æƒ³ä¸åˆ°çš„å¤±æ•ˆæ¨¡å¼æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œå¤±æ•ˆæ¨¡å¼æ˜¯æˆ‘ä»¬ä¸çŸ¥é“éœ€è¦å»æµ‹è¯•ã€å¯»æ‰¾æˆ–æ£€æŸ¥çš„ã€‚æˆ‘ä»¬å¦‚ä½•å‘ç°è¿™äº›äº‹æƒ…ï¼Œç‰¹åˆ«æ˜¯å¦‚æœå®ƒä»¬æ˜¯æŸç§ç¨‹åº¦ä¸Šç—…æ€çš„å¤±æ•ˆæ¨¡å¼ï¼Œæˆ–è€…æ¨¡å‹åœ¨æŸç§æ„ä¹‰ä¸Šæ•…æ„åšäº†ä¸€äº›æˆ‘ä»¬ä¸å¸Œæœ›çš„äº‹æƒ…ã€‚é‚£ä¹ˆï¼Œæˆ‘è®¤ä¸ºæˆ‘ä»¬èƒ½åšåˆ°è¿™ä¸€ç‚¹çš„å”¯ä¸€æ–¹æ³•å°±æ˜¯åˆ°è¾¾ä¸€ä¸ªçœŸæ­£ç†è§£è¿™äº›ç³»ç»Ÿå†…éƒ¨è¿ä½œçš„é˜¶æ®µã€‚
- en: ğŸ˜Šï¼ŒSo that's another reason that I'm interested in thisã€‚Now actually doing interpretly
    on language models and transformers it's new to me before this year I spent like
    eight years working on trying reverse engineer continents and vision models and
    so the ideas in this talk are new things that I've been thinking about with my
    collaborators and we're still probably a month or two maybe longer from publishing
    them and this is also the first public talk that I've given on it so know the
    things that I'm gonna to talk about there's I think honestly still a little bit
    confused for me and definitely are going to be confusing in my articulation of
    them so if I say things that are confusing know please feel free to ask me questions
    there might be some points for me to go quickly because there's a lot of content
    but definitely at the end I will be available for a while to chat out this stuff
    and yeah also I apologize if I'm unfamiliar with Zoom and make mistakes but yeah
    so with that saidã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥è¿™å°±æ˜¯æˆ‘å¯¹æ­¤æ„Ÿå…´è¶£çš„å¦ä¸€ä¸ªåŸå› ã€‚å®é™…ä¸Šï¼Œå¯¹è¯­è¨€æ¨¡å‹å’Œå˜å‹å™¨è¿›è¡Œè§£é‡Šå¯¹æˆ‘æ¥è¯´æ˜¯æ–°çš„ã€‚åœ¨è¿™ä¸€å¹´ä¹‹å‰ï¼Œæˆ‘èŠ±äº†å¤§çº¦å…«å¹´çš„æ—¶é—´è¯•å›¾é€†å‘å·¥ç¨‹å†…å®¹å’Œè§†è§‰æ¨¡å‹ï¼Œå› æ­¤è¿™æ¬¡æ¼”è®²ä¸­çš„æƒ³æ³•æ˜¯æˆ‘å’Œæˆ‘çš„åˆä½œè€…ä»¬æ€è€ƒçš„æ–°äº‹ç‰©ï¼Œæˆ‘ä»¬å¯èƒ½è¿˜è¦ä¸€åˆ°ä¸¤ä¸ªæœˆï¼Œç”šè‡³æ›´é•¿æ—¶é—´æ‰èƒ½å‘è¡¨è¿™äº›å†…å®¹ã€‚è¿™ä¹Ÿæ˜¯æˆ‘é¦–æ¬¡å…¬å¼€è¿›è¡Œç›¸å…³æ¼”è®²ï¼Œæ‰€ä»¥è¯·ç†è§£ï¼Œæˆ‘å³å°†è®¨è®ºçš„å†…å®¹å¯¹æˆ‘æ¥è¯´ä»ç„¶æœ‰äº›å›°æƒ‘ï¼Œç¡®å®ä¼šåœ¨æˆ‘çš„è¡¨è¿°ä¸­é€ æˆæ··æ·†ã€‚å¦‚æœæˆ‘è¯´çš„äº‹æƒ…è®©ä½ æ„Ÿåˆ°å›°æƒ‘ï¼Œè¯·éšæ—¶é—®æˆ‘é—®é¢˜ã€‚æœ‰äº›ç‚¹æˆ‘å¯èƒ½ä¼šå¿«é€Ÿè®²è¿°ï¼Œå› ä¸ºå†…å®¹å¾ˆå¤šï¼Œä½†åœ¨æœ€åæˆ‘ä¼šæœ‰ä¸€æ®µæ—¶é—´å¯ä»¥å’Œå¤§å®¶è®¨è®ºè¿™äº›å†…å®¹ã€‚æ­¤å¤–ï¼Œå¦‚æœæˆ‘å¯¹Zoomä¸ç†Ÿæ‚‰è€ŒçŠ¯é”™è¯¯ï¼Œæˆ‘ä¹Ÿå¾ˆæŠ±æ­‰ï¼Œä½†å°±æ˜¯è¿™æ ·ã€‚
- en: let's dive inã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_5.png)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ·±å…¥æ¢è®¨ä¸€ä¸‹ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_5.png)
- en: And so I wanted to start with a mysteryã€‚Before we go and try to actually dig
    into what's going on inside these modelsã€‚I wanted to motivate it by a really strange
    piece of behavior that we discovered and wanted to understandã€‚å—¯ã€‚And by the wayï¼Œ
    I should say all this work is done with my colleagues phanthropicã€‚and especially
    my colleaguesï¼Œ Catherine and Nelsonã€‚Okayï¼Œ so onto the mysteryã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘æƒ³ä»¥ä¸€ä¸ªè°œå›¢å¼€å§‹ã€‚åœ¨æˆ‘ä»¬è¯•å›¾æ·±å…¥äº†è§£è¿™äº›æ¨¡å‹å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…ä¹‹å‰ï¼Œæˆ‘æƒ³é€šè¿‡ä¸€ä¸ªæˆ‘ä»¬å‘ç°å¹¶æƒ³è¦ç†è§£çš„éå¸¸å¥‡æ€ªçš„è¡Œä¸ºæ¥æ¿€åŠ±è¿™ä¸€ç‚¹ã€‚å—¯ã€‚é¡ºä¾¿è¯´ä¸€å¥ï¼Œæˆ‘åº”è¯¥æåˆ°è¿™é¡¹å·¥ä½œæ˜¯å’Œæˆ‘çš„åŒäº‹**Philanthropic**ä¸€èµ·å®Œæˆçš„ï¼Œç‰¹åˆ«æ˜¯æˆ‘çš„åŒäº‹**å‡¯ç‘Ÿç³**å’Œ**çº³å°”é€Š**ã€‚å¥½çš„ï¼Œé‚£ä¹ˆè¿›å…¥è°œå›¢ã€‚
- en: I think probably the most interesting and most exciting thing about about transformers
    is their ability to do in context learning or sometimes people will call it meta
    learningã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_7.png)
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºå…³äºå˜å‹å™¨ï¼ˆtransformersï¼‰æœ€æœ‰è¶£å’Œæœ€ä»¤äººå…´å¥‹çš„äº‹æƒ…å°±æ˜¯å®ƒä»¬çš„ä¸Šä¸‹æ–‡å­¦ä¹ èƒ½åŠ›ï¼Œæˆ–è€…æœ‰æ—¶äººä»¬ç§°ä¹‹ä¸ºå…ƒå­¦ä¹ ï¼ˆmeta learningï¼‰ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_7.png)
- en: You knowï¼Œ the GT3 paper goes and describes things as you know language models
    are few shot learners like there's lots of impressive things about GT3 but they
    choose to focus on that and know now everyone's talked about prompt engineering
    and Andre Karahi was joking about how you know software 3ã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼ŒGT3è®ºæ–‡æè¿°äº†è¯­è¨€æ¨¡å‹ä½œä¸ºå°‘é‡å­¦ä¹ è€…çš„æƒ…å†µï¼ŒGT3æœ‰å¾ˆå¤šä»¤äººå°è±¡æ·±åˆ»çš„åœ°æ–¹ï¼Œä½†ä»–ä»¬é€‰æ‹©ä¸“æ³¨äºè¿™ä¸€ç‚¹ï¼Œç°åœ¨æ¯ä¸ªäººéƒ½åœ¨è®¨è®ºæç¤ºå·¥ç¨‹ï¼Œå®‰å¾·çƒˆÂ·å¡æ‹‰å¸Œè¿˜å¼€ç©ç¬‘è¯´ä½ çŸ¥é“è½¯ä»¶3ã€‚
- en: 0 is designing the prompt and so the ability of language models of these large
    transformers to respond to their context and learn from their context and change
    their behavior and response to their context you know really seems like probably
    the most surprising and striking and remarkable thing about themã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 0æ˜¯è®¾è®¡æç¤ºï¼Œå› æ­¤è¿™äº›å¤§å‹å˜æ¢å™¨è¯­è¨€æ¨¡å‹åœ¨å“åº”å…¶ä¸Šä¸‹æ–‡ã€ä»ä¸­å­¦ä¹ å¹¶æ ¹æ®å…¶ä¸Šä¸‹æ–‡æ”¹å˜è¡Œä¸ºå’Œå“åº”çš„èƒ½åŠ›ï¼Œä¼¼ä¹ç¡®å®æ˜¯å®ƒä»¬æœ€ä»¤äººæƒŠè®¶ã€å¼•äººæ³¨ç›®å’Œå“è¶Šçš„ç‰¹æ€§ã€‚
- en: And some of my colleagues previously published a paper that has a trick in it
    that I really loveã€‚which is so we're all used to looking at learning curvesã€‚you
    train your model and you you know as your model trainsï¼Œ the loss goes downã€‚The
    I is full of bit discontinuous and it goes downã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„ä¸€äº›åŒäº‹ä¹‹å‰å‘è¡¨äº†ä¸€ç¯‡æˆ‘éå¸¸å–œæ¬¢çš„è®ºæ–‡ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªæŠ€å·§ã€‚æˆ‘ä»¬éƒ½ä¹ æƒ¯äºæŸ¥çœ‹å­¦ä¹ æ›²çº¿ã€‚ä½ è®­ç»ƒæ¨¡å‹æ—¶ï¼ŒæŸå¤±ä¼šé™ä½ã€‚æ›²çº¿æœ‰äº›ä¸è¿ç»­å¹¶ä¸”ä¸‹é™ã€‚
- en: Another thing that you can do is you can go and take a fully trained model and
    you can go and ask you know as we go through the contextã€‚you know as we go and
    when we predict the first token and then the second token and the third tokenã€‚we
    get better at predicting each token because we have more information to go and
    predict it on so you know the first the first token the loss should be the entropy
    of the unigrams and then the next token should be the entro of the biograms and
    it fallsã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¯ä»¥åšçš„å¦ä¸€ä»¶äº‹æ˜¯ï¼Œå¯ä»¥æ‹¿ä¸€ä¸ªå®Œå…¨è®­ç»ƒå¥½çš„æ¨¡å‹ï¼Œé—®ä½ çŸ¥é“åœ¨æˆ‘ä»¬å¤„ç†ä¸Šä¸‹æ–‡æ—¶ã€‚ä½ çŸ¥é“å½“æˆ‘ä»¬é¢„æµ‹ç¬¬ä¸€ä¸ªæ ‡è®°ï¼Œç„¶åæ˜¯ç¬¬äºŒä¸ªæ ‡è®°å’Œç¬¬ä¸‰ä¸ªæ ‡è®°æ—¶ã€‚æˆ‘ä»¬åœ¨é¢„æµ‹æ¯ä¸ªæ ‡è®°æ—¶å˜å¾—æ›´å¥½ï¼Œå› ä¸ºæˆ‘ä»¬æœ‰æ›´å¤šçš„ä¿¡æ¯æ¥è¿›è¡Œé¢„æµ‹ï¼Œæ‰€ä»¥ä½ çŸ¥é“ç¬¬ä¸€ä¸ªæ ‡è®°çš„æŸå¤±åº”è¯¥æ˜¯å•å…ƒè¯­æ³•çš„ç†µï¼Œç„¶åä¸‹ä¸€ä¸ªæ ‡è®°åº”è¯¥æ˜¯åŒå…ƒè¯­æ³•çš„ç†µï¼Œå®ƒä¸æ–­ä¸‹é™ã€‚
- en: It keeps falling and it keeps getting betterã€‚And in some senseã€‚'s that's the
    model's ability to go and predict to go and do in context learningã€‚the ability
    to go and predict you know to be better at predicting later tokens than you are
    predicting early tokens that is in some sense a mathematical definition of what
    it means to be good at this magical in context learning or meta learning that
    these models can do so that's kind of cool because that it gives us a way to go
    and look at whether models are good at in context learningã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä¸æ–­ä¸‹é™ï¼Œä¸”ä¸æ–­å˜å¾—æ›´å¥½ã€‚ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œè¿™å°±æ˜¯æ¨¡å‹è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ï¼Œèƒ½å¤Ÿåœ¨é¢„æµ‹åç»­æ ‡è®°æ—¶æ¯”é¢„æµ‹æ—©æœŸæ ‡è®°æ›´å¥½ï¼Œä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œè¿™æ˜¯ä¸€ç§æ•°å­¦å®šä¹‰ï¼Œè¯´æ˜äº†åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æˆ–å…ƒå­¦ä¹ æ–¹é¢è¿™äº›æ¨¡å‹çš„ä¼˜ç§€ä¹‹å¤„ï¼Œè¿™çœŸæ˜¯å¤ªé…·äº†ï¼Œå› ä¸ºè¿™ç»™äº†æˆ‘ä»¬ä¸€ç§æ–¹æ³•æ¥åˆ¤æ–­æ¨¡å‹åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_9.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_9.png)'
- en: ä¸ã€‚if I could just ask a question like a clarification question please when you
    say learning there are no actual parameter of the yeah I mean that is the remarkable
    thing about in context learning right so yeah indeed we traditionally think about
    neural networks as learning over the course of training by going and modifying
    their parameters but somehow models appear to also be able to learn in some sense
    if you give them a couple examples in their context they can then go and do that
    later in their context even though no parameters changed and so it' it's some
    kind of quite different notion of learning as you're gesturing outã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ã€‚å¦‚æœæˆ‘å¯ä»¥é—®ä¸€ä¸ªæ¾„æ¸…æ€§çš„é—®é¢˜ï¼Œå½“ä½ è¯´å­¦ä¹ æ—¶ï¼Œå®é™…ä¸Šæ²¡æœ‰å‚æ•°ï¼Œæ˜¯çš„ï¼Œæˆ‘æ˜¯è¯´ä¸Šä¸‹æ–‡å­¦ä¹ çš„å¥‡å¦™ä¹‹å¤„åœ¨äºï¼Œç¡®å®ï¼Œæˆ‘ä»¬ä¼ ç»Ÿä¸Šè®¤ä¸ºç¥ç»ç½‘ç»œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€šè¿‡ä¿®æ”¹å…¶å‚æ•°æ¥å­¦ä¹ ï¼Œä½†ä¸çŸ¥æ€ä¹ˆçš„ï¼Œæ¨¡å‹ä¼¼ä¹ä¹Ÿèƒ½å¤Ÿåœ¨æŸç§æ„ä¹‰ä¸Šå­¦ä¹ ï¼Œå¦‚æœä½ ç»™ä»–ä»¬å‡ ä¸ªç¤ºä¾‹ï¼Œä»–ä»¬å°±å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œï¼Œå°½ç®¡æ²¡æœ‰å‚æ•°æ”¹å˜ï¼Œè¿™æ˜¯ä¸€ç§ç›¸å½“ä¸åŒçš„å­¦ä¹ æ¦‚å¿µã€‚
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_11.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_11.png)'
- en: Okay I think that's making more sense so I meanï¼Œ could you also just describe
    in context learning in this case as conditioning as in like conditioning on the
    first five tokens of a 10 token sentence next five tokens Yeah I think that reason
    people sometimes think about this as in contextex learning or meta learning is
    that you can do things where you like actually take a training set and you embed
    the training set in your context like if just two or three examples and then suddenly
    your model can go and do this task and so you can do few shot learning by embedding
    things in the context yeah the formal setup is that you're just conditioning on
    this context and it's just that somehow this ability like this thing like there's
    there's some sense you know for a long time people I mean I guess really the history
    of this is we started to get good at neural networks learning right and we could
    go and train language trained vision models and language models that could do
    all these remarkable things but then people started to be like well you know these
    systems are they take so many more examples thenã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæˆ‘è§‰å¾—è¿™è¶Šæ¥è¶Šæœ‰æ„ä¹‰äº†ï¼Œä½ èƒ½å¦åœ¨è¿™ç§æƒ…å†µä¸‹æè¿°ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå°±åƒæ˜¯åœ¨ä¸€ä¸ªåä¸ªæ ‡è®°çš„å¥å­çš„å‰äº”ä¸ªæ ‡è®°ä¸Šè¿›è¡Œæ¡ä»¶åŒ–ï¼Œç„¶åæ˜¯ä¸‹äº”ä¸ªæ ‡è®°ã€‚æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºäººä»¬æœ‰æ—¶ä¼šå°†è¿™è§†ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ æˆ–å…ƒå­¦ä¹ ï¼Œå› ä¸ºä½ å¯ä»¥åšä¸€äº›äº‹æƒ…ï¼Œæ¯”å¦‚å®é™…å–ä¸€ä¸ªè®­ç»ƒé›†ï¼Œå°†è®­ç»ƒé›†åµŒå…¥ä½ çš„ä¸Šä¸‹æ–‡ä¸­ï¼Œæ¯”å¦‚ä»…ä»…ä¸¤ä¸ªæˆ–ä¸‰ä¸ªç¤ºä¾‹ï¼Œç„¶åçªç„¶é—´ä½ çš„æ¨¡å‹å°±èƒ½å»åšè¿™ä¸ªä»»åŠ¡ï¼Œå› æ­¤ä½ å¯ä»¥é€šè¿‡åœ¨ä¸Šä¸‹æ–‡ä¸­åµŒå…¥äº‹ç‰©æ¥è¿›è¡Œå°‘é‡å­¦ä¹ ã€‚æ˜¯çš„ï¼Œæ­£å¼çš„è®¾ç½®æ˜¯ä½ åªæ˜¯å¯¹è¿™ä¸ªä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶åŒ–ï¼Œè€Œè¿™ç§èƒ½åŠ›å°±åƒæ˜¯æœ‰æŸç§æ„Ÿè§‰ï¼Œé•¿æœŸä»¥æ¥äººä»¬ï¼Œå®é™…ä¸Šæˆ‘æƒ³è¿™æ®µå†å²æ˜¯æˆ‘ä»¬å¼€å§‹åœ¨ç¥ç»ç½‘ç»œå­¦ä¹ ä¸Šå–å¾—è¿›å±•ï¼Œå¯¹å§ï¼Œæˆ‘ä»¬å¯ä»¥è®­ç»ƒè¯­è¨€ã€è§†è§‰æ¨¡å‹å’Œå¯ä»¥åšæ‰€æœ‰è¿™äº›æ˜¾è‘—äº‹æƒ…çš„è¯­è¨€æ¨¡å‹ï¼Œä½†éšåäººä»¬å¼€å§‹è§‰å¾—è¿™äº›ç³»ç»Ÿéœ€è¦æ›´å¤šçš„ç¤ºä¾‹ã€‚
- en: humansumans do to go and learn how can we go and fix this and we had all these
    ideas about meta learning develop where we wanted to go and and train modelsã€‚ğŸ˜¡ï¼ŒExplicititely
    to be able to learn from a few examples and people develop all these complicated
    schemes and then the like truly like absurd thing about transformer language models
    is without any effort at allã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: humansumans æ‰€åšçš„äº‹æƒ…æ˜¯å»å­¦ä¹ ï¼Œæˆ‘ä»¬è¯¥å¦‚ä½•å»è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æœ‰å¾ˆå¤šå…³äºå…ƒå­¦ä¹ çš„å‘å±•æƒ³æ³•ï¼Œæƒ³çŸ¥é“å»å“ªé‡Œè®­ç»ƒæ¨¡å‹ã€‚ğŸ˜¡ï¼Œæ˜ç¡®åœ°èƒ½å¤Ÿä»å°‘é‡ç¤ºä¾‹ä¸­å­¦ä¹ ï¼Œäººä»¬å‘å±•å‡ºè¿™äº›å¤æ‚çš„æ–¹æ¡ˆï¼ŒçœŸæ­£è’è°¬çš„æ˜¯ï¼Œå˜å‹å™¨è¯­è¨€æ¨¡å‹å‡ ä¹æ— éœ€ä»»ä½•åŠªåŠ›ã€‚
- en: we get this for free that you can go and just give them a couple examples in
    their context and they can learn in their context to go and do new things I think
    that was like like that was in some sense the like most striking thing about the
    G3 paperã€‚And soï¼Œ yeahï¼Œ this ability to go and have the just conditioning on a
    contextã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å…è´¹è·å¾—è¿™äº›ï¼Œèƒ½å¤Ÿä»…ä»…åœ¨ä¸Šä¸‹æ–‡ä¸­ç»™å‡ºå‡ ä¸ªç¤ºä¾‹ï¼Œä»–ä»¬å°±èƒ½å­¦ä¹ æ–°çš„äº‹æƒ…ï¼Œæˆ‘è®¤ä¸ºè¿™åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯ G3 è®ºæ–‡ä¸­æœ€å¼•äººæ³¨ç›®çš„äº‹æƒ…ã€‚å› æ­¤ï¼Œæ˜¯çš„ï¼Œè¿™ç§èƒ½å¤Ÿå¯¹ä¸Šä¸‹æ–‡è¿›è¡Œæ¡ä»¶åŒ–çš„èƒ½åŠ›ã€‚
- en: go and give you you knowï¼Œ new abilities for free and the ability to generalize
    to new things is in some sense the mostã€‚yeahï¼Œ to me the most striking and shocking
    thing about transformer language modelsã€‚ğŸ¤§That makes senseï¼Œ I mean I guessã€‚From
    my perspectiveã€‚I'm trying to square like the notion of learning in this case withï¼Œ
    you knowã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒå¯ä»¥å…è´¹æä¾›æ–°çš„èƒ½åŠ›ï¼Œè€Œåœ¨æŸç§æ„ä¹‰ä¸Šï¼Œèƒ½å¤Ÿæ¨å¹¿åˆ°æ–°äº‹ç‰©æ˜¯æœ€é‡è¦çš„ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œå˜å‹å™¨è¯­è¨€æ¨¡å‹æœ€å¼•äººæ³¨ç›®å’Œä»¤äººéœ‡æƒŠçš„äº‹æƒ…å°±æ˜¯è¿™ä¸€ç‚¹ã€‚ğŸ¤§è¿™å¾ˆæœ‰é“ç†ï¼Œæˆ‘æƒ³ã€‚ä»¥æˆ‘çš„è§‚ç‚¹æ¥çœ‹ï¼Œæˆ‘æ­£åœ¨åŠªåŠ›å°†è¿™ç§æƒ…å†µä¸‹çš„å­¦ä¹ æ¦‚å¿µä¸ä¼—ä¸åŒçš„ä¸œè¥¿ç»“åˆèµ·æ¥ã€‚
- en: if you or I were given a prompt of like one plus one equals two two plus three
    equals five as the sort of few shots set up and then somebody else put you know
    like five plus three equals and we had to fill it out in that case I wouldn't
    say that we've learned arithmetic because we already sort of knew it but rather
    we're just sort of conditioning on the prompt to know what it is that we should
    then generate rightï¼Ÿ
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æˆ–æˆ‘å¾—åˆ°ä¸€ä¸ªæç¤ºï¼Œæ¯”å¦‚â€œä¸€åŠ ä¸€ç­‰äºäºŒï¼ŒäºŒåŠ ä¸‰ç­‰äºäº”â€ä½œä¸ºå°‘é‡ç¤ºä¾‹è®¾ç½®ï¼Œç„¶åå…¶ä»–äººè¾“å…¥â€œåƒäº”åŠ ä¸‰ç­‰äºâ€ï¼Œæˆ‘ä»¬å¿…é¡»å¡«è¡¥è¿™ä¸ªç©ºç™½ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä¸ä¼šè¯´æˆ‘ä»¬å­¦ä¹ äº†ç®—æœ¯ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»æœ‰ç‚¹çŸ¥é“å®ƒï¼Œè€Œæ˜¯æˆ‘ä»¬åªæ˜¯åœ¨æç¤ºä¸Šè¿›è¡Œæ¡ä»¶åŒ–ï¼Œä»¥çŸ¥é“æˆ‘ä»¬åº”è¯¥ç”Ÿæˆä»€ä¹ˆï¼Œå¯¹å§ï¼Ÿ
- en: But it seems to me like that'sã€‚Yeah I think that's on a spectrum though because
    you can also go and give like completely nonsensical problems where the model
    would never have seen seen like mimic this function and give a couple examples
    of the function and the models never seen it before and I can go and do that later
    in the context and I think what you did learn in a lot of these cases so you might
    not have you might have you might not have learned atic like you might have had
    some innate faculty for arithmetic that you're using but you might have learned
    oh okay right now we're doing arithmetic problemsã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨æˆ‘çœ‹æ¥ï¼Œè¿™ä¼¼ä¹æ˜¯ä¸€ä¸ªå…‰è°±ï¼Œå› ä¸ºä½ ä¹Ÿå¯ä»¥ç»™å‡ºå®Œå…¨æ— æ„ä¹‰çš„é—®é¢˜ï¼Œæ¨¡å‹ä»æœªè§è¿‡è¿™ç§å‡½æ•°ï¼Œç»™å‡ºè¯¥å‡½æ•°çš„å‡ ä¸ªç¤ºä¾‹ï¼Œè€Œæ¨¡å‹ä¹‹å‰ä»æœªè§è¿‡å®ƒï¼Œæˆ‘å¯ä»¥ç¨ååœ¨ä¸Šä¸‹æ–‡ä¸­è¿™æ ·åšã€‚æˆ‘è®¤ä¸ºåœ¨å¾ˆå¤šæƒ…å†µä¸‹ä½ å­¦åˆ°çš„ä¸œè¥¿æ˜¯ï¼Œä½ å¯èƒ½å¹¶æ²¡æœ‰å­¦ä¹ åˆ°ç®—æœ¯ï¼Œä½ å¯èƒ½åœ¨ä½¿ç”¨æŸç§å¤©ç”Ÿçš„ç®—æœ¯èƒ½åŠ›ï¼Œä½†ä½ å¯èƒ½å­¦åˆ°äº†ï¼Œâ€œå“¦ï¼Œå¥½å§ï¼Œç°åœ¨æˆ‘ä»¬åœ¨åšç®—æœ¯é—®é¢˜â€ã€‚
- en: Got it in case this is I agree that there's like an element of semantics here
    yeah no this is helpful just to clarify exactly sort of what the what you know
    us Larry thank you for walking through thatã€‚Of courseã€‚So something that'sï¼Œ I thinkï¼Œ
    really striking about all of thisã€‚ğŸ˜Šã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜ç™½äº†ï¼Œå¦‚æœè¿™æ˜¯æˆ‘åŒæ„è¿™é‡Œå­˜åœ¨ä¸€ä¸ªè¯­ä¹‰å…ƒç´ ï¼Œæ˜¯çš„ï¼Œè¿™å¾ˆæœ‰å¸®åŠ©ï¼Œåªæ˜¯ä¸ºäº†æ¾„æ¸…ä¸€ä¸‹æˆ‘ä»¬æ‰€è®¨è®ºçš„å†…å®¹ã€‚è°¢è°¢ä½ é€æ­¥è®²è§£è¿™ä¸€åˆ‡ã€‚å½“ç„¶ã€‚æˆ‘è®¤ä¸ºè¿™ä¸€åˆ‡ä¸­æœ€å¼•äººæ³¨ç›®çš„äº‹æƒ…æ˜¯ã€‚ğŸ˜Šã€‚
- en: I well okay so we've talked about how we can we can sort of look at the learning
    curve and we can also look at this in contexttex learning curveã€‚but really those
    are just two slices of a two dimensionional spaceã€‚so like in some sense the more
    fundamental thing is how good are we at producing the endth token at a different
    given point in training and something that you'll notice if you look at this so
    we when we talk about the loss curve we're just talking about if you average over
    this dimensionã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è®¨è®ºäº†å¦‚ä½•æŸ¥çœ‹å­¦ä¹ æ›²çº¿ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŸ¥çœ‹è¿™ç§ä¸Šä¸‹æ–‡å­¦ä¹ æ›²çº¿ï¼Œä½†å®é™…ä¸Šè¿™åªæ˜¯äºŒç»´ç©ºé—´çš„ä¸¤ä¸ªåˆ‡ç‰‡ã€‚ä»æŸç§æ„ä¹‰ä¸Šè¯´ï¼Œæ›´æ ¹æœ¬çš„é—®é¢˜æ˜¯æˆ‘ä»¬åœ¨è®­ç»ƒçš„ä¸åŒç‚¹ä¸Šç”Ÿæˆæœ€åä¸€ä¸ªæ ‡è®°çš„èƒ½åŠ›å¦‚ä½•ï¼Œä½ ä¼šæ³¨æ„åˆ°ï¼Œå¦‚æœä½ æŸ¥çœ‹è¿™ä¸€ç‚¹ï¼Œå½“æˆ‘ä»¬è°ˆè®ºæŸå¤±æ›²çº¿æ—¶ï¼Œæˆ‘ä»¬åªæ˜¯åœ¨è°ˆè®ºå¦‚æœä½ åœ¨è¿™ä¸ªç»´åº¦ä¸Šå–å¹³å‡ã€‚
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_13.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_13.png)'
- en: If you if you like average like this and project on to the the training step
    thats that's your loss curve and if you the thing that we are calling the in context
    learning curve is just this lineã€‚Yeahï¼Œ this line down me the endter hereã€‚å—¯ã€‚And
    something that's kind of striking is there's there's this discontinuity in it
    like there's this point where where you know the model seems to get radically
    better in a veryã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å–œæ¬¢è¿™æ ·çš„å¹³å‡å€¼å¹¶å°†å…¶æŠ•å½±åˆ°è®­ç»ƒæ­¥éª¤ä¸Šï¼Œé‚£å°±æ˜¯ä½ çš„æŸå¤±æ›²çº¿ï¼Œè€Œæˆ‘ä»¬ç§°ä¹‹ä¸ºä¸Šä¸‹æ–‡å­¦ä¹ æ›²çº¿çš„ä¸œè¥¿å°±æ˜¯è¿™æ¡çº¿ã€‚æ˜¯çš„ï¼Œè¿™æ¡çº¿åœ¨è¿™é‡Œç»“æŸã€‚å—¯ã€‚æœ‰ä¸€ç‚¹å¼•äººæ³¨ç›®çš„æ˜¯ï¼Œå®ƒå­˜åœ¨ä¸€ä¸ªä¸è¿ç»­ç‚¹ï¼Œå°±åƒæœ‰ä¸€ä¸ªæ—¶åˆ»ï¼Œæ¨¡å‹ä¼¼ä¹åœ¨æŸä¸ªæ–¹é¢å˜å¾—æä¸ºä¼˜ç§€ã€‚
- en: very short time span and going and predicting late tokensã€‚So it's not that different
    in early time stepsï¼Œ but in late time stepsï¼Œ suddenly you get betterã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_15.png)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨éå¸¸çŸ­çš„æ—¶é—´è·¨åº¦å†…è¿›è¡Œé¢„æµ‹åæœŸæ ‡è®°ã€‚æ‰€ä»¥åœ¨æ—©æœŸæ—¶é—´æ­¥é•¿ä¸­å¹¶æ²¡æœ‰å¤ªå¤§ä¸åŒï¼Œä½†åœ¨åæœŸæ—¶é—´æ­¥é•¿ä¸­ï¼Œçªç„¶ä½ å˜å¾—æ›´å¥½ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_15.png)
- en: And a way that you can make this more striking is you can take the difference
    in your ability to predict the 50th token and your ability to predict the 500th
    token you can subtract from the 500th tokenã€‚the 50th token lossã€‚And what you see
    is that over the course of trainingã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿è¿™ä¸€ç‚¹æ›´åŠ æ˜¾è‘—çš„ä¸€ç§æ–¹æ³•æ˜¯ï¼Œä½ å¯ä»¥è®¡ç®—é¢„æµ‹ç¬¬50ä¸ªæ ‡è®°çš„èƒ½åŠ›ä¸é¢„æµ‹ç¬¬500ä¸ªæ ‡è®°çš„èƒ½åŠ›ä¹‹é—´çš„å·®å¼‚ï¼Œä½ å¯ä»¥ä»ç¬¬500ä¸ªæ ‡è®°ä¸­å‡å»ç¬¬50ä¸ªæ ‡è®°çš„æŸå¤±ã€‚ä½ ä¼šçœ‹åˆ°ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ã€‚
- en: you know you're not very good at this and you get a little bit better and then
    suddenly you have this cliff and then you never get better the difference between
    these at least never gets better so the model gets better at predicting things
    but its ability to go and predict late tokens over early tokens never gets betterã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œä½ å¯¹æ­¤å¹¶ä¸æ˜¯å¾ˆæ“…é•¿ï¼Œä½ å˜å¾—ç¨å¾®å¥½ä¸€ç‚¹ï¼Œç„¶åçªç„¶ä½ æœ‰äº†ä¸€ä¸ªæ‚¬å´–ï¼Œç„¶åä½ å†ä¹Ÿä¸ä¼šå˜å¾—æ›´å¥½äº†ï¼Œè‡³å°‘è¿™ä¸¤è€…ä¹‹é—´çš„å·®å¼‚æ°¸è¿œä¸ä¼šæ”¹å–„ï¼Œå› æ­¤æ¨¡å‹åœ¨é¢„æµ‹äº‹ç‰©æ—¶å˜å¾—æ›´å¥½ï¼Œä½†å®ƒåœ¨é¢„æµ‹åæœŸæ ‡è®°ä¸æ—©æœŸæ ‡è®°ä¹‹é—´çš„èƒ½åŠ›æ°¸è¿œä¸ä¼šæé«˜ã€‚
- en: ğŸ˜¡ï¼ŒAnd so there's in the span of just a few hundred steps in trainingã€‚the model
    has gotten radically better at its ability to go and do this kind of in context
    learningã€‚And so he might askï¼Œ you knowï¼Œ what's going on at that pointï¼ŸAnd this
    is just one modelã€‚but well so first of all it's worth noting this isn't a small
    changeã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜¡ï¼Œæ‰€ä»¥åœ¨ä»…ä»…å‡ ç™¾ä¸ªè®­ç»ƒæ­¥éª¤çš„æ—¶é—´è·¨åº¦å†…ï¼Œæ¨¡å‹åœ¨è¿›è¡Œè¿™ç§ä¸Šä¸‹æ–‡å­¦ä¹ çš„èƒ½åŠ›ä¸Šæœ‰äº†æ˜¾è‘—çš„æå‡ã€‚æ‰€ä»¥ä»–å¯èƒ½ä¼šé—®ï¼Œé‚£æ—¶å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿè¿™åªæ˜¯ä¸€ä¸ªæ¨¡å‹ï¼Œä½†é¦–å…ˆå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªå°çš„å˜åŒ–ã€‚
- en: so can we don't think about this very oftenï¼Œ but often we just look at law scoress
    we' like did the model do better than another model or worse than another model
    but you can think about this as in terms of NAs and it's just the information
    the quantity and NA and you can convert that into Tibitã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬å¹¶ä¸å¸¸å¸¸è€ƒè™‘è¿™ä¸€ç‚¹ï¼Œä½†é€šå¸¸æˆ‘ä»¬åªæ˜¯å…³æ³¨æŸå¤±åˆ†æ•°ï¼Œæ¯”å¦‚æ¨¡å‹æ˜¯å¦æ¯”å¦ä¸€ä¸ªæ¨¡å‹è¡¨ç°æ›´å¥½æˆ–æ›´å·®ï¼Œä½†ä½ å¯ä»¥ä»NAsçš„è§’åº¦è€ƒè™‘è¿™ä¸€ç‚¹ï¼Œè¿™åªæ˜¯ä¿¡æ¯çš„é‡å’ŒNAï¼Œä½ å¯ä»¥å°†å…¶è½¬æ¢ä¸ºTibitã€‚
- en: And so like one way you can interpret this is it's something roughly like you
    know the model 0ã€‚4nas is about 0ã€‚5 bits is about like every other token the model
    gets to go and sample twice and pick a better one it's actually it's even stronger
    than that that's sort of an underestimate of how big a deal going and getting
    better by 0ã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œä½ å¯ä»¥è¿™æ ·ç†è§£ï¼šæ¨¡å‹0.4naså¤§çº¦æ˜¯0.5æ¯”ç‰¹ï¼Œå·®ä¸å¤šæ¯ä¸ªå…¶ä»–çš„æ ‡è®°æ¨¡å‹éƒ½å¯ä»¥å»é‡‡æ ·ä¸¤æ¬¡å¹¶é€‰æ‹©æ›´å¥½çš„ä¸€ä¸ªã€‚å®é™…ä¸Šï¼Œæƒ…å†µæ¯”è¿™æ›´å¼ºï¼Œè¿™åªæ˜¯å¯¹è·å¾—æ›´å¥½ç»“æœçš„ä¸€ä¸ªä½ä¼°ã€‚
- en: 4nases so this is like a real big difference in the model's ability to go and
    predict late tokensã€‚And we can visualize this in different ways can we can also
    go and ask you know how much better are we getting at going and predicting later
    tokens and look at the derivative and then we can see very clearly that there's
    there's some kind of discontinuity in that derivative at this point and we can
    take the second derivative then and we can with well derivative with respect to
    training and now we see that there's like there's very very clearly this lying
    here so something and just the span of a few steps and a few hundred steps is
    causing some big change and we have some kind of phase change going onã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æ¨¡å‹åœ¨é¢„æµ‹åç»­æ ‡è®°æ—¶èƒ½åŠ›çš„çœŸæ­£å·¨å¤§å·®å¼‚ã€‚æˆ‘ä»¬å¯ä»¥ç”¨ä¸åŒçš„æ–¹å¼å¯è§†åŒ–è¿™ä¸€ç‚¹ï¼Œä¹Ÿå¯ä»¥é—®ä¸€ä¸‹æˆ‘ä»¬åœ¨é¢„æµ‹åç»­æ ‡è®°æ—¶çš„æ”¹å–„ç¨‹åº¦ï¼Œå¹¶æŸ¥çœ‹å¯¼æ•°ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°åœ¨è¿™ä¸€ç‚¹ä¸Šå¯¼æ•°å­˜åœ¨æŸç§ä¸è¿ç»­æ€§ï¼Œæˆ‘ä»¬å¯ä»¥å–äºŒé˜¶å¯¼æ•°ï¼Œå¹¶ç›¸å¯¹äºè®­ç»ƒè¿›è¡Œæ¨å¯¼ï¼Œç°åœ¨æˆ‘ä»¬çœ‹åˆ°åœ¨è¿™å‡ æ­¥å’Œå‡ ç™¾æ­¥ä¹‹é—´å­˜åœ¨æ˜æ˜¾çš„å˜åŒ–ï¼Œè¿™å¯¼è‡´äº†æŸç§å¤§çš„å˜åŒ–ï¼Œæˆ‘ä»¬æœ‰æŸç§ç›¸å˜å‘ç”Ÿã€‚
- en: And this is true across model sizesã€‚And you can actually see it a little bit
    in the loss curve and there's this little bump here and that corresponds to the
    point where you have this change we actually could have seen in the loss curve
    earlier tooã€‚it's this bump hereã€‚Excuse me so so we have this phase change going
    on and there's I think a really tempting theory to have which is that somehow
    whatever you know there's some this change in the model's output and its behaviors
    and in the sort of outward facing properties corresponds presumably to some kind
    of change in the algorithms that are running inside the model so if we observe
    this big phase changeã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç‚¹åœ¨ä¸åŒæ¨¡å‹è§„æ¨¡ä¸­éƒ½æ˜¯å¦‚æ­¤ã€‚ä½ å®é™…ä¸Šå¯ä»¥åœ¨æŸå¤±æ›²çº¿ä¸­çœ‹åˆ°ä¸€ç‚¹ç‚¹å˜åŒ–ï¼Œè¿™ä¸ªå°å³°å€¼å¯¹åº”äºæˆ‘ä»¬å¯ä»¥æ›´æ—©çœ‹åˆ°çš„æŸå¤±æ›²çº¿ä¸­çš„å˜åŒ–ã€‚å°±æ˜¯è¿™ä¸ªå³°å€¼ã€‚æŠ±æ­‰ï¼Œæ‰€ä»¥æˆ‘ä»¬æœ‰è¿™ç§ç›¸å˜å‘ç”Ÿï¼Œæˆ‘è®¤ä¸ºæœ‰ä¸€ä¸ªå¾ˆè¯±äººçš„ç†è®ºï¼Œå°±æ˜¯æ— è®ºå¦‚ä½•ï¼Œæ¨¡å‹è¾“å‡ºå’Œè¡Œä¸ºçš„å˜åŒ–ï¼Œä»¥åŠå¤–éƒ¨è¡¨ç°çš„å±æ€§ï¼Œå¯èƒ½å¯¹åº”äºæ¨¡å‹å†…éƒ¨ç®—æ³•çš„æŸç§å˜åŒ–ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬è§‚å¯Ÿåˆ°è¿™ç§å¤§çš„ç›¸å˜ã€‚
- en: especially in a very small window in the model's behaviorã€‚presumably there's
    some change in the circuits inside the model that is drivingã€‚At least that's a
    you know a natural hypothesis so if we want to ask that though we need to go and
    be able to understandã€‚you know what are the algorithms that's running inside the
    modelã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å°¤å…¶æ˜¯åœ¨æ¨¡å‹è¡Œä¸ºçš„ä¸€ä¸ªéå¸¸å°çš„çª—å£ä¸­ã€‚å¯ä»¥æ¨æµ‹æ¨¡å‹å†…éƒ¨ç”µè·¯çš„æŸäº›å˜åŒ–æ­£åœ¨æ¨åŠ¨è¿™ä¸€ç‚¹ã€‚è‡³å°‘è¿™æ˜¯ä¸€ä¸ªè‡ªç„¶çš„å‡è®¾ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬æƒ³è¦é—®è¿™ä¸ªï¼Œæˆ‘ä»¬éœ€è¦èƒ½å¤Ÿç†è§£æ¨¡å‹å†…éƒ¨è¿è¡Œçš„ç®—æ³•ã€‚
- en: how can we turn the parameters in the model back into this algorithm so that's
    going to be our goalã€‚Now it's going to require us to cover a lot of ground in
    a relatively short amount of timeã€‚so I'm going to go a little bit quickly through
    the next section and I will highlight sort of the key takeaways and then I will
    be very happy to go and you know explore any of this in as much depthã€‚I'm free
    for another hour after this call and just happy to talk in as much depth as people
    want about the details of thisã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•å°†æ¨¡å‹ä¸­çš„å‚æ•°è½¬åŒ–ä¸ºè¿™ä¸ªç®—æ³•ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬çš„ç›®æ ‡ã€‚ç°åœ¨è¿™éœ€è¦æˆ‘ä»¬åœ¨ç›¸å¯¹è¾ƒçŸ­çš„æ—¶é—´å†…è¦†ç›–å¤§é‡å†…å®¹ï¼Œæ‰€ä»¥æˆ‘å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ç¨å¾®å¿«ä¸€ç‚¹ï¼Œå¹¶çªå‡ºä¸€äº›å…³é”®è¦ç‚¹ï¼Œç„¶åæˆ‘éå¸¸ä¹æ„æ·±å…¥æ¢è®¨å…¶ä¸­ä»»ä½•å†…å®¹ã€‚é€šè¯åæˆ‘è¿˜æœ‰ä¸€ä¸ªå°æ—¶ï¼Œéšæ—¶æ„¿æ„æ·±å…¥è®¨è®ºç»†èŠ‚ã€‚
- en: ğŸ˜Šã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_17.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_17.png)
- en: So it turns out the space change doesn't happen in a one layer attentionally
    transformer and it does happen in a two layer attentionally transformerã€‚so if
    we could understand a one layer attentionally transformer and a two layer only
    attentionally transformer that might give us a pretty big clue as to what's going
    onã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥äº‹å®è¯æ˜ï¼Œåœ¨å•å±‚æ³¨æ„åŠ›å˜æ¢å™¨ä¸­ï¼Œç©ºé—´å˜åŒ–å¹¶æ²¡æœ‰å‘ç”Ÿï¼Œè€Œåœ¨åŒå±‚æ³¨æ„åŠ›å˜æ¢å™¨ä¸­æ˜¯ä¼šå‘ç”Ÿçš„ã€‚å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿç†è§£å•å±‚æ³¨æ„åŠ›å˜æ¢å™¨å’ŒåŒå±‚ä»…æ³¨æ„åŠ›å˜æ¢å™¨ï¼Œè¿™å¯èƒ½ä¼šç»™æˆ‘ä»¬ä¸€ä¸ªå¾ˆå¤§çš„çº¿ç´¢ï¼Œå¸®åŠ©æˆ‘ä»¬ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆã€‚
- en: ğŸ˜¡ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_19.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜¡ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_19.png)
- en: hanã€‚So we're attentionally we're also going to leave out layer norm and biases
    to simplify things so you know you one way you could describe a attentionally
    transformer is we're going to embed our tokensã€‚And then we're going to apply a
    bunch of attention heads and add them into the residual stream and then apply
    our un embedding and that like give us our logicsã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨æ³¨æ„åŠ›ä¸Šä¹Ÿå°†çœç•¥å±‚å½’ä¸€åŒ–å’Œåç½®ï¼Œä»¥ç®€åŒ–äº‹æƒ…ã€‚æ‰€ä»¥ä½ çŸ¥é“ï¼Œä¸€ç§æè¿°æ³¨æ„åŠ›å˜æ¢å™¨çš„æ–¹å¼æ˜¯æˆ‘ä»¬å°†åµŒå…¥æˆ‘ä»¬çš„æ ‡è®°ã€‚ç„¶åæˆ‘ä»¬å°†åº”ç”¨ä¸€å †æ³¨æ„åŠ›å¤´å¹¶å°†å®ƒä»¬æ·»åŠ åˆ°æ®‹å·®æµä¸­ï¼Œç„¶ååº”ç”¨æˆ‘ä»¬çš„è§£åµŒå…¥ï¼Œè¿™æ ·å°±èƒ½å¾—åˆ°æˆ‘ä»¬çš„é€»è¾‘ã€‚
- en: And we could go and write that out as equations if we want multiply by an embedding
    matrixã€‚ğŸ˜Šã€‚Apply attention headsã€‚And then compute the logsï¼Œ print the animeã€‚Andã€‚And
    the part here that's a little tricky is understanding the attention heads and
    this might be a somewhat conventional way of describing attention end and it actually
    kind of obscures a lot of the structure of attention in and I think that oftentimes
    we make attention heads more complex than they are we sort of hide the interesting
    structure so what is this saying what's saying you for every token compute value
    value vector and then go and mix the value vectors according to the attention
    matrix and then project them with the output matrix back into the residual stringã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†å…¶å†™ä¸ºæ–¹ç¨‹å¼ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ä¹˜ä»¥åµŒå…¥çŸ©é˜µã€‚ğŸ˜Šã€‚åº”ç”¨æ³¨æ„åŠ›å¤´ã€‚ç„¶åè®¡ç®—æ—¥å¿—ï¼Œæ‰“å°åŠ¨ç”»ã€‚è€Œè¿™é‡Œæœ‰ä¸€ç‚¹æ£˜æ‰‹çš„æ˜¯ç†è§£æ³¨æ„åŠ›å¤´ï¼Œè¿™å¯èƒ½æ˜¯æè¿°æ³¨æ„åŠ›çš„ä¸€ç§ä¼ ç»Ÿæ–¹å¼ï¼Œå®é™…ä¸Šæ¨¡ç³Šäº†æ³¨æ„åŠ›çš„ç»“æ„ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬é€šå¸¸ä¼šä½¿æ³¨æ„åŠ›å¤´æ¯”å®é™…æ›´å¤æ‚ï¼Œéšè—äº†æœ‰è¶£çš„ç»“æ„ã€‚é‚£ä¹ˆè¿™æ˜¯ä»€ä¹ˆæ„æ€å‘¢ï¼Ÿå®ƒçš„æ„æ€æ˜¯å¯¹äºæ¯ä¸ªæ ‡è®°è®¡ç®—å€¼å‘é‡ï¼Œç„¶åæ ¹æ®æ³¨æ„åŠ›çŸ©é˜µæ··åˆå€¼å‘é‡ï¼Œç„¶åç”¨è¾“å‡ºçŸ©é˜µå°†å®ƒä»¬æŠ•å½±å›æ®‹å·®æµã€‚
- en: ğŸ˜Šï¼ŒAnd so there's there's another notation which you could think of this as a
    as using tensor products or using usingã€‚Wellï¼Œ I guess left and right multiplying
    there's a few ways you can interpret thisã€‚but I'll just sort of try to explain
    what this notation meansã€‚14å¹´çº§ã€‚ğŸ˜Šã€‚For every x our residual streamï¼Œ we have a vector
    for every single tokenã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥è¿˜æœ‰å¦ä¸€ç§ç¬¦å·è¡¨ç¤ºæ³•ï¼Œä½ å¯ä»¥æŠŠå®ƒçœ‹ä½œæ˜¯ä½¿ç”¨å¼ é‡ç§¯ï¼Œæˆ–è€…ä½¿ç”¨ã€‚å¥½å§ï¼Œæˆ‘æƒ³å·¦ä¹˜å’Œå³ä¹˜ï¼Œä½ å¯ä»¥ç”¨å‡ ç§æ–¹å¼æ¥è§£é‡Šè¿™ä¸€ç‚¹ã€‚ä½†æˆ‘ä¼šå°½é‡è§£é‡Šè¿™ä¸ªç¬¦å·çš„æ„æ€ã€‚14å¹´çº§ã€‚ğŸ˜Šã€‚å¯¹äºæˆ‘ä»¬çš„æ¯ä¸ª
    x æ®‹å·®æµï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªé’ˆå¯¹æ¯ä¸ªå•ç‹¬æ ‡è®°çš„å‘é‡ã€‚
- en: And this means go and multiply independently the vector for each token by Wvã€‚so
    compute the value vector for every tokenã€‚ğŸ˜¡ï¼ŒThis oneï¼Œ on the other handã€‚means notice
    that it's now on the we A is on the left hand sideã€‚It means goingã€‚go and multiply
    theã€‚Attention matrix or going go into linear combinations of the values of value
    vectors so don't don't change the value vectorsã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€å»ç‹¬ç«‹åœ°å°†æ¯ä¸ªæ ‡è®°çš„å‘é‡ä¹˜ä»¥ Wvã€‚æ‰€ä»¥è®¡ç®—æ¯ä¸ªæ ‡è®°çš„å€¼å‘é‡ã€‚ğŸ˜¡ï¼Œå¦ä¸€æ–¹é¢ï¼Œè¿™æ„å‘³ç€æ³¨æ„åˆ°ç°åœ¨æˆ‘ä»¬ A åœ¨å·¦ä¾§ã€‚è¿™æ„å‘³ç€å»åšã€‚å»ä¹˜ä»¥æ³¨æ„åŠ›çŸ©é˜µï¼Œæˆ–è€…è¿›è¡Œå€¼å‘é‡çš„çº¿æ€§ç»„åˆï¼Œæ‰€ä»¥ä¸è¦æ”¹å˜å€¼å‘é‡ã€‚
- en: you know point wiseï¼Œ but go and mix them together according to the attention
    patternã€‚create a weighted sumã€‚And then againï¼Œ independently for every positionã€‚go
    and apply the output matrixã€‚And you can apply the distributor property to this
    and it just reveals that actually didn't matter that you did the attention sort
    of in the middleã€‚you could have done the attention at the beginning you could
    have done it at the endã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“é€ç‚¹çš„ï¼Œä½†æ ¹æ®æ³¨æ„åŠ›æ¨¡å¼å°†å®ƒä»¬æ··åˆåœ¨ä¸€èµ·ã€‚åˆ›å»ºåŠ æƒå’Œã€‚ç„¶åï¼Œå†æ¬¡ï¼Œå¯¹äºæ¯ä¸ªä½ç½®ï¼Œå»åº”ç”¨è¾“å‡ºçŸ©é˜µã€‚ä½ å¯ä»¥å°†åˆ†é…å±æ€§åº”ç”¨äºæ­¤ï¼Œå®ƒåªä¼šæ­ç¤ºå®é™…ä¸Šåœ¨ä¸­é—´è¿›è¡Œæ³¨æ„åŠ›å¹¶ä¸é‡è¦ã€‚ä½ å¯ä»¥åœ¨å¼€å§‹æ—¶è¿›è¡Œæ³¨æ„åŠ›ï¼Œä¹Ÿå¯ä»¥åœ¨ç»“æŸæ—¶è¿›è¡Œã€‚
- en: that's the independent and the thing that actually matters is there's this WVWO
    matrix that describes what it's really saying is you know WVWO describes what
    information the attention head reads from each position and how it writes it to
    its destinationã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç‹¬ç«‹çš„ï¼Œå®é™…ä¸Šé‡è¦çš„æ˜¯æœ‰ä¸€ä¸ª WVWO çŸ©é˜µï¼Œå®ƒæè¿°äº†æ³¨æ„åŠ›å¤´ä»æ¯ä¸ªä½ç½®è¯»å–çš„ä¿¡æ¯ä»¥åŠå¦‚ä½•å°†å…¶å†™å…¥ç›®æ ‡ã€‚
- en: whereas A describes which tokens we read from and write toã€‚And that's kind of
    getting more fundamental structure and attention head an attention head goes and
    moves information from one position to another and the process of which position
    gets moved from and too is independent from what information gets movedã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: è€ŒAæè¿°äº†æˆ‘ä»¬ä»å“ªä¸ªä»¤ç‰Œè¯»å–å’Œå†™å…¥ã€‚è¿™ç§æè¿°æ›´ä¸ºåŸºæœ¬ï¼Œæ³¨æ„åŠ›å¤´è´Ÿè´£å°†ä¿¡æ¯ä»ä¸€ä¸ªä½ç½®ç§»åŠ¨åˆ°å¦ä¸€ä¸ªä½ç½®ï¼Œè€Œå“ªä¸ªä½ç½®è¢«ç§»åŠ¨åˆ°å“ªä¸ªä½ç½®çš„è¿‡ç¨‹ä¸ç§»åŠ¨ä»€ä¹ˆä¿¡æ¯æ˜¯ç‹¬ç«‹çš„ã€‚
- en: And if you rewrite your transformer that wayï¼Œ well first we can go andr the
    sum of attention heads just as in this formã€‚And then we can go and write that
    as the entire layer by going and adding an identityã€‚And if we go and plug that
    all in to our transformer and go and expandã€‚Andã€‚we have to go and multiply everything
    throughï¼Œ we get this interesting equationã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ä»¥è¿™ç§æ–¹å¼é‡å†™ä½ çš„å˜æ¢å™¨ï¼Œé¦–å…ˆæˆ‘ä»¬å¯ä»¥å°†æ³¨æ„åŠ›å¤´çš„æ€»å’Œå†™æˆè¿™ç§å½¢å¼ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡æ·»åŠ ä¸€ä¸ªæ’ç­‰æ˜ å°„æ¥å†™å‡ºæ•´ä¸ªå±‚ã€‚å¦‚æœæˆ‘ä»¬å°†è¿™ä¸€åˆ‡ä»£å…¥æˆ‘ä»¬çš„å˜æ¢å™¨å¹¶è¿›è¡Œå±•å¼€ã€‚æˆ‘ä»¬å¿…é¡»å°†æ‰€æœ‰å†…å®¹ç›¸ä¹˜ï¼Œå¾—åˆ°è¿™ä¸ªæœ‰è¶£çš„æ–¹ç¨‹ã€‚
- en: And so we get this one termï¼Œ this corresponds to just the path directly through
    the residual streamã€‚And it's going to want to store pgram statisticsï¼Œ it's justã€‚you
    know all I guess is the previous token and tries to predict the next tokenã€‚And
    so it gets to try and predict or try to store by statistics and then for every
    attention headã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°è¿™ä¸€é¡¹ï¼Œå®ƒå¯¹åº”äºç›´æ¥é€šè¿‡æ®‹å·®æµçš„è·¯å¾„ã€‚å®ƒæƒ³è¦å­˜å‚¨pgramç»Ÿè®¡ä¿¡æ¯ï¼Œå®é™…ä¸Šï¼Œå®ƒå°±æ˜¯çŸ¥é“æ‰€æœ‰çš„ï¼Œå‡è®¾æ˜¯å‰ä¸€ä¸ªä»¤ç‰Œï¼Œå¹¶è¯•å›¾é¢„æµ‹ä¸‹ä¸€ä¸ªä»¤ç‰Œã€‚å› æ­¤ï¼Œå®ƒä¼šå°è¯•é¢„æµ‹æˆ–é€šè¿‡ç»Ÿè®¡ä¿¡æ¯è¿›è¡Œå­˜å‚¨ï¼Œå¯¹äºæ¯ä¸ªæ³¨æ„åŠ›å¤´ã€‚
- en: we get this matrix that saysï¼Œ okay well we have the attention pattern so it
    looks that describes which token looks at which token and we have this matrix
    here which describes how for every possible token you can attend to how it affects
    the logics and that's just a table you can look at it just says you know for this
    attention head if it looks at this token it's going to increase the probability
    of these tokens in a one layer attention only transformer that's all there isã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°è¿™ä¸ªçŸ©é˜µï¼Œå®ƒè¡¨ç¤ºï¼Œå¥½çš„ï¼Œæˆ‘ä»¬æœ‰æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™ä¸ªçŸ©é˜µæè¿°äº†å“ªä¸ªä»¤ç‰Œå…³æ³¨å“ªä¸ªä»¤ç‰Œï¼ŒåŒæ—¶æˆ‘ä»¬è¿™é‡Œè¿˜æœ‰ä¸€ä¸ªçŸ©é˜µï¼Œæè¿°äº†æ¯ä¸ªå¯èƒ½çš„ä»¤ç‰Œå¦‚ä½•å½±å“é€»è¾‘ï¼Œè¿™åªæ˜¯ä¸€ä¸ªè¡¨æ ¼ï¼Œä½ å¯ä»¥æŸ¥çœ‹ï¼Œå®ƒåªæ˜¯è¯´æ˜ï¼Œå¯¹äºè¿™ä¸ªæ³¨æ„åŠ›å¤´ï¼Œå¦‚æœå®ƒå…³æ³¨è¿™ä¸ªä»¤ç‰Œï¼Œå®ƒå°†å¢åŠ è¿™äº›ä»¤ç‰Œåœ¨å•å±‚æ³¨æ„åŠ›å˜æ¢å™¨ä¸­çš„æ¦‚ç‡ï¼Œè¿™å°±æ˜¯å…¨éƒ¨ã€‚
- en: ğŸ˜Šï¼ŒYeahï¼Œ so this is just the interpretation I was describingã€‚å—¯ã€‚And another thing
    that's worth noting is according to thisã€‚the attention only transformer is linear
    if you fix the attention pattern now of course it's the attention pattern isn't
    fixedã€‚but whenever you able to have the opportunity to go and make something linear
    linear functions are really easy to understand and so if you can fix a small number
    of things and make something linear that's actually it's a lot of leverageã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ˜¯çš„ï¼Œè¿™å°±æ˜¯æˆ‘æ‰€æè¿°çš„è§£é‡Šã€‚å—¯ã€‚è¿˜æœ‰ä¸€ç‚¹å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ ¹æ®è¿™ä¸€ç‚¹ï¼Œæ³¨æ„åŠ›å˜æ¢å™¨æ˜¯çº¿æ€§çš„ï¼Œå¦‚æœä½ å›ºå®šäº†æ³¨æ„åŠ›æ¨¡å¼ï¼Œå½“ç„¶ï¼Œæ³¨æ„åŠ›æ¨¡å¼å¹¶ä¸æ˜¯å›ºå®šçš„ã€‚ä½†æ¯å½“ä½ æœ‰æœºä¼šå»ä½¿æŸäº›ä¸œè¥¿çº¿æ€§æ—¶ï¼Œçº¿æ€§å‡½æ•°å…¶å®å¾ˆå®¹æ˜“ç†è§£ï¼Œå› æ­¤å¦‚æœä½ èƒ½å¤Ÿå›ºå®šå°‘æ•°å‡ ä¸ªä¸œè¥¿å¹¶ä½¿æŸäº›ä¸œè¥¿çº¿æ€§ï¼Œè¿™å®é™…ä¸Šæ˜¯å¾ˆæœ‰æ æ†ä½œç”¨çš„ã€‚
- en: Okayã€‚å—¯ã€‚And yeahï¼Œ we could talk about how the attention pattern is computed as
    wellã€‚you if you expand it outï¼Œ you'll get an equation like thisã€‚And noticeï¼Œ wellã€‚I
    think it'll be easierã€‚Okayã€‚I think the core story though to take away from all
    of these is we have these two matrices that actually look kind of similarã€‚so this
    one here tells you if you attended to a tokenï¼Œ how are the logics affectedï¼Ÿ
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚å—¯ã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬å¯ä»¥è®¨è®ºä¸€ä¸‹æ³¨æ„åŠ›æ¨¡å¼æ˜¯å¦‚ä½•è®¡ç®—çš„ã€‚å¦‚æœä½ å±•å¼€å®ƒï¼Œä½ ä¼šå¾—åˆ°è¿™æ ·çš„æ–¹ç¨‹ã€‚è¯·æ³¨æ„ï¼Œæˆ‘è®¤ä¸ºè¿™ä¼šæ›´ç®€å•ã€‚å¥½çš„ã€‚æˆ‘è®¤ä¸ºä»è¿™ä¸€åˆ‡ä¸­è¦å¾—å‡ºçš„æ ¸å¿ƒæ•…äº‹æ˜¯ï¼Œæˆ‘ä»¬æœ‰è¿™ä¸¤ä¸ªçœ‹èµ·æ¥ç›¸ä¼¼çš„çŸ©é˜µã€‚é‚£ä¹ˆè¿™ä¸ªçŸ©é˜µå‘Šè¯‰ä½ ï¼Œå¦‚æœä½ å…³æ³¨ä¸€ä¸ªä»¤ç‰Œï¼Œé€»è¾‘æ˜¯å¦‚ä½•å—åˆ°å½±å“çš„ï¼Ÿ
- en: And you can just think of it as a giant matrix of for every possible token input
    token and how are the logics affectedï¼Ÿ
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆä¸€ä¸ªå·¨å¤§çš„çŸ©é˜µï¼Œé’ˆå¯¹æ¯ä¸€ä¸ªå¯èƒ½çš„è¾“å…¥ä»¤ç‰Œï¼Œé€»è¾‘æ˜¯å¦‚ä½•å—åˆ°å½±å“çš„ï¼Ÿ
- en: By that tokenï¼Œ are they made more likely or less likelyï¼ŸAnd we have this oneï¼Œ
    which sorter saysã€‚how much does every token want to attend to every other tokenï¼ŸOne
    way that you can picture this isã€‚Okayï¼Œ that's really there's really three tokens
    involved when we're thinking about an attention headã€‚we have the token that we're
    going to move information to and that's attending backwardsã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡é‚£ä¸ªä»¤ç‰Œï¼Œå®ƒä»¬æ˜¯å˜å¾—æ›´å¯èƒ½è¿˜æ˜¯ä¸å¤ªå¯èƒ½ï¼Ÿè€Œæˆ‘ä»¬æœ‰è¿™ä¸ªï¼Œå®ƒæœ‰ç‚¹å„¿è¯´ã€‚æ¯ä¸ªä»¤ç‰Œæƒ³è¦å…³æ³¨å…¶ä»–æ¯ä¸ªä»¤ç‰Œçš„ç¨‹åº¦ã€‚ä½ å¯ä»¥è¿™æ ·æƒ³è±¡ï¼Œå¥½çš„ï¼Œå½“æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªæ³¨æ„åŠ›å¤´æ—¶ï¼Œå®é™…ä¸Šæ¶‰åŠä¸‰ä¸ªä»¤ç‰Œã€‚æˆ‘ä»¬æœ‰è¦ç§»åŠ¨ä¿¡æ¯çš„ä»¤ç‰Œï¼Œå®ƒæ˜¯å‘åå…³æ³¨çš„ã€‚
- en: We have the source token that's going to get attended to and we have the output
    token whose logicits are going to be affectedã€‚And you can just trace through this
    so you can ask what happensã€‚how does the attending to this token affect the outputï¼Œ
    well first we embed the tokenã€‚Then we multiply by WV to get the value vectorï¼Œ
    the information gets moved by the attention patternã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰æºæ ‡è®°å°†è¦è¢«å…³æ³¨ï¼Œè¿˜æœ‰è¾“å‡ºæ ‡è®°çš„é€»è¾‘å°†ä¼šå—åˆ°å½±å“ã€‚ä½ å¯ä»¥è·Ÿè¸ªè¿™ä¸ªè¿‡ç¨‹ï¼Œè¯¢é—®å…³æ³¨è¿™ä¸ªæ ‡è®°ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œé¦–å…ˆæˆ‘ä»¬åµŒå…¥æ ‡è®°ã€‚ç„¶åä¹˜ä»¥ WV ä»¥è·å¾—å€¼å‘é‡ï¼Œä¿¡æ¯é€šè¿‡æ³¨æ„åŠ›æ¨¡å¼è¢«ç§»åŠ¨ã€‚
- en: We multipied by WO to add it back into the residual stream but get hit by the
    an embedding and we affect the logics and that's where that one matrix comes from
    and we can also ask you know what decides you know whether a token gets a high
    score when we're computing the attention pattern and it just says you knowbed
    embed the tokenã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹˜ä»¥ WO ä»¥å°†å…¶æ·»åŠ å›æ®‹å·®æµä¸­ï¼Œä½†è¢«åµŒå…¥å½±å“äº†é€»è¾‘ï¼Œè¿™å°±æ˜¯é‚£ä¸ªçŸ©é˜µçš„æ¥æºã€‚æˆ‘ä»¬è¿˜å¯ä»¥è¯¢é—®æ˜¯ä»€ä¹ˆå†³å®šäº†å½“æˆ‘ä»¬è®¡ç®—æ³¨æ„åŠ›æ¨¡å¼æ—¶ï¼ŒæŸä¸ªæ ‡è®°è·å¾—é«˜åˆ†çš„åŸå› ï¼Œå®ƒä»…ä»…æ˜¯è¯´åµŒå…¥é‚£ä¸ªæ ‡è®°ã€‚
- en: Turn it into a queryï¼Œ embed the other tokenï¼Œ turn it into a keyã€‚And dot product
    to them and seeã€‚that's where those two matrices come fromã€‚So I knew that I'm going
    quite quicklyã€‚Maybe I'll just briefly pause here and if anyone wants to ask for
    clarificationsã€‚this would be a good time and then we'll actually go and reverse
    engineer and sayã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å…¶è½¬åŒ–ä¸ºæŸ¥è¯¢ï¼ŒåµŒå…¥å¦ä¸€ä¸ªæ ‡è®°ï¼Œå°†å…¶è½¬åŒ–ä¸ºé”®ã€‚ç„¶åè®¡ç®—å®ƒä»¬çš„ç‚¹ç§¯ã€‚é‚£å°±æ˜¯è¿™ä¸¤ä¸ªçŸ©é˜µçš„æ¥æºã€‚æ‰€ä»¥æˆ‘çŸ¥é“æˆ‘è®²å¾—æ¯”è¾ƒå¿«ã€‚ä¹Ÿè®¸æˆ‘åœ¨è¿™é‡Œç¨ä½œåœé¡¿ï¼Œå¦‚æœæœ‰äººæƒ³è¯¢é—®æ¾„æ¸…ï¼Œè¿™å°†æ˜¯ä¸€ä¸ªå¥½æ—¶æœºï¼Œç„¶åæˆ‘ä»¬å°†å®é™…è¿›è¡Œé€†å‘å·¥ç¨‹ã€‚
- en: you know everything that's going on in a one layer intention transformer is
    now in the palm of our handsã€‚It's a very toy modelã€‚No one actually uses one layer
    attention to the transformersã€‚but we'll be able to understandã€‚The one layer attentionally
    transformerã€‚So justã€‚you're sering that yes the quick key circuit is learning the
    attention weightsã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œå•å±‚æ³¨æ„åŠ›å˜æ¢å™¨ä¸­å‘ç”Ÿçš„ä¸€åˆ‡ç°åœ¨å°½åœ¨æˆ‘ä»¬çš„æŒæ¡ä¹‹ä¸­ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ç®€å•çš„æ¨¡å‹ã€‚å®é™…ä¸Šæ²¡æœ‰äººä½¿ç”¨å•å±‚æ³¨æ„åŠ›å˜æ¢å™¨ï¼Œä½†æˆ‘ä»¬èƒ½å¤Ÿç†è§£å•å±‚æ³¨æ„åŠ›å˜æ¢å™¨ã€‚æ‰€ä»¥ï¼Œç¡®å®ï¼Œä½ çœ‹åˆ°çš„å¿«é€Ÿé”®ç”µè·¯æ­£åœ¨å­¦ä¹ æ³¨æ„åŠ›æƒé‡ã€‚
- en: And like essentially is a responsible of running the sort the attention between
    different tokens yeah yeah so this this matrix when it yeah you know all three
    of those parts are learnedã€‚but that's that's what expresses whether a attention
    pattern yeah that's what generates the attention patterns gets run for every pair
    of tokens and you can you can you can think of values in that matrix as just being
    how much every token wants to attend to every other token if it was in the context
    we're ignoring positional letting here so there's a little bit that we're sort
    of aligning over there as well but sort of in a global sense how much does every
    token wants to attend every other token right the circuit like the circuit is
    using the attention thatã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: åƒæœ¬è´¨ä¸Šè´Ÿè´£è¿è¡Œä¸åŒæ ‡è®°ä¹‹é—´çš„æ³¨æ„åŠ›æ’åºä¸€æ ·ï¼Œè¿™ä¸ªçŸ©é˜µåœ¨å­¦ä¹ æ—¶ä¼šæ¶‰åŠåˆ°æ‰€æœ‰ä¸‰ä¸ªéƒ¨åˆ†ã€‚ä½†è¿™å°±æ˜¯å®ƒè¡¨è¾¾çš„æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™å°±æ˜¯ç”Ÿæˆæ³¨æ„åŠ›æ¨¡å¼çš„åœ°æ–¹ï¼Œå¯¹äºæ¯å¯¹æ ‡è®°éƒ½è¿›è¡Œè®¡ç®—ã€‚ä½ å¯ä»¥æŠŠè¿™ä¸ªçŸ©é˜µä¸­çš„å€¼çœ‹ä½œæ¯ä¸ªæ ‡è®°æƒ³è¦å…³æ³¨å…¶ä»–æ ‡è®°çš„ç¨‹åº¦ï¼Œå‡è®¾æˆ‘ä»¬åœ¨è¿™é‡Œå¿½ç•¥ä½ç½®çš„ä¿¡æ¯ï¼Œæ‰€ä»¥åœ¨æŸç§ç¨‹åº¦ä¸Šï¼Œæˆ‘ä»¬ä¹Ÿæ˜¯åœ¨å¯¹å…¶è¿›è¡Œå…¨å±€å¯¹é½ï¼Œä½†æ€»çš„æ¥è¯´ï¼Œæ¯ä¸ªæ ‡è®°æƒ³è¦å…³æ³¨å…¶ä»–æ ‡è®°çš„ç¨‹åº¦å¦‚ä½•ï¼Œå¯¹å§ï¼Œç”µè·¯å°±åƒæ˜¯ä½¿ç”¨è¿™ç§æ³¨æ„åŠ›ã€‚
- en: Yesã€‚Like affect the final outputs it's sort of saying if if the attention head
    assume that the attention head attends to some tokenã€‚so let's set aside the question
    of how back gets' compute just assume that it attends to some tokenã€‚how would
    it affect the outputs if it attended to that tokenã€‚And you just you can just calculate
    that it's just a big table of values that says you know for this tokenã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚åƒå½±å“æœ€ç»ˆè¾“å‡ºï¼Œå‡è®¾æ³¨æ„åŠ›å¤´å…³æ³¨æŸä¸ªæ ‡è®°ã€‚é‚£ä¹ˆæˆ‘ä»¬æš‚æ—¶æ”¾ä¸‹å¦‚ä½•è®¡ç®—çš„é—®é¢˜ï¼Œå‡è®¾å®ƒå…³æ³¨æŸä¸ªæ ‡è®°ã€‚å…³æ³¨é‚£ä¸ªæ ‡è®°ä¼šå¦‚ä½•å½±å“è¾“å‡ºï¼Ÿä½ å¯ä»¥è®¡ç®—ï¼Œè¿™åªæ˜¯ä¸€ä¸ªå¤§å€¼è¡¨ï¼Œè¡¨ç¤ºå¯¹äºè¿™ä¸ªæ ‡è®°ã€‚
- en: It's going to make this token more likely this token will make this token less
    likelyã€‚Okayã€‚that't justã€‚And it's completely independent like it's just two separate
    matricesï¼Œ they're notã€‚you knowï¼Œ the formulas that might make them seem entangledï¼Œ
    but they're actually separateã€‚Rightã€‚so to meï¼Œ it seems like the lecture supervision
    is coming from the output value set and the query key are seems more like unsupervised
    kind of thing because there's noã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†ä½¿å¾—æŸä¸ªæ ‡è®°æ›´å¯èƒ½ï¼Œè€Œè¿™ä¸ªæ ‡è®°å°†ä½¿å¾—å¦ä¸€ä¸ªæ ‡è®°çš„å¯èƒ½æ€§é™ä½ã€‚å¥½çš„ã€‚è¿™ä»…ä»…æ˜¯ã€‚å¹¶ä¸”å®ƒä»¬æ˜¯å®Œå…¨ç‹¬ç«‹çš„ï¼Œå°±åƒæ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„çŸ©é˜µï¼Œå®ƒä»¬å¹¶ä¸æ˜¯ã€‚ä½ çŸ¥é“ï¼Œå…¬å¼å¯èƒ½è®©å®ƒä»¬çœ‹èµ·æ¥æ˜¯çº ç¼ çš„ï¼Œä½†å®é™…ä¸Šæ˜¯åˆ†å¼€çš„ã€‚å¯¹æˆ‘æ¥è¯´ï¼Œè®²åº§çš„ç›‘ç£ä¼¼ä¹æ¥è‡ªè¾“å‡ºå€¼é›†ï¼Œè€ŒæŸ¥è¯¢é”®åˆ™æ›´åƒæ˜¯æ— ç›‘ç£çš„ä¸œè¥¿ï¼Œå› ä¸ºæ²¡æœ‰ã€‚
- en: I meanï¼Œ therere just I think in the sense that every yeah in a model like every
    every neuron is in some senseã€‚you know like signal is is somehow downstream from
    the ultimate the ultimate signal and so you know yeah the output value signal
    the output value start is getting more more direct is perhaps getting more direct
    signal but yeahã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ„æ€æ˜¯ï¼Œæˆ‘è§‰å¾—åœ¨æŸç§æ„ä¹‰ä¸Šï¼Œåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œæ¯ä¸€ä¸ªç¥ç»å…ƒåœ¨æŸç§æ„ä¹‰ä¸Šéƒ½æ˜¯è¿™æ ·çš„ã€‚ä½ çŸ¥é“ï¼Œå°±åƒä¿¡å·åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯æ¥è‡ªæœ€ç»ˆä¿¡å·çš„ä¸‹æ¸¸ï¼Œæ‰€ä»¥ä½ çŸ¥é“ï¼Œè¾“å‡ºå€¼ä¿¡å·çš„è¾“å‡ºå€¼å¼€å§‹å˜å¾—è¶Šæ¥è¶Šç›´æ¥ï¼Œå¯èƒ½ä¼šå¾—åˆ°æ›´ç›´æ¥çš„ä¿¡å·ï¼Œä½†ç¡®å®å¦‚æ­¤ã€‚
- en: æ˜¯ã€‚We will be able to dig into this in lots of detail in as much detail as you
    want in a little bit so we can maybe I'll push forward and I think also actually
    an example of how to use this reverse engineer one layer model will maybe make
    it a little bit more motivatedã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ã€‚æˆ‘ä»¬å°†èƒ½å¤Ÿæ·±å…¥æ¢è®¨è¿™ä¸ªé—®é¢˜ï¼Œå°½å¯èƒ½è¯¦ç»†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ä¹Ÿè®¸æ¨è¿›ä¸€ä¸‹ï¼Œæˆ‘è®¤ä¸ºå®é™…ä¸Šå¦‚ä½•ä½¿ç”¨è¿™ä¸ªé€†å‘å·¥ç¨‹ä¸€ä¸ªå±‚æ¬¡æ¨¡å‹çš„ä¾‹å­ï¼Œå¯èƒ½ä¼šè®©å®ƒæ›´æœ‰åŠ¨æœºã€‚
- en: Okayï¼Œ soã€‚Just just to emphasize thisï¼Œ there's three different tokens that we
    can talk aboutã€‚there's a token that gets attended toï¼Œ there's the token that does
    the attention to call the destination and then there's the token that gets affected
    yet gets the next token which its probabilities are affectedã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥ã€‚åªæ˜¯ä¸ºäº†å¼ºè°ƒè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥è®¨è®ºä¸‰ç§ä¸åŒçš„ä»¤ç‰Œã€‚ä¸€ä¸ªæ˜¯è¢«å…³æ³¨çš„ä»¤ç‰Œï¼Œä¸€ä¸ªæ˜¯è¿›è¡Œå…³æ³¨çš„ä»¤ç‰Œä»¥è°ƒç”¨ç›®æ ‡ï¼Œç„¶åè¿˜æœ‰ä¸€ä¸ªæ˜¯è¢«å½±å“çš„ä»¤ç‰Œï¼Œå®ƒçš„ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„æ¦‚ç‡ä¹Ÿä¼šå—åˆ°å½±å“ã€‚
- en: å—¯ã€‚And so something we can do is notice that the only token that connects to
    both of these is the token that gets attended toã€‚so these two are sort of they're
    bridgedã€‚By their their interaction with the source tokenã€‚so something that's kind
    of natural is to ask for a given source tokenï¼Œ you knowã€‚how does it interact with
    both of theseï¼ŸSo let's takeï¼Œ for instanceï¼Œ the token perfectã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥æ³¨æ„åˆ°ï¼Œå”¯ä¸€è¿æ¥è¿™ä¸¤ä¸ªçš„ä»¤ç‰Œæ˜¯è¢«å…³æ³¨çš„ä»¤ç‰Œã€‚æ‰€ä»¥è¿™ä¸¤ä¸ªæ˜¯é€šè¿‡å®ƒä»¬ä¸æºä»¤ç‰Œçš„äº’åŠ¨å»ºç«‹äº†æ¡¥æ¢ã€‚å› æ­¤ï¼Œæœ‰ä¸€ç§è‡ªç„¶çš„æ–¹å¼æ˜¯é—®ï¼Œå¯¹äºç»™å®šçš„æºä»¤ç‰Œï¼Œä½ çŸ¥é“ï¼Œå®ƒå¦‚ä½•ä¸è¿™ä¸¤ä¸ªäº’åŠ¨ï¼Ÿæ‰€ä»¥æˆ‘ä»¬æ‹¿ï¼Œæ¯”å¦‚è¯´ï¼Œä»¤ç‰Œå®Œç¾ã€‚
- en: Whichken one thing we can ask is which tokens want to attend to perfectã€‚Coyleï¼Œ
    apparentlyã€‚the tokens that most want to attend are perfect are are and looks and
    is and providesã€‚So our is the most looks as the next most and so onã€‚And then when
    we attend to perfect and this is with one single attentionedã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é—®çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå“ªäº›ä»¤ç‰Œå¸Œæœ›å…³æ³¨å®Œç¾ã€‚æ˜¾ç„¶ï¼Œæœ€å¸Œæœ›å…³æ³¨å®Œç¾çš„ä»¤ç‰Œæ˜¯â€œå®Œç¾â€ã€â€œæ˜¯â€ã€â€œçœ‹â€å’Œâ€œæä¾›â€ã€‚æ‰€ä»¥â€œæ˜¯â€æ˜¯æœ€ä¸»è¦çš„ï¼Œæ¥ä¸‹æ¥æ˜¯â€œçœ‹â€ï¼Œä¾æ­¤ç±»æ¨ã€‚ç„¶åå½“æˆ‘ä»¬å…³æ³¨å®Œç¾æ—¶ï¼Œè¿™åªæ˜¯ä¸€æ¬¡å•ä¸€çš„å…³æ³¨ã€‚
- en: so you know it' would be different if we did a different attentionedã€‚it wants
    to really increase the probability of perfect and then to a lesser extentã€‚super
    and absolute and pureã€‚And we can ask you what sequences of tokens are made more
    likely by of this particular set of things wanting to attend to each other and
    becoming more likelyã€‚ğŸ˜¡ï¼ŒThings are the formã€‚We have our token that we attended
    back to and we have some skip of some number of tokensã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ çŸ¥é“ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨ä¸åŒçš„å…³æ³¨æ–¹å¼ï¼Œé‚£ç»“æœä¼šæœ‰æ‰€ä¸åŒã€‚å®ƒå®é™…ä¸Šæƒ³è¦å¢åŠ å®Œç¾çš„æ¦‚ç‡ï¼Œç„¶ååœ¨è¾ƒå°ç¨‹åº¦ä¸Šå¢åŠ è¶…çº§ã€ç»å¯¹å’Œçº¯ç²¹çš„æ¦‚ç‡ã€‚æˆ‘ä»¬å¯ä»¥é—®ä½ ï¼Œè¿™ä¸€ç‰¹å®šç»„å¸Œæœ›äº’ç›¸å…³æ³¨å¹¶å˜å¾—æ›´å¯èƒ½çš„ä»¤ç‰Œåºåˆ—æ˜¯ä»€ä¹ˆã€‚ğŸ˜¡ï¼Œäº‹æƒ…æ˜¯å½¢å¼ä¸Šçš„ã€‚æˆ‘ä»¬æœ‰æˆ‘ä»¬å…³æ³¨çš„ä»¤ç‰Œï¼Œå¹¶ä¸”æˆ‘ä»¬è·³è¿‡äº†ä¸€äº›ä»¤ç‰Œã€‚
- en: they don't have to be adjacentï¼Œ but then later on we see the token R and it
    tends back to perfect and increases the probability of perfectã€‚So you can think
    of these as being like we're sort of creating changing the probability of what
    we might call skip trigrams where we haveã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä¸å¿…æ˜¯ç›¸é‚»çš„ï¼Œä½†åæ¥æˆ‘ä»¬çœ‹åˆ°ä»¤ç‰ŒRï¼Œå®ƒå€¾å‘äºå®Œç¾å¹¶å¢åŠ å®Œç¾çš„æ¦‚ç‡ã€‚å› æ­¤ï¼Œä½ å¯ä»¥æŠŠè¿™äº›è§†ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ›å»ºæˆ–æ”¹å˜æˆ‘ä»¬å¯èƒ½ç§°ä¹‹ä¸ºè·³è¿‡ä¸‰å…ƒç»„çš„æ¦‚ç‡çš„äº‹ç‰©ã€‚
- en: you knowï¼Œ we skip over a bunch of tokens in the middleã€‚but we're affecting the
    probability really of trigramsã€‚So perfect or perfectï¼Œ perfectï¼Œ look superã€‚We can
    look at another one so we have the token largeã€‚these tokens contains using specify
    want to go and look back to it and an increase in probability of large and small
    and the skip tris that are affected are things like large using largeã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œæˆ‘ä»¬åœ¨ä¸­é—´è·³è¿‡äº†ä¸€äº›ä»¤ç‰Œã€‚ä½†æˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨å½±å“ä¸‰å…ƒç»„çš„æ¦‚ç‡ã€‚æ‰€ä»¥å®Œç¾æˆ–è€…å®Œç¾ï¼Œå®Œç¾ï¼Œçœ‹èµ·æ¥è¶…çº§ã€‚æˆ‘ä»¬å¯ä»¥çœ‹çœ‹å¦ä¸€ä¸ªï¼Œæˆ‘ä»¬æœ‰ä»¤ç‰Œå¤§ã€‚è¿™äº›ä»¤ç‰ŒåŒ…å«äº†å¸Œæœ›å›å¤´çœ‹å¹¶å¢åŠ å¤§å’Œå°çš„æ¦‚ç‡ï¼Œä»¥åŠå—åˆ°å½±å“çš„è·³è¿‡ä¸‰å…ƒç»„çš„äº‹ç‰©ï¼Œæ¯”å¦‚å¤§ä½¿ç”¨å¤§ã€‚
- en: large contains small and things like thisã€‚ğŸ˜Šï¼Œif we see the number twoã€‚we increase
    the probability of other numbers and we affect tokens or skip diagrams like twoï¼Œ
    oneã€‚twoï¼Œ two has threeã€‚Now you're all in a technical field so you'll probably
    recognize this oneã€‚we have Lambda and then we see backslash and then we want to
    increase the probability of Lambda and sorted and Lambda and operators so it's
    all fall lateekã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§åŒ…å«å°ï¼Œåƒè¿™æ ·çš„äº‹æƒ…ã€‚ğŸ˜Šï¼Œå¦‚æœæˆ‘ä»¬çœ‹åˆ°æ•°å­—äºŒï¼Œæˆ‘ä»¬ä¼šå¢åŠ å…¶ä»–æ•°å­—çš„æ¦‚ç‡ï¼Œå½±å“åƒ2ï¼Œ1çš„æ ‡è®°æˆ–è·³è¿‡å›¾è¡¨ã€‚äºŒï¼ŒäºŒæœ‰ä¸‰ä¸ªã€‚ç°åœ¨ä½ ä»¬éƒ½æ˜¯æŠ€æœ¯é¢†åŸŸçš„äººï¼Œåº”è¯¥èƒ½è®¤è¯†åˆ°è¿™ä¸ªã€‚æˆ‘ä»¬æœ‰Lambdaï¼Œç„¶åçœ‹åˆ°åæ–œæ ï¼Œç„¶åæˆ‘ä»¬æƒ³å¢åŠ Lambdaã€æ’åºå’ŒLambdaæ“ä½œç¬¦çš„æ¦‚ç‡ï¼Œæ‰€ä»¥è¿™éƒ½æ˜¯è½åæŠ€æœ¯ã€‚
- en: It wants toï¼Œ it's if it sees Lambda it thinks thatï¼Œ you knowï¼Œ maybe next time
    I use a backlashã€‚I should go and put in some latex math symbolã€‚Also same thing
    for HTML we see NSP for non breakinging space and then we see an n percent we
    want to go and make that more likely the takeaway from all this is that a one
    layer our attention only transformer is totally acting on these skip trisã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒæƒ³è¦ï¼Œå¦‚æœå®ƒçœ‹åˆ°Lambdaï¼Œå®ƒä¼šè®¤ä¸ºï¼Œä¸‹æ¬¡æˆ‘ä½¿ç”¨åæ–œæ æ—¶ï¼Œä¹Ÿè®¸æˆ‘åº”è¯¥æ”¾å…¥ä¸€äº›LaTeXæ•°å­¦ç¬¦å·ã€‚å¯¹äºHTMLä¹Ÿæ˜¯åŒæ ·çš„äº‹æƒ…ï¼Œæˆ‘ä»¬çœ‹åˆ°NSPè¡¨ç¤ºä¸é—´æ–­ç©ºæ ¼ï¼Œç„¶åçœ‹åˆ°ä¸€ä¸ªç™¾åˆ†å·ï¼Œæˆ‘ä»¬æƒ³è®©å®ƒæ›´å¯èƒ½ã€‚æ‰€æœ‰è¿™äº›çš„ç»“è®ºæ˜¯ï¼Œä¸€å±‚æ³¨æ„åŠ›å˜æ¢å™¨å®Œå…¨åœ¨ä½œç”¨äºè¿™äº›è·³è¿‡ä¸‰å…ƒç»„ã€‚
- en: ğŸ˜Šï¼ŒAndã€‚Everything that doesï¼Œ I meanï¼Œ I guess it also has this pathway by which
    it affects diagramgramsã€‚but mostly it's just affecting these skip diagramsã€‚And
    there's lots of themã€‚it's just like these giant tables of skip tris that are made
    more or less likelyã€‚There's lots of other fun things that does sometimes the tokenization
    will split up a word in multiple waysã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè€Œä¸”ã€‚æ‰€æœ‰è¿™äº›ï¼Œå—¯ï¼Œæˆ‘æƒ³ï¼Œå®ƒä¹Ÿæœ‰å½±å“å›¾è¡¨çš„è·¯å¾„ã€‚ä½†ä¸»è¦æ˜¯å½±å“è¿™äº›è·³è¿‡å›¾è¡¨ã€‚è¿˜æœ‰å¾ˆå¤šï¼Œå®ƒå°±åƒæ˜¯è¿™äº›å·¨å¤§çš„è·³è¿‡ä¸‰å…ƒç»„è¡¨ï¼Œä½¿å¾—æŸäº›å¯èƒ½æ€§æ›´å¤§æˆ–æ›´å°ã€‚è¿˜æœ‰å…¶ä»–æœ‰è¶£çš„äº‹æƒ…ï¼Œæœ‰æ—¶æ ‡è®°åŒ–ä¼šä»¥å¤šç§æ–¹å¼æ‹†åˆ†ä¸€ä¸ªå•è¯ã€‚
- en: so like we have Indie well lets that's not good k we have like the word Piike
    and then we we see the the token P and then we predict ICã€‚ğŸ˜Šï¼ŒWhen we predict spikes
    and stuff like thatã€‚Or these these ones are kind of funã€‚maybe they're actually
    worth talking about for a secondã€‚ so we see the token voidã€‚ğŸ˜Šã€‚And then we see an
    L and make we predict Lloyd or Rï¼Œ and we predict Ralphï¼Œ Cï¼Œ Catherineã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æœ‰Indieï¼Œå¥½å§ï¼Œè¿™ä¸å¤ªå¥½kï¼Œæˆ‘ä»¬æœ‰å•è¯Piikeï¼Œç„¶åæˆ‘ä»¬çœ‹åˆ°æ ‡è®°Pï¼Œç„¶åæˆ‘ä»¬é¢„æµ‹ICã€‚ğŸ˜Šï¼Œå½“æˆ‘ä»¬é¢„æµ‹å°–å³°ç­‰ä¸œè¥¿æ—¶ï¼Œæˆ–è€…è¿™äº›ä¸œè¥¿æœ‰ç‚¹æœ‰è¶£ã€‚ä¹Ÿè®¸å€¼å¾—è®¨è®ºä¸€ä¸‹ã€‚æˆ‘ä»¬çœ‹åˆ°æ ‡è®°voidã€‚ğŸ˜Šã€‚ç„¶åæˆ‘ä»¬çœ‹åˆ°ä¸€ä¸ªLï¼Œå¯èƒ½æˆ‘ä»¬é¢„æµ‹Lloydæˆ–Rï¼Œé¢„æµ‹Ralphï¼ŒCï¼ŒCatherineã€‚
- en: And but we'll see in a second that well yeah we'll come back to that that in
    a secondã€‚So we increase the probability of things like Lloyd Lloyd and Lloyd Catherine
    or Pixmap if anyone's worked with QT we see Pixmap and we increase the probability
    of P Xmap againã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ç¨åä¼šçœ‹åˆ°ï¼Œå—¯ï¼Œæ˜¯çš„ï¼Œæˆ‘ä»¬ç¨åä¼šå›åˆ°è¿™ä¸€ç‚¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¢åŠ åƒLloyd Lloydå’ŒLloyd Catherineæˆ–Pixmapçš„å¯èƒ½æ€§ï¼Œå¦‚æœæœ‰äººç”¨è¿‡QTï¼Œæˆ‘ä»¬çœ‹åˆ°Pixmapï¼Œå¹¶å†æ¬¡å¢åŠ P
    Xmapçš„æ¦‚ç‡ã€‚
- en: but also Qã€‚ğŸ˜Šï¼ŒCanvasã€‚Andã€‚But of courseï¼Œ there's a problem with thisã€‚which is
    it doesn't get to pick which one of these goes with which one so if you want to
    go and make Pixm pix mapapã€‚And Pixmap Q Canvas more probableï¼Œ you also have to
    go and create and make Pixmap Pixmap P canvasvas more probableã€‚And if you want
    to make Lloyd Lloyd and Lloyd Catherineã€‚More probableã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¹Ÿæœ‰Qã€‚ğŸ˜Šï¼ŒCanvasã€‚è¿˜æœ‰ã€‚å½“ç„¶ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªé—®é¢˜ï¼Œé‚£å°±æ˜¯å®ƒæ— æ³•é€‰æ‹©å“ªäº›ä¸å“ªäº›åŒ¹é…ï¼Œæ‰€ä»¥å¦‚æœä½ æƒ³åˆ¶ä½œPixmåƒç´ å›¾ã€‚å¹¶ä¸”è®©Pixmap Q Canvasæ›´å¯èƒ½ï¼Œä½ è¿˜å¿…é¡»å»åˆ›å»ºå¹¶ä½¿Pixmap
    Pixmap P canvasvasæ›´å¯èƒ½ã€‚å¦‚æœä½ æƒ³è®©Lloyd Lloydå’ŒLloyd Catherineæ›´å¯èƒ½ã€‚
- en: you also have to make Lloyd Cloyd and Lloyd Latherinï¼Œ more probableã€‚And so there's
    actually like bugs that transformers have like weird at least you know in these
    really tiny one layer at attention only transformers there's these bugs that you
    know they seem weird until you realize that it's this giant table of skip tris
    that's that's operating and the nature of that is that you're going to beã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ è¿˜å¿…é¡»è®©Lloyd Cloydå’ŒLloyd Latherinæ›´å¯èƒ½ã€‚å› æ­¤ï¼Œå˜æ¢å™¨ç¡®å®å­˜åœ¨ä¸€äº›å¥‡æ€ªçš„bugï¼Œè‡³å°‘åœ¨è¿™äº›åªæœ‰ä¸€å±‚çš„æ³¨æ„åŠ›å˜æ¢å™¨ä¸­æœ‰è¿™äº›bugï¼Œè¿™äº›çœ‹èµ·æ¥å¾ˆå¥‡æ€ªï¼Œç›´åˆ°ä½ æ„è¯†åˆ°è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„è·³è¿‡ä¸‰å…ƒç»„è¡¨åœ¨è¿ä½œï¼Œè€Œè¿™ç§æ€§è´¨æ˜¯ä½ å°†ä¼šã€‚
- en: ğŸ˜Šï¼Œyeahï¼Œ it sort of forces you if you want to go and do this to go in and also
    make some weird predictionsã€‚Chrisï¼Œ brieflyleyã€‚Is there a reason why the source
    open you have a space before the first characterï¼Ÿ
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ˜¯çš„ï¼Œè¿™ç§æ–¹å¼åœ¨ä½ æƒ³è¿™æ ·åšæ—¶è¿«ä½¿ä½ å»åšä¸€äº›å¥‡æ€ªçš„é¢„æµ‹ã€‚Chrisï¼Œç®€è¦åœ°è¯´ã€‚ä½ ä¸ºä»€ä¹ˆåœ¨ç¬¬ä¸€ä¸ªå­—ç¬¦å‰æœ‰ä¸€ä¸ªç©ºæ ¼å‘¢ï¼Ÿ
- en: Yesï¼Œ that's just the I was giving examples where the tokenization breaks in
    a particular way and okay because spaces get included in the tokenization when
    there's a space in front of something and then there's an example where the space
    isn't in front of itã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘åªæ˜¯ä¸¾äº†ä¸€äº›ä¾‹å­ï¼Œè¯´æ˜æ ‡è®°åŒ–æ˜¯å¦‚ä½•ä»¥ç‰¹å®šæ–¹å¼æ–­è£‚çš„ï¼Œå¥½çš„ï¼Œå› ä¸ºåœ¨æŸäº›ä¸œè¥¿å‰é¢æœ‰ç©ºæ ¼æ—¶ï¼Œç©ºæ ¼ä¼šè¢«åŒ…å«åœ¨æ ‡è®°åŒ–ä¸­ï¼Œç„¶åæœ‰ä¸€ä¸ªä¾‹å­æ˜¯ç©ºæ ¼ä¸åœ¨å®ƒå‰é¢ã€‚
- en: they can get tokenized in different ways Go it cool thanksã€‚Yeahï¼Œ great questionã€‚Okayã€‚so
    some just to abstract away some common patterns that we're seeingã€‚I think one
    pretty common thing is what you might describe as like beã€‚A Bã€‚so you're you go
    in you you see some token and then you see another token that might precede that
    token and then they're likeã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬å¯ä»¥ä»¥ä¸åŒçš„æ–¹å¼è¿›è¡Œæ ‡è®°åŒ–ï¼Œå¤ªé…·äº†ï¼Œè°¢è°¢ã€‚æ˜¯çš„ï¼Œä¼Ÿå¤§çš„é—®é¢˜ã€‚å¥½çš„ï¼Œæ‰€ä»¥æœ‰äº›åªæ˜¯ä¸ºäº†æŠ½è±¡å‡ºæˆ‘ä»¬çœ‹åˆ°çš„ä¸€äº›å…±åŒæ¨¡å¼ã€‚æˆ‘è®¤ä¸ºä¸€ä¸ªç›¸å½“å¸¸è§çš„æƒ…å†µæ˜¯ä½ å¯ä»¥æè¿°çš„åƒæ˜¯A
    Bã€‚æ‰€ä»¥ä½ è¿›å»åï¼Œçœ‹åˆ°ä¸€äº›æ ‡è®°ï¼Œç„¶åä½ çœ‹åˆ°å¦ä¸€ä¸ªå¯èƒ½åœ¨é‚£ä¸ªæ ‡è®°ä¹‹å‰çš„æ ‡è®°ï¼Œç„¶åå®ƒä»¬å°±åƒã€‚
- en: probably the token that I saw earlier is going to occur againã€‚Or sometimes you
    predict a slightly differenttokenï¼Œ so like maybe an example of the first one is
    twoã€‚oneï¼Œ twoã€‚But you could also do two has threeã€‚And so three isn't the same as
    twoã€‚but it's kind of similar so that's that's one thing another one is this this
    example where you have a token that suddenly it's tokenized together one time
    and then it's split apart so you see the token and then you see something that
    might be the first part of the token and then you predict the second partã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¯èƒ½æˆ‘ä¹‹å‰çœ‹åˆ°çš„æ ‡è®°ä¼šå†æ¬¡å‡ºç°ã€‚æˆ–è€…æœ‰æ—¶ä½ é¢„æµ‹ä¸€ä¸ªç¨å¾®ä¸åŒçš„æ ‡è®°ï¼Œæ¯”å¦‚ç¬¬ä¸€ä¸ªçš„ä¾‹å­æ˜¯ä¸¤ä¸ªã€‚ä¸€ï¼ŒäºŒã€‚ä½†æ˜¯ä½ ä¹Ÿå¯ä»¥åšä¸¤ä¸ªåŠ ä¸‰ã€‚æ‰€ä»¥ä¸‰å¹¶ä¸ç­‰åŒäºäºŒï¼Œä½†æœ‰ç‚¹ç›¸ä¼¼ï¼Œè¿™å°±æ˜¯ä¸€ä»¶äº‹ã€‚å¦ä¸€ä¸ªä¾‹å­æ˜¯ä½ æœ‰ä¸€ä¸ªæ ‡è®°ï¼Œçªç„¶å®ƒè¢«ä¸€æ¬¡æ€§æ ‡è®°åœ¨ä¸€èµ·ï¼Œç„¶ååˆè¢«æ‹†åˆ†å¼€ï¼Œä½ çœ‹åˆ°è¿™ä¸ªæ ‡è®°ï¼Œç„¶åçœ‹åˆ°å¯èƒ½æ˜¯æ ‡è®°çš„ç¬¬ä¸€éƒ¨åˆ†ï¼Œç„¶åä½ é¢„æµ‹ç¬¬äºŒéƒ¨åˆ†ã€‚
- en: ğŸ˜Šï¼Œå—¯ã€‚I think the thing that's really striking about this is think there are all
    in some ways a really crude kind of in context learningã€‚And and in particularï¼Œ
    these models get about 0ã€‚1nas rather than about 0ã€‚4nas up in context learning
    and they never go through the phase changeã€‚so they're doing some kind of really
    crude in context learning and also they're dedicating almost all their attention
    heads to this kind of crude in context learningã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå—¯ã€‚æˆ‘è®¤ä¸ºè¿™éå¸¸å¼•äººæ³¨ç›®çš„æ˜¯ï¼ŒæŸç§ç¨‹åº¦ä¸Šè¿™éƒ½æ˜¯ä¸€ç§éå¸¸ç²—ç³™çš„ä¸Šä¸‹æ–‡å­¦ä¹ ã€‚è€Œä¸”å°¤å…¶æ˜¯ï¼Œè¿™äº›æ¨¡å‹çš„ä¸Šä¸‹æ–‡å­¦ä¹ å¤§çº¦æ˜¯0.1è€Œä¸æ˜¯0.4ï¼Œå¹¶ä¸”å®ƒä»¬ä»æœªç»å†ç›¸ä½å˜åŒ–ã€‚å› æ­¤ï¼Œå®ƒä»¬è¿›è¡Œäº†ä¸€ç§éå¸¸ç²—ç³™çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå¹¶ä¸”å‡ ä¹å°†æ‰€æœ‰çš„æ³¨æ„åŠ›å¤´éƒ½æŠ•å…¥åˆ°è¿™ç§ç²—ç³™çš„ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ã€‚
- en: so they're not very good at itï¼Œ but they're dedicating their capacity to itã€‚ğŸ˜Šã€‚I'm
    noticing that it's 1037ï¼Œ and I want to just check how long I can go because I
    maybe I should like super accelerate ifesã€‚OhChrisï¼Œ I think it's fine because like
    students are also asking questions in betweenã€‚So you shouldn be goodã€‚ Okayï¼Œ so
    maybe my plan will be that I'll talk until like 1055 or 11ã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å®ƒä»¬å¹¶ä¸æ˜¯å¾ˆæ“…é•¿ï¼Œä½†å®ƒä»¬æ­£åœ¨æŠ•å…¥èƒ½åŠ›ã€‚ğŸ˜Šã€‚æˆ‘æ³¨æ„åˆ°ç°åœ¨æ˜¯1037ï¼Œæˆ‘æƒ³æ£€æŸ¥ä¸€ä¸‹æˆ‘èƒ½æŒç»­å¤šä¹…ï¼Œå› ä¸ºæˆ‘å¯èƒ½åº”è¯¥åŠ é€Ÿä¸€ä¸‹ã€‚å“¦ï¼Œå…‹é‡Œæ–¯ï¼Œæˆ‘è§‰å¾—æ²¡å…³ç³»ï¼Œå› ä¸ºå­¦ç”Ÿä»¬ä¹Ÿåœ¨é—´æ­‡ä¸­æé—®ã€‚æ‰€ä»¥ä½ åº”è¯¥æ²¡é—®é¢˜ã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘çš„è®¡åˆ’å¯èƒ½æ˜¯æˆ‘ä¼šè®²åˆ°1055æˆ–11ç‚¹ã€‚
- en: And then if youï¼Œ I can go and answer questions for a while after after thatã€‚Yeahï¼Œ
    it worksã€‚fantasticã€‚So you can see this as a very con crude kind of in context
    learning like basically what we're saying is it's sort of all this flavor of okayã€‚well
    I saw this tokenï¼Œ probably these other tokensã€‚the same token or similar tokens
    are more likely to go and acc cur laterator and look this is an opportunity that
    sort of looks like I can inject the token that I saw earlier I'm going to inject
    it here and say that it's more likely that' like that's basically what it's doingã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¦‚æœä½ ï¼Œæˆ‘å¯ä»¥åœ¨ä¹‹åå›ç­”ä¸€æ®µæ—¶é—´çš„é—®é¢˜ã€‚æ˜¯çš„ï¼Œæ²¡é—®é¢˜ã€‚å¤ªå¥½äº†ã€‚æ‰€ä»¥ä½ å¯ä»¥æŠŠè¿™çœ‹ä½œæ˜¯ä¸€ç§éå¸¸ç²—ç³™çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬æ‰€è¯´çš„æ˜¯ï¼Œè¿™ç§æƒ…å†µç±»ä¼¼äºæˆ‘çœ‹åˆ°è¿™ä¸ªæ ‡è®°ï¼Œå¯èƒ½è¿™äº›å…¶ä»–æ ‡è®°ï¼Œç±»ä¼¼çš„æ ‡è®°æ›´æœ‰å¯èƒ½å»ç»§ç»­å…³è”ï¼Œçœ‹çœ‹è¿™æ˜¯ä¸€ä¸ªæœºä¼šï¼Œçœ‹èµ·æ¥æˆ‘å¯ä»¥æ³¨å…¥æˆ‘ä¹‹å‰çœ‹åˆ°çš„æ ‡è®°ï¼Œæˆ‘å°†æŠŠå®ƒæ³¨å…¥åˆ°è¿™é‡Œï¼Œè¡¨æ˜å®ƒæ›´æœ‰å¯èƒ½ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯è¿™æ ·ã€‚
- en: And it's dedicating almost all of its capacity to that so you know these it's
    sort of the opposite of what we thought with RnNs in the past like used to be
    that everyone was like oh you know RNNsã€‚it's so hard to get the care about long
    distance contextã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”å®ƒå‡ ä¹æŠŠæ‰€æœ‰èƒ½åŠ›éƒ½æŠ•å…¥åˆ°äº†è¿™ä¸€ç‚¹ä¸Šï¼Œæ‰€ä»¥ä½ çŸ¥é“ï¼Œè¿™æœ‰ç‚¹å’Œæˆ‘ä»¬è¿‡å»å¯¹RNNçš„æƒ³æ³•ç›¸åï¼Œè¿‡å»æ¯ä¸ªäººéƒ½è§‰å¾—å“¦ï¼Œä½ çŸ¥é“RNNã€‚è¦è€ƒè™‘é•¿è·ç¦»ä¸Šä¸‹æ–‡æ˜¯å¤šä¹ˆå›°éš¾ã€‚
- en: you know maybe we need to go and like use dams or something no if you train
    a transformer it dedicates and you give it a a long enough context it's dedicating
    almost all its capacity to this type of stuff just kind of interestingã€‚There are
    some attention ins which are more primarily positionalã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œä¹Ÿè®¸æˆ‘ä»¬éœ€è¦å»ä½¿ç”¨æ°´åä¹‹ç±»çš„ä¸œè¥¿ï¼Œä½†å¦‚æœä½ è®­ç»ƒä¸€ä¸ªå˜æ¢å™¨ï¼Œå®ƒä¼šä¸“æ³¨äºä½ ç»™å®ƒçš„è¶³å¤Ÿé•¿çš„ä¸Šä¸‹æ–‡ï¼Œå®ƒå‡ ä¹å°†å…¶å…¨éƒ¨èƒ½åŠ›éƒ½æŠ•å…¥åˆ°è¿™ç±»ä¸œè¥¿ä¸­ï¼Œè¿™çœŸæœ‰è¶£ã€‚æœ‰ä¸€äº›æ³¨æ„åŠ›å¤´ä¸»è¦æ˜¯ä½ç½®æ€§çš„ã€‚
- en: usually we a model that I've been training that has two layer or it's only a
    one layer model has 12 attention endsã€‚and usually around two or three of those
    will become these more positional that are shorter term things that do something
    more like local trigram statistics and then everything else becomes these skip
    trisã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: é€šå¸¸æˆ‘è®­ç»ƒçš„æ¨¡å‹æ˜¯ä¸€ä¸ªæœ‰ä¸¤å±‚çš„æ¨¡å‹ï¼Œæˆ–è€…åªæ˜¯ä¸€ä¸ªæœ‰12ä¸ªæ³¨æ„åŠ›å¤´çš„å•å±‚æ¨¡å‹ã€‚é€šå¸¸å…¶ä¸­ä¸¤ä¸ªæˆ–ä¸‰ä¸ªå°†æˆä¸ºè¿™äº›æ›´å…·ä½ç½®æ€§çš„ã€è¾ƒçŸ­æœŸçš„ä¸œè¥¿ï¼Œåšä¸€äº›æ›´åƒæ˜¯å±€éƒ¨ä¸‰å…ƒç»„ç»Ÿè®¡çš„äº‹æƒ…ï¼Œè€Œå…¶ä»–ä¸€åˆ‡éƒ½ä¼šå˜æˆè¿™äº›è·³è·ƒä¸‰å…ƒç»„ã€‚
- en: Yeahï¼Œ so some takeaways from thisï¼Œ yeahï¼Œ you can understand one layer additional
    only transformers in terms of these OV and QK circuitsã€‚Transformers desperately
    want to do in context learningï¼Œ they desperatelyã€‚desperately desperately want
    to go and look at these long distance contacts and and predict thingsã€‚there's
    just so much so much entropy that they can go and reduce out of thatã€‚ğŸ˜¡ï¼Œå—¯ã€‚ğŸ˜Šã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ‰€ä»¥ä¸€äº›æ”¶è·æ˜¯ï¼Œä½ å¯ä»¥ç†è§£å•å±‚é™„åŠ å˜æ¢å™¨çš„OVå’ŒQKç”µè·¯ã€‚å˜æ¢å™¨éå¸¸æ¸´æœ›è¿›è¡Œä¸Šä¸‹æ–‡å­¦ä¹ ï¼Œå®ƒä»¬è¿«åˆ‡ï¼Œè¿«åˆ‡ï¼Œè¿«åˆ‡æƒ³å»çœ‹è¿™äº›é•¿è·ç¦»çš„è”ç³»å¹¶é¢„æµ‹äº‹æƒ…ã€‚å®ƒä»¬å¯ä»¥ä»ä¸­å‡å°‘é‚£ä¹ˆå¤šç†µã€‚ğŸ˜¡ï¼Œå—¯ã€‚ğŸ˜Šã€‚
- en: The constraints of a one layer are intention transformer force it to make certain
    bugsã€‚but it won't do the right thingã€‚And if you freeze the attention patternssï¼Œ
    these models are linearã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_21.png)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å•å±‚çš„é™åˆ¶ä½¿å¾—æ„å›¾å˜æ¢å™¨è¿«ä½¿å®ƒäº§ç”ŸæŸäº›é”™è¯¯ã€‚ä½†å®ƒä¸ä¼šåšæ­£ç¡®çš„äº‹æƒ…ã€‚å¦‚æœä½ å†»ç»“æ³¨æ„åŠ›æ¨¡å¼ï¼Œè¿™äº›æ¨¡å‹æ˜¯çº¿æ€§çš„ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_21.png)
- en: Okayï¼Œ a quick aside because so far this type of work has required us to do a
    lot of very manual inspection like we're walking through these giant matricesã€‚but
    there's a way that we can escape that we don't have to use look at these giant
    matrices if we don't want toã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œç¨å¾®æ’ä¸€å¥ï¼Œå› ä¸ºåˆ°ç›®å‰ä¸ºæ­¢ï¼Œè¿™ç§å·¥ä½œéœ€è¦æˆ‘ä»¬è¿›è¡Œå¤§é‡çš„æ‰‹åŠ¨æ£€æŸ¥ï¼Œå°±åƒæˆ‘ä»¬åœ¨èµ°è¿‡è¿™äº›å·¨å¤§çš„çŸ©é˜µã€‚ä½†æœ‰ä¸€ç§æ–¹æ³•å¯ä»¥è®©æˆ‘ä»¬é€ƒè„±ï¼Œå¦‚æœæˆ‘ä»¬ä¸æƒ³çš„è¯ï¼Œæˆ‘ä»¬ä¸å¿…æŸ¥çœ‹è¿™äº›å·¨å¤§çš„çŸ©é˜µã€‚
- en: ğŸ˜Šã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_23.png)
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_23.png)
- en: We can use ienvalue some eigvectorsï¼Œ so recall that an icon canvalueã€‚And eigenvector
    just means that if you multiply that vectorctor by the matrixã€‚it's equivalent
    to just scalingã€‚And often in my experiencesã€‚this haven't been very useful for
    interpreterability because we're usually mapping between different spacesã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œæ‰€ä»¥å›æƒ³ä¸€ä¸‹ï¼Œç‰¹å¾å€¼çš„å®šä¹‰ã€‚ç‰¹å¾å‘é‡åªæ˜¯æ„å‘³ç€å¦‚æœä½ å°†é‚£ä¸ªå‘é‡ä¹˜ä»¥çŸ©é˜µï¼Œå®ƒç­‰ä»·äºä»…ä»…æ˜¯ç¼©æ”¾ã€‚åœ¨æˆ‘ç»éªŒä¸­ï¼Œè¿™é€šå¸¸å¯¹äºå¯è§£é‡Šæ€§å¹¶æ²¡æœ‰å¤ªå¤§å¸®åŠ©ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸åœ¨ä¸åŒç©ºé—´ä¹‹é—´è¿›è¡Œæ˜ å°„ã€‚
- en: but if you're mapping onto the same spaceï¼Œ eigenvalues and eigenvectors are
    a beautiful way to think about guysã€‚ğŸ˜Šï¼ŒSo we're going to draw them on a raial plot
    and we're going to have a log radial scale because they're going to vary their
    magnitude is going to vary in by many orders of magnitudeã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¦‚æœä½ æ˜ å°„åˆ°åŒä¸€ä¸ªç©ºé—´ï¼Œç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡æ˜¯ä¸€ä¸ªç¾å¦™çš„æ–¹å¼æ¥æ€è€ƒè¿™äº›ä¸œè¥¿ã€‚ğŸ˜Šå› æ­¤ï¼Œæˆ‘ä»¬å°†å®ƒä»¬ç»˜åˆ¶åœ¨æåæ ‡å›¾ä¸Šï¼Œå¹¶å°†ä½¿ç”¨å¯¹æ•°æåæ ‡å°ºåº¦ï¼Œå› ä¸ºå®ƒä»¬çš„å¹…åº¦ä¼šå˜åŒ–ï¼Œå˜åŒ–çš„æ•°é‡çº§ä¼šæœ‰å¾ˆå¤šä¸ªæ•°é‡çº§ã€‚
- en: Okay so we can just go and you know our OV circuit maps from tokens to tokens
    that's the same vector space on the input in the output and we can ask you know
    what does it mean if we see eigenvalues of a particular kind well positive eigenvalues
    and this is really the most important part mean copying so if you have a positive
    eigenvalue it means that there's some set of tokens where if you see them you
    increase their probabilityã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç»§ç»­ï¼Œæˆ‘ä»¬çš„OVç”µè·¯å°†ä»¤ç‰Œæ˜ å°„åˆ°ä»¤ç‰Œï¼Œè¿™åœ¨è¾“å…¥å’Œè¾“å‡ºä¸­æ˜¯ç›¸åŒçš„å‘é‡ç©ºé—´ï¼Œæˆ‘ä»¬å¯ä»¥é—®ï¼Œå¦‚æœæˆ‘ä»¬çœ‹åˆ°æŸç§ç‰¹å®šç±»å‹çš„ç‰¹å¾å€¼ï¼Œè¿™æ„å‘³ç€ä»€ä¹ˆï¼Œå¥½å§ï¼Œæ­£ç‰¹å¾å€¼ï¼Œè€Œè¿™å®é™…ä¸Šæ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼Œæ„å‘³ç€å¤åˆ¶ã€‚æ‰€ä»¥å¦‚æœä½ æœ‰ä¸€ä¸ªæ­£ç‰¹å¾å€¼ï¼Œè¿™æ„å‘³ç€å­˜åœ¨ä¸€ç»„ä»¤ç‰Œï¼Œå½“ä½ çœ‹åˆ°å®ƒä»¬æ—¶ï¼Œä½ å¢åŠ äº†å®ƒä»¬çš„æ¦‚ç‡ã€‚
- en: And if you have a lot of positive eigenvaluesï¼Œ you're doing a lot of copyingã€‚if
    you only have positive eigenvaluesï¼Œ everything you do is copyingã€‚Now imaginary
    eigenvalues mean that you see a token and then you want to go and increase the
    probability of unrelated tokens and finally negative eigenvalues are anticocking
    they're likeã€‚if you see this tokenï¼Œ you make it less probable in the futureã€‚ğŸ˜¡ã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æœ‰å¾ˆå¤šæ­£ç‰¹å¾å€¼ï¼Œä½ åœ¨åšå¾ˆå¤šå¤åˆ¶ã€‚å¦‚æœä½ åªæœ‰æ­£ç‰¹å¾å€¼ï¼Œé‚£ä¹ˆä½ æ‰€åšçš„ä¸€åˆ‡éƒ½æ˜¯å¤åˆ¶ã€‚ç°åœ¨ï¼Œè™šæ•°ç‰¹å¾å€¼æ„å‘³ç€ä½ çœ‹åˆ°ä¸€ä¸ªè¯ï¼Œç„¶åä½ æƒ³å¢åŠ æ— å…³è¯çš„æ¦‚ç‡ï¼Œæœ€åè´Ÿç‰¹å¾å€¼æ˜¯åå¤åˆ¶çš„ï¼Œå®ƒä»¬å°±åƒï¼Œå¦‚æœä½ çœ‹åˆ°è¿™ä¸ªè¯ï¼Œä½ å°±ä¼šé™ä½å®ƒæœªæ¥å‡ºç°çš„æ¦‚ç‡ã€‚ğŸ˜¡
- en: Well that's really nice because now we don't have to go and dig through these
    giant matrices that are vocab size by vocab size we can just look at the eigenvalues
    and so these are the eigenvalues for our one layer attentionally transformer and
    we can see that you know forã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œè¿™çœŸçš„å¾ˆä¸é”™ï¼Œå› ä¸ºç°åœ¨æˆ‘ä»¬ä¸éœ€è¦å»æŒ–æ˜è¿™äº›å·¨å¤§çš„çŸ©é˜µï¼ˆè¯æ±‡å¤§å°ä¹˜ä»¥è¯æ±‡å¤§å°ï¼‰ï¼Œæˆ‘ä»¬åªéœ€æŸ¥çœ‹ç‰¹å¾å€¼ï¼Œè¿™äº›æ˜¯æˆ‘ä»¬å•å±‚æ³¨æ„åŠ›å˜æ¢å™¨çš„ç‰¹å¾å€¼ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå—¯ã€‚
- en: ğŸ˜Šï¼ŒMany of these they're almost entirely positiveï¼Œ these ones are sort of entirely
    positiveã€‚these ones are almost entirely positive and really these ones are even
    almost entirely positive and there's only two that have a significant number of
    imaginary and negative eigenvaluesã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Š è®¸å¤šè¿™äº›å‡ ä¹éƒ½æ˜¯æ­£çš„ï¼Œè¿™äº›å‡ ä¹å®Œå…¨æ˜¯æ­£çš„ï¼Œå®é™…ä¸Šè¿™äº›å‡ ä¹å®Œå…¨æ˜¯æ­£çš„ï¼Œåªæœ‰ä¸¤ä¸ªæœ‰å¤§é‡çš„è™šæ•°å’Œè´Ÿç‰¹å¾å€¼ã€‚
- en: ğŸ˜Šï¼ŒAnd so what this is telling us is it's it's just in one picture we can seeï¼Œ
    you knowï¼Œ okayã€‚they're reallyï¼Œ you knowã€‚10 out of 12 of these of these attention
    heads are just doing copying they just they just are doing this long distance
    you know well I saw token probably it's going to occur again type stuff that's
    kind of cool we can we can summarize it really quicklyã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Š æ‰€ä»¥è¿™å‘Šè¯‰æˆ‘ä»¬çš„æ˜¯ï¼Œé€šè¿‡ä¸€å¹…å›¾ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œå—¯ï¼Œå¥½å§ã€‚è¿™12ä¸ªæ³¨æ„åŠ›å¤´ä¸­æœ‰10ä¸ªåªæ˜¯åšå¤åˆ¶ï¼Œå®ƒä»¬å°±æ˜¯åœ¨åšè¿™ç§é•¿è·ç¦»çš„äº‹æƒ…ï¼Œå—¯ï¼Œæˆ‘çœ‹åˆ°è¿™ä¸ªè¯å¯èƒ½ä¼šå†æ¬¡å‡ºç°ï¼Œè¿™çœŸä¸é”™ï¼Œæˆ‘ä»¬å¯ä»¥éå¸¸å¿«é€Ÿåœ°æ€»ç»“å®ƒã€‚
- en: ğŸ˜Šï¼ŒOkayï¼Œ now the other thing that you can yeahï¼Œ so this is this is for a secondã€‚we're
    going to look at a two layer model in a second and'll we'll see that also a lot
    of its heads are doing this kind of copyingish stuffã€‚they have large positive
    eigenvaluesã€‚ğŸ˜Šï¼ŒYou can do a histogram like you know one thing at school is you
    can just add up the eenvalue and divide them by their absolute values and you
    get a number between zero and oneã€‚which is like how copying how copying is's just
    the head or between negative one and one how copying is's just the head you can
    just do a histogram and you can see oh yeah almost all the heads are doing doing
    lots of copyingã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Š å¥½çš„ï¼Œç°åœ¨ä½ å¯ä»¥ï¼Œå—¯ï¼Œè¿™ä¸ªæ˜¯ä¸ºäº†ç¬¬äºŒã€‚æˆ‘ä»¬ç¨åå°†æŸ¥çœ‹ä¸€ä¸ªåŒå±‚æ¨¡å‹ï¼Œæˆ‘ä»¬ä¼šå‘ç°å®ƒçš„è®¸å¤šå¤´ä¹Ÿåœ¨åšè¿™ç§å¤åˆ¶çš„äº‹æƒ…ã€‚å®ƒä»¬æœ‰å¾ˆå¤§çš„æ­£ç‰¹å¾å€¼ã€‚ğŸ˜Š ä½ å¯ä»¥åšä¸€ä¸ªç›´æ–¹å›¾ï¼Œåƒå­¦æ ¡é‡Œçš„ä¸€æ ·ï¼Œä½ å¯ä»¥æŠŠç‰¹å¾å€¼åŠ èµ·æ¥ï¼Œç„¶åé™¤ä»¥å®ƒä»¬çš„ç»å¯¹å€¼ï¼Œè¿™æ ·ä½ å°±ä¼šå¾—åˆ°ä¸€ä¸ªä»‹äºé›¶å’Œä¸€ä¹‹é—´çš„æ•°å­—ï¼Œè¿™å°±åƒæ˜¯å¤åˆ¶çš„ç¨‹åº¦ã€‚
- en: You it's nice to be able to go and summarize your model and I think this is
    sort of like we've gone from a very bottom up way and we didn't start with assumptions
    about what model2 and we tried to understand its structure and then we were able
    to summarize it in useful ways and now we're able to go and say something about
    itã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿæ€»ç»“ä½ çš„æ¨¡å‹æ˜¯å¾ˆå¥½çš„ï¼Œæˆ‘è§‰å¾—è¿™æœ‰ç‚¹åƒæˆ‘ä»¬ä»éå¸¸åº•å±‚çš„æ–¹å¼å¼€å§‹ï¼Œæˆ‘ä»¬æ²¡æœ‰å¯¹æ¨¡å‹åšå‡ºå‡è®¾ï¼Œè€Œæ˜¯å°è¯•ç†è§£å…¶ç»“æ„ï¼Œç„¶åæˆ‘ä»¬èƒ½å¤Ÿä»¥æœ‰ç”¨çš„æ–¹å¼æ€»ç»“å®ƒï¼Œç°åœ¨æˆ‘ä»¬èƒ½å¤Ÿè¯´äº›å…³äºå®ƒçš„äº‹æƒ…ã€‚
- en: Now another thing you might ask is what do the eigenvalues of the QK circuit
    mean and in our example so far they haven't been that they wouldn't have been
    that interestingã€‚but in a minute they will be and so I'll briefly describe what
    they mean a positive iconenvalue would mean you want to attend to the same tokensã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ä½ å¯èƒ½ä¼šé—®çš„æ˜¯ï¼ŒQKç”µè·¯çš„ç‰¹å¾å€¼æ˜¯ä»€ä¹ˆæ„æ€ï¼Œåœ¨æˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢çš„ä¾‹å­ä¸­ï¼Œå®ƒä»¬å¹¶ä¸é‚£ä¹ˆæœ‰è¶£ã€‚ä½†ç¨åå®ƒä»¬ä¼šå˜å¾—æœ‰è¶£ï¼Œå› æ­¤æˆ‘ä¼šç®€å•æè¿°å®ƒä»¬çš„æ„ä¹‰ï¼Œæ­£ç‰¹å¾å€¼æ„å‘³ç€ä½ æƒ³å…³æ³¨ç›¸åŒçš„è¯ã€‚
- en: ğŸ˜¡ï¼ŒAnd imagineagin your eigenvalueï¼Œ and this is what you would mostly see in
    our the models that we've seen so far means you want to go in and attend to a
    unrelated or different token and a negative eenvalue would mean you want to avoid
    attending to the same profileã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜¡ æƒ³è±¡ä¸€ä¸‹ä½ çš„ç‰¹å¾å€¼ï¼Œè¿™åœ¨æˆ‘ä»¬åˆ°ç›®å‰ä¸ºæ­¢çœ‹åˆ°çš„æ¨¡å‹ä¸­ï¼Œä½ ä¸»è¦ä¼šçœ‹åˆ°çš„æ„æ€æ˜¯ä½ æƒ³å…³æ³¨ä¸€ä¸ªæ— å…³æˆ–ä¸åŒçš„è¯ï¼Œè€Œè´Ÿç‰¹å¾å€¼åˆ™æ„å‘³ç€ä½ æƒ³é¿å…å…³æ³¨ç›¸åŒçš„ç‰¹å¾ã€‚
- en: So that will be relevant in secondã€‚Yeahï¼Œ so those are going to mostly be useful
    to think about in multilayer attention in transformers when we can have chains
    of attention hint and so we can ask you knowã€‚wellï¼Œ I'll get to that in a secondã€‚ğŸ˜Šï¼ŒYeah
    so there's a table summarizing that unfortunately this approach completely breaks
    down once you have MLP layers MLP layersã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°†åœ¨ç¨åç›¸å…³ã€‚æ˜¯çš„ï¼Œè¿™äº›åœ¨å¤šå±‚æ³¨æ„åŠ›å˜æ¢å™¨ä¸­é€šå¸¸ä¼šå¾ˆæœ‰ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥æœ‰æ³¨æ„åŠ›é“¾ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥é—®ä½ ï¼Œå—¯ï¼Œæˆ‘é©¬ä¸Šä¼šè¯´ã€‚ğŸ˜Šæ˜¯çš„ï¼Œæ‰€ä»¥æœ‰ä¸€å¼ æ€»ç»“è¡¨ï¼Œä¸å¹¸çš„æ˜¯ï¼Œä¸€æ—¦ä½ æœ‰
    MLP å±‚ï¼Œè¿™ç§æ–¹æ³•å®Œå…¨å¤±æ•ˆã€‚
- en: you know now you have these nonlinearities since you don't get this property
    where your model is mostly linear and you can you can just look at a matrixã€‚but
    if you're working with only attentionally transformers this is a very nice way
    to think about Pã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ï¼Œç°åœ¨ä½ æœ‰äº†è¿™äº›éçº¿æ€§ï¼Œå› ä¸ºä½ æ²¡æœ‰è¿™ç§å±æ€§ï¼Œå³æ¨¡å‹ä¸»è¦æ˜¯çº¿æ€§çš„ï¼Œä½ å¯ä»¥ä»…ä»…æŸ¥çœ‹ä¸€ä¸ªçŸ©é˜µã€‚ä½†å¦‚æœä½ åªå¤„ç†æ³¨æ„åŠ›å˜æ¢å™¨ï¼Œè¿™æ˜¯æ€è€ƒ P çš„ä¸€ç§å¾ˆå¥½çš„æ–¹å¼ã€‚
- en: ğŸ˜Šã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_25.png)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_25.png)
- en: Okayï¼Œ so recall that one layer attentionally transformers don't undergo this
    phase change that we talked about at in the beginning likere right now we're on
    a huntã€‚we're trying to go and answer this mystery of how what the hell is going
    on in that phase change where models suddenly get good at in context learning
    we want to answer that and one layer attentionally transformers don't undergo
    that phase change but two layer attentionally transformers do so we'd like to
    know what's different about two layer attentionally transformersã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œå›æƒ³ä¸€ä¸‹ï¼Œå•å±‚æ³¨æ„åŠ›å˜æ¢å™¨ä¸ä¼šç»å†æˆ‘ä»¬ä¸€å¼€å§‹è°ˆåˆ°çš„ç›¸å˜ã€‚å°±åƒç°åœ¨æˆ‘ä»¬åœ¨å¯»æ‰¾ï¼Œæˆ‘ä»¬è¯•å›¾è§£ç­”è¿™ä¸ªç›¸å˜çš„å¥¥ç§˜ï¼Œæ¨¡å‹æ˜¯å¦‚ä½•çªç„¶åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­å˜å¾—ä¼˜ç§€çš„ã€‚æˆ‘ä»¬æƒ³è¦å›ç­”è¿™ä¸ªé—®é¢˜ï¼Œè€Œå•å±‚æ³¨æ„åŠ›å˜æ¢å™¨ä¸ç»å†è¿™ä¸ªç›¸å˜ï¼Œä½†åŒå±‚æ³¨æ„åŠ›å˜æ¢å™¨ä¼šç»å†ï¼Œæ‰€ä»¥æˆ‘ä»¬æƒ³çŸ¥é“åŒå±‚æ³¨æ„åŠ›å˜æ¢å™¨æœ‰ä»€ä¹ˆä¸åŒã€‚
- en: ğŸ˜¡ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_27.png)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜¡ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_27.png)
- en: Okayï¼Œ wellï¼Œ so in our in our previous when we were dealing with one layer of
    attention transformersã€‚we were able to go and rewrite them in this form and it
    gave us a lot of ability to go and understand the model because we could go and
    say well you know this is bygrams and then each one of these is looking somewhere
    and we hit this matrix that describes how it affects thingsã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œåœ¨ä¹‹å‰å¤„ç†å•å±‚æ³¨æ„åŠ›å˜æ¢å™¨æ—¶ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä»¥è¿™ç§å½¢å¼é‡å†™å®ƒä»¬ï¼Œè¿™è®©æˆ‘ä»¬æœ‰äº†å¾ˆå¤§çš„ç†è§£æ¨¡å‹çš„èƒ½åŠ›ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥è¯´ï¼Œè¿™å°±æ˜¯åŒå…ƒç»„ï¼Œè€Œè¿™äº›æ¯ä¸€ä¸ªéƒ½åœ¨æŸä¸ªåœ°æ–¹æŸ¥æ‰¾ï¼Œæˆ‘ä»¬å¾—åˆ°äº†è¿™ä¸ªæè¿°å…¶å½±å“çš„çŸ©é˜µã€‚
- en: And yeahï¼Œ so that gave us a lot of ability to think about these thingsã€‚and we
    can also just write in this factored form where we have the embedding and then
    we have the attention heads and then we have the un embeddingã€‚ğŸ˜Šï¼ŒOkay wellï¼Œ and
    for simplicityï¼Œ we often go and write WOV for WO WV because they always come togetherã€‚it's
    always the case like it's in some senseï¼Œ an illusion that WO and WV are different
    matricesã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ï¼Œè¿™è®©æˆ‘ä»¬åœ¨æ€è€ƒè¿™äº›é—®é¢˜æ—¶æœ‰äº†å¾ˆå¤§çš„èƒ½åŠ›ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ç”¨è¿™ç§åˆ†è§£å½¢å¼ä¹¦å†™ï¼Œå…¶ä¸­åŒ…å«åµŒå…¥ã€æ³¨æ„åŠ›å¤´å’ŒååµŒå…¥ã€‚ğŸ˜Šå¥½å§ï¼Œä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬é€šå¸¸å†™ WOV è€Œä¸æ˜¯
    WO WVï¼Œå› ä¸ºå®ƒä»¬æ€»æ˜¯ä¸€èµ·å‡ºç°ã€‚è¿™åœ¨æŸç§æ„ä¹‰ä¸Šæ˜¯ä¸€ä¸ªå¹»è§‰ï¼ŒWO å’Œ WV æ˜¯ä¸åŒçš„çŸ©é˜µã€‚
- en: they're just one low rank matrix're never they're always used together and similar
    WQ and WK it's sort of an illusion that they' they're different matrices they're
    always just used together and keys and queries are just sort of they're just an
    artifact of these low rank matricesã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶å®å®ƒä»¬åªæ˜¯ä¸€ä¸ªä½ç§©çŸ©é˜µï¼Œå®ƒä»¬æ€»æ˜¯ä¸€èµ·ä½¿ç”¨ï¼Œç±»ä¼¼çš„ WQ å’Œ WK ä¹Ÿæ˜¯ä¸€ç§å¹»è§‰ï¼Œå®ƒä»¬æ˜¯ä¸åŒçš„çŸ©é˜µï¼Œä½†æ€»æ˜¯ä¸€èµ·ä½¿ç”¨ï¼Œé”®å’ŒæŸ¥è¯¢åªæ˜¯è¿™äº›ä½ç§©çŸ©é˜µçš„ä¸€ä¸ªå‰¯äº§å“ã€‚
- en: So in anyï¼Œ it's useful we want to write those togetherã€‚Okayï¼Œ greatã€‚so a two
    layer intentionally transformerï¼Œ what we do is we go through the embedding matrixã€‚ğŸ˜Šã€‚Then
    we go through the layer one attention endï¼Œ then we go through the layer two attention
    endã€‚And then we go through the un embedding and for the attention headsã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ— è®ºå¦‚ä½•ï¼Œå°†å®ƒä»¬å†™åœ¨ä¸€èµ·æ˜¯æœ‰ç”¨çš„ã€‚å¥½çš„ï¼Œå¾ˆå¥½ã€‚æ‰€ä»¥åŒå±‚æœ‰æ„å˜æ¢å™¨ï¼Œæˆ‘ä»¬æ‰€åšçš„æ˜¯é€šè¿‡åµŒå…¥çŸ©é˜µã€‚ğŸ˜Šç„¶åé€šè¿‡ç¬¬ä¸€å±‚æ³¨æ„åŠ›ï¼Œå†é€šè¿‡ç¬¬äºŒå±‚æ³¨æ„åŠ›ã€‚æœ€åï¼Œæˆ‘ä»¬ç»è¿‡ååµŒå…¥ä»¥åŠæ³¨æ„åŠ›å¤´ã€‚
- en: we always have this identity as wellï¼Œ which corresponds just going down the
    residual stream so we can go down the residual stream or we can go through an
    attention headã€‚Next upï¼Œ we can also go down the residual stream where we can go
    through an attention headã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ€»æ˜¯æœ‰è¿™ä¸ªæ’ç­‰å¼ï¼Œå®ƒå¯¹åº”äºæ²¿ç€æ®‹å·®æµä¸‹å»ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ²¿ç€æ®‹å·®æµä¸‹å»ï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸€ä¸ªæ³¨æ„åŠ›å¤´ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æ²¿ç€æ®‹å·®æµä¸‹å»ï¼Œæˆ–è€…é€šè¿‡ä¸€ä¸ªæ³¨æ„åŠ›å¤´ã€‚
- en: And there's this useful identityï¼Œ the mixed product identity that any tensor
    product or other ways of interpreting this obeyã€‚which is that if you have an attention
    head and we have sameã€‚you know we have the weights and the attention pattern and
    the WOV matrix and the attention patternã€‚the attention patterns multiply together
    and the OV circuits multiply together and they behave flyã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ä¸ªæœ‰ç”¨çš„æ’ç­‰å¼ï¼Œæ··åˆä¹˜ç§¯æ’ç­‰å¼ï¼Œä»»ä½•å¼ é‡ä¹˜ç§¯æˆ–å…¶ä»–è§£é‡Šæ–¹å¼éƒ½éµå¾ªè¿™ä¸€ç‚¹ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªæ³¨æ„åŠ›å¤´ï¼Œä¸”æˆ‘ä»¬æœ‰ç›¸åŒçš„ï¼Œä½ çŸ¥é“ï¼Œæˆ‘ä»¬æœ‰æƒé‡å’Œæ³¨æ„åŠ›æ¨¡å¼ï¼Œä»¥åŠWOVçŸ©é˜µå’Œæ³¨æ„åŠ›æ¨¡å¼ï¼Œæ³¨æ„åŠ›æ¨¡å¼ç›¸ä¹˜ï¼ŒOVç”µè·¯ç›¸ä¹˜ï¼Œå®ƒä»¬çš„è¡Œä¸ºç›¸åŒã€‚
- en: ğŸ˜Šï¼ŒGreatï¼Œ so we can just expand out that equation we can just take that big product
    we had at the beginning and we can just expand it out and we get three different
    kinds of terms so one thing we do is we get this this path that just goes directly
    through the residual stream where we embed and unembed and that's going to want
    to represent some bigram statisticsã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¤ªå¥½äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ‰©å±•é‚£ä¸ªæ–¹ç¨‹å¼ï¼Œæˆ‘ä»¬å¯ä»¥å°†æœ€å¼€å§‹çš„é‚£ä¸ªå¤§ä¹˜ç§¯å±•å¼€ï¼Œå¾—åˆ°ä¸‰ç§ä¸åŒç±»å‹çš„é¡¹ã€‚æˆ‘ä»¬åšçš„ä¸€ä»¶äº‹æ˜¯å¾—åˆ°è¿™æ¡è·¯å¾„ï¼Œå®ƒç›´æ¥é€šè¿‡æ®‹å·®æµï¼Œè¿›è¡ŒåµŒå…¥å’Œè§£åµŒå…¥ï¼Œè¿™å°†æƒ³è¦è¡¨ç¤ºæŸäº›äºŒå…ƒç»Ÿè®¡ã€‚
- en: ğŸ˜Šï¼ŒThen we get things that look like the attention head terms that we had previouslyã€‚And
    finallyã€‚we get these terms that correspond to going through two attention headã€‚Andã€‚ğŸ˜Šã€‚Now
    it's worth noting that these terms are not actually the same as because the attention
    head the attention patterns in the second layer can be computed from the outputs
    of the first layerã€‚those are also going to be more expressiveï¼Œ but at a high level
    you can think of there as being these three different kinds of terms and we sometimes
    call these terms virtual attention heads because they don't exist like they aren't
    sort of explicitly represented in the modelã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç„¶åæˆ‘ä»¬å¾—åˆ°çœ‹èµ·æ¥åƒä¹‹å‰æ³¨æ„åŠ›å¤´é¡¹çš„ä¸œè¥¿ã€‚æœ€åï¼Œæˆ‘ä»¬å¾—åˆ°äº†å¯¹åº”äºé€šè¿‡ä¸¤ä¸ªæ³¨æ„åŠ›å¤´çš„é¡¹ã€‚å¹¶ä¸”ã€‚ğŸ˜Šã€‚ç°åœ¨å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›é¡¹å®é™…ä¸Šå¹¶ä¸ç›¸åŒï¼Œå› ä¸ºç¬¬äºŒå±‚çš„æ³¨æ„åŠ›å¤´å’Œæ³¨æ„åŠ›æ¨¡å¼å¯ä»¥ä»ç¬¬ä¸€å±‚çš„è¾“å‡ºä¸­è®¡ç®—å‡ºæ¥ã€‚å®ƒä»¬ä¹Ÿä¼šæ›´åŠ è¡¨è¾¾ï¼Œä½†åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œä½ å¯ä»¥å°†å…¶è§†ä¸ºè¿™ä¸‰ç§ä¸åŒçš„é¡¹ï¼Œæˆ‘ä»¬æœ‰æ—¶ç§°è¿™äº›é¡¹ä¸ºè™šæ‹Ÿæ³¨æ„åŠ›å¤´ï¼Œå› ä¸ºå®ƒä»¬å¹¶ä¸å­˜åœ¨ï¼Œæˆ–åœ¨æ¨¡å‹ä¸­å¹¶æœªæ˜ç¡®è¡¨ç¤ºã€‚
- en: but in fact they have an attention pattern they have no ease or they're sort
    in almost all functional ways like a tiny little attention head and there's exponentially
    many of themã€‚å—¯ã€‚Turns out they're not going to be that important in this modelï¼Œ
    but in other modelsã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®é™…ä¸Šï¼Œå®ƒä»¬å…·æœ‰æ³¨æ„åŠ›æ¨¡å¼ï¼Œå‡ ä¹åœ¨æ‰€æœ‰åŠŸèƒ½ä¸Šåƒä¸€ä¸ªå¾®å°çš„æ³¨æ„åŠ›å¤´ï¼Œè€Œä¸”å®ƒä»¬çš„æ•°é‡æ˜¯æŒ‡æ•°çº§çš„ã€‚å—¯ã€‚äº‹å®è¯æ˜ï¼Œå®ƒä»¬åœ¨è¿™ä¸ªæ¨¡å‹ä¸­å¹¶ä¸é‚£ä¹ˆé‡è¦ï¼Œä½†åœ¨å…¶ä»–æ¨¡å‹ä¸­å¯èƒ½æ˜¯ã€‚
- en: it can be importantã€‚Rightï¼Œ so one one thing that' I said this is it allows us
    to think about attention and in a really principled wayã€‚we don't have to go and
    think aboutã€‚You know I think there's like people people look at attention patterns
    all the time and I think a concern you to have as wellã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½å¾ˆé‡è¦ã€‚å¯¹ï¼Œæ­£å¦‚æˆ‘æ‰€è¯´ï¼Œè¿™è®©æˆ‘ä»¬èƒ½å¤Ÿä»¥ä¸€ç§çœŸæ­£æœ‰åŸåˆ™çš„æ–¹å¼æ€è€ƒæ³¨æ„åŠ›ã€‚æˆ‘ä»¬ä¸å¿…å»è€ƒè™‘ã€‚æˆ‘æƒ³äººä»¬ä¸€ç›´åœ¨è§‚å¯Ÿæ³¨æ„åŠ›æ¨¡å¼ï¼Œæˆ‘æƒ³ä½ ä¹Ÿä¼šæœ‰ä¸€äº›æ‹…å¿§ã€‚
- en: you know there's multiple attention patterns like you know the information that's
    being moved by one attention headã€‚it might have been moved there were by another
    attention ahead and not have originated there it might still be moved somewhere
    elseã€‚but in fact this give us a way to avoid all those concerns and just think
    about things in a single principle wayã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_29.png)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“æœ‰å¤šç§æ³¨æ„åŠ›æ¨¡å¼ï¼Œæ¯”å¦‚ä¿¡æ¯æ˜¯ç”±ä¸€ä¸ªæ³¨æ„åŠ›å¤´ç§»åŠ¨çš„ã€‚å®ƒå¯èƒ½æ˜¯ç”±å¦ä¸€ä¸ªæ³¨æ„åŠ›å¤´ç§»åŠ¨çš„ï¼Œå¹¶ä¸æ˜¯èµ·æºäºé‚£é‡Œï¼Œå®ƒå¯èƒ½ä»ç„¶è¢«ç§»åŠ¨åˆ°å…¶ä»–åœ°æ–¹ã€‚ä½†å®é™…ä¸Šï¼Œè¿™ç»™æˆ‘ä»¬æä¾›äº†ä¸€ç§é¿å…æ‰€æœ‰è¿™äº›é¡¾è™‘çš„æ–¹å¼ï¼Œåªéœ€ä»å•ä¸€åŸåˆ™çš„è§’åº¦æ€è€ƒäº‹ç‰©ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_29.png)
- en: Okayï¼Œ in any caseã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_31.png)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œåæ­£ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_31.png)
- en: An important question to ask is how important are these different terms we could
    study all of them how important are they and it turns out you can just there's
    an algorithm you can use where you knock out attention knock out these terms and
    you go and you ask how important are they and it turns out that by far the most
    important thing is these individual attention head terms in this model by far
    the most important thing the virtual attention heads basically don't matter that
    muchã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯ï¼Œè¿™äº›ä¸åŒçš„é¡¹æœ‰å¤šé‡è¦ã€‚æˆ‘ä»¬å¯ä»¥ç ”ç©¶æ‰€æœ‰è¿™äº›é¡¹ï¼Œå®ƒä»¬æœ‰å¤šé‡è¦ï¼Œäº‹å®è¯æ˜ï¼Œä½ å¯ä»¥ä½¿ç”¨ä¸€ä¸ªç®—æ³•ï¼Œé€šè¿‡æ·˜æ±°æ³¨æ„åŠ›ï¼Œæ·˜æ±°è¿™äº›é¡¹ï¼Œç„¶åä½ å»è¯¢é—®å®ƒä»¬æœ‰å¤šé‡è¦ï¼Œç»“æœæ˜¯ï¼Œåœ¨è¿™ä¸ªæ¨¡å‹ä¸­ï¼Œæœ€é‡è¦çš„æ— ç–‘æ˜¯è¿™äº›å•ç‹¬çš„æ³¨æ„åŠ›å¤´é¡¹ï¼Œè™šæ‹Ÿæ³¨æ„åŠ›å¤´åŸºæœ¬ä¸Šæ²¡æœ‰é‚£ä¹ˆé‡è¦ã€‚
- en: They only have an effective of 0ã€‚3nas using the above ones and the bigrams are
    still pretty usefulã€‚so if we want to try to and understandchan this model we should
    probably go and focus our attention on the virtual attention hints are not going
    to be the best way to go in and go in focus our attentionã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä»…ä½¿ç”¨ä¸Šè¿°æ–¹æ³•æœ‰æ•ˆä¸º0.3nasï¼Œè€ŒäºŒå…ƒç»„ä»ç„¶ç›¸å½“æœ‰ç”¨ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬æƒ³å°è¯•ç†è§£è¿™ä¸ªæ¨¡å‹ï¼Œå¯èƒ½åº”è¯¥å°†æ³¨æ„åŠ›é›†ä¸­åœ¨è™šæ‹Ÿæ³¨æ„åŠ›æç¤ºä¸Šï¼Œè¿™ä¸æ˜¯æœ€ä½³çš„è¿›å…¥æ–¹å¼ã€‚
- en: especially since there's a lot of them there's 124 of them for 0ã€‚3nas very little
    that you would understand pro studying one of those termssã€‚ğŸ˜Šã€‚So the thing that
    we probably want to do when we know that these are diagramgram statisticsã€‚so what
    we really want to do is we want to understand the individual attention head termsã€‚
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹åˆ«æ˜¯å› ä¸ºæœ‰å¾ˆå¤šè¿™æ ·çš„ä¾‹å­ï¼Œå…±æœ‰124ä¸ªï¼Œå¯¹äº0.3nasæ¥è¯´ï¼Œå­¦ä¹ è¿™äº›æœ¯è¯­å‡ ä¹æ²¡æœ‰ä»€ä¹ˆå¯ç†è§£çš„ã€‚ğŸ˜Šã€‚æ‰€ä»¥æˆ‘ä»¬çŸ¥é“è¿™äº›æ˜¯å›¾ç¤ºç»Ÿè®¡æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³è¦åšçš„äº‹æƒ…æ˜¯ç†è§£å„ä¸ªæ³¨æ„åŠ›å¤´çš„æœ¯è¯­ã€‚
- en: This is the algorithm I'm going to skip over it for time and we can ignore that
    term because it's small and it turns out also that the layer two attention tense
    are doing way more than layer one attention tense and that's that's surprising
    like the layer two attention tense are more expressive because they can use the
    layer one attention tense to construct their attention patternã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘å°†è·³è¿‡çš„ç®—æ³•ï¼Œå‡ºäºæ—¶é—´è€ƒè™‘ï¼Œæˆ‘ä»¬å¯ä»¥å¿½ç•¥è¿™ä¸ªæœ¯è¯­ï¼Œå› ä¸ºå®ƒå¾ˆå°ï¼Œäº‹å®è¯æ˜ï¼Œç¬¬äºŒå±‚çš„æ³¨æ„åŠ›å¤´æ¯”ç¬¬ä¸€å±‚çš„æ³¨æ„åŠ›å¤´åšå¾—æ›´å¤šï¼Œè¿™ä»¤äººæƒŠè®¶ï¼Œå› ä¸ºç¬¬äºŒå±‚çš„æ³¨æ„åŠ›å¤´æ›´å…·è¡¨ç°åŠ›ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥ä½¿ç”¨ç¬¬ä¸€å±‚çš„æ³¨æ„åŠ›å¤´æ¥æ„é€ å®ƒä»¬çš„æ³¨æ„åŠ›æ¨¡å¼ã€‚
- en: ğŸ˜Šï¼ŒOkayï¼Œ so if we could just go and understand the layer to attention headsã€‚we
    probably understand a lot of what's going on in this modelã€‚Andã€‚And the trick is
    that the attention heads are now constructed from the previous layer rather than
    just from the tokensã€‚so this is still the sameï¼Œ but the attention headã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¥½çš„ï¼Œæ‰€ä»¥å¦‚æœæˆ‘ä»¬èƒ½ç†è§£å±‚çº§çš„æ³¨æ„åŠ›å¤´ï¼Œæˆ‘ä»¬å¯èƒ½å°±èƒ½ç†è§£è¿™ä¸ªæ¨¡å‹ä¸­çš„è®¸å¤šå†…å®¹ã€‚è€Œä¸”ï¼Œå…³é”®æ˜¯è¿™äº›æ³¨æ„åŠ›å¤´ç°åœ¨æ˜¯ä»å‰ä¸€å±‚æ„é€ çš„ï¼Œè€Œä¸ä»…ä»…æ˜¯ä»æ ‡è®°ä¸­æ„é€ çš„ã€‚æ‰€ä»¥è¿™ä¸€ç‚¹ä»ç„¶ç›¸åŒï¼Œä½†æ³¨æ„åŠ›å¤´ã€‚
- en: the attention pattern is more more complex and if you write it out you get this
    complex equation that says you know you embed the tokens and you go and you shuffle
    things around using the attention heads for the keys and then you multiply by
    WQK then you multiply shuffle things around again with the queries and then you
    go and multiply by the embedding again because they were embedded and then you
    get back to the tokensã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„åŠ›æ¨¡å¼æ›´å¤æ‚ï¼Œå¦‚æœä½ æŠŠå®ƒå†™å‡ºæ¥ï¼Œä½ ä¼šå¾—åˆ°è¿™ä¸ªå¤æ‚çš„æ–¹ç¨‹ï¼Œè¯´æ˜ä½ åµŒå…¥æ ‡è®°ï¼Œç„¶åä½ ä½¿ç”¨æ³¨æ„åŠ›å¤´ä¸ºé”®è¿›è¡Œæ´—ç‰Œï¼Œç„¶åä¹˜ä»¥WQKï¼Œç„¶åå†æ¬¡ç”¨æŸ¥è¯¢æ´—ç‰Œï¼Œç„¶åå†æ¬¡ä¹˜ä»¥åµŒå…¥ï¼Œå› ä¸ºå®ƒä»¬è¢«åµŒå…¥ï¼Œç„¶åä½ åˆå›åˆ°æ ‡è®°ã€‚
- en: ğŸ˜Šï¼ŒAndã€‚ğŸ˜Šï¼Œå•Šã€‚But let's actually look at them so one thing that's remember that
    when we see positive eigenvalues in OB circuit we're doing copying so one thing
    we can say is well seven out of 12 and in fact the ones with the largest eigenvalues
    are doing copying so we still have a lot of attention that they're doing copyingã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè€Œä¸”ã€‚ğŸ˜Šï¼Œå•Šã€‚ä½†è®©æˆ‘ä»¬å®é™…çœ‹çœ‹å®ƒä»¬ï¼Œæ‰€ä»¥è¦è®°ä½ï¼Œå½“æˆ‘ä»¬åœ¨OBç”µè·¯ä¸­çœ‹åˆ°æ­£ç‰¹å¾å€¼æ—¶ï¼Œæˆ‘ä»¬åœ¨è¿›è¡Œå¤åˆ¶ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´æœ‰ä¸ƒä¸ªå‡ºè‡ª12ï¼Œå®é™…ä¸Šå…·æœ‰æœ€å¤§ç‰¹å¾å€¼çš„é‚£äº›æ­£åœ¨è¿›è¡Œå¤åˆ¶ï¼Œå› æ­¤æˆ‘ä»¬ä»ç„¶æœ‰å¾ˆå¤šæ³¨æ„åŠ›åœ¨è¿›è¡Œå¤åˆ¶ã€‚
- en: ğŸ˜Šï¼Œå—¯ã€‚And yeahï¼Œ the QK circuitï¼Œ so one thing you could do is you could try to
    understand things in terms of this more complex QK which you could also just try
    and understand what the attention patterns are doing empirically so let's look
    at one of these copying onesã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå—¯ã€‚è€Œä¸”æ˜¯çš„ï¼ŒQKç”µè·¯ï¼Œæ‰€ä»¥ä½ å¯ä»¥å°è¯•ä»è¿™ä¸ªæ›´å¤æ‚çš„QKæ¥ç†è§£äº‹ç‰©ï¼Œæˆ–è€…å¯ä»¥å°è¯•å®è¯ä¸Šç†è§£æ³¨æ„åŠ›æ¨¡å¼åœ¨åšä»€ä¹ˆï¼Œæ‰€ä»¥æˆ‘ä»¬æ¥çœ‹å…¶ä¸­ä¸€ä¸ªå¤åˆ¶çš„ä¾‹å­ã€‚
- en: I've given it the first paragraph of Harry Potterï¼Œ and we can just look at where
    it intenseã€‚Andã€‚And something really happenedï¼Œ interesting happensã€‚so almost all
    the time we just attend back to the first tokenã€‚we have this special token at
    the beginning of the sequenceã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç»™äº†å®ƒã€Šå“ˆåˆ©Â·æ³¢ç‰¹ã€‹çš„ç¬¬ä¸€æ®µï¼Œæˆ‘ä»¬å¯ä»¥çœ‹çœ‹å®ƒåœ¨å“ªäº›åœ°æ–¹é›†ä¸­æ³¨æ„åŠ›ã€‚è€Œä¸”ï¼Œç¡®å®å‘ç”Ÿäº†ä¸€äº›æœ‰è¶£çš„äº‹æƒ…ã€‚å‡ ä¹æ‰€æœ‰æ—¶å€™ï¼Œæˆ‘ä»¬åªæ˜¯å›åˆ°ç¬¬ä¸€ä¸ªæ ‡è®°ã€‚æˆ‘ä»¬åœ¨åºåˆ—çš„å¼€å¤´æœ‰è¿™ä¸ªç‰¹æ®Šæ ‡è®°ã€‚
- en: And we usually think of that as just being a null tension operation it's a way
    for it to not do anything in factã€‚if you look the value vector is basically zeroï¼Œ
    it's just not cocking any information from thatã€‚ğŸ˜¡ã€‚Andã€‚But whenever we see repeated
    textï¼Œ something interesting happensï¼Œ so when we get to Mrã€‚ğŸ˜¡ã€‚Tryries to look at
    and it' a little bit weakï¼Œ then we get to Dã€‚And intends to Ersã€‚
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œæˆ‘ä»¬é€šå¸¸è®¤ä¸ºè¿™åªæ˜¯ä¸€ä¸ªç©ºæ³¨æ„åŠ›æ“ä½œï¼Œå®ƒå®é™…ä¸Šæ²¡æœ‰åšä»»ä½•äº‹æƒ…ã€‚å¦‚æœä½ çœ‹è¿™ä¸ªå€¼å‘é‡ï¼ŒåŸºæœ¬ä¸Šæ˜¯é›¶ï¼Œå®ƒæ ¹æœ¬æ²¡æœ‰ä»ä¸­è·å–ä»»ä½•ä¿¡æ¯ã€‚ğŸ˜¡ã€‚è€Œä¸”ï¼Œå½“æˆ‘ä»¬çœ‹åˆ°é‡å¤çš„æ–‡æœ¬æ—¶ï¼Œä¼šå‘ç”Ÿä¸€äº›æœ‰è¶£çš„äº‹æƒ…ï¼Œæ‰€ä»¥å½“æˆ‘ä»¬åˆ°è¾¾Mrã€‚ğŸ˜¡ã€‚Tryriesçœ‹èµ·æ¥æœ‰ç‚¹å¼±ï¼Œç„¶åæˆ‘ä»¬åˆ°è¾¾Dã€‚è€Œä¸”æ‰“ç®—æ˜¯Ersã€‚
- en: That's interestingã€‚And then we get to earthã€‚And it attends to leaveã€‚And so it's
    not attending to the same tokenã€‚It's attending to the same tokenã€‚Shifted one forwardã€‚Wellï¼Œ
    that's really interestingã€‚And there's actually a lot of attention nets that are
    doing thisã€‚So here we have one where now we hit the potter pot and we attended
    Tsã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆæœ‰è¶£ã€‚ç„¶åæˆ‘ä»¬åˆ°è¾¾äº†åœ°çƒã€‚å®ƒå¼€å§‹ç¦»å¼€ã€‚å› æ­¤å®ƒå¹¶ä¸æ˜¯å…³æ³¨ç›¸åŒçš„æ ‡è®°ã€‚å®ƒæ˜¯åœ¨å…³æ³¨ç›¸åŒçš„æ ‡è®°ï¼Œä½†å‘å‰ç§»åŠ¨äº†ä¸€ä½ã€‚å—¯ï¼Œè¿™çœŸæœ‰æ„æ€ã€‚å®é™…ä¸Šï¼Œæœ‰å¾ˆå¤šæ³¨æ„åŠ›ç½‘ç»œåœ¨åšè¿™ä¸ªã€‚æ‰€ä»¥æˆ‘ä»¬è¿™é‡Œæœ‰ä¸€ä¸ªï¼Œå½“æˆ‘ä»¬åˆ°è¾¾é™¶å™¨æ—¶ï¼Œæˆ‘ä»¬å…³æ³¨çš„æ˜¯Tã€‚
- en: Maybe that's the same attention that I don't remember when I was constructing
    this exampleã€‚ğŸ˜Šã€‚And turns out this is a super common thingï¼Œ so you go and you look
    for the previous exampleã€‚you shift one forward and you're like okayï¼Œ well last
    time I saw thisï¼Œ this is what happenedã€‚probably the same thing is going to happenã€‚And
    we can go and look at the effect that the attention head has on the logicsã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸è¿™æ˜¯æˆ‘åœ¨æ„å»ºè¿™ä¸ªä¾‹å­æ—¶çš„åŒä¸€æ³¨æ„åŠ›ã€‚æˆ‘è®°ä¸å¤ªæ¸…äº†ã€‚ğŸ˜Šã€‚ç»“æœå‘ç°è¿™æ˜¯ä¸€ä¸ªè¶…çº§å¸¸è§çš„äº‹æƒ…ï¼Œå› æ­¤ä½ å»æŸ¥çœ‹ä¹‹å‰çš„ä¾‹å­ã€‚ä½ å‘å‰ç§»åŠ¨ä¸€ä½ï¼Œä½ ä¼šè§‰å¾—ï¼Œå¥½å§ï¼Œä¸Šæ¬¡æˆ‘çœ‹åˆ°è¿™ä¸ªï¼Œè¿™å°±æ˜¯å‘ç”Ÿçš„äº‹æƒ…ã€‚å¯èƒ½åŒæ ·çš„äº‹æƒ…å°†ä¼šå‘ç”Ÿã€‚æˆ‘ä»¬å¯ä»¥å»çœ‹çœ‹æ³¨æ„åŠ›å¤´å¯¹é€»è¾‘çš„å½±å“ã€‚
- en: most of the time it's not affecting thingsï¼Œ but in these cases it's able to
    go and predict when it's doing us this thing of going and looking one forward
    it's able to go and predict the next tokenã€‚So we call this an induction headï¼Œ
    an induction head looks for the previous copy looks forward and says a probably
    the same thing that happened last time is going to happen you can think of this
    as being a nearest neighbors it's like an in context nearest neighbor's algorithm
    it's going and searching through your contextã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°æ—¶å€™è¿™å¹¶ä¸å½±å“äº‹æƒ…ï¼Œä½†åœ¨è¿™äº›æƒ…å†µä¸‹å®ƒèƒ½å¤Ÿé¢„æµ‹å½“å®ƒå‘å‰çœ‹æ—¶çš„ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚æ‰€ä»¥æˆ‘ä»¬ç§°ä¹‹ä¸ºå½’çº³å¤´ï¼Œå½’çº³å¤´å¯»æ‰¾å‰ä¸€ä¸ªå‰¯æœ¬ï¼Œå‘å‰çœ‹ï¼Œå¹¶è¯´å¯èƒ½ä¸Šæ¬¡å‘ç”Ÿçš„äº‹æƒ…å°†ä¼šå†æ¬¡å‘ç”Ÿã€‚ä½ å¯ä»¥å°†å…¶è§†ä¸ºæœ€è¿‘é‚»ï¼Œç±»ä¼¼äºä¸Šä¸‹æ–‡ä¸­çš„æœ€è¿‘é‚»ç®—æ³•ï¼Œå®ƒåœ¨ä¸Šä¸‹æ–‡ä¸­è¿›è¡Œæœç´¢ã€‚
- en: finding similar things and then predicting that's what's going to happen nextã€‚The
    way that these actually work isï¼Œ I meanï¼Œ there's actually two waysã€‚but in a model
    that uses rotary attention or something like thisï¼Œ you only have oneã€‚Andã€‚You shift
    your keyï¼Œ first you have an earlier retention hand shifts your key for one so
    you take the value of the previous token and you embeddedbed it in your present
    tokenã€‚
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰¾åˆ°ç›¸ä¼¼çš„ä¸œè¥¿ï¼Œç„¶åé¢„æµ‹æ¥ä¸‹æ¥ä¼šå‘ç”Ÿä»€ä¹ˆã€‚å®ƒä»¬çš„å·¥ä½œæ–¹å¼ï¼Œå®é™…ä¸Šï¼Œæœ‰ä¸¤ç§æ–¹å¼ã€‚ä½†åœ¨ä½¿ç”¨æ—‹è½¬æ³¨æ„åŠ›æˆ–ç±»ä¼¼æ¨¡å‹ä¸­ï¼Œä½ åªæœ‰ä¸€ç§æ–¹å¼ã€‚ä½ é¦–å…ˆç§»åŠ¨ä½ çš„é”®ï¼Œæ—©æœŸçš„ä¿æŒå¤´å°†ä½ çš„é”®å‘å‰ç§»åŠ¨ä¸€ä½ï¼Œå› æ­¤ä½ å°†å‰ä¸€ä¸ªæ ‡è®°çš„å€¼åµŒå…¥åˆ°å½“å‰æ ‡è®°ä¸­ã€‚
- en: And then you have your query in your keyï¼Œ go and look atï¼Œ yeahã€‚try to go and
    match so you look for the same thingã€‚ğŸ˜¡ã€‚And then you go and you predict that whatever
    you saw is going to be the next token so that's the high level algorithm sometimes
    you can do clever things where actually it'll care about multiple earlier tokens
    and it'll look for like short phrases and so on so induction heads can really
    vary in how much of the previous context they care about or what aspects of the
    previous context they care about but this general trick of looking for the same
    thing shift forward predict that is what induction heads doã€‚
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ æœ‰ä½ çš„æŸ¥è¯¢åœ¨ä½ çš„é”®ä¸­ï¼Œå»æŸ¥çœ‹ï¼Œæ²¡é”™ã€‚è¯•ç€å»åŒ¹é…ï¼Œæ‰€ä»¥ä½ å¯»æ‰¾ç›¸åŒçš„ä¸œè¥¿ã€‚ğŸ˜¡ã€‚ç„¶åä½ å»é¢„æµ‹ä½ çœ‹åˆ°çš„ä¸œè¥¿å°†æ˜¯ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œæ‰€ä»¥è¿™æ˜¯é«˜å±‚æ¬¡çš„ç®—æ³•ï¼Œæœ‰æ—¶ä½ å¯ä»¥åšä¸€äº›èªæ˜çš„äº‹æƒ…ï¼Œå®é™…ä¸Šå®ƒä¼šå…³æ³¨å¤šä¸ªæ—©æœŸçš„æ ‡è®°ï¼Œå¹¶ä¸”ä¼šå¯»æ‰¾çŸ­è¯­ç­‰ï¼Œæ‰€ä»¥å½’çº³å¤´åœ¨å®ƒå…³æ³¨çš„å…ˆå‰ä¸Šä¸‹æ–‡çš„ç¨‹åº¦æˆ–æ–¹é¢ä¸Šå¯èƒ½ä¼šæœ‰å¾ˆå¤§å˜åŒ–ï¼Œä½†è¿™ä¸ªå¯»æ‰¾ç›¸åŒçš„ä¸œè¥¿å‘å‰ç§»åŠ¨é¢„æµ‹çš„é€šç”¨æŠ€å·§å°±æ˜¯å½’çº³å¤´æ‰€åšçš„ã€‚
- en: Lots of examples of thisã€‚And the cool thing is you can now you can use the QK
    eigenvalues to characterize thisã€‚you can sayï¼Œ wellï¼Œ you know we're looking for
    the same thing shifted by one but looking for the same thing if you expand through
    the attention notes's in the right way that'll work out and we're copying and
    so an induction head is one which has both positive OV eigenvalues and also positive
    QK eigenvaluesã€‚
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å¾ˆå¤šè¿™æ ·çš„ä¾‹å­ã€‚å¾ˆé…·çš„æ˜¯ï¼Œä½ ç°åœ¨å¯ä»¥ä½¿ç”¨QKç‰¹å¾å€¼æ¥è¡¨å¾è¿™ä¸ªã€‚ä½ å¯ä»¥è¯´ï¼Œå¥½å§ï¼Œæˆ‘ä»¬åœ¨å¯»æ‰¾å‘å‰ç§»åŠ¨ä¸€ä¸ªçš„ç›¸åŒä¸œè¥¿ï¼Œä½†å¦‚æœä½ ä»¥æ­£ç¡®çš„æ–¹å¼æ‰©å±•æ³¨æ„åŠ›ç½‘ç»œï¼Œè¿™ä¼šå¥æ•ˆï¼Œæˆ‘ä»¬åœ¨å¤åˆ¶ï¼Œæ‰€ä»¥å½’çº³å¤´æ˜¯ä¸€ä¸ªå…·æœ‰æ­£OVç‰¹å¾å€¼å’Œæ­£QKç‰¹å¾å€¼çš„å¤´ã€‚
- en: And so you can just put that on a plot and you have your induction heads in
    the corner to OV eigenvaluesã€‚your QK eigenvaluesï¼Œ and I think actually OV is this
    axisqK is this one axis doesn't matter and in the corner you have your eenvalues
    or your induction headsã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯ä»¥æŠŠå®ƒæ”¾åœ¨å›¾ä¸Šï¼Œå·¦ä¸‹è§’æ˜¯ä½ çš„å½’çº³å¤´å¯¹åº”çš„OVç‰¹å¾å€¼å’ŒQKç‰¹å¾å€¼ï¼Œæˆ‘è®¤ä¸ºå…¶å®OVæ˜¯è¿™ä¸ªè½´ï¼ŒQKæ˜¯å¦ä¸€ä¸ªè½´ï¼Œæ— æ‰€è°“ï¼Œåœ¨è§’è½é‡Œæ˜¯ä½ çš„ç‰¹å¾å€¼æˆ–ä½ çš„å½’çº³å¤´ã€‚
- en: ğŸ˜Šï¼ŒYeahï¼Œ and so this seems to be well okay we now have an actual hypothesisã€‚the
    hypothesis is the way that that phase change we're seeing the phase change is
    the discovery of these induction hits that would be the hypothesis and these are
    way more effective than regular you know than this first algorithm we had which
    was just sort of blindly copy things wherever it could be plausible now we can
    go and like actually recognize patterns and look at what happened and predict
    that similar things are going to happen again that's a way better algorithmã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ˜¯çš„ï¼Œæ‰€ä»¥è¿™ä¼¼ä¹æ˜¯å¥½çš„ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªå®é™…çš„å‡è®¾ã€‚è¿™ä¸ªå‡è®¾æ˜¯ï¼Œæˆ‘ä»¬æ‰€çœ‹åˆ°çš„ç›¸å˜æ˜¯è¿™äº›å½’çº³æç¤ºçš„å‘ç°ï¼Œè¿™å°†æ˜¯æˆ‘ä»¬çš„å‡è®¾ï¼Œè€Œè¿™äº›è¦æ¯”æˆ‘ä»¬æœ€åˆçš„ç®—æ³•æœ‰æ•ˆå¾—å¤šï¼Œé‚£ä¸ªç®—æ³•åªæ˜¯ç›²ç›®åœ°å¤åˆ¶ä»»ä½•å¯èƒ½åˆç†çš„å†…å®¹ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥çœŸæ­£è¯†åˆ«æ¨¡å¼ï¼Œè§‚å¯Ÿå‘ç”Ÿäº†ä»€ä¹ˆï¼Œå¹¶é¢„æµ‹ç±»ä¼¼çš„äº‹æƒ…å°†å†æ¬¡å‘ç”Ÿï¼Œè¿™æ˜¯ä¸€ç§æ›´å¥½çš„ç®—æ³•ã€‚
- en: å—¯ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_33.png)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_33.png)
- en: Yeah so there's other attention hints that are doing more local things I'm going
    to go and skip over that and return to our mystery because I am running out of
    time I have five more minutes okay so what what is going along with this in context
    learning well now now we've hypothesis let's check it so we think it might be
    induction hintsã€‚
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿˜æœ‰å…¶ä»–æ³¨æ„åŠ›æç¤ºæ­£åœ¨è¿›è¡Œæ›´å±€éƒ¨çš„äº‹æƒ…ï¼Œæˆ‘å°†è·³è¿‡è¿™ä¸€éƒ¨åˆ†ï¼Œå›åˆ°æˆ‘ä»¬çš„è°œé¢˜ï¼Œå› ä¸ºæˆ‘å¿«æ²¡æ—¶é—´äº†ï¼Œæˆ‘è¿˜æœ‰äº”åˆ†é’Ÿã€‚å¥½å§ï¼Œè¿™ä¸ä¸Šä¸‹æ–‡å­¦ä¹ æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿç°åœ¨æˆ‘ä»¬æœ‰äº†å‡è®¾ï¼Œæ¥æ£€éªŒä¸€ä¸‹ï¼Œæˆ‘ä»¬è®¤ä¸ºå¯èƒ½æ˜¯å½’çº³æç¤ºã€‚
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_35.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_35.png)'
- en: å—¯mã€‚And there's a few reasons we believe usã€‚So one thing is going to be that
    induction headsã€‚Wellã€‚okayï¼Œ I'll just go over to the endã€‚So one thing you can do
    is you can just ablate the attention endã€‚And it turns itï¼Œ you can color here we
    have attention heads colored by how much they are an induction headã€‚And this is
    the start of the bumpï¼Œ this is the end of the bump hereã€‚
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯mã€‚æˆ‘ä»¬ç›¸ä¿¡æœ‰å‡ ä¸ªåŸå› ã€‚é¦–å…ˆæ˜¯å½’çº³å¤´ã€‚å¥½å§ï¼Œæˆ‘ä¼šç›´æ¥è¯´åˆ°æœ€åã€‚æ‰€ä»¥ä½ å¯ä»¥åšçš„ä¸€ä»¶äº‹æ˜¯æ¶ˆèæ³¨æ„åŠ›å¤´ã€‚è¿™æ ·ï¼Œä½ å¯ä»¥ç»™è¿™é‡Œçš„æ³¨æ„åŠ›å¤´ä¸Šè‰²ï¼Œè¡¨ç¤ºå®ƒä»¬ä½œä¸ºå½’çº³å¤´çš„ç¨‹åº¦ã€‚è¿™æ˜¯éš†èµ·çš„å¼€å§‹ï¼Œè¿™é‡Œæ˜¯éš†èµ·çš„ç»“æŸã€‚
- en: and we can see that they first of all induction heads are forming like previously
    we didn't have induction heads here now they're just starting to form here and
    then we have really intense induction heads here and hereã€‚And the attention heads
    where you obblate themï¼Œ you get aã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œé¦–å…ˆï¼Œå½’çº³å¤´æ­£åœ¨å½¢æˆï¼Œä¹‹å‰è¿™é‡Œæ²¡æœ‰å½’çº³å¤´ï¼Œç°åœ¨å®ƒä»¬åˆšå¼€å§‹åœ¨è¿™é‡Œå½¢æˆï¼Œç„¶åè¿™é‡Œå’Œè¿™é‡Œæœ‰éå¸¸å¼ºçƒˆçš„å½’çº³å¤´ã€‚å¯¹äºæ³¨æ„åŠ›å¤´ï¼Œå½“ä½ å¯¹å®ƒä»¬è¿›è¡Œæ¶ˆèæ—¶ï¼Œä½ ä¼šå¾—åˆ°ä¸€ä¸ªã€‚
- en: You get a loss or so we're lucky not at loss this meta learning scoreã€‚the difference
    between or an in context learning scoreã€‚the difference between the 500th token
    and the 50th tokenã€‚And that's all explained by induction handsã€‚Now we actually
    have one induction head that doesn't contribute to it actually it does the opposite
    so that's kind of interesting maybe it's doing something shorter shorter distance
    and there's also this interesting thing where like they all rush to be induction
    heads and then they discover only only a few win out in the end so there's some
    interesting dynamics going on there but it really seems like in these small modelsã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šå¾—åˆ°ä¸€ä¸ªæŸå¤±ï¼Œæˆ–è€…è¯´ï¼Œæˆ‘ä»¬å¹¸è¿çš„æ˜¯æ²¡æœ‰æŸå¤±ï¼Œè¿™ä¸ªå…ƒå­¦ä¹ åˆ†æ•°ã€‚ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†æ•°çš„å·®å¼‚ã€‚ç¬¬500ä¸ªæ ‡è®°å’Œç¬¬50ä¸ªæ ‡è®°ä¹‹é—´çš„å·®å¼‚ã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥ç”¨å½’çº³å¤´æ¥è§£é‡Šã€‚ç°åœ¨æˆ‘ä»¬å®é™…ä¸Šæœ‰ä¸€ä¸ªå½’çº³å¤´å¹¶æ²¡æœ‰è´¡çŒ®ï¼Œç›¸åï¼Œå®ƒçš„ä½œç”¨æ­£å¥½ç›¸åï¼Œè¿™å¾ˆæœ‰è¶£ï¼Œä¹Ÿè®¸å®ƒæ­£åœ¨åšä¸€äº›è¾ƒçŸ­è·ç¦»çš„äº‹æƒ…ï¼Œè¿˜æœ‰ä¸€ä¸ªæœ‰è¶£çš„ç°è±¡ï¼Œå°±æ˜¯å®ƒä»¬éƒ½äº‰ç€æˆä¸ºå½’çº³å¤´ï¼Œæœ€ç»ˆåªæœ‰å°‘æ•°å‡ ä¸ªèƒœå‡ºï¼Œæ‰€ä»¥è¿™é‡Œæœ‰ä¸€äº›æœ‰è¶£çš„åŠ¨æ€ï¼Œä½†åœ¨è¿™äº›å°æ¨¡å‹ä¸­ï¼Œæƒ…å†µä¼¼ä¹çœŸçš„æ˜¯è¿™æ ·ã€‚
- en: ğŸ˜Šï¼ŒAll within context learning is explained by these induction notessã€‚ğŸ˜¡ï¼ŒOkayã€‚What
    about large modelsã€‚Wellï¼Œ in large modelsï¼Œ that's going to be harder to go and
    ask thisã€‚But one thing you can do is you can askï¼Œ okayï¼Œ you knowï¼Œ we can look
    at ourï¼Œ our inductionã€‚our in context learning score over timeã€‚ They get this sharp
    phase changeã€‚ Oh lookã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä¸Šä¸‹æ–‡å­¦ä¹ éƒ½æ˜¯ç”±è¿™äº›å½’çº³æç¤ºè§£é‡Šçš„ã€‚ğŸ˜¡ï¼Œå¥½å§ã€‚å¤§æ¨¡å‹å‘¢ï¼Ÿåœ¨å¤§æ¨¡å‹ä¸­ï¼Œè¿™ä¼šæ›´éš¾å»é—®ã€‚ä½†ä½ å¯ä»¥åšçš„ä¸€ä»¶äº‹æ˜¯ï¼Œä½ çŸ¥é“ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹æˆ‘ä»¬çš„å½’çº³ã€æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡å­¦ä¹ åˆ†æ•°éšæ—¶é—´å˜åŒ–çš„æƒ…å†µã€‚å®ƒä»¬ä¼šå‡ºç°è¿™ä¸ªæ˜æ˜¾çš„ç›¸å˜ã€‚å“¦ï¼Œçœ‹ã€‚
- en: Induction heads form at exactly the same point in timeã€‚So that's only correlational
    evidenceã€‚but it's pretty suggestive correlational evidenceï¼Œ even especially given
    that we have an obviousã€‚you know like the obvious effect that induction heads
    should have is is this I guess it could be that there's other mechanisms being
    discovered at the same time in large modelsã€‚but it has to be in a very small windowã€‚ğŸ˜Šï¼ŒSoã€‚ğŸ˜Šï¼ŒReally
    suggests that thing that's driving that change is in context learningã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: å¼•å¯¼å¤´åœ¨å®Œå…¨ç›¸åŒçš„æ—¶é—´ç‚¹å½¢æˆã€‚æ‰€ä»¥è¿™ä»…ä»…æ˜¯ç›¸å…³è¯æ®ï¼Œä½†è¿™æ˜¯ä¸€ç§ç›¸å½“æœ‰æç¤ºæ€§çš„ç›¸å…³è¯æ®ï¼Œå°¤å…¶æ˜¯è€ƒè™‘åˆ°æˆ‘ä»¬æœ‰ä¸€ä¸ªæ˜æ˜¾çš„ï¼Œä½ çŸ¥é“ï¼Œå¼•å¯¼å¤´åº”è¯¥å…·æœ‰çš„æ˜¾è‘—æ•ˆæœï¼Œæˆ‘æƒ³å¯èƒ½æ˜¯åœ¨å¤§å‹æ¨¡å‹ä¸­åŒæ—¶å‘ç°å…¶ä»–æœºåˆ¶ï¼Œä½†è¿™å¿…é¡»åœ¨ä¸€ä¸ªéå¸¸å°çš„çª—å£å†…ã€‚ğŸ˜Šï¼Œæ‰€ä»¥ã€‚ğŸ˜Šï¼Œè¿™çœŸçš„è¡¨æ˜é©±åŠ¨è¿™ä¸€å˜åŒ–çš„ä¸œè¥¿æ˜¯åœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­ã€‚
- en: Okayï¼Œ soã€‚Obviouslyï¼Œ induction heads can go and copy textã€‚ğŸ˜¡ã€‚But a question you
    might ask is you know can they can they do translation like there's all these
    amazing things that models can do that it's not obvious you know in context learning
    or this sort of copying mechanism could do so I just want to very quicklyã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥ã€‚æ˜¾ç„¶ï¼Œå¼•å¯¼å¤´å¯ä»¥å»å¤åˆ¶æ–‡æœ¬ã€‚ğŸ˜¡ã€‚ä½†ä½ å¯èƒ½ä¼šé—®ï¼Œæ˜¯å¦èƒ½è¿›è¡Œç¿»è¯‘ï¼Œå› ä¸ºæ¨¡å‹å¯ä»¥åšè®¸å¤šä»¤äººæƒŠå¹çš„äº‹æƒ…ï¼Œè€Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ æˆ–è¿™ç§å¤åˆ¶æœºåˆ¶ä¸­å¹¶ä¸æ˜æ˜¾ï¼Œæ‰€ä»¥æˆ‘åªæ˜¯æƒ³å¾ˆå¿«åœ°ã€‚
- en: Look at a few fun examplesã€‚So here we have an attention patternã€‚Yeahã€‚I guess
    I need to open lexoscopeã€‚Let me try doing that againã€‚Sorryã€‚I should have thought
    this through a bit more before this talkã€‚å—¯ã€‚ğŸ˜Šï¼ŒChrisã€‚could you zoom in a littleï¼Œ
    pleaseï¼Œ Yeahï¼Œ yeahï¼Œ thank youã€‚And soã€‚Okayï¼Œ I'm notã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹å‡ ä¸ªæœ‰è¶£çš„ä¾‹å­ã€‚æ‰€ä»¥è¿™é‡Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ³¨æ„åŠ›æ¨¡å¼ã€‚æ˜¯çš„ã€‚æˆ‘æƒ³æˆ‘éœ€è¦æ‰“å¼€lexoscopeã€‚è®©æˆ‘å†è¯•ä¸€æ¬¡ã€‚æŠ±æ­‰ï¼Œæˆ‘åœ¨è¿™æ¬¡æ¼”è®²å‰åº”è¯¥è€ƒè™‘å¾—æ›´å‘¨å…¨ã€‚å—¯ã€‚ğŸ˜Šï¼Œå…‹é‡Œæ–¯ï¼Œä½ èƒ½ç¨å¾®æ”¾å¤§ä¸€ç‚¹å—ï¼Ÿæ˜¯çš„ï¼Œæ„Ÿè°¢ä½ ã€‚é‚£ä¹ˆã€‚å¥½çš„ï¼Œæˆ‘ä¸æ˜¯ã€‚
- en: my French isn't that greatï¼Œ but my name is Christopherï¼Œ I'm from Canadaã€‚What
    we can do here is we can look at where this attention attends as we go and we
    do this and it'll become especially clear on the second sentenceã€‚so here we're
    on the periodã€‚And we tend to showã€‚Now we're on andjos is I in Frenchã€‚ Okayã€‚now
    we're on the eye and we attend to Swedenã€‚Now we're on the am and we a trend to
    do which is from and then from to Canadaã€‚
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ³•è¯­ä¸æ˜¯å¾ˆå¥½ï¼Œä½†æˆ‘å«**å…‹é‡Œæ–¯æ‰˜å¼—**ï¼Œæ¥è‡ªåŠ æ‹¿å¤§ã€‚æˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œæ¢è®¨ä¸€ä¸‹æ³¨æ„åŠ›å¦‚ä½•éšç€æ—¶é—´æµåŠ¨è€Œé›†ä¸­ï¼Œè¿™åœ¨ç¬¬äºŒä¸ªå¥å­ä¸­ä¼šå˜å¾—ç‰¹åˆ«æ˜æ˜¾ã€‚ç°åœ¨æˆ‘ä»¬åœ¨å¥å·ä¸Šã€‚æˆ‘ä»¬å€¾å‘äºå±•ç¤ºã€‚ç°åœ¨æˆ‘ä»¬åœ¨â€œanjosâ€ä¸Šï¼Œæˆ‘ç”¨æ³•è¯­è¯´â€œæˆ‘â€ã€‚å¥½çš„ã€‚ç°åœ¨æˆ‘ä»¬åœ¨â€œeyeâ€ä¸Šï¼Œæˆ‘ä»¬å…³æ³¨ç‘å…¸ã€‚ç°åœ¨æˆ‘ä»¬åœ¨â€œamâ€ä¸Šï¼Œæˆ‘ä»¬çš„è¶‹åŠ¿æ˜¯ä»è¿™é‡Œåˆ°åŠ æ‹¿å¤§ã€‚
- en: and so we're doing a cross lingual induction headï¼Œ which we can use for translationã€‚And
    indeedã€‚if you look at examplesï¼Œ this is where this seems to be a major driving
    force in the model's ability to go and correctly do translationã€‚Another fun example
    isã€‚I think maybe maybe the most impressive thing about in context learning to
    me has been the model's ability to go and learn arbitrary functions like you mean
    just show the model function and can start mimicking that function well okayã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬æ­£åœ¨åšä¸€ä¸ªè·¨è¯­è¨€å¼•å¯¼å¤´ï¼Œè¿™å¯ä»¥ç”¨äºç¿»è¯‘ã€‚ç¡®å®ï¼Œå¦‚æœä½ çœ‹ä¸€äº›ä¾‹å­ï¼Œè¿™ä¼¼ä¹æ˜¯æ¨¡å‹èƒ½å¤Ÿæ­£ç¡®è¿›è¡Œç¿»è¯‘çš„ä¸»è¦é©±åŠ¨åŠ›ã€‚å¦ä¸€ä¸ªæœ‰è¶£çš„ä¾‹å­æ˜¯ï¼Œæˆ‘è®¤ä¸ºå¯¹æˆ‘æ¥è¯´ï¼Œåœ¨ä¸Šä¸‹æ–‡å­¦ä¹ ä¸­æœ€ä»¤äººå°è±¡æ·±åˆ»çš„äº‹æƒ…æ˜¯æ¨¡å‹èƒ½å¤Ÿå­¦ä¹ ä»»æ„å‡½æ•°ï¼Œåªéœ€å±•ç¤ºç»™æ¨¡å‹ä¸€ä¸ªå‡½æ•°ï¼Œå®ƒå°±èƒ½å¼€å§‹æ¨¡ä»¿é‚£ä¸ªå‡½æ•°ï¼Œå¥½çš„ã€‚
- en: I I have a question Yes yeah so do these induction head only do kind of a look
    ahead copy or like can they also do some sort of like a complex structure recognitionã€‚Yeah
    yeah so they can both use a larger context previous context and they can copy
    more abstract thingsã€‚
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æœ‰ä¸€ä¸ªé—®é¢˜ã€‚æ˜¯çš„ï¼Œè¿™äº›å¼•å¯¼å¤´æ˜¯å¦ä»…ä»…è¿›è¡Œå‰ç»æ€§å¤åˆ¶ï¼Œæˆ–è€…å®ƒä»¬èƒ½å¦ä¹Ÿè¿›è¡ŒæŸç§å¤æ‚ç»“æ„è¯†åˆ«ï¼Ÿæ˜¯çš„ï¼Œå®ƒä»¬æ—¢å¯ä»¥ä½¿ç”¨æ›´å¤§çš„ä¸Šä¸‹æ–‡å’Œä»¥å‰çš„ä¸Šä¸‹æ–‡ï¼Œä¹Ÿå¯ä»¥å¤åˆ¶æ›´æŠ½è±¡çš„ä¸œè¥¿ã€‚
- en: so like the translation one is showing you that they can copy rather than the
    literal token a translated version it's what I call soft induction head and yeah
    you you can have them copy similar words you can have them look at longer context
    you can look for a more structural things the way that we usually characterize
    them is whether in large models just whether they empirically behave like an induction
    head so the definition gets a little bit blurry when you try to encompass these
    more sort of a blurry boundary but yeah seem to be a lot of attention heads that
    are doing sort more and more abstract versions and yeah my favorite version is
    this one that I'm about to show you which is used let's isolate a single one of
    these which can do pattern recognition so it can learn functions in the context
    and learn how to do it I've just made up a nonsense function here we're going
    encode one binary variable with the choice of whether to do a color or aã€‚
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åƒç¿»è¯‘çš„é‚£ä¸ªæ˜¾ç¤ºç»™ä½ ä»¬çš„æ˜¯å®ƒä»¬å¯ä»¥å¤åˆ¶ï¼Œè€Œä¸æ˜¯å­—é¢ä¸Šçš„ç¿»è¯‘ç‰ˆæœ¬ï¼Œæˆ‘ç§°ä¹‹ä¸ºè½¯å½’çº³å¤´ã€‚æ˜¯çš„ï¼Œä½ å¯ä»¥è®©å®ƒä»¬å¤åˆ¶ç›¸ä¼¼çš„è¯ï¼Œä½ å¯ä»¥è®©å®ƒä»¬æŸ¥çœ‹æ›´é•¿çš„ä¸Šä¸‹æ–‡ï¼Œä½ å¯ä»¥å¯»æ‰¾æ›´ç»“æ„åŒ–çš„ä¸œè¥¿ã€‚æˆ‘ä»¬é€šå¸¸å°†å®ƒä»¬è¡¨å¾ä¸ºæ˜¯å¦åœ¨å¤§å‹æ¨¡å‹ä¸­ç»éªŒæ€§åœ°è¡¨ç°å¾—åƒå½’çº³å¤´ï¼Œå› æ­¤å½“ä½ è¯•å›¾åŒ…å«è¿™äº›æ›´æ¨¡ç³Šçš„è¾¹ç•Œæ—¶ï¼Œå®šä¹‰å°±ä¼šå˜å¾—æœ‰ç‚¹æ¨¡ç³Šï¼Œä½†ä¼¼ä¹æœ‰å¾ˆå¤šæ³¨æ„åŠ›å¤´åœ¨åšæ›´æŠ½è±¡çš„ç‰ˆæœ¬ã€‚æˆ‘æœ€å–œæ¬¢çš„ç‰ˆæœ¬æ˜¯æˆ‘å³å°†å±•ç¤ºçš„è¿™ä¸ªï¼Œå®ƒå¯ä»¥åšæ¨¡å¼è¯†åˆ«ï¼Œæ‰€ä»¥å®ƒå¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ åŠŸèƒ½ã€‚æˆ‘è¿™é‡Œç¼–é€ äº†ä¸€ä¸ªæ— æ„ä¹‰çš„å‡½æ•°ï¼Œæˆ‘ä»¬å°†ç¼–ç ä¸€ä¸ªäºŒå…ƒå˜é‡ï¼Œé€‰æ‹©æ˜¯å¦åšé¢œè‰²æˆ–ä¸€ä¸ªã€‚
- en: That was the first wordï¼ŸThenã€‚We're going to say we have green or June hereã€‚Let's
    zoom in moreã€‚So we have color or month and animal or fruit and then we have to
    map it either true or false so that's our goal and it's going to be an exO we
    have a binary variable represented in this way we do an exOR I'mã€‚
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¬¬ä¸€ä¸ªè¯æ˜¯ä»€ä¹ˆï¼Ÿç„¶åã€‚æˆ‘ä»¬è¦è¯´è¿™é‡Œæœ‰ç»¿è‰²æˆ–å…­æœˆä»½ã€‚è®©æˆ‘ä»¬æ›´æ·±å…¥ä¸€ç‚¹ã€‚æ‰€ä»¥æˆ‘ä»¬æœ‰é¢œè‰²æˆ–æœˆä»½å’ŒåŠ¨ç‰©æˆ–æ°´æœï¼Œç„¶åæˆ‘ä»¬å¿…é¡»å°†å…¶æ˜ å°„ä¸ºçœŸæˆ–å‡ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çš„ç›®æ ‡ï¼Œæœ€ç»ˆä¼šæ˜¯ä¸€ä¸ªå¼‚æˆ–ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªä»¥è¿™ç§æ–¹å¼è¡¨ç¤ºçš„äºŒå…ƒå˜é‡ã€‚
- en: Pretty confident this was never in the training set because I just made it up
    and it seems like a nonsense problemã€‚Okayï¼Œ so then when we can go and ask you
    know can the model go and push that well it can and it uses induction heads to
    do it and what we can do is we can look at the so we look at a colon where it's
    going to go and try and predict the next word and for instance hereã€‚
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘éå¸¸æœ‰ä¿¡å¿ƒè¿™ä»æœªå‡ºç°åœ¨è®­ç»ƒé›†ä¸­ï¼Œå› ä¸ºæˆ‘åˆšç¼–é€ å‡ºæ¥ï¼Œå®ƒçœ‹èµ·æ¥åƒæ˜¯ä¸ªæ— æ„ä¹‰çš„é—®é¢˜ã€‚å¥½çš„ï¼Œé‚£ä¹ˆæˆ‘ä»¬å¯ä»¥é—®ï¼Œæ¨¡å‹èƒ½å¦å»æ¨åŠ¨è¿™ä¸ªï¼Œå®ƒå¯ä»¥ï¼Œå®ƒä½¿ç”¨å½’çº³å¤´æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚æˆ‘ä»¬å¯ä»¥çœ‹çœ‹ï¼Œæ‰€ä»¥æˆ‘ä»¬æŸ¥çœ‹ä¸€ä¸ªå†’å·ï¼Œå®ƒè¦å»é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œä¾‹å¦‚åœ¨è¿™é‡Œã€‚
- en: And we have April dogï¼Œ so it's a month and then an animalï¼Œ and it should be
    trueã€‚And what it does is it looks for a previous cases where there was an animal
    a month and then an animalã€‚especially one where the month is the same and goes
    and looks and says that it's trueã€‚And so a model can go and learnï¼Œ learn a functionï¼Œ
    a completely arbitrary functionã€‚
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰å››æœˆçš„ç‹—ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªæœˆä»½å’ŒåŠ¨ç‰©ï¼Œå®ƒåº”è¯¥æ˜¯çœŸçš„ã€‚å®ƒä¼šå¯»æ‰¾ä¹‹å‰çš„æ¡ˆä¾‹ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªåŠ¨ç‰©ã€ä¸€ä¸ªæœˆä»½ï¼Œç„¶åæ˜¯ä¸€ä¸ªåŠ¨ç‰©ï¼Œç‰¹åˆ«æ˜¯å½“æœˆä»½ç›¸åŒæ—¶ï¼Œå»çœ‹çœ‹å¹¶è¯´è¿™æ˜¯æ­£ç¡®çš„ã€‚å› æ­¤ï¼Œä¸€ä¸ªæ¨¡å‹å¯ä»¥å»å­¦ä¹ ï¼Œå­¦ä¹ ä¸€ä¸ªå®Œå…¨ä»»æ„çš„åŠŸèƒ½ã€‚
- en: By going and doing this kind of pattern recognition induction hitã€‚And so this
    to me made it a lot more plausibleï¼Œ but these models actuallyã€‚Can youã€‚Can do in
    context learning like the generality of all these amazing things we see these
    large language models do can be explained by inductionists we don't know that
    it could be that there's other things going on it's very possible that there's
    lots of other things going on but it seems a lot more plausible to me than it
    did when when we startedã€‚
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡è¿›è¡Œè¿™ç§æ¨¡å¼è¯†åˆ«å½’çº³å¤´ã€‚å› æ­¤ï¼Œå¯¹æˆ‘æ¥è¯´ï¼Œè¿™ä½¿å¾—è¿™äº›æ¨¡å‹æ›´å¯ä¿¡ï¼Œä½†è¿™äº›æ¨¡å‹å®é™…ä¸Šèƒ½å¦åšåˆ°ï¼Ÿå®ƒä»¬å¯ä»¥åœ¨ä¸Šä¸‹æ–‡ä¸­å­¦ä¹ ï¼Œæ‰€æœ‰è¿™äº›æƒŠäººçš„äº‹æƒ…çš„æ™®éæ€§ï¼Œæˆ‘ä»¬çœ‹åˆ°è¿™äº›å¤§å‹è¯­è¨€æ¨¡å‹æ‰€åšçš„å¯ä»¥é€šè¿‡å½’çº³ä¸»ä¹‰æ¥è§£é‡Šã€‚æˆ‘ä»¬ä¸çŸ¥é“ï¼Œå¯èƒ½è¿˜æœ‰å…¶ä»–äº‹æƒ…åœ¨å‘ç”Ÿï¼Œä½†è¿™åœ¨æˆ‘çœ‹æ¥ä¼¼ä¹æ¯”æˆ‘ä»¬å¼€å§‹æ—¶æ›´å¯ä¿¡ã€‚
- en: ğŸ˜Šï¼ŒI'm conscious that I am actually over timeï¼Œ I mean just quickly go through
    these last few slidesã€‚yesï¼Œ so I think thinking of this as like an in contextt
    in your s neighborpers I think is a really useful way to think about thisã€‚ğŸ˜Šï¼ŒOther
    things could absolutely be contributingã€‚This might explain why transformers do
    in context learning over long contexts better than LSTMs and LSTM can't do this
    because it's not linear on the amount of compute it needs it's like quadratic
    or N log n if it was really clever so transformers are LSTM's impossible to do
    this transformers do do this and actually they diverge at the same point but if
    you look wellã€‚
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæˆ‘æ„è¯†åˆ°æˆ‘å®é™…ä¸Šè¶…æ—¶äº†ï¼Œå¿«ç‚¹è¿‡ä¸€ä¸‹è¿™æœ€åå‡ å¼ å¹»ç¯ç‰‡ã€‚æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºæŠŠè¿™ä¸ªè§†ä¸ºä¸Šä¸‹æ–‡ä¸­çš„é‚»å±…æ€ç»´æ˜¯ä¸€ä¸ªéå¸¸æœ‰ç”¨çš„æ€è€ƒæ–¹å¼ã€‚ğŸ˜Šï¼Œå…¶ä»–äº‹æƒ…ç»å¯¹å¯èƒ½åœ¨è´¡çŒ®ã€‚è¿™å¯èƒ½è§£é‡Šäº†ä¸ºä»€ä¹ˆå˜æ¢å™¨åœ¨é•¿ä¸Šä¸‹æ–‡ä¸­çš„ä¸Šä¸‹æ–‡å­¦ä¹ è¡¨ç°å¾—æ¯”LSTMå¥½ï¼Œè€ŒLSTMæ— æ³•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå› ä¸ºå®ƒå¯¹æ‰€éœ€è®¡ç®—é‡ä¸æ˜¯çº¿æ€§çš„ï¼Œè€Œæ˜¯å¹³æ–¹æˆ–N
    log nï¼Œå¦‚æœå®ƒçœŸçš„å¾ˆèªæ˜çš„è¯ã€‚æ‰€ä»¥å˜æ¢å™¨èƒ½åšåˆ°è¿™ä¸€ç‚¹ï¼Œè€ŒLSTMåˆ™æ— æ³•åšåˆ°è¿™ä¸€ç‚¹ï¼Œå˜æ¢å™¨åœ¨åŒä¸€ç‚¹ä¸Šä¼šåˆ†æ­§ï¼Œä½†å¦‚æœä½ ä»”ç»†è§‚å¯Ÿã€‚
- en: I can go into this in Mar Hill after if people wantã€‚There's a really nice paper
    by Marcus Hutter explaining trying to predict and explain why we observe scaling
    laws and modelsã€‚it's worth noting that the arguments in this paper go exactly
    through to this exampleï¼Œ this theoryã€‚in factï¼Œ they sort of work better for the
    case of thinking about this in contextex learning with essentially in nearest
    neighbor's algorithm than they do in the regular caseã€‚
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¯ä»¥åœ¨Mar Hillæ·±å…¥è®¨è®ºè¿™ä¸ªï¼Œå¦‚æœäººä»¬æƒ³è¦çš„è¯ã€‚Marcus Hutteræœ‰ä¸€ç¯‡éå¸¸å¥½çš„è®ºæ–‡ï¼Œè§£é‡Šäº†æˆ‘ä»¬ä¸ºä»€ä¹ˆè§‚å¯Ÿåˆ°ç¼©æ”¾å®šå¾‹å’Œæ¨¡å‹ï¼Œè¯•å›¾è¿›è¡Œé¢„æµ‹å’Œè§£é‡Šã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™ç¯‡è®ºæ–‡ä¸­çš„è®ºç‚¹æ°å¥½é€‚ç”¨äºè¿™ä¸ªä¾‹å­å’Œç†è®ºã€‚äº‹å®ä¸Šï¼Œå®ƒä»¬åœ¨è€ƒè™‘ä¸æœ€è¿‘é‚»ç®—æ³•ç›¸å…³çš„ä¸Šä¸‹æ–‡å­¦ä¹ æ—¶æ•ˆæœæ›´ä½³ï¼Œè€Œä¸æ˜¯åœ¨å¸¸è§„æƒ…å†µä¸‹ã€‚
- en: '![](img/ee54ecf5185d58647f4bc57e2de3efea_37.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ee54ecf5185d58647f4bc57e2de3efea_37.png)'
- en: Yeah I'm happy to answer questions I can go into as much detail as people want
    about any of this and I can also if you send me an email send me more information
    about all this and yeah you know again this work is not yet published and you
    don't have to keep it secret but you know just if you could be thoughtful about
    the fact that it's unpublished work and probably is a month or two away from coming
    out I'd be really grateful for that thank you so much for your time yeah thanks
    a lotã€‚
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘å¾ˆä¹æ„å›ç­”é—®é¢˜ï¼Œæˆ‘å¯ä»¥æ ¹æ®äººä»¬æƒ³è¦çš„ç¨‹åº¦æ·±å…¥è®¨è®ºè¿™äº›å†…å®¹ã€‚å¦‚æœä½ å‘æˆ‘ç”µå­é‚®ä»¶ï¼Œæˆ‘ä¹Ÿå¯ä»¥å‘é€æ›´å¤šç›¸å…³ä¿¡æ¯ã€‚æ˜¯çš„ï¼Œä½ çŸ¥é“ï¼Œè¿™é¡¹å·¥ä½œå°šæœªå‘è¡¨ï¼Œä½ ä¸å¿…ä¿å¯†ï¼Œä½†å¦‚æœä½ èƒ½è€ƒè™‘åˆ°è¿™æ˜¯ä¸€é¡¹æœªå‘è¡¨çš„å·¥ä½œï¼Œå¯èƒ½è¿˜æœ‰ä¸€ä¸¤ä¸ªæœˆæ‰ä¼šå‘å¸ƒï¼Œæˆ‘å°†éå¸¸æ„Ÿæ¿€ã€‚éå¸¸æ„Ÿè°¢ä½ çš„æ—¶é—´ï¼Œæ˜¯çš„ï¼ŒçœŸçš„éå¸¸æ„Ÿè°¢ã€‚
- en: ğŸ˜Šï¼ŒInterã€‚So I'll also open kind of like some general questions and then we can
    do like a round of questions from the students so I was very excited to know like
    so what is the like the line of work that you're currently working on is it like
    extending this so what do you think is like the next things you try to do to make
    it more interpret what are the next yeahã€‚
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼ŒInterã€‚æ‰€ä»¥æˆ‘ä¹Ÿä¼šæ‰“å¼€ä¸€äº›ä¸€èˆ¬æ€§é—®é¢˜ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥è¿›è¡Œå­¦ç”Ÿæé—®çš„ç¯èŠ‚ã€‚æˆ‘éå¸¸å…´å¥‹åœ°æƒ³çŸ¥é“ï¼Œä½ ç›®å‰æ­£åœ¨ä»äº‹çš„å·¥ä½œæ˜¯ä»€ä¹ˆï¼Œæ˜¯åœ¨æ‰©å±•è¿™ä¸ªå—ï¼Ÿä½ è®¤ä¸ºæ¥ä¸‹æ¥è¦åšçš„äº‹æƒ…æ˜¯ä»€ä¹ˆï¼Œä»¥ä½¿å…¶æ›´å…·å¯è§£é‡Šæ€§ï¼Œæ¥ä¸‹æ¥æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ
- en: I mean I want to just reverse engineer language models I want to figure out
    the entirety of what's going on in these language models andã€‚You know like one
    thing that we totally don't understand is MLP layers more we understand some things
    about them but we don't really understand MLP layers very well there's a lot of
    stuff going on in large models that we don't understand I want to know how models
    do arithmetic I want to know another thing that i'm very interested is what's
    going on when you have multiple speakers the model can clearly represent like
    it has like a basic theory of mind multiple speakers in a dialogue I want to understand
    what's going on with that but honestly there's just so much we don't understand
    it's really it's sort of hard to answer the question because there's just so much
    to figure out and we have a lot of different threads of research in doing this
    but yeahã€‚
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘çš„æ„æ€æ˜¯ï¼Œæˆ‘æƒ³è¦é€†å‘å·¥ç¨‹è¯­è¨€æ¨¡å‹ï¼Œæˆ‘æƒ³è¦å¼„æ¸…æ¥šè¿™äº›è¯­è¨€æ¨¡å‹ä¸­å‘ç”Ÿçš„æ‰€æœ‰äº‹æƒ…ã€‚ä½ çŸ¥é“ï¼Œæˆ‘ä»¬å®Œå…¨ä¸ç†è§£çš„ä¸€ä»¶äº‹æ˜¯MLPå±‚ï¼Œè™½ç„¶æˆ‘ä»¬å¯¹å®ƒä»¬äº†è§£ä¸€äº›ï¼Œä½†å¹¶ä¸èƒ½å¾ˆå¥½åœ°ç†è§£MLPå±‚ã€‚å¤§å‹æ¨¡å‹ä¸­æœ‰å¾ˆå¤šæˆ‘ä»¬ä¸ç†è§£çš„ä¸œè¥¿ï¼Œæˆ‘æƒ³çŸ¥é“æ¨¡å‹æ˜¯å¦‚ä½•è¿›è¡Œç®—æœ¯è¿ç®—çš„ã€‚æˆ‘è¿˜éå¸¸æ„Ÿå…´è¶£çš„æ˜¯ï¼Œå½“æœ‰å¤šä¸ªè¯´è¯è€…æ—¶ï¼Œæ¨¡å‹æ˜¾ç„¶èƒ½å¤Ÿè¡¨ç¤ºå‡ºåŸºæœ¬çš„å¿ƒç†ç†è®ºï¼Œæˆ‘æƒ³ç†è§£å…¶ä¸­çš„è¿ä½œï¼Œä½†è€å®è¯´ï¼Œæˆ‘ä»¬è¿˜æœ‰å¾ˆå¤šä¸ç†è§£çš„åœ°æ–¹ï¼Œå›ç­”è¿™ä¸ªé—®é¢˜çœŸçš„å¾ˆéš¾ï¼Œå› ä¸ºéœ€è¦å¼„æ¸…æ¥šçš„äº‹æƒ…å®åœ¨å¤ªå¤šäº†ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªé¢†åŸŸæœ‰å¾ˆå¤šä¸åŒçš„ç ”ç©¶æ–¹å‘ï¼Œä½†å°±æ˜¯è¿™æ ·ã€‚
- en: The interpretpoly team at Anthropic is just sort ofã€‚Has a bunch of threads trying
    to go and figure out what's going on inside these models and sort of a similar
    flavor to this of just trying to figure out how do the parameters actually encode
    algorithms and can we reverse engineer those into into meaningful computer programs
    that we can understandï¼Ÿ
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Anthropic çš„ interpretpoly å›¢é˜Ÿæ­£åœ¨åŠªåŠ›æ¢ç´¢è¿™äº›æ¨¡å‹å†…éƒ¨å‘ç”Ÿçš„äº‹æƒ…ï¼Œè¯•å›¾å¼„æ¸…æ¥šå‚æ•°æ˜¯å¦‚ä½•ç¼–ç ç®—æ³•çš„ï¼Œä»¥åŠæˆ‘ä»¬èƒ½å¦å°†å…¶é€†å‘å·¥ç¨‹ä¸ºæœ‰æ„ä¹‰çš„è®¡ç®—æœºç¨‹åºï¼Œä»¥ä¾¿æˆ‘ä»¬ç†è§£ï¼Ÿ
- en: ğŸ˜Šï¼Œanother question I just like to you're talking about like how the have tend
    to do metal learning in that So it's like you spend a lot of time talking more
    like they had something like that was like interesting but like can you formalize
    the sort of metal learning algorithm they might be learning is it possible to
    say like oh maybe this is a sort of like internal algorithm that's going that's
    making them like good metal learners something like that I don't know I mean I
    think that there's roughly two algorithms One is this algorithm we saw in the
    one layer model we see in other models too especially early onã€‚
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¦ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œä½ è°ˆåˆ°ä»–ä»¬å¦‚ä½•è¿›è¡Œå…ƒå­¦ä¹ ã€‚ä½ èŠ±äº†å¾ˆå¤šæ—¶é—´è°ˆè®ºç±»ä¼¼çš„äº‹æƒ…ï¼Œè¿™å¾ˆæœ‰è¶£ï¼Œä½†ä½ èƒ½å¦å°†ä»–ä»¬å¯èƒ½æ­£åœ¨å­¦ä¹ çš„å…ƒå­¦ä¹ ç®—æ³•å½¢å¼åŒ–ï¼Ÿæ˜¯å¦å¯ä»¥è¯´ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ç§å†…éƒ¨ç®—æ³•ï¼Œä½¿ä»–ä»¬æˆä¸ºä¼˜ç§€çš„å…ƒå­¦ä¹ è€…ï¼Œæˆ‘ä¸çŸ¥é“ï¼Œæˆ‘è®¤ä¸ºå¤§è‡´ä¸Šæœ‰ä¸¤ä¸ªç®—æ³•ï¼Œä¸€ä¸ªæ˜¯åœ¨å•å±‚æ¨¡å‹ä¸­çœ‹åˆ°çš„ç®—æ³•ï¼Œæˆ‘ä»¬åœ¨å…¶ä»–æ¨¡å‹ä¸­ä¹Ÿçœ‹åˆ°äº†ï¼Œç‰¹åˆ«æ˜¯åœ¨æ—©æœŸã€‚
- en: which is just know try to copy know you saw a word probably a similar word that
    iss gonna happen later look for places that it might fit in and increase the probability
    So that's one thing that we see and the other thing we see is induction head which
    you can just summarize as in context to your neighbors basically and it seemed
    know possibly with other things but it seems like those two algorithms and the
    specific instantiations that we are looking at it seem to be what's driving in
    context learning that would be my pre theoryã€‚
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯è¯•å›¾å¤åˆ¶ï¼ŒçŸ¥é“ä½ çœ‹åˆ°çš„ä¸€ä¸ªè¯ï¼Œå¯èƒ½æ˜¯ä¹‹åä¼šå‡ºç°çš„ç±»ä¼¼è¯ï¼Œå¯»æ‰¾é€‚åˆçš„åœ°æ–¹å¹¶æé«˜æ¦‚ç‡ã€‚æ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„ä¸€ä»¶äº‹ï¼Œå¦ä¸€ä¸ªæ˜¯å½’çº³å¤´ï¼ŒåŸºæœ¬ä¸Šå¯ä»¥æ€»ç»“ä¸ºå¯¹é‚»å±…çš„ä¸Šä¸‹æ–‡ï¼Œå®ƒä¼¼ä¹ä¸å…¶ä»–äº‹ç‰©ä¸€èµ·æœ‰æ•ˆï¼Œä½†è¿™ä¸¤ç§ç®—æ³•åŠå…¶å…·ä½“å®ä¾‹ä¼¼ä¹æ˜¯é©±åŠ¨ä¸Šä¸‹æ–‡å­¦ä¹ çš„å…³é”®ï¼Œè¿™å°±æ˜¯æˆ‘çš„åˆæ­¥ç†è®ºã€‚
- en: Yeahï¼Œ sounds very interestingã€‚Yeahï¼Œ okayï¼Œ so let's open like a run of first
    two questionsã€‚So yeahã€‚feel free to go ahead for questionsã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_39.png)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œå¬èµ·æ¥å¾ˆæœ‰è¶£ã€‚å¥½çš„ï¼Œæˆ‘ä»¬å°±å¼€å§‹å‰ä¸¤ä¸ªé—®é¢˜å§ã€‚è¯·éšæ„æé—®ã€‚![](img/ee54ecf5185d58647f4bc57e2de3efea_39.png)
