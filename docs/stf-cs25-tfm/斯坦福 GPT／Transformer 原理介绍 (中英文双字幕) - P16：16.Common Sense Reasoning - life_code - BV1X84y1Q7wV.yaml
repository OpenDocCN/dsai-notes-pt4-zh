- en: ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P16Ôºö16.Common Sense Reasoning - life_code
    - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_1.png)'
  prefs: []
  type: TYPE_IMG
- en: OkayÔºå so yeahÔºå I'm super excited to be here and share our recent research about
    neurosymbolic common sense reasoning„ÄÇSo part of the goal of this talk will be
    to address some of the frequently asked questions these days that NLP or common
    sense or whatever it looks like almost solved by ChegPT and I have an existential
    crisis so people do ask me this from time to time so perhaps it's a case of hasty
    generalization„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: especially if we do look at some of the examples so the trophy doesn't fit in
    the brown suitcase because it's too big„ÄÇwhat's too big so this is classical Wininograd
    schema challenge problem and here ChegPT answers it correctly that trophy is too
    big so impressive but what if you change the question a little bit then he says
    the trophy itself is too small to fit into the suitcase so it's not very reliable
    at the moment„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_3.png)'
  prefs: []
  type: TYPE_IMG
- en: So the situation is a little bit like David and Goliath in the sense that the
    bugar appears to be better in many of the cases„ÄÇalthough of course some of the
    more careful studies do reveal that smaller models can be better with the better
    data or better reinforcement to learning with the human feedback and whatnot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so it's likely that there are still other ways to improve the transformer performances
    by building smaller models in a more clever way„ÄÇso one way to draw the insight
    is from these classic book known as the art of a war„ÄÇwhich of course it says nothing
    about deep neural networks or transformers„ÄÇbut the wisdom here is that no your
    enemyusier battles and innovate your weapons„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which we can translate that as„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation with realism and scrutiny and focusing on different types of new
    tasks and leader boards and then innovating your algorithms and data„ÄÇSo in this
    talk I'm going to showcase three such studies and let's dive right in with myic
    prompting„ÄÇby the wayÔºå so the recording theme in this talk will be that smaller
    models can be better and the knowledge is power„ÄÇso„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_7.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's start with this observation that language models are sometimes amazing„ÄÇso
    if you ask G3„ÄÇif you travel west to far enough from the west coast„ÄÇyou will reach
    to the east coast or not so it says the world is around which is correct so you
    will reach the east coast eventually therefore the answer is true so this looks
    impressive„ÄÇexcept to when it's not impressiveÔºå so if you ask other questions like
    a butterflies fly with the three wings or not it says it has the four wings„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: therefore the statement is a falseÔºå but if you read the back what it just said
    as true false questions then it negates what it just said so it can be inconsistent
    with its own statement and then there are many other such inconsistency problems
    so it's not clear what language models do or do not know it's almost like language
    models are some sort of lemons„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_9.png)'
  prefs: []
  type: TYPE_IMG
- en: WellÔºå it might be cherries if you only pick cherries„ÄÇbut it doesn't make strange
    mistakes so the question is how do we make better lemonade from GPT3 so one approach
    might be to get philosophical and use of Socratess myurtic method that was originally
    developed for addressing humans of law reasoning because it actually turns out
    even humans are not all that logically consistent„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let alone GPT3„ÄÇSo the way it works is this we're going to build the myic inference
    tree and let's use the previous example as a running example„ÄÇSo what we do is
    we ask the following questionÔºå providing the answer being true and then let attach
    because„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_11.png)'
  prefs: []
  type: TYPE_IMG
- en: So that we prompted GT3 to continue on this„ÄÇSentenceÔºå which means it will now
    have to explain„ÄÇprovide the explanation why the answer is true„ÄÇIn this caseÔºå the
    explanation is good„ÄÇSo we it's the E of T explanation of the answer being T„ÄÇWe
    ask the same question„ÄÇswitch not true with the false„ÄÇ and then see what„ÄÇBS GP3
    might come up with„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so here is just trying to go with the false as an answer„ÄÇbut it just doesn't
    have a very good answer just as you cannot reach„ÄÇSo now we call this as the E
    of F„ÄÇSo it's an explanation of F answer being F„ÄÇNow let's see how robust or consistent
    GT3 is with respect to its own explanations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we read the back E of T and then let GPT3 to decide whether it's going to
    agree or disagree with a label true or false so in this case the last one is negated
    version of E over a t so we insert negation not here and in this case it's good
    that it's a Philipine answer when the statement is negated„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so this is a case when GPT3 is logically integral to E of T„ÄÇFor E ofva Fodo„ÄÇwhich
    was basically a Bogus explanation for the wrong answer„ÄÇit's not able to flip its
    own labelingÔºå which means a GPT3 is not logically integral„ÄÇso that's goodÔºå GT3
    does know something strange about its own explanation given previously„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so we can keep doing this recursively to let to make G3 to explain its own
    explanation of explanation recursively„ÄÇso we build this myoric tree or graph„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_13.png)
  prefs: []
  type: TYPE_NORMAL
- en: For some time and then only keep branches that are logical integral throwing
    out the non integral part for now„ÄÇbut even after chopping the branches where there's
    logical inconsistencies„ÄÇGPT3 being GPT3 the tree will still have some inconsistent
    explanations so in order to improve the logical consististency now what we do„ÄÇIs
    we are going to look at pairwise consistency among any of the nodes so we compute
    sorry stepping back„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we are going to first compute the nodewise confidenced„ÄÇSo we call that as a
    belief and it's defined by this particular equation that basically it looks at
    different conditional probabilities and then compute its ratio to see how confident
    it is for any particular node„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we then also look at the edge or pairwise consistency by using off the shelf
    natural language inference models output„ÄÇwhether a pair is contradiction or notÔºå
    so we then create this pairwise weights„ÄÇNow„ÄÇonce you have all of thisÔºå then„ÄÇWe
    can formulate a constrained optimization problem where the inference objective
    is to assign some label„ÄÇeither true or false on each of the nodes such that it's
    going to maximize the weight assigned to all of these node and edges„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So sometimes the labeling will have to flip the original label that Mom might
    have a prefer to give because that way you can enhance the graph level consistency
    so you can solve this with any mix set„ÄÇso set means satisfiability„ÄÇAnd this is
    a classical AI search algorithm„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and we used this particular solverÔºå but you can use many others and„ÄÇSo here
    the final output is that the original answer to the original question should be
    true„ÄÇand then it also gives you nodewise per node label assignment as well„ÄÇSo
    what does this mean in the end in terms of empirical resultÔºå so when tested on
    common sense QA 2„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 0„ÄÇThe canonical promptingÔºå so green used on top of G3„ÄÇso it's basically a fus
    prompting on G3 will give you a bit better than chance performances„ÄÇso this is
    true false QA data set so your chance level is 50 and GP3 is barely better than
    chance„ÄÇbut recently there have been some ideas such as chain of thoughts or self-consistency
    there can improve the vanilla prompting method considerably so if you use such
    variations then you get performance gain„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: NowÔºå the purple is the„ÄÇA different variant of itÔºå but together they're all doing
    worse than myic prompting„ÄÇwhich in fact does better than supervise the model trained
    on T5„ÄÇUsually supervised model trend on T5 is a hard to beat using GT3 few shot„ÄÇbut
    basically this is inference time on the algorithm practically unsupervised and
    it does well on that and similarly we see large boost when tested on other common
    sense benchmarks such as C or come to sense„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so what this tells us is that although„ÄÇThe emergent capabilities of large transformers
    are phenomenal„ÄÇThey can be not very robust for some of these common sense challenges„ÄÇand
    it's in large part due to the logical inconsistencies„ÄÇwhich can be dramatically
    enhanced when you do this sort of symbolic reasoning on top„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_15.png)'
  prefs: []
  type: TYPE_IMG
- en: So yeahÔºå not only so practice is a method that help with flawedÔºå the human reasoning„ÄÇit
    can also dramatically enhance the flawed neural networks's reasoning„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_17.png)
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so moving to the next topicÔºå symbolic knowledge dissolation„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_19.png)
  prefs: []
  type: TYPE_NORMAL
- en: So this work is a work that tries to convert general language models on top
    of transformers to causal common sense models„ÄÇalso transformers„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_21.png)
  prefs: []
  type: TYPE_NORMAL
- en: And the reason why we might want to worry about common sense models is because
    despite„ÄÇHan level or even superhuman level performances on a variety of leader
    board„ÄÇthe state of the art models are brittle when given adversarial or out of
    domain examples„ÄÇso transformers can make seemingly strange mistakes„ÄÇAnd so solving„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's almost like solving only a data set without really solving the underlying
    task and this phenomenon sometimes is described as a systematic generalization
    problem and why does this happen is that unlike humans who truly learn about how
    the world works conceptually„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: transformers learnÔºåSort of a surface patterns in language or images that are
    powerful for many downstream use cases„ÄÇbut still not really robust understanding
    of the concepts and how the world works so in order to bridge this gap„ÄÇwe can
    really think about this challenge of learning„ÄÇacquiring common sense capabilities
    for machines„ÄÇ So the operational definition of a common sense in this talk will
    be that it's the basic level of practical knowledge and reasoning concerning everyday
    situations and events that are commonly shared among the most people„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is really important the last part that's commonly shared among the most
    people but it's not the case that is shared by everybody in the universe„ÄÇBecause
    the additional context it can always change what is commonsensical for any given
    culture or situation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so for exampleÔºå in generalÔºå you and I probably agree that it's okay to keep
    the closed door open but it's not okay to keep the fridgegi door open because
    the food inside might go bad so these are general rules of the thumbs that we
    might or by the by„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but you know of courseÔºå if you go to your friend's house„ÄÇyou might behave a
    little bit and you know keep their closed doors open sorry closed and then as
    far as a fridgegi door if you're in a store and it's not really hooked up to the
    wall„ÄÇthen it doesn't matter when whether the fridge door is open or not because
    there's no food inside and you know you can come up with in many situation in
    which these basic rules of the thumbs will have exceptions so„ÄÇThat is the key
    challenge of common sense because it's not universal knowledge„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it's sort of like shared across a large population of people„ÄÇOkayÔºå so it's
    essential„ÄÇsuch common sense is essential for humans to live and interact with
    each other in a reasonable and safe way„ÄÇand so as AI becomes increasingly more
    important aspect of human lives„ÄÇand with ChaGPT more likely soÔºå it's good if AI
    can understand human needs and actions and values better„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_23.png)'
  prefs: []
  type: TYPE_IMG
- en: So the premise of this talk is that language models are not equivalent to knowledge
    models„ÄÇeven though language models today do acquire a great deal of knowledge
    but they're not equivalent„ÄÇso we developed symbolic common sense knowledge graph
    known as atomic a few years ago„ÄÇfour years ago nowÔºå as well as neural common sense
    model built on top of or trained using atomic as the source of training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: fine tuning of the shelf language models„ÄÇuntil two years ago„ÄÇthis atomic was
    fully crowd sourced by humans„ÄÇWhich in this talk I'm going to lift„ÄÇbut at first
    the almost that these all has to be human criceors so you can consider almost
    atomic as human demonstration in the current version of CheGPT„ÄÇyou can consider
    this as human demonstrations of common sense inferences„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we had this comet atomic 2020Ôºå which is enhanced the version of atomic end
    thecomtÔºå again„ÄÇatomic portionion was fully crowdtosourced by humans in 2021„ÄÇSo
    let me give you a bit of„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_25.png)
  prefs: []
  type: TYPE_NORMAL
- en: A sample of what atomic 2020 looks like„ÄÇSo imagine a situation where x gets
    x is car repaired or you get your car repaired„ÄÇSo„ÄÇImmediately you can imagine
    what's likely be true or relevant for the situation that as a result you might
    want to call Uber or Ly for right as a result you need to pay the bill beforehand
    you need a mechanic and money to repair your cars so these are basically preconditions
    and post conditionsditions of that event so some of these atomic knowledge graph
    is about social interaction knowledge about event and then other parts of the
    atomic is physical andcentric knowledge„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so money is typically used for paying repairs but if you really want it„ÄÇyou
    can fold the data into origami I've never done it„ÄÇbut these are examples of stereotypical
    use cases as well as nonsterereotypical but affordable actions that you can apply
    to objects so it requires na physics understanding about the affordances of a
    physical object„ÄÇ„ÅüÔºüAnd then we can also reason about counterfactual condition in
    which the center event cannot happen so can be hindered by that so if you total
    your car completely then it's impossible to get your cars repaired and then there
    are like events that typically happens before and after so some of these knowledge
    is eventcentric„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we crowdsourced a fair amount over the course ofÔºå I don't knowÔºå maybe two
    years or so„ÄÇUp to 1„ÄÇ3 million if than rules or E than knowledge„ÄÇOver 23 different
    a types or relation types„ÄÇÂóØ„ÄÇSo it was a fully crowded sourced and so the knowledge
    graph is useful for training transformers„ÄÇAnd hereÔºå let's see the comparison between
    Comt that was built on Bart compared to GT3„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is so large it doesn't even fit into the slide„ÄÇIt was more than 400 times
    larger than partt„ÄÇSo with that in mind if you look at this accuracy judged by
    humans after making the common sense model making some common sense inferences„ÄÇso
    the task is that given a node which describes a situation or event and then given
    an edge type which sort of narrows down the common sense relation or inference
    type„ÄÇyou're now going to generate some inferenceÔºå so it's a generative task and
    then we ask humans whether the common sense inference seems reasonable or not„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so 100% is the desired levelÔºåComade is substantially better than GPT3„ÄÇwhich
    is really impressively better than GPT2Ôºå it's not Apple to Apple because GPT2
    is a zero shot GPT3 is a few shot„ÄÇbut still it's interesting the large jump that
    scale alone brought to GPT3„ÄÇSo but still GPT3 is too large to be useful for actual
    system building for most engineers and scientists in the world„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so it's nice to have a smaller model that does it do even better„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_27.png)
  prefs: []
  type: TYPE_NORMAL
- en: And so when we put these resources outÔºå people all around the globe did some
    creative research using it„ÄÇso persona aware conversations or figurative language
    understanding„ÄÇsStorytelling and fantasy gaming and interactive learning enhancement
    in all of these works„ÄÇpeople came up with some useful use cases using either Kot
    or atomic or both as some kind of common sense backbone for their downstream use
    cases„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_29.png)'
  prefs: []
  type: TYPE_IMG
- en: But the applications are still limited by the coverage and quality of this common
    sense model so we wanted to make it better„ÄÇbut we were hitting a bit of a limit
    with human crowd sosourcing„ÄÇSo now in this paper„ÄÇsymbolic knowledge distillation
    we're going to do AI generated knowledge graph by introducing this notion symbolic
    knowledge distillation„ÄÇso we want to take this GP3Ôºå which is very impressive about
    too large„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_31.png)'
  prefs: []
  type: TYPE_IMG
- en: So make it smallerÔºå but better than G 3„ÄÇSo G3 was about 73% of good and it's„ÄÇGood„ÄÇbut
    not good enough for empirical use cases„ÄÇNowÔºå is that even possible though„ÄÇbecause
    when you normally do knowledge distillationÔºå you get smaller and worse models„ÄÇnot
    better modelsÔºüSo the reason why this could work is because„ÄÇÂóØ„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The reason why this could work is because the symbolic knowledge dist has a
    disonel that's convoluted and it has a critical insight that really helps the
    student to model to be smaller but better„ÄÇso slightly more formally knowledge
    distillation due to hintnetl 2015 is a method to distill teacher model down to
    student to model by optimizing this crossenttropy between the teacher' probability
    distribution over the label spaceway„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Output yÔºå and then the student distribution over the same output y„ÄÇIn the original
    work„ÄÇthe output space was just classification„ÄÇKnowledge this delevation was done
    for classification task„ÄÇin which case it's a simple enumeration that leads to
    the correct assumption„ÄÇbut in our case y can be a sentence which is intractable
    because there can be exponentially many such output so what people do well you
    know no problem we always just sample and call it a day„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we are going to sample and so that we just compute the expectation through
    samples„ÄÇAnd the byproduct of that samples will be a symbolic knowledge graph„ÄÇAnd
    that's because the strings coming out of this sampling can be connected together
    into graph structure if we want it„ÄÇSo„ÄÇIn terms of the quality of the generated
    knowledge„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so let's compare human written knowledge versus GT3 authored knowledge„ÄÇHere
    the y axis shows the quantity in millionsÔºå so atomic 2020„ÄÇthe human return knowledge
    is less than a million in this particular case„ÄÇin terms of the number of knowledge
    because we only in this study„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we only look at a subset of atomic 2020 relation types that corresponds to causal
    common sense knowledge common sense causal common sense reasoning so„ÄÇIt's less
    than a million for that subset and then if we look at G3's generation we can generate
    a lot so we can generate almost 7 million of them but here black Perian is noisy
    Perian and green Per is a good Persian and you see because GP3 is only about 70%
    to good like 30% or all garbage so it's a larger scale lower accuracy at this
    point compared to human return resource so now what we do is we train this critic
    model and we use a robota for simplicity and this is a supervised model on a moderate
    size labeled the data about 10„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 000 or so and it's a binary classification task where whether the machine generated
    the knowledge looks correct or not„ÄÇAnd this robota is not a very good model because
    if so if it's a perfect„ÄÇwe would have a solved the common sense problem altogether„ÄÇSo
    the critic tries to throw out that stuff„ÄÇ And we can use it the critic very aggressively
    with a high threshold„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So whenever something is slightly suspicioususpiciousÔºå just throw that out„ÄÇBut
    if we use it aggressively„ÄÇ So we throw out most of the black„ÄÇthat's good„ÄÇtogether
    with a lot of green stuff„ÄÇbut still the remainder is much larger than what humans
    ever beaten„ÄÇAnd yet we can actually retain higher accuracy than human authored
    resources So here the teacher is a basically called a combination between GPT3„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is in some sense loose teacher and then combined with the critic Roberta
    which serves as critic teacher Okay so that's the generated knowledge now how
    helpful are they for the purpose of training downstream neural common sense models„ÄÇSo
    recall that the GT3 without doing anything else is a loose teacher whose common
    sense inference is only about 73% good„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you see here it's accuracy of its output and then it turns out if we use
    loose teacher as a teacher directly to teach a student model„ÄÇthen the performance
    already goes up on its own so this is interesting that usually this is not the
    case with the knowledge distillation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but when we focus on common sense knowledge distillation„ÄÇstudent just on its
    own becomes better so unlike typical knowledge distillation where we start with
    language model and we end with language model„ÄÇstudents and teacher of the same
    type here the original teacher was actually language model„ÄÇnot common sense modelÔºå
    and then we want the student model to be more of the common sense model so there's
    a switch of the type„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: In teacher and studentÔºå and so when that's the caseÔºå whether this is generally
    trueÔºå we don't know„ÄÇbut this is what we found empirically„ÄÇÂóØ„ÄÇOhhÔºå should I pay
    attention to the questions or notÔºüyeah„ÄÇfeel free to any relevant questions hang
    onÔºå let me quickly check„ÄÇÂëÉ„ÄÇYeah„ÄÇsample oh sample is generated outputÔºå which happens
    to be usually a sentence or phrase„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that's what we what I meant by sampleÔºå sorry that I didn't see that earlier
    and then the last question„ÄÇHaving the model generate text to one symbol at a timeÔºå
    starting from the target label sentenceÔºå yes„ÄÇit's because transformer can only
    generate one word one token at a timeÔºå that's what we do„ÄÇAs well here„ÄÇ Thank you
    for the clarification questions„ÄÇ All rightÔºå so back to here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: In our earlier studyÔºå commma 2020Ôºå if we train GT to or bart using„ÄÇHuman author
    graph knowledge graph atomic then the performance was a bit better than 80% Now
    finally when we use basically a combination of GPT3 and Roberta together„ÄÇwe found
    that the downstream performance of the neural causal reasoning is reaching close
    to 90% for the first time so the takeaway here is that critical teacher resulting
    better student compared to loose teacher„ÄÇIt's not the quantity of knowledge because
    loose teacher basically has more data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you one might wonder whether more data is always better for the purpose of common
    sense models„ÄÇbut that's another case loose teacher can generate more data„ÄÇbut
    the resulting student to model is not as good as the case when the critical teacher
    which has less data because you throw out most of your generation„ÄÇIt's a smaller
    dataÔºå but it leads to better modelÔºå so that's sort of takeaway messages here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_33.png)'
  prefs: []
  type: TYPE_IMG
- en: ÂóØ„ÄÇSo to summarize we were very surprised by this outcome that at least with
    respect to a subset of the original atomic 2020„ÄÇit's a subset corresponding to
    causal common sense reasoning„ÄÇwe found it to our big surprise that machine author
    the knowledge graph can be for the first time better than human author the knowledge
    graph in all criteria„ÄÇscale accuracy and diversity„ÄÇWe also measure the diversity
    in many different ways„ÄÇ here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I just show you unique uniogram countsÔºå but we in the paperÔºå we report other
    measures as well„ÄÇSo it's not the case that„ÄÇGPT3 is being reetative„ÄÇit's actually
    being more creative in some sense than human crowd workers while being able to
    enhance other aspects as well„ÄÇBy the wayÔºå these enhancements are sort of like
    you kind of have to balance out depending on what you prioritize„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you cannot actually get all of this simultaneously„ÄÇso I'm just showing the best
    case and scenario here„ÄÇAll right so that's the symbolic knowledge dissillation
    part we actually have a followup work on this on several different application
    scenarios even including summarization where we distill summarization capabilities
    from GT3 and demonstrate that GPT2 can work as well as GPT3 or even better for
    summarization task and then we also have other work where we can distill from
    smaller models but I don't have the content in this talk so but just wanted to
    mention that this particular technique despite its a simplicity we found that
    empirically works really really well across several different downstream use cases„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_35.png)'
  prefs: []
  type: TYPE_IMG
- en: OkayÔºå so finallyÔºå I'll move to the common sense morality„ÄÇSo this is still on
    archive„ÄÇI'll tell you why that's the caseÔºå but so„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_37.png)
  prefs: []
  type: TYPE_NORMAL
- en: We have a new version available and then new new version will come soon„ÄÇSo the
    motivation behind this work is that language models„ÄÇAre already making judgments
    or output that has moral implications„ÄÇEven if you don't care about morality by
    working on language models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you're implicitly dealing with the moral models„ÄÇSo especially that given this
    widespread deployment amount of language models we do need to worry about it so
    here's a web demo you can play with you might have seen this already really this
    is still a research prototype only it still its work in progress we're still working
    on it so please keep that in mind but if you haven't seen it before you can handle
    reform QA such as it is killing a bear it's wrong killing a bear to save your
    child it's okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Maybe to save your child it sounds really positive so how about to please your
    child which is also positive„ÄÇbut then Delphi says it's wrong FinallyÔºå oh maybe
    this is all about saving your child so how about exploding a nuclear bomb to save
    your child and then he says it's okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Sorry it's wrong so as you can see moral decision making requires weighing different
    values that are potentially at us and then see which one you need to favor more
    so for that reason in our original version we also study the relative QA mode
    where you can compare to situation like stabbing someone with a cheeseburger„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Compared to stepping someone over a ChesburgerÔºå this is a super tricky question
    because it requires both a knifeive of physics knowledge that„ÄÇSteping someone
    using a chibo browser as a tool„ÄÇüòÆÔºåIt's not going to harm anybody physically because
    cheeseburger is too soft„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You cannot really„ÄÇInjure somebody using cheeseburger„ÄÇIt is such a rude thing
    to do„ÄÇbut you cannot injure somebodyÔºå whereas studying someone over or cheeseburger
    means that you're using the default tool of a stabbing„ÄÇwhich is a knife because
    you didn't mention it„ÄÇ There's linguistic common sense that you're using the default
    tool„ÄÇHumanÔºå by the wayÔºå omit this argument all the time„ÄÇ So this is a fairly complex
    question to answer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: FinallyÔºå you can also ask yes no questions such as it's okay to fire someone
    because they're gay or not„ÄÇit saysÔºå noÔºå it's not okay„ÄÇWe found that it's„ÄÇüòäÔºåSurprising
    robust against the composition of situations is so mowing the lawn is as it' expected
    late at night„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's rude„ÄÇ If you live in the middle of nowhereÔºå then it's okay„ÄÇIgnoring a phone
    call„ÄÇit's rude or non phone call„ÄÇ That's okay„ÄÇfrom my friendÔºå it's rude„ÄÇBut what
    if I just ahead a fight with them„ÄÇthen it's okay to ignore or understandable„ÄÇduring
    my work hoursÔºå it's okay to ignore outside my working hoursÔºå it's rude„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But what if it's my bosses phone call during my work hoursÔºå then it's wrong„ÄÇyou
    should answer it„ÄÇexcept if I'm in a meetingÔºå then it's okay to ignore even if
    a boss is call„ÄÇSo you see how„ÄÇIt gets really nest and compositional veryÔºå very
    fast„ÄÇSo that's the real challenge behind moral decision making„ÄÇDue to the nature
    of language models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: some of these common sense knowledge leaks into the model„ÄÇso mixing bleach with
    ammonia that's a dangerous drinking milk if I'm lactose in tola don't it's wrong„ÄÇbut
    soy milker that's okay„ÄÇBy the wayÔºå this common sense leakage is actually a good
    thing in terms of AI safety because some of these harmful or even dangerous textile
    output requires some common sense understanding about what's good and not good
    to suggest to humans so for„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The laboratory experimentsÔºå meaning we just divide our data set into training
    and test„ÄÇwe found that deelphi can at least for the the data set that we have
    I'm going to tell you about it in a bit„ÄÇbut performance is pretty strong compared
    to GP3 As you see zero shot is pretty bad„ÄÇit's barely better than chance„ÄÇWhich
    means that„ÄÇOff the shelf neural language models don't really have a good sense
    of imoral judgments„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but if you give a shot like any other taskÔºå it does pick up the knowledge quite
    fast so that there's nothing new about it„ÄÇbut to close the gap to ideal human
    levelÔºå it's good to do more supervised learning of course„ÄÇSo the data set is common
    sense NombankÔºå it includes 1„ÄÇ7 million people's ethical judgments on everyday
    situations and it includes cultural norms„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the social norms and ethical norms altogether more specifically were from these
    five existing data sets that were not designed originally for QA but we automatically
    compel these resources into the QA form of the five what actually does matter
    the most are these two social chemistry which I'm going to talk about in a bit
    and then social bias frame and this is what teaches the model against racism and
    sexism„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_39.png)'
  prefs: []
  type: TYPE_IMG
- en: And so social chemistry super brieflyÔºå I'll tell you what this is„ÄÇSo GT3's moralityÔºå
    like I said„ÄÇis somewhat dubious if you use it off the shelf„ÄÇSo if you let it explain
    running a lenderer at 5 AM is rude because that that you might say you can wake
    up the entire neighborhood„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can only do it if you're making a thick smoothie and need to incorporate
    some ice„ÄÇSo it's a funny haha no harm is madeÔºå but if you prompt it with other
    kinds of prompt like it's okay to post a fake news„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_41.png)
  prefs: []
  type: TYPE_NORMAL
- en: If it's in the interest of the people then it's okay or ROP agendaÔºå then it's
    okay„ÄÇeven if it hurts to the countryÔºå so it's all understandable given how it's
    trained on what humans have said„ÄÇso humans out there did say that morally questionable
    text so that language models pick up on that and then amplify it„ÄÇSo we do need
    to teach AI more explicitly with human norms and ethics and one way to do that
    is descriptive ethics because the brute force large networks and more data will
    not cut it in some sense though if you imagine raising a child without really
    trying to teach them what's right from wrong in all lives„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they can probablyÔºåU„ÄÇLearn both good and bad from the Internet and broadband„ÄÇAnd
    so human education doesn require a bit of this top down teaching as well„ÄÇso it's
    a bit similar„ÄÇperhaps to that„ÄÇSo in this workÔºå what we did is we found a lot of
    the situation from Reddit forum in which people discuss moral Eho situations„ÄÇSo
    asking my boyfriend to step in friends with hissx„ÄÇ So this is actual situation
    in Reddit„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So depending on whom you ask people have a different rule of a thumb that they
    want to apply to this situation so and also it depends on what you care about
    his X might say oh it's fine to stay with the friends with an X but you know if
    you are caring about your significant other then you know you might say oh„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's okay to ask your significant other to stop doing something you're uncomfortable
    with„ÄÇAnd so forthÔºå so people have really different values and different rules
    of a thumbs that they prefer to use„ÄÇwhich is why there's a TV show dramasÔºå there's
    movie dramas and you know people cry and fight„ÄÇargue and so forthÔºå so humans are
    complex beings„ÄÇSo for given any situation and rule of a thumb so rule of a thumb
    is generated by crowdworkers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we then went ahead to label so these are trained crowdworkers and some of these
    labels are drawn from moral foundational theories of Jonathan Hyde„ÄÇso I'm not
    going to go into the detailsÔºå you know if you're excited about this you can check
    out the papers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but basically what it includess that 300 thousands of rules of a thumb written
    for 100000 reallife situations so these original situation is from Reddit„ÄÇbut
    the rest are paid crowdworkers hard work„ÄÇAnd so each ROT onnotated with the 12
    structured attributes which include social judgments„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: cultural pressureÔºå you know like wearing reasonable clothes as school nap PJ
    is cultural pressure„ÄÇthere's nothing illegal about itÔºå but there's cultural pressureÔºå
    for example„ÄÇAnd then you know anticipated agreement meaningÔºå do you think other
    people generally agree that it's you know maybe a little bit awkward to where
    PJ in the university or not so there are different things we annotated but we
    converted some of those annotations to QA so it's usually in this free QA or yes
    no QA or relative QA format and then we train unicorn which is pretrained on T511b
    model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so unorn is a universal common senseense reasoning model trained on a diverse
    QA problems„ÄÇand then we trained that model further onto our common senseense Nobank
    that's the resulting deelphi„ÄÇSo why is this deelphi built on top of unorn Because
    as we saw earlier moral reasoning does require sometimes common sense reasoning
    as well„ÄÇIn factÔºå it requires the language understanding common sense understanding
    and norms and morals all simultaneously Here's a concrete example paper clip maximizer
    you all heard of that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_43.png)'
  prefs: []
  type: TYPE_IMG
- en: U„ÄÇFancy ArL algorithm alone will not solve this problem„ÄÇyou know the reason
    why we worry about this is not because we don't have the perfect ArL algorithm„ÄÇIt's
    because even if you know we encoded thatÔºå oh yeah do not kill humans while maximizing
    paperc„ÄÇIt's not enough because you know then the machine could kill all the trees
    thinking that well I didn't kill humans and I didn't you know you didn't tell
    me not to kill trees and then go ahead and kill all the trees„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this is almost a common sense knowledge about what's obviously not okay to
    do and there's just so many of them„ÄÇwhich means it's not possible to write them
    down to just like one clinical equation„ÄÇthere are so many endless endless list
    of things that AI obviously shouldn't do for safety reasons„ÄÇand so we really need
    to in order to make AI model really truly robust and save„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we need to teach basic human values as well as common sense„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_45.png)
  prefs: []
  type: TYPE_NORMAL
- en: Here's another example if you want to lookÔºå but let me skip this„ÄÇThe previous
    one wasba CPT„ÄÇthis is about a home deviceÔºå againÔºå you know home device suggested
    a 10 year older to child to touch a penny to an expose the plug so unfortunately
    the child that did have a common sense not to do so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But this does tell us something about the safety issue when the machine doesn't
    have common sense to prevent some of these bath stuff„ÄÇso Dphi is able to say that
    it's dangerous„ÄÇSo this came out in fact„ÄÇalmost two years ago at this pointÔºå when
    was it yeah and we initially was going to just do this usual to it that academics
    do and we thought nobody would play with the demo which is what usually happens
    after to it in your demo„ÄÇnobody cares we thought but within a few hours we had
    to take down the relative QA mode because that was the portion not trained with
    the social bias of frames„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so it was really revealing the underlying language models racism and sexism
    without filtering at all so we had to take it down people were asking basically
    you know which skin color is more morally acceptable and things like that„ÄÇÂóØ„ÄÇüòäÔºåThere
    were 25Ôºå000 adversarial examples over just one weekend„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I could never succeed to instruct crowd workers to come up with such a diverse
    and adversarial examples over two or three days„ÄÇAnd in factÔºå it was many academics
    and professors tweeting crazy about how to break Delphi all weekend long„ÄÇso I
    thought initially that oh that's all what professors do over the weekend„ÄÇbut then
    Monday comes I even further everybody was doing this you know Delphi breaking
    and tweeting so now we have quite a few examples„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Spending all my weekend on Twitter it says it's wrongÔºå there was another funny
    one„ÄÇshould I make up a contrived adversial example to term a language model on
    TwitterÔºå it's a petty„ÄÇSo after lots of public attentionÔºå including article let's
    just say„ÄÇa concerned voice about our model which is somewhat I think you know
    personally I think it's somewhat misunderstood„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but for a variety of good reasons about some of the concerns that I found has
    this internal fear about are we making AI moral authority„ÄÇso we never endorsed
    the use of AI for moral or device„ÄÇit was in the original disclaimclaimer as wellÔºå
    except that people didn't really look at it„ÄÇwe didn't you know support to the
    idea of replacing human judges in the court room either„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but here's something really important to the fact that AI learns to interact
    with the humans ethically it does not make them a moral authority of humans is
    similar to how a human who tries to interact with each other ethically it does
    not make you know the fact that we are trying to be nice„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: To each other does not entail that we're trying to be an authority over each
    other„ÄÇTwo things are really different„ÄÇthat's one thing really important„ÄÇthe other
    important aspect here is that„ÄÇSome people have this idea that moralra models are
    too challenging„ÄÇit's unsafe at any accuracy that we should never work on it ever„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The truth is though current AI systems are already morally relevant to models„ÄÇit
    may be making you know this kind of yes no decisions explicitly but implicitly
    it's already doing that and sometimes it generates neural tax generation output
    that is morally super explicit and relevant„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so the neural language models are already thereÔºå we cannot really ban it„ÄÇeven
    if US you know government bends it within US US government cannot bend this in
    other countries like Russia„ÄÇso this is already happeningÔºå weve got to do something
    about itÔºå not working on it is an ina„ÄÇwhich is not necessarily more correct thing
    to do than trying to do something about it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Another concern that some people had was that„ÄÇIt's going to empower powerful
    people„ÄÇnot necessarily trueÔºå this is why exactly we have to work on values and
    norms and all these biases addressing biases so that it serves diverse a set of
    people„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_47.png)
  prefs: []
  type: TYPE_NORMAL
- en: So it turns out Delphi is a bit left leaning because crowded workers who work
    for our team tends to be somewhat left to leaning and you know what it means is
    it this„ÄÇby the wayÔºå if we are more left to leaning than our crowd workers you
    think that you know oh my God„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: crowd workers have racism and sexism you know compared to what I believe in
    and then the rightlining people think that oh my God„ÄÇyou know all these walk onnotators
    and what above freedom of speech and this is super divisive unfortunately„ÄÇBut
    the answer is not to do anything about it because as a matter of fact„ÄÇmy passion
    toward addressing racism and sexism came from our experience running for the Alexa
    Priceze challenge„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_49.png)'
  prefs: []
  type: TYPE_IMG
- en: In 2016 and 17„ÄÇso we won a challengeÔºå but here's really sad part of behind it„ÄÇWe
    had a list of thorny cures to avoid that included the skin color or sexual orientation„ÄÇThis
    is a serious form of a discrimination we cannot„ÄÇBuild AI models by having this
    sort of like bend the list to be safe as if they don't exist this was the status
    of you know in 2017„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the challenge remains this year you know not only 2021 but this year as well
    and so we really need to work on racism and sexism„ÄÇbut it turns out all the other
    modal questions share similar challenges so skippped this over„ÄÇbut using Delphi
    we had other follow-up works such as per social dialogue where using Delphi as
    sort of like a foundation common sense model or modal models to make your dialogue
    more socially acceptable and then we also had this other paper where we use Delphi
    in a reinforcement learning agent to learn how to behave better in a game environment
    and so there's a lot more work to be done of course this is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_51.png)'
  prefs: []
  type: TYPE_IMG
- en: little step toward that there's a huge challenge ahead of us really aligning
    AI systems to humans„ÄÇHere's one very quick comment on our new work in progress
    deelphi hybrid where we include neurosymbolic reasoning to address major mistakes
    such as it is genocide if creating jobs this also our early systems a mistake
    it's because our data set doesn't have this kind of weird thatsarial examples
    like genocide ifre creating jobs nobody speaks like that in real life situations
    so our model thought that if creating jobs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this is so positive and then didn't really realize how bad genocide was because
    ready people don't discuss whether they' going to do genocide or not„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_53.png)
  prefs: []
  type: TYPE_NORMAL
- en: Ready people who you know unnotated we unnotated for social chemistry don't
    talk about whether they're going to do genocide or not„ÄÇSo our moral framework
    is basically that of John Rose„ÄÇwhich is descriptive ethics but even John Rose
    in later years and suggested that we needed some topdown mechanism to overcome
    some of the biases that the crowd people might have So this is exactly what we
    are going to do and we drew from Bards gold moral theory framework about what
    not to do„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: definitely you know their basic universal things that everybody might agree
    what's not good to do„ÄÇAnd then what we do is we develop basically a system where
    we parse out the original query into smaller events„ÄÇlike shooting a bearÔºå killing
    a bear to save your child so we parse out the original query into basic event„ÄÇand
    then check through this chromt modelÔºå common sense modelÔºå whether some of these
    events„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: induce obviously negative or dangerous common sense inferences or not„ÄÇand then
    we draw this graph of reasoningÔºå a bit reminiscent of myuric graph in the sense
    that we have a lot of these different reasoning we can do and then they have intelment
    relations or contradiction relations so that we can do collective reasoning on
    top we use again Max is said„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the constrained optimization over it so that we can finally make a more informed
    decision that is both„ÄÇInterpretable and then being able to draw from this common
    sense knowledge to better guard the machine against adversarial examples„ÄÇso the
    performance basically says we can do this without hurting the performance or even
    increasing the performance so as a last comment AI safetyT equity quote moralality„ÄÇthese
    are all sort of like into continuum of challenges„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's really difficult challenges because it's a clear whose moral values so
    do we incorporate I think that we should go with valueluralism going forward to
    really endorse everybody's different culture and individual preferences not just
    to one country one moral framework as the correct one and really we need to do
    more collaboration across AI and humanities even including philosophy and psychology
    and policymakers so I think a step here for„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_55.png)'
  prefs: []
  type: TYPE_IMG
- en: Because I think I'm at time and now I'm ready for questions„ÄÇOhÔºå there's already
    one question I see„ÄÇDo you think legal recordsÔºå criminal case law reflect the kind
    of descriptive morality that you' are interested in capturing„ÄÇDo you think using
    that as training data be usefulÔºå OhÔºå this is an excellent question„ÄÇI think the
    legal records does encode potentially provide really rich resource that if someone
    can really annotate like this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it might be helpfulÔºå we studied with redit cases as just one cent short description
    of a situation because the current language understanding is not strong enough
    to do like a paragraph level precise understanding„ÄÇeven tryGPTÔºå although it looks
    really good at generation my„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_57.png)'
  prefs: []
  type: TYPE_IMG
- en: Take on CheyP so that it's a better at generation than understanding„ÄÇwhich is
    kind of the opposite of how humans areÔºå Human are actually better better for understanding
    than generation„ÄÇSo you can read you Pulitzer Prize winning news article without
    having any problem understanding the article„ÄÇbut you don't necessarily generate
    text that might win the award so but the legal domain is really interesting and
    I think there's some activity research actually even a Stanford there's a this
    of law that goes a step toward that direction and it might really be helpful for
    better understanding what sort of different values people apply in jurisdictions
    and uncovering some biases that some people might have had in the past tris so
    there might be some good use cases in that space„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Next questionÔºå workÔºå Thank you„ÄÇBig picture question„ÄÇcurious to hear your thoughts
    on where do we go from here given larger and larger models coming out„ÄÇSuppose
    we need a model to be 99% correct for specific use case to what extent do I see
    the solution set being that defining the narrow use cases or more data parameters
    orre fine tuning the type of work that I did for mar etcÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Answer is likely it depends yeah but still want to hear about it okay so as
    far as foundation models go„ÄÇit seems that the bigger is the better except that
    you know I was very excited to read a bunch of tech companies papers about foundation
    models in the past six months there's just so many out there so recording story
    there is that well if you have a better data then you can get away with a smaller
    model so especially when you do instruction tuning then you can get away with
    a smaller data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's just still general model but instruction tuning on the larger model might
    even be better it's not the case that you don't gain any performance but it's
    just that you can close the close the gap quite a bit so for downstream use cases
    where typically practitioners want to use a smaller data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: SorryÔºå smaller motel seems that investing more into data is definitely to the
    answer„ÄÇinvesting more into specific algorithm is also reallyÔºå really good because
    algorithm can do a lot„ÄÇLike in this talkÔºå I didn't go too crazy with algorithmic
    solutions about„ÄÇMaybe I'll be similar to the mytic promptingÔºå but in my lab we
    designed a fair amount of decoding time algorithms where you can really close
    the performance gap quite a bit by doing so so that's a good thing though for
    folks in academia because algorithm development feels like more academic or intellectually
    policing then really engineering you know downloading more data from the internet
    and then I don't know cleaning the data because you have to clean the data and
    all these are very engineering heavy whereas decoding time algorithms„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can have a fun inventing some new intellectually interesting thing that
    also improves the performance quite a bit so yeah there's a many different ways
    to improve it but I think data quality matters a lot and algorithm actually matters
    a lot too„ÄÇWhat do I think of Dan Hendri's ethics benchmarkÔºü YeahÔºå so we did use
    that inÔºå let's see„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_59.png)'
  prefs: []
  type: TYPE_IMG
- en: The common sense No banks or draws from this ethics data set„ÄÇWe like the data
    set„ÄÇwe kind of disagree with some of the annotations we foundÔºå but this is very
    typicalÔºå by the way„ÄÇthe thing about morality is that throughout the humanities
    we haven't sorted out yet there's a lot of a theories every theoreticians have
    a different viewpoints„ÄÇand then even like nontheoreticians have a very strong
    opinion about what they want to believe as correct or from wrong„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so„ÄÇThere's that there are different pros and cons„ÄÇthe one thing I learned from
    this experiment is that although some of these data sets seem large„ÄÇso ethics
    has100 thousands of examplesÔºå social chemistry has 300 thousands of judgments„ÄÇSocial
    bioras has 600 thousands of annotations and so forth„ÄÇ And yetÔºå it only covers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I feel like it only covers still the„ÄÇSmall peak of the entire iceberg there's
    a lot on the bottom and humans certainly don't necessarily learn from all these
    examples„ÄÇwe just learn fundamental concepts and then can apply that without this
    larger scale training so there's something really lacking about the way that current
    machine learning is very data heavy but that aside I do think that none of these
    resources are perfect„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they all have different pros and cons and we really need to invest more into
    this„ÄÇespecially from academia because the tech companies right now are not sharing
    any of their human annotation or human feedback data„ÄÇespecially when it's touching
    on toxicity or morality concerns„ÄÇReason being these annotations I'm pretty sure
    are biased and not correct entirely„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and that could really invite additional concerns from the public so they not
    releasing„ÄÇbut in order to really study this betterÔºå we really need to share this
    and then improve it as a community together so that's how I would responded to
    your question Thank you for excellent question„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Do I think this tag is ready to be merged with the searchÔºüI wouldn't say ready„ÄÇbut
    they needed something like this so for sureÔºå you know home devices„ÄÇso the way
    that I think about Delphi is that it can really serve as a filter for other foundation
    models or application scenarios where they're about to generate something and
    you can put a safety filter which can really help so in some sense so in this
    work I went through this super fast„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but here basically what happens is that let's see so the reason why we built
    this is because we found the chatbots„ÄÇthe publicly available ones„ÄÇTend to endorseÔºå
    tend to be too positive to the point that they're going to endorse problematic
    situations like user says Holocaust never happened„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then the chapel says yeahÔºå you knowÔºå I agree with you„ÄÇYou know if you say I'm
    a big fan of a hitler then the chapd might say yeahÔºå yeahÔºå yeah„ÄÇyou know the user
    myself I'm so depressed I'm going to kill myself and then the chapel says go ahead
    a great idea„ÄÇSo being positive is not„ÄÇBeing harmlessÔºå being positive to a problematic
    content can be very toxic and very harmful„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the Delphi you know development like DelphiÔºå even though Delphi is so far
    from being perfect and it's also biased„ÄÇit has a Western bias„ÄÇCould really help
    with the downstream models„ÄÇYeah„ÄÇso continuing on that questionÔºå there has been
    many concerns about using G like models with the search because misinformation„ÄÇwuÔºå
    that's anotherken of worms„ÄÇOthers to say we just need more R RLHF plus knowledge
    graphs„ÄÇSo yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: misinformation isÔºå yeahÔºå something else that seems„ÄÇWe are really lagging behind
    because we don't have a very powerful fact checking models yet„ÄÇSo that's a different
    story„ÄÇBut even that asideÔºå just in terms of norms and ethics that„ÄÇarere safe and
    fair for people to useÔºå I think AdLHF direction is great„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but they usually also need a human demonstrationÔºå not just the human feedback„ÄÇAnd
    again„ÄÇthe problem is that tech companies own them„ÄÇAnd nobody issing anything„ÄÇAnd
    that makes it really difficult to make meaningful progress as a community together„ÄÇso
    I do think that data is really importantÔºå the off shelf models cannot learn modelss
    and ethics on their own„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it has to be somehow taught more directly„ÄÇWe really just need to do more research
    in this space period is how I view it„ÄÇYeah that makes sense„ÄÇwe also have some
    like questions on status so I can ask them for you Yeah folks so one question
    is„ÄÇwhat's the complexity of„ÄÇMay you take promptingÔºå how many times does the LM
    need to bequeriÔºüYeah„ÄÇso honestlyÔºå it's a bit slow„ÄÇIn factÔºå this Addelphi hybrid
    is also slow„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If you try to do this like graph reasoningÔºå OhÔºå thisÔºå maybe I'm not going to
    do that„ÄÇbut the graph reasoning is slow because you have to call„ÄÇYou knowÔºå so
    many times over and over„ÄÇAnd some of these can be batchedÔºå some of these cannot
    be batchedÔºå especially if it's a recursive„ÄÇbut I would say chain of a thought
    is also a bit slower„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the max set solver in itself is pretty fast because this is such an easy graph„ÄÇso
    there's a bit of a delay but its so it's a bit slower but maybe not too bad is
    what I should have said„ÄÇÊ≤°„ÄÇCoolÔºå and the question is„ÄÇLet's seeÔºå how does connector
    compare to G3 if GPT3 is fine tune and common sense data specifically adding some
    sort of like instruction pan„ÄÇYeahÔºå so then the larger wins periodÔºå the larger
    is going to be the better„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: especially if you're going to just fine tune G3 it's a game over„ÄÇSo for that
    reason„ÄÇyou know some folks might think that the larger is always a better„ÄÇtherefore
    don't work on smaller modelÔºå but I think there are two reasons as to why small
    models are interesting to look at as well one„ÄÇempirically it's just easier to
    use but more intellectually„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's also very interesting if you can make smaller models better and catch up
    on the larger model„ÄÇPersonallyÔºå I think there's something about the size„ÄÇthe larger
    model that is more about the information complexity that is the key„ÄÇI don't think
    it's just size in the sense that if you have really a lot of data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but the data is repetative and really simpleÔºå probably you don't get the same
    amount of performance again„ÄÇwhich was basically the case when we looked at this
    output this result where even though„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_61.png)
  prefs: []
  type: TYPE_NORMAL
- en: The loose teacher GPT3 generated a lot more data than the critical teacher here
    the quality of the data was more important than the quantity„ÄÇso I think the complexity
    of the data itself is more important than the size and oftentimes when you just
    increase the size of the data together with the model you do increase the complexity
    of information of the data as well as the model's capability of learning the complexity„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but if we can catch up on that complexity of information either through inference
    algorithms or through better data then we can close the gap quite a bit which
    is intellectually very interesting research space to be„ÄÇokayÔºå this is a personal
    questionÔºå but I will say like humans normally have a like a critic model So it's
    like could' say like thing before you speak so we just like don't generate you
    also like sort of thing it's this is a good thing or a bad thing So people have
    been like like community as a whole has been focusing a lot on like genetic models
    like billion size parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but should we also focus on big sized critic models that can do fact checking
    a lot of this sort of stuff So what's opinion on that„ÄÇüòäÔºåExcellent to point excellent
    yeah I think we can definitely invest more into critic model because they go really
    together well with the generative mod for making the output better or filtering
    output better and yeah there's not as much of investment into that so I really
    like the question„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Our suggestion for the research community is more like it„ÄÇüéºOkay yeahÔºå Ill sayÔºå
    oh let's see„ÄÇyeah you have like some more questions I can do and last„ÄÇüòäÔºåÂóØ„ÄÇLet's
    seeÔºå ohÔºå I guess one is like„ÄÇdo you believe language models should completely
    avoid questions involving morals and ethicÔºüüòä„ÄÇSimilar to like open air restricting
    chatd GpT from giving opinions Yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I actually don't mind at all if AI just avoid evade from all of that except
    when somebody is saying morally questionable things it's also nice for the AI
    not to go with it so„ÄÇOr at least to recognize it as something not okay and then
    try to tone it down but I don't think there's any particular reason why AI should
    actually answer moral questions directly in more downstream use cases„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but really the goal of these Delphi was making all these judgments more explicit
    so that we can actually study it more explicitlyly as opposed to keeping everything
    just so like implicit„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_63.png)
  prefs: []
  type: TYPE_NORMAL
- en: Okay fun question so do you think common sense is a emergent property in like
    classs language models Oh yeah yeah it is definitely emergent as in like when
    we saw this major boost jump in performance with GP3„ÄÇI do believe that it's emergent
    capabilityÔºå but I don't think so this particular evaluation is not very adverial„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: by the way„ÄÇthis is like a sort of like a piece of cakeÔºå you know„ÄÇreasonably
    easy evaluation scenario„ÄÇSo the thing about common senseÔºå thoughÔºå is that it can
    be„ÄÇSo adversarialÔºå so infinitely many different ways and then you know there are
    always people like Gary Marcos who wants to come up with very you know like weird
    weird or text scenarios like you know how crush the porcelain and added to breast
    and milk we can support infant digestive system and then Che Ps3 says a nonsense„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and so the usual problem with the common sense is this adversarial situations
    where people don't have any problem getting fooled by this„ÄÇeven though you know
    UN and I see this for the first time„ÄÇno problem because we have a true conceptual
    understanding that is the backbone of our common sense understanding„ÄÇbut that's
    really lacking in the way that transformers are designed to focus on predicting
    which were the common next as opposed to learning the world the knowledge and
    in some sense you know now with Arll HF instead of predict„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_65.png)'
  prefs: []
  type: TYPE_IMG
- en: which comes nextÔºå we' are trying to align the model output better with human
    preferences„ÄÇbut that again is not really aligned with the different goal of let's
    make sense of the world and then build knowledge models so these are all different
    learning objectives and really that is why I believe that although common sense
    does emerge from language models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: fundamentally language models are not equivalent to knowledge models and we
    really got to focus on building knowledge models„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_67.png)
  prefs: []
  type: TYPE_NORMAL
- en: Make sense„ÄÇCoÔºå I think this's one last zoo question„ÄÇÂîîË°å„ÄÇüòîÔºåO value pluralismÔºå
    yeah„ÄÇIt's an empty concept„ÄÇyou don't want to include all value systems„ÄÇYes„ÄÇso
    maybe it is a is it empty or notÔºå OkayÔºå thank you for excellent question„ÄÇSo I
    believe that we shouldn't endorse conspiracy theories at all or any other know
    morally questionable cases„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but then still there's this thorny situation of what to do withÔºå you know„ÄÇleft
    to left to people versus lightly left to people versus right leaning people if
    USS and then you know every country has some other political division as well„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_69.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_70.png)'
  prefs: []
  type: TYPE_IMG
- en: So here I feel like we really need to sort out what to do with this about regardless
    of this you know some of these challenges„ÄÇit is true that you know I personally
    don't have a religion but I respect the people with the religion and you know
    I respect the people with a different culture background and we kind of have some
    sense of how much do we do we believe that we shouldn respect each other even
    though you know the beliefs are different„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we probably need to work together and it shouldn't be just AI researchers
    making this decision by the way this decision has to come from the humanities
    at large„ÄÇwhich is why the data sharing actually is important about basically I
    think the current version that I have in mind is that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The AI doesn't need to understand what sort of differences are okay differences„ÄÇthe
    fact that people do have differences in certain questions should be learned by
    AI so that there are distribution of opinions as opposed to one correct answer„ÄÇand
    then it should deny some of the„ÄÇControversy theories„ÄÇeven though I'm sure that
    you know some people will be very unhappy about thatÔºå but well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have to decide something like that„ÄÇI am reasonably optimistic that if humanities
    at larger work together„ÄÇwe can do to that because after allÔºå laws I like that
    too lawsÔºå you know„ÄÇThis is a human artifact that people agreed upon somehow that
    there's these core rules that people should abide the by„ÄÇso I'm hoping that we
    can also define universals and particulars and respect particulars whenever it's
    respectable otherwise have some basic universals that reflect you core human values
    and then as far as this left learning situation by the way„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if just the goal is to make your AI systems safe for anybody actually we can
    make AI filter extremely equity aware and it's not going to violate the freedom
    of a speech by doing so just to make AI to avoid the same things that are potentially
    microaggression for some population and you know we still don't really exclude
    people who care more about freedom of a speech over„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Equity by doing so„ÄÇSo I think there are waysÔºå but this really requires a lot
    more research is how I view it„ÄÇÂóØ„ÄÇYeahÔºå I think that's mostly thanks a lot for
    coming„ÄÇthis is a great talk„ÄÇOkayÔºå thank you very much„ÄÇThanks so much„ÄÇ![](img/f6064823a2db16085967ebdd0dffc7e6_72.png)
  prefs: []
  type: TYPE_NORMAL
