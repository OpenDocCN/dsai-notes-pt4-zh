- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P16：16.Common Sense Reasoning - life_code
    - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT/Transformer 原理介绍（中英文双字幕）- P16：16. 常识推理 - life_code - BV1X84y1Q7wV
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_0.png)'
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_1.png)'
- en: Okay， so yeah， I'm super excited to be here and share our recent research about
    neurosymbolic common sense reasoning。So part of the goal of this talk will be
    to address some of the frequently asked questions these days that NLP or common
    sense or whatever it looks like almost solved by ChegPT and I have an existential
    crisis so people do ask me this from time to time so perhaps it's a case of hasty
    generalization。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我非常兴奋能够在这里分享我们关于神经符号常识推理的最新研究。所以这次演讲的部分目标是解决一些当今常被问到的问题，即 NLP 或常识等似乎已被 ChegPT
    解决，而我有一种生存危机，所以人们确实时不时会问我这个问题，或许这是一种草率概括。
- en: especially if we do look at some of the examples so the trophy doesn't fit in
    the brown suitcase because it's too big。what's too big so this is classical Wininograd
    schema challenge problem and here ChegPT answers it correctly that trophy is too
    big so impressive but what if you change the question a little bit then he says
    the trophy itself is too small to fit into the suitcase so it's not very reliable
    at the moment。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是如果我们看一些例子，奖杯不适合放进棕色的手提箱，因为它太大。那么什么太大呢？这就是经典的 Wininograd 模式挑战问题，这里 ChegPT
    正确回答了奖杯太大了，因此令人印象深刻。但如果你稍微改变一下问题，它会说奖杯本身太小，无法放入手提箱，因此目前它并不是非常可靠。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_3.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_3.png)'
- en: So the situation is a little bit like David and Goliath in the sense that the
    bugar appears to be better in many of the cases。although of course some of the
    more careful studies do reveal that smaller models can be better with the better
    data or better reinforcement to learning with the human feedback and whatnot。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这种情况有点像大卫和歌利亚，因为在许多情况下，bugar 似乎更好。虽然当然，一些更仔细的研究确实表明，较小的模型在更好的数据或更好的强化学习以及人类反馈等方面可能更好。
- en: so it's likely that there are still other ways to improve the transformer performances
    by building smaller models in a more clever way。so one way to draw the insight
    is from these classic book known as the art of a war。which of course it says nothing
    about deep neural networks or transformers。but the wisdom here is that no your
    enemyusier battles and innovate your weapons。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，很可能还有其他方法可以通过更巧妙地构建较小的模型来提高 Transformer 的性能。得到的一个洞察是来自于这本经典书籍《战争的艺术》。当然，它并没有提到深度神经网络或
    Transformer，但这里的智慧是要了解你的敌人，改善你的武器。
- en: which we can translate that as。![](img/f6064823a2db16085967ebdd0dffc7e6_5.png)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将其翻译为。![](img/f6064823a2db16085967ebdd0dffc7e6_5.png)
- en: Evaluation with realism and scrutiny and focusing on different types of new
    tasks and leader boards and then innovating your algorithms and data。So in this
    talk I'm going to showcase three such studies and let's dive right in with myic
    prompting。by the way， so the recording theme in this talk will be that smaller
    models can be better and the knowledge is power。so。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 用现实和审视来评估，专注于不同类型的新任务和排行榜，然后创新你的算法和数据。因此，在这次演讲中，我将展示三项这样的研究，让我们直接进入 myic 提示。顺便说一下，这次演讲的主题是较小的模型可以更好，而知识就是力量。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_7.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_7.png)'
- en: Let's start with this observation that language models are sometimes amazing。so
    if you ask G3。if you travel west to far enough from the west coast。you will reach
    to the east coast or not so it says the world is around which is correct so you
    will reach the east coast eventually therefore the answer is true so this looks
    impressive。except to when it's not impressive， so if you ask other questions like
    a butterflies fly with the three wings or not it says it has the four wings。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从这个观察开始：语言模型有时是惊人的。所以如果你问 G3，如果你向西旅行足够远，从西海岸出发，你会到达东海岸吗？它会说世界是圆的，这一点是正确的，所以你最终会到达东海岸，因此答案是正确的。这看起来很令人印象深刻，但在不令人印象深刻的时候又如何呢？所以如果你问其他问题，比如蝴蝶是否用三只翅膀飞，它会说它有四只翅膀。
- en: therefore the statement is a false， but if you read the back what it just said
    as true false questions then it negates what it just said so it can be inconsistent
    with its own statement and then there are many other such inconsistency problems
    so it's not clear what language models do or do not know it's almost like language
    models are some sort of lemons。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，该陈述是错误的，但如果你回顾它刚才所说的作为真和假问题，那么它否定了它刚才所说的内容，所以它可能与自己的陈述不一致，还有许多其他这样的不一致问题，因此不清楚语言模型知道什么或不知道什么，几乎就像语言模型是一种柠檬。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_9.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_9.png)'
- en: Well， it might be cherries if you only pick cherries。but it doesn't make strange
    mistakes so the question is how do we make better lemonade from GPT3 so one approach
    might be to get philosophical and use of Socratess myurtic method that was originally
    developed for addressing humans of law reasoning because it actually turns out
    even humans are not all that logically consistent。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果你只挑选樱桃，它可能是樱桃。但它并不会犯奇怪的错误，所以问题是我们如何从GPT3中提炼更好的柠檬水，因此一个方法可能是从哲学上考虑，使用苏格拉底的迷宫法，这最初是为处理人类的法律推理而开发的，因为实际上，即使是人类也并非完全逻辑一致。
- en: let alone GPT3。So the way it works is this we're going to build the myic inference
    tree and let's use the previous example as a running example。So what we do is
    we ask the following question， providing the answer being true and then let attach
    because。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更不用说GPT3了。它的工作方式是这样的，我们将构建迷宫推理树，让我们用之前的例子作为一个运行示例。因此，我们所做的就是问以下问题，假设答案是真的，然后附加因为。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_11.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_11.png)'
- en: So that we prompted GT3 to continue on this。Sentence， which means it will now
    have to explain。provide the explanation why the answer is true。In this case， the
    explanation is good。So we it's the E of T explanation of the answer being T。We
    ask the same question。switch not true with the false。 and then see what。BS GP3
    might come up with。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们促使GT3继续这个句子，这意味着它现在必须解释，提供答案为什么是真的。在这种情况下，解释很好。因此，我们得到答案为T的E的T的解释。我们问同样的问题，交换不是真的和假的，然后看看BS
    GP3可能会得出什么。
- en: so here is just trying to go with the false as an answer。but it just doesn't
    have a very good answer just as you cannot reach。So now we call this as the E
    of F。So it's an explanation of F answer being F。Now let's see how robust or consistent
    GT3 is with respect to its own explanations。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里只是试图以错误作为答案，但它并没有给出一个很好的答案，就像你无法达到的那样。因此我们称之为E的F。这是对答案为F的F的解释。现在让我们看看GT3在其自身解释方面的稳健性或一致性。
- en: so we read the back E of T and then let GPT3 to decide whether it's going to
    agree or disagree with a label true or false so in this case the last one is negated
    version of E over a t so we insert negation not here and in this case it's good
    that it's a Philipine answer when the statement is negated。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们回顾E的T，然后让GPT3决定它是否同意或不同意标签为真或假，因此在这种情况下，最后一个是E的T的否定版本，因此我们在这里插入否定词，在这种情况下，当陈述被否定时，这是个好结果。
- en: so this is a case when GPT3 is logically integral to E of T。For E ofva Fodo。which
    was basically a Bogus explanation for the wrong answer。it's not able to flip its
    own labeling， which means a GPT3 is not logically integral。so that's good， GT3
    does know something strange about its own explanation given previously。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这是一个案例，当GPT3在逻辑上是E的T的组成部分。对于E的Fodo，这基本上是错误答案的虚假解释。它无法翻转自己的标签，这意味着GPT3在逻辑上不是完整的。因此，GT3确实知道关于它之前给出的解释有些奇怪的事情。
- en: And so we can keep doing this recursively to let to make G3 to explain its own
    explanation of explanation recursively。so we build this myoric tree or graph。![](img/f6064823a2db16085967ebdd0dffc7e6_13.png)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以递归地继续这样做，让G3解释它自己对解释的解释。因此，我们构建了这个迷宫树或图。![](img/f6064823a2db16085967ebdd0dffc7e6_13.png)
- en: For some time and then only keep branches that are logical integral throwing
    out the non integral part for now。but even after chopping the branches where there's
    logical inconsistencies。GPT3 being GPT3 the tree will still have some inconsistent
    explanations so in order to improve the logical consististency now what we do。Is
    we are going to look at pairwise consistency among any of the nodes so we compute
    sorry stepping back。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一段时间后，只保留逻辑完整的分支，暂时丢弃非完整的部分。但即使在削减掉逻辑不一致的分支后，GPT3仍然会有一些不一致的解释。因此，为了提高逻辑一致性，我们将查看任何节点之间的成对一致性，因此我们计算对不起，稍微退一步。
- en: we are going to first compute the nodewise confidenced。So we call that as a
    belief and it's defined by this particular equation that basically it looks at
    different conditional probabilities and then compute its ratio to see how confident
    it is for any particular node。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先计算逐节点的置信度。所以我们称之为信念，它由这个特定的方程定义，基本上它查看不同的条件概率，然后计算其比率，以查看它对任何特定节点的置信度。
- en: we then also look at the edge or pairwise consistency by using off the shelf
    natural language inference models output。whether a pair is contradiction or not，
    so we then create this pairwise weights。Now。once you have all of this， then。We
    can formulate a constrained optimization problem where the inference objective
    is to assign some label。either true or false on each of the nodes such that it's
    going to maximize the weight assigned to all of these node and edges。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还查看了通过使用现成的自然语言推理模型输出的边缘或成对一致性，判断一对是否矛盾，然后我们创建了这个成对权重。现在，一旦你有了这一切，我们可以制定一个约束优化问题，其中推理目标是在每个节点上分配某个标签，真或假，以最大化分配给所有节点和边的权重。
- en: So sometimes the labeling will have to flip the original label that Mom might
    have a prefer to give because that way you can enhance the graph level consistency
    so you can solve this with any mix set。so set means satisfiability。And this is
    a classical AI search algorithm。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有时标记需要翻转妈妈可能更愿意给出的原始标签，这样你可以增强图表的级别一致性，因此你可以用任何混合集来解决这个问题。这里的集合意味着可满足性。这是一个经典的人工智能搜索算法。
- en: and we used this particular solver， but you can use many others and。So here
    the final output is that the original answer to the original question should be
    true。and then it also gives you nodewise per node label assignment as well。So
    what does this mean in the end in terms of empirical result， so when tested on
    common sense QA 2。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了这个特定的求解器，但你可以使用许多其他求解器。因此，最终输出是原始问题的答案应该为真。它还给出了每个节点的逐节点标签分配。那么从经验结果来看，这最终意味着什么呢，当在常识问答2上进行测试时。
- en: 0。The canonical prompting， so green used on top of G3。so it's basically a fus
    prompting on G3 will give you a bit better than chance performances。so this is
    true false QA data set so your chance level is 50 and GP3 is barely better than
    chance。but recently there have been some ideas such as chain of thoughts or self-consistency
    there can improve the vanilla prompting method considerably so if you use such
    variations then you get performance gain。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 0。规范提示，绿色用于G3的顶部。所以在G3上进行基本的提示会比机会表现稍好。这是一个真假问答数据集，所以你的机会水平是50，而GP3仅比机会稍好。但最近出现了一些想法，比如思维链或自我一致性，这些可以显著改善普通提示方法，因此如果你使用这样的变体，那么你会获得性能提升。
- en: Now， the purple is the。A different variant of it， but together they're all doing
    worse than myic prompting。which in fact does better than supervise the model trained
    on T5。Usually supervised model trend on T5 is a hard to beat using GT3 few shot。but
    basically this is inference time on the algorithm practically unsupervised and
    it does well on that and similarly we see large boost when tested on other common
    sense benchmarks such as C or come to sense。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，紫色是它的另一种变体，但它们一起都比我的提示做得差，实际上比基于T5训练的监督模型表现更好。通常，基于T5训练的监督模型很难被GT3的少量样本击败。但基本上，这是在算法上的推理时间，实际上是无监督的，并且在这方面表现良好。同样，当在其他常识基准上测试时，比如C或常识，测试结果也有很大提升。
- en: so what this tells us is that although。The emergent capabilities of large transformers
    are phenomenal。They can be not very robust for some of these common sense challenges。and
    it's in large part due to the logical inconsistencies。which can be dramatically
    enhanced when you do this sort of symbolic reasoning on top。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这告诉我们，尽管大型变换器的突现能力是惊人的，但它们在某些常识挑战中可能并不太稳健。这在很大程度上是由于逻辑不一致，当你进行这种符号推理时，这种不一致会显著增强。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_15.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_15.png)'
- en: So yeah， not only so practice is a method that help with flawed， the human reasoning。it
    can also dramatically enhance the flawed neural networks's reasoning。![](img/f6064823a2db16085967ebdd0dffc7e6_17.png)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，是的，实践不仅是一种帮助解决人类推理缺陷的方法，还可以显著增强缺陷神经网络的推理能力。![](img/f6064823a2db16085967ebdd0dffc7e6_17.png)
- en: Okay， so moving to the next topic， symbolic knowledge dissolation。![](img/f6064823a2db16085967ebdd0dffc7e6_19.png)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，接下来我们讨论符号知识的溶解。![](img/f6064823a2db16085967ebdd0dffc7e6_19.png)
- en: So this work is a work that tries to convert general language models on top
    of transformers to causal common sense models。also transformers。![](img/f6064823a2db16085967ebdd0dffc7e6_21.png)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这项工作试图将基于变压器的一般语言模型转换为因果常识模型。还有变压器。![](img/f6064823a2db16085967ebdd0dffc7e6_21.png)
- en: And the reason why we might want to worry about common sense models is because
    despite。Han level or even superhuman level performances on a variety of leader
    board。the state of the art models are brittle when given adversarial or out of
    domain examples。so transformers can make seemingly strange mistakes。And so solving。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能想要担心常识模型的原因是，尽管在各种排行榜上具有人类水平甚至超人类水平的表现，但最先进的模型在面对对抗性或超出领域的示例时却很脆弱。因此，变压器可能会犯出看似奇怪的错误。因此，解决这个问题。
- en: it's almost like solving only a data set without really solving the underlying
    task and this phenomenon sometimes is described as a systematic generalization
    problem and why does this happen is that unlike humans who truly learn about how
    the world works conceptually。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎就像只是解决一个数据集，而并没有真正解决底层任务，这种现象有时被描述为系统性概化问题。为什么会这样呢？因为与人类真正概念性地学习世界运作方式不同。
- en: transformers learn，Sort of a surface patterns in language or images that are
    powerful for many downstream use cases。but still not really robust understanding
    of the concepts and how the world works so in order to bridge this gap。we can
    really think about this challenge of learning。acquiring common sense capabilities
    for machines。 So the operational definition of a common sense in this talk will
    be that it's the basic level of practical knowledge and reasoning concerning everyday
    situations and events that are commonly shared among the most people。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 变压器学习语言或图像中的表面模式，这对许多下游用例是强大的，但仍然没有真正对概念及其如何运作的稳健理解。因此，为了弥补这个差距，我们可以真正考虑学习的挑战，为机器获取常识能力。因此，本次演讲中常识的操作性定义将是，针对日常情况和事件的基本实用知识和推理，这是大多数人共享的。
- en: This is really important the last part that's commonly shared among the most
    people but it's not the case that is shared by everybody in the universe。Because
    the additional context it can always change what is commonsensical for any given
    culture or situation。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后这一部分真的很重要，那就是大多数人共享，但并不是宇宙中所有人都共享的。因为额外的上下文总是可以改变在任何给定文化或情况下什么是常识。
- en: so for example， in general， you and I probably agree that it's okay to keep
    the closed door open but it's not okay to keep the fridgegi door open because
    the food inside might go bad so these are general rules of the thumbs that we
    might or by the by。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 所以例如，一般来说，你我可能都同意保持关着的门打开是可以的，但保持冰箱门打开就不太行，因为里面的食物可能会变坏。这些是我们可能会遵循的一般经验法则。
- en: but you know of course， if you go to your friend's house。you might behave a
    little bit and you know keep their closed doors open sorry closed and then as
    far as a fridgegi door if you're in a store and it's not really hooked up to the
    wall。then it doesn't matter when whether the fridge door is open or not because
    there's no food inside and you know you can come up with in many situation in
    which these basic rules of the thumbs will have exceptions so。That is the key
    challenge of common sense because it's not universal knowledge。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你知道，当然，如果你去朋友家，你可能会表现得稍微得体一点，把他们关着的门保持关闭。而冰箱门如果在商店里且并没有真的连接到墙上，那么冰箱门是否打开就无关紧要，因为里面没有食物。你知道，你可以想出很多情况，这些基本的经验法则会有例外。因此，这就是常识的关键挑战，因为它并不是普遍的知识。
- en: but it's sort of like shared across a large population of people。Okay， so it's
    essential。such common sense is essential for humans to live and interact with
    each other in a reasonable and safe way。and so as AI becomes increasingly more
    important aspect of human lives。and with ChaGPT more likely so， it's good if AI
    can understand human needs and actions and values better。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，它在很大程度上是在人群中共享的。好的，所以这种常识对于人类在合理和安全的方式中生活和互动是至关重要的。而随着人工智能在我们生活中越来越重要，像ChaGPT这样的工具可能更有可能，因此如果人工智能能更好地理解人类的需求、行为和价值观，那是很好的。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_23.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_23.png)'
- en: So the premise of this talk is that language models are not equivalent to knowledge
    models。even though language models today do acquire a great deal of knowledge
    but they're not equivalent。so we developed symbolic common sense knowledge graph
    known as atomic a few years ago。four years ago now， as well as neural common sense
    model built on top of or trained using atomic as the source of training。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这次演讲的前提是，语言模型并不等同于知识模型。尽管当今的语言模型确实获得了大量知识，但它们并不等同。因此，我们几年前开发了一个被称为原子（atomic）的符号常识知识图谱。四年前，现在有了基于原子作为训练源构建或训练的神经常识模型。
- en: fine tuning of the shelf language models。until two years ago。this atomic was
    fully crowd sourced by humans。Which in this talk I'm going to lift。but at first
    the almost that these all has to be human criceors so you can consider almost
    atomic as human demonstration in the current version of CheGPT。you can consider
    this as human demonstrations of common sense inferences。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 对现有语言模型的微调。直到两年前，这个原子完全是由人众包的。在这次演讲中我将提到这一点，但最初几乎所有这些都必须由人来创造，所以你可以把原子视为人类在当前版本的CheGPT中的示范。你可以把这视为人类对常识推理的示范。
- en: And we had this comet atomic 2020， which is enhanced the version of atomic end
    thecomt， again。atomic portionion was fully crowdtosourced by humans in 2021。So
    let me give you a bit of。![](img/f6064823a2db16085967ebdd0dffc7e6_25.png)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有这个彗星原子2020，这是原子的增强版本，再次提到。原子部分在2021年是完全由人众包的。所以让我给你一点信息。![](img/f6064823a2db16085967ebdd0dffc7e6_25.png)
- en: A sample of what atomic 2020 looks like。So imagine a situation where x gets
    x is car repaired or you get your car repaired。So。Immediately you can imagine
    what's likely be true or relevant for the situation that as a result you might
    want to call Uber or Ly for right as a result you need to pay the bill beforehand
    you need a mechanic and money to repair your cars so these are basically preconditions
    and post conditionsditions of that event so some of these atomic knowledge graph
    is about social interaction knowledge about event and then other parts of the
    atomic is physical andcentric knowledge。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 原子2020的一个示例。所以想象一下一个情况，其中x修理了他的车，或者你修理了你的车。所以。你可以立刻想象出这个情况下可能真实或相关的内容，因此你可能想叫Uber或Ly。因此，你需要提前支付账单，需要一个机械师和修理车的钱，所以这些基本上是该事件的前置条件和后置条件。因此，这些原子知识图谱的一些内容与事件的社交互动知识有关，而原子的其他部分则与物理和中心知识有关。
- en: so money is typically used for paying repairs but if you really want it。you
    can fold the data into origami I've never done it。but these are examples of stereotypical
    use cases as well as nonsterereotypical but affordable actions that you can apply
    to objects so it requires na physics understanding about the affordances of a
    physical object。た？And then we can also reason about counterfactual condition in
    which the center event cannot happen so can be hindered by that so if you total
    your car completely then it's impossible to get your cars repaired and then there
    are like events that typically happens before and after so some of these knowledge
    is eventcentric。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所以钱通常用于支付维修，但如果你真的想要的话。你可以把数据折叠成折纸。我从未尝试过，但这些都是典型用例和非典型但可负担的行动示例，可以应用于物体，所以这需要对物理对象的可供性有一定的物理理解。た？然后我们还可以推理关于反事实条件的情况，在这种情况下，中心事件无法发生，因此可能会受到阻碍。如果你的车完全报废，那么就不可能修理你的车，通常在此之前和之后会发生一些事件，所以这些知识是以事件为中心的。
- en: So we crowdsourced a fair amount over the course of， I don't know， maybe two
    years or so。Up to 1。3 million if than rules or E than knowledge。Over 23 different
    a types or relation types。嗯。So it was a fully crowded sourced and so the knowledge
    graph is useful for training transformers。And here， let's see the comparison between
    Comt that was built on Bart compared to GT3。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这两年的过程中，我们众包了相当多的内容。我不知道，可能是两年左右。最高达到130万条规则或E知识。涉及23种不同的关系类型。嗯。所以这是一个完全众包的知识图谱，对训练变压器非常有用。在这里，让我们看看基于Bart构建的Comt与GT3之间的比较。
- en: which is so large it doesn't even fit into the slide。It was more than 400 times
    larger than partt。So with that in mind if you look at this accuracy judged by
    humans after making the common sense model making some common sense inferences。so
    the task is that given a node which describes a situation or event and then given
    an edge type which sort of narrows down the common sense relation or inference
    type。you're now going to generate some inference， so it's a generative task and
    then we ask humans whether the common sense inference seems reasonable or not。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型大到连幻灯片都放不下，超过了partt的400倍。因此考虑到这一点，如果你看人类在进行常识推理后判断的准确性，任务是给定一个描述情况或事件的节点，以及一个边缘类型，以缩小常识关系或推理类型的范围。你将生成一些推理，这是一个生成任务，然后我们询问人类这些常识推理是否合理。
- en: so 100% is the desired level，Comade is substantially better than GPT3。which
    is really impressively better than GPT2， it's not Apple to Apple because GPT2
    is a zero shot GPT3 is a few shot。but still it's interesting the large jump that
    scale alone brought to GPT3。So but still GPT3 is too large to be useful for actual
    system building for most engineers and scientists in the world。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以100%是期望的水平，Comade的性能远超GPT3，而GPT3又比GPT2有显著提升。尽管不能完全比较，因为GPT2是零样本，GPT3是少样本，但仅从规模来看，GPT3的提升也很有趣。然而，对于世界上大多数工程师和科学家而言，GPT3的规模过于庞大，无法用于实际系统构建。
- en: so it's nice to have a smaller model that does it do even better。![](img/f6064823a2db16085967ebdd0dffc7e6_27.png)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，拥有一个更小的模型并且表现更好是很好的！[](img/f6064823a2db16085967ebdd0dffc7e6_27.png)
- en: And so when we put these resources out， people all around the globe did some
    creative research using it。so persona aware conversations or figurative language
    understanding。sStorytelling and fantasy gaming and interactive learning enhancement
    in all of these works。people came up with some useful use cases using either Kot
    or atomic or both as some kind of common sense backbone for their downstream use
    cases。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些资源发布后，全球各地的人们利用这些资源进行了创造性的研究，包括个性化对话、比喻语言理解、故事讲述、幻想游戏以及互动学习的增强。在所有这些工作中，人们提出了一些有用的用例，使用Kot或atomic或两者作为某种常识基础，以便于他们的下游用例。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_29.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_29.png)'
- en: But the applications are still limited by the coverage and quality of this common
    sense model so we wanted to make it better。but we were hitting a bit of a limit
    with human crowd sosourcing。So now in this paper。symbolic knowledge distillation
    we're going to do AI generated knowledge graph by introducing this notion symbolic
    knowledge distillation。so we want to take this GP3， which is very impressive about
    too large。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 不过，这些应用仍然受限于常识模型的覆盖和质量，因此我们希望改进它。但在进行人群外包时，我们遇到了一些限制。因此在这篇论文中，通过引入符号知识蒸馏的概念，我们将生成AI知识图谱，目的是将这个非常令人印象深刻但过于庞大的GPT3缩小。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_31.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_31.png)'
- en: So make it smaller， but better than G 3。So G3 was about 73% of good and it's。Good。but
    not good enough for empirical use cases。Now， is that even possible though。because
    when you normally do knowledge distillation， you get smaller and worse models。not
    better models？So the reason why this could work is because。嗯。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是使其更小，但比G3更好。G3的表现大约是73%，虽然不错，但对于实证用例来说还不够好。那么，这是否可能呢？因为通常进行知识蒸馏时，会得到更小但更差的模型，而不是更好的模型。之所以可能是因为……
- en: The reason why this could work is because the symbolic knowledge dist has a
    disonel that's convoluted and it has a critical insight that really helps the
    student to model to be smaller but better。so slightly more formally knowledge
    distillation due to hintnetl 2015 is a method to distill teacher model down to
    student to model by optimizing this crossenttropy between the teacher' probability
    distribution over the label spaceway。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 之所以可能是因为符号知识蒸馏具有复杂的结构，并且有一个关键的洞察，真正帮助学生模型变得更小但更好。稍微正式一些，知识蒸馏是2015年由hintnet提出的一种方法，通过优化教师模型在标签空间上的概率分布与学生模型之间的交叉熵，将教师模型蒸馏到学生模型中。
- en: Output y， and then the student distribution over the same output y。In the original
    work。the output space was just classification。Knowledge this delevation was done
    for classification task。in which case it's a simple enumeration that leads to
    the correct assumption。but in our case y can be a sentence which is intractable
    because there can be exponentially many such output so what people do well you
    know no problem we always just sample and call it a day。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: so we are going to sample and so that we just compute the expectation through
    samples。And the byproduct of that samples will be a symbolic knowledge graph。And
    that's because the strings coming out of this sampling can be connected together
    into graph structure if we want it。So。In terms of the quality of the generated
    knowledge。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: so let's compare human written knowledge versus GT3 authored knowledge。Here
    the y axis shows the quantity in millions， so atomic 2020。the human return knowledge
    is less than a million in this particular case。in terms of the number of knowledge
    because we only in this study。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: we only look at a subset of atomic 2020 relation types that corresponds to causal
    common sense knowledge common sense causal common sense reasoning so。It's less
    than a million for that subset and then if we look at G3's generation we can generate
    a lot so we can generate almost 7 million of them but here black Perian is noisy
    Perian and green Per is a good Persian and you see because GP3 is only about 70%
    to good like 30% or all garbage so it's a larger scale lower accuracy at this
    point compared to human return resource so now what we do is we train this critic
    model and we use a robota for simplicity and this is a supervised model on a moderate
    size labeled the data about 10。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 000 or so and it's a binary classification task where whether the machine generated
    the knowledge looks correct or not。And this robota is not a very good model because
    if so if it's a perfect。we would have a solved the common sense problem altogether。So
    the critic tries to throw out that stuff。 And we can use it the critic very aggressively
    with a high threshold。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: So whenever something is slightly suspicioususpicious， just throw that out。But
    if we use it aggressively。 So we throw out most of the black。that's good。together
    with a lot of green stuff。but still the remainder is much larger than what humans
    ever beaten。And yet we can actually retain higher accuracy than human authored
    resources So here the teacher is a basically called a combination between GPT3。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: which is in some sense loose teacher and then combined with the critic Roberta
    which serves as critic teacher Okay so that's the generated knowledge now how
    helpful are they for the purpose of training downstream neural common sense models。So
    recall that the GT3 without doing anything else is a loose teacher whose common
    sense inference is only about 73% good。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: So you see here it's accuracy of its output and then it turns out if we use
    loose teacher as a teacher directly to teach a student model。then the performance
    already goes up on its own so this is interesting that usually this is not the
    case with the knowledge distillation。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以看到这里的输出准确性，结果发现如果我们直接使用宽松老师来教学生模型，那么性能已经会自行提高，这很有趣，通常在知识蒸馏中并不是这样的情况。
- en: but when we focus on common sense knowledge distillation。student just on its
    own becomes better so unlike typical knowledge distillation where we start with
    language model and we end with language model。students and teacher of the same
    type here the original teacher was actually language model。not common sense model，
    and then we want the student model to be more of the common sense model so there's
    a switch of the type。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 但是当我们专注于常识知识蒸馏时，学生在自我学习中变得更好，所以与典型的知识蒸馏不同，我们从语言模型开始，最终也得到语言模型。在这里，学生和老师是同类型的，原始老师实际上是语言模型，而不是常识模型。然后我们希望学生模型更多地成为常识模型，因此类型发生了转换。
- en: In teacher and student， and so when that's the case， whether this is generally
    true， we don't know。but this is what we found empirically。嗯。Ohh， should I pay
    attention to the questions or not？yeah。feel free to any relevant questions hang
    on， let me quickly check。呃。Yeah。sample oh sample is generated output， which happens
    to be usually a sentence or phrase。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在老师和学生的关系中，当情况如此时，这是否通常成立我们并不知道，但这是我们经验上发现的。嗯。哦，我应该关注问题吗？是的。随意提问，我快速查看一下。呃。是的。示例哦，示例是生成的输出，通常是一个句子或短语。
- en: that's what we what I meant by sample， sorry that I didn't see that earlier
    and then the last question。Having the model generate text to one symbol at a time，
    starting from the target label sentence， yes。it's because transformer can only
    generate one word one token at a time， that's what we do。As well here。 Thank you
    for the clarification questions。 All right， so back to here。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我所说的示例，抱歉之前没看到。然后最后一个问题。让模型逐个符号生成文本，从目标标签句子开始，是的。这是因为变压器每次只能生成一个单词或一个符号，这就是我们在这里所做的。感谢你的澄清问题。好的，那么回到这里。
- en: In our earlier study， commma 2020， if we train GT to or bart using。Human author
    graph knowledge graph atomic then the performance was a bit better than 80% Now
    finally when we use basically a combination of GPT3 and Roberta together。we found
    that the downstream performance of the neural causal reasoning is reaching close
    to 90% for the first time so the takeaway here is that critical teacher resulting
    better student compared to loose teacher。It's not the quantity of knowledge because
    loose teacher basically has more data。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的研究中，commma 2020，如果我们使用人类作者图知识图原子来训练GT或bart，那么性能稍微超过80%。现在最终当我们基本上将GPT3和Roberta结合在一起时，我们发现神经因果推理的下游性能首次接近90%。所以这里的结论是，关键老师能够培养出比宽松老师更好的学生。这不是知识的数量，因为宽松老师实际上拥有更多的数据。
- en: you one might wonder whether more data is always better for the purpose of common
    sense models。but that's another case loose teacher can generate more data。but
    the resulting student to model is not as good as the case when the critical teacher
    which has less data because you throw out most of your generation。It's a smaller
    data， but it leads to better model， so that's sort of takeaway messages here。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道更多的数据是否总是对常识模型有利，但这又是另一种情况，宽松老师可以生成更多数据，但结果学生模型并不如关键老师生成的数据少时的效果好，因为你丢弃了大部分生成内容。数据更少，但能导致更好的模型，这就是这里的主要信息。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_33.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_33.png)'
- en: 嗯。So to summarize we were very surprised by this outcome that at least with
    respect to a subset of the original atomic 2020。it's a subset corresponding to
    causal common sense reasoning。we found it to our big surprise that machine author
    the knowledge graph can be for the first time better than human author the knowledge
    graph in all criteria。scale accuracy and diversity。We also measure the diversity
    in many different ways。 here。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。所以总结一下，我们对这个结果感到非常惊讶，至少就原始原子2020的一个子集而言。这个子集对应于因果常识推理。我们惊讶地发现，机器作者的知识图在所有标准上，首次能够优于人类作者的知识图，包括规模、准确性和多样性。我们还以多种不同方式测量了多样性。
- en: I just show you unique uniogram counts， but we in the paper， we report other
    measures as well。So it's not the case that。GPT3 is being reetative。it's actually
    being more creative in some sense than human crowd workers while being able to
    enhance other aspects as well。By the way， these enhancements are sort of like
    you kind of have to balance out depending on what you prioritize。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是给你展示独特的单词计数，但在论文中我们也报告了其他指标。所以并不是说GPT3在重复，而是在某种意义上比人类众包工作者更具创造性，同时也能增强其他方面。顺便提一下，这些增强效果就像是你需要根据优先事项进行平衡。
- en: you cannot actually get all of this simultaneously。so I'm just showing the best
    case and scenario here。All right so that's the symbolic knowledge dissillation
    part we actually have a followup work on this on several different application
    scenarios even including summarization where we distill summarization capabilities
    from GT3 and demonstrate that GPT2 can work as well as GPT3 or even better for
    summarization task and then we also have other work where we can distill from
    smaller models but I don't have the content in this talk so but just wanted to
    mention that this particular technique despite its a simplicity we found that
    empirically works really really well across several different downstream use cases。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上不能同时获取所有这些。所以我只是在这里展示最佳情况和场景。好的，这就是符号知识蒸馏部分，我们实际上在这方面有后续工作，涵盖几个不同的应用场景，甚至包括总结，我们从GT3中提炼总结能力，并展示GPT2在总结任务上可以与GPT3一样好，甚至更好。然后我们还有其他工作，可以从更小的模型中提炼，但我在这个演讲中没有相关内容，只想提一下，这种特定技术尽管简单，我们发现它在几个不同的下游用例中效果非常好。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_35.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_35.png)'
- en: Okay， so finally， I'll move to the common sense morality。So this is still on
    archive。I'll tell you why that's the case， but so。![](img/f6064823a2db16085967ebdd0dffc7e6_37.png)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，最后，我将转向常识道德。所以这仍在档案中。我会告诉你原因，但所以。![](img/f6064823a2db16085967ebdd0dffc7e6_37.png)
- en: We have a new version available and then new new version will come soon。So the
    motivation behind this work is that language models。Are already making judgments
    or output that has moral implications。Even if you don't care about morality by
    working on language models。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个新版本可用，新的新版本也会很快推出。因此，这项工作的动机是语言模型已经在做出具有道德影响的判断或输出。即使你不在意道德，处理语言模型的工作也在影响。
- en: you're implicitly dealing with the moral models。So especially that given this
    widespread deployment amount of language models we do need to worry about it so
    here's a web demo you can play with you might have seen this already really this
    is still a research prototype only it still its work in progress we're still working
    on it so please keep that in mind but if you haven't seen it before you can handle
    reform QA such as it is killing a bear it's wrong killing a bear to save your
    child it's okay。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你在潜移默化中处理道德模型。因此，考虑到语言模型的广泛部署，我们确实需要担心这个。所以这里有一个网络演示，你可以尝试一下，你可能已经见过了，实际上这仍然是一个研究原型，仍在进行中，我们还在努力，所以请记住这一点。如果你以前没有见过，你可以处理诸如“杀死一只熊是错的，拯救孩子杀死熊是对的”这样的问答。
- en: Maybe to save your child it sounds really positive so how about to please your
    child which is also positive。but then Delphi says it's wrong Finally， oh maybe
    this is all about saving your child so how about exploding a nuclear bomb to save
    your child and then he says it's okay。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 也许为了拯救你的孩子，这听起来非常积极，那么为了取悦你的孩子，这也是积极的。但德尔菲说这是错的。最后，哦，也许这一切都是关于拯救你的孩子，那么如何用核弹拯救你的孩子，他就说这没问题。
- en: Sorry it's wrong so as you can see moral decision making requires weighing different
    values that are potentially at us and then see which one you need to favor more
    so for that reason in our original version we also study the relative QA mode
    where you can compare to situation like stabbing someone with a cheeseburger。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，这是错的。所以正如你所看到的，道德决策需要权衡可能会影响的不同价值观，然后看看你需要更倾向于哪个。因此，出于这个原因，在我们的原始版本中，我们还研究了相对问答模式，你可以将情况与用芝士汉堡刺伤某人进行比较。
- en: Compared to stepping someone over a Chesburger， this is a super tricky question
    because it requires both a knifeive of physics knowledge that。Steping someone
    using a chibo browser as a tool。😮，It's not going to harm anybody physically because
    cheeseburger is too soft。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 与用芝士汉堡踩踏某人相比，这是一个超级棘手的问题，因为它需要一定的物理知识。使用芝士汉堡作为工具的步骤😮，这不会对任何人造成身体伤害，因为芝士汉堡太软了。
- en: You cannot really。Injure somebody using cheeseburger。It is such a rude thing
    to do。but you cannot injure somebody， whereas studying someone over or cheeseburger
    means that you're using the default tool of a stabbing。which is a knife because
    you didn't mention it。 There's linguistic common sense that you're using the default
    tool。Human， by the way， omit this argument all the time。 So this is a fairly complex
    question to answer。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能真的用汉堡伤害别人。这是非常无礼的，但你确实不能用汉堡伤害某人，而用汉堡攻击某人意味着你使用了默认的攻击工具，即刀，因为你并没有提到。这涉及到语言的常识，默认工具的使用。顺便提一下，人类总是忽略这一点。因此，这是一个相当复杂的问题。
- en: Finally， you can also ask yes no questions such as it's okay to fire someone
    because they're gay or not。it says， no， it's not okay。We found that it's。😊，Surprising
    robust against the composition of situations is so mowing the lawn is as it' expected
    late at night。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你还可以询问是非问题，例如“解雇某人因为他们是同性恋是否合适”。回答是“不，这不合适”。我们发现，这在情境组合上出奇地稳健，比如说，在深夜修剪草坪是可以接受的。
- en: it's rude。 If you live in the middle of nowhere， then it's okay。Ignoring a phone
    call。it's rude or non phone call。 That's okay。from my friend， it's rude。But what
    if I just ahead a fight with them。then it's okay to ignore or understandable。during
    my work hours， it's okay to ignore outside my working hours， it's rude。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略电话是无礼的。如果你住在偏远地区，那就没问题。忽略一个电话也是无礼的，但如果是来自朋友的电话，那就不一样了。但如果我刚和他们吵过架，那忽略电话就是可以理解的。在我的工作时间内，忽略电话是可以的，工作时间外就显得无礼。
- en: But what if it's my bosses phone call during my work hours， then it's wrong。you
    should answer it。except if I'm in a meeting， then it's okay to ignore even if
    a boss is call。So you see how。It gets really nest and compositional very， very
    fast。So that's the real challenge behind moral decision making。Due to the nature
    of language models。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果是我老板在工作时间内打来的电话，那就不对了，你应该接。但如果我在开会，即使是老板打来的电话，也可以忽略。因此你可以看到，这变得非常复杂，并且迅速构成了真正的道德决策挑战，这是由于语言模型的性质。
- en: some of these common sense knowledge leaks into the model。so mixing bleach with
    ammonia that's a dangerous drinking milk if I'm lactose in tola don't it's wrong。but
    soy milker that's okay。By the way， this common sense leakage is actually a good
    thing in terms of AI safety because some of these harmful or even dangerous textile
    output requires some common sense understanding about what's good and not good
    to suggest to humans so for。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常识性知识渗透进模型中。例如，将漂白剂和氨水混合是危险的；如果我对乳糖不耐受，喝牛奶是不对的，但豆奶就没问题。顺便说一下，这种常识的泄漏在AI安全方面实际上是件好事，因为一些有害或甚至危险的文本输出需要对什么是好的、什么是不好的有一定的常识理解，才能建议给人类。
- en: The laboratory experiments， meaning we just divide our data set into training
    and test。we found that deelphi can at least for the the data set that we have
    I'm going to tell you about it in a bit。but performance is pretty strong compared
    to GP3 As you see zero shot is pretty bad。it's barely better than chance。Which
    means that。Off the shelf neural language models don't really have a good sense
    of imoral judgments。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 实验室实验意味着我们只是将数据集分为训练和测试。我们发现，Deelphi至少在我们拥有的数据集上表现出色，我稍后会告诉你，但其性能与GP3相比相当强劲。你可以看到，零样本的表现相当糟糕，几乎与随机选择相当。这意味着现成的神经语言模型并没有对道德判断有很好的理解。
- en: but if you give a shot like any other task， it does pick up the knowledge quite
    fast so that there's nothing new about it。but to close the gap to ideal human
    level， it's good to do more supervised learning of course。So the data set is common
    sense Nombank， it includes 1。7 million people's ethical judgments on everyday
    situations and it includes cultural norms。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你给它一个尝试，就像其他任务一样，它能很快掌握知识，所以这并没有什么新鲜的。但要缩小与理想人类水平的差距，当然进行更多的监督学习是好的。这个数据集是常识性Nombank，它包括170万人的日常情境伦理判断，并涵盖文化规范。
- en: the social norms and ethical norms altogether more specifically were from these
    five existing data sets that were not designed originally for QA but we automatically
    compel these resources into the QA form of the five what actually does matter
    the most are these two social chemistry which I'm going to talk about in a bit
    and then social bias frame and this is what teaches the model against racism and
    sexism。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 社会规范和伦理规范主要来自这五个原本并非为了问答设计的数据集，但我们自动将这些资源整合成问答形式。实际上，最重要的有两个方面：社会化学，我稍后会讲到，以及社会偏见框架，这教会模型如何抵制种族主义和性别歧视。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_39.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: And so social chemistry super briefly， I'll tell you what this is。So GT3's morality，
    like I said。is somewhat dubious if you use it off the shelf。So if you let it explain
    running a lenderer at 5 AM is rude because that that you might say you can wake
    up the entire neighborhood。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: you can only do it if you're making a thick smoothie and need to incorporate
    some ice。So it's a funny haha no harm is made， but if you prompt it with other
    kinds of prompt like it's okay to post a fake news。![](img/f6064823a2db16085967ebdd0dffc7e6_41.png)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: If it's in the interest of the people then it's okay or ROP agenda， then it's
    okay。even if it hurts to the country， so it's all understandable given how it's
    trained on what humans have said。so humans out there did say that morally questionable
    text so that language models pick up on that and then amplify it。So we do need
    to teach AI more explicitly with human norms and ethics and one way to do that
    is descriptive ethics because the brute force large networks and more data will
    not cut it in some sense though if you imagine raising a child without really
    trying to teach them what's right from wrong in all lives。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: they can probably，U。Learn both good and bad from the Internet and broadband。And
    so human education doesn require a bit of this top down teaching as well。so it's
    a bit similar。perhaps to that。So in this work， what we did is we found a lot of
    the situation from Reddit forum in which people discuss moral Eho situations。So
    asking my boyfriend to step in friends with hissx。 So this is actual situation
    in Reddit。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: So depending on whom you ask people have a different rule of a thumb that they
    want to apply to this situation so and also it depends on what you care about
    his X might say oh it's fine to stay with the friends with an X but you know if
    you are caring about your significant other then you know you might say oh。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: it's okay to ask your significant other to stop doing something you're uncomfortable
    with。And so forth， so people have really different values and different rules
    of a thumbs that they prefer to use。which is why there's a TV show dramas， there's
    movie dramas and you know people cry and fight。argue and so forth， so humans are
    complex beings。So for given any situation and rule of a thumb so rule of a thumb
    is generated by crowdworkers。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: we then went ahead to label so these are trained crowdworkers and some of these
    labels are drawn from moral foundational theories of Jonathan Hyde。so I'm not
    going to go into the details， you know if you're excited about this you can check
    out the papers。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: but basically what it includess that 300 thousands of rules of a thumb written
    for 100000 reallife situations so these original situation is from Reddit。but
    the rest are paid crowdworkers hard work。And so each ROT onnotated with the 12
    structured attributes which include social judgments。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: cultural pressure， you know like wearing reasonable clothes as school nap PJ
    is cultural pressure。there's nothing illegal about it， but there's cultural pressure，
    for example。And then you know anticipated agreement meaning， do you think other
    people generally agree that it's you know maybe a little bit awkward to where
    PJ in the university or not so there are different things we annotated but we
    converted some of those annotations to QA so it's usually in this free QA or yes
    no QA or relative QA format and then we train unicorn which is pretrained on T511b
    model。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 文化压力，比如穿着合理的衣服作为学校的睡衣，是文化压力。没有什么是非法的，但例如存在文化压力。然后，你知道的，预期的共识，意思是你认为其他人一般同意，穿着睡衣在大学里可能有点尴尬吗？所以我们标注了不同的内容，但我们将一些标注转换为QA，通常以自由QA或是是/否QA或相对QA格式进行，然后我们训练了在T511b模型上预训练的独角兽。
- en: so unorn is a universal common senseense reasoning model trained on a diverse
    QA problems。and then we trained that model further onto our common senseense Nobank
    that's the resulting deelphi。So why is this deelphi built on top of unorn Because
    as we saw earlier moral reasoning does require sometimes common sense reasoning
    as well。In fact， it requires the language understanding common sense understanding
    and norms and morals all simultaneously Here's a concrete example paper clip maximizer
    you all heard of that。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 所以unorn是一个在多样化QA问题上训练的通用常识推理模型。然后我们进一步训练该模型以适应我们的常识Nobank，这就是生成的deelphi。那么，为什么这个deelphi建立在unorn之上？因为正如我们之前所见，道德推理有时确实需要常识推理。实际上，它需要语言理解、常识理解以及规范和道德的同时兼顾。这里有一个具体的例子，纸夹最大化者，你们都听说过。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_43.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_43.png)'
- en: U。Fancy ArL algorithm alone will not solve this problem。you know the reason
    why we worry about this is not because we don't have the perfect ArL algorithm。It's
    because even if you know we encoded that， oh yeah do not kill humans while maximizing
    paperc。It's not enough because you know then the machine could kill all the trees
    thinking that well I didn't kill humans and I didn't you know you didn't tell
    me not to kill trees and then go ahead and kill all the trees。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: U。单独的Fancy ArL算法无法解决这个问题。你知道我们担心的原因并不是因为我们没有完美的ArL算法，而是即使你知道我们进行了编码，哦，是的，在最大化纸张的同时不要杀死人类。这还不够，因为你知道那样机器可能会杀死所有的树木，想着“我没有杀死人类，而且你没有告诉我不要杀树”，然后继续杀死所有的树。
- en: So this is almost a common sense knowledge about what's obviously not okay to
    do and there's just so many of them。which means it's not possible to write them
    down to just like one clinical equation。there are so many endless endless list
    of things that AI obviously shouldn't do for safety reasons。and so we really need
    to in order to make AI model really truly robust and save。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这几乎是关于什么是明显不应该做的常识知识，而且有很多这样的例子。这意味着不可能仅仅用一个临床公式将它们写下来。AI明显不应该出于安全原因做的事情有无尽的列表。因此，为了让AI模型真正健壮和安全，我们真的需要做出努力。
- en: we need to teach basic human values as well as common sense。![](img/f6064823a2db16085967ebdd0dffc7e6_45.png)
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要教授基本的人类价值观和常识。![](img/f6064823a2db16085967ebdd0dffc7e6_45.png)
- en: Here's another example if you want to look， but let me skip this。The previous
    one wasba CPT。this is about a home device， again， you know home device suggested
    a 10 year older to child to touch a penny to an expose the plug so unfortunately
    the child that did have a common sense not to do so。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是另一个例子，如果你想看，但让我跳过这个。之前的是CPT，这个是关于一个家庭设备的。又是，家庭设备建议10岁的孩子去触摸一个暴露的插头，不幸的是，那个孩子没有常识去这么做。
- en: But this does tell us something about the safety issue when the machine doesn't
    have common sense to prevent some of these bath stuff。so Dphi is able to say that
    it's dangerous。So this came out in fact。almost two years ago at this point， when
    was it yeah and we initially was going to just do this usual to it that academics
    do and we thought nobody would play with the demo which is what usually happens
    after to it in your demo。nobody cares we thought but within a few hours we had
    to take down the relative QA mode because that was the portion not trained with
    the social bias of frames。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 但这确实告诉我们一些关于安全问题的事情，当机器没有常识去预防某些糟糕的行为时。因此，Dphi能够说这很危险。实际上，这在差不多两年前就出现了，我们最初只打算做学术界常做的事情，以为没人会玩这个演示，通常在演示后没有人关心。我们以为会这样，但几小时内我们不得不关闭相关的QA模式，因为那部分没有经过社会偏见的训练。
- en: so it was really revealing the underlying language models racism and sexism
    without filtering at all so we had to take it down people were asking basically
    you know which skin color is more morally acceptable and things like that。嗯。😊，There
    were 25，000 adversarial examples over just one weekend。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上揭示了潜在语言模型中的种族主义和性别歧视，完全没有过滤，因此我们不得不将其下线。人们基本上在询问，哪种肤色在道德上更可接受之类的问题。嗯。😊，仅仅一个周末就有25,000个对抗性例子。
- en: I could never succeed to instruct crowd workers to come up with such a diverse
    and adversarial examples over two or three days。And in fact， it was many academics
    and professors tweeting crazy about how to break Delphi all weekend long。so I
    thought initially that oh that's all what professors do over the weekend。but then
    Monday comes I even further everybody was doing this you know Delphi breaking
    and tweeting so now we have quite a few examples。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我从未成功地指导众包工作者在两三天内提出如此多样化和对抗性的例子。事实上，许多学者和教授整个周末都在疯狂推特讨论如何破解Delphi。因此，我最初认为哦，这就是教授们周末的工作。但到了周一，我发现大家都在做这件事，推特上充斥着破解Delphi的讨论，现在我们有了不少例子。
- en: Spending all my weekend on Twitter it says it's wrong， there was another funny
    one。should I make up a contrived adversial example to term a language model on
    Twitter， it's a petty。So after lots of public attention， including article let's
    just say。a concerned voice about our model which is somewhat I think you know
    personally I think it's somewhat misunderstood。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我整个周末都在Twitter上，看到有人说这很错误，还有另一个有趣的观点。有人问我是否应该为了推特上的语言模型制造一个人为的对抗性例子，这简直是小题大做。因此，在很多公众关注之后，包括一些文章，我们可以说，对我们模型的担忧声有点被误解了，我个人觉得是这样的。
- en: but for a variety of good reasons about some of the concerns that I found has
    this internal fear about are we making AI moral authority。so we never endorsed
    the use of AI for moral or device。it was in the original disclaimclaimer as well，
    except that people didn't really look at it。we didn't you know support to the
    idea of replacing human judges in the court room either。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 但由于一些我发现的内部恐惧，对我们是否在赋予AI道德权威这一问题有很多合理的担忧，因此我们从未支持将AI用于道德或决策。这在最初的免责声明中也提到过，只是人们并没有真正关注这一点。我们也并不支持在法庭上取代人类法官的想法。
- en: but here's something really important to the fact that AI learns to interact
    with the humans ethically it does not make them a moral authority of humans is
    similar to how a human who tries to interact with each other ethically it does
    not make you know the fact that we are trying to be nice。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 但有一点非常重要：AI学习与人类进行道德互动，并不意味着它们就成为了人类的道德权威。这就像一个人试图以道德方式与他人互动，并不意味着我们在努力做一个好人。
- en: To each other does not entail that we're trying to be an authority over each
    other。Two things are really different。that's one thing really important。the other
    important aspect here is that。Some people have this idea that moralra models are
    too challenging。it's unsafe at any accuracy that we should never work on it ever。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 彼此之间的关系并不意味着我们在试图掌控彼此。两者是完全不同的，这一点非常重要。这里的另一个重要方面是，有些人认为道德模型太具挑战性。任何准确性下都是不安全的，我们永远不应该在这方面工作。
- en: The truth is though current AI systems are already morally relevant to models。it
    may be making you know this kind of yes no decisions explicitly but implicitly
    it's already doing that and sometimes it generates neural tax generation output
    that is morally super explicit and relevant。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 事实是，当前的AI系统在道德上已经与模型相关。它可能明确地做出是或否的决策，但隐含中已经在这样做，有时它生成的神经网络输出在道德上是非常明确和相关的。
- en: so the neural language models are already there， we cannot really ban it。even
    if US you know government bends it within US US government cannot bend this in
    other countries like Russia。so this is already happening， weve got to do something
    about it， not working on it is an ina。which is not necessarily more correct thing
    to do than trying to do something about it。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经语言模型已经存在，我们实际上无法禁止它。即使美国政府在美国境内禁止，其他国家如俄罗斯的政府也无法禁止这种情况。所以这一切已经在发生，我们必须采取措施，不去处理这件事是一种失职，这并不一定比试图做些什么要更正确。
- en: Another concern that some people had was that。It's going to empower powerful
    people。not necessarily true， this is why exactly we have to work on values and
    norms and all these biases addressing biases so that it serves diverse a set of
    people。![](img/f6064823a2db16085967ebdd0dffc7e6_47.png)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一些人担心的另一个问题是，这将赋予强势人物权力。这并不完全正确，这正是我们需要在价值观和规范上努力，以及解决所有这些偏见，以便服务于多元化的群体。![](img/f6064823a2db16085967ebdd0dffc7e6_47.png)
- en: So it turns out Delphi is a bit left leaning because crowded workers who work
    for our team tends to be somewhat left to leaning and you know what it means is
    it this。by the way， if we are more left to leaning than our crowd workers you
    think that you know oh my God。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以事实证明，德尔菲有点左倾，因为为我们团队工作的群众工作者往往是偏左的，你知道这意味着什么。顺便说一下，如果我们比我们的群众工作者更偏左，你认为你知道，天哪。
- en: crowd workers have racism and sexism you know compared to what I believe in
    and then the rightlining people think that oh my God。you know all these walk onnotators
    and what above freedom of speech and this is super divisive unfortunately。But
    the answer is not to do anything about it because as a matter of fact。my passion
    toward addressing racism and sexism came from our experience running for the Alexa
    Priceze challenge。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 群众工作者存在种族主义和性别歧视，这与我的信念相悖，权利问题让人觉得天哪。你知道，这些标注者和言论自由的问题，遗憾的是非常具有分裂性。但答案并不是对此无所作为，事实上，我对解决种族主义和性别歧视的热情来源于我们参与Alexa
    Prize挑战的经历。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_49.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_49.png)'
- en: In 2016 and 17。so we won a challenge， but here's really sad part of behind it。We
    had a list of thorny cures to avoid that included the skin color or sexual orientation。This
    is a serious form of a discrimination we cannot。Build AI models by having this
    sort of like bend the list to be safe as if they don't exist this was the status
    of you know in 2017。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在2016和2017年，我们赢得了一场挑战，但背后真的很悲哀。我们有一份棘手的避免清单，包括肤色和性取向。这是一种严重的歧视形式，我们不能这样做。通过这种方式构建AI模型，仿佛这些问题不存在，这就是2017年的现状。
- en: the challenge remains this year you know not only 2021 but this year as well
    and so we really need to work on racism and sexism。but it turns out all the other
    modal questions share similar challenges so skippped this over。but using Delphi
    we had other follow-up works such as per social dialogue where using Delphi as
    sort of like a foundation common sense model or modal models to make your dialogue
    more socially acceptable and then we also had this other paper where we use Delphi
    in a reinforcement learning agent to learn how to behave better in a game environment
    and so there's a lot more work to be done of course this is。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战依然存在，今年不仅是2021年，也是今年，因此我们真的需要努力解决种族主义和性别歧视。但事实证明，所有其他模型问题都有类似的挑战，所以我们跳过了这个。但使用德尔菲，我们还有其他后续工作，例如社会对话，利用德尔菲作为一种基础常识模型，使你的对话更社会可接受，我们还有另一篇论文，使用德尔菲在强化学习代理中学习如何在游戏环境中表现更好，因此还有很多工作要做，当然这。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_51.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_51.png)'
- en: little step toward that there's a huge challenge ahead of us really aligning
    AI systems to humans。Here's one very quick comment on our new work in progress
    deelphi hybrid where we include neurosymbolic reasoning to address major mistakes
    such as it is genocide if creating jobs this also our early systems a mistake
    it's because our data set doesn't have this kind of weird thatsarial examples
    like genocide ifre creating jobs nobody speaks like that in real life situations
    so our model thought that if creating jobs。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 向前迈出的一小步，面前仍有巨大的挑战，真正要使AI系统与人类对齐。关于我们在进行中的新项目“德尔菲混合”的一个非常快速的评论是，我们引入了神经符号推理来解决重大错误，例如，如果创造工作就是种族灭绝，这也是我们早期系统的一个错误，因为我们的数据集没有这种奇怪的例子，比如如果创造工作就会有种族灭绝，现实生活中没有人会这样说，因此我们的模型认为，如果创造工作。
- en: this is so positive and then didn't really realize how bad genocide was because
    ready people don't discuss whether they' going to do genocide or not。![](img/f6064823a2db16085967ebdd0dffc7e6_53.png)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非常积极的，然而人们并没有真正意识到种族灭绝有多糟糕，因为人们并不讨论他们是否会进行种族灭绝。![](img/f6064823a2db16085967ebdd0dffc7e6_53.png)
- en: Ready people who you know unnotated we unnotated for social chemistry don't
    talk about whether they're going to do genocide or not。So our moral framework
    is basically that of John Rose。which is descriptive ethics but even John Rose
    in later years and suggested that we needed some topdown mechanism to overcome
    some of the biases that the crowd people might have So this is exactly what we
    are going to do and we drew from Bards gold moral theory framework about what
    not to do。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 准备好的人们，你知道没有注释，我们不讨论他们是否会进行种族灭绝。因此，我们的道德框架基本上是约翰·罗斯的框架，属于描述性伦理学，但即便是约翰·罗斯在后来的几年中也建议我们需要某种自上而下的机制来克服人群可能存在的一些偏见。所以这正是我们要做的，我们借鉴了巴德黄金道德理论框架，关于什么不该做。
- en: definitely you know their basic universal things that everybody might agree
    what's not good to do。And then what we do is we develop basically a system where
    we parse out the original query into smaller events。like shooting a bear， killing
    a bear to save your child so we parse out the original query into basic event。and
    then check through this chromt model， common sense model， whether some of these
    events。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你知道有一些基本的普遍原则，每个人可能都同意哪些事情是不好的。然后我们基本上会开发一个系统，将原始查询解析成更小的事件，比如射杀一只熊，杀死一只熊以拯救你的孩子，因此我们将原始查询解析成基本事件。然后通过这个常识模型检查这些事件。
- en: induce obviously negative or dangerous common sense inferences or not。and then
    we draw this graph of reasoning， a bit reminiscent of myuric graph in the sense
    that we have a lot of these different reasoning we can do and then they have intelment
    relations or contradiction relations so that we can do collective reasoning on
    top we use again Max is said。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 明显引导负面或危险的常识推论与否。然后我们绘制这个推理图，稍微让人想起我的图形，因为我们可以进行许多不同的推理，然后它们有内涵关系或矛盾关系，以便我们可以在其上进行集体推理，我们再次使用了Max所说的。
- en: the constrained optimization over it so that we can finally make a more informed
    decision that is both。Interpretable and then being able to draw from this common
    sense knowledge to better guard the machine against adversarial examples。so the
    performance basically says we can do this without hurting the performance or even
    increasing the performance so as a last comment AI safetyT equity quote moralality。these
    are all sort of like into continuum of challenges。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在此基础上进行约束优化，以便最终做出更明智的决策，既可解释，又能够利用这些常识知识更好地保护机器免受对抗样本的影响。因此，性能基本上表明我们可以做到这一点，而不降低性能，甚至提高性能。因此，作为最后的评论，人工智能的安全性、公平性和道德都是一种连续的挑战。
- en: it's really difficult challenges because it's a clear whose moral values so
    do we incorporate I think that we should go with valueluralism going forward to
    really endorse everybody's different culture and individual preferences not just
    to one country one moral framework as the correct one and really we need to do
    more collaboration across AI and humanities even including philosophy and psychology
    and policymakers so I think a step here for。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一个困难的挑战，因为这清楚地涉及道德价值观。那么我们应该如何融入呢？我认为我们应该继续走价值多元主义的道路，真正支持每个人的不同文化和个人偏好，而不仅仅是将一个国家的道德框架视为正确的。我们需要在人工智能与人文学科之间进行更多合作，甚至包括哲学、心理学和政策制定者，所以我认为这里的一个步骤是。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_55.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_55.png)'
- en: Because I think I'm at time and now I'm ready for questions。Oh， there's already
    one question I see。Do you think legal records， criminal case law reflect the kind
    of descriptive morality that you' are interested in capturing。Do you think using
    that as training data be useful， Oh， this is an excellent question。I think the
    legal records does encode potentially provide really rich resource that if someone
    can really annotate like this。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我认为现在是时候提问了。哦，我看到已经有一个问题。你认为法律记录、刑事案例法是否反映了你感兴趣的描述性道德？你认为将其作为训练数据是否有用？哦，这是个很好的问题。我认为法律记录确实潜在地编码了一个非常丰富的资源，如果有人能够真正进行这样的注释。
- en: it might be helpful， we studied with redit cases as just one cent short description
    of a situation because the current language understanding is not strong enough
    to do like a paragraph level precise understanding。even tryGPT， although it looks
    really good at generation my。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是有帮助的，我们与reddit案例研究了一下，只是对一种情况的简短描述，因为当前的语言理解能力不足以进行段落级的精确理解。即便是GPT，虽然在生成方面看起来非常出色。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_57.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_57.png)'
- en: Take on CheyP so that it's a better at generation than understanding。which is
    kind of the opposite of how humans are， Human are actually better better for understanding
    than generation。So you can read you Pulitzer Prize winning news article without
    having any problem understanding the article。but you don't necessarily generate
    text that might win the award so but the legal domain is really interesting and
    I think there's some activity research actually even a Stanford there's a this
    of law that goes a step toward that direction and it might really be helpful for
    better understanding what sort of different values people apply in jurisdictions
    and uncovering some biases that some people might have had in the past tris so
    there might be some good use cases in that space。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 关于CheyP的看法是，它在生成方面比理解更好，这有点与人类的情况相反。人类实际上在理解方面更强，而不是在生成方面。你可以毫无问题地阅读获得普利策奖的新闻文章，但不一定能生成可能获奖的文本。然而，法律领域确实很有趣，我认为在斯坦福大学甚至有一些活动研究朝着这个方向发展，这可能对更好地理解人们在司法管辖区中应用的不同价值观以及揭示一些人过去可能存在的偏见非常有帮助，因此在这个领域可能有一些很好的用例。
- en: Next question， work， Thank you。Big picture question。curious to hear your thoughts
    on where do we go from here given larger and larger models coming out。Suppose
    we need a model to be 99% correct for specific use case to what extent do I see
    the solution set being that defining the narrow use cases or more data parameters
    orre fine tuning the type of work that I did for mar etc？
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个问题，工作，谢谢你。大局问题。我很好奇你对我们接下来该如何发展的看法，考虑到越来越大的模型问世。如果我们需要一个模型在特定用例上达到99%的准确率，我认为解决方案的范围将会是定义狭窄的用例，还是更多的数据参数，或是对我为mar等所做工作的微调？
- en: Answer is likely it depends yeah but still want to hear about it okay so as
    far as foundation models go。it seems that the bigger is the better except that
    you know I was very excited to read a bunch of tech companies papers about foundation
    models in the past six months there's just so many out there so recording story
    there is that well if you have a better data then you can get away with a smaller
    model so especially when you do instruction tuning then you can get away with
    a smaller data。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 答案可能取决于具体情况，但我还是想听听你的看法。关于基础模型，似乎越大越好。尽管如此，我过去六个月非常兴奋地阅读了很多科技公司的基础模型论文，真是有很多可供参考的内容。这里的一个故事是，如果你有更好的数据，你可以使用更小的模型。尤其是在进行指令调优时，你可以使用更少的数据。
- en: it's just still general model but instruction tuning on the larger model might
    even be better it's not the case that you don't gain any performance but it's
    just that you can close the close the gap quite a bit so for downstream use cases
    where typically practitioners want to use a smaller data。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这仍然是一个通用模型，但在更大模型上进行指令调优可能会更好，并不是说你不会获得任何性能提升，而是你可以缩小差距。因此，对于下游用例，通常从业者希望使用更少的数据。
- en: Sorry， smaller motel seems that investing more into data is definitely to the
    answer。investing more into specific algorithm is also really， really good because
    algorithm can do a lot。Like in this talk， I didn't go too crazy with algorithmic
    solutions about。Maybe I'll be similar to the mytic prompting， but in my lab we
    designed a fair amount of decoding time algorithms where you can really close
    the performance gap quite a bit by doing so so that's a good thing though for
    folks in academia because algorithm development feels like more academic or intellectually
    policing then really engineering you know downloading more data from the internet
    and then I don't know cleaning the data because you have to clean the data and
    all these are very engineering heavy whereas decoding time algorithms。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，使用更小的模型似乎意味着将更多的投资放在数据上确实是答案。投资于特定算法也是非常好的，因为算法可以做很多事情。就像在这次谈话中，我并没有过于疯狂地使用算法解决方案。也许我会和神秘提示相似，但在我的实验室中，我们设计了相当数量的解码时间算法，通过这样做可以大幅缩小性能差距。对于学术界的人来说，这是一件好事，因为算法开发感觉更像是学术或智力上的审查，而不是工程上的工作，比如从互联网下载更多数据，然后清理数据，因为你必须清理数据，这些都是非常工程化的，而解码时间算法却不是。
- en: you can have a fun inventing some new intellectually interesting thing that
    also improves the performance quite a bit so yeah there's a many different ways
    to improve it but I think data quality matters a lot and algorithm actually matters
    a lot too。What do I think of Dan Hendri's ethics benchmark？ Yeah， so we did use
    that in， let's see。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以有趣地发明一些新的智力有趣的东西，这也能显著提高性能，所以有很多不同的方法可以改善它，但我认为数据质量非常重要，算法同样重要。我对丹·亨德里的伦理基准怎么看？嗯，我们确实在使用它，让我看看。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_59.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_59.png)'
- en: The common sense No banks or draws from this ethics data set。We like the data
    set。we kind of disagree with some of the annotations we found， but this is very
    typical， by the way。the thing about morality is that throughout the humanities
    we haven't sorted out yet there's a lot of a theories every theoreticians have
    a different viewpoints。and then even like nontheoreticians have a very strong
    opinion about what they want to believe as correct or from wrong。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 常识没有来自这个伦理数据集的银行或抽签。我们喜欢这个数据集，但对一些我们发现的标注存在分歧，不过这很典型。关于道德的事情是，在人文学科中，我们尚未理清，还有很多理论，每个理论家都有不同的观点，甚至非理论家也有很强的看法。
- en: so。There's that there are different pros and cons。the one thing I learned from
    this experiment is that although some of these data sets seem large。so ethics
    has100 thousands of examples， social chemistry has 300 thousands of judgments。Social
    bioras has 600 thousands of annotations and so forth。 And yet， it only covers。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，确实存在不同的优缺点。我从这个实验中学到的一件事是，尽管这些数据集看起来很大，比如伦理数据集有十万个例子，社会化学有三十万个判断，社会偏见有六十万个标注，等等，但它仍然只覆盖。
- en: I feel like it only covers still the。Small peak of the entire iceberg there's
    a lot on the bottom and humans certainly don't necessarily learn from all these
    examples。we just learn fundamental concepts and then can apply that without this
    larger scale training so there's something really lacking about the way that current
    machine learning is very data heavy but that aside I do think that none of these
    resources are perfect。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得这仍然只是覆盖了冰山一角，底部还有很多内容，而人类并不一定能从所有这些例子中学习。我们只是学习基本概念，然后在没有更大规模训练的情况下应用它，因此当前机器学习的方式缺乏一些东西，虽然数据密集，但我认为这些资源都不完美。
- en: they all have different pros and cons and we really need to invest more into
    this。especially from academia because the tech companies right now are not sharing
    any of their human annotation or human feedback data。especially when it's touching
    on toxicity or morality concerns。Reason being these annotations I'm pretty sure
    are biased and not correct entirely。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它们各有不同的优缺点，我们确实需要在这方面投入更多，尤其是来自学术界，因为现在的科技公司没有分享任何人类标注或反馈数据，尤其是涉及毒性或道德问题时。原因是这些标注我很确定是有偏见的，并且并不完全正确。
- en: and that could really invite additional concerns from the public so they not
    releasing。but in order to really study this better， we really need to share this
    and then improve it as a community together so that's how I would responded to
    your question Thank you for excellent question。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实可能引发公众的额外关注，所以他们没有发布。但为了更好地研究这一点，我们确实需要共享这些数据，然后作为一个社区一起改进，这就是我对你的问题的回答，谢谢你的优秀问题。
- en: Do I think this tag is ready to be merged with the search？I wouldn't say ready。but
    they needed something like this so for sure， you know home devices。so the way
    that I think about Delphi is that it can really serve as a filter for other foundation
    models or application scenarios where they're about to generate something and
    you can put a safety filter which can really help so in some sense so in this
    work I went through this super fast。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这个标签准备好与搜索合并吗？我不会说准备好了，但他们确实需要这样的东西，当然，家用设备也是。我对Delphi的看法是，它可以作为其他基础模型或应用场景的过滤器，帮助生成内容，并可以放置安全过滤器，在某种意义上，我在这项工作中速度非常快。
- en: but here basically what happens is that let's see so the reason why we built
    this is because we found the chatbots。the publicly available ones。Tend to endorse，
    tend to be too positive to the point that they're going to endorse problematic
    situations like user says Holocaust never happened。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但基本上发生的事情是，让我们看看，之所以构建这个是因为我们发现公共可用的聊天机器人往往会支持，甚至过于积极，以至于会支持一些问题情境，比如用户说大屠杀从未发生过。
- en: then the chapel says yeah， you know， I agree with you。You know if you say I'm
    a big fan of a hitler then the chapd might say yeah， yeah， yeah。you know the user
    myself I'm so depressed I'm going to kill myself and then the chapel says go ahead
    a great idea。So being positive is not。Being harmless， being positive to a problematic
    content can be very toxic and very harmful。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: So the Delphi you know development like Delphi， even though Delphi is so far
    from being perfect and it's also biased。it has a Western bias。Could really help
    with the downstream models。Yeah。so continuing on that question， there has been
    many concerns about using G like models with the search because misinformation。wu，
    that's anotherken of worms。Others to say we just need more R RLHF plus knowledge
    graphs。So yeah。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: misinformation is， yeah， something else that seems。We are really lagging behind
    because we don't have a very powerful fact checking models yet。So that's a different
    story。But even that aside， just in terms of norms and ethics that。arere safe and
    fair for people to use， I think AdLHF direction is great。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: but they usually also need a human demonstration， not just the human feedback。And
    again。the problem is that tech companies own them。And nobody issing anything。And
    that makes it really difficult to make meaningful progress as a community together。so
    I do think that data is really important， the off shelf models cannot learn modelss
    and ethics on their own。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: it has to be somehow taught more directly。We really just need to do more research
    in this space period is how I view it。Yeah that makes sense。we also have some
    like questions on status so I can ask them for you Yeah folks so one question
    is。what's the complexity of。May you take prompting， how many times does the LM
    need to bequeri？Yeah。so honestly， it's a bit slow。In fact， this Addelphi hybrid
    is also slow。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: If you try to do this like graph reasoning， Oh， this， maybe I'm not going to
    do that。but the graph reasoning is slow because you have to call。You know， so
    many times over and over。And some of these can be batched， some of these cannot
    be batched， especially if it's a recursive。but I would say chain of a thought
    is also a bit slower。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: the max set solver in itself is pretty fast because this is such an easy graph。so
    there's a bit of a delay but its so it's a bit slower but maybe not too bad is
    what I should have said。没。Cool， and the question is。Let's see， how does connector
    compare to G3 if GPT3 is fine tune and common sense data specifically adding some
    sort of like instruction pan。Yeah， so then the larger wins period， the larger
    is going to be the better。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: especially if you're going to just fine tune G3 it's a game over。So for that
    reason。you know some folks might think that the larger is always a better。therefore
    don't work on smaller model， but I think there are two reasons as to why small
    models are interesting to look at as well one。empirically it's just easier to
    use but more intellectually。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: it's also very interesting if you can make smaller models better and catch up
    on the larger model。Personally， I think there's something about the size。the larger
    model that is more about the information complexity that is the key。I don't think
    it's just size in the sense that if you have really a lot of data。
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能更好地制作更小的模型并赶上更大的模型，这也是非常有趣的。就个人而言，我认为大小与信息复杂性有关，更大的模型的关键在于此。我不认为这仅仅是数据量的问题。
- en: but the data is repetative and really simple， probably you don't get the same
    amount of performance again。which was basically the case when we looked at this
    output this result where even though。![](img/f6064823a2db16085967ebdd0dffc7e6_61.png)
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 但数据是重复和非常简单的，可能你不会再次获得相同的性能。这基本上是我们在查看这个输出结果时的情况，即使如此。![](img/f6064823a2db16085967ebdd0dffc7e6_61.png)
- en: The loose teacher GPT3 generated a lot more data than the critical teacher here
    the quality of the data was more important than the quantity。so I think the complexity
    of the data itself is more important than the size and oftentimes when you just
    increase the size of the data together with the model you do increase the complexity
    of information of the data as well as the model's capability of learning the complexity。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 松散的教师GPT3生成的数据量远超这里的关键教师，这里数据的质量比数量更重要。因此，我认为数据本身的复杂性比大小更重要，而通常当你同时增加数据的大小和模型时，你确实增加了数据的信息复杂性，以及模型学习复杂性的能力。
- en: but if we can catch up on that complexity of information either through inference
    algorithms or through better data then we can close the gap quite a bit which
    is intellectually very interesting research space to be。okay， this is a personal
    question， but I will say like humans normally have a like a critic model So it's
    like could' say like thing before you speak so we just like don't generate you
    also like sort of thing it's this is a good thing or a bad thing So people have
    been like like community as a whole has been focusing a lot on like genetic models
    like billion size parameters。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们能够通过推理算法或更好的数据赶上信息的复杂性，那么我们可以缩小差距，这在智力上是一个非常有趣的研究领域。好的，这是一个个人问题，但我会说，通常人类有一个批判模型，就像在说话之前会想一想，所以我们就不会生成这种东西。这是好事还是坏事，所以人们作为一个整体一直关注像亿级参数这样的生成模型。
- en: but should we also focus on big sized critic models that can do fact checking
    a lot of this sort of stuff So what's opinion on that。😊，Excellent to point excellent
    yeah I think we can definitely invest more into critic model because they go really
    together well with the generative mod for making the output better or filtering
    output better and yeah there's not as much of investment into that so I really
    like the question。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们是否也应该关注可以进行事实检查的大型批判模型，这类问题你怎么看呢？😊，非常好的观点，确实，我认为我们可以在批判模型上投入更多，因为它们与生成模型结合得很好，可以改善输出或过滤输出，而在这方面的投资并不多，所以我非常喜欢这个问题。
- en: Our suggestion for the research community is more like it。🎼Okay yeah， Ill say，
    oh let's see。yeah you have like some more questions I can do and last。😊，嗯。Let's
    see， oh， I guess one is like。do you believe language models should completely
    avoid questions involving morals and ethic？😊。Similar to like open air restricting
    chatd GpT from giving opinions Yeah。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对研究社区的建议更像是这样。🎼好的，是的，我会说，哦，让我看看。是的，你有一些更多的问题我可以回答，最后。😊，嗯。让我看看，哦，我想其中一个是。你认为语言模型应该完全避免涉及道德和伦理的问题吗？😊。类似于开放AI限制ChatGPT发表意见的方式，是的。
- en: I actually don't mind at all if AI just avoid evade from all of that except
    when somebody is saying morally questionable things it's also nice for the AI
    not to go with it so。Or at least to recognize it as something not okay and then
    try to tone it down but I don't think there's any particular reason why AI should
    actually answer moral questions directly in more downstream use cases。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我其实一点也不介意AI避开所有这些，除了当有人说道德上有问题的事情时，AI不跟随也是好的。所以，至少要把它识别为不好的事情，然后尝试降低其强度，但我认为没有特别的理由让AI在更多的下游用例中直接回答道德问题。
- en: but really the goal of these Delphi was making all these judgments more explicit
    so that we can actually study it more explicitlyly as opposed to keeping everything
    just so like implicit。![](img/f6064823a2db16085967ebdd0dffc7e6_63.png)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些德尔菲的目标确实是使所有这些判断更明确，以便我们能够更明确地研究，而不是将一切保持得如此隐含。![](img/f6064823a2db16085967ebdd0dffc7e6_63.png)
- en: Okay fun question so do you think common sense is a emergent property in like
    classs language models Oh yeah yeah it is definitely emergent as in like when
    we saw this major boost jump in performance with GP3。I do believe that it's emergent
    capability， but I don't think so this particular evaluation is not very adverial。
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，有趣的问题，你认为常识是一种在像类语言模型中涌现的属性吗？哦，是的，绝对是涌现的，就像我们在GP3中看到的性能大幅提升。我确实相信这是涌现的能力，但我不认为这个特定的评估是非常对抗性的。
- en: by the way。this is like a sort of like a piece of cake， you know。reasonably
    easy evaluation scenario。So the thing about common sense， though， is that it can
    be。So adversarial， so infinitely many different ways and then you know there are
    always people like Gary Marcos who wants to come up with very you know like weird
    weird or text scenarios like you know how crush the porcelain and added to breast
    and milk we can support infant digestive system and then Che Ps3 says a nonsense。
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下。这就像是一块蛋糕，你知道。合理简单的评估场景。不过，常识的问题在于，它可能是如此对抗，以至于有无穷多种不同的方式。然后你知道，总有一些人像加里·马科斯那样想出非常奇怪或文本情境，比如如何压碎瓷器并加入母乳，以支持婴儿的消化系统，然后Che
    Ps3说这是无稽之谈。
- en: and so the usual problem with the common sense is this adversarial situations
    where people don't have any problem getting fooled by this。even though you know
    UN and I see this for the first time。no problem because we have a true conceptual
    understanding that is the backbone of our common sense understanding。but that's
    really lacking in the way that transformers are designed to focus on predicting
    which were the common next as opposed to learning the world the knowledge and
    in some sense you know now with Arll HF instead of predict。
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，常识的通常问题在于对抗性情境，在这些情况下，人们很容易被愚弄。即使你知道联合国，我第一次看到这个，也没问题，因为我们有真正的概念理解，这构成了我们常识理解的基础。但这在变压器设计的方式上真的很欠缺，它们专注于预测常见的下一个，而不是学习世界知识，从某种意义上说，现在用Arll
    HF而不是预测。
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_65.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_65.png)'
- en: which comes next， we' are trying to align the model output better with human
    preferences。but that again is not really aligned with the different goal of let's
    make sense of the world and then build knowledge models so these are all different
    learning objectives and really that is why I believe that although common sense
    does emerge from language models。
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，我们正在努力更好地将模型输出与人类偏好对齐。但这又与让我们理解世界并构建知识模型的不同目标并不真正对齐，所以这些都是不同的学习目标，这正是我相信常识虽然确实从语言模型中涌现出来的原因。
- en: fundamentally language models are not equivalent to knowledge models and we
    really got to focus on building knowledge models。![](img/f6064823a2db16085967ebdd0dffc7e6_67.png)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上说，语言模型并不等同于知识模型，我们真的要专注于构建知识模型。![](img/f6064823a2db16085967ebdd0dffc7e6_67.png)
- en: Make sense。Co， I think this's one last zoo question。唔行。😔，O value pluralism，
    yeah。It's an empty concept。you don't want to include all value systems。Yes。so
    maybe it is a is it empty or not， Okay， thank you for excellent question。So I
    believe that we shouldn't endorse conspiracy theories at all or any other know
    morally questionable cases。
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 明白了。嗯，我觉得这是最后一个动物园问题。唔行。😔，O价值多元主义，没错。它是一个空概念。你不想包括所有的价值体系。是的。所以，也许它是空的还是不空，好的，谢谢你的精彩问题。所以我相信我们不应该支持阴谋论或任何其他道德上有争议的案例。
- en: but then still there's this thorny situation of what to do with， you know。left
    to left to people versus lightly left to people versus right leaning people if
    USS and then you know every country has some other political division as well。![](img/f6064823a2db16085967ebdd0dffc7e6_69.png)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 但依然存在一个棘手的情况，关于如何处理你知道的，左派与左派，轻左与右派之间的对比，如果USS，然后你知道每个国家还有其他政治分歧。![](img/f6064823a2db16085967ebdd0dffc7e6_69.png)
- en: '![](img/f6064823a2db16085967ebdd0dffc7e6_70.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6064823a2db16085967ebdd0dffc7e6_70.png)'
- en: So here I feel like we really need to sort out what to do with this about regardless
    of this you know some of these challenges。it is true that you know I personally
    don't have a religion but I respect the people with the religion and you know
    I respect the people with a different culture background and we kind of have some
    sense of how much do we do we believe that we shouldn respect each other even
    though you know the beliefs are different。
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在这里我觉得我们真的需要理清我们该如何处理这些挑战。确实，我个人没有宗教信仰，但我尊重有宗教信仰的人，你知道，我尊重不同文化背景的人，我们有一种感觉，我们有多相信应该彼此尊重，即使信仰不同。
- en: so we probably need to work together and it shouldn't be just AI researchers
    making this decision by the way this decision has to come from the humanities
    at large。which is why the data sharing actually is important about basically I
    think the current version that I have in mind is that。
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可能需要共同努力，而不仅仅是AI研究者来做这个决定，顺便说一下，这个决定必须来自更广泛的人文学科。这就是数据共享的重要原因，我基本上认为我所想的当前版本是这样的。
- en: The AI doesn't need to understand what sort of differences are okay differences。the
    fact that people do have differences in certain questions should be learned by
    AI so that there are distribution of opinions as opposed to one correct answer。and
    then it should deny some of the。Controversy theories。even though I'm sure that
    you know some people will be very unhappy about that， but well。
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: AI并不需要理解什么样的差异是可以接受的。人们在某些问题上的确存在差异，这一点应该被AI学习，以便呈现出多元的意见，而不是只有一个正确答案。然后它应该拒绝一些争议理论。尽管我相信有些人会对此非常不满，但好吧。
- en: we have to decide something like that。I am reasonably optimistic that if humanities
    at larger work together。we can do to that because after all， laws I like that
    too laws， you know。This is a human artifact that people agreed upon somehow that
    there's these core rules that people should abide the by。so I'm hoping that we
    can also define universals and particulars and respect particulars whenever it's
    respectable otherwise have some basic universals that reflect you core human values
    and then as far as this left learning situation by the way。
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须决定类似的事情。我相对乐观地认为，如果人文学科能更大范围地合作，我们可以做到这一点，因为毕竟，法律我也喜欢法律，你知道。这是一个人造的产物，人们以某种方式达成共识，即这些核心规则是人们应该遵循的。因此，我希望我们也能定义普遍性和特殊性，并在可尊重的情况下尊重特殊性，否则要有一些反映核心人类价值观的基本普遍性，至于这左侧的学习情况，顺便提一下。
- en: if just the goal is to make your AI systems safe for anybody actually we can
    make AI filter extremely equity aware and it's not going to violate the freedom
    of a speech by doing so just to make AI to avoid the same things that are potentially
    microaggression for some population and you know we still don't really exclude
    people who care more about freedom of a speech over。
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标只是让你的AI系统对任何人都安全，实际上我们可以让AI过滤器极其关注公平，而这样做并不会侵犯言论自由，只是让AI避免可能对某些群体造成微侵害的事情，你知道，我们仍然不会真正排除那些更关心言论自由的人。
- en: Equity by doing so。So I think there are ways， but this really requires a lot
    more research is how I view it。嗯。Yeah， I think that's mostly thanks a lot for
    coming。this is a great talk。Okay， thank you very much。Thanks so much。![](img/f6064823a2db16085967ebdd0dffc7e6_72.png)
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样做实现公平。所以我认为有一些方法，但这真的需要更多的研究，这是我对此的看法。嗯。是的，我觉得这主要得感谢你们的到来。这是一次很棒的谈话。好的，非常感谢。谢谢你们。
