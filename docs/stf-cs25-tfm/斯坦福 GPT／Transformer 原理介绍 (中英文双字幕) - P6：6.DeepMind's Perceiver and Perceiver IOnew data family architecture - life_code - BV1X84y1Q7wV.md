# ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P6Ôºö6.DeepMind's Perceiver and Perceiver IOnew data family architecture - life_code - BV1X84y1Q7wV

![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_0.png)

So today I'm going to be talking about some recent work that we've been doing at Deepmind developing this line of architectures that we're calling perceives and I'll be motivating this in terms of a goal that we have which is to develop a general purpose architectures and so just right off the bat I want to motivate why we care about general purpose architectures and so the first the first both of the reasons are fairly pragmatic„ÄÇ

 but basically the idea is if we're thinking about all of the data that we could possibly imagine collecting in the world„ÄÇ

 a lot of it is basically involves what we think of as sort of traditional sense modalities„ÄÇ

 whether these things range from touch andropriception to echoloc to the kind of perception you need to ingest text however you want to format that to things to more exotic things like event-based cameras„ÄÇ



![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_2.png)

Whiskering touching with wither sensesÔºå things like smell and depth and all the way up to the kinds of sense modalities that we really think about when we're thinking about scientific perception and so basically as if we think about the full set of data and what it would take to actually model each of these different modalities it basically looks effectively intractable to try to engineer inductive biases that will work for every single one of these so we don't want to engineer them one by one this is an approach that's worked and in some ways that it's maybe a reasonable description of how we think about developing new architectures for different problems but it's just not going to scale we can't afford as a community to design to hand designign inductive biases that will work for each and every one of these and so rather than doing that we want to sort of build architectures that at least at first pass will allow us to handle everything„ÄÇ

There's another practical argument for why we should work on general purpose architectures and that's because it will allow us to build simpler more unified systems so if you look at how in particular complex multimodal data streams are typically approached in the sort of the sensory computer vision or pattern recognition literatures effectively the typical way this is done is by using inductive biases that we know hold for the individual modalities and then engineer ways of combining those different subsystems so this can mean building specific specific head specific input modules for each of these things and then trying out the various different ways of combining them„ÄÇ

So this can work but it gives us systems that in principle really will only work on one or a small number of domains and it gives us systems that are very hard to maintain„ÄÇ

 tend to be fragileÔºå tend to depend on specific processing assumptions about the input modalities so rather than do that we sort of want to move in the direction of having unified black box architectures that kind of just work and the idea here is that if we can get to that point we can abstract the architecture construction process and really focus on other more high-level problems so this is sort of the motivation for this line of work„ÄÇ

And the way that we're going to be doing this is of course„ÄÇ

 by working on the most general purpose architecture that we have so far„ÄÇ

 which is basically a transformerÔºå and you'll all be very familiar with the basic building blocks of a transformer„ÄÇ

 but just at a very high level we can think about what they do right„ÄÇ

 which is they use a general purpose inductive biasÔºå so they're non-local„ÄÇ

 which means they're not making domainspecific assumptions about which point should be compared to each other„ÄÇ

 rather they tend to be global in terms of the attentional focus that they have„ÄÇ

They use position as a feature rather than a hard constraint of the architecture„ÄÇ

 and this is in contrast to just sort of MLP based architectures or confNs in the way that they typically work„ÄÇ

 which used position as an architectural component to constrain how compute is happening„ÄÇ

And then of course finally there's extensive weight sharing in the way that they're designed and because they focus on mat Mos„ÄÇ

 they tend to be TBU and GPU friendly so these are all very nice things about the way transformers work„ÄÇ

Of course on the other hand they have very poor compute memory scaling and there are two components of this„ÄÇ

 so attention itself scales quadraically so there's there's this big O of m squared L complexity at the heart of transformers and I like writing it this way because it really emphasizes that this is a property of basically as you make your models bigger either at the input size or as you make them deeper„ÄÇ

 you're just this problem is just going to get worse and because you have this scaling in depth as well„ÄÇ

 there's another sort of practical thing that happens here because're the amount of compute that we're doing is proportional to the input size so there's no bottleneck in the way that standard transformers work even the linear scaling becomes a problem and so in practice for very very large transformers this can often be the bottleneck that really matters but they're both at play here and so we really want to sort of tamp down both of these and so sort of the perspective here is that have really general purpose architectures„ÄÇ

We can't have ones that are just sort of in principle general„ÄÇ

 we have ones that we have to have ones that you can actually use on the scales and the kinds of data that we care about„ÄÇ

And so just to this will all be old half for all of you„ÄÇ

 but just the way that standard QKV attention works is basically like this„ÄÇ

 so it's all about matrix multiplications so we have some input we compute the query keys and values by having a 1D convolution a one by one convolution that we run over the input we then compute the attention the attention scores this is a matrix multiply that has the following these sorts of shapes we then use the output here to compute„ÄÇ

The weights to compute the actual output of the attention module itself„ÄÇ

 and then finally we won run this through an additional MLP which is applied convolutionally to get the outputs„ÄÇ

So this is the starting point of what we're working on here„ÄÇ

And let me just briefly just reiterate why we would want the sort of advantages that we have with these sort of standard transformers„ÄÇ

 so nonlocality is one of the two sort of inductive bias principles that we have here it's useful I think to contrast this to way that the effective locality that you get in conves and what this actually means so if we look at basically as a function of depth which inputs can see which other ones which means like how easily it is to express a function of two input points let's say we look at these this yellow and purple point here at the input now I've sent them sort of as far apart as possible but we might ask how deep would the effective computation have to be before you actually process these two and if you look at a three by three convolution„ÄÇ

You're talking you're having to look basically until the very end of the network until you're processing these things together and what this means is that the functions that you can express that actually look at both of these points into up being quite shallow because they have to be built on top of this very very deep stack that just gives you the locality and so in point of fact if you look at for example the way Resnets work so you have an initial block which has a 7 by7 convolution and then afterwards it's  three by three cons all the way up you need 283 by three cons with that standard processing stack before all of the 224 by 224 pixels in an image are looking at each other and what this means is that in a Resnet 50 the points on the very edge of the pixels actually never see each other and this this is I found this a little bit counterintuitive but it suggests that we really are constraining quite a lot the functions that are easy to express with these models and so there are some functions of images you just can't you can't capture with a resant 50„ÄÇ

On the other handÔºå if you look at an architecture that has global attention over the full over the full input„ÄÇ

 so a transformerÔºå if you could scale it that way or perceives we're going to be talking about„ÄÇ

 all of the pixels can interact so the model can basically capture these things and express these functions much more easily can be expressed in things that put locality first„ÄÇ

We also the other sort of interesting property of these sorts of architectures is that position is feurized and this basically means that we're no longer sort of encoding the architectural location of something to figure out where it's located with respect to the other ones and this allows the network to basically use any positional information that it wants„ÄÇ

 but can also discard it as it prefers and so this is the standard way it's done„ÄÇ

 of course in the context of architectures that use Fourier or sinusoidal like features„ÄÇ

 but there's a lot of flexibility here„ÄÇOkayÔºå so now just thinking in terms of how conves relate to transformers sort of at the opposite end„ÄÇ

 it may look like that we have a sort of scalability versus general generality tradeoff and so if we look at convenants the way that they're applied so typically we can think about using them on grid structured data„ÄÇ

 theyre of course generalizations of convolutions that work on„ÄÇ

on data sets with more interesting topologyÔºå but typically we can think of them as operating on grids of in some sort of space„ÄÇ

 whereas transformers apply to generic setsÔºå so transformers are more general from this point of view„ÄÇ

On the other handÔºå they scale muchÔºå much worseÔºå so conves are linearÔºå both in the input points„ÄÇ

 the filter size and the number of layers of that architecture„ÄÇ

 whereas transformers have this quadratic scaling and they're still linear in the depth„ÄÇ

So from this point of viewÔºå what we're interested in doing in the perceive line of work was to scale transformers but to keep the generality property„ÄÇ

 so we want something that lives in between these two extremes„ÄÇ

And the way that we do this is by looking at self-attention and sort of modifying it in a way that allows us to scale better„ÄÇ

 so to walk through what self-attention actually does in sort of standard transformers„ÄÇ

 we take our input array which here is written number as the indices which is the number of tokens or the number of pixels basically the number of input units depending what you're looking at and the number and the channels„ÄÇ

 we have a 1D convolution so this is big O of M with respect to the QK and V„ÄÇ

We then compute the attention maps using the output of this operation„ÄÇ

 this gives us a matrix multiplyÔºå which is the source of the quadratic scaling„ÄÇAnd then finally„ÄÇ

 we compute output features with another matrix multiply„ÄÇ

This is already we're already rate limited here because for even standard resolution images in this quite large so it's around 50„ÄÇ

000 for standard imagenet images which are going to very small so this is something that just isn't going to work if we want deep architectures„ÄÇ

So what we do is we replace at the input to the architecture„ÄÇ

 we replace the self- attentionten with a cross attention layer and we do this using basically a learned query„ÄÇ

 and so we're replacing only the query from the input here with a learned component and so these indices and channels you can just think of these as basically working like a learned initial state for an RNN„ÄÇ

 there's a variety of names that this idea goes under in the literature„ÄÇ

 we refer to them as sort of late as latetnts but they're sometimes called inducing points or other things„ÄÇ

So the basic idea is we're learning the inputs to the query and keeping the key value of it the same„ÄÇ

The down or the sort of upside of this is that when we compute the attention map after this„ÄÇ

 now we basically turn this from a square matrix to a rectangular matrix and reduces the complexity of the matrix multiplied to big of MN„ÄÇ

 so now it's linear in the input size„ÄÇAnd second the second matrix multiply has the exact same property„ÄÇ

 so it becomes from quadratic becomes linear and the quite cool thing about this is that okay so the cross attention is linear and complexity but the output is actually smaller and so this I think is actually the more important point here is that this allows us to map something which is quite large into something that has size that's independent of the input we saw we have full control over this it's a hyperparameter and this allows us to build deep networks on top of this on top of this latent so because this is of a small size that we can control„ÄÇ

 we can afford to have quadratic complexity on top of this„ÄÇ

And so we use this idea sorry I'm still a little bit confused as to how you guys are able to turn the query into a rectangle in the second step„ÄÇ

 is it because you replaced the query with a learned something that is significantly smaller compared to the input size in the first step„ÄÇ

Yeah that's exactly right so if you look at so the underlying matrix multiply here which is written as the QK transpose so this will basically so the outer dimension here has shape n which is determined by the query and so by shrinking that query we're just changing the output of the matrix multiply„ÄÇ

OkayÔºå thank you„ÄÇÂóØ„ÄÇüòäÔºåYeahÔºå so I guess„ÄÇSorry go ahead please Okay go so basically you only do that for the query right so key and value remain like the original size matrices„ÄÇ

 correct„ÄÇThat's right yeahÔºå OkayÔºå but so basically so„ÄÇ

I don't know what i'm not understanding basically so the problem for me is that for a query now in my head I'm like looking for„ÄÇ

 let's say I have like the I token now there is no IF query anymore„ÄÇDoesn't that cause a problemÔºü

Like when I'm trying to use it„ÄÇAnd like to compute scores„ÄÇ

Yeah so what's happening here is your compare you'll have a smaller subset ofque so if you think about this not in terms of matrix multiplies but in terms of comparing each query to each key so in normal self attentionten we have one query for each key so every point compares to every other point right so here what we've done is instead of comparing every point to every other point we have a set of sort of cluster centers you might be able to think about them as so it's a smaller number and we compare each of those to each of the input points„ÄÇ

But we don't know which„ÄÇWhich tokens technically belong to which clustersÔºå rightÔºü

That's right so it has to be learned yeah exactly so one way to think about this about what's happening here is that we're we're instead of so in a normal selfcent selfatten and a transformer by comparing to all to all we're sort of saying okay„ÄÇ

 I know what the feature is at this point and I want it to attend to similar features here what we're saying is we're learning a bunch of supplementary points that that should be sort of maximally similar to some subset of the inputs„ÄÇ

So correct if I'm wrongÔºå but this is essentially doing some sort of hard tension where you're saying like like instead of like querying over all the points„ÄÇ

 Is select like some pointsÔºå which we think are like vital similar and only like puts all attention over this„ÄÇ

Hardening like this points you are selectedingÔºå rightÔºü

Yeah so they're related that would be one way to think about it the sort of the slight modifier to that idea though is that they basically live in an abstract space„ÄÇ

 so they're not assigned sort of one to one to one of the input queries or to one of the input points they're sort of learned so they can be somewhere in the middle but I think that's a good way to think about it that's a good intuition„ÄÇ

But the I guess one of the places where I'm a little confused here is the you have here indices and indices for the two like the purple and green matrices on the far left„ÄÇ

But those indices are not necessarily corresponding to inputs like in the NLP space those would not necessarily be tokens right these are just sort of random disease„ÄÇ

 but the impact matrix in this case is the result of some kind of mapping from the input tokens to an N D matrix„ÄÇ

 is that right„ÄÇNo it's actually so they it basically acts like it's it's a learned set of weights is one way to think about it so they function exactly the same way that learned position encodings do so it's basically just a you know it's a learned embedding but it's not it's not conditioned on anything it's just sort of„ÄÇ

It just it's just a set of weights„ÄÇOkayÔºå that makes more senseÔºå thank you„ÄÇOkay„ÄÇ

 so if there are no more questionsÔºå I'm going to keep goingÔºå but of course feel free to interrupt me„ÄÇ

So the way that given this ideaÔºå so we have this learned latent arrayÔºå which again„ÄÇ

 it functions sort of like an RNN initial state or it's a set of weights„ÄÇ

 we basically randomly initialize that and then we use this to attend onto the input byte array and so the byte array here is the flatten set of pixels for example for imagenet„ÄÇ

And the output of this is going to live in the same space as so the same index space as the latent array does„ÄÇ

 and there's residual connections in the way that you would normally do in an intention layer as well„ÄÇ

So once we're in this space we can then build an architecture by taking by using a standard transformer but phrased in the latent space rather than in the input space and this is going to allow us to basically end up because we sort of distilled the input down to the smaller space we can still flexibly allow all of these points to interact so this should be still nearly as expressive as the transformer as a normal transformer is and then each of the modules here now is quadratic in the latent size rather than the input size so this is something that we can control quite a lot„ÄÇ

So in the original version of the procedureÔºå we found it was very helpful to have additional crossats„ÄÇ

 so this is certainly something that you can do and the reason sort of the intuition behind this is that if this bottleneck is quite severe„ÄÇ

 we can't maintain all of the information from the input and so we want these queries which are now sort of conditioned on the past to be able to look back at the input point and so this is something that we found to be quite helpful when tuning for the first paper but the caveat I will say is that we're no longer recommending this is best practice because these crossats end up being quite heavy but this is something that you can explore certainly if you want sort of more conditional queries or if you want to be able to cross attendends and new inputs that are coming in„ÄÇ

The other thing that we found quite helpful in the context of data sets that have a limited amount of data„ÄÇ

 which for these architectures includes INe is to allow weight sharing in depth and so this basically just amounts to tying the weights for the different cross attention and different self-attention layers as they're repeated so this ends up looking like an RN that's enrolled in depth„ÄÇ

So this is just at a high level this gives us an architecture that we can apply to images but doesn't make any assumptions about image structure„ÄÇ

 so it's one that you can use elsewhere and we basically we give information about the input sort of spatial structure by having positional encodings and here we use a 2D4ier feature position encoding and just to show you kind of what that looks like here to give you a sense so each of the input points is assigned basically so you'llll be some position here and we have sinusoidoidal and cosineus sooidial features in 2D so this is basically a 4ier8d composition of the position of the 2D input and a couple of things that we found where that if we sampled the frequency that's the maximum frequency that it's used up to the NOSs frequency of the signal we end up doing better than if you use a lower version of this and this basically is because this will allow every other point to be aware of every distinct„ÄÇ

Point in the imageÔºå whereas if you sample at a lower frequencyÔºå you can end up with aliasing„ÄÇ

 and so not all points will be sort of legible„ÄÇWe also found that sampling the spectrum relatively densely tends to help and the contrast here at the time we were developing this with respect to Nerf so NF at least in earlier implementations used quite a small number of frequency bands we found that the more we added the better we did so in general this is something to be attentive to„ÄÇ

And then finallyÔºå as opposed to language where you typically have addition of whatever your embedding is with the sinusoidoidal or position encoding that you use here we found that concatenating them performed consistently better„ÄÇ

And so this may be because the positioning the content embedding is not as sparse as it is in language„ÄÇ

 we're not totally sureÔºå but this is something that I observed consistently„ÄÇ

And before I move on to results I just want to contrast this to some sort of other approaches for using transformers in the image context„ÄÇ

 so obvious the obvious precedent here is visual transformers and I think this is a this line of work is great especially in the image context„ÄÇ

 but there are some caveats about it that make it less suitable for sort of more general purpose use„ÄÇ

 so one is that so vision transformers do use an input 2D convolution„ÄÇ

 so this is often phrase in terms of patches input patches a special case of a 2D transformer„ÄÇ

 so it does restrict the class of inputs you can use it for„ÄÇ

And because we're basically building this patching or convolution into it„ÄÇ

 this means that this as an approach really isn't sufficient to get it to work on non-gririd data„ÄÇ

 there are other ways you could adapt itÔºå but this is something that you will have to special case for every domain you're looking at„ÄÇ

And then finallyÔºå because we have this sort of input where we're telling the architecture what it should look at first in the initial grouping„ÄÇ

 this does amount to getting rid of the nonlocity assumption it's not super clear doing how much doing this just once will make a difference„ÄÇ

 but this is something to be aware of when you're thinking about this architecture„ÄÇ

And then finally cross attention itself is used quite broadly in the vision literature„ÄÇ

 so just a couple to highlight a couple of examplesÔºå debtor„ÄÇ

 which is an object detection method from Facebook„ÄÇ

 basically has a convolutional backbone that's then used to give an output feature map„ÄÇ

 this is then passed into a transform encoder decoder and of course whenever year encoder decoder you think cross attention because from the encoder to the decoder there's a cross attention step„ÄÇ

 and so they're using basically the cross attention to go from some feature map representation to something that looks more like the object bounding boxes„ÄÇ

There's also quite nice work on learning self-supervised or unsupervised object segmentation models„ÄÇ

 and in this work they're doing something very similar where they have a convolutional backbone„ÄÇ

 they then use something like these latent the latetnts that we introduce here„ÄÇ

 to do when they call them slots here but basically to assign some of the output pixels to different slots so that they sort of have independent complementary decoding of the slots in the segmentation model here„ÄÇ

And there's a lot of other things„ÄÇOkayÔºå so first I would add just so now I'm going to sort of walk you through results of this of this model hi can I oh go ahead I'll go after you go for it Okay cool sorry for that can you go back a couple of slides where you had like you know like how how like the inputs like flow into like I think one about yeah that one okay so two questions that latent transformer is basically like a self attention is that correctÔºü

Yeah so the latent transformer is a self a fully selfational transformer got it and why see like for like the key in value they flow directly into the cross attention and there is like the query also flowing into it but the latent array array is flowing into the cross attention imp parallel to the query can you explain thatÔºü

Yeah so this is just speaking here it's meant to depict the residual connection so the cross attention this is sort of a cross attention depicted as a cross attention module and so the cross attention itself has the attention it has a residual connection and then there's an MLP so that's what that's meant to indicate okay but it's basically the QKV standard„ÄÇ

Go it„ÄÇThanks„ÄÇüòäÔºåHÔºå I had a question that is slightly related to this if we can just stay up this slide actually„ÄÇ

 so I think one thing that's interesting about this„ÄÇSure sorryÔºå I lost„ÄÇYou're cutting off„ÄÇ

It's mostly consisting of attention layersÔºå whether it's self attention or creating image transformers„ÄÇ

 can you hear meÔºå is that coming throughÔºüNoÔºå it's cutting offÔºå I thinkÔºå but I think so recent„ÄÇOh„ÄÇ

 okay„ÄÇShould I type it is that should I type Yeah I think that's a good idea yeah„ÄÇ

 I'll type it thanks sorry„ÄÇAll rightÔºås kind„ÄÇFeel free to go aheadÔºå I'll type it slowly and„ÄÇ



![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_4.png)

Sounds good sounds good to me Yeah actually can I chime in drew while you're on that previous slide the flow so these residual connections I actually didn't know the cross attention used that how reliant are these sequential cross attention layers on the residual connections„ÄÇ



![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_6.png)

YeahÔºå so here„ÄÇHere in the initial so two things I will say is that in the initial cross attention it doesn't really make a difference so this is something we've ablated when we get to the proIO version of this we also did the same thing in the decoder cross attention and it can make some of difference it can make a difference there depending on what you're doing I think it's actually essential in when you're using repeated cross attention of this way so when you have this sort of iterative structure and the reason for this is that the thing that's actually used to condition the query is basically that's your full representation of sort of the state of the architecture so far and so the skip connection is from it's in basically the query channel it's in the late the latent space and so this is basically what allows you to end up with this sort of dense dense and stable architecture„ÄÇ

Thank you„ÄÇüòäÔºåÂóØ„ÄÇüòäÔºåOkay„ÄÇSo to imagenet okay so in standard imagenet processing basically we compare against a few so this is a little bit out of date at this point„ÄÇ

 but against a few sort of just sort of sanity check baselines here so comparing against Res 50 and then at the time the best vision transformer model that was purely on imagenet and we're definitely in the ballpark this isn't these aren't„ÄÇ

Anywhere in your state of the art results but this is an architecture that again it' not using any 2D convolutions and so the fact that it was able to do this well we found very„ÄÇ

 very surprising at the time one of the quite cool things about this is that because this architecture is not making any assumptions the architecture itself isn't making any assumptions about the spatial structure of of the input images we can look at permuted imagenet and in the first version of this what we do is basically we compute the features using the 2D position so the 2D position is sort of fixed to a position to the pixel and then we just shuffle it all and so this is basically we'll give you a sense of how dependent the baselines are on input image structure and so if we look at if we look at the transformer perceive by construction they don't change so this is not an empirical finding this is a property of the models but we find that Resnet 50 falls the performance falls by about half and VIT which again only has one layer where it's relying on the spatial structure„ÄÇ

Also it has about a 15 point drop and so this suggests that it's relying quite a lot on that very first one to give it some information about the structure„ÄÇ

We can push this a little bit by instead of relying on 2D Fourier features learning completely learned positional encodings and this basically this is an architecture now this is a model that has absolutely no information about the input structure and so shuffling them and learning them again is absolutely equivalent and we find that this architecture also can be pushed about 70% and we've gotten slightly better numbers here in general this seems to work worse but so the 2D information is useful but it's quite cool that you can get what would have been numbers comparable to state of the art about5 or six years ago so this is quite cool„ÄÇ

Sorry I'm a little thick hereÔºå you're saying the difference between the last two rows is that the second to last row has a two dimensional position of betdding and the last one has a one dimensional position of betdding essentially„ÄÇ

 as I right„ÄÇSo it's learnedÔºå so it's basically it'll be it's I believe a 256 dimensional vector that's learned„ÄÇ

 but it doesn't it basically it means that the model itself has no information about the input's spatial structure„ÄÇ

So the 2D positional encodings that we're using you end up having about 200„ÄÇ

 it's 200 some features depending on what you're looking at„ÄÇ

 but theyre they give you very detailed information about the 2D structure of the input because they're based on a 480 composition of the input space„ÄÇ

OkayÔºå that makes senseÔºå thank you„ÄÇHi drew can I ask a question about frequency you use to generate those sensor wave„ÄÇ

YeahÔºå so like a cover slide before„ÄÇs„ÄÇ

![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_8.png)

YeahÔºåYeahÔºå yeah„ÄÇ yeah„ÄÇ they like„ÄÇ So basicallyÔºå I do have taken some lecture in signal processing„ÄÇ

 And I know if I„ÄÇ

![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_10.png)

Want to avoidÔºå you knowÔºå alienency„ÄÇ I need to sample withÔºå at least my frequency„ÄÇ

 So I'm curious to know why to use frequency starting from one to the ni frequency instead of starting from micro frequency to some very high frequency„ÄÇ

OhI see so basically so the maximum frequency that's used is always NquiIS so anything about Nquist is going to be alias so you're not actually going to be able to resolve it because it's in pixel space right so we sample one is basically just giving you an oscillation that covers the entire image and so this is basically just to sample the full range of nonallias frequencies„ÄÇ

Âïä okayÔºå cool„ÄÇ thank you„ÄÇ good„ÄÇYeah„ÄÇOkayÔºå so so after after the image results„ÄÇ

 we wanted to try it on other domains and in particular we were interested in how this could be used to work on sort of multimodal domains„ÄÇ

 so ones combining various different types of input features and„ÄÇ

one challenge or one sort of problem that you encounter in these sorts of spaces is that the data from different modalities end up having different features and they always have different semantics so if you take the positional encoding plus the RGB for video you end up with some number of channels and that if you have audio that corresponds the data may be paired but it tends to have fewer features and it only has a1D positional encoding so the way that we handle this is basically by learning modality specific position encodings and so these are basically embeddings that are special and learned for each of the modalities and what this does is basically tags ends up tagging the features that come from audio or video with some information that the network can learn that allows it to distinguish which ones which but given these these padded these sort of learned padded feature vectors we then concatenate them all and that's how we process multimobile data so basically the input to the architecture still looks like just one big array it's just that when constructing this we know that some of those features some of the rows and that„ÄÇ

Aray come from video and some come from audioÔºå but the model itself isn't given information about that other than what it learns„ÄÇ

We also have some great questionsÔºå so can go first„ÄÇreturn turnÔºüYeahÔºå sorry„ÄÇ I thought it's„ÄÇ but yeah„ÄÇ

 yeahÔºå sure„ÄÇ if you can hear me it's just a reason„ÄÇLike starting a lot transformer stuff formally„ÄÇ

 so I just didn't know what position one that was„ÄÇOhÔºå so what a position we thatÔºüYes„ÄÇ

 so basically a positional embedding is it's a feature that says this so the simplest way to think about it is in text so text the input is 1D so things live in some 1D sequence and for each point there you feurize where it's located in that sequence so the simplest thing to do would be if you have negative one to one is the full range it's just denotes actually where it's located in that sequence but we typically will add sort of will want to featureurize this to have more dimensions than just a single one and so the Fourier trans the Fourier decomposition is one way to do this to sort of give it privileged information about the high frequency structure but we can also just use the position to index on some embedding array which is how we do it when morere learning things so basically it's just a set of weights that are added to the feature for that point that give the network information about where it's located in the groundary sequence„ÄÇ

If you want to go nextÔºüÂóØ„ÄÇSorry I had to find a mute button unmute button Okay so I actually have two questions regarding the fewer features the I think like do you guys sample them like uniformly or are they like do you like to learn theseÔºü



![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_12.png)

![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_13.png)

YeahÔºå so basically we sample them linearly so basically we take the full space and we sample them linearly with whatever the budget is there are so in various settings we have actually tried learning these so you could actually initialize an array with them and then learn them and that does help sometimes actually and you could potentially learn you could try a more sophisticated strategy on this too„ÄÇ

Okay cool my follow up question is that basically I feel like the selling point of your research right is that you don't make any structural assumptions right you can take any type of like format however for like the encoding wouldn't the dimensionality so for example like if it's text it's 1D right if it's a like an image it will be 2D and if it's like a video like 3D you have like more„ÄÇ

Like the positional encoding will have like more points right wouldn't that like inherently give away„ÄÇ

Like the nature of the input„ÄÇYeah so it does so I completely agree with this you're totally right the version of this where we have learned position encodings is the most pure from that point of view„ÄÇ

 so it's one that gives it basically no information about the ground truth spatial structure what it does give the model so when you do the learned position encoding it will say that for example there's a correspondence between point K on image1 and point K on image 2 so that's basically the least amount of information you can give it while still allowing it to sort of figure out what the structural relationship between the input points is so this is the direction that we've been trying to push in in general giving the architecture access to sort of ground truth structural information like this lives on this point in 2D is helpful so there's a couple things here there's sort of from a practical point of view if you want good results you need to exploit these things or it's helpful to exploit these things but we do want to move in the direction where we're relying on these things less„ÄÇ

And so this is basically something we're actively looking into„ÄÇOkay makes senseÔºå thank you„ÄÇ

So I think has posted her question on the chat„ÄÇI also see you have your hand raisedÔºå so if you want„ÄÇ

 you can give it a try if not I'll read out the question„ÄÇOkay„ÄÇ

 I'll try just let me know if it's choppy„ÄÇYeah so is it good right now so far so Oh good okay cool so I was curious looking at the perceive diagram you had it's a bunch of attention layers right like cross attention and self attention and I think there's been this like small trend in recent work in vision transformers to try to sort of replace the last few layers instead of having attention like make them be convolutions to address this attention scaling problem right in a different manner and so here like the perceive architecture is trying to make self attention less expensive and they they're just trying to replace it and they kind of just avoid the problem„ÄÇ

And so I'm curious and so I've seen papers both ways like some that try to do things like the ones you cited and then some that are trying to do this as well and in my mind everyone always has like the good results and stuff so I'm curious if you think there's a reason to do one or the other or if you think this alternative approach is also promising or is there a reason you know research should go in one direction or the other„ÄÇ

YeahÔºå so to my mindÔºå the big tradeoff is one between„ÄÇ

 so the vision literature I think it's just exploded in terms of these sort of hybrids and people the write the exact right place on the Pato curve for the tradeoff of speed and performance„ÄÇ

 but they're basically looking primarily on vision specific problems„ÄÇ

 so something the computer vision community itself typically doesn't regularize itself away from things that don't work on things that aren't vision„ÄÇ

So you end up with things that are veryÔºå very efficient and very performant on vision problems so I think from that point of view it's an incredibly important line of work and that's probably the right way of doing things what we're sort of aiming for is something is the things that are as general as possible while still kind of being performant„ÄÇ

So got it so this kind of thing is critical Oh sorry you off go ahead no no please go ahead I was going to say so this kind of thing is like it's important just to summary so you feel like it's important to focus on attention because that's kind of critical for NLP like you can't just sort of put in a convolution at the end and sort of fix the problem but envision maybe you can and it's fine is that a right way of understanding it„ÄÇ

That's part of it vision and NLP aren't the only two domains„ÄÇ

 and so the thing that we're for are really basically so the kinds of problems that we're interested in doing with this include things like event- based cameras„ÄÇ

 cell biologyÔºå sort of proteinsÔºå all of these sorts of things where we may or may not have the right convolutional inductive biases for to even know how to build those sorts of things or they end up being whole research programs like the mesh based convolution work„ÄÇ

Oh CoÔºå thank you„ÄÇ I also had like one more question about the architecture„ÄÇ So I saw that„ÄÇ

 I'm sorry if you said this and I just missed itÔºå but you had cross attention and then like that to transformer and then cost attention„ÄÇ

 I'm curious what happens if you replace the self attention in those layers with cross attention„ÄÇ

 Does it affect your accuracy is that even feasible is that a valid question„ÄÇYeah„ÄÇ

 so the sort of thing that you could do is you could modify this to make it sort of hierarchical so that there are multiple stages across attention we haven't gotten this working yet„ÄÇ

 but it doesn't mean it's not a good idea„ÄÇSo there there might be there might be a right way to do this that we haven't figured out right„ÄÇ

 but it's something we have tried a little bit Oh cool okayÔºå thank you so much I appreciate it„ÄÇYeah„ÄÇ

 problem„ÄÇOkayÔºå let me„ÄÇWe're running short on timeÔºå so maybe I'll skip ahead„ÄÇÂóØ„ÄÇOkay so„ÄÇ

Before we run too much timeÔºå I want to at least talk about the sort of the modifications to this architecture that we've made to make it work sort of even more generally„ÄÇ

 so one of the problems of the sort of the first the first architecture that we looked at here the basic receiver is that it works basically for arbitrary inputs but it's designed to work only on classification and regression tasks as an output and so basically we wanted to see if we could use the the same cross attention strategy for decoding and it turns out you can it's something that works pretty well just kind of out of the box so the idea is that we have if we have our cross attentiontion input and self-attention sort of to do the processing we can introduce a set of additional queries and these are basically queries that give the semantics of each of the points that you're trying to decode„ÄÇ

And we pass these input to another cross attention layer which is configured in basically the opposite way that the encoder cross attention is configured„ÄÇ

 so now the queries are going to be something that's potentially large and the keys and values are coming from this latent and so what this allows us to do basically is to keep all of the nice advantages of the original perceive so we have an encoder that scales linearly we have a processor stage„ÄÇ

 this sort of latent self-atten that scales independently of the input size and we now have a decoder that keeps the decoupling but gives this linear scaling with respect to output size and so by doing this we can now basically apply the same approach to basically dense output tasks„ÄÇ

And so to give you a sense of how this worksÔºå just sort of intuitively„ÄÇ

If we're doing auto encoding on this image of puppies„ÄÇ

 basically what we do is we encode process and then to decode„ÄÇ

 we take a query that corresponds to each of the points and then we pass it into this decoder so we can query one of the points„ÄÇ

 we get one pixel query another oneÔºå we get another one and all the way up until we get all 10000 points and that's how we can do reconstruction with this„ÄÇ

And the cool thing about this is that it opens up a bunch of new applications and we can get different kinds of outputs just by changing how the queries work„ÄÇ

So if we want to do something like multimodal auto encoding where we have some of the outputs or videos„ÄÇ

 we use the same construction trick to get positions that to get queries that have the relevant semantics for each of the points that we're decoding and we can do this even though basically the sizes of these different data so the number of points they have is quite diverse so in the multimodal auto encoding experiments that we have in this paper we do this for video audio and labels at the same time„ÄÇ

 so that all of them are just passed into their uniform network and then decoded one by one in this way„ÄÇ

But we can also do mass language modeling now by conditioning on the position in a sequence„ÄÇ

 we can do multitask classification by having basically an index that gives which task you're querying from the network„ÄÇ

And we can do things like optical flow by passing in input features as well as the positions„ÄÇ

And so I'm just going to just skip to a couple of the different„ÄÇ

I can share these slides with you all afterwards to look through them„ÄÇ

Some of these things are quite cool but„ÄÇJust quickly„ÄÇ

 I want to talk about language and then optical flow„ÄÇ

So for language basically what we wanted to do with this was to see if we could use this to replace tokenization and why might we care about getting rid of tokenization so one we use tokenization primarily because transformers scale poorly with sequence length and tokenizing cut sequence length by that a factor of four„ÄÇ

But but there are various problems that arise with this and so might we care about removing tokenizersÔºü

So for one„ÄÇTokenizers perform less well on rare words„ÄÇ

 so if you compare the sort of the bytebased decomposition„ÄÇ

 theTF based UTF8 encoding input of an input sequence like this„ÄÇ

 you can see that there's basically a uniform allocation of points in memory to each of the input characters„ÄÇ

 the exception or diacritics which end up splitting it into two but if you look at the sentence piece tokenization so it's learned that pepper is one token but jalapeno gets split into five in this case„ÄÇ

 so this basically says the amount of capacity that you allocate depends on how rare the word is which can lead to suboptimal encodings„ÄÇ

They're also bridled to subtle perturbationsÔºå a famous example of this is that if you enter so if you've ever played around a GPT3„ÄÇ

 you'll notice that the output can be quite sensitive to where if you add a space or emit a space at the end„ÄÇ

 and this basically is because the space can end up being factorized into different parts of the tokenization„ÄÇ

 problem there are other things that can happen there too but this is one cause of that„ÄÇ

And finally tokens don't transfer across across languages„ÄÇ

 so if you wanted to have a model that without any tuning could be used on many different languages at the same time„ÄÇ

 tokenizers are a blocker for thisÔºå so if we can get rid of them it'll simplify the pipeline it'll also make things less brittle and then hopefully lead to more general models„ÄÇ

So the way that we do mass language modeling is the same as the way that I showed in that schematic autoencoding experiment„ÄÇ

 so we mask some fraction of our inputs about 15% is sort of the standard magic number„ÄÇ

 we then decode at each of the positions that are masked and we task the model with decoding whatever characters we masked at those locations„ÄÇ

And then once we have this modelÔºå so this is what we do for pretraining„ÄÇ

 we can then fine tune it by replacing the decoder with a multitask decoder that takes in the tasks that we're using on the downstream evaluation setting and training the model to reconstruct the logits on a per task basis„ÄÇ

OkayÔºå so to look at how this model performsÔºå we basically first compare it to Bt basease so this is just a solid benchmark that we understand very well and first by looking at sort of matched two models that have matched flops we can see that perceive IO and Bt basease work on par you see there's a different tradeoff here so to get the same number of flops basically we make perceive IO deeper and this ends up giving it more parameters„ÄÇ

 but on a perflps basis it ends up performing about the same„ÄÇOn the other hand„ÄÇ

 if we remove the tokenizer from Bt and keep the flops the same„ÄÇ

 we see that the number of parameters in the depth just drastically fall down and this is because Bert scales quite poorly with sequence length because it uses a normal transformer„ÄÇ

But if we use a perceive without the tokenizationÔºå we can see that we only get a slight reduction in the number of parameters at the flops count„ÄÇ

 but the performance performs almost exactly the same so this means that the perceive in the setting is performing basically the same with and without the tokenization it's learning a different strategy it's using different parameters but it basically can be brought to the same performance„ÄÇ

We can then scale this more by sort of leaning into what happens in the tokenizer free setting and we see that we can get a moderate performance boost as well„ÄÇ

I think it's also in the language setting it's also useful to look at what the attention„ÄÇ

 the attention maps that are sort of learned and what's being visualized here are basically for each of the latents for some subset of the lates„ÄÇ

 we're looking at where it's attending to in the input sequence and some of these end up being local so point looking at specific points in the sentence„ÄÇ

Some of them are periodicÔºå so they look sort of at recurring points over the sequence and some of them also look like they pick out syntactic features„ÄÇ

 which is quite niceÔºå so they pick out basically exclamation points or capital letters or other punctuation that's quite useful and decodable from very right right at the beginning of the sequence„ÄÇ

We can also basically use this exact same architecture on optical flow and optical flow is basically an important classical problem in computer vision where given a pair of frames in a video we want to basically track all of the points so figure out the motion from every point from one frame to the other and so optical flow is usually visualize using these sort of colorized images that are shown in the bottom and what this gives you basically is a per pixel indication of the velocity at every single point„ÄÇ

And so you can see that so the blade that the character here is holding is moving to the right„ÄÇ

 whereas this creature behind her is sort of as moving downwards„ÄÇ

So there are a couple of problems with optical flow that make it interesting to to sort of approach„ÄÇ

 so one it's a dense task and it basically involves long range correspondences but the standard training protocol there's basically no largescale realistic training data just because it's incredibly hard to sort of label all of the pixels in a real world scene and figure out where they go to so the typical way to do this is to train on some synthetic data and then evaluate on more realistic scenes and optical flow is also interesting because it's basically the locust of some of the most complicated visual architectures in the literature so the previous say of the art result here is this method called raft which won the best paper reward at DCCCVE last year and I'm just highlighting this to give you a sense of how much work people do into sort of hand engineering these architectures so this is a very very cleverly designed architecture and basically it incorporates things like global correlation volumes that are explicitly computed„ÄÇ

Different offsets to basically allow the model to reason about how things at different scales are moving with respect to each other„ÄÇ

It also has local neighborhood gather operations as well as update blocks to keep track of what's happening within each specific correlation block„ÄÇ

 and then finally there's a flow specific upsampling operators that were developed„ÄÇ

So in contrast to thisÔºå we're basically we wanted to see how well perceive IO would do here and just to give you a sense of sort of what we were expecting coming into this„ÄÇ

 we thought wellÔºå maybe so perceivedIO is throwing a lot of the structure away so we were hoping that we would get some good results but it would probably overfit and there's this sort of problem of the domain transfer that's happening here but on the other hand self-attention seems to be a reasonable way to match this sort of correspondence thing„ÄÇ

What we actually found was that just by doing the veryÔºå very simple preprocessing here„ÄÇ

 so extracting basically a patch around each pixel and then using the standard perceive IO architecture„ÄÇ

 we were able to get state of the art results here and so this is basically was validation of this general approach of trying to have general purpose architectures that would transfer over„ÄÇ

And so basically with minimal tuningÔºå we are able to get results that are both of the sort of compelling benchmarks on both of the the Cel evaluation methods and to get comparable results on Kity„ÄÇ

 so these are the standard ones„ÄÇAnd we can also sort of visualize what happens when we apply this on real world data„ÄÇ

 so there's no ground truth here so we can't really compare it„ÄÇ

 but it's still useful to sort of see how it moves around and we can see that qualitatively it's able to capture a lot of the fine structure and to sort of get the right motion for the things that are very clearly moving in the specific direction„ÄÇ

We can also sort ofÔºå it's also I think informative to look at what happens how it manage to represent how it manages to represent sort of small structure is this video playing„ÄÇ

Yeah can„ÄÇOkay coolÔºå so the thing to look at here is the fine water droplets that are sort of flying through the air as that bird flies by and because we're decoding at every single output point„ÄÇ

 the architecture is able to represent those so it's able to capture very„ÄÇ

 very fine sale segmentation that would be difficult to capture if you had for example„ÄÇ

 a convolutional upsamplar here„ÄÇOkayÔºå so I'm just going to the light light gone off in this room„ÄÇÂóØ„ÄÇ

I'm also interested that you also try other task like depth estimation„ÄÇ

 because especially where looks like it can still also work well on that modalities„ÄÇYeah„ÄÇ

 so we haven't published anythingÔºå but some internal results suggest that that it works just fine„ÄÇ

re basically there don't seem to be one of the surprising things„ÄÇ

 the things that we were a little bit unsure about was how much information was going to be contained in this latent because basically you're abstracting quite a lot and it doesn't have any 2D structure intrinsically but it does seem like it seems to be able to represent things quite well and these sorts of decoding mechanisms do seem to be able to do that„ÄÇ

„Åã„Çã„Åß„ÄÇSo I'm just going to interest in this interest of timem I'm going to skip ahead to the conclusion Dr I had one question with respect to the metrics that you've shared for the optical flow„ÄÇ

 the numbers so like the in the table it was like central final clean entity where these different data sets or different metrics like same metric for different dataset sets or like these are three different metrics„ÄÇ

Yeah so these are three different data setsÔºå so Cintelcle and Stel final are basically two theyre two ways of doing the final rendering for Cintel in all cases these methods are trained just on the autoflow data set so they're trained on this sort of general purpose kind of wacky synthetic motion data set and then we're we're evaluating on these different domains without fine tuning„ÄÇ

Okay„ÄÇYeahÔºå the flow has quite the data sets are quite small„ÄÇ

 so it's generally even problematic to fine tune„ÄÇThank you„ÄÇÂóØ„ÄÇOkayÔºå so just to summarizeÔºå right„ÄÇ

 what was the ground roof to find the endpoint error„ÄÇYeah„ÄÇ

 so the way this works is Cintel is a computerÔºå it's basically a relatively high quality CGI movie that was basically open source and so they actually have the ground truth so if you know the ground truth 3D state you can compute the pixel correspondence from frame to frame so that's what's used on Cintel and then Kity they basically have they have like a LiDAR sensor that's used to figure out the depth of all points and then they compute the correspondences so the ground truth is actually the ground truth optical flow but in general it's hard to get dense optical flow it's very expensive to collect it„ÄÇ

GreatÔºå thanks„ÄÇÂóØ„ÄÇüòäÔºåOkayÔºå so so basically just to summarize so the perceives are tension-based architectures that scale linearly and work as drop in replacements for transformers on a variety of settings„ÄÇ

 they also seem to be able to achieve results that are comparable at least in performance with models that rely on 2D convolutions but of course there is a trade off here and so it's good to be very aware of this of generality versus speed and specific domains and so as was pointed out in settings where you can use 2D convolutions„ÄÇ

 it's certainly good to have them in the loop„ÄÇIt's basically a unified architecture that allows joint modeling of different modalities of different sizes and basically overall it seems to be quite flexible architecture that's able to produce a state of the art or near state of the art results on a variety of different domains ands in the two papers we look at a number of other domains that I didn't talk about including 3D point cloud modeling replacing the transformer that's used in the starcraft and the sort of starcraft behavioral cloning agent in a couple of others so this does we have a lot of evidence that this general approach seems to work broadly and there's a lot of things we still haven't tried so we're very interested in pushing this and always open for suggestions and so forth„ÄÇ



![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_15.png)

So we're relying on a large body of related work because we're sort of drawing from a lot of different areas here„ÄÇ

 so here are some highlights and then I just want to thank my co-authors on this work„ÄÇAnd of course„ÄÇ

 I'm happy to talk more„ÄÇThanksÔºåYeahÔºå thanks a lot„ÄÇ Thanks very„ÄÇSo one question I had is like„ÄÇ

 so what do you think is like the future of like like perceived like models like do you think like this are going to like being used more in like the sort of like the transform transformer community to replace like con and another stuffÔºü

YeahÔºå so the I think„ÄÇBroadly speakingÔºå I think of perceives now is sort of because we know how to adapt them pretty well to sort of domains where we don't have a great idea of the right way to structure an architecture„ÄÇ

 an inductive biasÔºå so I think that's one of the really strong cases for it„ÄÇ

 so settings in which you don't really know what the right way to structure a problem is„ÄÇ

 I also think these kinds of approaches can be used in conjunction with cons for sort of things that are as domain agnostic has needed„ÄÇ

But I think multimodal and and new domains is really to that's the real that's where this these are obvious choices got it also like what do you think are the current bottlenecks with this and if you don't mind like if you can disclose like border is what are you working on towards next with this stuff„ÄÇ

So I can't talk about too many details about thatÔºå but the a couple of domains so one„ÄÇ

 we don't really have a great handle on how to use them on sort of small small scale data so data where where„ÄÇ

You don don't have the data to sort of recover the inductive bias so this is I think a really important area„ÄÇ

 the other thing that we haven't sort of talked about here but you could probably imagine that we'll be thinking about would be how to train on multiple modalities or sort of multiple things at the same time so right now all of these architectures are sort of trained in isolation but there are a lot of opportunities for sort of figuring out how to pose problems together and use a single architecture on all of them„ÄÇ

Got itÔºå what I'm sure if you triedÔºå but can you also use this for like tableular data stuffÔºüYeah„ÄÇ

 so effectively the architecture treats any input data as tabular data„ÄÇ

 so I think that's exactly the right way to think about it„ÄÇcourse sounds good thanks to the top„ÄÇ

 I will open make general questions for the students„ÄÇSo it's jump recording„ÄÇ



![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_17.png)