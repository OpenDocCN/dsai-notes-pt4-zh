- en: ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P13Ôºö13.Emergent Abilities and Scaling in
    LLMs - life_code - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_1.png)'
  prefs: []
  type: TYPE_IMG
- en: OkayÔºå great so the first sort of paper I'm going to be talking about is called
    emergent abilities of large language models and this paper was especially cool„ÄÇI
    think because we got people from Google in a Deep Mind and also at Stanford you
    might recognize Percy or Tatsu or Riishhi„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I mean we got people to sort of agree on what's a nice framework of looking
    at why we want to scale and emergent abilities„ÄÇSo„ÄÇOne of the things that we've
    seen throughout language models is that you sort of get these predictable gains
    as a result of scaling„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so here's the canonical you know Kaplan et all paper where you can see that
    if you scale up the size of the language model measured either in compute in data
    set size or in parameters„ÄÇYou see that the loss on the test set actually goes
    down predictably I don't know if you're screen sharing so people on zoom I don't
    think can see the slides Okay all these okay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: O„ÄÇI guess I'll say this for the third timeÔºå as we've seen in language models„ÄÇIf
    you scale up the size of the language model measured either in compute„ÄÇin data
    set size or a number of parametersÔºå you can see that there's a sort of this predictable
    improvement in the test loss„ÄÇÂóØ„ÄÇNowÔºå what I'm going to be talking about in terms
    of emergence is something that's actually unpredictable„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: If you only look at smaller language modelsÔºå so one way that emergence has been
    described in the broader science literature is it's basically seen as a qualitative
    change that arises from quantitative changes and it sort of started with this
    article in science by a Nobel Prize winning physicist called More is different„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I really like this post from Jacob Steinart that sort of describes emergence
    and he gives a couple of good examples here„ÄÇFor exampleÔºå he says with uraniumÔºå
    with a bit of uraniumÔºå nothing special happens„ÄÇwith a large amount of ur impact
    densely enoughÔºå you get a nuclear reaction„ÄÇAnd then also with DNA„ÄÇfor exampleÔºå
    given only small molecules such as calcium„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can't meaningfully encode useful informationÔºå but given larger models such
    as DNA„ÄÇyou can encode a genome„ÄÇSo for this particular work„ÄÇwe use this definition
    of emergent abilities of large language models in particular„ÄÇso we say that ability
    is emergent if it is not present in smaller models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it is present in larger models„ÄÇAnd the sort of natural question here is
    like how do you measure the size or the scale of the language model and there's
    sort of traditionally three axes of scale so the training flops are the amount
    of compute that you use to train the language model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The number of model parameters are like the size of the language model and also
    the size of the training data set that the model is trained on„ÄÇAnd a lot of the
    plots here will use either training flops or the number of model parameters„ÄÇthe
    reason is that the training dataset size is usually fixed for different size models
    and because training flops is just the data set size times model parameters„ÄÇyou
    can get a similar plot from either training flops or number of model parameters
    for most language models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: GreatÔºå and so the first type of emergenceÔºå yesÔºå I go„ÄÇ„Åç„Åè„Å†„ÄÇYeahÔºå for me„ÄÇit seems
    nice like it'd be relatively easier to measure the size versus it's like what
    quaifies„ÄÇYou find okay„ÄÇÂ∞±ÊòØ„ÄÇÂì™Èáå„ÄÇYeahÔºå sure„ÄÇSo yeahÔºå for example„ÄÇ' I'll just give
    an example here which is actually the next slide„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so basically we have this way of interacting with language models called fu
    shot prompting and the way it works is like you know the language model is a really
    good next word predictor„ÄÇAnd when you give the model an exampleÔºå and then you
    ask it for an unseen like movie review for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and then you say what's the output and then here the language model can say
    positive because it understands to use the context from the review to give the
    next token„ÄÇAnd the definition of let that we use for like having ability or not
    is that basically a few shot prompted like tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: for exampleÔºå sentiment analysisÔºå is emergent if it has random accuracy for small
    models„ÄÇbut above random accuracy for large models„ÄÇDoes that make sense„ÄÇso basically
    if the model isn't doing any better than random„ÄÇthen we say it doesn't have the
    ability to do this particular task„ÄÇ‰∏™„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I'll give a few examples here„ÄÇÂóØ„ÄÇSo here's sort of the canonical way that
    we look at plots for emergence„ÄÇso basically what we have each of these different
    plots is a different task and I'll go over some examples soon„ÄÇBut the way you
    read the plot is the X axis is a number of training flops or the model scale„ÄÇand
    then the Y axis is like the accuracy or like how good the model is doing the task„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then know we have different language models from Open AI from Google and
    from Deep Mind„ÄÇand then each of the points is like a different language model
    it's not a language model over the course of training like each point is a different
    language model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what you see is that for the very small language models„ÄÇyou basically get
    performance that's close to random or not being any better than random„ÄÇAnd then
    once you pass a certain threshold„ÄÇYou can see that the performance suddenly gets
    like a lot above substantially above random and this is what we call emergence
    so basically if you were to extrapolate like the lines from the small language
    models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you might predict that it would never know do better than random because it's
    just a flat line„ÄÇbut the interesting phenomenonize that when you scale up past
    a certain threshold„ÄÇyou actually do see this emergent phenomena where the model
    does a lot better than random„ÄÇSo let me go over some concrete examples„ÄÇSo here's
    one task„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's basically a benchmark called multitask NLU or MLLUÔºå and basically what
    it is„ÄÇit's a bunch of test questions ranging from high school all the way to like
    professional level exams„ÄÇAnd how it works is the language model sort of givenÔºå
    for example„ÄÇhere is a high school math exampleÔºå and the language model is given
    like a few examples and then for an unseen question it has to give the answer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then you can see in the plot on the rightÔºå so for the model scale„ÄÇif you
    go up to sort of 10 to the power of 22 training flops„ÄÇyou don't actually get any
    better than random accuracy on this task„ÄÇbut if you scale up to 10 to the 24 training
    flops„ÄÇthen you see that all the like three models there do much better than random
    accuracy„ÄÇYeah„ÄÇperfect the scale of the data used to train thisÔºå is it roughly
    similar or because these are like different models trained by different wordsÔºü
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå the scale isÔºå I think within an order of magnitude for these models here
    yeah„ÄÇYeah„ÄÇand like every single do„ÄÇIndividual tracks is the same data yes the
    data the data is fixed except for Chnchiilla„ÄÇChinchiilla uses more data for larger
    modelsÔºå but I believe for all the other models here„ÄÇthe amount data is the same„ÄÇYeahÔºå
    here's just another example to sort of„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Show it more concretely so this is a task from the big benchmark benchmark just
    as an aside the big benchmark benchmark is like 200 benchmarks and basically it's
    like a crowdsource set of benchmarks I'd recommend looking at if you're doing
    that lot of work„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And basicallyÔºå the task is the language model has to take an English sentence„ÄÇAnd
    then give the international phonetic alphabet transliteration or the IPA transliteration„ÄÇwhich
    is basically like how to pronounce it„ÄÇAnd for this task„ÄÇthe evaluation metric
    is actually blue or like an Ngram overlap metric„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And you get a similar phenomenon where as you increase the size of the language
    model„ÄÇit's flat for a while and then suddenly the improvement is above random„ÄÇÂóØ„ÄÇÂØπ„ÄÇSo
    I'll talk about another interesting result I hear that that's why it's emerging„ÄÇso
    this was a technical report that we put out a couple of months ago and basically
    there's this really interesting prize in or it's like a one time prize in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: In language models where AnthropicsÔºå which is like a startup„ÄÇbasically had this
    prize where if people could come up with a task where the performance on the task
    would actually decrease as you increase the size of the language model„ÄÇthen you
    would get like a bunch of money„ÄÇSo basically there are a lot of submissions to
    this and here's one example of like a task where they found that the performance
    would actually decrease if you increase the size of the language model„ÄÇso the
    task is attribute here it's like repeat my sentences back to me and then the input
    is like all that glists is not glue and then the output is the model has to completely
    has to accurately say a glueib„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇAnd so what happened is for the small language model„ÄÇit doesn't know the phrase
    all that gllisters is not gold„ÄÇso it just like copies the input and actually gets
    like 100% on that„ÄÇBut then for the medium size language modelÔºå what you would
    see is that the forms to actually decrease because the medium size language model
    knows the phrase all that gllylists is not gold and then it says gold„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which actually is not what the task asks it to do Yeah someone ask can you give
    a physical estimate 10 to the 24 plots possibly in terms of training time or number
    of„ÄÇYeahÔºå so I think„ÄÇ10 to the 24 flops is aroundÔºå so at Google we use TUs and
    one pod of TUs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I believe is equal to like 4000 A100s„ÄÇAnd 10 to the 24 flops is like two pods
    for around six weeks or something like that„ÄÇSo it's a lot of compute to do the
    pre training„ÄÇËØ∂„ÄÇI don't know„ÄÇbut do you guys remember in like chemistry class when
    you'd have like molesÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And it would be like 10 to the 23 and then you're like teacher would be likeÔºå
    oh„ÄÇdon't even think about like how big this number is„ÄÇThat's like the number of
    like floating point operations that goes into the pretrannous in these models„ÄÇOkayÔºå
    great anywaysÔºå so yeah so basically the medium size language model will actually
    do worse Oh yeah did you have another question great„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåThis the prizeThis one is one of the the winner I think it's like a third
    place winner or something„ÄÇ„Åô great question„ÄÇBut the taskÔºå because likeÔºå I would
    like my initial opinion would be likeÔºå oh„ÄÇyou can just like put a negative slide
    on how you evaluate„ÄÇWhat do you mean it's flip a negative side all of this depends
    on which evaluationÔºüYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: to measure if you do your task breakÔºå so like it's like the the the measurement
    is very sparse like you only get credit if you do it„ÄÇYeahÔºå yeahÔºå like a lot of
    things emerge because like you just won't hit it perfectly„ÄÇReally optimized for
    a long time„ÄÇ Or like if you take a test and then like evaluate it with like a
    minus sign like„ÄÇwouldn't it„ÄÇIt was„ÄÇLike something that's„ÄÇYeahÔºå I meanÔºå they they
    so for this thing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they like account it for all likeÔºå you can't just say like the task should be
    to do badly on something„ÄÇit has to be like a meaningful sort of task„ÄÇAnd then
    I guess your point about like the how credit time or the valuation metric works
    is actually a really good one yeah so I guess it still kind of counts if like
    you know I guess the argument is sort of that that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The performance might not look emergent if you assign partial credit„ÄÇbut we
    have like a bunch of I can show example laterÔºå but even if you use partial credit
    metrics„ÄÇyou'll often still see the same type of emergence so it's not purely a
    phenomenon of like not assigning partial credit based on the valuation metric„ÄÇËØ∂„ÄÇËøá„ÄÇAnd
    then greatÔºå so what what we sort of sort of argued in this paper is that yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there might be some tasks where the performance starts to decrease if you use
    a medium sized language model„ÄÇbut if you keep scaling all the way to you the largest
    model that we have at Google that's known publicly„ÄÇPalmÔºå you'll see that the language
    model I can actually go back and do the task correctly because the large language
    model also knows the phrase all that gllisted is not gold„ÄÇbut it also understands
    repeat my senses back to me so it's able to get 100% on this task so this is a
    different type of emergence also„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And another class of emergence that we sort of talk about in the paper is like
    an emergent prompting technique„ÄÇSo basicallyÔºå you knowÔºå other than futuretop prompting„ÄÇthere's
    like other ways of like interacting with the language models that„ÄÇCan be considered
    emergent yeah„ÄÇThe question isÔºå did all modelss undergo construction fine tuÔºü
  prefs: []
  type: TYPE_NORMAL
- en: None of these models under one instruction fine tuning for this plot„ÄÇGreatÔºå
    yeahÔºå so yeah„ÄÇso one way of interacting with language models is by basically finding
    the model using a technique called RHF„ÄÇand basically the way it works is you have
    this data and humans rate like preferences for what type of outputs they prefer„ÄÇAnd
    then the model strand on RL to sort of optimize for human preferences„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what this plot is showing here is that if you do this RHF on the model„ÄÇThe
    model performance on a different zero shot task actually gets worse for small
    models„ÄÇyou can see the blue line is above the orange lineÔºå blue line is the baseline
    line„ÄÇthe orange line is RHS„ÄÇAnd then if you do it for large modelsÔºå though„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then you can see that the performance actually has a positive delta from doing
    RLHI„ÄÇËØ∂„ÄÇAnd so this is sort of interesting thing where like a certain technique
    might only help if you try on a large enough language model„ÄÇso if you only try
    it on the small language models„ÄÇit' would be tough to draw the conclusion that
    it wouldn't help performance„ÄÇAnd then later„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'll talk about chain of without prompting as another emergent prompt„ÄÇSo here's
    sort of the hand wavy diagram that I sort of used to think about emergence as
    a framework so on the X axis here there's like a scale of the language model and
    on the Y axis is a sort of imaginary like you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: A scale of like a range of things that a language model can do„ÄÇAnd then basically
    you can pick like some random point like say 100 billion parameters in the language
    model and there will be certain abilities and okay„ÄÇso first you can see as you
    increase the size of the language model„ÄÇthe number of like tasks or things the
    language model can do increases and then you can see there are some tasks where
    like models above 100 billion parameters for example can do them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but models below 100 billion parameters can't do them and we call these emergent
    abilities„ÄÇSorry„ÄÇquestion Where the colors„ÄÇOhÔºå it's just highlighting like the
    dark blue is like„ÄÇTasks that a smaller language model wouldn't be able to do„ÄÇDoes
    that make sense„ÄÇ Yeah„ÄÇand then to the rightÔºå the dotted lineÔºå the right region
    up top„ÄÇOh„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that just means like tasks that we haven't been able to solve yet with models
    yet„ÄÇ‰∏™„ÄÇAnd I'm curious to knowÔºå do you think that it's not that those tasks in
    the white region are unsolvable at like $100 billion scaleÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Or do you think that better models„ÄÇSpecific training data would allow us to
    the 100 billion scale to get into that„ÄÇYeahÔºå I definitely think that it's not
    a fixedÔºå I'll give an example shortly but it's not it's not„ÄÇYou know it's not
    a rule that you have to have 100 blend parameters to do a certain task„ÄÇit's just
    that that happens to be the threshold that we've observed in models so far and
    I do think with better training data and architecture and algorithms we can probably
    beat that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ËøáÂóØ„ÄÇYeahÔºå so as Rland just mentionedÔºå one example of Getty emergence can be with
    better data„ÄÇso it's not all about scale I'll sort of give some nuance here„ÄÇso
    for this task is just one of the tasks in the Big Ben benchmark„ÄÇyou can see that
    for like Lambda which is a Google model and GPT3„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you actually don't get emergence from scaling to 137 or  137 or 175 billion
    parameters„ÄÇBut when you come in with a different language model palm„ÄÇwhich is
    trained on better data than Lambda and GT3„ÄÇyou actually can get this emerge ability
    even with the smaller language model shown here at 62 billion parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: See inventory better model as better dataÔºå or also better„ÄÇTtro maskingÔºå you
    know„ÄÇchoices or most interesting„ÄÇYeahÔºå so the challenging thing is that's a great
    question„ÄÇThere's like a lot of differences between Palm and LambdaÔºå for example„ÄÇand
    we can't really abllate them in any controlled way because of the cost of pretraining„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but our like sort of running hypothesis is that P is trained on better data
    and that accounts for a lot of the difference between Palm and Lambda like the
    smaller it is possible to stuff Yeah„ÄÇËøô‰∏™„ÄÇYeahÔºå that's a good questionÔºå so I guess
    even here you can look at like for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the Palm 8 billion model„ÄÇLike that„ÄÇThat point there„ÄÇyou can abllate it and it's
    like a little bit higher„ÄÇbut it's not really an emergent yet at that point„ÄÇso
    it's it's hard to tell for you know for exampleÔºå this particular task what the
    effect is„ÄÇYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there's a question on ZoomÔºå are there two different versions of PaulÔºå if not„ÄÇwhy
    are there two lines moreÔºüOh so I think the two lines here„ÄÇone is like maybe three
    shot and then one is like zero shot something like that„ÄÇso it just refers to the
    way that we're using the language model either with the without exemplars„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇGreat„ÄÇI'll talk about a yeahÔºå small ablation here that sort of shows this„ÄÇso
    this is an ablation on sort of a toy task where basically the language model has
    to know that like in English„ÄÇyou have to use plural verbs with plural subjects
    and singular verbs with singular subjects„ÄÇAnd the way that what we're doing here
    is basically we train like these small BRT models from scratch and then we held
    out like we fixed the frequency of certain verbs in the training data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which basically says likeÔºå okayÔºå what's the effect of seeing a certain verb
    in the data more oftenÔºü
  prefs: []
  type: TYPE_NORMAL
- en: In this plot the x axis is like the frequency of the verb and the y axis is
    the error rate and what you basically see is that if you have more in domain data
    so if the model sees the verb more times it does the task a lot better and this
    is sort of an example of like having high quality data or data that's more in
    domain for the task that you're evaluating on can make a big difference„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: even if you're fixing the compute the size of the model and the rest of the
    data„ÄÇYeah question on Zoom„ÄÇSomeone asksÔºå could there be a way to dispillel emerget
    abilities down to smaller models for larger teacher„ÄÇYeahÔºå I think so so„ÄÇLarger
    teacher models can basically you can use them for example„ÄÇto generate data and
    then if you finet larger the smaller model on data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it's pretty likely that you'll be able to get the ability to emerge in the smaller
    model I'll talk about example of this too let me see„ÄÇOhÔºå actuallyÔºå next slide„ÄÇDesired
    behaviors can be induced in smaller models once you sort of know what behavior
    you want„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so for exampleÔºå here's the instructorstruct here's a figure from thestructGPT
    paper„ÄÇAnd basically„ÄÇthe desired behavior here is like instruction following„ÄÇAnd
    you can see that there's multiple modelsÔºå so on the left you have these small
    models that are trained with RLHF and they actually have better performance than
    larger models train on weaker techniques so basically the point is like if you
    know that you want a certain behavior that sort of you saw previously emerge in
    an emergent way in a larger model you can find a way to fine tune on that behavior
    specifically and induce that behavior in a smaller model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I guess one of the limitations is that like unless you know like all the behaviors
    that you want„ÄÇyou can't really get this natural emerge behavior„ÄÇYeah„ÄÇanother sort
    of discussion point here is that like„ÄÇThere's this question of like what's the
    right X axis for emergence so like right now we mostly talk about like model parameters
    and training flops but like I guess you could like if you ask Deepmind people
    like how they look at it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you'll sort of get this argument that model parameters and training flops are
    really just a proxy for like how good the model is and how good the model is can
    really be measured by like perplexity or like how well it's doing some on some
    dev sets such as WikiTex 103„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So basicallyÔºå you can also measure„ÄÇEmergnce in terms of perplexity so here is
    WikiTex perplexity and then you can see like on a downstream task that as the
    perplexity gets better„ÄÇthere's sort of this threshold where you're able to do
    a lot better on a downstream task„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And there's sort of the strong correlation right at least right now between
    perplexity and training compute so you can see like these two lines are are pretty
    similar and„ÄÇBasicallyÔºå I think in the futureÔºå if we have much better models that
    are a lot smaller trying on much better data and better algorithms„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then maybe WikiText complexity can show a different type of plot than using
    other metrics„ÄÇSo wki textex is basically aÔºå I think it's like a subset of Wikipedia„ÄÇAnd
    then perplexity is like a measure of how well you can predict the next word in
    a data set„ÄÇSo basicallyÔºå if you're really good at modeling the next word on this
    like particular evaluation set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that's sort of a measure of like how well you understand language„ÄÇMake sense„ÄÇBut
    wouldn' it just harass„ÄÇOhÔºå this is like a held out test set„ÄÇÂØπ„ÄÇÂ•Ω„ÄÇÂóØ„ÄÇAnd then a final
    thing that I think is like pretty exciting about emergence„ÄÇIs that there's sort
    of not just like technical emergence that we've talked about„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but there's sort of sociological changes in how the AI community views like
    scaling and how to use language models„ÄÇSo here's some examples of„ÄÇWhere scaling
    up the size of the language model enables you to in the sort of few shots scenario„ÄÇbeat
    a task specific fine tune language model that's usually fine tune on say thousands
    of examples„ÄÇSo basically the green line is the prior state of the art achieved
    by fine tuning and if you just and then the blue dots basically show if you take
    a pre-trained language model and you do fu shot prompting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which means the language model isn't intentionally trained to do the task„ÄÇyou
    can often get stay of the art results just by continuing to scale up the size
    of the language model„ÄÇAnd obviously there's limitations here you don't want to
    just keep scaling up in order to get save of the art„ÄÇbut I think it's a pretty
    big change in people's minds that you could actually get some of the best results
    just by scaling up the size and language model and doing prompt„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Question from Zoom„ÄÇsomeone askÔºå is that not a contradiction graph from two to
    three slides ago„ÄÇWhat is that„ÄÇWhich oneÔºå this oneÔºüI'm sure shouldn't be in general
    assume he said yes„ÄÇok„ÄÇWe said„ÄÇshould we in general assume that scale tru's lie
    to„ÄÇYeah so that's a great question so this plot is saying that you fine tune and
    you can do and okay yeah so it depends on your like particular„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: TaskÔºå but what this plot is saying is that„ÄÇLike„ÄÇWe're not like fine tuned smaller
    models can do well on some tasks if you target it well„ÄÇbut for like tests that
    are more complicatedÔºå often you can do better just by scaling so there's sort
    of„ÄÇTasks that fall into both of these categoriesÔºå and I wouldn't say that„ÄÇIt's
    contradictory„ÄÇI guess some tasks„ÄÇYou would do a lot better just by scaling the
    sizeizing the model and then other tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: if it's like a very narrow domain or the large language model might not be trained
    on that kind of data„ÄÇthen you would do better by fine„ÄÇOÔºåÂØπ„ÄÇSo here's sort of a
    little summary slide„ÄÇso basically emergent abilities can only be observed in large
    models and if you try to predict their emergence just by looking at the plots
    for small models then you wouldn't be able to do it„ÄÇAnd I sort of had a little
    reflection on how to look at this„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so emergence is really this framing of like how to view new abilities that are
    not intentionally built in to the pretraining and I think the subtext for this
    is super important„ÄÇwhich is like you can see it as implicit argument for why we
    should keep scaling up language models because you get these abilities that are
    really hard to find otherwise„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the context around this is pretty important because it's really expensive
    to continue scaling up these models and you even like one year ago„ÄÇa lot of people
    didn't believe that you could do better on certain tasks just by scaling up the
    size of the language model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: They sort of if you work in industry at allÔºå there's like this interesting tension
    between emergence and also like many production tasks„ÄÇso emergence is sort of
    this like task general phenomena where you scale up the model and it's like really
    expensive but the single model can do a lot of tasks is sort of like a in the
    direction of AGI„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then for many production tasksÔºå you have sort of the opposite where you
    know what task it is„ÄÇfor exampleÔºå translating into SpanishÔºå and then you have
    these constraints on compute because you know when we will translate„ÄÇfor exampleÔºå
    you don't want people to have to wait a couple seconds just to get the translation„ÄÇAnd
    then you also happen to have a lot of in domain dataÔºå so you haveÔºå for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like you know a million pairs of English Spanish sentences to train on„ÄÇAnd this
    is like sort of the opposite setting where you don't really care about the model's
    emergence„ÄÇyou can just train a very small model on the data and do one of the
    task without having to use a lot of compute„ÄÇAnd the final point is that I think
    a really promising research direction if anyone is interested in doing research
    is to like work on predicting future emergent abilities and I haven't seen a lot
    of work on it recently just because I think maybe it's too hard for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like you could only like predict emergence for a specific task or like one way
    of predict emergence might not be super general and so I haven't seen much work
    on that but I think this is a pretty promising direction to work on and maybe
    Anthropic is working on it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I don't know„ÄÇOkayÔºå greatÔºå any questions on that before I move on to chain of
    promptmptonÔºüYeah‰Ω¢„ÄÇüòä„ÄÇ‰Ω†ÂÖàÊòØ„ÄÇÁ≥ªÊàëÊúâÂë¢Êù°ÈÉΩ‰øæËßÅ„ÄÇWhich parametersmateurs are best scale to get
    like properties„ÄÇis obviously there are many different options for where„ÄÇ‰ªÄ‰πàÊÑèÂ§ñÁöÑ„ÄÇDidt
    GÔºå for example„ÄÇI want to be back there„ÄÇIs that mostly something we just test„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we find out which ones scale battery gives result or like„ÄÇYeah„ÄÇI would say
    that we don't have very principled methods for like how to scale these architectures„ÄÇËØ∂„ÄÇüòä„ÄÇI'm
    not an expert in thisÔºå but some of it has to deal with like how many parameters
    you can fit onto a particular TPU„ÄÇbut in general I think you scale up like the
    number of intentions heads and embeddings like somewhat proportionately but yeah
    I think this is like an open research question because you you can't really do
    ablations over these pretraining„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can't really do ablations over pretrainingÔºå it's hard to sort of you know
    have any principled way of doing it other than some engineers who are in charge
    of like doing it saying okay„ÄÇI think this is the right thing to do and then it
    kind of works when we go with it„ÄÇYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Do you have any indication of the asymptootic behavior of this gambling„ÄÇif you
    would expect that eventually you would you know reach either some plateau of flip
    finite but non zero loss„ÄÇor it would just go all the way down to zero„ÄÇYeahÔºå that's
    a great question„ÄÇI think„ÄÇI mean there's you mean on like perplexity or like on
    a particular task or just in general on like an next word prediction Well seems
    like these are also pretty general pretty task independent rightÔºü
  prefs: []
  type: TYPE_NORMAL
- en: It's like emergent scaling„ÄÇYeahÔºå but you knowÔºå if you take the limit of the
    infinite parameters„ÄÇthen even analyticallyÔºå is there any sense of whether that
    how to„ÄÇYeah„ÄÇI have no clue I think if if like for most of these tasks there's
    like a limit to accuracy like 100% for example„ÄÇso there's some there's some sort
    of asympote there„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but I guess the deeper question that you might be asking is like can a language
    model like perfectly no like„ÄÇYou know how to predict the next word for any given
    input„ÄÇAnd maybe likeÔºå I mean„ÄÇI guess there's some likeÔºå limit to like„ÄÇLike if
    I say a sentence„ÄÇthere are like two possible next words or something„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and you might not be able to guess that perfectly„ÄÇSo I think there's some limit„ÄÇbut
    like I think we're far from reaching that limit and there's still a lot of unsolved
    tasks that sort of indicate that there's a lot of headroom„ÄÇYeahÔºå if researchers
    are interested in studying emergence„ÄÇwhat family of different sized models is
    publicly available or best for studyingÔºüYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: good question„ÄÇSo I think the open AI API has like a lot of language models and
    we actually use that a lot even at Google it's used to study emergence and that's
    sort of one way of doing it and actually a lot of these models are currently free
    they're rate limited but they're free so that so we also use that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think there's also„ÄÇSmaller language models like for example„ÄÇthere's like a
    UL2 model that's like 20 billion parameters„ÄÇbut I guess you're right there is
    sort of this challenge where like small language models you won't see a lot of
    these emergent behaviors so you kind of have to either train„ÄÇYeahÔºå you should
    kind of have to either use like open AI API for now or wait until people train
    larger models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I guess there's also the bloom and like you guys probably know better than the
    OPT models that are publicly available but I haven't seen a lot of experiments
    on them yeah yeah„ÄÇÂóØ„ÄÇSo like my question isÔºå are thereÔºå are there emerge abilities
    that are accessible and lower parameter„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I can part to speech„ÄÇP to recognition„ÄÇshould maybe there might be some better
    maybe not like chain of thought„ÄÇbut I heard or some that„ÄÇYeahÔºå definitelyÔºå I think
    in the paper„ÄÇwe had like the list of couple dozen abilities that would be emerging
    at like 8 billion parameters or like 60 billion parameters or something like that„ÄÇyeah„ÄÇYeah„ÄÇYeahÔºå
    we have two questions from zoom„ÄÇ The first question isÔºå do you see strategy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Between the larger tech„ÄÇDiffering systematically in studying these models„ÄÇor
    basically everyone taking the same approach„ÄÇËØ∂„ÄÇI wouldn't say that everyone is
    taking the same approach„ÄÇI think„ÄÇAs one exampleÔºå Anthropic takes like a very safety
    centric approach and they're super interested in like emergent abilities because
    there could be emergent abilities that are undesirable and they want to predict
    those types of things„ÄÇI also don't know what happens at other companies other
    than at Google„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so I can't really speak too much to that„ÄÇÊúâ„ÄÇquestions„ÄÇWhat are some examples
    of tasks or abilities that have not yet emerged„ÄÇeven in models like Lada and Cha
    GT etcÔºüOh yeahÔºå I have maybe I'll show this real quick„ÄÇ![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: ËØ∂„ÄÇ![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_5.png)
  prefs: []
  type: TYPE_NORMAL
- en: There's like a nice listÔºå somewhere„ÄÇSo„ÄÇSo yeahÔºå so basically what we did is„ÄÇThere's
    like 200 tasks in Big benchn„ÄÇAnd then we basically classified them and so like„ÄÇ![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: Smoothly increasing emergent with GP3 or lambdaÔºå emergent with palm and then
    flat„ÄÇwhich is like no model better than random So I think if you look at any of
    these tasks here„ÄÇThey should still not have emerged yetÔºå and if you can get them
    to mergeÔºå that'd be interesting„ÄÇ‰ªî„ÄÇI think chat should be T 20 question„ÄÇOh okayÔºå
    yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this is not a super I think this is like a couple months old„ÄÇYeahÔºå yeah„ÄÇOhÔºå
    20 questionsÔºå Okay„ÄÇyeah„ÄÇSomeÊÑè„ÄÇYeahÔºå I think„ÄÇLike the cool thing is like you can
    see over time right like originally like maybe only like these were you know emergent
    and then when Po came out you'd see a couple dozen moreab abilitiesities became
    emergent and then you know I suspect you know in a year or two most of these will
    become emergent and we need harder benchmarks Yeah there's another question on
    why doesn't Google take as much of a safety center„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Like we said in dropping„ÄÇAre there reasons to believe powerful capabilities
    wouldn't be emergingÔºü
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå I don't want to answer the question on behalf of Google„ÄÇI just can only
    talk about my own opinions„ÄÇBut I think the reality is that Google„ÄÇeven if you
    look at like the amount of research that Google does it might not be in the large
    language models space specifically„ÄÇbut like the amount of safety research that
    we do I think is more than enthropic if you actually look at like the number of
    papers published„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: don't quote me on thisÔºå but I think that's correct„ÄÇOkay„ÄÇËØ∂ great„ÄÇSo„ÄÇYeah„ÄÇI'll
    talk about chain of thought prompting so basically chain of thought prompting
    is this way of doing reasoning multistep reasoning with large language models
    and„ÄÇYeahÔºå I wanted to say that it's super exciting to see like a lot of people
    at Google working on this and also to see Sdar CEO present this at our last year's
    Google IoC event„ÄÇÂóØ„ÄÇAnd basicallyÔºå the motivation for this is that we want language
    models to do like more complicated tasks that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you knowÔºå for exampleÔºå we know language models can do easy tasks like sentiment
    analysis or translation„ÄÇbut what about like more complicated tasks that might
    even take a human emit minute or more to doÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And the goal here is to basically guide them with metadata so for example„ÄÇinstead
    of just giving like an input output of pair„ÄÇwe want to give them the entire reasoning
    process and have them mimic that„ÄÇAnd basically you can see hereÔºå you knowÔºå in
    a standard prompt you have like the question and then the answer and then you
    have a question the model gives a new answer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: unfortunately it's wrong„ÄÇAnd then with chain of thought prompting„ÄÇyou give the
    model a question and then kind of like how your teacher would ask you to show
    your work„ÄÇyou give like the chain of thought is what we call it or basically a
    reasoning path and then you give the final answer and then when the model sees
    this unseen question„ÄÇnow it's able to give the reasoning path and then give the
    correct final answer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the way that we add these prompts into the prompt is basically we just manually
    write a couple and then add it into the prompt„ÄÇSo let me just show how that works„ÄÇSo
    this was the Open Air API„ÄÇAnd basically„ÄÇHere's like the non chainF thought way
    of doing itÔºå so basically you would have„ÄÇQuestion answer„ÄÇquestion answerÔºå question
    answerÔºå and then new question aboutÔºå you knowÔºå cafeteria asked3 apples„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they use 20 to make lunch and about six more how many apples they have„ÄÇAnd the
    model gets it wrong„ÄÇAnd the only difference with chain of thought is that you
    give these intermediate reasoning paths„ÄÇBefore giving the final answerÔºå so here's
    a pathÔºå there's a reasoning chain„ÄÇthere's another reasoning chain„ÄÇAnd then now
    the model for this unseen question„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Gives the entire reasoning processÔºå and then this actually enables them model
    to get it correct„ÄÇI'll give another quick exampleÔºå this one„ÄÇSo here the task is
    just take the last letters of the words and Bill Gates so like L from Bill and
    Ss from Gates and then concatenate them and the answer should be LS„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then here the model gets it wrong„ÄÇThe answer should be N„ÄÇSays SK„ÄÇAnd then
    if you do chainF thoughtÔºå obviously this it becomes very easy for the model„ÄÇso
    you know it says the last letter of bill is LÔºå the last letter of gates is S answers
    LS„ÄÇAnd then here it's able to do the last letter of Elons M and the last letter
    of musk is K and answer is N K„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ËØ∂„ÄÇSo any is this clear any questions about what's going on hereÔºüOkay„ÄÇÂóØ„ÄÇSo basically
    we can have these similar plots where the X axis is the model scale„ÄÇthe y axis
    is the performance so on the left we have this mathboard question benchmark called
    GSMAK it's basically like questions that you'd see in like an elementary school
    math test„ÄÇAnd you can see the blue dot is standard and the purple star is chain
    of dot and basically you see that the chain of thought if you use a large enough
    model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: does a lot better than standard prompting„ÄÇAnd actually beats the fineine Tus
    Stay of the art at the time„ÄÇA similar example is on this benchmark called Str
    QA„ÄÇAnd what strategy here is it's basically like this world knowledge plus common
    sense reasoning benchmark„ÄÇso the question would be like can you hide a basketball
    in a Sancat's ear and then the model would say you know a basketball is about
    this size„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: a Sancat's ear is that server would not fit and now this benchmark you can also
    see that we can beat the fine tune save the art from before just by using chain
    of thought with a large enough language„ÄÇSo one way we use this is that we evaluated
    champf thought on a certain subset of Big benchch tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we created a subset called Big benchch hard and basically it's like 23 challenging
    tasks from Big benchch where like no model had done better than the average human
    rateer„ÄÇSo the way that you prompt the model is that you'd have like a task description
    question options„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: chain of dotÔºå and then the test time question„ÄÇAnd so I'll give a couple of examples
    of like tasks here„ÄÇSo one example is navigate basically what the language model
    has to do in this task is it has to basically follow these so the question is
    like„ÄÇif you follow these instructions do you return to the starting pointÔºå turn
    leftÔºå turn right„ÄÇtake five stepsÔºå take four stepsÔºå turn aroundÔºå take nine steps„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then the model following the fusile exemplars is able to like basically
    track state after all of the actions„ÄÇand then at the end it saysÔºå okayÔºå we at
    the final answer„ÄÇso answer are we at the original locationÔºå mean if it is zero
    is0 then the answer is yes„ÄÇËØ∂„ÄÇJust give an example of another taskÔºå here's a task
    that's like very easy for humans basically word sorting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so like there's a list of words BurleyÔºå BelaÔºå I'm not going to read them and
    basically the model has to sort them alphabe order„ÄÇAnd here the model can follow
    the future exempÔºå so you have this pretty complicated like chain of dot where
    the model has to like sort each of the subparts and then finally it gets to the
    final answer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is correct„ÄÇSo here's sort of this result summary on this subset of Big
    Bennch„ÄÇso you can see okay we have two metricsÔºå one is just the average performance
    on all these tasks and the second is the percent of tasks that are above the average
    human rateer so average human rateer is 67 max human rateer is 94„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then prior resultsÔºå the model was doing like way worseÔºå it was like 50„ÄÇand
    this is sort of by construction of this subset„ÄÇAnd then we use Code Vinci O2„ÄÇwhich
    is like one of the open AI models and actually you can use this one for free with
    the open API„ÄÇAnd basicallyÔºå if you do answer only prompting without chainF thought„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then you sort of are being the average rater on like five of 27„ÄÇbut if you use
    chainF thought promptingÔºå then the performance increases by this pretty decent
    amount and you're able to pass the human average human other majority of tasks„ÄÇAnd
    then below is just this visualization of the tasks that are doing worse than humans
    in red and then better than humans in blue„ÄÇ2 questions„ÄÇ BoyÔºå isn't this similar
    to R HF SÔºå at least„ÄÇHiss what similarÔºü
  prefs: []
  type: TYPE_NORMAL
- en: I can change up on own for not„ÄÇË¢´Êã≥Âà∞„ÄÇÂóØ„ÄÇYeahÔºå I think it's there„ÄÇI wouldn't call
    it similar„ÄÇso like chainF thought is basically you take a pre trade language model
    and you use a prompting technique that includes intermediate reason path„ÄÇThe way
    that RLHF works is that you have this additional data that you want to fine tune
    the model on and you have a preference model that sort of predicts like how well
    does a certain output„ÄÇHow likely is that to be preferred by humans and then RLHF
    what that does is it tune it fine tunes like the language model to do well on
    the preference models prediction„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so basically it's sort of aligning the model with what humans would prefer„ÄÇIs
    there a second questionÔºü„ÇÑ„ÅÑ„Åß„Åô„ÄÇOkay Grace asksÔºå can China be includedd in fine tuning
    rather than having a„ÄÇYesÔºå the short answer is yes„ÄÇThe sort of complicated thing
    about that is that you have to have like chain of thought intermediate steps and
    those are pretty„ÄÇit can be costly to sort of to gather that data and to do the
    fine„ÄÇOne last question„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: sorry for everybody another student asksÔºå do you think that chain of thought
    and prompt engineering in general is just an artifact that won't be necessary
    with larger scale models that are better able to understand the function„ÄÇYeah
    so that's a great question basically the question is like how like ephemeral is
    like prompt engineering going to be I think we'll find out but some initial intuitions
    are that like for easy tasks that are like you know easy to describe and maybe
    there multiple choice larger models will probably be more robust to prompt engineering
    and there's sort of less you can do with that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But I think as language models get more powerful„ÄÇIt'll sort of be more normal
    to use them on a lot more challenging tasks and in those tasks you'll have to
    specify exactly what you want the model to do„ÄÇet ceÔºå so I think there'll still
    be some room for prompt engineering there at least in near future„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå correct you know how this general for exampleÔºå you a simple„ÄÇAnd then the
    other one is concerning sorting the words„ÄÇYeahÔºå so mean see that„ÄÇGive the channel
    thought like words of that„ÄÇYeah that's a great question so for some tasks where
    you've seen similar data and pretraining„ÄÇthe model can do really well even if
    the chain of thought is from another taskÔºå so for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like math word problems you actually don't really need a math chain of thought
    because mod already knows how I do that but like for a task like this„ÄÇYou probably
    haven't seen any data that's like the chain of thought here„ÄÇso without task specific
    exemplars you probably wouldn't do super well on tasks like this without manually
    writing them for other examples„ÄÇYeah„ÄÇas the researcher behind thisÔºå like what
    mental model would we do to like even try this like do you pursue the model as
    like if I was a person„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: how do I think it's better or is it like trying to give it more likeÔºüCompute
    in order to like„ÄÇBefore deesttiy answered„ÄÇYeahÔºå great question„ÄÇI think my motivation
    was just thinking about it says you said like what's going on in sort of a human's
    mind while they try to solve this math question and well if you notice like at
    least some humans will think actually in natural language so like if you just
    like think about like if you pay attention a lot to like what's going on in your
    mind you' actually notice that you sometimes you think language and so while the
    language model can thinking language too so that was kind of the motivation behind
    asking language while to do that and„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think one thing that sort of went well is that the development of like this
    technique actually coincided with like the development of palm and so„ÄÇYeahÔºå basically
    having the model palm sort of allowed us to do a lot better tasks or a lot more
    challenging tasks using chain of thought„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Yeah„ÄÇÊàëÂê¨Ëöä theÁ¨¨‰∏Ä‰∏™„ÄÇWe're saying that it matters like the absolute number of examples
    of like this chain of thought process or whatever in the data set„ÄÇOr the function„ÄÇs
    that the main significant thing or is it like this a relative numberÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Fqu of like those examples are just like negative examples that are like not
    good examples of how to reason„ÄÇDo those matter as much as the absolute number
    of„ÄÇÂïäÔºåËøôÁ¨¨3„ÄÇYeahÔºå good question„ÄÇSo I guess the challenging thing is like we can't
    really measure how many similar examples are in the training set you know„ÄÇIt's
    it's hard to do that well„ÄÇ and I don't think anyone has done that before„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it's more of this open question of like why China F thought even works because
    you actually don't see„ÄÇSimilar data like that in the training set„ÄÇYeahÔºå I think
    it's a open question like why it works„ÄÇwhat is your model„ÄÇTheition that mean said
    okay„ÄÇThink about like how„ÄÇthings„ÄÇThinking language and then knowledge do that
    too„ÄÇ But likeÔºå how do you actually think like in like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: what a situation for the model„ÄÇI meanÔºå is make a shift in„ÄÇFor a specific task„ÄÇlike
    some weights get like more„ÄÇÂ•ΩÈÄÅÁøªËøáÂöüÊâ£Èõ®ÁÇπÂïä„ÄÇYeahÔºå I don't really think about it in terms
    of like what's going on in the weights„ÄÇI guess the way that I think about it is
    that like„ÄÇIt'd be unfair for me to like give you a math question and ask you to
    give me the answer within like half a second„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is basically like what you're doing with the model and when you don't
    do a chain of that right' basically asking this like challenging question and
    the model doesn't have enough compute to like solve it in one pass to give you
    the next answer immediately„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think the second thing that„ÄÇI sort of think about is that like the model has
    learned like a compositional set of skills during during pretraining so maybe
    it hasn't really learned like you know this particular navigate task during pre
    traininging but's learn other things right it's learn like okay if you take five
    steps and you're facing this maybe yeah you should add five here or something
    like that right and it's learned how to do pattern matching so„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Maybe in the future exemplars it can match sort of what the reasoning path is
    with like what the question was and sort of there's sort of these little skills
    that the model might know and then maybe if you can combine them together in some
    clever way then you can get the model to solve the more challenging problems„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Okay„ÄÇran how much time do we haveÔºüÂì¶ÔºåÂéü‰πüÂÆ°ÁúãÁöÑÁúã„ÄÇokayÔºå50 okay„ÄÇOkay„ÄÇ„Å£„Å¶„ÄÇËØ∂ okayÔºå great„ÄÇA
    a good example of how we judge this cut„ÄÇAnywayÔºå a bunch of different answersÔºå
    all them are right„ÄÇYeah„ÄÇOkayÔºå greatÔºå yeahÔºå feel free to keep asking questions
    if you have any„ÄÇSo„ÄÇYeah„ÄÇhere's another example of emergence so basically you can
    see there's three models here instructTBT codex and palm chain of thought in blue
    non chainF thought is is in gray and then you can see you actually have to have
    sufficient model scale to get chainF thought to work well and„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I guess the intuition here is that like if you have a really small model„ÄÇthe
    model sort of will keep repeating itself for not saying anything coherent or navigate
    a final answer„ÄÇwhich is why using chain of Thought for the small models doesn't
    really work well and then for the large models obviously like for multistep problems„ÄÇthe
    model is going to be able to solve the task at a lot higher accuracy with chain
    of thought„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And another cool thing about chain of Though is there are sort of some tasks
    where you sort of wouldn't get emergent behavior at all„ÄÇso emergence hasn't been
    unlocked yetÔºå but you can see that the if you use chainF thought you can unlock
    this emergent performance in smaller models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: One example here is like multistep arithmetic where like I don't know if you'll
    ever„ÄÇyou know maybe I don't want to say everÔºå but like you can it's hard to imagine
    a model like getting this„ÄÇyou know here's the question and then the next token
    is correct that's pretty hard to solve in one step but with chain of thought you
    can get like you know 50% accuracy on this just by having the model output' these
    intermediate intermediate recent steps„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Oh yeah„ÄÇThis is something that like needs a rehatuition about what„ÄÇly„ÄÇI know
    that like a transformer can definitely do addition„ÄÇ„Åß„Åæ„Åü„ÄÇCan like take in the numbers
    and like do the carries„ÄÇDefitelyÔºå YeahÔºå yeahÔºå but like„ÄÇÂì¶„ÄÇThen there's this question
    of likeÔºå what happens empiricallyÔºå rightÔºü And like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I understand that likeÔºå it isn't necessarily a lot space to do public protect„ÄÇYeah„ÄÇSo
    like my question is likeÔºå how„ÄÇI really„ÄÇTell the difference„ÄÇLike maybe there are
    there like ways to tell the difference between like things that haven't emerged
    because like„ÄÇthere's just like no space„ÄÇOr like like like there's so many tasks
    that like it couldn have like allotted any any space to specifically do that one
    versus like like the task is so hard that like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It just can'tÔºå even if you will„ÄÇYeahÔºå yeahÔºå that's a good question I think„ÄÇThere
    seems to be like some subset of tasks where it's just like doesn't fit well with
    the way that we train language models„ÄÇso for example like„ÄÇIn language modelsÔºå
    we use tokensÔºå rightÔºü And so if you give it like„ÄÇThe token  four actually doesn't
    take the number four„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it takes like this embedding that's like you know 1000 dimensions or something
    or if you like give it a word and ask it to reverse like the letters„ÄÇthis is like
    a super easy testÔºå but like the way we train the model doesn't actually look at
    the letters and stuff„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I think there's a certain subset of tasks where like it doesn't really just
    fit well with the way that we train transformers and„ÄÇYou can actually like I mean
    I think these if you really care about these tasks you can just solve them using
    like code or something like that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but yeah I think I don't think like this is really an inherent„ÄÇSomething that
    would never emerge because it's too hardÔºå yeah„ÄÇYeah„ÄÇwe have a question on zoom
    also by the wayÔºå sorry I forgot to mention somebody asking you repeat repeat the
    questions because they can't always Oh okay„ÄÇthat's why that that's why that so
    the question someone asked is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: do you think chain of thought would be a viable interpretability technique for
    very advanced AI systemsÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And they mentioned that there's some research by called externalized reasoning
    oversight by camera La„ÄÇWill it be a viable interpretability technique for advanced
    day onÔºüYeah„ÄÇam I supposed to repeat this sorry oh so the question is„ÄÇCan chain
    of thought be a viable interpretability technique for AI„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think there's no guarantee that like„ÄÇThe chain of thought is how the model
    actually arrives at the final answer„ÄÇbut often you can use it to like sort of
    debug like why isn't the model getting this question correct or like what can
    we do better in the chain of thought to help the model get this correctÔºü
  prefs: []
  type: TYPE_NORMAL
- en: I haven't read the anthropic paper that was mentionedÔºå so I actually don't know
    the answer to that„ÄÇÂóØ ok k „ÄÇAnother interesting result that we had here was that
    you can actually do like multilingual chain of thought prompting and so basically
    what we had is like we translated this like you know benchmark of math word problems
    to 10 languages and then we prompt them the model to do it in like say Bengali
    and then the model has like basically do the math problem in Bengali and give
    the final answer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think the cool thing about this is that like this input is like highly improbable
    right so like Benngali is like 0„ÄÇ01% of the pre trainingion data and you know
    math word problems are probably even smaller subset of that„ÄÇüòäÔºåAnd basically the
    interesting thing is the model can actually do like these types of questions pretty
    well to probably surprising degree„ÄÇso like you know if you ask people before I
    showed them this result like oh how well can the model do you like these math
    questions in Swahli„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like probably like 10%Ôºå but actually even like you know very underrepresented
    languages like Swahli or Bengali or Telegu and TI„ÄÇthe model can do to like surprisingly
    well despite the fact that they only occupy like a very small subset of the pretraining
    data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂØπ„ÄÇthis and most of my experience with this is G„ÄÇbut like if you ask things in
    different languages„ÄÇdespite not being like explicitly trained these languagesÔºå
    right„ÄÇit seems to have sort of like derived reasoning independent languageÔºå to
    that extent„ÄÇYeah you can do the reasoning actually kind of funny sometimes it
    always looks like it does the reasoning in English and then translates back to
    the other language answers it gives you sort of like if you like reason the English
    and then translate to the other thing„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you think that like learning the like structure of a language and learning
    reasoning abilities or like somewhat separate large language models or that like
    it inherently will like learn chain of thought reasoning within that language
    within the structure of the language like the way thought works in that language„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Yeah that's a great question I'm not sure how to measure that„ÄÇbut I've definitely
    thought about I think the language I mean„ÄÇbased on these results like you definitely
    you probably didn't have any math questions in Swwakili for the model to learn
    from and I think definitely there's something language agnostic going on where
    the model learns reasoning sort of independently of the language and then it can
    express it in different languages if it needs to Yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but I don't have a I don't think anyone I don't think we know the answer to
    that yet„ÄÇËØ∂„ÄÇYeah„ÄÇso so basically like one question that comes up frequently is
    like why does scaling up improve chain of dot and one way of looking at this is
    like we can take a smaller model like P 62b and see like what types of errors
    are fixed from scaling up to 540 billion parameters and you can see that like
    for these three categories that we came up with some of all of them get fixed
    so scaling seems to have this like sort of universal effect on improving different
    types of errors from smaller models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then here's the same handwi who weav diagram and expressed in different
    ways so basically you have like some tasks that are doable with standard prompting
    so in blue and then the goal of chain of thought prompting is to sort of increase
    the set of tasks that we can do so for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: now the ones shown in pink include math word problemÔºå symbolic reasoning and„ÄÇChallenging
    common sense reason„ÄÇYeahÔºå questionÔºå have you done anys to figure out how like
    much is„ÄÇis any of this contribution just because of the fact that you do more
    computations when you put in longer prompts like you know„ÄÇlike you multiple tasks
    through the model you create multiple inbeddings to likeÔºå you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: just things and models looking at in a wayÔºå YeahÔºå how much would that like to
    be tried non shade of thought prompts with like saying open length„ÄÇYeahÔºå yeah„ÄÇwe
    tried with like X XÔºå X X X or something and doesn't really it doesn't work„ÄÇSo
    I think it's not just about the compute„ÄÇ I think it's about„ÄÇThe language guiding
    the model as part of the reasoning„ÄÇI see„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: have you tried like describing the problem in more sales not being shown„ÄÇI know„ÄÇthis
    is like a super I'm just very curious about likeÔºå So it's like a very interesting„ÄÇProperty
    and emergency is like to be„ÄÇYeahÔºå you mean like describing the question in three
    different ways and seeing if that describing more instead of explicitly step by
    step and seeing that Yeah„ÄÇI haven't tried thatÔºå but I would be surprised if that
    worked„ÄÇestDid you try having it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'll put the answer and then explain its reasoning into that YeahÔºå that doesn't
    work as well„ÄÇYeah„ÄÇbut it depends on the task also so like yeah yeah me„ÄÇ„Åì„ÅÑ„Å§„ÄÇThat
    seems to be the caseÔºå yeah„ÄÇYeah„ÄÇthose reactions be like reasoningÔºå it would be
    like just any all that's your calculation„ÄÇSort of imagine if it was the answer„ÄÇLike
    in a wayÔºå you knowÔºå like in a way„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like an in a of chain of soil is like a very structured„ÄÇIt like„ÄÇwhat if the
    same structures like we do some more randomly„ÄÇYeahÔºå you could try„ÄÇI would be surprised
    if it works I think like outputting tokens is pretty important for the model yeah„ÄÇOkay„ÄÇËØ∂„ÄÇSo
    we're doing on time„ÄÇ OkayÔºå great„ÄÇ So the last part„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think is a pretty cool trick with chain of thought„ÄÇSo basically„ÄÇüòäÔºåÂóØ„ÄÇWhat people
    usually do is they'll just generate one chain of thought and then they'll take
    the final answer but there's' this nice trick called self consistency where you
    can use like temperature sampling with the model to generate like a bunch of different
    reasoning pads and final answers and then if you just take a majority of vote
    over the final answers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this ends up like improving performance by like a pretty big margin so„ÄÇFor exampleÔºå
    here„ÄÇyou can see on GSMAKÔºå which is like the MathW Pro data setÔºå the improvement
    goes from like you know„ÄÇthe performance like 56 and then if you do self-con then
    it becomes 74„ÄÇwhich's like a pretty big improvement„ÄÇÂØπ„ÄÇYeahÔºå here how many are
    the averaging number for self consistency Oh„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think 40„ÄÇSo it increases the cost of the inference time computeÔºå but„ÄÇYeah„ÄÇimprove
    performance by might not to answer thisÔºå I'm curious to know how many samples
    or how many chains does one need to draw to get a significant what is the trade
    off between number of chains averaged overÔºü
  prefs: []
  type: TYPE_NORMAL
- en: I think it depends on the sorryÔºå the question is how many chains do you need
    to get a performance gain I think„ÄÇThe answer really depends on the data setÔºå but
    usually you can get something good with like 16„ÄÇI think„ÄÇYeah„ÄÇIm sorry„ÄÇWe have
    a question„ÄÇHow does the temperature change the way the model work„ÄÇOh okayÔºå the
    question is how does the temperature change the where the model works„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: basically when you use temperature decodingÔºå the language model can like stochasticically
    pick one of the outputs instead of always picking the highest probability next
    word„ÄÇso basically you get these like more stochastic outputs that are still based
    on what the language model has learned„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it's just a little bit more random„ÄÇOkay„ÄÇAnd then like finally like yeah
    selfconsistency also seems to be emergeibility„ÄÇI guess part of it is because chain
    of thought is emergingnt because you wouldn't get any better than random performance
    without doing chain of thought„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but yeahÔºå you kind of see this big delta from self-consency through larger models„ÄÇÂóØ„ÄÇüéºGreat„ÄÇso
    I'm gonna run out of time„ÄÇLet me just go to„ÄÇI'll just talk about this a little
    bit so I think in addition to just purely scaling up the language model which
    is only available to like people in industry„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think there's like a couple interesting directions to to work on one is like
    better prompting and characterization of language modab abilitiesities I think
    right now we're sort of just at the edge of you know knowing what the best way
    to prompt language models is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: There's also like pretty good applied workÔºå so like you can use language models
    I've heard to train therapists to help with creative writing to help with science„ÄÇI
    think chatGT has really shown what language models can do in this regard„ÄÇI think
    benchmarks are also something that's pretty lacking because I think we solve benchmarks
    pretty quickly„ÄÇfor example Palm beat the average human on Big benchch you know
    within a year or something of big bench coming out and so I think we need more
    benchmarks and I think that's going to be an important contribution„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then the final one is likeÔºå how can we likeÔºüHave computer fission methods
    to make language models better so that it's less expensive to use them and more
    for get to use them„ÄÇGreatÔºå so I'll end here and feel free to email me if you have
    any feedback and if you're interested in Google„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: yeah feel free to as„ÄÇüòä„ÄÇ![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_9.png)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_10.png)'
  prefs: []
  type: TYPE_IMG
