- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P13：13.Emergent Abilities and Scaling in
    LLMs - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT/Transformer 原理介绍（中英文双字幕）- P13：13.大型语言模型的涌现能力和扩展 - life_code - BV1X84y1Q7wV
- en: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_0.png)'
- en: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_1.png)'
- en: Okay， great so the first sort of paper I'm going to be talking about is called
    emergent abilities of large language models and this paper was especially cool。I
    think because we got people from Google in a Deep Mind and also at Stanford you
    might recognize Percy or Tatsu or Riishhi。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，那么我要谈论的第一篇论文是关于大型语言模型的涌现能力，这篇论文特别酷。我认为，因为我们从谷歌、Deep Mind 和斯坦福得到了人们的参与，你可能会认识
    Percy、Tatsu 或 Riishhi。
- en: I mean we got people to sort of agree on what's a nice framework of looking
    at why we want to scale and emergent abilities。So。One of the things that we've
    seen throughout language models is that you sort of get these predictable gains
    as a result of scaling。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，我们让人们在为什么要扩展和涌现能力方面达成了一种美好的框架。因此，我们在整个语言模型中看到的一件事是，随着规模的扩大，你确实会得到这些可预测的收益。
- en: so here's the canonical you know Kaplan et all paper where you can see that
    if you scale up the size of the language model measured either in compute in data
    set size or in parameters。You see that the loss on the test set actually goes
    down predictably I don't know if you're screen sharing so people on zoom I don't
    think can see the slides Okay all these okay。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这里是你知道的经典 Kaplan 等人的论文，你可以看到，如果你扩展语言模型的规模，无论是计算资源、数据集大小还是参数数量，你会发现测试集上的损失实际上是可预测的。我不知道你是否在分享屏幕，所以在
    Zoom 上的人可能看不到幻灯片。好吧，这些都没问题。
- en: O。I guess I'll say this for the third time， as we've seen in language models。If
    you scale up the size of the language model measured either in compute。in data
    set size or a number of parameters， you can see that there's a sort of this predictable
    improvement in the test loss。嗯。Now， what I'm going to be talking about in terms
    of emergence is something that's actually unpredictable。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: O。我想我要再说一次，因为我们在语言模型中已经看到过。如果你扩展语言模型的规模，无论是计算资源、数据集大小还是参数数量，你都可以看到测试损失有一种可预测的改善。嗯。现在，关于涌现的讨论实际上是关于一些不可预测的事物。
- en: If you only look at smaller language models， so one way that emergence has been
    described in the broader science literature is it's basically seen as a qualitative
    change that arises from quantitative changes and it sort of started with this
    article in science by a Nobel Prize winning physicist called More is different。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果只看较小的语言模型，那么涌现在更广泛的科学文献中被描述为一种从数量上的变化而产生的质的变化，它的起源可以追溯到一位诺贝尔奖获得者的科学文章，称为“更多的不同”。
- en: And I really like this post from Jacob Steinart that sort of describes emergence
    and he gives a couple of good examples here。For example， he says with uranium，
    with a bit of uranium， nothing special happens。with a large amount of ur impact
    densely enough， you get a nuclear reaction。And then also with DNA。for example，
    given only small molecules such as calcium。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我真的很喜欢 Jacob Steinart 的这篇文章，他在这里描述了涌现，并举了几个很好的例子。例如，他说对于铀来说，如果只有一点点铀，没有什么特别的事情发生；但如果密集地影响到足够多的铀，你会得到核反应。还有
    DNA 的例子，例如，如果只有钙这样的小分子。
- en: you can't meaningfully encode useful information， but given larger models such
    as DNA。you can encode a genome。So for this particular work。we use this definition
    of emergent abilities of large language models in particular。so we say that ability
    is emergent if it is not present in smaller models。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能有意义地编码有用的信息，但是对于像 DNA 这样的更大模型，你可以编码基因组。因此，对于这项特定工作，我们使用了大型语言模型特别的涌现能力定义。因此，我们说，如果某种能力在较小的模型中不存在，那么它就是涌现的。
- en: but it is present in larger models。And the sort of natural question here is
    like how do you measure the size or the scale of the language model and there's
    sort of traditionally three axes of scale so the training flops are the amount
    of compute that you use to train the language model。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但是它存在于更大的模型中。这里的一个自然问题是，如何衡量语言模型的大小或规模，传统上有三个规模的维度，即训练时的计算资源、用于训练语言模型的计算量。
- en: The number of model parameters are like the size of the language model and also
    the size of the training data set that the model is trained on。And a lot of the
    plots here will use either training flops or the number of model parameters。the
    reason is that the training dataset size is usually fixed for different size models
    and because training flops is just the data set size times model parameters。you
    can get a similar plot from either training flops or number of model parameters
    for most language models。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 模型参数的数量就像是语言模型的大小，以及模型所训练的数据集的大小。这些图中的很多将使用训练的浮点运算数量或模型参数的数量。原因是对于不同规模的模型，训练数据集的大小通常是固定的，而训练的浮点运算数量只是数据集大小乘以模型参数。对于大多数语言模型，你可以从训练的浮点运算数量或模型参数数量得到类似的图。
- en: Great， and so the first type of emergence， yes， I go。きくだ。Yeah， for me。it seems
    nice like it'd be relatively easier to measure the size versus it's like what
    quaifies。You find okay。就是。哪里。Yeah， sure。So yeah， for example。' I'll just give
    an example here which is actually the next slide。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，所以第一种涌现类型，是的，我继续。きくだ。对我来说，这似乎很不错，测量规模相对简单，而不是确定什么是合格的。你发现好吧。就是。哪里。好的，当然。所以是的，举个例子。我将在这里给出一个例子，实际上是下一张幻灯片。
- en: so basically we have this way of interacting with language models called fu
    shot prompting and the way it works is like you know the language model is a really
    good next word predictor。And when you give the model an example， and then you
    ask it for an unseen like movie review for example。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们与语言模型交互的一种方式叫做fu shot提示，其工作方式是你知道语言模型是一个非常好的下一个词预测器。当你给模型一个例子，然后询问它一个未见过的电影评论时。
- en: and then you say what's the output and then here the language model can say
    positive because it understands to use the context from the review to give the
    next token。And the definition of let that we use for like having ability or not
    is that basically a few shot prompted like tasks。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你会问，输出是什么。在这里，语言模型可以说“积极”，因为它理解如何利用评论中的上下文来给出下一个标记。我们用来判断是否具备能力的定义基本上是几个镜头提示的任务。
- en: for example， sentiment analysis， is emergent if it has random accuracy for small
    models。but above random accuracy for large models。Does that make sense。so basically
    if the model isn't doing any better than random。then we say it doesn't have the
    ability to do this particular task。个。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，情感分析，对于小模型来说，如果它的准确率是随机的，那么它是涌现的；但对于大模型来说，则高于随机准确率。这样说有道理吗？基本上，如果模型的表现没有比随机好，那么我们就说它没有能力去执行这个特定的任务。
- en: And I'll give a few examples here。嗯。So here's sort of the canonical way that
    we look at plots for emergence。so basically what we have each of these different
    plots is a different task and I'll go over some examples soon。But the way you
    read the plot is the X axis is a number of training flops or the model scale。and
    then the Y axis is like the accuracy or like how good the model is doing the task。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里给出几个例子。嗯。这里是我们看待涌现图的典型方式。基本上，每个不同的图都是一个不同的任务，我很快会举几个例子。但是你阅读图表的方式是，X轴是训练的浮点运算数量或模型规模，而Y轴则是准确率或模型在执行任务时的表现。
- en: And then know we have different language models from Open AI from Google and
    from Deep Mind。and then each of the points is like a different language model
    it's not a language model over the course of training like each point is a different
    language model。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有来自OpenAI、Google和DeepMind的不同语言模型，每个点代表一个不同的语言模型，而不是一个在训练过程中变化的语言模型。
- en: And what you see is that for the very small language models。you basically get
    performance that's close to random or not being any better than random。And then
    once you pass a certain threshold。You can see that the performance suddenly gets
    like a lot above substantially above random and this is what we call emergence
    so basically if you were to extrapolate like the lines from the small language
    models。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到，对于非常小的语言模型，性能基本上接近随机，或者说并没有比随机表现得更好。而一旦超过某个阈值，你会发现性能突然显著高于随机，这就是我们所称的涌现。因此，如果你要从小语言模型推断这些线。
- en: you might predict that it would never know do better than random because it's
    just a flat line。but the interesting phenomenonize that when you scale up past
    a certain threshold。you actually do see this emergent phenomena where the model
    does a lot better than random。So let me go over some concrete examples。So here's
    one task。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会预测它永远不会比随机更好，因为它只是一个平坦的线条。但有趣的现象是，当你超过某个阈值时，你实际上会看到这种涌现现象，模型的表现远远好于随机。所以让我举一些具体的例子。这里是一个任务。
- en: it's basically a benchmark called multitask NLU or MLLU， and basically what
    it is。it's a bunch of test questions ranging from high school all the way to like
    professional level exams。And how it works is the language model sort of given，
    for example。here is a high school math example， and the language model is given
    like a few examples and then for an unseen question it has to give the answer。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它基本上是一个称为多任务自然语言理解（multitask NLU或MLLU）的基准，基本上它是一堆测试问题，从高中一直到专业水平的考试。它的工作方式是，语言模型被给定，例如，这里有一个高中数学例题，然后对于一个未见过的问题，它必须给出答案。
- en: And then you can see in the plot on the right， so for the model scale。if you
    go up to sort of 10 to the power of 22 training flops。you don't actually get any
    better than random accuracy on this task。but if you scale up to 10 to the 24 training
    flops。then you see that all the like three models there do much better than random
    accuracy。Yeah。perfect the scale of the data used to train this， is it roughly
    similar or because these are like different models trained by different words？
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你可以在右侧的图表中看到，对于模型规模，如果你达到大约`10^22`的训练浮点运算量，你在这个任务上的准确率实际上并没有比随机好，但如果你扩展到`10^24`的训练浮点运算量，那么你会看到所有三个模型的表现都远超随机准确率。是的。用于训练的数据规模大致相似吗？还是因为这些模型是用不同的数据训练的？
- en: Yeah， the scale is， I think within an order of magnitude for these models here
    yeah。Yeah。and like every single do。Individual tracks is the same data yes the
    data the data is fixed except for Chnchiilla。Chinchiilla uses more data for larger
    models， but I believe for all the other models here。the amount data is the same。Yeah，
    here's just another example to sort of。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我认为这些模型的规模在一个数量级之内。是的。而且每一个单独的轨道都是相同的数据，是的，数据是固定的，除了Chinchilla。Chinchilla对更大的模型使用了更多数据，但我相信这里其他所有模型的数据量都是相同的。是的，这里只是另一个例子来说明。
- en: Show it more concretely so this is a task from the big benchmark benchmark just
    as an aside the big benchmark benchmark is like 200 benchmarks and basically it's
    like a crowdsource set of benchmarks I'd recommend looking at if you're doing
    that lot of work。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地展示一下，这是一项来自大基准测试的任务，顺便提一下，大基准测试有大约200个基准，基本上是一个众包的基准集，我建议你在进行大量工作时查看一下。
- en: And basically， the task is the language model has to take an English sentence。And
    then give the international phonetic alphabet transliteration or the IPA transliteration。which
    is basically like how to pronounce it。And for this task。the evaluation metric
    is actually blue or like an Ngram overlap metric。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个任务是语言模型必须处理一个英文句子。然后给出国际音标的音译，或者说是IPA音译。基本上就是如何发音。对于这个任务，评估指标实际上是蓝色（BLEU）或者类似于N-gram重叠的指标。
- en: And you get a similar phenomenon where as you increase the size of the language
    model。it's flat for a while and then suddenly the improvement is above random。嗯。对。So
    I'll talk about another interesting result I hear that that's why it's emerging。so
    this was a technical report that we put out a couple of months ago and basically
    there's this really interesting prize in or it's like a one time prize in。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到一个类似的现象，即当你增加语言模型的规模时，起初是平坦的，随后突然间提升超过随机水平。嗯。对。所以我会谈谈我听到的另一个有趣的结果，这就是为什么它在出现。这是我们几个月前发布的一份技术报告，基本上这里有一个非常有趣的奖项，或者说是一次性奖项。
- en: In language models where Anthropics， which is like a startup。basically had this
    prize where if people could come up with a task where the performance on the task
    would actually decrease as you increase the size of the language model。then you
    would get like a bunch of money。So basically there are a lot of submissions to
    this and here's one example of like a task where they found that the performance
    would actually decrease if you increase the size of the language model。so the
    task is attribute here it's like repeat my sentences back to me and then the input
    is like all that glists is not glue and then the output is the model has to completely
    has to accurately say a glueib。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在语言模型中，Anthropics是一个创业公司，基本上设立了这个奖项，如果人们能提出一个任务，随着语言模型规模的增加，任务的表现反而会下降，那么你就能获得一笔钱。所以实际上有很多提交，以下是一个示例任务，他们发现随着语言模型规模的增加，表现确实会下降。这个任务是让你重复我的句子，输入是“所有闪光的东西并不是金子”，输出是模型必须准确地说出“金子”。
- en: 嗯。And so what happened is for the small language model。it doesn't know the phrase
    all that gllisters is not gold。so it just like copies the input and actually gets
    like 100% on that。But then for the medium size language model， what you would
    see is that the forms to actually decrease because the medium size language model
    knows the phrase all that gllylists is not gold and then it says gold。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。发生的情况是对于小型语言模型，它不知道短语“所有闪光的东西并不是金子”。所以它只是复制输入，实际上获得了100%的正确率。但是对于中型语言模型，你会看到表现实际上会下降，因为中型语言模型知道短语“所有闪光的东西并不是金子”，然后它说金子。
- en: which actually is not what the task asks it to do Yeah someone ask can you give
    a physical estimate 10 to the 24 plots possibly in terms of training time or number
    of。Yeah， so I think。10 to the 24 flops is around， so at Google we use TUs and
    one pod of TUs。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其实这并不是任务要求它做的事情。是的，有人问你能否给出10的24次方的物理估计，可能是训练时间或数量。是的，我认为10的24次方浮点运算大约是，在谷歌我们使用TU，一个TU的集群。
- en: I believe is equal to like 4000 A100s。And 10 to the 24 flops is like two pods
    for around six weeks or something like that。So it's a lot of compute to do the
    pre training。诶。I don't know。but do you guys remember in like chemistry class when
    you'd have like moles？
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这相当于大约4000个A100s。10的24次方浮点运算大约需要两个集群运行六周左右。所以为了进行预训练需要大量的计算。诶。我不知道。但是你们还记得在化学课上有过摩尔的概念吗？
- en: And it would be like 10 to the 23 and then you're like teacher would be like，
    oh。don't even think about like how big this number is。That's like the number of
    like floating point operations that goes into the pretrannous in these models。Okay，
    great anyways， so yeah so basically the medium size language model will actually
    do worse Oh yeah did you have another question great。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 然后大约是10的23次方，你的老师可能会说，哦，别去想这个数字有多大。这大约是这些模型在预训练中需要的浮点运算的数量。好的，太好了，所以基本上中等大小的语言模型实际上会表现得更差。哦，是的，你还有其他问题吗，太好了。
- en: 😊，This the prizeThis one is one of the the winner I think it's like a third
    place winner or something。す great question。But the task， because like， I would
    like my initial opinion would be like， oh。you can just like put a negative slide
    on how you evaluate。What do you mean it's flip a negative side all of this depends
    on which evaluation？Yeah。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，这个奖项是我认为的第三名或类似的。很好的问题。但这个任务，因为我最初的看法是，哦，你可以在评估上放一个负分。你是什么意思？翻转负面评估，这一切都取决于哪个评估？是的。
- en: to measure if you do your task break， so like it's like the the the measurement
    is very sparse like you only get credit if you do it。Yeah， yeah， like a lot of
    things emerge because like you just won't hit it perfectly。Really optimized for
    a long time。 Or like if you take a test and then like evaluate it with like a
    minus sign like。wouldn't it。It was。Like something that's。Yeah， I mean， they they
    so for this thing。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量你的任务完成情况，就像是这个测量非常稀疏，只有在你完成时才能获得积分。是的，很多事情都是因为你不会完美地完成。真的优化了很长时间。或者说如果你参加考试，然后用负号来评估那样的话，不是吗？是的，没错。就像是这样的东西。是的，我的意思是，他们对于这个事情。
- en: they like account it for all like， you can't just say like the task should be
    to do badly on something。it has to be like a meaningful sort of task。And then
    I guess your point about like the how credit time or the valuation metric works
    is actually a really good one yeah so I guess it still kind of counts if like
    you know I guess the argument is sort of that that。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 他们考虑到所有的事情，你不能仅仅说任务应该在某些事情上表现不佳，任务必须是有意义的。而且我想你关于信用时间或评估指标如何运作的观点确实很好，所以我想如果你知道，我猜这个论点是这样的。
- en: The performance might not look emergent if you assign partial credit。but we
    have like a bunch of I can show example later， but even if you use partial credit
    metrics。you'll often still see the same type of emergence so it's not purely a
    phenomenon of like not assigning partial credit based on the valuation metric。诶。过。And
    then great， so what what we sort of sort of argued in this paper is that yeah。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你分配部分信用，性能可能看起来并不突现。但我们有一堆示例可以稍后展示，即使你使用部分信用指标，你仍然会看到同类型的突现，因此这并不是基于评估指标不分配部分信用的纯现象。
- en: there might be some tasks where the performance starts to decrease if you use
    a medium sized language model。but if you keep scaling all the way to you the largest
    model that we have at Google that's known publicly。Palm， you'll see that the language
    model I can actually go back and do the task correctly because the large language
    model also knows the phrase all that gllisted is not gold。but it also understands
    repeat my senses back to me so it's able to get 100% on this task so this is a
    different type of emergence also。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 可能有一些任务，如果使用中等规模的语言模型，性能开始下降。但如果你一直扩展到我们在谷歌公开的最大模型Palm，你会发现这个语言模型实际上能够返回并正确执行任务，因为大型语言模型也知道“所有发光的东西并非金子”，它还理解“重复我的感官”，所以能够在这个任务上获得100%的准确率，这也是一种不同类型的突现。
- en: And another class of emergence that we sort of talk about in the paper is like
    an emergent prompting technique。So basically， you know， other than futuretop prompting。there's
    like other ways of like interacting with the language models that。Can be considered
    emergent yeah。The question is， did all modelss undergo construction fine tu？
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在论文中我们讨论的另一类突现是突现提示技术。基本上，除了未来提示，还有其他与语言模型互动的方式，可以被视为突现。问题是，所有模型是否经历了构建微调？
- en: None of these models under one instruction fine tuning for this plot。Great，
    yeah， so yeah。so one way of interacting with language models is by basically finding
    the model using a technique called RHF。and basically the way it works is you have
    this data and humans rate like preferences for what type of outputs they prefer。And
    then the model strand on RL to sort of optimize for human preferences。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型在这个图上都没有进行一次指令微调。很好，是的。所以，与语言模型互动的一种方式是基本上使用一种称为RHF的技术来找到模型。基本上，它的工作原理是你有这些数据，人与人之间会评估他们偏好的输出类型。然后模型在强化学习中进行训练，以优化人类的偏好。
- en: And what this plot is showing here is that if you do this RHF on the model。The
    model performance on a different zero shot task actually gets worse for small
    models。you can see the blue line is above the orange line， blue line is the baseline
    line。the orange line is RHS。And then if you do it for large models， though。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图展示的是，如果你对模型进行RH微调，模型在另一个零样本任务上的性能实际上会对小模型变差。你可以看到蓝线在橙线之上，蓝线是基线，橙线是RHF。如果你对大型模型进行同样的操作。
- en: then you can see that the performance actually has a positive delta from doing
    RLHI。诶。And so this is sort of interesting thing where like a certain technique
    might only help if you try on a large enough language model。so if you only try
    it on the small language models。it' would be tough to draw the conclusion that
    it wouldn't help performance。And then later。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你会看到，实际上通过进行RLHI，性能有了正向增量。这是一个有趣的事情，因为某种技术可能仅在你尝试大型语言模型时才有效。如果你只在小型语言模型上尝试，很难得出它不会帮助性能的结论。然后，稍后。
- en: I'll talk about chain of without prompting as another emergent prompt。So here's
    sort of the hand wavy diagram that I sort of used to think about emergence as
    a framework so on the X axis here there's like a scale of the language model and
    on the Y axis is a sort of imaginary like you know。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我将讨论无需提示的链作为另一种突现提示。因此，这里是我用来思考突现作为框架的一个大致图示，X轴是语言模型的规模，Y轴是某种想象的东西，你知道的。
- en: A scale of like a range of things that a language model can do。And then basically
    you can pick like some random point like say 100 billion parameters in the language
    model and there will be certain abilities and okay。so first you can see as you
    increase the size of the language model。the number of like tasks or things the
    language model can do increases and then you can see there are some tasks where
    like models above 100 billion parameters for example can do them。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一系列语言模型可以做的事情。然后基本上你可以选择某个随机点，比如说在语言模型中 1000 亿参数，将会有某些能力。因此，你可以看到，随着语言模型规模的增加，模型可以完成的任务数量也在增加，然后你可以看到，有些任务是超过
    1000 亿参数的模型可以做到的。
- en: but models below 100 billion parameters can't do them and we call these emergent
    abilities。Sorry。question Where the colors。Oh， it's just highlighting like the
    dark blue is like。Tasks that a smaller language model wouldn't be able to do。Does
    that make sense。 Yeah。and then to the right， the dotted line， the right region
    up top。Oh。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但低于 1000 亿参数的模型无法做到这些，我们称之为涌现能力。抱歉。问题是颜色在哪里。哦，它只是突出显示，比如深蓝色是像小型语言模型无法完成的任务。这样说有道理吗？是的。然后右侧，虚线，上方的区域。哦。
- en: that just means like tasks that we haven't been able to solve yet with models
    yet。个。And I'm curious to know， do you think that it's not that those tasks in
    the white region are unsolvable at like $100 billion scale？
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅意味着我们尚未能够解决的任务。而我很好奇，你认为在 1000 亿规模下，那些白色区域的任务是不可解决的吗？
- en: Or do you think that better models。Specific training data would allow us to
    the 100 billion scale to get into that。Yeah， I definitely think that it's not
    a fixed， I'll give an example shortly but it's not it's not。You know it's not
    a rule that you have to have 100 blend parameters to do a certain task。it's just
    that that happens to be the threshold that we've observed in models so far and
    I do think with better training data and architecture and algorithms we can probably
    beat that。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为更好的模型。特定的训练数据会让我们达到 1000 亿规模吗？是的，我绝对认为这不是固定的，我会很快给一个例子，但这不是规则。你知道，不是说你必须拥有
    1000 亿参数才能完成某项任务。这只是我们观察到的模型阈值，而我确实认为，通过更好的训练数据、架构和算法，我们可能会超越这个阈值。
- en: 过嗯。Yeah， so as Rland just mentioned， one example of Getty emergence can be with
    better data。so it's not all about scale I'll sort of give some nuance here。so
    for this task is just one of the tasks in the Big Ben benchmark。you can see that
    for like Lambda which is a Google model and GPT3。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。是的，正如 Rland 刚刚提到的，涌现的一个例子可以是更好的数据。所以并不是所有都是规模，我会稍微给点细微差别。对于这个任务来说，它只是 Big
    Ben 基准中的一个任务。你可以看到，像 Lambda 这样的谷歌模型和 GPT3。
- en: you actually don't get emergence from scaling to 137 or  137 or 175 billion
    parameters。But when you come in with a different language model palm。which is
    trained on better data than Lambda and GT3。you actually can get this emerge ability
    even with the smaller language model shown here at 62 billion parameters。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上不会从扩展到 137 或 175 亿参数中获得涌现。但当你使用不同的语言模型 Palm 时，它在比 Lambda 和 GT3 更好的数据上训练，你实际上可以获得这种涌现能力，即使是这里显示的
    62 亿参数的小型语言模型。
- en: See inventory better model as better data， or also better。Ttro masking， you
    know。choices or most interesting。Yeah， so the challenging thing is that's a great
    question。There's like a lot of differences between Palm and Lambda， for example。and
    we can't really abllate them in any controlled way because of the cost of pretraining。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 将库存更好的模型视为更好的数据，或者更好。Ttro 掩蔽，你知道的。选择或最有趣的。是的，挑战在于这是一个很好的问题。比如，Palm 和 Lambda
    之间有很多差异。我们无法以任何受控方式消除它们，因为预训练的成本。
- en: but our like sort of running hypothesis is that P is trained on better data
    and that accounts for a lot of the difference between Palm and Lambda like the
    smaller it is possible to stuff Yeah。这个。Yeah， that's a good question， so I guess
    even here you can look at like for example。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们的假设是，P 在更好的数据上训练，这解释了 Palm 和 Lambda 之间的许多差异。像是更小的模型也有可能。是的，这个。是的，这是个好问题，所以我想即使在这里，你也可以看看，比如说。
- en: the Palm 8 billion model。Like that。That point there。you can abllate it and it's
    like a little bit higher。but it's not really an emergent yet at that point。so
    it's it's hard to tell for you know for example， this particular task what the
    effect is。Yeah。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Palm 80 亿模型。像是那个点。你可以消融它，稍微高一点，但在那个点上还不是涌现。所以，对于这特定任务来说，效果很难判断。是的。
- en: there's a question on Zoom， are there two different versions of Paul， if not。why
    are there two lines more？Oh so I think the two lines here。one is like maybe three
    shot and then one is like zero shot something like that。so it just refers to the
    way that we're using the language model either with the without exemplars。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Zoom上有个问题，Paul是否有两个不同的版本，如果没有，为什么有两条线？哦，所以我认为这两条线，一个可能是三次示例，另一个可能是零次示例之类的。因此它只是指我们在使用语言模型时是否有示例。
- en: 嗯。Great。I'll talk about a yeah， small ablation here that sort of shows this。so
    this is an ablation on sort of a toy task where basically the language model has
    to know that like in English。you have to use plural verbs with plural subjects
    and singular verbs with singular subjects。And the way that what we're doing here
    is basically we train like these small BRT models from scratch and then we held
    out like we fixed the frequency of certain verbs in the training data set。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，太好了。我会谈论一个小的消融实验，基本上展示了这一点。这是一个玩具任务的消融实验，语言模型需要知道在英语中，复数主语要用复数动词，单数主语要用单数动词。我们在这里做的就是从头开始训练这些小的BRT模型，然后我们固定了一些动词在训练数据集中的频率。
- en: which basically says like， okay， what's the effect of seeing a certain verb
    in the data more often？
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上是在说，好的，看到某个动词在数据中更频繁的效果是什么？
- en: In this plot the x axis is like the frequency of the verb and the y axis is
    the error rate and what you basically see is that if you have more in domain data
    so if the model sees the verb more times it does the task a lot better and this
    is sort of an example of like having high quality data or data that's more in
    domain for the task that you're evaluating on can make a big difference。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，x轴类似于动词的频率，y轴是错误率，你基本上看到的是，如果你有更多的领域内数据，也就是说如果模型看到动词的次数更多，它执行任务的效果会好得多，这实际上是高质量数据或与评估任务更相关的数据可以产生巨大差异的一个例子。
- en: even if you're fixing the compute the size of the model and the rest of the
    data。Yeah question on Zoom。Someone asks， could there be a way to dispillel emerget
    abilities down to smaller models for larger teacher。Yeah， I think so so。Larger
    teacher models can basically you can use them for example。to generate data and
    then if you finet larger the smaller model on data。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你固定了计算、模型大小和其他数据。是的，Zoom上有人问，是否有办法将新出现的能力缩小到更小的模型以供更大的教师使用。是的，我想可以。因此，更大的教师模型基本上可以用来生成数据，然后如果你在数据上对更小的模型进行微调。
- en: it's pretty likely that you'll be able to get the ability to emerge in the smaller
    model I'll talk about example of this too let me see。Oh， actually， next slide。Desired
    behaviors can be induced in smaller models once you sort of know what behavior
    you want。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 很可能你能够在更小的模型中获得出现的能力，我也会谈论这个例子的，等我看看。哦，实际上，下一张幻灯片。一旦你知道想要什么行为，就可以在更小的模型中诱导出期望的行为。
- en: so for example， here's the instructorstruct here's a figure from thestructGPT
    paper。And basically。the desired behavior here is like instruction following。And
    you can see that there's multiple models， so on the left you have these small
    models that are trained with RLHF and they actually have better performance than
    larger models train on weaker techniques so basically the point is like if you
    know that you want a certain behavior that sort of you saw previously emerge in
    an emergent way in a larger model you can find a way to fine tune on that behavior
    specifically and induce that behavior in a smaller model。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里是来自thestructGPT论文的插图。基本上，这里期望的行为是遵循指令。你可以看到有多个模型，左侧是这些使用RLHF训练的小模型，它们的表现实际上比使用较弱技术训练的大模型要好。因此，重点是，如果你知道你想要某种行为，这种行为曾在更大的模型中以涌现的方式出现，你可以找到一种专门微调该行为并在更小的模型中诱导该行为的方法。
- en: I guess one of the limitations is that like unless you know like all the behaviors
    that you want。you can't really get this natural emerge behavior。Yeah。another sort
    of discussion point here is that like。There's this question of like what's the
    right X axis for emergence so like right now we mostly talk about like model parameters
    and training flops but like I guess you could like if you ask Deepmind people
    like how they look at it。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我想其中一个局限性是，除非你知道你想要的所有行为，否则你真的无法获得这种自然出现的行为。是的，另一个讨论点是，关于“什么是出现的正确x轴”的问题，所以现在我们主要讨论模型参数和训练的浮点数，但我想如果你问Deepmind的人，他们会如何看待这个问题。
- en: you'll sort of get this argument that model parameters and training flops are
    really just a proxy for like how good the model is and how good the model is can
    really be measured by like perplexity or like how well it's doing some on some
    dev sets such as WikiTex 103。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你将会得到这样一种论点，模型参数和训练计算量实际上只是衡量模型质量的一个代理，模型的好坏可以通过困惑度或它在某些开发集上的表现来真正衡量，比如WikiTex
    103。
- en: So basically， you can also measure。Emergnce in terms of perplexity so here is
    WikiTex perplexity and then you can see like on a downstream task that as the
    perplexity gets better。there's sort of this threshold where you're able to do
    a lot better on a downstream task。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，你也可以通过困惑度来衡量。这里是WikiTex困惑度，然后你可以看到在下游任务中，随着困惑度的改善，存在一个阈值，使你能够在下游任务上表现得更好。
- en: And there's sort of the strong correlation right at least right now between
    perplexity and training compute so you can see like these two lines are are pretty
    similar and。Basically， I think in the future， if we have much better models that
    are a lot smaller trying on much better data and better algorithms。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 至少目前，困惑度和训练计算之间有很强的相关性，所以你可以看到这两条线是非常相似的。基本上，我认为在未来，如果我们有更好而且更小的模型，训练在更好的数据和更好的算法上。
- en: then maybe WikiText complexity can show a different type of plot than using
    other metrics。So wki textex is basically a， I think it's like a subset of Wikipedia。And
    then perplexity is like a measure of how well you can predict the next word in
    a data set。So basically， if you're really good at modeling the next word on this
    like particular evaluation set。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，或许WikiText的复杂性可以展示出与使用其他指标不同的图表。所以WikiTex基本上是我认为是维基百科的一个子集。而困惑度是衡量你在数据集中预测下一个单词的能力。因此，如果你在这个特定评估集上非常擅长建模下一个单词。
- en: that's sort of a measure of like how well you understand language。Make sense。But
    wouldn' it just harass。Oh， this is like a held out test set。对。好。嗯。And then a final
    thing that I think is like pretty exciting about emergence。Is that there's sort
    of not just like technical emergence that we've talked about。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这在某种程度上是衡量你理解语言的能力。明白了吗？但是这样不是有点强迫吗？哦，这是一个保留的测试集。对。好。嗯。最后，我认为关于出现的事情是非常令人兴奋的，不仅仅是我们所谈论的技术性出现。
- en: but there's sort of sociological changes in how the AI community views like
    scaling and how to use language models。So here's some examples of。Where scaling
    up the size of the language model enables you to in the sort of few shots scenario。beat
    a task specific fine tune language model that's usually fine tune on say thousands
    of examples。So basically the green line is the prior state of the art achieved
    by fine tuning and if you just and then the blue dots basically show if you take
    a pre-trained language model and you do fu shot prompting。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，AI社区在看待规模和如何使用语言模型方面，社会学上发生了一些变化。这是一些示例，说明在少量样本场景中，扩大语言模型的规模使你能够超越一个通常在数千个示例上进行微调的任务特定微调语言模型。因此，绿色线是通过微调实现的前状态，而蓝点基本上显示的是如果你使用预训练语言模型进行少量提示。
- en: which means the language model isn't intentionally trained to do the task。you
    can often get stay of the art results just by continuing to scale up the size
    of the language model。And obviously there's limitations here you don't want to
    just keep scaling up in order to get save of the art。but I think it's a pretty
    big change in people's minds that you could actually get some of the best results
    just by scaling up the size and language model and doing prompt。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着语言模型并不是专门训练来完成这个任务的。你往往只需继续扩大语言模型的规模，就可以获得最先进的结果。显然，这里有局限性，你不想只通过扩大规模来获得最先进的结果。但我认为人们的思维方式发生了很大变化，你实际上可以通过扩大语言模型的规模和进行提示获得一些最佳结果。
- en: Question from Zoom。someone ask， is that not a contradiction graph from two to
    three slides ago。What is that。Which one， this one？I'm sure shouldn't be in general
    assume he said yes。ok。We said。should we in general assume that scale tru's lie
    to。Yeah so that's a great question so this plot is saying that you fine tune and
    you can do and okay yeah so it depends on your like particular。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Zoom上的问题。有人问，这不是从两到三张幻灯片前的矛盾图吗？那是什么？哪个，这个吗？我确定一般不应该假设他说的是“是”。好的，我们说。我们是否一般假设规模真的会说谎？是的，这是个很好的问题，所以这个图表表明你可以微调，并且可以，这取决于你的具体情况。
- en: Task， but what this plot is saying is that。Like。We're not like fine tuned smaller
    models can do well on some tasks if you target it well。but for like tests that
    are more complicated， often you can do better just by scaling so there's sort
    of。Tasks that fall into both of these categories， and I wouldn't say that。It's
    contradictory。I guess some tasks。You would do a lot better just by scaling the
    sizeizing the model and then other tasks。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 任务，但这个图表所说的是。就像。我们并不是说精细调优的小模型在某些任务上表现良好，如果你针对得当。但对于那些更复杂的测试，通常通过扩展规模你能获得更好的结果，因此这些任务可以归入这两个类别，我并不认为这是矛盾的。我想某些任务，确实可以通过扩大模型的规模获得更好的结果，而其他任务。
- en: if it's like a very narrow domain or the large language model might not be trained
    on that kind of data。then you would do better by fine。O，对。So here's sort of a
    little summary slide。so basically emergent abilities can only be observed in large
    models and if you try to predict their emergence just by looking at the plots
    for small models then you wouldn't be able to do it。And I sort of had a little
    reflection on how to look at this。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是一个非常狭窄的领域，而大型语言模型可能没有在那种数据上训练，那么通过精细调整你会获得更好的结果。对。所以这里有一个小总结幻灯片。基本上，涌现能力只能在大型模型中观察到，如果你仅仅通过查看小模型的图表来预测它们的涌现，那么你是无法做到的。我对如何看待这一点也进行了一些反思。
- en: so emergence is really this framing of like how to view new abilities that are
    not intentionally built in to the pretraining and I think the subtext for this
    is super important。which is like you can see it as implicit argument for why we
    should keep scaling up language models because you get these abilities that are
    really hard to find otherwise。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以涌现实际上是如何看待那些未在预训练中有意构建的新能力的框架，我认为这背后的潜台词非常重要。也就是说，你可以把它看作是一个隐含的论据，说明我们为什么应该不断扩展语言模型，因为这样你会得到那些在其他情况下很难找到的能力。
- en: And the context around this is pretty important because it's really expensive
    to continue scaling up these models and you even like one year ago。a lot of people
    didn't believe that you could do better on certain tasks just by scaling up the
    size of the language model。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 而围绕这个背景是相当重要的，因为继续扩展这些模型的成本非常高，甚至在一年前，很多人都不相信通过扩大语言模型的规模可以在某些任务上表现更好。
- en: They sort of if you work in industry at all， there's like this interesting tension
    between emergence and also like many production tasks。so emergence is sort of
    this like task general phenomena where you scale up the model and it's like really
    expensive but the single model can do a lot of tasks is sort of like a in the
    direction of AGI。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在行业中工作的话，会存在一种有趣的紧张关系，既涉及涌现现象，又涉及许多生产任务。所以涌现是一种任务的普遍现象，当你扩展模型时，成本会很高，但单一模型可以执行很多任务，这在某种程度上是朝着AGI的方向发展。
- en: And then for many production tasks， you have sort of the opposite where you
    know what task it is。for example， translating into Spanish， and then you have
    these constraints on compute because you know when we will translate。for example，
    you don't want people to have to wait a couple seconds just to get the translation。And
    then you also happen to have a lot of in domain data， so you have， for example。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于许多生产任务，你有一种相反的情况，你知道它是什么任务。例如，翻译成西班牙语，并且你对计算有这些限制，因为你知道翻译时机。例如，你不希望人们等待几秒钟才能获得翻译。而且你也恰好有很多领域内的数据，比如说。
- en: like you know a million pairs of English Spanish sentences to train on。And this
    is like sort of the opposite setting where you don't really care about the model's
    emergence。you can just train a very small model on the data and do one of the
    task without having to use a lot of compute。And the final point is that I think
    a really promising research direction if anyone is interested in doing research
    is to like work on predicting future emergent abilities and I haven't seen a lot
    of work on it recently just because I think maybe it's too hard for example。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 比如，你知道有一百万对英语和西班牙语的句子可以用来训练。这就像是相反的情况，你并不太关心模型的涌现，你可以只用数据训练一个非常小的模型来完成某个任务，而不必消耗大量计算资源。最后一点是，我认为这是一个非常有前途的研究方向，如果有人对研究感兴趣，可以尝试预测未来的涌现能力，而我最近并没有看到太多相关的工作，可能是因为这太困难了，例如。
- en: like you could only like predict emergence for a specific task or like one way
    of predict emergence might not be super general and so I haven't seen much work
    on that but I think this is a pretty promising direction to work on and maybe
    Anthropic is working on it。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 像你只能为特定任务预测出现，或者预测出现的一种方式可能不是特别普遍，所以我没见过太多关于这一点的研究，但我觉得这是一个相当有前景的研究方向，也许Anthropic正在研究这个。
- en: I don't know。Okay， great， any questions on that before I move on to chain of
    promptmpton？Yeah佢。😊。你先是。系我有呢条都俾见。Which parametersmateurs are best scale to get
    like properties。is obviously there are many different options for where。什么意外的。Didt
    G， for example。I want to be back there。Is that mostly something we just test。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我不知道。好的，很棒，在我继续谈论提示链之前，有什么问题吗？嗯，佢。😊。你先是。系我有呢条都俾见。哪些参数最佳规模来获取类似的属性。显然，这里有许多不同的选择。什么意外的。比如说，Didt
    G。我想回到那里。这主要是我们测试的内容吗？
- en: And we find out which ones scale battery gives result or like。Yeah。I would say
    that we don't have very principled methods for like how to scale these architectures。诶。😊。I'm
    not an expert in this， but some of it has to deal with like how many parameters
    you can fit onto a particular TPU。but in general I think you scale up like the
    number of intentions heads and embeddings like somewhat proportionately but yeah
    I think this is like an open research question because you you can't really do
    ablations over these pretraining。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们发现哪些扩展的电池给出了结果，或者像那样。嗯。我想说我们并没有非常原则性的方法来扩展这些架构。诶。😊。我不是这方面的专家，但有些内容与能够适配到特定TPU上的参数数量有关。但总体上我觉得你在一定程度上要成比例地扩展意图头和嵌入数量，但我认为这是一个开放的研究问题，因为你不能真正对这些预训练进行消融。
- en: you can't really do ablations over pretraining， it's hard to sort of you know
    have any principled way of doing it other than some engineers who are in charge
    of like doing it saying okay。I think this is the right thing to do and then it
    kind of works when we go with it。Yeah。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上不能对预训练进行消融，想要有任何原则性的方法是困难的，除非一些负责的人工程师说，好的。我认为这就是正确的做法，然后当我们这样做时，它就能正常工作。嗯。
- en: Do you have any indication of the asymptootic behavior of this gambling。if you
    would expect that eventually you would you know reach either some plateau of flip
    finite but non zero loss。or it would just go all the way down to zero。Yeah， that's
    a great question。I think。I mean there's you mean on like perplexity or like on
    a particular task or just in general on like an next word prediction Well seems
    like these are also pretty general pretty task independent right？
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有对这种赌博的渐近行为有任何指示。如果你预期最终会到达某个稳定点，翻转有限但非零的损失，或者它就会一路降到零。嗯，这个问题很好。我觉得。我是说你是说在困惑度上，还是在特定任务上，或者只是一般来说像下一个单词预测。好像这些也是相当普遍的，比较独立于任务，对吧？
- en: It's like emergent scaling。Yeah， but you know， if you take the limit of the
    infinite parameters。then even analytically， is there any sense of whether that
    how to。Yeah。I have no clue I think if if like for most of these tasks there's
    like a limit to accuracy like 100% for example。so there's some there's some sort
    of asympote there。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是涌现的扩展。嗯，但你知道，如果你取无限参数的极限。那么即使在分析上，也有没有任何意义。嗯。我不知道，如果对于大多数这些任务，准确性是有限的，比如说100%。所以那里有某种渐近线。
- en: but I guess the deeper question that you might be asking is like can a language
    model like perfectly no like。You know how to predict the next word for any given
    input。And maybe like， I mean。I guess there's some like， limit to like。Like if
    I say a sentence。there are like two possible next words or something。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 但我想你可能在问更深层的问题是：语言模型是否可以完美地知道如何预测任何给定输入的下一个单词。也许，我的意思是。我想有某种限制。如果我说一个句子，可能有两个可能的下一个单词之类的。
- en: and you might not be able to guess that perfectly。So I think there's some limit。but
    like I think we're far from reaching that limit and there's still a lot of unsolved
    tasks that sort of indicate that there's a lot of headroom。Yeah， if researchers
    are interested in studying emergence。what family of different sized models is
    publicly available or best for studying？Yeah。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 而且你可能无法完美预测。所以我认为这是有一些限制的。但我觉得我们离达到那个限制还很远，仍然有很多未解决的任务表明还有很多提升空间。嗯，如果研究人员有兴趣研究出现，哪个不同规模的模型是公开可用或最佳的研究选择？嗯。
- en: good question。So I think the open AI API has like a lot of language models and
    we actually use that a lot even at Google it's used to study emergence and that's
    sort of one way of doing it and actually a lot of these models are currently free
    they're rate limited but they're free so that so we also use that。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 好问题。所以我认为OpenAI的API有很多语言模型，我们实际上在谷歌也使用得很多，它被用来研究新兴现象，这就是一种方法，实际上很多模型目前是免费的，虽然有限制，但它们是免费的，所以我们也使用它。
- en: I think there's also。Smaller language models like for example。there's like a
    UL2 model that's like 20 billion parameters。but I guess you're right there is
    sort of this challenge where like small language models you won't see a lot of
    these emergent behaviors so you kind of have to either train。Yeah， you should
    kind of have to either use like open AI API for now or wait until people train
    larger models。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为还有更小的语言模型，比如说，像UL2模型，它大约有200亿个参数。但我想你是对的，确实存在这样一个挑战，即小语言模型不会出现很多新兴行为，因此你需要训练。是的，你可能需要现在使用OpenAI
    API，或者等待人们训练更大的模型。
- en: I guess there's also the bloom and like you guys probably know better than the
    OPT models that are publicly available but I haven't seen a lot of experiments
    on them yeah yeah。嗯。So like my question is， are there， are there emerge abilities
    that are accessible and lower parameter。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我想也有一些新的能力，像你们可能比公开的OPT模型更了解这些，但我还没有看到很多实验，嗯。所以我想问的是，是否有可以访问的、参数较低的新兴能力。
- en: I can part to speech。P to recognition。should maybe there might be some better
    maybe not like chain of thought。but I heard or some that。Yeah， definitely， I think
    in the paper。we had like the list of couple dozen abilities that would be emerging
    at like 8 billion parameters or like 60 billion parameters or something like that。yeah。Yeah。Yeah，
    we have two questions from zoom。 The first question is， do you see strategy。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以进行语音识别。可能会有一些更好的，可能不是像思维链那样，但我听说过一些。是的，确实，我认为在论文中，我们列出了几打将在约80亿参数或600亿参数左右出现的能力。是的，是的。我们在Zoom上有两个问题。第一个问题是，你看到策略了吗。
- en: Between the larger tech。Differing systematically in studying these models。or
    basically everyone taking the same approach。诶。I wouldn't say that everyone is
    taking the same approach。I think。As one example， Anthropic takes like a very safety
    centric approach and they're super interested in like emergent abilities because
    there could be emergent abilities that are undesirable and they want to predict
    those types of things。I also don't know what happens at other companies other
    than at Google。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在较大的技术之间，系统性地研究这些模型，还是基本上大家采取相同的方式。诶。我不会说每个人都采取相同的方法。我认为，作为一个例子，Anthropic采取了非常以安全为中心的方法，他们对新兴能力非常感兴趣，因为可能会出现一些不希望出现的新兴能力，他们想预测这些情况。我也不知道其他公司发生了什么，除了谷歌。
- en: so I can't really speak too much to that。有。questions。What are some examples
    of tasks or abilities that have not yet emerged。even in models like Lada and Cha
    GT etc？Oh yeah， I have maybe I'll show this real quick。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_3.png)
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我不能多谈这个。有问题。有哪些任务或能力尚未出现，即使在像Lada和ChatGPT等模型中？哦，好的，也许我会迅速展示这个。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_3.png)
- en: 诶。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_5.png)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 诶。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_5.png)
- en: There's like a nice list， somewhere。So。So yeah， so basically what we did is。There's
    like 200 tasks in Big benchn。And then we basically classified them and so like。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_7.png)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个很好的列表，在哪里。所以，基本上我们所做的是。在Big Bench上有200个任务。然后我们基本上对它们进行了分类，所以像这样。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_7.png)
- en: Smoothly increasing emergent with GP3 or lambda， emergent with palm and then
    flat。which is like no model better than random So I think if you look at any of
    these tasks here。They should still not have emerged yet， and if you can get them
    to merge， that'd be interesting。仔。I think chat should be T 20 question。Oh okay，
    yeah。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随着GP3或lambda的平滑增加，新兴能力与Palm相关，然后是平坦的。这就像没有模型比随机更好。所以我认为如果你看这些任务，它们仍然应该尚未出现，如果你能让它们合并，那就很有趣。嗯。我认为聊天应该是T
    20问题。哦，好的，嗯。
- en: this is not a super I think this is like a couple months old。Yeah， yeah。Oh，
    20 questions， Okay。yeah。Some意。Yeah， I think。Like the cool thing is like you can
    see over time right like originally like maybe only like these were you know emergent
    and then when Po came out you'd see a couple dozen moreab abilitiesities became
    emergent and then you know I suspect you know in a year or two most of these will
    become emergent and we need harder benchmarks Yeah there's another question on
    why doesn't Google take as much of a safety center。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是超级新的，我认为这大概几个月前的事情。是的，是的。哦，20个问题，好的。是的。有点意思。是的，我认为。很酷的事情是，你可以看到随着时间的推移，最初可能只有这些是新出现的，当Po发布时，你会看到更多的能力出现，然后我猜想在一两年内，这些大多数都会出现，我们需要更严格的基准。是的，还有另一个问题，为什么谷歌没有采取那么多的安全中心。
- en: Like we said in dropping。Are there reasons to believe powerful capabilities
    wouldn't be emerging？
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在放弃时所说的。有理由相信强大的能力不会出现吗？
- en: Yeah， I don't want to answer the question on behalf of Google。I just can only
    talk about my own opinions。But I think the reality is that Google。even if you
    look at like the amount of research that Google does it might not be in the large
    language models space specifically。but like the amount of safety research that
    we do I think is more than enthropic if you actually look at like the number of
    papers published。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我不想代表谷歌回答这个问题。我只能谈谈我自己的看法。但我认为现实是，即使你看看谷歌的研究量，它可能并不专注于大型语言模型领域。但就我们所做的安全研究的数量而言，我认为这比其他公司要多，如果你实际查看发表的论文数量。
- en: don't quote me on this， but I think that's correct。Okay。诶 great。So。Yeah。I'll
    talk about chain of thought prompting so basically chain of thought prompting
    is this way of doing reasoning multistep reasoning with large language models
    and。Yeah， I wanted to say that it's super exciting to see like a lot of people
    at Google working on this and also to see Sdar CEO present this at our last year's
    Google IoC event。嗯。And basically， the motivation for this is that we want language
    models to do like more complicated tasks that。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我不敢确定，但我认为这是正确的。好的。诶，很好。所以。是的。我会谈谈思维链提示，基本上思维链提示是一种在大型语言模型中进行多步骤推理的方法。是的，我想说看到谷歌的很多人都在研究这个，尤其是看到Sdar首席执行官在去年谷歌IoC活动中展示这个真的很令人兴奋。嗯。基本上，这个的动机是我们希望语言模型能够处理更复杂的任务。
- en: you know， for example， we know language models can do easy tasks like sentiment
    analysis or translation。but what about like more complicated tasks that might
    even take a human emit minute or more to do？
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，比如说，我们知道语言模型可以做简单的任务，比如情感分析或翻译。但更复杂的任务呢，可能甚至需要人类花费一分钟或更多的时间去做？
- en: And the goal here is to basically guide them with metadata so for example。instead
    of just giving like an input output of pair。we want to give them the entire reasoning
    process and have them mimic that。And basically you can see here， you know， in
    a standard prompt you have like the question and then the answer and then you
    have a question the model gives a new answer。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的目标基本上是通过元数据引导它们，例如。我们不仅仅想给出输入和输出的对。我们想提供整个推理过程，让它们模仿。基本上，你可以看到，在标准提示中，你有问题和答案，然后有一个问题，模型给出新的答案。
- en: unfortunately it's wrong。And then with chain of thought prompting。you give the
    model a question and then kind of like how your teacher would ask you to show
    your work。you give like the chain of thought is what we call it or basically a
    reasoning path and then you give the final answer and then when the model sees
    this unseen question。now it's able to give the reasoning path and then give the
    correct final answer。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，这是错误的。然后通过思维链提示。你给模型一个问题，然后就像老师让你展示你的思路一样。你给出我们称之为思维链的推理路径，最后给出答案。当模型看到这个未见过的问题时，现在它能够给出推理路径，并且给出正确的最终答案。
- en: And the way that we add these prompts into the prompt is basically we just manually
    write a couple and then add it into the prompt。So let me just show how that works。So
    this was the Open Air API。And basically。Here's like the non chainF thought way
    of doing it， so basically you would have。Question answer。question answer， question
    answer， and then new question about， you know， cafeteria asked3 apples。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些提示加入到提示中的方式基本上是手动写几个，然后添加到提示中。让我展示一下是如何工作的。这是Open Air API。基本上。这是非思维链的做法，所以基本上你会有。问题答案。问题答案，问题答案，然后是关于，比如说，食堂问3个苹果的新问题。
- en: they use 20 to make lunch and about six more how many apples they have。And the
    model gets it wrong。And the only difference with chain of thought is that you
    give these intermediate reasoning paths。Before giving the final answer， so here's
    a path， there's a reasoning chain。there's another reasoning chain。And then now
    the model for this unseen question。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 他们用20来做午餐，另外大约六个，问他们有多少个苹果。模型出错了。链式思考唯一的区别是，你在给出最终答案之前提供这些中间推理路径，所以这是一个路径，还有一个推理链。现在模型对于这个未见的问题。
- en: Gives the entire reasoning process， and then this actually enables them model
    to get it correct。I'll give another quick example， this one。So here the task is
    just take the last letters of the words and Bill Gates so like L from Bill and
    Ss from Gates and then concatenate them and the answer should be LS。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 提供整个推理过程，这实际上使模型能够正确回答。我再给一个快速的例子，就是这个。任务就是取单词的最后字母，比尔·盖茨的最后字母，所以L来自比尔，S来自盖茨，然后连接起来，答案应该是LS。
- en: And then here the model gets it wrong。The answer should be N。Says SK。And then
    if you do chainF thought， obviously this it becomes very easy for the model。so
    you know it says the last letter of bill is L， the last letter of gates is S answers
    LS。And then here it's able to do the last letter of Elons M and the last letter
    of musk is K and answer is N K。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里模型出错了。答案应该是N。SK说。然后如果你进行链式思考，显然这对模型来说变得很简单。它说比尔的最后一个字母是L，盖茨的最后一个字母是S，答案是LS。然后这里它能够得到埃隆·M的最后一个字母和马斯克的最后一个字母是K，答案是N
    K。
- en: 诶。So any is this clear any questions about what's going on here？Okay。嗯。So basically
    we can have these similar plots where the X axis is the model scale。the y axis
    is the performance so on the left we have this mathboard question benchmark called
    GSMAK it's basically like questions that you'd see in like an elementary school
    math test。And you can see the blue dot is standard and the purple star is chain
    of dot and basically you see that the chain of thought if you use a large enough
    model。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 诶。那么，有没有什么不清楚的地方？关于这里发生的事情有问题吗？好的。嗯。基本上我们可以有这些类似的图，其中X轴是模型规模，Y轴是性能。在左侧我们有一个叫做GSMAK的数学板问题基准，基本上像是你在小学数学测试中看到的问题。你可以看到蓝点是标准，紫星是链式思考，基本上你会看到，如果你使用足够大的模型，链式思考的表现要好得多。
- en: does a lot better than standard prompting。And actually beats the fineine Tus
    Stay of the art at the time。A similar example is on this benchmark called Str
    QA。And what strategy here is it's basically like this world knowledge plus common
    sense reasoning benchmark。so the question would be like can you hide a basketball
    in a Sancat's ear and then the model would say you know a basketball is about
    this size。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 而且实际上超过了当时的最新技术。一个类似的例子是在一个叫做Str QA的基准上。这里的策略基本上是世界知识加常识推理基准。问题可能是你能把篮球藏在一只猫的耳朵里吗？然后模型会说，篮球大约是这个大小。
- en: a Sancat's ear is that server would not fit and now this benchmark you can also
    see that we can beat the fine tune save the art from before just by using chain
    of thought with a large enough language。So one way we use this is that we evaluated
    champf thought on a certain subset of Big benchch tasks。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 猫的耳朵那样的尺寸是放不下的，现在在这个基准中你也可以看到，我们可以通过使用链式思考和足够大的语言模型来超过之前的最新技术。因此，我们的一种使用方式是，我们在某个Big
    benchch任务的子集中评估了链式思考。
- en: so we created a subset called Big benchch hard and basically it's like 23 challenging
    tasks from Big benchch where like no model had done better than the average human
    rateer。So the way that you prompt the model is that you'd have like a task description
    question options。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们创建了一个叫做Big benchch hard的子集，基本上它就像是23个来自Big benchch的挑战性任务，其中没有模型的表现超过平均人类评分者。你提示模型的方式是提供任务描述、问题和选项。
- en: chain of dot， and then the test time question。And so I'll give a couple of examples
    of like tasks here。So one example is navigate basically what the language model
    has to do in this task is it has to basically follow these so the question is
    like。if you follow these instructions do you return to the starting point， turn
    left， turn right。take five steps， take four steps， turn around， take nine steps。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 链式思考，然后是测试时间的问题。所以我给几个任务的例子。一个例子是导航，基本上语言模型在这个任务中需要做的就是遵循这些，所以问题是：如果你遵循这些指令，是否返回到起点，向左转，向右转，走五步，走四步，转身，走九步。
- en: And then the model following the fusile exemplars is able to like basically
    track state after all of the actions。and then at the end it says， okay， we at
    the final answer。so answer are we at the original location， mean if it is zero
    is0 then the answer is yes。诶。Just give an example of another task， here's a task
    that's like very easy for humans basically word sorting。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，跟随可融合样本的模型基本上能够在所有操作之后跟踪状态。最后它会说，我们得到了最终答案。那么，答案是我们是否回到了原始位置，意味着如果它是零，那么答案就是是的。诶。再举个简单的例子，这是一个对人类来说非常简单的任务，基本上是单词排序。
- en: so like there's a list of words Burley， Bela， I'm not going to read them and
    basically the model has to sort them alphabe order。And here the model can follow
    the future exemp， so you have this pretty complicated like chain of dot where
    the model has to like sort each of the subparts and then finally it gets to the
    final answer。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有一份单词列表，Burley，Bela，我不会逐一读它们，基本上模型必须按字母顺序对它们进行排序。在这里，模型可以跟随未来的示例，所以你有一个相当复杂的链式思考，模型必须对每个子部分进行排序，最后得出最终答案。
- en: which is correct。So here's sort of this result summary on this subset of Big
    Bennch。so you can see okay we have two metrics， one is just the average performance
    on all these tasks and the second is the percent of tasks that are above the average
    human rateer so average human rateer is 67 max human rateer is 94。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这是正确的。那么这是关于这个Big Bennch子集的结果总结。你可以看到我们有两个指标，一个是所有任务的平均表现，另一个是超过平均人类评分者的任务百分比，所以平均人类评分者是67，最高人类评分者是94。
- en: And then prior results， the model was doing like way worse， it was like 50。and
    this is sort of by construction of this subset。And then we use Code Vinci O2。which
    is like one of the open AI models and actually you can use this one for free with
    the open API。And basically， if you do answer only prompting without chainF thought。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在之前的结果中，模型的表现很差，大约是50。这是通过这个子集的构建得出的。然后我们使用Code Vinci O2。这是开放AI模型之一，实际上你可以通过开放API免费使用这个模型。基本上，如果你只进行回答提示而不进行链式思考。
- en: then you sort of are being the average rater on like five of 27。but if you use
    chainF thought prompting， then the performance increases by this pretty decent
    amount and you're able to pass the human average human other majority of tasks。And
    then below is just this visualization of the tasks that are doing worse than humans
    in red and then better than humans in blue。2 questions。 Boy， isn't this similar
    to R HF S， at least。Hiss what similar？
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你在27个任务中大约是平均评分者的水平。但是如果你使用链式思考提示，性能会显著提高，能够超过大多数任务中的人类平均水平。然后下面是一个可视化，显示出表现低于人类的任务用红色标出，表现优于人类的任务用蓝色标出。两个问题。伙计，这难道不和R
    HF S相似吗？嘿，是什么相似呢？
- en: I can change up on own for not。被拳到。嗯。Yeah， I think it's there。I wouldn't call
    it similar。so like chainF thought is basically you take a pre trade language model
    and you use a prompting technique that includes intermediate reason path。The way
    that RLHF works is that you have this additional data that you want to fine tune
    the model on and you have a preference model that sort of predicts like how well
    does a certain output。How likely is that to be preferred by humans and then RLHF
    what that does is it tune it fine tunes like the language model to do well on
    the preference models prediction。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以改变自己的看法，不。被拳到。嗯。是的，我想这是存在的。我不会称它为相似。像链式思考基本上是你采用一个预训练语言模型，并使用包含中间推理路径的提示技术。RLHF的工作方式是你有这些额外的数据，你想对模型进行微调，并且有一个偏好模型，预测某个输出的表现如何。某个输出被人类偏好的可能性有多大，然后RLHF的作用就是微调语言模型，以便在偏好模型的预测上表现良好。
- en: so basically it's sort of aligning the model with what humans would prefer。Is
    there a second question？やいです。Okay Grace asks， can China be includedd in fine tuning
    rather than having a。Yes， the short answer is yes。The sort of complicated thing
    about that is that you have to have like chain of thought intermediate steps and
    those are pretty。it can be costly to sort of to gather that data and to do the
    fine。One last question。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上这就是将模型与人类的偏好对齐。有第二个问题吗？やいです。好的，Grace问，中国是否可以纳入微调，而不是要有一个。是的，简短的回答是可以的。复杂的事情在于你必须有链式思考的中间步骤，这些步骤是相当复杂的。收集这些数据和进行微调可能是成本较高的。最后一个问题。
- en: sorry for everybody another student asks， do you think that chain of thought
    and prompt engineering in general is just an artifact that won't be necessary
    with larger scale models that are better able to understand the function。Yeah
    so that's a great question basically the question is like how like ephemeral is
    like prompt engineering going to be I think we'll find out but some initial intuitions
    are that like for easy tasks that are like you know easy to describe and maybe
    there multiple choice larger models will probably be more robust to prompt engineering
    and there's sort of less you can do with that。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，另一位学生问，你认为思维链和提示工程一般只是一个产物，在更大规模、能够更好理解功能的模型面前是否不再必要。是的，这是个好问题，基本上这个问题是关于提示工程将有多短暂。我认为我们会找到答案，但一些初步的直觉是，对于那些易于描述的简单任务，或许有多个选择的大型模型会更能抵抗提示工程，能做的事情就会少一些。
- en: But I think as language models get more powerful。It'll sort of be more normal
    to use them on a lot more challenging tasks and in those tasks you'll have to
    specify exactly what you want the model to do。et ce， so I think there'll still
    be some room for prompt engineering there at least in near future。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我认为随着语言模型变得更强大，使用它们处理更多挑战性任务将会更加普遍，而在这些任务中，你必须明确指出你希望模型做什么。因此，我认为在至少不久的将来，仍会有一些提示工程的空间。
- en: Yeah， correct you know how this general for example， you a simple。And then the
    other one is concerning sorting the words。Yeah， so mean see that。Give the channel
    thought like words of that。Yeah that's a great question so for some tasks where
    you've seen similar data and pretraining。the model can do really well even if
    the chain of thought is from another task， so for example。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，没错，你知道这个一般是怎样的，比如你一个简单的。然后另一个是关于排序单词的。是的，所以我的意思是看到这一点。给出思维链的像是那样的单词。是的，这是个好问题，对于一些任务，如果你已经见过类似的数据和预训练，模型可以做得很好，即使思维链来自另一个任务，例如。
- en: like math word problems you actually don't really need a math chain of thought
    because mod already knows how I do that but like for a task like this。You probably
    haven't seen any data that's like the chain of thought here。so without task specific
    exemplars you probably wouldn't do super well on tasks like this without manually
    writing them for other examples。Yeah。as the researcher behind this， like what
    mental model would we do to like even try this like do you pursue the model as
    like if I was a person。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 像数学文字问题，你其实不需要数学思维链，因为模型已经知道我怎么做，但像这样的任务，你可能没有见过类似思维链的数据。因此，没有特定任务的示例，你可能在这样的任务上表现不佳，除非手动为其他示例编写。是的。作为这个研究的背后，像我们应该用什么心理模型去尝试呢？你是否把模型当作一个人。
- en: how do I think it's better or is it like trying to give it more like？Compute
    in order to like。Before deesttiy answered。Yeah， great question。I think my motivation
    was just thinking about it says you said like what's going on in sort of a human's
    mind while they try to solve this math question and well if you notice like at
    least some humans will think actually in natural language so like if you just
    like think about like if you pay attention a lot to like what's going on in your
    mind you' actually notice that you sometimes you think language and so while the
    language model can thinking language too so that was kind of the motivation behind
    asking language while to do that and。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是否更好，或者是否更像是试图给予更多的计算，以便于。在 deesttiy 回答之前。是的，太好了，我认为我的动机只是考虑你所说的，当人类试图解决这个数学问题时，他们的脑中发生了什么。如果你注意到，至少一些人确实会用自然语言思考，所以如果你多关注一下你的脑中发生了什么，你会发现有时你在用语言思考，而语言模型也能用语言思考，因此这就是让我询问语言模型这样做的动机。
- en: I think one thing that sort of went well is that the development of like this
    technique actually coincided with like the development of palm and so。Yeah， basically
    having the model palm sort of allowed us to do a lot better tasks or a lot more
    challenging tasks using chain of thought。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为一件事情做得不错的是，这项技术的发展实际上与 palm 的发展相吻合。因此，基本上，拥有 palm 模型使我们能够更好地完成很多任务或更多挑战性任务，利用思维链。
- en: Yeah。我听蚊 the第一个。We're saying that it matters like the absolute number of examples
    of like this chain of thought process or whatever in the data set。Or the function。s
    that the main significant thing or is it like this a relative number？
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 是的。我听到的第一个。我们说这很重要，比如在数据集中这个链思维过程的绝对例子数量。或者这个功能。是主要的显著事物，还是相对数量？
- en: Fqu of like those examples are just like negative examples that are like not
    good examples of how to reason。Do those matter as much as the absolute number
    of。啊，这第3。Yeah， good question。So I guess the challenging thing is like we can't
    really measure how many similar examples are in the training set you know。It's
    it's hard to do that well。 and I don't think anyone has done that before。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这些示例中的一部分只是一些负面示例，不是很好的推理示例。这些与绝对数量一样重要吗？啊，这第3。是的，好的问题。所以我想具有挑战性的是，我们实际上无法测量训练集中有多少类似示例。你知道。这很难做到，我认为以前没有人做到过。
- en: So it's more of this open question of like why China F thought even works because
    you actually don't see。Similar data like that in the training set。Yeah， I think
    it's a open question like why it works。what is your model。Theition that mean said
    okay。Think about like how。things。Thinking language and then knowledge do that
    too。 But like， how do you actually think like in like。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这更多是一个开放的问题，比如为什么中国F思维有效，因为实际上你并没有在训练集中看到类似的数据。是的，我认为这是一个开放的问题，为什么它有效。你的模型是什么？这个意思就是好的。想想像如何。事情。思考语言，然后知识也是这样。但你实际上是如何思考的，就像在那样。
- en: what a situation for the model。I mean， is make a shift in。For a specific task。like
    some weights get like more。好送翻过嚟扣雨点啊。Yeah， I don't really think about it in terms
    of like what's going on in the weights。I guess the way that I think about it is
    that like。It'd be unfair for me to like give you a math question and ask you to
    give me the answer within like half a second。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于模型来说，这是什么情况。我是说，会在特定任务中发生转变。就像一些权重会变得更。好送翻过来扣雨点啊。是的，我并不真的考虑权重中的事情。我想我思考的方式是，给你一个数学问题，让你在半秒内给我答案，这是不公平的。
- en: which is basically like what you're doing with the model and when you don't
    do a chain of that right' basically asking this like challenging question and
    the model doesn't have enough compute to like solve it in one pass to give you
    the next answer immediately。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就像你与模型所做的事情，当你没有正确执行链思维时，基本上是在提出这个挑战性的问题，而模型没有足够的计算能力来一次性解决它，立即给你下一个答案。
- en: I think the second thing that。I sort of think about is that like the model has
    learned like a compositional set of skills during during pretraining so maybe
    it hasn't really learned like you know this particular navigate task during pre
    traininging but's learn other things right it's learn like okay if you take five
    steps and you're facing this maybe yeah you should add five here or something
    like that right and it's learned how to do pattern matching so。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为第二件事就是，我会考虑模型在预训练期间学习了一组组合技能，所以也许它并没有真正学习这个特定的导航任务，但它学到了其他东西。对吧？它学会了，比如说如果你走五步，面对这个，也许是的，你应该在这里加五，或者类似的，它学会了如何进行模式匹配。
- en: Maybe in the future exemplars it can match sort of what the reasoning path is
    with like what the question was and sort of there's sort of these little skills
    that the model might know and then maybe if you can combine them together in some
    clever way then you can get the model to solve the more challenging problems。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 也许在未来的示例中，它可以匹配推理路径和问题之间的关系，模型可能知道这些小技能，如果你能够以某种巧妙的方式将它们结合在一起，那么你就可以让模型解决更具挑战性的问题。
- en: Okay。ran how much time do we have？哦，原也审看的看。okay，50 okay。Okay。って。诶 okay， great。A
    a good example of how we judge this cut。Anyway， a bunch of different answers，
    all them are right。Yeah。Okay， great， yeah， feel free to keep asking questions
    if you have any。So。Yeah。here's another example of emergence so basically you can
    see there's three models here instructTBT codex and palm chain of thought in blue
    non chainF thought is is in gray and then you can see you actually have to have
    sufficient model scale to get chainF thought to work well and。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我们还有多少时间？哦，原来也审看的看。好的，50好的。好的。嗯，好的。一个判断这个切割的好例子。无论如何，很多不同的答案，都是正确的。是的。好的，很好，如果你有任何问题，请随时问。是的。这是另一个涌现的例子，所以基本上你可以看到这里有三个模型：instructTBT、codex和palm，链思维是蓝色，非链F思维是灰色，然后你可以看到实际上你必须有足够的模型规模才能让链F思维运作良好。
- en: I guess the intuition here is that like if you have a really small model。the
    model sort of will keep repeating itself for not saying anything coherent or navigate
    a final answer。which is why using chain of Thought for the small models doesn't
    really work well and then for the large models obviously like for multistep problems。the
    model is going to be able to solve the task at a lot higher accuracy with chain
    of thought。
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我想这里的直觉是，如果你有一个非常小的模型，模型会不断重复自己，无法给出连贯的回答或找到最终答案。这就是为什么在小模型中使用思维链效果不佳，而对于大模型，显然在多步骤问题中，模型能够以更高的准确性解决任务。
- en: And another cool thing about chain of Though is there are sort of some tasks
    where you sort of wouldn't get emergent behavior at all。so emergence hasn't been
    unlocked yet， but you can see that the if you use chainF thought you can unlock
    this emergent performance in smaller models。
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 关于思维链的另一个有趣之处是，有些任务根本不会出现突现行为。因此，突现尚未被激发，但如果你使用思维链，可以在较小模型中解锁这种突现性能。
- en: One example here is like multistep arithmetic where like I don't know if you'll
    ever。you know maybe I don't want to say ever， but like you can it's hard to imagine
    a model like getting this。you know here's the question and then the next token
    is correct that's pretty hard to solve in one step but with chain of thought you
    can get like you know 50% accuracy on this just by having the model output' these
    intermediate intermediate recent steps。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个例子是多步骤算术，我不知道你是否能做到。你知道，也许我不想说永远，但很难想象一个模型能完成这个。你知道这是问题，然后下一个令牌是正确的，这在一步内解决起来相当困难，但通过思维链，你可以仅通过让模型输出这些中间步骤来达到约50%的准确率。
- en: Oh yeah。This is something that like needs a rehatuition about what。ly。I know
    that like a transformer can definitely do addition。でまた。Can like take in the numbers
    and like do the carries。Defitely， Yeah， yeah， but like。哦。Then there's this question
    of like， what happens empirically， right？ And like。
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，对。这是需要重新思考的一件事。我知道变换器肯定能进行加法。而且能接收数字并进行进位。没错，没错，但接下来有个问题，实际上会发生什么，对吧？
- en: I understand that like， it isn't necessarily a lot space to do public protect。Yeah。So
    like my question is like， how。I really。Tell the difference。Like maybe there are
    there like ways to tell the difference between like things that haven't emerged
    because like。there's just like no space。Or like like like there's so many tasks
    that like it couldn have like allotted any any space to specifically do that one
    versus like like the task is so hard that like。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我明白这不是一个很大的空间来进行公共保护。所以我的问题是，如何才能真正区分呢？也许有些方法可以区分那些因为空间不足而未出现的任务，或者任务如此之多，以至于没有专门的空间来处理那个任务。
- en: It just can't， even if you will。Yeah， yeah， that's a good question I think。There
    seems to be like some subset of tasks where it's just like doesn't fit well with
    the way that we train language models。so for example like。In language models，
    we use tokens， right？ And so if you give it like。The token  four actually doesn't
    take the number four。
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 它就是无法做到，即使你愿意。是的，是个好问题。我认为似乎有一些任务根本不适合我们训练语言模型的方式。例如，在语言模型中，我们使用的是令牌，对吧？所以如果你给它，比如令牌四，它实际上并不接收数字四。
- en: it takes like this embedding that's like you know 1000 dimensions or something
    or if you like give it a word and ask it to reverse like the letters。this is like
    a super easy test， but like the way we train the model doesn't actually look at
    the letters and stuff。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 它需要一个大约1000维的嵌入，或者如果你给它一个单词并要求它反转字母。这是一个超级简单的测试，但我们训练模型的方式实际上并没有关注字母等内容。
- en: So I think there's a certain subset of tasks where like it doesn't really just
    fit well with the way that we train transformers and。You can actually like I mean
    I think these if you really care about these tasks you can just solve them using
    like code or something like that。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为有一部分任务与我们训练变换器的方式并不完全契合。实际上，如果你真的关注这些任务，你可以用代码之类的方式解决它们。
- en: but yeah I think I don't think like this is really an inherent。Something that
    would never emerge because it's too hard， yeah。Yeah。we have a question on zoom
    also by the way， sorry I forgot to mention somebody asking you repeat repeat the
    questions because they can't always Oh okay。that's why that that's why that so
    the question someone asked is。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为这并不是一种内在的东西。是不会出现的，因为这太难了。对了，我们在Zoom上也有一个问题，抱歉我忘了提到，有人问你能否重复一下问题，因为他们不总是听得清。哦，好的。就是这个，所以有人问的问题是。
- en: do you think chain of thought would be a viable interpretability technique for
    very advanced AI systems？
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为思维链会成为非常先进的AI系统的可解释性技术吗？
- en: And they mentioned that there's some research by called externalized reasoning
    oversight by camera La。Will it be a viable interpretability technique for advanced
    day on？Yeah。am I supposed to repeat this sorry oh so the question is。Can chain
    of thought be a viable interpretability technique for AI。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 他们提到有一些研究，叫做“外部化推理监督”，是由Camera La做的。它会成为先进AI的可解释性技术吗？是的。我是不是要重复这个？抱歉，所以问题是：思维链能成为AI的可解释性技术吗？
- en: I think there's no guarantee that like。The chain of thought is how the model
    actually arrives at the final answer。but often you can use it to like sort of
    debug like why isn't the model getting this question correct or like what can
    we do better in the chain of thought to help the model get this correct？
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为没有保证会这样。思维链是模型如何最终得出答案的过程。但通常你可以用它来调试，比如为什么模型没有正确回答这个问题，或者我们能做些什么来改善思维链，帮助模型正确回答？
- en: I haven't read the anthropic paper that was mentioned， so I actually don't know
    the answer to that。嗯 ok k 。Another interesting result that we had here was that
    you can actually do like multilingual chain of thought prompting and so basically
    what we had is like we translated this like you know benchmark of math word problems
    to 10 languages and then we prompt them the model to do it in like say Bengali
    and then the model has like basically do the math problem in Bengali and give
    the final answer。
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我还没有阅读提到的与人类有关的论文，所以我实际上不知道答案。嗯，好吧。我们这里还有一个有趣的结果是，你实际上可以进行多语言的思维链提示，基本上我们做的就是将这个数学文字问题的基准翻译成10种语言，然后我们提示模型在，比如孟加拉语中进行操作，模型基本上要用孟加拉语解决数学问题并给出最终答案。
- en: I think the cool thing about this is that like this input is like highly improbable
    right so like Benngali is like 0。01% of the pre trainingion data and you know
    math word problems are probably even smaller subset of that。😊，And basically the
    interesting thing is the model can actually do like these types of questions pretty
    well to probably surprising degree。so like you know if you ask people before I
    showed them this result like oh how well can the model do you like these math
    questions in Swahli。
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这很酷的一点是，这种输入是高度不可能的，对吧，孟加拉语在预训练数据中只占0.01%，而数学文字问题可能是其中更小的一个子集。😊而且有趣的是，模型实际上能相当好地处理这些类型的问题，可能出乎意料地好。所以在我给他们展示这个结果之前，如果你问人们，哦，模型在斯瓦希里语中能多好地处理这些数学问题。
- en: like probably like 10%， but actually even like you know very underrepresented
    languages like Swahli or Bengali or Telegu and TI。the model can do to like surprisingly
    well despite the fact that they only occupy like a very small subset of the pretraining
    data。
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 大概有10%，但实际上即使是非常少数代表性的语言，比如斯瓦希里语、孟加拉语或泰卢固语，模型也能做得出乎意料地好，尽管它们在预训练数据中只占很小的一部分。
- en: 对。this and most of my experience with this is G。but like if you ask things in
    different languages。despite not being like explicitly trained these languages，
    right。it seems to have sort of like derived reasoning independent language， to
    that extent。Yeah you can do the reasoning actually kind of funny sometimes it
    always looks like it does the reasoning in English and then translates back to
    the other language answers it gives you sort of like if you like reason the English
    and then translate to the other thing。
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对。我的大部分经验都是关于G的。但是如果你用不同的语言提问，尽管没有进行显式训练，模型似乎有一种独立于语言的推理能力。是的，推理确实有点有趣，有时它看起来总是用英语进行推理，然后再翻译成其他语言的答案，就像你用英语推理然后再翻译成其他语言。
- en: So you think that like learning the like structure of a language and learning
    reasoning abilities or like somewhat separate large language models or that like
    it inherently will like learn chain of thought reasoning within that language
    within the structure of the language like the way thought works in that language。
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你认为学习语言的结构和学习推理能力是某种程度上分开的，还是大型语言模型会内在地学习该语言内的思维链推理，基于该语言的结构，就像思维在该语言中如何运作。
- en: Yeah that's a great question I'm not sure how to measure that。but I've definitely
    thought about I think the language I mean。based on these results like you definitely
    you probably didn't have any math questions in Swwakili for the model to learn
    from and I think definitely there's something language agnostic going on where
    the model learns reasoning sort of independently of the language and then it can
    express it in different languages if it needs to Yeah。
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这是个好问题，我不确定怎么衡量这个。但我确实考虑过，我认为根据这些结果，你在斯瓦希里语中可能没有任何数学问题供模型学习，而且我认为确实有一些语言无关的东西在起作用，模型在推理方面是独立于语言学习的，然后如果需要，它可以用不同的语言表达出来。是的。
- en: but I don't have a I don't think anyone I don't think we know the answer to
    that yet。诶。Yeah。so so basically like one question that comes up frequently is
    like why does scaling up improve chain of dot and one way of looking at this is
    like we can take a smaller model like P 62b and see like what types of errors
    are fixed from scaling up to 540 billion parameters and you can see that like
    for these three categories that we came up with some of all of them get fixed
    so scaling seems to have this like sort of universal effect on improving different
    types of errors from smaller models。
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我没有，我觉得没有人，我认为我们还不知道答案。嗯。是的。所以基本上，有一个经常出现的问题是，为什么扩大规模会改善思维链，其中一种看法是我们可以取一个较小的模型，比如P
    62b，看看从扩展到5400亿参数时修复了哪些类型的错误。你可以看到，我们提出的这三类错误中的一些都得到了修复，因此扩展似乎对改善较小模型的不同类型错误有一种普遍的效果。
- en: And then here's the same handwi who weav diagram and expressed in different
    ways so basically you have like some tasks that are doable with standard prompting
    so in blue and then the goal of chain of thought prompting is to sort of increase
    the set of tasks that we can do so for example。
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后这里是同一个手写的维恩图，以不同的方式表达。所以基本上，你有一些可以通过标准提示完成的任务，所以用蓝色表示，然后思维链提示的目标是增加我们可以执行的任务集。例如。
- en: now the ones shown in pink include math word problem， symbolic reasoning and。Challenging
    common sense reason。Yeah， question， have you done anys to figure out how like
    much is。is any of this contribution just because of the fact that you do more
    computations when you put in longer prompts like you know。like you multiple tasks
    through the model you create multiple inbeddings to like， you know。
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，粉色显示的那些包括数学文字问题、符号推理和具有挑战性的常识推理。是的，问题是，你有没有做过一些实验来弄清楚，像这样的贡献到底有多少是因为当你输入更长的提示时进行更多计算的事实，比如说你通过模型创建多个嵌入来处理多个任务。
- en: just things and models looking at in a way， Yeah， how much would that like to
    be tried non shade of thought prompts with like saying open length。Yeah， yeah。we
    tried with like X X， X X X or something and doesn't really it doesn't work。So
    I think it's not just about the compute。 I think it's about。The language guiding
    the model as part of the reasoning。I see。
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 只是从模型的角度来看，是的，那样的话想要尝试多大程度上的非思维链提示，比如说开放长度。是的，是的。我们尝试过像XX，XX X之类的，但实际上并没有效果。所以我认为这不仅仅与计算有关。我认为还与语言引导模型作为推理的一部分有关。我明白了。
- en: have you tried like describing the problem in more sales not being shown。I know。this
    is like a super I'm just very curious about like， So it's like a very interesting。Property
    and emergency is like to be。Yeah， you mean like describing the question in three
    different ways and seeing if that describing more instead of explicitly step by
    step and seeing that Yeah。I haven't tried that， but I would be surprised if that
    worked。estDid you try having it。
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有尝试以更多样的方式描述问题，而不是显示的那样。我知道。这真的是一个超级有趣的问题，所以这就是一种非常有趣的特性和紧急性。是的，你是说以三种不同的方式描述问题，看看这种描述是否更多，而不是逐步明确地描述，看看那样是否有效。是的。我还没有尝试，但如果有效我会感到惊讶。你试过让它这样吗？
- en: I'll put the answer and then explain its reasoning into that Yeah， that doesn't
    work as well。Yeah。but it depends on the task also so like yeah yeah me。こいつ。That
    seems to be the case， yeah。Yeah。those reactions be like reasoning， it would be
    like just any all that's your calculation。Sort of imagine if it was the answer。Like
    in a way， you know， like in a way。
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我会把答案和解释的推理放进去，是的，这样做效果不佳。是的。但这也取决于任务，所以是的，是的我。这个家伙。似乎确实如此，是的。是的。那些反应像推理，可能就是你的计算。可以想象一下如果这是答案。某种程度上，你知道，就像某种程度上。
- en: like an in a of chain of soil is like a very structured。It like。what if the
    same structures like we do some more randomly。Yeah， you could try。I would be surprised
    if it works I think like outputting tokens is pretty important for the model yeah。Okay。诶。So
    we're doing on time。 Okay， great。 So the last part。
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 就像思维链的一部分非常结构化。它就像。如果结构相同，我们就更随机一些。是的，你可以试试。我会惊讶它是否有效，我认为输出标记对模型来说是相当重要的，是的。好的。诶。那么我们时间怎么样。好的，太好了。那么最后一部分。
- en: I think is a pretty cool trick with chain of thought。So basically。😊，嗯。What people
    usually do is they'll just generate one chain of thought and then they'll take
    the final answer but there's' this nice trick called self consistency where you
    can use like temperature sampling with the model to generate like a bunch of different
    reasoning pads and final answers and then if you just take a majority of vote
    over the final answers。
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这是一种很酷的思维链技巧。基本上。😊，嗯。人们通常会生成一条思维链，然后得出最终答案，但有一个很好的技巧叫做自一致性，你可以用温度采样与模型结合生成许多不同的推理路径和最终答案，然后如果你只对最终答案进行多数投票。
- en: this ends up like improving performance by like a pretty big margin so。For example，
    here。you can see on GSMAK， which is like the MathW Pro data set， the improvement
    goes from like you know。the performance like 56 and then if you do self-con then
    it becomes 74。which's like a pretty big improvement。对。Yeah， here how many are
    the averaging number for self consistency Oh。
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这最终就像是在性能上有了相当大的提升。例如，在GSMAK上，你可以看到，这就像MathW Pro数据集，性能从56提升到了74，这就是一个相当大的提升。对。是的，这里关于自一致性的平均数有多少哦。
- en: I think 40。So it increases the cost of the inference time compute， but。Yeah。improve
    performance by might not to answer this， I'm curious to know how many samples
    or how many chains does one need to draw to get a significant what is the trade
    off between number of chains averaged over？
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为是40。因此它增加了推理时间的计算成本，但。是的。提高性能可能无法回答这个问题，我很好奇需要绘制多少样本或多少个链才能得到显著结果，链的数量平均下来有什么权衡？
- en: I think it depends on the sorry， the question is how many chains do you need
    to get a performance gain I think。The answer really depends on the data set， but
    usually you can get something good with like 16。I think。Yeah。Im sorry。We have
    a question。How does the temperature change the way the model work。Oh okay， the
    question is how does the temperature change the where the model works。
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我觉得这要看，抱歉，问题是需要多少个链才能获得性能提升。我认为答案真的取决于数据集，但通常你可以用大约16个链获得不错的结果。我想。是的。抱歉。我们有一个问题。温度如何改变模型的工作方式。哦，好吧，问题是温度如何改变模型的工作方式。
- en: basically when you use temperature decoding， the language model can like stochasticically
    pick one of the outputs instead of always picking the highest probability next
    word。so basically you get these like more stochastic outputs that are still based
    on what the language model has learned。
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，当你使用温度解码时，语言模型可以随机选择一个输出，而不是总是选择概率最高的下一个单词。因此，基本上你会得到这些更随机的输出，它们仍然基于语言模型所学到的内容。
- en: but it's just a little bit more random。Okay。And then like finally like yeah
    selfconsistency also seems to be emergeibility。I guess part of it is because chain
    of thought is emergingnt because you wouldn't get any better than random performance
    without doing chain of thought。
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 但这只是有点随机。好的。最后，像是自一致性似乎也有可出现性。我想部分原因是思维链正在涌现，因为如果不进行思维链，你的表现不会比随机更好。
- en: but yeah， you kind of see this big delta from self-consency through larger models。嗯。🎼Great。so
    I'm gonna run out of time。Let me just go to。I'll just talk about this a little
    bit so I think in addition to just purely scaling up the language model which
    is only available to like people in industry。
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 但你确实能看到自一致性通过更大模型带来的巨大差异。嗯。🎼太好了。所以我快没时间了。让我稍微谈一下这个，我认为除了纯粹扩大语言模型的规模之外，这在行业中只有一部分人能够做到。
- en: I think there's like a couple interesting directions to to work on one is like
    better prompting and characterization of language modab abilitiesities I think
    right now we're sort of just at the edge of you know knowing what the best way
    to prompt language models is。
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为有几个有趣的方向可以探索，一个是更好地提示和表征语言模型的能力，我认为现在我们只是刚刚触及，了解提示语言模型的最佳方式。
- en: There's also like pretty good applied work， so like you can use language models
    I've heard to train therapists to help with creative writing to help with science。I
    think chatGT has really shown what language models can do in this regard。I think
    benchmarks are also something that's pretty lacking because I think we solve benchmarks
    pretty quickly。for example Palm beat the average human on Big benchch you know
    within a year or something of big bench coming out and so I think we need more
    benchmarks and I think that's going to be an important contribution。
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些相当不错的应用工作，所以我听说可以使用语言模型来训练治疗师，以帮助创意写作和科学。我认为chatGT真的展示了语言模型在这方面的能力。我觉得基准测试也很缺乏，因为我们通常很快就解决了基准测试。例如，Palm在Big
    Bench的测试中超越了平均水平的人类，你知道，在Big Bench发布后的大约一年内，所以我认为我们需要更多的基准测试，这将是一个重要的贡献。
- en: And then the final one is like， how can we like？Have computer fission methods
    to make language models better so that it's less expensive to use them and more
    for get to use them。Great， so I'll end here and feel free to email me if you have
    any feedback and if you're interested in Google。
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个问题是，我们如何能够利用计算机裂变方法来改善语言模型，使其使用成本更低，使用起来更方便。很好，所以我在这里结束，如果你有任何反馈或对Google感兴趣，随时给我发邮件。
- en: yeah feel free to as。😊。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_9.png)
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，随时提问。😊。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_9.png)
- en: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_10.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_10.png)'
