# 斯坦福 GPT/Transformer 原理介绍（中英文双字幕）- P13：13.大型语言模型的涌现能力和扩展 - life_code - BV1X84y1Q7wV

![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_0.png)

![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_1.png)

好的，那么我要谈论的第一篇论文是关于大型语言模型的涌现能力，这篇论文特别酷。我认为，因为我们从谷歌、Deep Mind 和斯坦福得到了人们的参与，你可能会认识 Percy、Tatsu 或 Riishhi。

我的意思是，我们让人们在为什么要扩展和涌现能力方面达成了一种美好的框架。因此，我们在整个语言模型中看到的一件事是，随着规模的扩大，你确实会得到这些可预测的收益。

所以这里是你知道的经典 Kaplan 等人的论文，你可以看到，如果你扩展语言模型的规模，无论是计算资源、数据集大小还是参数数量，你会发现测试集上的损失实际上是可预测的。我不知道你是否在分享屏幕，所以在 Zoom 上的人可能看不到幻灯片。好吧，这些都没问题。

O。我想我要再说一次，因为我们在语言模型中已经看到过。如果你扩展语言模型的规模，无论是计算资源、数据集大小还是参数数量，你都可以看到测试损失有一种可预测的改善。嗯。现在，关于涌现的讨论实际上是关于一些不可预测的事物。

如果只看较小的语言模型，那么涌现在更广泛的科学文献中被描述为一种从数量上的变化而产生的质的变化，它的起源可以追溯到一位诺贝尔奖获得者的科学文章，称为“更多的不同”。

我真的很喜欢 Jacob Steinart 的这篇文章，他在这里描述了涌现，并举了几个很好的例子。例如，他说对于铀来说，如果只有一点点铀，没有什么特别的事情发生；但如果密集地影响到足够多的铀，你会得到核反应。还有 DNA 的例子，例如，如果只有钙这样的小分子。

你不能有意义地编码有用的信息，但是对于像 DNA 这样的更大模型，你可以编码基因组。因此，对于这项特定工作，我们使用了大型语言模型特别的涌现能力定义。因此，我们说，如果某种能力在较小的模型中不存在，那么它就是涌现的。

但是它存在于更大的模型中。这里的一个自然问题是，如何衡量语言模型的大小或规模，传统上有三个规模的维度，即训练时的计算资源、用于训练语言模型的计算量。

模型参数的数量就像是语言模型的大小，以及模型所训练的数据集的大小。这些图中的很多将使用训练的浮点运算数量或模型参数的数量。原因是对于不同规模的模型，训练数据集的大小通常是固定的，而训练的浮点运算数量只是数据集大小乘以模型参数。对于大多数语言模型，你可以从训练的浮点运算数量或模型参数数量得到类似的图。

很好，所以第一种涌现类型，是的，我继续。きくだ。对我来说，这似乎很不错，测量规模相对简单，而不是确定什么是合格的。你发现好吧。就是。哪里。好的，当然。所以是的，举个例子。我将在这里给出一个例子，实际上是下一张幻灯片。

基本上，我们与语言模型交互的一种方式叫做 fu shot 提示，其工作方式是你知道语言模型是一个非常好的下一个词预测器。当你给模型一个例子，然后询问它一个未见过的电影评论时。

然后你会问，输出是什么。在这里，语言模型可以说“积极”，因为它理解如何利用评论中的上下文来给出下一个标记。我们用来判断是否具备能力的定义基本上是几个镜头提示的任务。

例如，情感分析，对于小模型来说，如果它的准确率是随机的，那么它是涌现的；但对于大模型来说，则高于随机准确率。这样说有道理吗？基本上，如果模型的表现没有比随机好，那么我们就说它没有能力去执行这个特定的任务。

我在这里给出几个例子。嗯。这里是我们看待涌现图的典型方式。基本上，每个不同的图都是一个不同的任务，我很快会举几个例子。但是你阅读图表的方式是，X 轴是训练的浮点运算数量或模型规模，而 Y 轴则是准确率或模型在执行任务时的表现。

现在，我们有来自 OpenAI、Google 和 DeepMind 的不同语言模型，每个点代表一个不同的语言模型，而不是一个在训练过程中变化的语言模型。

你会看到，对于非常小的语言模型，性能基本上接近随机，或者说并没有比随机表现得更好。而一旦超过某个阈值，你会发现性能突然显著高于随机，这就是我们所称的涌现。因此，如果你要从小语言模型推断这些线。

你可能会预测它永远不会比随机更好，因为它只是一个平坦的线条。但有趣的现象是，当你超过某个阈值时，你实际上会看到这种涌现现象，模型的表现远远好于随机。所以让我举一些具体的例子。这里是一个任务。

它基本上是一个称为多任务自然语言理解（multitask NLU 或 MLLU）的基准，基本上它是一堆测试问题，从高中一直到专业水平的考试。它的工作方式是，语言模型被给定，例如，这里有一个高中数学例题，然后对于一个未见过的问题，它必须给出答案。

然后你可以在右侧的图表中看到，对于模型规模，如果你达到大约`10²²`的训练浮点运算量，你在这个任务上的准确率实际上并没有比随机好，但如果你扩展到`10²⁴`的训练浮点运算量，那么你会看到所有三个模型的表现都远超随机准确率。是的。用于训练的数据规模大致相似吗？还是因为这些模型是用不同的数据训练的？

是的，我认为这些模型的规模在一个数量级之内。是的。而且每一个单独的轨道都是相同的数据，是的，数据是固定的，除了 Chinchilla。Chinchilla 对更大的模型使用了更多数据，但我相信这里其他所有模型的数据量都是相同的。是的，这里只是另一个例子来说明。

更具体地展示一下，这是一项来自大基准测试的任务，顺便提一下，大基准测试有大约 200 个基准，基本上是一个众包的基准集，我建议你在进行大量工作时查看一下。

基本上，这个任务是语言模型必须处理一个英文句子。然后给出国际音标的音译，或者说是 IPA 音译。基本上就是如何发音。对于这个任务，评估指标实际上是蓝色（BLEU）或者类似于 N-gram 重叠的指标。

你会看到一个类似的现象，即当你增加语言模型的规模时，起初是平坦的，随后突然间提升超过随机水平。嗯。对。所以我会谈谈我听到的另一个有趣的结果，这就是为什么它在出现。这是我们几个月前发布的一份技术报告，基本上这里有一个非常有趣的奖项，或者说是一次性奖项。

在语言模型中，Anthropics 是一个创业公司，基本上设立了这个奖项，如果人们能提出一个任务，随着语言模型规模的增加，任务的表现反而会下降，那么你就能获得一笔钱。所以实际上有很多提交，以下是一个示例任务，他们发现随着语言模型规模的增加，表现确实会下降。这个任务是让你重复我的句子，输入是“所有闪光的东西并不是金子”，输出是模型必须准确地说出“金子”。

嗯。发生的情况是对于小型语言模型，它不知道短语“所有闪光的东西并不是金子”。所以它只是复制输入，实际上获得了 100%的正确率。但是对于中型语言模型，你会看到表现实际上会下降，因为中型语言模型知道短语“所有闪光的东西并不是金子”，然后它说金子。

其实这并不是任务要求它做的事情。是的，有人问你能否给出 10 的 24 次方的物理估计，可能是训练时间或数量。是的，我认为 10 的 24 次方浮点运算大约是，在谷歌我们使用 TU，一个 TU 的集群。

我认为这相当于大约 4000 个 A100s。10 的 24 次方浮点运算大约需要两个集群运行六周左右。所以为了进行预训练需要大量的计算。诶。我不知道。但是你们还记得在化学课上有过摩尔的概念吗？

然后大约是 10 的 23 次方，你的老师可能会说，哦，别去想这个数字有多大。这大约是这些模型在预训练中需要的浮点运算的数量。好的，太好了，所以基本上中等大小的语言模型实际上会表现得更差。哦，是的，你还有其他问题吗，太好了。

😊，这个奖项是我认为的第三名或类似的。很好的问题。但这个任务，因为我最初的看法是，哦，你可以在评估上放一个负分。你是什么意思？翻转负面评估，这一切都取决于哪个评估？是的。

为了衡量你的任务完成情况，就像是这个测量非常稀疏，只有在你完成时才能获得积分。是的，很多事情都是因为你不会完美地完成。真的优化了很长时间。或者说如果你参加考试，然后用负号来评估那样的话，不是吗？是的，没错。就像是这样的东西。是的，我的意思是，他们对于这个事情。

他们考虑到所有的事情，你不能仅仅说任务应该在某些事情上表现不佳，任务必须是有意义的。而且我想你关于信用时间或评估指标如何运作的观点确实很好，所以我想如果你知道，我猜这个论点是这样的。

如果你分配部分信用，性能可能看起来并不突现。但我们有一堆示例可以稍后展示，即使你使用部分信用指标，你仍然会看到同类型的突现，因此这并不是基于评估指标不分配部分信用的纯现象。

可能有一些任务，如果使用中等规模的语言模型，性能开始下降。但如果你一直扩展到我们在谷歌公开的最大模型 Palm，你会发现这个语言模型实际上能够返回并正确执行任务，因为大型语言模型也知道“所有发光的东西并非金子”，它还理解“重复我的感官”，所以能够在这个任务上获得 100%的准确率，这也是一种不同类型的突现。

在论文中我们讨论的另一类突现是突现提示技术。基本上，除了未来提示，还有其他与语言模型互动的方式，可以被视为突现。问题是，所有模型是否经历了构建微调？

这些模型在这个图上都没有进行一次指令微调。很好，是的。所以，与语言模型互动的一种方式是基本上使用一种称为 RHF 的技术来找到模型。基本上，它的工作原理是你有这些数据，人与人之间会评估他们偏好的输出类型。然后模型在强化学习中进行训练，以优化人类的偏好。

这个图展示的是，如果你对模型进行 RH 微调，模型在另一个零样本任务上的性能实际上会对小模型变差。你可以看到蓝线在橙线之上，蓝线是基线，橙线是 RHF。如果你对大型模型进行同样的操作。

然后你会看到，实际上通过进行 RLHI，性能有了正向增量。这是一个有趣的事情，因为某种技术可能仅在你尝试大型语言模型时才有效。如果你只在小型语言模型上尝试，很难得出它不会帮助性能的结论。然后，稍后。

我将讨论无需提示的链作为另一种突现提示。因此，这里是我用来思考突现作为框架的一个大致图示，X 轴是语言模型的规模，Y 轴是某种想象的东西，你知道的。

一系列语言模型可以做的事情。然后基本上你可以选择某个随机点，比如说在语言模型中 1000 亿参数，将会有某些能力。因此，你可以看到，随着语言模型规模的增加，模型可以完成的任务数量也在增加，然后你可以看到，有些任务是超过 1000 亿参数的模型可以做到的。

但低于 1000 亿参数的模型无法做到这些，我们称之为涌现能力。抱歉。问题是颜色在哪里。哦，它只是突出显示，比如深蓝色是像小型语言模型无法完成的任务。这样说有道理吗？是的。然后右侧，虚线，上方的区域。哦。

这仅意味着我们尚未能够解决的任务。而我很好奇，你认为在 1000 亿规模下，那些白色区域的任务是不可解决的吗？

你认为更好的模型。特定的训练数据会让我们达到 1000 亿规模吗？是的，我绝对认为这不是固定的，我会很快给一个例子，但这不是规则。你知道，不是说你必须拥有 1000 亿参数才能完成某项任务。这只是我们观察到的模型阈值，而我确实认为，通过更好的训练数据、架构和算法，我们可能会超越这个阈值。

嗯。是的，正如 Rland 刚刚提到的，涌现的一个例子可以是更好的数据。所以并不是所有都是规模，我会稍微给点细微差别。对于这个任务来说，它只是 Big Ben 基准中的一个任务。你可以看到，像 Lambda 这样的谷歌模型和 GPT3。

你实际上不会从扩展到 137 或 175 亿参数中获得涌现。但当你使用不同的语言模型 Palm 时，它在比 Lambda 和 GT3 更好的数据上训练，你实际上可以获得这种涌现能力，即使是这里显示的 62 亿参数的小型语言模型。

将库存更好的模型视为更好的数据，或者更好。Ttro 掩蔽，你知道的。选择或最有趣的。是的，挑战在于这是一个很好的问题。比如，Palm 和 Lambda 之间有很多差异。我们无法以任何受控方式消除它们，因为预训练的成本。

但我们的假设是，P 在更好的数据上训练，这解释了 Palm 和 Lambda 之间的许多差异。像是更小的模型也有可能。是的，这个。是的，这是个好问题，所以我想即使在这里，你也可以看看，比如说。

Palm 80 亿模型。像是那个点。你可以消融它，稍微高一点，但在那个点上还不是涌现。所以，对于这特定任务来说，效果很难判断。是的。

Zoom 上有个问题，Paul 是否有两个不同的版本，如果没有，为什么有两条线？哦，所以我认为这两条线，一个可能是三次示例，另一个可能是零次示例之类的。因此它只是指我们在使用语言模型时是否有示例。

嗯，太好了。我会谈论一个小的消融实验，基本上展示了这一点。这是一个玩具任务的消融实验，语言模型需要知道在英语中，复数主语要用复数动词，单数主语要用单数动词。我们在这里做的就是从头开始训练这些小的 BRT 模型，然后我们固定了一些动词在训练数据集中的频率。

基本上是在说，好的，看到某个动词在数据中更频繁的效果是什么？

在这个图中，x 轴类似于动词的频率，y 轴是错误率，你基本上看到的是，如果你有更多的领域内数据，也就是说如果模型看到动词的次数更多，它执行任务的效果会好得多，这实际上是高质量数据或与评估任务更相关的数据可以产生巨大差异的一个例子。

即使你固定了计算、模型大小和其他数据。是的，Zoom 上有人问，是否有办法将新出现的能力缩小到更小的模型以供更大的教师使用。是的，我想可以。因此，更大的教师模型基本上可以用来生成数据，然后如果你在数据上对更小的模型进行微调。

很可能你能够在更小的模型中获得出现的能力，我也会谈论这个例子的，等我看看。哦，实际上，下一张幻灯片。一旦你知道想要什么行为，就可以在更小的模型中诱导出期望的行为。

例如，这里是来自 thestructGPT 论文的插图。基本上，这里期望的行为是遵循指令。你可以看到有多个模型，左侧是这些使用 RLHF 训练的小模型，它们的表现实际上比使用较弱技术训练的大模型要好。因此，重点是，如果你知道你想要某种行为，这种行为曾在更大的模型中以涌现的方式出现，你可以找到一种专门微调该行为并在更小的模型中诱导该行为的方法。

我想其中一个局限性是，除非你知道你想要的所有行为，否则你真的无法获得这种自然出现的行为。是的，另一个讨论点是，关于“什么是出现的正确 x 轴”的问题，所以现在我们主要讨论模型参数和训练的浮点数，但我想如果你问 Deepmind 的人，他们会如何看待这个问题。

你将会得到这样一种论点，模型参数和训练计算量实际上只是衡量模型质量的一个代理，模型的好坏可以通过困惑度或它在某些开发集上的表现来真正衡量，比如 WikiTex 103。

所以基本上，你也可以通过困惑度来衡量。这里是 WikiTex 困惑度，然后你可以看到在下游任务中，随着困惑度的改善，存在一个阈值，使你能够在下游任务上表现得更好。

至少目前，困惑度和训练计算之间有很强的相关性，所以你可以看到这两条线是非常相似的。基本上，我认为在未来，如果我们有更好而且更小的模型，训练在更好的数据和更好的算法上。

然后，或许 WikiText 的复杂性可以展示出与使用其他指标不同的图表。所以 WikiTex 基本上是我认为是维基百科的一个子集。而困惑度是衡量你在数据集中预测下一个单词的能力。因此，如果你在这个特定评估集上非常擅长建模下一个单词。

这在某种程度上是衡量你理解语言的能力。明白了吗？但是这样不是有点强迫吗？哦，这是一个保留的测试集。对。好。嗯。最后，我认为关于出现的事情是非常令人兴奋的，不仅仅是我们所谈论的技术性出现。

但是，AI 社区在看待规模和如何使用语言模型方面，社会学上发生了一些变化。这是一些示例，说明在少量样本场景中，扩大语言模型的规模使你能够超越一个通常在数千个示例上进行微调的任务特定微调语言模型。因此，绿色线是通过微调实现的前状态，而蓝点基本上显示的是如果你使用预训练语言模型进行少量提示。

这意味着语言模型并不是专门训练来完成这个任务的。你往往只需继续扩大语言模型的规模，就可以获得最先进的结果。显然，这里有局限性，你不想只通过扩大规模来获得最先进的结果。但我认为人们的思维方式发生了很大变化，你实际上可以通过扩大语言模型的规模和进行提示获得一些最佳结果。

Zoom 上的问题。有人问，这不是从两到三张幻灯片前的矛盾图吗？那是什么？哪个，这个吗？我确定一般不应该假设他说的是“是”。好的，我们说。我们是否一般假设规模真的会说谎？是的，这是个很好的问题，所以这个图表表明你可以微调，并且可以，这取决于你的具体情况。

任务，但这个图表所说的是。就像。我们并不是说精细调优的小模型在某些任务上表现良好，如果你针对得当。但对于那些更复杂的测试，通常通过扩展规模你能获得更好的结果，因此这些任务可以归入这两个类别，我并不认为这是矛盾的。我想某些任务，确实可以通过扩大模型的规模获得更好的结果，而其他任务。

如果这是一个非常狭窄的领域，而大型语言模型可能没有在那种数据上训练，那么通过精细调整你会获得更好的结果。对。所以这里有一个小总结幻灯片。基本上，涌现能力只能在大型模型中观察到，如果你仅仅通过查看小模型的图表来预测它们的涌现，那么你是无法做到的。我对如何看待这一点也进行了一些反思。

所以涌现实际上是如何看待那些未在预训练中有意构建的新能力的框架，我认为这背后的潜台词非常重要。也就是说，你可以把它看作是一个隐含的论据，说明我们为什么应该不断扩展语言模型，因为这样你会得到那些在其他情况下很难找到的能力。

而围绕这个背景是相当重要的，因为继续扩展这些模型的成本非常高，甚至在一年前，很多人都不相信通过扩大语言模型的规模可以在某些任务上表现更好。

如果你在行业中工作的话，会存在一种有趣的紧张关系，既涉及涌现现象，又涉及许多生产任务。所以涌现是一种任务的普遍现象，当你扩展模型时，成本会很高，但单一模型可以执行很多任务，这在某种程度上是朝着 AGI 的方向发展。

对于许多生产任务，你有一种相反的情况，你知道它是什么任务。例如，翻译成西班牙语，并且你对计算有这些限制，因为你知道翻译时机。例如，你不希望人们等待几秒钟才能获得翻译。而且你也恰好有很多领域内的数据，比如说。

比如，你知道有一百万对英语和西班牙语的句子可以用来训练。这就像是相反的情况，你并不太关心模型的涌现，你可以只用数据训练一个非常小的模型来完成某个任务，而不必消耗大量计算资源。最后一点是，我认为这是一个非常有前途的研究方向，如果有人对研究感兴趣，可以尝试预测未来的涌现能力，而我最近并没有看到太多相关的工作，可能是因为这太困难了，例如。

像你只能为特定任务预测出现，或者预测出现的一种方式可能不是特别普遍，所以我没见过太多关于这一点的研究，但我觉得这是一个相当有前景的研究方向，也许 Anthropic 正在研究这个。

我不知道。好的，很棒，在我继续谈论提示链之前，有什么问题吗？嗯，佢。😊。你先是。系我有呢条都俾见。哪些参数最佳规模来获取类似的属性。显然，这里有许多不同的选择。什么意外的。比如说，Didt G。我想回到那里。这主要是我们测试的内容吗？

然后我们发现哪些扩展的电池给出了结果，或者像那样。嗯。我想说我们并没有非常原则性的方法来扩展这些架构。诶。😊。我不是这方面的专家，但有些内容与能够适配到特定 TPU 上的参数数量有关。但总体上我觉得你在一定程度上要成比例地扩展意图头和嵌入数量，但我认为这是一个开放的研究问题，因为你不能真正对这些预训练进行消融。

你实际上不能对预训练进行消融，想要有任何原则性的方法是困难的，除非一些负责的人工程师说，好的。我认为这就是正确的做法，然后当我们这样做时，它就能正常工作。嗯。

你有没有对这种赌博的渐近行为有任何指示。如果你预期最终会到达某个稳定点，翻转有限但非零的损失，或者它就会一路降到零。嗯，这个问题很好。我觉得。我是说你是说在困惑度上，还是在特定任务上，或者只是一般来说像下一个单词预测。好像这些也是相当普遍的，比较独立于任务，对吧？

这就像是涌现的扩展。嗯，但你知道，如果你取无限参数的极限。那么即使在分析上，也有没有任何意义。嗯。我不知道，如果对于大多数这些任务，准确性是有限的，比如说 100%。所以那里有某种渐近线。

但我想你可能在问更深层的问题是：语言模型是否可以完美地知道如何预测任何给定输入的下一个单词。也许，我的意思是。我想有某种限制。如果我说一个句子，可能有两个可能的下一个单词之类的。

而且你可能无法完美预测。所以我认为这是有一些限制的。但我觉得我们离达到那个限制还很远，仍然有很多未解决的任务表明还有很多提升空间。嗯，如果研究人员有兴趣研究出现，哪个不同规模的模型是公开可用或最佳的研究选择？嗯。

好问题。所以我认为 OpenAI 的 API 有很多语言模型，我们实际上在谷歌也使用得很多，它被用来研究新兴现象，这就是一种方法，实际上很多模型目前是免费的，虽然有限制，但它们是免费的，所以我们也使用它。

我认为还有更小的语言模型，比如说，像 UL2 模型，它大约有 200 亿个参数。但我想你是对的，确实存在这样一个挑战，即小语言模型不会出现很多新兴行为，因此你需要训练。是的，你可能需要现在使用 OpenAI API，或者等待人们训练更大的模型。

我想也有一些新的能力，像你们可能比公开的 OPT 模型更了解这些，但我还没有看到很多实验，嗯。所以我想问的是，是否有可以访问的、参数较低的新兴能力。

我可以进行语音识别。可能会有一些更好的，可能不是像思维链那样，但我听说过一些。是的，确实，我认为在论文中，我们列出了几打将在约 80 亿参数或 600 亿参数左右出现的能力。是的，是的。我们在 Zoom 上有两个问题。第一个问题是，你看到策略了吗。

在较大的技术之间，系统性地研究这些模型，还是基本上大家采取相同的方式。诶。我不会说每个人都采取相同的方法。我认为，作为一个例子，Anthropic 采取了非常以安全为中心的方法，他们对新兴能力非常感兴趣，因为可能会出现一些不希望出现的新兴能力，他们想预测这些情况。我也不知道其他公司发生了什么，除了谷歌。

所以我不能多谈这个。有问题。有哪些任务或能力尚未出现，即使在像 Lada 和 ChatGPT 等模型中？哦，好的，也许我会迅速展示这个。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_3.png)

诶。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_5.png)

有一个很好的列表，在哪里。所以，基本上我们所做的是。在 Big Bench 上有 200 个任务。然后我们基本上对它们进行了分类，所以像这样。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_7.png)

随着 GP3 或 lambda 的平滑增加，新兴能力与 Palm 相关，然后是平坦的。这就像没有模型比随机更好。所以我认为如果你看这些任务，它们仍然应该尚未出现，如果你能让它们合并，那就很有趣。嗯。我认为聊天应该是 T 20 问题。哦，好的，嗯。

这不是超级新的，我认为这大概几个月前的事情。是的，是的。哦，20 个问题，好的。是的。有点意思。是的，我认为。很酷的事情是，你可以看到随着时间的推移，最初可能只有这些是新出现的，当 Po 发布时，你会看到更多的能力出现，然后我猜想在一两年内，这些大多数都会出现，我们需要更严格的基准。是的，还有另一个问题，为什么谷歌没有采取那么多的安全中心。

正如我们在放弃时所说的。有理由相信强大的能力不会出现吗？

是的，我不想代表谷歌回答这个问题。我只能谈谈我自己的看法。但我认为现实是，即使你看看谷歌的研究量，它可能并不专注于大型语言模型领域。但就我们所做的安全研究的数量而言，我认为这比其他公司要多，如果你实际查看发表的论文数量。

我不敢确定，但我认为这是正确的。好的。诶，很好。所以。是的。我会谈谈思维链提示，基本上思维链提示是一种在大型语言模型中进行多步骤推理的方法。是的，我想说看到谷歌的很多人都在研究这个，尤其是看到 Sdar 首席执行官在去年谷歌 IoC 活动中展示这个真的很令人兴奋。嗯。基本上，这个的动机是我们希望语言模型能够处理更复杂的任务。

你知道，比如说，我们知道语言模型可以做简单的任务，比如情感分析或翻译。但更复杂的任务呢，可能甚至需要人类花费一分钟或更多的时间去做？

这里的目标基本上是通过元数据引导它们，例如。我们不仅仅想给出输入和输出的对。我们想提供整个推理过程，让它们模仿。基本上，你可以看到，在标准提示中，你有问题和答案，然后有一个问题，模型给出新的答案。

不幸的是，这是错误的。然后通过思维链提示。你给模型一个问题，然后就像老师让你展示你的思路一样。你给出我们称之为思维链的推理路径，最后给出答案。当模型看到这个未见过的问题时，现在它能够给出推理路径，并且给出正确的最终答案。

我们将这些提示加入到提示中的方式基本上是手动写几个，然后添加到提示中。让我展示一下是如何工作的。这是 Open Air API。基本上。这是非思维链的做法，所以基本上你会有。问题答案。问题答案，问题答案，然后是关于，比如说，食堂问 3 个苹果的新问题。

他们用 20 来做午餐，另外大约六个，问他们有多少个苹果。模型出错了。链式思考唯一的区别是，你在给出最终答案之前提供这些中间推理路径，所以这是一个路径，还有一个推理链。现在模型对于这个未见的问题。

提供整个推理过程，这实际上使模型能够正确回答。我再给一个快速的例子，就是这个。任务就是取单词的最后字母，比尔·盖茨的最后字母，所以 L 来自比尔，S 来自盖茨，然后连接起来，答案应该是 LS。

然后这里模型出错了。答案应该是 N。SK 说。然后如果你进行链式思考，显然这对模型来说变得很简单。它说比尔的最后一个字母是 L，盖茨的最后一个字母是 S，答案是 LS。然后这里它能够得到埃隆·M 的最后一个字母和马斯克的最后一个字母是 K，答案是 N K。

诶。那么，有没有什么不清楚的地方？关于这里发生的事情有问题吗？好的。嗯。基本上我们可以有这些类似的图，其中 X 轴是模型规模，Y 轴是性能。在左侧我们有一个叫做 GSMAK 的数学板问题基准，基本上像是你在小学数学测试中看到的问题。你可以看到蓝点是标准，紫星是链式思考，基本上你会看到，如果你使用足够大的模型，链式思考的表现要好得多。

而且实际上超过了当时的最新技术。一个类似的例子是在一个叫做 Str QA 的基准上。这里的策略基本上是世界知识加常识推理基准。问题可能是你能把篮球藏在一只猫的耳朵里吗？然后模型会说，篮球大约是这个大小。

猫的耳朵那样的尺寸是放不下的，现在在这个基准中你也可以看到，我们可以通过使用链式思考和足够大的语言模型来超过之前的最新技术。因此，我们的一种使用方式是，我们在某个 Big benchch 任务的子集中评估了链式思考。

所以我们创建了一个叫做 Big benchch hard 的子集，基本上它就像是 23 个来自 Big benchch 的挑战性任务，其中没有模型的表现超过平均人类评分者。你提示模型的方式是提供任务描述、问题和选项。

链式思考，然后是测试时间的问题。所以我给几个任务的例子。一个例子是导航，基本上语言模型在这个任务中需要做的就是遵循这些，所以问题是：如果你遵循这些指令，是否返回到起点，向左转，向右转，走五步，走四步，转身，走九步。

然后，跟随可融合样本的模型基本上能够在所有操作之后跟踪状态。最后它会说，我们得到了最终答案。那么，答案是我们是否回到了原始位置，意味着如果它是零，那么答案就是是的。诶。再举个简单的例子，这是一个对人类来说非常简单的任务，基本上是单词排序。

所以有一份单词列表，Burley，Bela，我不会逐一读它们，基本上模型必须按字母顺序对它们进行排序。在这里，模型可以跟随未来的示例，所以你有一个相当复杂的链式思考，模型必须对每个子部分进行排序，最后得出最终答案。

这是正确的。那么这是关于这个 Big Bennch 子集的结果总结。你可以看到我们有两个指标，一个是所有任务的平均表现，另一个是超过平均人类评分者的任务百分比，所以平均人类评分者是 67，最高人类评分者是 94。

然后在之前的结果中，模型的表现很差，大约是 50。这是通过这个子集的构建得出的。然后我们使用 Code Vinci O2。这是开放 AI 模型之一，实际上你可以通过开放 API 免费使用这个模型。基本上，如果你只进行回答提示而不进行链式思考。

然后你在 27 个任务中大约是平均评分者的水平。但是如果你使用链式思考提示，性能会显著提高，能够超过大多数任务中的人类平均水平。然后下面是一个可视化，显示出表现低于人类的任务用红色标出，表现优于人类的任务用蓝色标出。两个问题。伙计，这难道不和 R HF S 相似吗？嘿，是什么相似呢？

我可以改变自己的看法，不。被拳到。嗯。是的，我想这是存在的。我不会称它为相似。像链式思考基本上是你采用一个预训练语言模型，并使用包含中间推理路径的提示技术。RLHF 的工作方式是你有这些额外的数据，你想对模型进行微调，并且有一个偏好模型，预测某个输出的表现如何。某个输出被人类偏好的可能性有多大，然后 RLHF 的作用就是微调语言模型，以便在偏好模型的预测上表现良好。

所以基本上这就是将模型与人类的偏好对齐。有第二个问题吗？やいです。好的，Grace 问，中国是否可以纳入微调，而不是要有一个。是的，简短的回答是可以的。复杂的事情在于你必须有链式思考的中间步骤，这些步骤是相当复杂的。收集这些数据和进行微调可能是成本较高的。最后一个问题。

抱歉，另一位学生问，你认为思维链和提示工程一般只是一个产物，在更大规模、能够更好理解功能的模型面前是否不再必要。是的，这是个好问题，基本上这个问题是关于提示工程将有多短暂。我认为我们会找到答案，但一些初步的直觉是，对于那些易于描述的简单任务，或许有多个选择的大型模型会更能抵抗提示工程，能做的事情就会少一些。

但是我认为随着语言模型变得更强大，使用它们处理更多挑战性任务将会更加普遍，而在这些任务中，你必须明确指出你希望模型做什么。因此，我认为在至少不久的将来，仍会有一些提示工程的空间。

是的，没错，你知道这个一般是怎样的，比如你一个简单的。然后另一个是关于排序单词的。是的，所以我的意思是看到这一点。给出思维链的像是那样的单词。是的，这是个好问题，对于一些任务，如果你已经见过类似的数据和预训练，模型可以做得很好，即使思维链来自另一个任务，例如。

像数学文字问题，你其实不需要数学思维链，因为模型已经知道我怎么做，但像这样的任务，你可能没有见过类似思维链的数据。因此，没有特定任务的示例，你可能在这样的任务上表现不佳，除非手动为其他示例编写。是的。作为这个研究的背后，像我们应该用什么心理模型去尝试呢？你是否把模型当作一个人。

我认为这是否更好，或者是否更像是试图给予更多的计算，以便于。在 deesttiy 回答之前。是的，太好了，我认为我的动机只是考虑你所说的，当人类试图解决这个数学问题时，他们的脑中发生了什么。如果你注意到，至少一些人确实会用自然语言思考，所以如果你多关注一下你的脑中发生了什么，你会发现有时你在用语言思考，而语言模型也能用语言思考，因此这就是让我询问语言模型这样做的动机。

我认为一件事情做得不错的是，这项技术的发展实际上与 palm 的发展相吻合。因此，基本上，拥有 palm 模型使我们能够更好地完成很多任务或更多挑战性任务，利用思维链。

是的。我听到的第一个。我们说这很重要，比如在数据集中这个链思维过程的绝对例子数量。或者这个功能。是主要的显著事物，还是相对数量？

这些示例中的一部分只是一些负面示例，不是很好的推理示例。这些与绝对数量一样重要吗？啊，这第 3。是的，好的问题。所以我想具有挑战性的是，我们实际上无法测量训练集中有多少类似示例。你知道。这很难做到，我认为以前没有人做到过。

所以这更多是一个开放的问题，比如为什么中国 F 思维有效，因为实际上你并没有在训练集中看到类似的数据。是的，我认为这是一个开放的问题，为什么它有效。你的模型是什么？这个意思就是好的。想想像如何。事情。思考语言，然后知识也是这样。但你实际上是如何思考的，就像在那样。

对于模型来说，这是什么情况。我是说，会在特定任务中发生转变。就像一些权重会变得更。好送翻过来扣雨点啊。是的，我并不真的考虑权重中的事情。我想我思考的方式是，给你一个数学问题，让你在半秒内给我答案，这是不公平的。

这基本上就像你与模型所做的事情，当你没有正确执行链思维时，基本上是在提出这个挑战性的问题，而模型没有足够的计算能力来一次性解决它，立即给你下一个答案。

我认为第二件事就是，我会考虑模型在预训练期间学习了一组组合技能，所以也许它并没有真正学习这个特定的导航任务，但它学到了其他东西。对吧？它学会了，比如说如果你走五步，面对这个，也许是的，你应该在这里加五，或者类似的，它学会了如何进行模式匹配。

也许在未来的示例中，它可以匹配推理路径和问题之间的关系，模型可能知道这些小技能，如果你能够以某种巧妙的方式将它们结合在一起，那么你就可以让模型解决更具挑战性的问题。

好的。我们还有多少时间？哦，原来也审看的看。好的，50 好的。好的。嗯，好的。一个判断这个切割的好例子。无论如何，很多不同的答案，都是正确的。是的。好的，很好，如果你有任何问题，请随时问。是的。这是另一个涌现的例子，所以基本上你可以看到这里有三个模型：instructTBT、codex 和 palm，链思维是蓝色，非链 F 思维是灰色，然后你可以看到实际上你必须有足够的模型规模才能让链 F 思维运作良好。

我想这里的直觉是，如果你有一个非常小的模型，模型会不断重复自己，无法给出连贯的回答或找到最终答案。这就是为什么在小模型中使用思维链效果不佳，而对于大模型，显然在多步骤问题中，模型能够以更高的准确性解决任务。

关于思维链的另一个有趣之处是，有些任务根本不会出现突现行为。因此，突现尚未被激发，但如果你使用思维链，可以在较小模型中解锁这种突现性能。

一个例子是多步骤算术，我不知道你是否能做到。你知道，也许我不想说永远，但很难想象一个模型能完成这个。你知道这是问题，然后下一个令牌是正确的，这在一步内解决起来相当困难，但通过思维链，你可以仅通过让模型输出这些中间步骤来达到约 50%的准确率。

哦，对。这是需要重新思考的一件事。我知道变换器肯定能进行加法。而且能接收数字并进行进位。没错，没错，但接下来有个问题，实际上会发生什么，对吧？

我明白这不是一个很大的空间来进行公共保护。所以我的问题是，如何才能真正区分呢？也许有些方法可以区分那些因为空间不足而未出现的任务，或者任务如此之多，以至于没有专门的空间来处理那个任务。

它就是无法做到，即使你愿意。是的，是个好问题。我认为似乎有一些任务根本不适合我们训练语言模型的方式。例如，在语言模型中，我们使用的是令牌，对吧？所以如果你给它，比如令牌四，它实际上并不接收数字四。

它需要一个大约 1000 维的嵌入，或者如果你给它一个单词并要求它反转字母。这是一个超级简单的测试，但我们训练模型的方式实际上并没有关注字母等内容。

所以我认为有一部分任务与我们训练变换器的方式并不完全契合。实际上，如果你真的关注这些任务，你可以用代码之类的方式解决它们。

但我认为这并不是一种内在的东西。是不会出现的，因为这太难了。对了，我们在 Zoom 上也有一个问题，抱歉我忘了提到，有人问你能否重复一下问题，因为他们不总是听得清。哦，好的。就是这个，所以有人问的问题是。

你认为思维链会成为非常先进的 AI 系统的可解释性技术吗？

他们提到有一些研究，叫做“外部化推理监督”，是由 Camera La 做的。它会成为先进 AI 的可解释性技术吗？是的。我是不是要重复这个？抱歉，所以问题是：思维链能成为 AI 的可解释性技术吗？

我认为没有保证会这样。思维链是模型如何最终得出答案的过程。但通常你可以用它来调试，比如为什么模型没有正确回答这个问题，或者我们能做些什么来改善思维链，帮助模型正确回答？

我还没有阅读提到的与人类有关的论文，所以我实际上不知道答案。嗯，好吧。我们这里还有一个有趣的结果是，你实际上可以进行多语言的思维链提示，基本上我们做的就是将这个数学文字问题的基准翻译成 10 种语言，然后我们提示模型在，比如孟加拉语中进行操作，模型基本上要用孟加拉语解决数学问题并给出最终答案。

我认为这很酷的一点是，这种输入是高度不可能的，对吧，孟加拉语在预训练数据中只占 0.01%，而数学文字问题可能是其中更小的一个子集。😊而且有趣的是，模型实际上能相当好地处理这些类型的问题，可能出乎意料地好。所以在我给他们展示这个结果之前，如果你问人们，哦，模型在斯瓦希里语中能多好地处理这些数学问题。

大概有 10%，但实际上即使是非常少数代表性的语言，比如斯瓦希里语、孟加拉语或泰卢固语，模型也能做得出乎意料地好，尽管它们在预训练数据中只占很小的一部分。

对。我的大部分经验都是关于 G 的。但是如果你用不同的语言提问，尽管没有进行显式训练，模型似乎有一种独立于语言的推理能力。是的，推理确实有点有趣，有时它看起来总是用英语进行推理，然后再翻译成其他语言的答案，就像你用英语推理然后再翻译成其他语言。

所以你认为学习语言的结构和学习推理能力是某种程度上分开的，还是大型语言模型会内在地学习该语言内的思维链推理，基于该语言的结构，就像思维在该语言中如何运作。

是的，这是个好问题，我不确定怎么衡量这个。但我确实考虑过，我认为根据这些结果，你在斯瓦希里语中可能没有任何数学问题供模型学习，而且我认为确实有一些语言无关的东西在起作用，模型在推理方面是独立于语言学习的，然后如果需要，它可以用不同的语言表达出来。是的。

但是我没有，我觉得没有人，我认为我们还不知道答案。嗯。是的。所以基本上，有一个经常出现的问题是，为什么扩大规模会改善思维链，其中一种看法是我们可以取一个较小的模型，比如 P 62b，看看从扩展到 5400 亿参数时修复了哪些类型的错误。你可以看到，我们提出的这三类错误中的一些都得到了修复，因此扩展似乎对改善较小模型的不同类型错误有一种普遍的效果。

然后这里是同一个手写的维恩图，以不同的方式表达。所以基本上，你有一些可以通过标准提示完成的任务，所以用蓝色表示，然后思维链提示的目标是增加我们可以执行的任务集。例如。

现在，粉色显示的那些包括数学文字问题、符号推理和具有挑战性的常识推理。是的，问题是，你有没有做过一些实验来弄清楚，像这样的贡献到底有多少是因为当你输入更长的提示时进行更多计算的事实，比如说你通过模型创建多个嵌入来处理多个任务。

只是从模型的角度来看，是的，那样的话想要尝试多大程度上的非思维链提示，比如说开放长度。是的，是的。我们尝试过像 XX，XX X 之类的，但实际上并没有效果。所以我认为这不仅仅与计算有关。我认为还与语言引导模型作为推理的一部分有关。我明白了。

你有没有尝试以更多样的方式描述问题，而不是显示的那样。我知道。这真的是一个超级有趣的问题，所以这就是一种非常有趣的特性和紧急性。是的，你是说以三种不同的方式描述问题，看看这种描述是否更多，而不是逐步明确地描述，看看那样是否有效。是的。我还没有尝试，但如果有效我会感到惊讶。你试过让它这样吗？

我会把答案和解释的推理放进去，是的，这样做效果不佳。是的。但这也取决于任务，所以是的，是的我。这个家伙。似乎确实如此，是的。是的。那些反应像推理，可能就是你的计算。可以想象一下如果这是答案。某种程度上，你知道，就像某种程度上。

就像思维链的一部分非常结构化。它就像。如果结构相同，我们就更随机一些。是的，你可以试试。我会惊讶它是否有效，我认为输出标记对模型来说是相当重要的，是的。好的。诶。那么我们时间怎么样。好的，太好了。那么最后一部分。

我认为这是一种很酷的思维链技巧。基本上。😊，嗯。人们通常会生成一条思维链，然后得出最终答案，但有一个很好的技巧叫做自一致性，你可以用温度采样与模型结合生成许多不同的推理路径和最终答案，然后如果你只对最终答案进行多数投票。

这最终就像是在性能上有了相当大的提升。例如，在 GSMAK 上，你可以看到，这就像 MathW Pro 数据集，性能从 56 提升到了 74，这就是一个相当大的提升。对。是的，这里关于自一致性的平均数有多少哦。

我认为是 40。因此它增加了推理时间的计算成本，但。是的。提高性能可能无法回答这个问题，我很好奇需要绘制多少样本或多少个链才能得到显著结果，链的数量平均下来有什么权衡？

我觉得这要看，抱歉，问题是需要多少个链才能获得性能提升。我认为答案真的取决于数据集，但通常你可以用大约 16 个链获得不错的结果。我想。是的。抱歉。我们有一个问题。温度如何改变模型的工作方式。哦，好吧，问题是温度如何改变模型的工作方式。

基本上，当你使用温度解码时，语言模型可以随机选择一个输出，而不是总是选择概率最高的下一个单词。因此，基本上你会得到这些更随机的输出，它们仍然基于语言模型所学到的内容。

但这只是有点随机。好的。最后，像是自一致性似乎也有可出现性。我想部分原因是思维链正在涌现，因为如果不进行思维链，你的表现不会比随机更好。

但你确实能看到自一致性通过更大模型带来的巨大差异。嗯。🎼太好了。所以我快没时间了。让我稍微谈一下这个，我认为除了纯粹扩大语言模型的规模之外，这在行业中只有一部分人能够做到。

我认为有几个有趣的方向可以探索，一个是更好地提示和表征语言模型的能力，我认为现在我们只是刚刚触及，了解提示语言模型的最佳方式。

还有一些相当不错的应用工作，所以我听说可以使用语言模型来训练治疗师，以帮助创意写作和科学。我认为 chatGT 真的展示了语言模型在这方面的能力。我觉得基准测试也很缺乏，因为我们通常很快就解决了基准测试。例如，Palm 在 Big Bench 的测试中超越了平均水平的人类，你知道，在 Big Bench 发布后的大约一年内，所以我认为我们需要更多的基准测试，这将是一个重要的贡献。

最后一个问题是，我们如何能够利用计算机裂变方法来改善语言模型，使其使用成本更低，使用起来更方便。很好，所以我在这里结束，如果你有任何反馈或对 Google 感兴趣，随时给我发邮件。

是的，随时提问。😊。![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_9.png)

![](img/1fe3b7e42f45e3d11a61cea42eaf51cb_10.png)
