- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P6：6.DeepMind's Perceiver and Perceiver
    IOnew data family architecture - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT/Transformer 原理介绍 (中英文双字幕) - P6：6.DeepMind的Perceiver和Perceiver IO新数据家族架构
    - life_code - BV1X84y1Q7wV
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_0.png)'
- en: So today I'm going to be talking about some recent work that we've been doing
    at Deepmind developing this line of architectures that we're calling perceives
    and I'll be motivating this in terms of a goal that we have which is to develop
    a general purpose architectures and so just right off the bat I want to motivate
    why we care about general purpose architectures and so the first the first both
    of the reasons are fairly pragmatic。
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所以今天我将讨论我们在DeepMind进行的一些最新工作，开发我们称之为“感知”的这一系列架构。我将以我们发展的通用架构目标为动机，所以我想直接阐明我们为什么关心通用架构，第一两个理由都是相当务实的。
- en: but basically the idea is if we're thinking about all of the data that we could
    possibly imagine collecting in the world。a lot of it is basically involves what
    we think of as sort of traditional sense modalities。whether these things range
    from touch andropriception to echoloc to the kind of perception you need to ingest
    text however you want to format that to things to more exotic things like event-based
    cameras。![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_2.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 但基本上这个想法是，如果我们考虑到在世界上可能收集的所有数据，其中很多基本上涉及我们认为的传统感官方式。这些东西可能包括从触觉、前庭感知到回声定位，以及你需要以任何想要的格式摄取文本的那种感知，甚至还有更奇特的事物，比如基于事件的摄像头。![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_2.png)
- en: Whiskering touching with wither senses， things like smell and depth and all
    the way up to the kinds of sense modalities that we really think about when we're
    thinking about scientific perception and so basically as if we think about the
    full set of data and what it would take to actually model each of these different
    modalities it basically looks effectively intractable to try to engineer inductive
    biases that will work for every single one of these so we don't want to engineer
    them one by one this is an approach that's worked and in some ways that it's maybe
    a reasonable description of how we think about developing new architectures for
    different problems but it's just not going to scale we can't afford as a community
    to design to hand designign inductive biases that will work for each and every
    one of these and so rather than doing that we want to sort of build architectures
    that at least at first pass will allow us to handle everything。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 触觉与其他感官的结合，包括嗅觉、深度感知，以及我们在考虑科学感知时真正关注的感官模式。基本上，如果我们考虑到所有数据的完整集合，以及实际上对每种不同的感官模式进行建模所需的东西，尝试为每一个模式工程归纳偏见几乎是不切实际的。因此，我们不想一个个地进行工程设计，这种方法在某种程度上确实有效，也许合理地描述了我们如何考虑为不同问题开发新架构，但这根本无法扩展。作为一个社区，我们无法承担为每一个模式手动设计归纳偏见的成本，因此，我们希望构建至少在初步上能处理所有事物的架构。
- en: There's another practical argument for why we should work on general purpose
    architectures and that's because it will allow us to build simpler more unified
    systems so if you look at how in particular complex multimodal data streams are
    typically approached in the sort of the sensory computer vision or pattern recognition
    literatures effectively the typical way this is done is by using inductive biases
    that we know hold for the individual modalities and then engineer ways of combining
    those different subsystems so this can mean building specific specific head specific
    input modules for each of these things and then trying out the various different
    ways of combining them。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个关于为什么我们应该研究通用架构的实用论据，那就是这将使我们能够构建更简单、更统一的系统。如果你查看复杂的多模态数据流在感知计算机视觉或模式识别文献中的典型处理方式，实际上，通常的做法是利用我们知道适用于单个模态的归纳偏见，然后工程设计结合这些不同子系统的方法。这可能意味着为每一个模态构建特定的输入模块，然后尝试各种不同的组合方式。
- en: So this can work but it gives us systems that in principle really will only
    work on one or a small number of domains and it gives us systems that are very
    hard to maintain。tend to be fragile， tend to depend on specific processing assumptions
    about the input modalities so rather than do that we sort of want to move in the
    direction of having unified black box architectures that kind of just work and
    the idea here is that if we can get to that point we can abstract the architecture
    construction process and really focus on other more high-level problems so this
    is sort of the motivation for this line of work。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这可以起作用，但它给我们提供的系统原则上只会在一个或少数几个领域内有效，并且这些系统维护起来非常困难，往往脆弱，依赖于输入模态的特定处理假设。因此，我们希望朝着拥有统一黑箱架构的方向发展，这种架构可以更方便地工作。这里的想法是，如果我们能达到这一点，我们就可以抽象出架构构建过程，真正关注其他更高层次的问题，这就是这项工作的动力所在。
- en: And the way that we're going to be doing this is of course。by working on the
    most general purpose architecture that we have so far。which is basically a transformer，
    and you'll all be very familiar with the basic building blocks of a transformer。but
    just at a very high level we can think about what they do right。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做到这一点的方法当然是通过使用到目前为止最通用的架构，也就是变换器。你们都会非常熟悉变换器的基本构建块，但从一个非常高的层面来看，我们可以考虑它们的功能。
- en: which is they use a general purpose inductive bias， so they're non-local。which
    means they're not making domainspecific assumptions about which point should be
    compared to each other。rather they tend to be global in terms of the attentional
    focus that they have。They use position as a feature rather than a hard constraint
    of the architecture。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器使用通用的归纳偏差，因此它们是非局部的。这意味着它们不会对哪些点应该相互比较做出领域特定的假设。相反，它们在注意力聚焦上倾向于全球性。它们将位置作为特征，而不是架构的硬性约束。
- en: and this is in contrast to just sort of MLP based architectures or confNs in
    the way that they typically work。which used position as an architectural component
    to constrain how compute is happening。And then of course finally there's extensive
    weight sharing in the way that they're designed and because they focus on mat
    Mos。they tend to be TBU and GPU friendly so these are all very nice things about
    the way transformers work。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这与基于MLP的架构或典型工作方式的卷积网络形成对比，它们将位置作为架构组件来限制计算的发生。当然，最后，它们在设计上有广泛的权重共享，并且由于它们关注矩阵运算，它们通常对TPU和GPU友好，因此这是变换器工作方式的一些优点。
- en: Of course on the other hand they have very poor compute memory scaling and there
    are two components of this。so attention itself scales quadraically so there's
    there's this big O of m squared L complexity at the heart of transformers and
    I like writing it this way because it really emphasizes that this is a property
    of basically as you make your models bigger either at the input size or as you
    make them deeper。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，另一方面，它们在计算内存扩展方面表现很差，这有两个组成部分。因此，注意力本身的扩展是平方的，变换器的核心是这种大O的m平方L复杂度。我喜欢以这种方式书写，因为它真正强调这是一个属性，基本上是随着你增大模型，无论是输入大小还是深度。
- en: you're just this problem is just going to get worse and because you have this
    scaling in depth as well。there's another sort of practical thing that happens
    here because're the amount of compute that we're doing is proportional to the
    input size so there's no bottleneck in the way that standard transformers work
    even the linear scaling becomes a problem and so in practice for very very large
    transformers this can often be the bottleneck that really matters but they're
    both at play here and so we really want to sort of tamp down both of these and
    so sort of the perspective here is that have really general purpose architectures。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题只会变得更糟，因为你还涉及到深度的扩展。这里还有另一个实际的问题，因为我们进行的计算量与输入大小成正比，因此在标准变换器的工作方式中没有瓶颈，即使线性扩展也会成为一个问题。因此，在实际应用中，对于非常非常大的变换器，这通常是一个真正重要的瓶颈，但这两者都在这里起作用，所以我们确实希望抑制这两者，从而使我们拥有真正通用的架构。
- en: We can't have ones that are just sort of in principle general。we have ones that
    we have to have ones that you can actually use on the scales and the kinds of
    data that we care about。And so just to this will all be old half for all of you。but
    just the way that standard QKV attention works is basically like this。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能仅仅有一些原则上通用的函数。我们必须有一些实际可以在我们关心的尺度和数据类型上使用的函数。因此，对你们来说这可能都是旧的，但标准的QKV注意力的工作方式基本上是这样的。
- en: so it's all about matrix multiplications so we have some input we compute the
    query keys and values by having a 1D convolution a one by one convolution that
    we run over the input we then compute the attention the attention scores this
    is a matrix multiply that has the following these sorts of shapes we then use
    the output here to compute。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这全是关于矩阵乘法的。我们有一些输入，通过一维卷积和一乘一卷积计算查询键和值，然后计算注意力，注意力得分是一个矩阵乘法，具有以下这些形状。然后我们使用这里的输出来计算。
- en: The weights to compute the actual output of the attention module itself。and
    then finally we won run this through an additional MLP which is applied convolutionally
    to get the outputs。So this is the starting point of what we're working on here。And
    let me just briefly just reiterate why we would want the sort of advantages that
    we have with these sort of standard transformers。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 权重，以计算注意力模块的实际输出。最后，我们通过一个额外的多层感知机（MLP）运行这个，应用卷积来获取输出。这是我们在这里工作的起点。让我简要重申一下，为什么我们会想要这些标准变换器的优势。
- en: so nonlocality is one of the two sort of inductive bias principles that we have
    here it's useful I think to contrast this to way that the effective locality that
    you get in conves and what this actually means so if we look at basically as a
    function of depth which inputs can see which other ones which means like how easily
    it is to express a function of two input points let's say we look at these this
    yellow and purple point here at the input now I've sent them sort of as far apart
    as possible but we might ask how deep would the effective computation have to
    be before you actually process these two and if you look at a three by three convolution。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 所以非局部性是我们这里的两种归纳偏差原则之一，我认为将其与有效局部性进行对比是有用的，这在卷积中意味着什么。如果我们基本上从深度的角度来看，哪个输入可以看到其他输入，这意味着表达两个输入点的函数有多容易。假设我们看一下输入中的这个黄色和紫色点，我把它们尽可能地分开，但我们可能会问，有效计算必须多深才能处理这两个点，如果你看一下三乘三的卷积。
- en: You're talking you're having to look basically until the very end of the network
    until you're processing these things together and what this means is that the
    functions that you can express that actually look at both of these points into
    up being quite shallow because they have to be built on top of this very very
    deep stack that just gives you the locality and so in point of fact if you look
    at for example the way Resnets work so you have an initial block which has a 7
    by7 convolution and then afterwards it's  three by three cons all the way up you
    need 283 by three cons with that standard processing stack before all of the 224
    by 224 pixels in an image are looking at each other and what this means is that
    in a Resnet 50 the points on the very edge of the pixels actually never see each
    other and this this is I found this a little bit counterintuitive but it suggests
    that we really are constraining quite a lot the functions that are easy to express
    with these models and so there are some functions of images you just can't you
    can't capture with a resant 50。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须基本上等到网络的最后，才能将这些东西一起处理。这意味着你能够表达的函数实际上看起来非常浅，因为它们必须建立在这个非常深的堆栈之上，而这个堆栈仅提供局部性。实际上，如果你看看例如ResNet的工作方式，你会发现初始块有一个七乘七的卷积，然后之后是三乘三的卷积，直到最后，你需要283个三乘三的卷积，才能使图像中的224乘224像素互相查看。这意味着在ResNet
    50中，像素边缘的点实际上从未看到彼此。我发现这一点有点违反直觉，但它表明我们确实在很大程度上限制了这些模型中容易表达的函数，因此有些图像函数你根本无法用ResNet
    50捕捉到。
- en: On the other hand， if you look at an architecture that has global attention
    over the full over the full input。so a transformer， if you could scale it that
    way or perceives we're going to be talking about。all of the pixels can interact
    so the model can basically capture these things and express these functions much
    more easily can be expressed in things that put locality first。We also the other
    sort of interesting property of these sorts of architectures is that position
    is feurized and this basically means that we're no longer sort of encoding the
    architectural location of something to figure out where it's located with respect
    to the other ones and this allows the network to basically use any positional
    information that it wants。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你看一个在整个输入上具有全局注意力的架构。因此，变压器，如果可以这样缩放，或者说我们将讨论的感知。所有像素都可以相互作用，因此模型基本上可以捕捉这些东西，并更容易地表达这些功能，这些功能可以以将局部性放在首位的方式来表达。我们还发现这些架构的另一个有趣属性是位置被特征化，这基本上意味着我们不再对某物的架构位置进行编码，以确定它相对于其他物体的位置，这允许网络基本上使用它想要的任何位置信息。
- en: but can also discard it as it prefers and so this is the standard way it's done。of
    course in the context of architectures that use Fourier or sinusoidal like features。but
    there's a lot of flexibility here。Okay， so now just thinking in terms of how conves
    relate to transformers sort of at the opposite end。it may look like that we have
    a sort of scalability versus general generality tradeoff and so if we look at
    convenants the way that they're applied so typically we can think about using
    them on grid structured data。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但也可以根据需要丢弃，这是标准的做法。当然是在使用傅里叶或正弦特征的架构背景下。但这里有很多灵活性。好的，那么现在考虑一下卷积与变压器在对立端的关系。可能看起来我们在可扩展性与通用性之间有一种权衡，因此如果我们观察卷积的应用方式，通常我们可以认为它们是在网格结构数据上使用。
- en: theyre of course generalizations of convolutions that work on。on data sets with
    more interesting topology， but typically we can think of them as operating on
    grids of in some sort of space。whereas transformers apply to generic sets， so
    transformers are more general from this point of view。On the other hand， they
    scale much， much worse， so conves are linear， both in the input points。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，它们是可以应用于具有更有趣拓扑的数据集的卷积的广义，但通常我们可以认为它们在某种空间的网格上操作。而变压器适用于通用集合，因此从这个角度来看，变压器更为通用。另一方面，它们的缩放要差得多，因此卷积在输入点上是线性的。
- en: the filter size and the number of layers of that architecture。whereas transformers
    have this quadratic scaling and they're still linear in the depth。So from this
    point of view， what we're interested in doing in the perceive line of work was
    to scale transformers but to keep the generality property。so we want something
    that lives in between these two extremes。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构的过滤器大小和层数。而变压器具有这种平方缩放，且在深度上仍然是线性的。因此，从这个角度来看，我们在感知工作中的兴趣在于缩放变压器，但保持通用性属性。所以我们希望有一些介于这两种极端之间的东西。
- en: And the way that we do this is by looking at self-attention and sort of modifying
    it in a way that allows us to scale better。so to walk through what self-attention
    actually does in sort of standard transformers。we take our input array which here
    is written number as the indices which is the number of tokens or the number of
    pixels basically the number of input units depending what you're looking at and
    the number and the channels。we have a 1D convolution so this is big O of M with
    respect to the QK and V。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做的方法是观察自注意力，并在某种程度上修改它，以便更好地进行缩放。因此，为了理解自注意力在标准变压器中实际上做了什么。我们取输入数组，这里以索引形式写出，表示标记的数量或像素的数量，基本上是输入单元的数量以及数量和通道。我们有一个一维卷积，因此这是关于QK和V的O（M）。
- en: We then compute the attention maps using the output of this operation。this gives
    us a matrix multiply， which is the source of the quadratic scaling。And then finally。we
    compute output features with another matrix multiply。This is already we're already
    rate limited here because for even standard resolution images in this quite large
    so it's around 50。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用该操作的输出计算注意力图。这给我们带来了一个矩阵乘法，这是平方缩放的来源。最后，我们用另一个矩阵乘法计算输出特征。这里我们已经受到速率限制，因为即使是标准分辨率的图像也相当大，所以大约是50。
- en: 000 for standard imagenet images which are going to very small so this is something
    that just isn't going to work if we want deep architectures。So what we do is we
    replace at the input to the architecture。we replace the self- attentionten with
    a cross attention layer and we do this using basically a learned query。and so
    we're replacing only the query from the input here with a learned component and
    so these indices and channels you can just think of these as basically working
    like a learned initial state for an RNN。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于标准的 imagenet 图像，大小将会非常小，因此如果我们想要深度架构，这种情况是行不通的。所以我们所做的是在架构的输入处进行替换。我们将自注意力层替换为交叉注意力层，这基本上是通过一个学习到的查询来实现的。因此，我们只替换输入中的查询，使用一个学习到的组件，这些索引和通道可以被看作是基本上作为
    RNN 的学习初始状态。
- en: there's a variety of names that this idea goes under in the literature。we refer
    to them as sort of late as latetnts but they're sometimes called inducing points
    or other things。So the basic idea is we're learning the inputs to the query and
    keeping the key value of it the same。The down or the sort of upside of this is
    that when we compute the attention map after this。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，这个想法有很多不同的名字。我们将它称为某种潜变量，但有时也称为诱导点或其他名称。基本思想是我们学习查询的输入，并保持它的键和值相同。这样做的好处是，当我们在此之后计算注意力图时。
- en: now we basically turn this from a square matrix to a rectangular matrix and
    reduces the complexity of the matrix multiplied to big of MN。so now it's linear
    in the input size。And second the second matrix multiply has the exact same property。so
    it becomes from quadratic becomes linear and the quite cool thing about this is
    that okay so the cross attention is linear and complexity but the output is actually
    smaller and so this I think is actually the more important point here is that
    this allows us to map something which is quite large into something that has size
    that's independent of the input we saw we have full control over this it's a hyperparameter
    and this allows us to build deep networks on top of this on top of this latent
    so because this is of a small size that we can control。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们基本上将其从一个方阵转换为一个矩形矩阵，并将矩阵乘法的复杂度降低到 MN。因此，现在它在输入大小上是线性的。第二个矩阵乘法也具有相同的特性。因此，从二次变为线性，这一点很酷。交叉注意力的复杂度是线性的，但输出实际上更小，所以我认为这里更重要的一点是，这使我们能够将一些非常大的东西映射到一个与输入无关的大小上。我们对此有完全的控制，这是一种超参数，这使我们能够在此基础上构建深层网络，因为这是一个可以控制的小尺寸。
- en: we can afford to have quadratic complexity on top of this。And so we use this
    idea sorry I'm still a little bit confused as to how you guys are able to turn
    the query into a rectangle in the second step。is it because you replaced the query
    with a learned something that is significantly smaller compared to the input size
    in the first step。Yeah that's exactly right so if you look at so the underlying
    matrix multiply here which is written as the QK transpose so this will basically
    so the outer dimension here has shape n which is determined by the query and so
    by shrinking that query we're just changing the output of the matrix multiply。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在此基础上承受二次复杂度。所以我们使用这个想法，对不起，我仍然有点困惑，你们是如何在第二步中将查询转化为矩形的。是因为你们将查询替换为一个学习到的东西，而这个东西的大小相比于第一步的输入大小要小得多吗？是的，正是如此。如果你看看这里的基本矩阵乘法，它写作
    QK 转置，所以外部维度的形状是 n，这由查询决定，因此通过缩小查询，我们只是在改变矩阵乘法的输出。
- en: Okay， thank you。嗯。😊，Yeah， so I guess。Sorry go ahead please Okay go so basically
    you only do that for the query right so key and value remain like the original
    size matrices。correct。That's right yeah， Okay， but so basically so。I don't know
    what i'm not understanding basically so the problem for me is that for a query
    now in my head I'm like looking for。let's say I have like the I token now there
    is no IF query anymore。Doesn't that cause a problem？
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，谢谢。嗯。😊，是的，我想。对不起，请继续。好的，所以基本上你只对查询这样做，对吧？所以键和值仍然保持原始大小的矩阵。对吗？没错，好的，但基本上。嗯，我不太明白，基本上对我来说，问题是对于一个查询，现在在我脑海中我在寻找。假设我现在有
    I token，现在没有 IF 查询了。这不会导致问题吗？
- en: Like when I'm trying to use it。And like to compute scores。Yeah so what's happening
    here is your compare you'll have a smaller subset ofque so if you think about
    this not in terms of matrix multiplies but in terms of comparing each query to
    each key so in normal self attentionten we have one query for each key so every
    point compares to every other point right so here what we've done is instead of
    comparing every point to every other point we have a set of sort of cluster centers
    you might be able to think about them as so it's a smaller number and we compare
    each of those to each of the input points。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我在尝试使用它时一样。并且要计算得分。是的，所以这里发生的事情是你会比较，你会有一个较小的子集，因此如果你从比较每个查询与每个键的角度来看，而不是从矩阵乘法的角度来看，在正常的自注意力机制中，我们对每个键都有一个查询，因此每个点都与其他每个点进行比较，对吧？所以在这里我们所做的是，不是将每个点与其他每个点进行比较，而是我们有一组可以想象成“聚类中心”的东西，所以这是一个较小的数量，我们将每一个聚类中心与每一个输入点进行比较。
- en: But we don't know which。Which tokens technically belong to which clusters， right？
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不知道哪些标记技术上属于哪些聚类，对吧？
- en: That's right so it has to be learned yeah exactly so one way to think about
    this about what's happening here is that we're we're instead of so in a normal
    selfcent selfatten and a transformer by comparing to all to all we're sort of
    saying okay。I know what the feature is at this point and I want it to attend to
    similar features here what we're saying is we're learning a bunch of supplementary
    points that that should be sort of maximally similar to some subset of the inputs。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 没错，所以它必须是学习的，没错。所以我们可以这样思考这里发生的事情：在正常的自注意力和变换器中，通过对所有点进行比较，我们可以说，好的。我知道这个点的特征，我想让它关注相似的特征。在这里我们所说的是，我们正在学习一组补充点，这些点应该与某个输入子集尽可能相似。
- en: So correct if I'm wrong， but this is essentially doing some sort of hard tension
    where you're saying like like instead of like querying over all the points。Is
    select like some points， which we think are like vital similar and only like puts
    all attention over this。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我说错了，请纠正我，但这本质上是在进行某种硬性约束，你是在说，比如说，不是对所有点进行查询，而是选择一些我们认为是重要且相似的点，并且只对这些点进行所有的注意力。
- en: Hardening like this points you are selecteding， right？
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这样加固是指你选择的点，对吧？
- en: Yeah so they're related that would be one way to think about it the sort of
    the slight modifier to that idea though is that they basically live in an abstract
    space。so they're not assigned sort of one to one to one of the input queries or
    to one of the input points they're sort of learned so they can be somewhere in
    the middle but I think that's a good way to think about it that's a good intuition。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以它们是相关的，这可以是一种思考方式，但这种想法的稍微修改是，它们基本上生活在一个抽象空间中。因此，它们并不是一一对应于某个输入查询或某个输入点，而是某种程度上是学习得来的，所以它们可以在中间某个位置，但我认为这是一个好的思考方式，这是一种良好的直觉。
- en: But the I guess one of the places where I'm a little confused here is the you
    have here indices and indices for the two like the purple and green matrices on
    the far left。But those indices are not necessarily corresponding to inputs like
    in the NLP space those would not necessarily be tokens right these are just sort
    of random disease。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我想我在这里有些困惑的是，你这里有索引和两个矩阵（紫色和绿色）的索引，在最左边。但这些索引不一定对应于输入，例如在 NLP 领域，这些不一定是标记，对吧？这些只是一些随机的标识。
- en: but the impact matrix in this case is the result of some kind of mapping from
    the input tokens to an N D matrix。is that right。No it's actually so they it basically
    acts like it's it's a learned set of weights is one way to think about it so they
    function exactly the same way that learned position encodings do so it's basically
    just a you know it's a learned embedding but it's not it's not conditioned on
    anything it's just sort of。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 但是在这种情况下，影响矩阵是某种从输入标记映射到 N 维矩阵的结果。是这样吗？不，实际上它基本上就像是，它是一个学习的权重集，这可以是一种思考方式，所以它的功能与学习的位置编码完全相同，因此它基本上只是一种学习的嵌入，但并不是基于任何条件的，它只是一种。
- en: It just it's just a set of weights。Okay， that makes more sense， thank you。Okay。so
    if there are no more questions， I'm going to keep going， but of course feel free
    to interrupt me。So the way that given this idea， so we have this learned latent
    array， which again。it functions sort of like an RNN initial state or it's a set
    of weights。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一组权重。好的，这样更有意义，谢谢你。好的。如果没有其他问题，我将继续，但当然请随时打断我。基于这个想法，我们有这个学习的潜在数组，它的功能有点像
    RNN 的初始状态，或者说它是一组权重。
- en: we basically randomly initialize that and then we use this to attend onto the
    input byte array and so the byte array here is the flatten set of pixels for example
    for imagenet。And the output of this is going to live in the same space as so the
    same index space as the latent array does。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基本上是随机初始化，然后利用它来关注输入的字节数组，因此这里的字节数组是像 ImageNet 这样的像素平面集合。而这个输出将与潜在数组处于相同的空间中，即相同的索引空间。
- en: and there's residual connections in the way that you would normally do in an
    intention layer as well。So once we're in this space we can then build an architecture
    by taking by using a standard transformer but phrased in the latent space rather
    than in the input space and this is going to allow us to basically end up because
    we sort of distilled the input down to the smaller space we can still flexibly
    allow all of these points to interact so this should be still nearly as expressive
    as the transformer as a normal transformer is and then each of the modules here
    now is quadratic in the latent size rather than the input size so this is something
    that we can control quite a lot。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 并且在意图层的方式中有残差连接。因此，一旦我们进入这个空间，就可以通过使用标准的变换器，在潜在空间中构建一个架构，而不是在输入空间中，这将使我们基本上能够因为我们将输入提炼到较小的空间中，仍然灵活地允许所有这些点进行交互，所以这仍然应该与普通变换器一样具有表现力，而且这里的每个模块现在的潜在大小是二次的，而不是输入大小，所以这是我们可以相当控制的。
- en: So in the original version of the procedure， we found it was very helpful to
    have additional crossats。so this is certainly something that you can do and the
    reason sort of the intuition behind this is that if this bottleneck is quite severe。we
    can't maintain all of the information from the input and so we want these queries
    which are now sort of conditioned on the past to be able to look back at the input
    point and so this is something that we found to be quite helpful when tuning for
    the first paper but the caveat I will say is that we're no longer recommending
    this is best practice because these crossats end up being quite heavy but this
    is something that you can explore certainly if you want sort of more conditional
    queries or if you want to be able to cross attendends and new inputs that are
    coming in。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在程序的原始版本中，我们发现增加交叉注意力是非常有帮助的。这绝对是你可以做的事情，背后的直觉是如果这个瓶颈相当严重，我们无法保持所有输入的信息，因此我们希望这些查询现在在某种程度上依赖于过去，能够回顾输入点。因此，这在为第一篇论文进行调优时被发现非常有帮助，但我会说的警告是，我们不再推荐这作为最佳实践，因为这些交叉注意力最终是相当沉重的，但如果你希望有更多的条件查询，或者希望能够交叉关注新输入，这绝对是你可以探索的事情。
- en: The other thing that we found quite helpful in the context of data sets that
    have a limited amount of data。which for these architectures includes INe is to
    allow weight sharing in depth and so this basically just amounts to tying the
    weights for the different cross attention and different self-attention layers
    as they're repeated so this ends up looking like an RN that's enrolled in depth。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集有有限数据量的背景下，我们发现另一件非常有帮助的事情，针对这些架构（包括 INe），是允许在深度中共享权重。因此，这基本上只是意味着将不同的交叉注意力和自注意力层的权重绑定在一起，因为它们是重复的，所以这最终看起来像是在深度中注册的
    RN。
- en: So this is just at a high level this gives us an architecture that we can apply
    to images but doesn't make any assumptions about image structure。so it's one that
    you can use elsewhere and we basically we give information about the input sort
    of spatial structure by having positional encodings and here we use a 2D4ier feature
    position encoding and just to show you kind of what that looks like here to give
    you a sense so each of the input points is assigned basically so you'llll be some
    position here and we have sinusoidoidal and cosineus sooidial features in 2D so
    this is basically a 4ier8d composition of the position of the 2D input and a couple
    of things that we found where that if we sampled the frequency that's the maximum
    frequency that it's used up to the NOSs frequency of the signal we end up doing
    better than if you use a lower version of this and this basically is because this
    will allow every other point to be aware of every distinct。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，这为我们提供了一种可以应用于图像的架构，但不对图像结构做任何假设。因此，你可以在其他地方使用它，我们基本上通过使用位置编码提供有关输入空间结构的信息，这里我们使用2D傅里叶特征位置编码。为了给你一个概念，每个输入点基本上会被分配一个位置，我们在2D中使用正弦和余弦特征，这基本上是2D输入位置的傅里叶合成。我们发现，如果我们取样的频率是信号的最大频率，我们的表现会比使用较低版本的频率要好，这基本上是因为这将使每个点都能意识到每个不同的点。
- en: Point in the image， whereas if you sample at a lower frequency， you can end
    up with aliasing。and so not all points will be sort of legible。We also found that
    sampling the spectrum relatively densely tends to help and the contrast here at
    the time we were developing this with respect to Nerf so NF at least in earlier
    implementations used quite a small number of frequency bands we found that the
    more we added the better we did so in general this is something to be attentive
    to。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的每个点，而如果你在较低频率下取样，可能会出现混叠。因此，并不是所有点都会清晰可读。我们还发现，较密集地采样频谱往往有助于对比，在我们开发这项技术时，与Nerf相比，Nerf在早期实现中使用的频带数量相对较少，我们发现增加频带数量能显著提高效果，因此一般来说，这是一个需要关注的方面。
- en: And then finally， as opposed to language where you typically have addition of
    whatever your embedding is with the sinusoidoidal or position encoding that you
    use here we found that concatenating them performed consistently better。And so
    this may be because the positioning the content embedding is not as sparse as
    it is in language。
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，与语言不同，通常你会将嵌入与正弦或位置编码相加，但我们发现连接它们的表现一直更好。这可能是因为内容嵌入的定位不像语言中那样稀疏。
- en: we're not totally sure， but this is something that I observed consistently。And
    before I move on to results I just want to contrast this to some sort of other
    approaches for using transformers in the image context。so obvious the obvious
    precedent here is visual transformers and I think this is a this line of work
    is great especially in the image context。but there are some caveats about it that
    make it less suitable for sort of more general purpose use。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不完全确定，但这是我一贯观察到的事情。在我转向结果之前，我想将其与在图像上下文中使用变换器的其他一些方法进行对比。因此，这里显而易见的先例是视觉变换器，我认为这项工作在图像上下文中特别出色。但关于它有一些限制，使其不太适合更通用的用途。
- en: so one is that so vision transformers do use an input 2D convolution。so this
    is often phrase in terms of patches input patches a special case of a 2D transformer。so
    it does restrict the class of inputs you can use it for。And because we're basically
    building this patching or convolution into it。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，视觉变换器确实使用了输入的2D卷积。这通常被描述为输入补丁，2D变换器的特殊情况。因此，这限制了你可以使用的输入类别。而且因为我们基本上将这种补丁或卷积构建进来了。
- en: this means that this as an approach really isn't sufficient to get it to work
    on non-gririd data。there are other ways you could adapt it， but this is something
    that you will have to special case for every domain you're looking at。And then
    finally， because we have this sort of input where we're telling the architecture
    what it should look at first in the initial grouping。this does amount to getting
    rid of the nonlocity assumption it's not super clear doing how much doing this
    just once will make a difference。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着这种方法在非网格数据上实际上是不够的。还有其他方法可以调整它，但这是你需要为每个领域特别处理的事情。最后，由于我们有这种输入，告诉架构在初始分组中应该首先关注什么，这确实意味着消除了非局部性假设，但不太清楚这样做一次会有多大差别。
- en: but this is something to be aware of when you're thinking about this architecture。And
    then finally cross attention itself is used quite broadly in the vision literature。so
    just a couple to highlight a couple of examples， debtor。which is an object detection
    method from Facebook。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 但这是在考虑这种架构时需要注意的事情。最后，交叉注意力在视觉文献中被广泛使用。因此，我只想强调几个例子，比如来自 Facebook 的一个物体检测方法。
- en: basically has a convolutional backbone that's then used to give an output feature
    map。this is then passed into a transform encoder decoder and of course whenever
    year encoder decoder you think cross attention because from the encoder to the
    decoder there's a cross attention step。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上有一个卷积主干，然后用于生成输出特征图。这被传入一个变换编码器-解码器，当然，每当你想到编码器-解码器时，就会想到交叉注意力，因为从编码器到解码器有一个交叉注意力步骤。
- en: and so they're using basically the cross attention to go from some feature map
    representation to something that looks more like the object bounding boxes。There's
    also quite nice work on learning self-supervised or unsupervised object segmentation
    models。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 所以他们基本上使用交叉注意力将某些特征图表示转换为更像物体边界框的东西。关于学习自监督或无监督物体分割模型的研究也相当不错。
- en: and in this work they're doing something very similar where they have a convolutional
    backbone。they then use something like these latent the latetnts that we introduce
    here。to do when they call them slots here but basically to assign some of the
    output pixels to different slots so that they sort of have independent complementary
    decoding of the slots in the segmentation model here。And there's a lot of other
    things。Okay， so first I would add just so now I'm going to sort of walk you through
    results of this of this model hi can I oh go ahead I'll go after you go for it
    Okay cool sorry for that can you go back a couple of slides where you had like
    you know like how how like the inputs like flow into like I think one about yeah
    that one okay so two questions that latent transformer is basically like a self
    attention is that correct？
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，他们做了非常类似的事情，使用了一个卷积主干。他们然后使用我们在这里引入的潜在变量，称之为“槽”，基本上是将一些输出像素分配到不同的槽，以便在分割模型中对槽进行独立的互补解码。还有很多其他内容。好的，我首先会补充一下，现在我将带你们了解这个模型的结果。嗨，我可以哦，你先说，我在你之后。好的，抱歉，你能回到几张幻灯片吗？你知道的，就是输入是如何流入的，我想是那张，对，就那张。好的，有两个问题，潜在变换器基本上是自注意力，对吗？
- en: Yeah so the latent transformer is a self a fully selfational transformer got
    it and why see like for like the key in value they flow directly into the cross
    attention and there is like the query also flowing into it but the latent array
    array is flowing into the cross attention imp parallel to the query can you explain
    that？
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，潜在变换器是一个完全自注意力的变换器，明白了。为什么像关键和价值直接流入交叉注意力，同时查询也流入，但潜在数组是并行流入交叉注意力的，能解释一下吗？
- en: Yeah so this is just speaking here it's meant to depict the residual connection
    so the cross attention this is sort of a cross attention depicted as a cross attention
    module and so the cross attention itself has the attention it has a residual connection
    and then there's an MLP so that's what that's meant to indicate okay but it's
    basically the QKV standard。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这里只是想描述残差连接，因此交叉注意力被描绘为一个交叉注意力模块，交叉注意力本身具有注意力和残差连接，然后还有一个 MLP，这就是这部分的意思。好的，但基本上是
    QKV 标准。
- en: Go it。Thanks。😊，H， I had a question that is slightly related to this if we can
    just stay up this slide actually。so I think one thing that's interesting about
    this。Sure sorry， I lost。You're cutting off。It's mostly consisting of attention
    layers， whether it's self attention or creating image transformers。can you hear
    me， is that coming through？No， it's cutting off， I think， but I think so recent。Oh。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。谢谢。😊，我有一个稍微相关的问题，如果我们可以继续这张幻灯片。关于这个事情我认为有趣的一点。抱歉，我失去了。你在打断我。它主要由注意力层组成，无论是自注意力还是创建图像变换器。你能听到我吗，传过来了没有？不，信号有点卡顿，但我想，最近的。哦。
- en: okay。Should I type it is that should I type Yeah I think that's a good idea
    yeah。I'll type it thanks sorry。All right，s kind。Feel free to go ahead， I'll type
    it slowly and。![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_4.png)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我应该打字吗，是的，我觉得这是个好主意。我会打的，谢谢，抱歉。好的，随意进行，我会慢慢打。![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_4.png)
- en: Sounds good sounds good to me Yeah actually can I chime in drew while you're
    on that previous slide the flow so these residual connections I actually didn't
    know the cross attention used that how reliant are these sequential cross attention
    layers on the residual connections。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 听起来不错，听起来对我来说也很好。实际上，我可以插嘴吗，德鲁，当你在那张幻灯片上时，这些残差连接我其实不知道交叉注意力是如何依赖这些的，这些序列交叉注意层对残差连接的依赖程度如何。
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_6.png)'
- en: Yeah， so here。Here in the initial so two things I will say is that in the initial
    cross attention it doesn't really make a difference so this is something we've
    ablated when we get to the proIO version of this we also did the same thing in
    the decoder cross attention and it can make some of difference it can make a difference
    there depending on what you're doing I think it's actually essential in when you're
    using repeated cross attention of this way so when you have this sort of iterative
    structure and the reason for this is that the thing that's actually used to condition
    the query is basically that's your full representation of sort of the state of
    the architecture so far and so the skip connection is from it's in basically the
    query channel it's in the late the latent space and so this is basically what
    allows you to end up with this sort of dense dense and stable architecture。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这里。在初始阶段，我会说两件事，在初始的交叉注意力中，这其实并没有什么区别。我们在制作proIO版本时进行了消融实验，在解码器的交叉注意力中也做了同样的事情，这可能会有一些区别，具体取决于你在做什么。我认为在使用这种重复交叉注意力时，实际上是必需的，所以当你有这种迭代结构时，原因在于，实际用于条件查询的东西基本上是你目前架构状态的完整表示，因此跳跃连接是在查询通道中，处于潜在空间中。这基本上使你能够得到这种密集且稳定的架构。
- en: Thank you。😊，嗯。😊，Okay。So to imagenet okay so in standard imagenet processing
    basically we compare against a few so this is a little bit out of date at this
    point。but against a few sort of just sort of sanity check baselines here so comparing
    against Res 50 and then at the time the best vision transformer model that was
    purely on imagenet and we're definitely in the ballpark this isn't these aren't。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 谢谢。😊，嗯。😊，好的。那么，关于imagenet，实际上我们在标准的imagenet处理过程中，基本上是比较几个基准，所以这个信息在目前来看有点过时。但对比一下Res
    50，以及当时纯粹在imagenet上表现最佳的视觉变换模型，我们绝对在合理范围内。
- en: Anywhere in your state of the art results but this is an architecture that again
    it' not using any 2D convolutions and so the fact that it was able to do this
    well we found very。very surprising at the time one of the quite cool things about
    this is that because this architecture is not making any assumptions the architecture
    itself isn't making any assumptions about the spatial structure of of the input
    images we can look at permuted imagenet and in the first version of this what
    we do is basically we compute the features using the 2D position so the 2D position
    is sort of fixed to a position to the pixel and then we just shuffle it all and
    so this is basically we'll give you a sense of how dependent the baselines are
    on input image structure and so if we look at if we look at the transformer perceive
    by construction they don't change so this is not an empirical finding this is
    a property of the models but we find that Resnet 50 falls the performance falls
    by about half and VIT which again only has one layer where it's relying on the
    spatial structure。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在你最先进的结果中，但这是一个架构，它同样没有使用任何二维卷积，因此能够做到这一点让我们感到非常惊讶。这个架构有一个很酷的地方是，因为它没有对输入图像的空间结构做出任何假设，我们可以查看置换的Imagenet，在这个架构的第一个版本中，我们基本上使用二维位置计算特征，因此二维位置是固定在像素上的，然后我们将其全部洗牌，这基本上会让你感受到基线对输入图像结构的依赖性。所以如果我们观察变换器，由于构造原因，它们不会改变，因此这不是一个经验发现，而是模型的一个特性，但我们发现Resnet
    50的性能下降了大约一半，而VIT只有一层依赖于空间结构。
- en: Also it has about a 15 point drop and so this suggests that it's relying quite
    a lot on that very first one to give it some information about the structure。We
    can push this a little bit by instead of relying on 2D Fourier features learning
    completely learned positional encodings and this basically this is an architecture
    now this is a model that has absolutely no information about the input structure
    and so shuffling them and learning them again is absolutely equivalent and we
    find that this architecture also can be pushed about 70% and we've gotten slightly
    better numbers here in general this seems to work worse but so the 2D information
    is useful but it's quite cool that you can get what would have been numbers comparable
    to state of the art about5 or six years ago so this is quite cool。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它大约有15个点的下降，这表明它在相当程度上依赖于第一个以提供有关结构的一些信息。我们可以通过不依赖于二维傅里叶特征而完全学习位置编码来推动这一点，基本上这是一个现在完全没有关于输入结构信息的架构，所以洗牌并再次学习是完全等效的，我们发现这个架构也能推动大约70%，而且我们在这里得到了稍微更好的数字，总体来说，这似乎效果更差，但二维信息是有用的，能够得到五六年前与当时最先进技术相当的数字，真是很酷。
- en: Sorry I'm a little thick here， you're saying the difference between the last
    two rows is that the second to last row has a two dimensional position of betdding
    and the last one has a one dimensional position of betdding essentially。as I right。So
    it's learned， so it's basically it'll be it's I believe a 256 dimensional vector
    that's learned。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，我有点迟钝，你是说最后两行之间的区别在于倒数第二行有一个二维的投注位置，而最后一行本质上有一个一维的投注位置。我说得对吗？所以它是被学习的，基本上它会是我相信的一个256维的向量。
- en: but it doesn't it basically it means that the model itself has no information
    about the input's spatial structure。So the 2D positional encodings that we're
    using you end up having about 200。it's 200 some features depending on what you're
    looking at。but theyre they give you very detailed information about the 2D structure
    of the input because they're based on a 480 composition of the input space。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 但它的意思基本上是模型本身对输入的空间结构没有任何信息。因此，我们使用的二维位置编码最终大约有200个。这是200多个特征，具体取决于你在看什么，但它们能给你非常详细的信息，关于输入的二维结构，因为它们是基于输入空间的480个组成。
- en: Okay， that makes sense， thank you。Hi drew can I ask a question about frequency
    you use to generate those sensor wave。Yeah， so like a cover slide before。s。![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_8.png)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这有道理，谢谢。嗨，德鲁，我可以问一个关于你用来生成那些传感器波的频率的问题吗？是的，就像之前的封面幻灯片。![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_8.png)
- en: Yeah，Yeah， yeah。 yeah。they like。So basically， I do have taken some lecture in
    signal processing。And I know if I。![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_10.png)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，是的，没错。他们喜欢。所以基本上，我确实在信号处理方面上过一些课。我知道如果我。![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_10.png)
- en: Want to avoid， you know， alienency。I need to sample with， at least my frequency。So
    I'm curious to know why to use frequency starting from one to the ni frequency
    instead of starting from micro frequency to some very high frequency。OhI see so
    basically so the maximum frequency that's used is always NquiIS so anything about
    Nquist is going to be alias so you're not actually going to be able to resolve
    it because it's in pixel space right so we sample one is basically just giving
    you an oscillation that covers the entire image and so this is basically just
    to sample the full range of nonallias frequencies。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 想要避免，你知道的，别扭。我需要至少按我的频率进行采样。因此，我很好奇为什么要使用从1到奈奎斯特频率的频率，而不是从微频率开始到非常高的频率。哦，我明白了，所以使用的最大频率总是奈奎斯特，因此任何超过奈奎斯特的频率都会产生混叠，因此实际上无法解析，因为它是在像素空间中。因此，采样1基本上只是给你一个覆盖整个图像的振荡，这基本上是为了采样非混叠频率的全范围。
- en: 啊 okay， cool。thank you。good。Yeah。Okay， so so after after the image results。we
    wanted to try it on other domains and in particular we were interested in how
    this could be used to work on sort of multimodal domains。so ones combining various
    different types of input features and。one challenge or one sort of problem that
    you encounter in these sorts of spaces is that the data from different modalities
    end up having different features and they always have different semantics so if
    you take the positional encoding plus the RGB for video you end up with some number
    of channels and that if you have audio that corresponds the data may be paired
    but it tends to have fewer features and it only has a1D positional encoding so
    the way that we handle this is basically by learning modality specific position
    encodings and so these are basically embeddings that are special and learned for
    each of the modalities and what this does is basically tags ends up tagging the
    features that come from audio or video with some information that the network
    can learn that allows it to distinguish which ones which but given these these
    padded these sort of learned padded feature vectors we then concatenate them all
    and that's how we process multimobile data so basically the input to the architecture
    still looks like just one big array it's just that when constructing this we know
    that some of those features some of the rows and that。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，好吧，酷。谢谢。很好。是的。好的，接下来在图像结果之后。我们想尝试在其他领域，尤其是我们对如何在多模态领域中应用这项技术感兴趣。一个挑战或在这些领域中遇到的问题是，来自不同模态的数据最终会有不同的特征，且语义总是不同。因此，如果你将位置编码加上视频的RGB，你会得到一定数量的通道，而如果你有对应的音频，数据可能是成对的，但特征往往较少，且只有1D位置编码。我们处理这种情况的方式基本上是学习模态特定的位置编码，这些编码是专门为每种模态学习的嵌入。这基本上为来自音频或视频的特征打上标签，使网络能够学习并区分它们。鉴于这些填充的特征向量，我们随后将它们连接在一起，这就是我们处理多模态数据的方式。因此，架构的输入看起来仍然像一个大数组，只是构建时我们知道其中一些特征和行是这样。
- en: Aray come from video and some come from audio， but the model itself isn't given
    information about that other than what it learns。We also have some great questions，
    so can go first。return turn？Yeah， sorry。 I thought it's。but yeah。yeah， sure。if
    you can hear me it's just a reason。Like starting a lot transformer stuff formally。so
    I just didn't know what position one that was。Oh， so what a position we that？Yes。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Aray来源于视频，有些来自音频，但模型本身并没有关于这些的具体信息，除了它所学习的内容。我们还有一些很好的问题，所以可以先问。返回转向？是的，抱歉。我以为是这样，但没错，当然。如果你能听到我，这只是一个原因。就像正式开始很多变换的东西。我只是不知道那是哪个位置。哦，那我们的位置是什么？是的。
- en: so basically a positional embedding is it's a feature that says this so the
    simplest way to think about it is in text so text the input is 1D so things live
    in some 1D sequence and for each point there you feurize where it's located in
    that sequence so the simplest thing to do would be if you have negative one to
    one is the full range it's just denotes actually where it's located in that sequence
    but we typically will add sort of will want to featureurize this to have more
    dimensions than just a single one and so the Fourier trans the Fourier decomposition
    is one way to do this to sort of give it privileged information about the high
    frequency structure but we can also just use the position to index on some embedding
    array which is how we do it when morere learning things so basically it's just
    a set of weights that are added to the feature for that point that give the network
    information about where it's located in the groundary sequence。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以基本上，位置嵌入是一个特征，表示这个东西，最简单的思考方式是在文本中，所以文本的输入是一维的，事物生活在某个一维序列中，对于每个点，你会表示它在那个序列中的位置。最简单的方法是如果你有负一到一的完整范围，它实际上表示它在那个序列中的位置，但我们通常会想要特征化，使其维度超过单一维度，因此傅里叶变换，傅里叶分解是一种方法，可以提供关于高频结构的特权信息，但我们也可以仅使用位置来索引某个嵌入数组，这就是我们在更多学习中所做的，基本上它只是一个权重集，添加到该点的特征上，给网络提供关于它在基础序列中位置的信息。
- en: If you want to go next？嗯。Sorry I had to find a mute button unmute button Okay
    so I actually have two questions regarding the fewer features the I think like
    do you guys sample them like uniformly or are they like do you like to learn these？
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想继续？嗯，抱歉，我得找个静音按钮。解除静音。好的，我实际上有两个问题，关于特征较少的部分，我想你们是均匀采样还是说你们喜欢学习这些特征？
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_12.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_12.png)'
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_13.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_13.png)'
- en: Yeah， so basically we sample them linearly so basically we take the full space
    and we sample them linearly with whatever the budget is there are so in various
    settings we have actually tried learning these so you could actually initialize
    an array with them and then learn them and that does help sometimes actually and
    you could potentially learn you could try a more sophisticated strategy on this
    too。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，所以基本上我们线性地对它们进行采样，基本上我们取完整空间，并用预算线性采样，在各种设置中，我们实际上尝试学习这些，所以你可以初始化一个数组，然后学习它，有时确实有帮助，你也可以尝试更复杂的策略。
- en: Okay cool my follow up question is that basically I feel like the selling point
    of your research right is that you don't make any structural assumptions right
    you can take any type of like format however for like the encoding wouldn't the
    dimensionality so for example like if it's text it's 1D right if it's a like an
    image it will be 2D and if it's like a video like 3D you have like more。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，酷，我的后续问题是，我觉得你的研究的卖点基本上是你不做任何结构假设，对吧？你可以接受任何格式，但是对于编码来说，维度不会影响吗？例如，如果是文本就是一维，对吧？如果是图像就是二维，而如果是视频就是三维，你会有更多的维度。
- en: Like the positional encoding will have like more points right wouldn't that
    like inherently give away。Like the nature of the input。Yeah so it does so I completely
    agree with this you're totally right the version of this where we have learned
    position encodings is the most pure from that point of view。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 就是位置编码会有更多的点，对吧？这不会固有地透露输入的本质吗？是的，确实如此，我完全同意，你说得对，从这个角度来看，我们学习位置编码的版本是最纯粹的。
- en: so it's one that gives it basically no information about the ground truth spatial
    structure what it does give the model so when you do the learned position encoding
    it will say that for example there's a correspondence between point K on image1
    and point K on image 2 so that's basically the least amount of information you
    can give it while still allowing it to sort of figure out what the structural
    relationship between the input points is so this is the direction that we've been
    trying to push in in general giving the architecture access to sort of ground
    truth structural information like this lives on this point in 2D is helpful so
    there's a couple things here there's sort of from a practical point of view if
    you want good results you need to exploit these things or it's helpful to exploit
    these things but we do want to move in the direction where we're relying on these
    things less。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这基本上给了它关于真实空间结构的信息非常有限。它给模型的是什么呢，当你进行学习的位置编码时，它会说，比如图像1上的点K和图像2上的点K之间存在对应关系。因此，这基本上是你能给它的最少信息，同时仍然允许它理解输入点之间的结构关系。这是我们一直在推动的方向，通常让架构访问这样的真实结构信息，比如这个点在二维上的位置是有帮助的。所以这里有几件事，从实际的角度来看，如果你想要好的结果，你需要利用这些东西，利用这些东西是有帮助的，但我们确实想朝着减少依赖这些东西的方向发展。
- en: And so this is basically something we're actively looking into。Okay makes sense，
    thank you。So I think has posted her question on the chat。I also see you have your
    hand raised， so if you want。you can give it a try if not I'll read out the question。Okay。I'll
    try just let me know if it's choppy。Yeah so is it good right now so far so Oh
    good okay cool so I was curious looking at the perceive diagram you had it's a
    bunch of attention layers right like cross attention and self attention and I
    think there's been this like small trend in recent work in vision transformers
    to try to sort of replace the last few layers instead of having attention like
    make them be convolutions to address this attention scaling problem right in a
    different manner and so here like the perceive architecture is trying to make
    self attention less expensive and they they're just trying to replace it and they
    kind of just avoid the problem。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这基本上是我们正在积极研究的内容。好的，明白了，谢谢。所以我认为她在聊天中发布了她的问题。我也看到你举了手，所以如果你想的话，可以试试，如果不想的话，我会读出问题。好的，我会尝试，告诉我如果有点卡顿。是的，所以现在还不错吗？哦，好，太好了，我很好奇看着你提到的感知图，它是很多注意力层，对吧，比如交叉注意力和自注意力。我觉得最近的视觉变压器工作中有一种小趋势，试图用卷积替代最后几个层，而不是使用注意力，以不同的方式解决这个注意力扩展问题。这里的感知架构正试图让自注意力成本更低，他们只是想替代它，并且避免了这个问题。
- en: And so I'm curious and so I've seen papers both ways like some that try to do
    things like the ones you cited and then some that are trying to do this as well
    and in my mind everyone always has like the good results and stuff so I'm curious
    if you think there's a reason to do one or the other or if you think this alternative
    approach is also promising or is there a reason you know research should go in
    one direction or the other。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我很好奇，我看到有些论文是两种方式的，有些试图做你提到的那样，还有一些也在尝试这样做。在我看来，每个人总是有好的结果等等。所以我好奇你是否认为有理由去做其中一种，还是你觉得这种替代方法也很有前景，或者有没有理由研究应该朝着某个方向发展。
- en: Yeah， so to my mind， the big tradeoff is one between。so the vision literature
    I think it's just exploded in terms of these sort of hybrids and people the write
    the exact right place on the Pato curve for the tradeoff of speed and performance。but
    they're basically looking primarily on vision specific problems。so something the
    computer vision community itself typically doesn't regularize itself away from
    things that don't work on things that aren't vision。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，在我看来，主要的权衡是一个……视觉文献我认为在这些混合体方面已经爆炸性增长，人们写在Pato曲线上速度和性能的最佳平衡点。但是他们基本上主要关注视觉特定的问题。因此，计算机视觉社区通常并不会自我规范，以避免那些不适用于视觉的问题。
- en: So you end up with things that are very， very efficient and very performant
    on vision problems so I think from that point of view it's an incredibly important
    line of work and that's probably the right way of doing things what we're sort
    of aiming for is something is the things that are as general as possible while
    still kind of being performant。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你最终会得到在视觉问题上非常高效和表现出色的东西。我认为从这个角度来看，这是一个极其重要的研究方向，这可能是正确的做法。我们所追求的，是尽可能通用，同时仍然保持高效。
- en: So got it so this kind of thing is critical Oh sorry you off go ahead no no
    please go ahead I was going to say so this kind of thing is like it's important
    just to summary so you feel like it's important to focus on attention because
    that's kind of critical for NLP like you can't just sort of put in a convolution
    at the end and sort of fix the problem but envision maybe you can and it's fine
    is that a right way of understanding it。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 明白了，所以这种事情是关键的。哦，抱歉，你继续。没有，没有，请继续。我想说这种事情很重要，简而言之，你觉得关注注意力很重要，因为这对NLP至关重要；你不能仅仅在最后放入一个卷积来解决问题，但或许可以，这样理解对吗？
- en: That's part of it vision and NLP aren't the only two domains。and so the thing
    that we're for are really basically so the kinds of problems that we're interested
    in doing with this include things like event- based cameras。cell biology， sort
    of proteins， all of these sorts of things where we may or may not have the right
    convolutional inductive biases for to even know how to build those sorts of things
    or they end up being whole research programs like the mesh based convolution work。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这其中的一部分是，视觉和NLP并不是唯一两个领域。所以我们真正关注的，是我们想用这个解决的问题，包括事件驱动相机、细胞生物学、蛋白质等这些方面，我们可能没有正确的卷积归纳偏置来知道如何构建这些东西，或者它们最终成为整个研究项目，比如基于网格的卷积工作。
- en: Oh Co， thank you。I also had like one more question about the architecture。So
    I saw that。I'm sorry if you said this and I just missed it， but you had cross
    attention and then like that to transformer and then cost attention。I'm curious
    what happens if you replace the self attention in those layers with cross attention。Does
    it affect your accuracy is that even feasible is that a valid question。Yeah。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 哦，Co，谢谢你。我还有一个关于架构的问题。所以我看到你提到的。我很抱歉如果你说过这个而我没听到，但你提到的交叉注意力，然后是变换器，再到代价注意力。我很好奇，如果你把这些层中的自注意力替换为交叉注意力，会发生什么？这会影响你的准确性吗？这甚至可行吗？这是一个有效的问题吗？是的。
- en: so the sort of thing that you could do is you could modify this to make it sort
    of hierarchical so that there are multiple stages across attention we haven't
    gotten this working yet。but it doesn't mean it's not a good idea。So there there
    might be there might be a right way to do this that we haven't figured out right。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以做的事情是修改它，使其具有某种层次结构，这样就会有多个阶段的交叉注意力。我们还没有让这个工作，但这并不意味着这不是一个好主意。所以，可能有一种正确的方法来做到这一点，而我们还没有找出正确的方法。
- en: but it's something we have tried a little bit Oh cool okay， thank you so much
    I appreciate it。Yeah。problem。Okay， let me。We're running short on time， so maybe
    I'll skip ahead。嗯。Okay so。Before we run too much time， I want to at least talk
    about the sort of the modifications to this architecture that we've made to make
    it work sort of even more generally。so one of the problems of the sort of the
    first the first architecture that we looked at here the basic receiver is that
    it works basically for arbitrary inputs but it's designed to work only on classification
    and regression tasks as an output and so basically we wanted to see if we could
    use the the same cross attention strategy for decoding and it turns out you can
    it's something that works pretty well just kind of out of the box so the idea
    is that we have if we have our cross attentiontion input and self-attention sort
    of to do the processing we can introduce a set of additional queries and these
    are basically queries that give the semantics of each of the points that you're
    trying to decode。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们试了一点点，哦，酷，谢谢你，我很感激。是的，没问题。好吧，让我。我们时间不多了，也许我可以跳过一些内容。嗯。好吧。所以，在我们花太多时间之前，我想至少谈谈我们为使这个架构更通用而做的修改。因此，我们在这里看到的第一个架构的问题是，它基本上适用于任意输入，但仅设计用于分类和回归任务输出，因此我们希望看看是否可以使用相同的交叉注意力策略进行解码，结果证明是可以的，这种方法相当有效，几乎可以直接使用。我们的想法是，如果我们有交叉注意力输入和自注意力来进行处理，我们可以引入一组额外的查询，这些查询基本上为每个你试图解码的点提供语义。
- en: And we pass these input to another cross attention layer which is configured
    in basically the opposite way that the encoder cross attention is configured。so
    now the queries are going to be something that's potentially large and the keys
    and values are coming from this latent and so what this allows us to do basically
    is to keep all of the nice advantages of the original perceive so we have an encoder
    that scales linearly we have a processor stage。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些输入传递给另一个交叉注意力层，该层的配置基本上与编码器交叉注意力的配置相反。所以现在查询将是潜在的大，而键和值则来自这个潜在空间，这基本上让我们保留了原始感知的所有优点，因此我们有一个线性扩展的编码器和一个处理阶段。
- en: this sort of latent self-atten that scales independently of the input size and
    we now have a decoder that keeps the decoupling but gives this linear scaling
    with respect to output size and so by doing this we can now basically apply the
    same approach to basically dense output tasks。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这种潜在自注意力独立于输入大小扩展，我们现在有一个解码器，保持解耦，但与输出大小呈线性扩展，因此通过这样做，我们基本上可以将相同的方法应用于密集输出任务。
- en: And so to give you a sense of how this works， just sort of intuitively。If we're
    doing auto encoding on this image of puppies。basically what we do is we encode
    process and then to decode。we take a query that corresponds to each of the points
    and then we pass it into this decoder so we can query one of the points。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让你更直观地理解它的工作方式。如果我们对这张小狗的图像进行自编码，基本上我们的做法是编码处理，然后解码。我们取一个对应于每个点的查询，然后将其传递给这个解码器，这样我们就可以查询其中一个点。
- en: we get one pixel query another one， we get another one and all the way up until
    we get all 10000 points and that's how we can do reconstruction with this。And
    the cool thing about this is that it opens up a bunch of new applications and
    we can get different kinds of outputs just by changing how the queries work。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得一个像素查询，再来一个，直到获得所有10000个点，这就是我们如何进行重建的。而这件事的酷点在于，它开启了一些新应用，我们可以通过改变查询的方式得到不同种类的输出。
- en: So if we want to do something like multimodal auto encoding where we have some
    of the outputs or videos。we use the same construction trick to get positions that
    to get queries that have the relevant semantics for each of the points that we're
    decoding and we can do this even though basically the sizes of these different
    data so the number of points they have is quite diverse so in the multimodal auto
    encoding experiments that we have in this paper we do this for video audio and
    labels at the same time。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果我们想做一些多模态自编码，比如我们有一些输出是视频，我们使用相同的构造技巧来获取位置，得到具有相关语义的查询，尽管这些不同数据的大小，即点的数量差异很大，在这篇论文的多模态自编码实验中，我们同时进行了视频、音频和标签的处理。
- en: so that all of them are just passed into their uniform network and then decoded
    one by one in this way。But we can also do mass language modeling now by conditioning
    on the position in a sequence。we can do multitask classification by having basically
    an index that gives which task you're querying from the network。And we can do
    things like optical flow by passing in input features as well as the positions。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的输入都被传递到统一网络中，然后逐个解码。但我们现在也可以通过对序列中的位置进行条件处理来进行大规模语言建模。我们可以通过基本上提供一个索引来进行多任务分类，指示你正在从网络查询哪个任务。我们还可以通过传递输入特征以及位置来处理光流。
- en: And so I'm just going to just skip to a couple of the different。I can share
    these slides with you all afterwards to look through them。Some of these things
    are quite cool but。Just quickly。I want to talk about language and then optical
    flow。So for language basically what we wanted to do with this was to see if we
    could use this to replace tokenization and why might we care about getting rid
    of tokenization so one we use tokenization primarily because transformers scale
    poorly with sequence length and tokenizing cut sequence length by that a factor
    of four。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我将跳过几个不同的部分。我可以在之后将这些幻灯片分享给大家供大家查看。这些内容中有些相当酷，但我想快速谈谈语言和光流。因此，关于语言，我们想做的是看看是否可以用它来替代标记化，以及为什么我们关心去掉标记化。首先，我们主要使用标记化是因为变换器在序列长度上扩展较差，而标记化将序列长度减少了四倍。
- en: But but there are various problems that arise with this and so might we care
    about removing tokenizers？
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，出现了各种各样的问题，那么我们为什么要关心去掉标记器呢？
- en: So for one。Tokenizers perform less well on rare words。so if you compare the
    sort of the bytebased decomposition。theTF based UTF8 encoding input of an input
    sequence like this。you can see that there's basically a uniform allocation of
    points in memory to each of the input characters。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，标记器在处理稀有单词时表现不佳。所以如果你比较基于字节的分解和基于TF的UTF8编码输入序列，你会发现基本上每个输入字符在内存中都有均匀的点分配。
- en: the exception or diacritics which end up splitting it into two but if you look
    at the sentence piece tokenization so it's learned that pepper is one token but
    jalapeno gets split into five in this case。so this basically says the amount of
    capacity that you allocate depends on how rare the word is which can lead to suboptimal
    encodings。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例外或变音符号会导致它被拆分为两个，但如果你查看句子片段的标记化，就会发现“pepper”是一个标记，而“jalapeno”在这种情况下被拆分为五个。这基本上表明你分配的容量取决于单词的稀有程度，这可能导致次优编码。
- en: They're also bridled to subtle perturbations， a famous example of this is that
    if you enter so if you've ever played around a GPT3。you'll notice that the output
    can be quite sensitive to where if you add a space or emit a space at the end。and
    this basically is because the space can end up being factorized into different
    parts of the tokenization。problem there are other things that can happen there
    too but this is one cause of that。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 它们还容易受到细微扰动的影响，一个著名的例子是，如果你玩过GPT-3，你会注意到输出对在末尾添加或省略空格非常敏感。这基本上是因为空格可能会被分解成标记化的不同部分。这儿还有其他问题，但这是其中一个原因。
- en: And finally tokens don't transfer across across languages。so if you wanted to
    have a model that without any tuning could be used on many different languages
    at the same time。tokenizers are a blocker for this， so if we can get rid of them
    it'll simplify the pipeline it'll also make things less brittle and then hopefully
    lead to more general models。So the way that we do mass language modeling is the
    same as the way that I showed in that schematic autoencoding experiment。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，标记在不同语言间不转移。因此，如果你想拥有一个可以在许多不同语言上使用而不需调优的模型，标记器就是一个障碍。所以如果我们能去掉它们，将简化管道，还会使事情变得更不脆弱，进而希望能导致更通用的模型。我们进行大规模语言建模的方式与我在那个示意图自动编码实验中展示的方式相同。
- en: so we mask some fraction of our inputs about 15% is sort of the standard magic
    number。we then decode at each of the positions that are masked and we task the
    model with decoding whatever characters we masked at those locations。And then
    once we have this model， so this is what we do for pretraining。we can then fine
    tune it by replacing the decoder with a multitask decoder that takes in the tasks
    that we're using on the downstream evaluation setting and training the model to
    reconstruct the logits on a per task basis。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们掩盖了约15%的输入，这大致是标准的魔法数字。然后我们在被掩盖的位置进行解码，并要求模型解码在这些位置掩盖的字符。一旦我们有了这个模型，这就是我们进行预训练所做的。然后我们可以通过将解码器替换为多任务解码器来微调它，后者接受我们在下游评估设置中使用的任务，并训练模型按任务基础重构logits。
- en: Okay， so to look at how this model performs， we basically first compare it to
    Bt basease so this is just a solid benchmark that we understand very well and
    first by looking at sort of matched two models that have matched flops we can
    see that perceive IO and Bt basease work on par you see there's a different tradeoff
    here so to get the same number of flops basically we make perceive IO deeper and
    this ends up giving it more parameters。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，为了查看这个模型的表现，我们基本上首先将其与Bt basease进行比较，这只是一个我们非常了解的可靠基准。首先通过查看两个模型匹配的flops，我们可以看到perceive
    IO和Bt basease的表现相当，你会看到这里存在不同的权衡，因此为了获得相同数量的flops，基本上我们让perceive IO更深，这最终导致了更多的参数。
- en: but on a perflps basis it ends up performing about the same。On the other hand。if
    we remove the tokenizer from Bt and keep the flops the same。we see that the number
    of parameters in the depth just drastically fall down and this is because Bert
    scales quite poorly with sequence length because it uses a normal transformer。But
    if we use a perceive without the tokenization， we can see that we only get a slight
    reduction in the number of parameters at the flops count。
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 但在每flop基础上，它的表现基本上相同。另一方面，如果我们从Bt中去除tokenizer并保持flops不变，我们会看到参数的数量和深度急剧下降，这是因为Bert在序列长度上扩展得相当糟糕，因为它使用了普通的transformer。但如果我们使用没有tokenization的perceive，我们可以看到在flops计数上参数的减少只是轻微的。
- en: but the performance performs almost exactly the same so this means that the
    perceive in the setting is performing basically the same with and without the
    tokenization it's learning a different strategy it's using different parameters
    but it basically can be brought to the same performance。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但性能几乎完全相同，这意味着在这种设置中perceive的表现与有无tokenization基本上相同，它学习了不同的策略，使用了不同的参数，但它基本上可以达到相同的性能。
- en: We can then scale this more by sort of leaning into what happens in the tokenizer
    free setting and we see that we can get a moderate performance boost as well。I
    think it's also in the language setting it's also useful to look at what the attention。the
    attention maps that are sort of learned and what's being visualized here are basically
    for each of the latents for some subset of the lates。we're looking at where it's
    attending to in the input sequence and some of these end up being local so point
    looking at specific points in the sentence。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以通过深入了解tokenizer-free设置中发生的事情来进一步扩展这个模型，我们看到我们也可以获得适度的性能提升。我认为在语言设置中，查看注意力图也是有用的，这里可视化的内容基本上是针对某些潜在特征，我们在输入序列中查看它关注的地方，其中一些最终是局部的，指向句子中的特定点。
- en: Some of them are periodic， so they look sort of at recurring points over the
    sequence and some of them also look like they pick out syntactic features。which
    is quite nice， so they pick out basically exclamation points or capital letters
    or other punctuation that's quite useful and decodable from very right right at
    the beginning of the sequence。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些是周期性的，因此它们看起来会在序列的重复点上，而其中一些看起来也像是提取了句法特征，这非常不错，因此它们基本上提取感叹号、大写字母或其他从序列开始就很有用且可解码的标点符号。
- en: We can also basically use this exact same architecture on optical flow and optical
    flow is basically an important classical problem in computer vision where given
    a pair of frames in a video we want to basically track all of the points so figure
    out the motion from every point from one frame to the other and so optical flow
    is usually visualize using these sort of colorized images that are shown in the
    bottom and what this gives you basically is a per pixel indication of the velocity
    at every single point。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也基本可以在光流上使用完全相同的架构，而光流基本上是计算机视觉中的一个重要经典问题，给定视频中的一对帧，我们基本上想要跟踪所有的点，以便从一帧到另一帧计算每个点的运动，因此光流通常使用这些底部显示的彩色图像进行可视化，这基本上给你提供了每个点的速度的逐像素指示。
- en: And so you can see that so the blade that the character here is holding is moving
    to the right。whereas this creature behind her is sort of as moving downwards。So
    there are a couple of problems with optical flow that make it interesting to to
    sort of approach。so one it's a dense task and it basically involves long range
    correspondences but the standard training protocol there's basically no largescale
    realistic training data just because it's incredibly hard to sort of label all
    of the pixels in a real world scene and figure out where they go to so the typical
    way to do this is to train on some synthetic data and then evaluate on more realistic
    scenes and optical flow is also interesting because it's basically the locust
    of some of the most complicated visual architectures in the literature so the
    previous say of the art result here is this method called raft which won the best
    paper reward at DCCCVE last year and I'm just highlighting this to give you a
    sense of how much work people do into sort of hand engineering these architectures
    so this is a very very cleverly designed architecture and basically it incorporates
    things like global correlation volumes that are explicitly computed。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，角色手中的刀刃向右移动，而她身后的生物则是向下移动。因此，光流存在几个有趣的挑战，所以它是一个密集任务，基本上涉及长距离的对应关系，但标准的训练协议中基本上没有大规模的真实训练数据，因为标记真实场景中的所有像素并找出它们的去向是非常困难的，因此通常的做法是先在一些合成数据上进行训练，然后在更真实的场景中进行评估。此外，光流也很有趣，因为它基本上是文献中一些最复杂的视觉架构的核心，所以之前的最先进结果是一个叫做RAFT的方法，它在去年DCCCVE上获得了最佳论文奖，我强调这一点是为了让你了解人们在手动设计这些架构上付出了多少努力，因此这是一个非常巧妙设计的架构，基本上包含了像显式计算的全局关联体积这样的东西。
- en: Different offsets to basically allow the model to reason about how things at
    different scales are moving with respect to each other。It also has local neighborhood
    gather operations as well as update blocks to keep track of what's happening within
    each specific correlation block。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的偏移量基本上允许模型推理不同尺度的事物是如何相互移动的。它还具有局部邻域聚合操作以及更新模块，以跟踪每个特定关联模块内发生的情况。
- en: and then finally there's a flow specific upsampling operators that were developed。So
    in contrast to this， we're basically we wanted to see how well perceive IO would
    do here and just to give you a sense of sort of what we were expecting coming
    into this。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，开发了一些特定于流的上采样操作。因此，与此相对，我们基本上想看看PerceivedIO在这里的表现如何，只是为了让你了解我们对这一点的预期。
- en: we thought well， maybe so perceivedIO is throwing a lot of the structure away
    so we were hoping that we would get some good results but it would probably overfit
    and there's this sort of problem of the domain transfer that's happening here
    but on the other hand self-attention seems to be a reasonable way to match this
    sort of correspondence thing。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想，也许PerceivedIO抛弃了很多结构，所以我们希望能够得到一些好的结果，但它可能会过拟合，并且这里存在领域转移的问题，但另一方面，自注意力似乎是匹配这种对应关系的合理方法。
- en: What we actually found was that just by doing the very， very simple preprocessing
    here。so extracting basically a patch around each pixel and then using the standard
    perceive IO architecture。we were able to get state of the art results here and
    so this is basically was validation of this general approach of trying to have
    general purpose architectures that would transfer over。And so basically with minimal
    tuning， we are able to get results that are both of the sort of compelling benchmarks
    on both of the the Cel evaluation methods and to get comparable results on Kity。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上发现，仅仅通过这里非常、非常简单的预处理，即提取每个像素周围的一个补丁，然后使用标准的感知IO架构，我们能够在这里获得最新的结果，因此这基本上验证了这种尝试采用通用架构以便进行迁移的总体方法。因此，基本上在最小调整下，我们能够在这两种Cel评估方法上获得令人信服的基准结果，并在Kity上获得可比结果。
- en: so these are the standard ones。And we can also sort of visualize what happens
    when we apply this on real world data。so there's no ground truth here so we can't
    really compare it。but it's still useful to sort of see how it moves around and
    we can see that qualitatively it's able to capture a lot of the fine structure
    and to sort of get the right motion for the things that are very clearly moving
    in the specific direction。We can also sort of， it's also I think informative to
    look at what happens how it manage to represent how it manages to represent sort
    of small structure is this video playing。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是标准指标。我们还可以可视化在真实世界数据上应用此方法时发生的情况。所以这里没有基准，我们不能真正进行比较，但仍然有助于看到它是如何移动的，从定性上看，它能够捕捉到很多细微的结构，并能正确表示那些明显在特定方向移动的事物。我们也可以看看它是如何成功表示小结构的，这段视频正在播放。
- en: Yeah can。Okay cool， so the thing to look at here is the fine water droplets
    that are sort of flying through the air as that bird flies by and because we're
    decoding at every single output point。the architecture is able to represent those
    so it's able to capture very。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 可以的。好的，很酷，这里需要关注的是那些在鸟飞过时飞翔的细小水滴，因为我们在每个输出点进行解码。架构能够表示这些，因此能够捕捉到非常细微的变化。
- en: very fine sale segmentation that would be difficult to capture if you had for
    example。a convolutional upsamplar here。Okay， so I'm just going to the light light
    gone off in this room。嗯。I'm also interested that you also try other task like
    depth estimation。because especially where looks like it can still also work well
    on that modalities。Yeah。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 非常细致的分割，如果你在这里使用卷积上采样器，就很难捕捉到这些。好的，所以我只是想提到这个房间里的灯光熄灭了。嗯。我也很感兴趣你是否尝试过其他任务，比如深度估计，特别是看起来它在那种模式下也能很好地工作。是的。
- en: so we haven't published anything， but some internal results suggest that that
    it works just fine。re basically there don't seem to be one of the surprising things。the
    things that we were a little bit unsure about was how much information was going
    to be contained in this latent because basically you're abstracting quite a lot
    and it doesn't have any 2D structure intrinsically but it does seem like it seems
    to be able to represent things quite well and these sorts of decoding mechanisms
    do seem to be able to do that。
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们还没有发布任何内容，但一些内部结果表明这确实有效。基本上似乎没有什么令人惊讶的事情。我们有点不确定的是这个潜在空间中将包含多少信息，因为基本上你在抽象很多，并且它本身并没有任何2D结构，但似乎它能够很好地表示事物，这些解码机制似乎也能做到这一点。
- en: かるで。So I'm just going to interest in this interest of timem I'm going to skip
    ahead to the conclusion Dr I had one question with respect to the metrics that
    you've shared for the optical flow。the numbers so like the in the table it was
    like central final clean entity where these different data sets or different metrics
    like same metric for different dataset sets or like these are three different
    metrics。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当然。所以我现在出于时间的考虑，跳到结论。我有一个关于你分享的光流指标的问题，表格中的数字像是中心最终清洁实体，这些是不同的数据集，还是说这些是针对不同数据集的相同指标，或者是这三个不同的指标？
- en: Yeah so these are three different data sets， so Cintelcle and Stel final are
    basically two theyre two ways of doing the final rendering for Cintel in all cases
    these methods are trained just on the autoflow data set so they're trained on
    this sort of general purpose kind of wacky synthetic motion data set and then
    we're we're evaluating on these different domains without fine tuning。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，这些是三种不同的数据集，所以Cintelcle和Stel final基本上是Cintel的最终渲染的两种方式，在所有情况下，这些方法仅在光流数据集上进行训练，因此它们是在这种一般用途的奇特合成运动数据集上进行训练的，然后我们在这些不同领域上进行评估，而没有进行微调。
- en: Okay。Yeah， the flow has quite the data sets are quite small。so it's generally
    even problematic to fine tune。Thank you。嗯。Okay， so just to summarize， right。what
    was the ground roof to find the endpoint error。Yeah。so the way this works is Cintel
    is a computer， it's basically a relatively high quality CGI movie that was basically
    open source and so they actually have the ground truth so if you know the ground
    truth 3D state you can compute the pixel correspondence from frame to frame so
    that's what's used on Cintel and then Kity they basically have they have like
    a LiDAR sensor that's used to figure out the depth of all points and then they
    compute the correspondences so the ground truth is actually the ground truth optical
    flow but in general it's hard to get dense optical flow it's very expensive to
    collect it。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。是的，流量的数据集相当小，因此一般来说甚至在微调时也会有问题。谢谢。嗯。好的，简单总结一下，没错。那什么是找到端点误差的真实情况呢？是的。这种方式是Cintel，这是一个计算机生成图像，基本上是一个相对高质量的CGI电影，基本上是开源的，因此他们实际上有真实情况的依据，如果你知道真实的3D状态，就可以计算每帧之间的像素对应关系，这就是在Cintel上使用的，然后Kity基本上有一个LiDAR传感器，用于确定所有点的深度，然后计算对应关系，所以真实情况实际上是光流的真实情况，但一般来说，获取密集光流是很困难的，收集成本非常高。
- en: Great， thanks。嗯。😊，Okay， so so basically just to summarize so the perceives are
    tension-based architectures that scale linearly and work as drop in replacements
    for transformers on a variety of settings。they also seem to be able to achieve
    results that are comparable at least in performance with models that rely on 2D
    convolutions but of course there is a trade off here and so it's good to be very
    aware of this of generality versus speed and specific domains and so as was pointed
    out in settings where you can use 2D convolutions。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，谢谢。嗯。😊，好的，基本上总结一下，感知器是基于张力的架构，能够线性扩展，并在多种设置中作为变压器的替代品。它们似乎也能够在性能上与依赖于2D卷积的模型相媲美，但当然这里有权衡，所以要非常注意一般性与速度以及特定领域之间的关系，正如在可以使用2D卷积的设置中所指出的那样。
- en: it's certainly good to have them in the loop。It's basically a unified architecture
    that allows joint modeling of different modalities of different sizes and basically
    overall it seems to be quite flexible architecture that's able to produce a state
    of the art or near state of the art results on a variety of different domains
    ands in the two papers we look at a number of other domains that I didn't talk
    about including 3D point cloud modeling replacing the transformer that's used
    in the starcraft and the sort of starcraft behavioral cloning agent in a couple
    of others so this does we have a lot of evidence that this general approach seems
    to work broadly and there's a lot of things we still haven't tried so we're very
    interested in pushing this and always open for suggestions and so forth。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 将它们纳入循环中当然是有益的。这基本上是一个统一的架构，允许对不同大小的不同模态进行联合建模，整体来看，它似乎是一个相当灵活的架构，能够在各种不同领域产生最先进或接近最先进的结果。在这两篇论文中，我们研究了许多我没有提到的其他领域，包括3D点云建模，替代在《星际争霸》中使用的变压器，以及其他一些行为克隆代理，因此我们有很多证据表明这种通用方法似乎在广泛应用中有效，并且还有很多事情我们尚未尝试，所以我们非常希望推动这一点，并且始终欢迎建议等。
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_15.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_15.png)'
- en: So we're relying on a large body of related work because we're sort of drawing
    from a lot of different areas here。so here are some highlights and then I just
    want to thank my co-authors on this work。And of course。I'm happy to talk more。Thanks，Yeah，
    thanks a lot。 Thanks very。So one question I had is like。so what do you think is
    like the future of like like perceived like models like do you think like this
    are going to like being used more in like the sort of like the transform transformer
    community to replace like con and another stuff？
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们依赖于大量相关工作的基础，因为我们从许多不同的领域汲取灵感。这些是一些亮点，我只想感谢我的合著者们。当然，我乐意进一步讨论。谢谢，是的，非常感谢。我有一个问题：你认为像感知模型的未来会是什么样的？你觉得这些模型会在变换器社区中被更多使用，以替代卷积网络等其他东西吗？
- en: Yeah， so the I think。Broadly speaking， I think of perceives now is sort of because
    we know how to adapt them pretty well to sort of domains where we don't have a
    great idea of the right way to structure an architecture。an inductive bias， so
    I think that's one of the really strong cases for it。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我认为，从广义上讲，我认为感知模型现在是一种选择，因为我们知道如何很好地将它们适应于我们对如何构建架构没有很清晰想法的领域。这种归纳偏差，我认为这是一个非常强有力的案例。
- en: so settings in which you don't really know what the right way to structure a
    problem is。I also think these kinds of approaches can be used in conjunction with
    cons for sort of things that are as domain agnostic has needed。But I think multimodal
    and and new domains is really to that's the real that's where this these are obvious
    choices got it also like what do you think are the current bottlenecks with this
    and if you don't mind like if you can disclose like border is what are you working
    on towards next with this stuff。
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 所以在你并不确切知道如何构建问题的情况下，我也认为这些方法可以与需要领域无关的内容结合使用。但我认为多模态和新领域确实是显而易见的选择。你觉得目前在这方面有哪些瓶颈？如果你不介意的话，能否透露一下你在这方面的下一步工作是什么？
- en: So I can't talk about too many details about that， but the a couple of domains
    so one。we don't really have a great handle on how to use them on sort of small
    small scale data so data where where。You don don't have the data to sort of recover
    the inductive bias so this is I think a really important area。the other thing
    that we haven't sort of talked about here but you could probably imagine that
    we'll be thinking about would be how to train on multiple modalities or sort of
    multiple things at the same time so right now all of these architectures are sort
    of trained in isolation but there are a lot of opportunities for sort of figuring
    out how to pose problems together and use a single architecture on all of them。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我不能谈论太多细节，但有几个领域，首先，我们对如何在小规模数据上使用它们并没有很好的把握，所以数据的数量不足以恢复归纳偏差，我认为这是一个非常重要的领域。另一方面，我们还没有讨论的事情，但你可能会想象我们会考虑的是如何同时在多种模式或多种事物上进行训练，因此现在所有这些架构都是孤立训练的，但有很多机会可以一起解决问题并在所有问题上使用单一架构。
- en: Got it， what I'm sure if you tried， but can you also use this for like tableular
    data stuff？Yeah。so effectively the architecture treats any input data as tabular
    data。so I think that's exactly the right way to think about it。course sounds good
    thanks to the top。I will open make general questions for the students。So it's
    jump recording。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 明白了，我不确定你是否尝试过，但你能否将其用于表格数据？是的，因此该架构将任何输入数据视为表格数据，所以我认为这正是思考的正确方式。课程听起来不错，谢谢你的提醒。我会为学生们提出一般性的问题。所以我们开始录音。
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_17.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_17.png)'
