- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P6ï¼š6.DeepMind's Perceiver and Perceiver
    IOnew data family architecture - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPT/Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P6ï¼š6.DeepMindçš„Perceiverå’ŒPerceiver IOæ–°æ•°æ®å®¶æ—æ¶æ„
    - life_code - BV1X84y1Q7wV
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_0.png)'
- en: So today I'm going to be talking about some recent work that we've been doing
    at Deepmind developing this line of architectures that we're calling perceives
    and I'll be motivating this in terms of a goal that we have which is to develop
    a general purpose architectures and so just right off the bat I want to motivate
    why we care about general purpose architectures and so the first the first both
    of the reasons are fairly pragmaticã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»Šå¤©æˆ‘å°†è®¨è®ºæˆ‘ä»¬åœ¨DeepMindè¿›è¡Œçš„ä¸€äº›æœ€æ–°å·¥ä½œï¼Œå¼€å‘æˆ‘ä»¬ç§°ä¹‹ä¸ºâ€œæ„ŸçŸ¥â€çš„è¿™ä¸€ç³»åˆ—æ¶æ„ã€‚æˆ‘å°†ä»¥æˆ‘ä»¬å‘å±•çš„é€šç”¨æ¶æ„ç›®æ ‡ä¸ºåŠ¨æœºï¼Œæ‰€ä»¥æˆ‘æƒ³ç›´æ¥é˜æ˜æˆ‘ä»¬ä¸ºä»€ä¹ˆå…³å¿ƒé€šç”¨æ¶æ„ï¼Œç¬¬ä¸€ä¸¤ä¸ªç†ç”±éƒ½æ˜¯ç›¸å½“åŠ¡å®çš„ã€‚
- en: but basically the idea is if we're thinking about all of the data that we could
    possibly imagine collecting in the worldã€‚a lot of it is basically involves what
    we think of as sort of traditional sense modalitiesã€‚whether these things range
    from touch andropriception to echoloc to the kind of perception you need to ingest
    text however you want to format that to things to more exotic things like event-based
    camerasã€‚![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_2.png)
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åŸºæœ¬ä¸Šè¿™ä¸ªæƒ³æ³•æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬è€ƒè™‘åˆ°åœ¨ä¸–ç•Œä¸Šå¯èƒ½æ”¶é›†çš„æ‰€æœ‰æ•°æ®ï¼Œå…¶ä¸­å¾ˆå¤šåŸºæœ¬ä¸Šæ¶‰åŠæˆ‘ä»¬è®¤ä¸ºçš„ä¼ ç»Ÿæ„Ÿå®˜æ–¹å¼ã€‚è¿™äº›ä¸œè¥¿å¯èƒ½åŒ…æ‹¬ä»è§¦è§‰ã€å‰åº­æ„ŸçŸ¥åˆ°å›å£°å®šä½ï¼Œä»¥åŠä½ éœ€è¦ä»¥ä»»ä½•æƒ³è¦çš„æ ¼å¼æ‘„å–æ–‡æœ¬çš„é‚£ç§æ„ŸçŸ¥ï¼Œç”šè‡³è¿˜æœ‰æ›´å¥‡ç‰¹çš„äº‹ç‰©ï¼Œæ¯”å¦‚åŸºäºäº‹ä»¶çš„æ‘„åƒå¤´ã€‚![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_2.png)
- en: Whiskering touching with wither sensesï¼Œ things like smell and depth and all
    the way up to the kinds of sense modalities that we really think about when we're
    thinking about scientific perception and so basically as if we think about the
    full set of data and what it would take to actually model each of these different
    modalities it basically looks effectively intractable to try to engineer inductive
    biases that will work for every single one of these so we don't want to engineer
    them one by one this is an approach that's worked and in some ways that it's maybe
    a reasonable description of how we think about developing new architectures for
    different problems but it's just not going to scale we can't afford as a community
    to design to hand designign inductive biases that will work for each and every
    one of these and so rather than doing that we want to sort of build architectures
    that at least at first pass will allow us to handle everythingã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: è§¦è§‰ä¸å…¶ä»–æ„Ÿå®˜çš„ç»“åˆï¼ŒåŒ…æ‹¬å—…è§‰ã€æ·±åº¦æ„ŸçŸ¥ï¼Œä»¥åŠæˆ‘ä»¬åœ¨è€ƒè™‘ç§‘å­¦æ„ŸçŸ¥æ—¶çœŸæ­£å…³æ³¨çš„æ„Ÿå®˜æ¨¡å¼ã€‚åŸºæœ¬ä¸Šï¼Œå¦‚æœæˆ‘ä»¬è€ƒè™‘åˆ°æ‰€æœ‰æ•°æ®çš„å®Œæ•´é›†åˆï¼Œä»¥åŠå®é™…ä¸Šå¯¹æ¯ç§ä¸åŒçš„æ„Ÿå®˜æ¨¡å¼è¿›è¡Œå»ºæ¨¡æ‰€éœ€çš„ä¸œè¥¿ï¼Œå°è¯•ä¸ºæ¯ä¸€ä¸ªæ¨¡å¼å·¥ç¨‹å½’çº³åè§å‡ ä¹æ˜¯ä¸åˆ‡å®é™…çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä¸æƒ³ä¸€ä¸ªä¸ªåœ°è¿›è¡Œå·¥ç¨‹è®¾è®¡ï¼Œè¿™ç§æ–¹æ³•åœ¨æŸç§ç¨‹åº¦ä¸Šç¡®å®æœ‰æ•ˆï¼Œä¹Ÿè®¸åˆç†åœ°æè¿°äº†æˆ‘ä»¬å¦‚ä½•è€ƒè™‘ä¸ºä¸åŒé—®é¢˜å¼€å‘æ–°æ¶æ„ï¼Œä½†è¿™æ ¹æœ¬æ— æ³•æ‰©å±•ã€‚ä½œä¸ºä¸€ä¸ªç¤¾åŒºï¼Œæˆ‘ä»¬æ— æ³•æ‰¿æ‹…ä¸ºæ¯ä¸€ä¸ªæ¨¡å¼æ‰‹åŠ¨è®¾è®¡å½’çº³åè§çš„æˆæœ¬ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›æ„å»ºè‡³å°‘åœ¨åˆæ­¥ä¸Šèƒ½å¤„ç†æ‰€æœ‰äº‹ç‰©çš„æ¶æ„ã€‚
- en: There's another practical argument for why we should work on general purpose
    architectures and that's because it will allow us to build simpler more unified
    systems so if you look at how in particular complex multimodal data streams are
    typically approached in the sort of the sensory computer vision or pattern recognition
    literatures effectively the typical way this is done is by using inductive biases
    that we know hold for the individual modalities and then engineer ways of combining
    those different subsystems so this can mean building specific specific head specific
    input modules for each of these things and then trying out the various different
    ways of combining themã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰ä¸€ä¸ªå…³äºä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥ç ”ç©¶é€šç”¨æ¶æ„çš„å®ç”¨è®ºæ®ï¼Œé‚£å°±æ˜¯è¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ„å»ºæ›´ç®€å•ã€æ›´ç»Ÿä¸€çš„ç³»ç»Ÿã€‚å¦‚æœä½ æŸ¥çœ‹å¤æ‚çš„å¤šæ¨¡æ€æ•°æ®æµåœ¨æ„ŸçŸ¥è®¡ç®—æœºè§†è§‰æˆ–æ¨¡å¼è¯†åˆ«æ–‡çŒ®ä¸­çš„å…¸å‹å¤„ç†æ–¹å¼ï¼Œå®é™…ä¸Šï¼Œé€šå¸¸çš„åšæ³•æ˜¯åˆ©ç”¨æˆ‘ä»¬çŸ¥é“é€‚ç”¨äºå•ä¸ªæ¨¡æ€çš„å½’çº³åè§ï¼Œç„¶åå·¥ç¨‹è®¾è®¡ç»“åˆè¿™äº›ä¸åŒå­ç³»ç»Ÿçš„æ–¹æ³•ã€‚è¿™å¯èƒ½æ„å‘³ç€ä¸ºæ¯ä¸€ä¸ªæ¨¡æ€æ„å»ºç‰¹å®šçš„è¾“å…¥æ¨¡å—ï¼Œç„¶åå°è¯•å„ç§ä¸åŒçš„ç»„åˆæ–¹å¼ã€‚
- en: So this can work but it gives us systems that in principle really will only
    work on one or a small number of domains and it gives us systems that are very
    hard to maintainã€‚tend to be fragileï¼Œ tend to depend on specific processing assumptions
    about the input modalities so rather than do that we sort of want to move in the
    direction of having unified black box architectures that kind of just work and
    the idea here is that if we can get to that point we can abstract the architecture
    construction process and really focus on other more high-level problems so this
    is sort of the motivation for this line of workã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å¯ä»¥èµ·ä½œç”¨ï¼Œä½†å®ƒç»™æˆ‘ä»¬æä¾›çš„ç³»ç»ŸåŸåˆ™ä¸Šåªä¼šåœ¨ä¸€ä¸ªæˆ–å°‘æ•°å‡ ä¸ªé¢†åŸŸå†…æœ‰æ•ˆï¼Œå¹¶ä¸”è¿™äº›ç³»ç»Ÿç»´æŠ¤èµ·æ¥éå¸¸å›°éš¾ï¼Œå¾€å¾€è„†å¼±ï¼Œä¾èµ–äºè¾“å…¥æ¨¡æ€çš„ç‰¹å®šå¤„ç†å‡è®¾ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¸Œæœ›æœç€æ‹¥æœ‰ç»Ÿä¸€é»‘ç®±æ¶æ„çš„æ–¹å‘å‘å±•ï¼Œè¿™ç§æ¶æ„å¯ä»¥æ›´æ–¹ä¾¿åœ°å·¥ä½œã€‚è¿™é‡Œçš„æƒ³æ³•æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬èƒ½è¾¾åˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠ½è±¡å‡ºæ¶æ„æ„å»ºè¿‡ç¨‹ï¼ŒçœŸæ­£å…³æ³¨å…¶ä»–æ›´é«˜å±‚æ¬¡çš„é—®é¢˜ï¼Œè¿™å°±æ˜¯è¿™é¡¹å·¥ä½œçš„åŠ¨åŠ›æ‰€åœ¨ã€‚
- en: And the way that we're going to be doing this is of courseã€‚by working on the
    most general purpose architecture that we have so farã€‚which is basically a transformerï¼Œ
    and you'll all be very familiar with the basic building blocks of a transformerã€‚but
    just at a very high level we can think about what they do rightã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¦åšåˆ°è¿™ä¸€ç‚¹çš„æ–¹æ³•å½“ç„¶æ˜¯é€šè¿‡ä½¿ç”¨åˆ°ç›®å‰ä¸ºæ­¢æœ€é€šç”¨çš„æ¶æ„ï¼Œä¹Ÿå°±æ˜¯å˜æ¢å™¨ã€‚ä½ ä»¬éƒ½ä¼šéå¸¸ç†Ÿæ‚‰å˜æ¢å™¨çš„åŸºæœ¬æ„å»ºå—ï¼Œä½†ä»ä¸€ä¸ªéå¸¸é«˜çš„å±‚é¢æ¥çœ‹ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘å®ƒä»¬çš„åŠŸèƒ½ã€‚
- en: which is they use a general purpose inductive biasï¼Œ so they're non-localã€‚which
    means they're not making domainspecific assumptions about which point should be
    compared to each otherã€‚rather they tend to be global in terms of the attentional
    focus that they haveã€‚They use position as a feature rather than a hard constraint
    of the architectureã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å˜æ¢å™¨ä½¿ç”¨é€šç”¨çš„å½’çº³åå·®ï¼Œå› æ­¤å®ƒä»¬æ˜¯éå±€éƒ¨çš„ã€‚è¿™æ„å‘³ç€å®ƒä»¬ä¸ä¼šå¯¹å“ªäº›ç‚¹åº”è¯¥ç›¸äº’æ¯”è¾ƒåšå‡ºé¢†åŸŸç‰¹å®šçš„å‡è®¾ã€‚ç›¸åï¼Œå®ƒä»¬åœ¨æ³¨æ„åŠ›èšç„¦ä¸Šå€¾å‘äºå…¨çƒæ€§ã€‚å®ƒä»¬å°†ä½ç½®ä½œä¸ºç‰¹å¾ï¼Œè€Œä¸æ˜¯æ¶æ„çš„ç¡¬æ€§çº¦æŸã€‚
- en: and this is in contrast to just sort of MLP based architectures or confNs in
    the way that they typically workã€‚which used position as an architectural component
    to constrain how compute is happeningã€‚And then of course finally there's extensive
    weight sharing in the way that they're designed and because they focus on mat
    Mosã€‚they tend to be TBU and GPU friendly so these are all very nice things about
    the way transformers workã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸åŸºäºMLPçš„æ¶æ„æˆ–å…¸å‹å·¥ä½œæ–¹å¼çš„å·ç§¯ç½‘ç»œå½¢æˆå¯¹æ¯”ï¼Œå®ƒä»¬å°†ä½ç½®ä½œä¸ºæ¶æ„ç»„ä»¶æ¥é™åˆ¶è®¡ç®—çš„å‘ç”Ÿã€‚å½“ç„¶ï¼Œæœ€åï¼Œå®ƒä»¬åœ¨è®¾è®¡ä¸Šæœ‰å¹¿æ³›çš„æƒé‡å…±äº«ï¼Œå¹¶ä¸”ç”±äºå®ƒä»¬å…³æ³¨çŸ©é˜µè¿ç®—ï¼Œå®ƒä»¬é€šå¸¸å¯¹TPUå’ŒGPUå‹å¥½ï¼Œå› æ­¤è¿™æ˜¯å˜æ¢å™¨å·¥ä½œæ–¹å¼çš„ä¸€äº›ä¼˜ç‚¹ã€‚
- en: Of course on the other hand they have very poor compute memory scaling and there
    are two components of thisã€‚so attention itself scales quadraically so there's
    there's this big O of m squared L complexity at the heart of transformers and
    I like writing it this way because it really emphasizes that this is a property
    of basically as you make your models bigger either at the input size or as you
    make them deeperã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå¦ä¸€æ–¹é¢ï¼Œå®ƒä»¬åœ¨è®¡ç®—å†…å­˜æ‰©å±•æ–¹é¢è¡¨ç°å¾ˆå·®ï¼Œè¿™æœ‰ä¸¤ä¸ªç»„æˆéƒ¨åˆ†ã€‚å› æ­¤ï¼Œæ³¨æ„åŠ›æœ¬èº«çš„æ‰©å±•æ˜¯å¹³æ–¹çš„ï¼Œå˜æ¢å™¨çš„æ ¸å¿ƒæ˜¯è¿™ç§å¤§Oçš„må¹³æ–¹Lå¤æ‚åº¦ã€‚æˆ‘å–œæ¬¢ä»¥è¿™ç§æ–¹å¼ä¹¦å†™ï¼Œå› ä¸ºå®ƒçœŸæ­£å¼ºè°ƒè¿™æ˜¯ä¸€ä¸ªå±æ€§ï¼ŒåŸºæœ¬ä¸Šæ˜¯éšç€ä½ å¢å¤§æ¨¡å‹ï¼Œæ— è®ºæ˜¯è¾“å…¥å¤§å°è¿˜æ˜¯æ·±åº¦ã€‚
- en: you're just this problem is just going to get worse and because you have this
    scaling in depth as wellã€‚there's another sort of practical thing that happens
    here because're the amount of compute that we're doing is proportional to the
    input size so there's no bottleneck in the way that standard transformers work
    even the linear scaling becomes a problem and so in practice for very very large
    transformers this can often be the bottleneck that really matters but they're
    both at play here and so we really want to sort of tamp down both of these and
    so sort of the perspective here is that have really general purpose architecturesã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªé—®é¢˜åªä¼šå˜å¾—æ›´ç³Ÿï¼Œå› ä¸ºä½ è¿˜æ¶‰åŠåˆ°æ·±åº¦çš„æ‰©å±•ã€‚è¿™é‡Œè¿˜æœ‰å¦ä¸€ä¸ªå®é™…çš„é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬è¿›è¡Œçš„è®¡ç®—é‡ä¸è¾“å…¥å¤§å°æˆæ­£æ¯”ï¼Œå› æ­¤åœ¨æ ‡å‡†å˜æ¢å™¨çš„å·¥ä½œæ–¹å¼ä¸­æ²¡æœ‰ç“¶é¢ˆï¼Œå³ä½¿çº¿æ€§æ‰©å±•ä¹Ÿä¼šæˆä¸ºä¸€ä¸ªé—®é¢˜ã€‚å› æ­¤ï¼Œåœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯¹äºéå¸¸éå¸¸å¤§çš„å˜æ¢å™¨ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä¸ªçœŸæ­£é‡è¦çš„ç“¶é¢ˆï¼Œä½†è¿™ä¸¤è€…éƒ½åœ¨è¿™é‡Œèµ·ä½œç”¨ï¼Œæ‰€ä»¥æˆ‘ä»¬ç¡®å®å¸Œæœ›æŠ‘åˆ¶è¿™ä¸¤è€…ï¼Œä»è€Œä½¿æˆ‘ä»¬æ‹¥æœ‰çœŸæ­£é€šç”¨çš„æ¶æ„ã€‚
- en: We can't have ones that are just sort of in principle generalã€‚we have ones that
    we have to have ones that you can actually use on the scales and the kinds of
    data that we care aboutã€‚And so just to this will all be old half for all of youã€‚but
    just the way that standard QKV attention works is basically like thisã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸èƒ½ä»…ä»…æœ‰ä¸€äº›åŸåˆ™ä¸Šé€šç”¨çš„å‡½æ•°ã€‚æˆ‘ä»¬å¿…é¡»æœ‰ä¸€äº›å®é™…å¯ä»¥åœ¨æˆ‘ä»¬å…³å¿ƒçš„å°ºåº¦å’Œæ•°æ®ç±»å‹ä¸Šä½¿ç”¨çš„å‡½æ•°ã€‚å› æ­¤ï¼Œå¯¹ä½ ä»¬æ¥è¯´è¿™å¯èƒ½éƒ½æ˜¯æ—§çš„ï¼Œä½†æ ‡å‡†çš„QKVæ³¨æ„åŠ›çš„å·¥ä½œæ–¹å¼åŸºæœ¬ä¸Šæ˜¯è¿™æ ·çš„ã€‚
- en: so it's all about matrix multiplications so we have some input we compute the
    query keys and values by having a 1D convolution a one by one convolution that
    we run over the input we then compute the attention the attention scores this
    is a matrix multiply that has the following these sorts of shapes we then use
    the output here to computeã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™å…¨æ˜¯å…³äºçŸ©é˜µä¹˜æ³•çš„ã€‚æˆ‘ä»¬æœ‰ä¸€äº›è¾“å…¥ï¼Œé€šè¿‡ä¸€ç»´å·ç§¯å’Œä¸€ä¹˜ä¸€å·ç§¯è®¡ç®—æŸ¥è¯¢é”®å’Œå€¼ï¼Œç„¶åè®¡ç®—æ³¨æ„åŠ›ï¼Œæ³¨æ„åŠ›å¾—åˆ†æ˜¯ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œå…·æœ‰ä»¥ä¸‹è¿™äº›å½¢çŠ¶ã€‚ç„¶åæˆ‘ä»¬ä½¿ç”¨è¿™é‡Œçš„è¾“å‡ºæ¥è®¡ç®—ã€‚
- en: The weights to compute the actual output of the attention module itselfã€‚and
    then finally we won run this through an additional MLP which is applied convolutionally
    to get the outputsã€‚So this is the starting point of what we're working on hereã€‚And
    let me just briefly just reiterate why we would want the sort of advantages that
    we have with these sort of standard transformersã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: æƒé‡ï¼Œä»¥è®¡ç®—æ³¨æ„åŠ›æ¨¡å—çš„å®é™…è¾“å‡ºã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡ä¸€ä¸ªé¢å¤–çš„å¤šå±‚æ„ŸçŸ¥æœºï¼ˆMLPï¼‰è¿è¡Œè¿™ä¸ªï¼Œåº”ç”¨å·ç§¯æ¥è·å–è¾“å‡ºã€‚è¿™æ˜¯æˆ‘ä»¬åœ¨è¿™é‡Œå·¥ä½œçš„èµ·ç‚¹ã€‚è®©æˆ‘ç®€è¦é‡ç”³ä¸€ä¸‹ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬ä¼šæƒ³è¦è¿™äº›æ ‡å‡†å˜æ¢å™¨çš„ä¼˜åŠ¿ã€‚
- en: so nonlocality is one of the two sort of inductive bias principles that we have
    here it's useful I think to contrast this to way that the effective locality that
    you get in conves and what this actually means so if we look at basically as a
    function of depth which inputs can see which other ones which means like how easily
    it is to express a function of two input points let's say we look at these this
    yellow and purple point here at the input now I've sent them sort of as far apart
    as possible but we might ask how deep would the effective computation have to
    be before you actually process these two and if you look at a three by three convolutionã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥éå±€éƒ¨æ€§æ˜¯æˆ‘ä»¬è¿™é‡Œçš„ä¸¤ç§å½’çº³åå·®åŸåˆ™ä¹‹ä¸€ï¼Œæˆ‘è®¤ä¸ºå°†å…¶ä¸æœ‰æ•ˆå±€éƒ¨æ€§è¿›è¡Œå¯¹æ¯”æ˜¯æœ‰ç”¨çš„ï¼Œè¿™åœ¨å·ç§¯ä¸­æ„å‘³ç€ä»€ä¹ˆã€‚å¦‚æœæˆ‘ä»¬åŸºæœ¬ä¸Šä»æ·±åº¦çš„è§’åº¦æ¥çœ‹ï¼Œå“ªä¸ªè¾“å…¥å¯ä»¥çœ‹åˆ°å…¶ä»–è¾“å…¥ï¼Œè¿™æ„å‘³ç€è¡¨è¾¾ä¸¤ä¸ªè¾“å…¥ç‚¹çš„å‡½æ•°æœ‰å¤šå®¹æ˜“ã€‚å‡è®¾æˆ‘ä»¬çœ‹ä¸€ä¸‹è¾“å…¥ä¸­çš„è¿™ä¸ªé»„è‰²å’Œç´«è‰²ç‚¹ï¼Œæˆ‘æŠŠå®ƒä»¬å°½å¯èƒ½åœ°åˆ†å¼€ï¼Œä½†æˆ‘ä»¬å¯èƒ½ä¼šé—®ï¼Œæœ‰æ•ˆè®¡ç®—å¿…é¡»å¤šæ·±æ‰èƒ½å¤„ç†è¿™ä¸¤ä¸ªç‚¹ï¼Œå¦‚æœä½ çœ‹ä¸€ä¸‹ä¸‰ä¹˜ä¸‰çš„å·ç§¯ã€‚
- en: You're talking you're having to look basically until the very end of the network
    until you're processing these things together and what this means is that the
    functions that you can express that actually look at both of these points into
    up being quite shallow because they have to be built on top of this very very
    deep stack that just gives you the locality and so in point of fact if you look
    at for example the way Resnets work so you have an initial block which has a 7
    by7 convolution and then afterwards it's  three by three cons all the way up you
    need 283 by three cons with that standard processing stack before all of the 224
    by 224 pixels in an image are looking at each other and what this means is that
    in a Resnet 50 the points on the very edge of the pixels actually never see each
    other and this this is I found this a little bit counterintuitive but it suggests
    that we really are constraining quite a lot the functions that are easy to express
    with these models and so there are some functions of images you just can't you
    can't capture with a resant 50ã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¿…é¡»åŸºæœ¬ä¸Šç­‰åˆ°ç½‘ç»œçš„æœ€åï¼Œæ‰èƒ½å°†è¿™äº›ä¸œè¥¿ä¸€èµ·å¤„ç†ã€‚è¿™æ„å‘³ç€ä½ èƒ½å¤Ÿè¡¨è¾¾çš„å‡½æ•°å®é™…ä¸Šçœ‹èµ·æ¥éå¸¸æµ…ï¼Œå› ä¸ºå®ƒä»¬å¿…é¡»å»ºç«‹åœ¨è¿™ä¸ªéå¸¸æ·±çš„å †æ ˆä¹‹ä¸Šï¼Œè€Œè¿™ä¸ªå †æ ˆä»…æä¾›å±€éƒ¨æ€§ã€‚å®é™…ä¸Šï¼Œå¦‚æœä½ çœ‹çœ‹ä¾‹å¦‚ResNetçš„å·¥ä½œæ–¹å¼ï¼Œä½ ä¼šå‘ç°åˆå§‹å—æœ‰ä¸€ä¸ªä¸ƒä¹˜ä¸ƒçš„å·ç§¯ï¼Œç„¶åä¹‹åæ˜¯ä¸‰ä¹˜ä¸‰çš„å·ç§¯ï¼Œç›´åˆ°æœ€åï¼Œä½ éœ€è¦283ä¸ªä¸‰ä¹˜ä¸‰çš„å·ç§¯ï¼Œæ‰èƒ½ä½¿å›¾åƒä¸­çš„224ä¹˜224åƒç´ äº’ç›¸æŸ¥çœ‹ã€‚è¿™æ„å‘³ç€åœ¨ResNet
    50ä¸­ï¼Œåƒç´ è¾¹ç¼˜çš„ç‚¹å®é™…ä¸Šä»æœªçœ‹åˆ°å½¼æ­¤ã€‚æˆ‘å‘ç°è¿™ä¸€ç‚¹æœ‰ç‚¹è¿åç›´è§‰ï¼Œä½†å®ƒè¡¨æ˜æˆ‘ä»¬ç¡®å®åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šé™åˆ¶äº†è¿™äº›æ¨¡å‹ä¸­å®¹æ˜“è¡¨è¾¾çš„å‡½æ•°ï¼Œå› æ­¤æœ‰äº›å›¾åƒå‡½æ•°ä½ æ ¹æœ¬æ— æ³•ç”¨ResNet
    50æ•æ‰åˆ°ã€‚
- en: On the other handï¼Œ if you look at an architecture that has global attention
    over the full over the full inputã€‚so a transformerï¼Œ if you could scale it that
    way or perceives we're going to be talking aboutã€‚all of the pixels can interact
    so the model can basically capture these things and express these functions much
    more easily can be expressed in things that put locality firstã€‚We also the other
    sort of interesting property of these sorts of architectures is that position
    is feurized and this basically means that we're no longer sort of encoding the
    architectural location of something to figure out where it's located with respect
    to the other ones and this allows the network to basically use any positional
    information that it wantsã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€æ–¹é¢ï¼Œå¦‚æœä½ çœ‹ä¸€ä¸ªåœ¨æ•´ä¸ªè¾“å…¥ä¸Šå…·æœ‰å…¨å±€æ³¨æ„åŠ›çš„æ¶æ„ã€‚å› æ­¤ï¼Œå˜å‹å™¨ï¼Œå¦‚æœå¯ä»¥è¿™æ ·ç¼©æ”¾ï¼Œæˆ–è€…è¯´æˆ‘ä»¬å°†è®¨è®ºçš„æ„ŸçŸ¥ã€‚æ‰€æœ‰åƒç´ éƒ½å¯ä»¥ç›¸äº’ä½œç”¨ï¼Œå› æ­¤æ¨¡å‹åŸºæœ¬ä¸Šå¯ä»¥æ•æ‰è¿™äº›ä¸œè¥¿ï¼Œå¹¶æ›´å®¹æ˜“åœ°è¡¨è¾¾è¿™äº›åŠŸèƒ½ï¼Œè¿™äº›åŠŸèƒ½å¯ä»¥ä»¥å°†å±€éƒ¨æ€§æ”¾åœ¨é¦–ä½çš„æ–¹å¼æ¥è¡¨è¾¾ã€‚æˆ‘ä»¬è¿˜å‘ç°è¿™äº›æ¶æ„çš„å¦ä¸€ä¸ªæœ‰è¶£å±æ€§æ˜¯ä½ç½®è¢«ç‰¹å¾åŒ–ï¼Œè¿™åŸºæœ¬ä¸Šæ„å‘³ç€æˆ‘ä»¬ä¸å†å¯¹æŸç‰©çš„æ¶æ„ä½ç½®è¿›è¡Œç¼–ç ï¼Œä»¥ç¡®å®šå®ƒç›¸å¯¹äºå…¶ä»–ç‰©ä½“çš„ä½ç½®ï¼Œè¿™å…è®¸ç½‘ç»œåŸºæœ¬ä¸Šä½¿ç”¨å®ƒæƒ³è¦çš„ä»»ä½•ä½ç½®ä¿¡æ¯ã€‚
- en: but can also discard it as it prefers and so this is the standard way it's doneã€‚of
    course in the context of architectures that use Fourier or sinusoidal like featuresã€‚but
    there's a lot of flexibility hereã€‚Okayï¼Œ so now just thinking in terms of how conves
    relate to transformers sort of at the opposite endã€‚it may look like that we have
    a sort of scalability versus general generality tradeoff and so if we look at
    convenants the way that they're applied so typically we can think about using
    them on grid structured dataã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¹Ÿå¯ä»¥æ ¹æ®éœ€è¦ä¸¢å¼ƒï¼Œè¿™æ˜¯æ ‡å‡†çš„åšæ³•ã€‚å½“ç„¶æ˜¯åœ¨ä½¿ç”¨å‚…é‡Œå¶æˆ–æ­£å¼¦ç‰¹å¾çš„æ¶æ„èƒŒæ™¯ä¸‹ã€‚ä½†è¿™é‡Œæœ‰å¾ˆå¤šçµæ´»æ€§ã€‚å¥½çš„ï¼Œé‚£ä¹ˆç°åœ¨è€ƒè™‘ä¸€ä¸‹å·ç§¯ä¸å˜å‹å™¨åœ¨å¯¹ç«‹ç«¯çš„å…³ç³»ã€‚å¯èƒ½çœ‹èµ·æ¥æˆ‘ä»¬åœ¨å¯æ‰©å±•æ€§ä¸é€šç”¨æ€§ä¹‹é—´æœ‰ä¸€ç§æƒè¡¡ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬è§‚å¯Ÿå·ç§¯çš„åº”ç”¨æ–¹å¼ï¼Œé€šå¸¸æˆ‘ä»¬å¯ä»¥è®¤ä¸ºå®ƒä»¬æ˜¯åœ¨ç½‘æ ¼ç»“æ„æ•°æ®ä¸Šä½¿ç”¨ã€‚
- en: theyre of course generalizations of convolutions that work onã€‚on data sets with
    more interesting topologyï¼Œ but typically we can think of them as operating on
    grids of in some sort of spaceã€‚whereas transformers apply to generic setsï¼Œ so
    transformers are more general from this point of viewã€‚On the other handï¼Œ they
    scale muchï¼Œ much worseï¼Œ so conves are linearï¼Œ both in the input pointsã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œå®ƒä»¬æ˜¯å¯ä»¥åº”ç”¨äºå…·æœ‰æ›´æœ‰è¶£æ‹“æ‰‘çš„æ•°æ®é›†çš„å·ç§¯çš„å¹¿ä¹‰ï¼Œä½†é€šå¸¸æˆ‘ä»¬å¯ä»¥è®¤ä¸ºå®ƒä»¬åœ¨æŸç§ç©ºé—´çš„ç½‘æ ¼ä¸Šæ“ä½œã€‚è€Œå˜å‹å™¨é€‚ç”¨äºé€šç”¨é›†åˆï¼Œå› æ­¤ä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œå˜å‹å™¨æ›´ä¸ºé€šç”¨ã€‚å¦ä¸€æ–¹é¢ï¼Œå®ƒä»¬çš„ç¼©æ”¾è¦å·®å¾—å¤šï¼Œå› æ­¤å·ç§¯åœ¨è¾“å…¥ç‚¹ä¸Šæ˜¯çº¿æ€§çš„ã€‚
- en: the filter size and the number of layers of that architectureã€‚whereas transformers
    have this quadratic scaling and they're still linear in the depthã€‚So from this
    point of viewï¼Œ what we're interested in doing in the perceive line of work was
    to scale transformers but to keep the generality propertyã€‚so we want something
    that lives in between these two extremesã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: è¯¥æ¶æ„çš„è¿‡æ»¤å™¨å¤§å°å’Œå±‚æ•°ã€‚è€Œå˜å‹å™¨å…·æœ‰è¿™ç§å¹³æ–¹ç¼©æ”¾ï¼Œä¸”åœ¨æ·±åº¦ä¸Šä»ç„¶æ˜¯çº¿æ€§çš„ã€‚å› æ­¤ï¼Œä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬åœ¨æ„ŸçŸ¥å·¥ä½œä¸­çš„å…´è¶£åœ¨äºç¼©æ”¾å˜å‹å™¨ï¼Œä½†ä¿æŒé€šç”¨æ€§å±æ€§ã€‚æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›æœ‰ä¸€äº›ä»‹äºè¿™ä¸¤ç§æç«¯ä¹‹é—´çš„ä¸œè¥¿ã€‚
- en: And the way that we do this is by looking at self-attention and sort of modifying
    it in a way that allows us to scale betterã€‚so to walk through what self-attention
    actually does in sort of standard transformersã€‚we take our input array which here
    is written number as the indices which is the number of tokens or the number of
    pixels basically the number of input units depending what you're looking at and
    the number and the channelsã€‚we have a 1D convolution so this is big O of M with
    respect to the QK and Vã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿™æ ·åšçš„æ–¹æ³•æ˜¯è§‚å¯Ÿè‡ªæ³¨æ„åŠ›ï¼Œå¹¶åœ¨æŸç§ç¨‹åº¦ä¸Šä¿®æ”¹å®ƒï¼Œä»¥ä¾¿æ›´å¥½åœ°è¿›è¡Œç¼©æ”¾ã€‚å› æ­¤ï¼Œä¸ºäº†ç†è§£è‡ªæ³¨æ„åŠ›åœ¨æ ‡å‡†å˜å‹å™¨ä¸­å®é™…ä¸Šåšäº†ä»€ä¹ˆã€‚æˆ‘ä»¬å–è¾“å…¥æ•°ç»„ï¼Œè¿™é‡Œä»¥ç´¢å¼•å½¢å¼å†™å‡ºï¼Œè¡¨ç¤ºæ ‡è®°çš„æ•°é‡æˆ–åƒç´ çš„æ•°é‡ï¼ŒåŸºæœ¬ä¸Šæ˜¯è¾“å…¥å•å…ƒçš„æ•°é‡ä»¥åŠæ•°é‡å’Œé€šé“ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªä¸€ç»´å·ç§¯ï¼Œå› æ­¤è¿™æ˜¯å…³äºQKå’ŒVçš„Oï¼ˆMï¼‰ã€‚
- en: We then compute the attention maps using the output of this operationã€‚this gives
    us a matrix multiplyï¼Œ which is the source of the quadratic scalingã€‚And then finallyã€‚we
    compute output features with another matrix multiplyã€‚This is already we're already
    rate limited here because for even standard resolution images in this quite large
    so it's around 50ã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬ä½¿ç”¨è¯¥æ“ä½œçš„è¾“å‡ºè®¡ç®—æ³¨æ„åŠ›å›¾ã€‚è¿™ç»™æˆ‘ä»¬å¸¦æ¥äº†ä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œè¿™æ˜¯å¹³æ–¹ç¼©æ”¾çš„æ¥æºã€‚æœ€åï¼Œæˆ‘ä»¬ç”¨å¦ä¸€ä¸ªçŸ©é˜µä¹˜æ³•è®¡ç®—è¾“å‡ºç‰¹å¾ã€‚è¿™é‡Œæˆ‘ä»¬å·²ç»å—åˆ°é€Ÿç‡é™åˆ¶ï¼Œå› ä¸ºå³ä½¿æ˜¯æ ‡å‡†åˆ†è¾¨ç‡çš„å›¾åƒä¹Ÿç›¸å½“å¤§ï¼Œæ‰€ä»¥å¤§çº¦æ˜¯50ã€‚
- en: 000 for standard imagenet images which are going to very small so this is something
    that just isn't going to work if we want deep architecturesã€‚So what we do is we
    replace at the input to the architectureã€‚we replace the self- attentionten with
    a cross attention layer and we do this using basically a learned queryã€‚and so
    we're replacing only the query from the input here with a learned component and
    so these indices and channels you can just think of these as basically working
    like a learned initial state for an RNNã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ ‡å‡†çš„ imagenet å›¾åƒï¼Œå¤§å°å°†ä¼šéå¸¸å°ï¼Œå› æ­¤å¦‚æœæˆ‘ä»¬æƒ³è¦æ·±åº¦æ¶æ„ï¼Œè¿™ç§æƒ…å†µæ˜¯è¡Œä¸é€šçš„ã€‚æ‰€ä»¥æˆ‘ä»¬æ‰€åšçš„æ˜¯åœ¨æ¶æ„çš„è¾“å…¥å¤„è¿›è¡Œæ›¿æ¢ã€‚æˆ‘ä»¬å°†è‡ªæ³¨æ„åŠ›å±‚æ›¿æ¢ä¸ºäº¤å‰æ³¨æ„åŠ›å±‚ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯é€šè¿‡ä¸€ä¸ªå­¦ä¹ åˆ°çš„æŸ¥è¯¢æ¥å®ç°çš„ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åªæ›¿æ¢è¾“å…¥ä¸­çš„æŸ¥è¯¢ï¼Œä½¿ç”¨ä¸€ä¸ªå­¦ä¹ åˆ°çš„ç»„ä»¶ï¼Œè¿™äº›ç´¢å¼•å’Œé€šé“å¯ä»¥è¢«çœ‹ä½œæ˜¯åŸºæœ¬ä¸Šä½œä¸º
    RNN çš„å­¦ä¹ åˆå§‹çŠ¶æ€ã€‚
- en: there's a variety of names that this idea goes under in the literatureã€‚we refer
    to them as sort of late as latetnts but they're sometimes called inducing points
    or other thingsã€‚So the basic idea is we're learning the inputs to the query and
    keeping the key value of it the sameã€‚The down or the sort of upside of this is
    that when we compute the attention map after thisã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ–‡çŒ®ä¸­ï¼Œè¿™ä¸ªæƒ³æ³•æœ‰å¾ˆå¤šä¸åŒçš„åå­—ã€‚æˆ‘ä»¬å°†å®ƒç§°ä¸ºæŸç§æ½œå˜é‡ï¼Œä½†æœ‰æ—¶ä¹Ÿç§°ä¸ºè¯±å¯¼ç‚¹æˆ–å…¶ä»–åç§°ã€‚åŸºæœ¬æ€æƒ³æ˜¯æˆ‘ä»¬å­¦ä¹ æŸ¥è¯¢çš„è¾“å…¥ï¼Œå¹¶ä¿æŒå®ƒçš„é”®å’Œå€¼ç›¸åŒã€‚è¿™æ ·åšçš„å¥½å¤„æ˜¯ï¼Œå½“æˆ‘ä»¬åœ¨æ­¤ä¹‹åè®¡ç®—æ³¨æ„åŠ›å›¾æ—¶ã€‚
- en: now we basically turn this from a square matrix to a rectangular matrix and
    reduces the complexity of the matrix multiplied to big of MNã€‚so now it's linear
    in the input sizeã€‚And second the second matrix multiply has the exact same propertyã€‚so
    it becomes from quadratic becomes linear and the quite cool thing about this is
    that okay so the cross attention is linear and complexity but the output is actually
    smaller and so this I think is actually the more important point here is that
    this allows us to map something which is quite large into something that has size
    that's independent of the input we saw we have full control over this it's a hyperparameter
    and this allows us to build deep networks on top of this on top of this latent
    so because this is of a small size that we can controlã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬åŸºæœ¬ä¸Šå°†å…¶ä»ä¸€ä¸ªæ–¹é˜µè½¬æ¢ä¸ºä¸€ä¸ªçŸ©å½¢çŸ©é˜µï¼Œå¹¶å°†çŸ©é˜µä¹˜æ³•çš„å¤æ‚åº¦é™ä½åˆ° MNã€‚å› æ­¤ï¼Œç°åœ¨å®ƒåœ¨è¾“å…¥å¤§å°ä¸Šæ˜¯çº¿æ€§çš„ã€‚ç¬¬äºŒä¸ªçŸ©é˜µä¹˜æ³•ä¹Ÿå…·æœ‰ç›¸åŒçš„ç‰¹æ€§ã€‚å› æ­¤ï¼Œä»äºŒæ¬¡å˜ä¸ºçº¿æ€§ï¼Œè¿™ä¸€ç‚¹å¾ˆé…·ã€‚äº¤å‰æ³¨æ„åŠ›çš„å¤æ‚åº¦æ˜¯çº¿æ€§çš„ï¼Œä½†è¾“å‡ºå®é™…ä¸Šæ›´å°ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºè¿™é‡Œæ›´é‡è¦çš„ä¸€ç‚¹æ˜¯ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå°†ä¸€äº›éå¸¸å¤§çš„ä¸œè¥¿æ˜ å°„åˆ°ä¸€ä¸ªä¸è¾“å…¥æ— å…³çš„å¤§å°ä¸Šã€‚æˆ‘ä»¬å¯¹æ­¤æœ‰å®Œå…¨çš„æ§åˆ¶ï¼Œè¿™æ˜¯ä¸€ç§è¶…å‚æ•°ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºæ·±å±‚ç½‘ç»œï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå¯ä»¥æ§åˆ¶çš„å°å°ºå¯¸ã€‚
- en: we can afford to have quadratic complexity on top of thisã€‚And so we use this
    idea sorry I'm still a little bit confused as to how you guys are able to turn
    the query into a rectangle in the second stepã€‚is it because you replaced the query
    with a learned something that is significantly smaller compared to the input size
    in the first stepã€‚Yeah that's exactly right so if you look at so the underlying
    matrix multiply here which is written as the QK transpose so this will basically
    so the outer dimension here has shape n which is determined by the query and so
    by shrinking that query we're just changing the output of the matrix multiplyã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ‰¿å—äºŒæ¬¡å¤æ‚åº¦ã€‚æ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ªæƒ³æ³•ï¼Œå¯¹ä¸èµ·ï¼Œæˆ‘ä»ç„¶æœ‰ç‚¹å›°æƒ‘ï¼Œä½ ä»¬æ˜¯å¦‚ä½•åœ¨ç¬¬äºŒæ­¥ä¸­å°†æŸ¥è¯¢è½¬åŒ–ä¸ºçŸ©å½¢çš„ã€‚æ˜¯å› ä¸ºä½ ä»¬å°†æŸ¥è¯¢æ›¿æ¢ä¸ºä¸€ä¸ªå­¦ä¹ åˆ°çš„ä¸œè¥¿ï¼Œè€Œè¿™ä¸ªä¸œè¥¿çš„å¤§å°ç›¸æ¯”äºç¬¬ä¸€æ­¥çš„è¾“å…¥å¤§å°è¦å°å¾—å¤šå—ï¼Ÿæ˜¯çš„ï¼Œæ­£æ˜¯å¦‚æ­¤ã€‚å¦‚æœä½ çœ‹çœ‹è¿™é‡Œçš„åŸºæœ¬çŸ©é˜µä¹˜æ³•ï¼Œå®ƒå†™ä½œ
    QK è½¬ç½®ï¼Œæ‰€ä»¥å¤–éƒ¨ç»´åº¦çš„å½¢çŠ¶æ˜¯ nï¼Œè¿™ç”±æŸ¥è¯¢å†³å®šï¼Œå› æ­¤é€šè¿‡ç¼©å°æŸ¥è¯¢ï¼Œæˆ‘ä»¬åªæ˜¯åœ¨æ”¹å˜çŸ©é˜µä¹˜æ³•çš„è¾“å‡ºã€‚
- en: Okayï¼Œ thank youã€‚å—¯ã€‚ğŸ˜Šï¼ŒYeahï¼Œ so I guessã€‚Sorry go ahead please Okay go so basically
    you only do that for the query right so key and value remain like the original
    size matricesã€‚correctã€‚That's right yeahï¼Œ Okayï¼Œ but so basically soã€‚I don't know
    what i'm not understanding basically so the problem for me is that for a query
    now in my head I'm like looking forã€‚let's say I have like the I token now there
    is no IF query anymoreã€‚Doesn't that cause a problemï¼Ÿ
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè°¢è°¢ã€‚å—¯ã€‚ğŸ˜Šï¼Œæ˜¯çš„ï¼Œæˆ‘æƒ³ã€‚å¯¹ä¸èµ·ï¼Œè¯·ç»§ç»­ã€‚å¥½çš„ï¼Œæ‰€ä»¥åŸºæœ¬ä¸Šä½ åªå¯¹æŸ¥è¯¢è¿™æ ·åšï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥é”®å’Œå€¼ä»ç„¶ä¿æŒåŸå§‹å¤§å°çš„çŸ©é˜µã€‚å¯¹å—ï¼Ÿæ²¡é”™ï¼Œå¥½çš„ï¼Œä½†åŸºæœ¬ä¸Šã€‚å—¯ï¼Œæˆ‘ä¸å¤ªæ˜ç™½ï¼ŒåŸºæœ¬ä¸Šå¯¹æˆ‘æ¥è¯´ï¼Œé—®é¢˜æ˜¯å¯¹äºä¸€ä¸ªæŸ¥è¯¢ï¼Œç°åœ¨åœ¨æˆ‘è„‘æµ·ä¸­æˆ‘åœ¨å¯»æ‰¾ã€‚å‡è®¾æˆ‘ç°åœ¨æœ‰
    I tokenï¼Œç°åœ¨æ²¡æœ‰ IF æŸ¥è¯¢äº†ã€‚è¿™ä¸ä¼šå¯¼è‡´é—®é¢˜å—ï¼Ÿ
- en: Like when I'm trying to use itã€‚And like to compute scoresã€‚Yeah so what's happening
    here is your compare you'll have a smaller subset ofque so if you think about
    this not in terms of matrix multiplies but in terms of comparing each query to
    each key so in normal self attentionten we have one query for each key so every
    point compares to every other point right so here what we've done is instead of
    comparing every point to every other point we have a set of sort of cluster centers
    you might be able to think about them as so it's a smaller number and we compare
    each of those to each of the input pointsã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘åœ¨å°è¯•ä½¿ç”¨å®ƒæ—¶ä¸€æ ·ã€‚å¹¶ä¸”è¦è®¡ç®—å¾—åˆ†ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥è¿™é‡Œå‘ç”Ÿçš„äº‹æƒ…æ˜¯ä½ ä¼šæ¯”è¾ƒï¼Œä½ ä¼šæœ‰ä¸€ä¸ªè¾ƒå°çš„å­é›†ï¼Œå› æ­¤å¦‚æœä½ ä»æ¯”è¾ƒæ¯ä¸ªæŸ¥è¯¢ä¸æ¯ä¸ªé”®çš„è§’åº¦æ¥çœ‹ï¼Œè€Œä¸æ˜¯ä»çŸ©é˜µä¹˜æ³•çš„è§’åº¦æ¥çœ‹ï¼Œåœ¨æ­£å¸¸çš„è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œæˆ‘ä»¬å¯¹æ¯ä¸ªé”®éƒ½æœ‰ä¸€ä¸ªæŸ¥è¯¢ï¼Œå› æ­¤æ¯ä¸ªç‚¹éƒ½ä¸å…¶ä»–æ¯ä¸ªç‚¹è¿›è¡Œæ¯”è¾ƒï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥åœ¨è¿™é‡Œæˆ‘ä»¬æ‰€åšçš„æ˜¯ï¼Œä¸æ˜¯å°†æ¯ä¸ªç‚¹ä¸å…¶ä»–æ¯ä¸ªç‚¹è¿›è¡Œæ¯”è¾ƒï¼Œè€Œæ˜¯æˆ‘ä»¬æœ‰ä¸€ç»„å¯ä»¥æƒ³è±¡æˆâ€œèšç±»ä¸­å¿ƒâ€çš„ä¸œè¥¿ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªè¾ƒå°çš„æ•°é‡ï¼Œæˆ‘ä»¬å°†æ¯ä¸€ä¸ªèšç±»ä¸­å¿ƒä¸æ¯ä¸€ä¸ªè¾“å…¥ç‚¹è¿›è¡Œæ¯”è¾ƒã€‚
- en: But we don't know whichã€‚Which tokens technically belong to which clustersï¼Œ rightï¼Ÿ
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬ä¸çŸ¥é“å“ªäº›æ ‡è®°æŠ€æœ¯ä¸Šå±äºå“ªäº›èšç±»ï¼Œå¯¹å§ï¼Ÿ
- en: That's right so it has to be learned yeah exactly so one way to think about
    this about what's happening here is that we're we're instead of so in a normal
    selfcent selfatten and a transformer by comparing to all to all we're sort of
    saying okayã€‚I know what the feature is at this point and I want it to attend to
    similar features here what we're saying is we're learning a bunch of supplementary
    points that that should be sort of maximally similar to some subset of the inputsã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ²¡é”™ï¼Œæ‰€ä»¥å®ƒå¿…é¡»æ˜¯å­¦ä¹ çš„ï¼Œæ²¡é”™ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¿™æ ·æ€è€ƒè¿™é‡Œå‘ç”Ÿçš„äº‹æƒ…ï¼šåœ¨æ­£å¸¸çš„è‡ªæ³¨æ„åŠ›å’Œå˜æ¢å™¨ä¸­ï¼Œé€šè¿‡å¯¹æ‰€æœ‰ç‚¹è¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬å¯ä»¥è¯´ï¼Œå¥½çš„ã€‚æˆ‘çŸ¥é“è¿™ä¸ªç‚¹çš„ç‰¹å¾ï¼Œæˆ‘æƒ³è®©å®ƒå…³æ³¨ç›¸ä¼¼çš„ç‰¹å¾ã€‚åœ¨è¿™é‡Œæˆ‘ä»¬æ‰€è¯´çš„æ˜¯ï¼Œæˆ‘ä»¬æ­£åœ¨å­¦ä¹ ä¸€ç»„è¡¥å……ç‚¹ï¼Œè¿™äº›ç‚¹åº”è¯¥ä¸æŸä¸ªè¾“å…¥å­é›†å°½å¯èƒ½ç›¸ä¼¼ã€‚
- en: So correct if I'm wrongï¼Œ but this is essentially doing some sort of hard tension
    where you're saying like like instead of like querying over all the pointsã€‚Is
    select like some pointsï¼Œ which we think are like vital similar and only like puts
    all attention over thisã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘è¯´é”™äº†ï¼Œè¯·çº æ­£æˆ‘ï¼Œä½†è¿™æœ¬è´¨ä¸Šæ˜¯åœ¨è¿›è¡ŒæŸç§ç¡¬æ€§çº¦æŸï¼Œä½ æ˜¯åœ¨è¯´ï¼Œæ¯”å¦‚è¯´ï¼Œä¸æ˜¯å¯¹æ‰€æœ‰ç‚¹è¿›è¡ŒæŸ¥è¯¢ï¼Œè€Œæ˜¯é€‰æ‹©ä¸€äº›æˆ‘ä»¬è®¤ä¸ºæ˜¯é‡è¦ä¸”ç›¸ä¼¼çš„ç‚¹ï¼Œå¹¶ä¸”åªå¯¹è¿™äº›ç‚¹è¿›è¡Œæ‰€æœ‰çš„æ³¨æ„åŠ›ã€‚
- en: Hardening like this points you are selectedingï¼Œ rightï¼Ÿ
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åŠ å›ºæ˜¯æŒ‡ä½ é€‰æ‹©çš„ç‚¹ï¼Œå¯¹å§ï¼Ÿ
- en: Yeah so they're related that would be one way to think about it the sort of
    the slight modifier to that idea though is that they basically live in an abstract
    spaceã€‚so they're not assigned sort of one to one to one of the input queries or
    to one of the input points they're sort of learned so they can be somewhere in
    the middle but I think that's a good way to think about it that's a good intuitionã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ‰€ä»¥å®ƒä»¬æ˜¯ç›¸å…³çš„ï¼Œè¿™å¯ä»¥æ˜¯ä¸€ç§æ€è€ƒæ–¹å¼ï¼Œä½†è¿™ç§æƒ³æ³•çš„ç¨å¾®ä¿®æ”¹æ˜¯ï¼Œå®ƒä»¬åŸºæœ¬ä¸Šç”Ÿæ´»åœ¨ä¸€ä¸ªæŠ½è±¡ç©ºé—´ä¸­ã€‚å› æ­¤ï¼Œå®ƒä»¬å¹¶ä¸æ˜¯ä¸€ä¸€å¯¹åº”äºæŸä¸ªè¾“å…¥æŸ¥è¯¢æˆ–æŸä¸ªè¾“å…¥ç‚¹ï¼Œè€Œæ˜¯æŸç§ç¨‹åº¦ä¸Šæ˜¯å­¦ä¹ å¾—æ¥çš„ï¼Œæ‰€ä»¥å®ƒä»¬å¯ä»¥åœ¨ä¸­é—´æŸä¸ªä½ç½®ï¼Œä½†æˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªå¥½çš„æ€è€ƒæ–¹å¼ï¼Œè¿™æ˜¯ä¸€ç§è‰¯å¥½çš„ç›´è§‰ã€‚
- en: But the I guess one of the places where I'm a little confused here is the you
    have here indices and indices for the two like the purple and green matrices on
    the far leftã€‚But those indices are not necessarily corresponding to inputs like
    in the NLP space those would not necessarily be tokens right these are just sort
    of random diseaseã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘æƒ³æˆ‘åœ¨è¿™é‡Œæœ‰äº›å›°æƒ‘çš„æ˜¯ï¼Œä½ è¿™é‡Œæœ‰ç´¢å¼•å’Œä¸¤ä¸ªçŸ©é˜µï¼ˆç´«è‰²å’Œç»¿è‰²ï¼‰çš„ç´¢å¼•ï¼Œåœ¨æœ€å·¦è¾¹ã€‚ä½†è¿™äº›ç´¢å¼•ä¸ä¸€å®šå¯¹åº”äºè¾“å…¥ï¼Œä¾‹å¦‚åœ¨ NLP é¢†åŸŸï¼Œè¿™äº›ä¸ä¸€å®šæ˜¯æ ‡è®°ï¼Œå¯¹å§ï¼Ÿè¿™äº›åªæ˜¯ä¸€äº›éšæœºçš„æ ‡è¯†ã€‚
- en: but the impact matrix in this case is the result of some kind of mapping from
    the input tokens to an N D matrixã€‚is that rightã€‚No it's actually so they it basically
    acts like it's it's a learned set of weights is one way to think about it so they
    function exactly the same way that learned position encodings do so it's basically
    just a you know it's a learned embedding but it's not it's not conditioned on
    anything it's just sort ofã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå½±å“çŸ©é˜µæ˜¯æŸç§ä»è¾“å…¥æ ‡è®°æ˜ å°„åˆ° N ç»´çŸ©é˜µçš„ç»“æœã€‚æ˜¯è¿™æ ·å—ï¼Ÿä¸ï¼Œå®é™…ä¸Šå®ƒåŸºæœ¬ä¸Šå°±åƒæ˜¯ï¼Œå®ƒæ˜¯ä¸€ä¸ªå­¦ä¹ çš„æƒé‡é›†ï¼Œè¿™å¯ä»¥æ˜¯ä¸€ç§æ€è€ƒæ–¹å¼ï¼Œæ‰€ä»¥å®ƒçš„åŠŸèƒ½ä¸å­¦ä¹ çš„ä½ç½®ç¼–ç å®Œå…¨ç›¸åŒï¼Œå› æ­¤å®ƒåŸºæœ¬ä¸Šåªæ˜¯ä¸€ç§å­¦ä¹ çš„åµŒå…¥ï¼Œä½†å¹¶ä¸æ˜¯åŸºäºä»»ä½•æ¡ä»¶çš„ï¼Œå®ƒåªæ˜¯ä¸€ç§ã€‚
- en: It just it's just a set of weightsã€‚Okayï¼Œ that makes more senseï¼Œ thank youã€‚Okayã€‚so
    if there are no more questionsï¼Œ I'm going to keep goingï¼Œ but of course feel free
    to interrupt meã€‚So the way that given this ideaï¼Œ so we have this learned latent
    arrayï¼Œ which againã€‚it functions sort of like an RNN initial state or it's a set
    of weightsã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ç»„æƒé‡ã€‚å¥½çš„ï¼Œè¿™æ ·æ›´æœ‰æ„ä¹‰ï¼Œè°¢è°¢ä½ ã€‚å¥½çš„ã€‚å¦‚æœæ²¡æœ‰å…¶ä»–é—®é¢˜ï¼Œæˆ‘å°†ç»§ç»­ï¼Œä½†å½“ç„¶è¯·éšæ—¶æ‰“æ–­æˆ‘ã€‚åŸºäºè¿™ä¸ªæƒ³æ³•ï¼Œæˆ‘ä»¬æœ‰è¿™ä¸ªå­¦ä¹ çš„æ½œåœ¨æ•°ç»„ï¼Œå®ƒçš„åŠŸèƒ½æœ‰ç‚¹åƒ
    RNN çš„åˆå§‹çŠ¶æ€ï¼Œæˆ–è€…è¯´å®ƒæ˜¯ä¸€ç»„æƒé‡ã€‚
- en: we basically randomly initialize that and then we use this to attend onto the
    input byte array and so the byte array here is the flatten set of pixels for example
    for imagenetã€‚And the output of this is going to live in the same space as so the
    same index space as the latent array doesã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åŸºæœ¬ä¸Šæ˜¯éšæœºåˆå§‹åŒ–ï¼Œç„¶ååˆ©ç”¨å®ƒæ¥å…³æ³¨è¾“å…¥çš„å­—èŠ‚æ•°ç»„ï¼Œå› æ­¤è¿™é‡Œçš„å­—èŠ‚æ•°ç»„æ˜¯åƒ ImageNet è¿™æ ·çš„åƒç´ å¹³é¢é›†åˆã€‚è€Œè¿™ä¸ªè¾“å‡ºå°†ä¸æ½œåœ¨æ•°ç»„å¤„äºç›¸åŒçš„ç©ºé—´ä¸­ï¼Œå³ç›¸åŒçš„ç´¢å¼•ç©ºé—´ã€‚
- en: and there's residual connections in the way that you would normally do in an
    intention layer as wellã€‚So once we're in this space we can then build an architecture
    by taking by using a standard transformer but phrased in the latent space rather
    than in the input space and this is going to allow us to basically end up because
    we sort of distilled the input down to the smaller space we can still flexibly
    allow all of these points to interact so this should be still nearly as expressive
    as the transformer as a normal transformer is and then each of the modules here
    now is quadratic in the latent size rather than the input size so this is something
    that we can control quite a lotã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶ä¸”åœ¨æ„å›¾å±‚çš„æ–¹å¼ä¸­æœ‰æ®‹å·®è¿æ¥ã€‚å› æ­¤ï¼Œä¸€æ—¦æˆ‘ä»¬è¿›å…¥è¿™ä¸ªç©ºé—´ï¼Œå°±å¯ä»¥é€šè¿‡ä½¿ç”¨æ ‡å‡†çš„å˜æ¢å™¨ï¼Œåœ¨æ½œåœ¨ç©ºé—´ä¸­æ„å»ºä¸€ä¸ªæ¶æ„ï¼Œè€Œä¸æ˜¯åœ¨è¾“å…¥ç©ºé—´ä¸­ï¼Œè¿™å°†ä½¿æˆ‘ä»¬åŸºæœ¬ä¸Šèƒ½å¤Ÿå› ä¸ºæˆ‘ä»¬å°†è¾“å…¥æç‚¼åˆ°è¾ƒå°çš„ç©ºé—´ä¸­ï¼Œä»ç„¶çµæ´»åœ°å…è®¸æ‰€æœ‰è¿™äº›ç‚¹è¿›è¡Œäº¤äº’ï¼Œæ‰€ä»¥è¿™ä»ç„¶åº”è¯¥ä¸æ™®é€šå˜æ¢å™¨ä¸€æ ·å…·æœ‰è¡¨ç°åŠ›ï¼Œè€Œä¸”è¿™é‡Œçš„æ¯ä¸ªæ¨¡å—ç°åœ¨çš„æ½œåœ¨å¤§å°æ˜¯äºŒæ¬¡çš„ï¼Œè€Œä¸æ˜¯è¾“å…¥å¤§å°ï¼Œæ‰€ä»¥è¿™æ˜¯æˆ‘ä»¬å¯ä»¥ç›¸å½“æ§åˆ¶çš„ã€‚
- en: So in the original version of the procedureï¼Œ we found it was very helpful to
    have additional crossatsã€‚so this is certainly something that you can do and the
    reason sort of the intuition behind this is that if this bottleneck is quite severeã€‚we
    can't maintain all of the information from the input and so we want these queries
    which are now sort of conditioned on the past to be able to look back at the input
    point and so this is something that we found to be quite helpful when tuning for
    the first paper but the caveat I will say is that we're no longer recommending
    this is best practice because these crossats end up being quite heavy but this
    is something that you can explore certainly if you want sort of more conditional
    queries or if you want to be able to cross attendends and new inputs that are
    coming inã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨ç¨‹åºçš„åŸå§‹ç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¢åŠ äº¤å‰æ³¨æ„åŠ›æ˜¯éå¸¸æœ‰å¸®åŠ©çš„ã€‚è¿™ç»å¯¹æ˜¯ä½ å¯ä»¥åšçš„äº‹æƒ…ï¼ŒèƒŒåçš„ç›´è§‰æ˜¯å¦‚æœè¿™ä¸ªç“¶é¢ˆç›¸å½“ä¸¥é‡ï¼Œæˆ‘ä»¬æ— æ³•ä¿æŒæ‰€æœ‰è¾“å…¥çš„ä¿¡æ¯ï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›è¿™äº›æŸ¥è¯¢ç°åœ¨åœ¨æŸç§ç¨‹åº¦ä¸Šä¾èµ–äºè¿‡å»ï¼Œèƒ½å¤Ÿå›é¡¾è¾“å…¥ç‚¹ã€‚å› æ­¤ï¼Œè¿™åœ¨ä¸ºç¬¬ä¸€ç¯‡è®ºæ–‡è¿›è¡Œè°ƒä¼˜æ—¶è¢«å‘ç°éå¸¸æœ‰å¸®åŠ©ï¼Œä½†æˆ‘ä¼šè¯´çš„è­¦å‘Šæ˜¯ï¼Œæˆ‘ä»¬ä¸å†æ¨èè¿™ä½œä¸ºæœ€ä½³å®è·µï¼Œå› ä¸ºè¿™äº›äº¤å‰æ³¨æ„åŠ›æœ€ç»ˆæ˜¯ç›¸å½“æ²‰é‡çš„ï¼Œä½†å¦‚æœä½ å¸Œæœ›æœ‰æ›´å¤šçš„æ¡ä»¶æŸ¥è¯¢ï¼Œæˆ–è€…å¸Œæœ›èƒ½å¤Ÿäº¤å‰å…³æ³¨æ–°è¾“å…¥ï¼Œè¿™ç»å¯¹æ˜¯ä½ å¯ä»¥æ¢ç´¢çš„äº‹æƒ…ã€‚
- en: The other thing that we found quite helpful in the context of data sets that
    have a limited amount of dataã€‚which for these architectures includes INe is to
    allow weight sharing in depth and so this basically just amounts to tying the
    weights for the different cross attention and different self-attention layers
    as they're repeated so this ends up looking like an RN that's enrolled in depthã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®é›†æœ‰æœ‰é™æ•°æ®é‡çš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬å‘ç°å¦ä¸€ä»¶éå¸¸æœ‰å¸®åŠ©çš„äº‹æƒ…ï¼Œé’ˆå¯¹è¿™äº›æ¶æ„ï¼ˆåŒ…æ‹¬ INeï¼‰ï¼Œæ˜¯å…è®¸åœ¨æ·±åº¦ä¸­å…±äº«æƒé‡ã€‚å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šåªæ˜¯æ„å‘³ç€å°†ä¸åŒçš„äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›å±‚çš„æƒé‡ç»‘å®šåœ¨ä¸€èµ·ï¼Œå› ä¸ºå®ƒä»¬æ˜¯é‡å¤çš„ï¼Œæ‰€ä»¥è¿™æœ€ç»ˆçœ‹èµ·æ¥åƒæ˜¯åœ¨æ·±åº¦ä¸­æ³¨å†Œçš„
    RNã€‚
- en: So this is just at a high level this gives us an architecture that we can apply
    to images but doesn't make any assumptions about image structureã€‚so it's one that
    you can use elsewhere and we basically we give information about the input sort
    of spatial structure by having positional encodings and here we use a 2D4ier feature
    position encoding and just to show you kind of what that looks like here to give
    you a sense so each of the input points is assigned basically so you'llll be some
    position here and we have sinusoidoidal and cosineus sooidial features in 2D so
    this is basically a 4ier8d composition of the position of the 2D input and a couple
    of things that we found where that if we sampled the frequency that's the maximum
    frequency that it's used up to the NOSs frequency of the signal we end up doing
    better than if you use a lower version of this and this basically is because this
    will allow every other point to be aware of every distinctã€‚
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä»é«˜å±‚æ¬¡æ¥çœ‹ï¼Œè¿™ä¸ºæˆ‘ä»¬æä¾›äº†ä¸€ç§å¯ä»¥åº”ç”¨äºå›¾åƒçš„æ¶æ„ï¼Œä½†ä¸å¯¹å›¾åƒç»“æ„åšä»»ä½•å‡è®¾ã€‚å› æ­¤ï¼Œä½ å¯ä»¥åœ¨å…¶ä»–åœ°æ–¹ä½¿ç”¨å®ƒï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šé€šè¿‡ä½¿ç”¨ä½ç½®ç¼–ç æä¾›æœ‰å…³è¾“å…¥ç©ºé—´ç»“æ„çš„ä¿¡æ¯ï¼Œè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨2Då‚…é‡Œå¶ç‰¹å¾ä½ç½®ç¼–ç ã€‚ä¸ºäº†ç»™ä½ ä¸€ä¸ªæ¦‚å¿µï¼Œæ¯ä¸ªè¾“å…¥ç‚¹åŸºæœ¬ä¸Šä¼šè¢«åˆ†é…ä¸€ä¸ªä½ç½®ï¼Œæˆ‘ä»¬åœ¨2Dä¸­ä½¿ç”¨æ­£å¼¦å’Œä½™å¼¦ç‰¹å¾ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯2Dè¾“å…¥ä½ç½®çš„å‚…é‡Œå¶åˆæˆã€‚æˆ‘ä»¬å‘ç°ï¼Œå¦‚æœæˆ‘ä»¬å–æ ·çš„é¢‘ç‡æ˜¯ä¿¡å·çš„æœ€å¤§é¢‘ç‡ï¼Œæˆ‘ä»¬çš„è¡¨ç°ä¼šæ¯”ä½¿ç”¨è¾ƒä½ç‰ˆæœ¬çš„é¢‘ç‡è¦å¥½ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯å› ä¸ºè¿™å°†ä½¿æ¯ä¸ªç‚¹éƒ½èƒ½æ„è¯†åˆ°æ¯ä¸ªä¸åŒçš„ç‚¹ã€‚
- en: Point in the imageï¼Œ whereas if you sample at a lower frequencyï¼Œ you can end
    up with aliasingã€‚and so not all points will be sort of legibleã€‚We also found that
    sampling the spectrum relatively densely tends to help and the contrast here at
    the time we were developing this with respect to Nerf so NF at least in earlier
    implementations used quite a small number of frequency bands we found that the
    more we added the better we did so in general this is something to be attentive
    toã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾åƒä¸­çš„æ¯ä¸ªç‚¹ï¼Œè€Œå¦‚æœä½ åœ¨è¾ƒä½é¢‘ç‡ä¸‹å–æ ·ï¼Œå¯èƒ½ä¼šå‡ºç°æ··å ã€‚å› æ­¤ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰ç‚¹éƒ½ä¼šæ¸…æ™°å¯è¯»ã€‚æˆ‘ä»¬è¿˜å‘ç°ï¼Œè¾ƒå¯†é›†åœ°é‡‡æ ·é¢‘è°±å¾€å¾€æœ‰åŠ©äºå¯¹æ¯”ï¼Œåœ¨æˆ‘ä»¬å¼€å‘è¿™é¡¹æŠ€æœ¯æ—¶ï¼Œä¸Nerfç›¸æ¯”ï¼ŒNerfåœ¨æ—©æœŸå®ç°ä¸­ä½¿ç”¨çš„é¢‘å¸¦æ•°é‡ç›¸å¯¹è¾ƒå°‘ï¼Œæˆ‘ä»¬å‘ç°å¢åŠ é¢‘å¸¦æ•°é‡èƒ½æ˜¾è‘—æé«˜æ•ˆæœï¼Œå› æ­¤ä¸€èˆ¬æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªéœ€è¦å…³æ³¨çš„æ–¹é¢ã€‚
- en: And then finallyï¼Œ as opposed to language where you typically have addition of
    whatever your embedding is with the sinusoidoidal or position encoding that you
    use here we found that concatenating them performed consistently betterã€‚And so
    this may be because the positioning the content embedding is not as sparse as
    it is in languageã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œä¸è¯­è¨€ä¸åŒï¼Œé€šå¸¸ä½ ä¼šå°†åµŒå…¥ä¸æ­£å¼¦æˆ–ä½ç½®ç¼–ç ç›¸åŠ ï¼Œä½†æˆ‘ä»¬å‘ç°è¿æ¥å®ƒä»¬çš„è¡¨ç°ä¸€ç›´æ›´å¥½ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºå†…å®¹åµŒå…¥çš„å®šä½ä¸åƒè¯­è¨€ä¸­é‚£æ ·ç¨€ç–ã€‚
- en: we're not totally sureï¼Œ but this is something that I observed consistentlyã€‚And
    before I move on to results I just want to contrast this to some sort of other
    approaches for using transformers in the image contextã€‚so obvious the obvious
    precedent here is visual transformers and I think this is a this line of work
    is great especially in the image contextã€‚but there are some caveats about it that
    make it less suitable for sort of more general purpose useã€‚
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¹¶ä¸å®Œå…¨ç¡®å®šï¼Œä½†è¿™æ˜¯æˆ‘ä¸€è´¯è§‚å¯Ÿåˆ°çš„äº‹æƒ…ã€‚åœ¨æˆ‘è½¬å‘ç»“æœä¹‹å‰ï¼Œæˆ‘æƒ³å°†å…¶ä¸åœ¨å›¾åƒä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨å˜æ¢å™¨çš„å…¶ä»–ä¸€äº›æ–¹æ³•è¿›è¡Œå¯¹æ¯”ã€‚å› æ­¤ï¼Œè¿™é‡Œæ˜¾è€Œæ˜“è§çš„å…ˆä¾‹æ˜¯è§†è§‰å˜æ¢å™¨ï¼Œæˆ‘è®¤ä¸ºè¿™é¡¹å·¥ä½œåœ¨å›¾åƒä¸Šä¸‹æ–‡ä¸­ç‰¹åˆ«å‡ºè‰²ã€‚ä½†å…³äºå®ƒæœ‰ä¸€äº›é™åˆ¶ï¼Œä½¿å…¶ä¸å¤ªé€‚åˆæ›´é€šç”¨çš„ç”¨é€”ã€‚
- en: so one is that so vision transformers do use an input 2D convolutionã€‚so this
    is often phrase in terms of patches input patches a special case of a 2D transformerã€‚so
    it does restrict the class of inputs you can use it forã€‚And because we're basically
    building this patching or convolution into itã€‚
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè§†è§‰å˜æ¢å™¨ç¡®å®ä½¿ç”¨äº†è¾“å…¥çš„2Då·ç§¯ã€‚è¿™é€šå¸¸è¢«æè¿°ä¸ºè¾“å…¥è¡¥ä¸ï¼Œ2Då˜æ¢å™¨çš„ç‰¹æ®Šæƒ…å†µã€‚å› æ­¤ï¼Œè¿™é™åˆ¶äº†ä½ å¯ä»¥ä½¿ç”¨çš„è¾“å…¥ç±»åˆ«ã€‚è€Œä¸”å› ä¸ºæˆ‘ä»¬åŸºæœ¬ä¸Šå°†è¿™ç§è¡¥ä¸æˆ–å·ç§¯æ„å»ºè¿›æ¥äº†ã€‚
- en: this means that this as an approach really isn't sufficient to get it to work
    on non-gririd dataã€‚there are other ways you could adapt itï¼Œ but this is something
    that you will have to special case for every domain you're looking atã€‚And then
    finallyï¼Œ because we have this sort of input where we're telling the architecture
    what it should look at first in the initial groupingã€‚this does amount to getting
    rid of the nonlocity assumption it's not super clear doing how much doing this
    just once will make a differenceã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€è¿™ç§æ–¹æ³•åœ¨éç½‘æ ¼æ•°æ®ä¸Šå®é™…ä¸Šæ˜¯ä¸å¤Ÿçš„ã€‚è¿˜æœ‰å…¶ä»–æ–¹æ³•å¯ä»¥è°ƒæ•´å®ƒï¼Œä½†è¿™æ˜¯ä½ éœ€è¦ä¸ºæ¯ä¸ªé¢†åŸŸç‰¹åˆ«å¤„ç†çš„äº‹æƒ…ã€‚æœ€åï¼Œç”±äºæˆ‘ä»¬æœ‰è¿™ç§è¾“å…¥ï¼Œå‘Šè¯‰æ¶æ„åœ¨åˆå§‹åˆ†ç»„ä¸­åº”è¯¥é¦–å…ˆå…³æ³¨ä»€ä¹ˆï¼Œè¿™ç¡®å®æ„å‘³ç€æ¶ˆé™¤äº†éå±€éƒ¨æ€§å‡è®¾ï¼Œä½†ä¸å¤ªæ¸…æ¥šè¿™æ ·åšä¸€æ¬¡ä¼šæœ‰å¤šå¤§å·®åˆ«ã€‚
- en: but this is something to be aware of when you're thinking about this architectureã€‚And
    then finally cross attention itself is used quite broadly in the vision literatureã€‚so
    just a couple to highlight a couple of examplesï¼Œ debtorã€‚which is an object detection
    method from Facebookã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™æ˜¯åœ¨è€ƒè™‘è¿™ç§æ¶æ„æ—¶éœ€è¦æ³¨æ„çš„äº‹æƒ…ã€‚æœ€åï¼Œäº¤å‰æ³¨æ„åŠ›åœ¨è§†è§‰æ–‡çŒ®ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚å› æ­¤ï¼Œæˆ‘åªæƒ³å¼ºè°ƒå‡ ä¸ªä¾‹å­ï¼Œæ¯”å¦‚æ¥è‡ª Facebook çš„ä¸€ä¸ªç‰©ä½“æ£€æµ‹æ–¹æ³•ã€‚
- en: basically has a convolutional backbone that's then used to give an output feature
    mapã€‚this is then passed into a transform encoder decoder and of course whenever
    year encoder decoder you think cross attention because from the encoder to the
    decoder there's a cross attention stepã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šæœ‰ä¸€ä¸ªå·ç§¯ä¸»å¹²ï¼Œç„¶åç”¨äºç”Ÿæˆè¾“å‡ºç‰¹å¾å›¾ã€‚è¿™è¢«ä¼ å…¥ä¸€ä¸ªå˜æ¢ç¼–ç å™¨-è§£ç å™¨ï¼Œå½“ç„¶ï¼Œæ¯å½“ä½ æƒ³åˆ°ç¼–ç å™¨-è§£ç å™¨æ—¶ï¼Œå°±ä¼šæƒ³åˆ°äº¤å‰æ³¨æ„åŠ›ï¼Œå› ä¸ºä»ç¼–ç å™¨åˆ°è§£ç å™¨æœ‰ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ­¥éª¤ã€‚
- en: and so they're using basically the cross attention to go from some feature map
    representation to something that looks more like the object bounding boxesã€‚There's
    also quite nice work on learning self-supervised or unsupervised object segmentation
    modelsã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä»–ä»¬åŸºæœ¬ä¸Šä½¿ç”¨äº¤å‰æ³¨æ„åŠ›å°†æŸäº›ç‰¹å¾å›¾è¡¨ç¤ºè½¬æ¢ä¸ºæ›´åƒç‰©ä½“è¾¹ç•Œæ¡†çš„ä¸œè¥¿ã€‚å…³äºå­¦ä¹ è‡ªç›‘ç£æˆ–æ— ç›‘ç£ç‰©ä½“åˆ†å‰²æ¨¡å‹çš„ç ”ç©¶ä¹Ÿç›¸å½“ä¸é”™ã€‚
- en: and in this work they're doing something very similar where they have a convolutional
    backboneã€‚they then use something like these latent the latetnts that we introduce
    hereã€‚to do when they call them slots here but basically to assign some of the
    output pixels to different slots so that they sort of have independent complementary
    decoding of the slots in the segmentation model hereã€‚And there's a lot of other
    thingsã€‚Okayï¼Œ so first I would add just so now I'm going to sort of walk you through
    results of this of this model hi can I oh go ahead I'll go after you go for it
    Okay cool sorry for that can you go back a couple of slides where you had like
    you know like how how like the inputs like flow into like I think one about yeah
    that one okay so two questions that latent transformer is basically like a self
    attention is that correctï¼Ÿ
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œä»–ä»¬åšäº†éå¸¸ç±»ä¼¼çš„äº‹æƒ…ï¼Œä½¿ç”¨äº†ä¸€ä¸ªå·ç§¯ä¸»å¹²ã€‚ä»–ä»¬ç„¶åä½¿ç”¨æˆ‘ä»¬åœ¨è¿™é‡Œå¼•å…¥çš„æ½œåœ¨å˜é‡ï¼Œç§°ä¹‹ä¸ºâ€œæ§½â€ï¼ŒåŸºæœ¬ä¸Šæ˜¯å°†ä¸€äº›è¾“å‡ºåƒç´ åˆ†é…åˆ°ä¸åŒçš„æ§½ï¼Œä»¥ä¾¿åœ¨åˆ†å‰²æ¨¡å‹ä¸­å¯¹æ§½è¿›è¡Œç‹¬ç«‹çš„äº’è¡¥è§£ç ã€‚è¿˜æœ‰å¾ˆå¤šå…¶ä»–å†…å®¹ã€‚å¥½çš„ï¼Œæˆ‘é¦–å…ˆä¼šè¡¥å……ä¸€ä¸‹ï¼Œç°åœ¨æˆ‘å°†å¸¦ä½ ä»¬äº†è§£è¿™ä¸ªæ¨¡å‹çš„ç»“æœã€‚å—¨ï¼Œæˆ‘å¯ä»¥å“¦ï¼Œä½ å…ˆè¯´ï¼Œæˆ‘åœ¨ä½ ä¹‹åã€‚å¥½çš„ï¼ŒæŠ±æ­‰ï¼Œä½ èƒ½å›åˆ°å‡ å¼ å¹»ç¯ç‰‡å—ï¼Ÿä½ çŸ¥é“çš„ï¼Œå°±æ˜¯è¾“å…¥æ˜¯å¦‚ä½•æµå…¥çš„ï¼Œæˆ‘æƒ³æ˜¯é‚£å¼ ï¼Œå¯¹ï¼Œå°±é‚£å¼ ã€‚å¥½çš„ï¼Œæœ‰ä¸¤ä¸ªé—®é¢˜ï¼Œæ½œåœ¨å˜æ¢å™¨åŸºæœ¬ä¸Šæ˜¯è‡ªæ³¨æ„åŠ›ï¼Œå¯¹å—ï¼Ÿ
- en: Yeah so the latent transformer is a self a fully selfational transformer got
    it and why see like for like the key in value they flow directly into the cross
    attention and there is like the query also flowing into it but the latent array
    array is flowing into the cross attention imp parallel to the query can you explain
    thatï¼Ÿ
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ½œåœ¨å˜æ¢å™¨æ˜¯ä¸€ä¸ªå®Œå…¨è‡ªæ³¨æ„åŠ›çš„å˜æ¢å™¨ï¼Œæ˜ç™½äº†ã€‚ä¸ºä»€ä¹ˆåƒå…³é”®å’Œä»·å€¼ç›´æ¥æµå…¥äº¤å‰æ³¨æ„åŠ›ï¼ŒåŒæ—¶æŸ¥è¯¢ä¹Ÿæµå…¥ï¼Œä½†æ½œåœ¨æ•°ç»„æ˜¯å¹¶è¡Œæµå…¥äº¤å‰æ³¨æ„åŠ›çš„ï¼Œèƒ½è§£é‡Šä¸€ä¸‹å—ï¼Ÿ
- en: Yeah so this is just speaking here it's meant to depict the residual connection
    so the cross attention this is sort of a cross attention depicted as a cross attention
    module and so the cross attention itself has the attention it has a residual connection
    and then there's an MLP so that's what that's meant to indicate okay but it's
    basically the QKV standardã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™é‡Œåªæ˜¯æƒ³æè¿°æ®‹å·®è¿æ¥ï¼Œå› æ­¤äº¤å‰æ³¨æ„åŠ›è¢«æç»˜ä¸ºä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›æ¨¡å—ï¼Œäº¤å‰æ³¨æ„åŠ›æœ¬èº«å…·æœ‰æ³¨æ„åŠ›å’Œæ®‹å·®è¿æ¥ï¼Œç„¶åè¿˜æœ‰ä¸€ä¸ª MLPï¼Œè¿™å°±æ˜¯è¿™éƒ¨åˆ†çš„æ„æ€ã€‚å¥½çš„ï¼Œä½†åŸºæœ¬ä¸Šæ˜¯
    QKV æ ‡å‡†ã€‚
- en: Go itã€‚Thanksã€‚ğŸ˜Šï¼ŒHï¼Œ I had a question that is slightly related to this if we can
    just stay up this slide actuallyã€‚so I think one thing that's interesting about
    thisã€‚Sure sorryï¼Œ I lostã€‚You're cutting offã€‚It's mostly consisting of attention
    layersï¼Œ whether it's self attention or creating image transformersã€‚can you hear
    meï¼Œ is that coming throughï¼ŸNoï¼Œ it's cutting offï¼Œ I thinkï¼Œ but I think so recentã€‚Ohã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚è°¢è°¢ã€‚ğŸ˜Šï¼Œæˆ‘æœ‰ä¸€ä¸ªç¨å¾®ç›¸å…³çš„é—®é¢˜ï¼Œå¦‚æœæˆ‘ä»¬å¯ä»¥ç»§ç»­è¿™å¼ å¹»ç¯ç‰‡ã€‚å…³äºè¿™ä¸ªäº‹æƒ…æˆ‘è®¤ä¸ºæœ‰è¶£çš„ä¸€ç‚¹ã€‚æŠ±æ­‰ï¼Œæˆ‘å¤±å»äº†ã€‚ä½ åœ¨æ‰“æ–­æˆ‘ã€‚å®ƒä¸»è¦ç”±æ³¨æ„åŠ›å±‚ç»„æˆï¼Œæ— è®ºæ˜¯è‡ªæ³¨æ„åŠ›è¿˜æ˜¯åˆ›å»ºå›¾åƒå˜æ¢å™¨ã€‚ä½ èƒ½å¬åˆ°æˆ‘å—ï¼Œä¼ è¿‡æ¥äº†æ²¡æœ‰ï¼Ÿä¸ï¼Œä¿¡å·æœ‰ç‚¹å¡é¡¿ï¼Œä½†æˆ‘æƒ³ï¼Œæœ€è¿‘çš„ã€‚å“¦ã€‚
- en: okayã€‚Should I type it is that should I type Yeah I think that's a good idea
    yeahã€‚I'll type it thanks sorryã€‚All rightï¼Œs kindã€‚Feel free to go aheadï¼Œ I'll type
    it slowly andã€‚![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_4.png)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚æˆ‘åº”è¯¥æ‰“å­—å—ï¼Œæ˜¯çš„ï¼Œæˆ‘è§‰å¾—è¿™æ˜¯ä¸ªå¥½ä¸»æ„ã€‚æˆ‘ä¼šæ‰“çš„ï¼Œè°¢è°¢ï¼ŒæŠ±æ­‰ã€‚å¥½çš„ï¼Œéšæ„è¿›è¡Œï¼Œæˆ‘ä¼šæ…¢æ…¢æ‰“ã€‚![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_4.png)
- en: Sounds good sounds good to me Yeah actually can I chime in drew while you're
    on that previous slide the flow so these residual connections I actually didn't
    know the cross attention used that how reliant are these sequential cross attention
    layers on the residual connectionsã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥ä¸é”™ï¼Œå¬èµ·æ¥å¯¹æˆ‘æ¥è¯´ä¹Ÿå¾ˆå¥½ã€‚å®é™…ä¸Šï¼Œæˆ‘å¯ä»¥æ’å˜´å—ï¼Œå¾·é²ï¼Œå½“ä½ åœ¨é‚£å¼ å¹»ç¯ç‰‡ä¸Šæ—¶ï¼Œè¿™äº›æ®‹å·®è¿æ¥æˆ‘å…¶å®ä¸çŸ¥é“äº¤å‰æ³¨æ„åŠ›æ˜¯å¦‚ä½•ä¾èµ–è¿™äº›çš„ï¼Œè¿™äº›åºåˆ—äº¤å‰æ³¨æ„å±‚å¯¹æ®‹å·®è¿æ¥çš„ä¾èµ–ç¨‹åº¦å¦‚ä½•ã€‚
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_6.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_6.png)'
- en: Yeahï¼Œ so hereã€‚Here in the initial so two things I will say is that in the initial
    cross attention it doesn't really make a difference so this is something we've
    ablated when we get to the proIO version of this we also did the same thing in
    the decoder cross attention and it can make some of difference it can make a difference
    there depending on what you're doing I think it's actually essential in when you're
    using repeated cross attention of this way so when you have this sort of iterative
    structure and the reason for this is that the thing that's actually used to condition
    the query is basically that's your full representation of sort of the state of
    the architecture so far and so the skip connection is from it's in basically the
    query channel it's in the late the latent space and so this is basically what
    allows you to end up with this sort of dense dense and stable architectureã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™é‡Œã€‚åœ¨åˆå§‹é˜¶æ®µï¼Œæˆ‘ä¼šè¯´ä¸¤ä»¶äº‹ï¼Œåœ¨åˆå§‹çš„äº¤å‰æ³¨æ„åŠ›ä¸­ï¼Œè¿™å…¶å®å¹¶æ²¡æœ‰ä»€ä¹ˆåŒºåˆ«ã€‚æˆ‘ä»¬åœ¨åˆ¶ä½œproIOç‰ˆæœ¬æ—¶è¿›è¡Œäº†æ¶ˆèå®éªŒï¼Œåœ¨è§£ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ä¸­ä¹Ÿåšäº†åŒæ ·çš„äº‹æƒ…ï¼Œè¿™å¯èƒ½ä¼šæœ‰ä¸€äº›åŒºåˆ«ï¼Œå…·ä½“å–å†³äºä½ åœ¨åšä»€ä¹ˆã€‚æˆ‘è®¤ä¸ºåœ¨ä½¿ç”¨è¿™ç§é‡å¤äº¤å‰æ³¨æ„åŠ›æ—¶ï¼Œå®é™…ä¸Šæ˜¯å¿…éœ€çš„ï¼Œæ‰€ä»¥å½“ä½ æœ‰è¿™ç§è¿­ä»£ç»“æ„æ—¶ï¼ŒåŸå› åœ¨äºï¼Œå®é™…ç”¨äºæ¡ä»¶æŸ¥è¯¢çš„ä¸œè¥¿åŸºæœ¬ä¸Šæ˜¯ä½ ç›®å‰æ¶æ„çŠ¶æ€çš„å®Œæ•´è¡¨ç¤ºï¼Œå› æ­¤è·³è·ƒè¿æ¥æ˜¯åœ¨æŸ¥è¯¢é€šé“ä¸­ï¼Œå¤„äºæ½œåœ¨ç©ºé—´ä¸­ã€‚è¿™åŸºæœ¬ä¸Šä½¿ä½ èƒ½å¤Ÿå¾—åˆ°è¿™ç§å¯†é›†ä¸”ç¨³å®šçš„æ¶æ„ã€‚
- en: Thank youã€‚ğŸ˜Šï¼Œå—¯ã€‚ğŸ˜Šï¼ŒOkayã€‚So to imagenet okay so in standard imagenet processing
    basically we compare against a few so this is a little bit out of date at this
    pointã€‚but against a few sort of just sort of sanity check baselines here so comparing
    against Res 50 and then at the time the best vision transformer model that was
    purely on imagenet and we're definitely in the ballpark this isn't these aren'tã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: è°¢è°¢ã€‚ğŸ˜Šï¼Œå—¯ã€‚ğŸ˜Šï¼Œå¥½çš„ã€‚é‚£ä¹ˆï¼Œå…³äºimagenetï¼Œå®é™…ä¸Šæˆ‘ä»¬åœ¨æ ‡å‡†çš„imagenetå¤„ç†è¿‡ç¨‹ä¸­ï¼ŒåŸºæœ¬ä¸Šæ˜¯æ¯”è¾ƒå‡ ä¸ªåŸºå‡†ï¼Œæ‰€ä»¥è¿™ä¸ªä¿¡æ¯åœ¨ç›®å‰æ¥çœ‹æœ‰ç‚¹è¿‡æ—¶ã€‚ä½†å¯¹æ¯”ä¸€ä¸‹Res
    50ï¼Œä»¥åŠå½“æ—¶çº¯ç²¹åœ¨imagenetä¸Šè¡¨ç°æœ€ä½³çš„è§†è§‰å˜æ¢æ¨¡å‹ï¼Œæˆ‘ä»¬ç»å¯¹åœ¨åˆç†èŒƒå›´å†…ã€‚
- en: Anywhere in your state of the art results but this is an architecture that again
    it' not using any 2D convolutions and so the fact that it was able to do this
    well we found veryã€‚very surprising at the time one of the quite cool things about
    this is that because this architecture is not making any assumptions the architecture
    itself isn't making any assumptions about the spatial structure of of the input
    images we can look at permuted imagenet and in the first version of this what
    we do is basically we compute the features using the 2D position so the 2D position
    is sort of fixed to a position to the pixel and then we just shuffle it all and
    so this is basically we'll give you a sense of how dependent the baselines are
    on input image structure and so if we look at if we look at the transformer perceive
    by construction they don't change so this is not an empirical finding this is
    a property of the models but we find that Resnet 50 falls the performance falls
    by about half and VIT which again only has one layer where it's relying on the
    spatial structureã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ æœ€å…ˆè¿›çš„ç»“æœä¸­ï¼Œä½†è¿™æ˜¯ä¸€ä¸ªæ¶æ„ï¼Œå®ƒåŒæ ·æ²¡æœ‰ä½¿ç”¨ä»»ä½•äºŒç»´å·ç§¯ï¼Œå› æ­¤èƒ½å¤Ÿåšåˆ°è¿™ä¸€ç‚¹è®©æˆ‘ä»¬æ„Ÿåˆ°éå¸¸æƒŠè®¶ã€‚è¿™ä¸ªæ¶æ„æœ‰ä¸€ä¸ªå¾ˆé…·çš„åœ°æ–¹æ˜¯ï¼Œå› ä¸ºå®ƒæ²¡æœ‰å¯¹è¾“å…¥å›¾åƒçš„ç©ºé—´ç»“æ„åšå‡ºä»»ä½•å‡è®¾ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ç½®æ¢çš„Imagenetï¼Œåœ¨è¿™ä¸ªæ¶æ„çš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬ä¸­ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šä½¿ç”¨äºŒç»´ä½ç½®è®¡ç®—ç‰¹å¾ï¼Œå› æ­¤äºŒç»´ä½ç½®æ˜¯å›ºå®šåœ¨åƒç´ ä¸Šçš„ï¼Œç„¶åæˆ‘ä»¬å°†å…¶å…¨éƒ¨æ´—ç‰Œï¼Œè¿™åŸºæœ¬ä¸Šä¼šè®©ä½ æ„Ÿå—åˆ°åŸºçº¿å¯¹è¾“å…¥å›¾åƒç»“æ„çš„ä¾èµ–æ€§ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬è§‚å¯Ÿå˜æ¢å™¨ï¼Œç”±äºæ„é€ åŸå› ï¼Œå®ƒä»¬ä¸ä¼šæ”¹å˜ï¼Œå› æ­¤è¿™ä¸æ˜¯ä¸€ä¸ªç»éªŒå‘ç°ï¼Œè€Œæ˜¯æ¨¡å‹çš„ä¸€ä¸ªç‰¹æ€§ï¼Œä½†æˆ‘ä»¬å‘ç°Resnet
    50çš„æ€§èƒ½ä¸‹é™äº†å¤§çº¦ä¸€åŠï¼Œè€ŒVITåªæœ‰ä¸€å±‚ä¾èµ–äºç©ºé—´ç»“æ„ã€‚
- en: Also it has about a 15 point drop and so this suggests that it's relying quite
    a lot on that very first one to give it some information about the structureã€‚We
    can push this a little bit by instead of relying on 2D Fourier features learning
    completely learned positional encodings and this basically this is an architecture
    now this is a model that has absolutely no information about the input structure
    and so shuffling them and learning them again is absolutely equivalent and we
    find that this architecture also can be pushed about 70% and we've gotten slightly
    better numbers here in general this seems to work worse but so the 2D information
    is useful but it's quite cool that you can get what would have been numbers comparable
    to state of the art about5 or six years ago so this is quite coolã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œå®ƒå¤§çº¦æœ‰15ä¸ªç‚¹çš„ä¸‹é™ï¼Œè¿™è¡¨æ˜å®ƒåœ¨ç›¸å½“ç¨‹åº¦ä¸Šä¾èµ–äºç¬¬ä¸€ä¸ªä»¥æä¾›æœ‰å…³ç»“æ„çš„ä¸€äº›ä¿¡æ¯ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä¸ä¾èµ–äºäºŒç»´å‚…é‡Œå¶ç‰¹å¾è€Œå®Œå…¨å­¦ä¹ ä½ç½®ç¼–ç æ¥æ¨åŠ¨è¿™ä¸€ç‚¹ï¼ŒåŸºæœ¬ä¸Šè¿™æ˜¯ä¸€ä¸ªç°åœ¨å®Œå…¨æ²¡æœ‰å…³äºè¾“å…¥ç»“æ„ä¿¡æ¯çš„æ¶æ„ï¼Œæ‰€ä»¥æ´—ç‰Œå¹¶å†æ¬¡å­¦ä¹ æ˜¯å®Œå…¨ç­‰æ•ˆçš„ï¼Œæˆ‘ä»¬å‘ç°è¿™ä¸ªæ¶æ„ä¹Ÿèƒ½æ¨åŠ¨å¤§çº¦70%ï¼Œè€Œä¸”æˆ‘ä»¬åœ¨è¿™é‡Œå¾—åˆ°äº†ç¨å¾®æ›´å¥½çš„æ•°å­—ï¼Œæ€»ä½“æ¥è¯´ï¼Œè¿™ä¼¼ä¹æ•ˆæœæ›´å·®ï¼Œä½†äºŒç»´ä¿¡æ¯æ˜¯æœ‰ç”¨çš„ï¼Œèƒ½å¤Ÿå¾—åˆ°äº”å…­å¹´å‰ä¸å½“æ—¶æœ€å…ˆè¿›æŠ€æœ¯ç›¸å½“çš„æ•°å­—ï¼ŒçœŸæ˜¯å¾ˆé…·ã€‚
- en: Sorry I'm a little thick hereï¼Œ you're saying the difference between the last
    two rows is that the second to last row has a two dimensional position of betdding
    and the last one has a one dimensional position of betdding essentiallyã€‚as I rightã€‚So
    it's learnedï¼Œ so it's basically it'll be it's I believe a 256 dimensional vector
    that's learnedã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: æŠ±æ­‰ï¼Œæˆ‘æœ‰ç‚¹è¿Ÿé’ï¼Œä½ æ˜¯è¯´æœ€åä¸¤è¡Œä¹‹é—´çš„åŒºåˆ«åœ¨äºå€’æ•°ç¬¬äºŒè¡Œæœ‰ä¸€ä¸ªäºŒç»´çš„æŠ•æ³¨ä½ç½®ï¼Œè€Œæœ€åä¸€è¡Œæœ¬è´¨ä¸Šæœ‰ä¸€ä¸ªä¸€ç»´çš„æŠ•æ³¨ä½ç½®ã€‚æˆ‘è¯´å¾—å¯¹å—ï¼Ÿæ‰€ä»¥å®ƒæ˜¯è¢«å­¦ä¹ çš„ï¼ŒåŸºæœ¬ä¸Šå®ƒä¼šæ˜¯æˆ‘ç›¸ä¿¡çš„ä¸€ä¸ª256ç»´çš„å‘é‡ã€‚
- en: but it doesn't it basically it means that the model itself has no information
    about the input's spatial structureã€‚So the 2D positional encodings that we're
    using you end up having about 200ã€‚it's 200 some features depending on what you're
    looking atã€‚but theyre they give you very detailed information about the 2D structure
    of the input because they're based on a 480 composition of the input spaceã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®ƒçš„æ„æ€åŸºæœ¬ä¸Šæ˜¯æ¨¡å‹æœ¬èº«å¯¹è¾“å…¥çš„ç©ºé—´ç»“æ„æ²¡æœ‰ä»»ä½•ä¿¡æ¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„äºŒç»´ä½ç½®ç¼–ç æœ€ç»ˆå¤§çº¦æœ‰200ä¸ªã€‚è¿™æ˜¯200å¤šä¸ªç‰¹å¾ï¼Œå…·ä½“å–å†³äºä½ åœ¨çœ‹ä»€ä¹ˆï¼Œä½†å®ƒä»¬èƒ½ç»™ä½ éå¸¸è¯¦ç»†çš„ä¿¡æ¯ï¼Œå…³äºè¾“å…¥çš„äºŒç»´ç»“æ„ï¼Œå› ä¸ºå®ƒä»¬æ˜¯åŸºäºè¾“å…¥ç©ºé—´çš„480ä¸ªç»„æˆã€‚
- en: Okayï¼Œ that makes senseï¼Œ thank youã€‚Hi drew can I ask a question about frequency
    you use to generate those sensor waveã€‚Yeahï¼Œ so like a cover slide beforeã€‚sã€‚![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_8.png)
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œè¿™æœ‰é“ç†ï¼Œè°¢è°¢ã€‚å—¨ï¼Œå¾·é²ï¼Œæˆ‘å¯ä»¥é—®ä¸€ä¸ªå…³äºä½ ç”¨æ¥ç”Ÿæˆé‚£äº›ä¼ æ„Ÿå™¨æ³¢çš„é¢‘ç‡çš„é—®é¢˜å—ï¼Ÿæ˜¯çš„ï¼Œå°±åƒä¹‹å‰çš„å°é¢å¹»ç¯ç‰‡ã€‚![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_8.png)
- en: Yeahï¼ŒYeahï¼Œ yeahã€‚ yeahã€‚they likeã€‚So basicallyï¼Œ I do have taken some lecture in
    signal processingã€‚And I know if Iã€‚![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_10.png)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ˜¯çš„ï¼Œæ²¡é”™ã€‚ä»–ä»¬å–œæ¬¢ã€‚æ‰€ä»¥åŸºæœ¬ä¸Šï¼Œæˆ‘ç¡®å®åœ¨ä¿¡å·å¤„ç†æ–¹é¢ä¸Šè¿‡ä¸€äº›è¯¾ã€‚æˆ‘çŸ¥é“å¦‚æœæˆ‘ã€‚![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_10.png)
- en: Want to avoidï¼Œ you knowï¼Œ alienencyã€‚I need to sample withï¼Œ at least my frequencyã€‚So
    I'm curious to know why to use frequency starting from one to the ni frequency
    instead of starting from micro frequency to some very high frequencyã€‚OhI see so
    basically so the maximum frequency that's used is always NquiIS so anything about
    Nquist is going to be alias so you're not actually going to be able to resolve
    it because it's in pixel space right so we sample one is basically just giving
    you an oscillation that covers the entire image and so this is basically just
    to sample the full range of nonallias frequenciesã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: æƒ³è¦é¿å…ï¼Œä½ çŸ¥é“çš„ï¼Œåˆ«æ‰­ã€‚æˆ‘éœ€è¦è‡³å°‘æŒ‰æˆ‘çš„é¢‘ç‡è¿›è¡Œé‡‡æ ·ã€‚å› æ­¤ï¼Œæˆ‘å¾ˆå¥½å¥‡ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ä»1åˆ°å¥ˆå¥æ–¯ç‰¹é¢‘ç‡çš„é¢‘ç‡ï¼Œè€Œä¸æ˜¯ä»å¾®é¢‘ç‡å¼€å§‹åˆ°éå¸¸é«˜çš„é¢‘ç‡ã€‚å“¦ï¼Œæˆ‘æ˜ç™½äº†ï¼Œæ‰€ä»¥ä½¿ç”¨çš„æœ€å¤§é¢‘ç‡æ€»æ˜¯å¥ˆå¥æ–¯ç‰¹ï¼Œå› æ­¤ä»»ä½•è¶…è¿‡å¥ˆå¥æ–¯ç‰¹çš„é¢‘ç‡éƒ½ä¼šäº§ç”Ÿæ··å ï¼Œå› æ­¤å®é™…ä¸Šæ— æ³•è§£æï¼Œå› ä¸ºå®ƒæ˜¯åœ¨åƒç´ ç©ºé—´ä¸­ã€‚å› æ­¤ï¼Œé‡‡æ ·1åŸºæœ¬ä¸Šåªæ˜¯ç»™ä½ ä¸€ä¸ªè¦†ç›–æ•´ä¸ªå›¾åƒçš„æŒ¯è¡ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯ä¸ºäº†é‡‡æ ·éæ··å é¢‘ç‡çš„å…¨èŒƒå›´ã€‚
- en: å•Š okayï¼Œ coolã€‚thank youã€‚goodã€‚Yeahã€‚Okayï¼Œ so so after after the image resultsã€‚we
    wanted to try it on other domains and in particular we were interested in how
    this could be used to work on sort of multimodal domainsã€‚so ones combining various
    different types of input features andã€‚one challenge or one sort of problem that
    you encounter in these sorts of spaces is that the data from different modalities
    end up having different features and they always have different semantics so if
    you take the positional encoding plus the RGB for video you end up with some number
    of channels and that if you have audio that corresponds the data may be paired
    but it tends to have fewer features and it only has a1D positional encoding so
    the way that we handle this is basically by learning modality specific position
    encodings and so these are basically embeddings that are special and learned for
    each of the modalities and what this does is basically tags ends up tagging the
    features that come from audio or video with some information that the network
    can learn that allows it to distinguish which ones which but given these these
    padded these sort of learned padded feature vectors we then concatenate them all
    and that's how we process multimobile data so basically the input to the architecture
    still looks like just one big array it's just that when constructing this we know
    that some of those features some of the rows and thatã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: å•Šï¼Œå¥½å§ï¼Œé…·ã€‚è°¢è°¢ã€‚å¾ˆå¥½ã€‚æ˜¯çš„ã€‚å¥½çš„ï¼Œæ¥ä¸‹æ¥åœ¨å›¾åƒç»“æœä¹‹åã€‚æˆ‘ä»¬æƒ³å°è¯•åœ¨å…¶ä»–é¢†åŸŸï¼Œå°¤å…¶æ˜¯æˆ‘ä»¬å¯¹å¦‚ä½•åœ¨å¤šæ¨¡æ€é¢†åŸŸä¸­åº”ç”¨è¿™é¡¹æŠ€æœ¯æ„Ÿå…´è¶£ã€‚ä¸€ä¸ªæŒ‘æˆ˜æˆ–åœ¨è¿™äº›é¢†åŸŸä¸­é‡åˆ°çš„é—®é¢˜æ˜¯ï¼Œæ¥è‡ªä¸åŒæ¨¡æ€çš„æ•°æ®æœ€ç»ˆä¼šæœ‰ä¸åŒçš„ç‰¹å¾ï¼Œä¸”è¯­ä¹‰æ€»æ˜¯ä¸åŒã€‚å› æ­¤ï¼Œå¦‚æœä½ å°†ä½ç½®ç¼–ç åŠ ä¸Šè§†é¢‘çš„RGBï¼Œä½ ä¼šå¾—åˆ°ä¸€å®šæ•°é‡çš„é€šé“ï¼Œè€Œå¦‚æœä½ æœ‰å¯¹åº”çš„éŸ³é¢‘ï¼Œæ•°æ®å¯èƒ½æ˜¯æˆå¯¹çš„ï¼Œä½†ç‰¹å¾å¾€å¾€è¾ƒå°‘ï¼Œä¸”åªæœ‰1Dä½ç½®ç¼–ç ã€‚æˆ‘ä»¬å¤„ç†è¿™ç§æƒ…å†µçš„æ–¹å¼åŸºæœ¬ä¸Šæ˜¯å­¦ä¹ æ¨¡æ€ç‰¹å®šçš„ä½ç½®ç¼–ç ï¼Œè¿™äº›ç¼–ç æ˜¯ä¸“é—¨ä¸ºæ¯ç§æ¨¡æ€å­¦ä¹ çš„åµŒå…¥ã€‚è¿™åŸºæœ¬ä¸Šä¸ºæ¥è‡ªéŸ³é¢‘æˆ–è§†é¢‘çš„ç‰¹å¾æ‰“ä¸Šæ ‡ç­¾ï¼Œä½¿ç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å¹¶åŒºåˆ†å®ƒä»¬ã€‚é‰´äºè¿™äº›å¡«å……çš„ç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬éšåå°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¤„ç†å¤šæ¨¡æ€æ•°æ®çš„æ–¹å¼ã€‚å› æ­¤ï¼Œæ¶æ„çš„è¾“å…¥çœ‹èµ·æ¥ä»ç„¶åƒä¸€ä¸ªå¤§æ•°ç»„ï¼Œåªæ˜¯æ„å»ºæ—¶æˆ‘ä»¬çŸ¥é“å…¶ä¸­ä¸€äº›ç‰¹å¾å’Œè¡Œæ˜¯è¿™æ ·ã€‚
- en: Aray come from video and some come from audioï¼Œ but the model itself isn't given
    information about that other than what it learnsã€‚We also have some great questionsï¼Œ
    so can go firstã€‚return turnï¼ŸYeahï¼Œ sorryã€‚ I thought it'sã€‚but yeahã€‚yeahï¼Œ sureã€‚if
    you can hear me it's just a reasonã€‚Like starting a lot transformer stuff formallyã€‚so
    I just didn't know what position one that wasã€‚Ohï¼Œ so what a position we thatï¼ŸYesã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Arayæ¥æºäºè§†é¢‘ï¼Œæœ‰äº›æ¥è‡ªéŸ³é¢‘ï¼Œä½†æ¨¡å‹æœ¬èº«å¹¶æ²¡æœ‰å…³äºè¿™äº›çš„å…·ä½“ä¿¡æ¯ï¼Œé™¤äº†å®ƒæ‰€å­¦ä¹ çš„å†…å®¹ã€‚æˆ‘ä»¬è¿˜æœ‰ä¸€äº›å¾ˆå¥½çš„é—®é¢˜ï¼Œæ‰€ä»¥å¯ä»¥å…ˆé—®ã€‚è¿”å›è½¬å‘ï¼Ÿæ˜¯çš„ï¼ŒæŠ±æ­‰ã€‚æˆ‘ä»¥ä¸ºæ˜¯è¿™æ ·ï¼Œä½†æ²¡é”™ï¼Œå½“ç„¶ã€‚å¦‚æœä½ èƒ½å¬åˆ°æˆ‘ï¼Œè¿™åªæ˜¯ä¸€ä¸ªåŸå› ã€‚å°±åƒæ­£å¼å¼€å§‹å¾ˆå¤šå˜æ¢çš„ä¸œè¥¿ã€‚æˆ‘åªæ˜¯ä¸çŸ¥é“é‚£æ˜¯å“ªä¸ªä½ç½®ã€‚å“¦ï¼Œé‚£æˆ‘ä»¬çš„ä½ç½®æ˜¯ä»€ä¹ˆï¼Ÿæ˜¯çš„ã€‚
- en: so basically a positional embedding is it's a feature that says this so the
    simplest way to think about it is in text so text the input is 1D so things live
    in some 1D sequence and for each point there you feurize where it's located in
    that sequence so the simplest thing to do would be if you have negative one to
    one is the full range it's just denotes actually where it's located in that sequence
    but we typically will add sort of will want to featureurize this to have more
    dimensions than just a single one and so the Fourier trans the Fourier decomposition
    is one way to do this to sort of give it privileged information about the high
    frequency structure but we can also just use the position to index on some embedding
    array which is how we do it when morere learning things so basically it's just
    a set of weights that are added to the feature for that point that give the network
    information about where it's located in the groundary sequenceã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åŸºæœ¬ä¸Šï¼Œä½ç½®åµŒå…¥æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œè¡¨ç¤ºè¿™ä¸ªä¸œè¥¿ï¼Œæœ€ç®€å•çš„æ€è€ƒæ–¹å¼æ˜¯åœ¨æ–‡æœ¬ä¸­ï¼Œæ‰€ä»¥æ–‡æœ¬çš„è¾“å…¥æ˜¯ä¸€ç»´çš„ï¼Œäº‹ç‰©ç”Ÿæ´»åœ¨æŸä¸ªä¸€ç»´åºåˆ—ä¸­ï¼Œå¯¹äºæ¯ä¸ªç‚¹ï¼Œä½ ä¼šè¡¨ç¤ºå®ƒåœ¨é‚£ä¸ªåºåˆ—ä¸­çš„ä½ç½®ã€‚æœ€ç®€å•çš„æ–¹æ³•æ˜¯å¦‚æœä½ æœ‰è´Ÿä¸€åˆ°ä¸€çš„å®Œæ•´èŒƒå›´ï¼Œå®ƒå®é™…ä¸Šè¡¨ç¤ºå®ƒåœ¨é‚£ä¸ªåºåˆ—ä¸­çš„ä½ç½®ï¼Œä½†æˆ‘ä»¬é€šå¸¸ä¼šæƒ³è¦ç‰¹å¾åŒ–ï¼Œä½¿å…¶ç»´åº¦è¶…è¿‡å•ä¸€ç»´åº¦ï¼Œå› æ­¤å‚…é‡Œå¶å˜æ¢ï¼Œå‚…é‡Œå¶åˆ†è§£æ˜¯ä¸€ç§æ–¹æ³•ï¼Œå¯ä»¥æä¾›å…³äºé«˜é¢‘ç»“æ„çš„ç‰¹æƒä¿¡æ¯ï¼Œä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥ä»…ä½¿ç”¨ä½ç½®æ¥ç´¢å¼•æŸä¸ªåµŒå…¥æ•°ç»„ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬åœ¨æ›´å¤šå­¦ä¹ ä¸­æ‰€åšçš„ï¼ŒåŸºæœ¬ä¸Šå®ƒåªæ˜¯ä¸€ä¸ªæƒé‡é›†ï¼Œæ·»åŠ åˆ°è¯¥ç‚¹çš„ç‰¹å¾ä¸Šï¼Œç»™ç½‘ç»œæä¾›å…³äºå®ƒåœ¨åŸºç¡€åºåˆ—ä¸­ä½ç½®çš„ä¿¡æ¯ã€‚
- en: If you want to go nextï¼Ÿå—¯ã€‚Sorry I had to find a mute button unmute button Okay
    so I actually have two questions regarding the fewer features the I think like
    do you guys sample them like uniformly or are they like do you like to learn theseï¼Ÿ
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³ç»§ç»­ï¼Ÿå—¯ï¼ŒæŠ±æ­‰ï¼Œæˆ‘å¾—æ‰¾ä¸ªé™éŸ³æŒ‰é’®ã€‚è§£é™¤é™éŸ³ã€‚å¥½çš„ï¼Œæˆ‘å®é™…ä¸Šæœ‰ä¸¤ä¸ªé—®é¢˜ï¼Œå…³äºç‰¹å¾è¾ƒå°‘çš„éƒ¨åˆ†ï¼Œæˆ‘æƒ³ä½ ä»¬æ˜¯å‡åŒ€é‡‡æ ·è¿˜æ˜¯è¯´ä½ ä»¬å–œæ¬¢å­¦ä¹ è¿™äº›ç‰¹å¾ï¼Ÿ
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_12.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_12.png)'
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_13.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_13.png)'
- en: Yeahï¼Œ so basically we sample them linearly so basically we take the full space
    and we sample them linearly with whatever the budget is there are so in various
    settings we have actually tried learning these so you could actually initialize
    an array with them and then learn them and that does help sometimes actually and
    you could potentially learn you could try a more sophisticated strategy on this
    tooã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæ‰€ä»¥åŸºæœ¬ä¸Šæˆ‘ä»¬çº¿æ€§åœ°å¯¹å®ƒä»¬è¿›è¡Œé‡‡æ ·ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬å–å®Œæ•´ç©ºé—´ï¼Œå¹¶ç”¨é¢„ç®—çº¿æ€§é‡‡æ ·ï¼Œåœ¨å„ç§è®¾ç½®ä¸­ï¼Œæˆ‘ä»¬å®é™…ä¸Šå°è¯•å­¦ä¹ è¿™äº›ï¼Œæ‰€ä»¥ä½ å¯ä»¥åˆå§‹åŒ–ä¸€ä¸ªæ•°ç»„ï¼Œç„¶åå­¦ä¹ å®ƒï¼Œæœ‰æ—¶ç¡®å®æœ‰å¸®åŠ©ï¼Œä½ ä¹Ÿå¯ä»¥å°è¯•æ›´å¤æ‚çš„ç­–ç•¥ã€‚
- en: Okay cool my follow up question is that basically I feel like the selling point
    of your research right is that you don't make any structural assumptions right
    you can take any type of like format however for like the encoding wouldn't the
    dimensionality so for example like if it's text it's 1D right if it's a like an
    image it will be 2D and if it's like a video like 3D you have like moreã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œé…·ï¼Œæˆ‘çš„åç»­é—®é¢˜æ˜¯ï¼Œæˆ‘è§‰å¾—ä½ çš„ç ”ç©¶çš„å–ç‚¹åŸºæœ¬ä¸Šæ˜¯ä½ ä¸åšä»»ä½•ç»“æ„å‡è®¾ï¼Œå¯¹å§ï¼Ÿä½ å¯ä»¥æ¥å—ä»»ä½•æ ¼å¼ï¼Œä½†æ˜¯å¯¹äºç¼–ç æ¥è¯´ï¼Œç»´åº¦ä¸ä¼šå½±å“å—ï¼Ÿä¾‹å¦‚ï¼Œå¦‚æœæ˜¯æ–‡æœ¬å°±æ˜¯ä¸€ç»´ï¼Œå¯¹å§ï¼Ÿå¦‚æœæ˜¯å›¾åƒå°±æ˜¯äºŒç»´ï¼Œè€Œå¦‚æœæ˜¯è§†é¢‘å°±æ˜¯ä¸‰ç»´ï¼Œä½ ä¼šæœ‰æ›´å¤šçš„ç»´åº¦ã€‚
- en: Like the positional encoding will have like more points right wouldn't that
    like inherently give awayã€‚Like the nature of the inputã€‚Yeah so it does so I completely
    agree with this you're totally right the version of this where we have learned
    position encodings is the most pure from that point of viewã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: å°±æ˜¯ä½ç½®ç¼–ç ä¼šæœ‰æ›´å¤šçš„ç‚¹ï¼Œå¯¹å§ï¼Ÿè¿™ä¸ä¼šå›ºæœ‰åœ°é€éœ²è¾“å…¥çš„æœ¬è´¨å—ï¼Ÿæ˜¯çš„ï¼Œç¡®å®å¦‚æ­¤ï¼Œæˆ‘å®Œå…¨åŒæ„ï¼Œä½ è¯´å¾—å¯¹ï¼Œä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œæˆ‘ä»¬å­¦ä¹ ä½ç½®ç¼–ç çš„ç‰ˆæœ¬æ˜¯æœ€çº¯ç²¹çš„ã€‚
- en: so it's one that gives it basically no information about the ground truth spatial
    structure what it does give the model so when you do the learned position encoding
    it will say that for example there's a correspondence between point K on image1
    and point K on image 2 so that's basically the least amount of information you
    can give it while still allowing it to sort of figure out what the structural
    relationship between the input points is so this is the direction that we've been
    trying to push in in general giving the architecture access to sort of ground
    truth structural information like this lives on this point in 2D is helpful so
    there's a couple things here there's sort of from a practical point of view if
    you want good results you need to exploit these things or it's helpful to exploit
    these things but we do want to move in the direction where we're relying on these
    things lessã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™åŸºæœ¬ä¸Šç»™äº†å®ƒå…³äºçœŸå®ç©ºé—´ç»“æ„çš„ä¿¡æ¯éå¸¸æœ‰é™ã€‚å®ƒç»™æ¨¡å‹çš„æ˜¯ä»€ä¹ˆå‘¢ï¼Œå½“ä½ è¿›è¡Œå­¦ä¹ çš„ä½ç½®ç¼–ç æ—¶ï¼Œå®ƒä¼šè¯´ï¼Œæ¯”å¦‚å›¾åƒ1ä¸Šçš„ç‚¹Kå’Œå›¾åƒ2ä¸Šçš„ç‚¹Kä¹‹é—´å­˜åœ¨å¯¹åº”å…³ç³»ã€‚å› æ­¤ï¼Œè¿™åŸºæœ¬ä¸Šæ˜¯ä½ èƒ½ç»™å®ƒçš„æœ€å°‘ä¿¡æ¯ï¼ŒåŒæ—¶ä»ç„¶å…è®¸å®ƒç†è§£è¾“å…¥ç‚¹ä¹‹é—´çš„ç»“æ„å…³ç³»ã€‚è¿™æ˜¯æˆ‘ä»¬ä¸€ç›´åœ¨æ¨åŠ¨çš„æ–¹å‘ï¼Œé€šå¸¸è®©æ¶æ„è®¿é—®è¿™æ ·çš„çœŸå®ç»“æ„ä¿¡æ¯ï¼Œæ¯”å¦‚è¿™ä¸ªç‚¹åœ¨äºŒç»´ä¸Šçš„ä½ç½®æ˜¯æœ‰å¸®åŠ©çš„ã€‚æ‰€ä»¥è¿™é‡Œæœ‰å‡ ä»¶äº‹ï¼Œä»å®é™…çš„è§’åº¦æ¥çœ‹ï¼Œå¦‚æœä½ æƒ³è¦å¥½çš„ç»“æœï¼Œä½ éœ€è¦åˆ©ç”¨è¿™äº›ä¸œè¥¿ï¼Œåˆ©ç”¨è¿™äº›ä¸œè¥¿æ˜¯æœ‰å¸®åŠ©çš„ï¼Œä½†æˆ‘ä»¬ç¡®å®æƒ³æœç€å‡å°‘ä¾èµ–è¿™äº›ä¸œè¥¿çš„æ–¹å‘å‘å±•ã€‚
- en: And so this is basically something we're actively looking intoã€‚Okay makes senseï¼Œ
    thank youã€‚So I think has posted her question on the chatã€‚I also see you have your
    hand raisedï¼Œ so if you wantã€‚you can give it a try if not I'll read out the questionã€‚Okayã€‚I'll
    try just let me know if it's choppyã€‚Yeah so is it good right now so far so Oh
    good okay cool so I was curious looking at the perceive diagram you had it's a
    bunch of attention layers right like cross attention and self attention and I
    think there's been this like small trend in recent work in vision transformers
    to try to sort of replace the last few layers instead of having attention like
    make them be convolutions to address this attention scaling problem right in a
    different manner and so here like the perceive architecture is trying to make
    self attention less expensive and they they're just trying to replace it and they
    kind of just avoid the problemã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™åŸºæœ¬ä¸Šæ˜¯æˆ‘ä»¬æ­£åœ¨ç§¯æç ”ç©¶çš„å†…å®¹ã€‚å¥½çš„ï¼Œæ˜ç™½äº†ï¼Œè°¢è°¢ã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºå¥¹åœ¨èŠå¤©ä¸­å‘å¸ƒäº†å¥¹çš„é—®é¢˜ã€‚æˆ‘ä¹Ÿçœ‹åˆ°ä½ ä¸¾äº†æ‰‹ï¼Œæ‰€ä»¥å¦‚æœä½ æƒ³çš„è¯ï¼Œå¯ä»¥è¯•è¯•ï¼Œå¦‚æœä¸æƒ³çš„è¯ï¼Œæˆ‘ä¼šè¯»å‡ºé—®é¢˜ã€‚å¥½çš„ï¼Œæˆ‘ä¼šå°è¯•ï¼Œå‘Šè¯‰æˆ‘å¦‚æœæœ‰ç‚¹å¡é¡¿ã€‚æ˜¯çš„ï¼Œæ‰€ä»¥ç°åœ¨è¿˜ä¸é”™å—ï¼Ÿå“¦ï¼Œå¥½ï¼Œå¤ªå¥½äº†ï¼Œæˆ‘å¾ˆå¥½å¥‡çœ‹ç€ä½ æåˆ°çš„æ„ŸçŸ¥å›¾ï¼Œå®ƒæ˜¯å¾ˆå¤šæ³¨æ„åŠ›å±‚ï¼Œå¯¹å§ï¼Œæ¯”å¦‚äº¤å‰æ³¨æ„åŠ›å’Œè‡ªæ³¨æ„åŠ›ã€‚æˆ‘è§‰å¾—æœ€è¿‘çš„è§†è§‰å˜å‹å™¨å·¥ä½œä¸­æœ‰ä¸€ç§å°è¶‹åŠ¿ï¼Œè¯•å›¾ç”¨å·ç§¯æ›¿ä»£æœ€åå‡ ä¸ªå±‚ï¼Œè€Œä¸æ˜¯ä½¿ç”¨æ³¨æ„åŠ›ï¼Œä»¥ä¸åŒçš„æ–¹å¼è§£å†³è¿™ä¸ªæ³¨æ„åŠ›æ‰©å±•é—®é¢˜ã€‚è¿™é‡Œçš„æ„ŸçŸ¥æ¶æ„æ­£è¯•å›¾è®©è‡ªæ³¨æ„åŠ›æˆæœ¬æ›´ä½ï¼Œä»–ä»¬åªæ˜¯æƒ³æ›¿ä»£å®ƒï¼Œå¹¶ä¸”é¿å…äº†è¿™ä¸ªé—®é¢˜ã€‚
- en: And so I'm curious and so I've seen papers both ways like some that try to do
    things like the ones you cited and then some that are trying to do this as well
    and in my mind everyone always has like the good results and stuff so I'm curious
    if you think there's a reason to do one or the other or if you think this alternative
    approach is also promising or is there a reason you know research should go in
    one direction or the otherã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å¾ˆå¥½å¥‡ï¼Œæˆ‘çœ‹åˆ°æœ‰äº›è®ºæ–‡æ˜¯ä¸¤ç§æ–¹å¼çš„ï¼Œæœ‰äº›è¯•å›¾åšä½ æåˆ°çš„é‚£æ ·ï¼Œè¿˜æœ‰ä¸€äº›ä¹Ÿåœ¨å°è¯•è¿™æ ·åšã€‚åœ¨æˆ‘çœ‹æ¥ï¼Œæ¯ä¸ªäººæ€»æ˜¯æœ‰å¥½çš„ç»“æœç­‰ç­‰ã€‚æ‰€ä»¥æˆ‘å¥½å¥‡ä½ æ˜¯å¦è®¤ä¸ºæœ‰ç†ç”±å»åšå…¶ä¸­ä¸€ç§ï¼Œè¿˜æ˜¯ä½ è§‰å¾—è¿™ç§æ›¿ä»£æ–¹æ³•ä¹Ÿå¾ˆæœ‰å‰æ™¯ï¼Œæˆ–è€…æœ‰æ²¡æœ‰ç†ç”±ç ”ç©¶åº”è¯¥æœç€æŸä¸ªæ–¹å‘å‘å±•ã€‚
- en: Yeahï¼Œ so to my mindï¼Œ the big tradeoff is one betweenã€‚so the vision literature
    I think it's just exploded in terms of these sort of hybrids and people the write
    the exact right place on the Pato curve for the tradeoff of speed and performanceã€‚but
    they're basically looking primarily on vision specific problemsã€‚so something the
    computer vision community itself typically doesn't regularize itself away from
    things that don't work on things that aren't visionã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œåœ¨æˆ‘çœ‹æ¥ï¼Œä¸»è¦çš„æƒè¡¡æ˜¯ä¸€ä¸ªâ€¦â€¦è§†è§‰æ–‡çŒ®æˆ‘è®¤ä¸ºåœ¨è¿™äº›æ··åˆä½“æ–¹é¢å·²ç»çˆ†ç‚¸æ€§å¢é•¿ï¼Œäººä»¬å†™åœ¨Patoæ›²çº¿ä¸Šé€Ÿåº¦å’Œæ€§èƒ½çš„æœ€ä½³å¹³è¡¡ç‚¹ã€‚ä½†æ˜¯ä»–ä»¬åŸºæœ¬ä¸Šä¸»è¦å…³æ³¨è§†è§‰ç‰¹å®šçš„é—®é¢˜ã€‚å› æ­¤ï¼Œè®¡ç®—æœºè§†è§‰ç¤¾åŒºé€šå¸¸å¹¶ä¸ä¼šè‡ªæˆ‘è§„èŒƒï¼Œä»¥é¿å…é‚£äº›ä¸é€‚ç”¨äºè§†è§‰çš„é—®é¢˜ã€‚
- en: So you end up with things that are veryï¼Œ very efficient and very performant
    on vision problems so I think from that point of view it's an incredibly important
    line of work and that's probably the right way of doing things what we're sort
    of aiming for is something is the things that are as general as possible while
    still kind of being performantã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ æœ€ç»ˆä¼šå¾—åˆ°åœ¨è§†è§‰é—®é¢˜ä¸Šéå¸¸é«˜æ•ˆå’Œè¡¨ç°å‡ºè‰²çš„ä¸œè¥¿ã€‚æˆ‘è®¤ä¸ºä»è¿™ä¸ªè§’åº¦æ¥çœ‹ï¼Œè¿™æ˜¯ä¸€ä¸ªæå…¶é‡è¦çš„ç ”ç©¶æ–¹å‘ï¼Œè¿™å¯èƒ½æ˜¯æ­£ç¡®çš„åšæ³•ã€‚æˆ‘ä»¬æ‰€è¿½æ±‚çš„ï¼Œæ˜¯å°½å¯èƒ½é€šç”¨ï¼ŒåŒæ—¶ä»ç„¶ä¿æŒé«˜æ•ˆã€‚
- en: So got it so this kind of thing is critical Oh sorry you off go ahead no no
    please go ahead I was going to say so this kind of thing is like it's important
    just to summary so you feel like it's important to focus on attention because
    that's kind of critical for NLP like you can't just sort of put in a convolution
    at the end and sort of fix the problem but envision maybe you can and it's fine
    is that a right way of understanding itã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜ç™½äº†ï¼Œæ‰€ä»¥è¿™ç§äº‹æƒ…æ˜¯å…³é”®çš„ã€‚å“¦ï¼ŒæŠ±æ­‰ï¼Œä½ ç»§ç»­ã€‚æ²¡æœ‰ï¼Œæ²¡æœ‰ï¼Œè¯·ç»§ç»­ã€‚æˆ‘æƒ³è¯´è¿™ç§äº‹æƒ…å¾ˆé‡è¦ï¼Œç®€è€Œè¨€ä¹‹ï¼Œä½ è§‰å¾—å…³æ³¨æ³¨æ„åŠ›å¾ˆé‡è¦ï¼Œå› ä¸ºè¿™å¯¹NLPè‡³å…³é‡è¦ï¼›ä½ ä¸èƒ½ä»…ä»…åœ¨æœ€åæ”¾å…¥ä¸€ä¸ªå·ç§¯æ¥è§£å†³é—®é¢˜ï¼Œä½†æˆ–è®¸å¯ä»¥ï¼Œè¿™æ ·ç†è§£å¯¹å—ï¼Ÿ
- en: That's part of it vision and NLP aren't the only two domainsã€‚and so the thing
    that we're for are really basically so the kinds of problems that we're interested
    in doing with this include things like event- based camerasã€‚cell biologyï¼Œ sort
    of proteinsï¼Œ all of these sorts of things where we may or may not have the right
    convolutional inductive biases for to even know how to build those sorts of things
    or they end up being whole research programs like the mesh based convolution workã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å…¶ä¸­çš„ä¸€éƒ¨åˆ†æ˜¯ï¼Œè§†è§‰å’ŒNLPå¹¶ä¸æ˜¯å”¯ä¸€ä¸¤ä¸ªé¢†åŸŸã€‚æ‰€ä»¥æˆ‘ä»¬çœŸæ­£å…³æ³¨çš„ï¼Œæ˜¯æˆ‘ä»¬æƒ³ç”¨è¿™ä¸ªè§£å†³çš„é—®é¢˜ï¼ŒåŒ…æ‹¬äº‹ä»¶é©±åŠ¨ç›¸æœºã€ç»†èƒç”Ÿç‰©å­¦ã€è›‹ç™½è´¨ç­‰è¿™äº›æ–¹é¢ï¼Œæˆ‘ä»¬å¯èƒ½æ²¡æœ‰æ­£ç¡®çš„å·ç§¯å½’çº³åç½®æ¥çŸ¥é“å¦‚ä½•æ„å»ºè¿™äº›ä¸œè¥¿ï¼Œæˆ–è€…å®ƒä»¬æœ€ç»ˆæˆä¸ºæ•´ä¸ªç ”ç©¶é¡¹ç›®ï¼Œæ¯”å¦‚åŸºäºç½‘æ ¼çš„å·ç§¯å·¥ä½œã€‚
- en: Oh Coï¼Œ thank youã€‚I also had like one more question about the architectureã€‚So
    I saw thatã€‚I'm sorry if you said this and I just missed itï¼Œ but you had cross
    attention and then like that to transformer and then cost attentionã€‚I'm curious
    what happens if you replace the self attention in those layers with cross attentionã€‚Does
    it affect your accuracy is that even feasible is that a valid questionã€‚Yeahã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: å“¦ï¼ŒCoï¼Œè°¢è°¢ä½ ã€‚æˆ‘è¿˜æœ‰ä¸€ä¸ªå…³äºæ¶æ„çš„é—®é¢˜ã€‚æ‰€ä»¥æˆ‘çœ‹åˆ°ä½ æåˆ°çš„ã€‚æˆ‘å¾ˆæŠ±æ­‰å¦‚æœä½ è¯´è¿‡è¿™ä¸ªè€Œæˆ‘æ²¡å¬åˆ°ï¼Œä½†ä½ æåˆ°çš„äº¤å‰æ³¨æ„åŠ›ï¼Œç„¶åæ˜¯å˜æ¢å™¨ï¼Œå†åˆ°ä»£ä»·æ³¨æ„åŠ›ã€‚æˆ‘å¾ˆå¥½å¥‡ï¼Œå¦‚æœä½ æŠŠè¿™äº›å±‚ä¸­çš„è‡ªæ³¨æ„åŠ›æ›¿æ¢ä¸ºäº¤å‰æ³¨æ„åŠ›ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿè¿™ä¼šå½±å“ä½ çš„å‡†ç¡®æ€§å—ï¼Ÿè¿™ç”šè‡³å¯è¡Œå—ï¼Ÿè¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„é—®é¢˜å—ï¼Ÿæ˜¯çš„ã€‚
- en: so the sort of thing that you could do is you could modify this to make it sort
    of hierarchical so that there are multiple stages across attention we haven't
    gotten this working yetã€‚but it doesn't mean it's not a good ideaã€‚So there there
    might be there might be a right way to do this that we haven't figured out rightã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯ä»¥åšçš„äº‹æƒ…æ˜¯ä¿®æ”¹å®ƒï¼Œä½¿å…¶å…·æœ‰æŸç§å±‚æ¬¡ç»“æ„ï¼Œè¿™æ ·å°±ä¼šæœ‰å¤šä¸ªé˜¶æ®µçš„äº¤å‰æ³¨æ„åŠ›ã€‚æˆ‘ä»¬è¿˜æ²¡æœ‰è®©è¿™ä¸ªå·¥ä½œï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€è¿™ä¸æ˜¯ä¸€ä¸ªå¥½ä¸»æ„ã€‚æ‰€ä»¥ï¼Œå¯èƒ½æœ‰ä¸€ç§æ­£ç¡®çš„æ–¹æ³•æ¥åšåˆ°è¿™ä¸€ç‚¹ï¼Œè€Œæˆ‘ä»¬è¿˜æ²¡æœ‰æ‰¾å‡ºæ­£ç¡®çš„æ–¹æ³•ã€‚
- en: but it's something we have tried a little bit Oh cool okayï¼Œ thank you so much
    I appreciate itã€‚Yeahã€‚problemã€‚Okayï¼Œ let meã€‚We're running short on timeï¼Œ so maybe
    I'll skip aheadã€‚å—¯ã€‚Okay soã€‚Before we run too much timeï¼Œ I want to at least talk
    about the sort of the modifications to this architecture that we've made to make
    it work sort of even more generallyã€‚so one of the problems of the sort of the
    first the first architecture that we looked at here the basic receiver is that
    it works basically for arbitrary inputs but it's designed to work only on classification
    and regression tasks as an output and so basically we wanted to see if we could
    use the the same cross attention strategy for decoding and it turns out you can
    it's something that works pretty well just kind of out of the box so the idea
    is that we have if we have our cross attentiontion input and self-attention sort
    of to do the processing we can introduce a set of additional queries and these
    are basically queries that give the semantics of each of the points that you're
    trying to decodeã€‚
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬è¯•äº†ä¸€ç‚¹ç‚¹ï¼Œå“¦ï¼Œé…·ï¼Œè°¢è°¢ä½ ï¼Œæˆ‘å¾ˆæ„Ÿæ¿€ã€‚æ˜¯çš„ï¼Œæ²¡é—®é¢˜ã€‚å¥½å§ï¼Œè®©æˆ‘ã€‚æˆ‘ä»¬æ—¶é—´ä¸å¤šäº†ï¼Œä¹Ÿè®¸æˆ‘å¯ä»¥è·³è¿‡ä¸€äº›å†…å®¹ã€‚å—¯ã€‚å¥½å§ã€‚æ‰€ä»¥ï¼Œåœ¨æˆ‘ä»¬èŠ±å¤ªå¤šæ—¶é—´ä¹‹å‰ï¼Œæˆ‘æƒ³è‡³å°‘è°ˆè°ˆæˆ‘ä»¬ä¸ºä½¿è¿™ä¸ªæ¶æ„æ›´é€šç”¨è€Œåšçš„ä¿®æ”¹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œçœ‹åˆ°çš„ç¬¬ä¸€ä¸ªæ¶æ„çš„é—®é¢˜æ˜¯ï¼Œå®ƒåŸºæœ¬ä¸Šé€‚ç”¨äºä»»æ„è¾“å…¥ï¼Œä½†ä»…è®¾è®¡ç”¨äºåˆ†ç±»å’Œå›å½’ä»»åŠ¡è¾“å‡ºï¼Œå› æ­¤æˆ‘ä»¬å¸Œæœ›çœ‹çœ‹æ˜¯å¦å¯ä»¥ä½¿ç”¨ç›¸åŒçš„äº¤å‰æ³¨æ„åŠ›ç­–ç•¥è¿›è¡Œè§£ç ï¼Œç»“æœè¯æ˜æ˜¯å¯ä»¥çš„ï¼Œè¿™ç§æ–¹æ³•ç›¸å½“æœ‰æ•ˆï¼Œå‡ ä¹å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚æˆ‘ä»¬çš„æƒ³æ³•æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬æœ‰äº¤å‰æ³¨æ„åŠ›è¾“å…¥å’Œè‡ªæ³¨æ„åŠ›æ¥è¿›è¡Œå¤„ç†ï¼Œæˆ‘ä»¬å¯ä»¥å¼•å…¥ä¸€ç»„é¢å¤–çš„æŸ¥è¯¢ï¼Œè¿™äº›æŸ¥è¯¢åŸºæœ¬ä¸Šä¸ºæ¯ä¸ªä½ è¯•å›¾è§£ç çš„ç‚¹æä¾›è¯­ä¹‰ã€‚
- en: And we pass these input to another cross attention layer which is configured
    in basically the opposite way that the encoder cross attention is configuredã€‚so
    now the queries are going to be something that's potentially large and the keys
    and values are coming from this latent and so what this allows us to do basically
    is to keep all of the nice advantages of the original perceive so we have an encoder
    that scales linearly we have a processor stageã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†è¿™äº›è¾“å…¥ä¼ é€’ç»™å¦ä¸€ä¸ªäº¤å‰æ³¨æ„åŠ›å±‚ï¼Œè¯¥å±‚çš„é…ç½®åŸºæœ¬ä¸Šä¸ç¼–ç å™¨äº¤å‰æ³¨æ„åŠ›çš„é…ç½®ç›¸åã€‚æ‰€ä»¥ç°åœ¨æŸ¥è¯¢å°†æ˜¯æ½œåœ¨çš„å¤§ï¼Œè€Œé”®å’Œå€¼åˆ™æ¥è‡ªè¿™ä¸ªæ½œåœ¨ç©ºé—´ï¼Œè¿™åŸºæœ¬ä¸Šè®©æˆ‘ä»¬ä¿ç•™äº†åŸå§‹æ„ŸçŸ¥çš„æ‰€æœ‰ä¼˜ç‚¹ï¼Œå› æ­¤æˆ‘ä»¬æœ‰ä¸€ä¸ªçº¿æ€§æ‰©å±•çš„ç¼–ç å™¨å’Œä¸€ä¸ªå¤„ç†é˜¶æ®µã€‚
- en: this sort of latent self-atten that scales independently of the input size and
    we now have a decoder that keeps the decoupling but gives this linear scaling
    with respect to output size and so by doing this we can now basically apply the
    same approach to basically dense output tasksã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ½œåœ¨è‡ªæ³¨æ„åŠ›ç‹¬ç«‹äºè¾“å…¥å¤§å°æ‰©å±•ï¼Œæˆ‘ä»¬ç°åœ¨æœ‰ä¸€ä¸ªè§£ç å™¨ï¼Œä¿æŒè§£è€¦ï¼Œä½†ä¸è¾“å‡ºå¤§å°å‘ˆçº¿æ€§æ‰©å±•ï¼Œå› æ­¤é€šè¿‡è¿™æ ·åšï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šå¯ä»¥å°†ç›¸åŒçš„æ–¹æ³•åº”ç”¨äºå¯†é›†è¾“å‡ºä»»åŠ¡ã€‚
- en: And so to give you a sense of how this worksï¼Œ just sort of intuitivelyã€‚If we're
    doing auto encoding on this image of puppiesã€‚basically what we do is we encode
    process and then to decodeã€‚we take a query that corresponds to each of the points
    and then we pass it into this decoder so we can query one of the pointsã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†è®©ä½ æ›´ç›´è§‚åœ°ç†è§£å®ƒçš„å·¥ä½œæ–¹å¼ã€‚å¦‚æœæˆ‘ä»¬å¯¹è¿™å¼ å°ç‹—çš„å›¾åƒè¿›è¡Œè‡ªç¼–ç ï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬çš„åšæ³•æ˜¯ç¼–ç å¤„ç†ï¼Œç„¶åè§£ç ã€‚æˆ‘ä»¬å–ä¸€ä¸ªå¯¹åº”äºæ¯ä¸ªç‚¹çš„æŸ¥è¯¢ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™è¿™ä¸ªè§£ç å™¨ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥æŸ¥è¯¢å…¶ä¸­ä¸€ä¸ªç‚¹ã€‚
- en: we get one pixel query another oneï¼Œ we get another one and all the way up until
    we get all 10000 points and that's how we can do reconstruction with thisã€‚And
    the cool thing about this is that it opens up a bunch of new applications and
    we can get different kinds of outputs just by changing how the queries workã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è·å¾—ä¸€ä¸ªåƒç´ æŸ¥è¯¢ï¼Œå†æ¥ä¸€ä¸ªï¼Œç›´åˆ°è·å¾—æ‰€æœ‰10000ä¸ªç‚¹ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•è¿›è¡Œé‡å»ºçš„ã€‚è€Œè¿™ä»¶äº‹çš„é…·ç‚¹åœ¨äºï¼Œå®ƒå¼€å¯äº†ä¸€äº›æ–°åº”ç”¨ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ”¹å˜æŸ¥è¯¢çš„æ–¹å¼å¾—åˆ°ä¸åŒç§ç±»çš„è¾“å‡ºã€‚
- en: So if we want to do something like multimodal auto encoding where we have some
    of the outputs or videosã€‚we use the same construction trick to get positions that
    to get queries that have the relevant semantics for each of the points that we're
    decoding and we can do this even though basically the sizes of these different
    data so the number of points they have is quite diverse so in the multimodal auto
    encoding experiments that we have in this paper we do this for video audio and
    labels at the same timeã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œå¦‚æœæˆ‘ä»¬æƒ³åšä¸€äº›å¤šæ¨¡æ€è‡ªç¼–ç ï¼Œæ¯”å¦‚æˆ‘ä»¬æœ‰ä¸€äº›è¾“å‡ºæ˜¯è§†é¢‘ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„æ„é€ æŠ€å·§æ¥è·å–ä½ç½®ï¼Œå¾—åˆ°å…·æœ‰ç›¸å…³è¯­ä¹‰çš„æŸ¥è¯¢ï¼Œå°½ç®¡è¿™äº›ä¸åŒæ•°æ®çš„å¤§å°ï¼Œå³ç‚¹çš„æ•°é‡å·®å¼‚å¾ˆå¤§ï¼Œåœ¨è¿™ç¯‡è®ºæ–‡çš„å¤šæ¨¡æ€è‡ªç¼–ç å®éªŒä¸­ï¼Œæˆ‘ä»¬åŒæ—¶è¿›è¡Œäº†è§†é¢‘ã€éŸ³é¢‘å’Œæ ‡ç­¾çš„å¤„ç†ã€‚
- en: so that all of them are just passed into their uniform network and then decoded
    one by one in this wayã€‚But we can also do mass language modeling now by conditioning
    on the position in a sequenceã€‚we can do multitask classification by having basically
    an index that gives which task you're querying from the networkã€‚And we can do
    things like optical flow by passing in input features as well as the positionsã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰çš„è¾“å…¥éƒ½è¢«ä¼ é€’åˆ°ç»Ÿä¸€ç½‘ç»œä¸­ï¼Œç„¶åé€ä¸ªè§£ç ã€‚ä½†æˆ‘ä»¬ç°åœ¨ä¹Ÿå¯ä»¥é€šè¿‡å¯¹åºåˆ—ä¸­çš„ä½ç½®è¿›è¡Œæ¡ä»¶å¤„ç†æ¥è¿›è¡Œå¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åŸºæœ¬ä¸Šæä¾›ä¸€ä¸ªç´¢å¼•æ¥è¿›è¡Œå¤šä»»åŠ¡åˆ†ç±»ï¼ŒæŒ‡ç¤ºä½ æ­£åœ¨ä»ç½‘ç»œæŸ¥è¯¢å“ªä¸ªä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡ä¼ é€’è¾“å…¥ç‰¹å¾ä»¥åŠä½ç½®æ¥å¤„ç†å…‰æµã€‚
- en: And so I'm just going to just skip to a couple of the differentã€‚I can share
    these slides with you all afterwards to look through themã€‚Some of these things
    are quite cool butã€‚Just quicklyã€‚I want to talk about language and then optical
    flowã€‚So for language basically what we wanted to do with this was to see if we
    could use this to replace tokenization and why might we care about getting rid
    of tokenization so one we use tokenization primarily because transformers scale
    poorly with sequence length and tokenizing cut sequence length by that a factor
    of fourã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘å°†è·³è¿‡å‡ ä¸ªä¸åŒçš„éƒ¨åˆ†ã€‚æˆ‘å¯ä»¥åœ¨ä¹‹åå°†è¿™äº›å¹»ç¯ç‰‡åˆ†äº«ç»™å¤§å®¶ä¾›å¤§å®¶æŸ¥çœ‹ã€‚è¿™äº›å†…å®¹ä¸­æœ‰äº›ç›¸å½“é…·ï¼Œä½†æˆ‘æƒ³å¿«é€Ÿè°ˆè°ˆè¯­è¨€å’Œå…‰æµã€‚å› æ­¤ï¼Œå…³äºè¯­è¨€ï¼Œæˆ‘ä»¬æƒ³åšçš„æ˜¯çœ‹çœ‹æ˜¯å¦å¯ä»¥ç”¨å®ƒæ¥æ›¿ä»£æ ‡è®°åŒ–ï¼Œä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬å…³å¿ƒå»æ‰æ ‡è®°åŒ–ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¸»è¦ä½¿ç”¨æ ‡è®°åŒ–æ˜¯å› ä¸ºå˜æ¢å™¨åœ¨åºåˆ—é•¿åº¦ä¸Šæ‰©å±•è¾ƒå·®ï¼Œè€Œæ ‡è®°åŒ–å°†åºåˆ—é•¿åº¦å‡å°‘äº†å››å€ã€‚
- en: But but there are various problems that arise with this and so might we care
    about removing tokenizersï¼Ÿ
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ï¼Œå‡ºç°äº†å„ç§å„æ ·çš„é—®é¢˜ï¼Œé‚£ä¹ˆæˆ‘ä»¬ä¸ºä»€ä¹ˆè¦å…³å¿ƒå»æ‰æ ‡è®°å™¨å‘¢ï¼Ÿ
- en: So for oneã€‚Tokenizers perform less well on rare wordsã€‚so if you compare the
    sort of the bytebased decompositionã€‚theTF based UTF8 encoding input of an input
    sequence like thisã€‚you can see that there's basically a uniform allocation of
    points in memory to each of the input charactersã€‚
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæ ‡è®°å™¨åœ¨å¤„ç†ç¨€æœ‰å•è¯æ—¶è¡¨ç°ä¸ä½³ã€‚æ‰€ä»¥å¦‚æœä½ æ¯”è¾ƒåŸºäºå­—èŠ‚çš„åˆ†è§£å’ŒåŸºäºTFçš„UTF8ç¼–ç è¾“å…¥åºåˆ—ï¼Œä½ ä¼šå‘ç°åŸºæœ¬ä¸Šæ¯ä¸ªè¾“å…¥å­—ç¬¦åœ¨å†…å­˜ä¸­éƒ½æœ‰å‡åŒ€çš„ç‚¹åˆ†é…ã€‚
- en: the exception or diacritics which end up splitting it into two but if you look
    at the sentence piece tokenization so it's learned that pepper is one token but
    jalapeno gets split into five in this caseã€‚so this basically says the amount of
    capacity that you allocate depends on how rare the word is which can lead to suboptimal
    encodingsã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¤–æˆ–å˜éŸ³ç¬¦å·ä¼šå¯¼è‡´å®ƒè¢«æ‹†åˆ†ä¸ºä¸¤ä¸ªï¼Œä½†å¦‚æœä½ æŸ¥çœ‹å¥å­ç‰‡æ®µçš„æ ‡è®°åŒ–ï¼Œå°±ä¼šå‘ç°â€œpepperâ€æ˜¯ä¸€ä¸ªæ ‡è®°ï¼Œè€Œâ€œjalapenoâ€åœ¨è¿™ç§æƒ…å†µä¸‹è¢«æ‹†åˆ†ä¸ºäº”ä¸ªã€‚è¿™åŸºæœ¬ä¸Šè¡¨æ˜ä½ åˆ†é…çš„å®¹é‡å–å†³äºå•è¯çš„ç¨€æœ‰ç¨‹åº¦ï¼Œè¿™å¯èƒ½å¯¼è‡´æ¬¡ä¼˜ç¼–ç ã€‚
- en: They're also bridled to subtle perturbationsï¼Œ a famous example of this is that
    if you enter so if you've ever played around a GPT3ã€‚you'll notice that the output
    can be quite sensitive to where if you add a space or emit a space at the endã€‚and
    this basically is because the space can end up being factorized into different
    parts of the tokenizationã€‚problem there are other things that can happen there
    too but this is one cause of thatã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬è¿˜å®¹æ˜“å—åˆ°ç»†å¾®æ‰°åŠ¨çš„å½±å“ï¼Œä¸€ä¸ªè‘—åçš„ä¾‹å­æ˜¯ï¼Œå¦‚æœä½ ç©è¿‡GPT-3ï¼Œä½ ä¼šæ³¨æ„åˆ°è¾“å‡ºå¯¹åœ¨æœ«å°¾æ·»åŠ æˆ–çœç•¥ç©ºæ ¼éå¸¸æ•æ„Ÿã€‚è¿™åŸºæœ¬ä¸Šæ˜¯å› ä¸ºç©ºæ ¼å¯èƒ½ä¼šè¢«åˆ†è§£æˆæ ‡è®°åŒ–çš„ä¸åŒéƒ¨åˆ†ã€‚è¿™å„¿è¿˜æœ‰å…¶ä»–é—®é¢˜ï¼Œä½†è¿™æ˜¯å…¶ä¸­ä¸€ä¸ªåŸå› ã€‚
- en: And finally tokens don't transfer across across languagesã€‚so if you wanted to
    have a model that without any tuning could be used on many different languages
    at the same timeã€‚tokenizers are a blocker for thisï¼Œ so if we can get rid of them
    it'll simplify the pipeline it'll also make things less brittle and then hopefully
    lead to more general modelsã€‚So the way that we do mass language modeling is the
    same as the way that I showed in that schematic autoencoding experimentã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œæ ‡è®°åœ¨ä¸åŒè¯­è¨€é—´ä¸è½¬ç§»ã€‚å› æ­¤ï¼Œå¦‚æœä½ æƒ³æ‹¥æœ‰ä¸€ä¸ªå¯ä»¥åœ¨è®¸å¤šä¸åŒè¯­è¨€ä¸Šä½¿ç”¨è€Œä¸éœ€è°ƒä¼˜çš„æ¨¡å‹ï¼Œæ ‡è®°å™¨å°±æ˜¯ä¸€ä¸ªéšœç¢ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬èƒ½å»æ‰å®ƒä»¬ï¼Œå°†ç®€åŒ–ç®¡é“ï¼Œè¿˜ä¼šä½¿äº‹æƒ…å˜å¾—æ›´ä¸è„†å¼±ï¼Œè¿›è€Œå¸Œæœ›èƒ½å¯¼è‡´æ›´é€šç”¨çš„æ¨¡å‹ã€‚æˆ‘ä»¬è¿›è¡Œå¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡çš„æ–¹å¼ä¸æˆ‘åœ¨é‚£ä¸ªç¤ºæ„å›¾è‡ªåŠ¨ç¼–ç å®éªŒä¸­å±•ç¤ºçš„æ–¹å¼ç›¸åŒã€‚
- en: so we mask some fraction of our inputs about 15% is sort of the standard magic
    numberã€‚we then decode at each of the positions that are masked and we task the
    model with decoding whatever characters we masked at those locationsã€‚And then
    once we have this modelï¼Œ so this is what we do for pretrainingã€‚we can then fine
    tune it by replacing the decoder with a multitask decoder that takes in the tasks
    that we're using on the downstream evaluation setting and training the model to
    reconstruct the logits on a per task basisã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤æˆ‘ä»¬æ©ç›–äº†çº¦15%çš„è¾“å…¥ï¼Œè¿™å¤§è‡´æ˜¯æ ‡å‡†çš„é­”æ³•æ•°å­—ã€‚ç„¶åæˆ‘ä»¬åœ¨è¢«æ©ç›–çš„ä½ç½®è¿›è¡Œè§£ç ï¼Œå¹¶è¦æ±‚æ¨¡å‹è§£ç åœ¨è¿™äº›ä½ç½®æ©ç›–çš„å­—ç¬¦ã€‚ä¸€æ—¦æˆ‘ä»¬æœ‰äº†è¿™ä¸ªæ¨¡å‹ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬è¿›è¡Œé¢„è®­ç»ƒæ‰€åšçš„ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡å°†è§£ç å™¨æ›¿æ¢ä¸ºå¤šä»»åŠ¡è§£ç å™¨æ¥å¾®è°ƒå®ƒï¼Œåè€…æ¥å—æˆ‘ä»¬åœ¨ä¸‹æ¸¸è¯„ä¼°è®¾ç½®ä¸­ä½¿ç”¨çš„ä»»åŠ¡ï¼Œå¹¶è®­ç»ƒæ¨¡å‹æŒ‰ä»»åŠ¡åŸºç¡€é‡æ„logitsã€‚
- en: Okayï¼Œ so to look at how this model performsï¼Œ we basically first compare it to
    Bt basease so this is just a solid benchmark that we understand very well and
    first by looking at sort of matched two models that have matched flops we can
    see that perceive IO and Bt basease work on par you see there's a different tradeoff
    here so to get the same number of flops basically we make perceive IO deeper and
    this ends up giving it more parametersã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œä¸ºäº†æŸ¥çœ‹è¿™ä¸ªæ¨¡å‹çš„è¡¨ç°ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šé¦–å…ˆå°†å…¶ä¸Bt baseaseè¿›è¡Œæ¯”è¾ƒï¼Œè¿™åªæ˜¯ä¸€ä¸ªæˆ‘ä»¬éå¸¸äº†è§£çš„å¯é åŸºå‡†ã€‚é¦–å…ˆé€šè¿‡æŸ¥çœ‹ä¸¤ä¸ªæ¨¡å‹åŒ¹é…çš„flopsï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°perceive
    IOå’ŒBt baseaseçš„è¡¨ç°ç›¸å½“ï¼Œä½ ä¼šçœ‹åˆ°è¿™é‡Œå­˜åœ¨ä¸åŒçš„æƒè¡¡ï¼Œå› æ­¤ä¸ºäº†è·å¾—ç›¸åŒæ•°é‡çš„flopsï¼ŒåŸºæœ¬ä¸Šæˆ‘ä»¬è®©perceive IOæ›´æ·±ï¼Œè¿™æœ€ç»ˆå¯¼è‡´äº†æ›´å¤šçš„å‚æ•°ã€‚
- en: but on a perflps basis it ends up performing about the sameã€‚On the other handã€‚if
    we remove the tokenizer from Bt and keep the flops the sameã€‚we see that the number
    of parameters in the depth just drastically fall down and this is because Bert
    scales quite poorly with sequence length because it uses a normal transformerã€‚But
    if we use a perceive without the tokenizationï¼Œ we can see that we only get a slight
    reduction in the number of parameters at the flops countã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†åœ¨æ¯flopåŸºç¡€ä¸Šï¼Œå®ƒçš„è¡¨ç°åŸºæœ¬ä¸Šç›¸åŒã€‚å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæˆ‘ä»¬ä»Btä¸­å»é™¤tokenizerå¹¶ä¿æŒflopsä¸å˜ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°å‚æ•°çš„æ•°é‡å’Œæ·±åº¦æ€¥å‰§ä¸‹é™ï¼Œè¿™æ˜¯å› ä¸ºBertåœ¨åºåˆ—é•¿åº¦ä¸Šæ‰©å±•å¾—ç›¸å½“ç³Ÿç³•ï¼Œå› ä¸ºå®ƒä½¿ç”¨äº†æ™®é€šçš„transformerã€‚ä½†å¦‚æœæˆ‘ä»¬ä½¿ç”¨æ²¡æœ‰tokenizationçš„perceiveï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨flopsè®¡æ•°ä¸Šå‚æ•°çš„å‡å°‘åªæ˜¯è½»å¾®çš„ã€‚
- en: but the performance performs almost exactly the same so this means that the
    perceive in the setting is performing basically the same with and without the
    tokenization it's learning a different strategy it's using different parameters
    but it basically can be brought to the same performanceã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ€§èƒ½å‡ ä¹å®Œå…¨ç›¸åŒï¼Œè¿™æ„å‘³ç€åœ¨è¿™ç§è®¾ç½®ä¸­perceiveçš„è¡¨ç°ä¸æœ‰æ— tokenizationåŸºæœ¬ä¸Šç›¸åŒï¼Œå®ƒå­¦ä¹ äº†ä¸åŒçš„ç­–ç•¥ï¼Œä½¿ç”¨äº†ä¸åŒçš„å‚æ•°ï¼Œä½†å®ƒåŸºæœ¬ä¸Šå¯ä»¥è¾¾åˆ°ç›¸åŒçš„æ€§èƒ½ã€‚
- en: We can then scale this more by sort of leaning into what happens in the tokenizer
    free setting and we see that we can get a moderate performance boost as wellã€‚I
    think it's also in the language setting it's also useful to look at what the attentionã€‚the
    attention maps that are sort of learned and what's being visualized here are basically
    for each of the latents for some subset of the latesã€‚we're looking at where it's
    attending to in the input sequence and some of these end up being local so point
    looking at specific points in the sentenceã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¯ä»¥é€šè¿‡æ·±å…¥äº†è§£tokenizer-freeè®¾ç½®ä¸­å‘ç”Ÿçš„äº‹æƒ…æ¥è¿›ä¸€æ­¥æ‰©å±•è¿™ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬çœ‹åˆ°æˆ‘ä»¬ä¹Ÿå¯ä»¥è·å¾—é€‚åº¦çš„æ€§èƒ½æå‡ã€‚æˆ‘è®¤ä¸ºåœ¨è¯­è¨€è®¾ç½®ä¸­ï¼ŒæŸ¥çœ‹æ³¨æ„åŠ›å›¾ä¹Ÿæ˜¯æœ‰ç”¨çš„ï¼Œè¿™é‡Œå¯è§†åŒ–çš„å†…å®¹åŸºæœ¬ä¸Šæ˜¯é’ˆå¯¹æŸäº›æ½œåœ¨ç‰¹å¾ï¼Œæˆ‘ä»¬åœ¨è¾“å…¥åºåˆ—ä¸­æŸ¥çœ‹å®ƒå…³æ³¨çš„åœ°æ–¹ï¼Œå…¶ä¸­ä¸€äº›æœ€ç»ˆæ˜¯å±€éƒ¨çš„ï¼ŒæŒ‡å‘å¥å­ä¸­çš„ç‰¹å®šç‚¹ã€‚
- en: Some of them are periodicï¼Œ so they look sort of at recurring points over the
    sequence and some of them also look like they pick out syntactic featuresã€‚which
    is quite niceï¼Œ so they pick out basically exclamation points or capital letters
    or other punctuation that's quite useful and decodable from very right right at
    the beginning of the sequenceã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ä¸€äº›æ˜¯å‘¨æœŸæ€§çš„ï¼Œå› æ­¤å®ƒä»¬çœ‹èµ·æ¥ä¼šåœ¨åºåˆ—çš„é‡å¤ç‚¹ä¸Šï¼Œè€Œå…¶ä¸­ä¸€äº›çœ‹èµ·æ¥ä¹Ÿåƒæ˜¯æå–äº†å¥æ³•ç‰¹å¾ï¼Œè¿™éå¸¸ä¸é”™ï¼Œå› æ­¤å®ƒä»¬åŸºæœ¬ä¸Šæå–æ„Ÿå¹å·ã€å¤§å†™å­—æ¯æˆ–å…¶ä»–ä»åºåˆ—å¼€å§‹å°±å¾ˆæœ‰ç”¨ä¸”å¯è§£ç çš„æ ‡ç‚¹ç¬¦å·ã€‚
- en: We can also basically use this exact same architecture on optical flow and optical
    flow is basically an important classical problem in computer vision where given
    a pair of frames in a video we want to basically track all of the points so figure
    out the motion from every point from one frame to the other and so optical flow
    is usually visualize using these sort of colorized images that are shown in the
    bottom and what this gives you basically is a per pixel indication of the velocity
    at every single pointã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹ŸåŸºæœ¬å¯ä»¥åœ¨å…‰æµä¸Šä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¶æ„ï¼Œè€Œå…‰æµåŸºæœ¬ä¸Šæ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€ä¸ªé‡è¦ç»å…¸é—®é¢˜ï¼Œç»™å®šè§†é¢‘ä¸­çš„ä¸€å¯¹å¸§ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæƒ³è¦è·Ÿè¸ªæ‰€æœ‰çš„ç‚¹ï¼Œä»¥ä¾¿ä»ä¸€å¸§åˆ°å¦ä¸€å¸§è®¡ç®—æ¯ä¸ªç‚¹çš„è¿åŠ¨ï¼Œå› æ­¤å…‰æµé€šå¸¸ä½¿ç”¨è¿™äº›åº•éƒ¨æ˜¾ç¤ºçš„å½©è‰²å›¾åƒè¿›è¡Œå¯è§†åŒ–ï¼Œè¿™åŸºæœ¬ä¸Šç»™ä½ æä¾›äº†æ¯ä¸ªç‚¹çš„é€Ÿåº¦çš„é€åƒç´ æŒ‡ç¤ºã€‚
- en: And so you can see that so the blade that the character here is holding is moving
    to the rightã€‚whereas this creature behind her is sort of as moving downwardsã€‚So
    there are a couple of problems with optical flow that make it interesting to to
    sort of approachã€‚so one it's a dense task and it basically involves long range
    correspondences but the standard training protocol there's basically no largescale
    realistic training data just because it's incredibly hard to sort of label all
    of the pixels in a real world scene and figure out where they go to so the typical
    way to do this is to train on some synthetic data and then evaluate on more realistic
    scenes and optical flow is also interesting because it's basically the locust
    of some of the most complicated visual architectures in the literature so the
    previous say of the art result here is this method called raft which won the best
    paper reward at DCCCVE last year and I'm just highlighting this to give you a
    sense of how much work people do into sort of hand engineering these architectures
    so this is a very very cleverly designed architecture and basically it incorporates
    things like global correlation volumes that are explicitly computedã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥çœ‹åˆ°ï¼Œè§’è‰²æ‰‹ä¸­çš„åˆ€åˆƒå‘å³ç§»åŠ¨ï¼Œè€Œå¥¹èº«åçš„ç”Ÿç‰©åˆ™æ˜¯å‘ä¸‹ç§»åŠ¨ã€‚å› æ­¤ï¼Œå…‰æµå­˜åœ¨å‡ ä¸ªæœ‰è¶£çš„æŒ‘æˆ˜ï¼Œæ‰€ä»¥å®ƒæ˜¯ä¸€ä¸ªå¯†é›†ä»»åŠ¡ï¼ŒåŸºæœ¬ä¸Šæ¶‰åŠé•¿è·ç¦»çš„å¯¹åº”å…³ç³»ï¼Œä½†æ ‡å‡†çš„è®­ç»ƒåè®®ä¸­åŸºæœ¬ä¸Šæ²¡æœ‰å¤§è§„æ¨¡çš„çœŸå®è®­ç»ƒæ•°æ®ï¼Œå› ä¸ºæ ‡è®°çœŸå®åœºæ™¯ä¸­çš„æ‰€æœ‰åƒç´ å¹¶æ‰¾å‡ºå®ƒä»¬çš„å»å‘æ˜¯éå¸¸å›°éš¾çš„ï¼Œå› æ­¤é€šå¸¸çš„åšæ³•æ˜¯å…ˆåœ¨ä¸€äº›åˆæˆæ•°æ®ä¸Šè¿›è¡Œè®­ç»ƒï¼Œç„¶ååœ¨æ›´çœŸå®çš„åœºæ™¯ä¸­è¿›è¡Œè¯„ä¼°ã€‚æ­¤å¤–ï¼Œå…‰æµä¹Ÿå¾ˆæœ‰è¶£ï¼Œå› ä¸ºå®ƒåŸºæœ¬ä¸Šæ˜¯æ–‡çŒ®ä¸­ä¸€äº›æœ€å¤æ‚çš„è§†è§‰æ¶æ„çš„æ ¸å¿ƒï¼Œæ‰€ä»¥ä¹‹å‰çš„æœ€å…ˆè¿›ç»“æœæ˜¯ä¸€ä¸ªå«åšRAFTçš„æ–¹æ³•ï¼Œå®ƒåœ¨å»å¹´DCCCVEä¸Šè·å¾—äº†æœ€ä½³è®ºæ–‡å¥–ï¼Œæˆ‘å¼ºè°ƒè¿™ä¸€ç‚¹æ˜¯ä¸ºäº†è®©ä½ äº†è§£äººä»¬åœ¨æ‰‹åŠ¨è®¾è®¡è¿™äº›æ¶æ„ä¸Šä»˜å‡ºäº†å¤šå°‘åŠªåŠ›ï¼Œå› æ­¤è¿™æ˜¯ä¸€ä¸ªéå¸¸å·§å¦™è®¾è®¡çš„æ¶æ„ï¼ŒåŸºæœ¬ä¸ŠåŒ…å«äº†åƒæ˜¾å¼è®¡ç®—çš„å…¨å±€å…³è”ä½“ç§¯è¿™æ ·çš„ä¸œè¥¿ã€‚
- en: Different offsets to basically allow the model to reason about how things at
    different scales are moving with respect to each otherã€‚It also has local neighborhood
    gather operations as well as update blocks to keep track of what's happening within
    each specific correlation blockã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸åŒçš„åç§»é‡åŸºæœ¬ä¸Šå…è®¸æ¨¡å‹æ¨ç†ä¸åŒå°ºåº¦çš„äº‹ç‰©æ˜¯å¦‚ä½•ç›¸äº’ç§»åŠ¨çš„ã€‚å®ƒè¿˜å…·æœ‰å±€éƒ¨é‚»åŸŸèšåˆæ“ä½œä»¥åŠæ›´æ–°æ¨¡å—ï¼Œä»¥è·Ÿè¸ªæ¯ä¸ªç‰¹å®šå…³è”æ¨¡å—å†…å‘ç”Ÿçš„æƒ…å†µã€‚
- en: and then finally there's a flow specific upsampling operators that were developedã€‚So
    in contrast to thisï¼Œ we're basically we wanted to see how well perceive IO would
    do here and just to give you a sense of sort of what we were expecting coming
    into thisã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œå¼€å‘äº†ä¸€äº›ç‰¹å®šäºæµçš„ä¸Šé‡‡æ ·æ“ä½œã€‚å› æ­¤ï¼Œä¸æ­¤ç›¸å¯¹ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šæƒ³çœ‹çœ‹PerceivedIOåœ¨è¿™é‡Œçš„è¡¨ç°å¦‚ä½•ï¼Œåªæ˜¯ä¸ºäº†è®©ä½ äº†è§£æˆ‘ä»¬å¯¹è¿™ä¸€ç‚¹çš„é¢„æœŸã€‚
- en: we thought wellï¼Œ maybe so perceivedIO is throwing a lot of the structure away
    so we were hoping that we would get some good results but it would probably overfit
    and there's this sort of problem of the domain transfer that's happening here
    but on the other hand self-attention seems to be a reasonable way to match this
    sort of correspondence thingã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æƒ³ï¼Œä¹Ÿè®¸PerceivedIOæŠ›å¼ƒäº†å¾ˆå¤šç»“æ„ï¼Œæ‰€ä»¥æˆ‘ä»¬å¸Œæœ›èƒ½å¤Ÿå¾—åˆ°ä¸€äº›å¥½çš„ç»“æœï¼Œä½†å®ƒå¯èƒ½ä¼šè¿‡æ‹Ÿåˆï¼Œå¹¶ä¸”è¿™é‡Œå­˜åœ¨é¢†åŸŸè½¬ç§»çš„é—®é¢˜ï¼Œä½†å¦ä¸€æ–¹é¢ï¼Œè‡ªæ³¨æ„åŠ›ä¼¼ä¹æ˜¯åŒ¹é…è¿™ç§å¯¹åº”å…³ç³»çš„åˆç†æ–¹æ³•ã€‚
- en: What we actually found was that just by doing the veryï¼Œ very simple preprocessing
    hereã€‚so extracting basically a patch around each pixel and then using the standard
    perceive IO architectureã€‚we were able to get state of the art results here and
    so this is basically was validation of this general approach of trying to have
    general purpose architectures that would transfer overã€‚And so basically with minimal
    tuningï¼Œ we are able to get results that are both of the sort of compelling benchmarks
    on both of the the Cel evaluation methods and to get comparable results on Kityã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å®é™…ä¸Šå‘ç°ï¼Œä»…ä»…é€šè¿‡è¿™é‡Œéå¸¸ã€éå¸¸ç®€å•çš„é¢„å¤„ç†ï¼Œå³æå–æ¯ä¸ªåƒç´ å‘¨å›´çš„ä¸€ä¸ªè¡¥ä¸ï¼Œç„¶åä½¿ç”¨æ ‡å‡†çš„æ„ŸçŸ¥IOæ¶æ„ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¿™é‡Œè·å¾—æœ€æ–°çš„ç»“æœï¼Œå› æ­¤è¿™åŸºæœ¬ä¸ŠéªŒè¯äº†è¿™ç§å°è¯•é‡‡ç”¨é€šç”¨æ¶æ„ä»¥ä¾¿è¿›è¡Œè¿ç§»çš„æ€»ä½“æ–¹æ³•ã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šåœ¨æœ€å°è°ƒæ•´ä¸‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¿™ä¸¤ç§Celè¯„ä¼°æ–¹æ³•ä¸Šè·å¾—ä»¤äººä¿¡æœçš„åŸºå‡†ç»“æœï¼Œå¹¶åœ¨Kityä¸Šè·å¾—å¯æ¯”ç»“æœã€‚
- en: so these are the standard onesã€‚And we can also sort of visualize what happens
    when we apply this on real world dataã€‚so there's no ground truth here so we can't
    really compare itã€‚but it's still useful to sort of see how it moves around and
    we can see that qualitatively it's able to capture a lot of the fine structure
    and to sort of get the right motion for the things that are very clearly moving
    in the specific directionã€‚We can also sort ofï¼Œ it's also I think informative to
    look at what happens how it manage to represent how it manages to represent sort
    of small structure is this video playingã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯æ ‡å‡†æŒ‡æ ‡ã€‚æˆ‘ä»¬è¿˜å¯ä»¥å¯è§†åŒ–åœ¨çœŸå®ä¸–ç•Œæ•°æ®ä¸Šåº”ç”¨æ­¤æ–¹æ³•æ—¶å‘ç”Ÿçš„æƒ…å†µã€‚æ‰€ä»¥è¿™é‡Œæ²¡æœ‰åŸºå‡†ï¼Œæˆ‘ä»¬ä¸èƒ½çœŸæ­£è¿›è¡Œæ¯”è¾ƒï¼Œä½†ä»ç„¶æœ‰åŠ©äºçœ‹åˆ°å®ƒæ˜¯å¦‚ä½•ç§»åŠ¨çš„ï¼Œä»å®šæ€§ä¸Šçœ‹ï¼Œå®ƒèƒ½å¤Ÿæ•æ‰åˆ°å¾ˆå¤šç»†å¾®çš„ç»“æ„ï¼Œå¹¶èƒ½æ­£ç¡®è¡¨ç¤ºé‚£äº›æ˜æ˜¾åœ¨ç‰¹å®šæ–¹å‘ç§»åŠ¨çš„äº‹ç‰©ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥çœ‹çœ‹å®ƒæ˜¯å¦‚ä½•æˆåŠŸè¡¨ç¤ºå°ç»“æ„çš„ï¼Œè¿™æ®µè§†é¢‘æ­£åœ¨æ’­æ”¾ã€‚
- en: Yeah canã€‚Okay coolï¼Œ so the thing to look at here is the fine water droplets
    that are sort of flying through the air as that bird flies by and because we're
    decoding at every single output pointã€‚the architecture is able to represent those
    so it's able to capture veryã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥çš„ã€‚å¥½çš„ï¼Œå¾ˆé…·ï¼Œè¿™é‡Œéœ€è¦å…³æ³¨çš„æ˜¯é‚£äº›åœ¨é¸Ÿé£è¿‡æ—¶é£ç¿”çš„ç»†å°æ°´æ»´ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æ¯ä¸ªè¾“å‡ºç‚¹è¿›è¡Œè§£ç ã€‚æ¶æ„èƒ½å¤Ÿè¡¨ç¤ºè¿™äº›ï¼Œå› æ­¤èƒ½å¤Ÿæ•æ‰åˆ°éå¸¸ç»†å¾®çš„å˜åŒ–ã€‚
- en: very fine sale segmentation that would be difficult to capture if you had for
    exampleã€‚a convolutional upsamplar hereã€‚Okayï¼Œ so I'm just going to the light light
    gone off in this roomã€‚å—¯ã€‚I'm also interested that you also try other task like
    depth estimationã€‚because especially where looks like it can still also work well
    on that modalitiesã€‚Yeahã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: éå¸¸ç»†è‡´çš„åˆ†å‰²ï¼Œå¦‚æœä½ åœ¨è¿™é‡Œä½¿ç”¨å·ç§¯ä¸Šé‡‡æ ·å™¨ï¼Œå°±å¾ˆéš¾æ•æ‰åˆ°è¿™äº›ã€‚å¥½çš„ï¼Œæ‰€ä»¥æˆ‘åªæ˜¯æƒ³æåˆ°è¿™ä¸ªæˆ¿é—´é‡Œçš„ç¯å…‰ç†„ç­äº†ã€‚å—¯ã€‚æˆ‘ä¹Ÿå¾ˆæ„Ÿå…´è¶£ä½ æ˜¯å¦å°è¯•è¿‡å…¶ä»–ä»»åŠ¡ï¼Œæ¯”å¦‚æ·±åº¦ä¼°è®¡ï¼Œç‰¹åˆ«æ˜¯çœ‹èµ·æ¥å®ƒåœ¨é‚£ç§æ¨¡å¼ä¸‹ä¹Ÿèƒ½å¾ˆå¥½åœ°å·¥ä½œã€‚æ˜¯çš„ã€‚
- en: so we haven't published anythingï¼Œ but some internal results suggest that that
    it works just fineã€‚re basically there don't seem to be one of the surprising thingsã€‚the
    things that we were a little bit unsure about was how much information was going
    to be contained in this latent because basically you're abstracting quite a lot
    and it doesn't have any 2D structure intrinsically but it does seem like it seems
    to be able to represent things quite well and these sorts of decoding mechanisms
    do seem to be able to do thatã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬è¿˜æ²¡æœ‰å‘å¸ƒä»»ä½•å†…å®¹ï¼Œä½†ä¸€äº›å†…éƒ¨ç»“æœè¡¨æ˜è¿™ç¡®å®æœ‰æ•ˆã€‚åŸºæœ¬ä¸Šä¼¼ä¹æ²¡æœ‰ä»€ä¹ˆä»¤äººæƒŠè®¶çš„äº‹æƒ…ã€‚æˆ‘ä»¬æœ‰ç‚¹ä¸ç¡®å®šçš„æ˜¯è¿™ä¸ªæ½œåœ¨ç©ºé—´ä¸­å°†åŒ…å«å¤šå°‘ä¿¡æ¯ï¼Œå› ä¸ºåŸºæœ¬ä¸Šä½ åœ¨æŠ½è±¡å¾ˆå¤šï¼Œå¹¶ä¸”å®ƒæœ¬èº«å¹¶æ²¡æœ‰ä»»ä½•2Dç»“æ„ï¼Œä½†ä¼¼ä¹å®ƒèƒ½å¤Ÿå¾ˆå¥½åœ°è¡¨ç¤ºäº‹ç‰©ï¼Œè¿™äº›è§£ç æœºåˆ¶ä¼¼ä¹ä¹Ÿèƒ½åšåˆ°è¿™ä¸€ç‚¹ã€‚
- en: ã‹ã‚‹ã§ã€‚So I'm just going to interest in this interest of timem I'm going to skip
    ahead to the conclusion Dr I had one question with respect to the metrics that
    you've shared for the optical flowã€‚the numbers so like the in the table it was
    like central final clean entity where these different data sets or different metrics
    like same metric for different dataset sets or like these are three different
    metricsã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ã€‚æ‰€ä»¥æˆ‘ç°åœ¨å‡ºäºæ—¶é—´çš„è€ƒè™‘ï¼Œè·³åˆ°ç»“è®ºã€‚æˆ‘æœ‰ä¸€ä¸ªå…³äºä½ åˆ†äº«çš„å…‰æµæŒ‡æ ‡çš„é—®é¢˜ï¼Œè¡¨æ ¼ä¸­çš„æ•°å­—åƒæ˜¯ä¸­å¿ƒæœ€ç»ˆæ¸…æ´å®ä½“ï¼Œè¿™äº›æ˜¯ä¸åŒçš„æ•°æ®é›†ï¼Œè¿˜æ˜¯è¯´è¿™äº›æ˜¯é’ˆå¯¹ä¸åŒæ•°æ®é›†çš„ç›¸åŒæŒ‡æ ‡ï¼Œæˆ–è€…æ˜¯è¿™ä¸‰ä¸ªä¸åŒçš„æŒ‡æ ‡ï¼Ÿ
- en: Yeah so these are three different data setsï¼Œ so Cintelcle and Stel final are
    basically two theyre two ways of doing the final rendering for Cintel in all cases
    these methods are trained just on the autoflow data set so they're trained on
    this sort of general purpose kind of wacky synthetic motion data set and then
    we're we're evaluating on these different domains without fine tuningã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè¿™äº›æ˜¯ä¸‰ç§ä¸åŒçš„æ•°æ®é›†ï¼Œæ‰€ä»¥Cintelcleå’ŒStel finalåŸºæœ¬ä¸Šæ˜¯Cintelçš„æœ€ç»ˆæ¸²æŸ“çš„ä¸¤ç§æ–¹å¼ï¼Œåœ¨æ‰€æœ‰æƒ…å†µä¸‹ï¼Œè¿™äº›æ–¹æ³•ä»…åœ¨å…‰æµæ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå› æ­¤å®ƒä»¬æ˜¯åœ¨è¿™ç§ä¸€èˆ¬ç”¨é€”çš„å¥‡ç‰¹åˆæˆè¿åŠ¨æ•°æ®é›†ä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œç„¶åæˆ‘ä»¬åœ¨è¿™äº›ä¸åŒé¢†åŸŸä¸Šè¿›è¡Œè¯„ä¼°ï¼Œè€Œæ²¡æœ‰è¿›è¡Œå¾®è°ƒã€‚
- en: Okayã€‚Yeahï¼Œ the flow has quite the data sets are quite smallã€‚so it's generally
    even problematic to fine tuneã€‚Thank youã€‚å—¯ã€‚Okayï¼Œ so just to summarizeï¼Œ rightã€‚what
    was the ground roof to find the endpoint errorã€‚Yeahã€‚so the way this works is Cintel
    is a computerï¼Œ it's basically a relatively high quality CGI movie that was basically
    open source and so they actually have the ground truth so if you know the ground
    truth 3D state you can compute the pixel correspondence from frame to frame so
    that's what's used on Cintel and then Kity they basically have they have like
    a LiDAR sensor that's used to figure out the depth of all points and then they
    compute the correspondences so the ground truth is actually the ground truth optical
    flow but in general it's hard to get dense optical flow it's very expensive to
    collect itã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚æ˜¯çš„ï¼Œæµé‡çš„æ•°æ®é›†ç›¸å½“å°ï¼Œå› æ­¤ä¸€èˆ¬æ¥è¯´ç”šè‡³åœ¨å¾®è°ƒæ—¶ä¹Ÿä¼šæœ‰é—®é¢˜ã€‚è°¢è°¢ã€‚å—¯ã€‚å¥½çš„ï¼Œç®€å•æ€»ç»“ä¸€ä¸‹ï¼Œæ²¡é”™ã€‚é‚£ä»€ä¹ˆæ˜¯æ‰¾åˆ°ç«¯ç‚¹è¯¯å·®çš„çœŸå®æƒ…å†µå‘¢ï¼Ÿæ˜¯çš„ã€‚è¿™ç§æ–¹å¼æ˜¯Cintelï¼Œè¿™æ˜¯ä¸€ä¸ªè®¡ç®—æœºç”Ÿæˆå›¾åƒï¼ŒåŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªç›¸å¯¹é«˜è´¨é‡çš„CGIç”µå½±ï¼ŒåŸºæœ¬ä¸Šæ˜¯å¼€æºçš„ï¼Œå› æ­¤ä»–ä»¬å®é™…ä¸Šæœ‰çœŸå®æƒ…å†µçš„ä¾æ®ï¼Œå¦‚æœä½ çŸ¥é“çœŸå®çš„3DçŠ¶æ€ï¼Œå°±å¯ä»¥è®¡ç®—æ¯å¸§ä¹‹é—´çš„åƒç´ å¯¹åº”å…³ç³»ï¼Œè¿™å°±æ˜¯åœ¨Cintelä¸Šä½¿ç”¨çš„ï¼Œç„¶åKityåŸºæœ¬ä¸Šæœ‰ä¸€ä¸ªLiDARä¼ æ„Ÿå™¨ï¼Œç”¨äºç¡®å®šæ‰€æœ‰ç‚¹çš„æ·±åº¦ï¼Œç„¶åè®¡ç®—å¯¹åº”å…³ç³»ï¼Œæ‰€ä»¥çœŸå®æƒ…å†µå®é™…ä¸Šæ˜¯å…‰æµçš„çœŸå®æƒ…å†µï¼Œä½†ä¸€èˆ¬æ¥è¯´ï¼Œè·å–å¯†é›†å…‰æµæ˜¯å¾ˆå›°éš¾çš„ï¼Œæ”¶é›†æˆæœ¬éå¸¸é«˜ã€‚
- en: Greatï¼Œ thanksã€‚å—¯ã€‚ğŸ˜Šï¼ŒOkayï¼Œ so so basically just to summarize so the perceives are
    tension-based architectures that scale linearly and work as drop in replacements
    for transformers on a variety of settingsã€‚they also seem to be able to achieve
    results that are comparable at least in performance with models that rely on 2D
    convolutions but of course there is a trade off here and so it's good to be very
    aware of this of generality versus speed and specific domains and so as was pointed
    out in settings where you can use 2D convolutionsã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å¾ˆå¥½ï¼Œè°¢è°¢ã€‚å—¯ã€‚ğŸ˜Šï¼Œå¥½çš„ï¼ŒåŸºæœ¬ä¸Šæ€»ç»“ä¸€ä¸‹ï¼Œæ„ŸçŸ¥å™¨æ˜¯åŸºäºå¼ åŠ›çš„æ¶æ„ï¼Œèƒ½å¤Ÿçº¿æ€§æ‰©å±•ï¼Œå¹¶åœ¨å¤šç§è®¾ç½®ä¸­ä½œä¸ºå˜å‹å™¨çš„æ›¿ä»£å“ã€‚å®ƒä»¬ä¼¼ä¹ä¹Ÿèƒ½å¤Ÿåœ¨æ€§èƒ½ä¸Šä¸ä¾èµ–äº2Då·ç§¯çš„æ¨¡å‹ç›¸åª²ç¾ï¼Œä½†å½“ç„¶è¿™é‡Œæœ‰æƒè¡¡ï¼Œæ‰€ä»¥è¦éå¸¸æ³¨æ„ä¸€èˆ¬æ€§ä¸é€Ÿåº¦ä»¥åŠç‰¹å®šé¢†åŸŸä¹‹é—´çš„å…³ç³»ï¼Œæ­£å¦‚åœ¨å¯ä»¥ä½¿ç”¨2Då·ç§¯çš„è®¾ç½®ä¸­æ‰€æŒ‡å‡ºçš„é‚£æ ·ã€‚
- en: it's certainly good to have them in the loopã€‚It's basically a unified architecture
    that allows joint modeling of different modalities of different sizes and basically
    overall it seems to be quite flexible architecture that's able to produce a state
    of the art or near state of the art results on a variety of different domains
    ands in the two papers we look at a number of other domains that I didn't talk
    about including 3D point cloud modeling replacing the transformer that's used
    in the starcraft and the sort of starcraft behavioral cloning agent in a couple
    of others so this does we have a lot of evidence that this general approach seems
    to work broadly and there's a lot of things we still haven't tried so we're very
    interested in pushing this and always open for suggestions and so forthã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å°†å®ƒä»¬çº³å…¥å¾ªç¯ä¸­å½“ç„¶æ˜¯æœ‰ç›Šçš„ã€‚è¿™åŸºæœ¬ä¸Šæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„æ¶æ„ï¼Œå…è®¸å¯¹ä¸åŒå¤§å°çš„ä¸åŒæ¨¡æ€è¿›è¡Œè”åˆå»ºæ¨¡ï¼Œæ•´ä½“æ¥çœ‹ï¼Œå®ƒä¼¼ä¹æ˜¯ä¸€ä¸ªç›¸å½“çµæ´»çš„æ¶æ„ï¼Œèƒ½å¤Ÿåœ¨å„ç§ä¸åŒé¢†åŸŸäº§ç”Ÿæœ€å…ˆè¿›æˆ–æ¥è¿‘æœ€å…ˆè¿›çš„ç»“æœã€‚åœ¨è¿™ä¸¤ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è®¸å¤šæˆ‘æ²¡æœ‰æåˆ°çš„å…¶ä»–é¢†åŸŸï¼ŒåŒ…æ‹¬3Dç‚¹äº‘å»ºæ¨¡ï¼Œæ›¿ä»£åœ¨ã€Šæ˜Ÿé™…äº‰éœ¸ã€‹ä¸­ä½¿ç”¨çš„å˜å‹å™¨ï¼Œä»¥åŠå…¶ä»–ä¸€äº›è¡Œä¸ºå…‹éš†ä»£ç†ï¼Œå› æ­¤æˆ‘ä»¬æœ‰å¾ˆå¤šè¯æ®è¡¨æ˜è¿™ç§é€šç”¨æ–¹æ³•ä¼¼ä¹åœ¨å¹¿æ³›åº”ç”¨ä¸­æœ‰æ•ˆï¼Œå¹¶ä¸”è¿˜æœ‰å¾ˆå¤šäº‹æƒ…æˆ‘ä»¬å°šæœªå°è¯•ï¼Œæ‰€ä»¥æˆ‘ä»¬éå¸¸å¸Œæœ›æ¨åŠ¨è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸”å§‹ç»ˆæ¬¢è¿å»ºè®®ç­‰ã€‚
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_15.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_15.png)'
- en: So we're relying on a large body of related work because we're sort of drawing
    from a lot of different areas hereã€‚so here are some highlights and then I just
    want to thank my co-authors on this workã€‚And of courseã€‚I'm happy to talk moreã€‚Thanksï¼ŒYeahï¼Œ
    thanks a lotã€‚ Thanks veryã€‚So one question I had is likeã€‚so what do you think is
    like the future of like like perceived like models like do you think like this
    are going to like being used more in like the sort of like the transform transformer
    community to replace like con and another stuffï¼Ÿ
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¾èµ–äºå¤§é‡ç›¸å…³å·¥ä½œçš„åŸºç¡€ï¼Œå› ä¸ºæˆ‘ä»¬ä»è®¸å¤šä¸åŒçš„é¢†åŸŸæ±²å–çµæ„Ÿã€‚è¿™äº›æ˜¯ä¸€äº›äº®ç‚¹ï¼Œæˆ‘åªæƒ³æ„Ÿè°¢æˆ‘çš„åˆè‘—è€…ä»¬ã€‚å½“ç„¶ï¼Œæˆ‘ä¹æ„è¿›ä¸€æ­¥è®¨è®ºã€‚è°¢è°¢ï¼Œæ˜¯çš„ï¼Œéå¸¸æ„Ÿè°¢ã€‚æˆ‘æœ‰ä¸€ä¸ªé—®é¢˜ï¼šä½ è®¤ä¸ºåƒæ„ŸçŸ¥æ¨¡å‹çš„æœªæ¥ä¼šæ˜¯ä»€ä¹ˆæ ·çš„ï¼Ÿä½ è§‰å¾—è¿™äº›æ¨¡å‹ä¼šåœ¨å˜æ¢å™¨ç¤¾åŒºä¸­è¢«æ›´å¤šä½¿ç”¨ï¼Œä»¥æ›¿ä»£å·ç§¯ç½‘ç»œç­‰å…¶ä»–ä¸œè¥¿å—ï¼Ÿ
- en: Yeahï¼Œ so the I thinkã€‚Broadly speakingï¼Œ I think of perceives now is sort of because
    we know how to adapt them pretty well to sort of domains where we don't have a
    great idea of the right way to structure an architectureã€‚an inductive biasï¼Œ so
    I think that's one of the really strong cases for itã€‚
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘è®¤ä¸ºï¼Œä»å¹¿ä¹‰ä¸Šè®²ï¼Œæˆ‘è®¤ä¸ºæ„ŸçŸ¥æ¨¡å‹ç°åœ¨æ˜¯ä¸€ç§é€‰æ‹©ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å¦‚ä½•å¾ˆå¥½åœ°å°†å®ƒä»¬é€‚åº”äºæˆ‘ä»¬å¯¹å¦‚ä½•æ„å»ºæ¶æ„æ²¡æœ‰å¾ˆæ¸…æ™°æƒ³æ³•çš„é¢†åŸŸã€‚è¿™ç§å½’çº³åå·®ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸å¼ºæœ‰åŠ›çš„æ¡ˆä¾‹ã€‚
- en: so settings in which you don't really know what the right way to structure a
    problem isã€‚I also think these kinds of approaches can be used in conjunction with
    cons for sort of things that are as domain agnostic has neededã€‚But I think multimodal
    and and new domains is really to that's the real that's where this these are obvious
    choices got it also like what do you think are the current bottlenecks with this
    and if you don't mind like if you can disclose like border is what are you working
    on towards next with this stuffã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åœ¨ä½ å¹¶ä¸ç¡®åˆ‡çŸ¥é“å¦‚ä½•æ„å»ºé—®é¢˜çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä¹Ÿè®¤ä¸ºè¿™äº›æ–¹æ³•å¯ä»¥ä¸éœ€è¦é¢†åŸŸæ— å…³çš„å†…å®¹ç»“åˆä½¿ç”¨ã€‚ä½†æˆ‘è®¤ä¸ºå¤šæ¨¡æ€å’Œæ–°é¢†åŸŸç¡®å®æ˜¯æ˜¾è€Œæ˜“è§çš„é€‰æ‹©ã€‚ä½ è§‰å¾—ç›®å‰åœ¨è¿™æ–¹é¢æœ‰å“ªäº›ç“¶é¢ˆï¼Ÿå¦‚æœä½ ä¸ä»‹æ„çš„è¯ï¼Œèƒ½å¦é€éœ²ä¸€ä¸‹ä½ åœ¨è¿™æ–¹é¢çš„ä¸‹ä¸€æ­¥å·¥ä½œæ˜¯ä»€ä¹ˆï¼Ÿ
- en: So I can't talk about too many details about thatï¼Œ but the a couple of domains
    so oneã€‚we don't really have a great handle on how to use them on sort of small
    small scale data so data where whereã€‚You don don't have the data to sort of recover
    the inductive bias so this is I think a really important areaã€‚the other thing
    that we haven't sort of talked about here but you could probably imagine that
    we'll be thinking about would be how to train on multiple modalities or sort of
    multiple things at the same time so right now all of these architectures are sort
    of trained in isolation but there are a lot of opportunities for sort of figuring
    out how to pose problems together and use a single architecture on all of themã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä¸èƒ½è°ˆè®ºå¤ªå¤šç»†èŠ‚ï¼Œä½†æœ‰å‡ ä¸ªé¢†åŸŸï¼Œé¦–å…ˆï¼Œæˆ‘ä»¬å¯¹å¦‚ä½•åœ¨å°è§„æ¨¡æ•°æ®ä¸Šä½¿ç”¨å®ƒä»¬å¹¶æ²¡æœ‰å¾ˆå¥½çš„æŠŠæ¡ï¼Œæ‰€ä»¥æ•°æ®çš„æ•°é‡ä¸è¶³ä»¥æ¢å¤å½’çº³åå·®ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä¸€ä¸ªéå¸¸é‡è¦çš„é¢†åŸŸã€‚å¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬è¿˜æ²¡æœ‰è®¨è®ºçš„äº‹æƒ…ï¼Œä½†ä½ å¯èƒ½ä¼šæƒ³è±¡æˆ‘ä»¬ä¼šè€ƒè™‘çš„æ˜¯å¦‚ä½•åŒæ—¶åœ¨å¤šç§æ¨¡å¼æˆ–å¤šç§äº‹ç‰©ä¸Šè¿›è¡Œè®­ç»ƒï¼Œå› æ­¤ç°åœ¨æ‰€æœ‰è¿™äº›æ¶æ„éƒ½æ˜¯å­¤ç«‹è®­ç»ƒçš„ï¼Œä½†æœ‰å¾ˆå¤šæœºä¼šå¯ä»¥ä¸€èµ·è§£å†³é—®é¢˜å¹¶åœ¨æ‰€æœ‰é—®é¢˜ä¸Šä½¿ç”¨å•ä¸€æ¶æ„ã€‚
- en: Got itï¼Œ what I'm sure if you triedï¼Œ but can you also use this for like tableular
    data stuffï¼ŸYeahã€‚so effectively the architecture treats any input data as tabular
    dataã€‚so I think that's exactly the right way to think about itã€‚course sounds good
    thanks to the topã€‚I will open make general questions for the studentsã€‚So it's
    jump recordingã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜ç™½äº†ï¼Œæˆ‘ä¸ç¡®å®šä½ æ˜¯å¦å°è¯•è¿‡ï¼Œä½†ä½ èƒ½å¦å°†å…¶ç”¨äºè¡¨æ ¼æ•°æ®ï¼Ÿæ˜¯çš„ï¼Œå› æ­¤è¯¥æ¶æ„å°†ä»»ä½•è¾“å…¥æ•°æ®è§†ä¸ºè¡¨æ ¼æ•°æ®ï¼Œæ‰€ä»¥æˆ‘è®¤ä¸ºè¿™æ­£æ˜¯æ€è€ƒçš„æ­£ç¡®æ–¹å¼ã€‚è¯¾ç¨‹å¬èµ·æ¥ä¸é”™ï¼Œè°¢è°¢ä½ çš„æé†’ã€‚æˆ‘ä¼šä¸ºå­¦ç”Ÿä»¬æå‡ºä¸€èˆ¬æ€§çš„é—®é¢˜ã€‚æ‰€ä»¥æˆ‘ä»¬å¼€å§‹å½•éŸ³ã€‚
- en: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_17.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1222cb1eda9b25cb98d5ff5f1d8e043f_17.png)'
