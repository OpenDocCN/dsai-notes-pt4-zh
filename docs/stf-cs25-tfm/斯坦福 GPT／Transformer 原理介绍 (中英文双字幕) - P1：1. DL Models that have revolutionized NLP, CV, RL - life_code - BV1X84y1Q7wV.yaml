- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P1：1. DL Models that have revolutionized
    NLP, CV, RL - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P1：1. 改变 NLP、CV、RL 的 DL 模型 - life_code -
    BV1X84y1Q7wV
- en: '![](img/5ecc633c0934862bc1bde6086b437e97_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
- en: '![](img/5ecc633c0934862bc1bde6086b437e97_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Hey everyone welcome to the first and introductory lecture for CS25 Transformers
    United so CS25 was a class that the three of us created and taught at Stanford
    in the fall of 2021 and the subject of the class is not as the picture might suggest
    it's not about robots that can transform into cars it's about deep learning models
    and specifically a particular kind of deep learning models that have revolutionized
    multiple fields starting from natural language processing to things like computer
    vision and reinforcement learning to name a few we have an exciting set of videos
    lined up for you we have some truly fantastic speakers come gift talks about how
    they were applying transformers in their own research。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: And we hope you will enjoy and learn from these talks this video is purely an
    introductory lecture to talk a little bit about transformers and before we get
    started I'd like to introduce the instructors so my name is A I am a software
    engineer at a company called appliedlied Intuition Before this I was a master
    student ins at Stanford。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望您能享受并从这些讲座中学习，这段视频纯粹是介绍性讲座，谈论一下变压器。在开始之前，我想介绍一下讲师，所以我叫A，是一家名为Applied Intuition的公司的软件工程师。在此之前，我是在斯坦福大学的硕士生。
- en: And。I am one of the Co instructorors for CS 25， Chaanya Dave。if the two of you
    could introduce yourselves。So hi everyone I am a PhD student at exampleford before
    this I was pursuing a master's here researching a lot in generative modeling enforcement
    learning and robots so nice to youall yeah that was Dave forceden say his name
    Chaanya if you want to introduce yourself Yeah hi everyone my name is Chaania
    and I'm currently working as an ML engineer at a startup called BoWorks before
    that I was a masters rate at Stanford specializing in NLP and was a member of
    the prize winning Stanford team for the Alexa Pri Challenge。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: All right， awesome。So。Moving on to the rest of this talk。essentially what we
    hope you will learn watching these videos and what we hope the people who took
    our class in the fall of 2021 learned is three things one is we hope you will
    have an understanding of how transformers work。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，太棒了。那么，接下来我们要谈论这个讲座的其他内容。我们希望你在观看这些视频时能够学到的，以及我们希望在2021年秋季参加我们课程的人学到的有三件事，第一件是我们希望你能理解变压器的工作原理。
- en: Secondly， we hope you will learn and by the end of these talks understand how
    transformers are being applied beyond just natural language processing。And thirdly，
    we hope that some of these talks will spark some new ideas within you and hopefully
    lead to new directions of research。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: new kinds of innovation， and things of that sort。And。To begin。we're going to
    talk a little bit about transformers and introduce some of the context behind
    transformers as well。and for that I'd like to hand it off to Dave。嗯。So hi everyone
    so welcome to our Transformer seminar so I'll start first with an overview of
    the attention timeline and how it came to be the key idea about transformers was
    the sub attention mechanism that was developed in 2017 and this all started with
    this point paper called attention is all you need was 22 before 2017 we used to
    have this prehistoric era where we had older models like RNs LSTMs and simpler
    attention mechanisms and eventually like the growth in transformers has explored
    into other fields and has become prominent in like all of machine learning and
    I'll go and see and show how this has been used so in the prehistoric era there
    used to be RNLs。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: There were different models like the sequence sequence LSTMs G they were good
    at encoding some sort of memory but they did not work for encoding long sequences
    and they were very bad at encoding context so here is an example where if you
    have a sentence like I grew up in France dot dot dot so I speak fluent dash then
    you want to fill this with like French based on context but like LST models might
    not know what it is and might just make a very big mistake here similarly we can
    show some sort of correlation map here where if you have a pronoun like it we
    wanted to correlate to one of the past nouns that you have seen so far like an
    animal but again older models were like really not good at this context encoding。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的模型，比如序列序列LSTM，它们在编码某种记忆方面表现不错，但在编码长序列时效果不佳，并且在编码上下文方面也非常差。因此这里有一个例子，如果你有一个句子，比如“我在法国长大……所以我说流利的”，那么你想根据上下文填入一些像法语的内容，但像LSTM模型可能不知道这是什么，可能会在这里犯一个很大的错误。同样，我们可以在这里展示某种相关性图谱，如果你有一个代词像“它”，我们想要将其与之前看到的某个名词相关联，比如一个动物，但同样，旧模型在这种上下文编码上真的很差。
- en: So where we are currently now is on the verge of takeoff we are beginning to
    realize the potential of transformers in different fields。we have started to use
    them to solve long sequence problems in protein folding such as like the alpha
    fold model from open from deep which is 95% accuracy on different challenges in
    offlinearrow we can use it for few shortard zero centralization for text and image
    we can also use this for like content generation so here's example from openAI
    where you can give a different text prompt and have an AI gen fictional image
    for you。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们正处于起飞的边缘，我们开始意识到变压器在不同领域的潜力。我们已经开始使用它们来解决蛋白质折叠中的长序列问题，比如来自DeepMind的AlphaFold模型，在不同挑战中的准确率达到95%。我们可以将其用于文本和图像的少量短期无中心化应用，我们还可以用它进行内容生成。以下是来自OpenAI的一个例子，你可以给出不同的文本提示，让AI为你生成虚构的图像。
- en: And so there's a contestus that you can also watch on YouTube。which basically
    says that LS students are died and long lived transformers。So what's the future
    so we can enable a lot more applications for transformers they can be applied
    to any form of sequence modeling so we can use we could use them for really understanding
    we can use them for finance and a lot more so basically imagine all sorts of genetic
    modeling problems。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless there are a lot of missing ingredientss so like the human brain
    we need some sort of external memory unit which is the hippocampus first and they
    are saying some early works here so one nice work you might want to check out
    is called neural curing machines similarly the current attention mechanisms are
    very computationally complex in terms of time and this scale corically which we
    will discuss later and we want to make them more linear and the third problem
    is like we want to align our current sort of language models with how the human
    brain works and human values and this is also a big issue。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Okay， so now I will deep dive deep dive I will。Dive deeper into the tension
    mechanisms and show how they came out to。So initially there used to be very simple
    mechanisms where attention was inspired by the process of importance ratinging。operating
    attention on different parts of image where like similar to human where you might
    focus more on like a foreground if you have a image of a do compared to like the
    rest of background so in the case of soft attention what you do is you learn the
    simple soft attention weighting for each pixel which can be weight between0 to1
    the problem over here is that this is a very expensive computation and then you
    can show as it' shown in the figure on the left you can see we are calculating
    this attention for the whole image what you can do instead is you can。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Clic a zero to one attention map where we directly put a one on where where
    the dog is and a zero wherever it's a background this is like less competition
    expensive but the problem is it's not defensiable and makes things harder to train。Going
    forwards， we also have different varieties of basic attention mechanisms that
    came that were proposed before self retention so the first variety here is global
    attention models so in global attention models for each hidden。
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Layer input even layer output you learn an attention weight A of P and this
    is element wise multiply with your current output to calculate your final outputs
    Y similarly you have local attention models where instead of calculating the global
    attention over the whole sequence length you only calculate the attention over
    a small window and then you wait by the attention of the window into like the
    current output to get like the final output you need。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: So moving on， I'll pass on to Chattanya to discuss cell retention mechanisms
    and classrooms。Thank you Dr for covering a brief overview of how the primitive
    versions of attention worked now just before we talk about selfat just a bit of
    a trivia that the term was first introduced by a paper which from Lial which was
    which provided a framework for a self-attentive self-attentive mechanism for sentence
    and and now moving on to the main crux of the transformers paper which was the
    self-at block so self-atten is the basis is the main building block for how for
    what makes a transformers model work so well and to enable them and make them
    so powerful so to think of it more easily we can break down the self-at as a search
    rettrial problem so the problem is that given a query queue and we need to find
    a set of key scale which are most similar to Q and return the corresponding key
    values called V。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Now these three vectors can be drawn from the same source， for example。we can
    have that Q k and V are all equal to a single vector x where x can be an output
    of a previous layer。And transformers。These vectors are obtained by applying different
    linear transformations to ws so as to enable the model to capture more complex
    interactions between the different tokens at different places of the sentence
    now how attention is computed is just a weighted summation of the similarities
    between the query and key vectors which is weighted by the respective value for
    those keys。
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这三个向量可以从同一个来源绘制，例如，我们可以将 Q、k 和 V 都等于一个单一的向量 x，其中 x 可以是前一层的输出。变压器。这些向量是通过对
    ws 应用不同的线性变换获得的，以使模型能够捕捉句子中不同位置不同符号之间的更复杂交互。现在注意力的计算只是查询向量和键向量之间相似度的加权求和，权重由这些键的各自值加权。
- en: And in the transformformers paper， they use the scaled dot product as a similarity
    function for the queries and keys。And another important aspect of the transformers
    was the introduction of multih cell retention。so what multih self retention means
    is that the cell retention is for at every layer。the self retention is performed
    multiple times which enables the model to learn multiple representation subspaces。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: So in a way you can think of it that each head has a power to look at different
    things and to learn different semantics。for example， one head can be learning
    to try to predict what is the part of speech for those tokens。one head might be
    learning， what is the syntactic structure or the sentence and all those things
    that are there to understand what the upcoming sentence means。Now to better understand
    what the self attention works and what are the different computations。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，从某种意义上来说，你可以认为每个头都有能力关注不同的事物并学习不同的语义。例如，一个头可能在学习尝试预测那些标记的词性。另一个头可能在学习句子的句法结构，以及理解即将到来的句子的含义所需的所有内容。现在，为了更好地理解自注意力的工作原理以及不同的计算。
- en: there is a short video。あそう that。In this， so as you can see， there are three
    incoming tokens。so input one input to input three， we apply linear transformations
    to get the key value vectors for each input and then give one a query cube comes。we
    calculate similarity with the key respective key vectors and then multiply those
    scores with the value vector and then add them all up to get the output。The same
    computation is then performed on all the tokens and we get the output of the self
    potential layer so as you can see here the final output of the self potential
    layer is in dark green that's at the top of the screen。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个简短的视频。あそう那。在这里，你可以看到有三个输入标记。输入一个到输入三，我们应用线性变换来获取每个输入的关键值向量，然后给出一个查询立方体。我们计算与各自关键向量的相似性，然后将这些分数与值向量相乘，最后将它们全部相加以获得输出。然后对所有标记执行相同的计算，因此你可以看到，自注意层的最终输出是深绿色的，位于屏幕的顶部。
- en: So now again for the final token， we perform everything same queries multiplied
    by keys。we get the similarities scores， and then those similarities scores be
    the value vectors。and then we finally perform the education to get the sales attention
    output of the transformers。来。part from self-re there are some other necessary
    ingredients that makes the transformers so powerful one important aspect is the
    presence of positional representations or the embedding layer so the way or inance
    worked very well was that since they process each the information in a sequential
    ordering so therefore this notion of ordering right and which is also very important
    in understanding language because we all know that we read any piece of text from
    left to right in most in most of the languages and also right to left in some
    languages so there is a notion of ordering which is lost in kind of selfre because
    every word is is attending to every other world that's why this paper introduced
    separate embedding layer for introducing position representations。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The second important aspect is having nonlinearities。so if you think of all
    the competition that is happening in the cell potential layer it's all linear
    because it's all metrics multiplication。but as you all know that deep learning
    models work well when are to when they are able to learn more complex mappings
    between input and output which can be attained by a simple MLP。And the third important
    component of the self of the transformers is the masking。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: so masking is what allows to paralyze the operations since every word can attend
    to every other word in the decoder part of the transformers which otherwise is
    going to be talking about later is the problem comes that you don't want the decoder
    to look into the future because that can result in data leakage so that's why
    masking helps the decoder to avoid that future information and learn only what
    has been what the model has processed so far。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，掩蔽是使操作能够被禁用的原因，因为在变换器的解码器部分，每个词可以关注其他所有词，而接下来要讨论的问题是，你不希望解码器查看未来的信息，因为这可能导致数据泄露，所以掩蔽有助于解码器避免未来信息的影响，只学习到目前为止模型所处理的内容。
- en: So now on the encoder E architecture of the transformers。Yeah， thanks。Sasonia
    for talking about self retention。So self-atten is sort of the key ingredient or
    one of the key ingredients that allows transformers to work so well。but at a very
    high level the model that was proposed in the vwa etal paper of 2017 was like
    previous language models in the sense that it had an encoded decoder architecture
    what that means is let's say you're working on a translation problem you want
    to translate English to French the way that would work is you would read in the
    entire input of your English sentence you would encode that input so that's the
    encoder part of the network and then you would generate token by token the corresponding
    French translation and the decoder is the part of the network that is responsible
    for generating those tokens。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: So you can think of these encoder blocks and decoder blocks as essentially something
    like Lego。they have these subcompons that make them up and in particular the encoder
    block has three main subcomps the first is a self retention layer that Cheania
    talked about earlier and as talked about earlier as well。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可以把这些编码器块和解码器块看作是类似乐高的东西。它们由这些子组件组成，特别是编码器块有三个主要的子组件，第一个是自我保留层，之前的Cheania也提到过这一点。
- en: you need a feet forward layer after that because the self retention layer only
    performs linear operations and so you need something that can capture the non-
    nonlinearities。You also have a layer norm after this， and lastly， there are residual
    connections between different encoder blocks。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 之后你需要一个前馈层，因为自我保留层只执行线性操作，因此你需要一些能够捕捉非线性特征的东西。你在这之后还有一个层归一化，最后，不同编码器块之间有残差连接。
- en: The decoder is very similar to the encoder， but there's one difference which
    is that it has this extra layer because the decoder doesn't just do multiha attention
    on the output of the previous layers。so for context the encoder does multiha attention
    each self retention layer in the encoder block。
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器与编码器非常相似，但有一个区别，就是它多了一层，因为解码器不仅对前一层的输出进行多头注意力处理。所以为了提供背景，编码器在编码块中的每个自注意力层都进行多头注意力处理。
- en: In each of the encoded blocks it does。Multiha attention looking at the previous
    layers of the encoder blocks。the decoder， however， does that in the sense that
    it also looks at the previous layers of the decoder。but it also looks at the output
    of the encoder and so it needs us a multi head attention layer over the encoder
    blocks。And lastly there's masking as well so if you are because every token can
    look at every other token you want to sort of make sure in the decoder that you're
    not looking into the future so if you're in position three for instance。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: you shouldn't be able to look at position four and position five。W持。So those
    are sort of all the components that led to the creation of the model in the Vaswani
    atal paper and。😊，Let's talk a little bit about the advantages and drawbacks of
    this model。So the two main advantages， which are huge advantages and which are
    why transformers have done such a good job of revolutionizing many。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: many fields within deep learning are as follows so so the first is there is
    this constant part length between any two positions in a sequence because every。Token
    in the sequences looking at every other token and this basically solves the problem
    that di talked about earlier with long sequences you don't have this problem with
    long sequences where if you're trying to predict a token that depends on a word。
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: That was far far behind in a sentence you don't have the problem of losing that
    context now the distance between them is only one in terms of the part length
    also because the nature of the computation that's happening transformer model
    lend themselves really well to parallellyization and because of the advances that
    we've had with GPUs basically if you take a transformer model with n parameters
    and you take a model that isn't a transformer say like an MSTM with also with
    n parameters training the transformer model is going to be much faster because
    of the parallellyization that it leverages。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这在很久以前的一个句子中，你现在没有失去上下文的问题，二者之间的距离在部分长度上仅为一。同时，由于正在发生的计算的性质，变换器模型非常适合并行化，并且由于我们在GPU方面取得的进展，基本上，如果你使用一个具有n个参数的变换器模型，和一个不是变换器的模型，比如一个MSTM，同样也具有n个参数，那么训练变换器模型将会快得多，因为它利用了并行化的优势。
- en: So those are the advantages， the disadvantages are basically selfatten takes
    quadratic time because every token looks at every other token or n square as you
    might know does not scale and there's actually been a lot of work in trying to
    tackle this。so we've linked to some here big bird leanformer and reformer are
    all approaches to try and make this linear or quasiy linear essentially。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 😊，And yeah， we highly recommend to recommend going through J Aammer's block。the
    Illustrated transformer， which provides great visualizations and explains everything
    that we just talked about in great detail。Yeah， and I'd like to pass it on to
    Chaanya for applications of transformers。Yeah。so now moving on to some of the
    recent work， some of the work that very shortly followed the Transers paper。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: so one of the models that came out was GPT， the GPT architecture which was released
    by OpenEI so OpenI had the latest model that Openia has in the GPT series is the
    GPT3 so it consists of only the decoder blocks from transformers and is strain
    on our traditional language modeling task which is predicting the current token
    which is creating the next token given the last T tokens the model has seen and
    for any downstream task now the model can just you can just train a classification
    layer on the last hidden state can be which can have any number of labels and
    since the model is generative in nature。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: you can also use the pretrained network as for generative kind of tasks such
    as summarizemarization and natural language and natural language。Re。For that instance。Another
    important aspect that GPT3 gained popularity was its ability to be able to perform
    in context learning what the authors called into context learning so this is the
    ability wherein the model can perform can learn under few short settings what
    the task is to complete the task without performing any great inputs for example
    let's say the model is shown a bunch of addition examples and then if you pass
    in a new input and leave and just leave it at equal to sign the model tries to
    predict the next token which very well comes out to be the sum of the the sum
    of the numbers that is shown another example can be also the spell correction
    task or the translation task so this was the ability that made GT3 so much talked
    about in the NLP world and right now also like many applications have been made
    using GPT。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Which includes one of them being the VS code Copit。which tries to generate a
    piece of code given top string kind of natural language text。Another major model
    that came out that was based on the transformers architecture was Bt。so BRT lends
    it name from it's an acronym for bidirectional encoding encoder representations
    of transformers。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: it consists of only the encoder blocks of the transformers which is unlike GPT3
    which had only the decoder blocks。Now this， because of this change there there
    comes a problem because BRT has only the encoder block。so it sees the entire piece
    of text， it cannot be pretrained on a knife language modern task because of the
    problem of data leakage from the future。So what the authors came up with was a
    clever idea and they came up with a novel task called mass language modeling。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: which included to replace certain birds with a placeholder and then the model
    tries to predict those words given the entire context。Now apart from this total
    level task there was the authors also added a second objective called the next
    sentence prediction。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: which was a sentence level task wherein given two chunks of text。the model tried
    to predict whether the second sentence followed the other sentence or not followed
    the first sentence or not and now for after pretraining this model for any downstream
    task。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: the model can be further fine tuned within additional classification layer just
    like it was in GPP3。So these are the two models that were that have been like
    very popular and have made a lot of applications made their way in a lot of applications。but
    the landscape has changed quite a lot since we have taken this class there are
    models with different techniques like E deerta and there are also models that
    do well in like other modalities and which we are going to be talking about in
    other lecture seriess as well so yeah that's all from this lecture and thank you
    for tuning in。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Yeah， to just want to end by saying thank you all for watching this and we have
    a really exciting set of videos with truly amazing speakers and we hope you are
    able to derive value from that。![](img/5ecc633c0934862bc1bde6086b437e97_3.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: ， thanks Mark， thank you， thank you everyone。![](img/5ecc633c0934862bc1bde6086b437e97_5.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
