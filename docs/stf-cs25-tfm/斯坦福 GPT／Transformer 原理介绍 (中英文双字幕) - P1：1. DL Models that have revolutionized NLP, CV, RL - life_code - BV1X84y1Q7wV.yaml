- en: ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P1Ôºö1. DL Models that have revolutionized
    NLP, CV, RL - life_code - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/5ecc633c0934862bc1bde6086b437e97_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5ecc633c0934862bc1bde6086b437e97_1.png)'
  prefs: []
  type: TYPE_IMG
- en: Hey everyone welcome to the first and introductory lecture for CS25 Transformers
    United so CS25 was a class that the three of us created and taught at Stanford
    in the fall of 2021 and the subject of the class is not as the picture might suggest
    it's not about robots that can transform into cars it's about deep learning models
    and specifically a particular kind of deep learning models that have revolutionized
    multiple fields starting from natural language processing to things like computer
    vision and reinforcement learning to name a few we have an exciting set of videos
    lined up for you we have some truly fantastic speakers come gift talks about how
    they were applying transformers in their own research„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And we hope you will enjoy and learn from these talks this video is purely an
    introductory lecture to talk a little bit about transformers and before we get
    started I'd like to introduce the instructors so my name is A I am a software
    engineer at a company called appliedlied Intuition Before this I was a master
    student ins at Stanford„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And„ÄÇI am one of the Co instructorors for CS 25Ôºå Chaanya Dave„ÄÇif the two of you
    could introduce yourselves„ÄÇSo hi everyone I am a PhD student at exampleford before
    this I was pursuing a master's here researching a lot in generative modeling enforcement
    learning and robots so nice to youall yeah that was Dave forceden say his name
    Chaanya if you want to introduce yourself Yeah hi everyone my name is Chaania
    and I'm currently working as an ML engineer at a startup called BoWorks before
    that I was a masters rate at Stanford specializing in NLP and was a member of
    the prize winning Stanford team for the Alexa Pri Challenge„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: All rightÔºå awesome„ÄÇSo„ÄÇMoving on to the rest of this talk„ÄÇessentially what we
    hope you will learn watching these videos and what we hope the people who took
    our class in the fall of 2021 learned is three things one is we hope you will
    have an understanding of how transformers work„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: SecondlyÔºå we hope you will learn and by the end of these talks understand how
    transformers are being applied beyond just natural language processing„ÄÇAnd thirdlyÔºå
    we hope that some of these talks will spark some new ideas within you and hopefully
    lead to new directions of research„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: new kinds of innovationÔºå and things of that sort„ÄÇAnd„ÄÇTo begin„ÄÇwe're going to
    talk a little bit about transformers and introduce some of the context behind
    transformers as well„ÄÇand for that I'd like to hand it off to Dave„ÄÇÂóØ„ÄÇSo hi everyone
    so welcome to our Transformer seminar so I'll start first with an overview of
    the attention timeline and how it came to be the key idea about transformers was
    the sub attention mechanism that was developed in 2017 and this all started with
    this point paper called attention is all you need was 22 before 2017 we used to
    have this prehistoric era where we had older models like RNs LSTMs and simpler
    attention mechanisms and eventually like the growth in transformers has explored
    into other fields and has become prominent in like all of machine learning and
    I'll go and see and show how this has been used so in the prehistoric era there
    used to be RNLs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: There were different models like the sequence sequence LSTMs G they were good
    at encoding some sort of memory but they did not work for encoding long sequences
    and they were very bad at encoding context so here is an example where if you
    have a sentence like I grew up in France dot dot dot so I speak fluent dash then
    you want to fill this with like French based on context but like LST models might
    not know what it is and might just make a very big mistake here similarly we can
    show some sort of correlation map here where if you have a pronoun like it we
    wanted to correlate to one of the past nouns that you have seen so far like an
    animal but again older models were like really not good at this context encoding„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So where we are currently now is on the verge of takeoff we are beginning to
    realize the potential of transformers in different fields„ÄÇwe have started to use
    them to solve long sequence problems in protein folding such as like the alpha
    fold model from open from deep which is 95% accuracy on different challenges in
    offlinearrow we can use it for few shortard zero centralization for text and image
    we can also use this for like content generation so here's example from openAI
    where you can give a different text prompt and have an AI gen fictional image
    for you„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so there's a contestus that you can also watch on YouTube„ÄÇwhich basically
    says that LS students are died and long lived transformers„ÄÇSo what's the future
    so we can enable a lot more applications for transformers they can be applied
    to any form of sequence modeling so we can use we could use them for really understanding
    we can use them for finance and a lot more so basically imagine all sorts of genetic
    modeling problems„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless there are a lot of missing ingredientss so like the human brain
    we need some sort of external memory unit which is the hippocampus first and they
    are saying some early works here so one nice work you might want to check out
    is called neural curing machines similarly the current attention mechanisms are
    very computationally complex in terms of time and this scale corically which we
    will discuss later and we want to make them more linear and the third problem
    is like we want to align our current sort of language models with how the human
    brain works and human values and this is also a big issue„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so now I will deep dive deep dive I will„ÄÇDive deeper into the tension
    mechanisms and show how they came out to„ÄÇSo initially there used to be very simple
    mechanisms where attention was inspired by the process of importance ratinging„ÄÇoperating
    attention on different parts of image where like similar to human where you might
    focus more on like a foreground if you have a image of a do compared to like the
    rest of background so in the case of soft attention what you do is you learn the
    simple soft attention weighting for each pixel which can be weight between0 to1
    the problem over here is that this is a very expensive computation and then you
    can show as it' shown in the figure on the left you can see we are calculating
    this attention for the whole image what you can do instead is you can„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Clic a zero to one attention map where we directly put a one on where where
    the dog is and a zero wherever it's a background this is like less competition
    expensive but the problem is it's not defensiable and makes things harder to train„ÄÇGoing
    forwardsÔºå we also have different varieties of basic attention mechanisms that
    came that were proposed before self retention so the first variety here is global
    attention models so in global attention models for each hidden„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Layer input even layer output you learn an attention weight A of P and this
    is element wise multiply with your current output to calculate your final outputs
    Y similarly you have local attention models where instead of calculating the global
    attention over the whole sequence length you only calculate the attention over
    a small window and then you wait by the attention of the window into like the
    current output to get like the final output you need„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So moving onÔºå I'll pass on to Chattanya to discuss cell retention mechanisms
    and classrooms„ÄÇThank you Dr for covering a brief overview of how the primitive
    versions of attention worked now just before we talk about selfat just a bit of
    a trivia that the term was first introduced by a paper which from Lial which was
    which provided a framework for a self-attentive self-attentive mechanism for sentence
    and and now moving on to the main crux of the transformers paper which was the
    self-at block so self-atten is the basis is the main building block for how for
    what makes a transformers model work so well and to enable them and make them
    so powerful so to think of it more easily we can break down the self-at as a search
    rettrial problem so the problem is that given a query queue and we need to find
    a set of key scale which are most similar to Q and return the corresponding key
    values called V„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now these three vectors can be drawn from the same sourceÔºå for example„ÄÇwe can
    have that Q k and V are all equal to a single vector x where x can be an output
    of a previous layer„ÄÇAnd transformers„ÄÇThese vectors are obtained by applying different
    linear transformations to ws so as to enable the model to capture more complex
    interactions between the different tokens at different places of the sentence
    now how attention is computed is just a weighted summation of the similarities
    between the query and key vectors which is weighted by the respective value for
    those keys„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And in the transformformers paperÔºå they use the scaled dot product as a similarity
    function for the queries and keys„ÄÇAnd another important aspect of the transformers
    was the introduction of multih cell retention„ÄÇso what multih self retention means
    is that the cell retention is for at every layer„ÄÇthe self retention is performed
    multiple times which enables the model to learn multiple representation subspaces„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So in a way you can think of it that each head has a power to look at different
    things and to learn different semantics„ÄÇfor exampleÔºå one head can be learning
    to try to predict what is the part of speech for those tokens„ÄÇone head might be
    learningÔºå what is the syntactic structure or the sentence and all those things
    that are there to understand what the upcoming sentence means„ÄÇNow to better understand
    what the self attention works and what are the different computations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there is a short video„ÄÇ„ÅÇ„Åù„ÅÜ that„ÄÇIn thisÔºå so as you can seeÔºå there are three
    incoming tokens„ÄÇso input one input to input threeÔºå we apply linear transformations
    to get the key value vectors for each input and then give one a query cube comes„ÄÇwe
    calculate similarity with the key respective key vectors and then multiply those
    scores with the value vector and then add them all up to get the output„ÄÇThe same
    computation is then performed on all the tokens and we get the output of the self
    potential layer so as you can see here the final output of the self potential
    layer is in dark green that's at the top of the screen„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now again for the final tokenÔºå we perform everything same queries multiplied
    by keys„ÄÇwe get the similarities scoresÔºå and then those similarities scores be
    the value vectors„ÄÇand then we finally perform the education to get the sales attention
    output of the transformers„ÄÇÊù•„ÄÇpart from self-re there are some other necessary
    ingredients that makes the transformers so powerful one important aspect is the
    presence of positional representations or the embedding layer so the way or inance
    worked very well was that since they process each the information in a sequential
    ordering so therefore this notion of ordering right and which is also very important
    in understanding language because we all know that we read any piece of text from
    left to right in most in most of the languages and also right to left in some
    languages so there is a notion of ordering which is lost in kind of selfre because
    every word is is attending to every other world that's why this paper introduced
    separate embedding layer for introducing position representations„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The second important aspect is having nonlinearities„ÄÇso if you think of all
    the competition that is happening in the cell potential layer it's all linear
    because it's all metrics multiplication„ÄÇbut as you all know that deep learning
    models work well when are to when they are able to learn more complex mappings
    between input and output which can be attained by a simple MLP„ÄÇAnd the third important
    component of the self of the transformers is the masking„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so masking is what allows to paralyze the operations since every word can attend
    to every other word in the decoder part of the transformers which otherwise is
    going to be talking about later is the problem comes that you don't want the decoder
    to look into the future because that can result in data leakage so that's why
    masking helps the decoder to avoid that future information and learn only what
    has been what the model has processed so far„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So now on the encoder E architecture of the transformers„ÄÇYeahÔºå thanks„ÄÇSasonia
    for talking about self retention„ÄÇSo self-atten is sort of the key ingredient or
    one of the key ingredients that allows transformers to work so well„ÄÇbut at a very
    high level the model that was proposed in the vwa etal paper of 2017 was like
    previous language models in the sense that it had an encoded decoder architecture
    what that means is let's say you're working on a translation problem you want
    to translate English to French the way that would work is you would read in the
    entire input of your English sentence you would encode that input so that's the
    encoder part of the network and then you would generate token by token the corresponding
    French translation and the decoder is the part of the network that is responsible
    for generating those tokens„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you can think of these encoder blocks and decoder blocks as essentially something
    like Lego„ÄÇthey have these subcompons that make them up and in particular the encoder
    block has three main subcomps the first is a self retention layer that Cheania
    talked about earlier and as talked about earlier as well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you need a feet forward layer after that because the self retention layer only
    performs linear operations and so you need something that can capture the non-
    nonlinearities„ÄÇYou also have a layer norm after thisÔºå and lastlyÔºå there are residual
    connections between different encoder blocks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The decoder is very similar to the encoderÔºå but there's one difference which
    is that it has this extra layer because the decoder doesn't just do multiha attention
    on the output of the previous layers„ÄÇso for context the encoder does multiha attention
    each self retention layer in the encoder block„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: In each of the encoded blocks it does„ÄÇMultiha attention looking at the previous
    layers of the encoder blocks„ÄÇthe decoderÔºå howeverÔºå does that in the sense that
    it also looks at the previous layers of the decoder„ÄÇbut it also looks at the output
    of the encoder and so it needs us a multi head attention layer over the encoder
    blocks„ÄÇAnd lastly there's masking as well so if you are because every token can
    look at every other token you want to sort of make sure in the decoder that you're
    not looking into the future so if you're in position three for instance„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you shouldn't be able to look at position four and position five„ÄÇWÊåÅ„ÄÇSo those
    are sort of all the components that led to the creation of the model in the Vaswani
    atal paper and„ÄÇüòäÔºåLet's talk a little bit about the advantages and drawbacks of
    this model„ÄÇSo the two main advantagesÔºå which are huge advantages and which are
    why transformers have done such a good job of revolutionizing many„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: many fields within deep learning are as follows so so the first is there is
    this constant part length between any two positions in a sequence because every„ÄÇToken
    in the sequences looking at every other token and this basically solves the problem
    that di talked about earlier with long sequences you don't have this problem with
    long sequences where if you're trying to predict a token that depends on a word„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That was far far behind in a sentence you don't have the problem of losing that
    context now the distance between them is only one in terms of the part length
    also because the nature of the computation that's happening transformer model
    lend themselves really well to parallellyization and because of the advances that
    we've had with GPUs basically if you take a transformer model with n parameters
    and you take a model that isn't a transformer say like an MSTM with also with
    n parameters training the transformer model is going to be much faster because
    of the parallellyization that it leverages„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So those are the advantagesÔºå the disadvantages are basically selfatten takes
    quadratic time because every token looks at every other token or n square as you
    might know does not scale and there's actually been a lot of work in trying to
    tackle this„ÄÇso we've linked to some here big bird leanformer and reformer are
    all approaches to try and make this linear or quasiy linear essentially„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåAnd yeahÔºå we highly recommend to recommend going through J Aammer's block„ÄÇthe
    Illustrated transformerÔºå which provides great visualizations and explains everything
    that we just talked about in great detail„ÄÇYeahÔºå and I'd like to pass it on to
    Chaanya for applications of transformers„ÄÇYeah„ÄÇso now moving on to some of the
    recent workÔºå some of the work that very shortly followed the Transers paper„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so one of the models that came out was GPTÔºå the GPT architecture which was released
    by OpenEI so OpenI had the latest model that Openia has in the GPT series is the
    GPT3 so it consists of only the decoder blocks from transformers and is strain
    on our traditional language modeling task which is predicting the current token
    which is creating the next token given the last T tokens the model has seen and
    for any downstream task now the model can just you can just train a classification
    layer on the last hidden state can be which can have any number of labels and
    since the model is generative in nature„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you can also use the pretrained network as for generative kind of tasks such
    as summarizemarization and natural language and natural language„ÄÇRe„ÄÇFor that instance„ÄÇAnother
    important aspect that GPT3 gained popularity was its ability to be able to perform
    in context learning what the authors called into context learning so this is the
    ability wherein the model can perform can learn under few short settings what
    the task is to complete the task without performing any great inputs for example
    let's say the model is shown a bunch of addition examples and then if you pass
    in a new input and leave and just leave it at equal to sign the model tries to
    predict the next token which very well comes out to be the sum of the the sum
    of the numbers that is shown another example can be also the spell correction
    task or the translation task so this was the ability that made GT3 so much talked
    about in the NLP world and right now also like many applications have been made
    using GPT„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Which includes one of them being the VS code Copit„ÄÇwhich tries to generate a
    piece of code given top string kind of natural language text„ÄÇAnother major model
    that came out that was based on the transformers architecture was Bt„ÄÇso BRT lends
    it name from it's an acronym for bidirectional encoding encoder representations
    of transformers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it consists of only the encoder blocks of the transformers which is unlike GPT3
    which had only the decoder blocks„ÄÇNow thisÔºå because of this change there there
    comes a problem because BRT has only the encoder block„ÄÇso it sees the entire piece
    of textÔºå it cannot be pretrained on a knife language modern task because of the
    problem of data leakage from the future„ÄÇSo what the authors came up with was a
    clever idea and they came up with a novel task called mass language modeling„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which included to replace certain birds with a placeholder and then the model
    tries to predict those words given the entire context„ÄÇNow apart from this total
    level task there was the authors also added a second objective called the next
    sentence prediction„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which was a sentence level task wherein given two chunks of text„ÄÇthe model tried
    to predict whether the second sentence followed the other sentence or not followed
    the first sentence or not and now for after pretraining this model for any downstream
    task„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the model can be further fine tuned within additional classification layer just
    like it was in GPP3„ÄÇSo these are the two models that were that have been like
    very popular and have made a lot of applications made their way in a lot of applications„ÄÇbut
    the landscape has changed quite a lot since we have taken this class there are
    models with different techniques like E deerta and there are also models that
    do well in like other modalities and which we are going to be talking about in
    other lecture seriess as well so yeah that's all from this lecture and thank you
    for tuning in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå to just want to end by saying thank you all for watching this and we have
    a really exciting set of videos with truly amazing speakers and we hope you are
    able to derive value from that„ÄÇ![](img/5ecc633c0934862bc1bde6086b437e97_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: Ôºå thanks MarkÔºå thank youÔºå thank you everyone„ÄÇ![](img/5ecc633c0934862bc1bde6086b437e97_5.png)
  prefs: []
  type: TYPE_NORMAL
