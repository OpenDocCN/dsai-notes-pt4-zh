- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P1ï¼š1. DL Models that have revolutionized
    NLP, CV, RL - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P1ï¼š1. æ”¹å˜ NLPã€CVã€RL çš„ DL æ¨¡å‹ - life_code -
    BV1X84y1Q7wV
- en: '![](img/5ecc633c0934862bc1bde6086b437e97_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
- en: '![](img/5ecc633c0934862bc1bde6086b437e97_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
- en: Hey everyone welcome to the first and introductory lecture for CS25 Transformers
    United so CS25 was a class that the three of us created and taught at Stanford
    in the fall of 2021 and the subject of the class is not as the picture might suggest
    it's not about robots that can transform into cars it's about deep learning models
    and specifically a particular kind of deep learning models that have revolutionized
    multiple fields starting from natural language processing to things like computer
    vision and reinforcement learning to name a few we have an exciting set of videos
    lined up for you we have some truly fantastic speakers come gift talks about how
    they were applying transformers in their own researchã€‚
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: And we hope you will enjoy and learn from these talks this video is purely an
    introductory lecture to talk a little bit about transformers and before we get
    started I'd like to introduce the instructors so my name is A I am a software
    engineer at a company called appliedlied Intuition Before this I was a master
    student ins at Stanfordã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›æ‚¨èƒ½äº«å—å¹¶ä»è¿™äº›è®²åº§ä¸­å­¦ä¹ ï¼Œè¿™æ®µè§†é¢‘çº¯ç²¹æ˜¯ä»‹ç»æ€§è®²åº§ï¼Œè°ˆè®ºä¸€ä¸‹å˜å‹å™¨ã€‚åœ¨å¼€å§‹ä¹‹å‰ï¼Œæˆ‘æƒ³ä»‹ç»ä¸€ä¸‹è®²å¸ˆï¼Œæ‰€ä»¥æˆ‘å«Aï¼Œæ˜¯ä¸€å®¶åä¸ºApplied Intuitionçš„å…¬å¸çš„è½¯ä»¶å·¥ç¨‹å¸ˆã€‚åœ¨æ­¤ä¹‹å‰ï¼Œæˆ‘æ˜¯åœ¨æ–¯å¦ç¦å¤§å­¦çš„ç¡•å£«ç”Ÿã€‚
- en: Andã€‚I am one of the Co instructorors for CS 25ï¼Œ Chaanya Daveã€‚if the two of you
    could introduce yourselvesã€‚So hi everyone I am a PhD student at exampleford before
    this I was pursuing a master's here researching a lot in generative modeling enforcement
    learning and robots so nice to youall yeah that was Dave forceden say his name
    Chaanya if you want to introduce yourself Yeah hi everyone my name is Chaania
    and I'm currently working as an ML engineer at a startup called BoWorks before
    that I was a masters rate at Stanford specializing in NLP and was a member of
    the prize winning Stanford team for the Alexa Pri Challengeã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: All rightï¼Œ awesomeã€‚Soã€‚Moving on to the rest of this talkã€‚essentially what we
    hope you will learn watching these videos and what we hope the people who took
    our class in the fall of 2021 learned is three things one is we hope you will
    have an understanding of how transformers workã€‚
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œå¤ªæ£’äº†ã€‚é‚£ä¹ˆï¼Œæ¥ä¸‹æ¥æˆ‘ä»¬è¦è°ˆè®ºè¿™ä¸ªè®²åº§çš„å…¶ä»–å†…å®¹ã€‚æˆ‘ä»¬å¸Œæœ›ä½ åœ¨è§‚çœ‹è¿™äº›è§†é¢‘æ—¶èƒ½å¤Ÿå­¦åˆ°çš„ï¼Œä»¥åŠæˆ‘ä»¬å¸Œæœ›åœ¨2021å¹´ç§‹å­£å‚åŠ æˆ‘ä»¬è¯¾ç¨‹çš„äººå­¦åˆ°çš„æœ‰ä¸‰ä»¶äº‹ï¼Œç¬¬ä¸€ä»¶æ˜¯æˆ‘ä»¬å¸Œæœ›ä½ èƒ½ç†è§£å˜å‹å™¨çš„å·¥ä½œåŸç†ã€‚
- en: Secondlyï¼Œ we hope you will learn and by the end of these talks understand how
    transformers are being applied beyond just natural language processingã€‚And thirdlyï¼Œ
    we hope that some of these talks will spark some new ideas within you and hopefully
    lead to new directions of researchã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: new kinds of innovationï¼Œ and things of that sortã€‚Andã€‚To beginã€‚we're going to
    talk a little bit about transformers and introduce some of the context behind
    transformers as wellã€‚and for that I'd like to hand it off to Daveã€‚å—¯ã€‚So hi everyone
    so welcome to our Transformer seminar so I'll start first with an overview of
    the attention timeline and how it came to be the key idea about transformers was
    the sub attention mechanism that was developed in 2017 and this all started with
    this point paper called attention is all you need was 22 before 2017 we used to
    have this prehistoric era where we had older models like RNs LSTMs and simpler
    attention mechanisms and eventually like the growth in transformers has explored
    into other fields and has become prominent in like all of machine learning and
    I'll go and see and show how this has been used so in the prehistoric era there
    used to be RNLsã€‚
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: There were different models like the sequence sequence LSTMs G they were good
    at encoding some sort of memory but they did not work for encoding long sequences
    and they were very bad at encoding context so here is an example where if you
    have a sentence like I grew up in France dot dot dot so I speak fluent dash then
    you want to fill this with like French based on context but like LST models might
    not know what it is and might just make a very big mistake here similarly we can
    show some sort of correlation map here where if you have a pronoun like it we
    wanted to correlate to one of the past nouns that you have seen so far like an
    animal but again older models were like really not good at this context encodingã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸åŒçš„æ¨¡å‹ï¼Œæ¯”å¦‚åºåˆ—åºåˆ—LSTMï¼Œå®ƒä»¬åœ¨ç¼–ç æŸç§è®°å¿†æ–¹é¢è¡¨ç°ä¸é”™ï¼Œä½†åœ¨ç¼–ç é•¿åºåˆ—æ—¶æ•ˆæœä¸ä½³ï¼Œå¹¶ä¸”åœ¨ç¼–ç ä¸Šä¸‹æ–‡æ–¹é¢ä¹Ÿéå¸¸å·®ã€‚å› æ­¤è¿™é‡Œæœ‰ä¸€ä¸ªä¾‹å­ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªå¥å­ï¼Œæ¯”å¦‚â€œæˆ‘åœ¨æ³•å›½é•¿å¤§â€¦â€¦æ‰€ä»¥æˆ‘è¯´æµåˆ©çš„â€ï¼Œé‚£ä¹ˆä½ æƒ³æ ¹æ®ä¸Šä¸‹æ–‡å¡«å…¥ä¸€äº›åƒæ³•è¯­çš„å†…å®¹ï¼Œä½†åƒLSTMæ¨¡å‹å¯èƒ½ä¸çŸ¥é“è¿™æ˜¯ä»€ä¹ˆï¼Œå¯èƒ½ä¼šåœ¨è¿™é‡ŒçŠ¯ä¸€ä¸ªå¾ˆå¤§çš„é”™è¯¯ã€‚åŒæ ·ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨è¿™é‡Œå±•ç¤ºæŸç§ç›¸å…³æ€§å›¾è°±ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªä»£è¯åƒâ€œå®ƒâ€ï¼Œæˆ‘ä»¬æƒ³è¦å°†å…¶ä¸ä¹‹å‰çœ‹åˆ°çš„æŸä¸ªåè¯ç›¸å…³è”ï¼Œæ¯”å¦‚ä¸€ä¸ªåŠ¨ç‰©ï¼Œä½†åŒæ ·ï¼Œæ—§æ¨¡å‹åœ¨è¿™ç§ä¸Šä¸‹æ–‡ç¼–ç ä¸ŠçœŸçš„å¾ˆå·®ã€‚
- en: So where we are currently now is on the verge of takeoff we are beginning to
    realize the potential of transformers in different fieldsã€‚we have started to use
    them to solve long sequence problems in protein folding such as like the alpha
    fold model from open from deep which is 95% accuracy on different challenges in
    offlinearrow we can use it for few shortard zero centralization for text and image
    we can also use this for like content generation so here's example from openAI
    where you can give a different text prompt and have an AI gen fictional image
    for youã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ç›®å‰æˆ‘ä»¬æ­£å¤„äºèµ·é£çš„è¾¹ç¼˜ï¼Œæˆ‘ä»¬å¼€å§‹æ„è¯†åˆ°å˜å‹å™¨åœ¨ä¸åŒé¢†åŸŸçš„æ½œåŠ›ã€‚æˆ‘ä»¬å·²ç»å¼€å§‹ä½¿ç”¨å®ƒä»¬æ¥è§£å†³è›‹ç™½è´¨æŠ˜å ä¸­çš„é•¿åºåˆ—é—®é¢˜ï¼Œæ¯”å¦‚æ¥è‡ªDeepMindçš„AlphaFoldæ¨¡å‹ï¼Œåœ¨ä¸åŒæŒ‘æˆ˜ä¸­çš„å‡†ç¡®ç‡è¾¾åˆ°95%ã€‚æˆ‘ä»¬å¯ä»¥å°†å…¶ç”¨äºæ–‡æœ¬å’Œå›¾åƒçš„å°‘é‡çŸ­æœŸæ— ä¸­å¿ƒåŒ–åº”ç”¨ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç”¨å®ƒè¿›è¡Œå†…å®¹ç”Ÿæˆã€‚ä»¥ä¸‹æ˜¯æ¥è‡ªOpenAIçš„ä¸€ä¸ªä¾‹å­ï¼Œä½ å¯ä»¥ç»™å‡ºä¸åŒçš„æ–‡æœ¬æç¤ºï¼Œè®©AIä¸ºä½ ç”Ÿæˆè™šæ„çš„å›¾åƒã€‚
- en: And so there's a contestus that you can also watch on YouTubeã€‚which basically
    says that LS students are died and long lived transformersã€‚So what's the future
    so we can enable a lot more applications for transformers they can be applied
    to any form of sequence modeling so we can use we could use them for really understanding
    we can use them for finance and a lot more so basically imagine all sorts of genetic
    modeling problemsã€‚
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless there are a lot of missing ingredientss so like the human brain
    we need some sort of external memory unit which is the hippocampus first and they
    are saying some early works here so one nice work you might want to check out
    is called neural curing machines similarly the current attention mechanisms are
    very computationally complex in terms of time and this scale corically which we
    will discuss later and we want to make them more linear and the third problem
    is like we want to align our current sort of language models with how the human
    brain works and human values and this is also a big issueã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Okayï¼Œ so now I will deep dive deep dive I willã€‚Dive deeper into the tension
    mechanisms and show how they came out toã€‚So initially there used to be very simple
    mechanisms where attention was inspired by the process of importance ratingingã€‚operating
    attention on different parts of image where like similar to human where you might
    focus more on like a foreground if you have a image of a do compared to like the
    rest of background so in the case of soft attention what you do is you learn the
    simple soft attention weighting for each pixel which can be weight between0 to1
    the problem over here is that this is a very expensive computation and then you
    can show as it' shown in the figure on the left you can see we are calculating
    this attention for the whole image what you can do instead is you canã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Clic a zero to one attention map where we directly put a one on where where
    the dog is and a zero wherever it's a background this is like less competition
    expensive but the problem is it's not defensiable and makes things harder to trainã€‚Going
    forwardsï¼Œ we also have different varieties of basic attention mechanisms that
    came that were proposed before self retention so the first variety here is global
    attention models so in global attention models for each hiddenã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Layer input even layer output you learn an attention weight A of P and this
    is element wise multiply with your current output to calculate your final outputs
    Y similarly you have local attention models where instead of calculating the global
    attention over the whole sequence length you only calculate the attention over
    a small window and then you wait by the attention of the window into like the
    current output to get like the final output you needã€‚
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: So moving onï¼Œ I'll pass on to Chattanya to discuss cell retention mechanisms
    and classroomsã€‚Thank you Dr for covering a brief overview of how the primitive
    versions of attention worked now just before we talk about selfat just a bit of
    a trivia that the term was first introduced by a paper which from Lial which was
    which provided a framework for a self-attentive self-attentive mechanism for sentence
    and and now moving on to the main crux of the transformers paper which was the
    self-at block so self-atten is the basis is the main building block for how for
    what makes a transformers model work so well and to enable them and make them
    so powerful so to think of it more easily we can break down the self-at as a search
    rettrial problem so the problem is that given a query queue and we need to find
    a set of key scale which are most similar to Q and return the corresponding key
    values called Vã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: Now these three vectors can be drawn from the same sourceï¼Œ for exampleã€‚we can
    have that Q k and V are all equal to a single vector x where x can be an output
    of a previous layerã€‚And transformersã€‚These vectors are obtained by applying different
    linear transformations to ws so as to enable the model to capture more complex
    interactions between the different tokens at different places of the sentence
    now how attention is computed is just a weighted summation of the similarities
    between the query and key vectors which is weighted by the respective value for
    those keysã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è¿™ä¸‰ä¸ªå‘é‡å¯ä»¥ä»åŒä¸€ä¸ªæ¥æºç»˜åˆ¶ï¼Œä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥å°† Qã€k å’Œ V éƒ½ç­‰äºä¸€ä¸ªå•ä¸€çš„å‘é‡ xï¼Œå…¶ä¸­ x å¯ä»¥æ˜¯å‰ä¸€å±‚çš„è¾“å‡ºã€‚å˜å‹å™¨ã€‚è¿™äº›å‘é‡æ˜¯é€šè¿‡å¯¹
    ws åº”ç”¨ä¸åŒçš„çº¿æ€§å˜æ¢è·å¾—çš„ï¼Œä»¥ä½¿æ¨¡å‹èƒ½å¤Ÿæ•æ‰å¥å­ä¸­ä¸åŒä½ç½®ä¸åŒç¬¦å·ä¹‹é—´çš„æ›´å¤æ‚äº¤äº’ã€‚ç°åœ¨æ³¨æ„åŠ›çš„è®¡ç®—åªæ˜¯æŸ¥è¯¢å‘é‡å’Œé”®å‘é‡ä¹‹é—´ç›¸ä¼¼åº¦çš„åŠ æƒæ±‚å’Œï¼Œæƒé‡ç”±è¿™äº›é”®çš„å„è‡ªå€¼åŠ æƒã€‚
- en: And in the transformformers paperï¼Œ they use the scaled dot product as a similarity
    function for the queries and keysã€‚And another important aspect of the transformers
    was the introduction of multih cell retentionã€‚so what multih self retention means
    is that the cell retention is for at every layerã€‚the self retention is performed
    multiple times which enables the model to learn multiple representation subspacesã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: So in a way you can think of it that each head has a power to look at different
    things and to learn different semanticsã€‚for exampleï¼Œ one head can be learning
    to try to predict what is the part of speech for those tokensã€‚one head might be
    learningï¼Œ what is the syntactic structure or the sentence and all those things
    that are there to understand what the upcoming sentence meansã€‚Now to better understand
    what the self attention works and what are the different computationsã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œä»æŸç§æ„ä¹‰ä¸Šæ¥è¯´ï¼Œä½ å¯ä»¥è®¤ä¸ºæ¯ä¸ªå¤´éƒ½æœ‰èƒ½åŠ›å…³æ³¨ä¸åŒçš„äº‹ç‰©å¹¶å­¦ä¹ ä¸åŒçš„è¯­ä¹‰ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªå¤´å¯èƒ½åœ¨å­¦ä¹ å°è¯•é¢„æµ‹é‚£äº›æ ‡è®°çš„è¯æ€§ã€‚å¦ä¸€ä¸ªå¤´å¯èƒ½åœ¨å­¦ä¹ å¥å­çš„å¥æ³•ç»“æ„ï¼Œä»¥åŠç†è§£å³å°†åˆ°æ¥çš„å¥å­çš„å«ä¹‰æ‰€éœ€çš„æ‰€æœ‰å†…å®¹ã€‚ç°åœ¨ï¼Œä¸ºäº†æ›´å¥½åœ°ç†è§£è‡ªæ³¨æ„åŠ›çš„å·¥ä½œåŸç†ä»¥åŠä¸åŒçš„è®¡ç®—ã€‚
- en: there is a short videoã€‚ã‚ãã† thatã€‚In thisï¼Œ so as you can seeï¼Œ there are three
    incoming tokensã€‚so input one input to input threeï¼Œ we apply linear transformations
    to get the key value vectors for each input and then give one a query cube comesã€‚we
    calculate similarity with the key respective key vectors and then multiply those
    scores with the value vector and then add them all up to get the outputã€‚The same
    computation is then performed on all the tokens and we get the output of the self
    potential layer so as you can see here the final output of the self potential
    layer is in dark green that's at the top of the screenã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰ä¸€ä¸ªç®€çŸ­çš„è§†é¢‘ã€‚ã‚ãã†é‚£ã€‚åœ¨è¿™é‡Œï¼Œä½ å¯ä»¥çœ‹åˆ°æœ‰ä¸‰ä¸ªè¾“å…¥æ ‡è®°ã€‚è¾“å…¥ä¸€ä¸ªåˆ°è¾“å…¥ä¸‰ï¼Œæˆ‘ä»¬åº”ç”¨çº¿æ€§å˜æ¢æ¥è·å–æ¯ä¸ªè¾“å…¥çš„å…³é”®å€¼å‘é‡ï¼Œç„¶åç»™å‡ºä¸€ä¸ªæŸ¥è¯¢ç«‹æ–¹ä½“ã€‚æˆ‘ä»¬è®¡ç®—ä¸å„è‡ªå…³é”®å‘é‡çš„ç›¸ä¼¼æ€§ï¼Œç„¶åå°†è¿™äº›åˆ†æ•°ä¸å€¼å‘é‡ç›¸ä¹˜ï¼Œæœ€åå°†å®ƒä»¬å…¨éƒ¨ç›¸åŠ ä»¥è·å¾—è¾“å‡ºã€‚ç„¶åå¯¹æ‰€æœ‰æ ‡è®°æ‰§è¡Œç›¸åŒçš„è®¡ç®—ï¼Œå› æ­¤ä½ å¯ä»¥çœ‹åˆ°ï¼Œè‡ªæ³¨æ„å±‚çš„æœ€ç»ˆè¾“å‡ºæ˜¯æ·±ç»¿è‰²çš„ï¼Œä½äºå±å¹•çš„é¡¶éƒ¨ã€‚
- en: So now again for the final tokenï¼Œ we perform everything same queries multiplied
    by keysã€‚we get the similarities scoresï¼Œ and then those similarities scores be
    the value vectorsã€‚and then we finally perform the education to get the sales attention
    output of the transformersã€‚æ¥ã€‚part from self-re there are some other necessary
    ingredients that makes the transformers so powerful one important aspect is the
    presence of positional representations or the embedding layer so the way or inance
    worked very well was that since they process each the information in a sequential
    ordering so therefore this notion of ordering right and which is also very important
    in understanding language because we all know that we read any piece of text from
    left to right in most in most of the languages and also right to left in some
    languages so there is a notion of ordering which is lost in kind of selfre because
    every word is is attending to every other world that's why this paper introduced
    separate embedding layer for introducing position representationsã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: The second important aspect is having nonlinearitiesã€‚so if you think of all
    the competition that is happening in the cell potential layer it's all linear
    because it's all metrics multiplicationã€‚but as you all know that deep learning
    models work well when are to when they are able to learn more complex mappings
    between input and output which can be attained by a simple MLPã€‚And the third important
    component of the self of the transformers is the maskingã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: so masking is what allows to paralyze the operations since every word can attend
    to every other word in the decoder part of the transformers which otherwise is
    going to be talking about later is the problem comes that you don't want the decoder
    to look into the future because that can result in data leakage so that's why
    masking helps the decoder to avoid that future information and learn only what
    has been what the model has processed so farã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæ©è”½æ˜¯ä½¿æ“ä½œèƒ½å¤Ÿè¢«ç¦ç”¨çš„åŸå› ï¼Œå› ä¸ºåœ¨å˜æ¢å™¨çš„è§£ç å™¨éƒ¨åˆ†ï¼Œæ¯ä¸ªè¯å¯ä»¥å…³æ³¨å…¶ä»–æ‰€æœ‰è¯ï¼Œè€Œæ¥ä¸‹æ¥è¦è®¨è®ºçš„é—®é¢˜æ˜¯ï¼Œä½ ä¸å¸Œæœ›è§£ç å™¨æŸ¥çœ‹æœªæ¥çš„ä¿¡æ¯ï¼Œå› ä¸ºè¿™å¯èƒ½å¯¼è‡´æ•°æ®æ³„éœ²ï¼Œæ‰€ä»¥æ©è”½æœ‰åŠ©äºè§£ç å™¨é¿å…æœªæ¥ä¿¡æ¯çš„å½±å“ï¼Œåªå­¦ä¹ åˆ°ç›®å‰ä¸ºæ­¢æ¨¡å‹æ‰€å¤„ç†çš„å†…å®¹ã€‚
- en: So now on the encoder E architecture of the transformersã€‚Yeahï¼Œ thanksã€‚Sasonia
    for talking about self retentionã€‚So self-atten is sort of the key ingredient or
    one of the key ingredients that allows transformers to work so wellã€‚but at a very
    high level the model that was proposed in the vwa etal paper of 2017 was like
    previous language models in the sense that it had an encoded decoder architecture
    what that means is let's say you're working on a translation problem you want
    to translate English to French the way that would work is you would read in the
    entire input of your English sentence you would encode that input so that's the
    encoder part of the network and then you would generate token by token the corresponding
    French translation and the decoder is the part of the network that is responsible
    for generating those tokensã€‚
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: So you can think of these encoder blocks and decoder blocks as essentially something
    like Legoã€‚they have these subcompons that make them up and in particular the encoder
    block has three main subcomps the first is a self retention layer that Cheania
    talked about earlier and as talked about earlier as wellã€‚
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ä½ å¯ä»¥æŠŠè¿™äº›ç¼–ç å™¨å—å’Œè§£ç å™¨å—çœ‹ä½œæ˜¯ç±»ä¼¼ä¹é«˜çš„ä¸œè¥¿ã€‚å®ƒä»¬ç”±è¿™äº›å­ç»„ä»¶ç»„æˆï¼Œç‰¹åˆ«æ˜¯ç¼–ç å™¨å—æœ‰ä¸‰ä¸ªä¸»è¦çš„å­ç»„ä»¶ï¼Œç¬¬ä¸€ä¸ªæ˜¯è‡ªæˆ‘ä¿ç•™å±‚ï¼Œä¹‹å‰çš„Cheaniaä¹Ÿæåˆ°è¿‡è¿™ä¸€ç‚¹ã€‚
- en: you need a feet forward layer after that because the self retention layer only
    performs linear operations and so you need something that can capture the non-
    nonlinearitiesã€‚You also have a layer norm after thisï¼Œ and lastlyï¼Œ there are residual
    connections between different encoder blocksã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹åä½ éœ€è¦ä¸€ä¸ªå‰é¦ˆå±‚ï¼Œå› ä¸ºè‡ªæˆ‘ä¿ç•™å±‚åªæ‰§è¡Œçº¿æ€§æ“ä½œï¼Œå› æ­¤ä½ éœ€è¦ä¸€äº›èƒ½å¤Ÿæ•æ‰éçº¿æ€§ç‰¹å¾çš„ä¸œè¥¿ã€‚ä½ åœ¨è¿™ä¹‹åè¿˜æœ‰ä¸€ä¸ªå±‚å½’ä¸€åŒ–ï¼Œæœ€åï¼Œä¸åŒç¼–ç å™¨å—ä¹‹é—´æœ‰æ®‹å·®è¿æ¥ã€‚
- en: The decoder is very similar to the encoderï¼Œ but there's one difference which
    is that it has this extra layer because the decoder doesn't just do multiha attention
    on the output of the previous layersã€‚so for context the encoder does multiha attention
    each self retention layer in the encoder blockã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è§£ç å™¨ä¸ç¼–ç å™¨éå¸¸ç›¸ä¼¼ï¼Œä½†æœ‰ä¸€ä¸ªåŒºåˆ«ï¼Œå°±æ˜¯å®ƒå¤šäº†ä¸€å±‚ï¼Œå› ä¸ºè§£ç å™¨ä¸ä»…å¯¹å‰ä¸€å±‚çš„è¾“å‡ºè¿›è¡Œå¤šå¤´æ³¨æ„åŠ›å¤„ç†ã€‚æ‰€ä»¥ä¸ºäº†æä¾›èƒŒæ™¯ï¼Œç¼–ç å™¨åœ¨ç¼–ç å—ä¸­çš„æ¯ä¸ªè‡ªæ³¨æ„åŠ›å±‚éƒ½è¿›è¡Œå¤šå¤´æ³¨æ„åŠ›å¤„ç†ã€‚
- en: In each of the encoded blocks it doesã€‚Multiha attention looking at the previous
    layers of the encoder blocksã€‚the decoderï¼Œ howeverï¼Œ does that in the sense that
    it also looks at the previous layers of the decoderã€‚but it also looks at the output
    of the encoder and so it needs us a multi head attention layer over the encoder
    blocksã€‚And lastly there's masking as well so if you are because every token can
    look at every other token you want to sort of make sure in the decoder that you're
    not looking into the future so if you're in position three for instanceã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: you shouldn't be able to look at position four and position fiveã€‚WæŒã€‚So those
    are sort of all the components that led to the creation of the model in the Vaswani
    atal paper andã€‚ğŸ˜Šï¼ŒLet's talk a little bit about the advantages and drawbacks of
    this modelã€‚So the two main advantagesï¼Œ which are huge advantages and which are
    why transformers have done such a good job of revolutionizing manyã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: many fields within deep learning are as follows so so the first is there is
    this constant part length between any two positions in a sequence because everyã€‚Token
    in the sequences looking at every other token and this basically solves the problem
    that di talked about earlier with long sequences you don't have this problem with
    long sequences where if you're trying to predict a token that depends on a wordã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: That was far far behind in a sentence you don't have the problem of losing that
    context now the distance between them is only one in terms of the part length
    also because the nature of the computation that's happening transformer model
    lend themselves really well to parallellyization and because of the advances that
    we've had with GPUs basically if you take a transformer model with n parameters
    and you take a model that isn't a transformer say like an MSTM with also with
    n parameters training the transformer model is going to be much faster because
    of the parallellyization that it leveragesã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨å¾ˆä¹…ä»¥å‰çš„ä¸€ä¸ªå¥å­ä¸­ï¼Œä½ ç°åœ¨æ²¡æœ‰å¤±å»ä¸Šä¸‹æ–‡çš„é—®é¢˜ï¼ŒäºŒè€…ä¹‹é—´çš„è·ç¦»åœ¨éƒ¨åˆ†é•¿åº¦ä¸Šä»…ä¸ºä¸€ã€‚åŒæ—¶ï¼Œç”±äºæ­£åœ¨å‘ç”Ÿçš„è®¡ç®—çš„æ€§è´¨ï¼Œå˜æ¢å™¨æ¨¡å‹éå¸¸é€‚åˆå¹¶è¡ŒåŒ–ï¼Œå¹¶ä¸”ç”±äºæˆ‘ä»¬åœ¨GPUæ–¹é¢å–å¾—çš„è¿›å±•ï¼ŒåŸºæœ¬ä¸Šï¼Œå¦‚æœä½ ä½¿ç”¨ä¸€ä¸ªå…·æœ‰nä¸ªå‚æ•°çš„å˜æ¢å™¨æ¨¡å‹ï¼Œå’Œä¸€ä¸ªä¸æ˜¯å˜æ¢å™¨çš„æ¨¡å‹ï¼Œæ¯”å¦‚ä¸€ä¸ªMSTMï¼ŒåŒæ ·ä¹Ÿå…·æœ‰nä¸ªå‚æ•°ï¼Œé‚£ä¹ˆè®­ç»ƒå˜æ¢å™¨æ¨¡å‹å°†ä¼šå¿«å¾—å¤šï¼Œå› ä¸ºå®ƒåˆ©ç”¨äº†å¹¶è¡ŒåŒ–çš„ä¼˜åŠ¿ã€‚
- en: So those are the advantagesï¼Œ the disadvantages are basically selfatten takes
    quadratic time because every token looks at every other token or n square as you
    might know does not scale and there's actually been a lot of work in trying to
    tackle thisã€‚so we've linked to some here big bird leanformer and reformer are
    all approaches to try and make this linear or quasiy linear essentiallyã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒAnd yeahï¼Œ we highly recommend to recommend going through J Aammer's blockã€‚the
    Illustrated transformerï¼Œ which provides great visualizations and explains everything
    that we just talked about in great detailã€‚Yeahï¼Œ and I'd like to pass it on to
    Chaanya for applications of transformersã€‚Yeahã€‚so now moving on to some of the
    recent workï¼Œ some of the work that very shortly followed the Transers paperã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: so one of the models that came out was GPTï¼Œ the GPT architecture which was released
    by OpenEI so OpenI had the latest model that Openia has in the GPT series is the
    GPT3 so it consists of only the decoder blocks from transformers and is strain
    on our traditional language modeling task which is predicting the current token
    which is creating the next token given the last T tokens the model has seen and
    for any downstream task now the model can just you can just train a classification
    layer on the last hidden state can be which can have any number of labels and
    since the model is generative in natureã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: you can also use the pretrained network as for generative kind of tasks such
    as summarizemarization and natural language and natural languageã€‚Reã€‚For that instanceã€‚Another
    important aspect that GPT3 gained popularity was its ability to be able to perform
    in context learning what the authors called into context learning so this is the
    ability wherein the model can perform can learn under few short settings what
    the task is to complete the task without performing any great inputs for example
    let's say the model is shown a bunch of addition examples and then if you pass
    in a new input and leave and just leave it at equal to sign the model tries to
    predict the next token which very well comes out to be the sum of the the sum
    of the numbers that is shown another example can be also the spell correction
    task or the translation task so this was the ability that made GT3 so much talked
    about in the NLP world and right now also like many applications have been made
    using GPTã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Which includes one of them being the VS code Copitã€‚which tries to generate a
    piece of code given top string kind of natural language textã€‚Another major model
    that came out that was based on the transformers architecture was Btã€‚so BRT lends
    it name from it's an acronym for bidirectional encoding encoder representations
    of transformersã€‚
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: it consists of only the encoder blocks of the transformers which is unlike GPT3
    which had only the decoder blocksã€‚Now thisï¼Œ because of this change there there
    comes a problem because BRT has only the encoder blockã€‚so it sees the entire piece
    of textï¼Œ it cannot be pretrained on a knife language modern task because of the
    problem of data leakage from the futureã€‚So what the authors came up with was a
    clever idea and they came up with a novel task called mass language modelingã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: which included to replace certain birds with a placeholder and then the model
    tries to predict those words given the entire contextã€‚Now apart from this total
    level task there was the authors also added a second objective called the next
    sentence predictionã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: which was a sentence level task wherein given two chunks of textã€‚the model tried
    to predict whether the second sentence followed the other sentence or not followed
    the first sentence or not and now for after pretraining this model for any downstream
    taskã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: the model can be further fine tuned within additional classification layer just
    like it was in GPP3ã€‚So these are the two models that were that have been like
    very popular and have made a lot of applications made their way in a lot of applicationsã€‚but
    the landscape has changed quite a lot since we have taken this class there are
    models with different techniques like E deerta and there are also models that
    do well in like other modalities and which we are going to be talking about in
    other lecture seriess as well so yeah that's all from this lecture and thank you
    for tuning inã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: Yeahï¼Œ to just want to end by saying thank you all for watching this and we have
    a really exciting set of videos with truly amazing speakers and we hope you are
    able to derive value from thatã€‚![](img/5ecc633c0934862bc1bde6086b437e97_3.png)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: ï¼Œ thanks Markï¼Œ thank youï¼Œ thank you everyoneã€‚![](img/5ecc633c0934862bc1bde6086b437e97_5.png)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
