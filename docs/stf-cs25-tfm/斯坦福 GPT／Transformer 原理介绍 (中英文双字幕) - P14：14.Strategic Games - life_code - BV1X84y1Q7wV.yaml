- en: ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P14Ôºö14.Strategic Games - life_code - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/e3abef98963297997c09e5d05b736a64_0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/e3abef98963297997c09e5d05b736a64_1.png)'
  prefs: []
  type: TYPE_IMG
- en: So it spend sometimes like two months training the bots„ÄÇOn thousands of CPUs„ÄÇTabytes
    and memory sometimesÔºå but when it came time to actually play against the humans„ÄÇthey
    would act almost instantlyÔºå it was just a lookup table„ÄÇAnd the humans„ÄÇwhen they
    were in a tough spotÔºå they would not act instantly„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they would think they would sit there and they would think for five seconds„ÄÇmaybe
    five minutes if it was a really difficult decision and it was clear that that
    was allowing them to come up with better strategies„ÄÇAnd so I wanted to investigate
    this behavior in our bots like if we could add this to our bots„ÄÇhow much of a
    difference would it makeÔºå the ability to instead of acting instantly to take some
    time and compute a better strategy for the spot that the agent was in„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And this is what I found„ÄÇSo on the X axis here we have like the number of buckets„ÄÇthe
    number you can think of this as like the number of parameters in your model and
    on the Y axis we have distance from national equilibrium so this is basically
    like how much you would lose towards worstk's adversary so the lower this number
    is the better your poker bot is and you can see as you scale the number of parameters„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: your performance improves and as you increase the number of parameters by about100
    x you know your exploitability goes down by about half and indeed you're getting
    a much better poker bot„ÄÇBut you can see the blue line here is if you don't have
    search and the orange line is if you do add search„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üò°ÔºåAnd you can see just adding searchÔºå adding the ability to sit there and think
    for a bit„ÄÇimprove the performance of these models„ÄÇAnd it reduced the exploitability„ÄÇthe
    distance from ra equilibrium by about 7 x„ÄÇAnd if you were to extend that blue
    line and see how many prim would you need in order to be comparable to adding
    search„ÄÇthe answer is you would need to scale up your model by about 100Ôºå000 x„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this was pretty mind blowlowing to me when I saw thisÔºå I meanÔºå over the course
    of my PhD„ÄÇthe first three yearsÔºå first three or four years of my PhD„ÄÇI' managed
    to scale up these models by about 100 x„ÄÇÂóØ„ÄÇAnd I was proud of thatÔºå I mean„ÄÇthat's
    like a pretty pretty impressive resultÔºå I think„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But what this plot was showing me was that just adding search„ÄÇWas the equivalent
    of scaling things up by about 100Ôºå000 x„ÄÇand so all of my previous research up
    until this point„ÄÇWould just be a footnote compared to adding search„ÄÇSo„ÄÇWhen I
    saw thisÔºå it became clear„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this was the answer to beating top humans in pokerÔºå and so for the next yearÔºå
    basically nonstop„ÄÇI worked on scaling search„ÄÇNow there's a question that naturally
    comes up which is why wasn't this considered before there's a few factors so first
    of all I should say like the search had been considered in poker before and it's
    actually quite natural to say like well you know if you had search and chess and
    search and go like why would you not consider search in poker„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: There's a few reasonsÔºå one is that„ÄÇCulturally like the poker research grew out
    of game theory and reinforcement learning and so it wasn't really from the same
    background as like the people are working on chess and working on go„ÄÇWhen you
    scale search scaling test time compute it makes all your experiments much more
    expensive and just like more unpleasant to work with and there are just like incentive
    structures I mean people are always thinking about winning the next annual computer
    poker competition and the ACPC limited the resources that you could use at test
    time so search wasn't really possible effectively in the ACPC„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I think the biggest factor is that people just didn't think it would make
    such a huge difference I mean I think it's reasonable to look at something like
    search and think like oh yeah that might make like a 10 X difference you probably
    wouldnt think it makes a 10000 x difference and so there were some people working
    on it but it wasn't really the focus of a lot of people's research„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So anywayÔºå focused on scaling search and that led to the 2017 Bras versus AIT
    competition where we again played our bot against four top poker pros„ÄÇ120Ôºå000
    hands of poker $200Ôºå0000 prize money and this time the bot won by 15 big blinds
    per100 instead of nine big blinds per100„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This was a crushing victoryÔºå each human lost individually to the bot and„ÄÇ![](img/e3abef98963297997c09e5d05b736a64_3.png)
  prefs: []
  type: TYPE_NORMAL
- en: Four standard deviations of statistical significance„ÄÇWe followed this up in
    2019 with six player poke AI competition„ÄÇThe big difference here is that we figured
    out how to do depth limited search„ÄÇso before the 2017 bot it would always have
    the search to the end of the game here it only had to do search a few moves ahead
    and it could stop there„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so this time againÔºå it wont with statistical significance„ÄÇand what's really
    surprising about this spot is that despite it being a much larger game„ÄÇthe six
    player Pokerbot Plebuvis cost under $150 to train on cloud computing resources
    and it runs on 28 CPU cores at inference time„ÄÇthere's no GPUs„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3abef98963297997c09e5d05b736a64_5.png)'
  prefs: []
  type: TYPE_IMG
- en: So I think what this shows is that„ÄÇThis really wasn an algorithmic improvement„ÄÇI
    mean this would have been doable 20 years ago if if people knew how to do it„ÄÇAnd
    I think it also shows the power of searchÔºå you know„ÄÇif you can figure out how
    to scale that compute at test time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it really can make a huge difference and bring down your your training costs
    by a huge amount„ÄÇËØ∂„ÄÇüòä„ÄÇOkayÔºå anywayÔºå yeahÔºå so I wanted to say also this is not limited
    to poker if you look at go you see a similar pattern„ÄÇso this is a plot from the
    alphago zero paper„ÄÇOn the X axis„ÄÇwe have different versions of alphaphago and
    on the y axisÔºå we have elo rating„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which is a way of comparing different botsÔºå but also a way of comparing bots
    to humans„ÄÇAnd you can see if„ÄÇOkayÔºå so superhuid performance is around 3Ôºå600 elo„ÄÇAnd
    you can see Alpha Go Lee„ÄÇthe version that played against Li Addal in 2016Ôºå that's
    right over the line of superhuman performance„ÄÇAlphago is zeroÔºå the strongest version
    of alphaphago is around 5Ôºå200 elo„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But if you take out the test time searchÔºå if you just play according to the
    policy net and not do any modc research in alphago was zero at test time„ÄÇthen
    the eO rating drops to around 3Ôºå000Ôºå which is substantially below expert human
    performance„ÄÇSo what this shows is that if you take out Montegli research at test
    time„ÄÇüò°„ÄÇThe alphaphaGo zero is not superhuman and in fact nobody has made a superhuman
    gobot that does not use search in some form„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: nobody has made a raw neural network that can beat top humans in go„ÄÇAnd I should
    say also„ÄÇthis is just if you're taking out the search at test time„ÄÇI'm not even
    talking about taking it out of training timeÔºå if you took it out of training time„ÄÇwe
    wouldn't even got off the ground„ÄÇNow there's a question of like okay well„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: surely you could just scale up the modelÔºå scale up the matter of training and
    you would eventually you know surpass the humanment performance and match the
    performance if you added search and that's true yes„ÄÇif you scale up the models
    and if you scale up the training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then you would eventually match the performance with search„ÄÇbut there's a question
    of like how much would you have to scale it up by„ÄÇNow„ÄÇa rough rule of thumb is
    that in order to increase your eO rating by about 120 points„ÄÇyou either have to
    double the amount of model size and training or you have to double the amount
    of test time search„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so if you look at that gap of around 2000 elow points and you calculate
    the number of deadlings that you would need„ÄÇthe answer is that in order to get
    the raw policy net from 3Ôºå000 e to 5200 elow„ÄÇyou would need to scale your model
    and your training by about 100Ôºå00x„ÄÇOkay„ÄÇso why is this importantÔºüüò°ÔºåI think you
    look at what's happening today with large language models and transformers and
    and you see something similar„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I meanÔºå you're getting„ÄÇHuge there's a question of like what do I mean by search
    there's a specific kinds of search like multiult research„ÄÇthe ability to just
    like plan ahead what you're going to do instead of just like acting instantly
    based on your precomputed policy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but really what I mean by search more broadly is the ability to scale the amount
    of computation to get better performance„ÄÇI think that's the real value that search
    is adding instead of just like acting according to your precomputd„ÄÇYou knowÔºå front
    loading all of your computations so you're doing everything all your computation
    ahead of time and then like at inference time acting basically instantly„ÄÇCould
    you get a better solution if you had five minutes to output an action instead
    of 100 millisecondsÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Okay„ÄÇSo„ÄÇYeahÔºå I think you look at„ÄÇI'm sorryÔºå there's a question„ÄÇdoes a transformer
    with a search circuit count as search„ÄÇor do you mean and engineering search algorithmsÔºüI
    don't want to get bogged down into like the details of like how to do this because
    like the answer is nobody really knows yet„ÄÇnobody really has a general way of
    doing search and all the domains that we've done search successfully like poker
    and go it's done in a fairly domain specific way„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you know go use this algorithm called multiola research„ÄÇAnd yeah„ÄÇyou could think
    of beamS searcharch as like one simple form of searchÔºå but„ÄÇit does seem like there
    should be betterÔºå better ways in the future„ÄÇYeah„ÄÇSo anyway„ÄÇwhere I'm going with
    this is like you look at how large language models are being trained today and
    you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you're seeing millions of dollars being thrown at pre training like„ÄÇI wouldn't
    be surprised if we see a large language model that would cost like $100 million
    to train„ÄÇWe might even get to a billion dollars„ÄÇBut„ÄÇThe inference cost is still
    going to be very small„ÄÇAnd so there's a question of likeÔºå could you do substantially
    better if you couldÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Scall the amount of inference cost as well„ÄÇMaybe that could like amortize some
    of your training cost„ÄÇYeah„ÄÇOkayÔºå so there's this„ÄÇThere's this lecture called the
    Bi lessons by Richard Sutton that says the biggest lesson that can be learned„ÄÇand
    so it's a really great essay I recommend reading it„ÄÇbut one of the big takeaways
    is like he says the biggest lesson that can be learned from over 70 years of AI
    research is that general methods that leverage computation are ultimately the
    most effective„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The two methods that seem to scale arbitrarily in this way are search and learning„ÄÇNow„ÄÇI
    think we've done a great job with generalizing searchÔºå sorryÔºå generalizing learning„ÄÇand
    I think there's still room for improvement when it comes to search„ÄÇU and yeah„ÄÇthe
    next goal really is about generalityÔºå like can we develop a truly general way
    of scaling inference compute instead of just doing things like Montecalo research
    that are specific to a better domain to a specific domain„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And also better than things like chain of thought„ÄÇÂóØ„ÄÇWhat this would look like
    is that you have much higher test time compute„ÄÇBut you have much more capable
    models„ÄÇAnd I think for certain domains that trade off is worth it„ÄÇlike if you
    think about what inference cost we're willing to pay for proof of the Raymond
    hypothesis„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I think we'd be willing to pay a lot„ÄÇOr you knowÔºå the cost of„ÄÇWhat cost were
    we want to pay for new life saving drugsÔºå I think we'd be willing to pay a lot„ÄÇSo
    I think that there is an opportunity here„ÄÇOkayÔºå anywayÔºå so that's why I prelude„ÄÇI
    guess any questions about that before I move on to CiceroÔºüBy the way„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the reason why I'm talking about this is because it's going to inform„ÄÇüò°„ÄÇThe
    approach that we took to CiceroÔºå which I think is quite different from the approach
    that a lot of other the researchers might have taken to this problem„ÄÇSomeone asked
    can you give an example of search well multicarural research is one form of search„ÄÇyou
    could also think of like breadth for search depth for search these kinds of things
    they're all search I would also argue that like chain of thought is doing something
    similar to search where it it's allowing the model to like leverage extra compute
    at test time to get better performances„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But I think that that's the main thing that you want„ÄÇthe ability to like leverage
    extra compute at a test time„ÄÇWhat's the search what's the space that you are searching
    over again like in in a game like go it's like different board positions„ÄÇbut you
    could also imagine searching over like you know different sentences that you could
    say things like that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: There's a lot of flexibility there as well„ÄÇOkayÔºå so now I want to get into Cicero
    so first thing I should say when it comes to Cicero„ÄÇthis is„ÄÇA big team effort„ÄÇThis
    was like this is actually one of the great things about working on this project
    that there was just such a diverse talent pool„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: experts in reinforcement learningÔºå planningÔºå game theoryÔºå natural language processing„ÄÇall
    working together on this and it would not have been possible without without everybody„ÄÇSo
    the motivation for diplomacy actually came from 2019„ÄÇwe were looking at all the
    breakthroughs that were happening at the time and I think a good example of this
    is this XKCD comic that came out in 2012 that shows like different categories
    games„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: games that are solvedÔºå games where computers can beat documents„ÄÇgames where
    computers still lose documents and games where computers may never outplay documents„ÄÇAnd
    in this categoryÔºå computer still lose to top humans you had four gamesÔºå go Arima„ÄÇpoker
    and Starcraftft„ÄÇIn 2015„ÄÇüòäÔºåActuallyÔºå one of my colleagues„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: David Wu made the first day out to beat Top humans in arema„ÄÇin 2016 we have
    Algo beating Lisa do and Go„ÄÇIn 2017„ÄÇyou have the work that I just described where
    we beattop humans in poker„ÄÇAnd in 2019„ÄÇwe had Alpha star beating X humans in Starcraftft„ÄÇSo
    that shows the incredible amount of progress that had happened in strategic reasoning
    over the past several years„ÄÇleading up to 2019„ÄÇAnd at the same timeÔºå we also had
    GP2 come out in 2019„ÄÇand it showed that language model like„ÄÇAnd natural language
    processing was progressing much faster than I think a lot of people„ÄÇincluding
    us expected„ÄÇAnd so we were thinking about what after the sixth player poker work
    I was discussing with my colleagues„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: what should we work on nextÔºüAnd we were throwing around like different domains
    to work on„ÄÇGiven the incredible amount of progress in AIÔºå we wanted to pick something
    really ambitious„ÄÇsomething that we thought„ÄÇYou couldn't just tackle by scaling
    up the existing approaches that you really needed something new in order to address„ÄÇAnd
    we landed on diplomacy because we thought that it would be the hardest game to
    make an AI for„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So what is diplomacyÔºüDiploomacy is a natural language strategy game„ÄÇit takes
    place right before World War I you play as one of the seven great powers of Europe„ÄÇEnglandÔºå
    FranceÔºå GermanyÔºå AustriaÔºå Russia and Turkey„ÄÇand your goal is to control a majority
    of the map in practice that rarely happens like if you control a majority of a
    majority of the map and you've won in practice nobody ends up winning outright
    and so your score„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Is proportional to the percentage of the map that you controlÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Now what's really interesting about diplomacy is that it is a natural language
    negotiation game„ÄÇso you have these conversations like what you're seeing here
    between Germany and England where they will probably communicate with each other
    before making their moves and so you can have Germany ask like want to support
    Sweden„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: England says let me think on that and so on„ÄÇSo this is a popular strategy game
    developed in the 1950s„ÄÇit was JFK and Kisser's favorite game actually„ÄÇAnd like
    I said„ÄÇeach chart involves sophisticated private natural language negotiations„ÄÇand
    I want to make clear like this is not„ÄÇNegotiations like you would see in a game
    like Sotos and Katan„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: for exampleÔºå you're seeing„ÄÇIt's much more like survivorÔºå if you've ever seen
    a TV show survivor„ÄÇyou have discussions around like alliances that you'd like
    to build discussions around like specific tactics that you'd like to execute on
    the current current and also you know like more long- term strategy around like
    where do we go from here and how do we divide resources„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Now the way the game worksÔºå you have these negotiations that last between five
    and 15 minutes depending on the version of the game„ÄÇand then on each term and
    all these negotiations are done privately in parawise negotiation„ÄÇalso I think
    that you are not mutedÔºå okay thank you„ÄÇAnd then after the negotiation period completes„ÄÇeverybody
    will simultaneously write down their moves„ÄÇüò°„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so a player could promise you something like I'm going to support you into
    this territory this term„ÄÇbut then when people actually write down their moves„ÄÇthey
    might not write that down and so you only find out if they were true to their
    word when all the moves are revealed simultaneously„ÄÇAnd so for this reasonÔºå alliances
    and trust building is key„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the ability to trust that somebody's going to follow through on their promises„ÄÇthat's
    really what this game is all about and the ability to convince people that you
    are going to follow through on your promises is really what this game is all about„ÄÇAnd
    so for this reason„ÄÇDiploacy has long been considered a challenge problem for AI„ÄÇThere
    research in the game going back to the 80sÔºå the research really only picked up„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It picked up like quite intensely in starting in 2019 when researchers from
    DeepMdÔºå ourselvesÔºå MiA„ÄÇother places started working on this„ÄÇNow a lot of that
    research„ÄÇthe vast majority of that research actually was focused on the non languageguage
    version of the game„ÄÇwhich was seen as a stepping stone into the full natural language
    version„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: though we decided to focus from the start on the full natural language version
    of the game„ÄÇSo to give you a sense of like what these negotiations and dialogue
    look like„ÄÇhere is one example so here„ÄÇEnglandÔºå you can see they move their fleet
    in Norway to St„ÄÇPetersburg and that occupies the Russian territory„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so this is what the board state looks like after that move and now there's
    this conversation between Austria and Russia„ÄÇAustria saysÔºå well what happened
    up north Russia says England stabbed I'm afraid the end may be close from you
    my friend Austria says yeah that's rough are you going to be okay up there Russia
    says I hope so England seems to still want to work together Austria says can you
    make a deal with Germany so the players are now discussing like what should be
    discussed with other players Russia says good idea then Austria says if we find
    as long as you can defend Sevetopol so Sevesttopol is this territory down to the
    south you can see that Turkey has a fleet and an army in the Black Sea in Armen
    next to Sevestoppo and so they could potentially attack that territory next turn„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇüòäÔºåAustria says can you support hold Sevetooles Ukraine and Romania I'll support
    hold Romania„ÄÇRussia says yepÔºå they'm already doing soÔºå Austria says awesome„ÄÇhopefully
    we can start getting you back on your feet„ÄÇSo this is an example of the kinds
    of conversations that you'll see in a game of diplomacy in this conversation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Austria is actually our bot Cicero„ÄÇüòäÔºåSo that kind of gives you a sense of the
    sophistication of the agent dialogue„ÄÇËØ∂„ÄÇOkay I'll skip this for okay so I guess
    I'll go into this I don't want to take up too much time really what makes diplomacy
    interesting is that support is key so here for example„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Budapest and Warsaw the red and the purple units both try to move into glacia
    and so since it's a one verse one they both bounce back and now they was into
    the territory in the middle panel you can see Vienna supports Budapest into glacciia
    and so now it's a two verse one and that red unit will indeed enter Galacciia„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what's really interesting about the po is that it doesn't just have to be
    your own units that are supporting you„ÄÇit could be another player's units as wellÔºå
    so for example„ÄÇthe green player could support the red player into glacciia and
    then that ready unit would still go in there„ÄÇSo support is really what the game
    is all about negotiating over support and so for that reason diplomacy has this
    reputation as the game that ruins friendships it's really difficult to have an
    alliance with somebody for three or four hours and then have that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåHave them backstb you and basically is ruining your game„ÄÇBut if you talk to
    expert diplomacy playersÔºå they view it differently„ÄÇThey say diplomacy is ultimately
    about building trust in an environment that encourages you to not trust anyone„ÄÇAnd
    that's why we decided to work on the gameÔºå you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: could we make an AI that is able to build trust with players in an environment
    that encourages them to not trust anybody can thebo like honestly communicate
    that it's going to do something and and evaluate whether another person is being
    honest when they are saying that they're going to do something„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Okay so„ÄÇW diplomacyÔºå it sits in this nice intersection of reinforcement learning
    and planning„ÄÇand also natural language„ÄÇThere's two perspectives that we can take
    on why diplomacy is a really interesting domain one is the multiedging perspective
    so here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üò°ÔºåAll the previous game we have results like chess go poker„ÄÇthese have all been
    in purely zero sum two player zero sum domains„ÄÇand in these domains selfplay is
    guaranteed to converge to an optimal solution basically what this means is you
    can start to having the bot play completely from scratch with no human data„ÄÇAnd
    by playing against itself repeatedlyÔºå it will eventually converge to this unbeatable
    optimal solution called the mini Max equilibrium„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But that result only holds in two players zero sum„ÄÇthat whole paradigm only
    holds in two players zero sum games„ÄÇWhen you go to domains that involve cooperationÔºå
    in addition to competition„ÄÇThen success requires understanding human behavior
    and conventions you can't just treat the other players like machines anymore„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you have to treat them like humansÔºå you have to model„ÄÇüò°ÔºåHuman irrationalityÔºå
    human subbotality„ÄÇone example of this is actually language like„ÄÇYou can imagine
    if you were to train a bo completely from scratch in the game of diplomacy„ÄÇLike
    the full natural language version of the game„ÄÇthere's no reason why the bot would
    learn to communicate in English„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it would learn to communicate in some weird gibberish robot language„ÄÇand then
    when you stick it in a game with six humans„ÄÇit's not going to be able to cooperate
    with them„ÄÇüò°ÔºåÂóØ„ÄÇSo we have to find a way to incorporate human data and be able to
    learn how humans behave in order to succeed in this game„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üò°ÔºåËØ∂ ok k „ÄÇThere's also the NLP perspective„ÄÇWhich is that current language models„ÄÇAre
    essentially just imitating human like text„ÄÇNow there's been some progress with
    things like RLHF„ÄÇbut„ÄÇThat's still like not really the way that humans communicate
    they communicate with an intention in minds right they come up with this intention
    and then they communicate with the goal of communicating that intention and they
    understand that others are trying to do the same„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üò°ÔºåAnd so there's a question of likeÔºå can we move beyond shitt chatÔºüTo grounded
    intentional dialogue„ÄÇSo Cicero is an AI agent for diplomaacy that integrates high
    level strategic play and open domain dialogue„ÄÇAnd we use 50Ôºå000 human games of
    diplomacy acquired through a partnership with the website webdplomacy„ÄÇnet„ÄÇSo we
    entered Cicero in an online diplomacy league just to give you the results up front„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Cicero was not detected as an AI agent for 40 games with 828 players„ÄÇthere was
    one player that mentioned after the fact that like they kind of like made a joke
    about us being a bot but they didn't really follow up on it and nobody else followed
    up on it and they later accuseduse somebody else of also being a bot so we weren't
    sure how seriously to take that accusation but I think it's sad to say it made
    it through all 40 games not being detected as bot and then we in fact we told
    the players afterwards that it was a bot the whole time these are the kinds of
    responses that we got people were quite surprised„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: pleasantly surprised fortunatelyÔºå nobody was was upset with us but they were
    quite surprised that„ÄÇThere was a bot that had been playing this game with them
    the whole time„ÄÇSo in terms of results„ÄÇCicero placed in the top 10% of playersÔºå
    it's a high variance game and so if you look at players that played five or more
    games that played second out of 19„ÄÇand it achieved more than double the average
    human score„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I would describe this as a strong level of human performance„ÄÇI wouldn't go
    as far as to say that this is superhuman by any means„ÄÇbut it is currently quite
    a strong result„ÄÇNowÔºå to give you a picture of how Cicero works„ÄÇSo„ÄÇThe input that
    we feed into the model is the board state and the recent action history that's
    shown on the top left here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and also the dialogue that it's had with all the players up until now„ÄÇSo that's
    going to get fed into a dialogue conditional action model that's going to predict
    what Cicero thinks all the players are going to do this turn and what they think
    we will do this term„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: These„ÄÇThese lead to what we call anchor policies that are then used for planning„ÄÇüò°ÔºåÂóØ„ÄÇNow„ÄÇplanning
    here„ÄÇAgainÔºå this is like the part where we leverage extra compute at test time
    in order to get better performance„ÄÇSo essentially we take these initial predictions
    of what everybody's going to do„ÄÇwhat are called anchor policies„ÄÇAnd we improve
    upon these predictions using this planning process called pickle„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: where basically we account for the fact that players„ÄÇWillll pick actions that
    have higher expected value with higher probability we're essentially adding this
    like rationality prior to all the players to assume that they're not going to
    blunder as often as the model might suggest and they're going to pick smarter
    actions with higher probability than the initial model might suggest„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what we find is that this actually gives us a better prediction of what
    all the players will do than just relying on the raw neural net itself„ÄÇThis gives
    us the action that we actually play in the game„ÄÇand it also gives us what we call
    intense„ÄÇSo intents are an action for ourselves and an action for the dialogue
    partner that we're speaking to„ÄÇAnd now we have this dialogue conditional so we
    have we have this dialogue model that conditions on these intents„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so the intents are fed into the dialogue model along with the board state and
    action history and also the dialogue that we've had so so far and that dialogue
    model will then generate„ÄÇCandidate messages that„ÄÇAre conditional on those intents„ÄÇThese
    candidate messages go through a series of filters that filter out nonsenseÔºå grounding
    issues„ÄÇand also like value action low expected value messages„ÄÇAnd ultimately„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we get out a message to send to our dialogue partner„ÄÇNow every time we send
    or receive a message„ÄÇwe will repeat this whole process„ÄÇSo there's actually a lot
    that is quite novel in Cicero and I'm going to try to talk about„ÄÇThe contributions
    as much as possibleÔºå I might go through this a little quickly just so we have
    time for questions„ÄÇThe first one is a controllable dialogue model that conditions
    on the game state and a set of intended actions for the speaker and the recipient„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we have a questionÔºå what is the action space here for the modelÔºüËØ∂„ÄÇAnd the
    action space„ÄÇFor the action prediction model is like all the actions that you
    could take in the game that a player could take in the game„ÄÇFor the dialogue modelÔºå
    it's like messages that you can send„ÄÇGot itÔºå so oh the sick„ÄÇH„ÄÇüòäÔºåOkay„ÄÇso we try
    what we call an intent model that predicts what actions people will take at the
    end of truthful turns„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: basically where we're trying to predict what are people intending to do when
    they communicate a certain message„ÄÇAnd„ÄÇThen we use this to automatically annotate
    the data set with basically what we expect people's intentions were when they
    sent that message and we filter out„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We filter out as much as possible lies from the data set so that the text in
    the data is annotated with the truthful intention„ÄÇAnd then during play Cicero
    conditions the dialogue model on the truthful intention that it intends to take„ÄÇand
    the goal then is that the hope then is that it will generate a message consistent
    with that intention„ÄÇAnd that is then fed into„ÄÇInto everything else that' that's
    you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: sorry that the intentions that we generate through planning are fed into the
    dial model„ÄÇSo to give you an example of what this looks likeÔºå this gives us a
    way to control the dialogue model through a set of intentions„ÄÇüò°ÔºåLikeÔºå here„ÄÇËØ∂„ÄÇWe
    are Cicero England in pinkÔºå and their action is to move to Belgium„ÄÇamong other
    things„ÄÇAnd so if we feed this attention into the dialogue model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: then the message that might get generated is something like England saying to
    France„ÄÇdo you mind supporting meÔºå do you mind supporting Eddie to BelgiumÔºüOn the
    other hand„ÄÇlet's say Cicero's action is to support France to Belgium„ÄÇThen if you
    feed that into the dial model„ÄÇthen the message that's generated might say something
    like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: let me know if you want me to support you to BelgiumÔºå otherwise I'll probably
    poke Hol„ÄÇNow„ÄÇwhat we find is that conditioning the dialogue model on these intentions
    in this way„ÄÇit makes the model more controllableÔºå but it also leads to higher
    quality dialogue with less nonsense„ÄÇSo we found that it led to dialogue that was
    more consistent with the state„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: more consistent with the planÔºå higher qualityÔºå lower perplexityÔºå and I think
    the argument„ÄÇthe reasoning for why this is the case is that we're kind of like
    relieving the dialogue model of the burden of having to come up with„ÄÇA good strategy
    we're allowing the dialogue model to do what it does best to focus on what it
    does best„ÄÇwhich is dialogue„ÄÇAnd we're relieving it of the strategic components
    of the game because we're feeding that strategy into the dialect model„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so that's one main contributionÔºå this controllable dialogue model that
    conditions on a plan„ÄÇThe second is a planning engine that accounts for dialogue
    and human behavior„ÄÇSo„ÄÇOkay„ÄÇI mentioned that a lot of„ÄÇPrevious work on games„ÄÇWas
    done using self play in two players here or some settings„ÄÇNowÔºå the problem with
    like pure self play„ÄÇIs that it can learn strong policiesÔºå but it doesn't„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It doesn't stick with human conventions and it can't account for dialogue„ÄÇit's
    just going to ignore the human data and the human way of playing if you just do
    self play„ÄÇSo that's one extreme„ÄÇThe other extreme that you can go is you just
    do supervised learning on human data„ÄÇCreate this model of how humans play and
    then train with those you know imitation humans„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And if you do thisÔºå you'll end up with a bot that's consistent with dialogue
    and human conventions„ÄÇbut it's only as strong as the training dataÔºå and we found
    that it was actually very easily manipulable through adversarial dialogue„ÄÇso for
    example you could send messages to it saying like thanks for agreeing to support
    me at the Paris and it will think like well I've only ever seen that message in
    my training data when I've agreed to support the person of the Paris and so I
    guess I'm supporting them at the Paris thisern even though that might be a terrible
    move for the bot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I came up with this algorithm called PickleÔºå that kind of like is a happy
    medium between these two extremes„ÄÇÂóØ„ÄÇThe way pickle works is it''s basically trying
    to it's it's doing self play„ÄÇbut regularized toward„ÄÇSticking to the human imitation
    policy„ÄÇSo it has a KL penalty for deviating from the human imitation policy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we have this parameter lambda that controls how easy it is to deviate from
    the human imitation policy at lambmbda equals zero„ÄÇIt just ignores the human imageization
    policy completely and just does pure self play„ÄÇAnd so just like do cell play as
    if from scratchÔºå add them be equals zero„ÄÇAt Lambda equals infinity„ÄÇit's just playing
    the human imation policy and not doing self Plaal„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But for intermediate values of LambdaÔºå what we find is that it actually gives
    you a good medium between sticking to human conventions and performing strongly„ÄÇSo
    you can kind of see this behavior emerge here„ÄÇSorry there's a question is this
    similar to offline Rl or also incorporates exploration„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so I'd say there's actually a lot of similar work on you know having a KL penalty„ÄÇAnd
    so yes I would say that it's like very similar to a lot of that work and it's
    also been done actually an alphapha star where they had a KL penalty„ÄÇthough that
    was more about aiding exploration like using human data to aid exploration rather
    than trying to better imitate humans so I think what's interesting about the pickle
    work is that one we find it imitates humans better not than just doing supervised
    learning alone and two we are„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Doing a bit of theory of mind where we assume that the other players are also
    like we're using this as a model for our behavior„ÄÇwhat we expect other people
    to think our behavior is in addition to modeling the other players„ÄÇSo it's like
    a common knowledge„ÄÇCommon knowledge like„ÄÇAlgorithm that we're using here„ÄÇOkay„ÄÇso
    the kind of behavior that you see from this„ÄÇYou can see hereÔºå let's say England
    agreesÔºå sorry„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so let's say we're in this situationÔºå this actually came up real in a real game„ÄÇAnd
    it inspired a figure from our paper„ÄÇSo England and France are fighting„ÄÇFrance
    is the bot and France asks if„ÄÇIf England is willing to disengage„ÄÇAnd let's say
    England says„ÄÇyesÔºå I will move out of English channel if you head back to NAO„ÄÇWellÔºå
    we can see that Cicero does„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: in fact back off„ÄÇLeeaaves and„ÄÇGoes to NAO and the disengagement is successful„ÄÇand
    so this shows that the bot strategy really is reflecting the dialogue that it's
    had with this other player„ÄÇAnother message that England might send is something
    like„ÄÇI'm sorry you've been fighting me this whole gameÔºå I can't trust you that
    you won't stab me„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so in this caseÔºå Cicero will continue its attack on EnglandÔºå and you can
    see again„ÄÇthis is reflectiveÔºå it's changing its behavior depending on the dialogue„ÄÇBut
    you can also have this kind of message where you know England saysÔºå yes„ÄÇI'll leave
    English channelnnel if you move tota Munich at Hall to Belgium„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so these are really bad moves for Ccero to follow and so if you just look at
    the raw policy net„ÄÇIt might actually do thisÔºå it might actually do these moves
    because England suggested it„ÄÇbut because we're using pickle that incorporates
    like it accounts for the expected value of different actions„ÄÇit will actually
    harshly back off but ignore the suggested moves because it recognize that those
    will leave it very vulnerable to an attack„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå I's skip this slide for time„ÄÇÂóØ„ÄÇOh„ÄÇAnother thing I should say is that„ÄÇWe're
    not just doing planningÔºå we're actually doing this in a full self play reinforcement
    learning loop„ÄÇAnd againÔºå the goal here is it's really about modeling humans better
    than supervised learning alone and we found that doing this selfplay reinforcement
    learning with pickle allowed us to better model human behavior than just doing
    imitation learning„ÄÇFinallyÔºå we have an ensemble of message filtering techniques
    that filters both nonsensical and strategically un soundund messages„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: UmSo to give you an example of what these filters look likeÔºå here„ÄÇone that we
    developed is value based filtering„ÄÇüò°ÔºåSo„ÄÇThe motivation for this is that when we
    feed into our dialogue model is a plan for ourselves and for our speaking partner„ÄÇbut
    it's the entire plan that we have for ourselves„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and so we might end up feeding into the dialogue model the fact that we're going
    to attack the player that we're speaking to„ÄÇNowÔºå the dialogue model isÔºå you knowÔºå
    to be honest kind of dumb and it doesn't really know„ÄÇThat it shouldn't be telling
    this player that theyre going to be attacked this term„ÄÇAnd so you have these messages
    that might be sentÔºå something like the second one shown here where England says
    to France„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we have hostile intentions towards youÔºå you must be wipeding the boardÔºå please
    provide a croissant„ÄÇSo this is actually like a message that the bot sent to a
    player not to a player it was a this was like preliminary testing and kind of
    like motivated this whole approach„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇSo we don't want the bot to send these kinds of messages if it's going to
    attack a player„ÄÇwe want it to send something that's likeÔºå you knowÔºå not an outright
    lie necessarily„ÄÇbut just something either not send a message or send something
    that's much much much more bland„ÄÇüò°„ÄÇAnd so we filter out these kinds of messages
    by looking at the value like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: What we do is we generate a bunch of candidate messages„ÄÇAnd then we see if we
    were to send this messageÔºå what is the behavior that we would expect the other
    players to take like what actions will we expect them to do after we send this
    message and what do they expect we will do after we send this messageÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And then we see„ÄÇWhat is the expected value of the action that we intend to take„ÄÇgiven
    the prediction of what everybody else is going to doÔºüüò°„ÄÇSo if our intention is
    to attack FranceÔºå then we can see well„ÄÇif I were to send this message to FranceÔºå
    then they're going to get really defensive and defend against an attack from us
    and our attack is going to be unsuccessful and so therefore I probably shouldn't
    send this message to them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇAnd so in this webÔºå we can actually filter out messages that have low expected
    value„ÄÇAnd we thought that this worked surprisingly well„ÄÇDialogue examples„ÄÇI'll
    go through one just for the sake of time„ÄÇSo here we have„ÄÇCicero is France and
    France is saying France is conversing with TurkeyÔºå who's a human player„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and they're debating over who's going to get tunisÔºå this territory circled in
    red„ÄÇYou can see they both have fleets next to the territory„ÄÇif they both go for
    it neither them we're going to get it„ÄÇand so they need to work out some sort of
    deal„ÄÇSo France saysÔºå I'll work with you„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but I need tunes for nowÔºå Turkey saysÔºå nopeÔºå you got to let me have it„ÄÇFrance
    says noÔºå I need it„ÄÇand then France suggestsÔºå you knowÔºå you can take these other
    territories instead you have Serbia and Rome to take„ÄÇTurkey says they're impossible
    targets and then Cicero suggests specific moves„ÄÇThat would allow Turkey to capture
    these territory so Cicero says Greece Iionion I unate to Tian„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Turkey saysm you're right good ideas and then France says then in the fall you
    take Rome Austria collapses and so that allows Turkey to you know make progress
    against Austria but conveniently it also allows France to capture tus because
    Turkey will be using those units for something else„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so limitations in future directions intent representation is just an action
    per player„ÄÇOkay„ÄÇso there's a question of like„ÄÇThe intentions that we're feeding
    into the dialogue model is an action that we're going to take for this turn and
    for the next turn for ourselves and for the other player„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But ideallyÔºå we would have a richer set of intentions„ÄÇwe would be able to like
    condition on things like long term strategy or style of communication or like
    asking questions„ÄÇThat is that's one of the limitations of this approach now of
    course„ÄÇThe richer you make the space of intentionsÔºå the more room there is for
    things to go wrong„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and you also have to like then train the model to be able to handle these like
    wider space of intentions„ÄÇüò°ÔºåThere was a questionÔºå do you think the dialogue model
    is learning an internal model„ÄÇinternal world model to be so good at predicting
    movesÔºüThis is this is arguably why we're„ÄÇConditioning on intentionsÔºå we're relieving
    the dialogue model of having to come up with a good world model because we're
    telling it like these are the moves that we are planning to take this turn and
    these are the moves that we would like this other player to take this turn„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we're like„ÄÇWe're able to like„ÄÇHave the world model separate from the dialogue
    model„ÄÇbut condition on the output from the world model„ÄÇOkay„ÄÇanother limitation
    is that Cicero's value model doesn't condition on dialogue„ÄÇand so it has a limited
    understanding of the long term effects of dialogue„ÄÇÂóØ„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This greatly limits our ability to„ÄÇPlan what kind of messages we should be sending„ÄÇAnd
    this is actually why we always condition Cicero's dialogue generation on its truthful
    intentions„ÄÇYou could argue that there is situations in diplomacy where you would
    want to lie to the other player„ÄÇthe best players rarely lieÔºå but they do lie sometimes„ÄÇAnd„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: You have to understand the trade off between like if you lieÔºå you are going
    to„ÄÇüò°„ÄÇIt's going to be much harder to work with this person in the future„ÄÇAnd so
    you have to make sure that the„ÄÇValue that you're getting positionally is worth
    that loss of trust and that broken relationship„ÄÇno„ÄÇBecause Cicero's value model
    doesn't condition on dialogue„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It can't really understand this trade off„ÄÇüò°ÔºåAnd so for this reason„ÄÇwe actually
    always conditioned it on its truthful intentions„ÄÇNo„ÄÇIt is possible to have Cicero
    Viol condition on dialogue„ÄÇbut you would need way more data and it would make
    things much more expensive and so we weren't able to do it further for this spot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And finallyÔºå there's a big question that I mentioned earlierÔºå which is„ÄÇis there
    a more general way of scaling inference time compute to achieve better performanceÔºü
  prefs: []
  type: TYPE_NORMAL
- en: The way that we've done planning in Cicero is I would argue a bit domain specific„ÄÇI
    think it's like the idea of pickle is quite general„ÄÇbut I think that there are
    potentially more general ways of doing planning„ÄÇËØ∂„ÄÇSomebody's asking looking forward
    to the next two to three years„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: what criteria will you use to select the next game to try to conquer honestly„ÄÇLike
    I said we chose diplomacy because we thought it'd be the hardest game to make
    an AI for it and I think that that's true I don't think that we're going to be
    working on games anywhere because I can't think of any other game that if we were
    to succeed at that it would be truly impressive„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so I think where the research is going in the future„ÄÇIs generality„ÄÇLike„ÄÇInstead
    of getting an AI to play this specific gameÔºå can we get an AI that is able to
    play diplomacy„ÄÇbut could also play go or poker or could also write essays and
    stories and solve math problems and write theorems„ÄÇI think what we will see„ÄÇIs
    games serving as benchmarksÔºüFor progress„ÄÇBut not as„ÄÇThe goalÔºå you know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it'll be part of the test set but not part of the training set„ÄÇand I think that's
    the way it should be going forward„ÄÇFinallyly„ÄÇI want to add that diplomacy is an
    amazing testbed for multi agentent AI and Gred dialogue„ÄÇIf you are interested
    in these kinds of domainsÔºå I highly recommend taking advantage of the fact that
    we are„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We've open sourced all of our coded models and the dialogue and action data
    is available through a what's called an RFP where you can apply to get access
    to the dialogue and data„ÄÇOkayÔºå so thanks for listening to wrap of Cicero combined
    strategic reasonaing and natural language and diplomaacy in place in the top 10%
    of human players and the paper is in science and code of models are publicly available
    at this URL so thanks and for there any time I'll take questions great thanks
    a lot my talk„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we'll also open some questions from the classÔºå but you finished their Zoom
    questions so if anyone has like Zoom questions there I think no we can answer
    those„ÄÇYeahÔºå there's one question are you concerned about AIs out competingeting
    humans at real world diplomatic strategic negotiation at deception tasks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so like I said we're not very focused on deception even though you know arguably
    deception is a part of the game of diplomacy„ÄÇÁ≥ª„ÄÇThink for diplomatic and strategic
    negotiation„ÄÇI don't like look the way that we've developed is real„ÄÇIt's designed
    to play diplomacy the game of diplomacy specifically and you can't use it out
    of the box for other tasks that said I do think that the techniques are quite„ÄÇGeneral
    and so hopefully others can build on that and to be able to do different things„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and I think it is you know entirely possible that over the next several years„ÄÇyou
    will see this entering into real world negotiations much more often I actually
    think that diplomacy is a big step towards real world applicability compared to
    breakies in games like go and poker„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Because because now your action space is really like the space of natural language
    and you have to model human behavior„ÄÇDo you think in the future we could appoint
    AI to the UN CouncilÔºü
  prefs: []
  type: TYPE_NORMAL
- en: hopefully only if it does better than humansÔºå but that would be very interesting
    to see„ÄÇGreat„ÄÇI'm also curious like what what's like the future things that you're
    working on in this direction like do you think you can do something like Alpago
    zero where you just like take this take this like repeated model and then maybe
    just make it Excel play or like what sort of future actions are you thinking for
    improving this sort of box„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåI think the future directions are really focused around generality„ÄÇlike I
    think one of the big insights of Cicero is like this ability to leverage planning
    to get better performance with language models and in this strategic domain„ÄÇI
    think there' was a lot of opportunity to do that sort of thing in a broader space
    of domains„ÄÇI mean you look at language models today and they do token by token
    prediction„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And I think there's a big opportunity to go beyond that so that that's what
    i'm excited to look into I'm also curious like I didn't understand the exact details
    how using planning or Montete research with your like the models that you have„ÄÇSo
    is it like we didn't use Monla research in CiceroÔºå Mongla research„ÄÇIs a very good
    heuristic„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it's a heuristic that is„ÄÇParticularly useful for deterministic perfect information
    games„ÄÇAnd I think in order to like have a truly general form of planning we need
    to go more abstract than multic research we use this algorithm called pickle based
    on a regret minimization algorithm I don't really want to go into the details
    of it because it's not that important for the class„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but the idea is like it is this iterative algorithm that will gradually refine
    the prediction of what everybody's going do and get better and better predictions
    the more iterations that you run„ÄÇBut and that's similar research„ÄÇYeah„ÄÇCoÔºå yeahÔºå
    sure„ÄÇÂØπ„ÄÇGo for itÔºå you're un mututed„ÄÇOkay„ÄÇSo yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: my question is like when we were talking about gender likeibility„ÄÇhow does the
    communication between different modules of the model look likeÔºü
  prefs: []
  type: TYPE_NORMAL
- en: Particularly when we're talking about the dialogue model„ÄÇLike how do you send
    information from the policy network to the dialogue model and in the future„ÄÇif
    you have a model that's good at different tasks„ÄÇare we going to have like a really
    big policy net that learns all of them or like separate language modules for all
    of them like how do you break it downÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So we actually convert the policy like action for ourselves and for our dialogue
    partner into a string naturaltro language string and to that into the dialogue
    model along with all the dialogue that it's had so far„ÄÇso it's just all text in
    text out„ÄÇAnd that works great„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then what was the second part of your questionÔºüSomething like„ÄÇare we just
    going to have like one giant policy net trained on everythingÔºüYeah„ÄÇit was like
    so if you're only using text first doesn't it limit the model and if you're using
    it for different games like are you thinking like when you say in the future you
    will work on generalizability„ÄÇare you thinking about a big policy network that
    is trained on separate games or is able to like understand different games at
    the same time or do we have like separate policy networks for different gamesÔºü
  prefs: []
  type: TYPE_NORMAL
- en: And yeahÔºå like doesn't this like text interface limit the model in terms of
    communication like if you're using vectors it might like„ÄÇYeahÔºå it might be a bit
    of a bottom way„ÄÇI meanÔºå I think ideally you go in this direction where you have
    like you know a foundational model that works for pretty much everything does
    text I mean certainly yeah just like a text in text out that like limits what
    you can do in terms communication„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but hopefully we get beyond that„ÄÇ![](img/e3abef98963297997c09e5d05b736a64_7.png)
  prefs: []
  type: TYPE_NORMAL
- en: I think it's a reasonable choice for now„ÄÇThank you„ÄÇWe questions„ÄÇAnd I think
    we have more some questions Okay so there's a question in the chat i'd love to
    hear speculation on the future for instance we've seen some startups that are
    fine tuning LLMs to be biased or experts in say subject experts of„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This seems like a pretty general question„ÄÇÁ≥ª„ÄÇDon't have strong opinions on this„ÄÇËØ∂„ÄÇÁ≥ª‰∏ãÈù¢„ÄÇLikeÔºå
    I'm not„ÄÇI'm not too„ÄÇI'm not too focused myself on fine tuning language models
    to specific tasks„ÄÇI think the direction that i'm much more interested in going
    forward is you know like the more general forms of planning„ÄÇso I don't think I
    can really comment on„ÄÇYou knowÔºå how do you„ÄÇTwo in these language models„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: In these ways„ÄÇSo what sort of planning methods are you like interested in looking
    at like like MCps is one so let me sorry I got to step out for just one second„ÄÇGot
    the switch roomsÔºå excuse me„ÄÇOkayÔºå never mind we're all good sorry what was the
    question Oh yes I was I was just asking like what sort of planning algorithms
    do you think are very interesting to combine So you can think like we have like
    so many options like we have like planet kind of stuff or Rl there's like MCps
    there's like the work you did with Cro so what do you think are the most interesting
    algorithms that you think will scale well can generalize„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåWell I think that's the big question that a lot of people are trying to figure
    out today and it's not really clear what the answer is I mean I think you know
    you look at something the chain of thought„ÄÇI think there's a lot of limitations
    to chain of thought„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and I think that it should be possible to do a lot better„ÄÇbut it is really impressive
    to see just how general of an approach it is„ÄÇAnd so I think it would be nice to
    see„ÄÇËØ∂„ÄÇTo see things that are general in that way„ÄÇbut hopefully able to achieve
    better performance„ÄÇGo„ÄÇÂ•Ω„ÄÇAlso„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: when you see like citra is like an encoder decoder model in the sense it encodes
    the world„ÄÇand then you have the dialg modelÔºå which is time to de it„ÄÇIt was an
    encoder decoder modelÔºå yes„ÄÇI don't think that that's necessarily the right choiceÔºå
    but that's what we used„ÄÇAny questionsÔºüÂóØ„ÄÇOkay I think youre mostly good„ÄÇAyÔºå thanks
    a lot this was a good well yeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: hope you all enjoyed it and if there are any questions feel free to email me
    reach out I'm happy to happy to chat„ÄÇ![](img/e3abef98963297997c09e5d05b736a64_9.png)
  prefs: []
  type: TYPE_NORMAL
