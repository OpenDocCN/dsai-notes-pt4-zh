- en: 斯坦福 GPT／Transformer 原理介绍 (中英文双字幕) - P14：14.Strategic Games - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯坦福 GPT/Transformer 原理介绍 (中英文双字幕) - P14：14.战略游戏 - life_code - BV1X84y1Q7wV
- en: '![](img/e3abef98963297997c09e5d05b736a64_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3abef98963297997c09e5d05b736a64_0.png)'
- en: '![](img/e3abef98963297997c09e5d05b736a64_1.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3abef98963297997c09e5d05b736a64_1.png)'
- en: So it spend sometimes like two months training the bots。On thousands of CPUs。Tabytes
    and memory sometimes， but when it came time to actually play against the humans。they
    would act almost instantly， it was just a lookup table。And the humans。when they
    were in a tough spot， they would not act instantly。
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 所以有时候需要两个月来训练这些机器人。在成千上万的CPU上，有时候需要数TB的内存，但当真正与人类对战时，他们几乎是瞬间行动的，这只是一个查找表。而人类在困境中并不会立即行动。
- en: they would think they would sit there and they would think for five seconds。maybe
    five minutes if it was a really difficult decision and it was clear that that
    was allowing them to come up with better strategies。And so I wanted to investigate
    this behavior in our bots like if we could add this to our bots。how much of a
    difference would it make， the ability to instead of acting instantly to take some
    time and compute a better strategy for the spot that the agent was in。
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 他们会坐在那里思考五秒钟，可能在非常困难的决定中思考五分钟，很明显，这使他们能够想出更好的策略。所以我想在我们的机器人中调查这种行为，看看如果我们能够将其添加到我们的机器人中，会有多大不同，能够不是立即行动，而是花些时间计算出更好的策略。
- en: And this is what I found。So on the X axis here we have like the number of buckets。the
    number you can think of this as like the number of parameters in your model and
    on the Y axis we have distance from national equilibrium so this is basically
    like how much you would lose towards worstk's adversary so the lower this number
    is the better your poker bot is and you can see as you scale the number of parameters。
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我发现的。因此，在X轴上我们有桶的数量，你可以把这个想象成模型中的参数数量，而在Y轴上我们有距离纳什均衡的距离，这基本上是你在最坏对手面前损失多少，所以这个数字越低，你的扑克机器人就越好，你可以看到随着参数数量的增加。
- en: your performance improves and as you increase the number of parameters by about100
    x you know your exploitability goes down by about half and indeed you're getting
    a much better poker bot。But you can see the blue line here is if you don't have
    search and the orange line is if you do add search。
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 你的表现有所提高，当你将参数数量增加约100倍时，你会发现可利用性降低了大约一半，实际上你得到了一个更好的扑克机器人。但你可以看到这里的蓝线是如果没有搜索，橙线是如果你添加了搜索。
- en: 😡，And you can see just adding search， adding the ability to sit there and think
    for a bit。improve the performance of these models。And it reduced the exploitability。the
    distance from ra equilibrium by about 7 x。And if you were to extend that blue
    line and see how many prim would you need in order to be comparable to adding
    search。the answer is you would need to scale up your model by about 100，000 x。Okay。
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 😡，你可以看到，仅仅添加搜索，增加坐下来思考的能力，就提升了这些模型的表现。它减少了可利用性，距离纳什均衡大约降低了7倍。如果你想延长那条蓝线，看看需要多少个参数才能与添加搜索相媲美，答案是你需要将模型的规模扩大约100,000倍。好的。
- en: So this was pretty mind blowlowing to me when I saw this， I mean， over the course
    of my PhD。the first three years， first three or four years of my PhD。I' managed
    to scale up these models by about 100 x。嗯。And I was proud of that， I mean。that's
    like a pretty pretty impressive result， I think。
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当我看到这一点时，我感到非常震撼，意思是，在我博士学位的过程中，前三年，前三到四年，我成功地将这些模型的规模扩大了约100倍。嗯。我对此感到自豪，我认为这真是一个相当相当令人印象深刻的结果。
- en: But what this plot was showing me was that just adding search。Was the equivalent
    of scaling things up by about 100，000 x。and so all of my previous research up
    until this point。Would just be a footnote compared to adding search。So。When I
    saw this， it became clear。
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但这张图表向我展示的是，仅仅添加搜索，相当于将规模扩大约100,000倍。所以我之前的所有研究到这一点为止，只能算作脚注，跟添加搜索相比。因此，当我看到这一点时，变得清晰起来。
- en: this was the answer to beating top humans in poker， and so for the next year，
    basically nonstop。I worked on scaling search。Now there's a question that naturally
    comes up which is why wasn't this considered before there's a few factors so first
    of all I should say like the search had been considered in poker before and it's
    actually quite natural to say like well you know if you had search and chess and
    search and go like why would you not consider search in poker。
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是战胜顶尖人类扑克选手的答案，因此在接下来的一年里，我几乎不间断地致力于扩展搜索。现在自然会出现一个问题，那就是为什么之前没有考虑到这一点。首先，我应该说扑克中的搜索在之前是被考虑过的，实际上可以说在国际象棋和围棋中都有搜索，那么为什么在扑克中不考虑搜索呢？
- en: There's a few reasons， one is that。Culturally like the poker research grew out
    of game theory and reinforcement learning and so it wasn't really from the same
    background as like the people are working on chess and working on go。When you
    scale search scaling test time compute it makes all your experiments much more
    expensive and just like more unpleasant to work with and there are just like incentive
    structures I mean people are always thinking about winning the next annual computer
    poker competition and the ACPC limited the resources that you could use at test
    time so search wasn't really possible effectively in the ACPC。
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个原因，其一是文化方面，扑克研究起源于博弈论和强化学习，并不是与从事国际象棋和围棋研究的人相同的背景。当你扩展搜索，扩展测试时间的计算时，会使你的实验成本大幅增加，也让工作变得更加不愉快。还有一些激励结构，毕竟人们总是想着赢得下一个年度计算机扑克比赛，而ACPC限制了你在测试时可以使用的资源，因此在ACPC中实际上是无法有效进行搜索的。
- en: And I think the biggest factor is that people just didn't think it would make
    such a huge difference I mean I think it's reasonable to look at something like
    search and think like oh yeah that might make like a 10 X difference you probably
    wouldnt think it makes a 10000 x difference and so there were some people working
    on it but it wasn't really the focus of a lot of people's research。
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为最大的因素是人们根本没有想到这会产生如此巨大的差异。我认为合理地看待像搜索这样的东西，认为它可能会带来10倍的差异，但你可能不会想到它会产生10,000倍的差异，因此虽然有一些人在研究这个，但它并不是很多人研究的重点。
- en: So anyway， focused on scaling search and that led to the 2017 Bras versus AIT
    competition where we again played our bot against four top poker pros。120，000
    hands of poker $200，0000 prize money and this time the bot won by 15 big blinds
    per100 instead of nine big blinds per100。
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 所以无论如何，专注于扩展搜索，这导致了2017年Bras与AIT的比赛，我们再次将我们的机器人与四位顶级扑克高手对战。120,000手扑克，$200,000的奖金，这次机器人每100手赢了15个大盲注，而不是之前的9个大盲注。
- en: This was a crushing victory， each human lost individually to the bot and。![](img/e3abef98963297997c09e5d05b736a64_3.png)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一场毁灭性的胜利，每位人类选手都单独输给了机器人。![](img/e3abef98963297997c09e5d05b736a64_3.png)
- en: Four standard deviations of statistical significance。We followed this up in
    2019 with six player poke AI competition。The big difference here is that we figured
    out how to do depth limited search。so before the 2017 bot it would always have
    the search to the end of the game here it only had to do search a few moves ahead
    and it could stop there。
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 四个标准差的统计显著性。我们在2019年进行了六人扑克人工智能比赛。最大的区别在于我们找到了如何进行深度限制搜索。因此，在2017年的机器人比赛中，它总是需要搜索到游戏的尽头，而这次只需搜索几步之后就可以停止。
- en: And so this time again， it wont with statistical significance。and what's really
    surprising about this spot is that despite it being a much larger game。the six
    player Pokerbot Plebuvis cost under $150 to train on cloud computing resources
    and it runs on 28 CPU cores at inference time。there's no GPUs。
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这次再次获得了统计显著性。令人惊讶的是，尽管这是一个更大的游戏，这个六人扑克机器人Plebuvis在云计算资源上训练的费用不足150美元，并且在推理时运行于28个CPU核心上，没有使用GPU。
- en: '![](img/e3abef98963297997c09e5d05b736a64_5.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3abef98963297997c09e5d05b736a64_5.png)'
- en: So I think what this shows is that。This really wasn an algorithmic improvement。I
    mean this would have been doable 20 years ago if if people knew how to do it。And
    I think it also shows the power of search， you know。if you can figure out how
    to scale that compute at test time。
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为这表明，这实际上并不是算法上的改进。我是说，如果人们知道怎么做，20年前就能做到这一点。我认为这也显示了搜索的威力，如果你能找出如何在测试时扩展计算。
- en: it really can make a huge difference and bring down your your training costs
    by a huge amount。诶。😊。Okay， anyway， yeah， so I wanted to say also this is not limited
    to poker if you look at go you see a similar pattern。so this is a plot from the
    alphago zero paper。On the X axis。we have different versions of alphaphago and
    on the y axis， we have elo rating。
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这真的可以带来巨大的差异，并大幅降低你的训练成本。诶。😊。好的，无论如何，我还想说，这不仅限于扑克，如果你看看围棋，你会看到类似的模式。这是来自AlphaGo
    Zero论文的一个图。X轴是不同版本的AlphaGo，Y轴是elo评分。
- en: which is a way of comparing different bots， but also a way of comparing bots
    to humans。And you can see if。Okay， so superhuid performance is around 3，600 elo。And
    you can see Alpha Go Lee。the version that played against Li Addal in 2016， that's
    right over the line of superhuman performance。Alphago is zero， the strongest version
    of alphaphago is around 5，200 elo。
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种比较不同机器人，以及将机器人与人类进行比较的方法。你可以看到，如果你查看超级人类的表现，大约是3600 elo。你可以看到Alpha Go Lee，2016年与李世石对弈的版本，正好超过超级人类表现的线。最强的AlphaGo版本大约是5200
    elo。
- en: But if you take out the test time search， if you just play according to the
    policy net and not do any modc research in alphago was zero at test time。then
    the eO rating drops to around 3，000， which is substantially below expert human
    performance。So what this shows is that if you take out Montegli research at test
    time。😡。The alphaphaGo zero is not superhuman and in fact nobody has made a superhuman
    gobot that does not use search in some form。
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你去掉测试时间的搜索，如果你只是根据策略网络进行游戏，而不在AlphaGo Zero的测试时间进行任何蒙特卡洛研究，那么elo评分会降到大约3000，这远低于专家人类的表现。这显示，如果在测试时间去掉蒙特卡洛研究，😡，AlphaGo
    Zero并不是超级人类的，实际上没有人制作出不使用某种形式搜索的超级人类围棋机器人。
- en: nobody has made a raw neural network that can beat top humans in go。And I should
    say also。this is just if you're taking out the search at test time。I'm not even
    talking about taking it out of training time， if you took it out of training time。we
    wouldn't even got off the ground。Now there's a question of like okay well。
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人制作出能击败顶级人类的原始神经网络。如果我还要说，这是如果在测试时间去掉搜索。我甚至不谈在训练时间去掉搜索，如果你在训练时间去掉它，我们甚至无法起步。现在有一个问题就是，好吧。
- en: surely you could just scale up the model， scale up the matter of training and
    you would eventually you know surpass the humanment performance and match the
    performance if you added search and that's true yes。if you scale up the models
    and if you scale up the training。
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以扩大模型，增加训练的量，最终超越人类的表现，如果你添加搜索，这也是正确的。如果你扩大模型和训练规模。
- en: then you would eventually match the performance with search。but there's a question
    of like how much would you have to scale it up by。Now。a rough rule of thumb is
    that in order to increase your eO rating by about 120 points。you either have to
    double the amount of model size and training or you have to double the amount
    of test time search。
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你最终会通过搜索匹配表现。但是有一个问题是，你需要扩大多少规模。大致经验法则是，要将你的elo评分提高约120点，你要么需要将模型大小和训练量翻倍，要么需要将测试时间的搜索翻倍。
- en: And so if you look at that gap of around 2000 elow points and you calculate
    the number of deadlings that you would need。the answer is that in order to get
    the raw policy net from 3，000 e to 5200 elow。you would need to scale your model
    and your training by about 100，00x。Okay。so why is this important？😡，I think you
    look at what's happening today with large language models and transformers and
    and you see something similar。
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看那大约2000 elo点的差距，计算一下你需要的deadlings数量，答案是为了让原始策略网络从3000 elo提升到5200 elo，你需要将模型和训练扩大大约100000倍。好的。那么这为什么重要呢？😡，我认为看看今天的大型语言模型和变换器，你会看到类似的情况。
- en: I mean， you're getting。Huge there's a question of like what do I mean by search
    there's a specific kinds of search like multiult research。the ability to just
    like plan ahead what you're going to do instead of just like acting instantly
    based on your precomputed policy。
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我的意思是，你得到了。巨大的问题是，我所说的搜索是什么意思，有一种特定的搜索类型，比如多目标搜索，能够提前规划你将要做的事情，而不是仅仅根据预先计算的策略瞬间行动。
- en: but really what I mean by search more broadly is the ability to scale the amount
    of computation to get better performance。I think that's the real value that search
    is adding instead of just like acting according to your precomputd。You know， front
    loading all of your computations so you're doing everything all your computation
    ahead of time and then like at inference time acting basically instantly。Could
    you get a better solution if you had five minutes to output an action instead
    of 100 milliseconds？
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 但我更广义上所指的搜索是能够扩展计算量以获得更好的性能。我认为这才是真正的价值，而不仅仅是根据你的预计算来行动。你知道，把所有计算都提前完成，然后在推理时几乎瞬间行动。如果你有五分钟输出一个动作，而不是100毫秒，能否得到更好的解决方案？
- en: Okay。So。Yeah， I think you look at。I'm sorry， there's a question。does a transformer
    with a search circuit count as search。or do you mean and engineering search algorithms？I
    don't want to get bogged down into like the details of like how to do this because
    like the answer is nobody really knows yet。nobody really has a general way of
    doing search and all the domains that we've done search successfully like poker
    and go it's done in a fairly domain specific way。
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧。所以。是的，我觉得你要看一下。抱歉，有个问题。带有搜索电路的变压器算作搜索吗？还是你指的是工程搜索算法？我不想深入探讨如何做到这一点，因为答案是目前没人真正知道。我们在扑克和围棋等领域成功进行搜索的方式都是相当特定于领域的。
- en: you know go use this algorithm called multiola research。And yeah。you could think
    of beamS searcharch as like one simple form of search， but。it does seem like there
    should be better， better ways in the future。Yeah。So anyway。where I'm going with
    this is like you look at how large language models are being trained today and
    you know。
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，去使用这个叫做多层研究的算法。是的。你可以把束搜索看作一种简单的搜索形式，但看起来未来应该会有更好的方法。是的。所以，无论如何，我的意思是，你要看看今天大型语言模型是如何训练的。
- en: you're seeing millions of dollars being thrown at pre training like。I wouldn't
    be surprised if we see a large language model that would cost like $100 million
    to train。We might even get to a billion dollars。But。The inference cost is still
    going to be very small。And so there's a question of like， could you do substantially
    better if you could？
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到数百万美元被投入到预训练中。 我不会惊讶如果我们看到一个大型语言模型训练成本达到1亿美元。我们甚至可能达到10亿美元。但推理成本仍然会非常小。因此有个问题：如果可以的话，你能否做得更好？
- en: Scall the amount of inference cost as well。Maybe that could like amortize some
    of your training cost。Yeah。Okay， so there's this。There's this lecture called the
    Bi lessons by Richard Sutton that says the biggest lesson that can be learned。and
    so it's a really great essay I recommend reading it。but one of the big takeaways
    is like he says the biggest lesson that can be learned from over 70 years of AI
    research is that general methods that leverage computation are ultimately the
    most effective。
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 也要考虑推理成本的规模。也许这可以摊销你的一些训练成本。是的。好吧，有一场讲座叫做理查德·萨顿的《Bi教训》，它说最大的教训就是可以学到的。所以这是篇很好的文章，我推荐阅读。但其中一个主要的收获是，他说从70多年AI研究中学到的最大教训是，利用计算的通用方法最终是最有效的。
- en: The two methods that seem to scale arbitrarily in this way are search and learning。Now。I
    think we've done a great job with generalizing search， sorry， generalizing learning。and
    I think there's still room for improvement when it comes to search。U and yeah。the
    next goal really is about generality， like can we develop a truly general way
    of scaling inference compute instead of just doing things like Montecalo research
    that are specific to a better domain to a specific domain。
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 似乎以这种方式无限扩展的两种方法是搜索和学习。现在。我认为我们在推广搜索方面做得很好，抱歉，推广学习方面。我认为在搜索方面仍然有改进的空间。你知道，接下来的目标真的就是通用性：我们能否开发一种真正通用的方法来扩展推理计算，而不仅仅是做诸如蒙特卡洛研究那样特定于某一领域的事情。
- en: And also better than things like chain of thought。嗯。What this would look like
    is that you have much higher test time compute。But you have much more capable
    models。And I think for certain domains that trade off is worth it。like if you
    think about what inference cost we're willing to pay for proof of the Raymond
    hypothesis。
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 而且这比诸如思维链之类的东西要好。嗯。这看起来会是你有更高的测试时间计算，但你有更强大的模型。我认为在某些领域，这种权衡是值得的。如果你考虑到我们愿意为“雷蒙德假设”的证明支付的推理成本。
- en: I think we'd be willing to pay a lot。Or you know， the cost of。What cost were
    we want to pay for new life saving drugs， I think we'd be willing to pay a lot。So
    I think that there is an opportunity here。Okay， anyway， so that's why I prelude。I
    guess any questions about that before I move on to Cicero？By the way。
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们愿意支付很多钱。或者你知道，这个成本。我们愿意为新的救命药物支付多少成本，我认为我们愿意支付很多。所以我觉得这里有一个机会。好吧，反正，这就是我前面的铺垫。我想在我继续谈论西塞罗之前，有什么问题吗？顺便说一下。
- en: the reason why I'm talking about this is because it's going to inform。😡。The
    approach that we took to Cicero， which I think is quite different from the approach
    that a lot of other the researchers might have taken to this problem。Someone asked
    can you give an example of search well multicarural research is one form of search。you
    could also think of like breadth for search depth for search these kinds of things
    they're all search I would also argue that like chain of thought is doing something
    similar to search where it it's allowing the model to like leverage extra compute
    at test time to get better performances。
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我谈论这个的原因是因为它将影响。😡。我们对西塞罗的处理方式，我认为与其他许多研究人员可能采取的方式有很大不同。有人问，能否举一个多模态研究的搜索例子。你也可以想象像广度优先搜索、深度优先搜索这些类型的搜索，它们都是搜索。我还想说，思维链在某种程度上类似于搜索，因为它允许模型在测试时利用额外的计算资源，以获得更好的性能。
- en: But I think that that's the main thing that you want。the ability to like leverage
    extra compute at a test time。What's the search what's the space that you are searching
    over again like in in a game like go it's like different board positions。but you
    could also imagine searching over like you know different sentences that you could
    say things like that。
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 但我认为这就是你想要的主要内容。能够在测试时利用额外的计算资源。搜索的空间是什么？你在搜索什么？在围棋这样一个游戏中，就是不同的棋盘位置。但你也可以想象搜索像不同的句子之类的东西。
- en: There's a lot of flexibility there as well。Okay， so now I want to get into Cicero
    so first thing I should say when it comes to Cicero。this is。A big team effort。This
    was like this is actually one of the great things about working on this project
    that there was just such a diverse talent pool。
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也有很多灵活性。好吧，现在我想谈谈西塞罗。首先我应该说，关于西塞罗。这是一个大型团队合作。这实际上是参与这个项目的一个伟大之处，因为这里有如此多样的人才。
- en: experts in reinforcement learning， planning， game theory， natural language processing。all
    working together on this and it would not have been possible without without everybody。So
    the motivation for diplomacy actually came from 2019。we were looking at all the
    breakthroughs that were happening at the time and I think a good example of this
    is this XKCD comic that came out in 2012 that shows like different categories
    games。
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习、规划、博弈论和自然语言处理方面的专家，大家一起合作，这一切都是不可能的，没有每个人的努力。因此，外交的动机实际上来自2019年。我们当时在关注所有的突破，我认为一个很好的例子是2012年发布的这幅XKCD漫画，展示了不同类别的游戏。
- en: games that are solved， games where computers can beat documents。games where
    computers still lose documents and games where computers may never outplay documents。And
    in this category， computer still lose to top humans you had four games， go Arima。poker
    and Starcraftft。In 2015。😊，Actually， one of my colleagues。
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 已经解决的游戏，计算机可以击败文档的游戏。计算机仍然会输给文档的游戏，以及计算机可能永远无法超越文档的游戏。在这一类别中，计算机仍然输给顶级人类的四个游戏，包括围棋、阿里马、扑克和星际争霸。在2015年。😊实际上，我的一位同事。
- en: David Wu made the first day out to beat Top humans in arema。in 2016 we have
    Algo beating Lisa do and Go。In 2017。you have the work that I just described where
    we beattop humans in poker。And in 2019。we had Alpha star beating X humans in Starcraftft。So
    that shows the incredible amount of progress that had happened in strategic reasoning
    over the past several years。leading up to 2019。And at the same time， we also had
    GP2 come out in 2019。and it showed that language model like。And natural language
    processing was progressing much faster than I think a lot of people。including
    us expected。And so we were thinking about what after the sixth player poker work
    I was discussing with my colleagues。
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 大卫·吴在2016年首次突破，战胜了顶尖人类选手。在2016年，我们的算法战胜了Lisa在围棋中的表现。在2017年，我刚刚描述的工作中，我们在扑克比赛中击败了顶尖人类。而在2019年，我们的AlphaStar战胜了多名人类选手在《星际争霸》中。这显示了在战略推理方面，在过去几年中发生的惊人进展，直至2019年。同时，我们在2019年也推出了GP2，这表明语言模型和自然语言处理的进展速度远超许多人的预期，包括我们自己。因此，我们在思考第六位扑克选手工作的下一步。
- en: what should we work on next？And we were throwing around like different domains
    to work on。Given the incredible amount of progress in AI， we wanted to pick something
    really ambitious。something that we thought。You couldn't just tackle by scaling
    up the existing approaches that you really needed something new in order to address。And
    we landed on diplomacy because we thought that it would be the hardest game to
    make an AI for。
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一步应该做什么？我们讨论了不同的领域。考虑到人工智能的惊人进展，我们希望选择一个非常雄心勃勃的项目，一项我们认为无法仅通过扩大现有方法来解决的任务，因此我们选择了外交，因为我们认为这是为人工智能开发的最困难的游戏。
- en: So what is diplomacy？Diploomacy is a natural language strategy game。it takes
    place right before World War I you play as one of the seven great powers of Europe。England，
    France， Germany， Austria， Russia and Turkey。and your goal is to control a majority
    of the map in practice that rarely happens like if you control a majority of a
    majority of the map and you've won in practice nobody ends up winning outright
    and so your score。
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，什么是外交？外交是一种自然语言策略游戏。游戏发生在第一次世界大战前，你将作为欧洲七大强国之一进行游戏：英格兰、法国、德国、奥地利、俄罗斯和土耳其。你的目标是在地图上控制大多数区域，实际上，这种情况很少发生；如果你控制了大多数区域，实际上就意味着你赢了，但通常没有人能彻底获胜，因此你的得分会受到影响。
- en: Is proportional to the percentage of the map that you control？
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这与您控制的地图百分比成正比吗？
- en: Now what's really interesting about diplomacy is that it is a natural language
    negotiation game。so you have these conversations like what you're seeing here
    between Germany and England where they will probably communicate with each other
    before making their moves and so you can have Germany ask like want to support
    Sweden。
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，外交游戏中真正有趣的是，它是一种自然语言的谈判游戏。因此，你会看到德国和英格兰之间的对话，他们会在行动前进行交流，比如德国可能会问：“你想支持瑞典吗？”
- en: England says let me think on that and so on。So this is a popular strategy game
    developed in the 1950s。it was JFK and Kisser's favorite game actually。And like
    I said。each chart involves sophisticated private natural language negotiations。and
    I want to make clear like this is not。Negotiations like you would see in a game
    like Sotos and Katan。
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 英格兰说：“让我想想。”所以这是一个在1950年代开发的受欢迎的策略游戏。实际上，这也是JFK和基辛格的最爱。正如我所说，每一局都涉及复杂的私人自然语言谈判。我想明确指出，这并不是像你在游戏《卡坦岛》中看到的那种谈判。
- en: for example， you're seeing。It's much more like survivor， if you've ever seen
    a TV show survivor。you have discussions around like alliances that you'd like
    to build discussions around like specific tactics that you'd like to execute on
    the current current and also you know like more long- term strategy around like
    where do we go from here and how do we divide resources。
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你会看到，这更像是一场生存游戏，如果你看过真人秀《幸存者》。你会围绕想要建立的联盟进行讨论，围绕想要在当前局势中执行的具体战术进行讨论，还会讨论更长期的策略，比如我们从这里往哪里去，以及如何分配资源。
- en: Now the way the game works， you have these negotiations that last between five
    and 15 minutes depending on the version of the game。and then on each term and
    all these negotiations are done privately in parawise negotiation。also I think
    that you are not muted， okay thank you。And then after the negotiation period completes。everybody
    will simultaneously write down their moves。😡。
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在游戏的运作方式是，谈判持续时间在五到十五分钟之间，具体取决于游戏的版本。而且所有这些谈判都是以对等的方式私下进行的。我还认为你没有被静音，谢谢你。然后在谈判期结束后，大家会同时写下他们的行动。😡
- en: And so a player could promise you something like I'm going to support you into
    this territory this term。but then when people actually write down their moves。they
    might not write that down and so you only find out if they were true to their
    word when all the moves are revealed simultaneously。And so for this reason， alliances
    and trust building is key。
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，一个玩家可能会承诺你，比如说，我会在这个回合支持你进入这个领土。但是当人们实际写下他们的行动时，他们可能不会这样写，所以你只有在所有的行动同时揭晓时才能知道他们是否遵守了诺言。因此，联盟和建立信任至关重要。
- en: the ability to trust that somebody's going to follow through on their promises。that's
    really what this game is all about and the ability to convince people that you
    are going to follow through on your promises is really what this game is all about。And
    so for this reason。Diploacy has long been considered a challenge problem for AI。There
    research in the game going back to the 80s， the research really only picked up。
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 相信有人会履行承诺的能力，这实际上就是这个游戏的核心，而说服他人你会履行承诺的能力同样是这个游戏的核心。因此，出于这个原因，外交长期以来被认为是人工智能的挑战性问题。关于这个游戏的研究可以追溯到80年代，研究真正开始得到重视。
- en: It picked up like quite intensely in starting in 2019 when researchers from
    DeepMd， ourselves， MiA。other places started working on this。Now a lot of that
    research。the vast majority of that research actually was focused on the non languageguage
    version of the game。which was seen as a stepping stone into the full natural language
    version。
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这方面的研究在2019年开始时非常激烈，当时DeepMind的研究人员，以及我们自己和其他地方的人开始进行这方面的工作。现在很多研究，绝大多数研究实际上集中在游戏的非语言版本上，这被视为通向完整自然语言版本的垫脚石。
- en: though we decided to focus from the start on the full natural language version
    of the game。So to give you a sense of like what these negotiations and dialogue
    look like。here is one example so here。England， you can see they move their fleet
    in Norway to St。Petersburg and that occupies the Russian territory。
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们决定从一开始就专注于游戏的完整自然语言版本。为了让你了解这些谈判和对话的样子，这里有一个例子。在这个例子中，英格兰，你可以看到他们将舰队从挪威移动到圣彼得堡，这占领了俄罗斯的领土。
- en: And so this is what the board state looks like after that move and now there's
    this conversation between Austria and Russia。Austria says， well what happened
    up north Russia says England stabbed I'm afraid the end may be close from you
    my friend Austria says yeah that's rough are you going to be okay up there Russia
    says I hope so England seems to still want to work together Austria says can you
    make a deal with Germany so the players are now discussing like what should be
    discussed with other players Russia says good idea then Austria says if we find
    as long as you can defend Sevetopol so Sevesttopol is this territory down to the
    south you can see that Turkey has a fleet and an army in the Black Sea in Armen
    next to Sevestoppo and so they could potentially attack that territory next turn。
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这就是在那次行动后的棋盘状态，现在奥地利和俄罗斯之间有这样的对话。奥地利说，北边发生了什么？俄罗斯说，英格兰背叛了，我担心你我朋友的结局可能要来了。奥地利说，是的，那很糟糕，你能在那边撑得住吗？俄罗斯说，我希望如此，英格兰似乎仍然想要合作。奥地利说，你能和德国达成协议吗？所以玩家们现在在讨论应该和其他玩家讨论什么。俄罗斯说，好主意，然后奥地利说，如果我们能找到，只要你能保住塞瓦斯托波尔就行。塞瓦斯托波尔是南边的一个领土，你可以看到土耳其在黑海和塞瓦斯托波尔旁边有一支舰队和一支军队，他们可能会在下一轮攻击那个领土。
- en: 嗯。😊，Austria says can you support hold Sevetooles Ukraine and Romania I'll support
    hold Romania。Russia says yep， they'm already doing so， Austria says awesome。hopefully
    we can start getting you back on your feet。So this is an example of the kinds
    of conversations that you'll see in a game of diplomacy in this conversation。
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。😊，奥地利说，你能支持保住塞瓦斯托波尔、乌克兰和罗马尼亚吗？我会支持保住罗马尼亚。俄罗斯说，是的，他们已经在这样做，奥地利说，太好了。希望我们能开始让你恢复元气。这是你在外交游戏中会看到的对话的一种例子。
- en: Austria is actually our bot Cicero。😊，So that kind of gives you a sense of the
    sophistication of the agent dialogue。诶。Okay I'll skip this for okay so I guess
    I'll go into this I don't want to take up too much time really what makes diplomacy
    interesting is that support is key so here for example。
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 奥地利实际上是我们的机器人西塞罗。😊，这让你对代理对话的复杂性有了一些了解。诶。好的，我会跳过这一点，所以我想我会进入这个，我不想占用太多时间，实际上让外交变得有趣的是支持是关键，所以这里举个例子。
- en: Budapest and Warsaw the red and the purple units both try to move into glacia
    and so since it's a one verse one they both bounce back and now they was into
    the territory in the middle panel you can see Vienna supports Budapest into glacciia
    and so now it's a two verse one and that red unit will indeed enter Galacciia。
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在布达佩斯和华沙，红色和紫色单位都试图进入格拉奇亚，由于这是一个一对一的对抗，他们都反弹回去。现在在中间面板上，你可以看到维也纳支持布达佩斯进入格拉奇亚，因此现在是两个对一个，那个红色单位确实会进入格拉奇亚。
- en: And what's really interesting about the po is that it doesn't just have to be
    your own units that are supporting you。it could be another player's units as well，
    so for example。the green player could support the red player into glacciia and
    then that ready unit would still go in there。So support is really what the game
    is all about negotiating over support and so for that reason diplomacy has this
    reputation as the game that ruins friendships it's really difficult to have an
    alliance with somebody for three or four hours and then have that。
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 而关于这个协议真正有趣的是，不仅仅是你自己的单位在支持你，其他玩家的单位也可以支持你。例如，绿色玩家可以支持红色玩家进入格拉奇亚，那么那个红色单位仍然会进入那里。所以支持实际上就是游戏的全部，谈判关于支持的事情，因此，外交有了“毁掉友谊的游戏”的名声，和某人维持三到四个小时的联盟真的很困难，然后再有这样的情况。
- en: 😊，Have them backstb you and basically is ruining your game。But if you talk to
    expert diplomacy players， they view it differently。They say diplomacy is ultimately
    about building trust in an environment that encourages you to not trust anyone。And
    that's why we decided to work on the game， you know。
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，让他们背叛你，这基本上是在毁掉你的游戏。但如果你和专业外交玩家交谈，他们的看法会有所不同。他们认为，外交最终是建立信任的过程，而环境却促使你不去信任任何人。这就是我们决定致力于这个游戏的原因，你知道的。
- en: could we make an AI that is able to build trust with players in an environment
    that encourages them to not trust anybody can thebo like honestly communicate
    that it's going to do something and and evaluate whether another person is being
    honest when they are saying that they're going to do something。
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否创造一个能够在一个鼓励玩家不信任任何人的环境中建立信任的AI？它能否诚实地沟通自己将要做的事情，并评估另一个人在说他们要做某事时是否诚实？
- en: Okay so。W diplomacy， it sits in this nice intersection of reinforcement learning
    and planning。and also natural language。There's two perspectives that we can take
    on why diplomacy is a really interesting domain one is the multiedging perspective
    so here。
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以。在外交游戏中，它恰好位于强化学习和规划的交汇点上，还有自然语言。我们可以从两个角度来看为什么外交是一个非常有趣的领域，一个是多边的视角。
- en: 😡，All the previous game we have results like chess go poker。these have all been
    in purely zero sum two player zero sum domains。and in these domains selfplay is
    guaranteed to converge to an optimal solution basically what this means is you
    can start to having the bot play completely from scratch with no human data。And
    by playing against itself repeatedly， it will eventually converge to this unbeatable
    optimal solution called the mini Max equilibrium。
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 😡，之前的所有游戏，比如国际象棋、围棋和扑克，这些都是纯粹的零和两个玩家的领域。在这些领域，自我对弈可以保证收敛到最优解，基本上这意味着你可以让机器人完全从头开始游戏，没有任何人类数据。通过反复自我对弈，它最终会收敛到这个无与伦比的最优解，称为迷你最大均衡。
- en: But that result only holds in two players zero sum。that whole paradigm only
    holds in two players zero sum games。When you go to domains that involve cooperation，
    in addition to competition。Then success requires understanding human behavior
    and conventions you can't just treat the other players like machines anymore。
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个结果仅在两个玩家的零和游戏中成立。这整个范式仅适用于两个玩家的零和游戏。当你进入需要合作的领域时，除了竞争，成功就需要理解人类行为和约定，你不能再把其他玩家当作机器对待。
- en: you have to treat them like humans， you have to model。😡，Human irrationality，
    human subbotality。one example of this is actually language like。You can imagine
    if you were to train a bo completely from scratch in the game of diplomacy。Like
    the full natural language version of the game。there's no reason why the bot would
    learn to communicate in English。
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须像对待人类一样对待它们，你必须建模。😡人类的不理性，人类的有限性。其中一个例子实际上是语言。你可以想象，如果你从零开始训练一个机器人进行外交游戏，像这个游戏的完整自然语言版本，机器人没有理由会学会用英语交流。
- en: it would learn to communicate in some weird gibberish robot language。and then
    when you stick it in a game with six humans。it's not going to be able to cooperate
    with them。😡，嗯。So we have to find a way to incorporate human data and be able to
    learn how humans behave in order to succeed in this game。
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 它会学会用一些奇怪的机器人语言进行交流，当你把它放入与六个人类的游戏中时，它无法与他们合作。😡嗯。所以我们必须找到一种方法来整合人类数据，并能够学习人类的行为，以便在这个游戏中取得成功。
- en: 😡，诶 ok k 。There's also the NLP perspective。Which is that current language models。Are
    essentially just imitating human like text。Now there's been some progress with
    things like RLHF。but。That's still like not really the way that humans communicate
    they communicate with an intention in minds right they come up with this intention
    and then they communicate with the goal of communicating that intention and they
    understand that others are trying to do the same。
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 😡诶，好的。还有一个NLP的视角。当前的语言模型基本上只是模仿人类的文本。虽然在RLHF等方面取得了一些进展，但这仍然不是人类交流的方式，人类是有意图地进行交流，他们想出这个意图，然后以传达这个意图为目标进行交流，并理解其他人也在尝试做到这一点。
- en: 😡，And so there's a question of like， can we move beyond shitt chat？To grounded
    intentional dialogue。So Cicero is an AI agent for diplomaacy that integrates high
    level strategic play and open domain dialogue。And we use 50，000 human games of
    diplomacy acquired through a partnership with the website webdplomacy。net。So we
    entered Cicero in an online diplomacy league just to give you the results up front。
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 😡所以有一个问题，就是我们能否超越糟糕的聊天？实现有目的的对话。因此，Cicero是一个外交AI代理，它结合了高水平的战略玩法和开放领域的对话。我们使用了通过与网站webdplomacy.net合作获得的50,000场人类外交游戏。因此，我们将Cicero投入一个在线外交联盟，提前给你结果。
- en: Cicero was not detected as an AI agent for 40 games with 828 players。there was
    one player that mentioned after the fact that like they kind of like made a joke
    about us being a bot but they didn't really follow up on it and nobody else followed
    up on it and they later accuseduse somebody else of also being a bot so we weren't
    sure how seriously to take that accusation but I think it's sad to say it made
    it through all 40 games not being detected as bot and then we in fact we told
    the players afterwards that it was a bot the whole time these are the kinds of
    responses that we got people were quite surprised。
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Cicero在828名玩家的40场比赛中未被识别为AI代理。有一位玩家事后提到他们开玩笑说我们是机器人，但他们并没有真正跟进，其他人也没有跟进，后来他们还指控其他人也是机器人，因此我们不确定该如何认真对待这个指控，但我觉得很遗憾的是，它在40场比赛中都没有被识别为机器人，事实上我们在赛后告诉玩家它一直是机器人，这就是我们得到的回应，人们相当惊讶。
- en: pleasantly surprised fortunately， nobody was was upset with us but they were
    quite surprised that。There was a bot that had been playing this game with them
    the whole time。So in terms of results。Cicero placed in the top 10% of players，
    it's a high variance game and so if you look at players that played five or more
    games that played second out of 19。and it achieved more than double the average
    human score。
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，没有人对我们感到不满，但他们非常惊讶，竟然有一个机器人一直在与他们一起玩这个游戏。就结果而言，Cicero排在前10%的玩家中，这是一个高变异性的游戏，如果你看看玩了五场或更多比赛的玩家，它在19人中排第二，达到了超过两倍于平均人类得分的成绩。
- en: So I would describe this as a strong level of human performance。I wouldn't go
    as far as to say that this is superhuman by any means。but it is currently quite
    a strong result。Now， to give you a picture of how Cicero works。So。The input that
    we feed into the model is the board state and the recent action history that's
    shown on the top left here。
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我会将其描述为一种强大的人的表现水平。我不会说这绝对是超人，但这目前确实是一个很强的结果。现在，为了让你了解Cicero是如何工作的，我们输入到模型中的信息是棋盘状态和左上角显示的最近行动历史。
- en: and also the dialogue that it's had with all the players up until now。So that's
    going to get fed into a dialogue conditional action model that's going to predict
    what Cicero thinks all the players are going to do this turn and what they think
    we will do this term。
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 还有与所有玩家进行的对话直到现在。因此，这将被输入到一个对话条件行动模型中，该模型将预测 Cicero 认为所有玩家在这一轮将要做的事情，以及他们认为我们将在这一轮做什么。
- en: These。These lead to what we call anchor policies that are then used for planning。😡，嗯。Now。planning
    here。Again， this is like the part where we leverage extra compute at test time
    in order to get better performance。So essentially we take these initial predictions
    of what everybody's going to do。what are called anchor policies。And we improve
    upon these predictions using this planning process called pickle。
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导致了我们所称的锚定策略，这些策略随后用于规划。😡，嗯。现在，规划在这里。再次强调，这是我们在测试时利用额外计算资源以获得更好性能的部分。因此，我们基本上会对大家将要做的初始预测，即所谓的锚定策略进行改进，使用这一名为
    pickle 的规划过程。
- en: where basically we account for the fact that players。Willll pick actions that
    have higher expected value with higher probability we're essentially adding this
    like rationality prior to all the players to assume that they're not going to
    blunder as often as the model might suggest and they're going to pick smarter
    actions with higher probability than the initial model might suggest。
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们考虑到玩家将以更高的概率选择期望值更高的行动。我们本质上是为所有玩家添加了一种理性假设，假设他们不会像模型所暗示的那样经常失误，并且他们会以比初始模型所暗示的更高的概率选择更聪明的行动。
- en: And what we find is that this actually gives us a better prediction of what
    all the players will do than just relying on the raw neural net itself。This gives
    us the action that we actually play in the game。and it also gives us what we call
    intense。So intents are an action for ourselves and an action for the dialogue
    partner that we're speaking to。And now we have this dialogue conditional so we
    have we have this dialogue model that conditions on these intents。
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，这实际上比仅仅依赖原始神经网络给出了更好的对所有玩家将要做的事情的预测。这给出了我们在游戏中实际采取的行动，也给出了我们所称的意图。因此，意图是我们自己和我们正在对话的对话伙伴的行动。现在我们有这个对话条件，因此我们有一个对话模型，以这些意图为条件。
- en: so the intents are fed into the dialogue model along with the board state and
    action history and also the dialogue that we've had so so far and that dialogue
    model will then generate。Candidate messages that。Are conditional on those intents。These
    candidate messages go through a series of filters that filter out nonsense， grounding
    issues。and also like value action low expected value messages。And ultimately。
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，意图与棋盘状态、行动历史以及到目前为止的对话一起输入到对话模型中，这个对话模型随后将生成。基于这些意图的候选消息。这些候选消息会通过一系列过滤器，过滤掉无意义、基础问题，以及像低预期值消息这样的价值行动。最终。
- en: we get out a message to send to our dialogue partner。Now every time we send
    or receive a message。we will repeat this whole process。So there's actually a lot
    that is quite novel in Cicero and I'm going to try to talk about。The contributions
    as much as possible， I might go through this a little quickly just so we have
    time for questions。The first one is a controllable dialogue model that conditions
    on the game state and a set of intended actions for the speaker and the recipient。
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向对话伙伴发送消息。现在每次我们发送或接收消息时，都会重复整个过程。因此，Cicero 中有很多新颖的地方，我会尽量谈论。贡献尽可能多，我可能会稍微快一点，以便我们有时间进行提问。第一个是一个可控对话模型，它以游戏状态和说话者及接收者的一组意图行动为条件。
- en: So we have a question， what is the action space here for the model？诶。And the
    action space。For the action prediction model is like all the actions that you
    could take in the game that a player could take in the game。For the dialogue model，
    it's like messages that you can send。Got it， so oh the sick。H。😊，Okay。so we try
    what we call an intent model that predicts what actions people will take at the
    end of truthful turns。
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个问题，模型的行动空间是什么？诶。行动预测模型的行动空间是玩家在游戏中可以采取的所有行动。对于对话模型，它是你可以发送的消息。明白了，所以哦，那个病。H。😊，好的。我们尝试一个我们称之为意图模型的模型，预测人们在真实轮次结束时将采取的行动。
- en: basically where we're trying to predict what are people intending to do when
    they communicate a certain message。And。Then we use this to automatically annotate
    the data set with basically what we expect people's intentions were when they
    sent that message and we filter out。
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们试图预测人们在传达某条消息时的意图。然后，我们用这个自动注释数据集，基本上记录我们期望人们发送该消息时的意图，并过滤掉。
- en: We filter out as much as possible lies from the data set so that the text in
    the data is annotated with the truthful intention。And then during play Cicero
    conditions the dialogue model on the truthful intention that it intends to take。and
    the goal then is that the hope then is that it will generate a message consistent
    with that intention。And that is then fed into。Into everything else that' that's
    you know。
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尽可能从数据集中过滤掉谎言，以便数据中的文本注释真实意图。然后，在游戏中，西塞罗将对话模型基于其打算采取的真实意图进行条件化。目标是生成与该意图一致的消息。这又被输入到其他所有内容中。
- en: sorry that the intentions that we generate through planning are fed into the
    dial model。So to give you an example of what this looks like， this gives us a
    way to control the dialogue model through a set of intentions。😡，Like， here。诶。We
    are Cicero England in pink， and their action is to move to Belgium。among other
    things。And so if we feed this attention into the dialogue model。
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 抱歉，我们通过规划生成的意图被输入到对话模型中。所以给你一个例子，这让我们通过一组意图来控制对话模型。😡，就像这里。嘿。我们是西塞罗英格兰，穿粉色，他们的行动是移动到比利时，其他事项也是如此。如果我们将这个意图输入对话模型。
- en: then the message that might get generated is something like England saying to
    France。do you mind supporting me， do you mind supporting Eddie to Belgium？On the
    other hand。let's say Cicero's action is to support France to Belgium。Then if you
    feed that into the dial model。then the message that's generated might say something
    like。
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可能生成的消息就像是英格兰对法国说：“你介意支持我吗？你介意支持埃迪去比利时吗？”另一方面，假设西塞罗的行动是支持法国去比利时。那么，如果你将其输入对话模型，生成的消息可能会是这样的。
- en: let me know if you want me to support you to Belgium， otherwise I'll probably
    poke Hol。Now。what we find is that conditioning the dialogue model on these intentions
    in this way。it makes the model more controllable， but it also leads to higher
    quality dialogue with less nonsense。So we found that it led to dialogue that was
    more consistent with the state。
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想让我支持你去比利时，请告诉我，否则我可能会打扰霍尔。现在，我们发现以这种方式将对话模型与这些意图结合起来，可以使模型更加可控，同时也提高了对话质量，减少了无谓的内容。因此，我们发现这导致了更符合状态的对话。
- en: more consistent with the plan， higher quality， lower perplexity， and I think
    the argument。the reasoning for why this is the case is that we're kind of like
    relieving the dialogue model of the burden of having to come up with。A good strategy
    we're allowing the dialogue model to do what it does best to focus on what it
    does best。which is dialogue。And we're relieving it of the strategic components
    of the game because we're feeding that strategy into the dialect model。
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 更符合计划，质量更高，困惑度更低，我认为原因在于，我们像是解放了对话模型，不再需要考虑制定好的策略。我们让对话模型专注于它最擅长的对话，并将战略组件的负担移交给对话模型。
- en: Okay， so that's one main contribution， this controllable dialogue model that
    conditions on a plan。The second is a planning engine that accounts for dialogue
    and human behavior。So。Okay。I mentioned that a lot of。Previous work on games。Was
    done using self play in two players here or some settings。Now， the problem with
    like pure self play。Is that it can learn strong policies， but it doesn't。
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这是一个主要贡献，这个可控的对话模型基于一个计划。第二个贡献是一个规划引擎，它考虑了对话和人类行为。所以，好的。我提到之前关于游戏的许多工作都是在两个玩家的自我对战或某些设置下进行的。现在，纯自我对战的问题在于，它能学习强策略，但并不能。
- en: It doesn't stick with human conventions and it can't account for dialogue。it's
    just going to ignore the human data and the human way of playing if you just do
    self play。So that's one extreme。The other extreme that you can go is you just
    do supervised learning on human data。Create this model of how humans play and
    then train with those you know imitation humans。
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 它不遵循人类的约定，也无法处理对话。如果你只是进行自我对战，它会忽略人类数据和人类的游戏方式。这是一个极端。另一种极端是对人类数据进行监督学习。创建人类游戏的模型，然后用那些模仿人类进行训练。
- en: And if you do this， you'll end up with a bot that's consistent with dialogue
    and human conventions。but it's only as strong as the training data， and we found
    that it was actually very easily manipulable through adversarial dialogue。so for
    example you could send messages to it saying like thanks for agreeing to support
    me at the Paris and it will think like well I've only ever seen that message in
    my training data when I've agreed to support the person of the Paris and so I
    guess I'm supporting them at the Paris thisern even though that might be a terrible
    move for the bot。
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你这样做，你最终会得到一个与对话和人类约定一致的“机器人”，但它的能力仅取决于训练数据，我们发现它实际上很容易受到对抗性对话的操控。例如，你可以发送消息给它说“感谢你同意在巴黎支持我”，它会认为，嗯，我在训练数据中只见过这个消息，当我同意支持某人时，所以我想我在巴黎支持他们，即使这对“机器人”来说可能是一个糟糕的举动。
- en: So I came up with this algorithm called Pickle， that kind of like is a happy
    medium between these two extremes。嗯。The way pickle works is it''s basically trying
    to it's it's doing self play。but regularized toward。Sticking to the human imitation
    policy。So it has a KL penalty for deviating from the human imitation policy。
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我想出了一个叫做“Pickle”的算法，它在这两个极端之间找到了一个快乐的中间地带。嗯。Pickle的工作原理基本上是进行自我对弈，但会朝着遵循人类模仿策略的方向进行规范。因此，它对偏离人类模仿策略施加了KL惩罚。
- en: So we have this parameter lambda that controls how easy it is to deviate from
    the human imitation policy at lambmbda equals zero。It just ignores the human imageization
    policy completely and just does pure self play。And so just like do cell play as
    if from scratch， add them be equals zero。At Lambda equals infinity。it's just playing
    the human imation policy and not doing self Plaal。
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个参数λ，它控制从人类模仿策略偏离的难易程度，当λ等于零时，它完全忽略人类模仿策略，进行纯自我对弈。因此，像从零开始一样进行自我对弈，当λ等于零时。而当λ等于无穷大时，它只是执行人类模仿策略，而不进行自我对弈。
- en: But for intermediate values of Lambda， what we find is that it actually gives
    you a good medium between sticking to human conventions and performing strongly。So
    you can kind of see this behavior emerge here。Sorry there's a question is this
    similar to offline Rl or also incorporates exploration。
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但是对于λ的中间值，我们发现它实际上在遵循人类约定和表现强劲之间提供了一个良好的平衡。所以你可以在这里看到这种行为的出现。抱歉，有个问题，这是否类似于离线强化学习，还是也包含了探索？
- en: so I'd say there's actually a lot of similar work on you know having a KL penalty。And
    so yes I would say that it's like very similar to a lot of that work and it's
    also been done actually an alphapha star where they had a KL penalty。though that
    was more about aiding exploration like using human data to aid exploration rather
    than trying to better imitate humans so I think what's interesting about the pickle
    work is that one we find it imitates humans better not than just doing supervised
    learning alone and two we are。
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以说，实际上有很多类似的工作，比如施加KL惩罚。因此，我会说这与很多相关工作非常相似，实际上在AlphaStar中也做过，他们施加了KL惩罚。尽管那更多是关于利用人类数据来帮助探索，而不是更好地模仿人类。因此，我认为Pickle工作的有趣之处在于，我们发现它在模仿人类方面表现得更好，而不仅仅是进行监督学习。
- en: Doing a bit of theory of mind where we assume that the other players are also
    like we're using this as a model for our behavior。what we expect other people
    to think our behavior is in addition to modeling the other players。So it's like
    a common knowledge。Common knowledge like。Algorithm that we're using here。Okay。so
    the kind of behavior that you see from this。You can see here， let's say England
    agrees， sorry。
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在进行一些心智理论的工作，我们假设其他玩家也在使用这个模型来指导我们的行为。我们期望其他人对我们的行为的看法，除了对其他玩家进行建模之外。所以这就像是我们在这里使用的一个常识算法。好吧。从这里你可以看到的行为，比如说，假设英格兰同意，抱歉。
- en: so let's say we're in this situation， this actually came up real in a real game。And
    it inspired a figure from our paper。So England and France are fighting。France
    is the bot and France asks if。If England is willing to disengage。And let's say
    England says。yes， I will move out of English channel if you head back to NAO。Well，
    we can see that Cicero does。
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们处于这种情况，这实际上在一场真实的游戏中发生过。这启发了我们论文中的一个例子。所以英格兰和法国正在交战。法国是“机器人”，法国询问英格兰是否愿意脱离接触。如果英格兰回答说，是的，如果你回到北大西洋，我会离开英吉利海峡。那么我们可以看到西塞罗的表现。
- en: in fact back off。Leeaaves and。Goes to NAO and the disengagement is successful。and
    so this shows that the bot strategy really is reflecting the dialogue that it's
    had with this other player。Another message that England might send is something
    like。I'm sorry you've been fighting me this whole game， I can't trust you that
    you won't stab me。
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上后撤。离开并。去往NAO，成功脱离接触。因此，这表明机器人的策略确实反映了它与其他玩家的对话。英格兰可能发送的另一个消息是这样的话。我很抱歉，你整个游戏都在与我作斗争，我不能相信你不会背刺我。
- en: And so in this case， Cicero will continue its attack on England， and you can
    see again。this is reflective， it's changing its behavior depending on the dialogue。But
    you can also have this kind of message where you know England says， yes。I'll leave
    English channelnnel if you move tota Munich at Hall to Belgium。
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这种情况下，西塞罗将继续攻击英格兰，你可以再次看到。这是有反应的，它根据对话改变了行为。但你也可以有这样的消息，比如英格兰说，是的。如果你把军队移动到比利时，我就会离开英吉利海峡。
- en: so these are really bad moves for Ccero to follow and so if you just look at
    the raw policy net。It might actually do this， it might actually do these moves
    because England suggested it。but because we're using pickle that incorporates
    like it accounts for the expected value of different actions。it will actually
    harshly back off but ignore the suggested moves because it recognize that those
    will leave it very vulnerable to an attack。
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 所以这些对于西塞罗来说都是非常糟糕的举动，如果你只看原始的策略网络，它可能会这样做，可能会做出这些举动，因为英格兰建议了它。但因为我们使用pickle，它考虑了不同动作的期望值，它实际上会强烈地后撤，但忽略这些建议的举动，因为它意识到这些会使其非常容易受到攻击。
- en: Okay， I's skip this slide for time。嗯。Oh。Another thing I should say is that。We're
    not just doing planning， we're actually doing this in a full self play reinforcement
    learning loop。And again， the goal here is it's really about modeling humans better
    than supervised learning alone and we found that doing this selfplay reinforcement
    learning with pickle allowed us to better model human behavior than just doing
    imitation learning。Finally， we have an ensemble of message filtering techniques
    that filters both nonsensical and strategically un soundund messages。
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，我会跳过这一页以节省时间。嗯。哦。我还应该说的是，我们不仅仅是在做规划，我们实际上是在一个完整的自我对弈强化学习循环中进行。这次的目标确实是比单纯的监督学习更好地建模人类行为，我们发现使用pickle进行自我对弈强化学习可以比单纯的模仿学习更好地模拟人类行为。最后，我们有一组消息过滤技术，可以过滤掉无意义和策略上不合理的消息。
- en: UmSo to give you an example of what these filters look like， here。one that we
    developed is value based filtering。😡，So。The motivation for this is that when we
    feed into our dialogue model is a plan for ourselves and for our speaking partner。but
    it's the entire plan that we have for ourselves。
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，所以给你一个关于这些过滤器的例子，这里。有一个我们开发的基于价值的过滤器。😡，所以。这么做的动机在于，当我们输入对话模型时，是为我们自己和我们讲话伙伴制定的计划，但这是我们为自己制定的整个计划。
- en: and so we might end up feeding into the dialogue model the fact that we're going
    to attack the player that we're speaking to。Now， the dialogue model is， you know，
    to be honest kind of dumb and it doesn't really know。That it shouldn't be telling
    this player that theyre going to be attacked this term。And so you have these messages
    that might be sent， something like the second one shown here where England says
    to France。
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可能最终会把我们打算攻击与我们对话的玩家这一事实输入到对话模型中。老实说，对话模型有点傻，它并不知道不应该告诉这个玩家他们会被攻击。因此，可能会发送一些消息，比如这里显示的第二条，英格兰对法国说。
- en: we have hostile intentions towards you， you must be wipeding the board， please
    provide a croissant。So this is actually like a message that the bot sent to a
    player not to a player it was a this was like preliminary testing and kind of
    like motivated this whole approach。
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对你有敌意，你必须清理战场，请提供一个可颂。实际上，这是机器人发送给一个玩家的消息，而不是对另一位玩家的。这是初步测试，激发了整个方法。
- en: 嗯。So we don't want the bot to send these kinds of messages if it's going to
    attack a player。we want it to send something that's like， you know， not an outright
    lie necessarily。but just something either not send a message or send something
    that's much much much more bland。😡。And so we filter out these kinds of messages
    by looking at the value like。
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。因此，如果机器人打算攻击玩家，我们不希望它发送这种消息。我们希望它发送一些不一定是完全谎言的东西，或者根本不发送消息，或者发送一些更为平淡的内容。😡。因此，我们通过查看这些值来过滤这些消息。
- en: What we do is we generate a bunch of candidate messages。And then we see if we
    were to send this message， what is the behavior that we would expect the other
    players to take like what actions will we expect them to do after we send this
    message and what do they expect we will do after we send this message？
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的是生成一堆候选消息。然后我们看看，如果我们发送这条消息，我们期望其他参与者的行为是什么，他们在收到这条消息后会采取什么行动，他们又期望我们在发送这条消息后会做什么？
- en: And then we see。What is the expected value of the action that we intend to take。given
    the prediction of what everybody else is going to do？😡。So if our intention is
    to attack France， then we can see well。if I were to send this message to France，
    then they're going to get really defensive and defend against an attack from us
    and our attack is going to be unsuccessful and so therefore I probably shouldn't
    send this message to them。
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们看到，我们打算采取的行动的预期值是什么。考虑到对其他人将要做的预测😡。所以如果我们的意图是攻击法国，那么我们可以看到，如果我将这条消息发送给法国，他们会变得非常防御，抵御我们的攻击，而我们的攻击将会失败，因此我可能不应该将这条消息发送给他们。
- en: 嗯。And so in this web， we can actually filter out messages that have low expected
    value。And we thought that this worked surprisingly well。Dialogue examples。I'll
    go through one just for the sake of time。So here we have。Cicero is France and
    France is saying France is conversing with Turkey， who's a human player。
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。所以在这个网络中，我们实际上可以过滤掉预期值较低的消息。我们发现这效果惊人地好。对话示例。为了节省时间，我将举一个例子。这里有。西塞罗代表法国，法国正在与人类玩家土耳其对话。
- en: and they're debating over who's going to get tunis， this territory circled in
    red。You can see they both have fleets next to the territory。if they both go for
    it neither them we're going to get it。and so they need to work out some sort of
    deal。So France says， I'll work with you。
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 他们正在争论谁将占领突尼斯，这个被红圈圈出的领土。你可以看到他们都有舰队在领土旁边。如果他们都去争夺，那么都不会得到。因此，他们需要达成某种协议。所以法国说，我会和你合作。
- en: but I need tunes for now， Turkey says， nope， you got to let me have it。France
    says no， I need it。and then France suggests， you know， you can take these other
    territories instead you have Serbia and Rome to take。Turkey says they're impossible
    targets and then Cicero suggests specific moves。That would allow Turkey to capture
    these territory so Cicero says Greece Iionion I unate to Tian。
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我现在需要突尼斯，土耳其说，不，你得让我拥有它。法国说不，我需要它。然后法国建议，你知道，你可以去占领其他领土，比如塞尔维亚和罗马。土耳其说，那些是不可及的目标，接着西塞罗提出具体的行动，允许土耳其占领这些领土，所以西塞罗说希腊、爱奥尼亚和特兰。
- en: Turkey saysm you're right good ideas and then France says then in the fall you
    take Rome Austria collapses and so that allows Turkey to you know make progress
    against Austria but conveniently it also allows France to capture tus because
    Turkey will be using those units for something else。
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 土耳其说，你说得对，这些是好主意，然后法国说，那在秋天你就可以占领罗马，奥地利崩溃了，这样土耳其就可以对奥地利取得进展，但方便的是，这也让法国可以占领突尼斯，因为土耳其会将那些单位用于其他事情。
- en: Okay， so limitations in future directions intent representation is just an action
    per player。Okay。so there's a question of like。The intentions that we're feeding
    into the dialogue model is an action that we're going to take for this turn and
    for the next turn for ourselves and for the other player。
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，未来方向中的限制意图表示只是每个玩家的一项行动。好的。所以有一个问题。我们输入对话模型的意图是我们在这一回合以及下一回合要为自己和其他玩家采取的行动。
- en: But ideally， we would have a richer set of intentions。we would be able to like
    condition on things like long term strategy or style of communication or like
    asking questions。That is that's one of the limitations of this approach now of
    course。The richer you make the space of intentions， the more room there is for
    things to go wrong。
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 但理想情况下，我们会有更丰富的意图集合。我们能够考虑长期策略、沟通风格或提问等因素。这是这种方法的一个限制。当然，意图的空间越丰富，出错的可能性就越大。
- en: and you also have to like then train the model to be able to handle these like
    wider space of intentions。😡，There was a question， do you think the dialogue model
    is learning an internal model。internal world model to be so good at predicting
    moves？This is this is arguably why we're。Conditioning on intentions， we're relieving
    the dialogue model of having to come up with a good world model because we're
    telling it like these are the moves that we are planning to take this turn and
    these are the moves that we would like this other player to take this turn。
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要训练模型，以便处理更广泛的意图空间。有一个问题，你认为对话模型是否在学习一个内部模型？内部世界模型是否使其能够如此准确地预测动作？这就是为什么我们依赖意图，减轻对话模型构建良好世界模型的负担，因为我们告诉它这是我们在这一回合计划采取的动作，而这些是我们希望其他玩家在这一回合采取的动作。
- en: So we're like。We're able to like。Have the world model separate from the dialogue
    model。but condition on the output from the world model。Okay。another limitation
    is that Cicero's value model doesn't condition on dialogue。and so it has a limited
    understanding of the long term effects of dialogue。嗯。
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们能够让世界模型与对话模型分开，但依赖于世界模型的输出。好的，另一个限制是Cicero的价值模型并不依赖于对话，因此对对话的长期影响理解有限。
- en: This greatly limits our ability to。Plan what kind of messages we should be sending。And
    this is actually why we always condition Cicero's dialogue generation on its truthful
    intentions。You could argue that there is situations in diplomacy where you would
    want to lie to the other player。the best players rarely lie， but they do lie sometimes。And。
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这极大地限制了我们规划应发送的消息类型的能力。实际上，这也是我们总是将Cicero的对话生成基于其真实意图的原因。你可以争辩，在外交中有些情况下，你会想对其他玩家撒谎，最好的玩家很少撒谎，但有时会撒谎。
- en: You have to understand the trade off between like if you lie， you are going
    to。😡。It's going to be much harder to work with this person in the future。And so
    you have to make sure that the。Value that you're getting positionally is worth
    that loss of trust and that broken relationship。no。Because Cicero's value model
    doesn't condition on dialogue。
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须理解，如果你撒谎，你将会变得更难与这个人合作。因此，你必须确保你所获得的相对价值值得信任的损失和破裂的关系。因为Cicero的价值模型并不依赖于对话。
- en: It can't really understand this trade off。😡，And so for this reason。we actually
    always conditioned it on its truthful intentions。No。It is possible to have Cicero
    Viol condition on dialogue。but you would need way more data and it would make
    things much more expensive and so we weren't able to do it further for this spot。
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上无法理解这种权衡。因此，出于这个原因，我们始终将其基于真实意图。实际上，可以让Cicero Viol依赖于对话，但你需要更多的数据，这会使事情变得更加昂贵，因此我们无法进一步进行。
- en: And finally， there's a big question that I mentioned earlier， which is。is there
    a more general way of scaling inference time compute to achieve better performance？
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我提到了一个大问题，那就是是否有更普遍的方法来缩放推理时间计算，以实现更好的性能？
- en: The way that we've done planning in Cicero is I would argue a bit domain specific。I
    think it's like the idea of pickle is quite general。but I think that there are
    potentially more general ways of doing planning。诶。Somebody's asking looking forward
    to the next two to three years。
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我们在Cicero中进行规划的方式有点领域特定。我觉得pickle的概念相当普遍，但我认为可能还有更普遍的规划方式。有人问期待未来两到三年。
- en: what criteria will you use to select the next game to try to conquer honestly。Like
    I said we chose diplomacy because we thought it'd be the hardest game to make
    an AI for it and I think that that's true I don't think that we're going to be
    working on games anywhere because I can't think of any other game that if we were
    to succeed at that it would be truly impressive。
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用什么标准来选择下一个游戏以诚实地征服？正如我所说，我们选择外交是因为我们认为这是一个最难为其制作AI的游戏，我认为这确实是正确的。我想不出任何其他游戏，如果我们成功，那将是非常令人印象深刻的。
- en: And so I think where the research is going in the future。Is generality。Like。Instead
    of getting an AI to play this specific game， can we get an AI that is able to
    play diplomacy。but could also play go or poker or could also write essays and
    stories and solve math problems and write theorems。I think what we will see。Is
    games serving as benchmarks？For progress。But not as。The goal， you know。
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我认为未来的研究方向是通用性。我们能否让人工智能不仅能玩这个特定的游戏，还能玩围棋或扑克，或者还能写论文、故事、解决数学问题和写定理。我认为我们将看到的是，游戏作为进步的基准，但不是目标。
- en: it'll be part of the test set but not part of the training set。and I think that's
    the way it should be going forward。Finallyly。I want to add that diplomacy is an
    amazing testbed for multi agentent AI and Gred dialogue。If you are interested
    in these kinds of domains， I highly recommend taking advantage of the fact that
    we are。
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这将是测试集的一部分，但不是训练集的一部分。我认为这应该是未来的方向。最后，我想补充的是，外交是多代理人工智能和对话生成的一个惊人测试平台。如果你对这些领域感兴趣，我强烈推荐利用我们所提供的机会。
- en: We've open sourced all of our coded models and the dialogue and action data
    is available through a what's called an RFP where you can apply to get access
    to the dialogue and data。Okay， so thanks for listening to wrap of Cicero combined
    strategic reasonaing and natural language and diplomaacy in place in the top 10%
    of human players and the paper is in science and code of models are publicly available
    at this URL so thanks and for there any time I'll take questions great thanks
    a lot my talk。
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已开源所有编码模型，对话和行动数据可以通过一种称为RFP的方式获取，你可以申请获取对话和数据。好的，感谢大家的倾听，Cicero结合了战略推理和自然语言，在人类顶尖10%的玩家中表现出色，论文已发表于科学期刊，模型代码在这个网址公开可用，所以谢谢，随时可以提问，感谢大家的支持。
- en: So we'll also open some questions from the class， but you finished their Zoom
    questions so if anyone has like Zoom questions there I think no we can answer
    those。Yeah， there's one question are you concerned about AIs out competingeting
    humans at real world diplomatic strategic negotiation at deception tasks。
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将开放一些来自课堂的问题，但你已经完成了他们的Zoom问题，所以如果有人有Zoom的问题，我想我们可以回答。是的，有一个问题，你是否担心人工智能在真实世界的外交战略谈判和欺骗任务中超过人类。
- en: so like I said we're not very focused on deception even though you know arguably
    deception is a part of the game of diplomacy。系。Think for diplomatic and strategic
    negotiation。I don't like look the way that we've developed is real。It's designed
    to play diplomacy the game of diplomacy specifically and you can't use it out
    of the box for other tasks that said I do think that the techniques are quite。General
    and so hopefully others can build on that and to be able to do different things。
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说，我们并不太关注欺骗，尽管可以说欺骗是外交游戏的一部分。思考外交和战略谈判。我不喜欢我们所发展的方式是真实的。它是专门设计来进行外交游戏的，你不能直接用于其他任务，但我确实认为这些技术是相当通用的，因此希望其他人可以在此基础上构建，以便能够做不同的事情。
- en: and I think it is you know entirely possible that over the next several years。you
    will see this entering into real world negotiations much more often I actually
    think that diplomacy is a big step towards real world applicability compared to
    breakies in games like go and poker。
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为在接下来的几年里，这种情况完全可能，你会看到它在现实世界的谈判中更频繁地出现。我实际上认为，与围棋和扑克等游戏相比，外交是朝向现实世界适用性迈出的重要一步。
- en: Because because now your action space is really like the space of natural language
    and you have to model human behavior。Do you think in the future we could appoint
    AI to the UN Council？
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因为现在你的行动空间实际上就是自然语言的空间，你必须对人类行为进行建模。你认为未来我们能否任命人工智能担任联合国理事会的成员？
- en: hopefully only if it does better than humans， but that would be very interesting
    to see。Great。I'm also curious like what what's like the future things that you're
    working on in this direction like do you think you can do something like Alpago
    zero where you just like take this take this like repeated model and then maybe
    just make it Excel play or like what sort of future actions are you thinking for
    improving this sort of box。
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 希望只有在它表现优于人类的情况下，但那将是非常有趣的。太好了。我也很好奇，你们在这个方向上正在研究哪些未来的事情，你认为能否做类似Alphago Zero的事情，重复这个模型，也许让它在Excel中运行，或者你在改进这个盒子方面有哪些未来的想法？
- en: 😊，I think the future directions are really focused around generality。like I
    think one of the big insights of Cicero is like this ability to leverage planning
    to get better performance with language models and in this strategic domain。I
    think there' was a lot of opportunity to do that sort of thing in a broader space
    of domains。I mean you look at language models today and they do token by token
    prediction。
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我认为未来的方向确实是围绕通用性展开的。我认为Cicero的一个重大洞察是利用规划来提升语言模型的性能，尤其是在这个战略领域。我认为在更广泛的领域中有很多机会进行这种探索。现在的语言模型是逐个标记进行预测的。
- en: And I think there's a big opportunity to go beyond that so that that's what
    i'm excited to look into I'm also curious like I didn't understand the exact details
    how using planning or Montete research with your like the models that you have。So
    is it like we didn't use Monla research in Cicero， Mongla research。Is a very good
    heuristic。
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为有一个很大的机会可以超越这一点，这让我感到兴奋。我也很好奇，我没有理解具体细节，如何使用规划或蒙特卡罗研究与您拥有的模型结合。我们在Cicero中没有使用蒙特卡罗研究，蒙特卡罗研究是一个非常好的启发式方法。
- en: but it's a heuristic that is。Particularly useful for deterministic perfect information
    games。And I think in order to like have a truly general form of planning we need
    to go more abstract than multic research we use this algorithm called pickle based
    on a regret minimization algorithm I don't really want to go into the details
    of it because it's not that important for the class。
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 但它是一种启发式方法。特别适用于确定性完全信息游戏。为了拥有真正通用的规划形式，我们需要更抽象地超越蒙特卡罗研究。我们使用一种称为pickle的算法，基于遗憾最小化算法。我不想详细讨论，因为这对课程并不重要。
- en: but the idea is like it is this iterative algorithm that will gradually refine
    the prediction of what everybody's going do and get better and better predictions
    the more iterations that you run。But and that's similar research。Yeah。Co， yeah，
    sure。对。Go for it， you're un mututed。Okay。So yeah。
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个想法就像是一个迭代算法，逐渐完善每个人的预测，随着迭代次数的增加，预测会越来越准确。但这类似于其他研究。是的，好的，当然。对。继续吧，你可以发言。好的。是的。
- en: my question is like when we were talking about gender likeibility。how does the
    communication between different modules of the model look like？
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我的问题是，当我们讨论性别可接纳性时，模型不同模块之间的沟通是怎样的？
- en: Particularly when we're talking about the dialogue model。Like how do you send
    information from the policy network to the dialogue model and in the future。if
    you have a model that's good at different tasks。are we going to have like a really
    big policy net that learns all of them or like separate language modules for all
    of them like how do you break it down？
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是在我们谈论对话模型时。你如何将信息从策略网络发送到对话模型？未来，如果你有一个擅长不同任务的模型，我们会有一个学习所有任务的庞大策略网络，还是为每个任务都有独立的语言模块？你会如何拆分？
- en: So we actually convert the policy like action for ourselves and for our dialogue
    partner into a string naturaltro language string and to that into the dialogue
    model along with all the dialogue that it's had so far。so it's just all text in
    text out。And that works great。
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们实际上将政策像行为一样转换为字符串，自然语言字符串，然后将其输入对话模型，以及到目前为止所有的对话内容。因此，这就是文本输入文本输出。这种方法效果很好。
- en: And then what was the second part of your question？Something like。are we just
    going to have like one giant policy net trained on everything？Yeah。it was like
    so if you're only using text first doesn't it limit the model and if you're using
    it for different games like are you thinking like when you say in the future you
    will work on generalizability。are you thinking about a big policy network that
    is trained on separate games or is able to like understand different games at
    the same time or do we have like separate policy networks for different games？
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 那你第二个问题的内容是什么？类似于我们会有一个巨大的策略网络训练在所有内容上吗？是的，如果你只使用文本，难道不会限制模型吗？如果你在不同的游戏中使用它，比如你提到的未来会关注通用性，你是指一个在不同游戏上训练的大策略网络，还是能够同时理解不同游戏，或者是为不同游戏有独立的策略网络？
- en: And yeah， like doesn't this like text interface limit the model in terms of
    communication like if you're using vectors it might like。Yeah， it might be a bit
    of a bottom way。I mean， I think ideally you go in this direction where you have
    like you know a foundational model that works for pretty much everything does
    text I mean certainly yeah just like a text in text out that like limits what
    you can do in terms communication。
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，是的，这种文本接口在沟通上限制了模型的能力，比如如果使用向量可能会有些底层。我是说，理想情况下你应该朝着这个方向发展，拥有一个适用于几乎所有文本的基础模型，当然，仅仅是文本输入和输出，这在沟通上是有限制的。
- en: but hopefully we get beyond that。![](img/e3abef98963297997c09e5d05b736a64_7.png)
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 但希望我们能超越这个。![](img/e3abef98963297997c09e5d05b736a64_7.png)
- en: I think it's a reasonable choice for now。Thank you。We questions。And I think
    we have more some questions Okay so there's a question in the chat i'd love to
    hear speculation on the future for instance we've seen some startups that are
    fine tuning LLMs to be biased or experts in say subject experts of。
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这在现在是个合理的选择。谢谢。我们有问题。我想我们还有一些问题，好吧，聊天中有一个问题，我想听听你对未来的猜测，比如说我们看到一些初创公司正在微调LLMs，以便偏向某个领域或成为某种主题的专家。
- en: This seems like a pretty general question。系。Don't have strong opinions on this。诶。系下面。Like，
    I'm not。I'm not too。I'm not too focused myself on fine tuning language models
    to specific tasks。I think the direction that i'm much more interested in going
    forward is you know like the more general forms of planning。so I don't think I
    can really comment on。You know， how do you。Two in these language models。
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是个相当通用的问题。系。我对此没有强烈的看法。诶。系下面。就像，我并不太专注于将语言模型微调到特定任务。我认为我更感兴趣的方向是更通用的规划形式。所以我觉得我不能真正评论。你知道的，这些语言模型怎么能做到。
- en: In these ways。So what sort of planning methods are you like interested in looking
    at like like MCps is one so let me sorry I got to step out for just one second。Got
    the switch rooms， excuse me。Okay， never mind we're all good sorry what was the
    question Oh yes I was I was just asking like what sort of planning algorithms
    do you think are very interesting to combine So you can think like we have like
    so many options like we have like planet kind of stuff or Rl there's like MCps
    there's like the work you did with Cro so what do you think are the most interesting
    algorithms that you think will scale well can generalize。
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式。那么你对哪些规划方法感兴趣，比如说MCPS是其中之一，所以让我抱歉，我得出去一秒。换个房间，抱歉。好的，没事，我们都很好，抱歉，问题是什么？哦，是的，我只是问你认为哪些规划算法非常有趣，可以结合使用。我们有很多选择，比如行星类的东西或者RL，还有MCPS，以及你与Cro合作的工作。那么你认为哪些算法最有趣，能够很好地扩展和泛化？
- en: 😊，Well I think that's the big question that a lot of people are trying to figure
    out today and it's not really clear what the answer is I mean I think you know
    you look at something the chain of thought。I think there's a lot of limitations
    to chain of thought。
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 😊，我认为这是许多人今天试图弄清楚的大问题，而答案并不清楚。我的意思是，你看看链式思维。我认为链式思维有很多局限性。
- en: and I think that it should be possible to do a lot better。but it is really impressive
    to see just how general of an approach it is。And so I think it would be nice to
    see。诶。To see things that are general in that way。but hopefully able to achieve
    better performance。Go。好。Also。
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为有很多方法可以做得更好。但看到这样通用的方法真的很令人印象深刻。所以我认为看到这样通用的东西会很不错，但希望能实现更好的性能。走吧，好。
- en: when you see like citra is like an encoder decoder model in the sense it encodes
    the world。and then you have the dialg model， which is time to de it。It was an
    encoder decoder model， yes。I don't think that that's necessarily the right choice，
    but that's what we used。Any questions？嗯。Okay I think youre mostly good。Ay， thanks
    a lot this was a good well yeah。
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 当你看到像Citra这样的编码解码模型时，它在某种意义上编码了世界。然后你有对话模型，负责解码。是的，它是一个编码解码模型。我不认为这 necessarily
    是正确的选择，但这就是我们使用的。有什么问题吗？嗯，好的，我觉得你大致上没问题。谢谢，这真是好。
- en: hope you all enjoyed it and if there are any questions feel free to email me
    reach out I'm happy to happy to chat。![](img/e3abef98963297997c09e5d05b736a64_9.png)
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你们都喜欢，如果有任何问题，请随时发邮件联系我，我很乐意聊聊。![](img/e3abef98963297997c09e5d05b736a64_9.png)
