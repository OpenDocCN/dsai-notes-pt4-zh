- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P3ï¼š3.Transformers in VisionTackling problems
    in Computer Vision - life_code - BV1X84y1Q7wV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P3ï¼š3.Transformers in VisionTackling problems
    in Computer Vision - life_code - BV1X84y1Q7wV
- en: '![](img/83f0bde19a507ae0c355af3acd90a0b3_0.png)'
  id: totrans-1
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83f0bde19a507ae0c355af3acd90a0b3_0.png)'
- en: Today I'm going to talk to you about vision transformers since this is all about
    transformersã€‚specifically their application for visual representation learningã€‚but
    before we jump into transformers I'm going to spend like 10 or 15 minutes giving
    you a lot of context on all of this and the specific thoughtss on the vision part
    of things because I think a majority of what you have seen and will see will be
    about languageã€‚
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: ä»Šå¤©æˆ‘å°†å’Œå¤§å®¶è®¨è®ºè§†è§‰å˜æ¢å™¨ï¼Œå› ä¸ºè¿™å®Œå…¨æ˜¯å…³äºå˜æ¢å™¨çš„ï¼Œç‰¹åˆ«æ˜¯å®ƒä»¬åœ¨è§†è§‰è¡¨ç¤ºå­¦ä¹ ä¸­çš„åº”ç”¨ã€‚ä½†åœ¨æˆ‘ä»¬æ·±å…¥æ¢è®¨å˜æ¢å™¨ä¹‹å‰ï¼Œæˆ‘ä¼šèŠ±å¤§çº¦10åˆ°15åˆ†é’Ÿä¸ºå¤§å®¶æä¾›å¾ˆå¤šèƒŒæ™¯ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯å…³äºè§†è§‰æ–¹é¢çš„æ€è€ƒï¼Œå› ä¸ºæˆ‘è®¤ä¸ºä½ ä»¬çœ‹åˆ°çš„å’Œå°†è¦çœ‹åˆ°çš„å¤§éƒ¨åˆ†å†…å®¹å°†æ˜¯å…³äºè¯­è¨€çš„ã€‚
- en: '![](img/83f0bde19a507ae0c355af3acd90a0b3_2.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83f0bde19a507ae0c355af3acd90a0b3_2.png)'
- en: Rightï¼Œ so let's get startedï¼Œ my goal and that of my cross collaborators is to
    find general visual representation and you're going to soon see what that meansã€‚And
    why or what can we do if we imagine we have a general visual representationã€‚the
    hope is that with this we can kickstart all kinds of tasks that require visual
    input that means most tasks that you do when you have your eyes open basicallyã€‚Because
    if you have good understanding of what you seeã€‚
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å¼€å§‹å§ï¼Œæˆ‘å’Œæˆ‘çš„è·¨é¢†åŸŸåˆä½œä¼™ä¼´çš„ç›®æ ‡æ˜¯æ‰¾åˆ°ä¸€èˆ¬è§†è§‰è¡¨ç¤ºï¼Œæ‚¨å¾ˆå¿«å°±ä¼šçœ‹åˆ°è¿™æ„å‘³ç€ä»€ä¹ˆã€‚å¦‚æœæˆ‘ä»¬æƒ³è±¡æœ‰ä¸€ä¸ªä¸€èˆ¬è§†è§‰è¡¨ç¤ºï¼Œæˆ‘ä»¬å¯ä»¥åšä»€ä¹ˆã€‚å¸Œæœ›é€šè¿‡è¿™ä¸ªæˆ‘ä»¬èƒ½å¯åŠ¨å„ç§éœ€è¦è§†è§‰è¾“å…¥çš„ä»»åŠ¡ï¼Œè¿™æ„å‘³ç€å½“ä½ çå¼€çœ¼ç›æ—¶åŸºæœ¬ä¸Šå¤§å¤šæ•°ä½ åšçš„ä»»åŠ¡ã€‚å› ä¸ºå¦‚æœä½ å¯¹æ‰€è§æœ‰å¾ˆå¥½çš„ç†è§£ã€‚
- en: then you can much quicker understand what's going on and what you should doã€‚å—¯ã€‚And
    eventually I have now a little kid since the year and so I really want that when
    he's grown up that there is like some kind of robotã€‚it doesn't need to be nice
    and pretty like in movies or just maybe an arm or whatever that my kid could teach
    or my parents who cannot program can teach to do some boring task that they really
    don't want to do and I believe one component of this is a good visual representation
    that generalizes to understanding the word visually everywhere it's not all that's
    required but it's one part and the part that I'm trying to pushã€‚
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä½ å¯ä»¥æ›´å¿«åœ°ç†è§£å‘ç”Ÿäº†ä»€ä¹ˆä»¥åŠä½ åº”è¯¥åšä»€ä¹ˆã€‚å—¯ã€‚æœ€ç»ˆæˆ‘ç°åœ¨æœ‰ä¸€ä¸ªå°å­©ï¼Œè‡ªå»å¹´ä»¥æ¥ï¼Œæˆ‘çœŸçš„å¸Œæœ›å½“ä»–é•¿å¤§æ—¶ï¼Œæœ‰æŸç§æœºå™¨äººã€‚å®ƒä¸éœ€è¦åƒç”µå½±ä¸­é‚£æ ·æ¼‚äº®ï¼Œåªéœ€è¦å¯èƒ½æ˜¯ä¸€åªæ‰‹è‡‚æˆ–å…¶ä»–çš„ï¼Œæˆ‘çš„å­©å­å¯ä»¥æ•™æˆ–è€…æˆ‘çš„çˆ¶æ¯ä¸èƒ½ç¼–ç¨‹çš„å¯ä»¥æ•™å®ƒåšä¸€äº›ä»–ä»¬çœŸçš„ä¸æƒ³åšçš„æ— èŠä»»åŠ¡ï¼Œæˆ‘ç›¸ä¿¡å…¶ä¸­ä¸€ä¸ªç»„æˆéƒ¨åˆ†æ˜¯è‰¯å¥½çš„è§†è§‰è¡¨ç¤ºï¼Œå®ƒèƒ½å¤Ÿåœ¨è§†è§‰ä¸Šæ™®éç†è§£ä¸–ç•Œï¼Œè¿™å¹¶ä¸æ˜¯æ‰€éœ€çš„å…¨éƒ¨ï¼Œä½†å®ƒæ˜¯ä¸€ä¸ªéƒ¨åˆ†ï¼Œä¹Ÿæ˜¯æˆ‘æƒ³æ¨åŠ¨çš„éƒ¨åˆ†ã€‚
- en: So this is for context and motivation on working on general visual representation
    and one good example of a general visual representation is the humansã€‚And I'm
    going to show you what I mean by thatã€‚So here is a task that I give youã€‚there
    is three classesï¼Œ class Aï¼Œ B and Cï¼Œ and I give you five images of each classï¼Œ
    okayã€‚ğŸ˜Šã€‚![](img/83f0bde19a507ae0c355af3acd90a0b3_4.png)
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯å…³äºä¸€èˆ¬è§†è§‰è¡¨ç¤ºçš„èƒŒæ™¯å’ŒåŠ¨æœºï¼Œä¸€ä¸ªå¥½çš„ä¾‹å­å°±æ˜¯äººç±»ã€‚æˆ‘å°†å‘ä½ ä»¬å±•ç¤ºæˆ‘æ‰€è¯´çš„æ„æ€ã€‚è¿™é‡Œæ˜¯æˆ‘ç»™ä½ ä»¬çš„ä¸€ä¸ªä»»åŠ¡ã€‚æœ‰ä¸‰ç±»ï¼Œç±»Aã€ç±»Bå’Œç±»Cï¼Œæˆ‘ç»™ä½ ä»¬æ¯ç±»äº”å¼ å›¾ç‰‡ï¼Œå¥½çš„ã€‚ğŸ˜Šã€‚![](img/83f0bde19a507ae0c355af3acd90a0b3_4.png)
- en: And here I give you a new imageã€‚And I'm sure that by now you all know which
    class it isã€‚I'm not gonna ask because I don't actually see you if I was in the
    room I would do the raised handsã€‚but I'm sure you know it's class A nowï¼Œ okay
    this is fine we have seen millions of flowers in our lives hopefully but there
    is other kinds of pictures like this satellite images that you don't see much
    in your life some people may have never seen it some sometimes like when you fly
    or maybe on TV or in the Internet or so but it's red or rareã€‚
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæˆ‘ç»™ä½ ä»¬ä¸€å¼ æ–°å›¾ã€‚æˆ‘ç›¸ä¿¡åˆ°ç°åœ¨ä½ ä»¬éƒ½çŸ¥é“è¿™æ˜¯ä»€ä¹ˆç±»ã€‚æˆ‘ä¸ä¼šé—®ï¼Œå› ä¸ºæˆ‘å®é™…ä¸Šçœ‹ä¸åˆ°ä½ ä»¬ï¼Œå¦‚æœæˆ‘åœ¨æˆ¿é—´é‡Œæˆ‘ä¼šè®©å¤§å®¶ä¸¾æ‰‹ï¼Œä½†æˆ‘ç›¸ä¿¡ä½ ä»¬ç°åœ¨çŸ¥é“è¿™æ˜¯ç±»Aï¼Œå¥½çš„ï¼Œè¿™æ²¡é—®é¢˜ï¼Œæˆ‘ä»¬åœ¨ç”Ÿæ´»ä¸­è§è¿‡æ•°ç™¾ä¸‡ç§èŠ±ï¼Œä½†è¿˜æœ‰å…¶ä»–ç±»å‹çš„å›¾ç‰‡ï¼Œæ¯”å¦‚ä½ ä»¬åœ¨ç”Ÿæ´»ä¸­ä¸å¸¸è§çš„å«æ˜Ÿå›¾åƒï¼Œæœ‰äº›äººå¯èƒ½ä»æœªè§è¿‡ï¼Œå¶å°”åœ¨é£è¡Œæ—¶æˆ–è®¸åœ¨ç”µè§†ä¸Šæˆ–äº’è”ç½‘ä¸Šè§è¿‡ï¼Œä½†è¿™å¾ˆå°‘è§ã€‚
- en: but still same storyï¼Œ three classes Class A BC five images of each and I show
    you a new imageã€‚this might be a little bit less trivial than the flowerã€‚but I
    think I've spent enough time talking that by now most of you should know that
    this is class B shows this is basketball court rightï¼Ÿ
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä¾ç„¶æ˜¯åŒæ ·çš„æ•…äº‹ï¼Œä¸‰ç±»ï¼šç±»Aã€Bã€Cï¼Œæ¯ç±»äº”å¼ å›¾ç‰‡ï¼Œæˆ‘ç»™ä½ ä»¬ä¸€å¼ æ–°å›¾ç‰‡ã€‚è¿™å¯èƒ½æ¯”èŠ±ç¨å¾®ä¸é‚£ä¹ˆç®€å•ï¼Œä½†æˆ‘è§‰å¾—æˆ‘å·²ç»èŠ±äº†è¶³å¤Ÿçš„æ—¶é—´æ¥è®²ï¼Œæ‰€ä»¥ç°åœ¨ä½ ä»¬å¤§å¤šæ•°äººåº”è¯¥çŸ¥é“è¿™æ˜¯ç±»Bï¼Œè¿™é‡Œæ˜¯ç¯®çƒåœºï¼Œå¯¹å§ï¼Ÿ
- en: å—¯ã€‚Alright now even more abstractï¼Œ you don't see this in real life all rightã€‚but
    still I give you images of class A and B just two to make it a bit easier here
    because you need to use your brain a little bit more and I show you this in your
    image and now I should do a little bit of smart talk to let you think and like
    you see that there is like s boxes and whatnot and by now I hope that most of
    you know that this is class A why because there is three objects in class A and
    class B is always what is it five objects no matter what they are what they look
    likeã€‚
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚å¥½äº†ï¼Œç°åœ¨æ›´æŠ½è±¡äº†ï¼Œä½ åœ¨ç°å®ç”Ÿæ´»ä¸­çœ‹ä¸åˆ°è¿™äº›ï¼Œå¯¹å§ã€‚ä½†æˆ‘ä»ç„¶ç»™ä½ å±•ç¤ºAç±»å’ŒBç±»çš„å›¾åƒï¼Œåªå±•ç¤ºä¸¤ä¸ªï¼Œä½¿å…¶æ›´å®¹æ˜“ä¸€ç‚¹ï¼Œå› ä¸ºä½ éœ€è¦åœ¨è¿™é‡Œå¤šåŠ¨è„‘ç­‹ã€‚æˆ‘ç°åœ¨å±•ç¤ºç»™ä½ è¿™ä¸ªå›¾åƒï¼Œæˆ‘åº”è¯¥è¿›è¡Œä¸€äº›èªæ˜çš„å¯¹è¯ï¼Œè®©ä½ æ€è€ƒã€‚ä½ ä¼šçœ‹åˆ°æœ‰ä¸€äº›æ¡†ç­‰ç­‰ã€‚æˆ‘å¸Œæœ›ç°åœ¨å¤§å¤šæ•°äººéƒ½çŸ¥é“è¿™æ˜¯Aç±»ï¼Œå› ä¸ºAç±»æœ‰ä¸‰ä¸ªå¯¹è±¡ï¼Œè€ŒBç±»å§‹ç»ˆæ˜¯äº”ä¸ªå¯¹è±¡ï¼Œæ— è®ºå®ƒä»¬æ˜¯ä»€ä¹ˆï¼Œé•¿ä»€ä¹ˆæ ·ã€‚
- en: Okayï¼Œ I think by now you more or less understand what I mean when I mean a good
    visual representationã€‚general visual representation rightï¼Œ some some I don't know
    how to call it in your brain in your eyes such that you can quickly see something
    new and understand what's going on with just a few examplesã€‚
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½å§ï¼Œæˆ‘æƒ³åˆ°ç°åœ¨ä½ æˆ–å¤šæˆ–å°‘æ˜ç™½æˆ‘æ‰€è¯´çš„è‰¯å¥½çš„è§†è§‰è¡¨ç¤ºæ˜¯ä»€ä¹ˆæ„æ€ã€‚é€šç”¨çš„è§†è§‰è¡¨ç¤ºï¼Œå¯¹ï¼Œä½ çš„è„‘æµ·å’Œçœ¼ç›ä¸­æœ‰æŸç§ä¸œè¥¿ï¼Œä½¿ä½ èƒ½å¤Ÿè¿…é€Ÿçœ‹åˆ°æ–°äº‹ç‰©ï¼Œå¹¶é€šè¿‡å°‘é‡ç¤ºä¾‹ç†è§£æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ã€‚
- en: '![](img/83f0bde19a507ae0c355af3acd90a0b3_6.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/83f0bde19a507ae0c355af3acd90a0b3_6.png)'
- en: And then generalize from thatã€‚Rightï¼Œ and that's the goalï¼Œ then the next stepï¼Œ
    if we have the goalã€‚how do we measure progress towards itï¼ŸAnd this is a paper
    we did a few years ago with my collaboratorsã€‚which we call the visualual task
    adaptation benchmarkã€‚it's kind of formalization of the little game that we just
    playedã€‚
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä»ä¸­è¿›è¡Œæ³›åŒ–ã€‚å¯¹ï¼Œè¿™å°±æ˜¯ç›®æ ‡ã€‚æ¥ä¸‹æ¥çš„æ­¥éª¤ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ç›®æ ‡ï¼Œå¦‚ä½•è¡¡é‡æœç€ç›®æ ‡çš„è¿›å±•ï¼Ÿè¿™æ˜¯æˆ‘ä»¬å‡ å¹´å‰ä¸æˆ‘çš„åˆä½œè€…å®Œæˆçš„ä¸€ç¯‡è®ºæ–‡ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºè§†è§‰ä»»åŠ¡é€‚åº”åŸºå‡†ã€‚è¿™æ˜¯ä¸€ç§å¯¹æˆ‘ä»¬åˆšåˆšè¿›è¡Œçš„å°æ¸¸æˆçš„å½¢å¼åŒ–ã€‚
- en: So it's a benchmark and there is some component that you or anybody who participated
    in benchmark doesã€‚which is creating a model with some dataï¼Œ we don't really care
    what data what model and how whatnotã€‚Just your home with the motherã€‚Then we come
    with this landscape of all possible visual tasks that kind of makes senseã€‚ğŸ˜Šï¼ŒWwhich
    is a vague statement and we sample some tasks from that and this is kind of the
    task that you have just seen they were actually taken out of this task adaptation
    benchmark and we have for a first step made 19 such tasks where we try to cover
    broad types of visual tasks not just classes of natural images like these dogs
    and cats things but also of very specialized images like satellite image also
    nonclass task that involve counting like the one I showed you before right but
    that can be expressed in this simple classification API but that logically require
    some more thinkingã€‚
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªåŸºå‡†ï¼Œå‚ä¸åŸºå‡†çš„ä½ æˆ–ä»»ä½•äººéƒ½éœ€è¦åšä¸€äº›äº‹æƒ…ï¼Œé‚£å°±æ˜¯ç”¨ä¸€äº›æ•°æ®åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼Œæˆ‘ä»¬å¹¶ä¸å¤ªå…³å¿ƒæ˜¯ä»€ä¹ˆæ•°æ®ã€ä»€ä¹ˆæ¨¡å‹å’Œå…¶ä»–ç»†èŠ‚ã€‚åªè¦åœ¨å®¶é‡Œä¸æ¯äº²åœ¨ä¸€èµ·ã€‚ç„¶åæˆ‘ä»¬ä¼šæœ‰ä¸€ä¸ªæ‰€æœ‰å¯èƒ½çš„è§†è§‰ä»»åŠ¡çš„å…¨æ™¯ï¼Œè¿™äº›ä»»åŠ¡åœ¨é€»è¾‘ä¸Šæ˜¯æœ‰æ„ä¹‰çš„ã€‚ğŸ˜Šï¼Œè¿™è™½ç„¶æ˜¯ä¸ªæ¨¡ç³Šçš„è¯´æ³•ï¼Œæˆ‘ä»¬ä»ä¸­æŠ½å–äº†ä¸€äº›ä»»åŠ¡ï¼Œè¿™å°±æ˜¯ä½ åˆšæ‰çœ‹åˆ°çš„é‚£äº›ä»»åŠ¡ï¼Œå®é™…ä¸Šæ˜¯ä»è¿™ä¸ªä»»åŠ¡é€‚åº”åŸºå‡†ä¸­æå–çš„ã€‚æˆ‘ä»¬åˆæ­¥åˆ¶ä½œäº†19ä¸ªè¿™æ ·çš„ä»»åŠ¡ï¼Œè¯•å›¾æ¶µç›–å„ç§ç±»å‹çš„è§†è§‰ä»»åŠ¡ï¼Œä¸ä»…ä»…æ˜¯åƒè¿™äº›ç‹—å’ŒçŒ«è¿™æ ·çš„è‡ªç„¶å›¾åƒç±»åˆ«ï¼Œè¿˜æœ‰éå¸¸ä¸“ä¸šçš„å›¾åƒï¼Œå¦‚å«æ˜Ÿå›¾åƒï¼Œä»¥åŠæ¶‰åŠè®¡æ•°çš„éåˆ†ç±»ä»»åŠ¡ï¼Œæ¯”å¦‚æˆ‘ä¹‹å‰ç»™ä½ å±•ç¤ºçš„é‚£ç§ï¼Œä½†å¯ä»¥ç”¨è¿™ä¸ªç®€å•çš„åˆ†ç±»APIè¡¨è¾¾ï¼Œè™½ç„¶é€»è¾‘ä¸Šéœ€è¦æ›´å¤šæ€è€ƒã€‚
- en: ğŸ˜Šï¼ŒSome things like distanceï¼Œ we have something with cars and with distance of
    the closest car and things like thatã€‚It should cover a broad range of variation
    and thenã€‚With the model that you came and to this benchmarkã€‚you can do some adaptation
    step on each of the data setsã€‚ one after anotherã€‚
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼ŒæŸäº›äº‹ç‰©å¦‚è·ç¦»ï¼Œæˆ‘ä»¬æœ‰ä¸æ±½è½¦ç›¸å…³çš„å†…å®¹ä»¥åŠæœ€æ¥è¿‘çš„æ±½è½¦çš„è·ç¦»ç­‰ã€‚è¿™åº”è¯¥æ¶µç›–å¹¿æ³›çš„å˜åŒ–èŒƒå›´ã€‚ç„¶åï¼Œå€ŸåŠ©ä½ æ‰€æå‡ºçš„æ¨¡å‹å’Œè¿™ä¸ªåŸºå‡†ï¼Œä½ å¯ä»¥å¯¹æ¯ä¸ªæ•°æ®é›†é€ä¸€è¿›è¡Œä¸€äº›é€‚åº”æ­¥éª¤ã€‚
- en: while at the same time doesn't really matterã€‚but then you should haveï¼Œ as a
    resultï¼Œ a model thatã€‚Of this data setï¼Œ which is very smart it just has seen a
    few examples for each class that then performs where thereã€‚and then we just take
    the average score across all of these tasks and this is what we call the VTAP
    task and this is how for now we judge how good of a general visual representation
    does your model and adaptation algorithm haveï¼Ÿ
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åŒæ—¶ï¼Œè¿™å®é™…ä¸Šå¹¶ä¸é‡è¦ã€‚ä½†ç»“æœåº”è¯¥æ˜¯ï¼Œä½ æ‹¥æœ‰ä¸€ä¸ªæ¨¡å‹ï¼ŒåŸºäºè¿™ä¸ªæ•°æ®é›†ï¼Œå®ƒéå¸¸èªæ˜ï¼Œå®ƒä»…ä»…è§è¿‡æ¯ä¸ªç±»åˆ«çš„å‡ ä¸ªç¤ºä¾‹ï¼Œç„¶åèƒ½å¤Ÿåœ¨è¿™äº›ä»»åŠ¡ä¸­è¡¨ç°å‡ºè‰²ã€‚ç„¶åæˆ‘ä»¬åªæ˜¯è®¡ç®—æ‰€æœ‰è¿™äº›ä»»åŠ¡çš„å¹³å‡å¾—åˆ†ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬æ‰€ç§°çš„VTAPä»»åŠ¡ï¼Œè¿™ä¹Ÿæ˜¯æˆ‘ä»¬ç›®å‰åˆ¤æ–­ä½ çš„æ¨¡å‹å’Œé€‚åº”ç®—æ³•çš„é€šç”¨è§†è§‰è¡¨ç¤ºèƒ½åŠ›çš„æ–¹æ³•ã€‚
- en: And now just for some neuralclï¼Œ this preparation we have words that we often
    use pretrainingã€‚sometimes we call it the upstream like upstream dataï¼Œ upstream
    trainingï¼Œ somethingã€‚so I may use this word interchangeably with pretraining and
    then there is the second part which we usually call transfer and then sometimes
    we say downstreamã€‚And the adaptation for our in principle is whatever you wantã€‚
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨åªæ˜¯å…³äºä¸€äº›ç¥ç»ç½‘ç»œçš„å‡†å¤‡ï¼Œæˆ‘ä»¬ç»å¸¸ä½¿ç”¨â€œé¢„è®­ç»ƒâ€è¿™ä¸ªè¯ã€‚æœ‰æ—¶æˆ‘ä»¬ç§°ä¹‹ä¸ºä¸Šæ¸¸ï¼Œæ¯”å¦‚ä¸Šæ¸¸æ•°æ®ã€ä¸Šæ¸¸è®­ç»ƒï¼Œç­‰ç­‰ã€‚æ‰€ä»¥æˆ‘å¯èƒ½ä¼šå°†è¿™ä¸ªè¯ä¸é¢„è®­ç»ƒäº’æ¢ä½¿ç”¨ï¼Œç„¶åè¿˜æœ‰ç¬¬äºŒéƒ¨åˆ†ï¼Œæˆ‘ä»¬é€šå¸¸ç§°ä¹‹ä¸ºè¿ç§»ï¼Œæœ‰æ—¶æˆ‘ä»¬ç§°ä¹‹ä¸ºä¸‹æ¸¸ã€‚å¯¹äºæˆ‘ä»¬çš„é€‚åº”æ¥è¯´ï¼ŒåŸåˆ™ä¸Šæ˜¯ä½ æƒ³è¦çš„ä»»ä½•ä¸œè¥¿ã€‚
- en: but for our work we almost always just use very simple fine tuning without any
    birdss and whistle because it it's simple and works well in general we try to
    do things as simple as possible that still work well and so sometimes I even just
    say like fine tuning when fine tuning that means moving from this pretrainning
    through the transferã€‚
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å¯¹äºæˆ‘ä»¬çš„å·¥ä½œï¼Œæˆ‘ä»¬å‡ ä¹æ€»æ˜¯ä½¿ç”¨éå¸¸ç®€å•çš„å¾®è°ƒï¼Œè€Œæ²¡æœ‰ä»»ä½•å¤æ‚çš„èŠ±å“¨ï¼Œå› ä¸ºå®ƒç®€å•ä¸”é€šå¸¸æ•ˆæœè‰¯å¥½ï¼Œæˆ‘ä»¬å°½é‡åšåˆ°å°½å¯èƒ½ç®€å•ï¼ŒåŒæ—¶ä»èƒ½è‰¯å¥½åœ°å·¥ä½œï¼Œå› æ­¤æœ‰æ—¶æˆ‘ç”šè‡³ä¼šè¯´åƒâ€œå¾®è°ƒâ€ï¼Œå½“å¾®è°ƒæ—¶ï¼Œè¿™æ„å‘³ç€ä»é¢„è®­ç»ƒè¿ç§»åˆ°è¿ç§»ã€‚
- en: ğŸ˜Šï¼ŒAll rightï¼Œ so so far for the settingï¼Œ so far so goodã€‚Good then the question
    is how do we get there and we spend a lot of time thinking about this and trying
    different things and this is also roughly the outline of all that I have available
    to talk about which doesn't mean we're going to cover everythingã€‚
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¥½çš„ï¼Œåˆ°ç›®å‰ä¸ºæ­¢çš„è®¾ç½®è¿˜ä¸é”™ã€‚é‚£ä¹ˆé—®é¢˜æ˜¯æˆ‘ä»¬å¦‚ä½•è¾¾åˆ°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬èŠ±äº†å¾ˆå¤šæ—¶é—´æ€è€ƒè¿™ä¸ªé—®é¢˜å¹¶å°è¯•ä¸åŒçš„æ–¹æ³•ï¼Œè¿™ä¹Ÿå¤§è‡´æ˜¯æˆ‘å¯ä»¥è®¨è®ºçš„å†…å®¹å¤§çº²ï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€æˆ‘ä»¬ä¼šæ¶µç›–æ‰€æœ‰å†…å®¹ã€‚
- en: ğŸ˜Šï¼ŒSo I'm not going to go like to the outline exactlyã€‚but you will see this again
    and again and as you see vision transformerã€‚your transformer only comes a little
    bit laterï¼Œ there's some stuff before thatã€‚ğŸ˜Šã€‚So this one just really quickly because
    it doesn't matter for this courseã€‚
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥æˆ‘ä¸ä¼šå®Œå…¨æŒ‰ç…§å¤§çº²æ¥è®²ã€‚ä½†ä½ ä¼šä¸€æ¬¡åˆä¸€æ¬¡åœ°çœ‹åˆ°è¿™ä¸ªï¼Œå½“ä½ çœ‹åˆ°è§†è§‰å˜æ¢å™¨æ—¶ã€‚ä½ çš„å˜æ¢å™¨ç¨åæ‰ä¼šå‡ºç°ï¼Œè¿˜æœ‰ä¸€äº›å†…å®¹åœ¨é‚£ä¹‹å‰ã€‚ğŸ˜Šã€‚æ‰€ä»¥è¿™ä¸ªåªæ˜¯éå¸¸å¿«é€Ÿåœ°æä¸€ä¸‹ï¼Œå› ä¸ºè¿™å¯¹äºæœ¬è¯¾ç¨‹æ¥è¯´å¹¶ä¸é‡è¦ã€‚
- en: is that we spend some time trying self supervised pre trainingã€‚which is very
    popular in language and envision only recently years become popularã€‚It doesn't
    work that way like you don't need to understand these barsã€‚but basically higher
    is better and here just look at the blue onesï¼Œ that's the VA scoreã€‚
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬èŠ±äº†ä¸€äº›æ—¶é—´å°è¯•è‡ªç›‘ç£é¢„è®­ç»ƒï¼Œè¿™åœ¨è¯­è¨€å¤„ç†ä¸Šéå¸¸æµè¡Œï¼Œè€Œåœ¨è§†è§‰é¢†åŸŸæœ€è¿‘å‡ å¹´ä¹Ÿå˜å¾—æµè¡Œã€‚å®ƒå¹¶ä¸æ˜¯ä»¥è¿™ç§æ–¹å¼è¿ä½œï¼Œä½ ä¸éœ€è¦ç†è§£è¿™äº›æ¡å½¢å›¾ï¼Œä½†åŸºæœ¬ä¸Šè¶Šé«˜è¶Šå¥½ï¼Œçœ‹çœ‹è“è‰²çš„éƒ¨åˆ†ï¼Œé‚£æ˜¯
    VA å¾—åˆ†ã€‚
- en: For this few short beat up and self supervised learning performs like this bar
    we tried multiple methods and multiple models and so on it was a proper good benchmarkã€‚but
    it was a couple of years agoã€‚Then we move on to semi supervised trainingsã€‚so a
    few labeled examples and the ton of unlabeled examples that's this next blue upã€‚do
    you actually see the mouse cursorï¼Œ sorryï¼ŸWe don't see the mouse curorã€‚ Maybe I
    need to do someã€‚
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™å‡ çŸ­æ—¶é—´å†…ï¼Œè‡ªç›‘ç£å­¦ä¹ çš„è¡¨ç°å°±åƒè¿™ä¸ªæ¡å½¢å›¾ï¼Œæˆ‘ä»¬å°è¯•äº†å¤šç§æ–¹æ³•å’Œå¤šç§æ¨¡å‹ç­‰ç­‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å½“å¥½çš„åŸºå‡†ï¼Œä½†è¿™æ˜¯å‡ å¹´å‰çš„äº‹æƒ…ã€‚ç„¶åæˆ‘ä»¬è½¬å‘åŠç›‘ç£è®­ç»ƒã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸€äº›å¸¦æ ‡ç­¾çš„ç¤ºä¾‹å’Œå¤§é‡çš„æœªæ ‡è®°ç¤ºä¾‹ï¼Œè¿™å°±æ˜¯ä¸‹ä¸€ä¸ªè“æ¡ã€‚ä½ èƒ½çœ‹åˆ°é¼ æ ‡å…‰æ ‡å—ï¼ŒæŠ±æ­‰ï¼Ÿæˆ‘ä»¬çœ‹ä¸åˆ°é¼ æ ‡å…‰æ ‡ã€‚ä¹Ÿè®¸æˆ‘éœ€è¦åšä¸€äº›è°ƒæ•´ã€‚
- en: canYeahã€‚Yeahï¼Œ so then semis that blue bar which is a lot higher than this other
    blue barã€‚so what this means to us is that by adding a few labeled examples we
    are able to get much better or much more general visual representationã€‚ğŸ˜Šï¼ŒThen
    I'm not going to spend more time on this and how exactly and so onã€‚but I'm going
    to move to the next one which was for us kind of a breakthrough when we figured
    out that if we just stayed up fully supervised pretrainingã€‚
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: å¯ä»¥çš„ã€‚æ˜¯çš„ï¼Œç„¶åè¿™ä¸ªåŠç›‘ç£çš„è“æ¡æ¯”å¦ä¸€ä¸ªè“æ¡é«˜å¾—å¤šã€‚è¿™å¯¹æˆ‘ä»¬æ¥è¯´æ„å‘³ç€é€šè¿‡æ·»åŠ ä¸€äº›å¸¦æ ‡ç­¾çš„ç¤ºä¾‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè·å¾—æ›´å¥½æˆ–æ›´é€šç”¨çš„è§†è§‰è¡¨ç¤ºã€‚ğŸ˜Šï¼Œç„¶åæˆ‘ä¸ä¼šå†èŠ±æ›´å¤šæ—¶é—´åœ¨è¿™ä¸ªé—®é¢˜ä¸Šï¼Œä»¥åŠå…·ä½“æ˜¯å¦‚ä½•è¿›è¡Œçš„ã€‚ä½†æˆ‘å°†è½¬å‘ä¸‹ä¸€ä¸ªï¼Œè¿™å¯¹æˆ‘ä»¬æ¥è¯´æ˜¯ä¸€ä¸ªçªç ´ï¼Œå½“æˆ‘ä»¬å‘ç°å¦‚æœæˆ‘ä»¬åªä¿æŒå®Œå…¨ç›‘ç£çš„é¢„è®­ç»ƒã€‚
- en: then we get really much better representations than everything we've seen before
    and here I want to briefly spend some time on that one because it's the precursor
    to using vision or transform us in visionã€‚ğŸ˜Šï¼ŒSo the it is simple that there are
    tons of images on the internet and that's always what you hear is motivation for
    selfupvised or unsupervised learning rightã€‚
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¾—åˆ°çš„è¡¨ç¤ºæ¯”ä¹‹å‰è§è¿‡çš„æ‰€æœ‰å†…å®¹éƒ½è¦å¥½å¾—å¤šï¼Œæˆ‘æƒ³åœ¨è¿™é‡Œç¨å¾®èŠ±ä¸€ç‚¹æ—¶é—´è®¨è®ºè¿™ä¸ªï¼Œå› ä¸ºå®ƒæ˜¯ä½¿ç”¨è§†è§‰æˆ–å°†æˆ‘ä»¬è½¬å˜ä¸ºè§†è§‰çš„å‰å¥ã€‚ğŸ˜Šï¼Œæ‰€ä»¥å¾ˆç®€å•ï¼Œç½‘ä¸Šæœ‰å¤§é‡çš„å›¾åƒï¼Œè¿™æ€»æ˜¯ä½ å¬åˆ°çš„è‡ªç›‘ç£æˆ–æ— ç›‘ç£å­¦ä¹ çš„åŠ¨æœºï¼Œå¯¹å§ã€‚
- en: but actually where these images come from there's almost always some extra information
    like surrounding the image on the web or if you collect it otherwise there's some
    extra information there that you could use as some weak source of information
    or some weak label rightï¼Ÿ
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®é™…ä¸Šè¿™äº›å›¾åƒæ¥è‡ªå“ªé‡Œï¼Œå‡ ä¹æ€»æ˜¯æœ‰ä¸€äº›é¢å¤–çš„ä¿¡æ¯ï¼Œæ¯”å¦‚åœ¨ç½‘ç»œä¸Šå›´ç»•å›¾åƒçš„ä¿¡æ¯ï¼Œæˆ–è€…å¦‚æœä½ ä»¥å…¶ä»–æ–¹å¼æ”¶é›†ï¼Œé‚£é‡Œçš„æŸäº›é¢å¤–ä¿¡æ¯å¯ä»¥ä½œä¸ºä¸€äº›å¼±ä¿¡æ¯æºæˆ–å¼±æ ‡ç­¾ï¼Œå¯¹å§ï¼Ÿ
- en: ğŸ˜Šï¼ŒThen it happens that in Google there's some team that actually does this for
    productionã€‚and they have collected already a large data set with some pipeline
    that from the surrounding signals somewhat automaticallyã€‚but very noisily annotates
    the imagesã€‚And we wanted to figure out how far can we go when we scale up preachingï¼Ÿ
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç„¶ååœ¨è°·æ­Œæœ‰ä¸€ä¸ªå›¢é˜Ÿå®é™…ä¸Šä¸ºç”Ÿäº§åšè¿™ä»¶äº‹ã€‚ä»–ä»¬å·²ç»æ”¶é›†äº†ä¸€ä¸ªå¤§å‹æ•°æ®é›†ï¼Œå¹¶ä¸”æœ‰ä¸€äº›ç®¡é“å¯ä»¥ä»å‘¨å›´ä¿¡å·ä¸­æœ‰äº›è‡ªåŠ¨ä½†éå¸¸å˜ˆæ‚åœ°æ ‡æ³¨å›¾åƒã€‚æˆ‘ä»¬æƒ³å¼„æ¸…æ¥šï¼Œå½“æˆ‘ä»¬æ‰©å±•è®²é“æ—¶å¯ä»¥èµ°å¤šè¿œï¼Ÿ
- en: Then long story shortï¼Œ you need a couple ingredientsã€‚One is patienceã€‚I really
    like this plotã€‚This is one of the curves of just pretraining on large data with
    large models Okay doesnt the details don't really matterã€‚The just is that if I
    zoom into this little boxï¼Œ I see this here and this is the metric for the training
    like the performance in upstream that I see after spending8 GPU weeks of computeã€‚What
    does GPU week mean it means8 GPUus for a week orã€‚Sorryã€‚
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åé•¿è¯çŸ­è¯´ï¼Œä½ éœ€è¦å‡ ä¸ªæˆåˆ†ã€‚ä¸€ä¸ªæ˜¯è€å¿ƒã€‚æˆ‘éå¸¸å–œæ¬¢è¿™ä¸ªå›¾ã€‚è¿™æ˜¯ä¸€ä¸ªå…³äºåœ¨å¤§æ•°æ®å’Œå¤§æ¨¡å‹ä¸Šé¢„è®­ç»ƒçš„æ›²çº¿ã€‚å¥½çš„ï¼Œç»†èŠ‚å…¶å®å¹¶ä¸é‡è¦ã€‚åªæ˜¯åœ¨æˆ‘ç¼©æ”¾åˆ°è¿™ä¸ªå°æ¡†æ—¶ï¼Œæˆ‘çœ‹åˆ°è¿™é‡Œï¼Œè¿™æ˜¯è®­ç»ƒçš„æŒ‡æ ‡ï¼Œåƒæ˜¯æˆ‘åœ¨èŠ±è´¹å…«ä¸ªGPUå‘¨çš„è®¡ç®—åçœ‹åˆ°çš„ä¸Šæ¸¸æ€§èƒ½ã€‚GPUå‘¨æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿå®ƒæ˜¯æŒ‡å…«ä¸ªGPUå·¥ä½œä¸€å‘¨ï¼Œæˆ–è€…ï¼ŒæŠ±æ­‰ã€‚
- en: one GPU for eight weeks or 8 GPUs for one week or 16 GPUs for half week and
    so on rightã€‚but this looks reasonable person would sayï¼Œ yeah there's no progress
    for a week on8 GPUs This is flat I'm gonna to stop and try something else but
    we are not reasonable So if keep're going and this is what the exact same spot
    looks like after8 GPU month of training and you can clearly see that things are
    progressing right So it may not always be obvious and you need patienceã€‚
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€å°GPUå·¥ä½œå…«å‘¨ï¼Œæˆ–è€…å…«å°GPUå·¥ä½œä¸€å‘¨ï¼Œæˆ–è€…åå…­å°GPUå·¥ä½œåŠå‘¨ç­‰ç­‰ï¼Œå¯¹å§ï¼Ÿä½†æ˜¯è¿™çœ‹èµ·æ¥åˆç†çš„äººä¼šè¯´ï¼Œå—¯ï¼Œå…«å°GPUä¸€å‘¨æ²¡æœ‰è¿›å±•ï¼Œè¿™å°±æ˜¯å¹³çš„ï¼Œæˆ‘è¦åœæ­¢å¹¶å°è¯•åˆ«çš„ï¼Œä½†æˆ‘ä»¬å¹¶ä¸åˆç†ã€‚æ‰€ä»¥å¦‚æœç»§ç»­è¿›è¡Œï¼Œè¿™æ˜¯ç»è¿‡å…«å°GPUä¸€ä¸ªæœˆè®­ç»ƒåçœ‹èµ·æ¥çš„å®Œå…¨ç›¸åŒçš„åœ°æ–¹ï¼Œä½ å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°äº‹æƒ…åœ¨è¿›å±•ã€‚å› æ­¤ï¼Œè¿™å¯èƒ½å¹¶ä¸æ€»æ˜¯æ˜¾è€Œæ˜“è§ï¼Œä½ éœ€è¦è€å¿ƒã€‚
- en: The second thing is that you actually need to scale up everything so this was
    work done with resnets not yet with transformers as you see a lot of renet models
    here the X axis is the number of images available in vision there is this imagenet
    dataset which is a very common super common data set for 320 which has 1ã€‚
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä»¶äº‹æ˜¯ä½ å®é™…ä¸Šéœ€è¦æ‰©å±•æ‰€æœ‰å†…å®¹ï¼Œè¿™é¡¹å·¥ä½œæ˜¯ç”¨æ®‹å·®ç½‘ç»œå®Œæˆçš„ï¼Œè€Œä¸æ˜¯ç”¨å˜å‹å™¨ï¼Œæ­£å¦‚ä½ åœ¨è¿™é‡Œçœ‹åˆ°çš„å¾ˆå¤šæ®‹å·®ç½‘ç»œæ¨¡å‹ï¼ŒXè½´æ˜¯å¯ç”¨å›¾åƒçš„æ•°é‡ï¼Œåœ¨è§†è§‰ä¸Šæœ‰è¿™ä¸ªImagenetæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸å¸¸è§çš„æ•°æ®é›†ï¼ŒåŒ…å«320ä¸ªã€‚
- en: 3 million images there's another one which has 10 times more images that's still
    public and then there is one subset from this internal group that has 300 million
    labeled imagesã€‚So the Y axis is a measure of accuracy on some tasksï¼Œ and we tried
    manyï¼Œ they all look similarã€‚
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 300ä¸‡å¼ å›¾åƒï¼Œè¿˜æœ‰ä¸€ä¸ªåŒ…å«åå€æ›´å¤šå›¾åƒçš„ä»ç„¶å…¬å¼€çš„æ•°æ®é›†ï¼Œç„¶åæœ‰ä¸€ä¸ªæ¥è‡ªè¿™ä¸ªå†…éƒ¨ç»„çš„å­é›†ï¼ŒåŒ…å«3äº¿ä¸ªæ ‡æ³¨çš„å›¾åƒã€‚æ‰€ä»¥Yè½´æ˜¯æŸäº›ä»»åŠ¡çš„å‡†ç¡®æ€§åº¦é‡ï¼Œæˆ‘ä»¬å°è¯•äº†å¾ˆå¤šï¼Œå®ƒä»¬çœ‹èµ·æ¥éƒ½å¾ˆç›¸ä¼¼ã€‚
- en: And the dots are differently sized resonnet the blue dot is the standard resonate
    50 that everybody uses if this one you train on more data it looks promising at
    firstã€‚but if you go to even more data it looks like okay this doesn't really seem
    that useful and this is what most people have been doing for a long time and a
    lot of people even in Google were like yeah I tried this internal checkpoint on
    this kinds of data it doesn't really have that muchã€‚
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹çš„å¤§å°ä¸åŒï¼Œè“ç‚¹æ˜¯æ ‡å‡†çš„ResNet 50ï¼Œå¤§å®¶éƒ½åœ¨ç”¨ã€‚å¦‚æœä½ ç”¨æ›´å¤šçš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼Œå¼€å§‹çœ‹èµ·æ¥å¾ˆæœ‰å¸Œæœ›ã€‚ä½†å¦‚æœä½ ç”¨æ›´å¤šçš„æ•°æ®çœ‹èµ·æ¥ï¼Œè¿™ä¼¼ä¹å¹¶ä¸å¤ªæœ‰ç”¨ï¼Œè¿™ä¹Ÿæ˜¯å¤§å¤šæ•°äººå¾ˆé•¿æ—¶é—´ä»¥æ¥ä¸€ç›´åœ¨åšçš„ï¼Œå¾ˆå¤šäººç”šè‡³åœ¨è°·æ­Œçš„äººéƒ½è¯´ï¼Œå—¯ï¼Œæˆ‘åœ¨è¿™ç§æ•°æ®ä¸Šå°è¯•è¿‡è¿™ä¸ªå†…éƒ¨æ£€æŸ¥ç‚¹ï¼Œå®ƒå®é™…ä¸Šæ²¡æœ‰å¤ªå¤šã€‚
- en: Howeverï¼Œ what we found out and in hindsight is kind of obvious is that you actually
    need to scale not just the dataã€‚but also the model here this blue dot is a gigantic
    resonnet that is slow as hellã€‚but when you scale this up together with the data
    you keep getting benefit with adding more dataã€‚And then if you do these two thingsï¼Œ
    scale up everything and be patientï¼Œ be patient could also beã€‚
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œæˆ‘ä»¬å‘ç°çš„å¹¶ä¸”äº‹åçœ‹æ¥æœ‰ç‚¹æ˜æ˜¾çš„æ˜¯ï¼Œä½ å®é™…ä¸Šéœ€è¦ä¸ä»…æ‰©å±•æ•°æ®ï¼Œè¿˜éœ€è¦æ‰©å±•æ¨¡å‹ã€‚è¿™è“ç‚¹æ˜¯ä¸€ä¸ªå·¨å‹çš„ResNetï¼Œæ…¢å¾—è¦å‘½ã€‚ä½†å½“ä½ ä¸æ•°æ®ä¸€èµ·æ‰©å±•æ—¶ï¼Œä½ ä¼šæŒç»­å—ç›Šäºæ·»åŠ æ›´å¤šæ•°æ®ã€‚å¦‚æœä½ åšåˆ°è¿™ä¸¤ä»¶äº‹ï¼Œæ‰©å±•æ‰€æœ‰å†…å®¹å¹¶è€å¿ƒï¼Œè€å¿ƒä¹Ÿå¯ä»¥æ˜¯ã€‚
- en: Quite scale up your patientsã€‚å—¯ã€‚Then you get a lot of benefitsã€‚So here there
    is a few shot transfer learningã€‚what I showed you before and on the X axis is
    size of the model on the Y axis is the accuracy on one of these tasksã€‚but again
    others with similar and these three different curves are pretrained with different
    data set sizesã€‚the green one being the standard oneï¼Œ you don't really see benefit
    or small benefit from going with larger models The blue one is 10 times largerã€‚
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®æ˜¯å¤§å¹…æå‡ä½ çš„æ‚£è€…ã€‚å—¯ã€‚è¿™æ ·ä½ ä¼šå¾—åˆ°å¾ˆå¤šå¥½å¤„ã€‚æ‰€ä»¥è¿™é‡Œæœ‰å‡ æ¬¡è¿ç§»å­¦ä¹ çš„ç¤ºä¾‹ã€‚æˆ‘ä¹‹å‰å±•ç¤ºçš„ï¼ŒXè½´æ˜¯æ¨¡å‹çš„å¤§å°ï¼ŒYè½´æ˜¯è¿™äº›ä»»åŠ¡ä¹‹ä¸€çš„å‡†ç¡®æ€§ï¼Œä½†å…¶ä»–çš„ä»»åŠ¡ä¹Ÿæ˜¯ç±»ä¼¼çš„ï¼Œè¿™ä¸‰æ¡ä¸åŒçš„æ›²çº¿æ˜¯ç”¨ä¸åŒæ•°æ®é›†å¤§å°é¢„è®­ç»ƒçš„ï¼Œç»¿è‰²çš„æ›²çº¿æ˜¯æ ‡å‡†çš„ï¼Œä»è¾ƒå¤§çš„æ¨¡å‹ä¸Šä½ å¹¶æ²¡æœ‰çœ‹åˆ°æ˜æ˜¾çš„å¥½å¤„æˆ–åªæ˜¯å°å¹…çš„å¥½å¤„ï¼Œè“è‰²çš„æ›²çº¿æ˜¯åå€å¤§ã€‚
- en: you start seeing some slope upwardsï¼Œ but really only with this giant dataã€‚you
    start like getting better and better and better at least few shot transfer learning
    when you pretrain on more data with larger and larger modelsã€‚ğŸ˜Šï¼Œsecond benefit
    that we did not anticipate really at allã€‚but then found out is that these models
    are super robust when you scale everything up This object netã€‚
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¼€å§‹çœ‹åˆ°ä¸€äº›å‘ä¸Šçš„æ–œç‡ï¼Œä½†å®é™…ä¸Šåªæœ‰åœ¨è¿™ä¸ªå·¨å¤§çš„æ•°æ®ä¸Šã€‚å½“ä½ åœ¨æ›´å¤šæ•°æ®å’Œæ›´å¤§çš„æ¨¡å‹ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼Œå‡ æ¬¡è¿ç§»å­¦ä¹ çš„è¡¨ç°ä¼šè¶Šæ¥è¶Šå¥½ã€‚ğŸ˜Šï¼Œç¬¬äºŒä¸ªå¥½å¤„æˆ‘ä»¬å®Œå…¨æ²¡æœ‰é¢„æ–™åˆ°ï¼Œä½†åæ¥å‘ç°çš„æ˜¯ï¼Œè¿™äº›æ¨¡å‹åœ¨ä¸€åˆ‡è§„æ¨¡æå‡æ—¶éå¸¸ç¨³å¥ï¼Œè¿™ä¸ªå¯¹è±¡ç½‘ç»œã€‚
- en: it's a data set that' specifically designed to measure robustness and it shows
    things in crazy like a chair in a bathtub and things like that and you should
    recognize it as chair And here the pink dots are basically how existing models
    and exact is again how large is the model and pink dot is existing ones from the
    literature and then these lines same color coding is what we found outã€‚
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä¸€ä¸ªä¸“é—¨è®¾è®¡ç”¨æ¥æµ‹é‡ç¨³å¥æ€§çš„æ•°æ®é›†ï¼Œå®ƒå±•ç¤ºäº†ä¸€äº›å¥‡æ€ªçš„æƒ…å†µï¼Œæ¯”å¦‚æ¤…å­åœ¨æµ´ç¼¸é‡Œï¼Œä½ åº”è¯¥è¯†åˆ«å‡ºè¿™æ˜¯æ¤…å­ã€‚è¿™é‡Œçš„ç²‰è‰²ç‚¹åŸºæœ¬ä¸Šè¡¨ç¤ºç°æœ‰æ¨¡å‹ï¼Œè€Œâ€œç¡®åˆ‡çš„â€åˆ™æ˜¯æ¨¡å‹çš„å¤§å°ï¼Œç²‰è‰²ç‚¹æ˜¯æ–‡çŒ®ä¸­çš„ç°æœ‰æ¨¡å‹ï¼Œç„¶åè¿™äº›çº¿åŒæ ·çš„é¢œè‰²ç¼–ç æ˜¯æˆ‘ä»¬å‘ç°çš„ç»“æœã€‚
- en: And again you see this large data and then going to large model just gives you
    amazing benefits on like in this case out of a distribution robustã€‚ğŸ˜Šï¼ŒOkay so this
    was like amazing scale up everything be patient and get huge benefit sorry Lucas
    sorry for interrupting youã€‚
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å†ä¸€æ¬¡ï¼Œä½ ä¼šçœ‹åˆ°å¤§é‡æ•°æ®ï¼Œç„¶åä½¿ç”¨å¤§æ¨¡å‹ä¼šåœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­æä¾›æƒŠäººçš„å¥½å¤„ï¼Œç‰¹åˆ«æ˜¯åœ¨åˆ†å¸ƒç¨³å¥æ€§æ–¹é¢ã€‚ğŸ˜Šï¼Œå¥½çš„ï¼Œæ‰€ä»¥è¿™ä¸€åˆ‡éƒ½æ˜¯åœ¨æƒŠäººçš„è§„æ¨¡ä¸Šæå‡ï¼Œä¸€å®šè¦è€å¿ƒï¼Œè·å¾—å·¨å¤§çš„å¥½å¤„ï¼Œå¯¹ä¸èµ·ï¼Œå¢å¡æ–¯ï¼Œæ‰“æ–­ä½ äº†ã€‚
- en: but there is a question from a student in the class rightã€‚ğŸ˜Šã€‚Do you want to unmute
    yourself and ask it yourselfï¼Ÿ
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è¿‡ï¼Œç­ä¸Šæœ‰ä¸ªå­¦ç”Ÿçš„é—®é¢˜ï¼Œå¯¹å§ã€‚ğŸ˜Šã€‚ä½ æƒ³è‡ªå·±è§£å¼€é™éŸ³æé—®å—ï¼Ÿ
- en: Yeah I can I can ask my question can people hear me maybe there's a a one right
    once I can we just step away real quick yeah so the question that I want to know
    is what work has been done characterizing the parameters after pretrain finishes
    like the reason why I'm motivating this question isã€‚
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæˆ‘å¯ä»¥é—®æˆ‘çš„é—®é¢˜ï¼Œå¤§å®¶èƒ½å¬åˆ°æˆ‘å—ï¼Ÿä¹Ÿè®¸è¿˜æœ‰å…¶ä»–äººå¯ä»¥ï¼Œæˆ‘ä»¬èƒ½å¿«é€Ÿç¦»å¼€ä¸€ä¸‹å—ï¼Ÿæ‰€ä»¥æˆ‘æƒ³çŸ¥é“çš„æ˜¯ï¼Œåœ¨é¢„è®­ç»ƒç»“æŸåï¼Œå·²ç»åšäº†ä»€ä¹ˆå·¥ä½œæ¥è¡¨å¾å‚æ•°ï¼Œæ¿€åŠ±è¿™ä¸ªé—®é¢˜çš„åŸå› æ˜¯ã€‚
- en: It seems like we do this tremendous amount of pre trainingã€‚but it seems like
    we might be able to significantly reduce that if we just have smarter initialization
    schemesã€‚Yeahï¼Œ you knowï¼Œ I've been thinking this for a long time actually alsoã€‚and
    they've come to conclude that I think notã€‚ğŸ˜Šï¼ŒI think there is like two parts One
    is what I like to call handwa V the numers of the weightsã€‚
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥æˆ‘ä»¬åšäº†å¤§é‡çš„é¢„è®­ç»ƒã€‚ä½†ä¼¼ä¹å¦‚æœæˆ‘ä»¬æœ‰æ›´èªæ˜çš„åˆå§‹åŒ–æ–¹æ¡ˆï¼Œå°±å¯ä»¥æ˜¾è‘—å‡å°‘è¿™ä¸€ç‚¹ã€‚æ˜¯çš„ï¼Œä½ çŸ¥é“ï¼Œæˆ‘å…¶å®æƒ³äº†å¾ˆä¹…è¿™ä¸ªé—®é¢˜ã€‚æœ€åæˆ‘å¾—å‡ºç»“è®ºï¼Œæˆ‘è®¤ä¸ºä¸æ˜¯ã€‚ğŸ˜Šï¼Œæˆ‘è®¤ä¸ºæœ‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€ä¸ªæ˜¯æˆ‘å–œæ¬¢ç§°ä¹‹ä¸ºæƒé‡æ•°å­—çš„â€œæ‰‹æ®µâ€ã€‚
- en: you know that everything is in a nice range such that it can have nice input
    output functions and so on and that your optimizer can do steps that you know
    make reasonable change to the input output function but not too large and so on
    I think that is part of it and that you can get through good in it or good normalizations
    and whatnot but then I also think there is I do think that these models memorize
    a lot and thenã€‚
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ çŸ¥é“ä¸€åˆ‡éƒ½åœ¨ä¸€ä¸ªåˆé€‚çš„èŒƒå›´å†…ï¼Œä»¥ä¾¿å¯ä»¥æœ‰è‰¯å¥½çš„è¾“å…¥è¾“å‡ºå‡½æ•°ï¼Œå¹¶ä¸”ä½ çš„ä¼˜åŒ–å™¨å¯ä»¥æ‰§è¡Œæ­¥éª¤ï¼Œä½¿è¾“å…¥è¾“å‡ºå‡½æ•°çš„å˜åŒ–æ˜¯åˆç†çš„ï¼Œä½†åˆä¸ä¼šå¤ªå¤§ç­‰ç­‰ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯å…¶ä¸­çš„ä¸€éƒ¨åˆ†ï¼Œä½ å¯ä»¥é€šè¿‡è‰¯å¥½çš„åˆå§‹åŒ–æˆ–å½’ä¸€åŒ–æ¥å®ç°ï¼Œä½†æˆ‘ä¹Ÿè®¤ä¸ºè¿™äº›æ¨¡å‹ç¡®å®è®°ä½äº†å¾ˆå¤šä¸œè¥¿ï¼Œç„¶åã€‚
- en: ğŸ˜Šï¼ŒPersonallyï¼Œ I believeï¼Œ but I don't know of evidenceã€‚or so that these models
    do more kind ofã€‚you knowã€‚Remembering similarity to things they've seen in training
    and then as you grow things up they have more memory and they have seen more things
    so they should be better on more newer things because there's more similar things
    they have seen and this I don't think you can like just create one shot from initializationã€‚
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä¸ªäººè®¤ä¸ºï¼Œä½†æˆ‘æ²¡æœ‰è¯æ®ã€‚æˆ–è€…è¯´è¿™äº›æ¨¡å‹çš„ç¡®æ˜¯åšæ›´å¤šçš„äº‹æƒ…ã€‚ä½ çŸ¥é“å—ï¼Ÿè®°ä½å®ƒä»¬åœ¨è®­ç»ƒä¸­è§è¿‡çš„ä¸œè¥¿çš„ç›¸ä¼¼æ€§ï¼Œéšç€ä½ ä¸æ–­å‘å±•ï¼Œå®ƒä»¬çš„è®°å¿†ä¹Ÿä¼šå¢åŠ ï¼Œå®ƒä»¬è§è¿‡çš„ä¸œè¥¿ä¹Ÿä¼šæ›´å¤šï¼Œå› æ­¤å®ƒä»¬åœ¨å¤„ç†æ›´æ–°çš„å†…å®¹æ—¶åº”è¯¥è¡¨ç°å¾—æ›´å¥½ï¼Œå› ä¸ºå®ƒä»¬è§è¿‡æ›´å¤šç›¸ä¼¼çš„ä¸œè¥¿ï¼Œè€Œæˆ‘è®¤ä¸ºä½ ä¸èƒ½ä»…ä»…ä»åˆå§‹åŒ–ä¸­åˆ›é€ å‡ºä¸€ä¸ªã€‚
- en: But I don't have immediate points or to a paper at the top of my head now to
    answer your questionlyã€‚thank youã€‚I think we also have more questions or has posted
    on the chat and is raising his hand maybe in this audio you want to ask your question
    pleaseï¼Ÿ
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ç°åœ¨è„‘æµ·ä¸­æ²¡æœ‰ç«‹å³èƒ½æƒ³åˆ°çš„è¦ç‚¹æˆ–è®ºæ–‡æ¥å›ç­”ä½ çš„é—®é¢˜ã€‚è°¢è°¢ã€‚æˆ‘æƒ³æˆ‘ä»¬è¿˜æœ‰æ›´å¤šé—®é¢˜ï¼Œæˆ–è€…åœ¨èŠå¤©ä¸­å‘å¸ƒäº†é—®é¢˜ï¼Œæœ‰äººä¸¾æ‰‹ï¼Œä¹Ÿè®¸åœ¨è¿™ä¸ªéŸ³é¢‘ä¸­ä½ æƒ³æé—®å—ï¼Ÿ
- en: Yeahï¼Œ for sure if I can go ahead so I just said a quick clarification on this
    chart right hereã€‚the chart number threeï¼Œ the bit L and bit N and bitS are they
    the same model architectureã€‚but just trained on different data set so the bitS
    is trained on the 1ã€‚3 million all the way to the 300 million image data set for
    bit Lã€‚
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œå½“ç„¶å¯ä»¥ï¼Œæˆ‘åªæ˜¯æƒ³å¯¹è¿™ä¸ªå›¾è¡¨è¿›è¡Œå¿«é€Ÿæ¾„æ¸…ã€‚å›¾è¡¨ç¼–å·ä¸‰ï¼Œbit L å’Œ bit N ä»¥åŠ bit S æ˜¯åŒæ ·çš„æ¨¡å‹æ¶æ„ï¼Œä½†åªæ˜¯è®­ç»ƒäºä¸åŒçš„æ•°æ®é›†ï¼Œæ‰€ä»¥
    bit S æ˜¯åœ¨130ä¸‡åˆ°3äº¿çš„å›¾åƒæ•°æ®é›†ä¸Šè®­ç»ƒçš„ï¼Œé’ˆå¯¹ bit Lã€‚
- en: Yes and no the architecture is here on the x axis so within one vertical slice
    these are the same architecture and then the different points are random restarts
    because when you do future learning there is a lot of variance in which few examples
    do you see and then again these next vertical slice is the same model and so on
    and as you go to the right the model gets larger and so you can see that for this
    little data going to larger model doesn't really help you much for freetraining
    and only for this giant data air means the giant data not necessarily giant model
    in this caseã€‚
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œä¸ï¼Œæ¶æ„åœ¨ x è½´ä¸Šï¼Œæ‰€ä»¥åœ¨ä¸€ä¸ªå‚ç›´åˆ‡ç‰‡å†…ï¼Œè¿™äº›æ˜¯ç›¸åŒçš„æ¶æ„ï¼Œç„¶åä¸åŒçš„ç‚¹æ˜¯éšæœºé‡å¯ï¼Œå› ä¸ºå½“ä½ è¿›è¡Œæœªæ¥å­¦ä¹ æ—¶ï¼Œçœ‹åˆ°çš„å‡ ä¸ªç¤ºä¾‹ä¹‹é—´ä¼šæœ‰å¾ˆå¤§çš„å˜åŒ–ã€‚ç„¶åï¼Œæ¥ä¸‹æ¥çš„å‚ç›´åˆ‡ç‰‡æ˜¯ç›¸åŒçš„æ¨¡å‹ï¼Œä¾æ­¤ç±»æ¨ï¼Œéšç€ä½ å‘å³ç§»åŠ¨ï¼Œæ¨¡å‹ä¼šå˜å¾—æ›´å¤§ï¼Œå› æ­¤ä½ å¯ä»¥çœ‹åˆ°å¯¹äºè¿™å°æ•°æ®é›†æ¥è¯´ï¼Œå˜å¤§æ¨¡å‹å¹¶æ²¡æœ‰å¤ªå¤§å¸®åŠ©ï¼Œåªæœ‰åœ¨è¿™ä¸ªå·¨å¤§çš„æ•°æ®é›†ä¸Šï¼Œè¿™æ„å‘³ç€å·¨å¤§çš„æ•°æ®ä¸ä¸€å®šæ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹çš„å·¨å‹æ¨¡å‹ã€‚
- en: ğŸ˜Šï¼ŒRightï¼Œ it makes a lot of senseï¼Œ thank youï¼Œ okayã€‚Do you have a question of
    I say you're raising your hand as wellï¼Ÿ
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ²¡é”™ï¼Œè¿™å¾ˆæœ‰é“ç†ï¼Œè°¢è°¢ï¼Œå¥½çš„ã€‚ä½ æœ‰é—®é¢˜å—ï¼Ÿæˆ‘çœ‹åˆ°ä½ ä¹Ÿä¸¾æ‰‹äº†ï¼Ÿ
- en: å–‚ä¸ªäººä½ å¥½å’¯ã€‚He yeahï¼Œ thanksã€‚What is the intuition for the upstream performance and
    figure one spiking so suddenly atï¼Ÿ
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å–‚ï¼Œä¸ªäººä½ å¥½ã€‚å˜¿ï¼Œå¥½çš„ï¼Œè°¢è°¢ã€‚ä¸Šæ¸¸æ€§èƒ½çš„ç›´è§‰æ˜¯ä»€ä¹ˆï¼Œå›¾1çªç„¶æ¿€å¢æ˜¯ä¸ºä»€ä¹ˆï¼Ÿ
- en: I guess or3 points in training Yeah right yeah Yeah yeah again like round 1ã€‚like
    I don't know that just seems like an a looking training curve like yeah what's
    the intuition behind that Yeah this is old school computer vision thing or I a
    few years ago is this when the learning rate changes and computer vision it used
    to be very common to have the learning rate in kind of staircase pattern so it's
    constant for a while and then you stopped you divide the learning rate by 10 usually
    boom smaller and then you continue and this gives you this huge jump nowadays
    people don't use this much anymore and this work was like three years ago I think
    or two or few years ago I don't remember it was very common back then and nowadays
    people use more continuously changing learning rate schedule and then you don't
    really have this sudden change anymore but if you would overlay it would be like
    more continuously but going roughly the same and then in language I think most
    people or many people useã€‚
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘æƒ³åœ¨è®­ç»ƒä¸­æœ‰3ä¸ªç‚¹ï¼Œå¯¹å§ï¼Ÿæ˜¯çš„ï¼Œç¡®å®æ˜¯ã€‚å†æ¬¡è¯´ï¼Œåƒç¬¬ä¸€è½®ã€‚æˆ‘ä¸çŸ¥é“ï¼Œè¿™ä¼¼ä¹å°±åƒä¸€ä¸ªçœ‹èµ·æ¥ä¸é”™çš„è®­ç»ƒæ›²çº¿ï¼Œå—¯ï¼Œé‚£èƒŒåçš„ç›´è§‰æ˜¯ä»€ä¹ˆï¼Ÿè¿™æ˜¯è€æ´¾è®¡ç®—æœºè§†è§‰çš„ä¸œè¥¿ï¼Œå‡ å¹´å‰æ˜¯è¿™æ ·çš„ï¼Œå½“å­¦ä¹ ç‡å‘ç”Ÿå˜åŒ–æ—¶ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œé€šå¸¸å¾ˆå¸¸è§çš„å­¦ä¹ ç‡æ˜¯é˜¶æ¢¯æ¨¡å¼ï¼Œæ‰€ä»¥ä¸€æ®µæ—¶é—´æ˜¯æ’å®šçš„ï¼Œç„¶åä½ åœæ­¢ï¼ŒæŠŠå­¦ä¹ ç‡é€šå¸¸é™¤ä»¥10ï¼Œç„¶åç»§ç»­ï¼Œè¿™ä¼šç»™ä½ å¸¦æ¥å·¨å¤§çš„è·³è·ƒã€‚å¦‚ä»Šäººä»¬ä¸å†ä½¿ç”¨è¿™ç§æ–¹æ³•äº†ï¼Œæˆ‘æƒ³è¿™é¡¹å·¥ä½œæ˜¯ä¸‰å¹´å‰çš„äº‹ï¼Œæˆ–è€…ä¸¤ä¸‰å¹´å‰ï¼Œæˆ‘ä¸è®°å¾—äº†ï¼Œé‚£æ—¶è¿™éå¸¸å¸¸è§ï¼Œè€Œå¦‚ä»Šäººä»¬ä½¿ç”¨æ›´è¿ç»­å˜åŒ–çš„å­¦ä¹ ç‡è®¡åˆ’ï¼Œå› æ­¤ä½ ä¸å†ä¼šæœ‰çªç„¶çš„å˜åŒ–ï¼Œä½†æ˜¯å¦‚æœä½ å åŠ å®ƒï¼Œç»“æœä¼šæ›´è¿ç»­ï¼Œä½†å¤§è‡´ç›¸åŒï¼Œè€Œåœ¨è¯­è¨€ä¸Šï¼Œæˆ‘è®¤ä¸ºå¤§å¤šæ•°äººæˆ–è®¸è®¸å¤šäººåœ¨ä½¿ç”¨ã€‚
- en: ğŸ˜Šï¼ŒThey're just linearly decreasinging learning rate schedule where also you
    don't see this effect because learning rate continuously decreasesã€‚Yesï¼Œ sounds
    goodï¼Œ thanksã€‚And then this is what because you ask for about this this dotted
    lineã€‚actually here if you're like here okay you could say okayï¼Œ but this is excessiveï¼Œ
    rightã€‚maybe it does really seem almost flatï¼Œ maybe you could have started the
    leakK earlier and earlier and earlier and then you would get the same but much
    quicker and this one shows what would happen then and you do land at much worse
    place in the end than with the patientã€‚
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä»–ä»¬åªæ˜¯åœ¨é€æ¸å‡å°‘çš„å­¦ä¹ ç‡è°ƒåº¦ï¼Œä½ ä¹Ÿçœ‹ä¸åˆ°è¿™ç§æ•ˆæœï¼Œå› ä¸ºå­¦ä¹ ç‡æŒç»­ä¸‹é™ã€‚æ˜¯çš„ï¼Œå¬èµ·æ¥ä¸é”™ï¼Œè°¢è°¢ã€‚é‚£ä¹ˆè¿™æ˜¯å› ä¸ºä½ é—®äº†è¿™ä¸ªè™šçº¿ã€‚å®é™…ä¸Šè¿™é‡Œï¼Œå¦‚æœä½ åœ¨è¿™é‡Œï¼Œå¥½å§ï¼Œä½ å¯ä»¥è¯´ï¼Œå¥½å§ï¼Œä½†è¿™å¤ªè¿‡äº†ï¼Œå¯¹å§ã€‚ä¹Ÿè®¸å®ƒç¡®å®çœ‹èµ·æ¥å‡ ä¹æ˜¯å¹³å¦çš„ï¼Œä¹Ÿè®¸ä½ å¯ä»¥æ›´æ—©åœ°å¼€å§‹å­¦ä¹ ç‡ï¼Œç„¶åä½ ä¼šå¾—åˆ°ç›¸åŒçš„æ•ˆæœï¼Œä½†æ›´å¿«ï¼Œè€Œè¿™æ˜¾ç¤ºäº†é‚£æ ·ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œæœ€åä½ ä¼šè½åœ¨ä¸€ä¸ªæ¯”è€å¿ƒè®­ç»ƒæ—¶æ›´ç³Ÿç³•çš„åœ°æ–¹ã€‚
- en: ğŸ˜Šï¼ŒOkayï¼Œ yeahï¼Œ yeahï¼Œ that makes sense thanksã€‚Was there more question or I continueã€‚What
    do you have your answer because I need to mentionï¼Œ I don't see what I just seem
    as likeã€‚Yeahã€‚it's fineï¼Œ we can coordinate that Lucasã€‚Hiï¼Œ yeahï¼Œ so I just want
    to make sure that I'm like on the same pageã€‚so basically what you're trying to
    do is multitask learning with convolutional neural network slash LSTF right that's
    kind of like resnetã€‚
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¥½çš„ï¼Œæ˜¯çš„ï¼Œæ²¡é”™ï¼Œè°¢è°¢ã€‚è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Œè¿˜æ˜¯æˆ‘ç»§ç»­ï¼Ÿä½ æœ‰ä»€ä¹ˆç­”æ¡ˆï¼Œå› ä¸ºæˆ‘éœ€è¦æåˆ°ï¼Œæˆ‘çœ‹ä¸å‡ºæˆ‘ä¼¼ä¹åƒã€‚æ˜¯çš„ï¼Œæ²¡å…³ç³»ï¼Œæˆ‘ä»¬å¯ä»¥åè°ƒä¸€ä¸‹ï¼Œå¢å¡æ–¯ã€‚å—¨ï¼Œæ˜¯çš„ï¼Œæˆ‘åªæ˜¯æƒ³ç¡®ä¿æˆ‘å’Œä½ åœ¨åŒä¸€é¡µä¸Šã€‚å› æ­¤ï¼ŒåŸºæœ¬ä¸Šä½ è¦åšçš„æ˜¯ä¸å·ç§¯ç¥ç»ç½‘ç»œ/LSTFçš„å¤šä»»åŠ¡å­¦ä¹ ï¼Œå¯¹å§ï¼Ÿè¿™æœ‰ç‚¹åƒResNetã€‚
- en: But you're doing multitask learningï¼Œ correctï¼Œ You knowã€‚where does the multitaask
    come from or where does itã€‚Because like initially like you showed like different
    I okay so there is two phases The first one is the pre training and this pretraining
    I didn't mention it yeah I just said I don't care what you do in the pretrain
    just pretrain somehow and giving me the model and then I test it on multiple tasks
    independently and I tested on multiple task means like transfer it to that task
    which in our case means fine unit just on the task and see how well it does and
    so on but it could mean other things like later we moved to just learning linear
    linear regression on top of the embeddings for each task and now doing the pretraining
    what you do is just regular supervised learning but just scaling everything up
    and regular supervised learning is justã€‚
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯ä½ åœ¨åšå¤šä»»åŠ¡å­¦ä¹ ï¼Œå¯¹å—ï¼Ÿä½ çŸ¥é“ï¼Œå¤šä»»åŠ¡æ¥è‡ªå“ªé‡Œï¼Œæˆ–è€…å®ƒæ˜¯ä»€ä¹ˆï¼Ÿå› ä¸ºæœ€åˆä½ å±•ç¤ºäº†ä¸åŒçš„ã€‚æˆ‘æ˜ç™½äº†ï¼Œæ‰€ä»¥æœ‰ä¸¤ä¸ªé˜¶æ®µï¼Œç¬¬ä¸€ä¸ªæ˜¯é¢„è®­ç»ƒï¼Œè€Œè¿™ä¸ªé¢„è®­ç»ƒæˆ‘æ²¡æœ‰æåˆ°ã€‚æˆ‘åªæ˜¯è¯´æˆ‘ä¸åœ¨ä¹ä½ åœ¨é¢„è®­ç»ƒä¸­åšä»€ä¹ˆï¼Œåªè¦ä»¥æŸç§æ–¹å¼è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åç»™æˆ‘æ¨¡å‹ï¼Œç„¶åæˆ‘åœ¨å¤šä¸ªä»»åŠ¡ä¸Šç‹¬ç«‹æµ‹è¯•å®ƒï¼Œæµ‹è¯•å¤šä¸ªä»»åŠ¡æ„å‘³ç€å°†å…¶è½¬ç§»åˆ°é‚£ä¸ªä»»åŠ¡ä¸Šï¼Œåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œè¿™æ„å‘³ç€ä»…åœ¨è¯¥ä»»åŠ¡ä¸Šè¿›è¡Œå¾®è°ƒï¼Œçœ‹çœ‹æ•ˆæœå¦‚ä½•ç­‰ç­‰ï¼Œä½†è¿™å¯èƒ½æ„å‘³ç€å…¶ä»–äº‹æƒ…ï¼Œæ¯”å¦‚åæ¥æˆ‘ä»¬è½¬å‘åœ¨æ¯ä¸ªä»»åŠ¡çš„åµŒå…¥ä¸Šå­¦ä¹ çº¿æ€§å›å½’ï¼Œè€Œç°åœ¨è¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼Œä½ æ‰€åšçš„å°±æ˜¯å¸¸è§„çš„ç›‘ç£å­¦ä¹ ï¼Œåªæ˜¯å°†ä¸€åˆ‡è§„æ¨¡åŒ–ï¼Œå¸¸è§„çš„ç›‘ç£å­¦ä¹ å°±æ˜¯è¿™æ ·ã€‚
- en: ğŸ˜Šï¼ŒWellï¼Œ not multitaskï¼Œ but multila in the sense that an image could have a couple
    labels or notã€‚but it usually doesn't have a this minority got it thanksã€‚Yeahã€‚just
    have a quick follow up about the question rather than like the discussion ratherin
    started about this its like imization or it's more memorizing the data in a pretraining
    dataset setã€‚So I know in the language side there's a quite interesting phenomenon
    that you can pretrain on a synthetic language that it doesn't have any semantic
    meaningã€‚
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå¥½å§ï¼Œä¸æ˜¯å¤šä»»åŠ¡ï¼Œè€Œæ˜¯å¤šæ ‡ç­¾çš„æ„æ€ï¼Œä¸€ä¸ªå›¾åƒå¯ä»¥æœ‰å‡ ä¸ªæ ‡ç­¾æˆ–è€…æ²¡æœ‰ã€‚ä½†å®ƒé€šå¸¸ä¸ä¼šæœ‰è¿™ç§å°‘æ•°æ ‡ç­¾ï¼Œæ˜ç™½äº†ï¼Œè°¢è°¢ã€‚æ˜¯çš„ï¼Œå…³äºè¿™ä¸ªé—®é¢˜æˆ‘åªæ˜¯æƒ³å¿«é€Ÿè·Ÿè¿›ï¼Œè€Œä¸æ˜¯å…³äºè®¨è®ºï¼Œè€Œæ˜¯å¼€å§‹æ—¶è°ˆè®ºå®ƒæ›´åƒæ˜¯ä¼˜åŒ–æˆ–è€…æ›´å¤šåœ°æ˜¯è®°å¿†é¢„è®­ç»ƒæ•°æ®é›†ä¸­çš„æ•°æ®ã€‚æ‰€ä»¥æˆ‘çŸ¥é“åœ¨è¯­è¨€æ–¹é¢ï¼Œæœ‰ä¸€ä¸ªå¾ˆæœ‰è¶£çš„ç°è±¡ï¼Œä½ å¯ä»¥åœ¨ä¸€ä¸ªæ²¡æœ‰ä»»ä½•è¯­ä¹‰æ„ä¹‰çš„åˆæˆè¯­è¨€ä¸Šè¿›è¡Œé¢„è®­ç»ƒã€‚
- en: but it only have structural like know paired premises or things like thatã€‚And
    that actually it gives you almost the same boost in your downstream transfer as
    a normal pretrainingã€‚So I wonder if say like so this means like for language right
    the structure seems makes a lot of contribution which can be replaced by utilizationã€‚but
    I don't know if it's an image is a different caseã€‚
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†å®ƒåªæœ‰ç»“æ„æ€§çš„é…å¯¹å‰ææˆ–ç±»ä¼¼çš„ä¸œè¥¿ã€‚å®é™…ä¸Šï¼Œè¿™å‡ ä¹ç»™ä½ åœ¨ä¸‹æ¸¸è¿ç§»ä¸­ä¸æ­£å¸¸é¢„è®­ç»ƒç›¸åŒçš„æå‡ã€‚å› æ­¤ï¼Œæˆ‘æƒ³çŸ¥é“ï¼Œæ¯”å¦‚è¯´ï¼Œè¿™æ„å‘³ç€å¯¹äºè¯­è¨€ï¼Œç»“æ„ä¼¼ä¹è´¡çŒ®å¾ˆå¤§ï¼Œå¯ä»¥é€šè¿‡åˆ©ç”¨æ¥æ›¿ä»£ã€‚ä½†æ˜¯æˆ‘ä¸çŸ¥é“åœ¨å›¾åƒæ–¹é¢æ˜¯å¦æ˜¯ä¸åŒçš„æƒ…å†µã€‚
- en: maybe have people done maybe some synthetic pretraining dataset set for image
    there was a paper I forgot the name and the authorsã€‚but it creates completely
    synthetic imageã€‚and like not even rendering of some realistic things but just
    completely patternsã€‚
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸æœ‰äººåšè¿‡ä¸€äº›åˆæˆé¢„è®­ç»ƒæ•°æ®é›†ï¼Œç”¨äºå›¾åƒï¼Œæœ‰ä¸€ç¯‡è®ºæ–‡æˆ‘å¿˜äº†åå­—å’Œä½œè€…ã€‚ä½†å®ƒåˆ›å»ºäº†å®Œå…¨åˆæˆçš„å›¾åƒï¼Œç”šè‡³ä¸æ˜¯ä¸€äº›ç°å®äº‹ç‰©çš„æ¸²æŸ“ï¼Œè€Œåªæ˜¯å®Œå…¨çš„å›¾æ¡ˆã€‚
- en: waves and shapes and so on and uses that for pretrain and then shows that they
    get almost the same performance as imate quickly they actually do this with vision
    transformers butã€‚Yeahï¼Œ they never go further or it's not hereã€‚You knowã€‚they kind
    of that you can almost get to this point hereã€‚Then it's not clear how much further
    can you go away thisã€‚ And I think probably not much furtherã€‚
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¢å½¢å’Œå½¢çŠ¶ç­‰ï¼Œå¹¶ç”¨å®ƒä»¬è¿›è¡Œé¢„è®­ç»ƒï¼Œç„¶åæ˜¾ç¤ºå®ƒä»¬å‡ ä¹è·å¾—äº†ä¸å›¾åƒç›¸åŒçš„æ€§èƒ½ï¼Œå®é™…ä¸Šä»–ä»¬ç”¨è§†è§‰å˜æ¢å™¨åšåˆ°è¿™ä¸€ç‚¹ã€‚ä½†ã€‚æ˜¯çš„ï¼Œä»–ä»¬ä»æœªè¿›ä¸€æ­¥å‘å±•ï¼Œæˆ–è€…ä¸åœ¨è¿™é‡Œã€‚ä½ çŸ¥é“ã€‚ä»–ä»¬å‡ ä¹å¯ä»¥åˆ°è¾¾è¿™ä¸ªç‚¹ã€‚ç„¶åä¸æ¸…æ¥šä½ èƒ½èµ°å¤šè¿œã€‚æˆ‘è®¤ä¸ºå¯èƒ½æ²¡å¤šè¿œã€‚
- en: But it's just me guessing they're not much firmï¼Œ I don't have evidence for itã€‚So
    I have one question and then we can continue the talk said that we think like
    a large vision models are like learning some sort of similarity the data set their
    train on so do you think like they are like beaving like protopical networks in
    a senseã€‚
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘åªæ˜¯çŒœæµ‹å®ƒä»¬ä¸å¤ªå¯é ï¼Œæˆ‘æ²¡æœ‰è¯æ®ã€‚å› æ­¤æˆ‘æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥ç»§ç»­è°ˆè®ºï¼Œæˆ‘ä»¬è®¤ä¸ºå¤§å‹è§†è§‰æ¨¡å‹åœ¨æŸç§ç¨‹åº¦ä¸Šå°±åƒæ˜¯åœ¨å­¦ä¹ å®ƒä»¬æ‰€è®­ç»ƒçš„æ•°æ®é›†çš„ç›¸ä¼¼æ€§ï¼Œä½ è§‰å¾—å®ƒä»¬åœ¨æŸç§æ„ä¹‰ä¸ŠåƒåŸå‹ç½‘ç»œå—ï¼Ÿ
- en: They are we having like what networks so like prototypical networks essentially
    like when you like doing fact few short learningã€‚you just say like I'm going to
    learn a network I can learn the metric spaceã€‚ğŸ˜Šï¼ŒProbably not exactlyã€‚but close
    eachã€‚å—¯ã€‚I meanï¼Œ I cannot really say because this is just some intuitive guess that
    I haveã€‚That's what they doï¼Œ but nobody really knows what the models they readã€‚ğŸ˜Šï¼Œå˜¢å•Šã€‚I
    meanã€‚
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬æ˜¯è¿™æ ·çš„ç½‘ç»œï¼Œæ‰€ä»¥åƒåŸå‹ç½‘ç»œæœ¬è´¨ä¸Šå°±æ˜¯å½“ä½ è¿›è¡Œå°‘é‡å­¦ä¹ æ—¶ã€‚ä½ åªéœ€è¯´æˆ‘å°†å­¦ä¹ ä¸€ä¸ªç½‘ç»œï¼Œæˆ‘å¯ä»¥å­¦ä¹ åº¦é‡ç©ºé—´ã€‚ğŸ˜Šï¼Œå¯èƒ½ä¸å®Œå…¨å¦‚æ­¤ï¼Œä½†æ¥è¿‘ã€‚å—¯ã€‚æˆ‘æ˜¯è¯´ï¼Œæˆ‘çœŸçš„ä¸èƒ½è¯´ï¼Œå› ä¸ºè¿™åªæ˜¯æˆ‘çš„ä¸€äº›ç›´è§‰çŒœæµ‹ã€‚è¿™å°±æ˜¯å®ƒä»¬çš„å·¥ä½œæ–¹å¼ï¼Œä½†æ²¡æœ‰äººçœŸæ­£çŸ¥é“å®ƒä»¬è¯»å–çš„æ¨¡å‹ã€‚ğŸ˜Šï¼Œå˜¢å•Šã€‚æˆ‘æ˜¯è¯´ã€‚
- en: we do get much work when we do something like prototypical networks for the
    future learning with these pretrain modelsã€‚we do get much worse performance than
    when we do fine tu so there is a bit more to it thereã€‚however I don't knowã€‚What
    is this moreï¼ŸOkayï¼Œ thanksã€‚All rightï¼Œ let's continueã€‚Okayï¼Œ yeahï¼Œ soã€‚Rightã€‚and I
    didn't mentionï¼Œ but like on INe which is the top benchmark in computer vision
    with this work with the big transferã€‚
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬ä½¿ç”¨è¿™äº›é¢„è®­ç»ƒæ¨¡å‹è¿›è¡ŒåŸå‹ç½‘ç»œçš„æœªæ¥å­¦ä¹ æ—¶ï¼Œæˆ‘ä»¬çš„è¡¨ç°ç¡®å®æ¯”è¿›è¡Œå¾®è°ƒæ—¶è¦å·®å¾—å¤šï¼Œæ‰€ä»¥è¿˜æœ‰æ›´å¤šçš„ä¸œè¥¿åœ¨è¿™é‡Œã€‚ç„¶è€Œï¼Œæˆ‘ä¸çŸ¥é“ã€‚è¿™æ›´å¤šæ˜¯ä»€ä¹ˆï¼Ÿå¥½çš„ï¼Œè°¢è°¢ã€‚å¥½å§ï¼Œç»§ç»­ã€‚å¥½çš„ï¼Œæ˜¯çš„ï¼Œæ‰€ä»¥ã€‚å¯¹ã€‚æˆ‘æ²¡æåˆ°ï¼Œä½†åƒINeè¿™æ ·ï¼Œè¿™æ˜¯è®¡ç®—æœºè§†è§‰é¢†åŸŸçš„é¡¶çº§åŸºå‡†ï¼Œä¸è¿™é¡¹å¤§è§„æ¨¡è½¬ç§»å·¥ä½œç›¸å…³ã€‚
- en: we finally were able to increase the score after there was like a long period
    of a couple years of no improvement but many attachments that you see the grade
    say the yayã€‚awesome pretraining scaling up everything and leveraging the data
    and then okay let's not care about thatã€‚
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæˆ‘ä»¬èƒ½å¤Ÿæé«˜åˆ†æ•°ï¼Œåœ¨ç»å†äº†å‡ å¹´çš„æ²¡æœ‰æ”¹å–„çš„æ¼«é•¿æ—¶æœŸä¹‹åï¼Œä½†è®¸å¤šé™„ä»¶è®©ä½ çœ‹åˆ°æˆç»©ï¼Œå¤ªæ£’äº†ã€‚å¾ˆæ£’çš„é¢„è®­ç»ƒï¼Œæ‰©å¤§ä¸€åˆ‡å¹¶åˆ©ç”¨æ•°æ®ï¼Œç„¶åå¥½å§ï¼Œæˆ‘ä»¬ä¸å…³å¿ƒè¿™ä¸ªã€‚
- en: ğŸ˜Šï¼ŒYeahï¼Œ that's okay this is just a little aside that if we are in the setting
    that I mentioned of pretraining on huge amounts of data and then testing on many
    other tasksã€‚you should of course be careful that you don't have images from the
    other tasks in your pretraining data right otherwise you have seen and training
    and then you're not really generalizing and you just pulling yourself with good
    scores and this is a real danger when we get huge amounts of data because like
    im that images can totally be in huge amounts of data right So we actually use
    the internal pipelineã€‚
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ˜¯çš„ï¼Œæ²¡é—®é¢˜ï¼Œè¿™åªæ˜¯ä¸€ä¸ªå°æ’æ›²ï¼Œå¦‚æœæˆ‘ä»¬å¤„äºæˆ‘æåˆ°çš„åœ¨å¤§é‡æ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒç„¶ååœ¨è®¸å¤šå…¶ä»–ä»»åŠ¡ä¸Šæµ‹è¯•çš„è®¾ç½®ä¸­ã€‚å½“ç„¶ï¼Œä½ åº”è¯¥å°å¿ƒä¸è¦åœ¨ä½ çš„é¢„è®­ç»ƒæ•°æ®ä¸­æœ‰æ¥è‡ªå…¶ä»–ä»»åŠ¡çš„å›¾åƒï¼Œå¦åˆ™ä½ å°±ä¼šçœ‹åˆ°å¹¶è®­ç»ƒï¼Œè¿™æ ·ä½ å®é™…ä¸Šå¹¶æ²¡æœ‰çœŸæ­£çš„æ³›åŒ–ï¼Œä½ åªæ˜¯é€šè¿‡è‰¯å¥½çš„åˆ†æ•°è‡ªæˆ‘æ‹‰é«˜ï¼Œè¿™åœ¨æˆ‘ä»¬å¤„ç†å¤§é‡æ•°æ®æ—¶æ˜¯ä¸€ä¸ªçœŸæ­£çš„å±é™©ï¼Œå› ä¸ºè¿™äº›å›¾åƒå¯ä»¥å®Œå…¨åŒ…å«åœ¨å¤§é‡æ•°æ®ä¸­ã€‚æ‰€ä»¥æˆ‘ä»¬å®é™…ä¸Šä½¿ç”¨å†…éƒ¨ç®¡é“ã€‚
- en: there is really good at finding duplicates and also new duplicates like when
    they are shiftedã€‚rotated squeezed color change a bit whatnot it still find it
    and we use this to completely remove all images from the test dataset sets that
    we test on later and we actually found that a lot of classic just vision data
    sets have clear duplicates between the training and validation set between the
    training set ofã€‚
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒåœ¨å¯»æ‰¾é‡å¤é¡¹ä»¥åŠæ–°é‡å¤é¡¹æ–¹é¢è¡¨ç°éå¸¸å‡ºè‰²ï¼Œæ¯”å¦‚å½“å®ƒä»¬è¢«ç§»åŠ¨ã€æ—‹è½¬ã€å‹ç¼©ã€é¢œè‰²ç¨å¾®å˜åŒ–æ—¶ï¼Œå®ƒä»ç„¶èƒ½å¤Ÿæ‰¾åˆ°ï¼Œæˆ‘ä»¬åˆ©ç”¨è¿™ä¸€ç‚¹å®Œå…¨ç§»é™¤äº†åç»­æµ‹è¯•æ•°æ®é›†ä¸­æ‰€æœ‰çš„å›¾åƒï¼Œå®é™…ä¸Šæˆ‘ä»¬å‘ç°è®¸å¤šç»å…¸çš„è§†è§‰æ•°æ®é›†åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¹‹é—´æœ‰æ˜æ˜¾çš„é‡å¤é¡¹ã€‚
- en: ğŸ˜Šï¼ŒImagineNe and CFR10 and00 test sets and so onã€‚so near duplicates are quite
    widespread problem envision vision and this slide is just to say heyã€‚there are
    problems but in all that we present we actually took care that in duplicate training
    as best as we can we don't have near duplicatesã€‚
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œä¾‹å¦‚ImageNetå’ŒCIFAR10ç­‰æµ‹è¯•é›†ã€‚å› æ­¤ï¼Œè¿‘é‡å¤æ˜¯è§†è§‰ä¸­ä¸€ä¸ªç›¸å½“æ™®éçš„é—®é¢˜ï¼Œè¿™å¼ å¹»ç¯ç‰‡åªæ˜¯æƒ³è¯´ï¼Œæœ‰é—®é¢˜ï¼Œä½†åœ¨æˆ‘ä»¬æ‰€å±•ç¤ºçš„æ‰€æœ‰å†…å®¹ä¸­ï¼Œæˆ‘ä»¬å®é™…ä¸Šå°½å¯èƒ½åœ°ç¡®ä¿åœ¨é‡å¤è®­ç»ƒä¸­æ²¡æœ‰è¿‘é‡å¤é¡¹ã€‚
- en: å—¯ã€‚Right now back to being like heyï¼Œ we figured out large dataã€‚a larger model
    and then things get really good and that's how we got to transformers basically
    in computer visionã€‚everything was convolutional networks for many years and basically
    there was nothing LTMM is scheme however in language we saw transformation recently
    right that everything used to be LSTM everywhereã€‚LSTM was king and then came the
    transformer and in the case when there is a lot of data available suddenly transformer
    work much better than LSTM or little data that was still not the case exactlyã€‚
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚ç°åœ¨å›åˆ°æˆ‘ä»¬å‘ç°å¤§æ•°æ®å’Œæ›´å¤§æ¨¡å‹çš„é—®é¢˜ï¼Œç„¶åäº‹æƒ…å˜å¾—çœŸçš„å¾ˆå¥½ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨è®¡ç®—æœºè§†è§‰ä¸­èµ°å‘å˜æ¢å™¨çš„ã€‚å¤šå¹´æ¥ï¼Œä¸€åˆ‡éƒ½æ˜¯å·ç§¯ç½‘ç»œï¼ŒåŸºæœ¬ä¸Šæ²¡æœ‰å…¶ä»–æ–¹æ¡ˆã€‚ç„¶è€Œåœ¨è¯­è¨€å¤„ç†ä¸­ï¼Œæˆ‘ä»¬æœ€è¿‘çœ‹åˆ°äº†å˜æ¢å™¨çš„å‡ºç°ï¼Œæ›¾ç»åˆ°å¤„éƒ½æ˜¯LSTMã€‚LSTMæ›¾ç»æ˜¯ç‹è€…ï¼Œç„¶åå˜æ¢å™¨æ¥äº†ï¼Œå½“å¯ç”¨æ•°æ®é‡å¾ˆå¤§æ—¶ï¼Œå˜æ¢å™¨çš„è¡¨ç°çªç„¶å¥½äºLSTMï¼Œä½†åœ¨æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œæƒ…å†µå¹¶ä¸æ˜¯è¿™æ ·ã€‚
- en: ğŸ˜Šï¼ŒSo what we then thought is that okayï¼Œ so we are now in this where we have
    tos of data and do we see benefit from itã€‚can we see even more benefit if we try
    also out the transform architecture and visionã€‚And that's basically what we did
    to be third there were a few other attempts that try not transformer In before
    that I don't want to detail too much here because I don't want to like point fingers
    too much but they were are not really using transformers for learning everything
    from the data it was always like get something out of a reson at first like object
    detection proposals or highle feature maps or things like that and then stick
    a little transformer on topã€‚
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥æˆ‘ä»¬å½“æ—¶æƒ³ï¼Œæ—¢ç„¶æˆ‘ä»¬ç°åœ¨æ‹¥æœ‰å¤§é‡æ•°æ®ï¼Œæˆ‘ä»¬èƒ½å¦ä»ä¸­è·å¾—å¥½å¤„ï¼Ÿå¦‚æœæˆ‘ä»¬å°è¯•å˜æ¢æ¶æ„å’Œè§†è§‰ï¼Œèƒ½å¦è·å¾—æ›´å¤šå¥½å¤„ï¼Ÿè¿™åŸºæœ¬ä¸Šå°±æ˜¯æˆ‘ä»¬æ‰€åšçš„ï¼Œç¬¬ä¸‰æ–¹ä¹Ÿæœ‰ä¸€äº›å…¶ä»–å°è¯•åœ¨æ­¤ä¹‹å‰å¹¶æœªä½¿ç”¨å˜æ¢å™¨ï¼Œæˆ‘ä¸æƒ³åœ¨è¿™é‡Œè¯¦ç»†è¯´æ˜ï¼Œå› ä¸ºæˆ‘ä¸æƒ³è¿‡äºæŒ‡è´£ï¼Œä½†ä»–ä»¬å®é™…ä¸Šå¹¶æ²¡æœ‰çœŸæ­£åˆ©ç”¨å˜æ¢å™¨ä»æ•°æ®ä¸­å­¦ä¹ ï¼Œæ€»æ˜¯å…ˆä»æŸç§ç»“æœä¸­æå–ä¿¡æ¯ï¼Œæ¯”å¦‚ç›®æ ‡æ£€æµ‹ææ¡ˆæˆ–é«˜åº¦ç‰¹å¾å›¾ä¹‹ç±»çš„ï¼Œç„¶ååœ¨ä¸Šé¢åŠ ä¸€ç‚¹å˜æ¢å™¨ã€‚
- en: but we wanted to go like all the way just transformer everythingã€‚And so we came
    up with the simplest and more naturalï¼Œ I believeã€‚way of applying transformers
    to visionï¼Œ which is you take the imageã€‚you cut it in two pieces and that's it
    like a puzzleï¼Œ tech ta patchesã€‚
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯æˆ‘ä»¬æƒ³è¦å®Œå…¨ä½¿ç”¨å˜æ¢å™¨ã€‚å› æ­¤æˆ‘ä»¬æƒ³å‡ºäº†æœ€ç®€å•ã€æœ€è‡ªç„¶çš„åº”ç”¨å˜æ¢å™¨äºè§†è§‰çš„æ–¹æ³•ï¼Œå°±æ˜¯ä½ è·å–å›¾åƒï¼Œå°†å…¶åˆ‡å‰²æˆä¸¤éƒ¨åˆ†ï¼Œå°±åƒæ‹¼å›¾ä¸€æ ·ï¼Œåˆ†å‰²æˆè¡¥ä¸ã€‚
- en: And that's it Each of these patchesï¼Œ you take it and you project it into your
    embedding spaceã€‚which is the input to the transformer embedding space is just
    abstract space of let's sayã€‚768 dimensionsï¼Œ for exampleï¼Œ how do you embed itï¼Œ
    you just take the pixel values and put the linear projection projection layer
    on top them take all the pixelsã€‚ğŸ˜Šï¼ŒPlatten the vector matrix multiply into whatever
    size you want and use the same matrix for other patchesã€‚
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: å°±è¿™æ ·ã€‚æ¯ä¸ªè¡¥ä¸ï¼Œä½ å°†å…¶æå–å¹¶æŠ•å½±åˆ°ä½ çš„åµŒå…¥ç©ºé—´ä¸­ã€‚è¿™ä¸ªç©ºé—´æ˜¯å˜æ¢å™¨çš„è¾“å…¥ï¼ŒåµŒå…¥ç©ºé—´åªæ˜¯ä¸€ä¸ªæŠ½è±¡ç©ºé—´ï¼Œå‡è®¾æœ‰768ç»´åº¦ã€‚ä¾‹å¦‚ï¼Œä½ å¦‚ä½•è¿›è¡ŒåµŒå…¥ï¼Ÿä½ åªéœ€å–åƒç´ å€¼å¹¶åœ¨å…¶ä¸ŠåŠ ä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚ï¼Œè·å–æ‰€æœ‰åƒç´ ã€‚ğŸ˜Šï¼Œå°†å‘é‡å±•å¹³ï¼ŒçŸ©é˜µä¹˜æ³•è½¬æ¢æˆä½ æƒ³è¦çš„ä»»æ„å¤§å°ï¼Œå¹¶å¯¹å…¶ä»–è¡¥ä¸ä½¿ç”¨ç›¸åŒçš„çŸ©é˜µã€‚
- en: and here we just went the simplest ever with non overlapping patches and everything
    you can and the people later did go on and like sayã€‚heyï¼Œ this is almost a convolutionï¼Œ
    let's make proper convolutionï¼Œ let's make stack of them whatnotã€‚But this is our
    for natureï¼Œ this is just the simplest way to do it firstã€‚ğŸ˜Šã€‚Then we have these
    embedded patches and we treat them exactly literally like the were the tokens
    in language and then give them to exactly the bird transformer from language folks
    and just like in language we add this class token or I think a language is like
    end of sentence token or something and we add the positiondding embeddings to
    the tokens that can be learnedã€‚
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°±ä»¥æœ€ç®€å•çš„æ–¹å¼è¿›è¡Œäº†å¤„ç†ï¼Œä½¿ç”¨ä¸é‡å çš„è¡¥ä¸å’Œä½ èƒ½æƒ³åˆ°çš„ä¸€åˆ‡ï¼Œåæ¥äººä»¬å¼€å§‹è¯´ï¼šâ€œå˜¿ï¼Œè¿™å‡ ä¹å°±æ˜¯å·ç§¯ï¼Œè®©æˆ‘ä»¬åšä¸€ä¸ªçœŸæ­£çš„å·ç§¯ï¼Œå †å å®ƒä»¬ä¹‹ç±»çš„ã€‚â€ä½†è¿™æ˜¯æˆ‘ä»¬è‡ªç„¶çš„æ–¹å¼ï¼Œè¿™åªæ˜¯æœ€ç®€å•çš„åšæ³•ã€‚ğŸ˜Šæ¥ç€ï¼Œæˆ‘ä»¬æœ‰äº†è¿™äº›åµŒå…¥è¡¥ä¸ï¼Œæˆ‘ä»¬å°†å®ƒä»¬è§†ä½œè¯­è¨€ä¸­çš„ä»¤ç‰Œï¼Œç„¶åå°†å®ƒä»¬ä¼ é€’ç»™æ¥è‡ªè¯­è¨€é¢†åŸŸçš„é¸Ÿç±»å˜æ¢å™¨ï¼Œæ­£å¦‚åœ¨è¯­è¨€ä¸­ï¼Œæˆ‘ä»¬æ·»åŠ äº†è¿™ä¸ªç±»ä»¤ç‰Œï¼Œæˆ–è€…æˆ‘è®¤ä¸ºè¯­è¨€ä¸­æœ‰ä¸€ä¸ªå¥å­ç»“æŸä»¤ç‰Œä¹‹ç±»çš„ï¼Œå¹¶ä¸”æˆ‘ä»¬å°†ä½ç½®åµŒå…¥æ·»åŠ åˆ°å¯ä»¥å­¦ä¹ çš„ä»¤ç‰Œä¸­ã€‚
- en: And then we feed all of these to a transformer encoderã€‚which has MLP head which
    reads out this class token and then maps it to softm layer for classificationã€‚for
    exampleã€‚And that's it That is the vision transformerã€‚so it's literally you take
    bird transformerï¼Œ but instead of words or sentence tokens pe in pictures transform
    this into tokens and that's itã€‚
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å°†æ‰€æœ‰è¿™äº›æ•°æ®è¾“å…¥åˆ°ä¸€ä¸ªå˜æ¢å™¨ç¼–ç å™¨ä¸­ï¼Œç¼–ç å™¨æœ‰ä¸€ä¸ªMLPå¤´éƒ¨ï¼Œå®ƒè¯»å–è¿™ä¸ªç±»ä»¤ç‰Œï¼Œç„¶åå°†å…¶æ˜ å°„åˆ°softmaxå±‚è¿›è¡Œåˆ†ç±»ã€‚ä¾‹å¦‚ã€‚è¿™å°±æ˜¯è§†è§‰å˜æ¢å™¨ã€‚æ‰€ä»¥å­—é¢ä¸Šè®²ï¼Œä½ æ‹¿åˆ°çš„æ˜¯é¸Ÿç±»å˜æ¢å™¨ï¼Œä½†ä¸æ˜¯å•è¯æˆ–å¥å­ä»¤ç‰Œï¼Œè€Œæ˜¯å°†å›¾ç‰‡è½¬æ¢ä¸ºä»¤ç‰Œï¼Œä»…æ­¤è€Œå·²ã€‚
- en: ğŸ˜Šï¼ŒAnd then just same story is before scale everything up computeï¼Œ data setï¼Œ
    model sizeï¼Œ patienceã€‚everything and see what happensï¼Œ is this good or notã€‚That
    was the question and now we can see a plot here this is similar plot as before
    the gray area is actually what were all of the bit dots before and now the bubbles
    are vision transformers of different sizes and the bubble is kind of the size
    of the model I'll do it's a bit hard to say exactly and what you can see first
    is that with little data imagenet is the 1ã€‚
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œç„¶åå°±åƒä¹‹å‰ä¸€æ ·ï¼Œæ‰©å±•æ‰€æœ‰çš„è®¡ç®—ã€æ•°æ®é›†ã€æ¨¡å‹è§„æ¨¡ã€è€å¿ƒã€‚ä¸€åˆ‡éƒ½çœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œè¿™æ˜¯å¥½è¿˜æ˜¯ä¸å¥½ã€‚è¿™å°±æ˜¯é—®é¢˜ï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥çœ‹åˆ°è¿™é‡Œçš„å›¾è¡¨ï¼Œè¿™æ˜¯ä¸ä¹‹å‰ç±»ä¼¼çš„å›¾è¡¨ï¼Œç°è‰²åŒºåŸŸå®é™…ä¸Šæ˜¯ä¹‹å‰æ‰€æœ‰å°ç‚¹çš„ä½ç½®ï¼Œç°åœ¨æ°”æ³¡æ˜¯ä¸åŒå¤§å°çš„è§†è§‰å˜æ¢å™¨ï¼Œæ°”æ³¡çš„å¤§å°æœ‰ç‚¹éš¾ä»¥å‡†ç¡®è¯´å‡ºï¼Œä½ é¦–å…ˆå¯ä»¥çœ‹åˆ°çš„æ˜¯ï¼Œåœ¨æ•°æ®é‡è¾ƒå°‘çš„æƒ…å†µä¸‹ï¼Œimagenetæ˜¯ç¬¬ä¸€ã€‚
- en: 3 million images it works worse than resonatesã€‚ğŸ˜Šï¼ŒSo if we would not believe
    in this idea and just try this veryã€‚Okayï¼Œ this is a crap ideaã€‚and13 million images
    seems not that laterã€‚Then the10 times larger data states started laying in the
    same ballpark as a resonnet and when we go to much larger data with a much larger
    transformer then we actually start outperforming this reson and we outperform
    it just by littleã€‚but this resonnet was really hard to get and is extremely clumsy
    and slow and big so we were very excited by thisã€‚
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 300ä¸‡å¼ å›¾åƒçš„æƒ…å†µä¸‹ï¼Œå®ƒçš„è¡¨ç°ä¸å¦‚resnetã€‚ğŸ˜Šæ‰€ä»¥å¦‚æœæˆ‘ä»¬ä¸ç›¸ä¿¡è¿™ä¸ªæƒ³æ³•ï¼Œåªæ˜¯å°è¯•ä¸€ä¸‹ï¼šâ€œå¥½å§ï¼Œè¿™æ˜¯ä¸ªç³Ÿç³•çš„ä¸»æ„ã€‚â€è€Œ1300ä¸‡å¼ å›¾åƒä¼¼ä¹å¹¶æ²¡æœ‰é‚£ä¹ˆé¥è¿œã€‚ç„¶å10å€æ›´å¤§çš„æ•°æ®é›†å¼€å§‹ä¸resnetåœ¨åŒä¸€èŒƒå›´å†…ï¼Œå½“æˆ‘ä»¬è¿›å…¥æ›´å¤§çš„æ•°æ®é›†ï¼Œä½¿ç”¨æ›´å¤§çš„å˜æ¢å™¨æ—¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šå¼€å§‹è¶…è¶Šresnetï¼Œå¹¶ä¸”æˆ‘ä»¬åªæ˜¯ç•¥å¾®è¶…è¶Šäº†å®ƒã€‚ä½†è¿™ä¸ªresnetç¡®å®å¾ˆéš¾è·å¾—ï¼Œè€Œä¸”éå¸¸ç¬¨æ‹™ã€ç¼“æ…¢ä¸”åºå¤§ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯¹æ­¤æ„Ÿåˆ°éå¸¸å…´å¥‹ã€‚
- en: Now we did more controlled studies and everything and one of them is like using
    subset of this same data set and there's lots of curvebsã€‚but basically just look
    at the dark gray one and the light blue one these are roughly similarly fast and
    clumsy or easy to use or difficult to use a bit which is a resonnet variant and
    with the vision transformer and what you can see vision transformer when we have
    little in quotes little data is really bad compared to resonance but as we start
    having a lot of data actually it starts outperforming the resonnet and this is
    very promising because I think everything that looks huge and a lot and so on
    now in five or 10 yearsã€‚
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è¿›è¡Œäº†æ›´å¤šçš„æ§åˆ¶ç ”ç©¶ï¼Œå†…å®¹åŒ…æ‹¬ä½¿ç”¨è¿™ä¸ªç›¸åŒæ•°æ®é›†çš„å­é›†ï¼Œå¹¶ä¸”æœ‰å¾ˆå¤šæ›²çº¿ã€‚ä½†åŸºæœ¬ä¸Šï¼Œåªéœ€å…³æ³¨æ·±ç°è‰²å’Œæµ…è“è‰²è¿™ä¸¤ä¸ªæ›²çº¿ï¼Œå®ƒä»¬å¤§è‡´ä¸Šåœ¨é€Ÿåº¦å’Œä½¿ç”¨æ–¹ä¾¿ç¨‹åº¦ä¸Šæ˜¯ç›¸ä¼¼çš„ï¼Œç¨å¾®æœ‰ç‚¹ç¬¨æ‹™æˆ–å®¹æ˜“ä½¿ç”¨ï¼Œè¿™æ˜¯ä¸€ä¸ªresnetå˜ä½“ä¸è§†è§‰å˜æ¢å™¨çš„æ¯”è¾ƒã€‚ä½ å¯ä»¥çœ‹åˆ°ï¼Œå½“æˆ‘ä»¬æ•°æ®é‡è¾ƒå°‘æ—¶ï¼Œè§†è§‰å˜æ¢å™¨çœŸçš„å¾ˆå·®ï¼Œç›¸æ¯”ä¹‹ä¸‹ï¼Œresnetè¡¨ç°å¾—æ›´å¥½ï¼Œä½†éšç€æ•°æ®é‡çš„å¢åŠ ï¼Œå®é™…ä¸Šè§†è§‰å˜æ¢å™¨å¼€å§‹è¶…è¶Šresnetï¼Œè¿™éå¸¸æœ‰å‰æ™¯ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºç°åœ¨çœ‹èµ·æ¥å·¨å¤§ä¸”æ•°é‡åºå¤§çš„ä¸œè¥¿åœ¨äº”å¹´æˆ–åå¹´åä¼šå‘ç”Ÿæ”¹å˜ã€‚
- en: it's maybe regular like 10 years ago Ied this one seemed to be huge and massive
    amount of data normal anymoreã€‚ğŸ˜Šï¼ŒSo we should look to the future and this looks
    promising for the futureã€‚Then back to the same benchmarkï¼Œ that was another lit
    jumpã€‚Yeahï¼Œ we have some questionsã€‚Yepã€‚ğŸ˜Šã€‚There is also this section about meã€‚ğŸ˜Šï¼ŒSoï¼Œ
    it'sã€‚In that orderã€‚
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯èƒ½æ˜¯åå¹´å‰çš„å¸¸æ€ï¼Œè¿™ä¸ªæ•°æ®é‡çœ‹èµ·æ¥åºå¤§è€Œä¸”ä¸å†æ­£å¸¸ã€‚ğŸ˜Šæ‰€ä»¥æˆ‘ä»¬åº”è¯¥å±•æœ›æœªæ¥ï¼Œè¿™çœ‹èµ·æ¥å¯¹æœªæ¥å¾ˆæœ‰å‰æ™¯ã€‚ç„¶åå›åˆ°åŒä¸€åŸºå‡†ï¼Œè¿™æ˜¯å¦ä¸€ä¸ªå°è·³è·ƒã€‚æ˜¯çš„ï¼Œæˆ‘ä»¬æœ‰ä¸€äº›é—®é¢˜ã€‚å—¯ã€‚ğŸ˜Šè¿˜æœ‰è¿™ä¸ªå…³äºæˆ‘çš„éƒ¨åˆ†ã€‚ğŸ˜Šæ‰€ä»¥ï¼Œè¿™æ ·çš„é¡ºåºã€‚
- en: if you want to unmute yourself and ask the questionsã€‚Sureï¼Œ yeahã€‚and I think
    Deval already answered part of the questionã€‚but I was wondering in the input to
    these transform when you're chunking up the image into little puzzle pieces and
    then fighting themã€‚does the order of feeding these patches in matterï¼Œ like if
    you switch the orderã€‚
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è§£é™¤é™éŸ³å¹¶æé—®ï¼Œå¯ä»¥çš„ï¼Œå¥½çš„ã€‚æˆ‘è®¤ä¸ºDevalå·²ç»å›ç­”äº†éƒ¨åˆ†é—®é¢˜ã€‚ä½†æˆ‘æƒ³çŸ¥é“ï¼Œåœ¨è¿™äº›è½¬æ¢çš„è¾“å…¥ä¸­ï¼Œå½“ä½ å°†å›¾åƒåˆ‡å‰²æˆå°æ‹¼å›¾å—å¹¶å°†å®ƒä»¬ç»„åˆæ—¶ï¼Œè¾“å…¥è¿™äº›è¡¥ä¸çš„é¡ºåºæ˜¯å¦é‡è¦ï¼Œå¦‚æœä½ æ”¹å˜é¡ºåºã€‚
- en: does the prediction maybe changeï¼ŸYeahï¼Œ that's a good questionã€‚And I actually
    have a slide on something like thisï¼Œ but not exactlyã€‚let me jump thereã€‚So first
    of allï¼Œ if the order is consistent during trainingï¼Œ rightã€‚and you don't shuffle
    the order again for each new imageã€‚
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: é¢„æµ‹å¯èƒ½ä¼šæ”¹å˜å—ï¼Ÿæ˜¯çš„ï¼Œè¿™æ˜¯ä¸ªå¥½é—®é¢˜ã€‚å®é™…ä¸Šï¼Œæˆ‘æœ‰ä¸€å¼ å¹»ç¯ç‰‡è°ˆè®ºç±»ä¼¼çš„å†…å®¹ï¼Œä½†ä¸æ˜¯å®Œå…¨ç›¸åŒçš„ã€‚è®©æˆ‘è·³è¿‡å»ã€‚é¦–å…ˆï¼Œå¦‚æœè®­ç»ƒæœŸé—´é¡ºåºæ˜¯ä¸€è‡´çš„ï¼Œå¯¹å§ï¼Ÿå¹¶ä¸”å¯¹äºæ¯ä¸ªæ–°å›¾åƒï¼Œä½ ä¸å†æ‰“ä¹±é¡ºåºã€‚
- en: then it's literally the exact same you get the same curve saying everything
    because we don't encode the order anywayã€‚If you start randomizing the order all
    the time during trainingã€‚then performance gets quite a lot worseã€‚and let me show
    you why is the slide was on my plan to present anywaysã€‚then if you ask about let's
    jump hereï¼Œ these are this is a visualization of the position embeddingsã€‚
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¹ˆï¼Œè¿™å®é™…ä¸Šæ˜¯å®Œå…¨ç›¸åŒçš„ï¼Œä½ å¾—åˆ°ç›¸åŒçš„æ›²çº¿ï¼Œæ‰€æœ‰çš„è¯´æ³•ï¼Œå› ä¸ºæˆ‘ä»¬åæ­£ä¸ç¼–ç é¡ºåºã€‚å¦‚æœä½ åœ¨è®­ç»ƒæœŸé—´æ€»æ˜¯å¼€å§‹éšæœºåŒ–é¡ºåºï¼Œé‚£ä¹ˆæ€§èƒ½ä¼šå¤§å¹…ä¸‹é™ã€‚è®©æˆ‘ç»™ä½ å±•ç¤ºä¸ºä»€ä¹ˆï¼Œå¹»ç¯ç‰‡æ˜¯æˆ‘åŸæœ¬è®¡åˆ’å±•ç¤ºçš„ã€‚å¦‚æœä½ é—®æˆ‘ä»¬è·³åˆ°è¿™é‡Œï¼Œè¿™äº›æ˜¯ä½ç½®åµŒå…¥çš„å¯è§†åŒ–ã€‚
- en: ğŸ˜Šï¼ŒWhat does it mean so in this case we had 14 by 14 patches that we cut the
    image inã€‚so it means we have also 14 by 14 position embeddings although we just
    see them as one round sequence of what is it 00ã€‚50 somethingï¼Œ or I don't knowï¼Œ140
    somethingã€‚And now each of these pictures shows the position embedding which corresponds
    to this location how similar is it to all the other position embedding so let's
    look at this one for exampleã€‚yellow means a perfectly similar like exactly the
    same and blue means opposite in terms of coine similarity so this position embedding
    is most similar to itself which is the pixel here and then the neighboring pixels
    is how similar is it to the position embeddings that correspond originally to
    the neighboring patch and we do see a very clear pattern that each position embedding
    is very similar to the embedding from its surrounding patches and we didn't implement
    any of this right we just had this position embeddings at randomly initialized
    variables and they are learned as freely as the rest of the parameters of the
    model but they learn to recover this notion of what are my neighbor patches even
    though we don't give this information anywhere at any time besides the raw image
    data in the task to please classã€‚
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šè¿™æ„å‘³ç€ä»€ä¹ˆï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†å›¾åƒåˆ‡å‰²æˆ14ä¹˜14çš„è¡¥ä¸ã€‚æ‰€ä»¥è¿™ä¹Ÿæ„å‘³ç€æˆ‘ä»¬æœ‰14ä¹˜14çš„ä½ç½®åµŒå…¥ï¼Œå°½ç®¡æˆ‘ä»¬åªå°†å®ƒä»¬è§†ä¸ºä¸€ä¸ªè¿ç»­çš„åºåˆ—ï¼Œæ˜¯ä»€ä¹ˆï¼Œ00ï¼Œ50å¤šï¼Œæˆ–è€…æˆ‘ä¸çŸ¥é“ï¼Œ140å¤šã€‚ç°åœ¨ï¼Œè¿™äº›å›¾ç‰‡å±•ç¤ºäº†ä¸è¯¥ä½ç½®å¯¹åº”çš„åµŒå…¥ï¼Œå®ƒä¸æ‰€æœ‰å…¶ä»–ä½ç½®åµŒå…¥çš„ç›¸ä¼¼åº¦ã€‚æ¯”å¦‚çœ‹è¿™ä¸ªï¼Œé»„è‰²è¡¨ç¤ºå®Œå…¨ç›¸ä¼¼ï¼Œæ­£å¥½ç›¸åŒï¼Œè“è‰²è¡¨ç¤ºåœ¨ä½™å¼¦ç›¸ä¼¼åº¦æ–¹é¢ç›¸åã€‚å› æ­¤ï¼Œè¿™ä¸ªä½ç½®åµŒå…¥ä¸è‡ªèº«æœ€ç›¸ä¼¼ï¼Œå°±æ˜¯è¿™é‡Œçš„åƒç´ ï¼Œç„¶åé‚»è¿‘åƒç´ ä¸æœ€åˆå¯¹åº”äºé‚»è¿‘è¡¥ä¸çš„ä½ç½®åµŒå…¥çš„ç›¸ä¼¼åº¦ã€‚æˆ‘ä»¬ç¡®å®çœ‹åˆ°ä¸€ä¸ªéå¸¸æ˜æ˜¾çš„æ¨¡å¼ï¼Œæ¯ä¸ªä½ç½®åµŒå…¥ä¸å…¶å‘¨å›´è¡¥ä¸çš„åµŒå…¥éå¸¸ç›¸ä¼¼ï¼Œè€Œæˆ‘ä»¬å¹¶æ²¡æœ‰å®ç°ä»»ä½•è¿™äº›ã€‚æˆ‘ä»¬åªæ˜¯å°†è¿™äº›ä½ç½®åµŒå…¥éšæœºåˆå§‹åŒ–ä¸ºå˜é‡ï¼Œå¹¶ä¸”å®ƒä»¬åƒæ¨¡å‹çš„å…¶ä½™å‚æ•°ä¸€æ ·è‡ªç”±å­¦ä¹ ï¼Œä½†å®ƒä»¬å­¦ä¼šäº†æ¢å¤é‚»è¿‘è¡¥ä¸æ˜¯ä»€ä¹ˆçš„æ¦‚å¿µï¼Œå°½ç®¡æˆ‘ä»¬åœ¨ä»»ä½•æ—¶å€™éƒ½æ²¡æœ‰æä¾›è¿™äº›ä¿¡æ¯ï¼Œåªæ˜¯åœ¨åŸå§‹å›¾åƒæ•°æ®ä¸­ç»™å‡ºäº†ä»»åŠ¡å»åˆ†ç±»ã€‚
- en: ã§ã™ã¾ã€‚è¯¶ã€‚So that's pretty cool I thinkï¼Œ but it also means that if you take the
    trained model now and give in patches in a completely differently shuffled orderã€‚it's
    going to perform poorly because these learned position and beings not make sense
    anymoreã€‚ğŸ˜Šã€‚
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™çœŸé…·ï¼Œæˆ‘è®¤ä¸ºï¼Œä½†è¿™ä¹Ÿæ„å‘³ç€ï¼Œå¦‚æœä½ ç°åœ¨æ‹¿åˆ°è®­ç»ƒå¥½çš„æ¨¡å‹å¹¶ä»¥å®Œå…¨ä¸åŒçš„éšæœºé¡ºåºè¾“å…¥è¡¥ä¸ï¼Œå®ƒçš„è¡¨ç°ä¼šå¾ˆç³Ÿç³•ï¼Œå› ä¸ºè¿™äº›å­¦ä¹ åˆ°çš„ä½ç½®åµŒå…¥ä¸å†æœ‰æ„ä¹‰ã€‚ğŸ˜Š
- en: We did try also to implement like position embeddings which encode the location
    as hard coded by us and other fancy position embeddings like relative onesã€‚but
    basically none of that really outperformed is freely learned and then the freely
    learned is simple you just random in it let it learn as part of SGD and that's
    it and so we go with that and just doing thatã€‚
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¹Ÿå°è¯•å®ç°åƒä½ç½®åµŒå…¥è¿™æ ·çš„ä¸œè¥¿ï¼Œå®ƒé€šè¿‡æˆ‘ä»¬ç¡¬ç¼–ç çš„ä½ç½®æ¥ç¼–ç ä½ç½®ï¼Œè¿˜æœ‰å…¶ä»–èŠ±å“¨çš„ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚ä½†åŸºæœ¬ä¸Šæ²¡æœ‰ä»»ä½•ä¸€ç§çœŸçš„ä¼˜äºè‡ªç”±å­¦ä¹ çš„ï¼Œè€Œè‡ªç”±å­¦ä¹ å¾ˆç®€å•ï¼Œä½ åªéœ€éšæœºåˆå§‹åŒ–ï¼Œè®©å®ƒä½œä¸ºSGDçš„ä¸€éƒ¨åˆ†å­¦ä¹ ï¼Œå°±æ˜¯è¿™æ ·ï¼Œæ‰€ä»¥æˆ‘ä»¬å°±è¿™æ ·åšã€‚
- en: we have one more question fromã€‚Heyï¼Œ yeahï¼Œ I was wondering to read it yeahï¼Œ this
    slideã€‚I think something that's really interesting is we're talking about scaling
    up the data and scaling up the model of the thought as wellã€‚But it seems like
    you're reaching an passing show right when you keep doing this thingã€‚So I'm curious
    if you have any thoughts on that like is that or these points just look like that
    or is there kind of a best you can sort of do where we keep yeah whatever pretrain
    of like the data or the parameters you're actually not going to get Yeah I have
    another slide but much further in the talk about that where I would like to not
    jump on it if you don't mind and then maybe in 10 15 minutes we will be thereã€‚
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªé—®é¢˜ã€‚å˜¿ï¼Œæ²¡é”™ï¼Œæˆ‘åœ¨æƒ³è¦è¯»è¿™ä¸ªï¼Œå¯¹ï¼Œå°±æ˜¯è¿™ä¸€é¡µã€‚æˆ‘è§‰å¾—éå¸¸æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬åœ¨è°ˆè®ºæ•°æ®çš„æ‰©å±•å’Œæ€ç»´æ¨¡å‹çš„æ‰©å±•ã€‚ä½†ä¼¼ä¹ä½ åœ¨ç»§ç»­è¿™æ ·åšæ—¶ï¼Œè¾¾åˆ°äº†ä¸€ä¸ªç“¶é¢ˆã€‚æ‰€ä»¥æˆ‘å¾ˆå¥½å¥‡ä½ å¯¹æ­¤æ˜¯å¦æœ‰ä»»ä½•æƒ³æ³•ï¼Œæ˜¯å¦è¿™äº›ç‚¹çœ‹èµ·æ¥å°±æ˜¯è¿™æ ·ï¼Œæˆ–è€…è¯´æœ‰ä»€ä¹ˆæœ€ä½³æ–¹å¼å¯ä»¥åšåˆ°ï¼Œè®©æˆ‘ä»¬ç»§ç»­ï¼Œæ— è®ºæ˜¯å…³äºæ•°æ®çš„é¢„è®­ç»ƒè¿˜æ˜¯ä½ å®é™…ä¸Šæ— æ³•è·å¾—çš„å‚æ•°ã€‚æ˜¯çš„ï¼Œæˆ‘è¿˜æœ‰å¦ä¸€å¼ å¹»ç¯ç‰‡ï¼Œä½†åœ¨æ¼”è®²çš„åé¢æˆ‘æƒ³å…ˆä¸è·³åˆ°é‚£å„¿ï¼Œå¦‚æœä½ ä¸ä»‹æ„ï¼Œç„¶åä¹Ÿè®¸åœ¨10åˆ°15åˆ†é’Ÿå†…æˆ‘ä»¬ä¼šåˆ°è¾¾é‚£å„¿ã€‚
- en: Sounds greatï¼Œ thanksã€‚Yeahï¼Œ but maybe to be a bit optimisticã€‚it does seem like
    the transformers have a better slope here in the end and the resonates plateau
    earlierã€‚ğŸ˜Šï¼ŒSorryï¼Œ Lucasï¼Œ I didn't mean did not mean to interrupt Are there any
    questions before we proceed Yeahã€‚can I ask my question real quick Sorry about
    that So what I'm curious to know is how does this VIT compare to if you equi a
    convnetã€‚
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å¬èµ·æ¥ä¸é”™ï¼Œè°¢è°¢ã€‚æ˜¯çš„ï¼Œä½†ä¹Ÿè®¸æœ‰ç‚¹ä¹è§‚ã€‚æœ€ç»ˆä¼¼ä¹å˜å‹å™¨åœ¨è¿™é‡Œæœ‰æ›´å¥½çš„æ–œç‡ï¼Œè€Œresnetè¾ƒæ—©è¾¾åˆ°å¹³å°æœŸã€‚ğŸ˜Š å¯¹ä¸èµ·ï¼Œå¢å¡æ–¯ï¼Œæˆ‘ä¸æ˜¯æ•…æ„æ‰“æ–­ä½ ï¼Œåœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰è¿˜æœ‰å…¶ä»–é—®é¢˜å—ï¼Ÿæ˜¯çš„ï¼Œæˆ‘å¯ä»¥å¿«é€Ÿé—®ä¸€ä¸‹æˆ‘çš„é—®é¢˜å—ï¼Ÿå¯¹ä¸èµ·ï¼Œæˆ‘å¥½å¥‡çš„æ˜¯ï¼Œè¿™ä¸ªVITå’Œä¸€ä¸ªå·ç§¯ç½‘ç»œç›¸æ¯”æ€ä¹ˆæ ·ã€‚
- en: So for exampleï¼Œ Resnetã€‚With an attention mechanismã€‚Like how much of this is
    due to the structure of a transformer and the particular way it operates versus
    just the benefit of attention a vanilla conf does not have access toã€‚Yeahï¼Œ so
    this is manyï¼Œ this has been tried many times beforeã€‚And the first time that I
    know of was actually from I mis pronounce's nameã€‚
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯”å¦‚è¯´ï¼ŒResnetã€‚ä¸ä¸€ä¸ªæ³¨æ„åŠ›æœºåˆ¶ã€‚è¿™ä¸ªç»“æ„çš„å˜å‹å™¨å’Œå®ƒç‰¹å®šçš„è¿ä½œæ–¹å¼æœ‰å¤šå°‘æ˜¯ç”±æ­¤é€ æˆçš„ï¼Œè¿˜æ˜¯ä»…ä»…æ˜¯æ™®é€šå·ç§¯ç½‘ç»œæ²¡æœ‰æ¥è§¦åˆ°çš„æ³¨æ„åŠ›çš„å¥½å¤„ï¼Ÿæ˜¯çš„ï¼Œè¿™ä¸ªé—®é¢˜å·²ç»è¢«å°è¯•è¿‡å¾ˆå¤šæ¬¡äº†ã€‚æˆ‘çŸ¥é“çš„ç¬¬ä¸€æ¬¡å…¶å®æ˜¯æ¥è‡ªäºæˆ‘é”™å‘éŸ³çš„åå­—ã€‚
- en: but I mean there the inventor of Resnet and some of his colleaguesã€‚they called
    it non blocker networksã€‚This was way I think even before the transform paperã€‚if
    I remember correctly and they basically inserted as blocks at various locations
    in the resonnet and then they showed improvementã€‚but it was likeã€‚Tinny improvements
    thatã€‚ It was a cool block and a simple paperã€‚
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘æŒ‡çš„æ˜¯Resnetçš„å‘æ˜è€…å’Œä»–çš„ä¸€äº›åŒäº‹ã€‚ä»–ä»¬ç§°ä¹‹ä¸ºéé˜»å¡ç½‘ç»œã€‚æˆ‘è®¤ä¸ºè¿™æ˜¯åœ¨å˜æ¢å™¨è®ºæ–‡ä¹‹å‰çš„äº‹æƒ…ï¼Œå¦‚æœæˆ‘æ²¡è®°é”™ï¼Œä»–ä»¬åŸºæœ¬ä¸Šåœ¨resnetçš„å„ä¸ªä½ç½®æ’å…¥äº†è¿™äº›å—ï¼Œç„¶åå±•ç¤ºäº†æ”¹è¿›ã€‚ä½†é‚£åªæ˜¯å¾®å°çš„æ”¹è¿›ã€‚é‚£æ˜¯ä¸ªå¾ˆé…·çš„å—å’Œä¸€ç¯‡ç®€å•çš„è®ºæ–‡ã€‚
- en: but it was not really worth itã€‚And it people usually place the attentionã€‚I you
    can imagine if you place the attention just on the pixels and don't do this patch
    cuttingã€‚this is way too expensive computationï¼Œ Rightï¼Œ rightï¼Œ If you have 2 to
    4 by 2 to 4 pixelsã€‚that's likeã€‚Yeah I cannot do this in my headï¼Œ I don't know
    40000 or so maybe pixels attending to 40ã€‚
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†è¿™å¹¶ä¸å€¼å¾—ã€‚è€Œäººä»¬é€šå¸¸å°†æ³¨æ„åŠ›æ”¾ç½®åœ¨åƒç´ ä¸Šï¼Œä½ å¯ä»¥æƒ³è±¡ï¼Œå¦‚æœä½ ä»…ä»…åœ¨åƒç´ ä¸Šæ”¾ç½®æ³¨æ„åŠ›ï¼Œè€Œä¸è¿›è¡Œè¿™ç§åˆ‡ç‰‡å¤„ç†ï¼Œè¿™æ ·è®¡ç®—æˆæœ¬å¤ªé«˜äº†ï¼Œå¯¹å§ï¼Œå¦‚æœä½ æœ‰2åˆ°4ä¹˜2åˆ°4ä¸ªåƒç´ ï¼Œé‚£å°±æ˜¯ã€‚æ˜¯çš„ï¼Œæˆ‘æ— æ³•åœ¨è„‘æµ·ä¸­è®¡ç®—ï¼Œæˆ‘ä¸çŸ¥é“å¤§çº¦æ˜¯40000ä¸ªåƒç´ å¯¹ç€40ã€‚
- en: 000 others that doesn't work so people just do it in the very high and very
    final layers of the resnet like where it's maybe 7 by7 and then they add a bit
    of sprinkle a bit of attention thereã€‚but then you don't really get much benefit
    of skating because it's essentially still a resnetã€‚
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 000ä¸ªå…¶ä»–çš„åŠæ³•ä¸èµ·ä½œç”¨ï¼Œæ‰€ä»¥äººä»¬åªåœ¨resnetçš„éå¸¸é«˜å’Œæœ€ç»ˆå±‚ä¸­è¿™æ ·åšï¼Œæ¯”å¦‚åœ¨7x7çš„åœ°æ–¹ï¼Œç„¶åä»–ä»¬åœ¨é‚£å„¿ç¨å¾®æ·»åŠ ä¸€ç‚¹æ³¨æ„åŠ›ã€‚ä½†è¿™æ ·ä½ å®é™…ä¸Šå¹¶æ²¡æœ‰è·å¾—å¤ªå¤šå¥½å¤„ï¼Œå› ä¸ºå®ƒæœ¬è´¨ä¸Šä»ç„¶æ˜¯ä¸€ä¸ªresnetã€‚
- en: And there is in resonancesatesï¼Œ there is this block called Sic S that has been
    getting really or has gotten really popular and improves the resnet quite a bitã€‚and
    that is also kind of a form of attentionï¼Œ but like nicely tailored to imagesã€‚I'm
    not doing it's arguableã€‚But yeahï¼Œ it has been tried many times beforeã€‚but it just
    it doesn't show or it hasn't been shown to have this scaling benefit as much as
    theã€‚
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ResNetä¸­ï¼Œæœ‰ä¸€ä¸ªè¢«ç§°ä¸ºSic Sçš„æ¨¡å—ï¼Œè¿™ä¸ªæ¨¡å—å˜å¾—éå¸¸æµè¡Œï¼Œå¹¶ä¸”åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ”¹å–„äº†ResNetã€‚è¿™ä¹Ÿæ˜¯ä¸€ç§æ³¨æ„åŠ›çš„å½¢å¼ï¼Œä½†å¾ˆå¥½åœ°é’ˆå¯¹å›¾åƒè¿›è¡Œäº†è°ƒæ•´ã€‚æˆ‘å¹¶ä¸æ˜¯åœ¨äº‰è®ºè¿™ä¸ªã€‚ä½†æ˜¯ï¼Œæ˜¯çš„ï¼Œå®ƒå·²ç»è¢«å¤šæ¬¡å°è¯•è¿‡ã€‚ä½†å®ƒå¹¶æ²¡æœ‰æ˜¾ç¤ºå‡ºåƒâ€¦â€¦é‚£æ ·çš„ç¼©æ”¾å¥½å¤„ã€‚
- en: So I think I'm missing something critical hereï¼Œ which is you just contract or
    it's computationally difficultã€‚But eventuallyallyï¼Œ you're at a low level in the
    resnetã€‚But why is it any different than doing an attention layer in division vision
    transformerï¼Ÿ
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘è§‰å¾—æˆ‘åœ¨è¿™é‡Œé—æ¼äº†ä¸€äº›å…³é”®çš„ä¸œè¥¿ï¼Œå°±æ˜¯ä½ åªæ˜¯æ”¶ç¼©ï¼Œæˆ–è€…è¯´åœ¨è®¡ç®—ä¸Šæ˜¯å›°éš¾çš„ã€‚ä½†æœ€ç»ˆï¼Œä½ å¤„äºResNetçš„ä½å±‚ã€‚ä½†æ˜¯ä¸ºä»€ä¹ˆè¿™ä¸åœ¨è§†è§‰å˜æ¢å™¨ä¸­åšä¸€ä¸ªæ³¨æ„åŠ›å±‚æœ‰ä»€ä¹ˆä¸åŒï¼Ÿ
- en: Because we cut the patches firstï¼Œ so we have maybe 14 by 14 patchesï¼Œ which is
    not that muchã€‚Okayã€‚But I'm confusedï¼Œ likeã€‚You could imagine not at a high level
    and not at a high layer in the resonnetã€‚but at a relatively true layer after you've
    applied like one or two convolutional filtersã€‚Convolutional layersï¼Œ excuse meï¼Œ
    then you have something size to the patchesã€‚
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºæˆ‘ä»¬å…ˆåˆ‡å‰²äº†è¡¥ä¸ï¼Œæ‰€ä»¥å¯èƒ½æœ‰14ä¹˜14çš„è¡¥ä¸ï¼Œè¿™å¹¶ä¸æ˜¯å¾ˆå¤šã€‚å¥½å§ã€‚ä½†æˆ‘æœ‰ç‚¹å›°æƒ‘ï¼Œä½ å¯ä»¥æƒ³è±¡åœ¨ä¸é«˜çš„å±‚æ¬¡å’Œä¸é«˜çš„å±‚æ¬¡çš„ResNetä¸­ï¼Œè€Œæ˜¯åœ¨ä½ åº”ç”¨äº†ä¸€ä¸¤ä¸ªå·ç§¯æ»¤æ³¢å™¨åç›¸å¯¹çœŸå®çš„ä¸€å±‚ã€‚å·ç§¯å±‚ï¼Œæ‰“æ‰°ä¸€ä¸‹ï¼Œé‚£ä¹ˆä½ ä¼šå¾—åˆ°ä¸€äº›ä¸è¡¥ä¸å¤§å°ç›¸å…³çš„ä¸œè¥¿ã€‚
- en: That's still 50 by 50 of the early layersï¼Œ and that's but 50 by 50 is significantly
    less than not like 400 by 400 or 100ã€‚But it's still 2ï¼Œ500 tokens attending to
    2500 tokensï¼Œ which yeah I meanï¼Œ it's a lotã€‚but it's not comparableã€‚I don't knowã€‚Okayï¼Œ
    cool Thank youã€‚ Yeahï¼Œ I meanï¼Œ it could be triedã€‚ Okayã€‚maybe another answer to
    your question is then we're slowly getting to this my next slide after the set
    of questions where we do try something almost like what you said have a very small
    part of the resnet and then stick transformer on top of it but like the full transformer
    encoder on top of it and not just sprinkle a few attention layers and then continue
    at columns and so on and this is this process and we call them hybrid but that's
    almost literally what you said actually like a few early layers from the resnet
    and with different varying amount and then stick the whole transform encoderã€‚
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»ç„¶æ˜¯æ—©æœŸå±‚çš„50ä¹˜50ï¼Œè€Œ50ä¹˜50æ˜¾è‘—å°‘äº400ä¹˜400æˆ–100ã€‚å¯æ˜¯ä»ç„¶æ˜¯2500ä¸ªtokensåœ¨å…³æ³¨2500ä¸ªtokensï¼Œæ˜¯çš„ï¼Œæˆ‘çš„æ„æ€æ˜¯ï¼Œè¿™å¾ˆå¤šï¼Œä½†ä¸èƒ½ç›¸æå¹¶è®ºã€‚æˆ‘ä¸çŸ¥é“ã€‚å¥½å§ï¼Œé…·ï¼Œè°¢è°¢ä½ ã€‚æ˜¯çš„ï¼Œæˆ‘çš„æ„æ€æ˜¯ï¼Œå¯ä»¥å°è¯•ã€‚å¥½å§ï¼Œä¹Ÿè®¸ä½ é—®é¢˜çš„å¦ä¸€ä¸ªç­”æ¡ˆæ˜¯ï¼Œæˆ‘ä»¬æ…¢æ…¢æ¥è¿‘æˆ‘ä¸‹ä¸€å¼ å¹»ç¯ç‰‡ï¼Œåœ¨ä¸€ç³»åˆ—é—®é¢˜ä¹‹åï¼Œæˆ‘ä»¬ç¡®å®å°è¯•äº†ä¸€äº›å‡ ä¹å°±åƒä½ æ‰€è¯´çš„ï¼Œä½¿ç”¨éå¸¸å°çš„ä¸€éƒ¨åˆ†ResNetï¼Œç„¶ååœ¨å…¶ä¸Šæ”¾ç½®å˜æ¢å™¨ï¼Œä½†æ˜¯çœŸæ­£çš„å…¨å˜æ¢å™¨ç¼–ç å™¨ï¼Œè€Œä¸ä»…ä»…æ˜¯æ’’ä¸Šä¸€äº›æ³¨æ„åŠ›å±‚ï¼Œç„¶åç»§ç»­åˆ—ç­‰ç­‰ï¼Œè¿™æ˜¯è¿™ä¸ªè¿‡ç¨‹ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºæ··åˆï¼Œä½†è¿™å‡ ä¹å°±æ˜¯ä½ æ‰€è¯´çš„ï¼Œå®é™…ä¸Šæ˜¯æ¥è‡ªResNetçš„ä¸€äº›æ—©æœŸå±‚ï¼Œæ•°é‡ä¸åŒï¼Œç„¶åè´´ä¸Šæ•´ä¸ªå˜æ¢å™¨ç¼–ç å™¨ã€‚
- en: ğŸ˜Šï¼ŒAnd thisã€‚Seem to work well tooï¼Œ especially for the when you exactly in this
    case is amount of compute so for the little compute it seems to work wellã€‚but
    then the scaling behavior of the pure renet is a little better so we focused on
    that I think we later tried also hybrid further to the right and it was a bit
    lower but it was after the paper so it's not on this plot which I just put out
    of the paper but you can already see the trend hereã€‚
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè€Œä¸”è¿™ä¸ªã€‚ä¼¼ä¹ä¹Ÿèƒ½å¾ˆå¥½åœ°å·¥ä½œï¼Œå°¤å…¶æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®¡ç®—é‡ä¸å¤§ï¼Œæ‰€ä»¥å¯¹äºå°çš„è®¡ç®—æ¥è¯´ï¼Œå®ƒä¼¼ä¹è¡¨ç°ä¸é”™ã€‚ä½†æ˜¯çº¯ResNetçš„ç¼©æ”¾è¡Œä¸ºç¨å¾®å¥½ä¸€äº›ï¼Œæ‰€ä»¥æˆ‘ä»¬å…³æ³¨äº†è¿™ä¸€ç‚¹ã€‚æˆ‘è®¤ä¸ºæˆ‘ä»¬åæ¥ä¹Ÿå°è¯•äº†æ··åˆè¿›ä¸€æ­¥å³ç§»ï¼Œç»“æœç¨å¾®ä½ä¸€äº›ï¼Œä½†é‚£æ˜¯åœ¨è®ºæ–‡ä¹‹åï¼Œæ‰€ä»¥ä¸åœ¨è¿™ä¸ªå›¾ä¸Šï¼Œæˆ‘åˆšä»è®ºæ–‡ä¸­æ”¾å‡ºæ¥ï¼Œä½†ä½ å¯ä»¥åœ¨è¿™é‡Œçœ‹åˆ°è¶‹åŠ¿ã€‚
- en: å—¯ã€‚Yeahï¼Œ so if you don't scale all the way upï¼Œ then this is a totally reasonable
    thing to do have a little bit of resnet and then the encoder from Transerã€‚Do you
    wantan to ask your questionã€‚Yeahï¼Œ I was just wondering about basicallyã€‚there's
    like like a short section of the paper about like fine tuning and like higher
    resolutionã€‚And in that caseï¼Œ rightï¼Œ like the pretrain like position and embeddingï¼Œ
    Sorry or like skewed rightã€‚
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å—¯ã€‚æ˜¯çš„ï¼Œå¦‚æœä½ ä¸å®Œå…¨ç¼©æ”¾åˆ°é¡¶ç«¯ï¼Œé‚£ä¹ˆè¿™æ ·åšä¸€ç‚¹ResNetç„¶åå†åŠ ä¸Šå˜æ¢å™¨çš„ç¼–ç å™¨æ˜¯å®Œå…¨åˆç†çš„ã€‚ä½ æƒ³é—®ä½ çš„é—®é¢˜å—ï¼Ÿæ˜¯çš„ï¼Œæˆ‘åªæ˜¯æƒ³é—®ä¸€ä¸‹ï¼ŒåŸºæœ¬ä¸Šï¼Œè®ºæ–‡ä¸­æœ‰ä¸€ä¸ªå…³äºå¾®è°ƒå’Œé«˜åˆ†è¾¨ç‡çš„ç®€çŸ­éƒ¨åˆ†ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé¢„è®­ç»ƒçš„ä½ç½®å’ŒåµŒå…¥ï¼ŒæŠ±æ­‰ï¼Œæˆ–è€…è¯´æ˜¯åæ–œçš„ï¼Œå¯¹å§ã€‚
- en: And then it's basically says that you guys are like interpoolatingã€‚Can you like
    talk on that like a little bitï¼Œ like how do you interpolate what's going onã€‚Yeahã€‚actuallyï¼Œ
    when when I check the slides earlier todayï¼Œ I was likeï¼Œ ohã€‚it would be good to
    have a slide on that And we don't have an nice visualization in the paper eitherã€‚
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶ååŸºæœ¬ä¸Šå®ƒè¯´ä½ ä»¬å°±åƒæ˜¯åœ¨æ’å€¼ã€‚ä½ èƒ½è°ˆè°ˆè¿™ä¸ªå—ï¼Œæ¯”å¦‚ä½ æ˜¯æ€ä¹ˆæ’å€¼å‘ç”Ÿçš„äº‹æƒ…ã€‚å¯¹ï¼Œå®é™…ä¸Šï¼Œå½“æˆ‘ä»Šå¤©æ—©äº›æ—¶å€™æŸ¥çœ‹å¹»ç¯ç‰‡æ—¶ï¼Œæˆ‘å°±æƒ³ï¼Œå“¦ï¼Œæœ€å¥½æœ‰ä¸€å¼ å…³äºè¿™ä¸ªçš„å¹»ç¯ç‰‡ï¼Œè€Œä¸”æˆ‘ä»¬åœ¨è®ºæ–‡ä¸­ä¹Ÿæ²¡æœ‰å¥½çš„å¯è§†åŒ–ã€‚
- en: because it's a bit difficult to explainï¼Œ but this is the best startingã€‚Poin
    it haveã€‚So if you want to increase the resolution of the image means and you keep
    the patch size fixedã€‚it means you have more patches suddenly rightï¼Œ And then as
    you sayã€‚the fashion embeddings like how what do you even use as a position embeddingsã€‚
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸ºè¿™æœ‰ç‚¹éš¾ä»¥è§£é‡Šï¼Œä½†è¿™æ˜¯æœ€å¥½çš„èµ·ç‚¹ã€‚æ‰€ä»¥å¦‚æœä½ æƒ³æé«˜å›¾åƒçš„åˆ†è¾¨ç‡ï¼Œå¹¶ä¸”ä¿æŒè¡¥ä¸å¤§å°å›ºå®šã€‚è¿™æ„å‘³ç€ä½ çªç„¶æœ‰æ›´å¤šè¡¥ä¸ï¼Œå¯¹å§ï¼Ÿç„¶åæ­£å¦‚ä½ æ‰€è¯´ï¼Œæ—¶å°šåµŒå…¥ï¼Œæ¯”å¦‚ä½ åˆ°åº•ç”¨ä»€ä¹ˆä½œä¸ºä½ç½®åµŒå…¥ã€‚
- en: right and basically you can see here that we see that they learn a very regular
    structureï¼Œ rightã€‚we don't really know what is the structure of these position
    embeddings that Iã€‚We just see the similarity to each other and that it is very
    regularã€‚And so this gave us the intuition that we may be able to just take themã€‚Kind
    of imaging these boxesã€‚
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ï¼ŒåŸºæœ¬ä¸Šä½ å¯ä»¥çœ‹åˆ°è¿™é‡Œæˆ‘ä»¬çœ‹åˆ°å®ƒä»¬å­¦ä¹ äº†ä¸€ä¸ªéå¸¸è§„å¾‹çš„ç»“æ„ï¼Œå¯¹å§ã€‚æˆ‘ä»¬å¹¶ä¸çŸ¥é“è¿™äº›ä½ç½®åµŒå…¥çš„ç»“æ„æ˜¯ä»€ä¹ˆã€‚æˆ‘ä»¬åªçœ‹åˆ°å½¼æ­¤ä¹‹é—´çš„ç›¸ä¼¼æ€§ï¼Œä¸”éå¸¸è§„å¾‹ã€‚å› æ­¤è¿™ç»™äº†æˆ‘ä»¬ä¸€ç§ç›´è§‰ï¼Œæˆ‘ä»¬ä¹Ÿè®¸å¯ä»¥å°†å®ƒä»¬æƒ³è±¡æˆè¿™äº›æ¡†ã€‚
- en: they slide apart and new boxes appear between them and they are just the interpolation
    of the surrounding onesã€‚And that's basically what we do with the position and
    beddingsã€‚We create new ones where they are missing ones because we need more and
    by interpoolating the surrounding or more precisely we basically see them as a
    picture in this caseã€‚14 by 14 with 700 something channels or whatever is the dimensionality
    and then we basically resize this like you would resize the picture by near interpolationã€‚
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬åˆ†å¼€ï¼Œå¹¶ä¸”åœ¨å®ƒä»¬ä¹‹é—´å‡ºç°æ–°çš„æ¡†ï¼Œå®ƒä»¬åªæ˜¯å‘¨å›´çš„æ’å€¼ã€‚è¿™åŸºæœ¬ä¸Šå°±æ˜¯æˆ‘ä»¬å¯¹ä½ç½®å’ŒåµŒå…¥æ‰€åšçš„ã€‚æˆ‘ä»¬åœ¨ç¼ºå¤±çš„åœ°æ–¹åˆ›å»ºæ–°çš„ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦æ›´å¤šï¼Œé€šè¿‡æ’å€¼å‘¨å›´çš„ï¼Œæˆ–è€…æ›´å‡†ç¡®åœ°è¯´ï¼Œæˆ‘ä»¬åŸºæœ¬ä¸Šå°†å®ƒä»¬è§†ä¸ºä¸€å¼ å›¾ç‰‡ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹æ˜¯14ä¹˜14ï¼Œæœ‰700å¤šä¸ªé€šé“æˆ–è€…å…¶ä»–ç»´åº¦ï¼Œç„¶åæˆ‘ä»¬åŸºæœ¬ä¸Šåƒé€šè¿‡è¿‘é‚»æ’å€¼é‚£æ ·è°ƒæ•´è¿™ä¸ªå¤§å°ã€‚
- en: And that way we will get more in new position em that we don't understand where
    they areã€‚but they follow the same pattern as the learned ones just at a higher
    resolutionï¼Œ basicallyã€‚ç‚¹è¿‡å»ã€‚Yeahï¼Œ that's a good questionã€‚So when no you creating
    theã€‚As input right now you're doing ajectã€‚Recently firstã€‚Has there been work to
    do that wayï¼Œ has everything weird good there right very close to each otherï¼Ÿ
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·æˆ‘ä»¬å°±èƒ½åœ¨æˆ‘ä»¬ä¸ç†è§£çš„ä½ç½®è·å¾—æ›´å¤šæ–°ä½ç½®ã€‚å®ƒä»¬éµå¾ªä¸å·²å­¦ä¹ ä½ç½®ç›¸åŒçš„æ¨¡å¼ï¼Œåªæ˜¯åŸºæœ¬ä¸Šåˆ†è¾¨ç‡æ›´é«˜ã€‚ç‚¹è¿‡å»ã€‚æ˜¯çš„ï¼Œè¿™æ˜¯ä¸ªå¥½é—®é¢˜ã€‚é‚£ä¹ˆå½“ä½ åˆ›å»ºè¾“å…¥æ—¶ï¼Œç°åœ¨ä½ æ­£åœ¨è¿›è¡Œajectã€‚æœ€è¿‘é¦–å…ˆã€‚æœ‰æ²¡æœ‰å·¥ä½œåšåˆ°è¿™ä¸€ç‚¹ï¼Œä¸€åˆ‡éƒ½å¾ˆå¥‡æ€ªå—ï¼Œå½¼æ­¤ä¹‹é—´éå¸¸æ¥è¿‘ï¼Ÿ
- en: Yeahï¼Œ there were quite a few wordsã€‚They tried varying ugly thingsã€‚One that I
    especially liked recentlyã€‚It's called early convolutions help transformers C better
    or is something like thatã€‚And they basically sayï¼Œ okayï¼Œ instead of this linear
    projectionã€‚instead of this one big linear projectionã€‚ we replace it by a stack
    of three by three convolution with a straight2ã€‚
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œæœ‰å¾ˆå¤šè¯ã€‚ä»–ä»¬å°è¯•äº†å„ç§å¥‡æ€ªçš„ä¸œè¥¿ã€‚å…¶ä¸­æˆ‘æœ€è¿‘ç‰¹åˆ«å–œæ¬¢çš„ä¸€ä¸ªæ˜¯ï¼Œæ—©æœŸå·ç§¯å¸®åŠ©å˜æ¢å™¨Cæ›´å¥½ï¼Œæˆ–è€…ç±»ä¼¼çš„ä¸œè¥¿ã€‚ä»–ä»¬åŸºæœ¬ä¸Šè¯´ï¼Œå¥½å§ï¼Œä»£æ›¿è¿™ä¸ªçº¿æ€§æŠ•å½±ã€‚ä»£æ›¿è¿™ä¸ªå¤§çš„çº¿æ€§æŠ•å½±ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªä¸‰ä¹˜ä¸‰å·ç§¯å †å æ¥æ›¿ä»£ã€‚
- en: And then they have also non nonlinearities between a normalization between themã€‚But
    such that's the overall stride is the same as the patch the dispatyifyingã€‚So the
    outcome will then be the same dimensionality as after this patch cutting and then
    projectingã€‚And then they showed thatã€‚ğŸ˜Šï¼ŒSupposedly it makes it a bit easier to
    optimize in the sense that more optimized settings are good settingsã€‚
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå®ƒä»¬ä¹‹é—´è¿˜æœ‰éçº¿æ€§å…³ç³»å’Œå½’ä¸€åŒ–ã€‚ä½†æ€»ä½“è€Œè¨€æ­¥å¹…ä¸è¡¥ä¸çš„å·®å¼‚æ˜¯ç›¸åŒçš„ã€‚å› æ­¤ï¼Œç»“æœå°†ä¸åœ¨è¿™ä¸ªè¡¥ä¸åˆ‡å‰²åå’ŒæŠ•å½±åçš„ç»´åº¦ç›¸åŒã€‚ç„¶åä»–ä»¬å±•ç¤ºäº†ã€‚ğŸ˜Šï¼Œæ®è¯´è¿™åœ¨ä¼˜åŒ–ä¸Šä¼šç¨å¾®ç®€å•ä¸€äº›ï¼Œå› ä¸ºæ›´ä¼˜åŒ–çš„è®¾ç½®æ˜¯å¥½çš„è®¾ç½®ã€‚
- en: In many scenariosï¼Œ it performs the sameã€‚But like more robustly to get thereã€‚And
    they also show some scenarios where this performs much betterï¼Œ like for exampleã€‚when
    pretraining onï¼Œ actually when they pretrain on more dataï¼Œ that seems to perform
    even betterã€‚å—¯ã€‚I havent played a bit with it and tried to reproduce itï¼Œ I don't
    have it fully reproduce itã€‚
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è®¸å¤šåœºæ™¯ä¸­ï¼Œå®ƒçš„è¡¨ç°æ˜¯ç›¸åŒçš„ã€‚ä½†æ›´ç¨³å¥åœ°è¾¾åˆ°è¿™ä¸ªæ•ˆæœã€‚è€Œä¸”ä»–ä»¬ä¹Ÿå±•ç¤ºäº†ä¸€äº›åœºæ™¯ï¼Œåœ¨è¿™äº›åœºæ™¯ä¸­ï¼Œå®ƒçš„è¡¨ç°æ›´å¥½ï¼Œæ¯”å¦‚è¯´ã€‚å½“åœ¨æ›´å¤šæ•°æ®ä¸Šè¿›è¡Œé¢„è®­ç»ƒæ—¶ï¼Œä¼¼ä¹è¡¨ç°å¾—æ›´å¥½ã€‚å—¯ã€‚æˆ‘è¿˜æ²¡å°è¯•å»å¤ç°å®ƒï¼Œæˆ‘å¹¶æ²¡æœ‰å®Œå…¨å¤ç°å‡ºæ¥ã€‚
- en: but I don't see as much benefit as in the paper yetã€‚but that's not to say that
    the paper is wrong just left I didn't get there yetã€‚That is one example of themï¼Œ
    there are other papers that do stuffã€‚but this one I found especially interesting
    because it's simpleã€‚Thank youã€‚All rightã€‚
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ç›®å‰è¿˜æ²¡æœ‰çœ‹åˆ°å’Œè®ºæ–‡ä¸€æ ·çš„å¥½å¤„ã€‚ä¸è¿‡è¿™å¹¶ä¸æ„å‘³ç€è®ºæ–‡æ˜¯é”™çš„ï¼Œåªæ˜¯æˆ‘è¿˜æ²¡åˆ°è¾¾é‚£ä¸ªç‚¹ã€‚è¿™æ˜¯å®ƒä»¬çš„ä¸€ä¸ªä¾‹å­ï¼Œè¿˜æœ‰å…¶ä»–è®ºæ–‡ä¹Ÿåšäº†ç±»ä¼¼çš„ç ”ç©¶ã€‚ä¸è¿‡æˆ‘å‘ç°è¿™ä¸€ç¯‡å°¤å…¶æœ‰è¶£ï¼Œå› ä¸ºå®ƒç®€å•ã€‚è°¢è°¢ã€‚å¥½çš„ã€‚
- en: continue we don't have more questionsã€‚Alrightï¼Œ then let's seeã€‚Yeahã€‚I have like
    three more interesting details from the paper and thenã€‚Depending on if you want
    more discussion or more contentï¼Œ I have more contentã€‚like also the question aboutï¼Œ
    does it satur it here or notï¼Ÿå—¯ã€‚Alrightã€‚
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ç»§ç»­ï¼Œæˆ‘ä»¬æ²¡æœ‰æ›´å¤šé—®é¢˜ã€‚é‚£ä¹ˆï¼Œçœ‹çœ‹å§ã€‚æ˜¯çš„ã€‚æˆ‘è¿˜æœ‰ä¸‰æ¡æ¥è‡ªè®ºæ–‡çš„æœ‰è¶£ç»†èŠ‚ã€‚ç„¶åï¼Œå–å†³äºä½ æƒ³è¦æ›´å¤šè®¨è®ºè¿˜æ˜¯æ›´å¤šå†…å®¹ï¼Œæˆ‘æœ‰æ›´å¤šå†…å®¹ã€‚æ¯”å¦‚å…³äºï¼Œæ˜¯å¦åœ¨è¿™é‡Œè¾¾åˆ°é¥±å’Œçš„é—®é¢˜ï¼Ÿå—¯ã€‚å¥½çš„ã€‚
- en: so another interesting thing that we had in the paperï¼Œ but it is buried in the
    appendixã€‚And then follow up papers from others have been written on this by nowï¼Œ
    actuallyï¼Œ is likeã€‚how should we scale these transformers right in the the high
    level shape of the transformerã€‚There's lots of settings that you could choose
    And we actually tried many of themã€‚
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬åœ¨è®ºæ–‡ä¸­æœ‰çš„å¦ä¸€ä¸ªæœ‰è¶£çš„ç‚¹ï¼Œä½†å®ƒè¢«åŸ‹åœ¨é™„å½•é‡Œã€‚ç°åœ¨ï¼Œå…¶ä»–äººä¹Ÿå¯¹æ­¤å†™äº†åç»­è®ºæ–‡ï¼Œå®é™…ä¸Šæ˜¯è¿™æ ·çš„ã€‚æˆ‘ä»¬åº”è¯¥å¦‚ä½•æ‰©å±•è¿™äº›å˜å‹å™¨ï¼Œå¯¹å§ï¼Œå˜å‹å™¨çš„é«˜å±‚å½¢çŠ¶ã€‚æœ‰å¾ˆå¤šè®¾ç½®å¯ä»¥é€‰æ‹©ï¼Œæˆ‘ä»¬å®é™…ä¸Šå°è¯•äº†å¾ˆå¤šã€‚
- en: So we started with the like reasonable medium size transformer is built in the
    middleã€‚And then we varied things one by oneï¼Œ Such thatï¼Œ we always double the computeã€‚Soï¼Œ
    for exampleã€‚this pink lineã€‚If we go to the right at this point increases the widthã€‚ğŸ˜Šã€‚Such
    that we double the compute X axis easy is a computeã€‚
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥æˆ‘ä»¬ä»ä¸€ä¸ªåˆç†çš„ä¸­ç­‰è§„æ¨¡çš„å˜å‹å™¨å¼€å§‹ï¼Œå»ºåœ¨ä¸­é—´ã€‚ç„¶åæˆ‘ä»¬é€ä¸€æ”¹å˜ä¸€äº›å‚æ•°ï¼Œè¿™æ ·æˆ‘ä»¬æ€»æ˜¯å°†è®¡ç®—é‡ç¿»å€ã€‚æ‰€ä»¥ï¼Œæ¯”å¦‚è¯´ã€‚è¿™æ¡ç²‰è‰²çº¿ã€‚å¦‚æœæˆ‘ä»¬åœ¨è¿™ä¸€ç‚¹å‘å³ç§»åŠ¨ï¼Œå®½åº¦å°±å¢åŠ äº†ã€‚ğŸ˜Šã€‚è¿™æ ·æˆ‘ä»¬å°±åŒå€äº†è®¡ç®—ï¼ŒXè½´å¾ˆç®€å•æ˜¯è®¡ç®—é‡ã€‚
- en: Relative to this starting point and we have all of these different settingsï¼Œ
    there's the widthã€‚which is the how wide is are the vectors with which cell function
    is doneã€‚which is for the base model 768 and then most larger or smallerã€‚There
    is like meï¼Œ as you seeã€‚scaling this does not seem promisingï¼Œ so we didn't scale
    that muchã€‚
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å¯¹äºè¿™ä¸ªèµ·å§‹ç‚¹ï¼Œæˆ‘ä»¬æœ‰æ‰€æœ‰è¿™äº›ä¸åŒçš„è®¾ç½®ï¼Œå®½åº¦ã€‚å³è¿›è¡Œç»†èƒåŠŸèƒ½è®¡ç®—çš„å‘é‡çš„å®½åº¦ã€‚åŸºç¡€æ¨¡å‹ä¸º768ï¼Œå…¶ä»–æ¨¡å‹åˆ™æ›´å¤§æˆ–æ›´å°ã€‚å°±åƒæˆ‘æ‰€çœ‹åˆ°çš„é‚£æ ·ã€‚æ‰©å±•è¿™ä¸€ç‚¹ä¼¼ä¹ä¸å¤ªæœ‰å‰æ™¯ï¼Œå› æ­¤æˆ‘ä»¬å¹¶æ²¡æœ‰æ‰©å±•å¾—å¤ªå¤šã€‚
- en: Then there's other things like the width of the multilayer perceptron in or
    some people call it the one by one convolution in these attentions and this seems
    to scale a bit nicer this orange part I actually wonder where it went to the left
    I don't remember I don't know if it's hidden somewhere or if you just didn't scale
    it down but any risk then another thing to scale which does not exist in the transformers
    from text is the patch size as you make the patch smaller you get more and more
    tokens out of an image and thus more and more compute capacityã€‚
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…¶ä»–ä¸€äº›ä¸œè¥¿ï¼Œæ¯”å¦‚å¤šå±‚æ„ŸçŸ¥å™¨çš„å®½åº¦ï¼Œæˆ–è€…æœ‰äººç§°ä¹‹ä¸ºè¿™äº›æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ä¸€å¯¹ä¸€å·ç§¯ï¼Œè¿™ä¼¼ä¹æ‰©å±•å¾—æ›´å¥½ï¼Œè¿™éƒ¨åˆ†æ©™è‰²æˆ‘å®é™…ä¸Šå¾ˆå¥½å¥‡å®ƒä¸ºä»€ä¹ˆä¼šå‘å·¦ç§»ï¼Œæˆ‘ä¸è®°å¾—äº†ï¼Œä¸çŸ¥é“å®ƒæ˜¯å¦éšè—åœ¨æŸä¸ªåœ°æ–¹ï¼Œæˆ–è€…æ˜¯å¦åªæ˜¯æ²¡æœ‰ç¼©å°ï¼Œä½†ä»»ä½•é£é™©è¿˜æœ‰å¦ä¸€ä¸ªè¦æ‰©å±•çš„ä¸œè¥¿ï¼Œåœ¨æ–‡æœ¬çš„å˜å‹å™¨ä¸­ä¸å­˜åœ¨çš„æ˜¯è¡¥ä¸å¤§å°ï¼Œå½“ä½ ç¼©å°è¡¥ä¸æ—¶ï¼Œä½ ä¼šä»ä¸€å¼ å›¾åƒä¸­è·å¾—è¶Šæ¥è¶Šå¤šçš„æ ‡è®°ï¼Œä»è€Œè·å¾—æ›´å¤šçš„è®¡ç®—èƒ½åŠ›ã€‚
- en: This is the green oneï¼Œ which also seems to scale nicelyã€‚ Then the depth is an
    interesting oneã€‚is yellow oneã€‚And this is the number of encoder blocksã€‚as we scaleï¼Œ
    it first seems like wowã€‚this is the thing you want to scaleï¼Œ but then it does
    seem to plateau and it scales really badly if you decrease the depthã€‚ğŸ˜Šï¼ŒSo that's
    not a good thing to decreaseã€‚ Howeverï¼Œ the width seems to be a good thing to decrease
    if you want to go through smaller models and then the blue is just scaling everything
    together such that the compute is kept like everything by roughly the same amountã€‚
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ç»¿è‰²çš„éƒ¨åˆ†ï¼Œä¼¼ä¹ä¹Ÿå¾ˆå¥½åœ°ç¼©æ”¾ã€‚ç„¶åæ·±åº¦æ˜¯ä¸€ä¸ªæœ‰è¶£çš„ç‚¹ï¼Œæ˜¯é»„è‰²çš„éƒ¨åˆ†ã€‚è¿™æ˜¯ç¼–ç å—çš„æ•°é‡ã€‚éšç€æˆ‘ä»¬æ‰©å±•ï¼Œèµ·åˆä¼¼ä¹å¾ˆæ£’ï¼Œè¿™æ­£æ˜¯ä½ æƒ³æ‰©å±•çš„ä¸œè¥¿ï¼Œä½†éšåä¼¼ä¹åœæ»ä¸å‰ï¼Œå¦‚æœä½ å‡å°‘æ·±åº¦ï¼Œå®ƒçš„ç¼©æ”¾æ•ˆæœçœŸçš„å¾ˆå·®ã€‚ğŸ˜Šï¼Œæ‰€ä»¥å‡å°‘æ·±åº¦å¹¶ä¸æ˜¯å¥½äº‹ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æƒ³é€šè¿‡è¾ƒå°çš„æ¨¡å‹ï¼Œå®½åº¦ä¼¼ä¹æ˜¯ä¸€ä¸ªå¯ä»¥å‡å°‘çš„å¥½ç‚¹ï¼Œç„¶åè“è‰²åˆ™æ˜¯å°†æ‰€æœ‰å†…å®¹ä¸€èµ·ç¼©æ”¾ï¼Œä½¿å¾—è®¡ç®—å¤§è‡´ä¿æŒç›¸åŒçš„æ•°é‡ã€‚
- en: that seems to scale nicely as well as the restã€‚And these is relatively simple
    or at least conceptuallyã€‚So we like thisã€‚So we went to that whenever we scale
    up or down the modelsã€‚ğŸ˜Šï¼Œå—¯ã€‚This I want I really like is the inference speedï¼Œ because
    if you have the image size of 2 to4 pixelsã€‚it actually means you have 2 to 4 by
    2 to4 pixelsï¼Œ rightã€‚
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¼¼ä¹ä¹Ÿèƒ½å¾ˆå¥½åœ°ç¼©æ”¾ä»¥åŠå…¶ä½™éƒ¨åˆ†ã€‚è¿™äº›ç›¸å¯¹ç®€å•ï¼Œè‡³å°‘åœ¨æ¦‚å¿µä¸Šã€‚æ‰€ä»¥æˆ‘ä»¬å–œæ¬¢è¿™ä¸ªã€‚å› æ­¤ï¼Œæ— è®ºä½•æ—¶æˆ‘ä»¬æ‰©å±•æˆ–ç¼©å°æ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬éƒ½è¿™æ ·åšã€‚ğŸ˜Šï¼Œå—¯ã€‚æˆ‘éå¸¸å–œæ¬¢çš„æ˜¯æ¨ç†é€Ÿåº¦ï¼Œå› ä¸ºå¦‚æœä½ æœ‰2åˆ°4åƒç´ çš„å›¾åƒå¤§å°ï¼Œè¿™å®é™…ä¸Šæ„å‘³ç€ä½ æœ‰2åˆ°4ä¹˜ä»¥2åˆ°4çš„åƒç´ ï¼Œå¯¹å§ï¼Ÿ
- en: So if you have let's then you patchify it with 16 by 16 patchï¼Œ for exampleï¼Œ
    patch sizeã€‚then you haveã€‚ğŸ˜Šï¼Œ14 by 14 patchesã€‚So that's the sequence length is actually
    150ï¼Œ right and thenã€‚On top of the sequence lengthï¼Œ you have the self attention
    operationï¼Œ which is square againã€‚So overall with the image with respect to image
    sizeã€‚
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥å¦‚æœä½ æœ‰ï¼Œç„¶åç”¨16ä¹˜16çš„è¡¥ä¸è¿›è¡Œè¡¥ä¸åŒ–ï¼Œæ¯”å¦‚è¡¥ä¸å¤§å°ã€‚ç„¶åä½ æœ‰ã€‚ğŸ˜Šï¼Œ14ä¹˜14çš„è¡¥ä¸ã€‚æ‰€ä»¥åºåˆ—é•¿åº¦å®é™…ä¸Šæ˜¯150ï¼Œå¯¹å§ï¼Ÿç„¶ååœ¨åºåˆ—é•¿åº¦ä¹‹ä¸Šï¼Œä½ æœ‰è‡ªæ³¨æ„åŠ›æ“ä½œï¼Œè¿™åˆæ˜¯å¹³æ–¹çš„ã€‚å› æ­¤ï¼Œæ•´ä½“ä¸Šä¸å›¾åƒå¤§å°ç›¸å…³ã€‚
- en: the self attention operation is to the fourth power what zip called quaticã€‚So
    that is really bad like everybody who sees all of something to the fourth is like
    what the hell are you doing this is never gonnaã€‚So we checked what does it look
    like in practice with the image sizes that we operate in and this is what we see
    here And the way A axis is the how fast it goes basically how fast it does inference
    and on the X axises is verying the input sizeã€‚ğŸ˜Šï¼ŒAnd thisã€‚Or this means it doesn't
    look so bad yetã€‚Basicallyã€‚
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: è‡ªæ³¨æ„åŠ›æ“ä½œæ˜¯å››æ¬¡æ–¹çš„ï¼ŒZipç§°ä¹‹ä¸ºâ€œquaticâ€ã€‚æ‰€ä»¥è¿™çœŸçš„å¾ˆç³Ÿç³•ï¼Œä»»ä½•çœ‹åˆ°å››æ¬¡æ–¹çš„äººéƒ½ä¼šè¯´ä½ åœ¨åšä»€ä¹ˆï¼Œè¿™æ°¸è¿œä¸ä¼šã€‚äºæ˜¯æˆ‘ä»¬æ£€æŸ¥äº†åœ¨æˆ‘ä»¬æ“ä½œçš„å›¾åƒå¤§å°ä¸‹å®é™…æƒ…å†µå¦‚ä½•ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„ï¼ŒAè½´çš„é€Ÿåº¦åŸºæœ¬ä¸Šå°±æ˜¯æ¨ç†çš„é€Ÿåº¦ï¼ŒXè½´æ˜¯è¾“å…¥å¤§å°çš„å˜åŒ–ã€‚ğŸ˜Šï¼Œæ‰€ä»¥è¿™æ„å‘³ç€ç›®å‰çœ‹èµ·æ¥è¿˜ä¸ç®—å¤ªç³Ÿã€‚
- en: when you go here to the 512 to the real laï¼Œ then you see that the transformers
    actually start going down a lot more than the renetsã€‚But in this reasonable image
    sizeï¼Œ let's call it very typicalã€‚it doesn't seem so bad in practice yetï¼Œ so we're
    not getting hit by the evil big old yetã€‚But as we go largerï¼Œ it will likely be
    a problem and there have been a lot of follow up works trying to make that betterã€‚
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ æ¥åˆ°512åˆ°çœŸå®çš„LAæ—¶ï¼Œä½ ä¼šå‘ç°å˜å‹å™¨å®é™…ä¸Šä¸‹é™å¾—æ¯”é‡å®šå‘å¤šå¾—å¤šã€‚ä½†åœ¨è¿™ä¸ªåˆç†çš„å›¾åƒå¤§å°ä¸Šï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºéå¸¸å…¸å‹çš„ã€‚å®é™…ä¸Šä¼¼ä¹è¿˜ä¸ç®—å¤ªç³Ÿï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜æ²¡æœ‰é­é‡åˆ°é‚£ä¸ªå¯æ¶çš„å¤§è€è™ã€‚ä½†éšç€æˆ‘ä»¬è§„æ¨¡çš„æ‰©å¤§ï¼Œè¿™å¯èƒ½ä¼šæˆä¸ºä¸€ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”æœ‰å¾ˆå¤šåç»­å·¥ä½œåœ¨åŠªåŠ›æ”¹å–„è¿™ä¸€ç‚¹ã€‚
- en: Rightã€‚Thenã€‚å¯¹å¯¹å¯¹ã€‚This is the last one from the original it paper this is looking
    at the input quotes receptive field sizeã€‚so in the self attention operation how
    far ago do heads typically attemptï¼ŸAnd here on the X axisã€‚we see the layer in
    the network to the right is more towards the outputã€‚the classes and to the left
    is more towards the input the patchesã€‚ğŸ˜Šã€‚
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ï¼Œæ²¡é”™ã€‚è¿™æ˜¯æ¥è‡ªåŸå§‹è®ºæ–‡çš„æœ€åä¸€éƒ¨åˆ†ï¼Œå®ƒå…³æ³¨è¾“å…¥å¼•ç”¨çš„æ„Ÿå—é‡å¤§å°ã€‚é‚£ä¹ˆåœ¨è‡ªæ³¨æ„åŠ›æ“ä½œä¸­ï¼Œå¤´éƒ¨é€šå¸¸å°è¯•å¤šè¿œï¼Ÿåœ¨Xè½´ä¸Šï¼Œæˆ‘ä»¬çœ‹åˆ°ç½‘ç»œçš„å±‚æ¬¡ï¼Œå³ä¾§æ›´æ¥è¿‘è¾“å‡ºï¼Œç±»ï¼Œå·¦ä¾§åˆ™æ›´æ¥è¿‘è¾“å…¥çš„è¡¥ä¸ã€‚ğŸ˜Š
- en: And the y axis is how far on average acrossï¼Œ I think the whole validation setã€‚does
    the self attention look and does look means that the peak of the cell attention
    or the maxã€‚how far is it weighï¼Œ something like thatã€‚ğŸ˜Šï¼Œå—¯ã€‚And each dog is a different
    head because we can use multi head self attention right and so what this shows
    is that in the early layers actually you have some heads that go look farã€‚but
    also a lot of heads that look very nearby them so locally and as we go deeper
    in the model we only are left with heads that on average look furtherã€‚
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Yè½´æ˜¯å¹³å‡è·ç¦»ï¼Œæˆ‘æƒ³æ˜¯æ•´ä¸ªéªŒè¯é›†ã€‚è‡ªæ³¨æ„åŠ›çš„æ•ˆæœå¦‚ä½•ï¼Œè¿™æ„å‘³ç€è‡ªæ³¨æ„åŠ›çš„å³°å€¼æˆ–æœ€å¤§å€¼ï¼Œæƒé‡æ˜¯å¤šå°‘ï¼Œåƒè¿™æ ·çš„ä¸œè¥¿ã€‚ğŸ˜Šï¼Œå—¯ã€‚æ¯ä¸ªç‹—æ˜¯ä¸åŒçš„å¤´ï¼Œå› ä¸ºæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å¤šå¤´è‡ªæ³¨æ„åŠ›ï¼Œå¯¹å§ï¼Ÿæ‰€ä»¥è¿™è¡¨æ˜ï¼Œåœ¨æ—©æœŸå±‚ä¸­ï¼Œå®é™…ä¸Šæœ‰ä¸€äº›å¤´æ˜¯çœ‹å¾—å¾ˆè¿œçš„ï¼Œä½†ä¹Ÿæœ‰å¾ˆå¤šå¤´æ˜¯çœ‹å¾—å¾ˆè¿‘çš„ï¼Œæ‰€ä»¥æ˜¯å±€éƒ¨çš„ï¼Œéšç€æˆ‘ä»¬æ·±å…¥æ¨¡å‹ï¼Œæˆ‘ä»¬åªå‰©ä¸‹å¹³å‡çœ‹å¾—æ›´è¿œçš„å¤´ã€‚
- en: ğŸ˜Šï¼ŒSoã€‚Is just some kind of analysis there is not immediately action to take about
    thisã€‚but it iss interesting to see that earlier layersã€‚they learn a mixture of
    looking to a local neighborhood and looking globally and later layers only look
    globally anymoreã€‚ğŸ˜Šï¼ŒRightã€‚So this is about the original vision transformersã€‚å—¯ã€‚
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæ‰€ä»¥ã€‚è¿™åªæ˜¯ä¸€ç§åˆ†æï¼Œæ²¡æœ‰ç«‹å³é‡‡å–è¡ŒåŠ¨çš„å†…å®¹ï¼Œä½†çœ‹åˆ°æ—©æœŸå±‚å­¦ä¹ çš„æ˜¯å±€éƒ¨é‚»åŸŸå’Œå…¨å±€ä¹‹é—´çš„æ··åˆï¼Œåæ¥å±‚åªå…³æ³¨å…¨å±€ï¼Œè¿™å¾ˆæœ‰è¶£ã€‚ğŸ˜Šï¼Œå¯¹ã€‚æ‰€ä»¥è¿™æ˜¯å…³äºåŸå§‹è§†è§‰å˜æ¢å™¨çš„ã€‚å—¯ã€‚
- en: No I don't know how long we want me to continue speaking or discussing I have
    a couple options that I can talk about which is one project that was furthers
    scalingating ups and this one also has the answer to the I can also jump straight
    to the answer if you don't want to hear the rest but to the question of like'
    how does it continue to the right are we satparating There is another project
    about how to train vision transformers when you don't have massive amounts of
    data can you still do itã€‚
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ï¼Œæˆ‘ä¸çŸ¥é“æˆ‘ä»¬æƒ³è®©æˆ‘ç»§ç»­è¯´å¤šä¹…æˆ–è®¨è®ºï¼Œæˆ‘æœ‰å‡ ä¸ªé€‰é¡¹å¯ä»¥è°ˆè®ºï¼Œå…¶ä¸­ä¸€ä¸ªé¡¹ç›®æ˜¯è¿›ä¸€æ­¥çš„æ‰©å±•ï¼Œè¿™ä¸ªé¡¹ç›®ä¹Ÿæœ‰ç­”æ¡ˆã€‚å¦‚æœä½ ä¸æƒ³å¬å‰©ä¸‹çš„å†…å®¹ï¼Œæˆ‘ä¹Ÿå¯ä»¥ç›´æ¥è·³åˆ°ç­”æ¡ˆï¼Œä½†å¯¹äºâ€œå®ƒæ˜¯å¦‚ä½•ç»§ç»­å‘å³çš„ï¼Œæˆ‘ä»¬æ˜¯å¦åœ¨åˆ†ç¦»â€çš„é—®é¢˜ï¼Œè¿˜æœ‰å¦ä¸€ä¸ªé¡¹ç›®æ˜¯å…³äºå¦‚ä½•åœ¨æ²¡æœ‰å¤§é‡æ•°æ®çš„æƒ…å†µä¸‹è®­ç»ƒè§†è§‰å˜æ¢å™¨ï¼Œä½ è¿˜å¯ä»¥è¿™æ ·åšå—ï¼Ÿ
- en: is it reasonable or is it maybe just unreasonable to doã€‚This one is maybe too
    unrelatedã€‚Let's not talk about thisã€‚And the last one is like I talk all about
    these benefits of a really large model when you pret them on lotss of dataã€‚Okayï¼Œ
    that's nice thats how we get a good model but then actually using a model that
    is massive is not fun at all Like it doesn't fit on your GP you need like multiple
    GPUus to even use it So people are not happy to use it and usually still go back
    to small each modelsã€‚
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ ·åšæ˜¯å¦åˆç†ï¼Œè¿˜æ˜¯å¯èƒ½æ ¹æœ¬ä¸åˆç†ã€‚è¿™å¯èƒ½æœ‰ç‚¹ä¸ç›¸å…³ã€‚æˆ‘ä»¬ä¸è°ˆè¿™ä¸ªã€‚æœ€åä¸€ä¸ªå°±æ˜¯æˆ‘è°ˆäº†å¾ˆå¤šå…³äºåœ¨å¤§é‡æ•°æ®ä¸Šè®­ç»ƒå¤§å‹æ¨¡å‹çš„å¥½å¤„ã€‚å¥½çš„ï¼Œè¿™å¾ˆå¥½ï¼Œè¿™å°±æ˜¯æˆ‘ä»¬å¦‚ä½•å¾—åˆ°ä¸€ä¸ªå¥½çš„æ¨¡å‹ï¼Œä½†å®é™…ä¸Šä½¿ç”¨ä¸€ä¸ªå·¨å¤§çš„æ¨¡å‹ä¸€ç‚¹ä¹Ÿä¸å¥½ç©ã€‚å°±åƒå®ƒä¸é€‚åˆä½ çš„GPUï¼Œä½ éœ€è¦å¤šä¸ªGPUæ‰èƒ½ä½¿ç”¨ã€‚æ‰€ä»¥äººä»¬ä¸ä¹æ„ä½¿ç”¨å®ƒï¼Œé€šå¸¸è¿˜æ˜¯ä¼šå›åˆ°å°æ¨¡å‹ã€‚
- en: even though you know like larger model should be betterã€‚what can we do about
    itã€‚ğŸ˜Šï¼Œå—¯ã€‚That's another project we hadï¼Œ she's about thisã€‚Soã€‚I would say it's up
    to you guys what you prefer to doï¼Œ or if you have plenty of questionã€‚we can continue
    with the questions now because I think now the original one hour would be overrã€‚
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å³ä½¿ä½ çŸ¥é“åƒå¤§å‹æ¨¡å‹åº”è¯¥æ›´å¥½ï¼Œæˆ‘ä»¬èƒ½å¯¹æ­¤åšäº›ä»€ä¹ˆå‘¢ï¼ŸğŸ˜Šï¼Œå—¯ã€‚è¿™æ˜¯æˆ‘ä»¬å¦ä¸€ä¸ªé¡¹ç›®çš„ä¸»é¢˜ã€‚æ‰€ä»¥ï¼Œæˆ‘ä¼šè¯´è¿™å–å†³äºä½ ä»¬æƒ³åšä»€ä¹ˆï¼Œæˆ–è€…å¦‚æœä½ ä»¬æœ‰å¾ˆå¤šé—®é¢˜ï¼Œæˆ‘ä»¬ç°åœ¨å¯ä»¥ç»§ç»­æé—®ï¼Œå› ä¸ºæˆ‘è®¤ä¸ºåŸå®šçš„ä¸€å°æ—¶æ—¶é—´å¿«è¦ç»“æŸäº†ã€‚
- en: Rightã€‚So I think one suggestion was like we can continue the talk and we would
    also be recording it so people that can like just like go and see it if they missed
    or something so we could do thatã€‚Yeahï¼Œ the other thing is two people have their
    hands raised so we canã€‚Okay questionsions for usã€‚
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ã€‚æ‰€ä»¥æˆ‘è®¤ä¸ºä¸€ä¸ªå»ºè®®æ˜¯ï¼Œæˆ‘ä»¬å¯ä»¥ç»§ç»­è®¨è®ºï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜ä¼šå½•åˆ¶å®ƒï¼Œè¿™æ ·é‚£äº›é”™è¿‡çš„äººå¯ä»¥å»çœ‹ä¸€ä¸‹ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¿™æ ·åšã€‚æ˜¯çš„ï¼Œå¦å¤–æœ‰ä¸¤ä¸ªäººä¸¾æ‰‹äº†ï¼Œæˆ‘ä»¬å¯ä»¥ã€‚å¥½çš„ï¼Œé—®æˆ‘ä»¬é—®é¢˜å§ã€‚
- en: After you guysã€‚And fight hide the wayã€‚So you guys want to ask your questionsï¼ŸYeahã€‚I
    just had a pretty basic questionï¼Œ so if an object lies on the border between the
    patchesã€‚does that impact the model's performance in any wayï¼ŸYeahï¼Œ I meanï¼Œ that's
    not a basic questionã€‚It's a good questionã€‚There is a mix of answers so one is
    we didn't specifically go and test this It would be an interesting thing to test
    in a very controlled way with some of the trained models that's for sure The other
    thing is thatã€‚
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ ä»¬ä¹‹åã€‚æ‰“æ¶éšè—çš„æ–¹å¼ã€‚ä½ ä»¬æƒ³é—®ä½ ä»¬çš„é—®é¢˜å—ï¼Ÿæ˜¯çš„ã€‚æˆ‘æœ‰ä¸ªå¾ˆåŸºæœ¬çš„é—®é¢˜ï¼Œæ‰€ä»¥å¦‚æœä¸€ä¸ªç‰©ä½“ä½äºè¡¥ä¸ä¹‹é—´çš„è¾¹ç•Œï¼Œè¿™æ˜¯å¦ä¼šå½±å“æ¨¡å‹çš„æ€§èƒ½ï¼Ÿæ˜¯çš„ï¼Œæˆ‘çš„æ„æ€æ˜¯ï¼Œé‚£ä¸æ˜¯ä¸€ä¸ªåŸºæœ¬é—®é¢˜ã€‚è¿™æ˜¯ä¸ªå¥½é—®é¢˜ã€‚æœ‰å¤šç§ç­”æ¡ˆï¼Œä¸€ä¸ªæ˜¯æˆ‘ä»¬æ²¡æœ‰ç‰¹åˆ«å»æµ‹è¯•è¿™ä¸ªï¼Œç¡®å®åœ¨ä¸€äº›å—æ§çš„ç¯å¢ƒä¸‹ç”¨ä¸€äº›è®­ç»ƒè¿‡çš„æ¨¡å‹æµ‹è¯•ä¸€ä¸‹ä¼šå¾ˆæœ‰è¶£ã€‚å¦ä¸€ä¸ªäº‹æƒ…æ˜¯ã€‚
- en: When you have massive data set like 300 million images it's an insane amount
    I used to try to conceptualize how much is image net 1 million images and I think
    I did the math is like if you go to an image and look at all of the images each
    image for a couple of seconds you were sitting there for a month or something
    like that don't rememberã€‚
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ æœ‰åƒ3äº¿å¼ å›¾åƒè¿™æ ·çš„åºå¤§æ•°æ®é›†æ—¶ï¼Œè¿™ç®€ç›´æ˜¯ç–¯ç‹‚çš„æ•°é‡ã€‚æˆ‘æ›¾è¯•å›¾æ¦‚å¿µåŒ–ImageNetçš„ç™¾ä¸‡å¼ å›¾åƒï¼Œæˆ‘æƒ³æˆ‘åšè¿‡è®¡ç®—ï¼Œå¦‚æœä½ å»æŸ¥çœ‹æ¯ä¸€å¼ å›¾åƒï¼Œæ¯å¼ å›¾åƒåœç•™å‡ ç§’é’Ÿï¼Œä½ å¯èƒ½ä¼šåä¸€ä¸ªæœˆå·¦å³ï¼Œä¸å¤ªè®°å¾—äº†ã€‚
- en: but 300 million is just insanely massive and then on top of that we do actually
    use random augmentations like random crop out of the imageã€‚So I would say it's
    the default that you see objects that don't fall on a patch during the training
    already and if you look at here basically this is the standard model like how
    the patches are when we have 14 by 14 they look roughly this size alsoã€‚
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æ˜¯3äº¿å¼ å›¾åƒç®€ç›´æ˜¯ç–¯ç‹‚çš„åºå¤§ï¼Œæ­¤å¤–ï¼Œæˆ‘ä»¬ç¡®å®ä½¿ç”¨éšæœºå¢å¼ºï¼Œæ¯”å¦‚éšæœºè£å‰ªå›¾åƒã€‚å› æ­¤ï¼Œæˆ‘è®¤ä¸ºè¿™æ˜¯ä½ åœ¨è®­ç»ƒä¸­çœ‹åˆ°çš„å¯¹è±¡ä¸è½åœ¨åŒºåŸŸä¸Šçš„é»˜è®¤æƒ…å†µï¼Œå¦‚æœä½ åœ¨è¿™é‡Œçœ‹ï¼ŒåŸºæœ¬ä¸Šè¿™æ˜¯æ ‡å‡†æ¨¡å‹ï¼Œåƒ14ä¹˜14çš„åŒºåŸŸå¤§è‡´æ˜¯è¿™ä¸ªå¤§å°ã€‚
- en: Then an object is usually scattered across many patches actuallyã€‚because objects
    in typical images are relatively large right people don't take a picture where
    the object of interest is super tiny in the corner so that's the default that
    you see during pretraining and so I believe that the model just learns to do that
    much better actuallyã€‚
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åä¸€ä¸ªå¯¹è±¡é€šå¸¸ä¼šæ•£å¸ƒåœ¨å¤šä¸ªåŒºåŸŸï¼Œå› ä¸ºåœ¨å…¸å‹å›¾åƒä¸­ï¼Œç‰©ä½“ç›¸å¯¹è¾ƒå¤§ï¼Œäººä»¬ä¸ä¼šæ‹æ‘„å¯¹è±¡åœ¨è§’è½é‡Œçš„å¾®å°å›¾åƒã€‚å› æ­¤ï¼Œè¿™æ˜¯ä½ åœ¨é¢„è®­ç»ƒä¸­çœ‹åˆ°çš„é»˜è®¤æƒ…å†µï¼Œæˆ‘ç›¸ä¿¡æ¨¡å‹å°±æ˜¯è¿™æ ·å­¦ä¹ å¾—æ›´å¥½çš„ã€‚
- en: Then the other answer to the question is likeï¼Œ okayã€‚maybe if you did some nicer
    thing than this very crude patch cuttingï¼Œ like for exampleã€‚this con stack of convolutions
    that I mentionedï¼Œ maybe this is even better could beã€‚Thank youã€‚Yeahã€‚so you mentioned
    thatã€‚Or transformersï¼Œ or at least you mentioned in the paper thatã€‚
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åå¯¹è¿™ä¸ªé—®é¢˜çš„å¦ä¸€ä¸ªå›ç­”æ˜¯ï¼Œå¥½çš„ã€‚ä¹Ÿè®¸å¦‚æœä½ åšä¸€äº›æ¯”è¿™ç§éå¸¸ç²—ç³™çš„åŒºåŸŸåˆ‡å‰²æ›´å¥½çš„äº‹æƒ…ï¼Œæ¯”å¦‚æˆ‘æåˆ°çš„è¿™ä¸ªå·ç§¯çš„å †å ï¼Œå¯èƒ½è¿™ç”šè‡³ä¼šæ›´å¥½ã€‚è°¢è°¢ã€‚æ˜¯çš„ï¼Œä½ æåˆ°è¿‡ï¼Œæˆ–è€…å˜æ¢å™¨ï¼Œæˆ–è€…è‡³å°‘ä½ åœ¨è®ºæ–‡ä¸­æåˆ°è¿‡ã€‚
- en: They laugh like local caity and likeã€‚ã‚ã€‚I was just thinkingï¼Œ are these sort ofï¼ŸPropertyã€‚å—¯å—¯ã€‚ç³»ã€‚And
    it's especially when you're in theã€‚Why is it thatï¼Ÿä½ ä¸ªä¸è¯´ã€‚å—¯å“¼ã€‚ğŸ˜Šï¼Œå—¯ã€‚The audio was not
    that goodã€‚but I believe I understood the question is that we say that transformer
    lack locality bias or prior or whateverã€‚and why is this even something that we
    wantï¼Œ rightï¼Ÿ
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ä»–ä»¬ç¬‘ç€è¯´ï¼Œåœ°æ–¹æ€§ã€‚å—¯ï¼Œæˆ‘åªæ˜¯æƒ³ï¼Œè¿™äº›ç®—ä¸ç®—ï¼Ÿå—¯å—¯ã€‚å°¤å…¶æ˜¯å½“ä½ åœ¨â€¦â€¦ä¸ºä»€ä¹ˆä¼šè¿™æ ·ï¼Ÿä½ ä¸ªä¸è¯´ã€‚å—¯å“¼ã€‚ğŸ˜Šï¼Œå—¯ã€‚éŸ³é¢‘ä¸æ˜¯å¾ˆå¥½ï¼Œä½†æˆ‘ç›¸ä¿¡æˆ‘ç†è§£äº†é—®é¢˜ï¼Œå°±æ˜¯æˆ‘ä»¬è¯´å˜æ¢å™¨ç¼ºä¹åœ°æ–¹æ€§åè§æˆ–å…ˆéªŒï¼Œæˆ–è€…å…¶ä»–ä»€ä¹ˆçš„ï¼Œä¸ºä»€ä¹ˆè¿™ç”šè‡³æ˜¯æˆ‘ä»¬æƒ³è¦çš„å‘¢ï¼Œå¯¹å§ï¼Ÿ
- en: Wouldn't we want our models to know about locality if they are about pictures
    in the first place yes and noã€‚so this that's why I give the context in the beginning
    this is all about what happens when you scaling things upã€‚Andã€‚Specificallyï¼Œ whatã€‚In
    ideal wordï¼Œ at least in our mindï¼Œ we want gigantic amounts of dataã€‚and we believe
    that it will just keep growing as the years go by and there will be more and more
    data just generally thereã€‚
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬çš„æ¨¡å‹æ˜¯å…³äºå›¾åƒçš„ï¼Œéš¾é“ä¸æƒ³äº†è§£åœ°æ–¹æ€§å—ï¼Ÿæ˜¯å’Œä¸æ˜¯ã€‚æ‰€ä»¥è¿™å°±æ˜¯æˆ‘ä¸€å¼€å§‹ç»™å‡ºèƒŒæ™¯çš„åŸå› ï¼Œè¿™ä¸€åˆ‡éƒ½æ˜¯å…³äºå½“ä½ æ‰©å±•äº‹ç‰©æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆã€‚å…·ä½“æ¥è¯´ï¼Œç†æƒ³æƒ…å†µä¸‹ï¼Œè‡³å°‘åœ¨æˆ‘ä»¬å¿ƒä¸­ï¼Œæˆ‘ä»¬å¸Œæœ›æœ‰å·¨é‡çš„æ•°æ®ï¼Œæˆ‘ä»¬ç›¸ä¿¡éšç€æ—¶é—´çš„æ¨ç§»ï¼Œè¿™ç§æ•°æ®åªä¼šä¸æ–­å¢é•¿ã€‚
- en: And then we want the model to have as little ofã€‚Our thinking built inã€‚because
    what we may think that is good to solve the task may actually not be best to solve
    the taskã€‚You knowï¼Œ maybe like a analogy would be likeï¼Œ what was it alphago that
    made some moves that experts would say this is crazyã€‚This is a silly moveï¼Œ But
    it actually then was much betterã€‚ And in a similar wayã€‚
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åæˆ‘ä»¬å¸Œæœ›æ¨¡å‹å°½é‡å°‘åœ°å†…ç½®æˆ‘ä»¬çš„æ€ç»´ï¼Œå› ä¸ºæˆ‘ä»¬è®¤ä¸ºå¥½çš„è§£å†³ä»»åŠ¡çš„æ–¹æ³•å®é™…ä¸Šå¯èƒ½å¹¶ä¸æ˜¯æœ€ä½³çš„è§£å†³æ–¹æ¡ˆã€‚ä½ çŸ¥é“ï¼Œä¹Ÿè®¸å¯ä»¥ç±»æ¯”ä¸€ä¸‹ï¼Œæ¯”å¦‚AlphaGoçš„ä¸€äº›ä¸¾åŠ¨ï¼Œä¸“å®¶ä»¬ä¼šè¯´è¿™å¾ˆç–¯ç‹‚ã€‚è¿™æ˜¯ä¸€ä¸ªæ„šè ¢çš„ä¸¾åŠ¨ï¼Œä½†å®é™…ä¸Šå´æ›´å¥½ã€‚ä»¥ç±»ä¼¼çš„æ–¹å¼ã€‚
- en: we want to encode as little as possible into the modelã€‚such that if we just
    throw massive amounts of data and the difficult task at itã€‚that it might find
    things that are even better that we didn't think of beforeã€‚ğŸ˜Šã€‚This is our approach
    because we believe that like as I mentionedï¼Œ I think alreadyã€‚
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½å°‘åœ°ç¼–ç åˆ°æ¨¡å‹ä¸­ï¼Œè¿™æ ·å¦‚æœæˆ‘ä»¬æŠ›å‡ºå¤§é‡æ•°æ®å’Œå›°éš¾ä»»åŠ¡ï¼Œå®ƒå¯èƒ½ä¼šå‘ç°ä¸€äº›æˆ‘ä»¬ä¹‹å‰æ²¡æƒ³åˆ°çš„æ›´å¥½çš„ä¸œè¥¿ã€‚ğŸ˜Šè¿™æ˜¯æˆ‘ä»¬çš„åšæ³•ï¼Œå› ä¸ºæˆ‘ä»¬ç›¸ä¿¡ï¼Œå°±åƒæˆ‘æåˆ°çš„ï¼Œæˆ‘è®¤ä¸ºå·²ç»ã€‚
- en: what seems massive and excessive now will be the norm in five years or soã€‚so
    that's where we want to go and look what's the direction howeverã€‚if you want to
    like just get something working now and don't have massive amounts of data and
    don't want to use pretrain model for some reason whichã€‚Always use a pre modelï¼Œ
    but if you don't want toï¼Œ then it makes utter sense to believe in some of your
    prior intuition and knowledge of what should probably help the model like localityã€‚
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨çœ‹ä¼¼åºå¤§å’Œè¿‡åº¦çš„äº‹æƒ…ï¼Œåœ¨äº”å¹´å·¦å³å°†æˆä¸ºå¸¸æ€ã€‚æ‰€ä»¥æˆ‘ä»¬å¸Œæœ›å»çœ‹çœ‹æ–¹å‘æ˜¯ä»€ä¹ˆã€‚ç„¶è€Œï¼Œå¦‚æœä½ åªæ˜¯æƒ³è®©æŸä¸ªä¸œè¥¿ç°åœ¨å·¥ä½œï¼Œè€Œæ²¡æœ‰å¤§é‡æ•°æ®ï¼Œå¹¶ä¸”å› ä¸ºæŸç§åŸå› ä¸æƒ³ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ï¼Œé‚£ä¹ˆã€‚æ€»æ˜¯ä½¿ç”¨é¢„æ¨¡å‹ï¼Œä½†å¦‚æœä½ ä¸æƒ³ï¼Œé‚£ä¹ˆç›¸ä¿¡ä½ çš„ä¸€äº›å…ˆå‰ç›´è§‰å’ŒçŸ¥è¯†æ˜¯æœ‰æ„ä¹‰çš„ï¼Œæ¯”å¦‚å±€éƒ¨æ€§ã€‚
- en: ğŸ˜Šï¼Œå—¯ã€‚I hope this answered your questionã€‚The suppose this is a PowerPotã€‚Or sort
    of point we aboutã€‚Thankã€‚W mean like any vision taskï¼Œ like isn't that sort ofã€‚Like
    I don't knowã€‚maybe I'm not seeing lateã€‚Thatly why we do not want food and thatã€‚You
    may be elaboraterating on thatï¼Œ but why is it that we don' wantï¼Ÿ
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œå—¯ã€‚æˆ‘å¸Œæœ›è¿™å›ç­”äº†ä½ çš„é—®é¢˜ã€‚å‡è®¾è¿™æ˜¯ä¸€ä¸ªPowerPotã€‚æˆ–è€…æˆ‘ä»¬å…³äºçš„æŸç§è§‚ç‚¹ã€‚è°¢è°¢ã€‚æˆ‘ä»¬æ˜¯è¯´ä»»ä½•è§†è§‰ä»»åŠ¡ï¼Œéš¾é“ä¸æ˜¯è¿™ç§æƒ…å†µå—ï¼Ÿåƒæˆ‘ä¸çŸ¥é“ã€‚ä¹Ÿè®¸æˆ‘æ²¡çœ‹åˆ°é‚£æ ·ã€‚æ­£æ˜¯è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸æƒ³è¦é£Ÿç‰©çš„åŸå› ã€‚ä½ å¯ä»¥å¯¹æ­¤åšæ›´è¯¦ç»†çš„é˜è¿°ï¼Œä½†ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸æƒ³è¦å‘¢ï¼Ÿ
- en: Like locality or what translation thatã€‚Wellï¼Œ ideallyã€‚we want the model that
    is powerful enough to learn about this concept itself if it is useful to solve
    the taskã€‚if it's not useful to solve the task thenã€‚If we hard code it inã€‚there
    is no way for the model not to do thisï¼Œ rightï¼Ÿå—¯ã€‚That is ideally the outcome in
    a similar wayã€‚
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åƒå±€éƒ¨æ€§æˆ–è€…ä»€ä¹ˆç¿»è¯‘é‚£æ ·ã€‚å—¯ï¼Œç†æƒ³æƒ…å†µä¸‹ã€‚æˆ‘ä»¬å¸Œæœ›æ¨¡å‹æœ‰è¶³å¤Ÿå¼ºå¤§çš„èƒ½åŠ›å»å­¦ä¹ è¿™ä¸ªæ¦‚å¿µæœ¬èº«ï¼Œå¦‚æœå®ƒå¯¹äºè§£å†³ä»»åŠ¡æ˜¯æœ‰ç”¨çš„ã€‚å¦‚æœå¯¹è§£å†³ä»»åŠ¡æ²¡æœ‰ç”¨ï¼Œé‚£ä¹ˆã€‚å¦‚æœæˆ‘ä»¬ç¡¬ç¼–ç å®ƒï¼Œæ¨¡å‹å°±æ²¡æœ‰åŠæ³•ä¸è¿™æ ·åšï¼Œå¯¹å§ï¼Ÿå—¯ã€‚è¿™ç†æƒ³çš„ç»“æœåœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯ç›¸ä¼¼çš„ã€‚
- en: alsoï¼Œ that in languageï¼Œ you knowï¼Œ it seemed to be nonsense to not encode the
    from left to right direction of textã€‚like in R Sã€‚But then comes transformer and
    just doesn'tã€‚And works much better if you throw a lot of data at itã€‚and it recovers
    that plus less some more or a more flexible variant of it or something like that
    that is even better for survey taskã€‚
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œåœ¨è¯­è¨€ä¸­ï¼Œä½ çŸ¥é“ï¼Œä¸ç¼–ç ä»å·¦åˆ°å³çš„æ–‡æœ¬æ–¹å‘ä¼¼ä¹æ¯«æ— æ„ä¹‰ã€‚å°±åƒåœ¨R Sä¸­ã€‚ä½†æ¥ä¸‹æ¥å‡ºç°å˜æ¢å™¨ï¼Œç»“æœå´å¹¶æ²¡æœ‰é‚£æ ·ã€‚å¦‚æœä½ ç»™å®ƒå¤§é‡æ•°æ®ï¼Œå®ƒçš„æ•ˆæœä¼šæ›´å¥½ï¼Œå¹¶ä¸”èƒ½å¤Ÿæ¢å¤è¿™äº›ï¼ŒåŠ ä¸Šä¸€äº›æ›´çµæ´»çš„å˜ä½“ï¼Œæˆ–è€…åƒè¿™æ ·çš„ä¸œè¥¿ï¼Œå¯¹äºè°ƒæŸ¥ä»»åŠ¡æ¥è¯´æ›´å¥½ã€‚
- en: So basicallyï¼Œ the human being thatã€‚We are not as smart to design the thingã€‚The
    model in the way that will be best for the taskã€‚let's rather give it all the flexibility
    and all the data it needs to figure out what is the best way of solvinging the
    taskã€‚I really there is a philosophy of approaching itã€‚ I'm not saying this is
    the only true wayï¼Œ rightã€‚
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥åŸºæœ¬ä¸Šï¼Œäººç±»æ˜¯ã€‚æˆ‘ä»¬å¹¶ä¸èªæ˜åˆ°èƒ½å¤Ÿä»¥æœ€é€‚åˆä»»åŠ¡çš„æ–¹å¼è®¾è®¡è¿™ä¸ªä¸œè¥¿ã€‚æˆ‘ä»¬å®æ„¿ç»™å®ƒæ‰€æœ‰çš„çµæ´»æ€§å’Œå®ƒè§£å†³ä»»åŠ¡æ‰€éœ€çš„æ‰€æœ‰æ•°æ®ã€‚æˆ‘çœŸçš„è®¤ä¸ºè¿™æ˜¯ä¸€ç§æ¥è¿‘çš„å“²å­¦ã€‚æˆ‘å¹¶ä¸æ˜¯è¯´è¿™å°±æ˜¯å”¯ä¸€æ­£ç¡®çš„æ–¹æ³•ï¼Œå¯¹å§ã€‚
- en: Okayï¼Œ so we have around seven minutes left before the scheduled end of the talkã€‚And
    Lucasã€‚we want to be mindful of your time as well because it's it is evening where
    you are soã€‚One thing we could do is you could I don't see any more questions right
    now so you could actually sort of go over the last few bits maybe skipping through
    the details and just talking about the final results I going do these two on the
    high level and those two that are still very very tied to transformers and answer
    some question that happened before like the first question was like okay are we
    saturating yes or no andã€‚
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿˜æœ‰å¤§çº¦ä¸ƒåˆ†é’Ÿçš„æ—¶é—´ï¼Œç›´åˆ°è°ˆè¯çš„é¢„å®šç»“æŸã€‚è€Œä¸”ï¼Œå¢å¡æ–¯ã€‚æˆ‘ä»¬ä¹Ÿæƒ³å…³æ³¨ä½ çš„æ—¶é—´ï¼Œå› ä¸ºä½ é‚£è¾¹æ˜¯æ™šä¸Šã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥åšçš„ä¸€ä»¶äº‹æ˜¯ï¼Œæˆ‘ç°åœ¨æ²¡æœ‰çœ‹åˆ°æ›´å¤šçš„é—®é¢˜ï¼Œæ‰€ä»¥ä½ å¯ä»¥å®é™…æµè§ˆæœ€åå‡ éƒ¨åˆ†ï¼Œå¯èƒ½è·³è¿‡ç»†èŠ‚ï¼Œåªè®¨è®ºæœ€ç»ˆç»“æœã€‚æˆ‘å°†è¿™ä¸¤ä¸ªæ”¾åœ¨é«˜å±‚æ¬¡ä¸Šï¼Œé‚£ä¸¤ä¸ªä»ç„¶éå¸¸ä¸å˜æ¢å™¨ç›¸å…³ï¼Œå¹¶å›ç­”ä¹‹å‰å‘ç”Ÿçš„ä¸€äº›é—®é¢˜ï¼Œæ¯”å¦‚ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¥½çš„ï¼Œæˆ‘ä»¬æ˜¯å¦è¾¾åˆ°äº†é¥±å’Œï¼Œæ˜¯è¿˜æ˜¯ä¸æ˜¯ã€‚
- en: Hereï¼Œ noã€‚This was the bit on this benchmark from the original transforming paperã€‚but
    then it's like these transformers when we use themã€‚which just notice they have
    really nice scaling properties and they seem actually to be easier to scale upã€‚Without
    paying massive compute as much as resonancesonates just from gut feelinging from
    us having experience with bothã€‚
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œï¼Œä¸ã€‚è¿™æ˜¯åŸå§‹å˜æ¢è®ºæ–‡ä¸­å…³äºè¿™ä¸ªåŸºå‡†çš„éƒ¨åˆ†ã€‚ä½†ç„¶åä½¿ç”¨è¿™äº›å˜æ¢å™¨æ—¶ï¼Œæˆ‘ä»¬æ³¨æ„åˆ°å®ƒä»¬å…·æœ‰éå¸¸å¥½çš„æ‰©å±•æ€§ï¼Œå¹¶ä¸”å®é™…ä¸Šä¼¼ä¹æ›´å®¹æ˜“è¿›è¡Œæ‰©å±•ã€‚æ²¡æœ‰åƒå…±æŒ¯é‚£æ ·æ”¯ä»˜å·¨é¢è®¡ç®—ï¼Œè¿™åªæ˜¯åŸºäºæˆ‘ä»¬å¯¹ä¸¤è€…çš„ç»éªŒç›´è§‰ã€‚
- en: And so we went and look what happens if we scale vision transformer just as
    far up as we possibly can and we spent quite a lot of our blood into making this
    happen one part of it is scaling the data set so we went back to this Google internal
    team that this 300 million data set is just one out of many that they work with
    and we asked around and they basically had the 3 billion like 10 types largerggger
    data set that we could also like play around with so they go and scale up the
    data set andã€‚
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å»çœ‹çœ‹å¦‚æœå°†è§†è§‰å˜æ¢å™¨æ‰©å±•åˆ°æé™ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œæˆ‘ä»¬ä¸ºå®ç°è¿™ä¸€ç›®æ ‡ä»˜å‡ºäº†å¾ˆå¤šåŠªåŠ›ï¼Œå…¶ä¸­ä¸€éƒ¨åˆ†æ˜¯æ‰©å±•æ•°æ®é›†ã€‚æˆ‘ä»¬å›åˆ°äº†è¿™ä¸ªè°·æ­Œå†…éƒ¨å›¢é˜Ÿï¼Œä»–ä»¬çš„3äº¿æ•°æ®é›†åªæ˜¯ä»–ä»¬ä½¿ç”¨çš„ä¼—å¤šæ•°æ®é›†ä¹‹ä¸€ã€‚æˆ‘ä»¬è¯¢é—®åå‘ç°ï¼Œä»–ä»¬åŸºæœ¬ä¸Šæœ‰30äº¿ä¸ªæ›´å¤§çš„æ•°æ®é›†å¯ä»¥ä½¿ç”¨ï¼Œå› æ­¤ä»–ä»¬è¿›è¡Œäº†æ•°æ®é›†çš„æ‰©å±•ã€‚
- en: This just showingï¼Œ yesï¼Œ just scaling up the data set and switching it gives
    you benefitsã€‚But that's not all of itã€‚Then the next thing is we needed to figure
    out how to use less memory on device like on view or TPUã€‚because already previously
    with this scoreï¼Œ we fitted the model as much as we could withã€‚So we did a lot
    of clicks that I will skip for now and are able to scale much largerã€‚ This is
    likeã€‚
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä»…ä»…è¡¨æ˜ï¼Œæ˜¯çš„ï¼Œæ‰©å±•æ•°æ®é›†å’Œåˆ‡æ¢æ•°æ®é›†å¸¦æ¥äº†å¥½å¤„ã€‚ä½†è¿™è¿˜ä¸æ˜¯å…¨éƒ¨ã€‚æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦å¼„æ¸…æ¥šå¦‚ä½•åœ¨è®¾å¤‡ä¸Šä½¿ç”¨æ›´å°‘çš„å†…å­˜ï¼Œæ¯”å¦‚åœ¨GPUæˆ–TPUä¸Šã€‚å› ä¸ºä¹‹å‰å·²ç»åœ¨è¿™ä¸ªå¾—åˆ†ä¸‹å°½å¯èƒ½åœ°æ‹Ÿåˆäº†æ¨¡å‹ã€‚å› æ­¤æˆ‘ä»¬åšäº†å¾ˆå¤šçš„è°ƒæ•´ï¼Œç°åœ¨èƒ½å¤Ÿæ‰©å±•å¾—æ›´å¤§ã€‚è¿™æ˜¯åƒã€‚
- en: ğŸ˜Šï¼ŒThis plot shows the size of the modern in the different shape characters that
    I mentioned beforeã€‚like the weight of the MIP on X axisesï¼Œ the surf at width on
    the Y axisesã€‚and then the different plots are different layers for the depthã€‚ğŸ˜Šã€‚These
    books are how large a transformer we did in the original paperã€‚
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè¿™ä¸ªå›¾å±•ç¤ºäº†æˆ‘ä¹‹å‰æåˆ°çš„ä¸åŒå½¢çŠ¶å­—ç¬¦ä¸­çš„ç°ä»£å¤§å°ã€‚Xè½´æ˜¯MIPçš„æƒé‡ï¼ŒYè½´æ˜¯å®½åº¦ï¼Œç„¶åä¸åŒçš„å›¾è¡¨ç¤ºä¸åŒå±‚çš„æ·±åº¦ã€‚ğŸ˜Šè¿™äº›å›¾å±•ç¤ºäº†æˆ‘ä»¬åœ¨åŸå§‹è®ºæ–‡ä¸­æ„å»ºçš„å˜æ¢å™¨æœ‰å¤šå¤§ã€‚
- en: and then boom one step further and two steps furtherã€‚this is just super massive
    transformer we did in this scalingatingã€‚Paper and with all of our tricks how much
    larger we could go a little largerã€‚Then yeah some learning rate stuff but it is
    really coolã€‚
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åâ€œç °â€è¿›ä¸€æ­¥è¿ˆè¿›ä¸€æ­¥ï¼Œå†è¿›ä¸€æ­¥ã€‚è¿™åªæ˜¯æˆ‘ä»¬åœ¨è¿™ä¸ªæ‰©å±•ä¸­çš„è¶…çº§å¤§å˜æ¢å™¨ã€‚é€šè¿‡æ‰€æœ‰çš„æŠ€å·§ï¼Œæˆ‘ä»¬èƒ½å°†å…¶åšå¾—æ›´å¤§ä¸€ç‚¹ã€‚ç„¶åæ˜¯ä¸€äº›å­¦ä¹ ç‡çš„è°ƒæ•´ï¼Œä½†è¿™çœŸçš„å¾ˆé…·ã€‚
- en: I recommend people to look at square root learning rate scheduleã€‚which is cool
    and often just mentioned as a side noteã€‚å—¯ã€‚It's also coolã€‚but I'm going to skip
    it for the interestã€‚And basically in interest of time and basically we scale it
    up a lot and of course againã€‚we get always this in Inet number a bit higherï¼Œ This
    is actually plus 2% than what we had beforeã€‚
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å»ºè®®å¤§å®¶æŸ¥çœ‹å¹³æ–¹æ ¹å­¦ä¹ ç‡è®¡åˆ’ã€‚è¿™å¾ˆé…·ï¼Œé€šå¸¸è¢«æåŠä½œä¸ºé™„å¸¦è¯´æ˜ã€‚å—¯ã€‚è¿™ä¹Ÿå¾ˆé…·ï¼Œä½†å‡ºäºå…´è¶£æˆ‘å°†è·³è¿‡å®ƒã€‚åŸºæœ¬ä¸Šè€ƒè™‘åˆ°æ—¶é—´ï¼Œæˆ‘ä»¬è¿›è¡Œäº†å¤§é‡æ‰©å±•ï¼Œå½“ç„¶å†æ¬¡å¾—åˆ°äº†ç¨å¾®æ›´é«˜çš„ImageNetæ•°å€¼ï¼Œè¿™å®é™…ä¸Šæ¯”ä¹‹å‰æé«˜äº†2%ã€‚
- en: which is very significant in this high percentage range thereã€‚But also what's
    very interesting is the fu shot again by just keep scaling up everythingã€‚we get
    super large boosting fu shot again this is imagegenenet top one accuracy and for
    exampleã€‚it' just 10 images per imnet classã€‚ğŸ˜Šï¼ŒWhich means 10000 images totaltter
    because thousand classesã€‚
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé«˜ç™¾åˆ†æ¯”èŒƒå›´å†…ï¼Œè¿™ä¸€ç‚¹éå¸¸é‡è¦ã€‚ä½†åŒæ ·æœ‰è¶£çš„æ˜¯ï¼Œé€šè¿‡ä¸æ–­æ‰©å¤§æ‰€æœ‰å†…å®¹ï¼Œæˆ‘ä»¬è·å¾—äº†è¶…çº§å¤§çš„æå‡ã€‚è¿™æ˜¯ImageNetçš„é¡¶çº§å‡†ç¡®ç‡ï¼Œä¾‹å¦‚ï¼Œæ¯ä¸ªImageNetç±»åˆ«ä»…éœ€10å¼ å›¾åƒğŸ˜Šï¼Œè¿™æ„å‘³ç€æ€»å…±10000å¼ å›¾åƒï¼Œå› ä¸ºæœ‰ä¸€åƒä¸ªç±»åˆ«ã€‚
- en: We get this big of a dropã€‚We get 85% of one accuracyï¼Œ which is whatã€‚What is
    what you typically get when using the full data set basicallyã€‚So doã€‚again up makes
    actually few short work significantly betterã€‚And then I'm going to skip on thisã€‚Wellï¼Œ
    this actually has an interesting messageã€‚ This is three times the same storyã€‚
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾—åˆ°äº†å¦‚æ­¤å¤§çš„ä¸‹é™ã€‚æˆ‘ä»¬è·å¾—äº†85%çš„å‡†ç¡®ç‡ï¼Œè¿™æ˜¯ä»€ä¹ˆï¼ŸåŸºæœ¬ä¸Šä½¿ç”¨å®Œæ•´æ•°æ®é›†æ—¶ä½ é€šå¸¸ä¼šå¾—åˆ°çš„ã€‚å› æ­¤ï¼Œå®é™…ä¸Šæé«˜äº†å°‘é‡çŸ­æœŸå·¥ä½œçš„è¡¨ç°ã€‚ç„¶åæˆ‘å°†è·³è¿‡è¿™ä¸€éƒ¨åˆ†ã€‚å¥½å§ï¼Œè¿™å®é™…ä¸Šæœ‰ä¸€ä¸ªæœ‰è¶£çš„ä¿¡æ¯ã€‚è¿™æ˜¯ä¸‰æ¬¡ç›¸åŒçš„æ•…äº‹ã€‚
- en: but measured in a slightly different wayï¼Œ which is that if you make the model
    largerã€‚it actually needs to see fewer images to get to a similarar score like
    this blue line is a tiny vision transformer and the base vision transformer in
    a large oneã€‚and the way X is the errorã€‚ so lower is betterã€‚ And actually you need
    to see still we're talking in millions of images And here it's1 hundred0 million
    imagesã€‚but still you need to see a lot fewer images with the larger modelsã€‚
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†ä»¥ç•¥å¾®ä¸åŒçš„æ–¹å¼è¿›è¡Œæµ‹é‡ï¼Œå³å¦‚æœä½ è®©æ¨¡å‹å˜å¤§ï¼Œå®ƒå®é™…ä¸Šéœ€è¦çœ‹åˆ°æ›´å°‘çš„å›¾åƒæ‰èƒ½è¾¾åˆ°ç±»ä¼¼çš„è¯„åˆ†ã€‚è¿™æ¡è“çº¿æ˜¯ä¸€ä¸ªå°å‹è§†è§‰å˜æ¢å™¨ï¼Œè€ŒåŸºç¡€è§†è§‰å˜æ¢å™¨åˆ™æ˜¯ä¸€ä¸ªå¤§å‹çš„ã€‚Xä»£è¡¨è¯¯å·®ï¼Œå› æ­¤æ•°å€¼è¶Šä½è¶Šå¥½ã€‚å®é™…ä¸Šæˆ‘ä»¬ä»åœ¨è®¨è®ºæ•°ç™¾ä¸‡å¼ å›¾åƒï¼Œè€Œè¿™é‡Œæ˜¯åäº¿å¼ å›¾åƒï¼Œä½†ä½ ä»ç„¶éœ€è¦ç”¨æ›´å¤§çš„æ¨¡å‹çœ‹åˆ°æ›´å°‘çš„å›¾åƒã€‚
- en: doesnt mean a lot less compute right because the model is larger and the slowerã€‚ğŸ˜Šã€‚But
    that's interestingã€‚And then there's some scaling laws that are popular in language
    and we I pink maybe for the first time in discriminative image learning show that
    yeahã€‚
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¹¶ä¸æ„å‘³ç€è®¡ç®—é‡å¤§å¹…å‡å°‘ï¼Œå¯¹å§ï¼Œå› ä¸ºæ¨¡å‹æ›´å¤§ï¼Œé€Ÿåº¦æ›´æ…¢ã€‚ğŸ˜Šè¿™å¾ˆæœ‰è¶£ã€‚ç„¶ååœ¨è¯­è¨€é¢†åŸŸæœ‰ä¸€äº›æµè¡Œçš„ç¼©æ”¾æ³•åˆ™ï¼Œæˆ‘ä»¬å¯èƒ½ç¬¬ä¸€æ¬¡åœ¨åˆ¤åˆ«æ€§å›¾åƒå­¦ä¹ ä¸­æ˜¾ç¤ºå‡ºè¿™ä¸€ç‚¹ã€‚
- en: they appear to be here tooã€‚Andã€‚Thenï¼Œ rightï¼Œ then we want toã€‚So I had the order
    of the slide mixed up in my headã€‚So I'm a bit surprisedã€‚But then another thread
    was that besides further scaling up the modelã€‚we want to to push even further
    into this direction ofã€‚ğŸ˜Šã€‚
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: å®ƒä»¬ä¼¼ä¹ä¹Ÿå‡ºç°åœ¨è¿™é‡Œã€‚é‚£ä¹ˆï¼Œå¯¹äº†ï¼Œæˆ‘ä»¬æƒ³è¦ã€‚æ‰€ä»¥æˆ‘è„‘ä¸­æŠŠå¹»ç¯ç‰‡çš„é¡ºåºææ··äº†ã€‚å› æ­¤æˆ‘æœ‰ç‚¹æƒŠè®¶ã€‚ä¸è¿‡ï¼Œå¦ä¸€ä¸ªä¸»é¢˜æ˜¯é™¤äº†è¿›ä¸€æ­¥æ‰©å¤§æ¨¡å‹ï¼Œæˆ‘ä»¬è¿˜å¸Œæœ›åœ¨è¿™ä¸ªæ–¹å‘ä¸Šæ›´è¿›ä¸€æ­¥ã€‚ğŸ˜Š
- en: Less hand engineering of things into the model architectureã€‚And then with the
    vision transformer transformer in generalã€‚what is the obviously most hand engineered
    part of it is the ser entrancetã€‚So it right what can we do something like more
    generic than that and less smart than that' basically and we ended up by replacing
    it essentially with just multilay perceptron thatã€‚
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å°‘çš„æ‰‹å·¥å·¥ç¨‹å°†äº‹ç‰©èå…¥æ¨¡å‹æ¶æ„ã€‚ç„¶åå¯¹äºè§†è§‰å˜å‹å™¨ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œæ˜¾ç„¶æœ€æ‰‹å·¥è®¾è®¡çš„éƒ¨åˆ†æ˜¯è¾“å…¥å±‚ã€‚å› æ­¤ï¼Œæˆ‘ä»¬èƒ½åšç‚¹æ›´é€šç”¨ä¸”ä¸é‚£ä¹ˆæ™ºèƒ½çš„äº‹æƒ…å—ï¼ŸåŸºæœ¬ä¸Šæˆ‘ä»¬æœ€ç»ˆå°†å…¶æ›¿æ¢ä¸ºä»…ä»…æ˜¯å¤šå±‚æ„ŸçŸ¥å™¨ã€‚
- en: howeverã€‚Has a little bit of structureï¼Œ but much less than self attention that
    they it give the structure or the say here fine and we're coming back to this
    plot where the question wasã€‚are't we saturating now this plot is slightly different
    we again have this bit resonate here in black and the full green line is the transformer
    and the other color also the full lines our division transformer right is this
    exactly same numbers as from before Howeverã€‚
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œã€‚å®ƒæœ‰ä¸€ç‚¹ç»“æ„ï¼Œä½†è¿œä¸å¦‚è‡ªæ³¨æ„åŠ›é‚£ä¹ˆå¤šã€‚å®ƒæä¾›çš„ç»“æ„ï¼Œæˆ–è€…è¯´åœ¨è¿™é‡Œå¾ˆç»†è‡´ï¼Œæˆ‘ä»¬å›åˆ°è¿™ä¸ªå›¾è¡¨ï¼Œé—®é¢˜æ˜¯ã€‚æˆ‘ä»¬ç°åœ¨ä¸æ˜¯åœ¨é¥±å’Œå—ï¼Ÿè¿™ä¸ªå›¾ç¨æœ‰ä¸åŒï¼Œæˆ‘ä»¬å†æ¬¡åœ¨é»‘è‰²ä¸­æœ‰è¿™ç‚¹ResNetï¼Œè€Œå®Œæ•´çš„ç»¿è‰²çº¿æ˜¯å˜å‹å™¨ï¼Œå…¶ä»–é¢œè‰²çš„å®Œæ•´çº¿ä¹Ÿæ˜¯æˆ‘ä»¬çš„åˆ†ç¦»å˜å‹å™¨ï¼Œå®é™…ä¸Šä¸ä¹‹å‰çš„æ•°å­—å®Œå…¨ç›¸åŒã€‚ç„¶è€Œã€‚
- en: now we also threw in this mix mixer architectureï¼Œ which we believe is even more
    flexible and less hand linear data transformer and as you see with less data it's
    even worseã€‚ğŸ¼Howeverï¼Œ with much more dataï¼Œ it may be surpassing the transferer
    or it may be random noiseã€‚
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬è¿˜åŠ å…¥äº†è¿™ä¸ªæ··åˆæ¶æ„ï¼Œæˆ‘ä»¬è®¤ä¸ºå®ƒç”šè‡³æ›´çµæ´»ï¼Œæ›´å°‘æ˜¯æ‰‹å·¥çº¿æ€§æ•°æ®å˜å‹å™¨ã€‚æ­£å¦‚ä½ æ‰€è§ï¼Œåœ¨æ•°æ®è¾ƒå°‘çš„æƒ…å†µä¸‹æ•ˆæœæ›´å·®ã€‚ğŸ¼ç„¶è€Œï¼Œæ•°æ®é‡å¤§å¾—å¤šæ—¶ï¼Œå®ƒå¯èƒ½ä¼šè¶…è¿‡å˜å‹å™¨ï¼Œæˆ–è€…å®ƒå¯èƒ½æ˜¯éšæœºå™ªå£°ã€‚
- en: Not clear at this pointï¼Œ rightï¼Œ because it's the only point where this happensã€‚So
    we need to go furtherï¼Œ so we use this 3 billion data setï¼Œ for exampleï¼Œ from the
    previous paperã€‚That I mentioned here and try to extend these lines to the right
    to see what happens we don't extend many of them because these are very expensive
    experiments that require a ton of patientsã€‚but we extend it two most interesting
    and it seems that it continues and that first of allã€‚
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸€ç‚¹ç°åœ¨ä¸å¤ªæ¸…æ¥šï¼Œå¯¹å§ï¼Œå› ä¸ºè¿™æ˜¯å”¯ä¸€å‘ç”Ÿè¿™ç§æƒ…å†µçš„åœ°æ–¹ã€‚æ‰€ä»¥æˆ‘ä»¬éœ€è¦è¿›ä¸€æ­¥æ¢è®¨ï¼Œå› æ­¤æˆ‘ä»¬ä½¿ç”¨è¿™ä¸ª30äº¿çš„æ•°æ®é›†ï¼Œä¸¾ä¾‹æ¥è¯´ï¼Œå°±æ˜¯æ¥è‡ªä¹‹å‰æåˆ°çš„è®ºæ–‡ã€‚æˆ‘ä»¬å°è¯•å°†è¿™äº›çº¿å‘å³å»¶ä¼¸ï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼Œä½†æˆ‘ä»¬æ²¡æœ‰å»¶ä¼¸å¾ˆå¤šï¼Œå› ä¸ºè¿™äº›éƒ½æ˜¯éå¸¸æ˜‚è´µçš„å®éªŒï¼Œéœ€è¦å¤§é‡çš„æ‚£è€…ã€‚ä¸è¿‡æˆ‘ä»¬å»¶ä¼¸äº†ä¸¤ä¸ªæœ€æœ‰è¶£çš„ï¼Œå®ƒä¼¼ä¹è¿˜åœ¨ç»§ç»­ï¼Œé¦–å…ˆã€‚
- en: yes the vision transformer keepsã€‚Increasingï¼Œ we don't have such experiment with
    the resonnet because it doesn't look promising enough to pay the cost of doing
    itã€‚But it also seems that the mixerï¼Œ what we believe is even more flexible architecture
    actually is consistently above the transformalã€‚
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: æ˜¯çš„ï¼Œè§†è§‰å˜å‹å™¨åœ¨ä¸æ–­å¢åŠ ï¼Œæˆ‘ä»¬æ²¡æœ‰è¿›è¡Œä¸ResNetç›¸åŒçš„å®éªŒï¼Œå› ä¸ºå®ƒçœ‹èµ·æ¥ä¸å¤Ÿæœ‰å‰æ™¯ï¼Œæ— æ³•æ”¯ä»˜æˆæœ¬ã€‚ä½†ä¼¼ä¹Mixeræˆ‘ä»¬è®¤ä¸ºæ˜¯æ›´çµæ´»çš„æ¶æ„ï¼Œå®é™…ä¸Šå§‹ç»ˆé«˜äºå˜å‹å™¨ã€‚
- en: which is good newsã€‚ğŸ˜Šï¼ŒAnd yeahï¼Œ it is good newsã€‚So were now right at theã€‚Time
    when I should stop right or open to more questions againã€‚Yeahã€‚I guess as a question
    and I ask a follow up on the scaling that you shown earlier it's really just my
    question I'm curious how this model size compares to model sizes worked or like
    the natural language like especially when flow from like smaller models to much
    bigger models whatever they comparable at all in terms of model size and if not
    like why do you thinkã€‚
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¯æ˜¯ä¸ªå¥½æ¶ˆæ¯ã€‚ğŸ˜Šæ˜¯çš„ï¼Œè¿™ç¡®å®æ˜¯å¥½æ¶ˆæ¯ã€‚æ‰€ä»¥æˆ‘ä»¬ç°åœ¨æ­£å¥½å¤„äºè¯¥åœä¸‹æ¥è¿˜æ˜¯å†æ¬¡å¼€æ”¾æ›´å¤šé—®é¢˜çš„æ—¶åˆ»ã€‚æ˜¯çš„ã€‚æˆ‘æƒ³é—®ä¸€ä¸ªé—®é¢˜ï¼Œå¹¶ä¸”æˆ‘æƒ³è·Ÿè¿›ä½ ä¹‹å‰å±•ç¤ºçš„ç¼©æ”¾ï¼Œè¿™å…¶å®åªæ˜¯æˆ‘çš„ä¸€ä¸ªé—®é¢˜ï¼Œæˆ‘å¾ˆå¥½å¥‡è¿™ä¸ªæ¨¡å‹çš„å¤§å°ä¸è‡ªç„¶è¯­è¨€ä¸­çš„æ¨¡å‹å¤§å°ç›¸æ¯”å¦‚ä½•ï¼Œç‰¹åˆ«æ˜¯å½“ä»å°æ¨¡å‹è½¬å‘æ›´å¤§æ¨¡å‹æ—¶ï¼Œæ— è®ºåœ¨æ¨¡å‹å¤§å°ä¸Šæ˜¯å¦æœ‰å¯æ¯”æ€§ï¼Œå¦‚æœæ²¡æœ‰ï¼Œä½ è®¤ä¸ºåŸå› æ˜¯ä»€ä¹ˆã€‚
- en: What the models for these two house Yeah actually a colleague of mine has a
    slide which I hate but he loves it's the model number of parameters in NLP and
    in vision and the question is how do you measure your model side if you just measure
    a number of parameters then these vision models are much smallerer howeverã€‚
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºè¿™ä¸¤ç§æ¨¡å‹çš„å‚æ•°æ•°é‡ï¼Œæ˜¯çš„ï¼Œå®é™…ä¸Šæˆ‘æœ‰ä¸€ä¸ªåŒäº‹æœ‰ä¸€å¼ æˆ‘ä¸å–œæ¬¢ä½†ä»–å–œæ¬¢çš„å¹»ç¯ç‰‡ï¼Œå±•ç¤ºäº†NLPå’Œè§†è§‰ä¸­çš„æ¨¡å‹å‚æ•°æ•°é‡ï¼Œé—®é¢˜æ˜¯å¦‚æœä½ åªæµ‹é‡å‚æ•°æ•°é‡ï¼Œé‚£ä¹ˆè¿™äº›è§†è§‰æ¨¡å‹å°±è¦å°å¾—å¤šã€‚
- en: ğŸ˜Šï¼ŒThe language modelsï¼Œ number of parametersï¼Œ like a huge chunk of it is in the
    dictionaryã€‚for exampleï¼Œ which for us just doesn't existï¼Œ its linear beddingã€‚which
    is trivial number of parametersã€‚So in terms of number of parametersï¼Œ it's much
    smallerã€‚My personal opinion is number of parameters doesn't mean that muchã€‚
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œè¯­è¨€æ¨¡å‹çš„å‚æ•°æ•°é‡ï¼Œå…¶ä¸­å¾ˆå¤§ä¸€éƒ¨åˆ†åœ¨å­—å…¸ä¸­ï¼Œä¾‹å¦‚ï¼Œè¿™å¯¹äºæˆ‘ä»¬æ¥è¯´æ˜¯ä¸å­˜åœ¨çš„ï¼Œå®ƒæ˜¯çº¿æ€§çš„åµŒå…¥ï¼Œè¿™æ˜¯ä¸€ä¸ªå¾®ä¸è¶³é“çš„å‚æ•°æ•°é‡ã€‚å› æ­¤ï¼Œå°±å‚æ•°æ•°é‡è€Œè¨€ï¼Œå®ƒè¦å°å¾—å¤šã€‚ä¸ªäººè®¤ä¸ºï¼Œå‚æ•°æ•°é‡å¹¶æ²¡æœ‰é‚£ä¹ˆé‡è¦ã€‚
- en: Then the other way that you could measure is maybe in terms of computeã€‚like
    how much floating point operations does it do on one data pointï¼ŸAnd in terms of
    thisã€‚it's in the same ballparkï¼Œ howeverï¼Œ last time I checkedï¼Œ which is quite a
    few months agoã€‚the largest language model was still like four times more or five
    times more division vision modelã€‚
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å¦ä¸€ç§æµ‹é‡æ–¹å¼ï¼Œä¹Ÿè®¸æ˜¯ä»è®¡ç®—æ–¹é¢æ¥è¡¡é‡ï¼Œä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ªæ•°æ®ç‚¹ä¸Šè¿›è¡Œå¤šå°‘æµ®ç‚¹è¿ç®—ï¼Ÿå°±è¿™ä¸€ç‚¹è€Œè¨€ï¼Œå®ƒä»¬åœ¨åŒä¸€ä¸ªèŒƒå›´å†…ï¼Œç„¶è€Œï¼Œä¸Šæ¬¡æˆ‘æ£€æŸ¥æ—¶ï¼Œå‡ ä¸ªæœˆå‰ï¼Œæœ€å¤§çš„è¯­è¨€æ¨¡å‹ä»ç„¶æ˜¯è§†è§‰æ¨¡å‹çš„å››å€æˆ–äº”å€ã€‚
- en: I believeã€‚ğŸ˜Šï¼ŒYeahï¼Œ so that's the two ways of measuring model sizeã€‚I don't think
    either of the ways is the one way to measure model size and I think it's actually
    an interesting research topic like how to properly measure and like order models
    in terms of capacity is not clear you know why the vision is or sorry the vision
    is much smallerã€‚
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ç›¸ä¿¡ã€‚ğŸ˜Šï¼Œæ˜¯çš„ï¼Œæ‰€ä»¥è¿™æ˜¯æµ‹é‡æ¨¡å‹å¤§å°çš„ä¸¤ç§æ–¹å¼ã€‚æˆ‘ä¸è®¤ä¸ºè¿™ä¸¤ç§æ–¹å¼æ˜¯å”¯ä¸€çš„æµ‹é‡æ–¹æ³•ï¼Œæˆ‘è§‰å¾—è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„ç ”ç©¶è¯¾é¢˜ï¼Œå¦‚ä½•æ­£ç¡®æµ‹é‡å¹¶æŒ‰èƒ½åŠ›æ’åºæ¨¡å‹å¹¶ä¸æ˜ç¡®ï¼Œä½ çŸ¥é“è§†è§‰æ¨¡å‹è¦å°å¾—å¤šã€‚
- en: I think it's just there is less interest in it and so less resources spent on
    it basically like in Google there are many moreã€‚manyï¼Œ many more groups doing research
    with language than with vision and I think we are one of the few groups that have
    access to a lot of resource and are interested in scaling up things in vision
    so much whereas in language it seems there are a lot of groups that are doing
    thatã€‚
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘è®¤ä¸ºè¿™ä¸»è¦æ˜¯å› ä¸ºå¯¹æ­¤çš„å…´è¶£è¾ƒå°‘ï¼Œå› æ­¤åœ¨è¿™æ–¹é¢çš„èµ„æºæŠ•å…¥ä¹Ÿç›¸å¯¹è¾ƒå°‘ï¼Œæ¯”å¦‚åœ¨è°·æ­Œï¼Œæœ‰æ›´å¤šçš„å›¢é˜Ÿåœ¨è¯­è¨€ç ”ç©¶ä¸Šï¼Œè€Œä¸æ˜¯è§†è§‰ç ”ç©¶ï¼Œæˆ‘è§‰å¾—æˆ‘ä»¬æ˜¯å°‘æ•°å‡ ä¸ªæœ‰å¾ˆå¤šèµ„æºå¹¶ä¸”å¯¹è§†è§‰é¢†åŸŸçš„æ‰©å±•æ„Ÿå…´è¶£çš„å›¢é˜Ÿä¹‹ä¸€ï¼Œè€Œåœ¨è¯­è¨€æ–¹é¢ä¼¼ä¹æœ‰å¾ˆå¤šå›¢é˜Ÿåœ¨è¿›è¡Œç›¸å…³å·¥ä½œã€‚
- en: ğŸ˜Šï¼ŒI think that's the main reasonï¼Œ actuallyï¼Œ It's not that we don't want to go
    beyond that orã€‚Like if we canï¼Œ we would go even moreã€‚Asonï¼Œ thank youã€‚Right so
    we are actually over time at this pointï¼Œ so anyone who has to leave please feel
    free to do so I think before we do thatã€‚Lucasï¼Œ thank you so much for joiningã€‚From
    all the way fromã€‚
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ğŸ˜Šï¼Œæˆ‘è®¤ä¸ºè¿™ä¸»è¦æ˜¯åŸå› ï¼Œå®é™…ä¸Šå¹¶ä¸æ˜¯æˆ‘ä»¬ä¸æƒ³è¶…è¶Šè¿™ä¸€ç‚¹ã€‚å¦‚æœå¯ä»¥çš„è¯ï¼Œæˆ‘ä»¬ä¼šåšå¾—æ›´å¤šã€‚é˜¿æ£®ï¼Œè°¢è°¢ã€‚å¥½å§ï¼Œæˆ‘ä»¬ç°åœ¨å®é™…ä¸Šè¶…æ—¶äº†ï¼Œæ‰€ä»¥ä»»ä½•éœ€è¦ç¦»å¼€çš„äººè¯·éšæ„ã€‚æˆ‘æƒ³åœ¨æ­¤ä¹‹å‰ï¼Œå¢å¡æ–¯ï¼Œéå¸¸æ„Ÿè°¢ä½ åŠ å…¥æˆ‘ä»¬ï¼Œä»è¿œé“è€Œæ¥ã€‚
- en: Across the ocean and we know it it's in the eveningã€‚so thank you for taking
    the free time to come and talk to us hereã€‚Yeahï¼Œ thanks for an invitationã€‚I always
    like to talk about the workã€‚ğŸ˜Šã€‚![](img/83f0bde19a507ae0c355af3acd90a0b3_8.png)
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çŸ¥é“ï¼Œæµ·çš„é‚£è¾¹ç°åœ¨æ˜¯æ™šä¸Šã€‚è°¢è°¢ä½ æŠ½å‡ºæ—¶é—´æ¥è¿™é‡Œä¸æˆ‘ä»¬äº¤è°ˆã€‚æ˜¯çš„ï¼Œæ„Ÿè°¢é‚€è¯·ã€‚æˆ‘æ€»æ˜¯å–œæ¬¢è°ˆè®ºå·¥ä½œã€‚ğŸ˜Šï¼[](img/83f0bde19a507ae0c355af3acd90a0b3_8.png)
