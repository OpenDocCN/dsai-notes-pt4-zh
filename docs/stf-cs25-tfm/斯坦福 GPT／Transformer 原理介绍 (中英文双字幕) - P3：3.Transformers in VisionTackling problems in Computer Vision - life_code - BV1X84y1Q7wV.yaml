- en: æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P3ï¼š3.Transformers in VisionTackling problems
    in Computer Vision - life_code - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/83f0bde19a507ae0c355af3acd90a0b3_0.png)'
  prefs: []
  type: TYPE_IMG
- en: Today I'm going to talk to you about vision transformers since this is all about
    transformersã€‚specifically their application for visual representation learningã€‚but
    before we jump into transformers I'm going to spend like 10 or 15 minutes giving
    you a lot of context on all of this and the specific thoughtss on the vision part
    of things because I think a majority of what you have seen and will see will be
    about languageã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83f0bde19a507ae0c355af3acd90a0b3_2.png)'
  prefs: []
  type: TYPE_IMG
- en: Rightï¼Œ so let's get startedï¼Œ my goal and that of my cross collaborators is to
    find general visual representation and you're going to soon see what that meansã€‚And
    why or what can we do if we imagine we have a general visual representationã€‚the
    hope is that with this we can kickstart all kinds of tasks that require visual
    input that means most tasks that you do when you have your eyes open basicallyã€‚Because
    if you have good understanding of what you seeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: then you can much quicker understand what's going on and what you should doã€‚å—¯ã€‚And
    eventually I have now a little kid since the year and so I really want that when
    he's grown up that there is like some kind of robotã€‚it doesn't need to be nice
    and pretty like in movies or just maybe an arm or whatever that my kid could teach
    or my parents who cannot program can teach to do some boring task that they really
    don't want to do and I believe one component of this is a good visual representation
    that generalizes to understanding the word visually everywhere it's not all that's
    required but it's one part and the part that I'm trying to pushã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So this is for context and motivation on working on general visual representation
    and one good example of a general visual representation is the humansã€‚And I'm
    going to show you what I mean by thatã€‚So here is a task that I give youã€‚there
    is three classesï¼Œ class Aï¼Œ B and Cï¼Œ and I give you five images of each classï¼Œ
    okayã€‚ğŸ˜Šã€‚![](img/83f0bde19a507ae0c355af3acd90a0b3_4.png)
  prefs: []
  type: TYPE_NORMAL
- en: And here I give you a new imageã€‚And I'm sure that by now you all know which
    class it isã€‚I'm not gonna ask because I don't actually see you if I was in the
    room I would do the raised handsã€‚but I'm sure you know it's class A nowï¼Œ okay
    this is fine we have seen millions of flowers in our lives hopefully but there
    is other kinds of pictures like this satellite images that you don't see much
    in your life some people may have never seen it some sometimes like when you fly
    or maybe on TV or in the Internet or so but it's red or rareã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but still same storyï¼Œ three classes Class A BC five images of each and I show
    you a new imageã€‚this might be a little bit less trivial than the flowerã€‚but I
    think I've spent enough time talking that by now most of you should know that
    this is class B shows this is basketball court rightï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚Alright now even more abstractï¼Œ you don't see this in real life all rightã€‚but
    still I give you images of class A and B just two to make it a bit easier here
    because you need to use your brain a little bit more and I show you this in your
    image and now I should do a little bit of smart talk to let you think and like
    you see that there is like s boxes and whatnot and by now I hope that most of
    you know that this is class A why because there is three objects in class A and
    class B is always what is it five objects no matter what they are what they look
    likeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Okayï¼Œ I think by now you more or less understand what I mean when I mean a good
    visual representationã€‚general visual representation rightï¼Œ some some I don't know
    how to call it in your brain in your eyes such that you can quickly see something
    new and understand what's going on with just a few examplesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83f0bde19a507ae0c355af3acd90a0b3_6.png)'
  prefs: []
  type: TYPE_IMG
- en: And then generalize from thatã€‚Rightï¼Œ and that's the goalï¼Œ then the next stepï¼Œ
    if we have the goalã€‚how do we measure progress towards itï¼ŸAnd this is a paper
    we did a few years ago with my collaboratorsã€‚which we call the visualual task
    adaptation benchmarkã€‚it's kind of formalization of the little game that we just
    playedã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So it's a benchmark and there is some component that you or anybody who participated
    in benchmark doesã€‚which is creating a model with some dataï¼Œ we don't really care
    what data what model and how whatnotã€‚Just your home with the motherã€‚Then we come
    with this landscape of all possible visual tasks that kind of makes senseã€‚ğŸ˜Šï¼ŒWwhich
    is a vague statement and we sample some tasks from that and this is kind of the
    task that you have just seen they were actually taken out of this task adaptation
    benchmark and we have for a first step made 19 such tasks where we try to cover
    broad types of visual tasks not just classes of natural images like these dogs
    and cats things but also of very specialized images like satellite image also
    nonclass task that involve counting like the one I showed you before right but
    that can be expressed in this simple classification API but that logically require
    some more thinkingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒSome things like distanceï¼Œ we have something with cars and with distance of
    the closest car and things like thatã€‚It should cover a broad range of variation
    and thenã€‚With the model that you came and to this benchmarkã€‚you can do some adaptation
    step on each of the data setsã€‚ one after anotherã€‚
  prefs: []
  type: TYPE_NORMAL
- en: while at the same time doesn't really matterã€‚but then you should haveï¼Œ as a
    resultï¼Œ a model thatã€‚Of this data setï¼Œ which is very smart it just has seen a
    few examples for each class that then performs where thereã€‚and then we just take
    the average score across all of these tasks and this is what we call the VTAP
    task and this is how for now we judge how good of a general visual representation
    does your model and adaptation algorithm haveï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: And now just for some neuralclï¼Œ this preparation we have words that we often
    use pretrainingã€‚sometimes we call it the upstream like upstream dataï¼Œ upstream
    trainingï¼Œ somethingã€‚so I may use this word interchangeably with pretraining and
    then there is the second part which we usually call transfer and then sometimes
    we say downstreamã€‚And the adaptation for our in principle is whatever you wantã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but for our work we almost always just use very simple fine tuning without any
    birdss and whistle because it it's simple and works well in general we try to
    do things as simple as possible that still work well and so sometimes I even just
    say like fine tuning when fine tuning that means moving from this pretrainning
    through the transferã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒAll rightï¼Œ so so far for the settingï¼Œ so far so goodã€‚Good then the question
    is how do we get there and we spend a lot of time thinking about this and trying
    different things and this is also roughly the outline of all that I have available
    to talk about which doesn't mean we're going to cover everythingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒSo I'm not going to go like to the outline exactlyã€‚but you will see this again
    and again and as you see vision transformerã€‚your transformer only comes a little
    bit laterï¼Œ there's some stuff before thatã€‚ğŸ˜Šã€‚So this one just really quickly because
    it doesn't matter for this courseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: is that we spend some time trying self supervised pre trainingã€‚which is very
    popular in language and envision only recently years become popularã€‚It doesn't
    work that way like you don't need to understand these barsã€‚but basically higher
    is better and here just look at the blue onesï¼Œ that's the VA scoreã€‚
  prefs: []
  type: TYPE_NORMAL
- en: For this few short beat up and self supervised learning performs like this bar
    we tried multiple methods and multiple models and so on it was a proper good benchmarkã€‚but
    it was a couple of years agoã€‚Then we move on to semi supervised trainingsã€‚so a
    few labeled examples and the ton of unlabeled examples that's this next blue upã€‚do
    you actually see the mouse cursorï¼Œ sorryï¼ŸWe don't see the mouse curorã€‚ Maybe I
    need to do someã€‚
  prefs: []
  type: TYPE_NORMAL
- en: canYeahã€‚Yeahï¼Œ so then semis that blue bar which is a lot higher than this other
    blue barã€‚so what this means to us is that by adding a few labeled examples we
    are able to get much better or much more general visual representationã€‚ğŸ˜Šï¼ŒThen
    I'm not going to spend more time on this and how exactly and so onã€‚but I'm going
    to move to the next one which was for us kind of a breakthrough when we figured
    out that if we just stayed up fully supervised pretrainingã€‚
  prefs: []
  type: TYPE_NORMAL
- en: then we get really much better representations than everything we've seen before
    and here I want to briefly spend some time on that one because it's the precursor
    to using vision or transform us in visionã€‚ğŸ˜Šï¼ŒSo the it is simple that there are
    tons of images on the internet and that's always what you hear is motivation for
    selfupvised or unsupervised learning rightã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but actually where these images come from there's almost always some extra information
    like surrounding the image on the web or if you collect it otherwise there's some
    extra information there that you could use as some weak source of information
    or some weak label rightï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒThen it happens that in Google there's some team that actually does this for
    productionã€‚and they have collected already a large data set with some pipeline
    that from the surrounding signals somewhat automaticallyã€‚but very noisily annotates
    the imagesã€‚And we wanted to figure out how far can we go when we scale up preachingï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: Then long story shortï¼Œ you need a couple ingredientsã€‚One is patienceã€‚I really
    like this plotã€‚This is one of the curves of just pretraining on large data with
    large models Okay doesnt the details don't really matterã€‚The just is that if I
    zoom into this little boxï¼Œ I see this here and this is the metric for the training
    like the performance in upstream that I see after spending8 GPU weeks of computeã€‚What
    does GPU week mean it means8 GPUus for a week orã€‚Sorryã€‚
  prefs: []
  type: TYPE_NORMAL
- en: one GPU for eight weeks or 8 GPUs for one week or 16 GPUs for half week and
    so on rightã€‚but this looks reasonable person would sayï¼Œ yeah there's no progress
    for a week on8 GPUs This is flat I'm gonna to stop and try something else but
    we are not reasonable So if keep're going and this is what the exact same spot
    looks like after8 GPU month of training and you can clearly see that things are
    progressing right So it may not always be obvious and you need patienceã€‚
  prefs: []
  type: TYPE_NORMAL
- en: The second thing is that you actually need to scale up everything so this was
    work done with resnets not yet with transformers as you see a lot of renet models
    here the X axis is the number of images available in vision there is this imagenet
    dataset which is a very common super common data set for 320 which has 1ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: 3 million images there's another one which has 10 times more images that's still
    public and then there is one subset from this internal group that has 300 million
    labeled imagesã€‚So the Y axis is a measure of accuracy on some tasksï¼Œ and we tried
    manyï¼Œ they all look similarã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And the dots are differently sized resonnet the blue dot is the standard resonate
    50 that everybody uses if this one you train on more data it looks promising at
    firstã€‚but if you go to even more data it looks like okay this doesn't really seem
    that useful and this is what most people have been doing for a long time and a
    lot of people even in Google were like yeah I tried this internal checkpoint on
    this kinds of data it doesn't really have that muchã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Howeverï¼Œ what we found out and in hindsight is kind of obvious is that you actually
    need to scale not just the dataã€‚but also the model here this blue dot is a gigantic
    resonnet that is slow as hellã€‚but when you scale this up together with the data
    you keep getting benefit with adding more dataã€‚And then if you do these two thingsï¼Œ
    scale up everything and be patientï¼Œ be patient could also beã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Quite scale up your patientsã€‚å—¯ã€‚Then you get a lot of benefitsã€‚So here there
    is a few shot transfer learningã€‚what I showed you before and on the X axis is
    size of the model on the Y axis is the accuracy on one of these tasksã€‚but again
    others with similar and these three different curves are pretrained with different
    data set sizesã€‚the green one being the standard oneï¼Œ you don't really see benefit
    or small benefit from going with larger models The blue one is 10 times largerã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you start seeing some slope upwardsï¼Œ but really only with this giant dataã€‚you
    start like getting better and better and better at least few shot transfer learning
    when you pretrain on more data with larger and larger modelsã€‚ğŸ˜Šï¼Œsecond benefit
    that we did not anticipate really at allã€‚but then found out is that these models
    are super robust when you scale everything up This object netã€‚
  prefs: []
  type: TYPE_NORMAL
- en: it's a data set that' specifically designed to measure robustness and it shows
    things in crazy like a chair in a bathtub and things like that and you should
    recognize it as chair And here the pink dots are basically how existing models
    and exact is again how large is the model and pink dot is existing ones from the
    literature and then these lines same color coding is what we found outã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And again you see this large data and then going to large model just gives you
    amazing benefits on like in this case out of a distribution robustã€‚ğŸ˜Šï¼ŒOkay so this
    was like amazing scale up everything be patient and get huge benefit sorry Lucas
    sorry for interrupting youã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but there is a question from a student in the class rightã€‚ğŸ˜Šã€‚Do you want to unmute
    yourself and ask it yourselfï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: Yeah I can I can ask my question can people hear me maybe there's a a one right
    once I can we just step away real quick yeah so the question that I want to know
    is what work has been done characterizing the parameters after pretrain finishes
    like the reason why I'm motivating this question isã€‚
  prefs: []
  type: TYPE_NORMAL
- en: It seems like we do this tremendous amount of pre trainingã€‚but it seems like
    we might be able to significantly reduce that if we just have smarter initialization
    schemesã€‚Yeahï¼Œ you knowï¼Œ I've been thinking this for a long time actually alsoã€‚and
    they've come to conclude that I think notã€‚ğŸ˜Šï¼ŒI think there is like two parts One
    is what I like to call handwa V the numers of the weightsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: you know that everything is in a nice range such that it can have nice input
    output functions and so on and that your optimizer can do steps that you know
    make reasonable change to the input output function but not too large and so on
    I think that is part of it and that you can get through good in it or good normalizations
    and whatnot but then I also think there is I do think that these models memorize
    a lot and thenã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒPersonallyï¼Œ I believeï¼Œ but I don't know of evidenceã€‚or so that these models
    do more kind ofã€‚you knowã€‚Remembering similarity to things they've seen in training
    and then as you grow things up they have more memory and they have seen more things
    so they should be better on more newer things because there's more similar things
    they have seen and this I don't think you can like just create one shot from initializationã€‚
  prefs: []
  type: TYPE_NORMAL
- en: But I don't have immediate points or to a paper at the top of my head now to
    answer your questionlyã€‚thank youã€‚I think we also have more questions or has posted
    on the chat and is raising his hand maybe in this audio you want to ask your question
    pleaseï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: Yeahï¼Œ for sure if I can go ahead so I just said a quick clarification on this
    chart right hereã€‚the chart number threeï¼Œ the bit L and bit N and bitS are they
    the same model architectureã€‚but just trained on different data set so the bitS
    is trained on the 1ã€‚3 million all the way to the 300 million image data set for
    bit Lã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Yes and no the architecture is here on the x axis so within one vertical slice
    these are the same architecture and then the different points are random restarts
    because when you do future learning there is a lot of variance in which few examples
    do you see and then again these next vertical slice is the same model and so on
    and as you go to the right the model gets larger and so you can see that for this
    little data going to larger model doesn't really help you much for freetraining
    and only for this giant data air means the giant data not necessarily giant model
    in this caseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒRightï¼Œ it makes a lot of senseï¼Œ thank youï¼Œ okayã€‚Do you have a question of
    I say you're raising your hand as wellï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: å–‚ä¸ªäººä½ å¥½å’¯ã€‚He yeahï¼Œ thanksã€‚What is the intuition for the upstream performance and
    figure one spiking so suddenly atï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: I guess or3 points in training Yeah right yeah Yeah yeah again like round 1ã€‚like
    I don't know that just seems like an a looking training curve like yeah what's
    the intuition behind that Yeah this is old school computer vision thing or I a
    few years ago is this when the learning rate changes and computer vision it used
    to be very common to have the learning rate in kind of staircase pattern so it's
    constant for a while and then you stopped you divide the learning rate by 10 usually
    boom smaller and then you continue and this gives you this huge jump nowadays
    people don't use this much anymore and this work was like three years ago I think
    or two or few years ago I don't remember it was very common back then and nowadays
    people use more continuously changing learning rate schedule and then you don't
    really have this sudden change anymore but if you would overlay it would be like
    more continuously but going roughly the same and then in language I think most
    people or many people useã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒThey're just linearly decreasinging learning rate schedule where also you
    don't see this effect because learning rate continuously decreasesã€‚Yesï¼Œ sounds
    goodï¼Œ thanksã€‚And then this is what because you ask for about this this dotted
    lineã€‚actually here if you're like here okay you could say okayï¼Œ but this is excessiveï¼Œ
    rightã€‚maybe it does really seem almost flatï¼Œ maybe you could have started the
    leakK earlier and earlier and earlier and then you would get the same but much
    quicker and this one shows what would happen then and you do land at much worse
    place in the end than with the patientã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒOkayï¼Œ yeahï¼Œ yeahï¼Œ that makes sense thanksã€‚Was there more question or I continueã€‚What
    do you have your answer because I need to mentionï¼Œ I don't see what I just seem
    as likeã€‚Yeahã€‚it's fineï¼Œ we can coordinate that Lucasã€‚Hiï¼Œ yeahï¼Œ so I just want
    to make sure that I'm like on the same pageã€‚so basically what you're trying to
    do is multitask learning with convolutional neural network slash LSTF right that's
    kind of like resnetã€‚
  prefs: []
  type: TYPE_NORMAL
- en: But you're doing multitask learningï¼Œ correctï¼Œ You knowã€‚where does the multitaask
    come from or where does itã€‚Because like initially like you showed like different
    I okay so there is two phases The first one is the pre training and this pretraining
    I didn't mention it yeah I just said I don't care what you do in the pretrain
    just pretrain somehow and giving me the model and then I test it on multiple tasks
    independently and I tested on multiple task means like transfer it to that task
    which in our case means fine unit just on the task and see how well it does and
    so on but it could mean other things like later we moved to just learning linear
    linear regression on top of the embeddings for each task and now doing the pretraining
    what you do is just regular supervised learning but just scaling everything up
    and regular supervised learning is justã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒWellï¼Œ not multitaskï¼Œ but multila in the sense that an image could have a couple
    labels or notã€‚but it usually doesn't have a this minority got it thanksã€‚Yeahã€‚just
    have a quick follow up about the question rather than like the discussion ratherin
    started about this its like imization or it's more memorizing the data in a pretraining
    dataset setã€‚So I know in the language side there's a quite interesting phenomenon
    that you can pretrain on a synthetic language that it doesn't have any semantic
    meaningã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but it only have structural like know paired premises or things like thatã€‚And
    that actually it gives you almost the same boost in your downstream transfer as
    a normal pretrainingã€‚So I wonder if say like so this means like for language right
    the structure seems makes a lot of contribution which can be replaced by utilizationã€‚but
    I don't know if it's an image is a different caseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: maybe have people done maybe some synthetic pretraining dataset set for image
    there was a paper I forgot the name and the authorsã€‚but it creates completely
    synthetic imageã€‚and like not even rendering of some realistic things but just
    completely patternsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: waves and shapes and so on and uses that for pretrain and then shows that they
    get almost the same performance as imate quickly they actually do this with vision
    transformers butã€‚Yeahï¼Œ they never go further or it's not hereã€‚You knowã€‚they kind
    of that you can almost get to this point hereã€‚Then it's not clear how much further
    can you go away thisã€‚ And I think probably not much furtherã€‚
  prefs: []
  type: TYPE_NORMAL
- en: But it's just me guessing they're not much firmï¼Œ I don't have evidence for itã€‚So
    I have one question and then we can continue the talk said that we think like
    a large vision models are like learning some sort of similarity the data set their
    train on so do you think like they are like beaving like protopical networks in
    a senseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: They are we having like what networks so like prototypical networks essentially
    like when you like doing fact few short learningã€‚you just say like I'm going to
    learn a network I can learn the metric spaceã€‚ğŸ˜Šï¼ŒProbably not exactlyã€‚but close
    eachã€‚å—¯ã€‚I meanï¼Œ I cannot really say because this is just some intuitive guess that
    I haveã€‚That's what they doï¼Œ but nobody really knows what the models they readã€‚ğŸ˜Šï¼Œå˜¢å•Šã€‚I
    meanã€‚
  prefs: []
  type: TYPE_NORMAL
- en: we do get much work when we do something like prototypical networks for the
    future learning with these pretrain modelsã€‚we do get much worse performance than
    when we do fine tu so there is a bit more to it thereã€‚however I don't knowã€‚What
    is this moreï¼ŸOkayï¼Œ thanksã€‚All rightï¼Œ let's continueã€‚Okayï¼Œ yeahï¼Œ soã€‚Rightã€‚and I
    didn't mentionï¼Œ but like on INe which is the top benchmark in computer vision
    with this work with the big transferã€‚
  prefs: []
  type: TYPE_NORMAL
- en: we finally were able to increase the score after there was like a long period
    of a couple years of no improvement but many attachments that you see the grade
    say the yayã€‚awesome pretraining scaling up everything and leveraging the data
    and then okay let's not care about thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒYeahï¼Œ that's okay this is just a little aside that if we are in the setting
    that I mentioned of pretraining on huge amounts of data and then testing on many
    other tasksã€‚you should of course be careful that you don't have images from the
    other tasks in your pretraining data right otherwise you have seen and training
    and then you're not really generalizing and you just pulling yourself with good
    scores and this is a real danger when we get huge amounts of data because like
    im that images can totally be in huge amounts of data right So we actually use
    the internal pipelineã€‚
  prefs: []
  type: TYPE_NORMAL
- en: there is really good at finding duplicates and also new duplicates like when
    they are shiftedã€‚rotated squeezed color change a bit whatnot it still find it
    and we use this to completely remove all images from the test dataset sets that
    we test on later and we actually found that a lot of classic just vision data
    sets have clear duplicates between the training and validation set between the
    training set ofã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒImagineNe and CFR10 and00 test sets and so onã€‚so near duplicates are quite
    widespread problem envision vision and this slide is just to say heyã€‚there are
    problems but in all that we present we actually took care that in duplicate training
    as best as we can we don't have near duplicatesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚Right now back to being like heyï¼Œ we figured out large dataã€‚a larger model
    and then things get really good and that's how we got to transformers basically
    in computer visionã€‚everything was convolutional networks for many years and basically
    there was nothing LTMM is scheme however in language we saw transformation recently
    right that everything used to be LSTM everywhereã€‚LSTM was king and then came the
    transformer and in the case when there is a lot of data available suddenly transformer
    work much better than LSTM or little data that was still not the case exactlyã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒSo what we then thought is that okayï¼Œ so we are now in this where we have
    tos of data and do we see benefit from itã€‚can we see even more benefit if we try
    also out the transform architecture and visionã€‚And that's basically what we did
    to be third there were a few other attempts that try not transformer In before
    that I don't want to detail too much here because I don't want to like point fingers
    too much but they were are not really using transformers for learning everything
    from the data it was always like get something out of a reson at first like object
    detection proposals or highle feature maps or things like that and then stick
    a little transformer on topã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but we wanted to go like all the way just transformer everythingã€‚And so we came
    up with the simplest and more naturalï¼Œ I believeã€‚way of applying transformers
    to visionï¼Œ which is you take the imageã€‚you cut it in two pieces and that's it
    like a puzzleï¼Œ tech ta patchesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And that's it Each of these patchesï¼Œ you take it and you project it into your
    embedding spaceã€‚which is the input to the transformer embedding space is just
    abstract space of let's sayã€‚768 dimensionsï¼Œ for exampleï¼Œ how do you embed itï¼Œ
    you just take the pixel values and put the linear projection projection layer
    on top them take all the pixelsã€‚ğŸ˜Šï¼ŒPlatten the vector matrix multiply into whatever
    size you want and use the same matrix for other patchesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: and here we just went the simplest ever with non overlapping patches and everything
    you can and the people later did go on and like sayã€‚heyï¼Œ this is almost a convolutionï¼Œ
    let's make proper convolutionï¼Œ let's make stack of them whatnotã€‚But this is our
    for natureï¼Œ this is just the simplest way to do it firstã€‚ğŸ˜Šã€‚Then we have these
    embedded patches and we treat them exactly literally like the were the tokens
    in language and then give them to exactly the bird transformer from language folks
    and just like in language we add this class token or I think a language is like
    end of sentence token or something and we add the positiondding embeddings to
    the tokens that can be learnedã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And then we feed all of these to a transformer encoderã€‚which has MLP head which
    reads out this class token and then maps it to softm layer for classificationã€‚for
    exampleã€‚And that's it That is the vision transformerã€‚so it's literally you take
    bird transformerï¼Œ but instead of words or sentence tokens pe in pictures transform
    this into tokens and that's itã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒAnd then just same story is before scale everything up computeï¼Œ data setï¼Œ
    model sizeï¼Œ patienceã€‚everything and see what happensï¼Œ is this good or notã€‚That
    was the question and now we can see a plot here this is similar plot as before
    the gray area is actually what were all of the bit dots before and now the bubbles
    are vision transformers of different sizes and the bubble is kind of the size
    of the model I'll do it's a bit hard to say exactly and what you can see first
    is that with little data imagenet is the 1ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: 3 million images it works worse than resonatesã€‚ğŸ˜Šï¼ŒSo if we would not believe
    in this idea and just try this veryã€‚Okayï¼Œ this is a crap ideaã€‚and13 million images
    seems not that laterã€‚Then the10 times larger data states started laying in the
    same ballpark as a resonnet and when we go to much larger data with a much larger
    transformer then we actually start outperforming this reson and we outperform
    it just by littleã€‚but this resonnet was really hard to get and is extremely clumsy
    and slow and big so we were very excited by thisã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Now we did more controlled studies and everything and one of them is like using
    subset of this same data set and there's lots of curvebsã€‚but basically just look
    at the dark gray one and the light blue one these are roughly similarly fast and
    clumsy or easy to use or difficult to use a bit which is a resonnet variant and
    with the vision transformer and what you can see vision transformer when we have
    little in quotes little data is really bad compared to resonance but as we start
    having a lot of data actually it starts outperforming the resonnet and this is
    very promising because I think everything that looks huge and a lot and so on
    now in five or 10 yearsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: it's maybe regular like 10 years ago Ied this one seemed to be huge and massive
    amount of data normal anymoreã€‚ğŸ˜Šï¼ŒSo we should look to the future and this looks
    promising for the futureã€‚Then back to the same benchmarkï¼Œ that was another lit
    jumpã€‚Yeahï¼Œ we have some questionsã€‚Yepã€‚ğŸ˜Šã€‚There is also this section about meã€‚ğŸ˜Šï¼ŒSoï¼Œ
    it'sã€‚In that orderã€‚
  prefs: []
  type: TYPE_NORMAL
- en: if you want to unmute yourself and ask the questionsã€‚Sureï¼Œ yeahã€‚and I think
    Deval already answered part of the questionã€‚but I was wondering in the input to
    these transform when you're chunking up the image into little puzzle pieces and
    then fighting themã€‚does the order of feeding these patches in matterï¼Œ like if
    you switch the orderã€‚
  prefs: []
  type: TYPE_NORMAL
- en: does the prediction maybe changeï¼ŸYeahï¼Œ that's a good questionã€‚And I actually
    have a slide on something like thisï¼Œ but not exactlyã€‚let me jump thereã€‚So first
    of allï¼Œ if the order is consistent during trainingï¼Œ rightã€‚and you don't shuffle
    the order again for each new imageã€‚
  prefs: []
  type: TYPE_NORMAL
- en: then it's literally the exact same you get the same curve saying everything
    because we don't encode the order anywayã€‚If you start randomizing the order all
    the time during trainingã€‚then performance gets quite a lot worseã€‚and let me show
    you why is the slide was on my plan to present anywaysã€‚then if you ask about let's
    jump hereï¼Œ these are this is a visualization of the position embeddingsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒWhat does it mean so in this case we had 14 by 14 patches that we cut the
    image inã€‚so it means we have also 14 by 14 position embeddings although we just
    see them as one round sequence of what is it 00ã€‚50 somethingï¼Œ or I don't knowï¼Œ140
    somethingã€‚And now each of these pictures shows the position embedding which corresponds
    to this location how similar is it to all the other position embedding so let's
    look at this one for exampleã€‚yellow means a perfectly similar like exactly the
    same and blue means opposite in terms of coine similarity so this position embedding
    is most similar to itself which is the pixel here and then the neighboring pixels
    is how similar is it to the position embeddings that correspond originally to
    the neighboring patch and we do see a very clear pattern that each position embedding
    is very similar to the embedding from its surrounding patches and we didn't implement
    any of this right we just had this position embeddings at randomly initialized
    variables and they are learned as freely as the rest of the parameters of the
    model but they learn to recover this notion of what are my neighbor patches even
    though we don't give this information anywhere at any time besides the raw image
    data in the task to please classã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ã§ã™ã¾ã€‚è¯¶ã€‚So that's pretty cool I thinkï¼Œ but it also means that if you take the
    trained model now and give in patches in a completely differently shuffled orderã€‚it's
    going to perform poorly because these learned position and beings not make sense
    anymoreã€‚ğŸ˜Šã€‚
  prefs: []
  type: TYPE_NORMAL
- en: We did try also to implement like position embeddings which encode the location
    as hard coded by us and other fancy position embeddings like relative onesã€‚but
    basically none of that really outperformed is freely learned and then the freely
    learned is simple you just random in it let it learn as part of SGD and that's
    it and so we go with that and just doing thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: we have one more question fromã€‚Heyï¼Œ yeahï¼Œ I was wondering to read it yeahï¼Œ this
    slideã€‚I think something that's really interesting is we're talking about scaling
    up the data and scaling up the model of the thought as wellã€‚But it seems like
    you're reaching an passing show right when you keep doing this thingã€‚So I'm curious
    if you have any thoughts on that like is that or these points just look like that
    or is there kind of a best you can sort of do where we keep yeah whatever pretrain
    of like the data or the parameters you're actually not going to get Yeah I have
    another slide but much further in the talk about that where I would like to not
    jump on it if you don't mind and then maybe in 10 15 minutes we will be thereã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Sounds greatï¼Œ thanksã€‚Yeahï¼Œ but maybe to be a bit optimisticã€‚it does seem like
    the transformers have a better slope here in the end and the resonates plateau
    earlierã€‚ğŸ˜Šï¼ŒSorryï¼Œ Lucasï¼Œ I didn't mean did not mean to interrupt Are there any
    questions before we proceed Yeahã€‚can I ask my question real quick Sorry about
    that So what I'm curious to know is how does this VIT compare to if you equi a
    convnetã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So for exampleï¼Œ Resnetã€‚With an attention mechanismã€‚Like how much of this is
    due to the structure of a transformer and the particular way it operates versus
    just the benefit of attention a vanilla conf does not have access toã€‚Yeahï¼Œ so
    this is manyï¼Œ this has been tried many times beforeã€‚And the first time that I
    know of was actually from I mis pronounce's nameã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but I mean there the inventor of Resnet and some of his colleaguesã€‚they called
    it non blocker networksã€‚This was way I think even before the transform paperã€‚if
    I remember correctly and they basically inserted as blocks at various locations
    in the resonnet and then they showed improvementã€‚but it was likeã€‚Tinny improvements
    thatã€‚ It was a cool block and a simple paperã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but it was not really worth itã€‚And it people usually place the attentionã€‚I you
    can imagine if you place the attention just on the pixels and don't do this patch
    cuttingã€‚this is way too expensive computationï¼Œ Rightï¼Œ rightï¼Œ If you have 2 to
    4 by 2 to 4 pixelsã€‚that's likeã€‚Yeah I cannot do this in my headï¼Œ I don't know
    40000 or so maybe pixels attending to 40ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: 000 others that doesn't work so people just do it in the very high and very
    final layers of the resnet like where it's maybe 7 by7 and then they add a bit
    of sprinkle a bit of attention thereã€‚but then you don't really get much benefit
    of skating because it's essentially still a resnetã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And there is in resonancesatesï¼Œ there is this block called Sic S that has been
    getting really or has gotten really popular and improves the resnet quite a bitã€‚and
    that is also kind of a form of attentionï¼Œ but like nicely tailored to imagesã€‚I'm
    not doing it's arguableã€‚But yeahï¼Œ it has been tried many times beforeã€‚but it just
    it doesn't show or it hasn't been shown to have this scaling benefit as much as
    theã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So I think I'm missing something critical hereï¼Œ which is you just contract or
    it's computationally difficultã€‚But eventuallyallyï¼Œ you're at a low level in the
    resnetã€‚But why is it any different than doing an attention layer in division vision
    transformerï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: Because we cut the patches firstï¼Œ so we have maybe 14 by 14 patchesï¼Œ which is
    not that muchã€‚Okayã€‚But I'm confusedï¼Œ likeã€‚You could imagine not at a high level
    and not at a high layer in the resonnetã€‚but at a relatively true layer after you've
    applied like one or two convolutional filtersã€‚Convolutional layersï¼Œ excuse meï¼Œ
    then you have something size to the patchesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: That's still 50 by 50 of the early layersï¼Œ and that's but 50 by 50 is significantly
    less than not like 400 by 400 or 100ã€‚But it's still 2ï¼Œ500 tokens attending to
    2500 tokensï¼Œ which yeah I meanï¼Œ it's a lotã€‚but it's not comparableã€‚I don't knowã€‚Okayï¼Œ
    cool Thank youã€‚ Yeahï¼Œ I meanï¼Œ it could be triedã€‚ Okayã€‚maybe another answer to
    your question is then we're slowly getting to this my next slide after the set
    of questions where we do try something almost like what you said have a very small
    part of the resnet and then stick transformer on top of it but like the full transformer
    encoder on top of it and not just sprinkle a few attention layers and then continue
    at columns and so on and this is this process and we call them hybrid but that's
    almost literally what you said actually like a few early layers from the resnet
    and with different varying amount and then stick the whole transform encoderã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒAnd thisã€‚Seem to work well tooï¼Œ especially for the when you exactly in this
    case is amount of compute so for the little compute it seems to work wellã€‚but
    then the scaling behavior of the pure renet is a little better so we focused on
    that I think we later tried also hybrid further to the right and it was a bit
    lower but it was after the paper so it's not on this plot which I just put out
    of the paper but you can already see the trend hereã€‚
  prefs: []
  type: TYPE_NORMAL
- en: å—¯ã€‚Yeahï¼Œ so if you don't scale all the way upï¼Œ then this is a totally reasonable
    thing to do have a little bit of resnet and then the encoder from Transerã€‚Do you
    wantan to ask your questionã€‚Yeahï¼Œ I was just wondering about basicallyã€‚there's
    like like a short section of the paper about like fine tuning and like higher
    resolutionã€‚And in that caseï¼Œ rightï¼Œ like the pretrain like position and embeddingï¼Œ
    Sorry or like skewed rightã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And then it's basically says that you guys are like interpoolatingã€‚Can you like
    talk on that like a little bitï¼Œ like how do you interpolate what's going onã€‚Yeahã€‚actuallyï¼Œ
    when when I check the slides earlier todayï¼Œ I was likeï¼Œ ohã€‚it would be good to
    have a slide on that And we don't have an nice visualization in the paper eitherã€‚
  prefs: []
  type: TYPE_NORMAL
- en: because it's a bit difficult to explainï¼Œ but this is the best startingã€‚Poin
    it haveã€‚So if you want to increase the resolution of the image means and you keep
    the patch size fixedã€‚it means you have more patches suddenly rightï¼Œ And then as
    you sayã€‚the fashion embeddings like how what do you even use as a position embeddingsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: right and basically you can see here that we see that they learn a very regular
    structureï¼Œ rightã€‚we don't really know what is the structure of these position
    embeddings that Iã€‚We just see the similarity to each other and that it is very
    regularã€‚And so this gave us the intuition that we may be able to just take themã€‚Kind
    of imaging these boxesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: they slide apart and new boxes appear between them and they are just the interpolation
    of the surrounding onesã€‚And that's basically what we do with the position and
    beddingsã€‚We create new ones where they are missing ones because we need more and
    by interpoolating the surrounding or more precisely we basically see them as a
    picture in this caseã€‚14 by 14 with 700 something channels or whatever is the dimensionality
    and then we basically resize this like you would resize the picture by near interpolationã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And that way we will get more in new position em that we don't understand where
    they areã€‚but they follow the same pattern as the learned ones just at a higher
    resolutionï¼Œ basicallyã€‚ç‚¹è¿‡å»ã€‚Yeahï¼Œ that's a good questionã€‚So when no you creating
    theã€‚As input right now you're doing ajectã€‚Recently firstã€‚Has there been work to
    do that wayï¼Œ has everything weird good there right very close to each otherï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: Yeahï¼Œ there were quite a few wordsã€‚They tried varying ugly thingsã€‚One that I
    especially liked recentlyã€‚It's called early convolutions help transformers C better
    or is something like thatã€‚And they basically sayï¼Œ okayï¼Œ instead of this linear
    projectionã€‚instead of this one big linear projectionã€‚ we replace it by a stack
    of three by three convolution with a straight2ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And then they have also non nonlinearities between a normalization between themã€‚But
    such that's the overall stride is the same as the patch the dispatyifyingã€‚So the
    outcome will then be the same dimensionality as after this patch cutting and then
    projectingã€‚And then they showed thatã€‚ğŸ˜Šï¼ŒSupposedly it makes it a bit easier to
    optimize in the sense that more optimized settings are good settingsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: In many scenariosï¼Œ it performs the sameã€‚But like more robustly to get thereã€‚And
    they also show some scenarios where this performs much betterï¼Œ like for exampleã€‚when
    pretraining onï¼Œ actually when they pretrain on more dataï¼Œ that seems to perform
    even betterã€‚å—¯ã€‚I havent played a bit with it and tried to reproduce itï¼Œ I don't
    have it fully reproduce itã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but I don't see as much benefit as in the paper yetã€‚but that's not to say that
    the paper is wrong just left I didn't get there yetã€‚That is one example of themï¼Œ
    there are other papers that do stuffã€‚but this one I found especially interesting
    because it's simpleã€‚Thank youã€‚All rightã€‚
  prefs: []
  type: TYPE_NORMAL
- en: continue we don't have more questionsã€‚Alrightï¼Œ then let's seeã€‚Yeahã€‚I have like
    three more interesting details from the paper and thenã€‚Depending on if you want
    more discussion or more contentï¼Œ I have more contentã€‚like also the question aboutï¼Œ
    does it satur it here or notï¼Ÿå—¯ã€‚Alrightã€‚
  prefs: []
  type: TYPE_NORMAL
- en: so another interesting thing that we had in the paperï¼Œ but it is buried in the
    appendixã€‚And then follow up papers from others have been written on this by nowï¼Œ
    actuallyï¼Œ is likeã€‚how should we scale these transformers right in the the high
    level shape of the transformerã€‚There's lots of settings that you could choose
    And we actually tried many of themã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So we started with the like reasonable medium size transformer is built in the
    middleã€‚And then we varied things one by oneï¼Œ Such thatï¼Œ we always double the computeã€‚Soï¼Œ
    for exampleã€‚this pink lineã€‚If we go to the right at this point increases the widthã€‚ğŸ˜Šã€‚Such
    that we double the compute X axis easy is a computeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Relative to this starting point and we have all of these different settingsï¼Œ
    there's the widthã€‚which is the how wide is are the vectors with which cell function
    is doneã€‚which is for the base model 768 and then most larger or smallerã€‚There
    is like meï¼Œ as you seeã€‚scaling this does not seem promisingï¼Œ so we didn't scale
    that muchã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Then there's other things like the width of the multilayer perceptron in or
    some people call it the one by one convolution in these attentions and this seems
    to scale a bit nicer this orange part I actually wonder where it went to the left
    I don't remember I don't know if it's hidden somewhere or if you just didn't scale
    it down but any risk then another thing to scale which does not exist in the transformers
    from text is the patch size as you make the patch smaller you get more and more
    tokens out of an image and thus more and more compute capacityã€‚
  prefs: []
  type: TYPE_NORMAL
- en: This is the green oneï¼Œ which also seems to scale nicelyã€‚ Then the depth is an
    interesting oneã€‚is yellow oneã€‚And this is the number of encoder blocksã€‚as we scaleï¼Œ
    it first seems like wowã€‚this is the thing you want to scaleï¼Œ but then it does
    seem to plateau and it scales really badly if you decrease the depthã€‚ğŸ˜Šï¼ŒSo that's
    not a good thing to decreaseã€‚ Howeverï¼Œ the width seems to be a good thing to decrease
    if you want to go through smaller models and then the blue is just scaling everything
    together such that the compute is kept like everything by roughly the same amountã€‚
  prefs: []
  type: TYPE_NORMAL
- en: that seems to scale nicely as well as the restã€‚And these is relatively simple
    or at least conceptuallyã€‚So we like thisã€‚So we went to that whenever we scale
    up or down the modelsã€‚ğŸ˜Šï¼Œå—¯ã€‚This I want I really like is the inference speedï¼Œ because
    if you have the image size of 2 to4 pixelsã€‚it actually means you have 2 to 4 by
    2 to4 pixelsï¼Œ rightã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So if you have let's then you patchify it with 16 by 16 patchï¼Œ for exampleï¼Œ
    patch sizeã€‚then you haveã€‚ğŸ˜Šï¼Œ14 by 14 patchesã€‚So that's the sequence length is actually
    150ï¼Œ right and thenã€‚On top of the sequence lengthï¼Œ you have the self attention
    operationï¼Œ which is square againã€‚So overall with the image with respect to image
    sizeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: the self attention operation is to the fourth power what zip called quaticã€‚So
    that is really bad like everybody who sees all of something to the fourth is like
    what the hell are you doing this is never gonnaã€‚So we checked what does it look
    like in practice with the image sizes that we operate in and this is what we see
    here And the way A axis is the how fast it goes basically how fast it does inference
    and on the X axises is verying the input sizeã€‚ğŸ˜Šï¼ŒAnd thisã€‚Or this means it doesn't
    look so bad yetã€‚Basicallyã€‚
  prefs: []
  type: TYPE_NORMAL
- en: when you go here to the 512 to the real laï¼Œ then you see that the transformers
    actually start going down a lot more than the renetsã€‚But in this reasonable image
    sizeï¼Œ let's call it very typicalã€‚it doesn't seem so bad in practice yetï¼Œ so we're
    not getting hit by the evil big old yetã€‚But as we go largerï¼Œ it will likely be
    a problem and there have been a lot of follow up works trying to make that betterã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Rightã€‚Thenã€‚å¯¹å¯¹å¯¹ã€‚This is the last one from the original it paper this is looking
    at the input quotes receptive field sizeã€‚so in the self attention operation how
    far ago do heads typically attemptï¼ŸAnd here on the X axisã€‚we see the layer in
    the network to the right is more towards the outputã€‚the classes and to the left
    is more towards the input the patchesã€‚ğŸ˜Šã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And the y axis is how far on average acrossï¼Œ I think the whole validation setã€‚does
    the self attention look and does look means that the peak of the cell attention
    or the maxã€‚how far is it weighï¼Œ something like thatã€‚ğŸ˜Šï¼Œå—¯ã€‚And each dog is a different
    head because we can use multi head self attention right and so what this shows
    is that in the early layers actually you have some heads that go look farã€‚but
    also a lot of heads that look very nearby them so locally and as we go deeper
    in the model we only are left with heads that on average look furtherã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒSoã€‚Is just some kind of analysis there is not immediately action to take about
    thisã€‚but it iss interesting to see that earlier layersã€‚they learn a mixture of
    looking to a local neighborhood and looking globally and later layers only look
    globally anymoreã€‚ğŸ˜Šï¼ŒRightã€‚So this is about the original vision transformersã€‚å—¯ã€‚
  prefs: []
  type: TYPE_NORMAL
- en: No I don't know how long we want me to continue speaking or discussing I have
    a couple options that I can talk about which is one project that was furthers
    scalingating ups and this one also has the answer to the I can also jump straight
    to the answer if you don't want to hear the rest but to the question of like'
    how does it continue to the right are we satparating There is another project
    about how to train vision transformers when you don't have massive amounts of
    data can you still do itã€‚
  prefs: []
  type: TYPE_NORMAL
- en: is it reasonable or is it maybe just unreasonable to doã€‚This one is maybe too
    unrelatedã€‚Let's not talk about thisã€‚And the last one is like I talk all about
    these benefits of a really large model when you pret them on lotss of dataã€‚Okayï¼Œ
    that's nice thats how we get a good model but then actually using a model that
    is massive is not fun at all Like it doesn't fit on your GP you need like multiple
    GPUus to even use it So people are not happy to use it and usually still go back
    to small each modelsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: even though you know like larger model should be betterã€‚what can we do about
    itã€‚ğŸ˜Šï¼Œå—¯ã€‚That's another project we hadï¼Œ she's about thisã€‚Soã€‚I would say it's up
    to you guys what you prefer to doï¼Œ or if you have plenty of questionã€‚we can continue
    with the questions now because I think now the original one hour would be overrã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Rightã€‚So I think one suggestion was like we can continue the talk and we would
    also be recording it so people that can like just like go and see it if they missed
    or something so we could do thatã€‚Yeahï¼Œ the other thing is two people have their
    hands raised so we canã€‚Okay questionsions for usã€‚
  prefs: []
  type: TYPE_NORMAL
- en: After you guysã€‚And fight hide the wayã€‚So you guys want to ask your questionsï¼ŸYeahã€‚I
    just had a pretty basic questionï¼Œ so if an object lies on the border between the
    patchesã€‚does that impact the model's performance in any wayï¼ŸYeahï¼Œ I meanï¼Œ that's
    not a basic questionã€‚It's a good questionã€‚There is a mix of answers so one is
    we didn't specifically go and test this It would be an interesting thing to test
    in a very controlled way with some of the trained models that's for sure The other
    thing is thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: When you have massive data set like 300 million images it's an insane amount
    I used to try to conceptualize how much is image net 1 million images and I think
    I did the math is like if you go to an image and look at all of the images each
    image for a couple of seconds you were sitting there for a month or something
    like that don't rememberã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but 300 million is just insanely massive and then on top of that we do actually
    use random augmentations like random crop out of the imageã€‚So I would say it's
    the default that you see objects that don't fall on a patch during the training
    already and if you look at here basically this is the standard model like how
    the patches are when we have 14 by 14 they look roughly this size alsoã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Then an object is usually scattered across many patches actuallyã€‚because objects
    in typical images are relatively large right people don't take a picture where
    the object of interest is super tiny in the corner so that's the default that
    you see during pretraining and so I believe that the model just learns to do that
    much better actuallyã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Then the other answer to the question is likeï¼Œ okayã€‚maybe if you did some nicer
    thing than this very crude patch cuttingï¼Œ like for exampleã€‚this con stack of convolutions
    that I mentionedï¼Œ maybe this is even better could beã€‚Thank youã€‚Yeahã€‚so you mentioned
    thatã€‚Or transformersï¼Œ or at least you mentioned in the paper thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: They laugh like local caity and likeã€‚ã‚ã€‚I was just thinkingï¼Œ are these sort ofï¼ŸPropertyã€‚å—¯å—¯ã€‚ç³»ã€‚And
    it's especially when you're in theã€‚Why is it thatï¼Ÿä½ ä¸ªä¸è¯´ã€‚å—¯å“¼ã€‚ğŸ˜Šï¼Œå—¯ã€‚The audio was not
    that goodã€‚but I believe I understood the question is that we say that transformer
    lack locality bias or prior or whateverã€‚and why is this even something that we
    wantï¼Œ rightï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: Wouldn't we want our models to know about locality if they are about pictures
    in the first place yes and noã€‚so this that's why I give the context in the beginning
    this is all about what happens when you scaling things upã€‚Andã€‚Specificallyï¼Œ whatã€‚In
    ideal wordï¼Œ at least in our mindï¼Œ we want gigantic amounts of dataã€‚and we believe
    that it will just keep growing as the years go by and there will be more and more
    data just generally thereã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And then we want the model to have as little ofã€‚Our thinking built inã€‚because
    what we may think that is good to solve the task may actually not be best to solve
    the taskã€‚You knowï¼Œ maybe like a analogy would be likeï¼Œ what was it alphago that
    made some moves that experts would say this is crazyã€‚This is a silly moveï¼Œ But
    it actually then was much betterã€‚ And in a similar wayã€‚
  prefs: []
  type: TYPE_NORMAL
- en: we want to encode as little as possible into the modelã€‚such that if we just
    throw massive amounts of data and the difficult task at itã€‚that it might find
    things that are even better that we didn't think of beforeã€‚ğŸ˜Šã€‚This is our approach
    because we believe that like as I mentionedï¼Œ I think alreadyã€‚
  prefs: []
  type: TYPE_NORMAL
- en: what seems massive and excessive now will be the norm in five years or soã€‚so
    that's where we want to go and look what's the direction howeverã€‚if you want to
    like just get something working now and don't have massive amounts of data and
    don't want to use pretrain model for some reason whichã€‚Always use a pre modelï¼Œ
    but if you don't want toï¼Œ then it makes utter sense to believe in some of your
    prior intuition and knowledge of what should probably help the model like localityã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼Œå—¯ã€‚I hope this answered your questionã€‚The suppose this is a PowerPotã€‚Or sort
    of point we aboutã€‚Thankã€‚W mean like any vision taskï¼Œ like isn't that sort ofã€‚Like
    I don't knowã€‚maybe I'm not seeing lateã€‚Thatly why we do not want food and thatã€‚You
    may be elaboraterating on thatï¼Œ but why is it that we don' wantï¼Ÿ
  prefs: []
  type: TYPE_NORMAL
- en: Like locality or what translation thatã€‚Wellï¼Œ ideallyã€‚we want the model that
    is powerful enough to learn about this concept itself if it is useful to solve
    the taskã€‚if it's not useful to solve the task thenã€‚If we hard code it inã€‚there
    is no way for the model not to do thisï¼Œ rightï¼Ÿå—¯ã€‚That is ideally the outcome in
    a similar wayã€‚
  prefs: []
  type: TYPE_NORMAL
- en: alsoï¼Œ that in languageï¼Œ you knowï¼Œ it seemed to be nonsense to not encode the
    from left to right direction of textã€‚like in R Sã€‚But then comes transformer and
    just doesn'tã€‚And works much better if you throw a lot of data at itã€‚and it recovers
    that plus less some more or a more flexible variant of it or something like that
    that is even better for survey taskã€‚
  prefs: []
  type: TYPE_NORMAL
- en: So basicallyï¼Œ the human being thatã€‚We are not as smart to design the thingã€‚The
    model in the way that will be best for the taskã€‚let's rather give it all the flexibility
    and all the data it needs to figure out what is the best way of solvinging the
    taskã€‚I really there is a philosophy of approaching itã€‚ I'm not saying this is
    the only true wayï¼Œ rightã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Okayï¼Œ so we have around seven minutes left before the scheduled end of the talkã€‚And
    Lucasã€‚we want to be mindful of your time as well because it's it is evening where
    you are soã€‚One thing we could do is you could I don't see any more questions right
    now so you could actually sort of go over the last few bits maybe skipping through
    the details and just talking about the final results I going do these two on the
    high level and those two that are still very very tied to transformers and answer
    some question that happened before like the first question was like okay are we
    saturating yes or no andã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Hereï¼Œ noã€‚This was the bit on this benchmark from the original transforming paperã€‚but
    then it's like these transformers when we use themã€‚which just notice they have
    really nice scaling properties and they seem actually to be easier to scale upã€‚Without
    paying massive compute as much as resonancesonates just from gut feelinging from
    us having experience with bothã€‚
  prefs: []
  type: TYPE_NORMAL
- en: And so we went and look what happens if we scale vision transformer just as
    far up as we possibly can and we spent quite a lot of our blood into making this
    happen one part of it is scaling the data set so we went back to this Google internal
    team that this 300 million data set is just one out of many that they work with
    and we asked around and they basically had the 3 billion like 10 types largerggger
    data set that we could also like play around with so they go and scale up the
    data set andã€‚
  prefs: []
  type: TYPE_NORMAL
- en: This just showingï¼Œ yesï¼Œ just scaling up the data set and switching it gives
    you benefitsã€‚But that's not all of itã€‚Then the next thing is we needed to figure
    out how to use less memory on device like on view or TPUã€‚because already previously
    with this scoreï¼Œ we fitted the model as much as we could withã€‚So we did a lot
    of clicks that I will skip for now and are able to scale much largerã€‚ This is
    likeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒThis plot shows the size of the modern in the different shape characters that
    I mentioned beforeã€‚like the weight of the MIP on X axisesï¼Œ the surf at width on
    the Y axisesã€‚and then the different plots are different layers for the depthã€‚ğŸ˜Šã€‚These
    books are how large a transformer we did in the original paperã€‚
  prefs: []
  type: TYPE_NORMAL
- en: and then boom one step further and two steps furtherã€‚this is just super massive
    transformer we did in this scalingatingã€‚Paper and with all of our tricks how much
    larger we could go a little largerã€‚Then yeah some learning rate stuff but it is
    really coolã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I recommend people to look at square root learning rate scheduleã€‚which is cool
    and often just mentioned as a side noteã€‚å—¯ã€‚It's also coolã€‚but I'm going to skip
    it for the interestã€‚And basically in interest of time and basically we scale it
    up a lot and of course againã€‚we get always this in Inet number a bit higherï¼Œ This
    is actually plus 2% than what we had beforeã€‚
  prefs: []
  type: TYPE_NORMAL
- en: which is very significant in this high percentage range thereã€‚But also what's
    very interesting is the fu shot again by just keep scaling up everythingã€‚we get
    super large boosting fu shot again this is imagegenenet top one accuracy and for
    exampleã€‚it' just 10 images per imnet classã€‚ğŸ˜Šï¼ŒWhich means 10000 images totaltter
    because thousand classesã€‚
  prefs: []
  type: TYPE_NORMAL
- en: We get this big of a dropã€‚We get 85% of one accuracyï¼Œ which is whatã€‚What is
    what you typically get when using the full data set basicallyã€‚So doã€‚again up makes
    actually few short work significantly betterã€‚And then I'm going to skip on thisã€‚Wellï¼Œ
    this actually has an interesting messageã€‚ This is three times the same storyã€‚
  prefs: []
  type: TYPE_NORMAL
- en: but measured in a slightly different wayï¼Œ which is that if you make the model
    largerã€‚it actually needs to see fewer images to get to a similarar score like
    this blue line is a tiny vision transformer and the base vision transformer in
    a large oneã€‚and the way X is the errorã€‚ so lower is betterã€‚ And actually you need
    to see still we're talking in millions of images And here it's1 hundred0 million
    imagesã€‚but still you need to see a lot fewer images with the larger modelsã€‚
  prefs: []
  type: TYPE_NORMAL
- en: doesnt mean a lot less compute right because the model is larger and the slowerã€‚ğŸ˜Šã€‚But
    that's interestingã€‚And then there's some scaling laws that are popular in language
    and we I pink maybe for the first time in discriminative image learning show that
    yeahã€‚
  prefs: []
  type: TYPE_NORMAL
- en: they appear to be here tooã€‚Andã€‚Thenï¼Œ rightï¼Œ then we want toã€‚So I had the order
    of the slide mixed up in my headã€‚So I'm a bit surprisedã€‚But then another thread
    was that besides further scaling up the modelã€‚we want to to push even further
    into this direction ofã€‚ğŸ˜Šã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Less hand engineering of things into the model architectureã€‚And then with the
    vision transformer transformer in generalã€‚what is the obviously most hand engineered
    part of it is the ser entrancetã€‚So it right what can we do something like more
    generic than that and less smart than that' basically and we ended up by replacing
    it essentially with just multilay perceptron thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: howeverã€‚Has a little bit of structureï¼Œ but much less than self attention that
    they it give the structure or the say here fine and we're coming back to this
    plot where the question wasã€‚are't we saturating now this plot is slightly different
    we again have this bit resonate here in black and the full green line is the transformer
    and the other color also the full lines our division transformer right is this
    exactly same numbers as from before Howeverã€‚
  prefs: []
  type: TYPE_NORMAL
- en: now we also threw in this mix mixer architectureï¼Œ which we believe is even more
    flexible and less hand linear data transformer and as you see with less data it's
    even worseã€‚ğŸ¼Howeverï¼Œ with much more dataï¼Œ it may be surpassing the transferer
    or it may be random noiseã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Not clear at this pointï¼Œ rightï¼Œ because it's the only point where this happensã€‚So
    we need to go furtherï¼Œ so we use this 3 billion data setï¼Œ for exampleï¼Œ from the
    previous paperã€‚That I mentioned here and try to extend these lines to the right
    to see what happens we don't extend many of them because these are very expensive
    experiments that require a ton of patientsã€‚but we extend it two most interesting
    and it seems that it continues and that first of allã€‚
  prefs: []
  type: TYPE_NORMAL
- en: yes the vision transformer keepsã€‚Increasingï¼Œ we don't have such experiment with
    the resonnet because it doesn't look promising enough to pay the cost of doing
    itã€‚But it also seems that the mixerï¼Œ what we believe is even more flexible architecture
    actually is consistently above the transformalã€‚
  prefs: []
  type: TYPE_NORMAL
- en: which is good newsã€‚ğŸ˜Šï¼ŒAnd yeahï¼Œ it is good newsã€‚So were now right at theã€‚Time
    when I should stop right or open to more questions againã€‚Yeahã€‚I guess as a question
    and I ask a follow up on the scaling that you shown earlier it's really just my
    question I'm curious how this model size compares to model sizes worked or like
    the natural language like especially when flow from like smaller models to much
    bigger models whatever they comparable at all in terms of model size and if not
    like why do you thinkã€‚
  prefs: []
  type: TYPE_NORMAL
- en: What the models for these two house Yeah actually a colleague of mine has a
    slide which I hate but he loves it's the model number of parameters in NLP and
    in vision and the question is how do you measure your model side if you just measure
    a number of parameters then these vision models are much smallerer howeverã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒThe language modelsï¼Œ number of parametersï¼Œ like a huge chunk of it is in the
    dictionaryã€‚for exampleï¼Œ which for us just doesn't existï¼Œ its linear beddingã€‚which
    is trivial number of parametersã€‚So in terms of number of parametersï¼Œ it's much
    smallerã€‚My personal opinion is number of parameters doesn't mean that muchã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Then the other way that you could measure is maybe in terms of computeã€‚like
    how much floating point operations does it do on one data pointï¼ŸAnd in terms of
    thisã€‚it's in the same ballparkï¼Œ howeverï¼Œ last time I checkedï¼Œ which is quite a
    few months agoã€‚the largest language model was still like four times more or five
    times more division vision modelã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I believeã€‚ğŸ˜Šï¼ŒYeahï¼Œ so that's the two ways of measuring model sizeã€‚I don't think
    either of the ways is the one way to measure model size and I think it's actually
    an interesting research topic like how to properly measure and like order models
    in terms of capacity is not clear you know why the vision is or sorry the vision
    is much smallerã€‚
  prefs: []
  type: TYPE_NORMAL
- en: I think it's just there is less interest in it and so less resources spent on
    it basically like in Google there are many moreã€‚manyï¼Œ many more groups doing research
    with language than with vision and I think we are one of the few groups that have
    access to a lot of resource and are interested in scaling up things in vision
    so much whereas in language it seems there are a lot of groups that are doing
    thatã€‚
  prefs: []
  type: TYPE_NORMAL
- en: ğŸ˜Šï¼ŒI think that's the main reasonï¼Œ actuallyï¼Œ It's not that we don't want to go
    beyond that orã€‚Like if we canï¼Œ we would go even moreã€‚Asonï¼Œ thank youã€‚Right so
    we are actually over time at this pointï¼Œ so anyone who has to leave please feel
    free to do so I think before we do thatã€‚Lucasï¼Œ thank you so much for joiningã€‚From
    all the way fromã€‚
  prefs: []
  type: TYPE_NORMAL
- en: Across the ocean and we know it it's in the eveningã€‚so thank you for taking
    the free time to come and talk to us hereã€‚Yeahï¼Œ thanks for an invitationã€‚I always
    like to talk about the workã€‚ğŸ˜Šã€‚![](img/83f0bde19a507ae0c355af3acd90a0b3_8.png)
  prefs: []
  type: TYPE_NORMAL
