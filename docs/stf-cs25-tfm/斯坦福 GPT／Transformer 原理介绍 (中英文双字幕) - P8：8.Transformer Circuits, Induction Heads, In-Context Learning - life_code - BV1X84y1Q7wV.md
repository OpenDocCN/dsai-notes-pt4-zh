# æ–¯å¦ç¦ GPTï¼Transformer åŸç†ä»‹ç» (ä¸­è‹±æ–‡åŒå­—å¹•) - P8ï¼š8.Transformer Circuits, Induction Heads, In-Context Learning - life_code - BV1X84y1Q7wV

![](img/ee54ecf5185d58647f4bc57e2de3efea_0.png)

![](img/ee54ecf5185d58647f4bc57e2de3efea_1.png)

Thank you all for having me it's exciting to be here one of my favorite things is talking about what is going on inside neural networks or at least what we're trying to figure out is going on inside neural networks so it's always fun to chat about thatã€‚

ğŸ˜Šï¼Œå—¯ã€‚Oh goshï¼Œ I have to figure out how to do thingsï¼Œ okayã€‚

What what I want okay there we go now now we are advancing slides that seems promising so I think interpretly means lots of different things to different people it's a very a very broad term and people mean all sorts of different things by itã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_3.png)

And so I wanted to talk just briefly about the kind of interpretability that I spent my time thinking aboutã€‚

 which is what I'd call mechanistic interpretability so most of my work actually has not been on language models or on RNNs or transformers on understanding vision conves and try to understand how do the parameters in those models actually mapped to algorithms so you can like think of the parameters of a neural network as being like a compiled computer program and the neurons are kind of like variables or registers and somehows there are these complex computer programs that are embedded in those weights and we'd like to turn them back in to computer programs that that humans can understand it's a kind of kind of reverse engineering problemã€‚

ğŸ˜Šï¼ŒAnd so this is kind of a fun example that we found where there was a car neuron and you could actually see that you know we have the car neuron and it's constructed from like a wheel neuron and it looks for in the case of the wheel neuron it's looking for the wheels on the bottom those are positive weights and it doesn't want to see them on the top so it's negative weights there and there's also a window neuron it's looking for the windows on the top and not on the bottom and so we're actually seeing there it's an algorithm it's an algorithm that goes and turns it's just saying you well a car is has wheel on the bottom and windows on the top and chromrome in the middle and that's actually like just the strongest neurons for that and so we're actually seeing a meaningful algorithm and that's not an exception that's sort of the general story that if you're willing to go and look at neural network weights and you're willing to invest a lot of energy and trying to per engineerine them there's meaningful algorithms written in the weights waiting for you to find themã€‚

And there's a bunch of reasons I think that's an interesting thing to think about one is you know just no one knows how to go and do the things that neural networks can do like no one knows how to write a computer program that can accurately classify imagenet let alone you know the language modeling tasks that we're doing no one knows how like directly write a computer program that can do the things that G3 does and yet somehow breaking descent is able to go and discover a way to do this and I want to know what's going on I want to know you know how what is it discovered that it can do in these systemsã€‚

There's another reason why I think this is importantã€‚

 which is is safety so you know if if we want to go and use these systems in places where they have big effect on the world and I think a question we need to ask ourselves is you know what what happens when these models have unanticipated failure modes failure modes we didn't know to go and test for to look for to check for how can we how can we discover those things especially if they're really pathological failure modes or the models in some sense deliberately doing something that we don't want well the only way that I really see that we can do that is if we can get to a point where we really understand what's going on inside these systemsã€‚

ğŸ˜Šï¼ŒSo that's another reason that I'm interested in thisã€‚

Now actually doing interpretly on language models and transformers it's new to me before this year I spent like eight years working on trying reverse engineer continents and vision models and so the ideas in this talk are new things that I've been thinking about with my collaborators and we're still probably a month or two maybe longer from publishing them and this is also the first public talk that I've given on it so know the things that I'm gonna to talk about there's I think honestly still a little bit confused for me and definitely are going to be confusing in my articulation of them so if I say things that are confusing know please feel free to ask me questions there might be some points for me to go quickly because there's a lot of content but definitely at the end I will be available for a while to chat out this stuff and yeah also I apologize if I'm unfamiliar with Zoom and make mistakes but yeah so with that saidã€‚

 let's dive inã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_5.png)

And so I wanted to start with a mysteryã€‚Before we go and try to actually dig into what's going on inside these modelsã€‚

 I wanted to motivate it by a really strange piece of behavior that we discovered and wanted to understandã€‚

å—¯ã€‚And by the wayï¼Œ I should say all this work is done with my colleagues phanthropicã€‚

 and especially my colleaguesï¼Œ Catherine and Nelsonã€‚Okayï¼Œ so onto the mysteryã€‚

 I think probably the most interesting and most exciting thing about about transformers is their ability to do in context learning or sometimes people will call it meta learningã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_7.png)

You knowï¼Œ the GT3 paper goes and describes things as you know language models are few shot learners like there's lots of impressive things about GT3 but they choose to focus on that and know now everyone's talked about prompt engineering and Andre Karahi was joking about how you know software 3ã€‚

0 is designing the prompt and so the ability of language models of these large transformers to respond to their context and learn from their context and change their behavior and response to their context you know really seems like probably the most surprising and striking and remarkable thing about themã€‚

And some of my colleagues previously published a paper that has a trick in it that I really loveã€‚

 which is so we're all used to looking at learning curvesã€‚

 you train your model and you you know as your model trainsï¼Œ the loss goes downã€‚

The I is full of bit discontinuous and it goes downã€‚

Another thing that you can do is you can go and take a fully trained model and you can go and ask you know as we go through the contextã€‚

 you know as we go and when we predict the first token and then the second token and the third tokenã€‚

 we get better at predicting each token because we have more information to go and predict it on so you know the first the first token the loss should be the entropy of the unigrams and then the next token should be the entro of the biograms and it fallsã€‚

It keeps falling and it keeps getting betterã€‚And in some senseã€‚

's that's the model's ability to go and predict to go and do in context learningã€‚

 the ability to go and predict you know to be better at predicting later tokens than you are predicting early tokens that is in some sense a mathematical definition of what it means to be good at this magical in context learning or meta learning that these models can do so that's kind of cool because that it gives us a way to go and look at whether models are good at in context learningã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_9.png)

ä¸ã€‚if I could just ask a question like a clarification question please when you say learning there are no actual parameter of the yeah I mean that is the remarkable thing about in context learning right so yeah indeed we traditionally think about neural networks as learning over the course of training by going and modifying their parameters but somehow models appear to also be able to learn in some sense if you give them a couple examples in their context they can then go and do that later in their context even though no parameters changed and so it' it's some kind of quite different notion of learning as you're gesturing outã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_11.png)

Okay I think that's making more sense so I meanï¼Œ could you also just describe in context learning in this case as conditioning as in like conditioning on the first five tokens of a 10 token sentence next five tokens Yeah I think that reason people sometimes think about this as in contextex learning or meta learning is that you can do things where you like actually take a training set and you embed the training set in your context like if just two or three examples and then suddenly your model can go and do this task and so you can do few shot learning by embedding things in the context yeah the formal setup is that you're just conditioning on this context and it's just that somehow this ability like this thing like there's there's some sense you know for a long time people I mean I guess really the history of this is we started to get good at neural networks learning right and we could go and train language trained vision models and language models that could do all these remarkable things but then people started to be like well you know these systems are they take so many more examples thenã€‚

humansumans do to go and learn how can we go and fix this and we had all these ideas about meta learning develop where we wanted to go and and train modelsã€‚

ğŸ˜¡ï¼ŒExplicititely to be able to learn from a few examples and people develop all these complicated schemes and then the like truly like absurd thing about transformer language models is without any effort at allã€‚

 we get this for free that you can go and just give them a couple examples in their context and they can learn in their context to go and do new things I think that was like like that was in some sense the like most striking thing about the G3 paperã€‚

And soï¼Œ yeahï¼Œ this ability to go and have the just conditioning on a contextã€‚

 go and give you you knowï¼Œ new abilities for free and the ability to generalize to new things is in some sense the mostã€‚

 yeahï¼Œ to me the most striking and shocking thing about transformer language modelsã€‚

ğŸ¤§That makes senseï¼Œ I mean I guessã€‚From my perspectiveã€‚

I'm trying to square like the notion of learning in this case withï¼Œ you knowã€‚

 if you or I were given a prompt of like one plus one equals two two plus three equals five as the sort of few shots set up and then somebody else put you know like five plus three equals and we had to fill it out in that case I wouldn't say that we've learned arithmetic because we already sort of knew it but rather we're just sort of conditioning on the prompt to know what it is that we should then generate rightï¼Ÿ

But it seems to me like that'sã€‚Yeah I think that's on a spectrum though because you can also go and give like completely nonsensical problems where the model would never have seen seen like mimic this function and give a couple examples of the function and the models never seen it before and I can go and do that later in the context and I think what you did learn in a lot of these cases so you might not have you might have you might not have learned atic like you might have had some innate faculty for arithmetic that you're using but you might have learned oh okay right now we're doing arithmetic problemsã€‚

Got it in case this is I agree that there's like an element of semantics here yeah no this is helpful just to clarify exactly sort of what the what you know us Larry thank you for walking through thatã€‚

Of courseã€‚So something that'sï¼Œ I thinkï¼Œ really striking about all of thisã€‚ğŸ˜Šã€‚

I well okay so we've talked about how we can we can sort of look at the learning curve and we can also look at this in contexttex learning curveã€‚

 but really those are just two slices of a two dimensionional spaceã€‚

 so like in some sense the more fundamental thing is how good are we at producing the endth token at a different given point in training and something that you'll notice if you look at this so we when we talk about the loss curve we're just talking about if you average over this dimensionã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_13.png)

If you if you like average like this and project on to the the training step thats that's your loss curve and if you the thing that we are calling the in context learning curve is just this lineã€‚

Yeahï¼Œ this line down me the endter hereã€‚å—¯ã€‚And something that's kind of striking is there's there's this discontinuity in it like there's this point where where you know the model seems to get radically better in a veryã€‚

 very short time span and going and predicting late tokensã€‚

So it's not that different in early time stepsï¼Œ but in late time stepsï¼Œ suddenly you get betterã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_15.png)

And a way that you can make this more striking is you can take the difference in your ability to predict the 50th token and your ability to predict the 500th token you can subtract from the 500th tokenã€‚

 the 50th token lossã€‚And what you see is that over the course of trainingã€‚

 you know you're not very good at this and you get a little bit better and then suddenly you have this cliff and then you never get better the difference between these at least never gets better so the model gets better at predicting things but its ability to go and predict late tokens over early tokens never gets betterã€‚

ğŸ˜¡ï¼ŒAnd so there's in the span of just a few hundred steps in trainingã€‚

 the model has gotten radically better at its ability to go and do this kind of in context learningã€‚

And so he might askï¼Œ you knowï¼Œ what's going on at that pointï¼ŸAnd this is just one modelã€‚

 but well so first of all it's worth noting this isn't a small changeã€‚

 so can we don't think about this very oftenï¼Œ but often we just look at law scoress we' like did the model do better than another model or worse than another model but you can think about this as in terms of NAs and it's just the information the quantity and NA and you can convert that into Tibitã€‚

And so like one way you can interpret this is it's something roughly like you know the model 0ã€‚

4nas is about 0ã€‚5 bits is about like every other token the model gets to go and sample twice and pick a better one it's actually it's even stronger than that that's sort of an underestimate of how big a deal going and getting better by 0ã€‚

4nases so this is like a real big difference in the model's ability to go and predict late tokensã€‚

And we can visualize this in different ways can we can also go and ask you know how much better are we getting at going and predicting later tokens and look at the derivative and then we can see very clearly that there's there's some kind of discontinuity in that derivative at this point and we can take the second derivative then and we can with well derivative with respect to training and now we see that there's like there's very very clearly this lying here so something and just the span of a few steps and a few hundred steps is causing some big change and we have some kind of phase change going onã€‚

And this is true across model sizesã€‚And you can actually see it a little bit in the loss curve and there's this little bump here and that corresponds to the point where you have this change we actually could have seen in the loss curve earlier tooã€‚

 it's this bump hereã€‚Excuse me so so we have this phase change going on and there's I think a really tempting theory to have which is that somehow whatever you know there's some this change in the model's output and its behaviors and in the sort of outward facing properties corresponds presumably to some kind of change in the algorithms that are running inside the model so if we observe this big phase changeã€‚

 especially in a very small window in the model's behaviorã€‚

 presumably there's some change in the circuits inside the model that is drivingã€‚

At least that's a you know a natural hypothesis so if we want to ask that though we need to go and be able to understandã€‚

 you know what are the algorithms that's running inside the modelã€‚

 how can we turn the parameters in the model back into this algorithm so that's going to be our goalã€‚

Now it's going to require us to cover a lot of ground in a relatively short amount of timeã€‚

 so I'm going to go a little bit quickly through the next section and I will highlight sort of the key takeaways and then I will be very happy to go and you know explore any of this in as much depthã€‚

 I'm free for another hour after this call and just happy to talk in as much depth as people want about the details of thisã€‚

ğŸ˜Šã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_17.png)

So it turns out the space change doesn't happen in a one layer attentionally transformer and it does happen in a two layer attentionally transformerã€‚

 so if we could understand a one layer attentionally transformer and a two layer only attentionally transformer that might give us a pretty big clue as to what's going onã€‚

ğŸ˜¡ã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_19.png)

hanã€‚So we're attentionally we're also going to leave out layer norm and biases to simplify things so you know you one way you could describe a attentionally transformer is we're going to embed our tokensã€‚

And then we're going to apply a bunch of attention heads and add them into the residual stream and then apply our un embedding and that like give us our logicsã€‚

And we could go and write that out as equations if we want multiply by an embedding matrixã€‚ğŸ˜Šã€‚

Apply attention headsã€‚And then compute the logsï¼Œ print the animeã€‚Andã€‚

And the part here that's a little tricky is understanding the attention heads and this might be a somewhat conventional way of describing attention end and it actually kind of obscures a lot of the structure of attention in and I think that oftentimes we make attention heads more complex than they are we sort of hide the interesting structure so what is this saying what's saying you for every token compute value value vector and then go and mix the value vectors according to the attention matrix and then project them with the output matrix back into the residual stringã€‚

ğŸ˜Šï¼ŒAnd so there's there's another notation which you could think of this as a as using tensor products or using usingã€‚

Wellï¼Œ I guess left and right multiplying there's a few ways you can interpret thisã€‚

 but I'll just sort of try to explain what this notation meansã€‚14å¹´çº§ã€‚ğŸ˜Šã€‚

For every x our residual streamï¼Œ we have a vector for every single tokenã€‚

And this means go and multiply independently the vector for each token by Wvã€‚

 so compute the value vector for every tokenã€‚ğŸ˜¡ï¼ŒThis oneï¼Œ on the other handã€‚

 means notice that it's now on the we A is on the left hand sideã€‚ It means goingã€‚

 go and multiply theã€‚Attention matrix or going go into linear combinations of the values of value vectors so don't don't change the value vectorsã€‚

 you know point wiseï¼Œ but go and mix them together according to the attention patternã€‚

 create a weighted sumã€‚And then againï¼Œ independently for every positionã€‚

 go and apply the output matrixã€‚And you can apply the distributor property to this and it just reveals that actually didn't matter that you did the attention sort of in the middleã€‚

 you could have done the attention at the beginning you could have done it at the endã€‚

 that's the independent and the thing that actually matters is there's this WVWO matrix that describes what it's really saying is you know WVWO describes what information the attention head reads from each position and how it writes it to its destinationã€‚

 whereas A describes which tokens we read from and write toã€‚

And that's kind of getting more fundamental structure and attention head an attention head goes and moves information from one position to another and the process of which position gets moved from and too is independent from what information gets movedã€‚

And if you rewrite your transformer that wayï¼Œ well first we can go andr the sum of attention heads just as in this formã€‚

And then we can go and write that as the entire layer by going and adding an identityã€‚

And if we go and plug that all in to our transformer and go and expandã€‚Andã€‚

we have to go and multiply everything throughï¼Œ we get this interesting equationã€‚

And so we get this one termï¼Œ this corresponds to just the path directly through the residual streamã€‚

And it's going to want to store pgram statisticsï¼Œ it's justã€‚

 you know all I guess is the previous token and tries to predict the next tokenã€‚

And so it gets to try and predict or try to store by statistics and then for every attention headã€‚

 we get this matrix that saysï¼Œ okay well we have the attention pattern so it looks that describes which token looks at which token and we have this matrix here which describes how for every possible token you can attend to how it affects the logics and that's just a table you can look at it just says you know for this attention head if it looks at this token it's going to increase the probability of these tokens in a one layer attention only transformer that's all there isã€‚

ğŸ˜Šï¼ŒYeahï¼Œ so this is just the interpretation I was describingã€‚å—¯ã€‚

And another thing that's worth noting is according to thisã€‚

 the attention only transformer is linear if you fix the attention pattern now of course it's the attention pattern isn't fixedã€‚

 but whenever you able to have the opportunity to go and make something linear linear functions are really easy to understand and so if you can fix a small number of things and make something linear that's actually it's a lot of leverageã€‚

Okayã€‚å—¯ã€‚And yeahï¼Œ we could talk about how the attention pattern is computed as wellã€‚

 you if you expand it outï¼Œ you'll get an equation like thisã€‚And noticeï¼Œ wellã€‚

 I think it'll be easierã€‚Okayã€‚I think the core story though to take away from all of these is we have these two matrices that actually look kind of similarã€‚

 so this one here tells you if you attended to a tokenï¼Œ how are the logics affectedï¼Ÿ

And you can just think of it as a giant matrix of for every possible token input token and how are the logics affectedï¼Ÿ

By that tokenï¼Œ are they made more likely or less likelyï¼ŸAnd we have this oneï¼Œ which sorter saysã€‚

 how much does every token want to attend to every other tokenï¼ŸOne way that you can picture this isã€‚

Okayï¼Œ that's really there's really three tokens involved when we're thinking about an attention headã€‚

 we have the token that we're going to move information to and that's attending backwardsã€‚

We have the source token that's going to get attended to and we have the output token whose logicits are going to be affectedã€‚

And you can just trace through this so you can ask what happensã€‚

 how does the attending to this token affect the outputï¼Œ well first we embed the tokenã€‚

Then we multiply by WV to get the value vectorï¼Œ the information gets moved by the attention patternã€‚

We multipied by WO to add it back into the residual stream but get hit by the an embedding and we affect the logics and that's where that one matrix comes from and we can also ask you know what decides you know whether a token gets a high score when we're computing the attention pattern and it just says you knowbed embed the tokenã€‚

Turn it into a queryï¼Œ embed the other tokenï¼Œ turn it into a keyã€‚And dot product to them and seeã€‚

 that's where those two matrices come fromã€‚So I knew that I'm going quite quicklyã€‚

Maybe I'll just briefly pause here and if anyone wants to ask for clarificationsã€‚

 this would be a good time and then we'll actually go and reverse engineer and sayã€‚

 you know everything that's going on in a one layer intention transformer is now in the palm of our handsã€‚

It's a very toy modelã€‚No one actually uses one layer attention to the transformersã€‚

 but we'll be able to understandã€‚The one layer attentionally transformerã€‚So justã€‚

 you're sering that yes the quick key circuit is learning the attention weightsã€‚

And like essentially is a responsible of running the sort the attention between different tokens yeah yeah so this this matrix when it yeah you know all three of those parts are learnedã€‚

 but that's that's what expresses whether a attention pattern yeah that's what generates the attention patterns gets run for every pair of tokens and you can you can you can think of values in that matrix as just being how much every token wants to attend to every other token if it was in the context we're ignoring positional letting here so there's a little bit that we're sort of aligning over there as well but sort of in a global sense how much does every token wants to attend every other token right the circuit like the circuit is using the attention thatã€‚

Yesã€‚Like affect the final outputs it's sort of saying if if the attention head assume that the attention head attends to some tokenã€‚

 so let's set aside the question of how back gets' compute just assume that it attends to some tokenã€‚

 how would it affect the outputs if it attended to that tokenã€‚

And you just you can just calculate that it's just a big table of values that says you know for this tokenã€‚

It's going to make this token more likely this token will make this token less likelyã€‚Okayã€‚

 that't justã€‚And it's completely independent like it's just two separate matricesï¼Œ they're notã€‚

 you knowï¼Œ the formulas that might make them seem entangledï¼Œ but they're actually separateã€‚Rightã€‚

 so to meï¼Œ it seems like the lecture supervision is coming from the output value set and the query key are seems more like unsupervised kind of thing because there's noã€‚

I meanï¼Œ therere just I think in the sense that every yeah in a model like every every neuron is in some senseã€‚

 you know like signal is is somehow downstream from the ultimate the ultimate signal and so you know yeah the output value signal the output value start is getting more more direct is perhaps getting more direct signal but yeahã€‚

æ˜¯ã€‚We will be able to dig into this in lots of detail in as much detail as you want in a little bit so we can maybe I'll push forward and I think also actually an example of how to use this reverse engineer one layer model will maybe make it a little bit more motivatedã€‚

Okayï¼Œ soã€‚Just just to emphasize thisï¼Œ there's three different tokens that we can talk aboutã€‚

 there's a token that gets attended toï¼Œ there's the token that does the attention to call the destination and then there's the token that gets affected yet gets the next token which its probabilities are affectedã€‚

å—¯ã€‚And so something we can do is notice that the only token that connects to both of these is the token that gets attended toã€‚

 so these two are sort of they're bridgedã€‚By their their interaction with the source tokenã€‚

 so something that's kind of natural is to ask for a given source tokenï¼Œ you knowã€‚

 how does it interact with both of theseï¼ŸSo let's takeï¼Œ for instanceï¼Œ the token perfectã€‚

Whichken one thing we can ask is which tokens want to attend to perfectã€‚Coyleï¼Œ apparentlyã€‚

 the tokens that most want to attend are perfect are are and looks and is and providesã€‚

So our is the most looks as the next most and so onã€‚

And then when we attend to perfect and this is with one single attentionedã€‚

 so you know it' would be different if we did a different attentionedã€‚

 it wants to really increase the probability of perfect and then to a lesser extentã€‚

 super and absolute and pureã€‚And we can ask you what sequences of tokens are made more likely by of this particular set of things wanting to attend to each other and becoming more likelyã€‚

ğŸ˜¡ï¼ŒThings are the formã€‚We have our token that we attended back to and we have some skip of some number of tokensã€‚

 they don't have to be adjacentï¼Œ but then later on we see the token R and it tends back to perfect and increases the probability of perfectã€‚

So you can think of these as being like we're sort of creating changing the probability of what we might call skip trigrams where we haveã€‚

 you knowï¼Œ we skip over a bunch of tokens in the middleã€‚

 but we're affecting the probability really of trigramsã€‚So perfect or perfectï¼Œ perfectï¼Œ look superã€‚

We can look at another one so we have the token largeã€‚

 these tokens contains using specify want to go and look back to it and an increase in probability of large and small and the skip tris that are affected are things like large using largeã€‚

 large contains small and things like thisã€‚ğŸ˜Šï¼Œif we see the number twoã€‚

 we increase the probability of other numbers and we affect tokens or skip diagrams like twoï¼Œ oneã€‚

 twoï¼Œ two has threeã€‚Now you're all in a technical field so you'll probably recognize this oneã€‚

 we have Lambda and then we see backslash and then we want to increase the probability of Lambda and sorted and Lambda and operators so it's all fall lateekã€‚

It wants toï¼Œ it's if it sees Lambda it thinks thatï¼Œ you knowï¼Œ maybe next time I use a backlashã€‚

 I should go and put in some latex math symbolã€‚Also same thing for HTML we see NSP for non breakinging space and then we see an n percent we want to go and make that more likely the takeaway from all this is that a one layer our attention only transformer is totally acting on these skip trisã€‚

ğŸ˜Šï¼ŒAndã€‚Everything that doesï¼Œ I meanï¼Œ I guess it also has this pathway by which it affects diagramgramsã€‚

 but mostly it's just affecting these skip diagramsã€‚And there's lots of themã€‚

 it's just like these giant tables of skip tris that are made more or less likelyã€‚

There's lots of other fun things that does sometimes the tokenization will split up a word in multiple waysã€‚

 so like we have Indie well lets that's not good k we have like the word Piike and then we we see the the token P and then we predict ICã€‚

ğŸ˜Šï¼ŒWhen we predict spikes and stuff like thatã€‚Or these these ones are kind of funã€‚

 maybe they're actually worth talking about for a secondã€‚ so we see the token voidã€‚ğŸ˜Šã€‚

And then we see an L and make we predict Lloyd or Rï¼Œ and we predict Ralphï¼Œ Cï¼Œ Catherineã€‚

And but we'll see in a second that well yeah we'll come back to that that in a secondã€‚

 So we increase the probability of things like Lloyd Lloyd and Lloyd Catherine or Pixmap if anyone's worked with QT we see Pixmap and we increase the probability of P Xmap againã€‚

 but also Qã€‚ğŸ˜Šï¼ŒCanvasã€‚Andã€‚But of courseï¼Œ there's a problem with thisã€‚

 which is it doesn't get to pick which one of these goes with which one so if you want to go and make Pixm pix mapapã€‚

And Pixmap Q Canvas more probableï¼Œ you also have to go and create and make Pixmap Pixmap P canvasvas more probableã€‚

And if you want to make Lloyd Lloyd and Lloyd Catherineã€‚More probableã€‚

 you also have to make Lloyd Cloyd and Lloyd Latherinï¼Œ more probableã€‚

And so there's actually like bugs that transformers have like weird at least you know in these really tiny one layer at attention only transformers there's these bugs that you know they seem weird until you realize that it's this giant table of skip tris that's that's operating and the nature of that is that you're going to beã€‚

ğŸ˜Šï¼Œyeahï¼Œ it sort of forces you if you want to go and do this to go in and also make some weird predictionsã€‚

Chrisï¼Œ brieflyleyã€‚Is there a reason why the source open you have a space before the first characterï¼Ÿ

Yesï¼Œ that's just the I was giving examples where the tokenization breaks in a particular way and okay because spaces get included in the tokenization when there's a space in front of something and then there's an example where the space isn't in front of itã€‚

 they can get tokenized in different ways Go it cool thanksã€‚Yeahï¼Œ great questionã€‚Okayã€‚

 so some just to abstract away some common patterns that we're seeingã€‚

 I think one pretty common thing is what you might describe as like beã€‚A Bã€‚

 so you're you go in you you see some token and then you see another token that might precede that token and then they're likeã€‚

 probably the token that I saw earlier is going to occur againã€‚

Or sometimes you predict a slightly differenttokenï¼Œ so like maybe an example of the first one is twoã€‚

 oneï¼Œ twoã€‚But you could also do two has threeã€‚And so three isn't the same as twoã€‚

 but it's kind of similar so that's that's one thing another one is this this example where you have a token that suddenly it's tokenized together one time and then it's split apart so you see the token and then you see something that might be the first part of the token and then you predict the second partã€‚

ğŸ˜Šï¼Œå—¯ã€‚I think the thing that's really striking about this is think there are all in some ways a really crude kind of in context learningã€‚

And and in particularï¼Œ these models get about 0ã€‚1nas rather than about 0ã€‚

4nas up in context learning and they never go through the phase changeã€‚

 so they're doing some kind of really crude in context learning and also they're dedicating almost all their attention heads to this kind of crude in context learningã€‚

 so they're not very good at itï¼Œ but they're dedicating their capacity to itã€‚ğŸ˜Šã€‚

I'm noticing that it's 1037ï¼Œ and I want to just check how long I can go because I maybe I should like super accelerate ifesã€‚

OhChrisï¼Œ I think it's fine because like students are also asking questions in betweenã€‚

 So you shouldn be goodã€‚ Okayï¼Œ so maybe my plan will be that I'll talk until like 1055 or 11ã€‚

 And then if youï¼Œ I can go and answer questions for a while after after thatã€‚Yeahï¼Œ it worksã€‚

 fantasticã€‚So you can see this as a very con crude kind of in context learning like basically what we're saying is it's sort of all this flavor of okayã€‚

 well I saw this tokenï¼Œ probably these other tokensã€‚

 the same token or similar tokens are more likely to go and acc cur laterator and look this is an opportunity that sort of looks like I can inject the token that I saw earlier I'm going to inject it here and say that it's more likely that' like that's basically what it's doingã€‚

And it's dedicating almost all of its capacity to that so you know these it's sort of the opposite of what we thought with RnNs in the past like used to be that everyone was like oh you know RNNsã€‚

 it's so hard to get the care about long distance contextã€‚

 you know maybe we need to go and like use dams or something no if you train a transformer it dedicates and you give it a a long enough context it's dedicating almost all its capacity to this type of stuff just kind of interestingã€‚

There are some attention ins which are more primarily positionalã€‚

 usually we a model that I've been training that has two layer or it's only a one layer model has 12 attention endsã€‚

 and usually around two or three of those will become these more positional that are shorter term things that do something more like local trigram statistics and then everything else becomes these skip trisã€‚

Yeahï¼Œ so some takeaways from thisï¼Œ yeahï¼Œ you can understand one layer additional only transformers in terms of these OV and QK circuitsã€‚

Transformers desperately want to do in context learningï¼Œ they desperatelyã€‚

 desperately desperately want to go and look at these long distance contacts and and predict thingsã€‚

 there's just so much so much entropy that they can go and reduce out of thatã€‚ğŸ˜¡ï¼Œå—¯ã€‚ğŸ˜Šã€‚

The constraints of a one layer are intention transformer force it to make certain bugsã€‚

 but it won't do the right thingã€‚And if you freeze the attention patternssï¼Œ these models are linearã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_21.png)

Okayï¼Œ a quick aside because so far this type of work has required us to do a lot of very manual inspection like we're walking through these giant matricesã€‚

 but there's a way that we can escape that we don't have to use look at these giant matrices if we don't want toã€‚

ğŸ˜Šã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_23.png)

We can use ienvalue some eigvectorsï¼Œ so recall that an icon canvalueã€‚

And eigenvector just means that if you multiply that vectorctor by the matrixã€‚

 it's equivalent to just scalingã€‚And often in my experiencesã€‚

 this haven't been very useful for interpreterability because we're usually mapping between different spacesã€‚

 but if you're mapping onto the same spaceï¼Œ eigenvalues and eigenvectors are a beautiful way to think about guysã€‚

ğŸ˜Šï¼ŒSo we're going to draw them on a raial plot and we're going to have a log radial scale because they're going to vary their magnitude is going to vary in by many orders of magnitudeã€‚

Okay so we can just go and you know our OV circuit maps from tokens to tokens that's the same vector space on the input in the output and we can ask you know what does it mean if we see eigenvalues of a particular kind well positive eigenvalues and this is really the most important part mean copying so if you have a positive eigenvalue it means that there's some set of tokens where if you see them you increase their probabilityã€‚

And if you have a lot of positive eigenvaluesï¼Œ you're doing a lot of copyingã€‚

 if you only have positive eigenvaluesï¼Œ everything you do is copyingã€‚

Now imaginary eigenvalues mean that you see a token and then you want to go and increase the probability of unrelated tokens and finally negative eigenvalues are anticocking they're likeã€‚

 if you see this tokenï¼Œ you make it less probable in the futureã€‚ğŸ˜¡ã€‚

Well that's really nice because now we don't have to go and dig through these giant matrices that are vocab size by vocab size we can just look at the eigenvalues and so these are the eigenvalues for our one layer attentionally transformer and we can see that you know forã€‚

ğŸ˜Šï¼ŒMany of these they're almost entirely positiveï¼Œ these ones are sort of entirely positiveã€‚

 these ones are almost entirely positive and really these ones are even almost entirely positive and there's only two that have a significant number of imaginary and negative eigenvaluesã€‚

ğŸ˜Šï¼ŒAnd so what this is telling us is it's it's just in one picture we can seeï¼Œ you knowï¼Œ okayã€‚

 they're reallyï¼Œ you knowã€‚10 out of 12 of these of these attention heads are just doing copying they just they just are doing this long distance you know well I saw token probably it's going to occur again type stuff that's kind of cool we can we can summarize it really quicklyã€‚

ğŸ˜Šï¼ŒOkayï¼Œ now the other thing that you can yeahï¼Œ so this is this is for a secondã€‚

 we're going to look at a two layer model in a second and'll we'll see that also a lot of its heads are doing this kind of copyingish stuffã€‚

 they have large positive eigenvaluesã€‚ğŸ˜Šï¼ŒYou can do a histogram like you know one thing at school is you can just add up the eenvalue and divide them by their absolute values and you get a number between zero and oneã€‚

 which is like how copying how copying is's just the head or between negative one and one how copying is's just the head you can just do a histogram and you can see oh yeah almost all the heads are doing doing lots of copyingã€‚

You it's nice to be able to go and summarize your model and I think this is sort of like we've gone from a very bottom up way and we didn't start with assumptions about what model2 and we tried to understand its structure and then we were able to summarize it in useful ways and now we're able to go and say something about itã€‚

Now another thing you might ask is what do the eigenvalues of the QK circuit mean and in our example so far they haven't been that they wouldn't have been that interestingã€‚

 but in a minute they will be and so I'll briefly describe what they mean a positive iconenvalue would mean you want to attend to the same tokensã€‚

ğŸ˜¡ï¼ŒAnd imagineagin your eigenvalueï¼Œ and this is what you would mostly see in our the models that we've seen so far means you want to go in and attend to a unrelated or different token and a negative eenvalue would mean you want to avoid attending to the same profileã€‚

So that will be relevant in secondã€‚Yeahï¼Œ so those are going to mostly be useful to think about in multilayer attention in transformers when we can have chains of attention hint and so we can ask you knowã€‚

 wellï¼Œ I'll get to that in a secondã€‚ğŸ˜Šï¼ŒYeah so there's a table summarizing that unfortunately this approach completely breaks down once you have MLP layers MLP layersã€‚

 you know now you have these nonlinearities since you don't get this property where your model is mostly linear and you can you can just look at a matrixã€‚

 but if you're working with only attentionally transformers this is a very nice way to think about Pã€‚

ğŸ˜Šã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_25.png)

Okayï¼Œ so recall that one layer attentionally transformers don't undergo this phase change that we talked about at in the beginning likere right now we're on a huntã€‚

 we're trying to go and answer this mystery of how what the hell is going on in that phase change where models suddenly get good at in context learning we want to answer that and one layer attentionally transformers don't undergo that phase change but two layer attentionally transformers do so we'd like to know what's different about two layer attentionally transformersã€‚

ğŸ˜¡ã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_27.png)

Okayï¼Œ wellï¼Œ so in our in our previous when we were dealing with one layer of attention transformersã€‚

 we were able to go and rewrite them in this form and it gave us a lot of ability to go and understand the model because we could go and say well you know this is bygrams and then each one of these is looking somewhere and we hit this matrix that describes how it affects thingsã€‚

And yeahï¼Œ so that gave us a lot of ability to think about these thingsã€‚

 and we can also just write in this factored form where we have the embedding and then we have the attention heads and then we have the un embeddingã€‚

ğŸ˜Šï¼ŒOkay wellï¼Œ and for simplicityï¼Œ we often go and write WOV for WO WV because they always come togetherã€‚

 it's always the case like it's in some senseï¼Œ an illusion that WO and WV are different matricesã€‚

 they're just one low rank matrix're never they're always used together and similar WQ and WK it's sort of an illusion that they' they're different matrices they're always just used together and keys and queries are just sort of they're just an artifact of these low rank matricesã€‚

So in anyï¼Œ it's useful we want to write those togetherã€‚Okayï¼Œ greatã€‚

 so a two layer intentionally transformerï¼Œ what we do is we go through the embedding matrixã€‚ğŸ˜Šã€‚

Then we go through the layer one attention endï¼Œ then we go through the layer two attention endã€‚

And then we go through the un embedding and for the attention headsã€‚

 we always have this identity as wellï¼Œ which corresponds just going down the residual stream so we can go down the residual stream or we can go through an attention headã€‚

Next upï¼Œ we can also go down the residual stream where we can go through an attention headã€‚

And there's this useful identityï¼Œ the mixed product identity that any tensor product or other ways of interpreting this obeyã€‚

 which is that if you have an attention head and we have sameã€‚

 you know we have the weights and the attention pattern and the WOV matrix and the attention patternã€‚

 the attention patterns multiply together and the OV circuits multiply together and they behave flyã€‚

ğŸ˜Šï¼ŒGreatï¼Œ so we can just expand out that equation we can just take that big product we had at the beginning and we can just expand it out and we get three different kinds of terms so one thing we do is we get this this path that just goes directly through the residual stream where we embed and unembed and that's going to want to represent some bigram statisticsã€‚

ğŸ˜Šï¼ŒThen we get things that look like the attention head terms that we had previouslyã€‚And finallyã€‚

 we get these terms that correspond to going through two attention headã€‚Andã€‚ğŸ˜Šã€‚

Now it's worth noting that these terms are not actually the same as because the attention head the attention patterns in the second layer can be computed from the outputs of the first layerã€‚

 those are also going to be more expressiveï¼Œ but at a high level you can think of there as being these three different kinds of terms and we sometimes call these terms virtual attention heads because they don't exist like they aren't sort of explicitly represented in the modelã€‚

 but in fact they have an attention pattern they have no ease or they're sort in almost all functional ways like a tiny little attention head and there's exponentially many of themã€‚

å—¯ã€‚Turns out they're not going to be that important in this modelï¼Œ but in other modelsã€‚

 it can be importantã€‚Rightï¼Œ so one one thing that' I said this is it allows us to think about attention and in a really principled wayã€‚

 we don't have to go and think aboutã€‚You know I think there's like people people look at attention patterns all the time and I think a concern you to have as wellã€‚

 you know there's multiple attention patterns like you know the information that's being moved by one attention headã€‚

 it might have been moved there were by another attention ahead and not have originated there it might still be moved somewhere elseã€‚

 but in fact this give us a way to avoid all those concerns and just think about things in a single principle wayã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_29.png)

Okayï¼Œ in any caseã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_31.png)

An important question to ask is how important are these different terms we could study all of them how important are they and it turns out you can just there's an algorithm you can use where you knock out attention knock out these terms and you go and you ask how important are they and it turns out that by far the most important thing is these individual attention head terms in this model by far the most important thing the virtual attention heads basically don't matter that muchã€‚

They only have an effective of 0ã€‚3nas using the above ones and the bigrams are still pretty usefulã€‚

 so if we want to try to and understandchan this model we should probably go and focus our attention on the virtual attention hints are not going to be the best way to go in and go in focus our attentionã€‚

 especially since there's a lot of them there's 124 of them for 0ã€‚

3nas very little that you would understand pro studying one of those termssã€‚ğŸ˜Šã€‚

So the thing that we probably want to do when we know that these are diagramgram statisticsã€‚

 so what we really want to do is we want to understand the individual attention head termsã€‚

This is the algorithm I'm going to skip over it for time and we can ignore that term because it's small and it turns out also that the layer two attention tense are doing way more than layer one attention tense and that's that's surprising like the layer two attention tense are more expressive because they can use the layer one attention tense to construct their attention patternã€‚

ğŸ˜Šï¼ŒOkayï¼Œ so if we could just go and understand the layer to attention headsã€‚

 we probably understand a lot of what's going on in this modelã€‚Andã€‚

And the trick is that the attention heads are now constructed from the previous layer rather than just from the tokensã€‚

 so this is still the sameï¼Œ but the attention headã€‚

 the attention pattern is more more complex and if you write it out you get this complex equation that says you know you embed the tokens and you go and you shuffle things around using the attention heads for the keys and then you multiply by WQK then you multiply shuffle things around again with the queries and then you go and multiply by the embedding again because they were embedded and then you get back to the tokensã€‚

ğŸ˜Šï¼ŒAndã€‚ğŸ˜Šï¼Œå•Šã€‚But let's actually look at them so one thing that's remember that when we see positive eigenvalues in OB circuit we're doing copying so one thing we can say is well seven out of 12 and in fact the ones with the largest eigenvalues are doing copying so we still have a lot of attention that they're doing copyingã€‚

ğŸ˜Šï¼Œå—¯ã€‚And yeahï¼Œ the QK circuitï¼Œ so one thing you could do is you could try to understand things in terms of this more complex QK which you could also just try and understand what the attention patterns are doing empirically so let's look at one of these copying onesã€‚

I've given it the first paragraph of Harry Potterï¼Œ and we can just look at where it intenseã€‚Andã€‚

And something really happenedï¼Œ interesting happensã€‚

 so almost all the time we just attend back to the first tokenã€‚

 we have this special token at the beginning of the sequenceã€‚

And we usually think of that as just being a null tension operation it's a way for it to not do anything in factã€‚

 if you look the value vector is basically zeroï¼Œ it's just not cocking any information from thatã€‚ğŸ˜¡ã€‚

Andã€‚But whenever we see repeated textï¼Œ something interesting happensï¼Œ so when we get to Mrã€‚ğŸ˜¡ã€‚

Tryries to look at and it' a little bit weakï¼Œ then we get to Dã€‚And intends to Ersã€‚

 That's interestingã€‚And then we get to earthã€‚And it attends to leaveã€‚

And so it's not attending to the same tokenã€‚ It's attending to the same tokenã€‚Shifted one forwardã€‚

Wellï¼Œ that's really interestingã€‚ And there's actually a lot of attention nets that are doing thisã€‚

 So here we have one where now we hit the potter pot and we attended Tsã€‚

 Maybe that's the same attention that I don't remember when I was constructing this exampleã€‚ğŸ˜Šã€‚

And turns out this is a super common thingï¼Œ so you go and you look for the previous exampleã€‚

 you shift one forward and you're like okayï¼Œ well last time I saw thisï¼Œ this is what happenedã€‚

 probably the same thing is going to happenã€‚And we can go and look at the effect that the attention head has on the logicsã€‚

 most of the time it's not affecting thingsï¼Œ but in these cases it's able to go and predict when it's doing us this thing of going and looking one forward it's able to go and predict the next tokenã€‚

So we call this an induction headï¼Œ an induction head looks for the previous copy looks forward and says a probably the same thing that happened last time is going to happen you can think of this as being a nearest neighbors it's like an in context nearest neighbor's algorithm it's going and searching through your contextã€‚

 finding similar things and then predicting that's what's going to happen nextã€‚

The way that these actually work isï¼Œ I meanï¼Œ there's actually two waysã€‚

 but in a model that uses rotary attention or something like thisï¼Œ you only have oneã€‚Andã€‚

You shift your keyï¼Œ first you have an earlier retention hand shifts your key for one so you take the value of the previous token and you embeddedbed it in your present tokenã€‚

And then you have your query in your keyï¼Œ go and look atï¼Œ yeahã€‚

 try to go and match so you look for the same thingã€‚ğŸ˜¡ã€‚

And then you go and you predict that whatever you saw is going to be the next token so that's the high level algorithm sometimes you can do clever things where actually it'll care about multiple earlier tokens and it'll look for like short phrases and so on so induction heads can really vary in how much of the previous context they care about or what aspects of the previous context they care about but this general trick of looking for the same thing shift forward predict that is what induction heads doã€‚

Lots of examples of thisã€‚And the cool thing is you can now you can use the QK eigenvalues to characterize thisã€‚

 you can sayï¼Œ wellï¼Œ you know we're looking for the same thing shifted by one but looking for the same thing if you expand through the attention notes's in the right way that'll work out and we're copying and so an induction head is one which has both positive OV eigenvalues and also positive QK eigenvaluesã€‚

And so you can just put that on a plot and you have your induction heads in the corner to OV eigenvaluesã€‚

 your QK eigenvaluesï¼Œ and I think actually OV is this axisqK is this one axis doesn't matter and in the corner you have your eenvalues or your induction headsã€‚

ğŸ˜Šï¼ŒYeahï¼Œ and so this seems to be well okay we now have an actual hypothesisã€‚

 the hypothesis is the way that that phase change we're seeing the phase change is the discovery of these induction hits that would be the hypothesis and these are way more effective than regular you know than this first algorithm we had which was just sort of blindly copy things wherever it could be plausible now we can go and like actually recognize patterns and look at what happened and predict that similar things are going to happen again that's a way better algorithmã€‚

å—¯ã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_33.png)

Yeah so there's other attention hints that are doing more local things I'm going to go and skip over that and return to our mystery because I am running out of time I have five more minutes okay so what what is going along with this in context learning well now now we've hypothesis let's check it so we think it might be induction hintsã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_35.png)

å—¯mã€‚And there's a few reasons we believe usã€‚ So one thing is going to be that induction headsã€‚Wellã€‚

 okayï¼Œ I'll just go over to the endã€‚ So one thing you can do is you can just ablate the attention endã€‚

And it turns itï¼Œ you can color here we have attention heads colored by how much they are an induction headã€‚

And this is the start of the bumpï¼Œ this is the end of the bump hereã€‚

 and we can see that they first of all induction heads are forming like previously we didn't have induction heads here now they're just starting to form here and then we have really intense induction heads here and hereã€‚

And the attention heads where you obblate themï¼Œ you get aã€‚

You get a loss or so we're lucky not at loss this meta learning scoreã€‚

 the difference between or an in context learning scoreã€‚

 the difference between the 500th token and the 50th tokenã€‚

And that's all explained by induction handsã€‚Now we actually have one induction head that doesn't contribute to it actually it does the opposite so that's kind of interesting maybe it's doing something shorter shorter distance and there's also this interesting thing where like they all rush to be induction heads and then they discover only only a few win out in the end so there's some interesting dynamics going on there but it really seems like in these small modelsã€‚

ğŸ˜Šï¼ŒAll within context learning is explained by these induction notessã€‚ğŸ˜¡ï¼ŒOkayã€‚What about large modelsã€‚

 Wellï¼Œ in large modelsï¼Œ that's going to be harder to go and ask thisã€‚

 But one thing you can do is you can askï¼Œ okayï¼Œ you knowï¼Œ we can look at ourï¼Œ our inductionã€‚

 our in context learning score over timeã€‚ They get this sharp phase changeã€‚ Oh lookã€‚

Induction heads form at exactly the same point in timeã€‚So that's only correlational evidenceã€‚

 but it's pretty suggestive correlational evidenceï¼Œ even especially given that we have an obviousã€‚

 you know like the obvious effect that induction heads should have is is this I guess it could be that there's other mechanisms being discovered at the same time in large modelsã€‚

 but it has to be in a very small windowã€‚ğŸ˜Šï¼ŒSoã€‚ğŸ˜Šï¼ŒReally suggests that thing that's driving that change is in context learningã€‚

Okayï¼Œ soã€‚Obviouslyï¼Œ induction heads can go and copy textã€‚ğŸ˜¡ã€‚

But a question you might ask is you know can they can they do translation like there's all these amazing things that models can do that it's not obvious you know in context learning or this sort of copying mechanism could do so I just want to very quicklyã€‚

Look at a few fun examplesã€‚So here we have an attention patternã€‚Yeahã€‚

 I guess I need to open lexoscopeã€‚Let me try doing that againã€‚Sorryã€‚

 I should have thought this through a bit more before this talkã€‚å—¯ã€‚ğŸ˜Šï¼ŒChrisã€‚

 could you zoom in a littleï¼Œ pleaseï¼Œ Yeahï¼Œ yeahï¼Œ thank youã€‚And soã€‚Okayï¼Œ I'm notã€‚

 my French isn't that greatï¼Œ but my name is Christopherï¼Œ I'm from Canadaã€‚

What we can do here is we can look at where this attention attends as we go and we do this and it'll become especially clear on the second sentenceã€‚

 so here we're on the periodã€‚And we tend to showã€‚Now we're on andjos is I in Frenchã€‚ Okayã€‚

 now we're on the eye and we attend to Swedenã€‚Now we're on the am and we a trend to do which is from and then from to Canadaã€‚

 and so we're doing a cross lingual induction headï¼Œ which we can use for translationã€‚And indeedã€‚

 if you look at examplesï¼Œ this is where this seems to be a major driving force in the model's ability to go and correctly do translationã€‚

Another fun example isã€‚I think maybe maybe the most impressive thing about in context learning to me has been the model's ability to go and learn arbitrary functions like you mean just show the model function and can start mimicking that function well okayã€‚

 I I have a question Yes yeah so do these induction head only do kind of a look ahead copy or like can they also do some sort of like a complex structure recognitionã€‚

Yeah yeah so they can both use a larger context previous context and they can copy more abstract thingsã€‚

 so like the translation one is showing you that they can copy rather than the literal token a translated version it's what I call soft induction head and yeah you you can have them copy similar words you can have them look at longer context you can look for a more structural things the way that we usually characterize them is whether in large models just whether they empirically behave like an induction head so the definition gets a little bit blurry when you try to encompass these more sort of a blurry boundary but yeah seem to be a lot of attention heads that are doing sort more and more abstract versions and yeah my favorite version is this one that I'm about to show you which is used let's isolate a single one of these which can do pattern recognition so it can learn functions in the context and learn how to do it I've just made up a nonsense function here we're going encode one binary variable with the choice of whether to do a color or aã€‚

That was the first wordï¼ŸThenã€‚We're going to say we have green or June hereã€‚Let's zoom in moreã€‚

So we have color or month and animal or fruit and then we have to map it either true or false so that's our goal and it's going to be an exO we have a binary variable represented in this way we do an exOR I'mã€‚

Pretty confident this was never in the training set because I just made it up and it seems like a nonsense problemã€‚

Okayï¼Œ so then when we can go and ask you know can the model go and push that well it can and it uses induction heads to do it and what we can do is we can look at the so we look at a colon where it's going to go and try and predict the next word and for instance hereã€‚

And we have April dogï¼Œ so it's a month and then an animalï¼Œ and it should be trueã€‚

And what it does is it looks for a previous cases where there was an animal a month and then an animalã€‚

 especially one where the month is the same and goes and looks and says that it's trueã€‚

And so a model can go and learnï¼Œ learn a functionï¼Œ a completely arbitrary functionã€‚

By going and doing this kind of pattern recognition induction hitã€‚

And so this to me made it a lot more plausibleï¼Œ but these models actuallyã€‚Can youã€‚

Can do in context learning like the generality of all these amazing things we see these large language models do can be explained by inductionists we don't know that it could be that there's other things going on it's very possible that there's lots of other things going on but it seems a lot more plausible to me than it did when when we startedã€‚

ğŸ˜Šï¼ŒI'm conscious that I am actually over timeï¼Œ I mean just quickly go through these last few slidesã€‚

 yesï¼Œ so I think thinking of this as like an in contextt in your s neighborpers I think is a really useful way to think about thisã€‚

ğŸ˜Šï¼ŒOther things could absolutely be contributingã€‚This might explain why transformers do in context learning over long contexts better than LSTMs and LSTM can't do this because it's not linear on the amount of compute it needs it's like quadratic or N log n if it was really clever so transformers are LSTM's impossible to do this transformers do do this and actually they diverge at the same point but if you look wellã€‚

I can go into this in Mar Hill after if people wantã€‚

There's a really nice paper by Marcus Hutter explaining trying to predict and explain why we observe scaling laws and modelsã€‚

 it's worth noting that the arguments in this paper go exactly through to this exampleï¼Œ this theoryã€‚

 in factï¼Œ they sort of work better for the case of thinking about this in contextex learning with essentially in nearest neighbor's algorithm than they do in the regular caseã€‚



![](img/ee54ecf5185d58647f4bc57e2de3efea_37.png)

Yeah I'm happy to answer questions I can go into as much detail as people want about any of this and I can also if you send me an email send me more information about all this and yeah you know again this work is not yet published and you don't have to keep it secret but you know just if you could be thoughtful about the fact that it's unpublished work and probably is a month or two away from coming out I'd be really grateful for that thank you so much for your time yeah thanks a lotã€‚

ğŸ˜Šï¼ŒInterã€‚So I'll also open kind of like some general questions and then we can do like a round of questions from the students so I was very excited to know like so what is the like the line of work that you're currently working on is it like extending this so what do you think is like the next things you try to do to make it more interpret what are the next yeahã€‚

I mean I want to just reverse engineer language models I want to figure out the entirety of what's going on in these language models andã€‚

You know like one thing that we totally don't understand is MLP layers more we understand some things about them but we don't really understand MLP layers very well there's a lot of stuff going on in large models that we don't understand I want to know how models do arithmetic I want to know another thing that i'm very interested is what's going on when you have multiple speakers the model can clearly represent like it has like a basic theory of mind multiple speakers in a dialogue I want to understand what's going on with that but honestly there's just so much we don't understand it's really it's sort of hard to answer the question because there's just so much to figure out and we have a lot of different threads of research in doing this but yeahã€‚

The interpretpoly team at Anthropic is just sort ofã€‚

Has a bunch of threads trying to go and figure out what's going on inside these models and sort of a similar flavor to this of just trying to figure out how do the parameters actually encode algorithms and can we reverse engineer those into into meaningful computer programs that we can understandï¼Ÿ

ğŸ˜Šï¼Œanother question I just like to you're talking about like how the have tend to do metal learning in that So it's like you spend a lot of time talking more like they had something like that was like interesting but like can you formalize the sort of metal learning algorithm they might be learning is it possible to say like oh maybe this is a sort of like internal algorithm that's going that's making them like good metal learners something like that I don't know I mean I think that there's roughly two algorithms One is this algorithm we saw in the one layer model we see in other models too especially early onã€‚

 which is just know try to copy know you saw a word probably a similar word that iss gonna happen later look for places that it might fit in and increase the probability So that's one thing that we see and the other thing we see is induction head which you can just summarize as in context to your neighbors basically and it seemed know possibly with other things but it seems like those two algorithms and the specific instantiations that we are looking at it seem to be what's driving in context learning that would be my pre theoryã€‚

Yeahï¼Œ sounds very interestingã€‚Yeahï¼Œ okayï¼Œ so let's open like a run of first two questionsã€‚ So yeahã€‚

 feel free to go ahead for questionsã€‚

![](img/ee54ecf5185d58647f4bc57e2de3efea_39.png)