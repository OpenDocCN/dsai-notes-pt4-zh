- en: ÊñØÂù¶Á¶è GPTÔºèTransformer ÂéüÁêÜ‰ªãÁªç (‰∏≠Ëã±ÊñáÂèåÂ≠óÂπï) - P4Ôºö4.Decision TransformerReinforcement Learning
    via Sequence Modeling - life_code - BV1X84y1Q7wV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '![](img/041a2ae7aae080f075f80cd7c4c33a68_0.png)'
  prefs: []
  type: TYPE_IMG
- en: So I'm excited to talk today about our recent work on using Transers for reinforcement
    learning and this is joint work with a bunch of really exciting collaborators„ÄÇmost
    of them at UC Berkeley and some of them at Facebook and Google„ÄÇI should mention
    this work was led by way to talented undergradsÔºå Li Chen and Kevin Blue„ÄÇ![](img/041a2ae7aae080f075f80cd7c4c33a68_2.png)
  prefs: []
  type: TYPE_NORMAL
- en: And I'm excited to present the results we hadÔºå so let's try to motivate why
    we even care about this problem„ÄÇSo we have seen in the last„ÄÇThree or four yearsÔºå
    that transformers since the introduction in 2017 have taken over lots and lots
    of different fields of artificial intelligence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we saw them having a big impact for language processingÔºå we saw them being
    used for vision„ÄÇthe vision transformer very recently they were in nature trying
    to solve protein folding and very soon they might just replace as computer scientists
    by having automatically generate code„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So with all of these advancesÔºå it seems like we are getting closer to having
    a unified model for decision making for artificial intelligence„ÄÇBut artificial
    intelligence is much more about not just having perception„ÄÇbut also using the
    perception knowledge to make decisions„ÄÇAnd this is what this talk is going to
    be about„ÄÇBut before I go into actually thinking about how we will use these models
    decision making„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: here is a motivation for why I think it is important to ask this question„ÄÇSo
    unlike models for RL„ÄÇwhen we look at transformers for perception modalities like
    I showed in the previous slide„ÄÇwe find that these models are very scalable and
    have very stable training dynamics so you can keep as long as you have enough
    computation and you have more and more data that can be sourced„ÄÇyou can train
    bigger and bigger models and you'll see very smooth reductions in the loss„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And the overall training dynamics are very stableÔºå and this makes it very easy
    for practitioners and researchers to build these models and learn richer and richer
    distributions„ÄÇSo like I saidÔºå all of these advances have so far occurred in perception
    what we've been interested in this talk is to think about how we can go from perception„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: looking at imagesÔºå looking at text and all these kinds of sensory signals to
    then going into the field of actually taking actions and making our agents do
    interesting things in the world„ÄÇüòäÔºåÂïä„ÄÇAnd here throughout the talkÔºå we should be
    thinking about why this perspective is going to enable us to do scalable learning„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like I showed on the previous slideÔºå as well as brings stability into the whole
    procedure„ÄÇüòä„ÄÇSo sequential decision making is a very broad area and what I'm specifically
    going to be focusing on today is the one route to sequential decision making that's
    reinforcement learning„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So just as a brief backgroundÔºå what is reinforcement learning„ÄÇso we are given
    an agent who' is in a current state and the agent is going to interact with the
    environment by taking actions„ÄÇAnd by taking these actionsÔºå the environment is
    going to return to it a reward„ÄÇFor how good that action was as well as next state
    into which the agent will transition and this whole feedback loop will continue„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Âïä„ÄÇThe goal here for an intelligent agent is to then using trial and error„ÄÇso
    try out different actionsÔºå see what rewards will lead to learn a policy which
    maps your states to actions such that the policy maximizes the agent's cumulative
    rewards over time horizon„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you take a sequence of actions and then based on the reward you accumulate
    for that sequence of actions„ÄÇwill' judge how good your policy is„ÄÇThis talk is
    also going to be specifically focused on a form of reinforcement learning that
    goes by the name of offline reinforcement learning„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the idea here is that what changes from the previous picture where I was
    talking about online learning„ÄÇonline reinforcement learning is that here now instead
    of doing actively interacting with the environment„ÄÇyou have a collection of log
    data of interactions so think about some robot that's going out in the fields
    and it collects a bunch of sensory data and you have all logged it and using that
    log data you now want to train another agent„ÄÇit could be another robot to then
    learn something interesting about that environment just by looking at the log
    data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: G so theres no trial and error component„ÄÇWhich is currently one of the extensions
    of this framework„ÄÇwhich would be very excitingÔºå so I'll talk about this towards
    the end of the talk why it's exciting to think about how we can extend this framework
    to include an exploration component and have trial and error„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so not to go more concretely into what the motivating challenge of the
    stock was now that they have introduced ArL„ÄÇso let's look at some statistics„ÄÇSo
    large language models„ÄÇI have billions of parameters and they have today they have
    roughly about 100 layers in Transformer„ÄÇthey are very stable to train using supervised
    learningÔºå style losses„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which are the building blocks of autoregressive generationÔºå for instance„ÄÇor
    for mass language modeling as in B„ÄÇAnd this is like a field that's growing every
    day and there's a course on it at Stanford that we're all taking just because
    it has had such a monumental impact on AI„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂóØ„ÄÇRL policiesÔºå on the other handÔºå and I'm talking about deep RL„ÄÇthe maximum
    they would extend to is maybe millions of parameters at 20 layers„ÄÇÂóØ„ÄÇAnd what's
    really unnerving is that they're very unstable to train so the current algorithms
    for reinforcement learning they build on mostly dynamic programming which involves
    solving an inter loop optimization problem that's very unstable and it's very
    common to see practitioners in RL looking at reward reward code that look like
    this so what I really want you to see here is the variance in the returns that
    we tend to get in RL it's really huge even after doing multiple rounds of experimentation
    and that's often that is really at the code got to done with the fact that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Our algorithms learning checks need better improvements so that the performance
    can be stably achieved by Asians„ÄÇIn complex environments„ÄÇSo what this work is
    hoping to do is it's meant to introduce transformers„ÄÇAnd I'll first show in one
    slide what exactly that model looks like and we're going to go into deeper details
    of each of the components„ÄÇI have good question„ÄÇYeahÔºå I can ask a question real
    quick„ÄÇYes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and thank you what I'm curious to know is is the what is the cause for why RL
    typically has several orders of magnitude fewer parametersÔºü
  prefs: []
  type: TYPE_NORMAL
- en: That's a great questionÔºå so typically when you think about reinforcement learning
    algorithms„ÄÇüòä„ÄÇIn deep art in particularÔºå so the most common algorithmsÔºå for example„ÄÇhave
    different networks playing different roles in the task so you'll have a network
    for instance„ÄÇplaying the role of an actor so trying to figure out a policy and
    then therell a different network that's playing the role of a critic„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And these networks are trained on data that's adaptively gathered so unlike
    perception where you will have a huge data set of interactions on which you can
    train your models in this case the architectures and even the environments to
    some extent„ÄÇare very simplistic because of the fact that we are trying to train
    very small components the functions that we're training and then bringing them
    all together and these functions are often trained in not super complex environments„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so it's a mix of different issues I wouldn't say it's purely just got to with
    the fact that the learning objectives are fault„ÄÇbut it's a combination of the
    environments we use the combination of the targets that each of the neural networks
    are predicting which leads to networks which are much bigger than„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: What we currently see tending to orÔºå and that's why it's very common to see
    neural networks with much fewer layers being used in RL as opposed to perception„ÄÇThank
    you„ÄÇYou want to ask a questionep yeahÔºå I was going to is there a reason why you
    chose offline RL versus online RLÔºü
  prefs: []
  type: TYPE_NORMAL
- en: That's another great question so the question is why offline R is opposed to
    online RL and the plain reason is because all this is a first work trying to look
    at reinforcement learning so offline RL avoids this problem of exploration„ÄÇYou
    are given a log data set of interactions you're not allowed to further interact
    with the environment so just from this data set you're trying to unearth a policy
    of what the optimal agent would look like so it would right like if you do online
    RL wouldn't that like just give you this opportunity of exploration basically
    it would it would what would also do which is a technically challenging here is
    that the exploration would be harder to encode so offline R is the first step
    there is no reason why we should not study why online RL cannot be done it's just
    that it provides a more contained setup where ideas from transformers will directly
    extend„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå sounds goodÔºå so let's look at the model and it's really simple on purpose„ÄÇSo
    what we're going to do is we're going to look at our offline data„ÄÇwhich is essentially
    in the form of tra treesÔºå so offline data would look like a sequence of states„ÄÇactionsÔºå
    returns over multiple time steps„ÄÇIt's a sequence that's natural to think of us
    as directly feeding as input to a transformer in this case we use a causal transformer
    as it's common in GPT„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So we go from left to right and because the data set comes with the notion of
    time step causality here is much more well intended than the general meaning that's
    used for perception this is really causality how it should be in perspective of
    time„ÄÇWhat we predict out of this transformer are the actions conditioned on everything
    that comes before that token in the sequence„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So if you want to predict the action at this t minus one stepÔºå we'll use everything
    that came„ÄÇAt time step D minus twoÔºå as well as the returns and states at time
    step D minus one„ÄÇOkay„ÄÇSo we will go into the details of how exactly each of these
    are encoded„ÄÇbut essentially this is in a one linerÔºå it's taking the tragic pre
    datata from the offline data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: treating it as a sequence of tokensÔºå passing it through a causal transformer
    and getting a sequence of actions as the output„ÄÇOkayÔºå so how exactly do we do
    the forward path through the networkÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So one important aspect of this workÔºå which is the U states' actions and this
    quantity called returns to room„ÄÇSo these are not direct rewards„ÄÇThese are returns
    to go and let's see what they really mean„ÄÇSo this is a trajectory that goes as
    input„ÄÇand the returns to go are the sum of rewards starting from the current time
    step into until the end of the episode„ÄÇSo really what we want the transformer
    is to get better at using a target return„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: this is how you should think of returns to go„ÄÇAs the input in deciding what
    action to take„ÄÇThis perspective is going to have multiple advantages if it willll
    allow us to actually do much more than offline RL and generalizing to different
    tasks by just changing the returns to go„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here it's very importantÔºå so at time step one we will just have the overall
    sum of rewards for the entire tragic dream at time step two we subtract the reward
    we get by taking the first action and then have the sum of rewards for the remainder
    of the tragic dream„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: OkayÔºå so that's how we quality turns to go like how many more rewards in accumulation
    you need to acquire to fulfill your„ÄÇRe your return goal that is set the beginning„ÄÇWhat
    is the output„ÄÇthe output is the sequence of predicted actionsÔºå so as I showed
    in the previous slide„ÄÇwe use a causal transformer so we predict in sequence the
    desired actions„ÄÇThe attentionÔºå which is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Going to be computed inside the transformer with taking an important hyperparameter
    key„ÄÇwhich is the context length„ÄÇWe see that in perception as well hereÔºå and for
    the rest of the talk„ÄÇI'm going to use the notation K to denote how many tokens
    in the past would we be attending over to predict the action and the current time
    step„ÄÇOkayÔºå so againÔºå digging a little bit deeper into code„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: There are some subtle differences with how a decision transformer operates as
    opposed to a normal transformer„ÄÇthe first is that here the time step notion is
    going to be„ÄÇÂì¶„ÄÇHave a much bigger semantics that extends across three tokens„ÄÇSo
    in perception„ÄÇyou just think about the time step per wordÔºå for instanceÔºå like
    in NLP or per patch for vision„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and in this case we will have a time step encapsulating three tokensÔºå one for
    the states„ÄÇone for the actions and one for the rewards„ÄÇAnd then we embed each
    of these tokens and then add the pollution embedding as it common in the transformer
    and we feed those inputs to the transformer at the output we only care about one
    of these fee tokens in those default side I will say experiments where even the
    other tokens might be of interest as target predictions but for now let's keep
    it simple„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we want to learn a policy a policy which is trying to predict actions„ÄÇSo when
    we try to decode„ÄÇwe'll only be looking at the actions from the hidden representation
    in the prefin layer„ÄÇOkay„ÄÇso this is the forward pass now what do we do with this
    networkÔºå we train it how do we train itÔºüüòä„ÄÇSorryÔºå just a quick question on semantics
    there if you go back one slide„ÄÇThe plus in this case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the syntax means that you are actually adding the values element wise and not
    concatenating them„ÄÇis that rightÔºüThat is correct„ÄÇÂÖ¨„ÄÇOkayÔºå so what's the last function
    follow up on that„ÄÇI thought it was concotÔºå why are we just adding itÔºüSorryÔºå can
    you go backÔºüYeah„ÄÇI think it's a design choiceÔºå you can pattern in itÔºå you can
    add it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it leads to different functions being encoded in our case it was editioned„ÄÇO„ÄÇWhy
    did you try the other one and it just didn't work or why is that because I think
    intuitively concatenating would like make more sense„ÄÇSo I think both of them have
    different use cases for the functional of encoding like one is really mixing in
    the embeddings for the state and basically shifting it so and you add something
    if you think of the embedding of the states as a vector and you add something
    you are actually shifting it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Whereas in the concatetnation caseÔºå you are actually increasing the dimensionality
    of this space„ÄÇüòä„ÄÇYeahÂëÄ„ÄÇSo those are different choices which are doing very different
    things we found this one to be worked better i'm not sure I remember if the results
    was very significantly different if you would concatetnate them„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but this is the one which we operate with„ÄÇBut wouldn't there because like if
    you're shifting it right like if you have an em bedding for a state and let's
    say like you perform certain actions and you like end up at the same state again„ÄÇYou
    would want these embeddings to be the same however now you're at a different time
    step so like you shifted it so wouldn't that be like harder to learnÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So there's a bigger and interesting question in that what you said is basically„ÄÇare
    we losing the Marco propertyÔºüBecause as you said that if we come back to the same
    state at a different time step„ÄÇshouldn't we be doing similar operations and the
    answer here is yesÔºå we are actually being nonmarco„ÄÇAnd this might seem very nonintuitive
    at first that why is nonmarkovness important here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: and I want to refer to another paper which came very much in conjunction with
    this a transer that actually shows in more detail and its basically says that
    if you were trying to predict the transition dynamics„ÄÇthen you could have actually
    had a Markovvian system built in hereÔºå which would do just as good„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: HoweverÔºå for the perspective of trying to actually predict actions„ÄÇit does help
    to look at the previous time stepsÔºå even more so when you have missing observations
    so for instance„ÄÇif you have the observations being a subset of the true state
    so looking at the previous states and actions helps you better fill in the missing
    pieces in some sense so this is commonly known as partial observability where
    you by looking at the previous tokens you can do a better job at predicting the
    actions that you should take at the current time step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So nonmarkness is on purposeÔºå and it's non intuitive„ÄÇbut I think it's one of
    the things that separates this framework from existing ones„ÄÇSo it will basically
    help you because like RL usually works on like infinite like works better on like
    infinite horizon problems right so technically the way you formulated it it would
    work better on finite horizon problems I'm assuming because you want to take different
    actions based on like a history based on like given the fact that now you have
    to a different time step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Yeah yeah so if you wanted to work on In Verorizon maybe something like discounting
    would work just as well to get that effect in this case we were using a discount
    factor of one or basically like no discounting at all„ÄÇbut you're right if I think
    we really want to extend it to In horizonizon you would need to change the discount
    factor„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Thanks„ÄÇOkayÔºå so questions„ÄÇOh„ÄÇI think it was just answered in chatÔºå but I'll
    ask it anyways„ÄÇI think I might have missed this or maybe you're about to talk
    about it„ÄÇthe offline data that was collectedÔºå what policy was used to collect
    it„ÄÇSo this is a very important question and it will be something I mentioned the
    experiments„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we were using the benchmarks that exist for our scenario where essentially
    the way these benchmarks are constructed as„ÄÇYou train an agent using online RL
    and then you look at its replay buffer at some time step„ÄÇSo while training so
    while it's like a medium sort of expert„ÄÇyou collect it the transition its' experience
    so far and make that as the offline data' it's something which is like our framework
    is very agnostic to what offline data that you use so I've not discussed so far
    but make something that in our experiments is based on traditional benchmarks
    got it so the reason I ask isn't I'm sure that your framework can accommodate
    any offline data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but it seems to me like the results that you're about to present are going to
    be heavily contingent on what that that data collection policy is„ÄÇIndeedÔºå indeedÔºå
    and also so we willÔºå I think I have a slide where we show an experiment where
    the amount of data can make a difference in how we compare with baselines and
    essentially we will see how distant transformer especially shines when there is
    small amounts of offline data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Áü•ÈÅì„ÄÇOkayÔºå cool„ÄÇthank you„ÄÇOkay great questions so let let's go ahead so we have
    defined our model which is going to look at these tra trees and now let's see
    how we train it so very simple we are trying to fill actions„ÄÇwe'll try to match
    them to the ones we have in our data set if they are continuous using the mean
    square error if they are discrete and we can use across entropy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåUm„ÄÇBut there is something very deep in here for our research„ÄÇwhich is that
    these objectives are very stable to train and easily regularize because they've
    been developed for supervised learning„ÄÇIn contrastÔºå what R is more used to is
    dynamic programming„ÄÇstyle objectives which are based on the Belman equation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And those end up being much harder to optimize and scale„ÄÇand that's why you
    see a lot of the variance in the results as well„ÄÇOkay so this is how we train
    the model now how do we use the model and that's the point about trying to do
    rollouts for the model so here again this is going to be similar to doing an autoregressive
    generation there was an important token here which was the returns to go„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And what we need to set during evaluationÔºå presumably we want export level performance
    because that will have the highest returns„ÄÇso we set the initial returns to go„ÄÇNot
    based on a tragic tree because now we don't have a tragic tree we're going to
    generate a tragic tree so this is at entrance time„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so we will set it to the expatory term for instance„ÄÇSo in code what this whole
    procedure would look like is basically you set this returns to go token to as
    some target return and you set your initial state to one from the environment
    distribution of initial states„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then you just roll out your decision transformer„ÄÇSo you get a new action„ÄÇthis
    action will also give you a state and reward from the environment„ÄÇyou append them
    to your sequence and you get a new returns to go and you take just the context
    and key because that's what's used by the transformer to making predictions and
    then field it back to the distant transformer„ÄÇSo it's regular auto regressive
    generationÔºå but the only key point to notice is how you initialize the transformer
    for RL„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So add one question here„ÄÇSo how much is it choice on the expert target written
    method„ÄÇis it does it have to be like the mean expert reward or can it be like
    the maximum reward possible in demandÔºü
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåLikeÔºå does that try of the number really matterÔºüThat's a very good question„ÄÇso
    we generally would set it to be slightly higher than the max return in the data
    so I think the fact that we use was 1„ÄÇ1 timesÔºå but I think we have done a lot
    of experimentation in the range and it's fairly robust to what choice you use„ÄÇSo
    for exampleÔºå for hopper export returns about 3600 and we have found very stable
    performance from all the way from like 30„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 5400 to even going to very high numbers like 5000 it works„ÄÇÂóØ„ÄÇYeahÔºå so however„ÄÇI
    would want to point out that this is something which is not typically needed in
    regular RL like knowing the exposure return here we are actually going beyond
    regular Rl and that we can choose a return we want so we also actually need this
    information about what the exposure return is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: At that point„ÄÇThanks„ÄÇThere's another question„ÄÇYesÔºå so yeahÔºå it' just you„ÄÇyou
    cannot be on the regular oilÔºå but I'm curious about„ÄÇDo you also like restrict
    this framework to only offline oil cost„ÄÇIf you wanna run this kind of framework
    in online oil„ÄÇyou'll have to determine the returns to go a priorary„ÄÇ So this kind
    of framework„ÄÇI think is kind of restricted to only offline oil„ÄÇ Do you think so„ÄÇYes„ÄÇand
    I think asking this question as well earlier that yesÔºå I think for now this is
    the first work„ÄÇso we were focusing on offline Rs where this information can be
    gathered from the offline data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Âïä„ÄÇIt is possible to think about strategies on how you can even get this online
    what you'll need is a curriculum so early on during training as you're gathering
    data„ÄÇYou will set when you're doing rolloutsÔºå you will set your export return
    to whatever you see in the data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And then incremented as when we start seeing that the transformer can actually
    exceed that performance„ÄÇSo you can think of specifying a curriculum from slow
    to high for what that export return could be for Biitzitchu rollout„ÄÇChs the the
    decision transforming„ÄÇI say cool„ÄÇ Thank you„ÄÇSo yeahÔºå this was about the model„ÄÇso
    we discussed how this model isÔºå what the input to these model areÔºå what the outputs
    are„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: what the loss function is used for training this modelÔºå and how do we use this
    model at test time„ÄÇThere is a connection to this framework as being one way to
    instantiate what is often known as all as probabilistic inference„ÄÇSo we can formulate
    RL as a graphical model problem where you have these states and actions being
    used to determine what the next state is„ÄÇAnd to encode a notion of optimalityÔºå
    typically you would also have these additional a variablesÔºå 01„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: 02 and so on forthÔºå which are implicitly saying that encoding some notion of
    reward„ÄÇand conditioned on this optimality being trueÔºå RL is the task of learning
    a policy„ÄÇwhich is the mapping from states to actions such that we get optimal
    behavior„ÄÇAnd if you really squint your eyesÔºå you can see that these optimality
    variables and decision transformers are actually being encoded by the returns
    to go„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåSo if then we give„ÄÇA value that's high enough at this time during rollouts
    like the expert return„ÄÇwe are essentially saying that conditioned on this being„ÄÇThe
    mathematical form of quantification of optimality„ÄÇRoll out your listen transformer
    to hopefully satisfy this condition„ÄÇSo„ÄÇYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so this was all I want to talk about the model' can you explain thatÔºå pleaseÔºü
  prefs: []
  type: TYPE_NORMAL
- en: What do you mean by optimality variables in the decision transformer and how
    do you mean like return to goÔºü
  prefs: []
  type: TYPE_NORMAL
- en: RightÔºå so optimality variables we can think in the more simplest context as
    legislative or binary„ÄÇso one is if you„ÄÇSolt the goal and zero as if you did not
    solve the goal„ÄÇAnd what basically in that caseÔºå you could also think of your decision
    transformer as at test time and we encode the returns to go„ÄÇWe could set it to
    oneÔºå which will basically mean that conditioned on optimalities optimality here
    means„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Solving the goal as one„ÄÇGenerate me the sequence of actions such that this would
    be true„ÄÇOf course„ÄÇour learning is not perfectÔºå so it's not guarantee we'll get
    that„ÄÇbut we have trained the transformer in a way to interpret the returns to
    go as some notion of optimality„ÄÇSo is it is it if I'm interpreting this correctlyÔºå
    it's roughly like saying„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: show me what an optimal sequence of transitions look like„ÄÇBecause you've learned
    the model has learned sort of both successful and unsuccessful transitions„ÄÇExactly
    exactly and as we shall seen some experimentsÔºå we can for the binary case„ÄÇit's
    either optimal or non optimalÔºå but rarely this can be a continuous variable which
    it is in our experiment so we can also see what happens in between experimentally„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåÂì¶„ÄÇOkayÔºå so let's jump into the experiments so there are a bunch of experiments
    and I've picked out a few which I think are interesting and give the key results
    in the paper but feel free to refer the paper for you and even more detailed analysis
    on some of the components of our module„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So this first we can look at how well does it do on offline RL„ÄÇso there are
    benchmarks for the Atari suite of environments and the Open eye gym and we have
    another environment keyto door which is especially hard because it contains Pass
    Was and requires you to do credit assignment that I'll talk about later„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: But across the board we see that thisilent transformer is competitive with the
    state of the art model free offline RL methods in this case„ÄÇthis was a version
    of Q learning designed for offline RL„ÄÇAnd it can do excellent when especially
    when there is long term assignment where traditional methods based on T learning
    would fail„ÄÇYeahÔºå so the takeaway here should not be that we should we are at the
    stage where we can just substitute the existing algorithms for the decision transformer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but this is a very strong evidence in favor that this paradigm which is building
    on transformers will permit us to better aerate and improve the models to hopefully
    surpass the existing algorithms uniformly„ÄÇand there is some early evidence of
    that in harder environments which do require credit long term credit assignment„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Can I ask a question here about the baselineÔºå specifically TD learningÔºü
  prefs: []
  type: TYPE_NORMAL
- en: I'm curious to know because I know that a lot of TD learning agents are feed
    forward networks are these baselines do they have recurrence„ÄÇYeahÔºå yeahÔºå so I
    think the conservative dual learning baselines here did have recur„ÄÇbut I'm not
    very sure so I can check back on this offline and get back to you on this„ÄÇO k„ÄÇWill
    be okay„ÄÇThank you„ÄÇAlso another quick question„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so just how exactly do you evaluate the decision transformer here in the experiment
    so because you need to supply the returns to go so do you use the optimal„ÄÇLike
    policy to get what's optimal reward and that in„ÄÇso so here we basically look at
    the offline data that that is useful for training„ÄÇAnd we said whatever was the
    maximum return in the offline dataÔºå we set the„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Desire target are going to go as slightly higher than thatÔºå so 1„ÄÇ1 broke coefficient
    used„ÄÇI see so and the performance sorry I'm not really what we're seeing or else
    but how is the performance defined here it just like is how much reward you get
    actually from the Yes yes so you can specify target return to go„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but there's no guarantee that the actual actions that you take will achieve
    that return so yeah so you you measure the true environmental return based on
    yeah I see but then like just curious like so what so are these performance the
    percentage you get like for like how much I guess your you recover from„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The next few months yeah so these are not percentages„ÄÇthese are some we have
    normalizing the returns so that everything costs between the 200„ÄÇYeahÔºå yeah„ÄÇI
    see then just wonder if you have a like a rough idea about how much like reward
    actually is recovered by the student transformers like does they say like if you
    specify„ÄÇI want to get like 50 rewardsÔºå does it get 49 or is this even better sometimes
    or„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That's an excellent question and my next„ÄÇSo here we're going to answer precisely
    this question that will asked is like if you feed in the target return„ÄÇit could
    be expert or it could also non be expert how well does the mall actually do in
    attaining itÔºü
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåSo the x axis is what we specify as the target return we want„ÄÇand the y axis
    is basically how muchÔºå how well do we actually getÔºüFor reference„ÄÇwe have this
    green lineÔºå which is the oraclesÔºå so which means„ÄÇüòä„ÄÇWhatever you desire that this
    student transformer gives it to you so this would have been the ideal case„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so it's a diagonal„ÄÇWe also haveÔºå because this is offline RL we have in orange„ÄÇwhat
    was the best project between data setsÔºå so the offline data is not perfect„ÄÇso
    we just plot what is the upper bound on the offline data performance„ÄÇAnd here
    we find that for the majority of the environments„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: there is a good fit between the target return we feed in and the actual performance
    of the model„ÄÇAnd there are some other observations which I wanted to take from
    this slide is that„ÄÇBecause they can vary this notion of reward„ÄÇWe can in some
    sense do multitask R by reward condition return conditioning this is not the only
    way to do multitask parallel you can specify a task via natural language you can
    via goal state and so on forth„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but this is one notion where the notion of a task could be how much reward you
    want„ÄÇÂóØ„ÄÇAnd another thing to notice is occasionally these smallest extrapolate
    this is not a trend we have been seeing consistently„ÄÇbut we do see some signs
    of itÔºå So if you look at for example Sequest„ÄÇüòäÔºåHere„ÄÇthe highest return tri cleanerer
    data sets was pretty low„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And if we specify a return higher than that„ÄÇFor our decision transformer„ÄÇwe
    do find that the model is able to achieve„ÄÇSo it is able to generate tragic trees
    with returns higher than it ever saw in the day„ÄÇÂóØ„ÄÇI do believe that future work
    in this space trying to improve this model should think about how can this trend
    be more consistent across environments because this would really achieve the goal
    of offine RL„ÄÇwhich is given suboptimal behaviorÔºå how do you get optimal behavior
    out of itÔºü
  prefs: []
  type: TYPE_NORMAL
- en: But remains to be seen how well this trend can be made consistent across environments„ÄÇCan
    I jump in with a question„ÄÇYesÔºå so I think that last point is really interesting
    and it's cool that you guys occasionally see it„ÄÇI'm curious to know what happens„ÄÇSo
    this is all condition you give as an input„ÄÇwhat return you would like and it tries
    to select a sequence of actions that gives it„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I'm curious to know what happens if you just give it ridiculous inputs like
    for example„ÄÇyou know here here the order of magnitude for the return is like 50
    to 100„ÄÇWhat happens if you put in 10000„ÄÇGood question and this is something we
    tried early on I don't want to say if we went up to 10000 but we try like really
    high returns which not even an expert could get and generally we see this leveling
    performance so you can see hints of it in half Che arm and you know pong as well
    or Walker to some extent that if you look at the very end things start saturating„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåSo if you exceed what is like certain threshold which often corresponds with
    the best tragic threshold„ÄÇbut not always beyond that everything is similar returns
    so it's not that it so at least one good thing is it does not degrade in performance
    so it would have been a little bit worrying if you specified a return of 10000
    and gives you return which is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Train0 or something really new„ÄÇSo it's good that it stabilizes„ÄÇbut it's not
    that it keeps increasing on and onÔºå so there would be a point where the performance
    would get saturated„ÄÇOkayÔºå thank you„ÄÇI was also curious like so usually for transfer
    models you need a lot of data„ÄÇso do you know how well like like how much data
    do you need like how where does it scale the data the performanceÔºü
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå so here we use„ÄÇThe standard„ÄÇUmData like the D4 RL benchmarks for Mexico„ÄÇwhich
    I think have million u transitions in the order of millions„ÄÇFor„ÄÇAtari„ÄÇwe used
    one person of the replay bufferÔºå which is smaller than the one we used for the
    Mujoco benchmarks„ÄÇand I actually have a result in the very next slide which shows
    dis transformer especially being useful when you have a little data„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So yeahÔºå so I guess one question to ask before you move on„ÄÇIn the last slide„ÄÇwhat
    do you mean again by return conditioning for like the multitask partÔºüYeah„ÄÇso if
    you think about the returns to go at test time„ÄÇthe one you feed have to feed in
    as the starting token„ÄÇAs one girl„ÄÇSpecifying what policy you want„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂñÇ„ÄÇHow is that multitÔºüSo it's multitask in the sense that because you can get
    different policies by changing your target return to go„ÄÇyou're essentially getting
    different behaviors encoded so think about for instance a hopper and you specify
    a return to go that's really low„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So you're basically sayingÔºå get me an agent which will just stick around its
    initial state„ÄÇAnd not going into unchared territory„ÄÇAnd if you give it reallyÔºå
    really high„ÄÇthen you're asking it to do the traditional taskÔºå which is to hop
    and go as far as possible without falling but can you qualify those multitask
    because that basically just means that your return conditioning is a cue for it
    to memorize right which is usually like one of the pitfalls of multitask„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I'm not sure if it's a task identifierÔºå that's what I'm trying to say„ÄÇSo
    I'm not sure if it's memorization because like I think the purpose of this„ÄÇI mean„ÄÇlike
    having an offline data set that's fixed is basically saying that it's very„ÄÇvery
    specific to if you had the same start state and you took the same actions and
    you had the same target fair turns„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: that would qualify as memorization„ÄÇBut here at test time we allow all of these
    things to change and in fact they do change so your initial state would be different„ÄÇyour
    target return it could be very could be a different scalar than one you ever saw
    during training„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It yeah and so so essentially the model has to learn to generate that behavior
    starting from a different initial state and maybe a different value of the target
    return than it saw during during during training if the dynamics are stochastic
    that also makes so that even if you memorize the actions you're not guaranteed
    to get the next the same next state„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so you would actually have a bad correlation with the performance of the dynamics
    are also stochastic„ÄÇAlsoÔºå I was very curious how much time does it take to train
    business transformerÔºüIn general„ÄÇSo it takes about a few hoursÔºå so I want to say
    like about„ÄÇ45 hours„ÄÇDepending on what quality GPU useÔºå but yeah thats that's a
    reasonable estimate y call it thanks„ÄÇOkay„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so actually while doing this experimentÔºå this project„ÄÇwe thought of a baseline
    which we were surprised as not there in previously traditional on offine RL but
    makes very much sense and we thought we should also think about whether decision
    transformers are actually doing something very similar that baseline and the baseline
    is what we call as person behavioral cloning„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So behavioral cloÔºå what it does is basically it ignores the returns„ÄÇAnd simply
    imitates the agent looking by just trying to map the actions given the current
    states„ÄÇThis is not a good idea with an offline inter set which will have project
    trees of both low returns and high returns„ÄÇSo traditional vehicle cloningÔºå it's
    common to see that as a baseline in offline Ral methods and it is unless you have
    a very high quality data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: does it is not a good baseline for offline RL„ÄÇHoweverÔºå there is a version that
    we call as Per BC„ÄÇwhich actually makes quite a lot of sense and in this version„ÄÇWe
    filter out the top tra trees from our offline set„ÄÇonce stop the ones which have
    the highest rewards„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you know the rewards for each transition you calculate the returns of the tra
    trees and you take the tra trees with the highest returns and keep a certain percentage
    of them„ÄÇwhich is going to be hyperparameter here„ÄÇAnd once you keep those top fraction
    of your tragic trees„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: you then just ask your module to imitate them„ÄÇWhi so imitation learning also
    uses especially when it's used in the form of behavioral planning„ÄÇit uses supervised
    learningÔºå essentially it's a supervised learning problem so you could actually
    also get supervised learning objective functions if you did this filtering step„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Oh„ÄÇSo„ÄÇAnd what we find actually that for the moderate and highal regimes„ÄÇthe
    recent transform is actually very comparable to Pu and VC so it's a very strong
    baseline which I think all the future work in offline Rs should include there's
    actually an eyeC submission from last week which has a much more detailed analysis
    on justice this baseline that introduced in this paper„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And„ÄÇWhat we do find is that for low data regimes the dis transformer does much
    better than person behavioral clo„ÄÇso this is for the Atari benchmarks where like
    I previously mentioned we have a much smaller data set„ÄÇAs compared to the Mojoko
    environmentsÔºå and here we find that for even after varying the different fraction
    of the percentage hyperparameter here„ÄÇwe are generally not able to get the strong
    performance status in transformer gets„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So 10% DCc basically means that we filter out and keep the top 10% of the trajectories
    if you go even lower then the start data set becomes fairly small so the baseline
    become meaningless„ÄÇBut for even the reasonable rangesÔºå we'd never find the performance
    matching that of the transformers for these Atari benchmarks„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: ÂëÉÁöÑÈìÅ I may„ÄÇSo I notice in table threeÔºå for exampleÔºå which is not this table with
    one just before in the paper„ÄÇthere's a report on the CQL performanceÔºå which to
    me also feels you know intuitively pretty similar to the percent BC in the sense
    of like you pick trajectories you know performing well and you try and stay roughly
    within sort of the same kind of policy distribution„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It state space distribution„ÄÇI was curious on this one„ÄÇdo you have a sense of
    what the CQL performance was relative to say the percent BC performance hereÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So that's a great questionÔºå question is that even for CQL you rely on this notion
    of pessimism where you want to pick trajectories where you're more confident in
    and try to make sure policy remains in that region„ÄÇso I don't have the numbers
    with CQL on this table„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but if you look at the detailed results for Atring„ÄÇThen I think should they
    should have the SQL for sure because that's the numbers we are reporting here
    so I can tell you what the sQL performance is actually pretty good and it's very
    competitive„ÄÇWhen the decision transformer for aaryÔºüSo this TD learning baseline
    here is SeQ„ÄÇSo financially by extensionÔºå I would imagine it doing better than
    person B„ÄÇYeah„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And apologize apologize if this was mentionedÔºå I just missed it„ÄÇbut if you have
    the sense that this is basically like a failure of CQL to be able to extrapolate
    well or sort of stitch together different parts of trajectories„ÄÇWhereas a decision
    transformer can sort of make that extrapolation between you have like the first
    half of one trajectory is really good the second half of one trajectory is really
    good so you can actually piece those together a decision transformer where you
    can't necessarily do that with CQL because the path connecting those may not necessarily
    be well covered by the behavior policy„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: YeahÔºå yeahÔºå so and this actually goes to one of the intuitions which I did not
    emphasize too much„ÄÇbut we have a discussion on the paper where essentially why
    do we expect a transformer or any model for that matter to look at offline data
    that's suboptimal and get something a policy which generates optimal loadoutsÔºü
  prefs: []
  type: TYPE_NORMAL
- en: The intuition is that as Scott was mentioningÔºå you could perhaps„ÄÇStitch together„ÄÇGood
    behaviors from suboptimal trajectories and that stitching could perhaps lead to
    a behavior that was better than anything you saw in individual tra trees in your
    data set„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: It's something we find earlier evidence of small skill experiment for graphs„ÄÇand
    that's really our hope also that something that will transformers very good at
    because it can attend to very long sequences so it could identify those segments
    of behavior„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which when stitch together would give you optimal optimal behavior„ÄÇSo„ÄÇAnd it's
    very much possible that is something unique to this in transformers and something
    like CQL would not be able to do person BCC because it's filtering out the data
    is automatically being limited and not being able to do that because those segments
    of good behavior could be in trajectories which overall do not have a high return„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but so if you filter them outÔºå you are losing all of that information„ÄÇOkay„ÄÇso
    I said there is hyperparameter the context in K and like with most of perception„ÄÇone
    of the big advantages of transformers opposed to other sequence models like LSTMs
    is that they can process really large sequences„ÄÇAnd here at a first glanceÔºå it
    might seem that being Markcoan would have been helpful for RL„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which also was a question that was raised earlier„ÄÇso we did this experiment
    where we did compare performance with context and K equals1 and here we had context
    and between 30 for the environments in 50 for pong and we find that increasing
    the context length is very„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: very important to get good performance„ÄÇOkay„ÄÇNowÂëÉ„ÄÇSo so far I've showed you how
    this in transformformer„ÄÇwhich is very simpleÔºå there was no slide I had which was
    going into the details of dynamic programming„ÄÇwhich is the crux of most RÔºå this
    was just pure supervised learning in an autoregressive framework that was getting
    us as good performance„ÄÇÂóØ„ÄÇWhat about cases where this approach actually starts
    outperforming some of the traditional methods are so to probe a little bit further
    we start looking at sparse reward environments and basically we just took our
    existing mojoku environments and then instead of giving it the information for
    reward for every transition we fed it the cumulative reward at the end of the
    trajectory so every transition will have a zero reward except the very end where
    you get the entire reward at once it's a very sparse reward perform scenario for
    that reason„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And here we find that„ÄÇÂì¶„ÄÇCompared to the original denses resultsÔºå the delayed
    results for Dt„ÄÇthey will deteriorate a little bitÔºå which is expected because now
    you are withholding some of the more fine grain information at every time step„ÄÇbut
    the drop is„ÄÇNot too significant compared to the original E performance here„ÄÇwhereas
    for something like CQL there is a drastic drop in performance so CQL suffers quite
    a lot in sparse reward scenarios„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but the distant transformer does more„ÄÇAnd just for completeness if you also
    have performance of behavioral training and person behavioral clo„ÄÇwhich because
    they don't look at reward information„ÄÇexcept maybe person BC looks at only for
    preprocessing the data set„ÄÇthese are agnostic to whether the environments as spa
    wordss or not„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Would you expect this to be different if you were doing online R„ÄÇÂóØ„ÄÇWhat's the
    intuition for it being different April I would say no„ÄÇbut maybe I'm missing out
    on a key piece of intuition behind that question„ÄÇÂóØ„ÄÇI think that because you're
    training like offlineÔºå right„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like your're the next input will always be the correct action in that sense„ÄÇSo
    you don't just like deviate and like go off the rails technically because you
    just don't know„ÄÇSo I could see like how online would have like a really hard„ÄÇLike
    cold start basically because it just doesn't know and it's just happeningping
    in the dark until it like maybe eventually hits the jackpot„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: RightÔºå right I think I agree that so that's a good piece of intuition out there
    that yeah„ÄÇI think here because offline R is really„ÄÇGetting rid of the trial and
    error aspect of it and firstpa reward environment that would be harder so„ÄÇThe
    drop in DT performance should be more prominent there„ÄÇI'm not sure how it would
    compare with the drop performance for other algorithms but it does seem like an
    interesting„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Set up to test DTN„ÄÇWell and I did to maybe i'm wrong here„ÄÇbut my understanding
    with the decision transformer as well is this is this critical piece that in the
    training you use the rewards to go right so is it is it not the sense that essentially
    like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: „ÅØ„ÄÇFor each trajectory from the initial state based on the training regime„ÄÇThe
    model has access to whether or not the final result was a success or failureÔºå
    rightÔºü
  prefs: []
  type: TYPE_NORMAL
- en: But that's sort of that's sort of the unique aspect of the training regime for
    decision transformers whereas in CQL„ÄÇMy understanding is that it's based on sort
    of a per transition„ÄÇTraining regime and so each transition is decoupled somewhat
    to what what the final reward was„ÄÇis that correctÔºüYesÔºå although like one difficulty
    which at a first glance you kind of imagined the discipline transform we're having
    is that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That initial token will not change throughout the trajectory„ÄÇYeah because it's
    this past reward scenarioÔºå so except the very last token where it will drop down
    to zero all of a sudden the token remains the same throughout„ÄÇBut maybe that that
    but I think you're right that maybe just even at this start„ÄÇPeding it in a manner
    which looks at the future rewards that you need to get to is perhaps one part
    of the reason why the drop in performance is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Not noticeable„ÄÇYeahÔºå I meanÔºå I guess one sort of abl experiment here would be
    if you change the training regime so that only the last trajectory had the award„ÄÇbut
    I'm trying to think about whether or not that would just be compensated for by
    sort of the attention mechanism anyway„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And vice versa rightÔºå if you embedded that reward information into the CQL training
    procedure as well„ÄÇI'd be curious to see what would happen there„ÄÇYeahÔºå a good experiments„ÄÇOkayÔºå
    so related to this„ÄÇtheres another environment we tested I gave you a brief preview
    of the results in one of the earlier slides„ÄÇso this is called the keycudo environment„ÄÇüòäÔºåAnd
    it has three phases„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so in the first phase you the agent is placed in a room with the key„ÄÇA good
    agent will pick up the key and then they do it replaced in an empty room„ÄÇAnd in
    phase three„ÄÇit will be placed in a room at the door where it will actually use
    the key that it collected in phase one if it did to open the door„ÄÇüòäÔºåSo essentially
    the agent is going to receive a binding reward corresponding to whether it reached
    and opened the door in phase three„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Conditioned on the fact that it„ÄÇDid pick up the key in phase one„ÄÇSo there is
    this national notion of that you want to assign credit to something that happened
    to an event that happened really in the past„ÄÇso it's very challenging„ÄÇAnd sensible
    scenario if you wanted to estimate models for how well they are at long term credit
    assignment„ÄÇAnd here we find that so we tested it for different amounts of project
    trees„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so here the number of project trees basically see how often would you actually
    see this kind„ÄÇOf behavior and the listen transformer is and person behavioral
    to both of these actually baseline„ÄÇDo do much better than other models which struggle
    at this„ÄÇdoes„ÄÇÂì¶„ÄÇThere's a related experiment there which is also of interest„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: so generally a lot of oral algorithms have this notion of an actor and a critic„ÄÇactor
    is basically someone that takes actions condition on the states to think of a
    policy„ÄÇa critic is basically evaluating how good these actions are in terms of
    achieving a long term„ÄÇin terms of the cummulative sum of rewards in the long term„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: This is a good environment because we can see how how well the distant transformer
    would do if it was trained as a critic so here what we did is„ÄÇInstead of having
    the actions as the output targetÔºå what if you substituted that with the rewardsÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So that's very much possibleÔºå we can again use the same causal transformer machinery
    to only look at transitions in the previous time step and try to play the reward„ÄÇAnd
    here we see this interesting pattern where in the three phases that we had in
    that key to door environment„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: We do see the reward probability changing very much in how we expect so basically
    the three scenarios so the agent„ÄÇthe first scenario let's look at blue„ÄÇIn which
    the agent does not pick up the key in pays fund„ÄÇSo the reward probabilityÔºå they
    all start around the same„ÄÇbut as it becomes apparent that the agent is not going
    to pick up the key„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the reward starts going down„ÄÇAnd then it stays very much close to zero throughout
    the episode because there is no way you will have the key to open the door in
    the future phases„ÄÇIf you pick up the key„ÄÇThere are two possibilities„ÄÇÂì¶„ÄÇWhich remain
    which are essentially the same in phase two where you are in an empty room„ÄÇWhich
    is just a distractor to make the episode really long„ÄÇBut at the very end„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: the two possibilities are one that you take the key and you actually reach the
    door„ÄÇwhich is the one we see in orange and brown here where you see that the reward
    probability goes up„ÄÇüòäÔºåAnd is is this other possibility that you actually pick
    up the key but do not reach the door in which case„ÄÇagainÔºå it starts seeing that
    the reward probability that's predicted starts going downÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So the takeaway from this experiment is that mission transformers are not just
    great actors„ÄÇwhich is what we've been seeing so far in the results from the optimized
    policy„ÄÇbut they are also very impressive critics in doing this long-tered and
    assignment where the reward is also very sparse„ÄÇSo just to be correctÔºå are you
    predicting the rewards to go at each time stuff or is it is this like the the
    like the reward at each time stuffÔºü
  prefs: []
  type: TYPE_NORMAL
- en: But you predicting„ÄÇSo this was the rewards to go and I can also check my impression
    was in this particular the experiment it didn't really make a difference„ÄÇwhether
    we were predicting rewards to go or the actual rewards„ÄÇBut I think Lu returns
    to go further this one„ÄÇAlso most curious so how do you get the probability distribution
    on the rewards„ÄÇis it just like you just evaluate a lot of different episodes and
    just part the rewards or are you explicitly predicting some sort of distributionÔºü
  prefs: []
  type: TYPE_NORMAL
- en: OhÔºå so this is a binary reward so you can have a probabilistic outcome„ÄÇÂì™ÁöÑ„ÄÇhave
    a question„ÄÇyeah„ÄÇSo generallyÔºå we will call like someÔºå something predicts the state
    value or state actual value as critic„ÄÇBut in this caseÔºå youÔºå you ask decision
    transformer to only predict the reward„ÄÇSo why should you still call it a critic„ÄÇSo
    I think the analogy here gets a bit clear with returns to go„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: like if you think what returns to goÔºå it's really capturing that essence that
    you want to see the future rewards so I mean it's just going to predict the return
    to go instead of single step reward„ÄÇrightÔºüYeah„ÇÑÔºå yeah„ÄÇOkayÔºå soÔºå so if you're gonna
    predict the returns to go is kind of counterintuitive to me because in phase  one„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: when the agent is still in the key roomÔºå I think you should have a like high
    returns to go if it pick up the K„ÄÇBut in in the plot youÔºå you knowÔºå in the key
    roomÔºå the agents pick up K and the agent that„ÄÇDidn't pick tea has the same kind
    level of returns to go„ÄÇ So that's quite countertuitive to me„ÄÇI think this is reflecting
    on a good propertyÔºå which is that your distribution„ÄÇLike if you interpret„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: it turns to going the right way„ÄÇIn phase oneÔºå you don't know which of these
    three outcomes are really possible and phase one also I'm talking about the very
    big meaning basically„ÄÇslowly you will learn about itÔºå but essentially in phase
    one„ÄÇif you see the returns to go as one or zero for all three possibilities are
    equally likely„ÄÇAnd all three possibilitiesÔºå so if we try to evaluate the predicted„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Reward for these possibilities it shouldn't be the same„ÄÇBecause we really haven't
    done how we don't know what's going to happen in phase three sorry it's my mistake
    because previous that salter green line is the agent which doesn't pick up the„ÄÇbut
    it turns out a blue line is agent which doesn't pick up„ÄÇOkayÔºå so yeahÔºå it's my
    best take okay„ÄÇIt makes sense to me„ÄÇ Thank you„ÄÇAlsomosts not like fully clear
    from the paper„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but did you do experiments where like predicting both the actions and both the
    rewards to go and like does it like„ÄÇüòäÔºåK candidate liberal performance if we didn't
    work together so actually we did some preliminary experiments on that and it didn't
    help us much however I do want to again put in a plug for a paper that came concurruently
    tragically transform summer which tried to predict states actions and rewards
    actually all three of them they were in a modelal based set up where it made sense
    also to„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Try to learn each of the components like the transition dynamicsÔºå the policy„ÄÇand
    maybe even the critic in their setup together„ÄÇWe did not find any significant
    improvements„ÄÇso in favor of simplicity and keeping it models freeÔºå we did not
    try to predict them together„ÄÇGo it„ÄÇOkayÔºå so the summary„ÄÇWe show dis transformersÔºå
    which is„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: A first work in trying to approach R based on sequence modeling„ÄÇThe main advantages
    of previous approaches is it's simple by design„ÄÇthe hope is with further extensionsÔºå
    we will find it to scale much better than existing algorithms„ÄÇIt is stable to
    train because the loss functions we are using have been tested and are traded
    upon a lot by„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Research and perception„ÄÇAnd in the futureÔºå we will also hope that because of
    these similarities with in the architecture and the training with how perception
    based tasks are conducted„ÄÇif it also be easy to integrate them within this loop
    so the states the actions or even the task of interest„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: they could be specified based on„ÄÇPerceptual based cs„ÄÇor you could have a target
    task being specified by natural language instruction and because these models
    can very well play with these kinds of inputs„ÄÇthe hope is that they would be easy
    to integrate within the decision making process„ÄÇAnd emly„ÄÇwe saw strong performance
    in the range of offline R settings„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And especially with performance in scenarios which required us to do long term
    assignment„ÄÇSo there's a lot of future workÔºå this is definitely not the end„ÄÇthis
    is a first work in rethinking how do we build our agents that can scale and generalize„ÄÇÂóØ„ÄÇA
    few things that I picked outÔºå which I feel would be very exciting to extend„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: The first is multimodalityÔºå so really one of our big motivations with going
    after these kinds of models is that we can combine different kinds of inputs both
    online and offline„ÄÇTo really build decision making agents which work like humans„ÄÇwe
    process so many inputs around us in different modalities and we act on them so
    we do take decisions and we want the same to happen in artificial agents and maybe
    decision transformers is one important step in that route„ÄÇMultiitaask so I should
    or I described very limited form of multitasking here„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: which was based on the desired returns to go„ÄÇBut it could be more richer in
    terms of„ÄÇSpecifying a command to be a robot or a desired goal stateÔºå which could
    beÔºå for exampleÔºå even visual„ÄÇSo trying to better explore the different multitask
    capabilities of this model would also be an interesting extension„ÄÇFinallyÔºå multi
    agent„ÄÇAs human beings we never act in isolation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: we are always acting within an environment that involves manyÔºå many more agents„ÄÇthings
    become partially observable in those scenarios„ÄÇwhich plays to the strengths of
    distant transformformers being non-marovviian by design„ÄÇso I think there is great
    possibilities of exploring even multi agentent scenarios where the fact the transformers
    can process very large sequences compared to existing algorithms could again help
    build better models of other agents in your environment and act„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So that yeahÔºå there's some just useful links in case you're interested the project
    website„ÄÇthe paper and the code are all public„ÄÇAnd I'm happy to take any more questions„ÄÇUOkayÔºå
    so I said„ÄÇthanks for the good talk„ÄÇreally appreciate it„ÄÇEveryone had a good time
    here„ÄÇËØ∂„ÄÇSo I think we are like near the class limit so usually I have like a round
    of record five questions for the speaker that just like the students usually know„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: but if someone is hurryÔºå you can you just like ask general questions folks„ÄÇüòä„ÄÇBefore
    we stop the recordingÔºå so if anyone wants to like leave earlier at this time„ÄÇjust
    feel free to ask your questions„ÄÇOtherwiseÔºå I would just like continue on„ÄÇSo what
    do you think is like the future of like transformers in LL do you think they will
    take over like they've already taken over like language and vision„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: do you think for like model based and like model field learning„ÄÇyou think like
    youll see a lot more like transformers pop up in LL literature„ÄÇYes„ÄÇI think we'll
    see a flurry of workÔºå if not already„ÄÇwe have there's so many works using transformers
    at this year's eyeC conference„ÄÇHaving said that„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: I feel that an important piece of the puzzle that needs to be solved is explorationiration„ÄÇIt's
    non trivialÔºå and it will„ÄÇmy guess is that you will have to forego some of the
    advantages that I talked about for transformers in terms of loss functions to
    actually enable explorationir„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So it remains to be seen whether those modified dose functions„ÄÇFor exploration
    actually hurt performance significantly„ÄÇbut as long as we cannot cross that bottleneckÔºå
    I think it isÔºå I'm not„ÄÇI do not want to commit that this is indeed the future
    of our„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Go it also your question Sure I'm not sure understood that point„ÄÇso you're saying
    that in order to apply transformers in RL to do exploration there have to be particular
    loss functions and and they're tricky for some reason„ÄÇCould you explain more like
    what are the modified loss functions and why do they seem trickyÔºü
  prefs: []
  type: TYPE_NORMAL
- en: So a sense in exploration„ÄÇYou have to do the opposite of exploitationÔºå which
    is not a with you„ÄÇAnd there is right now no nothing inbuilt in the transformer
    right now„ÄÇwhich encourages that sort of random behavior„ÄÇWhere you seek out„ÄÇUnfaili
    unfamiliar parts of in state space„ÄÇThat is something which is inbuil into traditional
    algorithms so usually has some sort of entropy bonus to encourage explorationir
    and those are the sort of modifications which one will also need to think about
    if one were to use this transforms for online RL So what happens if somebody I
    mean just naively suppose I have this exact same setup and the way that I sample
    the action is I sample epsilon greedily or I create a boltzman distribution and
    I sample from that I mean what happens it seems„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: That's what OrL does so does what happens„ÄÇSo ArL does a little bit more than
    that„ÄÇit indeed does those kinds of things where it would change the distributionÔºå
    for example„ÄÇthat your postal distribution sample from it„ÄÇBut it's alsoÔºå there
    are these„ÄÇas I said the devil lies in the detailÔºå it's also about how it controls
    that explorationir component with the exploitation„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåAnd it remains to be seen whether that is compatible with distant transformers„ÄÇI
    don't want to jump the gunÔºå but I would say it it's I mean„ÄÇPreliminary evidence
    suggests that it's not directly transferable exact same setup to the online case
    that's what we have found„ÄÇthere has to be some adjustments to be made to make
    but we are still figuring it out„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So the reason why I ask is as you said the devil's in the details and so someone
    naively like me might just come along and try doing what's an RL„ÄÇI want to hear
    more about thisÔºå so you're saying that what works in RL may not work for decision
    transformers„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Can you tell us whyÔºå like what pathologies emergeÔºüWhat are those devilsÔºüHiding
    in the details„ÄÇalso remind you like the sorry we are like also over time so definitely
    okay turn feel free to like follow up but like to me that's really exciting and
    I'm sure that it's tricky„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Yeah I will just ask like two more questions and like you finish sh those so
    one is like Eric did you think like something like this transformer is a way to
    solve a k center problem in oil instead of using some sort of like discon factoror„ÄÇüòäÔºåThis
    isSo that can you repeat the question sorry Yes„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I think usually in parallel we have to allow on some sort of disor to encode
    the rewards to go that but it in transformableer transformer is able to do this
    credit assignment without that„ÄÇSo do you think like something like this book is
    like the way you should do it like we should„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Like instead of having some discountÔºå try to directly predict the rewards„ÄÇSo
    I would go on to say that I feel that discount factor is an important consideration
    in general and it's not incompatible with discipline transformers„ÄÇSo basically
    what would change and I think the code actually gives that functionality where„ÄÇThe
    returns to Google would be computed as the discounted sum of rewards„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: And so it is very much compatibleÔºå so there's scenarios where our context length
    is still is not enough to actually capture the long term behavior we really need
    for credit assignment„ÄÇmaybe traditional tricks that are used could be brought
    in back to solve those kinds of problems„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Got it„ÄÇYeahÔºå also thought like when I was reading the distance transform work
    that the interesting thing is„ÄÇThen you don't have a fixed gamma like a gamma is
    like usually a hypopenimeter like„ÄÇYou don't have a fixed gamma so do you think
    like can you also like learn this thing and could this also be like„ÄÇcan you have
    a different gamma for each gesture or somethingÔºå possiblyÔºü
  prefs: []
  type: TYPE_NORMAL
- en: That would be interesting actuallyÔºå I had not thought of that„ÄÇbut maybe learning
    to predict the discount factor could be another extension of this work„ÄÇüòäÔºåAlso„ÄÇdo
    you think like this like a decision transformer work is like like is it compatible
    with Q learning so if you have something like EQL stuff like that„ÄÇcan you also
    implement those sort of loss functionsÔºüüòäÔºåOn top of a decision transformer„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: So I think maybe I could imagine ways in which you could encode pessimism in
    here as well„ÄÇwhich is key to how CQL worksÔºå and actually most of oftenal algorithms
    work„ÄÇincluding the model based ones„ÄÇOur focus here deliberately was to go out
    is simplicity because we feel that part of the reason why oural literature has
    been so scattered as well if you think about different sub problems that everyone
    tries to solve has been because everyone's tried to„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: üòäÔºåPick up on IDOs which are very well suited for that narrow problem like for
    example„ÄÇyou have whether you're doing offline or you're doing online or you're
    doing imitation„ÄÇyou're doing multitals and all these different variants„ÄÇAnd so
    by designÔºå we did not we were very„ÄÇwe did not want to incorporate exactly the
    components that exist in the current algorithms because then it just where it
    starts looking more like„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: Architecture change that opposed to a more conceptual change engine to thinking
    about RS sequence modeling„ÄÇVery generally„ÄÇcor itÔºå yeah just„ÄÇSo do you think like
    we can use some sort of like like3 learning objectives instead of supervised learningÔºü
  prefs: []
  type: TYPE_NORMAL
- en: ËØ∂„ÄÇIt's possible and maybe like I'm saying that for certain like for online RLÔºå
    it might be necessary„ÄÇüòäÔºåOfÔºå we were happy to see it was not necessary„ÄÇBut it remains
    to be seen more generally for a transformer model or any other model for that
    matter„ÄÇencompassing R more broadly whether that becomes a necessity„ÄÇÂì¶‰πü„ÄÇWellÔºå thanks
    for your time„ÄÇthis was great„ÄÇ
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/041a2ae7aae080f075f80cd7c4c33a68_4.png)'
  prefs: []
  type: TYPE_IMG
