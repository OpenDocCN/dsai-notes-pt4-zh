- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:06:03'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:06:03
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过解释解决方案程序从LLMs中提炼算法推理
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2404.08148](https://ar5iv.labs.arxiv.org/html/2404.08148)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2404.08148](https://ar5iv.labs.arxiv.org/html/2404.08148)
- en: Jierui Li  &  Raymond Mooney
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jierui Li & Raymond Mooney
- en: The University of Texas at Austin
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学奥斯汀分校
- en: '{jierui, mooney}@cs.utexas.edu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{jierui, mooney}@cs.utexas.edu'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Distilling explicit chain-of-thought reasoning paths has emerged as an effective
    method for improving the reasoning abilities of large language models (LLMs) across
    various tasks. However, when tackling complex tasks that pose significant challenges
    for state-of-the-art models, this technique often struggles to produce effective
    chains of thought that lead to correct answers. In this work, we propose a novel
    approach to distilling reasoning abilities from LLMs by leveraging their capacity
    to explain solutions. We apply our method to solving competitive-level programming
    challenges. More specifically, we employ an LLM to generate explanations for a
    set of  pairs, then use  pairs
    to fine-tune a smaller language model, which we refer to as the Reasoner, to learn
    algorithmic reasoning that can generate "how-to-solve" hints for unseen problems.
    Our experiments demonstrate that learning from explanations enables the Reasoner
    to more effectively guide the program implementation by a Coder, resulting in
    higher solve rates than strong chain-of-thought baselines on competitive-level
    programming problems. It also outperforms models that learn directly from  pairs. We curated an additional test set in the CodeContests
    format, which includes 246 more recent problems posted after the models’ knowledge
    cutoff.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 提炼明确的链式思维推理路径已成为提高大型语言模型（LLMs）在各种任务中推理能力的有效方法。然而，在处理对最先进模型构成重大挑战的复杂任务时，这种技术通常难以产生有效的思维链来得到正确答案。在这项工作中，我们提出了一种新颖的方法，通过利用LLMs解释解决方案的能力来提炼推理能力。我们将我们的方法应用于解决竞争级编程挑战。更具体地，我们使用LLM为一组对生成解释，然后使用对来微调一个较小的语言模型，我们称之为推理器，以学习能够生成“如何解决”提示的算法推理，以应对未见过的问题。我们的实验表明，从解释中学习使推理器能够更有效地指导编码者的程序实现，从而在竞争级编程问题上比强大的链式思维基准取得更高的解决率。它也优于那些直接从对中学习的模型。我们在CodeContests格式中策划了一个额外的测试集，其中包含246个模型知识截止日期后发布的最新问题。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent Large Language Models (LLMs) have shown impressive capabilities for various
    reasoning tasks, including multi-hop question answering (Wang et al., [2022](#bib.bib22);
    Lyu et al., [2023](#bib.bib15)), symbolic reasoning (Hua & Zhang, [2022](#bib.bib10)),
    and math word problem-solving (Chen et al., [2022](#bib.bib4); Zhou et al., [2023](#bib.bib29)).
    Chain-of-thought (CoT) prompting (Wei et al., [2022](#bib.bib23)) addresses limitations
    of previous LLMs by instructing them to generate intermediate steps towards the
    final answer, thereby decomposing complex problems step-by-step.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的大型语言模型（LLMs）在多种推理任务中表现出了令人印象深刻的能力，包括多跳问答（Wang et al., [2022](#bib.bib22);
    Lyu et al., [2023](#bib.bib15)）、符号推理（Hua & Zhang, [2022](#bib.bib10)）以及数学文字题解答（Chen
    et al., [2022](#bib.bib4); Zhou et al., [2023](#bib.bib29)）。链式思维（CoT）提示（Wei et
    al., [2022](#bib.bib23)）通过指导模型生成中间步骤来解决之前LLMs的局限性，从而逐步分解复杂问题。
- en: However, challenges remain, particularly in complex reasoning tasks like algorithmic
    programming. For example, the majority of human competitors still outperform advanced
    models like GPT-4 in Codeforces contests (OpenAI, [2023b](#bib.bib18)). Complex
    programming problems have stringent time and space complexity constraints, where
    straightforward implementation methods like (Chen et al., [2021](#bib.bib3); Yin
    et al., [2018](#bib.bib25); Hendrycks et al., [2021](#bib.bib7)), often yield
    time-consuming brute-force solutions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，挑战仍然存在，尤其是在复杂的推理任务如算法编程中。例如，大多数人类竞争者在Codeforces竞赛中仍然表现优于像GPT-4这样的先进模型（OpenAI,
    [2023b](#bib.bib18)）。复杂的编程问题具有严格的时间和空间复杂度约束，像（Chen et al., [2021](#bib.bib3);
    Yin et al., [2018](#bib.bib25); Hendrycks et al., [2021](#bib.bib7)）这样的直接实现方法通常会产生耗时的暴力解决方案。
- en: A number of efforts have been made to tackle this challenging task (Li et al.,
    [2022](#bib.bib14); Zhang et al., [2023](#bib.bib28); Olausson et al., [2023](#bib.bib16);
    Ridnik et al., [2024](#bib.bib20)) by adding extra clustering or verification
    steps to filter or iteratively refine generated programs. While those methods
    focus on flow engineering for code generation, there have been limited attempts
    to explicitly enhance models’ intrinsic reasoning abilities in this context.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为解决这一具有挑战性的任务，已经采取了许多努力（Li 等，[2022](#bib.bib14)；Zhang 等，[2023](#bib.bib28)；Olausson
    等，[2023](#bib.bib16)；Ridnik 等，[2024](#bib.bib20)），通过增加额外的聚类或验证步骤来过滤或迭代优化生成的程序。虽然这些方法专注于代码生成的流程工程，但在这种背景下，明确增强模型的内在推理能力的尝试较为有限。
- en: '![Refer to caption](img/a3c0092d2f4d42b4ab18057e5c289a47.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a3c0092d2f4d42b4ab18057e5c289a47.png)'
- en: 'Figure 1: Comparison between Solve-based and Explain-based chain-of-thoughts
    distilling. Top: Solve-based CoT distilling is likely to generate incorrect or
    inefficient solutions. Bottom: Explain-based CoT distilling can generate high-quality
    reasoning processes by explaining the oracle solution.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：基于解决方案和基于解释的链式思维提取的比较。上图：基于解决方案的 CoT 提取可能会生成不正确或低效的解决方案。下图：基于解释的 CoT 提取通过解释理想解可以生成高质量的推理过程。
- en: Human-written rationales for solving algorithmic reasoning problems, known as
    editorials, are hard to collect as they are often posted on personal blogs or
    as tutorial videos. An alternative is to distill such natural-language-described
    problem-solving strategies from larger models. Distilling explicit chain-of-thoughts
    (CoT) reasoning processes has been shown as an effective method to learn multi-step
    reasoning from larger models (Hsieh et al., [2023](#bib.bib9); Yue et al., [2023](#bib.bib26)).
    Usually, a teacher model is required to solve a set of problems while giving CoT
    reasoning paths at the same time, as illustrated in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"). However, when facing challenging tasks where state-of-the-art
    models struggle to generate effective solutions¹¹1Experiments show that only 12%
    of GPT-4’s generated solutions are correct given 200 problems randomly sampled
    from the CodeContests training set., it becomes infeasible to gather reasoning
    processes at scale.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 解决算法推理问题的人类编写的推理过程，通常称为编辑稿，难以收集，因为这些内容通常发布在个人博客或教程视频中。另一种方法是从更大的模型中提取这种自然语言描述的问题解决策略。已经证明，提取明确的链式思维（CoT）推理过程是一种有效的方法，可以从更大的模型中学习多步骤推理（Hsieh
    等，[2023](#bib.bib9)；Yue 等，[2023](#bib.bib26)）。通常，需要一个教师模型在解决一组问题的同时提供CoT推理路径，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from
    LLMs via Explaining Solution Programs") 所示。然而，当面对那些最先进的模型难以生成有效解决方案的挑战性任务时¹¹实验表明，GPT-4
    生成的解决方案中只有 12% 是正确的，给定从 CodeContests 训练集中随机抽取的 200 个问题。 收集推理过程在大规模上变得不可行。
- en: To tackle this problem, we propose to utilize large language models’ capacities
    in understanding and explaining solutions rather than solving problems. Specifically,
    we leverage a state-of-the-art LLM to read both the problem statement and an oracle
    human-written solution program, and generate an editorial-style chain-of-thought
    on how to solve the problem by explaining the solution. Then, a student LLM learns
    the algorithmic reasoning processes from the explicit chain-of-thought paths.
    We compare our explain-based CoT distilling method with solve-based CoT distilling
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning
    from LLMs via Explaining Solution Programs"). While solve-based CoT distilling
    requires the teacher model to reach the correct and efficient solution to hard
    problems, our explain-based distilling only requires the teacher model to faithfully
    explain the correct solution. Since explaining competitive-level code is a feasible
    task for strong LLMs (Li et al., [2023](#bib.bib13)), explain-based distilling
    can yield CoT reasoning processes with high quality and less noise.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们提议利用大型语言模型在理解和解释解决方案方面的能力，而不是直接解决问题。具体而言，我们利用最先进的 LLM 来读取问题陈述和一个人工编写的解决方案程序，并生成类似编辑风格的
    chain-of-thought，解释如何解决问题。然后，学生 LLM 从明确的 chain-of-thought 路径中学习算法推理过程。我们在图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs") 中将我们基于解释的 CoT 提炼方法与基于解决方案的 CoT 提炼方法进行了比较。虽然基于解决方案的 CoT 提炼需要教师模型能够对难题提供正确和高效的解决方案，但我们的基于解释的提炼只要求教师模型忠实地解释正确的解决方案。由于解释竞争级别代码对于强大的
    LLM 是可行的任务（Li et al., [2023](#bib.bib13)），基于解释的提炼可以产生高质量且噪声较少的 CoT 推理过程。
- en: 'We further proposed a reason-then-implement framework to solve algorithmic
    reasoning problems, utilizing the proposed explain-based CoT distilling. The framework
    consists of 3 major components: 1) an Explainer to annotate explanations for a
    set of ; 2) a Reasoner to learn to generate intermediate
    reasoning processes for a given problem; and 3) a Coder to implement the solution
    for an unseen problem given the output from the Reasoner. The framework and its
    fine-tuning and inference stages are presented in Figure [2](#S1.F2 "Figure 2
    ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution
    Programs").'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步提出了一个先推理后实现的框架来解决算法推理问题，利用了所提出的基于解释的 CoT 提炼。该框架包括三个主要组成部分：1）一个解释器，用于为一组注释解释；2）一个推理器，用于学习生成给定问题的中间推理过程；3）一个编码器，根据推理器的输出为未见问题实现解决方案。该框架及其微调和推理阶段在图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from
    LLMs via Explaining Solution Programs") 中展示。
- en: The resulting usage of this Reasoner fine-tuned with explanations provides superior
    performance over direct code generation techniques and strong zero-shot chain-of-thought
    prompting baselines. Experiments on open and closed models also show that compared
    to direct learning on public-available  pairs, models
    learned on explicit natural language reasoning processes can generalize better
    to unseen problems.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 经过解释微调后的推理器在性能上优于直接代码生成技术，并且在零-shot chain-of-thought 提示基线中表现出色。开放和封闭模型的实验还表明，与直接学习公共可用的对比，通过明确的自然语言推理过程学习的模型对未见问题具有更好的泛化能力。
- en: '![Refer to caption](img/9f96ca19ce157a12bbc3e2a010c1a687.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9f96ca19ce157a12bbc3e2a010c1a687.png)'
- en: 'Figure 2: The framework of our approach. We use Explainer LLM to generate explanations
    given  pairs; then train Reasoner LLM to generate explanations
    given problem statements. During inference time, given the problem, the Reasoner
    can generate a reasoning process in the same format as solution explanations,
    which could be provided to the Coder as a hint to solve the problem better.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们方法的框架。我们使用解释器 LLM 生成给定对的解释；然后训练推理器 LLM 生成给定问题陈述的解释。在推理时，给定问题，推理器可以生成与解决方案解释相同格式的推理过程，这可以作为提示提供给编码器，以更好地解决问题。
- en: Decomposing algorithmic problem-solving into separate Coder and Reasoner modules
    surpasses the effectiveness of models fine-tuned directly on code. The success
    of our approach is rooted in the nuanced semantic richn meaning of natural language.
    The proposed method relies solely on providing better problem reasoning to instruct
    the implementation, which can be combined with different Coder and pipelines such
    as self-debugging (Chen et al., [2023](#bib.bib5)).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 将算法问题解决分解为独立的 Coder 和 Reasoner 模块，比直接对代码进行微调的模型效果更好。我们方法的成功根源于自然语言的细致语义丰富性。所提方法完全依赖于提供更好的问题推理来指导实现，可以与不同的
    Coder 和管道结合，如自我调试 (Chen et al., [2023](#bib.bib5))。
- en: 'To summarize, our work makes the following contributions:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们的工作做出了以下贡献：
- en: '1.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'We introduce an alternative, adaptable framework to distill complex reasoning
    processes from LLMs: instead of having LLMs solve problems, it leverages LLMs
    to explain solutions.'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一种替代的、可适应的框架，以从 LLM 中提炼复杂的推理过程：与其让 LLM 解决问题，不如利用 LLM 解释解决方案。
- en: '2.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Our proposed distilling method is data-efficient: by fine-tuning with 8248
    data points, 8M tokens in total, consistent performance gains in solve rates are
    observed on GPT-3.5 and Deepseek coder 7b.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出的提炼方法数据高效：通过用 8248 个数据点进行微调，总共 8M 令牌，在 GPT-3.5 和 Deepseek coder 7b 上观察到一致的解决率性能提升。
- en: '3.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: For fair evaluation, we propose a new test set extracted from Codeforces. We
    advocate to take code efficiency into consideration in addition to code correctness.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了公平评价，我们提出了一个从 Codeforces 提取的新测试集。我们主张在考虑代码正确性的同时，也要考虑代码效率。
- en: 2 Related work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 Solving algorithmic-reasoning programming problems
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 解决算法推理编程问题
- en: Early endeavors in applying deep learning to tackle algorithmic reasoning in
    programming problems, such as those by Balog et al. ([2017](#bib.bib1)), primarily
    utilized traditional approaches like SMT solvers and heuristic search. These methods
    were adept at generating short programs for relatively simple problems. Expanding
    on this, Polosukhin & Skidanov ([2018](#bib.bib19)) curated a dataset of human-written
    problem statements and solutions for Codeforces challenges. They introduced baseline
    sequence models capable of solving a modest portion of this dataset. The advent
    of Transformer-based models, such as Alphacode (Li et al., [2022](#bib.bib14)),
    dramatically improved performance on these challenges. Subsequent advancements
    by Zelikman et al. ([2023](#bib.bib27)) and Olausson et al. ([2023](#bib.bib16)),
    as well as Zhang et al. ([2023](#bib.bib28)), further refined code generation
    techniques. They integrated additional layers for clustering or verification,
    enhancing the models’ ability to filter and iteratively refine the generated programs,
    thereby improving both accuracy and efficiency.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 早期将深度学习应用于解决编程问题中的算法推理的尝试，如 Balog 等人 ([2017](#bib.bib1))，主要利用了传统的方法，如 SMT 求解器和启发式搜索。这些方法擅长为相对简单的问题生成短程序。在此基础上，Polosukhin
    & Skidanov ([2018](#bib.bib19)) 编制了一个人类编写的 Codeforces 挑战问题陈述和解决方案的数据集。他们引入了基线序列模型，能够解决这个数据集的一小部分。基于
    Transformer 的模型，如 Alphacode (Li et al., [2022](#bib.bib14))，在这些挑战中显著提高了性能。随后，Zelikman
    et al. ([2023](#bib.bib27)) 和 Olausson et al. ([2023](#bib.bib16))，以及 Zhang et
    al. ([2023](#bib.bib28)) 的进一步改进精细化了代码生成技术。他们集成了额外的层用于聚类或验证，提高了模型过滤和迭代精炼生成程序的能力，从而提高了准确性和效率。
- en: 2.2 Distilling reasoning abilities
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 提炼推理能力
- en: Knowledge distillation, particularly from large models to smaller and more efficient
    counterparts, has been extensively studied (Bucila et al., [2006](#bib.bib2);
    Hinton et al., [2015](#bib.bib8)). The advent of LLMs has further intensified
    research in this area, with a focus on distilling knowledge from LLMs into smaller
    models. This has often been coupled with the curation of LLM datasets (Taori et al.,
    [2023](#bib.bib21); Hsieh et al., [2023](#bib.bib9)). Specifically, Hsieh et al.
    ([2023](#bib.bib9)) employed CoT prompting to extract rationales from LLMs for
    training smaller models in a multi-task learning setting. Similarly, Yang et al.
    ([2023](#bib.bib24)) and Zhu et al. ([2023](#bib.bib30)) distilled reasoning abilities
    from LLMs using CoT or program-of-thought processes. These approaches typically
    treat data synthesis as analogous to the target task, leveraging advanced models
    for higher-quality output generation. Our method differs as we replace the problem-solving
    CoT generation with an oracle-solution-explaining task. It transforms the nature
    of the task to facilitate the generation of high-quality outputs in a more manageable
    framework.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏，特别是从大型模型到较小且更高效的模型，已经得到了广泛研究（Bucila 等，[2006](#bib.bib2)；Hinton 等，[2015](#bib.bib8)）。LLMs
    的出现进一步加剧了该领域的研究，重点是将LLMs中的知识蒸馏到较小的模型中。这通常与LLM数据集的策划相结合（Taori 等，[2023](#bib.bib21)；Hsieh
    等，[2023](#bib.bib9)）。具体而言，Hsieh 等（[2023](#bib.bib9)）使用 CoT 提示从 LLMs 中提取推理过程，以便在多任务学习设置中训练较小的模型。类似地，Yang
    等（[2023](#bib.bib24)）和 Zhu 等（[2023](#bib.bib30)）使用 CoT 或程序思维过程从 LLMs 中提取推理能力。这些方法通常将数据合成视为类似于目标任务，利用先进模型生成更高质量的输出。我们的方法不同，因为我们将问题解决
    CoT 生成替换为一个神谕解决方案解释任务。这改变了任务的性质，以便在更可管理的框架中生成高质量的输出。
- en: 3 Method
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'Our method employs a two-step framework: In the learning stage, an Explainer
    generates a set of chain-of-thought reasoning processes by explaining solutions,
    and a Reasoner is fine-tuned on this data to generate a reasoning process given
    only a problem description. In the inference stage, the Reasoner generates the
    reasoning process for an unseen problem, which, along with the problem, is passed
    to a 0-shot Coder to implement. We include sample prompts in appendix [A](#A1
    "Appendix A Appendix ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs").'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法采用了一个两步框架：在学习阶段，一个解释者通过解释解决方案生成一系列思维链推理过程，理由生成器在这些数据上进行微调，以便仅凭问题描述生成推理过程。在推理阶段，理由生成器为一个未见过的问题生成推理过程，该过程和问题一起传递给一个0-shot编码器以实现。我们在附录[A](#A1
    "附录 A 附录 ‣ 从LLMs中提取算法推理")中包含了示例提示。
- en: 3.1 Problem Formulation
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题表述
- en: 'We formalize our task with a problem set consisting of $n$ is a text sequence
    that clearly describes the following aspects:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将任务形式化为一个问题集，其中 $n$ 是一个文本序列，明确描述以下方面：
- en: •
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Problem statement: a natural language description of the problem, as shown
    in the upper left box in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Distilling
    Algorithmic Reasoning from LLMs via Explaining Solution Programs").'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 问题陈述：问题的自然语言描述，如图[1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 从LLMs中提取算法推理")左上角框中所示。
- en: •
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Input/Output: input/output formats and constraints for the submitted program
    $s$.'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输入/输出：提交程序 $s$ 的输入/输出格式和约束条件。
- en: •
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Example: An example of a correct input/output pair.'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：正确输入/输出对的示例。
- en: •
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Note (Optional): Explanation of the Example input/output.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注（可选）：示例输入/输出的说明。
- en: Each $p_{i}$.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 $p_{i}$。
- en: '3.2 Explainer: Extracting reasoning processes through explaining solution programs'
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 解释者：通过解释解决方案程序提取推理过程
- en: 'The Explainer is tasked with explaining a solution program. It serves the role
    of Teacher in Solve-based CoT distilling, as described in Figure [1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"). Given a pair, $$ in natural language.
    This is inspired by how human competitors learn problem-solving skills from past
    problems: they learn by reading editorials, step-by-step guidelines on approaching
    and solving the problems. While human-written editorials are hard to collect or
    annotate at scale, we ask an LLM to automatically generate them. We design an
    editorial-style chain-of-thought reasoning template and ask the Explainer to follow
    it to explain solutions. Specifically, an editorial for an algorithmic reasoning
    problem refers to a comprehensive explanation or walk-through on how to solve
    a problem, which includes problem analysis, strategy development, solution explanation,
    time/space complexity analysis, etc.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 解释者负责解释解决方案程序。它在基于 Solve 的 CoT 提炼中扮演教师的角色，如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs")
    所述。给定一个对，$$ 用自然语言描述。这一做法受到了人类竞争者从过去的问题中学习解决问题技能的启发：他们通过阅读社论、逐步指导来学习如何处理和解决问题。虽然人类编写的社论难以大规模收集或注释，但我们要求
    LLM 自动生成这些社论。我们设计了一个社论风格的链式思维推理模板，并要求解释者按照它来解释解决方案。具体而言，算法推理问题的社论指的是对如何解决问题的全面解释或详细讲解，包括问题分析、策略发展、解决方案解释、时间/空间复杂度分析等。
- en: 'We leverage the capabilities of LLMs to explain solution code to generate automated
    explanations for problem-solution pairs 1) is needed in our main experiments.
    The context window is set to 4096 and we truncate single examples with more than
    4096 tokens.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 GPT-4-0613（OpenAI，[2023a](#bib.bib17); [b](#bib.bib18)）作为解释器，因为这是我们能够访问的最强模型。我们还选择了最强的具有微调访问权限的模型作为推理器和编码器。对于封闭模型，我们选择
    GPT-3.5-turbo-1106（以下简称 GPT-3.5）；对于开放模型，我们选择 Deepseek Coder 7B（以下简称 Deepseek 7b）（Guo
    et al., [2024](#bib.bib6)）。²²2[https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1.5](https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1.5)
    采样多个（>1）时需要的温度 $t$ 是我们主要实验中的必要条件。上下文窗口设置为 4096，并且我们截断包含超过 4096 个标记的单个示例。
- en: Data
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据
- en: As suggested by Huang et al. ([2023](#bib.bib11)), to test the capacity of closed
    models like GPT, using recently-released data can reflect the model’s performance
    more faithfully. To ensure novel test data given that GPT-3.5 was trained on data
    including problems and solutions from Codeforces, we extracted 246 recent problems
    from Codeforces as our main test set, CF Prob. It contains 246 real online contest
    problems, the earliest of which dates to Aug. 2023, guaranteeing they were posted
    after the knowledge cutoff for GPT-3.5 (Jan. 2022). We also experimented with
    the 165 problems from the CodeContests benchmark (Li et al., [2022](#bib.bib14))
    test set, whose earliest problem was published in Oct. 2021\. Table [1](#S4.T1
    "Table 1 ‣ Data ‣ 4.1 Experimental Setup ‣ 4 Experiments ‣ Distilling Algorithmic
    Reasoning from LLMs via Explaining Solution Programs") gives statistics on the
    difficulty ratings of both sets.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 Huang et al. ([2023](#bib.bib11)) 所建议的，为了测试像 GPT 这样的封闭模型的能力，使用最近发布的数据可以更真实地反映模型的表现。为了确保新的测试数据，因为
    GPT-3.5 在包括 Codeforces 上的问题和解决方案的数据上进行过训练，我们从 Codeforces 提取了 246 个最近问题作为我们的主要测试集
    CF Prob。它包含 246 个真实的在线竞赛问题，其中最早的问题日期为 2023 年 8 月，确保它们是在 GPT-3.5 的知识截止日期（2022 年
    1 月）之后发布的。我们还实验了来自 CodeContests 基准（Li et al., [2022](#bib.bib14)）测试集中的 165 个问题，其最早的问题于
    2021 年 10 月发布。表 [1](#S4.T1 "Table 1 ‣ Data ‣ 4.1 Experimental Setup ‣ 4 Experiments
    ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs")
    给出了这两个数据集的难度评级统计。
- en: '|  | Difficulty Ratings |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 难度评级 |'
- en: '| --- | --- |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Dataset | total | [800, 1000] | (1000, 1500] | (1500, 2000] | (2000, 3600]
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 总计 | [800, 1000] | (1000, 1500] | (1500, 2000] | (2000, 3600] |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| CodeContests | $165$ |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| CodeContests | $165$ |'
- en: '| CF Prob | $246$ |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| CF Prob | $246$ |'
- en: 'Table 1: Difficulty statistics (higher ratings = more difficult) for CodeContests
    and our proposed CF Prob. Both are sourced from Codeforces.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：CodeContests 和我们提出的 CF Prob. 的难度统计（较高评分 = 更难）。两者都来自 Codeforces。
- en: Metric
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标
- en: We adapt pass@k (Chen et al., [2021](#bib.bib3)) as our evaluation metric for
    solve rates. For each problem $p_{i}$ that passes all hidden tests. We first filter
    in programs by executing and examining their output on the public test cases before
    submitting them. Measuring solve rates using the online judge provides a fairer
    comparison between models and human participants as it also requires efficiency
    in solutions, rejecting brute-force solutions when an efficient algorithm exists
    for a problem.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用 pass@k (Chen et al., [2021](#bib.bib3)) 作为我们解决率的评估指标。对于每个通过所有隐藏测试的问题 $p_{i}$。我们首先通过执行和检查其在公共测试用例上的输出来筛选程序，然后再提交它们。使用在线评测系统来测量解决率为模型和人工参与者提供了更公平的比较，因为它还要求解决方案的效率，拒绝在问题存在高效算法时的蛮力解决方案。
- en: 'We have models generate $10$:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '我们有模型生成 $10$:'
- en: '|  | $1$2 |  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: where $g_{i}$th problem.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $g_{i}$ 第个问题。
- en: 4.2 Main Results
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: Zero-shot Baselines
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 零-shot 基准
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Direct Prompting Given the problem statement $p_{i}$, directly prompt the model
    to generate solution programs.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 直接提示 给定问题陈述 $p_{i}$，直接提示模型生成解决方案程序。
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Naive Chain-of-thought Adapt the well-known “Letś think step-by-step” CoT prompting
    proposed in (Kojima et al., [2022](#bib.bib12)).
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 幼稚的链式思维 采用 (Kojima et al., [2022](#bib.bib12)) 提出的著名“让我们一步一步思考” CoT 提示。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Editorial Chain-of-thought Instruct the model to analyze the problem following
    an editorial chain-of-thought style giving aspects mentioned in Section [3.2](#S3.SS2
    "3.2 Explainer: Extracting reasoning processes through explaining solution programs
    ‣ 3 Method ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution
    Programs").'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 编辑链式思维 指示模型按照编辑链式思维风格分析问题，给出第 [3.2](#S3.SS2 "3.2 说明：通过解释解决方案程序提取推理过程 ‣ 3 方法
    ‣ 从 LLM 中提炼算法推理") 节中提到的方面。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Zero-Reasoner&Zero-Coder Use a 0-shot Reasoner to generate natural language
    reasoning processes as hints for the 0-shot Coder.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Zero-Reasoner&Zero-Coder 使用 0-shot Reasoner 生成自然语言推理过程作为 0-shot Coder 的提示。
- en: Fine-tuning Methods
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精调方法
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fine-tuned Coder Fine-tune the Coder to generate code given the problem, using
    the same 8248  pairs from CodeContests (Li et al.,
    [2022](#bib.bib14)).
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精调编码器 精调编码器以生成代码，基于 CodeContests (Li et al., [2022](#bib.bib14)) 中相同的 8248  对。
- en: •
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fine-tuned Reasoner &Zero-Coder Use the Reasoner fine-tuned on solution explanations
    to generate natural-language reasoning hints for the 0-shot Coder. In addition
    to using the full reasoning processes. We also separately tested the effect of
    each editorial aspect (e.g., common pitfalls) and selected the highest performing
    aspect as the best aspect.
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精调推理器&Zero-Coder 使用在解决方案解释上精调的推理器生成自然语言推理提示给 0-shot Coder。除了使用完整的推理过程，我们还单独测试了每个编辑方面的效果（例如，常见陷阱），并选择表现最好的方面作为最佳方面。
- en: The results are presented in Table [2](#S4.T2 "Table 2 ‣ Fine-tuning Methods
    ‣ 4.2 Main Results ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs
    via Explaining Solution Programs"). With our fine-tuned Reasoner, the Coder achieves
    the best solve@10 and solve@5 rates across open/closed models and two datasets,
    supporting the effectiveness of our method. Compared to using the original model
    as the Reasoner (0-shot Reasoner), the best aspect (Step-by-Step) from the reasoning
    process alone can boost solve@10 from 3.3% to 6.1%, suggesting that our fine-tuning
    method does distill some algorithmic reasoning ability into the student model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 结果见表 [2](#S4.T2 "表 2 ‣ 精调方法 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 从 LLM 中提炼算法推理")。通过我们的精调推理器，编码器在开放/封闭模型和两个数据集上都达到了最佳的
    solve@10 和 solve@5 率，支持我们方法的有效性。与使用原始模型作为推理器（0-shot Reasoner）相比，仅凭推理过程中的最佳方面（逐步思考）就可以将
    solve@10 从 3.3% 提升到 6.1%，这表明我们的精调方法确实将一些算法推理能力提炼到了学生模型中。
- en: Note that solely fine-tuning pairs actually hurts
    performance. One reason might be that solutions to Codeforces problems are often
    written under a time constraint with poor readability, making it difficult for
    the model to generalize given the limited amount of fine-tuning data. Since instruction-tuned
    models would generate intermediate thought processes by default, we experimented
    to explicitly forbid the model from any reasoning/analysis preceding implementation.
    Compared to the direct prompt, the solve@10 rate on CF Prob w/GPT-3.5 drops from
    3.3% to 2.0%. Even with this setting, the generated code often contains comments
    and meaningful variable/function naming that are less in human competitors’ code.
    This observation further supports our claim that meaningful, semantic-rich natural
    language can help the model generalize to unseen problems.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，单纯地微调  对性能实际上是有害的。一个原因可能是，Codeforces 问题的解决方案通常是在时间限制下编写的，易读性差，使得模型难以在有限的微调数据下进行泛化。由于指令微调的模型默认会生成中间思考过程，我们尝试明确禁止模型在实现之前进行任何推理/分析。与直接提示相比，在
    CF Prob 上使用 GPT-3.5 的 solve@10 率从 3.3% 降低到 2.0%。即便在这种设置下，生成的代码通常包含评论和有意义的变量/函数命名，这在人工竞争者的代码中较少见。这一观察进一步支持了我们的观点，即有意义的、富含语义的自然语言可以帮助模型对未见过的问题进行泛化。
- en: Unless unspecified otherwise, we use CF Prob and GPT-3.5 for experiments onwards.
    ³³3We use Deepseek Coder for tuning experiments and GPT-3.5 for all other experiments
    mainly considering their accessibility and efficiency.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除非另有说明，我们在后续实验中使用 CF Prob 和 GPT-3.5。我们使用 Deepseek 编码器进行微调实验，并使用 GPT-3.5 进行所有其他实验，主要考虑到它们的可访问性和效率。
- en: '| Dataset | CF-Prob | CodeContests |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | CF-Prob | CodeContests |'
- en: '| Coder/Reasoner Model | GPT-3.5 | Deepseek 7b | GPT-3.5 | Deepseek 7b |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 编码器/推理器模型 | GPT-3.5 | Deepseek 7b | GPT-3.5 | Deepseek 7b |'
- en: '| Zero-shot Methods |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 零-shot 方法 |'
- en: '|  | solve@1 | solve@5 | solve@10 | solve@1 | solve@5 | solve@10 | solve@1
    | solve@5 | solve@10 | solve@1 | solve@5 | solve@10 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | solve@1 | solve@5 | solve@10 | solve@1 | solve@5 | solve@10 | solve@1
    | solve@5 | solve@10 | solve@1 | solve@5 | solve@10 |'
- en: '| Direct Prompt | $1.1$ |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 直接提示 | $1.1$ |'
- en: '| Naive CoT | $1.2$ |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 朴素 CoT | $1.2$ |'
- en: '| Editorial CoT | $1.1$ |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 编辑 CoT | $1.1$ |'
- en: '| 0-Reasoner Coder | $1.2$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 0-推理器编码器 | $1.2$ |'
- en: '| Fine-tuning Methods |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 微调方法 |'
- en: '| Fine-tune Coder | $0.5$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 微调编码器 | $0.5$ |'
- en: '| Fted Reasoner w/Full | $1.1$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| Fted 推理器（全） | $1.1$ |'
- en: '| Fted Reasoner w/Best | $1.1$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| Fted 推理器（最佳） | $1.1$ |'
- en: '|       Relative Increment |  | $+37\%$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|       相对增量 |  | $+37\%$ |'
- en: 'Table 2: Performance of baselines and our methods. Fine-tuned Reasoner (Full)
    refers to using all aspects from the reasoning process, while Fine-tuned Reasoner
    (Best) only uses the best aspect. Relative Increment(by percentage) is what’s
    over the best baseline. Solve@k: Solve rates(Percentage) when sampling k.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：基线和我们的方法的性能。微调推理器（全）指的是使用推理过程中的所有方面，而微调推理器（最佳）仅使用最佳方面。相对增量（以百分比表示）是超出最佳基线的部分。Solve@k：在抽样k时的解决率（百分比）。
- en: 4.3 Effect of Tuning and Sampling
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 调整和抽样的影响
- en: Fine-tuning Strategy
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 微调策略
- en: 'As we mentioned in Sec. [3.3](#S3.SS3 "3.3 Reasoner: Fine-tuned to generate
    reasoning processes for problems ‣ 3 Method ‣ Distilling Algorithmic Reasoning
    from LLMs via Explaining Solution Programs"), harder problems are more challenging
    and frequently beyond the models’ capacity. Therefore, the silver explanation
    given by the Explainer might not be as accurate, adding noise to the training
    of the Reasoner. So it might be more effective to focus fine-tuning on the simpler
    subset. We compare three strategies to fine-tune deepseek-coder 6.7b: 1) Uniformly
    fine-tune on the entire dataset; 2) Fine-tune with simpler problems weighted more,
    and 3) Uniformly fine-tune on the simple subset only. The solve@10 rates as depicted
    in Table [3](#S4.T3 "Table 3 ‣ Program Sampling Strategy ‣ 4.3 Effect of Tuning
    and Sampling ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs via
    Explaining Solution Programs")b, indicate that the strategy of just up-weighting
    simpler problems yields superior results. This suggests that in scenarios where
    the training data is noisy and limited, including noisier data while assigning
    greater weight to cleaner data can serve as a viable approach to enhancing generalization.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第[3.3节](#S3.SS3 "3.3 推理器：微调以生成问题的推理过程 ‣ 3 方法 ‣ 通过解释解决方案程序从LLMs中提取算法推理")中提到的，较难的问题更加具有挑战性，且通常超出了模型的能力范围。因此，解释器给出的银奖解释可能不够准确，从而给推理器的训练带来了噪声。因此，专注于更简单的子集进行微调可能更有效。我们比较了三种策略来微调deepseek-coder
    6.7b：1）在整个数据集上均匀微调；2）对更简单的问题加权更多进行微调；3）仅在简单子集上均匀微调。表[3](#S4.T3 "表 3 ‣ 程序采样策略 ‣
    4.3 调优和采样效果 ‣ 4 实验 ‣ 通过解释解决方案程序从LLMs中提取算法推理")b中显示，单纯加权简单问题的策略取得了更好的结果。这表明，在训练数据噪声大且有限的情况下，将噪声数据包括在内，同时对更干净的数据赋予更大的权重，可以作为提高泛化能力的有效方法。
- en: Program Sampling Strategy
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 程序采样策略
- en: Since our method partitions problem-solving between the Reasoner and Coder,
    it can adopt various sampling strategies. Specifically, the Coder can either a)
    implement different programs given one result from the Reasoner or b) implement
    one program for each of the different results from the Reasoner. Formally, to
    sample $k$ in Table [3](#S4.T3 "Table 3 ‣ Program Sampling Strategy ‣ 4.3 Effect
    of Tuning and Sampling ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from
    LLMs via Explaining Solution Programs"). For cases where 1 deterministic sequence
    is needed (T=1 or M=1), we do not sample; otherwise, temperature is set to 0.5.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的方法将问题解决过程分为推理器和编码器，因此可以采用各种采样策略。具体而言，编码器可以 a) 在从推理器获得一个结果的情况下实现不同的程序，或
    b) 为从推理器获得的每个不同结果实现一个程序。形式上，样本 $k$ 如表[3](#S4.T3 "表 3 ‣ 程序采样策略 ‣ 4.3 调优和采样效果 ‣
    4 实验 ‣ 通过解释解决方案程序从LLMs中提取算法推理")所示。在需要1个确定性序列的情况下（T=1或M=1），我们不进行采样；否则，温度设定为0.5。
- en: '| Data Split | Data Size | Training Steps | Data Weight | solve@10 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 数据划分 | 数据大小 | 训练步骤 | 数据权重 | solve@10 |'
- en: '| Simple+Hard | 8248 | 1547 | Uniform | 3.3 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 简单+困难 | 8248 | 1547 | 均匀 | 3.3 |'
- en: '| Simple+Hard | 8248 | 1617 | 2xSimple:1xHard | 3.7 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 简单+困难 | 8248 | 1617 | 2倍简单:1倍困难 | 3.7 |'
- en: '| Simple | 4691 | 1172 | Uniform | 3.3 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 简单 | 4691 | 1172 | 均匀 | 3.3 |'
- en: '| (a)fine-tuning strategy comparison w/Deepseek Coder |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| (a) 微调策略与Deepseek Coder的比较 |'
- en: '| Sample Reasoner | Sample Coder | solve@10 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 示例推理器 | 示例编码器 | solve@10 |'
- en: '| M=1 | T=10 | 2.8 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| M=1 | T=10 | 2.8 |'
- en: '| M=2 | T=5 | 3.7 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| M=2 | T=5 | 3.7 |'
- en: '| M=5 | T=2 | 4.1 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| M=5 | T=2 | 4.1 |'
- en: '| M=10 | T=1 | 4.9 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| M=10 | T=1 | 4.9 |'
- en: '| (b)sampling strategy comparison w/GPT-3.5 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| (b) 采样策略与GPT-3.5的比较 |'
- en: 'Table 3: (a)Fine-tune settings testing whether to up-weight simpler problems.
    Training steps are kept similar unless data points are significantly fewer. (b)Sampling
    more from Reasoner vs. sampling more from Coder. Both solve@10(percent) are Fine-tuned
    Reasoner w/Full on CF Prob.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：（a）测试是否对简单问题进行加权的微调设置。训练步骤保持相似，除非数据点明显更少。（b）从推理器采样更多与从编码器采样更多。两者的solve@10（百分比）都是对CF问题的全数据微调推理器。
- en: We found that for fixed $k=10=M\times T$ for the experiments in Table [2](#S4.T2
    "Table 2 ‣ Fine-tuning Methods ‣ 4.2 Main Results ‣ 4 Experiments ‣ Distilling
    Algorithmic Reasoning from LLMs via Explaining Solution Programs")(a) and the
    following section.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现，对于表[2](#S4.T2 "表 2 ‣ 微调方法 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 通过解释解决方案程序从LLMs中提取算法推理")中实验的固定
    $k=10=M\times T$ 和以下部分。
- en: 4.4 Ablation Study
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 消融研究
- en: Since providing the full reasoning process with all aspects can distract the
    model, we found that providing a single aspect can yield a higher overall solve
    rate. We therefore studied the effects of using different aspects of the explanations
    generated by the Reasoner.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提供所有方面的完整推理过程可能会分散模型的注意力，我们发现提供单一方面可以获得更高的总体解决率。因此，我们研究了使用Reasoner生成的不同解释方面的效果。
- en: '|  | Solve@1 | Solve@5 | Solve@10 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | Solve@1 | Solve@5 | Solve@10 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| *0-shot Reasoner | $1.2$ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| *0-shot Reasoner | $1.2$ |'
- en: '| w/ Full Reasoning process | $1.1$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| w/ 完整推理过程 | $1.1$ |'
- en: '| w/ Conceptual Evolution | 1.5 | 4.1 | $5.3$ |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| w/ 概念演变 | 1.5 | 4.1 | $5.3$ |'
- en: '| w/ Key to Solution | $1.0$ |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| w/ 解决方案关键 | $1.0$ |'
- en: '| w/ Solution Description | $1.1$ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| w/ 解决方案描述 | $1.1$ |'
- en: '| w/ Common Pitfalls | $1.2$ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| w/ 常见陷阱 | $1.2$ |'
- en: '| w/ Step-by-Step Solution Explanation | $1.1$ | 6.1 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| w/ 逐步解决方案解释 | $1.1$ | 6.1 |'
- en: 'Table 4: Ablation Study on the different aspects described in Section [3.2](#S3.SS2
    "3.2 Explainer: Extracting reasoning processes through explaining solution programs
    ‣ 3 Method ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution
    Programs"). Rather than fine-tuning another model, we give only each individual
    aspect of the Reasoner’s output to the Coder. Solve@k: Solve rates(percentage)
    when sampling k.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：关于第[3.2](#S3.SS2 "3.2 解释器：通过解释解决方案程序提取推理过程 ‣ 3 方法 ‣ 通过解释解决方案程序从LLMs提炼算法推理")节中描述的不同方面的消融研究。我们没有微调另一个模型，而是将Reasoner的每个独立方面的输出提供给Coder。Solve@k：在采样k时的解决率（百分比）。
- en: The results in Table [4](#S4.T4 "Table 4 ‣ 4.4 Ablation Study ‣ 4 Experiments
    ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs")
    show that each aspect alone can achieve better solve rates against the un-fine-tuned
    0-shot Reasoner, proving the effectiveness of our explanation fine-tuning. Among
    all the different aspects, Conceptual Evolution achieves the best solve@1 and
    solve@5 while Step-by-Step Solution Explanation achieves the best solve@10.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表[4](#S4.T4 "表4 ‣ 4.4 消融研究 ‣ 4 实验 ‣ 通过解释解决方案程序从LLMs提炼算法推理")中的结果显示，单独考虑每个方面的解决率都优于未经微调的0-shot
    Reasoner，证明了我们解释微调的有效性。在所有不同方面中，概念演变实现了最佳的solve@1和solve@5，而逐步解决方案解释实现了最佳的solve@10。
- en: 'Since Conceptual Evolution provides a solving strategy, it indicates how one
    should think about the problem and where to start, while also touching on the
    high-level idea behind the solution, as well as the choice of algorithm. Therefore,
    by giving a good starting point, it allows the Coder to follow up the idea and
    implement a correct solution. On the other hand, Step-by-Step Solution focuses
    more on implementation details, giving specific instructions on what and how to
    implement, making it easy for the Coder to follow. However, it is not as robust
    as higher-level aspects: If the Reasoner makes a mistake in the details, the Coder
    may not spot the bug and produce an incorrect implementation.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于概念演变提供了解决策略，它表明了应如何思考问题和从哪里开始，同时还涉及了解决方案背后的高层次思想，以及算法的选择。因此，通过提供一个良好的起点，它使Coder能够跟进这个想法并实施正确的解决方案。另一方面，逐步解决方案更关注实现细节，给出了具体的实施步骤，使Coder易于跟随。然而，它不如高层次的方面稳健：如果Reasoner在细节上出错，Coder可能无法发现错误并产生不正确的实现。
- en: 4.5 Program Submission Status
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 程序提交状态
- en: In our experiments, we find that, compared to our method, programs sampled with
    zero-shot methods pass more public tests, but yield significantly lower solve
    rates. This indicates that the pre-trained large language models like GPT-3.5
    and Deepseek Coder tend to give brute-force solution programs initially. We further
    study this phenomenon by analyzing the statistics of the statuses of submitted
    programs.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们发现，与我们的方法相比，使用零样本方法采样的程序通过更多的公共测试，但解决率显著较低。这表明像GPT-3.5和Deepseek Coder这样的预训练大型语言模型最初倾向于给出暴力破解的解决方案。我们通过分析提交程序状态的统计数据进一步研究了这一现象。
- en: In competitive-level programming, online judge systems give TLE (Time Limit
    Exceeded) to submissions that do not run efficiently enough to be considered correct.
    As illustrated in Figure [5](#footnote5 "footnote 5 ‣ Figure 4 ‣ 4.5 Program Submission
    Status ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"), the baseline has $42\%$ of the programs that pass the public
    tests are accepted solutions, indicating that the Reasoner learns to use efficient
    algorithms rather than implementing brute-force solutions.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞争级别的编程中，在线评测系统对运行效率不足以被视为正确的提交给予TLE（时间限制超出）。如图[5](#footnote5 "脚注 5 ‣ 图 4 ‣
    4.5 程序提交状态 ‣ 4 实验 ‣ 通过解释解决方案程序提炼算法推理")所示，基准模型中有$42\%$的程序通过了公共测试并被接受，这表明推理器学会了使用高效算法，而不是实现暴力解决方案。
- en: '![Refer to caption](img/c3585252d3ba42c45486b088e14993e0.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c3585252d3ba42c45486b088e14993e0.png)'
- en: 'Figure 3: Final online judgment of programs that pass public tests. Accepted:
    correct; TLE: time limit exceeded; WA: wrong answer(s) on private tests; Other:
    memory limit exceeded, runtime error etc.⁵⁵5This is for all submissions, i.e.,
    one problem might have up to $k$ submissions, which is different from the problem-wise
    solve rate.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：通过公共测试的程序最终在线判断。Accepted: 正确；TLE: 超过时间限制；WA: 私有测试中的错误答案；Other: 内存限制超出，运行时错误等。⁵⁵5这适用于所有提交，即一个问题可能有最多$k$次提交，这与按问题计算的解决率不同。'
- en: '![Refer to caption](img/a165742fb0158dac61feec29a061861a.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a165742fb0158dac61feec29a061861a.png)'
- en: 'Figure 4: The problem difficulty statistics for problems solved with fine-tuned
    or zero-shot Reasoner when sampling 100 reasoning processes per problem.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：在每个问题采样100个推理过程时，使用微调或零-shot推理器解决的问题难度统计。
- en: 4.6 Solved Problems Analysis
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6 已解决问题分析
- en: Given that compared to zero-shot settings, using the Fine-tuned Reasoner only
    helps solve several more problems, we investigate further whether this improvement
    is due to learning from explanations or is merely coincidental. Thus, we aggregated
    all solved problems from 5 experiments with different sampling strategies and
    temperatures with both a zero-shot baseline Reasoner and our fine-tuned Reasoner.
    In other words, for each problem, we evaluated 50 programs generated under the
    zero-shot setting and another 50 under the explanation Fine-tuned setting. marking
    problems solved by either to obtain a final set of 23 “solvable” problems.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到与零-shot设置相比，使用微调的推理器仅能解决更多问题，我们进一步探讨了这种改进是否由于从解释中学习还是仅仅是巧合。因此，我们汇总了5次实验中使用零-shot基准推理器和我们微调的推理器的不同采样策略和温度下的所有已解决问题。换句话说，对于每个问题，我们评估了在零-shot设置下生成的50个程序和在解释微调设置下生成的另外50个程序，通过标记由任一方法解决的问题，获得了最终的23个“可解”问题集。
- en: Then, we measured solve@100 for this “solvable” subset. For each problem in
    the solvable set, we sample 100 explanations from both the Zero-shot and Fine-tuned
    Reasoners. By asking the same Coder to implement them, we found that the Explanation-Fine-tuned
    Reasoner achieves a 100% solve@100 while Zero-shot Reasoner only achieves 60.9%
    on this set.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们测量了这个“可解”子集的solve@100。对于可解集合中的每个问题，我们从零-shot和微调的推理器中各采样了100个解释。通过让同一个编码者实现这些解释，我们发现解释-微调的推理器在这个集合上达到了100%的solve@100，而零-shot推理器仅达到了60.9%。
- en: The difficulty distribution of these problems is shown in Figure [4](#S4.F4
    "Figure 4 ‣ 4.5 Program Submission Status ‣ 4 Experiments ‣ Distilling Algorithmic
    Reasoning from LLMs via Explaining Solution Programs"). We observed that with
    a fine-tuned Reasoner, more difficult problems can be solved, while the zero-shot
    original model mainly solves problems with the lowest difficulty rating (800).
    This highlights that learning from explanations can enhance the Reasoner’s ability
    to address more complex problems.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题的难度分布如图[4](#S4.F4 "图 4 ‣ 4.5 程序提交状态 ‣ 4 实验 ‣ 通过解释解决方案程序提炼算法推理")所示。我们观察到，使用微调的推理器可以解决更难的问题，而零-shot原始模型主要解决难度评级最低的问题（800）。这突显了从解释中学习可以增强推理器处理更复杂问题的能力。
- en: 4.7 Case Study
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7 案例研究
- en: '| Problem Statement: Yarik recently found an array $a$ is not, as 2 and 4 are
    both even and adjacent. You need to help Yarik by finding the maximum sum of such
    a subarray. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 问题陈述：Yarik最近发现一个数组$a$不是，因为2和4都是偶数并且相邻。你需要通过找到这样的子数组的最大和来帮助Yarik。 |'
- en: '| --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Fine-tuned Reasoner’s Reasoning Process  $\checkmark$: |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 微调的推理器推理过程  $\checkmark$: |'
- en: '| … checking all possible subarrays would be inefficient. Recognizing that
    the problem exhibits optimal substructure and overlapping subproblems, dynamic
    programming becomes a natural choice. … the maximum sum of a subarray ending at
    index $i$ and the tracking of the maximum sum. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| … 检查所有可能的子数组将是低效的。认识到该问题展示了最优子结构和重叠子问题，动态规划成为自然选择。… 子数组在索引 $i$ 结束时的最大和以及最大和的跟踪。
    |'
- en: '| Zero-shot Reasoner’s Reasoning Process  $\times$: |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 零样本推理器的推理过程  $\times$: |'
- en: '| To solve this problem, we can use a dynamic programming approach. We can
    iterate through the array and keep track of the maximum alternating sum ending
    at each element. We will maintain two values, one for the maximum alternating
    sum ending at the current element with an odd index and another for the maximum
    alternating sum ending at the current element with an even index. |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 要解决这个问题，我们可以使用动态规划方法。我们可以遍历数组，并跟踪每个元素结束时的最大交替和。我们将维护两个值，一个用于以奇数索引结束的当前元素的最大交替和，另一个用于以偶数索引结束的当前元素的最大交替和。
    |'
- en: 'Table 5: An algorithmic reasoning problem from CF Prob with reasoning processes
    from the fine-tuned and 0-shot Reasoner.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 5：来自CF Prob的算法推理问题，包含**微调**和**零样本推理器**的推理过程。
- en: Table [5](#S4.T5 "Table 5 ‣ 4.7 Case Study ‣ 4 Experiments ‣ Distilling Algorithmic
    Reasoning from LLMs via Explaining Solution Programs") shows a problem⁶⁶6[https://codeforces.com/contest/1899/problem/C](https://codeforces.com/contest/1899/problem/C)
    that can be solved by 14% of sampled programs from experiments with Fine-tuned
    Reasoner but none by the zero-shot Reasoner. We compare the output of our Fine-tuned
    Reasoner to that of the 0-shot Reasoner. The generated program and further analysis
    in sampled reasoning processes are given in Appendix [A](#A1 "Appendix A Appendix
    ‣ Distilling Algorithmic Reasoning from LLMs via Explaining Solution Programs").
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[5](#S4.T5 "表格 5 ‣ 4.7 案例研究 ‣ 4 实验 ‣ 通过解释解决方案程序从LLMs提取算法推理")展示了一个问题⁶⁶6[https://codeforces.com/contest/1899/problem/C](https://codeforces.com/contest/1899/problem/C)，该问题可以通过**微调推理器**的14%样本程序解决，但**零样本推理器**则无解。我们将**微调推理器**的输出与**零样本推理器**的输出进行比较。生成的程序和进一步的分析在附录[A](#A1
    "附录 A 附录 ‣ 通过解释解决方案程序从LLMs提取算法推理")中给出。
- en: This problem can be solved using dynamic programming while specifying the rule
    of having “alternating parities for adjacent elements”. The Fine-tuned Reasoner
    analyzes the inner logic of how to update the DP status and final answer correctly
    considering the condition. By contrast, the 0-Shot Reasoner only gives plausible
    surface-level reasoning output by connecting “alternating parities” to keywords
    like “odd” or “even”.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过动态规划解决，同时指定“相邻元素的交替奇偶性”规则。**微调推理器**分析了如何正确地更新DP状态和最终答案的内在逻辑，同时考虑到条件。相比之下，**零样本推理器**仅通过将“交替奇偶性”与“奇数”或“偶数”等关键词关联，给出似是而非的表层推理输出。
- en: 5 Conclusions and Future Work
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论与未来工作
- en: In this study, we propose to distill large language models’ ability to explain
    solutions into reasoning abilities used to help solve problems. More specifically,
    we use an LLM (Teacher) to explain, in natural language, the reasoning processes
    for  pairs and fine-tune a smaller LLM (Student) on
    such generated reasoning processes. Using a two-phase framework of reason-then-implement
    where the student model plays the role of a Reasoner to instruct a zero-shot Coder
    to generate the implementation. Experiments on real problems from Codeforces demonstrate
    that our explain-based distilling outperforms several strong 0-shot baselines
    as well as fine-tuning with code solutions alone. Even when absolute solve rates
    are low given how challenging the competitive-level algorithmic problems are,
    our method can significantly improve the performance and solve relatively 37%
    to 75% more problems than the strongest baselines across all experiments. Our
    training set for fine-tuning only contains 8248 data points and 8M tokens, and
    it suggests the efficiency of the proposed distilling method. Quantitative and
    qualitative analyses reveal that the fine-tuned Reasoner learns to avoid brute-force
    implementations and favors more efficient programs compared to its non-fine-tuned
    counterparts. We also curated an up-to-date test set for algorithmic reasoning
    challenges.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们提出将大型语言模型解释解决方案的能力提炼为帮助解决问题的推理能力。更具体地说，我们使用一个 LLM（教师）用自然语言解释对的推理过程，并对一个较小的
    LLM（学生）进行微调，使其基于这些生成的推理过程进行训练。使用一个先推理后实现的两阶段框架，其中学生模型充当推理者的角色，指导一个零样本编码器生成实现。来自
    Codeforces 的实际问题实验表明，我们基于解释的提炼方法优于几种强大的零样本基线以及仅使用代码解决方案的微调方法。即使在竞争级算法问题的绝对解决率较低的情况下，我们的方法也能显著提高性能，并在所有实验中比最强的基线多解决相对
    37% 到 75% 的问题。我们的微调训练集仅包含 8248 个数据点和 8M 标记，这表明了提出的提炼方法的效率。定量和定性分析显示，微调后的推理者学会避免蛮力实现，倾向于比未微调的模型更高效的程序。我们还策划了一个最新的算法推理挑战测试集。
- en: Automatically generating and learning from explanations of problem solutions
    to improve reasoning presents a promising path forward for tackling a broader
    array of complex reasoning tasks. Future investigations could extend this insight
    into domains where current LLMs struggle, such as abstract problem-solving in
    mathematics or logical deduction in law.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成并从问题解决方案的解释中学习以改善推理能力，是解决更广泛复杂推理任务的一条有前途的路径。未来的研究可以将这种洞察力扩展到当前大语言模型（LLMs）难以应对的领域，如数学中的抽象问题解决或法律中的逻辑推理。
- en: Ethics Statement
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 伦理声明
- en: This work focuses on proposing a method to enhance the reasoning ability of
    large language models on algorithmic reasoning. We will release the dataset CF
    Prob and the explanations generated for training if it’s not against OpenAI’s
    policy. We recognize the potential ways our framework can be misused, particularly
    in the context of online coding competitions. We firmly oppose any application
    of our work that would facilitate dishonest practices including cheating.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作专注于提出一种方法，以增强大型语言模型在算法推理方面的推理能力。如果不违反 OpenAI 的政策，我们将发布数据集 CF Prob 以及用于训练的生成解释。我们认识到我们的框架可能被滥用的潜在方式，特别是在在线编程竞赛的背景下。我们坚决反对任何会促成不诚实行为，包括作弊的应用。
- en: Another issue is that the data is in English and Python language only, which
    might need translation if used with other languages. Given the current performance
    of our models and the nascent state of algorithmic reasoning automation, we assess
    that the immediate industry application of our findings is unlikely. Consequently,
    we foresee minimal risk of adverse effects stemming from premature deployment
    in real-world scenarios.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是数据仅为英文和 Python 语言，这可能需要翻译以适应其他语言。鉴于我们模型的当前表现以及算法推理自动化的初步状态，我们评估发现的行业应用短期内不太可能。因此，我们预见到在实际应用中由于过早部署而导致的不良影响风险非常小。
- en: References
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Balog et al. (2017) Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian
    Nowozin, and Daniel Tarlow. Deepcoder: Learning to write programs. In *International
    Conference on Learning Representations*, 2017. URL [https://openreview.net/forum?id=ByldLrqlx](https://openreview.net/forum?id=ByldLrqlx).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Balog 等 (2017) Matej Balog, Alexander L. Gaunt, Marc Brockschmidt, Sebastian
    Nowozin, 和 Daniel Tarlow. Deepcoder: 学习编写程序。发表于 *国际学习表征会议*，2017年。网址 [https://openreview.net/forum?id=ByldLrqlx](https://openreview.net/forum?id=ByldLrqlx)。'
- en: Bucila et al. (2006) Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil.
    Model compression. In *Knowledge Discovery and Data Mining*, 2006. URL [https://api.semanticscholar.org/CorpusID:11253972](https://api.semanticscholar.org/CorpusID:11253972).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bucila et al. (2006) Cristian Bucila, Rich Caruana, 和 Alexandru Niculescu-Mizil.
    模型压缩。载于 *知识发现与数据挖掘*，2006年。网址 [https://api.semanticscholar.org/CorpusID:11253972](https://api.semanticscholar.org/CorpusID:11253972)。
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code. *CoRR*, abs/2107.03374,
    2021. URL [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374).
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Pondé de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas
    Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov,
    Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder,
    Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter,
    Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios
    Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol,
    Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu
    Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam,
    Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira
    Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,
    Ilya Sutskever, 和 Wojciech Zaremba. 评估训练于代码的大型语言模型。*CoRR*，abs/2107.03374，2021年。网址
    [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374)。
- en: 'Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen.
    Program of thoughts prompting: Disentangling computation from reasoning for numerical
    reasoning tasks, 2022.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2022) Wenhu Chen, Xueguang Ma, Xinyi Wang, 和 William W. Cohen.
    思维提示程序：将计算与推理分离以应对数字推理任务，2022年。
- en: Chen et al. (2023) Xinyun Chen, Maxwell Lin, Nathanael Schärli, and Denny Zhou.
    Teaching large language models to self-debug, 2023.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. (2023) Xinyun Chen, Maxwell Lin, Nathanael Schärli, 和 Denny Zhou.
    教授大型语言模型自我调试，2023年。
- en: 'Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng
    Liang. Deepseek-coder: When the large language model meets programming – the rise
    of code intelligence, 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. (2024) Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, 和 Wenfeng
    Liang. Deepseek-coder：当大型语言模型遇上编程——代码智能的崛起，2024年。
- en: Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    and Jacob Steinhardt. Measuring coding challenge competence with apps, 2021.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks et al. (2021) Dan Hendrycks, Steven Basart, Saurav Kadavath, Mantas
    Mazeika, Akul Arora, Ethan Guo, Collin Burns, Samir Puranik, Horace He, Dawn Song,
    和 Jacob Steinhardt. 通过应用程序测量编码挑战能力，2021年。
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network, 2015.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 从神经网络中提炼知识，2015年。
- en: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister.
    Distilling step-by-step! outperforming larger language models with less training
    data and smaller model sizes, 2023.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsieh et al. (2023) Cheng-Yu Hsieh, Chun-Liang Li, Chih-Kuan Yeh, Hootan Nakhost,
    Yasuhisa Fujii, Alexander Ratner, Ranjay Krishna, Chen-Yu Lee, 和 Tomas Pfister.
    逐步提炼！用更少的训练数据和更小的模型尺寸超越更大的语言模型，2023年。
- en: 'Hua & Zhang (2022) Wenyue Hua and Yongfeng Zhang. System 1 + system 2 = better
    world: Neural-symbolic chain of logic reasoning. In *Findings of the Association
    for Computational Linguistics: EMNLP 2022*, pp.  601–612, Abu Dhabi, United Arab
    Emirates, December 2022\. Association for Computational Linguistics. URL [https://aclanthology.org/2022.findings-emnlp.42](https://aclanthology.org/2022.findings-emnlp.42).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hua & Zhang (2022) Wenyue Hua 和 Yongfeng Zhang. 系统1 + 系统2 = 更好的世界：神经-符号逻辑推理链。载于
    *计算语言学协会会议成果：EMNLP 2022*，第601–612页，阿布扎比，阿联酋，2022年12月。计算语言学协会。网址 [https://aclanthology.org/2022.findings-emnlp.42](https://aclanthology.org/2022.findings-emnlp.42)。
- en: Huang et al. (2023) Yiming Huang, Zhenghao Lin, Xiao Liu, Yeyun Gong, Shuai
    Lu, Fangyu Lei, Yaobo Liang, Yelong Shen, Chen Lin, Nan Duan, and Weizhu Chen.
    Competition-level problems are effective llm evaluators, 2023.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang et al. (2023) 黄毅铭、林正豪、刘晓、龚叶云、吕帅、雷芳宇、梁耀博、沈业龙、林晨、段楠和陈伟柱。比赛级问题是有效的 llm 评估器，2023年。
- en: Kojima et al. (2022) Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka
    Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In
    *Advances in Neural Information Processing Systems*, 2022.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kojima et al. (2022) Kojima Takeshi、Shixiang Shane Gu、Machel Reid、Yutaka Matsuo
    和 Yusuke Iwasawa。大型语言模型是零样本推理器。在 *神经信息处理系统进展* 中，2022年。
- en: Li et al. (2023) Jierui Li, Szymon Tworkowski, Yingying Wu, and Raymond Mooney.
    Explaining competitive-level programming solutions using llms, 2023.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023) 李杰瑞、Szymon Tworkowski、吴莹莹 和 Raymond Mooney。使用 llms 解释竞赛级编程解决方案，2023年。
- en: 'Li et al. (2022) Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian
    Schrittwieser, Ré mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal
    Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d’Autume, Igor Babuschkin,
    Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James
    Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas,
    Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with AlphaCode.
    *Science*, 378(6624):1092–1097, dec 2022. doi: 10.1126/science.abq1158. URL [https://doi.org/10.1126%2Fscience.abq1158](https://doi.org/10.1126%2Fscience.abq1158).'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2022) 李宇佳、David Choi、Junyoung Chung、Nate Kushman、Julian Schrittwieser、Rémi
    Leblond、Tom Eccles、James Keeling、Felix Gimeno、Agustin Dal Lago、Thomas Hubert、Peter
    Choy、Cyprien de Masson d’Autume、Igor Babuschkin、陈心云、黄博森、Johannes Welbl、Sven Gowal、Alexey
    Cherepanov、James Molloy、Daniel J. Mankowitz、Esme Sutherland Robson、Pushmeet Kohli、Nando
    de Freitas、Koray Kavukcuoglu 和 Oriol Vinyals。使用 AlphaCode 进行竞赛级代码生成。*Science*，378(6624)：1092–1097，2022年12月。doi:
    10.1126/science.abq1158。网址 [https://doi.org/10.1126%2Fscience.abq1158](https://doi.org/10.1126%2Fscience.abq1158)。'
- en: Lyu et al. (2023) Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao,
    Eric Wong, Marianna Apidianaki, and Chris Callison-Burch. Faithful chain-of-thought
    reasoning, 2023.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyu et al. (2023) 吕青、Shreya Havaldar、Adam Stein、李章、Delip Rao、Eric Wong、Marianna
    Apidianaki 和 Chris Callison-Burch。忠实的思维链推理，2023年。
- en: Olausson et al. (2023) Theo X. Olausson, Jeevana Priya Inala, Chenglong Wang,
    Jianfeng Gao, and Armando Solar-Lezama. Is self-repair a silver bullet for code
    generation?, 2023.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olausson et al. (2023) Theo X. Olausson、Jeevana Priya Inala、王成龙、Jianfeng Gao
    和 Armando Solar-Lezama。自我修复是否是代码生成的灵丹妙药？2023年。
- en: 'OpenAI (2023a) OpenAI. ChatGPT: Optimizing Language Models for Dialogue. [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt),
    2023a.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023a) OpenAI。ChatGPT：优化对话的语言模型。 [https://openai.com/blog/chatgpt](https://openai.com/blog/chatgpt)，2023年。
- en: OpenAI (2023b) OpenAI. Gpt-4 technical report, 2023b.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023b) OpenAI。GPT-4 技术报告，2023年。
- en: 'Polosukhin & Skidanov (2018) Illia Polosukhin and Alexander Skidanov. Neural
    program search: Solving programming tasks from description and examples. *CoRR*,
    abs/1802.04335, 2018. URL [http://arxiv.org/abs/1802.04335](http://arxiv.org/abs/1802.04335).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polosukhin & Skidanov (2018) Illia Polosukhin 和 Alexander Skidanov。神经程序搜索：从描述和示例中解决编程任务。*CoRR*，abs/1802.04335，2018年。网址
    [http://arxiv.org/abs/1802.04335](http://arxiv.org/abs/1802.04335)。
- en: 'Ridnik et al. (2024) Tal Ridnik, Dedy Kredo, and Itamar Friedman. Code generation
    with alphacodium: From prompt engineering to flow engineering, 2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ridnik et al. (2024) Tal Ridnik、Dedy Kredo 和 Itamar Friedman。使用 Alphacodium
    进行代码生成：从提示工程到流程工程，2024年。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca:
    A strong, replicable instruction-following model. [https://crfm.stanford.edu/blog.html](https://crfm.stanford.edu/blog.html),
    2023. Accessed: date-of-access.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori et al. (2023) Rohan Taori、Ishaan Gulrajani、张天毅、Yann Dubois、李雪晨、Carlos
    Guestrin、Percy Liang 和 Tatsunori B. Hashimoto。Alpaca：一个强大且可复制的指令跟随模型。 [https://crfm.stanford.edu/blog.html](https://crfm.stanford.edu/blog.html)，2023年。访问时间：访问日期。
- en: Wang et al. (2022) Boshi Wang, Xiang Deng, and Huan Sun. Iteratively prompt
    pre-trained language models for chain of thought. In *Proceedings of the 2022
    Conference on Empirical Methods in Natural Language Processing*, pp.  2714–2730,
    Abu Dhabi, United Arab Emirates, December 2022\. Association for Computational
    Linguistics. URL [https://aclanthology.org/2022.emnlp-main.174](https://aclanthology.org/2022.emnlp-main.174).
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022) 王博施、邓翔和孙欢。迭代提示预训练语言模型以进行思维链。在 *2022年自然语言处理实证方法会议论文集* 中，页码
    2714–2730，阿布扎比，阿拉伯联合酋长国，2022年12月。计算语言学协会。网址 [https://aclanthology.org/2022.emnlp-main.174](https://aclanthology.org/2022.emnlp-main.174)。
- en: Wei et al. (2022) Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi,
    Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large
    language models. *arXiv preprint arXiv:2201.11903*, 2022.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2022）Jason Wei、Xuezhi Wang、Dale Schuurmans、Maarten Bosma、Ed Chi、Quoc Le
    和 Denny Zhou。**思维链提示激发大型语言模型中的推理**。*arXiv 预印本 arXiv:2201.11903*，2022年。
- en: Yang et al. (2023) Bohao Yang, Chen Tang, Kun Zhao, Chenghao Xiao, and Chenghua
    Lin. Effective distillation of table-based reasoning ability from llms, 2023.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等（2023）Bohao Yang、Chen Tang、Kun Zhao、Chenghao Xiao 和 Chenghua Lin。**从**
    llms 中有效提炼基于表格的推理能力，2023年。
- en: Yin et al. (2018) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and
    Graham Neubig. Learning to mine aligned code and natural language pairs from stack
    overflow, 2018.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等（2018）Pengcheng Yin、Bowen Deng、Edgar Chen、Bogdan Vasilescu 和 Graham Neubig。**从
    Stack Overflow 学习挖掘对齐的代码和自然语言对**，2018年。
- en: 'Yue et al. (2023) Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan
    Sun, Yu Su, and Wenhu Chen. Mammoth: Building math generalist models through hybrid
    instruction tuning, 2023.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue 等（2023）Xiang Yue、Xingwei Qu、Ge Zhang、Yao Fu、Wenhao Huang、Huan Sun、Yu Su
    和 Wenhu Chen。**Mammoth：通过混合指令调整构建数学通用模型**，2023年。
- en: 'Zelikman et al. (2023) Eric Zelikman, Qian Huang, Gabriel Poesia, Noah D. Goodman,
    and Nick Haber. Parsel: A (de-)compositional framework for algorithmic reasoning
    with language models, 2023.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zelikman 等（2023）Eric Zelikman、Qian Huang、Gabriel Poesia、Noah D. Goodman 和 Nick
    Haber。**Parsel：一种（去）组合的算法推理框架**，2023年。
- en: 'Zhang et al. (2023) Kexun Zhang, Danqing Wang, Jingtao Xia, William Yang Wang,
    and Lei Li. Algo: Synthesizing algorithmic programs with llm-generated oracle
    verifiers, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2023）Kexun Zhang、Danqing Wang、Jingtao Xia、William Yang Wang 和 Lei Li。**Algo：用
    LLM 生成的 oracle 验证器合成算法程序**，2023年。
- en: Zhou et al. (2023) Denny Zhou, Nathanael Schärli, Le Hou, Jason Wei, Nathan
    Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and
    Ed Chi. Least-to-most prompting enables complex reasoning in large language models,
    2023.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2023）Denny Zhou、Nathanael Schärli、Le Hou、Jason Wei、Nathan Scales、Xuezhi
    Wang、Dale Schuurmans、Claire Cui、Olivier Bousquet、Quoc Le 和 Ed Chi。**最小到最大**提示使大型语言模型能够进行复杂推理，2023年。
- en: 'Zhu et al. (2023) Xuekai Zhu, Biqing Qi, Kaiyan Zhang, Xingwei Long, and Bowen
    Zhou. Pad: Program-aided distillation specializes large models in reasoning, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2023）Xuekai Zhu、Biqing Qi、Kaiyan Zhang、Xingwei Long 和 Bowen Zhou。**Pad：程序辅助蒸馏使大型模型专注于推理**，2023年。
- en: Appendix A Appendix
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: A.1 Generated Reasoning Process Analysis
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 生成的推理过程分析
- en: '| Problem Statement: Yarik recently found an array $a$ is not, as 2 and 4 are
    both even and adjacent. You need to help Yarik by finding the maximum sum of such
    a subarray. |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 问题陈述：Yarik 最近发现一个数组 $a$ 不符合要求，因为 2 和 4 都是偶数并且相邻。你需要通过找到这样的子数组的最大和来帮助 Yarik。
    |'
- en: '| --- |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Fine-tuned Reasoner Generated Process $\checkmark$: |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 微调推理器生成的过程 $\checkmark$： |  |'
- en: '| … checking all possible subarrays would be inefficient. Recognizing that
    the problem exhibits optimal substructure and overlapping subproblems, dynamic
    programming becomes a natural choice. … the maximum sum of a subarray ending at
    index $i$:    To solve this problem, we can use a dynamic programming approach.
    We can iterate through the array and keep track of the maximum alternating sum
    ending at each element. We will maintain two values, one for the maximum alternating
    sum ending at the current element with an odd index and another for the maximum
    alternating sum ending at the current element with an even index. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| … 检查所有可能的子数组将会低效。认识到问题具有最优子结构和重叠子问题，动态规划成为自然选择。… 以索引 $i$ 结束的子数组的最大和：    为了解决这个问题，我们可以使用动态规划方法。我们可以遍历数组，并跟踪每个元素结束时的最大交替和。我们将维护两个值，一个是当前元素奇数索引结束的最大交替和，另一个是当前元素偶数索引结束的最大交替和。
    |'
- en: '| GPT-3.5 Coder Generated Program: |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 编码器生成的程序： |  |'
- en: '| def  max_alternating_subarray_sum(arr): n  =  len(arr) dp  =  [0]  *  n dp[0]  =  arr[0]
    for  i  in  range(1,  n): if  arr[i]  %  2  !=  arr[i-1]  %  2: dp[i]  =  max(dp[i-1]  +  arr[i],  arr[i])
    else: dp[i]  =  arr[i] return  max(dp) t  =  int(input()) for  _  in  range(t):
    n  =  int(input()) arr  =  list(map(int,  input().split())) result  =  max_alternating_subarray_sum(arr)
    print(result) | def  max_alternating_sum(arr): n  =  len(arr) dp_odd,  dp_even  =  [0]  *  n,  [0]  *  n
    dp_odd[0]  =  arr[0] dp_even[0]  =  0 for  i  in  range(1,  n): dp_odd[i]  =  max(dp_even[i-1]  +  arr[i],  arr[i])
    dp_even[i]  =  max(dp_odd[i-1]  -  arr[i],  0) return  max(dp_odd[-1],  dp_even[-1])
    t  =  int(input()) for  _  in  range(t): n  =  int(input()) arr  =  list(map(int,  input().split()))
    result  =  max_alternating_sum(arr) print(result) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| def max_alternating_subarray_sum(arr): n = len(arr) dp = [0] * n dp[0] =
    arr[0] for i in range(1, n): if arr[i] % 2 != arr[i-1] % 2: dp[i] = max(dp[i-1]
    + arr[i], arr[i]) else: dp[i] = arr[i] return max(dp) t = int(input()) for _ in
    range(t): n = int(input()) arr = list(map(int, input().split())) result = max_alternating_subarray_sum(arr)
    print(result) | def max_alternating_sum(arr): n = len(arr) dp_odd, dp_even = [0]
    * n, [0] * n dp_odd[0] = arr[0] dp_even[0] = 0 for i in range(1, n): dp_odd[i]
    = max(dp_even[i-1] + arr[i], arr[i]) dp_even[i] = max(dp_odd[i-1] - arr[i], 0)
    return max(dp_odd[-1], dp_even[-1]) t = int(input()) for _ in range(t): n = int(input())
    arr = list(map(int, input().split())) result = max_alternating_sum(arr) print(result)
    |'
- en: 'Table 6: An algorithmic reasoning problem from CF Prob with reasoning processes
    from the finetuned and 0-shot Reasoner. The incorrect process leads to a failed
    solution(right side).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：来自CF Prob的一个算法推理问题，展示了来自微调和零样本推理器的推理过程。错误的过程导致了失败的解决方案（右侧）。
- en: We ask a human expert to evaluate the generated reasoning processes by Fine-tuned
    and Zero-shot Reasoner and compare them. Unlike [5](#S4.T5 "Table 5 ‣ 4.7 Case
    Study ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"), we look at all 10 reasoning processes for both. We found
    that while the 0-shot model shows the correct general understanding that it can
    be solved as a dynamic programming problem, it fails to further reason meaningfully.
    9 out of 10 reasoning processes suggest maintaining ‘MaxOdd‘ and ‘MaxEven‘ variables
    to consider odd and even positions separately. Only one reasoning process suggests
    a variant of Kadane’s algorithm, but fails to understand what does “ must have
    alternating parities for adjacent elements” means.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们请一位人类专家评估由微调和零样本推理器生成的推理过程并进行比较。与[5](#S4.T5 "表5 ‣ 4.7 案例研究 ‣ 4 实验 ‣ 通过解释解决方案程序提炼算法推理")不同，我们查看了两者的所有10个推理过程。我们发现，尽管零样本模型展示了可以将其解决为动态规划问题的正确总体理解，但它未能进一步进行有意义的推理。10个推理过程中的9个建议保持‘MaxOdd’和‘MaxEven’变量，以便分别考虑奇数和偶数位置。只有一个推理过程建议使用Kadane算法的变体，但未能理解“必须有相邻元素的交替奇偶性”是什么意思。
- en: 'On the contrary, with the fine-tuned Reasoner, 3 out 10 samples realize ‘odd‘
    and ‘even‘ should be considered as the condition to update the sum, and 2 out
    10 use a greedy method instead. This observation indicates a more human-mimicking
    property: by learning from explanation, the Reasoner tends to try different strategies.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，使用微调的推理器时，10个样本中有3个认识到‘odd’和‘even’应被视为更新和的条件，2个使用贪心方法。这一观察表明了更具人类模拟的特性：通过从解释中学习，推理器倾向于尝试不同的策略。
- en: As shown in [6](#A1.T6 "Table 6 ‣ A.1 Generated Reasoning Process Analysis ‣
    Appendix A Appendix ‣ Distilling Algorithmic Reasoning from LLMs via Explaining
    Solution Programs"), the main difference between 2 programs is their used algorithm.
    The correct reasoning process can guide the Coder to implement the correct solution,
    suggesting a better understanding of the problem condition.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如[6](#A1.T6 "表6 ‣ A.1 生成的推理过程分析 ‣ 附录A 附录 ‣ 通过解释解决方案程序提炼算法推理")所示，两个程序之间的主要区别是所使用的算法。正确的推理过程可以指导编码人员实现正确的解决方案，表明对问题条件的更好理解。
- en: A.2 Prompts and Examples
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 提示和示例
- en: 'System Prompt: You are an expert in algorithm and programming.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示：你是算法和编程方面的专家。
- en: 'User Prompt: Analyze the following competitive programming problem and its
    provided solution. Write an editorial to help students understand the problem-solving
    approach and strategy. Consider the problem’s details, the solution’s approach,
    and the idea behind it.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提示：分析以下竞赛编程问题及其提供的解决方案。写一篇编辑文章，以帮助学生理解问题解决方法和策略。考虑问题的细节、解决方案的方法以及背后的思想。
- en: ———–
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ———–
- en: 'Problem: In this problem, we only consider strings consisting of lowercase
    English letters. Strings $s$ are said to be isomorphic when the following conditions
    are satisfied:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在此问题中，我们仅考虑由小写英文字母组成的字符串。当满足以下条件时，字符串 $s$ 被称为同构的：
- en: '* $|s|=|t|$ holds.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '* $|s|=|t|$ 适用。'
- en: '* For every pair i, j, one of the following holds:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '* 对于每对 i, j，以下之一成立：'
- en: '* $s_{i}=s_{j}$.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '* $s_{i}=s_{j}$。'
- en: '* $s_{i}\neq s_{j}$.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '* $s_{i}\neq s_{j}$。'
- en: 'For example, ‘abcac‘ and ‘zyxzx‘ are isomorphic, while ‘abcac‘ and ‘ppppp‘
    are not. A string s is said to be in normal form when the following condition
    is satisfied: * For every string t that is isomorphic to $s,s\leq t$ denotes lexicographic
    comparison.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，‘abcac‘ 和 ‘zyxzx‘ 是同构的，而‘abcac‘ 和‘ppppp‘ 不是。当满足以下条件时，字符串 s 被称为正常形式：* 对于每个与
    $s$ 同构的字符串 t，$s\leq t$ 表示字典序比较。
- en: For example, ‘abcac‘ is in normal form, but ‘zyxzx‘ is not since it is isomorphic
    to ‘abcac‘, which is lexicographically smaller than ‘zyxzx‘.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，‘abcac‘ 是正常形式，但‘zyxzx‘ 不是，因为它与‘abcac‘ 同构，而‘abcac‘ 在字典序上小于‘zyxzx‘。
- en: You are given an integer N. Print all strings of length N that are in normal
    form, in lexicographically ascending order.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个整数 N。打印所有长度为 N 的正常形式字符串，按字典序升序排列。
- en: Constraints
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 约束条件
- en: '* $1\leq N\leq 10$'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '* $1\leq N\leq 10$'
- en: '* All values in input are integers.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '* 输入中的所有值都是整数。'
- en: Input
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: 'Input is given from Standard Input in the following format:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 输入通过标准输入给出，格式如下：
- en: N
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: N
- en: Output
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: 'Assume that there are K strings of length N that are in normal form: $w_{1},\ldots,w_{K}$
    in lexicographical order. Output should be in the following format:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有 K 个长度为 N 的正常形式字符串：$w_{1},\ldots,w_{K}$ 按字典序排列。输出应采用以下格式：
- en: $w_{1}$
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: $w_{1}$
- en: Examples
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: Input
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: '1'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '1'
- en: Output
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: a
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: a
- en: Input
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: '2'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '2'
- en: Output
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: aa
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: aa
- en: ab
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ab
- en: ———–
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ———–
- en: Below is an accepted solution. Analyze it in the context of the problem.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个接受的解决方案。请在问题的背景下进行分析。
- en: 'n  =  list(map(int,  input().split(’  ’)))[0]def  dfs(i,  mx,  n,  res,  cur  =  []):if  i==n:res.append(’’.join(cur[::]))returnfor  v  in  range(0,  mx  +  1):dfs(i+1,  mx+(1  if  v==mx  else  0),  n  ,  res,  cur+[chr(v+ord(’a’))])res  =[]dfs(0,0,n,res)for  w  in  res:  print(w)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 'n  =  list(map(int,  input().split(’  ’)))[0]def  dfs(i,  mx,  n,  res,  cur  =  []):if  i==n:res.append(’’.join(cur[::]))returnfor  v  in  range(0,  mx  +  1):dfs(i+1,  mx+(1  if  v==mx  else  0),  n  ,  res,  cur+[chr(v+ord(’a’))])res  =[]dfs(0,0,n,res)for  w  in  res:  print(w)'
- en: 'Answer using the following format, with clear, detailed explanations. Use precise
    terms and avoid ambiguity. Use specific descriptions instead of vague expressions
    like "rearrange it" or "apply some operation". Specific instructions for each
    point are as follows in brackets"":'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 请使用以下格式回答，提供清晰、详细的解释。使用准确的术语，避免模糊。使用具体描述而非“重新排列”或“应用某种操作”等模糊表达。每一点的具体指示如下：
- en: '1). Problem Restatement: Understand every aspect of the problem first. Summarize
    the problem statement to remove the narrative/storytelling or thematic elements
    like characters or background story, abstract it into a formal statement while
    describing constraints, input-output specifications.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 1). 问题重述：首先理解问题的各个方面。总结问题陈述，去除叙述/故事情节或背景故事的元素，将其抽象为正式声明，同时描述约束条件、输入输出规格。
- en: '2). Step-by-Step Solution Explanation: Explain the solution code step-by-step
    in an algorithm level instead of explaining code line-by-line. Focus on the algorithm
    rather than implementation details like how to read input.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 2). 步骤解析：逐步解释解决方案代码，而不是逐行解释代码。关注算法而不是实现细节，如如何读取输入。
- en: '3). Solution Description: Based on the understanding of both the problem and
    the solution, describe the solution approach verbally. Explain the solution and
    the high-level reasoning behind it. Explain the WHY of the core algorithms/data
    structures/problem modeling.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 3). 解决方案描述：根据对问题和解决方案的理解，口头描述解决方案方法。解释解决方案及其背后的高层次推理。解释核心算法/数据结构/问题建模的原因。
- en: '4). Conceptual Evolution: Given all discussed above, describe how one would
    arrive at this solution. This can include how to analyze and approach the problem,
    the choices of type of algorithm used (e.g., dynamic programming, greedy, graph
    theory), the intuition behind the approach, and why this approach works for this
    problem. This is a narrative of the problem-solving journey.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 4). 概念演变：基于以上讨论，描述如何得出该解决方案。这可以包括如何分析和处理问题，所用算法类型的选择（例如动态规划、贪心、图论），方法背后的直觉，以及为何该方法适用于此问题。这是解决问题过程的叙述。
- en: '5). Common Pitfalls: Pitfalls in the problem description OR common errors that
    students might make while attempting the problem OR corner/edge cases or offset
    handled in the solution.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 5). 常见陷阱：问题描述中的陷阱或学生在尝试问题时可能犯的常见错误，或解决方案中处理的角落/边缘情况。
- en: 6). Key to Solution:Use one sentence to illustrate the "aha!" steps (key idea
    or trick) in the solution. Be concise, specific and informative.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 6). 解决方案关键：用一句话说明解决方案中的“aha！”步骤（关键思想或技巧）。简洁、具体且信息丰富。
- en: 'Figure 5: Explainer Prompt Example: Example of the prompt we are using to generate
    Explanations from Explainer (GPT-4). The problem and solution is from Li et al.
    ([2022](#bib.bib14)).'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：解释提示示例：我们用来从解释器（GPT-4）生成解释的提示示例。问题和解决方案来自 Li 等人 ([2022](#bib.bib14))。
- en: 'System Prompt: You are an expert in algorithm and programming.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示：你是算法和编程方面的专家。
- en: 'User Prompt: Given the algorithmic reasoning problem below, analyze it first,
    then develop a verbal description of the solution. Focus on the idea rather than
    the implementation details.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提示：给定下面的算法推理问题，首先分析它，然后制定解决方案的口头描述。重点在于思想，而非实现细节。
- en: 'Problem: In this problem, we only consider strings consisting of lowercase
    English letters. Strings $s$ are said to be isomorphic when the following conditions
    are satisfied:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：在此问题中，我们仅考虑由小写字母组成的字符串。字符串 $s$ 当满足以下条件时被认为是同构的：
- en: '* $|s|=|t|$ holds.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '* $|s|=|t|$ 成立。'
- en: '* For every pair i, j, one of the following holds:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '* 对于每一对 i, j，以下条件之一成立：'
- en: '* $s_{i}=s_{j}$.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '* $s_{i}=s_{j}$。'
- en: '* $s_{i}\neq s_{j}$.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '* $s_{i}\neq s_{j}$。'
- en: 'For example, ‘abcac‘ and ‘zyxzx‘ are isomorphic, while ‘abcac‘ and ‘ppppp‘
    are not. A string s is said to be in normal form when the following condition
    is satisfied: * For every string t that is isomorphic to $s,s\leq t$ denotes lexicographic
    comparison.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，‘abcac‘ 和 ‘zyxzx‘ 是同构的，而 ‘abcac‘ 和 ‘ppppp‘ 不是。当满足以下条件时，字符串 s 被认为是正常形式的：*
    对于每个与 $s$ 同构的字符串 t，$s\leq t$ 表示字典序比较。
- en: For example, ‘abcac‘ is in normal form, but ‘zyxzx‘ is not since it is isomorphic
    to ‘abcac‘, which is lexicographically smaller than ‘zyxzx‘.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，‘abcac‘ 是正常形式的，但 ‘zyxzx‘ 不是，因为它与 ‘abcac‘ 同构，而 ‘abcac‘ 在字典序上小于 ‘zyxzx‘。
- en: You are given an integer N. Print all strings of length N that are in normal
    form, in lexicographically ascending order.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个整数 N。打印所有长度为 N 的正常形式字符串，按字典序升序排列。
- en: Constraints
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 约束条件
- en: '* $1\leq N\leq 10$'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '* $1\leq N\leq 10$'
- en: '* All values in input are integers.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '* 输入中的所有值都是整数。'
- en: Input
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: 'Input is given from Standard Input in the following format:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 输入以以下格式从标准输入提供：
- en: N
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: N
- en: Output
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: 'Assume that there are K strings of length N that are in normal form: $w_{1},\ldots,w_{K}$
    in lexicographical order. Output should be in the following format:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有 K 个长度为 N 的正常形式字符串：$w_{1},\ldots,w_{K}$ 按字典序排列。输出应采用以下格式：
- en: $w_{1}$
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: $w_{1}$
- en: Examples
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 示例
- en: Input
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: '1'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '1'
- en: Output
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: a
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: a
- en: Input
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: '2'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '2'
- en: Output
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: aa
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: aa
- en: ab
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ab
- en: Assistant Prompt Let’s walk through the problem and think about how to solve
    it first.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 助手提示：让我们先思考问题并考虑如何解决它。
- en: 'Conceptual Evolution:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 概念演变：
- en: To arrive at this solution, one must understand the definition of isomorphic
    strings and normal form. The key insight is that a string in normal form will
    use the earliest possible character (’a’) and only introduce a new character when
    it cannot use an existing one. This insight leads to a recursive generation of
    strings, where at each step, we either reuse an existing character or introduce
    the next character in the alphabet. The DFS algorithm is a natural fit for this
    recursive construction process.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到这个解决方案，必须理解同构字符串和正常形式的定义。关键的洞察是，正常形式的字符串将使用最早的字符（‘a’），只有在无法使用现有字符时才引入新字符。这一洞察力导致了字符串的递归生成，在每一步中，我们要么重用现有字符，要么引入下一个字母。DFS
    算法非常适合这种递归构造过程。
- en: 'Key to Solution:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案关键：
- en: The äha!s̈tep is realizing that a string in normal form will always start with
    ’a’ and introduce new characters sequentially only when necessary, which can be
    efficiently generated using a depth-first search.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: äha!s̈tep 是意识到正常形式的字符串将始终以 ‘a’ 开头，并仅在必要时顺序引入新字符，这可以通过深度优先搜索高效生成。
- en: 'Solution Description:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案描述：
- en: The solution uses a depth-first search (DFS) approach to generate all possible
    strings in normal form. It builds strings character by character, ensuring that
    each new character either matches a previous character or is the next unused character
    in the alphabet. This guarantees that the generated strings are in normal form
    and lexicographically minimal.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 该解决方案使用深度优先搜索（DFS）方法生成所有可能的规范形式字符串。它逐个字符构建字符串，确保每个新字符要么匹配先前的字符，要么是字母表中下一个未使用的字符。这保证了生成的字符串是规范形式且词典序最小的。
- en: 'Common Pitfalls:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 常见陷阱：
- en: '- Not understanding the definition of isomorphic strings and normal form could
    lead to incorrect string generation.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '- 不理解同构字符串和规范形式的定义可能导致字符串生成不正确。'
- en: '- Failing to ensure that the strings are generated in lexicographic order might
    result in an incorrect output sequence.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '- 未能确保字符串按词典顺序生成可能会导致输出序列不正确。'
- en: '- Overlooking the base case in the recursive function could lead to infinite
    recursion or missing strings.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '- 忽视递归函数中的基本情况可能导致无限递归或遗漏字符串。'
- en: '- Not considering the constraint that N is at most 10 could lead to inefficient
    solutions that do not scale.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果不考虑N最大为10的限制，可能会导致效率低下的解决方案，无法扩展。'
- en: 'Step-by-Step Solution Explanation:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤说明：
- en: '- Read the integer N from the input.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '- 从输入中读取整数N。'
- en: '- Define a recursive function ‘dfs‘ that will build the strings in normal form.
    This function takes the current position ‘i‘, the maximum character index ‘mx‘
    used so far, the string length ‘n‘, a list ‘res‘ to store the results, and the
    current string ‘cur‘.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '- 定义一个递归函数`dfs`，该函数将在规范形式中构建字符串。此函数接受当前的位置`i`、截至目前使用的最大字符索引`mx`、字符串长度`n`、一个用于存储结果的列表`res`以及当前字符串`cur`。'
- en: '- If ‘i‘ equals ‘n‘, it means we have constructed a string of length ‘n‘. Add
    the current string ‘cur‘ to the results list ‘res‘ and return.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果`i`等于`n`，意味着我们已经构建了一个长度为`n`的字符串。将当前字符串`cur`添加到结果列表`res`中，并返回。'
- en: '- Iterate over all possible next characters, represented by their index ‘v‘
    from 0 to ‘mx + 1‘. The character to be added is determined by converting ‘v‘
    to a character starting from ’a’.'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '- 遍历所有可能的下一个字符，用索引`v`从0到`mx + 1`表示。要添加的字符由将`v`转换为从`a`开始的字符来确定。'
- en: '- For each character, recursively call ‘dfs‘ with the next position ‘i + 1‘,
    updating ‘mx‘ to ‘mx + 1‘ if ‘v‘ equals ‘mx‘ (indicating that we are using a new
    character for the first time), and appending the new character to ‘cur‘.'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于每个字符，递归调用`dfs`，位置为`i + 1`，如果`v`等于`mx`（表示我们第一次使用一个新字符），则将`mx`更新为`mx + 1`，并将新字符追加到`cur`中。'
- en: '- Initialize an empty list ‘res‘ to store the resulting strings.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '- 初始化一个空列表`res`来存储结果字符串。'
- en: '- Call ‘dfs‘ starting from position 0, with ‘mx‘ set to 0 (since we start with
    ’a’).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '- 从位置0开始调用`dfs`，`mx`设置为0（因为我们从`a`开始）。'
- en: '- After the recursive calls complete, print each string in ‘res‘.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '- 递归调用完成后，打印`res`中的每个字符串。'
- en: 'Figure 6: Reasoner Prompt Example: Example of the prompt we are using to Fine-tune
    the Reasoner (GPT-3.5) to generate explanations of the solution. The content from
    Assistant Prompt is the Explainer’s response to the query in Figure [5](#A1.F5
    "Figure 5 ‣ A.2 Prompts and Examples ‣ Appendix A Appendix ‣ Distilling Algorithmic
    Reasoning from LLMs via Explaining Solution Programs")'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：推理器提示示例：我们用来微调推理器（GPT-3.5）以生成解决方案解释的提示示例。助手提示中的内容是解释器对图[5](#A1.F5 "图5 ‣ A.2
    提示和示例 ‣ 附录A ‣ 通过解释解决方案程序提炼算法推理")查询的响应。
- en: 'System Prompt: You are an expert in algorithm and programming.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 系统提示：你是算法和编程方面的专家。
- en: 'User Prompt: Please solve the given algorithmic reasoning problem. You are
    also given a Hint on how to solve the problem. Analyze problem and hint, think
    carefully before implementing. The ultimate goal is to develop a complete Python
    3 program to solve the problem. The program should be designed to read input and
    produce output in the specified format mentioned in problem. Enclose the code
    within triple backticks (“‘) for clarity. Keep in mind that this code is intended
    for direct submission to an online judge system, so don’t include test cases.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 用户提示：请解决给定的算法推理问题。你还会获得一个关于如何解决该问题的提示。分析问题和提示，仔细思考后再实现。最终目标是开发一个完整的Python 3程序来解决该问题。程序应设计为读取输入并按问题中指定的格式输出结果。请用三个反引号（```）将代码括起来以便于阅读。请记住，这段代码是为了直接提交到在线评测系统的，因此不要包括测试用例。
- en: 'Here’s the Problem:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 问题如下：
- en: A subarray is a continuous part of array. Yarik recently found an array a of
    n elements and became very interested in finding the maximum sum of a non empty
    subarray. However, Yarik doesn’t like consecutive integers with the same parity,
    so the subarray he chooses must have alternating parities for adjacent elements.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 子数组是数组的连续部分。Yarik 最近发现了一个包含 n 个元素的数组，并对找到一个非空子数组的最大和非常感兴趣。然而，Yarik 不喜欢具有相同奇偶性的连续整数，因此他选择的子数组必须具有相邻元素的交替奇偶性。
- en: For example, [1, 2, 3] is acceptable, but [1, 2, 4] is not, as 2 and 4 are both
    even and adjacent. You need to help Yarik by finding the maximum sum of such a
    subarray.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[1, 2, 3] 是可以接受的，但 [1, 2, 4] 不行，因为 2 和 4 都是偶数且相邻。你需要帮助 Yarik 找到这样的子数组的最大和。
- en: Input
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 输入
- en: The first line contains an integer $t(1\leq t\leq 10^{4})$— number of test cases.
    Each test case is described as follows.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行包含一个整数 $t(1\leq t\leq 10^{4})$——测试用例的数量。每个测试用例的描述如下。
- en: The first line of each test case contains an integer n $(1\leq n\leq 2·10^{5})$—
    length of the array.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 每个测试用例的第一行包含一个整数n $(1\leq n\leq 2·10^{5})$——数组的长度。
- en: The second line of each test case contains n integers $a_{1},a_{2},…,a_{n}~{}(-10^{3}\leq
    a_{i}\leq 10^{3})$— elements of the array.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 每个测试用例的第二行包含n个整数 $a_{1},a_{2},…,a_{n}~{}(-10^{3}\leq a_{i}\leq 10^{3})$——数组的元素。
- en: It is guaranteed that the sum of n for all test cases does not exceed $2·10^{5}$.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 保证所有测试用例中n的总和不超过 $2·10^{5}$。
- en: Output
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 输出
- en: For each test case, output a single integer— the answer to the problem.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个测试用例，输出一个整数——问题的答案。
- en: 'Here’s the Hint on how to solve the problem:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 解决问题的提示如下：
- en: Let’s analyze the problem and think how to solve it. Let’s walk through the
    problem and think about how to solve it first.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析这个问题，思考如何解决。让我们先走一遍这个问题，思考如何解决它。
- en: 'Conceptual Evolution:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 概念演变：
- en: To arrive at this solution, one might start by considering the brute force approach
    of checking all possible subarrays, which would be inefficient. Recognizing that
    the problem exhibits optimal substructure and overlapping subproblems, dynamic
    programming becomes a natural choice. The intuition is that the maximum sum of
    a subarray ending at index ‘i‘ can be extended by including ‘a[i]‘ if it maintains
    the alternating parity condition. This insight leads to the iterative update of
    ‘dp‘ and the tracking of the maximum sum.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得出这个解决方案，人们可能会首先考虑暴力方法检查所有可能的子数组，这将是低效的。认识到问题展示了最优子结构和重叠子问题，动态规划成为一种自然选择。直觉是，结尾在索引‘i‘的子数组的最大和可以通过包含‘a[i]‘来扩展，只要它保持交替奇偶性条件。这一洞察力导致了‘dp‘的迭代更新和最大和的跟踪。
- en: 'Key to Solution:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案的关键：
- en: The "aha!" step is realizing that dynamic programming can be used to efficiently
    track the maximum sum of subarrays ending at each index, while maintaining the
    alternating parity condition.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: “aha!” 步骤是认识到动态规划可以用来高效地跟踪每个索引结尾的子数组的最大和，同时保持交替奇偶条件。
- en: 'Solution Description:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案描述：
- en: The solution uses dynamic programming to keep track of the maximum sum of subarrays
    ending at each index, while ensuring the alternating parity condition. The key
    insight is that the maximum sum of a subarray ending at index ‘i‘ can be derived
    from the maximum sum of a subarray ending at index ‘i-1‘ if the parity condition
    is satisfied. The ‘dp‘ array is used to store these intermediate results, and
    the ‘ans‘ variable is updated to keep track of the maximum sum found so far.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 该解决方案使用动态规划跟踪以每个索引结尾的子数组的最大和，同时确保交替奇偶条件。关键在于，如果满足奇偶条件，结尾在索引‘i‘的子数组的最大和可以从结尾在索引‘i-1‘的子数组的最大和中得出。‘dp‘数组用于存储这些中间结果，并且‘ans‘变量用于跟踪目前为止找到的最大和。
- en: 'Common Pitfalls:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 常见陷阱：
- en: '- Not considering the case where the array has only one element, which should
    be handled separately.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '- 不考虑数组只有一个元素的情况，这种情况应单独处理。'
- en: '- Misunderstanding the alternating parity condition and not updating the ‘dp‘
    array correctly.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '- 误解交替奇偶条件，未正确更新‘dp‘数组。'
- en: '- Forgetting to initialize the ‘dp‘ array with a value that does not affect
    the maximum sum (e.g., 0).'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '- 忘记将‘dp‘数组初始化为不会影响最大和的值（例如0）。'
- en: '- Overlooking the possibility of negative numbers in the array, which could
    lead to incorrect updates of the ‘dp‘ array.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '- 忽略了数组中可能存在负数，这可能导致‘dp‘数组的不正确更新。'
- en: 'Step-by-Step Solution Explanation:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步解决方案解释：
- en: '- Read the number of test cases ‘t‘.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '- 读取测试用例的数量‘t‘。'
- en: '- For each test case:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '- 对于每个测试用例：'
- en: '- Read the length of the array ‘n‘.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '- 读取数组‘n‘的长度。'
- en: '- Read the elements of the array ‘a‘.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '- 读取数组‘a‘的元素。'
- en: '- Initialize a dynamic programming (DP) array ‘dp‘ with a length of ‘n‘ and
    fill it with a large negative number (e.g., $-10^{9}$) to represent the maximum
    sum of a subarray ending at each index.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '- 初始化一个长度为‘n‘的动态规划（DP）数组‘dp‘，并填充一个很大的负数（例如，$-10^{9}$）以表示每个索引处以该索引结尾的子数组的最大和。'
- en: '- Initialize a variable ‘ans‘ to 0, which will hold the maximum sum found.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '- 初始化一个变量‘ans‘为0，用于保存找到的最大和。'
- en: '- If the array has only one element, set ‘ans‘ to that element’s value.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果数组只有一个元素，将‘ans‘设为该元素的值。'
- en: '- Otherwise, iterate over the array starting from the second element:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '- 否则，从第二个元素开始遍历数组：'
- en: '- If the current element and the previous element have different parities and
    they are not the first two elements of the array, update the ‘dp‘ array at the
    current index with the maximum of the current element and the sum of the current
    element and the ‘dp‘ value at the previous index.'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果当前元素和前一个元素的奇偶性不同，并且它们不是数组的前两个元素，则在当前索引处更新‘dp‘数组，值为当前元素和当前元素加上前一个索引处‘dp‘值中的较大者。'
- en: '- If the current element and the previous element have the same parity, update
    the ‘dp‘ array at the current index with the current element (as the previous
    subarray cannot be extended).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '- 如果当前元素和前一个元素具有相同的奇偶性，则在当前索引处更新‘dp‘数组，值为当前元素（因为之前的子数组无法扩展）。'
- en: '- Update ‘ans‘ with the maximum of its current value and the ‘dp‘ value at
    the current index.'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '- 更新‘ans‘，取其当前值和当前索引处‘dp‘值中的较大者。'
- en: '- Print the value of ‘ans‘ for the current test case.'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '- 打印当前测试用例的‘ans‘值。'
- en: 'Figure 7: Coder Prompt Example: Example of the prompt we are using to instruct
    the Coder generate code given the problem and the verbal solution(also included
    in this figure). This is the full version for what’s in Table [5](#S4.T5 "Table
    5 ‣ 4.7 Case Study ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs
    via Explaining Solution Programs")'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 编码器提示示例: 示例说明了我们用来指导编码器根据问题和口头解决方案（也包含在此图中）生成代码的提示。这是表格[5](#S4.T5 "Table
    5 ‣ 4.7 Case Study ‣ 4 Experiments ‣ Distilling Algorithmic Reasoning from LLMs
    via Explaining Solution Programs")中内容的完整版本。'
