- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:49:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:26
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Integer Scale：一种加速细粒度量化的免费午餐
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14597](https://ar5iv.labs.arxiv.org/html/2405.14597)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14597](https://ar5iv.labs.arxiv.org/html/2405.14597)
- en: Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Yifan Lu, Yerui Sun, Lin Ma, Yuchen
    Xie
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 李青源，孟然，李一铎，张博，陆一凡，孙业瑞，马林，谢宇晨
- en: Meituan Inc.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 美团公司
- en: '{liqingyuan02,mengran03,liyiduo,zhangbo97,luyifan04}@meituan.com'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{liqingyuan02,mengran03,liyiduo,zhangbo97,luyifan04}@meituan.com'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We introduce *Integer Scale*, a novel post-training quantization scheme for
    large language models that effectively resolves the inference bottleneck in current
    fine-grained quantization approaches while maintaining similar accuracies. Integer
    Scale is a free lunch as it requires no extra calibration or fine-tuning which
    will otherwise incur additional costs. It can be used plug-and-play for most fine-grained
    quantization methods. Its integration results in at most 1.85$\times$ compared
    with their FP16 versions respectively.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了*Integer Scale*，一种新颖的训练后量化方案，用于大型语言模型。它有效地解决了当前细粒度量化方法中的推理瓶颈，同时保持了类似的准确性。Integer
    Scale 是一种免费午餐，因为它不需要额外的校准或微调，否则会产生额外成本。它可以即插即用，适用于大多数细粒度量化方法。与其 FP16 版本相比，其集成结果最多提升1.85$\times$。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The size of language models has continued to grow exponentially throughout recent
    years. To name some iconic models, Transformers [[46](#bib.bib46)] initially bear
    65M parameters, BERT [[10](#bib.bib10)] exceeds with 340M, GPT-3 [[4](#bib.bib4)]
    prevails with 175B, PaLM  [[7](#bib.bib7)] trumps with 540B and most lately GPT-4
     [[35](#bib.bib35)] is estimated to have reached 1.8T parameters. This seemingly
    unstoppable trend is largely promoted by the so-called scaling law  [[19](#bib.bib19)]
    where a model’s capability, via a proxy metric of auto-regressive maximum-likelihood
    loss, exhibits a power-law relationship to its number of parameters, dataset sizes,
    and compute respectively. Not surprisingly, the intimidating number of parameters
    of Large Language Models (LLMs) place an almost insurmountable hurdle for inference,
    potentially preventing their pervasive applications.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，语言模型的规模持续呈指数级增长。举几个标志性模型的例子，Transformers [[46](#bib.bib46)] 最初具有 65M 参数，BERT
    [[10](#bib.bib10)] 超过了 340M，GPT-3 [[4](#bib.bib4)] 以 175B 获胜，PaLM [[7](#bib.bib7)]
    以 540B 领先，而最近的 GPT-4 [[35](#bib.bib35)] 估计已经达到 1.8T 参数。这一看似不可阻挡的趋势主要受到所谓的扩展法则
    [[19](#bib.bib19)] 的推动，该法则表明模型的能力通过自回归最大似然损失的代理度量，与其参数数量、数据集规模和计算资源呈幂律关系。毫不奇怪，大型语言模型（LLMs）的参数数量令人望而生畏，为推理带来了几乎难以逾越的障碍，可能阻碍其广泛应用。
- en: However, optimizing the serving efficiency of LLMs is a non-trivial task. LLMs
    generally comprise a compute-intense *pre-filling* stage and a memory-bound *self-decoding*
    stage. Exploiting integer matrix multiplication speeds up the computation, but
    directly applying post-training quantization usually generates a large performance
    drop. Quantization-aware training methods like LLM-QAT [[28](#bib.bib28)] require
    costly computing resources to fine-tune all the weights. In contrast, post-training
    quantization is more affordable and commonly used in practice. For instance, SmoothQuant [[48](#bib.bib48)]
    transforms activation outliers into weights for better quantization accuracy.
    Recently, fine-granularity grouping [[37](#bib.bib37)] is often used as a general
    paradigm to reduce the quantization errors, as in ZeroQuant [[49](#bib.bib49)],
    GPTQ [[14](#bib.bib14)], AWQ [[26](#bib.bib26)] and FPTQ [[24](#bib.bib24)]. FPTQ
    proposes a fine-grained W4A8 strategy to address the memory-bound issue as a trade-off
    between W4A16 and W8A8\. While its high quantization accuracy benefits from fine-grained
    quantization, the actual inference is also stalled by inefficient operations introduced
    by its intrinsic computational complexity due to fine granularity.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，优化 LLMs 的服务效率是一个复杂的任务。LLMs 通常包括一个计算密集型的*预填充*阶段和一个内存绑定的*自解码*阶段。利用整数矩阵乘法可以加速计算，但直接应用训练后量化通常会导致性能大幅下降。像
    LLM-QAT [[28](#bib.bib28)] 这样的量化感知训练方法需要昂贵的计算资源来微调所有权重。相比之下，训练后量化更为经济，且在实践中更为常用。例如，SmoothQuant
    [[48](#bib.bib48)] 将激活异常值转换为权重，以提高量化准确性。最近，细粒度分组 [[37](#bib.bib37)] 经常被用作减少量化误差的一种通用范式，如
    ZeroQuant [[49](#bib.bib49)]、GPTQ [[14](#bib.bib14)]、AWQ [[26](#bib.bib26)] 和
    FPTQ [[24](#bib.bib24)]。FPTQ 提出了细粒度 W4A8 策略，以在 W4A16 和 W8A8 之间作为权衡解决内存绑定问题。虽然其高量化准确性得益于细粒度量化，但由于细粒度带来的固有计算复杂性，实际推理也因效率低下而受到阻碍。
- en: 'In this paper, we are driven to design a faster fine-grained quantization scheme
    called *Integer Scale* that renders fewer quantization errors (Table [3](#S5.T3
    "Table 3 ‣ 5.2 Experiment Result on LAMBADA, C4, and WikiText-2 ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs"))
    and simultaneously achieves boosted speed (see Figure [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs")). Our contributions are multi-fold:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '在本文中，我们致力于设计一种更快的细粒度量化方案，称为*整数比例*，它能够减少量化误差（见表[3](#S5.T3 "Table 3 ‣ 5.2 Experiment
    Result on LAMBADA, C4, and WikiText-2 ‣ 5 Experiments ‣ Integer Scale: A Free
    Lunch for Faster Fine-grained Quantization of LLMs")）并同时提高速度（见图[1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs")）。我们的贡献有多方面：'
- en: '![Refer to caption](img/3de890c0550b48b6e38831d345ceb04a.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3de890c0550b48b6e38831d345ceb04a.png)'
- en: 'Figure 1: End-to-end latency comparison of W4A8 (Integer Scale) compared with
    W4A8 (Float Scale) and W4A16 (Marlin) on LLaMA-2 models. The speedup ratio is
    written on top of the bars.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：W4A8（整数比例）与 W4A8（浮点比例）以及 W4A16（Marlin）在 LLaMA-2 模型上的端到端延迟比较。加速比写在条形图上方。
- en: '1.'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: We unveil the intrinsic inference bottleneck of fine-grained LLM quantization
    approaches and find a hassle-free cure, called Integer Scale, with negligible
    accuracy loss. Our approach can be used as an out-of-box plugin for the state-of-the-art
    quantization methods (e.g. GPTQ [[14](#bib.bib14)], AWQ [[26](#bib.bib26)], Omniquant [[41](#bib.bib41)],
    QuaRot [[3](#bib.bib3)] etc.) with minimum modifications.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们揭示了细粒度 LLM 量化方法的固有推理瓶颈，并找到了一种无麻烦的解决方案，即整数比例，具有微不足道的准确性损失。我们的方法可以作为最先进量化方法（如
    GPTQ [[14](#bib.bib14)]、AWQ [[26](#bib.bib26)]、Omniquant [[41](#bib.bib41)]、QuaRot
    [[3](#bib.bib3)] 等）的现成插件，进行最小的修改。
- en: '2.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The orchestration of fine-grained quantization and the integer scale scheme
    not only retains the performance of the existing methods but also effectively
    addresses the quantization difficulty of LLMs built with the mixture-of-experts
    technique [[18](#bib.bib18)] and LLaMA-3 [[1](#bib.bib1)].
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 细粒度量化和整数比例方案的协调不仅保留了现有方法的性能，而且有效解决了基于专家混合技术构建的 LLMs [[18](#bib.bib18)] 和 LLaMA-3
    [[1](#bib.bib1)] 的量化难题。
- en: '3.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Our integer scale, when applied to fine-grained W4A8 paradigms, achieves at
    most 1.85$\times$ over its float scale counterpart, while being comparable in
    performance. This suggests the viability of our approach as we have achieved a
    new Pareto-front of speed vs. accuracy.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的整数尺度在细粒度 W4A8 模式中应用时，最多能比其浮点尺度对应物提升 1.85$\times$，同时性能相当。这表明我们的方法具有可行性，因为我们已经实现了速度与准确性的新帕累托前沿。
- en: 2 Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 2.1 LLM Serving Frameworks and Optimization Techniques
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 LLM 服务框架和优化技术
- en: vLLM [[20](#bib.bib20)] brings about paged attention [[20](#bib.bib20)] and
    continuous batching. FasterTransformer [[33](#bib.bib33)] provides a highly optimized
    inference framework featuring cutlass GEMMs, CUDA kernels. Built on top of FasterTransformer [[33](#bib.bib33)],
    LMDeploy [[8](#bib.bib8)] features an efficient backend called TurboMind that
    seeks extreme optimization through persistent batching, KV caching, and a low-bit
    quantization toolkit. Another sprout from FasterTransformer is TensorRT-LLM [[34](#bib.bib34)],
    which is tailored particularly for NVIDIA GPUs and ensembles many up-to-date inference
    techniques like flash attention [[9](#bib.bib9)], FP8 quantization [[30](#bib.bib30)],
    in-flight batching, graph optimization, etc. Marlin [[13](#bib.bib13)] ships so
    far the fastest W4A16 kernel along with a bag of optimization tricks, while QServe [[27](#bib.bib27)]
    brings an advanced W4A8 kernel implementation. FP6-LLM [[47](#bib.bib47)] delicately
    devises a software solution to support the FP6 precision on NVIDIA A100 GPUs.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: vLLM [[20](#bib.bib20)] 引入了分页注意力 [[20](#bib.bib20)] 和连续批处理。FasterTransformer [[33](#bib.bib33)]
    提供了一个高度优化的推理框架，特点是切片 GEMMs 和 CUDA 内核。在 FasterTransformer [[33](#bib.bib33)] 的基础上，LMDeploy [[8](#bib.bib8)]
    提供了一个名为 TurboMind 的高效后端，通过持久批处理、KV 缓存和低位量化工具包来寻求极致优化。另一个源自 FasterTransformer 的是
    TensorRT-LLM [[34](#bib.bib34)]，特别针对 NVIDIA GPUs，集成了许多最新的推理技术，如闪电注意力 [[9](#bib.bib9)]、FP8
    量化 [[30](#bib.bib30)]、在飞行中批处理、图优化等。Marlin [[13](#bib.bib13)] 目前提供了最快的 W4A16 内核及一系列优化技巧，而
    QServe [[27](#bib.bib27)] 则带来了先进的 W4A8 内核实现。FP6-LLM [[47](#bib.bib47)] 精心设计了一个支持
    NVIDIA A100 GPUs 上 FP6 精度的软件解决方案。
- en: 2.2 LLM Quantization Algorithms
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 LLM 量化算法
- en: Quantization is one of the most adopted optimization techniques to compress
    LLMs to their extremity. Nevertheless, it becomes more challenging as we chase
    for the quantization of lower bit widths (e.g. 4-bit, 2-bit, or binary), it faces
    more critical accuracy loss. It also requires efficient hardware-aware implementations
    that demand strenuous engineering effort.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是压缩 LLMs 到极限的最常用优化技术之一。然而，随着我们追求更低位宽的量化（如 4 位、2 位或二进制），它面临着更严重的准确性损失。此外，它还需要高效的硬件感知实现，这要求付出大量工程努力。
- en: Weight-only Quantization. GPTQ [[14](#bib.bib14)] renovates OBQ [[12](#bib.bib12)]
    to obtain an approximate second-order method that compensates for the quantization
    error. AWQ [[26](#bib.bib26)] is a mixed-precision weight-only method that locates
    salient weight channels and searches for the corresponding optimal scales. Omniquant [[41](#bib.bib41)]
    introduces learnable weight clipping that restricts extreme weight values and
    proposes learnable smoothing factors that tackle the activation outliers following
    SmoothQuant [[48](#bib.bib48)]. Extreme low-bit approaches also focus on weight-only
    quantization. Norm Tweaking [[22](#bib.bib22)] exploits layer norm tuning to alleviate
    the performance degradation, QuiP [[6](#bib.bib6)] profits from orthogonal matrices
    and AQLM [[11](#bib.bib11)] from additive quantization with a codebook for 2-bit
    quantization, while PB-LLM [[40](#bib.bib40)] uses partial 1-bit quantization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 仅权重量化。GPTQ [[14](#bib.bib14)] 改进了 OBQ [[12](#bib.bib12)]，获得了一种近似的二阶方法，以补偿量化误差。AWQ [[26](#bib.bib26)]
    是一种混合精度的仅权重方法，它定位显著的权重通道，并寻找对应的最优尺度。Omniquant [[41](#bib.bib41)] 引入了可学习的权重裁剪，限制极端的权重值，并提出了可学习的平滑因子，以应对激活异常值，继
    SmoothQuant [[48](#bib.bib48)] 之后。极低位宽的方法也关注于仅权重量化。Norm Tweaking [[22](#bib.bib22)]
    利用层归一化调优来减轻性能降级，QuiP [[6](#bib.bib6)] 利用正交矩阵，而 AQLM [[11](#bib.bib11)] 利用具有 2
    位量化代码本的加性量化，同时 PB-LLM [[40](#bib.bib40)] 使用部分 1 位量化。
- en: The weight-only scheme alleviates the memory-bound issue but its activation
    remains in FP16\. Recent speculative parallel decoding methods [[21](#bib.bib21),
    [25](#bib.bib25), [5](#bib.bib5)] lead the decoding phase to a compute-bound scenario,
    which makes this scheme less promising in the future.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 仅权重方案缓解了内存限制问题，但其激活仍为 FP16。最近的推测性并行解码方法[[21](#bib.bib21), [25](#bib.bib25),
    [5](#bib.bib5)]使解码阶段成为计算瓶颈，这使得该方案在未来不太具有前景。
- en: Weight-Activation Quantization. ZeroQuant [[49](#bib.bib49)] presents a fine-grained
    quantization scheme coupled with distillation. SmoothQuant [[48](#bib.bib48)]
    enables W8A8 post-training quantization by smoothing the outliers with a heuristic
    factor and ships with a handcrafted CUDA kernel that ensures hardware efficiency.
    OdysseyLLM [[23](#bib.bib23)] is a coarse-grained W4A8 scheme that reduces the
    performance gap compared with W4A16 and W8A8\. QUIK [[2](#bib.bib2)] implements
    W4A4 quantization with mixed-precision.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 权重-激活量化。ZeroQuant [[49](#bib.bib49)] 提出了与蒸馏结合的细粒度量化方案。SmoothQuant [[48](#bib.bib48)]
    通过使用启发式因子平滑异常值来实现 W8A8 后训练量化，并配有确保硬件效率的手工编写的 CUDA 内核。OdysseyLLM [[23](#bib.bib23)]
    是一种粗粒度 W4A8 方案，减少了与 W4A16 和 W8A8 的性能差距。QUIK [[2](#bib.bib2)] 实现了混合精度的 W4A4 量化。
- en: Fine granularity generally further enhances the quantized accuracy. FPTQ [[24](#bib.bib24)]
    is a W4A8 fine-grained solution. Atom [[52](#bib.bib52)] is a fine-grained mixed-precision
    W4A4 method. However, they typically suffer from low latency issues which cancel
    out the benefits from lower bit widths. DGQ [[51](#bib.bib51)] attempts to apply
    a dual quantization scheme to improve the efficiency of the fine-grained approach.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度通常进一步提高量化精度。FPTQ [[24](#bib.bib24)] 是一种 W4A8 细粒度解决方案。Atom [[52](#bib.bib52)]
    是一种细粒度的混合精度 W4A4 方法。然而，它们通常存在低延迟问题，这会抵消低位宽的好处。DGQ [[51](#bib.bib51)] 尝试应用双重量化方案来提高细粒度方法的效率。
- en: Rotation-based Quantization. QuiP [[6](#bib.bib6)], QuiP# [[45](#bib.bib45)],
    QuaRot [[3](#bib.bib3)] are a line of quantization methods that profits from the
    computation invariance of the orthogonal matrices for outlier suppression. To
    undo the rotation effect, extra online transformations are applied. When implemented
    efficiently, this overhead can be deemed nearly negligible.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 基于旋转的量化。QuiP [[6](#bib.bib6)], QuiP# [[45](#bib.bib45)], QuaRot [[3](#bib.bib3)]
    是一系列从正交矩阵的计算不变性中获益的量化方法，以进行异常值抑制。为了取消旋转效应，应用了额外的在线变换。当高效实现时，这种开销几乎可以忽略不计。
- en: '![Refer to caption](img/9ae67912458f1bce51c57362b40a5675.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/9ae67912458f1bce51c57362b40a5675.png)'
- en: 'Figure 2: (a) Fine-grained quantization divides activation $X$.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: (a) 细粒度量化划分激活 $X$。'
- en: 3 Motivation
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 动机
- en: 3.1 Fine Granularity Strengthens Current Quantization Approaches
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 细粒度增强当前量化方法
- en: Fine granularity approaches [[24](#bib.bib24), [26](#bib.bib26), [52](#bib.bib52)]
    bear prevailing benefits over many state-of-the-art LLM quantization methods.
    In extreme cases, it even produces reasonable results when coarse methods fail.
    It can be applied as a plug-in method to boost the accuracy of the existing methods.
    Formally, the output $\mathbf{O}_{i}$ of a fine-grained weight-activation quantization
    GEMM can be written as,
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 细粒度方法 [[24](#bib.bib24), [26](#bib.bib26), [52](#bib.bib52)] 相较于许多最先进的 LLM 量化方法具有显著优势。在极端情况下，当粗略方法失败时，它甚至能产生合理的结果。它可以作为插件方法来提升现有方法的准确性。正式地，细粒度权重-激活量化
    GEMM 的输出 $\mathbf{O}_{i}$ 可以写作，
- en: '|  | $\mathbf{O}_{i}=\mathbf{s}_{a_{i}}*\sum_{g}(\mathbf{X}_{g_{i}}\times\mathbf{W}_{g_{i}}^{\top})*\mathbf{s}_{g_{i}}$
    |  | (1) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{O}_{i}=\mathbf{s}_{a_{i}}*\sum_{g}(\mathbf{X}_{g_{i}}\times\mathbf{W}_{g_{i}}^{\top})*\mathbf{s}_{g_{i}}$
    |  | (1) |'
- en: where $\mathbf{s}_{a_{i}}$. Depending on the precision of matrix multiplication,
    specific type conversions are required to perform either scalar or matrix multiplication.
    For instance, if we adopt a fine-grained W8A8 scheme with integer tensor cores
    for the computation, the INT32 result has to be converted to float for the later
    dequantization.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{s}_{a_{i}}$。根据矩阵乘法的精度，需要进行特定的类型转换以执行标量或矩阵乘法。例如，如果我们采用细粒度的 W8A8 方案并使用整数张量核心进行计算，则
    INT32 结果必须转换为浮点数以进行后续的去量化。
- en: 'This process is depicted in Figure [2](#S2.F2 "Figure 2 ‣ 2.2 LLM Quantization
    Algorithms ‣ 2 Related Work ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs") (a), where it typically considers weights in groups and
    each has its float scale. We apply the fine-granularity strategy to approaches
    that cover commonly-used bit widths range in W4A16, W8A8, W4A8, and W4A4 in Table [1](#S3.T1
    "Table 1 ‣ 3.1 Fine Granularity Strengthens Current Quantization Approaches ‣
    3 Motivation ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") to exhibit that group-wise fine-granularity consistently improves the
    quantized performance compared with its original coarse counterpart. Note on the
    LLaMA-3-70B model, the vanilla Round-to-Nearest (RTN) caused a large performance
    collapse while its fine-grained version can easily handle it. As we drive from
    W8A8 to lower bits, the quantization error increases. Especially, when applying
    QuaRot [[3](#bib.bib3)] on LLaMA-3-70B at W4A4, the perplexity bursts into an
    unreasonable value, and fine-granularity can alleviate the issue.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程如图 [2](#S2.F2 "图 2 ‣ 2.2 LLM 量化算法 ‣ 2 相关工作 ‣ 整数缩放：LLM 细粒度量化的免费午餐") (a) 所示，通常考虑按组的权重，每组有其浮点缩放。我们将细粒度策略应用于表 [1](#S3.T1
    "表 1 ‣ 3.1 细粒度增强当前量化方法 ‣ 3 动机 ‣ 整数缩放：LLM 细粒度量化的免费午餐") 中的 W4A16、W8A8、W4A8 和 W4A4
    等常用位宽范围的方案，以展示与其原始粗粒度对应方法相比，组级细粒度 consistently 改善了量化性能。注意到 LLaMA-3-70B 模型中，普通的
    Round-to-Nearest (RTN) 导致了性能的大幅下滑，而其细粒度版本则可以轻松应对。随着从 W8A8 降至较低位，量化误差增加。特别是，当在
    W4A4 上对 LLaMA-3-70B 应用 QuaRot [[3](#bib.bib3)] 时，困惑度突增到不合理的值，细粒度可以缓解这个问题。
- en: 'Table 1: Applying fine granularity (denoted by ‘FG’) to the state-of-the-art
    quantization methods on LLaMA-2 models. Perplexity is tested on C4 (the lower
    the better). Group = -1 indicates coarse-grained weight quantization while 128
    means fine-grained with a group size of 128\.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在 LLaMA-2 模型上将细粒度（由“FG”表示）应用于最先进的量化方法。困惑度在 C4 上测试（值越低越好）。Group = -1 表示粗粒度权重量化，而
    128 意味着具有 128 的组大小的细粒度量化。
- en: '| Dataset | Bitwidth | Method |  | Group | LLaMA-2 | LLaMA-3 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 位宽 | 方法 |  | 组 | LLaMA-2 | LLaMA-3 |'
- en: '| C4 |  |  |  |  | 7B | 13B | 70B | 8B | 70B |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| C4 |  |  |  |  | 7B | 13B | 70B | 8B | 70B |'
- en: '| FP16 | Baseline |  |  | 7.05 | 6.46 | 5.52 | 8.88 | 6.73 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 基线 |  |  | 7.05 | 6.46 | 5.52 | 8.88 | 6.73 |'
- en: '| W8A8 | RTN [[49](#bib.bib49)] |  | -1 | 7.19 | 6.51 | 5.64 | 9.05 | 75.05
    |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | RTN [[49](#bib.bib49)] |  | -1 | 7.19 | 6.51 | 5.64 | 9.05 | 75.05
    |'
- en: '|  | RTN w/ FG |  | 128 | 7.2 | 6.51 | 5.64 | 9.04 | 7.15 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN w/ FG |  | 128 | 7.2 | 6.51 | 5.64 | 9.04 | 7.15 |'
- en: '| W8A8 | SmoothQuant [[48](#bib.bib48)] |  | -1 | 7.2 | 6.51 | 5.58 | 9.03
    | 7.38 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | SmoothQuant [[48](#bib.bib48)] |  | -1 | 7.2 | 6.51 | 5.58 | 9.03
    | 7.38 |'
- en: '|  | SmoothQuant w/ FG |  | 128 | 7.2 | 6.51 | 5.58 | 9.03 | 7.48 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | SmoothQuant w/ FG |  | 128 | 7.2 | 6.51 | 5.58 | 9.03 | 7.48 |'
- en: '| W8A8 | FPTQ [[24](#bib.bib24)] |  | -1 | 7.08 | 6.50 | 5.55 | 8.97 | 8.88
    |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| W8A8 | FPTQ [[24](#bib.bib24)] |  | -1 | 7.08 | 6.50 | 5.55 | 8.97 | 8.88
    |'
- en: '|  | FPTQ w/ FG |  | 128 | 7.08 | 6.50 | 5.54 | 8.95 | 6.81 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | FPTQ w/ FG |  | 128 | 7.08 | 6.50 | 5.54 | 8.95 | 6.81 |'
- en: '| W4A16 | GPTQ [[14](#bib.bib14)] |  | -1 | 7.47 | 6.84 | 5.71 | 10.54 | 7.83
    |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | GPTQ [[14](#bib.bib14)] |  | -1 | 7.47 | 6.84 | 5.71 | 10.54 | 7.83
    |'
- en: '|  | GPTQ w/ FG |  | 128 | 7.22 | 6.65 | 5.61 | 9.70 | 7.26 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ w/ FG |  | 128 | 7.22 | 6.65 | 5.61 | 9.70 | 7.26 |'
- en: '| W4A8 | Odyssey [[23](#bib.bib23)] |  | -1 | 7.58 | 6.70 | 5.78 | 10.25 |
    12.15 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| W4A8 | Odyssey [[23](#bib.bib23)] |  | -1 | 7.58 | 6.70 | 5.78 | 10.25 |
    12.15 |'
- en: '|  | Odyssey w/ FG |  | 128 | 7.26 | 6.60 | 5.60 | 9.56 | 7.09 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | Odyssey w/ FG |  | 128 | 7.26 | 6.60 | 5.60 | 9.56 | 7.09 |'
- en: '| W4A4 | QuaRot [[3](#bib.bib3)] |  | -1 | 7.87 | 7.11 | 5.92 | 12.06 | 544.50
    |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | QuaRot [[3](#bib.bib3)] |  | -1 | 7.87 | 7.11 | 5.92 | 12.06 | 544.50
    |'
- en: '|  | QuaRot w/ FG |  | 128 | 7.82 | 7.08 | 5.90 | 11.8 | 132.20 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | QuaRot w/ FG |  | 128 | 7.82 | 7.08 | 5.90 | 11.8 | 132.20 |'
- en: 3.2 Fine-grained Quantization Suffers from the Inference Bottleneck
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 细粒度量化面临推理瓶颈
- en: '![Refer to caption](img/021b43c45df24a5ed6ffc0c4809c58cf.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/021b43c45df24a5ed6ffc0c4809c58cf.png)'
- en: 'Figure 3: Kernel latency comparison between W4A8 w/ Float Scale vs. FP16\.
    The red line denotes its acceleration ratios over FP16.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：W4A8 使用浮点缩放与 FP16 的内核延迟比较。红线表示相对于 FP16 的加速比。
- en: 'Although fine-grained quantization can achieve higher accuracy, as demonstrated
    in [[24](#bib.bib24)], we have found it to be particularly slow during inference,
    which is also noted in the Dual-Granularity Quantization (DGQ) method [[51](#bib.bib51)].
    The advantages of using lower bit widths are often offset by the computational
    overhead they introduce. Figure [3](#S3.F3 "Figure 3 ‣ 3.2 Fine-grained Quantization
    Suffers from the Inference Bottleneck ‣ 3 Motivation ‣ Integer Scale: A Free Lunch
    for Faster Fine-grained Quantization of LLMs") compares the kernel latency under
    typical inference batch sizes (drops from 3.15$\times$). Notably, the fine-grained
    kernel is significantly slower when compared to FP16 at larger batch sizes, making
    it less practical for deployment. Further analysis confirms that fine-grained
    approaches inherently require numerous costly type conversions. The result of
    each integer matrix multiplication has to be converted to float precision to multiply
    the corresponding float scale, as depicted in Figure [2](#S2.F2 "Figure 2 ‣ 2.2
    LLM Quantization Algorithms ‣ 2 Related Work ‣ Integer Scale: A Free Lunch for
    Faster Fine-grained Quantization of LLMs") (b).'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管细粒度量化可以实现更高的准确性，如在[[24](#bib.bib24)]中所示，但我们发现它在推理过程中特别缓慢，这也在双粒度量化（DGQ）方法[[51](#bib.bib51)]中有所提及。使用更低位宽的优势往往被它们带来的计算开销所抵消。图[3](#S3.F3
    "Figure 3 ‣ 3.2 细粒度量化受推理瓶颈影响 ‣ 3 动机 ‣ 整数缩放：快速细粒度量化的免费午餐")比较了典型推理批次大小下的内核延迟（下降了3.15$\times$）。值得注意的是，与FP16相比，细粒度内核在较大批次大小下显著更慢，使其在部署中不够实用。进一步分析确认，细粒度方法本质上需要大量成本高昂的类型转换。每次整数矩阵乘法的结果必须转换为浮点精度，以乘以相应的浮点缩放，如图[2](#S2.F2
    "Figure 2 ‣ 2.2 LLM量化算法 ‣ 2 相关工作 ‣ 整数缩放：快速细粒度量化的免费午餐")（b）所示。
- en: The intrinsic computation drawbacks disable its use in practice. This incoherent
    situation calls for a novel fine-grained scheme that is both computationally efficient
    and accuracy-retaining.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 内在的计算缺陷使得该方法在实际中无法使用。这种不一致的情况呼唤一种既计算高效又能保持准确度的新型细粒度方案。
- en: 4 Method
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: 4.1 Integer Scale with Adaptive Scale Amplifier
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 自适应缩放放大器的整数缩放
- en: 'Motivated by the previous discussion, it is then critical to boost the fine-grained
    inference. Figure [2](#S2.F2 "Figure 2 ‣ 2.2 LLM Quantization Algorithms ‣ 2 Related
    Work ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs")
    (b) has shown that using float scale triggers numerous costly type conversions.
    For instance, a typical Dense layer of size 4096$\times$, called *adaptive scale
    amplifier*, which can be easily computed based on the available float scales.
    Our method is put formally as,'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 受前述讨论的启发，提升细粒度推理是至关重要的。图[2](#S2.F2 "Figure 2 ‣ 2.2 LLM量化算法 ‣ 2 相关工作 ‣ 整数缩放：快速细粒度量化的免费午餐")（b）表明，使用浮点缩放会引发大量成本高昂的类型转换。例如，典型的4096$\times$尺寸的密集层，被称为*自适应缩放放大器*，可以基于现有的浮点缩放轻松计算。我们的方法正式描述为，
- en: '|  | $1$2 |  | (2) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'To find the common amplifier, we use a heuristic search algorithm that starts
    from $2^{0}$ that guarantees amplified scales to be bigger than 1, see Listing [1](#LST1
    "Listing 1 ‣ 4.1 Integer Scale with Adaptive Scale Amplifier ‣ 4 Method ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到通用放大器，我们使用了一个启发式搜索算法，从$2^{0}$开始，确保放大后的尺度大于1，参见列表[1](#LST1 "Listing 1 ‣ 4.1
    自适应缩放放大器的整数缩放 ‣ 4 方法 ‣ 整数缩放：快速细粒度量化的免费午餐")。
- en: 1scale_min  =  scales.min()2n,  tmp  =  0,  scale_min3while  tmp  <  1:4  tmp  =  scale_min  *  (2**n)5  n+=16scale_amplifier  =  2**(n-1)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 1scale_min  =  scales.min()2n,  tmp  =  0,  scale_min3while  tmp  <  1:4  tmp  =  scale_min  *  (2**n)5  n+=16scale_amplifier  =  2**(n-1)
- en: 'Listing 1: Quick Heuristic Search for Integer Scale Amplifier'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1：整数缩放放大器的快速启发式搜索
- en: 'Ideally, we can use the above heuristic method to find the optimal amplifier
    per layer. However, based on the scale analysis of LLaMA-2-7B in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1 Integer Scale with Adaptive Scale Amplifier ‣ 4 Method ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") (a,b,c), we
    find that the number of bit shifts required to amplify the scale mainly falls
    to 9 or 10\. The weight MSE when using an amplifier of $2^{10}$ as our default
    amplifier to avoid possible overflow as the later ablation (Table [7](#S6.T7 "Table
    7 ‣ 6.1 Fixed Amplifier vs. Heuristic Search ‣ 6 Ablation Study ‣ Integer Scale:
    A Free Lunch for Faster Fine-grained Quantization of LLMs")) shows a bigger amplifier
    has no clear gains.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，我们可以使用上述启发式方法找到每层的最佳放大器。然而，根据图 [4](#S4.F4 "图 4 ‣ 4.1 带自适应放大器的整数缩放 ‣ 4
    方法 ‣ 整数缩放：更快细粒度量化的免费午餐") (a,b,c) 中对 LLaMA-2-7B 的规模分析，我们发现放大规模所需的位移数主要集中在 9 或 10。使用
    $2^{10}$ 作为默认放大器以避免可能的溢出（稍后的消融研究（表 [7](#S6.T7 "表 7 ‣ 6.1 固定放大器 vs. 启发式搜索 ‣ 6 消融研究
    ‣ 整数缩放：更快细粒度量化的免费午餐")）显示更大的放大器没有明显的收益。
- en: '![Refer to caption](img/db617ecb62ce258b69d9bbb41f2227ed.png)![Refer to caption](img/ec1c1dc2c8ef353dc5fa7614568d6c43.png)![Refer
    to caption](img/4e3eece6818f037c2cf8c11bea70b52b.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/db617ecb62ce258b69d9bbb41f2227ed.png)![参见标题](img/ec1c1dc2c8ef353dc5fa7614568d6c43.png)![参见标题](img/4e3eece6818f037c2cf8c11bea70b52b.png)'
- en: 'Figure 4: (a) The range of amplified ($\alpha=2^{10}$) float scales of LLaMA-2-7B
    in the first layer (others are similar) mapped to 16-bit integers. The majority
    of amplified scales can be represented within 8 bits. (b) The number of bit shifts
    required to amplify scales per linear layer. (c) Weight MSE between integer scale
    and float scale under different amplifiers.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: (a) LLaMA-2-7B 在第一层的放大 ($\alpha=2^{10}$) 浮点缩放范围（其他层类似），映射到 16 位整数。大多数放大尺度可以在
    8 位内表示。 (b) 每个线性层放大尺度所需的位移数。 (c) 在不同放大器下，整数尺度与浮点尺度之间的权重 MSE。'
- en: 4.2 Kernel Implementation
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 内核实现
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4.2 Kernel Implementation ‣ 4 Method ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") illustrates
    the difference in typical kernels. Current hardware supports a standard MatMul
    GEMM which isn’t suited for fine-grained approaches. Each group of $A_{i}$.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [2](#S4.T2 "表 2 ‣ 4.2 内核实现 ‣ 4 方法 ‣ 整数缩放：更快细粒度量化的免费午餐") 说明了典型内核之间的差异。当前硬件支持标准的
    MatMul GEMM，这不适合细粒度的方法。每组 $A_{i}$。
- en: 'Table 2: Comparison of kernel computation logic.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 内核计算逻辑比较。'
- en: '| MatMul | Atom | Ours |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| MatMul | Atom | 我们的方法 |'
- en: '| --- | --- | --- |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $C_{1}=A_{1}*W_{1}+C_{0}$ |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| $C_{1}=A_{1}*W_{1}+C_{0}$ |'
- en: '| $C_{2}=A_{2}*W_{2}+C_{1}$ |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| $C_{2}=A_{2}*W_{2}+C_{1}$ |'
- en: '| $\cdots$ |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| $\cdots$ |'
- en: 'We also present our computation strategy in Figure [2](#S2.F2 "Figure 2 ‣ 2.2
    LLM Quantization Algorithms ‣ 2 Related Work ‣ Integer Scale: A Free Lunch for
    Faster Fine-grained Quantization of LLMs") (c). Since the result of group-wise
    weight and activation matrix multiplication (e.g., $x_{00}\times g_{00}$, executed
    with integer tensor cores) becomes INT32, we only need to convert the amplified
    scale to INT32 offline. Each group is then accumulated to have the final result.
    The large number of type conversions on the matrix is thus reduced to only once
    for activation dequantization. Besides, we exploit the efficient weight processing
    and kernel fusion technique of OdysseyLLM’s FastGEMM  [[23](#bib.bib23)] for fast
    inference. The combination makes fine-grained kernels substantially efficient,
    enabling fine-grained approaches as a feasible solution.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在图 [2](#S2.F2 "图 2 ‣ 2.2 LLM 量化算法 ‣ 2 相关工作 ‣ 整数缩放：更快细粒度量化的免费午餐") (c) 中展示了我们的计算策略。由于按组进行的权重和激活矩阵乘法的结果（例如，$x_{00}\times
    g_{00}$，使用整数张量核心执行）变为 INT32，我们只需要离线将放大缩放转换为 INT32。每组的结果随后被累加以得到最终结果。因此，矩阵上的类型转换次数减少为仅一次，用于激活去量化。此外，我们利用
    OdysseyLLM 的 FastGEMM [[23](#bib.bib23)] 的高效权重处理和内核融合技术进行快速推理。这种组合使得细粒度内核显著高效，使细粒度方法成为一种可行的解决方案。
- en: 5 Experiments
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 5.1 Setup
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: Models and Datasets. We benchmark Integer Scale and other state-of-the-art quantization
    methods on the well-known LLaMA-2 [[44](#bib.bib44)] and LLaMA-3 [[1](#bib.bib1)]
    models and Mixtral 8x7B [[18](#bib.bib18)]. Several datasets are used for evaluation,
    including LAMBADA [[36](#bib.bib36)], C4 [[38](#bib.bib38)], WikiText-2 [[29](#bib.bib29)],
    MMLU [[16](#bib.bib16)], and a set of Common Sense QA [[42](#bib.bib42)] tasks
    like WinoGrande [[39](#bib.bib39)], PIQA [[43](#bib.bib43)], HellaSwag [[50](#bib.bib50)],
    ARC[e]. For CommonSense QA tasks, we utilized the Language Model Evaluation Harness
     [[15](#bib.bib15)] tool.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。我们在著名的 LLaMA-2 [[44](#bib.bib44)] 和 LLaMA-3 [[1](#bib.bib1)] 模型以及 Mixtral
    8x7B [[18](#bib.bib18)] 上对 Integer Scale 和其他最先进的量化方法进行了基准测试。使用了多个数据集进行评估，包括 LAMBADA
    [[36](#bib.bib36)]、C4 [[38](#bib.bib38)]、WikiText-2 [[29](#bib.bib29)]、MMLU [[16](#bib.bib16)]
    和一组常识 QA [[42](#bib.bib42)] 任务，如 WinoGrande [[39](#bib.bib39)]、PIQA [[43](#bib.bib43)]、HellaSwag
    [[50](#bib.bib50)]、ARC[e]。对于常识 QA 任务，我们使用了 Language Model Evaluation Harness [[15](#bib.bib15)]
    工具。
- en: Inference Framework. We adopt an end-to-end inference pipeline with cutlass [[32](#bib.bib32)]
    that mainly profits GPU Tensor Core execution, kernel fusion policies, and graph
    optimization. Unless otherwise notified, we use the same framework for fair comparisons.
    Note for LLaMA models with W4A16, we use Marlin [[13](#bib.bib13)] for inference
    as it claims to be the fastest available framework. For Mixtral 8x7B, we had to
    use our W4A16 implementation as Marlin hasn’t supported it yet. The latency is
    tested on a single NVIDIA A100 GPU, except for LLaMA-2-70B and Mistral 8x7B we
    use four such GPUs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 推理框架。我们采用了一个端到端的推理管道，使用了 cutlass [[32](#bib.bib32)]，该框架主要利用 GPU Tensor Core
    执行、内核融合策略和图优化。除非另有通知，我们将使用相同的框架以确保公平比较。关于具有 W4A16 的 LLaMA 模型，我们使用 Marlin [[13](#bib.bib13)]
    进行推理，因为它声称是最快的可用框架。对于 Mixtral 8x7B，我们不得不使用我们自己的 W4A16 实现，因为 Marlin 尚未支持它。除 LLaMA-2-70B
    和 Mistral 8x7B 外，我们在单个 NVIDIA A100 GPU 上测试延迟，对于这两个模型，我们使用了四个这样的 GPU。
- en: Baselines. In our experiments, we choose GPTQ [[14](#bib.bib14)], AWQ [[26](#bib.bib26)],
    and Omniquant  [[41](#bib.bib41)] as our baselines, given that they are the most
    prevalent fine-grained quantization schemes. Throughout the paper, we adopt per-token
    activation quantization, and per-channel weight quantization by default.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线。在我们的实验中，我们选择了 GPTQ [[14](#bib.bib14)]、AWQ [[26](#bib.bib26)] 和 Omniquant
    [[41](#bib.bib41)] 作为基准线，因为它们是最常见的细粒度量化方案。在整篇文章中，我们默认采用每个 token 激活量化和每个通道权重量化。
- en: 5.2 Experiment Result on LAMBADA, C4, and WikiText-2
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 LAMBADA、C4 和 WikiText-2 的实验结果
- en: 'Table [3](#S5.T3 "Table 3 ‣ 5.2 Experiment Result on LAMBADA, C4, and WikiText-2
    ‣ 5 Experiments ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") exhibits the quantization result of LLaMA-2 and Mixtral models when
    applying Integer Scale (IS) to GPTQ [[14](#bib.bib14)], AWQ [[26](#bib.bib26)],
    and Omniquant [[41](#bib.bib41)] on LAMBADA, WikiText-2, and C4 datasets. Our
    approach generally shows on-par or better performance, indicating that the Integer
    Scale applies to the existing quantization methods and retains the quantized performance
    at lower bits like W4A8\. Note since Ominiquant on LLaMA-2-70B originally fails,
    so does its integer scale variation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S5.T3 "Table 3 ‣ 5.2 Experiment Result on LAMBADA, C4, and WikiText-2
    ‣ 5 Experiments ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") 显示了在 LAMBADA、WikiText-2 和 C4 数据集上，应用 Integer Scale (IS) 到 GPTQ [[14](#bib.bib14)]、AWQ
    [[26](#bib.bib26)] 和 Omniquant [[41](#bib.bib41)] 时，LLaMA-2 和 Mixtral 模型的量化结果。我们的方法通常表现出相当或更好的性能，表明
    Integer Scale 适用于现有的量化方法，并在较低位如 W4A8 中保持了量化性能。注意，由于 Ominiquant 在 LLaMA-2-70B 上原本失败，其整数尺度变体也一样。'
- en: 'Table 3: Comparison with state-of-the-art quantization methods on LAMBADA (accuracy),
    C4 (PPL), and WikiText (PPL). For all models tested, we set the weight group size
    to 128 and apply symmetric quantization. Integer Scale (IS) with amplifier 1024
    is used.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：在 LAMBADA（准确度）、C4（PPL）和 WikiText（PPL）上与最先进量化方法的比较。对于所有测试的模型，我们将权重组大小设置为
    128 并应用对称量化。使用了带有放大器 1024 的 Integer Scale (IS)。
- en: '| Dataset | HyperParam |  | LLaMA-2 | Mixtral |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 超参数 |  | LLaMA-2 | Mixtral |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|  | Method | BitWidth |  | 7B | 13B | 70B | 8x7B |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 位宽 |  | 7B | 13B | 70B | 8x7B |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LAMBADA | FP16 | W16A16 |  | 73.70% | 76.64% | 79.57% | 77.62% |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LAMBADA | FP16 | W16A16 |  | 73.70% | 76.64% | 79.57% | 77.62% |'
- en: '| GPTQ | W4A8 |  | 71.65% | 75.88% | 78.54% | 73.89% |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 |  | 71.65% | 75.88% | 78.54% | 73.89% |'
- en: '| GPTQ w/ IS | W4A8 |  | 71.66% [+0.01] | 75.39% [-0.48] | 78.67% [+0.13] |
    73.93% [+0.03] |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 |  | 71.66% [+0.01] | 75.39% [-0.48] | 78.67% [+0.13] |
    73.93% [+0.03] |'
- en: '| AWQ | W4A8 |  | 70.15% | 75.47% | 78.48% | 76.24% |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 |  | 70.15% | 75.47% | 78.48% | 76.24% |'
- en: '| AWQ w/ IS | W4A8 |  | 70.07% [-0.07] | 75.02% [-0.44] | 78.42% [-0.05] |
    74.30% [-1.94] |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 |  | 70.07% [-0.07] | 75.02% [-0.44] | 78.42% [-0.05] |
    74.30% [-1.94] |'
- en: '| Omniquant | W4A8 |  | 71.76% | 75.98% | NaN | 76.09% |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 |  | 71.76% | 75.98% | NaN | 76.09% |'
- en: '| Omniquant w/ IS | W4A8 |  | 70.91% [-0.85] | 75.60% [-0.36] | NaN | 76.01%
    [-0.07] |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 |  | 70.91% [-0.85] | 75.60% [-0.36] | NaN | 76.01%
    [-0.07] |'
- en: '| WikiText-2 | FP16 | W16A16 |  | 5.65 | 4.95 | 3.36 | 3.93 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| WikiText-2 | FP16 | W16A16 |  | 5.65 | 4.95 | 3.36 | 3.93 |'
- en: '| GPTQ | W4A8 |  | 12.32 | 5.16 | 3.66 | 4.51 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 |  | 12.32 | 5.16 | 3.66 | 4.51 |'
- en: '| GPTQ w/ IS | W4A8 |  | 13.13 [+0.81] | 5.18 [+0.02] | 3.69 [+0.03] | 4.59
    [+0.08] |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 |  | 13.13 [+0.81] | 5.18 [+0.02] | 3.69 [+0.03] | 4.59
    [+0.08] |'
- en: '| AWQ | W4A8 |  | 6.12 | 5.27 | 3.66 | 4.30 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 |  | 6.12 | 5.27 | 3.66 | 4.30 |'
- en: '| AWQ w/ IS | W4A8 |  | 6.19 [+0.07] | 5.30 [+0.03] | 3.70 [+0.04] | 4.42 [+0.12]
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 |  | 6.19 [+0.07] | 5.30 [+0.03] | 3.70 [+0.04] | 4.42 [+0.12]
    |'
- en: '| Omniquant | W4A8 |  | 5.94 | 5.16 | NaN | 4.27 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 |  | 5.94 | 5.16 | NaN | 4.27 |'
- en: '| Omniquant w/ IS | W4A8 |  | 5.97 [+0.03] | 5.17 [+0.01] | NaN | 4.36 [+0.09]
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 |  | 5.97 [+0.03] | 5.17 [+0.01] | NaN | 4.36 [+0.09]
    |'
- en: '| C4 | FP16 | W16A16 |  | 7.05 | 6.46 | 5.52 | 6.88 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| C4 | FP16 | W16A16 |  | 7.05 | 6.46 | 5.52 | 6.88 |'
- en: '| GPTQ | W4A8 |  | 39.96 | 6.66 | 5.75 | 7.31 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 |  | 39.96 | 6.66 | 5.75 | 7.31 |'
- en: '| GPTQ w/ IS | W4A8 |  | 37.25 [+2.71] | 6.68 [+0.02] | 5.78 [+0.03] | 7.39
    [+0.08] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 |  | 37.25 [+2.71] | 6.68 [+0.02] | 5.78 [+0.03] | 7.39
    [+0.08] |'
- en: '| AWQ | W4A8 |  | 7.57 | 6.79 | 5.73 | 7.15 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 |  | 7.57 | 6.79 | 5.73 | 7.15 |'
- en: '| AWQ w/ IS | W4A8 |  | 7.64 [+0.07] | 6.83 [+0.04] | 5.76 [+0.03] | 7.27 [+0.12]
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 |  | 7.64 [+0.07] | 6.83 [+0.04] | 5.76 [+0.03] | 7.27 [+0.12]
    |'
- en: '| Omniquant | W4A8 |  | 7.41 | 6.65 | NaN | 7.12 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 |  | 7.41 | 6.65 | NaN | 7.12 |'
- en: '| Omniquant w/ IS | W4A8 |  | 7.44 [+0.03] | 6.67 [+0.02] | NaN | 7.21 [+0.09]
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 |  | 7.44 [+0.03] | 6.67 [+0.02] | NaN | 7.21 [+0.09]
    |'
- en: 5.3 Experiment Result on Common Sense QA
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 在常识问答上的实验结果
- en: 'Table [4](#S5.T4 "Table 4 ‣ 5.3 Experiment Result on Common Sense QA ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") compares
    the Common Sense QA [[42](#bib.bib42)] result of applying the Integer Scale on
    state-of-the-art quantization approaches. A similar conclusion to Section [5.2](#S5.SS2
    "5.2 Experiment Result on LAMBADA, C4, and WikiText-2 ‣ 5 Experiments ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") can be reached.
    More results on MMLU [[16](#bib.bib16)] can be found in Table [8](#A2.T8 "Table
    8 ‣ B.1 Experiment Result on MMLU ‣ Appendix B Additional Discussions ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") in Section [B](#A2
    "Appendix B Additional Discussions ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs").'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4](#S5.T4 "表 4 ‣ 5.3 在常识问答上的实验结果 ‣ 5 实验 ‣ 整数尺度：更快的细粒度量化的免费午餐") 比较了应用整数尺度在最先进量化方法上的常识问答 [[42](#bib.bib42)]
    结果。可以得出与第 [5.2](#S5.SS2 "5.2 在LAMBADA、C4和WikiText-2上的实验结果 ‣ 5 实验 ‣ 整数尺度：更快的细粒度量化的免费午餐")节类似的结论。更多MMLU [[16](#bib.bib16)]的结果见第 [8](#A2.T8
    "表 8 ‣ B.1 在MMLU上的实验结果 ‣ 附录B 额外讨论 ‣ 整数尺度：更快的细粒度量化的免费午餐")表和[B](#A2 "附录B 额外讨论 ‣
    整数尺度：更快的细粒度量化的免费午餐")节。
- en: 'Table 4: Comparison with state-of-the-art quantization methods on Common Sense
    QA. For all models tested, we set the weight group size to 128 and apply symmetric
    quantization. Integer Scale (IS) with amplifier 1024 is used.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：与当前最先进量化方法在常识问答上的比较。对于所有测试的模型，我们设置权重组大小为128并应用对称量化。使用了放大器为1024的整数尺度（IS）。
- en: '| Model | HyperParam | Common Sense QA |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 超参数 | 常识问答 |'
- en: '| --- | --- | --- |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '|  | Method | BitWidth | WinoGrande | PIQA | HellaSwag | ARC_e | Avg |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 位宽 | WinoGrande | PIQA | HellaSwag | ARC_e | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-2-7B | FP16 | W16A16 | 0.6906 | 0.7911 | 0.7598 | 0.7458 | 0.7468 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | FP16 | W16A16 | 0.6906 | 0.7911 | 0.7598 | 0.7458 | 0.7468 |'
- en: '| GPTQ | W4A8 | 0.6819 | 0.7829 | 0.7380 | 0.6961 | 0.7247 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 | 0.6819 | 0.7829 | 0.7380 | 0.6961 | 0.7247 |'
- en: '| GPTQ w/ IS | W4A8 | 0.6882 | 0.7845 | 0.7359 | 0.6932 | 0.7255 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 | 0.6882 | 0.7845 | 0.7359 | 0.6932 | 0.7255 |'
- en: '| AWQ | W4A8 | 0.6890 | 0.7807 | 0.7418 | 0.6856 | 0.7243 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 | 0.6890 | 0.7807 | 0.7418 | 0.6856 | 0.7243 |'
- en: '| AWQ w/ IS | W4A8 | 0.6803 | 0.7818 | 0.7399 | 0.6717 | 0.7184 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 | 0.6803 | 0.7818 | 0.7399 | 0.6717 | 0.7184 |'
- en: '| Omniquant | W4A8 | 0.6930 | 0.7873 | 0.7427 | 0.6890 | 0.7280 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 | 0.6930 | 0.7873 | 0.7427 | 0.6890 | 0.7280 |'
- en: '| Omniquant w/ IS | W4A8 | 0.6882 | 0.7818 | 0.7393 | 0.6898 | 0.7248 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 | 0.6882 | 0.7818 | 0.7393 | 0.6898 | 0.7248 |'
- en: '| LLaMA-2-13B | FP16 | W16A16 | 0.7222 | 0.8052 | 0.7938 | 0.7744 | 0.7739
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | FP16 | W16A16 | 0.7222 | 0.8052 | 0.7938 | 0.7744 | 0.7739
    |'
- en: '| GPTQ | W4A8 | 0.7080 | 0.8003 | 0.7858 | 0.7980 | 0.773 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 | 0.7080 | 0.8003 | 0.7858 | 0.7980 | 0.773 |'
- en: '| GPTQ w/ IS | W4A8 | 0.7040 | 0.8025 | 0.7854 | 0.7917 | 0.7709 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 | 0.7040 | 0.8025 | 0.7854 | 0.7917 | 0.7709 |'
- en: '| AWQ | W4A8 | 0.7182 | 0.7976 | 0.7758 | 0.7677 | 0.7648 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 | 0.7182 | 0.7976 | 0.7758 | 0.7677 | 0.7648 |'
- en: '| AWQ w/ IS | W4A8 | 0.7246 | 0.7992 | 0.7734 | 0.7668 | 0.7660 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 | 0.7246 | 0.7992 | 0.7734 | 0.7668 | 0.7660 |'
- en: '| Omniquant | W4A8 | 0.7214 | 0.7992 | 0.7810 | 0.7710 | 0.7682 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 | 0.7214 | 0.7992 | 0.7810 | 0.7710 | 0.7682 |'
- en: '| Omniquant w/ IS | W4A8 | 0.7127 | 0.7954 | 0.7786 | 0.7715 | 0.7646 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 | 0.7127 | 0.7954 | 0.7786 | 0.7715 | 0.7646 |'
- en: '| LLaMA-2-70B | FP16 | W16A16 | 0.7798 | 0.8275 | 0.8381 | 0.8098 | 0.8138
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B | FP16 | W16A16 | 0.7798 | 0.8275 | 0.8381 | 0.8098 | 0.8138
    |'
- en: '| GPTQ | W4A8 | 0.7664 | 0.8313 | 0.8314 | 0.8131 | 0.8106 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 | 0.7664 | 0.8313 | 0.8314 | 0.8131 | 0.8106 |'
- en: '| GPTQ w/ IS | W4A8 | 0.7585 | 0.8324 | 0.8287 | 0.8077 | 0.8068 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 | 0.7585 | 0.8324 | 0.8287 | 0.8077 | 0.8068 |'
- en: '| AWQ | W4A8 | 0.7664 | 0.8194 | 0.8202 | 0.8005 | 0.8016 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 | 0.7664 | 0.8194 | 0.8202 | 0.8005 | 0.8016 |'
- en: '| AWQ w/ IS | W4A8 | 0.7624 | 0.8199 | 0.8218 | 0.7929 | 0.7993 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 | 0.7624 | 0.8199 | 0.8218 | 0.7929 | 0.7993 |'
- en: '| Mixtral-8x7B | FP16 | W16A16 | 0.7648 | 0.8368 | 0.8403 | 0.835 | 0.8192
    |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | FP16 | W16A16 | 0.7648 | 0.8368 | 0.8403 | 0.835 | 0.8192
    |'
- en: '| GPTQ | W4A8 | 0.7553 | 0.8161 | 0.8272 | 0.8056 | 0.8011 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 | 0.7553 | 0.8161 | 0.8272 | 0.8056 | 0.8011 |'
- en: '| GPTQ w/ IS | W4A8 | 0.7427 | 0.8145 | 0.8280 | 0.7925 | 0.7944 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 | 0.7427 | 0.8145 | 0.8280 | 0.7925 | 0.7944 |'
- en: '| AWQ | W4A8 | 0.7506 | 0.8341 | 0.8288 | 0.8228 | 0.8091 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 | 0.7506 | 0.8341 | 0.8288 | 0.8228 | 0.8091 |'
- en: '| AWQ w/ IS | W4A8 | 0.7443 | 0.8286 | 0.8252 | 0.8131 | 0.8028 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 | 0.7443 | 0.8286 | 0.8252 | 0.8131 | 0.8028 |'
- en: '| Omniquant | W4A8 | 0.7553 | 0.8308 | 0.8338 | 0.8165 | 0.8091 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 | 0.7553 | 0.8308 | 0.8338 | 0.8165 | 0.8091 |'
- en: '| Omniquant w/ IS | W4A8 | 0.7506 | 0.8308 | 0.8337 | 0.8178 | 0.8082 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 | 0.7506 | 0.8308 | 0.8337 | 0.8178 | 0.8082 |'
- en: 5.4 W4A8 Kernel Latency Comparison
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 W4A8 内核延迟比较
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") (a)
    illustrates the comparison of kernel implementations under various bandwidths.
    Marlin [[13](#bib.bib13)] ships so far the most advanced W4A16 kernel implementation.
    Odyssey’s W4A8 scheme largely benefits its specific FastGEMM and has the optimal
    acceleration ratio over FP16\. It can be seen that fine-grained W4A8 with integer
    scale becomes a feasible scheme between W4A16 and non-fine-grained W4A8 to have
    better accuracy. Interestingly, we discover a “performance cliff” (gray-colored)
    where the acceleration ratio suddenly drops when it transits from memory-bound
    to compute-bound scenarios. This is due to the sudden drop from the ideal 4$\times$
    vs. FP16.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S5.F5 "图 5 ‣ 5.4 W4A8 内核延迟比较 ‣ 5 实验 ‣ 整数尺度：更快细粒度量化的免费午餐") (a) 说明了在不同带宽下内核实现的比较。Marlin [[13](#bib.bib13)]
    目前推出了最先进的 W4A16 内核实现。Odyssey 的 W4A8 方案在其特定的 FastGEMM 中获得了极大的好处，并且在 FP16 上具有最佳的加速比。可以看出，具有整数尺度的细粒度
    W4A8 成为 W4A16 和非细粒度 W4A8 之间的一个可行方案，能实现更好的精度。有趣的是，我们发现了一个“性能悬崖”（灰色区域），即在从内存绑定到计算绑定场景过渡时，加速比突然下降。这是由于从理想的
    4$\times$ 到 FP16 的突然下降。
- en: '![Refer to caption](img/a1db0a2fa61741afa72176913e29491f.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a1db0a2fa61741afa72176913e29491f.png)'
- en: (a) Kernel Acceleration
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 内核加速
- en: '![Refer to caption](img/3a3269e898de57a881fa2ca276b24c26.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3a3269e898de57a881fa2ca276b24c26.png)'
- en: (b) Mistral 8x7B Speedup
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: (b) Mistral 8x7B 加速
- en: 'Figure 5: (a) Fine-grained W4A8 kernel (K=4096, N=22016) with the integer scale
    (W4A8 Integer Scale) boosts its float scale counterpart (W4A8 Float Scale). The
    gray region denotes the “performance cliff”. (b) End-to-end speed boost on Mixtral
    8x7B over FP16 under various batch sizes.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: (a) 细粒度 W4A8 内核 (K=4096, N=22016) 与整数尺度 (W4A8 整数尺度) 相比，提升了其浮点尺度对应物 (W4A8
    浮点尺度)。灰色区域表示“性能悬崖”。 (b) 在不同批量大小下，Mixtral 8x7B 相对 FP16 的端到端加速。'
- en: 5.5 Speed boost on Mixture-of-experts
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 Mixture-of-experts 的加速
- en: 'Figure [5](#S5.F5 "Figure 5 ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") (c)
    shows the end-to-end latency of the W4A8 Integer Scale scheme applied to the Mixtral
    8$\times$ boost, compared with FP16 and W4A16 respectively.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S5.F5 "图 5 ‣ 5.4 W4A8 内核延迟比较 ‣ 5 实验 ‣ 整数尺度：更快细粒度量化的免费午餐") (c) 显示了将 W4A8
    整数尺度方案应用于 Mixtral 8$\times$ 提升后的端到端延迟，与 FP16 和 W4A16 分别比较。
- en: 5.6 Our Quantization Recipe for LLaMA-3
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 我们的 LLaMA-3 量化配方
- en: 'Table 5: Our LLaMA-3 Integer Scale recipe.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：我们的LLaMA-3整数尺度配方。
- en: '| Model | BitWidth | $\alpha$ | Group | C4 | WikiText-2 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位宽 | $\alpha$ | 组 | C4 | WikiText-2 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA-3-8B | W4A8 | - | 128 | 9.331 | 6.352 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-8B | W4A8 | - | 128 | 9.331 | 6.352 |'
- en: '| LLaMA-3-8B | W4A8 | 8192 | 128 | 9.379 | 6.382 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-8B | W4A8 | 8192 | 128 | 9.379 | 6.382 |'
- en: '| LLaMA-3-70B | W4A8 | - | 128 | 7.061 | 3.280 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-70B | W4A8 | - | 128 | 7.061 | 3.280 |'
- en: '| LLaMA-3-70B | W4A8 | 8192 | 128 | 7.092 | 3.312 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-3-70B | W4A8 | 8192 | 128 | 7.092 | 3.312 |'
- en: 'LLaMA-3 is difficult to quantize at lower bits compared with its predecessors,
    as confirmed in  [[17](#bib.bib17)]. To counter the problem, we apply QuaRot [[3](#bib.bib3)]
    with a fine-grained paradigm. We adopt 8-bit per-token activation quantization
    and 4-bit per-channel symmetric weight quantization with a group size of 128\.
    Besides, we use fine-grained W8A8 for down projection layers following the observation
    in  [[24](#bib.bib24)]. Table [5](#S5.T5 "Table 5 ‣ 5.6 Our Quantization Recipe
    for LLaMA-3 ‣ 5 Experiments ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs") exhibits the result of our LLaMA-3 scheme, while integer
    scale outperforms GPTQ’s W4A16 (-1.16% in C4 perplexity) shown in Table [1](#S3.T1
    "Table 1 ‣ 3.1 Fine Granularity Strengthens Current Quantization Approaches ‣
    3 Motivation ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs").'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '与其前身相比，LLaMA-3在低位宽下量化较为困难，这一点在[[17](#bib.bib17)]中得到了确认。为了解决这个问题，我们采用了QuaRot[[3](#bib.bib3)]，并使用了细粒度范式。我们采用了每个标记8位激活量化和每个通道4位对称权重量化，组大小为128。此外，我们在降维层中使用了细粒度的W8A8，依据了[[24](#bib.bib24)]中的观察结果。表[5](#S5.T5
    "Table 5 ‣ 5.6 Our Quantization Recipe for LLaMA-3 ‣ 5 Experiments ‣ Integer Scale:
    A Free Lunch for Faster Fine-grained Quantization of LLMs")展示了我们LLaMA-3方案的结果，而整数尺度在表[1](#S3.T1
    "Table 1 ‣ 3.1 Fine Granularity Strengthens Current Quantization Approaches ‣
    3 Motivation ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs")中显示比GPTQ的W4A16 (-1.16%在C4困惑度)表现更好。'
- en: 5.7 Comparison with Marlin’s W4A16 Scheme
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 与Marlin的W4A16方案比较
- en: 'Table 6: C4 and WikiText-2 perplexity, and MMLU zero-shot accuracy of LLaMA-2-7B
    quantized with Marlin’s implementation of GPTQ (W4A16) vs. GPTQ w/ Integer Scale
    (W4A8).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：用Marlin的GPTQ实现（W4A16）与GPTQ的整数尺度（W4A8）量化的LLaMA-2-7B的C4和WikiText-2困惑度以及MMLU零样本准确率。
- en: '| Method | BitWidth | C4 | WikiText-2 | MMLU |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位宽 | C4 | WikiText-2 | MMLU |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| GPTQ | W4A16 | 7.2093 | 5.8212 | 39.11% |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A16 | 7.2093 | 5.8212 | 39.11% |'
- en: '| GPTQ w/ Integer Scale | W4A8 | 7.4011 | 5.9433 | 38.54% |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ Integer Scale | W4A8 | 7.4011 | 5.9433 | 38.54% |'
- en: 'We compare our Integer Scale scheme with Marlin’s implementation of GPTQ [[13](#bib.bib13)]
    in Table [6](#S5.T6 "Table 6 ‣ 5.7 Comparison with Marlin’s W4A16 Scheme ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs").
    We are mostly on par with GPTQ at W4A16 when tested on C4, WikiText-2, and MMLU.
    Their acceleration ratios vs. FP16 are compared in Figure [5](#S5.F5 "Figure 5
    ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments ‣ Integer Scale: A Free Lunch
    for Faster Fine-grained Quantization of LLMs") where W4A8 surpasses W4A16 mainly
    due to faster tensor core execution at lower bit widths. This attests that fine-grained
    W4A8 with the Integer Scale is a competitive strategy in terms of both quantization
    loss and speed.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[6](#S5.T6 "Table 6 ‣ 5.7 Comparison with Marlin’s W4A16 Scheme ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs")中将我们的整数尺度方案与Marlin的GPTQ实现[[13](#bib.bib13)]进行了比较。与GPTQ的W4A16相比，我们在C4、WikiText-2和MMLU上的表现基本持平。它们的加速比与FP16的比较见图[5](#S5.F5
    "Figure 5 ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments ‣ Integer Scale:
    A Free Lunch for Faster Fine-grained Quantization of LLMs")，其中W4A8由于在较低位宽下张量核心执行速度更快，超越了W4A16。这证明了细粒度的W4A8与整数尺度在量化损失和速度方面都是一种具有竞争力的策略。'
- en: 5.8 Comparison with QServe’s W4A8 Kernel
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.8 与QServe的W4A8内核比较
- en: 'Figure [6](#S5.F6 "Figure 6 ‣ 5.8 Comparison with QServe’s W4A8 Kernel ‣ 5
    Experiments ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") presents the kernel speed comparison with QServe [[27](#bib.bib27)],
    which ships an advanced W4A8 kernel. For coarse-grained W4A8 kernel with M=1,
    K=4096, and N=22016, our W4A8 kernel execution is substantially faster than QServe
    at all batch sizes. A similar conclusion is affirmed for the fine-grained kernel
    at a typical group size of 128\. Both being the same bit widths, our fine-grained
    kernel with Integer Scale is substantially faster than QServe’s, with a maximum
    of being 1.53$\times$. It turns out that the main difference lies in the intrinsic
    complexity of Dual Quantization [[51](#bib.bib51)] they adopted which first quantizes
    weights in 8-bit and again in 4-bit. Note the second step is kept asymmetric to
    counter quantization loss. This *asymmetric* scheme requires element-wise multiplication
    and subtraction that must be done in costly CUDA cores. See more details in [B.2](#A2.SS2
    "B.2 More Discussion with QServe ‣ Appendix B Additional Discussions ‣ Integer
    Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs").'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [6](#S5.F6 "Figure 6 ‣ 5.8 Comparison with QServe’s W4A8 Kernel ‣ 5 Experiments
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs") 展示了与
    QServe [[27](#bib.bib27)] 的 W4A8 核心的速度比较，QServe 提供了一种先进的 W4A8 核心。对于粗粒度的 W4A8 核心，M=1，K=4096
    和 N=22016，我们的 W4A8 核心在所有批量大小下的执行速度都明显快于 QServe。对于典型的组大小为 128 的细粒度核心，也得出了类似的结论。两者位宽相同，我们的细粒度核心在整数尺度下比
    QServe 的核心快了最大 1.53$\times$。结果表明，主要差异在于他们采用的双量化 [[51](#bib.bib51)] 的内在复杂性，该方法首先将权重量化为
    8 位，然后再量化为 4 位。注意，第二步保持非对称以对抗量化损失。这种 *非对称* 方案需要在昂贵的 CUDA 核心中进行逐元素的乘法和减法操作。更多细节请参见
    [B.2](#A2.SS2 "B.2 More Discussion with QServe ‣ Appendix B Additional Discussions
    ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs")。'
- en: '![Refer to caption](img/07ca1b5ed82adfd0a796fa7c18f83d0b.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/07ca1b5ed82adfd0a796fa7c18f83d0b.png)'
- en: (a) Fine-grained Integer Scale
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 细粒度整数尺度
- en: '![Refer to caption](img/fa10c7858ca106bc62da9448b59bb343.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fa10c7858ca106bc62da9448b59bb343.png)'
- en: (b) Coarse-grained
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 粗粒度
- en: 'Figure 6: Kernel speed comparison with QServe’s W4A8 at K=4096, N=22016\. The
    acceleration ratio is against FP16\. Both our fine and coarse-grained kernels
    are faster.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 与 QServe 的 W4A8 在 K=4096，N=22016 下的核心速度比较。加速比相对于 FP16。我们的细粒度和粗粒度核心都更快。'
- en: 6 Ablation Study
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 消融研究
- en: 6.1 Fixed Amplifier vs. Heuristic Search
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 固定放大器与启发式搜索
- en: 'To find the optimal amplifier for the integer scale, we test several amplifiers
    in Table [7](#S6.T7 "Table 7 ‣ 6.1 Fixed Amplifier vs. Heuristic Search ‣ 6 Ablation
    Study ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization of LLMs").
    It turns out that using an amplifier bigger than 1024 doesn’t bring substantial
    gains while $2^{10}$.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '为了找到整数尺度下的最佳放大器，我们测试了表 [7](#S6.T7 "Table 7 ‣ 6.1 Fixed Amplifier vs. Heuristic
    Search ‣ 6 Ablation Study ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs") 中的几个放大器。结果表明，使用大于 1024 的放大器不会带来显著的增益，而是 $2^{10}$。'
- en: 'Table 7: Ablation on the amplifier value. Perplexity is tested on C4.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 放大器值的消融实验。困惑度在 C4 上进行测试。'
- en: '| BitWidth | Amplifier | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B | LLaMA-3-8B
    | LLaMA-3-70B |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| BitWidth | 放大器 | LLaMA-2-7B | LLaMA-2-13B | LLaMA-2-70B | LLaMA-3-8B | LLaMA-3-70B
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4A16 | - | 7.43 | 6.64 | 5.66 | 10.00 | 9.06 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | - | 7.43 | 6.64 | 5.66 | 10.00 | 9.06 |'
- en: '| W4A16 | Heuristic | 7.46 | 6.65 | 5.66 | 10.03 | 9.10 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | 启发式 | 7.46 | 6.65 | 5.66 | 10.03 | 9.10 |'
- en: '| W4A16 | 128 | 6.75 | 7.57 | 5.81 | 15.52 | 13.84 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | 128 | 6.75 | 7.57 | 5.81 | 15.52 | 13.84 |'
- en: '| W4A16 | 512 | 7.45 | 6.65 | 5.67 | 10.09 | 9.27 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | 512 | 7.45 | 6.65 | 5.67 | 10.09 | 9.27 |'
- en: '| W4A16 | 1024 | 7.45 | 6.64 | 5.66 | 10.03 | 9.04 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | 1024 | 7.45 | 6.64 | 5.66 | 10.03 | 9.04 |'
- en: '| W4A16 | 4096 | 7.45 | 6.64 | 5.67 | 10.00 | 8.91 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | 4096 | 7.45 | 6.64 | 5.67 | 10.00 | 8.91 |'
- en: 6.2 Speed Comparison of Float Scale vs. Integer Scale
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 浮点尺度与整数尺度的速度比较
- en: 'We compare the difference in inference speed using float and integer scales
    to showcase the latency advantage of using the Integer scale in Figure [5](#S5.F5
    "Figure 5 ‣ 5.4 W4A8 Kernel Latency Comparison ‣ 5 Experiments ‣ Integer Scale:
    A Free Lunch for Faster Fine-grained Quantization of LLMs") (a). The speedup is
    at most 2.3$\times$, suggesting the reduction of costly type conversions is more
    than necessary.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '我们比较了使用浮点和整数尺度的推理速度差异，以展示整数尺度在图 [5](#S5.F5 "Figure 5 ‣ 5.4 W4A8 Kernel Latency
    Comparison ‣ 5 Experiments ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs") (a) 中的延迟优势。加速比最多达到 2.3$\times$，表明减少昂贵的类型转换是十分必要的。'
- en: 7 Conclusion
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper, we introduced a plug-and-play scheme called *Integer Scale* that
    can be applied to speed up the existing fine-grained quantization approaches.
    We showed through extensive experiments that the Integer Scale not only benefits
    from the performance boost due to fine granularity but also well resolves the
    intrinsic computational overhead. It can serve as a default free-lunch technique
    with fine-grained approaches of various bandwidths to render an overall competitive
    quantization strategy. Moreover, the same strategy can be applied to Mixtral 8$\times$7B
    based on a mixture-of-experts and LLaMA-3, which were previously difficult to
    quantize at lower bit widths.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一种称为 *Integer Scale* 的即插即用方案，旨在加速现有的细粒度量化方法。通过大量实验，我们展示了 Integer Scale
    不仅由于细粒度而受益于性能提升，而且很好地解决了固有的计算开销。它可以作为一种默认的免费技术，与各种带宽的细粒度方法结合，提供一种总体上具有竞争力的量化策略。此外，同样的策略可以应用于基于专家混合的
    Mixtral 8$\times$7B 和 LLaMA-3，这些模型之前在较低位宽下量化困难。
- en: References
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: AI@Meta [2024] AI@Meta. Llama 3 model card. 2024. URL [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta [2024] AI@Meta。Llama 3 模型卡。2024年。网址 [https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: Ashkboos et al. [2023] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end
    4-bit inference on generative large language models. *arXiv preprint arXiv:2310.09259*,
    2023.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos et al. [2023] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler 和 Dan Alistarh。朝着端到端的 4 位推理在生成性大型语言模型上。*arXiv
    预印本 arXiv:2310.09259*，2023年。
- en: 'Ashkboos et al. [2024] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L
    Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman.
    Quarot: Outlier-free 4-bit inference in rotated llms. *arXiv preprint arXiv:2404.00456*,
    2024.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos et al. [2024] Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci,
    Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler 和 James Hensman。Quarot：在旋转
    LLM 中进行无离群点的 4 位推理。*arXiv 预印本 arXiv:2404.00456*，2024年。
- en: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, et al. Language models are few-shot learners. In *Conference on Neural
    Information Processing Systems (NeurIPS)*, 2020.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown et al. [2020] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared
    D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell 等。语言模型是少样本学习者。在 *神经信息处理系统大会 (NeurIPS)*，2020年。
- en: 'Cai et al. [2024] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D.
    Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework
    with multiple decoding heads. *arXiv preprint arXiv: 2401.10774*, 2024.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai et al. [2024] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason
    D. Lee, Deming Chen 和 Tri Dao。Medusa：具有多个解码头的简单 LLM 推理加速框架。*arXiv 预印本 arXiv: 2401.10774*，2024年。'
- en: 'Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. Quip: 2-bit quantization of large language models with guarantees. *Advances
    in Neural Information Processing Systems*, 36, 2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chee et al. [2024] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher
    M De Sa。Quip：具有保证的大型语言模型的 2 位量化。*神经信息处理系统进展*，36，2024年。
- en: 'Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. *arXiv
    preprint arXiv:2204.02311*, 2022.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chowdhery et al. [2022] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten
    Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton,
    Sebastian Gehrmann 等。Palm：利用路径扩展语言建模。*arXiv 预印本 arXiv:2204.02311*，2022年。
- en: 'Contributors [2023] LMDeploy Contributors. Lmdeploy: A toolkit for compressing,
    deploying, and serving llm. [https://github.com/InternLM/lmdeploy](https://github.com/InternLM/lmdeploy),
    2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Contributors [2023] LMDeploy Contributors。Lmdeploy：一个用于压缩、部署和服务 LLM 的工具包。 [https://github.com/InternLM/lmdeploy](https://github.com/InternLM/lmdeploy)，2023年。
- en: 'Dao et al. [2022] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher
    Ré. FlashAttention: Fast and memory-efficient exact attention with io-awareness.
    *arXiv preprint arXiv:2205.14135*, 2022.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao et al. [2022] Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra 和 Christopher
    Ré。FlashAttention：具有 IO 感知的快速且内存高效的精确注意力。*arXiv 预印本 arXiv:2205.14135*，2022年。
- en: 'Devlin et al. [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *North American Chapter of the Association for Computational
    Linguistics (NAACL)*, 2019.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等人 [2019] Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    BERT：用于语言理解的深度双向变换器预训练。在*北美计算语言学协会（NAACL）*，2019年。
- en: Egiazarian et al. [2024] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev,
    Elias Frantar, Artem Babenko, and Dan Alistarh. Extreme compression of large language
    models via additive quantization. *arXiv preprint arXiv:2401.06118*, 2024.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Egiazarian 等人 [2024] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias
    Frantar, Artem Babenko 和 Dan Alistarh. 通过加法量化实现大型语言模型的极限压缩。*arXiv 预印本 arXiv:2401.06118*，2024年。
- en: 'Frantar and Alistarh [2022] Elias Frantar and Dan Alistarh. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *Advances in
    Neural Information Processing Systems*, 35:4475–4488, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh [2022] Elias Frantar 和 Dan Alistarh. 最优脑压缩：用于准确的训练后量化和修剪的框架。*神经信息处理系统进展*，35:4475–4488，2022年。
- en: 'Frantar and Alistarh [2024] Elias Frantar and Dan Alistarh. Marlin: a fast
    4-bit inference kernel for medium batchsizes. [https://github.com/IST-DASLab/marlin](https://github.com/IST-DASLab/marlin),
    2024.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 和 Alistarh [2024] Elias Frantar 和 Dan Alistarh. Marlin：一个用于中等批量大小的快速
    4 位推理内核。 [https://github.com/IST-DASLab/marlin](https://github.com/IST-DASLab/marlin)，2024年。
- en: 'Frantar et al. [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人 [2022] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh.
    Gptq：用于生成预训练变换器的准确训练后量化。*arXiv 预印本 arXiv:2210.17323*，2022年。
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
    URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang 和 Andy
    Zou. 少样本语言模型评估框架，2021年9月。URL [https://doi.org/10.5281/zenodo.5371628](https://doi.org/10.5281/zenodo.5371628)。
- en: Hendrycks et al. [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language
    understanding. *Proceedings of the International Conference on Learning Representations
    (ICLR)*, 2021.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 [2021] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song 和 Jacob Steinhardt. 大规模多任务语言理解的测量。*国际学习表征会议（ICLR）论文集*，2021年。
- en: Huang et al. [2024] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao
    Lv, Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How good
    are low-bit quantized llama3 models? an empirical study. *arXiv preprint arXiv:2404.14047*,
    2024.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2024] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv,
    Hong Chen, Jie Luo, Xiaojuan Qi, Xianglong Liu 和 Michele Magno. 低位量化 Llama3 模型的效果如何？一项实证研究。*arXiv
    预印本 arXiv:2404.14047*，2024年。
- en: Jiang et al. [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*,
    2024.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 [2024] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand 等人。专家混合模型。*arXiv 预印本 arXiv:2401.04088*，2024年。
- en: Kaplan et al. [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown,
    Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
    Scaling laws for neural language models. *arXiv preprint arXiv:2001.08361*, 2020.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaplan 等人 [2020] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu 和 Dario Amodei. 神经语言模型的扩展规律。*arXiv
    预印本 arXiv:2001.08361*，2020年。
- en: Kwon et al. [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*, 2023.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等人 [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
    Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang 和 Ion Stoica. 基于分页注意力的大型语言模型服务的高效内存管理。在*ACM
    SIGOPS 第29届操作系统原理研讨会论文集*，2023年。
- en: Leviathan et al. [2023] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast
    inference from transformers via speculative decoding. In Andreas Krause, Emma
    Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett,
    editors, *Proceedings of the 40th International Conference on Machine Learning*,
    volume 202 of *Proceedings of Machine Learning Research*, pages 19274–19286\.
    PMLR, 23–29 Jul 2023. URL [https://proceedings.mlr.press/v202/leviathan23a.html](https://proceedings.mlr.press/v202/leviathan23a.html).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Leviathan et al. [2023] Yaniv Leviathan, Matan Kalman, 和 Yossi Matias. 通过推测解码实现变换器的快速推理。在
    Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato,
    和 Jonathan Scarlett 主编的 *第40届国际机器学习大会论文集* 中，第202卷，*机器学习研究论文集*，第19274–19286页。PMLR，2023年7月23–29日。网址
    [https://proceedings.mlr.press/v202/leviathan23a.html](https://proceedings.mlr.press/v202/leviathan23a.html)。
- en: 'Li et al. [2024a] Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm
    tweaking: High-performance low-bit quantization of large language models. In *Proceedings
    of the AAAI Conference on Artificial Intelligence*, volume 38, pages 18536–18544,
    2024a.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2024a] Liang Li, Qingyuan Li, Bo Zhang, 和 Xiangxiang Chu. Norm tweaking:
    大语言模型的高性能低位量化。在 *AAAI 人工智能会议论文集*，第38卷，第18536–18544页，2024年。'
- en: Li et al. [2023a] Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan
    Lu, Xiangxiang Chu, Yerui Sun, and Yuchen Xie. A speed odyssey for deployable
    quantization of llms. *arXiv preprint arXiv:2311.09550*, 2023a.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023a] Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan
    Lu, Xiangxiang Chu, Yerui Sun, 和 Yuchen Xie. 用于可部署量化的大语言模型的速度历程。*arXiv 预印本 arXiv:2311.09550*，2023年。
- en: 'Li et al. [2023b] Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang
    Chu, Yerui Sun, Li Du, and Yuchen Xie. Fptq: Fine-grained post-training quantization
    for large language models. *arXiv preprint arXiv:2308.15987*, 2023b.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2023b] Qingyuan Li, Yifan Zhang, Liang Li, Peng Yao, Bo Zhang, Xiangxiang
    Chu, Yerui Sun, Li Du, 和 Yuchen Xie. FPTQ: 大语言模型的细粒度后训练量化。*arXiv 预印本 arXiv:2308.15987*，2023年。'
- en: 'Li et al. [2024b] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle:
    Speculative sampling requires rethinking feature uncertainty. In *International
    Conference on Machine Learning*, 2024b.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. [2024b] Yuhui Li, Fangyun Wei, Chao Zhang, 和 Hongyang Zhang. Eagle:
    推测采样需要重新思考特征不确定性。在 *国际机器学习大会*，2024年。'
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration, 2023.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. AWQ: 面向大语言模型压缩和加速的激活感知权重量化，2023年。'
- en: 'Lin et al. [2024] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan
    Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design
    for efficient llm serving. *arXiv preprint arXiv:2405.04532*, 2024.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2024] Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan
    Xiao, Chuang Gan, 和 Song Han. Qserve: W4A8KV4 量化和系统共同设计以实现高效的大语言模型服务。*arXiv 预印本
    arXiv:2405.04532*，2024年。'
- en: 'Liu et al. [2023] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    Llm-qat: Data-free quantization aware training for large language models, 2023.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    LLM-QAT: 针对大语言模型的无数据量化感知训练，2023年。'
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016年。
- en: Micikevicius et al. [2022] Paulius Micikevicius, Dusan Stosic, Neil Burgess,
    Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke,
    Patrick Judd, John Kamalu, et al. Fp8 formats for deep learning. *arXiv preprint
    arXiv:2209.05433*, 2022.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Micikevicius et al. [2022] Paulius Micikevicius, Dusan Stosic, Neil Burgess,
    Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke,
    Patrick Judd, John Kamalu, 等等。深度学习的 FP8 格式。*arXiv 预印本 arXiv:2209.05433*，2022年。
- en: Nagel et al. [2021] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei
    Bondarenko, Mart van Baalen, and Tijmen Blankevoort. A white paper on neural network
    quantization. *arXiv preprint arXiv:2106.08295*, 2021.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel et al. [2021] Markus Nagel, Marios Fournarakis, Rana Ali Amjad, Yelysei
    Bondarenko, Mart van Baalen, 和 Tijmen Blankevoort. 关于神经网络量化的白皮书。*arXiv 预印本 arXiv:2106.08295*，2021年。
- en: NVIDIA [2023a] NVIDIA. cutlass. [https://github.com/NVIDIA/cutlass](https://github.com/NVIDIA/cutlass),
    2023a.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA [2023a] NVIDIA. cutlass. [https://github.com/NVIDIA/cutlass](https://github.com/NVIDIA/cutlass)，2023年。
- en: NVIDIA [2023b] NVIDIA. Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer),
    2023b.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA [2023b] NVIDIA. Fastertransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)，2023年。
- en: NVIDIA [2023c] NVIDIA. Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM),
    2023c.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NVIDIA [2023c] NVIDIA. Tensorrt-llm. [https://github.com/NVIDIA/TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)，2023c。
- en: OpenAI [2023] OpenAI. Gpt-4 technical report, 2023.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI [2023] OpenAI. Gpt-4 技术报告，2023。
- en: 'Paperno et al. [2016] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou,
    Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda,
    and Raquel Fernández. The LAMBADA dataset: Word prediction requiring a broad discourse
    context. *arXiv preprint arXiv:1606.06031*, 2016.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paperno et al. [2016] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou,
    Quan Ngoc Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda,
    和 Raquel Fernández. LAMBADA 数据集：需要广泛话语背景的词预测。*arXiv 预印本 arXiv:1606.06031*，2016。
- en: 'Park et al. [2022] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.
    Lut-gemm: Quantized matrix multiplication based on luts for efficient inference
    in large-scale generative language models. *arXiv preprint arXiv:2206.09557*,
    2022.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park et al. [2022] Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, 和 Dongsoo Lee.
    Lut-gemm：基于 LUT 的量化矩阵乘法，用于大规模生成语言模型中的高效推理。*arXiv 预印本 arXiv:2206.09557*，2022。
- en: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the
    limits of transfer learning with a unified text-to-text transformer. *Journal
    of Machine Learning Research*, 21(140):1–67, 2020.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel et al. [2020] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, 和 Peter Liu. 探索统一文本到文本变换器的迁移学习极限。*机器学习研究杂志*，21(140):1–67，2020。
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. Winogrande：大规模的对抗性 Winograd 结构挑战。*计算机协会通讯*，64(9):99–106，2021。
- en: 'Shang et al. [2023] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm:
    Partially binarized large language models. *arXiv preprint arXiv:2310.00034*,
    2023.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang et al. [2023] Yuzhang Shang, Zhihang Yuan, Qiang Wu, 和 Zhen Dong. Pb-llm：部分二值化大型语言模型。*arXiv
    预印本 arXiv:2310.00034*，2023。
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. *arXiv preprint arXiv:2308.13137*,
    2023.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, 和 Ping Luo. Omniquant：用于大型语言模型的全方向标定量化。*arXiv
    预印本 arXiv:2308.13137*，2023。
- en: 'Talmor et al. [2019] Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan
    Berant. Commonsenseqa: A question answering challenge targeting commonsense knowledge,
    2019.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Talmor et al. [2019] Alon Talmor, Jonathan Herzig, Nicholas Lourie, 和 Jonathan
    Berant. Commonsenseqa：一个针对常识知识的问答挑战，2019。
- en: 'Tata and Patel [2003] Sandeep Tata and Jignesh M Patel. PiQA: An algebra for
    querying protein data sets. In *International Conference on Scientific and Statistical
    Database Management*, 2003.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tata and Patel [2003] Sandeep Tata 和 Jignesh M Patel. PiQA：查询蛋白质数据集的代数。*国际科学与统计数据库管理会议*，2003。
- en: 'Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron et al. [2023] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等. Llama 2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023。
- en: 'Tseng et al. [2024] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    and Christopher De Sa. Quip#: Even better llm quantization with hadamard incoherence
    and lattice codebooks. *arXiv preprint arXiv:2402.04396*, 2024.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tseng et al. [2024] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov,
    和 Christopher De Sa. Quip#: 更好的 LLM 量化，具有 Hadamard 不一致性和格子码本。*arXiv 预印本 arXiv:2402.04396*，2024。'
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. In *Conference on Neural Information Processing Systems (NeurIPS)*,
    2017.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力即你所需。*神经信息处理系统大会
    (NeurIPS)*，2017。
- en: 'Xia et al. [2024] Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei
    Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou,
    et al. Fp6-llm: Efficiently serving large language models through fp6-centric
    algorithm-system co-design. *arXiv preprint arXiv:2401.14112*, 2024.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xia 等人 [2024] Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao,
    Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou 等人。Fp6-llm：通过
    fp6 设计算法-系统高效服务大语言模型。*arXiv 预印本 arXiv:2401.14112*，2024年。
- en: 'Xiao et al. [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. In *International Conference on Machine Learning*, pages
    38087–38099\. PMLR, 2023.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等人 [2023] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth
    和 Song Han。Smoothquant：准确且高效的大语言模型后训练量化。发表于 *国际机器学习会议*，页码 38087–38099。PMLR，2023年。
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. ZeroQuant: Efficient and affordable post-training
    quantization for large-scale transformers. *arXiv preprint arXiv:2206.01861*,
    2022.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人 [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu,
    Conglong Li 和 Yuxiong He。ZeroQuant：高效且经济的大规模变换器后训练量化。*arXiv 预印本 arXiv:2206.01861*，2022年。
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等人 [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi 和 Yejin
    Choi。Hellaswag：机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*，2019年。
- en: 'Zhang et al. [2024] Luoming Zhang, Wen Fei, Weijia Wu, Yefei He, Zhenyu Lou,
    and Hong Zhou. Dual grained quantization: Efficient fine-grained quantization
    for LLM, 2024. URL [https://openreview.net/forum?id=ktmMkOOeYb](https://openreview.net/forum?id=ktmMkOOeYb).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2024] Luoming Zhang, Wen Fei, Weijia Wu, Yefei He, Zhenyu Lou 和 Hong
    Zhou。双重粒度量化：高效的 LLM 精细化量化，2024年。网址 [https://openreview.net/forum?id=ktmMkOOeYb](https://openreview.net/forum?id=ktmMkOOeYb)。
- en: 'Zhao et al. [2023] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. Atom:
    Low-bit quantization for efficient and accurate llm serving. *arXiv preprint arXiv:2310.19102*,
    2023.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等人 [2023] Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen, Size
    Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen 和 Baris Kasikci。Atom：高效且准确的低位量化用于
    LLM 服务。*arXiv 预印本 arXiv:2310.19102*，2023年。
- en: Appendix A Background Knowledge on LLM Quantization
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A LLM 量化背景知识
- en: A.1 Symmetric vs. Asymmetric Quantization
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 对称量化与非对称量化
- en: We suggest referring to the white paper [[31](#bib.bib31)] for a thorough understanding
    of network quantization. We draw some key concepts here as a quick manual. Both
    symmetric and asymmetric quantization use uniform quantization that maps float
    values to integer values with a single scale. Symmetric quantization computes
    the scale $s$ as,
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议参考白皮书 [[31](#bib.bib31)] 以彻底理解网络量化。我们在这里提取了一些关键概念作为快速手册。对称量化和非对称量化都使用统一量化，将浮点值映射到整数值，使用一个缩放比例。对称量化计算缩放比例
    $s$ 如下：
- en: '|  | $s=\frac{&#124;X&#124;_{max}}{2^{n-1}-1}$ |  | (3) |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '|  | $s=\frac{&#124;X&#124;_{max}}{2^{n-1}-1}$ |  | (3) |'
- en: '|  | $Q(X)=clamp(\lfloor X/s\rceil,-2^{n-1},2^{n-1}-1)$ |  | (4) |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(X)=clamp(\lfloor X/s\rceil,-2^{n-1},2^{n-1}-1)$ |  | (4) |'
- en: For asymmetric quantization, a zero point is utilized.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对于非对称量化，使用了零点。
- en: '|  | $s=\frac{X_{max}-X_{min}}{2^{n}-1},z=\lfloor\frac{-X_{min}}{s}\rceil$
    |  | (5) |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  | $s=\frac{X_{max}-X_{min}}{2^{n}-1},z=\lfloor\frac{-X_{min}}{s}\rceil$
    |  | (5) |'
- en: '|  | $Q(X)=clamp(\lfloor X/s\rceil+z,0,2^{n}-1)$ |  | (6) |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(X)=clamp(\lfloor X/s\rceil+z,0,2^{n}-1)$ |  | (6) |'
- en: A.2 Per-tensor, Per-token, Per-channel quantization, Group-wise Quantization
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 每张量、每标记、每通道量化，组内量化
- en: Take symmetric quantization as an example, *per-tensor quantization* uses the
    same scale for all tensor values. *Per-channel/token quantization* uses a scale
    for a row or a column of the tensor. We can divide each channel into groups for
    *group-wise quantization* [[26](#bib.bib26)], also called fined-grained quantization.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以对称量化为例，*每张量量化* 对所有张量值使用相同的缩放比例。*每通道/标记量化* 对张量的每一行或每一列使用一个缩放比例。我们可以将每个通道划分为组进行*组内量化*
    [[26](#bib.bib26)]，也称为精细化量化。
- en: Appendix B Additional Discussions
  id: totrans-258
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 附加讨论
- en: B.1 Experiment Result on MMLU
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 MMLU 实验结果
- en: 'Table [8](#A2.T8 "Table 8 ‣ B.1 Experiment Result on MMLU ‣ Appendix B Additional
    Discussions ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") compares the result on MMLU [[16](#bib.bib16)].'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [8](#A2.T8 "表 8 ‣ B.1 MMLU 实验结果 ‣ 附录 B 附加讨论 ‣ 整数缩放：快速精细化量化 LLM 的免费午餐") 比较了
    MMLU [[16](#bib.bib16)] 上的结果。
- en: 'Table 8: Comparison with state-of-the-art quantization methods on MMLU. For
    all models tested, we set the weight group size to 128 and apply symmetric quantization.
    Integer Scale (IS) with amplifier 1024 is used.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：与最先进量化方法在 MMLU 上的比较。对于所有测试的模型，我们将权重组大小设置为 128，并应用对称量化。使用放大器 1024 的整数比例（IS）。
- en: '| Model | HyperParam | MMLU |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 超参数 | MMLU |'
- en: '|  | Method | BitWidth | Hums. | STEM | Social | Other | Avg |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '|  | 方法 | 位宽 | 人文 | STEM | 社会 | 其他 | 平均 |'
- en: '| LLaMA-2-7B | FP16 | W16A16 | 36.92% | 30.75% | 40.92% | 45.68% | 38.49% |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-7B | FP16 | W16A16 | 36.92% | 30.75% | 40.92% | 45.68% | 38.49% |'
- en: '| GPTQ | W4A8 | 33.69% | 30.45% | 40.36% | 42.91% | 36.58% |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 | 33.69% | 30.45% | 40.36% | 42.91% | 36.58% |'
- en: '| GPTQ w/ IS | W4A8 | 34.64% | 31.35% | 39.36% | 43.18% | 36.94% [+0.36%] |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 | 34.64% | 31.35% | 39.36% | 43.18% | 36.94% [+0.36%] |'
- en: '| AWQ | W4A8 | 34.86% | 29.69% | 40.98% | 41.27% | 36.57% |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 | 34.86% | 29.69% | 40.98% | 41.27% | 36.57% |'
- en: '| AWQ w/ IS | W4A8 | 34.13% | 30.19% | 40.40% | 41.52% | 36.36% [-0.21%] |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 | 34.13% | 30.19% | 40.40% | 41.52% | 36.36% [-0.21%] |'
- en: '| Omniquant | W4A8 | 34.39% | 31.84% | 42.28% | 43.77% | 37.74% |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 | 34.39% | 31.84% | 42.28% | 43.77% | 37.74% |'
- en: '| Omniquant w/ IS | W4A8 | 33.65% | 31.05% | 40.17% | 43.18% | 36.72% [-1.02%]
    |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 | 33.65% | 31.05% | 40.17% | 43.18% | 36.72% [-1.02%]
    |'
- en: '| LLaMA-2-13B | FP16 | W16A16 | 54.43% | 44.27% | 63.41% | 60.76% | 55.68%
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-13B | FP16 | W16A16 | 54.43% | 44.27% | 63.41% | 60.76% | 55.68%
    |'
- en: '| GPTQ | W4A8 | 51.88% | 43.57% | 62.01% | 60.21% | 54.24% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 | 51.88% | 43.57% | 62.01% | 60.21% | 54.24% |'
- en: '| GPTQ w/ IS | W4A8 | 52.18% | 43.27% | 61.33% | 60.83% | 54.27% [+0.03%] |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 | 52.18% | 43.27% | 61.33% | 60.83% | 54.27% [+0.03%] |'
- en: '| AWQ | W4A8 | 50.07% | 41.75% | 60.90% | 59.19% | 52.76% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 | 50.07% | 41.75% | 60.90% | 59.19% | 52.76% |'
- en: '| AWQ w/ IS | W4A8 | 49.65% | 42.64% | 59.80% | 58.45% | 52.40% [-0.36%] |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 | 49.65% | 42.64% | 59.80% | 58.45% | 52.40% [-0.36%] |'
- en: '| Omniquant | W4A8 | 52.56% | 43.21% | 62.56% | 60.67% | 54.61% |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 | 52.56% | 43.21% | 62.56% | 60.67% | 54.61% |'
- en: '| Omniquant w/ IS | W4A8 | 52.05% | 43.14% | 61.72% | 60.02% | 54.09% [-0.52%]
    |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 | 52.05% | 43.14% | 61.72% | 60.02% | 54.09% [-0.52%]
    |'
- en: '| LLaMA-2-70B | FP16 | W16A16 | 65.16% | 57.79% | 80.44% | 74.61% | 69.11%
    |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-2-70B | FP16 | W16A16 | 65.16% | 57.79% | 80.44% | 74.61% | 69.11%
    |'
- en: '| GPTQ | W4A8 | 62.49% | 55.17% | 78.55% | 73.01% | 66.86% |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 | 62.49% | 55.17% | 78.55% | 73.01% | 66.86% |'
- en: '| GPTQ w/ IS | W4A8 | 62.42% | 55.14% | 78.39% | 72.73% | 66.74% [-0.12%] |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 | 62.42% | 55.14% | 78.39% | 72.73% | 66.74% [-0.12%] |'
- en: '| AWQ | W4A8 | 63.44% | 55.86% | 78.45% | 72.12% | 67.11% |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 | 63.44% | 55.86% | 78.45% | 72.12% | 67.11% |'
- en: '| AWQ w/ IS | W4A8 | 63.70% | 55.33% | 78.00% | 71.75% | 66.89% [-0.22%] |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 | 63.70% | 55.33% | 78.00% | 71.75% | 66.89% [-0.22%] |'
- en: '| Mixtral-8x7B | FP16 | W16A16 | 64.46% | 61.30% | 81.51% | 77.39% | 70.50%
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | FP16 | W16A16 | 64.46% | 61.30% | 81.51% | 77.39% | 70.50%
    |'
- en: '| GPTQ | W4A8 | 61.70% | 58.78% | 78.78% | 73.81% | 67.61% |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | W4A8 | 61.70% | 58.78% | 78.78% | 73.81% | 67.61% |'
- en: '| GPTQ w/ IS | W4A8 | 61.66% | 57.55% | 77.58% | 73.60% | 67.02% |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ w/ IS | W4A8 | 61.66% | 57.55% | 77.58% | 73.60% | 67.02% |'
- en: '| AWQ | W4A8 | 64.48% | 60.17% | 80.05% | 75.20% | 69.44% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | W4A8 | 64.48% | 60.17% | 80.05% | 75.20% | 69.44% |'
- en: '| AWQ w/ IS | W4A8 | 62.85% | 59.18% | 79.07% | 74.58% | 68.32% |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| AWQ w/ IS | W4A8 | 62.85% | 59.18% | 79.07% | 74.58% | 68.32% |'
- en: '| Omniquant | W4A8 | 63.00% | 58.78% | 80.21% | 75.69% | 68.79% |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant | W4A8 | 63.00% | 58.78% | 80.21% | 75.69% | 68.79% |'
- en: '| Omniquant w/ IS | W4A8 | 62.17% | 58.81% | 79.92% | 75.17% | 68.34% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Omniquant w/ IS | W4A8 | 62.17% | 58.81% | 79.92% | 75.17% | 68.34% |'
- en: B.2 More Discussion with QServe
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 关于 QServe 的更多讨论
- en: Due to the adopted asymmetry quantization, QServe’s kernel is prone to complex
    computation logic that can be formulated as,
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 由于采用了不对称量化，QServe 的内核容易出现复杂的计算逻辑，具体公式如下：
- en: '|  | $1$2 |  | (7) |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: '|  | $1$2 |  | (8) |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (8) |'
- en: '|  | $\cdots$ |  | (9) |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '|  | $\cdots$ |  | (9) |'
- en: where $s_{i}$ is element-wise multiplication, and the subtraction is performed
    with a vadd4 instruction.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s_{i}$ 是逐元素乘法，减法则使用 vadd4 指令进行。
- en: 'Figure [7](#A2.F7 "Figure 7 ‣ B.2 More Discussion with QServe ‣ Appendix B
    Additional Discussions ‣ Integer Scale: A Free Lunch for Faster Fine-grained Quantization
    of LLMs") gives the additional comparison on kernel (N=4096, K=4096), where our
    fine and coarse-grained kernels also outperform QServe, indicating our flexibility
    in different inputs.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [7](#A2.F7 "图 7 ‣ B.2 关于 QServe 的更多讨论 ‣ 附录 B 额外讨论 ‣ 整数比例：快速细粒度量化的免费午餐") 展示了内核（N=4096，K=4096）的额外比较，其中我们的细粒度和粗粒度内核也优于
    QServe，显示了我们在不同输入中的灵活性。
- en: '![Refer to caption](img/2111332f0440466232609ae146cdd3b3.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2111332f0440466232609ae146cdd3b3.png)'
- en: (a) Fine-grained
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 细粒度
- en: '![Refer to caption](img/dd3ffcd44ec6d88df3eddf8b67565a66.png)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dd3ffcd44ec6d88df3eddf8b67565a66.png)'
- en: (b) Coarse-grained
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 粗粒度
- en: 'Figure 7: Kernel (N=4096,K=4096) speed comparison with QServe. The acceleration
    ratio is against FP16.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：与 QServe 的内核（N=4096,K=4096）速度比较。加速比以 FP16 为基准。
- en: B.3 Max Activation Values Per Layer
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3 每层最大激活值
- en: 'To verify whether our amplifier choice is feasible and not causing overflows,
    we illustrate the maximum layerwise activation values on the investigated models
    in Figure [8](#A2.F8 "Figure 8 ‣ B.3 Max Activation Values Per Layer ‣ Appendix
    B Additional Discussions ‣ Integer Scale: A Free Lunch for Faster Fine-grained
    Quantization of LLMs"). It appears no layer’s output goes near the INT32 upper
    bound. We refrain from selecting a higher amplifier to improve performance since
    it will generate few gains and in the meantime increase the overflow risk.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的放大器选择是否可行且不会导致溢出，我们在图 [8](#A2.F8 "图 8 ‣ B.3 每层最大激活值 ‣ 附录 B 额外讨论 ‣ 整数比例：更快细粒度量化的免费午餐")
    中展示了被调查模型的最大逐层激活值。看起来没有层的输出接近 INT32 上限。我们避免选择更高的放大器来提高性能，因为这样做收益有限，同时增加溢出风险。
- en: '![Refer to caption](img/806d8b3fe6cfca447713bdc6d807058a.png)![Refer to caption](img/515529ed3a1830d8651c1cc36b0ad46e.png)![Refer
    to caption](img/c828a53d32f5d4efcdba22f20dafb411.png)![Refer to caption](img/471b2673813b67a68cfc331e5b8c851f.png)![Refer
    to caption](img/3c9491a47504c0735e6a7b231fe55120.png)![Refer to caption](img/fdee8bc35aeaf21239e4971154fd1bcc.png)![Refer
    to caption](img/197132d6a4629f67d2eade9d34bf90c7.png)![Refer to caption](img/4a3ec601c3ae655a0c64a79bc4a4e568.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/806d8b3fe6cfca447713bdc6d807058a.png)![参见说明](img/515529ed3a1830d8651c1cc36b0ad46e.png)![参见说明](img/c828a53d32f5d4efcdba22f20dafb411.png)![参见说明](img/471b2673813b67a68cfc331e5b8c851f.png)![参见说明](img/3c9491a47504c0735e6a7b231fe55120.png)![参见说明](img/fdee8bc35aeaf21239e4971154fd1bcc.png)![参见说明](img/197132d6a4629f67d2eade9d34bf90c7.png)![参见说明](img/4a3ec601c3ae655a0c64a79bc4a4e568.png)'
- en: 'Figure 8: Maximum activation values per layer of quantized LLaMA-2 models and
    Mixtral 8x7B using an amplifier of 1024.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：使用 1024 的放大器的量化 LLaMA-2 模型和 Mixtral 8x7B 每层的最大激活值。
- en: B.4 Limitations
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4 限制
- en: Our method is limited by the possible overflow risks. However, we can trade
    off with per group removal of the amplifier which introduces extra computations.
    We can opt to use this degraded version of our GEMM kernels for layers that suffer
    from overflow risks.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法受限于可能的溢出风险。然而，我们可以通过每组去除放大器来权衡，这会引入额外的计算。我们可以选择在有溢出风险的层中使用这种降级版的 GEMM 内核。
