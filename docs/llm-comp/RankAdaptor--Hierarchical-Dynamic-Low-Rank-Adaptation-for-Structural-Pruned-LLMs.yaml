- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 19:04:39'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:39
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RankAdaptor：用于结构剪枝LLMs的分层动态低秩适配
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15734](https://ar5iv.labs.arxiv.org/html/2406.15734)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.15734](https://ar5iv.labs.arxiv.org/html/2406.15734)
- en: 'Changhai Zhou¹¹footnotemark: 1'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 周长海¹¹脚注标记：1
- en: Fudan University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 复旦大学
- en: zhouch23@m.fudan.edu.cn
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: zhouch23@m.fudan.edu.cn
- en: '&Shijie Han'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&韩世界'
- en: Columbia University
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: sh4460@columbia.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: sh4460@columbia.edu
- en: '&Shiyang Zhang'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&张士扬'
- en: Columbia University
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 哥伦比亚大学
- en: sz3209@columbia.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: sz3209@columbia.edu
- en: Shichao Weng
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 翁世超
- en: Fudan University
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 复旦大学
- en: scweng23@m.fudan.edu.cn
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: scweng23@m.fudan.edu.cn
- en: '&Zekai Liu'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '&刘泽凯'
- en: Fudan University
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 复旦大学
- en: zkliu23@m.fudan.edu.cn
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: zkliu23@m.fudan.edu.cn
- en: '&Cheng Jin'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '&金成'
- en: Fudan University
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 复旦大学
- en: jc@fudan.edu.cn Equal contribution
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: jc@fudan.edu.cn 共同贡献
- en: Abstract
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The efficient compression of large language models (LLMs) is becoming increasingly
    popular. However, recovering the accuracy of compressed LLMs is still a major
    challenge. Structural pruning with standard Low-Rank Adaptation (LoRA) is a common
    technique in current LLMs compression. In structural pruning, the model architecture
    is modified unevenly, resulting in suboptimal performance in various downstream
    tasks via standard LoRA with fixed rank. To address this problem, we introduce
    RankAdaptor, an efficient fine-tuning method with hierarchical dynamic rank scheduling
    for pruned LLMs. An end-to-end automatic optimization flow is developed that utilizes
    a lightweight performance model to determine the different ranks during fine-tuning.
    Comprehensive experiments on popular benchmarks shows that RankAdaptor consistently
    outperforms standard LoRA with structural pruning over different pruning settings.
    Without increasing the trainable parameters, RankAdaptor further reduces the accuracy
    performance gap between the recovery of pruned model and the original model compared
    to standard LoRA.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的高效压缩正变得越来越流行。然而，恢复压缩LLMs的准确性仍然是一个重大挑战。结构剪枝与标准低秩适配（LoRA）是当前LLMs压缩中的一种常见技术。在结构剪枝中，模型架构不均匀地被修改，导致通过固定秩的标准LoRA在各种下游任务中表现不佳。为了解决这个问题，我们引入了RankAdaptor，这是一种具有分层动态秩调度的高效微调方法，适用于剪枝后的LLMs。开发了一种端到端自动优化流程，利用轻量级性能模型在微调过程中确定不同的秩。对流行基准的综合实验表明，RankAdaptor在不同的剪枝设置下始终优于标准LoRA。RankAdaptor在不增加可训练参数的情况下，进一步缩小了剪枝模型与原始模型恢复之间的准确性性能差距，相较于标准LoRA。
- en: 1 Introduction
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: In recent years, large language models (LLMs) have provided innovative solutions
    across various natural language processing (NLP) tasks, such as machine translation
    [[31](#bib.bib31), [23](#bib.bib23), [1](#bib.bib1)], sentiment analysis [[32](#bib.bib32),
    [5](#bib.bib5)], and speech recognition [[19](#bib.bib19), [7](#bib.bib7)]. However,
    the exceptional performance of LLMs comes at the cost of a massive number of parameters
    and high-end hardware resources. To manage these demands, popular compression
    techniques like pruning [[17](#bib.bib17), [29](#bib.bib29), [22](#bib.bib22),
    [8](#bib.bib8)], quantization [[24](#bib.bib24), [14](#bib.bib14)], and distillation
    [[10](#bib.bib10), [26](#bib.bib26)] have been introduced. Regardless of the compression
    approach, compressed LLMs typically require fine-tuning to recover their pre-compression
    performance. Therefore, designing an efficient fine-tuning framework for compressed
    LLMs, enabling them to regain their original performance, has become a meaningful
    research endeavor.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，大型语言模型（LLMs）在各种自然语言处理（NLP）任务中提供了创新的解决方案，例如机器翻译 [[31](#bib.bib31), [23](#bib.bib23),
    [1](#bib.bib1)]、情感分析 [[32](#bib.bib32), [5](#bib.bib5)] 和语音识别 [[19](#bib.bib19),
    [7](#bib.bib7)]。然而，LLMs的卓越性能以巨大的参数数量和高端硬件资源为代价。为了管理这些需求，已经引入了流行的压缩技术，如剪枝 [[17](#bib.bib17),
    [29](#bib.bib29), [22](#bib.bib22), [8](#bib.bib8)]、量化 [[24](#bib.bib24), [14](#bib.bib14)]
    和蒸馏 [[10](#bib.bib10), [26](#bib.bib26)]。无论压缩方法如何，压缩后的LLMs通常需要微调以恢复其压缩前的性能。因此，设计一个高效的微调框架，以使压缩后的LLMs恢复其原始性能，已成为一个有意义的研究课题。
- en: 'Among compression techniques, pruning is the popular method, removing redundant
    weight connections to decrease model scale and computational demands. It primarily
    involves three stages: discovery, estimation, and recovery. While most existing
    research focuses on the first two stages, less attention is paid to the recovery
    phase [[17](#bib.bib17), [29](#bib.bib29)], specifically the fine-tuning method
    in this stage, which may directly affect models’ output accuracy. Few studies
    explore fine-tuning methods specific to compressed models, and researchers often
    apply the standard LoRA method directly [[12](#bib.bib12)]. However, although
    applying standard LoRA fine-tuning in the recovery stage can reduce the gap with
    unpruned accuracy, there is room for improvement. Standard LoRA applies same rank
    configuration to all layers of pruned models, but the pruned models lack structural
    regularity. Thus, a one-size-fits-all rank value may not optimally address the
    unique needs of each layer, potentially affecting downstream performance.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩技术中，剪枝是一种流行的方法，通过去除冗余的权重连接来减少模型规模和计算需求。它主要包括三个阶段：发现、估计和恢复。虽然大多数现有研究集中在前两个阶段，但对恢复阶段[[17](#bib.bib17),
    [29](#bib.bib29)]的关注较少，特别是这一阶段中的微调方法，这可能直接影响模型的输出准确性。很少有研究探讨针对压缩模型的微调方法，研究人员通常直接应用标准LoRA方法[[12](#bib.bib12)]。然而，尽管在恢复阶段应用标准LoRA微调可以缩小与未剪枝准确性之间的差距，但仍有改进的空间。标准LoRA对所有剪枝模型的层应用相同的秩配置，但剪枝模型缺乏结构规律性。因此，一刀切的秩值可能无法最佳地解决每一层的独特需求，从而可能影响下游性能。
- en: Although LoRA and its variants have demonstrated outstanding performance in
    fine-tuning tasks for current LLMs, their performance in the domain of model compression
    remains unexplored. Pruning leads to varying degrees of weight and connectivity
    reduction across the entire model network, so it is necessary to find a dynamic
    fine-tuning method to adapt to this imbalance. In the configuration of LoRA fine-tuning,
    directly related hyperparameter is the low-rank number of each layer. DyLoRA Li
    et al. [[15](#bib.bib15)] attempts to use dynamic ideas to select the best unified
    rank value for LoRA instead of setting ahead, which has been proven to be effective
    for unpruned models. But it still cannot adapt to the different hierarchical requirements
    of pruning layers with different pruning degrees. In addition, other fine-tuning
    methods like QLoRA [[6](#bib.bib6)] with quantization and LISA [[20](#bib.bib20)]
    with importance sampling on non-frozen layers enable efficient fine-tuning process.
    However, since they all disregard rank’s influence on LoRA fine-tuning for structurally
    pruned network, so they still remain unsuitable for pruned models.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LoRA及其变体在当前LLMs的微调任务中表现出色，但它们在模型压缩领域的表现尚未探索。剪枝导致整个模型网络中权重和连接性的不同程度减少，因此需要找到一种动态微调方法以适应这种不平衡。在LoRA微调的配置中，直接相关的超参数是每一层的低秩数。DyLoRA
    Li等人[[15](#bib.bib15)]尝试使用动态思想来选择LoRA的最佳统一秩值，而不是提前设定，这已被证明对未剪枝模型有效。但它仍然无法适应不同剪枝程度的剪枝层的不同分层需求。此外，其他微调方法，如具有量化的QLoRA
    [[6](#bib.bib6)]和对非冻结层进行重要性采样的LISA [[20](#bib.bib20)]，能够实现高效的微调过程。然而，由于它们都忽视了秩对结构剪枝网络中LoRA微调的影响，因此仍不适合剪枝模型。
- en: Our approach can effectively mitigate the above challenges, we introduce RankAdaptor,
    a hierarchical dynamic fine-tuning method tailored for structural pruned LLMs.
    This approach employs a performance model to allocate optimal rank values to the
    layers of the pruned LLMs, which have been pruned to varying degrees during the
    LoRA fine-tuning process. This allocation aims to enhance the recovered performance
    of the pruned LLMs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法可以有效缓解上述挑战，我们引入了RankAdaptor，这是一种针对结构剪枝LLMs的分层动态微调方法。这种方法采用性能模型来为剪枝LLMs的各层分配最佳秩值，这些模型在LoRA微调过程中被剪枝到不同程度。此分配旨在提高剪枝LLMs的恢复性能。
- en: 'The contribution of this paper can be summarized as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献可以总结如下：
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Hierarchical Dynamic Rank Scheduling: We address the limitations of standard
    LoRA in recovering pruned LLMs by proposing a hierarchical dynamic rank scheduling
    approach. This method tailors the rank values for each layer based on its specific
    recovery needs, improving overall model performance.'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 分层动态秩调度：我们通过提出一种分层动态秩调度方法来解决标准LoRA在恢复剪枝LLMs时的局限性。这种方法根据每一层的特定恢复需求调整秩值，从而提高整体模型性能。
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Automated Performance Model: We develop an end-to-end automatic optimization
    flow utilizing a lightweight performance model. This model dynamically determines
    optimal rank values for each layer, providing solutions for finding the optimal
    rank value set of the pruned LLMs.'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动化性能模型：我们开发了一个端到端的自动优化流程，利用轻量级性能模型。该模型动态确定每层的最优秩值，为找到剪枝后的 LLMs 的最优秩值集提供了解决方案。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Superior Benchmark Performance: Our extensive experiments on benchmarks demonstrate
    that RankAdaptor consistently outperforms standard LoRA across various pruning
    settings and LLM architectures, achieving higher accuracy scores and reducing
    the performance gap between pruned and original models. For example, on the BoolQ
    task, after pruning LLaMA-7B by 20% and 30%, the RankAdaptor recovers 92.13% and
    90.59% of the original model’s accuracy, compared to 86.6% and 85.44% with LoRA.'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 优越的基准性能：我们在基准测试中的广泛实验表明，RankAdaptor 在各种剪枝设置和 LLM 架构中始终优于标准的 LoRA，达到更高的准确率，并缩小了剪枝模型与原始模型之间的性能差距。例如，在
    BoolQ 任务中，将 LLaMA-7B 剪枝 20% 和 30% 后，RankAdaptor 恢复了原始模型 92.13% 和 90.59% 的准确率，而
    LoRA 分别为 86.6% 和 85.44%。
- en: 2 Background and Motivation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与动机
- en: '![Refer to caption](img/83ed24b79ec7d763caf5c696cd7e0ff3.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/83ed24b79ec7d763caf5c696cd7e0ff3.png)'
- en: 'Figure 1: Illustration of the entire process of structural pruning and recovery.
    The baseline approach is detailed in Section [2.1](#S2.SS1 "2.1 Structural Pruning
    Framework ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs"), and the proposed method is described
    in Section [3.2](#S3.SS2 "3.2 RankAdaptor ‣ 3 Method ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：结构剪枝和恢复的整个过程的示意图。基线方法在第 [2.1](#S2.SS1 "2.1 结构剪枝框架 ‣ 2 背景与动机 ‣ RankAdaptor：用于结构剪枝LLMs的分层动态低秩自适应")
    节中详细介绍，提出的方法在第 [3.2](#S3.SS2 "3.2 RankAdaptor ‣ 3 方法 ‣ RankAdaptor：用于结构剪枝LLMs的分层动态低秩自适应")
    节中描述。
- en: 2.1 Structural Pruning Framework
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 结构剪枝框架
- en: 'The structural pruning framework [[17](#bib.bib17)] is designed for task-agnostic
    compression of LLMs. It involves three stages: (1) Discovery stage, where it identifies
    clusters of interconnected structures within the LLM. (2) Estimation stage, assessing
    each cluster’s impact on the model’s performance to determine which to prune.
    (3) Recover stage, which focuses on mitigating the performance loss from pruning
    by applying LoRA fine-tuning.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 结构剪枝框架 [[17](#bib.bib17)] 旨在任务无关的 LLMs 压缩。它涉及三个阶段：（1）发现阶段，识别 LLM 内部的互连结构簇；（2）估计阶段，评估每个簇对模型性能的影响，以确定剪枝对象；（3）恢复阶段，通过应用
    LoRA 微调来减轻剪枝带来的性能损失。
- en: 'Discovery Stage. Pruning starts by establishing structural dependencies. Assume
    neuron $N_{i}$:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 发现阶段。剪枝开始于建立结构依赖关系。假设神经元 $N_{i}$：
- en: '|  | $N_{j}\in\operatorname{Out}(N_{i})\wedge\operatorname{Deg}^{-}(N_{j})=1\Rightarrow
    N_{j}\text{ is dependent on }N_{i}$ |  | (1) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $N_{j}\in\operatorname{Out}(N_{i})\wedge\operatorname{Deg}^{-}(N_{j})=1\Rightarrow
    N_{j}\text{ 依赖于 }N_{i}$ |  | (1) |'
- en: This directional dependency means if the neuron $N_{j}$ must also be pruned
    due to its dependency. Starting with any neuron, it can trigger others that depend
    on it, leading to a cascade of activations. This process identifies and groups
    dependent neurons for pruning. The LLM-Pruner automatically identify and remove
    coupled structures in different LLMs.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方向依赖关系意味着如果神经元 $N_{j}$ 也必须因其依赖性而被剪枝。从任何神经元开始，它可以触发其他依赖于它的神经元，导致激活的级联。这一过程识别并分组依赖神经元以进行剪枝。LLM-Pruner
    自动识别并移除不同 LLM 中的耦合结构。
- en: 'Estimation Stage. After grouping all coupled structures within the model, weights
    within the same group should be pruned simultaneously. Consider a dataset $\mathcal{D}=\{x_{i},y_{i}\}_{i=1}^{N}$
    denotes the weights associated with a structure. To determine the least impactful
    group in model performance, the significance is assessed using the formula with
    Taylor expansion:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 估计阶段。在模型内分组所有耦合结构后，同一组内的权重应同时进行剪枝。考虑一个数据集 $\mathcal{D}=\{x_{i},y_{i}\}_{i=1}^{N}$
    表示与结构相关的权重。为了确定对模型性能影响最小的组，使用泰勒展开公式评估其重要性：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'where $k$ is the loss for next-token predictions. At the end of this stage,
    the importance of each group is determined by aggregating the scores of its constituent
    weights in four way: Summation, Product, Max, and Last-Only. The groups are ranked
    according to their importance, and those with lower significance are pruned based
    on a predefined ratio.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$k$是下一词预测的损失。在这个阶段结束时，每组的重要性通过四种方式聚合其组成权重的分数来确定：求和、乘积、最大值和仅最后。根据重要性对组进行排名，重要性较低的组根据预定义的比例被剪枝。
- en: 'Recover Stage. After pruning the LLM, fine-tuning is necessary to recover the
    model’s performance. The commonly used fine-tuning method is low-rank adaptation,
    LoRA. The objective of this stage is to minimize the performance discrepancy between
    the pruned model and the original model, and ultimately yielding a recovered LLM
    mentioned in Figure [1](#S2.F1 "Figure 1 ‣ 2 Background and Motivation ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 恢复阶段。在剪枝LLM之后，需要进行微调以恢复模型的性能。常用的微调方法是低秩适应，即LoRA。这个阶段的目标是最小化剪枝模型与原始模型之间的性能差异，并最终生成图[1](#S2.F1
    "图 1 ‣ 2 背景和动机 ‣ RankAdaptor：用于结构剪枝LLM的分层动态低秩适应")中提到的恢复LLM。
- en: '![Refer to caption](img/3e13e0b3581ba1e64abd756734164d50.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3e13e0b3581ba1e64abd756734164d50.png)'
- en: 'Figure 2: Schematic diagram of $\Delta W$ in all layers being decomposed into
    low-rank matrices with a fixed rank in LoRA fine-tuning.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：在LoRA微调中，$\Delta W$在所有层中被分解为具有固定秩的低秩矩阵的示意图。
- en: 2.2 Low-Rank Adaptation (LoRA)
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 低秩适应（LoRA）
- en: 'In the context of structural pruning for LLMs, LoRA fine-tuning is employed
    to recover performance with minimal parameter updates. For a pruned LLM consisting
    of $n$, is updated. The forward computation can be expressed as:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM的结构剪枝背景下，LoRA微调被用于在最小参数更新的情况下恢复性能。对于一个剪枝后的LLM，由$n$组成，被更新。前向计算可以表示为：
- en: '|  | $f(x)=(W+\Delta W)X+b=(WX+b)+(AB)X.$ |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(x)=(W+\Delta W)X+b=(WX+b)+(AB)X.$ |  | (3) |'
- en: Given the rank $r$. This optimization can reduce the trainable parameters during
    the learning.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 给定秩$r$。这种优化可以在学习过程中减少可训练参数。
- en: 2.3 A Motivating Example
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 一个激励性的例子
- en: Due to the uneven distribution of importance within the internal architecture
    of LLMs [[34](#bib.bib34)], and the criteria for judgment during the discovery
    and estimation stages of structural pruning are the importance of the structure
    to model performance, pruning operations are conducted unevenly across different
    layers. Thus, the network of pruned LLM becomes highly complex and lacks a unified
    structure. While applying standard LoRA with a uniform rank value across all layers
    can achieve a certain degree of recovery, it fails to adequately meet the unique
    recovery needs of layers pruned to varying extents, resulting in suboptimal recovery
    performance.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 由于LLM内部结构中重要性的分布不均[[34](#bib.bib34)]，以及在结构剪枝的发现和估计阶段的判断标准是结构对模型性能的重要性，剪枝操作在不同层中进行不均匀。因此，剪枝后的LLM网络变得高度复杂，缺乏统一结构。虽然应用具有统一秩值的标准LoRA可以实现一定程度的恢复，但它无法充分满足剪枝程度不同层的独特恢复需求，导致恢复性能次优。
- en: 'Some existing studies [[28](#bib.bib28), [35](#bib.bib35)] indicate that the
    later layers in LLMs tend to capture more complex and semantic information compared
    to the earlier layers, thereby rendering them more consequential. Therefore, we
    use LLaMA-7B pruned 20% as experiment model and maintain the rank of all layers
    to be the fixed values. Then, we allocate rank values in an incremental step-wise
    way from the bottom to the top layers. More details can be found in Figure [2.3](#S2.SS3
    "2.3 A Motivating Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs") and Table [2.3](#S2.SS3
    "2.3 A Motivating Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一些现有研究[[28](#bib.bib28), [35](#bib.bib35)]表明，LLM的后期层比早期层更倾向于捕捉复杂和语义信息，因此它们的影响更为重要。因此，我们使用LLaMA-7B剪枝20%作为实验模型，并保持所有层的秩为固定值。然后，我们以递增逐步的方式从底层到顶层分配秩值。更多细节见图[2.3](#S2.SS3
    "2.3 一个激励性的例子 ‣ 2 背景和动机 ‣ RankAdaptor：用于结构剪枝LLM的分层动态低秩适应")和表[2.3](#S2.SS3 "2.3
    一个激励性的例子 ‣ 2 背景和动机 ‣ RankAdaptor：用于结构剪枝LLM的分层动态低秩适应")。
- en: '![Refer to caption](img/8121e8dbfe5ee05d0e15aba65cdc00e1.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8121e8dbfe5ee05d0e15aba65cdc00e1.png)'
- en: 'Figure 3: Overall performance of seven benchmarks for the different fine-tuning
    configurations. LoRA denotes using fixed ranks for different layers, whereas LoRA*
    indicates using different ranks.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：不同微调配置下七个基准测试的总体性能。LoRA表示对不同层使用固定等级，而LoRA*表示使用不同等级。
- en: '| Layer | Fixed | Different |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 固定 | 不同 |'
- en: '| --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1~8 | 8 | 4 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 1~8 | 8 | 4 |'
- en: '| 9~16 | 6 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 9~16 | 6 |'
- en: '| 17~24 | 10 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 17~24 | 10 |'
- en: '| 25~32 | 12 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 25~32 | 12 |'
- en: 'Table 1: Rank Value Configurations. Fixed refers to applying a constant rank
    value across multiple layers of the pruned model, while Different indicates assigning
    distinct rank values to different layers of the pruned model.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：等级值配置。固定指在剪枝模型的多个层中应用常数等级值，而不同则指将不同的等级值分配给剪枝模型的不同层。
- en: 'The preliminary exploration depicted in Figure [2.3](#S2.SS3 "2.3 A Motivating
    Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs") underscore the efficacy of using a different
    rank allocation for recovering pruned LLMs. The experiments compare three different
    fine-tuning algorithm:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '图[2.3](#S2.SS3 "2.3 A Motivating Example ‣ 2 Background and Motivation ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs")中描述的初步探索强调了使用不同等级分配来恢复剪枝后的LLMs的有效性。实验比较了三种不同的微调算法：'
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Without Fine-Tuning: Represented by the blue color, this approach exhibits
    the lowest performance across all tasks. This indicates that if the pruned model
    is not fine-tuned, there is a significant performance degradation compared to
    the original model.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无微调：用蓝色表示，这种方法在所有任务中表现最差。这表明，如果剪枝后的模型没有经过微调，相比原始模型会出现显著的性能下降。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-Tuning with Fixed Rank: The results depicted in red illustrate the model’s
    performance when a standard LoRA with fixed rank value is applied uniformly across
    all layers. While this approach enhances performance relative to the blue configuration,
    it fails to adequately address the distinct recovery requirements of layers pruned
    to varying degrees. Therefore, despite an overall improvement, the recovery process
    remains suboptimal.'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用固定等级微调：红色所示的结果展示了在所有层上均匀应用固定等级值的标准LoRA时模型的性能。虽然这种方法相较于蓝色配置提高了性能，但未能充分解决对剪枝程度不同的层的不同恢复需求。因此，尽管总体上有所改善，恢复过程仍然不够理想。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Fine-Tuning with Different Rank: Compared with the red configuration, the green
    color demonstrates the higher recovery performance which is achieved by using
    a different rank allocation strategy. This approach involves incrementally adjusting
    the rank values across layers, as detailed in Table [2.3](#S2.SS3 "2.3 A Motivating
    Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs"), thereby adapting to the specific requirements
    of each layer.'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '使用不同等级微调：与红色配置相比，绿色显示了通过使用不同等级分配策略所实现的更高恢复性能。这种方法涉及在各层之间逐步调整等级值，如表[2.3](#S2.SS3
    "2.3 A Motivating Example ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs")所述，从而适应每一层的特定需求。'
- en: According to the motivating example, we have experimentally demonstrated the
    effectiveness of using different rank values for fine-tuning pruned models across
    layers. Based on this finding, we describe our proposed method in the next section,
    which addresses the limitations of the standard LoRA and provides a more tailored
    recovery strategy for pruned LLMs.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据激励示例，我们已经实验性地证明了在层之间使用不同等级值微调剪枝模型的有效性。基于这一发现，我们在下一节中描述了我们提出的方法，它解决了标准LoRA的局限性，并为剪枝后的LLMs提供了更具针对性的恢复策略。
- en: 3 Method
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: Fine-tuning is crucial for recovering the performance of pruned LLMs. However,
    there is currently a lack of efficient fine-tuning methods specifically designed
    for pruned models. To fully harness the potential of pruned models, we propose
    RankAdaptor, a hierarchical dynamic fine-tuning algorithm tailored for pruned
    LLMs. RankAdaptor utilizes a lightweight performance model to automatically identify
    the optimal rank values for each layer, thereby enhancing the performance of pruned
    models. In real-world scenarios, a complete fine-tuning process for a single rank
    configuration often takes several hours, but the performance model can predict
    the downstream task performance of billions of possible combinations in less than
    an hour.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 微调对于恢复剪枝 LLM 的性能至关重要。然而，目前缺乏专门针对剪枝模型的高效微调方法。为了充分发挥剪枝模型的潜力，我们提出了 RankAdaptor，这是一种为剪枝
    LLM 定制的层次动态微调算法。RankAdaptor 利用轻量级性能模型自动识别每一层的最优秩值，从而提升剪枝模型的性能。在实际场景中，单一秩配置的完整微调过程通常需要几个小时，但性能模型可以在不到一小时的时间内预测数十亿个可能组合的下游任务性能。
- en: 3.1 Problem Formulation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题定义
- en: 'The motivating example in Section [2.3](#S2.SS3 "2.3 A Motivating Example ‣
    2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation
    for Structural Pruned LLMs") demonstrates the effectiveness of hierarchical different
    LoRA in pruned model recovery. However, recovering the performance of pruned LLMs
    presents two significant challenges: i) The importance of each layer is difficult
    to evaluate for different tasks, making it challenging to determine the optimal
    hierarchical rank values. ii) The vast solution space created by different potential
    rank values for each layer makes exhaustive evaluation impractical.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 [2.3](#S2.SS3 "2.3 A Motivating Example ‣ 2 Background and Motivation ‣
    RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs")
    节中的激励示例展示了层次不同 LoRA 在剪枝模型恢复中的有效性。然而，恢复剪枝 LLM 的性能面临两个重大挑战：i) 对于不同任务，评估每层的重要性非常困难，这使得确定最佳层次秩值变得具有挑战性。ii)
    每层的不同潜在秩值所创建的广阔解空间使得彻底评估变得不切实际。'
- en: '![Refer to caption](img/cb68eb81aa72f9f8c73dcf2265e32935.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cb68eb81aa72f9f8c73dcf2265e32935.png)'
- en: 'Figure 4: Schematic diagram of $\Delta W$ being decomposed into low-rank matrices
    with different ranks in LoRA fine-tuning.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：$\Delta W$ 在 LoRA 微调中被分解成不同秩的低秩矩阵的示意图。
- en: In the original LLMs, the importance of each layer for downstream tasks varies.
    Structural pruning trims parameters based on their importance, further altering
    it and leading to an uneven distribution of importance across layers. In LoRA,
    the rank $r$ is the number of layers, which is astronomically large. Exhaustively
    evaluating all possible rank combinations is extremely time-consuming. This highlights
    the necessity of developing a more efficient method, and the performance model
    is key to achieving this goal.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在原始 LLM 中，每一层对下游任务的重要性各不相同。结构剪枝根据参数的重要性修剪参数，进一步改变了这一点，导致层之间的重要性分布不均。在 LoRA 中，秩
    $r$ 是层的数量，这个数字极其庞大。彻底评估所有可能的秩组合非常耗时。这突显了开发更高效方法的必要性，而性能模型是实现这一目标的关键。
- en: To address these challenges and incorporate the hierarchical and dynamic nature
    of our approach, we propose an automated end-to-end process that leverages a lightweight
    performance model. Given a pruned model $PL$.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些挑战并融合我们方法的层次性和动态性，我们提出了一种自动化的端到端流程，该流程利用了轻量级性能模型。给定一个剪枝模型 $PL$。
- en: 'Our overall objective is to find the optimal rank set $R_{HD}^{*}$ of all possible
    rank value combinations, which can be expressed as:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的总体目标是找到所有可能秩值组合的最优秩集 $R_{HD}^{*}$，可以表示为：
- en: '|  | $R_{HD}^{*}=\arg\max_{R_{HD}\in S}\mathcal{Q}(R_{HD})$ |  | (4) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $R_{HD}^{*}=\arg\max_{R_{HD}\in S}\mathcal{Q}(R_{HD})$ |  | (4) |'
- en: Due to the vastness of the solution space, we do not search the entire space
    to expedite the solving process. Consequently, $R_{HD}^{*}$ represents a locally
    optimal solution rather than a globally optimal one.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 由于解空间的广阔，我们不对整个空间进行搜索以加快解决过程。因此，$R_{HD}^{*}$ 表示的是局部最优解，而非全局最优解。
- en: '![Refer to caption](img/c8ce7c33e2ba8fbc599cf02e4c6f03b8.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c8ce7c33e2ba8fbc599cf02e4c6f03b8.png)'
- en: 'Figure 5: RankAdaptor Workflow: an end-to-end learning-based algorithm to optimize
    hierarchical dynamic rank values for pruned LLMs.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：RankAdaptor 工作流程：一种端到端的基于学习的算法，用于优化剪枝 LLM 的层次动态秩值。
- en: 3.2 RankAdaptor
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 RankAdaptor
- en: 'We propose a learning-based algorithm in Figure [5](#S3.F5 "Figure 5 ‣ 3.1
    Problem Formulation ‣ 3 Method ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation
    for Structural Pruned LLMs") to solve this problem. We build a performance model
    to estimate the performance of the recovered model on given downstream tasks.
    Suppose $R_{HD}$ is fed back to the performance model as a training set, utilizing
    an incremental learning technique to update the weights. There are three important
    phases in our design.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提出了图[5](#S3.F5 "Figure 5 ‣ 3.1 Problem Formulation ‣ 3 Method ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs")中的一种基于学习的算法来解决这个问题。我们建立了一个性能模型来估计恢复模型在给定下游任务上的表现。假设$R_{HD}$作为训练集反馈到性能模型中，利用增量学习技术来更新权重。我们的设计中有三个重要阶段。'
- en: Initialization Phase. It is necessary to train a lightweight performance model
    to find $R_{HD}^{*}$.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化阶段。需要训练一个轻量级性能模型来找到$R_{HD}^{*}$。
- en: Incremental Learning Phase. In this phase, $R_{HD}$ is fed back into the performance
    model for incremental learning, enabling the model to continuously enhance its
    prediction accuracy over successive iterations.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 增量学习阶段。在此阶段，将$R_{HD}$反馈到性能模型中进行增量学习，使模型能够在连续的迭代中不断提高预测准确性。
- en: Converge Phase. When the gap between $\mathcal{Q}(R_{HD})$.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛阶段。当$\mathcal{Q}(R_{HD})$之间的差距。
- en: 4 Experiments
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Experimental Setup
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: 'LLMs and Benchmarks. To demonstrate the effectiveness of RankAdaptor, we test
    it on three open source large language models: LLaMA-7B [[27](#bib.bib27)], LLaMA-13B
    [[27](#bib.bib27)] and Vicuna-7B [[36](#bib.bib36)], and specific version is in
    the Appendix [D](#A4 "Appendix D Version of LLMs ‣ RankAdaptor: Hierarchical Dynamic
    Low-Rank Adaptation for Structural Pruned LLMs"). We conduct these LLMs on zero-shot
    classification tests for commonsense reasoning datasets, including BoolQ [[3](#bib.bib3)],
    PIQA [[2](#bib.bib2)], HellaSwag [[30](#bib.bib30)], WinoGrande [[21](#bib.bib21)],
    ARC-easy [[4](#bib.bib4)], ARC-challenge [[4](#bib.bib4)], and OpenbookQA [[18](#bib.bib18)].'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLMs和基准测试。为了展示RankAdaptor的有效性，我们在三个开源大型语言模型上进行了测试：LLaMA-7B [[27](#bib.bib27)]，LLaMA-13B
    [[27](#bib.bib27)]和Vicuna-7B [[36](#bib.bib36)]，具体版本见附录[D](#A4 "Appendix D Version
    of LLMs ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs")。我们对这些LLMs进行了零-shot分类测试，涉及常识推理数据集，包括BoolQ [[3](#bib.bib3)]，PIQA [[2](#bib.bib2)]，HellaSwag
    [[30](#bib.bib30)]，WinoGrande [[21](#bib.bib21)]，ARC-easy [[4](#bib.bib4)]，ARC-challenge
    [[4](#bib.bib4)]和OpenbookQA [[18](#bib.bib18)]。'
- en: Details of Performance Model. To validate the effectiveness of RankAdaptor,
    our performance model employs a basic MLP architecture. However, more suitable
    performance models tailored for RankAdaptor are expected to emerge, aiding in
    identifying superior rank sets. The Mixture of Experts (MoE) technique, where
    multiple sub-models are trained on different data subsets and a gating network
    determines which expert to consult for a given input, is a promising approach.
    In our experiment, we construct a performance model for the seven tasks, with
    an outer layer as a loss computation tool influenced by routing factors, and an
    inner layer consisting of seven identical three-layer MLPs. Each inner MLP receives
    a rank set as input and outputs an accuracy. Before the experiment, the model
    undergoes pre-training on a real data subset to achieve a certain predictive accuracy
    level, and the results from each iteration are used for further retraining.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 性能模型的细节。为了验证RankAdaptor的有效性，我们的性能模型采用了基本的MLP架构。然而，预计会出现更适合RankAdaptor的性能模型，有助于识别更优的排名集合。专家混合（MoE）技术，即多个子模型在不同的数据子集上训练，并由一个门控网络决定在给定输入时咨询哪个专家，是一种有前景的方法。在我们的实验中，我们为七个任务构建了一个性能模型，外层作为受路由因素影响的损失计算工具，内层由七个相同的三层MLP组成。每个内部MLP接收一个排名集合作为输入，并输出一个准确度。在实验前，模型在一个真实数据子集上进行了预训练，以达到一定的预测准确性水平，每次迭代的结果用于进一步的再训练。
- en: 'Implementation Details. Our implementation utilizes the following software
    and hardware configurations: PyTorch version 2.1.2; Transformers library version
    4.41.0; PEFT (Parameter-Efficient Fine-Tuning) library version 0.6.0; CUDA version
    12.4; GPU: NVIDIA A800 with 80GB memory; Operating System: Ubuntu. We set the
    number of examples to 10, which is used for estimating the importance of each
    weight group. The estimation stage only uses the first-order Taylor expansion
    in Equation [2](#S2.E2 "In 2.1 Structural Pruning Framework ‣ 2 Background and
    Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs"). The MLP dimensions for the inner layers of the performance model
    are set to (32-32-32-1), meaning each inner MLP consists of three hidden layers
    with 32 neurons and an output layer with a single neuron. Micro-batch size is
    configured to 16, which specifies the number of examples processed in each step
    of model training.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '实施细节。我们的实施使用了以下软件和硬件配置：PyTorch 版本 2.1.2；Transformers 库版本 4.41.0；PEFT（Parameter-Efficient
    Fine-Tuning）库版本 0.6.0；CUDA 版本 12.4；GPU：NVIDIA A800，80GB 内存；操作系统：Ubuntu。我们将样本数量设置为
    10，用于估算每个权重组的重要性。估算阶段仅使用了方程 [2](#S2.E2 "In 2.1 Structural Pruning Framework ‣
    2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation
    for Structural Pruned LLMs") 中的一阶泰勒展开。性能模型的内部层的 MLP 维度设置为 (32-32-32-1)，即每个内部 MLP
    由三个隐藏层（每层 32 个神经元）和一个输出层（单个神经元）组成。微批量大小配置为 16，指定了模型训练每一步处理的样本数量。'
- en: Pruning Rate. Existing research indicates that specific layers of LLaMA-7B,
    Vicuna-7B, and LLaMA-13B are crucial to the models’ architecture and should remain
    unpruned [[17](#bib.bib17)]. Therefore, we prune only layers 5-30 of LLaMA-7B
    and Vicuna-7B, and layers 5-36 of LLaMA-13B to achieve the predefined global pruning
    rate. Specifically, we prune 25%, 32%, 38%, and 63.5% of the middle layers to
    achieve global pruning rates of 20%, 25%, 30%, and 50%. Regarding the unpruned
    layers, we also keep their rank values the same as the standard LoRA.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝率。现有研究表明，LLaMA-7B、Vicuna-7B 和 LLaMA-13B 的特定层对模型架构至关重要，应该保持未剪枝 [[17](#bib.bib17)]。因此，我们仅对
    LLaMA-7B 和 Vicuna-7B 的第 5-30 层以及 LLaMA-13B 的第 5-36 层进行剪枝，以达到预定义的全局剪枝率。具体而言，我们剪枝
    25%、32%、38% 和 63.5% 的中间层，以实现全局剪枝率为 20%、25%、30% 和 50%。对于未剪枝的层，我们也保持其等级值与标准 LoRA
    相同。
- en: Size of the Solution Space. In conventional LoRA, setting fixed rank values
    within the range of 2 to 16 achieves favorable model recovery. To ensure that
    the trainable parameter count of RankAdaptor remains at the same level as conventional
    LoRA, the range of rank values in this experiment for Hierarchical Dynamic LoRA
    was set to $\left\{2,4,6,8,10,12\right\}$. Different models follow the same calculation
    pattern.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案空间的大小。在传统的 LoRA 中，将固定的等级值设置在 2 到 16 之间可以实现良好的模型恢复。为了确保 RankAdaptor 的可训练参数数量与传统
    LoRA 保持在同一水平，此实验中层次动态 LoRA 的等级值范围设置为 $\left\{2,4,6,8,10,12\right\}$。不同模型遵循相同的计算模式。
- en: 4.2 Main Results
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: 'Baseline and Configuration. We utilized the publicly available code from LLMPruner
    [[17](#bib.bib17)], applying LoRA, LoftQ, and Quantimize as fine-tuning methods
    during the recovery phase, and tested them across various benchmarks. Additionally,
    since neither LLaMA nor LLM-Pruner disclosed the relevant testing prompts in their
    papers, we employed Gao et al. [[9](#bib.bib9)] to create open prompts for the
    benchmarks shown in Section [4.1](#S4.SS1 "4.1 Experimental Setup ‣ 4 Experiments
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs").'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '基线和配置。我们利用了来自 LLMPruner [[17](#bib.bib17)] 的公开代码，在恢复阶段应用了 LoRA、LoftQ 和 Quantimize
    作为微调方法，并在各种基准测试中进行了测试。此外，由于 LLaMA 和 LLM-Pruner 在其论文中均未公开相关测试提示，我们采用了 Gao 等人 [[9](#bib.bib9)]
    的方法创建了用于第 [4.1](#S4.SS1 "4.1 Experimental Setup ‣ 4 Experiments ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs") 节中基准测试的开放提示。'
- en: 'Experimental Results. We present the performance of the original LLM, pruned
    LLM and recovered LLM recovered by LoRA and RankAdaptor with the best results
    on each task in Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣
    RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs")
    below, and Table [4](#A1.T4 "Table 4 ‣ A.1 Performance in Vicuna-7B and LLaMA-13B.
    ‣ Appendix A More Results and Analysis ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs"), [5](#A1.T5 "Table 5 ‣ A.1 Performance
    in Vicuna-7B and LLaMA-13B. ‣ Appendix A More Results and Analysis ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs") in the appendix.
    The specific value of $R$ used in each experiment is shown Table [4](#A1.T4 "Table
    4 ‣ A.1 Performance in Vicuna-7B and LLaMA-13B. ‣ Appendix A More Results and
    Analysis ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs") in the appendix.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果。我们展示了原始LLM、剪枝LLM以及通过LoRA和RankAdaptor恢复的LLM在每个任务上的最佳结果，见下表[2](#S4.T2 "表2
    ‣ 4.2 主要结果 ‣ 4 实验 ‣ RankAdaptor：层次动态低秩适应用于结构剪枝LLMs")，以及附录中的表[4](#A1.T4 "表4 ‣ A.1
    Vicuna-7B和LLaMA-13B的性能 ‣ 附录A 更多结果和分析 ‣ RankAdaptor：层次动态低秩适应用于结构剪枝LLMs")，[5](#A1.T5
    "表5 ‣ A.1 Vicuna-7B和LLaMA-13B的性能 ‣ 附录A 更多结果和分析 ‣ RankAdaptor：层次动态低秩适应用于结构剪枝LLMs")。每个实验中使用的$R$的具体值见附录中的表[4](#A1.T4
    "表4 ‣ A.1 Vicuna-7B和LLaMA-13B的性能 ‣ 附录A 更多结果和分析 ‣ RankAdaptor：层次动态低秩适应用于结构剪枝LLMs")。
- en: 'Table 2: Zero-shot performance of LLaMA-7B in unpruned, pruned, LoRA recovery,
    and RankAdaptor recovery. ‘Bold’ indicates the best performance at each pruning
    rate. Results are the optimal performance for each task. Specific $R$ configurations
    in Appendix [B](#A2 "Appendix B Specific 𝑅 configurations in LLaMA-7B Experiments
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs"). Reported in percentage (%).'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：LLaMA-7B在未剪枝、剪枝、LoRA恢复和RankAdaptor恢复下的零-shot性能。‘粗体’表示在每个剪枝率下的最佳表现。结果为每个任务的最佳性能。具体$R$配置见附录[B](#A2
    "附录B LLaMA-7B实验中的具体𝑅配置 ‣ RankAdaptor：层次动态低秩适应用于结构剪枝LLMs")。以百分比（%）报告。
- en: '| Pruning Rate | Recover | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 | 恢复 | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Rate = 0% | - | 73.09 | 78.35 | 72.98 | 67.09 | 67.42 | 41.38 | 42.40 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 率 = 0% | - | 73.09 | 78.35 | 72.98 | 67.09 | 67.42 | 41.38 | 42.40 |'
- en: '| Rate = 20% | w/o Tuning | 56.94 | 75.73 | 66.83 | 60.06 | 60.94 | 36.43 |
    39.80 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 率 = 20% | 无调优 | 56.94 | 75.73 | 66.83 | 60.06 | 60.94 | 36.43 | 39.80 |'
- en: '| LoRA | 63.30 | 76.82 | 68.68 | 63.38 | 63.76 | 37.11 | 40.60 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 63.30 | 76.82 | 68.68 | 63.38 | 63.76 | 37.11 | 40.60 |'
- en: '| RankAdaptor | 67.34 | 77.31 | 69.07 | 64.17 | 65.36 | 37.80 | 41.60 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 67.34 | 77.31 | 69.07 | 64.17 | 65.36 | 37.80 | 41.60 |'
- en: '| Rate = 25% | w/o Tuning | 59.94 | 73.23 | 62.35 | 58.80 | 55.81 | 34.90 |
    39.40 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 率 = 25% | 无调优 | 59.94 | 73.23 | 62.35 | 58.80 | 55.81 | 34.90 | 39.40 |'
- en: '| LoRA | 61.93 | 76.01 | 65.97 | 61.96 | 62.21 | 35.92 | 39.40 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 61.93 | 76.01 | 65.97 | 61.96 | 62.21 | 35.92 | 39.40 |'
- en: '| RankAdaptor | 67.43 | 76.06 | 66.08 | 64.40 | 62.63 | 36.77 | 40.40 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 67.43 | 76.06 | 66.08 | 64.40 | 62.63 | 36.77 | 40.40 |'
- en: '| Rate = 30% | w/o Tuning | 58.96 | 71.22 | 58.10 | 58.88 | 52.19 | 32.34 |
    38.40 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 率 = 30% | 无调优 | 58.96 | 71.22 | 58.10 | 58.88 | 52.19 | 32.34 | 38.40 |'
- en: '| LoRA | 62.45 | 74.37 | 63.14 | 61.96 | 59.22 | 33.70 | 39.60 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 62.45 | 74.37 | 63.14 | 61.96 | 59.22 | 33.70 | 39.60 |'
- en: '| RankAdaptor | 66.21 | 75.19 | 63.61 | 63.14 | 60.10 | 34.64 | 40.20 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 66.21 | 75.19 | 63.61 | 63.14 | 60.10 | 34.64 | 40.20 |'
- en: '| Rate = 50% | w/o Tuning | 57.98 | 60.94 | 34.35 | 52.25 | 31.82 | 27.30 |
    35.80 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 率 = 50% | 无调优 | 57.98 | 60.94 | 34.35 | 52.25 | 31.82 | 27.30 | 35.80 |'
- en: '| LoRA | 43.76 | 69.04 | 45.01 | 50.99 | 45.20 | 28.75 | 34.60 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 43.76 | 69.04 | 45.01 | 50.99 | 45.20 | 28.75 | 34.60 |'
- en: '| RankAdaptor | 51.65 | 69.48 | 45.03 | 51.93 | 45.66 | 28.41 | 35.00 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 51.65 | 69.48 | 45.03 | 51.93 | 45.66 | 28.41 | 35.00 |'
- en: Different Tasks Analysis. Across various tasks, RankAdaptor consistently outperforms
    the LoRA, particularly for tasks like BoolQ, PIQA, HellaSwag, WinoGrande, and
    ARC-easy. For instance, on the BoolQ task with 20% pruning for LLaMA-7B, RankAdaptor
    achieves a 92.1% recovery rate compared to 86.6% for LoRA. This demonstrates RankAdaptor’s
    effectiveness in adapting the rank values to better suit the characteristics of
    different tasks and recover more of the original performance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 不同任务分析。在各种任务中，RankAdaptor 一贯优于 LoRA，尤其是在 BoolQ、PIQA、HellaSwag、WinoGrande 和 ARC-easy
    等任务上。例如，在 LLaMA-7B 的 BoolQ 任务中，20% 剪枝下 RankAdaptor 达到了 92.1% 的恢复率，而 LoRA 为 86.6%。这表明
    RankAdaptor 在调整秩值以更好地适应不同任务特征并恢复更多原始性能方面的有效性。
- en: Different Pruning Rates Analysis. RankAdaptor performs better than LoRA at each
    level of pruning. For example, at a 25% pruning rate, LLaMA-7B recovered 92.26%
    of its original accuracy in the BoolQ task using RankAdaptor, compared to 84.73%
    with LoRA. Across the board, RankAdaptor shows a smaller degradation in performance
    with increasing pruning rates compared to LoRA. For example, in the Vicuna-7B
    model at a 30% pruning rate, RankAdaptor recovers 82.63% of the original performance
    in the HellaSwag task, compared to 80.51% with LoRA.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 不同剪枝率分析。RankAdaptor 在每个剪枝水平上表现优于 LoRA。例如，在 25% 剪枝率下，LLaMA-7B 使用 RankAdaptor
    在 BoolQ 任务中恢复了原始准确率的 92.26%，而使用 LoRA 恢复了 84.73%。总体而言，RankAdaptor 在剪枝率增加时表现出比 LoRA
    更小的性能下降。例如，在 Vicuna-7B 模型中，30% 剪枝率下 RankAdaptor 在 HellaSwag 任务中恢复了原始性能的 82.63%，而
    LoRA 为 80.51%。
- en: Summary. The experimental results clearly indicate that RankAdaptor is a promising
    technique for recovering the performance of pruned LLMs, as evidenced by its consistently
    higher recovery percentages compared to the LoRA across various tasks and pruning
    rates. While the recovery rate gains vary, RankAdaptor consistently demonstrates
    its superiority, making it an efficient fine-tuning method for structural pruned
    LLMs.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。实验结果清楚地表明，RankAdaptor 是恢复剪枝 LLM 性能的有前途的技术，体现在其在各种任务和剪枝率下的恢复百分比始终高于 LoRA。虽然恢复率的提升有所不同，但
    RankAdaptor 一贯展示了其优越性，使其成为结构性剪枝 LLM 的高效微调方法。
- en: 'Generation Comparison. We present a comparison of pruned LLMs recovered by
    RankAdaptor and LoRA for generative tasks in Appendix [C](#A3 "Appendix C Generation
    Comparison. ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs"). Remarkably, the results produced by RankAdaptor are surprisingly
    compelling, further accentuating its effectiveness as an efficient model recovery
    technique.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '生成比较。我们在附录 [C](#A3 "Appendix C Generation Comparison. ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs") 中展示了 RankAdaptor 和 LoRA
    对生成任务的剪枝 LLMs 的比较。值得注意的是，RankAdaptor 产生的结果令人惊讶地引人注目，进一步突显了其作为高效模型恢复技术的有效性。'
- en: 4.3 Ablation Study
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: 'In this part, we use LLaMA-7B with a 20% global pruning rate and the RankAdaptor
    recovery method to conduct all ablation experiments and all results are shown
    in Table [3](#S4.T3 "Table 3 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们使用 LLaMA-7B，剪枝率为 20%，以及 RankAdaptor 恢复方法进行所有消融实验，所有结果见表 [3](#S4.T3
    "Table 3 ‣ 4.3 Ablation Study ‣ 4 Experiments ‣ RankAdaptor: Hierarchical Dynamic
    Low-Rank Adaptation for Structural Pruned LLMs")。'
- en: 'Table 3: Performance comparison for the ablation study under the overall best
    $R$ configuration for seven tasks on LLaMA-7B. The results are reported in percentage
    (%).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：LLaMA-7B 七个任务在最佳 $R$ 配置下的消融研究性能比较。结果以百分比（%）形式报告。
- en: '| Benchmark | Sample Size | Element-wise Importance | Setting of Performance
    Model | Micro-batch Size |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 样本大小 | 元素重要性 | 性能模型设置 | 微批量大小 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| $N$=50 | Element¹ | Element² | Setting1 | Setting2 | Setting3 | Micro-4 |
    Micro-8 | Micro-16 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $N$=50 | Element¹ | Element² | Setting1 | Setting2 | Setting3 | Micro-4 |
    Micro-8 | Micro-16 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| ARC-e | 63.97 | 65.32 | 63.97 | 62.84 | 63.97 | 64.73 | 64.65 | 64.52 | 63.97
    | 65.24 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ARC-e | 63.97 | 65.32 | 63.97 | 62.84 | 63.97 | 64.73 | 64.65 | 64.52 | 63.97
    | 65.24 |'
- en: '| ARC-c | 37.29 | 37.71 | 37.29 | 36.77 | 37.29 | 36.60 | 37.54 | 38.65 | 37.29
    | 37.54 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ARC-c | 37.29 | 37.71 | 37.29 | 36.77 | 37.29 | 36.60 | 37.54 | 38.65 | 37.29
    | 37.54 |'
- en: '| WinoGrande | 63.61 | 63.14 | 63.61 | 63.22 | 63.61 | 63.46 | 63.06 | 62.04
    | 63.61 | 63.14 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| WinoGrande | 63.61 | 63.14 | 63.61 | 63.22 | 63.61 | 63.46 | 63.06 | 62.04
    | 63.61 | 63.14 |'
- en: '| OBQA | 39.80 | 41.00 | 39.80 | 39.80 | 39.80 | 40.80 | 40.80 | 40.00 | 39.80
    | 40.80 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| OBQA | 39.80 | 41.00 | 39.80 | 39.80 | 39.80 | 40.80 | 40.80 | 40.00 | 39.80
    | 40.80 |'
- en: '| BoolQ | 65.81 | 64.43 | 65.81 | 66.48 | 66.91 | 64.43 | 64.86 | 67.28 | 65.81
    | 66.91 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| BoolQ | 65.81 | 64.43 | 65.81 | 66.48 | 66.91 | 64.43 | 64.86 | 67.28 | 65.81
    | 66.91 |'
- en: '| PIQA | 76.99 | 77.15 | 76.99 | 76.82 | 76.99 | 76.99 | 77.04 | 76.50 | 76.99
    | 76.93 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 76.99 | 77.15 | 76.99 | 76.82 | 76.99 | 76.99 | 77.04 | 76.50 | 76.99
    | 76.93 |'
- en: '| HellaSwag | 68.56 | 68.52 | 68.56 | 67.88 | 68.56 | 68.75 | 69.00 | 68.08
    | 68.56 | 68.78 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| HellaSwag | 68.56 | 68.52 | 68.56 | 67.88 | 68.56 | 68.75 | 69.00 | 68.08
    | 68.56 | 68.78 |'
- en: 'Sample Size. We conduct ablation experiments to assess the impact of sample
    size during the estimation phase [2.1](#S2.SS1 "2.1 Structural Pruning Framework
    ‣ 2 Background and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation
    for Structural Pruned LLMs"), specifically comparing performance with $N=10$)
    yields competitive results, such as in WinoGrande. This underscores the need for
    careful selection of sample size based on the specific requirements of the task.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '样本大小。我们进行了消融实验以评估估计阶段样本大小的影响[2.1](#S2.SS1 "2.1结构剪枝框架 ‣ 2 背景与动机 ‣ RankAdaptor:
    结构剪枝LLM的分层动态低秩适应")，具体比较了$N=10$的性能，结果表现具有竞争力，例如在WinoGrande中。这突显了根据任务的具体要求仔细选择样本大小的必要性。'
- en: 'Element-wise Importance.We further conduct tests on the proposed importance
    estimation techniques. The results compare the first-order (Element¹) and second-order
    (Element²) Taylor approximations for evaluating the importance of each parameter,
    as described in Equation [2](#S2.E2 "In 2.1 Structural Pruning Framework ‣ 2 Background
    and Motivation ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs"). Our findings indicate that Element¹ provides better performance
    than Element² across the most benchmarks. While higher-order derivatives may theoretically
    offer more precise adjustments, their complexity may outweigh the marginal performance
    gains observed in practice.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '元素重要性。我们进一步对所提出的重要性估计技术进行了测试。结果比较了用于评估每个参数重要性的一级（Element¹）和二级（Element²）泰勒近似，如公式[2](#S2.E2
    "在2.1结构剪枝框架 ‣ 2 背景与动机 ‣ RankAdaptor: 结构剪枝LLM的分层动态低秩适应")所述。我们的发现表明，Element¹在大多数基准测试中表现优于Element²。虽然高阶导数在理论上可能提供更精确的调整，但其复杂性可能会超出实际中观察到的边际性能提升。'
- en: Setting of Performance Model. To investigate the impact of different inner MLP
    dimensions in the performance model, we test three configurations. The first setting
    consists of three hidden layers with 32 neurons each, followed by an output layer
    with a single neuron, abbreviated as 32-32-32-1\. The other two configurations
    are 32-64-32-1 and 32-16-32-1, following the same notation. The results illustrate
    that varying dimensions of inner MLP layers have nuanced impacts on performance
    across different benchmarks. For inner MLP dimensions, Setting1 provides the highest
    performance on tasks such as ARC-e and BoolQ, while Setting3 shows competitive
    performance on PIQA and HellaSwag.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 性能模型的设置。为了研究不同内部MLP维度对性能模型的影响，我们测试了三种配置。第一个设置由三层隐藏层组成，每层32个神经元，接着是一个输出层，缩写为32-32-32-1。其他两个配置分别为32-64-32-1和32-16-32-1，遵循相同的符号。结果显示，内部MLP层的不同维度对不同基准测试的性能有细微的影响。对于内部MLP维度，设置1在如ARC-e和BoolQ等任务上表现最佳，而设置3在PIQA和HellaSwag上表现具有竞争力。
- en: Micro-batch Sizes. We finally assess the impact of different micro-batch sizes
    (4, 8, and 16). The results indicate that larger micro-batch sizes can lead to
    better performance on certain tasks, though not universally across all benchmarks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 微批次大小。我们最后评估了不同微批次大小（4、8和16）的影响。结果表明，较大的微批次大小在某些任务上可以带来更好的性能，但并非在所有基准测试中都能普遍适用。
- en: 4.4 Discussion
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 讨论
- en: 'We observe that in certain cases, the performance of the pruned model without
    any recovery method can surpass that of the models recovered through LoRA and
    RankAdaptor. As shown in Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs"), when LLaMA-7B is pruned at a 50% rate, its performance on the BoolQ task
    is 57.98%. However, when recovered by LoRA and RankAdaptor, the performances decline
    to 43.76% and 51.65%, respectively. A potential explanation for this phenomenon
    is that the data distribution used for fine-tuning may differ from the BoolQ dataset,
    leading to a negative impact on the pruned model’s performance from LoRA and RankAdaptor.
    It is important to note that RankAdaptor consistently outperforms LoRA when recovery
    methods are employed.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '我们观察到，在某些情况下，没有任何恢复方法的剪枝模型的性能可能会超过通过 LoRA 和 RankAdaptor 恢复的模型。如表 [2](#S4.T2
    "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ RankAdaptor: Hierarchical Dynamic
    Low-Rank Adaptation for Structural Pruned LLMs") 所示，当 LLaMA-7B 以 50% 的剪枝率进行剪枝时，其在
    BoolQ 任务上的表现为 57.98%。然而，当通过 LoRA 和 RankAdaptor 恢复时，性能分别下降到 43.76% 和 51.65%。对此现象的一个可能解释是用于微调的数据分布可能与
    BoolQ 数据集不同，导致 LoRA 和 RankAdaptor 对剪枝模型性能产生负面影响。值得注意的是，当采用恢复方法时，RankAdaptor 一直优于
    LoRA。'
- en: 5 Related Work
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: 5.1 Efficient Pruning of LLMs
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 高效剪枝 LLMs
- en: Structured Pruning. LLM-Pruner [[17](#bib.bib17)] employs structured pruning
    to remove non-essential interconnected structures by utilizing gradient information.
    This approach allows compressed models to restore good performance in multitask
    with basic fine-tuning. Xia et al. [[29](#bib.bib29)] introduces "Sheared LLaMA"
    to compress pre-trained LLMs. It employs dynamic batch loading to improve data
    efficiency during pruning and retraining. This approach retains high performance
    on various tasks with less computational effort than training from scratch. Santacroce
    et al. [[22](#bib.bib22)] presents Globally Unique Movement (GUM), a novel pruning
    technique that focuses on the sensitivity and uniqueness of LLMs’ network components.
    GUM selects models’ neurons that uniquely contribute to model output and are sensitive
    to loss changes to prune, thus maintaining high accuracy. This approach optimizes
    the trade-off between information retention and computational efficiency.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化剪枝。LLM-Pruner [[17](#bib.bib17)] 采用结构化剪枝，通过利用梯度信息来去除非必要的互联结构。这种方法使压缩后的模型在进行基本微调时恢复良好的多任务性能。Xia
    等 [[29](#bib.bib29)] 提出了“剪切 LLaMA”以压缩预训练的 LLMs。它利用动态批量加载来提高剪枝和再训练过程中的数据效率。这种方法在各种任务中保持高性能，且比从头训练所需的计算量更少。Santacroce
    等 [[22](#bib.bib22)] 提出了全球唯一运动（GUM），一种新颖的剪枝技术，专注于 LLMs 网络组件的敏感性和唯一性。GUM 选择对模型输出唯一贡献且对损失变化敏感的神经元进行剪枝，从而保持高准确性。这种方法优化了信息保留与计算效率之间的权衡。
- en: Unstructured Pruning. SparseGPT [[8](#bib.bib8)] is a pruning method that does
    not require retraining. SparseGPT [[8](#bib.bib8)] transforms the pruning process
    into a series of large-scale sparse regression problems, which can be quickly
    solved through Hessian matrix inversion. It efficiently prunes large models to
    high sparsity in a single step while maintaining high accuracy. Wanda [[25](#bib.bib25)]
    prunes LLMs by selectively removing weights based on their sizes and input activations.
    It adaptively adjusts sparsity levels to achieve a reduction of more than half
    without sacrificing accuracy.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 无结构剪枝。SparseGPT [[8](#bib.bib8)] 是一种不需要重新训练的剪枝方法。SparseGPT [[8](#bib.bib8)]
    将剪枝过程转化为一系列大规模稀疏回归问题，这些问题可以通过Hessian矩阵反演快速解决。它高效地在一步中将大型模型剪枝到高稀疏度，同时保持高准确性。Wanda
    [[25](#bib.bib25)] 通过根据权重的大小和输入激活 selectively 剪枝 LLMs。它自适应调整稀疏度水平，以在不牺牲准确性的情况下实现超过一半的减少。
- en: 5.2 Parameter Efficient Fine-Tuning
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 参数高效微调
- en: Houlsby et al. [[11](#bib.bib11)] introduce a transfer learning method that
    integrates adapter modules into pre-trained Transformer models. It can efficiently
    tackle various NLP tasks with few additional parameters and achieving performance
    similar to full fine-tuning. LLM-Adapters [[13](#bib.bib13)] is a method that
    integrates small adapters with few extra parameters to LLMs for efficient fine-tuning.
    This approach allows smaller models to perform as well as larger ones on specific
    reasoning tasks. While the adapter takes a serial approach to integrating trainable
    components into pre-trained Transformer models, low-rank adaptation (LoRA) [[12](#bib.bib12)]
    presents a parallel method of infusing rank decomposition matrices into each layer
    of the model’s architecture. Specifically, LoRA adds trainable matrices to each
    layer of the model and the pre-trained weights are kept the same. LoRA reduces
    the number of trainable parameters compared to fine-tuning the entire model, which
    makes model adaptation faster and less resource-intensive. LoRA-FA [[33](#bib.bib33)]
    freezes the projection-down weight of the low-rank adaptation (LoRA) layers and
    only updates the projection-up weight to reduce the memory requirements for fine-tuning.
    QLora [[6](#bib.bib6)] combines low-rank adapters and quantized 4-bit weights
    to efficient fine-tune LLMs. It significantly reduces the GPU memory requirements
    and achieves performance comparable to full 16-bit fine-tuning. LoftQ [[16](#bib.bib16)]
    applies quantization and low-rank approximation alternatively to obtain an good
    initialization for LoRA fine-tuning. It mitigates the discrepancy between quantized
    weights and pre-trained weights, enabling efficient fine-tuning of quantized models,
    especially in the challenging low-bit regimes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Houlsby 等人 [[11](#bib.bib11)] 介绍了一种转移学习方法，将适配器模块集成到预训练的Transformer模型中。它可以通过少量额外参数有效处理各种NLP任务，并实现类似于完全微调的性能。LLM-Adapters
    [[13](#bib.bib13)] 是一种将少量额外参数的小型适配器集成到LLM中的方法，用于高效微调。这种方法使得较小的模型在特定推理任务上能够表现得与较大的模型一样好。虽然适配器采用串行方法将可训练组件集成到预训练的Transformer模型中，低秩适配（LoRA）
    [[12](#bib.bib12)] 提供了一种将秩分解矩阵注入到模型架构中每一层的并行方法。具体来说，LoRA在模型的每一层中添加了可训练矩阵，预训练的权重保持不变。与完全微调模型相比，LoRA减少了可训练参数的数量，使模型适配更快且资源消耗更少。LoRA-FA
    [[33](#bib.bib33)] 冻结了低秩适配（LoRA）层的投影下重量，只更新投影上重量，以减少微调的内存需求。QLora [[6](#bib.bib6)]
    结合低秩适配器和量化的4位权重来高效微调LLM。它显著减少了GPU内存需求，并实现了与完全16位微调相当的性能。LoftQ [[16](#bib.bib16)]
    交替应用量化和低秩逼近，以获得LoRA微调的良好初始化。它减轻了量化权重与预训练权重之间的差异，使得量化模型的高效微调成为可能，特别是在挑战性的低位位数范围内。
- en: 6 Conclusion
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we introduce RankAdaptor, a novel fine-tuning algorithm tailored
    for recovering the performance of pruned LLMs. RankAdaptor employs a hierarchical
    dynamic fine-tuning strategy leveraging a lightweight performance model to adaptively
    adjust hierarchical rank values. This approach addresses the limitations of standard
    fixed-rank LoRA, which often yields suboptimal performance recovery due to the
    uneven architectural modifications induced by structural pruning. Comprehensive
    evaluations on multiple open-source LLMs and benchmark tasks demonstrate that
    RankAdaptor consistently outperforms standard LoRA across different pruning settings.
    RankAdaptor represents a significant advancement in fine-tuning pruned LLMs. Its
    adaptive rank scheduling and end-to-end optimization provide substantial improvements
    over standard techniques, making it a promising tool for enhancing the performance
    of pruned language models in various applications.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了RankAdaptor，这是一种新颖的微调算法，旨在恢复剪枝后LLM的性能。RankAdaptor采用分层动态微调策略，利用轻量级性能模型自适应地调整分层等级值。这种方法解决了标准固定等级LoRA的局限性，后者由于结构剪枝引起的不均匀架构修改，通常导致性能恢复不佳。对多个开源LLM和基准任务的全面评估表明，RankAdaptor在不同剪枝设置下始终优于标准LoRA。RankAdaptor在微调剪枝LLM方面代表了一个重要的进步。其自适应等级调度和端到端优化相较于标准技术提供了显著的改进，使其成为提升剪枝语言模型在各种应用中性能的有前途的工具。
- en: Limitations. The first step of this work only focus on the designing efficient
    parameter fine-tuning for the LLM structural pruning. A general automatic flow
    with optimal performance model is considered for the various LLM compression tasks
    such as quantization or distillation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 限制。该工作的第一步仅关注设计高效的 LLM 结构剪枝参数微调。考虑到各种 LLM 压缩任务，如量化或蒸馏，提出了一种具有最佳性能模型的通用自动流程。
- en: References
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aycock and Bawden [2024] Seth Aycock and Rachel Bawden. Topic-guided example
    selection for domain adaptation in llm-based machine translation. In *Proceedings
    of the 18th Conference of the European Chapter of the Association for Computational
    Linguistics: Student Research Workshop*, pages 175–195, 2024.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aycock 和 Bawden [2024] 塞斯·艾科克和瑞秋·博登。基于主题的示例选择用于 LLM 基于机器翻译的领域适应。在 *第 18 届欧洲计算语言学协会学生研究研讨会会议论文集*，175–195
    页，2024 年。
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等人 [2020] 约纳坦·比斯克、罗温·泽勒斯、蒋锋、高烨进等人。Piqa: 通过自然语言推理物理常识。在 *AAAI 人工智能会议论文集*，第
    34 卷，7432–7439 页，2020 年。'
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. In *Proceedings of the 2019 Conference of the North
    American Chapter of the Association for Computational Linguistics: Human Language
    Technologies, Volume 1 (Long and Short Papers)*, pages 2924–2936, 2019.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等人 [2019] 克里斯托弗·克拉克、肯顿·李、明伟·张、汤姆·奎亚特科夫斯基、迈克尔·柯林斯、克里斯蒂娜·托塔诺娃。Boolq: 探索自然是/否问题的惊人难度。在
    *2019 年北美计算语言学协会年会: 人类语言技术会议论文集第 1 卷（长短篇论文）*，2924–2936 页，2019 年。'
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人 [2018] 彼得·克拉克、艾萨克·考威、奥伦·埃齐奥尼、图沙尔·科特、阿什什·萨巴瓦尔、卡丽莎·肖尼克、奥文德·塔弗约德。认为你解决了问答问题？试试
    ARC，AI2 推理挑战。在 *arXiv 预印本 arXiv:1803.05457*，2018 年。
- en: Deng et al. [2023] Xiang Deng, Vasilisa Bashlovkina, Feng Han, Simon Baumgartner,
    and Michael Bendersky. What do llms know about financial markets? a case study
    on reddit market sentiment analysis. In *Companion Proceedings of the ACM Web
    Conference 2023*, pages 107–110, 2023.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等人 [2023] 邓翔、瓦西丽莎·巴什洛夫基娜、范汉、西蒙·鲍姆加特纳、迈克尔·本德斯基。大语言模型对金融市场的了解如何？以 Reddit
    市场情绪分析为案例研究。在 *ACM 网络会议 2023 附录会议论文集*，107–110 页，2023 年。
- en: 'Dettmers et al. [2024] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *Advances in Neural
    Information Processing Systems*, 36, 2024.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2024] 提姆·德特梅斯、阿尔蒂多罗·帕尼奥尼、阿里·霍尔茨曼、卢克·泽特尔莫耶。Qlora: 量化 LLM 的高效微调。在
    *神经信息处理系统进展*，第 36 卷，2024 年。'
- en: Fathullah et al. [2024] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng
    Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli,
    et al. Prompting large language models with speech recognition abilities. In *ICASSP
    2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing
    (ICASSP)*, pages 13351–13355\. IEEE, 2024.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fathullah 等人 [2024] 雅西尔·法图拉、吴春阳、埃戈尔·拉科姆金、贾俊腾、尚冠元、李可、郭金熙、熊文瀚、杰伊·马哈德卡尔、欧兹莱姆·卡林利等人。赋予大型语言模型语音识别能力的提示。在
    *ICASSP 2024-2024 IEEE 国际声学、语音与信号处理会议*，13351–13355 页，IEEE，2024 年。
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot. In *International Conference
    on Machine Learning*, pages 10323–10337\. PMLR, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 和 Alistarh [2023] 埃利亚斯·弗兰塔和丹·阿利斯塔赫。Sparsegpt: 大型语言模型可以在一次操作中准确剪枝。在
    *国际机器学习会议*，10323–10337 页，PMLR，2023 年。'
- en: Gao et al. [2023] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model
    evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高等人 [2023] 李奥·高、乔纳森·陶、巴伯·阿巴西、斯特拉·比德曼、席德·布莱克、安东尼·迪波菲、查尔斯·福斯特、劳伦斯·戈尔丁、杰弗里·许、阿兰·勒·诺阿赫、浩南·李、凯尔·麦克唐奈、尼克拉斯·穆宁霍夫、克里斯·奥切帕、杰森·方、拉里亚·雷诺兹、海莉·舍尔科普夫、阿维亚·斯科温、林塘·苏塔维卡、埃里克·唐、阿尼什·泰特、本·王、凯文·王和安迪·邹。用于少样本语言模型评估的框架，2023年12月。网址
    [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)。
- en: 'Gu et al. [2023] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge
    distillation of large language models. In *The Twelfth International Conference
    on Learning Representations*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顾等人 [2023] 余贤·顾、李·董、付如·韦和敏労·黄。Minillm：大型语言模型的知识蒸馏。发表于*第十二届国际学习表征会议*，2023年。
- en: Houlsby et al. [2019] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *International conference
    on machine learning*, pages 2790–2799\. PMLR, 2019.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 霍尔斯比等人 [2019] 尼尔·霍尔斯比、安德烈·久尔久、斯坦尼斯瓦夫·贾斯特热布斯基、布鲁娜·莫罗内、昆汀·德·拉鲁西耶、安德里亚·盖斯蒙多、莫娜·阿塔里扬和西尔万·杰利。用于NLP的参数高效迁移学习。发表于*国际机器学习会议*，页码
    2790–2799，PMLR，2019年。
- en: 'Hu et al. [2021] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language
    models. In *International Conference on Learning Representations*, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等人 [2021] 爱德华·J·胡、菲利普·沃利斯、泽远·艾伦-朱、元志·李、善·王、卢·王、伟柱·陈等。Lora：大型语言模型的低秩适配。发表于*国际学习表征会议*，2021年。
- en: 'Hu et al. [2023] Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim,
    Lidong Bing, Xing Xu, Soujanya Poria, and Roy Lee. Llm-adapters: An adapter family
    for parameter-efficient fine-tuning of large language models. In *Proceedings
    of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages
    5254–5276, 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胡等人 [2023] 朱强·胡、雷·王、依怀·兰、婉瑜·徐、李明·林、黎东·炳、邢·徐、苏佳妮·波里亚和罗伊·李。Llm-adapters：一种用于大型语言模型参数高效微调的适配器家族。发表于*2023年自然语言处理实证方法会议论文集*，页码
    5254–5276，2023年。
- en: Lee et al. [2023] Janghwan Lee, Minsoo Kim, Seungcheol Baek, Seok Hwang, Wonyong
    Sung, and Jungwook Choi. Enhancing computation efficiency in large language models
    through weight and activation quantization. In *Proceedings of the 2023 Conference
    on Empirical Methods in Natural Language Processing*, pages 14726–14739, 2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2023] 张焕·李、敏洙·金、胜哲·白、锡赫·黄、元永·崇和正旭·崔。通过权重和激活量化提升大型语言模型的计算效率。发表于*2023年自然语言处理实证方法会议论文集*，页码
    14726–14739，2023年。
- en: 'Li et al. [2020] Yinghui Li, Jing Yang, and Jiliang Wang. Dylora: Towards energy
    efficient dynamic lora transmission control. In *IEEE INFOCOM 2020-IEEE Conference
    on Computer Communications*, pages 2312–2320\. IEEE, 2020.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2020] 英辉·李、静·杨和季良·王。Dylora：面向能源高效的动态Lora传输控制。发表于*IEEE INFOCOM 2020-IEEE计算机通信会议*，页码
    2312–2320，IEEE，2020年。
- en: 'Li et al. [2023] Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng
    He, Weizhu Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for
    large language models. In *The Twelfth International Conference on Learning Representations*,
    2023.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 李等人 [2023] 一潇·李、艺凡·余、陈·梁、尼科斯·卡兰帕齐亚基斯、彭程·赫、伟柱·陈和拓·赵。Loftq：针对大型语言模型的Lora微调感知量化。发表于*第十二届国际学习表征会议*，2023年。
- en: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 马等人 [2023] 辛银·马、宫凡·方和辛超·王。Llm-pruner：关于大型语言模型的结构化剪枝。*神经信息处理系统进展*，36：21702–21720，2023年。
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. In *Proceedings of the 2018 Conference on Empirical Methods
    in Natural Language Processing*, pages 2381–2391, 2018.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米哈伊洛夫等人 [2018] 托多尔·米哈伊洛夫、彼得·克拉克、图沙尔·霍特和阿希什·萨巴尔瓦尔。盔甲能导电吗？用于开放书籍问答的新数据集。发表于*2018年自然语言处理实证方法会议论文集*，页码
    2381–2391，2018年。
- en: 'Min and Wang [2023] Zeping Min and Jinbo Wang. Exploring the integration of
    large language models into automatic speech recognition systems: An empirical
    study. In *International Conference on Neural Information Processing*, pages 69–84\.
    Springer, 2023.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min 和 Wang [2023] Zeping Min 和 Jinbo Wang. 探索大型语言模型在自动语音识别系统中的集成：一项实证研究。发表于
    *神经信息处理国际会议*，第69–84页。Springer，2023年。
- en: 'Pan et al. [2024] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang,
    Chi Han, and Tong Zhang. Lisa: Layerwise importance sampling for memory-efficient
    large language model fine-tuning. *arXiv preprint arXiv:2403.17919*, 2024.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pan et al. [2024] Rui Pan, Xiang Liu, Shizhe Diao, Renjie Pi, Jipeng Zhang,
    Chi Han, 和 Tong Zhang. Lisa: 分层重要性采样用于内存高效的大型语言模型微调。 *arXiv 预印本 arXiv:2403.17919*，2024年。'
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. Winogrande: 大规模对抗性 Winograd 语法挑战。 *ACM 通讯*，64(9):99–106，2021年。'
- en: Santacroce et al. [2023] Michael Santacroce, Zixin Wen, Yelong Shen, and Yuanzhi
    Li. What matters in the structured pruning of generative language models? *arXiv
    preprint arXiv:2302.03773*, 2023.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Santacroce et al. [2023] Michael Santacroce, Zixin Wen, Yelong Shen, 和 Yuanzhi
    Li. 生成语言模型结构化剪枝中的关键因素是什么？ *arXiv 预印本 arXiv:2302.03773*，2023年。
- en: 'Sato et al. [2020] Shoetsu Sato, Jin Sakuma, Naoki Yoshinaga, Masashi Toyoda,
    and Masaru Kitsuregawa. Vocabulary adaptation for domain adaptation in neural
    machine translation. In *Findings of the Association for Computational Linguistics:
    EMNLP 2020*, pages 4269–4279, 2020.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sato et al. [2020] Shoetsu Sato, Jin Sakuma, Naoki Yoshinaga, Masashi Toyoda,
    和 Masaru Kitsuregawa. 神经机器翻译中的领域适应词汇调整。发表于 *计算语言学协会：EMNLP 2020 会议论文集*，第4269–4279页，2020年。
- en: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. In *The Twelfth International
    Conference on Learning Representations*, 2023.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao et al. [2023] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, 和 Ping Luo. Omniquant: 面向大型语言模型的全向标定量化。发表于
    *第十二届国际学习表征会议*，2023年。'
- en: Sun et al. [2023] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. In *The Twelfth International
    Conference on Learning Representations*, 2023.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2023] Mingjie Sun, Zhuang Liu, Anna Bair, 和 J Zico Kolter. 一种简单有效的大型语言模型剪枝方法。发表于
    *第十二届国际学习表征会议*，2023年。
- en: 'Tan et al. [2023] Shicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Shu
    Zhao, Peng Zhang, and Jie Tang. Gkd: A general knowledge distillation framework
    for large-scale pre-trained language model. In *Proceedings of the 61st Annual
    Meeting of the Association for Computational Linguistics (Volume 5: Industry Track)*,
    pages 134–148, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tan et al. [2023] Shicheng Tan, Weng Lam Tam, Yuanchun Wang, Wenwen Gong, Shu
    Zhao, Peng Zhang, 和 Jie Tang. Gkd: 大规模预训练语言模型的通用知识蒸馏框架。发表于 *第61届计算语言学协会年会（第5卷：行业轨道）*，第134–148页，2023年。'
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, 等等。Llama: 开放且高效的基础语言模型。 *arXiv 预印本 arXiv:2302.13971*，2023年。'
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力即你所需。 *神经信息处理系统进展*，30，2017年。
- en: 'Xia et al. [2023] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared
    llama: Accelerating language model pre-training via structured pruning. In *The
    Twelfth International Conference on Learning Representations*, 2023.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia et al. [2023] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, 和 Danqi Chen. 剪切
    Llama: 通过结构化剪枝加速语言模型预训练。发表于 *第十二届国际学习表征会议*，2023年。'
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800, 2019.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers et al. [2019] 罗万·泽勒斯、阿里·霍尔茨曼、约纳坦·比斯克、阿里·法赫迪和耶金·崔。Hellaswag：机器真的可以完成你的句子吗？载于*第57届计算语言学协会年会论文集*，页码
    4791–4800，2019。
- en: 'Zhang et al. [2023a] Biao Zhang, Barry Haddow, and Alexandra Birch. Prompting
    large language model for machine translation: A case study. In *International
    Conference on Machine Learning*, pages 41092–41110\. PMLR, 2023a.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023a] 张彪、巴里·哈多和亚历山德拉·伯奇。提示大型语言模型进行机器翻译：一个案例研究。载于*国际机器学习会议*，页码 41092–41110。PMLR，2023a。
- en: Zhang et al. [2023b] Boyu Zhang, Hongyang Yang, Tianyu Zhou, Muhammad Ali Babar,
    and Xiao-Yang Liu. Enhancing financial sentiment analysis via retrieval augmented
    large language models. In *Proceedings of the Fourth ACM International Conference
    on AI in Finance*, pages 349–356, 2023b.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023b] 张博宇、杨宏洋、周天宇、穆罕默德·阿里·巴巴尔和刘晓洋。通过检索增强大型语言模型提升金融情感分析。载于*第四届ACM国际金融人工智能会议论文集*，页码
    349–356，2023b。
- en: 'Zhang et al. [2023c] Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu,
    and Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models
    fine-tuning. *arXiv preprint arXiv:2308.03303*, 2023c.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [2023c] 张龙腾、张琳、施绍怀、楚晓文和李博。Lora-fa：用于大型语言模型微调的内存高效低秩适配。*arXiv预印本 arXiv:2308.03303*，2023c。
- en: '[34] Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng,
    Weizhu Chen, and Tuo Zhao. Adaptive budget allocation for parameter-efficient
    fine-tuning. In *The Eleventh International Conference on Learning Representations*.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 张庆如、陈敏硕、亚历山大·布赫林、何鹏程、余城、陈伟柱和赵拓。用于参数高效微调的自适应预算分配。载于*第十一届国际学习表示会议*。'
- en: 'Zhao et al. [2024] Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu, Huiqi Deng,
    Hengyi Cai, Shuaiqiang Wang, Dawei Yin, and Mengnan Du. Explainability for large
    language models: A survey. *ACM Transactions on Intelligent Systems and Technology*,
    15(2):1–38, 2024.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等人 [2024] 赵海燕、陈汉杰、杨凡、刘宁浩、邓慧琪、蔡恒毅、王帅强、尹大伟和杜梦楠。大型语言模型的可解释性：综述。*ACM智能系统与技术汇刊*，15(2)：1–38，2024。
- en: Zheng et al. [2024] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,
    Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al.
    Judging llm-as-a-judge with mt-bench and chatbot arena. *Advances in Neural Information
    Processing Systems*, 36, 2024.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2024] 郑莲敏、姜伟林、盛颖、庄思远、吴张豪、庄永浩、林子、李卓涵、李大成、邢晔等。使用mt-bench和聊天机器人竞技场评估llm-as-a-judge。*神经信息处理系统进展*，36，2024。
- en: Appendix A More Results and Analysis
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 更多结果和分析
- en: A.1 Performance in Vicuna-7B and LLaMA-13B.
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 Vicuna-7B和LLaMA-13B的性能。
- en: 'We list the performance of the configuration described in Section [4.1](#S4.SS1
    "4.1 Experimental Setup ‣ 4 Experiments ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs") for Vicuna-7B in Table [4](#A1.T4 "Table
    4 ‣ A.1 Performance in Vicuna-7B and LLaMA-13B. ‣ Appendix A More Results and
    Analysis ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural
    Pruned LLMs") and LLaMA-13B in Table [5](#A1.T5 "Table 5 ‣ A.1 Performance in
    Vicuna-7B and LLaMA-13B. ‣ Appendix A More Results and Analysis ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs").'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表 [4](#A1.T4 "表 4 ‣ A.1 Vicuna-7B和LLaMA-13B的性能。 ‣ 附录A 更多结果和分析 ‣ RankAdaptor:
    层次动态低秩适配用于结构化剪枝LLMs") 中列出了第 [4.1](#S4.SS1 "4.1 实验设置 ‣ 4 实验 ‣ RankAdaptor: 层次动态低秩适配用于结构化剪枝LLMs")
    节中描述的Vicuna-7B的配置性能，并在表 [5](#A1.T5 "表 5 ‣ A.1 Vicuna-7B和LLaMA-13B的性能。 ‣ 附录A 更多结果和分析
    ‣ RankAdaptor: 层次动态低秩适配用于结构化剪枝LLMs") 中列出了LLaMA-13B的配置性能。'
- en: 'Table 4: Zero-shot performance of Vicuna-7B in unpruned, pruned, pruned with
    LoRA recovery, and pruned with RankAdaptor recovery settings; ’Bold’ represents
    the best performance at the same pruning rate across the three pruned settings.
    The data in the table is from the optimal performance on each individual task.
    The results are reported in percentage (%).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：Vicuna-7B在未剪枝、剪枝、剪枝加LoRA恢复和剪枝加RankAdaptor恢复设置下的零样本性能；‘**粗体**’表示在相同剪枝率下三种剪枝设置中的最佳性能。表中的数据来自每个单独任务的最佳性能。结果以百分比（%）报告。
- en: '| Pruning Rate | Recover | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 | 恢复 | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Rate = 0% | - | 75.69 | 77.75 | 71.06 | 67.80 | 69.07 | 40.78 | 42.20 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 = 0% | - | 75.69 | 77.75 | 71.06 | 67.80 | 69.07 | 40.78 | 42.20 |'
- en: '| Rate = 20% | W/O Tuning | 52.35 | 76.17 | 64.79 | 59.59 | 65.99 | 38.14 |
    40.00 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 = 20% | 无调优 | 52.35 | 76.17 | 64.79 | 59.59 | 65.99 | 38.14 | 40.00 |'
- en: '| LoRA | 57.77 | 77.58 | 67.16 | 63.14 | 67.30 | 37.71 | 40.40 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 57.77 | 77.58 | 67.16 | 63.14 | 67.30 | 37.71 | 40.40 |'
- en: '| RankAdaptor | 61.19 | 77.15 | 67.32 | 63.85 | 67.68 | 38.05 | 41.20 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 61.19 | 77.15 | 67.32 | 63.85 | 67.68 | 38.05 | 41.20 |'
- en: '| Rate = 25% | W/O Tuning | 43.98 | 74.76 | 61.58 | 57.06 | 63.72 | 37.20 |
    39.80 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 = 25% | 无调优 | 43.98 | 74.76 | 61.58 | 57.06 | 63.72 | 37.20 | 39.80 |'
- en: '| LoRA | 50.34 | 75.24 | 64.10 | 61.33 | 63.93 | 35.67 | 40.60 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 50.34 | 75.24 | 64.10 | 61.33 | 63.93 | 35.67 | 40.60 |'
- en: '| RankAdaptor | 58.50 | 76.17 | 64.23 | 61.96 | 63.30 | 36.01 | 42.00 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 58.50 | 76.17 | 64.23 | 61.96 | 63.30 | 36.01 | 42.00 |'
- en: '| Rate = 30% | W/O Tuning | 43.12 | 73.45 | 55.64 | 57.22 | 58.96 | 34.30 |
    37.80 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 = 30% | 无调优 | 43.12 | 73.45 | 55.64 | 57.22 | 58.96 | 34.30 | 37.80 |'
- en: '| LoRA | 58.81 | 74.37 | 60.70 | 60.62 | 59.01 | 33.79 | 38.80 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 58.81 | 74.37 | 60.70 | 60.62 | 59.01 | 33.79 | 38.80 |'
- en: '| RankAdaptor | 57.58 | 75.57 | 61.63 | 60.22 | 60.94 | 34.81 | 39.00 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 57.58 | 75.57 | 61.63 | 60.22 | 60.94 | 34.81 | 39.00 |'
- en: '| Rate = 50% | W/O Tuning | 62.29 | 60.28 | 33.91 | 54.54 | 35.14 | 28.16 |
    33.80 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 = 50% | 无调优 | 62.29 | 60.28 | 33.91 | 54.54 | 35.14 | 28.16 | 33.80 |'
- en: '| LoRA | 59.51 | 66.87 | 43.18 | 52.01 | 48.40 | 26.45 | 34.00 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 59.51 | 66.87 | 43.18 | 52.01 | 48.40 | 26.45 | 34.00 |'
- en: '| RankAdaptor | 59.91 | 67.46 | 43.50 | 52.41 | 48.70 | 27.65 | 35.80 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 59.91 | 67.46 | 43.50 | 52.41 | 48.70 | 27.65 | 35.80 |'
- en: 'Table 5: Zero-shot performance of LLaMA-13B in unpruned, pruned, pruned with
    LoRA recovery, and pruned with RankAdaptor recovery settings; ’Bold’ represents
    the best performance at the same pruning rate across the three pruned settings.
    The data in the table is from the optimal performance on each individual task.
    The results are reported in percentage (%).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：LLaMA-13B 在未剪枝、剪枝、剪枝加 LoRA 恢复和剪枝加 RankAdaptor 恢复设置下的零样本性能；‘**Bold**’ 表示在相同剪枝率下三种剪枝设置中的最佳性能。表中的数据来自每个单独任务的最佳性能。结果以百分比（%）报告。
- en: '| Pruning Rate | Recover | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 | 恢复 | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Rate = 0% | N/A | 68.50 | 79.11 | 76.21 | 70.09 | 74.58 | 44.54 | 42.20 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 = 0% | 无适用 | 68.50 | 79.11 | 76.21 | 70.09 | 74.58 | 44.54 | 42.20 |'
- en: '| Rate = 50% | W/O Tuning | 58.90 | 66.32 | 42.24 | 52.25 | 39.06 | 29.18 |
    33.20 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 = 50% | 无调优 | 58.90 | 66.32 | 42.24 | 52.25 | 39.06 | 29.18 | 33.20 |'
- en: '| LoRA | 61.93 | 71.38 | 53.36 | 53.59 | 53.11 | 29.95 | 38.00 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 61.93 | 71.38 | 53.36 | 53.59 | 53.11 | 29.95 | 38.00 |'
- en: '| RankAdaptor | 62.05 | 71.71 | 53.33 | 54.22 | 53.20 | 30.89 | 39.40 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 62.05 | 71.71 | 53.33 | 54.22 | 53.20 | 30.89 | 39.40 |'
- en: Performance Analysis of Vicuna-7B. At pruning rates of 20% and 25%, RankAdaptor
    shows a noticeable improvement over LoRA, achieving the best performance on most
    tasks. For instance, on the BoolQ task with a 20% pruning rate, RankAdaptor reaches
    61.19%, while LoRA only achieves 57.77%. As the pruning rate increases to 30%,
    the performance gap between RankAdaptor and LoRA narrows, but RankAdaptor still
    maintains a slight advantage on most tasks. When the pruning rate reaches 50%,
    RankAdaptor outperforms LoRA only on the PIQA and OpenbookQA tasks, while performing
    comparably or slightly worse on the other tasks. This suggests that at extreme
    pruning conditions, the advantage of RankAdaptor is not as prominent. Overall,
    RankAdaptor exhibits a certain advantage over LoRA, but this advantage tends to
    diminish as the pruning rate increases.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Vicuna-7B 性能分析。在 20% 和 25% 的剪枝率下，RankAdaptor 相比 LoRA 显示出显著的改善，在大多数任务上表现最佳。例如，在
    20% 剪枝率的 BoolQ 任务中，RankAdaptor 达到 61.19%，而 LoRA 仅为 57.77%。随着剪枝率增加到 30%，RankAdaptor
    和 LoRA 之间的性能差距缩小，但 RankAdaptor 在大多数任务上仍保持轻微优势。当剪枝率达到 50% 时，RankAdaptor 仅在 PIQA
    和 OpenbookQA 任务上优于 LoRA，而在其他任务上表现相当或略差。这表明在极端剪枝条件下，RankAdaptor 的优势不再显著。总体而言，RankAdaptor
    对 LoRA 展示了一定的优势，但随着剪枝率增加，这种优势趋于减弱。
- en: Performance Analysis of LLaMA-7B. At the 50% pruning rate, RankAdaptor outperforms
    LoRA on the BoolQ, PIQA, WinoGrande, ARC-easy, ARC-challenge, and OpenbookQA tasks
    but falls slightly behind on the HellaSwag task. This indicates that even in extreme
    pruning conditions, RankAdaptor can maintain superior performance over LoRA on
    most tasks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA-7B 性能分析。在 50% 剪枝率下，RankAdaptor 在 BoolQ、PIQA、WinoGrande、ARC-easy、ARC-challenge
    和 OpenbookQA 任务上优于 LoRA，但在 HellaSwag 任务上稍逊一筹。这表明即使在极端剪枝条件下，RankAdaptor 在大多数任务上仍能保持优于
    LoRA 的性能。
- en: A.2 Overall Optimal Performance across Multiple Tasks.
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 多任务整体最优性能。
- en: 'In the experiment, we also use the performance model with RankAdaptor to achieve
    overall optimization on multiple tasks. Table [6](#A1.T6 "Table 6 ‣ A.2 Overall
    Optimal Performance across Multiple Tasks. ‣ Appendix A More Results and Analysis
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs") shows the performance of each task and average performance of seven tasks
    in the comprehensive optimization results.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在实验中，我们还使用了带有 RankAdaptor 的性能模型，以实现多个任务的整体优化。表[6](#A1.T6 "表 6 ‣ A.2 多任务整体最优性能
    ‣ 附录 A 更多结果和分析 ‣ RankAdaptor：结构剪枝 LLMs 的分层动态低秩适应")展示了每个任务的性能以及七个任务在综合优化结果中的平均性能。
- en: 'Table 6: Zero-shot performance of LLaMA-7B in unpruned, pruned, pruned with
    LoRA recovery, and pruned with RankAdaptor recovery settings; ’Bold’ represents
    the best performance at the same pruning rate across the three pruned settings.
    The data in the table is from the overall performance across multiple tasks. The
    results are reported in percentage (%).'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：LLaMA-7B 在未剪枝、剪枝、使用 LoRA 恢复剪枝以及使用 RankAdaptor 恢复剪枝设置下的零样本性能；’**粗体**’代表在三个剪枝设置中相同剪枝比率下的最佳性能。表中的数据来源于多个任务的整体性能。结果以百分比（%）报告。
- en: '| Pruning Rate | Recover | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e |
    ARC-c | OBQA | Average |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝比率 | 恢复 | BoolQ | PIQA | HellaSwag | WinoGrande | ARC-e | ARC-c | OBQA
    | 平均值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Rate = 0% | N/A | 73.09 | 78.35 | 72.98 | 67.09 | 67.42 | 41.38 | 42.40 |
    63.10 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 0% | 不适用 | 73.09 | 78.35 | 72.98 | 67.09 | 67.42 | 41.38 | 42.40 | 63.10
    |'
- en: '| Rate = 20% | W/O Tuning | 56.94 | 75.73 | 66.83 | 60.06 | 60.94 | 36.43 |
    39.80 | 56.96 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 20% | 无调整 | 56.94 | 75.73 | 66.83 | 60.06 | 60.94 | 36.43 | 39.80 |
    56.96 |'
- en: '| LoRA | 63.30 | 76.82 | 68.68 | 63.38 | 63.76 | 37.11 | 40.60 | 59.52 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 63.30 | 76.82 | 68.68 | 63.38 | 63.76 | 37.11 | 40.60 | 59.52 |'
- en: '| RankAdaptor | 66.91 | 76.93 | 68.78 | 63.14 | 65.24 | 37.54 | 40.80 | 59.90
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 66.91 | 76.93 | 68.78 | 63.14 | 65.24 | 37.54 | 40.80 | 59.90
    |'
- en: '| Rate = 25% | W/O Tuning | 59.94 | 73.23 | 62.35 | 58.80 | 55.81 | 34.90 |
    39.40 | 54.63 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 25% | 无调整 | 59.94 | 73.23 | 62.35 | 58.80 | 55.81 | 34.90 | 39.40 |
    54.63 |'
- en: '| LoRA | 61.93 | 76.01 | 65.97 | 61.96 | 62.21 | 35.92 | 39.40 | 57.63 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 61.93 | 76.01 | 65.97 | 61.96 | 62.21 | 35.92 | 39.40 | 57.63 |'
- en: '| RankAdaptor | 63.15 | 75.95 | 65.97 | 62.04 | 62.63 | 36.77 | 39.60 | 58.16
    |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 63.15 | 75.95 | 65.97 | 62.04 | 62.63 | 36.77 | 39.60 | 58.16
    |'
- en: '| Rate = 30% | W/O Tuning | 58.96 | 71.22 | 58.10 | 58.88 | 52.19 | 32.34 |
    38.40 | 52.58 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 30% | 无调整 | 58.96 | 71.22 | 58.10 | 58.88 | 52.19 | 32.34 | 38.40 |
    52.58 |'
- en: '| LoRA | 62.45 | 74.37 | 63.14 | 61.96 | 59.22 | 33.70 | 39.60 | 56.63 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 62.45 | 74.37 | 63.14 | 61.96 | 59.22 | 33.70 | 39.60 | 56.63 |'
- en: '| RankAdaptor | 62.72 | 74.39 | 63.61 | 63.14 | 60.10 | 33.62 | 40.20 | 56.97
    |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 62.72 | 74.39 | 63.61 | 63.14 | 60.10 | 33.62 | 40.20 | 56.97
    |'
- en: '| Rate = 50% | W/O Tuning | 57.98 | 60.94 | 34.35 | 52.25 | 31.82 | 27.30 |
    35.80 | 42.92 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 比率 = 50% | 无调整 | 57.98 | 60.94 | 34.35 | 52.25 | 31.82 | 27.30 | 35.80 |
    42.92 |'
- en: '| LoRA | 43.76 | 69.04 | 45.01 | 50.99 | 45.20 | 28.75 | 34.60 | 45.62 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 43.76 | 69.04 | 45.01 | 50.99 | 45.20 | 28.75 | 34.60 | 45.62 |'
- en: '| RankAdaptor | 51.65 | 69.48 | 45.03 | 51.93 | 45.66 | 28.41 | 35.00 | 46.74
    |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| RankAdaptor | 51.65 | 69.48 | 45.03 | 51.93 | 45.66 | 28.41 | 35.00 | 46.74
    |'
- en: 'The results in Table [6](#A1.T6 "Table 6 ‣ A.2 Overall Optimal Performance
    across Multiple Tasks. ‣ Appendix A More Results and Analysis ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs") has demonstrated RankAdaptor
    has outperformed the LoRA, showcasing its ability to effectively balance and optimize
    overall performance when considering multiple tasks simultaneously. While its
    advantage over LoRA has diminished at the extreme 50% pruning rate, RankAdaptor
    has still achieved a higher average accuracy. Notably, at the 20% pruning rate,
    it has recovered most of the original model’s performance with an average accuracy
    of 59.90%, only 3.2 percentage points lower than the unpruned model’s 63.10%.
    Throughout all pruning rates, RankAdaptor has significantly surpassed the unpruned
    models without tuning, highlighting its capability as an efficient model recovery
    method. Although it may not outperform LoRA on every individual task, RankAdaptor
    has demonstrated its ability to strike a better overall performance balance across
    multiple tasks, making it a promising solution for practical large language model
    deployment in resource-constrained environments.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [6](#A1.T6 "Table 6 ‣ A.2 Overall Optimal Performance across Multiple Tasks.
    ‣ Appendix A More Results and Analysis ‣ RankAdaptor: Hierarchical Dynamic Low-Rank
    Adaptation for Structural Pruned LLMs") 中的结果展示了 RankAdaptor 的表现优于 LoRA，展示了其在考虑多个任务时有效平衡和优化整体性能的能力。虽然在极端的
    50% 剪枝率下其相对于 LoRA 的优势有所减弱，但 RankAdaptor 仍实现了更高的平均准确率。值得注意的是，在 20% 剪枝率下，它恢复了大部分原始模型的性能，平均准确率为
    59.90%，仅比未剪枝模型的 63.10% 低 3.2 个百分点。在所有剪枝率下，RankAdaptor 显著超越了未经调整的模型，突显了其作为高效模型恢复方法的能力。尽管它可能在每个单独任务上未必超越
    LoRA，但 RankAdaptor 展示了其在多个任务中取得更好整体性能平衡的能力，使其成为资源受限环境中大语言模型部署的有前景的解决方案。'
- en: Appendix B Specific $R$ configurations in LLaMA-7B Experiments
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B LLaMA-7B 实验中特定 $R$ 配置
- en: 'In Table [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs"), we present
    the optimal performance achieved by RankAdaptor on each task. Table [7](#A2.T7
    "Table 7 ‣ Appendix B Specific 𝑅 configurations in LLaMA-7B Experiments ‣ RankAdaptor:
    Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned LLMs") displays
    the rank configurations corresponding to all reported performance results. We
    make these rank configurations publicly available to foster reproducibility and
    enable further research by other scholars.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [2](#S4.T2 "Table 2 ‣ 4.2 Main Results ‣ 4 Experiments ‣ RankAdaptor: Hierarchical
    Dynamic Low-Rank Adaptation for Structural Pruned LLMs") 中，我们展示了 RankAdaptor 在每个任务上实现的最佳性能。表格
    [7](#A2.T7 "Table 7 ‣ Appendix B Specific 𝑅 configurations in LLaMA-7B Experiments
    ‣ RankAdaptor: Hierarchical Dynamic Low-Rank Adaptation for Structural Pruned
    LLMs") 展示了所有报告的性能结果对应的排名配置。我们将这些排名配置公开，以促进可重复性并支持其他学者的进一步研究。'
- en: 'Table 7: Specific Composition of $R$ in LLaMA-7B Experiment'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: LLaMA-7B 实验中 $R$ 的具体组成'
- en: '| Pruning Rate | Tasks | Layers’ Rank Values (1~32/40) |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝率 | 任务 | 层的排名值（1~32/40）'
- en: '| --- | --- | --- |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 20% | BoolQ | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2,
    8, 2, 8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 20% | BoolQ | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2,
    8, 2, 8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
- en: '| PIQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
- en: '| Hella | 8, 8, 8, 8, 2, 2, 4, 10, 10, 6, 10, 10, 10, 6, 6, 2, 2, 10, 2, 4,
    2, 10, 10, 10, 4, 10, 10, 6, 6, 2, 8, 8 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Hella | 8, 8, 8, 8, 2, 2, 4, 10, 10, 6, 10, 10, 10, 6, 6, 2, 2, 10, 2, 4,
    2, 10, 10, 10, 4, 10, 10, 6, 6, 2, 8, 8 |'
- en: '| Wino | 8, 8, 8, 8, 8, 10, 4, 10, 4, 6, 6, 2, 10, 8, 12, 12, 10, 12, 12, 10,
    6, 6, 8, 8, 10, 6, 6, 12, 2, 8, 8, 8 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| Wino | 8, 8, 8, 8, 8, 10, 4, 10, 4, 6, 6, 2, 10, 8, 12, 12, 10, 12, 12, 10,
    6, 6, 8, 8, 10, 6, 6, 12, 2, 8, 8, 8 |'
- en: '| ARC-e | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| ARC-e | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
- en: '| ARC-c | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| ARC-c | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
- en: '| OBQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| OBQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 10, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2,
    8, 10, 12, 10, 6, 4, 4, 4, 2, 12, 8, 8 |'
- en: '| 25% | BoolQ | 8, 8, 8, 8, 12, 2, 8, 2, 8, 12, 4, 2, 10, 12, 10, 4, 2, 2,
    12, 8, 10, 2, 12, 12, 8, 4, 4, 2, 2, 12, 8, 8 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 25% | BoolQ | 8, 8, 8, 8, 12, 2, 8, 2, 8, 12, 4, 2, 10, 12, 10, 4, 2, 2,
    12, 8, 10, 2, 12, 12, 8, 4, 4, 2, 2, 12, 8, 8 |'
- en: '| PIQA | 8, 8, 8, 8, 4, 2, 2, 10, 10, 2, 10, 10, 10, 2, 2, 2, 4, 10, 4, 6,
    10, 2, 2, 6, 10, 2, 2, 10, 10, 2, 8, 8 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 8, 8, 8, 8, 4, 2, 2, 10, 10, 2, 10, 10, 10, 2, 2, 2, 4, 10, 4, 6,
    10, 2, 2, 6, 10, 2, 2, 10, 10, 2, 8, 8 |'
- en: '| Hella | 8, 8, 8, 8, 4, 10, 12, 12, 6, 10, 6, 6, 8, 2, 2, 12, 2, 12, 12, 6,
    4, 10, 6, 2, 2, 8, 4, 2, 2, 8, 8, 8 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| Hella | 8, 8, 8, 8, 4, 10, 12, 12, 6, 10, 6, 6, 8, 2, 2, 12, 2, 12, 12, 6,
    4, 10, 6, 2, 2, 8, 4, 2, 2, 8, 8, 8 |'
- en: '| Wino | 8, 8, 8, 8, 8, 4, 12, 8, 2, 2, 12, 2, 10, 12, 2, 12, 12, 10, 8, 12,
    4, 6, 6, 4, 10, 4, 2, 10, 10, 12, 8, 8 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| Wino | 8, 8, 8, 8, 8, 4, 12, 8, 2, 2, 12, 2, 10, 12, 2, 12, 12, 10, 8, 12,
    4, 6, 6, 4, 10, 4, 2, 10, 10, 12, 8, 8 |'
- en: '| ARC-e | 8, 8, 8, 8, 2, 12, 2, 6, 12, 6, 12, 10, 6, 4, 8, 8, 12, 2, 2, 6,
    8, 4, 12, 12, 2, 4, 2, 6, 6, 2, 8, 8 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| ARC-e | 8, 8, 8, 8, 2, 12, 2, 6, 12, 6, 12, 10, 6, 4, 8, 8, 12, 2, 2, 6,
    8, 4, 12, 12, 2, 4, 2, 6, 6, 2, 8, 8 |'
- en: '| ARC-c | 8, 8, 8, 8, 2, 12, 2, 6, 12, 6, 12, 10, 6, 4, 8, 8, 12, 2, 2, 6,
    8, 4, 12, 12, 2, 4, 2, 6, 6, 2, 8, 8 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| ARC-c | 8, 8, 8, 8, 2, 12, 2, 6, 12, 6, 12, 10, 6, 4, 8, 8, 12, 2, 2, 6,
    8, 4, 12, 12, 2, 4, 2, 6, 6, 2, 8, 8 |'
- en: '| OBQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 1, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2, 8,
    10, 12, 12, 10, 4, 4, 6, 2, 12, 8, 8 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| OBQA | 8, 8, 8, 8, 4, 12, 12, 12, 12, 1, 8, 10, 6, 2, 8, 6, 8, 2, 8, 2, 8,
    10, 12, 12, 10, 4, 4, 6, 2, 12, 8, 8 |'
- en: '| 30% | BoolQ | 8, 8, 8, 8, 12, 2, 8, 2, 8, 12, 4, 2, 10, 12, 10, 4, 2, 2,
    12, 8, 10, 2, 12, 12, 8, 4, 4, 2, 2, 12, 8, 8 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 30% | BoolQ | 8, 8, 8, 8, 12, 2, 8, 2, 8, 12, 4, 2, 10, 12, 10, 4, 2, 2,
    12, 8, 10, 2, 12, 12, 8, 4, 4, 2, 2, 12, 8, 8 |'
- en: '| PIQA | 8, 8, 8, 8, 12, 6, 10, 4, 2, 4, 2, 4, 12, 8, 2, 2, 2, 12, 12, 12,
    12, 2, 12, 4, 4, 2, 10, 2, 2, 8, 8, 8 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 8, 8, 8, 8, 12, 6, 10, 4, 2, 4, 2, 4, 12, 8, 2, 2, 2, 12, 12, 12,
    12, 2, 12, 4, 4, 2, 10, 2, 2, 8, 8, 8 |'
- en: '| Hella | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| Hella | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
- en: '| Wino | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| Wino | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
- en: '| ARC-e | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| ARC-e | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
- en: '| ARC-c | 8, 8, 8, 8, 4, 10, 12, 12, 6, 10, 6, 6, 8, 2, 2, 12, 2, 12, 12, 6,
    4, 10, 6, 2, 2, 8, 4, 2, 2, 8, 8, 8 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| ARC-c | 8, 8, 8, 8, 4, 10, 12, 12, 6, 10, 6, 6, 8, 2, 2, 12, 2, 12, 12, 6,
    4, 10, 6, 2, 2, 8, 4, 2, 2, 8, 8, 8 |'
- en: '| OBQA | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| OBQA | 8, 8, 8, 8, 12, 6, 8, 4, 2, 12, 10, 4, 4, 2, 6, 4, 6, 10, 4, 2, 8,
    6, 12, 10, 4, 6, 6, 6, 8, 2, 8, 8 |'
- en: '| 50% | BoolQ | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6,
    4, 2, 6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 50% | BoolQ | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6,
    4, 2, 6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
- en: '| PIQA | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
- en: '| Hella | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| Hella | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
- en: '| Wino | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| Wino | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
- en: '| ARC-e | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| ARC-e | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
- en: '| ARC-c | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| ARC-c | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2,
    6, 2, 4, 2, 6, 10, 10, 4,'
- en: '| OBQA | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| OBQA | 8, 8, 8, 8, 12, 4, 6, 2, 2, 10, 4, 6, 12, 12, 2, 2, 12, 6, 4, 2, 6,
    2, 4, 2, 6, 10, 10, 4, 2, 2, 8, 8 |'
- en: Appendix C Generation Comparison.
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 生成比较。
- en: Complementing the evaluation of model performance on classification tasks in
    the experiments, we further investigate the generative capabilities of the recovered
    models. Notably, we conduct text generation tasks using LLaMA-7B and Vicuna-7B
    models recovered by LoRA and RankAdaptor at a 20% pruning rate. The results are
    remarkably promising. For article continuation, the models recovered by RankAdaptor
    demonstrate superior coherence in their generated sentences. Similarly, when tasked
    with step listing, RankAdaptor-recovered LLMs produce clearer and more logical
    step sequences. These compelling comparative results are illustrated in Figures
    1 and 2, showcasing the potential of RankAdaptor in preserving and enhancing generative
    abilities during model compression and recovery.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在实验中对分类任务的模型性能进行评估外，我们进一步调查了恢复模型的生成能力。特别是，我们使用 LoRA 和 RankAdaptor 在 20% 剪枝率下恢复的
    LLaMA-7B 和 Vicuna-7B 模型进行文本生成任务。结果非常有希望。对于文章续写任务，由 RankAdaptor 恢复的模型在生成句子的连贯性方面表现优越。同样，在步骤列举任务中，RankAdaptor
    恢复的 LLM 生成的步骤序列更清晰且更具逻辑。这些令人信服的比较结果在图 1 和图 2 中展示了 RankAdaptor 在模型压缩和恢复过程中保留和增强生成能力的潜力。
- en: '![Refer to caption](img/4e9b22e59b2ce213996ff540ddd3a235.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/4e9b22e59b2ce213996ff540ddd3a235.png)'
- en: 'Figure 6: Article continuation task comparison in LLaMA-7B'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: LLaMA-7B 中的文章续写任务比较'
- en: '![Refer to caption](img/547aa92323e76c316ebc16fd616a88ff.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/547aa92323e76c316ebc16fd616a88ff.png)'
- en: 'Figure 7: Step listing task comparison in Vicuna-7B'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: Vicuna-7B 中的步骤列举任务比较'
- en: Appendix D Version of LLMs
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D LLM 版本
- en: 'We provide the Hugging Face link of LLMs used in the experiment: LLaMA-7B:
    [https://huggingface.co/baffo32/decapoda-research-llama-7B-hf](https://huggingface.co/baffo32/decapoda-research-llama-7B-hf);
    Vicuna-7B: [https://huggingface.co/yahma/llama-13b-hf](https://huggingface.co/yahma/llama-13b-hf);
    LLaMA-13B: [https://huggingface.co/lmsys/vicuna-13b-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '我们提供了实验中使用的 LLM 的 Hugging Face 链接：LLaMA-7B: [https://huggingface.co/baffo32/decapoda-research-llama-7B-hf](https://huggingface.co/baffo32/decapoda-research-llama-7B-hf)；Vicuna-7B:
    [https://huggingface.co/yahma/llama-13b-hf](https://huggingface.co/yahma/llama-13b-hf)；LLaMA-13B:
    [https://huggingface.co/lmsys/vicuna-13b-v1.5](https://huggingface.co/lmsys/vicuna-13b-v1.5)'
