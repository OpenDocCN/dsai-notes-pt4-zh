- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:50:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ZeroQuant(4+2)：通过新的以FP6为中心的策略重新定义LLM的量化，以应对多样化的生成任务
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08583](https://ar5iv.labs.arxiv.org/html/2312.08583)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.08583](https://ar5iv.labs.arxiv.org/html/2312.08583)
- en: \useunderXiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash
    Bakhtiari,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunderXiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash
    Bakhtiari,
- en: Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase,
- en: Leon Song^∗, Zhewei Yao
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Leon Song^∗, Zhewei Yao
- en: DeepSpeed of Microsoft Our corresponding authors are Xiaoxia Wu (xiaoxiawu@microsoft.com)
    and Leon Song (leonsong@microsoft.com). Haojun Xia is currently a PhD student
    at The University of Sydney, Australia.Shiyang Chen is currently a PhD student
    at Rutgers University, USA.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 微软的DeepSpeed，我们的对应作者是Xiaoxia Wu（xiaoxiawu@microsoft.com）和Leon Song（leonsong@microsoft.com）。Haojun
    Xia目前是澳大利亚悉尼大学的博士生。Shiyang Chen目前是美国罗格斯大学的博士生。
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This study examines 4-bit quantization methods like GPTQ in large language models
    (LLMs), highlighting GPTQ’s overfitting and limited enhancement in Zero-Shot tasks.
    While prior works merely focusing on zero-shot measurement, we extend task scope
    to more generative categories such as code generation and abstractive summarization,
    in which we found that INT4 quantization can significantly underperform. However,
    simply shifting to higher precision formats like FP6 has been particularly challenging,
    thus overlooked, due to poor performance caused by the lack of sophisticated integration
    and system acceleration strategies on current AI hardware. Our results show that
    FP6, even with a coarse-grain quantization scheme, performs robustly across various
    algorithms and tasks, demonstrating its superiority in accuracy and versatility.
    Notably, with the FP6 quantization, StarCoder-15B model performs comparably to
    its FP16 counterpart in code generation, and for smaller models like the 406M
    it closely matches their baselines in summarization. Neither can be achieved by
    INT4\. To better accommodate various AI hardware and achieve the best system performance,
    we propose a novel 4+2 design for FP6 to achieve similar latency to the state-of-the-art
    INT4 fine-grain quantization. With our design, FP6 can become a promising solution
    to the current 4-bit quantization methods used in LLMs.¹¹1Code will be released
    soon as a part of [https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究考察了像GPTQ这样的4-bit量化方法在大语言模型（LLMs）中的应用，强调了GPTQ在零-shot任务中的过拟合和有限增强。虽然之前的工作仅关注于零-shot测量，但我们将任务范围扩展到更多生成类别，如代码生成和抽象摘要，其中我们发现INT4量化可能显著表现不佳。然而，由于当前AI硬件在复杂集成和系统加速策略上的缺乏，高精度格式如FP6的转换特别具有挑战性，因此被忽视。我们的结果表明，即使在粗粒度量化方案下，FP6在各种算法和任务中表现稳健，展示了其在准确性和多样性上的优势。值得注意的是，通过FP6量化，StarCoder-15B模型在代码生成中与其FP16对应模型表现相当，而对于像406M这样的小模型，其摘要生成接近于基线性能。这些都是INT4无法实现的。为了更好地适应各种AI硬件并实现最佳系统性能，我们提出了一种新的4+2设计，以实现与最先进的INT4细粒度量化类似的延迟。通过我们的设计，FP6有望成为当前LLMs中使用的4-bit量化方法的一个有前途的解决方案。¹¹1代码将很快作为[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)的一部分发布
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) such as GPT-3 [[5](#bib.bib5)] have significantly
    advanced the field of natural language processing. These models have shown exceptional
    capabilities in various complex tasks, from text generation to language understanding.
    However, the widespread adoption of LLMs is challenged by their extensive computational
    and memory demands. This issue is particularly acute in resource-constrained environments,
    where deploying such large models is not feasible. To mitigate these challenges,
    post-training quantization has been recognized as a crucial technique [[6](#bib.bib6),
    [20](#bib.bib20), [46](#bib.bib46), [41](#bib.bib41)]. It enables the compression
    of these models for efficient utilization in limited-resource settings without
    the need for extensive retraining. Nevertheless, this approach often necessitates
    a balance between reducing the model size and maintaining accuracy [[14](#bib.bib14)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs），如 GPT-3 [[5](#bib.bib5)]，在自然语言处理领域取得了显著进展。这些模型在各种复杂任务中展示了卓越的能力，从文本生成到语言理解。然而，LLM
    的广泛应用受到其巨大的计算和内存需求的挑战。这一问题在资源受限的环境中尤为严重，此时部署如此庞大的模型并不可行。为了缓解这些挑战，后训练量化被认为是一项关键技术
    [[6](#bib.bib6), [20](#bib.bib20), [46](#bib.bib46), [41](#bib.bib41)]。它使得这些模型能够在有限资源环境中高效利用，而无需大量重新训练。然而，这种方法通常需要在减少模型大小和保持准确性之间取得平衡
    [[14](#bib.bib14)]。
- en: Recent developments in the field of quantization, particularly in 4-bit quantization,
    have demonstrated potential in compressing LLMs effectively as their quality drops
    are greatly minimized due to advance algorithm design such as GPTQ [[19](#bib.bib19)]
    and LoRC [[69](#bib.bib69)]. However, these advancements have predominantly focused
    on zero-shot evaluations and the acceptable quality drops are for larger model
    size greater 13B, yet they often come with a significant trade-off for smaller
    model size such as $1$B. Moreover, they only focus on zero-shot measurement [[62](#bib.bib62),
    [69](#bib.bib69)]. In production environments, where replicating the original
    model’s performance across different tasks is critical, any loss of model quality
    is a major concern. Existing methods, while innovative, do not fully address the
    practical requirements for deploying LLMs in real-world applications.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化领域，特别是 4 位量化的最新进展展示了有效压缩 LLM 的潜力，因为通过先进的算法设计如 GPTQ [[19](#bib.bib19)] 和 LoRC
    [[69](#bib.bib69)]，其质量下降得到了大大降低。然而，这些进展主要集中在零-shot 评估上，且可接受的质量下降主要适用于大于 130 亿参数的模型，但对小型模型如
    $1$B 可能带来显著的折衷。此外，它们仅关注零-shot 测量 [[62](#bib.bib62), [69](#bib.bib69)]。在生产环境中，复制原始模型在不同任务上的表现至关重要，任何模型质量的损失都是主要关注点。尽管现有方法具有创新性，但未能完全解决在实际应用中部署
    LLM 的实际需求。
- en: 'Contribution. To address these challenges, our contributions are as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献。为了解决这些挑战，我们的贡献如下：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Broadened Evaluation Scope and Quantization Analysis. Our study reveals that
    existing quantization methods like GPTQ tend to overfit to calibrated datasets.
    More significantly, we have broadened the scope of 4-bit quantization analysis
    in LLMs to include tasks beyond Zero-Shot, such as code generation and abstractive
    summarization. We discover that INT4 quantization often underperforms, especially
    in smaller models, even as large as 13 billion parameters, exemplified by LLaMA-13b.
    See Section [3](#S3 "3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining
    LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")
    for details.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '扩展的评估范围和量化分析。我们的研究揭示了现有的量化方法，如 GPTQ，往往会对校准数据集进行过拟合。更重要的是，我们将 4 位量化分析的范围扩展到了超越零-shot
    的任务，如代码生成和抽象总结。我们发现，INT4 量化通常表现不佳，尤其是在较小的模型中，即使是参数量达到 130 亿的 LLaMA-13b 也不例外。详情请见第
    [3](#S3 "3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks") 节。'
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Superior Performance with FP6 Quantization. We illustrate that FP6, employing
    a basic round-to-nearest (RTN) algorithm and a coarse-grain quantization approach,
    consistently achieves accuracy on par with full-precision models, proving highly
    effective across a broad spectrum of generative tasks. The StarCoder-13B model
    with FP6 quantization matches the performance of its FP16 equivalent in code generation
    tasks. For smaller models such as the 406M, it aligns closely with baseline results
    in summarization. These achievements are beyond the capabilities of INT4 quantization.
    For a more in-depth exploration, refer to Section [4](#S4 "4 Sweet Spot Solution:
    FP6 ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks").'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'FP6量化的卓越性能。我们展示了FP6，采用基本的四舍五入（RTN）算法和粗粒度量化方法，在生成任务的广泛范围内始终实现与全精度模型相当的准确性。FP6量化的StarCoder-13B模型在代码生成任务中的表现与其FP16等效模型相当。对于如406M这样的小模型，它在总结任务中的表现与基准结果接近。这些成就超出了INT4量化的能力。有关更深入的探讨，请参见第[4](#S4
    "4 Sweet Spot Solution: FP6 ‣ ZeroQuant(4+2): Redefining LLMs Quantization with
    a New FP6-Centric Strategy for Diverse Generative Tasks")节。'
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Innovative 4+2 FP6 Design. We introduce an innovative 4+2 design for FP6 that
    overcomes prior integration and acceleration issues on AI hardware. This design
    attains latency similar to the state-of-the-art INT4 fine-grain quantization,
    establishing FP6 as a viable alternative to existing 4-bit quantization methods
    in LLMs. See Section [5](#S5 "5 System Support Discussion ‣ ZeroQuant(4+2): Redefining
    LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")
    for details.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '创新的4+2 FP6设计。我们介绍了一种创新的4+2 FP6设计，克服了以前在AI硬件上的集成和加速问题。这种设计实现了类似于最先进的INT4细粒度量化的延迟，使FP6成为LLM中现有4位量化方法的可行替代方案。有关详细信息，请参见第[5](#S5
    "5 System Support Discussion ‣ ZeroQuant(4+2): Redefining LLMs Quantization with
    a New FP6-Centric Strategy for Diverse Generative Tasks")节。'
- en: 2 Related Work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: In this study, we specifically focus on the quantization of Large Language Models
    (LLMs), diverging from other neural network architectures like BERT and ResNet
    models, which have been extensively explored in existing literature [[54](#bib.bib54),
    [71](#bib.bib71), [17](#bib.bib17), [63](#bib.bib63), [3](#bib.bib3), [16](#bib.bib16),
    [55](#bib.bib55), [28](#bib.bib28)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究专注于大型语言模型（LLMs）的量化，区别于现有文献中广泛探讨的BERT和ResNet模型等其他神经网络架构[[54](#bib.bib54),
    [71](#bib.bib71), [17](#bib.bib17), [63](#bib.bib63), [3](#bib.bib3), [16](#bib.bib16),
    [55](#bib.bib55), [28](#bib.bib28)]。
- en: Quantization generally refers to employing low-precision weights and activations
    to leverage faster arithmetic cores, such as INT8/INT4 tensor cores [[26](#bib.bib26)].
    However, the distinctive bandwidth constraints of LLMs have popularized weight-only
    quantization methods [[71](#bib.bib71), [19](#bib.bib19), [68](#bib.bib68), [62](#bib.bib62),
    [61](#bib.bib61)] as a strategy to reduce the memory footprint of these models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 量化通常指使用低精度的权重和激活值来利用更快的算术核心，如INT8/INT4张量核心[[26](#bib.bib26)]。然而，LLM独特的带宽限制使得仅权重量化方法[[71](#bib.bib71),
    [19](#bib.bib19), [68](#bib.bib68), [62](#bib.bib62), [61](#bib.bib61)]成为减少这些模型内存占用的策略。
- en: Most previous research evaluates the impact of quantization using metrics like
    zero-shot perplexity or accuracy [[66](#bib.bib66), [19](#bib.bib19), [8](#bib.bib8),
    [2](#bib.bib2), [29](#bib.bib29)]. However, given that the main real-world applications
    of LLMs, such as ChatGPT [[5](#bib.bib5)] and Codex [[21](#bib.bib21)], revolve
    around generation-based tasks, a more comprehensive evaluation framework for quantized
    LLMs is warranted.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的大多数研究使用零-shot困惑度或准确率等指标来评估量化的影响[[66](#bib.bib66), [19](#bib.bib19), [8](#bib.bib8),
    [2](#bib.bib2), [29](#bib.bib29)]。然而，鉴于LLM的主要实际应用，如ChatGPT[[5](#bib.bib5)]和Codex[[21](#bib.bib21)]，主要围绕生成任务，因此需要更全面的量化LLM的评估框架。
- en: 'While many studies focus on integer data formats for their ease of simulation
    and extensive ecosystem support [[31](#bib.bib31), [15](#bib.bib15), [19](#bib.bib19),
    [8](#bib.bib8), [29](#bib.bib29), [27](#bib.bib27)], recent works have also demonstrated
    the effectiveness of floating-point formats [[62](#bib.bib62), [13](#bib.bib13)].
    Nonetheless, these investigations typically center on conventional bit precisions
    such as 2/4/8 bits. Some research, like GPTQ, delves into 3-bit precision, but
    number concatenation methods, as discussed in Section [5](#S5 "5 System Support
    Discussion ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric
    Strategy for Diverse Generative Tasks"), limit their system performance.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管许多研究集中在整数数据格式上，因为它们易于模拟且支持广泛的生态系统 [[31](#bib.bib31), [15](#bib.bib15), [19](#bib.bib19),
    [8](#bib.bib8), [29](#bib.bib29), [27](#bib.bib27)]，但最近的研究也展示了浮点格式的有效性 [[62](#bib.bib62),
    [13](#bib.bib13)]。尽管如此，这些研究通常集中于传统的位精度，如 2/4/8 位。一些研究，如 GPTQ，*深入探讨*了 3 位精度，但如第
    [5](#S5 "5 System Support Discussion ‣ ZeroQuant(4+2): Redefining LLMs Quantization
    with a New FP6-Centric Strategy for Diverse Generative Tasks") 节所述的数字连接方法，限制了它们的系统性能。'
- en: Finally, while the push for lower precision quantization continues, the practicality
    of deploying a model of size $xB$ bits quantization is often overlooked, despite
    potential quality advantages [[14](#bib.bib14), [69](#bib.bib69)]. Our paper seeks
    to find an optimal balance where the quantized model retains similar accuracy
    to a full-precision model, an aspect largely missing in current literature [[50](#bib.bib50),
    [69](#bib.bib69), [53](#bib.bib53), [70](#bib.bib70), [34](#bib.bib34), [23](#bib.bib23),
    [59](#bib.bib59), [22](#bib.bib22), [67](#bib.bib67)].
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，尽管对低精度量化的推动持续进行，但部署一个大小为 $xB$ 位量化的模型的实用性常常被忽视，尽管这可能带来质量上的优势 [[14](#bib.bib14),
    [69](#bib.bib69)]。我们的论文旨在找到一个最佳平衡点，使量化模型保持与全精度模型相似的准确性，这是当前文献中大多缺失的一个方面 [[50](#bib.bib50),
    [69](#bib.bib69), [53](#bib.bib53), [70](#bib.bib70), [34](#bib.bib34), [23](#bib.bib23),
    [59](#bib.bib59), [22](#bib.bib22), [67](#bib.bib67)]。
- en: 3 Comprehensive Evaluation is Needed
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 综合评估是必要的
- en: For completeness, we here explain some foundational terminology and concepts
    in quantization.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们在此解释一些量化的基础术语和概念。
- en: Integer Quantization. Consider a full-precision ${\mathbf{x}}\in\mathbb{R}^{d}$
    is
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 整数量化。考虑一个全精度的 ${\mathbf{x}}\in\mathbb{R}^{d}$。
- en: '|  | $1$2 |  | (1) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $clamp$ based on prior works  [[61](#bib.bib61), [62](#bib.bib62), [68](#bib.bib68),
    [69](#bib.bib69)]. For history and details on how to set the parameters, see [[20](#bib.bib20)].
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $clamp$ 基于先前的工作 [[61](#bib.bib61), [62](#bib.bib62), [68](#bib.bib68), [69](#bib.bib69)]。有关如何设置参数的历史和细节，请参见
    [[20](#bib.bib20)]。
- en: Fine-grain Quantization (FGQ) and Coarse-grain Quantization (CGQ) relates to
    the value of $d$ in this case), resulting in a coarser quantization approach.
    FGQ gained significant attention in the realm of LLMs because the values in the
    weight matrices tend to have a wider distribution, as noted in [[69](#bib.bib69)].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 精细量化（FGQ）和粗略量化（CGQ）与 $d$ 的值相关，这会导致更粗略的量化方法。由于权重矩阵中的值往往有更广泛的分布，精细量化在 LLM 领域获得了显著关注，如
    [[69](#bib.bib69)] 所述。
- en: 'Alongside FGQ or CGQ, specific algorithms are employed for precision mapping
    in quantization. Given the focus on 4-bit quantization and the demonstrated efficacy
    of the INT4 format over FP4 (as detailed in the appendix) [[62](#bib.bib62)],
    the investigation primarily centers on a straightforward method, RTN, and the
    increasingly recognized and impactful algorithm, GPTQ [[18](#bib.bib18), [19](#bib.bib19)],
    with a solid foundation background [[33](#bib.bib33), [24](#bib.bib24)]. We now
    explain them briefly below:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 FGQ 或 CGQ 之外，量化中还采用了特定算法进行精度映射。鉴于对 4 位量化的关注以及 INT4 格式相对于 FP4 的有效性（如附录中详细描述的）
    [[62](#bib.bib62)]，研究主要集中于一种简单的方法 RTN 和越来越被认可且具有影响力的算法 GPTQ [[18](#bib.bib18),
    [19](#bib.bib19)]，以及一个扎实的背景 [[33](#bib.bib33), [24](#bib.bib24)]。我们现在简要解释如下：
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RTN. Round-to-nearest neighborhood simply map the weight matrices to its low-precision
    counterpart based on Equation [1](#S3.E1 "In 3 Comprehensive Evaluation is Needed
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks").'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'RTN。最近邻舍入简单地将权重矩阵映射到其低精度对应物，基于方程 [1](#S3.E1 "In 3 Comprehensive Evaluation
    is Needed ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric
    Strategy for Diverse Generative Tasks")。'
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GPTQ. Generative Pre-trained Transformer Quantization is a more advanced method
    of leveraging the activation information, which requires the inverse of the second-order
    input information. According to [[18](#bib.bib18), [19](#bib.bib19)], it reduces
    the precision of the model’s weights to a lower bit representation (down to 3
    or 4 bits per weight) without significant accuracy loss. Their code implementation
    is structured in a layer-by-layer manner, transferring the computational burden
    to the CPU when it’s not in use. This strategy allows for the execution of massive
    models, like those with 175B parameters, on a single GPU, overcoming previous
    limitations of scale and complexity. GPTQ enhances the practical deployment of
    these models, particularly in memory and computationally constrained environments.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GPTQ。生成预训练变换器量化是一种利用激活信息的更先进的方法，要求输入信息的二阶逆。根据[[18](#bib.bib18)，[19](#bib.bib19)]，它将模型权重的精度降低到更低的位表示（降低到每个权重3或4位），而不会显著损失准确性。他们的代码实现以逐层方式构建，将计算负担转移到不使用时的CPU上。这种策略使得可以在单个GPU上执行大规模模型，如175B参数的模型，克服了之前的规模和复杂性限制。GPTQ提升了这些模型的实际部署，特别是在内存和计算受限的环境中。
- en: 'Table 1: Zero-Shot Evaluation (Perplexity$\downarrow$). GPTQ quantization algorihtm
    for INT4 weight (W4A16) on LLaMA-1B (Left) and LLaMA-13B (Right). Different calibration
    datasets result in different results.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：零样本评估（困惑度$\downarrow$）。GPTQ量化算法在LLaMA-1B（左）和LLaMA-13B（右）上的INT4权重（W4A16）。不同的校准数据集导致不同的结果。
- en: Dataset LLaMA-1B (4096-seq) LLaMA-13B (2048-seq) Precision FGQ for GPTQ PTB
    PTB-new C4 C4-new PTB PTB-new C4 C4-new FP16 N/A N/A 37.39 58.34 8.91 9.4 19.23
    28.10 6.61 6.8 \cdashline1-12 INT4-GPTQ ✗ PTB 49.80 64.01 10.00 10.49 19.68 28.71
    6.91 7.17 ✗ C4 719.21 693.48 9.84 10.37 21.31 30.01 6.84 7.09 ✓ C4 1399.89 1396.76
    9.34 9.84 22.14 29.83 6.74 6.95
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 LLaMA-1B (4096-seq) LLaMA-13B (2048-seq) 精度 FGQ用于GPTQ PTB PTB-new C4 C4-new
    PTB PTB-new C4 C4-new FP16 N/A N/A 37.39 58.34 8.91 9.4 19.23 28.10 6.61 6.8 \cdashline1-12
    INT4-GPTQ ✗ PTB 49.80 64.01 10.00 10.49 19.68 28.71 6.91 7.17 ✗ C4 719.21 693.48
    9.84 10.37 21.31 30.01 6.84 7.09 ✓ C4 1399.89 1396.76 9.34 9.84 22.14 29.83 6.74
    6.95
- en: 'Table 2: Zero-Shot Evaluation (Perplexity$\downarrow$). Compare between GPTQ[C4]
    and RTN quantization algorithms for INT4 weight (W4A16) on LLaMA of size 1B, 13B
    and 65B. We apply fine-grain quantization (FGQ) in which the block-size is 256
    elements per scale (LLaMA-1B’s block-size is 128). We also report results for
    coarse-grain quantization (CGQ) (per row per scale). The evaluation datasets are
    Wikitext2, PTB, C4, PTB-new, and C4-new.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：零样本评估（困惑度$\downarrow$）。比较GPTQ[C4]和RTN量化算法在LLaMA 1B、13B和65B大小的INT4权重（W4A16）上的表现。我们应用了精细量化（FGQ），其中块大小为每个尺度256个元素（LLaMA-1B的块大小为128）。我们还报告了粗粒度量化（CGQ）的结果（每行每个尺度）。评估数据集包括Wikitext2、PTB、C4、PTB-new和C4-new。
- en: Quant Precision LLaMA-1B (4096-seq) LLaMA-13B (2048-seq) LLaMA-65B (2048-seq)
    FP16 24.31[7.53/37.39/8.91/58.34/9.40] 13.16[5.09/19.23/6.61/28.10/6.80] 6.41[3.56/8.00/5.62/8.88/5.98]
    FGQ INT4-GPTQ 564.73[7.83/1399.89/9.34/1396.76/9.84] 14.19[5.28/22.14/6.74/29.83/6.95]
    6.61[3.81/8.17/5.73/9.20/6.13] INT4-RTN 22.76[7.98/34.03/9.50/52.28/10.00] 14.32[5.35/22.49/6.80/29.96/7.00]
    7.30[3.79/10.13/5.74/10.54/6.31] CGQ INT4-GPTQ 288.22[8.20/719.21/9.84/693.48/10.37]
    14.13[5.37/21.31/6.84/30.01/7.09] 7.17[4.12/10.50/5.83/9.16/6.21] INT4-RTN 34.29[8.33/55.52/10.15/86.85/10.62]
    14.32[5.55/20.95/6.97/30.91/7.22] 7.72[4.20/10.59/5.90/11.44/6.47]
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 量化精度 LLaMA-1B (4096-seq) LLaMA-13B (2048-seq) LLaMA-65B (2048-seq) FP16 24.31[7.53/37.39/8.91/58.34/9.40]
    13.16[5.09/19.23/6.61/28.10/6.80] 6.41[3.56/8.00/5.62/8.88/5.98] FGQ INT4-GPTQ
    564.73[7.83/1399.89/9.34/1396.76/9.84] 14.19[5.28/22.14/6.74/29.83/6.95] 6.61[3.81/8.17/5.73/9.20/6.13]
    INT4-RTN 22.76[7.98/34.03/9.50/52.28/10.00] 14.32[5.35/22.49/6.80/29.96/7.00]
    7.30[3.79/10.13/5.74/10.54/6.31] CGQ INT4-GPTQ 288.22[8.20/719.21/9.84/693.48/10.37]
    14.13[5.37/21.31/6.84/30.01/7.09] 7.17[4.12/10.50/5.83/9.16/6.21] INT4-RTN 34.29[8.33/55.52/10.15/86.85/10.62]
    14.32[5.55/20.95/6.97/30.91/7.22] 7.72[4.20/10.59/5.90/11.44/6.47]
- en: 'Table 3: Zero-Shot Evaluation (Accuracy$\uparrow$). Compare between GPTQ[C4]
    and RTN quantization algorithms for INT4 weight (W4A16) on LLaMA-1B (Top) and
    LLaMA-13B (Bottom). We apply fine-grain quantization (FGQ) in which the block-size
    is 256 elements per scale except for LLaMA-1B’s (which is 128). arcC (arcE) stands
    for arc_challenges (arc_easy).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：零样本评估（准确率$\uparrow$）。比较GPTQ[C4]和RTN量化算法在LLaMA-1B（顶部）和LLaMA-13B（底部）上的INT4权重（W4A16）。我们应用了精细量化（FGQ），其中块大小为每个尺度256个元素，LLaMA-1B的块大小为128。arcC（arcE）代表arc_challenges（arc_easy）。
- en: Models Precision (FGQ) arcC arcE boolq cb copa piqa rte wic wsc storycloze MEAN
    LLaMA-1B FP16 26.71 53.11 61.13 39.29 76.00 73.83 50.18 50.00 36.54 69.64 53.64
    \cdashline2-13 (4096-seq) INT4-GPTQ 26.37 50.59 61.59 46.43 79.00 73.34 48.01
    50.00 36.54 68.24 54.01 INT4-RTN 26.11 51.09 58.07 50.00 74.00 72.91 48.38 50.00
    36.54 68.36 53.55 LLaMA-13B FP16 43.86 74.58 68.53 50.00 90.00 79.00 65.34 50.00
    35.58 78.23 63.51 \cdashline2-13 (2048-seq) INT4-GPTQ 43.00 73.44 67.83 41.07
    93.00 78.78 62.45 50.16 36.54 78.17 62.44 INT4-RTN 44.03 74.45 67.37 44.64 91.00
    78.84 63.18 49.84 36.54 78.42 62.83 LLaMA-65B FP16 47.01 75.08 82.32 64.29 91.00
    81.61 71.48 58.31 60.58 79.57 71.13 \cdashline2-13 (2048-seq) INT4-GPTQ 46.84
    75.08 80.76 58.93 94.00 81.18 72.92 56.27 60.58 79.31 70.59 INT4-RTN 47.10 75.25
    81.47 62.50 95.00 81.23 69.68 57.21 62.50 79.63 71.16
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 精度 (FGQ) arcC arcE boolq cb copa piqa rte wic wsc storycloze MEAN LLaMA-1B
    FP16 26.71 53.11 61.13 39.29 76.00 73.83 50.18 50.00 36.54 69.64 53.64 \cdashline2-13
    (4096-seq) INT4-GPTQ 26.37 50.59 61.59 46.43 79.00 73.34 48.01 50.00 36.54 68.24
    54.01 INT4-RTN 26.11 51.09 58.07 50.00 74.00 72.91 48.38 50.00 36.54 68.36 53.55
    LLaMA-13B FP16 43.86 74.58 68.53 50.00 90.00 79.00 65.34 50.00 35.58 78.23 63.51
    \cdashline2-13 (2048-seq) INT4-GPTQ 43.00 73.44 67.83 41.07 93.00 78.78 62.45
    50.16 36.54 78.17 62.44 INT4-RTN 44.03 74.45 67.37 44.64 91.00 78.84 63.18 49.84
    36.54 78.42 62.83 LLaMA-65B FP16 47.01 75.08 82.32 64.29 91.00 81.61 71.48 58.31
    60.58 79.57 71.13 \cdashline2-13 (2048-seq) INT4-GPTQ 46.84 75.08 80.76 58.93
    94.00 81.18 72.92 56.27 60.58 79.31 70.59 INT4-RTN 47.10 75.25 81.47 62.50 95.00
    81.23 69.68 57.21 62.50 79.63 71.16
- en: 'Table 4: Generation Tasks (Rouge$\uparrow$ versions fine-tuned by CNN/XSUM
    and code generation tasks in Human-X including Python and JavaScript (as the variances
    of other tasks such as CPP, Go and RUST are higher and so not included), averaged
    over at least 8 repeated experiments with standard deviation.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：生成任务（Rouge$\uparrow$版本经过CNN/XSUM微调，以及Human-X中的代码生成任务，包括Python和JavaScript（由于其他任务如CPP、Go和RUST的方差较高，因此未包括在内），在至少8次重复实验中取平均，计算标准差。
- en: BART${}_{\text{406M}}$1.36
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: BART${}_{\text{406M}}$1.36
- en: In addition to the algorithms previously mentioned, there has been significant
    progress in Post-Training Quantization (PTQ) for LLMs, highlighted by innovations
    such as SmoothQuant [[66](#bib.bib66)], AWQ [[38](#bib.bib38)], Quip [[8](#bib.bib8)],
    SqueezeLLM [[29](#bib.bib29)], QUIK [[2](#bib.bib2)], and LLM-FP4 [[40](#bib.bib40)]
    and many more [[69](#bib.bib69), [14](#bib.bib14)]. These methodologies, however,
    often necessitate the use of additional sparsity matrices or extra procedures
    to pinpoint sensitive weights. Furthermore, the majority of these studies concentrate
    predominantly on zero-shot perplexity and accuracy performance [[69](#bib.bib69),
    [19](#bib.bib19), [62](#bib.bib62)]. Yet, the extent to which these findings can
    be generalized to other generative tasks remains to be fully explored.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前提到的算法之外，在大规模语言模型（LLMs）的后训练量化（PTQ）方面已经取得了显著进展，突出表现为如SmoothQuant [[66](#bib.bib66)]、AWQ [[38](#bib.bib38)]、Quip [[8](#bib.bib8)]、SqueezeLLM [[29](#bib.bib29)]、QUIK [[2](#bib.bib2)]
    和LLM-FP4 [[40](#bib.bib40)] 以及更多 [[69](#bib.bib69), [14](#bib.bib14)]的创新。这些方法通常需要使用额外的稀疏矩阵或额外的程序来定位敏感权重。此外，这些研究大多主要集中在零-shot困惑度和准确性表现 [[69](#bib.bib69),
    [19](#bib.bib19), [62](#bib.bib62)]上。然而，这些发现是否可以推广到其他生成任务仍需深入探索。
- en: Experiment Settings
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置
- en: 'We assess performance across three metrics: Zero-Shot tasks, Code Generation,
    and Summarization. We also perform try to implement comparative experiments for
    those chat-based models and judged by GPT-4 based on the FastChat codes [[75](#bib.bib75)].
    Despite this, due to significant variability in our findings, we concluded that
    there is no clear link between bit precision and performance. These results are
    detailed further in the Appendix of our study.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过三个指标来评估性能：零-shot任务、代码生成和摘要生成。我们还尝试对基于聊天的模型进行比较实验，并根据GPT-4的判断依据FastChat代码 [[75](#bib.bib75)]。尽管如此，由于发现结果的显著变异，我们得出结论，位精度与性能之间没有明确的联系。这些结果在我们研究的附录中有进一步的详细说明。
- en: •
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Zero-Shot Tasks. Leveraging open-source repositories²²2[https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/compression](https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/compression),
    [https://github.com/qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa),
    and [https://github.com/jerry-chee/QuIP](https://github.com/jerry-chee/QuIP),
    we applied GPTQ quantization algorithms to measure both perplexity and accuracy
    in zero-shot contexts. The datasets used for perplexity measurement include PTB [[42](#bib.bib42)],
    Wikitext [[43](#bib.bib43)], and C4 [[51](#bib.bib51)].³³3Following the approach
    in [gptq-for-llama](https://github.com/qwopqwop200/GPTQ-for-LLaMa), we added two
    new validation sets: PTB-new, using the PTB test dataset, and C4-new, comprising
    the first 256$\times$seqlength. These new sets are implemented as per [QuIP](https://github.com/jerry-chee/QuIP).
    For accuracy, we randomly pick ten tasks: ARC (Challenge/Easy) [[4](#bib.bib4)],
    BoolQ [[9](#bib.bib9)], CB [[12](#bib.bib12)], Copa [[1](#bib.bib1)], PIQA [[56](#bib.bib56)],
    RTE [[11](#bib.bib11)], WSC [[35](#bib.bib35)], Storycloze [[45](#bib.bib45)]).
    Calibration for GPTQ used 128 (32) samples for LLaMa-1B/13B (-65B) models.⁴⁴4LLaMA-13B/65B
    are from [[57](#bib.bib57)] and LLaMA-1B is from [[65](#bib.bib65)]. They can
    be downloaded from HuggingFace with names: ‘princeton-nlp/Sheared-LLaMA-1.3B’,
    ‘huggyllama/llama-13b’, ‘huggyllama/llama-65b’. . We believe the results generalize
    to other models family such sh OPT [[72](#bib.bib72)] and BLOOM [[52](#bib.bib52)],
    The experiments were deterministic, using the seed 123.'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Zero-Shot 任务。利用开源代码库²²2[https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/compression](https://github.com/microsoft/DeepSpeed/tree/master/deepspeed/compression)、[https://github.com/qwopqwop200/GPTQ-for-LLaMa](https://github.com/qwopqwop200/GPTQ-for-LLaMa)
    和 [https://github.com/jerry-chee/QuIP](https://github.com/jerry-chee/QuIP)，我们应用了
    GPTQ 量化算法来测量 zero-shot 环境下的困惑度和准确性。用于困惑度测量的数据集包括 PTB [[42](#bib.bib42)]、Wikitext [[43](#bib.bib43)]
    和 C4 [[51](#bib.bib51)]。³³3按照 [gptq-for-llama](https://github.com/qwopqwop200/GPTQ-for-LLaMa)
    的方法，我们添加了两个新的验证集：PTB-new，使用 PTB 测试数据集，和 C4-new，包括前 256$\times$seqlength。这些新数据集的实现方式参考了
    [QuIP](https://github.com/jerry-chee/QuIP)。在准确性方面，我们随机选择了十个任务：ARC (Challenge/Easy) [[4](#bib.bib4)]、BoolQ [[9](#bib.bib9)]、CB [[12](#bib.bib12)]、Copa [[1](#bib.bib1)]、PIQA [[56](#bib.bib56)]、RTE [[11](#bib.bib11)]、WSC [[35](#bib.bib35)]、Storycloze [[45](#bib.bib45)]）。GPTQ
    的校准使用了 128 (32) 个样本用于 LLaMa-1B/13B (-65B) 模型。⁴⁴4LLaMA-13B/65B 来自 [[57](#bib.bib57)]，LLaMA-1B
    来自 [[65](#bib.bib65)]。它们可以从 HuggingFace 下载，名称为：‘princeton-nlp/Sheared-LLaMA-1.3B’、‘huggyllama/llama-13b’、‘huggyllama/llama-65b’。我们相信结果可以推广到其他模型家族，例如
    OPT [[72](#bib.bib72)] 和 BLOOM [[52](#bib.bib52)]。实验是确定性的，使用了种子 123。
- en: •
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Code Generation. Following [[76](#bib.bib76)] and their open-source implementation⁵⁵5[https://github.com/THUDM/CodeGeeX2](https://github.com/THUDM/CodeGeeX2),
    we adapted non-greedy generation settings (n=20, t=0.2, top_p=0.95). To mitigate
    variance, nine random seeds {111,222,…, 888, 1111} were employed. The models evaluated
    included CodeGeeX2-6B, StarCoder-15B [[36](#bib.bib36)], and CodeLLaMA-34B [[39](#bib.bib39)].⁶⁶6Available
    as ‘THUDM/codegeex2-6b’, ‘bigcode/starcoder’, and ‘codefuse-ai/CodeFuse-CodeLlama-34B’
    on HuggingFace. We focused on Python and JavaScript, noting instability in other
    programming languages.
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代码生成。根据 [[76](#bib.bib76)] 和他们的开源实现⁵⁵5[https://github.com/THUDM/CodeGeeX2](https://github.com/THUDM/CodeGeeX2)，我们调整了非贪婪生成设置（n=20，t=0.2，top_p=0.95）。为了减少方差，使用了九个随机种子
    {111,222,…, 888, 1111}。评估的模型包括 CodeGeeX2-6B、StarCoder-15B [[36](#bib.bib36)] 和
    CodeLLaMA-34B [[39](#bib.bib39)]。⁶⁶6在 HuggingFace 上可用的名称为 ‘THUDM/codegeex2-6b’、‘bigcode/starcoder’
    和 ‘codefuse-ai/CodeFuse-CodeLlama-34B’。我们专注于 Python 和 JavaScript，注意到其他编程语言存在不稳定性。
- en: •
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Summarization Tasks. Based on [[37](#bib.bib37), [61](#bib.bib61)] and their
    open-source codes,⁷⁷7[https://github.com/amazon-science/dq-bart](https://github.com/amazon-science/dq-bart)
    we utilized BART-large, fine-tuned for CNNDailyMail [[25](#bib.bib25)] and XSum [[47](#bib.bib47)]
    summarization tasks.⁸⁸8Models available as ‘facebook/bart-large-cnn’ and ‘facebook/bart-large-xsum’
    on HuggingFace. Default settings were applied for all other parameters.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要生成任务。基于 [[37](#bib.bib37)] 和 [[61](#bib.bib61)] 及其开源代码⁷⁷7[https://github.com/amazon-science/dq-bart](https://github.com/amazon-science/dq-bart)，我们使用了
    BART-large，针对 CNNDailyMail [[25](#bib.bib25)] 和 XSum [[47](#bib.bib47)] 摘要生成任务进行了微调。⁸⁸8在
    HuggingFace 上可用的模型为 ‘facebook/bart-large-cnn’ 和 ‘facebook/bart-large-xsum’。所有其他参数均应用了默认设置。
- en: 'We focus on thes experiments with FP16 activation and INT4 weights on LLMs.
    Our experimental setup includes a single-GPU environment, utilizing either a V100-32g
    or H100-80g GPU. Based on the results Table [1](#S3.T1 "Table 1 ‣ 3 Comprehensive
    Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New
    FP6-Centric Strategy for Diverse Generative Tasks"), Table [2](#S3.T2 "Table 2
    ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs Quantization
    with a New FP6-Centric Strategy for Diverse Generative Tasks"), and Table [4](#S3.T4
    "Table 4 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks"), we
    make the following two key observations.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '我们专注于具有FP16激活和INT4权重的LLMs实验。我们的实验设置包括一个单GPU环境，使用V100-32g或H100-80g GPU。根据表[1](#S3.T1
    "Table 1 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")、表[2](#S3.T2
    "Table 2 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")和表[4](#S3.T4
    "Table 4 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")，我们做出以下两个关键观察。'
- en: GPTQ’s Tendency to Overfit.
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: GPTQ的过拟合倾向。
- en: 'Although GPTQ is innovative in post-training quantization, it tends to overfit
    to particular datasets, especially noticeable in its fine-grain quantization results.
    As indicated in Table [1](#S3.T1 "Table 1 ‣ 3 Comprehensive Evaluation is Needed
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks"), we see that if we calibrate with specific dataset
    such as C4 for GPTQ, then the performance for this C4 dataset would be much better
    (see 9.34 or 6.74 using FGQ), while other datasets such as PTB would result in
    much worse performance (see 1399.89 and 22.14 using FGQ). Independently, [[60](#bib.bib60)]
    also notices this issue while examining LLaMA-7B.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管GPTQ在后训练量化方面具有创新性，但它倾向于过拟合特定数据集，特别是在其细粒度量化结果中尤为明显。如表[1](#S3.T1 "Table 1 ‣
    3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs Quantization
    with a New FP6-Centric Strategy for Diverse Generative Tasks")所示，我们发现，如果我们使用特定数据集（如C4）对GPTQ进行校准，那么该C4数据集的表现会大大提高（见9.34或6.74使用FGQ），而其他数据集如PTB则会导致表现大幅下降（见1399.89和22.14使用FGQ）。独立地，[[60](#bib.bib60)]在检查LLaMA-7B时也注意到了这个问题。'
- en: 'It is admitted that the over-fitting phenomena is less severe for larger models
    (moving from 1B to 13B or 65B). Indeed, as shown in Table [2](#S3.T2 "Table 2
    ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs Quantization
    with a New FP6-Centric Strategy for Diverse Generative Tasks"), we see that LLaMA-65B
    using GPTQ on FGQ for INT4-weight results in the best average perplexity 6.61
    comparing to RTN (7.17), much closer to the baseline 6.41\. However, its effectiveness
    in enhancing Zero-Shot performance is somewhat limited (detailed in Table [4](#S3.T4
    "Table 4 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")),
    suggesting a gap in its adaptability across various language modeling scenarios
    and highlighting the need for robustness in model evaluation. In particularly,
    we presents in Table [4](#S3.T4 "Table 4 ‣ 3 Comprehensive Evaluation is Needed
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks") the comparison between RTN and GPTQ on INT4 weight
    while keep the activation untouched, we can not claim that GPTQ and RTN are better
    than another based on zero-shot performance. In fact, for LLaMA-65B, the performance
    for RTN is surprisingly better than the one of FP16.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '众所周知，相比较大的模型（从1B到13B或65B），过拟合现象较为轻微。确实，如表[2](#S3.T2 "Table 2 ‣ 3 Comprehensive
    Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New
    FP6-Centric Strategy for Diverse Generative Tasks")所示，LLaMA-65B在FGQ上使用GPTQ的INT4权重时，得到的平均困惑度为6.61，相比RTN（7.17）更接近基准6.41。然而，它在增强Zero-Shot性能方面的效果有所限制（详见表[4](#S3.T4
    "Table 4 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")），这表明其在各种语言建模场景下的适应性存在差距，并突显了模型评估中对稳健性的需求。特别是，我们在表[4](#S3.T4
    "Table 4 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")中展示了RTN和GPTQ在INT4权重下的比较，同时保持激活不变，我们不能仅根据zero-shot性能来宣称GPTQ和RTN哪个更好。事实上，对于LLaMA-65B，RTN的表现意外地优于FP16的表现。'
- en: Expanding Evaluation Methods for Generative Models.
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 扩展生成模型的评估方法。
- en: 'Our current analysis, mainly centered on zero-shot performance as shown in
    Table [2](#S3.T2 "Table 2 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2):
    Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative
    Tasks") and Table [4](#S3.T4 "Table 4 ‣ 3 Comprehensive Evaluation is Needed ‣
    ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for
    Diverse Generative Tasks"), highlights the need for a broader scope in evaluation
    techniques. The core strength of LLMs lies in their ability to generate sequences.
    Therefore, this paper focuses on assessing summarization and code generation,
    as elaborated in Table [4](#S3.T4 "Table 4 ‣ 3 Comprehensive Evaluation is Needed
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks").⁹⁹9It should be noted that GPTQ tends to overfit
    to the calibrated dataset and poses implementation challenges, leading us to solely
    utilize RTN for our evaluations. This strategy underlines the importance of comprehensive
    and detailed testing methods that extend beyond zero-shot learning, aiming to
    fully evaluate the generative capabilities of LLMs. The data in Table [4](#S3.T4
    "Table 4 ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks") show
    a notable difference in performance with INT4, especially when compared to standard
    benchmarks. For example, the performance of the CodeLLaMA-34B model in Java-Script
    drops from 45.05 (FP16) to 43.45 (INT4, CGQ) or 43.22 (INT4, FGQ), a decrease
    of 1.6 and 1.83 points, respectively. While FGQ on INT4 offers considerable improvements
    over CGQ, gaps compared to FP16 persist, particularly for smaller models and in
    Java Scripts. Interestingly, the INT4 CodeLLaMA-34B on FGQ achieves 46.88 in Python
    code, surpassing its baseline, whereas the INT4 CodeGeeX2-6B on FGQ scores only
    29.8, falling behind even its INT4-CGQ performance. This highlights the inconsistency
    of INT4.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们当前的分析，主要集中在零-shot表现，如表[2](#S3.T2 "表 2 ‣ 3 需要全面评估 ‣ ZeroQuant(4+2)：以新的FP6为核心的策略重新定义LLM量化以应对多样的生成任务")和表[4](#S3.T4
    "表 4 ‣ 3 需要全面评估 ‣ ZeroQuant(4+2)：以新的FP6为核心的策略重新定义LLM量化以应对多样的生成任务")所示，突显了评估技术需要更广泛的范围。LLMs的核心优势在于生成序列的能力。因此，本文重点评估了总结和代码生成，如表[4](#S3.T4
    "表 4 ‣ 3 需要全面评估 ‣ ZeroQuant(4+2)：以新的FP6为核心的策略重新定义LLM量化以应对多样的生成任务")中详细说明的。⁹⁹9值得注意的是，GPTQ往往对标定数据集过拟合，并带来实施挑战，因此我们仅使用RTN进行评估。这一策略强调了超越零-shot学习的全面和详细测试方法的重要性，旨在全面评估LLMs的生成能力。表[4](#S3.T4
    "表 4 ‣ 3 需要全面评估 ‣ ZeroQuant(4+2)：以新的FP6为核心的策略重新定义LLM量化以应对多样的生成任务")中的数据表现出INT4的性能差异，特别是与标准基准相比。例如，CodeLLaMA-34B模型在Java-Script中的性能从45.05（FP16）下降至43.45（INT4，CGQ）或43.22（INT4，FGQ），分别下降了1.6和1.83点。虽然INT4上的FGQ相较于CGQ提供了显著改进，但与FP16相比仍存在差距，特别是在较小的模型和Java
    Scripts中。有趣的是，INT4 CodeLLaMA-34B在FGQ中在Python代码上的表现为46.88，超过了其基线，而INT4 CodeGeeX2-6B在FGQ中的得分仅为29.8，甚至落后于其INT4-CGQ性能。这突显了INT4的一致性问题。
- en: These results emphasize the need for research into the effectiveness of INT4
    in complex generative tasks.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果强调了对INT4在复杂生成任务中有效性的研究需求。
- en: '4 Sweet Spot Solution: FP6'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 甜蜜点解决方案：FP6
- en: 'Building on previous discussions around the challenges and limitations associated
    with INT4 quantization, particularly its instability and subpar outcomes in code
    generation and summarization tasks, this section delves into an emerging area
    of interest in floating point quantization research. Recent studies have increasingly
    focused on the use of floating point quantization for handling weights or activations
    within LLMs [[62](#bib.bib62), [40](#bib.bib40), [74](#bib.bib74), [44](#bib.bib44),
    [7](#bib.bib7), [32](#bib.bib32), [58](#bib.bib58)]. Notably, a simple FP8’s application
    in activation processes has shown remarkable improvements over the use of INT8
    [[62](#bib.bib62)]. Inspired by these advancements, a critical question arises:
    *Could increasing the bit precision, for instance to 5 or 6 bits, offer more stable
    and robust outcomes in generative tasks? This section aims to explore the extent
    of FP6’s (FP5’s) effectiveness and its resilience to different quantization algorithms,
    offering a potential solution to the dilemma posed by previous INT4 quantization
    challenges.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于之前对INT4量化相关挑战和局限性的讨论，特别是其在代码生成和摘要任务中的不稳定性和较差结果，本节探讨了浮点量化研究中的一个新兴领域。最近的研究越来越关注于使用浮点量化处理LLMs中的权重或激活[[62](#bib.bib62),
    [40](#bib.bib40), [74](#bib.bib74), [44](#bib.bib44), [7](#bib.bib7), [32](#bib.bib32),
    [58](#bib.bib58)]。值得注意的是，简单的FP8在激活过程中的应用相比INT8显示出了显著改进[[62](#bib.bib62)]。受到这些进展的启发，一个关键问题出现了：*增加位精度，例如到5位或6位，是否能在生成任务中提供更稳定和更强大的结果？本节旨在探讨FP6（FP5）的有效性及其对不同量化算法的韧性，为之前INT4量化挑战所提出的困境提供潜在的解决方案。*
- en: 'For completeness, we provide a simplified overview of the floating-point format.
    For a detailed explanation, please refer to [[10](#bib.bib10)]. A standard floating
    point number comprises three parts: the sign bit, the exponent bits, and the mantissa
    bits. This can be simplified as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们提供了浮点格式的简化概述。有关详细说明，请参见[[10](#bib.bib10)]。标准浮点数包括三部分：符号位、指数位和尾数位。这可以简化为：
- en: '|  | $x=S\times 2^{E-b}\times M,$ |  | (2) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $x=S\times 2^{E-b}\times M,$ |  | (2) |'
- en: where $S$).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$S$）。
- en: 'Following the implementation of [[73](#bib.bib73)], the maximum/minimum achievable
    value in FP6[E3M2] is $\pm 28$. However, these are not included in our weight
    quantization process using FP6[E3M2]. We do not think this slight difference will
    greatly impact the model performance. The FP16 (or BF16) weight matrix undergoes
    quantization as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[73](#bib.bib73)]的实现，在FP6[E3M2]中的最大/最小可实现值为$\pm 28$。然而，这些值不包含在我们使用FP6[E3M2]的权重量化过程中。我们认为这个微小的差异不会对模型性能产生重大影响。FP16（或BF16）权重矩阵的量化过程如下：
- en: '|  | $\hat{W}_{fp16}\approx Quant(W_{fp16})=S_{fp16}\times W_{fp6},$ |  | (3)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{W}_{fp16}\approx Quant(W_{fp16})=S_{fp16}\times W_{fp6},$ |  | (3)
    |'
- en: 'where $W_{fp16}$’s range without compromising on precision. Please see Section [5](#S5
    "5 System Support Discussion ‣ ZeroQuant(4+2): Redefining LLMs Quantization with
    a New FP6-Centric Strategy for Diverse Generative Tasks") for additional customizations
    in FP6[E3M2]. Similar settting is defined for F5[E3M1].'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '其中$W_{fp16}$的范围不影响精度。有关FP6[E3M2]的其他自定义设置，请参见第[5](#S5 "5 System Support Discussion
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks")节。F5[E3M1]也定义了类似的设置。'
- en: 'Why not INT6 instead of FP6. The choice of FP6 over INT6 is driven by two key
    factors: firstly, the FP format simplifies conversion processes, as final computations
    are typically performed using FP16 or BF16\. Secondly, there is no observed difference
    in accuracy between these formats, as supported by findings in [[62](#bib.bib62)]
    , eliminating the need for additional experimental validation.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不使用INT6而使用FP6。选择FP6而非INT6的原因有两个：首先，FP格式简化了转换过程，因为最终计算通常使用FP16或BF16进行。其次，这些格式之间的准确性没有观察到差异，正如[[62](#bib.bib62)]中的发现所支持的，因此无需额外的实验验证。
- en: 4.1 Results of FP6 and FP5 on all tasks
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 FP6和FP5在所有任务上的结果
- en: 'Table 5: Generation Tasks (Rouge or Pass@1 $\uparrow$ versions fine-tuned by
    CNN/XSUM and code generation tasks in Python and JavaScript, averaged over 8 iterations
    with standard deviation. FP6 (FP5) format is E3M2 (E3M1).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：生成任务（Rouge或Pass@1 $\uparrow$版本由CNN/XSUM微调和Python及JavaScript中的代码生成任务，经过8次迭代的平均值及标准差。FP6（FP5）格式为E3M2（E3M1）。
- en: BART${}_{\text{406M}}$1.93 FP6 (CGQ) 45.37/22.20/37.11 (44.06/21.07/30.67) 34.10±2.09
    31.61±1.74 35.64±1.26 33.60±1.91 44.31±1.88 44.51±1.30
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: BART${}_{\text{406M}}$1.93 FP6 (CGQ) 45.37/22.20/37.11 (44.06/21.07/30.67) 34.10±2.09
    31.61±1.74 35.64±1.26 33.60±1.91 44.31±1.88 44.51±1.30
- en: 'Table 6: Zero-Shot Evaluation (Perplexity$\downarrow$). Compare between GPTQ[C4]
    and RTN quantization algorithms for INT4 weight (W4A16) on LLaMA of size 1B, 13B
    and 65B. We apply fine-grain quantization (FGQ) in which the block-size is 256
    elements per scale (LLaMA-1B’s block-size is 128). We also report results for
    coarse-grain quantization (CGQ) (per row per scale). The evaluation datasets are
    Wikitext2, PTB, C4, PTB-new, and C4-new. FP6 (FP5) format is E3M2 (E3M1)'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：零样本评估（困惑度$\downarrow$）。比较GPTQ[C4]和RTN量化算法在LLaMA模型中INT4权重（W4A16）的表现，模型尺寸为1B、13B和65B。我们应用了细粒度量化（FGQ），其中块大小为每个尺度256个元素（LLaMA-1B的块大小为128）。我们还报告了粗粒度量化（CGQ）的结果（每行每尺度）。评估数据集包括Wikitext2、PTB、C4、PTB-new和C4-new。FP6（FP5）格式为E3M2（E3M1）。
- en: Precision FGQ LLaMA-1B (4096-seq) LLaMA-13B (2048-seq) LLaMA-65B (2048-seq)
    FP16 N/A 24.31[7.53/37.39/8.91/58.34/9.40] 13.16[5.09/19.23/6.61/28.10/6.80] 6.41[3.56/8.00/5.62/8.88/5.98]
    INT4-GPTQ[C4] ✓ 564.73[7.83/1399.89/9.34/1396.76/9.84] 14.19[5.28/22.14/6.74/29.83/6.95]
    6.61[3.81/8.17/5.73/9.20/6.13] INT4-RTN ✓ 22.76[7.98/34.03/9.50/52.28/10.00] 14.32[5.35/22.49/6.80/29.96/7.00]
    7.30[3.79/10.13/5.74/10.54/6.31] INT4-GPTQ[C4] ✗ 288.22[8.20/719.21/9.84/693.48/10.37]
    14.13[5.37/21.31/6.84/30.01/7.09] 7.17[4.12/10.50/5.83/9.16/6.21] INT4-RTN ✗ 34.29[8.33/55.52/10.15/86.85/10.62]
    14.32[5.55/20.95/6.97/30.91/7.22] 7.72[4.20/10.59/5.90/11.44/6.47] FP5-GPTQ[C4]
    ✓ 44.29[7.74/72.63/9.19/122.20/9.68] 13.76[5.22/20.43/6.71/29.53/6.92] 6.50[3.67/8.15/5.68/8.95/6.04]
    FP5-RTN ✓ 28.52[7.78/44.09/9.23/71.79/9.71] 13.95[5.20/21.01/6.71/29.92/6.92]
    6.83[3.70/9.73/5.69/8.98/6.06] FP5-GPTQ[C4] ✗ 32.03[7.77/50.66/9.23/82.73/9.76]
    13.90[5.22/21.02/6.71/29.64/6.93] 6.46[3.68/7.83/5.70/9.13/5.98] FP5-RTN ✗ 27.92[7.83/41.80/9.27/70.94/9.77]
    14.10[5.22/21.52/6.72/30.13/6.93] 6.50[3.68/8.08/5.69/8.98/6.06] FP6-GPTQ[C4]
    ✓ 22.77[7.59/34.04/8.98/53.76/9.47] 13.36[5.13/19.67/6.63/28.53/6.83] 6.47[3.59/8.12/5.63/8.91/6.10]
    FP6-RTN ✓ 23.42[7.60/35.84/8.99/55.20/9.49] 13.24[5.12/19.43/6.63/28.19/6.83]
    6.44[3.58/8.04/5.63/8.92/6.05] FP6-GPTQ[C4] ✗ 23.58[7.59/35.76/8.98/56.10/9.47]
    13.23[5.12/19.34/6.64/28.20/6.83] 6.42[3.61/8.01/5.63/8.87/6.00] FP6-RTN ✗ 24.83[7.60/38.79/8.99/59.26/9.49]
    13.09[5.12/19.06/6.64/27.81/6.83] 6.42[3.59/8.01/5.63/8.89/5.99]
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 FGQ LLaMA-1B（4096-seq） LLaMA-13B（2048-seq） LLaMA-65B（2048-seq） FP16 不适用 24.31[7.53/37.39/8.91/58.34/9.40]
    13.16[5.09/19.23/6.61/28.10/6.80] 6.41[3.56/8.00/5.62/8.88/5.98] INT4-GPTQ[C4]
    ✓ 564.73[7.83/1399.89/9.34/1396.76/9.84] 14.19[5.28/22.14/6.74/29.83/6.95] 6.61[3.81/8.17/5.73/9.20/6.13]
    INT4-RTN ✓ 22.76[7.98/34.03/9.50/52.28/10.00] 14.32[5.35/22.49/6.80/29.96/7.00]
    7.30[3.79/10.13/5.74/10.54/6.31] INT4-GPTQ[C4] ✗ 288.22[8.20/719.21/9.84/693.48/10.37]
    14.13[5.37/21.31/6.84/30.01/7.09] 7.17[4.12/10.50/5.83/9.16/6.21] INT4-RTN ✗ 34.29[8.33/55.52/10.15/86.85/10.62]
    14.32[5.55/20.95/6.97/30.91/7.22] 7.72[4.20/10.59/5.90/11.44/6.47] FP5-GPTQ[C4]
    ✓ 44.29[7.74/72.63/9.19/122.20/9.68] 13.76[5.22/20.43/6.71/29.53/6.92] 6.50[3.67/8.15/5.68/8.95/6.04]
    FP5-RTN ✓ 28.52[7.78/44.09/9.23/71.79/9.71] 13.95[5.20/21.01/6.71/29.92/6.92]
    6.83[3.70/9.73/5.69/8.98/6.06] FP5-GPTQ[C4] ✗ 32.03[7.77/50.66/9.23/82.73/9.76]
    13.90[5.22/21.02/6.71/29.64/6.93] 6.46[3.68/7.83/5.70/9.13/5.98] FP5-RTN ✗ 27.92[7.83/41.80/9.27/70.94/9.77]
    14.10[5.22/21.52/6.72/30.13/6.93] 6.50[3.68/8.08/5.69/8.98/6.06] FP6-GPTQ[C4]
    ✓ 22.77[7.59/34.04/8.98/53.76/9.47] 13.36[5.13/19.67/6.63/28.53/6.83] 6.47[3.59/8.12/5.63/8.91/6.10]
    FP6-RTN ✓ 23.42[7.60/35.84/8.99/55.20/9.49] 13.24[5.12/19.43/6.63/28.19/6.83]
    6.44[3.58/8.04/5.63/8.92/6.05] FP6-GPTQ[C4] ✗ 23.58[7.59/35.76/8.98/56.10/9.47]
    13.23[5.12/19.34/6.64/28.20/6.83] 6.42[3.61/8.01/5.63/8.87/6.00] FP6-RTN ✗ 24.83[7.60/38.79/8.99/59.26/9.49]
    13.09[5.12/19.06/6.64/27.81/6.83] 6.42[3.59/8.01/5.63/8.89/5.99]
- en: 'We conduct the experiments described in Section [3](#S3.SS0.SSS0.Px1 "Experiment
    Settings ‣ 3 Comprehensive Evaluation is Needed ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks"), namely
    code generation, summarization and zero-shot experiments. The results are shown
    in Table [6](#S4.T6 "Table 6 ‣ 4.1 Results of FP6 and FP5 on all tasks ‣ 4 Sweet
    Spot Solution: FP6 ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric
    Strategy for Diverse Generative Tasks") and Table [6](#S4.T6 "Table 6 ‣ 4.1 Results
    of FP6 and FP5 on all tasks ‣ 4 Sweet Spot Solution: FP6 ‣ ZeroQuant(4+2): Redefining
    LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks").
    Additional results are presented in Table [A.1](#A1.T1 "Table A.1 ‣ Appendix A
    Background of Quantization ‣ ZeroQuant(4+2): Redefining LLMs Quantization with
    a New FP6-Centric Strategy for Diverse Generative Tasks") and Table [A.2](#A1.T2
    "Table A.2 ‣ Appendix A Background of Quantization ‣ ZeroQuant(4+2): Redefining
    LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks"),
    which we defer to appendix as it we do not find a clear link between bit precision
    and performance.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了第 [3](#S3.SS0.SSS0.Px1 "实验设置 ‣ 3 综合评估是必要的 ‣ ZeroQuant(4+2)：通过新的 FP6 中心策略重新定义
    LLMs 量化以应对各种生成任务") 节中描述的实验，即代码生成、总结和零样本实验。结果如表 [6](#S4.T6 "表 6 ‣ 4.1 FP6 和 FP5
    在所有任务中的结果 ‣ 4 最佳解决方案：FP6 ‣ ZeroQuant(4+2)：通过新的 FP6 中心策略重新定义 LLMs 量化以应对各种生成任务")
    和表 [6](#S4.T6 "表 6 ‣ 4.1 FP6 和 FP5 在所有任务中的结果 ‣ 4 最佳解决方案：FP6 ‣ ZeroQuant(4+2)：通过新的
    FP6 中心策略重新定义 LLMs 量化以应对各种生成任务") 展示。附加结果在表 [A.1](#A1.T1 "表 A.1 ‣ 附录 A 量化背景 ‣ ZeroQuant(4+2)：通过新的
    FP6 中心策略重新定义 LLMs 量化以应对各种生成任务") 和表 [A.2](#A1.T2 "表 A.2 ‣ 附录 A 量化背景 ‣ ZeroQuant(4+2)：通过新的
    FP6 中心策略重新定义 LLMs 量化以应对各种生成任务") 中展示，我们将这些结果推迟到附录中，因为我们未找到位精度与性能之间的明确关联。
- en: 'In general, the FP6 quantization method, particularly with CGQ, stands out
    in this analysis, offering a balance of high performance and robustness across
    different tasks, models and even quantiztion algorithms (RTN and GPTQ), a notable
    improvement over both FP5 and INT4 quantizations. In addition, we make the following
    observation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，FP6 量化方法，特别是使用 CGQ，在本次分析中脱颖而出，提供了在不同任务、模型甚至量化算法（RTN 和 GPTQ）之间的高性能和稳健性的平衡，相较于
    FP5 和 INT4 量化有显著改进。此外，我们做出以下观察：
- en: FP5 Performance. FP5 with CGQ shows an improvement over INT4 quantization but
    still does not reach the high performance levels of FP16\. The gap between FP5
    and its baseline is particularly noticeable in the Python and JavaScript code
    generation tasks across CodeGeeX2-6B, StarCoder-15B, and CodeLLaMA-34B.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: FP5 性能。FP5 与 CGQ 相比显示了相对于 INT4 量化的改进，但仍未达到 FP16 的高性能水平。在 Python 和 JavaScript
    代码生成任务中，FP5 与其基线之间的差距在 CodeGeeX2-6B、StarCoder-15B 和 CodeLLaMA-34B 中尤为显著。
- en: 'FP6 Robustness. FP6 quantization, especially with CGQ, demonstrates a significant
    advancement, nearly matching the FP16 baseline across various tasks and models.
    This quantization method not only narrows the performance gap seen in FP5 and
    INT4 but also shows robustness in handling different tasks. The robustness is
    further accentuated when comparing CGQ and FGQ within the FP6 framework (as there
    is little difference between CGQ and FGQ), where FP6 with CGQ consistently maintains
    high performance close to baseline, indicating its effectiveness and stability
    across different scenarios. Moreover, FP6 is robust to quantization algorithms:
    either RTN or GPTQ results in similar results, particularly for CodeLLaMA-34B,
    as shwon in Table [6](#S4.T6 "Table 6 ‣ 4.1 Results of FP6 and FP5 on all tasks
    ‣ 4 Sweet Spot Solution: FP6 ‣ ZeroQuant(4+2): Redefining LLMs Quantization with
    a New FP6-Centric Strategy for Diverse Generative Tasks").'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: FP6 稳健性。FP6 量化，特别是使用 CGQ，展示了显著的进步，几乎在各种任务和模型中达到与 FP16 基线相当的水平。这种量化方法不仅缩小了 FP5
    和 INT4 之间的性能差距，还表现出在处理不同任务时的稳健性。当在 FP6 框架内比较 CGQ 和 FGQ 时（因为 CGQ 和 FGQ 之间几乎没有差异），FP6
    与 CGQ 一致地保持接近基线的高性能，表明其在不同场景下的有效性和稳定性。此外，FP6 对量化算法具有鲁棒性：无论是 RTN 还是 GPTQ 都能得到类似的结果，特别是对于
    CodeLLaMA-34B，如表 [6](#S4.T6 "表 6 ‣ 4.1 FP6 和 FP5 在所有任务中的结果 ‣ 4 最佳解决方案：FP6 ‣ ZeroQuant(4+2)：通过新的
    FP6 中心策略重新定义 LLMs 量化以应对各种生成任务") 所示。
- en: 5 System Support Discussion
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 系统支持讨论
- en: 5.1 4+2 format for FP6
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 FP6 的 4+2 格式
- en: 'In addressing the challenges of utilizing a non-standard 6-bit number format,
    which deviates from the conventional power-of-2 numerical systems (termed as "odd
    bit precision setting"), we propose a novel approach. This method is distinct
    from the two commonly considered strategies:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 针对使用非标准6位数字格式（偏离传统的2的幂数字系统，称为“奇数位精度设置”）的挑战，我们提出了一种新颖的方法。这种方法不同于两种常见的策略：
- en: '1.'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: The first approach involves directly converting the 6-bit format into an 8-bit
    floating-point (FP8) format. While this is a straightforward solution, it unfortunately
    negates the primary benefit of the 6-bit format, which is to conserve memory.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一种方法直接将6位格式转换为8位浮点数（FP8）格式。虽然这是一个简单的解决方案，但不幸的是，它否定了6位格式的主要优点，即节省内存。
- en: '2.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: The second approach entails grouping several 6-bit numbers together in a continuous
    memory block and representing them using either 32-bit integers (INT32) or a 32-bit
    floating-point (FP32) format. This method maintains the memory-saving advantage
    but adds complexity to the dequantization process.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二种方法涉及将几个6位数字组合在一个连续的内存块中，并使用32位整数（INT32）或32位浮点数（FP32）格式表示。这种方法保持了节省内存的优势，但增加了去量化过程的复杂性。
- en: 'Our unique strategy, however, focuses on dividing the 6-bit number into two
    distinct sub-numbers: the first sub-number representing the initial 4 bits, and
    the second sub-number accounting for the remaining 2 bits. Our poposed "4+2" method
    can be seen as an advanced variation of the second standard approach. The 4+2
    bit division is based on the fundamental principle that any positive integer can
    be expressed as a sum of powers of 2\. With this foundation, we divide the 6-bit
    number into two components:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的独特策略专注于将6位数分成两个不同的子数字：第一个子数字表示最初的4位，第二个子数字表示剩余的2位。我们提出的“4+2”方法可以看作是第二种标准方法的高级变体。4+2位的划分基于一个基本原则，即任何正整数都可以表示为2的幂的和。以此为基础，我们将6位数分成两个组件：
- en: •
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The first part, comprising the initial 4 bits, handles elements such as the
    sign bit and a 3-bit exponent.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一部分包括最初的4位，处理诸如符号位和3位指数等元素。
- en: •
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The second part, containing the remaining 2 bits, is dedicated to the 2-bit
    mantissa.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第二部分包含剩余的2位，专门用于2位尾数。
- en: This division into 4+2 bits facilitates simultaneous loading and dequantization
    of these sub-numbers, culminating in the generation of the final 16-bit floating-point
    (FP16) weight. Our approach innovatively balances the need for reduced memory
    footprint with the practicalities of dequantization, particularly in addressing
    the challenges of memory access across segmented numbers.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这种4+2位的划分便于同时加载和去量化这些子数字，从而生成最终的16位浮点数（FP16）权重。我们的方法创新性地平衡了减少内存占用的需求与去量化的实际操作，特别是在处理分段数字的内存访问挑战时。
- en: 5.2 Bias Shift
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 偏差移位
- en: 'Dequantizing FP6 to FP16 during runtime on GPUs can be significantly resource-intensive,
    primarily due to the complexity involved in manipulating the exponent field, as
    detailed in Section [4](#S4 "4 Sweet Spot Solution: FP6 ‣ ZeroQuant(4+2): Redefining
    LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks").'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '在GPU上进行FP6到FP16的去量化可能非常耗费资源，主要是因为处理指数域的复杂性，如第[4](#S4 "4 Sweet Spot Solution:
    FP6 ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks")节所述。'
- en: 'The bias term for the exponent, typically determined by the exponent bits,
    is 3 for FP6 and 15 for FP16\. Mathematically, the process of dequantizing FP6
    to FP16 (excluding the sign) is represented as:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 指数的偏差项，通常由指数位确定，对于FP6是3，对于FP16是15。数学上，FP6到FP16的去量化过程（不包括符号）表示为：
- en: '|  | $2^{E_{\text{FP16}}-15}\times M_{\text{FP16}}=2^{E_{\text{FP6}}-3}\times
    M_{\text{FP6}},$ |  | (4) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $2^{E_{\text{FP16}}-15}\times M_{\text{FP16}}=2^{E_{\text{FP6}}-3}\times
    M_{\text{FP6}},$ |  | (4) |'
- en: where the superscripts FP16/FP6 indicate the respective format. It is noteworthy
    that the scaling factor dequantization can be done after matrix multiplication
    before the accumulation for fine-grained (sub-row) quantization schemes or after
    accumulation for coarse-grained (row-wise) quantization schemes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 其中上标FP16/FP6表示各自的格式。值得注意的是，缩放因子的去量化可以在矩阵乘法后进行，针对细粒度（子行）量化方案，或在累加后进行，针对粗粒度（行级）量化方案。
- en: 'While padding can easily adjust the mantissa, aligning the exponents requires
    more effort due to the difference in biases. An analogy can be drawn with converting
    an INT4 number back to a symmetric INT8 format: if INT4 employs a symmetric format
    (for the mantissa), zero padding suffices. However, in an asymmetric format, padding
    alone is inadequate, and additional steps are necessary.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然填充可以轻松调整尾数，但由于偏置的差异，对指数进行对齐需要更多的工作。可以用将 INT4 数字转换回对称 INT8 格式的类比来说明：如果 INT4
    使用对称格式（用于尾数），零填充就足够了。然而，在非对称格式中，单纯的填充是不够的，还需要额外的步骤。
- en: 'To address this, we have customized our FP6 format with a non-standard exponent
    bias of 15\. This modification does not affect precision or accuracy because:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们定制了 FP6 格式，使用了非标准的指数偏置 15。这一修改不会影响精度或准确性，因为：
- en: '|  | $S_{\text{FP16}}\times 2^{E-3}\times M=(S_{\text{FP16}}\times 2^{12})\times
    2^{E-15}\times M,$ |  | (5) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{\text{FP16}}\times 2^{E-3}\times M=(S_{\text{FP16}}\times 2^{12})\times
    2^{E-15}\times M,$ |  | (5) |'
- en: meaning the bias shift can be seamlessly integrated into the scaling factor.
    Crucially, since $S_{\text{FP16}}$ still allows for accurate representation in
    FP16 format through simple exponent bit shifting, avoiding numerical errors.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着偏置移位可以无缝地集成到缩放因子中。关键是，由于 $S_{\text{FP16}}$ 仍然允许通过简单的指数位移在 FP16 格式中准确表示，从而避免了数值误差。
- en: '![Refer to caption](img/3d966d5e766140f3b41d88d5c75c0f86.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/3d966d5e766140f3b41d88d5c75c0f86.png)'
- en: (a) Before Bias Shift.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 偏置移位前。
- en: '![Refer to caption](img/57d2157dada2d2460accc5787083b026.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/57d2157dada2d2460accc5787083b026.png)'
- en: (b) After Bias Shift.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 偏置移位后。
- en: 'Figure 1: Comparison of the Traditional Method (Left) versus Our Proposed Method
    (Right): Demonstrating the Significant Run-time Advantages of Bias Shift.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：传统方法（左）与我们提出的方法（右）的比较：展示了偏置移位的显著运行时优势。
- en: 'Our bias shift method greatly simplifies the FP6-FP16 de-quantization process
    during runtime. To demonstrate this, we provide a side-by-side comparison in Figures [1a](#S5.F1.sf1
    "In Figure 1 ‣ 5.2 Bias Shift ‣ 5 System Support Discussion ‣ ZeroQuant(4+2):
    Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative
    Tasks") and [1b](#S5.F1.sf2 "In Figure 1 ‣ 5.2 Bias Shift ‣ 5 System Support Discussion
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的偏置移位方法大大简化了在运行时的 FP6-FP16 去量化过程。为了说明这一点，我们在图 [1a](#S5.F1.sf1 "在图 1 ‣ 5.2
    偏置移位 ‣ 5 系统支持讨论 ‣ ZeroQuant(4+2): 通过新的 FP6 以中心策略重新定义 LLMs 量化用于多样化生成任务") 和 [1b](#S5.F1.sf2
    "在图 1 ‣ 5.2 偏置移位 ‣ 5 系统支持讨论 ‣ ZeroQuant(4+2): 通过新的 FP6 以中心策略重新定义 LLMs 量化用于多样化生成任务")
    中提供了并排比较。'
- en: 'Figure [1a](#S5.F1.sf1 "In Figure 1 ‣ 5.2 Bias Shift ‣ 5 System Support Discussion
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks") outlines the original two-step process for de-quantizing
    each FP6 weight. The first step involves casting $W_{\text{fp6}}$. adds further
    complexity to the de-quantization during runtime.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1a](#S5.F1.sf1 "在图 1 ‣ 5.2 偏置移位 ‣ 5 系统支持讨论 ‣ ZeroQuant(4+2): 通过新的 FP6 以中心策略重新定义
    LLMs 量化用于多样化生成任务") 概述了去量化每个 FP6 权重的原始两步过程。第一步涉及 $W_{\text{fp6}}$ 的转换，这在运行时增加了进一步的复杂性。'
- en: 'However, with our bias shift strategy, as illustrated in Figure [1b](#S5.F1.sf2
    "In Figure 1 ‣ 5.2 Bias Shift ‣ 5 System Support Discussion ‣ ZeroQuant(4+2):
    Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative
    Tasks"), the exponent adjustment becomes a straightforward bit-level padding process.
    The addition of the constant integer 12, initially required in Step 1, can now
    be deferred to Step 2, eliminating any runtime overhead. This is possible because
    the multiplication of the quantization scales with the constant integer can be
    performed statically after the model is quantized and before runtime. Moreover,
    this streamlined approach also efficiently accommodates the de-quantization of
    subnormal numbers.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，正如图 [1b](#S5.F1.sf2 "在图 1 ‣ 5.2 偏置移位 ‣ 5 系统支持讨论 ‣ ZeroQuant(4+2): 通过新的 FP6
    以中心策略重新定义 LLMs 量化用于多样化生成任务") 所示，采用我们的偏置移位策略，指数调整变成了一个简单的位级填充过程。在步骤 1 中最初需要的常数整数
    12 现在可以推迟到步骤 2，从而消除了任何运行时开销。这是因为量化尺度与常数整数的乘法可以在模型量化后静态执行，并在运行时之前完成。此外，这种简化的方法也有效地处理了非标准数字的去量化。'
- en: 5.3 System evaluation
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 系统评估
- en: 'We conducted an evaluation of the latest GPU kernels across various weight-only
    quantization techniques, focusing on their system performance. This kernel-level
    assessment was carried out on the NVIDIA A100-40GB platform, running Linux 5.3.18
    and CUDA 11.8\. Our primary focus was on the performance of feed-forward (FFN)
    layers within the LLaMA models, specifically during the token generation phase,
    as detailed in [[57](#bib.bib57)]. Comprehensive data on matrix shapes and kernel
    latency is available in Appendix [B](#A2 "Appendix B Detailed Performance of GPU
    Kernels ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric
    Strategy for Diverse Generative Tasks"). We employed cuBLAS [[48](#bib.bib48)]
    as our benchmark for non-quantized performance (W16A16). We also included cutting-edge
    kernel support for F INT4 FGQ quantization (W4A16) from TensorRT-LLM [[49](#bib.bib49)]
    for comparative analysis.^(12)^(12)12Between the supported block-size: 64 and
    128, we chose 128.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对最新的GPU内核进行了评估，涉及各种仅重量化技术，重点关注它们的系统性能。该内核级评估在NVIDIA A100-40GB平台上进行，运行Linux
    5.3.18和CUDA 11.8。我们的主要关注点是LLaMA模型中前馈（FFN）层的性能，特别是在令牌生成阶段，如[[57](#bib.bib57)]所述。有关矩阵形状和内核延迟的详细数据可在附录[B](#A2
    "Appendix B Detailed Performance of GPU Kernels ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")中获得。我们使用cuBLAS
    [[48](#bib.bib48)]作为非量化性能（W16A16）的基准。我们还包括了TensorRT-LLM [[49](#bib.bib49)]提供的先进内核支持，用于F
    INT4 FGQ量化（W4A16）的比较分析。^(12)^(12)12在支持的块大小：64和128之间，我们选择了128。'
- en: '![Refer to caption](img/8a674d7a03f406347a3e83b60d5b74b0.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8a674d7a03f406347a3e83b60d5b74b0.png)'
- en: 'Figure 2: GPU Kernel Speedups compared to cuBLAS. FFN1 and FFN2 are defined
    for matrices of $4H\times H$. See detailed number in Table [B.1](#A2.T1 "Table
    B.1 ‣ Appendix B Detailed Performance of GPU Kernels ‣ ZeroQuant(4+2): Redefining
    LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：与cuBLAS相比的GPU内核加速。FFN1和FFN2用于$4H\times H$的矩阵。详细数字见表[B.1](#A2.T1 "Table B.1
    ‣ Appendix B Detailed Performance of GPU Kernels ‣ ZeroQuant(4+2): Redefining
    LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")'
- en: 'To elucidate the advantages of our proposed Bias Shift technique, detailed
    in Section [5.2](#S5.SS2 "5.2 Bias Shift ‣ 5 System Support Discussion ‣ ZeroQuant(4+2):
    Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative
    Tasks"), we also developed and tested an FP6 GPU kernel without Bias Shift.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '为了阐明我们提出的**偏置移位**技术的优势，如第[5.2节](#S5.SS2 "5.2 Bias Shift ‣ 5 System Support
    Discussion ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric
    Strategy for Diverse Generative Tasks")所述，我们还开发并测试了一个没有偏置移位的FP6 GPU内核。'
- en: 'According to the results presented in Figure [2](#S5.F2 "Figure 2 ‣ 5.3 System
    evaluation ‣ 5 System Support Discussion ‣ ZeroQuant(4+2): Redefining LLMs Quantization
    with a New FP6-Centric Strategy for Diverse Generative Tasks"), our FP6 kernel,
    enhanced with Bias-Shift, achieved speeds up to $2.37\times$ faster compared to
    the same FP6 kernel without Bias Shift, underscoring the crucial role of Bias-Shift
    as discussed in Section [5.2](#S5.SS2 "5.2 Bias Shift ‣ 5 System Support Discussion
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks").'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '根据图[2](#S5.F2 "Figure 2 ‣ 5.3 System evaluation ‣ 5 System Support Discussion
    ‣ ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy
    for Diverse Generative Tasks")中展示的结果，我们的FP6内核，经过**偏置移位**增强，与没有偏置移位的相同FP6内核相比，速度提高了$2.37\times$，这突显了第[5.2节](#S5.SS2
    "5.2 Bias Shift ‣ 5 System Support Discussion ‣ ZeroQuant(4+2): Redefining LLMs
    Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks")中讨论的**偏置移位**的关键作用。'
- en: 6 Discussion and Conclusion
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与结论
- en: Our paper introduces a novel approach to GPU kernel optimization, specifically
    targeting weight-only quantization methods. Despite the significance of our findings,
    they pave the way for further research and development in several areas.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的论文介绍了一种针对仅重量化方法的GPU内核优化新方法。尽管我们的发现具有重要意义，但它们为多个领域的进一步研究和开发铺平了道路。
- en: Evaluation Scope. A primary limitation of our study is still the narrow scope
    of evaluation although we have span to code generation and summarization. We suggest
    a vast field for future research. Expanding the scope to include diverse tasks
    and a focus on both performance and accuracy could enhance the robustness of our
    methods.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 评估范围。我们研究的一个主要局限性仍然是评估范围狭窄，尽管我们已经扩展到代码生成和总结。我们建议未来研究应涵盖更广泛的领域。扩大范围以包括多样化任务，并关注性能和准确性，可以提高我们方法的鲁棒性。
- en: Comparative Analysis. While our system evaluation provides valuable insights,
    it lacks in-depth comparison with state-of-the-art (SOTA) frameworks. A more comprehensive
    benchmarking against advanced frameworks would offer a clearer perspective on
    our approach’s efficacy and areas for improvement.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 比较分析。尽管我们的系统评估提供了有价值的见解，但缺乏与最先进（SOTA）框架的深入比较。对先进框架进行更全面的基准测试将为我们的方法的有效性和改进领域提供更清晰的视角。
- en: Technique Adaptability. A notable aspect of our work is the adaptability of
    our techniques, particularly in bit-precision and bias-shift. The potential to
    adapt our methods to emerging standards, such as 5-bit quantization, demonstrates
    their flexibility and future applicability in various contexts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 技术适应性。我们工作的一个显著方面是技术的适应性，特别是在位精度和偏差移位方面。将我们的方法适应于新兴标准，如5位量化，展示了其灵活性和在各种背景下的未来适用性。
- en: Future Directions. The advancement of Post-Training Quantization (PTQ) methods
    and their integration with other techniques presents exciting future possibilities.
    Our research lays the foundation for further advancements in model optimization,
    such as the 3-bit precision from evolving quantization techniques.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 未来方向。后训练量化（PTQ）方法的发展及其与其他技术的集成展示了令人兴奋的未来可能性。我们的研究为模型优化的进一步发展奠定了基础，例如从不断发展的量化技术中得到的3位精度。
- en: In conclusion, our study marks a significant contribution to odd-bit GPU kernel
    optimization. It also opens avenues for broader research, exploring the full potential
    of model optimization and quantization across diverse applications.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的研究标志着对奇数位GPU内核优化的重大贡献。它还为更广泛的研究开辟了途径，探索了模型优化和量化在各种应用中的全部潜力。
- en: Contributions
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 贡献
- en: Xiaoxia Wu led model quality measurement and paper writing. Haojun Xia, contributing
    to system section writing, designed the FP6 acceleration strategy and system support.
    Stephen Youn, Zhen Zheng, and Shiyang Chen handled DeepSpeed integration. Arash
    Bakhtiari and Michael Wyatt managed framework testing and external integration.
    Reza Yazdani Aminabadi, Yuxiong He and Olatunji Ruwase provided key discussions.
    Leon Song is the overall leader for algorithm and system design. Zhewei Yao initiated
    the project and led the FP6/5 format design.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Xiaoxia Wu 负责模型质量测量和论文撰写。Haojun Xia 参与系统部分的撰写，设计了 FP6 加速策略和系统支持。Stephen Youn、Zhen
    Zheng 和 Shiyang Chen 负责 DeepSpeed 集成。Arash Bakhtiari 和 Michael Wyatt 负责框架测试和外部集成。Reza
    Yazdani Aminabadi、Yuxiong He 和 Olatunji Ruwase 提供了关键讨论。Leon Song 是算法和系统设计的总体负责人。Zhewei
    Yao 启动了项目并领导了 FP6/5 格式设计。
- en: References
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ardavan Afshar, Ioakeim Perros, Evangelos E Papalexakis, Elizabeth Searles,
    Joyce Ho, and Jimeng Sun. Copa: Constrained parafac2 for sparse & large datasets.
    In Proceedings of the 27th ACM International Conference on Information and Knowledge
    Management, pages 793–802, 2018.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Ardavan Afshar, Ioakeim Perros, Evangelos E Papalexakis, Elizabeth Searles,
    Joyce Ho, 和 Jimeng Sun. Copa: 适用于稀疏大数据集的约束 parafac2. 见于第27届ACM国际信息与知识管理会议论文集，第793–802页，2018年。'
- en: '[2] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang,
    Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end 4-bit inference
    on generative large language models. arXiv preprint arXiv:2310.09259, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang,
    Jie Ren, Torsten Hoefler, 和 Dan Alistarh. 迈向生成大型语言模型的端到端4位推理. arXiv预印本 arXiv:2310.09259,
    2023年。'
- en: '[3] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
    Michael Lyu, and Irwin King. Binarybert: Pushing the limit of bert quantization.
    arXiv preprint arXiv:2012.15701, 2020.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,
    Michael Lyu, 和 Irwin King. BinaryBERT: 推动 BERT 量化的极限. arXiv预印本 arXiv:2012.15701,
    2020年。'
- en: '[4] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj,
    Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi,
    Nicholas Mattei, et al. A systematic classification of knowledge, reasoning, and
    context within the arc dataset. arXiv preprint arXiv:1806.00358, 2018.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish Yuvraj,
    Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche, Pavan Kapanipathi,
    Nicholas Mattei, 等人. ARC数据集中的知识、推理和上下文的系统分类. arXiv预印本 arXiv:1806.00358, 2018年。'
- en: '[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. arXiv preprint arXiv:2005.14165,
    2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    等人. 语言模型是少量示例学习者. arXiv预印本 arXiv:2005.14165, 2020年。'
- en: '[6] Yaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami, Michael W Mahoney, and
    Kurt Keutzer. Zeroq: A novel zero shot quantization framework. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13169–13178,
    2020.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Yaohui Cai、Zhewei Yao、Zhen Dong、Amir Gholami、Michael W Mahoney 和 Kurt Keutzer。Zeroq：一种新颖的零样本量化框架。发表于
    IEEE/CVF 计算机视觉与模式识别会议论文集，页码 13169–13178，2020年。'
- en: '[7] Léopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, Oguz H
    Elibol, and Hanlin Tang. Shifted and squeezed 8-bit floating point format for
    low-precision training of deep neural networks. arXiv preprint arXiv:2001.05674,
    2020.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Léopold Cambier、Anahita Bhiwandiwalla、Ting Gong、Mehran Nekuii、Oguz H Elibol
    和 Hanlin Tang。用于深度神经网络低精度训练的移位和压缩 8-bit 浮点格式。arXiv 预印本 arXiv:2001.05674，2020年。'
- en: '[8] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. Quip:
    2-bit quantization of large language models with guarantees. arXiv preprint arXiv:2307.13304,
    2023.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Jerry Chee、Yaohui Cai、Volodymyr Kuleshov 和 Christopher De Sa。Quip：具有保证的
    2-bit 大语言模型量化。arXiv 预印本 arXiv:2307.13304，2023年。'
- en: '[9] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of
    natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Christopher Clark、Kenton Lee、Ming-Wei Chang、Tom Kwiatkowski、Michael Collins
    和 Kristina Toutanova。Boolq：探索自然是/否问题的意外困难。arXiv 预印本 arXiv:1905.10044，2019年。'
- en: '[10] Wikipedia contributors. Floating-point arithmetic — Wikipedia, the free
    encyclopedia, last edited 2023. [Online; accessed 8-December-2023].'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Wikipedia 贡献者。浮点运算 — 维基百科，自由百科全书，最后编辑于 2023 年。[在线；访问日期 2023年12月8日]。'
- en: '[11] Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. Recognizing
    textual entailment: Models and applications. Synthesis Lectures on Human Language
    Technologies, 6(4):1–220, 2013.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Ido Dagan、Dan Roth、Mark Sammons 和 Fabio Massimo Zanzotto。识别文本蕴涵：模型与应用。Synthesis
    Lectures on Human Language Technologies，第 6 卷，第 4 期：1–220，2013年。'
- en: '[12] Marie-Catherine De Marneffe, Mandy Simons, and Judith Tonhauser. The commitmentbank:
    Investigating projection in naturally occurring discourse. In proceedings of Sinn
    und Bedeutung, volume 23, pages 107–124, 2019.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Marie-Catherine De Marneffe、Mandy Simons 和 Judith Tonhauser。承诺库：研究自然发生语篇中的投射。发表于
    Sinn und Bedeutung 会议论文集，第 23 卷，页码 107–124，2019年。'
- en: '[13] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。Qlora：高效微调量化
    LLMs。arXiv 预印本 arXiv:2305.14314，2023年。'
- en: '[14] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit
    inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Tim Dettmers 和 Luke Zettlemoyer. 4-bit 精度的案例：k-bit 推理扩展规律。arXiv 预印本 arXiv:2212.09720，2022年。'
- en: '[15] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt Keutzer.
    HAWQ: Hessian aware quantization of neural networks with mixed-precision. In Proceedings
    of the IEEE International Conference on Computer Vision, pages 293–302, 2019.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Zhen Dong、Zhewei Yao、Amir Gholami、Michael W Mahoney 和 Kurt Keutzer。HAWQ：具有混合精度的
    Hessian 关注神经网络量化。发表于 IEEE 国际计算机视觉大会论文集，页码 293–302，2019年。'
- en: '[16] Steven K Esser, Jeffrey L McKinstry, Deepika Bablani, Rathinakumar Appuswamy,
    and Dharmendra S Modha. Learned step size quantization. arXiv preprint arXiv:1902.08153,
    2019.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Steven K Esser、Jeffrey L McKinstry、Deepika Bablani、Rathinakumar Appuswamy
    和 Dharmendra S Modha。学习的步长量化。arXiv 预印本 arXiv:1902.08153，2019年。'
- en: '[17] Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval,
    Herve Jegou, and Armand Joulin. Training with quantization noise for extreme fixed-point
    compression. arXiv preprint arXiv:2004.07320, 2020.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Angela Fan、Pierre Stock、Benjamin Graham、Edouard Grave、Remi Gribonval、Herve
    Jegou 和 Armand Joulin。通过量化噪声进行极限固定点压缩的训练。arXiv 预印本 arXiv:2004.07320，2020年。'
- en: '[18] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework
    for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580,
    2022.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Elias Frantar 和 Dan Alistarh。最优大脑压缩：准确的训练后量化和剪枝框架。arXiv 预印本 arXiv:2208.11580，2022年。'
- en: '[19] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。Gptq：生成预训练变换器的准确训练后量化。arXiv
    预印本 arXiv:2210.17323，2022年。'
- en: '[20] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, and
    Kurt Keutzer. A survey of quantization methods for efficient neural network inference.
    arXiv preprint arXiv:2103.13630, 2021.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Amir Gholami、Sehoon Kim、Zhen Dong、Zhewei Yao、Michael W Mahoney 和 Kurt
    Keutzer。高效神经网络推理的量化方法调查。arXiv 预印本 arXiv:2103.13630，2021年。'
- en: '[21] GitHub. Github copilot. [https://github.com/features/copilot/](https://github.com/features/copilot/),
    2021.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] GitHub。Github copilot。 [https://github.com/features/copilot/](https://github.com/features/copilot/)，2021。'
- en: '[22] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang,
    Yunxin Liu, Minyi Guo, and Yuhao Zhu. Olive: Accelerating large language models
    via hardware-friendly outlier-victim pair quantization. In Proceedings of the
    50th Annual International Symposium on Computer Architecture, pages 1–15, 2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Cong Guo, Jiaming Tang, Weiming Hu, Jingwen Leng, Chen Zhang, Fan Yang,
    Yunxin Liu, Minyi Guo 和 Yuhao Zhu。Olive：通过硬件友好的异常值-受害者对量化加速大型语言模型。 在第50届国际计算机架构年会上，页码
    1–15，2023。'
- en: '[23] Han Guo, Philip Greengard, Eric P Xing, and Yoon Kim. Lq-lora: Low-rank
    plus quantized matrix decomposition for efficient language model finetuning. arXiv
    preprint arXiv:2311.12023, 2023.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Han Guo, Philip Greengard, Eric P Xing 和 Yoon Kim。Lq-lora：低秩加量化矩阵分解用于高效的语言模型微调。arXiv
    预印本 arXiv:2311.12023，2023。'
- en: '[24] Babak Hassibi and David G Stork. Second order derivatives for network
    pruning: Optimal brain surgeon. In Advances in neural information processing systems,
    pages 164–171, 1993.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Babak Hassibi 和 David G Stork。网络剪枝的二阶导数：最佳脑外科医生。在神经信息处理系统进展中，页码 164–171，1993。'
- en: '[25] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt,
    Will Kay, Mustafa Suleyman, and Phil Blunsom. Teaching machines to read and comprehend.
    arXiv preprint arXiv:1506.03340, 2015.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt,
    Will Kay, Mustafa Suleyman 和 Phil Blunsom。教机器阅读和理解。arXiv 预印本 arXiv:1506.03340，2015。'
- en: '[26] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
    Bengio. Quantized neural networks: Training neural networks with low precision
    weights and activations. The Journal of Machine Learning Research, 18(1):6869–6898,
    2017.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv 和 Yoshua
    Bengio。量化神经网络：使用低精度权重和激活进行神经网络训练。《机器学习研究期刊》，18(1)：6869–6898，2017。'
- en: '[27] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo,
    Se Jung Kwon, and Dongsoo Lee. Memory-efficient fine-tuning of compressed large
    language models via sub-4-bit integer quantization. arXiv preprint arXiv:2305.14152,
    2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo,
    Se Jung Kwon 和 Dongsoo Lee。通过小于4位整数量化实现压缩大型语言模型的内存高效微调。arXiv 预印本 arXiv:2305.14152，2023。'
- en: '[28] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney, and Kurt Keutzer.
    I-bert: Integer-only bert quantization. In International conference on machine
    learning, pages 5506–5518\. PMLR, 2021.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Sehoon Kim, Amir Gholami, Zhewei Yao, Michael W Mahoney 和 Kurt Keutzer。I-bert：仅整数的bert量化。在国际机器学习大会上，页码
    5506–5518。PMLR，2021。'
- en: '[29] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney 和 Kurt Keutzer。Squeezellm：稠密与稀疏量化。arXiv 预印本 arXiv:2306.07629，2023。'
- en: '[30] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan
    Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney, et al.
    Full stack optimization of transformer inference: a survey. arXiv preprint arXiv:2302.14017,
    2023.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan
    Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W Mahoney 等。变换器推理的全栈优化：综述。arXiv
    预印本 arXiv:2302.14017，2023。'
- en: '[31] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for
    efficient inference: A whitepaper. arXiv preprint arXiv:1806.08342, 2018.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Raghuraman Krishnamoorthi。量化深度卷积网络以提高推理效率：一份白皮书。arXiv 预印本 arXiv:1806.08342，2018。'
- en: '[32] Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,
    and Tijmen Blankevoort. Fp8 quantization: The power of the exponent. arXiv preprint
    arXiv:2208.09225, 2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters 和
    Tijmen Blankevoort。Fp8 量化：指数的力量。arXiv 预印本 arXiv:2208.09225，2022。'
- en: '[33] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In
    Advances in neural information processing systems, pages 598–605, 1990.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Yann LeCun, John S Denker 和 Sara A Solla。最佳脑损伤。在神经信息处理系统进展中，页码 598–605，1990。'
- en: '[34] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park.
    Owq: Lessons learned from activation outliers for weight quantization in large
    language models. arXiv preprint arXiv:2306.02272, 2023.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim 和 Eunhyeok Park。Owq：从激活异常值中汲取教训以进行大型语言模型的权重量化。arXiv
    预印本 arXiv:2306.02272，2023。'
- en: '[35] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema
    challenge. In Thirteenth International Conference on the Principles of Knowledge
    Representation and Reasoning. Citeseer, 2012.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Hector Levesque、Ernest Davis 和 Leora Morgenstern。Winograd 方案挑战。在第十三届知识表示与推理原则国际会议上。Citeseer，2012年。'
- en: '[36] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov,
    Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii
    Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,
    Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,
    Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham
    Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry
    Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya,
    Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor
    Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger,
    Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,
    Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel
    Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes,
    Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may
    the source be with you! 2305.06161, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Raymond Li、Loubna Ben Allal、Yangtian Zi、Niklas Muennighoff、Denis Kocetkov、Chenghao
    Mou、Marc Marone、Christopher Akiki、Jia Li、Jenny Chim、Qian Liu、Evgenii Zheltonozhskii、Terry
    Yue Zhuo、Thomas Wang、Olivier Dehaene、Mishig Davaadorj、Joel Lamy-Poirier、João Monteiro、Oleh
    Shliazhko、Nicolas Gontier、Nicholas Meade、Armel Zebaze、Ming-Ho Yee、Logesh Kumar
    Umapathi、Jian Zhu、Benjamin Lipkin、Muhtasham Oblokulov、Zhiruo Wang、Rudra Murthy、Jason
    Stillerman、Siva Sankalp Patel、Dmitry Abulkhanov、Marco Zocca、Manan Dey、Zhihan Zhang、Nour
    Fahmy、Urvashi Bhattacharyya、Wenhao Yu、Swayam Singh、Sasha Luccioni、Paulo Villegas、Maxim
    Kunakov、Fedor Zhdanov、Manuel Romero、Tony Lee、Nadav Timor、Jennifer Ding、Claire
    Schlesinger、Hailey Schoelkopf、Jan Ebert、Tri Dao、Mayank Mishra、Alex Gu、Jennifer
    Robinson、Carolyn Jane Anderson、Brendan Dolan-Gavitt、Danish Contractor、Siva Reddy、Daniel
    Fried、Dzmitry Bahdanau、Yacine Jernite、Carlos Muñoz Ferrandis、Sean Hughes、Thomas
    Wolf、Arjun Guha、Leandro von Werra 和 Harm de Vries。Starcoder：愿源代码与你同在！2305.06161，2023年。'
- en: '[37] Zheng Li, Zijian Wang, Ming Tan, Ramesh Nallapati, Parminder Bhatia, Andrew
    Arnold, Bing Xiang, and Dan Roth. Dq-bart: Efficient sequence-to-sequence model
    via joint distillation and quantization. In Proceedings of the 60th Annual Meeting
    of the Association for Computational Linguistics (Volume 2: Short Papers), pages
    203–211, 2022.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Zheng Li、Zijian Wang、Ming Tan、Ramesh Nallapati、Parminder Bhatia、Andrew
    Arnold、Bing Xiang 和 Dan Roth。DQ-BART：通过联合蒸馏和量化实现高效的序列到序列模型。在第60届计算语言学协会年会（卷2：短篇论文）论文集中，第203–211页，2022年。'
- en: '[38] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Xingyu Dang 和 Song Han。AWQ：用于
    LLM 压缩和加速的激活感知权重量化。arXiv 预印本 arXiv:2306.00978，2023年。'
- en: '[39] Bingchang Liu, Chaoyu Chen, Cong Liao, Zi Gong, Huan Wang, Zhichao Lei,
    Ming Liang, Dajun Chen, Min Shen, Hailian Zhou, Hang Yu, and Jianguo Li. Mftcoder:
    Boosting code llms with multitask fine-tuning. arXiv preprint arXiv, 2023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Bingchang Liu、Chaoyu Chen、Cong Liao、Zi Gong、Huan Wang、Zhichao Lei、Ming
    Liang、Dajun Chen、Min Shen、Hailian Zhou、Hang Yu 和 Jianguo Li。MFTCoder：通过多任务微调提升代码
    LLM。arXiv 预印本 arXiv，2023年。'
- en: '[40] Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong, and Kwang-Ting
    Cheng. Llm-fp4: 4-bit floating-point quantized transformers. arXiv preprint arXiv:2310.16836,
    2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Shih-yang Liu、Zechun Liu、Xijie Huang、Pingcheng Dong 和 Kwang-Ting Cheng。LLM-FP4:
    4位浮点量化变换器。arXiv 预印本 arXiv:2310.16836，2023年。'
- en: '[41] Zhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei Ma, and Wen Gao. Post-training
    quantization for vision transformer. Advances in Neural Information Processing
    Systems, 34, 2021.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Zhenhua Liu、Yunhe Wang、Kai Han、Wei Zhang、Siwei Ma 和 Wen Gao。视觉变换器的后训练量化。《神经信息处理系统进展》，34，2021年。'
- en: '[42] Mary Ann Marcinkiewicz. Building a large annotated corpus of english:
    The penn treebank. Using Large Corpora, page 273, 1994.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Mary Ann Marcinkiewicz。构建大型英文注释语料库：宾州树库。在《使用大规模语料库》中，第273页，1994年。'
- en: '[43] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. In International Conference on Learning Representations,
    2017.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher。指针哨兵混合模型。在国际学习表示会议上，2017年。'
- en: '[44] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep
    Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John
    Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433,
    2022.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Paulius Micikevicius、Dusan Stosic、Neil Burgess、Marius Cornea、Pradeep Dubey、Richard
    Grisenthwaite、Sangwon Ha、Alexander Heinecke、Patrick Judd、John Kamalu 等人。FP8 格式用于深度学习。arXiv
    预印本 arXiv:2209.05433，2022年。'
- en: '[45] Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers, and
    James Allen. Lsdsem 2017 shared task: The story cloze test. In Proceedings of
    the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level
    Semantics, pages 46–51, 2017.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Nasrin Mostafazadeh, Michael Roth, Annie Louis, Nathanael Chambers 和 James
    Allen. lsdsem 2017共享任务: 故事闭合测试。发表于第2届词汇、句法和话语层面语义模型链接研讨会论文集, 页码 46–51, 2017。'
- en: '[46] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen
    Blankevoort. Up or down? adaptive rounding for post-training quantization. In
    International Conference on Machine Learning, pages 7197–7206\. PMLR, 2020.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos 和 Tijmen
    Blankevoort. 向上还是向下？后训练量化的自适应舍入。发表于国际机器学习会议, 页码 7197–7206\. PMLR, 2020。'
- en: '[47] Sameer Narayan, Andre Martins, Alessandro Sordoni, Philip Bachman, Aaron
    Courville, and Yoshua Bengio. Don’t give me the details, just the summary!: topic-aware
    convolutional neural networks for extreme summarization. In Proceedings of the
    2018 Conference on Empirical Methods in Natural Language Processing, pages 3706–3716,
    2018.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Sameer Narayan, Andre Martins, Alessandro Sordoni, Philip Bachman, Aaron
    Courville 和 Yoshua Bengio. 不要给我细节，只要总结！: 面向主题的卷积神经网络用于极端总结。发表于2018年自然语言处理实证方法会议论文集,
    页码 3706–3716, 2018。'
- en: '[48] NVIDIA. cublas. ["https://developer.nvidia.com/cublas"](%22https://developer.nvidia.com/cublas%22),
    2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] NVIDIA. cublas. [“https://developer.nvidia.com/cublas”](https://developer.nvidia.com/cublas),
    2023。'
- en: '[49] NVIDIA. Tensorrt-llm. ["https://github.com/NVIDIA/TensorRT-LLM/"](%22https://github.com/NVIDIA/TensorRT-LLM/%22),
    2023.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] NVIDIA. Tensorrt-llm. [“https://github.com/NVIDIA/TensorRT-LLM/”](https://github.com/NVIDIA/TensorRT-LLM/),
    2023。'
- en: '[50] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee,
    and Dongsoo Lee. nuqmm: Quantized matmul for efficient inference of large-scale
    generative language models. arXiv preprint arXiv:2206.09557, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee
    和 Dongsoo Lee. nuqmm: 高效推断大规模生成语言模型的量化矩阵乘法。arXiv 预印本 arXiv:2206.09557, 2022。'
- en: '[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. The Journal of Machine Learning
    Research, 21(1):5485–5551, 2020.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li 和 Peter J Liu. 探索统一文本到文本变换器的迁移学习极限。机器学习研究期刊,
    21(1):5485–5551, 2020。'
- en: '[52] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
    arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé 等. Bloom: 一个176b参数的开放访问多语言模型。arXiv 预印本 arXiv:2211.05100, 2022。'
- en: '[53] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. Pb-llm: Partially
    binarized large language models. arXiv preprint arXiv:2310.00034, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Yuzhang Shang, Zhihang Yuan, Qiang Wu 和 Zhen Dong. PB-LLM: 部分二值化的大型语言模型。arXiv
    预印本 arXiv:2310.00034, 2023。'
- en: '[54] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
    Michael W Mahoney, and Kurt Keutzer. Q-BERT: Hessian based ultra low precision
    quantization of bert. In AAAI, pages 8815–8821, 2020.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
    Michael W Mahoney 和 Kurt Keutzer. Q-BERT: 基于Hessian的BERT超低精度量化。发表于 AAAI, 页码 8815–8821,
    2020。'
- en: '[55] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping
    Luo, and Ngai Wong. Compression of generative pre-trained language models via
    quantization. arXiv preprint arXiv:2203.10705, 2022.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping
    Luo 和 Ngai Wong. 通过量化压缩生成预训练语言模型。arXiv 预印本 arXiv:2203.10705, 2022。'
- en: '[56] Sandeep Tata and Jignesh M Patel. Piqa: An algebra for querying protein
    data sets. In 15th International Conference on Scientific and Statistical Database
    Management, 2003., pages 141–150\. IEEE, 2003.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Sandeep Tata 和 Jignesh M Patel. Piqa: 查询蛋白质数据集的代数。发表于第15届国际科学与统计数据库管理会议,
    页码 141–150\. IEEE, 2003。'
- en: '[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等. Llama: 开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971, 2023。'
- en: '[58] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin,
    Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga,
    et al. Fp8 versus int8 for efficient deep learning inference. arXiv preprint arXiv:2303.17951,
    2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 马特·范·巴伦、安德烈·库兹明、苏帕尔纳·S·奈尔、任宇维、埃里克·马胡林、奇拉格·帕特尔、桑达·苏布拉马尼安、李相赫、马克斯·纳戈尔、约瑟夫·索里亚加等。Fp8与int8在高效深度学习推理中的对比。arXiv
    预印本 arXiv:2303.17951，2023年。'
- en: '[59] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language
    models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 魏秀英、张云晨、李宇航、张相国、龚瑞浩、郭锦阳和刘向龙。Outlier suppression+：通过等效和最优的平移与缩放实现大语言模型的准确量化。arXiv
    预印本 arXiv:2304.09145，2023年。'
- en: '[60] Miles Williams and Nikolaos Aletras. How does calibration data affect
    the post-training pruning and quantization of large language models? arXiv preprint
    arXiv:2311.09755, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 迈尔斯·威廉姆斯和尼古拉斯·阿雷特拉斯。校准数据如何影响大型语言模型的后训练剪枝和量化？arXiv 预印本 arXiv:2311.09755，2023年。'
- en: '[61] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong
    He. Understanding int4 quantization for transformer models: Latency speedup, composability,
    and failure cases. arXiv preprint arXiv:2301.12017, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] 吴晓霞、李成、雷扎·雅兹达尼·阿敏阿巴迪、曹伟·姚和何玉雄。理解变换器模型的int4量化：延迟加速、可组合性和失败案例。arXiv 预印本
    arXiv:2301.12017，2023年。'
- en: '[62] Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap forward in
    llms post-training w4a8 quantization using floating-point formats. arXiv preprint
    arXiv:2307.09782, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 吴晓霞、曹伟·姚和何玉雄。Zeroquant-fp：在llms后训练w4a8量化中使用浮点格式的飞跃。arXiv 预印本 arXiv:2307.09782，2023年。'
- en: '[63] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme
    compression for pre-trained transformers made simple and efficient. arXiv preprint
    arXiv:2206.01859, 2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 吴晓霞、曹伟·姚、张敏佳、李从龙和何玉雄。简化高效的预训练变换器的极端压缩。arXiv 预印本 arXiv:2206.01859，2022年。'
- en: '[64] Haojun Xia, Zhen Zheng, Yuchao Li, Donglin Zhuang, Zhongzhu Zhou, Xiafei
    Qiu, Yong Li, Wei Lin, and Shuaiwen Leon Song. Flash-llm: Enabling cost-effective
    and highly-efficient large generative model inference with unstructured sparsity.
    arXiv preprint arXiv:2309.10285, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 逍俊·夏、郑振、李宇超、庄东林、周钟柱、邱霞飞、李勇、林伟和帅文·莱昂·宋。Flash-llm：通过无结构稀疏实现具有成本效益和高效的大型生成模型推理。arXiv
    预印本 arXiv:2309.10285，2023年。'
- en: '[65] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared llama:
    Accelerating language model pre-training via structured pruning. arXiv preprint
    arXiv:2310.06694, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 孟洲·夏、天宇·高、曾志远和陈丹奇。Sheared llama：通过结构化剪枝加速语言模型的预训练。arXiv 预印本 arXiv:2310.06694，2023年。'
- en: '[66] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. arXiv preprint arXiv:2211.10438, 2022.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 肖光轩、吉·林、米凯尔·塞兹内克、朱利安·德穆斯和宋汉。Smoothquant：准确高效的大型语言模型后训练量化。arXiv 预印本 arXiv:2211.10438，2022年。'
- en: '[67] Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng,
    and Yuxiong He. Zeroquant-hero: Hardware-enhanced robust optimized post-training
    quantization framework for w8a8 transformers. arXiv preprint arXiv:2310.17723,
    2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] 曹伟·姚、雷扎·雅兹达尼·阿敏阿巴迪、斯蒂芬·尤恩、吴晓霞、埃尔顿·郑和何玉雄。Zeroquant-hero：针对w8a8变换器的硬件增强鲁棒优化后训练量化框架。arXiv
    预印本 arXiv:2310.17723，2023年。'
- en: '[68] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] 曹伟·姚、雷扎·雅兹达尼·阿敏阿巴迪、张敏佳、吴晓霞、李从龙和何玉雄。Zeroquant：大规模变换器的高效且实惠的后训练量化。arXiv
    预印本 arXiv:2206.01861，2022年。'
- en: '[69] Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong He. Zeroquant-v2:
    Exploring post-training quantization in llms from comprehensive study to low rank
    compensation. arXiv preprint arXiv:2303.08302, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] 曹伟·姚、吴晓霞、李成、斯蒂芬·尤恩和何玉雄。Zeroquant-v2：从综合研究到低秩补偿探索语言模型的后训练量化。arXiv 预印本 arXiv:2303.08302，2023年。'
- en: '[70] Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang, Yuzhang Shang,
    Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based post-training
    quantization for large language models. arXiv preprint arXiv:2304.01089, 2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] 袁智航、牛林、刘佳伟、刘文宇、王兴刚、尚宇章、孙广宇、吴强、吴佳翔和吴冰哲。Rptq：基于重排序的大型语言模型后训练量化。arXiv 预印本
    arXiv:2304.01089，2023年。'
- en: '[71] Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat. Q8BERT:
    Quantized 8bit bert. arXiv preprint arXiv:1910.06188, 2019.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Ofir Zafrir、Guy Boudoukh、Peter Izsak、Moshe Wasserblat。《Q8BERT：量化的 8 位
    BERT》。arXiv 预印本 arXiv:1910.06188，2019年。'
- en: '[72] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] 苏珊·张、斯蒂芬·罗勒、纳曼·戈亚尔、米克尔·阿尔特克斯、莫亚·陈、邵辉·陈、克里斯托弗·德万、莫娜·迪亚布、李先、维多利亚·林等。《OPT：开放预训练转换器语言模型》。arXiv
    预印本 arXiv:2205.01068，2022年。'
- en: '[73] Tianyi Zhang, Zhiqiu Lin, Guandao Yang, and Christopher De Sa. Qpytorch:
    A low-precision arithmetic simulation framework, 2019.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] 张天意、林志秋、杨关道、克里斯托弗·德·萨。《Qpytorch：低精度算术仿真框架》，2019年。'
- en: '[74] Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang,
    Mao Yang, Shanghang Zhang, and Ningyi Xu. Integer or floating point? new outlooks
    for low-bit quantization on large language models. arXiv preprint arXiv:2305.12356,
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] 张怡佳、赵凌然、曹世杰、王文强、曹婷、杨凡、杨茂、张上航、许宁义。《整数还是浮点？大规模语言模型低比特量化的新展望》。arXiv 预印本 arXiv:2305.12356，2023年。'
- en: '[75] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,
    Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E.
    Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena,
    2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] 郑连敏、姜伟林、盛颖、庄思源、吴张浩、庄永浩、林紫、李卓涵、李大成、Eric P Xing、张浩、约瑟夫·E·冈萨雷斯、Ion Stoica。《通过
    mt-bench 和聊天机器人竞技场评判 LLM 作为裁判》，2023年。'
- en: '[76] Qinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan
    Wang, Lei Shen, Andi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex:
    A pre-trained model for code generation with multilingual evaluations on humaneval-x.
    In KDD, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] 郑钦凯、夏晓、邹旭、董宇霄、王珊、薛玉飞、王紫涵、沈磊、王安迪、李杨、苏腾、杨志林、唐杰。《Codegeex：用于代码生成的预训练模型，在
    humaneval-x 上进行多语言评估》。在 KDD 会议上，2023年。'
- en: Appendix A Background of Quantization
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 量化背景
- en: Throughout this work, we focus on post-training quantization (PTQ), i.e., no
    or minimal training effort is applied after quantization, for which large accuracy
    degradation usually exhibits for coarse-grained quantization (per matrix/tensor)
    due to their large quantization error. Particularly, we use the per-row quantization
    (one row of the weight matrix) from [[68](#bib.bib68)] as our coarsest-grained
    quantization method, and we use block-k quantization (for every k elements, they
    have their own scaling factor and/or zero point) as our finer-grained quantization
    scheme.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们专注于训练后量化（PTQ），即量化后没有或只有最小的训练工作，通常由于粗粒度量化（每个矩阵/张量）会出现较大的准确性下降，这是因为它们的量化误差较大。特别地，我们使用了来自
    [[68](#bib.bib68)] 的每行量化（权重矩阵的一行）作为我们最粗粒度的量化方法，并使用块-k 量化（每 k 个元素都有自己的缩放因子和/或零点）作为我们更细粒度的量化方案。
- en: 'Table A.1: GPT4-evaluation for the same model with different precision [[75](#bib.bib75)].
    There is no clear relation between different bits and rating performance.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.1：不同精度下的 GPT4 评估 [[75](#bib.bib75)]。不同位数与评分性能之间没有明显关系。
- en: Model (Precision) Writing Roleplay Coding STEM Humanities Reasoning Math Extraction
    vicuna-7b-v1.5 (Baseline) 7.55 7.95 3.21 8.36 9.68 4.6 3.6 6.4 vicuna-7b-v1.5
    (INT4) 7.37 7.60 3.21 8.61 9.34 5.7 2.6 6.2 vicuna-7b-v1.5 (FP6) 7.55 7.88 3.67
    8.55 9.21 4.8 2.2 6.1 vicuna-13b-v1.5 (Baseline) 7.90 7.75 2.73 7.32 9.24 5.2
    2.2 6.0 vicuna-13b-v1.5 (INT4) 8.42 7.56 2.91 7.93 9.58 4.7 2.7 6.4 vicuna-13b-v1.5
    (FP6) 8.06 7.60 2.94 7.76 9.31 4.8 2.8 5.9
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Model (Precision) Writing Roleplay Coding STEM Humanities Reasoning Math Extraction
    vicuna-7b-v1.5 (Baseline) 7.55 7.95 3.21 8.36 9.68 4.6 3.6 6.4 vicuna-7b-v1.5
    (INT4) 7.37 7.60 3.21 8.61 9.34 5.7 2.6 6.2 vicuna-7b-v1.5 (FP6) 7.55 7.88 3.67
    8.55 9.21 4.8 2.2 6.1 vicuna-13b-v1.5 (Baseline) 7.90 7.75 2.73 7.32 9.24 5.2
    2.2 6.0 vicuna-13b-v1.5 (INT4) 8.42 7.56 2.91 7.93 9.58 4.7 2.7 6.4 vicuna-13b-v1.5
    (FP6) 8.06 7.60 2.94 7.76 9.31 4.8 2.8 5.9
- en: 'Table A.2: Zero-Shot Evaluation (Accuracy). Compare between GPTQ[C4] and RTN
    quantization algorithms for INT4 weight (W4A16) on LLaMA-1B (Top) and LLaMA-13B
    (Bottom). We apply fine-grain quantization (FGQ) in which the block-size is 256
    elements per scale except for LLaMA-1B’s (which is 128). arcC (arcE) stands for
    arc_challenges (arc_easy).'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.2：零-shot 评估（准确率）。对比 GPTQ[C4] 和 RTN 量化算法在 LLaMA-1B（顶部）和 LLaMA-13B（底部）的 INT4
    权重（W4A16）。我们应用了细粒度量化（FGQ），其中块大小为每个尺度 256 个元素，除了 LLaMA-1B 的（为 128）。arcC（arcE）代表
    arc_challenges（arc_easy）。
- en: Models Precision FGQ arcC arcE boolq cb copa piqa rte wic wsc storycloze MEAN
    LLaMA-1B FP16 N/A 26.71 53.11 61.13 39.29 76.00 73.83 50.18 50.00 36.54 69.64
    53.64 \cdashline2-14 (4096-seq) INT4-GPTQ ✓ 26.37 50.59 61.59 46.43 79.00 73.34
    48.01 50.00 36.54 68.24 54.01 INT4-RTN ✓ 26.11 51.09 58.07 50.00 74.00 72.91 48.38
    50.00 36.54 68.36 53.55 \cdashline2-14 FP5-GPTQ ✗ 25.51 51.85 61.22 32.14 80.00
    73.01 48.38 50.00 36.54 68.94 52.76 FP5-RTN ✗ 26.62 50.67 61.41 39.29 79.00 72.85
    50.18 50.00 36.54 69.19 53.58 FP6-GPTQ ✗ 26.37 52.53 61.19 42.86 75.00 73.5 51.26
    50.00 36.54 69.76 53.90 FP6-RTN ✗ 26.37 52.95 60.95 37.50 78.00 73.39 54.51 50.00
    36.54 69.64 53.99 LLaMA-13B FP16 N/A 43.86 74.58 68.53 50.00 90.00 79.00 65.34
    50.00 35.58 78.23 63.51 \cdashline2-14 (2048-seq) INT4-GPTQ ✓ 43.00 73.44 67.83
    41.07 93.00 78.78 62.45 50.16 36.54 78.17 62.44 INT4-RTN ✓ 44.03 74.45 67.37 44.64
    91.00 78.84 63.18 49.84 36.54 78.42 62.83 \cdashline2-14 FP5-GPTQ ✗ 42.92 73.70
    65.81 44.64 90.00 78.67 64.62 50.00 36.54 78.23 62.51 FP5-RTN ✗ 41.72 74.03 68.47
    39.29 90.00 78.56 62.45 50.31 36.54 78.61 62.00 FP6-GPTQ ✗ 43.69 73.99 67.28 48.21
    90.00 78.84 64.98 50.31 36.54 78.42 63.23 FP6-RTN ✗ 43.77 74.20 68.38 46.43 91.00
    78.84 65.34 49.84 36.54 78.23 63.26 LLaMA-65B FP16 N/A 47.01 75.08 82.32 64.29
    91.00 81.61 71.48 58.31 60.58 79.57 71.13 \cdashline2-14 (2048-seq) INT4-GPTQ
    ✓ 46.84 75.08 80.76 58.93 94.00 81.18 72.92 56.27 60.58 79.31 70.59 INT4-RTN ✓
    47.10 75.25 81.47 62.50 95.00 81.23 69.68 57.21 62.50 79.63 71.16 \cdashline2-14
    FP5-GPTQ ✗ 46.50 75.51 82.35 69.64 93.00 81.28 71.84 57.05 57.69 79.76 71.46 FP5-RTN
    ✗ 46.50 75.59 82.87 60.71 94.00 81.39 73.65 57.21 60.58 80.08 71.26 FP6-GPTQ ✗
    46.84 74.96 82.51 64.29 91.00 81.23 70.04 59.72 61.54 79.63 71.18 FP6-RTN ✗ 47.10
    74.66 82.69 64.29 92.00 81.99 70.76 58.15 57.69 79.31 70.86
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 精度 FGQ arcC arcE boolq cb copa piqa rte wic wsc storycloze 平均 LLaMA-1B FP16
    不适用 26.71 53.11 61.13 39.29 76.00 73.83 50.18 50.00 36.54 69.64 53.64 \cdashline2-14
    (4096-seq) INT4-GPTQ ✓ 26.37 50.59 61.59 46.43 79.00 73.34 48.01 50.00 36.54 68.24
    54.01 INT4-RTN ✓ 26.11 51.09 58.07 50.00 74.00 72.91 48.38 50.00 36.54 68.36 53.55
    \cdashline2-14 FP5-GPTQ ✗ 25.51 51.85 61.22 32.14 80.00 73.01 48.38 50.00 36.54
    68.94 52.76 FP5-RTN ✗ 26.62 50.67 61.41 39.29 79.00 72.85 50.18 50.00 36.54 69.19
    53.58 FP6-GPTQ ✗ 26.37 52.53 61.19 42.86 75.00 73.5 51.26 50.00 36.54 69.76 53.90
    FP6-RTN ✗ 26.37 52.95 60.95 37.50 78.00 73.39 54.51 50.00 36.54 69.64 53.99 LLaMA-13B
    FP16 不适用 43.86 74.58 68.53 50.00 90.00 79.00 65.34 50.00 35.58 78.23 63.51 \cdashline2-14
    (2048-seq) INT4-GPTQ ✓ 43.00 73.44 67.83 41.07 93.00 78.78 62.45 50.16 36.54 78.17
    62.44 INT4-RTN ✓ 44.03 74.45 67.37 44.64 91.00 78.84 63.18 49.84 36.54 78.42 62.83
    \cdashline2-14 FP5-GPTQ ✗ 42.92 73.70 65.81 44.64 90.00 78.67 64.62 50.00 36.54
    78.23 62.51 FP5-RTN ✗ 41.72 74.03 68.47 39.29 90.00 78.56 62.45 50.31 36.54 78.61
    62.00 FP6-GPTQ ✗ 43.69 73.99 67.28 48.21 90.00 78.84 64.98 50.31 36.54 78.42 63.23
    FP6-RTN ✗ 43.77 74.20 68.38 46.43 91.00 78.84 65.34 49.84 36.54 78.23 63.26 LLaMA-65B
    FP16 不适用 47.01 75.08 82.32 64.29 91.00 81.61 71.48 58.31 60.58 79.57 71.13 \cdashline2-14
    (2048-seq) INT4-GPTQ ✓ 46.84 75.08 80.76 58.93 94.00 81.18 72.92 56.27 60.58 79.31
    70.59 INT4-RTN ✓ 47.10 75.25 81.47 62.50 95.00 81.23 69.68 57.21 62.50 79.63 71.16
    \cdashline2-14 FP5-GPTQ ✗ 46.50 75.51 82.35 69.64 93.00 81.28 71.84 57.05 57.69
    79.76 71.46 FP5-RTN ✗ 46.50 75.59 82.87 60.71 94.00 81.39 73.65 57.21 60.58 80.08
    71.26 FP6-GPTQ ✗ 46.84 74.96 82.51 64.29 91.00 81.23 70.04 59.72 61.54 79.63 71.18
    FP6-RTN ✗ 47.10 74.66 82.69 64.29 92.00 81.99 70.76 58.15 57.69 79.31 70.86
- en: Appendix B Detailed Performance of GPU Kernels
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B GPU 内核的详细性能
- en: The shapes of weight matrices are set according to the model specification of
    LLaMA-1B, LLaMA-13B, and LLaMA-65B, respectively. We mainly evaluate the kernel
    performance when the inference batch size is 8. As for the Fine-grained INT4 kernel,
    we set its quantization group size to 128 for the best of its performance. All
    the kernel latency shown here is measured in milliseconds.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 权重矩阵的形状根据 LLaMA-1B、LLaMA-13B 和 LLaMA-65B 的模型规格设置。我们主要评估推理批次大小为 8 时的内核性能。至于细粒度
    INT4 内核，我们将其量化组大小设置为 128 以获得最佳性能。这里显示的所有内核延迟以毫秒为单位测量。
- en: 'Table B.1: The corresponding number for Figure [2](#S5.F2 "Figure 2 ‣ 5.3 System
    evaluation ‣ 5 System Support Discussion ‣ ZeroQuant(4+2): Redefining LLMs Quantization
    with a New FP6-Centric Strategy for Diverse Generative Tasks").'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 表 B.1：图 [2](#S5.F2 "图 2 ‣ 5.3 系统评估 ‣ 5 系统支持讨论 ‣ ZeroQuant(4+2)：用新的 FP6 中心策略重新定义
    LLMs 的量化") 的对应编号。
- en: Layer Name Weight Shape Input Shape cuBLAS Fine-grained INT4 FP6 (w/o Bias-Shift)
    FP6 (w/ Bias-Shift) FFN1-1b 5504*2048 2048*8 0.016 0.019 0.021 0.013 FFN2-1b 2048*5504
    5504*8 0.02 0.043 0.022 0.016 FFN1-13b 13824*5120 5120*8 0.118 0.044 0.063 0.052
    FFN2-13b 5120*13824 13824*8 0.109 0.106 0.068 0.053 FFN1-65b 22016*8192 8192*8
    0.263 0.098 0.145 0.111 FFN2-65b 8192*22016 22016*8 0.266 0.167 0.157 0.114
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 层名称 权重 形状 输入形状 cuBLAS 细粒度 INT4 FP6（无偏移） FP6（有偏移） FFN1-1b 5504*2048 2048*8 0.016
    0.019 0.021 0.013 FFN2-1b 2048*5504 5504*8 0.02 0.043 0.022 0.016 FFN1-13b 13824*5120
    5120*8 0.118 0.044 0.063 0.052 FFN2-13b 5120*13824 13824*8 0.109 0.106 0.068 0.053
    FFN1-65b 22016*8192 8192*8 0.263 0.098 0.145 0.111 FFN2-65b 8192*22016 22016*8
    0.266 0.167 0.157 0.114
