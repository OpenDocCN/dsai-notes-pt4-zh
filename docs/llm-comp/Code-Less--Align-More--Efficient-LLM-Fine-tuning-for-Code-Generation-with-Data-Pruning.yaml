- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:54:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 18:54:04'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更少代码，更高对齐：数据剪枝下高效LLM微调用于代码生成
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.05040](https://ar5iv.labs.arxiv.org/html/2407.05040)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.05040](https://ar5iv.labs.arxiv.org/html/2407.05040)
- en: Yun-Da Tsai
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Yun-Da Tsai
- en: NVIDIA
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: yundat@nvidia.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: yundat@nvidia.com
- en: '&Mingjie Liu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '&Mingjie Liu'
- en: NVIDIA
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: mingjiel@nvidia.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: mingjiel@nvidia.com
- en: '&Haoxing Ren'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Haoxing Ren'
- en: NVIDIA
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: NVIDIA
- en: haoxingr@nvidia.com
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: haoxingr@nvidia.com
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent work targeting large language models (LLMs) for code generation demonstrated
    that increasing the amount of training data through synthetic code generation
    often leads to exceptional performance. In this paper we explore data pruning
    methods aimed at enhancing the efficiency of model training specifically for code
    LLMs. We present techniques that integrate various clustering and pruning metrics
    to selectively reduce training data without compromising the accuracy and functionality
    of the generated code. We observe significant redundancies in synthetic training
    data generation, where our experiments demonstrate that benchmark performance
    can be largely preserved by training on only 10% of the data. Moreover, we observe
    consistent improvements in benchmark results through moderate pruning of the training
    data. Our experiments show that these pruning strategies not only reduce the computational
    resources needed but also enhance the overall quality code generation.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 针对代码生成的大型语言模型（LLMs）的近期研究表明，通过合成代码生成增加训练数据的量通常会带来卓越的性能。在本文中，我们探讨了旨在提高代码LLM训练效率的数据剪枝方法。我们提出了将各种聚类和剪枝指标整合以选择性地减少训练数据而不影响生成代码的准确性和功能的技术。我们观察到在合成训练数据生成中存在显著的冗余，我们的实验表明，仅用10%的数据进行训练即可在很大程度上保持基准性能。此外，我们通过对训练数据进行适度剪枝观察到了基准结果的持续改善。我们的实验表明，这些剪枝策略不仅减少了所需的计算资源，还提升了整体代码生成的质量。
- en: \SetTblrInner
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: \SetTblrInner
- en: '[booktabs]abovesep=0pt, belowsep=0pt, rowsep=0.5pt \SetTblrInner[booktabs]cells
    = cmd= \NewTableCommand\seprule \NewTableCommand\uniquerule'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[booktabs]abovesep=0pt, belowsep=0pt, rowsep=0.5pt \SetTblrInner[booktabs]cells
    = cmd= \NewTableCommand\seprule \NewTableCommand\uniquerule'
- en: 'Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 更少代码，更高对齐：数据剪枝下高效LLM微调用于代码生成
- en: Yun-Da Tsai NVIDIA yundat@nvidia.com                        Mingjie Liu NVIDIA
    mingjiel@nvidia.com                        Haoxing Ren NVIDIA haoxingr@nvidia.com
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Yun-Da Tsai NVIDIA yundat@nvidia.com                        Mingjie Liu NVIDIA
    mingjiel@nvidia.com                        Haoxing Ren NVIDIA haoxingr@nvidia.com
- en: 1 Introduction
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The performance of large language models (LLMs) is heavily dependent on the
    size and quality of their training datasets, as highlighted by recent studies
    on scaling laws Achiam et al. ([2023](#bib.bib1)); Zhang et al. ([2024](#bib.bib2)).
    State-of-the-art code LLMs, such as CodeAlpaca Chaudhary ([2023](#bib.bib3)),
    WizardCoder Luo et al. ([2024](#bib.bib4)), and MagicCoder Wei et al. ([2023](#bib.bib5)),
    have achieved remarkable performance by significantly expanding their supervised
    fine-tuning datasets through synthetic code generation. Various synthetic code
    generation approaches have been developed, including the Self-Instruct technique Wang
    et al. ([2022](#bib.bib6)), Evol-Instruct Xu et al. ([2023a](#bib.bib7)), and
    OSS-Instruct Wei et al. ([2023](#bib.bib5)). However, such scaling approaches
    not only increase the training cost but also demands substantial computational
    resources, making it expensive and less accessible.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的性能在很大程度上依赖于其训练数据集的规模和质量，近期关于扩展定律的研究对此进行了强调（Achiam等人（[2023](#bib.bib1)）；Zhang等人（[2024](#bib.bib2)））。最先进的代码LLMs，例如CodeAlpaca（Chaudhary（[2023](#bib.bib3)））、WizardCoder（Luo等人（[2024](#bib.bib4)））和MagicCoder（Wei等人（[2023](#bib.bib5)）），通过合成代码生成显著扩大了其监督微调数据集，从而取得了显著的性能。各种合成代码生成方法已经被开发，包括Self-Instruct技术（Wang等人（[2022](#bib.bib6)））、Evol-Instruct（Xu等人（[2023a](#bib.bib7)））和OSS-Instruct（Wei等人（[2023](#bib.bib5)））。然而，这些扩展方法不仅增加了训练成本，还需要大量的计算资源，使得成本高昂且可及性较低。
- en: Achieving optimal performance in fine-tuned models for downstream tasks often
    relies on large, high-quality datasets. Recently, there has been a growing interest
    in more efficient fine-tuning methods for large language models (LLMs). One recent
    work introduces the Superficial Alignment Hypothesis Zhou et al. ([2023](#bib.bib9)),
    which suggests that most knowledge in LLMs is acquired during pretraining, and
    only minimal instruction tuning data is required to align models with human preferences.
    Promising strategies to reduce computational demands include parameter-efficient
    fine-tuning (PEFT) methods, which reduce the number of parameters needed for training Fu
    et al. ([2023](#bib.bib10)); Hu et al. ([2021](#bib.bib11)). Another research
    direction uses active learning to iteratively select data samples during training,
    thereby enhancing model learning Su et al. ([2022](#bib.bib12)); Diao et al. ([2023](#bib.bib13)).
    These methods primarily aim to improve model accuracy through iterative processes,
    requiring multiple rounds of training and data selection.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下游任务中实现最佳性能通常依赖于大型、高质量的数据集。最近，对于大型语言模型（LLMs）的更高效微调方法的兴趣日益增加。一项近期工作介绍了**表面对齐假设** Zhou
    et al. ([2023](#bib.bib9))，该假设提出LLMs中的大部分知识是在预训练期间获得的，只需要最少的指令调整数据即可将模型与人类偏好对齐。减少计算需求的有前景的策略包括参数高效微调（PEFT）方法，这些方法减少了训练所需的参数数量 Fu
    et al. ([2023](#bib.bib10)); Hu et al. ([2021](#bib.bib11))。另一研究方向使用主动学习在训练过程中迭代选择数据样本，从而增强模型学习 Su
    et al. ([2022](#bib.bib12)); Diao et al. ([2023](#bib.bib13))。这些方法主要旨在通过迭代过程提高模型准确性，需要多轮训练和数据选择。
- en: Data selection and pruning methods have also been well-explored in literature,
    with evidence suggesting that careful pruning can sometimes even surpass the performance
    of using the full dataset Penedo et al. ([2024](#bib.bib15)); Wang et al. ([2023](#bib.bib16)).
    Moreover, many of these methods are computationally intensive such as supervised
    metrics that involves multiple times of model training to keep track of loss and
    gradients Xia et al. ([2024](#bib.bib18)); Pruthi et al. ([2020](#bib.bib19))
    or heavy sampling method with Monte Carlo Schoch et al. ([2023](#bib.bib20)),
    limiting their scalability. Practical pruning methods that aims for large-scale
    data have been investigated in the contexts of LLM pretraining Das and Khetan
    ([2023](#bib.bib21)); Penedo et al. ([2024](#bib.bib15)) and fine-tuning Chen
    et al. ([2024](#bib.bib22)); Schoch et al. ([2023](#bib.bib20)) datasets, image
    datasets Moser et al. ([2024](#bib.bib23)); Meding et al. ([2021](#bib.bib24)),
    and vision-text training datasets Wang et al. ([2023](#bib.bib16)), and demonstrate
    success by applying clustering and by choosing proper indicator functions.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 数据选择和剪枝方法在文献中也得到了很好的探索，有证据表明，精心剪枝有时甚至可以超过使用完整数据集的性能 Penedo et al. ([2024](#bib.bib15));
    Wang et al. ([2023](#bib.bib16))。此外，许多这些方法计算密集，例如涉及多次模型训练以跟踪损失和梯度的监督指标 Xia et al.
    ([2024](#bib.bib18)); Pruthi et al. ([2020](#bib.bib19))，或使用蒙特卡罗的重采样方法 Schoch
    et al. ([2023](#bib.bib20))，限制了它们的扩展性。在LLM预训练 Das and Khetan ([2023](#bib.bib21));
    Penedo et al. ([2024](#bib.bib15)) 和微调 Chen et al. ([2024](#bib.bib22)); Schoch
    et al. ([2023](#bib.bib20)) 数据集、大规模图像数据集 Moser et al. ([2024](#bib.bib23)); Meding
    et al. ([2021](#bib.bib24))，以及视觉-文本训练数据集 Wang et al. ([2023](#bib.bib16)) 的实际剪枝方法已经被研究，并通过应用聚类和选择适当的指标函数展示了成功。
- en: Despite these advances, there remains a gap in efficient pruning strategies
    specifically tailored for coding datasets. Most large-scale code datasets are
    synthetically generated, resulting in many data samples with similar lexical appearances
    due to consistent formatting and style. Large-scale synthetic datasets commonly
    used for training code LLMs often suffer from significant redundancy and noise Wang
    et al. ([2023](#bib.bib16)). This redundancy arises from the impracticality of
    verifying the functional correctness of each program, leading to a substantial
    portion of instruction-code pairs being noisy. Therefore, enhancing data efficiency
    through careful selection and pruning of data samples is crucial for improving
    model performance without relying on excessively large datasets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了这些进展，但针对编码数据集的高效剪枝策略仍然存在差距。大多数大规模代码数据集是合成生成的，由于格式和风格一致，导致许多数据样本具有相似的词汇外观。用于训练代码LLMs的大规模合成数据集通常存在显著的冗余和噪声 Wang
    et al. ([2023](#bib.bib16))。这种冗余源于验证每个程序功能正确性的实际不可行性，导致大量指令-代码对存在噪声。因此，通过精心选择和剪枝数据样本来提高数据效率对于在不依赖过度大型数据集的情况下改善模型性能至关重要。
- en: 'In this work, we present a scalable and effective data pruning method to enhance
    code generation in large language models. Our approach clusters data samples based
    on problem instructions and their code solutions, applying dimensionality reduction
    to reduce computational load. We then select a representative subset from each
    cluster using various pruning metrics. Experiments on large-scale datasets and
    evaluations on downstream coding tasks show that our method maintains or even
    improves model performance while significantly reducing training data. Our contributions
    and key findings are summarized as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种可扩展且有效的数据修剪方法，以增强大型语言模型的代码生成。我们的方法基于问题指令及其代码解决方案对数据样本进行聚类，应用降维以减少计算负荷。然后，我们使用各种修剪指标从每个聚类中选择一个代表性子集。在大规模数据集上的实验和下游编码任务的评估表明，我们的方法能够维持甚至提高模型性能，同时显著减少训练数据。我们的贡献和关键发现总结如下：
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We are the first to study data pruning for large-scale synthetic code fine-tuning.
    We create an efficient and scalable pruning strategy based on unsupervised learning
    methods.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首次研究了大规模合成代码微调的数据修剪。我们基于无监督学习方法创建了一种高效且可扩展的修剪策略。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We find large redundancies in synthetic generated code datasets, as training
    on just 10% retains most benchmark performance, with slight degradation of 3.9%
    on HumanEval and 1.5% on MBPP compared with using all data.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们发现合成生成的代码数据集中存在大量冗余，训练仅使用10%数据就能保留大部分基准性能，与使用所有数据相比，HumanEval略微下降了3.9%，MBPP下降了1.5%。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We observe consistent improvement by moderately pruning the dataset, leading
    to improvements of up to 2.7% on HumanEval and 3.5% on MBPP compared with using
    all data.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们观察到，通过适度修剪数据集可以实现持续的改进，与使用所有数据相比，在HumanEval上提高了最多2.7%，在MBPP上提高了3.5%。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We perform detailed ablation studies, where results demonstrate the clustering
    algorithm to be critical, while pruning metrics to be less important.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们进行详细的消融研究，结果表明聚类算法至关重要，而修剪指标则不那么重要。
- en: 2 Related Work
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'In this section, we review the advancements of large language models (LLMs)
    for code generation in Section [2.1](#S2.SS1 "2.1 Large Language Models for Code
    Generation ‣ 2 Related Work ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning") and review prior work on instructional
    finetuning in Section [2.2](#S2.SS2 "2.2 Instructional Fine-tuning ‣ 2 Related
    Work ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with
    Data Pruning"). Finally, we discuss earlier research on data selection and pruning
    methods in Section [2.3](#S2.SS3 "2.3 Data Pruning for Efficient Training ‣ 2
    Related Work ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning").'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了第[2.1](#S2.SS1 "2.1 大型语言模型用于代码生成 ‣ 2 相关工作 ‣ 更少代码，更好对齐：高效LLM微调与数据修剪")节中大型语言模型（LLMs）在代码生成方面的进展，并在第[2.2](#S2.SS2
    "2.2 指令微调 ‣ 2 相关工作 ‣ 更少代码，更好对齐：高效LLM微调与数据修剪")节中回顾了指令微调的先前工作。最后，我们在第[2.3](#S2.SS3
    "2.3 高效训练的数据修剪 ‣ 2 相关工作 ‣ 更少代码，更好对齐：高效LLM微调与数据修剪")节中讨论了早期的数据选择和修剪方法研究。
- en: 2.1 Large Language Models for Code Generation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型用于代码生成
- en: Great advancements have been achieved in improving Large Language Models (LLMs)
    for code generation. Codealpaca Chaudhary ([2023](#bib.bib3)) extends the capabilities
    of the LLaMA model Touvron et al. ([2023a](#bib.bib26)) by incorporating 20,000
    instruction-following data points generated through the Self-Instruct technique Wang
    et al. ([2022](#bib.bib6)), which aligns language models with self-generated instructions.
    CodeLlama Roziere et al. ([2023](#bib.bib27)) further enhances this methodology
    by fine-tuning from LLaMA2 Touvron et al. ([2023b](#bib.bib28)), utilizing 14,000
    instruction-following data points also generated via the Self-Instruct technique.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在提高大型语言模型（LLMs）用于代码生成方面取得了重大进展。**Codealpaca** Chaudhary ([2023](#bib.bib3))通过引入通过Self-Instruct技术生成的20,000个指令跟随数据点，扩展了LLaMA模型**Touvron
    et al.** ([2023a](#bib.bib26))的能力，该技术使语言模型与自生成指令对齐。**CodeLlama** Roziere et al.
    ([2023](#bib.bib27))通过从LLaMA2 **Touvron et al.** ([2023b](#bib.bib28))进行微调，进一步增强了这种方法，利用了通过Self-Instruct技术生成的14,000个指令跟随数据点。
- en: Wizardcoder Luo et al. ([2024](#bib.bib4)) utilizes the Evol-Instruct method Xu
    et al. ([2023a](#bib.bib7)) to evolve the Codealpaca dataset further. This technique
    iteratively evolves instruction-following data in both depth and breadth dimensions.
    On the other hand, Magicoder Wei et al. ([2023](#bib.bib5)) employs the OSS-Instruct
    technique to create instruction-following data from unlabeled open-source code
    snippets, constructing a dataset of 75,000 samples based on the StarCoder dataset Lozhkov
    et al. ([2024](#bib.bib30)).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Wizardcoder Luo等人（[2024](#bib.bib4)）利用Evol-Instruct方法（Xu等人（[2023a](#bib.bib7)））进一步发展了Codealpaca数据集。这种技术在深度和广度维度上迭代地发展指令跟随数据。另一方面，Magicoder
    Wei等人（[2023](#bib.bib5)）采用OSS-Instruct技术从未标记的开源代码片段中创建指令跟随数据，基于StarCoder数据集（Lozhkov等人（[2024](#bib.bib30)））构建了一个75,000样本的数据集。
- en: 2.2 Instructional Fine-tuning
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 指导性微调
- en: Fine-tuning language models with instructional datasets has emerged as a powerful
    technique, offering notable improvements in model performance and alignment with
    human preferences and safety. By exploring a diverse array of instructional tasks,
    Wei et al. ([2021](#bib.bib31)) demonstrated a significant enhancement in zero-shot
    performance on unseen tasks through fine-tuning. Building on this, Chung et al.
    ([2024](#bib.bib32)) showed that scaling both the number of tasks and the model
    size can lead to substantial performance gains across different model architectures.
    Peng et al. ([2023](#bib.bib33)) further advanced this field by leveraging large
    language models (LLMs) to generate high-quality instruction-following data, resulting
    in improved zero-shot performance on new tasks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用指导性数据集对语言模型进行微调已成为一种强大的技术，显著提升了模型的性能和与人类偏好及安全性的对齐。通过探索各种指导任务，Wei等人（[2021](#bib.bib31)）展示了通过微调显著提升了对未见任务的零样本性能。在此基础上，Chung等人（[2024](#bib.bib32)）表明，任务数量和模型规模的扩展可以在不同模型架构中带来显著的性能提升。Peng等人（[2023](#bib.bib33)）进一步推动了这一领域，通过利用大型语言模型（LLMs）生成高质量的指令跟随数据，从而在新任务上的零样本性能得到改善。
- en: A recent study Zhou et al. ([2023](#bib.bib9)) introduces the Superficial Alignment
    Hypothesis, which posits that the bulk of knowledge in LLMs is acquired during
    pretraining. It further suggests that minimal fine-tuning data is sufficient to
    align these models with human preferences. The study demonstrates a noteworthy
    enhancement in LLM performance with just 1,000 high-quality instruction data points.
    Subsequently, a plethora of research endeavors have concentrated on refining dataset
    quality through diverse filtering methodologies for general instruction following Xu
    et al. ([2023b](#bib.bib34)); Chen et al. ([2024](#bib.bib22)); Liu et al. ([2023a](#bib.bib35)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一项研究Zhou等人（[2023](#bib.bib9)）提出了表面对齐假设，该假设认为LLMs的大部分知识是在预训练过程中获得的。它进一步建议，最少量的微调数据足以将这些模型与人类偏好对齐。研究显示，仅使用1,000个高质量的指令数据点，LLM性能就有显著提升。随后，大量研究工作集中在通过多种过滤方法来优化数据集质量，用于一般的指令跟随（Xu等人（[2023b](#bib.bib34)）；Chen等人（[2024](#bib.bib22)）；Liu等人（[2023a](#bib.bib35)））。
- en: 2.3 Data Pruning for Efficient Training
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 高效训练的数据剪枝
- en: Various pruning methods have been explored for selecting more informative samples
    for model training, each tailored to different scenarios. Data clustering has
    been widely used as a highly effective technique for data pruning. TLDR Wang et al.
    ([2023](#bib.bib16)) utilized KMeans clustering to group similar data points and
    uniformly sampled from each cluster. They employ Image-Text Matching (ITM) scores
    to identify suitable vision-text pairs, offering another perspective on sample
    selection. DEFT Das and Khetan ([2023](#bib.bib21)) utilizes unsupervised core-set
    selection for clustering-based data-efficient fine-tuning of LLMs. This approach
    significantly enhances data efficiency in fine-tuning for text-editing applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 各种剪枝方法已经被探索用于选择更具信息量的样本来进行模型训练，每种方法都针对不同的场景量身定制。数据聚类作为一种高效的数据剪枝技术被广泛使用。TLDR
    Wang等人（[2023](#bib.bib16)）利用KMeans聚类将相似的数据点分组，并从每个聚类中均匀采样。他们采用图像-文本匹配（ITM）分数来识别合适的视觉-文本对，提供了样本选择的另一种视角。DEFT
    Das和Khetan（[2023](#bib.bib21)）利用无监督核心集选择进行基于聚类的数据高效微调。这种方法显著提高了文本编辑应用中的微调数据效率。
- en: Metrics like Hardness Sorscher et al. ([2022](#bib.bib37)), Instruction Following
    Difficulty (IFD) Li et al. ([2023](#bib.bib38)) (Li et al., 2023), and SuperFiltering Li
    et al. ([2024](#bib.bib39)) focus on identifying "hard" samples that are either
    difficult to learn or easy to forget, tracking each data sample throughout training.
    In addition to these, sample influence metrics such as LESS Xia et al. ([2024](#bib.bib18))
    and TracIn Pruthi et al. ([2020](#bib.bib19)) monitor model gradients and the
    impact of individual samples, albeit with significant computational overhead for
    large models and datasets. Quality metrics from external oracles Chen et al. ([2024](#bib.bib22));
    Liu et al. ([2023a](#bib.bib35)), leverage strong language models like ChatGPT
    for data selection. However, utilizing external oracles may not always be feasible
    due to cost constraints.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 像硬度 Sorscher 等人 ([2022](#bib.bib37))、指令跟随难度 (IFD) Li 等人 ([2023](#bib.bib38))（Li
    等人，2023）和超级过滤 Li 等人 ([2024](#bib.bib39)) 这样的指标关注识别“困难”样本，这些样本要么难以学习，要么容易忘记，在训练过程中跟踪每个数据样本。除了这些，像
    LESS Xia 等人 ([2024](#bib.bib18)) 和 TracIn Pruthi 等人 ([2020](#bib.bib19)) 这样的样本影响指标监控模型梯度和个体样本的影响，尽管对大模型和数据集有显著的计算开销。外部神谕质量指标
    Chen 等人 ([2024](#bib.bib22)); Liu 等人 ([2023a](#bib.bib35)) 利用像 ChatGPT 这样的强大语言模型进行数据选择。然而，由于成本限制，利用外部神谕可能并不总是可行的。
- en: '![Refer to caption](img/7b381bd511aae9d7aeefaca461d5527b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7b381bd511aae9d7aeefaca461d5527b.png)'
- en: 'Figure 1: The overview of efficient data pruning for fine-tuning LLMs with
    large scale datasets. First, We reduce the encode instruction-following data into
    embedding and reduce the dimension of feature representation. Second, we apply
    clustering to identify and group up similar data samples. Finally, we applied
    pruning metrics to further reduce data size.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：高效数据修剪以微调大规模数据集中的LLM的概述。首先，我们将编码的指令跟随数据转化为嵌入，并减少特征表示的维度。其次，我们应用聚类以识别和分组相似的数据样本。最后，我们应用修剪指标以进一步减少数据规模。
- en: 3 Methodology
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'Our goal is to select high-quality, representative data samples so that training
    on these subsets yields performance that is comparable to or better than training
    on the entire dataset. The overview of efficient data pruning for fine-tuning
    LLMs with large scale datasets is illustrate in Figure [1](#S2.F1 "Figure 1 ‣
    2.3 Data Pruning for Efficient Training ‣ 2 Related Work ‣ Code Less, Align More:
    Efficient LLM Fine-tuning for Code Generation with Data Pruning"). First, we use
    an embedding model to project the instruction-code pairs into a vector representation.
    We further reduce the dimension of feature representation to reduce computation
    complexity of the following steps. We then apply clustering to identify and group
    up similar data samples. Finally, we applied pruning metrics to further reduce
    data size. The detail pseudo code is in Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Methodology
    ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是选择高质量的代表性数据样本，使得在这些子集上进行训练的性能与在整个数据集上训练的性能相当或更好。图[1](#S2.F1 "图1 ‣ 2.3
    高效数据修剪 ‣ 2 相关工作 ‣ 少代码，多对齐：数据修剪的高效LLM微调")展示了高效数据修剪以微调大规模数据集中的LLM的概述。首先，我们使用嵌入模型将指令-代码对投影到向量表示中。我们进一步减少特征表示的维度，以降低后续步骤的计算复杂性。然后，我们应用聚类以识别和分组相似的数据样本。最后，我们应用修剪指标以进一步减少数据规模。详细的伪代码见算法[1](#alg1
    "算法1 ‣ 3 方法论 ‣ 少代码，多对齐：数据修剪的高效LLM微调")。
- en: 'When dealing with coding datasets, two primary selection directions can be
    considered: syntactical and semantic. Selecting programs that are syntactically
    different but semantically equivalent, or vice versa, can be inefficient. Our
    design will focus on identifying syntactical differences. Detecting semantic differences
    between programs typically requires fuzzing techniques Chen et al. ([2018](#bib.bib40)),
    which involve creating larger test samples and executing programs to group them
    based on behavior. This approach contradicts our objective of reducing computational
    costs. Therefore, our method emphasizes syntactical analysis to achieve efficient
    and effective data selection.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理编码数据集时，可以考虑两种主要选择方向：语法和语义。选择语法上不同但语义等价的程序，或反之，可能效率不高。我们的设计将重点关注识别语法差异。检测程序之间的语义差异通常需要模糊测试技术
    Chen 等人 ([2018](#bib.bib40))，这涉及创建更大的测试样本并执行程序以根据行为对其进行分组。这种方法与我们降低计算成本的目标相矛盾。因此，我们的方法强调语法分析，以实现高效和有效的数据选择。
- en: Algorithm 1 Data Pruning Algorithm
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 数据修剪算法
- en: 1:Initialize $Embbedding$
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 1:初始化 $Embbedding$
- en: 3.1 Dimension Reduction
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 维度减少
- en: We convert each instruction-code pair into vector representation using a embedding
    model from raw text to enhance the efficiency of clustering and computation of
    pruning metrics Naik ([2024](#bib.bib41)). Recent research indicates that distances
    based on LLM embeddings effectively capture syntactic differences. To address
    the computational complexity, we employ Principle Component Analysis (PCA) Maćkiewicz
    and Ratajczak ([1993](#bib.bib42)) to reduce the dimensionality of the vector
    representations, as representations extracted from LLMs often exceed a thousand
    dimensions. Moreover, this approach prevents the subsequent utilization of several
    pruning metrics, which involve kernel methods, from being hindered in high-dimensional
    spaces by the curse of dimensionality.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用从原始文本到向量表示的嵌入模型将每对指令代码转换为向量，以提高聚类和修剪度量计算的效率 Naik ([2024](#bib.bib41))。最新研究表明，基于
    LLM 嵌入的距离能够有效捕捉句法差异。为了应对计算复杂性，我们采用主成分分析（PCA）Maćkiewicz 和 Ratajczak ([1993](#bib.bib42))
    来减少向量表示的维度，因为从 LLM 中提取的表示通常超过一千维。此外，这种方法避免了高维空间中由于维度诅咒而影响后续使用多个修剪度量（涉及核方法）的情况。
- en: 3.2 Clustering
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 聚类
- en: Clustering is a critical step in our methodology to group similar instruction-code
    pairs, which facilitates the selection of diverse and representative samples.
    Before clustering, we normalize the vector representations to ensure that each
    feature contributes equally to the distance calculations. From each cluster, we
    then sample instruction-code pairs to create a subset that is representative of
    the entire dataset. The sampling strategy is further decided by different pruning
    metrics.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是我们方法中的关键步骤，用于将相似的指令代码对分组，从而促进选择多样且具有代表性的样本。在聚类之前，我们规范化向量表示，以确保每个特征对距离计算的贡献均等。然后，我们从每个簇中抽样指令代码对，以创建一个代表整个数据集的子集。抽样策略进一步由不同的修剪度量决定。
- en: 3.2.1 KMeans
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 KMeans
- en: The KMeans algorithm Kanungo et al. ([2002](#bib.bib44)) partitions data into
    $k$ clusters. By minimizing the within-cluster sum-of-squares, KMeans ensures
    that each cluster is as compact as possible. The main advantage of KMeans is its
    scalability and efficiency in handling large datasets.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: KMeans 算法 Kanungo 等人 ([2002](#bib.bib44)) 将数据分成 $k$ 个簇。通过最小化簇内平方和，KMeans 确保每个簇尽可能紧凑。KMeans
    的主要优点是其在处理大数据集时的可扩展性和效率。
- en: 3.2.2 Agglomerative Clustering
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 聚合聚类
- en: Agglomerative Clustering Müllner ([2011](#bib.bib45)) builds nested clusters
    with linkage criteria. This method is advantageous since it does not require the
    number of clusters to be specified a priori. This flexibility allows for a more
    nuanced selection of representative samples, which is beneficial for maintaining
    the quality of the dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合聚类 Müllner ([2011](#bib.bib45)) 通过链接标准构建嵌套的簇。该方法的优点在于不需要预先指定簇的数量。这种灵活性允许更细致地选择代表性样本，这有助于保持数据集的质量。
- en: 3.2.3 HDBSCAN
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 HDBSCAN
- en: Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN) Rahman
    et al. ([2016](#bib.bib46)) performs clustering based on the concept of core samples,
    which are samples located in high-density areas measured by a distance metric.
    This approach aligns well with our design hypothesis to find the most syntactically
    representative data samples. Notably, HDBSCAN removes noisy samples not clustered
    into core samples as outliers.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 层次密度基础空间聚类与噪声（HDBSCAN） Rahman 等人 ([2016](#bib.bib46)) 基于核心样本的概念进行聚类，这些样本位于通过距离度量测量的高密度区域。这种方法与我们寻找最具句法代表性数据样本的设计假设相符。值得注意的是，HDBSCAN
    将未聚类到核心样本的噪声样本作为离群点移除。
- en: '| Model | Training | Benchmark | Improvement Over Base |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练 | 基准 | 相比基础的改进 |'
- en: '|  | Tokens | HumanEval (+) | MBPP (+) | HumanEval (+) | MBPP (+) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | Tokens | HumanEval (+) | MBPP (+) | HumanEval (+) | MBPP (+) |'
- en: '| GPT-3.5 Turbo | - | 72.6  (65.9) | 81.7  (69.4) | - | - |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| GPT-3.5 Turbo | - | 72.6 (65.9) | 81.7 (69.4) | - | - |'
- en: '| GPT-4 Turbo | - | 85.4  (81.7) | 83.0  (70.7) | - | - |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4 Turbo | - | 85.4 (81.7) | 83.0 (70.7) | - | - |'
- en: '| DeepSeek-Coder-Base | - | 47.6  (39.6) | 70.2  (56.6) | - | - |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-Coder-Base | - | 47.6 (39.6) | 70.2 (56.6) | - | - |'
- en: '| DeepSeek-Coder-Instruct | 2B | 73.8  (70.1) | 72.7  (63.4) | 26.2  (30.5)
    | 2.5  (6.8) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| DeepSeek-Coder-Instruct | 2B | 73.8 (70.1) | 72.7 (63.4) | 26.2 (30.5) |
    2.5 (6.8) |'
- en: '| Magicoder-DS | 90M | 66.5  (60.4) | 75.4  (61.9) | 18.9  (20.8) | 5.2  (5.3)
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Magicoder-DS | 90M | 66.5 (60.4) | 75.4 (61.9) | 18.9 (20.8) | 5.2 (5.3)
    |'
- en: '| Magicoder$\mathcal{S}$-DS | 240M | 76.8  (70.7) | 75.7  (64.4) | 29.2  (31.1)
    | 5.5  (7.8) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| Magicoder$\mathcal{S}$-DS | 240M | 76.8 (70.7) | 75.7 (64.4) | 29.2 (31.1)
    | 5.5 (7.8) |'
- en: '| Ours (full data) | 234M | 74.3  (70.8) | 74.5  (62.3) | 26.7  (31.2) | 4.3  (5.7)
    |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (完整数据) | 234M | 74.3 (70.8) | 74.5 (62.3) | 26.7 (31.2) | 4.3 (5.7) |'
- en: '| Ours (90%) | 192M | 77.0  (71.6) | 76.9  (64.0) | 29.4  (32.0) | 6.7  (7.4)
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (90%) | 192M | 77.0 (71.6) | 76.9 (64.0) | 29.4 (32.0) | 6.7 (7.4) |'
- en: '| Ours (50%) | 106M | 71.0  (64.0) | 78.0  (64.0) | 23.4  (24.4) | 7.8  (7.4)
    |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (50%) | 106M | 71.0 (64.0) | 78.0 (64.0) | 23.4 (24.4) | 7.8 (7.4) |'
- en: '| Ours (10%) | 21M | 70.4  (65.0) | 73.0  (60.2) | 22.8  (25.4) | 2.8  (3.6)
    |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (10%) | 21M | 70.4 (65.0) | 73.0 (60.2) | 22.8 (25.4) | 2.8 (3.6) |'
- en: '| Ours (1%) | 2M | 64.6  (58.0) | 74.3  (61.9) | 17.0  (18.4) | 4.1  (5.3)
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (1%) | 2M | 64.6 (58.0) | 74.3 (61.9) | 17.0 (18.4) | 4.1 (5.3) |'
- en: 'Table 1: $pass@1$ (%) results of different LLMs on HumanEval (+) and MBPP (+)
    with greedy decoding. We directly use results from prior work Guo et al. ([2024](#bib.bib47));
    Wei et al. ([2023](#bib.bib5)). All our results are reported using the HDBSCAN
    clustering algorithm with the diversity pruning metric (HDBSCAN-diversity). To
    account for the randomness of clustering and training, we report the averaged
    results from three runs evaluated with EvalPlus Liu et al. ([2023b](#bib.bib48)).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '表1: 不同 LLM 在 HumanEval (+) 和 MBPP (+) 上的 $pass@1$ (%) 结果，使用贪心解码。我们直接使用先前工作
    Guo 等 ([2024](#bib.bib47)); Wei 等 ([2023](#bib.bib5)) 的结果。我们所有的结果均使用 HDBSCAN 聚类算法和多样性修剪度量（HDBSCAN-diversity）报告。为了考虑聚类和训练的随机性，我们报告了三次运行的平均结果，这些结果使用
    EvalPlus Liu 等 ([2023b](#bib.bib48)) 进行评估。'
- en: 3.3 Pruning Metrics
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 修剪度量
- en: The criteria of choosing pruning metrics continually aligns with the idea of
    detecting syntactic difference and find most representative samples. We explain
    the pruning metrics explored in our experiments in the following sections.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 选择修剪度量的标准始终与检测句法差异和寻找最具代表性的样本的理念相一致。我们将在以下章节中解释我们实验中探索的修剪度量。
- en: 3.3.1 Diversity Metric
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 多样性度量
- en: We use a distance-based metric that simply evaluates the diversity score of
    a single instance shown as follow,
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用基于距离的度量，该度量简单地评估单个实例的多样性得分，如下所示，
- en: '|  | $d_{i}=\min_{\mathbf{x}\in\mathcal{K}\setminus\{\mathbf{x}_{i}\}}\text{dist}(\mathbf{x}_{i},\mathbf{x}),$
    |  | (1) |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $d_{i}=\min_{\mathbf{x}\in\mathcal{K}\setminus\{\mathbf{x}_{i}\}}\text{dist}(\mathbf{x}_{i},\mathbf{x}),$
    |  | (1) |'
- en: where $x_{i}$. We use the dot product of the embeddings as the distance function
    as our embeddings are normalized prior to pruning.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{i}$。我们使用嵌入的点积作为距离函数，因为我们的嵌入在修剪之前已被归一化。
- en: 3.3.2 Density Metric
  id: totrans-83
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 密度度量
- en: We applied kernel density estimation (KDE) to measure the density of samples
    in the feature space. KDE estimates the probability density function of a random
    variable. The density score for a sample $\mathbf{x}_{i}$ is given by,
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用了核密度估计（KDE）来测量特征空间中样本的密度。KDE 估计随机变量的概率密度函数。样本 $\mathbf{x}_{i}$ 的密度得分由下式给出，
- en: '|  | $\rho(\mathbf{x}_{i})=\frac{1}{nh^{d}}\sum_{j=1}^{n}K\left(\frac{\mathbf{x}_{i}-\mathbf{x}_{j}}{h}\right),$
    |  | (2) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\rho(\mathbf{x}_{i})=\frac{1}{nh^{d}}\sum_{j=1}^{n}K\left(\frac{\mathbf{x}_{i}-\mathbf{x}_{j}}{h}\right),$
    |  | (2) |'
- en: where $K$ (typically a Gaussian) measures the influence of nearby points on
    the density estimate. A high density score indicates that a sample is located
    in a region with many similar instances, suggesting it is less critical for maintaining
    diversity.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $K$（通常是高斯函数）测量附近点对密度估计的影响。高密度得分表明样本位于一个类似实例较多的区域，意味着它对保持多样性不那么重要。
- en: 3.3.3 Random
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 随机
- en: The simplest baseline is random selection, where we randomly sample data from
    the selected cluster or entire training dataset (without clustering) for instruction
    tuning.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的基线是随机选择，我们从选定的簇或整个训练数据集（没有聚类）中随机抽取数据用于指令调整。
- en: '![Refer to caption](img/7bdcde9ac1423d5ca7b185ea9e47f808.png)![Refer to caption](img/dd4cce74a103c8446e374b19563bbce1.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7bdcde9ac1423d5ca7b185ea9e47f808.png)![参见说明](img/dd4cce74a103c8446e374b19563bbce1.png)'
- en: 'Figure 2: Performance comparison of HDBSCAN-diversity and nocluster-random
    methods across different benchmarks. Our strategy outperform the baseline across
    different datasets with a large margin. We also maintain better or equivalent
    performance compare to full dataset even at the size of 10% on MBPP. The $pass@1$
    metric is plotted against varying compression ratios, demonstrating the robustness
    and effectiveness. HumanEval presents larger variance across experiments possibly
    due to less problems entries.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：HDBSCAN-diversity和nocluster-random方法在不同基准测试中的性能比较。我们的策略在不同数据集上的表现显著优于基线方法。即使在MBPP的10%数据量下，我们也保持了与完整数据集相当或更好的性能。$pass@1$指标在不同压缩比下进行绘图，展示了方法的稳健性和有效性。由于问题条目较少，HumanEval在不同实验中的方差较大。
- en: 4 Experiments
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 'In this section, we first present the experimental setup in Section [4.1](#S4.SS1
    "4.1 Setup ‣ 4 Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning"), followed by our primary findings in Section [4.5](#S4.SS5
    "4.5 Main Results ‣ 4 Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning
    for Code Generation with Data Pruning"). Here, we highlight the performance improvements
    of our pruning methods compared to full dataset training across four datasets:
    MBPP(+), and HumanEval(+). We also compare the $pass@1$ scores with baseline methods
    at various compression ratios.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先在第[4.1节](#S4.SS1 "4.1 设置 ‣ 4 实验 ‣ 减少代码，更多对齐：数据剪枝下高效的LLM微调")中介绍实验设置，然后在第[4.5节](#S4.SS5
    "4.5 主要结果 ‣ 4 实验 ‣ 减少代码，更多对齐：数据剪枝下高效的LLM微调")中展示我们的主要发现。在这里，我们强调了与完整数据集训练相比，我们的剪枝方法在四个数据集上的性能提升：MBPP(+)和HumanEval(+)。我们还在不同压缩比下将$pass@1$分数与基线方法进行了比较。
- en: 4.1 Setup
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: We employed DeepSeek-Coder-Base 6.7B Guo et al. ([2024](#bib.bib47)) as the
    base model due to its superior performance among open-source models. We used PCA Maćkiewicz
    and Ratajczak ([1993](#bib.bib42)) algorithm in all experiments and reduce the
    dimension to 10\. To account for randomness in clustering algorithm and training,
    we repeat each experiment 3 times and report the average and standard deviation.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用DeepSeek-Coder-Base 6.7B Guo等人 ([2024](#bib.bib47)) 作为基础模型，因为它在开源模型中表现优越。我们在所有实验中使用PCA Maćkiewicz和Ratajczak
    ([1993](#bib.bib42)) 算法，并将维度减少到10。为了考虑聚类算法和训练中的随机性，我们重复每个实验3次，并报告平均值和标准差。
- en: 4.2 Training
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 训练
- en: 'Datasets In our experiment, we adopt two synthetic code dataset as training
    data: Magicoder-OSS-Instruct-75K ¹¹1[https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K)
    (MIT License) and Magicoder-Evol-Instruct-110K ²²2[https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K](https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K)
    (Apache-2.0 License). Together we have a combined 185k entries in total as our
    target large scale dataset.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 在我们的实验中，我们采用了两个合成代码数据集作为训练数据：Magicoder-OSS-Instruct-75K ¹¹1[https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K](https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K)（MIT许可证）和Magicoder-Evol-Instruct-110K ²²2[https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K](https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K)（Apache-2.0许可证）。这两个数据集总共有185k条记录，作为我们的目标大规模数据集。
- en: We fine-tune the base model by combining and shuffling the two training dataset.
    This is different as in the original Magicoder Wei et al. ([2023](#bib.bib5))
    implementation, where they first fine-tune the base models for 2 epochs on OSS-Instruct
    data and continue training for 2 more epochs on Evol-Instruct data. We note that
    despite such difference in our implementation details, our full dataset performance
    closely matches the Magicoder$\mathcal{S}$-DS results.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过组合和打乱两个训练数据集来微调基础模型。这与原始的Magicoder Wei等人 ([2023](#bib.bib5)) 实现不同，他们首先在OSS-Instruct数据上微调基础模型2个周期，然后在Evol-Instruct数据上继续训练2个周期。尽管我们的实现细节有所不同，但我们注意到我们完整数据集的性能与Magicoder$\mathcal{S}$-DS结果非常接近。
- en: 'Training Training is conducted with 16 NVIDIA A100-80GB GPUs through the Distributed
    Data Parallel (DDP) module from PyTorch. We set the learning rate at 5e-5 with
    15 warmup steps and a linear learning rate scheduler. We use Adam Kingma and Ba
    ([2014](#bib.bib49)) as our optimizer with full parameter updates and truncate
    sequence length longer than 4096 tokens. We use a batch size of 512 samples Wei
    et al. ([2023](#bib.bib5)) when the dataset size exceeds $\geq 10\%$ of the original
    size, and a batch size of 32 Zhou et al. ([2023](#bib.bib9)) for heavily pruned
    small-scaled data experiments in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Evaluation
    ‣ 4 Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning"). We fine-tune for 2 epochs regardless of the dataset size.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '训练 训练使用 16 台 NVIDIA A100-80GB GPU 通过 PyTorch 的分布式数据并行（DDP）模块进行。我们将学习率设置为 5e-5，暖启动步骤为
    15，使用线性学习率调度器。我们使用 Adam Kingma 和 Ba ([2014](#bib.bib49)) 作为优化器，进行完全参数更新，并截断超过
    4096 个标记的序列长度。当数据集大小超过原始大小的 $\geq 10\%$ 时，我们使用 512 个样本的批量大小 Wei 等人 ([2023](#bib.bib5))，在图
    [3](#S4.F3 "Figure 3 ‣ 4.3 Evaluation ‣ 4 Experiments ‣ Code Less, Align More:
    Efficient LLM Fine-tuning for Code Generation with Data Pruning") 中对严重剪枝的小规模数据实验使用
    32 个样本的批量大小 Zhou 等人 ([2023](#bib.bib9))。我们对所有数据集大小进行 2 轮微调。'
- en: 4.3 Evaluation
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 评估
- en: '![Refer to caption](img/6efef175f63129a0cc2e7ddffad7acff.png)![Refer to caption](img/747677a9a30c3978af74cd4f07296d40.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6efef175f63129a0cc2e7ddffad7acff.png)![参考说明](img/747677a9a30c3978af74cd4f07296d40.png)'
- en: 'Figure 3: Comparison of performance under extreme data pruning conditions on
    the MBPP and HumanEval benchmarks. The $pass@1$ score on MBPP shows that even
    with just 1% of the data, our method achieves nearly equivalent performance to
    the full dataset, with a 4.1% improvement over the base model. On the HumanEval
    benchmark, while the performance with 1% of the data degrades compared to the
    full dataset training, it still achieves an 17.0% improvement over the base model.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：在 MBPP 和 HumanEval 基准测试下的极端数据剪枝条件下的性能比较。MBPP 上的 $pass@1$ 分数显示，即使只有 1% 的数据，我们的方法也能实现几乎与完整数据集相当的性能，比基线模型提高了
    4.1%。在 HumanEval 基准测试中，虽然 1% 数据的性能相比完整数据集训练有所下降，但仍比基线模型提高了 17.0%。
- en: Datasets HumanEval Chen et al. ([2021](#bib.bib50)) and MBPP Austin et al. ([2021](#bib.bib51))
    are two of the most widely used benchmarks for code generation. The two datasets
    contains 164 and 1401 problems respectively. Each task in these benchmarks includes
    a task description (e.g., docstring) as the prompt, where LLMs generate corresponding
    code whose correctness is checked by a handful of test cases. Because tests in
    these benchmarks can be insufficient, for more rigorous evaluation, we use HumanEval+
    and MBPP+, both powered by EvalPlus Liu et al. ([2023b](#bib.bib48)) to obtain
    80× and 35× more tests, respectively.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 HumanEval 陈等人 ([2021](#bib.bib50)) 和 MBPP 奥斯汀等人 ([2021](#bib.bib51)) 是最广泛使用的代码生成基准测试之一。这两个数据集分别包含
    164 个和 1401 个问题。这些基准测试中的每个任务都包括一个任务描述（例如，文档字符串）作为提示，LLMs 生成相应的代码，其正确性由少量测试用例检查。由于这些基准测试中的测试可能不够充分，为了进行更严格的评估，我们使用了
    HumanEval+ 和 MBPP+，这两个数据集都由 EvalPlus 刘等人 ([2023b](#bib.bib48)) 提供支持，分别获得了 80 倍和
    35 倍的测试量。
- en: 'Metric Following prior work Chen et al. ([2021](#bib.bib50)); Liu et al. ([2023b](#bib.bib48)),
    for each experiment we use the unbiased pass@k estimator shown as follow and mainly
    focus on comparing $pass@1$ metric:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 指标 参考先前的工作 陈等人 ([2021](#bib.bib50)); 刘等人 ([2023b](#bib.bib48))，我们对每个实验使用如下的无偏
    pass@k 估计器，并主要关注比较 $pass@1$ 指标：
- en: '|  | $pass@k:=\mathbb{E}_{\text{Problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right].$
    |  | (3) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $pass@k:=\mathbb{E}_{\text{Problems}}\left[1-\frac{\binom{n-c}{k}}{\binom{n}{k}}\right].$
    |  | (3) |'
- en: Inference We employ the EvalPlus Liu et al. ([2023b](#bib.bib48)) inference
    script with sanitation postprocessing. We adopted the vLLM Kwon et al. ([2023](#bib.bib52))
    framework and use greedy decoding for every code generation. The inference engine
    is setup with bf16 dtype, tensor parallel size of 2 and a maximum length of 4096.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 推理 我们使用 EvalPlus 刘等人 ([2023b](#bib.bib48)) 推理脚本并进行清理后处理。我们采用了 vLLM 权源等人 ([2023](#bib.bib52))
    框架，并对每次代码生成使用贪婪解码。推理引擎设置为 bf16 数据类型，张量并行大小为 2，最大长度为 4096。
- en: 4.4 Implementation Details
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 实现细节
- en: In our experiment, the PCA reduction is fitted on the benchmark dataset and
    then apply the projection to the instruction data. We used the OpenAI text-embedding-ada-002
    embedding model to encode data. All the clustering and kernel density estimation
    parameters are as default in sklearn Pedregosa et al. ([2011](#bib.bib53)). For
    algorithms that requires choosing an optimal number of clusters (such as KMeans)
    is crucial, we utilize the Elbow method Roy ([1953](#bib.bib54)) to find the point
    where adding more clusters does not significantly improve the variance explained.
    For pruning metrics, we applied the Scott’s Rule Scott ([2010](#bib.bib55)), a
    normal-reference rule for deciding the Gaussian kernel bandwidth, for kernel density
    estimation and random select 10% of the dataset as query set ($K$) for diversity
    metric.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，PCA 降维是基于基准数据集进行拟合，然后将投影应用于指令数据。我们使用了 OpenAI 的 text-embedding-ada-002
    嵌入模型来编码数据。所有的聚类和核密度估计参数均使用 sklearn Pedregosa 等人 ([2011](#bib.bib53)) 中的默认值。对于需要选择最优簇数的算法（如
    KMeans），我们利用肘部法则 Roy ([1953](#bib.bib54)) 找到添加更多簇不会显著改善方差解释的点。对于剪枝指标，我们应用了 Scott
    规则 Scott ([2010](#bib.bib55))，这是一个用于决定高斯核带宽的标准参考规则，用于核密度估计，并随机选择 10% 的数据集作为查询集
    ($K$) 以用于多样性指标。
- en: 4.5 Main Results
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 主要结果
- en: 'Table [1](#S3.T1 "Table 1 ‣ 3.2.3 HDBSCAN ‣ 3.2 Clustering ‣ 3 Methodology
    ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning") presents the $pass@1$ 700 samples), our method maintains competitive
    performance and achieves large improvements over the base model, underscoring
    the efficiency of our pruning strategy.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [1](#S3.T1 "表 1 ‣ 3.2.3 HDBSCAN ‣ 3.2 聚类 ‣ 3 方法论 ‣ 代码减少，更多对齐：高效的 LLM 微调以支持代码生成和数据剪枝")
    展示了 $pass@1$ 的 700 个样本），我们的方法保持了竞争力的性能，并在基准模型上取得了显著的改进，突显了我们剪枝策略的效率。
- en: 'Figure [2](#S3.F2 "Figure 2 ‣ 3.3.3 Random ‣ 3.3 Pruning Metrics ‣ 3 Methodology
    ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning") illustrates the detail of our pruning methods across four datasets:
    MBPP, MBPP+, HumanEval, and HumanEval+. Each subplot compares the $pass@1$ scores
    than full dataset even at 90% compression.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S3.F2 "图 2 ‣ 3.3.3 随机 ‣ 3.3 剪枝指标 ‣ 3 方法论 ‣ 代码减少，更多对齐：高效的 LLM 微调以支持代码生成和数据剪枝")
    说明了我们在四个数据集（MBPP、MBPP+、HumanEval 和 HumanEval+）上剪枝方法的详细情况。每个子图都比较了 $pass@1$ 分数，即使在
    90% 压缩下也与完整数据集相比。
- en: 'We further examine how our data pruning method performs when pushed to the
    extreme, aiming to achieve the smallest possible dataset size on the MBPP benchmark.
    The results are presented in Figure [3](#S4.F3 "Figure 3 ‣ 4.3 Evaluation ‣ 4
    Experiments ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning"). Remarkably, we found that even with just 1% of the data,
    our method achieves a 4.1% improvement over the base model, which is nearly equivalent
    to training on the full dataset. This demonstrates the robustness of our pruning
    method, highlighting its ability to maintain high performance with minimal data,
    thus significantly reducing the computational resources required.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步研究了当我们的数据剪枝方法在极端条件下运行时的表现，旨在在 MBPP 基准上实现尽可能小的数据集。结果见于图 [3](#S4.F3 "图 3
    ‣ 4.3 评估 ‣ 4 实验 ‣ 代码减少，更多对齐：高效的 LLM 微调以支持代码生成和数据剪枝")。值得注意的是，我们发现即使数据只有 1%，我们的方法也比基准模型提高了
    4.1%，几乎相当于在完整数据集上训练。这证明了我们剪枝方法的鲁棒性，突显了其在最小数据量下维持高性能的能力，从而显著减少所需的计算资源。
- en: Overall, these results demonstrate the effectiveness of data pruning strategy
    in preserving critical data features and maintaining model performance under significant
    data reduction, making it a superior choice for coding dataset pruning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些结果展示了数据剪枝策略在保留关键数据特征和在显著减少数据量的情况下维持模型性能的有效性，使其成为代码数据集剪枝的优选方案。
- en: 5 Ablation Studies
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 剥离研究
- en: Our research includes four ablation studies designed to evaluate the impact
    of (1) clustering algorithms (2) pruning metrics (3) dimension reduction (4) input
    for vector representation on the effectiveness of data pruning. In the studies,
    we will mainly focus on the MBPP benchmark since it provides more stable and consistent
    results.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究包括四项剥离研究，旨在评估（1）聚类算法（2）剪枝指标（3）降维（4）向量表示输入对数据剪枝有效性的影响。在这些研究中，我们将主要集中于 MBPP
    基准，因为它提供了更稳定和一致的结果。
- en: 5.1 Compare Clustering Algorithm
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 比较聚类算法
- en: 'In Figure [4](#S5.F4 "Figure 4 ‣ 5.1 Compare Clustering Algorithm ‣ 5 Ablation
    Studies ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning"), we present the results of applying different clustering algorithms
    without additional pruning metrics. The algorithms evaluated include Agglomerative
    Clustering, HDBSCAN, KMeans, and a baseline with no clustering (nocluster).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '在图[4](#S5.F4 "Figure 4 ‣ 5.1 Compare Clustering Algorithm ‣ 5 Ablation Studies
    ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data
    Pruning")中，我们展示了在没有额外修剪度量的情况下应用不同聚类算法的结果。评估的算法包括Agglomerative Clustering、HDBSCAN、KMeans和一个无聚类基准（nocluster）。'
- en: The results demonstrate that clustering algorithms generally improve performance
    compared to the nocluster baseline, particularly at higher compression ratios.
    HDBSCAN consistently maintains higher $pass@1$ scores, showcasing its robustness
    in preserving critical data features. KMeans and Agglomerative Clustering also
    perform well, though with higher variability. These findings highlight the importance
    of clustering algorithms in enhancing data efficiency for coding datasets.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，相比于无聚类基准，聚类算法通常能提高性能，特别是在较高的压缩比下。HDBSCAN始终保持较高的$pass@1$得分，展示了其在保持关键数据特征方面的稳健性。KMeans和Agglomerative
    Clustering也表现良好，但波动性较大。这些发现突显了聚类算法在提升代码数据集数据效率方面的重要性。
- en: '![Refer to caption](img/b922ca8b1900ce2b93155196f148b2b6.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b922ca8b1900ce2b93155196f148b2b6.png)'
- en: 'Figure 4: $pass@1$ scores compared to full dataset at the compression ratio
    of 90%.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：与全数据集在90%压缩比下的$pass@1$得分比较。
- en: '![Refer to caption](img/9dca266baf411d67f58b1b3d8394cb42.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9dca266baf411d67f58b1b3d8394cb42.png)'
- en: 'Figure 5: Comparison of different pruning metrics using HDBSCAN clustering
    algorithms. Diversity metric has marginal advantage but its benefit may be limited
    and dependent on the clustering algorithm.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：使用HDBSCAN聚类算法比较不同修剪度量。多样性度量有轻微优势，但其效果可能有限，并且依赖于聚类算法。
- en: 5.2 Compare Pruning Metrics
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 比较修剪度量
- en: 'We examine the impact of different pruning metrics on model performance. Using
    HDBSCAN clustering algorithm, we assess how these metrics influence performance
    as the data size decreases, as illustrated in Figure [5](#S5.F5 "Figure 5 ‣ 5.1
    Compare Clustering Algorithm ‣ 5 Ablation Studies ‣ Code Less, Align More: Efficient
    LLM Fine-tuning for Code Generation with Data Pruning"). The results indicate
    that the effectiveness of pruning metrics varies across different compression
    ratio. While Diversity metrics show slight improvements over other metrics, the
    margin of improvement is not substantial and only works between 10-40% compression
    ratio. This suggests that while more sophisticated pruning metrics can offer some
    benefits, their impact may be limited and also dependent on the clustering algorithm
    used.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '我们研究了不同修剪度量对模型性能的影响。使用HDBSCAN聚类算法，我们评估了这些度量如何随着数据规模的减少影响性能，如图[5](#S5.F5 "Figure
    5 ‣ 5.1 Compare Clustering Algorithm ‣ 5 Ablation Studies ‣ Code Less, Align More:
    Efficient LLM Fine-tuning for Code Generation with Data Pruning")所示。结果表明，修剪度量的有效性在不同压缩比下有所不同。虽然多样性度量相比其他度量显示出略微的改善，但这种改善幅度并不显著，并且仅在10-40%的压缩比范围内有效。这表明，尽管更复杂的修剪度量可以提供一些好处，但它们的影响可能有限，并且还依赖于所使用的聚类算法。'
- en: 5.3 Effect of PCA
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 PCA的效果
- en: 'In Table [2](#S5.T2 "Table 2 ‣ 5.3 Effect of PCA ‣ 5 Ablation Studies ‣ Code
    Less, Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning"),
    we evaluate the impact of applying Principal Component Analysis (PCA) on the performance
    of the KMeans clustering algorithm and Density metric at the compression ratio
    of 50%. The findings indicate that applying PCA generally degrades performance
    in terms of $pass@1$ scores for less than 0.6% on MBPP, and moderate negative
    impact of 4.3% on HumanEval. We hypothesize that the observed impact might be
    due to the imbalance between the MBPP and HumanEval datasets used for PCA training.
    Since the HumanEval dataset is significantly smaller than the MBPP dataset, it
    results in suboptimal extraction of principal components for HumanEval-like data.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[2](#S5.T2 "Table 2 ‣ 5.3 Effect of PCA ‣ 5 Ablation Studies ‣ Code Less,
    Align More: Efficient LLM Fine-tuning for Code Generation with Data Pruning")中，我们评估了在50%压缩比下应用主成分分析（PCA）对KMeans聚类算法和密度度量的性能影响。结果表明，应用PCA通常会使MBPP的$pass@1$得分下降不到0.6%，并对HumanEval产生4.3%的中等负面影响。我们推测观察到的影响可能由于用于PCA训练的MBPP和HumanEval数据集之间的不平衡。由于HumanEval数据集明显小于MBPP数据集，导致对类似HumanEval数据的主成分提取不够优化。'
- en: Nonetheless, reducing the dimension from 1536 to 10 leads to $\sim$12x speed
    up for KMeans. HDBSCAN clustering without PCA does not complete within 4 hours,
    thus we do not report its numbers.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，将维度从 1536 降低到 10 能使 KMeans 的速度提高约 12 倍。没有 PCA 的 HDBSCAN 聚类在 4 小时内未能完成，因此我们未报告其数据。
- en: '|  | No PCA | PCA |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 无 PCA | PCA |'
- en: '| --- | --- | --- |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Dimension | 1536 | 10 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 1536 | 10 |'
- en: '| --- | --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Runtime | 1307 sec | 183 sec |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 运行时间 | 1307 秒 | 183 秒 |'
- en: '| MBPP (+) | 74.4 (63.3) | 73.8 (62.4) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| MBPP (+) | 74.4 (63.3) | 73.8 (62.4) |'
- en: '| HumanEval (+) | 71.8 (65.0) | 67.5 (62.5) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| HumanEval (+) | 71.8 (65.0) | 67.5 (62.5) |'
- en: 'Table 2: Comparison of $pass@1$ scores, dimension, and data pruning runtime
    (excludes embedding and training) at 50% compression ratio for KMeans clustering
    with and without PCA (averaged over 3 runs).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 2：$pass@1$ 分数、维度和数据剪枝运行时（不包括嵌入和训练）的比较，压缩比例为 50% 对于带 PCA 和不带 PCA 的 KMeans
    聚类（平均 3 次运行）。
- en: 5.4 Embeddings for Instruction or Code
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 指令或代码的嵌入
- en: 'In Table [3](#S5.T3 "Table 3 ‣ 5.4 Embeddings for Instruction or Code ‣ 5 Ablation
    Studies ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning"), we investigate the influence of various inputs on the embedding
    model. Specifically, we examine the effects of using only the instruction, only
    the code solution, or both as inputs for generating embeddings. Our findings indicate
    that combining both instructions and code as embedding inputs yields better performance
    compared to using either one alone. There are no significant differences in the
    results when using only instructions or only code. This suggests that even though
    instructions and code samples often correspond closely, it is crucial to maintain
    diversity and select informative samples from both during data pruning.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [3](#S5.T3 "Table 3 ‣ 5.4 Embeddings for Instruction or Code ‣ 5 Ablation
    Studies ‣ Code Less, Align More: Efficient LLM Fine-tuning for Code Generation
    with Data Pruning") 中，我们研究了不同输入对嵌入模型的影响。具体来说，我们检查了仅使用指令、仅使用代码解决方案或同时使用这两者作为生成嵌入的输入的效果。我们的发现表明，将指令和代码结合作为嵌入输入相比于单独使用其中之一，能获得更好的性能。仅使用指令或仅使用代码的结果没有显著差异。这表明，尽管指令和代码样本通常密切相关，但在数据剪枝过程中保持多样性并从两者中选择有信息量的样本是至关重要的。'
- en: '| Feature Type | MBPP (+) | HumanEval (+) |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 特征类型 | MBPP (+) | HumanEval (+) |'
- en: '| --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Both | 76.3 (62.5) | 73.1 (69.6) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 两者 | 76.3 (62.5) | 73.1 (69.6) |'
- en: '| Instruction | 74.0 (63.7) | 69.1 (63.6) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 指令 | 74.0 (63.7) | 69.1 (63.6) |'
- en: '| Code | 74.1 (62.7) | 69.2 (63.3) |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 代码 | 74.1 (62.7) | 69.2 (63.3) |'
- en: 'Table 3: $pass@1$ scores for different embedding inputs with 50% compression
    ratio using KMeans clustering. Using both instruction and code brings slight benefits.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：使用 KMeans 聚类的不同嵌入输入的 $pass@1$ 分数，压缩比例为 50%。同时使用指令和代码带来了轻微的好处。
- en: 6 Conclusion
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This study presents an efficient data pruning strategy designed to improve the
    efficiency of fine-tuning large language models on coding datasets. Our results
    demonstrate that advanced clustering and pruning techniques can significantly
    improve data efficiency in LLMs, reducing computational costs while maintaining
    performance. Future work could focus on enhancing data quality by generating more
    informative data from clusters with low pruning metrics. We hope our findings
    provide valuable insights for developing more effective and scalable strategies
    in training code-focused LLMs, further enhancing synthetic data generation and
    the efficiency of human annotations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究提出了一种高效的数据剪枝策略，旨在提高大规模语言模型在编码数据集上的微调效率。我们的结果表明，先进的聚类和剪枝技术可以显著提高 LLM 的数据效率，减少计算成本，同时保持性能。未来的工作可以集中在通过从具有低剪枝指标的簇中生成更有信息量的数据来提升数据质量。我们希望我们的发现能为开发更有效和可扩展的代码聚焦
    LLM 训练策略提供有价值的见解，从而进一步提升合成数据生成和人工注释的效率。
- en: Limitations
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 限制
- en: One of the key limitations of our study is the inherent randomness from the
    clustering algorithms and training framework. Due to computational constraints,
    we only performed three runs and averaged the results for each of our experiments.
    While this approach provides a general indication of performance, it may not fully
    capture the variability and could lead to less accurate conclusions. More extensive
    experimentation with a larger number of runs would be necessary to achieve a higher
    degree of confidence in the results.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究的一个关键限制是聚类算法和训练框架中固有的随机性。由于计算限制，我们仅进行了三次实验，并对每次实验的结果进行了平均。虽然这种方法提供了性能的一般指示，但可能无法完全捕捉变异性，导致结论不够准确。为了达到更高的置信度，需要进行更广泛的实验和更多的运行次数。
- en: Throughout our experiments, we closely follow the hyperparameters described
    in Wei et al. ([2023](#bib.bib5)), using a batch size of 512 samples and training
    for 2 epochs. However, such a high batch size results in only a few gradient updates
    when training on smaller datasets. Therefore, we switch to a lower batch size
    of 32, as recommended in Zhou et al. ([2023](#bib.bib9)), when our pruned dataset
    is less than 10% of the original size. We acknowledge that different hyperparameter
    settings could affect training outcomes and defer the determination of optimal
    hyperparameter settings for various training data sizes as future work.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的实验中，我们严格遵循了 Wei 等 ([2023](#bib.bib5)) 描述的超参数设置，使用了 512 个样本的批量大小，并训练了 2 个时期。然而，这么高的批量大小导致在较小数据集上训练时仅进行少量梯度更新。因此，当我们修剪后的数据集少于原始数据集的
    10% 时，我们按照 Zhou 等 ([2023](#bib.bib9)) 的建议切换到 32 的较小批量大小。我们认识到不同的超参数设置可能会影响训练结果，并将各种训练数据大小的最佳超参数设置的确定作为未来工作。
- en: Potential Risks
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 潜在风险
- en: This study focus exclusively on English prompts for Python code generation,
    thus prompts in other languages might not produce accurate or functional code.
    Additionally, the lack of safety alignment means there is a risk of generating
    malicious code or harmful language, which could lead to security vulnerabilities
    or unintended consequences. Code generation using LLMs carries inherent risks,
    such as producing incorrect or suboptimal code, failing to adhere to best practices,
    or introducing security flaws. Furthermore, LLMs may inadvertently propagate biases
    present in their training data, leading to biased outcomes in the generated code.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究专注于 Python 代码生成的英文提示，因此其他语言的提示可能不会生成准确或有效的代码。此外，缺乏安全对齐意味着存在生成恶意代码或有害语言的风险，这可能导致安全漏洞或意外后果。使用
    LLM 进行代码生成具有固有风险，例如生成不正确或次优的代码、未遵循最佳实践或引入安全漏洞。此外，LLM 可能会无意中传播其训练数据中存在的偏见，导致生成的代码结果有偏差。
- en: Use of AI Assistants
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AI 助手的使用
- en: ChatGPT was utilized to refine paper writing and generate code templates for
    drawing figures. The authors took careful attention to ensure that AI-generated
    contents are accurate and align with the authors intentions.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 被用来优化论文写作和生成绘图代码模板。作者仔细确保 AI 生成的内容准确并符合作者的意图。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等 [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等。Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*，2023。
- en: 'Zhang et al. [2024] Biao Zhang, Zhongtao Liu, Colin Cherry, and Orhan Firat.
    When scaling meets llm finetuning: The effect of data, model and finetuning method.
    *arXiv preprint arXiv:2402.17193*, 2024.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2024] Biao Zhang, Zhongtao Liu, Colin Cherry 和 Orhan Firat。当规模遇上 LLM
    微调：数据、模型和微调方法的影响。*arXiv 预印本 arXiv:2402.17193*，2024。
- en: 'Chaudhary [2023] Sahil Chaudhary. Code alpaca: An instruction-following llama
    model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca),
    2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chaudhary [2023] Sahil Chaudhary。Code alpaca: 一种用于代码生成的指令跟随 llama 模型。 [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)，2023。'
- en: 'Luo et al. [2024] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering
    code large language models with evol-instruct. In *The Twelfth International Conference
    on Learning Representations*, 2024. URL [https://openreview.net/forum?id=UnUwSIgK5W](https://openreview.net/forum?id=UnUwSIgK5W).'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo 等 [2024] Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang
    Hu, Chongyang Tao, Jing Ma, Qingwei Lin, 和 Daxin Jiang. Wizardcoder: 通过 evol-instruct
    赋能代码大型语言模型。在*第十二届国际学习表征会议*，2024年。URL [https://openreview.net/forum?id=UnUwSIgK5W](https://openreview.net/forum?id=UnUwSIgK5W)。'
- en: 'Wei et al. [2023] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming
    Zhang. Magicoder: Source code is all you need. *arXiv preprint arXiv:2312.02120*,
    2023.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等 [2023] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, 和 Lingming Zhang.
    Magicoder: 你所需要的仅仅是源代码。*arXiv 预印本 arXiv:2312.02120*，2023年。'
- en: 'Wang et al. [2022] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu,
    Noah A Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning
    language models with self-generated instructions. *arXiv preprint arXiv:2212.10560*,
    2022.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等 [2022] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah
    A Smith, Daniel Khashabi, 和 Hannaneh Hajishirzi. Self-instruct: 将语言模型与自生成指令对齐。*arXiv
    预印本 arXiv:2212.10560*，2022年。'
- en: 'Xu et al. [2023a] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, and Daxin Jiang. Wizardlm: Empowering large language models
    to follow complex instructions. *arXiv preprint arXiv:2304.12244*, 2023a.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等 [2023a] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan
    Feng, Chongyang Tao, 和 Daxin Jiang. Wizardlm: 赋能大型语言模型以跟随复杂指令。*arXiv 预印本 arXiv:2304.12244*，2023年。'
- en: Tsai et al. [2023a] Yun-Da Tsai, Tzu-Hsien Tsai, and Shou-De Lin. Differential
    good arm identification. *arXiv preprint arXiv:2303.07154*, 2023a.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai 等 [2023a] Yun-Da Tsai, Tzu-Hsien Tsai, 和 Shou-De Lin. 差分良臂识别。*arXiv 预印本
    arXiv:2303.07154*，2023年。
- en: 'Zhou et al. [2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun,
    Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh,
    Mike Lewis, Luke Zettlemoyer, and Omer Levy. Lima: Less is more for alignment,
    2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou 等 [2023] Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning
    Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,
    Luke Zettlemoyer, 和 Omer Levy. Lima: 对齐的“少即是多”，2023年。'
- en: Fu et al. [2023] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong
    Bing, and Nigel Collier. On the effectiveness of parameter-efficient fine-tuning.
    In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 37,
    pages 12799–12807, 2023.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等 [2023] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing,
    和 Nigel Collier. 参数高效微调的有效性探讨。在*AAAI 人工智能会议论文集*，第37卷，第12799–12807页，2023年。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu 等 [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen. Lora: 大型语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685*，2021年。'
- en: Su et al. [2022] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu
    Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith, et al.
    Selective annotation makes language models better few-shot learners. *arXiv preprint
    arXiv:2209.01975*, 2022.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等 [2022] Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang,
    Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah A Smith 等. 选择性标注使语言模型成为更好的少样本学习者。*arXiv
    预印本 arXiv:2209.01975*，2022年。
- en: Diao et al. [2023] Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. Active
    prompting with chain-of-thought for large language models. *arXiv preprint arXiv:2302.12246*,
    2023.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Diao 等 [2023] Shizhe Diao, Pengcheng Wang, Yong Lin, 和 Tong Zhang. 使用链式思维对大型语言模型进行主动提示。*arXiv
    预印本 arXiv:2302.12246*，2023年。
- en: Tsai et al. [2024a] Yun-Da Tsai, Cayon Liow, Yin Sheng Siang, and Shou-De Lin.
    Toward more generalized malicious url detection models. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 38, pages 21628–21636, 2024a.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai 等 [2024a] Yun-Da Tsai, Cayon Liow, Yin Sheng Siang, 和 Shou-De Lin. 朝向更通用的恶意
    URL 检测模型。在*AAAI 人工智能会议论文集*，第38卷，第21628–21636页，2024年。
- en: Penedo et al. [2024] Guilherme Penedo, Hynek Kydlíček, Leandro von Werra, and
    Thomas Wolf. Fineweb, April 2024. URL [https://huggingface.co/datasets/HuggingFaceFW/fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb).
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo 等 [2024] Guilherme Penedo, Hynek Kydlíček, Leandro von Werra, 和 Thomas
    Wolf. Fineweb，2024年4月。URL [https://huggingface.co/datasets/HuggingFaceFW/fineweb](https://huggingface.co/datasets/HuggingFaceFW/fineweb)。
- en: Wang et al. [2023] Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao Zhang,
    Stan Weixian Lei, and Mike Zheng Shou. Too large; data reduction for vision-language
    pre-training. In *Proceedings of the IEEE/CVF International Conference on Computer
    Vision*, pages 3147–3157, 2023.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 [2023] Alex Jinpeng Wang, Kevin Qinghong Lin, David Junhao Zhang, Stan
    Weixian Lei 和 Mike Zheng Shou. Too large; data reduction for vision-language pre-training.
    In *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    页码 3147–3157, 2023。
- en: Tsai and Lin [2024] Yun-Da Tsai and Shou-De Lin. Handling concept drift in non-stationary
    bandit through predicting future rewards. In *Pacific-Asia Conference on Knowledge
    Discovery and Data Mining*, pages 161–173\. Springer, 2024.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai 和 Lin [2024] Yun-Da Tsai 和 Shou-De Lin. Handling concept drift in non-stationary
    bandit through predicting future rewards. In *Pacific-Asia Conference on Knowledge
    Discovery and Data Mining*, 页码 161–173\. Springer, 2024。
- en: 'Xia et al. [2024] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev
    Arora, and Danqi Chen. Less: Selecting influential data for targeted instruction
    tuning. *arXiv preprint arXiv:2402.04333*, 2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia 等 [2024] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora
    和 Danqi Chen. Less: Selecting influential data for targeted instruction tuning.
    *arXiv 预印本 arXiv:2402.04333*, 2024。'
- en: Pruthi et al. [2020] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan.
    Estimating training data influence by tracing gradient descent. *Advances in Neural
    Information Processing Systems*, 33:19920–19930, 2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pruthi 等 [2020] Garima Pruthi, Frederick Liu, Satyen Kale 和 Mukund Sundararajan.
    Estimating training data influence by tracing gradient descent. *Advances in Neural
    Information Processing Systems*, 33:19920–19930, 2020。
- en: Schoch et al. [2023] Stephanie Schoch, Ritwick Mishra, and Yangfeng Ji. Data
    selection for fine-tuning large language models using transferred shapley values.
    *arXiv preprint arXiv:2306.10165*, 2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schoch 等 [2023] Stephanie Schoch, Ritwick Mishra 和 Yangfeng Ji. Data selection
    for fine-tuning large language models using transferred shapley values. *arXiv
    预印本 arXiv:2306.10165*, 2023。
- en: 'Das and Khetan [2023] Devleena Das and Vivek Khetan. Deft: Data efficient fine-tuning
    for large language models via unsupervised core-set selection. *arXiv preprint
    arXiv:2310.16776*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Das 和 Khetan [2023] Devleena Das 和 Vivek Khetan. Deft: Data efficient fine-tuning
    for large language models via unsupervised core-set selection. *arXiv 预印本 arXiv:2310.16776*,
    2023。'
- en: 'Chen et al. [2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna,
    Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia
    Jin. Alpagasus: Training a better alpaca with fewer data, 2024.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 [2024] Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna,
    Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang 和 Hongxia Jin.
    Alpagasus: Training a better alpaca with fewer data, 2024。'
- en: Moser et al. [2024] Brian B Moser, Federico Raue, and Andreas Dengel. A study
    in dataset pruning for image super-resolution. *arXiv preprint arXiv:2403.17083*,
    2024.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Moser 等 [2024] Brian B Moser, Federico Raue 和 Andreas Dengel. A study in dataset
    pruning for image super-resolution. *arXiv 预印本 arXiv:2403.17083*, 2024。
- en: Meding et al. [2021] Kristof Meding, Luca M Schulze Buschoff, Robert Geirhos,
    and Felix A Wichmann. Trivial or impossible–dichotomous data difficulty masks
    model differences (on imagenet and beyond). *arXiv preprint arXiv:2110.05922*,
    2021.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Meding 等 [2021] Kristof Meding, Luca M Schulze Buschoff, Robert Geirhos 和 Felix
    A Wichmann. Trivial or impossible–dichotomous data difficulty masks model differences
    (on imagenet and beyond). *arXiv 预印本 arXiv:2110.05922*, 2021。
- en: Da Tsai and De Lin [2022] Yun Da Tsai and Shou De Lin. Fast online inference
    for nonlinear contextual bandit based on generative adversarial network. *arXiv
    preprint arXiv:2202.08867*, 2022.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Da Tsai 和 De Lin [2022] Yun Da Tsai 和 Shou De Lin. Fast online inference for
    nonlinear contextual bandit based on generative adversarial network. *arXiv 预印本
    arXiv:2202.08867*, 2022。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等. Llama: Open and efficient foundation language models. *arXiv 预印本
    arXiv:2302.13971*, 2023a。'
- en: 'Roziere et al. [2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, et al. Code llama: Open foundation models for code. *arXiv preprint arXiv:2308.12950*,
    2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere 等 [2023] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla,
    Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin 等.
    Code llama: Open foundation models for code. *arXiv 预印本 arXiv:2308.12950*, 2023。'
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale 等. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    预印本 arXiv:2307.09288*, 2023b。'
- en: Tsai et al. [2021] Yun-Da Tsai, ChengKuan Chen, and Shou-De Lin. Toward an effective
    black-box adversarial attack on functional javascript malware against commercial
    anti-virus. In *Proceedings of the 30th ACM International Conference on Information
    & Knowledge Management*, pages 4165–4172, 2021.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai et al. [2021] 蔡云达、陈承宽和林守德。针对商业防病毒的功能性 JavaScript 恶意软件的有效黑箱对抗攻击。载于*第30届ACM国际信息与知识管理大会论文集*，第4165–4172页，2021年。
- en: 'Lozhkov et al. [2024] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico
    Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu,
    Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. *arXiv
    preprint arXiv:2402.19173*, 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lozhkov et al. [2024] 安东·洛日科夫、雷蒙德·李、卢布娜·本·阿拉尔、费德里科·卡萨诺、乔尔·拉米-普瓦耶、努阿曼·塔齐、唐敖、德米特罗·皮赫塔、刘佳伟、魏宇翔等。Starcoder
    2 和 Stack v2：下一代。*arXiv 预印本 arXiv:2402.19173*，2024年。
- en: Wei et al. [2021] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei
    Yu, Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models
    are zero-shot learners. *arXiv preprint arXiv:2109.01652*, 2021.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. [2021] 杰森·魏、马尔滕·博斯玛、文森特·Y·赵、凯尔文·顾、亚当斯·魏·余、布赖恩·莱斯特、段楠、安德鲁·M·戴和阮国辉。微调语言模型是零样本学习者。*arXiv
    预印本 arXiv:2109.01652*，2021年。
- en: Chung et al. [2024] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *Journal of Machine Learning Research*,
    25(70):1–53, 2024.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung et al. [2024] 宋兴万、乐侯、肖恩·朗普雷、巴雷特·佐夫、易泰、威廉·费杜斯、李云轩、王学智、穆斯塔法·德赫赫尼、斯迪哈特·布拉赫马等。扩展指令微调语言模型。*机器学习研究杂志*，25(70):1–53，2024年。
- en: Peng et al. [2023] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and
    Jianfeng Gao. Instruction tuning with gpt-4. *arXiv preprint arXiv:2304.03277*,
    2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng et al. [2023] 彭宝林、李春远、何鹏程、米歇尔·加利和高剑峰。使用 GPT-4 的指令调优。*arXiv 预印本 arXiv:2304.03277*，2023年。
- en: 'Xu et al. [2023b] Yang Xu, Yongqiang Yao, Yufan Huang, Mengnan Qi, Maoquan
    Wang, Bin Gu, and Neel Sundaresan. Rethinking the instruction quality: Lift is
    what you need, 2023b.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. [2023b] 许杨、姚永强、黄宇凡、齐梦楠、王毛泉、顾斌和尼尔·孙达雷桑。重新思考指令质量：提升是你所需要的，2023b。
- en: Liu et al. [2023a] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian
    He. What makes good data for alignment? a comprehensive study of automatic data
    selection in instruction tuning. *arXiv preprint arXiv:2312.15685*, 2023a.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. [2023a] 刘伟、曾伟豪、何克青、姜永和何俊贤。什么样的数据适合对齐？指令调优中自动数据选择的综合研究。*arXiv 预印本
    arXiv:2312.15685*，2023a。
- en: Tsai et al. [2024b] Yun-Da Tsai, Ting-Yu Yen, Pei-Fu Guo, Zhe-Yan Li, and Shou-De
    Lin. Text-centric alignment for multi-modality learning. *arXiv preprint arXiv:2402.08086*,
    2024b.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai et al. [2024b] 蔡云达、严亭宇、郭佩夫、李哲燕和林守德。面向多模态学习的文本中心对齐。*arXiv 预印本 arXiv:2402.08086*，2024b。
- en: 'Sorscher et al. [2022] Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya
    Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling
    via data pruning. *Advances in Neural Information Processing Systems*, 35:19523–19536,
    2022.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sorscher et al. [2022] 本·索切尔、罗伯特·盖尔霍斯、沙尚克·谢赫、苏里亚·甘古利和阿里·莫尔科斯。超越神经缩放定律：通过数据剪枝超越幂律缩放。*神经信息处理系统进展*，35:19523–19536，2022年。
- en: 'Li et al. [2023] Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen,
    Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. From quantity to quality:
    Boosting llm performance with self-guided data selection for instruction tuning.
    *arXiv preprint arXiv:2308.12032*, 2023.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023] 李鸣、张永、李志涛、陈久海、陈立昌、程宁、王健宗、周天一和肖靖。由数量到质量：通过自指导的数据选择提升 LLM 性能。*arXiv
    预印本 arXiv:2308.12032*，2023年。
- en: 'Li et al. [2024] Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong
    Wang, Ning Cheng, and Tianyi Zhou. Superfiltering: Weak-to-strong data filtering
    for fast instruction-tuning. *arXiv preprint arXiv:2402.00530*, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2024] 李鸣、张永、何帅、李志涛、赵鸿宇、王健宗、程宁和周天一。超级过滤：弱到强的数据过滤以加速指令调优。*arXiv 预印本
    arXiv:2402.00530*，2024年。
- en: Chen et al. [2018] Chen Chen, Baojiang Cui, Jinxin Ma, Runpu Wu, Jianchao Guo,
    and Wenqian Liu. A systematic review of fuzzing techniques. *Computers & Security*,
    75:118–137, 2018.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen et al. [2018] 陈晨、崔宝江、马劲鑫、吴润普、郭建超和刘文倩。模糊测试技术的系统综述。*计算机与安全*，75:118–137，2018年。
- en: Naik [2024] Atharva Naik. On the limitations of embedding based methods for
    measuring functional correctness for code generation. *arXiv preprint arXiv:2405.01580*,
    2024.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Naik [2024] 阿特哈瓦·奈克。关于基于嵌入方法测量代码生成功能正确性的局限性。*arXiv 预印本 arXiv:2405.01580*，2024年。
- en: Maćkiewicz and Ratajczak [1993] Andrzej Maćkiewicz and Waldemar Ratajczak. Principal
    components analysis (pca). *Computers & Geosciences*, 19(3):303–342, 1993.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maćkiewicz 和 Ratajczak [1993] Andrzej Maćkiewicz 和 Waldemar Ratajczak。主成分分析
    (PCA)。*计算机与地球科学*，19(3):303–342，1993。
- en: 'Tsai et al. [2023b] YunDa Tsai, Mingjie Liu, and Haoxing Ren. Rtlfixer: Automatically
    fixing rtl syntax errors with large language models. *arXiv preprint arXiv:2311.16543*,
    2023b.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tsai 等 [2023b] YunDa Tsai, Mingjie Liu, 和 Haoxing Ren。Rtlfixer：利用大型语言模型自动修复
    RTL 语法错误。*arXiv 预印本 arXiv:2311.16543*，2023b。
- en: 'Kanungo et al. [2002] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine D
    Piatko, Ruth Silverman, and Angela Y Wu. An efficient k-means clustering algorithm:
    Analysis and implementation. *IEEE transactions on pattern analysis and machine
    intelligence*, 24(7):881–892, 2002.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kanungo 等 [2002] Tapas Kanungo, David M Mount, Nathan S Netanyahu, Christine
    D Piatko, Ruth Silverman, 和 Angela Y Wu。一种高效的 K-means 聚类算法：分析与实现。*IEEE 模式分析与机器智能学报*，24(7):881–892，2002。
- en: Müllner [2011] Daniel Müllner. Modern hierarchical, agglomerative clustering
    algorithms. *arXiv preprint arXiv:1109.2378*, 2011.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Müllner [2011] Daniel Müllner。现代层次化、聚合聚类算法。*arXiv 预印本 arXiv:1109.2378*，2011。
- en: 'Rahman et al. [2016] Md Farhadur Rahman, Weimo Liu, Saad Bin Suhaim, Saravanan
    Thirumuruganathan, Nan Zhang, and Gautam Das. Hdbscan: Density based clustering
    over location based services. *arXiv preprint arXiv:1602.03730*, 2016.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rahman 等 [2016] Md Farhadur Rahman, Weimo Liu, Saad Bin Suhaim, Saravanan Thirumuruganathan,
    Nan Zhang, 和 Gautam Das。Hdbscan：基于密度的聚类在位置服务中的应用。*arXiv 预印本 arXiv:1602.03730*，2016。
- en: 'Guo et al. [2024] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng
    Liang. Deepseek-coder: When the large language model meets programming – the rise
    of code intelligence, 2024.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo 等 [2024] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao
    Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, 和 Wenfeng
    Liang。Deepseek-coder：当大型语言模型遇上编程——代码智能的崛起，2024。
- en: Liu et al. [2023b] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming
    Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of
    large language models for code generation. In *Thirty-seventh Conference on Neural
    Information Processing Systems*, 2023b. URL [https://openreview.net/forum?id=1qvx610Cu7](https://openreview.net/forum?id=1qvx610Cu7).
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 [2023b] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, 和 Lingming Zhang。你的代码是由
    chatGPT 生成的真的正确吗？对大型语言模型在代码生成中的严格评估。在 *第37届神经信息处理系统大会*，2023b。URL [https://openreview.net/forum?id=1qvx610Cu7](https://openreview.net/forum?id=1qvx610Cu7)。
- en: 'Kingma and Ba [2014] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic
    optimization. *arXiv preprint arXiv:1412.6980*, 2014.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kingma 和 Ba [2014] Diederik P Kingma 和 Jimmy Ba。Adam：一种随机优化方法。*arXiv 预印本 arXiv:1412.6980*，2014。
- en: Chen et al. [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique
    Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, et al. Evaluating large language models trained on code. *arXiv
    preprint arXiv:2107.03374*, 2021.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等 [2021] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman 等。评估针对代码训练的大型语言模型。*arXiv 预印本 arXiv:2107.03374*，2021。
- en: Austin et al. [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma,
    Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc
    Le, et al. Program synthesis with large language models. *arXiv preprint arXiv:2108.07732*,
    2021.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Austin 等 [2021] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk
    Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le 等。使用大型语言模型进行程序合成。*arXiv
    预印本 arXiv:2108.07732*，2021。
- en: Kwon et al. [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin
    Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory
    management for large language model serving with pagedattention. In *Proceedings
    of the ACM SIGOPS 29th Symposium on Operating Systems Principles*, 2023.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwon 等 [2023] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng,
    Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, 和 Ion Stoica。具有分页注意力的大型语言模型服务的高效内存管理。在
    *ACM SIGOPS 第29届操作系统原理研讨会论文集*，2023。
- en: 'Pedregosa et al. [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
    B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas,
    A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-learn:
    Machine learning in Python. *Journal of Machine Learning Research*, 12:2825–2830,
    2011.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pedregosa 等 [2011] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
    O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A.
    Passos, D. Cournapeau, M. Brucher, M. Perrot, 和 E. Duchesnay。Scikit-learn：Python
    中的机器学习。*机器学习研究杂志*，12:2825–2830，2011。
- en: Roy [1953] Samarendra Nath Roy. On a heuristic method of test construction and
    its use in multivariate analysis. *The Annals of Mathematical Statistics*, 24(2):220–238,
    1953.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Roy [1953] Samarendra Nath Roy. 关于测试构建的启发式方法及其在多变量分析中的应用。*数学统计年鉴*，24(2):220–238，1953。
- en: 'Scott [2010] David W Scott. Scott’s rule. *Wiley Interdisciplinary Reviews:
    Computational Statistics*, 2(4):497–502, 2010.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scott [2010] David W Scott. Scott规则。*Wiley跨学科评论：计算统计*，2(4):497–502，2010。
- en: Appendix A Code Samples from Data Pruning
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 数据剪枝中的代码示例
- en: We show examples from our data pruning. The selected data samples adheres closely
    to the downstream coding tasks, from English problem description to code generation.
    We also provide an example of removed data from our pruning strategy.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了数据剪枝的示例。所选的数据样本与下游编码任务紧密相关，从英文问题描述到代码生成。我们还提供了一个从我们剪枝策略中移除的数据示例。
- en: 'Input You
    are given a Python function that takes in a list of integers and a target integer.
    The function is intended to find two distinct elements in the list that sum up
    to the target integer. However, the function is incomplete and needs to be filled
    in with the appropriate code to achieve this functionality. Complete the Python
    function ‘find_two_sum‘ to find and return the indices of two distinct elements
    in the input list that sum up to the target integer. If no such pair exists, return
    an empty list. The function signature is: def  find_two_sum(nums:  List[int],  target:  int)  ->  List[int]:
    For example, given the input ‘nums = [2, 7, 11, 15]’ and ‘target = 9’, the function
    should return ‘[0, 1]‘ since ‘nums[0] + nums[1] = 2 + 7 = 9’. Output from  typing  import  List
    \pardef  find_two_sum(nums:  List[int],  target:  int)  ->  List[int]: num_indices  =  {}
    for  i,  num  in  enumerate(nums): complement  =  target  -  num if  complement  in  num_indices:
    return  [num_indices[complement],  i] num_indices[num]  =  i return  []'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 'Input You
    are given a Python function that takes in a list of integers and a target integer.
    The function is intended to find two distinct elements in the list that sum up
    to the target integer. However, the function is incomplete and needs to be filled
    in with the appropriate code to achieve this functionality. Complete the Python
    function ‘find_two_sum‘ to find and return the indices of two distinct elements
    in the input list that sum up to the target integer. If no such pair exists, return
    an empty list. The function signature is: def  find_two_sum(nums:  List[int],  target:  int)  ->  List[int]:
    For example, given the input ‘nums = [2, 7, 11, 15]’ and ‘target = 9’, the function
    should return ‘[0, 1]‘ since ‘nums[0] + nums[1] = 2 + 7 = 9’. Output from  typing  import  List
    \pardef  find_two_sum(nums:  List[int],  target:  int)  ->  List[int]: num_indices  =  {}
    for  i,  num  in  enumerate(nums): complement  =  target  -  num if  complement  in  num_indices:
    return  [num_indices[complement],  i] num_indices[num]  =  i return  []'
- en: 'Figure 6: Example of a selected data sample.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：被选中数据样本的示例。
- en: 'Input You
    are given a list of integers representing the daily stock prices of a company.
    You need to write a function to calculate the maximum profit that can be obtained
    by buying and selling the stock at most once. If it is not possible to make any
    profit, return 0. Write a function ‘maxProfit’ that takes in a list of integers
    ‘prices‘ as input and returns the maximum profit that can be obtained. Function
    signature: def  maxProfit(prices:  List[int])  ->  int: Example: Input:  prices  =  [7,  1,  5,  3,  6,  4]
    Output:  5 Explanation:  Buy  on  day  2  (price  =  1)  and  sell  on  day  5  (price  =  6),  profit  =  6  -  1  =  5.
    \parInput:  prices  =  [7,  6,  4,  3,  1] Output:  0 Explanation:  In  this  case,  no  transaction  is  done,  i.e.,  max  profit  =  0.
    Assumptions: - The input list ‘prices’ will have at least one element. - The stock
    prices are given in chronological order. Output from  typing  import  List \pardef  maxProfit(prices:  List[int])  ->  int:
    if  not  prices: return  0 \parmin_price  =  prices[0] max_profit  =  0 \parfor  price  in  prices:
    if  price  <  min_price: min_price  =  price else: max_profit  =  max(max_profit,  price  -  min_price)
    \parreturn  max_profit The ‘maxProfit’ function iterates through the list of prices,
    keeping track of the minimum price encountered so far and the maximum profit that
    can be obtained. It updates these values as it iterates through the list and returns
    the maximum profit at the end.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 'Input You
    are given a list of integers representing the daily stock prices of a company.
    You need to write a function to calculate the maximum profit that can be obtained
    by buying and selling the stock at most once. If it is not possible to make any
    profit, return 0. Write a function ‘maxProfit’ that takes in a list of integers
    ‘prices‘ as input and returns the maximum profit that can be obtained. Function
    signature: def  maxProfit(prices:  List[int])  ->  int: Example: Input:  prices  =  [7,  1,  5,  3,  6,  4]
    Output:  5 Explanation:  Buy  on  day  2  (price  =  1)  and  sell  on  day  5  (price  =  6),  profit  =  6  -  1  =  5.
    \parInput:  prices  =  [7,  6,  4,  3,  1] Output:  0 Explanation:  In  this  case,  no  transaction  is  done,  i.e.,  max  profit  =  0.
    Assumptions: - The input list ‘prices’ will have at least one element. - The stock
    prices are given in chronological order. Output from  typing  import  List \pardef  maxProfit(prices:  List[int])  ->  int:
    if  not  prices: return  0 \parmin_price  =  prices[0] max_profit  =  0 \parfor  price  in  prices:
    if  price  <  min_price: min_price  =  price else: max_profit  =  max(max_profit,  price  -  min_price)
    \parreturn  max_profit The ‘maxProfit’ function iterates through the list of prices,
    keeping track of the minimum price encountered so far and the maximum profit that
    can be obtained. It updates these values as it iterates through the list and returns
    the maximum profit at the end.'
- en: 'Figure 7: Another example of a selected data sample.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：另一个被选中数据样本的示例。
- en: 'Input Background:
    You  are  a  Product  Manager,  named  Alice,  your  goal  is  Efficiently  create  a  successful  product,  and  the  constraint  is  .
    Question: \par#  Context ##  Original  Requirements [BOSS:  Write  a  podcast  search  engine  which  search  for  all  index  podcast.  Each  podcast  belongs  to  a  channel  and  each  podcast  contain  title,  description,  published,  audio  link,  transcript  text]
    \par##  Search  Information ###  Search  Results ###  Search  Summary \par##  mermaid  quadrantChart  code  syntax  example.  DONT  USE  QUOTO  IN  CODE  DUE  TO  INVALID  SYNTAX.  Replace  the    with  REAL  COMPETITOR  NAME
    \parquadrantChart title  Reach  and  engagement  of  campaigns x-axis  Low  Reach  –>  High  Reach
    y-axis  Low  Engagement  –>  High  Engagement … Output ##  Original  Requirements
    We  are  tasked  with  creating  a  podcast  search  engine  that  indexes  all  podcasts.  Each  podcast  is  associated  with  a  channel  and  contains  a  title,  description,  publication  date,  audio  link,  and  transcript  text.
    \par##  Product  Goals “‘python [ "Create  a  user-friendly  podcast  search  engine  that  indexes  all  podcasts",
    "Ensure  each  podcast  contains  necessary  information:  title,  description,  publication  date,  audio  link,  and  transcript  text",
    "Develop  a  system  that  efficiently  updates  and  maintains  the  podcast  index"
    ] “‘ \par##  User  Stories “‘python [ "As  a  user,  I  want  to  search  for  podcasts  by  title  or  description  so  that  I  can  find  the  content  I’m  interested  in",
    "As  a  user,  I  want  to  see  all  the  necessary  information  about  a  podcast  (title,  description,  publication  date,  audio  link,  transcript  text)  so  that  I  can  decide  whether  to  listen  to  it",
    "As  a  user,  I  want  the  search  engine  to  suggest  podcasts  based  on  my  search  history  so  that  I  can  discover  new  content",
    "As  a  user,  I  want  the  podcast  search  engine  to  be  updated  regularly  so  that  I  can  access  the  latest  podcasts",
    "As  a  user,  I  want  to  filter  search  results  by  publication  date,  channel,  or  transcript  text  so  that  I  can  narrow  down  my  search"
    ] “‘ \par##  Competitive  Analysis …'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 'Input Background:
    You  are  a  Product  Manager,  named  Alice,  your  goal  is  Efficiently  create  a  successful  product,  and  the  constraint  is  .
    Question: \par#  Context ##  Original  Requirements [BOSS:  Write  a  podcast  search  engine  which  search  for  all  index  podcast.  Each  podcast  belongs  to  a  channel  and  each  podcast  contain  title,  description,  published,  audio  link,  transcript  text]
    \par##  Search  Information ###  Search  Results ###  Search  Summary \par##  mermaid  quadrantChart  code  syntax  example.  DONT  USE  QUOTO  IN  CODE  DUE  TO  INVALID  SYNTAX.  Replace  the    with  REAL  COMPETITOR  NAME
    \parquadrantChart title  Reach  and  engagement  of  campaigns x-axis  Low  Reach  –>  High  Reach
    y-axis  Low  Engagement  –>  High  Engagement … Output ##  Original  Requirements
    We  are  tasked  with  creating  a  podcast  search  engine  that  indexes  all  podcasts.  Each  podcast  is  associated  with  a  channel  and  contains  a  title,  description,  publication  date,  audio  link,  and  transcript  text.
    \par##  Product  Goals “‘python [ "Create  a  user-friendly  podcast  search  engine  that  indexes  all  podcasts",
    "Ensure  each  podcast  contains  necessary  information:  title,  description,  publication  date,  audio  link,  and  transcript  text",
    "Develop  a  system  that  efficiently  updates  and  maintains  the  podcast  index"
    ] “‘ \par##  User  Stories “‘python [ "As  a  user,  I  want  to  search  for  podcasts  by  title  or  description  so  that  I  can  find  the  content  I’m  interested  in",
    "As  a  user,  I  want  to  see  all  the  necessary  information  about  a  podcast  (title,  description,  publication  date,  audio  link,  transcript  text)  so  that  I  can  decide  whether  to  listen  to  it",
    "As  a  user,  I  want  the  search  engine  to  suggest  podcasts  based  on  my  search  history  so  that  I  can  discover  new  content",
    "As  a  user,  I  want  the  podcast  search  engine  to  be  updated  regularly  so  that  I  can  access  the  latest  podcasts",
    "As  a  user,  I  want  to  filter  search  results  by  publication  date,  channel,  or  transcript  text  so  that  I  can  narrow  down  my  search"
    ] “‘ \par##  Competitive  Analysis …'
- en: 'Figure 8: Example of a removed data sample (outlier).'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：移除的数据样本（异常值）示例。
