- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:49:22'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:22
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'SliM-LLM: 面向大型语言模型的显著性驱动混合精度量化'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14917](https://ar5iv.labs.arxiv.org/html/2405.14917)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.14917](https://ar5iv.labs.arxiv.org/html/2405.14917)
- en: Wei Huang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 黄伟
- en: The University of Hong Kong &Haotong Qin
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 香港大学 &秦浩同
- en: ETH Zürich &Yangdong Liu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世联邦理工学院 &刘阳东
- en: Beihang University &Yawei Li
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 北京航空航天大学 &李亚伟
- en: ETH Zürich &Xianglong Liu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世联邦理工学院 &刘向龙
- en: Beihang University &Luca Benini
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 北京航空航天大学 &卢卡·贝尼尼
- en: ETH Zürich &Michele Magno
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世联邦理工学院 &米歇尔·马尼奥
- en: ETH Zürich &Xiaojuan Qi
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世联邦理工学院 &齐晓娟
- en: The University of Hong Kong Corresponding author
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 香港大学 通讯作者
- en: Abstract
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Large language models (LLMs) achieve remarkable performance in natural language
    understanding but require substantial computation resources and memory footprint.
    Post-training quantization (PTQ) is a powerful compression technique extensively
    investigated for its effectiveness in reducing memory usage and improving the
    inference efficiency of LLMs. However, existing PTQ methods are still not ideal
    in terms of accuracy and efficiency, especially with below 4 bit-widths. Standard
    PTQ methods using group-wise quantization suffer difficulties in quantizing LLMs
    accurately to such low-bit, but advanced methods remaining high-precision weights
    element-wisely are hard to realize its theoretical hardware efficiency. This paper
    presents a Salience-Driven Mixed-Precision Quantization scheme for LLMs, namely
    SliM-LLM. The scheme exploits the salience distribution of LLM weights to determine
    optimal bit-width and quantizers for accurate LLM quantization, while aligning
    bit-width partition to quantization groups for compact memory usage and fast integer
    computation on hardware inference. Specifically, the proposed SliM-LLM mainly
    relies on two novel techniques: (1) Salience-Determined Bit Allocation utilizes
    the clustering characteristics of salience distribution to allocate the bit-widths
    of each quantization group. This increases the accuracy of quantized LLMs and
    maintains the inference efficiency high; (2) Salience-Weighted Quantizer Calibration
    optimizes the parameters of the quantizer by considering the element-wise salience
    within the group. This balances the maintenance of salient information and minimization
    of errors. Comprehensive experiments show that SliM-LLM significantly improves
    the accuracy of various LLMs at ultra-low 2-3 bits, e.g., 2-bit LLaMA-7B achieves
    a 5.5-times memory-saving compared to the original model on NVIDIA A800 GPUs,
    and 48% decrease of perplexity compared to the state-of-the-art gradient-free
    PTQ method. Moreover, SliM-LLM^+, which is integrated from the extension of SliM-LLM
    with gradient-based quantizers, further reduces perplexity by 35.1%. We highlight
    that the structurally quantized features of SliM-LLM exhibit remarkable versatility
    and promote improvements in the accuracy of quantized LLMs while keeping inference
    efficiency on hardware. Our code is available at [https://github.com/Aaronhuang-778/SliM-LLM](https://github.com/Aaronhuang-778/SliM-LLM).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在自然语言理解方面取得了显著的性能，但需要大量的计算资源和内存占用。后训练量化（PTQ）是一种强大的压缩技术，广泛研究其在减少内存使用和提高LLMs推理效率方面的有效性。然而，现有的PTQ方法在准确性和效率方面仍然不理想，特别是在低于4位宽的情况下。使用分组量化的标准PTQ方法在准确量化LLMs至如此低位宽时遇到困难，但高精度权重逐元素的方法很难实现其理论硬件效率。本文提出了一种面向LLMs的显著性驱动混合精度量化方案，即SliM-LLM。该方案利用LLM权重的显著性分布来确定最佳位宽和量化器，以实现准确的LLM量化，同时将位宽划分对齐到量化组，以实现紧凑的内存使用和硬件推理上的快速整数计算。具体而言，提出的SliM-LLM主要依赖于两种新技术：（1）显著性决定的位分配利用显著性分布的聚类特性来分配每个量化组的位宽。这提高了量化LLMs的准确性，并保持了较高的推理效率；（2）显著性加权量化器校准通过考虑组内逐元素显著性来优化量化器的参数。这平衡了显著信息的维护和错误的最小化。综合实验表明，SliM-LLM在超低2-3位宽下显著提高了各种LLMs的准确性，例如，2位LLaMA-7B在NVIDIA
    A800 GPU上相比原始模型实现了5.5倍的内存节省，且与最先进的无梯度PTQ方法相比，困惑度降低了48%。此外，SliM-LLM^+，即从SliM-LLM扩展而来的集成了梯度量化器的版本，进一步将困惑度降低了35.1%。我们强调，SliM-LLM的结构化量化特性展示了显著的通用性，并促进了量化LLMs准确性的提升，同时保持了硬件上的推理效率。我们的代码可在[https://github.com/Aaronhuang-778/SliM-LLM](https://github.com/Aaronhuang-778/SliM-LLM)获取。
- en: 1 Introduction
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have exhibited exceptional performance across a
    wide array of natural language benchmarks [[3](#bib.bib3), [48](#bib.bib48), [19](#bib.bib19),
    [2](#bib.bib2)]. Notably, LLaMA [[41](#bib.bib41)] and GPT [[3](#bib.bib3)] series
    have significantly contributed to the ongoing evolution of LLMs towards universal
    language intelligence. The powerful language understanding capabilities of LLMs
    have been transferred to multi-modal domains [[25](#bib.bib25), [1](#bib.bib1),
    [40](#bib.bib40), [50](#bib.bib50)], laying the foundation for artificial general
    intelligence [[4](#bib.bib4)]. Despite these significant achievements, the substantial
    computational and memory requirements of LLMs pose efficiency challenges for real-world
    applications and deployments, particularly in resource-constrained environments.
    For example, the latest LLaMA-3-70B¹¹1 [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)
    model, with its 70 billion parameters, requires over 150GB of storage and a minimum
    of two NVIDIA A800 GPUs, each with 80GB of memory, for inference [[21](#bib.bib21)].
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在广泛的自然语言基准测试中展现了卓越的性能[[3](#bib.bib3), [48](#bib.bib48), [19](#bib.bib19),
    [2](#bib.bib2)]。值得注意的是，LLaMA[[41](#bib.bib41)]和GPT[[3](#bib.bib3)]系列对LLMs向通用语言智能的持续发展作出了重大贡献。LLMs强大的语言理解能力已经转移到多模态领域[[25](#bib.bib25),
    [1](#bib.bib1), [40](#bib.bib40), [50](#bib.bib50)]，为人工通用智能[[4](#bib.bib4)]奠定了基础。尽管取得了这些显著的成就，LLMs的计算和内存需求巨大，对现实世界应用和部署，尤其是在资源有限的环境中，带来了效率挑战。例如，最新的LLaMA-3-70B¹¹1
    [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)模型，其70亿参数需要超过150GB的存储和至少两个每个拥有80GB内存的NVIDIA
    A800 GPU用于推理[[21](#bib.bib21)]。
- en: To reduce the computation burden, post-training quantization (PTQ), as an efficient
    and effective compression approach [[11](#bib.bib11)], has also been explored
    and proven successful in quantizing the weights of pre-trained LLMs [[16](#bib.bib16),
    [20](#bib.bib20), [26](#bib.bib26), [38](#bib.bib38), [23](#bib.bib23), [6](#bib.bib6)].
    Faced with the dilemma of scaled-up LLMs and the limited computation resources,
    there is an urgent need for more aggressive compression [[20](#bib.bib20), [43](#bib.bib43)].
    However, despite considerable efforts, significant performance degradation still
    occurs in low bit-width scenarios ($\leqslant$ 3-bit). To maintain the performance,
    unstructured mixed-precision quantization schemes [[37](#bib.bib37), [20](#bib.bib20),
    [12](#bib.bib12)] or specialized transformation computations [[6](#bib.bib6),
    [43](#bib.bib43), [13](#bib.bib13), [7](#bib.bib7)] are necessary. Yet, these
    approaches impose additional burdens on hardware during the inference of quantized
    LLMs, suffering memory overhead of element-wise bit-maps and computation overhead
    of codebook decoding and bit-map addressing (even preventing efficient integer
    computation). Moreover, even though fine-tuning can improve the accuracy of quantized
    LLMs, it increases the overfitting risk and requires expensive computation resources
    and a long time [[26](#bib.bib26), [5](#bib.bib5)]. Consequently, ensuring the
    accuracy of LLMs while maintaining efficiency during deployment remains a significant
    challenge for current PTQ approaches.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少计算负担，作为一种高效且有效的压缩方法，后训练量化（PTQ）[[11](#bib.bib11)]也被探索并证明在量化预训练LLM的权重方面取得了成功[[16](#bib.bib16),
    [20](#bib.bib20), [26](#bib.bib26), [38](#bib.bib38), [23](#bib.bib23), [6](#bib.bib6)]。面对规模扩大的LLM和有限的计算资源的困境，迫切需要更激进的压缩方法[[20](#bib.bib20),
    [43](#bib.bib43)]。然而，尽管付出了相当大的努力，在低位宽场景（$\leqslant$ 3-bit）下仍然会出现显著的性能下降。为了维持性能，需要无结构混合精度量化方案[[37](#bib.bib37),
    [20](#bib.bib20), [12](#bib.bib12)]或专门的变换计算[[6](#bib.bib6), [43](#bib.bib43),
    [13](#bib.bib13), [7](#bib.bib7)]。然而，这些方法在量化LLM推理过程中对硬件施加了额外的负担，面临元素级位图的内存开销以及代码本解码和位图寻址的计算开销（甚至阻碍了高效的整数计算）。此外，尽管微调可以提高量化LLM的准确性，但它增加了过拟合风险，并且需要昂贵的计算资源和长时间[[26](#bib.bib26),
    [5](#bib.bib5)]。因此，在保持部署效率的同时确保LLM的准确性仍然是当前PTQ方法面临的重大挑战。
- en: '![Refer to caption](img/7ba144f6f03eb929552e5a1b20115e91.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7ba144f6f03eb929552e5a1b20115e91.png)'
- en: 'Figure 1: (a) The perplexity ($\downarrow$) of existing low-bit PTQ methods
    of LLaMA at 2-bit. Solid-line indicates structured quantization methods. (b) Compare
    PTQ methods with gradient quantizer at 3-bit. (c) Features of current low-bit
    quantization methods. C denotes codebook-based, S is statistic-based, and G represents
    gradient-based quantizers.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图1： (a) 现有低位 PTQ 方法的困惑度（$\downarrow$）在 2 位下的 LLaMA。实线表示结构化量化方法。 (b) 比较 3 位下的梯度量化器的
    PTQ 方法。 (c) 当前低位量化方法的特性。C 代表基于代码本，S 代表基于统计，G 代表基于梯度的量化器。
- en: 'This paper presents the Salience-Driven Mixed-Precision LLM (SliM-LLM) framework,
    an accurate and hardware-efficient PTQ method for LLMs ($\leqslant$ 3-bit). SliM-LLM
    can be seamlessly integrated into existing advanced PTQ pipelines [[16](#bib.bib16),
    [38](#bib.bib38)], as a plug-and-play approach with mixed-precision computing
    for improved performance (Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ SliM-LLM:
    Salience-Driven Mixed-Precision Quantization for Large Language Models")). Our
    approach builds on the observation that not all parameters are equally important [[12](#bib.bib12),
    [20](#bib.bib20), [37](#bib.bib37)]. Specifically, a subset of salient weights
    significantly influences an LLM’s capabilities and tends to be concentrated in
    specific channels. During structured group-wise quantization, the uneven distribution
    of these salient channels leads to differential importance across various groups.
    Based on this finding, we design a structured mixed-precision quantization approach
    for LLMs. First, we develop a novel Salience-Determined Bit Allocation (SBA) method
    to allocate the optimal bit-width configuration for each structured group based
    on the salience distribution, minimizing the weight output relative entropy. By
    implementing bit-width compensation constraints, SBA maintains the average bit-width,
    while improving the low-bit performance. Next, we introduce the Salience-Weighted
    Quantizer Calibration (SQC), which amplifies the awareness of locally salient
    weights, preventing the degradation of sensitive information within groups. SQC
    works collaboratively with SBA, exploiting the local and global salience of weights
    to preserve the performance of LLMs after quantization. Notably, SliM-LLM does
    not rely on fine-tuning processes, efficiently deploying weight quantization on
    various LLMs. Moreover, compared to the unstructured mixed-precision methods [[37](#bib.bib37),
    [12](#bib.bib12), [20](#bib.bib20)], SliM-LLM incurs no additional bits and computation
    overhead. We also deploy SliM-LLM on the application-level inference tool ²²2 [https://github.com/AutoGPTQ/AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ)
    for LLMs, facilitating mixed-precision inference on graphics processing units
    (GPUs) with high performance.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '本文提出了 Salience-Driven Mixed-Precision LLM (SliM-LLM) 框架，这是一种准确且硬件高效的 PTQ 方法，用于
    LLM（$\leqslant$ 3 位）。SliM-LLM 可以无缝集成到现有的先进 PTQ 流程中 [[16](#bib.bib16), [38](#bib.bib38)]，作为一种即插即用的混合精度计算方法，以提高性能
    (图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models"))。我们的方法建立在这样的观察基础上：并非所有参数的重要性相同 [[12](#bib.bib12),
    [20](#bib.bib20), [37](#bib.bib37)]。具体来说，一部分突出的权重显著影响 LLM 的能力，并且倾向于集中在特定通道。在结构化的组内量化过程中，这些突出通道的不均匀分布导致了各个组之间的重要性差异。基于这一发现，我们为
    LLM 设计了一种结构化的混合精度量化方法。首先，我们开发了一种新颖的 Salience-Determined Bit Allocation (SBA) 方法，根据突出的分布为每个结构化组分配最佳的位宽配置，最小化权重输出的相对熵。通过实施位宽补偿约束，SBA
    保持了平均位宽，同时改善了低位性能。接下来，我们引入了 Salience-Weighted Quantizer Calibration (SQC)，它放大了局部突出权重的感知，防止了组内敏感信息的降解。SQC
    与 SBA 协同工作，利用权重的局部和全局突出性，在量化后保持 LLM 的性能。值得注意的是，SliM-LLM 不依赖于微调过程，能够高效地在各种 LLM
    上部署权重量化。此外，与非结构化混合精度方法 [[37](#bib.bib37), [12](#bib.bib12), [20](#bib.bib20)]
    相比，SliM-LLM 不会增加额外的位数和计算开销。我们还将 SliM-LLM 部署在应用级推理工具 ²²2 [https://github.com/AutoGPTQ/AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ)
    上，为 LLM 提供高性能的混合精度推理，支持图形处理单元（GPU）。'
- en: Experiments show that for various LLM families, SliM-LLM surpasses existing
    PTQ methods on diverse benchmarks as a plug-and-play unit, particularly in low-bit
    scenarios. Using GPTQ as the backbone, SliM-LLM improves the perplexity scores
    of 2-bit LLaMA-13B and LLaMA2-13B on WikiText2 [[30](#bib.bib30)] from 20.44 and
    28.14 to 8.87 and 9.41, denoting performance improvements of over 56%, respectively.
    SliM-LLM even outperforms other unstructured mixed-precision PTQ methods, such
    as PB-LLM [[37](#bib.bib37)], APTQ [[18](#bib.bib18)] and LLM-MQ [[24](#bib.bib24)],
    in a deployment-friendly manner, showcasing its superior low-bit accuracy and
    efficiency. Moreover, we integrate SliM-LLM into OmniQuant [[38](#bib.bib38)]
    and obtain SliM-LLM^+ through gradient optimization to further improve quantization
    quality.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，对于各种LLM家族，SliM-LLM作为一个即插即用单元，在低位场景中特别是在各种基准测试中超越了现有的PTQ方法。以GPTQ为基础，SliM-LLM在WikiText2上将2位LLaMA-13B和LLaMA2-13B的困惑度评分从20.44和28.14分别提高到8.87和9.41，表示性能改进超过56%。SliM-LLM甚至在部署友好方式下优于其他非结构化混合精度PTQ方法，如PB-LLM[[37](#bib.bib37)]、APTQ[[18](#bib.bib18)]和LLM-MQ[[24](#bib.bib24)]，展示了其优越的低位精度和效率。此外，我们将SliM-LLM集成到OmniQuant[[38](#bib.bib38)]中，并通过梯度优化获得SliM-LLM^+，以进一步提高量化质量。
- en: 2 Related Work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Large Language Models (LLMs) have been significantly developed in diverse natural
    language processing domains, establishing a prominent paradigm in these fields [[4](#bib.bib4),
    [5](#bib.bib5), [51](#bib.bib51), [3](#bib.bib3), [41](#bib.bib41)]. Nevertheless,
    the exceptional success of LLMs depends on massive parameters and computations,
    posing significant challenges for deployment in resource-constrained environments.
    Consequently, research into the compression of LLMs has emerged as a promising
    field. Existing compression techniques for LLMs primarily include low-bit quantization,
    pruning, distillation, and low-rank decomposition [[46](#bib.bib46), [17](#bib.bib17),
    [16](#bib.bib16), [44](#bib.bib44), [38](#bib.bib38), [6](#bib.bib6), [52](#bib.bib52),
    [15](#bib.bib15), [20](#bib.bib20), [33](#bib.bib33), [7](#bib.bib7)]. Among these
    technologies, low-bit quantization gains remarkable attention, for efficiently
    reducing the model size without change of network structure[[52](#bib.bib52),
    [51](#bib.bib51), [5](#bib.bib5)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理领域取得了显著的发展，建立了这些领域中的**重要范式**[[4](#bib.bib4), [5](#bib.bib5),
    [51](#bib.bib51), [3](#bib.bib3), [41](#bib.bib41)]。然而，LLMs的卓越成功依赖于庞大的参数和计算量，这对资源有限的环境中的部署提出了重大挑战。因此，对LLMs压缩的研究成为了一个有前景的领域。现有的LLMs压缩技术主要包括低位量化、剪枝、蒸馏和低秩分解[[46](#bib.bib46),
    [17](#bib.bib17), [16](#bib.bib16), [44](#bib.bib44), [38](#bib.bib38), [6](#bib.bib6),
    [52](#bib.bib52), [15](#bib.bib15), [20](#bib.bib20), [33](#bib.bib33), [7](#bib.bib7)]。在这些技术中，低位量化引起了显著关注，因为它能够有效地减少模型大小而不改变网络结构[[52](#bib.bib52),
    [51](#bib.bib51), [5](#bib.bib5)]。
- en: '![Refer to caption](img/45d9676f9b79a3ace7a3ba18cc80d856.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/45d9676f9b79a3ace7a3ba18cc80d856.png)'
- en: 'Figure 2: Illustration of our proposed SliM-LLM. The Salience-Determined Bit
    Allocation (SBA) optimizes activation-aware structured precision, optimizing the
    global information distribution in quantization. Salience-Weighted Quantizer Calibration
    (SQC) detects discretely distributed salient weights, enhancing the local important
    information in LLMs.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们提出的SliM-LLM的示意图。显著性决定的位分配（SBA）优化了激活感知结构精度，优化了量化中的全局信息分布。显著性加权量化器校准（SQC）检测离散分布的显著权重，增强了LLMs中的局部重要信息。
- en: Quantization of LLMs can be generally divided into quantization-aware training
    (QAT) [[27](#bib.bib27)] and post-training quantization (PTQ) [[44](#bib.bib44),
    [16](#bib.bib16), [38](#bib.bib38)]. QAT, by employing a retraining strategy based
    on quantized perception, better preserves the performance of quantized models.
    LLM-QAT [[27](#bib.bib27)] addresses the data obstacle issue in QAT through data-free
    distillation. However, for LLMs with huge size of parameters, the cost of retraining
    is extremely inefficient[[5](#bib.bib5)]. Therefore, PTQ has become a more efficient
    choice for LLMs. For instance, LLM.int8() [[27](#bib.bib27)] and ZeroQuant [[47](#bib.bib47)]
    explore the quantization strategies for LLMs in block-wise, which is a low-cost
    grouping approach that reduces hardware burden. Smoothquant [[44](#bib.bib44)]
    scales weight and activation to decrease the difficulty of quantization. Subsequently,
    AWQ [[26](#bib.bib26)] and OWQ [[23](#bib.bib23)] also propose scaling transformations
    on outlier channels of weight to preserve their information representation capacity.
    GPTQ [[16](#bib.bib16)] reduces the group quantization error of LLMs through Hessian-based
    error compensation [[14](#bib.bib14)], achieving commendable quantization performance
    at 3-bit. OmniQuant [[38](#bib.bib38)] introduces a learnable scaling quantizer
    to reduce quantization errors in an output-aware manner. To enhance the accuracy
    of LLMs at 3-bit, APTQ [[18](#bib.bib18)] allocates different bit-width to different
    transformer blocks based on Hessian-trace, enhancing the accuracy of LLMs 3-bit.
    To achieve LLM quantization at ultra-low bit-width, recent novel efforts such
    as QuIP [[6](#bib.bib6)], QuIP# [[43](#bib.bib43)], and AQLM [[13](#bib.bib13)]
    promote quantization performance at 2-bit through learnable codebooks or additional
    fine-tuning. Meanwhile, approaches like SpQR[[12](#bib.bib12)], PB-LLM [[37](#bib.bib37)],
    and BiLLM [[20](#bib.bib20)] employ finer-grained partitioning for grouped quantization
    with unstructured mixed-precision for weights, further improving the PTQ performance.
    However, existing low-bit methods still rely on special structures and fine-grained
    grouping to ensure accuracy, which increases the difficulty of hardware deployment.
    Additionally, the extra fine-tuning training may pose a risk of domain-specific
    overfitting and undermine the efficiency of PTQ [[26](#bib.bib26)].
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: LLM的量化通常可以分为量化感知训练（QAT）[[27](#bib.bib27)]和训练后量化（PTQ）[[44](#bib.bib44), [16](#bib.bib16),
    [38](#bib.bib38)]。QAT通过基于量化感知的重新训练策略，更好地保留了量化模型的性能。LLM-QAT[[27](#bib.bib27)]通过无数据蒸馏解决了QAT中的数据障碍问题。然而，对于参数量巨大的LLM，重新训练的成本极为低效[[5](#bib.bib5)]。因此，PTQ成为LLM的更高效选择。例如，LLM.int8()[[27](#bib.bib27)]和ZeroQuant[[47](#bib.bib47)]探索了LLM的块级量化策略，这是一种低成本的分组方法，可以减少硬件负担。Smoothquant[[44](#bib.bib44)]通过缩放权重和激活来降低量化难度。随后，AWQ[[26](#bib.bib26)]和OWQ[[23](#bib.bib23)]也提出了对权重异常通道进行缩放变换，以保留其信息表示能力。GPTQ[[16](#bib.bib16)]通过基于Hessian的误差补偿[[14](#bib.bib14)]减少了LLM的组量化误差，在3-bit下实现了良好的量化性能。OmniQuant[[38](#bib.bib38)]引入了一种可学习的缩放量化器，以输出感知的方式减少量化误差。为了提高LLM在3-bit下的准确性，APTQ[[18](#bib.bib18)]根据Hessian-trace为不同的transformer块分配不同的位宽，从而提高了LLM的3-bit准确性。为了在超低位宽下实现LLM量化，近期的新颖努力如QuIP[[6](#bib.bib6)]、QuIP#[[43](#bib.bib43)]和AQLM[[13](#bib.bib13)]通过可学习的代码簿或额外的微调促进了2-bit量化性能。同时，SpQR[[12](#bib.bib12)]、PB-LLM[[37](#bib.bib37)]和BiLLM[[20](#bib.bib20)]等方法采用更细粒度的分区进行分组量化，并对权重采用无结构混合精度，从而进一步改善了PTQ性能。然而，现有的低位方法仍依赖于特殊结构和细粒度分组以确保准确性，这增加了硬件部署的难度。此外，额外的微调训练可能会带来领域特定的过拟合风险，并削弱PTQ[[26](#bib.bib26)]的效率。
- en: 3 SliM-LLM
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 SliM-LLM
- en: 'This section introduces a mixed-precision quantization technique called SliM-LLM,
    designed to overcome the performance and inference efficiency bottlenecks in mixed-precision
    frameworks. To address these challenges, we devise two novel strategies for LLMs,
    including the use of Salience-Determined Bit Allocation (SBA) based on global
    salience distribution to determine group bit-widths, and Salience-Weighted Quantizer
    Calibration (SQC) to enhance the perception of locally important weight information.
    We introduce SBA and SQC in Sec. [3.2](#S3.SS2 "3.2 Salience-Determined Bit Allocation
    ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models") and Sec. [3.3](#S3.SS3 "3.3 Salience-Weighted Quantizer Calibration
    ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models"), respectively.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '本节介绍了一种称为 SliM-LLM 的混合精度量化技术，旨在克服混合精度框架中的性能和推理效率瓶颈。为解决这些挑战，我们为 LLMs 设计了两种新策略，包括基于全局显著性分布的显著性驱动位分配（SBA）来确定组的位宽，以及显著性加权量化器校准（SQC）以增强对局部重要权重信息的感知。我们在
    Sec. [3.2](#S3.SS2 "3.2 显著性驱动位分配 ‣ 3 SliM-LLM ‣ SliM-LLM: 基于显著性的混合精度量化大语言模型")
    和 Sec. [3.3](#S3.SS3 "3.3 显著性加权量化器校准 ‣ 3 SliM-LLM ‣ SliM-LLM: 基于显著性的混合精度量化大语言模型")
    中分别介绍 SBA 和 SQC。'
- en: 3.1 Preliminaries
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 初步介绍
- en: 'Quantization Framework. We first present the general uniform quantization process
    of LLMs according to common practice [[27](#bib.bib27), [38](#bib.bib38), [1](#bib.bib1)].
    The quantization process requires mapping float-point weights distributed within
    the interval $[w_{\mathrm{min}},w_{\mathrm{max}}]$ follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 量化框架。我们首先根据常见的实践 [[27](#bib.bib27), [38](#bib.bib38), [1](#bib.bib1)] 展示 LLMs
    的一般均匀量化过程。量化过程需要将分布在 $[w_{\mathrm{min}},w_{\mathrm{max}}]$ 区间内的浮点权重映射如下：
- en: '|  | $\vspace{-0.8pt}\hat{\boldsymbol{w}}_{q}=\operatorname{clamp}(\lfloor\frac{\boldsymbol{w}_{f}}{\Delta}\rceil+z,0,2^{N}-1),~{}\Delta=\frac{w_{\mathrm{max}}-w_{\mathrm{min}}}{2^{N}-1},~{}z=-\lfloor\frac{w_{\mathrm{min}}}{\Delta}\rceil\vspace{-0.8pt}$
    |  | (1) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $\vspace{-0.8pt}\hat{\boldsymbol{w}}_{q}=\operatorname{clamp}(\lfloor\frac{\boldsymbol{w}_{f}}{\Delta}\rceil+z,0,2^{N}-1),~{}\Delta=\frac{w_{\mathrm{max}}-w_{\mathrm{min}}}{2^{N}-1},~{}z=-\lfloor\frac{w_{\mathrm{min}}}{\Delta}\rceil\vspace{-0.8pt}$
    |  | (1) |'
- en: 'where $\hat{\boldsymbol{w}}_{q}$ is quantization zero point, respectively.
    When converted to 1-bit quantization, the calculation follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\hat{\boldsymbol{w}}_{q}$ 是量化零点。当转换为 1 位量化时，计算如下：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'where $\hat{\boldsymbol{w}}_{b}$ is the number of elements in weight [[34](#bib.bib34)].
    We can formalize the per-layer loss in PTQ, following the common practice [[31](#bib.bib31),
    [16](#bib.bib16)]:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\hat{\boldsymbol{w}}_{b}$ 是权重中的元素数量 [[34](#bib.bib34)]。我们可以根据常见实践 [[31](#bib.bib31),
    [16](#bib.bib16)] 对 PTQ 中的每层损失进行形式化：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: where $\boldsymbol{x}\in\mathbb{R}^{t\times m}$ is proxy Hessian matrix by Levenberg-Marquardt
    approximation [[29](#bib.bib29), [14](#bib.bib14)] from a set of input activations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\boldsymbol{x}\in\mathbb{R}^{t\times m}$ 是通过 Levenberg-Marquardt 近似从一组输入激活中得到的代理
    Hessian 矩阵 [[29](#bib.bib29), [14](#bib.bib14)]。
- en: 'Parameter Salience. In LLMs, the importance of each individual element in weight
    matrix is various [[12](#bib.bib12), [15](#bib.bib15)]. According to Eq. ([3](#S3.E3
    "In 3.1 Preliminaries ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models")), the impact of quantizing a single element
    on the model’s output loss differs. Elements that significantly influence the
    loss are termed salient weights. Consequently, we follow the SparseGPT [[15](#bib.bib15)]
    to define the salience of each element as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '参数显著性。在 LLMs 中，权重矩阵中每个元素的重要性各不相同 [[12](#bib.bib12), [15](#bib.bib15)]。根据 Eq. ([3](#S3.E3
    "在 3.1 初步介绍 ‣ 3 SliM-LLM ‣ SliM-LLM: 基于显著性的混合精度量化大语言模型"))，量化单个元素对模型输出损失的影响是不同的。那些对损失有显著影响的元素被称为显著权重。因此，我们沿用
    SparseGPT [[15](#bib.bib15)] 来定义每个元素的显著性：'
- en: Definition 1.
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1。
- en: 'In the quadratic approximation of the loss as expressed in Eq. ([3](#S3.E3
    "In 3.1 Preliminaries ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models")), we give the Hessian matrix $H\in\mathbb{R}^{m\times
    m}$ to the output matrix for linear projection in LLMs.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '在损失的二次近似中，如 Eq. ([3](#S3.E3 "在 3.1 初步介绍 ‣ 3 SliM-LLM ‣ SliM-LLM: 基于显著性的混合精度量化大语言模型"))
    所示，我们将 $H\in\mathbb{R}^{m\times m}$ 的 Hessian 矩阵用于 LLMs 的线性投影输出矩阵。'
- en: where $[\boldsymbol{H}^{-1}]_{jj}$ to the salience measure of each weight element
    in LLMs, representing the impact of different weights on the output loss and the
    language capabilities, which also leads the generation of mixed-precision quantization
    strategies [[12](#bib.bib12), [37](#bib.bib37), [20](#bib.bib20), [24](#bib.bib24)]
    for LLMs. However, existing mixed-precision solutions require the discrete allocation
    of bit-widths across the entire weight matrix, which imposes a significant burden
    on hardware computations, thereby affecting the inference efficiency.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$[\boldsymbol{H}^{-1}]_{jj}$表示LLMs中每个权重元素的显著性度量，反映了不同权重对输出损失和语言能力的影响，同时也引导了混合精度量化策略的生成[[12](#bib.bib12),
    [37](#bib.bib37), [20](#bib.bib20), [24](#bib.bib24)]。然而，现有的混合精度解决方案需要在整个权重矩阵中离散分配位宽，这对硬件计算造成了重大负担，从而影响了推理效率。
- en: 3.2 Salience-Determined Bit Allocation
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 显著性决定的位分配
- en: '![Refer to caption](img/8d49a8a732ed1c29256e976084ca6703.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d49a8a732ed1c29256e976084ca6703.png)'
- en: 'Figure 3: Salience in LLaMA-7B.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：LLaMA-7B中的显著性。
- en: In the following, we reveal phenomenon of spatial clustering in the distribution
    of weight salience, which inspires our proposed concept of structured mixed-precision
    quantization for LLMs, and then present the Salience-Determined Bit Allocation
    (SBA) technique to optimize bit-width allocation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们揭示了权重显著性分布中的空间聚类现象，这激发了我们提出的结构化混合精度量化概念，并介绍了显著性决定的位分配（SBA）技术以优化位宽分配。
- en: 3.2.1 Spatial Clustering of Global Salience
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 全球显著性的空间聚类
- en: 'We first conduct an empirical investigation into the weight salience distribution.
    The results reveal that certain channels exhibit higher salience and show tendencies
    for spatial clustering. As illustrated in Fig. [3](#S3.F3 "Figure 3 ‣ 3.2 Salience-Determined
    Bit Allocation ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models"), salient clustering are identified around the $2100^{th}$
    layer. Also, clustered salience is detected in other layers (as shown in Fig. [3](#S3.F3
    "Figure 3 ‣ 3.2 Salience-Determined Bit Allocation ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models")). More examples of spatial
    clustering of salience are provided in Appendix [F](#A6 "Appendix F Extension
    on Salience Channel Clustering ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models").'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先进行关于权重显著性分布的实证研究。结果显示，某些通道具有更高的显著性，并且表现出空间聚类的趋势。如图[3](#S3.F3 "图 3 ‣ 3.2
    显著性决定的位分配 ‣ 3 SliM-LLM ‣ SliM-LLM：用于大型语言模型的显著性驱动混合精度量化")所示，显著性聚类在第$2100^{th}$层附近被识别。此外，在其他层中也检测到聚类显著性（见图[3](#S3.F3
    "图 3 ‣ 3.2 显著性决定的位分配 ‣ 3 SliM-LLM ‣ SliM-LLM：用于大型语言模型的显著性驱动混合精度量化")）。更多关于显著性空间聚类的例子见附录[F](#A6
    "附录 F 关于显著性通道聚类的扩展 ‣ SliM-LLM：用于大型语言模型的显著性驱动混合精度量化")。
- en: 'Then, we analyze the underlying causes of this phenomenon from a theoretical
    perspective. According to Definition [1](#Thmdefinition1 "Definition 1\. ‣ 3.1
    Preliminaries ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models"), the salience of weights in LLMs can be quantified
    numerically by the weight magnitudes and the diagonal elements of the Hessian
    matrix, which is further approximated by the product of input activations $\boldsymbol{x}\boldsymbol{x}^{\top}$.
    In LLMs, activations exhibit more extreme outliers, while the numerical differences
    in weights are relatively slight [[44](#bib.bib44), [32](#bib.bib32)]. Therefore,
    we propose an analysis of how outlier values in input activations influence the
    distribution of weight salience:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从理论角度分析这一现象的根本原因。根据定义[1](#Thmdefinition1 "定义 1 ‣ 3.1 基础知识 ‣ 3 SliM-LLM
    ‣ SliM-LLM：用于大型语言模型的显著性驱动混合精度量化")，LLMs中权重的显著性可以通过权重大小和Hessian矩阵的对角元素进行数值量化，后者进一步通过输入激活的乘积$\boldsymbol{x}\boldsymbol{x}^{\top}$进行近似。在LLMs中，激活值表现出更多的极端异常值，而权重的数值差异相对较小[[44](#bib.bib44),
    [32](#bib.bib32)]。因此，我们提出分析输入激活中的异常值如何影响权重显著性分布：
- en: Theorem 1.
  id: totrans-50
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理1。
- en: Given the input calibration activation $\boldsymbol{x}\in\mathbb{R}^{t\times
    m}$.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入校准激活$\boldsymbol{x}\in\mathbb{R}^{t\times m}$。
- en: 'Theorem [1](#ThmTheorem1 "Theorem 1\. ‣ 3.2.1 Spatial Clustering of Global
    Salience ‣ 3.2 Salience-Determined Bit Allocation ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models") elucidates the impact
    of outlier tokens on the channel-wise distribution of weight salience (detailed
    proof is provided in Appendix [F.1](#A6.SS1 "F.1 Discussion of Theorem 1 ‣ Appendix
    F Extension on Salience Channel Clustering ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models")). Additionally, recent studies [[32](#bib.bib32),
    [45](#bib.bib45)] indicate that outlier tokens in LLMs activations tend to cluster
    regionally at specific locations, resulting in sequences of consecutive significant
    tokens. According to Theorem [1](#ThmTheorem1 "Theorem 1\. ‣ 3.2.1 Spatial Clustering
    of Global Salience ‣ 3.2 Salience-Determined Bit Allocation ‣ 3 SliM-LLM ‣ SliM-LLM:
    Salience-Driven Mixed-Precision Quantization for Large Language Models"), these
    consecutive tokens will lead to channel clustering results of weight salience,
    as evidenced by the consecutive salient channels shown in Fig. [3](#S3.F3 "Figure
    3 ‣ 3.2 Salience-Determined Bit Allocation ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models"). This also means that
    when we divide the weight into multiple groups with continuous channels, the overall
    salience of different groups is different.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '定理 [1](#ThmTheorem1 "定理 1\. ‣ 3.2.1 全球显著性的空间聚类 ‣ 3.2 显著性决定的位分配 ‣ 3 SliM-LLM
    ‣ SliM-LLM: 以显著性驱动的大型语言模型的混合精度量化") 阐明了离群词元对权重显著性通道分布的影响（详细证明见附录 [F.1](#A6.SS1
    "F.1 定理 1 讨论 ‣ 附录 F 显著性通道聚类扩展 ‣ SliM-LLM: 以显著性驱动的大型语言模型的混合精度量化")）。此外，近期研究 [[32](#bib.bib32),
    [45](#bib.bib45)] 表明，LLM 激活中的离群词元往往在特定位置区域聚类，形成连续显著词元的序列。根据定理 [1](#ThmTheorem1
    "定理 1\. ‣ 3.2.1 全球显著性的空间聚类 ‣ 3.2 显著性决定的位分配 ‣ 3 SliM-LLM ‣ SliM-LLM: 以显著性驱动的大型语言模型的混合精度量化")，这些连续的词元将导致权重显著性的通道聚类结果，如图
    [3](#S3.F3 "图 3 ‣ 3.2 显著性决定的位分配 ‣ 3 SliM-LLM ‣ SliM-LLM: 以显著性驱动的大型语言模型的混合精度量化")
    中所示的连续显著通道所证实。这也意味着，当我们将权重划分为多个具有连续通道的组时，各组的整体显著性是不同的。'
- en: Due to the unstructured mixed-precision, which involves additional storage requirements
    and inference operations, the computational format is not deployment-friendly.
    However, the spatial clustering of weight salience observed in this section strongly
    inspired the development of structured mixed-precision strategies with flexible
    bit-widths while maintaining inference efficiency. Therefore, we aim to allocate
    bit-width structurally based on group-wise salience differences. This approach
    not only enhances quantization accuracy but also ensures the deployment efficiency
    of LLMs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于非结构化混合精度涉及额外的存储需求和推理操作，计算格式不适合部署。然而，本节观察到的权重显著性的空间聚类强烈启发了结构化混合精度策略的开发，这些策略具有灵活的位宽，同时保持推理效率。因此，我们旨在基于组间显著性差异结构性地分配位宽。这种方法不仅提高了量化精度，还确保了
    LLMs 的部署效率。
- en: 3.2.2 Salience-Determined Bit Allocation for Structured Group
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 结构化分组的显著性决定位分配
- en: 'To allocate optimal bit-widths to each group, we introduce a Salience-Determined
    Bit Allocation (SBA) technique for mixed-precision LLMs, as depicted in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models"). This technique, predicated on the differences in
    group salience, determines the optimal bit-width allocation for different groups
    by minimizing the distance of information entropy with the original weight output.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '为了为每个组分配最佳位宽，我们引入了一种显著性决定位分配 (SBA) 技术，用于混合精度 LLMs，如图 [1](#S1.F1 "图 1 ‣ 1 引言
    ‣ SliM-LLM: 以显著性驱动的大型语言模型的混合精度量化") 所示。这种技术以组间显著性差异为基础，通过最小化信息熵与原始权重输出的距离来确定不同组的最佳位宽分配。'
- en: 'Specifically, we first utilize the average salience as the importance indicator
    for each weight group and rank them accordingly. The proposed SBA optimizes the
    following formula to determine the optimal number of salient-unsalient quantization
    groups of LLMs:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，我们首先利用平均显著性作为每个权重组的重要性指标，并据此进行排名。提出的 SBA 优化以下公式以确定 LLMs 的显著-非显著量化组的最佳数量：
- en: '|  | $\displaystyle\begin{aligned} \text{Objective}:&amp;\mathop{\mathrm{argmin}}\limits_{g_{1},...g_{n}}\mathcal{D}_{kl}~{}(\boldsymbol{x}\boldsymbol{w}_{f}^{\top}&#124;&#124;\boldsymbol{x}\mathcal{Q}(\boldsymbol{w}_{f}&#124;[g_{1},...g_{n}])^{\top}),\\
    \text{Constrain}:&amp;&#124;\mathcal{G}_{N-1}&#124;=&#124;\mathcal{G}_{N+1}&#124;,~{}\mathcal{G}_{N-1}=\{g_{i}&#124;g_{i}=N-1\},~{}\mathcal{G}_{N+1}=\{g_{j}&#124;g_{j}=N+1\},\end{aligned}$
    |  | (4) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{aligned} \text{目标}:&\mathop{\mathrm{argmin}}\limits_{g_{1},...g_{n}}\mathcal{D}_{kl}~{}(\boldsymbol{x}\boldsymbol{w}_{f}^{\top}&#124;&#124;\boldsymbol{x}\mathcal{Q}(\boldsymbol{w}_{f}&#124;[g_{1},...g_{n}])^{\top}),\\
    \text{约束}:&&#124;\mathcal{G}_{N-1}&#124;=&#124;\mathcal{G}_{N+1}&#124;,~{}\mathcal{G}_{N-1}=\{g_{i}&#124;g_{i}=N-1\},~{}\mathcal{G}_{N+1}=\{g_{j}&#124;g_{j}=N+1\},\end{aligned}$
    |  | (4) |'
- en: where $\mathcal{D}_{kl}(\cdot||\cdot)$ is a set of groups with the same bit-width.
    We apply a compensation constraints strategy to maintain a consistent average
    bit-width for our SBA. For example, in 2-bit quantization, the groups with the
    highest salience are quantized to 3-bit. To offset the additional bits, we quantize
    an equal number of groups with the lowest salience to 1-bit, while the remaining
    groups are set to 2-bit.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}_{kl}(\cdot||\cdot)$ 是一组具有相同比特宽度的组。我们应用补偿约束策略来保持 SBA 的平均比特宽度一致。例如，在
    2 位量化中，重要性最高的组量化为 3 位。为了抵消额外的比特，我们将等数量的最低重要性组量化为 1 位，而其余组设置为 2 位。
- en: 'We utilize an effective double-pointer search (more detailed examples in Appendix [C](#A3
    "Appendix C Searching Details of Group-Wise Salience-Determined Bit Allocation
    ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models"))
    to optimize our objective in Eq. ([4](#S3.E4 "In 3.2.2 Salience-Determined Bit
    Allocation for Structured Group ‣ 3.2 Salience-Determined Bit Allocation ‣ 3 SliM-LLM
    ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models")).
    When the weight output channel size is $m$, which is highly efficient with limited
    searching space, e.g., only 16 iterations are needed in LLaMA-7B. We also provide
    detailed searching error examples in Appendix [C](#A3 "Appendix C Searching Details
    of Group-Wise Salience-Determined Bit Allocation ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models"). Notably, SBA diverges from traditional
    quantization with mean squared error (MSE) in Eq. ([3](#S3.E3 "In 3.1 Preliminaries
    ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models")) by instead utilizing the KL divergence as its measure of loss.
    Beyond simply reducing numerical quantization errors, SBA leverages relative entropy
    as a mixed bit-width metric, aiming to maximize the mutual information [[35](#bib.bib35)]
    between the quantized and original weights of the LLMs. This approach enhances
    the model’s capacity for information representation under lower bit quantization.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '我们利用一种有效的双指针搜索（更多详细示例见附录[C](#A3 "附录 C 组别重要性确定的比特分配搜索细节 ‣ SliM-LLM: 基于重要性的混合精度量化大型语言模型")）来优化公式中的目标（[4](#S3.E4
    "在 3.2.2 组别重要性确定的比特分配 ‣ 3.2 重要性确定的比特分配 ‣ 3 SliM-LLM ‣ SliM-LLM: 基于重要性的混合精度量化大型语言模型")）。当权重输出通道的大小为
    $m$ 时，这种方法在有限的搜索空间中效率很高，例如，在 LLaMA-7B 中只需 16 次迭代。我们还在附录[C](#A3 "附录 C 组别重要性确定的比特分配搜索细节
    ‣ SliM-LLM: 基于重要性的混合精度量化大型语言模型") 提供了详细的搜索误差示例。值得注意的是，SBA 与传统的均方误差（MSE）量化方法不同，它使用
    KL 散度作为其损失度量。SBA 不仅仅减少数值量化误差，还利用相对熵作为混合比特宽度指标，旨在最大化量化权重与原始权重之间的互信息[[35](#bib.bib35)]。这种方法在低比特量化下提高了模型的信息表示能力。'
- en: 3.3 Salience-Weighted Quantizer Calibration
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 重要性加权量化器校准
- en: In addition to the global group-wise distribution of salience, we notice that
    salience within the group still shows local differences in discrete distribution.
    Common existing quantizers apply uniform consideration across all weights to minimize
    the effect (error) of quantization, lacking the capability to perceive differences
    in local salience. Therefore, in this section, we introduce a Salience-Weighted
    Quantizer Calibration (SQC) to enhance the information of significant weights
    within the group by amplifying the quantizer awareness of salient weight.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 除了全球组别的重要性分布外，我们还注意到组内的重要性在离散分布中仍然表现出局部差异。常见的现有量化器对所有权重进行均匀考虑以最小化量化效应（误差），缺乏感知局部重要性差异的能力。因此，在本节中，我们引入了重要性加权量化器校准（SQC），通过放大量化器对显著权重的感知，来增强组内重要权重的信息。
- en: 3.3.1 Discrete Distribution of Local Salience
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 局部显著性离散分布
- en: '![Refer to caption](img/a220f5214e490267f5f9374991b6f709.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a220f5214e490267f5f9374991b6f709.png)'
- en: 'Figure 4: Local salience distribution of the $10^{th}$ MHA output layer in
    LLaMA-7B.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: LLaMA-7B 的第 $10$ 层 MHA 输出的局部显著性分布'
- en: 'In the aforementioned section, we allocate the bit-width for each group based
    on the global salience. However, the salience among different elements within
    the same group still locally exhibits an unstructured difference. Specifically,
    as depicted in the salience distribution in Fig. [4](#S3.F4 "Figure 4 ‣ 3.3.1
    Discrete Distribution of Local Salience ‣ 3.3 Salience-Weighted Quantizer Calibration
    ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models"), within the $10^{th}$ attention output layer of LLaMA-7b, a
    subset of sparse weights within the comparatively less salient Group-2 still maintains
    a high level of importance. In LLMs, a small number of weights with outliers may
    affect the distribution of salience in an unstructured manner, as described in
    Definition [1](#Thmdefinition1 "Definition 1\. ‣ 3.1 Preliminaries ‣ 3 SliM-LLM
    ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models").
    These discrete weights typically account for only approximately 1% of the total
    weights within the group but play a crucial role in the modeling capability of
    LLMs.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '在前述部分，我们根据全局显著性分配了每组的位宽。然而，同一组内不同元素之间的显著性仍然表现出局部的无结构差异。具体而言，如图[4](#S3.F4 "图
    4 ‣ 3.3.1 局部显著性离散分布 ‣ 3.3 显著性加权量化器校准 ‣ 3 SliM-LLM ‣ SliM-LLM: 面向大规模语言模型的显著性驱动混合精度量化")所示，在
    LLaMA-7b 的第 $10$ 层注意力输出层中，相对显著性较低的 Group-2 中的一个稀疏权重子集仍然保持了较高的重要性。在 LLM 中，少量的异常权重可能以无结构的方式影响显著性分布，如定义[1](#Thmdefinition1
    "定义 1\. ‣ 3.1 前言 ‣ 3 SliM-LLM ‣ SliM-LLM: 面向大规模语言模型的显著性驱动混合精度量化")所述。这些离散权重通常仅占组内总权重的约
    1%，但在 LLM 的建模能力中扮演了至关重要的角色。'
- en: 'The existing vanilla quantizers still face the challenge in representing significant
    weight information, for only considering mean error of all elements within a group.
    When quantizing weights according to Eq. ([1](#S3.E1 "In 3.1 Preliminaries ‣ 3
    SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language
    Models")) in group-wise format, a large number of relatively non-salient weights
    at the intra-group statistical level tend to dominate the parameters generated
    by the quantizer. This leads to a degradation of salient information within the
    group, thereby affecting the model performance of LLMs.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的普通量化器在表示重要权重信息方面仍面临挑战，因为它们仅考虑了组内所有元素的均值误差。当按照 Eq. ([1](#S3.E1 "在 3.1 前言
    ‣ 3 SliM-LLM ‣ SliM-LLM: 面向大规模语言模型的显著性驱动混合精度量化")) 以组为单位进行量化时，大量相对不显著的权重在组内统计水平上往往主导了量化器生成的参数。这导致了组内显著性信息的退化，从而影响
    LLM 的模型性能。'
- en: 3.3.2 Salience-Weighted Quantizer Calibration for Local Salience Awareness
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 面向局部显著性的显著性加权量化器校准
- en: 'To prevent the degradation of local salient weight information in each group,
    we propose the Salience-Weighted Quantizer Calibration (SQC), which enhances the
    expression of salient weights through locally unstructured salience awareness,
    thereby reducing the quantization error of these significant elements and improving
    the compressed performance of LLMs. SQC first introduces the calibration parameter
    $\gamma$ to the quantizer, liberating the perception interval during quantization:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止每个组中局部显著性权重信息的退化，我们提出了显著性加权量化器校准（SQC），它通过局部无结构的显著性感知来增强显著性权重的表达，从而减少这些重要元素的量化误差，并提高
    LLM 的压缩性能。SQC 首先将校准参数 $\gamma$ 引入量化器，释放量化过程中的感知区间：
- en: '|  | $\Delta=\frac{\gamma(w_{\mathrm{max}}-w_{\mathrm{min}})}{2^{N}-1},~{}z=-\lfloor\frac{\gamma
    w_{\mathrm{min}}}{\Delta}\rceil$ |  | (5) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta=\frac{\gamma(w_{\mathrm{max}}-w_{\mathrm{min}})}{2^{N}-1},~{}z=-\lfloor\frac{\gamma
    w_{\mathrm{min}}}{\Delta}\rceil$ |  | (5) |'
- en: 'where $\gamma$ expands the solution space of the quantizer, subsequently minimizes
    the loss of quantization in the salience-weighted objective:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma$ 扩展了量化器的解空间，随后最小化了在显著性加权目标中量化的损失：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where $\mathcal{L}$ at 50 to achieve a balance between efficiency and accuracy.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{L}$ 在 50 时实现了效率和准确性的平衡。
- en: 'SQC effectively mitigates the degradation of local salient weights within groups
    (more evidence is provided in Appendix [D](#A4 "Appendix D Extension Ablation
    on SQC ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language
    Models")). Additionally, SQC only requires the optimization of $\gamma$ during
    quantization and without distinguishing unstructured parts in storage and inference
    stages, thereby avoiding hardware overhead. Combined with SBA, they jointly enhance
    the awareness of local and global salient weights, capturing significant information
    in LLMs to improve quantization performance.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: SQC 有效缓解了组内局部显著性权重的降级（更多证据见附录 [D](#A4 "附录 D 对 SQC 的扩展消融 ‣ SliM-LLM：面向大规模语言模型的显著性驱动混合精度量化")）。此外，SQC
    仅需在量化过程中优化 $\gamma$，并且在存储和推理阶段无需区分非结构化部分，从而避免了硬件开销。结合 SBA，它们共同增强了对局部和全局显著性权重的感知，捕捉
    LLM 中的重要信息以改善量化性能。
- en: 'Table 1: Quantization results of LLaMA family with statistic quantizer. We
    report the WikiText2 perplexity in this table, C4 results are shown in Appendix [G](#A7
    "Appendix G More Comparisons ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models").'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用统计量化器的 LLaMA 系列量化结果。我们在此表中报告了 WikiText2 困惑度，C4 结果见附录 [G](#A7 "附录 G 更多比较
    ‣ SliM-LLM：面向大规模语言模型的显著性驱动混合精度量化")。
- en: '| #W PPL$\downarrow$ | Method | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B
    | 2-70B | 3-8B | 3-70B |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| #W PPL$\downarrow$ | 方法 | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    | 3-8B | 3-70B |'
- en: '| 16-bit | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 | 5.75 | 2.9
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 16-bit | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 | 5.75 | 2.9
    |'
- en: '| 3-bit | APTQ | 6.76 | - | - | - | - | - | - | - | - |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | APTQ | 6.76 | - | - | - | - | - | - | - | - |'
- en: '| LLM-MQ | - | - | - | - | - | 8.54 | - | - | - |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| LLM-MQ | - | - | - | - | - | 8.54 | - | - | - |'
- en: '| RTN | 7.01 | 5.88 | 4.87 | 4.24 | 6.66 | 5.51 | 3.97 | 27.91 | 11.84 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 7.01 | 5.88 | 4.87 | 4.24 | 6.66 | 5.51 | 3.97 | 27.91 | 11.84 |'
- en: '| AWQ | 6.46 | 5.51 | 4.63 | 3.99 | 6.24 | 5.32 | - | 8.22 | 4.81 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.46 | 5.51 | 4.63 | 3.99 | 6.24 | 5.32 | - | 8.22 | 4.81 |'
- en: '| GPTQ | 6.55 | 5.62 | 4.80 | 4.17 | 6.29 | 5.42 | 3.85 | 8.19 | 5.22 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 6.55 | 5.62 | 4.80 | 4.17 | 6.29 | 5.42 | 3.85 | 8.19 | 5.22 |'
- en: '| SliM-LLM | 6.40 | 5.48 | 4.61 | 3.99 | 6.24 | 5.26 | 3.67 | 7.16 | 4.08 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM | 6.40 | 5.48 | 4.61 | 3.99 | 6.24 | 5.26 | 3.67 | 7.16 | 4.08 |'
- en: '| 2-bit | LLM-MQ | - | - | - | - | - | 12.17 | - | - | - |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | LLM-MQ | - | - | - | - | - | 12.17 | - | - | - |'
- en: '| RTN | 1.9e3 | 781.20 | 68.04 | 15.08 | 4.2e3 | 122.08 | 27.27 | 1.9e3 | 4.6e5
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 1.9e3 | 781.20 | 68.04 | 15.08 | 4.2e3 | 122.08 | 27.27 | 1.9e3 | 4.6e5
    |'
- en: '| AWQ | 2.6e5 | 2.8e5 | 2.4e5 | 7.4e4 | 2.2e5 | 1.2e5 | - | 1.7e6 | 1.7e6 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 2.6e5 | 2.8e5 | 2.4e5 | 7.4e4 | 2.2e5 | 1.2e5 | - | 1.7e6 | 1.7e6 |'
- en: '| GPTQ | 152.31 | 20.44 | 13.01 | 9.51 | 60.45 | 28.14 | 8.78 | 210.00 | 11.90
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 152.31 | 20.44 | 13.01 | 9.51 | 60.45 | 28.14 | 8.78 | 210.00 | 11.90
    |'
- en: '| QuIP | 29.74 | 12.48 | 11.57 | 7.83 | 39.73 | 13.48 | 6.64 | 84.97 | 13.03
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 29.74 | 12.48 | 11.57 | 7.83 | 39.73 | 13.48 | 6.64 | 84.97 | 13.03
    |'
- en: '| PB-LLM | 24.61 | 17.73 | 12.65 | 7.85 | 25.37 | 49.81 | NAN | 44.12 | 11.68
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 24.61 | 17.73 | 12.65 | 7.85 | 25.37 | 49.81 | NAN | 44.12 | 11.68
    |'
- en: '| SliM-LLM | 14.58 | 8.87 | 7.33 | 5.90 | 16.01 | 9.41 | 6.28 | 39.66 | 9.46
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM | 14.58 | 8.87 | 7.33 | 5.90 | 16.01 | 9.41 | 6.28 | 39.66 | 9.46
    |'
- en: 3.4 Implementation Pipeline of SliM-LLM
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 SliM-LLM 的实现流程
- en: 'We integrate our mixed-precision framework into advanced PTQ methods, such
    as GPTQ [[16](#bib.bib16)] and OmniQuant [[38](#bib.bib38)], all of which are
    deployment-friendly with group-wise quantization. We primarily integrate SBA and
    SQC into GPTQ to get SliM-LLM. For SliM-LLM^+, the SBA is plugged into OmniQuant
    with a learnable quantizer. The complete pipeline of SliM-LLM is provided in Algorithm [1](#alg1
    "Algorithm 1 ‣ 3.4 Implementation Pipeline of SliM-LLM ‣ 3 SliM-LLM ‣ SliM-LLM:
    Salience-Driven Mixed-Precision Quantization for Large Language Models"), and
    a more detailed implementation is listed in Appendix [B.1](#A2.SS1 "B.1 Detailed
    Implementation ‣ Appendix B SliM-LLM Implementation ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models").'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将混合精度框架集成到先进的 PTQ 方法中，如 GPTQ [[16](#bib.bib16)] 和 OmniQuant [[38](#bib.bib38)]，这些方法都支持按组量化，便于部署。我们主要将
    SBA 和 SQC 集成到 GPTQ 中，形成 SliM-LLM。对于 SliM-LLM^+，SBA 插入到 OmniQuant 中，并配有可学习的量化器。SliM-LLM
    的完整流程见算法 [1](#alg1 "算法 1 ‣ 3.4 SliM-LLM 的实现流程 ‣ 3 SliM-LLM ‣ SliM-LLM：面向大规模语言模型的显著性驱动混合精度量化")，更详细的实现见附录
    [B.1](#A2.SS1 "B.1 详细实现 ‣ 附录 B SliM-LLM 实现 ‣ SliM-LLM：面向大规模语言模型的显著性驱动混合精度量化")。
- en: Algorithm 1 Main Framework of SliM-LLM.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 SliM-LLM 的主要框架。
- en: func $\operatorname{SliM-LLM}$)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\operatorname{SliM-LLM}$
- en: 'Input: $\boldsymbol{w}\in\mathbb{R}^{n\times m}$ - FP16 weight'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$\boldsymbol{w}\in\mathbb{R}^{n\times m}$ - FP16 权重
- en: $\boldsymbol{x}_{F}\in\mathbb{R}^{t\times m}$ - calibration data
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $\boldsymbol{x}_{F}\in\mathbb{R}^{t\times m}$ - 校准数据
- en: $\beta$ - group size
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: $\beta$ - 组大小
- en: $\lambda$ - hessian regularizer
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: $\lambda$ - Hessian正则化器
- en: $N$ - average bit-width
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: $N$ - 平均比特宽度
- en: 'Output: $\hat{\boldsymbol{w}}_{q}$ - quantized weight'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：$\hat{\boldsymbol{w}}_{q}$ - 量化权重
- en: 1:  $\boldsymbol{H}\coloneqq\frac{1}{P}\sum_{k=1}^{P}\boldsymbol{x}_{F}^{[k]}\boldsymbol{x}_{F}^{[k]T}$
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  $\boldsymbol{H}\coloneqq\frac{1}{P}\sum_{k=1}^{P}\boldsymbol{x}_{F}^{[k]}\boldsymbol{x}_{F}^{[k]T}$'
- en: 4 Experiments
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: We evaluated SliM-LLM and SliM-LLM^+ under weight-only conditions, focusing
    on 2/3-bit precisions. Per-channel group quantization is utilized in our framework
    with 128 set as group size in experiments. Since no back-propagation in SliM-LLM,
    the quantization is carried out on a single NVIDIA A800 GPU. For SliM-LLM^+, we
    employ the AdamW optimizer, following OmniQuant [[38](#bib.bib38)], which is also
    feasible on a single A800\. We randomly select 128 samples from WikiText2 [[30](#bib.bib30)]
    as calibration data, each with 2048 tokens.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在仅权重条件下评估了SliM-LLM和SliM-LLM^+，重点关注2/3-bit精度。在我们的框架中使用了每通道组量化，实验中设置了128为组大小。由于SliM-LLM中没有反向传播，量化是在单个NVIDIA
    A800 GPU上进行的。对于SliM-LLM^+，我们采用了AdamW优化器，参考OmniQuant [[38](#bib.bib38)]，这在单个A800上也是可行的。我们从WikiText2 [[30](#bib.bib30)]中随机选择了128个样本作为校准数据，每个样本包含2048个token。
- en: 'Models and Evaluation. To comprehensively demonstrate the low-bit performance
    advantages of SliM-LLM and SliM-LLM^+, we conduct experiments across OPT [[49](#bib.bib49)],
    LLaMA [[41](#bib.bib41)], LLaMA-2 [[42](#bib.bib42)] and LLaMA-3^([1](#footnote1
    "footnote 1 ‣ 1 Introduction ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models")). We employ the perplexity as our evaluation metric,
    which is widely recognized as a stable measure of language generation capabilities [[16](#bib.bib16),
    [26](#bib.bib26), [20](#bib.bib20), [37](#bib.bib37), [38](#bib.bib38), [6](#bib.bib6),
    [13](#bib.bib13), [21](#bib.bib21)], particularly in compression scenarios. Experiments
    are carried out on the WikiText2 [[30](#bib.bib30)] and C4 [[36](#bib.bib36)]datasets.
    Furthermore, to assess the practical application capabilities of quantized LLMs,
    we also evaluate their accuracy on zero-shot benchmarks, including PIQA [[2](#bib.bib2)],
    ARC [[10](#bib.bib10)], BoolQ [[9](#bib.bib9)], and HellaSwag [[10](#bib.bib10)].'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '模型与评估。为了全面展示SliM-LLM和SliM-LLM^+在低比特性能上的优势，我们在OPT [[49](#bib.bib49)]、LLaMA [[41](#bib.bib41)]、LLaMA-2 [[42](#bib.bib42)]和LLaMA-3^([1](#footnote1
    "footnote 1 ‣ 1 Introduction ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models"))上进行了实验。我们采用困惑度作为评估指标，这被广泛认为是语言生成能力的稳定测量标准 [[16](#bib.bib16)、[26](#bib.bib26)、[20](#bib.bib20)、[37](#bib.bib37)、[38](#bib.bib38)、[6](#bib.bib6)、[13](#bib.bib13)、[21](#bib.bib21)]，特别是在压缩场景中。实验在WikiText2 [[30](#bib.bib30)]和C4 [[36](#bib.bib36)]数据集上进行。此外，为了评估量化LLMs的实际应用能力，我们还在零-shot基准上评估了它们的准确性，包括PIQA [[2](#bib.bib2)]、ARC [[10](#bib.bib10)]、BoolQ [[9](#bib.bib9)]和HellaSwag [[10](#bib.bib10)]。'
- en: Baseline. Since SliM-LLM and SliM-LLM^+ are efficient PTQ approaches without
    additional training or fine-tuning, QAT and re-training methods are not within
    the comparison range of our work. The experiments evaluate existing advanced quantization
    methods and GPU-friendly computations, including vanilla round-to-nearest (RTN),
    GPTQ [[16](#bib.bib16)], AWQ [[26](#bib.bib26)]. And mixed-precision quantization
    techniques, including PB-LLM [[37](#bib.bib37)] ($\frac{1}{7}\times 8$-bit), LLM-MQ [[24](#bib.bib24)],
    and APTQ [[18](#bib.bib18)], as well as the codebook-based method QuIP [[6](#bib.bib6)]
    are also compared in this work. We compare SliM-LLM^+ with gradient optimizer-based
    methods such as OmniQuant [[38](#bib.bib38)] and AffineQuant [[28](#bib.bib28)].
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。由于SliM-LLM和SliM-LLM^+是高效的PTQ方法，不需要额外的训练或微调，因此QAT和再训练方法不在我们工作的比较范围内。实验评估了现有的先进量化方法和GPU友好的计算，包括原始的四舍五入到最近（RTN）、GPTQ [[16](#bib.bib16)]、AWQ [[26](#bib.bib26)]。以及混合精度量化技术，包括PB-LLM [[37](#bib.bib37)]（$\frac{1}{7}\times
    8$-bit）、LLM-MQ [[24](#bib.bib24)]、APTQ [[18](#bib.bib18)]，以及基于代码本的方法QuIP [[6](#bib.bib6)]。我们还将SliM-LLM^+与基于梯度优化器的方法进行比较，例如OmniQuant [[38](#bib.bib38)]和AffineQuant [[28](#bib.bib28)]。
- en: 4.1 Main Results.
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 主要结果。
- en: 'We show experiments within the LLaMA family in this section and detailed results
    for the OPT models are available in Appendix [G](#A7 "Appendix G More Comparisons
    ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models").
    For language generation tasks, as depicted in Tab. [1](#S3.T1 "Table 1 ‣ 3.3.2
    Salience-Weighted Quantizer Calibration for Local Salience Awareness ‣ 3.3 Salience-Weighted
    Quantizer Calibration ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models"), SliM-LLM markedly outperforms its backbone
    GPTQ, particularly under the 2-bit. Specifically, on LLaMA-7B, SliM-LLM achieves
    a 90% decrease in perplexity, while on LLaMA-3-8B, it improves performance by
    81%. In comparison with the unstructured mixed-precision PB-LLM and the codebook-based
    QuIP method, SliM-LLM further reduces the perplexity by 41%~51%. As shown in Tab. [1](#S3.T1
    "Table 1 ‣ 3.3.2 Salience-Weighted Quantizer Calibration for Local Salience Awareness
    ‣ 3.3 Salience-Weighted Quantizer Calibration ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models"), the performance of SliM-LLM^+
    is still ahead compared to OmniQuant and AffineQuant, further proving the effectiveness
    and of the mixed-precision framework proposed in our work. We also provide dialogue
    examples of 2-bit instruction fine-tuning Vicuna-13B [[8](#bib.bib8)] and LLaMA-13B
    in Appeandix [H](#A8 "Appendix H Real Dialog Examples ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models").'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在本节中展示了 LLaMA 系列的实验，OPT 模型的详细结果见附录 [G](#A7 "附录 G 更多比较 ‣ SliM-LLM: 基于显著性的混合精度量化方法用于大型语言模型")。对于语言生成任务，如表 [1](#S3.T1
    "表 1 ‣ 3.3.2 基于显著性的量化器校准以实现局部显著性感知 ‣ 3.3 基于显著性的量化器校准 ‣ 3 SliM-LLM ‣ SliM-LLM:
    基于显著性的混合精度量化方法用于大型语言模型")所示，SliM-LLM 显著优于其基础 GPTQ，特别是在 2-bit 模式下。具体来说，在 LLaMA-7B
    上，SliM-LLM 实现了 90% 的困惑度降低，而在 LLaMA-3-8B 上，其性能提高了 81%。与无结构混合精度 PB-LLM 和基于代码本的 QuIP
    方法相比，SliM-LLM 进一步减少了 41%~51% 的困惑度。如表 [1](#S3.T1 "表 1 ‣ 3.3.2 基于显著性的量化器校准以实现局部显著性感知
    ‣ 3.3 基于显著性的量化器校准 ‣ 3 SliM-LLM ‣ SliM-LLM: 基于显著性的混合精度量化方法用于大型语言模型")所示，SliM-LLM^+
    的表现仍然领先于 OmniQuant 和 AffineQuant，进一步证明了我们工作中提出的混合精度框架的有效性。我们还在附录 [H](#A8 "附录 H
    实际对话示例 ‣ SliM-LLM: 基于显著性的混合精度量化方法用于大型语言模型")中提供了 2-bit 指令微调 Vicuna-13B 和 LLaMA-13B
    的对话示例。'
- en: 'Morever, our method exhibits zero-shot advantages at 2-bit, as shown in Tab. [3](#S4.T3
    "Table 3 ‣ 4.1 Main Results. ‣ 4 Experiments ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models"), where SliM-LLM and SliM-LLM^+ still
    outperforms other methods. For instance, compared with GPTQ and OmniQuant, our
    approach achieves an average improvement of 4.19% and 1.91% on LLaMA-7B. Meanwhile,
    for LLaMA-65B, 2-bit SliM-LLM and SliM-LLM^+ is close to FP16 results (less than
    6% degradaion in accuracy). Overall, our proposed mixed-precision framwork demonstrates
    superior performance across different model sizes, with its advantages becoming
    increasingly significant at lower bit-width.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们的方法在 2-bit 模式下展现了零样本优势，如表 [3](#S4.T3 "表 3 ‣ 4.1 主要结果 ‣ 4 实验 ‣ SliM-LLM:
    基于显著性的混合精度量化方法用于大型语言模型")所示，SliM-LLM 和 SliM-LLM^+ 仍然优于其他方法。例如，与 GPTQ 和 OmniQuant
    相比，我们的方法在 LLaMA-7B 上的平均改进为 4.19% 和 1.91%。与此同时，对于 LLaMA-65B，2-bit SliM-LLM 和 SliM-LLM^+
    的表现接近 FP16 结果（准确率降低不到 6%）。总体而言，我们提出的混合精度框架在不同模型规模下表现优异，尤其在更低比特宽度下其优势愈加显著。'
- en: 'Table 2: Quantization results of LLaMA-1 and LLaMA-2 models with learnable
    quantizer. We report the WikiText2 perplexity in this Table, C4 results are shown
    in Appendix [G](#A7 "Appendix G More Comparisons ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：LLaMA-1 和 LLaMA-2 模型的量化结果，使用了可学习的量化器。我们在此表中报告了 WikiText2 的困惑度，C4 结果见附录 [G](#A7
    "附录 G 更多比较 ‣ SliM-LLM: 基于显著性的混合精度量化方法用于大型语言模型")。'
- en: '| #W PPL$\downarrow$ | Method | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B
    | 2-70B |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| #W PPL$\downarrow$ | 方法 | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
- en: '| 16-bit | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 16-bit | - | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 |'
- en: '| 3-bit | OmniQuant | 6.15 | 5.44 | 4.56 | 3.94 | 6.03 | 5.28 | 3.78 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | OmniQuant | 6.15 | 5.44 | 4.56 | 3.94 | 6.03 | 5.28 | 3.78 |'
- en: '| AffineQuant | 6.14 | 5.45 | 4.59 | - | 6.08 | 5.28 | - |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 6.14 | 5.45 | 4.59 | - | 6.08 | 5.28 | - |'
- en: '| SliM-LLM^+ | 6.07 | 5.37 | 4.34 | 3.72 | 5.94 | 5.11 | 3.35 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM^+ | 6.07 | 5.37 | 4.34 | 3.72 | 5.94 | 5.11 | 3.35 |'
- en: '| 2-bit | OmniQuant | 9.72 | 7.93 | 7.12 | 5.95 | 11.06 | 8.26 | 6.55 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | OmniQuant | 9.72 | 7.93 | 7.12 | 5.95 | 11.06 | 8.26 | 6.55 |'
- en: '| AffineQuant | 13.51 | 7.22 | 6.49 | - | 10.87 | 7.64 | - |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 13.51 | 7.22 | 6.49 | - | 10.87 | 7.64 | - |'
- en: '| SliM-LLM^+ | 9.68 | 7.17 | 6.41 | 5.74 | 10.87 | 7.59 | 6.44 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM^+ | 9.68 | 7.17 | 6.41 | 5.74 | 10.87 | 7.59 | 6.44 |'
- en: 'Table 3: Performance comparisons of different quantization methods for zero-shot
    tasks.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同量化方法在零样本任务中的性能比较。
- en: '| Model / Acc$\uparrow$ | #W | Method | PIQA | ARC-e | ARC-c | BoolQ | HellaSwag
    | Winogrande | Avg. |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 模型 / 准确率$\uparrow$ | #W | 方法 | PIQA | ARC-e | ARC-c | BoolQ | HellaSwag |
    Winogrande | 平均值 |'
- en: '| LLaMA-7B | 16-bit | - | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | 16-bit | - | 77.47 | 52.48 | 41.46 | 73.08 | 73.00 | 67.07 | 64.09
    |'
- en: '| \cdashline2-10 | 2-bit | GPTQ | 55.49 | 31.02 | 22.17 | 53.49 | 33.84 | 41.91
    | 39.65 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-10 | 2-bit | GPTQ | 55.49 | 31.02 | 22.17 | 53.49 | 33.84 | 41.91
    | 39.65 |'
- en: '|  | 2-bit | AWQ | 47.78 | 28.77 | 21.31 | 31.19 | 24.47 | 40.03 | 32.26 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | AWQ | 47.78 | 28.77 | 21.31 | 31.19 | 24.47 | 40.03 | 32.26 |'
- en: '|  | 2-bit | SliM-LLM | 57.83 | 33.46 | 25.09 | 56.05 | 36.70 | 52.64 | 43.84
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | SliM-LLM | 57.83 | 33.46 | 25.09 | 56.05 | 36.70 | 52.64 | 43.84
    |'
- en: '| \cdashline2-10 | 2-bit | OmniQuant | 63.63 | 43.91 | 27.32 | 58.02 | 48.78
    | 52.97 | 49.11 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-10 | 2-bit | OmniQuant | 63.63 | 43.91 | 27.32 | 58.02 | 48.78
    | 52.97 | 49.11 |'
- en: '|  | 2-bit | SliM-LLM^+ | 64.96 | 45.66 | 28.67 | 64.59 | 48.86 | 53.35 | 51.02
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | SliM-LLM^+ | 64.96 | 45.66 | 28.67 | 64.59 | 48.86 | 53.35 | 51.02
    |'
- en: '| LLaMA-13B | 16-bit | - | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 |
    66.33 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-13B | 16-bit | - | 79.10 | 59.89 | 44.45 | 68.01 | 76.21 | 70.31 |
    66.33 |'
- en: '| \cdashline2-10 | 2-bit | GPTQ | 70.37 | 47.74 | 35.88 | 51.57 | 61.39 | 60.84
    | 54.63 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-10 | 2-bit | GPTQ | 70.37 | 47.74 | 35.88 | 51.57 | 61.39 | 60.84
    | 54.63 |'
- en: '|  | 2-bit | AWQ | 49.23 | 30.01 | 29.49 | 30.88 | 26.72 | 46.30 | 35.44 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | AWQ | 49.23 | 30.01 | 29.49 | 30.88 | 26.72 | 46.30 | 35.44 |'
- en: '|  | 2-bit | SliM-LLM | 73.19 | 47.95 | 36.27 | 55.92 | 63.04 | 61.79 | 56.36
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | SliM-LLM | 73.19 | 47.95 | 36.27 | 55.92 | 63.04 | 61.79 | 56.36
    |'
- en: '| \cdashline2-10 | 2-bit | OmniQuant | 73.14 | 49.38 | 36.93 | 63.34 | 62.19
    | 61.77 | 57.64 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-10 | 2-bit | OmniQuant | 73.14 | 49.38 | 36.93 | 63.34 | 62.19
    | 61.77 | 57.64 |'
- en: '|  | 2-bit | SliM-LLM^+ | 74.15 | 50.26 | 37.04 | 64.31 | 63.57 | 63.11 | 58.74
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | SliM-LLM^+ | 74.15 | 50.26 | 37.04 | 64.31 | 63.57 | 63.11 | 58.74
    |'
- en: '| LLaMA-30B | 16-bit | - | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 |
    67.44 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-30B | 16-bit | - | 80.08 | 58.92 | 45.47 | 68.44 | 79.21 | 72.53 |
    67.44 |'
- en: '| \cdashline2-10 | 2-bit | GPTQ | 71.92 | 48.27 | 36.20 | 61.27 | 65.76 | 63.11
    | 57.76 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-10 | 2-bit | GPTQ | 71.92 | 48.27 | 36.20 | 61.27 | 65.76 | 63.11
    | 57.76 |'
- en: '|  | 2-bit | AWQ | 49.17 | 28.56 | 25.97 | 34.73 | 24.97 | 46.99 | 35.07 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | AWQ | 49.17 | 28.56 | 25.97 | 34.73 | 24.97 | 46.99 | 35.07 |'
- en: '|  | 2-bit | SliM-LLM | 75.52 | 51.29 | 39.29 | 62.01 | 66.10 | 64.07 | 59.71
    |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | SliM-LLM | 75.52 | 51.29 | 39.29 | 62.01 | 66.10 | 64.07 | 59.71
    |'
- en: '| \cdashline2-10 | 2-bit | OmniQuant | 76.23 | 53.23 | 39.52 | 63.34 | 65.57
    | 64.82 | 60.22 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-10 | 2-bit | OmniQuant | 76.23 | 53.23 | 39.52 | 63.34 | 65.57
    | 64.82 | 60.22 |'
- en: '|  | 2-bit | SliM-LLM^+ | 76.31 | 54.07 | 39.79 | 63.35 | 67.14 | 64.93 | 60.91
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | SliM-LLM^+ | 76.31 | 54.07 | 39.79 | 63.35 | 67.14 | 64.93 | 60.91
    |'
- en: '| LLaMA-65B | 16-bit | - | 80.79 | 58.71 | 46.24 | 82.29 | 80.72 | 77.50 |
    71.04 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-65B | 16-bit | - | 80.79 | 58.71 | 46.24 | 82.29 | 80.72 | 77.50 |
    71.04 |'
- en: '| \cdashline2-10 | 2-bit | GPTQ | 76.16 | 52.48 | 40.14 | 77.23 | 71.96 | 70.22
    | 64.70 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-10 | 2-bit | GPTQ | 76.16 | 52.48 | 40.14 | 77.23 | 71.96 | 70.22
    | 64.70 |'
- en: '|  | 2-bit | SliM-LLM | 77.09 | 53.72 | 40.25 | 77.51 | 72.05 | 70.91 | 65.26
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | SliM-LLM | 77.09 | 53.72 | 40.25 | 77.51 | 72.05 | 70.91 | 65.26
    |'
- en: '| \cdashline2-10 | 2-bit | OmniQuant | 77.78 | 53.71 | 40.90 | 78.04 | 74.55
    | 68.85 | 65.64 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-10 | 2-bit | OmniQuant | 77.78 | 53.71 | 40.90 | 78.04 | 74.55
    | 68.85 | 65.64 |'
- en: '|  | 2-bit | SliM-LLM^+ | 78.06 | 53.90 | 41.18 | 78.33 | 75.59 | 69.99 | 66.18
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | 2-bit | SliM-LLM^+ | 78.06 | 53.90 | 41.18 | 78.33 | 75.59 | 69.99 | 66.18
    |'
- en: 4.2 Ablation Results.
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 消融结果。
- en: 'We conduct a detailed ablation study to illustrate the benefits of bit-width
    allocation and the impact of each component. Fig. [5](#S4.F5 "Figure 5 ‣ 4.2 Ablation
    Results. ‣ 4 Experiments ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models")(a) compares three strategies for allocating bit-widths
    across groups, including random allocation, head-tail allocation by spatial order,
    and our proposed SBA. When the average bit-width remains constant, random and
    head-tail mixed-precision allocation prove ineffective and even result in performance
    degradation, as shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4.2 Ablation Results. ‣ 4
    Experiments ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models")(a). In contrast, SBA consistently delivers significant improvements
    in post-quantization performance, validating the efficacy of our mixed-precision
    approach. Fig. [5](#S4.F5 "Figure 5 ‣ 4.2 Ablation Results. ‣ 4 Experiments ‣
    SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language Models")(b)
    presents the ablation effects of SBA and SQC, demonstrating that both methods,
    based on the perception of global and local salience, enhance quantization performance.
    SBA is particularly effective in smaller models, and combining these two methods
    can further boost capabilities of LLMs. We also provide the detailed ablation
    results on group size in Appendix [E](#A5 "Appendix E Extension Ablation on Quantization
    Group-Size ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models").'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进行了一项详细的消融研究，以说明位宽分配的好处及各组件的影响。图 [5](#S4.F5 "Figure 5 ‣ 4.2 Ablation Results.
    ‣ 4 Experiments ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models")(a) 比较了三种分配位宽的策略，包括随机分配、按空间顺序的头尾分配和我们提出的 SBA。当平均位宽保持不变时，随机和头尾混合精度分配证明效果不佳，甚至导致性能下降，如图
    [5](#S4.F5 "Figure 5 ‣ 4.2 Ablation Results. ‣ 4 Experiments ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models")(a) 所示。相比之下，SBA 一贯地提供了显著的后量化性能改进，验证了我们混合精度方法的有效性。图
    [5](#S4.F5 "Figure 5 ‣ 4.2 Ablation Results. ‣ 4 Experiments ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models")(b) 展示了 SBA 和 SQC 的消融效果，证明了基于全球和局部显著性感知的这两种方法提高了量化性能。SBA
    在较小模型中尤其有效，结合这两种方法可以进一步提升 LLM 的能力。我们还在附录 [E](#A5 "Appendix E Extension Ablation
    on Quantization Group-Size ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models") 提供了关于组大小的详细消融结果。'
- en: '![Refer to caption](img/22a1729e02131ca9658911ba1ac19333.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/22a1729e02131ca9658911ba1ac19333.png)'
- en: 'Figure 5: Ablation results on OPT models. Random means randomly selecting the
    same number of lower/higher-bit groups; head-tail denotes using the head groups
    as the lower-bit and the same number of tails as the higher-bit on the original
    sequence of group.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：OPT 模型的消融结果。随机意味着随机选择相同数量的低/高位组；head-tail 表示在原始组序列中使用头部组作为低位，尾部组作为高位。
- en: 'Table 4: Deployment results of GPTQ and Slim-LLM on GPU. Group size is set
    to 128.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：GPTQ 和 Slim-LLM 在 GPU 上的部署结果。组大小设置为 128。
- en: '| #W | LLaMA-* | 1-7B | 1-13B | 2-7B |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| #W | LLaMA-* | 1-7B | 1-13B | 2-7B |'
- en: '|  |  | WM | RM | PPL$\downarrow$ | Token/s |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  |  | WM | RM | PPL$\downarrow$ | Token/s |'
- en: '| FP16 | - | 12.6G | 14.4G | 5.68 | 69.2 | 24.3G | 27.1G | 5.09 | 52.5 | 12.7G
    | 14.6G | 5.47 | 69.3 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | 12.6G | 14.4G | 5.68 | 69.2 | 24.3G | 27.1G | 5.09 | 52.5 | 12.7G
    | 14.6G | 5.47 | 69.3 |'
- en: '| 3-bit | GPTQ | 3.2G | 5.1G | 6.55 | 83.4 | 5.8G | 8.7G | 5.62 | 57.6 | 3.2G
    | 5.2G | 6.29 | 56.3 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | GPTQ | 3.2G | 5.1G | 6.55 | 83.4 | 5.8G | 8.7G | 5.62 | 57.6 | 3.2G
    | 5.2G | 6.29 | 56.3 |'
- en: '| SliM-LLM | 3.2G | 5.2G | 6.40 | 79.1 | 5.8G | 8.8G | 5.48 | 48.5 | 3.2G |
    5.4G | 6.26 | 55.9 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM | 3.2G | 5.2G | 6.40 | 79.1 | 5.8G | 8.8G | 5.48 | 48.5 | 3.2G |
    5.4G | 6.26 | 55.9 |'
- en: '| 2-bit | GPTQ | 2.2G | 4.1G | 152.31 | 83.9 | 4.0G | 7.5G | 20.44 | 92.6 |
    2.2G | 4.1G | 60.45 | 83.6 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | GPTQ | 2.2G | 4.1G | 152.31 | 83.9 | 4.0G | 7.5G | 20.44 | 92.6 |
    2.2G | 4.1G | 60.45 | 83.6 |'
- en: '| SliM-LLM | 2.3G | 4.4G | 14.58 | 61.2 | 4.1G | 7.8G | 8.87 | 73.7 | 2.3G
    | 4.1G | 16.01 | 64.4 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM | 2.3G | 4.4G | 14.58 | 61.2 | 4.1G | 7.8G | 8.87 | 73.7 | 2.3G
    | 4.1G | 16.01 | 64.4 |'
- en: 4.3 Efficient Inference on Device
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 设备上的高效推理
- en: 'We utilize the open-source AutoGPTQ to extend CUDA kernel supporting experimental
    mixed-precision inference, with detailed process in Appendix [B.2](#A2.SS2 "B.2
    Mixed Bit Storage and Computing ‣ Appendix B SliM-LLM Implementation ‣ SliM-LLM:
    Salience-Driven Mixed-Precision Quantization for Large Language Models"). We evaluate
    the deployment performance of LLaMA-7/13B and LLaMA-2-7B under 2/3-bit settings.
    The results indicate that our mixed-precision approach maintains a good compression
    rate on GPUs and significantly enhances model accuracy, only with a slight decrease
    in inference speed on the A800 (due to the inference alignment of different bit-width).
    Since current 1-bit operations lack well hardware support, additional consumption
    of storage and computation is required on device. There remains considerable scope
    for optimization in mixed-precision computing, and we aim to further improve this
    in future work.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '我们利用开源的 AutoGPTQ 扩展了支持实验性混合精度推理的 CUDA 内核，详细过程见附录 [B.2](#A2.SS2 "B.2 Mixed Bit
    Storage and Computing ‣ Appendix B SliM-LLM Implementation ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models")。我们评估了 LLaMA-7/13B 和 LLaMA-2-7B
    在 2/3 位设置下的部署性能。结果表明，我们的混合精度方法在 GPU 上保持了良好的压缩率，并显著提高了模型准确性，但 A800 上的推理速度略有下降（由于不同位宽的推理对齐）。由于当前
    1 位操作缺乏良好的硬件支持，设备上需要额外的存储和计算消耗。混合精度计算仍有很大的优化空间，我们旨在未来的工作中进一步改进。'
- en: 5 Conclusion
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this work, we introduce SliM-LLM, a structured mixed-precision PTQ framework
    tailored for LLMs, designed to enhance performance with low-bit weights in a deployment-friendly
    manner. The essence of SliM-LLM lies in employing the Salience-Determined Bit
    Allocation to dynamically allocate bit widths, thereby improving the preservation
    of global salience information. Within groups, the Salience-Weighted Quantizer
    Calibration is designed to enhance local information perception, further minimizing
    the loss associated with locally salient weights. Experiments validate the effectiveness
    of SliM-LLM, showing notable accuracy improvements across various LLMs, and ensuring
    efficiency in inference. In conclusion, SliM-LLM is versatile and can be seamlessly
    integrated with different quantization frameworks and successfully improves the
    performance of LLMs supporting practical deployment in resource-constrained environments.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们引入了 SliM-LLM，这是一种为 LLM 量身定制的结构化混合精度 PTQ 框架，旨在以低位权重的方式提升性能，且适合部署。SliM-LLM
    的核心在于使用显著性决定的位分配动态分配位宽，从而改善全球显著性信息的保留。在组内，显著性加权量化器校准旨在增强局部信息感知，进一步减少与局部显著性权重相关的损失。实验验证了
    SliM-LLM 的有效性，显示出在各种 LLM 上显著的准确性提升，并确保了推理效率。总之，SliM-LLM 具有多功能性，可以与不同的量化框架无缝集成，并成功提升
    LLM 的性能，支持在资源受限的环境中实际部署。
- en: References
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni
    Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.
    GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia
    Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat 等人。GPT-4
    技术报告。arXiv 预印本 arXiv:2303.08774，2023年。'
- en: '[2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning
    about physical commonsense in natural language. In Proceedings of the AAAI conference
    on artificial intelligence, volume 34, pages 7432–7439, 2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等人。Piqa：在自然语言中推理物理常识。载于
    AAAI 人工智能会议论文集，第 34 卷，第 7432–7439 页，2020年。'
- en: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. Advances in Neural Information Processing
    Systems, 33:1877–1901, 2020.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等人。语言模型是少样本学习者。神经信息处理系统进展，第 33 卷，第 1877–1901 页，2020年。'
- en: '[4] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
    Sparks of artificial general intelligence: Early experiments with GPT-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg 等人。人工通用智能的火花：对
    GPT-4 的早期实验。arXiv 预印本 arXiv:2303.12712，2023年。'
- en: '[5] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao
    Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey on evaluation of
    large language models. ACM Transactions on Intelligent Systems and Technology,
    15(3):1–45, 2024.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao
    Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang 等。关于大型语言模型评估的调查。ACM 智能系统与技术期刊，15(3):1–45，2024年。'
- en: '[6] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip:
    2-bit quantization of large language models with guarantees. Advances in Neural
    Information Processing Systems, 36, 2024.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher M De Sa。Quip：具有保证的大型语言模型的2位量化。神经信息处理系统进展，36，2024年。'
- en: '[7] Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding,
    Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu, et al. DB-LLM: Accurate dual-binarization
    for efficient llms. arXiv preprint arXiv:2402.11960, 2024.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Hong Chen, Chengtao Lv, Liang Ding, Haotong Qin, Xiabin Zhou, Yifu Ding,
    Xuebo Liu, Min Zhang, Jinyang Guo, Xianglong Liu 等。DB-LLM：高效 LLM 的准确双二值化。arXiv
    预印本 arXiv:2402.11960，2024年。'
- en: '[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. Vicuna:
    An open-source chatbot impressing GPT-4 with 90%* chatgpt quality. See https://vicuna.
    lmsys. org (accessed 14 April 2023), 2023.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,
    Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez 等。Vicuna：一个开源聊天机器人，以90%*
    ChatGPT质量令人印象深刻。见 https://vicuna.lmsys.org（访问日期：2023年4月14日），2023年。'
- en: '[9] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. BoolQ: Exploring the surprising difficulty of
    natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins 和 Kristina Toutanova。BoolQ：探索自然是/否问题的意外难度。arXiv 预印本 arXiv:1905.10044，2019年。'
- en: '[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick 和 Oyvind Tafjord。认为你已经解决了问答问题？试试 ARC，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457，2018年。'
- en: '[11] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Tim Dettmers, Mike Lewis, Younes Belkada 和 Luke Zettlemoyer。LLM.int8 ():
    用于大规模变换器的8位矩阵乘法。arXiv 预印本 arXiv:2208.07339，2022年。'
- en: '[12] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    SpQR: A sparse-quantized representation for near-lossless LLM weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler 和 Dan Alistarh。SpQR：一种用于近乎无损
    LLM 权重压缩的稀疏量化表示。arXiv 预印本 arXiv:2306.03078，2023年。'
- en: '[13] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem
    Babenko, and Dan Alistarh. Extreme compression of large language models via additive
    quantization. arXiv preprint arXiv:2401.06118, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Vage Egiazarian, Andrei Panferov, Denis Kuznedelev, Elias Frantar, Artem
    Babenko 和 Dan Alistarh。通过加法量化实现大型语言模型的极端压缩。arXiv 预印本 arXiv:2401.06118，2024年。'
- en: '[14] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework
    for accurate post-training quantization and pruning. Advances in Neural Information
    Processing Systems, 35:4475–4488, 2022.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Elias Frantar 和 Dan Alistarh。最佳脑压缩：一种准确的后训练量化和剪枝框架。神经信息处理系统进展，35:4475–4488，2022年。'
- en: '[15] Elias Frantar and Dan Alistarh. SparseGPT: Massive language models can
    be accurately pruned in one-shot. In International Conference on Machine Learning,
    pages 10323–10337\. PMLR, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Elias Frantar 和 Dan Alistarh。SparseGPT：大规模语言模型可以通过一次性剪枝实现准确修剪。在国际机器学习大会上，页面
    10323–10337。PMLR，2023年。'
- en: '[16] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh。GPTQ：生成预训练变换器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323，2022年。'
- en: '[17] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan
    Sajjad, Preslav Nakov, Deming Chen, and Marianne Winslett. Compressing large-scale
    transformer-based models: A case study on bert. Transactions of the Association
    for Computational Linguistics, 9:1061–1080, 2021.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad Ali Khan, Yin Yang, Hassan
    Sajjad, Preslav Nakov, Deming Chen 和 Marianne Winslett。压缩大规模基于变换器的模型：以 BERT 为案例研究。计算语言学协会会刊，9:1061–1080，2021年。'
- en: '[18] Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, and Hao Yu.
    APTQ: Attention-aware post-training mixed-precision quantization for large language
    models. arXiv preprint arXiv:2402.14866, 2024.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Ziyi Guan, Hantao Huang, Yupeng Su, Hong Huang, Ngai Wong, 和 Hao Yu. APTQ：针对大型语言模型的注意力感知后训练混合精度量化。arXiv
    预印本 arXiv:2402.14866, 2024。'
- en: '[19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding.
    arXiv preprint arXiv:2009.03300, 2020.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika,
    Dawn Song, 和 Jacob Steinhardt. 衡量大规模多任务语言理解。arXiv 预印本 arXiv:2009.03300, 2020。'
- en: '[20] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong
    Liu, Michele Magno, and Xiaojuan Qi. BiLLM: Pushing the limit of post-training
    quantization for llms. arXiv preprint arXiv:2402.04291, 2024.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong
    Liu, Michele Magno, 和 Xiaojuan Qi. BiLLM：推动LLM后训练量化的极限。arXiv 预印本 arXiv:2402.04291,
    2024。'
- en: '[21] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen,
    Jie Luo, Xiaojuan Qi, Xianglong Liu, and Michele Magno. How Good Are Low-bit Quantized
    LLaMA3 Models? An Empirical Study. arXiv preprint arXiv:2404.14047, 2024.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Wei Huang, Xudong Ma, Haotong Qin, Xingyu Zheng, Chengtao Lv, Hong Chen,
    Jie Luo, Xiaojuan Qi, Xianglong Liu, 和 Michele Magno. 低比特量化的LLaMA3模型效果如何？一项实证研究。arXiv
    预印本 arXiv:2404.14047, 2024。'
- en: '[22] Aravindh Krishnamoorthy and Deepak Menon. Matrix inversion using cholesky
    decomposition. In 2013 signal processing: Algorithms, architectures, arrangements,
    and applications (SPA), pages 70–72\. IEEE, 2013.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Aravindh Krishnamoorthy 和 Deepak Menon. 使用Cholesky分解进行矩阵求逆。见于 2013年信号处理：算法、架构、安排和应用（SPA），第70–72页。IEEE,
    2013。'
- en: '[23] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park.
    OWQ: Lessons learned from activation outliers for weight quantization in large
    language models. arXiv preprint arXiv:2306.02272, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, 和 Eunhyeok Park. OWQ：从激活异常中学到的权重量化教训。arXiv
    预印本 arXiv:2306.02272, 2023。'
- en: '[24] Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li,
    Kai Zhong, Guohao Dai, Huazhong Yang, and Yu Wang. LLM-MQ: Mixed-precision quantization
    for efficient LLM deployment. In Advances in Neural Information Processing Systems
    (NeurIPS) ENLSP Workshop, 2024.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li,
    Kai Zhong, Guohao Dai, Huazhong Yang, 和 Yu Wang. LLM-MQ：用于高效LLM部署的混合精度量化。见于神经信息处理系统（NeurIPS）ENLSP研讨会,
    2024。'
- en: '[25] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang
    Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality
    vision language models. arXiv preprint arXiv:2403.18814, 2024.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang
    Chu, Shaoteng Liu, 和 Jiaya Jia. Mini-gemini：挖掘多模态视觉语言模型的潜力。arXiv 预印本 arXiv:2403.18814,
    2024。'
- en: '[26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. AWQ: Activation-aware weight quantization for LLM compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, 和 Song Han.
    AWQ：用于LLM压缩和加速的激活感知权重量化。arXiv 预印本 arXiv:2306.00978, 2023。'
- en: '[27] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. LLM-QAT: Data-Free
    Quantization Aware Training for Large Language Models. arXiv preprint arXiv:2305.17888,
    2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra. LLM-QAT：面向大型语言模型的无数据量化感知训练。arXiv
    预印本 arXiv:2305.17888, 2023。'
- en: '[28] Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang,
    Shilei Wen, Fei Chao, and Rongrong Ji. Affinequant: Affine transformation quantization
    for large language models. arXiv preprint arXiv:2403.12544, 2024.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Yuexiao Ma, Huixia Li, Xiawu Zheng, Feng Ling, Xuefeng Xiao, Rui Wang,
    Shilei Wen, Fei Chao, 和 Rongrong Ji. Affinequant：大型语言模型的仿射变换量化。arXiv 预印本 arXiv:2403.12544,
    2024。'
- en: '[29] Donald W Marquardt. An algorithm for least-squares estimation of nonlinear
    parameters. Journal of the society for Industrial and Applied Mathematics, 11(2):431–441,
    1963.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Donald W Marquardt. 非线性参数的最小二乘估计算法。工业与应用数学学会杂志，11(2)：431–441, 1963。'
- en: '[30] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard Socher. 指针哨兵混合模型。arXiv
    预印本 arXiv:1609.07843, 2016。'
- en: '[31] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen
    Blankevoort. Up or down? adaptive rounding for post-training quantization. In
    International Conference on Machine Learning, pages 7197–7206\. PMLR, 2020.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Markus Nagel、Rana Ali Amjad、Mart Van Baalen、Christos Louizos 和 Tijmen
    Blankevoort。向上还是向下？用于后训练量化的自适应舍入。国际机器学习会议，页码 7197–7206。PMLR，2020年。'
- en: '[32] Aniruddha Nrusimha, Mayank Mishra, Naigang Wang, Dan Alistarh, Rameswar
    Panda, and Yoon Kim. Mitigating the impact of outlier channels for language model
    quantization with activation regularization. arXiv preprint arXiv:2404.03605,
    2024.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Aniruddha Nrusimha、Mayank Mishra、Naigang Wang、Dan Alistarh、Rameswar Panda
    和 Yoon Kim。通过激活正则化缓解离群通道对语言模型量化的影响。arXiv 预印本 arXiv:2404.03605，2024年。'
- en: '[33] Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda
    Liu, Jie Luo, Xianglong Liu, and Michele Magno. Accurate LoRA-Finetuning Quantization
    of LLMs via Information Retention. arXiv preprint arXiv:2402.05445, 2024.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Haotong Qin、Xudong Ma、Xingyu Zheng、Xiaoyang Li、Yang Zhang、Shouda Liu、Jie
    Luo、Xianglong Liu 和 Michele Magno。通过信息保留进行准确的 LoRA-Finetuning 量化。arXiv 预印本 arXiv:2402.05445，2024年。'
- en: '[34] Haotong Qin, Mingyuan Zhang, Yifu Ding, Aoyu Li, Zhongang Cai, Ziwei Liu,
    Fisher Yu, and Xianglong Liu. Bibench: Benchmarking and analyzing network binarization.
    arXiv preprint arXiv:2301.11233, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Haotong Qin、Mingyuan Zhang、Yifu Ding、Aoyu Li、Zhongang Cai、Ziwei Liu、Fisher
    Yu 和 Xianglong Liu。Bibench：网络二值化的基准测试与分析。arXiv 预印本 arXiv:2301.11233，2023年。'
- en: '[35] Haotong Qin, Xiangguo Zhang, Ruihao Gong, Yifu Ding, Yi Xu, and Xianglong
    Liu. Distribution-sensitive information retention for accurate binary neural network.
    International Journal of Computer Vision, 131(1):26–47, 2023.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Haotong Qin、Xiangguo Zhang、Ruihao Gong、Yifu Ding、Yi Xu 和 Xianglong Liu。针对准确二值神经网络的分布敏感信息保留。《计算机视觉国际杂志》，131(1)：26–47，2023年。'
- en: '[36] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. The Journal of Machine Learning
    Research, 21(1):5485–5551, 2020.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。探索统一文本到文本转换器的迁移学习极限。《机器学习研究杂志》，21(1)：5485–5551，2020年。'
- en: '[37] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. PB-LLM: Partially
    binarized large language models. arXiv preprint arXiv:2310.00034, 2023.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Yuzhang Shang、Zhihang Yuan、Qiang Wu 和 Zhen Dong。PB-LLM：部分二值化的大型语言模型。arXiv
    预印本 arXiv:2310.00034，2023年。'
- en: '[38] Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. arXiv preprint arXiv:2308.13137,
    2023.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Wenqi Shao、Mengzhao Chen、Zhaoyang Zhang、Peng Xu、Lirui Zhao、Zhiqian Li、Kaipeng
    Zhang、Peng Gao、Yu Qiao 和 Ping Luo。Omniquant：面向大型语言模型的全方向标定量化。arXiv 预印本 arXiv:2308.13137，2023年。'
- en: '[39] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple and effective
    pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Mingjie Sun、Zhuang Liu、Anna Bair 和 J Zico Kolter。针对大型语言模型的简单有效的剪枝方法。arXiv
    预印本 arXiv:2306.11695，2023年。'
- en: '[40] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al.
    Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805,
    2023.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Gemini 团队、Rohan Anil、Sebastian Borgeaud、Yonghui Wu、Jean-Baptiste Alayrac、Jiahui
    Yu、Radu Soricut、Johan Schalkwyk、Andrew M Dai、Anja Hauth 等。Gemini：一系列高能力的多模态模型。arXiv
    预印本 arXiv:2312.11805，2023年。'
- en: '[41] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama：开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971，2023年。'
- en: '[42] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi,
    Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
    et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,
    2023.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Hugo Touvron、Louis Martin、Kevin Stone、Peter Albert、Amjad Almahairi、Yasmine
    Babaei、Nikolay Bashlykov、Soumya Batra、Prajjwal Bhargava、Shruti Bhosale 等。Llama
    2：开放的基础和微调聊天模型。arXiv 预印本 arXiv:2307.09288，2023年。'
- en: '[43] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher
    De Sa. Quip#: Even better LLM quantization with hadamard incoherence and lattice
    codebooks. arXiv preprint arXiv:2402.04396, 2024.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Albert Tseng、Jerry Chee、Qingyao Sun、Volodymyr Kuleshov 和 Christopher De
    Sa。Quip#：通过 Hadamard 不一致性和格码本实现更好的 LLM 量化。arXiv 预印本 arXiv:2402.04396，2024年。'
- en: '[44] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, 和 Song
    Han. SmoothQuant: 大型语言模型的准确且高效的后训练量化。在国际机器学习会议上，页码 38087–38099\. PMLR, 2023。'
- en: '[45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient
    streaming language models with attention sinks. arXiv preprint arXiv:2309.17453,
    2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, 和 Mike Lewis. 高效的流式语言模型与注意力消耗。arXiv
    预印本 arXiv:2309.17453, 2023。'
- en: '[46] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, 和 Qi Tian. QA-LoRA: 量化感知的大型语言模型低秩适应。arXiv 预印本 arXiv:2309.14717,
    2023。'
- en: '[47] Z Yao, RY Aminabadi, M Zhang, X Wu, C Li, and Y Zeroquant He. Efficient
    and affordable post-training quantization for large-scale transformers. URL https://arxiv.
    org/abs/2206.01861, 2022.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Z Yao, RY Aminabadi, M Zhang, X Wu, C Li, 和 Y Zeroquant He. 大规模变换器的高效且经济的后训练量化。网址
    https://arxiv.org/abs/2206.01861, 2022。'
- en: '[48] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,
    2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, 和 Yejin Choi.
    Hellaswag: 机器真的可以完成你的句子吗？arXiv 预印本 arXiv:1905.07830, 2019。'
- en: '[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等. OPT: 开放的预训练变换器语言模型。arXiv
    预印本 arXiv:2205.01068, 2022。'
- en: '[50] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli
    Ouyang, and Xiangyu Yue. Meta-transformer: A unified framework for multimodal
    learning. arXiv preprint arXiv:2307.10802, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli
    Ouyang, 和 Xiangyu Yue. Meta-transformer: 一种用于多模态学习的统一框架。arXiv 预印本 arXiv:2307.10802,
    2023。'
- en: '[51] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng
    Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. A survey of
    large language models. arXiv preprint arXiv:2303.18223, 2023.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng
    Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong 等. 大型语言模型综述。arXiv 预印本
    arXiv:2303.18223, 2023。'
- en: '[52] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A survey on model
    compression for large language models. arXiv preprint arXiv:2308.07633, 2023.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, 和 Weiping Wang. 大型语言模型的模型压缩综述。arXiv
    预印本 arXiv:2308.07633, 2023。'
- en: Appendix A Broader Impacts and Limitations
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 更广泛的影响和限制
- en: A.1 Broader Impacts
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 更广泛的影响
- en: This paper introduces a mixed-precision technique to achieve accurate and efficient
    low-bit weight quantization for large language models (LLMs). This approach makes
    LLMs more efficient and accessible, potentially extending their pervasive impact.
    From a positive perspective, quantization makes the use of LLMs easier, benefiting
    a broader audience, particularly those in lower-income groups. It reduces the
    cost and hardware barriers to deploying LLMs and promotes edge inference of these
    models (mitigating the risk of privacy data breaches), contributing to societal
    productivity. On the downside, LLMs could be exploited by malicious users to generate
    and spread false information. Quantization does not prevent the inherent negative
    impacts of LLMs, nor does it exacerbate them.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了一种混合精度技术，以实现对大型语言模型（LLMs）的准确且高效的低比特权重量化。这种方法使得LLMs更高效、更易于获得，有可能扩大其普遍影响。从积极的角度来看，量化使得LLMs的使用变得更加便捷，惠及更广泛的受众，特别是那些低收入群体。它降低了部署LLMs的成本和硬件门槛，并促进了这些模型的边缘推理（减少了隐私数据泄露的风险），有助于社会生产力。然而，LLMs可能被恶意用户利用来生成和传播虚假信息。量化并不能防止LLMs固有的负面影响，也没有加剧这些影响。
- en: A.2 Limitations
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 限制
- en: Though the mixed-precision framework significantly improves the quantization
    performance of LLMs, the current out-of-the-box deployment tools still cannot
    well support efficient mixed-precision computing. Meanwhile, the support for 1/2/3-bit
    inference on GPUs remains limited, which affects the inferencing advantages of
    low-bit models. We believe there is significant room for improvement in the hardware
    efficiency of mixed-precision LLMs in the future.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管混合精度框架显著提高了 LLM 的量化性能，但目前现成的部署工具仍无法很好地支持高效的混合精度计算。同时，对 GPU 上 1/2/3 位推理的支持仍然有限，这影响了低位模型的推理优势。我们认为，未来在混合精度
    LLM 的硬件效率方面还有很大的改进空间。
- en: A.3 Experiments Reproducibility
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 实验可重复性
- en: Our code is included in the supplementary materials. For instructions on how
    to reproduce various experiments, please refer to the accompanying code scripts
    and algorithm description in our paper. We also provide the download and use details
    of the datasets mentioned in the experiment part.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码包含在补充材料中。有关如何重复各种实验的说明，请参阅我们论文中的附带代码脚本和算法描述。我们还提供了实验部分提到的数据集的下载和使用详情。
- en: Appendix B SliM-LLM Implementation
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B SliM-LLM 实现
- en: B.1 Detailed Implementation
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 详细实现
- en: In this section, we present the specific implementation details of SliM-LLM,
    which utilizes GPTQ [[16](#bib.bib16)] as its backbone for mixed-precision quantization
    and incorporates both SBA and SQC. SliM-LLM^+ is consistent with SliM-LLM in SBA
    computations but does not include the SQC component, instead retaining learnable
    weight clipping (LWC) approach in OmniQuant [[38](#bib.bib38)] for gradient optimization.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们展示了 SliM-LLM 的具体实现细节，该方法利用 GPTQ [[16](#bib.bib16)] 作为其混合精度量化的基础，并结合了
    SBA 和 SQC。SliM-LLM^+ 在 SBA 计算上与 SliM-LLM 一致，但不包括 SQC 组件，而是保留了 OmniQuant [[38](#bib.bib38)]
    中的可学习权重剪裁 (LWC) 方法以进行梯度优化。
- en: Algorithm 2 Detailed functions in SliM-LLM.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 SliM-LLM 中的详细函数。
- en: func $\operatorname{SBA}(\boldsymbol{w},\boldsymbol{x}_{F},\boldsymbol{H}^{\mathrm{in}},\beta,N)$
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: func $\operatorname{SBA}(\boldsymbol{w},\boldsymbol{x}_{F},\boldsymbol{H}^{\mathrm{in}},\beta,N)$
- en: 1:  $\mathcal{G}\{\cdot\}\coloneqq\{0\}$
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  $\mathcal{G}\{\cdot\}\coloneqq\{0\}$
- en: func $\operatorname{SQC}(\boldsymbol{w}^{b}_{s},\boldsymbol{w}^{b}_{us},g_{b})$
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: func $\operatorname{SQC}(\boldsymbol{w}^{b}_{s},\boldsymbol{w}^{b}_{us},g_{b})$
- en: 1:  $w_{\mathrm{max}}\coloneqq\operatorname{max}(\boldsymbol{w}^{b}_{s}\cup\boldsymbol{w}^{b}_{us})$
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  $w_{\mathrm{max}}\coloneqq\operatorname{max}(\boldsymbol{w}^{b}_{s}\cup\boldsymbol{w}^{b}_{us})$
- en: 'Algorithm [2](#alg2 "Algorithm 2 ‣ B.1 Detailed Implementation ‣ Appendix B
    SliM-LLM Implementation ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models") primarily encompasses the core details of both SBA
    and SQC. In SBA, the importance of each group is determined by sorting the average
    salience of groups, followed by a bi-pointer search that increases the number
    of ($N-1$ function are omitted, the default values from Eq. ([1](#S3.E1 "In 3.1
    Preliminaries ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models")) are used.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 [2](#alg2 "算法 2 ‣ B.1 详细实现 ‣ 附录 B SliM-LLM 实现 ‣ SliM-LLM: 以显著性为驱动的大型语言模型混合精度量化")
    主要涵盖了 SBA 和 SQC 的核心细节。在 SBA 中，每组的重要性通过排序组的平均显著性来确定，随后通过双指针搜索来增加（$N-1$ 函数被省略，使用
    Eq. ([1](#S3.E1 "在 3.1 基础知识 ‣ 3 SliM-LLM ‣ SliM-LLM: 以显著性为驱动的大型语言模型混合精度量化")）中的默认值）。'
- en: B.2 Mixed Bit Storage and Computing
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2 混合位存储与计算
- en: We developed a framework for storage and inference deployment supporting mixed-precision
    quantization based on AutoGPTQ. The deployment process is as follows. After completing
    mixed-precision quantization with SliM-LLM, it outputs scales, zeros, and group-wise
    bit-width generated during the quantization process to identify the quantization
    parameters and precision of each group in the Linear Projection weights. AutoGPTQ
    then packs the weights and zeros into integer-compressed representations (denoted
    by $\hat{\boldsymbol{w}}_{\mathrm{int}}$ respectively) based on the precision
    of different groups, significantly reducing storage and operational bit-width.
    After the quantized weights are packed, AutoGPTQ loads the model onto the GPU,
    where the mixed precision quantization kernel on the GPU performs dequantization
    on the weights and zeros of different groups and calculation with input activation,
    ultimately producing the final output.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开发了一个支持基于AutoGPTQ的混合精度量化的存储和推理部署框架。部署过程如下。在完成SliM-LLM的混合精度量化后，它会输出量化过程中生成的缩放因子、零值和按组划分的位宽，以识别线性投影权重中每组的量化参数和精度。然后，AutoGPTQ根据不同组的精度将权重和零值打包为整数压缩表示（分别用$\hat{\boldsymbol{w}}_{\mathrm{int}}$表示），显著减少存储和操作的位宽。在量化权重打包后，AutoGPTQ将模型加载到GPU上，在GPU上运行的混合精度量化内核对不同组的权重和零值进行去量化处理，并与输入激活进行计算，最终生成最终输出。
- en: 'In the mixed-precision deployment of AutoGPTQ, the weight memory layout is
    organized by group, with each group sharing the same precision, which is shown
    in Fig. [6](#A2.F6 "Figure 6 ‣ B.2 Mixed Bit Storage and Computing ‣ Appendix
    B SliM-LLM Implementation ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models"). Within each group, elements with the same precision
    are packed as integers, eliminating the need for additional padding, which saves
    space. Given that the bit-width of integers is a power of 2, this is compatible
    with group size that is also a power of 2\. For instance, even with the odd-bit
    such as 3-bit storage, integers can store these numbers without padding, as the
    commonly used group size is 128, a multiple of almost all definition of integer
    type. This ensures that elements within a group fully utilize the space provided
    by integers, without storing numbers of different precision within the same integer.
    $\hat{\boldsymbol{z}}_{\mathrm{int}}$ follow the original logic of AutoGPTQ but
    are packed with a uniform precision along the channel direction for ease of use.
    Other tensors, like scales, remain in the same floating-point format to ensure
    the correctness of dequantization calculations.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '在AutoGPTQ的混合精度部署中，权重内存布局按组组织，每组共享相同的精度，如图[6](#A2.F6 "Figure 6 ‣ B.2 Mixed Bit
    Storage and Computing ‣ Appendix B SliM-LLM Implementation ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models")所示。在每组内，相同精度的元素被打包为整数，消除了额外填充的需要，从而节省了空间。由于整数的位宽是2的幂，这与组大小也是2的幂兼容。例如，即使在像3位存储这样的奇数位情况下，整数也可以在不进行填充的情况下存储这些数字，因为常用的组大小是128，这几乎是所有整数类型定义的倍数。这确保了组内的元素充分利用了整数提供的空间，而不会在同一整数中存储不同精度的数字。$\hat{\boldsymbol{z}}_{\mathrm{int}}$遵循AutoGPTQ的原始逻辑，但在通道方向上以统一精度打包，方便使用。其他张量，如缩放因子，仍保持相同的浮点格式，以确保去量化计算的正确性。'
- en: To indicate the precision of each group, we also introduce an additional array
    to store bit-width of each group, where each number is represented as a 2-bit
    value aggregated into integers, marking the quantization precision of each group
    for accurate reconstruction. We use cumulative calculations to determine the starting
    index of each group, ensuring correctness despite changes in $\hat{\boldsymbol{w}}_{\mathrm{int}}$
    height and starting indices caused by varying precision. Using the above methods
    to store the quantized weights, zeros, and additional bit arrays effectively reduces
    memory usage during model storage and loading, thereby lowering the resource overhead
    required for model deployment.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示每组的精度，我们还引入了一个额外的数组来存储每组的位宽，其中每个数字以2位值表示，聚合到整数中，标记每组的量化精度以便于准确重建。我们使用累积计算来确定每组的起始索引，确保尽管$\hat{\boldsymbol{w}}_{\mathrm{int}}$的高度和起始索引因精度变化而发生变化，但仍能保持正确性。使用上述方法存储量化权重、零值和附加位数组，能够有效减少模型存储和加载期间的内存使用，从而降低模型部署所需的资源开销。
- en: Once the weights are packed, we follow the modified AutoGPTQ logic for GPU inference.
    The GPU processes and dequantizes the weights group by group for computation.
    During GPU computation, a thread dequantizes a segment of continuous memory data
    in one column of $\hat{\boldsymbol{w}}_{\mathrm{int}}$ and performs vector dot
    product calculations with the input activation shared within the block, accumulating
    the results in the corresponding result matrix. When threads form a logical block,
    the block handles the computation and reduction of a continuous channel region.
    We complete the linear layer computation by iterating through all logical blocks.
    Leveraging AutoGPTQ’s initial logic and CUDA Warp’s 32-thread units, we ensure
    similar code structure and data access logic for threads within each warp when
    group size is 128\. This method was primarily conducted to validate feasibility
    os SliM-LLM, demonstrating that the mixed precision quantization with integer
    packing does not cause additional computational overhead, indicating the efficiency
    and accuracy advantage of SliM-LLM. In summary, by dividing weight into several
    structured precision blocks and employing a reasonable GPU utilization strategy,
    Slim-LLM balances performance and efficiency.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦权重被打包，我们会遵循修改后的 AutoGPTQ 逻辑进行 GPU 推理。GPU 会逐组处理并解量化权重。在 GPU 计算过程中，一个线程会解量化
    $\hat{\boldsymbol{w}}_{\mathrm{int}}$ 一列中的一段连续内存数据，并与块内共享的输入激活执行向量点积计算，将结果累积到相应的结果矩阵中。当线程形成逻辑块时，该块负责计算和减少一个连续通道区域的结果。通过迭代所有逻辑块来完成线性层计算。利用
    AutoGPTQ 的初始逻辑和 CUDA Warp 的 32 线程单元，我们确保在组大小为 128 时，每个 warp 内的线程具有类似的代码结构和数据访问逻辑。此方法主要用于验证
    SliM-LLM 的可行性，表明混合精度量化与整数打包不会导致额外的计算开销，显示出 SliM-LLM 的效率和准确性优势。总之，通过将权重分成几个结构化的精度块并采用合理的
    GPU 利用策略，Slim-LLM 实现了性能和效率的平衡。
- en: '![Refer to caption](img/a79ac1fc5d259883f3d547ac88abe0dd.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a79ac1fc5d259883f3d547ac88abe0dd.png)'
- en: 'Figure 6: The memory layout shown in the figure is modified based on AutoGPTQ.
    The transposed original weights $\boldsymbol{w}^{\top}\in\mathbb{R}^{m\times n}$
    is also packed into integers to save memory.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：图中显示的内存布局基于 AutoGPTQ 进行了修改。转置后的原始权重 $\boldsymbol{w}^{\top}\in\mathbb{R}^{m\times
    n}$ 也被打包成整数以节省内存。
- en: Appendix C Searching Details of Group-Wise Salience-Determined Bit Allocation
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 组显著性决定的比特分配搜索详情
- en: 'We optimize the mixed-precision configuration based on the output information
    entropy (KL-divergence), searching for the optimal compensation bit-width ratio
    as shown in Eq. ([4](#S3.E4 "In 3.2.2 Salience-Determined Bit Allocation for Structured
    Group ‣ 3.2 Salience-Determined Bit Allocation ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models")).'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于输出信息熵（KL 散度）优化混合精度配置，搜索最佳补偿位宽比，如 Eq. ([4](#S3.E4 "在 3.2.2 结构化组的显著性决定比特分配
    ‣ 3.2 显著性决定比特分配 ‣ 3 SliM-LLM ‣ SliM-LLM：用于大型语言模型的显著性驱动混合精度量化")) 所示。
- en: 'Initially, we rank each group by their average salience, a metric for quantization,
    and employ a double-pointer that moves simultaneously from both the beginning
    (lowest salience) and end (highest salience) of the sorted list. This ensures
    an equal number of groups at low and high bit-widths, effectively balancing the
    global average bit-width compensation. We then calculate the relative entropy
    under the corresponding precision ratio and search for the optimal ratio. Fig [7](#A3.F7
    "Figure 7 ‣ Appendix C Searching Details of Group-Wise Salience-Determined Bit
    Allocation ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models") displays the search error curves related to the $2^{nd}$ Transformer
    layers in the OPT1.3B model, showcasing the search curves for certain self-attention
    layers (Query, Key, Value, FC2).'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们按平均显著性对每个组进行排序，显著性是量化的一个指标，并使用一个双指针同时从排序列表的开头（最低显著性）和末尾（最高显著性）移动。这确保了低位宽和高位宽组的数量相等，有效平衡了全局平均位宽补偿。然后，我们计算在相应精度比下的相对熵，并搜索最佳比率。图
    [7](#A3.F7 "图 7 ‣ 附录 C 组显著性决定的比特分配搜索 ‣ SliM-LLM：用于大型语言模型的显著性驱动混合精度量化") 显示了与 OPT1.3B
    模型中的第 $2$ 层 Transformer 相关的搜索误差曲线，展示了某些自注意力层（查询、键、值、FC2）的搜索曲线。
- en: 'Due to the limited range of the search, extreme scenarios involve either a
    half ($N-1$-bit (uniform precision). Fig [7](#A3.F7 "Figure 7 ‣ Appendix C Searching
    Details of Group-Wise Salience-Determined Bit Allocation ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models") demonstrates that lower
    quantization errors can be achieved under mixed-precision compared to quantization
    at the uniform bit-width. We also find that multiple low-error precision combinations
    are possible within a group of weights, allowing SBA to flexibly select the optimal
    ratio through its versatile search.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '由于搜索范围有限，极端情况涉及到半（$N-1$-bit（均匀精度））。图[7](#A3.F7 "Figure 7 ‣ Appendix C Searching
    Details of Group-Wise Salience-Determined Bit Allocation ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models")展示了在混合精度下可以实现比均匀比特宽度量化更低的量化误差。我们还发现，在一组权重中存在多种低误差精度组合，允许SBA通过其灵活的搜索来选择最佳比例。'
- en: '![Refer to caption](img/ef9e1a6f33f8a3e4e0f66fecb06dc514.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ef9e1a6f33f8a3e4e0f66fecb06dc514.png)'
- en: 'Figure 7: Error curves of SBA for select weights in the $5^{th}$ layers of
    OPT-1.3B.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：OPT-1.3B模型第$5^{th}$层选定权重的误差曲线。
- en: Appendix D Extension Ablation on SQC
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D SQC扩展消融
- en: 'In this section, we visualize the effectiveness of SQC in mitigating the degradation
    of information in locally salient weights. We observed the absolute error of weights
    in a randomly selected channel of the quantized OPT-1.3B model. As shown in Fig. [8](#A4.F8
    "Figure 8 ‣ Appendix D Extension Ablation on SQC ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models"), the overall absolute error of the weights
    post-quantization with a standard quantizer was 0.0055, while with SQC it was
    reduced to 0.0039\. This further demonstrates that the search parameter $\gamma$,
    as applied in Eq. ([5](#S3.E5 "In 3.3.2 Salience-Weighted Quantizer Calibration
    for Local Salience Awareness ‣ 3.3 Salience-Weighted Quantizer Calibration ‣ 3
    SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language
    Models")), effectively optimizes the quantizer parameters, thereby reducing quantization
    errors.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们通过可视化方法展示了SQC在缓解局部显著性权重信息退化方面的有效性。我们观察了量化OPT-1.3B模型中一个随机选择的通道的权重绝对误差。如图[8](#A4.F8
    "Figure 8 ‣ Appendix D Extension Ablation on SQC ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models")所示，标准量化器下的权重量化后整体绝对误差为0.0055，而使用SQC后降至0.0039。这进一步证明了在公式([5](#S3.E5
    "In 3.3.2 Salience-Weighted Quantizer Calibration for Local Salience Awareness
    ‣ 3.3 Salience-Weighted Quantizer Calibration ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models"))中应用的搜索参数$\gamma$，有效地优化了量化器参数，从而减少了量化误差。'
- en: 'More importantly, SQC effectively perceives the information of locally salient
    weights, as indicated by the red regions in Fig. [8](#A4.F8 "Figure 8 ‣ Appendix
    D Extension Ablation on SQC ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models"). Compared to the vanilla quantizer, SQC significantly
    reduces the error of salient weights. Specifically, the prominent weights at indices
    375 in Fig. [8](#A4.F8 "Figure 8 ‣ Appendix D Extension Ablation on SQC ‣ SliM-LLM:
    Salience-Driven Mixed-Precision Quantization for Large Language Models")(a) show
    higher quantization errors, while in Fig. [8](#A4.F8 "Figure 8 ‣ Appendix D Extension
    Ablation on SQC ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large
    Language Models")(b), this error is effectively reduced. This confirms SQC’s ability
    to perceive locally salient weights, effectively preventing the degradation of
    critical information.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '更重要的是，SQC有效地感知了局部显著性权重的信息，如图[8](#A4.F8 "Figure 8 ‣ Appendix D Extension Ablation
    on SQC ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for Large Language
    Models")中的红色区域所示。与普通量化器相比，SQC显著减少了显著权重的误差。具体来说，图[8](#A4.F8 "Figure 8 ‣ Appendix
    D Extension Ablation on SQC ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models")(a)中的375位置的显著权重显示出较高的量化误差，而在图[8](#A4.F8 "Figure 8 ‣
    Appendix D Extension Ablation on SQC ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models")(b)中，这种误差得到了有效降低。这证实了SQC感知局部显著性权重的能力，有效防止了关键信息的退化。'
- en: '![Refer to caption](img/e729911f2e1fa2b88088091db521bde2.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e729911f2e1fa2b88088091db521bde2.png)'
- en: 'Figure 8: Absolute channel error of the weight of the OPT-1.3B model. The red
    line represents the quantization error for the locally salient weights, and the
    gray represents other weights. (a) Vanilla quantizer error on the $794^{th}$ channel
    of OPT-1.3B'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：OPT-1.3B模型权重的绝对通道误差。红线代表局部显著性权重的量化误差，灰线代表其他权重。 (a) OPT-1.3B模型第$794^{th}$通道的普通量化器误差
- en: Appendix E Extension Ablation on Quantization Group-Size
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 量化组大小的扩展消融
- en: To investigate the impact of different group sizes on the quantization effectiveness
    of SliM-LLM, we evaluated performance with 256 and 512 columns at a 3-bit level,
    observing that larger group sizes enhance GPU efficiency during inference. The
    findings suggest that increased group granularity does not substantially elevate
    perplexity across four models, indicating that SliM-LLM is robust and conducive
    to more efficient deployment methods. In contrast, at 2-bit, we assessed group
    sizes of 64 and 32 columns. With finer group granularity, the models displayed
    reduced perplexity. This is attributed to smaller groups providing more detailed
    data representation and utilizing additional quantization parameters, although
    they also raise computational and storage demands. A group size of 128 strikes
    a better balance between efficiency and quantization performance.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了研究不同组大小对 SliM-LLM 量化效果的影响，我们在 3-bit 级别上评估了 256 和 512 列的性能，观察到较大的组大小在推理过程中提升了
    GPU 效率。研究结果表明，增加组的粒度不会显著提高四个模型的困惑度，这表明 SliM-LLM 是稳健的，并且有助于更高效的部署方法。相比之下，在 2-bit
    级别，我们评估了 64 和 32 列的组大小。较细的组粒度使模型显示出较低的困惑度。这归因于较小的组提供了更详细的数据表示，并利用了额外的量化参数，尽管这也增加了计算和存储需求。128
    的组大小在效率和量化性能之间达到了更好的平衡。
- en: 'Table 5: Ablation results on OPT-6.7B, LLaMA-7B, LLaMA-2-7B, LLaMA-3-8B with
    SliM-LLM under different group size (#g denotes the group size).'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '表 5: 在不同组大小下，SliM-LLM 对 OPT-6.7B、LLaMA-7B、LLaMA-2-7B、LLaMA-3-8B 的消融实验结果 (#g
    表示组大小)。'
- en: '| Precision / PPL$\downarrow$ | #g | OPT-6.7B | LLaMA-7B | LLaMA-2-7B | LLaMA-3-8B
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 精度 / PPL$\downarrow$ | #g | OPT-6.7B | LLaMA-7B | LLaMA-2-7B | LLaMA-3-8B
    |'
- en: '| 3-bit | 512 | 11.65 | 6.96 | 6.69 | 8.87 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | 512 | 11.65 | 6.96 | 6.69 | 8.87 |'
- en: '| 256 | 11.33 | 6.92 | 6.94 | 8.14 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 256 | 11.33 | 6.92 | 6.94 | 8.14 |'
- en: '| 128 | 11.27 | 6.40 | 6.24 | 7.62 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 11.27 | 6.40 | 6.24 | 7.62 |'
- en: '| 2-bit | 128 | 14.41 | 14.58 | 16.01 | 39.66 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | 128 | 14.41 | 14.58 | 16.01 | 39.66 |'
- en: '| 64 | 13.95 | 13.41 | 15.02 | 29.84 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 13.95 | 13.41 | 15.02 | 29.84 |'
- en: '| 32 | 12.47 | 11.91 | 11.95 | 16.93 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 12.47 | 11.91 | 11.95 | 16.93 |'
- en: Appendix F Extension on Salience Channel Clustering
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 显著性通道聚类的扩展
- en: F.1 Discussion of Theorem 1
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.1 定理 1 的讨论
- en: Theorem 1.
  id: totrans-257
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定理 1。
- en: Given the input calibration activation $\boldsymbol{x}\in\mathbb{R}^{t\times
    m}$.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入校准激活 $\boldsymbol{x}\in\mathbb{R}^{t\times m}$。
- en: Proof.
  id: totrans-259
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: 'Given $\boldsymbol{x}\in\mathbb{R}^{t\times m}$. We can get the Hessian matrix
    with Levenberg-Marquardt [[29](#bib.bib29)] approximation in Eq. ([3](#S3.E3 "In
    3.1 Preliminaries ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models")):'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '给定 $\boldsymbol{x}\in\mathbb{R}^{t\times m}$。我们可以通过 Levenberg-Marquardt [[29](#bib.bib29)]
    近似在 Eq. ([3](#S3.E3 "在 3.1 前言 ‣ 3 SliM-LLM ‣ SliM-LLM: 面向大语言模型的显著性驱动混合精度量化"))中得到
    Hessian 矩阵：'
- en: '|  | $$\begin{pmatrix}x_{11}&amp;x_{12}&amp;x_{13}&amp;\cdots&amp;x_{1m}\\
    x_{21}&amp;x_{22}&amp;x_{23}&amp;\cdots&amp;x_{2m}\\'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\begin{pmatrix}x_{11}&x_{12}&x_{13}&\cdots&x_{1m}\\ x_{21}&x_{22}&x_{23}&\cdots&x_{2m}\\'
- en: \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&\vdots&\vdots&\ddots&\vdots\\
- en: \vdots&amp;\vdots&amp;\vdots&amp;\boldsymbol{x_{p,q}^{*}}&amp;\vdots\\
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&\vdots&\vdots&\boldsymbol{x_{p,q}^{*}}&\vdots\\
- en: \vdots&amp;\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&\vdots&\vdots&\ddots&\vdots\\
- en: x_{t1}&amp;x_{t2}&amp;x_{t3}&amp;\cdots&amp;x_{tm}\\
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: x_{t1}&x_{t2}&x_{t3}&\cdots&x_{tm}\\
- en: \end{pmatrix}\cdot\begin{pmatrix}x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1t}\\
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}\cdot\begin{pmatrix}x_{11}&x_{12}&\cdots&x_{1t}\\
- en: x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2t}\\
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: x_{21}&x_{22}&\cdots&x_{2t}\\
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&\vdots&\ddots&\vdots\\
- en: \vdots&amp;\vdots&amp;\boldsymbol{x_{p,q}^{*}}&amp;\vdots\\
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&\vdots&\boldsymbol{x_{p,q}^{*}}&\vdots\\
- en: \vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&\vdots&\ddots&\vdots\\
- en: x_{m1}&amp;x_{m2}&amp;\cdots&amp;x_{mt}\\
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: x_{m1}&x_{m2}&\cdots&x_{mt}\\
- en: \end{pmatrix}=\begin{pmatrix}x_{11}^{2}..&amp;\cdots&amp;\cdots&amp;\cdots\\
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}=\begin{pmatrix}x_{11}^{2}..&\cdots&\cdots&\cdots\\
- en: \vdots&amp;\ddots&amp;\cdots&amp;\vdots\\
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&\ddots&\cdots&\vdots\\
- en: \vdots&amp;\vdots&amp;\boldsymbol{x_{p,q}^{*}}^{2}..&amp;\vdots\\
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: \vdots&\vdots&\boldsymbol{x_{p,q}^{*}}^{2}..&\vdots\\
- en: \cdots&amp;\cdots&amp;\cdots&amp;\ddots\\
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: \cdots&\cdots&\cdots&\ddots\\
- en: \end{pmatrix}$$ |  | (7) |
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: \end{pmatrix}$$ |  | (7) |
- en: 'where ${x_{p,q}^{*}}^{2}$ can be formulated as:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${x_{p,q}^{*}}^{2}$ 可以表示为：
- en: '|  | $\delta_{i,j}=\frac{w_{i,j}^{2}}{[\operatorname{diag}((\boldsymbol{x}\boldsymbol{x}^{\top}+\lambda\boldsymbol{I})^{-1})]^{2}}$
    |  | (8) |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta_{i,j}=\frac{w_{i,j}^{2}}{[\operatorname{diag}((\boldsymbol{x}\boldsymbol{x}^{\top}+\lambda\boldsymbol{I})^{-1})]^{2}}$
    |  | (8) |'
- en: 'where $(\boldsymbol{x}\boldsymbol{x}^{\top}+\lambda\boldsymbol{I})^{-1}$),
    while the values located at the diagonal of Hessian are large. Therefore, only
    considering the influence of diagonal elements [[39](#bib.bib39)], we can further
    approximate salience as:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $(\boldsymbol{x}\boldsymbol{x}^{\top}+\lambda\boldsymbol{I})^{-1}$)，而赫西矩阵对角线上的值很大。因此，仅考虑对角元素的影响
    [[39](#bib.bib39)]，我们可以进一步近似重要性为：
- en: '|  | $1$2 |  | (9) |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: Here the diagonal of $\boldsymbol{x}\boldsymbol{x}^{\top}$ channel of weights
    will also exhibit salience. ∎
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在 $\boldsymbol{x}\boldsymbol{x}^{\top}$ 权重通道的对角线中也会展示重要性。∎
- en: F.2 Distribution of salience, activation and weight magnitude
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.2 重要性、激活和权重幅度的分布
- en: 'Fig. [9](#A6.F9 "Figure 9 ‣ F.2 Distribution of salience, activation and weight
    magnitude ‣ Appendix F Extension on Salience Channel Clustering ‣ SliM-LLM: Salience-Driven
    Mixed-Precision Quantization for Large Language Models") illustrates the distribution
    of salience among certain weights in LLMs. This section provides additional examples
    to demonstrate how the distribution of weights and input activation characteristics
    influence the salience of parameters in LLMs. The figure captures seven linear
    projections in the multi-head self-attention (MHA) and feed-forward block (FFB)
    layers of the $2^{nd}$ Transformer modules in the LLaMA-7B model.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [9](#A6.F9 "图 9 ‣ F.2 重要性、激活和权重幅度的分布 ‣ 附录 F 重要性通道聚类的扩展 ‣ SliM-LLM: 基于重要性的混合精度量化用于大型语言模型")
    说明了 LLM 中某些权重的重要性分布。本节提供了额外的示例，展示了权重和输入激活特征的分布如何影响 LLM 中参数的显著性。该图捕捉了 LLaMA-7B
    模型 $2^{nd}$ Transformer 模块中多头自注意力 (MHA) 和前馈块 (FFB) 层的七个线性投影。'
- en: '![Refer to caption](img/4e7761a12d58a354a80b6e1578d50105.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4e7761a12d58a354a80b6e1578d50105.png)'
- en: 'Figure 9: Salience, activation and weight distribution in the $2^{nd}$ layers
    of LLaMA-7B'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: LLaMA-7B 的 $2^{nd}$ 层中的重要性、激活和权重分布'
- en: 'In line with previous findings [[32](#bib.bib32), [44](#bib.bib44)], activations
    demonstrate particularly marked outlier phenomena on anomalous tokens and channels,
    with extremes differing by more than two orders of magnitude. Notably, distinct
    anomalous channels are present in the MHA’s Query, Key, and Value layers, where
    outliers vary significantly across different tokens. This pattern is consistent
    in the FFB layers. We observe that disparities in weight magnitudes are less pronounced
    than those in activation, thus exerting a reduced impact on outlier channels.
    Moreover, weights exhibit structured distributions along rows or columns [[12](#bib.bib12),
    [20](#bib.bib20)], affecting the overall distribution of salience from a row-wise
    perspective (Fig. [9](#A6.F9 "Figure 9 ‣ F.2 Distribution of salience, activation
    and weight magnitude ‣ Appendix F Extension on Salience Channel Clustering ‣ SliM-LLM:
    Salience-Driven Mixed-Precision Quantization for Large Language Models")). However,
    the most prominent salience is predominantly driven by activation across channels
    (column-wise).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '与之前的发现一致 [[32](#bib.bib32), [44](#bib.bib44)]，激活在异常令牌和通道上表现出特别明显的离群现象，极端值相差超过两个数量级。值得注意的是，MHA
    的查询、键和值层中存在明显的异常通道，离群点在不同令牌之间变化显著。这一模式在 FFB 层中也保持一致。我们观察到权重幅度的差异不如激活的差异明显，因此对离群通道的影响较小。此外，权重沿行或列展示出结构化分布
    [[12](#bib.bib12), [20](#bib.bib20)]，从行的角度影响整体的重要性分布 (图 [9](#A6.F9 "图 9 ‣ F.2
    重要性、激活和权重幅度的分布 ‣ 附录 F 重要性通道聚类的扩展 ‣ SliM-LLM: 基于重要性的混合精度量化用于大型语言模型"))。然而，最显著的关键性主要由通道间的激活驱动
    (按列)。'
- en: F.3 Hessian Diagonal Clustering
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: F.3 赫西矩阵对角线聚类
- en: 'Sec. [3.2.1](#S3.SS2.SSS1 "3.2.1 Spatial Clustering of Global Salience ‣ 3.2
    Salience-Determined Bit Allocation ‣ 3 SliM-LLM ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models") demonstrates that outlier tokens in input
    activations result in significant values at the corresponding positions along
    the diagonal of the weight Hessian matrix. Additionally, due to the token sink
    phenomenon [[45](#bib.bib45), [32](#bib.bib32)], areas around significantly activated
    key tokens exhibit increased salience, creating clusters of salient regions along
    the Hessian matrix diagonal. To further elucidate this phenomenon, Fig. [10](#A6.F10
    "Figure 10 ‣ F.3 Hessian Diagonal Clustering ‣ Appendix F Extension on Salience
    Channel Clustering ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for
    Large Language Models") shows the values along the diagonal of the Hessian matrix
    for selected weights in the $2^{nd}$ layer, the token sink phenomenon results
    in a pronounced convergence of significant values along the Hessian matrix diagonal,
    with deep red areas indicating regional clustering. These findings reinforce the
    influence of input activations on the diagonal of the Hessian matrix, subsequently
    leading to a clustering phenomenon in the salience distribution of weights across
    channels.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '节 [3.2.1](#S3.SS2.SSS1 "3.2.1 全球显著性空间聚类 ‣ 3.2 显著性确定的位分配 ‣ 3 SliM-LLM ‣ SliM-LLM:
    基于显著性的混合精度量化用于大型语言模型") 演示了输入激活中的离群标记在权重 Hessian 矩阵对角线上的对应位置产生显著值。此外，由于标记汇聚现象 [[45](#bib.bib45),
    [32](#bib.bib32)]，显著激活的关键标记周围区域显现出更高的显著性，导致 Hessian 矩阵对角线上的显著区域聚集。为了进一步阐明这一现象，图
    [10](#A6.F10 "图 10 ‣ F.3 Hessian 对角线聚类 ‣ 附录 F 对显著性通道聚类的扩展 ‣ SliM-LLM: 基于显著性的混合精度量化用于大型语言模型")
    显示了第 $2^{nd}$ 层选定权重的 Hessian 矩阵对角线上的值，标记汇聚现象导致 Hessian 矩阵对角线上的显著值显著汇聚，深红色区域指示区域聚集。这些发现强化了输入激活对
    Hessian 矩阵对角线的影响，进而导致权重在通道间显著性分布的聚集现象。'
- en: '![Refer to caption](img/057a6c365f6a540c656e2233ab9ebc20.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/057a6c365f6a540c656e2233ab9ebc20.png)'
- en: 'Figure 10: Hessian diagonal magnitude in attention layers of $2^{nd}$ layers
    of LLaMA-7B'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: LLaMA-7B 第 $2^{nd}$ 层注意力层的 Hessian 对角线幅度'
- en: Appendix G More Comparisons
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 更多比较
- en: 'In this section, we provide supplementary experiments for SliM-LLM. Tab. [6](#A7.T6
    "Table 6 ‣ Appendix G More Comparisons ‣ SliM-LLM: Salience-Driven Mixed-Precision
    Quantization for Large Language Models") displays the comparative results of SliM-LLM
    and SliM-LLM* with other methods on the OPT series models. Tab. [7](#A7.T7 "Table
    7 ‣ Appendix G More Comparisons ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models") shows the performance of SliM-LLM when quantizing
    the LLaMA family models on the C4 dataset, while Tab. [8](#A7.T8 "Table 8 ‣ Appendix
    G More Comparisons ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization for
    Large Language Models") also compares the results of SliM-LLM* on the C4 dataset.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们提供了 SliM-LLM 的补充实验。表 [6](#A7.T6 "表 6 ‣ 附录 G 更多比较 ‣ SliM-LLM: 基于显著性的混合精度量化用于大型语言模型")
    显示了 SliM-LLM 和 SliM-LLM* 与其他方法在 OPT 系列模型上的比较结果。表 [7](#A7.T7 "表 7 ‣ 附录 G 更多比较 ‣
    SliM-LLM: 基于显著性的混合精度量化用于大型语言模型") 显示了 SliM-LLM 在 C4 数据集上量化 LLaMA 家族模型的表现，而表 [8](#A7.T8
    "表 8 ‣ 附录 G 更多比较 ‣ SliM-LLM: 基于显著性的混合精度量化用于大型语言模型") 还比较了 SliM-LLM* 在 C4 数据集上的结果。'
- en: 'Table 6: Quantization results of OPT Models on WikiText2 (group size is 128).'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: OPT 模型在 WikiText2 上的量化结果（组大小为 128）。'
- en: '| #W PPL$\downarrow$ | Method | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| #W PPL$\downarrow$ | 方法 | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
- en: '| 16-bit | - | 14.63 | 12.47 | 10.86 | 10.12 | 9.56 | 9.34 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 16-bit | - | 14.63 | 12.47 | 10.86 | 10.12 | 9.56 | 9.34 |'
- en: '| 3-bit | RTN | 1.2e2 | 3.0e2 | 23.54 | 46.03 | 18.80 | 1.4e6 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | RTN | 1.2e2 | 3.0e2 | 23.54 | 46.03 | 18.80 | 1.4e6 |'
- en: '| GPTQ | 16.47 | 13.69 | 11.65 | 10.35 | 9.73 | 10.96 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 16.47 | 13.69 | 11.65 | 10.35 | 9.73 | 10.96 |'
- en: '| AWQ | 16.32 | 13.58 | 11.41 | 10.68 | 9.85 | 9.60 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 16.32 | 13.58 | 11.41 | 10.68 | 9.85 | 9.60 |'
- en: '| QuIP | 16.21 | 13.79 | 11.51 | 10.50 | 9.75 | 9.59 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 16.21 | 13.79 | 11.51 | 10.50 | 9.75 | 9.59 |'
- en: '| SliM-LLM | 15.91 | 13.26 | 11.27 | 10.26 | 9.70 | 9.48 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM | 15.91 | 13.26 | 11.27 | 10.26 | 9.70 | 9.48 |'
- en: '| \cdashline2-8 | OmniQuant | 15.72 | 13.18 | 11.27 | 10.47 | 9.79 | 9.53 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | OmniQuant | 15.72 | 13.18 | 11.27 | 10.47 | 9.79 | 9.53 |'
- en: '|  | AffineQuant | 15.61 | 12.98 | 11.18 | 10.51 | 9.81 | - |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '|  | AffineQuant | 15.61 | 12.98 | 11.18 | 10.51 | 9.81 | - |'
- en: '|  | SliM-LLM^+ | 15.58 | 12.84 | 11.18 | 10.44 | 9.67 | 9.51 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  | SliM-LLM^+ | 15.58 | 12.84 | 11.18 | 10.44 | 9.67 | 9.51 |'
- en: '| 2-bit | RTN | 1.3e4 | 5.7e4 | 7.8e3 | 7.6e4 | 1.3e4 | 3.6e5 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | RTN | 1.3e4 | 5.7e4 | 7.8e3 | 7.6e4 | 1.3e4 | 3.6e5 |'
- en: '| GPTQ | 1.1e2 | 61.59 | 20.18 | 21.36 | 12.71 | 82.10 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 1.1e2 | 61.59 | 20.18 | 21.36 | 12.71 | 82.10 |'
- en: '| AWQ | 47.97 | 28.50 | 16.20 | 14.32 | 12.31 | 14.54 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 47.97 | 28.50 | 16.20 | 14.32 | 12.31 | 14.54 |'
- en: '| QuIP | 41.64 | 28.98 | 18.57 | 16.02 | 11.48 | 10.76 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 41.64 | 28.98 | 18.57 | 16.02 | 11.48 | 10.76 |'
- en: '| PB-LLM | 45.92 | 39.71 | 20.37 | 19.11 | 17.01 | 16.36 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 45.92 | 39.71 | 20.37 | 19.11 | 17.01 | 16.36 |'
- en: '| SliM-LLM | 30.71 | 24.08 | 14.41 | 13.68 | 11.34 | 10.94 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM | 30.71 | 24.08 | 14.41 | 13.68 | 11.34 | 10.94 |'
- en: '| \cdashline2-8 | OmniQuant | 23.95 | 18.13 | 14.43 | 12.94 | 11.39 | 30.84
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| \cdashline2-8 | OmniQuant | 23.95 | 18.13 | 14.43 | 12.94 | 11.39 | 30.84
    |'
- en: '|  | SliM-LLM^+ | 24.57 | 17.98 | 14.22 | 12.16 | 11.27 | 14.98 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '|  | SliM-LLM^+ | 24.57 | 17.98 | 14.22 | 12.16 | 11.27 | 14.98 |'
- en: 'Table 7: Quantization results of LLaMA Family with statistic quantizer on C4
    (group size is 128).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: 使用统计量化器对 LLaMA 家族在 C4 上的量化结果（组大小为 128）。'
- en: '| #W PPL$\downarrow$ | Method | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B
    | 2-70B | 3-8B | 3-70B |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| #W PPL$\downarrow$ | 方法 | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    | 3-8B | 3-70B |'
- en: '| 16-bit | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 | 9.22 | 6.85
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 16-bit | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 | 9.22 | 6.85
    |'
- en: '| 3-bit | APTQ | 6.24 | - | - | - | - | - | - | - | - |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | APTQ | 6.24 | - | - | - | - | - | - | - | - |'
- en: '| RTN | 8.62 | 7.49 | 6.58 | 6.10 | 8.40 | 7.18 | 6.02 | 1.1e2 | 22.39 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 8.62 | 7.49 | 6.58 | 6.10 | 8.40 | 7.18 | 6.02 | 1.1e2 | 22.39 |'
- en: '| AWQ | 7.92 | 7.07 | 6.37 | 5.94 | 7.84 | 6.94 | - | 11.62 | 8.03 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 7.92 | 7.07 | 6.37 | 5.94 | 7.84 | 6.94 | - | 11.62 | 8.03 |'
- en: '| GPTQ | 7.85 | 7.10 | 6.47 | 6.00 | 7.89 | 7.00 | 5.85 | 13.67 | 10.52 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 7.85 | 7.10 | 6.47 | 6.00 | 7.89 | 7.00 | 5.85 | 13.67 | 10.52 |'
- en: '|  | SliM-LLM | 6.14 | 6.05 | 6.33 | 5.94 | 7.74 | 5.26 | 5.09 | 13.10 | 8.64
    |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|  | SliM-LLM | 6.14 | 6.05 | 6.33 | 5.94 | 7.74 | 5.26 | 5.09 | 13.10 | 8.64
    |'
- en: '| 2-bit | RTN | 1.0e3 | 4.5e2 | 99.45 | 17.15 | 4.9e3 | 1.4e2 | 42.13 | 2.5e4
    | 4.6e5 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | RTN | 1.0e3 | 4.5e2 | 99.45 | 17.15 | 4.9e3 | 1.4e2 | 42.13 | 2.5e4
    | 4.6e5 |'
- en: '| AWQ | 1.9e5 | 2.3e5 | 2.4e5 | 7.5e4 | 1.7e5 | 9.4e4 | - | 2.1e6 | 1.4e6 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 1.9e5 | 2.3e5 | 2.4e5 | 7.5e4 | 1.7e5 | 9.4e4 | - | 2.1e6 | 1.4e6 |'
- en: '| GPTQ | 34.63 | 15.29 | 11.93 | 11.99 | 33.70 | 20.97 | NAN | 4.1e4 | 21.82
    |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 34.63 | 15.29 | 11.93 | 11.99 | 33.70 | 20.97 | NAN | 4.1e4 | 21.82
    |'
- en: '| QuIP | 33.74 | 21.94 | 10.95 | 13.99 | 31.94 | 16.16 | 8.17 | 1.3e2 | 22.24
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| QuIP | 33.74 | 21.94 | 10.95 | 13.99 | 31.94 | 16.16 | 8.17 | 1.3e2 | 22.24
    |'
- en: '| PB-LLM | 49.73 | 26.93 | 17.93 | 11.85 | 29.84 | 19.82 | 8.95 | 79.21 | 33.91
    |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM | 49.73 | 26.93 | 17.93 | 11.85 | 29.84 | 19.82 | 8.95 | 79.21 | 33.91
    |'
- en: '| SliM-LLM | 32.91 | 13.85 | 11.27 | 10.95 | 16.00 | 9.41 | 7.01 | 1.1e2 |
    15.92 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM | 32.91 | 13.85 | 11.27 | 10.95 | 16.00 | 9.41 | 7.01 | 1.1e2 |
    15.92 |'
- en: 'Table 8: Quantization results of LLaMA-1 and LLaMA-2 models with learnable
    quantizer on C4.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 使用可学习量化器对 LLaMA-1 和 LLaMA-2 模型在 C4 上的量化结果。'
- en: '| #W PPL$\downarrow$ | Method | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B
    | 2-70B |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| #W PPL$\downarrow$ | 方法 | 1-7B | 1-13B | 1-30B | 1-65B | 2-7B | 2-13B | 2-70B
    |'
- en: '| 16-bit | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 16-bit | - | 7.08 | 6.61 | 5.98 | 5.62 | 6.97 | 6.46 | 5.52 |'
- en: '| 3-bit | OmniQuant | 7.75 | 7.05 | 6.37 | 5.93 | 7.75 | 6.98 | 5.85 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 3-bit | OmniQuant | 7.75 | 7.05 | 6.37 | 5.93 | 7.75 | 6.98 | 5.85 |'
- en: '| AffineQuant | 7.75 | 7.04 | 6.40 | - | 7.83 | 6.99 | - |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 7.75 | 7.04 | 6.40 | - | 7.83 | 6.99 | - |'
- en: '| SliM-LLM^+ | 7.75 | 6.91 | 6.36 | 5.96 | 7.71 | 6.90 | 5.85 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM^+ | 7.75 | 6.91 | 6.36 | 5.96 | 7.71 | 6.90 | 5.85 |'
- en: '| 2-bit | OmniQuant | 12.97 | 10.36 | 9.36 | 8.00 | 15.02 | 11.05 | 8.52 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 2-bit | OmniQuant | 12.97 | 10.36 | 9.36 | 8.00 | 15.02 | 11.05 | 8.52 |'
- en: '| AffineQuant | 14.92 | 12.64 | 9.66 | - | 16.02 | 10.98 | - |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| AffineQuant | 14.92 | 12.64 | 9.66 | - | 16.02 | 10.98 | - |'
- en: '| SliM-LLM^+ | 14.99 | 10.22 | 9.33 | 7.52 | 18.18 | 10.24 | 8.40 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| SliM-LLM^+ | 14.99 | 10.22 | 9.33 | 7.52 | 18.18 | 10.24 | 8.40 |'
- en: Appendix H Real Dialog Examples
  id: totrans-335
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 实际对话示例
- en: 'In this section, we show some dialogue examples of LLaMA-2-13B and Vicuna-13B
    with SliM-LLM-2bit and GPTQ-2bit in Fig. [11](#A8.F11 "Figure 11 ‣ Appendix H
    Real Dialog Examples ‣ SliM-LLM: Salience-Driven Mixed-Precision Quantization
    for Large Language Models").'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了 LLaMA-2-13B 和 Vicuna-13B 的对话示例，使用 SliM-LLM-2bit 和 GPTQ-2bit，如图 [11](#A8.F11
    "图 11 ‣ 附录 H 实际对话示例 ‣ SliM-LLM: 基于显著性的混合精度量化用于大型语言模型")所示。'
- en: '![Refer to caption](img/a4fd11e8e21c0b51bc8455338bc0342f.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a4fd11e8e21c0b51bc8455338bc0342f.png)'
- en: 'Figure 11: Some examples of conversations. LLaMA-2-13B and Vicuna-13B are chosen
    to show the case of language supplementary and Q&A ability. And GPTQ-2bit is selected
    as the comparison. We color the text to show the reasonable or inappropriate responses.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: 一些对话示例。选择了 LLaMA-2-13B 和 Vicuna-13B 来展示语言补充和问答能力的案例，并选择了 GPTQ-2bit 作为对比。我们对文本进行了着色，以显示合理或不合适的回应。'
