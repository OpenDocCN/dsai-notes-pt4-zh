- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:50:17'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:17
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'BiLLM: Pushing the Limit of Post-Training Quantization for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'BiLLM: 推动 LLM 后训练量化的极限'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04291](https://ar5iv.labs.arxiv.org/html/2402.04291)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.04291](https://ar5iv.labs.arxiv.org/html/2402.04291)
- en: Wei Huang    Yangdong Liu    Haotong Qin^(🖂)    Ying Li    Shiming Zhang   
    Xianglong Liu    Michele Magno    Xiaojuan Qi
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Wei Huang    Yangdong Liu    Haotong Qin^(🖂)    Ying Li    Shiming Zhang   
    Xianglong Liu    Michele Magno    Xiaojuan Qi
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Pretrained large language models (LLMs) exhibit exceptional general language
    processing capabilities but come with significant demands on memory and computational
    resources. As a powerful compression technology, binarization can extremely reduce
    model weights to a mere 1 bit, lowering the expensive computation and memory requirements.
    However, existing quantization techniques fall short of maintaining LLM performance
    under ultra-low bit-widths. In response to this challenge, we present BiLLM, a
    groundbreaking 1-bit post-training quantization scheme tailored for pretrained
    LLMs. Based on the weight distribution of LLMs, BiLLM first identifies and structurally
    selects salient weights, and minimizes the compression loss through an effective
    binary residual approximation strategy. Moreover, considering the bell-shaped
    distribution of the non-salient weights, we propose an optimal splitting search
    to group and binarize them accurately. BiLLM achieving for the first time high-accuracy
    inference (e.g. 8.41 perplexity on LLaMA2-70B) with only 1.08-bit weights across
    various LLMs families and evaluation metrics, outperforms SOTA quantization methods
    of LLM by significant margins. Moreover, BiLLM enables the binarization process
    of the LLM with 7 billion weights within 0.5 hours on a single GPU, demonstrating
    satisfactory time efficiency. The code is available at [https://github.com/Aaronhuang-778/BiLLM](https://github.com/Aaronhuang-778/BiLLM).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLMs）展现了卓越的语言处理能力，但对内存和计算资源有着极大的需求。作为一种强大的压缩技术，二值化可以将模型权重大幅减少到仅1位，从而降低昂贵的计算和内存需求。然而，现有的量化技术在超低比特宽度下无法保持
    LLM 的性能。针对这一挑战，我们提出了 BiLLM，这是一种突破性的 1 位后训练量化方案，专为预训练的 LLM 设计。基于 LLM 的权重分布，BiLLM
    首先识别并结构性地选择显著权重，并通过有效的二进制残差近似策略最小化压缩损失。此外，考虑到非显著权重的钟形分布，我们提出了一种最佳分割搜索方法，以准确地对其进行分组和二值化。BiLLM
    首次实现了在不同 LLM 家族和评估指标下，仅使用 1.08 位权重的情况下进行高精度推理（例如 LLaMA2-70B 上的 8.41 困惑度），显著超越了
    SOTA 量化方法。此外，BiLLM 在单个 GPU 上在 0.5 小时内完成了 70 亿权重的 LLM 的二值化，展现了令人满意的时间效率。代码可在 [https://github.com/Aaronhuang-778/BiLLM](https://github.com/Aaronhuang-778/BiLLM)
    获取。
- en: Model Binarization, Large Language Model, Model Compression, Deep Learning
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 模型二值化、大型语言模型、模型压缩、深度学习
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recently, large language models (LLMs) based on transformers (Vaswani et al.,
    [2017](#bib.bib42)) have garnered significant attention in natural language processing.
    Pretrained LLMs like OPT (Zhang et al., [2022](#bib.bib51)) and LLaMA (Touvron
    et al., [2023a](#bib.bib40)), have demonstrated excellent performance across a
    range of evaluation benchmarks. However, LLMs pose substantial challenges in deployment
    on memory-constrained devices due to their immense parameter size and computation
    requirements. For instance, the widely-used LLaMA2-70B (Touvron et al., [2023b](#bib.bib41))
    model, with its 70 billion parameters, requires 150 GB of storage in half-precision
    (FP16) format. This necessitates at least two A100 GPUs, each with 80 GB of storage
    space, for inference.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于变换器的语言模型（LLMs）（Vaswani et al., [2017](#bib.bib42)）在自然语言处理领域引起了极大的关注。预训练的
    LLM，如 OPT（Zhang et al., [2022](#bib.bib51)）和 LLaMA（Touvron et al., [2023a](#bib.bib40)），在多个评估基准上展示了出色的性能。然而，由于其庞大的参数规模和计算需求，LLMs
    在内存受限的设备上部署时面临重大挑战。例如，广泛使用的 LLaMA2-70B（Touvron et al., [2023b](#bib.bib41)）模型，其
    700 亿个参数在半精度（FP16）格式下需要 150 GB 的存储。这需要至少两个 A100 GPU，每个 GPU 拥有 80 GB 的存储空间，以进行推理。
- en: '![Refer to caption](img/43e017d9c8f673628e426639c9d858e0.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/43e017d9c8f673628e426639c9d858e0.png)'
- en: 'Figure 1: The perplexity of LLaMA-13B on WikiText2 under different bit-widths.
    Round-to-nearest (RTN), GPTQ, and PB-LLM (10% weight of INT8) suffer accuracy
    loss at ultra-low bits, facing the sharply increasing perplexity ($\downarrow$).
    BiLLM demonstrates exceptional performance under binarization.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLaMA-13B在WikiText2下不同位宽的困惑度。Round-to-nearest (RTN)、GPTQ和PB-LLM（10% INT8权重）在超低位下准确率下降，面临困惑度急剧上升（$\downarrow$）。BiLLM在二值化下表现出卓越的性能。
- en: Model quantization has emerged as a highly effective technology for compressing
    neural networks, thereby reducing the model size of LLMs and substantially saving
    GPU memory consumption (Dettmers et al., [2022](#bib.bib9)). Current quantization
    techniques primarily fall into Quantization-Aware Training (QAT) and Post-Training
    Quantization (PTQ). QAT involves fine-tuning and retraining during the quantization
    process, while PTQ significantly streamlines the computation by eliminating back-propagation,
    enabling a faster quantization process and promoting the practicality of quantization (Frantar
    et al., [2022](#bib.bib17); Shang et al., [2023](#bib.bib39); Lin et al., [2023](#bib.bib25)).
    Given the deep structures and numerous parameters of LLMs, PTQ stands out for
    its ability to rapidly perform the quantization process, especially on time and
    resource-constrained scenarios (Zhu et al., [2023](#bib.bib53)).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 模型量化已经成为一种高度有效的神经网络压缩技术，从而减少了LLM的模型大小，并大幅节省了GPU内存消耗（Dettmers et al., [2022](#bib.bib9)）。当前的量化技术主要分为量化感知训练（QAT）和后训练量化（PTQ）。QAT涉及在量化过程中进行微调和重新训练，而PTQ通过消除反向传播显著简化了计算，从而加快了量化过程，并促进了量化的实用性（Frantar
    et al., [2022](#bib.bib17); Shang et al., [2023](#bib.bib39); Lin et al., [2023](#bib.bib25)）。鉴于LLM的深层结构和众多参数，PTQ因其在时间和资源有限的场景下能够快速进行量化处理而脱颖而出（Zhu
    et al., [2023](#bib.bib53)）。
- en: 'Despite the success of previous PTQ methods in 8-bit and 4-bit quantization (Dettmers
    et al., [2022](#bib.bib9), [2023b](#bib.bib11); Frantar et al., [2022](#bib.bib17);
    Xiao et al., [2023](#bib.bib46); Frantar & Alistarh, [2022](#bib.bib15)), the
    expanding size of LLMs demands more aggressive quantization approaches (Shang
    et al., [2023](#bib.bib39)). Neural network binarization, which reduces the weight
    bit-width to only 1 bit, is a promising approach (Helwegen et al., [2019](#bib.bib18);
    Qin et al., [2020](#bib.bib32), [2023](#bib.bib34)). However, as depicted in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs"), current advanced PTQ methods for LLMs exhibit a performance collapse
    under ultra-low bit ($\leqslant$3 bits) quantization. This phenomenon can be attributed
    to the significant difference between quantized and original weights. Even the
    recent binary PTQ method for LLMs, PB-LLM (Shang et al., [2023](#bib.bib39)),
    only maintains a perplexity metric of around 800 with an average weight of 1.7
    bits. This observation underscores the challenges existing PTQ methods face in
    promoting the weight binarization of LLMs.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管之前的PTQ方法在8-bit和4-bit量化中取得了成功（Dettmers et al., [2022](#bib.bib9), [2023b](#bib.bib11);
    Frantar et al., [2022](#bib.bib17); Xiao et al., [2023](#bib.bib46); Frantar &
    Alistarh, [2022](#bib.bib15)），LLM的不断扩展的规模要求更加激进的量化方法（Shang et al., [2023](#bib.bib39)）。神经网络二值化将权重位宽减少到仅1位，是一种有前途的方法（Helwegen
    et al., [2019](#bib.bib18); Qin et al., [2020](#bib.bib32), [2023](#bib.bib34)）。然而，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")所示，当前针对LLM的先进PTQ方法在超低位（$\leqslant$3位）量化下表现出性能崩溃。这一现象可以归因于量化权重与原始权重之间的显著差异。即使是最近的LLM二值化PTQ方法PB-LLM（Shang
    et al., [2023](#bib.bib39)），也仅维持了大约800的困惑度指标，平均权重为1.7位。这一观察结果突显了现有PTQ方法在推动LLM权重二值化方面面临的挑战。'
- en: 'In pursuit of this goal, we conducted an empirical study to analyze the distribution
    of pre-trained weights in LLMs. The findings derived from this study are presented
    in Appendix [G](#A7 "Appendix G Magnitude and Hessian Distribution of LLMs ‣ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs"), revealing two key
    observations:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '为了实现这一目标，我们进行了实证研究以分析LLM中预训练权重的分布。研究结果在附录[G](#A7 "Appendix G Magnitude and
    Hessian Distribution of LLMs ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")中展示，揭示了两个关键观察结果：'
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The second-order Hessian matrix of weights demonstrates an exceptionally long-tail
    distribution and is often used to measure the importance of weight elements in
    neural networks (LeCun et al., [1989](#bib.bib21); Dong et al., [2019](#bib.bib12)).
    As depicted in Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ BiLLM: Pushing the
    Limit of Post-Training Quantization for LLMs"), a small fraction of weights elements
    possesses significantly high Hessian values, substantially influencing the layer
    output. In contrast, most Hessian values cluster around 0.'
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重的二阶 Hessian 矩阵展示了一个极长尾分布，通常用于衡量神经网络中权重元素的重要性（LeCun 等，[1989](#bib.bib21)；Dong
    等，[2019](#bib.bib12)）。如图[2](#S1.F2 "图 2 ‣ 1 介绍 ‣ BiLLM：推动 LLMs 后训练量化的极限") 所示，少量的权重元素具有显著高的
    Hessian 值，对层输出有显著影响。相比之下，大多数 Hessian 值聚集在 0 附近。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The density distribution of weight magnitudes in LLMs follows a bell-shaped
    pattern. This bell-shaped distribution exhibits a significant resemblance to both
    the Gaussian or Laplace distribution in terms of its characteristics (Blundell
    et al., [2015](#bib.bib3)). Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs") illustrates that most
    weight values cluster around zero with a non-uniform bell-shaped distribution.'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: LLMs 中权重大小的密度分布遵循钟形模式。这种钟形分布在特征上与高斯分布或拉普拉斯分布非常相似（Blundell 等，[2015](#bib.bib3)）。图[2](#S1.F2
    "图 2 ‣ 1 介绍 ‣ BiLLM：推动 LLMs 后训练量化的极限") 说明大多数权重值集中在零附近，呈现非均匀的钟形分布。
- en: '![Refer to caption](img/a0c5162c44aa45e79d77edf21a72ebd1.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a0c5162c44aa45e79d77edf21a72ebd1.png)'
- en: 'Figure 2: The Hessian metrics (sensitivity) and magnitude (value) of weights
    in LLMs. The weights of different layers in LLMs are characterized by bell-shaped
    distribution, accompanied by a few salient values.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLMs 中权重的 Hessian 度量（敏感性）和幅度（值）。LLMs 中不同层的权重特征为钟形分布，并伴有少数显著值。
- en: 'The above implies: a) A minority of weights play an important role in LLMs,
    whereas the majority of weights exhibit characteristics of redundancy (Shang et al.,
    [2023](#bib.bib39); Dettmers et al., [2023b](#bib.bib11)); b) With the most aggressive
    bit-width, binarization incurs most severe error among quantization under bell-shaped
    distributions in LLMs (Jacob et al., [2018](#bib.bib19)).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 上述内容意味着：a) 在大型语言模型（LLMs）中，少数权重在模型中扮演重要角色，而大多数权重则表现出冗余特征（Shang 等，[2023](#bib.bib39)；Dettmers
    等，[2023b](#bib.bib11)）；b) 在 LLMs 的钟形分布下，最具攻击性的位宽下量化会导致最严重的误差（Jacob 等，[2018](#bib.bib19)）。
- en: 'Motivated by the above observation, we propose a novel 1-bit PTQ framework
    for LLMs, namely BiLLM, incorporating two core designs to achieve highly accurate
    weight binarization. First, guided by the Hessian-based metric, we select the
    salient weights structurally (Figure [3](#S2.F3 "Figure 3 ‣ 2 Related Works ‣
    BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") upper-right)
    to achieve a trade-off between accuracy and storage savings and develop a residual
    approximation to maximize the restoration of salient weights with highly dynamic
    range. Second, for the remaining non-salient weights (Figure [3](#S2.F3 "Figure
    3 ‣ 2 Related Works ‣ BiLLM: Pushing the Limit of Post-Training Quantization for
    LLMs") lower-right), we design an optimal splitting binarization strategy, where
    a meticulous search process is applied to determine an optimal break-point for
    weight distribution and binarization of the segments is then processed separately
    to minimize binarization errors. Moreover, BiLLM incorporates error compensation
    on a block-wise basis by default following existing common practices (Frantar
    et al., [2022](#bib.bib17); Shang et al., [2023](#bib.bib39)), which further reduces
    quantization error.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述观察，我们提出了一种新型的 1-bit PTQ 框架，用于 LLMs，即 BiLLM，结合了两个核心设计以实现高度准确的权重二值化。首先，在 Hessian
    基于度量的指导下，我们结构性地选择显著权重（图[3](#S2.F3 "图 3 ‣ 2 相关工作 ‣ BiLLM：推动 LLMs 后训练量化的极限") 右上）以实现准确性与存储节省之间的权衡，并开发了一个残差近似方法以最大化显著权重的恢复，同时具有高度动态范围。其次，对于剩余的非显著权重（图[3](#S2.F3
    "图 3 ‣ 2 相关工作 ‣ BiLLM：推动 LLMs 后训练量化的极限") 右下），我们设计了一种最优的拆分二值化策略，其中应用了精细的搜索过程来确定权重分布的最佳断点，然后分别处理各段的二值化，以最小化二值化误差。此外，BiLLM
    默认遵循现有的常见做法（Frantar 等，[2022](#bib.bib17)；Shang 等，[2023](#bib.bib39)）进行块级误差补偿，这进一步减少了量化误差。
- en: Extensive experiments demonstrate that BiLLM achieve the state-of-the-art (SOTA)
    performance for LLMs across multiple LLM families on various evaluation metrics,
    and first achieves extremely compact 1.07$\sim$1.11 bit-width in average for the
    PTQ binarization. For example, on the Wikitext2(Merity et al., [2016](#bib.bib28))
    metric, BiLLM achieved perplexities of 8.49 and 8.41 with only 1.08-bit weights
    on LLaMA-65B (Touvron et al., [2023a](#bib.bib40))and LLaMA2-70B (Touvron et al.,
    [2023b](#bib.bib41)), respectively, even surpassing the 9.34 performance of the
    FP16 OPT-66B (Zhang et al., [2022](#bib.bib51)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 大量实验表明，BiLLM 在各种评估指标上对多个 LLM 家族实现了最新的 (SOTA) 性能，并首次在 PTQ 二值化中实现了平均 1.07$\sim$1.11
    位宽的极致紧凑性。例如，在 Wikitext2 (Merity et al., [2016](#bib.bib28)) 指标上，BiLLM 在 LLaMA-65B
    (Touvron et al., [2023a](#bib.bib40)) 和 LLaMA2-70B (Touvron et al., [2023b](#bib.bib41))
    上仅使用 1.08 位权重分别达到了 8.49 和 8.41 的困惑度，甚至超越了 FP16 OPT-66B (Zhang et al., [2022](#bib.bib51))
    的 9.34 性能。
- en: 2 Related Works
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: '![Refer to caption](img/0d3e934ddf53468c1d4f7d992dc6f337.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0d3e934ddf53468c1d4f7d992dc6f337.png)'
- en: 'Figure 3: Schematic of the PTQ binarization framework for LLMs. The left side
    shows the structure of the Transformer block after binarization. The right side
    shows the binarization process of BiLLM, which consists of two parts, Residual
    Approximation for salient weights and Bell-shaped Splitting for non-salient weights.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：LLMs 的 PTQ 二值化框架示意图。左侧展示了二值化后的 Transformer 块的结构。右侧展示了 BiLLM 的二值化过程，包括两个部分：用于显著权重的残差近似和用于非显著权重的钟形分割。
- en: 2.1 Large Language Model Quantization
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型量化
- en: Quantization maps high-precision parameters to a discrete range. This method,
    which compresses parameters without altering the model structure, effectively
    reduces the storage and computational overhead of deep neural networks. Recent
    work has successfully applied QAT and PTQ to LLMs. QAT, through a quantization-aware
    retraining strategy, better preserves the performance of quantized models. LLM-QAT (Liu
    et al., [2023](#bib.bib26)) addressed data barrier issues in QAT training through
    data-free distillation. However, for LLMs with extremely large parameter sizes,
    the cost of retraining is prohibitively high and inefficient. Therefore, techniques
    such as QLoRA (Dettmers et al., [2023a](#bib.bib10)) focus on parameter-efficient
    fine-tuning (PEFT) methods for quantizing LLMs, enhancing the efficiency of QAT.
    Nevertheless, even these efficient fine-tuning quantization strategies require
    over 24 hours of GPU time.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 量化将高精度参数映射到离散范围。这种方法在不改变模型结构的情况下压缩参数，有效减少了深度神经网络的存储和计算开销。近期工作成功将 QAT 和 PTQ 应用于
    LLMs。QAT 通过量化感知再训练策略，更好地保持了量化模型的性能。LLM-QAT (Liu et al., [2023](#bib.bib26)) 通过无数据蒸馏解决了
    QAT 训练中的数据障碍问题。然而，对于参数规模极大的 LLMs，重新训练的成本非常高且效率低。因此，像 QLoRA (Dettmers et al., [2023a](#bib.bib10))
    这样的技术专注于量化 LLMs 的参数高效微调 (PEFT) 方法，从而提高 QAT 的效率。尽管如此，即便是这些高效的微调量化策略也需要超过 24 小时的
    GPU 时间。
- en: Therefore, the PTQ strategy has become a significant option for quantizing LLMs
    efficiently. Works like BRECQ (Li et al., [2021](#bib.bib23)), ZerqQuant ([Yao
    et al.,](#bib.bib47) ) and LLM.int8() (Dettmers et al., [2022](#bib.bib9)) enhance
    quantization accuracy by adding additional grouping labels for custom quantization
    blocks. Other studies adopt a feature segmentation strategy, such as PB-LLM (Shang
    et al., [2023](#bib.bib39)) and SpQR (Dettmers et al., [2023b](#bib.bib11)). They
    preserve the bit-width of outlier features or those with higher quantization errors
    to FP16 or INT8, mitigating the precision loss due to quantization. GPTQ (Frantar
    et al., [2022](#bib.bib17)) employs a more precise quantization framework, reducing
    the block quantization errors of LLMs through Hessian-based second-order error
    compensation (Frantar & Alistarh, [2022](#bib.bib15)), achieving commendable performance
    in low-bits (4 bits) quantization. Smoothquant (Xiao et al., [2023](#bib.bib46))
    introduces a strategy of scaling weight and activation outliers to simplify quantization.
    Subsequently, AWQ (Lin et al., [2023](#bib.bib25)) and OWQ (Lee et al., [2023](#bib.bib22))
    also proposed scale transformations of more crucial weight channels for activation
    features, preserving their information representation capacity.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，PTQ 策略已经成为有效量化 LLM 的重要选项。像 BRECQ（Li 等，[2021](#bib.bib23)）、ZerqQuant（[Yao
    等](#bib.bib47)）和 LLM.int8()（Dettmers 等，[2022](#bib.bib9)）等工作通过为自定义量化块添加额外的分组标签来提高量化精度。其他研究则采用特征分割策略，如
    PB-LLM（Shang 等，[2023](#bib.bib39)）和 SpQR（Dettmers 等，[2023b](#bib.bib11)）。它们将具有更高量化误差或异常特征的位宽保持在
    FP16 或 INT8，以减轻由于量化造成的精度损失。GPTQ（Frantar 等，[2022](#bib.bib17)）采用了更精确的量化框架，通过基于
    Hessian 的二阶误差补偿（Frantar & Alistarh，[2022](#bib.bib15)）减少 LLM 的块量化误差，在低比特（4 位）量化中表现出色。Smoothquant（Xiao
    等，[2023](#bib.bib46)）引入了缩放权重和激活异常值以简化量化的策略。随后，AWQ（Lin 等，[2023](#bib.bib25)）和 OWQ（Lee
    等，[2023](#bib.bib22)）也提出了对更重要的权重通道进行缩放变换以保持其信息表示能力。
- en: 2.2 Network Binarization
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 网络二值化
- en: 'Binarized compression can quantize parameters to only 1 bit, expressed as $\pm$1\.
    In forward propagation, the sign function is used to binarize the original parameter
    tensor:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 二值化压缩可以将参数量化为仅 1 位，表示为 $\pm$1。在前向传播中，使用符号函数将原始参数张量二值化：
- en: '|  | $\vspace{-0.1in}\mathbf{W}_{b}=\alpha\cdot\operatorname{sign}(\mathbf{W}_{f}),$
    |  | (1) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | $\vspace{-0.1in}\mathbf{W}_{b}=\alpha\cdot\operatorname{sign}(\mathbf{W}_{f}),$
    |  | (1) |'
- en: '|  | $\operatorname{sign}(x)=\begin{cases}1&amp;\text{if $x\geq 0$},\\ -1&amp;\text{others}.\end{cases}$
    |  | (2) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname{sign}(x)=\begin{cases}1&\text{如果 $x\geq 0$},\\ -1&\text{其他}.\end{cases}$
    |  | (2) |'
- en: where $\mathbf{W}_{f}\in\mathbb{R}^{n\times m}$.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}_{f}\in\mathbb{R}^{n\times m}$。
- en: Most previous binarization works adopt a framework based on QAT for quantization (Qin
    et al., [2023](#bib.bib34)). Straight through estimator (STE) (Bengio et al.,
    [2013](#bib.bib1)) is deployed to address the issue of gradient vanishing caused
    by the $\operatorname{sign}(\cdot)$. DoReFa-Net (Zhou et al., [2016](#bib.bib52))
    further expands upon XNOR-Net, employing quantized gradients to accelerate network
    training. Group segmentation is also applied in binarization tasks, with Syq (Faraone
    et al., [2018](#bib.bib14)) utilizing network weight to the small size of groups
    for minimizing binarization errors.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的大多数二值化工作采用了基于 QAT 的量化框架（Qin 等，[2023](#bib.bib34)）。采用直通估计器（STE）（Bengio 等，[2013](#bib.bib1)）来解决由
    $\operatorname{sign}(\cdot)$ 引起的梯度消失问题。DoReFa-Net（Zhou 等，[2016](#bib.bib52)）在
    XNOR-Net 的基础上进一步扩展，使用量化梯度加速网络训练。二值化任务中也应用了组分割策略，其中 Syq（Faraone 等，[2018](#bib.bib14)）利用网络权重将小组的大小最小化，以减少二值化误差。
- en: Based on the successful application of binarization in Transformers (Wang et al.,
    [2023](#bib.bib43)) and Bert (Qin et al., [2022](#bib.bib33)), we believe that
    the binarization of LLMs is filled with potential. PB-LLM (Shang et al., [2023](#bib.bib39))
    investigates the impact of binarized QAT and PTQ strategies on LLMs, but it is
    necessary to retain a significant proportion (over 30%) of the weights at 8 bits
    to enable LLMs to produce reasonable answers. Due to the presence of a large amount
    of INT8, LLMs still have a relatively high average bit-width. To address this
    issue, we proposed BiLLM, which aims to push the limit of PTQ binarization for
    LLMs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 基于二值化在变压器（Wang et al., [2023](#bib.bib43)）和Bert（Qin et al., [2022](#bib.bib33)）中的成功应用，我们认为LLMs的二值化充满潜力。PB-LLM（Shang
    et al., [2023](#bib.bib39)）研究了二值化QAT和PTQ策略对LLMs的影响，但需要保留超过30%的8位权重，以使LLMs能够产生合理的答案。由于存在大量INT8，LLMs仍具有相对较高的平均比特宽度。为解决此问题，我们提出了BiLLM，旨在推动LLMs
    PTQ二值化的极限。
- en: 3 Method
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'To achieve accurate binarization of LLMs, our approach is designing distinct
    binarization strategies for salient and non-salient weights. We first introduce
    the selection rules for salient weights and their binarization strategies in Section
    [3.1](#S3.SS1 "3.1 Salient Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs"). Then, we elaborate on the
    distribution-based binarization strategy for non-salient weights in Section [3.2](#S3.SS2
    "3.2 Bell-shaped Distribution Splitting for Binarization ‣ 3 Method ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs").'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '为了实现LLMs的准确二值化，我们的方法是为显著和非显著权重设计不同的二值化策略。我们首先在第[3.1节](#S3.SS1 "3.1 Salient
    Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")介绍显著权重的选择规则及其二值化策略。然后，我们在第[3.2节](#S3.SS2 "3.2 Bell-shaped
    Distribution Splitting for Binarization ‣ 3 Method ‣ BiLLM: Pushing the Limit
    of Post-Training Quantization for LLMs")详细说明基于分布的非显著权重二值化策略。'
- en: 3.1 Salient Weight Binarization for LLMs
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 LLMs的显著权重二值化
- en: 'In deep neural networks, not all parameters carry equal significance. Utilizing
    solely the magnitude of the weights can not fully capture the impact of each element
    on the model’s performance. The Hessian metric serves as a common benchmark for
    detecting parameter sensitivity (Dong et al., [2019](#bib.bib12); Dettmers et al.,
    [2023b](#bib.bib11), [2022](#bib.bib9)). We thus leverage the Hessian matrix to
    assess the salience of parameters in each under-binarized layer. We implement
    an optimized computation process to derive weight sensitivity, which allows us
    to obtain the importance metric of parameters without compromising efficiency:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度神经网络中，并非所有参数都具有相同的重要性。仅利用权重的幅度不能完全捕捉每个元素对模型性能的影响。Hessian度量作为检测参数敏感度的常见基准（Dong
    et al., [2019](#bib.bib12); Dettmers et al., [2023b](#bib.bib11), [2022](#bib.bib9)）。因此，我们利用Hessian矩阵评估每个未二值化层中参数的重要性。我们实施了优化的计算过程来导出权重敏感度，从而在不影响效率的情况下获取参数的重要性度量：
- en: '|  | $s_{i}=\frac{w_{i}^{2}}{[\mathbf{H}^{-1}]_{ii}^{2}},$ |  | (3) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{i}=\frac{w_{i}^{2}}{[\mathbf{H}^{-1}]_{ii}^{2}},$ |  | (3) |'
- en: where $\mathbf{H}$ serves as a criterion for assessing the significance of weight
    elements and is used as a feature indicator for structured selection.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\mathbf{H}$作为评估权重元素重要性的标准，并用作结构选择的特征指示符。
- en: 'Structural Searching Selection. Utilizing an unstructured selection enables
    the coverage of all salient elements. However, it requires the implementation
    of an additional 1-bit bitmap index (Chan & Ioannidis, [1998](#bib.bib4)), leading
    to increased average bit-width. This balance is inefficient, especially for Hessian
    outlier weights that constitute a mere 1-5% of the total (Yao et al., [2023](#bib.bib48)).
    In our analysis of sensitivity distribution within LLMs, we discovered that the
    majority of the weights’ sensitive Hessian values are predominantly concentrated
    in specific columns or rows (Appendix [G](#A7 "Appendix G Magnitude and Hessian
    Distribution of LLMs ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")). This pattern is attributed to the convergence effects inherent in
    the multi-head self-attention mechanism of these models and further motivates
    us to implement a structured approach for selecting salient weights, for reducing
    the additional bitmap. Given that BiLLM employs a per-channel (or per-row) type
    of binarization, we determine salience through a per-column segmentation on the
    whole weight matrix.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 结构化搜索选择。利用非结构化选择可以覆盖所有显著元素。然而，这需要实现额外的 1 位位图索引（Chan & Ioannidis, [1998](#bib.bib4)），导致平均位宽增加。这种平衡效率低下，特别是对于构成总数仅
    1-5% 的 Hessian 离群权重（Yao et al., [2023](#bib.bib48)）。在对 LLMs 的敏感性分布分析中，我们发现大多数权重的敏感
    Hessian 值主要集中在特定的列或行（附录 [G](#A7 "附录 G LLMs 的幅度和 Hessian 分布 ‣ BiLLM：推动 LLMs 后训练量化的极限")）。这种模式归因于这些模型的多头自注意力机制中固有的收敛效应，并进一步激励我们实施结构化方法来选择显著权重，以减少额外的位图。鉴于
    BiLLM 采用的是每通道（或每行）类型的二值化，我们通过对整个权重矩阵进行按列分割来确定显著性。
- en: 'We organize the column salience in descending order and introduce an optimized
    search algorithm aimed at minimizing quantization error, which in turn determines
    the number of columns within the salient group. To elaborate on this methodology,
    we initially define the objective of binarization quantization, grounded on Equation ([1](#S2.E1
    "Equation 1 ‣ 2.2 Network Binarization ‣ 2 Related Works ‣ BiLLM: Pushing the
    Limit of Post-Training Quantization for LLMs")):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将列显著性按降序排列，并引入一种优化搜索算法，旨在最小化量化误差，从而确定显著组中的列数。为了详细说明这一方法，我们首先基于方程式 ([1](#S2.E1
    "方程 1 ‣ 2.2 网络二值化 ‣ 2 相关工作 ‣ BiLLM：推动 LLMs 后训练量化的极限")) 定义二值化量化的目标：
- en: '|  | $\mathop{\arg\min}\limits_{\alpha,\mathbf{B}}&#124;&#124;\mathbf{W}-\alpha\mathbf{B}&#124;&#124;^{2},$
    |  | (4) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathop{\arg\min}\limits_{\alpha,\mathbf{B}}&#124;&#124;\mathbf{W}-\alpha\mathbf{B}&#124;&#124;^{2},$
    |  | (4) |'
- en: 'where $\mathbf{B}\in\{-1,+1\}^{k\times m}$. Then, the optimization function
    for selecting salient columns is defined as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{B}\in\{-1,+1\}^{k\times m}$。然后，选择显著列的优化函数定义为：
- en: '|  | $\mathop{\arg\min}\limits_{\mathbf{W}_{\text{uns}}}&#124;&#124;\mathbf{W}-(\alpha_{\text{sal}}\operatorname{sign}(\mathbf{W}_{\text{sal}})\cup\alpha_{\text{uns}}\operatorname{sign}(\mathbf{W}_{\text{uns}}))&#124;&#124;^{2},$
    |  | (5) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathop{\arg\min}\limits_{\mathbf{W}_{\text{uns}}}&#124;&#124;\mathbf{W}-(\alpha_{\text{sal}}\operatorname{sign}(\mathbf{W}_{\text{sal}})\cup\alpha_{\text{uns}}\operatorname{sign}(\mathbf{W}_{\text{uns}}))&#124;&#124;^{2},$
    |  | (5) |'
- en: where $\mathbf{W}_{\text{sal}}$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}_{\text{sal}}$。
- en: '![Refer to caption](img/580b8c3fe4a9d06096e46c60f31deb0b.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/580b8c3fe4a9d06096e46c60f31deb0b.png)'
- en: 'Figure 4: Illustration of salient weight binarization. The $\mathbf{B}_{1}$.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：显著权重二值化的示意图。$\mathbf{B}_{1}$。
- en: 'Binary Residual Approximation. Salient weights are limited in quantity, yet
    exhibit significant variance when aggregated. Direct preservation of these weights
    in INT8 or FP16 formats leads to an increase in the average weight bits, undermining
    the compressive benefits of binarization. Traditional binarization methods for
    salient weights, however, result in substantial quantization errors. To that end,
    we develop a residual approximation approach for binarizing salient weights. Contrary
    to the comprehensive high-order quantization (Li et al., [2017](#bib.bib24)) applied
    to the entire weight matrix, our technique minimizes binarization error through
    a second-order approximation of merely a select subset of salient weights. This
    method guarantees the precision of salient weights while simultaneously decreasing
    bit-width overhead. As illustrated in Figure [4](#S3.F4 "Figure 4 ‣ 3.1 Salient
    Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs"), this approach incorporates a recursive computation strategy
    for weight binarization compensation, applying a subsequent binarization process
    to the residuals remaining after the initial binary process. Building upon Equation ([4](#S3.E4
    "Equation 4 ‣ 3.1 Salient Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")), we propose a redesigned residual
    approximation optimization specifically for salient weights, which is defined
    as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '二进制残差近似。显著权重的数量有限，但在聚合时表现出显著的方差。直接在INT8或FP16格式中保存这些权重会增加平均权重位数，削弱二值化的压缩效益。然而，传统的显著权重二值化方法会导致显著的量化误差。为此，我们开发了一种用于二值化显著权重的残差近似方法。与应用于整个权重矩阵的全面高阶量化（Li
    et al., [2017](#bib.bib24)）不同，我们的技术通过对仅选择的显著权重子集进行二阶近似来最小化二值化误差。这种方法在减少比特宽度开销的同时，保证了显著权重的精度。如图
    [4](#S3.F4 "Figure 4 ‣ 3.1 Salient Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs") 所示，这种方法包括了一种递归计算策略用于权重二值化补偿，在初始二进制过程之后，对剩余的残差进行后续的二值化处理。基于方程
    ([4](#S3.E4 "Equation 4 ‣ 3.1 Salient Weight Binarization for LLMs ‣ 3 Method
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))，我们提出了一种专门针对显著权重的重新设计的残差近似优化，其定义如下：'
- en: '|  | $$\left\{\begin{array}[]{lr}\alpha_{o}^{*},\mathbf{B}_{o}^{*}=\mathop{\arg\min}\limits_{\alpha_{o},\mathbf{B}_{o}}&#124;&#124;\mathbf{W}-\alpha_{o}\mathbf{B}_{o}&#124;&#124;^{2}),\\
    \alpha_{r}^{*},\mathbf{B}_{r}^{*}=\mathop{\arg\min}\limits_{\alpha_{r},\mathbf{B}_{r}}&#124;&#124;(\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*})-\alpha_{r}\mathbf{B}_{r}&#124;&#124;^{2}),\\'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\left\{\begin{array}[]{lr}\alpha_{o}^{*},\mathbf{B}_{o}^{*}=\mathop{\arg\min}\limits_{\alpha_{o},\mathbf{B}_{o}}&#124;&#124;\mathbf{W}-\alpha_{o}\mathbf{B}_{o}&#124;&#124;^{2}),\\
    \alpha_{r}^{*},\mathbf{B}_{r}^{*}=\mathop{\arg\min}\limits_{\alpha_{r},\mathbf{B}_{r}}&#124;&#124;(\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*})-\alpha_{r}\mathbf{B}_{r}&#124;&#124;^{2}),\\'
- en: \end{array}\right.$$ |  | (6) |
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right.$$ |  | (6) |
- en: 'where $\mathbf{B}_{o}$. We efficiently solve for the two binarized optimization
    objectives using the same solution method as in Equation ([4](#S3.E4 "Equation
    4 ‣ 3.1 Salient Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing the Limit
    of Post-Training Quantization for LLMs")). Ultimately, we arrive at the following
    approximation:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{B}_{o}$。我们使用与方程 ([4](#S3.E4 "Equation 4 ‣ 3.1 Salient Weight Binarization
    for LLMs ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training Quantization for
    LLMs")) 相同的解法来有效地求解两个二值化优化目标。最终，我们得出了以下近似值：'
- en: '|  | $\mathbf{W}\approx\alpha_{o}^{*}\mathbf{B}_{o}^{*}+\alpha_{r}^{*}\mathbf{B}_{r}^{*}.$
    |  | (7) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{W}\approx\alpha_{o}^{*}\mathbf{B}_{o}^{*}+\alpha_{r}^{*}\mathbf{B}_{r}^{*}.$
    |  | (7) |'
- en: 'It can be easily proven that the residual approach of Equation ([7](#S3.E7
    "Equation 7 ‣ 3.1 Salient Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")) has a lower quantization error
    than the direct one of Equation ([4](#S3.E4 "Equation 4 ‣ 3.1 Salient Weight Binarization
    for LLMs ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training Quantization for
    LLMs")). We define the residual binarization error $\mathcal{E}$:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '可以很容易证明方程 ([7](#S3.E7 "Equation 7 ‣ 3.1 Salient Weight Binarization for LLMs
    ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))
    的残差方法具有比方程 ([4](#S3.E4 "Equation 4 ‣ 3.1 Salient Weight Binarization for LLMs
    ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))
    直接方法更低的量化误差。我们定义残差二值化误差 $\mathcal{E}$：'
- en: '|  | $\mathcal{E}_{rb}=&#124;&#124;\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*}-\alpha_{r}^{*}\mathbf{B}_{r}^{*}&#124;&#124;^{2}.$
    |  | (8) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{E}_{rb}=&#124;&#124;\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*}-\alpha_{r}^{*}\mathbf{B}_{r}^{*}&#124;&#124;^{2}.$
    |  | (8) |'
- en: The original binarized quantization error is calculatde as $||\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*}||^{2}$.
    Therefore, through the method of residual approximation, we are able to further
    reduce the binary quantization error of salient weights with ultra-low bit-width
    storage compared to retaining salient weights at 8 or 16 bits.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的二值化量化误差计算为 $||\mathbf{W}-\alpha_{o}^{*}\mathbf{B}_{o}^{*}||^{2}$。因此，通过残差近似法，我们能够进一步减少重要权重的二值化量化误差，相比于将重要权重保留为
    8 位或 16 位。
- en: '![Refer to caption](img/2b3002317f466872f8854396925d9aba.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2b3002317f466872f8854396925d9aba.png)'
- en: 'Figure 5: Distribution and splitting schematic of the $4^{th}$ projection layer
    in LLaMA2-7B. The top 5% of the Hessian elements are orange, and the optimal break-point
    divides the non-salient weights into sparse and concentrated areas.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：LLaMA2-7B 中第 $4$ 层投影的分布和拆分示意图。Hessian 元素的前 5% 是橙色的，最佳断点将非重要权重划分为稀疏区和集中区。
- en: 3.2 Bell-shaped Distribution Splitting for Binarization
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 钟形分布拆分用于二值化
- en: 'Following the removal of salient weights, the remaining weights maintain a
    bell-shaped distribution, which becomes closer to symmetric with the exclusion
    of salient weights’ impact, as depicted in Figure [5](#S3.F5 "Figure 5 ‣ 3.1 Salient
    Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs"). Binary quantization, representing an extreme form of
    uniform quantization, encounters more loss in the presence of non-uniform distributions.
    A practical approach involves the group-wise quantization (Park et al., [2018](#bib.bib30);
    Fang et al., [2020](#bib.bib13); Jain et al., [2019](#bib.bib20)) of weights according
    to their distribution. Balancing between quantization accuracy and compression
    efficiency, we identify a single break-point within the distribution. As shown
    in Figure [5](#S3.F5 "Figure 5 ‣ 3.1 Salient Weight Binarization for LLMs ‣ 3
    Method ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"), this
    partition divides the non-salient bell-shaped distribution into two categories:
    the sparse area and the concentrated area.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '在去除重要权重后，剩余权重保持钟形分布，随着重要权重影响的排除，这一分布变得更接近对称，如图 [5](#S3.F5 "图 5 ‣ 3.1 重要权重二值化
    ‣ 3 方法 ‣ BiLLM: 推动 LLM 后训练量化的极限") 所示。二值化作为均匀量化的一种极端形式，在非均匀分布的情况下会遇到更多损失。实际方法涉及根据权重分布进行组内量化
    (Park et al., [2018](#bib.bib30); Fang et al., [2020](#bib.bib13); Jain et al.,
    [2019](#bib.bib20))。在量化精度和压缩效率之间取得平衡，我们在分布中识别出一个断点。如图 [5](#S3.F5 "图 5 ‣ 3.1 重要权重二值化
    ‣ 3 方法 ‣ BiLLM: 推动 LLM 后训练量化的极限") 所示，这一划分将非重要的钟形分布分为两类：稀疏区和集中区。'
- en: 'The segmentation process identifies a break-point that categorizes non-salient
    weights into two groups: $A_{c}[-p,p]$. Then the mean squared quantization error
    of binarization is defined as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分割过程识别出一个断点，将非重要权重分为两组：$A_{c}[-p,p]$。然后定义了二值化的均方量化误差：
- en: '|  | $1$2 |  | (9) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: 'Since $g(x)$ is a symmetric function, the above formula is simplified to:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $g(x)$ 是对称函数，上述公式简化为：
- en: '|  | $\theta_{q}^{2}=2\int_{0}^{m}(\alpha-x)^{2}g(x)dx.$ |  | (10) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{q}^{2}=2\int_{0}^{m}(\alpha-x)^{2}g(x)dx.$ |  | (10) |'
- en: 'Then, the break-point $p$ divides the non-salient weights into two parts. According
    to the Equation ([10](#S3.E10 "Equation 10 ‣ 3.2 Bell-shaped Distribution Splitting
    for Binarization ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")), under the discontinuous weight distribution, we get a new binary
    quantization error:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，断点 $p$ 将非重要权重分为两部分。根据公式 ([10](#S3.E10 "公式 10 ‣ 3.2 钟形分布拆分 ‣ 3 方法 ‣ BiLLM:
    推动 LLM 后训练量化的极限"))，在不连续的权重分布下，我们得到一个新的二值化量化误差：'
- en: '|  | $\theta_{q,p}^{2}=&#124;&#124;\mathbf{W}_{s}-\alpha_{s}\mathbf{B}_{s}&#124;&#124;^{2}+&#124;&#124;\mathbf{W}_{c}-\alpha_{c}\mathbf{B}_{c}&#124;&#124;^{2},$
    |  | (11) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta_{q,p}^{2}=&#124;&#124;\mathbf{W}_{s}-\alpha_{s}\mathbf{B}_{s}&#124;&#124;^{2}+&#124;&#124;\mathbf{W}_{c}-\alpha_{c}\mathbf{B}_{c}&#124;&#124;^{2},$
    |  | (11) |'
- en: 'where $\mathbf{W}_{s}$ are the binarization scales, determined by Equation ([4](#S3.E4
    "Equation 4 ‣ 3.1 Salient Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")):'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{W}_{s}$ 是通过公式 ([4](#S3.E4 "公式 4 ‣ 3.1 重要权重二值化 ‣ 3 方法 ‣ BiLLM: 推动
    LLM 后训练量化的极限")) 确定的二值化尺度：'
- en: '|  | $\alpha_{s}=\frac{1}{n_{s}}&#124;&#124;\mathbf{W}_{s}&#124;&#124;_{\ell
    1},\alpha_{c}=\frac{1}{n_{c}}&#124;&#124;\mathbf{W}_{c}&#124;&#124;_{\ell 1},$
    |  | (12) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha_{s}=\frac{1}{n_{s}}&#124;&#124;\mathbf{W}_{s}&#124;&#124;_{\ell
    1},\alpha_{c}=\frac{1}{n_{c}}&#124;&#124;\mathbf{W}_{c}&#124;&#124;_{\ell 1},$
    |  | (12) |'
- en: 'where $n$ can be defined as:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: $n$ 可以定义为：
- en: '|  | $p^{*}=\mathop{\arg\min}_{p}(\theta_{q,p}^{2}).$ |  | (13) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $p^{*}=\mathop{\arg\min}_{p}(\theta_{q,p}^{2}).$ |  | (13) |'
- en: 'When the remaining weights follow an ideal Gaussian distribution, Equation ([11](#S3.E11
    "Equation 11 ‣ 3.2 Bell-shaped Distribution Splitting for Binarization ‣ 3 Method
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs")) is demonstrated
    to be a convex function with a global minimum, as evidenced in prior studies (Fang
    et al., [2020](#bib.bib13); You, [2010](#bib.bib49)). Nonetheless, the actual
    distribution of non-salient weights, while bell-shaped, diverges from the ideal
    Gaussian model. Simultaneously, we retain the block-wise compensation strategies
    of GPTQ (Frantar et al., [2022](#bib.bib17)) and OBC (Frantar & Alistarh, [2022](#bib.bib15))
    to offset quantization errors, which could change the distribution of weights.
    In response, we employ a percentile search method to identify the optimal break-point
    based on the objective function outlined in Equation ([13](#S3.E13 "Equation 13
    ‣ 3.2 Bell-shaped Distribution Splitting for Binarization ‣ 3 Method ‣ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs")). This percentile search
    strategy is efficient and straightforward, completing the binarization process
    for a 7B LLM within merely 30 minutes. Furthermore, our findings indicate that
    despite the deviation of non-salient weights from the ideal Gaussian distribution,
    the error curve associated with the search process still exhibits convex properties
    (as detailed in Appendix [C](#A3 "Appendix C Searching Curve of Salient Column
    and Non-salient Distribution ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")), confirming the feasibility of pinpointing the optimal break-point.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '当剩余权重遵循理想的高斯分布时，方程 ([11](#S3.E11 "Equation 11 ‣ 3.2 Bell-shaped Distribution
    Splitting for Binarization ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")) 被证明是一个具有全局最小值的凸函数，如之前的研究所示 (Fang et al., [2020](#bib.bib13);
    You, [2010](#bib.bib49))。然而，尽管非显著权重的分布呈钟形，但它偏离了理想的高斯模型。同时，我们保留了 GPTQ (Frantar
    et al., [2022](#bib.bib17)) 和 OBC (Frantar & Alistarh, [2022](#bib.bib15)) 的块级补偿策略来抵消量化误差，这可能会改变权重的分布。因此，我们采用百分位搜索方法来确定基于方程
    ([13](#S3.E13 "Equation 13 ‣ 3.2 Bell-shaped Distribution Splitting for Binarization
    ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))
    中列出的目标函数的最佳断点。这种百分位搜索策略高效且简单，在仅 30 分钟内完成 7B LLM 的二值化。此外，我们的研究表明，尽管非显著权重偏离了理想的高斯分布，搜索过程中相关的误差曲线仍然显示出凸性（详见附录
    [C](#A3 "Appendix C Searching Curve of Salient Column and Non-salient Distribution
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs")），确认了定位最佳断点的可行性。'
- en: 3.3 Pipeline of BiLLM
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 BiLLM 的流程
- en: 'As depicted in Figure [3](#S2.F3 "Figure 3 ‣ 2 Related Works ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") left, BiLLM primarily performs
    binary quantization on all Linear weights within the Transformer blocks. This
    section introduces the detailed pipeline of BiLLM.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [3](#S2.F3 "Figure 3 ‣ 2 Related Works ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs") 左侧所示，BiLLM 主要对 Transformer 块中的所有线性权重执行二值化。本节介绍了 BiLLM
    的详细流程。'
- en: 'Binarization Workflow. We first deploy the structural search of salient columns
    and a residual approximation binarization for salient columns. The process of
    salient columns incurs additional weight bits due to the search proportion and
    residual mechanism. Table [1](#S3.T1 "Table 1 ‣ 3.3 Pipeline of BiLLM ‣ 3 Method
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") presents the
    extra bits generated in some LLMs (Zhang et al., [2022](#bib.bib51); Touvron et al.,
    [2023a](#bib.bib40), [b](#bib.bib41)). It can be observed that the searching and
    residuals bring only about 0.1 additional weight bits. Then, for these non-uniformly
    distributed weights, we use a split binarization strategy searching optimal $p^{*}$.
    The concentrated area and the sparse area are binarized separately. This part
    incurs the cost of an additional 1 bit for hardware group identification, but
    the computing parameters are still compressed to 1 bit. By retaining only block-wise
    compensation(Frantar et al., [2022](#bib.bib17); Frantar & Alistarh, [2022](#bib.bib15))
    and eliminating column-wise quantization error compensation, we further enhance
    the efficiency of PTQ and ensure the effectiveness of distribution exploration.
    Algorithm [1](#alg1 "Algorithm 1 ‣ 3.3 Pipeline of BiLLM ‣ 3 Method ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") illustrates the complete process
    of BiLLM, and detailed implementation of BiLLM is shown in Appendix [A](#A1 "Appendix
    A BiLLM Implementation ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs").'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '二值化工作流程。我们首先部署了显著列的结构搜索和显著列的残差近似二值化。显著列的过程由于搜索比例和残差机制引入了额外的权重位。表[1](#S3.T1
    "Table 1 ‣ 3.3 Pipeline of BiLLM ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")展示了在一些LLMs中生成的额外位（Zhang et al., [2022](#bib.bib51)；Touvron
    et al., [2023a](#bib.bib40)，[b](#bib.bib41)）。可以观察到，搜索和残差仅带来了大约0.1个额外的权重位。然后，对于这些不均匀分布的权重，我们使用了拆分二值化策略来搜索最优的$p^{*}$。集中区域和稀疏区域分别进行二值化。这部分会增加额外的1个位用于硬件组识别，但计算参数仍压缩为1个位。通过仅保留块级补偿（Frantar
    et al., [2022](#bib.bib17)；Frantar & Alistarh, [2022](#bib.bib15)）并消除列级量化误差补偿，我们进一步提高了PTQ的效率，确保了分布探索的有效性。算法[1](#alg1
    "Algorithm 1 ‣ 3.3 Pipeline of BiLLM ‣ 3 Method ‣ BiLLM: Pushing the Limit of
    Post-Training Quantization for LLMs")展示了BiLLM的完整过程，BiLLM的详细实现见附录[A](#A1 "Appendix
    A BiLLM Implementation ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")。'
- en: '![Refer to caption](img/e9ee08fcb900c5da7bdbbdc830467085.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e9ee08fcb900c5da7bdbbdc830467085.png)'
- en: 'Figure 6: Weights and hardware overhead changes on Llama-7B. The left picture
    shows the calculation parameters as a function of the significant weight ratio;
    the right picture shows the hardware overhead as a function of the block.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：Llama-7B上的权重和硬件开销变化。左图显示了显著权重比率作为函数的计算参数；右图显示了作为块函数的硬件开销。
- en: 'Table 1: Average bit results from structural searching and residual binarization
    of OPT, LLaMA, and LLaMA2 families.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：OPT、LLaMA和LLaMA2系列的结构搜索和残差二值化的平均位结果。
- en: '| Model | 7B | 13B | 30B | 66B/65B/70B* |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 7B | 13B | 30B | 66B/65B/70B* |'
- en: '| OPT | 1.10 | 1.12 | 1.12 | 1.13 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| OPT | 1.10 | 1.12 | 1.12 | 1.13 |'
- en: '| LLaMA | 1.09 | 1.09 | 1.10 | 1.10 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | 1.09 | 1.09 | 1.10 | 1.10 |'
- en: '| LLaMA2 | 1.07 | 1.08 | N/A | 1.09 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 | 1.07 | 1.08 | N/A | 1.09 |'
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '*: OPT-66B, LLaMA-65B and LLaMA2-70B.'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*: OPT-66B、LLaMA-65B 和 LLaMA2-70B。'
- en: 'Extra Storing Bits. The extra bits is acceptable under the binary weight quantization
    of BiLLM. The weight parameters and additional hardware overhead are as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 额外存储位。在BiLLM的二值权重量化下，额外的位是可以接受的。权重参数和额外的硬件开销如下：
- en: '|  | $$\left\{\begin{array}[]{lr}N_{\text{param}}=2\times r_{\text{salient}}+1\times(1-r_{\text{salient}}),\\
    N_{\text{storing}}=1+\dfrac{1}{b_{\text{size}}},\\'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | $$\left\{\begin{array}[]{lr}N_{\text{param}}=2\times r_{\text{salient}}+1\times(1-r_{\text{salient}}),\\
    N_{\text{storing}}=1+\dfrac{1}{b_{\text{size}}},\\'
- en: \end{array}\right.$$ |  | (14) |
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: \end{array}\right.$$ |  | (14) |
- en: 'where $r_{salient}$ represents the identifier for the structured column of
    salient weights. For example, a 10% structural selection along with an OBC compensation
    of size 128 was employed. This results in a weight parameter bit-width of 1.1
    bits and a hardware flag bit-width of 1.008 bits. Figure [6](#S3.F6 "Figure 6
    ‣ 3.3 Pipeline of BiLLM ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs") illustrates the weight overhead for different proportions
    and block sizes. It is important to note that flag weights do not participate
    in the computation; actual calculations are executed solely with parameter weights.
    Therefore, additional hardware identification bits do not affect the acceleration
    effect of binary quantization.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r_{salient}$ 代表显著权重的结构化列标识符。例如，使用了10%的结构选择以及大小为128的OBC补偿。这导致权重参数的位宽为1.1位，硬件标志位宽为1.008位。图 [6](#S3.F6
    "图 6 ‣ 3.3 BiLLM 流程 ‣ 3 方法 ‣ BiLLM：推动后训练量化的极限") 说明了不同比例和块大小的权重开销。需要注意的是，标志权重不参与计算；实际计算仅使用参数权重。因此，额外的硬件识别位不会影响二进制量化的加速效果。
- en: 'Algorithm 1 Main Framework of BiLLM: Inner details of each function are shown
    in Algorithm [2](#alg2 "Algorithm 2 ‣ Appendix A BiLLM Implementation ‣ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs")'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 BiLLM 的主要框架：每个函数的内部细节在算法 [2](#alg2 "算法 2 ‣ 附录 A BiLLM 实现 ‣ BiLLM：推动后训练量化的极限")
    中展示
- en: func $\operatorname{BinaryLLM}$)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: func $\operatorname{BinaryLLM}$)
- en: 'Input: $\mathbf{W}\in\mathbb{R}^{n\times m}$ - weight matrix'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '输入: $\mathbf{W}\in\mathbb{R}^{n\times m}$ - 权重矩阵'
- en: $\mathbf{X}\in\mathbb{R}^{r\times d}$ - calibration data
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathbf{X}\in\mathbb{R}^{r\times d}$ - 校准数据
- en: $\beta$ - block size
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: $\beta$ - 块大小
- en: $\lambda$ - hessian regularizer
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: $\lambda$ - Hessian 正则化器
- en: 'Output: $\mathbf{B}$ - binarized weights'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '输出: $\mathbf{B}$ - 二值化权重'
- en: 1:  $\mathbf{H}\coloneqq 2\mathbf{X}\mathbf{X}^{\top}\ $
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  $\mathbf{H}\coloneqq 2\mathbf{X}\mathbf{X}^{\top}\ $'
- en: 4 Experiments
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Setup
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: We deploy BiLLM within the Pytorch (Paszke et al., [2019](#bib.bib31))-Huggingface
    libraries (Wolf et al., [2019](#bib.bib45)). All the binarization processes and
    experiments are conducted on a single 80 GB NVIDIA A100\. Given that BiLLM is
    an efficient PTQ framework, it eliminates the need for any fine-tuning, allowing
    for completion through a single quantization process.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Pytorch (Paszke et al., [2019](#bib.bib31))-Huggingface 库 (Wolf et al.,
    [2019](#bib.bib45)) 中部署了 BiLLM。所有的二值化过程和实验都在一台80 GB NVIDIA A100上进行。由于 BiLLM 是一个高效的
    PTQ 框架，它消除了任何微调的需要，通过一次量化过程即可完成。
- en: 'Table 2: Perplexity of RTN, GPTQ, PB-LLM, and BiLLM on OPT Family. The columns
    represent the perplexity results on Wikitext2 datasets with different model sizes.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: RTN、GPTQ、PB-LLM 和 BiLLM 在 OPT 系列上的困惑度。列表示不同模型大小的 Wikitext2 数据集上的困惑度结果。'
- en: '| Method | Block Size | Weight Bits | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 块大小 | 权重位 | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B |'
- en: '| Full Precision | - | 16.00 | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 | 9.34
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 全精度 | - | 16.00 | 14.62 | 12.47 | 10.86 | 10.13 | 9.56 | 9.34 |'
- en: '| RTN | - | 3.00 | 13337.38 | 15594.72 | 5797.32 | 3357.01 | 1566.00 | 6126.09
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| RTN | - | 3.00 | 13337.38 | 15594.72 | 5797.32 | 3357.01 | 1566.00 | 6126.09
    |'
- en: '| GPTQ | 128 | 3.00 | 20.97 | 16.88 | 14.86 | 11.61 | 10.27 | 10.51 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 128 | 3.00 | 20.97 | 16.88 | 14.86 | 11.61 | 10.27 | 10.51 |'
- en: '| \hdashlineRTN | - | 2.00 | 11272.65 | 9505.76 | 28363.14 | 194086.78 | 169616.47
    | 1165864.25 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| \hdashlineRTN | - | 2.00 | 11272.65 | 9505.76 | 28363.14 | 194086.78 | 169616.47
    | 1165864.25 |'
- en: '| GPTQ | 128 | 2.00 | 115.17 | 61.59 | 50.19 | 21.36 | 15.71 | 82.10 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 128 | 2.00 | 115.17 | 61.59 | 50.19 | 21.36 | 15.71 | 82.10 |'
- en: '| RTN | - | 1.00 | 17165.72 | 36516.69 | 11550.91 | 6986.35 | 6485.99 | 184796.30
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| RTN | - | 1.00 | 17165.72 | 36516.69 | 11550.91 | 6986.35 | 6485.99 | 184796.30
    |'
- en: '| GPTQ | 128 | 1.00 | 14884.73 | 14144.58 | 10622.81 | 15196.96 | 12478.37
    | 13106.45 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 128 | 1.00 | 14884.73 | 14144.58 | 10622.81 | 15196.96 | 12478.37
    | 13106.45 |'
- en: '| PB-LLM $\dagger$ | 128 | 1.70 | 265.52 | 124.35 | 105.16 | 81.92 | 25.14
    | 29.09 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| PB-LLM $\dagger$ | 128 | 1.70 | 265.52 | 124.35 | 105.16 | 81.92 | 25.14
    | 29.09 |'
- en: '| BiLLM  $\ddagger$ | 128 | 1.11 | 69.97 | 49.55 | 35.36 | 18.82 | 12.71 |
    12.06 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| BiLLM  $\ddagger$ | 128 | 1.11 | 69.97 | 49.55 | 35.36 | 18.82 | 12.71 |
    12.06 |'
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '-: Vanilla RTN conducts layer-wise quantization. $\dagger$: BiLLM uses structural
    searching for salient weights. The table gives the average bit-width of the OPT
    family.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-: Vanilla RTN 执行逐层量化。 $\dagger$: BiLLM 使用结构化搜索显著权重。表格显示了 OPT 系列的平均位宽。'
- en: 'Table 3: Perplexity of RTN, GPTQ, PB-LLM, BiLLM on LLaMA Family. The columns
    represent the perplexity results on Wikitext2 datasets with different model sizes.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: RTN、GPTQ、PB-LLM、BiLLM 在 LLaMA 系列上的困惑度。列表示不同模型大小的 Wikitext2 数据集上的困惑度结果。'
- en: '| Model | Method | Block Size | Weight Bits | 7B | 13B | 30B | 65B/70B* |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 块大小 | 权重位数 | 7B | 13B | 30B | 65B/70B* |'
- en: '|  | Full Precision | - | 16.00 | 5.68 | 5.09 | 4.10 | 3.53 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | 全精度 | - | 16.00 | 5.68 | 5.09 | 4.10 | 3.53 |'
- en: '|  | RTN | - | 2.00 | 106767.34 | 57409.93 | 26704.36 | 19832.87 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | - | 2.00 | 106767.34 | 57409.93 | 26704.36 | 19832.87 |'
- en: '|  | GPTQ | 128 | 2.00 | 152.31 | 20.44 | 13.01 | 8.78 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 128 | 2.00 | 152.31 | 20.44 | 13.01 | 8.78 |'
- en: '| LLaMA | RTN | - | 1.00 | 168388.00 | 1412020.25 | 14681.76 | 65253.24 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | RTN | - | 1.00 | 168388.00 | 1412020.25 | 14681.76 | 65253.24 |'
- en: '|  | GPTQ | 128 | 1.00 | 267001.72 | 113894.12 | 67093.73 | 25082.88 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 128 | 1.00 | 267001.72 | 113894.12 | 67093.73 | 25082.88 |'
- en: '|  | PB-LLM $\dagger$ | 128 | 1.70 | 102.36 | 36.60 | 33.67 | 12.53 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | PB-LLM $\dagger$ | 128 | 1.70 | 102.36 | 36.60 | 33.67 | 12.53 |'
- en: '|  | BiLLM  $\ddagger$ | 128 | 1.09 | 35.04 | 15.14 | 10.52 | 8.49 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM  $\ddagger$ | 128 | 1.09 | 35.04 | 15.14 | 10.52 | 8.49 |'
- en: '|  | Full Precision | - | 16.00 | 5.47 | 4.88 | N/A | 3.32 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | 全精度 | - | 16.00 | 5.47 | 4.88 | 不适用 | 3.32 |'
- en: '|  | RTN | - | 2.00 | 17788.93 | 51145.61 | N/A | 26066.13 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | - | 2.00 | 17788.93 | 51145.61 | 不适用 | 26066.13 |'
- en: '|  | GPTQ | 128 | 2.00 | 60.45 | 19.70 | N/A | 9.12 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 128 | 2.00 | 60.45 | 19.70 | 不适用 | 9.12 |'
- en: '| LLaMA2 | RTN | - | 1.00 | 157058.34 | 47902.32 | N/A | 160389.91 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2 | RTN | - | 1.00 | 157058.34 | 47902.32 | 不适用 | 160389.91 |'
- en: '|  | GPTQ | 128 | 1.00 | 115905.67 | 9387.80 | N/A | 74395.42 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 128 | 1.00 | 115905.67 | 9387.80 | 不适用 | 74395.42 |'
- en: '|  | PB-LLM $\dagger$ | 128 | 1.70 | 69.20 | 151.09 | N/A | 28.37 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|  | PB-LLM $\dagger$ | 128 | 1.70 | 69.20 | 151.09 | 不适用 | 28.37 |'
- en: '|  | BiLLM  $\ddagger$ | 128 | 1.08 | 32.48 | 16.77 | N/A | 8.41 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM  $\ddagger$ | 128 | 1.08 | 32.48 | 16.77 | 不适用 | 8.41 |'
- en: •
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The table gives the average bit-width of the LLaMA family. N/A: LLaMA2 do not
    have 30B version. *: LLaMA has 65B version and LLaMA2 has 70B version.'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表格展示了LLaMA家族的平均位宽。不适用：LLaMA2没有30B版本。*：LLaMA有65B版本，LLaMA2有70B版本。
- en: 'Models and Datasets. We facilitate our method on the OPT (Zhang et al., [2022](#bib.bib51))
    and LLaMA (Touvron et al., [2023a](#bib.bib40), [b](#bib.bib41)) families. Additionally,
    considering the customary need for instruction-based fine-tuning of LLMs to adapt
    to varying contexts, we also conducted experiments on Vicuna (Chiang et al., [2023](#bib.bib5)).
    In terms of evaluation metrics, we mainly focused on the perplexity of LLMs’ outputs,
    which is widely acknowledged in prior studies as a challenging yet stable indicator
    of LLM capabilities, particularly apt for network compression  ([Yao et al.,](#bib.bib47)
    ; Frantar et al., [2022](#bib.bib17); Frantar & Alistarh, [2023](#bib.bib16);
    Xiao et al., [2023](#bib.bib46)). We consider the test of WikiText2 (Merity et al.,
    [2016](#bib.bib28)), PTB (Marcus et al., [1994](#bib.bib27)), as well as a part
    of the C4 (Raffel et al., [2020](#bib.bib35)) data. Then, we further conduct the
    experiments on seven zero-shot evaluation tasks (PIQA (Bisk et al., [2020](#bib.bib2)),
    BoolQ (Clark et al., [2019](#bib.bib6)), OBQA (Mihaylov et al., [2018](#bib.bib29)),
    Winogrande (Sakaguchi et al., [2021](#bib.bib37)), ARC-e (Clark et al., [2018](#bib.bib7)),
    ARC-c (Clark et al., [2018](#bib.bib7)) Hellaswag (Zellers et al., [2019](#bib.bib50)))
    in the Appendix [D](#A4 "Appendix D Multi-evaluation Comparisons ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs"), further verifying the robustness
    of our proposed BiLLM to the binarization of LLMs.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。我们在OPT（Zhang et al., [2022](#bib.bib51)）和LLaMA（Touvron et al., [2023a](#bib.bib40)，[b](#bib.bib41)）家族上应用了我们的方法。此外，考虑到LLMs在不同上下文中需要基于指令的微调，我们还在Vicuna（Chiang
    et al., [2023](#bib.bib5)）上进行了实验。在评估指标方面，我们主要关注LLMs输出的困惑度，这在之前的研究中被广泛认可为LLM能力的具有挑战性但稳定的指标，特别适用于网络压缩（[Yao
    et al.,](#bib.bib47)；Frantar et al., [2022](#bib.bib17)；Frantar & Alistarh, [2023](#bib.bib16)；Xiao
    et al., [2023](#bib.bib46)）。我们考虑了WikiText2（Merity et al., [2016](#bib.bib28)）、PTB（Marcus
    et al., [1994](#bib.bib27)）以及部分C4（Raffel et al., [2020](#bib.bib35)）数据。然后，我们进一步在七个零样本评估任务（PIQA（Bisk
    et al., [2020](#bib.bib2)）、BoolQ（Clark et al., [2019](#bib.bib6)）、OBQA（Mihaylov
    et al., [2018](#bib.bib29)）、Winogrande（Sakaguchi et al., [2021](#bib.bib37)）、ARC-e（Clark
    et al., [2018](#bib.bib7)）、ARC-c（Clark et al., [2018](#bib.bib7)）Hellaswag（Zellers
    et al., [2019](#bib.bib50)））上进行了实验，进一步验证了我们提出的BiLLM对LLMs二值化的鲁棒性。
- en: Baseline. Our primary baseline is PB-LLM (Shang et al., [2023](#bib.bib39)),
    the most recent PTQ approach on binary LLMs. GPTQ (Frantar et al., [2022](#bib.bib17))
    and vanilla RTN are also selected. GPTQ is currently the advanced technology in
    PTQ, and many works(Lin et al., [2023](#bib.bib25); Dettmers et al., [2023b](#bib.bib11);
    Shang et al., [2023](#bib.bib39)) choose it as the baseline. Other methods oriented
    towards 8-bit and 4-bit quantization are deemed unsuitable for binarization and
    were thus not considered.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。我们的主要基线是 PB-LLM（Shang 等，[2023](#bib.bib39)），这是最新的二进制 LLM PTQ 方法。GPTQ（Frantar
    等，[2022](#bib.bib17)）和原始 RTN 也被选择。GPTQ 目前是 PTQ 的先进技术，许多工作（Lin 等，[2023](#bib.bib25)；Dettmers
    等，[2023b](#bib.bib11)；Shang 等，[2023](#bib.bib39)）将其作为基线。其他面向 8 位和 4 位量化的方法被认为不适合二值化，因此未予考虑。
- en: 4.2 Results
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果
- en: 'Comparison results. We conduct a meticulous comparison of the binary performance
    of different LLMs across various model sizes. We deploy the BiLLM on the OPT models (Zhang
    et al., [2022](#bib.bib51)) under the condition of a block size equal to 128\.
    As seen in Table [2](#S4.T2 "Table 2 ‣ 4.1 Setup ‣ 4 Experiments ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs"), the model outputs under the
    RTN and GPTQ methods have already collapsed at 1-bit weights, whereas BiLLM still
    maintains reasonable linguistic output capabilities with an average weight of
    1.1 bits. In comparison with PB-LLM at 1.7 bits, our method achieves a 35% reduction
    in weight bit-width while enhancing the performance of different sizes of the
    OPT model by 49.4% to 77.0%. It is noteworthy that when the parameter size exceeds
    30B, BiLLM can achieve performance nearly equivalent to that of GPTQ with 3-bit
    quantization.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '比较结果。我们对不同 LLM 在各种模型尺寸下的二进制性能进行了细致比较。我们在块大小为 128 的条件下将 BiLLM 部署在 OPT 模型上（Zhang
    等，[2022](#bib.bib51)）。如表 [2](#S4.T2 "Table 2 ‣ 4.1 Setup ‣ 4 Experiments ‣ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs") 所示，RTN 和 GPTQ 方法下的模型输出在
    1 位权重时已经崩溃，而 BiLLM 仍然以平均 1.1 位的权重保持了合理的语言输出能力。与 1.7 位的 PB-LLM 相比，我们的方法在降低 35%
    权重位宽的同时，将不同尺寸的 OPT 模型性能提高了 49.4% 至 77.0%。值得注意的是，当参数大小超过 30B 时，BiLLM 的性能几乎可以与 3
    位量化的 GPTQ 相媲美。'
- en: '![Refer to caption](img/1b91bf301c37bded51868bfe7f6d0e76.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/1b91bf301c37bded51868bfe7f6d0e76.png)'
- en: 'Figure 7: GPTQ, PB-LLM, BiLLM performed on the PTB and c4 datasets, mainly
    on LLaMA-7B, LLaMA2-7B, and OPT-6.7B, and we found that BiLLM performed relatively
    well.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：GPTQ、PB-LLM、BiLLM 在 PTB 和 c4 数据集上的表现，主要在 LLaMA-7B、LLaMA2-7B 和 OPT-6.7B 上进行，我们发现
    BiLLM 的表现相对较好。
- en: 'Table 4: Perplexity of BiLLM on Vicuna-7B and Vicuna-13B. The columns of different
    models represent the perplexity results on Wikitext2, PTB, and C4 datasets. The
    block size is set to 128.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：BiLLM 在 Vicuna-7B 和 Vicuna-13B 上的困惑度。不同模型的列表示了在 Wikitext2、PTB 和 C4 数据集上的困惑度结果。块大小设置为
    128。
- en: '| Model | Method | Weight Bits | Wiki -text2 $\downarrow$ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 权重位数 | Wiki -text2 $\downarrow$ |'
- en: '|  | GPTQ | 2.00 | 109.56 | 6227.73 | 64.28 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 109.56 | 6227.73 | 64.28 |'
- en: '| Vicuna-7B | PB-LLM | 1.70 | 68.01 | 477.52 | 67.23 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-7B | PB-LLM | 1.70 | 68.01 | 477.52 | 67.23 |'
- en: '|  | BiLLM | 1.08 | 33.00 | 332.17 | 36.24 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.08 | 33.00 | 332.17 | 36.24 |'
- en: '|  | GPTQ | 2.00 | 41.75 | 465.94 | 40.57 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 41.75 | 465.94 | 40.57 |'
- en: '| Vicuna-13B | PB-LLM | 1.70 | 362.17 | 772.44 | 346.16 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| Vicuna-13B | PB-LLM | 1.70 | 362.17 | 772.44 | 346.16 |'
- en: '|  | BiLLM | 1.08 | 36.57 | 300.31 | 28.76 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.08 | 36.57 | 300.31 | 28.76 |'
- en: 'Due to the exceptional performance of the LLaMA (Touvron et al., [2023a](#bib.bib40),
    [b](#bib.bib41)) series, they have become the foundation for many open-source
    models (Chiang et al., [2023](#bib.bib5)). Then, in Table [3](#S4.T3 "Table 3
    ‣ 4.1 Setup ‣ 4 Experiments ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs"), we evaluate the perplexity of outputs from the LLaMA series models
    using different methods. It can be observed that, even at ultra-low weight bit-width,
    BiLLM consistently outperforms the 2-bit RTN and GPTQ methods. And 1.08 bits BiLLM
    for LLaMA-65B and LLaMA2-70B even surpasses the output of the full-precision OPT-66B
    model, which demonstrates the further binary potential of the LLaMA family. We
    extend perplexity evaluation to the PTB and C4 datasets. Figure [7](#S4.F7 "Figure
    7 ‣ 4.2 Results ‣ 4 Experiments ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") illustrates the performance of the 7B parameter LLaMA series as well
    as the 6.7B OPT models. BiLLM continues to achieve a leading edge in performance
    compared to other methods (more additional comparisons are discussed in Appendix
    [D](#A4 "Appendix D Multi-evaluation Comparisons ‣ BiLLM: Pushing the Limit of
    Post-Training Quantization for LLMs")).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 LLaMA (Touvron et al., [2023a](#bib.bib40), [b](#bib.bib41)) 系列的卓越表现，它们已成为许多开源模型的基础
    (Chiang et al., [2023](#bib.bib5))。在表格 [3](#S4.T3 "Table 3 ‣ 4.1 Setup ‣ 4 Experiments
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") 中，我们使用不同的方法评估了
    LLaMA 系列模型的困惑度。可以观察到，即使在超低权重量位宽下，BiLLM 也始终优于 2 位 RTN 和 GPTQ 方法。而 1.08 位 BiLLM
    对于 LLaMA-65B 和 LLaMA2-70B 甚至超过了全精度 OPT-66B 模型的输出，这展示了 LLaMA 系列的进一步二值化潜力。我们将困惑度评估扩展到
    PTB 和 C4 数据集。图 [7](#S4.F7 "Figure 7 ‣ 4.2 Results ‣ 4 Experiments ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") 展示了 7B 参数 LLaMA 系列以及 6.7B OPT
    模型的表现。与其他方法相比，BiLLM 继续在性能上保持领先 (更多附加比较见附录 [D](#A4 "Appendix D Multi-evaluation
    Comparisons ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"))。'
- en: 'Experiments of instruction-tuned models. Instruction fine-tuning can significantly
    improve the application capabilities of the model and has become a necessary process
    for LLMs deployment in different scenarios (Wei et al., [2021](#bib.bib44); Sanh
    et al., [2021](#bib.bib38); Chiang et al., [2023](#bib.bib5)). We also deployed
    BiLLM on the recently popular fine-tuning instruction model Vicuna for benchmark
    testing. As shown in Table [4](#S4.T4 "Table 4 ‣ 4.2 Results ‣ 4 Experiments ‣
    BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"), the perplexity
    performance of GPTQ and PB-LLM are compared on Vicuna-7B and Vicuna-13B with three
    evaluations. BiLLM can achieve better performance at an average weight bit of
    1.08, which further proves that BiLLM’s universal LLMs binarization potential.
    We also provide dialogue examples of binary models in Appeandix [F](#A6 "Appendix
    F Dialog Examples ‣ BiLLM: Pushing the Limit of Post-Training Quantization for
    LLMs").'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '指令调优模型的实验。指令微调可以显著提高模型的应用能力，已成为 LLM 在不同场景中部署的必要过程 (Wei et al., [2021](#bib.bib44);
    Sanh et al., [2021](#bib.bib38); Chiang et al., [2023](#bib.bib5))。我们还在最近流行的微调指令模型
    Vicuna 上部署了 BiLLM 进行基准测试。如表格 [4](#S4.T4 "Table 4 ‣ 4.2 Results ‣ 4 Experiments
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") 所示，对 Vicuna-7B
    和 Vicuna-13B 上的 GPTQ 和 PB-LLM 的困惑度性能进行了三次评估比较。BiLLM 在平均权重量为 1.08 的情况下能实现更好的性能，这进一步证明了
    BiLLM 在通用 LLMs 二值化的潜力。我们还在附录 [F](#A6 "Appendix F Dialog Examples ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") 提供了二值模型的对话示例。'
- en: 'Zero-Shot results. To conduct a more comprehensive evaluation of binary LLMs,
    we extend our experiments to 7 zero-shot datasets. Appendix [D](#A4 "Appendix
    D Multi-evaluation Comparisons ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") provides detailed results of our approach compared to previous methods
    in ultra-low bit quantization, further showing the outlier of BiLLM.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zero-Shot 结果。为了对二值 LLMs 进行更全面的评估，我们将实验扩展到 7 个零样本数据集。附录 [D](#A4 "Appendix D
    Multi-evaluation Comparisons ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") 提供了我们方法与之前超低位量化方法的详细比较结果，进一步展示了 BiLLM 的异类表现。'
- en: '![Refer to caption](img/5cd60a51a968b07e1c4ee175174159b6.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5cd60a51a968b07e1c4ee175174159b6.png)'
- en: 'Figure 8: Ablation results of salient-only and splitting-only methods on OPT
    and LLaMA.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: OPT 和 LLaMA 上仅有显著性和仅有分割方法的消融结果。'
- en: 'Ablation results. BiLLM enhances binarization precision through two primary
    methods: structured salient binarization via residual approximation, and non-salient
    weight binarization via optimal splitting. To examine the effects of these strategies,
    we conducted decomposition experiments. As shown in Figure [8](#S4.F8 "Figure
    8 ‣ 4.2 Results ‣ 4 Experiments ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs"), both approaches significantly improve binary performance. Notably,
    we found that OPT-6.7B exhibits greater sensitivity to the splitting of non-salient
    weights (the blue line is lower than the green line), whereas LLaMA-7B is more
    responsive to salient weights’ residual approximation (the green line is lower
    than the blue line). This further indicates that different LLMs exhibit varying
    responses to distinct binarization optimization strategies, showing that the two
    binarization strategies proposed by BiLLM are efficient to various LLMs. We further
    discuss details on the block-size ablation results in Appendix [E](#A5 "Appendix
    E Ablation of BiLLM with different block size ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs").'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '消融结果。BiLLM 通过两种主要方法增强了二值化精度：通过残差逼近进行结构化显著二值化，以及通过最优分割进行非显著权重的二值化。为了检查这些策略的效果，我们进行了分解实验。如图
    [8](#S4.F8 "Figure 8 ‣ 4.2 Results ‣ 4 Experiments ‣ BiLLM: Pushing the Limit
    of Post-Training Quantization for LLMs") 所示，这两种方法显著改善了二进制性能。值得注意的是，我们发现 OPT-6.7B
    对非显著权重的分割更为敏感（蓝线低于绿线），而 LLaMA-7B 对显著权重的残差逼近更为响应（绿线低于蓝线）。这进一步表明，不同的 LLM 对不同的二值化优化策略有不同的反应，显示出
    BiLLM 提出的两种二值化策略对各种 LLM 都是有效的。我们在附录 [E](#A5 "Appendix E Ablation of BiLLM with
    different block size ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") 中进一步讨论了块大小消融结果的细节。'
- en: 5 Conclusions
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This work proposed a novel post-training binary quantization method named BiLLM,
    specifically tailored for compressing pre-trained LLMs. Inspired by the characteristics
    of weight’s value and Hessian distributions, we adopted a binary residual approximation
    for structurally salient weights to preserve their capabilities at ultra-low bits.
    For non-salient weights, we employed optimal segmentation for grouped binarization.
    Our results demonstrate that LLMs can undergo a one-time weight quantization at
    ultra-low bits without substantial loss of precision. BiLLM has pioneered the
    achievement of LLM performance guarantees at an average bit rate close to 1 bit.
    We validated the binarization performance of BiLLM across multiple open-source
    LLM families and conducted generalization tests on a fine-tuned instruction model.
    BiLLM advances the bit-width quantization frontier of LLMs, promising to facilitate
    the deployment of LLMs in edge scenarios and resource-constrained devices, and
    encourages further exploration in LLMs compression.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作提出了一种新颖的训练后二值量化方法，名为 BiLLM，专门针对压缩预训练的 LLMs 进行设计。受到权重值和 Hessian 分布特征的启发，我们采用了对结构上显著的权重进行二值残差逼近的方法，以在超低位下保留其能力。对于非显著权重，我们使用了分组二值化的最优分割方法。我们的结果表明，LLMs
    可以在超低位下进行一次权重量化，而不会造成显著的精度损失。BiLLM 在接近 1 位的平均比特率下，首次实现了 LLM 性能保证。我们验证了 BiLLM 在多个开源
    LLM 系列上的二值化性能，并对经过微调的指令模型进行了泛化测试。BiLLM 推进了 LLM 的比特宽度量化前沿，有望促进 LLM 在边缘场景和资源受限设备上的部署，并鼓励进一步探索
    LLM 压缩。
- en: 6 Impact Statements
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 影响声明
- en: This paper presents work whose goal is to advance the field of Machine Learning.
    There are many potential societal consequences of our work, none which we feel
    must be specifically highlighted here.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 本文展示了旨在推动机器学习领域进展的工作。我们的工作有许多潜在的社会影响，但我们认为这里没有必要特别强调这些影响。
- en: References
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bengio et al. (2013) Bengio, Y., Léonard, N., and Courville, A. Estimating or
    propagating gradients through stochastic neurons for conditional computation.
    *arXiv preprint arXiv:1308.3432*, 2013.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bengio 等 (2013) Bengio, Y., Léonard, N., 和 Courville, A. 通过随机神经元进行条件计算的梯度估计或传播。*arXiv
    预印本 arXiv:1308.3432*，2013。
- en: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning
    about physical commonsense in natural language. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 34, pp.  7432–7439, 2020.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk 等 (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., 等。Piqa：自然语言中的物理常识推理。发表于
    *AAAI 人工智能会议论文集*，第 34 卷，页码 7432–7439，2020。
- en: Blundell et al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,
    D. Weight uncertainty in neural network. In *International conference on machine
    learning*, pp.  1613–1622\. PMLR, 2015.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blundell et al. (2015) Blundell, C., Cornebise, J., Kavukcuoglu, K., 和 Wierstra,
    D. 神经网络中的权重不确定性。见于*国际机器学习会议*，第1613–1622页。PMLR，2015年。
- en: Chan & Ioannidis (1998) Chan, C.-Y. and Ioannidis, Y. E. Bitmap index design
    and evaluation. In *Proceedings of the 1998 ACM SIGMOD international conference
    on Management of data*, pp.  355–366, 1998.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chan & Ioannidis (1998) Chan, C.-Y. 和 Ioannidis, Y. E. 位图索引设计与评估。见于*1998年ACM
    SIGMOD国际数据管理会议论文集*，第355–366页，1998年。
- en: 'Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
    H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., et al. Vicuna: An open-source
    chatbot impressing gpt-4 with 90%* chatgpt quality. *See https://vicuna. lmsys.
    org (accessed 14 April 2023)*, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chiang et al. (2023) Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,
    H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., 等人。Vicuna：一个开源聊天机器人，其GPT-4表现达到90%*ChatGPT质量*。*参见
    https://vicuna.lmsys.org（访问日期：2023年4月14日）*，2023年。
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., 和 Toutanova, K. Boolq：探索自然是/否问题的意外难度。*arXiv预印本 arXiv:1905.10044*，2019年。
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*, 2018.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., 和 Tafjord, O. 认为你已经解决了问答问题？尝试ARC，AI2推理挑战。*arXiv预印本 arXiv:1803.05457*，2018年。
- en: 'Courbariaux et al. (2016) Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv,
    R., and Bengio, Y. Binarized neural networks: Training deep neural networks with
    weights and activations constrained to+ 1 or-1. *arXiv preprint arXiv:1602.02830*,
    2016.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Courbariaux et al. (2016) Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv,
    R., 和 Bengio, Y. 二值神经网络：训练具有权重和激活值被限制为+1或-1的深度神经网络。*arXiv预印本 arXiv:1602.02830*，2016年。
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer,
    L. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. *arXiv
    preprint arXiv:2208.07339*, 2022.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y., 和 Zettlemoyer,
    L. LLM. int8()：大规模变换器的8位矩阵乘法。*arXiv预印本 arXiv:2208.07339*，2022年。
- en: 'Dettmers et al. (2023a) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,
    L. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023a.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023a) Dettmers, T., Pagnoni, A., Holtzman, A., 和 Zettlemoyer,
    L. Qlora：高效微调量化LLMs。*arXiv预印本 arXiv:2305.14314*，2023a。
- en: 'Dettmers et al. (2023b) Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,
    D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D. Spqr:
    A sparse-quantized representation for near-lossless llm weight compression. *arXiv
    preprint arXiv:2306.03078*, 2023b.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers et al. (2023b) Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev,
    D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., 和 Alistarh, D. Spqr：一种稀疏量化表示，用于近无损的LLM权重压缩。*arXiv预印本
    arXiv:2306.03078*，2023b。
- en: 'Dong et al. (2019) Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., and Keutzer,
    K. Hawq: Hessian aware quantization of neural networks with mixed-precision. In
    *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pp. 
    293–302, 2019.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong et al. (2019) Dong, Z., Yao, Z., Gholami, A., Mahoney, M. W., 和 Keutzer,
    K. Hawq：具有混合精度的神经网络海森矩阵感知量化。见于*IEEE/CVF国际计算机视觉会议论文集*，第293–302页，2019年。
- en: 'Fang et al. (2020) Fang, J., Shafiee, A., Abdel-Aziz, H., Thorsley, D., Georgiadis,
    G., and Hassoun, J. H. Post-training piecewise linear quantization for deep neural
    networks. In *Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK,
    August 23–28, 2020, Proceedings, Part II 16*, pp.  69–86\. Springer, 2020.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. (2020) Fang, J., Shafiee, A., Abdel-Aziz, H., Thorsley, D., Georgiadis,
    G., 和 Hassoun, J. H. 训练后分段线性量化深度神经网络。见于*计算机视觉–ECCV 2020：第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第II部分16*，第69–86页。Springer，2020年。
- en: 'Faraone et al. (2018) Faraone, J., Fraser, N., Blott, M., and Leong, P. H.
    Syq: Learning symmetric quantization for efficient deep neural networks. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, pp.  4300–4309,
    2018.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faraone et al. (2018) Faraone, J., Fraser, N., Blott, M., 和 Leong, P. H. Syq：学习对称量化以提高深度神经网络的效率。见于*IEEE计算机视觉与模式识别会议论文集*，第4300–4309页，2018年。
- en: 'Frantar & Alistarh (2022) Frantar, E. and Alistarh, D. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *Advances in
    Neural Information Processing Systems*, 35:4475–4488, 2022.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar & Alistarh（2022）Frantar, E. 和 Alistarh, D. 最优脑压缩: 一个准确的后训练量化和剪枝框架。*神经信息处理系统进展*，35:4475–4488，2022。'
- en: 'Frantar & Alistarh (2023) Frantar, E. and Alistarh, D. Sparsegpt: Massive language
    models can be accurately pruned in one-shot. In *International Conference on Machine
    Learning*, pp.  10323–10337\. PMLR, 2023.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar & Alistarh（2023）Frantar, E. 和 Alistarh, D. Sparsegpt: 大规模语言模型可以在一次剪枝中准确地被剪枝。收录于
    *国际机器学习会议*，第10323–10337页，PMLR，2023。'
- en: 'Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等（2022）Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D. Gptq:
    针对生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022。'
- en: 'Helwegen et al. (2019) Helwegen, K., Widdicombe, J., Geiger, L., Liu, Z., Cheng,
    K.-T., and Nusselder, R. Latent weights do not exist: Rethinking binarized neural
    network optimization. *Advances in neural information processing systems*, 32,
    2019.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Helwegen 等（2019）Helwegen, K., Widdicombe, J., Geiger, L., Liu, Z., Cheng, K.-T.,
    和 Nusselder, R. 潜在权重不存在: 重新思考二值神经网络优化。*神经信息处理系统进展*，32，2019。'
- en: Jacob et al. (2018) Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,
    A., Adam, H., and Kalenichenko, D. Quantization and training of neural networks
    for efficient integer-arithmetic-only inference. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pp.  2704–2713, 2018.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob 等（2018）Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.,
    Adam, H., 和 Kalenichenko, D. 量化和训练神经网络以实现高效的整数算术推理。收录于 *IEEE 计算机视觉与模式识别会议论文集*，第2704–2713页，2018。
- en: 'Jain et al. (2019) Jain, S., Venkataramani, S., Srinivasan, V., Choi, J., Gopalakrishnan,
    K., and Chang, L. Biscaled-dnn: Quantizing long-tailed datastructures with two
    scale factors for deep neural networks. In *Proceedings of the 56th Annual Design
    Automation Conference 2019*, pp.  1–6, 2019.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jain 等（2019）Jain, S., Venkataramani, S., Srinivasan, V., Choi, J., Gopalakrishnan,
    K., 和 Chang, L. Biscaled-dnn: 通过两个尺度因子对深度神经网络中的长尾数据结构进行量化。收录于 *第56届年度设计自动化会议论文集
    2019*，第1–6页，2019。'
- en: LeCun et al. (1989) LeCun, Y., Denker, J., and Solla, S. Optimal brain damage.
    *Advances in neural information processing systems*, 2, 1989.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等（1989）LeCun, Y., Denker, J., 和 Solla, S. 最优脑损伤。*神经信息处理系统进展*，2，1989。
- en: 'Lee et al. (2023) Lee, C., Jin, J., Kim, T., Kim, H., and Park, E. Owq: Lessons
    learned from activation outliers for weight quantization in large language models.
    *arXiv preprint arXiv:2306.02272*, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee 等（2023）Lee, C., Jin, J., Kim, T., Kim, H., 和 Park, E. Owq: 从激活异常值中获得的教训，以进行大规模语言模型的权重量化。*arXiv
    预印本 arXiv:2306.02272*，2023。'
- en: 'Li et al. (2021) Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu,
    F., Wang, W., and Gu, S. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. *arXiv preprint arXiv:2102.05426*, 2021.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等（2021）Li, Y., Gong, R., Tan, X., Yang, Y., Hu, P., Zhang, Q., Yu, F., Wang,
    W., 和 Gu, S. Brecq: 通过块重构推动后训练量化的极限。*arXiv 预印本 arXiv:2102.05426*，2021。'
- en: Li et al. (2017) Li, Z., Ni, B., Zhang, W., Yang, X., and Gao, W. Performance
    guaranteed network acceleration via high-order residual quantization. In *Proceedings
    of the IEEE international conference on computer vision*, pp.  2584–2592, 2017.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等（2017）Li, Z., Ni, B., Zhang, W., Yang, X., 和 Gao, W. 通过高阶残差量化保证性能的网络加速。收录于
    *IEEE 国际计算机视觉会议论文集*，第2584–2592页，2017。
- en: 'Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. Awq: Activation-aware weight quantization for llm compression and acceleration.
    *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等（2023）Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., 和 Han, S. Awq:
    针对 LLM 压缩和加速的激活感知权重量化。*arXiv 预印本 arXiv:2306.00978*，2023。'
- en: 'Liu et al. (2023) Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad,
    Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free quantization
    aware training for large language models. *arXiv preprint arXiv:2305.17888*, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等（2023）Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi,
    Y., Krishnamoorthi, R., 和 Chandra, V. Llm-qat: 针对大规模语言模型的数据无关量化感知训练。*arXiv 预印本
    arXiv:2305.17888*，2023。'
- en: 'Marcus et al. (1994) Marcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre,
    R., Bies, A., Ferguson, M., Katz, K., and Schasberger, B. The penn treebank: Annotating
    predicate argument structure. In *Human Language Technology: Proceedings of a
    Workshop held at Plainsboro, New Jersey, March 8-11, 1994*, 1994.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Marcus等人（1994）Marcus, M., Kim, G., Marcinkiewicz, M. A., MacIntyre, R., Bies,
    A., Ferguson, M., Katz, K., 和 Schasberger, B. Penn Treebank: 注释谓词参数结构。见于*人类语言技术：在新泽西州普莱恩斯伯勒举行的研讨会记录，1994年3月8-11日*，1994年。'
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity等人（2016）Merity, S., Xiong, C., Bradbury, J., 和 Socher, R. 指针守卫混合模型。*arXiv预印本
    arXiv:1609.07843*，2016年。
- en: Mihaylov et al. (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
    Can a suit of armor conduct electricity? a new dataset for open book question
    answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov等人（2018）Mihaylov, T., Clark, P., Khot, T., 和 Sabharwal, A. 一套盔甲能导电吗？一个用于开放书籍问答的新数据集。*arXiv预印本
    arXiv:1809.02789*，2018年。
- en: Park et al. (2018) Park, E., Yoo, S., and Vajda, P. Value-aware quantization
    for training and inference of neural networks. In *Proceedings of the European
    Conference on Computer Vision (ECCV)*, pp.  580–595, 2018.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人（2018）Park, E., Yoo, S., 和 Vajda, P. 价值感知量化用于神经网络的训练和推理。见于*欧洲计算机视觉大会（ECCV）会议记录*，第580–595页，2018年。
- en: 'Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,
    J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. Pytorch:
    An imperative style, high-performance deep learning library. *Advances in neural
    information processing systems*, 32, 2019.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke等人（2019）Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan,
    G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., 等人。Pytorch: 一种命令式风格的高性能深度学习库。*神经信息处理系统进展*，32，2019年。'
- en: Qin et al. (2020) Qin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., and
    Song, J. Forward and backward information retention for accurate binary neural
    networks. In *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, pp.  2250–2259, 2020.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin等人（2020）Qin, H., Gong, R., Liu, X., Shen, M., Wei, Z., Yu, F., 和 Song, J.
    精确二值神经网络的前向和后向信息保留。见于*IEEE/CVF计算机视觉与模式识别会议记录*，第2250–2259页，2020年。
- en: 'Qin et al. (2022) Qin, H., Ding, Y., Zhang, M., Yan, Q., Liu, A., Dang, Q.,
    Liu, Z., and Liu, X. Bibert: Accurate fully binarized bert. *arXiv preprint arXiv:2203.06390*,
    2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin等人（2022）Qin, H., Ding, Y., Zhang, M., Yan, Q., Liu, A., Dang, Q., Liu, Z.,
    和 Liu, X. Bibert: 精确的完全二值化BERT。*arXiv预印本 arXiv:2203.06390*，2022年。'
- en: 'Qin et al. (2023) Qin, H., Zhang, M., Ding, Y., Li, A., Cai, Z., Liu, Z., Yu,
    F., and Liu, X. Bibench: Benchmarking and analyzing network binarization. *arXiv
    preprint arXiv:2301.11233*, 2023.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Qin等人（2023）Qin, H., Zhang, M., Ding, Y., Li, A., Cai, Z., Liu, Z., Yu, F.,
    和 Liu, X. Bibench: 网络二值化的基准测试与分析。*arXiv预印本 arXiv:2301.11233*，2023年。'
- en: Raffel et al. (2020) Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang,
    S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer
    learning with a unified text-to-text transformer. *The Journal of Machine Learning
    Research*, 21(1):5485–5551, 2020.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel等人（2020）Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena,
    M., Zhou, Y., Li, W., 和 Liu, P. J. 探索统一文本到文本变换器的迁移学习极限。*机器学习研究杂志*，21(1):5485–5551，2020年。
- en: 'Rastegari et al. (2016) Rastegari, M., Ordonez, V., Redmon, J., and Farhadi,
    A. Xnor-net: Imagenet classification using binary convolutional neural networks.
    In *European conference on computer vision*, pp.  525–542\. Springer, 2016.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Rastegari等人（2016）Rastegari, M., Ordonez, V., Redmon, J., 和 Farhadi, A. Xnor-net:
    使用二进制卷积神经网络进行ImageNet分类。见于*欧洲计算机视觉大会*，第525–542页，Springer，2016年。'
- en: 'Sakaguchi et al. (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. Winogrande: An adversarial winograd schema challenge at scale. *Communications
    of the ACM*, 64(9):99–106, 2021.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi等人（2021）Sakaguchi, K., Bras, R. L., Bhagavatula, C., 和 Choi, Y. Winogrande:
    大规模的对抗性Winograd模式挑战。*ACM通讯*，64(9):99–106，2021年。'
- en: Sanh et al. (2021) Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika,
    L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., et al. Multitask
    prompted training enables zero-shot task generalization. *arXiv preprint arXiv:2110.08207*,
    2021.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh等人（2021）Sanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai,
    Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., 等人。多任务提示训练使零样本任务泛化成为可能。*arXiv预印本
    arXiv:2110.08207*，2021年。
- en: 'Shang et al. (2023) Shang, Y., Yuan, Z., Wu, Q., and Dong, Z. Pb-llm: Partially
    binarized large language models. *arXiv preprint arXiv:2310.00034*, 2023.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shang等人（2023）Shang, Y., Yuan, Z., Wu, Q., 和 Dong, Z. Pb-llm: 部分二值化的大型语言模型。*arXiv预印本
    arXiv:2310.00034*，2023年。'
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023a.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等。Llama:
    开放而高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023a。'
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023b.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., 等。Llama 2:
    开放基础和微调聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023b。'
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. *Advances
    in neural information processing systems*, 30, 2017.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł., 和 Polosukhin, I. 注意力机制才是你所需要的一切。*神经信息处理系统进展*，30，2017。
- en: 'Wang et al. (2023) Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L.,
    Yang, F., Wang, R., Wu, Y., and Wei, F. Bitnet: Scaling 1-bit transformers for
    large language models. *arXiv preprint arXiv:2310.11453*, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. (2023) Wang, H., Ma, S., Dong, L., Huang, S., Wang, H., Ma, L.,
    Yang, F., Wang, R., Wu, Y., 和 Wei, F. Bitnet: 为大规模语言模型扩展 1 位变换器。*arXiv 预印本 arXiv:2310.11453*，2023。'
- en: Wei et al. (2021) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,
    B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot
    learners. *arXiv preprint arXiv:2109.01652*, 2021.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2021) Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester,
    B., Du, N., Dai, A. M., 和 Le, Q. V. 微调语言模型是零样本学习者。*arXiv 预印本 arXiv:2109.01652*，2021。
- en: 'Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. Huggingface’s
    transformers: State-of-the-art natural language processing. *arXiv preprint arXiv:1910.03771*,
    2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., 等。Huggingface 的变换器：最先进的自然语言处理。*arXiv
    预印本 arXiv:1910.03771*，2019。
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In *International Conference on Machine Learning*, pp.  38087–38099\.
    PMLR, 2023.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han,
    S. Smoothquant: 精确且高效的大规模语言模型后训练量化。在*国际机器学习会议*，第 38087–38099 页。PMLR，2023。'
- en: (47) Yao, Z., Aminabadi, R., Zhang, M., Wu, X., Li, C., and He, Y. Z. Efficient
    and affordable post-training quantization for large-scale transformers, 2022.
    *URL https://arxiv. org/abs/2206.01861*.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Yao, Z., Aminabadi, R., Zhang, M., Wu, X., Li, C., 和 He, Y. Z. 大规模变换器的高效且经济的后训练量化，2022。*URL
    https://arxiv.org/abs/2206.01861*。
- en: Yao et al. (2023) Yao, Z., Li, C., Wu, X., Youn, S., and He, Y. A comprehensive
    study on post-training quantization for large language models. *arXiv preprint
    arXiv:2303.08302*, 2023.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2023) Yao, Z., Li, C., Wu, X., Youn, S., 和 He, Y. 大规模语言模型的后训练量化综合研究。*arXiv
    预印本 arXiv:2303.08302*，2023。
- en: 'You (2010) You, Y. *Audio coding: theory and applications*. Springer Science
    & Business Media, 2010.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: You (2010) You, Y. *音频编码：理论与应用*。Springer Science & Business Media，2010。
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., 和 Choi,
    Y. Hellaswag: 机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*，2019。'
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., et al. Opt: Open pre-trained
    transformer language models. *arXiv preprint arXiv:2205.01068*, 2022.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., 等。Opt: 开放预训练的变换器语言模型。*arXiv
    预印本 arXiv:2205.01068*，2022。'
- en: 'Zhou et al. (2016) Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., and Zou, Y.
    Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth
    gradients. *arXiv preprint arXiv:1606.06160*, 2016.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhou et al. (2016) Zhou, S., Wu, Y., Ni, Z., Zhou, X., Wen, H., 和 Zou, Y. Dorefa-net:
    用低位宽梯度训练低位宽卷积神经网络。*arXiv 预印本 arXiv:1606.06160*，2016。'
- en: Zhu et al. (2023) Zhu, X., Li, J., Liu, Y., Ma, C., and Wang, W. A survey on
    model compression for large language models. *arXiv preprint arXiv:2308.07633*,
    2023.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu et al. (2023) Zhu, X., Li, J., Liu, Y., Ma, C., 和 Wang, W. 大规模语言模型的模型压缩调查。*arXiv
    预印本 arXiv:2308.07633*，2023。
- en: Appendix A BiLLM Implementation
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A BiLLM 实现
- en: 'Algorithm 2 BiLLM: Detailed functions process'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '算法 2 BiLLM: 详细函数过程'
- en: func $\operatorname{salient}$
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\operatorname{salient}$
- en: 1:  $\mathbf{S}\coloneqq\mathbf{W}^{2}/[\mathbf{H}^{c}_{b:b+\beta b:b+\beta}]^{2}\
    $
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  $\mathbf{S}\coloneqq\mathbf{W}^{2}/[\mathbf{H}^{c}_{b:b+\beta b:b+\beta}]^{2}\
    $
- en: func $\operatorname{binary}$
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\operatorname{binary}$
- en: 1:  $\alpha\coloneqq\dfrac{||\mathbf{W}||_{\ell 1}}{m}$
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  $\alpha\coloneqq\dfrac{||\mathbf{W}||_{\ell 1}}{m}$
- en: func $\operatorname{res\_approximation}$
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\operatorname{res\_approximation}$
- en: 1:  $\mathbf{B}_{1}\coloneqq\operatorname{binary}(\mathbf{W})$
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  $\mathbf{B}_{1}\coloneqq\operatorname{binary}(\mathbf{W})$
- en: func $\operatorname{seg\_search}$
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\operatorname{seg\_search}$
- en: 1:  $e=\operatorname{inf}$
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 1:  $e=\operatorname{inf}$
- en: 'BiLLM necessitates the structured selection of salient rows and their subsequent
    quantization through residual approximation binarization. This is followed by
    dividing the non-salient weights, which exhibit a bell-shaped distribution, into
    a sparse area and a concentrated area. The division requires the optimization
    of the segmentation point $p^{*}$ by minimizing quantization loss. Ultimately,
    the two regions of non-salient weights are binarized separately to derive the
    final binary weights for LLMs. The implementation details of the aforementioned
    function are enumerated in Algorithm [2](#alg2 "Algorithm 2 ‣ Appendix A BiLLM
    Implementation ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs").'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 'BiLLM 需要结构化选择显著行，并通过残差逼近二值化进行量化。然后将表现为钟形分布的非显著权重分为稀疏区域和集中区域。此划分需要通过最小化量化损失来优化分割点
    $p^{*}$。最终，两个非显著权重区域分别进行二值化，以得出 LLM 的最终二进制权重。上述函数的实现细节在算法 [2](#alg2 "算法 2 ‣ 附录
    A BiLLM 实现 ‣ BiLLM: 推动 LLM 后训练量化的极限") 中列出。'
- en: Appendix B Quantization Error
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 量化误差
- en: 'Quantization error definition for weight distribution The numerical range covered
    by the uniform quantizer spans from $[X_{min},X_{max}]$ represents the target
    bit-width of quantization. So the quantization step size is:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 权重分布的量化误差定义 均匀量化器覆盖的数值范围从 $[X_{min},X_{max}]$ 代表量化的目标位宽。因此量化步长为：
- en: '|  | $\Delta=\frac{X_{\text{max}}-X_{\text{min}}}{M}$ |  | (15) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta=\frac{X_{\text{max}}-X_{\text{min}}}{M}$ |  | (15) |'
- en: 'The boundaries can be calculated as:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 边界可以计算为：
- en: '|  | $b_{q}=X_{\text{min}}+\Delta\cdot l$ |  | (16) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | $b_{q}=X_{\text{min}}+\Delta\cdot l$ |  | (16) |'
- en: 'where $l\in{0,1,...,M}$ under binarization. Then we give the mean of each interval:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l\in{0,1,...,M}$ 在二值化下。然后我们给出每个区间的均值：
- en: '|  | $x_{q}=X_{\text{min}}+\Delta\cdot l-0.5\Delta$ |  | (17) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{q}=X_{\text{min}}+\Delta\cdot l-0.5\Delta$ |  | (17) |'
- en: 'where $l\in{1,...,M}$. In this quantization scheme, we can get the MSQE from
     (You, [2010](#bib.bib49)):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $l\in{1,...,M}$。在此量化方案中，我们可以从 (You, [2010](#bib.bib49)) 得到 MSQE：
- en: '|  | $1$2 |  | (18) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (18) |'
- en: 'then we let the $y$ part, so the Equation ([18](#A2.E18 "Equation 18 ‣ Appendix
    B Quantization Error ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")) becomes:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '然后我们考虑 $y$ 部分，因此方程 ([18](#A2.E18 "方程 18 ‣ 附录 B 量化误差 ‣ BiLLM: 推动 LLM 后训练量化的极限"))
    变为：'
- en: '|  | $1$2 |  | (19) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (19) |'
- en: 'consider the Equation ([16](#A2.E16 "Equation 16 ‣ Appendix B Quantization
    Error ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs")) and
    Equation ([17](#A2.E17 "Equation 17 ‣ Appendix B Quantization Error ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")), the above equation becomes:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '考虑方程 ([16](#A2.E16 "方程 16 ‣ 附录 B 量化误差 ‣ BiLLM: 推动 LLM 后训练量化的极限")) 和方程 ([17](#A2.E17
    "方程 17 ‣ 附录 B 量化误差 ‣ BiLLM: 推动 LLM 后训练量化的极限"))，上述方程变为：'
- en: '|  | $\theta^{2}=\sum_{l=1}^{M}\int_{-0.5\Delta}^{0.5\Delta}x^{2}f(x_{p}-x)dx$
    |  | (20) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta^{2}=\sum_{l=1}^{M}\int_{-0.5\Delta}^{0.5\Delta}x^{2}f(x_{p}-x)dx$
    |  | (20) |'
- en: 'The aforementioned reasoning indicates that the MSQE of a uniform quantizer
    depends on the PDF and the quantization bit-width. Due to previous observations
    of the weights in pretrained LLMs, we have eliminated the salient weights. The
    remaining distribution of non-salient weights’ $g(x)$ into Equation ([18](#A2.E18
    "Equation 18 ‣ Appendix B Quantization Error ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")), resulting in:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '上述推理表明，均匀量化器的 MSQE 依赖于 PDF 和量化位宽。由于之前对预训练 LLM 中权重的观察，我们已经消除了显著权重。剩余的非显著权重的分布
    $g(x)$ 代入方程 ([18](#A2.E18 "方程 18 ‣ 附录 B 量化误差 ‣ BiLLM: 推动 LLM 后训练量化的极限"))，结果为：'
- en: '|  | $\displaystyle\theta^{2}$ |  | (21) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\theta^{2}$ |  | (21) |'
- en: '|  |  | $\displaystyle=$ |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle=$ |  |'
- en: Appendix C Searching Curve of Salient Column and Non-salient Distribution
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录C 显著列与非显著分布的搜索曲线
- en: '![Refer to caption](img/4c8f987f1ac22b90d1aa19d7164e81d8.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4c8f987f1ac22b90d1aa19d7164e81d8.png)'
- en: 'Figure 9: Block-wise searching curve of salient columns in OPT-6.7B. The majority
    of the curves indicate that the minimal quantization error can be achieved at
    the block level by considering only a few columns as salient. The Out Projection
    layer has a larger number of salient columns, hence varying coverage for each
    block. The distribution in the FC layer is more dispersed. After optimal searching,
    the overall average weight bit is merely 1.1 bits.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：OPT-6.7B中显著列的块级搜索曲线。大多数曲线表明，通过仅考虑少量列作为显著列，在块级别可以实现最小量化误差。Out Projection层的显著列数量较多，因此每个块的覆盖范围各不相同。FC层的分布更为分散。经过优化搜索后，整体平均权重比特数仅为1.1比特。
- en: 'We implemented a column-level segmentation and formulated a minimal-error column
    number search, as delineated in Equation ([5](#S3.E5 "Equation 5 ‣ 3.1 Salient
    Weight Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs")). The identification of the optimal count of salient column
    groups commences with the column exhibiting the highest salience. To mitigate
    the increase in bit-width resulting from residual approximation, we confined the
    search range to between 3 to 30 columns. Figure [9](#A3.F9 "Figure 9 ‣ Appendix
    C Searching Curve of Salient Column and Non-salient Distribution ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs") illustrates the search curve
    pertinent to the inaugural Transformer block within the OPT6.7B model. It includes
    six layers of operators (Q, K, V, Out Projection, FC1, and FC2), with each layer
    showing the search curves for the first five blocks. Figure [15](#A7.F15 "Figure
    15 ‣ Appendix G Magnitude and Hessian Distribution of LLMs ‣ BiLLM: Pushing the
    Limit of Post-Training Quantization for LLMs") elucidates the clustering of salient
    weights, suggesting that a majority of the layers and blocks are capable of attaining
    minimal quantization errors with a limited number of salient columns. The block-wise
    changes in weight distribution brought about by OBC (Frantar & Alistarh, [2022](#bib.bib15))
    introduce fluctuations in the search curve; however, the structured selection
    still manages to encompass the majority of salient weights. In the Feedforward
    layer, where salient weight distribution is more scattered, the search curve leans
    towards employing residual approximation across an increased number of columns.
    Nonetheless, Table [1](#S3.T1 "Table 1 ‣ 3.3 Pipeline of BiLLM ‣ 3 Method ‣ BiLLM:
    Pushing the Limit of Post-Training Quantization for LLMs"), displaying the average
    weight bit numbers across various LLMs, confirms that this search strategy effectively
    maintains weight compression at approximately 1.1 bits.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '我们实现了一个列级别的分段，并制定了一个最小误差的列数搜索方法，如公式 ([5](#S3.E5 "Equation 5 ‣ 3.1 Salient Weight
    Binarization for LLMs ‣ 3 Method ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs")) 所述。确定最佳显著列组的数量从显著性最高的列开始。为了减少由于残差近似引起的比特宽度增加，我们将搜索范围限制在3到30列之间。图 [9](#A3.F9
    "Figure 9 ‣ Appendix C Searching Curve of Salient Column and Non-salient Distribution
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") 说明了与OPT6.7B模型中的首个Transformer块相关的搜索曲线。它包括六层操作符（Q、K、V、Out
    Projection、FC1 和 FC2），每一层显示了前五个块的搜索曲线。图 [15](#A7.F15 "Figure 15 ‣ Appendix G Magnitude
    and Hessian Distribution of LLMs ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") 阐明了显著权重的聚类，表明大多数层和块可以通过有限数量的显著列来达到最小量化误差。OBC (Frantar & Alistarh, [2022](#bib.bib15))
    引入的块级权重分布变化会导致搜索曲线的波动；然而，结构化选择仍然能够涵盖大多数显著权重。在显著权重分布更为分散的前馈层中，搜索曲线倾向于采用在更多列上进行残差近似。尽管如此，表
    [1](#S3.T1 "Table 1 ‣ 3.3 Pipeline of BiLLM ‣ 3 Method ‣ BiLLM: Pushing the Limit
    of Post-Training Quantization for LLMs") 显示了各种LLM中平均权重比特数，确认了这种搜索策略有效地保持了大约1.1比特的权重压缩。'
- en: 'Figure [10](#A3.F10 "Figure 10 ‣ Appendix C Searching Curve of Salient Column
    and Non-salient Distribution ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") shows the unstructured search curve for the non-salient weights in
    the OPT6.7B model, with the same composition as that in Figure [9](#A3.F9 "Figure
    9 ‣ Appendix C Searching Curve of Salient Column and Non-salient Distribution
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"). The horizontal
    axis represents the ratio between $p$. This phenomenon demonstrates that the non-salient
    weights exhibit characteristics closely resembling an ideal Gaussian or Laplacian
    distribution (You, [2010](#bib.bib49); Fang et al., [2020](#bib.bib13)).'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [10](#A3.F10 "图 10 ‣ 附录 C 显著列和非显著分布的搜索曲线 ‣ BiLLM：推动 LLM 后训练量化的极限") 展示了 OPT6.7B
    模型中非显著权重的无结构搜索曲线，其组成与图 [9](#A3.F9 "图 9 ‣ 附录 C 显著列和非显著分布的搜索曲线 ‣ BiLLM：推动 LLM 后训练量化的极限")
    中相同。横轴表示 $p$ 的比率。这一现象表明，非显著权重表现出与理想的高斯或拉普拉斯分布紧密相似的特征（You，[2010](#bib.bib49)；Fang
    等，[2020](#bib.bib13)）。
- en: '![Refer to caption](img/885063feb7f178ce01dce16d619179c7.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/885063feb7f178ce01dce16d619179c7.png)'
- en: 'Figure 10: Block-wise splitting curve of bell-shaped distribution in OPT6.7B.
    The overall presentation exhibits the characteristics of a convex function, fundamentally
    aligning with the theoretical optimal point in terms of theoretical basis.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：OPT6.7B 中钟形分布的块状分割曲线。整体展示了凸函数的特征，从理论基础上基本符合理论最优点。
- en: Appendix D Multi-evaluation Comparisons
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 多重评估比较
- en: '![Refer to caption](img/cf4e0650fe8a94019d5f78a37c584c4b.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/cf4e0650fe8a94019d5f78a37c584c4b.png)'
- en: 'Figure 11: GPTQ, PB-LLM, BiLLM performed on the PTB and C4 datasets, mainly
    on LLaMA-13B, LLaMA2-13B, OPT-13B, and so on. The results showed that BiLLM performed
    relatively well.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：GPTQ、PB-LLM、BiLLM 在 PTB 和 C4 数据集上的表现，主要涉及 LLaMA-13B、LLaMA2-13B、OPT-13B
    等。结果显示，BiLLM 的表现相对较好。
- en: Perplexity results on PTB and C4.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: PTB 和 C4 的困惑度结果。
- en: We use tables in the main text to show the perplexity of the three methods GPTQ,
    PB-LLM, and BiLLM on the Wikitext2 dataset, and bar charts to show the perplexity
    results for LLaMA-7B, LLaMA2-7B, and OPT-6.7B on the PTB and C4 datasets. In the
    appendix, we show the quantitative comparison results for models of other sizes
    on the PTB and C4 datasets with more images.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在正文中使用表格展示了 GPTQ、PB-LLM 和 BiLLM 在 Wikitext2 数据集上的困惑度，以及条形图展示了 LLaMA-7B、LLaMA2-7B
    和 OPT-6.7B 在 PTB 和 C4 数据集上的困惑度结果。在附录中，我们展示了其他尺寸模型在 PTB 和 C4 数据集上的定量比较结果，并附有更多图像。
- en: 'In Figure [11](#A4.F11 "Figure 11 ‣ Appendix D Multi-evaluation Comparisons
    ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs"), we find that
    although different models have different perplexity results, they still roughly
    follow the law that the larger the model, the lower the perplexity. BiLLM is generally
    still relatively better than the GPTQ and PB-LLM results in terms of perplexity
    with a lower bit-width configuration, while PB-LLM and GPTQ are higher or lower
    than each other, with slightly inferior results at very low bits.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 [11](#A4.F11 "图 11 ‣ 附录 D 多重评估比较 ‣ BiLLM：推动 LLM 后训练量化的极限") 中，我们发现虽然不同模型的困惑度结果不同，但它们大致仍然遵循模型越大，困惑度越低的规律。BiLLM
    在较低比特宽度配置下的困惑度通常仍然优于 GPTQ 和 PB-LLM 的结果，而 PB-LLM 和 GPTQ 在极低比特时表现较差。
- en: Zero-shot results
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 零-shot 结果
- en: 'For completeness of testing, we have also tested and compared metrics such
    as the accuracy of GPTQ, PB-LLM, and BiLLM on datasets such as PIQA and BoolQ,
    all using Zero Shot’s experimental setup. From Table [5](#A4.T5 "Table 5 ‣ Appendix
    D Multi-evaluation Comparisons ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs"), We find that despite the loss in quantification, a side-by-side comparison
    between the three methods still shows BiLLM to be superior overall, testing one
    level higher on some datasets, while the effect of some random perturbations,
    although present, does not pull down BiLLM’s performance across the board. This
    suggests that BiLLM’s quantization results have significantly improved performance
    at very low bits, and further validates the conclusions.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '为了测试的完整性，我们还测试并比较了GPTQ、PB-LLM和BiLLM在PIQA和BoolQ等数据集上的准确率，所有实验都使用了Zero Shot的实验设置。从表格
    [5](#A4.T5 "Table 5 ‣ Appendix D Multi-evaluation Comparisons ‣ BiLLM: Pushing
    the Limit of Post-Training Quantization for LLMs")中，我们发现尽管量化有损失，但三种方法的并列比较仍显示BiLLM总体上优于其他方法，在某些数据集上测试的水平更高，虽然一些随机扰动存在，但并未显著降低BiLLM的整体表现。这表明BiLLM的量化结果在非常低的位数下显著提升了性能，并进一步验证了结论。'
- en: 'Table 5: Accuracy on 7 data sets, from binarization LLaMA, LLaMA2, and OPT,
    and we also compare the results among GPTQ, PB-LLM, and BiLLM to validate the
    quantization effect.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在7个数据集上的准确率，来源于二值化LLaMA、LLaMA2和OPT，我们还比较了GPTQ、PB-LLM和BiLLM之间的结果，以验证量化效果。
- en: '| Model | Method | Weight Bits | Block Size | PIQA  $\uparrow$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | 权重位数 | 块大小 | PIQA  $\uparrow$ |'
- en: '|  | GPTQ | 2.00 | 128 | 52.8 | 50.0 | 28.2 | 49.3 | 26.6 | 29.5 | 26.3 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 128 | 52.8 | 50.0 | 28.2 | 49.3 | 26.6 | 29.5 | 26.3 |'
- en: '| LLaMA-7B | PB-LLM | 1.70 | 128 | 54.6 | 59.7 | 30.4 | 50.6 | 28.2 | 24.6
    | 28.7 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA-7B | PB-LLM | 1.70 | 128 | 54.6 | 59.7 | 30.4 | 50.6 | 28.2 | 24.6
    | 28.7 |'
- en: '|  | BiLLM | 1.09 | 128 | 61.2 | 62.7 | 31.8 | 51.1 | 36.0 | 25.7 | 36.8 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.09 | 128 | 61.2 | 62.7 | 31.8 | 51.1 | 36.0 | 25.7 | 36.8 |'
- en: '|  | GPTQ | 2.00 | 128 | 51.1 | 43.9 | 29.0 | 50.8 | 26.6 | 28.5 | 26.3 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 128 | 51.1 | 43.9 | 29.0 | 50.8 | 26.6 | 28.5 | 26.3 |'
- en: '| LLaMA2-7B | PB-LLM | 1.70 | 128 | 53.8 | 62.3 | 30.2 | 49.3 | 28.0 | 25.0
    | 27.7 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | PB-LLM | 1.70 | 128 | 53.8 | 62.3 | 30.2 | 49.3 | 28.0 | 25.0
    | 27.7 |'
- en: '|  | BiLLM | 1.08 | 128 | 60.6 | 61.8 | 33.2 | 52.4 | 36.2 | 24.4 | 34.8 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.08 | 128 | 60.6 | 61.8 | 33.2 | 52.4 | 36.2 | 24.4 | 34.8 |'
- en: '|  | GPTQ | 2.00 | 128 | 56.6 | 51.1 | 25.6 | 51.2 | 31.3 | 22.9 | 30.4 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2.00 | 128 | 56.6 | 51.1 | 25.6 | 51.2 | 31.3 | 22.9 | 30.4 |'
- en: '| OPT-6.7B | PB-LLM | 1.70 | 128 | 57.6 | 55.5 | 24.2 | 47.7 | 33.2 | 21.0
    | 31.0 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| OPT-6.7B | PB-LLM | 1.70 | 128 | 57.6 | 55.5 | 24.2 | 47.7 | 33.2 | 21.0
    | 31.0 |'
- en: '|  | BiLLM | 1.11 | 128 | 58.6 | 62.2 | 29.0 | 51.5 | 34.1 | 23.9 | 31.9 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '|  | BiLLM | 1.11 | 128 | 58.6 | 62.2 | 29.0 | 51.5 | 34.1 | 23.9 | 31.9 |'
- en: Appendix E Ablation of BiLLM with different block size
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E：不同块大小下的BiLLM消融实验
- en: To explore the effect of different chunk sizes on the quantization effect of
    BiLLM, we set up block size settings including 32 columns and 64 columns up to
    512 columns and performed quantization experiments on them. The results show that
    the overall perplexity is lower as the chunk granularity becomes finer and the
    number of bits used becomes relatively smaller. We believe this is because the
    smaller the chunks, the finer the data representation, and the more scale is used,
    but increasing the diversity of quantization results also increases the weighting
    overhead. A block size of 128 can better balance the bit-width and quantization
    effect.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探究不同块大小对BiLLM量化效果的影响，我们设置了32列、64列到512列的块大小，并在这些设置下进行了量化实验。结果显示，随着块粒度的变细和使用位数的减少，整体困惑度较低。我们认为这是因为块越小，数据表示越精细，使用的尺度越大，但增加了量化结果的多样性也增加了加权开销。128的块大小可以更好地平衡位宽和量化效果。
- en: 'Table 6: Perplexity on Wikitext2, PTB, and C4 with different block size settings
    on BiLLM.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：不同块大小设置下BiLLM在Wikitext2、PTB和C4上的困惑度。
- en: '|          Model |           Block Size |          Wikitext2 |          PTB
    |          C4 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '|          模型 |           块大小 |          Wikitext2 |          PTB |          C4
    |'
- en: '|  |          512 |          74.14 |          1078.90 |          81.76 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '|  |          512 |          74.14 |          1078.90 |          81.76 |'
- en: '|  |          256 |          48.91 |          574.34 |          57.60 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  |          256 |          48.91 |          574.34 |          57.60 |'
- en: '|          LLaMA-7B |          128 |          35.04 |          421.27 |          39.59
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|          LLaMA-7B |          128 |          35.04 |          421.27 |          39.59
    |'
- en: '|  |          64 |          27.23 |          399.81 |          27.74 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '|  |          64 |          27.23 |          399.81 |          27.74 |'
- en: '|  |          32 |          17.56 |          263.39 |          19.85 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '|  |          32 |          17.56 |          263.39 |          19.85 |'
- en: '|  |          512 |          52.90 |          267.82 |          43.86 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '|  |          512 |          52.90 |          267.82 |          43.86 |'
- en: '|  |          256 |          43.69 |          232.34 |          43.21 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '|  |          256 |          43.69 |          232.34 |          43.21 |'
- en: '|          LLaMA2-7B |          128 |          32.48 |          3877.38 |          40.52
    |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|          LLaMA2-7B |          128 |          32.48 |          3877.38 |          40.52
    |'
- en: '|  |          64 |          20.12 |          830.36 |          24.46 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  |          64 |          20.12 |          830.36 |          24.46 |'
- en: '|  |          32 |          13.58 |          440.40 |          17.34 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '|  |          32 |          13.58 |          440.40 |          17.34 |'
- en: '|  |          512 |          151.81 |          257.22 |          101.96 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |          512 |          151.81 |          257.22 |          101.96 |'
- en: '|  |          256 |          84.42 |          116.44 |          77.25 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |          256 |          84.42 |          116.44 |          77.25 |'
- en: '|          OPT-6.7B |          128 |          35.36 |          73.63 |          43.16
    |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|          OPT-6.7B |          128 |          35.36 |          73.63 |          43.16
    |'
- en: '|  |          64 |          33.36 |          48.16 |          31.94 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '|  |          64 |          33.36 |          48.16 |          31.94 |'
- en: '|  |          32 |          20.48 |          31.02 |          21.47 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  |          32 |          20.48 |          31.02 |          21.47 |'
- en: Appendix F Dialog Examples
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 对话示例
- en: In this section, we show some dialogue examples of binarized LLaMA-13B and Vicuna-13B.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了一些二值化 LLaMA-13B 和 Vicuna-13B 的对话示例。
- en: '![Refer to caption](img/1cf11059c5895950ec6af5a245d22c11.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1cf11059c5895950ec6af5a245d22c11.png)'
- en: 'Figure 12: Some examples of conversations. LLaMA-13B and Vicuna-13B are chosen
    to show the case of language supplementary and Q&A ability. And PB-LLM (int 8,
    10%) is selected as the comparison. We color the text to show the reasonable or
    inappropriate responses.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：一些对话示例。选择了 LLaMA-13B 和 Vicuna-13B 来展示语言补充和问答能力的情况。并选择了 PB-LLM（int 8, 10%）作为对比。我们通过为文本上色来显示合理或不恰当的响应。
- en: Appendix G Magnitude and Hessian Distribution of LLMs
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G LLMs 的幅度和 Hessian 分布
- en: 'Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ BiLLM: Pushing the Limit of
    Post-Training Quantization for LLMs") displays the distribution characteristics
    of weights and Hessian in LLMs. In this section, we provide additional examples
    to illustrate the bell-shaped distribution of weight values and the long-tailed
    distribution of Hessian weights. Figure [13](#A7.F13 "Figure 13 ‣ Appendix G Magnitude
    and Hessian Distribution of LLMs ‣ BiLLM: Pushing the Limit of Post-Training Quantization
    for LLMs") depicts the distributions of four linear layers in the first Transformer
    block of the OPT-1.3B model, while Figure [14](#A7.F14 "Figure 14 ‣ Appendix G
    Magnitude and Hessian Distribution of LLMs ‣ BiLLM: Pushing the Limit of Post-Training
    Quantization for LLMs") shows the distributions of seven linear layers in the
    sixth block of the LLaMA-7B model. The selection of these specific block positions
    is intended to demonstrate the universality of these distribution characteristics
    in LLMs.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ BiLLM：推动 LLMs 后训练量化的极限") 展示了 LLMs 中权重和 Hessian 的分布特征。在本节中，我们提供了额外的示例以说明权重值的钟形分布和
    Hessian 权重的长尾分布。图 [13](#A7.F13 "图 13 ‣ 附录 G LLMs 的幅度和 Hessian 分布 ‣ BiLLM：推动 LLMs
    后训练量化的极限") 描绘了 OPT-1.3B 模型第一个 Transformer 块中四个线性层的分布，而图 [14](#A7.F14 "图 14 ‣ 附录
    G LLMs 的幅度和 Hessian 分布 ‣ BiLLM：推动 LLMs 后训练量化的极限") 显示了 LLaMA-7B 模型第六个块中七个线性层的分布。这些特定块位置的选择旨在展示这些分布特征在
    LLMs 中的普遍性。
- en: 'Figure [15](#A7.F15 "Figure 15 ‣ Appendix G Magnitude and Hessian Distribution
    of LLMs ‣ BiLLM: Pushing the Limit of Post-Training Quantization for LLMs") displays
    the distribution of sensitive weights across 5 Transformer blocks within the OPT-1.3B
    model. We present the Hessian distribution results for both the attention and
    feedforward blocks, with the red portion indicating the top 10% of the most significant
    weight distribution. We observed that the salient weights of Q, K, and V in the
    OPT family tend to concentrate in some columns or rows. Moreover, we noticed that
    salient weights in the Out Projection layer of multi-head self-attention blocks
    are distinctly concentrated in specific columns, supporting our structured selection
    approach discussed in the main text. In contrast, the distribution of salient
    weights in the feedforward layers is more dispersed. Based on these observations,
    we adopt a sensitivity-based structured search method to identify salient columns.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图[15](#A7.F15 "图 15 ‣ 附录 G LLM的幅度和Hessian分布 ‣ BiLLM：推动LLMs后训练量化的极限")展示了OPT-1.3B模型中5个Transformer块的敏感权重分布。我们呈现了注意力和前馈块的Hessian分布结果，其中红色部分表示最重要的前10%权重分布。我们观察到，OPT系列中Q、K和V的显著权重倾向于集中在某些列或行中。此外，我们还注意到，在多头自注意力块的输出投影层中，显著权重明显集中在特定列，这支持了我们在正文中讨论的结构化选择方法。相比之下，前馈层中的显著权重分布则更为分散。基于这些观察结果，我们采用基于敏感度的结构化搜索方法来识别显著列。
- en: '![Refer to caption](img/39e70a3c7f0ae501dea416b298be1326.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/39e70a3c7f0ae501dea416b298be1326.png)'
- en: 'Figure 13: Different layers weight density distribution (blue) and hessian
    density distribution (orange) of the $1^{st}$ Transformer block of the OPT-1.3B
    model'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：OPT-1.3B模型的第$1^{st}$个Transformer块的不同层权重密度分布（蓝色）和Hessian密度分布（橙色）
- en: '![Refer to caption](img/ad6faeb860b4a40d544eb03818b49049.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/ad6faeb860b4a40d544eb03818b49049.png)'
- en: 'Figure 14: Different layers weight density distribution (blue) and hessian
    density distribution (orange) of the $6^{th}$ Transformer block of the LLaMA-7B
    model'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：LLaMA-7B模型的第$6^{th}$个Transformer块的不同层权重密度分布（蓝色）和Hessian密度分布（橙色）
- en: '![Refer to caption](img/a3601a55e6629a51a80a1c46b8de371f.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/a3601a55e6629a51a80a1c46b8de371f.png)'
- en: 'Figure 15: Distribution of top 10% salient elements in Hessian matrix. The
    distribution of $1^{st}-5^{th}$ Transformer blocks in OPT-1.3B'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：Hessian矩阵中前10%显著元素的分布。OPT-1.3B模型中第$1^{st}-5^{th}$个Transformer块的分布
