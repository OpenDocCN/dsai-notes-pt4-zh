- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:49:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:49:13'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'CLAQ: 推动低位后训练量化在大型语言模型中的极限'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.17233](https://ar5iv.labs.arxiv.org/html/2405.17233)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.17233](https://ar5iv.labs.arxiv.org/html/2405.17233)
- en: Haoyu Wang
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**王浩宇**'
- en: Shanghai Jiao Tong University
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 上海交通大学
- en: Shanghai, China
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 中国上海
- en: fayuge@sjtu.edu.cn
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: fayuge@sjtu.edu.cn
- en: '&Bei Liu^∗'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**刘蓓**^∗'
- en: Shanghai Jiao Tong University
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 上海交通大学
- en: Shanghai, China
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 中国上海
- en: beiliu@sjtu.edu.cn
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: beiliu@sjtu.edu.cn
- en: '&Hang Shao'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**邵航**'
- en: Shanghai Jiao Tong University
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 上海交通大学
- en: Shanghai, China
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 中国上海
- en: hangshao99@sjtu.edu.cn
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: hangshao99@sjtu.edu.cn
- en: '&Bo Xiao'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**肖博**'
- en: Meituan
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 美团
- en: Beijing, China
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 中国北京
- en: xiaobo09@meituan.com
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: xiaobo09@meituan.com
- en: '&Ke Zeng'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**曾克**'
- en: Meituan
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 美团
- en: Beijing, China
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 中国北京
- en: zengke02@meituan.com
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: zengke02@meituan.com
- en: '&Guanglu Wan'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**万广路**'
- en: Meituan
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 美团
- en: Beijing, China
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 中国北京
- en: wanguanglu@meituan.com
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: wanguanglu@meituan.com
- en: '&Yanmin Qian'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**钱艳敏**'
- en: Shanghai Jiao Tong University
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 上海交通大学
- en: Shanghai, China
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 中国上海
- en: yanminqian@sjtu.edu.cn Equal Contribution. This work was done when Haoyu Wang
    and Bei Liu were interns at Meituan.Corresponding Author.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: yanminqian@sjtu.edu.cn 同等贡献。此工作在王浩宇和刘蓓在美团实习期间完成。通讯作者。
- en: Abstract
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Parameter quantization for Large Language Models (LLMs) has attracted increasing
    attentions recently in reducing memory costs and improving computational efficiency.
    Early approaches have been widely adopted. However, the existing methods suffer
    from poor performance in low-bit (such as 2 to 3 bits) scenarios. In this paper,
    we present a novel and effective Column-Level Adaptive weight Quantization (CLAQ)
    framework by introducing three different types of adaptive strategies for LLM
    quantization. Firstly, a K-Means clustering based algorithm is proposed that allows
    dynamic generation of quantization centroids for each column of a parameter matrix.
    Secondly, we design an outlier-guided adaptive precision search strategy which
    can dynamically assign varying bit-widths to different columns. Finally, a dynamic
    outlier reservation scheme is developed to retain some parameters in their original
    float point precision, in trade off of boosted model performance. Experiments
    on various mainstream open source LLMs including LLaMA-1, LLaMA-2 and Yi demonstrate
    that our methods achieve the state-of-the-art results across different bit settings,
    especially in extremely low-bit scenarios. Code is available at [https://github.com/fayuge/CLAQ](https://github.com/fayuge/CLAQ).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 对大型语言模型（LLMs）的参数量化最近引起了越来越多的关注，特别是在减少内存成本和提高计算效率方面。早期的方法已经被广泛采用。然而，现有的方法在低位（如2到3位）场景中表现不佳。本文提出了一种新颖且有效的列级自适应权重量化（CLAQ）框架，通过引入三种不同类型的自适应策略来进行LLM量化。首先，提出了一种基于K均值聚类的算法，允许对每个参数矩阵的列动态生成量化中心。其次，我们设计了一种基于异常值引导的自适应精度搜索策略，可以动态地为不同列分配不同的位宽。最后，开发了一种动态异常值保留方案，在提升模型性能的同时保留一些参数的原始浮点精度。在包括LLaMA-1、LLaMA-2和Yi在内的各种主流开源LLMs上的实验表明，我们的方法在不同位设置下实现了最先进的结果，尤其是在极低位场景中。代码可在
    [https://github.com/fayuge/CLAQ](https://github.com/fayuge/CLAQ) 获得。
- en: 1 Introduction
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recent advancements in Large Language Models (LLMs) have yielded impressive
    accomplishments across a variety of tasks. Building on the pioneering work of
    GPT-3 [gpt3](#bib.bib3) , a series of influential LLMs such as OPT [zhang2022opt](#bib.bib47)
    , BLOOM [le2023bloom](#bib.bib23) , LLaMA [touvron2023LLaMA](#bib.bib37) , Mixtral
    [jiang2024mixtral](#bib.bib21) and Yi [young2024yi](#bib.bib45) have consistently
    demonstrated that expanding model size predictably promotes modeling accuracy
    (a.k.a. the scaling law [henighan2020scaling](#bib.bib17) ). The pursue of large
    (e.g. hundreds of billions of parameters), and even larger language models is
    often considered as a promising avenue towards Artificial General Intelligence
    (AGI) [bubeck2023sparks](#bib.bib4) .
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的最新进展在各种任务中取得了令人印象深刻的成就。基于GPT-3 [gpt3](#bib.bib3) 的开创性工作，一系列有影响力的LLMs如OPT
    [zhang2022opt](#bib.bib47) 、BLOOM [le2023bloom](#bib.bib23) 、LLaMA [touvron2023LLaMA](#bib.bib37)
    、Mixtral [jiang2024mixtral](#bib.bib21) 和Yi [young2024yi](#bib.bib45) 一直表现出扩展模型规模可以预期地提高建模准确性（即缩放法则
    [henighan2020scaling](#bib.bib17) ）。追求大型（如数百亿参数），甚至更大的语言模型，通常被认为是通向人工通用智能（AGI）[bubeck2023sparks](#bib.bib4)
    的一种有前途的途径。
- en: Nevertheless, the significant computational and memory demands of LLMs pose
    unprecedented challenges. For example, deploying the GPT-3 model requires a staggering
    350GB of memory merely to accommodate its parameters in half-precision format
    (FP16), with a minimum of five high-end A100-80G GPUs. Such immense memory demands,
    coupled with the enormous runtime communication overhead, hinder the integration
    of LLMs into practical applications. To tackle this problem, 8-bit Post-Training
    Quantization (PTQ) methods for both model weights and activations have been proposed
    [xiao2023smoothquant](#bib.bib40) ; [dettmers2022int8](#bib.bib8) , which have
    led to reduced model sizes in memory and negligible declines in accuracy. Unfortunately,
    when PTQ methods [chee2024quip](#bib.bib5) ; [lin2023awq](#bib.bib29) ; [kim2023squeezellm](#bib.bib22)
    are attempted in quantizing the parameters into 4 bits or less, notable decreases
    of performance have been observed, making further reduction of model sizes a challenging
    task. Additionally, several quantization-aware training (QAT) approaches [liu2023llmqat](#bib.bib30)
    ; [dettmers2024qlora](#bib.bib9) ; [li2023loftq](#bib.bib27) have been introduced
    to enhance the model performance. Nonetheless, these algorithms integrate a training
    phase within the quantization process, thus imposing substantial demands for computational
    resources.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，LLMs的显著计算和内存需求带来了前所未有的挑战。例如，部署GPT-3模型仅为了容纳其参数的半精度格式（FP16）就需要高达350GB的内存，并且至少需要五张高端A100-80G
    GPU。这么巨大的内存需求，加上巨大的运行时通信开销，阻碍了LLMs在实际应用中的整合。为了解决这个问题，已经提出了针对模型权重和激活的8位后训练量化（PTQ）方法[xiao2023smoothquant](#bib.bib40)；[dettmers2022int8](#bib.bib8)，这些方法在内存中减少了模型大小，并且准确率下降微乎其微。不幸的是，当PTQ方法[xiao2023smoothquant](#bib.bib40)；[dettmers2022int8](#bib.bib8)；[kim2023squeezellm](#bib.bib22)尝试将参数量化为4位或更少时，观察到性能显著下降，使得进一步减小模型大小变得具有挑战性。此外，已经引入了一些量化感知训练（QAT）方法[liu2023llmqat](#bib.bib30)；[dettmers2024qlora](#bib.bib9)；[li2023loftq](#bib.bib27)以提升模型性能。然而，这些算法在量化过程中集成了训练阶段，从而对计算资源提出了较高的需求。
- en: In this paper, we introduce a novel and effective training-free quantization
    framework for LLMs, namely Column-Level Adaptive weight Quantization (CLAQ), which
    utilizes three distinct column-level strategies to enhance the performance of
    low-bit (i.e., 2-bit and 3-bit) quantization. Specifically, we initially apply
    a K-Means clustering based quantization that enables the adaptive generation of
    quantization centroids for each column of a weight matrix. Then, a simple yet
    powerful quantization sensitivity metric (Outlier Order) is described, which aims
    to identify parameter columns that are highly sensitive to quantization. Leveraging
    this metric, we further proposed two strategies to enhance quantization performance,
    including Column-Level Adaptive Precision (AP) quantization and Column-Level Adaptive
    Outlier Reservation (OR). Both schemes can significantly boost the performance
    of the low-bit quantized LLMs with only a slight increase in memory footprint.
    Notably, our fusion model (AP+OR) yields the best results in extremely low-bit
    scenarios. Extensive experiments on mainstream public LLMs and benchmarks demonstrate
    that our method achieves the state-of-the-art performance across various quantization
    bit-widths.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一种新颖且有效的免训练量化框架，用于大规模语言模型（LLMs），即列级自适应权重量化（CLAQ），该框架利用三种不同的列级策略来提升低比特（即2位和3位）量化的性能。具体而言，我们首先应用基于K-Means聚类的量化方法，该方法能够自适应地为权重矩阵的每一列生成量化中心。接着，我们描述了一种简单而强大的量化灵敏度指标（异常值顺序），旨在识别对量化高度敏感的参数列。利用这一指标，我们进一步提出了两种增强量化性能的策略，包括列级自适应精度（AP）量化和列级自适应异常值保留（OR）。这两种方案可以显著提升低比特量化LLMs的性能，同时仅略微增加内存占用。值得注意的是，我们的融合模型（AP+OR）在极低比特场景下取得了最佳结果。对主流公共LLMs和基准测试的广泛实验表明，我们的方法在各种量化比特宽度下均达到了最先进的性能。
- en: 'The main contributions of our work can be summarized as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献可以总结如下：
- en: '1.'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'We introduce a novel centroid selection strategy for quantization: K-Means
    based weight quantization. The architecture of K-Means based quantization is illustrated
    in Figure [1](#S2.F1 "Figure 1 ‣ Quantization-Aware Training ‣ 2 LLM Quantization
    ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs"). Compared
    to existing methods, K-Means based quantization excels in more accurately approximating
    the parameter distribution of pre-trained models and dynamically generating quantization
    centroids. Our proposed quantization scheme surpasses all current 3-bit and 4-bit
    PTQ methods under equivalent or larger quantization group sizes.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '我们引入了一种新颖的质心选择策略用于量化：基于K均值的权重量化。基于K均值的量化架构如图[1](#S2.F1 "图 1 ‣ 量化感知训练 ‣ 2 LLM
    量化 ‣ CLAQ: 推动LLMs低位后训练量化的极限")所示。与现有方法相比，基于K均值的量化在更准确地逼近预训练模型的参数分布和动态生成量化质心方面表现优越。我们提出的量化方案在相同或更大的量化组大小下超越了所有现有的3位和4位PTQ方法。'
- en: '2.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We present a column-wise outlier ratio based quantization sensitivity metric,
    namely Outlier Order, to assess the importance of parameters in LLMs. Based on
    this metric, Adaptive Precision (AP) quantization and Outlier Reservation (OR)
    are proposed to allocate higher budget to columns identified as more sensitive
    to quantization errors. Notably, this sensitivity metric emerges as an extremely
    efficient guiding principle, which can be calculated once easily and directly
    applied to both AP and OR.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种基于列的异常值比例的量化敏感度指标，即异常值顺序（**Outlier Order**），用于评估LLMs中参数的重要性。基于这一指标，提出了自适应精度（**AP**）量化和异常值保留（**OR**）方法，以便为被识别为对量化误差更敏感的列分配更高的预算。值得注意的是，这一敏感度指标作为一种极为高效的指导原则，可以轻松计算一次，并直接应用于**AP**和**OR**。
- en: '3.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: We implement Adaptive Precision quantization with the guidance of Outlier Order.
    Different levels of precision are adaptively assigned within each weight matrix.
    By allocating higher quantization budget to columns with a greater concentration
    of outliers, we achieve a substantial reduction in quantization errors. Furthermore,
    adaptive precision quantization allows arbitrary equivalent bit-width from 2-bit
    to 4-bit, facilitating the deployment of LLMs across a variety of end-devices
    with different memory constraints.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在**Outlier Order**的指导下实现了自适应精度量化。在每个权重矩阵中，自适应分配不同级别的精度。通过为异常值浓度更大的列分配更高的量化预算，我们实现了量化误差的显著减少。此外，自适应精度量化允许从2位到4位的任意等效位宽，便于在具有不同内存限制的各种终端设备上部署LLMs。
- en: '4.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: We develop a novel Outliers Reservation (OR) strategy to further enhance the
    performance with only a slight increase in memory footprint. Based on Outlier
    Order, OR involves reserving FP16 precision outliers for columns with a higher
    concentration of outliers to preserve model performance, whereas columns with
    a lower outlier ratio are allocated less reservation budget. Moreover, the OR
    strategy can be combined with AP quantization, which exhibits the best results
    in low-bit quantization.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们开发了一种新颖的异常值保留（**OR**）策略，以在仅略微增加内存占用的情况下进一步提升性能。基于异常值顺序（**Outlier Order**），**OR**策略包括为具有较高异常值浓度的列保留FP16精度的异常值，以保持模型性能，而异常值比例较低的列则分配较少的保留预算。此外，**OR**策略可以与**AP**量化结合使用，在低位量化中表现出最佳结果。
- en: 2 LLM Quantization
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 LLM 量化
- en: Post-Training Quantization
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 后训练量化
- en: Post-training quantization methods [wang2020towards](#bib.bib38) ; [hubara2021accurate](#bib.bib20)
    ; [li2021brecq](#bib.bib28) ; [nagel2020up](#bib.bib32) quantize LLMs without
    fine-tuning the parameters, while compensating the quantization error through
    specially designed techniques. Previous work [xiao2023smoothquant](#bib.bib40)
    ; [yao2022zeroquant](#bib.bib43) ; [dettmers2022int8](#bib.bib8) ; [park2022qumm](#bib.bib33)
    ; [Dettmers2022tim](#bib.bib11) have utilized the vanilla Round To Nearest quantization.
    Based on the OBS approach [frantar2022OBQ](#bib.bib13) , GPTQ [frantar2022gptq](#bib.bib15)
    adjusted the remaining weights while quantizing, offering an effective PTQ paradigm.
    A series of work have [lin2023awq](#bib.bib29) ; [xiao2023smoothquant](#bib.bib40)
    ; [lee2024owq](#bib.bib24) ; [dettmers2023spqr](#bib.bib10) ; [yin2023owl](#bib.bib44)
    ; [heo2023rethinking](#bib.bib18) ; [wei2022outliersupp](#bib.bib39) noticed the
    significance of outlier treatment in LLM quantization. By preserving outliers,
    the performance of quantized models can be further improved. PTQ approach alleviates
    the computational burden associated with extensive re-training, thus facilitating
    the efficient deployment of LLMs in resource-constrained environments.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化方法 [wang2020towards](#bib.bib38)；[hubara2021accurate](#bib.bib20)；[li2021brecq](#bib.bib28)；[nagel2020up](#bib.bib32)
    在不微调参数的情况下对LLMs进行量化，同时通过专门设计的技术来补偿量化误差。之前的工作 [xiao2023smoothquant](#bib.bib40)；[yao2022zeroquant](#bib.bib43)；[dettmers2022int8](#bib.bib8)；[park2022qumm](#bib.bib33)；[Dettmers2022tim](#bib.bib11)
    使用了原始的四舍五入到最近量化。基于OBS方法 [frantar2022OBQ](#bib.bib13) ，GPTQ [frantar2022gptq](#bib.bib15)
    在量化的同时调整了剩余权重，提供了一种有效的PTQ范式。一系列工作 [lin2023awq](#bib.bib29)；[xiao2023smoothquant](#bib.bib40)；[lee2024owq](#bib.bib24)；[dettmers2023spqr](#bib.bib10)；[yin2023owl](#bib.bib44)；[heo2023rethinking](#bib.bib18)；[wei2022outliersupp](#bib.bib39)
    注意到LLM量化中异常值处理的重要性。通过保留异常值，可以进一步提高量化模型的性能。PTQ方法减轻了与广泛再训练相关的计算负担，从而促进了LLMs在资源受限环境中的高效部署。
- en: Quantization-Aware Training
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量化感知训练
- en: Quantization-Aware Training (QAT) incorporates a training phase within the quantization
    workflow. Owing to the model’s ability to adapt to the constraints imposed by
    quantization, QAT typically leads to superior model performance. [li2023loftq](#bib.bib27)
    ; [dettmers2024qlora](#bib.bib9) ; [xu2023qa](#bib.bib41) integrated quantization
    with the basis of LoRA [hu2021lora](#bib.bib19) . [xu2024onebit](#bib.bib42) performed
    extreme quantization with 1-bit parameters. In [shao2023omniquant](#bib.bib36)
    , instead of training the quantized parameters, a learnable equivalent transformation
    is trained for quantization. By learning the quantization error, QAT can mitigate
    the information loss brought by quantization schemes. Nonetheless, the advantage
    of QAT comes with the inclusion of specialized training phases, which not only
    requires additional time but also increases the computational overhead and complexity
    of the quantization task. In practical applications, it may also couple with model
    fine-tuning processes that complicates the workflow. Therefore, in this work,
    we focus on PTQ approach first, and leave the study of QAT approach as a future
    work.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 量化感知训练（QAT）在量化工作流程中引入了一个训练阶段。由于模型能够适应量化施加的约束，QAT通常能带来更优的模型性能。[li2023loftq](#bib.bib27)；[dettmers2024qlora](#bib.bib9)；[xu2023qa](#bib.bib41)
    在LoRA的基础上整合了量化。[xu2024onebit](#bib.bib42)进行了极端量化，使用了1位参数。在[shao2023omniquant](#bib.bib36)中，不是训练量化参数，而是训练了一个可学习的等效变换用于量化。通过学习量化误差，QAT可以减轻量化方案带来的信息损失。然而，QAT的优势伴随着专门训练阶段的加入，这不仅需要额外的时间，还增加了量化任务的计算开销和复杂性。在实际应用中，它还可能与模型微调过程相结合，复杂化工作流程。因此，在本工作中，我们首先关注PTQ方法，将QAT方法的研究留作未来工作。
- en: '![Refer to caption](img/bd7c45a35412c7f099ca19bd3d8de88e.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/bd7c45a35412c7f099ca19bd3d8de88e.png)'
- en: 'Figure 1: The K-Means clustering based quantization. Elements in weight matrix
    column are input of K-Means clustering and the quantization centroids are derived
    as the output of clustering algorithm. Then the pre-trained weights are quantized
    to the nearest K-Means class center.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：基于K-Means聚类的量化。权重矩阵列中的元素是K-Means聚类的输入，量化质心作为聚类算法的输出。然后，预训练的权重被量化到最近的K-Means类别中心。
- en: 3 CLAQ
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CLAQ
- en: 3.1 K-Means Based Quantization
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于K-Means的量化
- en: In general, prior quantization techniques on LLMs have typically adopted uniform
    quantization levels (i.e., quantization centroids are equally spaced), such as
    SmoothQuant [xiao2023smoothquant](#bib.bib40) and GPTQ [frantar2022gptq](#bib.bib15)
    . Alternative methods adopt strategies that sample quantization centroids from
    a normal distribution, as demonstrated by QLoRA [dettmers2024qlora](#bib.bib9)
    . Nonetheless, all of the above approaches fail to accurately capture the true
    distribution of a group of model parameters, given the heterogeneous nature of
    parameter magnitudes and distributions across various network layers.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，先前对 LLM 的量化技术通常采用均匀量化水平（即量化质心等距），例如 SmoothQuant [xiao2023smoothquant](#bib.bib40)
    和 GPTQ [frantar2022gptq](#bib.bib15) 。替代方法采用从正态分布中采样量化质心的策略，例如 QLoRA [dettmers2024qlora](#bib.bib9)
    。然而，以上所有方法都无法准确捕捉一组模型参数的真实分布，因为参数的量级和各网络层的分布是异质的。
- en: 'To tackle this issue, we first introduce a straightforward yet effective strategy:
    employing K-Means clustering for the selection of quantization centroids (Figure
    [1](#S2.F1 "Figure 1 ‣ Quantization-Aware Training ‣ 2 LLM Quantization ‣ CLAQ:
    Pushing the Limits of Low-Bit Post-Training Quantization for LLMs")). The clustering
    samples (i.e., quantization groups) are the columns of a parameter matrix from
    either self-attention or MLP structures in the LLMs.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这个问题，我们首先引入一个简单而有效的策略：采用 K-Means 聚类来选择量化质心（图 [1](#S2.F1 "Figure 1 ‣ Quantization-Aware
    Training ‣ 2 LLM Quantization ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs")）。聚类样本（即量化组）是来自 LLM 中自注意力或 MLP 结构的参数矩阵的列。'
- en: 'Specifically, when quantizing an parameter matrix $W$-th column is derived
    as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，当量化一个参数矩阵的 $W$-th 列时，结果如下：
- en: '|  | $C_{W_{j}}=KMeans(W_{j})$ |  | (1) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $C_{W_{j}}=KMeans(W_{j})$ |  | (1) |'
- en: 'Where $KMeans$ can be formalized as:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: $KMeans$ 可以形式化为：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'Inspired by [frantar2022gptq](#bib.bib15) , our approach quantizes the parameter
    matrix $W$ per column through clustering and quantization. We adopt the same approach
    as GPTQ [frantar2022gptq](#bib.bib15) for updating the remaining parameters. The
    advantage of K-Means based quantization is three-fold: Firstly, the centroids
    generated by K-Means clustering adaptively conform to the distribution of the
    LLM’s parameters. The better matching with the underlying parameter distribution
    can minimize information loss and mitigate quantization errors. Secondly, our
    K-Means quantization reduces storage requirements by storing a single vector of
    K-Means centroids as codebook per column, yet not losing the performance. Previous
    approaches have employed smaller quantization groups to boost low-bit quantization
    performance. However, smaller groups incur additional overhead of codebook storage.
    We found that with equivalent or even smaller model sizes, our algorithm demonstrates
    superior performance compared to prior works, illustrating the effectiveness of
    K-Means based quantization. Lastly, our approach is simple and efficient. Previous
    studies such as OmniQuant [shao2023omniquant](#bib.bib36) and LLM-QAT [liu2023llmqat](#bib.bib30)
    trained auxiliary network components to ensure quantization efficacy. In comparison,
    our method avoids the necessity of additional training.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 [frantar2022gptq](#bib.bib15) 的启发，我们的方法通过聚类和量化来对参数矩阵 $W$ 进行按列量化。我们采用与 GPTQ
    [frantar2022gptq](#bib.bib15) 相同的方法来更新其余参数。基于 K-Means 的量化有三方面的优势：首先，K-Means 聚类生成的质心自适应地符合
    LLM 参数的分布。与基础参数分布的更好匹配可以最小化信息损失并减轻量化误差。其次，我们的 K-Means 量化通过将 K-Means 质心的单个向量作为每列的代码本来减少存储需求，同时保持性能不变。之前的方法采用更小的量化组来提升低位量化性能。然而，更小的组会增加代码本存储的额外开销。我们发现，在模型大小相当或更小的情况下，我们的算法相比于之前的工作展现了更优的性能，说明了基于
    K-Means 的量化的有效性。最后，我们的方法简单高效。先前的研究如 OmniQuant [shao2023omniquant](#bib.bib36)
    和 LLM-QAT [liu2023llmqat](#bib.bib30) 训练了辅助网络组件以确保量化效果。相比之下，我们的方法避免了额外训练的必要。
- en: '![Refer to caption](img/d96b86e869bcdc9b73b1aa3e9696acf8.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d96b86e869bcdc9b73b1aa3e9696acf8.png)'
- en: 'Figure 2: The overall structure of CLAQ: quantized models are obtained from
    different quantization approaches. The single-precision K-Means based quantization
    runs without sensitivity calculation. We leverage outlier ratio based quantization
    sensitivity metric (a) to provide the guidance of the column-level adaptive outlier
    reservation (OR, b) and column-level adaptive precision (AP, c). The AP and OR
    strategies are orthogonal to each other.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：CLAQ的整体结构：量化模型来自不同的量化方法。单精度K均值量化在没有敏感性计算的情况下运行。我们利用异常值比例基础的量化敏感性度量（a）来提供列级自适应异常值保留（OR，b）和列级自适应精度（AP，c）的指导。AP和OR策略彼此正交。
- en: 3.2 Outlier Ratio Based Quantization Sensitivity Metric
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 异常值比例基础的量化敏感性度量
- en: Previous studies [xiao2023smoothquant](#bib.bib40) ; [lin2023awq](#bib.bib29)
    revealed that outliers in LLMs play a pivotal role in determining the performance
    of quantized models. Inspired by [yin2023owl](#bib.bib44) , we hypothesize that
    the number of outliers within the columns of the weight matrix can act as an indicator
    of the importance (or sensitivity) of each column for quantization. Therefore,
    we adopt outlier ratio as the key guiding metric for LLM quantization.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究[xiao2023smoothquant](#bib.bib40) ; [lin2023awq](#bib.bib29)揭示了LLM中的异常值在决定量化模型性能中的关键作用。受到[yin2023owl](#bib.bib44)的启发，我们假设权重矩阵中列的异常值数量可以作为每列量化重要性（或敏感性）的指标。因此，我们采用异常值比例作为LLM量化的关键指导指标。
- en: In this work, we introduce a novel, computationally efficient, and inference-agnostic
    adaptive precision guidance metric, termed Outlier Order. It can be utilized to
    allocate adaptive precision (AP) or apply Outlier Reservation (OR) according to
    the proportion of outliers within each column of the model weight. Columns with
    a high proportion of outliers, which are considered more important and sensitive
    to quantization, are allocated higher precision or more outlier reservation budget
    to preserve model capability, while columns with a lower outlier ratio are assigned
    lower precision or less reservation budget.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们引入了一种新颖、计算高效且推理无关的自适应精度指导指标，称为异常值排序。它可以用于根据模型权重中每列的异常值比例分配自适应精度（AP）或应用异常值保留（OR）。具有较高异常值比例的列被认为对量化更重要、更敏感，因此分配更高的精度或更多的异常值保留预算以保持模型能力，而具有较低异常值比例的列则分配较低的精度或较少的保留预算。
- en: 'Specifically, for an $i\times j$ is defined as:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 具体而言，对于一个$i\times j$定义如下：
- en: '|  | 
    |  | (3) |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | 
    |  | (3) |'
- en: where $Card$ as Outlier Order.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$Card$作为异常值排序。
- en: 3.3 Column-Level Adaptive Precision Quantization
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 列级自适应精度量化
- en: Mixed Precision quantization (MP) involves the integration of multiple quantization
    levels with varying precisions to mitigate the performance degradation in highly
    compressed models. Mixed-precision has been applied to LLMs in previous studies
    [daillm](#bib.bib26) ; [li2023lijinhao](#bib.bib25) . [daillm](#bib.bib26) leveraged
    gradient magnitudes of parameters as a guiding principle for determining quantization
    precision levels. [li2023lijinhao](#bib.bib25) adopted a conventional criterion
    based on the relative magnitude of parameters concerning the input. Nonetheless,
    their metrics of precision allocation can not accurately reflect the importance
    of parameters and the performances of mixed-precision quantization were unsatisfactory.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 混合精度量化（MP）涉及将多种量化水平与不同精度集成，以减轻高度压缩模型中的性能下降。混合精度已在之前的研究中应用于LLM [daillm](#bib.bib26)
    ; [li2023lijinhao](#bib.bib25) 。[daillm](#bib.bib26)利用参数的梯度幅度作为确定量化精度水平的指导原则。[li2023lijinhao](#bib.bib25)采用了基于输入相对参数幅度的传统标准。然而，他们的精度分配指标不能准确反映参数的重要性，混合精度量化的表现不尽如人意。
- en: 'To further improve the performance of low-bit quantization, we present a Column-Level
    Adaptive Precision (AP) strategy directed by Outlier Order, which avoids the heavy
    computational overhead brought by metric calculation. Additionally, experimental
    results reveal the AP approach guided by Outlier Order outperforms previous methods
    with other mixed-precision metrics. To perform the adaptive precision quantization,
    firstly, the parameters of the matrix are sorted per column based on their outlier
    ratio as described in section 3.2\. Secondly, the bit allocation scheme is determined
    according to the target compression ratio. For example, a 2.2-bit quantized model
    is derived by allocating top 10% outlier-concentrated columns to 4-bit, and allocating
    2-bit to the rest. Through AP quantization, the columns with higher sensitivity
    to quantization are allocated higher precision. Let $B$ of the model is denoted
    as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高低比特量化的性能，我们提出了一种由异常值排序指导的列级自适应精度（AP）策略，这避免了由度量计算带来的高计算开销。此外，实验结果表明，由异常值排序指导的AP方法在其他混合精度度量下优于以往的方法。为了执行自适应精度量化，首先，根据第3.2节中描述的异常值比例对矩阵的参数按列进行排序。其次，根据目标压缩比确定比特分配方案。例如，通过将前10%异常值集中列分配为4比特，而将其余列分配为2比特，可以得到一个2.2比特量化模型。通过AP量化，对量化更敏感的列分配更高的精度。设模型的$B$如下所示：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: where the threshold $T_{AP}$. Detailed analysis on various bit-width candidates
    in adaptive precision schemes can be found in Appendix D. Experiments on assigning
    different bits to different parameter matrices are described in Appendix G.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中阈值为$T_{AP}$。关于自适应精度方案中各种比特宽度候选的详细分析见附录D。关于将不同比特分配给不同参数矩阵的实验描述见附录G。
- en: 3.4 Column-Level Adaptive Outlier Reservation
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 列级自适应异常值保留
- en: As [lin2023awq](#bib.bib29) ; [lee2024owq](#bib.bib24) ; [kim2023squeezellm](#bib.bib22)
    ; [ashkboos2023quik](#bib.bib1) have revealed, the preservation of an appropriate
    fraction of outliers can significantly improve the performance of quantized models.
    However, previous approaches have simply retained outliers based on the entire
    parameter matrix and resulted in sub-optimal performances.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[lin2023awq](#bib.bib29)；[lee2024owq](#bib.bib24)；[kim2023squeezellm](#bib.bib22)；[ashkboos2023quik](#bib.bib1)所揭示的，适当保留一定比例的异常值可以显著提高量化模型的性能。然而，以往的方法仅仅基于整个参数矩阵保留异常值，导致性能不佳。
- en: In this work, we propose a Column-Level Adaptive Outlier Reservation (OR) strategy
    to adaptively preserve full-precision outliers across different columns. We leverage
    the outlier ratio defined in Sec 3.2 to guide their retention strategy. The analysis
    of outlier distribution in weight (see Appendix A) illustrates that 90% of outliers
    are concentrated within the top 10% of columns, leading us to adopt a retention
    policy that preserves a greater number of outliers in the initial 10% of columns,
    while conserving fewer outliers in the remaining 90%.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种列级自适应异常值保留（OR）策略，以自适应地保留不同列中的全精度异常值。我们利用第3.2节中定义的异常值比例来指导保留策略。对权重中异常值分布的分析（见附录A）表明，90%的异常值集中在前10%的列中，这导致我们采用了在初始10%列中保留更多异常值，同时在其余90%列中保留较少异常值的保留政策。
- en: 'To apply the OR strategy, firstly, we pick top 10% quantization-sensitive columns
    with Outlier Order metric. Then we preserve a higher proportion $o_{1}$-th column
    is presented below:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用OR策略，首先，我们选择前10%量化敏感的列，使用异常值排序度量。然后我们保留更高比例的异常值，$o_{1}$-th列如下所示：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: 'where $OR$ represents the threshold of choosing the top 10% columns of higher
    outlier ratios. Columns with higher outlier ratio will be assigned more FP16 outlier
    reservation. Same proportion of outliers is reserved in all matrices to control
    the size of the compressed model. The advantages of this approach are threefold:
    First, by retaining more full-precision parameters in quantization-susceptible
    columns, model performance is better preserved in quantization. Second, the metrics
    employed for outlier reservation align with those used for generating adaptive
    precision quantization, avoiding redundant computational overhead. Thirdly, the
    implementation of outlier reservation and adaptive precision quantization are
    capable of being employed concurrently. Our fusion model (AP+OR) outperforms other
    PTQ methods with similar memory footprints.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $OR$ 表示选择前 10% 高离群点比率列的阈值。离群点比率较高的列将分配更多的 FP16 离群点保留。所有矩阵中保留的离群点比例相同，以控制压缩模型的大小。这种方法的优势有三方面：首先，通过在对量化敏感的列中保留更多的全精度参数，模型性能在量化中得到更好的保留。其次，离群点保留所用的指标与生成自适应精度量化所用的指标一致，避免了冗余的计算开销。第三，离群点保留和自适应精度量化的实现能够同时使用。我们的融合模型
    (AP+OR) 在类似内存占用的 PTQ 方法中表现更优。
- en: 4 Experiments and Results
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验与结果
- en: 4.1 Experimental Settings
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 实验设置
- en: Models
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型
- en: The experiments are conducted on popular public LLMs. We adopted LLaMA [touvron2023LLaMA](#bib.bib37)
    and Yi [young2024yi](#bib.bib45) as the baseline models, known for their superior
    performances in English and Chinese language tasks, respectively. The pre-trained
    base versions are employed in our experiments.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 实验在流行的公共 LLM 上进行。我们采用了 LLaMA [touvron2023LLaMA](#bib.bib37) 和 Yi [young2024yi](#bib.bib45)
    作为基线模型，分别在英语和中文任务中表现优异。实验中使用了预训练的基础版本。
- en: Datasets
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 数据集
- en: We employ widely-used datasets C4 [raffel2020c4](#bib.bib34) and WiKiText2 [merity2016wiki2](#bib.bib31)
    as perplexity test sets. To evaluate downstream task capabilities, we tested on
    a diverse range of zero-shot benchmarks, including WinoGrande [sakaguchi2021winogrande](#bib.bib35)
    , PiQA [bisk2020piqa](#bib.bib2) , HellaSwag [zellers2019hellaswag](#bib.bib46)
    , ARC (comprising ARC-easy and ARC-challenge) [clark2018think](#bib.bib7) and
    BoolQ [clark2019boolq](#bib.bib6) . Additionally, C4 was chosen as the calibration
    dataset for tuning our quantization process. It is noteworthy to emphasize that
    the choice of calibration data might have an impact on experimental outcomes,
    as detailed in Appendix H.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用了广泛使用的数据集 C4 [raffel2020c4](#bib.bib34) 和 WiKiText2 [merity2016wiki2](#bib.bib31)
    作为困惑度测试集。为了评估下游任务能力，我们在一系列零样本基准上进行了测试，包括 WinoGrande [sakaguchi2021winogrande](#bib.bib35)、PiQA
    [bisk2020piqa](#bib.bib2)、HellaSwag [zellers2019hellaswag](#bib.bib46)、ARC（包括
    ARC-easy 和 ARC-challenge）[clark2018think](#bib.bib7) 和 BoolQ [clark2019boolq](#bib.bib6)。此外，C4
    被选择为调整量化过程的校准数据集。需要特别强调的是，校准数据的选择可能会影响实验结果，详见附录 H。
- en: 'Table 1: Results of perplexity on WikiText2 [merity2016wiki2](#bib.bib31) and
    C4 [raffel2020c4](#bib.bib34) of our proposed CLAQ and other methods. CLAQ models
    with * are fusion models employing column-level adaptive precision and outlier
    reservation techniques. The detail of fusion model setups is described in Appendix
    F. $g=128$ means group size of quantization is 128, followed by the equivalent
    bit-width in italics. The results of LLaMA-2 series and Yi-34B models can be found
    in Appendix E.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 我们提出的 CLAQ 和其他方法在 WikiText2 [merity2016wiki2](#bib.bib31) 和 C4 [raffel2020c4](#bib.bib34)
    上的困惑度结果。带有 * 的 CLAQ 模型是采用列级自适应精度和离群点保留技术的融合模型。融合模型设置的详细信息见附录 F。$g=128$ 表示量化的组大小为
    128，后跟斜体中的等效位宽。LLaMA-2 系列和 Yi-34B 模型的结果见附录 E。'
- en: '| Method | # Bits |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | # 位数 |'
- en: '&#124; LLaMA1-7B &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLaMA1-7B &#124;'
- en: '&#124; WikiText2↓/C4↓ &#124;'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText2↓/C4↓ &#124;'
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LLaMA1-13B &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLaMA1-13B &#124;'
- en: '&#124; WikiText2↓/C4↓ &#124;'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText2↓/C4↓ &#124;'
- en: '|'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LLaMA1-30B &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLaMA1-30B &#124;'
- en: '&#124; WikiText2↓/C4↓ &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText2↓/C4↓ &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LLaMA1-65B &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLaMA1-65B &#124;'
- en: '&#124; WikiText2↓/C4↓ &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText2↓/C4↓ &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| – | 16 | 5.63/7.08 | 5.02/6.61 | 4.04/5.97 | 3.49/5.61 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| – | 16 | 5.63/7.08 | 5.02/6.61 | 4.04/5.97 | 3.49/5.61 |'
- en: '| RTN[frantar2022gptq](#bib.bib15) | 4 | 6.43/7.93 | 5.55/6.98 | 4.57/6.34
    | 3.87/5.85 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| RTN[frantar2022gptq](#bib.bib15) | 4 | 6.43/7.93 | 5.55/6.98 | 4.57/6.34
    | 3.87/5.85 |'
- en: '| GPTQ[frantar2022gptq](#bib.bib15) | 4 | 5.99/7.43 | 5.26/6.83 | 4.35/6.20
    | 3.77/5.79 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ[frantar2022gptq](#bib.bib15) | 4 | 5.99/7.43 | 5.26/6.83 | 4.35/6.20
    | 3.77/5.79 |'
- en: '| LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 10.9/7.4 | 9.6/6.5 | 7.3/6.5 | –
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 10.9/7.4 | 9.6/6.5 | 7.3/6.5 | –
    |'
- en: '| AWQ[lin2023awq](#bib.bib29) | 4 | 6.08/7.52 | 5.34/6.86 | 4.39/6.17 | 3.76/5.77
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| AWQ[lin2023awq](#bib.bib29) | 4 | 6.08/7.52 | 5.34/6.86 | 4.39/6.17 | 3.76/5.77
    |'
- en: '| OmniQuant[shao2023omniquant](#bib.bib36) | 4 | 5.86/7.34 | 5.21/6.76 | 4.25/6.11
    | 3.71/5.73 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant[shao2023omniquant](#bib.bib36) | 4 | 5.86/7.34 | 5.21/6.76 | 4.25/6.11
    | 3.71/5.73 |'
- en: '| SqueezeLLM[kim2023squeezellm](#bib.bib22) | 4.05 | 5.79/7.21 | 5.18/6.71
    | 4.22/6.06 | 3.76/5.69 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeLLM[kim2023squeezellm](#bib.bib22) | 4.05 | 5.79/7.21 | 5.18/6.71
    | 4.22/6.06 | 3.76/5.69 |'
- en: '| SpQR[dettmers2023spqr](#bib.bib10) | 3.94/3.96/3.89/3.90 | 5.87/7.28 | 5.22/6.72
    | 4.25/6.08 | 3.68/5.70 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SpQR[dettmers2023spqr](#bib.bib10) | 3.94/3.96/3.89/3.90 | 5.87/7.28 | 5.22/6.72
    | 4.25/6.08 | 3.68/5.70 |'
- en: '| CLAQ | 4 | 5.78/7.21 | 5.15/6.70 | 4.17/6.06 | 3.62/5.69 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 4 | 5.78/7.21 | 5.15/6.70 | 4.17/6.06 | 3.62/5.69 |'
- en: '| GPTQ[frantar2022gptq](#bib.bib15) | 3 | 8.0/9.54 | 6.54/8.15 | 5.59/7.29
    | 4.94/6.69 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ[frantar2022gptq](#bib.bib15) | 3 | 8.0/9.54 | 6.54/8.15 | 5.59/7.29
    | 4.94/6.69 |'
- en: '| AWQ[lin2023awq](#bib.bib29) | 3 | 11.88/13.26 | 7.45/9.13 | 10.07/12.67 |
    5.21/7.11 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| AWQ[lin2023awq](#bib.bib29) | 3 | 11.88/13.26 | 7.45/9.13 | 10.07/12.67 |
    5.21/7.11 |'
- en: '| OmniQuant[shao2023omniquant](#bib.bib36) | 3 | 6.49/8.19 | 5.68/7.32 | 4.74/6.57
    | 4.04/6.07 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant[shao2023omniquant](#bib.bib36) | 3 | 6.49/8.19 | 5.68/7.32 | 4.74/6.57
    | 4.04/6.07 |'
- en: '| CLAQ | 3 | 6.47/7.87 | 5.61/7.14 | 4.79/6.49 | 4.11/6.00 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 3 | 6.47/7.87 | 5.61/7.14 | 4.79/6.49 | 4.11/6.00 |'
- en: '| AWQ[lin2023awq](#bib.bib29) | 3 ($g=128$,3.15) | 6.46/7.92 | 5.51/7.07 |
    4.63/6.37 | 3.99/5.94 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| AWQ[lin2023awq](#bib.bib29) | 3 ($g=128$,3.15) | 6.46/7.92 | 5.51/7.07 |
    4.63/6.37 | 3.99/5.94 |'
- en: '| OmniQuant[shao2023omniquant](#bib.bib36) | 3 ($g=128$,3.15) | 6.15/7.75 |
    5.44/7.05 | 4.56/6.37 | 3.94/5.93 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant[shao2023omniquant](#bib.bib36) | 3 ($g=128$,3.15) | 6.15/7.75 |
    5.44/7.05 | 4.56/6.37 | 3.94/5.93 |'
- en: '| SqueezeLLM[shao2023omniquant](#bib.bib36) | 3.24 | 6.13/7.56 | 5.45/6.92
    | 4.44/6.23 | 3.88/5.84 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| SqueezeLLM[shao2023omniquant](#bib.bib36) | 3.24 | 6.13/7.56 | 5.45/6.92
    | 4.44/6.23 | 3.88/5.84 |'
- en: '| CLAQ* | 3.12 | 5.97/7.40 | 5.27/6.83 | 4.35/6.18 | 3.75/5.78 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 3.12 | 5.97/7.40 | 5.27/6.83 | 4.35/6.18 | 3.75/5.78 |'
- en: '| CLAQ* | 3.23 | 5.95/7.37 | 5.24/6.81 | 4.33/6.16 | 3.74/5.76 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 3.23 | 5.95/7.37 | 5.24/6.81 | 4.33/6.16 | 3.74/5.76 |'
- en: '| GPTQ[frantar2022gptq](#bib.bib15) | 2 | 2148.66/691.25 | 8730.02/1467.84
    | 508.02/188.73 | 58.34/39.57 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ[frantar2022gptq](#bib.bib15) | 2 | 2148.66/691.25 | 8730.02/1467.84
    | 508.02/188.73 | 58.34/39.57 |'
- en: '| CLAQ | 2 | 27.64/24.37 | 21.05/19.69 | 15.37/13.43 | 97.16/48.95 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 2 | 27.64/24.37 | 21.05/19.69 | 15.37/13.43 | 97.16/48.95 |'
- en: '| AWQ[lin2023awq](#bib.bib29) | 2 ($g=64$,2.28) | 2.5e5/2.8e5 | 2.7e5/2.2e5
    | 2.3e5/2.3e5 | 7.4e4/7.4e4 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| AWQ[lin2023awq](#bib.bib29) | 2 ($g=64$,2.28) | 2.5e5/2.8e5 | 2.7e5/2.2e5
    | 2.3e5/2.3e5 | 7.4e4/7.4e4 |'
- en: '| OmniQuant[shao2023omniquant](#bib.bib36) | 2 ($g=64$,2.28) | 8.90/11.78 |
    7.34/9.75 | 6.59/8.65 | 5.65/7.60 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant[shao2023omniquant](#bib.bib36) | 2 ($g=64$,2.28) | 8.90/11.78 |
    7.34/9.75 | 6.59/8.65 | 5.65/7.60 |'
- en: '| decoupleQ[guo2024decoupleq](#bib.bib16) | 2 ($g=64$,2.28) | 8.18/- | 6.96/-
    | 5.81/- | 5.07/- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| decoupleQ[guo2024decoupleq](#bib.bib16) | 2 ($g=64$,2.28) | 8.18/- | 6.96/-
    | 5.81/- | 5.07/- |'
- en: '| CLAQ* | 2.12 | 7.57/8.87 | 6.41/7.92 | 5.40/7.05 | 4.70/6.46 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.12 | 7.57/8.87 | 6.41/7.92 | 5.40/7.05 | 4.70/6.46 |'
- en: '| CLAQ* | 2.24 | 6.93/8.35 | 5.99/7.52 | 5.03/6.74 | 4.41/6.22 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.24 | 6.93/8.35 | 5.99/7.52 | 5.03/6.74 | 4.41/6.22 |'
- en: 'Table 2: The evaluation result of Zero-Shot accuracy of our proposed CLAQ and
    other methods. CLAQ models with * are fusion models employing column-level adaptive
    precision and outlier reservation techniques. The results of LLaMA-2 series, Yi-34B
    model and experiments on other bit-width can be found in Appendix E.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：我们提出的CLAQ和其他方法的零-shot准确率评估结果。带*的CLAQ模型为融合模型，采用列级自适应精度和离群值保留技术。LLaMA-2系列、Yi-34B模型及其他位宽的实验结果见附录E。
- en: '| Model | Method | # Bits | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | Avg↑ |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | # 位 | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande | 平均↑
    |'
- en: '|  | FP16 | 16 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |'
- en: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 78.3 | 70.0 | 45.0 | 75.5 | 74.0
    | 69.0 | 68.63 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 78.3 | 70.0 | 45.0 | 75.5 | 74.0
    | 69.0 | 68.63 |'
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 78.67 | 73.23 | 42.83 | 74.16
    | 75.07 | 70.48 | 69.07 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 78.67 | 73.23 | 42.83 | 74.16
    | 75.07 | 70.48 | 69.07 |'
- en: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 77.09 | 51.89 | 40.87
    | 72.53 | 71.61 | 65.03 | 63.17 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 77.09 | 51.89 | 40.87
    | 72.53 | 71.61 | 65.03 | 63.17 |'
- en: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.63 | 78.45 | 67.13 | 38.23 | -
    | 56.01 | 67.48 | 61.46 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.63 | 78.45 | 67.13 | 38.23 | -
    | 56.01 | 67.48 | 61.46 |'
- en: '| LLaMA1-7B | CLAQ | 4 | 78.29 | 75.29 | 43.25 | 74.70 | 75.26 | 70.32 | 69.52
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B | CLAQ | 4 | 78.29 | 75.29 | 43.25 | 74.70 | 75.26 | 70.32 | 69.52
    |'
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 52.50 | 25.76 | 26.96 | 41.87
    | 25.77 | 52.49 | 37.56 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 52.50 | 25.76 | 26.96 | 41.87
    | 25.77 | 52.49 | 37.56 |'
- en: '|  | CLAQ* | 2.12 | 75.73 | 68.56 | 36.43 | 70.73 | 67.05 | 67.25 | 64.29 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 2.12 | 75.73 | 68.56 | 36.43 | 70.73 | 67.05 | 67.25 | 64.29 |'
- en: '|  | FP16 | 16 | 80.14 | 77.40 | 47.70 | 77.92 | 79.09 | 72.85 | 72.52 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 80.14 | 77.40 | 47.70 | 77.92 | 79.09 | 72.85 | 72.52 |'
- en: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 79.4 | 72.8 | 52.0 | 77.7 | 77.7
    | 71.5 | 71.85 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 79.4 | 72.8 | 52.0 | 77.7 | 77.7
    | 71.5 | 71.85 |'
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 79.60 | 76.60 | 47.87 | 76.45
    | 77.89 | 71.90 | 71.72 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 79.60 | 76.60 | 47.87 | 76.45
    | 77.89 | 71.90 | 71.72 |'
- en: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 78.40 | 57.28 | 42.91
    | 67.00 | 75.82 | 68.27 | 64.95 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 78.40 | 57.28 | 42.91
    | 67.00 | 75.82 | 68.27 | 64.95 |'
- en: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.63 | 78.94 | 74.37 | 43.17 | -
    | 59.02 | 69.77 | 65.05 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.63 | 78.94 | 74.37 | 43.17 | -
    | 59.02 | 69.77 | 65.05 |'
- en: '| LLaMA1-13B | CLAQ | 4 | 79.65 | 76.94 | 48.04 | 77.74 | 78.58 | 72.85 | 72.30
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-13B | CLAQ | 4 | 79.65 | 76.94 | 48.04 | 77.74 | 78.58 | 72.85 | 72.30
    |'
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 52.18 | 25.88 | 28.41 | 41.35
    | 25.67 | 49.64 | 37.19 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 52.18 | 25.88 | 28.41 | 41.35
    | 25.67 | 49.64 | 37.19 |'
- en: '|  | CLAQ* | 2.12 | 77.80 | 72.22 | 41.21 | 69.42 | 71.86 | 69.14 | 66.94 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 2.12 | 77.80 | 72.22 | 41.21 | 69.42 | 71.86 | 69.14 | 66.94 |'
- en: '|  | FP16 | 16 | 82.26 | 80.43 | 52.90 | 82.69 | 82.63 | 75.85 | 76.13 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 82.26 | 80.43 | 52.90 | 82.69 | 82.63 | 75.85 | 76.13 |'
- en: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 81.0 | 79.4 | 56.8 | 81.8 | 81.8
    | 75.1 | 75.98 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  | LLM-QAT[liu2023llmqat](#bib.bib30) | 4 | 81.0 | 79.4 | 56.8 | 81.8 | 81.8
    | 75.1 | 75.98 |'
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 81.12 | 76.81 | 50.00 | 79.36
    | 81.43 | 74.66 | 73.90 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 81.12 | 76.81 | 50.00 | 79.36
    | 81.43 | 74.66 | 73.90 |'
- en: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 79.81 | 58.79 | 45.22
    | 68.38 | 78.95 | 72.21 | 67.23 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 79.81 | 58.79 | 45.22
    | 68.38 | 78.95 | 72.21 | 67.23 |'
- en: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.69 | 81.01 | 76.05 | 47.18 | -
    | 62.50 | 72.93 | 67.93 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.69 | 81.01 | 76.05 | 47.18 | -
    | 62.50 | 72.93 | 67.93 |'
- en: '| LLaMA1-30B | CLAQ | 4 | 81.18 | 80.39 | 54.10 | 82.17 | 81.91 | 75.85 | 75.93
    |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-30B | CLAQ | 4 | 81.18 | 80.39 | 54.10 | 82.17 | 81.91 | 75.85 | 75.93
    |'
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 53.16 | 27.74 | 26.79 | 43.46
    | 27.10 | 48.46 | 37.79 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 53.16 | 27.74 | 26.79 | 43.46
    | 27.10 | 48.46 | 37.79 |'
- en: '|  | CLAQ* | 2.12 | 80.03 | 76.73 | 47.10 | 79.51 | 76.55 | 73.64 | 72.26 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 2.12 | 80.03 | 76.73 | 47.10 | 79.51 | 76.55 | 73.64 | 72.26 |'
- en: '|  | FP16 | 16 | 82.26 | 81.36 | 55.55 | 84.83 | 84.12 | 77.27 | 77.57 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 82.26 | 81.36 | 55.55 | 84.83 | 84.12 | 77.27 | 77.57 |'
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 82.48 | 80.89 | 54.01 | 83.55
    | 83.47 | 76.68 | 76.85 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 4 | 82.48 | 80.89 | 54.01 | 83.55
    | 83.47 | 76.68 | 76.85 |'
- en: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 81.01 | 58.12 | 46.33
    | 80.64 | 79.91 | 75.69 | 70.28 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '|  | OmniQuant[shao2023omniquant](#bib.bib36) | W6A6 | 81.01 | 58.12 | 46.33
    | 80.64 | 79.91 | 75.69 | 70.28 |'
- en: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.71 | 81.56 | 75.25 | 46.93 | -
    | 63.76 | 76.95 | 68.89 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | SpQR[dettmers2023spqr](#bib.bib10) | 4.71 | 81.56 | 75.25 | 46.93 | -
    | 63.76 | 76.95 | 68.89 |'
- en: '| LLaMA1-65B | CLAQ | 4 | 82.43 | 81.52 | 54.69 | 83.85 | 83.56 | 76.40 | 77.08
    |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-65B | CLAQ | 4 | 82.43 | 81.52 | 54.69 | 83.85 | 83.56 | 76.40 | 77.08
    |'
- en: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 53.59 | 28.62 | 27.22 | 54.80
    | 30.09 | 51.62 | 40.99 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ[frantar2022gptq](#bib.bib15) | 2 | 53.59 | 28.62 | 27.22 | 54.80
    | 30.09 | 51.62 | 40.99 |'
- en: '|  | CLAQ* | 2.12 | 80.20 | 78.96 | 50.17 | 79.14 | 79.39 | 75.30 | 73.86 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 2.12 | 80.20 | 78.96 | 50.17 | 79.14 | 79.39 | 75.30 | 73.86 |'
- en: Implementation Details
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 实现细节
- en: To implement CLAQ, we leveraged the accelerated K-Means clustering algorithm
    provided by the scikit-learn-intelex library ¹¹1https://github.com/intel/scikit-learn-intelex
    (Apache-2.0 license). Our experiments were built upon the GPTQ framework ²²2https://github.com/IST-DASLab/gptq
    (Apache-2.0 license). For benchmarking purposes, the LM-Evaluation-Harness toolkit
    ³³3https://github.com/EleutherAI/lm-evaluation-harness (MIT License) served as
    the assessment platform. All experiments were conducted on a server with 8 NVIDIA
    A100 GPUs, each with 80GB capacity. Notably, a single NVIDIA A100 GPU is sufficient
    for the majority of our experiments except evaluating the LLaMA-65B model on zero-shot
    tasks, where two GPUs were used to accommodate its memory demands. Our contribution
    was mainly confined to the quantization of model weights, with activations left
    unquantized. Unless otherwise specified within the experimental results table,
    a 16-bit precision was maintained for activations. Comprehensive details regarding
    the hyper-parameters employed in our experiments can be found in Appendix F.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现 CLAQ，我们利用了 scikit-learn-intelex 库提供的加速 K-Means 聚类算法 ¹¹1https://github.com/intel/scikit-learn-intelex（Apache-2.0
    许可证）。我们的实验基于 GPTQ 框架 ²²2https://github.com/IST-DASLab/gptq（Apache-2.0 许可证）。为了进行基准测试，LM-Evaluation-Harness
    工具包 ³³3https://github.com/EleutherAI/lm-evaluation-harness（MIT 许可证）作为评估平台。所有实验都在配备
    8 张 NVIDIA A100 GPU（每张 80GB 容量）的服务器上进行。值得注意的是，除了评估 LLaMA-65B 模型在零-shot 任务上的表现外，大多数实验仅使用单张
    NVIDIA A100 GPU，而在零-shot 任务中则使用了两张 GPU 以满足其内存需求。我们的贡献主要集中在模型权重的量化上，激活保持未量化。除非实验结果表格中另有说明，激活的精度保持为
    16-bit。有关我们实验中使用的超参数的详细信息，请参见附录 F。
- en: 4.2 Main Results
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: 'The main results of our experiments are presented in Table [1](#S4.T1 "Table
    1 ‣ Datasets ‣ 4.1 Experimental Settings ‣ 4 Experiments and Results ‣ CLAQ: Pushing
    the Limits of Low-Bit Post-Training Quantization for LLMs") and Table LABEL:table:2.
    Table [1](#S4.T1 "Table 1 ‣ Datasets ‣ 4.1 Experimental Settings ‣ 4 Experiments
    and Results ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for
    LLMs") shows the evaluations on perplexity, while the performance of zero-shot
    tasks is given in Table LABEL:table:2. It can be clearly observed that CLAQ consistently
    outperforms existing methods in various scenarios. Notably, in case of 3-bit and
    4-bit single-precision quantization with a comparable codebook size configuration,
    our method generally outperforms the baselines examined.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '我们实验的主要结果展示在表格 [1](#S4.T1 "Table 1 ‣ Datasets ‣ 4.1 Experimental Settings ‣
    4 Experiments and Results ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs") 和表格 LABEL:table:2 中。表格 [1](#S4.T1 "Table 1 ‣ Datasets
    ‣ 4.1 Experimental Settings ‣ 4 Experiments and Results ‣ CLAQ: Pushing the Limits
    of Low-Bit Post-Training Quantization for LLMs") 显示了困惑度的评估，而零-shot 任务的性能见表格 LABEL:table:2。可以清楚地观察到，CLAQ
    在各种场景下始终优于现有方法。特别是在 3-bit 和 4-bit 单精度量化中，在配置相当的代码本大小下，我们的方法通常优于检查的基线。'
- en: For lower bit-width, we employ the fusion approach of AP and OR. Results show
    that with an additional 0.1-bit memory footprint, model accuracy is significantly
    boosted. Especially for compression around 2-bit (2.1 bits in our case), our method
    exhibits a consistent and substantial lead over all existing methods, achieving
    the state-of-the-art (SOTA) performance (significant difference at 0.01 confidence
    level).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于较低的位宽，我们采用了 AP 和 OR 的融合方法。结果显示，增加 0.1-bit 的内存占用可以显著提升模型准确性。特别是在压缩约 2-bit（在我们的案例中为
    2.1 bits）时，我们的方法在所有现有方法中表现出一致而显著的领先，达到了最先进的（SOTA）性能（在 0.01 置信水平上具有显著差异）。
- en: Also, the zero-shot evaluation results demonstrate the superiority of our method.
    CLAQ outperforms other baselines in most cases. Specifically, in the case of 4-bit
    quantization, our solution closely matches the performance of full-precision models.
    For low-bit scenarios, our method significantly improve the accuracy of the quantized
    model in zero-shot tasks, and pushing the limits of large language model quantization
    under low-bit conditions. More zero-shot task results on LLaMA-2 series and Yi-34B
    models can be found in Appendix E.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，零-shot 评估结果展示了我们方法的优越性。在大多数情况下，CLAQ 优于其他基线。具体来说，在 4-bit 量化的情况下，我们的解决方案的性能与全精度模型非常接近。在低位场景下，我们的方法显著提高了量化模型在零-shot
    任务中的准确性，并推动了大语言模型在低位条件下量化的极限。更多关于 LLaMA-2 系列和 Yi-34B 模型的零-shot 任务结果请参见附录 E。
- en: 4.3 Ablation Study
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: 4.3.1 Adaptive Precision Quantization
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 自适应精度量化
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.3.2 Outlier Reservation ‣ 4.3 Ablation Study
    ‣ 4 Experiments and Results ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs") presents a comparison of different Adaptive Precision
    (AP) quantization methods across various precision levels. Inspired by [frantar2023sparsegpt](#bib.bib14)
    , we developed a magnitude-to-parameter method to guide the mixed-precision allocation.
    Experimental results demonstrate that our proposed column-level adaptive precision
    quantization strategy significantly outperforms other mixed-precision methods
    under comparable size. The improvements can be attributed to the superiority of
    our Outlier Order metric. According to the outlier distribution analysis (see
    Appendix A), a minority of columns exhibit higher sensitivity to quantization.
    Consequently, allocating additional precision on preserving these critical columns
    is an effective way to improve the performance of quantized models.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [3](#S4.T3 "表格 3 ‣ 4.3.2 异常值保留 ‣ 4.3 消融研究 ‣ 4 实验和结果 ‣ CLAQ：推动低位后训练量化的极限")
    展示了不同适应性精度 (AP) 量化方法在各种精度级别下的比较。受 [frantar2023sparsegpt](#bib.bib14) 启发，我们开发了一种幅度-参数方法来指导混合精度分配。实验结果表明，我们提出的列级适应性精度量化策略在相似规模下显著优于其他混合精度方法。改进的原因可归因于我们异常值顺序度量的优越性。根据异常值分布分析（见附录
    A），少数几列对量化表现出更高的敏感性。因此，分配额外的精度以保留这些关键列是提高量化模型性能的有效方法。
- en: 4.3.2 Outlier Reservation
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 异常值保留
- en: 'Table [4](#S4.T4 "Table 4 ‣ 4.3.2 Outlier Reservation ‣ 4.3 Ablation Study
    ‣ 4 Experiments and Results ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs") lists the results of the fixed and Outlier Reservation
    (OR) strategy, highlighting its superiority over static outlier preservation approaches.
    Remarkably, with an equal amount of increase in model sizes, the performance gain
    from retaining more outliers exceeds that from adaptive precision strategy. This
    result suggests the importance of outlier retention, indicating that directly
    preserving outliers at full precision is superior to quantizing them with a higher
    precision.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [4](#S4.T4 "表格 4 ‣ 4.3.2 异常值保留 ‣ 4.3 消融研究 ‣ 4 实验和结果 ‣ CLAQ：推动低位后训练量化的极限")
    列出了固定和异常值保留 (OR) 策略的结果，突出了其相对于静态异常值保留方法的优越性。值得注意的是，在模型规模相同的情况下，保留更多异常值所带来的性能提升超过了适应性精度策略的提升。这一结果表明，异常值保留的重要性，直接以全精度保留异常值优于以更高精度对其进行量化。
- en: 'Table 3: Ablation study results for our proposed column-level adaptive precision
    quantization. † denotes a column-wise mixed-precision quantization with the metric
    of activation-to-weight ratio proposed in [frantar2023sparsegpt](#bib.bib14) .'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3：我们提出的列级适应性精度量化的消融研究结果。† 表示一种基于[frantar2023sparsegpt](#bib.bib14) 提出的激活-权重比度量的列级混合精度量化。
- en: '| Model | # Bits | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag
    | Winogrande | Avg↑ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | # 位数 | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag |
    Winogrande | 平均↑ |'
- en: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
- en: '| CLAQ | 3 | 6.47 | 7.87 | 78.23 | 72.05 | 41.12 | 72.93 | 72.05 | 67.71 |
    67.35 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 3 | 6.47 | 7.87 | 78.23 | 72.05 | 41.12 | 72.93 | 72.05 | 67.71 |
    67.35 |'
- en: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
- en: '| +MP† | 2.5 | 8.63 | 9.98 | 75.35 | 65.15 | 36.51 | 69.08 | 62.96 | 64.71
    | 62.29 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| +MP† | 2.5 | 8.63 | 9.98 | 75.35 | 65.15 | 36.51 | 69.08 | 62.96 | 64.71
    | 62.29 |'
- en: '| +AP(ours) | 2.5 | 8.43 | 9.78 | 75.63 | 68.10 | 35.24 | 66.39 | 63.82 | 64.40
    | 62.26 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| +AP(ours) | 2.5 | 8.43 | 9.78 | 75.63 | 68.10 | 35.24 | 66.39 | 63.82 | 64.40
    | 62.26 |'
- en: '| +MP† | 2.2 | 11.60 | 12.70 | 71.00 | 57.91 | 29.43 | 65.22 | 51.07 | 59.03
    | 55.61 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| +MP† | 2.2 | 11.60 | 12.70 | 71.00 | 57.91 | 29.43 | 65.22 | 51.07 | 59.03
    | 55.61 |'
- en: '| +AP(ours) | 2.2 | 10.14 | 11.46 | 73.39 | 64.10 | 31.91 | 63.52 | 60.08 |
    62.83 | 59.31 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| +AP(ours) | 2.2 | 10.14 | 11.46 | 73.39 | 64.10 | 31.91 | 63.52 | 60.08 |
    62.83 | 59.31 |'
- en: '| +MP† | 2.1 | 16.42 | 16.57 | 66.86 | 44.02 | 26.53 | 62.56 | 44.03 | 53.59
    | 49.60 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| +MP† | 2.1 | 16.42 | 16.57 | 66.86 | 44.02 | 26.53 | 62.56 | 44.03 | 53.59
    | 49.60 |'
- en: '| +AP(ours) | 2.1 | 11.06 | 12.34 | 72.42 | 63.17 | 31.06 | 64.92 | 58.33 |
    61.25 | 58.53 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| +AP(ours) | 2.1 | 11.06 | 12.34 | 72.42 | 63.17 | 31.06 | 64.92 | 58.33 |
    61.25 | 58.53 |'
- en: 'Table 4: Ablation study results for our proposed column-level adaptive outlier
    reservation. Outlier fix refers to keeping a fixed proportion of outliers in each
    column.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 4：我们提出的列级适应性异常值保留的消融研究结果。异常值修正指的是在每列中保留固定比例的异常值。
- en: '| Model | # Bits | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag
    | Winogrande | Avg↑ |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | 平均↑ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
- en: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
- en: '| +Outlier fix | 2.28 | 7.49 | 8.87 | 74.70 | 68.56 | 37.88 | 71.52 | 67.03
    | 66.69 | 64.40 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| +离群点修正 | 2.28 | 7.49 | 8.87 | 74.70 | 68.56 | 37.88 | 71.52 | 67.03 | 66.69
    | 64.40 |'
- en: '| +OR(ours) | 2.28 | 6.88 | 8.35 | 77.75 | 69.36 | 39.76 | 74.40 | 70.62 |
    67.09 | 66.50 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| +OR（我们的） | 2.28 | 6.88 | 8.35 | 77.75 | 69.36 | 39.76 | 74.40 | 70.62 | 67.09
    | 66.50 |'
- en: '| +Outlier fix | 2.14 | 8.31 | 9.61 | 75.19 | 66.41 | 35.32 | 71.40 | 63.12
    | 66.14 | 62.93 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| +离群点修正 | 2.14 | 8.31 | 9.61 | 75.19 | 66.41 | 35.32 | 71.40 | 63.12 | 66.14
    | 62.93 |'
- en: '| +OR(ours) | 2.14 | 7.22 | 8.64 | 76.28 | 69.02 | 37.88 | 73.70 | 68.14 |
    68.11 | 65.52 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| +OR（我们的） | 2.14 | 7.22 | 8.64 | 76.28 | 69.02 | 37.88 | 73.70 | 68.14 | 68.11
    | 65.52 |'
- en: 5 Limitations and Future Works
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 局限性与未来工作
- en: 'The main limitations of our work lie in the simplification of outlier retention
    and adaptive precision search strategies. We adopt two-level precision in AP for
    the convenience of CUDA kernel development. However, when the assignable precision
    bit-width exceeds the equivalent of 0.5 bit (i.e., >2.5bit, >3.5bit), the adaptive
    precision strategy outlined in our method may not represent the optimal approach.
    The potential for generating superior adaptive precision configurations is discussed
    in Appendix G, where we propose a heuristic adaptive precision search algorithm.
    Our future work is twofold: (1) We will continue to investigate the accomplishment
    of superior adaptive precision quantization configurations through efficient search
    algorithms. (2) We are developing customized CUDA kernels to support efficient
    computation of the CLAQ approach on GPUs. Compared with the AP quantization, dynamically
    preserving outliers per column is more challenging in deployment. We are actively
    refining the underlying algorithms, and the code will be released soon in the
    future.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要局限性在于离群点保留和自适应精度搜索策略的简化。为了方便 CUDA 内核的开发，我们在 AP 中采用了两级精度。然而，当可分配的精度位宽超过
    0.5 位（即 >2.5 位，>3.5 位）时，我们方法中描述的自适应精度策略可能不是最佳方案。在附录 G 中讨论了生成优越自适应精度配置的潜力，我们提出了一种启发式自适应精度搜索算法。我们未来的工作有两个方面：（1）我们将继续研究通过高效搜索算法实现优越的自适应精度量化配置。（2）我们正在开发定制的
    CUDA 内核，以支持在 GPU 上高效计算 CLAQ 方法。与 AP 量化相比，动态保留每列的离群点在部署中更具挑战性。我们正在积极改进基础算法，并将在未来尽快发布代码。
- en: 6 Conclusion
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper, we have introduced a novel Column-Level Adaptive weight Quantization
    (CLAQ) framework, which integrates column-level adaptive precision quantization
    and outlier reservation techniques. Guided by the column-wise outlier proportion
    metric, the CLAQ method achieves efficient compression of LLMs while maximizing
    the preservation of model accuracy. Extensive evaluations on public LLM models
    confirm the superiority of CLAQ in enhancing performance across varied bit-widths,
    notably in ultra-low bit cases. The results lend help to making LLMs more efficient
    and accessible in resource-constrained applications.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了一种新颖的列级自适应权重量化（CLAQ）框架，该框架集成了列级自适应精度量化和离群点保留技术。在列级离群点比例度量的指导下，CLAQ
    方法实现了 LLM 的高效压缩，同时最大限度地保留了模型的准确性。对公共 LLM 模型的广泛评估确认了 CLAQ 在不同位宽下提升性能的优越性，尤其是在超低位宽的情况下。这些结果有助于提高
    LLM 在资源受限应用中的效率和可及性。
- en: References
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang,
    Jie Ren, Torsten Hoefler, and Dan Alistarh. Towards end-to-end 4-bit inference
    on generative large language models. arXiv preprint arXiv:2310.09259, 2023.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Saleh Ashkboos、Ilia Markov、Elias Frantar、Tingxuan Zhong、Xincheng Wang、Jie
    Ren、Torsten Hoefler 和 Dan Alistarh. 面向生成大型语言模型的端到端 4 位推理。arXiv 预印本 arXiv:2310.09259，2023
    年。
- en: '(2) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning
    about physical commonsense in natural language. In Proceedings of the AAAI conference
    on artificial intelligence, volume 34, pages 7432–7439, 2020.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(2) Yonatan Bisk、Rowan Zellers、Jianfeng Gao、Yejin Choi 等. Piqa: 关于自然语言中的物理常识推理。发表于
    AAAI 人工智能会议论文集，第 34 卷，第 7432–7439 页，2020 年。'
- en: (3) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
    Dhariwal, Arvind Neelakantan, et al. Language models are few-shot learners. In
    Advances in Neural Information Processing Systems, volume 33, pages 1877–1901,
    2020.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Tom Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared Kaplan、Prafulla
    Dhariwal、Arvind Neelakantan 等人. 语言模型是少量学习者。神经信息处理系统进展，卷33，第1877–1901页，2020年。
- en: '(4) Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
    Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al.
    Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv
    preprint arXiv:2303.12712, 2023.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Sébastien Bubeck、Varun Chandrasekaran、Ronen Eldan、Johannes Gehrke、Eric Horvitz、Ece
    Kamar、Peter Lee、Yin Tat Lee、Yuanzhi Li、Scott Lundberg 等人. 人工通用智能的火花：对gpt-4的早期实验。arXiv预印本
    arXiv:2303.12712，2023年。
- en: '(5) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M De Sa. Quip:
    2-bit quantization of large language models with guarantees. Advances in Neural
    Information Processing Systems, 36, 2024.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Jerry Chee、Yaohui Cai、Volodymyr Kuleshov 和 Christopher M De Sa. Quip：带有保障的大型语言模型2位量化。神经信息处理系统进展，36，2024年。
- en: '(6) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
    Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of
    natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Christopher Clark、Kenton Lee、Ming-Wei Chang、Tom Kwiatkowski、Michael Collins
    和 Kristina Toutanova. Boolq：探索自然是/否问题的惊人困难。arXiv预印本 arXiv:1905.10044，2019年。
- en: (7) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Peter Clark、Isaac Cowhey、Oren Etzioni、Tushar Khot、Ashish Sabharwal、Carissa
    Schoenick 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试arc，ai2推理挑战。arXiv预印本 arXiv:1803.05457，2018年。
- en: '(8) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3\.
    int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural
    Information Processing Systems, 35:30318–30332, 2022.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(8) Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer. Gpt3\. int8
    (): 规模化变换器的8位矩阵乘法。神经信息处理系统进展，35:30318–30332，2022年。'
- en: '(9) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. Advances in Neural Information Processing
    Systems, 36, 2024.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer. Qlora：高效微调量化llms。神经信息处理系统进展，36，2024年。
- en: '(10) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev,
    Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    arXiv preprint arXiv:2306.03078, 2023.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) Tim Dettmers、Ruslan Svirschevski、Vage Egiazarian、Denis Kuznedelev、Elias
    Frantar、Saleh Ashkboos、Alexander Borzunov、Torsten Hoefler 和 Dan Alistarh. Spqr：一种稀疏量化表示，用于近无损llm权重压缩。arXiv预印本
    arXiv:2306.03078，2023年。
- en: '(11) Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit
    inference scaling laws. In International Conference on Machine Learning, 2022.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Tim Dettmers 和 Luke Zettlemoyer. 4位精度的理由：k位推理扩展规律。国际机器学习会议，2022年。
- en: '(12) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W Mahoney,
    and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization of neural
    networks. Advances in neural information processing systems, 33:18518–18529, 2020.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Zhen Dong、Zhewei Yao、Daiyaan Arfeen、Amir Gholami、Michael W Mahoney 和 Kurt
    Keutzer. Hawq-v2：对神经网络的Hessian感知的迹权重量化。神经信息处理系统进展，33:18518–18529，2020年。
- en: '(13) Elias Frantar and Dan Alistarh. Optimal brain compression: A framework
    for accurate post-training quantization and pruning. Advances in Neural Information
    Processing Systems, 35:4475–4488, 2022.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) Elias Frantar 和 Dan Alistarh. 最优脑压缩：一种准确的训练后量化和剪枝框架。神经信息处理系统进展，35:4475–4488，2022年。
- en: '(14) Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can
    be accurately pruned in one-shot. In International Conference on Machine Learning,
    pages 10323–10337\. PMLR, 2023.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (14) Elias Frantar 和 Dan Alistarh. Sparsegpt：大规模语言模型可以在一次性操作中准确剪枝。国际机器学习会议，页10323–10337。PMLR，2023年。
- en: '(15) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh. Gptq：生成预训练变换器的准确训练后量化。arXiv预印本
    arXiv:2210.17323，2022年。
- en: '(16) Yi Guo, Fanliu Kong, Xiaoyang Li, Hui Li, Wei Chen, Xiaogang Tian, Jinping
    Cai, Yang Zhang, and Shouda Liu. decoupleq: Towards 2-bit post-training uniform
    quantization via decoupling parameters into integer and floating points. arXiv
    preprint arXiv:2404.12759, 2024.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(16) Yi Guo, Fanliu Kong, Xiaoyang Li, Hui Li, Wei Chen, Xiaogang Tian, Jinping
    Cai, Yang Zhang, 和 Shouda Liu。decoupleq: 通过将参数解耦为整数和浮点数实现2位后训练均匀量化。arXiv 预印本 arXiv:2404.12759，2024年。'
- en: (17) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob
    Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling
    laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701,
    2020.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob
    Jackson, Heewoo Jun, Tom B Brown, Prafulla Dhariwal, Scott Gray 等。自回归生成建模的扩展规律。arXiv
    预印本 arXiv:2010.14701，2020年。
- en: (18) Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon,
    and Dongsoo Lee. Rethinking channel dimensions to isolate outliers for low-bit
    weight quantization of large language models. arXiv preprint arXiv:2309.15531,
    2023.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon,
    和 Dongsoo Lee。重新思考通道维度以隔离异常值，用于大型语言模型的低位权重量化。arXiv 预印本 arXiv:2309.15531，2023年。
- en: '(19) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language
    models. arXiv preprint arXiv:2106.09685, 2021.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(19) Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, 和 Weizhu Chen。Lora: 大型语言模型的低秩适应。arXiv 预印本 arXiv:2106.09685，2021年。'
- en: (20) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.
    Accurate post training quantization with small calibration sets. In International
    Conference on Machine Learning, pages 4466–4475\. PMLR, 2021.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, 和 Daniel Soudry。使用小型校准集的准确后训练量化。在国际机器学习会议上，页4466–4475。PMLR，2021年。
- en: (21) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088,
    2024.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
    Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,
    Florian Bressand 等。Mixtral of experts。arXiv 预印本 arXiv:2401.04088，2024年。
- en: '(22) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629, 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(22) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, 和 Kurt Keutzer。Squeezellm: Dense-and-sparse quantization。arXiv
    预印本 arXiv:2306.07629，2023年。'
- en: '(23) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
    2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(23) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé 等。Bloom: 一个176b参数的开放访问多语言模型。2023年。'
- en: '(24) Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok Park.
    Owq: Outlier-aware weight quantization for efficient fine-tuning and inference
    of large language models. In Proceedings of the AAAI Conference on Artificial
    Intelligence, volume 38, pages 13355–13364, 2024.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, 和 Eunhyeok Park。Owq:
    面向高效微调和推理的大型语言模型的异常值感知权重量化。在AAAI人工智能会议论文集中，第38卷，页13355–13364，2024年。'
- en: '(25) Jinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu Wang,
    and Guohao Dai. Enabling fast 2-bit llm on gpus: Memory alignment, sparse outlier,
    and asynchronous dequantization. arXiv preprint arXiv:2311.16442, 2023.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) Jinhao Li, Shiyao Li, Jiaming Xu, Shan Huang, Yaoxiu Lian, Jun Liu, Yu
    Wang, 和 Guohao Dai。实现快速2位llm在gpu上的应用：内存对齐、稀疏异常值和异步反量化。arXiv 预印本 arXiv:2311.16442，2023年。
- en: '(26) Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li,
    Kai Zhong, Guohao Dai, Huazhong Yang, and Yu Wang. Llm-mq: Mixed-precision quantization
    for efficient llm deployment.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(26) Shiyao Li, Xuefei Ning, Ke Hong, Tengxuan Liu, Luning Wang, Xiuhong Li,
    Kai Zhong, Guohao Dai, Huazhong Yang, 和 Yu Wang。llm-mq: 高效llm部署的混合精度量化。'
- en: '(27) Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu
    Chen, and Tuo Zhao. Loftq: Lora-fine-tuning-aware quantization for large language
    models. arXiv preprint arXiv:2310.08659, 2023.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(27) Yixiao Li, Yifan Yu, Chen Liang, Pengcheng He, Nikos Karampatziakis, Weizhu
    Chen, 和 Tuo Zhao。Loftq: 针对大型语言模型的Lora微调感知量化。arXiv 预印本 arXiv:2310.08659，2023年。'
- en: '(28) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. arXiv preprint arXiv:2102.05426, 2021.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) Yuhang Li、Ruihao Gong、Xu Tan、Yang Yang、Peng Hu、Qi Zhang、Fengwei Yu、Wei
    Wang 和 Shi Gu。Brecq：通过块重建推动后训练量化的极限。arXiv 预印本 arXiv:2102.05426，2021。
- en: '(29) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (29) Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Xingyu Dang 和 Song Han。AWQ：面向
    LLM 压缩和加速的激活感知权重量化。arXiv 预印本 arXiv:2306.00978，2023。
- en: '(30) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free
    quantization aware training for large language models. arXiv preprint arXiv:2305.17888,
    2023.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) Zechun Liu、Barlas Oguz、Changsheng Zhao、Ernie Chang、Pierre Stock、Yashar
    Mehdad、Yangyang Shi、Raghuraman Krishnamoorthi 和 Vikas Chandra。LLM-QAT：针对大型语言模型的数据无关量化感知训练。arXiv
    预印本 arXiv:2305.17888，2023。
- en: (31) Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (31) Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher。指针哨兵混合模型。arXiv
    预印本 arXiv:1609.07843，2016。
- en: (32) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, and Tijmen
    Blankevoort. Up or down? adaptive rounding for post-training quantization. In
    International Conference on Machine Learning, pages 7197–7206\. PMLR, 2020.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) Markus Nagel、Rana Ali Amjad、Mart Van Baalen、Christos Louizos 和 Tijmen Blankevoort。向上还是向下？自适应四舍五入用于后训练量化。在国际机器学习大会上，页面
    7197–7206。PMLR，2020。
- en: '(33) Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon Kim, Beomseok
    Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee. Lut-gemm: Quantized
    matrix multiplication based on luts for efficient inference in large-scale generative
    language models. arXiv preprint arXiv:2206.09557, 2022.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (33) Gunho Park、Baeseong Park、Minsub Kim、Sungjae Lee、Jeonghoon Kim、Beomseok
    Kwon、Se Jung Kwon、Byeongwook Kim、Youngjoo Lee 和 Dongsoo Lee。Lut-gemm：基于 LUT 的量化矩阵乘法，用于大规模生成语言模型中的高效推理。arXiv
    预印本 arXiv:2206.09557，2022。
- en: (34) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of machine learning
    research, 21(140):1–67, 2020.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (34) Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J Liu。利用统一的文本到文本变换器探索迁移学习的极限。机器学习研究期刊，21(140):1–67，2020。
- en: '(35) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi.
    Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106, 2021.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (35) Keisuke Sakaguchi、Ronan Le Bras、Chandra Bhagavatula 和 Yejin Choi。Winogrande：大规模对抗
    Winograd 方案挑战。ACM 通讯，64(9):99–106，2021。
- en: '(36) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao, Zhiqian
    Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models. arXiv preprint arXiv:2308.13137,
    2023.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (36) Wenqi Shao、Mengzhao Chen、Zhaoyang Zhang、Peng Xu、Lirui Zhao、Zhiqian Li、Kaipeng
    Zhang、Peng Gao、Yu Qiao 和 Ping Luo。Omniquant：面向大型语言模型的全方向标定量化。arXiv 预印本 arXiv:2308.13137，2023。
- en: '(37) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (37) Hugo Touvron、Thibaut Lavril、Gautier Izacard、Xavier Martinet、Marie-Anne
    Lachaux、Timothée Lacroix、Baptiste Rozière、Naman Goyal、Eric Hambro、Faisal Azhar
    等。Llama：开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971，2023。
- en: (38) Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate
    post-training network quantization via bit-split and stitching. In International
    Conference on Machine Learning, pages 9847–9856\. PMLR, 2020.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (38) Peisong Wang、Qiang Chen、Xiangyu He 和 Jian Cheng。通过比特拆分和拼接实现准确的后训练网络量化。在国际机器学习大会上，页面
    9847–9856。PMLR，2020。
- en: '(39) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang,
    Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit
    of low-bit transformer language models. Advances in Neural Information Processing
    Systems, 35:17402–17414, 2022.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (39) Xiuying Wei、Yunchen Zhang、Xiangguo Zhang、Ruihao Gong、Shanghang Zhang、Qi
    Zhang、Fengwei Yu 和 Xianglong Liu。离群点抑制：推动低位变换器语言模型的极限。神经信息处理系统进展，35:17402–17414，2022。
- en: '(40) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, pages 38087–38099\.
    PMLR, 2023.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(40) Guangxuan Xiao、Ji Lin、Mickael Seznec、Hao Wu、Julien Demouth 和 Song Han。Smoothquant:
    Accurate and efficient post-training quantization for large language models。在国际机器学习会议上，页面
    38087–38099\. PMLR，2023 年。'
- en: '(41) Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng Zhang,
    Zhensu Chen, Xiaopeng Zhang, and Qi Tian. Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717, 2023.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(41) Yuhui Xu、Lingxi Xie、Xiaotao Gu、Xin Chen、Heng Chang、Hengheng Zhang、Zhensu
    Chen、Xiaopeng Zhang 和 Qi Tian。Qa-lora: Quantization-aware low-rank adaptation
    of large language models。arXiv 预印本 arXiv:2309.14717，2023 年。'
- en: '(42) Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu,
    Weidong Liu, and Wanxiang Che. Onebit: Towards extremely low-bit large language
    models. arXiv preprint arXiv:2402.11295, 2024.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(42) Yuzhuang Xu、Xu Han、Zonghan Yang、Shuo Wang、Qingfu Zhu、Zhiyuan Liu、Weidong
    Liu 和 Wanxiang Che。Onebit: Towards extremely low-bit large language models。arXiv
    预印本 arXiv:2402.11295，2024 年。'
- en: '(43) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35:27168–27183, 2022.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(43) Zhewei Yao、Reza Yazdani Aminabadi、Minjia Zhang、Xiaoxia Wu、Conglong Li
    和 Yuxiong He。Zeroquant: Efficient and affordable post-training quantization for
    large-scale transformers。神经信息处理系统进展，35:27168–27183，2022 年。'
- en: '(44) Lu Yin, You Wu, Zhenyu Zhang, Cheng-Yu Hsieh, Yaqing Wang, Yiling Jia,
    Mykola Pechenizkiy, Yi Liang, Zhangyang Wang, and Shiwei Liu. Outlier weighed
    layerwise sparsity (owl): A missing secret sauce for pruning llms to high sparsity.
    arXiv preprint arXiv:2310.05175, 2023.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(44) Lu Yin、You Wu、Zhenyu Zhang、Cheng-Yu Hsieh、Yaqing Wang、Yiling Jia、Mykola
    Pechenizkiy、Yi Liang、Zhangyang Wang 和 Shiwei Liu。Outlier weighed layerwise sparsity
    (owl): A missing secret sauce for pruning llms to high sparsity。arXiv 预印本 arXiv:2310.05175，2023
    年。'
- en: '(45) Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang,
    Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation
    models by 01\. ai. arXiv preprint arXiv:2403.04652, 2024.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(45) Alex Young、Bei Chen、Chao Li、Chengen Huang、Ge Zhang、Guanwei Zhang、Heng
    Li、Jiangcheng Zhu、Jianqun Chen、Jing Chang 等人。Yi: Open foundation models by 01\.
    ai。arXiv 预印本 arXiv:2403.04652，2024 年。'
- en: '(46) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
    Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830,
    2019.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(46) Rowan Zellers、Ari Holtzman、Yonatan Bisk、Ali Farhadi 和 Yejin Choi。Hellaswag:
    Can a machine really finish your sentence? arXiv 预印本 arXiv:1905.07830，2019 年。'
- en: '(47) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(47) Susan Zhang、Stephen Roller、Naman Goyal、Mikel Artetxe、Moya Chen、Shuohui
    Chen、Christopher Dewan、Mona Diab、Xian Li、Xi Victoria Lin 等人。Opt: Open pre-trained
    transformer language models。arXiv 预印本 arXiv:2205.01068，2022 年。'
- en: Appendix A Analysis on outlier ratio in LLM
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A LLM 中异常值比例的分析
- en: 'In Section 3.2, we underscored the significance of outliers in the context
    of Large Language Model (LLM) compression. Here, we present concrete examples
    to demonstrate the efficacy of our approach. Initially, we observe notable variations
    in outlier prevalence among different columns within each parameter matrix. Figure
    [3](#A1.F3 "Figure 3 ‣ Appendix A Analysis on outlier ratio in LLM ‣ CLAQ: Pushing
    the Limits of Low-Bit Post-Training Quantization for LLMs") illustrates the statistics
    of outliers detected when applying an outlier standard deviation $S=7$ to the
    layer layers.0.self_attn.o_proj.weight. The findings highlight that in the LLaMA
    architecture, outliers are confined to a minority of columns, with a striking
    imbalance in their distribution across columns. Consequently, rationing the limited
    precision to those columns exhibiting a higher concentration of outliers is advocated
    as a strategy to maximize performance gains.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '在第 3.2 节中，我们强调了异常值在大型语言模型（LLM）压缩中的重要性。在这里，我们展示了具体的例子以证明我们方法的有效性。最初，我们观察到在每个参数矩阵的不同列中，异常值的普遍性有显著差异。图
    [3](#A1.F3 "Figure 3 ‣ 附录 A LLM 中异常值比例的分析 ‣ CLAQ: 推进 LLM 低比特后训练量化的极限") 说明了在将异常值标准差
    $S=7$ 应用于层 layers.0.self_attn.o_proj.weight 时检测到的异常值统计数据。结果突出显示，在 LLaMA 架构中，异常值仅限于少数几列，并且在各列之间的分布存在显著不平衡。因此，建议将有限的精度分配给那些异常值浓度较高的列，以最大化性能提升。'
- en: '![Refer to caption](img/c3c3d34b67c382fc7f9cba0ea235454f.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c3c3d34b67c382fc7f9cba0ea235454f.png)'
- en: 'Figure 3: The sorted outliers ratio in a self-attention matrix of LLaMA1-7B,
    most columns contain few outliers.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：LLaMA1-7B 自注意力矩阵中的排序异常值比例，大多数列包含的异常值较少。
- en: 'Figure [4](#A1.F4 "Figure 4 ‣ Appendix A Analysis on outlier ratio in LLM ‣
    CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization for LLMs") displays
    the top 10% columns with higher outlier ratio in the same matrix, which are more
    evenly distributed with no apparent pattern. This intuitive evidence highlights
    the heterogeneity between these columns, further emphasizing the effectiveness
    for selective precision assignments.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#A1.F4 "图 4 ‣ 附录 A LLM 中异常值比例分析 ‣ CLAQ：推动低比特后训练量化在 LLM 中的极限") 显示了相同矩阵中前
    10% 异常值比例较高的列，这些列的分布更均匀，没有明显的模式。这一直观证据突显了这些列之间的异质性，进一步强调了选择性精度分配的有效性。
- en: '![Refer to caption](img/f37ac04740eb87a271afb22494a1c95e.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f37ac04740eb87a271afb22494a1c95e.png)'
- en: 'Figure 4: The position of columns with higher outlier ratio in matrix. Columns
    with dark colour are the top 10% outlier concentrated columns.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：矩阵中异常值比例较高的列的位置。颜色较深的列是前 10% 异常值集中列。
- en: 'Extending the analysis to the entire model, Figure [5](#A1.F5 "Figure 5 ‣ Appendix
    A Analysis on outlier ratio in LLM ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs") summarizes the quantization outcomes for all 32 decoder
    layers in LLaMA1-7B. From a holistic model perspective, the initial layers exhibit
    a disproportionately high outlier incidences. Aligning with our hypothesis, this
    suggests that these layers hold increased significance. Considering the disparities
    between layers, we have explored a heuristic-based search strategy for identifying
    potentially improved adaptive precision allocation schemes, details of this approach
    are elaborated in Appendix G.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 将分析扩展到整个模型，图 [5](#A1.F5 "图 5 ‣ 附录 A LLM 中异常值比例分析 ‣ CLAQ：推动低比特后训练量化在 LLM 中的极限")
    总结了 LLaMA1-7B 中所有 32 层解码器的量化结果。从整体模型角度来看，初始层异常值发生率过高。与我们的假设一致，这表明这些层具有更高的重要性。考虑到层间的差异，我们探索了一种基于启发式的搜索策略，以识别潜在的改进自适应精度分配方案，该方法的详细信息在附录
    G 中进行了阐述。
- en: '![Refer to caption](img/03f8f3776380e914fede861936afe6c3.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/03f8f3776380e914fede861936afe6c3.png)'
- en: 'Figure 5: The overall outlier ratio of 32 layers in LLaMA1-7B.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：LLaMA1-7B 32 层的整体异常值比例。
- en: Appendix B Ablation study on outlier definition
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 异常值定义的消融研究
- en: The selection of the criterion for defining outliers is integral to the efficacy
    of the Outlier Order search process. To this end, we conducted an ablation study
    examining the impact of varying the outlier standard, denoted as $S$ emerges as
    the optimal setting, striking a balance where it neither lowers the threshold
    excessively to include an abundance of values nor sets the bar so high that certain
    columns lack any outliers. This configuration ensures accurate identification
    of meaningful outliers that significantly influence the performance of the model
    quantization strategy.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 定义异常值的标准选择对异常值排序搜索过程的有效性至关重要。为此，我们进行了消融研究，考察了不同异常值标准的影响，$S$ 被确定为最佳设置，它在不将阈值过低以包含过多值的同时，也不将门槛设得过高以至于某些列没有任何异常值。这一配置确保了能够准确识别出对模型量化策略性能有显著影响的有意义的异常值。
- en: 'Table 5: Ablation study on the standard of outlier definition on LLaMA1-7B.
    CLAQ* denotes the column-level adaptive precision quantization is applied.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：LLaMA1-7B 中异常值定义标准的消融研究。CLAQ* 表示应用了列级自适应精度量化。
- en: '| Method | # Bits | Outlier Standard | WikiText2↓ | C4↓ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | 异常值标准 | WikiText2↓ | C4↓ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| CLAQ* | 2.2 | $S=1$ | 16.39 | 16.47 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=1$ | 16.39 | 16.47 |'
- en: '| CLAQ* | 2.2 | $S=3$ | 13.44 | 14.52 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=3$ | 13.44 | 14.52 |'
- en: '| CLAQ* | 2.2 | $S=5$ | 10.76 | 12.07 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=5$ | 10.76 | 12.07 |'
- en: '| CLAQ* | 2.2 | $S=7$ | 10.58 | 11.71 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=7$ | 10.58 | 11.71 |'
- en: '| CLAQ* | 2.2 | $S=9$ | 10.37 | 11.64 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=9$ | 10.37 | 11.64 |'
- en: '| CLAQ* | 2.2 | $S=11$ | 10.25 | 11.58 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=11$ | 10.25 | 11.58 |'
- en: '| CLAQ* | 2.2 | $S=13$ | 10.14 | 11.46 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=13$ | 10.14 | 11.46 |'
- en: '| CLAQ* | 2.2 | $S=15$ | 10.32 | 11.53 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=15$ | 10.32 | 11.53 |'
- en: '| CLAQ* | 2.2 | $S=17$ | 10.34 | 11.51 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.2 | $S=17$ | 10.34 | 11.51 |'
- en: Appendix C Ablation study on hyper parameter in outlier reservation
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 异常值保留中的超参数消融研究
- en: Observations have indicated that the top 10% of columns harbor approximately
    90% of the outliers, prompting an investigation into the optimal strategy for
    allocating outlier retention within these columns with top outlier ratio versus
    the remaining 90%. We keep this proportion of columns to retain adaptively outliers
    in our experiments. The challenge lies in determining the most efficacious ratio
    for preserving outliers in these two segments. Various configurations were trialed
    in pursuit of the most suitable distribution.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 观察结果表明，前10%的列大约包含90%的离群值，这促使我们调查在这些具有高离群值比例的列与剩余90%列之间分配离群值保留的最佳策略。我们在实验中保留了这个比例的列以适应性地保留离群值。挑战在于确定这两个区段中保留离群值的最有效比例。我们试验了各种配置以寻求最合适的分配方案。
- en: 'We did grid search on the choice of outlier reservation ratio. We found 3 promising
    configuration settings: Setting 1: 19% for high outlier ratio columns, 81% for
    90% low outlier ratio columns. Setting 2: 28% for high outlier ratio columns,
    72% for 90% low outlier ratio columns. Setting 3: 37% for high outlier ratio columns,
    63% for 90% low outlier ratio columns. Experimental results shows setting 3 outperforms
    in terms of PPL, while setting 2 obtained the higher accuracy on zero-shot tasks
    than other settings. The performance gap on PPL is more obvious than zero-shot
    accuracy. Finally, striking the balance between downstream task accuracy and perplexity,
    we decide to choose Setting 2 for our main experiments.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对离群值保留比例的选择进行了网格搜索。我们发现了3个有前景的配置设置：设置1：高离群值比例列的19%，90%低离群值比例列的81%。设置2：高离群值比例列的28%，90%低离群值比例列的72%。设置3：高离群值比例列的37%，90%低离群值比例列的63%。实验结果表明，设置3在PPL方面表现最佳，而设置2在零样本任务上的准确性高于其他设置。PPL上的性能差距比零样本准确性更为明显。最后，在下游任务准确性和困惑度之间取得平衡后，我们决定选择设置2作为我们的主要实验设置。
- en: 'Table 6: Experimental results of ablation study conducted on different hyper-parameter
    settings about outlier reservation proportion.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：关于离群值保留比例的不同超参数设置的消融研究实验结果。
- en: '| Model | # Bits | Exp. Config | WikiText2↓ | C4↓ | Zero-Shot Avg↑ |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | # 位 | 实验配置 | WikiText2↓ | C4↓ | 零样本平均↑ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA1-7B | 16 | – | 5.63 | 7.08 | 70.38 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B | 16 | – | 5.63 | 7.08 | 70.38 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| +OR | 2.28 | Setting1 | 7.01 | 8.47 | 65.98 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| +OR | 2.28 | Setting1 | 7.01 | 8.47 | 65.98 |'
- en: '| +OR | 2.28 | Setting2 | 6.88 | 8.35 | 66.50 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| +OR | 2.28 | Setting2 | 6.88 | 8.35 | 66.50 |'
- en: '| +OR | 2.28 | Setting3 | 6.77 | 8.31 | 65.21 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| +OR | 2.28 | Setting3 | 6.77 | 8.31 | 65.21 |'
- en: '| +OR | 2.14 | Setting1 | 7.55 | 8.93 | 64.31 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| +OR | 2.14 | Setting1 | 7.55 | 8.93 | 64.31 |'
- en: '| +OR | 2.14 | Setting2 | 7.22 | 8.64 | 65.52 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| +OR | 2.14 | Setting2 | 7.22 | 8.64 | 65.52 |'
- en: '| +OR | 2.14 | Setting3 | 7.00 | 8.49 | 65.30 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| +OR | 2.14 | Setting3 | 7.00 | 8.49 | 65.30 |'
- en: Appendix D Ablation study on adaptive precision
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 自适应精度的消融研究
- en: 'With the objective of facilitating deployment at the hardware level, our approach
    adopts a dual-precision quantization scheme. A dilemma arises when targeting a
    quantization granularity between 2 to 3 bits: given an equivalent model size constraint,
    is it more beneficial to incorporate 2+3 mixture, or 2+4 mixture? To address this
    question, we designed a series of experiments under three distinct outlier standard
    conditions. The outcomes of which are summarized in Table LABEL:table:7. The outcomes
    consistently lead to a single, definitive conclusion: opting for fewer columns
    at 4 bits is preferable. This finding aligns with observations detailed in Appendix
    A, which underscores that columns with a high outlier count are relatively rare.
    Prioritizing higher precision for these selected columns proves crucial in achieving
    superior overall quantization efficacy.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于硬件级别的部署，我们的方法采用了双精度量化方案。在目标量化粒度为2到3位之间时，面临一个两难问题：在模型大小约束相同的情况下，采用2+3混合还是2+4混合更为有利？为了解决这个问题，我们在三种不同的离群值标准条件下设计了一系列实验。结果总结在表
    LABEL:table:7 中。这些结果一致得出一个明确的结论：选择4位的列数较少是更为优选的。该发现与附录 A 中的观察结果一致，后者强调高离群值数量的列相对较少。优先考虑这些选定列的高精度对实现更高的整体量化效果至关重要。
- en: 'Table 7: Experimental results of ablation study on LLaMA1-7B about adaptive
    precision candidate bit-width selection.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：关于LLaMA1-7B的自适应精度候选位宽选择的消融研究实验结果。
- en: '| Method | # Bits | Bits in AP | Outlier Standard | WikiText2↓ | C4↓ |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | # 位 | AP 中的位数 | 离群值标准 | WikiText2↓ | C4↓ |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| AP | 2.1 | 2&3 | $S=5$ | 12.35 | 13.77 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| AP | 2.1 | 2&3 | $S=5$ | 12.35 | 13.77 |'
- en: '| AP | 2.1 | 2&4 | $S=5$ | 12.21 | 13.21 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| AP | 2.1 | 2&4 | $S=5$ | 12.21 | 13.21 |'
- en: '| AP | 2.1 | 2&3 | $S=9$ | 11.89 | 13.07 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| AP | 2.1 | 2&3 | $S=9$ | 11.89 | 13.07 |'
- en: '| AP | 2.1 | 2&4 | $S=9$ | 11.52 | 12.73 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| AP | 2.1 | 2&4 | $S=9$ | 11.52 | 12.73 |'
- en: '| AP | 2.1 | 2&3 | $S=13$ | 11.67 | 12.78 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| AP | 2.1 | 2&3 | $S=13$ | 11.67 | 12.78 |'
- en: '| AP | 2.1 | 2&4 | $S=13$ | 11.06 | 12.34 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| AP | 2.1 | 2&4 | $S=13$ | 11.06 | 12.34 |'
- en: Appendix E More experimental results
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 E 更多实验结果
- en: More experiments on LLaMA2 and Yi are shown in the following tables. The results
    are consistent with the conclusions in the main results section. Our proposed
    CLAQ outperformed other quantization method in most settings; especially, CLAQ
    with AP and OR achieves SOTA performance under the low-bit condition.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: LLaMA2 和 Yi 上的更多实验结果显示在以下表格中。结果与主要结果部分的结论一致。我们提出的 CLAQ 在大多数设置下优于其他量化方法；特别是，CLAQ
    与 AP 和 OR 在低位条件下达到了 SOTA 性能。
- en: 'Table 8: Experimental results of perplexity in LLaMA-2-Base models. CLAQ models
    with * are fusion models employing column-level adaptive precision and outlier
    reservation techniques. $g=128$ means group size of quantization is 128, followed
    by the equivalent bit-width in italics.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：LLaMA-2-Base 模型中困惑度的实验结果。带*的CLAQ模型为融合模型，采用了列级自适应精度和异常值保留技术。$g=128$ 表示量化的组大小为128，后跟斜体的等效位宽。
- en: '| Method | # Bits |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | # 位数 |'
- en: '&#124; LLaMA2-7B &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLaMA2-7B &#124;'
- en: '&#124; WikiText2↓/C4↓ &#124;'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText2↓/C4↓ &#124;'
- en: '|'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LLaMA2-13B &#124;'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LLaMA2-13B &#124;'
- en: '&#124; WikiText2↓/C4↓ &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText2↓/C4↓ &#124;'
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| - | 16 | 5.46/6.97 | 4.88/6.46 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| - | 16 | 5.46/6.97 | 4.88/6.46 |'
- en: '| GPTQ | 4 | 5.82/7.36 | 5.13/6.70 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 5.82/7.36 | 5.13/6.70 |'
- en: '| AWQ | 4 | 6.15/7.68 | 5.12/6.74 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4 | 6.15/7.68 | 5.12/6.74 |'
- en: '| AWQ | 4 ($g=128$,4.15) | 5.62/7.13 | 4.97/6.56 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 4 ($g=128$,4.15) | 5.62/7.13 | 4.97/6.56 |'
- en: '| OmniQuant | 4 | 5.74/7.35 | 5.02/6.65 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 4 | 5.74/7.35 | 5.02/6.65 |'
- en: '| OmniQuant | 4 ($g=128$,4.15) | 5.58/7.12 | 4.95/6.56 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 4 ($g=128$,4.15) | 5.58/7.12 | 4.95/6.56 |'
- en: '| CLAQ | 4 | 5.62/7.13 | 5.00/6.56 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 4 | 5.62/7.13 | 5.00/6.56 |'
- en: '| GPTQ | 3 | 8.49/9.92 | 6.40/8.00 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 3 | 8.49/9.92 | 6.40/8.00 |'
- en: '| AWQ | 3 | 24.00/23.85 | 10.45/13.07 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 3 | 24.00/23.85 | 10.45/13.07 |'
- en: '| OmniQuant | 3 | 6.58/8.65 | 5.58/7.44 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 3 | 6.58/8.65 | 5.58/7.44 |'
- en: '| CLAQ | 3 | 6.39/7.80 | 5.46/7.03 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 3 | 6.39/7.80 | 5.46/7.03 |'
- en: '| AWQ | 3 ($g=128$,3.15) | 6.24/7.84 | 5.32/6.94 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 3 ($g=128$,3.15) | 6.24/7.84 | 5.32/6.94 |'
- en: '| OmniQuant | 3 ($g=128$,3.15) | 6.03/7.75 | 5.28/6.98 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 3 ($g=128$,3.15) | 6.03/7.75 | 5.28/6.98 |'
- en: '| CLAQ* | 3.12 | 5.81/7.34 | 5.15/6.72 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 3.12 | 5.81/7.34 | 5.15/6.72 |'
- en: '| CLAQ* | 3.23 | 5.79/7.31 | 5.12/6.70 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 3.23 | 5.79/7.31 | 5.12/6.70 |'
- en: '| GPTQ | 2 | Nan/2477.60 | 1832.06/349.17 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 2 | Nan/2477.60 | 1832.06/349.17 |'
- en: '| CLAQ | 2 | 7579.07/1412.06 | 4853.96/3404.98 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 2 | 7579.07/1412.06 | 4853.96/3404.98 |'
- en: '| AWQ | 2 ($g=64$,2.28) | 2.1e5/1.6e5 | 1.2e5/9.5e4 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 2 ($g=64$,2.28) | 2.1e5/1.6e5 | 1.2e5/9.5e4 |'
- en: '| OmniQuant | 2 ($g=64$,2.28) | 9.62/12.72 | 7.56/10.05 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 2 ($g=64$,2.28) | 9.62/12.72 | 7.56/10.05 |'
- en: '| CLAQ* | 2.12 | 7.69/9.13 | 6.30/7.94 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.12 | 7.69/9.13 | 6.30/7.94 |'
- en: '| CLAQ* | 2.24 | 6.89/8.47 | 5.88/7.50 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.24 | 6.89/8.47 | 5.88/7.50 |'
- en: 'Table 9: Experimental results of perplexity in Yi-34B-Base model. CLAQ models
    with * are fusion models employing column-level adaptive precision and outlier
    reservation techniques.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：Yi-34B-Base 模型中困惑度的实验结果。带*的CLAQ模型为融合模型，采用了列级自适应精度和异常值保留技术。
- en: '| Method | # Bits |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | # 位数 |'
- en: '&#124; Yi-34B &#124;'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Yi-34B &#124;'
- en: '&#124; WikiText2↓/C4↓ &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WikiText2↓/C4↓ &#124;'
- en: '|'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| - | 16 | 24.73/29.57 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| - | 16 | 24.73/29.57 |'
- en: '| GPTQ | 4 | 32.10/40.55 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 32.10/40.55 |'
- en: '| CLAQ | 4 | 26.38/31.38 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 4 | 26.38/31.38 |'
- en: '| GPTQ | 3 | 59.16/69.45 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 3 | 59.16/69.45 |'
- en: '| CLAQ | 3 | 56.03/67.69 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 3 | 56.03/67.69 |'
- en: '| CLAQ* | 3.12 | 26.85/31.82 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 3.12 | 26.85/31.82 |'
- en: '| CLAQ* | 3.23 | 28.89/33.24 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 3.23 | 28.89/33.24 |'
- en: '| GPTQ | 2 | 9577.46/9018.70 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 2 | 9577.46/9018.70 |'
- en: '| CLAQ | 2 | 199.86/247.80 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 2 | 199.86/247.80 |'
- en: '| CLAQ* | 2.12 | 46.04/51.31 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.12 | 46.04/51.31 |'
- en: '| CLAQ* | 2.24 | 41.80/45.90 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ* | 2.24 | 41.80/45.90 |'
- en: 'Table 10: Experimental results of zero-shot tasks in LLaMA-2-Base and Yi-34B-Base
    models.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 表 10：LLaMA-2-Base 和 Yi-34B-Base 模型中的零样本任务实验结果。
- en: '| Model | Method | # Bits | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | Avg↑ |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | # 位数 | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande |
    平均↑ |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '|  | FP16 | 16 | 79.11 | 76.30 | 46.42 | 77.77 | 75.99 | 69.06 | 70.78 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 79.11 | 76.30 | 46.42 | 77.77 | 75.99 | 69.06 | 70.78 |'
- en: '|  | GPTQ | 4 | 79.05 | 74.71 | 45.14 | 77.58 | 74.33 | 69.30 | 70.02 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 4 | 79.05 | 74.71 | 45.14 | 77.58 | 74.33 | 69.30 | 70.02 |'
- en: '| LLaMA2-7B | CLAQ | 4 | 78.45 | 75.76 | 44.71 | 77.25 | 75.17 | 68.03 | 69.90
    |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | CLAQ | 4 | 78.45 | 75.76 | 44.71 | 77.25 | 75.17 | 68.03 | 69.90
    |'
- en: '|  | FP16 | 16 | 80.52 | 79.46 | 49.15 | 80.58 | 79.38 | 72.22 | 73.55 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 80.52 | 79.46 | 49.15 | 80.58 | 79.38 | 72.22 | 73.55 |'
- en: '|  | GPTQ | 4 | 80.41 | 78.58 | 47.18 | 79.27 | 78.08 | 71.51 | 72.51 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 4 | 80.41 | 78.58 | 47.18 | 79.27 | 78.08 | 71.51 | 72.51 |'
- en: '| LLaMA2-13B | CLAQ | 4 | 78.51 | 79.21 | 50.43 | 78.60 | 79.76 | 72.22 | 73.12
    |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | CLAQ | 4 | 78.51 | 79.21 | 50.43 | 78.60 | 79.76 | 72.22 | 73.12
    |'
- en: '|  | FP16 | 16 | 82.75 | 84.34 | 62.12 | 88.35 | 83.64 | 79.24 | 80.07 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 82.75 | 84.34 | 62.12 | 88.35 | 83.64 | 79.24 | 80.07 |'
- en: '|  | GPTQ | 4 | 81.66 | 81.48 | 59.04 | 86.15 | 81.95 | 76.16 | 77.74 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 4 | 81.66 | 81.48 | 59.04 | 86.15 | 81.95 | 76.16 | 77.74 |'
- en: '| Yi-34B | CLAQ | 4 | 82.64 | 83.29 | 60.15 | 88.59 | 83.26 | 77.19 | 79.19
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Yi-34B | CLAQ | 4 | 82.64 | 83.29 | 60.15 | 88.59 | 83.26 | 77.19 | 79.19
    |'
- en: 'Table 11: Experimental results of zero-shot accuracy in LLaMA-2-Base and Yi-34B-Base
    models. CLAQ models with* are fusion models employing column-level adaptive precision
    and outlier reservation techniques.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: LLaMA-2-Base 和 Yi-34B-Base 模型的零-shot准确性实验结果。带*的 CLAQ 模型是融合模型，采用了列级自适应精度和离群值保留技术。'
- en: '| Model | Method | # Bits | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | Avg↑ |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 方法 | # 位 | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande | 平均↑
    |'
- en: '|  | FP16 | 16 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01 | 70.38 |'
- en: '|  | GPTQ | 3 | 75.52 | 64.10 | 35.58 | 66.76 | 68.31 | 63.30 | 62.26 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 3 | 75.52 | 64.10 | 35.58 | 66.76 | 68.31 | 63.30 | 62.26 |'
- en: '|  | CLAQ* | 3.1 | 78.89 | 72.60 | 42.92 | 73.52 | 74.32 | 69.69 | 68.66 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 3.1 | 78.89 | 72.60 | 42.92 | 73.52 | 74.32 | 69.69 | 68.66 |'
- en: '|  | GPTQ | 2 | 52.50 | 25.76 | 26.96 | 41.87 | 25.77 | 52.49 | 37.56 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2 | 52.50 | 25.76 | 26.96 | 41.87 | 25.77 | 52.49 | 37.56 |'
- en: '| LLaMA1-7B | CLAQ* | 2.1 | 75.73 | 68.56 | 36.43 | 70.73 | 67.05 | 67.25 |
    64.29 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B | CLAQ* | 2.1 | 75.73 | 68.56 | 36.43 | 70.73 | 67.05 | 67.25 |
    64.29 |'
- en: '|  | FP16 | 16 | 80.14 | 77.40 | 47.70 | 77.92 | 79.09 | 72.85 | 72.52 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 80.14 | 77.40 | 47.70 | 77.92 | 79.09 | 72.85 | 72.52 |'
- en: '|  | GPTQ | 3 | 77.69 | 73.40 | 42.75 | 68.56 | 73.14 | 66.54 | 67.01 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 3 | 77.69 | 73.40 | 42.75 | 68.56 | 73.14 | 66.54 | 67.01 |'
- en: '|  | CLAQ* | 3.1 | 79.92 | 76.43 | 45.90 | 76.27 | 78.15 | 71.35 | 71.34 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 3.1 | 79.92 | 76.43 | 45.90 | 76.27 | 78.15 | 71.35 | 71.34 |'
- en: '|  | GPTQ | 2 | 52.18 | 25.88 | 28.41 | 41.35 | 25.67 | 49.64 | 37.19 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2 | 52.18 | 25.88 | 28.41 | 41.35 | 25.67 | 49.64 | 37.19 |'
- en: '| LLaMA1-13B | CLAQ* | 2.1 | 77.80 | 72.22 | 41.21 | 69.42 | 71.86 | 69.14
    | 66.94 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-13B | CLAQ* | 2.1 | 77.80 | 72.22 | 41.21 | 69.42 | 71.86 | 69.14
    | 66.94 |'
- en: '|  | FP16 | 16 | 82.26 | 80.43 | 52.90 | 82.69 | 82.63 | 75.85 | 76.13 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 82.26 | 80.43 | 52.90 | 82.69 | 82.63 | 75.85 | 76.13 |'
- en: '|  | GPTQ | 3 | 78.78 | 73.65 | 46.50 | 75.78 | 77.86 | 74.59 | 71.19 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 3 | 78.78 | 73.65 | 46.50 | 75.78 | 77.86 | 74.59 | 71.19 |'
- en: '|  | CLAQ* | 3.1 | 81.66 | 78.96 | 51.02 | 82.11 | 81.49 | 74.66 | 74.98 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 3.1 | 81.66 | 78.96 | 51.02 | 82.11 | 81.49 | 74.66 | 74.98 |'
- en: '|  | GPTQ | 2 | 53.16 | 27.74 | 26.79 | 43.46 | 27.10 | 48.46 | 37.79 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2 | 53.16 | 27.74 | 26.79 | 43.46 | 27.10 | 48.46 | 37.79 |'
- en: '| LLaMA1-30B | CLAQ* | 2.1 | 80.03 | 76.73 | 47.10 | 79.51 | 76.55 | 73.64
    | 72.26 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-30B | CLAQ* | 2.1 | 80.03 | 76.73 | 47.10 | 79.51 | 76.55 | 73.64
    | 72.26 |'
- en: '|  | FP16 | 16 | 82.26 | 81.36 | 55.55 | 84.83 | 84.12 | 77.27 | 77.57 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 82.26 | 81.36 | 55.55 | 84.83 | 84.12 | 77.27 | 77.57 |'
- en: '|  | GPTQ | 3 | 80.67 | 78.07 | 51.45 | 80.15 | 80.96 | 74.35 | 74.28 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 3 | 80.67 | 78.07 | 51.45 | 80.15 | 80.96 | 74.35 | 74.28 |'
- en: '|  | CLAQ* | 3.1 | 81.88 | 80.51 | 55.72 | 84.43 | 83.47 | 77.11 | 77.19 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 3.1 | 81.88 | 80.51 | 55.72 | 84.43 | 83.47 | 77.11 | 77.19 |'
- en: '|  | GPTQ | 2 | 53.59 | 28.62 | 27.22 | 54.80 | 30.09 | 51.62 | 40.99 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2 | 53.59 | 28.62 | 27.22 | 54.80 | 30.09 | 51.62 | 40.99 |'
- en: '| LLaMA1-65B | CLAQ* | 2.1 | 80.20 | 78.96 | 50.17 | 79.14 | 79.39 | 75.30
    | 73.86 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-65B | CLAQ* | 2.1 | 80.20 | 78.96 | 50.17 | 79.14 | 79.39 | 75.30
    | 73.86 |'
- en: '|  | FP16 | 16 | 79.11 | 76.30 | 46.42 | 77.77 | 75.99 | 69.06 | 70.78 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 79.11 | 76.30 | 46.42 | 77.77 | 75.99 | 69.06 | 70.78 |'
- en: '|  | GPTQ | 3 | 74.81 | 66.04 | 38.65 | 69.63 | 66.70 | 63.54 | 63.23 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 3 | 74.81 | 66.04 | 38.65 | 69.63 | 66.70 | 63.54 | 63.23 |'
- en: '|  | CLAQ* | 3.1 | 78.62 | 74.24 | 42.83 | 75.47 | 74.33 | 69.53 | 69.17 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 3.1 | 78.62 | 74.24 | 42.83 | 75.47 | 74.33 | 69.53 | 69.17 |'
- en: '|  | GPTQ | 2 | 52.12 | 26.52 | 27.90 | 42.45 | 26.11 | 48.07 | 37.20 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2 | 52.12 | 26.52 | 27.90 | 42.45 | 26.11 | 48.07 | 37.20 |'
- en: '| LLaMA2-7B | CLAQ* | 2.1 | 75.03 | 70.50 | 38.23 | 72.81 | 66.82 | 66.77 |
    65.03 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | CLAQ* | 2.1 | 75.03 | 70.50 | 38.23 | 72.81 | 66.82 | 66.77 |
    65.03 |'
- en: '|  | FP16 | 16 | 80.52 | 79.46 | 49.15 | 80.58 | 79.38 | 72.22 | 73.55 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 80.52 | 79.46 | 49.15 | 80.58 | 79.38 | 72.22 | 73.55 |'
- en: '|  | GPTQ | 3 | 77.97 | 73.02 | 43.86 | 74.07 | 73.88 | 68.11 | 68.49 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 3 | 77.97 | 73.02 | 43.86 | 74.07 | 73.88 | 68.11 | 68.49 |'
- en: '|  | CLAQ* | 3.1 | 79.54 | 78.07 | 49.23 | 76.67 | 77.81 | 71.43 | 71.63 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 3.1 | 79.54 | 78.07 | 49.23 | 76.67 | 77.81 | 71.43 | 71.63 |'
- en: '|  | GPTQ | 2 | 53.16 | 25.67 | 27.90 | 39.69 | 25.61 | 49.88 | 36.99 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2 | 53.16 | 25.67 | 27.90 | 39.69 | 25.61 | 49.88 | 36.99 |'
- en: '| LLaMA2-13B | CLAQ* | 2.1 | 77.53 | 74.75 | 43.43 | 76.88 | 72.79 | 70.80
    | 69.36 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | CLAQ* | 2.1 | 77.53 | 74.75 | 43.43 | 76.88 | 72.79 | 70.80
    | 69.36 |'
- en: '|  | FP16 | 16 | 82.75 | 84.34 | 62.12 | 88.35 | 83.64 | 79.24 | 80.07 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '|  | FP16 | 16 | 82.75 | 84.34 | 62.12 | 88.35 | 83.64 | 79.24 | 80.07 |'
- en: '|  | GPTQ | 3 | 75.30 | 64.23 | 38.23 | 64.98 | 67.93 | 64.72 | 62.57 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 3 | 75.30 | 64.23 | 38.23 | 64.98 | 67.93 | 64.72 | 62.57 |'
- en: '|  | CLAQ* | 3.1 | 83.30 | 83.33 | 58.79 | 87.86 | 82.56 | 77.35 | 78.87 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | CLAQ* | 3.1 | 83.30 | 83.33 | 58.79 | 87.86 | 82.56 | 77.35 | 78.87 |'
- en: '|  | GPTQ | 2 | 51.90 | 25.04 | 27.30 | 40.52 | 25.97 | 48.07 | 36.47 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTQ | 2 | 51.90 | 25.04 | 27.30 | 40.52 | 25.97 | 48.07 | 36.47 |'
- en: '| Yi-34B | CLAQ* | 2.1 | 81.07 | 80.43 | 54.27 | 79.36 | 77.49 | 74.90 | 74.59
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| Yi-34B | CLAQ* | 2.1 | 81.07 | 80.43 | 54.27 | 79.36 | 77.49 | 74.90 | 74.59
    |'
- en: Appendix F Experiment details
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 实验细节
- en: Our experiments are conducted based on a branching off from the GPTQ[frantar2022gptq](#bib.bib15)
    implementation. The quantization group size is set to the entire column. The calibration
    data consists of 128 random 2048 token segments from the C4 dataset. The K-Means
    clustering is calculated with the scikit-learn-intelex CPU version.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验是基于从 GPTQ[frantar2022gptq](#bib.bib15) 实现中分支出来的。量化组大小设置为整个列。校准数据由 C4 数据集中的
    128 个随机 2048 标记片段组成。K-Means 聚类使用 scikit-learn-intelex CPU 版本进行计算。
- en: 'The fusion models with AP+OR utilize two settings: (1) 2.12/3.12bit: 0.05 bit
    increment in adaptive precision with 2&4 bits, and 0.07 bit of full-precision
    outliers kept with Setting 2 in Appendix C. (2) 2.24/3.23bit: 0.1 bit increment
    in adaptive precision with 2&4 bits, and 0.13 bit of full-precision outliers kept
    with Setting 2 in Appendix C. The outlier standard is set to $S=13$ in all experiments.
    Based on our observations in Appendix A, we fixed the column proportion of outliers
    to be retained in our experiments. We always preserve more outliers in the most
    sensitive top 10% of columns, while retaining fewer outliers in the remaining
    90% of columns. The code for reproducing the results in Table [1](#S4.T1 "Table
    1 ‣ Datasets ‣ 4.1 Experimental Settings ‣ 4 Experiments and Results ‣ CLAQ: Pushing
    the Limits of Low-Bit Post-Training Quantization for LLMs"), Table LABEL:table:2
    and other tables in Appendix E is contained in supplemental materials. Check the
    ReadMe file for more information.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '使用 AP+OR 的融合模型有两个设置：(1) 2.12/3.12 位：在自适应精度中使用 2&4 位时，增量为 0.05 位，并且在附录 C 的设置
    2 中保留 0.07 位的全精度离群点。(2) 2.24/3.23 位：在自适应精度中使用 2&4 位时，增量为 0.1 位，并且在附录 C 的设置 2 中保留
    0.13 位的全精度离群点。所有实验中的离群点标准设置为 $S=13$。根据附录 A 中的观察结果，我们固定了实验中保留的离群点列比例。我们总是保留在最敏感的前
    10% 列中的更多离群点，同时在剩余的 90% 列中保留较少的离群点。用于重现表 [1](#S4.T1 "Table 1 ‣ Datasets ‣ 4.1
    Experimental Settings ‣ 4 Experiments and Results ‣ CLAQ: Pushing the Limits of
    Low-Bit Post-Training Quantization for LLMs")、表 LABEL:table:2 和附录 E 中其他表格的代码包含在补充材料中。更多信息请查看
    ReadMe 文件。'
- en: Appendix G Heuristic adaptive precision search algorithm
  id: totrans-394
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 启发式自适应精度搜索算法
- en: Our adaptive precision approach, detailed in Section 3.3, combines two precision
    levels. However, when dealing with expanded adaptive precision search landscapes,
    such as 2.5 bits instead of 2.1 bits, straightforward methodologies may not consistently
    discover the most advantageous configurations. To address this complexity, we
    propose a heuristic-based adaptive precision search algorithm. This algorithm,
    inspired by the HAWQ v2 [dong2020hawq](#bib.bib12) method, initially ranks weight
    matrices by their outlier ratios, ensuring matrices with higher outlier concentrations
    receive higher precision. This step transforms the search space into a combinatorial
    problem.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的自适应精度方法，在第 3.3 节中详细描述，结合了两种精度水平。然而，当处理扩展的自适应精度搜索空间时，例如 2.5 位而不是 2.1 位，简单的方法可能无法始终发现最有利的配置。为了解决这一复杂性，我们提出了一种基于启发式的自适应精度搜索算法。该算法受到
    HAWQ v2 [dong2020hawq](#bib.bib12) 方法的启发，最初根据权重矩阵的离群点比率对其进行排序，确保离群点浓度较高的矩阵获得更高的精度。这一步将搜索空间转变为组合问题。
- en: Next, we discretize the precision levels for each column to a defined set, thereby
    confining the search within a manageable subset of all possible combinations under
    the model size constraint. We traverse this reduced search space, exploring all
    feasible precision arrangements, and establish a precision scoring system to facilitate
    the selection of the most suitable configuration. In essence, this heuristic-driven
    adaptive precision search process defines discrete precision options for each
    column, enumerates all feasible combinations within the narrowed search space,
    and selects the combination with the highest precision score. This systematic
    approach optimizes the use of precision levels across matrices, thereby enhancing
    the overall quantization efficiency under practical constraints.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将每列的精度水平离散化为定义的集合，从而将搜索限制在所有可能组合的一个可管理子集内。我们遍历这个减少的搜索空间，探索所有可行的精度安排，并建立一个精度评分系统以便选择最合适的配置。实质上，这一启发式驱动的自适应精度搜索过程为每列定义离散的精度选项，枚举在缩小的搜索空间内的所有可行组合，并选择具有最高精度得分的组合。这种系统化的方法优化了矩阵间精度水平的使用，从而在实际约束下提高了整体量化效率。
- en: 'Specifically, in the case of an adaptive precision search involving 2-bit and
    3-bit quantization levels, we categorize each matrix’s potential precision into
    three classes: 2-bit, a combination of 2&3-bit, and a combination of 2&4-bit.
    We allocate a higher precision score $PS_{4}$ can be formulated as:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，对于涉及2位和3位量化水平的自适应精度搜索情况，我们将每个矩阵的潜在精度分为三类：2位、2&3位组合和2&4位组合。我们分配一个较高的精度得分
    $PS_{4}$ 可以表示为：
- en: '|  | $\displaystyle P=\left\{p_{3},p_{4}\right\}\quad M=\left\{M_{3},M_{4}\right\}$
    |  | (6) |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P=\left\{p_{3},p_{4}\right\}\quad M=\left\{M_{3},M_{4}\right\}$
    |  | (6) |'
- en: '|  | $1$2 |  | (7) |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: '|  | $\displaystyle P,L=\mathop{\arg\max}_{q}\enspace PS_{total}$ |  | (8)
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P,L=\mathop{\arg\max}_{q}\enspace PS_{total}$ |  | (8)
    |'
- en: 'Where $P$ for experiments in Table [12](#A7.T12 "Table 12 ‣ Appendix G Heuristic
    adaptive precision search algorithm ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training
    Quantization for LLMs"). The precision score for 2-bit columns are 0\. This heuristic
    search strategy realizes adaptive precision quantization by exhaustively enumerating
    all feasible combinations of precision allocations, subsequently computing a search
    criterion based on the weighted parameter precision scores, which are derived
    from the outlier ratios. The goal of adaptive precision search process is to find
    the combination with the highest precision score. Following this adaptive precision
    search strategy, in scenarios where the incremental bit-width is modest (i.e.,
    2.1 or 2.2 bits), the search results favor a configuration utilizing the maximum
    number of 2&4-bit matrices, which is similar to our previously described approach.
    In the case of a 2.5-bit search, 19 matrices allocated as 2&4-bit with 10% 4-bit
    columns, 205 2&3-bit matrices are allocated with 52.6% 3-bit columns. Nevertheless,
    the model consistently minimizes the proportion of parameters quantized solely
    at 2 bits. The results of the heuristic algorithm based adaptive precision quantization
    are detailed in Table [12](#A7.T12 "Table 12 ‣ Appendix G Heuristic adaptive precision
    search algorithm ‣ CLAQ: Pushing the Limits of Low-Bit Post-Training Quantization
    for LLMs"), evidencing that heuristic-based searching can indeed refine model
    performance.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $P$ 对应表 [12](#A7.T12 "表 12 ‣ 附录 G 启发式自适应精度搜索算法 ‣ CLAQ: 推动LLMs低位后训练量化的极限")
    的实验结果。2位列的精度得分为0。该启发式搜索策略通过穷举所有可能的精度分配组合来实现自适应精度量化，随后基于从离群值比例导出的加权参数精度得分计算搜索标准。自适应精度搜索过程的目标是找到具有最高精度得分的组合。遵循这一自适应精度搜索策略，在增量位宽较小的情况下（即2.1或2.2位），搜索结果偏向于利用最大数量的2&4位矩阵，这与我们之前描述的方法类似。在2.5位搜索的情况下，19个矩阵分配为2&4位，其中10%为4位列，205个2&3位矩阵分配为52.6%
    3位列。尽管如此，模型始终尽量减少仅量化为2位的参数比例。启发式算法基于的自适应精度量化结果详见表 [12](#A7.T12 "表 12 ‣ 附录 G 启发式自适应精度搜索算法
    ‣ CLAQ: 推动LLMs低位后训练量化的极限")，证据表明，基于启发式的搜索确实能够改善模型性能。'
- en: 'Table 12: Results of heuristic adaptive precision search at 2.5-bit.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: 2.5位的启发式自适应精度搜索结果'
- en: '| Model | # Bits | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag
    | Winogrande | Avg↑ |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 位数 | WikiText2↓ | C4↓ | PIQA | Arc-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | 平均↑ |'
- en: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B | 16 | 5.63 | 7.08 | 79.11 | 75.25 | 44.62 | 75.11 | 76.20 | 70.01
    | 70.38 |'
- en: '| CLAQ | 3 | 6.47 | 7.87 | 78.23 | 72.05 | 41.12 | 72.93 | 72.05 | 67.71 |
    67.35 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 3 | 6.47 | 7.87 | 78.23 | 72.05 | 41.12 | 72.93 | 72.05 | 67.71 |
    67.35 |'
- en: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 2 | 27.64 | 24.37 | 62.89 | 31.43 | 25.51 | 61.52 | 28.69 | 51.85
    | 43.65 |'
- en: '| +AP(ours) | 2.5 | 8.43 | 9.78 | 75.63 | 68.10 | 35.24 | 66.39 | 63.82 | 64.40
    | 62.26 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| +AP（我们的） | 2.5 | 8.43 | 9.78 | 75.63 | 68.10 | 35.24 | 66.39 | 63.82 | 64.40
    | 62.26 |'
- en: '| +AP(Heuristic AP search) | 2.5 | 7.88 | 9.40 | 75.95 | 68.73 | 36.01 | 72.39
    | 65.98 | 64.72 | 63.96 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| +AP（启发式 AP 搜索） | 2.5 | 7.88 | 9.40 | 75.95 | 68.73 | 36.01 | 72.39 | 65.98
    | 64.72 | 63.96 |'
- en: 'However, further analyses indicate that the stability of our heuristic adaptive
    precision search strategy is somewhat inconsistent, with results being heavily
    influenced by the precision score assignment. This highlights a key insight: developing
    an end-to-end, effective, and interpretable adaptive precision search strategy
    that consistently improves performance under varying conditions remains a significant
    challenge for future research. The pursuit of a robust and explainable algorithm
    capable of systematically navigating the extensive search-space of adaptive precision
    configurations, optimizing both model performance and computational efficiency,
    is a critical direction for advancing the field of model quantization.'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，进一步分析表明，我们的启发式自适应精度搜索策略的稳定性有些不一致，结果受到精度评分分配的影响较大。 这突显了一个关键见解：开发一种端到端的、有效的和可解释的自适应精度搜索策略，在不同条件下始终提高性能仍然是未来研究的一项重要挑战。
    寻求一种强大且可解释的算法，能够系统地导航自适应精度配置的广泛搜索空间，优化模型性能和计算效率，是推动模型量化领域发展的关键方向。
- en: Appendix H Ablation study on calibration data
  id: totrans-410
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 关于校准数据的消融研究
- en: We investigated the impact of calibration data on experimental outcomes, aligning
    with the data selection used in GPTQ [frantar2022gptq](#bib.bib15) . Calibration
    data consists of 128 random 2048 token segments from the C4 [raffel2020c4](#bib.bib34)
    dataset. We found the non-negligible effect of varying calibration datasets, emphasizing
    that tailoring the calibration data to specific datasets positively influences
    the perplexity (PPL) evaluation for those datasets. This highlights the importance
    of a calibrated and dataset-specific approach in enhancing quantization performance.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调查了校准数据对实验结果的影响，并对齐了 GPTQ 中使用的数据选择 [frantar2022gptq](#bib.bib15)。 校准数据由 C4
    [raffel2020c4](#bib.bib34) 数据集中 128 个随机的 2048 令牌片段组成。 我们发现不同校准数据集的影响不可忽视，强调了将校准数据量身定制到特定数据集对这些数据集的困惑度（PPL）评估的积极影响。
    这突显了在提高量化性能时，校准和数据集特定方法的重要性。
- en: 'Table 13: Experimental results of CLAQ models calibrated on C4 [raffel2020c4](#bib.bib34)
    and Wikitext2 [merity2016wiki2](#bib.bib31) .'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13：在 C4 [raffel2020c4](#bib.bib34) 和 Wikitext2 [merity2016wiki2](#bib.bib31)
    上校准的 CLAQ 模型实验结果。
- en: '| Model | # Bits | Calibration | WikiText2↓ | C4↓ |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | # 位 | 校准 | WikiText2↓ | C4↓ |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaMA1-7B | 16 | – | 5.63 | 7.08 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA1-7B | 16 | – | 5.63 | 7.08 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| CLAQ | 4 | on wiki | 5.72 | 7.23 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 4 | on wiki | 5.72 | 7.23 |'
- en: '| CLAQ | 4 | on c4 | 5.78 | 7.21 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 4 | on c4 | 5.78 | 7.21 |'
- en: '| CLAQ | 3 | on wiki | 6.04 | 7.89 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 3 | on wiki | 6.04 | 7.89 |'
- en: '| CLAQ | 3 | on c4 | 6.47 | 7.87 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 3 | on c4 | 6.47 | 7.87 |'
- en: '| CLAQ | 2 | on wiki | 32.35 | 44.95 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 2 | on wiki | 32.35 | 44.95 |'
- en: '| CLAQ | 2 | on c4 | 27.64 | 24.37 |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| CLAQ | 2 | on c4 | 27.64 | 24.37 |'
- en: Appendix I Broader Impact
  id: totrans-423
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 广泛影响
- en: Our proposed LLM compression approach has the potential to enhance the compression
    ratio of mainstream open-source LLMs by up to 8x, with nearly lossless compression
    enhancement reaching up to 4x. This expands the applicability of LLMs. However,
    it is beyond our scope to determine whether the compressed models maintain their
    intended behavior post-compression; we lack the capability to monitor the exact
    usage scenarios of our compressed models or ascertain whether they might be repurposed
    for unintended applications.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的 LLM 压缩方法有潜力将主流开源 LLM 的压缩比提高至 8 倍，几乎无损压缩的提升达到 4 倍。 这扩展了 LLM 的适用性。 然而，我们无法确定压缩后的模型是否保持其预期的行为；我们缺乏监控压缩模型的确切使用场景或确定它们是否可能被重新用于意图之外的应用的能力。
