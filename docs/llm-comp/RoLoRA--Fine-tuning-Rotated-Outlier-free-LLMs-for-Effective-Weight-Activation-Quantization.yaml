- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: RoLoRA：针对有效的权重-激活量化调整旋转去异常值的LLMs
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08044](https://ar5iv.labs.arxiv.org/html/2407.08044)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08044](https://ar5iv.labs.arxiv.org/html/2407.08044)
- en: Xijie Huang¹, Zechun Liu², Shih-yang Liu¹, Kwang-Ting Cheng¹
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Xijie Huang¹, Zechun Liu², Shih-yang Liu¹, Kwang-Ting Cheng¹
- en: ¹Hong Kong University of Science and Technology, ²Meta Reality Labs
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹香港科技大学，²Meta Reality Labs
- en: '{xhuangbs,sliuau}@connect.ust.hk, zechunliu@meta.com,timcheng@ust.hk All the
    work was done within HKUST and Zechun Liu served an advisory role.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{xhuangbs,sliuau}@connect.ust.hk, zechunliu@meta.com, timcheng@ust.hk 所有工作均在HKUST完成，Zechun
    Liu担任顾问角色。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Low-Rank Adaptation (LoRA), as a representative Parameter-Efficient Fine-Tuning
    (PEFT) method, significantly enhances the training efficiency by updating only
    a small portion of the weights in Large Language Models (LLMs). Recently, weight-only
    quantization techniques have also been applied to LoRA methods to reduce the memory
    footprint of fine-tuning. However, applying weight-activation quantization to
    the LoRA pipeline is under-explored, and we observe substantial performance degradation
    primarily due to the presence of activation outliers. In this work, we propose
    RoLoRA, the first LoRA-based scheme for effective weight-activation quantization.
    RoLoRA utilizes rotation for outlier elimination and proposes rotation-aware fine-tuning
    to preserve the outlier-free characteristics in rotated LLMs. Experimental results
    show RoLoRA consistently improves low-bit LoRA convergence and post-training quantization
    robustness in weight-activation settings. We evaluate RoLoRA across LLaMA2-7B/13B,
    LLaMA3-8B models, achieving up to 29.5% absolute accuracy gain of 4-bit weight-activation
    quantized LLaMA2-13B on commonsense reasoning tasks compared to LoRA baseline.
    We further demonstrate its effectiveness on Large Multimodal Models (LLaVA-1.5-7B).
    Codes are available at [https://github.com/HuangOwen/RoLoRA](https://github.com/HuangOwen/RoLoRA)
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩自适应（LoRA）作为一种代表性的参数高效微调（PEFT）方法，通过仅更新大型语言模型（LLMs）中的一小部分权重，大大提高了训练效率。最近，权重量化技术也被应用于LoRA方法中，以减少微调的内存占用。然而，将权重-激活量化应用于LoRA流程尚未得到充分探索，我们观察到性能显著下降，主要由于存在激活异常值。在这项工作中，我们提出了RoLoRA，这是第一个基于LoRA的有效权重-激活量化方案。RoLoRA利用旋转进行异常值消除，并提出了旋转感知微调，以保持旋转LLMs中的去异常值特性。实验结果表明，RoLoRA在权重-激活设置中一致提高了低比特LoRA收敛性和后训练量化鲁棒性。我们在LLaMA2-7B/13B、LLaMA3-8B模型上评估了RoLoRA，相较于LoRA基准，在常识推理任务中，4-bit权重-激活量化LLaMA2-13B的绝对准确率提升高达29.5%。我们进一步展示了它在大型多模态模型（LLaVA-1.5-7B）上的有效性。代码可在[https://github.com/HuangOwen/RoLoRA](https://github.com/HuangOwen/RoLoRA)找到。
- en: 'RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: RoLoRA：针对有效的权重-激活量化调整旋转去异常值的LLMs
- en: 'Xijie Huang¹, Zechun Liu²^†^†thanks: All the work was done within HKUST and
    Zechun Liu served an advisory role., Shih-yang Liu¹, Kwang-Ting Cheng¹ ¹Hong Kong
    University of Science and Technology, ²Meta Reality Labs {xhuangbs,sliuau}@connect.ust.hk,
    zechunliu@meta.com,timcheng@ust.hk'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Xijie Huang¹, Zechun Liu²^†^†致谢：所有工作均在HKUST完成，Zechun Liu担任顾问角色。 Shih-yang Liu¹,
    Kwang-Ting Cheng¹ ¹香港科技大学，²Meta Reality Labs {xhuangbs,sliuau}@connect.ust.hk,
    zechunliu@meta.com, timcheng@ust.hk
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: While we have witnessed the success of Large Language Models (LLMs) such as
    GPT-4 Achiam et al. ([2023](#bib.bib1)) and LLaMA Touvron et al. ([2023](#bib.bib44))
    across various tasks in recent years, the massive model size and expanding training
    cost for LLMs have necessitated the design of model compression and Parameter-Efficient
    Fine-Tuning (PEFT) methods. Low-rank Adaption (LoRA) Hu et al. ([2021](#bib.bib21)),
    as the most favored PEFT method, significantly enhances the fine-tuning efficiency
    of LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在近年来见证了如GPT-4 Achiam等 ([2023](#bib.bib1)) 和LLaMA Touvron等 ([2023](#bib.bib44))等大型语言模型（LLMs）在各种任务中的成功，但LLMs的庞大模型规模和不断扩大的训练成本使得模型压缩和参数高效微调（PEFT）方法的设计成为必要。低秩自适应（LoRA）
    Hu等 ([2021](#bib.bib21)) 作为最受青睐的PEFT方法，显著提高了LLMs的微调效率。
- en: Recently, quantization techniques, which convert high-precision parameters into
    lower-bit formats such as INT4, have been integrated with LoRA methods Dettmers
    et al. ([2024](#bib.bib14)); Li et al. ([2024](#bib.bib27)); Xu et al. ([2024](#bib.bib47));
    Qin et al. ([2024](#bib.bib35)). Existing quantization-LoRA schemes can save memory
    costs during fine-tuning, and some schemes Li et al. ([2024](#bib.bib27)); Xu
    et al. ([2024](#bib.bib47)) can also reduce inference costs by producing quantized
    LLMs directly. However, these methods only perform weight-only quantization, while
    LoRA weight-activation quantization is under-explored. Quantizing both weights
    and activations in low-bit further saves run-time GPU memory and accelerates compute-intensive
    matrix-multiplication operations. We observe that 4-bit or 6-bit weight-activation
    quantization with LoRA finetuning still incurs a high accuracy degradation in
    LLMs, attributing to the outliers in weight and activation distribution, which
    stretch the quantization range and increase the quantization error.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，将高精度参数转换为较低位格式（如INT4）的量化技术已与LoRA方法集成 Dettmers et al. ([2024](#bib.bib14));
    Li et al. ([2024](#bib.bib27)); Xu et al. ([2024](#bib.bib47)); Qin et al. ([2024](#bib.bib35))。现有的量化-LoRA方案可以在微调过程中节省内存成本，一些方案 Li
    et al. ([2024](#bib.bib27)); Xu et al. ([2024](#bib.bib47)) 还可以通过直接生成量化LLMs来减少推理成本。然而，这些方法仅进行权重量化，而LoRA权重-激活量化尚未得到充分探索。对权重和激活进行低位量化进一步节省了运行时GPU内存，并加速了计算密集型矩阵乘法操作。我们观察到，尽管4位或6位权重-激活量化与LoRA微调相结合，但LLMs仍会出现较高的准确度下降，这归因于权重和激活分布中的异常值，这些异常值拉伸了量化范围并增加了量化误差。
- en: 'Existing methods in the post-training quantization research community have
    endeavored to tackle the outlier challenge by mixed-precision subgrouping Zhao
    et al. ([2024](#bib.bib50)); Chee et al. ([2024](#bib.bib9)) or shifting outliers
    from activation to weight Xiao et al. ([2023](#bib.bib46)); Shao et al. ([2024](#bib.bib40)).
    More recently, applying rotation Ashkboos et al. ([2024](#bib.bib5)); Liu et al.
    ([2024c](#bib.bib33)) to the weight matrices of LLMs has demonstrated effectiveness
    in eliminating activation outliers and keeping computational invariance Ashkboos
    et al. ([2023a](#bib.bib3)). However, all these methods solve the problems from
    a post-training perspective, ignoring that outliers will emerge and change distribution
    during pre-training and fine-tuning Bondarenko et al. ([2021](#bib.bib7)). In
    this work, we take a step further to utilize the rotation for outliers-removal
    in LoRA fine-tuning setting and investigate the optimal solution for dynamically
    integrating rotation with LoRA to preserve the outlier-free characteristics and
    improve weight-activation quantization. Motivated by this target, we propose Rotated
    outlier-free Low-Rank Adaptation (RoLoRA), which initially apply in-block and
    between-block rotation to the pre-trained LLMs, and then utilize rotation-aware
    fine-tuning to produce outlier-free fine-tuned LLMs as shown in Figure [1](#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for
    Effective Weight-Activation Quantization"). We explore the optimal rotation-aware
    fine-tuning scheme based on approximation error analysis.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '现有的后训练量化研究方法已尝试通过混合精度子分组来解决异常值挑战 Zhao et al. ([2024](#bib.bib50)); Chee et al.
    ([2024](#bib.bib9))，或将异常值从激活转移到权重 Xiao et al. ([2023](#bib.bib46)); Shao et al.
    ([2024](#bib.bib40))。最近，将旋转应用于LLMs的权重矩阵 Ashkboos et al. ([2024](#bib.bib5)); Liu
    et al. ([2024c](#bib.bib33)) 已证明在消除激活异常值和保持计算不变性方面有效 Ashkboos et al. ([2023a](#bib.bib3))。然而，所有这些方法都从后训练的角度解决问题，忽略了在预训练和微调过程中异常值会出现并改变分布 Bondarenko
    et al. ([2021](#bib.bib7))。在这项工作中，我们进一步利用旋转在LoRA微调设置中去除异常值，并研究将旋转与LoRA动态集成以保持无异常值特性和改进权重-激活量化的最佳解决方案。基于这一目标，我们提出了旋转异常值-free低秩适应（RoLoRA），该方法首先对预训练的LLMs应用块内和块间旋转，然后利用旋转感知微调生成无异常值的微调LLMs，如图 [1](#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for
    Effective Weight-Activation Quantization")所示。我们基于近似误差分析探索了最佳的旋转感知微调方案。'
- en: 'Extensive experimental results prove the effectiveness of RoLoRA across diverse
    LLMs, tasks, and quantization settings. RoLoRA improves the 4-bit quantization
    for weights and activations (W4A4) performance up to 14.6 points on the MMLU benchmark
    compared to LoRA. Compared with existing low-bit LoRA methods, RoLoRA outperforms
    previous SOTA IR-QLoRA Qin et al. ([2024](#bib.bib35)) with up to 6.0 points on
    the MMLU benchmark. The proposed RoLoRA is highly efficient with negligible fine-tuning
    overhead compared to LoRA in the same setting. RoLoRA can also improve the quantization
    robustness of Large Multimodal Models (LMMs) such as LLaVA Liu et al. ([2024a](#bib.bib30)),
    and we observe the multimodal understanding is largely retained even after W4A4
    quantization as shown in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ RoLoRA:
    Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '大量实验结果证明了RoLoRA在多样化的LLMs、任务和量化设置中的有效性。与LoRA相比，RoLoRA在MMLU基准测试中对权重和激活的4-bit量化（W4A4）性能提升了多达14.6分。与现有的低比特LoRA方法相比，RoLoRA在MMLU基准测试中超越了之前的SOTA
    IR-QLoRA Qin等（[2024](#bib.bib35)）多达6.0分。与LoRA相比，所提出的RoLoRA在相同设置下具有极高的效率，几乎没有微调开销。RoLoRA还可以提高大型多模态模型（LMMs）如LLaVA Liu等（[2024a](#bib.bib30)）的量化鲁棒性，我们观察到即使在W4A4量化后，多模态理解仍然大大保留，如表[1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for
    Effective Weight-Activation Quantization")所示。'
- en: 'In summary, our work contributes as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们的工作贡献如下：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose RoLoRA, the first method that explores the feasibility of integrating
    rotation in LoRA with quantization settings.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了RoLoRA，这是首个探索将旋转集成到LoRA中的可行性与量化设置的方法。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: RoLoRA enables robust weight-activation quantization of fine-tuned LLMs, especially
    in low-bit settings such as W4A4 and W6A6.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RoLoRA 实现了对微调过的LLMs的稳健权重-激活量化，特别是在W4A4和W6A6等低比特设置中。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We verify the effectiveness of RoLoRA on the LLaMA series (2-7B, 2-13B, 3-8B)
    across quantizers (RTN/GPTQ), bitwidth (W4A4/W6A6), and benchmarks (Zero-shot
    commonsense, MMLU). We also demonstrate that RoLoRA can be applied to LMMs.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们验证了RoLoRA在LLaMA系列（2-7B、2-13B、3-8B）上的有效性，包括量化器（RTN/GPTQ）、位宽（W4A4/W6A6）和基准测试（零-shot常识、MMLU）。我们还展示了RoLoRA可以应用于LMMs。
- en: | ![[Uncaptioned image]](img/f213fe2c0aa6c3c82e6dee4f1b787fe3.png)  |  |
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: | ![[未加说明的图片]](img/f213fe2c0aa6c3c82e6dee4f1b787fe3.png)  |  |
- en: '| Question: User: Explain the visual content of the image in great detail.
    (category: “detail”) |  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 问题：用户：详细解释图像的视觉内容。（类别：“详细”） |  |'
- en: '| GPT-4 Response: The image displays three open pizza boxes with pizzas inside.
    The first pizza box, situated to the left, contains a smaller pizza. The second
    pizza box, positioned in the middle, features a larger pizza topped with tortilla
    chips. The third pizza box, located to the right, contains another smaller pizza.
    Beside the large pizza, there are two side orders of sauce in bowls, accompanying
    the delicious meal. The arrangement showcases a delicious spread of pizzas, inviting
    everyone to enjoy their different flavors and sizes. |  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| GPT-4回复：图片展示了三个打开的披萨盒，里面有披萨。第一个披萨盒位于左侧，里面有一个较小的披萨。第二个披萨盒位于中间，里面有一个较大的披萨，上面撒有玉米片。第三个披萨盒位于右侧，里面有另一个较小的披萨。在大披萨旁边，有两个装在碗里的配餐酱料，伴随着美味的餐点。这一安排展示了一系列美味的披萨，邀请大家享受不同的口味和大小。
    |  |'
- en: '| LoRA$\xrightarrow{\text{GPTQ}}$W4A4: The image shows a pizza. |  |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| LoRA$\xrightarrow{\text{GPTQ}}$W4A4：图片展示了一块披萨。 |  |'
- en: '| RoLoRA$\xrightarrow{\text{GPTQ}}$W4A4: The image features a pizza with a
    variety of toppings, including cheese, pepperonies, and chicken. There are three
    pizzas in total, with one of them being a pizza with chicken. The pizzas are placed
    in a cardboard box, which is open and ready for the pizzas. The pizzas are served
    in a box, making it a convenient and portable meal option. |  |'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '| RoLoRA$\xrightarrow{\text{GPTQ}}$W4A4：该图像展示了一种多种配料的比萨，包括奶酪、意大利辣肠和鸡肉。总共有三种比萨，其中一种是鸡肉比萨。比萨被放在一个打开的纸板箱中，准备好放入比萨。比萨被装在一个盒子里，使其成为一个方便的便携餐选项。
    |  |'
- en: 'Table 1: LLaVA-1.5-7B W4A4 quantization evaluation on the example from LLaVA-Bench.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LLaVA-1.5-7B W4A4在LLaVA-Bench示例上的量化评估。
- en: 2 Related Work
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Quantization Quantization methods are powerful tools for improving training
    and inference efficiency. The core insight is replacing full-precision weights
    and activations with lower-precision representation. Most existing LLM quantization
    techniques fall in the category of post-training quantization (PTQ) (Liu et al.,
    [2023b](#bib.bib31); Frantar et al., [2023](#bib.bib16); Lin et al., [2024](#bib.bib28);
    Shang et al., [2024](#bib.bib39); Chee et al., [2024](#bib.bib9)) that directly
    quantize the model without extensive training. Among these LLM PTQ methods, most
    of them apply weight-only quantization while few methods explore weight-activation
    quantization Xiao et al. ([2023](#bib.bib46)); Shao et al. ([2024](#bib.bib40));
    Zhao et al. ([2024](#bib.bib50)); Ashkboos et al. ([2024](#bib.bib5)). Compared
    to the weight-only quantization, quantizing both weights and activations enables
    low-precision multiply-accumulation (MAC) units. The core challenge is that outliers
    in activations cause high quantization errors. This work focuses on the weight-activation
    quantization in the LoRA pipeline.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法是提高训练和推理效率的强大工具。核心见解是将全精度权重和激活替换为较低精度的表示。大多数现有的LLM量化技术属于后训练量化（PTQ） （Liu
    et al., [2023b](#bib.bib31)；Frantar et al., [2023](#bib.bib16)；Lin et al., [2024](#bib.bib28)；Shang
    et al., [2024](#bib.bib39)；Chee et al., [2024](#bib.bib9)），这些技术直接对模型进行量化而不需要大量的训练。在这些LLM
    PTQ方法中，大多数方法仅应用权重量化，而只有少数方法探索权重-激活量化 Xiao et al. ([2023](#bib.bib46))；Shao et
    al. ([2024](#bib.bib40))；Zhao et al. ([2024](#bib.bib50))；Ashkboos et al. ([2024](#bib.bib5))。与仅进行权重量化相比，同时量化权重和激活可以实现低精度乘加（MAC）单元。核心挑战是激活中的异常值会导致较高的量化误差。本工作集中在LoRA管道中的权重-激活量化上。
- en: LoRA Considering that full parameter fine-tuning becomes computationally impractical
    as the scale of LLM continues to grow, Parameter-Efficient Fine-Tuning (PEFT)
    methods Li and Liang ([2021](#bib.bib26)); Hu et al. ([2023](#bib.bib22)); Zhang
    et al. ([2023](#bib.bib49)) are designed to reduce the cost by training a relatively
    small subset of parameters. Low-Rank Adaptation (LoRA) Hu et al. ([2021](#bib.bib21))
    is the most adopted PEFT method, considering its flexibility and efficiency. More
    recently, LoRA variants Kopiczko et al. ([2024](#bib.bib24)); Liu et al. ([2024b](#bib.bib32));
    Hayou et al. ([2024](#bib.bib18)) emerged to improve the effectiveness and efficiency
    of LoRA. Combining LoRA and quantization Dettmers et al. ([2024](#bib.bib14))
    has also been a promising direction as quantization can further save the GPU memory
    in LoRA finetuning. To further reduce the information distortion of low-bit finetuning,
    various improvements of QLoRA have been proposed Xu et al. ([2024](#bib.bib47));
    Li et al. ([2024](#bib.bib27)); Qin et al. ([2024](#bib.bib35)). However, these
    methods only apply quantization to the weight during fine-tuning to reduce memory
    consumption. This work is the first quantized LoRA scheme that considers the robustness
    to weight-activation quantization.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 考虑到随着LLM规模的持续增长，完全参数微调变得计算上不切实际，参数高效微调（PEFT）方法 Li and Liang ([2021](#bib.bib26))；Hu
    et al. ([2023](#bib.bib22))；Zhang et al. ([2023](#bib.bib49)) 旨在通过训练相对较小的参数子集来降低成本。低秩自适应（LoRA） Hu
    et al. ([2021](#bib.bib21)) 是最常采用的PEFT方法，考虑到其灵活性和效率。最近，LoRA变体 Kopiczko et al.
    ([2024](#bib.bib24))；Liu et al. ([2024b](#bib.bib32))；Hayou et al. ([2024](#bib.bib18)) 出现以提高LoRA的效果和效率。将LoRA和量化结合 Dettmers
    et al. ([2024](#bib.bib14)) 也被认为是一个有前景的方向，因为量化可以进一步节省LoRA微调中的GPU内存。为了进一步减少低位微调的信息失真，已提出了各种QLoRA的改进 Xu
    et al. ([2024](#bib.bib47))；Li et al. ([2024](#bib.bib27))；Qin et al. ([2024](#bib.bib35))。然而，这些方法仅在微调过程中对权重进行量化以减少内存消耗。本工作是首个考虑权重-激活量化鲁棒性的量化LoRA方案。
- en: '![Refer to caption](img/527580942bd74c4ae748573bb7f7a5c1.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/527580942bd74c4ae748573bb7f7a5c1.png)'
- en: 'Figure 1: Activation distribution before and after rotation. The visualized
    input activations are selected from layers.1.self_attn.q_proj in LLaMA2-7B.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：旋转前后激活分布。可视化的输入激活来自LLaMA2-7B中的layers.1.self_attn.q_proj。
- en: '![Refer to caption](img/72ccaa02e52bed59ba611368cf904c90.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/72ccaa02e52bed59ba611368cf904c90.png)'
- en: 'Figure 2: Overview of the proposed Rotated outlier-free LoRA (RoLoRA)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：提出的旋转无离群值LoRA（RoLoRA）概述
- en: 3 Preliminary and Motivation
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 初步研究与动机
- en: 3.1 Low-Rank Adaptation (LoRA)
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 低秩适配（LoRA）
- en: 'For a pre-trained weight matrix $W_{0}\in\mathbb{R}^{d\times k}$ can be represented
    as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个预训练的权重矩阵 $W_{0}\in\mathbb{R}^{d\times k}$，可以表示为：
- en: '|  | $W^{\prime}=W_{0}+\Delta W=W_{0}+AB,$ |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{\prime}=W_{0}+\Delta W=W_{0}+AB,$ |  | （1）|'
- en: where $W_{0}$, LoRA and its related variants do not introduce any extra latency
    during the inference compared to the original model.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在推理过程中，$W_{0}$、LoRA及其相关变体与原始模型相比，不引入额外的延迟。
- en: 3.2 Outlier in Transformer
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 Transformer中的离群值
- en: 'Starting from small-scale transformer models such as BERT and ViT, researchers
    have revealed that outliers exist within the weight and activation distribution Huang
    et al. ([2023](#bib.bib23)); Wei et al. ([2022](#bib.bib45)). Their existence
    in LLMs is also observed in various studies. As shown in the left side of Figure. [1](#S2.F1
    "Figure 1 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for
    Effective Weight-Activation Quantization"), activation outliers are distributed
    per channel. While these outliers improve the representative capacity of the transformers Sun
    et al. ([2024](#bib.bib42)), they bring non-trivial challenges for quantization Xiao
    et al. ([2023](#bib.bib46)); Liu et al. ([2023b](#bib.bib31)).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '从小规模的Transformer模型如BERT和ViT开始，研究人员已经揭示出权重和激活分布中存在离群值 Huang et al. ([2023](#bib.bib23));
    Wei et al. ([2022](#bib.bib45))。在LLMs中的存在也在各种研究中被观察到。如图[1](#S2.F1 "Figure 1 ‣
    2 Related Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization")左侧所示，激活离群值是按通道分布的。这些离群值虽然提高了Transformer的表征能力 Sun et al. ([2024](#bib.bib42))，但也为量化带来了非小的挑战
    Xiao et al. ([2023](#bib.bib46)); Liu et al. ([2023b](#bib.bib31))。'
- en: 'Most previous solutions to this outlier problem in quantization can be categorized
    into three types: (1) isolating these outlier values in a sub-group with higher
    precision, such as LLM.int8 Dettmers et al. ([2022](#bib.bib13)), Atom Zhao et al.
    ([2024](#bib.bib50)), QuiK Ashkboos et al. ([2023b](#bib.bib4)), and AdaDim Heo
    et al. ([2024](#bib.bib20)). However, there is non-trivial overhead for the grouping
    and mixed-precision. (2) shifting the challenge of quantization from activations
    to weights, such as SmoothQuant Xiao et al. ([2023](#bib.bib46)) and OmniQuant Shao
    et al. ([2024](#bib.bib40)). However, these methods negatively influence the weight
    quantization robustness and fail at W4A4 scenarios. (3) rotating activation or
    weight matrices to remove outliers, such as QuaRot Ashkboos et al. ([2024](#bib.bib5))
    and SpinQuant Liu et al. ([2024c](#bib.bib33)). Among these methods, recent rotation-based
    solutions demonstrate superior effectiveness. However, previous rotation-based
    methods tackle the outlier challenge from a post-training perspective and have
    not been explored under PEFT settings.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 以往解决量化中离群值问题的方案大致可以分为三类：（1）将这些离群值隔离在一个具有更高精度的子组中，例如LLM.int8 Dettmers et al.
    ([2022](#bib.bib13))、Atom Zhao et al. ([2024](#bib.bib50))、QuiK Ashkboos et al.
    ([2023b](#bib.bib4)) 和 AdaDim Heo et al. ([2024](#bib.bib20))。然而，这种分组和混合精度方法会带来非小的开销。（2）将量化挑战从激活移至权重，例如SmoothQuant
    Xiao et al. ([2023](#bib.bib46)) 和 OmniQuant Shao et al. ([2024](#bib.bib40))。然而，这些方法会对权重量化的鲁棒性产生负面影响，并且在W4A4场景下表现不佳。（3）通过旋转激活或权重矩阵来去除离群值，例如QuaRot
    Ashkboos et al. ([2024](#bib.bib5)) 和 SpinQuant Liu et al. ([2024c](#bib.bib33))。在这些方法中，最近的基于旋转的解决方案表现出优越的效果。然而，之前的基于旋转的方法从训练后的角度处理离群值问题，在PEFT设置下尚未被探索。
- en: 'Thus, it leads to a question: Can we preserve the outlier-free characteristics
    of rotated LLMs and benefit from them during PEFT? We show in this work that we
    can achieve such a target and step further to investigate the most promising rotation-based
    fine-tuning solutions in this work.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这引出了一个问题：我们能否在PEFT过程中保持旋转LLMs的无离群值特性，并从中受益？我们在这项工作中展示了我们可以实现这一目标，并进一步研究了在这项工作中最有前景的基于旋转的微调解决方案。
- en: 3.3 Eliminating Outlier with Rotation
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 使用旋转消除离群值
- en: 'A rotation matrix $R$ efficiently. based on the Hadamard transform (also known
    as the Walsh–Hadamard transform Ritter ([1996](#bib.bib37)) as an example of a
    generalized class of Fourier transforms):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转矩阵$R$高效地基于Hadamard变换（也称为Walsh–Hadamard变换，Ritter ([1996](#bib.bib37)) 作为傅里叶变换的广义类的一个例子）：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $\otimes$ operations. Previous research Ashkboos et al. ([2023a](#bib.bib3))
    has revealed that applying rotation on the weights of pre-norm transformers can
    retain its computational consistency and further lead to fewer outliers in the
    weight and activation distribution Ashkboos et al. ([2024](#bib.bib5)); Liu et al.
    ([2024c](#bib.bib33)). Concretely, the multiplication of weight matrices with
    a rotation matrix statistically blends weights with large and small magnitudes
    together into a more Gaussian-like distribution, thus producing activations with
    fewer outliers and easier to quantize. The outlier elimination effect of rotation
    is also theoretically proved in Chee et al. ([2024](#bib.bib9)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\otimes$操作。先前的研究 Ashkboos等人 ([2023a](#bib.bib3)) 表明，在预归一化变换器的权重上应用旋转可以保持计算一致性，并进一步导致权重和激活分布中的异常值减少 Ashkboos等人
    ([2024](#bib.bib5)); 刘等人 ([2024c](#bib.bib33))。具体来说，将权重矩阵与旋转矩阵相乘，从统计上将大和小幅度的权重混合成更接近高斯分布的分布，从而产生异常值更少、量化更容易的激活。旋转的去除异常值效果在
    Chee 等人 ([2024](#bib.bib9)) 的理论研究中也得到了证明。
- en: 4 Method
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 方法
- en: Motivated by existing challenges of activation outliers and the success of rotation-based
    solutions Ashkboos et al. ([2024](#bib.bib5)); Liu et al. ([2024c](#bib.bib33)),
    we introduce Rotated outlier-free Low-Rank Adaptation (RoLoRA). RoLoRA initially
    apply in-block and between-block rotation to the pre-trained LLMs, and rotation-aware
    fine-tuning on the rotated LLMs will retain the optimal outlier-free characteristic,
    producing fine-tuned LLMs highly robust to weight-activation quantization.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 受现有激活异常值挑战和基于旋转的解决方案成功的启发 Ashkboos等人 ([2024](#bib.bib5)); 刘等人 ([2024c](#bib.bib33))，我们引入了旋转无异常值低秩适应（RoLoRA）。RoLoRA最初对预训练的LLMs应用块内和块间旋转，而对旋转后的LLMs进行旋转感知微调将保留最佳的无异常值特性，生成对权重激活量化高度鲁棒的微调LLMs。
- en: 4.1 Applying Rotation
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 应用旋转
- en: Before starting fine-tuning with rotation, we first modify the model to keep
    computational invariance before and after rotation. First, we need to ensure no
    scaling operation in the normalization module. For the LLaMA series, this can
    be implemented by absorbing the RMSNorm scale parameters $\alpha$ into the weight
    matrix right after the RMSNorm layer Elhage et al. ([2023](#bib.bib15)).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始旋转微调之前，我们首先修改模型以保持旋转前后的计算不变性。首先，我们需要确保归一化模块中没有缩放操作。对于LLaMA系列，这可以通过在RMSNorm层 Elhage
    等人 ([2023](#bib.bib15)) 之后将RMSNorm缩放参数$\alpha$吸收到权重矩阵中来实现。
- en: 'Then, we perform between-block rotation to make sure that the outliers in between-block
    activation are eliminated. As shown in Figure [2](#S2.F2 "Figure 2 ‣ 2 Related
    Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we classify the weight matrices in LLMs into two groups: left-side
    weights, including $W_{q},W_{k},W_{v}$ in feed-forward network modules. For the
    weights of these two groups, we adopt different rotation strategies with'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '然后，我们执行块间旋转，以确保消除块间激活中的异常值。如图[2](#S2.F2 "Figure 2 ‣ 2 Related Work ‣ RoLoRA:
    Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization")所示，我们将LLMs中的权重矩阵分为两组：左侧权重，包括前馈网络模块中的$W_{q},W_{k},W_{v}$。对于这两组权重，我们采用不同的旋转策略。'
- en: '|  | $W^{R}_{\text{left}}\leftarrow RW_{\text{left}},W^{R}_{\text{right}}\leftarrow
    W_{\text{right}}R^{-1},$ |  | (3) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $W^{R}_{\text{left}}\leftarrow RW_{\text{left}},W^{R}_{\text{right}}\leftarrow
    W_{\text{right}}R^{-1},$ |  | (3) |'
- en: where the rotation $R$, the final output of the model will be identical to the
    original model. To avoid overflow issues in the rotation process, we converted
    the FP16 weights to FP64 and converted them back after the multiplication. These
    rotations are applied before any training and inference, which indicates that
    there will be no overhead after the merging to original weights.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中旋转$R$，模型的最终输出将与原始模型相同。为了避免旋转过程中出现溢出问题，我们将FP16权重转换为FP64，并在乘法操作后将其转换回去。这些旋转在任何训练和推理之前应用，这表明在合并回原始权重后不会有额外的开销。
- en: 'The rotation that directly applies to weights effectively reduces the outlier
    in between-block activation, and we refer to the operation as Between Block Rotation
    (BBR). Figure. [1](#S2.F1 "Figure 1 ‣ 2 Related Work ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization") demonstrates
    the effect of applying BBR as the activation distribution is smoother and de-centralized.
    However, another challenge remains that the activation in these modules still
    suffers from outliers, especially prevalent in FFN as discussed in previous research Bondarenko
    et al. ([2024](#bib.bib8)). We cannot directly apply rotation similar to BBR because
    of the non-linear operations such as SwiGLU Shazeer ([2020](#bib.bib41)) in FFN.
    To solve this, we adopt the online rotation node before inputting the activation
    input to $W_{down}$. This online rotation is implemented following the fast Hadamard
    kernel Chee et al. ([2024](#bib.bib9)); Ashkboos et al. ([2024](#bib.bib5)), which
    can be seen as a layer dynamically rotating the activation. This online rotation
    operation is highly efficient, and the overhead is negligible during training
    and inference. It is referred to as in-block rotation (IBR). Note that IBR can
    also be applied to the self-attention module, but we observe in the experiments
    of Table [6](#S5.T6 "Table 6 ‣ 5.4 Ablation Study and Analysis ‣ 5 Experiments
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") that there is no performance improvement with this rotation.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '直接应用于权重的旋转有效地减少了块间激活中的异常值，我们将此操作称为块间旋转 (BBR)。图 [1](#S2.F1 "Figure 1 ‣ 2 Related
    Work ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") 演示了应用 BBR 的效果，因为激活分布更平滑且去中心化。然而，另一个挑战是这些模块中的激活仍然存在异常值，尤其是在 FFN
    中，正如之前的研究 Bondarenko 等人 ([2024](#bib.bib8)) 所讨论的。由于 FFN 中的非线性操作如 SwiGLU Shazeer
    ([2020](#bib.bib41))，我们不能直接应用类似于 BBR 的旋转。为了解决这个问题，我们在将激活输入到 $W_{down}$ 之前采用了在线旋转节点。这种在线旋转是按照快速
    Hadamard 核 Chee 等人 ([2024](#bib.bib9)); Ashkboos 等人 ([2024](#bib.bib5)) 实现的，可以看作是一个动态旋转激活的层。这种在线旋转操作非常高效，训练和推理过程中开销几乎可以忽略不计。它被称为块内旋转
    (IBR)。注意，IBR 也可以应用于自注意力模块，但我们在表 [6](#S5.T6 "Table 6 ‣ 5.4 Ablation Study and Analysis
    ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization") 的实验中观察到这种旋转没有提升性能。'
- en: 4.2 Rotation-aware Fine-tuning
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 旋转感知微调
- en: After performing both BBR and IBR, the between-block and in-block activation
    outliers are eliminated. This characteristic can lower the quantization error
    during QLoRA training, enabling a more accurate gradient estimation and smoother
    optimization for fine-tuning. However, existing research Bondarenko et al. ([2021](#bib.bib7));
    Kovaleva et al. ([2021](#bib.bib25)) revealed that outliers will change distribution
    or emerge during fine-tuning and pre-training. This poses a new challenge of dynamically
    integrating rotation into LoRA to effectively maintain outlier-free characteristics.
    To design the optimal rotation-aware fine-tuning scheme, we first analyze the
    approximation difficulty when rotation is applied. We assume that the optimal
    weight distribution for specific downstream tasks is $W^{*}$. The optimization
    of LoRA fine-tuning could be indicated as
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行 BBR 和 IBR 后，块间和块内激活中的异常值被消除。这一特性可以降低 QLoRA 训练过程中的量化误差，实现更准确的梯度估计和更平滑的微调优化。然而，现有研究
    Bondarenko 等人 ([2021](#bib.bib7)); Kovaleva 等人 ([2021](#bib.bib25)) 揭示了异常值在微调和预训练过程中会改变分布或出现。这带来了一个新的挑战，即如何将旋转动态地整合到
    LoRA 中，以有效保持无异常值的特性。为了设计最优的旋转感知微调方案，我们首先分析了应用旋转时的近似难度。我们假设针对特定下游任务的最优权重分布为 $W^{*}$。LoRA
    微调的优化可以表示为
- en: '|  | $\displaystyle\underset{A,B}{\min}\&#124;W^{*}-(W_{0}+AB)\&#124;_{F},$
    |  | (4) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{A,B}{\min}\&#124;W^{*}-(W_{0}+AB)\&#124;_{F},$
    |  | (4) |'
- en: 'where the $\|\cdot\|_{F}$ denotes the Frobenious norm. To insert the LoRA module
    in the rotated models, we propose two rotation-aware fine-tuning schemes, namely
    LoRA After Rotation (LAR) and LoRA Before Rotation (LBR), as shown in Figure [3](#S4.F3
    "Figure 3 ‣ 4.2 Rotation-aware Fine-tuning ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\|\cdot\|_{F}$ 表示 Frobenius 范数。为了在旋转模型中插入 LoRA 模块，我们提出了两种旋转感知微调方案，即旋转后
    LoRA (LAR) 和旋转前 LoRA (LBR)，如图 [3](#S4.F3 "Figure 3 ‣ 4.2 Rotation-aware Fine-tuning
    ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") 所示。'
- en: '![Refer to caption](img/ab0b3bc1156acab3135a01bd1cce216b.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/ab0b3bc1156acab3135a01bd1cce216b.png)'
- en: 'Figure 3: Two schemes for performing rotation-aware fine-tuning: (a) LAR and
    (b) LBR.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：两种旋转感知微调方案：(a) LAR 和 (b) LBR。
- en: 'In LAR, we first merge the rotation matrix with pre-trained weights and then
    use $R_{1}W_{0}+AB$, and the optimization for these two schemes becomes:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在 LAR 中，我们首先将旋转矩阵与预训练权重合并，然后使用 $R_{1}W_{0}+AB$，这两种方案的优化变为：
- en: '|  | LAR: | $\displaystyle\underset{A,B}{\min}\&#124;AB-O_{\text{LAR}}\&#124;_{F},O_{\text{LAR}}=W_{FT}-R_{1}W_{0}$
    |  | (5) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | LAR: | $\displaystyle\underset{A,B}{\min}\&#124;AB-O_{\text{LAR}}\&#124;_{F},O_{\text{LAR}}=W_{FT}-R_{1}W_{0}$
    |  | (5) |'
- en: '|  | LBR: | $\displaystyle\underset{A,B}{\min}\&#124;AB-O_{\text{LBR}}\&#124;_{F},O_{\text{LBR}}=R_{1}^{-1}W_{FT}-W_{0}$
    |  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | LBR: | $\displaystyle\underset{A,B}{\min}\&#124;AB-O_{\text{LBR}}\&#124;_{F},O_{\text{LBR}}=R_{1}^{-1}W_{FT}-W_{0}$
    |  |'
- en: 'the final optimization is very different. We apply SVD of the approximation
    target $O_{\text{LAR}},O_{\text{LBR}}\in\mathbb{R}^{d\times k}$:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的优化方案非常不同。我们应用了近似目标 $O_{\text{LAR}},O_{\text{LBR}}\in\mathbb{R}^{d\times k}$
    的 SVD：
- en: '|  | $A=U_{[:,:r]}\,S_{[:r,:r]}^{1/2}\in\mathbb{R}^{d\times r},B=S_{[:r,:r]}^{1/2}\,V_{[:,:r]}^{T}\in\mathbb{R}^{r\times
    k}.$ |  | (6) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $A=U_{[:,:r]}\,S_{[:r,:r]}^{1/2}\in\mathbb{R}^{d\times r},B=S_{[:r,:r]}^{1/2}\,V_{[:,:r]}^{T}\in\mathbb{R}^{r\times
    k}.$ |  | (6) |'
- en: 'We verify the approximation error of different rank choices $r$ for the experiments.
    which is shown in Figure. [4](#S4.F4 "Figure 4 ‣ 4.2 Rotation-aware Fine-tuning
    ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"). Based on the results, LAR outperforms LBR in low-rank settings
    with lower approximation error, suggesting LAR is the better design for rotation-aware
    fine-tuning. The better approximation indicates that after the two-stage merging
    with rotation matrices and LoRA weights, the final weights can still retain the
    outlier-free property, which is further validated by ablation experiments in Section [5.4](#S5.SS4
    "5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization").'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '我们验证了不同秩选择 $r$ 的近似误差，结果如图所示。[4](#S4.F4 "图 4 ‣ 4.2 旋转感知微调 ‣ 4 方法 ‣ RoLoRA: 微调旋转无离群点的
    LLM 以实现有效的权重激活量化")。根据结果，LAR 在低秩设置下的近似误差低于 LBR，表明 LAR 是旋转感知微调的更佳设计。更好的近似表明，在经过两阶段的旋转矩阵和
    LoRA 权重合并后，最终的权重仍能保留无离群点的特性，这一点在第 [5.4](#S5.SS4 "5.4 消融研究与分析 ‣ 5 实验 ‣ RoLoRA:
    微调旋转无离群点的 LLM 以实现有效的权重激活量化") 节中通过消融实验进一步验证。'
- en: '![Refer to caption](img/5a96802b6da7a85ea2770cc742757c51.png)![Refer to caption](img/08eb5bf4bc0be31201b53330784887ce.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/5a96802b6da7a85ea2770cc742757c51.png)![参见图例](img/08eb5bf4bc0be31201b53330784887ce.png)'
- en: 'Figure 4: SVD approximation error of optimization targets with different LoRA-rotation
    integration schemes.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：不同 LoRA-旋转集成方案的优化目标的 SVD 近似误差。
- en: 'As a result of the optimal rotation-aware fine-tuning scheme under the LAR
    setting, we can effectively retain the outlier-free characteristic during LLM
    fine-tuning, as shown in Figure [5](#S5.F5 "Figure 5 ‣ 5.3 Visual Instruction
    Tuning ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization").'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '作为 LAR 设置下最佳旋转感知微调方案的结果，我们可以有效地在 LLM 微调过程中保留无离群点的特性，如图 [5](#S5.F5 "图 5 ‣ 5.3
    视觉指令微调 ‣ 5 实验 ‣ RoLoRA: 微调旋转无离群点的 LLM 以实现有效的权重激活量化") 所示。'
- en: 5 Experiments
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'Table 2: Comparison of the averaged accuracy on seven Zero-shot Common Sense
    Reasoning (ZCSR) tasks and MMLU benchmark across LLaMA series. The detailed accuracy
    for each tasks are listed in Table [9](#A1.T9 "Table 9 ‣ Appendix A Detailed Evaluation
    Results ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") and Table [10](#A1.T10 "Table 10 ‣ Appendix A Detailed Evaluation
    Results ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2：LLaMA 系列在七个零样本常识推理 (ZCSR) 任务和 MMLU 基准上的平均准确率比较。每个任务的详细准确率列在表 [9](#A1.T9
    "表 9 ‣ 附录 A 详细评估结果 ‣ RoLoRA: 微调旋转无离群点的 LLM 以实现有效的权重激活量化") 和表 [10](#A1.T10 "表 10
    ‣ 附录 A 详细评估结果 ‣ RoLoRA: 微调旋转无离群点的 LLM 以实现有效的权重激活量化") 中。'
- en: '| #Bits | Quantizer | Method | LLaMA-2 7B | LLaMA-2 13B | LLaMA-3 8B |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | 量化器 | 方法 | LLaMA-2 7B | LLaMA-2 13B | LLaMA-3 8B |'
- en: '| ZCSR⁷ Avg. | MMLU⁴ Avg. | ZCSR⁷ Avg. | MMLU⁴ Avg. | ZCSR⁷ Avg. | MMLU⁴ Avg.
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ZCSR⁷ 平均 | MMLU⁴ 平均 | ZCSR⁷ 平均 | MMLU⁴ 平均 | ZCSR⁷ 平均 | MMLU⁴ 平均 |'
- en: '| FP16 | - | LoRA | 68.4 | 43.5 | 70.5 | 52.4 | 70.0 | 62.7 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 68.4 | 43.5 | 70.5 | 52.4 | 70.0 | 62.7 |'
- en: '| W4A4 | RTN | LoRA | 35.8 | 23.5 | 34.4 | 24.2 | 36.7 | 23.3 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 35.8 | 23.5 | 34.4 | 24.2 | 36.7 | 23.3 |'
- en: '| RoLoRA | 54.1 ($\uparrow$8.8) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 54.1 ($\uparrow$8.8) |'
- en: '| GPTQ | LoRA | 37.0 | 23.5 | 34.4 | 24.4 | 36.6 | 23.9 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 37.0 | 23.5 | 34.4 | 24.4 | 36.6 | 23.9 |'
- en: '| RoLoRA | 62.3 ($\uparrow$14.6) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 62.3 ($\uparrow$14.6) |'
- en: '| W6A6 | RTN | LoRA | 65.3 | 35.9 | 67.3 | 47.3 | 67.7 | 55.3 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 65.3 | 35.9 | 67.3 | 47.3 | 67.7 | 55.3 |'
- en: '| RoLoRA | 66.8 ($\uparrow$4.1) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 66.8 ($\uparrow$4.1) |'
- en: '| GPTQ | LoRA | 65.5 | 35.7 | 68.0 | 47.6 | 67.8 | 54.3 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 65.5 | 35.7 | 68.0 | 47.6 | 67.8 | 54.3 |'
- en: '| RoLoRA | 67.1 ($\uparrow$5.1) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 67.1 ($\uparrow$5.1) |'
- en: 5.1 Settings
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 设置
- en: Model, LoRA, Quantizer The models for our experiments include LLaMA2-7B/13B Touvron
    et al. ([2023](#bib.bib44)) and LLaMA3-8B AI@Meta ([2024](#bib.bib2)). We follow
    the settings in LLaMA-Factory Zheng et al. ([2024](#bib.bib51)) to implement the
    training pipeline. The dataset for fine-tuning is Alpaca Taori et al. ([2023](#bib.bib43))
    with 52K samples. The weight PTQ methods are the baseline Round-To-Nearest (RTN)
    and widely used GPTQ Frantar et al. ([2023](#bib.bib16)), and the activation quantizer
    is RTN across all experiments.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模型、LoRA、量化器 我们实验的模型包括 LLaMA2-7B/13B Touvron 等人 ([2023](#bib.bib44)) 和 LLaMA3-8B
    AI@Meta ([2024](#bib.bib2))。我们按照 LLaMA-Factory Zheng 等人 ([2024](#bib.bib51)) 的设置实施训练流程。微调的数据集是
    Alpaca Taori 等人 ([2023](#bib.bib43))，共有 52K 样本。权重量化（PTQ）方法包括基准 Round-To-Nearest（RTN）和广泛使用的
    GPTQ Frantar 等人 ([2023](#bib.bib16))，激活量化器在所有实验中均为 RTN。
- en: Tasks Our RoLoRA was verified on seven zero-shot commonsense reasoning tasks
    using EleutherAI evaluation harness Gao et al. ([2021](#bib.bib17)). These tasks
    include BoolQ (Clark et al., [2019](#bib.bib11)), PIQA (Bisk et al., [2020](#bib.bib6)),
    HellaSwag (Zellers et al., [2019](#bib.bib48)), WinoGrande (Sakaguchi et al.,
    [2021](#bib.bib38)), ARC-easy and ARC-challenge (Clark et al., [2018](#bib.bib12)),
    and OBQA (Mihaylov et al., [2018](#bib.bib34)). Additionally, we also report the
    accuracy of Massively Multitask Language Understanding (MMLU) benchmark Hendrycks
    et al. ([2020](#bib.bib19)) for our evaluation.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 任务 我们的 RoLoRA 在七个零-shot 常识推理任务上进行了验证，这些任务使用了 EleutherAI 评估工具包 Gao 等人 ([2021](#bib.bib17))。这些任务包括
    BoolQ (Clark 等人，[2019](#bib.bib11))、PIQA (Bisk 等人，[2020](#bib.bib6))、HellaSwag
    (Zellers 等人，[2019](#bib.bib48))、WinoGrande (Sakaguchi 等人，[2021](#bib.bib38))、ARC-easy
    和 ARC-challenge (Clark 等人，[2018](#bib.bib12)) 和 OBQA (Mihaylov 等人，[2018](#bib.bib34))。此外，我们还报告了
    Massively Multitask Language Understanding (MMLU) 基准 Hendrycks 等人 ([2020](#bib.bib19))
    的准确率。
- en: Baselines We consider two settings for experiments. The first is conducting
    FP16 fine-tuning with RoLoRA, where we compare the W4A4 and W6A6 quantization
    results with LoRA. The second is conducting RoLoRA fine-tuning with 4-bit weight
    quantization, which we refer to as QRoLoRA, and comparing the W4A4 performance
    with other low-bit LoRA methods including QLoRA Dettmers et al. ([2024](#bib.bib14)),
    LoftQ Li et al. ([2024](#bib.bib27)), and IR-LoRA Qin et al. ([2024](#bib.bib35)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 基准 我们考虑了两种实验设置。第一种是在 RoLoRA 上进行 FP16 微调，在此过程中我们比较了 W4A4 和 W6A6 的量化结果与 LoRA 的结果。第二种是在
    4 位权重量化下进行 RoLoRA 微调，我们称之为 QRoLoRA，并与其他低比特 LoRA 方法（包括 QLoRA Dettmers 等人 ([2024](#bib.bib14))、LoftQ
    Li 等人 ([2024](#bib.bib27)) 和 IR-LoRA Qin 等人 ([2024](#bib.bib35))）比较 W4A4 的性能。
- en: 5.2 Main Results
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 主要结果
- en: 'We first evaluate RoLoRA against LoRA in FP16 fine-tuning and then apply weight-activation
    PTQ to the fine-tuned LLMs. To ensure a fair comparison, both RoLoRA and LoRA
    use the same settings (rank, epoch, learning rate, etc.). As listed in Table [2](#S5.T2
    "Table 2 ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization"), RoLoRA enhances the quantization robustness
    of the LLaMA series across various quantization settings on zero-shot commonsense
    reasoning and MMLU benchmarks. Specifically for the W4A4 low-bit setting, RoLoRA
    outperforms LoRA with an absolute up to 29.5% and 14.6% on ZCSR and MMLU, respectively.
    Furthermore, RoLoRA makes it feasible for near-lossless W6A6 quantization of the
    LLaMa series.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在 FP16 微调中评估 RoLoRA 与 LoRA，然后将权重量化（PTQ）应用于微调后的 LLM。为了确保公平比较，RoLoRA 和 LoRA
    使用相同的设置（秩、周期、学习率等）。如表[2](#S5.T2 "Table 2 ‣ 5 Experiments ‣ RoLoRA: Fine-tuning
    Rotated Outlier-free LLMs for Effective Weight-Activation Quantization")所示，RoLoRA
    在零-shot 常识推理和 MMLU 基准测试中的各种量化设置下，增强了 LLaMA 系列的量化鲁棒性。特别是在 W4A4 低比特设置下，RoLoRA 在
    ZCSR 和 MMLU 上分别超越 LoRA 最高达 29.5% 和 14.6%。此外，RoLoRA 使得对 LLaMa 系列进行接近无损的 W6A6 量化成为可能。'
- en: 'Table 3: Comparison of the averaged accuracy of different Low-bit LoRA methods
    on Zero-shot Common Sense Reasoning tasks and MMLU benchmark on LLaMA2-7B.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：不同低比特 LoRA 方法在零-shot 常识推理任务和 LLaMA2-7B 上 MMLU 基准的平均准确率比较。
- en: '| #Bits | Quantizer | Method | BoolQ | PIQA | HellaS. | WinoG. | Arc-e | Arc-c
    | OBQA | Avg. | Hums. | STEM | Social | Other | Avg. |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | 量化器 | 方法 | BoolQ | PIQA | HellaS. | WinoG. | Arc-e | Arc-c | OBQA
    | 平均 | Hums. | STEM | 社会 | 其他 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- | --- |'
- en: '|  | RTN | QLoRA Dettmers et al. ([2024](#bib.bib14)) | 47.1 | 51.5 | 27.5
    | 49.1 | 28.4 | 24.6 | 25.4 | 36.2 | 24.1 | 24.7 | 22.9 | 21.8 | 23.5 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | RTN | QLoRA 德特梅尔斯等人 ([2024](#bib.bib14)) | 47.1 | 51.5 | 27.5 | 49.1 |
    28.4 | 24.6 | 25.4 | 36.2 | 24.1 | 24.7 | 22.9 | 21.8 | 23.5 |'
- en: '|  | LoftQ Li et al. ([2024](#bib.bib27)) | 51.5 | 50.8 | 26.6 | 50.4 | 27.5
    | 26.0 | 25.0 | 36.8 | 23.9 | 24.0 | 22.2 | 22.2 | 23.2 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | LoftQ 李等人 ([2024](#bib.bib27)) | 51.5 | 50.8 | 26.6 | 50.4 | 27.5 | 26.0
    | 25.0 | 36.8 | 23.9 | 24.0 | 22.2 | 22.2 | 23.2 |'
- en: '| W4A16 | IR-QLoRA Qin et al. ([2024](#bib.bib35)) | 45.5 | 49.7 | 26.7 | 50.6
    | 25.7 | 26.8 | 26.8 | 36.0 | 24.3 | 24.6 | 23.9 | 21.9 | 23.7 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| W4A16 | IR-QLoRA 秦等人 ([2024](#bib.bib35)) | 45.5 | 49.7 | 26.7 | 50.6 | 25.7
    | 26.8 | 26.8 | 36.0 | 24.3 | 24.6 | 23.9 | 21.9 | 23.7 |'
- en: '| $\downarrow$0.8) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| $\downarrow$0.8) |'
- en: '| W4A4 | GPTQ | QLoRA Dettmers et al. ([2024](#bib.bib14)) | 51.4 | 51.6 |
    27.7 | 51.9 | 29.6 | 25.3 | 26.4 | 37.7 | 24.9 | 24.0 | 22.2 | 22.5 | 23.6 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | GPTQ | QLoRA 德特梅尔斯等人 ([2024](#bib.bib14)) | 51.4 | 51.6 | 27.7 | 51.9
    | 29.6 | 25.3 | 26.4 | 37.7 | 24.9 | 24.0 | 22.2 | 22.5 | 23.6 |'
- en: '| LoftQ Li et al. ([2024](#bib.bib27)) | 55.9 | 49.2 | 27.2 | 49.1 | 26.6 |
    26.1 | 24 | 36.9 | 24.1 | 23.8 | 23.3 | 22.7 | 23.6 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| LoftQ 李等人 ([2024](#bib.bib27)) | 55.9 | 49.2 | 27.2 | 49.1 | 26.6 | 26.1
    | 24 | 36.9 | 24.1 | 23.8 | 23.3 | 22.7 | 23.6 |'
- en: '|  | IR-QLoRA Qin et al. ([2024](#bib.bib35)) | 51.1 | 49.8 | 27.6 | 49.3 |
    27.6 | 24.6 | 27.4 | 36.8 | 24.6 | 24.8 | 22.9 | 22.7 | 23.9 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | IR-QLoRA 秦等人 ([2024](#bib.bib35)) | 51.1 | 49.8 | 27.6 | 49.3 | 27.6 |
    24.6 | 27.4 | 36.8 | 24.6 | 24.8 | 22.9 | 22.7 | 23.9 |'
- en: '|  | RoLoRA | 68.7 | 73.1 | 66.8 | 61.3 | 61.2 | 37.8 | 38.2 | 58.2 ($\uparrow$6.0)
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | RoLoRA | 68.7 | 73.1 | 66.8 | 61.3 | 61.2 | 37.8 | 38.2 | 58.2 ($\uparrow$6.0)
    |'
- en: 'We further evaluate RoLoRA against QLoRA Dettmers et al. ([2024](#bib.bib14))
    and serval baseline methods, including LoftQ Li et al. ([2024](#bib.bib27)), IR-QLoRA Qin
    et al. ([2024](#bib.bib35)), on 4-bit fine-tuning and then apply W4A4 PTQ to the
    low-bit fine-tuned LLaMA2-7B. The performance across seven commonsense reasoning
    tasks and four MMLU subtasks is detailed in Table [3](#S5.T3 "Table 3 ‣ 5.2 Main
    Results ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization"). We can see that RoLoRA consistently improves
    the performance of the quantized model using the same quantizer. In particular,
    for W4A4 GPTQ, RoLoRA exceeds QLoRA by 20.5% on the average accuracy of commonsense
    reasoning tasks. Across the experiments on both FP16 and 4-bit fine-tuning, we
    observe that RoLoRA achieves higher performance improvement on the LLMs quantized
    by GPTQ Frantar et al. ([2023](#bib.bib16)) in general. This observation supports
    our claim that RoLoRA retains the outlier-free activation in fine-tuning as GPTQ
    only helps lower the quantization error of weights but not for activation.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步评估了 RoLoRA 相对于 QLoRA 德特梅尔斯等人 ([2024](#bib.bib14)) 和若干基线方法，包括 LoftQ 李等人
    ([2024](#bib.bib27))、IR-QLoRA 秦等人 ([2024](#bib.bib35))，在 4 位精细调整中，然后将 W4A4 PTQ
    应用于低位精细调整的 LLaMA2-7B。七个常识推理任务和四个 MMLU 子任务的性能详细见表 [3](#S5.T3 "表 3 ‣ 5.2 主要结果 ‣
    5 实验 ‣ RoLoRA：微调旋转无异常 LLMs 以有效权重-激活量化")。我们可以看到，RoLoRA 一致提高了使用相同量化器的量化模型的性能。特别是，对于
    W4A4 GPTQ，RoLoRA 在常识推理任务的平均准确率上超越了 QLoRA 20.5%。在 FP16 和 4 位精细调整的实验中，我们观察到 RoLoRA
    在 GPTQ Frantar 等人 ([2023](#bib.bib16)) 量化的 LLMs 上取得了更高的性能提升。这一观察支持了我们的主张，即 RoLoRA
    在微调中保留了无异常激活，而 GPTQ 仅有助于降低权重的量化误差，而不能改善激活的量化误差。
- en: 5.3 Visual Instruction Tuning
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 视觉指令微调
- en: 'We further verify the effectiveness of RoLoRA on visual instruction tuning
    tasks with LLaVA-1.5-7B Liu et al. ([2023a](#bib.bib29)), which consists of a
    language model, Vicuna-7B Chiang et al. ([2023](#bib.bib10)), and a vision encoder
    CLIP ViT-L-336px Radford et al. ([2021](#bib.bib36)). We finetune the LLaVA-1.5-7B
    on LLaVA-Instruct-150K²²2https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K.
    We only perform quantization on the language model and evaluate the LLaVA with
    quantized Vicuna and full-precision vision encoder on LLaVA-bench (COCO) Liu et al.
    ([2024a](#bib.bib30)) with GPT-4 Achiam et al. ([2023](#bib.bib1)). The relative
    score across the conversation, detail description, and complex reasoning are reported
    in Table. [4](#S5.T4 "Table 4 ‣ 5.3 Visual Instruction Tuning ‣ 5 Experiments
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), where we can observe from the results that RoLoRA help improve
    the quantization robustness and keep the multi-modal ability during PTQ to the
    better extent with an increase up to 18.9 overall scores. We also provide an example
    of the detail description task on a given image shown in Table. [1](#S1.T1 "Table
    1 ‣ 1 Introduction ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization"). While the W4A4 LoRA model only gives a rough
    superficial description of the images, our W4A4 RoLoRA model fully elaborates
    the details, such as the toppings and containers.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步验证了 RoLoRA 在 LLaVA-1.5-7B Liu et al. ([2023a](#bib.bib29)) 的视觉指令调优任务中的有效性，该模型包括一个语言模型
    Vicuna-7B Chiang et al. ([2023](#bib.bib10)) 和一个视觉编码器 CLIP ViT-L-336px Radford
    et al. ([2021](#bib.bib36))。我们在 LLaVA-Instruct-150K²²2https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K
    上对 LLaVA-1.5-7B 进行了微调。我们仅对语言模型进行量化，并在 LLaVA-bench (COCO) Liu et al. ([2024a](#bib.bib30))
    上评估了量化后的 Vicuna 和全精度视觉编码器，使用 GPT-4 Achiam et al. ([2023](#bib.bib1))。表格 [4](#S5.T4
    "Table 4 ‣ 5.3 Visual Instruction Tuning ‣ 5 Experiments ‣ RoLoRA: Fine-tuning
    Rotated Outlier-free LLMs for Effective Weight-Activation Quantization") 中报告了对话、详细描述和复杂推理的相对评分，我们可以从结果中观察到
    RoLoRA 有助于提高量化鲁棒性，并在 PTQ 过程中保持更好的多模态能力，整体分数提高了 18.9。我们还提供了表格 [1](#S1.T1 "Table
    1 ‣ 1 Introduction ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization") 中给定图像的详细描述任务示例。虽然 W4A4 LoRA 模型仅提供了图像的粗略描述，但我们的
    W4A4 RoLoRA 模型则充分阐述了细节，例如配料和容器。'
- en: 'Table 4: Comparison of the W4A4 quantization performance on LLaVA-Bench of
    LLaVA-1.5-7B.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：LLaVA-1.5-7B 在 LLaVA-Bench 上的 W4A4 量化性能比较。
- en: '| #Bits | Quantizer | Method | Conv. | Detail | Reas. | Overall |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | 量化器 | 方法 | 转换 | 详细 | 理由 | 总体 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| W4A4 | RTN | LoRA | 43.2 | 29.6 | 31.6 | 34.9 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 43.2 | 29.6 | 31.6 | 34.9 |'
- en: '| RoLoRA | 68.8 | 40.5 | 51.9 | 53.8 ($\uparrow$18.9) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 68.8 | 40.5 | 51.9 | 53.8 ($\uparrow$18.9) |'
- en: '| GPTQ | LoRA | 70.6 | 41.8 | 47.9 | 53.5 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 70.6 | 41.8 | 47.9 | 53.5 |'
- en: '| RoLoRA | 67.5 | 48.3 | 66.2 | 60.8 ($\uparrow$7.3) | ![Refer to caption](img/d1555b770ca3b2cd3857fd6df3a43ead.png)![Refer
    to caption](img/50cc5ac8683797f4d364ffc0ce763737.png)![Refer to caption](img/60d0898aef6d4eebddbb4cec896e3558.png)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '| RoLoRA | 67.5 | 48.3 | 66.2 | 60.8 ($\uparrow$7.3) | ![参考说明](img/d1555b770ca3b2cd3857fd6df3a43ead.png)![参考说明](img/50cc5ac8683797f4d364ffc0ce763737.png)![参考说明](img/60d0898aef6d4eebddbb4cec896e3558.png)'
- en: 'Figure 5: Left: The training dynamics of the average Kurtosis of activations,
    Middle: The distribution of Kurtosis of activations across all layers in the final
    model after fine-tuning with LoRA and RoLoRA, Right: The accumulative quantization
    error of W4A4 GPTQ across all layers in the final model after fine-tuning with
    LoRA and RoLoRA.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：左：激活的平均峰度的训练动态，中：微调后最终模型中所有层的激活峰度分布，右：微调后最终模型中所有层的 W4A4 GPTQ 累积量化误差。
- en: 5.4 Ablation Study and Analysis
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融研究与分析
- en: 'When to Apply Rotation? Different from the Rotation-Aware Fine-tuning (RAF)
    scheme that rotates the LLMs before LoRA fine-tuning, we can also directly apply
    rotation on an already-finetuned LoRA model. This possible paradigm of LoRA$\rightarrow$PTQ
    is referred to as post-training rotation. We evaluate post-training rotation using
    the same training setting as RoLoRA across the LLaMA series. The W4A4 GPTQ performance
    on seven zero-shot commonsense reasoning tasks are listed in Table [5](#S5.T5
    "Table 5 ‣ 5.4 Ablation Study and Analysis ‣ 5 Experiments ‣ RoLoRA: Fine-tuning
    Rotated Outlier-free LLMs for Effective Weight-Activation Quantization"). The
    results indicate that applying rotation before LoRA can consistently enhance the
    quantization robustness of the fine-tuned LLMs.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 何时应用旋转？不同于在 LoRA 微调前旋转 LLM 的旋转感知微调（RAF）方案，我们也可以直接对已微调的 LoRA 模型应用旋转。这种可能的 LoRA$\rightarrow$PTQ
    范式被称为训练后旋转。我们使用与 RoLoRA 相同的训练设置评估训练后旋转，涵盖 LLaMA 系列。表 [5](#S5.T5 "表 5 ‣ 5.4 消融研究与分析
    ‣ 5 实验 ‣ RoLoRA：微调旋转异常点自由的 LLM 以实现有效的权重-激活量化") 中列出了七个零-shot 常识推理任务上的 W4A4 GPTQ
    性能。结果表明，在 LoRA 前应用旋转可以持续增强微调 LLM 的量化鲁棒性。
- en: 'Table 5: Ablation on when to apply rotation.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：关于旋转应用时机的消融实验。
- en: '| Method | LLaMA2-7B | LLaMA2-13B | LLaMA3-8B |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | LLaMA2-7B | LLaMA2-13B | LLaMA3-8B |'
- en: '| RoLoRA | 62.3 | 63.9 | 56.6 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 62.3 | 63.9 | 56.6 |'
- en: '| Post-Training Rotation | 58.7 ($\downarrow$1.4) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 训练后旋转 | 58.7 ($\downarrow$1.4) |'
- en: 'Where to Apply Rotation? In Figure [2](#S2.F2 "Figure 2 ‣ 2 Related Work ‣
    RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we introduce two types of rotation in our pipeline, namely Between-Block
    Rotation applied on all weight matrices and In-Block Rotation applied on down_proj
    in FFN. As discussed in Section [4.1](#S4.SS1 "4.1 Applying Rotation ‣ 4 Method
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we can also apply a similar head-wise IBR $R_{3}$ is the best
    option to eliminate outliers.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转应用于何处？在图 [2](#S2.F2 "图 2 ‣ 2 相关工作 ‣ RoLoRA：微调旋转异常点自由的 LLM 以实现有效的权重-激活量化")
    中，我们介绍了管道中的两种旋转类型，即应用于所有权重矩阵的块间旋转（Between-Block Rotation）和应用于 FFN 中 down_proj
    的块内旋转（In-Block Rotation）。如在第 [4.1](#S4.SS1 "4.1 旋转应用 ‣ 4 方法 ‣ RoLoRA：微调旋转异常点自由的
    LLM 以实现有效的权重-激活量化") 节中讨论的，我们还可以应用类似的头部级 IBR $R_{3}$ 作为消除异常点的最佳选项。
- en: 'Table 6: Ablation on where to apply rotation.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：关于旋转应用位置的消融实验。
- en: '| Method | Rotation | ZCSR⁷ Avg. |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 旋转 | ZCSR⁷ 平均 |'
- en: '| --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| RoLoRA | $R_{1},R_{2}$ | 54.1 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | $R_{1},R_{2}$ | 54.1 |'
- en: '| --- | --- | --- |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $(-)$13.7) |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| $(-)$13.7) |'
- en: '| $(-)$4.4) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| $(-)$4.4) |'
- en: '| $(+)$0.3) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| $(+)$0.3) |'
- en: 'How to Apply LoRA? In Section [4.2](#S4.SS2 "4.2 Rotation-aware Fine-tuning
    ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we propose two rotation-aware fine-tuning schemes LoRA After Rotation
    (LAR) and LoRA Before Rotation (LBR) shown in Figure [3](#S4.F3 "Figure 3 ‣ 4.2
    Rotation-aware Fine-tuning ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated Outlier-free
    LLMs for Effective Weight-Activation Quantization"). We prove that LAR is the
    better paradigm based on the approximation error analysis compared with full-finetuning.
    In Table [7](#S5.T7 "Table 7 ‣ 5.4 Ablation Study and Analysis ‣ 5 Experiments
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we quantitatively compare the W4A4 quantization performance of
    two schemes on the fine-tuning of the LLaMA2-7B. The LAR scheme demonstrates better
    effectiveness, which corresponds to the approximation analysis shown in Figure [4](#S4.F4
    "Figure 4 ‣ 4.2 Rotation-aware Fine-tuning ‣ 4 Method ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization").'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如何应用 LoRA？在第 [4.2](#S4.SS2 "4.2 旋转感知微调 ‣ 4 方法 ‣ RoLoRA：微调旋转异常点自由的 LLM 以实现有效的权重-激活量化")
    节中，我们提出了两种旋转感知的微调方案：旋转后 LoRA (LAR) 和旋转前 LoRA (LBR)，如图 [3](#S4.F3 "图 3 ‣ 4.2 旋转感知微调
    ‣ 4 方法 ‣ RoLoRA：微调旋转异常点自由的 LLM 以实现有效的权重-激活量化") 所示。我们通过近似误差分析证明，LAR 是比全量微调更好的范式。在表
    [7](#S5.T7 "表 7 ‣ 5.4 消融研究与分析 ‣ 5 实验 ‣ RoLoRA：微调旋转异常点自由的 LLM 以实现有效的权重-激活量化") 中，我们定量比较了两种方案在
    LLaMA2-7B 微调上的 W4A4 量化性能。LAR 方案表现出更好的效果，这与图 [4](#S4.F4 "图 4 ‣ 4.2 旋转感知微调 ‣ 4 方法
    ‣ RoLoRA：微调旋转异常点自由的 LLM 以实现有效的权重-激活量化") 中的近似分析相符。
- en: 'Table 7: Ablation on how to apply LoRA.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：关于如何应用 LoRA 的消融实验。
- en: '| #Bits-Quantizer | Method | ZCSR⁷ Avg. | MMLU⁴ Avg. |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| #Bits-Quantizer | 方法 | ZCSR⁷ 平均 | MMLU⁴ 平均 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| W4A4-GPTQ | LAR | 62.3 | 31.0 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| W4A4-GPTQ | LAR | 62.3 | 31.0 |'
- en: '| LBR | 61.1 ($\downarrow$0.6) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| LBR | 61.1 ($\downarrow$0.6) |'
- en: 'Outliers Retaining the outlier-free characteristic during LLM fine-tuning is
    the most important motivation for RoLoRA. To quantitatively validate the effect
    of outlier elimination, we use kurtosis $\kappa=\frac{\sum_{i}^{k}(\mathbf{x}_{i}-\mu)^{4}}{\sigma^{4}+\epsilon}$
    across all layers is significantly reduced, which further gives rise to the low
    quantization error compared to the LoRA baseline. We also compare the activation
    distribution of RoLoRA against LoRA across layers in Figure [7](#A3.F7 "Figure
    7 ‣ Appendix C Activation Distribution Visualization ‣ RoLoRA: Fine-tuning Rotated
    Outlier-free LLMs for Effective Weight-Activation Quantization") in the Appendix.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值 在LLM微调过程中保持无异常值特征是RoLoRA的主要动机。为了定量验证异常值消除的效果，我们使用各层的峭度$\kappa=\frac{\sum_{i}^{k}(\mathbf{x}_{i}-\mu)^{4}}{\sigma^{4}+\epsilon}$显著降低，从而进一步降低了量化误差与LoRA基线相比。我们还在附录中的图[7](#A3.F7
    "图7 ‣ 附录C 激活分布可视化 ‣ RoLoRA：有效的权重-激活量化旋转异常值无LMS微调")中比较了RoLoRA与LoRA在各层的激活分布。
- en: LoRA rank settings We explore the robustness of LoRA and RoLoRA towards various
    rank settings $r\in\{4,8,16,32,64\}$ when fine-tuning LLaMA2-7B and evaluated
    on zero-shot commonsense reasoning tasks. The optimal rank setting for RoLoRA
    and LoRA are 16 and 32, respectively. The lower optimal rank indicates the potential
    of our RoLoRA to save trainable parameters. Overall, RoLoRA consistently outperforms
    LoRA regardless of the rank setting, demonstrating its robustness.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA秩设置 我们探索了LoRA和RoLoRA对不同秩设置$r\in\{4,8,16,32,64\}$的鲁棒性，在微调LLaMA2-7B并在零-shot常识推理任务上评估。RoLoRA和LoRA的最佳秩设置分别为16和32。较低的最佳秩指示了我们RoLoRA节省可训练参数的潜力。总体而言，无论秩设置如何，RoLoRA始终优于LoRA，展示了其鲁棒性。
- en: '![Refer to caption](img/2f83175753160c38d5d60fce916239e5.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f83175753160c38d5d60fce916239e5.png)'
- en: 'Figure 6: Average accuracy of W4A4 LLaMA2-7B fine-tuned with RoLoRA for varying
    ranks $r$.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：不同秩$r$下，使用RoLoRA微调的W4A4 LLaMA2-7B的平均准确率。
- en: 'Efficiency For the fine-tuning efficiency of RoLoRA, the additional training
    time is only incurred by the online rotation operation ($R_{2}$, batch size as
    8, 3 total epochs) in Table [8](#S5.T8 "Table 8 ‣ 5.4 Ablation Study and Analysis
    ‣ 5 Experiments ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective
    Weight-Activation Quantization"), where RoLoRA significantly improve W4A4 quantized
    LLaMA2-7B performance with extremely low additional overhead.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: RoLoRA的微调效率中，额外的训练时间仅由在线旋转操作（$R_{2}$，批量大小为8，总共3个周期）引起，如表[8](#S5.T8 "表8 ‣ 5.4
    消融研究与分析 ‣ 5 实验 ‣ RoLoRA：有效的权重-激活量化旋转异常值无LMS微调")所示，其中RoLoRA以极低的额外开销显著提高了W4A4量化LLaMA2-7B的性能。
- en: 'Table 8: The fine-tuning costs comparison on LLaMA2-7B with batch size as 8
    on NVIDIA H800 80G GPUs.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：在NVIDIA H800 80G GPU上，以批量大小为8的LLaMA2-7B的微调成本比较。
- en: '| Method | Training Time | GPU Memory | ZCSR⁷ Avg. |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 训练时间 | GPU内存 | ZCSR⁷ 平均值 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LoRA | 3.55 h | 23.0 GB | 37.0 (GPTQ) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| LoRA | 3.55小时 | 23.0 GB | 37.0 (GPTQ) |'
- en: '| RoLoRA | 3.65 h | 23.1 GB | 62.3 (GPTQ) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 3.65小时 | 23.1 GB | 62.3 (GPTQ) |'
- en: 6 Conclusion
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: This paper presents RoLoRA, the first work to explore the feasibility of weight-activation
    quantization in LoRA. RoLoRA applies rotation for eliminating outliers in activation
    distribution and performs rotation-aware fine-tuning to preserve the outlier-free
    characteristics. We theoretically and empirically investigate how to integrate
    rotation into LoRA. RoLoRA improves the performance of W4A4 and W6A6 LLMs by a
    great margin across various tasks with the same training cost. Moreover, RoLoRA
    can also help visual instruction tuning.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了RoLoRA，这是首个探索LoRA中权重-激活量化可行性的工作。RoLoRA应用旋转来消除激活分布中的异常值，并进行旋转感知的微调以保留无异常值的特征。我们在理论和实践上研究了如何将旋转整合到LoRA中。RoLoRA在相同训练成本下显著提高了W4A4和W6A6
    LLM的性能，适用于各种任务。此外，RoLoRA也可以帮助视觉指令微调。
- en: Limitation
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 局限性
- en: In this work, we propose a rotation-based fine-tuning method that can effectively
    improve quantization robustness to low-bit weight-activation PTQ via retaining
    the outlier-free characteristics. The fine-tuning is conducted on NVIDIA H800
    GPUs, while the recent NVIDIA Blackwell-architecture GPUs with 4-bit floating
    point support may further improve the efficiency. We will take the limitations
    into account and improve in future work.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种基于旋转的微调方法，可以有效提高低位权重激活 PTQ 的量化鲁棒性，同时保持去除异常值的特性。微调在 NVIDIA H800
    GPU 上进行，而近期的 NVIDIA Blackwell 架构 GPU 具有 4 位浮点支持，可能进一步提高效率。我们将考虑限制，并在未来工作中改进。
- en: Acknowledgement
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This research is supported by HKSAR RGC General Research Fund (GRF) #16208823.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了香港特别行政区研究资助局（RGC）一般研究基金（GRF）#16208823 的支持。
- en: References
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. 2023. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等（2023）Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等. 2023. Gpt-4 技术报告。 *arXiv 预印本 arXiv:2303.08774*。
- en: AI@Meta (2024) AI@Meta. 2024. [Llama 3 model card](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md).
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AI@Meta（2024）AI@Meta. 2024. [Llama 3 模型卡](https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md)。
- en: 'Ashkboos et al. (2023a) Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari
    do Nascimento, Torsten Hoefler, and James Hensman. 2023a. Slicegpt: Compress large
    language models by deleting rows and columns. In *The Twelfth International Conference
    on Learning Representations*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos 等（2023a）Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento,
    Torsten Hoefler 和 James Hensman. 2023a. Slicegpt: 通过删除行和列压缩大型语言模型。在 *第十二届国际学习表征会议*
    上。'
- en: Ashkboos et al. (2023b) Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan
    Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. 2023b. Towards
    end-to-end 4-bit inference on generative large language models. *arXiv preprint
    arXiv:2310.09259*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ashkboos 等（2023b）Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong,
    Xincheng Wang, Jie Ren, Torsten Hoefler 和 Dan Alistarh. 2023b. 面向生成大型语言模型的端到端
    4 位推理。 *arXiv 预印本 arXiv:2310.09259*。
- en: 'Ashkboos et al. (2024) Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L
    Croci, Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman.
    2024. Quarot: Outlier-free 4-bit inference in rotated llms. *arXiv preprint arXiv:2404.00456*.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos 等（2024）Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian L Croci,
    Bo Li, Martin Jaggi, Dan Alistarh, Torsten Hoefler 和 James Hensman. 2024. Quarot:
    去除异常值的 4 位推理在旋转的 llms 中。 *arXiv 预印本 arXiv:2404.00456*。'
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等. 2020.
    Piqa: 关于自然语言中物理常识的推理。在 *AAAI 人工智能会议论文集* 中，第 34 卷，页码 7432–7439。'
- en: Bondarenko et al. (2021) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    2021. Understanding and overcoming the challenges of efficient transformer quantization.
    In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language
    Processing*, pages 7947–7969.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko 等（2021）Yelysei Bondarenko, Markus Nagel 和 Tijmen Blankevoort. 2021.
    理解和克服高效变换器量化的挑战。在 *2021年自然语言处理实证方法会议论文集* 中，页码 7947–7969。
- en: 'Bondarenko et al. (2024) Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort.
    2024. Quantizable transformers: Removing outliers by helping attention heads do
    nothing. *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bondarenko 等（2024）Yelysei Bondarenko, Markus Nagel 和 Tijmen Blankevoort. 2024.
    可量化变换器：通过帮助注意力头什么也不做来去除异常值。 *神经信息处理系统进展*，36。
- en: 'Chee et al. (2024) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher M
    De Sa. 2024. Quip: 2-bit quantization of large language models with guarantees.
    *Advances in Neural Information Processing Systems*, 36.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee 等（2024）Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher M De Sa.
    2024. Quip: 大型语言模型的 2 位量化保证。 *神经信息处理系统进展*，36。'
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. 2023. [Vicuna: An open-source chatbot impressing
    gpt-4 with 90%* chatgpt quality](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang 等（2023）Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion
    Stoica 和 Eric P. Xing. 2023. [Vicuna: 一个开源聊天机器人，以 90%* chatgpt 质量打动 gpt-4](https://lmsys.org/blog/2023-03-30-vicuna/)。'
- en: 'Clark et al. (2019) Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising
    difficulty of natural yes/no questions. *arXiv preprint arXiv:1905.10044*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人（2019）Christopher Clark、Kenton Lee、Ming-Wei Chang、Tom Kwiatkowski、Michael
    Collins 和 Kristina Toutanova。2019年。Boolq：探索自然的“是/否”问题的惊人难度。*arXiv 预印本 arXiv:1905.10044*。
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等人（2018）Peter Clark、Isaac Cowhey、Oren Etzioni、Tushar Khot、Ashish Sabharwal、Carissa
    Schoenick 和 Oyvind Tafjord。2018年。认为你解决了问答问题？试试 arc，AI2 推理挑战。*arXiv 预印本 arXiv:1803.05457*。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    2022. Gpt3\. int8 (): 8-bit matrix multiplication for transformers at scale. *Advances
    in Neural Information Processing Systems*, 35:30318–30332.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人（2022）Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer。2022年。Gpt3\.
    int8 (): 用于大规模变换器的 8 位矩阵乘法。*神经信息处理系统进展*，35:30318–30332。'
- en: 'Dettmers et al. (2024) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. *Advances in
    Neural Information Processing Systems*, 36.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等人（2024）Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。2024年。Qlora：高效微调量化语言模型。*神经信息处理系统进展*，36。
- en: Elhage et al. (2023) Nelson Elhage, Robert Lasenby, and Christopher Olah. 2023.
    Privileged bases in the transformer residual stream. *Transformer Circuits Thread*.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elhage 等人（2023）Nelson Elhage、Robert Lasenby 和 Christopher Olah。2023年。变换器残差流中的特权基。*变换器电路线程*。
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. In *The Eleventh International Conference on Learning Representations*.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人（2023）Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。2023年。Gptq：生成预训练变换器的准确后训练量化。在*第十一届国际学习表征会议*上。
- en: Gao et al. (2021) Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. 2021. A framework for few-shot language model evaluation. *Version v0\.
    0.1\. Sept*, page 8.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人（2021）Leo Gao、Jonathan Tow、Stella Biderman、Sid Black、Anthony DiPofi、Charles
    Foster、Laurence Golding、Jeffrey Hsu、Kyle McDonell、Niklas Muennighoff 等人。2021年。一种用于少样本语言模型评估的框架。*版本
    v0.0.1. 九月*，第 8 页。
- en: 'Hayou et al. (2024) Soufiane Hayou, Nikhil Ghosh, and Bin Yu. 2024. Lora+:
    Efficient low rank adaptation of large models. *arXiv preprint arXiv:2402.12354*.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hayou 等人（2024）Soufiane Hayou、Nikhil Ghosh 和 Bin Yu。2024年。Lora+：大型模型的高效低秩适配。*arXiv
    预印本 arXiv:2402.12354*。
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. In *International Conference on Learning Representations*.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人（2020）Dan Hendrycks、Collin Burns、Steven Basart、Andy Zou、Mantas Mazeika、Dawn
    Song 和 Jacob Steinhardt。2020年。测量大规模多任务语言理解。在*国际学习表征会议*上。
- en: Heo et al. (2024) Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim,
    Se Jung Kwon, and Dongsoo Lee. 2024. Rethinking channel dimensions to isolate
    outliers for low-bit weight quantization of large language models. In *The Twelfth
    International Conference on Learning Representations*.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Heo 等人（2024）Jung Hwan Heo、Jeonghoon Kim、Beomseok Kwon、Byeongwook Kim、Se Jung
    Kwon 和 Dongsoo Lee。2024年。重新思考通道维度以隔离异常值，用于大型语言模型的低位权重量化。在*第十二届国际学习表征会议*上。
- en: 'Hu et al. (2021) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large
    language models. In *International Conference on Learning Representations*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2021）Edward J Hu、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi Li、Shean Wang、Lu
    Wang 和 Weizhu Chen 等人。2021年。Lora：大型语言模型的低秩适配。在*国际学习表征会议*上。
- en: 'Hu et al. (2023) Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-Peng Lim,
    Lidong Bing, Xing Xu, Soujanya Poria, and Roy Ka-Wei Lee. 2023. Llm-adapters:
    An adapter family for parameter-efficient fine-tuning of large language models.
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人（2023）Zhiqiang Hu、Lei Wang、Yihuai Lan、Wanyu Xu、Ee-Peng Lim、Lidong Bing、Xing
    Xu、Soujanya Poria 和 Roy Ka-Wei Lee。2023年。Llm-adapters：一种用于大型语言模型的参数高效微调的适配器家族。在*2023年自然语言处理实证方法会议*上。
- en: Huang et al. (2023) Xijie Huang, Zhiqiang Shen, and Kwang-Ting Cheng. 2023.
    Variation-aware vision transformer quantization. *arXiv preprint arXiv:2307.00331*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等 (2023) Xijie Huang, Zhiqiang Shen 和 Kwang-Ting Cheng. 2023. 变异感知视觉变换器量化.
    *arXiv 预印本 arXiv:2307.00331*。
- en: 'Kopiczko et al. (2024) Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano.
    2024. VeRA: Vector-based random matrix adaptation. In *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kopiczko 等 (2024) Dawid Jan Kopiczko, Tijmen Blankevoort 和 Yuki M Asano. 2024.
    VeRA: 基于向量的随机矩阵适配. 见 *第十二届国际学习表征会议*。'
- en: 'Kovaleva et al. (2021) Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers, and
    Anna Rumshisky. 2021. Bert busters: Outlier dimensions that disrupt transformers.
    *Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021*.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kovaleva 等 (2021) Olga Kovaleva, Saurabh Kulshreshtha, Anna Rogers 和 Anna Rumshisky.
    2021. Bert 破坏者：破坏变换器的异常维度. *计算语言学协会发现：ACL-IJCNLP 2021*。
- en: 'Li and Liang (2021) Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 4582–4597.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 和 Liang (2021) Xiang Lisa Li 和 Percy Liang. 2021. Prefix-tuning: 优化连续提示以进行生成.
    见 *第59届计算语言学协会年会暨第11届国际自然语言处理联合会议论文集（第1卷：长论文）*，第4582–4597页。'
- en: 'Li et al. (2024) Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng
    He, Weizhu Chen, and Tuo Zhao. 2024. Loftq: Lora-fine-tuning-aware quantization
    for large language models. In *The Twelfth International Conference on Learning
    Representations*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2024) Yixiao Li, Yifan Yu, Chen Liang, Nikos Karampatziakis, Pengcheng
    He, Weizhu Chen 和 Tuo Zhao. 2024. Loftq: Lora-微调感知量化的大型语言模型. 见 *第十二届国际学习表征会议*。'
- en: 'Lin et al. (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming
    Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024.
    Awq: Activation-aware weight quantization for on-device llm compression and acceleration.
    *Proceedings of Machine Learning and Systems*, 6:87–100.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen,
    Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan 和 Song Han. 2024. Awq:
    激活感知权重量化用于设备上的 LLM 压缩与加速. *机器学习与系统会议论文集*，6:87–100。'
- en: Liu et al. (2023a) Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.
    Improved baselines with visual instruction tuning. *arXiv preprint arXiv:2310.03744*.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2023a) Haotian Liu, Chunyuan Li, Yuheng Li 和 Yong Jae Lee. 2023a. 通过视觉指令调优改善基线.
    *arXiv 预印本 arXiv:2310.03744*。
- en: Liu et al. (2024a) Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    2024a. Visual instruction tuning. *Advances in neural information processing systems*,
    36.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2024a) Haotian Liu, Chunyuan Li, Qingyang Wu 和 Yong Jae Lee. 2024a. 视觉指令调优.
    *神经信息处理系统进展*，36。
- en: 'Liu et al. (2023b) Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong,
    and Kwang-Ting Cheng. 2023b. Llm-fp4: 4-bit floating-point quantized transformers.
    In *The 2023 Conference on Empirical Methods in Natural Language Processing*.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2023b) Shih-yang Liu, Zechun Liu, Xijie Huang, Pingcheng Dong 和 Kwang-Ting
    Cheng. 2023b. Llm-fp4: 4-bit 浮点量化变换器. 见 *2023年自然语言处理经验方法会议*。'
- en: 'Liu et al. (2024b) Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov,
    Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. 2024b. Dora: Weight-decomposed
    low-rank adaptation. *arXiv preprint arXiv:2402.09353*.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2024b) Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang
    Frank Wang, Kwang-Ting Cheng 和 Min-Hung Chen. 2024b. Dora: 权重分解低秩适配. *arXiv 预印本
    arXiv:2402.09353*。'
- en: Liu et al. (2024c) Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv
    Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen
    Blankevoort. 2024c. Spinquant–llm quantization with learned rotations. *arXiv
    preprint arXiv:2405.16406*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等 (2024c) Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv
    Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian 和 Tijmen Blankevoort.
    2024c. Spinquant–llm 量化与学习旋转. *arXiv 预印本 arXiv:2405.16406*。
- en: Mihaylov et al. (2018) Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open
    book question answering. *arXiv preprint arXiv:1809.02789*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov 等 (2018) Todor Mihaylov, Peter Clark, Tushar Khot 和 Ashish Sabharwal.
    2018. 一套盔甲能导电吗？一个用于开放书籍问答的新数据集. *arXiv 预印本 arXiv:1809.02789*。
- en: Qin et al. (2024) Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang,
    Shouda Liu, Jie Luo, Xianglong Liu, and Michele Magno. 2024. Accurate lora-finetuning
    quantization of llms via information retention. *arXiv preprint arXiv:2402.05445*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin 等 (2024) Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang,
    Shouda Liu, Jie Luo, Xianglong Liu, 和 Michele Magno. 2024. 通过信息保留实现 llms 的准确 lora-finetuning
    量化。*arXiv 预印本 arXiv:2402.05445*。
- en: Radford et al. (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, et al. 2021. Learning transferable visual models from natural language
    supervision. In *International conference on machine learning*, pages 8748–8763\.
    PMLR.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等 (2021) Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,
    Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack
    Clark, 等. 2021. 从自然语言监督中学习可转移的视觉模型。在*国际机器学习会议*，页 8748–8763。PMLR。
- en: 'Ritter (1996) Terry Ritter. 1996. Walsh-hadamard transforms: A literature survey.
    *Research Comments from Cipers by Ritter*, page 10.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ritter (1996) Terry Ritter. 1996. Walsh-hadamard 变换：文献综述。*Ritter 的研究评论*，第 10
    页。
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等 (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, 和
    Yejin Choi. 2021. Winogrande：大规模的对抗性 Winograd 语义挑战。*ACM 通讯*, 64(9):99–106。
- en: 'Shang et al. (2024) Yuzhang Shang, Zhihang Yuan, and Zhen Dong. 2024. Pb-llm:
    Partially binarized large language models. In *The Twelfth International Conference
    on Learning Representations*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shang 等 (2024) Yuzhang Shang, Zhihang Yuan, 和 Zhen Dong. 2024. Pb-llm：部分二值化大规模语言模型。在*第十二届国际学习表征会议*。
- en: 'Shao et al. (2024) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. 2024. Omniquant:
    Omnidirectionally calibrated quantization for large language models. In *The Twelfth
    International Conference on Learning Representations*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等 (2024) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui Zhao,
    Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, 和 Ping Luo. 2024. Omniquant：面向大规模语言模型的全向校准量化。在*第十二届国际学习表征会议*。
- en: Shazeer (2020) Noam Shazeer. 2020. Glu variants improve transformer. *arXiv
    preprint arXiv:2002.05202*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer (2020) Noam Shazeer. 2020. Glu 变体改进变换器。*arXiv 预印本 arXiv:2002.05202*。
- en: Sun et al. (2024) Mingjie Sun, Xinlei Chen, J. Zico Kolter, and Zhuang Liu.
    2024. Massive activations in large language models. *arXiv preprint arXiv:2402.17762*.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等 (2024) Mingjie Sun, Xinlei Chen, J. Zico Kolter, 和 Zhuang Liu. 2024. 大规模语言模型中的大规模激活。*arXiv
    预印本 arXiv:2402.17762*。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等 (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen
    Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. 2023. Stanford alpaca：一种遵循指令的
    llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。
- en: 'Touvron et al. (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models.
    *arXiv preprint arXiv:2307.09288*.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等 (2023) Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad
    Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, 等. 2023. Llama 2：开放基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*。
- en: 'Wei et al. (2022) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. 2022. Outlier suppression:
    Pushing the limit of low-bit transformer language models. *Advances in Neural
    Information Processing Systems*, 35:17402–17414.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等 (2022) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang
    Zhang, Qi Zhang, Fengwei Yu, 和 Xianglong Liu. 2022. 异常值抑制：推动低位变换器语言模型的极限。*神经信息处理系统进展*,
    35:17402–17414。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao 等 (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    和 Song Han. 2023. Smoothquant：针对大规模语言模型的准确高效后训练量化。在*国际机器学习会议*，页 38087–38099。PMLR。
- en: 'Xu et al. (2024) Yuhui Xu, Lingxi Xie, Xiaotao Gu, Xin Chen, Heng Chang, Hengheng
    Zhang, Zhengsu Chen, XIAOPENG ZHANG, and Qi Tian. 2024. Qa-lora: Quantization-aware
    low-rank adaptation of large language models. In *The Twelfth International Conference
    on Learning Representations*.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许等（2024）Yuhui Xu、Lingxi Xie、Xiaotao Gu、Xin Chen、Heng Chang、Hengheng Zhang、Zhengsu
    Chen、XIAOPENG ZHANG 和 Qi Tian。2024年。Qa-lora：量化感知低秩适应的大型语言模型。在*第十二届国际学习表征会议*。
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? *arXiv
    preprint arXiv:1905.07830*.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zellers 等（2019）Rowan Zellers、Ari Holtzman、Yonatan Bisk、Ali Farhadi 和 Yejin Choi。2019年。Hellaswag：机器真的能完成你的句子吗？*arXiv预印本
    arXiv:1905.07830*。
- en: 'Zhang et al. (2023) Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou, Pan Lu,
    Yu Qiao, Hongsheng Li, and Peng Gao. 2023. Llama-adapter: Efficient fine-tuning
    of large language models with zero-initialized attention. In *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等（2023）Renrui Zhang、Jiaming Han、Chris Liu、Aojun Zhou、Pan Lu、Yu Qiao、Hongsheng
    Li 和 Peng Gao。2023年。Llama-adapter：零初始化注意力的大型语言模型高效微调。在*第十二届国际学习表征会议*。
- en: 'Zhao et al. (2024) Yilong Zhao, Chien-Yu Lin, Kan Zhu, Zihao Ye, Lequn Chen,
    Size Zheng, Luis Ceze, Arvind Krishnamurthy, Tianqi Chen, and Baris Kasikci. 2024.
    Atom: Low-bit quantization for efficient and accurate llm serving. *Proceedings
    of Machine Learning and Systems*, 6:196–209.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 赵等（2024）Yilong Zhao、Chien-Yu Lin、Kan Zhu、Zihao Ye、Lequn Chen、Size Zheng、Luis
    Ceze、Arvind Krishnamurthy、Tianqi Chen 和 Baris Kasikci。2024年。Atom：低比特量化用于高效且准确的LLM服务。*机器学习与系统会议论文集*，6:196–209。
- en: 'Zheng et al. (2024) Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan
    Luo, and Yongqiang Ma. 2024. [Llamafactory: Unified efficient fine-tuning of 100+
    language models](http://arxiv.org/abs/2403.13372). *arXiv preprint arXiv:2403.13372*.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等（2024）Yaowei Zheng、Richong Zhang、Junhao Zhang、Yanhan Ye、Zheyan Luo 和 Yongqiang
    Ma。2024年。[Llamafactory：100+语言模型的统一高效微调](http://arxiv.org/abs/2403.13372)。*arXiv预印本
    arXiv:2403.13372*。
- en: Appendix A Detailed Evaluation Results
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 详细评估结果
- en: 'Table [9](#A1.T9 "Table 9 ‣ Appendix A Detailed Evaluation Results ‣ RoLoRA:
    Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization")
    and Table [10](#A1.T10 "Table 10 ‣ Appendix A Detailed Evaluation Results ‣ RoLoRA:
    Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation Quantization")
    listed the full evaluation results on zero-shot commonsense reasoning tasks and
    MMLU benchmarks, respectively. We use the ‘acc_norm’ in the evaluation report
    given by EleutherAI evaluation harness Gao et al. ([2021](#bib.bib17)) as the
    accuracy if there are such metrics. Otherwise, we use ‘acc’.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 表[9](#A1.T9 "表 9 ‣ 附录A 详细评估结果 ‣ RoLoRA：微调旋转的无异常LLM以实现有效的权重-激活量化")和表[10](#A1.T10
    "表 10 ‣ 附录A 详细评估结果 ‣ RoLoRA：微调旋转的无异常LLM以实现有效的权重-激活量化")列出了零-shot常识推理任务和MMLU基准的完整评估结果。如果有这样的指标，我们使用EleutherAI评估工具Gao等人（[2021](#bib.bib17)）报告中的‘acc_norm’作为准确度，否则我们使用‘acc’。
- en: 'Table 9: Full accuracy comparison on zero-shot commonsense reasoning tasks
    of LLaMA series.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：LLaMA系列在零-shot常识推理任务上的全面准确度比较。
- en: '| #Bits | Quantizer | Method | BoolQ | PIQA | HellaS. | WinoG. | Arc-e | Arc-c
    | OBQA | Avg. |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| #位数 | 量化器 | 方法 | BoolQ | PIQA | HellaS. | WinoG. | Arc-e | Arc-c | OBQA |
    平均值 |'
- en: '| LLaMA2-7B |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B |'
- en: '| FP16 | - | LoRA | 81.2 | 79.8 | 78.6 | 70.6 | 73.9 | 47.7 | 46.8 | 68.4 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 81.2 | 79.8 | 78.6 | 70.6 | 73.9 | 47.7 | 46.8 | 68.4 |'
- en: '| W4A4 | RTN | LoRA | 46.0 | 49.5 | 27.0 | 49.6 | 27.8 | 24.2 | 26.8 | 35.8
    |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 46.0 | 49.5 | 27.0 | 49.6 | 27.8 | 24.2 | 26.8 | 35.8
    |'
- en: '| RoLoRA | 67.1 | 67.7 | 59.7 | 56.9 | 58.3 | 35.0 | 34.2 | 54.1 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 67.1 | 67.7 | 59.7 | 56.9 | 58.3 | 35.0 | 34.2 | 54.1 |'
- en: '| GPTQ | LoRA | 52.3 | 52.5 | 26.9 | 50.4 | 28.6 | 25.3 | 22.8 | 37.0 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 52.3 | 52.5 | 26.9 | 50.4 | 28.6 | 25.3 | 22.8 | 37.0 |'
- en: '| RoLoRA | 73.5 | 76.2 | 71.8 | 64.1 | 67.7 | 42.2 | 40.4 | 62.3 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 73.5 | 76.2 | 71.8 | 64.1 | 67.7 | 42.2 | 40.4 | 62.3 |'
- en: '| W6A6 | RTN | LoRA | 76.3 | 78.0 | 75.3 | 69.2 | 71.2 | 45.7 | 41.6 | 65.3
    |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 76.3 | 78.0 | 75.3 | 69.2 | 71.2 | 45.7 | 41.6 | 65.3
    |'
- en: '| RoLoRA | 77.9 | 79.1 | 76.3 | 68.5 | 74.8 | 47.3 | 43.6 | 66.8 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 77.9 | 79.1 | 76.3 | 68.5 | 74.8 | 47.3 | 43.6 | 66.8 |'
- en: '| GPTQ | LoRA | 76.3 | 78.2 | 75.4 | 69.5 | 72.1 | 46.1 | 40.8 | 65.5 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 76.3 | 78.2 | 75.4 | 69.5 | 72.1 | 46.1 | 40.8 | 65.5 |'
- en: '| RoLoRA | 77.4 | 79.1 | 76.5 | 70.4 | 75.2 | 47.2 | 44.0 | 67.1 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 77.4 | 79.1 | 76.5 | 70.4 | 75.2 | 47.2 | 44.0 | 67.1 |'
- en: '| LLaMA2-13B |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B |'
- en: '| FP16 | - | LoRA | 83.9 | 81.2 | 80.9 | 74.2 | 74.4 | 51.3 | 47.6 | 70.5 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 83.9 | 81.2 | 80.9 | 74.2 | 74.4 | 51.3 | 47.6 | 70.5 |'
- en: '| W4A4 | RTN | LoRA | 39.8 | 52.1 | 26.1 | 45.7 | 25.9 | 25.8 | 25.4 | 34.4
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 39.8 | 52.1 | 26.1 | 45.7 | 25.9 | 25.8 | 25.4 | 34.4
    |'
- en: '| RoLoRA | 70.6 | 73.9 | 67.2 | 59.6 | 66.8 | 38.7 | 34.2 | 58.7 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 70.6 | 73.9 | 67.2 | 59.6 | 66.8 | 38.7 | 34.2 | 58.7 |'
- en: '| GPTQ | LoRA | 38.0 | 50.2 | 26.0 | 49.0 | 25.9 | 26.4 | 25.4 | 34.4 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 38.0 | 50.2 | 26.0 | 49.0 | 25.9 | 26.4 | 25.4 | 34.4 |'
- en: '| RoLoRA | 74.0 | 77.2 | 73.9 | 66.0 | 73.3 | 43.9 | 38.8 | 63.9 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 74.0 | 77.2 | 73.9 | 66.0 | 73.3 | 43.9 | 38.8 | 63.9 |'
- en: '| W6A6 | RTN | LoRA | 80.8 | 78.1 | 77.8 | 70.3 | 73.0 | 49.2 | 42.2 | 67.3
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 80.8 | 78.1 | 77.8 | 70.3 | 73.0 | 49.2 | 42.2 | 67.3
    |'
- en: '| RoLoRA | 80.3 | 78.8 | 78.0 | 71.1 | 77.6 | 49.6 | 43.2 | 68.4 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 80.3 | 78.8 | 78.0 | 71.1 | 77.6 | 49.6 | 43.2 | 68.4 |'
- en: '| GPTQ | LoRA | 81.9 | 79.2 | 78.5 | 69.3 | 74.3 | 51.5 | 41.2 | 68.0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 81.9 | 79.2 | 78.5 | 69.3 | 74.3 | 51.5 | 41.2 | 68.0 |'
- en: '| RoLoRA | 80.6 | 79.3 | 78.1 | 72.5 | 77.4 | 49.4 | 44.0 | 68.8 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 80.6 | 79.3 | 78.1 | 72.5 | 77.4 | 49.4 | 44.0 | 68.8 |'
- en: '| LLaMA3-8B |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8B |'
- en: '| FP16 | - | LoRA | 64.6 | 82.4 | 81.4 | 75.1 | 81.8 | 56.5 | 48.0 | 70.0 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 64.6 | 82.4 | 81.4 | 75.1 | 81.8 | 56.5 | 48.0 | 70.0 |'
- en: '| W4A4 | RTN | LoRA | 46.7 | 52.2 | 29.7 | 47.6 | 29.3 | 24.7 | 26.6 | 36.7
    |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 46.7 | 52.2 | 29.7 | 47.6 | 29.3 | 24.7 | 26.6 | 36.7
    |'
- en: '| RoLoRA | 58.0 | 67.3 | 57.7 | 56.0 | 49.0 | 30.2 | 31.8 | 50.0 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 58.0 | 67.3 | 57.7 | 56.0 | 49.0 | 30.2 | 31.8 | 50.0 |'
- en: '| GPTQ | LoRA | 42.5 | 54.4 | 29.4 | 49.0 | 31.1 | 22.5 | 27.0 | 36.6 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 42.5 | 54.4 | 29.4 | 49.0 | 31.1 | 22.5 | 27.0 | 36.6 |'
- en: '| RoLoRA | 63.2 | 71.1 | 66.7 | 60.2 | 60.3 | 38.2 | 36.8 | 56.6 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 63.2 | 71.1 | 66.7 | 60.2 | 60.3 | 38.2 | 36.8 | 56.6 |'
- en: '| W6A6 | RTN | LoRA | 75.5 | 78.3 | 77.4 | 70.8 | 76.4 | 51.2 | 44.0 | 67.7
    |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 75.5 | 78.3 | 77.4 | 70.8 | 76.4 | 51.2 | 44.0 | 67.7
    |'
- en: '| RoLoRA | 78.6 | 79.5 | 76.7 | 71.1 | 77.6 | 49.8 | 40.8 | 67.8 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 78.6 | 79.5 | 76.7 | 71.1 | 77.6 | 49.8 | 40.8 | 67.8 |'
- en: '| GPTQ | LoRA | 77.9 | 78.3 | 77.9 | 71.3 | 75.2 | 50.5 | 43.2 | 67.8 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 77.9 | 78.3 | 77.9 | 71.3 | 75.2 | 50.5 | 43.2 | 67.8 |'
- en: '| RoLoRA | 78.1 | 79.3 | 76.8 | 71.9 | 76.7 | 50.9 | 42.8 | 68.1 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 78.1 | 79.3 | 76.8 | 71.9 | 76.7 | 50.9 | 42.8 | 68.1 |'
- en: 'Table 10: Full accuracy on MMLU Benchmark of LLaMA series.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: LLaMA 系列在 MMLU 基准上的完整准确率。'
- en: '| #Bits | Quantizer | Method | Hums. | Other | Social | STEM | Avg. |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| #Bits | Quantizer | Method | Hums. | Other | Social | STEM | Avg. |'
- en: '| LLaMA2-7B |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B |'
- en: '| FP16 | - | LoRA | 41.5 | 50.8 | 48.2 | 34.7 | 43.5 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 41.5 | 50.8 | 48.2 | 34.7 | 43.5 |'
- en: '| W4A4 | RTN | LoRA | 24.2 | 24.8 | 22.7 | 21.7 | 23.5 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 24.2 | 24.8 | 22.7 | 21.7 | 23.5 |'
- en: '| RoLoRA | 24.7 | 26.2 | 27.2 | 25.7 | 25.8 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 24.7 | 26.2 | 27.2 | 25.7 | 25.8 |'
- en: '| GPTQ | LoRA | 24.3 | 24.5 | 23.0 | 22.0 | 23.5 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 24.3 | 24.5 | 23.0 | 22.0 | 23.5 |'
- en: '| RoLoRA | 30.1 | 33.0 | 32.0 | 29.4 | 31.0 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 30.1 | 33.0 | 32.0 | 29.4 | 31.0 |'
- en: '| W6A6 | RTN | LoRA | 35.4 | 40.6 | 37.5 | 30.4 | 35.9 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 35.4 | 40.6 | 37.5 | 30.4 | 35.9 |'
- en: '| RoLoRA | 38.2 | 45.4 | 44.7 | 35.2 | 40.5 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 38.2 | 45.4 | 44.7 | 35.2 | 40.5 |'
- en: '| GPTQ | LoRA | 34.2 | 39.4 | 39.4 | 30.6 | 35.7 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 34.2 | 39.4 | 39.4 | 30.6 | 35.7 |'
- en: '| RoLoRA | 37.8 | 46.1 | 46.2 | 34.9 | 40.8 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 37.8 | 46.1 | 46.2 | 34.9 | 40.8 |'
- en: '| LLaMA2-13B |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B |'
- en: '| FP16 | - | LoRA | 49.6 | 59.2 | 59.9 | 42.8 | 52.4 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 49.6 | 59.2 | 59.9 | 42.8 | 52.4 |'
- en: '| W4A4 | RTN | LoRA | 25.0 | 25.7 | 23.4 | 22.4 | 24.2 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 25.0 | 25.7 | 23.4 | 22.4 | 24.2 |'
- en: '| RoLoRA | 28.9 | 32.5 | 33.2 | 28.4 | 30.5 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 28.9 | 32.5 | 33.2 | 28.4 | 30.5 |'
- en: '| GPTQ | LoRA | 25.5 | 24.2 | 24.1 | 23.4 | 24.4 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 25.5 | 24.2 | 24.1 | 23.4 | 24.4 |'
- en: '| RoLoRA | 37.7 | 42.3 | 43.7 | 32.7 | 38.9 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 37.7 | 42.3 | 43.7 | 32.7 | 38.9 |'
- en: '| W6A6 | RTN | LoRA | 44.3 | 52.8 | 55.0 | 38.6 | 47.3 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 44.3 | 52.8 | 55.0 | 38.6 | 47.3 |'
- en: '| RoLoRA | 45.0 | 52.9 | 55.2 | 39.1 | 47.7 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 45.0 | 52.9 | 55.2 | 39.1 | 47.7 |'
- en: '| GPTQ | LoRA | 44.8 | 54.7 | 53.8 | 39.0 | 47.6 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 44.8 | 54.7 | 53.8 | 39.0 | 47.6 |'
- en: '| RoLoRA | 45.6 | 53.7 | 55.2 | 38.7 | 47.9 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 45.6 | 53.7 | 55.2 | 38.7 | 47.9 |'
- en: '| LLaMA3-8B |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA3-8B |'
- en: '| FP16 | - | LoRA | 57.4 | 70.7 | 72.8 | 52.7 | 62.7 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | - | LoRA | 57.4 | 70.7 | 72.8 | 52.7 | 62.7 |'
- en: '| W4A4 | RTN | LoRA | 23.6 | 24.3 | 23.7 | 21.8 | 23.3 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| W4A4 | RTN | LoRA | 23.6 | 24.3 | 23.7 | 21.8 | 23.3 |'
- en: '| RoLoRA | 30.8 | 34.5 | 33.5 | 30.5 | 32.1 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 30.8 | 34.5 | 33.5 | 30.5 | 32.1 |'
- en: '| GPTQ | LoRA | 24.6 | 23.0 | 23.4 | 24.3 | 23.9 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 24.6 | 23.0 | 23.4 | 24.3 | 23.9 |'
- en: '| RoLoRA | 36.0 | 42.2 | 43.6 | 33.5 | 38.5 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 36.0 | 42.2 | 43.6 | 33.5 | 38.5 |'
- en: '| W6A6 | RTN | LoRA | 49.7 | 63.0 | 64.4 | 47.2 | 55.3 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| W6A6 | RTN | LoRA | 49.7 | 63.0 | 64.4 | 47.2 | 55.3 |'
- en: '| RoLoRA | 52.7 | 67.5 | 70.0 | 51.1 | 59.4 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 52.7 | 67.5 | 70.0 | 51.1 | 59.4 |'
- en: '| GPTQ | LoRA | 48.8 | 61.8 | 63.9 | 45.7 | 54.3 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | LoRA | 48.8 | 61.8 | 63.9 | 45.7 | 54.3 |'
- en: '| RoLoRA | 52.9 | 68.3 | 69.6 | 50.4 | 59.4 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| RoLoRA | 52.9 | 68.3 | 69.6 | 50.4 | 59.4 |'
- en: Appendix B Hyper-parameters for Reproduction
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 复现的超参数
- en: 'In Table [11](#A2.T11 "Table 11 ‣ Appendix B Hyper-parameters for Reproduction
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"), we list the detailed hyper-parameters for reproducing RoLoRA and
    LoRA results. We do not apply searches on any hyperparameters for better accuracy,
    all the settings for the LLaMA series and LLaVA align with the default settings
    of Zheng et al. ([2024](#bib.bib51)) and Liu et al. ([2024a](#bib.bib30)).'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [11](#A2.T11 "Table 11 ‣ Appendix B Hyper-parameters for Reproduction ‣
    RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") 中，我们列出了再现 RoLoRA 和 LoRA 结果的详细超参数。我们没有对任何超参数进行搜索以提高准确性，所有 LLaMA
    系列和 LLaVA 的设置都与 Zheng 等人 ([2024](#bib.bib51)) 和 Liu 等人 ([2024a](#bib.bib30)) 的默认设置一致。'
- en: 'Table 11: Detailed hyper-parameters for fine-tuning different LLMs and LMMs.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: 不同 LLM 和 LMM 微调的详细超参数。'
- en: '| Model | LLaMA2-7B | LLaMA2-13B | LLaMA3-8B | LLaVA-1.5-7B |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA2-7B | LLaMA2-13B | LLaMA3-8B | LLaVA-1.5-7B |'
- en: '| Epoch | 3 | 3 | 3 | 1 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 训练周期 | 3 | 3 | 3 | 1 |'
- en: '| Learning Rate | 3 | 3 | 3 | 1 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 3 | 3 | 3 | 1 |'
- en: '| Batch Size (Per GPU) | 8 | 4 | 8 | 2 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 批量大小（每 GPU） | 8 | 4 | 8 | 2 |'
- en: '| Gradient Accumulation | 1 | 2 | 1 | 64 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 梯度累积 | 1 | 2 | 1 | 64 |'
- en: '| Warmup Ratio | 0.01 | 0.01 | 0.01 | 0.03 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 热身比率 | 0.01 | 0.01 | 0.01 | 0.03 |'
- en: '| Optimizer | AdamW | AdamW | AdamW | AdamW |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 优化器 | AdamW | AdamW | AdamW | AdamW |'
- en: '| LoRA Rank $r$ | 16 | 16 | 16 | 128 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| LoRA 排名 $r$ | 16 | 16 | 16 | 128 |'
- en: '| LoRA Dropout | 0 | 0 | 0 | 0.05 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| LoRA 丢弃率 | 0 | 0 | 0 | 0.05 |'
- en: '| LoRA Target | $W_{q},W_{v}$ |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| LoRA 目标 | $W_{q},W_{v}$ |'
- en: '| Learning Rate | 1$e^{-4}$ |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1$e^{-4}$ |'
- en: Appendix C Activation Distribution Visualization
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 激活分布可视化
- en: 'We visualize the magnitude of the activation of fine-tuned LLaMA2-7B using
    LoRA and RoLoRA in Figure [7](#A3.F7 "Figure 7 ‣ Appendix C Activation Distribution
    Visualization ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization"). The visualizations reveal a noticeable amount of outliers presented
    in the LoRA fine-tuned model, but are highly eliminated in RoLoRA counterpart.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [7](#A3.F7 "Figure 7 ‣ Appendix C Activation Distribution Visualization
    ‣ RoLoRA: Fine-tuning Rotated Outlier-free LLMs for Effective Weight-Activation
    Quantization") 中可视化了使用 LoRA 和 RoLoRA 对 LLaMA2-7B 进行微调后的激活量。可视化结果揭示了在 LoRA 微调模型中存在显著数量的异常值，但在
    RoLoRA 对应模型中被有效消除。'
- en: '![Refer to caption](img/7e0d1683e37aac51b1dc395391226a29.png)![Refer to caption](img/761c6c2a6c68947cef0f467d528f8516.png)![Refer
    to caption](img/a995724a91e279744e7292e080aaf73b.png)![Refer to caption](img/0afb8891443f47fbed639199c8a8439c.png)![Refer
    to caption](img/3a0864878a767aad5a00b0ed87070e5e.png)![Refer to caption](img/6fe34c22d67859aa4bae1c9ce1d6bc93.png)![Refer
    to caption](img/00e6386ec3fccc7b6987824fc63b689e.png)![Refer to caption](img/1cef0b939bd4a95b44b0cb73932a06b6.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7e0d1683e37aac51b1dc395391226a29.png)![参考说明](img/761c6c2a6c68947cef0f467d528f8516.png)![参考说明](img/a995724a91e279744e7292e080aaf73b.png)![参考说明](img/0afb8891443f47fbed639199c8a8439c.png)![参考说明](img/3a0864878a767aad5a00b0ed87070e5e.png)![参考说明](img/6fe34c22d67859aa4bae1c9ce1d6bc93.png)![参考说明](img/00e6386ec3fccc7b6987824fc63b689e.png)![参考说明](img/1cef0b939bd4a95b44b0cb73932a06b6.png)'
- en: 'Figure 7: Final activation distribution of the fine-tuned model produced using
    RoLoRA and LoRA. We select the output activation of q_proj across layers with
    the index of 0, 1, 6, 11, 16, 21, 26, 31.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 使用 RoLoRA 和 LoRA 生成的微调模型的最终激活分布。我们选择了层索引为 0、1、6、11、16、21、26、31 的 q_proj
    输出激活。'
