- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '分类: 未分类'
- en: 'date: 2024-09-08 18:50:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:50:42'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '\M: Activation-Guided Quantization for Faster Inference of LLMs on the Edge'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '\M: 激活引导量化用于边缘LLMs的更快推理'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2312.05693](https://ar5iv.labs.arxiv.org/html/2312.05693)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2312.05693](https://ar5iv.labs.arxiv.org/html/2312.05693)
- en: 'Xuan Shen¹, Peiyan Dong¹¹footnotemark: 1¹, Lei Lu¹, Zhenglun Kong¹,'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xuan Shen¹, Peiyan Dong¹¹footnotemark: 1¹, Lei Lu¹, Zhenglun Kong¹,'
- en: Zhengang Li¹, Ming Lin², Chao Wu¹, Yanzhi Wang¹ These authors contributed equally.Work
    done before joining Oracle.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zhengang Li¹, Ming Lin², Chao Wu¹, Yanzhi Wang¹ 这些作者贡献相同。工作在加入Oracle之前完成。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) stand out for their impressive performance in intricate
    language modeling tasks. However, their demanding computational and memory needs
    pose obstacles for broad use on edge devices. Quantization is then introduced
    to boost LLMs’ on-device efficiency. Recent works show that 8-bit or lower weight
    quantization is feasible with minimal impact on end-to-end task performance, while
    the activation is still not quantized. On the other hand, mainstream commodity
    edge devices still struggle to execute these sub-8-bit quantized networks effectively.
    In this paper, we propose \M, an activation-guided quantization framework for
    popular Large Language Models (LLMs), and implement an end-to-end accelerator
    on multiple edge devices for faster inference. Considering the hardware profiling
    and activation analysis, we first introduce a basic activation quantization strategy
    to balance the trade-off of task performance and real inference speed. Then we
    leverage the activation-aware token pruning technique to reduce the outliers and
    the adverse impact on attentivity. Ultimately, we utilize the SIMD-based 4-bit
    multiplier and our efficient TRIP matrix multiplication to implement the accelerator
    for LLMs on the edge. We apply our framework on different scales of LLMs including
    LLaMA, OPT, and BLOOM with 4-bit or 8-bit for the activation and 4-bit for the
    weight quantization. Experiments show that \M achieves simultaneous quantization
    of model weights and activations while maintaining task performance comparable
    to existing weight-only quantization methods. Moreover, in the 8- and 4-bit scenario,
    \M achieves an on-device speedup of up to 2.55x compared to its FP16 counterparts
    across multiple edge devices, marking a pioneering advancement in this domain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）因其在复杂语言建模任务中的卓越表现而脱颖而出。然而，它们对计算和内存的高需求对在边缘设备上的广泛使用构成了障碍。因此，引入了量化技术以提高LLMs的设备效率。近期的研究表明，8位或更低的权重量化在对端到端任务性能的影响最小的情况下是可行的，但激活仍未量化。另一方面，主流商品边缘设备在有效执行这些子8位量化网络时仍面临困难。在本文中，我们提出了\M，一个用于流行大型语言模型（LLMs）的激活引导量化框架，并在多个边缘设备上实现了一个端到端加速器，以加快推理速度。考虑到硬件分析和激活分析，我们首先介绍了一种基本的激活量化策略，以平衡任务性能和实际推理速度的权衡。然后，我们利用激活感知的令牌修剪技术来减少异常值和对注意力的负面影响。最终，我们利用基于SIMD的4位乘法器和高效的TRIP矩阵乘法来实现边缘LLMs的加速器。我们将框架应用于不同规模的LLMs，包括LLaMA、OPT和BLOOM，激活量化为4位或8位，权重量化为4位。实验表明，\M在保持与现有仅权重量化方法相当的任务性能的同时，实现了模型权重和激活的同时量化。此外，在8位和4位场景中，\M在多个边缘设备上实现了最高2.55倍的设备加速，相较于其FP16对手，这标志着该领域的开创性进展。
- en: Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Large Language Models (LLMs) (Touvron et al. [2023](#bib.bib22); Zhang et al.
    [2022](#bib.bib26); Brown et al. [2020a](#bib.bib2); Radford et al. [2019](#bib.bib18);
    Brown et al. [2020b](#bib.bib3)) based on the Transformer (Vaswani et al. [2017](#bib.bib23))
    family have breakthrough performance in Natural Language Processing (NLP) research
    area.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer（Vaswani et al. [2017](#bib.bib23)）家族的大型语言模型（LLMs）（Touvron et al.
    [2023](#bib.bib22); Zhang et al. [2022](#bib.bib26); Brown et al. [2020a](#bib.bib2);
    Radford et al. [2019](#bib.bib18); Brown et al. [2020b](#bib.bib3)）在自然语言处理（NLP）研究领域取得了突破性的表现。
- en: Application Scenarios. In real-world decision scenarios, incorporating LLMs
    inference as a crucial element often necessitates stringent latency requirements.
    However, one drawback of LLMs is their computational and storage cost, which ranks
    among the highest for known models. Consider GPT3-175B as an example. When stored
    in a compact float16 format, its parameters require 326GB (in multiples of 1024)
    of memory. This surpasses the capacity of even the most powerful individual GPUs,
    not to mention the challenges of running it on hardware-limited edge devices with
    acceptable latency. Quantization, in particular, offers a promising approach to
    substantially improve the inference throughput and energy efficiency of LLMs on
    edge devices. This improvement is achieved by harnessing the highly effective
    8-bit fixed-point (INT8) operations supported by the SIMD units that are commonly
    found in edge platforms, such as CPUs and Raspberry Pis.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 应用场景。在实际的决策场景中，将大型语言模型（LLMs）的推理作为关键要素时，通常需要严格的延迟要求。然而，大型语言模型的一个缺点是其计算和存储成本，这在已知模型中排名最高。以GPT3-175B为例。当以紧凑的float16格式存储时，其参数需要326GB（以1024的倍数计）的内存。这超过了即使是最强大的单个GPU的容量，更不用说在硬件受限的边缘设备上以可接受的延迟运行时面临的挑战。量化，特别是，通过利用边缘平台（如CPU和Raspberry
    Pi）上常见的SIMD单元支持的高效8位定点（INT8）操作，提供了一种有前景的方法，能够显著提高LLMs在边缘设备上的推理吞吐量和能效。
- en: Current Limitations. Before fully realizing the on-device benefits of model
    quantization on LLMs, it’s crucial to address two pressing issues that demand
    careful attention. ❶ Existing works (Frantar et al. [2022](#bib.bib9); Lin et al.
    [2023](#bib.bib15); Xiao et al. [2022](#bib.bib25)) primarily concentrate on weight-only
    (4-bit) quantization while leaving activations in the floating-point (FP16) domain.
    This approach limits the efficient speed-up of model inference on common edge
    devices, which typically only support 16x16 and 8x8 integer multipliers. Specifically,
    activation quantization often has a detrimental effect on task performance, especially
    when the model size becomes large, due to the emergence of pronounced outliers
    in activations. Experiments done by work (Dettmers et al. [2022](#bib.bib5)) indicate
    that directly setting these outliers to zero can result in a substantial 45% degradation
    in task performance. Additionally, given the large model size of LLMs, limited
    academic computing power makes it challenging to afford the associated training
    costs. Consequently, Post-Training Quantization (PTQ) has become a prevalent approach,
    but it falls short of minimizing the quantization error caused by these outliers.
    In summary, quantizing the activations of LLMs while handling outliers inside
    activations is a crucial yet challenging issue. ❷ Mainstream edge processors,
    such as CPUs and Raspberry Pis, leverage SIMD units to execute multiple operations
    in parallel efficiently. SIMD instructions are adept at exploiting byte-level
    data (8-bit integers) parallelism and are well-supported in common ISAs (Instruction
    Set Architectures) and DNN processing frameworks. Examples include GEMMLOWP (Jacob
    and Warden [2017](#bib.bib11)) in TensorFlow Lite and QNNPACK (Dukhan, Wu, and
    Lu [2018](#bib.bib8)) in PyTorch. Their low-precision kernels merely zero-extend
    the sub-byte operands to align them with byte boundaries, treating them as 8-bit
    or 16-bit operands.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当前限制。在充分实现模型量化在LLMs上的设备内好处之前，必须解决两个迫切问题，需要认真对待。❶ 现有工作（Frantar等人 [2022](#bib.bib9);
    Lin等人 [2023](#bib.bib15); Xiao等人 [2022](#bib.bib25)）主要集中于仅对权重进行4位量化，同时将激活保留在浮点（FP16）域。这种方法限制了在常见边缘设备上高效加速模型推理的能力，而这些设备通常仅支持16x16和8x8整数乘法器。特别是，激活量化往往对任务性能产生负面影响，尤其是当模型规模较大时，由于激活中出现明显的异常值。由Dettmers等人
    [2022](#bib.bib5) 进行的实验表明，直接将这些异常值设置为零可能导致任务性能下降45%。此外，由于LLMs的大模型规模，有限的学术计算能力使得承受相关训练成本变得困难。因此，后训练量化（PTQ）已成为一种流行的方法，但它未能最小化这些异常值造成的量化误差。总之，在处理激活中的异常值时对LLMs进行激活量化是一个关键但具有挑战性的问题。❷
    主流边缘处理器，如CPU和Raspberry Pi，利用SIMD单元高效地并行执行多个操作。SIMD指令擅长利用字节级数据（8位整数）并行性，并在常见的ISA（指令集架构）和DNN处理框架中得到了良好支持。例如，在TensorFlow
    Lite中的GEMMLOWP（Jacob和Warden [2017](#bib.bib11)）和在PyTorch中的QNNPACK（Dukhan、Wu和Lu
    [2018](#bib.bib8)）。它们的低精度内核仅将子字节操作数零扩展到与字节边界对齐，将其视为8位或16位操作数。
- en: 'In this paper, we address the above on-device quantization issues while enjoying
    the powerful performance provided by LLMs. We propose \M, an activation-guided
    quantization framework for faster inference of LLMs on the edge. Specifically,
    we begin with a fundamental activation quantization strategy based on hardware
    latency profiling and activation analysis of LLMs, aiming to strike a balance
    between task performance and on-device inference speed. We subsequently utilize
    the activation-aware pruning method to optimize quantization. This is crucial
    because quantized tokens often exhibit numerous outliers, causing their attention
    to shift from the first position to nearby local positions. By pruning tokens,
    we effectively eliminate some outliers, as they typically concentrate within the
    same or adjacent channels of different tokens. Also, the removal of inattentive
    tokens can reduce the interaction distance between important tokens. Finally,
    we design the edge-oriented optimization for the hardware implementation of \M.
    It consists primarily of two components: a SIMD-based 4-bit multiplier to facilitate
    efficient 4x4 INT4 multiplication, and our efficient Two-Refine Improved by Pruning
    (TRIP) matrix multiplication designed to mitigate the adverse impact of outliers.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们解决了上述设备上的量化问题，同时享受了LLMs提供的强大性能。我们提出了\M，一种激活引导的量化框架，用于加快LLMs在边缘的推理。具体来说，我们从基于硬件延迟分析和LLMs激活分析的基本激活量化策略开始，旨在平衡任务性能和设备推理速度。然后，我们利用激活感知的剪枝方法来优化量化。这一点至关重要，因为量化的标记通常表现出许多异常值，使得它们的注意力从第一个位置转移到附近的局部位置。通过剪枝标记，我们有效地消除了一些异常值，因为它们通常集中在不同标记的相同或相邻通道内。此外，去除不专注的标记可以减少重要标记之间的交互距离。最后，我们为\M的硬件实现设计了面向边缘的优化。它主要包括两个组件：一个基于SIMD的4位乘法器，以促进高效的4x4
    INT4乘法，以及我们高效的两次改进剪枝（TRIP）矩阵乘法，旨在减轻异常值的负面影响。
- en: The popular LLMs models such as LLaMA (Touvron et al. [2023](#bib.bib22)), OPT (Zhang
    et al. [2022](#bib.bib26)), and BLOOM (Scao et al. [2022](#bib.bib20)) are adopted
    to verify the effectiveness of our framework and the efficiency of our method
    on multiple edge devices. \M can maintain state-of-the-art task performance comparable
    with weight-only works while achieving practical on-device speedup up to 2.55x.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 流行的LLMs模型如LLaMA（Touvron等 [2023](#bib.bib22)）、OPT（Zhang等 [2022](#bib.bib26)）和BLOOM（Scao等
    [2022](#bib.bib20)）被用来验证我们框架的有效性和我们方法在多个边缘设备上的效率。\M可以保持与仅权重方法相当的最先进的任务性能，同时实现高达2.55倍的实际设备加速。
- en: 'The contributions of this work are summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的贡献总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design the activation-guided and edge-oriented quantization strategy for
    the balance of latency decreasing and task performance.
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了激活引导和边缘导向的量化策略，以平衡延迟减少和任务性能。
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We design an activation-aware token pruning method to minimize the negative
    impact on task performance caused by the outliers and the local attentivity.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们设计了一种激活感知的标记剪枝方法，以最小化异常值和局部注意力对任务性能的负面影响。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose the SIMD-based 4-bit multiplier and an efficient TRIP matrix multiplication
    for effective hardware implementation.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了基于SIMD的4位乘法器和高效的TRIP矩阵乘法，以实现有效的硬件实现。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We achieve state-of-the-art task performance on several popular datasets with
    practical on-device speedup.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在多个流行数据集上实现了最先进的任务性能，并提供了实际的设备加速。
- en: Background and Related Works
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 背景和相关工作
- en: In this section, we first focus on the backgound of post-training quantization
    for LLMs. Then we discuss the low-bit computation on general edge devices.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先关注大语言模型（LLMs）的后训练量化背景。然后讨论一般边缘设备上的低位计算。
- en: Post-Training Quantization for LLMs
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 后训练量化用于LLMs
- en: Post-Training Quantization (PTQ) techniques are widely used for one-shot compressing
    models, particularly for Large Language Models (LLMs), given the high cost of
    retraining. These PTQ methods employ accurate solvers to address compression challenges
    on a per-layer or per-group basis, relying on a limited set of calibration data.
    Notably, recent advances in PTQ, like GPTQ (Frantar et al. [2022](#bib.bib9)),
    AWQ (Lin et al. [2023](#bib.bib15)), and SpQR (Dettmers et al. [2023](#bib.bib6)),
    have introduced well-crafted approaches capable of preserving LLM performance
    effectively. GPTQ leverages second-order information to correct errors, achieving
    commendable accuracy within a 3-4 bit range. AWQ proposes safeguarding only 1%
    of crucial weights to substantially diminish quantization errors. SpQR’s focus
    is on reducing quantization to 3-4 bits per parameter for smaller models. Moreover,
    they put forth a novel technique enabling nearly lossless compression of LLMs.
    Nonetheless, these works fall short of achieving practical inference acceleration
    on edge devices, as the activation part persists in a floating-point format, rendering
    the integer multiplier of the edge devices ineffective.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 后训练量化（PTQ）技术广泛用于一次性压缩模型，特别是对于大型语言模型（LLMs），因为重新训练的成本很高。这些PTQ方法采用精确的求解器，以层或组为单位解决压缩挑战，依赖于有限的校准数据集。值得注意的是，PTQ的最新进展，如GPTQ（Frantar等人
    [2022](#bib.bib9)）、AWQ（Lin等人 [2023](#bib.bib15)）和SpQR（Dettmers等人 [2023](#bib.bib6)），提出了精心设计的方法，能够有效地保持LLM的性能。GPTQ利用二阶信息来修正错误，在3-4位范围内实现了令人称赞的准确性。AWQ建议只保护1%的关键权重，以大幅减少量化误差。SpQR的重点是将量化减少到每个参数的3-4位，以适应较小的模型。此外，他们提出了一种新技术，使LLM的压缩几乎无损。然而，这些工作在边缘设备上未能实现实际的推理加速，因为激活部分仍以浮点格式存在，导致边缘设备的整数乘法器无效。
- en: Low-Bit Computation on Hardware Devices
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件设备上的低位计算
- en: 'Low-precision linear algebra kernels aim to maximize computing throughput on
    low-precision operands. This is achieved by extending existing wider bit-width
    linear algebra kernels. The use of lower-precision operands brings about two performance
    enhancements: increased cache capacity and the ability to leverage lower-precision
    SIMD instructions for processing multiple elements simultaneously. Pioneering
    examples of these low-precision linear algebra kernels, e.g., Google’s GEMMLOWP (Jacob
    and Warden [2017](#bib.bib11)) and Facebook’s QNNPACK (Dukhan, Wu, and Lu [2018](#bib.bib8)),
    excel at enhancing the efficiency of DNN inference when employing 8-bit quantization.
    However, pushing for more aggressive sub-byte quantization yields no added performance
    benefits due to the fact that mainstream CPUs solely support SIMD operations with
    a precision of 8 bits or wider. In specific, low-precision kernels essentially
    expand sub-byte operands to 8 bits and process them accordingly. Furthermore,
    the concept of Bit-serial computation emerges as a promising solution for data-parallel
    computation with sub-byte values. This approach involves sequentially processing
    each bit of two operands during multiplication, while simultaneously managing
    multiple operand pairs in parallel. Nonetheless, its practical implementation
    necessitates the popcount operation, which inherently limits runtime throughput.
    As a result, this method only presents significant advantages in ultra-low-bit
    scenarios (1 or 2 bits).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 低精度线性代数内核旨在最大化在低精度操作数上的计算吞吐量。这通过扩展现有的更宽位宽线性代数内核来实现。使用低精度操作数带来了两种性能提升：增加了缓存容量以及能够利用低精度SIMD指令同时处理多个元素。这些低精度线性代数内核的先锋实例，如谷歌的GEMMLOWP（Jacob和Warden
    [2017](#bib.bib11)）和Facebook的QNNPACK（Dukhan, Wu和Lu [2018](#bib.bib8)），在采用8位量化时在增强DNN推理效率方面表现出色。然而，推动更激进的子字节量化并没有带来额外的性能提升，因为主流CPU仅支持8位或更宽的SIMD操作。具体来说，低精度内核实质上将子字节操作数扩展到8位并相应地处理它们。此外，比特串行计算的概念作为一种有前景的解决方案出现，适用于具有子字节值的数据并行计算。这种方法涉及在乘法过程中逐位处理两个操作数，同时并行处理多个操作数对。然而，其实际实施需要popcount操作，这本质上限制了运行时吞吐量。因此，这种方法仅在超低位场景（1或2位）中表现出显著的优势。
- en: Activation Analysis of LLMs
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大语言模型（LLMs）的激活分析
- en: In this section, we analyze the attentivity of tokens in LLMs and the influence
    of token pruning on activation quantization. Besides, we deliver the latency profiling
    to analyze potential quantization strategy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们分析了大语言模型（LLMs）中标记的注意力以及标记剪枝对激活量化的影响。此外，我们还进行延迟剖析，以分析潜在的量化策略。
- en: '![Refer to caption](img/d53861d8bb3214428ecbf23d94dd9841.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d53861d8bb3214428ecbf23d94dd9841.png)'
- en: 'Figure 1: The (a), (b), and (c) shows attention maps with 16 tokens in the
    first and last layer of the model. The activation is not quantized in (a) and
    (b), while it is quantized in (c). The (d) shows the distribution of outliers
    in one activation with 2048 tokens. The visualization is based on the LLaMA-7B
    model with the Wikitext-2 dataset.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：图（a）、（b）和（c）显示了模型第一层和最后一层中16个标记的注意力图。在（a）和（b）中，激活未被量化，而在（c）中，激活已被量化。图（d）显示了2048个标记中一个激活的离群值分布。该可视化基于LLaMA-7B模型和Wikitext-2数据集。
- en: Token Importance in LLMs
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LLMs中的标记重要性
- en: In natural language processing, numerous non-essential words often exist within
    sentences, contributing little to the overall comprehension. This implies that
    we can efficiently process these words using fewer resources, potentially even
    excluding them, in order to mitigate complexity. As words are embedded into tokens
    in language models, we explore the attention mechanism to analyze the importance
    of each token. The previous works (Kong et al. [2022](#bib.bib13); Dong et al.
    [2023](#bib.bib7)) focus on the attention map in the transformer architectures.
    The attention probabilities are then accumulated across multiple rounds of attention
    as token importance scores. However, the causal attention masks used in LLMs ensure
    that, during the self-attention mechanism, each token can only interact with previous
    tokens instead of the following ones. Thus, this causal mechanism makes the accumulated
    probabilities not appropriate to the evaluation of token importance because of
    its unfair for the accumulated probabilities of former tokens.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理过程中，句子中常常存在许多非必要的词，这些词对整体理解贡献甚微。这意味着我们可以使用更少的资源高效处理这些词，甚至可能排除它们，以降低复杂性。由于词汇被嵌入到语言模型中的标记中，我们探索注意力机制来分析每个标记的重要性。以往的研究（Kong
    et al. [2022](#bib.bib13); Dong et al. [2023](#bib.bib7)）集中于变换器架构中的注意力图。然后将注意力概率在多个回合中累积作为标记重要性分数。然而，LLMs中使用的因果注意力掩码确保在自注意力机制期间，每个标记只能与之前的标记互动，而不能与之后的标记互动。因此，这种因果机制使得累积概率不适用于标记重要性的评估，因为它对前面标记的累积概率不公平。
- en: 'In LLMs, a distinct start token is placed at the beginning of the input sequence.
    The start token has a role in initializing the hidden layers and defining token
    positions within the sequence. These aspects are vital for producing text that
    is both coherent and contextually meaningful. To explore the relationship between
    the first start token and other tokens, we visualize the attention map at the
    first and last layer of the LLaMA-7B model with 16 tokens on the Wikitext-2 dataset
    in Figure [1](#Sx3.F1 "Figure 1 ‣ Activation Analysis of LLMs ‣ \M: Activation-Guided
    Quantization for Faster Inference of LLMs on the Edge") (a) and (b). According
    to the attention map, several tokens in the first layer demonstrate a shared triangular
    pattern, indicating that tokens tend to the adjacent positions, especially the
    previous position. While in the last layer, nearly all tokens share a vertical-stripe
    pattern, indicating that tokens all related with the first token. Then we explore
    the attention maps in the middle layers, showing that these maps are similar to
    the one in the last layer. Thus, it guides us to build the connection between
    the token importance and token attentivity to the start token.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '在大型语言模型中，一个独特的起始标记被放置在输入序列的开头。起始标记在初始化隐藏层和定义序列中标记的位置方面发挥作用。这些方面对于生成既连贯又具有语境意义的文本至关重要。为了探讨第一个起始标记与其他标记之间的关系，我们在图[1](#Sx3.F1
    "图 1 ‣ 激活分析 ‣ \M: 激活引导量化以加速LLMs在边缘的推断")（a）和（b）中可视化了LLaMA-7B模型在Wikitext-2数据集上16个标记的第一层和最后一层的注意力图。根据注意力图，第一层的几个标记显示出共享的三角形模式，表明标记倾向于相邻位置，特别是前一个位置。而在最后一层，几乎所有标记共享垂直条纹模式，表明所有标记都与第一个标记相关。然后我们探索中间层的注意力图，显示这些图与最后一层的图类似。因此，这指导我们建立标记重要性与标记对起始标记的关注度之间的联系。'
- en: Influence of Activation Quantization
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活量化的影响
- en: 'We analyze the distribution of outliers and visualize outlier in different
    channels in Figure [1](#Sx3.F1 "Figure 1 ‣ Activation Analysis of LLMs ‣ \M: Activation-Guided
    Quantization for Faster Inference of LLMs on the Edge") (d). We notice that the
    outliers are distributed in adjacent or even the same channels, since several
    straight lines with deep colors indicates that the channel index of the outliers
    unchange. Also, the attention map, which is generated by the query Q and key matrix
    K, can be influenced by the activation quantization as it is input-dependent.
    We visualize the quantized attention map at the last layer in Figure [1](#Sx3.F1
    "Figure 1 ‣ Activation Analysis of LLMs ‣ \M: Activation-Guided Quantization for
    Faster Inference of LLMs on the Edge") (c). The attention map shows a triangular
    pattern and the quantized tokens attend to the adjacent positions rather than
    the start token, demonstrating that the attention range becomes locality and the
    attentivity turns much weaker. This change implies a deterioration in the globality
    of representative features. From another perspective, the information reduction
    of the original attention map caused by quantization error will impact the final
    task performance adversely.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '我们分析了异常值的分布，并在图 [1](#Sx3.F1 "图 1 ‣ LLM 的激活分析 ‣ \M: 激活引导量化以加快边缘 LLM 推理") (d)
    中可视化了不同通道中的异常值。我们注意到异常值分布在相邻甚至相同的通道中，因为几条深色的直线表明异常值的通道索引不变。此外，由查询 Q 和键矩阵 K 生成的注意力图会受到激活量化的影响，因为它是输入依赖的。我们在图
    [1](#Sx3.F1 "图 1 ‣ LLM 的激活分析 ‣ \M: 激活引导量化以加快边缘 LLM 推理") (c) 中可视化了最后一层的量化注意力图。注意力图显示出三角形模式，量化的
    token 更关注相邻的位置而非起始 token，表明注意力范围变为局部，注意力变得较弱。这一变化意味着代表特征的全球性恶化。从另一个角度来看，由量化误差引起的原始注意力图的信息减少将对最终任务性能产生不利影响。'
- en: Latency Profiling on Hardware Devices
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 硬件设备上的延迟剖析
- en: '![Refer to caption](img/ed07b90a3eed9afdf05f7946f12247ee.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/ed07b90a3eed9afdf05f7946f12247ee.png)'
- en: 'Figure 2: Mobile Device profiling of one LLaMA block.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：LLaMA 块的移动设备剖析。
- en: 'To gain a deeper insight into the runtime distribution of LLMs, we conducted
    profiling on a widely used model, LLaMA, utilizing the on-board Snapdragon 870
    CPU, as shown in Figure [2](#Sx3.F2 "Figure 2 ‣ Latency Profiling on Hardware
    Devices ‣ Activation Analysis of LLMs ‣ \M: Activation-Guided Quantization for
    Faster Inference of LLMs on the Edge"). This profiling includes FP16, INT8, and
    INT4 precisions. Since nonlinear operators (LayerNorm/Softmax/SwiGLU) contribute
    a relatively smaller portion of latency, i.e., $<$ QK. In essence, focusing on
    low-bit quantization of both weights and activations for LLMs while ensuring task
    performance is crucial.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '为了深入了解 LLM 的运行时分布，我们对一个广泛使用的模型 LLaMA 进行了剖析，利用了板载的 Snapdragon 870 CPU，如图 [2](#Sx3.F2
    "图 2 ‣ 硬件设备上的延迟剖析 ‣ LLM 的激活分析 ‣ \M: 激活引导量化以加快边缘 LLM 推理") 所示。该剖析包括 FP16、INT8 和
    INT4 精度。由于非线性算子（LayerNorm/Softmax/SwiGLU）对延迟的贡献相对较小，即 $<$ QK。本质上，在确保任务性能的同时，关注
    LLM 的权重和激活的低位量化是至关重要的。'
- en: Methodology
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方法论
- en: We explain the activation quantization pipeline here and propose the activation-guided
    framework for the optimization of quantization. Also, we explain our hardware
    implementation of the 4-bit multiplier.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里解释了激活量化流程，并提出了用于优化量化的激活引导框架。同时，我们还解释了4位乘法器的硬件实现。
- en: Preliminary
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初步
- en: 'We here explain the quantizers we use for activation quantization in this work.
    We assume the bit-width used in quantization is $b$:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里解释了我们在此工作中用于激活量化的量化器。我们假设量化中使用的位宽是 $b$：
- en: '|  | $q=\left\{\begin{array}[]{lr}\{-2^{b-1},...,2^{b-1}-1\},&amp;\text{Signed}\\
    \{0,1,...,2^{b}-1\},&amp;\text{Unsigned}\end{array}\right.$ |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $q=\left\{\begin{array}[]{lr}\{-2^{b-1},...,2^{b-1}-1\},&amp;\text{有符号}\\
    \{0,1,...,2^{b}-1\},&amp;\text{无符号}\end{array}\right.$ |  | (1) |'
- en: There are various kinds of quantizers $Q(X|b)$, and the uniform quantizer (Jacob
    et al. [2018](#bib.bib10)) and the log2 quantizer (Cai, Takemoto, and Nakajo [2018](#bib.bib4))
    are widely used. In our work, we mainly use these two quantizers for activation
    quantization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种量化器 $Q(X|b)$，其中均匀量化器 (Jacob 等人 [2018](#bib.bib10)) 和 log2 量化器 (Cai, Takemoto
    和 Nakajo [2018](#bib.bib4)) 被广泛使用。在我们的工作中，我们主要使用这两种量化器进行激活量化。
- en: Unifrom Quantization has been supported by most hardware devices.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数硬件设备已支持均匀量化。
- en: '|  | $Q(X&#124;b)=\text{CLIP}(\lfloor\frac{X}{s}\rceil+zp,0,2^{b}-1)$ |  |
    (2) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(X&#124;b)=\text{CLIP}(\lfloor\frac{X}{s}\rceil+zp,0,2^{b}-1)$ |  |
    (2) |'
- en: The $s$ denote the scale and zero-point separately.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: $s$ 分别表示尺度和零点。
- en: Log2 Quantization imports the exponential operation into the linear quantization
    process.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Log2 量化将指数操作引入线性量化过程。
- en: '|  | $Q(X&#124;b)=\text{Sign(}X)\cdot\text{CLIP}(\lfloor-\text{log}_{2}\frac{X}{\text{max}(&#124;X&#124;)}\rceil,0,2^{b-1}-1)$
    |  | (3) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q(X&#124;b)=\text{Sign(}X)\cdot\text{CLIP}(\lfloor-\text{log}_{2}\frac{X}{\text{max}(&#124;X&#124;)}\rceil,0,2^{b-1}-1)$
    |  | (3) |'
- en: Activation Quantization Pipeline
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活量化流程
- en: '![Refer to caption](img/4e39abe401ddd0f1a30863f36a524230.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/4e39abe401ddd0f1a30863f36a524230.png)'
- en: 'Figure 3: Activation Quantization Pipeline.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 激活量化流程。'
- en: 'We present our activation quantization pipeline in Figure [3](#Sx4.F3 "Figure
    3 ‣ Activation Quantization Pipeline ‣ Methodology ‣ \M: Activation-Guided Quantization
    for Faster Inference of LLMs on the Edge"). While the embedding process, output
    module, and the yellow-highlighted nonlinear operations contribute relatively
    small proportions during model inference, we preserve their computation without
    alteration. Our primary focus is optimizing the matrix multiplication operations,
    constituting the largest share of the inference latency.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在图 [3](#Sx4.F3 "图 3 ‣ 激活量化流程 ‣ 方法论 ‣ \M: 激活引导量化以加速边缘上的LLM推理") 中展示了我们的激活量化流程。虽然嵌入过程、输出模块以及黄色高亮的非线性操作在模型推理过程中贡献相对较小，我们保留了它们的计算而没有更改。我们的主要关注点是优化矩阵乘法操作，因为它们占据了推理延迟的最大份额。'
- en: Within our pipeline, we target the acceleration of computations occurring in
    the self-attention and MLP modules, as indicated by the blue shading. Specifically,
    we perform activation quantization predominantly using 8-bit integers. However,
    we observe that specific activations following the self-attention mechanism can
    be quantized using 4-bit integers, resulting in further acceleration while upholding
    task performance standards. Accordingly, we propose our innovative 4-bit multiplier
    to effectively support the INT4 matrix multiplication.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的流程中，我们着重加速自注意力和 MLP 模块中发生的计算，如蓝色阴影所示。具体而言，我们主要使用 8 位整数进行激活量化。然而，我们观察到自注意力机制之后的特定激活可以使用
    4 位整数进行量化，从而进一步加速，同时保持任务性能标准。因此，我们提出了创新的 4 位乘法器，以有效支持 INT4 矩阵乘法。
- en: Activation-Guided Optimization
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 激活引导优化
- en: 'Based on the analysis of outliers and attention range in the previous section,
    it is intuitive for us to import the token pruning here for optimization. We visualize
    the token pruning process in Figure [4](#Sx4.F4 "Figure 4 ‣ Activation-Guided
    Optimization ‣ Methodology ‣ \M: Activation-Guided Quantization for Faster Inference
    of LLMs on the Edge"). Token pruning can reduce the outliers, which can decrease
    the quantization error caused by them. Token pruning can also reduce the distance
    between attentive tokens to help the model capture more features.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '基于上一节对异常值和注意力范围的分析，我们直观地认为在此引入令牌修剪以进行优化是合适的。我们在图 [4](#Sx4.F4 "图 4 ‣ 激活引导优化
    ‣ 方法论 ‣ \M: 激活引导量化以加速边缘上的LLM推理") 中可视化了令牌修剪过程。令牌修剪可以减少异常值，从而降低由它们引起的量化误差。令牌修剪还可以缩短注意力令牌之间的距离，帮助模型捕捉更多特征。'
- en: We first introduce the activation-aware token pruning improved activation quantization
    method we use in this work. Inspired by the work (Lin et al. [2022](#bib.bib16)),
    we propose the Two-Refine Improved by Pruning (TRIP) method here to address the
    difficulties in activation quantization.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍在这项工作中使用的激活感知令牌修剪改进的激活量化方法。受 (Lin et al. [2022](#bib.bib16)) 的启发，我们提出了“两次精化改进修剪（TRIP）”方法，以解决激活量化中的难点。
- en: For the one activation in transformer-based models, we assume it as $X\in\mathbb{R}^{m\times
    d}$.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于变换器的模型中的一个激活，我们假设它为 $X\in\mathbb{R}^{m\times d}$。
- en: '|  | $X^{P}=F^{P}(X)\in\mathbb{R}^{n\times d},n80%
    computation). Hence, transformers can obtain more speedup gain than CNN, whose
    memory movement, such as weight reshaping, image2column, and column2image under
    different data layouts in each convolution operation, can dilute the efficient
    quantization speedup on GeMM. Combined with the 4-bit compression and concatenation
    technique, Agile-Quant-4 can further improve this advantage, achieving approximately
    1.75x acceleration compared to INT8 multiplication. This is because, while the
    theoretical, computational workload is halved, overhead is introduced due to internal
    shifts of concatenated weights and the recovery of stored results in INT8 format.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '根据表 [4](#Sx5.T4 "Table 4 ‣ End-to-end Performance and Analysis ‣ Experiments
    and Results ‣ \M: Activation-Guided Quantization for Faster Inference of LLMs
    on the Edge")，可以得出以下结论：\M 可以根据模型带来 2.3 倍到 2.6 倍的整体加速，因为边缘设备上的高计算负载可以从量化算法下有限内存资源的更高利用效率中受益。具体来说，Agile-Quant-8
    将激活量化为 INT8 精度，并且在 GeMM 中相对于 FP16 可以实现约 1.8 倍到 1.9 倍的加速。这对像变换器这样的模型非常有利，因为大多数数学运算是矩阵运算（80% 计算）。因此，变换器可以获得比
    CNN 更多的加速增益，因为 CNN 的内存移动，如权重重塑、image2column 和 column2image，在每个卷积操作中的不同数据布局下，可以稀释
    GeMM 上高效的量化加速。结合 4 位压缩和拼接技术，Agile-Quant-4 可以进一步提升这一优势，相对于 INT8 乘法实现约 1.75 倍的加速。这是因为虽然理论上计算负载减少了一半，但由于拼接权重的内部移位和以
    INT8 格式恢复存储结果，引入了开销。'
- en: Conclusions and Limitations
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论与局限性
- en: In this paper, we propose \M, an activation-guided quantization framework for
    popular LLMs, and design an end-to-end accelerator on multiple edge devices. Considering
    the hardware profiling and activation analysis, we introduce the quantization
    strategy on model weights and activations. We also implement the activation-aware
    token pruning to enhance the activation quantization. Finally, we introduce the
    SIMD-based 4-bit multiplier and efficient TRIP matrix multiplication to implement
    the accelerator for LLMs on Android CPU and Raspberry Pi, reaching up to 2.55x
    speedup. Our next step is to explore the lower-bit LLMs and design the multiple
    lower-bit multipliers (2,3-bits) on edge processors to proceed with AI democratization.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了\M，一个针对流行 LLM 的激活引导量化框架，并设计了一个针对多个边缘设备的端到端加速器。考虑到硬件分析和激活分析，我们引入了模型权重和激活的量化策略。我们还实现了激活感知的令牌剪枝，以增强激活量化。最后，我们引入了基于
    SIMD 的 4 位乘法器和高效的 TRIP 矩阵乘法，以在 Android CPU 和 Raspberry Pi 上实现 LLM 的加速器，实现高达 2.55
    倍的加速。我们的下一步是探索低位 LLM，并在边缘处理器上设计多个低位乘法器（2、3 位），以推动人工智能的普及。
- en: References
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: ARM (2023) ARM. 2023. A collection of low-level machine learning functions optimized
    with SIMD technologies. https://arm-software.github.io/ComputeLibrary/v22.05/.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ARM (2023) ARM. 2023. 一组使用 SIMD 技术优化的低级机器学习函数。 https://arm-software.github.io/ComputeLibrary/v22.05/。
- en: 'Brown et al. (2020a) Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
    Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; et al. 2020a.
    Language models are few-shot learners. *NeurIPS*, 33: 1877–1901.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Brown 等 (2020a) Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;
    Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; 等. 2020a. 语言模型是少样本学习者。*NeurIPS*,
    33: 1877–1901。'
- en: Brown et al. (2020b) Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan,
    J.; Dhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal,
    S.; Herbert-Voss, A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler,
    D. M.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray,
    S.; Chess, B.; Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever,
    I.; and Amodei, D. 2020b. Language Models are Few-Shot Learners.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等（2020b）Brown, T. B.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J.; Dhariwal,
    P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-Voss,
    A.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.; Ziegler, D. M.; Wu, J.;
    Winter, C.; Hesse, C.; Chen, M.; Sigler, E.; Litwin, M.; Gray, S.; Chess, B.;
    Clark, J.; Berner, C.; McCandlish, S.; Radford, A.; Sutskever, I.; 和 Amodei, D.
    2020b. 语言模型是少量样本学习者。
- en: Cai, Takemoto, and Nakajo (2018) Cai, J.; Takemoto, M.; and Nakajo, H. 2018.
    A Deep Look into Logarithmic Quantization of Model Parameters in Neural Networks.
    In *IAIT*.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai, Takemoto 和 Nakajo（2018）Cai, J.; Takemoto, M.; 和 Nakajo, H. 2018. 深入探讨神经网络中模型参数的对数量化。
    在 *IAIT*。
- en: 'Dettmers et al. (2022) Dettmers, T.; Lewis, M.; Belkada, Y.; and Zettlemoyer,
    L. 2022. Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
    *arXiv preprint arXiv:2208.07339*.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2022）Dettmers, T.; Lewis, M.; Belkada, Y.; 和 Zettlemoyer, L. 2022.
    Llm. int8 ()：用于大规模变换器的 8 位矩阵乘法。 *arXiv preprint arXiv:2208.07339*。
- en: 'Dettmers et al. (2023) Dettmers, T.; Svirschevski, R.; Egiazarian, V.; Kuznedelev,
    D.; Frantar, E.; Ashkboos, S.; Borzunov, A.; Hoefler, T.; and Alistarh, D. 2023.
    SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression.
    *arXiv*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers 等（2023）Dettmers, T.; Svirschevski, R.; Egiazarian, V.; Kuznedelev,
    D.; Frantar, E.; Ashkboos, S.; Borzunov, A.; Hoefler, T.; 和 Alistarh, D. 2023.
    SpQR：用于接近无损 LLM 权重压缩的稀疏量化表示。 *arXiv*。
- en: 'Dong et al. (2023) Dong, P.; Sun, M.; Lu, A.; Xie, Y.; Liu, K.; Kong, Z.; Meng,
    X.; Li, Z.; Lin, X.; Fang, Z.; et al. 2023. Heatvit: Hardware-efficient adaptive
    token pruning for vision transformers. In *HPCA*, 442–455\. IEEE.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong 等（2023）Dong, P.; Sun, M.; Lu, A.; Xie, Y.; Liu, K.; Kong, Z.; Meng, X.;
    Li, Z.; Lin, X.; Fang, Z.; 等. 2023. Heatvit：面向视觉变换器的硬件高效自适应令牌剪枝。 在 *HPCA*，442–455。
    IEEE。
- en: 'Dukhan, Wu, and Lu (2018) Dukhan, M.; Wu, Y.; and Lu, H. 2018. QNNPACK: Open
    source library for optimized mobile deep learning.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dukhan, Wu 和 Lu（2018）Dukhan, M.; Wu, Y.; 和 Lu, H. 2018. QNNPACK：用于优化移动深度学习的开源库。
- en: 'Frantar et al. (2022) Frantar, E.; Ashkboos, S.; Hoefler, T.; and Alistarh,
    D. 2022. GPTQ: Accurate Post-training Compression for Generative Pretrained Transformers.
    *arXiv*.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等（2022）Frantar, E.; Ashkboos, S.; Hoefler, T.; 和 Alistarh, D. 2022.
    GPTQ：生成预训练变换器的准确后训练压缩。 *arXiv*。
- en: Jacob et al. (2018) Jacob, B.; Kligys, S.; Chen, B.; Zhu, M.; Tang, M.; Howard,
    A.; Adam, H.; and Kalenichenko, D. 2018. Quantization and training of neural networks
    for efficient integer-arithmetic-only inference. In *CVPR*, 2704–2713.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob 等（2018）Jacob, B.; Kligys, S.; Chen, B.; Zhu, M.; Tang, M.; Howard, A.;
    Adam, H.; 和 Kalenichenko, D. 2018. 神经网络的量化与训练以实现高效的整数算术推理。 在 *CVPR*，2704–2713。
- en: 'Jacob and Warden (2017) Jacob, B.; and Warden, P. 2017. gemmlowp: A small self-contained
    low-precision gemm library. *Retrieved June*, 14: 2018.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Jacob 和 Warden（2017）Jacob, B.; 和 Warden, P. 2017. gemmlowp：一个小型自包含低精度 gemm
    库。 *Retrieved June*，14: 2018。'
- en: Kim et al. (2022) Kim, S.; Shen, S.; Thorsley, D.; Gholami, A.; Kwon, W.; Hassoun,
    J.; and Keutzer, K. 2022. Learned token pruning for transformers. In *KDD*, 784–794.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等（2022）Kim, S.; Shen, S.; Thorsley, D.; Gholami, A.; Kwon, W.; Hassoun,
    J.; 和 Keutzer, K. 2022. 针对变换器的学习型令牌剪枝。 在 *KDD*，784–794。
- en: 'Kong et al. (2022) Kong, Z.; Ma, H.; Yuan, G.; Sun, M.; Xie, Y.; Dong, P.;
    Meng, X.; Shen, X.; Tang, H.; Qin, M.; et al. 2022. Peeling the Onion: Hierarchical
    Reduction of Data Redundancy for Efficient Vision Transformer Training. *arXiv*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kong 等（2022）Kong, Z.; Ma, H.; Yuan, G.; Sun, M.; Xie, Y.; Dong, P.; Meng, X.;
    Shen, X.; Tang, H.; Qin, M.; 等. 2022. 剥洋葱：用于高效视觉变换器训练的数据冗余分层减少。 *arXiv*。
- en: 'Liang et al. (2022) Liang, Y.; Ge, C.; Tong, Z.; Song, Y.; Wang, J.; and Xie,
    P. 2022. Not all patches are what you need: Expediting vision transformers via
    token reorganizations. *arXiv*.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang 等（2022）Liang, Y.; Ge, C.; Tong, Z.; Song, Y.; Wang, J.; 和 Xie, P. 2022.
    并非所有补丁都是你需要的：通过令牌重组加速视觉变换器。 *arXiv*。
- en: 'Lin et al. (2023) Lin, J.; Tang, J.; Tang, H.; Yang, S.; Dang, X.; and Han,
    S. 2023. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration.
    *arXiv*.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2023）Lin, J.; Tang, J.; Tang, H.; Yang, S.; Dang, X.; 和 Han, S. 2023.
    AWQ：面向 LLM 压缩和加速的激活感知权重量化。 *arXiv*。
- en: 'Lin et al. (2022) Lin, Y.; Zhang, T.; Sun, P.; Li, Z.; and Zhou, S. 2022. FQ-ViT:
    Post-Training Quantization for Fully Quantized Vision Transformer. In *IJCAI*,
    1173–1179.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin 等（2022）Lin, Y.; Zhang, T.; Sun, P.; Li, Z.; 和 Zhou, S. 2022. FQ-ViT：用于完全量化视觉变换器的后训练量化。
    在 *IJCAI*，1173–1179。
- en: Merity et al. (2016) Merity, S.; Xiong, C.; Bradbury, J.; and Socher, R. 2016.
    Pointer sentinel mixture models. *arXiv*.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等人（2016）Merity, S.; Xiong, C.; Bradbury, J.; 和 Socher, R. 2016. 指针哨兵混合模型。*arXiv*。
- en: 'Radford et al. (2019) Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;
    Sutskever, I.; et al. 2019. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8): 9.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Radford 等人（2019）Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; Sutskever,
    I.; 等. 2019. 语言模型是无监督的多任务学习者。*OpenAI博客*, 1(8): 9。'
- en: Raffel et al. (2020) Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang,
    S.; Matena, M.; Zhou, Y.; Li, W.; and Liu, P. J. 2020. Exploring the Limits of
    Transfer Learning with a Unified Text-to-Text Transformer. *Journal of Machine
    Learning Research*.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人（2020）Raffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.; Matena,
    M.; Zhou, Y.; Li, W.; 和 Liu, P. J. 2020. 通过统一的文本到文本转换器探索迁移学习的极限。*机器学习研究期刊*。
- en: 'Scao et al. (2022) Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilić, S.;
    Hesslow, D.; Castagné, R.; Luccioni, A. S.; Yvon, F.; Gallé, M.; et al. 2022.
    Bloom: A 176b-parameter open-access multilingual language model. *arXiv*.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Scao 等人（2022）Scao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ilić, S.; Hesslow,
    D.; Castagné, R.; Luccioni, A. S.; Yvon, F.; Gallé, M.; 等. 2022. Bloom: 一个具有176b参数的开放访问多语言模型。*arXiv*。'
- en: Shen et al. (2023) Shen, X.; Kong, Z.; Qin, M.; Dong, P.; Yuan, G.; Meng, X.;
    Tang, H.; Ma, X.; and Wang, Y. 2023. Data Level Lottery Ticket Hypothesis for
    Vision Transformers. In *IJCAI*.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人（2023）Shen, X.; Kong, Z.; Qin, M.; Dong, P.; Yuan, G.; Meng, X.; Tang,
    H.; Ma, X.; 和 Wang, Y. 2023. 数据级彩票票据假设在视觉转换器中的应用。收录于*IJCAI*。
- en: 'Touvron et al. (2023) Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
    M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez,
    A.; Joulin, A.; Grave, E.; and Lample, G. 2023. LLaMA: Open and Efficient Foundation
    Language Models. *arXiv*.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人（2023）Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,
    M.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.; Azhar, F.; Rodriguez,
    A.; Joulin, A.; Grave, E.; 和 Lample, G. 2023. LLaMA: 开放而高效的基础语言模型。*arXiv*。'
- en: Vaswani et al. (2017) Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
    L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. Attention is all you need.
    *NeurIPS*, 30.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人（2017）Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
    L.; Gomez, A. N.; Kaiser, Ł.; 和 Polosukhin, I. 2017. 注意力就是你所需要的一切。*NeurIPS*, 30。
- en: 'Wu, Yao, and He (2023) Wu, X.; Yao, Z.; and He, Y. 2023. ZeroQuant-FP: A Leap
    Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats.
    *arXiv preprint arXiv:2307.09782*.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu, Yao 和 He（2023）Wu, X.; Yao, Z.; 和 He, Y. 2023. ZeroQuant-FP: LLMs后训练W4A8量化的飞跃，使用浮点格式。*arXiv预印本
    arXiv:2307.09782*。'
- en: 'Xiao et al. (2022) Xiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; and
    Han, S. 2022. SmoothQuant: Accurate and Efficient Post-Training Quantization for
    Large Language Models. *arXiv*.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等人（2022）Xiao, G.; Lin, J.; Seznec, M.; Wu, H.; Demouth, J.; 和 Han, S.
    2022. SmoothQuant: 大型语言模型的准确高效后训练量化。*arXiv*。'
- en: 'Zhang et al. (2022) Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;
    Chen, S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; et al. 2022. Opt: Open pre-trained
    transformer language models. *arXiv*.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2022）Zhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.; Chen,
    S.; Dewan, C.; Diab, M.; Li, X.; Lin, X. V.; 等. 2022. Opt: 开放的预训练转换器语言模型。*arXiv*。'
- en: Zhang et al. (2023) Zhang, Y.; Zhao, L.; Cao, S.; Wang, W.; Cao, T.; Yang, F.;
    Yang, M.; Zhang, S.; and Xu, N. 2023. Integer or Floating Point? New Outlooks
    for Low-Bit Quantization on Large Language Models. *arXiv*.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人（2023）Zhang, Y.; Zhao, L.; Cao, S.; Wang, W.; Cao, T.; Yang, F.; Yang,
    M.; Zhang, S.; 和 Xu, N. 2023. 整数还是浮点？大型语言模型低位量化的新视角。*arXiv*。
- en: '| WQ | AQ | PPL of LLaMA-7B | PPL of LLaMA-13B | PPL of LLaMA-30B | PPL of
    LLaMA-65B |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| WQ | AQ | LLaMA-7B的PPL | LLaMA-13B的PPL | LLaMA-30B的PPL | LLaMA-65B的PPL |'
- en: '| # Bits | # Bits | WIKI | C4 | PTB | WIKI | C4 | PTB | WIKI | C4 | PTB | WIKI
    | C4 | PTB |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| # Bits | # Bits | WIKI | C4 | PTB | WIKI | C4 | PTB | WIKI | C4 | PTB | WIKI
    | C4 | PTB |'
- en: '| FP16 | FP16 | 5.68 | 7.08 | 27.34 | 5.09 | 6.61 | 19.23 | 4.10 | 5.98 | 16.29
    | 3.53 | 5.62 | 17.61 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | FP16 | 5.68 | 7.08 | 27.34 | 5.09 | 6.61 | 19.23 | 4.10 | 5.98 | 16.29
    | 3.53 | 5.62 | 17.61 |'
- en: '| INT4 | FP16 | 5.85 | 7.23 | 27.80 | 5.20 | 6.71 | 19.87 | 4.23 | 6.07 | 16.47
    | 3.65 | 5.69 | 24.44 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| INT4 | FP16 | 5.85 | 7.23 | 27.80 | 5.20 | 6.71 | 19.87 | 4.23 | 6.07 | 16.47
    | 3.65 | 5.69 | 24.44 |'
- en: '| Agile-Quant-8 | 6.16 | 7.66 | 29.76 | 5.57 | 7.39 | 21.59 | 4.55 | 6.71 |
    17.23 | 4.01 | 6.37 | 17.35 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| Agile-Quant-8 | 6.16 | 7.66 | 29.76 | 5.57 | 7.39 | 21.59 | 4.55 | 6.71 |
    17.23 | 4.01 | 6.37 | 17.35 |'
- en: '| Agile-Quant-8^∗ | 6.09 | 7.51 | 25.29 | 5.21 | 6.83 | 12.11 | 4.44 | 6.61
    | 12.36 | 3.92 | 5.95 | 12.87 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Agile-Quant-8^∗ | 6.09 | 7.51 | 25.29 | 5.21 | 6.83 | 12.11 | 4.44 | 6.61
    | 12.36 | 3.92 | 5.95 | 12.87 |'
- en: 'Table 5: Full results of the LLaMA models on Wikitext-2, C4, and PTB datasets.
    \M-8 denotes the 8-bit is used. ^∗ denotes the token pruning optimized results.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：LLaMA模型在Wikitext-2、C4和PTB数据集上的完整结果。 \M-8表示使用8位。 ^∗表示优化后的令牌修剪结果。
- en: '| WQ | AQ | PPL of OPT on PTB | PPL of BLOOM on PTB |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| WQ | AQ | OPT在PTB上的PPL | BLOOM在PTB上的PPL |'
- en: '| # Bits | # Bits | 125M | 350M | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B | 560M
    | 1.1B | 1.7B | 3B | 7.1B |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| # Bits | # Bits | 125M | 350M | 1.3B | 2.7B | 6.7B | 13B | 30B | 66B | 560M
    | 1.1B | 1.7B | 3B | 7.1B |'
- en: '| FP16 | FP16 | 38.99 | 31.08 | 20.29 | 17.97 | 15.77 | 14.52 | 14.04 | 13.36
    | 43.69 | 57.96 | 30.00 | 25.34 | 20.83 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | FP16 | 38.99 | 31.08 | 20.29 | 17.97 | 15.77 | 14.52 | 14.04 | 13.36
    | 43.69 | 57.96 | 30.00 | 25.34 | 20.83 |'
- en: '| INT4 | FP16 | 45.17 | 34.52 | 21.85 | 19.14 | 16.56 | 14.94 | 14.26 | 13.81
    | 46.97 | 62.47 | 31.84 | 26.49 | 21.67 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| INT4 | FP16 | 45.17 | 34.52 | 21.85 | 19.14 | 16.56 | 14.94 | 14.26 | 13.81
    | 46.97 | 62.47 | 31.84 | 26.49 | 21.67 |'
- en: '| Agile-Quant-8 | 37.57 | 29.33 | 18.78 | 16.46 | 13.81 | 12.78 | 12.12 | 12.07
    | 45.49 | 52.15 | 30.48 | 24.48 | 20.33 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Agile-Quant-8 | 37.57 | 29.33 | 18.78 | 16.46 | 13.81 | 12.78 | 12.12 | 12.07
    | 45.49 | 52.15 | 30.48 | 24.48 | 20.33 |'
- en: '| Agile-Quant-8^∗ | 34.26 | 27.63 | 16.62 | 15.98 | 13.34 | 12.32 | 12.65 |
    11.62 | 43.13 | 57.15 | 29.16 | 24.11 | 19.01 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Agile-Quant-8^∗ | 34.26 | 27.63 | 16.62 | 15.98 | 13.34 | 12.32 | 12.65 |
    11.62 | 43.13 | 57.15 | 29.16 | 24.11 | 19.01 |'
- en: 'Table 6: The results of the OPT and BLOOM models on the PTB dataset. \M-8 denotes
    the 8-bit is used. ^∗ denotes the token pruning optimized results.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：OPT和BLOOM模型在PTB数据集上的结果。\M-8表示使用了8位。^∗表示经过优化的标记修剪结果。
- en: Appendix
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录
- en: Additional Results
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 附加结果
- en: 'We deliver the additional results for LLaMA models on the C4 and PTB datasets
    in Table [5](#Sx6.T5 "Table 5 ‣ \M: Activation-Guided Quantization for Faster
    Inference of LLMs on the Edge"), and the OPT and BLOOM models on the PTB dataset
    in Table [6](#Sx6.T6 "Table 6 ‣ \M: Activation-Guided Quantization for Faster
    Inference of LLMs on the Edge").'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[5](#Sx6.T5 "Table 5 ‣ \M: Activation-Guided Quantization for Faster Inference
    of LLMs on the Edge")中提供了LLaMA模型在C4和PTB数据集上的附加结果，以及在表[6](#Sx6.T6 "Table 6 ‣ \M:
    Activation-Guided Quantization for Faster Inference of LLMs on the Edge")中提供的OPT和BLOOM模型在PTB数据集上的结果。'
- en: The Implementation Details of 4-bit Multipliers
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4位乘法器的实现细节
- en: Algorithm 1 4-bit Multiplier Implementation
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1 4位乘法器实现
- en: 14-bit_GeMM_4x4(i,  s1,  s2):2s1  =  [  4x4  matrix  of  src1  ]3s2  =  [  1x4  vector  of  src2  ]4c  =  [  4x4  matrix  of  zeros  ]5mask  =  [  1x4  vector  of  0x00FF00FF  ]6p  =  [  1x4  vector  with  zeros  ]7/*  Inner  loop  for  16  elements8  4  units  per  loop  */9for  j  in  range(4):10  /*  Lane-wise  Multiplication  */11  p  =  [multiply  the  j-th  row  of  s1  with  the  i-th  element  of  s2]12  /*  Product  Rearrangement  */13  t  =  [left  shift  elements  of  p  by  8  bits]14  p  =  [bitwise  OR  between  p,  t]15  p  =  [bitwise  AND  between  p,  mask]16  /*  Accumulation  */17  c.row[j]  =  [add  elements  of  p  to  the  corresponding  j-th  row  of  c]18return  c
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 14-bit_GeMM_4x4(i, s1, s2):2s1  =  [ 4x4 src1矩阵 ]3s2  =  [ 1x4 src2向量 ]4c  =  [
    4x4零矩阵 ]5mask  =  [ 1x4向量 0x00FF00FF ]6p  =  [ 1x4零向量 ]7/* 内循环处理16个元素8 每次循环4个单位
    */9for j in range(4):10  /* 按通道乘法 */11  p  =  [将s1的第j行与s2的第i个元素相乘]12  /* 乘积重排
    */13  t  =  [将p的元素左移8位]14  p  =  [p与t的按位或]15  p  =  [p与mask的按位与]16  /* 累积 */17  c.row[j]  =  [将p的元素加到c的对应第j行]18return  c
- en: The implementation below has been programmed based on ArmISAs and tested on
    the maincore 3.2GHz Cortex A77 of Snapdragon 870 onboard CPU. The kernels have
    been implemented within ArmComputeLibrary v22.05 Inference framework.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 以下实现基于ArmISAs编程，并在Snapdragon 870板载CPU的主核心3.2GHz Cortex A77上进行了测试。内核已在ArmComputeLibrary
    v22.05推理框架中实现。
- en: 'The breakdown of MLA (multiplication & Addition) with MUL and ADD does not
    introduce any obvious inference latency differences according to our benchmarks
    on both Snapdragon 870 onboard-CPU and RaspberryPi 4B, opening up the possibility
    of inserting result-adjustment auxiliary operations between the MUL and ADD to
    achieve SMMW (single-multiplication-multiple-weight). The 4-bit Multiplier has
    been implemented based on ARM ISAs following: MUL (Multiplication), LSL (LeftShift),
    ORR (BitwiseOR), AND (BitwiseAND) the same process as Figure [5](#Sx4.F5 "Figure
    5 ‣ Edge-Oriented Optimization ‣ Methodology ‣ \M: Activation-Guided Quantization
    for Faster Inference of LLMs on the Edge") in our paper.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '根据我们在Snapdragon 870板载CPU和RaspberryPi 4B上的基准测试，MLA（乘法和加法）的拆分没有引入明显的推理延迟差异，这为在MUL和ADD之间插入结果调整辅助操作以实现SMMW（单次乘法多次加权）打开了可能性。4位乘法器的实现基于ARM
    ISAs，遵循：MUL（乘法），LSL（左移），ORR（按位或），AND（按位与），过程与我们论文中的图[5](#Sx4.F5 "Figure 5 ‣ Edge-Oriented
    Optimization ‣ Methodology ‣ \M: Activation-Guided Quantization for Faster Inference
    of LLMs on the Edge")相同。'
