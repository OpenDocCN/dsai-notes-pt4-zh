- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 18:51:28'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:28
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SpQR：一种用于接近无损LLM权重压缩的稀疏量化表示
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.03078](https://ar5iv.labs.arxiv.org/html/2306.03078)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.03078](https://ar5iv.labs.arxiv.org/html/2306.03078)
- en: \newfloatcommand
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \newfloatcommand
- en: capbtabboxtable[][\FBwidth]
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: capbtabboxtable[][\FBwidth]
- en: Tim Dettmers
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 提姆·德特梅斯
- en: University of Washington
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 华盛顿大学
- en: '&Ruslan Svirschevski¹¹footnotemark: 1'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**鲁斯兰·斯维尔谢夫斯基**¹¹脚注标记：1'
- en: HSE University & Yandex
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 高等经济学院 & Yandex
- en: '&Vage Egiazarian¹¹footnotemark: 1'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**瓦赫·埃吉亚贾良**¹¹脚注标记：1'
- en: HSE University & Yandex
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 高等经济学院 & Yandex
- en: '&Denis Kuznedelev¹¹footnotemark: 1'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**德尼斯·库兹涅代列夫**¹¹脚注标记：1'
- en: Yandex & Skoltech
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Yandex 和 Skoltech
- en: '&Elias Frantar'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**伊利亚斯·弗朗塔**'
- en: IST Austria
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: IST 奥地利
- en: '&Saleh Ashkboos'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**萨利赫·阿什库布斯**'
- en: ETH Zurich
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世联邦理工学院
- en: '&Alexander Borzunov'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**亚历山大·博尔祖诺夫**'
- en: HSE University & Yandex
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 高等经济学院 & Yandex
- en: '&Torsten Hoefler'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**托斯滕·赫夫勒**'
- en: ETH Zurich
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 苏黎世联邦理工学院
- en: '&Dan Alistarh'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '**丹·阿利斯塔赫**'
- en: 'IST Austria & NeuralMagic Equal contributionCorresponding author: dettmers@cs.washington.edu'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: IST 奥地利 & NeuralMagic 等贡献作者：dettmers@cs.washington.edu
- en: Abstract
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Recent advances in large language model (LLM) pretraining have led to high-quality
    LLMs with impressive abilities. By compressing such LLMs via quantization to 3-4
    bits per parameter, they can fit into memory-limited devices such as laptops and
    mobile phones, enabling personalized use. However, quantization down to 3-4 bits
    per parameter usually leads to moderate-to-high accuracy losses, especially for
    smaller models in the 1-10B parameter range, which are well-suited for edge deployments.
    To address this accuracy issue, we introduce the Sparse-Quantized Representation
    (SpQR), a new compressed format and quantization technique which enables for the
    first time *near-lossless* compression of LLMs across model scales, while reaching
    similar compression levels to previous methods. SpQR works by identifying and
    isolating *outlier weights*, which cause particularly-large quantization errors,
    and storing them in higher precision, while compressing all other weights to 3-4
    bits, and achieves relative accuracy losses of less than $1\%$ in perplexity for
    highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B parameter
    LLM on a single 24 GB consumer GPU without any performance degradation at 15%
    speedup thus making powerful LLMs available to consumer without any downsides.
    SpQR comes with efficient algorithms for both encoding weights into its format,
    as well as decoding them efficiently at runtime¹¹1 [github.com/Vahe1994/SpQR](https://github.com/Vahe1994/SpQR);
    to be integrated into [github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes).
    Specifically, we provide an efficient GPU inference algorithm for SpQR which yields
    faster inference than 16-bit baselines at similar accuracy, while enabling memory
    compression gains of more than 4x.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在大型语言模型（LLM）预训练方面的进展使得高质量的LLM具备了令人印象深刻的能力。通过将这些LLM压缩到每个参数3-4位的量化，它们可以适应内存受限的设备，如笔记本电脑和手机，实现个性化使用。然而，将量化压缩到每个参数3-4位通常会导致中等到高的准确率损失，特别是对于1-10B参数范围内的小型模型，这些模型非常适合边缘部署。为了解决这一准确率问题，我们引入了稀疏量化表示（SpQR），这是一种新的压缩格式和量化技术，首次实现了LLM
    across模型规模的*接近无损*压缩，同时达到了与之前方法相似的压缩水平。SpQR通过识别和隔离*异常权重*，这些权重导致特别大的量化误差，并以更高的精度存储它们，同时将所有其他权重压缩到3-4位，达到了对于高度准确的LLaMA和Falcon
    LLMs来说，相对准确率损失少于$1\%$。这使得在单个24 GB消费级GPU上运行33B参数的LLM成为可能，而性能不下降，同时实现了15%的加速，从而使强大的LLM可以在不带来任何负面影响的情况下提供给消费者。SpQR提供了高效的算法，用于将权重编码到其格式中，以及在运行时高效解码¹¹1
    [github.com/Vahe1994/SpQR](https://github.com/Vahe1994/SpQR)；将集成到 [github.com/TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)。具体而言，我们提供了一个高效的GPU推断算法，该算法在类似准确率下比16位基线更快，同时实现了超过4倍的内存压缩增益。
- en: 1 Introduction
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Pretrained large language models (LLMs) improved rapidly from task-specific
    performance [[42](#bib.bib42), [5](#bib.bib5), [30](#bib.bib30)], to performing
    well on general tasks if prompted with instructions [[1](#bib.bib1), [39](#bib.bib39),
    [25](#bib.bib25)]. While the improved performance can be attributed to scaling
    in training data and parameters [[18](#bib.bib18), [4](#bib.bib4)] recent trends
    focused on smaller models trained on more data, that are easier to use at inference
    time [[15](#bib.bib15), [2](#bib.bib2), [33](#bib.bib33)]. For example, the 7B
    parameter LLaMA model trained on 1T tokens achieved an average performance only
    slightly lower than GPT-3 [[1](#bib.bib1)] despite being 25x smaller. Current
    techniques for LLM compression can shrink these models further by a factor of
    about 4x, while preserving their performance [[6](#bib.bib6), [43](#bib.bib43),
    [9](#bib.bib9), [7](#bib.bib7)]. This yields performance levels comparable to
    the largest GPT-3 model, with major reductions in terms of memory requirements.
    With such improvements, well-performing models could be efficiently served on
    end-user devices, such as laptops.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的大型语言模型（LLMs）从任务特定的性能[[42](#bib.bib42)、[5](#bib.bib5)、[30](#bib.bib30)]迅速改进到在提供指令时能在通用任务中表现良好[[1](#bib.bib1)、[39](#bib.bib39)、[25](#bib.bib25)]。尽管改进的性能可以归因于训练数据和参数的扩展[[18](#bib.bib18)、[4](#bib.bib4)]，近期趋势集中在使用更多数据训练的小型模型，这些模型在推理时更易于使用[[15](#bib.bib15)、[2](#bib.bib2)、[33](#bib.bib33)]。例如，7B参数的LLaMA模型在1T
    tokens上训练，尽管比GPT-3小25倍，但其平均性能仅略低于GPT-3[[1](#bib.bib1)]。当前的LLM压缩技术可以将这些模型进一步缩小约4倍，同时保持其性能[[6](#bib.bib6)、[43](#bib.bib43)、[9](#bib.bib9)、[7](#bib.bib7)]。这使得性能水平可与最大的GPT-3模型相媲美，同时在内存需求上大幅降低。通过这些改进，表现良好的模型可以高效地在终端设备如笔记本电脑上使用。
- en: The main challenge is to compress models enough to fit into such devices while
    also preserving generative quality. Specifically, studies show that, although
    accurate, existing techniques for 3 to 4-bit quantization still lead to significant
    accuracy degradation [[7](#bib.bib7), [9](#bib.bib9)]. Since LLM generation is
    sequential, depending on previously-generated tokens, small relative errors can
    accumulate and lead to severely corrupted outputs. To ensure reliable quality,
    it is critical to design low-bitwidth quantization that does not degrade predictive
    performance compared to the 16-bit model.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 主要挑战是将模型压缩到足够适合这些设备的同时保持生成质量。具体来说，研究表明，尽管准确，现有的3到4位量化技术仍导致显著的准确性下降[[7](#bib.bib7)、[9](#bib.bib9)]。由于LLM生成是顺序进行的，依赖于先前生成的tokens，较小的相对误差会累积并导致严重的输出损坏。为了确保可靠的质量，设计低位宽量化至关重要，这样它的预测性能不会比16位模型降低。
- en: 'In this work, we introduce Sparse-Quantized Representations (SpQR), a hybrid
    sparse-quantized format which can compress accurate pretrained LLMs to 3-4 bits
    per parameter while staying *near-lossless*: specifically, SpQR is the first weight
    quantization method which is able to reach such compression ratios while inducing
    end-to-end accuracy error as measured in perplexity of less than 1% relative to
    the dense baseline. SpQR works by combining two innovations. First, we isolate
    *outlier weights*, whose quantization we show to induce disproportionately high
    errors: these weights are kept in high precision, while the other weights are
    stored in a much lower, e.g. 3-bit, format. Second, we implement a variant of
    grouped quantization with very small group size, e.g. 16 contiguous elements,
    but we show that one can quantize the quantization scales themselves to a 3-bit
    representation.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们介绍了稀疏量化表示（SpQR），这是一种混合稀疏量化格式，可以将准确的预训练LLM压缩到每个参数3-4位，同时保持*接近无损*：具体来说，SpQR是第一个能够达到这种压缩比的权重量化方法，同时在相对于密集基线的困惑度测量中引入的端到端准确性误差低于1%。SpQR通过结合两项创新来工作。首先，我们隔离*异常权重*，我们表明这些权重的量化会引入不成比例的高误差：这些权重保持高精度，而其他权重则以更低的格式（如3位）存储。其次，我们实现了一种小组量化的变体，例如16个连续元素，但我们展示了可以将量化尺度本身量化到3位表示。
- en: 'To convert a given pretrained LLM into SpQR format, we adopt an extended version
    of the post-training quantization (PTQ) approach recently introduced by GPTQ [[9](#bib.bib9)].
    Specifically, the method passes calibration data through the uncompressed model;
    to compress each layer, it applies a layer-wise solver with respect to the L2
    error between the outputs of the uncompressed model, and those of the quantized
    weights. Our approach splits this process into two steps: an “outlier detection”
    step, in which we isolate weights whose direct quantization has outsize impact
    on layer output behavior, and an actual compression step, in which most ($\geq
    99\%$) of weights are compressed to low-bitwidth, the outliers are extracted,
    and the whole representation is rendered more efficient by further compressing
    the quantization metadata.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将给定的预训练LLM转换为SpQR格式，我们采用了GPTQ最近引入的扩展版后训练量化（PTQ）方法[[9](#bib.bib9)]。具体而言，该方法将校准数据通过未压缩模型；为了压缩每一层，它应用了一种层级求解器，针对未压缩模型的输出和量化权重的输出之间的L2误差。我们的方法将这一过程分为两个步骤：“异常检测”步骤，在此步骤中，我们隔离了那些直接量化对层输出行为有显著影响的权重，以及实际压缩步骤，在此步骤中，大多数（$\geq
    99\%$）权重被压缩到低位宽，异常被提取，整个表示通过进一步压缩量化元数据变得更高效。
- en: Our method is motivated by a new analysis showing that LLM weight quantization
    errors exhibit both vertical and horizontal group correlations, corresponding
    to systematic large errors corresponding to input feature dimensions and output
    hidden dimensions. While outlier input features have been observed before [[6](#bib.bib6),
    [43](#bib.bib43)], our work is the first to demonstrate that similar outliers
    occur *in the weights, for particular output hidden dimensions*. Unlike input
    feature outliers, the output hidden dimension outliers occur only in small segments
    for a particular output hidden dimension.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法受到一种新分析的启发，该分析表明LLM权重量化误差展示了垂直和水平的组相关性，分别对应于输入特征维度和输出隐藏维度的系统性大误差。虽然之前已经观察到异常输入特征[[6](#bib.bib6),
    [43](#bib.bib43)]，但我们的工作是第一个展示类似异常出现在*特定输出隐藏维度的权重中*。与输入特征异常不同，输出隐藏维度的异常仅在特定输出隐藏维度的小段中出现。
- en: Our quantization algorithm isolates such outliers and efficiently encodes a
    given model in SpQR format. To exploit the resulting structure, we develop a specialized
    sparse-matrix multiplication algorithm based on the compressed sparse row (CSR)
    format. To use SpQR for token-by-token generation, we combine this sparse algorithm
    together with a dense-quantized matrix multiplication for 3-4 bit weights. With
    this, SpQR reduces the memory footprint of LLMs by a factor of about 3.4x or more
    without degradation in accuracy, measured as language modeling loss or perplexity,
    while also being 20-30% faster for LLM generation compared to 16-bit inference.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的量化算法隔离了这些异常，并高效地将给定模型编码为SpQR格式。为了利用产生的结构，我们开发了一种基于压缩稀疏行（CSR）格式的专用稀疏矩阵乘法算法。为了使用SpQR进行逐词生成，我们将这种稀疏算法与用于3-4位权重的密集量化矩阵乘法相结合。通过这一点，SpQR将LLM的内存占用减少了约3.4倍或更多，同时在准确性上没有下降（以语言建模损失或困惑度衡量），并且在LLM生成方面比16位推理快20-30%。
- en: '![Refer to caption](img/fc60d9824b6733ff8c4ff91f1735ceef.png)![Refer to caption](img/095cdf7cf75831d6bd9c7a2742d4460c.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fc60d9824b6733ff8c4ff91f1735ceef.png)![参见说明](img/095cdf7cf75831d6bd9c7a2742d4460c.png)'
- en: 'Figure 1: Compressed LLM performance for LLaMA models. (left) LM loss on WikiText2
    vs model size. (right) Average performance on zero-shot tasks vs model size.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LLaMA模型的压缩LLM性能。（左）WikiText2上的语言模型损失与模型大小。（右）零-shot任务上的平均性能与模型大小。
- en: 2 Related Work
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: We focus our discussion on related *post-training quantization (PTQ) methods* [[22](#bib.bib22)],
    referring the reader to the recent survey of Gholami et al. [[12](#bib.bib12)]
    for full background on quantization. PTQ methods are a popular approach for *one-shot
    compression* of models with various sizes, based on a limited amount of calibration
    data, using accurate solvers, usually focused on layer- or group-wise compression
    sub-problems. Most PTQ methods, such as AdaRound [[22](#bib.bib22)], BitSplit [[40](#bib.bib40)],
    AdaQuant [[16](#bib.bib16)], BRECQ [[19](#bib.bib19)], or OBQ [[10](#bib.bib10)]
    were designed for vision models or small-scale language models, with less than
    100M parameters. All these recent approaches tend to use accurate solvers, which
    would not scale to GPT-scale models in terms of computational or memory cost,
    as they are 10-1000x larger in size.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的讨论集中在相关的*后训练量化 (PTQ) 方法* [[22](#bib.bib22)]上，并参考了Gholami等人的最新调查[[12](#bib.bib12)]以获取量化的完整背景。PTQ
    方法是一种流行的方法，用于在基于有限的校准数据、使用准确的求解器的情况下，对各种规模的模型进行*一键压缩*，通常侧重于层级或分组的压缩子问题。大多数PTQ方法，如AdaRound
    [[22](#bib.bib22)]、BitSplit [[40](#bib.bib40)]、AdaQuant [[16](#bib.bib16)]、BRECQ
    [[19](#bib.bib19)]或OBQ [[10](#bib.bib10)]，都是为视觉模型或小规模语言模型（参数少于100M）设计的。这些近期的方法都倾向于使用准确的求解器，但在计算或内存成本方面难以扩展到GPT规模的模型，因为它们的大小是10-1000倍。
- en: Recently, there has been significant interest in obtaining accurate post-training
    methods that scale to such massive models. Due to computational constraints, early
    work such as ZeroQuant [[44](#bib.bib44)], LLM.int8() [[6](#bib.bib6)], and nuQmm [[27](#bib.bib27)]
    used direct rounding of weights to the nearest quantization level, while customizing
    the quantization granularity (i.e., group size) to trade off space for increased
    accuracy. LLM.int8() [[6](#bib.bib6)] suggested isolating “outlier features” which
    would be quantized separately to higher bit-width. These approaches are able to
    induce relatively low quantization error, e.g. 5.5% relative LM Loss increase
    for LLaMA-7B at 4-bit weight quantization, provided that the quantization granularity
    is low enough. GPTQ [[9](#bib.bib9)] proposed a higher-accuracy approach (e.g.,
    4% LM Loss increase in the above setting), which works via an approximate large-scale
    solver for the problem of minimizing the layer-wise squared error.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，对能够扩展到如此大规模模型的准确后训练方法的兴趣显著增加。由于计算限制，早期工作如ZeroQuant [[44](#bib.bib44)]、LLM.int8()
    [[6](#bib.bib6)] 和 nuQmm [[27](#bib.bib27)] 使用了将权重直接四舍五入到最接近的量化级别，同时自定义量化粒度（即组大小），以在增加准确性与空间利用之间进行权衡。LLM.int8()
    [[6](#bib.bib6)] 提议隔离“异常特征”，这些特征将单独量化到更高的位宽。这些方法能够产生相对较低的量化误差，例如，在4位权重量化下，LLaMA-7B的相对语言模型损失增加为5.5%，前提是量化粒度足够低。GPTQ
    [[9](#bib.bib9)] 提出了更高准确度的方法（例如，在上述设置中，语言模型损失增加4%），它通过一个近似的大规模求解器来最小化层级平方误差的问题。
- en: 'Dettmers et al. [[7](#bib.bib7)] provided an in-depth overview of the accuracy-compression
    trade-offs underlying these methods, establishing that 4-bit quantization is an
    optimal point for round-to-nearest-based methods, whereas higher compression can
    be achieved via data-aware methods such as GPTQ. SparseGPT [[8](#bib.bib8)] presented
    an approach to jointly sparsify LLM weights to medium sparsities, together with
    quantization of the remaining weights to a fixed given bit-width. One common drawback
    of existing methods is that the accuracy loss relative to the original model is
    still significant (see Table [4](#S5.F4 "Figure 4 ‣ Main Results. ‣ 5 Experimental
    Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression")). This is especially relevant to relatively small but easily deployable
    models, e.g. in the 7-13B parameter range, where existing methods show drastic
    accuracy drops. We investigate this question here, and provide a new compression
    format which can lead to near-lossless 3-4 bits compression in this regime.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dettmers等人[[7](#bib.bib7)] 对这些方法中准确度与压缩之间的权衡进行了深入概述，确定4位量化是基于四舍五入的最优点，而通过如GPTQ等数据感知方法可以实现更高的压缩率。SparseGPT
    [[8](#bib.bib8)] 提出了一种方法，将LLM权重共同稀疏化到中等稀疏度，同时将剩余权重量化到固定的位宽。现有方法的一个常见缺点是，相对于原始模型的准确度损失仍然显著（见表[4](#S5.F4
    "图 4 ‣ 主要结果 ‣ 5 实验验证 ‣ SpQR: 一种接近无损的LLM权重压缩表示")）。这对于相对较小但易于部署的模型（例如7-13B参数范围内）尤为相关，因为现有方法在这些模型上会出现剧烈的准确度下降。我们在此研究了这个问题，并提供了一种新的压缩格式，可以在这一范围内实现接近无损的3-4位压缩。'
- en: A related question is that of performing both activation and weight quantization.
    There is early work [[6](#bib.bib6), [43](#bib.bib43), [44](#bib.bib44)], showing
    that both activations and weights could be quantized to 8-bits with relatively
    low accuracy impact. These complementary investigations yield interesting insights
    into the causes of compression error in the case of LLMs. Specifically, [[6](#bib.bib6),
    [43](#bib.bib43)] observe the presence of “outlier features” with significantly
    higher values in the input/output of large LLMs, which induce higher quantization
    error, and propose different mitigation strategies.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的问题是同时进行激活和权重量化。早期的研究[[6](#bib.bib6), [43](#bib.bib43), [44](#bib.bib44)]显示，激活和权重都可以量化为8位，且对准确性的影响相对较小。这些互补性的研究为LLMs中压缩误差的原因提供了有趣的见解。具体来说，[[6](#bib.bib6),
    [43](#bib.bib43)]观察到大规模LLMs的输入/输出中存在显著较高值的“异常特征”，这些异常特征会引发更高的量化误差，并提出了不同的缓解策略。
- en: We analyze this phenomenon from the point of view of weight quantization. In
    particular, we investigate the outlier structure, beyond input feature outliers
    in the weight matrix. While we find that input feature outliers of the current
    layer are correlated to hidden unit outliers weight in the previous layer there
    is not a strict correspondence. Such partially-structured outlier patterns necessitate
    a fine-grained hybrid compression format that goes beyond algorithms that exploit
    the column structure of outlier features found in previous work.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从权重量化的角度分析这一现象。特别地，我们调查了超出输入特征异常的权重矩阵中的异常结构。虽然我们发现当前层的输入特征异常与前一层的隐藏单元权重异常相关，但并没有严格的一一对应关系。这种部分结构化的异常模式需要一种细粒度的混合压缩格式，这超出了利用之前工作中发现的异常特征列结构的算法。
- en: Hybrid sparse-quantized formats have been investigated generally for deep networks.
    Some efficient CPU inference engines [[23](#bib.bib23), [11](#bib.bib11)] support
    a different block sparse-and-quantized format, in which each block of $4$ consecutive
    weights is either completely sparse or quantized to 8-bit format, whereas GPUs
    support a similar compound format in which every group of 4 weights contains 2
    zero weights, and the non-zero weights could be quantized. The FBGEMM package [[17](#bib.bib17)]
    proposed a format in which certain “outlier” weights are quantized separately,
    to reduce their impact on normalization. However, in this format, “outlier” weights
    are still quantized to exactly the same bit-width (8-bit) as regular weights;
    moreover, no procedure is given for converting a model to this format post-training.
    By contrast, 1) we provide an efficient and accurate post-training compression
    algorithm which identifies outliers as weights inducing high output error, 2)
    we propose a format compressing outliers to a higher bit-width relative to regular
    weights, and 3) our format stores outliers in blocks, allowing for efficient implementation
    of GPU kernels, which we provide as well.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 混合稀疏量化格式已被广泛研究用于深度网络。一些高效的CPU推理引擎[[23](#bib.bib23), [11](#bib.bib11)]支持一种不同的块稀疏和量化格式，其中每个4个连续权重的块要么完全稀疏，要么量化为8位格式，而GPU则支持一种类似的复合格式，其中每组4个权重包含2个零权重，非零权重可以被量化。FBGEMM包[[17](#bib.bib17)]提出了一种格式，在该格式中，某些“异常”权重被单独量化，以减少其对归一化的影响。然而，在这种格式中，“异常”权重仍然被量化为与普通权重完全相同的位宽（8位）；此外，没有提供将模型转换为这种格式的后训练过程。相比之下，1)
    我们提供了一种高效准确的后训练压缩算法，该算法将异常权重识别为导致高输出误差的权重，2) 我们提出了一种将异常权重压缩为相对于普通权重更高位宽的格式，3)
    我们的格式将异常权重存储在块中，从而实现了GPU内核的高效实现，我们也提供了这种实现。
- en: 3 Quantization sensitivity of LLM weights
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 LLM权重的量化敏感性
- en: 3.1 Parameter sensitivity under quantization
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 量化下的参数敏感性
- en: 'Not all parameters in a neural network are equally important. Intuitively,
    a weight could be seen as sensitive to quantization if its rounding error is large,
    i.e. it is not close to a quantization point, and/or the inputs it is usually
    multiplied with a large, amplifying even a small rounding error. These simple
    notions of sensitivity however disregard the fact that LLMs operate on very large
    vectors with significant correlations: a weight $w_{a}$. This idea is exploited
    by modern quantization algorithms [[9](#bib.bib9), [44](#bib.bib44)] and can lead
    to major improvements over vanilla rounding, especially a low bitwidths. Properly
    capturing this aspect of sensitivity requires a more robust definition.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的所有参数并不是同等重要的。直观上，如果一个权重的四舍五入误差很大，即它离量化点较远，和/或它通常乘以的输入很大，即使是小的四舍五入误差也会被放大，这个权重就被认为对量化敏感。然而，这些对灵敏度的简单概念忽略了LLMs在具有显著相关性的非常大的向量上操作的事实：一个权重$w_{a}$。现代量化算法[[9](#bib.bib9),
    [44](#bib.bib44)]利用了这一思想，并且在低位宽的情况下可以带来显著的改进。正确捕捉灵敏度的这一方面需要一个更稳健的定义。
- en: 'For computational tractability, we assess sensitivity on a per-layer level
    using a small set of *calibration inputs* $X$:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算上的可处理性，我们使用一小组*校准输入* $X$在每层级别上评估灵敏度：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'Crucially, all weights of $W^{\prime}$ is the inverse Hessian matrix corresponding
    to the optimization problem:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，$W^{\prime}$的所有权重是对应优化问题的逆Hessian矩阵：
- en: '|  | $s_{ij}=\frac{(w_{ij}-\text{quant}(w_{ij}))^{2}}{2(XX^{\top})^{-1}}.$
    |  | (2) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $s_{ij}=\frac{(w_{ij}-\text{quant}(w_{ij}))^{2}}{2(XX^{\top})^{-1}}.$
    |  | (2) |'
- en: This saliency measure can be approximated efficiently by quantization solvers,
    such as GPTQ [[9](#bib.bib9)]. In more detail, GPTQ quantizes weight matrices
    column-by-column while in each step adjusting the not-yet-quantized part to compensate
    for the quantization error in a similar sense as defined above. Consequentially,
    instead of statically deciding all sensitivities in advance, they can be computed
    dynamically as the algorithm processes each column, by using the inverse of the
    Hessian subselection corresponding to all not yet quantized weights. This matrix
    is already efficiently computed by GPTQ and thus does not impose any additional
    overheads. The main advantage of this approach is that $s_{ij}$ and thus accounts
    for adjustments due to previously quantized weights as well.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个显著性度量可以通过量化求解器高效地近似，例如GPTQ [[9](#bib.bib9)]。更详细地说，GPTQ逐列量化权重矩阵，同时在每一步调整尚未量化的部分，以补偿类似上述定义的量化误差。因此，与其静态地预先决定所有灵敏度，不如在算法处理每一列时动态计算它们，使用对应于尚未量化权重的Hessian子选择的逆矩阵。这个矩阵已经被GPTQ高效计算，因此不会带来额外的开销。这种方法的主要优势在于$s_{ij}$，并且也考虑了由于之前量化权重而产生的调整。
- en: 3.2 Exploring parameter sensitivity
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 探索参数灵敏度
- en: 'Before we define out main method, SpQR, we provide a motivating analysis of
    parameter sensitivity which uncovers that the location of sensitive weights in
    the weight matrix are not random but have particular structures. To highlight
    these structural elements during the quantization process, we calculate the the
    per-weight sensitivities and visualize them for the popular and highly-accurate
    LLaMA-65B model [[33](#bib.bib33)]. As the quantization method, we use GPTQ quantization
    to 3-bit, without weight grouping, following [[9](#bib.bib9)]. We use C4 [[29](#bib.bib29)]
    as the calibration dataset, and we estimate the error on 128 sequences of 2048
    tokens each. Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Exploring parameter sensitivity
    ‣ 3 Quantization sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression") depicts the output projection of the
    last self-attention layer of LLaMA-65B.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们定义主要方法SpQR之前，我们提供了一个参数灵敏度的激励分析，揭示了权重矩阵中敏感权重的位置不是随机的，而是具有特定的结构。为了在量化过程中突出这些结构元素，我们计算每个权重的灵敏度并对流行且高度准确的LLaMA-65B模型[[33](#bib.bib33)]进行可视化。作为量化方法，我们使用GPTQ量化到3位，无权重分组，参考[[9](#bib.bib9)]。我们使用C4
    [[29](#bib.bib29)]作为校准数据集，并在128个2048个标记的序列上估计误差。图[2](#S3.F2 "Figure 2 ‣ 3.2 Exploring
    parameter sensitivity ‣ 3 Quantization sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")描绘了LLaMA-65B最后一个自注意力层的输出投影。'
- en: '![Refer to caption](img/4ddf9ab1a0fa8707704291fdd89faa03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4ddf9ab1a0fa8707704291fdd89faa03.png)'
- en: 'Figure 2: Weight log-sensitivities from the last attention layer of LLaMA-65B.
    Dark-blue shades indicate higher sensitivity. The image on the left is a high-level
    view, resized to 1:32 scale with max-pooling. The two images in the middle are
    zoomed in from the main figure. The two images on the right are taken from other
    weight matrices.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：来自LLaMA-65B最后一个注意力层的权重对数敏感性。深蓝色阴影表示较高的敏感性。左侧的图像是高层次视图，经过1:32缩放和最大池化处理。中间的两个图像是从主图中放大的。右侧的两个图像取自其他权重矩阵。
- en: 'Using the sensitivity analysis, we observe several patterns in the weight matrix,
    often in a single row or column. Since the large weight matrices in LLaMA-65B
    have too many rows/columuns to be respresentable in a compact image (default:
    8k $\times$ rows and columns. This max pooling only affects the leftmost image.
    Using this visualization, we observe that the quantization error patterns vary
    both by layer type, for example attention vs multilayer perceptron (MLP), and
    layer depth. In particular, we find that more sensitive outliers are present for
    deeper layers. (Please see Appendix [A](#A1 "Appendix A Additional weight sensitivity
    analysis ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression") for additional results.) We now proceed to categorize outlier structures,
    taking this attention weight matrix as an exemplar. We make the following observations:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '使用敏感性分析，我们观察到权重矩阵中出现了几个模式，通常在单行或单列中。由于LLaMA-65B中的大权重矩阵有太多行/列，无法在紧凑的图像中表示（默认：8k
    $\times$ 行和列）。这种最大池化仅影响最左边的图像。使用这种可视化，我们观察到量化误差模式在不同层类型中变化，例如注意力层与多层感知器（MLP），以及层深度。特别地，我们发现更深层次的层存在更多的敏感外点。（有关更多结果，请参见附录
    [A](#A1 "Appendix A Additional weight sensitivity analysis ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")。）我们现在开始对外点结构进行分类，以这个注意力权重矩阵为例。我们做出以下观察：'
- en: •
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Row outliers are shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Exploring parameter
    sensitivity ‣ 3 Quantization sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression") bottom-center as regions
    of high sensitivity within one output unit. Some of these patterns span the entire
    row, while others are partial. In attention layers, some of the partial row outliers
    correspond to some subset of attention heads. Column outliers appear in Figure [2](#S3.F2
    "Figure 2 ‣ 3.2 Exploring parameter sensitivity ‣ 3 Quantization sensitivity of
    LLM weights ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression"), bottom-right, showing high sensitivity in select input dimensions
    (columns) across all rows. The latter are correlated to the “outlier feature”
    phenomenon reported in Dettmers et al. [[6](#bib.bib6)].'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '行外点显示在图 [2](#S3.F2 "Figure 2 ‣ 3.2 Exploring parameter sensitivity ‣ 3 Quantization
    sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression") 底部中心，作为一个输出单元中的高敏感性区域。这些模式中的一些跨越整个行，而另一些是部分的。在注意力层中，部分行外点对应于某些子集的注意力头。列外点出现在图
    [2](#S3.F2 "Figure 2 ‣ 3.2 Exploring parameter sensitivity ‣ 3 Quantization sensitivity
    of LLM weights ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM
    Weight Compression") 的底部右侧，显示了所有行中某些输入维度（列）的高敏感性。后者与Dettmers等人报告的“外点特征”现象相关[[6](#bib.bib6)]。'
- en: •
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Sensitive attention heads. (Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Exploring parameter
    sensitivity ‣ 3 Quantization sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression"), top-center) – regular
    stripes of width 128 highlight all weights corresponding to one attention head.
    This could be related to some attention heads having more important functions [[38](#bib.bib38),
    [37](#bib.bib37), [24](#bib.bib24)]. The corresponding “stripes” are horizontal
    for attention Q & K projections, vertical in output projection, and absent from
    value projections and any MLP weights. Of note, there is significant variation
    in individual weight sensitivity even within the sensitive heads.'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '敏感注意力头。（图 [2](#S3.F2 "Figure 2 ‣ 3.2 Exploring parameter sensitivity ‣ 3 Quantization
    sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression")，顶部中心）– 宽度为128的规则条纹突出显示了所有对应于一个注意力头的权重。这可能与某些注意力头具有更重要的功能有关[[38](#bib.bib38)，[37](#bib.bib37)，[24](#bib.bib24)]。对应的“条纹”在注意力Q和K投影中是水平的，在输出投影中是垂直的，而在值投影和任何MLP权重中则不存在。值得注意的是，即使在敏感头内部，个体权重的敏感性也存在显著变化。'
- en: •
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The Rotary embedding pattern, a repeating vertical pattern of sensitivity with
    a period of 64 units. We attribute this to the use of rotary embeddings [[32](#bib.bib32)]:
    each attention head (dim = 128) is split into two halves: the first 64 are “rotated”
    with cosine, and the other 64 use sine. Both sine and cosine rotation use the
    same set of frequencies. Typically, the weights that correspond to low-frequency
    sines and cosines are more sensitive than their high-frequency counterparts, as
    shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.2 Exploring parameter sensitivity ‣ 3
    Quantization sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression") (top-right). As expected, this pattern
    is absent from any layer not using rotary embeddings.'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'Rotary嵌入模式，即一个周期为64个单位的灵敏度垂直重复模式。我们将其归因于Rotary嵌入[[32](#bib.bib32)]的使用：每个注意力头（维度=128）被分成两个部分：前64个通过余弦进行“旋转”，其余64个使用正弦。正弦和余弦旋转使用相同的频率集。通常，对应于低频正弦和余弦的权重比其高频对手更敏感，如图[2](#S3.F2
    "图 2 ‣ 3.2 探索参数灵敏度 ‣ 3 LLM权重的量化灵敏度 ‣ SpQR: 一种接近无损的LLM权重压缩稀疏量化表示")（右上角）所示。如预期的那样，任何不使用Rotary嵌入的层中都没有这种模式。'
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Unstructured outliers. Besides the above, each layer has a number of individual
    sensitivity weights that do not fit into any of the above patterns. These unstructured
    outliers occur more frequently for columns with largest input index (i.e. on the
    right side of the images). This effect is difficult to see on a heatmap, so we
    provide additional figures and statistical tests in Appendix [A](#A1 "Appendix
    A Additional weight sensitivity analysis ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression"). We believe is probably an artefact
    of the GPTQ algorithm, which compresses one by one, using yet-uncompressed weights
    to compensate the error. Thus, the rightmost batch of weights accumulates the
    most error.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '无结构异常值。除了上述情况外，每一层还有一些不符合上述模式的单个灵敏度权重。这些无结构异常值在输入索引最大（即图像右侧）的位置更频繁出现。这种效应在热图上很难看到，因此我们在附录[A](#A1
    "附录 A 额外的权重灵敏度分析 ‣ SpQR: 一种接近无损的LLM权重压缩稀疏量化表示")中提供了额外的图示和统计测试。我们认为这可能是GPTQ算法的一个副作用，该算法逐个压缩，使用尚未压缩的权重来补偿误差。因此，最右边的一批权重累积了最多的误差。'
- en: Next, we will leverage these findings to propose a compressed representation
    which can support all these different outlier types.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将利用这些发现来提出一种压缩表示法，该方法可以支持所有这些不同的异常值类型。
- en: '4 SpQR: A Sensitivity-aware compressed representation'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '4 SpQR: 一种灵敏度感知的压缩表示法'
- en: 4.1 Overview
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 概述
- en: 'Existing LLM quantization algorithms treat low- and high-sensitivity weights
    equally; however, our above discussion suggests that this may lead to sub-optimal
    quantization. Ideally, we would want the representation to assign more of its
    “size budget” to sensitive weights. However, these weights are scattered in the
    weight matrix as either individual weights or small groups, for example, partial
    rows or attention head. To capture this structure, we are introducing two changes
    to the quantization procedure: one for capturing small sensitive groups, and another
    for capturing individual outliers.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的LLM量化算法将低灵敏度和高灵敏度的权重视为相同；然而，我们的上述讨论表明，这可能导致次优的量化。理想情况下，我们希望表示法将更多的“大小预算”分配给敏感权重。然而，这些权重在权重矩阵中分散存在，可能是单个权重或小组，例如，部分行或注意力头。为了捕捉这种结构，我们在量化过程中引入了两个变化：一个用于捕捉小的敏感组，另一个用于捕捉单个异常值。
- en: 'Capturing small groups of weights with bilevel quantization. In the previous
    section, we observed several cases where weights behave similarly in small consecutive
    groups, with abrupt changes between groups, for example for some attention head
    and partial row outliers (see Figure [7](#S5.F7 "Figure 7 ‣ Ablations. ‣ 5 Experimental
    Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression") left, bottom-center). When applying a standard approach, there will
    be many cases where these weights will be grouped together, sharing the same quantization
    statistics. To reduce the number of such cases, we use groupwise quantization
    with extremely small groups, typically of $\beta_{1}{=}8-32$ consecutive weights,
    there is a separate quantization scale and zero-point. This choice runs contrary
    to current intuition: for instance, the recent work of Yao et al. [[45](#bib.bib45)]
    explicitly recommends against small groups, arguing that the overhead for storing
    quantization statistics would outweigh the precision advantages.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用双级量化捕获小组的权重。在前一部分，我们观察到一些情况，其中权重在小的连续组中表现相似，组间存在突然的变化，例如在某些注意力头和部分行异常值（参见图
    [7](#S5.F7 "图 7 ‣ 消融实验 ‣ 5 实验验证 ‣ SpQR：一种稀疏量化表示用于近无损 LLM 权重压缩") 左侧，下中间）。当应用标准方法时，这些权重将被归为同一组，共享相同的量化统计数据。为了减少这种情况的数量，我们使用具有极小组的组内量化，通常为$\beta_{1}{=}8-32$个连续权重，每个组有独立的量化尺度和零点。这一选择与当前的直觉相悖：例如，Yao等人近期的工作[[45](#bib.bib45)]明确反对小组，认为存储量化统计数据的开销会超过精度上的优势。
- en: To circumvent this issue, we quantize the groupwise statistics themselves using
    the same quantization algorithm as for weights — asymmetric (min-max) quantization.
    Because of how min-max quantization works, the range of quantized values will
    fit to the groups with largest (or smallest) quantization scale, quantizing them
    perfectly. In other words, we group groupwise statistics from $\beta_{2}=16$ consecutive
    values and quantize them together in the same number of bits, such that groups
    with atypical quantization parameters end up using more of the “quantization budget”.
    Finally, both first and second-level quantization is directly within the quantization
    process, allowing the algorithm to compensate the second-level quantization error
    where possible.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绕过这个问题，我们使用与权重相同的量化算法——非对称（最小-最大）量化，对组内统计数据进行量化。由于最小-最大量化的工作原理，量化值的范围将适应具有最大（或最小）量化尺度的组，从而完美量化它们。换句话说，我们将$\beta_{2}=16$个连续值的组内统计数据进行分组，并在相同的比特数中进行量化，使得具有非典型量化参数的组最终使用更多的“量化预算”。最后，第一和第二级量化直接在量化过程中进行，使算法能够在可能的情况下补偿第二级量化误差。
- en: High-sensitivity outliers. Our analysis showed the existence of cases where
    a small percentage of sensitive weights come in small groups (in the self-attention)
    or individual “outliers” (in the MLP). In some cases, 1% of the weights account
    for over 75% of the total quantization error. Since these weights appear to lead
    to high, irreducible error, we choose to keep these outliers in high precision
    (16-bit). As these outliers are often unstructured, we encode them individually
    in a row-wise arrangement similar to a compressed-sparse-row (CSR) representation [[14](#bib.bib14)].
    This can encode both individual outliers and small structures that do not fit
    into the above definition of groups.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 高灵敏度异常值。我们的分析显示，存在少量敏感权重以小组（在自注意力中）或单个“异常值”（在MLP中）的形式出现的情况。在某些情况下，1%的权重占总量化误差的超过75%。由于这些权重似乎会导致高且不可减少的误差，我们选择以高精度（16位）保留这些异常值。由于这些异常值通常是非结构化的，我们以类似于压缩稀疏行（CSR）表示的逐行排列方式对其进行编码[[14](#bib.bib14)]。这可以编码单个异常值和不符合上述组定义的小结构。
- en: 'The procedure for detecting the outliers is described in detail in Alg. [1](#alg1
    "Algorithm 1 ‣ 4.1 Overview ‣ 4 SpQR: A Sensitivity-aware compressed representation
    ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression").
    If follows a rough two-step procedure: (1) find and isolate outliers as 16-bit
    weights, (2) quantize the non-outlier “base” weights into 3-4 bit and transfer
    the remaining quantization into the the 16-bit outliers weights. For the outlier
    isolation step, the algorithm implements a filtering technique based on the sensitivity
    criterion in Eq. ([2](#S3.E2 "In 3.1 Parameter sensitivity under quantization
    ‣ 3 Quantization sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression")), which is used to isolate and separate
    outliers from base weights. Globally, for each matrix, the algorithm aims to pick
    a sensitivity threshold $\tau$.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '异常值检测程序在算法 [1](#alg1 "Algorithm 1 ‣ 4.1 Overview ‣ 4 SpQR: A Sensitivity-aware
    compressed representation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression") 中详细描述。它遵循大致的两步程序：（1）将异常值识别并隔离为 16 位权重，（2）将非异常值“基础”权重量化为
    3-4 位，并将剩余的量化转移到 16 位异常值权重中。在异常值隔离步骤中，算法实现了一种基于敏感度标准的过滤技术（见 Eq. [2](#S3.E2 "In
    3.1 Parameter sensitivity under quantization ‣ 3 Quantization sensitivity of LLM
    weights ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression")），用于隔离和分离基础权重中的异常值。总体而言，对于每个矩阵，算法旨在选择一个敏感度阈值 $\tau$。'
- en: Following this first outlier detection step, we quantize the base weights ignoring
    all outliers that occur in the same quantization group. As such, the quantization
    statistics (e.g. scales) are computed by excluding outliers. This results in significant
    improvements in terms of error, since e.g. the min-max scales will be significantly
    reduced. The algorithm then proceeds to apply GPTQ to quantize the remaining weights.
    Interestingly, unlike [[6](#bib.bib6)], a weight can be chosen to be an outlier
    not only if it causes error by itself, but also if the GPTQ algorithm can employ
    this weight to compensate errors from many other weights. Thus, the resulting
    16-bit value will contain not the original weight, but a weight that was adjusted
    to minimize the output error. As such, SpQR goes beyond mere detection of outliers
    towards the more general notion of isolating and treating outliers that occur
    during the quantization process. Finally, the algorithm gathers and compresses
    sparse outlier matrix as well as the final quantization statistics with bilevel
    quantization and returns the compressed weights and their metadata.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成第一次异常值检测步骤后，我们量化基础权重，忽略所有出现在同一量化组中的异常值。因此，量化统计信息（例如，尺度）是通过排除异常值来计算的。这在误差方面带来了显著改善，因为例如最小-最大尺度将显著减少。然后，算法继续应用
    GPTQ 来量化剩余的权重。有趣的是，与 [[6](#bib.bib6)] 不同，一个权重不仅可以被选择为异常值，如果它本身导致误差，还可以如果 GPTQ
    算法可以利用这个权重来弥补其他许多权重的误差。因此，得到的 16 位值将包含的不是原始权重，而是一个经过调整以最小化输出误差的权重。因此，SpQR 不仅仅是检测异常值，而是更一般地隔离和处理量化过程中出现的异常值。最后，算法收集和压缩稀疏异常值矩阵以及最终量化统计信息，使用双级量化，并返回压缩后的权重及其元数据。
- en: 'Algorithm 1 SpQR quantization algorithm: the left snippet describes the full
    procedure, the right side contains subroutines for bilevel quantization and finding
    outliers.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 SpQR 量化算法：左侧代码片段描述了完整过程，右侧包含了双级量化和异常值检测的子程序。
- en: func  SpQRQuantize($W,X,b,\beta_{1},\beta_{2},\tau,\lambda$)
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 SpQRQuantize($W,X,b,\beta_{1},\beta_{2},\tau,\lambda$)
- en: 1:$W\in\mathcal{R}^{m\times n}$
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '1: $W\in\mathcal{R}^{m\times n}$'
- en: func  $\textbf{quantize}(M,\vec{s},\vec{z})$
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\textbf{quantize}(M,\vec{s},\vec{z})$
- en: 1:return  $\lfloor M/\vec{s}+\vec{z}+0.5\rfloor$
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 返回 $\lfloor M/\vec{s}+\vec{z}+0.5\rfloor$'
- en: func  $\textbf{dequantize}(Q,\vec{s},\vec{z})$
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\textbf{dequantize}(Q,\vec{s},\vec{z})$
- en: 1:return  $\vec{s}\cdot(Q-\vec{z})$
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 返回 $\vec{s}\cdot(Q-\vec{z})$'
- en: func  $\textbf{fit\_quantizer}(M,\beta)$
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\textbf{fit\_quantizer}(M,\beta)$
- en: 1:$\vec{m}:=\text{flatten}(M)$
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '1: $\vec{m}:=\text{flatten}(M)$'
- en: func  $\textbf{error}(W,H^{\text{ic}})$
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\textbf{error}(W,H^{\text{ic}})$
- en: 1:$\vec{s},\vec{z}:=\text{fit\_quantizer}(W,\beta_{1})$
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '1: $\vec{s},\vec{z}:=\text{fit\_quantizer}(W,\beta_{1})$'
- en: func  $\textbf{outliers}(W,H^{\text{ic}},\mathcal{O})$
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\textbf{outliers}(W,H^{\text{ic}},\mathcal{O})$
- en: 1:$E_{\text{base}}=\text{error}(W,H^{\text{ic}})$
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '1: $E_{\text{base}}=\text{error}(W,H^{\text{ic}})$'
- en: func  $\textbf{fit\_statistics}(W,\mathcal{S},\mathcal{O})$
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 $\textbf{fit\_statistics}(W,\mathcal{S},\mathcal{O})$
- en: 1:$W:=W\cdot(1-\text{is\_outlier}(W,O))$
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '1: $W:=W\cdot(1-\text{is\_outlier}(W,O))$'
- en: 'Implementation details. Our algorithm also contains several optimizations.
    As we are using small group sizes, it is often the case that a group contains
    all positive (or all negative) values. Standard quantizers [[10](#bib.bib10),
    [9](#bib.bib9)] require the maximum value to be positive and the minimum value
    to be negative. For small group sizes, removing this requirement results in slightly
    better quality. As a by-product of quantizing the quantization statistics, our
    algorithm allows non-integer zero points. We ablate these and other SpQR components
    in Section [5](#S5 "5 Experimental Validation ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression").'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '实现细节。我们的算法还包含几种优化。由于我们使用小组大小，通常一个组包含所有正值（或所有负值）。标准量化器 [[10](#bib.bib10), [9](#bib.bib9)]
    要求最大值为正，最小值为负。对于小组大小，去除这一要求会稍微提高质量。作为量化统计的副产品，我们的算法允许非整数零点。我们在第 [5](#S5 "5 实验验证
    ‣ SpQR: 一种用于近乎无损LLM权重压缩的稀疏量化表示") 节中对这些和其他SpQR组件进行消融实验。'
- en: 4.2 Implementing and Leveraging the Sparse Quantized Representation
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 实现和利用稀疏量化表示
- en: 'Our algorithm converts homogeneous weights into several data structures of
    various sizes and precisions. Overall, the representation consists of (1) quantized
    weights, (2) first level quantized quantization statistics, second level quantization
    statistics, and (3) the CSR outlier indices and values. We summarize the overall
    structure of SpQR in Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Implementing and Leveraging
    the Sparse Quantized Representation ‣ 4 SpQR: A Sensitivity-aware compressed representation
    ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")
    and describe each component below.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的算法将同质权重转换为几种不同大小和精度的数据结构。总体表示包括（1）量化权重，（2）一级量化统计、二级量化统计，以及（3）CSR异常值索引和数值。我们在图
    [3](#S4.F3 "图 3 ‣ 4.2 实现和利用稀疏量化表示 ‣ 4 SpQR: 一种灵敏度感知的压缩表示 ‣ SpQR: 一种用于近乎无损LLM权重压缩的稀疏量化表示")
    中总结了SpQR的整体结构，并在下面描述了每个组件。'
- en: '![Refer to caption](img/7f5a6663d9cbb0d27512ab01e46fd28c.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7f5a6663d9cbb0d27512ab01e46fd28c.png)'
- en: 'Figure 3: A high-level overview of the SpQR representation for a single weight
    tensor. The right side of the image depicts all stored data types and their dimensions.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3: 单个权重张量的SpQR表示的高层概述。图像的右侧描绘了所有存储的数据类型及其维度。'
- en: 'Storing quantized groups. All non-outlier weights are encoded as a structure
    that contains:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 存储量化组。所有非异常值的权重被编码为一个包含以下内容的结构：
- en: •
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a $b_{w}$-bit individual weight;
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 $b_{w}$ 位的单独权重；
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: a $b_{q}$;
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个 $b_{q}$；
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $16$ quantization scales and zero-points.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $16$ 量化尺度和零点。
- en: As a particular example for a SpQR representation, consider $b_{w}{=}b_{q}{=}3$-bit
    codes. Every 16 weights use a separate 3-bit scale and zero-point. Finally, there
    are four 16-bit scalars for the entire group used for second level quantization.
    To simplify GPU memory access, we keep the quantized values for outlier weights
    in place and adjust the 16-bit versions to compensate for that. We also store
    both quantized weights and quantized quantization statistics in a contiguous memory
    region for each group. When running on a different hardware (e.g. mobile CPUs),
    it is possible to further reduce the memory footprint by removing the quantized
    version of outliers. We leave this direction for future work.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 作为SpQR表示的一个特定示例，考虑 $b_{w}{=}b_{q}{=}3$ 位码。每16个权重使用一个单独的3位尺度和零点。最后，对于整个组，使用四个16位标量用于二级量化。为了简化GPU内存访问，我们将异常值的量化值保留在原地，并调整16位版本以补偿这一点。我们还将量化权重和量化量化统计存储在每个组的连续内存区域中。在不同的硬件（例如移动CPU）上运行时，可以通过去除异常值的量化版本进一步减少内存占用。我们将这个方向留给未来的工作。
- en: 'Storing outliers. Recall that our outliers are unstructured; for storage, we
    sort them by their row first and column second, so that outliers in the same row
    are contiguous in memory. For each outlier, we store two scalars: the 16-bit weight
    value and the 16-bit column index. For each row, we also store a single 32-bit
    number—the total number of outliers in the rows up to the current one for efficient
    inference. This results in an average storage cost of 32.03 to 32.1 bits per sensitive
    weight. This could be reduced significantly by grouping outliers, which we leave
    as future work.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 存储异常值。请记住，我们的异常值是非结构化的；为了存储，我们先按行排序，再按列排序，使得同一行中的异常值在内存中是连续的。对于每个异常值，我们存储两个标量：16位的权重值和16位的列索引。对于每一行，我们还存储一个32位的数字——当前行之前的所有行中的异常值总数，以便于高效推理。这导致每个敏感权重的平均存储成本为32.03到32.1位。通过对异常值进行分组，这一成本可以显著降低，我们将其作为未来的工作。
- en: Inference with SpQR. To illustrate the practicality of our approach, we design
    an efficient GPU-based decoding implementation for the SpQR format, focused on
    the popular token-by-token LLM generation as a use-case.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SpQR进行推理。为了说明我们方法的实际应用，我们为SpQR格式设计了一个高效的基于GPU的解码实现，专注于流行的逐词LLM生成作为应用案例。
- en: We leverage the fact that autoregressive inference on GPUs is memory-bound,
    so high compression rates can hide decoding overheads, to a significant extent.
    At a high level, our algorithm loads group statistics and the quantized weights
    into shared memory (SRAM), dequantizes to 16-bits, and then performs matrix multiplication
    with 16-bit inputs. For handling outliers, we design a sparse matrix algorithm
    that takes advantage of outliers that occur in rows. Roughly, the algorithm works
    as follows
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用了GPU上的自回归推理是内存绑定的这一事实，因此高压缩率可以在很大程度上隐藏解码开销。从高层次看，我们的算法将组统计信息和量化权重加载到共享内存（SRAM）中，解量化为16位，然后进行16位输入的矩阵乘法。为了处理异常值，我们设计了一种稀疏矩阵算法，利用了在行中出现的异常值。大致而言，算法如下：
- en: First, (1) we divide the matrix into equally sized blocks. Then, each GPU core
    (thread block) (2) loads a large slice of outliers into shared memory (SRAM),
    and each GPU core (3) determines if outliers are part of the segment or not. The
    corresponding weights are (4) loaded from main memory; finally, the matrix multiplication
    is performed.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，（1）我们将矩阵划分为大小相等的块。然后，每个GPU核心（线程块）（2）将大量异常值加载到共享内存（SRAM）中，每个GPU核心（3）确定异常值是否是该段的一部分。相应的权重（4）从主内存中加载；最后，执行矩阵乘法。
- en: 'This algorithm essentially performs load balancing through steps (1-3), while
    step (4) tends to have contiguous memory access due to the row-like patterns for
    the outliers. We will show in Section [5](#S5 "5 Experimental Validation ‣ SpQR:
    A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression") that
    this custom approach is faster than the sparse matrix algorithms in PyTorch.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '该算法本质上通过步骤（1-3）进行负载均衡，而步骤（4）由于异常值的行状模式，往往具有连续的内存访问。我们将在第[5](#S5 "5 Experimental
    Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression")节中展示，这种自定义方法比PyTorch中的稀疏矩阵算法更快。'
- en: 5 Experimental Validation
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验验证
- en: Experimental setup.
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验设置。
- en: 'We focus on three main settings: 1) evaluating what is the most compact representation
    with which SpQR can replicate the performance of a 16-bit model within 1% perplexity,
    2) controlling for the average number of bits per parameter across methods and
    assess the performance of SpQR compared to round-to-nearest and GPTQ baselines,
    3) what is the best trade-off in terms of model size and performance. For these
    settings, we evaluate the full SpQR algorithm on publicly-available LLMs. We focus
    on the LLaMA $\{7,13,30,65\}$B model family [[35](#bib.bib35)]. We quantize LLaMa
    models using the RedPajama dataset and Falcon models on RefinedWeb dataset [[36](#bib.bib36)],
    publicly-available replicas of the LLaMA and Falcon training data, respectively.
    In addition, we provide perplexity results for OPT models in Appendix [F](#A6
    "Appendix F Additional results for near-lossless compression ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression").'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注三个主要设置：1）评估SpQR在什么样的最紧凑表示下可以在1%困惑度内复制16位模型的性能；2）控制方法的每个参数的平均位数，并评估SpQR相对于四舍五入和GPTQ基线的性能；3）在模型大小和性能方面的最佳权衡。对于这些设置，我们在公开的LLM上评估完整的SpQR算法。我们关注LLaMA
    $\{7,13,30,65\}$B模型系列 [[35](#bib.bib35)]。我们使用RedPajama数据集对LLaMa模型进行量化，对Falcon模型使用RefinedWeb数据集 [[36](#bib.bib36)]，这两个数据集分别是LLaMA和Falcon训练数据的公开副本。此外，我们在附录 [F](#A6
    "附录 F 近无损压缩的额外结果 ‣ SpQR：一种稀疏量化表示，用于近无损LLM权重压缩")中提供OPT模型的困惑度结果。
- en: 'We compare SpQR against two other post-training quantization schemes: GPTQ [[9](#bib.bib9)]
    and simple rounding-to-nearest (RTN) quantization, which is used by most other
    LLM compression methods [[6](#bib.bib6), [44](#bib.bib44)]. Both baselines use
    4-bit quantization since it provides the best quality to size trade-off [[7](#bib.bib7)].
    For SpQR, we consider both 3-bit and 4-bit base quantization, though the resulting
    model size can be slightly larger due to the presence of outliers.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将SpQR与另外两种后训练量化方案进行比较：GPTQ [[9](#bib.bib9)]和简单的四舍五入（RTN）量化，大多数其他LLM压缩方法使用这种量化方式 [[6](#bib.bib6),
    [44](#bib.bib44)]。两个基线都使用4位量化，因为它提供了最佳的质量与大小权衡 [[7](#bib.bib7)]。对于SpQR，我们考虑了3位和4位基本量化，但由于存在离群值，结果模型的大小可能略大。
- en: 'We evaluate quantized model performance by two metrics. Firstly, we measure
    *perplexity*, measured on the WikiText2 [[21](#bib.bib21)], Penn Treebank [[20](#bib.bib20)]
    and C4 [[29](#bib.bib29)] datasets. Secondly, we measure zero-shot accuracy on
    five tasks: WinoGrande [[31](#bib.bib31)], PiQA [[34](#bib.bib34)], HellaSwag,
    ARC-easy and ARC-challenge [[3](#bib.bib3)]. We use the LM Evaluation Harness [[13](#bib.bib13)]
    with recommended parameters. We provide full configurations in Appendix [B](#A2
    "Appendix B Experimental Configurations ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression"), as well as code which we plan to release
    publicly. Our implementation takes around 4.5 hours on the largest model size
    (65B) on an NVIDIA A100 and about 6 on an A6000.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过两个指标评估量化模型的性能。首先，我们测量*困惑度*，在WikiText2 [[21](#bib.bib21)]、Penn Treebank [[20](#bib.bib20)]和C4 [[29](#bib.bib29)]数据集上进行测量。其次，我们测量五个任务的零-shot准确率：WinoGrande [[31](#bib.bib31)]、PiQA [[34](#bib.bib34)]、HellaSwag、ARC-easy和ARC-challenge [[3](#bib.bib3)]。我们使用LM
    Evaluation Harness [[13](#bib.bib13)]，按照推荐的参数配置。我们在附录 [B](#A2 "附录 B 实验配置 ‣ SpQR：一种稀疏量化表示，用于近无损LLM权重压缩")中提供完整配置，以及计划公开发布的代码。我们的实现需要大约4.5小时在最大模型尺寸（65B）上使用NVIDIA
    A100，A6000上约需6小时。
- en: 'To control for model size, we evaluate RTN and GPTQ with 4-bit base quantization.
    For SpQR we use 3-bit base quantization, a group size of 8 with 3-bit for the
    first quantization, a group size of 64 for the second quantization, and as many
    outliers as possible to still reach less than 4-bits per parameter on average.
    We aim to achieve *near-lossless* compression, for which we adopt the definition
    of the MLCommons benchmark [[28](#bib.bib28)]: 1% error relative to the uncompressed
    baseline. In all SpQR evaluations, we choose $\tau$.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了控制模型大小，我们对RTN和GPTQ进行4位基本量化评估。对于SpQR，我们使用3位基本量化，第一量化组大小为8，第一量化位数为3，第二量化组大小为64，第二量化位数为3，并尽可能多地使用离群值，以使每个参数的平均位数仍小于4位。我们旨在实现*近无损*压缩，为此我们采用MLCommons基准的定义 [[28](#bib.bib28)]：相对于未压缩基线1%的误差。在所有SpQR评估中，我们选择$\tau$。
- en: Main Results.
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结果。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression") measures actual model size versus perplexity
    on LLaMa models on WikiText2, and accuracy on zero-shot tasks. We observe that
    SpQR outperforms GPTQ (and correspondingly RTN) at similar model size by a significant
    margin, especially on smaller models. This improvement comes from both SpQR achieving
    more compression, while also reducing loss degradation. In addition, if we measure
    the bits per parameter needed to come within 1% of the 16-bit performance in terms
    of perplexity, Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression") shows that SpQR with
    4.6 to 4.71 bits per parameter approaches the non-quantized models with at most
    1% margin of error for all models (see Table [4](#S5.F4 "Figure 4 ‣ Main Results.
    ‣ 5 Experimental Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression") and Table [5](#S5.F5 "Figure 5 ‣ Main Results. ‣ 5 Experimental
    Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression") for exact values).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "图 1 ‣ 1 引言 ‣ SpQR: 一种近无损 LLM 权重压缩的稀疏量化表示") 测量了 LLaMa 模型在 WikiText2
    上的实际模型大小与困惑度，以及零-shot 任务的准确性。我们观察到 SpQR 在相似模型大小下显著优于 GPTQ（相应地也优于 RTN），特别是在较小的模型上。这种改进既来自
    SpQR 实现了更多的压缩，同时还减少了损失降级。此外，如果我们测量为了在困惑度方面达到 16 位性能的 1% 以内所需的每个参数位数，图 [1](#S1.F1
    "图 1 ‣ 1 引言 ‣ SpQR: 一种近无损 LLM 权重压缩的稀疏量化表示") 显示 SpQR 在每个参数 4.6 到 4.71 位的情况下，所有模型的误差最多为
    1%（具体值请参见表格 [4](#S5.F4 "图 4 ‣ 主要结果 ‣ 5 实验验证 ‣ SpQR: 一种近无损 LLM 权重压缩的稀疏量化表示") 和表格 [5](#S5.F5
    "图 5 ‣ 主要结果 ‣ 5 实验验证 ‣ SpQR: 一种近无损 LLM 权重压缩的稀疏量化表示")）。'
- en: 'The second set of results, presented in Table [4](#S5.F4 "Figure 4 ‣ Main Results.
    ‣ 5 Experimental Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression") for LLaMa and Table [5](#S5.F5 "Figure 5 ‣ Main Results.
    ‣ 5 Experimental Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression") for Falcon family models, controls model size by comparing
    SpQR and baseline methods with 4 bits per parameter. These results show that SpQR
    improves over previous methods, with the gap between SpQR and the next best method
    GPTQ being as large as the improvement of GPTQ over naive RTN. For 4-bit, SpQR
    halves the error relative to the 16-bit baseline compared to GPTQ.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '第二组结果展示在表格 [4](#S5.F4 "图 4 ‣ 主要结果 ‣ 5 实验验证 ‣ SpQR: 一种近无损 LLM 权重压缩的稀疏量化表示")（针对
    LLaMa）和表格 [5](#S5.F5 "图 5 ‣ 主要结果 ‣ 5 实验验证 ‣ SpQR: 一种近无损 LLM 权重压缩的稀疏量化表示")（针对 Falcon
    家族模型），通过将 SpQR 和基线方法与每个参数 4 位进行比较来控制模型大小。这些结果表明 SpQR 相比于以前的方法有显著提升，SpQR 和下一个最佳方法
    GPTQ 之间的差距与 GPTQ 相比于原始 RTN 的改进幅度相当。对于 4 位，SpQR 将相对 16 位基线的误差减半，相较于 GPTQ。'
- en: '{floatrow}\capbtabbox'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '{floatrow}\capbtabbox'
- en: '| LLaMa |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLaMa |  |'
- en: '| Size | Method | Avg bits | Wiki2 | C4 | PTB |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| Size | Method | Avg bits | Wiki2 | C4 | PTB |'
- en: '| 7B | – | 16.00 | 5.68 | 7.08 | 8.80 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | 16.00 | 5.68 | 7.08 | 8.80 |'
- en: '| SpQR | 4.63 | 5.73 | 7.13 | 8.88 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.63 | 5.73 | 7.13 | 8.88 |'
- en: '| RTN | 4 | 6.43 | 7.93 | 10.30 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 6.43 | 7.93 | 10.30 |'
- en: '| GPTQ | 4 | 6.13 | 7.43 | 9.27 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 6.13 | 7.43 | 9.27 |'
- en: '| SpQR | 3.94 | 5.87 | 7.28 | 9.07 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.94 | 5.87 | 7.28 | 9.07 |'
- en: '| 13B | – | 16.00 | 5.09 | 6.61 | 8.07 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 13B | – | 16.00 | 5.09 | 6.61 | 8.07 |'
- en: '| SpQR | 4.63 | 5.13 | 6.64 | 8.13 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.63 | 5.13 | 6.64 | 8.13 |'
- en: '| RTN | 4 | 5.55 | 6.98 | 8.65 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 5.55 | 6.98 | 8.65 |'
- en: '| GPTQ | 4 | 5.40 | 6.84 | 8.44 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 5.40 | 6.84 | 8.44 |'
- en: '| SpQR | 3.96 | 5.22 | 6.72 | 8.22 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.96 | 5.22 | 6.72 | 8.22 |'
- en: '| Size | Method | Avg bits | Wiki2 | C4 | PTB |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Size | Method | Avg bits | Wiki2 | C4 | PTB |'
- en: '| 30B | – | 16.00 | 4.10 | 5.98 | 7.30 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 30B | – | 16.00 | 4.10 | 5.98 | 7.30 |'
- en: '| SpQR | 4.69 | 4.14 | 6.01 | 7.33 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.69 | 4.14 | 6.01 | 7.33 |'
- en: '| RTN | 4 | 4.57 | 6.34 | 7.75 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 4.57 | 6.34 | 7.75 |'
- en: '| GPTQ | 4 | 4.48 | 6.20 | 7.54 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 4.48 | 6.20 | 7.54 |'
- en: '| SpQR | 3.89 | 4.25 | 6.08 | 7.38 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.89 | 4.25 | 6.08 | 7.38 |'
- en: '| 65B | – | 16.00 | 3.53 | 5.62 | 6.91 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 65B | – | 16.00 | 3.53 | 5.62 | 6.91 |'
- en: '| SpQR | 4.71 | 3.57 | 5.64 | 6.93 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.71 | 3.57 | 5.64 | 6.93 |'
- en: '| RTN | 4 | 3.87 | 5.85 | 7.17 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 3.87 | 5.85 | 7.17 |'
- en: '| GPTQ | 4 | 3.83 | 5.80 | 7.07 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 3.83 | 5.80 | 7.07 |'
- en: '| SpQR | 3.90 | 3.68 | 5.70 | 6.99 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.90 | 3.68 | 5.70 | 6.99 |'
- en: 'Figure 4: Perplexity on WikiText2 [[21](#bib.bib21)], C4 [[29](#bib.bib29)]
    and Penn Treebank [[20](#bib.bib20)] for SpQR and round-to-nearest (RTN) and GPTQ
    baselines with LLaMa. We can see that SpQR reaches performances within 1% of the
    perplexity with less than 4.71 bits per parameter. We also see that for 4-bits
    per parameter SpQR significantly improves on GPTQ with an improvement as large
    as the improvement from RTN to GPTQ.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：SpQR、最近邻法（RTN）和GPTQ基准在WikiText2 [[21](#bib.bib21)]、C4 [[29](#bib.bib29)]和Penn
    Treebank [[20](#bib.bib20)]上的困惑度。我们可以看到，SpQR的表现达到了困惑度的1%以内，参数每位少于4.71位。我们还可以看到，对于每位4位的SpQR相比于GPTQ有显著提升，提升幅度与RTN到GPTQ的提升幅度相当。
- en: '{floatrow}\capbtabbox'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '{floatrow}\capbtabbox'
- en: '| Falcon |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| Falcon |  |'
- en: '| Size | Method | Avg bits | Wiki2 | C4 | PTB |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Wiki2 | C4 | PTB |'
- en: '| 7B | – | 16.00 | 6.59 | 9.50 | 9.90 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | 16.00 | 6.59 | 9.50 | 9.90 |'
- en: '| SpQR | 4.44 | 6.64 | 9.58 | 9.97 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.44 | 6.64 | 9.58 | 9.97 |'
- en: '| RTN | 4 | 8.73 | 12.56 | 13.76 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 8.73 | 12.56 | 13.76 |'
- en: '| GPTQ | 4 | 6.91 | 9.93 | 10.33 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 6.91 | 9.93 | 10.33 |'
- en: '| SpQR | 3.92 | 6.74 | 9.70 | 19.114 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.92 | 6.74 | 9.70 | 19.114 |'
- en: '| Size | Method | Avg bits | Wiki2 | C4 | PTB |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Wiki2 | C4 | PTB |'
- en: '| 40B | – | 16.00 | 5.23 | 7.76 | 7.83 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 40B | – | 16.00 | 5.23 | 7.76 | 7.83 |'
- en: '| SpQR | 4.46 | 5.26 | 7.79 | 7.86 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.46 | 5.26 | 7.79 | 7.86 |'
- en: '| RTN | 4 | 6.52 | 9.76 | 10.63 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 6.52 | 9.76 | 10.63 |'
- en: '| GPTQ | 4 | 5.36 | 7.95 | 8.01 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 5.36 | 7.95 | 8.01 |'
- en: '| SpQR | 3.90 | 5.29 | 7.85 | 7.91 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.90 | 5.29 | 7.85 | 7.91 |'
- en: 'Figure 5: Perplexity on WikiText2 [[21](#bib.bib21)], C4 [[29](#bib.bib29)]
    and Penn Treebank [[20](#bib.bib20)] for SpQR and round-to-nearest (RTN) and GPTQ
    baselines on Falcon model. We can see that SpQR reaches performances within 1%
    of the perplexity with less than 4.5 bits per parameter. We also see that for
    4-bits per parameter SpQR significantly improves on GPTQ with an improvement as
    large as the improvement from RTN to GPTQ.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：SpQR、最近邻法（RTN）和GPTQ基准在Falcon模型上的困惑度。我们可以看到，SpQR的表现达到了困惑度的1%以内，参数每位少于4.5位。我们还可以看到，对于每位4位的SpQR相比于GPTQ有显著提升，提升幅度与RTN到GPTQ的提升幅度相当。
- en: Ablations.
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消融实验。
- en: 'The SpQR representation differs from standard quantization methods in two main
    ways: bilevel quantization with small quantization group size and unstructured
    outliers. To understand the effect of small group sizes, we compare 3-bit SpQR
    with group size 16, compressed using 3-bit bilevel quantization, versus a setup
    with group size 48, keeping quantization statistics in 16-bit. Both configurations
    result in approximately 3.6 average bits per parameter. For simplicity, neither
    uses outliers. We report both in Table [7](#S5.F7 "Figure 7 ‣ Ablations. ‣ 5 Experimental
    Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression"), the “3-bit statistics“ entry corresponds to group size 16 with
    3-bit statistics and “16-bit statistics“ stands for group size 16 with 16-bit
    statistics. Given the same (slightly smaller) memory footprint, using quantized
    statistics significantly improves language modeling loss.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: SpQR表示法与标准量化方法有两个主要不同点：双级量化的小量化组大小和非结构化的离群值。为了理解小组大小的效果，我们将3-bit SpQR（组大小为16，使用3-bit双级量化）与组大小为48的设置（保持量化统计为16-bit）进行比较。两种配置的平均位数都约为3.6位。为了简化起见，两者均未使用离群值。我们在表[7](#S5.F7
    "图7 ‣ 消融实验。 ‣ 5 实验验证 ‣ SpQR：一种近乎无损的LLM权重压缩稀疏量化表示")中报告了这两种情况，其中“3-bit统计”条目对应组大小16和3-bit统计，而“16-bit统计”代表组大小16和16-bit统计。鉴于相同（稍小的）内存占用，使用量化统计显著提高了语言建模的损失。
- en: 'Next, we ask whether it is necessary to use unstructured outliers, considering
    two outlier types. First, we use the criterion of Dettmers et al. [[7](#bib.bib7)]
    to find column outliers and quantize them in higher precision. The alternative
    is to treat the entire rows (output units / hidden units / neurons) as outliers:
    we run SpQR without outliers, then select $k$ output units that have the highest
    quantization error (i.e. MSE between layer predictions) and treat the entire rows
    as 16-bit outliers. We compare the three outlier types on top of 3-bit SpQR and
    report the results in Figure [7](#S5.F7 "Figure 7 ‣ Ablations. ‣ 5 Experimental
    Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression"). Overall, unstructured outliers reduce perplexity significantly
    faster than their row counterpart and the criterion of [[7](#bib.bib7)], even
    after accounting for the different memory footprint.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们探讨是否有必要使用非结构化的离群值，并考虑了两种离群值类型。首先，我们使用Dettmers等人的标准[[7](#bib.bib7)]来查找列离群值，并以更高的精度对其进行量化。另一种选择是将整个行（输出单元/隐藏单元/神经元）视为离群值：我们在没有离群值的情况下运行SpQR，然后选择量化误差最高的$k$个输出单元（即层预测之间的均方误差MSE），并将整个行视为16位离群值。我们在3位SpQR的基础上比较这三种离群值类型，并在图[7](#S5.F7
    "Figure 7 ‣ Ablations. ‣ 5 Experimental Validation ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")中报告结果。总体而言，非结构化离群值比其行对应物和标准[[7](#bib.bib7)]显著更快地降低困惑度，即使在考虑了不同的内存占用后也是如此。'
- en: 'Finally, we analyze the impact of the minor hyperparameter changes that we
    introduced at the end of Section [4](#S4 "4 SpQR: A Sensitivity-aware compressed
    representation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM
    Weight Compression"). In Table [7](#S5.F7 "Figure 7 ‣ Ablations. ‣ 5 Experimental
    Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression") (bottom), we evaluate quantization errors without these changes.
    The “Round zero” entry corresponds to a version of SpQR where the zero-point is
    a 3-bit integer. This reduces the memory footprint of SpQR, but results in a moderate
    increase in perplexity. Similarly, we evaluate SpQR without the “act order” flag.
    This option re-orders the input dimensions by the diagonal of the inverse hessian,
    which was introduced as a part of the GPTQ algorithm. Using this heuristic slightly
    improves loss, though not as much as from quantized groups.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们分析了在第[4](#S4 "4 SpQR: A Sensitivity-aware compressed representation ‣
    SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")节末尾引入的次要超参数变化的影响。在表[7](#S5.F7
    "Figure 7 ‣ Ablations. ‣ 5 Experimental Validation ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")（底部），我们评估了没有这些变化的量化误差。“Round
    zero”条目对应于一个SpQR版本，其中零点是3位整数。这减少了SpQR的内存占用，但导致了困惑度的适度增加。同样，我们评估了没有“act order”标志的SpQR。这个选项通过逆Hessian的对角线重新排序输入维度，这作为GPTQ算法的一部分引入。使用这个启发式方法可以稍微改善损失，但不如量化组的效果明显。'
- en: '{floatrow}\capbtabbox'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '{floatrow}\capbtabbox'
- en: 'Figure 6: Perplexity for LLaMA-65B model.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：LLaMA-65B模型的困惑度。
- en: '| Name | Wiki2 | C4 | PTB | Avg bits |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | Wiki2 | C4 | PTB | 平均位数 |'
- en: '| Uncompressed | 3.53 | 5.62 | 6.91 | 16 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 未压缩 | 3.53 | 5.62 | 6.91 | 16 |'
- en: '| GPTQ (4 bit) | 3.83 | 5.80 | 7.07 | 4 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ（4位） | 3.83 | 5.80 | 7.07 | 4 |'
- en: '| 3-bit statistics | 3.74 | 5.73 | 7.02 | 3.63 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 3位统计 | 3.74 | 5.73 | 7.02 | 3.63 |'
- en: '| 16-bit statistics | 3.84 | 5.83 | 7.12 | 3.67 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 16位统计 | 3.84 | 5.83 | 7.12 | 3.67 |'
- en: '| Round zero | 3.75 | 5.76 | 7.01 | 3.63 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Round zero | 3.75 | 5.76 | 7.01 | 3.63 |'
- en: '| w/o act order | 3.74 | 5.76 | 7.05 | 3.63 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 无act order | 3.74 | 5.76 | 7.05 | 3.63 |'
- en: \ffigbox![Refer to caption](img/34dec1bfe7c59e166e6b4352c07697e0.png)
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: \ffigbox![参考标题](img/34dec1bfe7c59e166e6b4352c07697e0.png)
- en: 'Figure 7: Different outlier types, LLaMA-65B.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：不同离群值类型，LLaMA-65B。
- en: 'To summarize, both small quantized groups and unstructured outliers independently
    improve perplexity and perform better than alternative strategies. SpQR also benefits
    from using the GPTQ activation order heuristic, though the gain is smaller than
    from outliers or small groups. Still, we opt to use the same activation order
    heuristic in the GPTQ baselines to ensure a fair comparison. To further explore
    the design space of SpQR, we provide an additional hyperparameter study in Appendix [C](#A3
    "Appendix C Hyperparameter sensitivity ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression").'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，小的量化组和非结构化离群点都独立地提高了困惑度，并且表现优于其他策略。SpQR 还受益于使用 GPTQ 激活顺序启发式，但增益小于离群点或小组。尽管如此，我们选择在
    GPTQ 基准测试中使用相同的激活顺序启发式，以确保公平比较。为了进一步探索 SpQR 的设计空间，我们在附录 [C](#A3 "附录 C 超参数敏感性 ‣
    SpQR：一种接近无损 LLM 权重压缩的稀疏量化表示")中提供了额外的超参数研究。
- en: Inference Time.
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 推理时间。
- en: 'Finally, we evaluate the inference speed of SpQR for autoregressive inference
    with a focus on measuring the token generation latency with batch size 1 on a
    single A100 GPU. We measure inference speed in two setups: i) generating 100 tokens
    from scratch and ii) adding 100 tokens on top of a 1024-token prefix (prompt).
    We compare our specialized sparse matrix multiplication algorithm with the algorithm
    implemented in PyTorch (cuSPARSE). We also compare against a 16-bit baseline.
    We measure the end-to-end latency as inference steps per second for the full SpQR
    algorithm, that is for both the dense and sparse multiplication part together.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估了 SpQR 在自回归推理中的推理速度，重点测量了在单个 A100 GPU 上，批量大小为 1 时的 token 生成延迟。我们在两种设置中测量推理速度：i)
    从头生成 100 个 tokens 和 ii) 在 1024-token 前缀（提示）上添加 100 个 tokens。我们将我们的专业稀疏矩阵乘法算法与
    PyTorch（cuSPARSE）中实现的算法进行比较。我们还与 16 位基准进行比较。我们测量了全 SpQR 算法的端到端延迟，即稠密和稀疏乘法部分的推理步骤每秒。
- en: '| Method | fp16 (baseline) | SpQR (PyTorch) | SpQR (optimized) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | fp16（基准） | SpQR（PyTorch） | SpQR（优化） |'
- en: '| LLaMA | 7B | 13B | 30B | 65B | 7B | 13B | 30B | 65B | 7B | 13B | 30B | 65B
    |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA | 7B | 13B | 30B | 65B | 7B | 13B | 30B | 65B | 7B | 13B | 30B | 65B
    |'
- en: '| scratch | $47\pm 2.3$ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| scratch | $47\pm 2.3$ |'
- en: '| prefix 1024 | $46\pm 2.4$ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| prefix 1024 | $46\pm 2.4$ |'
- en: 'Table 1: Inference speed comparison (tokens/s), OOM means the model did not
    fit in an A100 GPU. We see that our optimized SpQR algorithm is faster than the
    16-bit baseline and almost 2.0x faster than quantized matrix multiplication +
    standard PyTorch sparse matrix multiplication baseline.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：推理速度比较（tokens/s），OOM 表示模型无法适配到 A100 GPU。我们看到，我们优化后的 SpQR 算法比 16 位基准测试快，几乎比量化矩阵乘法
    + 标准 PyTorch 稀疏矩阵乘法基准测试快 2.0 倍。
- en: 'Results are shown in Table [1](#S5.T1 "Table 1 ‣ Inference Time. ‣ 5 Experimental
    Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression"). We can see that while standard sparse matrix multiplication in
    PyTorch is not faster than 16-bit inference, our specialized sparse matrix multiplication
    algorithm yields speedups of about 20-30%.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 结果见表 [1](#S5.T1 "表 1 ‣ 推理时间 ‣ 5 实验验证 ‣ SpQR：一种接近无损 LLM 权重压缩的稀疏量化表示")。我们可以看到，虽然
    PyTorch 中的标准稀疏矩阵乘法不比 16 位推理更快，但我们的专业稀疏矩阵乘法算法实现了约 20-30% 的加速。
- en: 6 Discussion & Limitations
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 讨论与局限性
- en: We have presented SpQR, an quantization approach which quantizes sensitive outliers
    in higher precision, to achieve near-lossless 16-bit accuracy with less than 4.75
    bits per parameter on average. We achieve even better quality-size-tradeoff when
    compressing to as little as 3.36 bits which makes SpQR an ideal method for compressing
    models for memory-limited devices. Despite our promising results, there are several
    limitations. The main limitation is that we do not evaluate the generative quality
    of quantized LLMs, but only the predictive performance in terms of zero-shot accuracy
    and perplexity. While we believe that perplexity measurements and generation quality
    are strongly related, this is a hypothesis we aim to investigate in future work.
    While we devise a sparse matrix multiplication algorithm to accelerate the computation
    with outliers, another limitation is that we do not fuse sparse matrix multiplication
    with regular quantized matrix multiplication. Such an approach would yield even
    better inference time performance. However, such an approach is also very difficult
    to implement. We leave the implementation of such an algorithm to future work.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了 SpQR，一种量化方法，该方法以更高的精度量化敏感的异常值，以实现接近无损的 16 位准确度，平均每个参数少于 4.75 位。在将模型压缩到仅
    3.36 位时，我们取得了更好的质量-大小权衡，这使得 SpQR 成为内存受限设备模型压缩的理想方法。尽管我们的结果很有前景，但仍存在一些局限性。主要的限制是我们没有评估量化
    LLM 的生成质量，只评估了零样本准确性和困惑度的预测性能。虽然我们相信困惑度测量和生成质量之间有很强的相关性，但这是我们未来工作中要研究的假设。虽然我们设计了一个稀疏矩阵乘法算法以加速计算异常值，但另一个限制是我们没有将稀疏矩阵乘法与常规量化矩阵乘法融合。这种方法将带来更好的推理时间性能，但也很难实现。我们将这种算法的实现留待未来工作。
- en: 7 Acknowledgements
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: D.K. was supported by Russian Science Foundation, grant 21-11-00373. D.A. and
    E.F. gratefully acknowledge funding from the European Research Council (ERC) under
    the European Union’s Horizon 2020 research and innovation programme (grant agreement
    No 805223 ScaleML). Authors also thank Ivan Komarov for his help in profiling
    and understanding the performance bottlenecks of SpQR on GPU.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: D.K. 获得了俄罗斯科学基金会的资助，资助号 21-11-00373。D.A. 和 E.F. 感谢欧洲研究委员会（ERC）在欧洲联盟地平线2020研究与创新计划下的资助（资助协议号
    805223 ScaleML）。作者还感谢 Ivan Komarov 在 GPU 上进行性能瓶颈分析和理解的帮助。
- en: References
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: BMR^+ [20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    et al. Language models are few-shot learners. In Conference on Neural Information
    Processing Systems (NeurIPS), 2020.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BMR^+ [20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell
    等。语言模型是少样本学习者。神经信息处理系统会议（NeurIPS），2020。
- en: 'BSA^+ [23] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley,
    Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai
    Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large language models
    across training and scaling. arXiv preprint arXiv:2304.01373, 2023.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'BSA^+ [23] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley,
    Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai
    Prashanth, Edward Raff 等。Pythia: 一个用于分析大型语言模型的工具套件，涵盖训练和扩展。arXiv 预印本 arXiv:2304.01373,
    2023。'
- en: CCE^+ [18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CCE^+ [18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick 和 Oyvind Tafjord。认为你已经解决了问答问题？试试 arc，ai2 推理挑战。arXiv 预印本 arXiv:1803.05457,
    2018。
- en: 'CND^+ [22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
    Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint
    arXiv:2204.02311, 2022.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CND^+ [22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma,
    Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian
    Gehrmann 等。Palm: 通过路径扩展语言建模。arXiv 预印本 arXiv:2204.02311, 2022。'
- en: 'DCLT [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
    BERT: Pre-training of deep bidirectional transformers for language understanding.
    In North American Chapter of the Association for Computational Linguistics (NAACL),
    2019.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'DCLT [19] Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova。BERT:
    深度双向变换器的预训练用于语言理解。北美计算语言学协会（NAACL），2019。'
- en: 'DLBZ [22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8():
    8-bit matrix multiplication for transformers at scale. Advances in Neural Information
    Processing Systems 35: Annual Conference on Neural Information Processing Systems
    2022, NeurIPS 2022, 2022.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DLBZ [22] Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer。LLM.int8()：大规模变换器的
    8 位矩阵乘法。《神经信息处理系统进展》第 35 卷：2022 年神经信息处理系统年会，NeurIPS 2022，2022 年。
- en: 'DZ [22] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit
    inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DZ [22] Tim Dettmers 和 Luke Zettlemoyer。4 位精度的理由：k 位推理缩放规律。arXiv 预印本 arXiv:2212.09720，2022
    年。
- en: FA [23] Elias Frantar and Dan Alistarh. Massive language models can be accurately
    pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FA [23] Elias Frantar 和 Dan Alistarh。大型语言模型可以在一次性剪枝中准确剪枝。arXiv 预印本 arXiv:2301.00774，2023
    年。
- en: 'FAHA [22] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
    Gptq: Accurate post-training quantization for generative pre-trained transformers.
    arXiv preprint arXiv:2210.17323, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FAHA [22] Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。Gptq：生成预训练变换器的准确训练后量化。arXiv
    预印本 arXiv:2210.17323，2022 年。
- en: 'FSA [22] Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal Brain Compression:
    A framework for accurate post-training quantization and pruning. arXiv preprint
    arXiv:2208.11580, 2022. Accepted to NeurIPS 2022, to appear.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FSA [22] Elias Frantar、Sidak Pal Singh 和 Dan Alistarh。最佳脑压缩：一种准确的训练后量化和剪枝框架。arXiv
    预印本 arXiv:2208.11580，2022 年。已接受至 NeurIPS 2022，待发表。
- en: 'GFS^+ [19] Yury Gorbachev, Mikhail Fedorov, Iliya Slavutin, Artyom Tugarev,
    Marat Fatekhov, and Yaroslav Tarkan. Openvino deep learning workbench: Comprehensive
    analysis and tuning of neural networks inference. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision Workshops, pages 0–0, 2019.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GFS^+ [19] Yury Gorbachev、Mikhail Fedorov、Iliya Slavutin、Artyom Tugarev、Marat
    Fatekhov 和 Yaroslav Tarkan。Openvino 深度学习工作台：神经网络推理的综合分析和调优。在 IEEE/CVF 国际计算机视觉大会工作坊论文集中，页码
    0–0，2019 年。
- en: GKD^+ [21] Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney,
    and Kurt Keutzer. A survey of quantization methods for efficient neural network
    inference. arXiv preprint arXiv:2103.13630, 2021.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GKD^+ [21] Amir Gholami、Sehoon Kim、Zhen Dong、Zhewei Yao、Michael W Mahoney 和
    Kurt Keutzer。高效神经网络推理的量化方法综述。arXiv 预印本 arXiv:2103.13630，2021 年。
- en: GTB^+ [21] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi,
    Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and
    Andy Zou. A framework for few-shot language model evaluation, September 2021.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GTB^+ [21] Leo Gao、Jonathan Tow、Stella Biderman、Sid Black、Anthony DiPofi、Charles
    Foster、Laurence Golding、Jeffrey Hsu、Kyle McDonell、Niklas Muennighoff、Jason Phang、Laria
    Reynolds、Eric Tang、Anish Thite、Ben Wang、Kevin Wang 和 Andy Zou。少样本语言模型评估框架，2021
    年 9 月。
- en: 'HABN^+ [21] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and
    Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient inference
    and training in neural networks. arXiv preprint arXiv:2102.00554, 2021.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HABN^+ [21] Torsten Hoefler、Dan Alistarh、Tal Ben-Nun、Nikoli Dryden 和 Alexandra
    Peste。深度学习中的稀疏性：神经网络高效推理和训练的剪枝与增长。arXiv 预印本 arXiv:2102.00554，2021 年。
- en: HBM^+ [22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya,
    Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes
    Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv
    preprint arXiv:2203.15556, 2022.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HBM^+ [22] Jordan Hoffmann、Sebastian Borgeaud、Arthur Mensch、Elena Buchatskaya、Trevor
    Cai、Eliza Rutherford、Diego de Las Casas、Lisa Anne Hendricks、Johannes Welbl、Aidan
    Clark 等。训练计算最优的大型语言模型。arXiv 预印本 arXiv:2203.15556，2022 年。
- en: HNH^+ [21] Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and Daniel Soudry.
    Accurate post training quantization with small calibration sets. In International
    Conference on Machine Learning (ICML), 2021.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HNH^+ [21] Itay Hubara、Yury Nahshan、Yair Hanani、Ron Banner 和 Daniel Soudry。使用小型校准集的准确训练后量化。在国际机器学习大会
    (ICML) 上，2021 年。
- en: 'KHB^+ [21] Daya Khudia, Jianyu Huang, Protonu Basu, Summer Deng, Haixin Liu,
    Jongsoo Park, and Mikhail Smelyanskiy. Fbgemm: Enabling high-performance low-precision
    deep learning inference. arXiv preprint arXiv:2101.05615, 2021.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KHB^+ [21] Daya Khudia、Jianyu Huang、Protonu Basu、Summer Deng、Haixin Liu、Jongsoo
    Park 和 Mikhail Smelyanskiy。Fbgemm：支持高性能低精度深度学习推理。arXiv 预印本 arXiv:2101.05615，2021
    年。
- en: KMH^+ [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling
    laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KMH^+ [20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin
    Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, 和 Dario Amodei. 神经语言模型的扩展法则。arXiv预印本
    arXiv:2001.08361，2020年。
- en: 'LGT^+ [21] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang, and Shi Gu. BRECQ: Pushing the limit of post-training quantization
    by block reconstruction. In International Conference on Learning Representations
    (ICLR), 2021.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LGT^+ [21] Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang, Fengwei
    Yu, Wei Wang, 和 Shi Gu. BRECQ: 通过块重构推动后训练量化的极限。在国际学习表征会议 (ICLR)，2021年。'
- en: 'MKM^+ [94] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre,
    Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. The penn treebank:
    Annotating predicate argument structure. In Human Language Technology: Proceedings
    of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994, 1994.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'MKM^+ [94] Mitch Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre,
    Ann Bies, Mark Ferguson, Karen Katz, 和 Britta Schasberger. Penn树库: 标注谓词论元结构。在人类语言技术:
    1994年3月8-11日在新泽西州普莱恩斯伯勒举行的研讨会论文集，1994年。'
- en: MXBS [16] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
    Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843, 2016.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MXBS [16] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard Socher. 指针哨兵混合模型。arXiv预印本
    arXiv:1609.07843，2016年。
- en: NAVB^+ [20] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos,
    and Tijmen Blankevoort. Up or down? Adaptive rounding for post-training quantization.
    In International Conference on Machine Learning (ICML), 2020.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NAVB^+ [20] Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos,
    和 Tijmen Blankevoort. 向上还是向下？后训练量化的自适应舍入。在国际机器学习会议 (ICML)，2020年。
- en: Neu [22] NeuralMagic. DeepSparse, 2022.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neu [22] NeuralMagic. DeepSparse，2022年。
- en: OEN^+ [22] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova
    DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al.
    In-context learning and induction heads. arXiv preprint arXiv:2209.11895, 2022.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OEN^+ [22] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova
    DasSarma, Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, 等等. 上下文学习与诱导头。arXiv预印本
    arXiv:2209.11895，2022年。
- en: Ope [23] OpenAI. Gpt-4 technical report. arXiv, 2023.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ope [23] OpenAI. Gpt-4技术报告。arXiv，2023年。
- en: 'PGM^+ [19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
    Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
    Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,
    Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala.
    PyTorch: An imperative style, high-performance deep learning library. In Conference
    on Neural Information Processing Systems (NeurIPS). 2019.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PGM^+ [19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury,
    Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
    Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,
    Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, 和 Soumith Chintala.
    PyTorch: 一种命令式风格的高性能深度学习库。在神经信息处理系统会议 (NeurIPS)。2019年。'
- en: 'PPK^+ [22] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo
    Lee, and Dongsoo Lee. nuQmm: Quantized matmul for efficient inference of large-scale
    generative language models. arXiv preprint arXiv:2206.09557, 2022.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PPK^+ [22] Gunho Park, Baeseong Park, Se Jung Kwon, Byeongwook Kim, Youngjoo
    Lee, 和 Dongsoo Lee. nuQmm: 高效推断大规模生成语言模型的量化矩阵乘法。arXiv预印本 arXiv:2206.09557，2022年。'
- en: RCK^+ [20] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson,
    Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark
    Charlebois, William Chou, et al. Mlperf inference benchmark. In 2020 ACM/IEEE
    47th Annual International Symposium on Computer Architecture (ISCA), pages 446–459\.
    IEEE, 2020.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RCK^+ [20] Vijay Janapa Reddi, Christine Cheng, David Kanter, Peter Mattson,
    Guenther Schmuelling, Carole-Jean Wu, Brian Anderson, Maximilien Breughe, Mark
    Charlebois, William Chou, 等等. Mlperf推断基准。在2020年ACM/IEEE第47届计算机体系结构国际年会 (ISCA)，页446–459。IEEE，2020年。
- en: RSR^+ [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer
    learning with a unified text-to-text transformer. Journal of Machine Learning
    Research, 21(140):1–67, 2020.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RSR^+ [20] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, 和 Peter Liu. 利用统一的文本到文本变换器探索迁移学习的极限。机器学习研究期刊，21(140):1–67，2020年。
- en: RWC^+ [19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI
    blog, 1(8):9, 2019.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RWC^+ [19] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei,
    和 Ilya Sutskever. 语言模型是无监督的多任务学习者。OpenAI博客，1(8):9，2019年。
- en: 'SBBC [21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin
    Choi. Winogrande: an adversarial winograd schema challenge at scale. Commun. ACM,
    64(9):99–106, 2021.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SBBC [21] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, 和 Yejin Choi。Winogrande：大规模对抗性Winograd范式挑战。Commun.
    ACM, 64(9):99–106, 2021。
- en: 'SLP^+ [21] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng
    Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint
    arXiv:2104.09864, 2021.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SLP^+ [21] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, 和 Yunfeng
    Liu。Roformer：带有旋转位置嵌入的增强型变换器。arXiv 预印本 arXiv:2104.09864, 2021。
- en: 'TLI^+ [23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv
    preprint arXiv:2302.13971, 2023.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TLI^+ [23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, 等等。Llama：开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971, 2023。
- en: 'TP [03] Sandeep Tata and Jignesh M Patel. PiQA: An algebra for querying protein
    data sets. In International Conference on Scientific and Statistical Database
    Management, 2003.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TP [03] Sandeep Tata 和 Jignesh M Patel。PiQA：查询蛋白质数据集的代数。在国际科学与统计数据库管理会议，2003年。
- en: '[35] TII UAE. The falcon family of large language models. [https://huggingface.co/tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b),
    May 2023.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] TII UAE。大型语言模型的Falcon系列。 [https://huggingface.co/tiiuae/falcon-40b](https://huggingface.co/tiiuae/falcon-40b),
    2023年5月。'
- en: '[36] TII UAE. The refined web dataset. [https://huggingface.co/datasets/tiiuae/falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb),
    May 2023.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] TII UAE。精炼的网络数据集。 [https://huggingface.co/datasets/tiiuae/falcon-refinedweb](https://huggingface.co/datasets/tiiuae/falcon-refinedweb),
    2023年5月。'
- en: Vig [19] Jesse Vig. A multiscale visualization of attention in the transformer
    model. arXiv preprint arXiv:1906.05714, 2019.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vig [19] Jesse Vig。变换器模型中注意力的多尺度可视化。arXiv 预印本 arXiv:1906.05714, 2019。
- en: 'VTM^+ [19] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan
    Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting,
    the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association
    for Computational Linguistics, pages 5797–5808, Florence, Italy, July 2019. Association
    for Computational Linguistics.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VTM^+ [19] Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, 和 Ivan Titov。分析多头自注意力：专业化头部做繁重的工作，其余部分可以被剪枝。在第57届计算语言学协会年会上，页面5797–5808，意大利佛罗伦萨，2019年7月。计算语言学协会。
- en: WBZ^+ [21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu,
    Brian Lester, Nan Du, Andrew M Dai, and Quoc V Le. Finetuned language models are
    zero-shot learners. arXiv preprint arXiv:2109.01652, 2021.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WBZ^+ [21] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu,
    Brian Lester, Nan Du, Andrew M Dai, 和 Quoc V Le。微调语言模型是零样本学习者。arXiv 预印本 arXiv:2109.01652,
    2021。
- en: WCHC [20] Peisong Wang, Qiang Chen, Xiangyu He, and Jian Cheng. Towards accurate
    post-training network quantization via bit-split and stitching. In International
    Conference on Machine Learning (ICML), 2020.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WCHC [20] Peisong Wang, Qiang Chen, Xiangyu He, 和 Jian Cheng。通过位分割和拼接实现准确的后训练网络量化。在国际机器学习大会（ICML），2020年。
- en: WMR^+ [21] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan
    Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane,
    Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick,
    Geoffrey Irving, and Iason Gabriel. Ethical and social risks of harm from language
    models, 2021.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WMR^+ [21] Laura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan
    Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,
    Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane,
    Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick,
    Geoffrey Irving, 和 Iason Gabriel。语言模型的伦理和社会风险，2021年。
- en: 'WSM^+ [18] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
    and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural
    language understanding. arXiv preprint arXiv:1804.07461, 2018.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: WSM^+ [18] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy,
    和 Samuel R Bowman。Glue：用于自然语言理解的多任务基准和分析平台。arXiv 预印本 arXiv:1804.07461, 2018。
- en: 'XLS^+ [22] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. arXiv preprint arXiv:2211.10438, 2022.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XLS^+ [22] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, 和 Song Han。Smoothquant：用于大型语言模型的准确且高效的后训练量化。arXiv
    预印本 arXiv:2211.10438, 2022。
- en: 'YAZ^+ [22] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YAZ^+ [22] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, 和 Yuxiong He. Zeroquant：高效且经济的后训练量化方法用于大规模变换器。arXiv预印本arXiv:2206.01861, 2022。
- en: YLW^+ [23] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A
    comprehensive study on post-training quantization for large language models, 2023.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YLW^+ [23] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, 和 Yuxiong He. 对大规模语言模型后训练量化的全面研究，2023。
- en: Table of contents
  id: totrans-230
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 目录
- en: '[1 Introduction](#S1 "In SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression")'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[1 介绍](#S1 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[2 Related Work](#S2 "In SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression")'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[2 相关工作](#S2 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[3 Quantization sensitivity of LLM weights](#S3 "In SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3 LLM权重的量化敏感性](#S3 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[3.1 Parameter sensitivity under quantization](#S3.SS1 "In 3 Quantization sensitivity
    of LLM weights ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM
    Weight Compression")'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.1 量化下的参数敏感性](#S3.SS1 "在3 LLM权重的量化敏感性 ‣ SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[3.2 Exploring parameter sensitivity](#S3.SS2 "In 3 Quantization sensitivity
    of LLM weights ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM
    Weight Compression")'
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[3.2 探索参数敏感性](#S3.SS2 "在3 LLM权重的量化敏感性 ‣ SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[4 SpQR: A Sensitivity-aware compressed representation](#S4 "In SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4 SpQR：一个敏感性意识的压缩表示法](#S4 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[4.1 Overview](#S4.SS1 "In 4 SpQR: A Sensitivity-aware compressed representation
    ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.1 概述](#S4.SS1 "在4 SpQR：一个敏感性意识的压缩表示法 ‣ SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[4.2 Implementing and Leveraging the Sparse Quantized Representation](#S4.SS2
    "In 4 SpQR: A Sensitivity-aware compressed representation ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[4.2 实施和利用稀疏量化表示法](#S4.SS2 "在4 SpQR：一个敏感性意识的压缩表示法 ‣ SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[5 Experimental Validation](#S5 "In SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression")'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[5 实验验证](#S5 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[6 Discussion & Limitations](#S6 "In SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression")'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[6 讨论与局限性](#S6 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[7 Acknowledgements](#S7 "In SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression")'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[7 致谢](#S7 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[A Additional weight sensitivity analysis](#A1 "In SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[A 额外的权重敏感性分析](#A1 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[B Experimental Configurations](#A2 "In SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression")'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[B 实验配置](#A2 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[C Hyperparameter sensitivity](#A3 "In SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression")'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[C 超参数敏感性](#A3 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[D Estimating model size](#A4 "In SpQR: A Sparse-Quantized Representation for
    Near-Lossless LLM Weight Compression")'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[估算模型大小](#A4 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[E Choice of optimal configuration for fixed average number of bits](#A5 "In
    SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[E 针对固定平均位数的最佳配置选择](#A5 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[F Additional results for near-lossless compression](#A6 "In SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression")'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[F 近无损压缩的附加结果](#A6 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[G Choice of optimal LLM configuration for specific hardware](#A7 "In SpQR:
    A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[G 针对特定硬件的最佳LLM配置选择](#A7 "在SpQR：用于近无损LLM权重压缩的稀疏量化表示法")'
- en: '[H Sensitivity to random seed](#A8 "In SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression")'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[H 对随机种子的敏感性](#A8 "在SpQR：一种接近无损LLM权重压缩的稀疏量化表示中")'
- en: '[I Generative examples](#A9 "In SpQR: A Sparse-Quantized Representation for
    Near-Lossless LLM Weight Compression")'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[I 生成示例](#A9 "在SpQR：一种接近无损LLM权重压缩的稀疏量化表示中")'
- en: '[J Broader impact](#A10 "In SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression")'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[J 更广泛的影响](#A10 "在SpQR：一种接近无损LLM权重压缩的稀疏量化表示中")'
- en: '[K On the use of LLMs in this work](#A11 "In SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression")'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '[K 本研究中LLMs的使用](#A11 "在SpQR：一种接近无损LLM权重压缩的稀疏量化表示中")'
- en: Appendix A Additional weight sensitivity analysis
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 额外的权重敏感性分析
- en: 'In this section, we provide additional visualizations of LLaMA weight sensitivities,
    as well as additional plots for different layer roles. As we observed earlier
    in Section [3.2](#S3.SS2 "3.2 Exploring parameter sensitivity ‣ 3 Quantization
    sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression"), the sensitivity matrices vary based on four main factors:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了LLaMA权重敏感性的额外可视化图，以及不同层角色的附加图表。正如我们在第[3.2](#S3.SS2 "3.2 探索参数敏感性 ‣
    3 LLM权重的量化敏感性 ‣ SpQR：一种接近无损LLM权重压缩的稀疏量化表示")节中观察到的，敏感性矩阵基于四个主要因素变化：
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the quantization scheme (e.g. row- or group-wise);
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 量化方案（例如按行或按组）；
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the layer depth, i.e. the index of the corresponding transformer block;
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层深度，即对应的变换器块的索引；
- en: •
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the role of that weight, e.g. self-attn query / key or MLP up / down projection;
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 权重的角色，例如自注意力查询/关键字或MLP上/下投影；
- en: •
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: the location within the chosen weight matrix;
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在所选权重矩阵中的位置；
- en: 'Here, we report additional observations about these factors and elaborate on
    some of our claims from Section [3.1](#S3.SS1 "3.1 Parameter sensitivity under
    quantization ‣ 3 Quantization sensitivity of LLM weights ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression"). We also report raw
    sensitivity matrices for various weight matrices at the end of the supplementary
    materials.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们报告了关于这些因素的额外观察，并详细阐述了第[3.1](#S3.SS1 "3.1 量化下的参数敏感性 ‣ 3 LLM权重的量化敏感性 ‣ SpQR：一种接近无损LLM权重压缩的稀疏量化表示")节中的一些主张。我们还报告了各种权重矩阵的原始敏感性矩阵，详见补充材料的最后部分。
- en: Relation between sensitivity and the chosen quantization scheme.
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 敏感性与所选量化方案之间的关系。
- en: We compare two configurations of GPTQ 3-bit. The first configuration uses one
    quantization scale & zero for each row. The second one uses blockwise quantization
    with one set of statistics for each block of 128 weights.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们比较了两种3-bit GPTQ配置。第一种配置使用每行一个量化比例和零。第二种配置使用块级量化，为每个128权重的块使用一组统计数据。
- en: '![Refer to caption](img/7987c4bb5d258124726a4f6a1500886a.png)![Refer to caption](img/8fe6ad1e1fe690b839a2adf72b65b602.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7987c4bb5d258124726a4f6a1500886a.png)![参考标题](img/8fe6ad1e1fe690b839a2adf72b65b602.png)'
- en: 'Figure 8: The weight sensitivities for LLaMA-65B 40th layer, attention query
    projection. The color scale represents sensitivity on a logarithmic scale, with
    higher sensitivity being darker. (top) 3-bit GPTQ with per-row quantization scales,
    (bottom) 3-bit GPTQ with block size 128.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：LLaMA-65B 第40层，注意力查询投影的权重敏感性。颜色比例尺表示对数尺度上的敏感性，敏感性越高，颜色越深。（上）每行量化比例的3-bit
    GPTQ，（下）块大小128的3-bit GPTQ。
- en: 'Figure [8](#A1.F8 "Figure 8 ‣ Relation between sensitivity and the chosen quantization
    scheme. ‣ Appendix A Additional weight sensitivity analysis ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression") demonstrates a typical
    example of how group size affects sensitivity. In the bottom-right plot, we observe
    that a subset of weights (width 128) has a significantly higher quantization error
    than the rest of the layer. Please note that the color scale represents sensitivity
    on a logarithmic scale, with higher sensitivity being darker.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 图[8](#A1.F8 "图 8 ‣ 敏感性与所选量化方案之间的关系 ‣ 附录 A 额外的权重敏感性分析 ‣ SpQR：一种接近无损LLM权重压缩的稀疏量化表示")展示了组大小如何影响敏感性的典型例子。在右下角的图中，我们观察到一个权重子集（宽度128）的量化误差显著高于该层的其他部分。请注意，颜色比例尺表示对数尺度上的敏感性，敏感性越高，颜色越深。
- en: On a more detailed examination, we found that this specific group contains a
    “vertical” outlier, i.e. the corresponding input feature has significantly higher
    variance, compared to other input dimensions.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 经过更详细的检查，我们发现这一特定组包含一个“垂直”异常值，即相应的输入特征相比其他输入维度具有显著更高的方差。
- en: In this example, the main effect of GPTQ block size 128 is that the problematic
    dimension leads to increased sensitivity in a group of $8192\times 128$ weights.
    In turn, GPTQ with per-row statistics has high quantization error across the entire
    row.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，GPTQ 块大小为 128 的主要效果是问题维度导致 $8192\times 128$ 权重组的敏感性增加。反过来，使用按行统计的 GPTQ
    在整行上具有较高的量化误差。
- en: 'The effect of rotary embeddings. Earlier in Figure [2](#S3.F2 "Figure 2 ‣ 3.2
    Exploring parameter sensitivity ‣ 3 Quantization sensitivity of LLM weights ‣
    SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")
    we note that attention query and key have a regular pattern of sensitivity that
    repeats every 64 rows. We attribute this to the fact that LLaMA uses rotary position
    embeddings. More specifically, this pattern is likely a side-effect of how rotary
    embeddings are implemented for this model.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '旋转嵌入的效果。我们早些时候在图 [2](#S3.F2 "图 2 ‣ 3.2 探索参数敏感性 ‣ 3 LLM 权重的量化敏感性 ‣ SpQR: 一种接近无损的
    LLM 权重压缩稀疏量化表示") 中注意到，注意力查询和键的敏感性具有每 64 行重复的规律性。我们将这一现象归因于 LLaMA 使用旋转位置嵌入。更具体地说，这种模式可能是旋转嵌入在该模型中实现的副作用。'
- en: To recall, rotary position embeddings are a technique that rotates attention
    head dimensions by an angle that depends on how many tokens are between key and
    query [[32](#bib.bib32)]. Furthermore, dimensions within each head are rotated
    with a different frequency. To implement this rotation, LLaMA multiplies each
    head by a precomputed tensor of sine and cosine functions with a different period.
    The first half (64 units) of the matrix is multiplied by cosines and the other
    half (64 units) is multiplied by sines.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，旋转位置嵌入是一种通过取决于键和查询之间有多少标记的角度来旋转注意力头维度的技术[[32](#bib.bib32)]。此外，每个头内的维度以不同的频率进行旋转。为了实现这种旋转，LLaMA
    将每个头与一个具有不同周期的正弦和余弦函数的预计算张量相乘。矩阵的前半部分（64 个单元）与余弦相乘，而另一半（64 个单元）与正弦相乘。
- en: To recall, sine and cosine components are equivalent up to a phase shift and
    show similar behavior in our analysis. In general, we observe that weights that
    correspond to low-frequency heads (bottom of each semi-head) typically have higher
    sensitivity. One possible explanation is that high-frequency heads can be more
    dependent on position-specific information, such as attending to the previous
    token — and less dependent on the weights that represent content information.
    However, this phenomenon merits further investigation and our current understanding
    should be treated as an educated guess.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，正弦和余弦分量在相位偏移下是等效的，并且在我们的分析中表现出类似的行为。一般而言，我们观察到对应于低频头（每个半头的底部）的权重通常具有更高的敏感性。一种可能的解释是，高频头可能更依赖于特定位置的信息，例如关注前一个标记，而较少依赖于表示内容信息的权重。然而，这种现象值得进一步研究，我们当前的理解应被视为有根据的猜测。
- en: 'GPTQ and the effect of quantization order. As we observe earlier in Section [3.2](#S3.SS2
    "3.2 Exploring parameter sensitivity ‣ 3 Quantization sensitivity of LLM weights
    ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"),
    the rightmost weights in each visualization tend to have higher quantization errors.
    This is likely a side-effect of the GPTQ algorithm, which compresses weights one
    input feature at a time, i.e. column by column in a left-to-right direction. Once
    a column is quantized, the algorithm uses the remaining unquantized weights to
    compensate for the error. Thus, the rightmost batch of weights accumulates the
    most error from preceding columns and has the least space to compensate it’s “own”
    quantization error.'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPTQ 和量化顺序的影响。正如我们在第 [3.2](#S3.SS2 "3.2 探索参数敏感性 ‣ 3 LLM 权重的量化敏感性 ‣ SpQR: 一种接近无损的
    LLM 权重压缩稀疏量化表示") 节中早先观察到的，每个可视化中的最右边的权重往往具有更高的量化误差。这可能是 GPTQ 算法的副作用，它一次压缩一个输入特征，即从左到右按列进行。量化一列后，算法使用剩余的未量化权重来补偿误差。因此，最右侧的一批权重积累了来自前面列的最多误差，并且补偿其“自身”量化误差的空间最小。'
- en: 'This difference is most pronounced in the earlier layers, where the quantization
    error is smaller overall (see Figure [9](#A1.F9 "Figure 9 ‣ Relation between sensitivity
    and the chosen quantization scheme. ‣ Appendix A Additional weight sensitivity
    analysis ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression")). To further verify this observation, we observe that this effect
    disappears if we shuffle the weight quantization order in the GPTQ algorithm.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '这种差异在较早的层中最为明显，在这些层中，量化误差整体较小（见图 [9](#A1.F9 "图 9 ‣ 敏感度与所选量化方案的关系 ‣ 附录 A 额外的权重敏感度分析
    ‣ SpQR: 一种用于近无损 LLM 权重压缩的稀疏量化表示")）。为了进一步验证这一观察，我们发现如果我们在 GPTQ 算法中打乱权重量化的顺序，这种效应会消失。'
- en: '![Refer to caption](img/e3e7ace10e4601b3577f8cc2e83c0e27.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e3e7ace10e4601b3577f8cc2e83c0e27.png)'
- en: 'Figure 9: The weight log-sensitivities for a deeper upward projection layer
    (in particular, this is layer #79). The heatmap on the left represents the sensitivities
    of each weight, with darker being more sensitive; the histogram on the right captures
    the sensitivities in the first 100 and last 100 columns (sorted across input dimensions).
    The latter figure clearly shows that later columns are more sensitive on average.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 更深的向上投影层（特别是第 79 层）的权重对数敏感度。左侧的热图表示每个权重的敏感度，颜色越深表示越敏感；右侧的直方图捕捉前 100 列和最后
    100 列的敏感度（按输入维度排序）。后者图清晰地显示了后面的列平均上更敏感。'
- en: 'Relation between weight sensitivity and layer depth. In terms of mean squared
    error, we observe that the first layers of LLaMA tend to have generally lower
    OBC error (defined as L2 distance between original and quantized layer predictions).
    To illustrate this, we report the average quantization error of GPTQ-3bit in Figure [10](#A1.F10
    "Figure 10 ‣ Relation between sensitivity and the chosen quantization scheme.
    ‣ Appendix A Additional weight sensitivity analysis ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression").'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '权重敏感度与层深度的关系。就均方误差而言，我们观察到 LLaMA 的第一层通常具有较低的 OBC 错误（定义为原始层预测和量化层预测之间的 L2 距离）。为说明这一点，我们在图 [10](#A1.F10
    "图 10 ‣ 敏感度与所选量化方案的关系 ‣ 附录 A 额外的权重敏感度分析 ‣ SpQR: 一种用于近无损 LLM 权重压缩的稀疏量化表示") 中报告了
    GPTQ-3bit 的平均量化误差。'
- en: '![Refer to caption](img/a15dc6662628a5645136664874f63a2c.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a15dc6662628a5645136664874f63a2c.png)'
- en: 'Figure 10: Figure: mean quantization error (vertical axis) as a function of
    layer depth (horizontal axis). Each plot corresponds to a different layer role.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 图：量化误差均值（纵轴）与层深度（横轴）的关系。每个图对应不同的层角色。'
- en: 'The absolute quantization error means little by itself since each quantized
    layer has a different input/output variance. However, we also observe that the
    first and last few layers have qualitative differences in behavior. Figures [16](#A9.F16
    "Figure 16 ‣ Appendix I Generative examples ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression") and [17](#A9.F17 "Figure 17 ‣ Appendix
    I Generative examples ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression") report weight sensitivities for the first, middle (40th),
    and last (79th) layer of LLaMA model separately to better illustrate this difference.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '绝对量化误差本身意义不大，因为每个量化层具有不同的输入/输出方差。然而，我们还观察到第一层和最后几层在行为上存在定性差异。图 [16](#A9.F16
    "图 16 ‣ 附录 I 生成示例 ‣ SpQR: 一种用于近无损 LLM 权重压缩的稀疏量化表示") 和 [17](#A9.F17 "图 17 ‣ 附录
    I 生成示例 ‣ SpQR: 一种用于近无损 LLM 权重压缩的稀疏量化表示") 分别报告了 LLaMA 模型的第一层、中间（第 40 层）和最后一层（第
    79 层）的权重敏感度，以更好地说明这一差异。'
- en: Appendix B Experimental Configurations
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 实验配置
- en: 'The SpQR representations proposed in this work have several adjustable hyperparameters
    that allow for great flexibility in targeting a desired size of the model. We
    introduce the notation and list the method hyperparameters below:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出的 SpQR 表示具有几个可调超参数，可灵活调整以满足模型大小的需求。我们在下文中介绍了符号表示并列出了方法的超参数：
- en: •
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $b_{w}$ - number of bits per weight
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $b_{w}$ - 每个权重的比特数
- en: •
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $b_{s}$ - number of bits per scale
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $b_{s}$ - 每个尺度的比特数
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $b_{z}$ - number of bits per zero
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $b_{z}$ - 每个零的比特数
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $r_{o}$ - outlier rate (fraction of weights that are not quantized)
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $r_{o}$ - 异常值率（未量化的权重比例）
- en: •
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\beta_{1}$ - block size for weight quantization
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\beta_{1}$ - 权重量化的块大小
- en: •
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\beta_{2}$ - block size for statistic quantization;
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\beta_{2}$ - 统计量化的块大小；
- en: •
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: $\tau$ - outlier threshold
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\tau$ - 异常值阈值
- en: The actual number of outliers depends not only on $\tau$.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的异常值数量不仅仅依赖于$\tau$。
- en: 'The full configuration we use to compress LLaMA-30B model near-losslessly in
    Table [4](#S5.F4 "Figure 4 ‣ Main Results. ‣ 5 Experimental Validation ‣ SpQR:
    A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression") has
    the following hyperparameters: $b_{w}=4,b_{s}=b_{z}=3,\beta_{1}=\beta_{2}=16,\tau=0.1$
    This translates to the following command line arguments in our supplementary code:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表 [4](#S5.F4 "图 4 ‣ 主要结果。 ‣ 5 实验验证 ‣ SpQR: 一种稀疏量化表示方法，用于近乎无损的LLM权重压缩")中压缩LLaMA-30B模型的完整配置具有以下超参数：$b_{w}=4,b_{s}=b_{z}=3,\beta_{1}=\beta_{2}=16,\tau=0.1$
    这对应于我们补充代码中的以下命令行参数：'
- en: '[PRE0]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Appendix C Hyperparameter sensitivity
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 超参数敏感性
- en: In this section, we analyze how SpQR performance depends on the choice of quantization
    group sizes. Please recall that the SpQR algorithm uses two types of groups, indexed
    by parameters $\beta_{1}$ are vertical.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们分析了SpQR性能如何依赖于量化组大小的选择。请回忆一下，SpQR算法使用了两种类型的组，参数$\beta_{1}$所索引的是垂直的。
- en: 'In Table [2](#A3.T2 "Table 2 ‣ Appendix C Hyperparameter sensitivity ‣ SpQR:
    A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression"),
    we evaluate SpQR with varying parameters $\beta_{1}$. We quantize LLaMA-65B with
    3-bit SpQR for weights and statistics and report perplexity on WikiText2, Penn
    Treebank, and C4 datasets. The upper-left section of the table contains the effective
    number of bits for each group configuration, and the remaining sections correspond
    to perplexities on different datasets.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '在表格 [2](#A3.T2 "表 2 ‣ 附录 C 超参数敏感性 ‣ SpQR: 一种稀疏量化表示方法，用于近乎无损的LLM权重压缩")中，我们评估了不同参数$\beta_{1}$下的SpQR。我们用3-bit
    SpQR对LLaMA-65B进行权重和统计量的量化，并报告了在WikiText2、Penn Treebank和C4数据集上的困惑度。表格的左上部分包含了每组配置的有效位数，其余部分对应于不同数据集上的困惑度。'
- en: '|  | Average bits | Wikitext2 Perplexity (3.53) |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '|  | 平均位数 | Wikitext2 困惑度 (3.53) |'
- en: '| $\beta_{1}$
    | 4 | 8 | 16 | 32 | 64 | 128 | 4 | 8 | 16 | 32 | 64 | 128 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| $\beta_{1}$
    | 4 | 8 | 16 | 32 | 64 | 128 | 4 | 8 | 16 | 32 | 64 | 128 |'
- en: '| 4 | 8.5 | 6.5 | 5.5 | 5 | 4.75 | 4.625 | 3.581 | 3.628 | 3.715 | 3.822 |
    4.003 | 4.23 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 8.5 | 6.5 | 5.5 | 5 | 4.75 | 4.625 | 3.581 | 3.628 | 3.715 | 3.822 |
    4.003 | 4.23 |'
- en: '| 8 | 5.75 | 4.75 | 4.25 | 4 | 3.875 | 3.813 | 3.625 | 3.64 | 3.649 | 3.666
    | 3.688 | 3.713 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 5.75 | 4.75 | 4.25 | 4 | 3.875 | 3.813 | 3.625 | 3.64 | 3.649 | 3.666
    | 3.688 | 3.713 |'
- en: '| 16 | 4.375 | 3.875 | 3.625 | 3.5 | 3.438 | 3.406 | 3.701 | 3.71 | 3.728 |
    3.726 | 3.739 | 3.741 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4.375 | 3.875 | 3.625 | 3.5 | 3.438 | 3.406 | 3.701 | 3.71 | 3.728 |
    3.726 | 3.739 | 3.741 |'
- en: '| 32 | 3.688 | 3.438 | 3.313 | 3.25 | 3.219 | 3.203 | 3.803 | 3.797 | 3.812
    | 3.812 | 3.815 | 3.85 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 3.688 | 3.438 | 3.313 | 3.25 | 3.219 | 3.203 | 3.803 | 3.797 | 3.812
    | 3.812 | 3.815 | 3.85 |'
- en: '| 64 | 3.344 | 3.219 | 3.156 | 3.125 | 3.109 | 3.102 | 3.884 | 3.901 | 3.907
    | 3.899 | 3.928 | 3.926 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 3.344 | 3.219 | 3.156 | 3.125 | 3.109 | 3.102 | 3.884 | 3.901 | 3.907
    | 3.899 | 3.928 | 3.926 |'
- en: '| 128 | 3.172 | 3.109 | 3.078 | 3.063 | 3.055 | 3.051 | 3.982 | 3.994 | 4.005
    | 3.992 | 4.017 | 4.013 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 3.172 | 3.109 | 3.078 | 3.063 | 3.055 | 3.051 | 3.982 | 3.994 | 4.005
    | 3.992 | 4.017 | 4.013 |'
- en: '|  | C4 Perplexity (5.62) | PTB Perplexity (6.91) |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '|  | C4 困惑度 (5.62) | PTB 困惑度 (6.91) |'
- en: '| $\beta_{1}$
    | 4 | 8 | 16 | 32 | 64 | 128 | 4 | 8 | 16 | 32 | 64 | 128 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| $\beta_{1}$
    | 4 | 8 | 16 | 32 | 64 | 128 | 4 | 8 | 16 | 32 | 64 | 128 |'
- en: '| 4 | 5.652 | 5.674 | 5.718 | 5.796 | 5.919 | 6.119 | 6.934 | 6.965 | 7.001
    | 7.054 | 7.194 | 7.395 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 5.652 | 5.674 | 5.718 | 5.796 | 5.919 | 6.119 | 6.934 | 6.965 | 7.001
    | 7.054 | 7.194 | 7.395 |'
- en: '| 8 | 5.683 | 5.688 | 5.696 | 5.703 | 5.709 | 5.718 | 6.962 | 6.98 | 6.991
    | 6.99 | 6.979 | 7.029 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 5.683 | 5.688 | 5.696 | 5.703 | 5.709 | 5.718 | 6.962 | 6.98 | 6.991
    | 6.99 | 6.979 | 7.029 |'
- en: '| 16 | 5.735 | 5.735 | 5.735 | 5.738 | 5.741 | 5.749 | 7.018 | 7.013 | 7.015
    | 7.016 | 7.012 | 7.03 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 5.735 | 5.735 | 5.735 | 5.738 | 5.741 | 5.749 | 7.018 | 7.013 | 7.015
    | 7.016 | 7.012 | 7.03 |'
- en: '| 32 | 5.793 | 5.789 | 5.792 | 5.796 | 5.794 | 5.802 | 7.042 | 7.053 | 7.083
    | 7.043 | 7.069 | 7.083 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 5.793 | 5.789 | 5.792 | 5.796 | 5.794 | 5.802 | 7.042 | 7.053 | 7.083
    | 7.043 | 7.069 | 7.083 |'
- en: '| 64 | 5.857 | 5.859 | 5.858 | 5.866 | 5.863 | 5.866 | 7.084 | 7.129 | 7.137
    | 7.118 | 7.137 | 7.12 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 64 | 5.857 | 5.859 | 5.858 | 5.866 | 5.863 | 5.866 | 7.084 | 7.129 | 7.137
    | 7.118 | 7.137 | 7.12 |'
- en: '| 128 | 5.932 | 5.931 | 5.935 | 5.939 | 5.944 | 5.936 | 7.185 | 7.197 | 7.232
    | 7.234 | 7.217 | 7.199 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 128 | 5.932 | 5.931 | 5.935 | 5.939 | 5.944 | 5.936 | 7.185 | 7.197 | 7.232
    | 7.234 | 7.217 | 7.199 |'
- en: 'Table 2: Weight block size $\beta_{1}$ performance on WikiText2, C4, and Penn
    Treebank (PTB). The uncompressed baseline value is provided in the corresponding
    heading.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：WikiText2、C4和Penn Treebank（PTB）上的权重块大小 $\beta_{1}$ 性能。未压缩的基线值在相应标题中提供。
- en: Appendix D Estimating model size
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录D 估算模型大小
- en: 'In this section, we provide a quick way to estimate the compressed model size
    before running the quantization. We express this estimate in terms of *average
    bits per parameter* defined as:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一种快速估算压缩模型大小的方法。我们将此估算表示为定义的*平均每参数比特数*：
- en: '|  | $\overline{b}=\frac{\mathrm{model\ size\ in\ bits}}{\mathrm{number\ of\
    parameters}}$ |  | (3) |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | $\overline{b}=\frac{\mathrm{model\ size\ in\ bits}}{\mathrm{number\ of\
    parameters}}$ |  | (3) |'
- en: Where $\mathrm{model\ size\ in\ bits}$ bits.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathrm{model\ size\ in\ bits}$ 比特。
- en: 'The storage and computational cost in transformer models are dominated by the
    linear projections in the attention and feedforward blocks. Consider quantization
    of a weight matrix (any of these) $\mathbb{R}^{d_{\mathrm{out}}\times d_{\mathrm{in}}}$.
    Then the average number of bits for a given configuration is:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在变换器模型中，存储和计算成本主要由注意力机制和前馈块中的线性投影决定。考虑对权重矩阵（这些中的任何一个）$\mathbb{R}^{d_{\mathrm{out}}\times
    d_{\mathrm{in}}}$进行量化。那么，对于给定配置的平均比特数是：
- en: '|  | $1$2 |  | (4) |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (4) |'
- en: Therefore, to increase (decrease) the size of the model one should either increase
    (decrease) the precision of model weights and quantization statistics or decrease
    (increase) the block size.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，要增加（减少）模型的大小，可以选择增加（减少）模型权重和量化统计的精度，或减少（增加）块的大小。
- en: 'For example, for configuration with $b_{w}=3,b_{s}=3,b_{z}=3,\beta_{1}=16,\beta_{2}=32$
    of outliers, the average number of bits is:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于配置 $b_{w}=3,b_{s}=3,b_{z}=3,\beta_{1}=16,\beta_{2}=32$ 的离群点，平均比特数为：
- en: '|  | $3+\frac{3+3}{16}+\frac{64}{16\cdot 32}+0.004\cdot 32\simeq 3.63$ |  |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '|  | $3+\frac{3+3}{16}+\frac{64}{16\cdot 32}+0.004\cdot 32\simeq 3.63$ |  |'
- en: Appendix E Choice of optimal configuration for fixed average number of bits
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录E 固定平均比特数的最佳配置选择
- en: 'As discussed above our method has multiple options for improvement of model
    performance at the cost of the increase of the model size: number of bits per
    weight $w_{b}$ for 1st and 2nd order quantization and the outlier rate. We evaluated
    several configurations with various options for the aforementioned parameters
    on perplexity benchmarks. Results are presented on Figure [11](#A5.F11 "Figure
    11 ‣ Appendix E Choice of optimal configuration for fixed average number of bits
    ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression").
    One can observe that small groups and small fraction of outliers allows to considerably
    improve model performance, but the gain is diminishing with the number of bits
    added (when the additional budget from small group is of order 0.1-0.5 of bits
    per parameter). It is better to store weights in higher precision instead of keeping
    them in lower precision but with very small groups or keeping large fraction of
    outliers. In our experiments optimal fraction of outliers is 0.2-0.5% depending
    on the model and groupsize.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '如上所述，我们的方法有多种选择可以提高模型性能，但会增加模型大小：1阶和2阶量化的每个权重比特数 $w_{b}$ 和离群率。我们在困惑度基准测试中评估了几种配置及其各种选项。结果显示在图
    [11](#A5.F11 "图 11 ‣ 附录 E 固定平均比特数的最佳配置选择 ‣ SpQR: 一种近无损的LLM权重压缩稀疏量化表示")。可以观察到，小的组和少量的离群点可以显著提高模型性能，但随着添加比特数的增加（当小组的额外预算为每个参数0.1-0.5比特的数量级）收益会逐渐减少。与其将权重保持在较低的精度但小组非常小或离群点较多，不如将权重存储在更高的精度中。在我们的实验中，最佳的离群点比例是0.2-0.5%，这取决于模型和组大小。'
- en: '![Refer to caption](img/9ec72bda0c7d814670867325d4b88321.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ec72bda0c7d814670867325d4b88321.png)'
- en: 'Figure 11: Perplexity of WikiText2 vs average number of bits. Different markers
    denote different $b_{w}$. Black colors correspond to quantization configurations
    without outliers and the brightness of the color is proportional to the outlier
    rate.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: WikiText2 的困惑度与平均位数。不同的标记表示不同的 $b_{w}$。黑色对应没有异常值的量化配置，颜色的亮度与异常值率成正比。'
- en: Appendix F Additional results for near-lossless compression
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 F 近无损压缩的额外结果
- en: 'In this section we report the list of quantization configurations for OPT in
    Table [12](#A6.F12 "Figure 12 ‣ Appendix F Additional results for near-lossless
    compression ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression") on WikiText2, Penn Treebank, and C4 datasets.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们报告了 OPT 在表格[12](#A6.F12 "图 12 ‣ 附录 F 近无损压缩的额外结果 ‣ SpQR: 近无损 LLM 权重压缩的稀疏量化表示")中的量化配置列表，数据集为
    WikiText2、Penn Treebank 和 C4。'
- en: 'In addition we report results for LM eval harness for LLaMa Table [13](#A6.F13
    "Figure 13 ‣ Appendix F Additional results for near-lossless compression ‣ SpQR:
    A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression").
    and recently released [Falcon](https://falconllm.tii.ae) models - Falcon-7B and
    Falcon-40B Table [14](#A6.F14 "Figure 14 ‣ Appendix F Additional results for near-lossless
    compression ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
    Compression").'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们报告了 LLaMa 的 LM eval harness 结果，见表 [13](#A6.F13 "图 13 ‣ 附录 F 近无损压缩的额外结果
    ‣ SpQR: 近无损 LLM 权重压缩的稀疏量化表示")。以及最近发布的 [Falcon](https://falconllm.tii.ae) 模型 -
    Falcon-7B 和 Falcon-40B 表 [14](#A6.F14 "图 14 ‣ 附录 F 近无损压缩的额外结果 ‣ SpQR: 近无损 LLM
    权重压缩的稀疏量化表示")。'
- en: '{floatrow}\capbtabbox'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '{floatrow}\capbtabbox'
- en: '| OPT |  |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| OPT |  |'
- en: '| Size | Method | Avg bits | Wiki2 | C4 | PTB |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Wiki2 | C4 | PTB |'
- en: '| 6.7B | – | 16.00 | 10.86 | 11.74 | 13.09 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 6.7B | – | 16.00 | 10.86 | 11.74 | 13.09 |'
- en: '| SpQR | 4.27 | 10.81 | 11.88 | 13.17 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.27 | 10.81 | 11.88 | 13.17 |'
- en: '| RTN | 4 | 12.10 | 13.38 | 16.09 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 12.10 | 13.38 | 16.09 |'
- en: '| GPTQ | 4 | 11.39 | 12.15 | 13.80 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 11.39 | 12.15 | 13.80 |'
- en: '| SpQR | 3.94 | 11.04 | 11.98 | 13.33 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.94 | 11.04 | 11.98 | 13.33 |'
- en: '| 13B | – | 16.00 | 10.12 | 11.20 | 12.34 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 13B | – | 16.00 | 10.12 | 11.20 | 12.34 |'
- en: '| SpQR | 4.27 | 10.22 | 11.27 | 12.41 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.27 | 10.22 | 11.27 | 12.41 |'
- en: '| RTN | 4 | 11.32 | 12.35 | 15.4 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 11.32 | 12.35 | 15.4 |'
- en: '| GPTQ | 4 | 10.31 | 11.36 | 12.58 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 10.31 | 11.36 | 12.58 |'
- en: '| SpQR | 3.93 | 10.28 | 11.34 | 12.52 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.93 | 10.28 | 11.34 | 12.52 |'
- en: '| Size | Method | Avg bits | Wiki2 | C4 | PTB |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Wiki2 | C4 | PTB |'
- en: '| 30B | – | 16.00 | 9.56 | 10.69 | 11.84 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 30B | – | 16.00 | 9.56 | 10.69 | 11.84 |'
- en: '| SpQR | 4.26 | 9.50 | 10.73 | 11.88 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.26 | 9.50 | 10.73 | 11.88 |'
- en: '| RTN | 4 | 10.97 | 11.90 | 14.17 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 10.97 | 11.90 | 14.17 |'
- en: '| GPTQ | 4 | 9.63 | 10.80 | 11.98 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 9.63 | 10.80 | 11.98 |'
- en: '| SpQR | 3.94 | 9.54 | 10.78 | 11.93 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.94 | 9.54 | 10.78 | 11.93 |'
- en: '| 66B | – | 16.00 | 9.33 | 10.28 | 11.36 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 66B | – | 16.00 | 9.33 | 10.28 | 11.36 |'
- en: '| SpQR | 4.23 | 9.37 | 10.32 | 11.40 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.23 | 9.37 | 10.32 | 11.40 |'
- en: '| RTN | 4 | 110 | 249 | 274 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 110 | 249 | 274 |'
- en: '| GPTQ | 4 | 9.55 | 10.50 | 11.58 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 9.55 | 10.50 | 11.58 |'
- en: '| SpQR | 3.91 | 9.32 | 10.35 | 11.43 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.91 | 9.32 | 10.35 | 11.43 |'
- en: 'Figure 12: Perplexity on WikiText2 [[21](#bib.bib21)], C4 [[29](#bib.bib29)]
    and Penn Treebank [[20](#bib.bib20)] for SpQR and round-to-nearest (RTN) and GPTQ
    baselines with OPT. We can see that SpQR reaches performances within 1% of the
    perplexity with less than 4.3 bits per parameter. We also see that for 4-bits
    per parameter SpQR significantly improves on GPTQ with an improvement as large
    as the improvement from RTN to GPTQ.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: WikiText2 上的困惑度 [[21](#bib.bib21)]，C4 [[29](#bib.bib29)] 和 Penn Treebank [[20](#bib.bib20)]
    的 SpQR 和近似（RTN）及 GPTQ 基准与 OPT。我们可以看到，SpQR 在每个参数少于 4.3 位的情况下，其性能达到困惑度的 1% 以内。我们还看到，对于每个参数
    4 位的 SpQR 显著优于 GPTQ，改进幅度与从 RTN 到 GPTQ 的改进幅度相当。'
- en: '{floatrow}\capbtabbox'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '{floatrow}\capbtabbox'
- en: '| LLaMA |  |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA |  |'
- en: '| Size | Method | Avg bits | Winogrande | Piqa | Hellaswag | Arc easy | Arc
    challenge | Avg score |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Winogrande | Piqa | Hellaswag | Arc easy | Arc challenge
    | 平均分数 |'
- en: '| 7B | – | 16.00 | 67.09 | 78.32 | 56.41 | 67.38 | 38.23 | 61.492 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | 16.00 | 67.09 | 78.32 | 56.41 | 67.38 | 38.23 | 61.492 |'
- en: '| SpQR | 4.63 | 67.48 | 78.45 | 56.01 | 67.13 | 38.23 | 61.460 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.63 | 67.48 | 78.45 | 56.01 | 67.13 | 38.23 | 61.460 |'
- en: '| RTN | 4 | 64.72 | 76.44 | 53.49 | 63.51 | 36.60 | 58.952 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 64.72 | 76.44 | 53.49 | 63.51 | 36.60 | 58.952 |'
- en: '| GPTQ | 4 | 65.35 | 77.58 | 54.99 | 63.55 | 36.35 | 59.564 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 65.35 | 77.58 | 54.99 | 63.55 | 36.35 | 59.564 |'
- en: '| SpQR | 3.45 | 67.48 | 78.13 | 55.27 | 65.87 | 38.05 | 60.960 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.45 | 67.48 | 78.13 | 55.27 | 65.87 | 38.05 | 60.960 |'
- en: '| 13B | – | 16.00 | 70.09 | 78.89 | 59.11 | 74.54 | 43.94 | 65.314 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 13B | – | 16.00 | 70.09 | 78.89 | 59.11 | 74.54 | 43.94 | 65.314 |'
- en: '| SpQR | 4.63 | 69.77 | 78.94 | 59.02 | 74.37 | 43.17 | 65.054 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.63 | 69.77 | 78.94 | 59.02 | 74.37 | 43.17 | 65.054 |'
- en: '| RTN | 4 | 69.61 | 78.24 | 57.34 | 72.56 | 42.58 | 64.066 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 69.61 | 78.24 | 57.34 | 72.56 | 42.58 | 64.066 |'
- en: '| GPTQ | 4 | 69.06 | 78.40 | 58.04 | 73.23 | 43.26 | 64.398 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 69.06 | 78.40 | 58.04 | 73.23 | 43.26 | 64.398 |'
- en: '| SpQR | 3.45 | 68.90 | 78.73 | 58.22 | 73.27 | 42.75 | 64.374 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.45 | 68.90 | 78.73 | 58.22 | 73.27 | 42.75 | 64.374 |'
- en: '| 30B | – | 16.00 | 72.93 | 80.96 | 62.66 | 75.34 | 46.76 | 67.730 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 30B | – | 16.00 | 72.93 | 80.96 | 62.66 | 75.34 | 46.76 | 67.730 |'
- en: '| SpQR | 4.69 | 72.93 | 81.01 | 62.50 | 76.05 | 47.18 | 67.934 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.69 | 72.93 | 81.01 | 62.50 | 76.05 | 47.18 | 67.934 |'
- en: '| RTN | 4 | 72.06 | 79.05 | 60.61 | 70.66 | 42.24 | 64.924 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 72.06 | 79.05 | 60.61 | 70.66 | 42.24 | 64.924 |'
- en: '| GPTQ | 4 | 72.61 | 79.92 | 61.07 | 71.8 | 44.28 | 65.936 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 72.61 | 79.92 | 61.07 | 71.8 | 44.28 | 65.936 |'
- en: '| SpQR | 3.49 | 73.32 | 80.47 | 61.96 | 74.75 | 46.93 | 67.486 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.49 | 73.32 | 80.47 | 61.96 | 74.75 | 46.93 | 67.486 |'
- en: '| 65B | – | 16.00 | 77.43 | 81.50 | 63.95 | 75.17 | 47.10 | 69.030 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| 65B | – | 16.00 | 77.43 | 81.50 | 63.95 | 75.17 | 47.10 | 69.030 |'
- en: '| SpQR | 4.71 | 76.95 | 81.56 | 63.76 | 75.25 | 46.93 | 68.890 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.71 | 76.95 | 81.56 | 63.76 | 75.25 | 46.93 | 68.890 |'
- en: '| RTN | 4 | 75.14 | 81.45 | 62.79 | 72.64 | 44.97 | 67.398 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4 | 75.14 | 81.45 | 62.79 | 72.64 | 44.97 | 67.398 |'
- en: '| GPTQ | 4 | 75.85 | 80.79 | 62.91 | 74.20 | 46.59 | 68.068 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4 | 75.85 | 80.79 | 62.91 | 74.20 | 46.59 | 68.068 |'
- en: '| SpQR | 3.52 | 76.09 | 81.18 | 63.54 | 74.37 | 45.05 | 68.046 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.52 | 76.09 | 81.18 | 63.54 | 74.37 | 45.05 | 68.046 |'
- en: 'Figure 13: LM eval harness results on LLaMA models.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '图 13: LLaMA 模型的 LM 评估工具结果。'
- en: '{floatrow}\capbtabbox'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '{floatrow}\capbtabbox'
- en: '| Falcon |  |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Falcon |  |'
- en: '| Size | Method | Avg bits | Winogrande | Piqa | Hellaswag | Arc easy | Arc
    challenge | Avg score |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| 大小 | 方法 | 平均位数 | Winogrande | Piqa | Hellaswag | Arc easy | Arc challenge
    | 平均分数 |'
- en: '| 7B | – | 16.00 | 67.32 | 79.49 | 57.77 | 74.71 | 40.1 0 | 63.878 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| 7B | – | 16.00 | 67.32 | 79.49 | 57.77 | 74.71 | 40.10 | 63.878 |'
- en: '| SpQR | 4.44 | 67.09 | 79.16 | 57.21 | 73.86 | 38.99 | 63.262 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.44 | 67.09 | 79.16 | 57.21 | 73.86 | 38.99 | 63.262 |'
- en: '| RTN | 4.00 | 65.51 | 77.37 | 51.86 | 68.69 | 33.7 | 59.426 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4.00 | 65.51 | 77.37 | 51.86 | 68.69 | 33.7 | 59.426 |'
- en: '| GPTQ | 4.00 | 66.38 | 79.11 | 56.68 | 73.15 | 38.48 | 62.760 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4.00 | 66.38 | 79.11 | 56.68 | 73.15 | 38.48 | 62.760 |'
- en: '| SpQR | 3.49 | 67.88 | 79.54 | 57.08 | 74.03 | 39.08 | 63.522 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.49 | 67.88 | 79.54 | 57.08 | 74.03 | 39.08 | 63.522 |'
- en: '| 40B | – | 16.00 | 76.62 | 82.32 | 64.06 | 82.03 | 50.26 | 71.058 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| 40B | – | 16.00 | 76.62 | 82.32 | 64.06 | 82.03 | 50.26 | 71.058 |'
- en: '| SpQR | 4.46 | 76.48 | 82.1 | 63.8 | 81.78 | 50.77 | 70.986 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 4.46 | 76.48 | 82.1 | 63.8 | 81.78 | 50.77 | 70.986 |'
- en: '| RTN | 4.00 | 75.69 | 80.30 | 60.52 | 79.92 | 49.83 | 69.252 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| RTN | 4.00 | 75.69 | 80.30 | 60.52 | 79.92 | 49.83 | 69.252 |'
- en: '| GPTQ | 4.00 | 75.93 | 81.23 | 63.05 | 80.85 | 50.00 | 70.212 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 4.00 | 75.93 | 81.23 | 63.05 | 80.85 | 50.00 | 70.212 |'
- en: '| SpQR | 3.45 | 76.32 | 81.77 | 63.70 | 81.10 | 49.83 | 70.544 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| SpQR | 3.45 | 76.32 | 81.77 | 63.70 | 81.10 | 49.83 | 70.544 |'
- en: 'Figure 14: LM eval harness results on Falcon models.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '图 14: Falcon 模型的 LM 评估工具结果。'
- en: Appendix G Choice of optimal LLM configuration for specific hardware
  id: totrans-400
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 G 针对特定硬件的最佳 LLM 配置选择
- en: 'In the preceding discussion, we were searching for optimal model configuration
    given some compression target without targeting any specific hardware or device.
    However, the question practitioner willing to deploy a model for a specific application
    would ask is: What is the best model and compression setup for a given memory
    constraint?'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的讨论中，我们在寻求在给定的压缩目标下的最佳模型配置，而未针对任何特定硬件或设备。然而，实践者在为特定应用部署模型时会问的问题是：对于给定的内存限制，最佳的模型和压缩设置是什么？
- en: 'In this section, we provide a list of recommendations for the choice of the
    best LLaMA model and the corresponding compression level that fits into the device
    memory (RAM or VRAM) without the need of offloading model parameters and activations.
    We cover a range of available budgets from mobile devices to high-end workstation
    GPUs. Recommendations are presented in Table [3](#A7.T3 "Table 3 ‣ Appendix G
    Choice of optimal LLM configuration for specific hardware ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression").'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们提供了选择最佳 LLaMA 模型及其相应压缩级别的建议，以适应设备内存（RAM 或 VRAM），而无需卸载模型参数和激活。我们涵盖了从移动设备到高端工作站
    GPU 的各种预算。建议见表 [3](#A7.T3 "Table 3 ‣ Appendix G Choice of optimal LLM configuration
    for specific hardware ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression")。'
- en: '| Device | Memory (GiB) | LLaMA | $\overline{b}$ |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 设备 | 内存 (GiB) | LLaMA | $\overline{b}$ |'
- en: '| iPhone13 | 4 | 7B | $\leq 3.5$ |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| iPhone13 | 4 | 7B | $\leq 3.5$ |'
- en: '| iPhone14 | 6 | 7B | $\simeq 4.5$ |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| iPhone14 | 6 | 7B | $\simeq 4.5$ |'
- en: '| 13B | $\leq 3.5$ |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 13B | $\leq 3.5$ |'
- en: '| Consumer laptop | 8 | 13B | $\leq 4$ |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 消费者笔记本 | 8 | 13B | $\leq 4$ |'
- en: '| RTX4070 | 10-12 | 14B | $\simeq 4.5$ |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| RTX4070 | 10-12 | 14B | $\simeq 4.5$ |'
- en: '| RTX4080 | 16 | 30B | $\leq 4$ |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| RTX4080 | 16 | 30B | $\leq 4$ |'
- en: '| RTX4090 | 24 | 30B | $\simeq 4.5$ |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| RTX4090 | 24 | 30B | $\simeq 4.5$ |'
- en: '| V100 | 32 | 65B | $\leq 3.5$ |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| V100 | 32 | 65B | $\leq 3.5$ |'
- en: '| A6000 | 48 | 65B | $\simeq 4.5$ |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| A6000 | 48 | 65B | $\simeq 4.5$ |'
- en: 'Table 3: Choice of the best LLaMA for a given memory constraint.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：给定内存约束下的最佳 LLaMA 选择。
- en: Appendix H Sensitivity to random seed
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 H 随机种子敏感性
- en: 'The experiments we report throughout Section [5](#S5 "5 Experimental Validation
    ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")
    use one fixed random seed (the default value from the supplementary code). To
    verify that our results are robust to randomness, we run SpQR with 5 random seeds
    (0-5) and measure the adjusted standard deviation.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在第 [5](#S5 "5 Experimental Validation ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression") 节中报告的实验使用了一个固定的随机种子（来自补充代码的默认值）。为了验证我们的结果对随机性具有鲁棒性，我们使用
    5 个随机种子（0-5）运行 SpQR 并测量调整后的标准差。'
- en: For this evaluation, we compress LLaMA-65B with SpQR using $b_{w}=b_{z}=b_{s}=3$
    (C4). In addition to the chosen random seed, these standard deviations can be
    affected by the inherent nondeterminism of GPU computation. Overall, the standard
    deviations are at least one order of magnitude smaller than the difference between
    SpQR, GPTQ, and RTN.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此次评估，我们使用 $b_{w}=b_{z}=b_{s}=3$（C4）来压缩 LLaMA-65B 和 SpQR。除了选择的随机种子，这些标准差可能会受到
    GPU 计算固有的非确定性的影响。总体而言，标准差至少比 SpQR、GPTQ 和 RTN 之间的差异小一个数量级。
- en: Appendix I Generative examples
  id: totrans-417
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 I 生成示例
- en: 'Finally, we showcase several examples of how SpQR quantization affects the
    generated samples. For this evaluation, we take several prompts and use the compressed
    language model to continue generating text from these prompts. We compare the
    original LLaMA-65B and two quantized versions: SpQR and RTN-4bit. More specifically,
    we use the SpQR configuration that corresponds to near-lossless compression from
    Table [4](#S5.F4 "Figure 4 ‣ Main Results. ‣ 5 Experimental Validation ‣ SpQR:
    A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression").
    We use greedy autoregressive inference for all generated samples to ensure reproducibility.
    The examples in Figure [15](#A9.F15 "Figure 15 ‣ Appendix I Generative examples
    ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")
    show that all models produce a valid text, but SpQR matches the 16-bit model more
    frequently. The near-lossless algorithm also seems to produce more semantically
    similar texts.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们展示了 SpQR 量化如何影响生成的样本。对于此次评估，我们取了几个提示，并使用压缩后的语言模型继续从这些提示生成文本。我们比较了原始 LLaMA-65B
    和两个量化版本：SpQR 和 RTN-4bit。更具体地，我们使用了与表 [4](#S5.F4 "Figure 4 ‣ Main Results. ‣ 5
    Experimental Validation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression") 中接近无损压缩相对应的 SpQR 配置。我们对所有生成的样本使用贪婪自回归推理以确保可重复性。图 [15](#A9.F15
    "Figure 15 ‣ Appendix I Generative examples ‣ SpQR: A Sparse-Quantized Representation
    for Near-Lossless LLM Weight Compression") 中的示例显示所有模型都生成了有效的文本，但 SpQR 更频繁地匹配 16
    位模型。近无损算法似乎也生成了更多语义相似的文本。'
- en: '| Prompt: | Every time data scientist Kim ran her new algorithm, it kept outputting
    the same unexpected result: a recipe for |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| Prompt: | 每次数据科学家 Kim 运行她的新算法时，它总是输出相同的意外结果：一个食谱 |'
- en: '| 16bit: | iced tea. Kim was working on a project to predict the likelihood
    of a patient being readmitted to the hospital within 30 days of discharge. She
    was using a machine learning algorithm to analyze the data, and the algorithm
    |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| 16bit: | 冰茶。Kim 正在进行一个项目，预测患者在出院后30天内再次入院的可能性。她使用了一个机器学习算法来分析数据，而这个算法 |'
- en: '| SpQR: | iced tea. Kim was working on a project to predict the likelihood
    of a patient being readmitted to the hospital within 30 days of discharge. She
    had been given a dataset of 100,000 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| SpQR: | 冰茶。Kim 正在进行一个项目，预测患者在出院后30天内再次入院的可能性。她得到了一份包含100,000条数据的数据集 |'
- en: '| RTN 4-bit: | iced tea. “I’m not sure what’s going on,” she said. “I’ve been
    running it for days, and it keeps giving me the same result.” “I’m not sure what’s
    going on |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| RTN 4-bit: | 冰茶。“我不确定发生了什么，”她说。“我已经运行了好几天，它一直给我相同的结果。” “我不确定发生了什么 |'
- en: '| Prompt: | Data scientist Grace was working late one night when she discovered
    a glitch in the matrix of her data. It spelled out |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| Prompt: | 数据科学家 Grace 在一个夜晚工作到很晚时发现了她数据矩阵中的一个故障。它拼出了 |'
- en: '| 16bit: | 100% proof that the world was going to end in 10 days. She tried
    to tell her boss, but he didn’t believe her. She tried to tell her friends, but
    they didn’t believe her |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| 16bit: | 100% 的证据表明世界将在10天内结束。她试图告诉她的老板，但他不相信她。她试图告诉她的朋友们，但他们也不相信她 |'
- en: '| SpQR: | 100% proof that the world was going to end in 10 days. She tried
    to tell her boss, but he just laughed and said, “I’m not going to believe in some
    data glitch.” She |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| SpQR: | 100% 的证据表明世界将在10天内结束。她试图告诉她的老板，但他只是笑了笑说：“我不会相信这些数据故障。”她 |'
- en: '| RTN 4-bit: | 100% in binary code. She was so excited she ran to tell her
    boss, but he was busy. She ran to tell her colleague, but he was busy. She ran
    to tell her friend, but she |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| RTN 4-bit: | 100% 的二进制代码。她非常兴奋，跑去告诉她的老板，但他很忙。她跑去告诉她的同事，但他也很忙。她跑去告诉她的朋友，但她
    |'
- en: '| Prompt: | Last Tuesday, just as the sun was setting, my neighbor’s dog, Rover,
    started to speak to me in perfect French. He said |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 提示: | 上周二，正当太阳下山时，我邻居的狗Rover开始用完美的法语与我交谈。他说 |'
- en: '| 16bit: | "Je suis un chien." I was shocked. I said, "Moi aussi." I’m not
    sure what to make of this. I’m not sure what to make of anything. I’m not sure
    what |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| 16bit: | “Je suis un chien。”我很震惊。我说，“Moi aussi。”我不确定怎么理解这个。我不确定怎么理解任何事情。我不确定
    |'
- en: '| SpQR: | "Je suis un chien." I was shocked. I said, "Rover, you can speak
    French?" He said, "Oui, je parle francais." I said, "How did you learn to speak
    French?" |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| SpQR: | “Je suis un chien。”我很震惊。我说，“Rover，你会说法语？”他说，“Oui, je parle francais。”我问，“你是怎么学会说法语的？”
    |'
- en: '| RTN 4-bit: | "Bonjour, Monsieur. Je m’appelle Rover. Je suis un chien. Je
    suis un chien français. Je suis un chien français qui parle français. Je suis
    un chien français qui parle français |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| RTN 4-bit: | “Bonjour, Monsieur. Je m’appelle Rover. Je suis un chien. Je
    suis un chien français. Je suis un chien français qui parle français. Je suis
    un chien français qui parle français |'
- en: '| Prompt: | In a parallel universe where cats are the rulers of Earth, our
    feline king was pondering his first decree |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| 提示: | 在一个猫是地球统治者的平行宇宙中，我们的猫王正在思考他的第一个法令 |'
- en: '| 16bit: | " to make all dogs illegal. "I"m not sure that"s a good idea," said
    his chief adviser, a cat named Whiskers. "Why not?" asked the king. "Because,
    |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 16bit: | “使所有的狗都非法。” “我”不确定这是个好主意，”他的首席顾问，一只名叫Whiskers的猫说。“为什么不呢？”国王问。“因为，
    |'
- en: '| SpQR: | " to make all dogs illegal. "I"m not sure that"s a good idea," said
    his chief adviser, a cat named Whiskers. "Why not?" asked the king. "Because,
    |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| SpQR: | “使所有的狗都非法。” “我”不确定这是个好主意，”他的首席顾问，一只名叫Whiskers的猫说。“为什么不呢？”国王问。“因为，
    |'
- en: '| RTN 4-bit: | " to make the world a better place for cats. He was about to
    sign the decree when he was interrupted by a knock on the door. "Come in," he
    said. The door opened and a cat entered. |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| RTN 4-bit: | “使世界对猫更美好。他正要签署法令时，门外传来了敲门声。“进来，”他说。门打开，一只猫走了进来。 |'
- en: 'Figure 15: Texts generated by different quantized LLaMA-65B models with the
    same prompt.'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 15: 以相同提示生成的不同量化LLaMA-65B模型的文本。'
- en: '![Refer to caption](img/09e9e79022e717d6af025aa5f09d6d3c.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/09e9e79022e717d6af025aa5f09d6d3c.png)'
- en: 'Figure 16: A grid of weight log-sensitivities for LLaMA-65B for 3-bit GPTQ
    compression with per-row quantization statistics. Each row corresponds to a specific
    layer type (e.g. attention query, mlp gate), and the columns represent layer depth.'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 16: LLaMA-65B在3-bit GPTQ压缩中，按行量化统计的权重对数敏感性的网格。每一行对应一个特定的层类型（例如注意力查询，mlp门），列代表层的深度。'
- en: '![Refer to caption](img/3bcf59dda451aec6892b15019b1cbeff.png)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3bcf59dda451aec6892b15019b1cbeff.png)'
- en: 'Figure 17: A grid of weight log-sensitivities for LLaMA-65B for 3-bit GPTQ
    compression with group-wise quantization of block size 128\. Each row corresponds
    to a specific layer type (e.g. attention query, mlp gate), and the columns represent
    layer depth.'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 'Figure 17: LLaMA-65B在3-bit GPTQ压缩中，以128块大小的组内量化的权重对数敏感性的网格。每一行对应一个特定的层类型（例如注意力查询，mlp门），列代表层的深度。'
- en: Appendix J Broader impact
  id: totrans-440
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录J 更广泛的影响
- en: Our method enables the deployment of high-quality LLMs in the 7-13B parameters
    range to memory-limited devices such as laptops and phones. With our method, it
    is possible to develop specialized 7B LLMs in hassle-free 16-bit and then enable
    the deployment of such LLMs to phones by applying SpQR. Since SpQR is practically
    lossless, this ensures a reliable performance level for deployed LLMs which is
    important for consumer applications. Since mobile phones are ubiquitous and LLMs
    powerful general-purpose tools, SpQR might have a wide-reaching effect on how
    LLMs are used by the general population to complete useful tasks.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法使得能够在内存有限的设备（如笔记本电脑和手机）上部署高质量的7-13B参数范围的LLMs。通过我们的方法，可以轻松地开发出专门的7B LLM，并通过应用SpQR实现将这些LLMs部署到手机上。由于SpQR几乎是无损的，这确保了部署的LLMs能达到可靠的性能水平，这对消费者应用至关重要。由于手机无处不在，而LLMs是强大的通用工具，SpQR可能对普通大众如何利用LLMs完成有用任务产生广泛的影响。
- en: 'LLMs are inherently a dual-use technology that can bring both significant benefits
    and serious harm. The ethical and societal risks of LLMs range from deliberate
    malicious use (e.g. generating spam) and accidental misuse to adverse economic
    side-effects [[41](#bib.bib41)]. However, we believe that the marginal impact
    of SpQR will be positive or neutral since the LLMs we use are already openly available.
    Better quantization algorithms like SpQR let users with low-end devices run larger
    and generally more accurate language models. In other words, our algorithm does
    not create models with new capabilities (and risks): it only makes existing models
    more accessible.'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: LLMs本质上是一种双用途技术，既可以带来显著的好处，也可能造成严重的伤害。LLMs的伦理和社会风险包括恶意使用（例如生成垃圾邮件）和意外误用以及不利的经济副作用[[41](#bib.bib41)]。然而，我们认为SpQR的边际影响将是积极的或中性的，因为我们使用的LLMs已经是公开的。像SpQR这样的更好量化算法使低端设备的用户能够运行更大且通常更准确的语言模型。换句话说，我们的算法不会创造具有新能力（和风险）的模型，而只是使现有模型更易于访问。
- en: Appendix K On the use of LLMs in this work
  id: totrans-443
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录K 本工作的LLMs使用情况
- en: 'Following the request in this year’s call for papers, we describe the use of
    large language models in our paper. We used two different chat-based language
    models: ChatGPT and Claude+. We used these models to accelerate the process of
    writing LaTeX code in Alg. [1](#alg1 "Algorithm 1 ‣ 4.1 Overview ‣ 4 SpQR: A Sensitivity-aware
    compressed representation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless
    LLM Weight Compression") and Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Implementing and
    Leveraging the Sparse Quantized Representation ‣ 4 SpQR: A Sensitivity-aware compressed
    representation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM
    Weight Compression") (via Tikz). We also used these LLMs to provide slight improvements
    to the table design throughout the paper.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '根据今年的征稿要求，我们在本文中描述了大型语言模型的使用。我们使用了两种不同的聊天型语言模型：ChatGPT和Claude+。我们利用这些模型加快了在Alg.
    [1](#alg1 "Algorithm 1 ‣ 4.1 Overview ‣ 4 SpQR: A Sensitivity-aware compressed
    representation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM
    Weight Compression")和Figure [3](#S4.F3 "Figure 3 ‣ 4.2 Implementing and Leveraging
    the Sparse Quantized Representation ‣ 4 SpQR: A Sensitivity-aware compressed representation
    ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression")（通过Tikz）中编写LaTeX代码的过程。我们还使用这些LLMs对本文中的表格设计进行了细微的改进。'
- en: 'In addition to this, we use ChatGPT to generate some prompts for Appendix [I](#A9
    "Appendix I Generative examples ‣ SpQR: A Sparse-Quantized Representation for
    Near-Lossless LLM Weight Compression"). Finally, we used Claude+ to produce possible
    formulations for the outlier criterion in Alg. [1](#alg1 "Algorithm 1 ‣ 4.1 Overview
    ‣ 4 SpQR: A Sensitivity-aware compressed representation ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression"). In all these cases,
    we used LLMs through chat-based user interfaces, instructing them to generate
    code (LaTeX) or suggest improvements. If the suggested changes would not work
    as expected, we reported them to the model in natural language, using the same
    chat-based interface.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们使用ChatGPT生成了附录 [I](#A9 "Appendix I Generative examples ‣ SpQR: A Sparse-Quantized
    Representation for Near-Lossless LLM Weight Compression") 的一些提示。最后，我们使用Claude+为Alg.
    [1](#alg1 "Algorithm 1 ‣ 4.1 Overview ‣ 4 SpQR: A Sensitivity-aware compressed
    representation ‣ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM
    Weight Compression") 中的离群值标准产生了可能的公式。在所有这些情况下，我们通过基于聊天的用户界面使用LLMs，指示它们生成代码（LaTeX）或建议改进。如果建议的更改未能如预期工作，我们会通过相同的聊天界面用自然语言将问题报告给模型。'
