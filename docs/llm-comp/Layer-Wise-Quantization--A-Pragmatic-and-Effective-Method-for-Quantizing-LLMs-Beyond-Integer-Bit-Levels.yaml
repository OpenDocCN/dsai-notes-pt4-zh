- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:47:57'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:47:57
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分层量化：一种实用且有效的量化LLM的超整数位级别的方法
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17415](https://ar5iv.labs.arxiv.org/html/2406.17415)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2406.17415](https://ar5iv.labs.arxiv.org/html/2406.17415)
- en: Razvan-Gabriel Dumitru^(1,2), Vikas Yadav¹, Rishabh Maheshwary¹, Paul-Ioan Clotan³,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Razvan-Gabriel Dumitru^(1,2), Vikas Yadav¹, Rishabh Maheshwary¹, Paul-Ioan Clotan³,
- en: Sathwik Tejaswi Madhusudhan¹, Mihai Surdeanu²
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Sathwik Tejaswi Madhusudhan¹, Mihai Surdeanu²
- en: ¹ServiceNow Research, ²University of Arizona, ³Università di Bologna
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ServiceNow Research, ²亚利桑那大学, ³博洛尼亚大学
- en: 'Correspondence: [razvandumm@gmail.com](razvandumm@gmail.com)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 联系方式：[razvandumm@gmail.com](razvandumm@gmail.com)
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'We present a simple variable quantization approach that quantizes different
    layers of a large language model (LLM) at different bit levels. Specifically,
    we quantize the most important layers to higher bit precision and less important
    layers to lower bits to achieve floating point quantization levels. We propose
    two effective strategies to measure the importance of layers within LLMs: the
    first measures the importance of a layer based on how different its output embeddings
    are from the input embeddings (the higher the better); the second estimates the
    importance of a layer using the number of layer weights that are much larger than
    average (the smaller the better). We show that quantizing different layers at
    varying bits according to our importance scores results in minimal performance
    drop with a far more compressed model size. Finally, we present several practical
    key takeaways from our variable layer-wise quantization experiments: (a) LLM performance
    under variable quantization remains close to the original model until 25-50% of
    layers are moved in lower quantization using our proposed ordering but only until
    5-10% if moved using no specific ordering; (b) Quantizing LLMs to lower bits performs
    substantially better than pruning unless extreme quantization (2-bit) is used;
    and (c) Layer-wise quantization to lower bits works better in the case of larger
    LLMs with more layers compared to smaller LLMs with fewer layers. The code used
    to run the experiments is available at: https://github.com/RazvanDu/LayerwiseQuant.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种简单的变量量化方法，该方法在不同的位级别量化大语言模型（LLM）的不同层。具体来说，我们将最重要的层量化为更高的位精度，将较不重要的层量化为较低的位，以实现浮点量化水平。我们提出了两种有效的策略来衡量LLM内层的重要性：第一种基于输出嵌入与输入嵌入的差异来衡量层的重要性（差异越大越好）；第二种通过层权重中远大于平均值的数量来估计层的重要性（数量越少越好）。我们展示了根据我们的重要性评分将不同层量化为不同位数会导致性能下降最小，并且模型尺寸大大压缩。最后，我们展示了来自我们变量分层量化实验的几个实际关键收获：（a）在我们的建议排序下，将LLM的部分层量化为较低的位，性能仍接近原始模型，直到25-50%层被移动，但如果没有特定排序则仅为5-10%；（b）将LLM量化为较低的位的效果明显优于剪枝，除非使用极端量化（2位）；（c）在较大的LLM（具有更多层）情况下，分层量化为较低的位比在较小的LLM（具有较少层）情况下效果更好。用于运行实验的代码可在以下地址获取：https://github.com/RazvanDu/LayerwiseQuant。
- en: '¹¹footnotetext: The work was done while Razvan-Gabriel Dumitru interned at
    ServiceNow.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹脚注：这项工作是在Razvan-Gabriel Dumitru在ServiceNow实习期间完成的。
- en: 'Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 分层量化：一种实用且有效的量化LLM的超整数位级别的方法
- en: 'Razvan-Gabriel Dumitru^(1,2), Vikas Yadav¹, Rishabh Maheshwary¹, Paul-Ioan
    Clotan³, Sathwik Tejaswi Madhusudhan¹, Mihai Surdeanu² ¹ServiceNow Research, ²University
    of Arizona, ³Università di Bologna Correspondence: [razvandumm@gmail.com](razvandumm@gmail.com)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Razvan-Gabriel Dumitru^(1,2), Vikas Yadav¹, Rishabh Maheshwary¹, Paul-Ioan Clotan³,
    Sathwik Tejaswi Madhusudhan¹, Mihai Surdeanu² ¹ServiceNow Research, ²亚利桑那大学, ³博洛尼亚大学
    联系方式：[razvandumm@gmail.com](razvandumm@gmail.com)
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: '![Refer to caption](img/46f96d9dbfa4325d4bc74886cb39298d.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/46f96d9dbfa4325d4bc74886cb39298d.png)'
- en: 'Figure 1: Overall intuition behind our approach. We first rank the layers in
    an LLM (e.g., LLaMa-2-13B) using an importance score (shown here is ranking based
    on our Layer Input Modification (LIM) score, see [section 3.1](#S3.SS1 "3.1 Layer
    Importance Scores ‣ 3 Method ‣ Layer-Wise Quantization: A Pragmatic and Effective
    Method for Quantizing LLMs Beyond Integer Bit-Levels")). The color intensity in
    each layer represent their LIM importance score. Then, the 30 most important layers
    are quantized in 4 bit while the remaining 10 least important layers are quantized
    in 2 bits, resulting in 3.5 bits as the average bit size.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：我们方法的整体直觉。我们首先使用重要性分数对 LLM（例如，LLaMa-2-13B）中的层进行排序（此处显示的是基于我们的层输入修改（LIM）分数的排序，见
    [第 3.1 节](#S3.SS1 "3.1 层重要性分数 ‣ 3 方法 ‣ 层级量化：一种超越整数位级别的实际有效方法")）。每一层的颜色强度代表其 LIM
    重要性分数。然后，30 个最重要的层在 4 位中进行量化，而其余 10 个最不重要的层在 2 位中进行量化，从而得到 3.5 位的平均位大小。
- en: Large Language Models (LLMs) have achieved remarkable performance on a variety
    of tasks, especially when scaled to billions of parameters Jiang et al. ([2023](#bib.bib13),
    [2024](#bib.bib14)); Touvron et al. ([2023](#bib.bib29)); Zhang et al. ([2022](#bib.bib38));
    Team et al. ([2023](#bib.bib28)). The largest open-source models available can
    have an upwards of 400B parameters, such as LLaMa3-400B Touvron et al. ([2023](#bib.bib29)).
    Even small models such as LLaMa3-8B require as much as 20GB of VRAM to run on
    a GPU at the original precision, making them unusable for low resource settings.
    In such settings model compression techniques such as quantization are critical
    Zhu et al. ([2023b](#bib.bib40)); Wan et al. ([2023](#bib.bib31)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种任务上取得了显著的性能，尤其是在规模扩大到数十亿参数时 Jiang 等人 ([2023](#bib.bib13), [2024](#bib.bib14));
    Touvron 等人 ([2023](#bib.bib29)); Zhang 等人 ([2022](#bib.bib38)); Team 等人 ([2023](#bib.bib28))。目前可用的最大开源模型可以拥有高达
    400B 参数，例如 LLaMa3-400B Touvron 等人 ([2023](#bib.bib29))。即便是像 LLaMa3-8B 这样的较小模型，在原始精度下也需要多达
    20GB 的 VRAM 来运行，使其在资源有限的环境中无法使用。在这种环境下，模型压缩技术如量化至关重要 Zhu 等人 ([2023b](#bib.bib40));
    Wan 等人 ([2023](#bib.bib31))。
- en: Most prominent techniques for model compression broadly cover pruning Ma et al.
    ([2023](#bib.bib19)), knowledge distillation Gu et al. ([2023](#bib.bib11)), and
    quantization Zhu et al. ([2023b](#bib.bib40)). Pruning yields improvements in
    inference speed, but often results in substantial performance drop Frantar and
    Alistarh ([2023](#bib.bib7)); Men et al. ([2024](#bib.bib20)). On the other hand,
    quantization has proven to be a more robust solution for model size compression
    with comparatively much smaller performance drops Lin et al. ([2024](#bib.bib17)).
    In our work, we primarily focus on memory reduction through quantization. Further,
    quantization can be training specific or post-training Yao et al. ([2024](#bib.bib35)),
    where trained models are quantized without requiring any further training. We
    focus on post-training quantization due to its practicality.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最显著的模型压缩技术主要包括剪枝 Ma 等人 ([2023](#bib.bib19))、知识蒸馏 Gu 等人 ([2023](#bib.bib11))
    和量化 Zhu 等人 ([2023b](#bib.bib40))。剪枝可以提高推理速度，但常常导致性能大幅下降 Frantar 和 Alistarh ([2023](#bib.bib7));
    Men 等人 ([2024](#bib.bib20))。另一方面，量化被证明是一种更稳健的模型尺寸压缩解决方案，性能下降相对较小 Lin 等人 ([2024](#bib.bib17))。在我们的工作中，我们主要关注通过量化实现内存减少。此外，量化可以是训练特定的或训练后
    Yao 等人 ([2024](#bib.bib35))，其中训练后的模型进行量化，无需进一步训练。由于其实用性，我们专注于训练后量化。
- en: 'The majority of quantization techniques proposed recently (Frantar et al.,
    [2023](#bib.bib8); Xiao et al., [2023](#bib.bib33); Yao et al., [2022](#bib.bib36),
    inter alia) focus on the quantization of all the LLM layers to a single precision
    bit, or quantizing only specific weights of the network Lin et al. ([2024](#bib.bib17)).
    This has shown to be costly, their effectiveness is data dependent, and their
    implementations can be considered moderately challenging. In contrast, we propose
    a simple technique that quantizes different layers at different bit precision
    depending on their importance. Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣
    Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels") shows an example of our idea. We show that our approach
    is simple to implement, performs well, and provides greater flexibility to compress
    models in varying bit precision as per memory requirements. Further, our approach
    is complementary to other quantization techniques. For example, in our experiments
    we coupled our idea with two effective and prominent quantization techniques:
    GPT-Q and Quanto, but any other techinques can be used.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '最近提出的大多数量化技术（Frantar等，[2023](#bib.bib8)；Xiao等，[2023](#bib.bib33)；Yao等，[2022](#bib.bib36)，以及其他）集中于将所有LLM层量化为单一精度位，或仅量化网络的特定权重Lin等（[2024](#bib.bib17)）。这被证明是昂贵的，其有效性依赖于数据，且实现难度适中。相比之下，我们提出了一种简单的技术，根据层的重要性量化不同层到不同的位精度。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective
    Method for Quantizing LLMs Beyond Integer Bit-Levels")展示了我们的想法示例。我们表明，我们的方法易于实现，表现良好，并且提供了在不同位精度下压缩模型的更大灵活性。此外，我们的方法可以与其他量化技术互补。例如，在我们的实验中，我们将我们的想法与两种有效且突出的量化技术结合使用：GPT-Q和Quanto，但也可以使用其他技术。'
- en: 'The contributions of our work are as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的贡献如下：
- en: '![Refer to caption](img/4c5d9b032dd99f8d14373e5492ffdfdf.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/4c5d9b032dd99f8d14373e5492ffdfdf.png)'
- en: 'Figure 2: Plots showing the effect of variable quantization for LLaMa2-13b
    and multiple datasets using Quanto. The leftmost point indicates LLM performance
    when all 40 layers of the LLM are represented in 4-bit; the rightmost point shows
    LLM performance when all layers are quantized to 2-bit. The dots on each curve
    (in each plot) show accuracy when the model is quantized to lower bits by converting
    less important layers to 2 bits one by one. Red and purple line indicate performance
    from 8bit and fp16 precision model (ceiling models). As shown, there is no considerable
    performance drop from fp16 or 8bit to 4bit precision. Hence, we focus our experiments
    on quantization below 4 bits. The vertical gray line indicates the quantization
    point that preserves 90% of the 4bit performance. The red line represents when
    layers are ordered randomly. We chose 3 random orders of the layers and quantized
    layers to 2 bits as per these orders. The standard deviation in performance from
    random orders are highlighted on the red curve. The curves are plotted on 2K evaluation
    data while results on full data is summarized in [table 1](#S3.T1 "In 3.2.1 Quanto
    ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '图2：显示了LLaMa2-13b和多个数据集在Quanto下变量化效果的图表。最左侧的点表示LLM在所有40层都以4位表示时的性能；最右侧的点显示了所有层都量化到2位时LLM的性能。每条曲线上的点（每个图表中）显示了当模型通过将不太重要的层逐一转换为2位时的准确性。红色和紫色线表示来自8位和fp16精度模型（天花板模型）的性能。如图所示，从fp16或8位到4位精度的性能没有显著下降。因此，我们将实验重点放在低于4位的量化上。垂直灰线表示保留90%
    4位性能的量化点。红线表示层按随机顺序排列时的情况。我们选择了3种层的随机顺序，并根据这些顺序将层量化为2位。随机顺序下的性能标准差在红色曲线上突出显示。曲线绘制在2K评估数据上，而在完整数据上的结果总结在[表1](#S3.T1
    "In 3.2.1 Quanto ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")中。'
- en: '![Refer to caption](img/64cc1d91c897604ff920334440c5fa57.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/64cc1d91c897604ff920334440c5fa57.png)'
- en: 'Figure 3: Plots showing the effect of variable quantization for Mistral-7b
    and multiple datasets using Quanto. All notations are the same as in [Figure 2](#S1.F2
    "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels").'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '图3：显示了Mistral-7b和多个数据集在Quanto下变量化效果的图表。所有符号与[图2](#S1.F2 "In 1 Introduction
    ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels")中相同。'
- en: •
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce a novel method for layer-wise quantization of LLMs at different
    bit levels to achieve flexible lower bit precision overall, enabling LLMs to fit
    into range of specified memory capacity. We accompany our method with a detailed
    study, which highlights various important findings such as importance of layer
    ranking for quantizing less important layers with lower bits and more important
    layers with higher bit precision. We show that models can be quantized significantly
    more from 4-bit while retaining 90% of performance when following our proposed
    order.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一种新颖的方法，用于在不同位级别下对LLMs进行层级量化，以实现整体的灵活低位精度，使LLMs能够适应指定的内存容量范围。我们结合详细的研究，突出了诸如层级排名对于用较低位数量化不重要层以及用更高位数量化重要层的重要性等各种重要发现。我们展示了模型可以从4位量化显著地提高，同时在遵循我们提出的顺序时保持90%的性能。
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose and study two layer importance scores for variable quantization.
    Our first score named layer input modification (LIM) is based on how much a layer
    changes its input representations into the output ones. This score is calculated
    with an unlabeled text calibration corpus. The second scoring method, called z-score
    distribution (ZD), measures the distribution of weight magnitudes within a layer
    to determine its importance. To our knowledge, we are the first to propose layer
    orderings for quantization. We validate these scores by empirically showing that
    when LLM layers are quantized to lower bits as per rankings from our two importance
    score, they retain performance much more strongly than several other layer ordering
    baselines.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出并研究了用于变量量化的两个层级重要性评分。我们的第一个评分方法称为层输入修改（LIM），基于层将输入表示转化为输出表示的程度。这个评分是通过一个未标记的文本校准语料库计算的。第二种评分方法，称为z-score分布（ZD），测量层内权重幅度的分布以确定其重要性。据我们所知，我们是首个提出量化层级排序的方法。我们通过实验证明了这些评分方法的有效性，当根据我们两种重要性评分的排名将LLM层量化到更低的位数时，它们的性能保持得比其他层级排序基准强得多。
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We evaluate the impact of our variable quantization method based on layer importance
    on five top performing LLMs from different model families and different sizes.
    We draw several important practical findings from these experiments: (a) LLM performance
    under variable quantization remains close to the original model when using our
    ordering until reaching the level of 3.0-3.25 bits on average; (b) Quantizing
    LLMs to lower bits performs substantially better than pruning; however, under
    extreme quantization settings (i.e., $<=2-bits$ bits) performs much better than
    three levels of quantization, suggesting that the interaction between layers with
    different quantization is complex and may need more investigation; and (d) Layer-wise
    quantization to lower bits works better in the case of larger LLMs with more layers
    compared to smaller LLMs with fewer layers.'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们评估了基于层级重要性的变量量化方法对来自不同模型家族和不同规模的五个顶级LLMs的影响。我们从这些实验中得出了几个重要的实际发现：（a）在使用我们的排序方法时，LLM在变量量化下的性能接近原始模型，直到达到平均3.0-3.25位；（b）将LLM量化到较低位数的表现显著优于剪枝；然而，在极端量化设置下（即$<=2-bits$位）比三级量化表现更好，这表明不同量化层之间的交互复杂，可能需要更多调查；（d）在层数较多的大型LLM中，层级量化到较低位数的效果优于层数较少的小型LLM。
- en: 2 Related Work
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: Quantization techniques for large language models (LLMs) aim to reduce the precision
    of weights and activations to lower-bits without significantly compromising performance Zhu
    et al. ([2023b](#bib.bib40)). Popular approaches include Post-Training Quantization
    (PTQ) Banner et al. ([2019](#bib.bib2)) and Quantization-Aware Training (QAT)
    Liu et al. ([2023](#bib.bib18)). PTQ can be divided into static quantization,
    which uses a small dataset to calibrate scaling factors for weights and activations,
    and dynamic quantization, which quantizes activations on-the-fly during inference.
    Our study utilizes the static PTQ techniques such as GPT-Q in our experiments
    for practicality.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的量化技术旨在将权重和激活的精度降低到更低位数，而不会显著影响性能。流行的方法包括后训练量化（PTQ）Banner 等人（[2019](#bib.bib2)）和量化感知训练（QAT）Liu
    等人（[2023](#bib.bib18)）。PTQ 可以分为静态量化，使用小数据集来校准权重和激活的缩放因子，以及动态量化，在推理过程中实时量化激活。我们的研究利用了静态PTQ技术，如GPT-Q，用于实验中的实用性。
- en: 'A few works have highlighted 4 bit precision as a robust quantization limit
    for wide variety of NLP tasks Dettmers and Zettlemoyer ([2023](#bib.bib6)). Our
    results ([fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels") and [3](#S1.F3
    "Figure 3 ‣ 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective
    Method for Quantizing LLMs Beyond Integer Bit-Levels")) also show similar findings
    that performance drop is very minute from bf16 to 8 bits, and then to 4 bits.
    Hence, our experiments focus more on quantizing LLMs below 4 bits to highlight
    effectiveness of layer-wise quantization based on layer importance. Specifically,
    our empirical results ([section 5](#S5 "5 Discussion of Results ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"))
    show that variable layer-wise quantization can retain 90% of the performance with
    a notable compression upto 2.85 bits overall. Concurrent (ArXiv preprint) work
    by Tai et al. ([2024](#bib.bib27)) also show effectiveness of variable quantization
    in vision language models.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究强调了4位精度作为广泛NLP任务的稳健量化极限（Dettmers 和 Zettlemoyer ([2023](#bib.bib6)）。我们的结果（[图2](#S1.F2
    "在 1 引言 ‣ 层级量化：量化LLMs超越整数位级别的务实且有效的方法") 和 [3](#S1.F3 "图3 ‣ 1 引言 ‣ 层级量化：量化LLMs超越整数位级别的务实且有效的方法")）也显示了类似的发现，即性能从bf16到8位，再到4位的下降非常微小。因此，我们的实验更多地关注于将LLMs量化到低于4位，以突出基于层重要性的层级量化的有效性。具体而言，我们的实证结果（[第5节](#S5
    "5 结果讨论 ‣ 层级量化：量化LLMs超越整数位级别的务实且有效的方法")）显示，变层级量化可以在总体压缩高达2.85位的情况下保持90%的性能。Tai
    等人（[2024](#bib.bib27)）的同步（ArXiv 预印本）工作也显示了在视觉语言模型中变量化的有效性。
- en: 'A few previous works have studied layer importance in transformers Vaswani
    et al. ([2017](#bib.bib30)); Simoulin and Crabbé ([2021](#bib.bib24)). Some of
    the recent, (unpublished) concurrent works such as ShortGPT Men et al. ([2024](#bib.bib20))
    have also proposed utilizing layer importance but primarily focusing on pruning.
    Shen et al. ([2020](#bib.bib23)) had utilized hessian information from each layer
    as an importance measure to quantize specific weight matrices at different bits.
    Different from these, our work is focused more on utilizing layer importance for
    quantizing more important layers (fully) in higher bits and less important layers
    in lower bits. Importantly, our proposed approach of layer-wise quantization of
    LLMs based on their layer importance is a method that can be easily extended with
    any quantization techniques like GPT-Q and Quanto ([section 3.2](#S3.SS2 "3.2
    Quantization Techniques ‣ 3 Method ‣ Layer-Wise Quantization: A Pragmatic and
    Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")). We also compare
    ShortGPT layer importance based pruning baseline with our layer-wise quantization
    approach in [section 6.1](#S6.SS1 "6.1 Pruning vs. Quantization ‣ 6 Analyses ‣
    Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels").'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一些早期工作研究了变换器中的层重要性（Vaswani 等人 [2017](#bib.bib30)；Simoulin 和 Crabbé [2021](#bib.bib24)）。一些近期的（未发表）同步工作，如
    ShortGPT（Men 等人 [2024](#bib.bib20)），也提出了利用层重要性，但主要集中在剪枝上。Shen 等人（[2020](#bib.bib23)）利用每层的Hessian信息作为重要性度量，以在不同位数下量化特定的权重矩阵。与这些不同，我们的工作更侧重于利用层重要性对更重要的层（完全）进行高位量化，而对不重要的层进行低位量化。重要的是，我们提出的基于层重要性的LLMs层级量化方法是一种可以轻松扩展到任何量化技术（如GPT-Q和Quanto）的的方法（[第3.2节](#S3.SS2
    "3.2 量化技术 ‣ 3 方法 ‣ 层级量化：量化LLMs超越整数位级别的务实且有效的方法")）。我们还在[第6.1节](#S6.SS1 "6.1 剪枝
    vs. 量化 ‣ 6 分析 ‣ 层级量化：量化LLMs超越整数位级别的务实且有效的方法")中将ShortGPT的层重要性剪枝基准与我们的层级量化方法进行了比较。
- en: 3 Method
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: In our study, given a LLM with N layers denoted as $\{\mathbf{L_{1},L_{2},L_{3},....L_{N}}\}$,
    we first compute the importance of each layer, and then quantize them differently
    based on their importance. To achieve the former, we propose two scoring methods
    that allow us to rank the layers based on their respective importance scores.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的研究中，给定一个包含N层的LLM，记作$\{\mathbf{L_{1},L_{2},L_{3},....L_{N}}\}$，我们首先计算每层的重要性，然后根据其重要性进行不同的量化。为了实现前者，我们提出了两种评分方法，使我们能够根据各自的重要性评分对层进行排名。
- en: 3.1 Layer Importance Scores
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 层重要性评分
- en: 'We call our first layer importance score layer input modification (LIM). The
    intuition behind LIM is that the more a layer changes its received input embeddings
    the more important it must be. More formally, the LIM score for a specific layer
    $\mathbf{L_{i}}$ as shown below:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称第一个层重要性评分为层输入修改（LIM）。LIM的直觉是，层对接收到的输入嵌入的改变越多，该层的重要性就越高。更正式地，特定层 $\mathbf{L_{i}}$
    的LIM评分如下所示：
- en: '|  | $\vspace{-2mm}\text{LIM}(\mathbf{L_{i}})=-\frac{\mathbf{L_{i}^{I}}\cdot\mathbf{L_{i}^{O}}}{\&#124;\mathbf{L_{i}^{I}}\&#124;\&#124;\mathbf{L_{i}^{O}}\&#124;}\vspace{+1mm}$
    |  | (1) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\vspace{-2mm}\text{LIM}(\mathbf{L_{i}})=-\frac{\mathbf{L_{i}^{I}}\cdot\mathbf{L_{i}^{O}}}{\&#124;\mathbf{L_{i}^{I}}\&#124;\&#124;\mathbf{L_{i}^{O}}\&#124;}\vspace{+1mm}$
    |  | (1) |'
- en: The dot product $\mathbf{L_{i}^{I}}\cdot\mathbf{L_{i}^{O}}$, we use all 50 documents
    of pg19! Rae et al. ([2019](#bib.bib21)) (an unlabelled text corpus) as calibration
    corpus.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 点积 $\mathbf{L_{i}^{I}}\cdot\mathbf{L_{i}^{O}}$，我们使用pg19的所有50份文档！Rae等人（[2019](#bib.bib21)）的（一个未标记的文本语料库）作为校准语料库。
- en: Our second score, called z-score distribution (ZD), is based on the distribution
    of parameter values and does not require any calibration data. Intuitively, this
    score considers a layer as more important if it contains more weights that are
    much higher than average.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个评分，称为z-score分布（ZD），基于参数值的分布，不需要任何校准数据。直观地，这个评分认为如果一个层包含的权重远高于平均值，则该层更重要。
- en: We examine the proportion of weights in a layer exhibiting a z-score greater
    than 1\. The z-score of a weight $w_{i}$ is defined as
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了层中权重的比例，这些权重的z-score大于1。权重 $w_{i}$ 的z-score定义为
- en: '|  | $z_{i}=\frac{w_{i}-\mu}{\sigma},$ |  |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{i}=\frac{w_{i}-\mu}{\sigma},$ |  |'
- en: where for layer $\mathbf{L_{i}}$, is expressed as
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于层 $\mathbf{L_{i}}$，表达为
- en: '|  | $$\text{ZD}(L_{i})=\frac{&#124;Li_{Zscore}> |  |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $$\text{ZD}(L_{i})=\frac{&#124;Li_{Zscore}> |  |'
- en: quantifying the ratio of weights whose z-scores exceed 1 (<math id=$$ layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 量化权重中z-score超过1的比例（<math id=$$ 层。
- en: 'Layer ranking baselines:'
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 层排序基准：
- en: 'To highlight the benefits of our layer importance scores, we compare them to
    two baselines. First, we randomly rank layers with different random seeds to highlight
    ordering of layer is important. Second, to specifically highlight the strengths
    of the LIM score, we consider a reverse LIM ordering as a baseline. Finally, we
    also considered the reverse index of layers, i.e., from layer 30 to layer 1, following
    Gromov et al. ([2024](#bib.bib10)) who has shown just removing layers from the
    top of the LLM to be an effective order for layer pruning. We implement the same
    pruning with specified reverse indexes from Gromov et al. ([2024](#bib.bib10))
    and show its comparison in [Figure 4(a)](#S5.F4.sf1 "In Figure 4 ‣ Item (3) ‣
    5 Discussion of Results ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了突出我们层重要性评分的优点，我们将其与两个基准进行比较。首先，我们使用不同的随机种子随机排列层，以突出层的排序是重要的。其次，为了特别突出LIM评分的优势，我们将反向LIM排序作为基准。最后，我们还考虑了层的反向索引，即从层30到层1，参考Gromov等人（[2024](#bib.bib10)）所展示的，移除LLM顶部的层作为有效的层修剪顺序。我们实现了相同的修剪方法，并使用Gromov等人（[2024](#bib.bib10)）指定的反向索引进行比较，结果展示在[图4(a)](#S5.F4.sf1
    "在图4 ‣ 项目 (3) ‣ 5 结果讨论 ‣ 分层量化：一种实用且有效的LLM量化方法，超越整数位级别")。
- en: 3.2 Quantization Techniques
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 量化技术
- en: In our work, we employ two post-training quantization techniques—GPT-Q and Quanto—to
    our proposed layer-wise quantization study.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的工作中，我们将两种后训练量化技术—GPT-Q和Quanto—应用于我们提出的分层量化研究。
- en: 3.2.1 Quanto
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 Quanto
- en: Quanto is a fast-acting quantization method¹¹1[https://github.com/huggingface/optimum-quanto](https://github.com/huggingface/optimum-quanto)
    that simplifies the process of reducing precision after training. In practice,
    Quanto achieves quicker quantization by applying uniform scaling factors across
    all layers of a model, avoiding the need for detailed data-driven analysis of
    each layer’s distribution. Similarly to most techniques, it allows for int2, int4,
    int8, and fp8 quantization. In our implementation for variable quantization of
    different layers, we quantized the same model to two different bit levels (lets
    say int2 and int4). Then, based on the layer importance order, we selected less
    important layers from the int2 quantized layer sets and more important layers
    from int4 quantized sets.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Quanto 是一种快速量化方法¹¹1[https://github.com/huggingface/optimum-quanto](https://github.com/huggingface/optimum-quanto)，简化了训练后精度降低的过程。在实践中，Quanto
    通过在模型的所有层中应用统一的缩放因子来实现更快的量化，避免了对每一层分布进行详细数据驱动分析的需要。与大多数技术类似，它支持 int2、int4、int8
    和 fp8 量化。在我们对不同层进行可变量化的实现中，我们将相同的模型量化为两个不同的位级别（例如 int2 和 int4）。然后，根据层的重要性顺序，我们从
    int2 量化层集选择较不重要的层，从 int4 量化集选择较重要的层。
- en: Quanto Quantization Model Avg. Layers WNGD ARC PIQA HLSWG MMLU Avg.Acc. bits
    2 bits 4-2 bit LLaMa-7B 4.0 0 67.9 74.2 77.3 55.9 38.4 62.7 Mistral-7B 4.0 0 72.5
    78.8 79.8 59.3 55.5 69.2 LLaMa-13B 4.0 0 71.0 78.7 79.2 59.0 50.0 67.6 QWEN-7B
    4.0 0 68.7 79.0 79.2 57.7 66.3 70.2 \cdashline2-10 LIM Ordering LLaMa-7B 3.68
    5 65.6 68.7 74.6 53.7 36.6 59.8 3.37 10 65.3 61.9 70.6 49.8 34.3 56.4 3.06 15
    60.8 45.6 64.3 42.2 27.6 48.1 Mistral-7B 3.68 5 71.7 74.0 76.7 56.4 54.9 66.7
    3.37 10 69.3 61.8 70.0 50.1 51.7 60.6 3.06 15 59.4 43.6 61.2 37.6 26.4 45.6 LLama-13B
    3.75 5 70.2 76.6 78.0 57.7 48.9 66.3 3.50 10 69.1 72.9 76.3 55.7 47.4 64.3 3.25
    15 69.7 66.9 73.7 52.6 45.8 61.7 Qwen-2-7B 3.64 5 51.1 46.3 67.3 40.9 24.1 46.0
    3.28 10 51.7 31.0 57.4 29.8 23.4 38.6 2.92 15 48.2 25.7 53.1 26.1 24.5 35.5 Z-score
    Ordering LLama-7B 3.68 5 65.7 68.7 74.9 53.0 33.5 59.1 3.37 10 64.1 59.2 69.7
    48.7 31.2 54.6 3.06 15 55.4 43.8 61.4 36.4 24.5 44.3 Mistral-7B 3.68 5 70.7 74.2
    77.5 56.3 53.0 66.3 3.37 10 53.3 39.3 60.0 30.5 23.4 41.3 3.06 15 51.7 27.5 53.3
    27.2 23.5 36.6 LLama-13B 3.75 5 70.3 76.0 77.2 57.1 48.1 65.7 3.50 10 70.7 72.3
    75.8 54.6 47.0 64.1 3.25 15 68.9 66.8 72.1 51.9 47.0 61.3 Qwen-2-7B 3.64 5 63.1
    61.0 70.5 48.4 55.6 59.7 3.28 10 51.5 29.3 53.6 27.0 25.3 37.3 2.92 15 49.9 26.0
    52.5 26.0 25.0 35.9
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Quanto 量化模型 平均 层数 WNGD ARC PIQA HLSWG MMLU 平均准确度 位数 2 位 4-2 位 LLaMa-7B 4.0 0
    67.9 74.2 77.3 55.9 38.4 62.7 Mistral-7B 4.0 0 72.5 78.8 79.8 59.3 55.5 69.2 LLaMa-13B
    4.0 0 71.0 78.7 79.2 59.0 50.0 67.6 QWEN-7B 4.0 0 68.7 79.0 79.2 57.7 66.3 70.2
    \cdashline2-10 LIM 排序 LLaMa-7B 3.68 5 65.6 68.7 74.6 53.7 36.6 59.8 3.37 10 65.3
    61.9 70.6 49.8 34.3 56.4 3.06 15 60.8 45.6 64.3 42.2 27.6 48.1 Mistral-7B 3.68
    5 71.7 74.0 76.7 56.4 54.9 66.7 3.37 10 69.3 61.8 70.0 50.1 51.7 60.6 3.06 15
    59.4 43.6 61.2 37.6 26.4 45.6 LLama-13B 3.75 5 70.2 76.6 78.0 57.7 48.9 66.3 3.50
    10 69.1 72.9 76.3 55.7 47.4 64.3 3.25 15 69.7 66.9 73.7 52.6 45.8 61.7 Qwen-2-7B
    3.64 5 51.1 46.3 67.3 40.9 24.1 46.0 3.28 10 51.7 31.0 57.4 29.8 23.4 38.6 2.92
    15 48.2 25.7 53.1 26.1 24.5 35.5 Z-score 排序 LLama-7B 3.68 5 65.7 68.7 74.9 53.0
    33.5 59.1 3.37 10 64.1 59.2 69.7 48.7 31.2 54.6 3.06 15 55.4 43.8 61.4 36.4 24.5
    44.3 Mistral-7B 3.68 5 70.7 74.2 77.5 56.3 53.0 66.3 3.37 10 53.3 39.3 60.0 30.5
    23.4 41.3 3.06 15 51.7 27.5 53.3 27.2 23.5 36.6 LLama-13B 3.75 5 70.3 76.0 77.2
    57.1 48.1 65.7 3.50 10 70.7 72.3 75.8 54.6 47.0 64.1 3.25 15 68.9 66.8 72.1 51.9
    47.0 61.3 Qwen-2-7B 3.64 5 63.1 61.0 70.5 48.4 55.6 59.7 3.28 10 51.5 29.3 53.6
    27.0 25.3 37.3 2.92 15 49.9 26.0 52.5 26.0 25.0 35.9
- en: 'Table 1: Accuracy on full evaluation datasets of different models quantized
    with Quanto. All layers start at 4 bits; we then quantize N number of layers in
    2 bits where N is mentioned in the “Layers 2 bits” column. We also show results
    for 8 to 4 bit quantization in Appendix in [table 4](#A1.T4 "In A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). Average
    performances within 90% of the 4 bit model are highlighted in bold.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：使用 Quanto 量化的不同模型在完整评估数据集上的准确性。所有层从 4 位开始；然后我们在“Layers 2 bits”列中提到的 N 数量的层中进行
    2 位量化。我们还在附录中的 [表 4](#A1.T4 "在 A.1 使用 3 个级别进行量化 ‣ 附录 A 附录 ‣ 层级量化：一种务实且有效的量化大规模语言模型的方法")
    显示了 8 位到 4 位量化的结果。4 位模型的平均性能在 90% 内的结果用粗体标出。
- en: '| GPT-Q Quantization |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| GPT-Q 量化 |'
- en: '| --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '|  | Model | Avg. | Layers | WNGD | ARC | PIQA | HLSWG | MMLU | Avg.Acc |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | 模型 | 平均 | 层数 | WNGD | ARC | PIQA | HLSWG | MMLU | 平均准确度 |'
- en: '|  |  | bits | 2 bits |  |  |  |  |  |  |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 位数 | 2 位 |  |  |  |  |  |  |'
- en: '| LIM Ordering | LLama-7B | 3.68 | 5 | 67.1 | 73.4 | 76.2 | 54.9 | 37.5 | 61.8
    |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| LIM 排序 | LLama-7B | 3.68 | 5 | 67.1 | 73.4 | 76.2 | 54.9 | 37.5 | 61.8 |'
- en: '| 3.37 | 10 | 67.6 | 67.5 | 73.4 | 52.9 | 35.2 | 59.3 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 3.37 | 10 | 67.6 | 67.5 | 73.4 | 52.9 | 35.2 | 59.3 |'
- en: '| 3.06 | 15 | 65.1 | 56.0 | 68.0 | 46.4 | 31.5 | 53.4 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 3.06 | 15 | 65.1 | 56.0 | 68.0 | 46.4 | 31.5 | 53.4 |'
- en: '|  | Mistral-7B | 3.68 | 5 | 73.1 | 77.4 | 78.0 | 59.1 | 55.4 | 68.6 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | Mistral-7B | 3.68 | 5 | 73.1 | 77.4 | 78.0 | 59.1 | 55.4 | 68.6 |'
- en: '|  | 3.37 | 10 | 70.6 | 74.8 | 76.9 | 56.8 | 54.9 | 66.8 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.37 | 10 | 70.6 | 74.8 | 76.9 | 56.8 | 54.9 | 66.8 |'
- en: '|  | 3.06 | 15 | 66.1 | 65.3 | 72.9 | 50.7 | 42.0 | 59.4 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.06 | 15 | 66.1 | 65.3 | 72.9 | 50.7 | 42.0 | 59.4 |'
- en: '|  | LLama-13B | 3.75 | 5 | 71.4 | 77.5 | 78.5 | 59.0 | 49.3 | 67.1 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | LLama-13B | 3.75 | 5 | 71.4 | 77.5 | 78.5 | 59.0 | 49.3 | 67.1 |'
- en: '|  | 3.50 | 10 | 71.8 | 76.4 | 77.7 | 58.2 | 48.2 | 66.5 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.50 | 10 | 71.8 | 76.4 | 77.7 | 58.2 | 48.2 | 66.5 |'
- en: '|  | 3.25 | 15 | 72.6 | 73.7 | 75.7 | 56.7 | 47.3 | 65.2 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.25 | 15 | 72.6 | 73.7 | 75.7 | 56.7 | 47.3 | 65.2 |'
- en: '|  | Qwen-2-7B | 3.64 | 5 | 62.0 | 67.2 | 77.0 | 52.0 | 56.6 | 63.0 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | Qwen-2-7B | 3.64 | 5 | 62.0 | 67.2 | 77.0 | 52.0 | 56.6 | 63.0 |'
- en: '|  | 3.28 | 10 | 56.7 | 53.1 | 71.4 | 45.5 | 29.4 | 51.2 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.28 | 10 | 56.7 | 53.1 | 71.4 | 45.5 | 29.4 | 51.2 |'
- en: '|  | 2.92 | 15 | 53.4 | 45.0 | 66.4 | 42.0 | 25.0 | 46.4 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | 2.92 | 15 | 53.4 | 45.0 | 66.4 | 42.0 | 25.0 | 46.4 |'
- en: '| Z-score Ordering | LLama-7B | 3.68 | 5 | 67.2 | 71.1 | 75.9 | 54.9 | 38.1
    | 61.4 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| Z-score排序 | LLama-7B | 3.68 | 5 | 67.2 | 71.1 | 75.9 | 54.9 | 38.1 | 61.4
    |'
- en: '| 3.37 | 10 | 68.0 | 66.9 | 73.1 | 52.1 | 33.7 | 58.8 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 3.37 | 10 | 68.0 | 66.9 | 73.1 | 52.1 | 33.7 | 58.8 |'
- en: '| 3.06 | 15 | 62.9 | 56.7 | 67.8 | 46.2 | 28.5 | 52.4 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 3.06 | 15 | 62.9 | 56.7 | 67.8 | 46.2 | 28.5 | 52.4 |'
- en: '|  | Mistral-7B | 3.68 | 5 | 72.9 | 77.7 | 78.8 | 59.2 | 55.1 | 68.7 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | Mistral-7B | 3.68 | 5 | 72.9 | 77.7 | 78.8 | 59.2 | 55.1 | 68.7 |'
- en: '|  | 3.37 | 10 | 69.1 | 72.1 | 74.8 | 53.2 | 38.9 | 61.6 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.37 | 10 | 69.1 | 72.1 | 74.8 | 53.2 | 38.9 | 61.6 |'
- en: '|  | 3.06 | 15 | 62.0 | 52.7 | 65.6 | 39.5 | 25.1 | 49.0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.06 | 15 | 62.0 | 52.7 | 65.6 | 39.5 | 25.1 | 49.0 |'
- en: '|  | LLama-13B | 3.75 | 5 | - | - | - | - | - | - |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | LLama-13B | 3.75 | 5 | - | - | - | - | - | - |'
- en: '|  | 3.50 | 10 | 71.3 | 75.0 | 76.4 | 57.4 | 48.2 | 65.6 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.50 | 10 | 71.3 | 75.0 | 76.4 | 57.4 | 48.2 | 65.6 |'
- en: '|  | 3.25 | 15 | 72.3 | 73.5 | 76.2 | 56.4 | 45.7 | 64.8 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.25 | 15 | 72.3 | 73.5 | 76.2 | 56.4 | 45.7 | 64.8 |'
- en: '|  | Qwen-2-7B | 3.64 | 5 | 69.1 | 71.1 | 75.2 | 54.0 | 64.3 | 66.8 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | Qwen-2-7B | 3.64 | 5 | 69.1 | 71.1 | 75.2 | 54.0 | 64.3 | 66.8 |'
- en: '|  | 3.28 | 10 | 65.6 | 65.2 | 71.6 | 48.2 | 47.5 | 59.6 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 3.28 | 10 | 65.6 | 65.2 | 71.6 | 48.2 | 47.5 | 59.6 |'
- en: '|  | 2.92 | 15 | 54.8 | 50.0 | 66.7 | 42.8 | 30.4 | 49.4 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | 2.92 | 15 | 54.8 | 50.0 | 66.7 | 42.8 | 30.4 | 49.4 |'
- en: 'Table 2: Comparison of different models and their performance across various
    tasks with GPT-Q quantization.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：不同模型及其在各种任务中的性能比较，使用GPT-Q量化。
- en: 3.2.2 GPT-Q
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 GPT-Q
- en: GPT-Q is a post-training quantization technique specifically designed for GPT
    models Frantar et al. ([2023](#bib.bib8)). Utilizing a dataset, it calculates
    the necessary scaling factors for quantization. After training, GPT-Q assesses
    the distribution of weights using this dataset to determine optimal scaling factors
    for converting floating-point representations to lower-bit formats such as int8
    or int4\. We only have several data points for GPT-Q and it adds significant execution
    time overhead to our experiments. Both Quanto and GPT-Q are available in HuggingFace
    library Wolf et al. ([2019](#bib.bib32)). We modified it for our use-case in the
    same way as described for Quanto.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: GPT-Q是一种专门为GPT模型设计的后训练量化技术 Frantar et al. ([2023](#bib.bib8))。利用一个数据集，它计算量化所需的缩放因子。训练后，GPT-Q使用此数据集评估权重的分布，以确定将浮点表示转换为更低位格式（如int8或int4）的最佳缩放因子。我们只有几个数据点用于GPT-Q，它为我们的实验增加了显著的执行时间开销。Quanto和GPT-Q都可以在HuggingFace库中找到
    Wolf et al. ([2019](#bib.bib32))。我们以与Quanto类似的方式修改了它以适应我们的用例。
- en: 4 Experiments
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Models and Hyperparameters
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 模型和超参数
- en: We studied quantization on 5 LLMs from different model families and different
    sizes – LLaMa-2-7b Touvron et al. ([2023](#bib.bib29)), LLaMa-2-13B, Mistral-7b
    Jiang et al. ([2023](#bib.bib13)), QWEN-1.8b, and QWEN-7b Bai et al. ([2023](#bib.bib1)).
    We evaluated these LLMs on 8 A100 GPUs with a batch size of 1 for inference using
    LLM harness library Gao et al. ([2023](#bib.bib9)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了来自不同模型家族和不同规模的5个LLM的量化 – LLaMa-2-7b Touvron et al. ([2023](#bib.bib29)),
    LLaMa-2-13B, Mistral-7b Jiang et al. ([2023](#bib.bib13)), QWEN-1.8b 和 QWEN-7b
    Bai et al. ([2023](#bib.bib1))。我们在8个A100 GPU上，使用LLM harness库 Gao et al. ([2023](#bib.bib9))，以批量大小为1进行推理评估这些LLM。
- en: 4.2 Evaluation Datasets
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 评估数据集
- en: 'We select five diverse NLP tasks for evaluating the quantization effects: Winogrande
    Sakaguchi et al. ([2021](#bib.bib22)), ARC-easy Clark et al. ([2018](#bib.bib4)),
    PIQA Bisk et al. ([2020](#bib.bib3)), HellaSwag Zellers et al. ([2019](#bib.bib37)),
    and MMLU Hendrycks et al. ([2020](#bib.bib12)). We also evaluate our approach
    on two generation datasets to cover diverse tasks of reasoning and answer generation:
    GSM8K Cobbe et al. ([2021](#bib.bib5)), which contains math questions, and the
    Natural Questions (open) dataset Kwiatkowski et al. ([2019](#bib.bib16)), which
    consists of open-domain answer generation task.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了五个多样化的NLP任务来评估量化效果：Winogrande Sakaguchi et al. ([2021](#bib.bib22))，ARC-easy
    Clark et al. ([2018](#bib.bib4))，PIQA Bisk et al. ([2020](#bib.bib3))，HellaSwag
    Zellers et al. ([2019](#bib.bib37))，和MMLU Hendrycks et al. ([2020](#bib.bib12))。我们还在两个生成数据集上评估了我们的方法，以涵盖推理和回答生成的多样任务：GSM8K
    Cobbe et al. ([2021](#bib.bib5))，该数据集包含数学问题，以及Natural Questions (open) 数据集 Kwiatkowski
    et al. ([2019](#bib.bib16))，其中包括开放领域回答生成任务。
- en: 5 Discussion of Results
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结果讨论
- en: 'Our main results are shown in [Figure 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels") and [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")
    (please also see [Figure 8](#A1.F8 "In A.1 Quantizing Layers Using 3 Levels ‣
    Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels"), [9](#A1.F9 "Figure 9 ‣ A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"), and [10](#A1.F10
    "Figure 10 ‣ A.1 Quantizing Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels") in Appendix). In both these figures, we ranked the layers in the
    respective LLMs in descending order of their importance score. The left most point
    in the plots indicates that all layers of Mistral-7b and LLaMa2-7B are quantized
    in 4-bits; as we move to the right on x-axis, we quantize the next least important
    layer to 2 bits. For example, the overall bit size of 3.75 mentioned on the x-axis
    represents 28 (most important) layers in 4-bit and 4 (least important) layers
    in 2-bit. The horizontal red dotted lines indicate the performance of the entire
    model represented in 8bit precision; as shown, this performs very similarly to
    the full model in 4 bit precision (the top left most point in [Figure 2](#S1.F2
    "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") and [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels")). Because the gap between the full model in 8 bit
    vs. 4 bit precision was less than 1% across the majority of the datasets, we focus
    mostly on quantizing below 4-bit precision in our experiments.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的主要结果显示在[图2](#S1.F2 "在1 引言 ‣ 分层量化：一种务实且有效的超越整数位级量化LLMs的方法")和[3](#S1.F3 "图3
    ‣ 1 引言 ‣ 分层量化：一种务实且有效的超越整数位级量化LLMs的方法")中（另请参见[图8](#A1.F8 "在A.1 使用3个级别量化层 ‣ 附录A
    附录 ‣ 分层量化：一种务实且有效的超越整数位级量化LLMs的方法")，[9](#A1.F9 "图9 ‣ A.1 使用3个级别量化层 ‣ 附录A 附录 ‣
    分层量化：一种务实且有效的超越整数位级量化LLMs的方法")，和[10](#A1.F10 "图10 ‣ A.1 使用3个级别量化层 ‣ 附录A 附录 ‣ 分层量化：一种务实且有效的超越整数位级量化LLMs的方法")在附录中）。在这些图中，我们按重要性评分的降序排列了相应LLMs中的层。图中的最左点表示Mistral-7b和LLaMa2-7B的所有层都量化为4位；随着x轴向右移动，我们将下一个不太重要的层量化为2位。例如，x轴上提到的整体比特大小3.75表示28个（最重要）层为4位，4个（最不重要）层为2位。水平红色虚线表示8位精度下整个模型的性能；如图所示，这与4位精度下的完整模型（[图2](#S1.F2
    "在1 引言 ‣ 分层量化：一种务实且有效的超越整数位级量化LLMs的方法")和[3](#S1.F3 "图3 ‣ 1 引言 ‣ 分层量化：一种务实且有效的超越整数位级量化LLMs的方法")中的最左点）非常相似。由于在大多数数据集上8位与4位精度之间的差距小于1%，我们在实验中主要关注低于4位精度的量化。
- en: 4 to 2 bit 8 to 4 bit Models Layers GSM8K NQ_open GSM8K NQ_open 2 bits F1 F1
    LIM Ordering LLama-Ins 5 7.5 15.0 10.5 36.6 10 1.5 6.7 13.5 37.9 15 0.5 3.1 11.0
    35.4 Mistral-Ins 5 24.5 16.2 34.5 29.6 10 20.0 9.2 36.5 29.8 15 4.5 5.1 34.0 27.7
    Z Ordering LLama-Ins 5 6.0 11.7 12.5 37.3 10 3.5 5.3 12.5 36.9 15 1.0 1.8 10.0
    34.7 Mistral-Ins 5 25.0 15.4 37.0 28.5 10 10.5 8.7 34.5 30.1 15 1.0 2.2 34.0 29.3
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 4 到 2 位 8 到 4 位 模型 层 GSM8K NQ_open GSM8K NQ_open 2 位 F1 F1 LIM 排序 LLama-Ins
    5 7.5 15.0 10.5 36.6 10 1.5 6.7 13.5 37.9 15 0.5 3.1 11.0 35.4 Mistral-Ins 5 24.5
    16.2 34.5 29.6 10 20.0 9.2 36.5 29.8 15 4.5 5.1 34.0 27.7 Z 排序 LLama-Ins 5 6.0
    11.7 12.5 37.3 10 3.5 5.3 12.5 36.9 15 1.0 1.8 10.0 34.7 Mistral-Ins 5 25.0 15.4
    37.0 28.5 10 10.5 8.7 34.5 30.1 15 1.0 2.2 34.0 29.3
- en: 'Table 3: Performance comparison of LLaMa-instruct-7b and Mistral-instruct-7b
    across two generation tasks - GSM8K and Natural Questions open split.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：LLaMa-instruct-7b 和 Mistral-instruct-7b 在两种生成任务 - GSM8K 和自然问题开放拆分的性能比较。
- en: 'We draw the following observations from our experiments:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从实验中得出了以下观察结果：
- en: (1)
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Variable quantization is useful: The first key finding of our work is that
    a fixed quantization technique can be extended to a variable number of bits by
    quantizing different layers at different bits according to their importance. This
    allows LLMs to be fit in the exact memory requirement of the user while retaining
    more of the original performance. Overall, our method of layer-wise quantization,
    guided by layer importance, proves to be an efficient strategy for attaining adaptable
    precision bit levels.'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可变量化是有用的：我们工作的第一个关键发现是，固定量化技术可以通过根据层的重要性在不同位数上量化不同层来扩展为可变位数。这使得 LLM 可以精确适应用户的内存需求，同时保持更多的原始性能。总体而言，我们的分层量化方法，在层重要性的指导下，被证明是一种有效的策略，用于实现适应性精度位级。
- en: (2)
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Layer importance scoring is crucial: In the figures we compare layer ranking
    using our LIM and ZD importance scores with ranking using a reverse LIM and random
    ordering. As seen in [fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")
    and [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels") the quantization
    of least important layers from 4-bit to 2-bit as per the LIM score ranking shows
    strong performance retention. In contrast, quantizing based on the reverse of
    LIM score shows much worse performance when most important layers are quantized
    to 2-bit, highlighting the strength of meaningful layer ranking. Further, LIM
    and ZD ranking performs substantially better than random ordering of layers baseline
    where we quantize layers to lower bits randomly. Lastly, LIM performs better than
    ZD, but the differences are not large. We consider this a success for ZD, which
    is simpler and does not require calibration data.'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层重要性评分至关重要：在图中，我们使用 LIM 和 ZD 重要性评分与使用反向 LIM 和随机排序的层级排名进行比较。如 [图 2](#S1.F2 "在
    1 引言 ‣ 分层量化：量化 LLM 超越整数位级别的务实有效方法") 和 [3](#S1.F3 "图 3 ‣ 1 引言 ‣ 分层量化：量化 LLM 超越整数位级别的务实有效方法")
    所示，根据 LIM 评分排名将最不重要的层从 4 位量化到 2 位表现出强劲的性能保持。相比之下，基于 LIM 评分的反向量化在将最重要的层量化为 2 位时表现显著较差，突显了有意义的层级排名的优势。此外，LIM
    和 ZD 排名比随机排序基线（即随机将层量化为较低位数）表现更好。最后，LIM 的表现优于 ZD，但差异不大。我们认为这对 ZD 是一种成功，因为 ZD 更简单且不需要校准数据。
- en: (3)
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Layer-wise Quantization is useful until 3.0-3.25 bits: As shown in [fig. 2](#S1.F2
    "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") and [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels"), our first key observation is that quantization to
    8 bit barely affects performance (red vs. purple line in each plot). While there
    is marginal drop in performance when models are quantized to 4 bits, we observed
    really noticeable drops only after the models are dropped below 3.0-3.25 bits
    on average using Quanto. For example, the bit size for which performance drops
    below 90% on Winogrande for Mistral-7b, LLaMa-7b, QWEN-7b, and LLaMa2-13b are
    3.2, 3.1, 3.85, and 2.85 respectively.'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层级量化在3.0-3.25位之前是有效的：如[图2](#S1.F2 "在1 引言 ‣ 层级量化：超越整数位水平的量化LLMs的务实有效方法")和[3](#S1.F3
    "图3 ‣ 1 引言 ‣ 层级量化：超越整数位水平的量化LLMs的务实有效方法")所示，我们的第一个关键观察是8位量化几乎不影响性能（每个图中的红线与紫线比较）。虽然当模型量化到4位时，性能略有下降，但我们发现只有在模型的平均量化位数低于3.0-3.25位时，性能才会出现真正显著的下降。例如，对于Mistral-7b、LLaMa-7b、QWEN-7b和LLaMa2-13b，在Winogrande上的性能下降低于90%的位数分别为3.2、3.1、3.85和2.85。
- en: (a) Quantizing from 8 to 4 bits against pruning
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （a）从8位量化到4位与剪枝比较
- en: '![Refer to caption](img/1d96745458d1ad0a760637b7d3304c63.png)'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考说明](img/1d96745458d1ad0a760637b7d3304c63.png)'
- en: (b) Quantizing from 4 to 2 bits against pruning
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: （b）从4位量化到2位与剪枝比较
- en: '![Refer to caption](img/0ca2cbea53bcc4b3e6c960c2815595e3.png)'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考说明](img/0ca2cbea53bcc4b3e6c960c2815595e3.png)'
- en: 'Figure 4: We compare quantization against pruning as a method to reduce the
    memory requirement of a model (LLaMa-2-7b here). One increment means two layers
    moved to lower quantization for the blue line (quantization), and one layer removed
    for the red and orange lines (pruning), thus reducing the same amount of memory.
    We show the average accuracy over MMLU, Winogrande, PIQA, and Hellaswag.'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4：我们将量化与剪枝作为减少模型（这里以LLaMa-2-7b为例）内存需求的方法进行比较。对于蓝线（量化），一个增量表示将两个层移动到更低的量化水平，而对于红线和橙线（剪枝），一个层被移除，从而减少相同量的内存。我们展示了MMLU、Winogrande、PIQA和Hellaswag上的平均准确率。
- en: '![Refer to caption](img/8bf6f6a141e8b255b93ed2c3b4ffa1f1.png)'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参考说明](img/8bf6f6a141e8b255b93ed2c3b4ffa1f1.png)'
- en: 'Figure 5: Comparison of LLaMa2-7b quantized between 8 and 4 bits with LLaMa2-13b
    quantized between 4 and 2 bits to check when the performance intersects'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5：比较LLaMa2-7b在8位和4位之间量化的性能与LLaMa2-13b在4位和2位之间量化的性能，以检查何时性能交叉
- en: (4)
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （4）
- en: 'Quantization is more useful for larger LLMs: Variable quantization of larger
    models ([fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")) using our
    importance score shows much better retention of performance with lower quantization
    bit precision compared to moderately-sized LLMs such as LLaMa-7B ([fig. 8](#A1.F8
    "In A.1 Quantizing Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")
    in appendix) and Mistral-7B ([fig. 3](#S1.F3 "In 1 Introduction ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")).
    Further, as we go down to even smaller LLMs such as QWEN-1.8B (see [fig. 10](#A1.F10
    "In A.1 Quantizing Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")
    in Appendix) with only 20 layers, we observed layer importance ranking to be not
    as effective. This observation aligns with many other previous works that have
    shown quantization to be substantially more effective for larger LLMs when compared
    to their smaller counterparts Jin et al. ([2024](#bib.bib15)).'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于较大的LLMs，量化更有用：使用我们的重要性分数对较大模型进行的可变量化（[图2](#S1.F2 "在1 引言 ‣ 层级量化：超越整数位水平的量化LLMs的务实有效方法")）显示，与中等规模的LLMs（如LLaMa-7B（[图8](#A1.F8
    "在A.1 使用3个级别量化层 ‣ 附录A 附录 ‣ 层级量化：超越整数位水平的量化LLMs的务实有效方法")在附录中）和Mistral-7B（[图3](#S1.F3
    "在1 引言 ‣ 层级量化：超越整数位水平的量化LLMs的务实有效方法")）相比，性能在低量化位精度下的保留要好得多。此外，当我们进入更小的LLMs，如QWEN-1.8B（见[图10](#A1.F10
    "在A.1 使用3个级别量化层 ‣ 附录A 附录 ‣ 层级量化：超越整数位水平的量化LLMs的务实有效方法")在附录中），只有20层时，我们观察到层级重要性排名不如预期有效。这一观察结果与许多先前的研究一致，这些研究表明，相较于较小的对应模型，量化对于较大的LLMs要显著更有效
    Jin et al. ([2024](#bib.bib15))。
- en: (5)
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （5）
- en: 'Our method is applicable to different quantization techniques: Tables [1](#S3.T1
    "Table 1 ‣ 3.2.1 Quanto ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels") and [2](#S3.T2 "Table 2 ‣ 3.2.1 Quanto ‣ 3.2 Quantization Techniques
    ‣ 3 Method ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing
    LLMs Beyond Integer Bit-Levels") summarize the overall results when quantizing
    individual layers with Quanto and GPT-Q, respectively. The tables show that our
    method can be coupled with any other quantization techniques. On average, GPT-Q
    leads to an average of 4% better accuracy than Quanto across all 5 tasks. Additionally,
    GPT-Q enables models such as LLaMa2-13B to be quantized down from 4 bits to 3.25
    bits with less than a 3% loss in average accuracy, as seen in Table [2](#S3.T2
    "Table 2 ‣ 3.2.1 Quanto ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels"). Importantly, similar to Quanto and GPT-Q, our methodology of layer-wise
    quantization can be extended to any quantization technique.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的方法适用于不同的量化技术：[表 1](#S3.T1 "表 1 ‣ 3.2.1 Quanto ‣ 3.2 量化技术 ‣ 3 方法 ‣ 层级量化：一种务实且有效的
    LLM 量化方法") 和 [表 2](#S3.T2 "表 2 ‣ 3.2.1 Quanto ‣ 3.2 量化技术 ‣ 3 方法 ‣ 层级量化：一种务实且有效的
    LLM 量化方法") 总结了分别使用 Quanto 和 GPT-Q 对单独层进行量化时的整体结果。这些表格显示，我们的方法可以与任何其他量化技术结合使用。平均而言，GPT-Q
    在所有 5 个任务中比 Quanto 高出 4% 的准确率。此外，GPT-Q 使得 LLaMa2-13B 等模型能够从 4 位量化到 3.25 位，平均准确率损失不到
    3%，如[表 2](#S3.T2 "表 2 ‣ 3.2.1 Quanto ‣ 3.2 量化技术 ‣ 3 方法 ‣ 层级量化：一种务实且有效的 LLM 量化方法")所示。重要的是，与
    Quanto 和 GPT-Q 类似，我们的层级量化方法可以扩展到任何量化技术。
- en: (6)
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (6)
- en: 'Effect on generation tasks: — We also evaluate our approach of variable quantization
    of different layers on generation tasks. As shown in [table 3](#S5.T3 "In 5 Discussion
    of Results ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing
    LLMs Beyond Integer Bit-Levels"), we observe substantial drop in performance on
    both GSM8K and NQ_open generation tasks when quantizing more layers in 2 bits.
    Importantly, the performance drop in these generation tasks is more drastic when
    compared to the average performance drop in classification tasks ([table 1](#S3.T1
    "In 3.2.1 Quanto ‣ 3.2 Quantization Techniques ‣ 3 Method ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels")),
    emphasizing the need for more dedicated research in quantization for generation
    tasks.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对生成任务的影响：— 我们还评估了在生成任务中对不同层进行变量量化的方法。如[表 3](#S5.T3 "在 5 结果讨论 ‣ 层级量化：一种务实且有效的
    LLM 量化方法")所示，我们观察到在将更多层量化为 2 位时，GSM8K 和 NQ_open 生成任务的性能大幅下降。重要的是，与分类任务的平均性能下降相比，这些生成任务的性能下降更为剧烈（见[表
    1](#S3.T1 "在 3.2.1 Quanto ‣ 3.2 量化技术 ‣ 3 方法 ‣ 层级量化：一种务实且有效的 LLM 量化方法")），这强调了生成任务量化需要更多专门的研究。
- en: 6 Analyses
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 分析
- en: We present several analyses to further spotlight on benefits from variable layer-wise
    quantization.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了几个分析，以进一步凸显变量层级量化的好处。
- en: 6.1 Pruning vs. Quantization
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 剪枝与量化
- en: 'We compare our variable quantization against variable pruning (using the same
    layer importance ranking) as an alternative for the same goal of reducing model
    memory requirement. In [Figure 4(a)](#S5.F4.sf1 "In Figure 4 ‣ Item (3) ‣ 5 Discussion
    of Results ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing
    LLMs Beyond Integer Bit-Levels"), we show that for not extreme quantization levels,
    it is significantly better to move layers into lower quantization levels instead
    of removing them. For example, when 2 least important layers are removed resulting
    in remaining 30 layers (each in 8 bit) of LLaMa2-7b, the average performance drops
    to 62.7% (shown by red line denoting work by Gromov et al. ([2024](#bib.bib10))).
    But on the quantization counterpart with same memory i.e., when 4 layers are quantized
    to 4 bit and remaining 28 layers are in 8 bit (shown by blue line), performance
    remains intact close to 66.8% as shown in [fig. 4(a)](#S5.F4.sf1 "In Figure 4
    ‣ Item (3) ‣ 5 Discussion of Results ‣ Layer-Wise Quantization: A Pragmatic and
    Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). When 12 layers
    are removed, the performance drops around 53% on average while having 8 layers
    in 8 bits and 24 layers quantized to 4 bits (shown by blue curve) to maintain
    the same size, average performance still remains intact and close to 66%. This
    highlights the important finding that quantization until 4 bits overall is a substantially
    more effective strategy compared to pruning for model compression.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的变量量化方法与变量修剪方法（使用相同的层重要性排序）进行了比较，作为减少模型内存需求的替代方案。在[图4(a)](#S5.F4.sf1 "在图4
    ‣ 项目(3) ‣ 5 结果讨论 ‣ 层级量化：一种务实且有效的量化LLMs方法")中，我们展示了对于非极端量化水平，将层移动到较低量化水平明显优于删除它们。例如，当删除2个最不重要的层后，剩下30层（每层为8位）的LLaMa2-7b的平均性能下降到62.7%（由红线表示，工作由Gromov等人([2024](#bib.bib10)))。但在量化情况下，i.e.，当4层被量化为4位，剩余28层为8位（由蓝线表示），性能保持在接近66.8%的水平，如[图4(a)](#S5.F4.sf1
    "在图4 ‣ 项目(3) ‣ 5 结果讨论 ‣ 层级量化：一种务实且有效的量化LLMs方法")所示。当删除12层时，性能在平均值上降到约53%，而8层为8位，24层量化为4位（由蓝色曲线表示）以保持相同的大小，平均性能仍保持在接近66%。这突出了一个重要发现：总体上，量化到4位是一种比修剪更有效的模型压缩策略。
- en: On the other hand, in case of extreme levels of quantization (i.e., $<4-bits$,
    pruning maybe the more effective strategy.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在极端量化水平（即，$<4-bit$）的情况下，修剪可能是更有效的策略。
- en: 6.2 Quantizing Larger vs. Smaller LLMs
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 大型与小型LLMs的量化
- en: We further evaluate and compare feasibility of quantizing larger LLMs more drastically
    (i.e., $<4bits$) shows better performance.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步评估和比较了对更大LLMs进行更剧烈的量化（即，$<4bits$）的可行性，结果显示性能更好。
- en: 7 Conclusion
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: We introduced a simple, flexible quantization approach that quantizes different
    layers at different bits based on their importance. We presented two layer importance
    scoring techniques which when used to select more important layers for quantizing
    them in 4 bits and less important layers in 2 bits lead to strong performance
    retention across several LLMs even until 2.85 overall bit size. Our work presents
    several key practical findings such as layer-wise quantization is more effective
    for larger LLMs (with more number of layers), in the same memory setting; quantization
    is better than pruning until a certain bit precision level, text generation tasks
    are affected more with quantization, etc. Overall, our work introduces layer-wise
    quantization and presents detailed empirical findings for motivating future research
    in this direction.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种简单灵活的量化方法，根据层的重要性对不同层进行不同位数的量化。我们介绍了两种层重要性评分技术，当用于选择更重要的层进行4位量化，而将较不重要的层进行2位量化时，能够在多个LLMs中保持较强的性能，即使在2.85的总体位数下也是如此。我们的工作提出了几个关键的实际发现，例如，在相同内存设置下，层级量化对较大的LLMs（层数更多）更为有效；量化在某些位精度水平下优于修剪；文本生成任务更容易受到量化的影响，等等。总体而言，我们的工作介绍了层级量化，并提供了详细的实证发现，以激励未来在这一方向上的研究。
- en: 8 Limitations
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 局限性
- en: 'Limitations of our work are as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的局限性如下：
- en: •
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Our experiments and results have focused more on quantization to lower bits
    i.e., $<4bits$ 1% on average) in performance between 8-bit and 4-bit quantized
    model (please see [table 4](#A1.T4 "In A.1 Quantizing Layers Using 3 Levels ‣
    Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") in appendix). We did not observe
    meaningful changes in performance from layer-wise quantization between 8-bit and
    4-bit because of such minute difference between their performances. Our study
    is limited to two level quantization i.e., more important layers in 4 bits and
    less important layers in 2 bits. One can also potentially apply three level of
    quantization i.e., most important layers in 8 bits, moderately important in 4
    bits and least important in 2 bits. In our experiments, we observed three level
    quantization to be always worse than two level quantization for similar memory
    sizes (please see [section A.1](#A1.SS1 "A.1 Quantizing Layers Using 3 Levels
    ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") in Appendix). .'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的实验和结果更多集中在将量化降至更低位数，即8位和4位量化模型之间的性能差异（请参见[表4](#A1.T4 "在A.1节 使用3级量化的层 ‣ 附录A
    附录 ‣ 层级量化：量化LLM超越整数位级的务实有效方法")）。我们没有观察到8位和4位层级量化之间的性能有显著变化，因为它们之间的性能差异微小。我们的研究局限于两级量化，即将更重要的层量化为4位，将不太重要的层量化为2位。也可以潜在地应用三级量化，即最重要的层为8位，中等重要的层为4位，最不重要的层为2位。在我们的实验中，我们发现对于类似的内存大小，三级量化总是比两级量化效果更差（请参见[附录A.1节](#A1.SS1
    "A.1 使用3级量化的层 ‣ 附录A 附录 ‣ 层级量化：量化LLM超越整数位级的务实有效方法")）。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We have proposed LIM and ZD scores for rating importance of each layer in LLM.
    These scores are very simple to implement, and only LIM needs a calibration corpus
    which is also an (readily available) unlabelled corpus. As future works, there
    can be more sophisticated measures to find importance of layers. For example,
    supervised methods that can show layer-wise impact on specific annotated NLP tasks
    Zhu et al. ([2023a](#bib.bib39)), in depth model interpretability approaches Singh
    et al. ([2024](#bib.bib25)); Sun et al. ([2023](#bib.bib26)), analyses of block
    of layers and their interactionsYang et al. ([2024](#bib.bib34)), etc. In this
    work, we limited and focused our study on LIM and ZD score to emphasize more on
    the main contribution of the paper - quantizing different layers at different
    bits as per layer importance. To the best of our knowledge, our work is the first
    to introduce variable quantization layer wise as per their importance and our
    study can be easily extended with more layer importance measuring scores in future
    work.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了LIM和ZD评分来评估LLM中每一层的重要性。这些评分非常容易实现，只有LIM需要一个校准语料库，该语料库也是一个（现成的）未标记语料库。作为未来工作，可以有更复杂的措施来评估层的重要性。例如，能够显示层级影响的监督方法，特定标注NLP任务的Zhu等（[2023a](#bib.bib39)）；深入的模型可解释性方法Singh等（[2024](#bib.bib25)）；Sun等（[2023](#bib.bib26)）；层块及其交互的分析Yang等（[2024](#bib.bib34)）等。在这项工作中，我们将研究限制并集中在LIM和ZD评分上，以更好地强调本文的主要贡献——根据层的重要性对不同层进行不同位数的量化。据我们所知，我们的工作是首次引入根据层的重要性进行的可变量化，我们的研究可以在未来的工作中很容易扩展到更多的层重要性测量评分。
- en: •
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'We presented comparison between pruning and quantization in [section 6.1](#S6.SS1
    "6.1 Pruning vs. Quantization ‣ 6 Analyses ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). A potential
    setup can also be combining pruning and quantization by removing few of the least
    important layers, quantizing remaining less important layers in lower 2 bits and
    keeping more important layers in 8 bits or above. Our work is primarily focused
    on introducing the idea of achieving variable quantization by quantizing different
    layers as per their importance. We leave exploration of combining quantization
    and pruning for future works.'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在[6.1节](#S6.SS1 "6.1 剪枝与量化的比较 ‣ 6 分析 ‣ 层级量化：量化LLM超越整数位级的务实有效方法")中展示了剪枝和量化之间的比较。一个可能的方案是通过移除少数不重要的层，将剩余的不重要层量化为较低的2位，将更重要的层保持在8位或更高。我们的工作主要集中在引入通过根据层的重要性进行量化以实现可变量化的概念。我们将剪枝和量化的结合探索留待未来的工作。
- en: 9 Ethical Considerations
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 伦理考虑
- en: Our study is focused on post training quantization (PTQ) of LLMs. We have not
    finetuned LLMs for any specific task or data and have selected well established
    LLMs such as LLaMa, Mistral, and QWEN in our experiments. Our study also uses
    widely used evaluation datsets such as MMLU, HellaSwag, ARC easy, etc. that contain
    safe test cases. Thus, we believe our experiments do not contain any harmful cases
    and to the best of our knowledge, we did not observe any unsafe outputs in our
    evaluations.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的研究集中在LLMs的后训练量化（PTQ）上。我们没有对LLMs进行特定任务或数据的微调，而是在实验中选择了已建立的LLMs，如LLaMa、Mistral和QWEN。我们的研究还使用了广泛使用的评估数据集，如MMLU、HellaSwag、ARC
    easy等，这些数据集包含安全的测试用例。因此，我们相信我们的实验不包含任何有害的案例，并且据我们所知，在评估中没有观察到任何不安全的输出。
- en: References
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Bai et al. (2023) Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong
    Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang
    Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui
    Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, and Tianhang Zhu. 2023. Qwen technical report. *arXiv preprint arXiv:2309.16609*.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bai等（2023）Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng,
    Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin,
    Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men,
    Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang,
    Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian
    Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang,
    Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan
    Zhou, 和 Tianhang Zhu. 2023. Qwen技术报告。*arXiv预印本 arXiv:2309.16609*。
- en: Banner et al. (2019) Ron Banner, Yury Nahshan, and Daniel Soudry. 2019. Post
    training 4-bit quantization of convolutional networks for rapid-deployment. *Advances
    in Neural Information Processing Systems*, 32.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Banner等（2019）Ron Banner, Yury Nahshan, 和 Daniel Soudry. 2019. 卷积网络的后训练4位量化以实现快速部署。*神经信息处理系统进展*，32。
- en: 'Bisk et al. (2020) Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    2020. Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk等（2020）Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi等. 2020. Piqa:
    以自然语言推理物理常识。见 *AAAI人工智能会议论文集*，第34卷，页7432–7439。'
- en: Clark et al. (2018) Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved
    question answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark等（2018）Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, 和 Oyvind Tafjord. 2018. 认为你解决了问答问题？试试arc，AI2推理挑战。*arXiv预印本
    arXiv:1803.05457*。
- en: Cobbe et al. (2021) Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen,
    Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro
    Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve
    math word problems. *arXiv preprint arXiv:2110.14168*.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe等（2021）Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo
    Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,
    Christopher Hesse, 和 John Schulman. 2021. 训练验证器以解决数学词问题。*arXiv预印本 arXiv:2110.14168*。
- en: 'Dettmers and Zettlemoyer (2023) Tim Dettmers and Luke Zettlemoyer. 2023. The
    case for 4-bit precision: k-bit inference scaling laws. In *International Conference
    on Machine Learning*, pages 7750–7774\. PMLR.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dettmers和Zettlemoyer（2023）Tim Dettmers 和 Luke Zettlemoyer. 2023. 4位精度的案例：k位推理扩展规律。见
    *国际机器学习会议*，页7750–7774\. PMLR。
- en: 'Frantar and Alistarh (2023) Elias Frantar and Dan Alistarh. 2023. Sparsegpt:
    Massive language models can be accurately pruned in one-shot. In *International
    Conference on Machine Learning*, pages 10323–10337\. PMLR.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar和Alistarh（2023）Elias Frantar 和 Dan Alistarh. 2023. Sparsegpt: 大型语言模型可以一次性准确修剪。见
    *国际机器学习会议*，页10323–10337\. PMLR。'
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. 2023. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. In *The Eleventh International Conference on Learning Representations*.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar等（2023）Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh.
    2023. Gptq: 准确的生成预训练变换器的后训练量化。见 *第十一届国际学习表征会议*。'
- en: Gao et al. (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid
    Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. [A framework for few-shot language
    model evaluation](https://doi.org/10.5281/zenodo.10256836).
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2023) Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black,
    Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac’h,
    Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria
    Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish
    Thite, Ben Wang, Kevin Wang, 和 Andy Zou. 2023. [一个少量样本语言模型评估框架](https://doi.org/10.5281/zenodo.10256836).
- en: Gromov et al. (2024) Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo
    Glorioso, and Daniel A Roberts. 2024. The unreasonable ineffectiveness of the
    deeper layers. *arXiv preprint arXiv:2403.17887*.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gromov 等人 (2024) Andrey Gromov, Kushal Tirumala, Hassan Shapourian, Paolo Glorioso,
    和 Daniel A Roberts. 2024. 更深层次的非理性无效性. *arXiv 预印本 arXiv:2403.17887*.
- en: 'Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2023. Minillm:
    Knowledge distillation of large language models. In *The Twelfth International
    Conference on Learning Representations*.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gu 等人 (2023) Yuxian Gu, Li Dong, Furu Wei, 和 Minlie Huang. 2023. Minillm: 大型语言模型的知识蒸馏.
    在 *第十二届国际学习表征会议* 上.'
- en: Hendrycks et al. (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou,
    Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask
    language understanding. In *International Conference on Learning Representations*.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 (2020) Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas
    Mazeika, Dawn Song, 和 Jacob Steinhardt. 2020. 大规模多任务语言理解的测量. 在 *国际学习表征会议* 上.
- en: Jiang et al. (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. *arXiv preprint
    arXiv:2310.06825*.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 (2023) Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris
    Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna
    Lengyel, Guillaume Lample, Lucile Saulnier, 等. 2023. Mistral 7b. *arXiv 预印本 arXiv:2310.06825*.
- en: Jiang et al. (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. *arXiv preprint
    arXiv:2401.04088*.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等人 (2024) Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur
    Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas,
    Emma Bou Hanna, Florian Bressand, 等. 2024. 专家混合模型. *arXiv 预印本 arXiv:2401.04088*.
- en: Jin et al. (2024) Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan,
    Bin Wang, and Deyi Xiong. 2024. A comprehensive evaluation of quantization strategies
    for large language models. *arXiv preprint arXiv:2402.16775*.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jin 等人 (2024) Renren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin
    Wang, 和 Deyi Xiong. 2024. 大型语言模型量化策略的全面评估. *arXiv 预印本 arXiv:2402.16775*.
- en: 'Kwiatkowski et al. (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question
    answering research. *Transactions of the Association for Computational Linguistics*,
    7:453–466.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kwiatkowski 等人 (2019) Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
    Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin,
    Jacob Devlin, Kenton Lee, 等. 2019. 自然问题：问题回答研究的基准. *计算语言学学会会刊*, 7:453–466.
- en: 'Lin et al. (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming
    Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024.
    Awq: Activation-aware weight quantization for on-device llm compression and acceleration.
    *Proceedings of Machine Learning and Systems*, 6:87–100.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等人 (2024) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen,
    Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, 和 Song Han. 2024. Awq:
    针对设备上的 LLM 压缩与加速的激活感知权重量化. *机器学习与系统会议录*, 6:87–100.'
- en: 'Liu et al. (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra.
    2023. Llm-qat: Data-free quantization aware training for large language models.
    *arXiv preprint arXiv:2305.17888*.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等人 (2023) Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre
    Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra.
    2023. Llm-qat: 大型语言模型的数据无关量化感知训练. *arXiv 预印本 arXiv:2305.17888*.'
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner:
    On the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma 等人 (2023) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. 2023. Llm-pruner: 关于大型语言模型的结构化剪枝.
    *神经信息处理系统进展*, 36:21702–21720.'
- en: 'Men et al. (2024) Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin,
    Yaojie Lu, Xianpei Han, and Weipeng Chen. 2024. Shortgpt: Layers in large language
    models are more redundant than you expect. *arXiv preprint arXiv:2403.03853*.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Men 等（2024）Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie
    Lu, Xianpei Han 和 Weipeng Chen. 2024年。《Shortgpt：大型语言模型中的层比你预期的更冗余》。*arXiv 预印本
    arXiv:2403.03853*。
- en: Rae et al. (2019) Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier,
    and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence
    modelling. In *International Conference on Learning Representations*.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rae 等（2019）Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier 和
    Timothy P Lillicrap. 2019年。《用于长序列建模的压缩变换器》。发表于 *学习表示国际会议*。
- en: 'Sakaguchi et al. (2021) Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at
    scale. *Communications of the ACM*, 64(9):99–106.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaguchi 等（2021）Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula 和 Yejin
    Choi. 2021年。《Winogrande：大规模对抗性 Winograd 方案挑战》。*ACM 通讯*，64（9）：99–106。
- en: 'Shen et al. (2020) Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao,
    Amir Gholami, Michael W Mahoney, and Kurt Keutzer. 2020. Q-bert: Hessian based
    ultra low precision quantization of bert. In *Proceedings of the AAAI Conference
    on Artificial Intelligence*, volume 34, pages 8815–8821.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等（2020）Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei Yao, Amir Gholami,
    Michael W Mahoney 和 Kurt Keutzer. 2020年。《Q-bert：基于 Hessian 的 BERT 超低精度量化》。发表于
    *AAAI 人工智能会议论文集*，第34卷，第8815–8821页。
- en: 'Simoulin and Crabbé (2021) Antoine Simoulin and Benoit Crabbé. 2021. How many
    layers and why? an analysis of the model depth in transformers. In *Proceedings
    of the 59th Annual Meeting of the Association for Computational Linguistics and
    the 11th International Joint Conference on Natural Language Processing: Student
    Research Workshop*, pages 221–228.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simoulin 和 Crabbé（2021）Antoine Simoulin 和 Benoit Crabbé. 2021年。《层数有多少以及为什么？对变换器模型深度的分析》。发表于
    *第59届计算语言学协会年会和第11届国际自然语言处理联合会议：学生研究研讨会*，第221–228页。
- en: Singh et al. (2024) Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich
    Caruana, and Jianfeng Gao. 2024. Rethinking interpretability in the era of large
    language models. *arXiv preprint arXiv:2402.01761*.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Singh 等（2024）Chandan Singh, Jeevana Priya Inala, Michel Galley, Rich Caruana
    和 Jianfeng Gao. 2024年。《在大型语言模型时代重新思考可解释性》。*arXiv 预印本 arXiv:2402.01761*。
- en: Sun et al. (2023) Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2023.
    A simple and effective pruning approach for large language models. In *The Twelfth
    International Conference on Learning Representations*.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等（2023）Mingjie Sun, Zhuang Liu, Anna Bair 和 J Zico Kolter. 2023年。《一种简单有效的大型语言模型剪枝方法》。发表于
    *第十二届学习表示国际会议*。
- en: 'Tai et al. (2024) Yu-Shan Tai et al. 2024. Mptq-vit: Mixed-precisionpost-trainingquantizationforvisiontransformer.
    *arXiv preprint arXiv:2401.14895*.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tai 等（2024）Yu-Shan Tai 等. 2024年。《Mptq-vit：用于视觉变换器的混合精度后训练量化》。*arXiv 预印本 arXiv:2401.14895*。
- en: 'Team et al. (2023) Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models.
    *arXiv preprint arXiv:2312.11805*.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Team 等（2023）Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste
    Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth 等.
    2023年。《Gemini：一系列高度能力的多模态模型》。*arXiv 预印本 arXiv:2312.11805*。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等（2023）Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar 等. 2023年。《Llama：开放和高效的基础语言模型》。*arXiv 预印本 arXiv:2302.13971*。
- en: Vaswani et al. (2017) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention
    is all you need. *Advances in neural information processing systems*, 30.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion
    Jones, Aidan N Gomez, Łukasz Kaiser 和 Illia Polosukhin. 2017年。《注意力即一切》。*神经信息处理系统进展*，30。
- en: 'Wan et al. (2023) Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan
    Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. 2023. Efficient
    large language models: A survey. *arXiv preprint arXiv:2312.03863*.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wan 等（2023）Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan
    Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury 等. 2023年。《高效的大型语言模型：综述》。*arXiv
    预印本 arXiv:2312.03863*。
- en: 'Wolf et al. (2019) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,
    et al. 2019. Huggingface’s transformers: State-of-the-art natural language processing.
    *arXiv preprint arXiv:1910.03771*.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf 等人（2019）Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz
    等. 2019. Huggingface 的变换器：先进的自然语言处理。*arXiv 预印本 arXiv:1910.03771*。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization
    for large language models. In *International Conference on Machine Learning*,
    pages 38087–38099\. PMLR.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等人（2023）Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth
    和 Song Han. 2023. Smoothquant: 精确高效的大语言模型后训练量化。在 *国际机器学习会议*，页码 38087–38099\. PMLR。'
- en: 'Yang et al. (2024) Yifei Yang, Zouying Cao, and Hai Zhao. 2024. Laco: Large
    language model pruning via layer collapse. *arXiv preprint arXiv:2402.11187*.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang 等人（2024）Yifei Yang, Zouying Cao 和 Hai Zhao. 2024. Laco: 通过层崩溃进行大语言模型修剪。*arXiv
    预印本 arXiv:2402.11187*。'
- en: Yao et al. (2024) Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong
    He. 2024. Exploring post-training quantization in llms from comprehensive study
    to low rank compensation. In *Proceedings of the AAAI Conference on Artificial
    Intelligence*, volume 38, pages 19377–19385.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao 等人（2024）Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn 和 Yuxiong He. 2024.
    从综合研究到低秩补偿，探索 LLM 的后训练量化。在 *AAAI 人工智能会议论文集*，第 38 卷，页码 19377–19385。
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. 2022. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. *Advances in Neural Information Processing
    Systems*, 35:27168–27183.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等人（2022）Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li 和 Yuxiong He. 2022. Zeroquant: 高效且经济的大规模变换器后训练量化。*神经信息处理系统进展*，35:27168–27183。'
- en: 'Zellers et al. (2019) Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In
    *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*,
    pages 4791–4800.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等人（2019）Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi 和 Yejin
    Choi. 2019. Hellaswag: 机器真的能完成你的句子吗？在 *第57届计算语言学协会年会论文集*，页码 4791–4800。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. 2022. Opt: Open pre-trained transformer language models. *arXiv preprint
    arXiv:2205.01068*.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等人（2022）Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya
    Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin 等.
    2022. Opt: 开放预训练变换器语言模型。*arXiv 预印本 arXiv:2205.01068*。'
- en: 'Zhu et al. (2023a) Ligeng Zhu, Lanxiang Hu, Ji Lin, and Song Han. 2023a. Lift:
    Efficient layer-wise fine-tuning for large model models.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhu 等人（2023a）Ligeng Zhu, Lanxiang Hu, Ji Lin 和 Song Han. 2023a. Lift: 高效的层级微调大模型。'
- en: Zhu et al. (2023b) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023b.
    A survey on model compression for large language models. *arXiv preprint arXiv:2308.07633*.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人（2023b）Xunyu Zhu, Jian Li, Yong Liu, Can Ma 和 Weiping Wang. 2023b. 大语言模型的模型压缩综述。*arXiv
    预印本 arXiv:2308.07633*。
- en: Appendix A Appendix
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: '![Refer to caption](img/f448b05f1d9124578ec1976f5adda595.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f448b05f1d9124578ec1976f5adda595.png)'
- en: 'Figure 6: We first rank the layers in an LLM (e.g., LLaMa2-13B) using an importance
    score (shown here is ranking based on our Layer Input Modification (LIM) score,
    see [section 3.1](#S3.SS1 "3.1 Layer Importance Scores ‣ 3 Method ‣ Layer-Wise
    Quantization: A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer
    Bit-Levels")).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 我们首先使用重要性分数对 LLM（例如 LLaMa2-13B）中的层进行排名（这里显示的是基于我们的层输入修改（LIM）分数的排名，见 [第
    3.1 节](#S3.SS1 "3.1 层重要性分数 ‣ 3 方法 ‣ 按层量化：超越整数位级的实用有效方法")）。'
- en: A.1 Quantizing Layers Using 3 Levels
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 使用 3 个级别进行层量化
- en: 'In all of our experiments, we have focused on two level quantization, i.e.,
    either less important layers are quantized in 2 bits and more important layers
    in 4 bits or less important in 4 bits and more important in 8 bits. As a plausible
    variant, LLM layers can also be easily quantized using three levels, i.e., least
    important layers in 2 bits, moderately important in 4 bits, and the most important
    ones in 8 bits. In our study, we observed three level quantization almost always
    performs worse than two level quantization. We show three level quantization of
    LLaMa2-7b with fixed overall model bit size of 4bit. We first quantize all the
    32 layers in 4 bits as shown by the leftmost bar of [section A.1](#A1.SS1 "A.1
    Quantizing Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization:
    A Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").
    We then convert two least important layers to 2 bits each and one most important
    layer to 8 bit, thus maintaining the overall bit size of the model to 4 bit. This
    is represented by second bar from the left in [fig. 7](#A1.F7 "In A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). We repeat
    the same process of converting two more layers in 2bits and a more important layer
    to 8bit represented by the consecutive bars in [fig. 7](#A1.F7 "In A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"). As observed,
    three level quantization always performs worse than one level quantization when
    the target bit-level is the same, thus we don’t propose the technique as a way
    to achieve better performance for a set quantization level, but as a way to achieve
    a variable level of quantization while retaining maximum performance.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们的所有实验中，我们专注于两级量化，即：不那么重要的层量化为 2 位，更重要的层量化为 4 位，或不那么重要的层量化为 4 位，更重要的层量化为
    8 位。作为一种可能的变体，LLM 层也可以使用三种级别进行量化，即最不重要的层量化为 2 位，中等重要的层量化为 4 位，最重要的层量化为 8 位。在我们的研究中，我们发现三级量化几乎总是表现不如两级量化。我们展示了在固定整体模型位大小为
    4 位的情况下，LLaMa2-7b 的三级量化。我们首先将所有 32 层量化为 4 位，如 [A.1](#A1.SS1 "A.1 Quantizing Layers
    Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and
    Effective Method for Quantizing LLMs Beyond Integer Bit-Levels") 部分最左边的条形图所示。然后，我们将两个最不重要的层转换为
    2 位，每个转换为 2 位，并将一个最重要的层转换为 8 位，从而保持模型的整体位大小为 4 位。这由 [图 7](#A1.F7 "In A.1 Quantizing
    Layers Using 3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels") 从左边第二个条形图表示。我们重复相同的过程，将两个更多的层转换为
    2 位，并将一个更重要的层转换为 8 位，这由 [图 7](#A1.F7 "In A.1 Quantizing Layers Using 3 Levels
    ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective Method
    for Quantizing LLMs Beyond Integer Bit-Levels") 中的连续条形图表示。如观察到的，三级量化总是表现不如单级量化，当目标位级别相同时，因此我们不建议将该技术作为实现特定量化水平更好性能的方法，而是作为实现可变量化水平并保持最大性能的方法。'
- en: '![Refer to caption](img/7e845cdb555a83ad7203017091e4b9e5.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7e845cdb555a83ad7203017091e4b9e5.png)'
- en: 'Figure 7: We compare different ways to achieve 4-bit quantization using three
    quantization levels. Each bar going from left to right represents adding one important
    layer in 8 bits and moving two less important layers to 2 bits, thus keeping an
    average of 4-bit quantization for all of the bars. Each bar having a value x on
    the x-axis represents the most important x layers in 8-bits, the least important
    2*x in 2-bits and the rest in 4-bits.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：我们比较了使用三种量化级别实现 4 位量化的不同方法。每个从左到右的条形图代表在 8 位中添加一个重要层，并将两个不那么重要的层移动到 2 位，从而保持所有条形图的平均
    4 位量化。每个在 x 轴上有值 x 的条形图表示在 8 位中最重要的 x 层，在 2 位中最不重要的 2*x 层，以及其余的在 4 位中。
- en: '![Refer to caption](img/c86635f436316f9d630a13dc43e2c489.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/c86635f436316f9d630a13dc43e2c489.png)'
- en: 'Figure 8: Similarly to the other bit plots the graphs showcase the accuracy
    on four distinct data sets when quantizing LLaMa2-7b from full 4-bit quantization
    to 2-bit by moving less important layers in 2-bit quantization'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：与其他位图类似，图表展示了在将 LLaMa2-7b 从完整的 4 位量化到 2 位时，在四个不同数据集上的准确性，方法是将不那么重要的层量化为
    2 位。
- en: '![Refer to caption](img/19b0a71d47a001e8b6fd3b442146a90d.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/19b0a71d47a001e8b6fd3b442146a90d.png)'
- en: 'Figure 9: Qwen2 quantized with quanto between 4 and 2 bits. All notations are
    same as in [fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：Qwen2 量化为 4 位和 2 位之间。所有符号与[图 2](#S1.F2 "在 1 引言 ‣ 分层量化：一种务实且有效的超越整数位级别的 LLM
    量化方法")中的相同。
- en: '![Refer to caption](img/37b3363cb069ffbdaf1a5cfb8336229d.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/37b3363cb069ffbdaf1a5cfb8336229d.png)'
- en: 'Figure 10: Qwen1.5-1.8b quantized with quanto between 4 and 2 bits. All notations
    are same as in [fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A
    Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：Qwen1.5-1.8b 量化为 4 位和 2 位之间。所有符号与[图 2](#S1.F2 "在 1 引言 ‣ 分层量化：一种务实且有效的超越整数位级别的
    LLM 量化方法")中的相同。
- en: Models Layers WNGD ARC PIQA HLSWG MMLU Average low-bits Accuracy BI Ordering
    LLaMa2-7B 5 69.06 75.88 77.69 57.05 41.28 64.2 10 68.82 76.3 77.42 57.11 41.96
    64.3 15 68.82 76.3 77.42 57.11 41.96 64.3 Mistral-7B 5 73.87 81.01 80.9 61.13
    58.56 71.1 10 73.95 80.3 80.95 61.28 58.39 71.0 15 73.79 80.42 80.79 61.17 58.14
    70.9 LLaMa2-13B 5 72.29 79.33 79.16 60.15 50.49 68.3 10 71.74 79.08 79.32 59.96
    50.53 68.1 15 71.74 79.37 79.32 59.96 50.55 68.2 Qwen-2-7B 5 70.71 79.2 80.03
    58.66 68.06 71.3 10 70.95 79.2 79.97 58.44 67.68 71.2 15 68.82 78.32 79.81 58.26
    67.66 70.6 Z Ordering LLaMa2-7B 5 68.82 76.13 77.91 57.02 40.85 64.1 10 68.66
    75.92 77.63 57.09 40.6 64.0 15 68.27 75.54 77.42 57.09 39.7 63.6 Mistral-7B 5
    73.87 80.76 80.9 61.24 58.62 71.1 10 74.19 80.47 81.12 61.03 58.2 71.0 15 74.42
    80.47 80.84 60.89 58.31 71.0 LLaMa2-13B 5 72.29 79.54 78.94 60 50.54 68.3 10 72.21
    79.58 79.16 59.99 50.51 68.3 15 72.05 79.46 79.37 59.98 50.59 68.3 Qwen-2-7B 5
    71.58 79.2 79.76 59.2 69.44 71.8 10 71.5 79.08 80.35 58.82 69.01 71.8 15 71.58
    78.61 79.92 58.54 68.65 71.5
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 模型 层数 WNGD ARC PIQA HLSWG MMLU 平均低位准确度 BI 排序 LLaMa2-7B 5 69.06 75.88 77.69 57.05
    41.28 64.2 10 68.82 76.3 77.42 57.11 41.96 64.3 15 68.82 76.3 77.42 57.11 41.96
    64.3 Mistral-7B 5 73.87 81.01 80.9 61.13 58.56 71.1 10 73.95 80.3 80.95 61.28
    58.39 71.0 15 73.79 80.42 80.79 61.17 58.14 70.9 LLaMa2-13B 5 72.29 79.33 79.16
    60.15 50.49 68.3 10 71.74 79.08 79.32 59.96 50.53 68.1 15 71.74 79.37 79.32 59.96
    50.55 68.2 Qwen-2-7B 5 70.71 79.2 80.03 58.66 68.06 71.3 10 70.95 79.2 79.97 58.44
    67.68 71.2 15 68.82 78.32 79.81 58.26 67.66 70.6 Z 排序 LLaMa2-7B 5 68.82 76.13
    77.91 57.02 40.85 64.1 10 68.66 75.92 77.63 57.09 40.6 64.0 15 68.27 75.54 77.42
    57.09 39.7 63.6 Mistral-7B 5 73.87 80.76 80.9 61.24 58.62 71.1 10 74.19 80.47
    81.12 61.03 58.2 71.0 15 74.42 80.47 80.84 60.89 58.31 71.0 LLaMa2-13B 5 72.29
    79.54 78.94 60 50.54 68.3 10 72.21 79.58 79.16 59.99 50.51 68.3 15 72.05 79.46
    79.37 59.98 50.59 68.3 Qwen-2-7B 5 71.58 79.2 79.76 59.2 69.44 71.8 10 71.5 79.08
    80.35 58.82 69.01 71.8 15 71.58 78.61 79.92 58.54 68.65 71.5
- en: 'Table 4: Accuracy results of different models across various tasks for 8bit
    and 4bit quantization.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同模型在 8bit 和 4bit 量化下各种任务的准确性结果。
- en: A.2 LLaMa2 and QWEN plots
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 LLaMa2 和 QWEN 图
- en: 'We show 4 bit to 2 bit variable layer-wise quantization for LLaMa2-7b and QWEN-7B
    in [fig. 8](#A1.F8 "In A.1 Quantizing Layers Using 3 Levels ‣ Appendix A Appendix
    ‣ Layer-Wise Quantization: A Pragmatic and Effective Method for Quantizing LLMs
    Beyond Integer Bit-Levels") and [fig. 9](#A1.F9 "In A.1 Quantizing Layers Using
    3 Levels ‣ Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic and Effective
    Method for Quantizing LLMs Beyond Integer Bit-Levels") respectively. All notations
    are same as in [fig. 2](#S1.F2 "In 1 Introduction ‣ Layer-Wise Quantization: A
    Pragmatic and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels").
    These curves were also generated on 2K evaluation instances from each of the datasets.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[图 8](#A1.F8 "在 A.1 使用 3 个级别量化层 ‣ 附录 A 附录 ‣ 分层量化：一种务实且有效的超越整数位级别的 LLM 量化方法")和[图
    9](#A1.F9 "在 A.1 使用 3 个级别量化层 ‣ 附录 A 附录 ‣ 分层量化：一种务实且有效的超越整数位级别的 LLM 量化方法")中展示了
    LLaMa2-7b 和 QWEN-7B 从 4 位到 2 位的可变层级量化。所有符号与[图 2](#S1.F2 "在 1 引言 ‣ 分层量化：一种务实且有效的超越整数位级别的
    LLM 量化方法")中的相同。这些曲线也在每个数据集的 2K 评估实例上生成。
- en: A.3 Commonality between layer importance
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.3 层重要性之间的共性
- en: 'To the best of our knowledge, we are the first one to propose layer importance
    and utilize the importance order to quantize different layers at different bits.
    in [Figure 6](#A1.F6 "In Appendix A Appendix ‣ Layer-Wise Quantization: A Pragmatic
    and Effective Method for Quantizing LLMs Beyond Integer Bit-Levels"), we show
    the intensity bars below for each layer based on their importance for four different
    LLMs with different number of layers and sizes. As observed, there is a substantial
    pattern overlap of least important layers across multiple LLMs. We observe that
    first and the last layer are the most two important layers. Many of the important
    layers also tend to be the initial few set of layers. Lesser important layers
    (shown by block of layers with faded intensity) tend to be towards halfway of
    middle and end of the network. These observations suggest generalized patterns
    in layer importance across LLMs and pre-computed layer importance orders can be
    roughly utilized to quantize a wide variety of LLMs.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，我们是第一个提出层重要性并利用重要性顺序在不同位数上量化不同层的方法的团队。在[图6](#A1.F6 "在附录A 附录 ‣ 层级量化：量化LLMs超越整数位级的务实有效方法")中，我们展示了根据层的重要性为四种不同的LLM（具有不同层数和大小）绘制的强度条。如观察所示，多种LLM中最不重要的层之间存在显著的模式重叠。我们观察到，第一层和最后一层是两个最重要的层。许多重要层往往是最初的几层。较不重要的层（通过强度逐渐减弱的层块显示）通常位于网络的中间和末端。这些观察结果表明了LLM层重要性的普遍模式，预先计算的层重要性顺序可以粗略用于量化各种LLM。
- en: It is worth noting that (unpublished) concurrent works like Gromov et al. ([2024](#bib.bib10))
    have presented an empirical finding that layers towards the end of the model can
    be removed except the last layer. The reverse order of layers indexes surprisingly
    has overlap with the importance order calculated with our LIM score but we believe
    our LIM score is more broadly applicable to any LLM where even the layers towards
    the end can be more important.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，（未发表的）并行研究如Gromov等人（[2024](#bib.bib10)）提出了一个实证发现，即模型末端的层可以被移除，除了最后一层。层索引的反向顺序与我们使用LIM评分计算的重要性顺序意外地重叠，但我们认为我们的LIM评分更广泛适用于任何LLM，即使是末端的层也可能更重要。
- en: A.4 Results of 8bit to 4bit quantization on different datasets
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.4 不同数据集上的8位到4位量化结果
- en: We show results of quantizing models lower than 8 bit. Following our proposed
    methodology, the same techinque can be applied to have the more important layers
    in 8 bits and the least important ones in 4 bits. While this does still increase
    the performance that can be fit within a memory requirement, the results are not
    as major as the ones for the 4-2 bit range, thus we mainly focus on that range.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了低于8位的模型量化结果。按照我们提出的方法，同样的技术可以应用于将更重要的层设为8位，将较不重要的层设为4位。虽然这确实增加了能够适配内存要求的性能，但结果不如4-2位范围的结果显著，因此我们主要关注该范围。
