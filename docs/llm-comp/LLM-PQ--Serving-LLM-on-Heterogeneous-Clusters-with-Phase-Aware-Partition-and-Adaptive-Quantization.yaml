- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:49:58'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:49:58'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: LLM-PQ：在异构集群上通过阶段感知分区和自适应量化服务LLM
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01136](https://ar5iv.labs.arxiv.org/html/2403.01136)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.01136](https://ar5iv.labs.arxiv.org/html/2403.01136)
- en: Juntao Zhao University of Hong KongHong Kong ,  Borui Wan University of Hong
    KongHong Kong ,  Yanghua Peng ByteDance Inc.USA ,  Haibin Lin ByteDance Inc.USA
     and  Chuan Wu University of Hong KongHong Kong
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Juntao Zhao 香港大学 香港，Borui Wan 香港大学 香港，Yanghua Peng 字节跳动 美国，Haibin Lin 字节跳动 美国
    和 Chuan Wu 香港大学 香港
- en: Abstract.
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: Recent breakthroughs in Large-scale language models (LLMs) have demonstrated
    impressive performance on various tasks. The immense sizes of LLMs have led to
    very high resource demand and cost for running the models. Though the models are
    largely served using uniform high-caliber GPUs nowadays, utilizing a heterogeneous
    cluster with a mix of available high- and low-capacity GPUs can potentially substantially
    reduce the serving cost. There is a lack of designs to support efficient LLM serving
    using a heterogeneous cluster, while the current solutions focus on model partition
    and uniform compression among homogeneous devices. This paper proposes LLM-PQ,
    a system that advocates adaptive model quantization and phase-aware partition
    to improve LLM serving efficiency on heterogeneous GPU clusters. We carefully
    decide on mixed-precision model quantization together with phase-aware model partition
    and micro-batch sizing in distributed LLM serving with an efficient algorithm,
    to greatly enhance inference throughput while fulfilling user-specified model
    quality targets. Extensive experiments on production inference workloads in 11
    different clusters demonstrate that LLM-PQ achieves up to 2.88$\times$ on average)
    throughput improvement in inference, showing great advantages over state-of-the-art
    works. Source code available at https://github.com/tonyzhao-jt/LLM-PQ.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，大规模语言模型（LLMs）取得了在各种任务上的显著突破。这些模型的巨大规模导致了非常高的资源需求和运行成本。虽然这些模型现在主要使用统一的高性能GPU，但利用一个包含高性能和低性能GPU的异构集群可以大幅降低服务成本。目前缺乏支持在异构集群上高效服务LLM的设计，而现有解决方案主要集中在模型分区和在同质设备上均匀压缩。本文提出了LLM-PQ，一个倡导自适应模型量化和阶段感知分区的系统，以提高在异构GPU集群上的LLM服务效率。我们在分布式LLM服务中，结合阶段感知模型分区和微批次大小，精心决定混合精度模型量化，并采用高效算法，大幅提升推理吞吐量，同时满足用户指定的模型质量目标。在11个不同集群的生产推理工作负载上的大量实验表明，LLM-PQ在推理中平均提高了2.88$\times$的吞吐量，显示出相较于现有最先进技术的巨大优势。源代码可在
    https://github.com/tonyzhao-jt/LLM-PQ 查阅。
- en: 1\. Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: Large-scale language models (LLMs) such as GPT3, LLaMA, OPT, and BLOOM ([scao2022bloom,](#bib.bib31)
    ; [Zhang2022OPTOP,](#bib.bib40) ; [Touvron2023LLaMAOA,](#bib.bib33) ) have exhibited
    unprecedented performance in pushing the envelope of various artificial intelligence
    (AI) tasks. The outstanding model performance is largely attributed to a very
    large model size ranging from a few hundred million to even half a trillion parameters.
    Training an LLM requires thousands of GPUs and millions of dollars ([gpt3,](#bib.bib3)
    ). Serving a trained LLM is also resource-demanding and cost-intensive, as an
    LLM cannot commonly be fit into a single GPU, therefore multiple GPUs are required
    for distributed inference.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模语言模型（LLMs），如GPT3、LLaMA、OPT和BLOOM ([scao2022bloom,](#bib.bib31) ; [Zhang2022OPTOP,](#bib.bib40)
    ; [Touvron2023LLaMAOA,](#bib.bib33) )在推动各种人工智能（AI）任务的边界上表现出了前所未有的性能。卓越的模型性能主要归功于非常大的模型规模，从几亿到甚至半万亿个参数不等。训练一个LLM需要数千个GPU和数百万美元
    ([gpt3,](#bib.bib3) )。服务一个训练好的LLM也需要大量资源和高昂成本，因为LLM通常无法放入单个GPU中，因此需要多个GPU进行分布式推理。
- en: To cope with the massive size of LLMs, a number of approaches have been proposed
    to enable their efficient deployment in practice. DeepSpeed ([Aminabadi2022DeepSpeedIE,](#bib.bib1)
    ), FasterTransformer and HuggingFace Text Generation Inference (TGI) ([huggingface_text_generation_inference,](#bib.bib13)
    ) integrate existing model parallelism techniques, such as tensor-parallelism
    (TP) and pipeline parallelism (PP), with memory footprint reduction schemes, e.g.,
    quantization or offloading, to lower the resource demands of model serving in
    a distributed manner. For memory footprint reduction schemes, quantization converts
    model weights into lower-precision formats (e.g., 8-bit), reducing memory consumption.
    Offloading methods ([flexgen,](#bib.bib32) ) leverage aggregate CPU and NVMe memory
    capacity to store weights or compute a portion of the GPU workload. However, the
    existing solutions are mainly designed for models serving on homogeneous clusters,
    limiting their performance in a heterogeneous cluster.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对 LLM 的庞大规模，已经提出了多种方法以实现其高效部署。DeepSpeed ([Aminabadi2022DeepSpeedIE,](#bib.bib1)
    )、FasterTransformer 和 HuggingFace 文本生成推理（TGI） ([huggingface_text_generation_inference,](#bib.bib13)
    ) 将现有的模型并行技术，如张量并行（TP）和流水线并行（PP），与内存占用减少方案（如量化或卸载）结合，以降低分布式模型服务的资源需求。对于内存占用减少方案，量化将模型权重转换为较低精度的格式（如
    8 位），从而减少内存消耗。卸载方法 ([flexgen,](#bib.bib32) ) 利用汇总的 CPU 和 NVMe 内存容量来存储权重或计算部分 GPU
    工作负载。然而，现有的解决方案主要设计用于同质集群上的模型服务，限制了其在异构集群中的性能。
- en: '![Refer to caption](img/aea12a54f0ebc6b51840210a022ef8d7.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/aea12a54f0ebc6b51840210a022ef8d7.png)'
- en: (a) GPU Portions
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GPU 部分
- en: '![Refer to caption](img/6a0836767d2966238500ad6710748275.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/6a0836767d2966238500ad6710748275.png)'
- en: (b) Average utilization of different types of GPUs in one month
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 一个月内不同类型 GPU 的平均利用率
- en: Figure 1\. GPU proportions and utilization rates in a real-world production
    AI cluster.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 实际生产 AI 集群中的 GPU 比例和利用率。
- en: 'A practical AI cloud or machine learning (ML) cluster often contains heterogeneous
    devices, e.g., GPUs of different models purchased at different times. Utilization
    of different types of GPUs may differ substantially. Fig. [1](#S1.F1 "Figure 1
    ‣ 1\. Introduction ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization") shows the proportion of different GPUs in
    a production cluster, with fewer percentages of high-calibre GPUs (NVIDIA A100,
    V100) the majority being relatively low-calibre inference GPUs (such as T4). The
    utilization rate of other GPUs is much lower than that of A100, which are used
    intensively for both training and inference of large models nowadays for the best
    performance. Efficiently exploiting available heterogeneous GPUs for LLM serving
    is worthwhile to explore, to fully utilize available resources and substantially
    reduce the cost of provisioning LLM-enabled applications.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '一个实际的 AI 云或机器学习（ML）集群通常包含异构设备，例如不同型号的 GPU，这些 GPU 可能是在不同时间购买的。不同类型的 GPU 的使用情况可能差异很大。图
    [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization") 显示了生产集群中不同 GPU
    的比例，其中高性能 GPU（如 NVIDIA A100、V100）的比例较少，大多数为相对低性能的推理 GPU（如 T4）。其他 GPU 的利用率远低于 A100，后者目前在大型模型的训练和推理中被广泛使用，以获得最佳性能。有效利用现有的异构
    GPU 进行 LLM 服务是值得探索的，以充分利用现有资源并大幅降低 LLM 启用应用程序的配置成本。'
- en: 'The commonly adopted TP and PP paradigms partition model operations/layers
    evenly among the GPUs, which is not suitable for heterogeneous GPUs and results
    in either low utilization of high-capacity GPUs or out-of-memory (OOM) errors
    on low-memory GPUs. The limited studies of models serving on heterogeneous clusters ([hu2021pipeline,](#bib.bib17)
    ) focus on the partition of encoder-based transformer models. However, mainstream
    LLMs with decoder-only structures contain two phases during inference: prompt
    processing (prefill) and token generation (decode). While the former phase is
    similar to the inference of encoder-based transformers, the latter has a totally
    different pattern (see Sec. [2.1](#S2.SS1 "2.1\. Generative Inference of LLM ‣
    2\. BackGround and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters
    with Phase-Aware Partition and Adaptive Quantization")), making the previous partition
    solutions not suitable. Besides, the execution time required for each phase, depending
    on the prompt length and token generation number, varies significantly. What is
    worse, in a heterogeneous cluster, this difference can even be amplified, causing
    model partitioning that focuses on the time of the first phase instead of both
    being far from optimal. Therefore, phase-aware model partition schemes warrant
    investigation. Additionally, extra memory required for pre-and post-processing
    during LLM inference, such as text embedding for converting input tokens to word
    vectors, should also be considered, especially when utilizing low-calibre GPUs
    which have limited GPU memory.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '通常采用的TP和PP范式将模型操作/层均匀分配到GPU上，这对于异质GPU来说并不合适，会导致高容量GPU的利用率低或低内存GPU上出现OOM错误。关于在异质集群上服务的模型的有限研究（[hu2021pipeline,](#bib.bib17)）主要集中在基于编码器的变换器模型的分区。然而，主流的只含解码器的LLM在推理过程中包含两个阶段：提示处理（预填充）和令牌生成（解码）。虽然前者类似于基于编码器的变换器的推理，但后者有完全不同的模式（见Sec.
    [2.1](#S2.SS1 "2.1\. Generative Inference of LLM ‣ 2\. BackGround and Motivation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization")），使得之前的分区解决方案不适用。此外，每个阶段所需的执行时间，根据提示长度和令牌生成数量，差异显著。在异质集群中，这种差异甚至会被放大，使得关注第一个阶段时间的模型分区方案远未达到最佳。因此，**基于阶段的模型分区方案**值得研究。此外，还应考虑LLM推理过程中预处理和后处理所需的额外内存，例如将输入令牌转换为词向量的文本嵌入，特别是在使用内存有限的低端GPU时。'
- en: When the model is partitioned among heterogeneous GPUs, adopting a single quantization
    precision across all model layers in different types of GPUs is always suboptimal.
    uniform single-precision model quantization can select a precision, e.g., INT4,
    that is suitable for GPUs with lower memory to avoid OOM (Out Of Memory) problem,
    but causing a notable portion of memory waste for those with abundant GPU memory.
    Adaptive mixed-precision quantization for LLM, which is not investigated in the
    literature ([frantar2023gptq,](#bib.bib14) ; [xiao2023smoothquant,](#bib.bib36)
    ), is more desirable. By using higher precision for model weights on GPUs with
    more available memory instead of forcing them to use the same one in those low-calibre
    GPUs, adaptive mixed-precision quantization can not only avoid memory waste but
    promote the model quality as well.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型在异质GPU之间进行分区时，在不同类型的GPU上采用单一的量化精度总是不理想的。统一的单精度模型量化可以选择一种适合内存较低的GPU的精度，例如INT4，以避免OOM（内存溢出）问题，但会导致内存丰富的GPU上显著的内存浪费。对于LLM，尚未在文献中研究的**自适应混合精度量化**（[frantar2023gptq,](#bib.bib14)
    ; [xiao2023smoothquant,](#bib.bib36)）更为理想。通过在内存更多的GPU上使用更高的精度进行模型权重量化，而不是强制它们在低端GPU上使用相同的精度，自适应混合精度量化不仅可以避免内存浪费，还能提升模型质量。
- en: 'In this work, we propose a novel system, LLM-PQ, to enable efficient LLM generative
    serving on heterogeneous GPU clusters. Instead of emphasizing the enhancement
    of throughput faced with infinite requests, as commonly pursued in recent works
    like vLLM ([vllm,](#bib.bib20) ). LLM-PQ directs its focus toward the efficient
    processing of a given workload, which is faced by the offline task. LLM-PQ advocates
    adaptive model quantization and phase-aware model partition, as well as efficient
    micro-batch scheduling for LLM pipeline serving. It jointly determines the quantization
    precisions, model layer partition, and hybrid micro-batch sizing strategies, given
    the LLM, available resources of the heterogeneous cluster, and user-specified
    model quality targets. Our contributions in designing LLM-PQ can be summarized
    as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一个新系统LLM-PQ，以在异构GPU集群上实现高效的LLM生成服务。与最近的工作（如vLLM ([vllm,](#bib.bib20)
    ）中常见的面对无限请求的吞吐量提升不同，LLM-PQ将重点放在对离线任务所面临的给定工作负载的高效处理上。LLM-PQ倡导自适应模型量化和阶段感知模型划分，以及高效的微批量调度。它在考虑LLM、异构集群的可用资源和用户指定的模型质量目标的情况下，联合确定量化精度、模型层划分和混合微批量大小策略。我们在设计LLM-PQ方面的贡献可以总结如下：
- en: $\triangleright$ We provide a cost model that details the memory requirements
    of LLM serving under a mixed-precision quantization scheme. We learn a linear
    regression model to accurately predict the latency of mixed-precision LLM inference
    workloads with varying sequence lengths and batch sizes based on their phase-aware
    computational characteristics.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 我们提供了一个成本模型，详细描述了在混合精度量化方案下LLM服务的内存需求。我们学习了一个线性回归模型，以准确预测具有不同序列长度和批量大小的混合精度LLM推断工作负载的延迟，基于其阶段感知计算特性。
- en: $\triangleright$ We introduce adaptive mixed-precision into the search space
    of heterogeneous pipeline serving of LLM and provide a variance indicator the
    measure the layer sensitivity towards different level quantization. We develop
    an iterative algorithm that first explores possible GPU orderings and different
    (phase, micro-batch size) pairs in the pruned search space, and then solves an
    integer linear programming (ILP) problem to determine the best partition and quantization
    bitwidths.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 我们将自适应混合精度引入LLM的异构管道服务搜索空间，并提供了一个方差指标来测量层对不同量化级别的敏感性。我们开发了一个迭代算法，该算法首先在剪枝后的搜索空间中探索可能的GPU排序和不同的（阶段，微批量大小）对，然后解决一个整数线性规划（ILP）问题，以确定最佳的划分和量化位宽。
- en: $\triangleright$ on average) as compared to state-of-the-art approaches.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: $\triangleright$ 与最先进的方法相比（平均）.
- en: 2\. BackGround and Motivation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景与动机
- en: 2.1\. Generative Inference of LLM
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. LLM的生成推断
- en: '![Refer to caption](img/4ee0e03eba4f4182aa7a178213e9ce67.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/4ee0e03eba4f4182aa7a178213e9ce67.png)'
- en: 'Figure 2\. Two phases in LLM generative serving: (Top) Prefill phase takes
    the prompt sequence to generate the initial key-value pairs. (Bottom) Decode phase
    takes previously generated token & stored KV pairs to generate the next token.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. LLM生成服务的两个阶段：（上）预填充阶段使用提示序列生成初始的键值对。（下）解码阶段利用先前生成的令牌和存储的KV对生成下一个令牌。
- en: 'LLM generally refers to a suite of decoder-only transformer models with large
    parameter sizes ([Zhang2022OPTOP,](#bib.bib40) ; [scao2022bloom,](#bib.bib31)
    ). Unlike encoder-based transformers like ViT-Huge ([vit,](#bib.bib11) ) and Bert-Large ([bert,](#bib.bib9)
    ) that are sequence-to-sequence, LLMs generate tokens one by one in an inference
    process that comprises two phases ([Zhang2022OPTOP,](#bib.bib40) ; [scao2022bloom,](#bib.bib31)
    )(Fig. [2](#S2.F2 "Figure 2 ‣ 2.1\. Generative Inference of LLM ‣ 2\. BackGround
    and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization")): prefill and decode ([flexgen,](#bib.bib32)
    ). In the prefill phase, the input prompt sequence produces key/value (KV) caches
    for each transformer layer, which is used in the attention mechanism as a context
    vector for later token generation. During the decode phase, stored KV pairs are
    updated as each subsequent token is generated one by one based on the preceding
    token; the token generation process continues until a stopping criterion is met,
    such as reaching the end of a sequence (EOS) or exceeding the maximum number of
    tokens allowed. During generative inference, each layer of the LLM undergoes a
    prefill phase followed by several passes in the decode phase (an example is given
    in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1\. Generative Inference of LLM ‣ 2\. BackGround
    and Motivation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization")).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 'LLM 通常指的是一套仅解码器的变换器模型，具有大型参数 ([Zhang2022OPTOP,](#bib.bib40) ; [scao2022bloom,](#bib.bib31)
    )。与 ViT-Huge ([vit,](#bib.bib11) ) 和 Bert-Large ([bert,](#bib.bib9) ) 这样的基于编码器的变换器不同，LLM
    在推理过程中生成一个个 token，这个过程包括两个阶段 ([Zhang2022OPTOP,](#bib.bib40) ; [scao2022bloom,](#bib.bib31)
    )(图 [2](#S2.F2 "图 2 ‣ 2.1\. LLM 的生成推理 ‣ 2\. 背景和动机 ‣ LLM-PQ: 在异构集群上使用阶段感知分区和自适应量化服务
    LLM"))：预填充和解码 ([flexgen,](#bib.bib32) )。在预填充阶段，输入提示序列为每个变换器层生成关键/值 (KV) 缓存，这些缓存在注意力机制中作为上下文向量用于后续
    token 的生成。在解码阶段，存储的 KV 对在每次生成新的 token 时进行更新；token 生成过程持续进行，直到满足停止条件，例如达到序列结束 (EOS)
    或超出允许的最大 token 数。在生成推理过程中，LLM 的每一层经历一个预填充阶段，随后是多个解码阶段的处理（如图 [2](#S2.F2 "图 2 ‣
    2.1\. LLM 的生成推理 ‣ 2\. 背景和动机 ‣ LLM-PQ: 在异构集群上使用阶段感知分区和自适应量化服务 LLM") 中所示的例子）。'
- en: 'The time taken by the prefill and decode phases varies to the prompt length.
    By sampling 10,000 conversations generated by chatGPT from the ShareGPT ([ryokoai_sharegpt52k,](#bib.bib30)
    ) dataset, we found that the prompt length varies substantially: $  <--shaq-efficient>  \  #  faster--  #  use  cost  model  or  profiled  result'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 'llmpq-algo  \--model-name  ${model_name}  --model_size  ${model_size}  \--device_names  "${device_names[@]}"  \--device_numbers  "${device_numbers[@]}"  \--omega_file  $omega_file  \  #  指示器文件--global_bz  $batch_size  --s  $s  --n  $n  \  #  工作负载--theta  $theta  \  #  用户标量--  <--shaq-efficient>  \  #  更快--  #  使用成本模型或分析结果'
- en: The output strategy can be launched directly. If the same GPU type is located
    on the same node, other configurations, such as ranks, will be derived automatically
    and registered to the distributed runtime. Alternatively, distributedconfigs same
    as those in PyTorch can be used to launch the strategy, but the noauto flag must
    be specified.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 输出策略可以直接启动。如果同一节点上有相同类型的GPU，其他配置，如排名，将自动推导并注册到分布式运行时。或者，可以使用与PyTorch相同的分布式配置来启动策略，但必须指定noauto标志。
- en: llmpq-dist  --strat_file_name  $strategy_file_path  \<--master_addr  --master_port>/
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: llmpq-dist  --strat_file_name  $strategy_file_path  \<--master_addr  --master_port>/
- en: 6\. Evaluation
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 评估
- en: 6.1\. Experimental Setup
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 实验设置
- en: 'Models & Precisions. We run BLOOM ([scao2022bloom,](#bib.bib31) ) and OPT ([Zhang2022OPTOP,](#bib.bib40)
    ) model families, focusing on middle- and large-sized models, specifically OPT-13b,
    30b, 66b, and BLOOM-176b. We evaluate candidate precisions: $BITs=\{3,4,8,16\}$.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 模型与精度 我们运行BLOOM ([scao2022bloom](#bib.bib31)) 和OPT ([Zhang2022OPTOP](#bib.bib40))
    模型系列，重点关注中型和大型模型，特别是OPT-13b、30b、66b和BLOOM-176b。我们评估候选精度：$BITs=\{3,4,8,16\}$。
- en: 'Baselines. We compare LLM-PQ with three baselines: (1) PipeEdge, where we apply
    uniform quantization and use PipeEdge ([hu2021pipeline,](#bib.bib17) ) for heterogeneous
    layer partition. (2) Uniform, which uses uniform quantization, evenly partitions
    the model layers among devices and decides micro-batch sizes that minimize the
    inference latency, mimicking the policy of existing serving systems such as HF-Transformers ([HF_transformers,](#bib.bib35)
    ) and Deepspeed ([Aminabadi2022DeepSpeedIE,](#bib.bib1) ). (3) Offloading, where
    we adopt CPU and disk swapping in FlexGen ([flexgen,](#bib.bib32) ) to maximize
    the throughput of token generation for low-calibre GPUs, we adopt even partition
    for this method. For (1)(2), we keep lowering the quantization bitwidth from the
    maximum (i.e., FP16) until the model can fit into the devices or no feasible solutions
    are available. For (1)(3), we use the same micro-batch size for prefill and decode
    phases by partitioning the global batch size by the number of pipeline stages.
    FlexGen is specialized for OPT models and thus has no results on BLOOM models.
    We did not conduct a comparison with vLLM ([vllm,](#bib.bib20) ) as it primarily
    focuses on the online task, and the paged attention mechanism is of no use when
    dealing with fixed token generation numbers. Also, vLLM didn’t support pipeline
    parallelism, making the comparison unfair in our case.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试。我们将 LLM-PQ 与三个基准进行比较：(1) PipeEdge，我们应用均匀量化并使用 PipeEdge ([hu2021pipeline,](#bib.bib17)
    ) 进行异构层划分。(2) Uniform，使用均匀量化，将模型层均匀划分到设备上，并决定微批次大小以最小化推理延迟，模拟现有服务系统的策略，如 HF-Transformers ([HF_transformers,](#bib.bib35)
    ) 和 Deepspeed ([Aminabadi2022DeepSpeedIE,](#bib.bib1) )。(3) Offloading，我们在 FlexGen ([flexgen,](#bib.bib32)
    ) 中采用 CPU 和磁盘交换，以最大化低规格 GPU 的 token 生成吞吐量，为此方法采用均匀划分。对于 (1)(2)，我们不断降低量化位宽，从最大值（即
    FP16）开始，直到模型可以适配设备或没有可行的解决方案。对于 (1)(3)，我们使用相同的微批次大小进行预填充和解码阶段，通过将全局批次大小按管道阶段数划分。FlexGen
    专门用于 OPT 模型，因此对 BLOOM 模型没有结果。我们没有与 vLLM ([vllm,](#bib.bib20) ) 进行比较，因为它主要集中于在线任务，并且分页注意机制在处理固定
    token 生成数量时没有用处。此外，vLLM 不支持管道并行，因此在我们的案例中比较不公平。
- en: Metrics. We evaluate LLM serving performance by (1) token generation throughput,
    (2) end-to-end serving latency of one batch, and (3) model quality, using perplexity
    (PPL) on WikiText2 ([wikitext2,](#bib.bib24) ), Penn Treebank (PTB) ([ptb,](#bib.bib23)
    ) and C4 ([c4,](#bib.bib29) ). The weight calibration data consists of 128 randomly
    selected 2048-token segments from the C4 dataset ([c4,](#bib.bib29) ).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 指标。我们通过（1）token 生成吞吐量，（2）一个批次的端到端服务延迟，以及（3）模型质量来评估 LLM 服务性能，使用 WikiText2 ([wikitext2,](#bib.bib24)
    )、Penn Treebank (PTB) ([ptb,](#bib.bib23) ) 和 C4 ([c4,](#bib.bib29) ) 的困惑度（PPL）。权重校准数据包括从
    C4 数据集 ([c4,](#bib.bib29) ) 随机选择的 128 个 2048-token 片段。
- en: Workload. We use synthetic datasets following the prompt length setup in the
    DeepSpeed paper  ([Aminabadi2022DeepSpeedIE,](#bib.bib1) ), i.e., 128 and 512\.
    By default, we pad input prompts to 512 tokens, use an input batch size of 32,
    and set the number of tokens to be generated to $n=100$. We follow the same setup
    as in ORCA ([orca,](#bib.bib38) ) to never emit the EOS but continue to generate
    tokens until reaching the expected token generation length.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载。我们使用合成数据集，遵循 DeepSpeed 论文中的提示长度设置（[Aminabadi2022DeepSpeedIE,](#bib.bib1)），即
    128 和 512。默认情况下，我们将输入提示填充到 512 个 token，使用的输入批次大小为 32，并将生成的 token 数设置为 $n=100$。我们遵循
    ORCA ([orca,](#bib.bib38) ) 的相同设置，不发出 EOS，而是继续生成 token，直到达到预期的 token 生成长度。
- en: 'Heterogeneous Clusters. Devices/nodes are in our production cluster. We construct
    a number of heterogeneous clusters for model serving (clusters 1-8 in Table [3](#S6.T3
    "Table 3 ‣ 6.1\. Experimental Setup ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on
    Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")),
    with a mix of common types of GPUs. GPUs of the same type are located on the same
    node, intra-connected with NV-LINK; Clusters 1,2,9,10,11 are on a single node
    and others consist of two nodes. Nodes in Clusters 3,5,8,11 are interconnected
    with 800Gbps Ethernet; 4,6, and 7 with 100Gbps Ethernet. All GPUs are equipped
    with GB/s SSD; Each node is equipped with two CPUs, P100 nodes with Intel Xeon
    CPU E5-2630 v4 2.2GHz, 64G RAM, V100 and A800 with Intel Xeon Gold 6230 2.1GHz,
    128G RAM and 450G RAM, T4 with Intel Xeon Platinum 8260 CPU, 108G RAM, A100-40G
    with AMD EPYC 7H12 64-Core, 256G RAM. OS: Ubuntu 20.04.6 LTS. We also show LLM-PQ’s
    performance on several homogenous clusters (clusters 9-11 in Table [3](#S6.T3
    "Table 3 ‣ 6.1\. Experimental Setup ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on
    Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")).'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '异构集群。设备/节点位于我们的生产集群中。我们为模型服务构建了一些异构集群（见表[3](#S6.T3 "Table 3 ‣ 6.1\. Experimental
    Setup ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization")），这些集群混合了不同类型的GPU。相同类型的GPU位于同一节点上，并通过NV-LINK相互连接；集群1、2、9、10、11位于单个节点上，其他集群则由两个节点组成。集群3、5、8、11的节点通过800Gbps以太网互连；集群4、6和7则通过100Gbps以太网互连。所有GPU均配备GB/s
    SSD；每个节点配备两个CPU，P100节点配备Intel Xeon CPU E5-2630 v4 2.2GHz，64G RAM，V100和A800配备Intel
    Xeon Gold 6230 2.1GHz，128G RAM和450G RAM，T4配备Intel Xeon Platinum 8260 CPU，108G
    RAM，A100-40G配备AMD EPYC 7H12 64核，256G RAM。操作系统：Ubuntu 20.04.6 LTS。我们还展示了LLM-PQ在几个同构集群（见表[3](#S6.T3
    "Table 3 ‣ 6.1\. Experimental Setup ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on
    Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")）上的性能。'
- en: 'Experiment Settings $\theta$, solver setup, and overhead table are provided
    in Appendix [A.2](#A1.SS2 "A.2\. Experiment ‣ Appendix A Appendix ‣ LLM-PQ: Serving
    LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization").
    The model size to run on each cluster is decided such that the total weight size
    of the non-quantized model is comparable to the overall device memory capacity
    in the cluster.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '实验设置$\theta$、求解器设置和开销表见附录[A.2](#A1.SS2 "A.2\. Experiment ‣ Appendix A Appendix
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization")。每个集群上运行的模型大小是根据未量化模型的总权重大小与集群的整体设备内存容量相当来决定的。'
- en: Table 3\. Cluster Configurations
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. 集群配置
- en: '| Cluster | Devices | Model Size | Cluster | Devices | Model Size |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 集群 | 设备 | 模型大小 | 集群 | 设备 | 模型大小 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 1xV100-32G | 13b | 2 | 1xA100-40G | 13b |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1xV100-32G | 13b | 2 | 1xA100-40G | 13b |'
- en: '| 3 | 3xT4-16G + 1xV100-32G | 30b | 4 | 3xP100-12G + 1xV100-32G | 30b |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3xT4-16G + 1xV100-32G | 30b | 4 | 3xP100-12G + 1xV100-32G | 30b |'
- en: '| 5 | 4xT4-16G + 2xV100-32G | 66b | 6 | 2xV100-32G + 2xA100-40G | 66b |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 4xT4-16G + 2xV100-32G | 66b | 6 | 2xV100-32G + 2xA100-40G | 66b |'
- en: '| 7 | 4xV100-32G + 4xA100-40G | 176b | 8 | 4xV100-32G + 2xA800-80G | 176b |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 4xV100-32G + 4xA100-40G | 176b | 8 | 4xV100-32G + 2xA800-80G | 176b |'
- en: '| 9 | 4xT4-16G | 30b | 10 | 4xV100-32G | 66b |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 4xT4-16G | 30b | 10 | 4xV100-32G | 66b |'
- en: '| 11 | 4xA800-80G | 176b |  |  |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 4xA800-80G | 176b |  |  |  |'
- en: 6.2\. Fidelity of Cost Models
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 成本模型的精度
- en: 'We evaluate our memory cost model on BLOOM of sizes 560m and 1b7, and OPT of
    13b, 30b, and 66b, with prompt length uniformly sampled between 128 and 512, the
    batch size chosen among 2, 4, and 8, generated token length sampled between 100
    and 200, and randomly generated precision setting from the available bitwidth
    set. We consider the memory consumption of model weights and KV caching here and
    compare the predicted memory usage with those collected from real systems. We
    also create 50 unseen workloads with different precisions, batch sizes (3,5 or
    7), prompt lengths, and past sequence lengths (384 or 768) for each device, evaluate
    our latency cost model on them. Fig. [7](#S6.F7 "Figure 7 ‣ 6.2\. Fidelity of
    Cost Models ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with
    Phase-Aware Partition and Adaptive Quantization") shows that the error of the
    memory cost model is almost negligible, and the average error of the latency cost
    model is less than 6%.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 BLOOM 的 560m 和 1b7 以及 OPT 的 13b、30b 和 66b 上评估了我们的内存成本模型，提示长度均匀采样在 128 到
    512 之间，批处理大小从 2、4 和 8 中选择，生成的 token 长度在 100 到 200 之间采样，并随机生成精度设置。我们在此考虑了模型权重和
    KV 缓存的内存消耗，并将预测的内存使用情况与实际系统中收集的结果进行比较。我们还为每个设备创建了 50 个未见过的工作负载，具有不同的精度、批处理大小（3、5
    或 7）、提示长度和过去的序列长度（384 或 768），并在这些工作负载上评估了我们的延迟成本模型。图 [7](#S6.F7 "图 7 ‣ 6.2\. 成本模型的保真度
    ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上服务 LLM 使用阶段感知分区和自适应量化") 显示内存成本模型的误差几乎可以忽略不计，延迟成本模型的平均误差小于
    6%。'
- en: We observed that, during the prefill phase, the cost of observations typically
    increases linearly with the workload. However, it is noteworthy that in the decode
    phase, a notable difference in latency occurs only when a substantial change in
    context length (50-100) is present.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，在预填充阶段，观察的成本通常会随着工作负载的增加而线性增长。然而，在解码阶段，只有在上下文长度（50-100）发生显著变化时，延迟才会出现明显差异。
- en: '![Refer to caption](img/28a659adebcfc0d03741f2c62f88b62f.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/28a659adebcfc0d03741f2c62f88b62f.png)'
- en: Figure 7\. Comparison of memory and latency reported by the cost models and
    obtained in real systems.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. 成本模型报告的内存和延迟与实际系统中获得的结果的比较。
- en: 6.3\. Serving in Heterogeneous Clusters
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 异构集群中的服务
- en: Table 4\. Serving performance comparison. The best results are marked in bold.
    The missing results are due to OOM. The $\times$ is derived comparing with the
    PipeEdge baseline.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 服务性能比较。最佳结果以粗体标记。缺失的结果是由于内存溢出（OOM）。$\times$ 是与 PipeEdge 基准相比得出的。
- en: '| Model Size | Cluster | Model | Scheme | PPL | Latency (s) | Throughput (Token/s)
    | Model Size | Cluster | Model | Scheme | PPL | Latency (s) | Throughput (Token/s)
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 模型大小 | 集群 | 模型 | 方案 | PPL | 延迟（秒） | 吞吐量（Token/s） | 模型大小 | 集群 | 模型 | 方案 |
    PPL | 延迟（秒） | 吞吐量（Token/s） |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '| 13b | 1 | OPT | PipeEdge | 11.78 | 233.77 | 13.69 | 66b | 5 | OPT | PipeEdge
    | 10.50 | 750.84 | 4.26 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 13b | 1 | OPT | PipeEdge | 11.78 | 233.77 | 13.69 | 66b | 5 | OPT | PipeEdge
    | 10.50 | 750.84 | 4.26 |'
- en: '| $Uniform{}^{*}$ |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| $Uniform{}^{*}$ |'
- en: '| FlexGen | 11.22 | 174.88 | 18.30(1.34$\times$ |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 11.22 | 174.88 | 18.30(1.34$\times$ |'
- en: '| $FlexGen-int8{}^{*}$) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| $FlexGen-int8{}^{*}$) |'
- en: '| $LLM-PQ{}^{*}$) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| $LLM-PQ{}^{*}$) |'
- en: '| 2 | OPT | PipeEdge | 11.38 | 30.84 | 103.76 | 6 | OPT | PipeEdge | 10.34
    | 115.03 | 27.82 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2 | OPT | PipeEdge | 11.38 | 30.84 | 103.76 | 6 | OPT | PipeEdge | 10.34
    | 115.03 | 27.82 |'
- en: '| Uniform | 11.38 | 30.84 | 103.76 | Uniform | 10.50 | 431.92 | 7.41(0.27$\times$)
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 11.38 | 30.84 | 103.76 | Uniform | 10.50 | 431.92 | 7.41(0.27$\times$)
    |'
- en: '| FlexGen | 11.22 | 71.09 | 45.01(0.43$\times$) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 11.22 | 71.09 | 45.01(0.43$\times$) |'
- en: '| FlexGen-int8 | 11.23 | 31.11 | 102.87(0.99$\times$) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 11.23 | 31.11 | 102.87(0.99$\times$) |'
- en: '| LLM-PQ | 11.23(-0.14) | 20.63 | 155.13(1.50$\times$) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 11.23(-0.14) | 20.63 | 155.13(1.50$\times$) |'
- en: '| 30b | 3 | OPT | PipeEdge | 10.70 | 146.40 | 21.86 | 176b | 7 | BLOOM | PipeEdge
    | 10.97 | 729.91 | 4.38 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 30b | 3 | OPT | PipeEdge | 10.70 | 146.40 | 21.86 | 176b | 7 | BLOOM | PipeEdge
    | 10.97 | 729.91 | 4.38 |'
- en: '| Uniform | 10.78 | 948.90 | 3.37(0.15$\times$ |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.78 | 948.90 | 3.37(0.15$\times$ |'
- en: '| FlexGen | 10.70 | 820.72 | 3.90(0.18$\times$ |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.70 | 820.72 | 3.90(0.18$\times$ |'
- en: '| FlexGen-int8 | 10.70 | 309.95 | 10.32(0.47$\times$ |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.70 | 309.95 | 10.32(0.47$\times$ |'
- en: '| LLM-PQ | 10.70 | 80.60 | 39.70(1.82$\times$) |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.70 | 80.60 | 39.70(1.82$\times$) |'
- en: '| 4 | OPT | PipeEdge | 10.78 | 449.55 | 7.12 | 8 | BLOOM | PipeEdge | 10.97
    | 848.98 | 3.77 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 4 | OPT | PipeEdge | 10.78 | 449.55 | 7.12 | 8 | BLOOM | PipeEdge | 10.97
    | 848.98 | 3.77 |'
- en: '| Uniform | $\dagger$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | $\dagger$ |'
- en: '| FlexGen | 10.70 | 1,348.16 | 2.37(0.33$\times$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.70 | 1,348.16 | 2.37(0.33$\times$ |'
- en: '| FlexGen-int8 | 10.70 | 448.18 | 7.14(1$\times$ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.70 | 448.18 | 7.14(1$\times$ |'
- en: '| LLM-PQ | 10.70(-0.08) | 214.19 | 14.94(2.10$\times$) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.70(-0.08) | 214.19 | 14.94(2.10$\times$) |'
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6.3\. Serving in Heterogeneous Clusters ‣ 6\. Evaluation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization") demonstrates that LLM-PQ achieves the highest inference
    throughput by dividing the total number of generated tokens in a batch by the
    corresponding end-to-end latency. and the best model accuracy in clusters 3, 4,
    6, 7, and 8\. In cluster 2, LLM-PQ incur a negligible perplexity drop (0.01) but
    achieves a much faster inference speed (1.5$\times$). In cluster 6, the perplexity
    of LLM-PQ is even better than in the FP16 case. As compared with PipeEdge and
    Uniform, LLM-PQ can better utilize memory in heterogeneous devices and conduct
    phase-aware and precision-aware model partitions. LLM-PQ also outperforms FlexGen
    and FlexGen-int8 in most cases as they suffer from heavy swapping overhead. The
    results on cluster 1 reveal that our micro-batch sizing reducing the peak temporary
    memory needed by the model, allowing the int8 quantized model to fit nicely into
    the device memory.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [4](#S6.T4 "表 4 ‣ 6.3\. 在异质集群中的服务 ‣ 6\. 评估 ‣ LLM-PQ: 在异质集群上使用相位感知分区和自适应量化服务LLM")
    显示，LLM-PQ通过将批次中生成的总令牌数除以相应的端到端延迟，达到了最高的推理吞吐量，并在集群 3、4、6、7 和 8 中取得了最佳的模型准确性。在集群
    2 中，LLM-PQ 产生了微不足道的困惑度下降（0.01），但实现了更快的推理速度（1.5$\times$）。在集群 6 中，LLM-PQ 的困惑度甚至优于
    FP16 情况。与 PipeEdge 和 Uniform 相比，LLM-PQ 更能有效利用异质设备中的内存，并进行相位感知和精度感知的模型分区。LLM-PQ
    在大多数情况下也优于 FlexGen 和 FlexGen-int8，因为它们遭受了严重的交换开销。集群 1 的结果揭示了我们的微批量尺寸减少了模型所需的峰值临时内存，使得
    int8 量化模型可以很好地适应设备内存。'
- en: 6.4\. Serving in Homogeneous Clusters
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 在同质集群中的服务
- en: Table 5\. Serving performance comparison in homogenous clusters. The best inference
    throughput is marked in bold.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 在同质集群中的服务性能比较。最佳推理吞吐量用**粗体**标记。
- en: '| Model | Cluster | Scheme | PPL | Latency (s) | Throughput (Token/s) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Model | Cluster | Scheme | PPL | Latency (s) | Throughput (Token/s) |'
- en: '| OPT-30b | 9 | PipeEdge | 10.78 | 1,045.93 | 3.06 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 9 | PipeEdge | 10.78 | 1,045.93 | 3.06 |'
- en: '| Uniform | 10.78 | 1,045.93 | 3.06 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.78 | 1,045.93 | 3.06 |'
- en: '| FlexGen | 10.70 | 1,033.39 | 3.10(1.01$\times$) |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.70 | 1,033.39 | 3.10(1.01$\times$) |'
- en: '| FlexGen-int8 | 10.70 | 313.46 | 10.21(3.34$\times$) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.70 | 313.46 | 10.21(3.34$\times$) |'
- en: '| LLM-PQ | 10.75 | 407.75 | 7.85(2.57$\times$) |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.75 | 407.75 | 7.85(2.57$\times$) |'
- en: '| OPT-66b | 10 | PipeEdge | 10.33 | 182.47 | 17.54 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 10 | PipeEdge | 10.33 | 182.47 | 17.54 |'
- en: '| Uniform | 10.50 | 477.52 | 6.70(0.38$\times$) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.50 | 477.52 | 6.70(0.38$\times$) |'
- en: '| FlexGen | 10.33 | 433.99 | 7.37(0.42$\times$) |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.33 | 433.99 | 7.37(0.42$\times$) |'
- en: '| FlexGen-int8 | 10.34 | 206.93 | 15.46(0.88$\times$) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.34 | 206.93 | 15.46(0.88$\times$) |'
- en: '| LLM-PQ | 10.33 | 178.11 | 17.97(1.02$\times$) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.33 | 178.11 | 17.97(1.02$\times$) |'
- en: '| BLOOM-176b | 11 | PipeEdge | 10.90 | 49.12 | 65.14 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| BLOOM-176b | 11 | PipeEdge | 10.90 | 49.12 | 65.14 |'
- en: '| Uniform | 10.97 | 895.45 | 3.57(0.05$\times$) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.97 | 895.45 | 3.57(0.05$\times$) |'
- en: '| LLM-PQ | 10.90 | 45.45 | 70.41(1.08$\times$) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.90 | 45.45 | 70.41(1.08$\times$) |'
- en: 'On homogeneous clusters, 9, 10, and 11, Table [5](#S6.T5 "Table 5 ‣ 6.4\. Serving
    in Homogeneous Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization") shows that LLM-PQ
    still achieves throughput gains, though smaller than on heterogeneous clusters.
    In the case of cluster 9, the performance and perplexity of LLM-PQ are inferior
    to that of FlexGen-int8\. This discrepancy is attributed to the limited GPU memory
    compared to the workload requirement, resulting in high compression and usage
    of more low-precision kernels. Consequently, the computational speed is slower,
    but the efficiency of swapping is enhanced. Among other cases, LLM-PQ performs
    the best on model quality and serving throughput.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '在同质集群中，表 [5](#S6.T5 "表 5 ‣ 6.4\. 在同质集群中的服务 ‣ 6\. 评估 ‣ LLM-PQ: 在异质集群上使用相位感知分区和自适应量化服务LLM")
    显示，LLM-PQ 仍然取得了吞吐量的提升，尽管比在异质集群中小。以集群 9 为例，LLM-PQ 的性能和困惑度低于 FlexGen-int8。这一差异归因于与工作负载需求相比，GPU
    内存有限，导致压缩率高且使用了更多低精度内核。因此，计算速度较慢，但交换效率有所提高。在其他情况下，LLM-PQ 在模型质量和服务吞吐量上表现最佳。'
- en: 6.5\. Effectiveness of Variance Indicator
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5\. 方差指标的有效性
- en: 'To further validate the effectiveness of our model variance indicator, we compare
    it with random assignment, where $\omega_{i,b}$ in (LABEL:eq:objective) to ensure
    that different indicators lead to similar inference latency, eliminating the influence
    of value range of the indicator. In Table [6](#S6.T6 "Table 6 ‣ 6.5\. Effectiveness
    of Variance Indicator ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization"), we observe that
    LLM-PQ achieve better perplexity than FP16 on cluster 6\. On cluster 9, with heavier
    quantization as mentioned above, Hessian-based and our indicators yield the same
    perplexity, outperforming the pure random indicator.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '为进一步验证我们模型方差指示器的有效性，我们将其与随机分配进行比较，其中(LABEL:eq:objective)中的$\omega_{i,b}$确保不同指示器导致类似的推理延迟，从而消除指示器值范围的影响。在表[6](#S6.T6
    "表 6 ‣ 6.5\. 方差指示器的有效性 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上使用阶段感知分区和自适应量化进行LLM服务")中，我们观察到LLM-PQ在集群6上实现了比FP16更好的困惑度。在集群9上，如上所述，具有更重的量化，Hessian基于和我们指示器产生相同的困惑度，优于纯随机指示器。'
- en: Table 6\. Effectiveness of LLM-PQ’s variance indicator. PPL is compared with
    Random, while $\times$ is compared with Hessian.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. LLM-PQ的方差指示器的有效性。PPL与Random比较，而$\times$与Hessian比较。
- en: '| Model | Cluster | Method | PPL | Overhead (s) |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Model | Cluster | Method | PPL | Overhead (s) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-66b | 6 | Random | 10.33 | 0 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 6 | Random | 10.33 | 0 |'
- en: '| Hessian | 10.33 | 25625.44 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| Hessian | 10.33 | 25625.44 |'
- en: '| LLM-PQ | 10.31(-0.02) | 434.78(58.15$\times$) |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.31(-0.02) | 434.78(58.15$\times$) |'
- en: '| OPT-30b | 9 | Random | 11.04 | 0 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 9 | Random | 11.04 | 0 |'
- en: '| Hessian | 10.75 | 15670.87 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| Hessian | 10.75 | 15670.87 |'
- en: '| LLM-PQ | 10.75(-0.29) | 215.60(72.69$\times$) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.75(-0.29) | 215.60(72.69$\times$) |'
- en: 6.6\. Serving with Shorter Prompts
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6\. 使用短提示进行服务
- en: 'We next experiment with input prompt length of 128 and maximal token generatoin
    number $n=200$. Table [7](#S6.T7 "Table 7 ‣ 6.6\. Serving with Shorter Prompts
    ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization") shows that LLM-PQ achieves substantial inference
    speed-ups without any accuracy degradation, and even shows accuracy improvements.
    This confirms the correctness of our two-phase latency modeling in LLM-PQ. We
    note that the throughput gain of LLM-PQ in cluster 4 is much lower than that with
    prompt length 512, which we attribute to the reduced KV cache memory and the fact
    that smaller prompts and larger token generation numbers make the inference system
    more akin to the one-phase system that PipeEdge focuses on.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '接下来，我们在输入提示长度为128和最大生成令牌数$n=200$下进行实验。表[7](#S6.T7 "表 7 ‣ 6.6\. 短提示的服务 ‣ 6\.
    评估 ‣ LLM-PQ: 在异构集群上使用阶段感知分区和自适应量化进行LLM服务")显示，LLM-PQ在没有任何准确性下降的情况下显著加快了推理速度，甚至显示了准确性的提升。这确认了我们在LLM-PQ中的两阶段延迟建模的正确性。我们注意到LLM-PQ在集群4中的吞吐量增益远低于提示长度为512时，我们认为这归因于减少的KV缓存内存以及较小的提示和更大的令牌生成数量使得推理系统更类似于PipeEdge关注的一阶段系统。'
- en: Table 7\. Serving performance comparison under shorter prompts. The best results
    are marked in bold.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 短提示下的服务性能比较。最佳结果以**粗体**标记。
- en: '| Model | Cluster | Scheme | PPL | Latency(s) | Throughput (Token/s) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| Model | Cluster | Scheme | PPL | Latency(s) | Throughput (Token/s) |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| OPT-13b | 1 | PipeEdge | 11.23 | 84.80 | 75.47 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13b | 1 | PipeEdge | 11.23 | 84.80 | 75.47 |'
- en: '| Uniform | 11.23 | 84.80 | 75.47(1.00$\times$) |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 11.23 | 84.80 | 75.47(1.00$\times$) |'
- en: '| FlexGen | 11.22 | 119.24 | 53.68(0.71$\times$) |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 11.22 | 119.24 | 53.68(0.71$\times$) |'
- en: '| FlexGen-int8 | 11.23 | 80.35 | 79.65(1.06$\times$) |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 11.23 | 80.35 | 79.65(1.06$\times$) |'
- en: '| LLM-PQ | 11.23 | 47.63 | 134.38(1.78$\times$) |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 11.23 | 47.63 | 134.38(1.78$\times$) |'
- en: '| OPT-30b | 4 | PipeEdge | 10.70 | 366.54 | 17.46 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 4 | PipeEdge | 10.70 | 366.54 | 17.46 |'
- en: '| Uniform | 10.80 | 281.83 | 22.71(1.30$\times$) |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.80 | 281.83 | 22.71(1.30$\times$) |'
- en: '| FlexGen | 10.70 | 2,147.03 | 2.98(0.17$\times$) |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.70 | 2,147.03 | 2.98(0.17$\times$) |'
- en: '| FlexGen-int8 | 10.70 | 681.78 | 9.39(0.54$\times$) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.70 | 681.78 | 9.39(0.54$\times$) |'
- en: '| LLM-PQ | 10.70 | 262.34 | 24.40(1.40$\times$) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.70 | 262.34 | 24.40(1.40$\times$) |'
- en: '| OPT-66b | 6 | PipeEdge | 10.33 | 132.34 | 48.36 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 6 | PipeEdge | 10.33 | 132.34 | 48.36 |'
- en: '| Uniform | 10.33 | 298.99 | 21.41(0.44$\times$) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| Uniform | 10.33 | 298.99 | 21.41(0.44$\times$) |'
- en: '| FlexGen | 10.33 | 408.19 | 15.68(0.32$\times$) |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen | 10.33 | 408.19 | 15.68(0.32$\times$) |'
- en: '| FlexGen-int8 | 10.34 | 376.69 | 16.99(0.35$\times$) |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| FlexGen-int8 | 10.34 | 376.69 | 16.99(0.35$\times$) |'
- en: '| LLM-PQ | 10.30(-0.03) | 75.98 | 84.23(1.74$\times$) |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| LLM-PQ | 10.30(-0.03) | 75.98 | 84.23(1.74$\times$) |'
- en: 6.7\. Approaches Expediting Optimizer Algorithm
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7\. 加速优化器算法的方法
- en: Table 8\. Effectiveness of Grouping and Heuristic approaches under time limit.
    The best results are marked in bold.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 表8\. 分组和启发式方法在时间限制下的有效性。最佳结果以粗体标出。
- en: '| Model | Cluster | Method | Throughput (token/s) | Overhead (s) |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 集群 | 方法 | 吞吐量 (token/s) | 开销 (s) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| OPT-30b | 3 | Group=2 | 39.70 | 1.07 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 3 | Group=2 | 39.70 | 1.07 |'
- en: '| Group=1 | 39.70(+0) | 3.29 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| Group=1 | 39.70(+0) | 3.29 |'
- en: '| Heuristic | 35.17 | 5.36 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 启发式 | 35.17 | 5.36 |'
- en: '| OPT-66b | 6 | Group=2 | 39.56 | 2.70 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 6 | Group=2 | 39.56 | 2.70 |'
- en: '| Group=1 | 44.93(+5.37) | 19.14 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| Group=1 | 44.93(+5.37) | 19.14 |'
- en: '| Heuristic | 28.45 | 7.70 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 启发式 | 28.45 | 7.70 |'
- en: '| OPT-30b | 4 | Group=2 | 14.72 | 12.29 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| OPT-30b | 4 | Group=2 | 14.72 | 12.29 |'
- en: '| Group=1 | 13.93(-0.79) | 204.59 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| Group=1 | 13.93(-0.79) | 204.59 |'
- en: '| Heuristic | 14.94(+0.22) | 1.99 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 启发式 | 14.94(+0.22) | 1.99 |'
- en: '| OPT-66b | 10 | Group=2 | 16.64 | 59.27 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| OPT-66b | 10 | Group=2 | 16.64 | 59.27 |'
- en: '| Group=1 | 17.57(+0.93) | 127.28 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| Group=1 | 17.57(+0.93) | 127.28 |'
- en: '| Heuristic | 17.97(+1.33) | 2.11 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 启发式 | 17.97(+1.33) | 2.11 |'
- en: In LLM-PQ, we provide two approaches, layer grouping, and a heuristic, to reduce
    and the complexity of the optimizer’s bitwidth selection, model partition, and
    placement. We evaluate the inference throughput and the time required to derive
    the solution when applying three strategies (group = 2, group = 1, and heuristic),
    on clusters 3, 4, 6, and 10\. group = 2 means group 2 decoder layers together
    for decision. We set a 60-second time limit for the ILP solver.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在LLM-PQ中，我们提供了两种方法，即层分组和启发式，以减少优化器的位宽选择、模型分区和放置的复杂性。我们在集群3、4、6和10上应用三种策略（group
    = 2、group = 1和启发式），评估推理吞吐量和推导解决方案所需的时间。group = 2表示将2层解码器层分组进行决策。我们为ILP求解器设定了60秒的时间限制。
- en: 'Group = 1 covers the entire solution space and typically produces better results
    compared to group = 2 (on clusters 6 and 10), but it introduces a larger overhead,
    as shown in Table [8](#S6.T8 "Table 8 ‣ 6.7\. Approaches Expediting Optimizer
    Algorithm ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with
    Phase-Aware Partition and Adaptive Quantization"). On cluster 4, group = 1 cannot
    find a good solution within the time limit. On cluster 3, group = 1 and group
    = 2 produce the same solution. Performance of the heuristic largely depends on
    the starting point produced by adabits (start point of optimization #3 in Sec. [4.3](#S4.SS3
    "4.3\. Optimizer ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization")). It leads to
    the best throughput with the smallest overhead in clusters 4 and 10.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 'Group = 1涵盖了整个解决空间，通常比group = 2（在集群6和10上）产生更好的结果，但它引入了更大的开销，如表[8](#S6.T8 "表8
    ‣ 6.7\. 加速优化器算法的方法 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上进行LLM服务，采用阶段感知分区和自适应量化")所示。在集群4上，group
    = 1无法在时间限制内找到一个好的解决方案。在集群3上，group = 1和group = 2产生相同的解决方案。启发式的性能在很大程度上依赖于adabits生成的起始点（第[4.3](#S4.SS3
    "4.3\. 优化器 ‣ 4\. 分配器设计 ‣ LLM-PQ: 在异构集群上进行LLM服务，采用阶段感知分区和自适应量化")节优化#3的起始点）。它在集群4和10中导致了最佳吞吐量和最小开销。'
- en: 'We highlight the utilization of heuristics significantly enhances the scalability
    of LLM-PQ in offline workloads: solving time of a cluster comprising two P100,
    V100, and A100 GPUs each for OPT66B is reduced to 31s.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强调，启发式的使用显著提升了LLM-PQ在离线工作负载中的可扩展性：由两个P100、V100和A100 GPU组成的集群在OPT66B的求解时间减少到31秒。
- en: 6.8\. Parameter Sensitivity
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.8\. 参数敏感性
- en: '![Refer to caption](img/f275d8bbaf67e18223bc1ef3c1cb5329.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f275d8bbaf67e18223bc1ef3c1cb5329.png)'
- en: (a) Cluster 9 OPT-30b
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 集群9 OPT-30b
- en: '![Refer to caption](img/24cf767685caa45db83b9e3fd617a955.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/24cf767685caa45db83b9e3fd617a955.png)'
- en: (b) Cluster 5 OPT-66b
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 集群5 OPT-66b
- en: Figure 8\. Sensitivity experiments on $\theta$.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. 关于$\theta$的敏感性实验。
- en: We next investigate the impact of user quality scalar $\theta$ generally results
    in lower inference throughput and higher model accuracy, as less weight is placed
    on inference latency and more on model quality in our ILP optimization.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们研究了用户质量标量$\theta$的影响，通常会导致较低的推理吞吐量和更高的模型准确性，因为在我们的ILP优化中，推理延迟的权重较少，模型质量的权重较多。
- en: 6.9\. Comparison with Pure Adaptive Quantization
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.9\. 与纯自适应量化的比较
- en: '![Refer to caption](img/e906cb128cd8412dbd1cffbe141a6356.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e906cb128cd8412dbd1cffbe141a6356.png)'
- en: Figure 9\. Comparison with pure adaptive quantization.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 图9\. 与纯自适应量化的比较。
- en: 'To verify the significance of concurrently considering adaptive bitwidth, layer
    partitioning, and micro-batch sizing, we further compare LLM-PQ with adabits used
    in the heuristic method. We evaluate the performance of adabits with same model
    setup on clusters 3, 5, and 6, 9 with prompt length 512 and on cluster 4 with
    prompt length 128\. In Fig. [9](#S6.F9 "Figure 9 ‣ 6.9\. Comparison with Pure
    Adaptive Quantization ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization") LLM-PQ outperforms
    adabits in all selected cases.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '为了验证同时考虑自适应比特宽度、层分区和微批处理大小的重要性，我们进一步将LLM-PQ与启发式方法中使用的adabits进行比较。我们在集群3、5和6、9（提示长度为512）以及集群4（提示长度为128）上评估了相同模型设置下adabits的性能。在图[9](#S6.F9
    "Figure 9 ‣ 6.9\. Comparison with Pure Adaptive Quantization ‣ 6\. Evaluation
    ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and
    Adaptive Quantization")中，LLM-PQ在所有选定的案例中都优于adabits。'
- en: 7\. Discussions
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7. 讨论
- en: Search for Tensor Parallelization. We did not incorporate tensor parallelism
    in our serving system implementation due to the favorable characteristics of the
    pipeline when dealing with heterogeneity, which results in reduced communication
    requirements. It can be readily included in our search space. Tensor parallelism
    heavily relies on the 2-d device mesh configuration, and tensor sharding strategies
    can be searched based on the device mesh enumeration. Given 2 nodes with 8 GPUs
    per node (totaling 16 devices), we can represent them as a device mesh of size
    2×8, 1×16, 4×4, 8×2, or 16×1, where the device communication with different bandwidths
    for the first and second-dimension, and the tensor-parallel can apply along either
    the first or second dimension ([zheng2022alpa,](#bib.bib41) ). As the possible
    device mesh is limited, it is similar to how we enumerate all possible 1-d device
    orderings. For the above reason, we can view the device along the tensor-parallel
    dimension as a new device with larger memory and different kernel performance
    (as tensor-parallel will introduce some communication overhead), and it is still
    a 1-d partition problem along another axis, which conforms to our solutions.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索张量并行化。由于在处理异质性时，管道具有良好的特性，从而减少了通信需求，我们没有在我们的服务系统实现中加入张量并行性。张量并行性可以很容易地包含在我们的搜索空间中。张量并行性高度依赖于2维设备网格配置，而张量分片策略可以根据设备网格枚举进行搜索。给定2个节点，每个节点8个GPU（共16个设备），我们可以将它们表示为大小为2×8、1×16、4×4、8×2或16×1的设备网格，其中设备通信具有不同带宽的第一和第二维度，并且张量并行可以沿着第一或第二维度应用（[zheng2022alpa](#bib.bib41)）。由于可能的设备网格有限，这类似于我们枚举所有可能的1维设备排序。因此，我们可以将沿张量并行维度的设备视为具有更大内存和不同内核性能的新设备（因为张量并行会引入一些通信开销），这仍然是沿另一个轴的1维分区问题，符合我们的解决方案。
- en: Other Quantization Schemes There is rapid development in quantization methods
    for LLM. The latest weight-only quantization methods, such as AWQ ([lin2023awq,](#bib.bib21)
    ), SpQR ([dettmers2023spqr,](#bib.bib8) ) and QLoRA ([dettmers2023qlora,](#bib.bib7)
    ), AWQ improves kernel efficiency through re-order free quantization and utilizes
    TensorCore. SpQR improves the accuracy of GPTQ through better outlier detection.
    QLoRA proposes a memory-efficient 4-bit finetuning method and introduces double
    quantization to further reduce the memory footprint by quantizing the scalars
    used in quantization. LLM-PQ views these schemes as candidate quantization schemes,
    and these new schemes can be efficiently integrated into our system.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 其他量化方案 在大规模语言模型（LLM）的量化方法中有着迅速的发展。最新的仅权重量化方法，如AWQ（[lin2023awq](#bib.bib21)）、SpQR（[dettmers2023spqr](#bib.bib8)）和QLoRA（[dettmers2023qlora](#bib.bib7)），AWQ通过无重排序量化提高了内核效率，并利用了TensorCore。SpQR通过更好的异常值检测提高了GPTQ的准确性。QLoRA提出了一种内存高效的4-bit微调方法，并引入了双重量化，通过量化用于量化的标量进一步减少内存占用。LLM-PQ将这些方案视为候选量化方案，这些新方案可以高效地集成到我们的系统中。
- en: Apply to ORCA or vLLM ORCA ([orca,](#bib.bib38) ) introduces iterative-level
    scheduling, while vLLM ([vllm,](#bib.bib20) ) possesses an efficient page-attention
    technology for memory management. LLM-PQ’s design is orthogonal to both of them.
    However, unlike the offline task, the online workload is unpredictable, and the
    available paged memory for Key-Value (KV) storage is affected by quantization
    level. While the available memory plays a crucial role in influencing throughput
    when confronted with an infinite number of requests, there is always a trade-off
    between the speed of quantized operators and the amount of available memory. This
    trade-off necessitates new design considerations for performance optimization
    when implementing LLM-PQ at runtime.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 申请到 ORCA 或 vLLM ORCA ([orca,](#bib.bib38)) 引入了迭代级调度，而 vLLM ([vllm,](#bib.bib20))
    具有高效的页面注意力技术用于内存管理。LLM-PQ 的设计与它们正交。然而，与离线任务不同，在线工作负载是不可预测的，可用于 Key-Value (KV)
    存储的分页内存受到量化级别的影响。虽然可用内存在面对无限数量请求时对吞吐量有关键影响，但量化操作的速度与可用内存之间总是存在权衡。这种权衡需要在运行时实现
    LLM-PQ 时进行性能优化的新设计考虑。
- en: 8\. Conclusion
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 结论
- en: We propose LLM-PQ, an efficient system for LLM serving atop heterogeneous clusters.
    We derive efficient cost models to accurately predict memory occupation and execution
    latency of mixed-precision LLM serving. We introduce adaptive mixed-precision
    into the search space of pipeline serving and proposed an efficient indicator
    to guide bitwidth selection in the search process. We jointly consider serving
    latency in different token generation phases based on various precision settings,
    micro-batch sizes, and layer partitions, and derive efficient optimized solutions.
    Our extensive experiments validate the performance of LLM-PQ on a variety of cluster
    setups, which surpasses state-of-the-art approaches of serving LLM on heterogeneous
    clusters.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了 LLM-PQ，一个高效的系统用于在异构集群上服务 LLM。我们推导了高效的成本模型，以准确预测混合精度 LLM 服务的内存占用和执行延迟。我们将自适应混合精度引入管道服务的搜索空间，并提出了一种高效的指标来指导搜索过程中的位宽选择。我们综合考虑了不同令牌生成阶段的服务延迟，基于各种精度设置、微批大小和层分区，推导出高效的优化解决方案。我们的大量实验验证了
    LLM-PQ 在多种集群设置上的性能，超越了在异构集群上服务 LLM 的最先进方法。
- en: References
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan,
    Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, and
    Yuxiong He. Deepspeed- inference: Enabling efficient inference of transformer
    models at unprecedented scale. SC22: International Conference for High Performance
    Computing, Networking, Storage and Analysis, pages 1–15, 2022.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan,
    Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, 和 Yuxiong
    He。Deepspeed- inference: 以空前的规模实现变压器模型的高效推理。SC22: 高性能计算、网络、存储与分析国际会议，页面 1–15,
    2022。'
- en: '[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi.
    Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth
    AAAI Conference on Artificial Intelligence, 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, 和 Yejin Choi。Piqa:
    以自然语言推理物理常识。在第三十四届 AAAI 人工智能会议，2020。'
- en: '[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon
    Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners. ArXiv, abs/2005.14165, 2020.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
    Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
    Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon
    Child, Aditya Ramesh, Daniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, 和 Dario
    Amodei。语言模型是少样本学习者。ArXiv, abs/2005.14165, 2020。'
- en: '[4] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael
    Mahoney, and Joseph Gonzalez. Actnn: Reducing training memory footprint via 2-bit
    activation compressed training. In International Conference on Machine Learning,
    2021.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael
    Mahoney, 和 Joseph Gonzalez。Actnn: 通过2-bit激活压缩训练减少训练内存占用。在国际机器学习会议，2021。'
- en: '[5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv:1803.05457v1, 2018.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal,
    Carissa Schoenick, 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试 arc，AI2 推理挑战。arXiv:1803.05457v1,
    2018。'
- en: '[6] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer. Llm. int8
    (): 用于大规模变换器的 8 位矩阵乘法。arXiv 预印本 arXiv:2208.07339, 2022。'
- en: '[7] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer. Qlora：高效的量化
    LLM 微调。arXiv 预印本 arXiv:2305.14314, 2023。'
- en: '[8] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias
    Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, and Dan Alistarh.
    Spqr: A sparse-quantized representation for near-lossless llm weight compression.
    ArXiv, abs/2306.03078, 2023.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias
    Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, 和 Dan Alistarh.
    Spqr：一种稀疏量化表示，用于近乎无损的 LLM 权重压缩。ArXiv, abs/2306.03078, 2023。'
- en: '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert:
    Pre-training of deep bidirectional transformers for language understanding. arXiv
    preprint arXiv:1810.04805, 2018.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, 和 Kristina Toutanova. Bert：深度双向变换器在语言理解中的预训练。arXiv
    预印本 arXiv:1810.04805, 2018。'
- en: '[10] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, and Kurt Keutzer.
    Hawq: Hessian aware quantization of neural networks with mixed-precision. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney, 和 Kurt Keutzer.
    Hawq：具备海森矩阵感知的混合精度神经网络量化。在 IEEE/CVF 国际计算机视觉会议（ICCV）论文集，2019年10月。'
- en: '[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929, 2020.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly 等人。图像胜过 16x16 个词：用于大规模图像识别的变换器。arXiv 预印本 arXiv:2010.11929, 2020。'
- en: '[12] Jiangsu Du, Ziming Liu, Jiarui Fang, Shenggui Li, Yongbin Li, Yutong Lu,
    and Yang You. Energonai: An inference system for 10-100 billion parameter transformer
    models. arXiv preprint arXiv:2209.02341, 2022.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Jiangsu Du, Ziming Liu, Jiarui Fang, Shenggui Li, Yongbin Li, Yutong Lu,
    和 Yang You. Energonai：一个用于 10-100 亿参数变换器模型的推理系统。arXiv 预印本 arXiv:2209.02341, 2022。'
- en: '[13] Hugging Face. Text generation inference. [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference),
    n.d. Accessed on: July 24, 2023.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Hugging Face. 文本生成推理。 [https://github.com/huggingface/text-generation-inference](https://github.com/huggingface/text-generation-inference),
    未注明日期。访问日期：2023年7月24日。'
- en: '[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. ArXiv,
    abs/2210.17323, 2022.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh. Gptq：生成预训练变换器的准确后训练量化。ArXiv,
    abs/2210.17323, 2022。'
- en: '[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ:
    Accurate quantization for generative pre-trained transformers. In The Eleventh
    International Conference on Learning Representations, 2023.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh. OPTQ：生成预训练变换器的准确量化。在第十一届国际学习表示会议上，2023年。'
- en: '[16] Gurobi Optimization, LLC. Gurobi Optimizer Reference Manual, 2023.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Gurobi Optimization, LLC. Gurobi 优化器参考手册, 2023。'
- en: '[17] Yang Hu, Connor Imes, Xuanang Zhao, Souvik Kundu, Peter A. Beerel, Stephen P.
    Crago, and John Paul Walters. Pipeline parallelism for inference on heterogeneous
    edge computing. ArXiv, abs/2110.14895, 2021.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Yang Hu, Connor Imes, Xuanang Zhao, Souvik Kundu, Peter A. Beerel, Stephen
    P. Crago, 和 John Paul Walters. 异构边缘计算上的推理管道并行。ArXiv, abs/2110.14895, 2021。'
- en: '[18] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia
    Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient
    training of giant neural networks using pipeline parallelism. Advances in neural
    information processing systems, 32, 2019.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia
    Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu 等人。Gpipe：使用管道并行技术有效训练巨型神经网络。神经信息处理系统进展，32，2019。'
- en: '[19] Fred Jelinek, Robert L Mercer, Lalit R Bahl, and James K Baker. Perplexity—a
    measure of the difficulty of speech recognition tasks. The Journal of the Acoustical
    Society of America, 62(S1):S63–S63, 1977.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Fred Jelinek、Robert L Mercer、Lalit R Bahl 和 James K Baker。困惑度——语音识别任务难度的衡量标准。《美国声学学会杂志》，62(S1):S63–S63，1977年。'
- en: '[20] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao
    Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for
    large language model serving with pagedattention. In Proceedings of the 29th Symposium
    on Operating Systems Principles, 2023.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Woosuk Kwon、Zhuohan Li、Siyuan Zhuang、Ying Sheng、Lianmin Zheng、Cody Hao
    Yu、Joseph Gonzalez、Hao Zhang 和 Ion Stoica。针对大型语言模型服务的高效内存管理与分页注意力。在第29届操作系统原理研讨会论文集，2023年。'
- en: '[21] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    ArXiv, abs/2306.00978, 2023.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Ji Lin、Jiaming Tang、Haotian Tang、Shang Yang、Xingyu Dang 和 Song Han。Awq：针对
    LLM 压缩和加速的激活感知权重量化。ArXiv，abs/2306.00978，2023年。'
- en: '[22] Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, and Xia Hu. Exact:
    Scalable graph neural networks training via extreme activation compression. In
    International Conference on Learning Representations, 2022.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Zirui Liu、Kaixiong Zhou、Fan Yang、Li Li、Rui Chen 和 Xia Hu。Exact：通过极端激活压缩进行可扩展图神经网络训练。在国际学习表征会议，2022年。'
- en: '[23] Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. Building
    a large annotated corpus of English: The Penn Treebank. Computational Linguistics,
    19(2):313–330, 1993.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Mitchell P. Marcus、Beatrice Santorini 和 Mary Ann Marcinkiewicz。构建大型英文注释语料库：Penn
    Treebank。《计算语言学》，19(2):313–330，1993年。'
- en: '[24] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models, 2016.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Stephen Merity、Caiming Xiong、James Bradbury 和 Richard Socher。指针哨兵混合模型，2016年。'
- en: '[25] NVIDIA. Fastertransformer: Transformer related optimization, including
    bert, gpt, n.d.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] NVIDIA。Fastertransformer：与变换器相关的优化，包括 BERT、GPT，无日期。'
- en: '[26] Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Ngoc Quan Pham,
    Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández.
    The LAMBADA dataset: Word prediction requiring a broad discourse context. In Proceedings
    of the 54th Annual Meeting of the Association for Computational Linguistics (Volume
    1: Long Papers), August 2016.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Denis Paperno、Germán Kruszewski、Angeliki Lazaridou、Ngoc Quan Pham、Raffaella
    Bernardi、Sandro Pezzelle、Marco Baroni、Gemma Boleda 和 Raquel Fernández。LAMBADA
    数据集：需要广泛话语上下文的词预测。在第54届计算语言学协会年会论文集（第1卷：长篇论文），2016年8月。'
- en: '[27] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
    Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,
    Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank
    Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch:
    An imperative style, high-performance deep learning library. In Neural Information
    Processing Systems, 2019.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Adam Paszke、Sam Gross、Francisco Massa、Adam Lerer、James Bradbury、Gregory
    Chanan、Trevor Killeen、Zeming Lin、Natalia Gimelshein、Luca Antiga、Alban Desmaison、Andreas
    Köpf、Edward Yang、Zach DeVito、Martin Raison、Alykhan Tejani、Sasank Chilamkurthy、Benoit
    Steiner、Lu Fang、Junjie Bai 和 Soumith Chintala。Pytorch：一种命令式风格的高性能深度学习库。在神经信息处理系统大会，2019年。'
- en: '[28] Aaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang,
    Fog Dong, Xipeng Guan, and Frost Ming. OpenLLM: Operating LLMs in production,
    2023.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Aaron Pham、Chaoyu Yang、Sean Sheng、Shenyang Zhao、Sauyon Lee、Bo Jiang、Fog
    Dong、Xipeng Guan 和 Frost Ming。OpenLLM：生产环境中的 LLM 操作，2023年。'
- en: '[29] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of
    transfer learning with a unified text-to-text transformer. arXiv e-prints, 2019.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Colin Raffel、Noam Shazeer、Adam Roberts、Katherine Lee、Sharan Narang、Michael
    Matena、Yanqi Zhou、Wei Li 和 Peter J. Liu。通过统一的文本到文本转换器探索迁移学习的极限。arXiv e-prints，2019年。'
- en: '[30] RyokoAI. Sharegpt52k. [https://huggingface.co/datasets/RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K),
    2021. Dataset accessed on [insert date].'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] RyokoAI。Sharegpt52k。 [https://huggingface.co/datasets/RyokoAI/ShareGPT52K](https://huggingface.co/datasets/RyokoAI/ShareGPT52K)，2021年。数据集访问日期
    [插入日期]。'
- en: '[31] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
    ArXiv, abs/2211.05100, 2022.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Teven Le Scao、Angela Fan、Christopher Akiki、Ellie Pavlick、Suzana Ilić、Daniel
    Hesslow、Roman Castagné、Alexandra Sasha Luccioni、François Yvon、Matthias Gallé 等。Bloom：一个
    176b 参数的开放访问多语言模型。ArXiv，abs/2211.05100，2022年。'
- en: '[32] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi
    Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. Flexgen: High-throughput
    generative inference of large language models with a single gpu. In Proceedings
    of the 40th International Conference on Machine Learning, 2023.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi
    Chen, Percy Liang, Christopher Ré, Ion Stoica 和 Ce Zhang。Flexgen：利用单个 GPU 进行大规模语言模型的高吞吐量生成推理。发表于第40届国际机器学习会议，2023。'
- en: '[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aur’elien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
    Llama: Open and efficient foundation language models. ArXiv, abs/2302.13971, 2023.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, Aurélien Rodriguez, Armand Joulin, Edouard Grave 和 Guillaume Lample。Llama：开放和高效的基础语言模型。ArXiv，abs/2302.13971，2023。'
- en: '[34] Borui Wan, Jun Zhao, and Chuan Wu. Adaptive message quantization and parallelization
    for distributed full-graph gnn training. ArXiv, abs/2306.01381, 2023.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Borui Wan, Jun Zhao 和 Chuan Wu. 自适应消息量化和并行化用于分布式全图 GNN 训练。ArXiv，abs/2306.01381，2023。'
- en: '[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven
    Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.
    Transformers: State-of-the-Art Natural Language Processing. In Proceedings of
    the 2020 Conference on Empirical Methods in Natural Language Processing: System
    Demonstrations, 2020.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
    Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven
    Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest 和 Alexander M. Rush。Transformers：最先进的自然语言处理。在2020年自然语言处理经验方法会议论文集中：系统演示，2020。'
- en: '[36] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song
    Han. Smoothquant: Accurate and efficient post-training quantization for large
    language models. In International Conference on Machine Learning, 2023.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth 和 Song
    Han. Smoothquant：大规模语言模型的准确高效后训练量化。发表于国际机器学习会议，2023。'
- en: '[37] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. In Advances in Neural Information Processing Systems,
    2022.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li 和 Yuxiong He。Zeroquant：大规模变换器的高效且经济的后训练量化。发表于神经信息处理系统进展，2022。'
- en: '[38] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim, and Byung-Gon
    Chun. Orca: A distributed serving system for Transformer-Based generative models.
    In 16th USENIX Symposium on Operating Systems Design and Implementation (OSDI
    22), 2022.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Gyeong-In Yu, Joo Seong Jeong, Geon-Woo Kim, Soojeong Kim 和 Byung-Gon
    Chun。Orca：用于基于变换器生成模型的分布式服务系统。在第16届USENIX操作系统设计与实现研讨会（OSDI 22），2022。'
- en: '[39] Biao Zhang and Rico Sennrich. Root Mean Square Layer Normalization. In
    Advances in Neural Information Processing Systems 32, 2019.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Biao Zhang 和 Rico Sennrich。均方根层归一化。在神经信息处理系统进展32中，2019。'
- en: '[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
    Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
    Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer
    language models. ArXiv, abs/2205.01068, 2022.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov,
    Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali
    Sridhar, Tianlu Wang 和 Luke Zettlemoyer。Opt：开放的预训练变换器语言模型。ArXiv，abs/2205.01068，2022。'
- en: '[41] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
    Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing, et al. Alpa: Automating
    inter-and $\{$ parallelism for distributed deep learning. In 16th USENIX Symposium
    on Operating Systems Design and Implementation (OSDI 22), 2022.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
    Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Eric P Xing 等人。Alpa：自动化的分布式深度学习的交叉和$\{$并行性。在第16届USENIX操作系统设计与实现研讨会（OSDI
    22），2022。'
- en: Appendix A Appendix
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 附录
- en: 'A.1\. Proof of Theorem [1](#S4.Thmtheorem1 "Theorem 1\. ‣ 4.2\. Indicator of
    Model Perturbation by Quantization ‣ 4\. Assigner Design ‣ LLM-PQ: Serving LLM
    on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")'
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 'A.1\. 定理证明 [1](#S4.Thmtheorem1 "定理 1\. ‣ 4.2\. 量化模型扰动的指示符 ‣ 4\. 分配器设计 ‣ LLM-PQ:
    在异构集群上提供LLM，带有相位感知分区和自适应量化")'
- en: Proof.
  id: totrans-319
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 证明。
- en: Let $\mathbf{X}$.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $\mathbf{X}$。
- en: In deterministic rounding [[36](#bib.bib36), [14](#bib.bib14)], quantized scalar
    can be either $\hat{w}=\lfloor\frac{w-q_{w}}{s_{w}}\rfloor$.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定性舍入[[36](#bib.bib36), [14](#bib.bib14)]中，量化标量可以是 $\hat{w}=\lfloor\frac{w-q_{w}}{s_{w}}\rfloor$。
- en: For stochastic rounding [[4](#bib.bib4), [22](#bib.bib22)], scalar $\hat{w}=\lfloor\frac{w-q_{w}}{s_{w}}\rfloor$
    ∎
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机舍入[[4](#bib.bib4), [22](#bib.bib22)]，标量 $\hat{w}=\lfloor\frac{w-q_{w}}{s_{w}}\rfloor$
    ∎
- en: A.2\. Experiment
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2\. 实验
- en: A.2.1\. $\theta$ and Solver Setup
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1\. $\theta$ 和解算器设置
- en: 'Table 9\. Solver setups for Table [4](#S6.T4 "Table 4 ‣ 6.3\. Serving in Heterogeneous
    Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with
    Phase-Aware Partition and Adaptive Quantization") and  [5](#S6.T5 "Table 5 ‣ 6.4\.
    Serving in Homogeneous Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization")'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '表 9\. 解算器设置，见表 [4](#S6.T4 "表 4 ‣ 6.3\. 在异构集群上提供 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")
    和  [5](#S6.T5 "表 5 ‣ 6.4\. 在同质集群上提供 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")'
- en: '| Cluster | Group | Heuristic? | $\theta$ |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | 组 | 启发式？ | $\theta$ |'
- en: '| --- | --- | --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | 1 | N | 1 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | N | 1 |'
- en: '| 2 | 1 | N | 1 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | N | 1 |'
- en: '| 3 | 1 | N | 1 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | N | 1 |'
- en: '| 4 | - | Y | 1000 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 4 | - | Y | 1000 |'
- en: '| 5 | - | Y | 50 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 5 | - | Y | 50 |'
- en: '| 6 | 1 | N | 100 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1 | N | 100 |'
- en: '| 7 | 1 | N | 10 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 1 | N | 10 |'
- en: '| 8 | 1 | N | 10 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 1 | N | 10 |'
- en: '| 9 | 1 | N | 1 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 1 | N | 1 |'
- en: '| 10 | - | Y | 1 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 10 | - | Y | 1 |'
- en: '| 11 | - | Y | 10 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 11 | - | Y | 10 |'
- en: 'Table [9](#A1.T9 "Table 9 ‣ A.2.1\. 𝜃 and Solver Setup ‣ A.2\. Experiment ‣
    Appendix A Appendix ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization") provides the $\theta$ and solver configurations
    used in both hetero- and homogeneous results for LLM-PQ.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [9](#A1.T9 "表 9 ‣ A.2.1\. 𝜃 和解算器设置 ‣ A.2\. 实验 ‣ 附录 A 附录 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")
    提供了用于异构和同质结果的 $\theta$ 和解算器配置，适用于 LLM-PQ。'
- en: A.2.2\. Overhead Table
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.2\. 开销表
- en: 'Table 10\. Problem solving overhead for Table [4](#S6.T4 "Table 4 ‣ 6.3\. Serving
    in Heterogeneous Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving LLM on Heterogeneous
    Clusters with Phase-Aware Partition and Adaptive Quantization") and  [5](#S6.T5
    "Table 5 ‣ 6.4\. Serving in Homogeneous Clusters ‣ 6\. Evaluation ‣ LLM-PQ: Serving
    LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization")'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10\. 问题解决开销，见表 [4](#S6.T4 "表 4 ‣ 6.3\. 在异构集群上提供 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")
    和  [5](#S6.T5 "表 5 ‣ 6.4\. 在同质集群上提供 ‣ 6\. 评估 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")'
- en: '| Cluster | Overhead(s) |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 聚类 | 开销 |'
- en: '| --- | --- |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | 0.2977 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.2977 |'
- en: '| 2 | 0.2977 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.2977 |'
- en: '| 3 | 2.78127 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2.78127 |'
- en: '| 4 | 2.28628 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 2.28628 |'
- en: '| 5 | 9.9239153 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 9.9239153 |'
- en: '| 6 | 115.981 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 115.981 |'
- en: '| 7 | 44.3031 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 44.3031 |'
- en: '| 8 | 19.31674 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 19.31674 |'
- en: '| 9 | 1.15838 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 1.15838 |'
- en: '| 10 | 2.45544 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 2.45544 |'
- en: '| 11 | 3.4 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 3.4 |'
- en: '| AVG | 18.38195685 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 18.38195685 |'
- en: '| SLOWEST | 115.981 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 最慢 | 115.981 |'
- en: 'Table [10](#A1.T10 "Table 10 ‣ A.2.2\. Overhead Table ‣ A.2\. Experiment ‣
    Appendix A Appendix ‣ LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware
    Partition and Adaptive Quantization") presents the solving latency of both hetero-
    and homogeneous results for LLM-PQ. We also provide a data point for the three-nodes
    cluster: Cluster of P100, V100, and A100 GPUs (two each type): solving time with
    31s for OPT66B using heuristic.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [10](#A1.T10 "表 10 ‣ A.2.2\. 开销表 ‣ A.2\. 实验 ‣ 附录 A 附录 ‣ LLM-PQ: 在异构集群上提供LLM，带有相位感知分区和自适应量化")
    展示了 LLM-PQ 在异构和同质结果中的求解延迟。我们还提供了一个三节点集群的数据点：由 P100、V100 和 A100 GPU 组成的集群（每种类型各两个）：使用启发式方法求解
    OPT66B 所需时间为 31 秒。'
