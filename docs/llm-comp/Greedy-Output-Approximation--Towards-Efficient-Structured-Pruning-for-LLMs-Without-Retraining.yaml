- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-08 19:04:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 19:04:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs
    Without Retraining'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贪婪输出近似：面向无重新训练的高效结构化剪枝
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19126](https://ar5iv.labs.arxiv.org/html/2407.19126)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2407.19126](https://ar5iv.labs.arxiv.org/html/2407.19126)
- en: 'Jianwei Li Department of Computer Science at North Carolina State University,
    Email: jli265@ncsu.edu    Yijun Dong Courant Institute of Mathematical Sciences
    at New York University, Email: yd1319@nyu.edu    Qi Lei Center for Data Science
    at New York University, Email: ql518@nyu.edu'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Jianwei Li 北卡罗来纳州立大学计算机科学系，电子邮件：jli265@ncsu.edu    Yijun Dong 纽约大学Courant数学科学研究所，电子邮件：yd1319@nyu.edu
       Qi Lei 纽约大学数据科学中心，电子邮件：ql518@nyu.edu
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: To remove redundant components of large language models (LLMs) without incurring
    significant computational costs, this work focuses on single-shot pruning without
    a retraining phase. We simplify the pruning process for Transformer-based LLMs
    by identifying a depth-2 pruning structure that functions independently. Additionally,
    we propose two inference-aware pruning criteria derived from the optimization
    perspective of output approximation, which outperforms traditional training-aware
    metrics such as gradient and Hessian. We also introduce a two-step reconstruction
    technique to mitigate pruning errors without model retraining. Experimental results
    demonstrate that our approach significantly reduces computational costs and hardware
    requirements while maintaining superior performance across various datasets and
    models.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在不增加显著计算成本的情况下去除大型语言模型（LLMs）的冗余组件，本研究专注于无需重新训练阶段的单次剪枝。我们通过识别一个独立工作的深度2剪枝结构，简化了基于Transformer的LLMs的剪枝过程。此外，我们提出了两种从输出近似优化角度派生的推理感知剪枝标准，这些标准优于传统的训练感知指标，如梯度和海森矩阵。我们还引入了一种两步重建技术，以减轻剪枝误差而无需模型重新训练。实验结果表明，我们的方法在保持各种数据集和模型的优越性能的同时，大幅度降低了计算成本和硬件需求。
- en: 1 Introduction
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: With the development of LLMs displaying emergent capabilities like sophisticated
    reasoning, the focus of the community has shifted to models with billions of parameters,
    for example, GPT-4 and Llama2 Achiam et al. ([2023](#bib.bib1)); Touvron et al.
    ([2023b](#bib.bib56)). This transition introduces unprecedented computational
    costs both in the training and the inference phases Touvron et al. ([2023a](#bib.bib55));
    Le Scao et al. ([2023](#bib.bib34)); Team et al. ([2023](#bib.bib52)); Banks and
    Warkentin ([2024](#bib.bib5)). To address this challenge, pruning plays a constructive
    role by removing redundant components from models, thereby reducing computational
    costs Gordon et al. ([2020](#bib.bib22)); Prasanna et al. ([2020](#bib.bib46));
    Wang et al. ([2020](#bib.bib59)); Li et al. ([2023a](#bib.bib36)). Notably, designing
    an optimal pruning strategy is an NP-hard problem (as it reduces to subset selection)
    and requires balancing accuracy, sparsity, generalizability, pruning costs, and
    hardware compatibility in practice Xu et al. ([2021](#bib.bib61)); Li et al. ([2023b](#bib.bib37));
    Du et al. ([2023](#bib.bib12)). Traditional pruning methods primarily focus on
    accuracy and sparsity, often neglecting other key factors. They typically involve
    model retraining and knowledge distillation to mitigate pruning errors. However,
    with current LLMs featuring billions of parameters, the training process is already
    a significant challenge, making the additional cost of model retraining even more
    unaffordable Frantar and Alistarh ([2022](#bib.bib18), [2023](#bib.bib19)). Given
    these challenges, there’s a pressing need for more efficient pruning approaches.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大型语言模型（LLMs）展现出如复杂推理等新兴能力，社区的关注点已经转移到拥有数十亿参数的模型上，例如 GPT-4 和 Llama2 Achiam
    等（[2023](#bib.bib1)）；Touvron 等（[2023b](#bib.bib56)）。这种转变在训练和推理阶段都引入了前所未有的计算成本
    Touvron 等（[2023a](#bib.bib55)）；Le Scao 等（[2023](#bib.bib34)）；Team 等（[2023](#bib.bib52)）；Banks
    和 Warkentin（[2024](#bib.bib5)）。为了解决这个挑战，剪枝通过从模型中移除冗余组件，发挥了积极作用，从而减少计算成本 Gordon
    等（[2020](#bib.bib22)）；Prasanna 等（[2020](#bib.bib46)）；Wang 等（[2020](#bib.bib59)）；Li
    等（[2023a](#bib.bib36)）。值得注意的是，设计一个最佳剪枝策略是一个 NP-hard 问题（因为它归结为子集选择），需要在实际应用中平衡准确性、稀疏性、可推广性、剪枝成本和硬件兼容性
    Xu 等（[2021](#bib.bib61)）；Li 等（[2023b](#bib.bib37)）；Du 等（[2023](#bib.bib12)）。传统的剪枝方法主要关注准确性和稀疏性，通常忽视其他关键因素。它们通常涉及模型的重新训练和知识蒸馏，以减少剪枝错误。然而，由于当前的
    LLMs 拥有数十亿的参数，训练过程已经是一个重大挑战，使得模型重新训练的额外成本更加难以承受 Frantar 和 Alistarh（[2022](#bib.bib18),
    [2023](#bib.bib19)）。鉴于这些挑战，迫切需要更高效的剪枝方法。
- en: 'Recently, some works have focused on structured pruning on pre-trained LLMs,
    directly addressing hardware compatibility and generalizability. This approach
    allows them to concentrate on the remaining trade-off factors: sparsity, accuracy,
    and pruning cost. For example, methods like LLM-Pruner, Shortened LLaMA, and Sheared
    LLaMA use a single-shot pruning strategy that requires only one round of retraining Kim
    et al. ([2024](#bib.bib32)); Xia et al. ([2023](#bib.bib60)); Ma et al. ([2023](#bib.bib40)).
    On the other hand, strategies such as FLAP, OPTIN, Sliced GPT, LLM Surgeon, Wanda,
    ZipLM, and KRP seek to eliminate the need for model retraining entirely An et al.
    ([2024](#bib.bib2)); Ashkboos et al. ([2024](#bib.bib4)); van der Ouderaa et al.
    ([2023](#bib.bib57)); Sun et al. ([2023](#bib.bib51)); Kurtić et al. ([2024](#bib.bib33));
    Khaki and Plataniotis ([2024](#bib.bib31)); Li et al. ([2023b](#bib.bib37)). However,
    these approaches have respective limitations, such as high computational costs
    from the calculation of higher-order information, a lack of fully structured pruning
    patterns Pool et al. ([2021](#bib.bib45)), or compromised performance in some
    cases. The development of these methods marks a crucial phase in the evolution
    of LLMs, aiming to enhance model capabilities while ensuring computational efficiency.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，一些研究关注于在预训练的 LLM 上进行结构化修剪，直接解决了硬件兼容性和泛化能力问题。这种方法使他们能够集中于剩余的权衡因素：稀疏性、准确性和修剪成本。例如，LLM-Pruner、Shortened
    LLaMA 和 Sheared LLaMA 等方法使用了一次性修剪策略，只需进行一次重新训练 Kim 等人 ([2024](#bib.bib32)); Xia
    等人 ([2023](#bib.bib60)); Ma 等人 ([2023](#bib.bib40))。另一方面，FLAP、OPTIN、Sliced GPT、LLM
    Surgeon、Wanda、ZipLM 和 KRP 等策略则试图完全消除模型重新训练的需求 An 等人 ([2024](#bib.bib2)); Ashkboos
    等人 ([2024](#bib.bib4)); van der Ouderaa 等人 ([2023](#bib.bib57)); Sun 等人 ([2023](#bib.bib51));
    Kurtić 等人 ([2024](#bib.bib33)); Khaki 和 Plataniotis ([2024](#bib.bib31)); Li 等人
    ([2023b](#bib.bib37))。然而，这些方法也存在各自的局限性，例如由于计算高阶信息而导致的高计算成本、缺乏完全结构化的修剪模式 Pool 等人
    ([2021](#bib.bib45))，或在某些情况下性能受损。这些方法的发展标志着 LLM 发展的关键阶段，旨在在提升模型能力的同时确保计算效率。
- en: '![Refer to caption](img/25c7d8614995ab36271d5cd331ec7ab6.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/25c7d8614995ab36271d5cd331ec7ab6.png)'
- en: 'Figure 1: Pruning metric analysis from the optimization perspective A: Function
    Approximation; B: Output Approximation; C: Objective Approximation.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1：从优化角度分析修剪度量 A: 函数近似; B: 输出近似; C: 目标近似。'
- en: 'With the existing challenges, we call for a pruning strategy that better trades
    off the accuracy, structure preservation, and computational costs. We delve into
    this kind of strategy by answering the following essential questions:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 面对现有挑战，我们呼吁采用一种更好地权衡准确性、结构保留和计算成本的修剪策略。我们通过回答以下几个关键问题深入探讨这种策略：
- en: Question 1. Does an inherent pruning structure exist in Transformer-based language
    models?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 1. Transformer 基于语言模型中是否存在固有的修剪结构？
- en: We discovered depth-2 pruning modules within Transformer architecture by analyzing
    structured pruning from both input and output channels. These structures preserve
    feature knowledge while reducing pruning complexity from residual connections.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分析输入和输出通道的结构化修剪，我们在 Transformer 架构中发现了深度为2的修剪模块。这些结构在减少残差连接的修剪复杂性的同时，保留了特征知识。
- en: Question 2. Is there an effective pruning criterion that does not require training
    awareness?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 2. 是否存在不需要训练感知的有效修剪标准？
- en: We identified two efficient and high-performing inference-aware pruning metrics
    based on output approximation for Transformer models. They are simpler and more
    computationally efficient than training-aware metrics.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于 Transformer 模型的输出近似识别了两种高效且高性能的推理感知修剪度量指标。它们比训练感知度量指标更简单且计算效率更高。
- en: Question 3. Is there a low-computation performance recovery technique available?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 问题 3. 是否有低计算性能恢复技术可用？
- en: Inspired by layer-wise reconstruction Li et al. ([2023b](#bib.bib37)), we developed
    a two-step module reconstruction strategy. This method updates the weights of
    the depth-2 module without computing parameter gradients, effectively minimizing
    pruning errors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 受到层级重建 Li 等人 ([2023b](#bib.bib37)) 的启发，我们开发了一种两步模块重建策略。这种方法在不计算参数梯度的情况下更新深度为2的模块的权重，有效地最小化了修剪误差。
- en: 'Answering the above questions altogether, this paper proposes an integrated
    and efficient pruning strategy with a focus on Transformer-based LLMs Vaswani
    et al. ([2017](#bib.bib58)). Specifically, we categorize all pruning metrics into
    three groups based on their implicit purpose: function (weights) approximation,
    output approximation, and objective approximation. Following the output approximation
    route, we introduce a similarity-based pruning strategy that exploits the redundancy
    in multi-head attention mechanisms by removing heads that extract similar information
    first rather than those with minimal impact. Additionally, we propose a second-moment-based
    pruning approach also under the output approximation category, which stands out
    for its ability to integrate information across multiple layers. We apply this
    metric for both attention and feed-forward modules to remove the elements with
    minimal impact on the model’s performance. Finally, we develop an optimization
    technique that eliminates the need for higher-order information by greedily reducing
    pruning error through weight reconstruction of the subsequent dense module. Our
    structured pruning experiments on pre-trained LLMs ranging from millions to billions
    of parameters demonstrate that our method ensures generalizability, hardware compatibility,
    and minimal pruning cost. Moreover, it outperforms or achieves comparative performance
    to other non-retraining methods and even some methods that require retraining.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 针对上述问题，本文提出了一种集成且高效的剪枝策略，重点关注基于 Transformer 的 LLMs Vaswani 等人 ([2017](#bib.bib58))。具体来说，我们根据隐含目的将所有剪枝度量分为三组：功能（权重）近似、输出近似和目标近似。沿着输出近似路线，我们引入了一种基于相似性的剪枝策略，该策略通过优先移除提取类似信息的头而不是那些影响最小的头，从而利用多头注意力机制中的冗余。此外，我们还提出了一种基于二阶矩的剪枝方法，也属于输出近似类别，其优势在于能够整合跨多个层的信息。我们将这一度量应用于注意力和前馈模块，以移除对模型性能影响最小的元素。最后，我们开发了一种优化技术，通过权重重构后续稠密模块来贪婪地减少剪枝误差，从而消除了对高阶信息的需求。我们在从百万到十亿参数的预训练
    LLMs 上进行的结构化剪枝实验表明，我们的方法确保了泛化能力、硬件兼容性和最小的剪枝成本。此外，它的性能优于或与其他非重训练方法和一些需要重训练的方法相当。
- en: 2 Related Work
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: 'Pruning and Structured Pruning: Pruning is a technique used in machine learning
    to reduce the size of a model by eliminating unnecessary parameters, which can
    lead to reductions in storage requirements and computational complexity without
    significantly affecting the model’s performance Frantar and Alistarh ([2022](#bib.bib18));
    Frankle et al. ([2020](#bib.bib17)). This process involves identifying and removing
    the parts of the model that have the least impact on its output, such as weights
    in a neural network with small magnitudes. By doing so, pruning discovers a more
    efficient model that is faster to execute and easier to deploy on devices with
    limited resources. Structured pruning, a method that imposes more constraints,
    focuses on eliminating entire units or structures within the model, such as neurons,
    channels, or layers, instead of individual weights Anwar et al. ([2017](#bib.bib3));
    Fang et al. ([2023](#bib.bib15)). Being the focus of our paper, structured pruning
    is especially advantageous due to its compatibility with standard hardware, whereas
    unstructured pruning requires specially designed accelerators to deploy in practical
    scenarios.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝与结构化剪枝：剪枝是一种在机器学习中用于通过去除不必要的参数来减少模型大小的技术，这可以减少存储需求和计算复杂性，同时对模型性能影响不大 Frantar
    和 Alistarh ([2022](#bib.bib18)); Frankle 等人 ([2020](#bib.bib17))。这一过程涉及识别和移除对输出影响最小的模型部分，例如在神经网络中具有小幅度的权重。通过这样做，剪枝发现了一种更高效的模型，该模型执行速度更快，并且更容易在资源有限的设备上部署。结构化剪枝是一种施加更多约束的方法，专注于消除模型中的整个单元或结构，如神经元、通道或层，而不是单个权重
    Anwar 等人 ([2017](#bib.bib3)); Fang 等人 ([2023](#bib.bib15))。作为本文的重点，结构化剪枝特别有利于其与标准硬件的兼容性，而非结构化剪枝则需要专门设计的加速器来在实际场景中部署。
- en: 'Data-free/dependent and Training/Inference-aware Metrics: When choosing which
    redundant components to remove from a model, the selection is typically guided
    by specific metrics Hoefler et al. ([2021](#bib.bib28)). These metrics can be
    broadly divided into data-free and data-dependent categories, depending on whether
    they rely on specific datasets. Additionally, they can be categorized as training-aware
    or inference-aware, based on whether they require model backpropagation. This
    paper focuses on inference-aware metrics and explores both data-free and data-dependent
    versions.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 无数据/依赖数据的和训练/推理感知的度量：在选择从模型中移除哪些冗余组件时，选择通常由特定的度量标准指导 Hoefler et al. ([2021](#bib.bib28))。这些度量标准可以大致分为无数据和依赖数据类别，具体取决于它们是否依赖于特定的数据集。此外，它们还可以根据是否需要模型反向传播进行分类为训练感知或推理感知。本文重点关注推理感知度量，并探讨了无数据和依赖数据的版本。
- en: 'Efficient and Low-Resource Pruning: As the number of parameters in LLMs increases,
    the quest for efficient pruning has become paramount. Methods such as LLM-Pruner,
    Sheared LLaMA, and Shortened LLaMA adopt a single-shot pruning strategy Kim et al.
    ([2024](#bib.bib32)); Xia et al. ([2023](#bib.bib60)); Ma et al. ([2023](#bib.bib40)).
    Yet, these approaches depend on computationally expensive metrics and still require
    retraining to minimize pruning-induced errors. In contrast, methods like OPTIN,
    Sliced GPT, LLM Surgeo, ZipLM, and KRP eliminate the need for retraining but still
    rely heavily on computationally expensive second-order Hessian information, which
    is a significant drawback for large-dimensional models Ashkboos et al. ([2024](#bib.bib4));
    Sun et al. ([2023](#bib.bib51)); Kurtić et al. ([2024](#bib.bib33)); Khaki and
    Plataniotis ([2024](#bib.bib31)); Li et al. ([2023b](#bib.bib37)). Meanwhile,
    FLAP and Wanda design specific pruning metrics that share similar ideas with the
    methods from the pre-deep learning age Engelbrecht et al. ([1999](#bib.bib14));
    Sietsma and Dow ([1988](#bib.bib49)); Engelbrecht and Cloete ([1996](#bib.bib13));
    Thimm and Fiesler ([1995b](#bib.bib54)), significantly reducing computational
    demand An et al. ([2024](#bib.bib2)); van der Ouderaa et al. ([2023](#bib.bib57)).
    This paper proposes a method that eliminates the need for both model retraining
    and computationally expensive metrics, offering superior or comparative performance
    compared to other non-retraining methods and even some methods that require retraining.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 高效且低资源的剪枝：随着LLM参数数量的增加，追求高效剪枝已成为关键。诸如LLM-Pruner、Sheared LLaMA和Shortened LLaMA等方法采用了一次性剪枝策略 Kim
    et al. ([2024](#bib.bib32)); Xia et al. ([2023](#bib.bib60)); Ma et al. ([2023](#bib.bib40))。然而，这些方法依赖于计算开销大的度量标准，并且仍需要重新训练以最小化剪枝引起的误差。相比之下，OPTIN、Sliced
    GPT、LLM Surgeo、ZipLM和KRP等方法消除了重新训练的需要，但仍然严重依赖于计算开销大的二阶Hessian信息，这对于大维度模型来说是一个重大缺点 Ashkboos
    et al. ([2024](#bib.bib4)); Sun et al. ([2023](#bib.bib51)); Kurtić et al. ([2024](#bib.bib33));
    Khaki and Plataniotis ([2024](#bib.bib31)); Li et al. ([2023b](#bib.bib37))。同时，FLAP和Wanda设计了特定的剪枝度量，这些度量与深度学习前时代的方法有类似的思想 Engelbrecht
    et al. ([1999](#bib.bib14)); Sietsma and Dow ([1988](#bib.bib49)); Engelbrecht
    and Cloete ([1996](#bib.bib13)); Thimm and Fiesler ([1995b](#bib.bib54))，显著减少了计算需求 An
    et al. ([2024](#bib.bib2)); van der Ouderaa et al. ([2023](#bib.bib57))。本文提出了一种方法，消除了模型重新训练和计算开销大的度量的需求，与其他非重新训练方法甚至一些需要重新训练的方法相比，提供了优越或相当的性能。
- en: 3 Methodology
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 'This section outlines our structured pruning scheme, which consists of three
    key components: pruning structure recognition, pruning criteria definition, and
    post-pruning recovery strategy.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了我们的结构化剪枝方案，其中包括三个关键组件：剪枝结构识别、剪枝标准定义以及剪枝后恢复策略。
- en: 3.1 Pruning Structure Recognition
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 剪枝结构识别
- en: Our approach involves single-shot pruning and targets structured components,
    such as entire rows or columns of weight matrices, rather than individual weights.
    We do not discuss layer or block pruning, as it disrupts inherent model correlations
    and requires retraining to restore layer dependencies.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法涉及一次性剪枝，并针对结构化组件，例如权重矩阵的整行或整列，而不是单个权重。我们不讨论层或块剪枝，因为这会破坏模型的固有相关性，并需要重新训练以恢复层之间的依赖关系。
- en: 3.1.1 Input or Output Channel Pruning for Sequential Layers
  id: totrans-30
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 顺序层的输入或输出通道剪枝
- en: 'To clarify structured pruning, it is important to understand that pruning neurons
    can be approached in two directions: input channels and output channels. Consider
    a linear function $f(X)=XW$, also known as feature selection. This paper focuses
    on a static approach to feature selection, where the same channels are removed
    for all samples, making feature selection equivalent to output channel pruning
    in the previous layer. An interesting phenomenon arises: in depth-2 sequential
    linear layers, pruning the input channels of the second layer simultaneously pruning
    the output channels of the first layer, using identical pruning indices. In contrast,
    pruning the output channels of the second layer does not affect the first layer.
    Both input and output channel pruning contribute to model compression, but they
    may have different impacts on performance.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要澄清结构化剪枝，需要理解剪枝神经元可以从两个方向进行：输入通道和输出通道。考虑一个线性函数 $f(X)=XW$，也称为特征选择。本文关注于静态特征选择方法，其中对所有样本移除相同的通道，使特征选择等同于前一层的输出通道剪枝。一个有趣的现象出现了：在深度为2的顺序线性层中，剪枝第二层的输入通道会同时剪枝第一层的输出通道，使用相同的剪枝索引。相反，剪枝第二层的输出通道不会影响第一层。输入和输出通道剪枝都有助于模型压缩，但它们可能对性能产生不同的影响。
- en: '![Refer to caption](img/9b0cf654b6de0e0e6afc095a049a0c21.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/9b0cf654b6de0e0e6afc095a049a0c21.png)'
- en: 'Figure 2: Pruning structure recognition. A: Two pruning strategies for the
    depth-2 module. B: Depth-2 modules identification in Transformer-based LLMs.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：剪枝结构识别。A：深度2模块的两种剪枝策略。B：基于Transformer的LLMs中的深度2模块识别。
- en: 3.1.2 Input or Output Channel Pruning for Transformer
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 Transformer的输入或输出通道剪枝
- en: 'Depth-2 Module Identification: In Transformer-based language models, the attention
    and feed-forward modules function as sequential layers with a depth of 2\. For
    the attention module, the first level includes the weight matrices $W_{Q}$ represent
    the weights for the query, key, value, and output in the attention block, respectively.
    The feed-forward module follows a similar structure: the upward projection and
    gated projection occur at the first level, while the downward projection occurs
    at the second level. These depth-2 modules have a unique characteristic: when
    pruning the input channels of layers at the second level, the output channel indices
    of the first-level layers must correspondingly match. This ensures that the structural
    integrity of the model is maintained while pruning.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 深度2模块识别：在基于Transformer的语言模型中，注意力和前馈模块作为深度为2的顺序层进行功能操作。对于注意力模块，第一级包括权重矩阵 $W_{Q}$，分别表示注意力块中的查询、键、值和输出的权重。前馈模块遵循类似的结构：向上投影和门控投影发生在第一级，而向下投影发生在第二级。这些深度2模块具有一个独特的特性：当剪枝第二级层的输入通道时，第一级层的输出通道索引必须相应匹配。这确保了在剪枝过程中模型的结构完整性得以保持。
- en: 'Pruning Strategies for Depth-2 Modules: Given these depth-2 modules, we can
    employ two pruning strategies to achieve the same compression ratio. The first
    strategy involves pruning the output channels of the layers in the first level
    while concurrently pruning the input channels of the layers in the second level.
    This approach ensures that the dependencies outside the module remain invariant.
    The second strategy involves pruning the output channels and the initial input
    $X$ is effectively equivalent to pruning the output channels of a preceding module
    in the sequence. As this dependency propagates backward through the layers, it
    ultimately affects the model’s embedding layer, meaning we are directly pruning
    the channels of the token embeddings.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 深度2模块的剪枝策略：鉴于这些深度2模块，我们可以采用两种剪枝策略以实现相同的压缩比。第一种策略涉及剪枝第一级层的输出通道，同时剪枝第二级层的输入通道。这种方法确保模块外部的依赖保持不变。第二种策略涉及剪枝输出通道，并且初始输入
    $X$ 实质上等同于剪枝序列中前一个模块的输出通道。由于这种依赖向后传播通过层，最终影响模型的嵌入层，这意味着我们直接剪枝了令牌嵌入的通道。
- en: 'Challenge of Residual Connection: Without considering the loss of tokens’ semantic
    information, the two pruning strategies described above should not differ significantly.
    However, residual connections impose substantial constraints on the second strategy.
    In the Transformer architecture, every depth-2 module determines residual connections.
    This means that the pruned channels must be strictly aligned across all modules.
    If the pruned indices of one of them do not align with others, it could lead to
    an unpredictable loss of information. This constraint severely limits the choice
    of channels for pruning and could significantly decrease performance. In contrast,
    the first strategy maintains a fixed number of output channels across these modules,
    avoiding this limitation. Each module can independently select which internal
    channels to prune based on its needs, resulting in a larger search space for optimization.
    This paper will focus on the first pruning strategy.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 残差连接的挑战：在不考虑标记语义信息丢失的情况下，上述两种剪枝策略不应有显著差异。然而，残差连接对第二种策略施加了重大约束。在Transformer架构中，每个深度-2模块决定残差连接。这意味着被剪枝的通道必须在所有模块中严格对齐。如果其中一个的剪枝索引与其他模块不对齐，可能导致不可预测的信息丢失。这个约束严重限制了剪枝通道的选择，可能显著降低性能。相比之下，第一种策略在这些模块中保持固定数量的输出通道，避免了这一限制。每个模块可以根据需要独立选择要剪枝的内部通道，从而获得更大的优化搜索空间。本文将重点关注第一种剪枝策略。
- en: 'Additional Structure for Attention Mechanism: The intricate topology of the
    attention block introduces an additional constraint: pruning must be conducted
    at the level of entire heads, encompassing continuous portions of the channels.
    Fortunately, given the design philosophy of multi-head attention—that each head
    is designed to capture correlations between tokens independently—this setup easily
    leads to redundancy, making it highly amenable to similarity analysis.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制的附加结构：注意力块的复杂拓扑引入了一个额外的约束：剪枝必须在整个头部的层面上进行，涵盖连续的通道部分。幸运的是，由于多头注意力的设计理念——每个头部独立捕捉标记之间的相关性——这种设置容易导致冗余，使其非常适合相似性分析。
- en: 3.2 Pruning Criteria Selection
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 剪枝标准选择
- en: This section begins by examining different pruning criteria from an optimization
    perspective. Then, we introduce two specific pruning metrics for the aforementioned
    depth-2 modules. Finally, an intuitive magnitude-based pruning method is employed
    to remove the least important channels.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先从优化的角度检查不同的剪枝标准。然后，我们介绍了两个针对上述深度-2模块的具体剪枝指标。最后，采用直观的基于幅度的剪枝方法来移除最不重要的通道。
- en: 3.2.1 Pruning Metric Analysis from an Optimization Perspective
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 从优化角度分析剪枝指标
- en: 'Previous work has categorized pruning metrics based on their relationship with
    data, as discussed in Section [2](#S2 "2 Related Work ‣ Greedy Output Approximation:
    Towards Efficient Structured Pruning for LLMs Without Retraining"). Diverging
    from these approaches, we analyze these metrics from an optimization perspective
    and describe in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Greedy Output Approximation:
    Towards Efficient Structured Pruning for LLMs Without Retraining"). Specifically,
    for a linear operation $f(X)=XW$. Objective approximation aims to directly approximate
    accuracy. This category encompasses metrics such as first-order or second-order
    information and regularization scores. However, this type of metric is computationally
    expensive as the optimization process involves backward propagation and calculation
    of the Hessian Matrix. By analyzing these strategies, this paper proposes two
    new metrics to guide the pruning of LLMs.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '之前的工作根据与数据的关系对剪枝指标进行了分类，如第[2](#S2 "2 Related Work ‣ Greedy Output Approximation:
    Towards Efficient Structured Pruning for LLMs Without Retraining")节所讨论的。不同于这些方法，我们从优化的角度分析这些指标，并在图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Greedy Output Approximation: Towards Efficient Structured
    Pruning for LLMs Without Retraining")中进行了描述。具体而言，对于线性操作$f(X)=XW$，目标近似旨在直接近似准确性。此类别包含诸如一阶或二阶信息和正则化分数等指标。然而，这种类型的指标计算开销大，因为优化过程涉及反向传播和Hessian矩阵的计算。通过分析这些策略，本文提出了两个新的指标来指导LLMs的剪枝。'
- en: 3.2.2 Similarity-based Metric for Attention Block
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 基于相似性的注意力块指标
- en: '![Refer to caption](img/6a4375ac106fd8d6d1751eacbc24dc5c.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/6a4375ac106fd8d6d1751eacbc24dc5c.png)'
- en: 'Figure 3: Similarity visualization of attention heads in A: block 4 and B:
    block 5 for Llama-7B. Heads with divergence less than $\tau=0.20$ are connected.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Llama-7B中A：块4和B：块5的注意力头相似性可视化。相似度小于$\tau=0.20$的头被连接起来。
- en: 'Previous research on pruning attention heads typically involves removing heads
    with the lowest importance scores. Surprisingly, our experiments indicate that
    random pruning also yields competitive results compared to magnitude-based pruning,
    especially when the pruning ratio is below 50%. Further experimentation with different
    random seeds, leading to various head indices for pruning, consistently produces
    comparable results. Notably, nearly all heads have been selected for removal at
    some point during this process, suggesting a potential oversight in our initial
    understanding. Recall that different attention heads are intended to independently
    capture correlations between tokens. Thus, it’s common for similar information
    to be extracted across different heads. This observation prompted us to reconsider
    our strategy: we prioritize removing similar heads before eliminating those with
    the least importance score. By identifying and pruning heads that capture redundant
    information, we can optimize the model more effectively while preserving its performance.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以往对剪枝注意力头的研究通常涉及移除重要性得分最低的头。令人惊讶的是，我们的实验表明，随机剪枝在剪枝比例低于50%时，与基于幅度的剪枝相比也能取得竞争力的结果。进一步的实验表明，不同的随机种子产生了各种剪枝头索引，但始终产生了可比的结果。值得注意的是，在这一过程中几乎所有的头在某个阶段都被选中进行移除，这表明我们最初的理解可能存在忽视。请记住，不同的注意力头旨在独立捕捉令牌之间的相关性。因此，不同的头之间提取到类似的信息是很常见的。这一观察促使我们重新考虑策略：我们优先移除相似的头，然后再消除那些重要性得分最低的头。通过识别和剪枝捕捉冗余信息的头，我们可以更有效地优化模型，同时保持其性能。
- en: 'Previous studies have conducted similarity analysis between neurons Engelbrecht
    and Cloete ([1996](#bib.bib13)); Sietsma and Dow ([1988](#bib.bib49)); Engelbrecht
    et al. ([1999](#bib.bib14)), examining the output differences across multiple
    samples to identify similar components. The redundant neurons are then removed,
    and the remaining neurons scale their weights or biases to minimize the impact
    of this removal. However, these methods are primarily effective in smaller neural
    networks, as the scaling technique struggles to handle the accumulated error across
    numerous layers. Fortunately, due to the parallelism and independence of attention
    heads, removing redundant heads does not lead to significant information loss
    that affects subsequent layers, thus eliminating the need for costly remedial
    operations. Based on this observation, we define a pairwise head divergence matrix
    $D\in\mathbb{R}^{h\times h}$ is calculated as:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的研究已经在神经元之间进行了相似性分析 Engelbrecht 和 Cloete ([1996](#bib.bib13))；Sietsma 和 Dow
    ([1988](#bib.bib49))；Engelbrecht 等 ([1999](#bib.bib14))，通过检查多个样本之间的输出差异来识别相似的组件。冗余的神经元随后被移除，剩余的神经元会调整其权重或偏置，以最小化这种移除的影响。然而，这些方法主要在较小的神经网络中有效，因为缩放技术难以处理累积的误差。幸运的是，由于注意力头的并行性和独立性，移除冗余的头不会导致显著的信息损失，从而影响后续层，因此无需进行成本高昂的补救操作。基于这一观察，我们定义了一对一的头发散矩阵$D\in\mathbb{R}^{h\times
    h}$，计算公式为：
- en: '|  | $1$2 |  | (1) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: 'where $M_{ij}^{(n)}=\frac{1}{2}(P_{i}^{(n)}+Q_{j}^{(n)})$ via an edge, we can
    clearly illustrate the relationships between these heads. Fig. [3](#S3.F3 "Figure
    3 ‣ 3.2.2 Similarity-based Metric for Attention Block ‣ 3.2 Pruning Criteria Selection
    ‣ 3 Methodology ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining") demonstrates that some heads fall into the same
    group, signaling information redundancy, whereas others stand alone, highlighting
    the uniqueness of their information. We also observe that specific layers form
    large groups, indicating higher redundancy. The details of our pruning strategy
    for the attention module are outlined in Algo [1](#alg1 "Algorithm 1 ‣ 3.2.2 Similarity-based
    Metric for Attention Block ‣ 3.2 Pruning Criteria Selection ‣ 3 Methodology ‣
    Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $M_{ij}^{(n)}=\frac{1}{2}(P_{i}^{(n)}+Q_{j}^{(n)})$ 通过一条边，我们可以清晰地说明这些头部之间的关系。图[3](#S3.F3
    "Figure 3 ‣ 3.2.2 Similarity-based Metric for Attention Block ‣ 3.2 Pruning Criteria
    Selection ‣ 3 Methodology ‣ Greedy Output Approximation: Towards Efficient Structured
    Pruning for LLMs Without Retraining")显示一些头部落在同一组中，表示信息冗余，而其他头部则独立存在，突出了它们信息的独特性。我们还观察到特定层形成大组，表示更高的冗余。我们对注意力模块的修剪策略的详细信息在算法[1](#alg1
    "Algorithm 1 ‣ 3.2.2 Similarity-based Metric for Attention Block ‣ 3.2 Pruning
    Criteria Selection ‣ 3 Methodology ‣ Greedy Output Approximation: Towards Efficient
    Structured Pruning for LLMs Without Retraining")中概述。'
- en: Algorithm 1 Attention Heads Pruning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 注意力头部修剪。
- en: 'Input: Pairwise head divergence matrix $D\in\mathbb{R}^{h\times h}$'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：成对头部发散矩阵 $D\in\mathbb{R}^{h\times h}$
- en: Algorithm 2 Pre-Pruning Recovery.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 预修剪恢复。
- en: 'Input: Depth-2 module $m_{i}$end procedure'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：深度-2 模块 $m_{i}$结束程序
- en: 3.2.3 Second-moment-based Metric for Depth-2 Module
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 基于第二时刻的深度-2 模块度量
- en: 'To prune the identified depth-2 module, we follow the structure mentioned in
    Sec [3.1](#S3.SS1 "3.1 Pruning Structure Recognition ‣ 3 Methodology ‣ Greedy
    Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining"),
    namely pruning output channels in the first level and input channels in the second
    level. Since the pruned channel indexes from these two directions must match,
    we have to consider them together.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '为了修剪识别出的深度-2 模块，我们遵循第[3.1](#S3.SS1 "3.1 Pruning Structure Recognition ‣ 3 Methodology
    ‣ Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining")节中提到的结构，即在第一层修剪输出通道，在第二层修剪输入通道。由于这两个方向的修剪通道索引必须匹配，我们必须将它们一起考虑。'
- en: 'This paper proposes a 2nd-moment-based pruning metric that is simple to calculate
    and incorporates information from multiple layers. To facilitate understanding,
    we use a basic feed-forward module as an example. Specifically, we describe the
    module as $f(X)=B\sigma(AX)$ can be derived as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提出了一种基于第二时刻的修剪度量，该度量计算简单并且结合了多个层的信息。为了便于理解，我们以一个基本的前馈模块为例。具体地，我们描述该模块为 $f(X)=B\sigma(AX)$
    可以如下推导：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'where $E[Y_{ij}^{2}]$ is:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E[Y_{ij}^{2}]$ 为：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: When the activation function is ReLU, we add a coefficient of 1/2 to the above
    equation. For GeLU or SiLU, our observations indicate that only a small portion
    of values fall within the non-linear region; therefore, we treat them as equivalent
    to ReLU. This approach offers more valuable information from the covariance matrix
    compared to methods based on output energy (first moment) Hagiwara ([1993b](#bib.bib24),
    [1994](#bib.bib25)); Hu et al. ([2016](#bib.bib30)). Unlike some statistical methods
    that require calibration datasets to collect feature values and then calculate
    statistical properties, our method can flexibly integrate information from both
    input and output channels, whereas those methods are limited to focusing only
    on output channels. When there is no way to estimate $\Sigma_{X}$ for attention
    and the feed-forward modules can be found in the Appendix.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当激活函数为ReLU时，我们在上述方程中加入系数1/2。对于GeLU或SiLU，我们的观察表明，只有少量值落在非线性区域，因此我们将它们视为等同于ReLU。这种方法提供了比基于输出能量（第一时刻）Hagiwara（[1993b](#bib.bib24),
    [1994](#bib.bib25)）；Hu等（[2016](#bib.bib30)）的方法更有价值的信息。与一些需要校准数据集来收集特征值并计算统计属性的统计方法不同，我们的方法可以灵活地整合输入和输出通道的信息，而这些方法仅限于关注输出通道。当无法估计$\Sigma_{X}$时，可以在附录中找到注意力和前馈模块。
- en: 3.3 Pre-Pruning Recovery Without Retraining
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 无需重新训练的预修剪恢复
- en: 'With the selection of the pruning structure and criteria, this paper proposes
    a module-wise pruning approach. Similar to layer-wise pruning, we prune these
    depth-2 modules sequentially. To prune one of them, we calculate importance scores
    for its inner channels based on the module’s structure, weights, and inputs. Notably,
    due to errors introduced by pruning preceding modules, the input to the current
    module inevitably deviates from its dense version. Consequently, even without
    pruning the current module, a discrepancy between its output and the original
    output is unavoidable. Recall that our design philosophy is to approximate the
    output as closely as possible. Thus, it is crucial to reconstruct the weights
    of the current module before pruning. This reconstruction ensures that the output
    of this module can still align as closely as possible with the original, even
    with the new input. This way, the pruning criteria for each channel can be optimally
    up-to-date. Inspired by Li et al. ([2023b](#bib.bib37)), this paper presents a
    pre-pruning recovery technique in Algo [2](#alg2 "Algorithm 2 ‣ 3.2.2 Similarity-based
    Metric for Attention Block ‣ 3.2 Pruning Criteria Selection ‣ 3 Methodology ‣
    Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining").'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '通过选择剪枝结构和标准，本文提出了一种模块级剪枝方法。类似于层级剪枝，我们顺序地剪枝这些深度为2的模块。为了剪枝其中一个模块，我们根据模块的结构、权重和输入计算其内部通道的重要性分数。值得注意的是，由于剪枝前模块引入的误差，当前模块的输入不可避免地偏离了其稠密版本。因此，即使不剪枝当前模块，其输出与原始输出之间的差异也是不可避免的。请记住，我们的设计理念是尽可能接近地近似输出。因此，在剪枝之前重建当前模块的权重是至关重要的。这种重建确保了即使在新的输入下，该模块的输出仍能尽可能接近原始输出。这样，每个通道的剪枝标准可以保持最新。受到
    Li 等人 ([2023b](#bib.bib37)) 的启发，本文在算法 [2](#alg2 "Algorithm 2 ‣ 3.2.2 Similarity-based
    Metric for Attention Block ‣ 3.2 Pruning Criteria Selection ‣ 3 Methodology ‣
    Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining") 中提出了一种预剪枝恢复技术。'
- en: With this technique, we can mitigate the errors accumulated from previously
    pruned modules without requiring model retraining. Unlike previous work, which
    primarily focuses on a single layer, our approach targets more complex structures,
    including intricate layer dependencies. We provide more details on applying this
    method to the attention and feed-forward modules in the Appendix.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种技术，我们可以减轻由于之前剪枝的模块累积的误差，而无需重新训练模型。与以往主要关注单层的方法不同，我们的方法针对更复杂的结构，包括复杂的层依赖关系。我们在附录中提供了将这种方法应用于注意力和前馈模块的更多细节。
- en: 4 Experiment
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: This section initially presents the fundamental setup for our experiments. Subsequently,
    we demonstrate the results of experiments and provide an in-depth analysis from
    multiple perspectives.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节首先介绍了我们实验的基本设置。随后，我们展示了实验结果并从多个角度进行深入分析。
- en: 4.1 Setup
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: 'Table 1: The zero-shot performance of the compressed LLaMA-7B (20% sparsity).
    Following the LLM-Pruner methodology Ma et al. ([2023](#bib.bib40)), we only prune
    the transformer blocks from the 4th to the 30th. The average performance is calculated
    across seven classification datasets. ’Bold’ indicates the best pruning-only performance,
    while ’underline’ represents the overall best performance.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 压缩后的 LLaMA-7B（20% 稀疏度）的零样本性能。按照 LLM-Pruner 方法 Ma 等人 ([2023](#bib.bib40))，我们只剪枝从第4到第30的变换器块。平均性能是基于七个分类数据集计算的。‘粗体’表示最佳剪枝性能，而‘下划线’表示整体最佳性能。'
- en: '| Pruning Methods | WikiText2 $\downarrow$ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝方法 | WikiText2 $\downarrow$ |'
- en: '| Dense Touvron et al. ([2023a](#bib.bib55)); Ma et al. ([2023](#bib.bib40))
    | 12.62 | 22.14 | 73.18 | 78.35 | 72.99 | 67.01 | 67.45 | 41.38 | 42.4 | 63.5
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 稠密 Touvron 等人 ([2023a](#bib.bib55)); Ma 等人 ([2023](#bib.bib40)) | 12.62 |
    22.14 | 73.18 | 78.35 | 72.99 | 67.01 | 67.45 | 41.38 | 42.4 | 63.5 |'
- en: '| Data Free Pruning |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 无数据剪枝 |'
- en: '| Random Hoefler et al. ([2021](#bib.bib28)) | 23.02 | 40.19 | 46.21 | 71.33
    | 59.35 | 56.51 | 47.97 | 32.0 | 36.30 | 49.95 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 随机 Hoefler 等人 ([2021](#bib.bib28)) | 23.02 | 40.19 | 46.21 | 71.33 | 59.35
    | 56.51 | 47.97 | 32.0 | 36.30 | 49.95 |'
- en: '| L1 norm Hoefler et al. ([2021](#bib.bib28)) | 179.02 | 311.75 | 51.28 | 60.22
    | 43.14 | 52.01 | 36.53 | 27.89 | 30.8 | 43.12 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| L1 范数 Hoefler 等人 ([2021](#bib.bib28)) | 179.02 | 311.75 | 51.28 | 60.22 |
    43.14 | 52.01 | 36.53 | 27.89 | 30.8 | 43.12 |'
- en: '| L2 norm Hoefler et al. ([2021](#bib.bib28)) | 582.41 | 1022.17 | 60.18 |
    58.54 | 37.04 | 53.27 | 32.91 | 27.56 | 29.8 | 42.76 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| L2 范数 Hoefler 等人 ([2021](#bib.bib28)) | 582.41 | 1022.17 | 60.18 | 58.54
    | 37.04 | 53.27 | 32.91 | 27.56 | 29.8 | 42.76 |'
- en: '| Ours (Self-Gen) | 21.76 | 34.3 | 63.51 | 72.63 | 56.54 | 54.46 | 51.68 |
    33.79 | 36.4 | 52.72 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 我们（Self-Gen） | 21.76 | 34.3 | 63.51 | 72.63 | 56.54 | 54.46 | 51.68 | 33.79
    | 36.4 | 52.72 |'
- en: '| Ours SG w/ remedy | 20.32 | 33.42 | 64.17 | 72.67 | 58.43 | 57.29 | 53.32
    | 34.15 | 37.23 | 53.89 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 我们SG w/ remedy | 20.32 | 33.42 | 64.17 | 72.67 | 58.43 | 57.29 | 53.32 |
    34.15 | 37.23 | 53.89 |'
- en: '| Data Dependent Pruning |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 数据依赖剪枝 |'
- en: '| Training-Aware Pruning |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 训练感知剪枝 |'
- en: '| LLM-Pruner Vec Ma et al. ([2023](#bib.bib40)) | 22.28 | 41.78 | 61.44 | 71.71
    | 57.27 | 54.22 | 55.77 | 33.96 | 38.4 | 53.52 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Vec Ma等人（[2023](#bib.bib40)） | 22.28 | 41.78 | 61.44 | 71.71 |
    57.27 | 54.22 | 55.77 | 33.96 | 38.4 | 53.52 |'
- en: '| LLM-Pruner E1 Ma et al. ([2023](#bib.bib40)) | 19.09 | 34.21 | 57.06 | 75.68
    | 66.8 | 59.83 | 60.94 | 36.52 | 40.0 | 56.69 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner E1 Ma等人（[2023](#bib.bib40)） | 19.09 | 34.21 | 57.06 | 75.68 |
    66.8 | 59.83 | 60.94 | 36.52 | 40.0 | 56.69 |'
- en: '| LLM-Pruner E2 Ma et al. ([2023](#bib.bib40)) | 19.77 | 36.66 | 59.39 | 75.57
    | 65.34 | 61.33 | 59.18 | 37.12 | 39.8 | 56.82 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner E2 Ma等人（[2023](#bib.bib40)） | 19.77 | 36.66 | 59.39 | 75.57 |
    65.34 | 61.33 | 59.18 | 37.12 | 39.8 | 56.82 |'
- en: '| Inference-Aware Pruning |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 推理感知剪枝 |'
- en: '| Wanda-sp Sun et al. ([2023](#bib.bib51)) | 27.45 | 49.52 | 64.16 | 75.21
    | 68.62 | 62.27 | 59.68 | 36.68 | 39.2 | 57.97 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| Wanda-sp Sun等人（[2023](#bib.bib51)） | 27.45 | 49.52 | 64.16 | 75.21 | 68.62
    | 62.27 | 59.68 | 36.68 | 39.2 | 57.97 |'
- en: '| Ours (Calibration) | 17.48 | 30.04 | 66.48 | 75.78 | 67.73 | 62.27 | 61.4
    | 35.49 | 39.6 | 58.39 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 我们（Calibration） | 17.48 | 30.04 | 66.48 | 75.78 | 67.73 | 62.27 | 61.4 |
    35.49 | 39.6 | 58.39 |'
- en: '| Ours C w/ remedy | 17.90 | 31.23 | 70.12 | 76.86 | 68.55 | 65.76 | 64.23
    | 38.54 | 40.5 | 60.65 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 我们C w/ remedy | 17.90 | 31.23 | 70.12 | 76.86 | 68.55 | 65.76 | 64.23 | 38.54
    | 40.5 | 60.65 |'
- en: '| Retraining-required Pruning |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 需要重新训练的剪枝 |'
- en: '| LLM-P. LoRA Ma et al. ([2023](#bib.bib40)) | 17.37 | 30.39 | 69.54 | 76.44
    | 68.11 | 65.11 | 63.43 | 37.88 | 40.0 | 60.07 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| LLM-P. LoRA Ma等人（[2023](#bib.bib40)） | 17.37 | 30.39 | 69.54 | 76.44 | 68.11
    | 65.11 | 63.43 | 37.88 | 40.0 | 60.07 |'
- en: 'Baselines: This paper presents a comprehensive comparison of state-of-the-art
    pruning methods across multiple dimensions, aiming for fair evaluations and in-depth
    analyses to uncover the reasons behind the observed results. First, we compare
    our approach with data-free pruning methods, including random pruning and magnitude-based
    pruning (L1 and L2 norms) Hoefler et al. ([2021](#bib.bib28)). Next, we evaluate
    our methods against data-dependent pruning techniques, encompassing training-aware,
    inference-aware, and retraining-required methods. In the training-aware category,
    we compare with various configurations of LLM-Pruner Ma et al. ([2023](#bib.bib40)),
    such as Element1, Element2, and Vector-wise magnitude pruning. Within the inference-aware
    category, we compare with the structured version of Wanda Sun et al. ([2023](#bib.bib51))
    and FLAP An et al. ([2024](#bib.bib2)). Additionally, we extend our comparisons
    to include the LLM-Pruner method augmented with retraining. Such comprehensive
    evaluations will demonstrate the effectiveness of our pruning approach.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线：本文全面比较了多种前沿剪枝方法，旨在进行公平评估和深入分析，以揭示观察到结果背后的原因。首先，我们将我们的方法与无数据剪枝方法进行比较，包括随机剪枝和基于幅度的剪枝（L1和L2范数）Hoefler等人（[2021](#bib.bib28)）。接下来，我们将我们的方法与数据依赖的剪枝技术进行评估，包括训练感知、推理感知和需要重新训练的方法。在训练感知类别中，我们与各种配置的LLM-Pruner
    Ma等人（[2023](#bib.bib40)）进行比较，如Element1、Element2和向量幅度剪枝。在推理感知类别中，我们与Wanda Sun等人（[2023](#bib.bib51)）的结构化版本和FLAP
    An等人（[2024](#bib.bib2)）进行比较。此外，我们还将比较扩展到包含经过重新训练的LLM-Pruner方法。这种全面的评估将展示我们剪枝方法的有效性。
- en: 'Models: Our primary experiments are categorized into two series based on the
    model scale: LLaMA-7B with 7 billion parameters and GPT-2 with 110 million parameters Radford
    et al. ([2019](#bib.bib47)); Touvron et al. ([2023a](#bib.bib55)). This aligns
    with our study’s goal to assess pruning performance across different model sizes
    and ensure a thorough examination. Additionally, we extend our experiments to
    other models, including LLaMA-13B, Vicuna-7B Chiang et al. ([2023](#bib.bib7)).
    This comprehensive selection allows us to explore a broader spectrum of capabilities
    and sizes, enhancing our understanding of how different architectures perform
    under various computational constraints. Additional experiment results can be
    found in the Appendix.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 模型：我们的主要实验分为两个系列，基于模型规模：具有70亿参数的LLaMA-7B和具有1.1亿参数的GPT-2 Radford等人（[2019](#bib.bib47)）；Touvron等人（[2023a](#bib.bib55)）。这与我们研究的目标一致，旨在评估不同模型规模下的剪枝性能，并确保全面检查。此外，我们还将实验扩展到其他模型，包括LLaMA-13B、Vicuna-7B
    Chiang等人（[2023](#bib.bib7)）。这种全面的选择使我们能够探索更广泛的能力和规模，从而增强我们对不同架构在各种计算约束下表现的理解。附加实验结果可以在附录中找到。
- en: 'Evaluation and Datasets: To evaluate performance, we adopt LLaMa’s approach
    by conducting zero-shot task classification on a range of common sense reasoning
    datasets: BoolQ Clark et al. ([2019](#bib.bib8)), PIQA Bisk et al. ([2020](#bib.bib6)),
    HellaSwag Zellers et al. ([2019](#bib.bib63)), WinoGrande Sakaguchi et al. ([2021](#bib.bib48)),
    ARC-easy Clark et al. ([2018](#bib.bib9)), ARC-challenge Clark et al. ([2018](#bib.bib9)),
    and OpenbookQA Mihaylov et al. ([2018](#bib.bib43)). Following the methodology
    in Gao et al. ([2021](#bib.bib21)), the model either ranks the options in multiple-choice
    tasks or generates answers in open-ended formats. Additionally, we enhance our
    evaluation by conducting a zero-shot perplexity (PPL) analysis on WikiText2 Merity
    et al. ([2016](#bib.bib42)) and the Penn Treebank (PTB) Marcus et al. ([1993](#bib.bib41)).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 评估与数据集：为了评估性能，我们采用 LLaMa 的方法，对一系列常识推理数据集进行零-shot 任务分类：BoolQ Clark 等 ([2019](#bib.bib8))，PIQA
    Bisk 等 ([2020](#bib.bib6))，HellaSwag Zellers 等 ([2019](#bib.bib63))，WinoGrande
    Sakaguchi 等 ([2021](#bib.bib48))，ARC-easy Clark 等 ([2018](#bib.bib9))，ARC-challenge
    Clark 等 ([2018](#bib.bib9))，和 OpenbookQA Mihaylov 等 ([2018](#bib.bib43))。按照 Gao
    等 ([2021](#bib.bib21)) 的方法，模型要么对多项选择任务中的选项进行排名，要么生成开放式答案。此外，我们通过对 WikiText2 Merity
    等 ([2016](#bib.bib42)) 和 Penn Treebank (PTB) Marcus 等 ([1993](#bib.bib41)) 进行零-shot
    困惑度 (PPL) 分析来增强我们的评估。
- en: 'Implementation: During the pruning phase, we randomly select 16 samples from
    Wikitext2 and Bookcorpus, truncated to a sequence length of 128 for LLaMA-7B and
    1024 for GPT-2\. These samples serve as calibration data for pruning metric calculation
    and covariance matrix extraction, respectively. During the recovery phase, we
    sample an additional 1,024 examples from the downstream dataset to guide optimization
    in the data-dependent comparison experiments.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 实施：在剪枝阶段，我们从 Wikitext2 和 Bookcorpus 中随机选择 16 个样本，LLaMA-7B 的序列长度截断为 128，GPT-2
    为 1024。这些样本用作剪枝度量计算和协方差矩阵提取的校准数据。在恢复阶段，我们从下游数据集中再抽取 1,024 个样本，以指导数据依赖的比较实验中的优化。
- en: 4.2 Results and Analysis
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 结果与分析
- en: 'Table 2: Similarity-based analysis for LLaMA-7B attention heads pruning (all
    blocks) with different $\tau$. ’Bold’ indicates the best performance.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：不同 $\tau$ 下对 LLaMA-7B 注意力头部剪枝的相似度分析（所有块）。‘**粗体**’表示最佳表现。
- en: '| Methods | # pruned heads | Wiki2 $\downarrow$ |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| Methods | # 剪枝的头部 | Wiki2 $\downarrow$ |'
- en: '| Dense | 0 | 12.62 | 22.14 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Dense | 0 | 12.62 | 22.14 |'
- en: '| Ours ($\tau=0.16$) | 88 | 12.96 | 22.45 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Ours ($\tau=0.16$) | 88 | 12.96 | 22.45 |'
- en: '| Random | 64 | 14.50 | 24.13 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Random | 64 | 14.50 | 24.13 |'
- en: '| L2 Norm | 64 | 14.69 | 25.64 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| L2 Norm | 64 | 14.69 | 25.64 |'
- en: '| 1st+2nd order | 64 | 13.45 | 24.19 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 1st+2nd order | 64 | 13.45 | 24.19 |'
- en: '| FLAP | 88 | 12.90 | 22.67 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| FLAP | 88 | 12.90 | 22.67 |'
- en: '| Ours ($\tau=0.19$) | 204 | 14.69 | 24.32 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Ours ($\tau=0.19$) | 204 | 14.69 | 24.32 |'
- en: '| Random | 192 | 18.75 | 35.73 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Random | 192 | 18.75 | 35.73 |'
- en: '| L2 Norm | 192 | 195.84 | 371.65 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| L2 Norm | 192 | 195.84 | 371.65 |'
- en: '| 1st+2nd order | 192 | 14.81 | 28.77 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 1st+2nd order | 192 | 14.81 | 28.77 |'
- en: '| FLAP | 204 | 13.22 | 24.42 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| FLAP | 204 | 13.22 | 24.42 |'
- en: 'We present the main results in Tab. [1](#S4.T1 "Table 1 ‣ 4.1 Setup ‣ 4 Experiment
    ‣ Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining"). For the data-free comparison experiments, we leverage the inherent
    ability of LLMs to generate sentences. Our pruning method uses these generated
    sentences as calibration data because, given that the LLMs are well-trained, these
    sentences naturally conform to the semantic and syntactic token distributions
    of the training data. Compared to traditional data-free metrics (L1 or L2), our
    data-free version, which relies solely on the model itself, achieves significant
    improvements in perplexity and up to a 20% enhancement in zero-shot evaluation
    for downstream tasks. Moreover, our method surpasses random pruning by at least
    6%, a significant improvement achieved without relying on existing datasets, while
    traditional metrics (L1 or L2) fail to outperform. These results demonstrate the
    superiority of our techniques in data-free pruning methods.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格[1](#S4.T1 "Table 1 ‣ 4.1 Setup ‣ 4 Experiment ‣ Greedy Output Approximation:
    Towards Efficient Structured Pruning for LLMs Without Retraining")中展示了主要结果。对于无数据比较实验，我们利用
    LLMs 生成句子的固有能力。我们的剪枝方法使用这些生成的句子作为校准数据，因为考虑到 LLMs 已经过训练，这些句子自然符合训练数据的语义和句法标记分布。与传统的无数据度量（L1
    或 L2）相比，我们完全依赖模型本身的无数据版本在困惑度上取得了显著改善，并且在下游任务的零-shot 评估中提高了多达 20%。此外，我们的方法比随机剪枝高出至少
    6%，这是一个显著的改进，且无需依赖现有数据集，而传统度量（L1 或 L2）未能超越。这些结果展示了我们技术在无数据剪枝方法中的优越性。'
- en: '![Refer to caption](img/d5cad0261bd3b5a2653b5b1bdb841c0d.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d5cad0261bd3b5a2653b5b1bdb841c0d.png)'
- en: 'Figure 4: Performance of compressed A: LLaMA-7B (w/o Remediation) and B: GPT-2 (w/
    Remediation) concerning the number of calibration samples.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 压缩后的 A: LLaMA-7B（无补救）和 B: GPT-2（有补救）在不同校准样本数量下的表现。'
- en: Our approach outperforms data-dependent pruning methods and the inference-only
    method Wanda-SP. Impressively, it also surpasses the state-of-the-art training-aware
    pruning method LLM-Pruner, which includes different configurations such as Element1,
    Element2, and Vector. Our approach consistently demonstrates better pruning results
    without requiring computationally intensive first-order and second-order information.
    Moreover, our method even achieves better results compared to LLM-Pruner with
    LoRA, despite the latter involving model retraining.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法优于数据依赖剪枝方法和仅推断方法 Wanda-SP。令人印象深刻的是，它还超越了最先进的训练感知剪枝方法 LLM-Pruner，该方法包括不同的配置，如
    Element1、Element2 和 Vector。我们的方法在没有要求计算密集型一阶和二阶信息的情况下，始终展示了更好的剪枝结果。此外，我们的方法即使与包含
    LoRA 的 LLM-Pruner 相比，仍然取得了更好的结果，尽管后者涉及模型再训练。
- en: 'We also compare our method with the state-of-the-art inference-only method
    FLAP and present the results in Tab. [3](#S4.T3 "Table 3 ‣ 4.2 Results and Analysis
    ‣ 4 Experiment ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining"). Our approach exhibits significantly better results
    on the GPT-2 model and achieves comparable performance with LLaMA-7B. Overall,
    our method demonstrates superior performance in both data-free and data-dependent
    pruning categories.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将我们的方法与最先进的仅推断方法 FLAP 进行了比较，并在表 [3](#S4.T3 "表 3 ‣ 4.2 结果与分析 ‣ 4 实验 ‣ 贪婪输出近似：朝向高效的结构化剪枝，无需再训练")
    中展示了结果。我们的方法在 GPT-2 模型上表现显著更好，并且在 LLaMA-7B 上也达到了可比的性能。总体而言，我们的方法在数据无关和数据依赖的剪枝类别中都展示了优越的性能。
- en: 'Table 3: Perplexity of compressed GPT-2 and LLaMA-7B (25% and 50% sparsity)
    on Wikitext2 and PTB. We prune the 4th to 30th transformer blocks for LLaMA-7B
    and all blocks for GPT-2\. ’Bold’ indicates the best performance, while ’underline’
    represents the second-best performance.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 压缩后的 GPT-2 和 LLaMA-7B 在 Wikitext2 和 PTB 上的困惑度（25% 和 50% 稀疏度）。我们对 LLaMA-7B
    进行了第 4 到第 30 个 Transformer 块的剪枝，对 GPT-2 则进行了全部块的剪枝。‘**粗体**’ 表示最佳性能，而 ‘*下划线*’ 代表第二佳性能。'
- en: '| Models | GPT-2: [0-12) | LLama-7b: [4-30) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | GPT-2: [0-12) | LLama-7b: [4-30) |'
- en: '| Datasets: PPL | WikiText2: 29.95 Radford et al. ([2019](#bib.bib47)) $\downarrow$
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 数据集: PPL | WikiText2: 29.95 Radford 等 ([2019](#bib.bib47)) $\downarrow$ |'
- en: '| Sparsity | 25% | 50% | 25% | 50% | 25% | 50% | 25% | 50% |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏度 | 25% | 50% | 25% | 50% | 25% | 50% | 25% | 50% |'
- en: '| Data Free Pruning |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 数据无关剪枝 |'
- en: '| Random Hoefler et al. ([2021](#bib.bib28)) | 189.73 | 1839.33 | 245.33 |
    2769.6 | 23.02 | 100.42 | 40.19 | 133.56 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 随机 Hoefler 等 ([2021](#bib.bib28)) | 189.73 | 1839.33 | 245.33 | 2769.6 |
    23.02 | 100.42 | 40.19 | 133.56 |'
- en: '| L1 norm Hoefler et al. ([2021](#bib.bib28)) | 338.3 | 1226.13 | 583.2 | 1290.45
    | 179.02 | 891.23 | 311.75 | 1034.69 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| L1 范数 Hoefler 等 ([2021](#bib.bib28)) | 338.3 | 1226.13 | 583.2 | 1290.45
    | 179.02 | 891.23 | 311.75 | 1034.69 |'
- en: '| L2 norm Hoefler et al. ([2021](#bib.bib28)) | 227.32 | 674.52 | 324.33 |
    800.14 | 582.41 | 14000.68 | 1022.17 | 28062.45 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| L2 范数 Hoefler 等 ([2021](#bib.bib28)) | 227.32 | 674.52 | 324.33 | 800.14
    | 582.41 | 14000.68 | 1022.17 | 28062.45 |'
- en: '| Ours (Self-Generation w/ remedy) | 119.29 | 586.87 | 152.93 | 723.39 | 21.76
    | 58.61 | 34.3 | 64.24 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 我们的方法（自生成 + 补救） | 119.29 | 586.87 | 152.93 | 723.39 | 21.76 | 58.61 | 34.3
    | 64.24 |'
- en: '| Data Dependent Pruning |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 数据依赖剪枝 |'
- en: '| Training-Aware Pruning |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 训练感知剪枝 |'
- en: '| LLM-Pruner Element1 Ma et al. ([2023](#bib.bib40)) | 9229.32 | 32453.23 |
    11993.24 | 8020.87 | 19.09 | 48.84 | 34.21 | 105.24 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Element1 Ma 等 ([2023](#bib.bib40)) | 9229.32 | 32453.23 | 11993.24
    | 8020.87 | 19.09 | 48.84 | 34.21 | 105.24 |'
- en: '| LLM-Pruner Element2 Ma et al. ([2023](#bib.bib40)) | 1897.32 | 14706.23 |
    2258.33 | 18598.33 | 19.77 | 72.89 | 36.66 | 138.33 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Element2 Ma 等 ([2023](#bib.bib40)) | 1897.32 | 14706.23 | 2258.33
    | 18598.33 | 19.77 | 72.89 | 36.66 | 138.33 |'
- en: '| LLM-Pruner Vector Ma et al. ([2023](#bib.bib40)) | 488.32 | 39025.12 | 6169.56
    | 18616.87 | 22.88 | 55.68 | 41.76 | 305.24 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Vector Ma 等 ([2023](#bib.bib40)) | 488.32 | 39025.12 | 6169.56
    | 18616.87 | 22.88 | 55.68 | 41.76 | 305.24 |'
- en: '| Inference-Aware Pruning |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 知识推断剪枝 |'
- en: '| Wanda-Structured Pruning Sun et al. ([2023](#bib.bib51)) | 586.34 | 4147.32
    | 355.17 | 3246.79 | 27.45 | 69.02 | 49.52 | 132.52 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| Wanda-Structured Pruning Sun 等 ([2023](#bib.bib51)) | 586.34 | 4147.32 |
    355.17 | 3246.79 | 27.45 | 69.02 | 49.52 | 132.52 |'
- en: '| FLAP UL-UM w/o remed An et al. ([2024](#bib.bib2)) | 818.14 | 3636.23 | 554.32
    | 2758.37 | 17.15 | 36.08 | 34.96 | 85.22 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| FLAP UL-UM 无补救 An 等 ([2024](#bib.bib2)) | 818.14 | 3636.23 | 554.32 | 2758.37
    | 17.15 | 36.08 | 34.96 | 85.22 |'
- en: '| FLAP UL-UM w/ remedy An et al. ([2024](#bib.bib2)) | 2197.32 | 3043.35 |
    2199.24 | 3561.76 | 15.76 | 26.87 | 32.1 | 66.18 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| FLAP UL-UM w/ remedy An et al. ([2024](#bib.bib2)) | 2197.32 | 3043.35 |
    2199.24 | 3561.76 | 15.76 | 26.87 | 32.1 | 66.18 |'
- en: '| Ours UL-UM (Calibration w/o remedy) | 81.96 | 317.37 | 186.68 | 936.57 |
    NA | NA | NA | NA |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Ours UL-UM (Calibration w/o remedy) | 81.96 | 317.37 | 186.68 | 936.57 |
    NA | NA | NA | NA |'
- en: '| FLAP AL-AM w/o remedy An et al. ([2024](#bib.bib2)) | 126.57 | 5538.32 |
    135.07 | 10244.95 | 17.01 | 34.09 | 30.99 | 71.76 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| FLAP AL-AM w/o remedy An et al. ([2024](#bib.bib2)) | 126.57 | 5538.32 |
    135.07 | 10244.95 | 17.01 | 34.09 | 30.99 | 71.76 |'
- en: '| FLAP AL-AM w/ remedy An et al. ([2024](#bib.bib2)) | 1349.25 | 5382.14 |
    1769.56 | 7476.08 | 15.06 | 26.55 | 29.45 | 57.89 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| FLAP AL-AM w/ remedy An et al. ([2024](#bib.bib2)) | 1349.25 | 5382.14 |
    1769.56 | 7476.08 | 15.06 | 26.55 | 29.45 | 57.89 |'
- en: '| Ours ML-MM (Calibration w/o remedy) | 79.4 | 251.34 | 130.54 | 756.33 | 17.48
    | 26.87 | 30.04 | 57.89 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Ours ML-MM (Calibration w/o remedy) | 79.4 | 251.34 | 130.54 | 756.33 | 17.48
    | 26.87 | 30.04 | 57.89 |'
- en: 4.3 Ablation Study
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: 'We also explore our pruning metrics by exclusively pruning attention heads.
    The experimental results in Tab. [2](#S4.T2 "Table 2 ‣ 4.2 Results and Analysis
    ‣ 4 Experiment ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining") demonstrate that for colossal LLMs like LLaMA-7B,
    our similarity analysis effectively identifies redundant attention heads with
    minimal negative impact on model performance. Compared to inference-aware metrics
    such as the L2 norm, training-aware metrics using first- and second-order information,
    and random pruning, our similarity-based metric consistently outperforms. When
    compared to the specifically designed metric of FLAP, we achieve better or comparable
    performance. These results strongly indicate that we should prioritize pruning
    redundant information rather than heads with small importance scores.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还通过专门修剪注意力头来探索我们的剪枝指标。表[2](#S4.T2 "Table 2 ‣ 4.2 Results and Analysis ‣ 4
    Experiment ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining")中的实验结果表明，对于像LLaMA-7B这样庞大的LLM，我们的相似性分析有效地识别了冗余的注意力头，对模型性能的负面影响极小。与L2范数等感知推断的指标、使用一阶和二阶信息的训练感知指标以及随机剪枝相比，我们的基于相似性的指标始终表现更好。与专门设计的FLAP指标相比，我们的表现更佳或相当。这些结果强烈表明，我们应优先剪枝冗余信息，而非重要性得分较小的头部。'
- en: 'Additionally, we designed experiments to explore the influence of the number
    of calibration samples. Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Results and Analysis
    ‣ 4 Experiment ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining") shows that in LLaMA-7B pruning-only experiments,
    our method is insensitive to the number of calibration samples, achieving comparable
    results with as few as 8 samples and as many as 128 samples. Conversely, in GPT-2
    pruning with remediation experiments, performance improves with an increasing
    number of calibration samples. These findings demonstrate that our pruning method
    is robust regardless of the number of calibration samples, while our pre-pruning
    recovery method benefits from a higher number of calibration samples. However,
    this improvement gradually diminishes once the number of samples reaches a critical
    threshold.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们设计了实验以探索校准样本数量的影响。图[4](#S4.F4 "Figure 4 ‣ 4.2 Results and Analysis ‣ 4
    Experiment ‣ Greedy Output Approximation: Towards Efficient Structured Pruning
    for LLMs Without Retraining")显示，在LLaMA-7B的仅剪枝实验中，我们的方法对校准样本数量不敏感，使用少至8个样本或多至128个样本均能取得相当的结果。相反，在GPT-2的修正剪枝实验中，随着校准样本数量的增加，性能有所提升。这些发现表明，我们的剪枝方法对校准样本数量具有鲁棒性，而我们的预剪枝恢复方法从更多的校准样本中受益。然而，一旦样本数量达到临界阈值，这种改进逐渐减小。'
- en: 5 Discussion, Limitation, and Conclusion
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论、局限性与结论
- en: 'Implicit Motivation and Call: In the pre-deep learning era, various pruning
    metrics and structures were designed. For example, variance-based pruning and
    bias-based remedy methods similar to FLAP were proposed by researchers 30 years
    ago Engelbrecht et al. ([1999](#bib.bib14)); Sietsma and Dow ([1988](#bib.bib49));
    Engelbrecht and Cloete ([1996](#bib.bib13)); Thimm and Fiesler ([1995b](#bib.bib54)).
    These early researchers already recognized that feature information is at least
    as crucial as model weights in constructing pruning criteria. In the early stages
    of deep learning (before 2022), many researchers found that multi-round model
    retraining could easily recover the lost performance induced by pruning, even
    when based solely on weight magnitudes. As a result, the importance of pruning
    metrics and structure design was often overlooked, with reliance placed on retraining
    to validate methods. However, this paradigm shifted after 2022, when colossal
    LLMs became mainstream in the community. Training such models is prohibitively
    expensive, making pruning that relies on multi-round retraining impractical. Although
    parameter-efficient training methods like LoRA can reduce costs, they still require
    rigorous data selection Hu et al. ([2021](#bib.bib29)); Ma et al. ([2023](#bib.bib40)).
    Thus, we urge the community to return to designing metrics that better account
    for the influence of both weights and features, rather than focusing solely on
    dataset competition. Motivated by this, this paper focuses on inference-aware
    pruning metrics that do not require retraining.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 隐性动机和呼吁：在深度学习之前，设计了各种剪枝度量和结构。例如，基于方差的剪枝和类似 FLAP 的偏置修正方法由研究人员 30 年前提出 Englebrecht
    等人 ([1999](#bib.bib14))；Sietsma 和 Dow ([1988](#bib.bib49))；Engelbrecht 和 Cloete
    ([1996](#bib.bib13))；Thimm 和 Fiesler ([1995b](#bib.bib54))。这些早期研究人员已经认识到，特征信息在构建剪枝标准中的重要性至少与模型权重一样重要。在深度学习的早期阶段（2022
    年之前），许多研究人员发现，多轮模型再训练可以轻松恢复由于剪枝导致的性能损失，即使仅基于权重大小。因此，剪枝度量和结构设计的重要性往往被忽视，通常依赖于再训练来验证方法。然而，在
    2022 年之后，这种范式发生了转变，当庞大的 LLMs 成为社区的主流时。训练此类模型的成本过高，使得依赖多轮再训练的剪枝方法变得不切实际。尽管像 LoRA
    这样的参数高效训练方法可以降低成本，但仍需严格的数据选择 Hu 等人 ([2021](#bib.bib29))；Ma 等人 ([2023](#bib.bib40))。因此，我们呼吁社区回到设计更好地考虑权重和特征影响的度量标准，而不是仅仅关注数据集竞争。基于此动机，本文专注于不需要再训练的推理感知剪枝度量。
- en: 'Limitation: This work evaluates the compressed LLMs primarily on perplexity
    and downstream tasks. However, we do not assess the emergent abilities of colossal
    LLMs, such as mathematical reasoning, safety alignment, common sense reasoning,
    contextual understanding, and creativity in text generation. Future research will
    focus on evaluating and enhancing these emergent abilities to provide a more comprehensive
    understanding of the compressed LLMs.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 限制：这项工作主要通过困惑度和下游任务评估了压缩的 LLMs。然而，我们没有评估庞大的 LLMs 的突现能力，如数学推理、安全对齐、常识推理、上下文理解和文本生成中的创造力。未来的研究将集中于评估和增强这些突现能力，以提供对压缩
    LLMs 的更全面理解。
- en: 'Conclusion: This paper introduces a novel approach to pruning large language
    models (LLMs) by identifying a depth-2 pruning structure and developing two inference-aware
    pruning criteria. These methods surpass traditional metrics and eliminate the
    need for computationally expensive retraining. Our two-step reconstruction technique
    further mitigates pruning errors, ensuring superior performance across various
    datasets and models. Overall, our approach significantly reduces computational
    costs and hardware requirements, offering an efficient solution for pruning colossal
    LLMs.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 结论：本文介绍了一种新的大规模语言模型（LLMs）剪枝方法，通过识别深度为2的剪枝结构并开发了两种考虑推理的剪枝标准。这些方法超越了传统的度量标准，并消除了计算成本高昂的再训练需求。我们的两步重建技术进一步减少了剪枝误差，确保在各种数据集和模型中表现优越。总体而言，我们的方法显著降低了计算成本和硬件需求，为修剪庞大的
    LLMs 提供了高效的解决方案。
- en: References
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,
    Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat, et al. Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*,
    2023.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等人 [2023] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge
    Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
    Shyamal Anadkat 等人. Gpt-4 技术报告。*arXiv 预印本 arXiv:2303.08774*，2023年。
- en: An et al. [2024] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. Fluctuation-based
    adaptive structured pruning for large language models. In *Proceedings of the
    AAAI Conference on Artificial Intelligence*, volume 38, pages 10865–10873, 2024.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An等人 [2024] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, 和 Jinqiao Wang. 基于波动的自适应结构化剪枝用于大型语言模型。收录于*Proceedings
    of the AAAI Conference on Artificial Intelligence*，第38卷，第10865–10873页，2024。
- en: Anwar et al. [2017] Sajid Anwar, Kyuyeon Hwang, and Wonyong Sung. Structured
    pruning of deep convolutional neural networks. *ACM Journal on Emerging Technologies
    in Computing Systems (JETC)*, 13(3):1–18, 2017.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anwar等人 [2017] Sajid Anwar, Kyuyeon Hwang, 和 Wonyong Sung. 深度卷积神经网络的结构化剪枝。*ACM
    Journal on Emerging Technologies in Computing Systems (JETC)*, 13(3):1–18, 2017。
- en: 'Ashkboos et al. [2024] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do
    Nascimento, Torsten Hoefler, and James Hensman. Slicegpt: Compress large language
    models by deleting rows and columns. *arXiv preprint arXiv:2401.15024*, 2024.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ashkboos等人 [2024] Saleh Ashkboos, Maximilian L Croci, Marcelo Gennari do Nascimento,
    Torsten Hoefler, 和 James Hensman. Slicegpt: 通过删除行和列来压缩大型语言模型。*arXiv preprint arXiv:2401.15024*,
    2024。'
- en: 'Banks and Warkentin [2024] Tris Warkentin Jeanine Banks and Tris Warkentin.
    Gemma: Introducing new state-of-the-art open models, 2024.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Banks和Warkentin [2024] Tris Warkentin Jeanine Banks 和 Tris Warkentin. Gemma:
    引入新的最先进开放模型，2024。'
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    Piqa: Reasoning about physical commonsense in natural language. In *Proceedings
    of the AAAI conference on artificial intelligence*, volume 34, pages 7432–7439,
    2020.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk等人 [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi 等. Piqa:
    在自然语言中推理物理常识。收录于*Proceedings of the AAAI conference on artificial intelligence*，第34卷，第7432–7439页，2020。'
- en: 'Chiang et al. [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez,
    et al. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.
    *See https://vicuna. lmsys. org (accessed 14 April 2023)*, 2(3):6, 2023.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chiang等人 [2023] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu,
    Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez 等.
    Vicuna: 一款开源聊天机器人，凭借90%* chatgpt质量给gpt-4留下深刻印象。*见 https://vicuna.lmsys.org (访问日期
    2023年4月14日)*，2(3):6, 2023。'
- en: 'Clark et al. [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty
    of natural yes/no questions. *arXiv preprint arXiv:1905.10044*, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark等人 [2019] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski,
    Michael Collins, 和 Kristina Toutanova. Boolq: 探索自然是/否问题的惊人难度。*arXiv preprint arXiv:1905.10044*,
    2019。'
- en: Clark et al. [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question
    answering? try arc, the ai2 reasoning challenge. *arXiv preprint arXiv:1803.05457*,
    2018.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark等人 [2018] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish
    Sabharwal, Carissa Schoenick, 和 Oyvind Tafjord. 认为你已经解决了问答问题？试试arc，AI2推理挑战。*arXiv
    preprint arXiv:1803.05457*, 2018。
- en: Cuadros et al. [2020] Xavier Suau Cuadros, Luca Zappella, and Nicholas Apostoloff.
    Filter distillation for network compression. In *Proceedings of the IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pages 3140–3149, 2020.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cuadros等人 [2020] Xavier Suau Cuadros, Luca Zappella, 和 Nicholas Apostoloff.
    网络压缩的滤波器蒸馏。收录于*Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*，第3140–3149页，2020。
- en: Dong et al. [2017] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune
    deep neural networks via layer-wise optimal brain surgeon. *Advances in Neural
    Information Processing Systems*, 30, 2017.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dong等人 [2017] Xin Dong, Shangyu Chen, 和 Sinno Pan. 通过层级最优脑外科医生学习剪枝深度神经网络。*Advances
    in Neural Information Processing Systems*, 30, 2017。
- en: Du et al. [2023] Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi,
    Xia Hu, and Ahmed Hassan Awadallah. Robustness challenges in model distillation
    and pruning for natural language understanding. In *Proceedings of the 17th Conference
    of the European Chapter of the Association for Computational Linguistics*, pages
    1766–1778, Dubrovnik, Croatia, May 2023\. Association for Computational Linguistics.
    URL [https://aclanthology.org/2023.eacl-main.129](https://aclanthology.org/2023.eacl-main.129).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Du等人 [2023] Mengnan Du, Subhabrata Mukherjee, Yu Cheng, Milad Shokouhi, Xia
    Hu, 和 Ahmed Hassan Awadallah. 自然语言理解模型蒸馏和剪枝中的鲁棒性挑战。收录于*Proceedings of the 17th
    Conference of the European Chapter of the Association for Computational Linguistics*，第1766–1778页，克罗地亚杜布罗夫尼克，2023年5月。计算语言学协会。网址
    [https://aclanthology.org/2023.eacl-main.129](https://aclanthology.org/2023.eacl-main.129)。
- en: Engelbrecht and Cloete [1996] Andries P Engelbrecht and Ian Cloete. A sensitivity
    analysis algorithm for pruning feedforward neural networks. In *Proceedings of
    International Conference on Neural Networks (ICNN’96)*, volume 2, pages 1274–1278\.
    IEEE, 1996.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engelbrecht和Cloete [1996] Andries P Engelbrecht和Ian Cloete。用于剪枝前馈神经网络的敏感性分析算法。在*国际神经网络会议（ICNN’96）论文集*，第2卷，页码1274–1278。IEEE，1996年。
- en: Engelbrecht et al. [1999] Andries P Engelbrecht, L Fletcher, and Ian Cloete.
    Variance analysis of sensitivity information for pruning multilayer feedforward
    neural networks. In *IJCNN’99\. International Joint Conference on Neural Networks.
    Proceedings (Cat. No. 99CH36339)*, volume 3, pages 1829–1833\. IEEE, 1999.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Engelbrecht等人 [1999] Andries P Engelbrecht、L Fletcher和Ian Cloete。剪枝多层前馈神经网络的敏感性信息方差分析。在*IJCNN’99.
    国际联合神经网络会议. 论文集（Cat. No. 99CH36339）*，第3卷，页码1829–1833。IEEE，1999年。
- en: 'Fang et al. [2023] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and
    Xinchao Wang. Depgraph: Towards any structural pruning. In *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 16091–16101,
    2023.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang等人 [2023] Gongfan Fang、Xinyin Ma、Mingli Song、Michael Bi Mi和Xinchao Wang。Depgraph：走向任何结构剪枝。在*IEEE/CVF计算机视觉与模式识别会议论文集*，页码16091–16101，2023年。
- en: 'Frankle and Carbin [2019] Jonathan Frankle and Michael Carbin. The lottery
    ticket hypothesis: Finding sparse, trainable neural networks. In *7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019*. OpenReview.net, 2019. URL [https://openreview.net/forum?id=rJl-b3RcF7](https://openreview.net/forum?id=rJl-b3RcF7).'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle和Carbin [2019] Jonathan Frankle和Michael Carbin。彩票票假设：寻找稀疏的可训练神经网络。在*第7届国际学习表示会议，ICLR
    2019，新奥尔良，美国，2019年5月6-9日*。OpenReview.net，2019年。网址 [https://openreview.net/forum?id=rJl-b3RcF7](https://openreview.net/forum?id=rJl-b3RcF7)。
- en: Frankle et al. [2020] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy,
    and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis.
    In *International Conference on Machine Learning*, pages 3259–3269\. PMLR, 2020.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frankle等人 [2020] Jonathan Frankle、Gintare Karolina Dziugaite、Daniel Roy和Michael
    Carbin。线性模式连通性和彩票票假设。在*国际机器学习会议*，页码3259–3269。PMLR，2020年。
- en: 'Frantar and Alistarh [2022] Elias Frantar and Dan Alistarh. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *Advances in
    Neural Information Processing Systems*, 35:4475–4488, 2022.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar和Alistarh [2022] Elias Frantar和Dan Alistarh。最佳脑压缩：准确的训练后量化和剪枝框架。在*神经信息处理系统进展*，第35卷：4475–4488，2022年。
- en: 'Frantar and Alistarh [2023] Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot. In *International Conference
    on Machine Learning*, pages 10323–10337\. PMLR, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar和Alistarh [2023] Elias Frantar和Dan Alistarh。Sparsegpt：大型语言模型可以通过一次性方法进行精确剪枝。在*国际机器学习会议*上，页码10323–10337。PMLR，2023年。
- en: Gale et al. [2019] Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity
    in deep neural networks. *ArXiv*, abs/1902.09574, 2019.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gale等人 [2019] Trevor Gale、Erich Elsen和Sara Hooker。深度神经网络中的稀疏状态。*ArXiv*，abs/1902.09574，2019年。
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. *Version v0\. 0.1\.
    Sept*, page 8, 2021.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao等人 [2021] Leo Gao、Jonathan Tow、Stella Biderman、Sid Black、Anthony DiPofi、Charles
    Foster、Laurence Golding、Jeffrey Hsu、Kyle McDonell、Niklas Muennighoff等人。少样本语言模型评估框架。*版本
    v0.0.1. 九月*，第8页，2021年。
- en: 'Gordon et al. [2020] Mitchell Gordon, Kevin Duh, and Nicholas Andrews. Compressing
    BERT: Studying the effects of weight pruning on transfer learning. In *Proceedings
    of the 5th Workshop on Representation Learning for NLP*, pages 143–155, Online,
    July 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.repl4nlp-1.18.
    URL [https://aclanthology.org/2020.repl4nlp-1.18](https://aclanthology.org/2020.repl4nlp-1.18).'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gordon等人 [2020] Mitchell Gordon、Kevin Duh和Nicholas Andrews。压缩BERT：研究权重剪枝对迁移学习的影响。在*第5届自然语言处理表示学习研讨会论文集*，页码143–155，在线，2020年7月。计算语言学协会。doi:
    10.18653/v1/2020.repl4nlp-1.18。网址 [https://aclanthology.org/2020.repl4nlp-1.18](https://aclanthology.org/2020.repl4nlp-1.18)。'
- en: 'Hagiwara [1993a] M. Hagiwara. Removal of hidden units and weights for back
    propagation networks. In *Proceedings of 1993 International Conference on Neural
    Networks (IJCNN-93-Nagoya, Japan)*, volume 1, pages 351–354 vol.1, 1993a. doi:
    10.1109/IJCNN.1993.713929.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hagiwara [1993a] M. Hagiwara。去除反向传播网络中的隐藏单元和权重。在*1993年国际神经网络会议论文集（IJCNN-93-名古屋，日本）*，第1卷，页码351–354，第1卷，1993年。doi:
    10.1109/IJCNN.1993.713929。'
- en: Hagiwara [1993b] Masafumi Hagiwara. Removal of hidden units and weights for
    back propagation networks. In *Proceedings of 1993 International Conference on
    Neural Networks (IJCNN-93-Nagoya, Japan)*, volume 1, pages 351–354\. IEEE, 1993b.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hagiwara [1993b] Masafumi Hagiwara。用于反向传播网络的隐藏单元和权重移除。在 *1993 国际神经网络会议 (IJCNN-93-Nagoya,
    Japan)* 论文集，卷1，页351–354。IEEE，1993b。
- en: Hagiwara [1994] Masafumi Hagiwara. A simple and effective method for removal
    of hidden units and weights. *Neurocomputing*, 6(2):207–218, 1994.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hagiwara [1994] Masafumi Hagiwara。隐藏单元和权重移除的一种简单有效的方法。*神经计算*，6(2):207–218，1994。
- en: 'Han et al. [2015] Song Han, Huizi Mao, and William J Dally. Deep compression:
    Compressing deep neural networks with pruning, trained quantization and huffman
    coding. *arXiv preprint arXiv:1510.00149*, 2015.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han 等人 [2015] Song Han、Huizi Mao 和 William J Dally。深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv
    预印本 arXiv:1510.00149*，2015。
- en: He et al. [2019] Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter
    pruning via geometric median for deep convolutional neural networks acceleration.
    In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*,
    pages 4340–4349, 2019.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2019] Yang He、Ping Liu、Ziwei Wang、Zhilan Hu 和 Yi Yang。通过几何中位数进行滤波器剪枝，以加速深度卷积神经网络。在
    *IEEE/CVF 计算机视觉与模式识别会议论文集*，页4340–4349，2019。
- en: 'Hoefler et al. [2021] Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden,
    and Alexandra Peste. Sparsity in deep learning: Pruning and growth for efficient
    inference and training in neural networks. *Journal of Machine Learning Research*,
    22(241):1–124, 2021.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoefler 等人 [2021] Torsten Hoefler、Dan Alistarh、Tal Ben-Nun、Nikoli Dryden 和 Alexandra
    Peste。深度学习中的稀疏性：神经网络高效推理和训练的剪枝与增长。*机器学习研究期刊*，22(241):1–124，2021。
- en: 'Hu et al. [2021] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of
    large language models. *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2021] Edward J Hu、Yelong Shen、Phillip Wallis、Zeyuan Allen-Zhu、Yuanzhi
    Li、Shean Wang、Lu Wang 和 Weizhu Chen。Lora：大语言模型的低秩适应。*arXiv 预印本 arXiv:2106.09685*，2021。
- en: 'Hu et al. [2016] Hengyuan Hu, Rui Peng, Yu-Wing Tai, and Chi-Keung Tang. Network
    trimming: A data-driven neuron pruning approach towards efficient deep architectures.
    *arXiv preprint arXiv:1607.03250*, 2016.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2016] Hengyuan Hu、Rui Peng、Yu-Wing Tai 和 Chi-Keung Tang。网络修剪：一种数据驱动的神经元剪枝方法，旨在高效的深度架构。*arXiv
    预印本 arXiv:1607.03250*，2016。
- en: 'Khaki and Plataniotis [2024] Samir Khaki and Konstantinos N Plataniotis. The
    need for speed: Pruning transformers with one recipe. *arXiv preprint arXiv:2403.17921*,
    2024.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khaki 和 Plataniotis [2024] Samir Khaki 和 Konstantinos N Plataniotis。对速度的需求：用一种方法剪枝变换器。*arXiv
    预印本 arXiv:2403.17921*，2024。
- en: 'Kim et al. [2024] Bo-Kyeong Kim, Geonmin Kim, Tae-Ho Kim, Thibault Castells,
    Shinkook Choi, Junho Shin, and Hyoung-Kyu Song. Shortened llama: A simple depth
    pruning for large language models. *arXiv preprint arXiv:2402.02834*, 2024.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等人 [2024] Bo-Kyeong Kim、Geonmin Kim、Tae-Ho Kim、Thibault Castells、Shinkook
    Choi、Junho Shin 和 Hyoung-Kyu Song。缩短的llama：大语言模型的简单深度剪枝。*arXiv 预印本 arXiv:2402.02834*，2024。
- en: 'Kurtić et al. [2024] Eldar Kurtić, Elias Frantar, and Dan Alistarh. Ziplm:
    Inference-aware structured pruning of language models. *Advances in Neural Information
    Processing Systems*, 36, 2024.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurtić 等人 [2024] Eldar Kurtić、Elias Frantar 和 Dan Alistarh。Ziplm：具有推理意识的语言模型结构化剪枝。*神经信息处理系统进展*，36，2024。
- en: 'Le Scao et al. [2023] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint*, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Le Scao 等人 [2023] Teven Le Scao、Angela Fan、Christopher Akiki、Ellie Pavlick、Suzana
    Ilić、Daniel Hesslow、Roman Castagné、Alexandra Sasha Luccioni、François Yvon、Matthias
    Gallé 等人。Bloom：一个176b参数的开放访问多语言模型。*arXiv 预印本*，2023。
- en: 'Lee et al. [2018] Namhoon Lee, Thalaiyasingam Ajanthan, and Philip HS Torr.
    Snip: Single-shot network pruning based on connection sensitivity. *arXiv preprint
    arXiv:1810.02340*, 2018.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2018] Namhoon Lee、Thalaiyasingam Ajanthan 和 Philip HS Torr。Snip：基于连接敏感性的单次网络剪枝。*arXiv
    预印本 arXiv:1810.02340*，2018。
- en: 'Li et al. [2023a] Jianwei Li, Weizhi Gao, Qi Lei, and Dongkuan Xu. Breaking
    through deterministic barriers: Randomized pruning mask generation and selection.
    *arXiv preprint arXiv:2310.13183*, 2023a.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2023a] Jianwei Li、Weizhi Gao、Qi Lei 和 Dongkuan Xu。突破确定性障碍：随机剪枝掩码生成和选择。*arXiv
    预印本 arXiv:2310.13183*，2023a。
- en: 'Li et al. [2023b] Jianwei Li, Qi Lei, Wei Cheng, and Dongkuan Xu. Towards robust
    pruning: An adaptive knowledge-retention pruning strategy for language models.
    *arXiv preprint arXiv:2310.13191*, 2023b.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. [2023b] Jianwei Li, Qi Lei, Wei Cheng, 和 Dongkuan Xu. 朝着鲁棒修剪的方向：一种针对语言模型的自适应知识保留修剪策略。*arXiv
    预印本 arXiv:2310.13191*，2023 年。
- en: Lis et al. [2019] Mieszko Lis, Maximilian Golub, and Guy Lemieux. Full deep
    neural network training on a pruned weight budget. *Proceedings of Machine Learning
    and Systems*, 1:252–263, 2019.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lis et al. [2019] Mieszko Lis, Maximilian Golub, 和 Guy Lemieux. 在修剪的权重预算下完全训练深度神经网络。*机器学习与系统会议论文集*，1:252–263，2019
    年。
- en: 'Luo et al. [2017] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter
    level pruning method for deep neural network compression. In *Proceedings of the
    IEEE international conference on computer vision*, pages 5058–5066, 2017.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Luo et al. [2017] Jian-Hao Luo, Jianxin Wu, 和 Weiyao Lin. Thinet: 一种用于深度神经网络压缩的过滤器级修剪方法。在
    *IEEE 国际计算机视觉会议论文集*，第 5058–5066 页，2017 年。'
- en: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models. *Advances in neural information
    processing systems*, 36:21702–21720, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. [2023] Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. LLM-Pruner: 关于大规模语言模型的结构性修剪。*神经信息处理系统进展*，36:21702–21720，2023
    年。'
- en: 'Marcus et al. [1993] Mitch Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz.
    Building a large annotated corpus of english: The penn treebank. *Computational
    linguistics*, 19(2):313–330, 1993.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Marcus et al. [1993] Mitch Marcus, Beatrice Santorini, 和 Mary Ann Marcinkiewicz.
    构建一个大型的英文注释语料库：Penn Treebank。*计算语言学*，19(2):313–330，1993 年。
- en: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, and Richard
    Socher. Pointer sentinel mixture models. *arXiv preprint arXiv:1609.07843*, 2016.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity et al. [2016] Stephen Merity, Caiming Xiong, James Bradbury, 和 Richard
    Socher. 指针哨兵混合模型。*arXiv 预印本 arXiv:1609.07843*，2016 年。
- en: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish
    Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book
    question answering. *arXiv preprint arXiv:1809.02789*, 2018.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mihaylov et al. [2018] Todor Mihaylov, Peter Clark, Tushar Khot, 和 Ashish Sabharwal.
    一套盔甲能导电吗？一个用于开放式书籍问答的新数据集。*arXiv 预印本 arXiv:1809.02789*，2018 年。
- en: Molchanov et al. [2019] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio,
    and Jan Kautz. Importance estimation for neural network pruning. In *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    11264–11272, 2019.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Molchanov et al. [2019] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio,
    和 Jan Kautz. 神经网络修剪的重要性估计。在 *IEEE/CVF 计算机视觉与模式识别会议论文集*，第 11264–11272 页，2019 年。
- en: Pool et al. [2021] Jeff Pool, Abhishek Sawarkar, and Jay Rodge. Accelerating
    inference with sparsity using the nvidia ampere architecture and nvidia tensorrt.
    *NVIDIA Developer Technical Blog, https://developer. nvidia. com/blog/accelerating-inference-with-sparsityusing-ampere-and-tensorrt*,
    2021.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pool et al. [2021] Jeff Pool, Abhishek Sawarkar, 和 Jay Rodge. 利用 NVIDIA Ampere
    架构和 NVIDIA TensorRT 加速稀疏性推理。*NVIDIA 开发者技术博客, https://developer.nvidia.com/blog/accelerating-inference-with-sparsityusing-ampere-and-tensorrt*，2021
    年。
- en: 'Prasanna et al. [2020] Sai Prasanna, Anna Rogers, and Anna Rumshisky. When
    BERT Plays the Lottery, All Tickets Are Winning. In *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing (EMNLP)*, pages 3208–3229,
    Online, November 2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.259.
    URL [https://aclanthology.org/2020.emnlp-main.259](https://aclanthology.org/2020.emnlp-main.259).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Prasanna et al. [2020] Sai Prasanna, Anna Rogers, 和 Anna Rumshisky. 当 BERT
    买彩票时，所有的票都是中奖的。在 *2020 年自然语言处理实证方法会议（EMNLP）论文集*，第 3208–3229 页，在线，2020 年 11 月。计算语言学协会。doi:
    10.18653/v1/2020.emnlp-main.259。网址 [https://aclanthology.org/2020.emnlp-main.259](https://aclanthology.org/2020.emnlp-main.259)。'
- en: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners.
    *OpenAI blog*, 1(8):9, 2019.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. [2019] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario
    Amodei, Ilya Sutskever 等. 语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8):9，2019 年。
- en: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale.
    *Communications of the ACM*, 64(9):99–106, 2021.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sakaguchi et al. [2021] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,
    和 Yejin Choi. Winogrande: 一种大规模的对抗性 Winograd 语法挑战。*ACM 通讯*，64(9):99–106，2021 年。'
- en: Sietsma and Dow [1988] Sietsma and Dow. Neural net pruning-why and how. In *IEEE
    1988 international conference on neural networks*, pages 325–333\. IEEE, 1988.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sietsma 和 Dow [1988] Sietsma 和 Dow. 神经网络修剪——为什么以及如何。在 *IEEE 1988 年国际神经网络会议*，第
    325–333 页。IEEE，1988 年。
- en: 'Singh and Alistarh [2020] Sidak Pal Singh and Dan Alistarh. Woodfisher: Efficient
    second-order approximation for neural network compression. *Advances in Neural
    Information Processing Systems*, 33:18098–18109, 2020.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh and Alistarh [2020] Sidak Pal Singh 和 Dan Alistarh. Woodfisher: 用于神经网络压缩的高效二阶近似。*神经信息处理系统进展*，第33卷:18098–18109，2020年。'
- en: Sun et al. [2023] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. A simple
    and effective pruning approach for large language models. *arXiv preprint arXiv:2306.11695*,
    2023.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. [2023] Mingjie Sun, Zhuang Liu, Anna Bair, 和 J Zico Kolter. 一种简单有效的大型语言模型剪枝方法。*arXiv
    预印本 arXiv:2306.11695*，2023年。
- en: 'Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: a family of highly capable multimodal models. *arXiv
    preprint arXiv:2312.11805*, 2023.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Team et al. [2023] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,
    Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai,
    Anja Hauth, et al. Gemini: 一类高效的多模态模型。*arXiv 预印本 arXiv:2312.11805*，2023年。'
- en: Thimm and Fiesler [1995a] Georg Thimm and Emile Fiesler. Evaluating pruning
    methods. In *Proceedings of the International Symposium on Artificial neural networks*,
    pages 20–25, 1995a.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thimm and Fiesler [1995a] Georg Thimm 和 Emile Fiesler. 剪枝方法的评估。收录于*国际人工神经网络研讨会论文集*，第20–25页，1995年a。
- en: Thimm and Fiesler [1995b] Georg Thimm and Emile Fiesler. Evaluating pruning
    methods. In *Proceedings of the International Symposium on Artificial neural networks*,
    pages 20–25, 1995b.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thimm and Fiesler [1995b] Georg Thimm 和 Emile Fiesler. 剪枝方法的评估。收录于*国际人工神经网络研讨会论文集*，第20–25页，1995年b。
- en: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023a.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023a] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: 开放且高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023年a。'
- en: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. *arXiv
    preprint arXiv:2307.09288*, 2023b.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023b] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
    Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,
    Shruti Bhosale, et al. Llama 2: 开放的基础和微调的聊天模型。*arXiv 预印本 arXiv:2307.09288*，2023年b。'
- en: van der Ouderaa et al. [2023] Tycho FA van der Ouderaa, Markus Nagel, Mart Van Baalen,
    Yuki M Asano, and Tijmen Blankevoort. The llm surgeon. *arXiv preprint arXiv:2312.17244*,
    2023.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van der Ouderaa et al. [2023] Tycho FA van der Ouderaa, Markus Nagel, Mart Van
    Baalen, Yuki M Asano, 和 Tijmen Blankevoort. LLM外科医生。*arXiv 预印本 arXiv:2312.17244*，2023年。
- en: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is
    all you need. *Advances in neural information processing systems*, 30, 2017.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. [2017] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N Gomez, Łukasz Kaiser, 和 Illia Polosukhin. 注意力机制就是你所需要的一切。*神经信息处理系统进展*，第30卷，2017年。
- en: 'Wang et al. [2020] Ziheng Wang, Jeremy Wohlwend, and Tao Lei. Structured pruning
    of large language models. In *Proceedings of the 2020 Conference on Empirical
    Methods in Natural Language Processing (EMNLP)*, pages 6151–6162, Online, November
    2020\. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.496.
    URL [https://aclanthology.org/2020.emnlp-main.496](https://aclanthology.org/2020.emnlp-main.496).'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang et al. [2020] Ziheng Wang, Jeremy Wohlwend, 和 Tao Lei. 大型语言模型的结构化剪枝。收录于*2020年自然语言处理实证方法会议论文集（EMNLP）*，第6151–6162页，在线，2020年11月。计算语言学协会。doi:
    10.18653/v1/2020.emnlp-main.496. 网址 [https://aclanthology.org/2020.emnlp-main.496](https://aclanthology.org/2020.emnlp-main.496)。'
- en: 'Xia et al. [2023] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. Sheared
    llama: Accelerating language model pre-training via structured pruning. *arXiv
    preprint arXiv:2310.06694*, 2023.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xia et al. [2023] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, 和 Danqi Chen. Sheared
    llama: 通过结构化剪枝加速语言模型预训练。*arXiv 预印本 arXiv:2310.06694*，2023年。'
- en: 'Xu et al. [2021] Canwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley,
    and Furu Wei. Beyond preserved accuracy: Evaluating loyalty and robustness of
    BERT compression. In *Proceedings of the 2021 Conference on Empirical Methods
    in Natural Language Processing*, pages 10653–10659, Online and Punta Cana, Dominican
    Republic, November 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.832.
    URL [https://aclanthology.org/2021.emnlp-main.832](https://aclanthology.org/2021.emnlp-main.832).'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人 [2021] Canwen Xu、Wangchunshu Zhou、Tao Ge、Ke Xu、Julian McAuley 和 Furu
    Wei。《超越保留的准确性：评估 BERT 压缩的忠诚度和鲁棒性》。在 *2021年自然语言处理实证方法会议论文集*，第10653–10659页，在线和多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。doi:
    10.18653/v1/2021.emnlp-main.832。网址 [https://aclanthology.org/2021.emnlp-main.832](https://aclanthology.org/2021.emnlp-main.832)。'
- en: 'Yu et al. [2018] Ruichi Yu, Ang Li, Chun-Fu Chen, Jui-Hsin Lai, Vlad I Morariu,
    Xintong Han, Mingfei Gao, Ching-Yung Lin, and Larry S Davis. Nisp: Pruning networks
    using neuron importance score propagation. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 9194–9203, 2018.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人 [2018] Ruichi Yu、Ang Li、Chun-Fu Chen、Jui-Hsin Lai、Vlad I Morariu、Xintong
    Han、Mingfei Gao、Ching-Yung Lin 和 Larry S Davis。《Nisp: 利用神经元重要性评分传播来剪枝网络》。在 *IEEE
    计算机视觉与模式识别会议论文集*，第9194–9203页，2018年。'
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? *arXiv preprint
    arXiv:1905.07830*, 2019.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等人 [2019] Rowan Zellers、Ari Holtzman、Yonatan Bisk、Ali Farhadi 和 Yejin
    Choi。《Hellaswag: 机器真的能完成你的句子吗？*arXiv 预印本 arXiv:1905.07830*》，2019年。'
- en: 'Zhu and Gupta [2017] Michael Zhu and Suyog Gupta. To prune, or not to prune:
    exploring the efficacy of pruning for model compression. *arXiv preprint arXiv:1710.01878*,
    2017.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 和 Gupta [2017] Michael Zhu 和 Suyog Gupta。《剪枝，还是不剪枝：探讨剪枝在模型压缩中的有效性》。*arXiv
    预印本 arXiv:1710.01878*，2017年。
- en: 6 Appendix-A
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 附录-A
- en: 'Pruning is a promising method that can effectively reduce model inference costs.
    In this paper, we discuss general pruning methods and various classification philosophies.
    We summarize previous work and categorize pruning from multiple perspectives:
    structured and unstructured, data-free and data-dependent, training-aware and
    inference-aware, and retraining-free and retraining-dependent. We also propose
    an innovative optimization-oriented view of pruning, which includes: A: Function
    Approximation, B: Output Approximation, and C: Objective Approximation. Our pruning
    pattern is designed based on this perspective.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '剪枝是一种有前途的方法，可以有效减少模型推理成本。本文讨论了常见的剪枝方法和各种分类哲学。我们总结了以往的工作，并从多个角度对剪枝进行了分类：结构化和非结构化、无数据和依赖数据、训练感知和推理感知、以及无需重训练和需重训练。我们还提出了一种创新的以优化为导向的剪枝视角，包括：A:
    函数近似，B: 输出近似，以及 C: 目标近似。我们的剪枝模式是基于这一视角设计的。'
- en: Additionally, we review more aspects of pruning, including the most popular
    techniques in the pre-deep learning era (before 2022), such as Iterative Magnitude
    Pruning and the comparison between random pruning and magnitude pruning. We also
    discussed the relationship between pruning and quantization. By considering these
    various dimensions and methodologies, we aim to provide a comprehensive understanding
    of pruning and its potential to enhance model efficiency.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还回顾了剪枝的更多方面，包括在深度学习时代之前（2022年之前）最受欢迎的技术，例如迭代幅度剪枝以及随机剪枝与幅度剪枝的比较。我们还讨论了剪枝与量化之间的关系。通过考虑这些不同的维度和方法论，我们旨在提供对剪枝的全面理解及其提升模型效率的潜力。
- en: 6.1 Iterative Magnitude Pruning
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 迭代幅度剪枝
- en: Iterative Magnitude Pruning (IMP) is the most renowned strategy for achieving
    state-of-the-art results, surpassing other methods such as Single-shot Network
    Pruning (SNIP) Frankle and Carbin [[2019](#bib.bib16)], Frankle et al. [[2020](#bib.bib17)],
    Frantar and Alistarh [[2022](#bib.bib18)], Lee et al. [[2018](#bib.bib35)]. This
    approach divides the pruning process into multiple stages by gradually increasing
    the sparsity. At each stage, the goal is to identify and remove redundant parameters
    or neurons. The most intuitive approach is to assign an importance score to each
    element and keep only the top-k elements, where the score can be based on the
    absolute value of weights, output sensitivity, gradients, or other fine-designed
    metrics Hagiwara [[1993a](#bib.bib23)], Gale et al. [[2019](#bib.bib20)], Thimm
    and Fiesler [[1995a](#bib.bib53)], Han et al. [[2015](#bib.bib26)], Zhu and Gupta
    [[2017](#bib.bib64)], Cuadros et al. [[2020](#bib.bib10)], Luo et al. [[2017](#bib.bib39)].
    Weight magnitude is the most straightforward and data-free method, while other
    metrics can be computationally expensive as they require training with data Yu
    et al. [[2018](#bib.bib62)], He et al. [[2019](#bib.bib27)], Lis et al. [[2019](#bib.bib38)],
    Molchanov et al. [[2019](#bib.bib44)], Singh and Alistarh [[2020](#bib.bib50)],
    Dong et al. [[2017](#bib.bib11)]. Moreover, IMP is accompanied by a retraining
    phase to restore performance, which can be computationally costly. Therefore,
    in the era of colossal LLMs, IMP and other methods that heavily depend on model
    retraining are no longer effective due to the immense costs involved.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代幅度剪枝（IMP）是实现最先进结果的最著名策略，超越了其他方法，如 Frankle 和 Carbin 提出的单次网络剪枝（SNIP）[[2019](#bib.bib16)]，Frankle
    等人 [[2020](#bib.bib17)]，Frantar 和 Alistarh [[2022](#bib.bib18)]，Lee 等人 [[2018](#bib.bib35)]。这种方法将剪枝过程分为多个阶段，通过逐步增加稀疏性来实现。在每个阶段，目标是识别并去除冗余的参数或神经元。最直观的方法是为每个元素分配一个重要性分数，仅保留前
    k 个元素，这些分数可以基于权重的绝对值、输出灵敏度、梯度或其他精心设计的指标 Hagiwara [[1993a](#bib.bib23)]，Gale 等人
    [[2019](#bib.bib20)]，Thimm 和 Fiesler [[1995a](#bib.bib53)]，Han 等人 [[2015](#bib.bib26)]，Zhu
    和 Gupta [[2017](#bib.bib64)]，Cuadros 等人 [[2020](#bib.bib10)]，Luo 等人 [[2017](#bib.bib39)]。权重幅度是最直接和无数据的方法，而其他指标可能在计算上比较昂贵，因为它们需要使用数据进行训练
    Yu 等人 [[2018](#bib.bib62)]，He 等人 [[2019](#bib.bib27)]，Lis 等人 [[2019](#bib.bib38)]，Molchanov
    等人 [[2019](#bib.bib44)]，Singh 和 Alistarh [[2020](#bib.bib50)]，Dong 等人 [[2017](#bib.bib11)]。此外，IMP
    附带一个重新训练阶段以恢复性能，这可能在计算上代价昂贵。因此，在庞大 LLM 的时代，IMP 和其他严重依赖模型重新训练的方法由于涉及巨大的成本已不再有效。
- en: 6.2 Randomized Pruning v.s. Magnitude Pruning
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 随机剪枝与幅度剪枝
- en: Excluding the influence of model retraining, we discovered an interesting phenomenon
    for model pruning. For colossal LLMs such as LLaMA-7B, randomized pruning surprisingly
    produced competitive results. Specifically, compared to traditional data-free
    pruning metrics like L1 and L2 norm values, randomized pruning achieved several
    times better results, even rivaling data-dependent pruning methods. However, this
    advantage only existed when the pruning ratio was less than 2x. As the pruning
    ratio increased, magnitude pruning gradually yielded better results. Initially,
    we attributed this phenomenon to the high redundancy of parameters in LLMs. However,
    our experiments with GPT-2 showed that randomized pruning was still weaker than
    magnitude pruning. Therefore, we speculate that for colossal LLMs like Llama-7B,
    feature information plays a more crucial role in model activations compared to
    smaller LLMs like GPT-2.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 排除模型重新训练的影响，我们发现了一个有趣的现象。在模型剪枝方面，对于像 LLaMA-7B 这样庞大的 LLM，随机剪枝竟然产生了具有竞争力的结果。具体来说，与传统的无数据剪枝指标如
    L1 和 L2 范数值相比，随机剪枝取得了几倍更好的结果，甚至与依赖数据的剪枝方法相媲美。然而，这种优势仅在剪枝比例小于 2x 时存在。随着剪枝比例的增加，幅度剪枝逐渐取得了更好的结果。最初，我们将这一现象归因于
    LLM 中参数的高冗余性。然而，我们对 GPT-2 的实验显示，随机剪枝仍然不如幅度剪枝。因此，我们推测对于像 LLaMA-7B 这样的庞大 LLM，特征信息在模型激活中比对于像
    GPT-2 这样的较小 LLM 更为关键。
- en: Magnitude-based pruning methods aim to remove weights or neurons from a neural
    network that appear least influential, primarily determined by the value of their
    weights. The rationale behind these methods is to reduce overall model size and
    computational requirements without a drastic loss in performance. However, several
    challenges arise with this approach, and one major challenge is the lack of variety
    if the magnitude is based on data-free metrics (L1 or L2). This kind of metric
    focuses solely on the magnitude of the weights for pruning decisions, potentially
    missing smaller weights that play pivotal roles, especially in edge cases or rarer
    instances.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 基于幅度的剪枝方法旨在从神经网络中移除那些影响最小的权重或神经元，主要由它们的权重值决定。这些方法的基本原理是减少整体模型大小和计算需求，同时不会导致性能的大幅下降。然而，这种方法存在一些挑战，其中一个主要挑战是如果幅度基于无数据指标（L1
    或 L2），则缺乏多样性。这种指标仅关注权重的幅度以进行剪枝决策，可能会遗漏那些在边缘情况或较少出现的实例中扮演关键角色的小权重。
- en: To illustrate this more clearly, consider the following example. The output
    of a neural network can be represented as $y=\sum(w_{i}\cdot f_{i})$. This example
    indicates that the influence of feature information plays a significant role in
    identifying redundant elements.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地说明这一点，请考虑以下示例。神经网络的输出可以表示为 $y=\sum(w_{i}\cdot f_{i})$。这个例子表明，特征信息的影响在识别冗余元素时发挥了重要作用。
- en: '![Refer to caption](img/963426c77631a8105d6ddb4d2b85966d.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/963426c77631a8105d6ddb4d2b85966d.png)'
- en: 'Figure 5: Mean activation value of Llama-7B and GPT-2 on Wikitext2.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：Llama-7B 和 GPT-2 在 Wikitext2 上的平均激活值。
- en: 'Based on the above observation, we speculate that LLaMA-7B’s feature information
    contributes more to the importance score of removed elements when the pruning
    ratio is less than 2x. As the pruning ratio gradually increases, the influence
    of the features on the activation values is no longer greater than that of the
    weights. Therefore, randomized pruning fails at larger pruning ratios. To validate
    our hypothesis, we conducted a statistical analysis on the feature values of LLaMA-7B,
    described in Figure [5](#S6.F5 "Figure 5 ‣ 6.2 Randomized Pruning v.s. Magnitude
    Pruning ‣ 6 Appendix-A ‣ Greedy Output Approximation: Towards Efficient Structured
    Pruning for LLMs Without Retraining"). Our results show that colossal LLMs like
    Llama-7B have larger activation values than smaller LLMs like GPT-2\. These findings
    further motivate us to design the pruning metrics that incorporate both feature
    and weight information instead of seeking dataset competition.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 根据上述观察，我们推测，当剪枝比小于2倍时，LLaMA-7B的特征信息对被移除元素的重要性评分的贡献更大。随着剪枝比的逐渐增加，特征对激活值的影响不再大于权重的影响。因此，在较大的剪枝比下，随机剪枝失败。为了验证我们的假设，我们对LLaMA-7B的特征值进行了统计分析，如图[5](#S6.F5
    "图 5 ‣ 6.2 随机剪枝 vs. 幅度剪枝 ‣ 6 附录-A ‣ 贪婪输出近似：朝着无需再训练的LLM高效结构化剪枝的方向")所示。我们的结果表明，像Llama-7B这样的庞大LLM的激活值比像GPT-2这样的较小LLM的激活值更大。这些发现进一步激励我们设计既包含特征信息又包含权重信息的剪枝度量，而不是寻求数据集竞争。
- en: '6.3 Pruning v.s. Quantization:'
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 剪枝 vs. 量化：
- en: Pruning, though considered less effective than quantization in the era of colossal
    LLMs, should not be underestimated. In practice, pruning and quantization can
    complement each other, yielding significant benefits when applied together Frantar
    and Alistarh [[2022](#bib.bib18)]. Even pruning a small percentage of parameters,
    such as 5%, can be valuable if it meets practical performance requirements. Therefore,
    integrating pruning into the optimization process is always worthwhile.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在庞大LLM时代剪枝被认为不如量化有效，但不应低估它。在实践中，剪枝和量化可以互补，当结合使用时能带来显著的好处 Frantar 和 Alistarh
    [[2022](#bib.bib18)]。即使是剪枝小比例的参数，例如5%，如果满足实际性能要求，也可以是有价值的。因此，将剪枝整合到优化过程中始终是值得的。
- en: 7 Appendix-B
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 附录-B
- en: 'In Section [3.1.2](#S3.SS1.SSS2 "3.1.2 Input or Output Channel Pruning for
    Transformer ‣ 3.1 Pruning Structure Recognition ‣ 3 Methodology ‣ Greedy Output
    Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining"),
    we introduced the 2nd moment-based pruning metric for a standard depth-2 module.
    However, there are different variants of depth-2 modules, including the attention
    module and the gated feed-forward module. We describe the calculation of the metric
    for these variants in the following section.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在[3.1.2](#S3.SS1.SSS2 "3.1.2 输入或输出通道剪枝对于 Transformer ‣ 3.1 剪枝结构识别 ‣ 3 方法论 ‣
    贪婪输出近似：朝着高效结构化剪枝 LLMs 无需重新训练")节中，我们介绍了基于二阶矩的剪枝度量方法用于标准的深度2模块。然而，深度2模块有不同的变体，包括注意力模块和门控前馈模块。我们将在以下章节中描述这些变体的度量计算方法。
- en: 'Notations: To better demonstrate our method, let us first establish the notations.
    We focus on the pruning of Transformer-based large language models, thus we refer
    to the attention mechanism as $\mathbf{Cat}_{i=1}^{h}[\sigma_{1}(\mathbf{X}\mathbf{W}_{i}^{K}\mathbf{W}_{i}^{Q}\mathbf{X}^{\top})\mathbf{X}\mathbf{W}_{i}^{V}]\mathbf{W}^{O}$
    refers to the activation function for all of them, which can be SoftMax, ReLU,
    GeLU, or SiLU function.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 符号：为了更好地展示我们的方法，让我们首先建立符号。我们关注基于 Transformer 的大规模语言模型的剪枝，因此我们将注意力机制表示为 $\mathbf{Cat}_{i=1}^{h}[\sigma_{1}(\mathbf{X}\mathbf{W}_{i}^{K}\mathbf{W}_{i}^{Q}\mathbf{X}^{\top})\mathbf{X}\mathbf{W}_{i}^{V}]\mathbf{W}^{O}$，该符号表示所有激活函数，可以是
    SoftMax、ReLU、GeLU 或 SiLU 函数。
- en: 7.1 Calculation of 2nd-Moment-Based Metric for Attention Module
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 注意力模块的二阶矩度量计算
- en: Based on the above notations, we can treat the entire output of $\sigma_{1}(\mathbf{X}\mathbf{W}{i}^{K}\mathbf{W}{i}^{Q}\mathbf{X}^{\top})\mathbf{X}$
    is the number of attention heads), as the attention heads operate independently.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述符号，我们可以将 $\sigma_{1}(\mathbf{X}\mathbf{W}{i}^{K}\mathbf{W}{i}^{Q}\mathbf{X}^{\top})\mathbf{X}$
    的整体输出视为注意力头的数量，因为注意力头是独立操作的。
- en: 7.2 Calculation of 2nd-Moment-Based Metric for Gate Feed-forward Module
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 门控前馈模块的二阶矩度量计算
- en: For the gated feedforward module $\mathbf{W}^{D}(\mathbf{W}^{U}\mathbf{X}\cdot\sigma_{2}(\mathbf{W}^{G}\mathbf{X}))$,
    and calculate the 2nd-moment metric separately for them. Finally, we use the product
    of their own metric as the 2nd-moment metric for the entire module.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 对于门控前馈模块 $\mathbf{W}^{D}(\mathbf{W}^{U}\mathbf{X}\cdot\sigma_{2}(\mathbf{W}^{G}\mathbf{X}))$，分别计算它们的二阶矩度量。最后，我们使用它们各自度量的乘积作为整个模块的二阶矩度量。
- en: Overall, these approaches allow us to effectively prune channels in both attention
    and gated feedforward modules by leveraging the 2nd moment-based metric.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这些方法使我们能够通过利用基于二阶矩的度量有效地剪枝注意力和门控前馈模块中的通道。
- en: Algorithm 3 Pre-Pruning Recovery for Attention Module.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 算法3 注意力模块的预剪枝恢复。
- en: '1:Input: Attention ModuleModel layers with weights $\{k\_proj,q\_proj,v\_proj,o\_proj\}$18:end procedure'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：具有权重 $\{k\_proj,q\_proj,v\_proj,o\_proj\}$ 的注意力模块18:结束程序'
- en: 8 Appendix-C
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 附录-C
- en: 'In Section [3.3](#S3.SS3 "3.3 Pre-Pruning Recovery Without Retraining ‣ 3 Methodology
    ‣ Greedy Output Approximation: Towards Efficient Structured Pruning for LLMs Without
    Retraining"), we only provide the pre-pruning recovery algorithm for the standard
    depth-2 module, thus we describe the details of the recovery process for the attention
    module and gated feed-forward module in Algo [3](#alg3 "Algorithm 3 ‣ 7.2 Calculation
    of 2nd-Moment-Based Metric for Gate Feed-forward Module ‣ 7 Appendix-B ‣ Greedy
    Output Approximation: Towards Efficient Structured Pruning for LLMs Without Retraining")
    and Algo [4](#alg4 "Algorithm 4 ‣ 8 Appendix-C ‣ Greedy Output Approximation:
    Towards Efficient Structured Pruning for LLMs Without Retraining"), respectively.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在[3.3](#S3.SS3 "3.3 无需重新训练的预剪枝恢复 ‣ 3 方法论 ‣ 贪婪输出近似：朝着高效结构化剪枝 LLMs 无需重新训练")节中，我们仅提供了标准深度2模块的预剪枝恢复算法，因此我们在算法[3](#alg3
    "算法 3 ‣ 7.2 门控前馈模块的二阶矩度量计算 ‣ 7 附录-B ‣ 贪婪输出近似：朝着高效结构化剪枝 LLMs 无需重新训练")和算法[4](#alg4
    "算法 4 ‣ 8 附录-C ‣ 贪婪输出近似：朝着高效结构化剪枝 LLMs 无需重新训练")中分别描述了注意力模块和门控前馈模块的恢复过程的详细信息。
- en: Algorithm 4 Pre-Pruning Recovery for Gate Feed-forward Module.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 算法4 门控前馈模块的预剪枝恢复。
- en: '1:Input: Gated Feed-forward Module with weights $\{up\_proj,gate\_proj,down\_proj\}$16:end procedure'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '1: 输入：具有权重 $\{up\_proj,gate\_proj,down\_proj\}$ 的门控前馈模块16:结束程序'
- en: '9 Appendix-D: Broader Impact'
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '9 附录-D: 更广泛的影响'
- en: Our proposed method efficiently prunes large language models with billions of
    parameters. Our proposal intends to mitigate AI risks from critical perspectives
    like economic inequality and concentration of power and further democratize the
    use of AI models.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出的方法有效地剪枝了具有数十亿参数的大型语言模型。我们的提案旨在从经济不平等和权力集中等关键视角缓解 AI 风险，并进一步民主化 AI 模型的使用。
- en: 'Table 4: The zero-shot performance of the compressed Vicuna-7B (20% sparsity).
    Following the LLM-Pruner methodology Ma et al. [[2023](#bib.bib40)], we only prune
    the transformer blocks from the 4th to the 30th. The average performance is calculated
    across seven classification datasets. ’Bold’ indicates the best pruning-only performance,
    while ’underline’ represents the overall best performance.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：压缩后的 Vicuna-7B（20% 稀疏性）的零-shot 性能。遵循 LLM-Pruner 方法 Ma et al. [[2023](#bib.bib40)]，我们只剪枝从第
    4 到第 30 层的变换器块。平均性能是基于七个分类数据集计算的。**粗体**表示仅剪枝性能最佳，而*下划线*表示整体最佳性能。
- en: '| Pruning Methods | WikiText2 $\downarrow$ |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝方法 | WikiText2 $\downarrow$ |'
- en: '| Dense Touvron et al. [[2023a](#bib.bib55)], Ma et al. [[2023](#bib.bib40)]
    | 16.11 | 61.37 | 76.57 | 77.75 | 70.64 | 67.40 | 65.11 | 41.21 | 40.80 | 62.78
    |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 密集 Touvron et al. [[2023a](#bib.bib55)], Ma et al. [[2023](#bib.bib40)] |
    16.11 | 61.37 | 76.57 | 77.75 | 70.64 | 67.40 | 65.11 | 41.21 | 40.80 | 62.78
    |'
- en: '| Data Free Pruning |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 数据无关剪枝 |'
- en: '| Random Hoefler et al. [[2021](#bib.bib28)] | 34.63 | 112.44 | 61.47 | 70.89
    | 54.67 | 56.27 | 55.60 | 31.74 | 34.60 | 52.18 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 随机 Hoefler et al. [[2021](#bib.bib28)] | 34.63 | 112.44 | 61.47 | 70.89 |
    54.67 | 56.27 | 55.60 | 31.74 | 34.60 | 52.18 |'
- en: '| L2 norm Hoefler et al. [[2021](#bib.bib28)] | 3339.98 | 5882.21 | 55.90 |
    56.15 | 32.37 | 51.85 | 30.01 | 28.41 | 28.20 | 40.41 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| L2 范数 Hoefler et al. [[2021](#bib.bib28)] | 3339.98 | 5882.21 | 55.90 | 56.15
    | 32.37 | 51.85 | 30.01 | 28.41 | 28.20 | 40.41 |'
- en: '| Ours SG w/o remedy | 28.45 | 92.3 | 62.51 | 72.63 | 56.54 | 57.46 | 58.68
    | 33.29 | 36.2 | 53.91 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 SG 无修正 | 28.45 | 92.3 | 62.51 | 72.63 | 56.54 | 57.46 | 58.68 | 33.29
    | 36.2 | 53.91 |'
- en: '| Data Dependent Pruning |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 数据依赖剪枝 |'
- en: '| LLM-Pruner Vec Ma et al. [[2023](#bib.bib40)] | 27.03 | 92.51 | 62.17 | 71.44
    | 55.80 | 53.43 | 55.77 | 33.28 | 37.80 | 52.81 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner Vec Ma et al. [[2023](#bib.bib40)] | 27.03 | 92.51 | 62.17 | 71.44
    | 55.80 | 53.43 | 55.77 | 33.28 | 37.80 | 52.81 |'
- en: '| LLM-Pruner E2 Ma et al. [[2023](#bib.bib40)] | 24.70 | 94.34 | 62.87 | 75.41
    | 64.00 | 58.41 | 60.98 | 37.12 | 39.00 | 56.83 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner E2 Ma et al. [[2023](#bib.bib40)] | 24.70 | 94.34 | 62.87 | 75.41
    | 64.00 | 58.41 | 60.98 | 37.12 | 39.00 | 56.83 |'
- en: '| LLM-Pruner E1 Ma et al. [[2023](#bib.bib40)] | 25.74 | 92.88 | 61.70 | 75.30
    | 63.75 | 56.20 | 63.22 | 36.60 | 37.00 | 56.25 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner E1 Ma et al. [[2023](#bib.bib40)] | 25.74 | 92.88 | 61.70 | 75.30
    | 63.75 | 56.20 | 63.22 | 36.60 | 37.00 | 56.25 |'
- en: '| Ours (C) w/o remedy | 19.88 | 90.04 | 62.48 | 75.68 | 65.23 | 61.27 | 63.4
    | 35.49 | 37.6 | 57.31 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (C) 无修正 | 19.88 | 90.04 | 62.48 | 75.68 | 65.23 | 61.27 | 63.4 | 35.49
    | 37.6 | 57.31 |'
- en: '| Data Dependent Pruning w/ Retraining |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 数据依赖剪枝与再训练 |'
- en: '| LLM-Pruner LoRA Ma et al. [[2023](#bib.bib40)] | 18.97 | 76.78 | 60.40 |
    75.63 | 65.45 | 63.22 | 63.05 | 37.71 | 39.00 | 57.78 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner LoRA Ma et al. [[2023](#bib.bib40)] | 18.97 | 76.78 | 60.40 |
    75.63 | 65.45 | 63.22 | 63.05 | 37.71 | 39.00 | 57.78 |'
- en: 'Table 5: The zero-shot performance of the compressed Llama-13B (20% sparsity).
    The average performance is calculated across seven classification datasets. ’Bold’
    indicates the best pruning-only performance, while ’underline’ represents the
    overall best performance.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：压缩后的 Llama-13B（20% 稀疏性）的零-shot 性能。平均性能是基于七个分类数据集计算的。**粗体**表示仅剪枝性能最佳，而*下划线*表示整体最佳性能。
- en: '| Pruning Methods | WikiText2 $\downarrow$ |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 剪枝方法 | WikiText2 $\downarrow$ |'
- en: '| Dense Touvron et al. [[2023a](#bib.bib55)], Ma et al. [[2023](#bib.bib40)]
    | 11.58 | 20.24 | 68.47 | 78.89 | 76.24 | 70.09 | 74.58 | 44.54 | 42.00 | 64.97
    |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 密集 Touvron et al. [[2023a](#bib.bib55)], Ma et al. [[2023](#bib.bib40)] |
    11.58 | 20.24 | 68.47 | 78.89 | 76.24 | 70.09 | 74.58 | 44.54 | 42.00 | 64.97
    |'
- en: '| Data Free Pruning |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 数据无关剪枝 |'
- en: '| Random Hoefler et al. [[2021](#bib.bib28)] | 19.24 | 31.84 | 63.33 | 73.18
    | 63.54 | 60.85 | 64.44 | 36.26 | 38.00 | 57.09 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 随机 Hoefler et al. [[2021](#bib.bib28)] | 19.24 | 31.84 | 63.33 | 73.18 |
    63.54 | 60.85 | 64.44 | 36.26 | 38.00 | 57.09 |'
- en: '| L2 norm Hoefler et al. [[2021](#bib.bib28)] | 61.15 | 91.43 | 61.50 | 67.57
    | 52.90 | 57.54 | 50.13 | 31.14 | 36.80 | 51.08 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| L2 范数 Hoefler et al. [[2021](#bib.bib28)] | 61.15 | 91.43 | 61.50 | 67.57
    | 52.90 | 57.54 | 50.13 | 31.14 | 36.80 | 51.08 |'
- en: '| Ours SG w/o remedy | 18.47 | 29.87 | 66.51 | 74.63 | 68.54 | 61.35 | 66.80
    | 36.26 | 38.41 | 58.92 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 SG 无修正 | 18.47 | 29.87 | 66.51 | 74.63 | 68.54 | 61.35 | 66.80 | 36.26
    | 38.41 | 58.92 |'
- en: '| Data Dependent Pruning |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 数据依赖剪枝 |'
- en: '| LLM-Pruner Channel Ma et al. [[2023](#bib.bib40)] | 49.03 | 106.48 | 62.39
    | 66.87 | 49.17 | 58.96 | 49.62 | 31.83 | 33.20 | 50.29 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner 通道 Ma et al. [[2023](#bib.bib40)] | 49.03 | 106.48 | 62.39 | 66.87
    | 49.17 | 58.96 | 49.62 | 31.83 | 33.20 | 50.29 |'
- en: '| LLM-Pruner E1 Ma et al. [[2023](#bib.bib40)] | 16.01 | 29.28 | 67.68 | 77.15
    | 73.41 | 65.11 | 68.35 | 38.40 | 42.40 | 61.79 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner E1 Ma 等人 [[2023](#bib.bib40)] | 16.01 | 29.28 | 67.68 | 77.15
    | 73.41 | 65.11 | 68.35 | 38.40 | 42.40 | 61.79 |'
- en: '| Ours (C) w/o remedy | 15.90 | 28.33 | 68.48 | 77.78 | 74.73 | 65.01 | 68.90
    | 39.40 | 43.11 | 62.48 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 我们的 (C) 无修正 | 15.90 | 28.33 | 68.48 | 77.78 | 74.73 | 65.01 | 68.90 | 39.40
    | 43.11 | 62.48 |'
- en: '| Data Dependent Pruning w/ Retraining |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 数据依赖修剪 w/ 再训练 |'
- en: '| LLM-Pruner LoRA Ma et al. [[2023](#bib.bib40)] | 15.18 | 28.08 | 70.31 |
    77.91 | 75.16 | 67.88 | 71.09 | 42.41 | 43.40 | 64.02 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| LLM-Pruner LoRA Ma 等人 [[2023](#bib.bib40)] | 15.18 | 28.08 | 70.31 | 77.91
    | 75.16 | 67.88 | 71.09 | 42.41 | 43.40 | 64.02 |'
