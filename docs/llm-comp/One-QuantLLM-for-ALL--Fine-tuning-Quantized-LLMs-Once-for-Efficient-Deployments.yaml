- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:48:59'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:48:59
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一次性量化LLM：为了高效部署的量化LLMs微调
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20202](https://ar5iv.labs.arxiv.org/html/2405.20202)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2405.20202](https://ar5iv.labs.arxiv.org/html/2405.20202)
- en: Ke Yi South China University of Technology The Hong Kong University of Science
    and Technology Yuhui Xu Salesforce AI Research
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ke Yi 南方科技大学 香港科技大学 Yuhui Xu Salesforce AI Research
- en: cs_kerry@mail.scut.edu.cn Heng Chang Tsinghua University Chen Tang Tsinghua
    University Yuan Meng Tsinghua University Tong Zhang South China University of
    Technology Jia Li The Hong Kong University of Science and Technology
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: cs_kerry@mail.scut.edu.cn Heng Chang 清华大学 Chen Tang 清华大学 Yuan Meng 清华大学 Tong
    Zhang 南方科技大学 Jia Li 香港科技大学
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Large Language Models (LLMs) have advanced rapidly but face significant memory
    demands. While quantization has shown promise for LLMs, current methods typically
    require lengthy training to alleviate the performance degradation from quantization
    loss. However, deploying LLMs across diverse scenarios with different resource
    constraints, e.g., servers and personal computers, requires repeated training
    per application, which amplifies the lengthy training problem. Given that, it
    is advantageous to train a once-for-all (OFA) supernet capable of yielding diverse
    optimal subnets for downstream applications through one-shot training. Nonetheless,
    the scale of current language models impedes efficiency and amplifies interference
    from weight sharing between subnets. We make an initial attempt to extend the
    once-for-all framework to large language models. Specifically, we decouple shared
    weights to eliminate the interference and incorporate Low-Rank adapters for training
    efficiency. Furthermore, we observe the imbalance allocation of training resources
    from the traditional uniform sampling. A non-parametric scheduler is introduced
    to adjust the sampling rate for each quantization configuration, achieving a more
    balanced allocation among subnets with varying demands. We validate the approach
    on LLaMA2 families, and downstream evaluation confirms our ability to maintain
    high performance while significantly reducing deployment time faced with multiple
    scenarios.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）迅速发展，但面临着显著的内存需求。尽管量化技术对LLMs显示了潜力，但当前的方法通常需要长时间的训练来缓解量化损失带来的性能下降。然而，在不同资源约束的多样化场景中部署LLMs，例如服务器和个人电脑，需要针对每个应用进行重复训练，这加剧了长时间训练的问题。因此，训练一个一次性（OFA）超级网络，通过一次性训练生成适用于下游应用的各种最佳子网络是有利的。然而，当前语言模型的规模阻碍了效率，并且加剧了子网络之间共享权重的干扰。我们首次尝试将一次性框架扩展到大型语言模型。具体而言，我们解耦共享权重以消除干扰，并结合低秩适配器以提高训练效率。此外，我们观察到传统均匀采样下训练资源的分配不均衡。引入了非参数调度器来调整每个量化配置的采样率，实现子网络间更均衡的资源分配。我们在LLaMA2系列上验证了这一方法，下游评估确认了我们在面对多种场景时，能够保持高性能，同时显著减少部署时间。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models have shown surprising performance in the past years. However,
    they suffer from huge storage and computational costs; for example, inference
    with a LLaMA (Touvron et al.,, [2023](#bib.bib21)) model with 70B parameters needs
    at least 280 GB of GPU memory. To further boost the LLMs development for fitting
    diverse scenarios, recent studies have adopted quantization to compress the model
    size and reduce the computational costs.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型在过去几年中展现了惊人的性能。然而，它们面临巨大的存储和计算成本；例如，使用一个拥有70B参数的LLaMA（Touvron等，[2023](#bib.bib21)）模型进行推理需要至少280
    GB的GPU内存。为了进一步推动LLMs的发展以适应不同的场景，最近的研究采用了量化技术来压缩模型大小并减少计算成本。
- en: 'Previous works have extensively explored Post-Training Quantization (Frantar
    et al.,, [2022](#bib.bib7); Xiao et al.,, [2023](#bib.bib24); Lin et al.,, [2023](#bib.bib13))
    and Quantization-Aware Training (Dettmers et al.,, [2024](#bib.bib6); Xu et al.,,
    [2023](#bib.bib25)) to alleviate the memory cost of LLMs. Post-training quantization
    (PTQ) offers swift model compression, albeit at the potential expense of performance.
    In contrast, Quantization-aware training (QAT) alleviates performance losses by
    simulating quantization errors during training, which is considerably more time-consuming
    than standard fine-tuning. When we need to deploy LLMs for diverse scenarios with
    different resource constraints, repeated quantization-aware training per scenario
    is unacceptable, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ One
    QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")
    (a). From the above analysis, the training major the cost of deployments; hence,
    it would be beneficial to train a once-for-all (OFA) supernet capable of delivering
    optimal subnets with diverse configurations (e.g., quantization bit-width) for
    each application, as shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ One
    QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")
    (b).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '以前的研究广泛探讨了后训练量化（Frantar et al., [2022](#bib.bib7); Xiao et al., [2023](#bib.bib24);
    Lin et al., [2023](#bib.bib13)）和量化感知训练（Dettmers et al., [2024](#bib.bib6); Xu
    et al., [2023](#bib.bib25)），以减轻大语言模型（LLMs）的内存成本。后训练量化（PTQ）提供了快速的模型压缩，尽管可能会牺牲性能。相比之下，量化感知训练（QAT）通过在训练期间模拟量化错误来减轻性能损失，但这一过程比标准的微调更为耗时。当我们需要在具有不同资源限制的多种场景中部署LLMs时，对于每种场景进行重复的量化感知训练是不可接受的，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs
    Once for Efficient Deployments")（a）所示。从上述分析可以看出，训练是部署的主要成本；因此，训练一个一次性（OFA）超级网络，以便为每个应用提供具有不同配置（例如，量化位宽）的最佳子网络，将是有益的，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs
    Once for Efficient Deployments")（b）所示。'
- en: '![Refer to caption](img/f9d31c69112b1414794f5a3480cdf901.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f9d31c69112b1414794f5a3480cdf901.png)'
- en: 'Figure 1: (a) Compressing Large Language Models (LLMs) for deployment across
    various platforms while ensuring performance is a challenging task. Applying Quantization-Aware
    Training (QAT) for each platform is both time-consuming and costly. (b) Instead,
    our objective is to one-shot fine-tune one quantized LLM that can be efficiently
    specialized for multiple platforms. The one-shot fine-tuning process significantly
    reduces the investment. (c) The LLM-QFA framework excels in swiftly delivering
    optimal networks under different resource constraints in one shot, whereas the
    traditional method requires repeated fine-tuning.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: （a）为了在各种平台上部署大型语言模型（LLMs）并确保性能是一项具有挑战性的任务。对每个平台应用量化感知训练（QAT）既耗时又昂贵。（b）相反，我们的目标是一次性微调一个量化LLM，使其能够高效地专门化于多个平台。一次性微调过程显著减少了投入。（c）LLM-QFA框架在不同资源限制下能够快速交付最佳网络，而传统方法需要重复微调。'
- en: 'To the best of our knowledge, once-for-all quantization-aware training for
    LLMs has not been investigated, primarily due to the large scale of current language
    models and the high cost of traditional QAT. Previous researches based on once-for-all
    mainly utilize the weight-sharing strategy, which helps avoid model size explosion
    caused by allocating weight for each configuration (Wang et al.,, [2020](#bib.bib22);
    Chen et al.,, [2021](#bib.bib3)). However, the weight-sharing combined with traditional
    QAT still has problems two-fold: 1) various quantization configurations (e.g.,
    2, 3, 4 bit-width) share the weight but have different orders of magnitude of
    quantization noise, resulting in the noteworthy interference problem and optimization
    challenges (Tang et al.,, [2024](#bib.bib17)). 2) Tradition QAT is based on full-finetuning,
    combined with the time-consuming process of simulating quantization errors, which
    is inefficient even under the weight-sharing scheme.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，一次性量化感知训练（OFA）尚未在LLMs中得到研究，主要是由于当前语言模型的规模庞大和传统QAT的高成本。基于一次性的先前研究主要利用权重共享策略，这有助于避免由于为每种配置分配权重而导致的模型规模膨胀（Wang
    et al., [2020](#bib.bib22); Chen et al., [2021](#bib.bib3)）。然而，结合传统QAT的权重共享仍然存在两方面的问题：1)
    不同的量化配置（例如，2、3、4位宽）共享权重，但具有不同量级的量化噪声，导致显著的干扰问题和优化挑战（Tang et al., [2024](#bib.bib17)）。2)
    传统QAT基于完全微调，结合了模拟量化错误的耗时过程，即使在权重共享方案下也效率低下。
- en: 'Furthermore, our observations reveal that the uniform sampling strategy used
    by traditional OFA brings an imbalance in the allocation of training resources.
    As illustrated in Figure [3](#S3.F3 "Figure 3 ‣ Resource-Balance Sampling Strategy.
    ‣ 3.2 One-Shot Optimization ‣ 3 Methodology ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments"), subnets derived from uniform
    sampling exhibit a bias on their average bit-width, which falls into a low variance
    distribution. Consequently, subnets whose average bit-width deviates from this
    distribution are prone to under-fitting.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们的观察揭示了传统OFA使用的均匀采样策略在训练资源分配上的不平衡。如图[3](#S3.F3 "图 3 ‣ 资源平衡采样策略。 ‣ 3.2 一次优化
    ‣ 3 方法 ‣ 一个QuantLLM适用于所有：一次性微调量化LLMs以实现高效部署")所示，从均匀采样得出的子网在其平均比特宽度上存在偏差，导致低方差分布。因此，平均比特宽度偏离该分布的子网容易出现欠拟合。
- en: Integrating these aspects, we propose the LLM-QFA (Quantization-Aware Fine-tuning
    one LLM for All scenarios) framework that efficiently fine-tunes the once-for-all
    supernet for later yielding optimal subnets for diverse scenarios. First, we introduce
    interference-less fine-tuning to decouple the weights of different configurations,
    accompanied by Low-Rank adapters to enable efficient training. Specifically, we
    quantize the weights with different quantization configurations and freeze them,
    then we apply Low-Rank adapters to each quantized weight for later fine-tuning.
    Second, we propose a resource-balanced sampling strategy, which is based on a
    non-parametric scheduler that dynamically adjusts the sampling strategy across
    training steps.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 综合这些方面，我们提出了LLM-QFA（量化感知微调一个LLM以适应所有场景）框架，该框架高效地微调一次性超网，从而为不同场景生成最佳子网。首先，我们引入了无干扰微调来解耦不同配置的权重，并配合低秩适配器以实现高效训练。具体来说，我们对权重进行不同量化配置的量化并冻结，然后将低秩适配器应用于每个量化权重，以便后续微调。其次，我们提出了一种资源平衡的采样策略，该策略基于非参数调度器，动态调整训练步骤中的采样策略。
- en: 'To evaluate our proposed framework, we conduct experiments on LLaMA2 models
    and validate the performance on the MMLU and Common Sense QA benchmarks. The results
    show that our proposed framework can yield diverse optimal quantized models for
    various scenarios. It is worth noting that our framework can be easily scaled
    up to even larger models since the training time per step is the same with previous
    LoRA-tuning (Xu et al.,, [2023](#bib.bib25)). We summarize our contributions as
    follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们提出的框架，我们在LLaMA2模型上进行实验，并在MMLU和常识问答基准上验证其性能。结果表明，我们提出的框架能够为各种场景生成不同的最佳量化模型。值得注意的是，由于每步的训练时间与以前的LoRA微调（Xu等，
    [2023](#bib.bib25)）相同，因此我们的框架可以很容易地扩展到更大的模型。我们总结了以下贡献：
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We first introduce the once-for-all training paradigm for large language models
    (LLMs), which helps to reduce the training cost for deploying LLMs across diverse
    scenarios.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们首先引入了一次性训练范式用于大型语言模型（LLMs），这有助于降低在不同场景下部署LLMs的训练成本。
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: we decouple weights of configurations to mitigate interference issues and incorporate
    Low-Rank adapters to enhance the training efficiency.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们解耦配置的权重以缓解干扰问题，并结合低秩适配器以提高训练效率。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To address the imbalance training caused by the uniform sampling strategy, we
    propose a resource-balanced sampling strategy that focuses on providing fair sampled
    opportunity across subnets with various resource demands.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了应对均匀采样策略带来的训练不平衡，我们提出了一种资源平衡的采样策略，旨在为具有不同资源需求的子网提供公平的采样机会。
- en: 2 Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 相关工作
- en: LLM Quantization
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM 量化
- en: Quantization is a compression technique that reduces the bit-width of weights
    and/or activations to save memory and accelerate inference. The quantization of
    LLM can be categorized into two main lines. The first one is post-training quantization
    (PTQ) (Frantar et al.,, [2022](#bib.bib7); Xiao et al.,, [2023](#bib.bib24); Lin
    et al.,, [2023](#bib.bib13); Kim et al.,, [2023](#bib.bib10)), which focuses on
    reducing the memory footprint without retraining. Although lots of designs are
    designed to mitigate the degradation of performance, *e.g.*, handling outliers
    in parameters (Kim et al.,, [2023](#bib.bib10); [Li et al., 2023a,](#bib.bib11)
    ) and dynamic quantization (Xiao et al.,, [2023](#bib.bib24); Lin et al.,, [2023](#bib.bib13)),
    PTQ still have to drop the ultra-low bit-width (*e.g.*, 2 bit and 3 bit) to guarantee
    the performance. Hence, the second line, Quantization-Aware Training (QAT) can
    help alleviate the performance drop. The first QAT method applied on LLM (Liu
    et al.,, [2023](#bib.bib14)) inherits the idea of traditional QAT, which is computationally
    expensive in the fine-tuning stage. To reduce the training cost, (Dettmers et al.,,
    [2024](#bib.bib6); Xu et al.,, [2023](#bib.bib25); Guo et al.,, [2023](#bib.bib8);
    [Li et al., 2023b,](#bib.bib12) ) utilizing LoRA-tuning on quantized weight and
    gain a decent performance. Specifically, (Xu et al.,, [2023](#bib.bib25)) adds
    constraints on LoRA to maintain the quantization property after merging between
    LoRA weight and quantization weight, which firstly brings LoRA-tuning to actual
    quantization-aware training. Though Lora-tuning can save memory footprint and
    training costs, when faced with diverse development scenarios with different resource
    constraints, LoRA-tuning still falls into the pitfall of repeated training.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种压缩技术，减少权重和/或激活的比特宽度，以节省内存并加速推理。LLM 的量化可以分为两个主要方向。第一个是训练后量化（PTQ）（Frantar
    et al., [2022](#bib.bib7); Xiao et al., [2023](#bib.bib24); Lin et al., [2023](#bib.bib13);
    Kim et al., [2023](#bib.bib10)），专注于减少内存占用而无需重新训练。尽管有许多设计旨在减轻性能退化，如处理参数中的异常值（Kim
    et al., [2023](#bib.bib10); [Li et al., 2023a,](#bib.bib11)）和动态量化（Xiao et al.,
    [2023](#bib.bib24); Lin et al., [2023](#bib.bib13)），PTQ 仍然需要降低超低比特宽度（*例如*，2 位和
    3 位）以保证性能。因此，第二种方法，量化感知训练（QAT），可以帮助缓解性能下降。首个应用于 LLM 的 QAT 方法（Liu et al., [2023](#bib.bib14)）继承了传统
    QAT 的理念，但在微调阶段计算开销很大。为了降低训练成本，（Dettmers et al., [2024](#bib.bib6); Xu et al.,
    [2023](#bib.bib25); Guo et al., [2023](#bib.bib8); [Li et al., 2023b,](#bib.bib12)）在量化权重上应用了
    LoRA 调优，并取得了不错的性能。具体而言，（Xu et al., [2023](#bib.bib25)）对 LoRA 施加了约束，以在 LoRA 权重与量化权重合并后保持量化属性，这首次将
    LoRA 调优带入实际的量化感知训练。尽管 LoRA 调优可以节省内存占用和训练成本，但在面对具有不同资源约束的多样化开发场景时，LoRA 调优仍然陷入了重复训练的陷阱。
- en: Once for All training
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 一次训练即可
- en: Once-for-all training (OFA) methods (Wang et al.,, [2020](#bib.bib22); Chen
    et al.,, [2021](#bib.bib3); Yu et al.,, [2020](#bib.bib26); Tang et al.,, [2023](#bib.bib19),
    [2022](#bib.bib18)) aim to train a one-shot supernet that can serve diverse scenarios
    with different resource constraints and save expensive retraining per scenario.
    On non-LLMs, the success of one-shot training comes from the weight-sharing scheme
    between different configurations (Chen et al.,, [2021](#bib.bib3); Yu et al.,,
    [2020](#bib.bib26)), while weight-sharing also brings interference between different
    bit-widths for quantization-aware training (Tang et al.,, [2024](#bib.bib17),
    [2023](#bib.bib19)). Moreover, traditional OFA with weight sharing necessitates
    fine-tuning entire parameters, which is impracticable for LLMs due to their extensive
    size.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一次性训练（OFA）方法（Wang et al., [2020](#bib.bib22); Chen et al., [2021](#bib.bib3);
    Yu et al., [2020](#bib.bib26); Tang et al., [2023](#bib.bib19), [2022](#bib.bib18)）旨在训练一个一次性超网络，能够服务于具有不同资源约束的多样化场景，并节省每个场景的昂贵重新训练。在非
    LLM 的情况下，一次性训练的成功来自于不同配置之间的权重共享方案（Chen et al., [2021](#bib.bib3); Yu et al., [2020](#bib.bib26)），而权重共享也会带来不同比特宽度之间的干扰，尤其是在量化感知训练中（Tang
    et al., [2024](#bib.bib17), [2023](#bib.bib19)）。此外，传统的 OFA 需要对整个参数进行微调，这对于 LLM
    来说由于其庞大规模而不可行。
- en: 3 Methodology
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: 3.1 Problem definition
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 问题定义
- en: This paper focuses on the dimension of quantization to compress the LLMs for
    efficient deployment across diverse scenarios, which involves 1) post-training
    quantization to compress LLMs and 2) constructing the layer-wise mixed-precision
    supernet based on quantized LLMs and 3) optimizing the supernet.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本文关注于量化的维度，以压缩 LLM，以便在各种场景下高效部署，这涉及 1) 训练后量化以压缩 LLM 以及 2) 基于量化 LLM 构建层级混合精度超网络，以及
    3) 优化超网络。
- en: Post-training Quantization
  id: totrans-33
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练后量化
- en: To reduce memory cost, it is effective to quantize the pre-trained weight of
    LLMs in low-bit representation; mathematically, given the bit-width $\mathbf{N}$,
    the quantization process can be defined as
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低内存成本，量化预训练LLM的权重为低位表示是有效的；从数学上讲，给定位宽$\mathbf{N}$，量化过程可以定义为
- en: '|  | $\hat{\mathbf{W}}=\lfloor\frac{\mathbf{W}-\beta}{\alpha}\rceil,\alpha=(\max(\mathbf{W})-\min(\mathbf{W}))/(2^{N}-1),\beta=\min(\mathbf{W}),$
    |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\mathbf{W}}=\lfloor\frac{\mathbf{W}-\beta}{\alpha}\rceil,\alpha=(\max(\mathbf{W})-\min(\mathbf{W}))/(2^{N}-1),\beta=\min(\mathbf{W}),$
    |  | (1) |'
- en: where $\alpha$. Here, only two float point numbers and a series of integers
    are needed for storage and computation memory,
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，只有两个浮点数和一系列整数用于存储和计算内存。
- en: Layer-wise Mixed-precision Supernet
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 层级混合精度超网
- en: In contrast to uniform bit-width quantization, mixed-precision quantization,
    which allows for varying bit-widths across different layers, can yield superior
    performance by capitalizing on the inherent redundancy in specific layers. In
    this work, we build a supernet containing different quantization bit-width configurations
    layer-wisely. Each single path of the supernets denotes a mixed-precision LLM
    and we aim to optimize all single paths, which can be formulated as
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与均匀位宽量化相比，混合精度量化允许在不同层之间使用不同的位宽，可以通过利用特定层中的固有冗余来获得更好的性能。在这项工作中，我们构建了一个包含不同量化位宽配置的超网。超网的每条单一路径表示一个混合精度LLM，我们的目标是优化所有单一路径，这可以表述为
- en: '|  | $1$2 |  | (2) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where $s_{i}$. Our target is to 1) optimize all the subnets at once and 2) offer
    optimal subnets under given resource constraints.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里$s_{i}$。我们的目标是1) 一次性优化所有子网和2) 在给定的资源约束下提供最佳子网。
- en: 3.2 One-Shot Optimization
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 一次性优化
- en: Interference-Less Fine-tuning.
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 无干扰微调。
- en: We have observed that previous one-shot training methodologies (Cai et al.,,
    [2019](#bib.bib2); Yu et al.,, [2020](#bib.bib26)) gained success from their weight-sharing
    scheme, which avoids large model sizes caused by saving the weight of each configuration.
    However, the weight-sharing scheme also brings interference problems. Specifically,
    high and low bit-width have different quantization noise, and significantly superimposed
    quantization noise leads to optimization challenges (Tang et al.,, [2024](#bib.bib17)).
    To alleviate interference between different configurations, the straightforward
    approach is to decouple shared weights and assign weights for each configuration,
    which is costly for large-size models.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到，之前的一次性训练方法（Cai et al., [2019](#bib.bib2); Yu et al., [2020](#bib.bib26)）通过其权重共享方案获得了成功，该方案避免了由于保存每个配置的权重而导致的大型模型尺寸。然而，权重共享方案也带来了干扰问题。具体来说，高位宽和低位宽具有不同的量化噪声，显著叠加的量化噪声导致了优化挑战（Tang
    et al., [2024](#bib.bib17)）。为了减轻不同配置之间的干扰，直接的方法是解耦共享权重并为每个配置分配权重，这对大型模型来说成本很高。
- en: '![Refer to caption](img/5e8b39cf2d4c4da57b6cb201d6a50213.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/5e8b39cf2d4c4da57b6cb201d6a50213.png)'
- en: 'Figure 2: An illustration of the goal of LLM-QFA. Compared with traditional
    OFA with Quantization-Aware Training, our approach circumvents interference issues
    by decoupling shared weight and incorporating the Low-Rank Adapter to further
    enhance the training efficiency. More notably, we employ a resource-balance sampling
    strategy to expedite the convergence of subnets across resource constraints.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：LLM-QFA目标的示意图。与传统的量化感知训练（OFA）相比，我们的方法通过解耦共享权重和结合低秩适配器来进一步提高训练效率，从而规避了干扰问题。更值得注意的是，我们采用了资源平衡采样策略，以加快在资源约束下子网的收敛速度。
- en: 'Hence, we incorporate Low-Rank adapters to represent each quantization configuration,
    which only brings negligible extra cost compared with the size of LLMs. Specifically,
    the forward process can be defined as:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们结合了低秩适配器来表示每种量化配置，这相比于LLMs的大小只带来了微不足道的额外成本。具体来说，前向过程可以定义为：
- en: '|  | $\mathbf{Y}={\alpha_{i}}\cdot\hat{\mathbf{W}_{i}}\cdot\mathbf{X}+{\beta_{i}}\cdot\mathbf{X}+\mathbf{B_{i}A_{i}}\cdot\mathbf{X},$
    |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{Y}={\alpha_{i}}\cdot\hat{\mathbf{W}_{i}}\cdot\mathbf{X}+{\beta_{i}}\cdot\mathbf{X}+\mathbf{B_{i}A_{i}}\cdot\mathbf{X},$
    |  | (3) |'
- en: where ${\alpha_{i}},{\beta_{i}},\hat{\mathbf{W_{i}}}$ denotes the weight of
    Low-Rank adapters. It is noticed that, during fine-tuning, only one of the Low-Rank
    adapters is updated, which is the key to avoiding interference between different
    configurations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\alpha_{i}},{\beta_{i}},\hat{\mathbf{W_{i}}}$ 表示低秩适配器的权重。值得注意的是，在微调过程中，仅更新一个低秩适配器，这是避免不同配置之间干扰的关键。
- en: To avoid heterogeneity between float point LoRA weights and quantized weight,
    which hinder the acceleration for inference, we follow QA-LoRA (Xu et al.,, [2023](#bib.bib25))
    to add constraints on adapters’ weight for preserving quantization property after
    merging.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免浮点 LoRA 权重和量化权重之间的异质性，这会阻碍推理加速，我们遵循 QA-LoRA (Xu et al., [2023](#bib.bib25))，在适配器权重上添加约束，以保持合并后的量化特性。
- en: Integrating the above designs, the task of optimizing all subnets can be formulated
    as
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 结合以上设计，优化所有子网的任务可以表述为
- en: '|  | $\min_{\mathbf{W}_{L}}\sum_{a_{i}}\mathcal{L}_{val}\big{(}f(\mathbf{W}_{L},\mathbf{W}_{Q},a_{i})\big{)},$
    |  | (4) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\mathbf{W}_{L}}\sum_{a_{i}}\mathcal{L}_{val}\big{(}f(\mathbf{W}_{L},\mathbf{W}_{Q},a_{i})\big{)},$
    |  | (4) |'
- en: where $f(\mathbf{W}_{L},\mathbf{W}_{Q},a_{i})$.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(\mathbf{W}_{L},\mathbf{W}_{Q},a_{i})$。
- en: Resource-Balance Sampling Strategy.
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资源平衡采样策略。
- en: Fine-tuning all the subnets is a multi-objective problem. Given the impracticality
    of enumerating and tuning every subnet at each training iteration, a simplistic
    yet sub-optimal approach is to uniformly sample a few subnets from the configuration
    space for fine-tuning. Specifically, each layer has a uniform probability of choosing
    one quantization configuration, which can be formulated as $\textbf{P}(Q_{l,i})=\frac{1}{N}$.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 微调所有子网是一个多目标问题。考虑到在每次训练迭代中枚举和调整每个子网的不切实际性，一个简单但次优的方法是从配置空间中均匀采样几个子网进行微调。具体来说，每一层选择一个量化配置的概率是均匀的，这可以表述为
    $\textbf{P}(Q_{l,i})=\frac{1}{N}$。
- en: 'Though it seems fair, the naive uniform sampling strategy is biased toward
    subnets whose average bit-width is close to its expected value. Assume variable
    $q_{i}$] are independent, hence the average of bit-width can be formulated as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管看起来公平，但天真的均匀采样策略偏向于那些平均比特宽度接近其期望值的子网。假设变量 $q_{i}$ 是独立的，因此比特宽度的平均值可以表述为：
- en: '|  | $\displaystyle\text{Var}[Bit(s)]$ |  | (5) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Var}[Bit(s)]$ |  | (5) |'
- en: where the $Bit(s)$. Hence, the subnet with an average bit-width far from the
    distribution center would get unbalanced training resources.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Bit(s)$。因此，平均比特宽度远离分布中心的子网将获得不平衡的训练资源。
- en: '![Refer to caption](img/0b4e11be7443e796d5008c7f1ae3d304.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0b4e11be7443e796d5008c7f1ae3d304.png)'
- en: 'Figure 3: (a) Distribution of average bit-width of samples obtained from uniform
    sampling, approximating a low variance Gaussian distribution. (b) Mixed Gaussian
    Distribution can approximate Uniform Distribution. (c) Showcase of our Resource-Balance
    sampling strategy.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3： (a) 从均匀采样中获得的样本的平均比特宽度分布，近似为低方差的高斯分布。 (b) 混合高斯分布可以近似均匀分布。 (c) 展示我们的资源平衡采样策略。
- en: The negative impact of a uniform sampling strategy has not been studied previously.
    One of the reasons is that the weight-sharing scheme has all configurations updated
    frequently, though suffering from interference problems. Under the interference-less
    setting, weights are updated more sparsely; hence, the unbalanced training would
    lead to more pronounced under-fitting.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀采样策略的负面影响以前没有被研究过。原因之一是权重共享方案使所有配置频繁更新，但遭受干扰问题。在无干扰的设置下，权重更新更稀疏，因此不平衡训练会导致更明显的欠拟合。
- en: 'Revealed by Figure [3](#S3.F3 "Figure 3 ‣ Resource-Balance Sampling Strategy.
    ‣ 3.2 One-Shot Optimization ‣ 3 Methodology ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments") (b), straightforwardly stacking
    normal distributions with different means can approximate a uniform distribution
    for $Bit(s)$ and alleviate the imbalance problem. From the implementation perspective,
    mixed Gaussian distribution can be achieved by setting different sampling strategies
    for configurations across training steps. The process can be formulated as'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S3.F3 "图 3 ‣ 资源平衡采样策略 ‣ 3.2 单次优化 ‣ 3 方法论 ‣ 一次性量化 LLM：高效部署的量化 LLM 微调")
    (b) 揭示了，通过简单地堆叠具有不同均值的正态分布可以近似均匀分布，从而减轻 $Bit(s)$ 的不平衡问题。从实现的角度来看，通过为训练步骤中的配置设置不同的采样策略，可以实现混合高斯分布。该过程可以表述为
- en: '|  | $\displaystyle\text{E}[Bit(s,t)]=(b_{N}-b_{1})\cdot\lvert 2\cdot\frac{t}{SL}-1\rvert,$
    |  | (6) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{E}[Bit(s,t)]=(b_{N}-b_{1})\cdot\lvert 2\cdot\frac{t}{SL}-1\rvert,$
    |  | (6) |'
- en: where $SL$, leading to a smooth switchover between schedule epochs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$SL$，导致在调度周期之间平滑切换。
- en: Compared to the uniform sampling strategy, our approach prevents bias on subnets
    in median size. Therefore, the subnet space converges more efficiently, which
    makes the following search process more effective. Compared to a shared-weight
    scheme, our approach can alleviate the interference problem with negligible extra
    memory costs. As a result, our approach provides a more efficient and effective
    way to optimize the Layer-wise Mixed-precision Supernet, which can be efficiently
    deployed in different scenarios with diverse resource constraints.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与均匀采样策略相比，我们的方法可以防止中等大小子网的偏差。因此，子网空间更高效地收敛，这使得后续的搜索过程更加有效。与共享权重方案相比，我们的方法可以缓解干扰问题，并且额外的内存成本几乎可以忽略不计。因此，我们的方法提供了一种更高效、更有效的优化分层混合精度超网的方式，可以在不同的资源约束场景中高效部署。
- en: 3.3 Search Optimized Subnet
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 搜索优化子网
- en: We decouple the fine-tuning process and the searching process. No extra retraining
    cost is needed when finding the optimal subnet under the given resource constraint.
    The searching process starts with random searching, where a few subnets are sampled.
    Then, correlation analysis between the subnets’ performance on the validation
    set and the quantization bit-width of each layer is conducted. Learning from the
    correlation, the sensitivity of each layer to quantization bit-width can be obtained
    and the search space can be further narrowed down. Finally, we further sample
    subnets from the narrowed search space, and the final optimal subnet is selected
    based on the performance of the validation set.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将微调过程与搜索过程解耦。在找到给定资源约束下的最佳子网时，无需额外的重新训练成本。搜索过程从随机搜索开始，随机选择一些子网。然后，进行子网在验证集上的性能与每层量化比特宽度之间的相关性分析。通过相关性学习，可以获得每层对量化比特宽度的敏感度，并进一步缩小搜索空间。最后，我们从缩小的搜索空间中进一步采样子网，并根据验证集的性能选择最终的最佳子网。
- en: 4 Experiments
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Settings
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: Models and Quantization.
  id: totrans-69
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型和量化。
- en: We conduct experiments on two LLMs, LLaMA2-7b and LLaMA2-13b. The quantization
    is based on GPTQ (Frantar et al.,, [2022](#bib.bib7)) with 2, 3, 4 bit-width quantization.
    The detailed quantization configuration, *e.g.*, group size, and order, are consistent
    with QA-LoRA (Xu et al.,, [2023](#bib.bib25)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对两个LLMs进行实验，LLaMA2-7b和LLaMA2-13b。量化基于GPTQ (Frantar et al.,, [2022](#bib.bib7))，具有2、3、4比特宽度量化。详细的量化配置，*例如*，组大小和顺序，与QA-LoRA
    (Xu et al.,, [2023](#bib.bib25))一致。
- en: Datasets and Training Details.
  id: totrans-71
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集和训练细节。
- en: We fine-tune models with Alpaca (Taori et al.,, [2023](#bib.bib20)), which contains
    52K instruction-following data generated from GPT 3.5 (Wang et al.,, [2022](#bib.bib23)).
    The length of one schedule epoch is 8k training steps. Following previous works(Dettmers
    et al.,, [2024](#bib.bib6); Xu et al.,, [2023](#bib.bib25)), we use a paged AdamW
    optimizer with a batch size 16 and a learning rate of $2\times 10^{-5}$. The training
    process is conducted on one A100 GPU, and only 8 GPU hours are needed to fine-tune
    one LLaMA2-7b-based supernet with 10K steps.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Alpaca (Taori et al.,, [2023](#bib.bib20))对模型进行微调，该数据集包含52K条由GPT 3.5 (Wang
    et al.,, [2022](#bib.bib23))生成的指令跟随数据。一个调度周期的长度为8k训练步骤。按照以前的工作(Dettmers et al.,,
    [2024](#bib.bib6); Xu et al.,, [2023](#bib.bib25))，我们使用了分页AdamW优化器，批量大小为16，学习率为$2\times
    10^{-5}$。训练过程在一个A100 GPU上进行，微调一个基于LLaMA2-7b的超网需要10K步骤，只需8个GPU小时。
- en: Evaluation.
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估。
- en: 'We evaluate the performance of the models on MMLU (Hendrycks et al.,, [2021](#bib.bib9))
    and Common Sense QA benchmarks. The MMLU dataset contains four categories: Humanities,
    STEM, Social, and Other. The Common Sense QA benchmarks include HellaSwag (Zellers
    et al.,, [2019](#bib.bib27)), PIQA (Bisk et al.,, [2020](#bib.bib1)), WinoGrande
    (Sakaguchi et al.,, [2021](#bib.bib16)), ARC-e, ARC-c (Clark et al.,, [2018](#bib.bib5)),
    BoolQ (Clark et al.,, [2019](#bib.bib4)), and OBQA (Mihaylov et al.,, [2018](#bib.bib15)).
    For the MMLU Benchmark, we search the optimal subnets on the MMLU evaluation dataset.
    Initially, we sampled the first 100 subnets randomly and subsequently employed
    a shrinkage strategy to sample an additional 50 subnets, denoted as [100, 50].
    For the Common Sense QA datasets, we similarly searched for optimal subnets on
    the ARC-C dataset with [100,50] setting. We report the $0$-shot accuracy on Common
    Sense QA benchmarks.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 MMLU (Hendrycks et al.,, [2021](#bib.bib9)) 和 Common Sense QA 基准测试中评估了模型的性能。MMLU
    数据集包含四个类别：人文、STEM、社会和其他。Common Sense QA 基准测试包括 HellaSwag (Zellers et al.,, [2019](#bib.bib27))、PIQA
    (Bisk et al.,, [2020](#bib.bib1))、WinoGrande (Sakaguchi et al.,, [2021](#bib.bib16))、ARC-e、ARC-c
    (Clark et al.,, [2018](#bib.bib5))、BoolQ (Clark et al.,, [2019](#bib.bib4)) 和
    OBQA (Mihaylov et al.,, [2018](#bib.bib15))。对于 MMLU 基准，我们在 MMLU 评估数据集上搜索了最佳子网。最初，我们随机采样了前
    100 个子网，然后采用收缩策略额外采样了 50 个子网，记作 [100, 50]。对于 Common Sense QA 数据集，我们同样在 ARC-C 数据集上进行了最佳子网搜索，设置为
    [100,50]。我们报告了在 Common Sense QA 基准上的 $0$-shot 准确率。
- en: 4.2 Main Results
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 主要结果
- en: '![Refer to caption](img/a5bd6e1067f9334dd79db5383504e4d0.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a5bd6e1067f9334dd79db5383504e4d0.png)'
- en: 'Figure 4: Left: The time required to obtain N specialized networks varies across
    methods. Our proposed QFA approach significantly reduces the time cost compared
    to the QA-LoRA method and achieves a comparable efficiency level to the pure quantization
    technique, GPTQ. Right: For each method, we obtain three specialized networks
    under (2, 3, 4) bit constraints on the LLaMA2-7b and LLaMA2-13B models. The average
    accuracy on the $5$-shot MMLU benchmark for networks quantized at (2, 3, 4) bits
    is reported. Although GPTQ can achieve a lower time cost, it is accompanied by
    an unacceptable level of performance degradation. Full results are provided in
    Table [4.2](#S4.SS2 "4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL:
    Fine-tuning Quantized LLMs Once for Efficient Deployments").'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4：左图：获取 N 个专业网络所需的时间在不同方法之间有所不同。我们提出的 QFA 方法与 QA-LoRA 方法相比显著减少了时间成本，并且达到了与纯量化技术
    GPTQ 相当的效率水平。右图：对于每种方法，我们在 LLaMA2-7b 和 LLaMA2-13B 模型上，在 (2, 3, 4) 位约束下获取了三个专业网络。报告了在
    $5$-shot MMLU 基准测试中 (2, 3, 4) 位量化网络的平均准确率。尽管 GPTQ 可以实现更低的时间成本，但其伴随的性能退化程度不可接受。完整结果见表
    [4.2](#S4.SS2 "4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments")。'
- en: 'Table 1: 0-shot and 5-shot accuracy (%) on the Massive Multitask Language Understanding
    (MMLU) dataset. Each block is based on the same foundation model specified in
    the first row. For each method, we present the metrics achieved under the bit-width
    resource constraints of 2, 3, 4, as well as the corresponding averages.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：在大规模多任务语言理解 (MMLU) 数据集上的 $0$-shot 和 $5$-shot 准确率（%）。每个数据块基于第一行中指定的相同基础模型。对于每种方法，我们展示了在
    2、3、4 位宽度资源约束下所实现的指标以及相应的平均值。
- en: '| Method | Bit | MMLU ($0$-shot) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 | MMLU ($0$-shot) |'
- en: '| --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Const. | Hums. | STEM | Social | Other | Avg. | Hums. | STEM | Social | Other
    | Avg. |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 常量 | 人文 | STEM | 社会 | 其他 | 平均 | 人文 | STEM | 社会 | 其他 | 平均 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA2-7b | 16 | 48.3 | 35.2 | 48.8 | 45.8 | 43.6 | 51.6 | 37.3 | 52.2 |
    49.9 | 46.8 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7b | 16 | 48.3 | 35.2 | 48.8 | 45.8 | 43.6 | 51.6 | 37.3 | 52.2 |
    49.9 | 46.8 |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 40.4 | 33.7 | 45.9 | 42.2 | 39.9 | 50.5
    | 36.9 | 50.5 | 47.5 | 45.1 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 40.4 | 33.7 | 45.9 | 42.2 | 39.9 | 50.5
    | 36.9 | 50.5 | 47.5 | 45.1 |'
- en: '| GPTQ | 3 | 28.8 | 25.8 | 25.6 | 28.0 | 27.0 | 31.6 | 28.2 | 25.6 | 32.9 |
    30.7 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 3 | 28.8 | 25.8 | 25.6 | 28.0 | 27.0 | 31.6 | 28.2 | 25.6 | 32.9 |
    30.7 |'
- en: '| GPTQ | 2 | 23.8 | 23.7 | 22.5 | 23.8 | 23.5 | 24.3 | 23.0 | 23.9 | 26.1 |
    24.2 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 2 | 23.8 | 23.7 | 22.5 | 23.8 | 23.5 | 24.3 | 23.0 | 23.9 | 26.1 |
    24.2 |'
- en: '| GPTQ | Avg. |  |  |  |  | 30.1 |  |  |  |  | 33.3 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 平均 |  |  |  |  | 30.1 |  |  |  |  | 33.3 |'
- en: '| \hdashline[0.8pt/1pt] QA-LoRA | 4 | 49.7 | 37.5 | 51.4 | 47.8 | 45.7 | 49.8
    | 36.8 | 49.8 | 47.8 | 45.1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] QA-LoRA | 4 | 49.7 | 37.5 | 51.4 | 47.8 | 45.7 | 49.8
    | 36.8 | 49.8 | 47.8 | 45.1 |'
- en: '| QA-LoRA | 3 | 43.3 | 33.7 | 44.8 | 42.9 | 40.5 | 40.2 | 34.8 | 44.1 | 40.8
    | 39.5 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 3 | 43.3 | 33.7 | 44.8 | 42.9 | 40.5 | 40.2 | 34.8 | 44.1 | 40.8
    | 39.5 |'
- en: '| QA-LoRA | 2 | 32.6 | 27.2 | 35.6 | 33.2 | 31.7 | 27.2 | 26.9 | 29.0 | 30.5
    | 28.3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 2 | 32.6 | 27.2 | 35.6 | 33.2 | 31.7 | 27.2 | 26.9 | 29.0 | 30.5
    | 28.3 |'
- en: '| QA-LoRA | Avg. |  |  |  |  | 39.3 |  |  |  |  | 37.6 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 平均 |  |  |  |  | 39.3 |  |  |  |  | 37.6 |'
- en: '| \hdashline[0.8pt/1pt] LLM-QFA | 4 | 50.3 | 37.4 | 49.8 | 46.8 | 45.2 | 48.4
    | 35.6 | 48.1 | 46.9 | 44.0 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] LLM-QFA | 4 | 50.3 | 37.4 | 49.8 | 46.8 | 45.2 | 48.4
    | 35.6 | 48.1 | 46.9 | 44.0 |'
- en: '| LLM-QFA | 3 | 42.3 | 34.4 | 48.1 | 42.9 | 41.2 | 41.4 | 33.3 | 46.2 | 41.2
    | 39.8 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 3 | 42.3 | 34.4 | 48.1 | 42.9 | 41.2 | 41.4 | 33.3 | 46.2 | 41.2
    | 39.8 |'
- en: '| LLM-QFA | 2 | 33.7 | 28.7 | 36.3 | 32.9 | 32.5 | 28.8 | 28.2 | 32.5 | 30.5
    | 29.8 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 2 | 33.7 | 28.7 | 36.3 | 32.9 | 32.5 | 28.8 | 28.2 | 32.5 | 30.5
    | 29.8 |'
- en: '| LLM-QFA | Avg. |  |  |  |  | 39.6 |  |  |  |  | 37.9 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 平均 |  |  |  |  | 39.6 |  |  |  |  | 37.9 |'
- en: '| LLaMA2-13b | 16 | 56.9 | 42.4 | 61.0 | 55.6 | 52.8 | 62.9 | 44.4 | 63.9 |
    56.7 | 55.7 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13b | 16 | 56.9 | 42.4 | 61.0 | 55.6 | 52.8 | 62.9 | 44.4 | 63.9 |
    56.7 | 55.7 |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 55.3 | 41.6 | 58.1 | 53.3 | 51.1 | 61.3
    | 43.3 | 62.5 | 57.2 | 54.9 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 55.3 | 41.6 | 58.1 | 53.3 | 51.1 | 61.3
    | 43.3 | 62.5 | 57.2 | 54.9 |'
- en: '| GPTQ | 3 | 42.0 | 31.8 | 43.6 | 41.3 | 39.0 | 41.4 | 36.5 | 46.7 | 43.7 |
    41.5 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 3 | 42.0 | 31.8 | 43.6 | 41.3 | 39.0 | 41.4 | 36.5 | 46.7 | 43.7 |
    41.5 |'
- en: '| GPTQ | 2 | 25.0 | 22.4 | 22.3 | 24.4 | 23.5 | 23.8 | 23.4 | 22.6 | 24.9 |
    23.7 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 2 | 25.0 | 22.4 | 22.3 | 24.4 | 23.5 | 23.8 | 23.4 | 22.6 | 24.9 |
    23.7 |'
- en: '| GPTQ | Avg. |  |  |  |  | 37.9 |  |  |  |  | 40.0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 平均 |  |  |  |  | 37.9 |  |  |  |  | 40.0 |'
- en: '| \hdashline[0.8pt/1pt] QA-LoRA | 4 | 56.9 | 41.5 | 60.4 | 54.9 | 52.3 | 59.6
    | 42.7 | 62.2 | 57.4 | 54.2 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] QA-LoRA | 4 | 56.9 | 41.5 | 60.4 | 54.9 | 52.3 | 59.6
    | 42.7 | 62.2 | 57.4 | 54.2 |'
- en: '| QA-LoRA | 3 | 54.0 | 40.0 | 57.1 | 52.5 | 49.9 | 56.8 | 41.9 | 59.0 | 53.5
    | 51.7 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 3 | 54.0 | 40.0 | 57.1 | 52.5 | 49.9 | 56.8 | 41.9 | 59.0 | 53.5
    | 51.7 |'
- en: '| QA-LoRA | 2 | 32.6 | 28.9 | 31.4 | 35.3 | 31.8 | 30.3 | 28.2 | 34.4 | 36.5
    | 32.0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 2 | 32.6 | 28.9 | 31.4 | 35.3 | 31.8 | 30.3 | 28.2 | 34.4 | 36.5
    | 32.0 |'
- en: '| QA-LoRA | Avg. |  |  |  |  | 45.3 |  |  |  |  | 45.8 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 平均 |  |  |  |  | 45.3 |  |  |  |  | 45.8 |'
- en: '| \hdashline[0.8pt/1pt] LLM-QFA | 4 | 57.4 | 41.3 | 60.4 | 55.8 | 52.5 | 59.1
    | 42.1 | 61.1 | 56.2 | 53.4 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] LLM-QFA | 4 | 57.4 | 41.3 | 60.4 | 55.8 | 52.5 | 59.1
    | 42.1 | 61.1 | 56.2 | 53.4 |'
- en: '| LLM-QFA | 3 | 56.3 | 40.3 | 58.8 | 54.6 | 51.3 | 56.7 | 40.6 | 59.9 | 54.5
    | 51.8 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 3 | 56.3 | 40.3 | 58.8 | 54.6 | 51.3 | 56.7 | 40.6 | 59.9 | 54.5
    | 51.8 |'
- en: '| LLM-QFA | 2 | 34.5 | 30.3 | 33.0 | 37.3 | 33.5 | 32.2 | 28.5 | 36.0 | 37.2
    | 33.1 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 2 | 34.5 | 30.3 | 33.0 | 37.3 | 33.5 | 32.2 | 28.5 | 36.0 | 37.2
    | 33.1 |'
- en: '| LLM-QFA | Avg. |  |  |  |  | 45.8 |  |  |  |  | 46.1 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 平均 |  |  |  |  | 45.8 |  |  |  |  | 46.1 |'
- en: Comparisons with on MMLU.
  id: totrans-109
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 与MMLU的比较。
- en: 'Figure [4](#S4.F4 "Figure 4 ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM
    for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments") reports the
    comparison between LLM-QFA and Quantization-Aware training methods (QA-LoRA) and
    the Post-Training Quantization method (GPTQ) under (2, 3, 4) bit-widths. LLM-QFA
    demonstrates significantly higher efficiency than QA-LoRA faced with multiple
    deployment scenarios. This advantage stems from the training cost associated with
    LLM-QFA remaining constant, in contrast to the methods that scale linearly with
    the number of deployment scenarios N. Although our approach incurs a modestly
    higher time cost than GPTQ, the substantial performance degradation observed in
    GPTQ is unacceptable. Table [4.2](#S4.SS2 "4.2 Main Results ‣ 4 Experiments ‣
    One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")
    illustrates that, despite delivering only comparable performance under the 4-bit
    constraint, the average metrics of our method across (2, 3, 4) bit constraints
    consistently surpass those of QA-LoRA and GPTQ, without the need for costly repeated
    training.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [4](#S4.F4 "图 4 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 一种量化LLM：一次微调量化LLM以实现高效部署") 报告了LLM-QFA与量化感知训练方法（QA-LoRA）和后训练量化方法（GPTQ）在（2、3、4）位宽下的比较。LLM-QFA
    在面对多个部署场景时表现出显著更高的效率。这一优势源于LLM-QFA的训练成本保持不变，而与部署场景数量N线性相关的方法则会增加。虽然我们的方法在时间成本上略高于GPTQ，但GPTQ的性能显著下降是不可接受的。表
    [4.2](#S4.SS2 "4.2 主要结果 ‣ 4 实验 ‣ 一种量化LLM：一次微调量化LLM以实现高效部署") 显示，尽管在4位约束下仅提供了相当的性能，我们的方法在（2、3、4）位约束下的平均指标始终超越QA-LoRA和GPTQ，而无需昂贵的重复训练。
- en: '![Refer to caption](img/0f85063429d9521f41f1043e5b48d1b8.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0f85063429d9521f41f1043e5b48d1b8.png)'
- en: 'Figure 5: LLM-QFA can deliver multiple optimal subnets under different constraints.
    Left: Comparison of ARC-C dataset; Right: Comparison of the rest of Common Sense
    QA tasks.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：LLM-QFA 在不同约束条件下可以提供多个最优子网。左：ARC-C 数据集的比较；右：其余常识问答任务的比较。
- en: Comparisons on Common Sense QA.
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在常识问答的比较。
- en: 'Table 2: $5$-shot accuracy (%) on the Common Sense QA tasks. Each block is
    based on the same foundation model specified in the first row. We organize all
    results under different quantization bit widths. Mixed precision configurations
    are searched on ARC-C, and the best configurations are tested on the rest of the
    Common Sense QA tasks.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在常识 QA 任务上的 $5$-shot 准确率（%）。每个数据块都基于第一行指定的相同基础模型。我们根据不同的量化位宽组织所有结果。混合精度配置在
    ARC-C 上进行搜索，最佳配置在其余的常识 QA 任务上进行了测试。
- en: '| Method | Bit | Eval | Test |  |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位宽 | 评估 | 测试 |  |'
- en: '| Const. | ARC-C | HellaSwag | PIQA | WinoGrande | ARC-e | BoolQ | OBQA | Avg.
    | Std. (%) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 常量 | ARC-C | HellaSwag | PIQA | WinoGrande | ARC-e | BoolQ | OBQA | 平均 |
    标准差 (%) |'
- en: '| LLaMA2-7B | 16 | 52.0 | 78.2 | 80.1 | 74.1 | 81.1 | 79.3 | 45.2 | 73.0 |
    1.59 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-7B | 16 | 52.0 | 78.2 | 80.1 | 74.1 | 81.1 | 79.3 | 45.2 | 73.0 |
    1.59 |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 50.8 | 77.0 | 79.5 | 73.8 | 80.2 | 74.1
    | 43.4 | 71.3 | 1.61 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 50.8 | 77.0 | 79.5 | 73.8 | 80.2 | 74.1
    | 43.4 | 71.3 | 1.61 |'
- en: '| QA-LoRA | 4 | 55.5 | 79.0 | 80.0 | 73.3 | 79.6 | 75.9 | 46.4 | 72.4 | 1.40
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 4 | 55.5 | 79.0 | 80.0 | 73.3 | 79.6 | 75.9 | 46.4 | 72.4 | 1.40
    |'
- en: '| LLM-QFA | 4 | 53.8 | 76.8 | 79.3 | 73.5 | 78.1 | 77.4 | 49.0 | 72.4 | 1.12
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 4 | 53.8 | 76.8 | 79.3 | 73.5 | 78.1 | 77.4 | 49.0 | 72.4 | 1.12
    |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 3 | 30.1 | 49.9 | 68.3 | 59.3 | 55.5 | 44.3
    | 35.0 | 52.1 | 1.13 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 3 | 30.1 | 49.9 | 68.3 | 59.3 | 55.5 | 44.3
    | 35.0 | 52.1 | 1.13 |'
- en: '| QA-LoRA | 3 | 47.8 | 72.4 | 75.0 | 68.4 | 73.6 | 72.0 | 44.8 | 67.7 | 1.08
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 3 | 47.8 | 72.4 | 75.0 | 68.4 | 73.6 | 72.0 | 44.8 | 67.7 | 1.08
    |'
- en: '| LLM-QFA | 3 | 49.1 | 72.3 | 76.7 | 69.0 | 73.8 | 72.8 | 43.4 | 68.0 | 1.26
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 3 | 49.1 | 72.3 | 76.7 | 69.0 | 73.8 | 72.8 | 43.4 | 68.0 | 1.26
    |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 2 | 25.8 | 26.2 | 51.1 | 50.6 | 26.0 | 41.7
    | 25.0 | 36.8 | 1.31 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 2 | 25.8 | 26.2 | 51.1 | 50.6 | 26.0 | 41.7
    | 25.0 | 36.8 | 1.31 |'
- en: '| QA-LoRA | 2 | 40.4 | 65.6 | 73.6 | 62.0 | 66.0 | 65.9 | 37.2 | 61.7 | 1.32
    |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 2 | 40.4 | 65.6 | 73.6 | 62.0 | 66.0 | 65.9 | 37.2 | 61.7 | 1.32
    |'
- en: '| LLM-QFA | 2 | 43.1 | 64.8 | 73.2 | 62.2 | 67.0 | 64.3 | 38.8 | 61.7 | 1.16
    |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 2 | 43.1 | 64.8 | 73.2 | 62.2 | 67.0 | 64.3 | 38.8 | 61.7 | 1.16
    |'
- en: '| LLaMA2-13B | 16 | 57.5 | 81.7 | 81.7 | 76.0 | 84.4 | 83.2 | 48.2 | 75.9 |
    1.60 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA2-13B | 16 | 57.5 | 81.7 | 81.7 | 76.0 | 84.4 | 83.2 | 48.2 | 75.9 |
    1.60 |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 56.5 | 81.1 | 80.9 | 75.6 | 83.3 | 81.7
    | 47.4 | 75.0 | 1.58 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 4 | 56.5 | 81.1 | 80.9 | 75.6 | 83.3 | 81.7
    | 47.4 | 75.0 | 1.58 |'
- en: '| QA-LoRA | 4 | 58.0 | 79.2 | 81.3 | 74.0 | 83.3 | 83.8 | 49.4 | 75.2 | 1.43
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 4 | 58.0 | 79.2 | 81.3 | 74.0 | 83.3 | 83.8 | 49.4 | 75.2 | 1.43
    |'
- en: '| LLM-QFA | 4 | 56.0 | 79.6 | 82.0 | 73.2 | 83.5 | 83.2 | 51.0 | 75.4 | 1.31
    |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 4 | 56.0 | 79.6 | 82.0 | 73.2 | 83.5 | 83.2 | 51.0 | 75.4 | 1.31
    |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 3 | 47.8 | 68.6 | 77.7 | 67.9 | 77.1 | 71.9
    | 42.8 | 67.7 | 1.38 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 3 | 47.8 | 68.6 | 77.7 | 67.9 | 77.1 | 71.9
    | 42.8 | 67.7 | 1.38 |'
- en: '| QA-LoRA | 3 | 53.5 | 67.0 | 79.4 | 66.7 | 80.1 | 76.3 | 41.8 | 68.5 | 1.72
    |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 3 | 53.5 | 67.0 | 79.4 | 66.7 | 80.1 | 76.3 | 41.8 | 68.5 | 1.72
    |'
- en: '| LLM-QFA | 3 | 53.7 | 75.1 | 79.7 | 70.3 | 80.5 | 78.4 | 48.0 | 72.0 | 1.27
    |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 3 | 53.7 | 75.1 | 79.7 | 70.3 | 80.5 | 78.4 | 48.0 | 72.0 | 1.27
    |'
- en: '| \hdashline[0.8pt/1pt] GPTQ | 2 | 27.8 | 25.8 | 50.2 | 50.2 | 26.6 | 37.8
    | 23.4 | 35.7 | 1.26 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| \hdashline[0.8pt/1pt] GPTQ | 2 | 27.8 | 25.8 | 50.2 | 50.2 | 26.6 | 37.8
    | 23.4 | 35.7 | 1.26 |'
- en: '| QA-LoRA | 2 | 49.1 | 70.8 | 76.6 | 66.4 | 76.1 | 74.1 | 44.8 | 68.1 | 1.21
    |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| QA-LoRA | 2 | 49.1 | 70.8 | 76.6 | 66.4 | 76.1 | 74.1 | 44.8 | 68.1 | 1.21
    |'
- en: '| LLM-QFA | 2 | 49.2 | 70.9 | 77.0 | 67.2 | 76.3 | 74.3 | 44.6 | 68.4 | 1.24
    |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| LLM-QFA | 2 | 49.2 | 70.9 | 77.0 | 67.2 | 76.3 | 74.3 | 44.6 | 68.4 | 1.24
    |'
- en: 'Table [4.2](#S4.SS2.SSS0.Px2 "Comparisons on Common Sense QA. ‣ Comparisons
    with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments") reports the result of Common Sense
    QA. Consistent with the findings from the MMLU benchmark, LLM-QFA demonstrates
    comparable performance with baselines at extreme bit-width (2, 4) and outperforms
    at median bit-width (3). The advantage is significant with LLaMA2-13B under 3-bit
    constraints, where LLM-QFA gains 3.5% accuracy improvement over QA-LoRA.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 表 [4.2](#S4.SS2.SSS0.Px2 "关于常识 QA 的比较。 ‣ 与 MMLU 的比较。 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 一次量化
    LLM 适用于所有：对量化 LLM 的一次微调以实现高效部署") 报告了常识 QA 的结果。与 MMLU 基准的发现一致，LLM-QFA 在极端位宽（2,
    4）下表现与基线相当，在中位宽（3）下表现更好。尤其是在 3-bit 约束下，LLM-QFA 比 QA-LoRA 提高了 3.5% 的准确率。
- en: LLM-QFA under Different Resource Constraints.
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM-QFA 在不同资源约束下的表现。
- en: 'Figure [5](#S4.F5 "Figure 5 ‣ Comparisons with on MMLU. ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient
    Deployments") summarizes the results of LLM-QFA under different bit-width constraints.
    LLM-QFA achieves 45.0% ARC-C accuracy with 2.1 average bit-width, being 5% more
    accurate than QA-LoRA with similar resource demands. Compared with QA-LoRA at
    3-bit, our approach can achieve the same level of performance with fewer resources,
    a 1.2x reduction on ARC-C, and a 1.1x reduction on the rest of Common Sense QA.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图[5](#S4.F5 "图5 ‣ MMLU的比较。 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 一个QuantLLM：为高效部署一次微调量化LLM")总结了不同位宽限制下LLM-QFA的结果。LLM-QFA在2.1的平均位宽下实现了45.0%的ARC-C准确率，比QA-LoRA在类似资源需求下高出5%。与3位的QA-LoRA相比，我们的方法可以在较少的资源下实现相同水平的性能，ARC-C减少了1.2倍，其它常识QA减少了1.1倍。
- en: Impact of Mixed Precision and Quality of Optimization.
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 混合精度的影响与优化质量。
- en: '![Refer to caption](img/e897e03c1fa053e36aed1e4c54bd4fcf.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e897e03c1fa053e36aed1e4c54bd4fcf.png)'
- en: 'Figure 6: Visualizing the degree of optimization by LLM-QFA. Subnets sampled
    from LLM-QFA show significant robustness over baselines with simple mixed-precision.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：通过LLM-QFA可视化优化程度。LLM-QFA采样的子网在简单混合精度下显示出显著的鲁棒性。
- en: 'Previous results have significant performance improvement under the median
    resource constraints. To verify that the improvement does not only benefit from
    mixed precision, we separately sample 100 mixed-precision configurations for both
    GPTQ and QA-LoRA and evaluate them on the ARC-C dataset. To be noticed, we evaluate
    mixed-precision QA-LoRA based on the fine-tuned QA-LoRA weight at (2, 3, 4) bit.
    Figure [6](#S4.F6 "Figure 6 ‣ Impact of Mixed Precision and Quality of Optimization.
    ‣ LLM-QFA under Different Resource Constraints. ‣ Comparisons on Common Sense
    QA. ‣ Comparisons with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM
    for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments") demonstrates
    that our approach has a more robust performance across the dimension of resource
    demands, further validating that our method can help optimize all the subnets
    instead of only benefiting from the mixed-precision setting. Although the mixed-precision
    version of QA-LoRA exhibits a modest improvement in performance at higher bit-widths,
    it incurs a threefold increase in training time to achieve these results. Moreover,
    the observed performance instability suggests a potential loss of optimal subnet
    configurations under certain constraints.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在中位资源限制下，之前的结果显示了显著的性能提升。为了验证这种提升不仅仅得益于混合精度，我们分别对GPTQ和QA-LoRA进行了100个混合精度配置的采样，并在ARC-C数据集上进行了评估。需要注意的是，我们基于(2,
    3, 4)位的微调QA-LoRA权重评估了混合精度QA-LoRA。图[6](#S4.F6 "图6 ‣ 混合精度的影响与优化质量。 ‣ 不同资源限制下的LLM-QFA。
    ‣ 常识QA的比较。 ‣ MMLU的比较。 ‣ 4.2 主要结果 ‣ 4 实验 ‣ 一个QuantLLM：为高效部署一次微调量化LLM")展示了我们的方法在资源需求维度上具有更强的鲁棒性，进一步验证了我们的方法可以帮助优化所有子网，而不仅仅是从混合精度设置中受益。尽管混合精度版本的QA-LoRA在较高位宽下表现出适度的性能提升，但它需要三倍的训练时间才能达到这些结果。此外，观察到的性能不稳定性表明在某些限制条件下可能会丧失最佳的子网配置。
- en: '![Refer to caption](img/11972393e9e86d0ed343427c2e019ad2.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/11972393e9e86d0ed343427c2e019ad2.png)'
- en: 'Figure 7: Verification of the effectiveness of Interference-Less Fine-Tuning
    and Resource-Balance Sampling Strategy.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：干扰无关微调和资源平衡采样策略有效性的验证。
- en: '![Refer to caption](img/2d64a88fe4f7f69bb88819db3ed588a0.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2d64a88fe4f7f69bb88819db3ed588a0.png)'
- en: 'Figure 8: Common Sense QA accuracy (%) of LLM-QFA with different scheduler
    settings.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：不同调度器设置下LLM-QFA的常识QA准确率（%）。
- en: 4.3 Ablation Study
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 消融研究
- en: Ablation on Interference-Less Fine-tuning.
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 干扰无关微调的消融。
- en: 'To ascertain the effectiveness of decoupling shared weight, we introduce a
    variant of our method termed shared-LoRA, wherein distinct quantization settings
    of a pre-trained weight share the same Low-Rank adapter. Figure [7](#S4.F7 "Figure
    7 ‣ Impact of Mixed Precision and Quality of Optimization. ‣ LLM-QFA under Different
    Resource Constraints. ‣ Comparisons on Common Sense QA. ‣ Comparisons with on
    MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning Quantized
    LLMs Once for Efficient Deployments") reports that the shared-LoRA version fails
    the origin version across all resource demands, which validates the interference
    problem in one-shot training for LLMs.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '为了确认解耦共享权重的有效性，我们引入了一种变体方法，称为shared-LoRA，其中不同量化设置的预训练权重共享相同的低秩适配器。图[7](#S4.F7
    "Figure 7 ‣ Impact of Mixed Precision and Quality of Optimization. ‣ LLM-QFA under
    Different Resource Constraints. ‣ Comparisons on Common Sense QA. ‣ Comparisons
    with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments")报告称，shared-LoRA版本在所有资源需求下都未能超越原始版本，这验证了在LLMs的一次性训练中的干扰问题。'
- en: Ablation on Resource-Balance Sampling.
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 资源平衡采样的消融实验。
- en: 'Similarly, we implement a uniform sampling version of our method. Figure [7](#S4.F7
    "Figure 7 ‣ Impact of Mixed Precision and Quality of Optimization. ‣ LLM-QFA under
    Different Resource Constraints. ‣ Comparisons on Common Sense QA. ‣ Comparisons
    with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments") also shows a consistently under-performing
    uniform sampling strategy; even the resource-concentrated area (3 bit) falls short
    in the comparison. This has motivated the development of a resource-balanced sampling
    strategy for training, which is designed to counteract the challenges of under-fitting
    and over-fitting encountered in one-shot training.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '同样，我们实现了方法的统一采样版本。图[7](#S4.F7 "Figure 7 ‣ Impact of Mixed Precision and Quality
    of Optimization. ‣ LLM-QFA under Different Resource Constraints. ‣ Comparisons
    on Common Sense QA. ‣ Comparisons with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments
    ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments")还显示了一致表现不佳的统一采样策略；即使是资源密集区域（3位）也在比较中落后。这促使了资源平衡采样策略的开发，该策略旨在应对一次性训练中遇到的欠拟合和过拟合挑战。'
- en: Ablation for Scheduler.
  id: totrans-153
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 调度器的消融实验。
- en: 'Lastly, we investigate two aspects of configuration for the scheduler, which
    are the length of epochs (SL) and schedule orders. In our main experiments, the
    epoch length is set to 8k training steps. For the short-term schedule, it is reduced
    to 1k steps, while for the long-term schedule, it is extended to 16k steps. Figure
    [8](#S4.F8 "Figure 8 ‣ Impact of Mixed Precision and Quality of Optimization.
    ‣ LLM-QFA under Different Resource Constraints. ‣ Comparisons on Common Sense
    QA. ‣ Comparisons with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM
    for ALL: Fine-tuning Quantized LLMs Once for Efficient Deployments") demonstrates
    that the short-term diminishes robustness and hinders convergence, particularly
    at lower bit configurations. Regarding the schedule orders, we initiate our training
    with 4-bit configurations, employing an easy-to-hard strategy. In this part, we
    assess the hard-to-easy setting. Figure [8](#S4.F8 "Figure 8 ‣ Impact of Mixed
    Precision and Quality of Optimization. ‣ LLM-QFA under Different Resource Constraints.
    ‣ Comparisons on Common Sense QA. ‣ Comparisons with on MMLU. ‣ 4.2 Main Results
    ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning Quantized LLMs Once for Efficient
    Deployments") demonstrates that the order has negligible impact.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，我们研究了调度器配置的两个方面，即纪元长度（SL）和调度顺序。在我们的主要实验中，纪元长度设置为8k训练步骤。对于短期调度，纪元长度缩短为1k步骤，而对于长期调度，纪元长度延长至16k步骤。图[8](#S4.F8
    "Figure 8 ‣ Impact of Mixed Precision and Quality of Optimization. ‣ LLM-QFA under
    Different Resource Constraints. ‣ Comparisons on Common Sense QA. ‣ Comparisons
    with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments")展示了短期调度减少了鲁棒性并阻碍了收敛，特别是在较低位配置下。关于调度顺序，我们在4位配置下开始训练，采用由易到难的策略。在这一部分，我们评估了由难到易的设置。图[8](#S4.F8
    "Figure 8 ‣ Impact of Mixed Precision and Quality of Optimization. ‣ LLM-QFA under
    Different Resource Constraints. ‣ Comparisons on Common Sense QA. ‣ Comparisons
    with on MMLU. ‣ 4.2 Main Results ‣ 4 Experiments ‣ One QuantLLM for ALL: Fine-tuning
    Quantized LLMs Once for Efficient Deployments")展示了顺序对结果的影响微乎其微。'
- en: 5 Conclusion
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: This work introduces the LLM-QFA framework, a once-for-all Quantization-Aware
    training approach to reduce the training cost of deploying large language models
    (LLMs) across diverse scenarios. By decoupling the weights of different configurations
    and incorporating Low-Rank adapters, we enhance training efficiency and mitigate
    interference issues. A resource-balanced sampling strategy ensures fair training
    across subnets with various resource demands. Our experiments on LLaMA2 models
    show that LLM-QFA deliver optimal quantized models, demonstrating its effectiveness
    in reducing computational and storage costs while maintaining performance. Our
    framework can be easily scaled up to even larger models since the training time
    per step is the same as with previous LoRA tuning.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了 LLM-QFA 框架，这是一种一劳永逸的量化感知训练方法，用于减少在各种场景中部署大型语言模型 (LLMs) 的训练成本。通过解耦不同配置的权重并结合低秩适配器，我们提高了训练效率并减轻了干扰问题。资源平衡的采样策略确保了在具有不同资源需求的子网络上公平训练。我们在
    LLaMA2 模型上的实验表明，LLM-QFA 提供了最佳的量化模型，展示了其在降低计算和存储成本同时保持性能的有效性。我们的框架可以轻松扩展到更大的模型，因为每步的训练时间与之前的
    LoRA 调优相同。
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Bisk et al., (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. (2020).
    Piqa: Reasoning about physical commonsense in natural language. In Proceedings
    of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等 (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., 等 (2020). Piqa: 在自然语言中推理物理常识。见于《AAAI
    人工智能会议论文集》，第 34 卷，第 7432–7439 页。'
- en: 'Cai et al., (2019) Cai, H., Gan, C., Wang, T., Zhang, Z., and Han, S. (2019).
    Once-for-all: Train one network and specialize it for efficient deployment. arXiv
    preprint arXiv:1908.09791.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cai 等 (2019) Cai, H., Gan, C., Wang, T., Zhang, Z., 和 Han, S. (2019). 一劳永逸:
    训练一个网络并专门化以实现高效部署。arXiv 预印本 arXiv:1908.09791。'
- en: 'Chen et al., (2021) Chen, M., Peng, H., Fu, J., and Ling, H. (2021). Autoformer:
    Searching transformers for visual recognition. In Proceedings of the IEEE/CVF
    international conference on computer vision, pages 12270–12280.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chen 等 (2021) Chen, M., Peng, H., Fu, J., 和 Ling, H. (2021). Autoformer: 为视觉识别搜索变换器。见于《IEEE/CVF
    计算机视觉国际会议论文集》，第 12270–12280 页。'
- en: 'Clark et al., (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. (2019). BoolQ: Exploring the surprising difficulty of natural
    yes/no questions. arXiv preprint arXiv:1905.10044.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等 (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., 和 Toutanova, K. (2019). BoolQ: 探索自然是/否问题的惊人难度。arXiv 预印本 arXiv:1905.10044。'
- en: Clark et al., (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. (2018). Think you have solved question answering?
    try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
    Schoenick, C., 和 Tafjord, O. (2018). 认为你已经解决了问答问题？尝试 arc，AI2 推理挑战。arXiv 预印本 arXiv:1803.05457。
- en: 'Dettmers et al., (2024) Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer,
    L. (2024). Qlora: Efficient finetuning of quantized llms. Advances in Neural Information
    Processing Systems, 36.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 (2024) Dettmers, T., Pagnoni, A., Holtzman, A., 和 Zettlemoyer, L.
    (2024). Qlora: 高效的量化 LLM 微调。神经信息处理系统进展，第 36 卷。'
- en: 'Frantar et al., (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. (2022). Gptq: Accurate post-training quantization for generative pre-trained
    transformers. arXiv preprint arXiv:2210.17323.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等 (2022) Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D. (2022).
    Gptq: 用于生成预训练变换器的准确后训练量化。arXiv 预印本 arXiv:2210.17323。'
- en: 'Guo et al., (2023) Guo, H., Greengard, P., Xing, E. P., and Kim, Y. (2023).
    Lq-lora: Low-rank plus quantized matrix decomposition for efficient language model
    finetuning. arXiv preprint arXiv:2311.12023.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Guo 等 (2023) Guo, H., Greengard, P., Xing, E. P., 和 Kim, Y. (2023). Lq-lora:
    低秩加量化矩阵分解用于高效语言模型微调。arXiv 预印本 arXiv:2311.12023。'
- en: Hendrycks et al., (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D., and Steinhardt, J. (2021). Measuring massive multitask language
    understanding. In International Conference on Learning Representations.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,
    Song, D., 和 Steinhardt, J. (2021). 测量大规模多任务语言理解。在国际学习表示会议上。
- en: 'Kim et al., (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen,
    S., Mahoney, M. W., and Keutzer, K. (2023). Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等 (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S.,
    Mahoney, M. W., 和 Keutzer, K. (2023). Squeezellm: 稠密和稀疏量化。arXiv 预印本 arXiv:2306.07629。'
- en: '(11) Li, S., Ning, X., Hong, K., Liu, T., Wang, L., Li, X., Zhong, K., Dai,
    G., Yang, H., and Wang, Y. (2023a). Llm-mq: Mixed-precision quantization for efficient
    llm deployment. Efficient Natural Language and Speech Processing Workshop at Advances
    in Neural Information Processing Systems.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （11）李，S.，宁，X.，洪，K.，刘，T.，王，L.，李，X.，钟，K.，戴，G.，杨，H.，和王，Y.（2023a）。Llm-mq：高效llm部署的混合精度量化。神经信息处理系统进展中的高效自然语言和语音处理研讨会。
- en: '(12) Li, Y., Yu, Y., Liang, C., He, P., Karampatziakis, N., Chen, W., and Zhao,
    T. (2023b). Loftq: Lora-fine-tuning-aware quantization for large language models.
    arXiv preprint arXiv:2310.08659.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （12）李，Y.，余，Y.，梁，C.，贺，P.，卡拉帕齐亚基斯，N.，陈，W.，和赵，T.（2023b）。Loftq：针对大型语言模型的Lora微调感知量化。arXiv预印本
    arXiv:2310.08659。
- en: 'Lin et al., (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. (2023). Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 林等人，（2023）林，J.，唐，J.，唐，H.，杨，S.，邓，X.，和韩，S.（2023）。Awq：用于llm压缩和加速的激活感知权重量化。arXiv预印本
    arXiv:2306.00978。
- en: 'Liu et al., (2023) Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad,
    Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. (2023). Llm-qat: Data-free quantization
    aware training for large language models. arXiv preprint arXiv:2305.17888.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等人，（2023）刘，Z.，奥古兹，B.，赵，C.，常，E.，斯托克，P.，梅赫达德，Y.，石，Y.，克里希纳穆尔提，R.，和钱德拉，V.（2023）。Llm-qat：用于大型语言模型的无数据量化感知训练。arXiv预印本
    arXiv:2305.17888。
- en: Mihaylov et al., (2018) Mihaylov, T., Clark, P., Khot, T., and Sabharwal, A.
    (2018). Can a suit of armor conduct electricity? a new dataset for open book question
    answering. In EMNLP.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 米哈伊洛夫等人，（2018）米哈伊洛夫，T.，克拉克，P.，霍特，T.，和萨巴尔瓦尔，A.（2018）。盔甲能导电吗？一个新的开放书籍问答数据集。在EMNLP。
- en: 'Sakaguchi et al., (2021) Sakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi,
    Y. (2021). Winogrande: An adversarial winograd schema challenge at scale. Communications
    of the ACM, 64(9):99–106.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坂口等人，（2021）坂口，K.，布拉斯，R. L.，巴格瓦图拉，C.，和崔，Y.（2021）。Winogrande：大规模的对抗性Winograd模式挑战。《ACM通讯》，64(9)：99–106。
- en: Tang et al., (2024) Tang, C., Meng, Y., Jiang, J., Xie, S., Lu, R., Ma, X.,
    Wang, Z., and Zhu, W. (2024). Retraining-free model quantization via one-shot
    weight-coupling learning. arXiv preprint arXiv:2401.01543.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唐等人，（2024）唐，C.，孟，Y.，姜，J.，谢，S.，陆，R.，马，X.，王，Z.，和朱，W.（2024）。无需重新训练的模型量化通过一次性权重耦合学习。arXiv预印本
    arXiv:2401.01543。
- en: 'Tang et al., (2022) Tang, C., Zhai, H., Ouyang, K., Wang, Z., Zhu, Y., and
    Zhu, W. (2022). Arbitrary bit-width network: A joint layer-wise quantization and
    adaptive inference approach. In Proceedings of the 30th ACM International Conference
    on Multimedia, pages 2899–2908.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唐等人，（2022）唐，C.，翟，H.，欧阳，K.，王，Z.，朱，Y.，和朱，W.（2022）。任意位宽网络：一种联合层级量化和自适应推理方法。在第30届ACM国际多媒体会议论文集中，第2899–2908页。
- en: 'Tang et al., (2023) Tang, C., Zhang, L. L., Jiang, H., Xu, J., Cao, T., Zhang,
    Q., Yang, Y., Wang, Z., and Yang, M. (2023). Elasticvit: Conflict-aware supernet
    training for deploying fast vision transformer on diverse mobile devices. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 5829–5840.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唐等人，（2023）唐，C.，张，L. L.，姜，H.，徐，J.，曹，T.，张，Q.，杨，Y.，王，Z.，和杨，M.（2023）。Elasticvit：面向多样化移动设备的快速视觉变换器的冲突感知超网络训练。在IEEE/CVF国际计算机视觉会议论文集中，第5829–5840页。
- en: 'Taori et al., (2023) Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,
    Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Stanford alpaca: An instruction-following
    llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 田等人，（2023）田，R.，古尔拉贾尼，I.，张，T.，杜布瓦，Y.，李，X.，格斯特林，C.，梁，P.，和哈希莫托，T. B.（2023）。斯坦福Alpaca：一个指令跟随的Llama模型。
    [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)。
- en: 'Touvron et al., (2023) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    (2023). Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图弗龙等人，（2023）图弗龙，H.，拉夫里尔，T.，伊扎卡德，G.，马丁内特，X.，拉肖，M.-A.，拉克鲁瓦，T.，罗齐埃，B.，戈亚尔，N.，汉布罗，E.，阿扎尔，F.，等（2023）。Llama：开放和高效的基础语言模型。arXiv预印本
    arXiv:2302.13971。
- en: 'Wang et al., (2020) Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., and
    Han, S. (2020). Hat: Hardware-aware transformers for efficient natural language
    processing. arXiv preprint arXiv:2005.14187.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 王等人，（2020）王，H.，吴，Z.，刘，Z.，蔡，H.，朱，L.，甘，C.，和韩，S.（2020）。Hat：硬件感知变压器以实现高效自然语言处理。arXiv预印本
    arXiv:2005.14187。
- en: 'Wang et al., (2022) Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A.,
    Khashabi, D., and Hajishirzi, H. (2022). Self-instruct: Aligning language model
    with self generated instructions. arXiv preprint arXiv:2212.10560.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wang 等人，（2022）Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi,
    D., 和 Hajishirzi, H.（2022）。Self-instruct: 将语言模型与自生成指令对齐。arXiv 预印本 arXiv:2212.10560。'
- en: 'Xiao et al., (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and
    Han, S. (2023). Smoothquant: Accurate and efficient post-training quantization
    for large language models. In International Conference on Machine Learning, pages
    38087–38099\. PMLR.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等人，（2023）Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., 和 Han, S.（2023）。Smoothquant:
    大型语言模型的准确高效后训练量化。在国际机器学习会议，页面38087–38099。PMLR。'
- en: 'Xu et al., (2023) Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H.,
    Chen, Z., Zhang, X., and Tian, Q. (2023). Qa-lora: Quantization-aware low-rank
    adaptation of large language models. arXiv preprint arXiv:2309.14717.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu 等人，（2023）Xu, Y., Xie, L., Gu, X., Chen, X., Chang, H., Zhang, H., Chen,
    Z., Zhang, X., 和 Tian, Q.（2023）。Qa-lora: 大型语言模型的量化感知低秩适配。arXiv 预印本 arXiv:2309.14717。'
- en: 'Yu et al., (2020) Yu, J., Jin, P., Liu, H., Bender, G., Kindermans, P.-J.,
    Tan, M., Huang, T., Song, X., Pang, R., and Le, Q. (2020). Bignas: Scaling up
    neural architecture search with big single-stage models. In Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part VII 16, pages 702–717\. Springer.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等人，（2020）Yu, J., Jin, P., Liu, H., Bender, G., Kindermans, P.-J., Tan, M.,
    Huang, T., Song, X., Pang, R., 和 Le, Q.（2020）。Bignas: 使用大型单阶段模型扩展神经架构搜索。在计算机视觉–ECCV
    2020: 第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，会议录，第VII部分，第16卷，页面702–717。Springer。'
- en: 'Zellers et al., (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. (2019). Hellaswag: Can a machine really finish your sentence? arXiv preprint
    arXiv:1905.07830.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers 等人，（2019）Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., 和 Choi,
    Y.（2019）。Hellaswag: 机器真的能完成你的句子吗？arXiv 预印本 arXiv:1905.07830。'
