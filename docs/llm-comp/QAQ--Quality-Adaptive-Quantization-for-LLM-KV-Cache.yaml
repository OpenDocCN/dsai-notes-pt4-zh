- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:49:51'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:49:51
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'QAQ: Quality Adaptive Quantization for LLM KV Cache'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'QAQ: 用于LLM KV缓存的质量自适应量化'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.04643](https://ar5iv.labs.arxiv.org/html/2403.04643)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.04643](https://ar5iv.labs.arxiv.org/html/2403.04643)
- en: Shichen Dong^(∗1), Wen Cheng^(∗1), Jiayu Qin¹, Wei Wang¹ ¹Nanjing University
    {scdong, wcheng, jiayuqin}@smail.nju.edu.cn, ww@nju.edu.cn
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Shichen Dong^(∗1)、Wen Cheng^(∗1)、Jiayu Qin¹、Wei Wang¹ ¹南京大学 {scdong, wcheng,
    jiayuqin}@smail.nju.edu.cn, ww@nju.edu.cn
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The emergence of LLMs has ignited a fresh surge of breakthroughs in NLP applications,
    particularly in domains such as question-answering systems and text generation.
    As the need for longer context grows, a significant bottleneck in model deployment
    emerges due to the linear expansion of the Key-Value (KV) cache with the context
    length. Existing methods primarily rely on various hypotheses, such as sorting
    the KV cache based on attention scores for replacement or eviction, to compress
    the KV cache and improve model throughput. However, heuristics used by these strategies
    may wrongly evict essential KV cache, which can significantly degrade model performance.
    In this paper, we propose QAQ, a Quality Adaptive Quantization scheme for the
    KV cache. We theoretically demonstrate that key cache and value cache exhibit
    distinct sensitivities to quantization, leading to the formulation of separate
    quantization strategies for their non-uniform quantization. Through the integration
    of dedicated outlier handling, as well as an improved attention-aware approach,
    QAQ achieves up to $10\times$ the compression ratio of the KV cache size with
    a neglectable impact on model performance. QAQ significantly reduces the practical
    hurdles of deploying LLMs, opening up new possibilities for longer-context applications.
    The code is available at github.com/ClubieDong/KVCacheQuantization.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的出现引发了自然语言处理（NLP）应用的新一轮突破，特别是在问答系统和文本生成等领域。随着对更长上下文的需求增加，由于上下文长度导致的键值（KV）缓存的线性扩展，模型部署中出现了一个显著的瓶颈。现有的方法主要依赖于各种假设，例如基于注意力分数对KV缓存进行排序以进行替换或驱逐，以压缩KV缓存并提高模型吞吐量。然而，这些策略使用的启发式方法可能会错误地驱逐重要的KV缓存，从而显著降低模型性能。在本文中，我们提出了QAQ，一种用于KV缓存的质量自适应量化方案。我们从理论上证明了键缓存和值缓存对量化的敏感性不同，从而形成了针对其非均匀量化的单独量化策略。通过集成专门的异常值处理以及改进的注意力感知方法，QAQ实现了高达$10\times$的KV缓存大小压缩比，并对模型性能的影响可以忽略不计。QAQ显著减少了部署LLMs的实际障碍，为长上下文应用开辟了新的可能性。代码可在
    github.com/ClubieDong/KVCacheQuantization 上获取。
- en: '^†^†footnotetext: $*$ Co-first author, contributed equally.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ^†^†脚注：$*$ 共同第一作者，贡献相同。
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large Language Models (LLMs) demonstrate state-of-the-art performance on various
    Natural Language Processing (NLP) benchmarks Beeching et al. ([2023](#bib.bib2)).
    These LLMs showcased exceptional potential across a multitude of practical applications,
    including but not limited to text generation, conversation processing, and knowledge
    question answering Chang et al. ([2023](#bib.bib5)). However, deploying these
    models efficiently poses a challenge due to the sequential nature of the generative
    inference process. That is, sequentially processing one token at a time requires
    accessing all previously generated tokens for computation. In practical computations,
    such as GPT series Brown et al. ([2020](#bib.bib4)), LLaMA series Touvron et al.
    ([2023](#bib.bib17)), and OPT series Zhang et al. ([2022](#bib.bib20)), the generative
    inference of these LLMs typically incorporates the KV cache mechanism to improve
    the efficiency of computation resource utilization. KV cache stores the computed
    values of the Key-Value vector from previous attention calculations and reuses
    them when computing the current token to save extra costs associated with redundant
    calculations. While being a widely used optimization method, as the model size
    and context length continue to increase, the storage overhead of the KV cache
    itself also grows dramatically, imposing significant pressure on the on-device,
    especially the high-cost GPU memory. Reducing the memory footprint of the KV cache
    has become a highly active research topic Zhu et al. ([2023](#bib.bib22)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在各种自然语言处理（NLP）基准测试中展示了**最先进**的性能 Beeching et al. ([2023](#bib.bib2))。这些LLMs在包括但不限于文本生成、对话处理和知识问答等众多实际应用中展现了**卓越的潜力**
    Chang et al. ([2023](#bib.bib5))。然而，由于生成推理过程的顺序特性，高效部署这些模型面临挑战。即，一次处理一个token需要访问所有之前生成的tokens进行计算。在实际计算中，如GPT系列
    Brown et al. ([2020](#bib.bib4))、LLaMA系列 Touvron et al. ([2023](#bib.bib17)) 和
    OPT系列 Zhang et al. ([2022](#bib.bib20))，这些LLMs的生成推理通常包括KV缓存机制，以提高计算资源的使用效率。KV缓存存储之前注意力计算中Key-Value向量的计算值，并在计算当前token时重用这些值，从而节省与冗余计算相关的额外成本。尽管KV缓存是一种**广泛使用的优化方法**，但随着模型大小和上下文长度的持续增加，KV缓存本身的存储开销也急剧增长，对设备，特别是高成本的GPU内存施加了**巨大压力**。减少KV缓存的内存占用已经成为一个**高度活跃的研究课题**
    Zhu et al. ([2023](#bib.bib22))。
- en: 'Currently, there is a substantial body of research addressing the efficient
    utilization of GPU memory in memory-constrained scenarios. Offloading is an intuitive
    solution for handling insufficient GPU memory during model inference. Although
    offloading can alleviate the pressure on GPU memory, implementing offloading specifically
    for the KV cache is a non-trivial problem, as it is constrained by various factors
    such as data transmission bandwidth limitations. Additionally, approaches like
    sparse transformers Child et al. ([2019](#bib.bib6)) and multi-query attention
    Pope et al. ([2023](#bib.bib15)) are designed to reduce cache sizes directly;
    however, applying them directly to optimize the KV cache may result in significant
    performance degradation Ge et al. ([2023](#bib.bib11)). Recently, pioneering studies
    have emerged, focusing on the direct optimization of the KV cache to minimize
    its footprint in GPU memory. However, these methods often rely on attention values
    to eliminate redundant portions from the KV cache, retaining what is considered
    essential. Nevertheless, these approaches may lead to erroneously removing crucial
    KV cache and significantly degrading the performance of the model Zhang et al.
    ([2023](#bib.bib21)). Naturally, it leads to the question: is there a direct quantization
    method that can avoid the drawbacks mentioned above, while achieving leading performance?'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，有大量研究针对内存受限场景下GPU内存的高效利用。卸载是处理模型推理过程中GPU内存不足的直观解决方案。虽然卸载可以缓解GPU内存的压力，但专门针对KV缓存的卸载实施是一个**非平凡的问题**，因为它受限于数据传输带宽等各种因素。此外，像稀疏变换器
    Child et al. ([2019](#bib.bib6)) 和多查询注意力 Pope et al. ([2023](#bib.bib15)) 的方法旨在直接减少缓存大小；然而，直接应用这些方法来优化KV缓存可能会导致显著的性能下降
    Ge et al. ([2023](#bib.bib11))。最近，出现了**开创性的研究**，专注于直接优化KV缓存以最小化其在GPU内存中的占用。然而，这些方法通常依赖注意力值来消除KV缓存中的冗余部分，保留被认为**重要的**内容。然而，这些方法可能导致错误地移除关键的KV缓存，从而显著降低模型的性能
    Zhang et al. ([2023](#bib.bib21))。自然地，这引出了一个问题：是否存在一种**直接量化方法**，可以避免上述缺点，同时实现**领先的性能**？
- en: In this paper, we propose QAQ, a quality adaptive quantization scheme for KV
    cache in LLMs. Quantization is a commonly employed method for compressing model
    sizes and has been widely utilized for the compression of weights and activations
    Zhu et al. ([2023](#bib.bib22)). However, compressing the KV cache remains a challenging
    task. There are three key insights that inspire QAQ.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们提出了QAQ，一种用于LLMs中的KV缓存的质量自适应量化方案。量化是一种常用的模型压缩方法，已广泛用于权重和激活的压缩（Zhu et al.
    ([2023](#bib.bib22))）。然而，压缩KV缓存仍然是一个具有挑战性的任务。有三个关键见解启发了QAQ。
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: First, key cache and value cache exhibit distinct sensitivities to quantization,
    a proposition validated through theoretical analyses and empirical experiments.
    This necessitates the development of separate quantization strategies for key
    cache and value cache.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，关键缓存和数值缓存对量化表现出不同的敏感性，这一命题通过理论分析和实证实验得到了验证。这需要为关键缓存和数值缓存制定独立的量化策略。
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Second, the hypothesis of persistence of importance has exceptions. Previous
    works proposed the hypothesis of the persistence of importance, advocating compression
    based on importance (attention-aware). We discovered that, despite its validity
    in the majority of cases, there exist a number of exceptions. This implies the
    need for careful handling of exceptions when quantizing based on attention.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其次，重要性持久性的假设存在例外。之前的工作提出了重要性持久性的假设，提倡基于重要性（注意力感知）的压缩。我们发现，尽管在大多数情况下有效，但也存在一些例外。这意味着在基于注意力进行量化时需要仔细处理例外情况。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Third, outliers play a crucial role. While this has been acknowledged in weight
    and activation quantization, we verified that outliers also exert a significant
    impact on the KV cache quantization. Consequently, a dedicated treatment for quantizing
    outliers is required.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三，离群值起着至关重要的作用。虽然在权重和激活量化中已经认识到这一点，但我们验证了离群值对KV缓存量化也有显著影响。因此，需要专门处理离群值的量化。
- en: With these considerations, QAQ achieves nearly a $10\times$ compression. We
    make our code publicly available for replication.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些考虑下，QAQ实现了接近$10\times$的压缩。我们将我们的代码公开以供复制。
- en: '![Refer to caption](img/86ba0047b149852bf14d1228f1c56b70.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86ba0047b149852bf14d1228f1c56b70.png)'
- en: 'Figure 1: Calculation process of quantized key and value vectors, $\frac{1}{\sqrt{D}}$
    is neglected.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：量化关键和数值向量的计算过程，$\frac{1}{\sqrt{D}}$ 被忽略。
- en: 2 Problem Description and Related Work
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 问题描述和相关工作
- en: In this section, we first introduce problem profiling, followed by related work.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍问题概述，然后是相关工作。
- en: 2.1 Problem Description
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 问题描述
- en: The inference process of an auto-regressive generative LLM typically includes
    two procedures, prompt encoding and token generation. In the prompt encoding procedure,
    for each token generation, the LLM requires contextual information from previous
    tokens, *i.e.*, key and value vectors (KV vectors). KV vectors are stored in the
    KV cache once they are generated to eliminate redundant computations. When a new
    token is generated in the token generation procedure, its associated KV vectors
    are appended to the KV cache, which implies that the size of the KV cache linearly
    increases with the length of the token sequence. However, the KV cache demonstrates
    a linear growth relationship with sequence length. As the model requires longer
    contexts, the KV cache becomes a substantial performance bottleneck. Taking OPT-175B
    as an example, with a total of 96 layers and a hidden size of 12288, its weights
    occupy 325GB memory, while the KV cache is $3.54\times$ larger, reaching 1152GB
    under its maximum sequence length.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归生成型LLM的推理过程通常包括两个程序，提示编码和令牌生成。在提示编码程序中，对于每个令牌生成，LLM需要来自先前令牌的上下文信息，即关键和数值向量（KV向量）。生成的KV向量会存储在KV缓存中，以消除冗余计算。当在令牌生成程序中生成新的令牌时，其相关的KV向量会附加到KV缓存中，这意味着KV缓存的大小随着令牌序列的长度线性增加。然而，KV缓存与序列长度表现出线性增长关系。随着模型对更长上下文的需求增加，KV缓存成为一个显著的性能瓶颈。以OPT-175B为例，其具有96层和12288的隐藏层大小，权重占用325GB内存，而KV缓存则大约大$3.54\times$，在最大序列长度下达到1152GB。
- en: 'We formally define the problem of quantization of the KV cache, we focus on
    the memory footprint of the KV cache in the attention calculation. Denote $\textbf{Q}\in\mathbb{R}^{1\times
    D}$ represents the product of S and V. All the notions and their shape are illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ QAQ: Quality Adaptive Quantization
    for LLM KV Cache").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '我们正式定义了 KV 缓存的量化问题，重点关注 KV 缓存在注意力计算中的内存占用。记 $\textbf{Q}\in\mathbb{R}^{1\times
    D}$ 表示 S 和 V 的乘积。所有概念及其形状如图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ QAQ: Quality
    Adaptive Quantization for LLM KV Cache") 所示。'
- en: For a specific quantization method $C$, we choose the method that minimizes
    the loss of accuracy and at the same time using the least memory, as follows.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的量化方法 $C$，我们选择最小化精度损失同时使用最少内存的方法，如下所示。
- en: '|  | $\displaystyle C^{*}=\text{argmin}_{C\in\mathcal{C}}\ \text{KVCacheMemory}(C)\quad$
    | s.t. |  |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle C^{*}=\text{argmin}_{C\in\mathcal{C}}\ \text{KVCacheMemory}(C)\quad$
    | 满足 |  |'
- en: '|  | $\displaystyle\left\lVert\text{Softmax}\left(\frac{1}{\sqrt{D}}\textbf{Q}\textbf{K}^{T}\right)-\text{Softmax}\left(\frac{1}{\sqrt{D}}\textbf{Q}\hat{\textbf{K}}^{T}\right)\right\rVert^{2}_{2}$
    |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\lVert\text{Softmax}\left(\frac{1}{\sqrt{D}}\textbf{Q}\textbf{K}^{T}\right)-\text{Softmax}\left(\frac{1}{\sqrt{D}}\textbf{Q}\hat{\textbf{K}}^{T}\right)\right\rVert^{2}_{2}$
    |  |'
- en: '|  | $\displaystyle\left\lVert\text{Softmax}\left(\frac{1}{\sqrt{D}}\textbf{Q}\textbf{K}^{T}\right)\textbf{V}-\text{Softmax}\left(\frac{1}{\sqrt{D}}\textbf{Q}\hat{\textbf{K}}^{T}\right)\hat{\textbf{V}}\right\rVert^{2}_{2}$
    |  |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\left\lVert\text{Softmax}\left(\frac{1}{\sqrt{D}}\textbf{Q}\textbf{K}^{T}\right)\textbf{V}-\text{Softmax}\left(\frac{1}{\sqrt{D}}\textbf{Q}\hat{\textbf{K}}^{T}\right)\hat{\textbf{V}}\right\rVert^{2}_{2}$
    |  |'
- en: where $\mathcal{C}$ is the constrained quantization loss for S and X, respectively,
    and are hyper-parameters to control the aim of accuracy.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{C}$ 是针对 S 和 X 的约束量化损失，而这些是控制精度目标的超参数。
- en: 2.2 Related Work
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 相关工作
- en: 'To alleviate the practical deployment challenges associated with the increasing
    scale of models, numerous methods have been developed in recent years for model
    compression. The memory footprint of a LLM consists of three components: model
    weights, activation, and KV cache. Early compression techniques primarily targeted
    the first two components Zhu et al. ([2023](#bib.bib22)). Among these, the most
    representative categories include quantization, pruning, distillation, and low-rank
    approximation. In the field of model compression, quantization is a widely embraced
    method. Quantization involves converting the floating-point representations within
    the model into discrete forms, such as integers, which can significantly reduce
    the storage requirement. Carefully designed quantization methods aim to minimize
    the accuracy loss to an acceptable range. Quantization can be categorized into
    two main types: quantization-aware training (QAT) and post-training quantization
    (PTQ). QAT is less practical due to the substantial re-training costs, while PTQ
    without careful design may lead to severe accuracy degradation. In the early stage
    of PTQ, certain approaches focus on quantizing only the weight of LLMs. OPTQ Frantar
    et al. ([2022b](#bib.bib9)) introduces 3 or 4 bits quantization for weights with
    improved performance. LLM.int8() Dettmers et al. ([2022](#bib.bib7)) exploits
    the importance of the outliers and employs vector-wise quantization and mixed-precision
    decomposition for outliers. AWQ Lin et al. ([2023](#bib.bib13)) finds only $1\%$
    of overall weights have a great impact on the performance of the model, it proposes
    attention-aware quantization based on this insight. ZeroQuant Yao et al. ([2022](#bib.bib18))
    integrates a quantization scheme for both weight and activation in LLMs. As the
    model demands higher capabilities for handling extremely long contexts, compressing
    the KV cache becomes pronounced. There is a limited body of recent art directly
    towards compressing the KV cache in LLM to mitigate the bottleneck. FastGen Ge
    et al. ([2023](#bib.bib11)) develops an adaptive compression method for the KV
    cache, leveraging the observation that abundant structures exist in attention
    modules. H2O Zhang et al. ([2023](#bib.bib21)) exploits the importance of a small
    set of tokens and proposes an efficient eviction strategy for the KV cache. Scissorhands
    Liu et al. ([2023](#bib.bib14)) validates the persistence of importance hypothesis
    for the KV cache and reduces the storage buffer. However, these methods rely on
    attention values to eliminate redundant parts in the KV cache, retaining the so-called
    important portions. Nevertheless, any misjudgment of importance leading to the
    loss of crucial cache can significantly degrade the performance of the model.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解与模型规模增加相关的实际部署挑战，近年来已经开发了许多模型压缩方法。LLM 的内存占用包括三个组成部分：模型权重、激活和 KV 缓存。早期的压缩技术主要针对前两部分
    Zhu 等人 ([2023](#bib.bib22))。其中，最具代表性的类别包括量化、剪枝、蒸馏和低秩近似。在模型压缩领域，量化是一种被广泛接受的方法。量化涉及将模型中的浮点表示转换为离散形式，如整数，这可以显著减少存储需求。精心设计的量化方法旨在将准确性损失降到可接受的范围。量化可以分为两种主要类型：量化感知训练（QAT）和训练后量化（PTQ）。由于需要大量的再训练成本，QAT
    的实用性较差，而未经精心设计的 PTQ 可能导致严重的准确性下降。在 PTQ 的早期阶段，某些方法专注于仅量化 LLM 的权重。OPTQ Frantar 等人
    ([2022b](#bib.bib9)) 引入了 3 或 4 位权重量化，性能有所提升。LLM.int8() Dettmers 等人 ([2022](#bib.bib7))
    利用异常值的重要性，采用了向量级量化和混合精度分解。AWQ Lin 等人 ([2023](#bib.bib13)) 发现仅 $1\%$ 的整体权重对模型性能有很大影响，并根据这一见解提出了基于注意力的量化。ZeroQuant
    Yao 等人 ([2022](#bib.bib18)) 为 LLM 中的权重和激活整合了一种量化方案。随着模型对处理极长上下文的能力要求提高，压缩 KV 缓存变得尤为突出。最近在直接压缩
    LLM 中的 KV 缓存以缓解瓶颈方面的研究较少。FastGen Ge 等人 ([2023](#bib.bib11)) 开发了一种自适应的 KV 缓存压缩方法，利用了注意力模块中存在大量结构的观察。H2O
    Zhang 等人 ([2023](#bib.bib21)) 利用少量标记的重要性，提出了一种高效的 KV 缓存驱逐策略。Scissorhands Liu 等人
    ([2023](#bib.bib14)) 验证了 KV 缓存重要性假设的持久性，并减少了存储缓冲区。然而，这些方法依赖于注意力值来消除 KV 缓存中的冗余部分，保留所谓的重要部分。然而，任何对重要性的误判导致关键缓存丢失，都可能显著降低模型性能。
- en: '![Refer to caption](img/aac032fb460fa829319609ac9eb448b4.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/aac032fb460fa829319609ac9eb448b4.png)'
- en: (a) KV cache exhibits different sensitivity to quantization.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (a) KV 缓存对量化的敏感性不同。
- en: '![Refer to caption](img/82574143a2b930c68a4eb7e56a670b4b.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/82574143a2b930c68a4eb7e56a670b4b.png)'
- en: (b) Exceptions in attention matrix.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 注意力矩阵中的例外。
- en: '![Refer to caption](img/72c0ab79ae6dea615f899863a0c835f5.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/72c0ab79ae6dea615f899863a0c835f5.png)'
- en: (c) Distribution of KV cache value and outliers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: (c) KV 缓存值和异常值的分布。
- en: 'Figure 2: The experiment demonstration of our key insights.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：我们关键洞察的实验演示。
- en: 3 Insights
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 个洞察
- en: In this section, we will introduce the exposition of the three key insights
    that inspire our design.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍激发我们设计的三个关键洞察的阐述。
- en: 3.1 Key Cache and Value Cache Exhibit Distinct Sensitivities to Quantization
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 关键缓存和价值缓存对量化表现出不同的敏感性
- en: Given the distinct roles of key vector and value vector in attention computations,
    the impact of quantization on the key cache and value cache yields disparate outcomes.
    The theoretical derivation supporting this assertion is as follows.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于关键向量和价值向量在注意力计算中的不同作用，量化对关键缓存和价值缓存的影响产生了不同的结果。支持这一断言的理论推导如下。
- en: 'We first investigate the partial derivative of $X_{j}$ as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先如下研究 $X_{j}$ 的偏导数：
- en: '|  | $\displaystyle\frac{\partial X_{j}}{\partial V_{ti}}=\begin{cases}0&amp;i\neq
    j\\ S_{t}&amp;i=j\end{cases}\text{.}$ |  |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial X_{j}}{\partial V_{ti}}=\begin{cases}0&amp;i\neq
    j\\ S_{t}&amp;i=j\end{cases}\text{.}$ |  |'
- en: 'Similarly, we investigate the partial derivative of $X_{j}$ as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们如下研究 $X_{j}$ 的偏导数：
- en: '|  | $\displaystyle\frac{\partial X_{j}}{\partial K_{ti}}=$ |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\frac{\partial X_{j}}{\partial K_{ti}}=$ |  |'
- en: 'From the above two equations, it is evident that changes in $K$. In other words,
    the key cache is more sensitive to quantization, quantizing key cache results
    in more severe performance degradation. To validate our theoretical analysis,
    we utilize a uniform quantization approach to quantize the key cache and value
    cache individually in LLaMA 2-7B. Subsequently, we assess the model’s accuracy
    in responding to 1,000 randomly sampled questions from HellaSwag Zellers et al.
    ([2019](#bib.bib19)). The results are depicted in Figure [2(a)](#S2.F2.sf1 "In
    Figure 2 ‣ 2.2 Related Work ‣ 2 Problem Description and Related Work ‣ QAQ: Quality
    Adaptive Quantization for LLM KV Cache"). Notably, the key cache and value cache
    exhibit distinct sensitivities to the same uniform quantization granularity. Specifically,
    when uniformly quantizing the value cache to 2 bits only, the model’s performance
    maintains a relatively high accuracy. However, in the case of only uniformly quantizing
    the key cache to 2 bits, the model’s performance degrades significantly, aligning
    with our derived conclusions. This underscores the necessity of employing distinct
    quantization strategies for the key cache and value cache to acquire the best
    performance. To the best of our knowledge, we are the first ever to exploit this
    observation.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '从上述两个方程可以看出，$K$ 的变化。换句话说，关键缓存对量化更加敏感，量化关键缓存会导致更严重的性能退化。为了验证我们的理论分析，我们使用统一量化方法分别对
    LLaMA 2-7B 中的关键缓存和价值缓存进行量化。随后，我们评估模型对 HellaSwag Zellers 等人 ([2019](#bib.bib19))
    中随机抽取的 1,000 个问题的响应准确性。结果如图 [2(a)](#S2.F2.sf1 "图 2 ‣ 2.2 相关工作 ‣ 2 问题描述及相关工作 ‣
    QAQ: 针对 LLM KV 缓存的质量自适应量化") 所示。值得注意的是，关键缓存和价值缓存对相同的统一量化粒度表现出不同的敏感性。具体而言，当仅将价值缓存统一量化到
    2 位时，模型的性能保持相对较高的准确性。然而，当仅将关键缓存统一量化到 2 位时，模型的性能显著下降，与我们的推导结论一致。这凸显了对关键缓存和价值缓存使用不同量化策略以获得最佳性能的必要性。据我们所知，我们是首次利用这一观察结果。'
- en: 3.2 Persistence of Importance has Exceptions
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 重要性的持续性存在例外
- en: The persistence of important tokens, which means the specific tokens that have
    larger attention value are the only ones significant for now and future steps
    in the LLM inference process, has been raised in the past works Liu et al. ([2023](#bib.bib14)).
    Although this finding holds in most cases, there are still some exceptions where
    tokens that were initially less significant become suddenly crucial in the subsequent
    process of generation.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 重要令牌的持续性，即具有更大注意力值的特定令牌是当前和未来步骤中唯一重要的令牌，在以往的工作中已被提出（Liu 等人 ([2023](#bib.bib14))）。虽然这一发现大多数情况下是成立的，但仍有一些例外，初始时不太重要的令牌在后续生成过程中会突然变得关键。
- en: 'Figure [2(b)](#S2.F2.sf2 "In Figure 2 ‣ 2.2 Related Work ‣ 2 Problem Description
    and Related Work ‣ QAQ: Quality Adaptive Quantization for LLM KV Cache") illustrates
    the attention computations matrix for the $1^{st}$ head of LLaMA 2-7B Touvron
    et al. ([2023](#bib.bib17)), after the inference process on a randomly selected
    question from HellaSwag Zellers et al. ([2019](#bib.bib19)). It is evident that
    the importance of the majority of tokens tends to persist, indicating a relatively
    stable importance level. However, there are exceptions where the importance of
    a few tokens deviates. Specifically, as illustrated by exception #1 and exception
    #2, certain tokens, initially considered less important, undergo a sudden shift
    in importance during a specific instance of inference. Without additional treatment
    for these exceptional cases, based on the assumptions of existing methodologies,
    these exceptional tokens might be evicted or discarded before demonstrating their
    importance. This could result in the loss of information when their importance
    changes and they are required for computation, subsequently impacting the model’s
    performance.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [2(b)](#S2.F2.sf2 "在图 2 ‣ 2.2 相关工作 ‣ 2 问题描述及相关工作 ‣ QAQ: 质量自适应量化用于 LLM KV
    缓存") 展示了 LLaMA 2-7B Touvron 等人 ([2023](#bib.bib17)) 在对 HellaSwag Zellers 等人 ([2019](#bib.bib19))
    随机选择的问题进行推理后的 $1^{st}$ 头的注意力计算矩阵。显然，大多数 token 的重要性往往保持稳定，显示出相对稳定的重要性水平。然而，也存在一些例外情况，某些
    token 的重要性在特定推理实例中发生了突变。具体而言，如例外 #1 和例外 #2 所示，某些初始被认为不重要的 token 在特定推理过程中突然变得重要。如果对这些异常情况没有额外处理，基于现有方法的假设，这些异常
    token 可能会在展示其重要性之前被驱逐或丢弃。这可能导致在它们变得重要并且需要进行计算时信息的丢失，从而影响模型的性能。'
- en: In our design, we propose the attention window as the solution, which stores
    the maximum value from the previous $n$ attention scores for each token, addressing
    the exceptional cases mentioned above.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的设计中，我们提出了注意力窗口作为解决方案，该窗口为每个 token 存储来自前 $n$ 次注意力评分中的最大值，以解决上述异常情况。
- en: 3.3 Outliers in KV Cache Matter
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 KV 缓存中的异常值问题
- en: The handling of outliers is of paramount importance as they can significantly
    impact model performance when formulating quantization strategies. Existing works
    have demonstrated a profound understanding of outliers in model weights and activations.
    For instance, LLM.int8() Dettmers et al. ([2022](#bib.bib7)) employs vector-wise
    quantization and mixed-precision decomposition to address outliers in the model’s
    weights. OWQ Lee et al. ([2023](#bib.bib12)) theoretically analyzes how activation
    outliers amplify errors in weight quantization.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值的处理至关重要，因为它们在制定量化策略时可能显著影响模型性能。现有研究已经深入理解了模型权重和激活中的异常值。例如，LLM.int8() Dettmers
    等人 ([2022](#bib.bib7)) 采用了按向量量化和混合精度分解来处理模型权重中的异常值。OWQ Lee 等人 ([2023](#bib.bib12))
    从理论上分析了激活异常值如何放大权重量化中的误差。
- en: 'The outliers in the KV cache also play an important role. After randomly testing
    1,000 questions from HellaSwag using the LLaMA 2-7B model, the normalized numerical
    distributions of the key cache and value cache are depicted in Figure [2(c)](#S2.F2.sf3
    "In Figure 2 ‣ 2.2 Related Work ‣ 2 Problem Description and Related Work ‣ QAQ:
    Quality Adaptive Quantization for LLM KV Cache"). We have magnified the tail for
    better visibility. Please note that the jagged distribution in the figure is a
    result of floating-point precision. The graph reveals a substantial presence of
    outliers in both the key cache and value cache. Further experiments indicate that
    neglecting to address these outliers significantly impacts the model’s performance.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 'KV 缓存中的异常值也起着重要作用。我们使用 LLaMA 2-7B 模型随机测试了 1,000 个 HellaSwag 问题，图 [2(c)](#S2.F2.sf3
    "在图 2 ‣ 2.2 相关工作 ‣ 2 问题描述及相关工作 ‣ QAQ: 质量自适应量化用于 LLM KV 缓存") 显示了键缓存和值缓存的归一化数值分布。我们放大了尾部以便更好地观察。请注意，图中的锯齿状分布是由于浮点精度造成的。图表揭示了键缓存和值缓存中都存在大量异常值。进一步的实验表明，忽视这些异常值会显著影响模型的性能。'
- en: Our approach involves the implementation of mixed precision, specifically assigning
    a separate storage precision for outliers during quantization. This strategy aims
    to minimize performance degradation attributable to precision loss associated
    with outlier values.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法涉及混合精度的实现，特别是在量化过程中为异常值分配单独的存储精度。此策略旨在最小化由于异常值相关的精度损失所导致的性能下降。
- en: 4 Quality Adaptive Quantization Method for Key Cache and Value Cache
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 关键缓存和值缓存的质量自适应量化方法
- en: In this section, we first derive the formulas for KV cache quantization. Next,
    we show how this quantification approach is employed in the text generation procedure.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先推导KV缓存量化的公式。接下来，我们展示这种量化方法在文本生成过程中的应用。
- en: 4.1 Quantitative Formula Derivation
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 定量公式推导
- en: 'The idea behind the derivation of our formula is as follows: We regard the
    quantized KV cache ($\hat{\textbf{K}}$ are satisfied.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们公式推导的思想如下：我们将量化后的KV缓存（$\hat{\textbf{K}}$满足。
- en: 'Value cache quantization. We start with the quantization of value cache, which
    is simpler in comparison. The value at index $d$ in the output of the self-attention
    module is given by:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 值缓存量化。我们从值缓存的量化开始，相比之下更为简单。自注意力模块输出中索引$d$处的值由下式给出：
- en: '|  | $\hat{X}_{d}=\sum_{t=1}^{T}S_{t}\cdot\hat{V}_{td}\text{,}$ |  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{X}_{d}=\sum_{t=1}^{T}S_{t}\cdot\hat{V}_{td}\text{,}$ |  |'
- en: 'where the standard deviation of $\hat{V}_{td}$ can be expressed as:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\hat{V}_{td}$的标准差可以表示为：
- en: '|  | $\sigma^{2\left(\textbf{X}\right)}_{d}=\sum_{t=1}^{T}S_{t}^{2}\cdot\sigma^{2\left(\textbf{V}\right)}_{t}\text{,}$
    |  |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma^{2\left(\textbf{X}\right)}_{d}=\sum_{t=1}^{T}S_{t}^{2}\cdot\sigma^{2\left(\textbf{V}\right)}_{t}\text{,}$
    |  |'
- en: 'which implies that the error of $X_{d}$, the following constraint must be satisfied:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着$X_{d}$的误差，必须满足以下约束：
- en: '|  | $\sigma^{\left(\textbf{V}\right)}_{t}\leq\frac{1}{\sqrt{T}}\cdot\frac{\sigma^{\left(\textbf{X}\right)}_{\text{max}}}{\left\lvert
    S_{t}\right\rvert}\text{.}$ |  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma^{\left(\textbf{V}\right)}_{t}\leq\frac{1}{\sqrt{T}}\cdot\frac{\sigma^{\left(\textbf{X}\right)}_{\text{max}}}{\left\lvert
    S_{t}\right\rvert}\text{.}$ |  |'
- en: This formula suggests that the error of value cache at each token should be
    inversely proportional to its corresponding attention value.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式表明，每个标记的值缓存误差应与其对应的注意力值成反比。
- en: 'Key cache quantization. Due to the involvement of the Softmax function, the
    quantization of key cache is inherently more complex. The attention score of token
    $t$ is given by:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 键缓存量化。由于涉及Softmax函数，键缓存的量化本质上更复杂。标记$t$的注意力分数由下式给出：
- en: '|  | $1$2 |  |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'Given that $\hat{A}_{t}$ according to the central limit theorem, where:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 给定$\hat{A}_{t}$根据中心极限定理，其中：
- en: '|  | $1$2 |  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: 'By definition, $e^{\hat{A}_{t}}$ with the mean and variance being:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，$e^{\hat{A}_{t}}$的均值和方差为：
- en: '|  | $\displaystyle\mathbb{E}\left[e^{\hat{A}_{t}}\right]$ |  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathbb{E}\left[e^{\hat{A}_{t}}\right]$ |  |'
- en: '|  | $\displaystyle\text{Var}\left[e^{\hat{A}_{t}}\right]$ |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\text{Var}\left[e^{\hat{A}_{t}}\right]$ |  |'
- en: 'Similarly, we expect the uniformity in error contribution from each token,
    implying that $e^{\hat{A}_{t}}$ is approximately:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们期望每个标记的误差贡献的均匀性，这意味着$e^{\hat{A}_{t}}$大约为：
- en: '|  | $\displaystyle\sigma^{2\left(\textbf{S}\right)}_{t}\approx$ |  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\sigma^{2\left(\textbf{S}\right)}_{t}\approx$ |  |'
- en: '|  | $\displaystyle=$ |  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle=$ |  |'
- en: 'To ensure that $\sigma^{2\left(\textbf{S}\right)}_{t}$, we have:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 为确保$\sigma^{2\left(\textbf{S}\right)}_{t}$，我们有：
- en: '|  | $1$2 |  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  |'
- en: This inequality suggests that the upper bound of $\sigma^{\left(\textbf{K}\right)}_{t}$),
    which varies with each inference. To address this variability, we precompute the
    distribution of the squared norm of query tensors and use the upper 10% quantile
    of this distribution in the formula. In this way, the inequality holds in 90%
    of cases.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这个不等式表明$\sigma^{\left(\textbf{K}\right)}_{t}$的上界会随着每次推理而变化。为了解决这种变动，我们预计算查询张量的平方范数的分布，并在公式中使用该分布的上10%分位数。这样，这个不等式在90%的情况下都成立。
- en: Determining quantization bits. In the final step, we determine the quantization
    bits of KV cache for each token $t$) calculated above. Owing to the symmetry between
    the key cache and value cache in this step, we only demonstrate the derivation
    using key cache K.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 确定量化位数。在最后一步中，我们确定每个标记$t$的KV缓存的量化位数。由于在此步骤中键缓存和值缓存之间的对称性，我们仅演示键缓存K的推导。
- en: 'To quantize key cache into $B^{\left(\textbf{K}\right)}_{t}$ satisfies:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 要将关键缓存量化为$B^{\left(\textbf{K}\right)}_{t}$，需要满足：
- en: '|  | $2\cdot\Delta^{\left(\textbf{K}\right)}_{t}\cdot 2^{B^{\left(\textbf{K}\right)}_{t}}=K_{t}^{\text{max}}-K_{t}^{\text{min}}\text{.}$
    |  |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $2\cdot\Delta^{\left(\textbf{K}\right)}_{t}\cdot 2^{B^{\left(\textbf{K}\right)}_{t}}=K_{t}^{\text{max}}-K_{t}^{\text{min}}\text{.}$
    |  |'
- en: 'Given that the standard deviation of the above uniform distribution $\sigma^{\left(\textbf{K}\right)}_{t}$,
    we get:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到上述均匀分布的标准差$\sigma^{\left(\textbf{K}\right)}_{t}$，我们得到：
- en: '|  | $B^{\left(\textbf{K}\right)}_{t}=\left\lceil\log_{2}\left(\frac{K_{t}^{\text{max}}-K_{t}^{\text{min}}}{2\sqrt{3}\cdot\sigma^{\left(\textbf{K}\right)}_{t}}\right)\right\rceil\text{.}$
    |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $B^{\left(\textbf{K}\right)}_{t}=\left\lceil\log_{2}\left(\frac{K_{t}^{\text{max}}-K_{t}^{\text{min}}}{2\sqrt{3}\cdot\sigma^{\left(\textbf{K}\right)}_{t}}\right)\right\rceil\text{.}$
    |  |'
- en: 4.2 Methods
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 方法
- en: Attention score prediction. Our quantization method necessitates the attention
    scores that quantify the degree to which future tokens attend to preceding tokens.
    However, these attention scores are not available at the time of quantization.
    To address this limitation, we invoke the *persistence of importance* theory,
    which posits that the attention scores (*i.e.*, importance) of each token remain
    relatively constant (*i.e.*, persistence) throughout the process of token generation.
    Consequently, we can approximate future attention scores by using the current
    ones, and use it in the quantization formulas.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力分数预测。我们的量化方法需要注意力分数，这些分数量化了未来标记对前序标记的关注程度。然而，这些注意力分数在量化时并不可用。为了解决这一限制，我们运用了*重要性持久性*理论，该理论认为每个标记的注意力分数（*即*，重要性）在标记生成过程中保持相对稳定（*即*，持久性）。因此，我们可以通过使用当前的注意力分数来近似未来的注意力分数，并在量化公式中使用它。
- en: 'Nevertheless, as stated in Section [3.2](#S3.SS2 "3.2 Persistence of Importance
    has Exceptions ‣ 3 Insights ‣ QAQ: Quality Adaptive Quantization for LLM KV Cache"),
    certain attention scores exhibit abrupt increments, which poses challenges to
    the quantization method, since quantization is irreversible, we can only quantize
    caches from higher to lower bits and not vice versa. To mitigate this issue, we
    propose *attention window*, a method that keeps track of the attention scores
    of each token and predicts the future attention scores as the maximum value within
    a window of the preceding $n$ scores. This strategy ensures that aggressive quantization
    to lower bits is undertaken only after a sequence of consistently low attention
    scores has been observed, thereby being confident that future scores will remain
    low.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，如第[3.2](#S3.SS2 "3.2 Persistence of Importance has Exceptions ‣ 3 Insights
    ‣ QAQ: Quality Adaptive Quantization for LLM KV Cache")节中所述，某些注意力分数表现出突然的增加，这对量化方法提出了挑战，因为量化是不可逆的，我们只能将缓存从高位量化到低位，而不能反向进行。为了解决这一问题，我们提出了*注意力窗口*方法，该方法跟踪每个标记的注意力分数，并预测未来的注意力分数为前$
    n $个分数窗口内的最大值。这个策略确保只有在观察到一系列持续低注意力分数之后，才会进行到低位的激进量化，从而对未来分数保持低水平有信心。'
- en: 'Outliers. Outliers of KV cache have a significant impact on the model’s performance,
    as highlighted in Section [3.3](#S3.SS3 "3.3 Outliers in KV Cache Matter ‣ 3 Insights
    ‣ QAQ: Quality Adaptive Quantization for LLM KV Cache"). We define outliers as
    the values in the KV cache that exceed the $\alpha\%$ is a hyperparameter that
    controls the proportion of values deemed as outliers. To mitigate the impact of
    outliers, we introduce a mixed-precision quantization approach. Specifically,
    we keep outliers unquantized and store them in a sparse matrix at full precision.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '异常值。KV缓存中的异常值对模型性能有显著影响，正如第[3.3](#S3.SS3 "3.3 Outliers in KV Cache Matter ‣
    3 Insights ‣ QAQ: Quality Adaptive Quantization for LLM KV Cache")节中强调的那样。我们将异常值定义为KV缓存中超出$\alpha\%$的值，这个$\alpha\%$是一个控制被视为异常值的比例的超参数。为了减少异常值的影响，我们引入了一种混合精度量化方法。具体来说，我们保持异常值不进行量化，并以全精度将其存储在稀疏矩阵中。'
- en: 'Compared to quantizing KV cache uniformly, the benefits of this method are
    twofold: 1) the important outliers themselves are stored accurately without quantization
    error; 2) the quantization of the remaining values in KV cache can be more granular
    because the distribution range is significantly reduced without outliers. Our
    experiments also demonstrate that this method effectively avoids the performance
    degradation caused by quantization.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 与均匀量化KV缓存相比，这种方法有两个好处：1）重要的异常值本身准确存储，没有量化误差；2）由于异常值的分布范围显著减少，其余值的量化可以更细粒度。我们的实验也表明，这种方法有效避免了量化引起的性能下降。
- en: Integration in text generation process. Current autoregressive LLMs generate
    text token-by-token. Within each inference iteration with our quantization method
    integrated, the model takes in a new token, combined with the quantized KV cache
    of previous tokens, and outputs the KV cache of the new token. We copy the unquantized
    new KV cache into CPU memory for future use, and calculate the quantization bits
    for all existing tokens using the previously derived formula. For tokens whose
    newly-calculated quantization bits are lower than the current bits, we further
    quantize them to the lower bits. For those otherwise, we take the unquantized
    KV cache from CPU memory and re-quantize it to the required bits. Although this
    method introduces additional transfers between CPU and GPU memory, our attention
    window technique can effectively reduce this overhead though cautious quantization
    strategy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本生成过程中的集成。当前的自回归LLM按令牌逐个生成文本。在每次推理迭代中，我们的方法将量化的KV缓存与新令牌结合，并输出新令牌的KV缓存。我们将未量化的新KV缓存复制到CPU内存中以备将来使用，并使用之前推导的公式计算所有现有令牌的量化位。对于新计算的量化位低于当前位的令牌，我们进一步将其量化到更低的位数。对于其他情况，我们从CPU内存中提取未量化的KV缓存，并重新量化到所需的位数。尽管这种方法引入了CPU和GPU内存之间的额外传输，但我们通过谨慎的量化策略可以有效减少这种开销。
- en: '| Methods | Backbone model | Task | Compression ratio with less than $1\%$
    acc. drop |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 基础模型 | 任务 | 在$1\%$准确度下降情况下的压缩比 |'
- en: '| ScissorhandsLiu et al. ([2023](#bib.bib14)) | OPT-6B | HellaSwag-Five shot
    | 3 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ScissorhandsLiu等（[2023](#bib.bib14)） | OPT-6B | HellaSwag-Five shot | 3 |'
- en: '| PIQA-Five shot | 5 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| PIQA-Five shot | 5 |'
- en: '| MathQA-Five shot | 5 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| MathQA-Five shot | 5 |'
- en: '| OPT-13B | HellaSwag-Five shot | 5 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| OPT-13B | HellaSwag-Five shot | 5 |'
- en: '| PIQA-Five shot | 5 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| PIQA-Five shot | 5 |'
- en: '| MathQA-Five shot | 5 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| MathQA-Five shot | 5 |'
- en: '| H2OZhang et al. ([2023](#bib.bib21)) | OPT-30B | PIQA | 5 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| H2OZhang等（[2023](#bib.bib21)） | OPT-30B | PIQA | 5 |'
- en: '| MathQA | 5 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| MathQA | 5 |'
- en: '| QAQ | LLaMA 2-7B | HellaSwag-Zero shot | 7.477 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| QAQ | LLaMA 2-7B | HellaSwag-Zero shot | 7.477 |'
- en: '| PIQA-Zero shot | 9.022 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| PIQA-Zero shot | 9.022 |'
- en: '| MathQA-Zero shot | 7.477 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| MathQA-Zero shot | 7.477 |'
- en: '| LLaMA 2-13B | HellaSwag-Zero shot | 8.394 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2-13B | HellaSwag-Zero shot | 8.394 |'
- en: '| PIQA-Zero shot | 9.024 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| PIQA-Zero shot | 9.024 |'
- en: '| MathQA-Zero shot | 7.888 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MathQA-Zero shot | 7.888 |'
- en: 'Table 1: The experiment performance of QAQ vs. SOTA methods.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：QAQ与SOTA方法的实验性能。
- en: '| Model | Task | w/o outliers acc. | w/o outliers compression ration | w 1%
    outliers acc. | w 1% outliers compression ration |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 任务 | 无离群点准确度 | 无离群点压缩比 | 有1%离群点准确度 | 有1%离群点压缩比 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| LLaMA 2-7B | HellaSwag | 0.572 | 7.981 | 0.722 | 7.629 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2-7B | HellaSwag | 0.572 | 7.981 | 0.722 | 7.629 |'
- en: '| PIQA | 0.707 | 7.695 | 0.763 | 7.333 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 0.707 | 7.695 | 0.763 | 7.333 |'
- en: '| MathQA | 0.250 | 7.969 | 0.279 | 7.497 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| MathQA | 0.250 | 7.969 | 0.279 | 7.497 |'
- en: '| LLaMA 2-13B | HellaSwag | 0.660 | 7.938 | 0.766 | 7.583 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2-13B | HellaSwag | 0.660 | 7.938 | 0.766 | 7.583 |'
- en: '| PIQA | 0.737 | 7.614 | 0.789 | 7.223 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 0.737 | 7.614 | 0.789 | 7.223 |'
- en: '| MathQA | 0.257 | 7.928 | 0.287 | 7.453 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| MathQA | 0.257 | 7.928 | 0.287 | 7.453 |'
- en: 'Table 2: The ablation result of outliers in QAQ.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：QAQ中的离群点消融结果。
- en: 5 Evaluation
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 评估
- en: In this section, we first introduce the experiment setting then we present the
    results that show QAQ archives up to near $10\times$ compression of KV cache memory
    footprint with no compromise in accuracy.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先介绍实验设置，然后展示结果，表明QAQ在不妥协准确性的情况下，达到了接近$10\times$的KV缓存内存足迹压缩。
- en: 5.1 Experiment Settings
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 实验设置
- en: 'Our experiments are based on a representative LLM model family, LLaMA 2 with
    model sizes ranging from 7 billion and 13 billion. The outliers ratio is set as
    $1\%$. We compare the accuracy of QAQ-LLaMA against the original LLaMA on a number
    of downstream tasks: HellaSwag Zellers et al. ([2019](#bib.bib19)), MathQA Amini
    et al. ([2019](#bib.bib1)), PIQA Bisk et al. ([2020](#bib.bib3)). The evaluation
    uses a similar architecture as lm-eval-harness Gao et al. ([2021](#bib.bib10))
    with zero shot in every task, all experiments are conducted on a server with 8
    NVIDIA V100 32GB GPUs.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实验基于一个具有代表性的LLM模型系列，LLaMA 2，其模型规模从70亿到130亿不等。离群点比例设定为$1\%$。我们将QAQ-LLaMA的准确性与原始LLaMA在多个下游任务上进行比较：HellaSwag
    Zellers等（[2019](#bib.bib19)）、MathQA Amini等（[2019](#bib.bib1)）、PIQA Bisk等（[2020](#bib.bib3)）。评估使用了类似于lm-eval-harness
    Gao等（[2021](#bib.bib10)）的架构，每个任务都进行零样本测试，所有实验均在配备8个NVIDIA V100 32GB GPU的服务器上进行。
- en: 5.2 Compression Ratio vs. Accuracy
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 压缩比与准确度
- en: 'We present the experimental evaluation results depicting the variations in
    accuracy and compression ratio for QAQ in Figure [3](#S5.F3 "Figure 3 ‣ 5.2 Compression
    Ratio vs. Accuracy ‣ 5 Evaluation ‣ QAQ: Quality Adaptive Quantization for LLM
    KV Cache"). The compression ratio is defined as the average ratio of the quantized
    KV cache size to the original KV cache size, where 1$\times$ compressed model’s
    performance approaches that of the uncompressed model, indicating the robust capability
    of QAQ to significantly compress the KV cache size without compromising performance.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '我们展示了实验评估结果，描绘了图[3](#S5.F3 "图3 ‣ 5.2 压缩比与准确率 ‣ 5 评估 ‣ QAQ: 质量自适应量化用于LLM KV缓存")中QAQ的准确率和压缩比的变化。压缩比定义为量化后的KV缓存大小与原始KV缓存大小的平均比率，其中1$\times$压缩模型的性能接近于未压缩模型，表明QAQ在不影响性能的情况下显著压缩KV缓存大小的强大能力。'
- en: '![Refer to caption](img/3d784e31352f340aaf0d47f62b0b5e4d.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3d784e31352f340aaf0d47f62b0b5e4d.png)'
- en: (a) LLaMA 2-7B Zero shot.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: (a) LLaMA 2-7B 零样本。
- en: '![Refer to caption](img/a69ee895e7ff330c5ab516eac581e189.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a69ee895e7ff330c5ab516eac581e189.png)'
- en: (b) LLaMA 2-13B Zero shot.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: (b) LLaMA 2-13B 零样本。
- en: 'Figure 3: Experiment performance of QAQ.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：QAQ的实验性能。
- en: 5.3 Comparison
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 比较
- en: 'We conducted a comparative analysis with existing state-of-the-art compression
    methodologies. In the realm of model quantization, the widespread practice of
    uniformly quantizing parameters has exhibited notable compression efficacy Frantar
    et al. ([2022a](#bib.bib8)). Considering the pivotal role played by outliers during
    quantization, we retained outliers within the framework of uniform quantization.
    The performance evaluation of QAQ is juxtaposed with that of the uniform quantization
    approach, as illustrated in Figure [4](#S5.F4 "Figure 4 ‣ 5.3 Comparison ‣ 5 Evaluation
    ‣ QAQ: Quality Adaptive Quantization for LLM KV Cache"). The compression ratio
    in the scatter plot is determined by the ratio of the compressed KV cache size
    to the original KV cache size. Closer proximity of data points to the upper-right
    corner of the figure indicates higher compression ratios and better performance
    guarantees, signifying superior overall performance. It is evident that the envelope
    formed by the QAQ, which is colored in red, data points encompasses the blue data
    points across all tasks in both models. This implies that the LLaMA models quantized
    using QAQ, whether in the 7B or 13B parameter specifications, exhibit significantly
    superior performance in the four downstream tasks compared to uniform quantization.
    This underscores the exceptional efficacy of the QAQ quantization strategy.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '我们对现有的最先进压缩方法进行了比较分析。在模型量化领域，普遍采用的均匀量化参数方法表现出显著的压缩效果 Frantar 等人 ([2022a](#bib.bib8))。考虑到离群点在量化过程中的关键作用，我们在均匀量化框架内保留了离群点。QAQ的性能评估与均匀量化方法进行对比，如图[4](#S5.F4
    "图4 ‣ 5.3 比较 ‣ 5 评估 ‣ QAQ: 质量自适应量化用于LLM KV缓存")所示。散点图中的压缩比由压缩后的KV缓存大小与原始KV缓存大小的比率决定。数据点靠近图的右上角表示更高的压缩比和更好的性能保证，表明整体性能优越。显然，QAQ所形成的红色数据点包络线涵盖了所有任务中两个模型的蓝色数据点。这意味着，无论是7B还是13B参数规格的LLaMA模型，使用QAQ进行量化在四个下游任务中都表现出显著优于均匀量化的性能。这突显了QAQ量化策略的卓越效果。'
- en: '![Refer to caption](img/cf0aeb870546153a59f7013f8fc98ec1.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf0aeb870546153a59f7013f8fc98ec1.png)'
- en: (a) HellaSwag LLaMA 2-7B.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: (a) HellaSwag LLaMA 2-7B。
- en: '![Refer to caption](img/d74aedf62657fe52d939a59b8f883d70.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d74aedf62657fe52d939a59b8f883d70.png)'
- en: (b) MathQA LLaMA 2-7B.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MathQA LLaMA 2-7B。
- en: '![Refer to caption](img/3a989b7422bfd280b0a21772c467b8fe.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3a989b7422bfd280b0a21772c467b8fe.png)'
- en: (c) PIQA LLaMA 2-7B.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: (c) PIQA LLaMA 2-7B。
- en: '![Refer to caption](img/4cff47d52e57ebe94d35112d261c0959.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4cff47d52e57ebe94d35112d261c0959.png)'
- en: (d) HellaSwag LLaMA 2-13B.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: (d) HellaSwag LLaMA 2-13B。
- en: '![Refer to caption](img/868391e458f4cc4acb2080ff6a6691ab.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/868391e458f4cc4acb2080ff6a6691ab.png)'
- en: (e) MathQA LLaMA 2-13B.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: (e) MathQA LLaMA 2-13B。
- en: '![Refer to caption](img/4186bbc3bb4d0a9a55ca142d863494b4.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4186bbc3bb4d0a9a55ca142d863494b4.png)'
- en: (f) PIQA LLaMA 2-13B.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: (f) PIQA LLaMA 2-13B。
- en: 'Figure 4: Experiment result of QAQ v.s. non-attention quantization.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：QAQ与非注意力量化的实验结果。
- en: 'For the existing SOTA techniques in compressing KV cache, we present a comparative
    analysis of QAQ in Table [1](#S4.T1 "Table 1 ‣ 4.2 Methods ‣ 4 Quality Adaptive
    Quantization Method for Key Cache and Value Cache ‣ QAQ: Quality Adaptive Quantization
    for LLM KV Cache"). It is evident that QAQ achieves a notable $1.6-1.8\times$
    improvement in lossless compression ratio across multiple downstream tasks when
    compared to the current SOTA methods. The results substantiate the outstanding
    performance of QAQ.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '对现有的 SOTA 技术在压缩 KV 缓存方面，我们在表格 [1](#S4.T1 "Table 1 ‣ 4.2 Methods ‣ 4 Quality
    Adaptive Quantization Method for Key Cache and Value Cache ‣ QAQ: Quality Adaptive
    Quantization for LLM KV Cache") 中呈现了 QAQ 的对比分析。显而易见，与当前的 SOTA 方法相比，QAQ 在多个下游任务中实现了
    $1.6-1.8\times$ 的无损压缩比改进。结果证实了 QAQ 的卓越性能。'
- en: 5.4 Ablations
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融
- en: To verify the crucial role of outliers in quantization, we conducted ablation
    experiments while keeping the remaining experimental conditions unchanged.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为验证异常值在量化中的关键作用，我们在保持其余实验条件不变的情况下进行了消融实验。
- en: 'Outliers. To evaluate the outliers’ role in the quantization method, we conduct
    the ablation experiment on the outliers. The experiments were conducted on the
    same three downstream tasks, distinguishing between two groups: one where outliers
    were treated separately and another where no special treatment was applied. The
    presence of outliers was determined based on the numerical distribution. The experimental
    results were averaged over 10 trials and are presented in Table [2](#S4.T2 "Table
    2 ‣ 4.2 Methods ‣ 4 Quality Adaptive Quantization Method for Key Cache and Value
    Cache ‣ QAQ: Quality Adaptive Quantization for LLM KV Cache"). The experimental
    results indicate that outliers have a substantial impact on KV cache quantization.
    In cases where outliers are not handled separately, the model’s performance on
    downstream tasks experiences a significant decline of $12\%-26\%$ additional overhead
    on the compression ratio. This demonstrates the efficient and accurate handling
    of outliers by QAQ.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '异常值。为了评估异常值在量化方法中的作用，我们对异常值进行了消融实验。实验在相同的三个下游任务上进行，区分为两组：一组是将异常值单独处理，另一组则没有进行特殊处理。异常值的存在是基于数值分布来确定的。实验结果在
    10 次试验后取平均，并在表格 [2](#S4.T2 "Table 2 ‣ 4.2 Methods ‣ 4 Quality Adaptive Quantization
    Method for Key Cache and Value Cache ‣ QAQ: Quality Adaptive Quantization for
    LLM KV Cache") 中呈现。实验结果表明，异常值对 KV 缓存量化有显著影响。在未单独处理异常值的情况下，模型在下游任务上的性能下降了 $12\%-26\%$，压缩比额外增加。这展示了
    QAQ 对异常值的高效和准确处理。'
- en: 'Attention window size. To verify the importance of handling exceptional cases
    in quantization, we conducted ablation experiments to investigate the impact of
    treating such cases differently. The experiments are conducted on the same three
    downstream tasks, involving two groups: one with a specified window size and the
    other without, where the absence of a specific setting implies a default window
    size of $1$ in performance. This further advances the performance of QAQ.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力窗口大小。为了验证在量化中处理特殊情况的重要性，我们进行了消融实验，以调查不同处理方式对这些情况的影响。实验在相同的三个下游任务中进行，涉及两组：一组具有指定窗口大小，另一组没有，其中没有特定设置的情况下默认窗口大小为
    $1$。这进一步推动了 QAQ 的性能。
- en: '| Model | Task | Attention window size | Acc. | Compression ratio |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Model | Task | Attention window size | Acc. | Compression ratio |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLaMA 2-7B | HellaSwag | 1 | 0.722 | 7.628 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2-7B | HellaSwag | 1 | 0.722 | 7.628 |'
- en: '| 5 | 0.730 | 7.377 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.730 | 7.377 |'
- en: '| PIQA | 1 | 0.755 | 7.820 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 1 | 0.755 | 7.820 |'
- en: '| 5 | 0.778 | 6.797 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.778 | 6.797 |'
- en: '| MathQA | 1 | 0.276 | 7.633 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| MathQA | 1 | 0.276 | 7.633 |'
- en: '| 5 | 0.284 | 7.331 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.284 | 7.331 |'
- en: '| LLaMA 2-13B | HellaSwag | 1 | 0.766 | 7.583 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| LLaMA 2-13B | HellaSwag | 1 | 0.766 | 7.583 |'
- en: '| 5 | 0.772 | 7.340 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.772 | 7.340 |'
- en: '| PIQA | 1 | 0.788 | 7.692 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| PIQA | 1 | 0.788 | 7.692 |'
- en: '| 5 | 0.794 | 6.802 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.794 | 6.802 |'
- en: '| MathQA | 1 | 0.283 | 7.588 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| MathQA | 1 | 0.283 | 7.588 |'
- en: '| 5 | 0.295 | 7.321 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.295 | 7.321 |'
- en: 'Table 3: The ablation result of attention window in QAQ.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 3: QAQ 中注意力窗口的消融结果。'
- en: 6 Conclusion
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Inspired by three key insights, we propose a quality adaptive quantization scheme,
    QAQ, for the KV cache to reduce its memory footprint. Our method demonstrates
    memory reduction of $10\times$ in the KV cache with neglectable loss of accuracy.
    The superior performance achieved by QAQ allows the model to accommodate longer
    contextual inputs, creating new possibilities for a broader range of applications.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 受到三个关键见解的启发，我们提出了一种质量自适应量化方案 QAQ，用于 KV 缓存，以减少其内存占用。我们的方法在 KV 缓存中实现了$10\times$的内存减少，并且准确性损失可以忽略不计。QAQ
    达到的卓越性能使模型能够处理更长的上下文输入，为更广泛的应用开辟了新的可能性。
- en: References
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Amini et al. [2019] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski,
    Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem
    solving with operation-based formalisms. arXiv preprint arXiv:1905.13319, 2019.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amini 等人 [2019] Aida Amini、Saadia Gabriel、Peter Lin、Rik Koncel-Kedziorski、Yejin
    Choi 和 Hannaneh Hajishirzi。MathQA：利用基于操作的形式化方法解决可解释的数学文字题。arXiv 预印本 arXiv:1905.13319，2019。
- en: Beeching et al. [2023] Edward Beeching, Clémentine Fourrier, Nathan Habib, Sheon
    Han, Nathan Lambert, Nazneen Rajani, Omar Sanseviero, Lewis Tunstall, and Thomas
    Wolf. Open LLM leaderboard. [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard),
    2023.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beeching 等人 [2023] Edward Beeching、Clémentine Fourrier、Nathan Habib、Sheon Han、Nathan
    Lambert、Nazneen Rajani、Omar Sanseviero、Lewis Tunstall 和 Thomas Wolf。开放 LLM 排行榜。
    [https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard](https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard)，2023。
- en: 'Bisk et al. [2020] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al.
    PIQA: Reasoning about physical commonsense in natural language. In Proceedings
    of the AAAI conference on artificial intelligence, volume 34, pages 7432–7439,
    2020.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bisk 等人 [2020] Yonatan Bisk、Rowan Zellers、Jianfeng Gao、Yejin Choi 等。PIQA：在自然语言中推理物理常识。在《AAAI
    人工智能会议论文集》，第 34 卷，第 7432–7439 页，2020。
- en: Brown et al. [2020] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,
    Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,
    Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
    Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher
    Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
    Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario
    Amodei. Language models are few-shot learners, 2020.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 [2020] Tom B. Brown、Benjamin Mann、Nick Ryder、Melanie Subbiah、Jared
    Kaplan、Prafulla Dhariwal、Arvind Neelakantan、Pranav Shyam、Girish Sastry、Amanda
    Askell、Sandhini Agarwal、Ariel Herbert-Voss、Gretchen Krueger、Tom Henighan、Rewon
    Child、Aditya Ramesh、Daniel M. Ziegler、Jeffrey Wu、Clemens Winter、Christopher Hesse、Mark
    Chen、Eric Sigler、Mateusz Litwin、Scott Gray、Benjamin Chess、Jack Clark、Christopher
    Berner、Sam McCandlish、Alec Radford、Ilya Sutskever 和 Dario Amodei。语言模型是少样本学习者，2020。
- en: Chang et al. [2023] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu,
    Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. A survey
    on evaluation of large language models. arXiv preprint arXiv:2307.03109, 2023.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chang 等人 [2023] Yupeng Chang、Xu Wang、Jindong Wang、Yuan Wu、Kaijie Zhu、Hao Chen、Linyi
    Yang、Xiaoyuan Yi、Cunxiang Wang、Yidong Wang 等。关于大语言模型评估的调查。arXiv 预印本 arXiv:2307.03109，2023。
- en: Child et al. [2019] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
    Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509,
    2019.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Child 等人 [2019] Rewon Child、Scott Gray、Alec Radford 和 Ilya Sutskever。使用稀疏转换器生成长序列。arXiv
    预印本 arXiv:1904.10509，2019。
- en: 'Dettmers et al. [2022] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint
    arXiv:2208.07339, 2022.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 [2022] Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer。Llm.
    int8 (): 大规模转换器的 8 位矩阵乘法。arXiv 预印本 arXiv:2208.07339，2022。'
- en: 'Frantar et al. [2022a] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
    Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained
    transformers. arXiv preprint arXiv:2210.17323, 2022.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人 [2022a] Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。Gptq：生成预训练转换器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323，2022。
- en: 'Frantar et al. [2022b] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and
    Dan Alistarh. Optq: Accurate quantization for generative pre-trained transformers.
    In The Eleventh International Conference on Learning Representations, 2022.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人 [2022b] Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。Optq：生成预训练转换器的准确量化。在第十一届国际学习表征会议，2022。
- en: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff,
    et al. A framework for few-shot language model evaluation. Version v0\. 0.1\.
    Sept, 2021.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. [2021] Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony
    DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff
    等. 少样本语言模型评估框架。版本 v0\. 0.1\. 2021年9月。
- en: 'Ge et al. [2023] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han,
    and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression
    for LLMs. arXiv preprint arXiv:2310.01801, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge et al. [2023] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han
    和 Jianfeng Gao. 模型告诉你该丢弃什么：针对 LLM 的自适应 KV 缓存压缩。arXiv 预印本 arXiv:2310.01801, 2023。
- en: 'Lee et al. [2023] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok
    Park. OWQ: Lessons learned from activation outliers for weight quantization in
    large language models. arXiv preprint arXiv:2306.02272, 2023.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. [2023] Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim 和 Eunhyeok
    Park. OWQ: 从激活异常中获得的权重量化经验教训。arXiv 预印本 arXiv:2306.02272, 2023。'
- en: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration. arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. [2023] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang
    和 Song Han. Awq: 激活感知的权重量化用于 LLM 压缩和加速。arXiv 预印本 arXiv:2306.00978, 2023。'
- en: 'Liu et al. [2023] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands:
    Exploiting the persistence of importance hypothesis for LLM KV cache compression
    at test time. arXiv preprint arXiv:2305.17118, 2023.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2023] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor
    Xie, Zhaozhuo Xu, Anastasios Kyrillidis, 和 Anshumali Shrivastava. Scissorhands:
    利用重要性持久性假设进行 LLM KV 缓存压缩。arXiv 预印本 arXiv:2305.17118, 2023。'
- en: Pope et al. [2023] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
    James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently
    scaling transformer inference. Proceedings of Machine Learning and Systems, 5,
    2023.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pope et al. [2023] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
    James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal 和 Jeff Dean. 高效扩展转换器推理。机器学习与系统会议录,
    5, 2023。
- en: Seltman [2012] Howard Seltman. Approximations for mean and variance of a ratio.
    unpublished note, 2012.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seltman [2012] Howard Seltman. 比率的均值和方差的近似。未出版的笔记, 2012。
- en: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. LLaMA: Open and efficient foundation language
    models. arXiv preprint arXiv:2302.13971, 2023.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. [2023] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar 等. LLaMA: 开放而高效的基础语言模型。arXiv 预印本 arXiv:2302.13971, 2023。'
- en: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. Advances in Neural Information Processing
    Systems, 35:27168–27183, 2022.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao et al. [2022] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li 和 Yuxiong He. Zeroquant: 大规模转换器的高效且经济的后训练量化。神经信息处理系统进展, 35:27168–27183,
    2022。'
- en: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi,
    and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint
    arXiv:1905.07830, 2019.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. [2019] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi
    和 Yejin Choi. Hellaswag: 机器真的能完成你的句子吗？arXiv 预印本 arXiv:1905.07830, 2019。'
- en: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    et al. OPT: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068,
    2022.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2022] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin
    等. OPT: 开放的预训练转换器语言模型。arXiv 预印本 arXiv:2205.01068, 2022。'
- en: 'Zhang et al. [2023] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al.
    H[2]O: Heavy-hitter oracle for efficient generative inference of large language
    models. arXiv preprint arXiv:2306.14048, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang et al. [2023] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin
    Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett 等. H[2]O:
    大型语言模型高效生成推理的重击预言器。arXiv 预印本 arXiv:2306.14048, 2023。'
- en: Zhu et al. [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models. arXiv preprint arXiv:2308.07633,
    2023.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等人 [2023] Xunyu Zhu, Jian Li, Yong Liu, Can Ma 和 Weiping Wang。关于大语言模型的模型压缩调查。arXiv
    预印本 arXiv:2308.07633，2023。
