- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:51:12'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:51:12
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular
    Quantizers'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ModuLoRA：通过与模块化量化器集成，在消费级GPU上微调3-bit LLMs
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16119](https://ar5iv.labs.arxiv.org/html/2309.16119)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.16119](https://ar5iv.labs.arxiv.org/html/2309.16119)
- en: Junjie Yin jyin27@jhu.edu
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 尹俊杰 jyin27@jhu.edu
- en: Department of Computer Science
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Johns Hopkins University Jiahao Dong jd787@cornell.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 约翰霍普金斯大学 冯家豪 jd787@cornell.edu
- en: Department of Computer Science
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Cornell University and Cornell Tech Yingheng Wang yw2349@cornell.edu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学与康奈尔科技 翁恒 yw2349@cornell.edu
- en: Department of Computer Science
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Cornell University Christopher De Sa cdesa@cs.cornell.edu
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学 克里斯托弗·德·萨 cdesa@cs.cornell.edu
- en: Department of Computer Science
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Cornell University Volodymyr Kuleshov kuleshov@cornell.edu
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学 弗拉基米尔·库列绍夫 kuleshov@cornell.edu
- en: Department of Computer Science
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学系
- en: Cornell University and Cornell Tech
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 康奈尔大学与康奈尔科技
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: We propose a memory-efficient finetuning algorithm for large language models
    (LLMs) that supports finetuning LLMs with 65B parameters in 3-bit or 4-bit precision
    on as little as one 48GB GPU. Our method, modular low-rank adaptation (ModuLoRA),
    integrates any user-specified weight quantizer with finetuning via low-rank adapters
    (LoRAs). Our approach relies on a simple quantization-agnostic backward pass that
    adaptively materializes low-precision LLM weights from a custom black-box quantization
    module. This approach enables finetuning 3-bit LLMs for the first time—leveraging
    state-of-the-art 3-bit OPTQ quantization often outperforms finetuning that relies
    on less sophisticated 4-bit and 8-bit methods. In our experiments, ModuLoRA attains
    competitive performance on text classification, natural language infernece, and
    instruction following tasks using significantly less memory than existing approaches,
    and we also surpass the state-of-the-art ROUGE score on a popular summarization
    task. We release ModuLoRA together with a series of low-precision models—including
    the first family of 3-bit instruction following Alpaca LLMs—as part of LLMTools,
    a user-friendly library for quantizing, running, and finetuning LLMs on consumer
    GPUs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种内存高效的微调算法，用于大规模语言模型（LLMs），支持在仅需一个48GB GPU上以3-bit或4-bit精度微调具有65B参数的LLMs。我们的方法，模块化低秩适应（ModuLoRA），将任何用户指定的权重量化器与通过低秩适配器（LoRAs）进行的微调集成。我们的方法依赖于一种简单的量化无关的反向传递，它自适应地从自定义黑箱量化模块中生成低精度LLM权重。这种方法首次使得3-bit
    LLM的微调成为可能——利用最先进的3-bit OPTQ量化技术通常优于依赖较少精细化的4-bit和8-bit方法的微调。在我们的实验中，ModuLoRA在文本分类、自然语言推理和指令跟随任务中表现出竞争力，使用的内存显著少于现有方法，我们还在一个流行的摘要任务上超越了最先进的ROUGE评分。我们将ModuLoRA及一系列低精度模型（包括第一批3-bit指令跟随的Alpaca
    LLMs）作为LLMTools的一部分发布，LLMTools是一个用户友好的库，用于在消费级GPU上进行量化、运行和微调LLMs。
- en: 1 Introduction
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) excel across diverse tasks such as code generation,
    instruction following, and reasoning (Brown et al., [2020](#bib.bib2); Scao et al.,
    [2023](#bib.bib31); Zhang et al., [2022](#bib.bib42)). However, the massive size
    of these models—often reaching into hundreds of billions of parameters—makes them
    challenging to deploy on downstream tasks and motivates research into efficient
    finetuning algorithms (Li & Liang, [2021](#bib.bib19); Hu et al., [2022](#bib.bib15)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 大规模语言模型（LLMs）在代码生成、指令跟随和推理等多种任务中表现出色（Brown等，[2020](#bib.bib2)；Scao等，[2023](#bib.bib31)；Zhang等，[2022](#bib.bib42)）。然而，这些模型的庞大规模——通常达到数百亿参数——使其在下游任务中的部署具有挑战性，激励了对高效微调算法的研究（Li
    & Liang，[2021](#bib.bib19)；Hu等，[2022](#bib.bib15)）。
- en: Here, we propose modular low-rank adaptation (ModuLoRA), a memory-efficient
    finetuning algorithm for large language models (LLMs) that runs on consumer-grade
    hardware. For example, ModuLoRA finetunes a LLAMA-30B model (Touvron et al., [2023](#bib.bib35))
    on one Nvidia RTX 3090 24GB GPU and a LLAMA-65B on one RTX A6000 48GB GPU.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提出了模块化低秩适应（ModuLoRA），一种内存高效的大规模语言模型（LLMs）微调算法，运行于消费级硬件上。例如，ModuLoRA在一台Nvidia
    RTX 3090 24GB GPU上微调LLAMA-30B模型（Touvron等，[2023](#bib.bib35)），以及在一台RTX A6000 48GB
    GPU上微调LLAMA-65B。
- en: Our approach adds high-precision low-rank adapters to the low-precision 3-bit
    or 4-bit weights of a frozen base LLM obtained via modern quantization algorithms
    (Hubara et al., [2021](#bib.bib16); Yao et al., [2021](#bib.bib38); Frantar et al.,
    [2023](#bib.bib12)). Crucially, ModuLoRA does not specify its own quantization
    procedure—rather, it integrates with user-defined quantizers via a simple quantization-agnostic
    backward pass. This backward pass adaptively materializes low-precision LLM weights
    obtained from a black-box quantizer and integrates them with high-precision low-rank
    adapters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法在通过现代量化算法（Hubara等，[2021](#bib.bib16)；Yao等，[2021](#bib.bib38)；Frantar等，[2023](#bib.bib12)）获得的低精度3-bit或4-bit权重的基础LLM上，添加了高精度低秩适配器。关键是，ModuLoRA并不指定其自身的量化程序——而是通过简单的量化无关的反向传递与用户定义的量化器集成。这个反向传递自适应地实现了从黑箱量化器获得的低精度LLM权重，并将它们与高精度低秩适配器集成。
- en: We release ModuLoRA as part of LLMTools, a user-friendly library that enables
    finetuning LLMs on consumer GPUs. When paired with the modern OPTQ quantizer (Frantar
    et al., [2023](#bib.bib12)), ModuLoRA enables finetuning 3-bit LLMs for the first
    time, often outperforming methods based on less sophisticated 4-bit and 8-bit
    quantization. Across tasks in classification, natural language inference, and
    instruction following, our 3-bit and 4-bit models achieve competitive performance
    using significantly less memory than existing approaches. On a popular summarization
    benchmark, we attain a new state-of-the-art ROUGE score using a quantized LLAMA-65B
    model. We open-source all our low-precision models, including the first 3-bit
    family of Alpaca models that feature strong instruction-following performance
    at multiple model sizes. Our findings reveal that high performance can be achieved
    using smaller quantized LLMs than previously thought.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将ModuLoRA作为LLMTools的一部分发布，这是一个用户友好的库，能够在消费级GPU上微调LLM。当与现代OPTQ量化器（Frantar等，[2023](#bib.bib12)）配合使用时，ModuLoRA首次实现了对3-bit
    LLM的微调，通常优于基于不那么复杂的4-bit和8-bit量化的方法。在分类、自然语言推理和指令跟随等任务中，我们的3-bit和4-bit模型使用显著更少的内存实现了有竞争力的性能。在一个流行的摘要基准上，我们使用量化的LLAMA-65B模型达到了新的最先进ROUGE分数。我们开源了所有低精度模型，包括首个具有强指令跟随性能的3-bit
    Alpaca模型系列，涵盖多个模型尺寸。我们的发现表明，使用比以前认为的更小的量化LLM也能实现高性能。
- en: 'Contributions.  In summary, this paper makes the following contributions: (1)
    we propose ModuLoRA, a memory-efficient finetuning method that operates over low-precision
    weights obtained via a user-specified black-box quantization module; (2) we release
    LLMTools, a user-friendly Python library that features an implementation of ModuLoRA and
    that enables users to easily finetune the largest LLMs on consumer GPUs; (3) we
    provide empirical evidence that high performance on downstream tasks can be achieved
    with a smaller LLM than previously thought.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 贡献。总结来说，本文做出了以下贡献：（1）我们提出了ModuLoRA，这是一种内存高效的微调方法，能够在用户指定的黑箱量化模块中操作低精度权重；（2）我们发布了LLMTools，这是一个用户友好的Python库，包含了ModuLoRA的实现，使用户能够轻松地在消费级GPU上微调最大的LLM；（3）我们提供了实证证据，证明使用比以前认为的更小的LLM也能在下游任务中实现高性能。
- en: 2 Background and Related Work
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景与相关工作
- en: We are interested in finetuning a pre-trained LLM for downstream tasks (Li &
    Liang, [2021](#bib.bib19); Lester et al., [2021](#bib.bib18); Houlsby et al.,
    [2019](#bib.bib14); Rebuffi et al., [2017](#bib.bib30)). LLMs use a transformer
    architecture where almost all of the learnable weights—and almost all of the memory
    used to store these weights—appear in linear layers.¹¹1These layers include the
    $K$ in full precision.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对微调预训练LLM以进行下游任务感兴趣（Li & Liang，[2021](#bib.bib19)；Lester等，[2021](#bib.bib18)；Houlsby等，[2019](#bib.bib14)；Rebuffi等，[2017](#bib.bib30)）。LLM使用变换器架构，其中几乎所有可学习的权重——以及几乎所有用于存储这些权重的内存——都出现在线性层中。¹¹1这些层包括全精度的$K$。
- en: 2.1 Large Language Model Finetuning
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 大型语言模型微调
- en: Because of the high memory requirements needed to fine-tune and store all the
    weights of a LLM, practitioners have developed a variety of *parameter-efficient
    fine tuning* methods that learn in a lower dimensional space. These methods include
    tuning only the output layer (Devlin et al., [2018](#bib.bib9)) and tuning the
    prompt or prefix passed as input to an LLM (Lester et al., [2021](#bib.bib18);
    Li & Liang, [2021](#bib.bib19); Liu et al., [2023a](#bib.bib22); [b](#bib.bib24)),
    as well as LoRA, which is the focus of this work.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微调和存储大型语言模型（LLM）所有权重所需的高内存要求，研究人员开发了各种*参数高效的微调*方法，这些方法在更低维空间中进行学习。这些方法包括仅调整输出层（Devlin
    等，[2018](#bib.bib9)）以及调整作为输入传递给LLM的提示或前缀（Lester 等，[2021](#bib.bib18)；Li & Liang，[2021](#bib.bib19)；Liu
    等，[2023a](#bib.bib22)；[b](#bib.bib24)），以及本文关注的LoRA。
- en: Low-Rank Adaptation (LoRA)
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 低秩适配（LoRA）
- en: 'The LoRA algorithm (Hu et al., [2022](#bib.bib15)) decomposes the weights $\mathbf{W}$;
    the rectangular case is a straightforward generalization.:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 算法（Hu 等，[2022](#bib.bib15)）对权重 $\mathbf{W}$ 进行分解；矩形情况是一个直接的推广。
- en: '|  | $\mathbf{W}=\mathbf{W}_{0}+\mathbf{A}\mathbf{B}^{\top}.$ |  | (1) |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{W}=\mathbf{W}_{0}+\mathbf{A}\mathbf{B}^{\top}.$ |  | (1) |'
- en: LoRA reduces the number of trained parameters by a factor of $2r/d$ in memory,
    which requires multiple high-end GPUs and precludes tuning large LLMs on commodity
    hardware.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 通过 $2r/d$ 的因子减少了训练参数的数量，这需要多个高端GPU，并且不允许在普通硬件上调整大型LLMs。
- en: 2.2 Low-Precision Machine Learning
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 低精度机器学习
- en: The computational requirements of modern machine learning models motivate a
    wide range of efficient machine learning algorithms (Li & Liang, [2021](#bib.bib19);
    Hu et al., [2022](#bib.bib15); Frantar et al., [2023](#bib.bib12)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现代机器学习模型的计算需求激发了广泛的高效机器学习算法（Li & Liang，[2021](#bib.bib19)；Hu 等，[2022](#bib.bib15)；Frantar
    等，[2023](#bib.bib12)）。
- en: Quantization
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量化
- en: Quantization methods for neural networks reduce the number of bits required
    to store model weights (Dong et al., [2019](#bib.bib10); [2020](#bib.bib11); Yao
    et al., [2022](#bib.bib39); Park et al., [2023](#bib.bib27)). A $b$-bit quantization
    method has the form
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的量化方法减少了存储模型权重所需的位数（Dong 等，[2019](#bib.bib10)；[2020](#bib.bib11)；Yao 等，[2022](#bib.bib39)；Park
    等，[2023](#bib.bib27)）。一个 $b$-位量化方法的形式是
- en: '|  | $\displaystyle(\hat{\mathbf{W}}_{q},\mathbf{z},\mathbf{s})=\mathcal{Q}(\mathbf{W})$
    |  | (2) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle(\hat{\mathbf{W}}_{q},\mathbf{z},\mathbf{s})=\mathcal{Q}(\mathbf{W})$
    |  | (2) |'
- en: Here, the quantization algorithm $\mathcal{Q}$ are extended with numpy-style
    broadcasting.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，量化算法 $\mathcal{Q}$ 扩展了numpy风格的广播功能。
- en: 'Recently, Frantar et al. ([2023](#bib.bib12)) proposed OPTQ, a quantization
    algorithm that scales to modern LLMs. The method iteretiavely runs two steps over
    the weight columns: (1) quantize with nearest rounding and compute the error,
    (2) update the remaining weights with a scaled error. Many of our experiments
    finetune LLMs quantized with OPTQ.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Frantar 等（[2023](#bib.bib12)）提出了 OPTQ，这是一种适用于现代LLMs的量化算法。该方法在权重列上迭代执行两个步骤：（1）使用最近的舍入进行量化并计算误差，（2）用缩放误差更新剩余权重。我们的许多实验对使用
    OPTQ 量化的LLMs进行微调。
- en: 'In concurrent work, Dettmers et al. ([2023](#bib.bib8)) proposed QLoRA, an
    approach for tuning quantized LLMs based on LoRA. While our work seeks to integrate
    with any user-defined quantization module (such as OPTQ), QLoRA defines its own
    quantization scheme, which is simpler than, say, OPTQ. One advantage of our approach
    is support for 3-bit finetuning (and potentially 2-bit via new quantizers; Chee
    et al. ([2023](#bib.bib4))); QLoRA only supports 4-bit finetuning. We will also
    identify settings where using advanced quantizers yields performance gains over
    QLoRA. See Section [5.1](#S5.SS1 "5.1 Comparison to Related Work ‣ 5 Discussion
    ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular
    Quantizers") for details.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '在并行工作中，Dettmers 等（[2023](#bib.bib8)）提出了 QLoRA，这是一种基于 LoRA 的量化 LLM 微调方法。虽然我们的工作旨在与任何用户定义的量化模块（如
    OPTQ）集成，但 QLoRA 定义了其自身的量化方案，这比例如 OPTQ 更简单。我们方法的一个优势是支持 3 位微调（以及通过新量化器可能支持 2 位；Chee
    等（[2023](#bib.bib4)））；QLoRA 仅支持 4 位微调。我们还将识别使用先进量化器比QLoRA带来性能提升的设置。详情请见第 [5.1](#S5.SS1
    "5.1 Comparison to Related Work ‣ 5 Discussion ‣ ModuLoRA: Finetuning 3-Bit LLMs
    on Consumer GPUs by Integrating with Modular Quantizers")节。'
- en: 3 Low-Precision Low-Rank Adaptation with a Modular Quantizer
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 低精度低秩适配与模块化量化器
- en: In this section, we describe modular low-rank adaptation (ModuLoRA), a memory-efficient
    finetuning algorithm for large language models (LLMs) that leverages custom quantization
    algorithms and runs on consumer GPU hardware.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了模块化低秩适应（ModuLoRA），这是一种用于大语言模型（LLMs）的内存高效微调算法，利用自定义量化算法并在消费级 GPU 硬件上运行。
- en: '\par\parclass  ModuLoRALinear(Module):"""Linear  ModuLoRA  Layer"""\pardef  __init__(self,  …):self.hatWq_z_s  =  quantize(pretrained_W)(self.A,  self.B)  =  lora_init(…)\pardef  forward(self,  x):(hatWq,  z,  s)  =  self.hatWq_z_sreturn  LPLinear.apply(x,  hatWq,  z,  s)  \  +  (x  @  self.B)  @  self.A.t()  +  self.bias\par\par\parclass  LPLinear(Function):"""Low-Precision  Linear  Map"""@staticmethoddef  forward(ctx,  input,  hatWq,  z,  s):ctx.save_for_backward(hatWq,  z,  s)hatW  =  dequantize(hatWq,  z,  s)output  =  input  @  hatW.t()return  output  #  hatW  is  deallocated@staticmethoddef  backward(ctx,  grad_output):hatWq,  z,  s  =  ctx.saved_tensors#  we  recompute  hatWhatW  =  dequantize(hatWq,  z,  s)grad_input  =  grad_output  @  hatW#  here  hatW  can  be  deallocatedreturn  grad_input,  None,  None,  None\par'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '\par\parclass  ModuLoRALinear(Module):"""线性  ModuLoRA  层"""\pardef  __init__(self,  …):self.hatWq_z_s  =  quantize(pretrained_W)(self.A,  self.B)  =  lora_init(…)\pardef  forward(self,  x):(hatWq,  z,  s)  =  self.hatWq_z_sreturn  LPLinear.apply(x,  hatWq,  z,  s)  \  +  (x  @  self.B)  @  self.A.t()  +  self.bias\par\par\parclass  LPLinear(Function):"""低精度  线性  映射"""@staticmethoddef  forward(ctx,  input,  hatWq,  z,  s):ctx.save_for_backward(hatWq,  z,  s)hatW  =  dequantize(hatWq,  z,  s)output  =  input  @  hatW.t()return  output  #  hatW  被  释放@staticmethoddef  backward(ctx,  grad_output):hatWq,  z,  s  =  ctx.saved_tensors#  我们  重新计算  hatWhatW  =  dequantize(hatWq,  z,  s)grad_input  =  grad_output  @  hatW#  在这里  hatW  可以  被  释放return  grad_input,  None,  None,  None\par'
- en: 'Figure 1: PyTorch pseudocode for ModuLoRA.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：ModuLoRA 的 PyTorch 伪代码。
- en: 3.1 Low-Rank Adaptation of Low-Precision Models
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 低秩适应的低精度模型
- en: 'The first step of our approach is quantization: we apply a black-box quantization
    algorithm $\mathcal{Q}$ as part of ModuLoRA—rather, we seek to support user-defined
    quantizers that are treated by our method is a black-box.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们方法的第一步是量化：我们应用一个黑箱量化算法 $\mathcal{Q}$ 作为 ModuLoRA 的一部分——实际上，我们希望支持用户定义的量化器，这些量化器被我们的方法视为黑箱。
- en: The core of our efforts focuses on finetuning the base quantized model. Our
    method first modifies the network by replacing each linear layer—originally defined
    by the affine map $x\mapsto x(\mathbf{W}^{(i)})^{\top}+\mathbf{b}^{(i)}$—with
    the reparameterized low precision `ModuLoRALinear} layer in Figure \ref`fig:lplora_code,
    given by
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的核心集中在微调基础量化模型。我们的方法首先通过用图 \ref`fig:lplora_code` 中所示的重新参数化低精度 `ModuLoRALinear`
    层替换每个线性层——原本由仿射映射 $x\mapsto x(\mathbf{W}^{(i)})^{\top}+\mathbf{b}^{(i)}$ 定义。
- en: '|  | $x\mapsto x(\hat{\mathbf{W}}^{(i)})^{\top}+x\mathbf{B}^{(i)}(\mathbf{A}^{(i)})^{\top}+\mathbf{b}^{(i)}.$
    |  | (3) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $x\mapsto x(\hat{\mathbf{W}}^{(i)})^{\top}+x\mathbf{B}^{(i)}(\mathbf{A}^{(i)})^{\top}+\mathbf{b}^{(i)}.$
    |  | (3) |'
- en: Here $\mathbf{A}^{(i)},\mathbf{B}^{(i)}\in\mathbb{R}^{d\times r}$’s.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\mathbf{A}^{(i)},\mathbf{B}^{(i)}\in\mathbb{R}^{d\times r}$。
- en: 3.1.1 The Structure of a Quantized Backward Pass
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 量化反向传递的结构
- en: We illustrate the technical challenges that arise in the design of a quantized
    backward pass in the context of a network of $n$ `ModuLoRALinear} layers. Each
    \mintinline`pythonModuLoRALinear is effectively a fully connected layer with reparameterized
    dense weights defined as
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说明了在 $n$ 个 `ModuLoRALinear` 层的网络设计中出现的技术挑战。每个 `ModuLoRALinear` 实际上是一个全连接层，其重新参数化的稠密权重定义为
- en: '|  | $\mathbf{W}^{(i)}_{l}=\hat{\mathbf{W}}^{(i)}+\mathbf{A}^{(i)}(\mathbf{B}^{(i)})^{\top},$
    |  | (4) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{W}^{(i)}_{l}=\hat{\mathbf{W}}^{(i)}+\mathbf{A}^{(i)}(\mathbf{B}^{(i)})^{\top},$
    |  | (4) |'
- en: biases $\mathbf{b}^{(i)}$, where we overload the Leibniz notation for derivatives
    to also denote gradients. By the chain rule,
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差 $\mathbf{b}^{(i)}$，我们重载了莱布尼茨符号来表示梯度。通过链式法则，
- en: '|  | $\frac{\mathrm{d}L}{\mathrm{d}\mathbf{A}^{(i)}}=\frac{\mathrm{d}L}{\mathrm{d}\bar{\mathbf{y}}_{i}}\cdot\frac{\mathrm{d}\bar{\mathbf{y}}_{i}}{\mathrm{d}\mathbf{A}^{(i)}}.$
    |  | (5) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\mathrm{d}L}{\mathrm{d}\mathbf{A}^{(i)}}=\frac{\mathrm{d}L}{\mathrm{d}\bar{\mathbf{y}}_{i}}\cdot\frac{\mathrm{d}\bar{\mathbf{y}}_{i}}{\mathrm{d}\mathbf{A}^{(i)}}.$
    |  | (5) |'
- en: Because of the additive structure of the weights $\mathbf{W}^{(i)}_{l}$. The
    second term can be computed via the chain rule of calculus as
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于权重 $\mathbf{W}^{(i)}_{l}$ 的加法结构。第二项可以通过微积分的链式法则计算为
- en: '|  | $1$2 |  | (6) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: where ${\mathrm{d}\mathbf{y}_{i}}/{\mathrm{d}\bar{\mathbf{y}}_{i}}$. Performing
    this multiplication in a stable and efficient way is a challenge that we must
    address.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathrm{d}\mathbf{y}_{i}}/{\mathrm{d}\bar{\mathbf{y}}_{i}}$。以稳定和高效的方式执行这种乘法是我们必须解决的挑战。
- en: 3.1.2 Efficient Mixed-Precision Computation of Forward and Backward Passes
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 高效的前向和反向传递混合精度计算
- en: If we could precompute all dequantized weight matrices $(\hat{\mathbf{W}}^{(i)})^{\top}$
    from the forward pass to use them in the backward pass.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们可以预计算所有从前向传递得到的解量化权重矩阵$(\hat{\mathbf{W}}^{(i)})^{\top}$，以在反向传递中使用它们。
- en: Efficient Mixed Precision Computation.
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高效混合精度计算。
- en: Our strategy is to *recompute* the high-precision materialization $\hat{\mathbf{W}}^{(i)}$
    and performs multiplication. Similarly, `backward} re-dequantizes $\tbWi$ and
    computes the gradient derived in Appendix \ref`sec:bwd_pass via dynamic programming.
    The
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的策略是*重新计算*高精度材料化$\hat{\mathbf{W}}^{(i)}$并进行乘法运算。类似地，`backward`重新解量化$\tbWi$并通过动态编程计算在附录\ref`sec:bwd_pass中推导出的梯度。
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'pythonLPLoRA module is small: all the intermediates are either the same size
    as the input $x$'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: pythonLPLoRA模块很小：所有中间值的大小与输入$x$相同
- en: '3.2 LLMTools: A Library for Efficient LLM Finetuning Using ModuLoRA.'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 LLMTools：一个使用ModuLoRA进行高效LLM微调的库
- en: 'We implement ModuLoRA as part of LLMTools, a user friendly library that enables
    users to interact with the largest LLMs on consumer hardware. The LLMTools library
    enables finetuning LLMs in 3-bit and 4-bit precision using the ModuLoRA algorithm.
    It also provides an easy-to-use Python API for quantization, inference, and finetuning,
    as well as modular support for multiple quantizers, LLMs (including LLAMA1, LLAMA2,
    BLOOM, and OPT), and optimization algorithms (including all that are compatible
    with the Hugging Face Trainer class). Lastly, LLMTools supports easily loading
    datasets and sharing models via the HuggingFace Hub. Our code is available at:
    [https://github.com/kuleshov-group/llmtools](https://github.com/kuleshov-group/llmtools);
    our evaluation code to reproduce our results is available at: [https://github.com/kuleshov-group/MODULoRA-Experiment](https://github.com/kuleshov-group/MODULoRA-Experiment).
    A key quantization algorithm implemented in LLMTools is OPTQ (Frantar et al.,
    [2023](#bib.bib12)). In order to integrate OPTQ with LoRA-based finetuning, LLMTools provides
    efficient CUDA implementations of mixed-precision matrix-vector multiplication,
    including row and weight materialization. We provide CUDA kernels for both row
    and weight materialization in both the forward and backward passes. For maximum
    efficiency, we materialize elements of $\hat{\mathbf{W}}^{(i)}_{q}$ all stored
    as float16.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了ModuLoRA作为LLMTools的一部分，这是一个用户友好的库，允许用户在消费级硬件上与最大的LLM进行交互。LLMTools库支持使用ModuLoRA算法在3-bit和4-bit精度下对LLM进行微调。它还提供了一个易于使用的Python
    API用于量化、推理和微调，以及对多个量化器、LLM（包括LLAMA1、LLAMA2、BLOOM和OPT）和优化算法（包括所有与Hugging Face Trainer类兼容的算法）的模块化支持。最后，LLMTools支持通过HuggingFace
    Hub轻松加载数据集和共享模型。我们的代码可以在：[https://github.com/kuleshov-group/llmtools](https://github.com/kuleshov-group/llmtools)找到；我们的评估代码以重现结果可以在：[https://github.com/kuleshov-group/MODULoRA-Experiment](https://github.com/kuleshov-group/MODULoRA-Experiment)找到。LLMTools中实现的一个关键量化算法是OPTQ（Frantar等，[2023](#bib.bib12)）。为了将OPTQ与基于LoRA的微调集成，LLMTools提供了高效的CUDA实现混合精度矩阵-向量乘法，包括行和权重材料化。我们在前向和反向传递中都提供了行和权重材料化的CUDA内核。为了实现最大效率，我们将所有存储为float16的元素$\hat{\mathbf{W}}^{(i)}_{q}$进行材料化。
- en: 4 Experiments
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: 4.1 Setup
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 设置
- en: 'Models.   We evaluate ModuLoRA and LLMTools on the recent LLAMA (Touvron et al.,
    [2023](#bib.bib35)) family of models, as well as open-source BLOOM (Scao et al.,
    [2023](#bib.bib31)) and OPT models (Zhang et al., [2022](#bib.bib42)). We quantize
    the models to 3 bits and 4 bits using OPTQ as in Frantar et al. ([2023](#bib.bib12))
    with calibration 128 samples from C4 (Raffel et al., [2020](#bib.bib29)). Baseline.  
    We use LoRA (as implemented in the PEFT library (Mangrulkar et al., [2022](#bib.bib25)))
    to finetune models quantized in 8 bits using the BitsAndBytes library (Dettmers
    et al., [2022](#bib.bib7)); we also compare to full-precision results from the
    literature. In concurrent work, Dettmers et al. ([2023](#bib.bib8)) proposed QLoRA,
    a related 4-bit finetuning algorithm implemented in the BitsAndBytes library.
    Accordingly, we present an experimental comparison of QLoRA with our approach,
    along with an in-depth discussion. Training.   We finetune all models for $3$,
    and report results from 3 random seeds. See Appendix [D](#A4 "Appendix D Hyperparamters
    Used in Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating
    with Modular Quantizers") for details on the hyperparameters used for each of
    our experiment.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '模型。我们在最近的LLAMA（Touvron et al., [2023](#bib.bib35)）模型系列以及开源BLOOM（Scao et al.,
    [2023](#bib.bib31)）和OPT模型（Zhang et al., [2022](#bib.bib42)）上评估ModuLoRA和LLMTools。我们使用OPTQ将模型量化为3-bit和4-bit，如Frantar
    et al. ([2023](#bib.bib12))所述，校准样本为C4（Raffel et al., [2020](#bib.bib29)）。基准。我们使用LoRA（如PEFT库中实现的（Mangrulkar
    et al., [2022](#bib.bib25)））来微调8-bit量化的模型，使用BitsAndBytes库（Dettmers et al., [2022](#bib.bib7)）；我们还与文献中的全精度结果进行比较。在相关工作中，Dettmers
    et al. ([2023](#bib.bib8))提出了QLoRA，这是一种在BitsAndBytes库中实现的相关4-bit微调算法。因此，我们提供了QLoRA与我们方法的实验比较，并进行了深入讨论。训练。我们对所有模型进行微调$3$，并报告了来自3个随机种子的结果。有关每个实验使用的超参数的详细信息，请参见附录
    [D](#A4 "Appendix D Hyperparamters Used in Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers")。'
- en: 4.2 Text Classification
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 文本分类
- en: Data & Metrics.   We start with a simple text classification task where we seek
    to classify a short text snippet (up to 50 words) into its genre (e.g., fiction,
    telephone chat, etc.). We finetune 13B to 65B LLAMA models on 392,702 snippets
    from five genres and evaluate on 9,815 held out instances (Williams et al., [2018](#bib.bib36)),
    reporting accuracy. This yields a challenging classification task for LLMs of
    all sizes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 数据与指标。我们从一个简单的文本分类任务开始，试图将一小段文本（最多50个单词）分类到其类型（例如，小说、电话聊天等）。我们在来自五个类别的392,702个片段上微调13B到65B的LLAMA模型，并在9,815个保留实例上进行评估（Williams
    et al., [2018](#bib.bib36)），报告准确率。这对所有规模的LLM都是一个具有挑战性的分类任务。
- en: '| LLAMA Tuning | 13B | 30B | 65B |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA 调优 | 13B | 30B | 65B |'
- en: '| --- | --- | --- | --- |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LLMTools (3-bit) | 93.5 $\pm$ 0.8 |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| LLMTools (3-bit) | 93.5 $\pm$ 0.8 |'
- en: '| LLMTools (4-bit) | 92.9 $\pm$ 0.9 |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| LLMTools (4-bit) | 92.9 $\pm$ 0.9 |'
- en: '| Bits&Bytes 8-bit (LLM.int8()) | 93.0 $\pm$ 1.0 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| Bits&Bytes 8-bit (LLM.int8()) | 93.0 $\pm$ 1.0 |'
- en: 'Table 1: Text classification accuracy (%) for LLAMAs finetuned with LoRA &
    ModuLoRA in 3, 4, 8 bits.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LLAMA模型在3-bit、4-bit、8-bit条件下通过LoRA和ModuLoRA微调后的文本分类准确率（%）。
- en: 'Results.   We observe that classification accuracy consistently improves as
    we increase the number of parameters of the LLM. ModuLoRA combined with a 3-bit
    or a 4-bit LLM offers comparable performance to 8-bit finetuning in Bits&Bytes
    while using significantly less memory (Table [1](#S4.T1 "Table 1 ‣ 4.2 Text Classification
    ‣ 4 Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating
    with Modular Quantizers")).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '结果。我们观察到，随着LLM参数数量的增加，分类准确率不断提高。ModuLoRA结合3-bit或4-bit LLM提供的性能与Bits&Bytes中的8-bit微调相当，同时使用了显著更少的内存（见表格
    [1](#S4.T1 "Table 1 ‣ 4.2 Text Classification ‣ 4 Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers")）。'
- en: 4.3 Natural Language Inference
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 自然语言推理
- en: Data & Metrics.   Next, we finetune LLMs on natural language inference tasks.
    The model is asked to predict a label from a small set (entailment, contradiction,
    or neutral) after being presented with a sentence pairing (a hypothesis and premise
    sentence pair). We finetune 7B to 65B LLAMA models on the Multi-Genre Natural
    Language Inference Corpus (MNLI)  (Williams et al., [2018](#bib.bib36)) and evaluate
    on the matched test sets (in-domain examples), reporting accuracy. Baselines from
    GPT-3 and T5 are included, as presented in Hu et al. (2022) and  Chung et al.
    ([2022](#bib.bib6)). Results.   Our 3-bit 65B LLAMA model matches the performance
    of a full-precision GPT-3+LoRA baseline. We also find that 3-bit and 4-bit models
    from LLMTools outperform 8-bit models from the Bits&Bytes library for the entire
    model size range. Both 3-bit and 4-bit ModuLoRA models either match or outperform
    their 4-bit QLoRA counterparts, often using less memory.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 数据与指标。   接下来，我们在自然语言推理任务上微调LLMs。模型在呈现句子对（假设和前提句对）后，被要求从一个小的集合中预测一个标签（蕴含、矛盾或中立）。我们在Multi-Genre自然语言推理语料库（MNLI）上微调了7B到65B的LLAMA模型
    (Williams et al., [2018](#bib.bib36)) 并在匹配的测试集（领域内示例）上进行评估，报告准确率。包括了来自GPT-3和T5的基准，参见Hu
    et al. (2022)和Chung et al. ([2022](#bib.bib6))。结果。   我们的3-bit 65B LLAMA模型的表现与全精度GPT-3+LoRA基准相匹配。我们还发现，来自LLMTools的3-bit和4-bit模型在整个模型大小范围内优于Bits&Bytes库中的8-bit模型。3-bit和4-bit
    ModuLoRA模型要么与4-bit QLoRA模型匹配，要么优于它们，通常使用更少的内存。
- en: 'Baselines Models Finetuning Adaptation Model Size # Trainable Parameters MNLI-m
    (accuracy) GPT-3 Full Finetuning 175B 175,255.8M 89.5 $\pm$ 0.1'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '基准模型微调适配 模型大小 # 可训练参数 MNLI-m（准确率） GPT-3 完全微调 175B 175,255.8M 89.5 $\pm$ 0.1'
- en: 'Table 2: Natural language inference on the MNLI-m dataset evaluated using classification
    accuracy (%). Our LLAMA-65B-3bit model approaches state-of-the-art scores using
    significantly less memory.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：在MNLI-m数据集上的自然语言推理，使用分类准确率（%）进行评估。我们的LLAMA-65B-3bit模型在显著减少内存使用的情况下接近最先进的得分。
- en: 4.4 Abstractive Summarization
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 抽象摘要
- en: 'Data & Metrics.   We finetune 7B-65B LLAMA and 7B-13B OPT models on the SAMSum
    dataset (Gliwa et al., [2019](#bib.bib13)), consisting of 14,732 (text, summary)
    training pairs and 819 test pairs. Our methodology fully mirrors the evaluation
    of GPT-style models finetuned using LoRA (Hu et al., [2022](#bib.bib15)). We evaluate
    summarization quality using ROUGE-1/2/L; we include GPT-3 baselines from Hu et al.
    ([2022](#bib.bib15)). Results.   Our 4-bit 65B LLAMA models finetuned with ModuLoRA outperform
    the GPT-3 baseline and even reach new state-of-the-art performance on this dataset
    (Table [3](#S4.T3 "Table 3 ‣ 4.4 Abstractive Summarization ‣ 4 Experiments ‣ ModuLoRA:
    Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers")).
    Importantly, ModuLoRA demonstrates performance improvements over the 4-bit QLoRA
    and the 8-bit BitsAndBytes methods. In the 7B to 65B model size range, ModuLoRA models
    (3-bit or 4-bit) outperform 8-bit LoRAs in BitsAndBytes and LLM.int8() and 4-bit
    LoRAs in BitsAndBytes and QLoRA. We argue that a data-driven lower precision quantization
    scheme can improve over a higher precision zero-shot quantizer like LLM.int8().
    Switching from 4-bit to 3-bit precision within ModuLoRA reduces ROUGE by only
    about 1%.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '数据与指标。   我们在SAMSum数据集 (Gliwa et al., [2019](#bib.bib13)) 上微调了7B-65B LLAMA和7B-13B
    OPT模型，该数据集包含14,732个（文本，摘要）训练对和819个测试对。我们的方法完全镜像了使用LoRA微调的GPT风格模型的评估 (Hu et al.,
    [2022](#bib.bib15))。我们使用ROUGE-1/2/L评估摘要质量；包括了来自Hu et al. ([2022](#bib.bib15))的GPT-3基准。结果。  
    我们使用ModuLoRA微调的4-bit 65B LLAMA模型优于GPT-3基准，甚至在此数据集上达到了新的最先进性能（表[3](#S4.T3 "Table
    3 ‣ 4.4 Abstractive Summarization ‣ 4 Experiments ‣ ModuLoRA: Finetuning 3-Bit
    LLMs on Consumer GPUs by Integrating with Modular Quantizers")）。重要的是，ModuLoRA在4-bit
    QLoRA和8-bit BitsAndBytes方法上显示出性能提升。在7B到65B模型大小范围内，ModuLoRA模型（3-bit或4-bit）优于BitsAndBytes和LLM.int8()中的8-bit
    LoRAs及BitsAndBytes和QLoRA中的4-bit LoRAs。我们认为，数据驱动的低精度量化方案可以优于像LLM.int8()这样的高精度零样本量化器。在ModuLoRA中，从4-bit精度切换到3-bit精度ROUGE仅减少了约1%。'
- en: 'Baselines Models Finetuning Adaptation # Trainable Parameters SAMSum (Rouge
    1/2/L) GPT-3 Full Finetuning 175,255.8M 52.0 / 28.0 / 44.5 GPT-3 Adapter 40.1M
    53.2 / 29.0 / 45.1 GPT-3 LoRA 4.7M 53.8 / 29.8 / 45.9 Pegasus SliC 2B 54.4 / 29.9
    / 45.9'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '基准模型微调适配 # 可训练参数 SAMSum（Rouge 1/2/L） GPT-3 完全微调 175,255.8M 52.0 / 28.0 / 44.5
    GPT-3 适配器 40.1M 53.2 / 29.0 / 45.1 GPT-3 LoRA 4.7M 53.8 / 29.8 / 45.9 Pegasus
    SliC 2B 54.4 / 29.9 / 45.9'
- en: LLAMA Finetuning 7B 13B 30B 65B LLMTools (3-bit) 51.2 / 28.2 / 44.0 52.4 / 29.6
    / 45.1 53.6 / 30.8 / 46.3 54.1 / 30.9 / 46.5 LLMTools (4-bit) 51.7 / 28.3 / 44.4
    53.2 / 30.2 / 46.1 53.9 / 31.2 / 46.9 55.9 / 32.7 / 49.0 Bits&Bytes 4-bit (QLoRA)
    51.6 / 28.3 / 44.5 51.3 / 28.1 / 44.1 53.0 / 30.2 / 45.7 53.8 / 30.5 / 45.9 Bits&Bytes 8-bit
    (LLM.int8()) 51.9 / 28.1 / 44.5 51.3 / 28.2 / 43.6 50.8 / 28.4 / 44.1 53.9 / 30.4
    / 46.3
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: LLAMA微调 7B 13B 30B 65B LLMTools (3-bit) 51.2 / 28.2 / 44.0 52.4 / 29.6 / 45.1
    53.6 / 30.8 / 46.3 54.1 / 30.9 / 46.5 LLMTools (4-bit) 51.7 / 28.3 / 44.4 53.2
    / 30.2 / 46.1 53.9 / 31.2 / 46.9 55.9 / 32.7 / 49.0 Bits&Bytes 4-bit (QLoRA) 51.6
    / 28.3 / 44.5 51.3 / 28.1 / 44.1 53.0 / 30.2 / 45.7 53.8 / 30.5 / 45.9 Bits&Bytes
    8-bit (LLM.int8()) 51.9 / 28.1 / 44.5 51.3 / 28.2 / 43.6 50.8 / 28.4 / 44.1 53.9
    / 30.4 / 46.3
- en: 'Table 3: Abstractive summarization on the SAMSum dataset evaluated using ROUGE
    1/2/L. Our LLAMA-65B-3bit model obtains state-of-the-art ROUGE 1/2 scores. All
    metrics have $\pm 0.5$ confidence intervals.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：在SAMSum数据集上的抽象总结，使用ROUGE 1/2/L进行评估。我们的LLAMA-65B-3bit模型获得了最先进的ROUGE 1/2评分。所有指标的置信区间为$\pm
    0.5$。
- en: Round-to-Nearest Quantization
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 最接近量化
- en: 'We also perform an ablation where we replace the OPTQ quantizer with a rount-to-nearest
    (RTN) approach (Table [4](#S4.T4 "Table 4 ‣ Other Model Families ‣ 4.4 Abstractive
    Summarization ‣ 4 Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs
    by Integrating with Modular Quantizers")); OPTQ performs better than RTN, highlighting
    the importance of advanced quantizers.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还进行了一个消融实验，将OPTQ量化器替换为最接近值（RTN）方法（表[4](#S4.T4 "Table 4 ‣ Other Model Families
    ‣ 4.4 Abstractive Summarization ‣ 4 Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs
    on Consumer GPUs by Integrating with Modular Quantizers")）；OPTQ的表现优于RTN，突显了先进量化器的重要性。'
- en: Other Model Families
  id: totrans-89
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 其他模型家族
- en: 'We also apply LLMTools to the OPT (Zhang et al., [2022](#bib.bib42)) families
    of models (Table [5](#S4.T5 "Table 5 ‣ Other Model Families ‣ 4.4 Abstractive
    Summarization ‣ 4 Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs
    by Integrating with Modular Quantizers")). Although these models perform worse
    than LLAMA, ModuLoRA matches or outperforms more memory-intensive 4-bit and 8-bit
    finetuning, which is consistent with our results on LLAMA.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '我们还将LLMTools应用于OPT（Zhang et al., [2022](#bib.bib42)）模型家族（表[5](#S4.T5 "Table
    5 ‣ Other Model Families ‣ 4.4 Abstractive Summarization ‣ 4 Experiments ‣ ModuLoRA:
    Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers")）。虽然这些模型的表现不如LLAMA，但ModuLoRA在更多内存消耗的4-bit和8-bit微调中匹配或超越了这些模型，这与我们在LLAMA上的结果一致。'
- en: SAMSum Performance Quantizer 7B 13B LLMTools (3-bit) OPTQ 51.2 / 28.2 / 44.0 / 44.2
    52.4 / 29.6 / 45.1 / 45.1 RTN 50.7 / 27.2 / 43.6 / 43.6 51.1 / 28.7 / 44.3 / 44.5
    LLMTools (4-bit) OPTQ 51.7 / 28.3 / 44.4 / 44.4 53.2 / 30.2 / 46.1 / 46.1 RTN
    51.2 / 28.5 / 44.2 / 44.2 52.5 / 29.9 / 45.5 / 45.5
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SAMSum性能 量化器 7B 13B LLMTools (3-bit) OPTQ 51.2 / 28.2 / 44.0 / 44.2 52.4 / 29.6
    / 45.1 / 45.1 RTN 50.7 / 27.2 / 43.6 / 43.6 51.1 / 28.7 / 44.3 / 44.5 LLMTools
    (4-bit) OPTQ 51.7 / 28.3 / 44.4 / 44.4 53.2 / 30.2 / 46.1 / 46.1 RTN 51.2 / 28.5
    / 44.2 / 44.2 52.5 / 29.9 / 45.5 / 45.5
- en: 'Table 4: OPTQ and RTN quantization with different LLaMA model sizes on the
    SAMSum dataset. The evaluation was done on ROUGE 1/2/L/LSum.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：在SAMSum数据集上使用不同LLaMA模型大小的OPTQ和RTN量化。评估是在ROUGE 1/2/L/LSum上进行的。
- en: OPT Finetuning 13B 30B LLMTools (3-bit) 48.8 / 26.7 / 41.9 49.9 / 27.1 / 42.5
    LLMTools (4-bit) 49.3 / 26.8 / 42.0 49.6 / 27.1 / 42.4 Bits&Bytes 4-bit (QLoRA)
    49.2 / 27.0 / 42.1 49.9 / 27.0 / 42.5 Bits&Bytes 8-bit (LLM.int8()) 48.8 / 26.5
    / 41.7 49.3 / 27.1 / 42.3
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: OPT微调 13B 30B LLMTools (3-bit) 48.8 / 26.7 / 41.9 49.9 / 27.1 / 42.5 LLMTools
    (4-bit) 49.3 / 26.8 / 42.0 49.6 / 27.1 / 42.4 Bits&Bytes 4-bit (QLoRA) 49.2 /
    27.0 / 42.1 49.9 / 27.0 / 42.5 Bits&Bytes 8-bit (LLM.int8()) 48.8 / 26.5 / 41.7
    49.3 / 27.1 / 42.3
- en: 'Table 5: Abstractive summarization with OPT models on the SAMSum dataset. ModuLoRA in
    3-bit and 4-bit precision matches ROUGE 1/2/L scores of 4-bit and 8-bit baselines.
    All metrics have $\pm 0.5$ confidence intervals.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：在SAMSum数据集上使用OPT模型进行的抽象总结。ModuLoRA在3-bit和4-bit精度下匹配4-bit和8-bit基准的ROUGE 1/2/L评分。所有指标的置信区间为$\pm
    0.5$。
- en: 4.5 Instruction Following
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 指令跟随
- en: 'Data & Metrics.   We finetune 7B-65B LLAMA models on the Alpaca dataset (Taori
    et al., [2023](#bib.bib34)), consisting 52,000 instructions, as well on the CodaAlpaca
    dataset (Chaudhary, [2023](#bib.bib3)), consisting of 20K code generation instructions
    (ses [8](#A3.T8 "In C.1 Additional Experiments on Code-Alpaca with LLaMA ‣ Appendix
    C Additional Empirical Experiments ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer
    GPUs by Integrating with Modular Quantizers")). We evaluate our Alpaca instruction-tuned
    models on the BigBenchHard (BBH) benchmark (Suzgun et al., [2022](#bib.bib33)),
    consisting of 23 challenging tasks on which LLMs do not exceed human performance.
    We evaluate 3-shot performance via "answer-only" prompting and use exact match
    accuracy as our measurement standard, testing on 6,511 samples ($\sim$ 1.5k tokens
    each). We include Flan and LLAMA baselines from Chia et al. ([2023](#bib.bib5)).Results.  
    We find that 3-bit and 4-bit performance drops only slightly relative to 8-bit
    and 16-bit. Crucially, 4-bit and 3-bit 65B models outperform 8-bit Beand 16-bit
    30B models, despite using fewer total bits. Furthermore, 4-bit ModuLoRA compares
    well to 4-bit QLoRA, and provides consistent performance improvements, especially
    at smaller model sizes, where sophisticated quantization ought to provide greater
    benefits. This further highlights the benefits of one-shot quantization methods.
    Appendix [C](#A3 "Appendix C Additional Empirical Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers") also reports
    experiments on the CodeAlpaca dataset.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '数据与指标。我们在Alpaca数据集(Taori等，[2023](#bib.bib34))上对7B-65B LLAMA模型进行微调，该数据集包含52,000条指令，以及在CodaAlpaca数据集(Chaudhary，[2023](#bib.bib3))上，该数据集包含20K条代码生成指令（参见[8](#A3.T8
    "在C.1附加实验中对Code-Alpaca进行的实验 ‣ 附录C附加实证实验 ‣ ModuLoRA: 通过与模块化量化器集成来微调3位LLM")）。我们在BigBenchHard
    (BBH)基准(Suzgun等，[2022](#bib.bib33))上评估了我们的Alpaca指令微调模型，该基准包含23个具有挑战性的任务，LLMs在这些任务中的表现未超过人类表现。我们通过“仅答题”提示评估3-shot性能，并使用精确匹配准确度作为我们的测量标准，测试了6,511个样本（$\sim$
    1.5k tokens每个）。我们包括了Chia等([2023](#bib.bib5))的Flan和LLAMA基线。结果。我们发现，相对于8-bit和16-bit，3-bit和4-bit性能仅稍微下降。关键是，尽管使用了更少的总位数，4-bit和3-bit
    65B模型仍优于8-bit和16-bit 30B模型。此外，4-bit ModuLoRA与4-bit QLoRA相比表现良好，并且提供了一致的性能提升，尤其是在较小的模型尺寸中，其中精细量化应该带来更大的好处。这进一步突出了单次量化方法的优势。附录[C](#A3
    "附录C附加实证实验 ‣ ModuLoRA: 通过与模块化量化器集成来微调3位LLM")还报告了对CodeAlpaca数据集的实验。'
- en: Baselines Model Method BASE (250M) L (780M) XL (3B) XXL (11B) FLAN-T5 No Finetuning
    30.8 30.3 39.9 47.4
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基线模型 方法 BASE (250M) L (780M) XL (3B) XXL (11B) FLAN-T5 无微调 30.8 30.3 39.9 47.4
- en: Model Methods 7B 13B 30B 65B LLAMA LLMTools (3-bit) 31.1 $\pm$ 0.4 No Finetuning
    30.9 37.1 39.3 42.6
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 模型方法 7B 13B 30B 65B LLAMA LLMTools (3-bit) 31.1 $\pm$ 0.4 无微调 30.9 37.1 39.3
    42.6
- en: 'Table 6: Instruction-tuned models evaluated on BigBench Hard (BBH). We finetune
    LLAMA models on the Alpaca dataset in 3 to 16 bits. We provide exact standard
    deviation here.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 表6：在BigBench Hard (BBH)上评估的指令微调模型。我们在Alpaca数据集上对LLAMA模型进行了3至16位的微调。这里提供了精确的标准差。
- en: 5 Discussion
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 讨论
- en: 5.1 Comparison to Related Work
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 与相关工作的比较
- en: Comparison to QLoRA
  id: totrans-102
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与QLoRA的比较
- en: In concurrent work, Dettmers et al. ([2023](#bib.bib8)) proposed QLoRA, a related
    approach for finetuning a quantized LLM. We highlight methodological and experimental
    differences below. From a methods perspective, ModuLoRA integrates with a user-specified
    black-box quantization module. In our experiments, we find that using a sophisticated
    data-driven quantizer like OPTQ improves performance over simpler zero-shot strategies,
    e.g., a round-to-nearest baseline. Unlike ModuLoRA, QLoRA defines a quantization
    approach similar to RTN, but also introduces a specialized packing routine, quantization
    of zeros and scales, and other innovations. From an experiments and capabilities
    perspective, integrating with OPTQ enables ModuLoRA to fintune models quantized
    in 3-bits, which QLoRA cannot do. We anticipate ModuLoRA will enable finetuning
    2-bit LLMs by integrating with new quantizers, such as Chee et al. ([2023](#bib.bib4)).
    Lastly, we identify settings where ModuLoRA yields LLMs with better performance
    than LLMs from QLoRA; this gap is likely due to the use of improved quantizers.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在并行工作中，Dettmers等人 ([2023](#bib.bib8)) 提出了QLoRA，这是一种用于微调量化LLM的相关方法。我们在下面突出了方法论和实验上的不同。从方法的角度来看，ModuLoRA
    与用户指定的黑箱量化模块集成。在我们的实验中，我们发现使用像OPTQ这样的复杂数据驱动量化器相比于更简单的零样本策略（如四舍五入基线）可以提升性能。与ModuLoRA不同，QLoRA定义了一种类似于RTN的量化方法，但还引入了专业的打包例程、零和尺度的量化以及其他创新。从实验和能力的角度来看，集成OPTQ使ModuLoRA能够微调3位量化的模型，而QLoRA无法做到这一点。我们预计ModuLoRA通过与新的量化器（如Chee等人
    ([2023](#bib.bib4))）集成，将能够微调2位LLM。最后，我们确定了ModuLoRA在某些设置下比QLoRA产生的LLM性能更好的情况；这一差距可能由于使用了改进的量化器。
- en: Comparison to Other Parameter-Efficient Finetuning Methods
  id: totrans-104
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 与其他参数高效微调方法的比较
- en: Recent Parameter-Efficient Finetuning (PEFT) methods have encompassed a range
    of techniques such as prompt tuning (Lester et al., [2021](#bib.bib18); Li & Liang,
    [2021](#bib.bib19); Qin & Eisner, [2021](#bib.bib28); Liu et al., [2022b](#bib.bib23)),
    modification of the embedding layer inputs (An et al., [2022](#bib.bib1)) or hidden
    states (Liu et al., [2022a](#bib.bib21)), inclusion of full layers (Houlsby et al.,
    [2019](#bib.bib14)), only tuning biases (Zaken et al., [2021](#bib.bib41)), and
    others (Sung et al., [2021](#bib.bib32); Karimi Mahabadi et al., [2021](#bib.bib17)).
    An important shortcoming of these methods is the need to store in memory a significant
    amount of frozen base model parameters. This limits their ability to finetune
    the largest LLMs on consumer GPU, a limitation that we address.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的参数高效微调（PEFT）方法涵盖了一系列技术，如提示微调（Lester等人，[2021](#bib.bib18)；Li & Liang，[2021](#bib.bib19)；Qin
    & Eisner，[2021](#bib.bib28)；Liu等人，[2022b](#bib.bib23)），嵌入层输入的修改（An等人，[2022](#bib.bib1)）或隐藏状态（Liu等人，[2022a](#bib.bib21)），全层的包含（Houlsby等人，[2019](#bib.bib14)），仅调整偏差（Zaken等人，[2021](#bib.bib41)），以及其他（Sung等人，[2021](#bib.bib32)；Karimi
    Mahabadi等人，[2021](#bib.bib17)）。这些方法的一个重要缺点是需要在内存中存储大量的冻结基模型参数。这限制了它们在消费级GPU上微调最大LLM的能力，而这是我们所解决的限制。
- en: 5.2 Running LLMs on Consumer GPUs
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 在消费级GPU上运行LLM
- en: Efficient LLM Algorithms
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 高效的LLM算法
- en: The computational requirements of modern deep neural networks motivate a wide
    range of efficient machine learning algorithms. Quantization methods reduce the
    number of bits required to store weights (Dong et al., [2019](#bib.bib10); [2020](#bib.bib11);
    Hubara et al., [2021](#bib.bib16); Li et al., [2021](#bib.bib20); Yao et al.,
    [2021](#bib.bib38)), including via adaptive methods (Nagel et al., [2020](#bib.bib26)).
    SmoothQuant (Xiao et al., [2023](#bib.bib37)) rescales between activations and
    weights to remove outliers from the activations and make quantization overall
    easier. ZeroQuant (Yao et al., [2022](#bib.bib39)) proposes a per-layer knowledge
    distillation method. LLM.int8() (Dettmers et al., [2022](#bib.bib7)) decompose
    matrix multiplications into a majority of 8 bit and a minority of 16 bit operations.
    LUT-GEMM (Park et al., [2023](#bib.bib27)) designs kernels to accelerate quantized
    matrix multiplications. RPTQ (Yuan et al., [2023](#bib.bib40)) reorders activations
    and quantizes them in groups, reducing the impact of range differences between
    channels.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现代深度神经网络的计算需求推动了多种高效的机器学习算法的发展。量化方法减少了存储权重所需的比特数（Dong 等，[2019](#bib.bib10)；[2020](#bib.bib11)；Hubara
    等，[2021](#bib.bib16)；Li 等，[2021](#bib.bib20)；Yao 等，[2021](#bib.bib38)），包括通过自适应方法（Nagel
    等，[2020](#bib.bib26)）。SmoothQuant（Xiao 等，[2023](#bib.bib37)）通过在激活值和权重之间重新缩放，去除激活值中的离群值，使量化整体上更容易。ZeroQuant（Yao
    等，[2022](#bib.bib39)）提出了一种按层知识蒸馏方法。LLM.int8()（Dettmers 等，[2022](#bib.bib7)）将矩阵乘法分解为大多数8位和少数16位操作。LUT-GEMM（Park
    等，[2023](#bib.bib27)）设计了加速量化矩阵乘法的内核。RPTQ（Yuan 等，[2023](#bib.bib40)）重新排列激活值并将其分组量化，减少了通道之间范围差异的影响。
- en: Running LLMs on Consumer GPUs
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在消费级GPU上运行LLM
- en: Our 3-bit and 4-bit methods enable finetuning a 65B LLM on on 48GB GPU and a
    30B LLM on on 24GB GPU, bringing LLM finetuning to consumer hardware. Moreover,
    fitting an entire LLM on GPU unlocks data parallelism, which is more efficient
    than model parallelism. Previous 8-bit quantization methods required a 96GB GPU
    to fully fit a 65B model. Finetuning GPUs on consumer hardware holds promise to
    accelerate model iteration and apply LLMs to a wider range of domains by a larger
    number of practitioners.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的3位和4位方法使得在48GB GPU上对65B LLM进行微调，以及在24GB GPU上对30B LLM进行微调成为可能，将LLM微调带到了消费级硬件上。此外，将整个LLM放入GPU中可以解锁数据并行性，这比模型并行性更高效。以前的8位量化方法需要96GB
    GPU才能完全适配65B模型。在消费级硬件上微调GPU有望加速模型迭代，并使LLM能够应用于更多领域，服务更多的从业者。
- en: 5.3 What is a Good Base LLM for Finetuning?
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 适合微调的基础LLM是什么？
- en: '| Models | Quantization | BBH | PPL |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 量化 | BBH | PPL |'
- en: '| --- | --- | --- | --- |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LLAMA (13B) | 3-bit | 35.3 | 6.63 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA (13B) | 3-bit | 35.3 | 6.63 |'
- en: '| 4-bit | 36.2 | 5.36 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 4-bit | 36.2 | 5.36 |'
- en: '| LLAMA (65B) | 3-bit | 43.3 | 5.04 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| LLAMA (65B) | 3-bit | 43.3 | 5.04 |'
- en: '| 4-bit | 43.7 | 3.84 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 4-bit | 43.7 | 3.84 |'
- en: 'Table 7: BBH vs. PPL'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '表 7: BBH 与 PPL 的对比'
- en: 'The traditional measure of a base LLM is perplexity. In the adjacent table,
    we report LLAMA perplexity (PPL) on Wiki2 as well as finetuning performance on
    BBH. Interestingly, the correlation is not perfect: large gaps in PPL admit small
    gaps in BBH. This questions LLM evaluation when the goal is finetuning, and suggests
    exploring new training strategies. More generally, our results provide empirical
    evidence that high performance on downstream tasks can be achieved with a smaller
    quantized LLM than previously thought. While existing methods (e.g., LLM.int8()+LoRA;
    Dettmers et al. ([2022](#bib.bib7))) operate in 8 bits, we find that 3-bit or
    4-bit finetuning yields the best results for a fixed bit budget. For example,
    we find that 4-bit and 3-bit 65B models outperform 8-bit and 16-bit 30B models
    on instruction following tasks. Similarly, we find that 3-bit models are able
    to attain a new state-of-the-art ROUGE score on the SamSum summarization task.
    The high performance of these models hints at the possibility of further pursuing
    2-bit models.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的基础LLM评估指标是困惑度。在相邻的表格中，我们报告了LLAMA在Wiki2上的困惑度（PPL）以及在BBH上的微调性能。有趣的是，这种相关性并不完全：PPL中的大差距在BBH中会有小差距。这对以微调为目标的LLM评估提出了质疑，并建议探索新的训练策略。更一般地说，我们的结果提供了实证证据，表明在下游任务上实现高性能可以使用比以前认为的更小的量化LLM。尽管现有方法（例如，LLM.int8()+LoRA；Dettmers
    等（[2022](#bib.bib7)））在8位中运行，但我们发现3位或4位微调在固定比特预算下能获得最佳结果。例如，我们发现4位和3位的65B模型在指令跟随任务上优于8位和16位的30B模型。类似地，我们发现3位模型能够在SamSum总结任务上达到新的ROUGE最佳成绩。这些模型的高性能暗示了进一步追求2位模型的可能性。
- en: 5.4 Limitations
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 限制
- en: 'An advantage of LoRA is that it has low inference overhead, since the low-rank
    adaptor can be added in to the full-precision weight matrix when deploying. One
    limitation of ModuLoRA is that it does not share this advantage relative to the
    black-box quantized model: the low-rank adaptor cannot be trivially added to the
    weight matrix because the weight matrix is quantized while the adaptor is not.
    So, the weight matrix and adaptor cannot be fused readily, and an implementation
    as in Figure [1](#S3.F1 "Figure 1 ‣ 3 Low-Precision Low-Rank Adaptation with a
    Modular Quantizer ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating
    with Modular Quantizers") is required at inference time. A second limitation of
    ModuLoRA is that making finetuning possible on widely available commodity hardware
    may make finetuning too easy, presenting potential problems related to LLM safety.
    Another limitation of ModuLoRA  is that the largest models in use today (e.g.
    GPT-4) can have up to 1 trillion parameters, and even at the minimum of 1 bit
    per parameter this still would take up 125 GB, which exceeds memory on commodity
    GPUs: thus a straightforward application of ModuLoRA will be unable to make these
    largest-scale models finetunable on commodity hardware.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: LoRA 的一个优点是它具有低推理开销，因为低秩适配器可以在部署时添加到全精度权重矩阵中。ModuLoRA 的一个限制是，相对于黑箱量化模型，它没有这个优点：低秩适配器不能轻易添加到权重矩阵中，因为权重矩阵是量化的而适配器不是。因此，权重矩阵和适配器不能被直接融合，推理时需要像图
    [1](#S3.F1 "图 1 ‣ 3 低精度低秩适配与模块化量化器 ‣ ModuLoRA：通过与模块化量化器集成在消费级 GPU 上微调 3 位 LLM")
    所示的实现。ModuLoRA 的第二个限制是，使广泛可用的消费级硬件上的微调成为可能可能使微调变得过于容易，从而带来与 LLM 安全性相关的潜在问题。ModuLoRA
    的另一个限制是，当今使用的最大模型（例如 GPT-4）可能具有高达 1 万亿个参数，即使在每个参数最低为 1 位的情况下，这仍然需要 125 GB 的内存，这超出了消费级
    GPU 的内存。因此，ModuLoRA 的直接应用将无法在消费级硬件上对这些最大规模的模型进行微调。
- en: 6 Conclusion
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Finetuning large language models typically requires substantial hardware and
    storage resources. Our method, ModuLoRA, enables 3-bit finetuning of 65B models
    on a single 48GB consumer GPU. At the core of our approach is a simple, quantization-agnostic
    backward pass that enables integrating low-rank adapters with frozen LLM weights
    obtained from a user-defined quantization module. By integrating with modern quantizers,
    ModuLoRA achieves state-of-the-art performance compared to both parameter-efficient
    and full fine-tuning techniques. We anticipate MODULORA will enable finetuning
    2-bit LLMs by integrating with new quantizers, such as Chee et al. ([2023](#bib.bib4)).
    ModuLoRA’s flexibility and competitive performance make fine-tuning more accessible
    and cost-effective in a resource-constrained setting. This assists open-source
    model development and facilitates scientific research. More broadly, we believe
    that ModuLoRA will help democratize access to large language models and make them
    available to a broader audience.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对大规模语言模型的微调通常需要大量的硬件和存储资源。我们的方法 ModuLoRA 使得在单个 48GB 的消费级 GPU 上实现 65B 模型的 3 位微调成为可能。我们方法的核心是一个简单的、与量化无关的反向传播过程，它允许将低秩适配器与从用户定义的量化模块获得的冻结
    LLM 权重集成。通过与现代量化器的集成，ModuLoRA 在参数效率和全微调技术方面都达到了最先进的性能。我们预期 ModuLoRA 将通过与新型量化器（如
    Chee 等人 ([2023](#bib.bib4))）的集成，使 2 位 LLM 的微调成为可能。ModuLoRA 的灵活性和竞争力使得在资源受限的环境中微调变得更加可及且具有成本效益。这有助于开源模型的发展和科学研究的促进。更广泛地，我们相信
    ModuLoRA 将有助于使大规模语言模型的获取更加民主化，让更多的用户能够使用这些模型。
- en: References
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'An et al. (2022) Shengnan An, Yifei Li, Zeqi Lin, Qian Liu, Bei Chen, Qiang
    Fu, Weizhu Chen, Nanning Zheng, and Jian-Guang Lou. Input-tuning: Adapting unfamiliar
    inputs to frozen pretrained models. *arXiv preprint arXiv:2203.03131*, 2022.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: An 等人（2022）盛楠·安、李一飞、林泽奇、刘倩、陈贝、傅强、陈伟柱、郑南宁、楼建光。输入调优：将不熟悉的输入适配到冻结的预训练模型中。*arXiv
    预印本 arXiv:2203.03131*，2022 年。
- en: Brown et al. (2020) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D
    Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
    Askell, and et. al. Language models are few-shot learners. In *Conference on Neural
    Information Processing Systems*, 2020.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）汤姆·布朗、班杰明·曼、尼克·赖德、梅拉尼·萨比亚、贾瑞德·D·卡普兰、普拉夫拉·达里瓦尔、阿尔文德·尼拉坎坦、普拉纳夫·夏姆、吉里什·萨斯特里、阿曼达·阿斯克尔
    等人。语言模型是少样本学习者。发表于 *神经信息处理系统会议*，2020 年。
- en: 'Chaudhary (2023) Sahil Chaudhary. Code alpaca: An instruction-following llama
    model for code generation. [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca),
    2023.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chaudhary (2023) Sahil Chaudhary. Code alpaca: 一种用于代码生成的指令跟随 llama 模型。 [https://github.com/sahil280114/codealpaca](https://github.com/sahil280114/codealpaca)，2023。'
- en: 'Chee et al. (2023) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De
    Sa. Quip: 2-bit quantization of large language models with guarantees, 2023.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chee 等 (2023) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov 和 Christopher De Sa.
    Quip: 具有保障的大型语言模型的 2 位量化, 2023。'
- en: 'Chia et al. (2023) Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria.
    Instructeval: Towards holistic evaluation of instruction-tuned large language
    models. *arXiv preprint arXiv:2306.04757*, 2023.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chia 等 (2023) Yew Ken Chia, Pengfei Hong, Lidong Bing 和 Soujanya Poria. Instructeval:
    面向指令调优大型语言模型的整体评估。*arXiv 预印本 arXiv:2306.04757*，2023。'
- en: Chung et al. (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
    Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*,
    2022.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chung 等 (2022) Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay,
    William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma 等. 扩展指令微调的语言模型。*arXiv
    预印本 arXiv:2210.11416*，2022。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    Llm.int8(): 8-bit matrix multiplication for transformers at scale. In *Conference
    on Neural Information Processing Systems*, 2022.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 (2022) Tim Dettmers, Mike Lewis, Younes Belkada 和 Luke Zettlemoyer.
    Llm.int8(): 用于大规模变换器的 8 位矩阵乘法。在 *神经信息处理系统会议*，2022。'
- en: 'Dettmers et al. (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms. *arXiv preprint arXiv:2305.14314*,
    2023.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等 (2023) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman 和 Luke Zettlemoyer.
    Qlora: 高效的量化LLMs微调。*arXiv 预印本 arXiv:2305.14314*，2023。'
- en: 'Devlin et al. (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. Bert: Pre-training of deep bidirectional transformers for language
    understanding. *arXiv preprint arXiv:1810.04805*, 2018.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等 (2018) Jacob Devlin, Ming-Wei Chang, Kenton Lee 和 Kristina Toutanova.
    Bert: 用于语言理解的深度双向变换器的预训练。*arXiv 预印本 arXiv:1810.04805*，2018。'
- en: 'Dong et al. (2019) Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney,
    and Kurt Keutzer. Hawq: Hessian aware quantization of neural networks with mixed-precision.
    In *International Conference on Computer Vision*, 2019.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等 (2019) Zhen Dong, Zhewei Yao, Amir Gholami, Michael W. Mahoney 和 Kurt
    Keutzer. Hawq: 具有混合精度的神经网络Hessian感知量化。在 *国际计算机视觉会议*，2019。'
- en: 'Dong et al. (2020) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael W.
    Mahoney, and Kurt Keutzer. Hawq-v2: Hessian aware trace-weighted quantization
    of neural networks. In *Conference on Neural Information Processing Systems*,
    2020.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dong 等 (2020) Zhen Dong, Zhewei Yao, Daiyaan Arfeen, Amir Gholami, Michael
    W. Mahoney 和 Kurt Keutzer. Hawq-v2: 关注Hessian的神经网络迹权重量化。在 *神经信息处理系统会议*，2020。'
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. Optq: Accurate quantization for generative pre-trained transformers.
    In *International Conference on Learning Representations*, 2023.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等 (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler 和 Dan Alistarh.
    Optq: 生成预训练变换器的精确量化。在 *国际学习表征会议*，2023。'
- en: 'Gliwa et al. (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander
    Wawer. SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization.
    In *Proceedings of the 2nd Workshop on New Frontiers in Summarization*, pp.  70–79,
    Hong Kong, China, November 2019\. Association for Computational Linguistics. doi:
    10.18653/v1/D19-5409. URL [https://aclanthology.org/D19-5409](https://aclanthology.org/D19-5409).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gliwa 等 (2019) Bogdan Gliwa, Iwona Mochol, Maciej Biesek 和 Aleksander Wawer.
    SAMSum 语料库: 一个用于抽象摘要的人类标注对话数据集。在 *第2届总结新前沿研讨会会议录*，第 70–79 页，香港，中国，2019年11月。计算语言学协会。doi:
    10.18653/v1/D19-5409。网址 [https://aclanthology.org/D19-5409](https://aclanthology.org/D19-5409)。'
- en: Houlsby et al. (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain
    Gelly. Parameter-efficient transfer learning for nlp. In *International Conference
    on Machine Learning*, pp.  2790–2799\. PMLR, 2019.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houlsby 等 (2019) Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna
    Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan 和 Sylvain Gelly.
    NLP 的参数高效迁移学习。在 *国际机器学习会议*，第 2790–2799 页。PMLR，2019。
- en: 'Hu et al. (2022) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language
    models. In *International Conference on Learning Representations*, 2022.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2022) Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
    Shean Wang, Lu Wang, Weizhu Chen, 等. Lora: 大型语言模型的低秩适配。在*国际学习表征会议*，2022年。'
- en: Hubara et al. (2021) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, and
    Daniel Soudry. Accurate post training quantization with small calibration sets.
    In *International Conference on Machine Learning*. PMLR, 2021.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hubara et al. (2021) Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, 和 Daniel
    Soudry. 使用小规模校准集进行准确的后训练量化。在*国际机器学习会议*。PMLR，2021年。
- en: 'Karimi Mahabadi et al. (2021) Rabeeh Karimi Mahabadi, James Henderson, and
    Sebastian Ruder. Compacter: Efficient low-rank hypercomplex adapter layers. *Advances
    in Neural Information Processing Systems*, 34:1022–1035, 2021.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Karimi Mahabadi et al. (2021) Rabeeh Karimi Mahabadi, James Henderson, 和 Sebastian
    Ruder. Compacter: 高效的低秩超复合适配器层。*神经信息处理系统进展*，34:1022–1035，2021年。'
- en: 'Lester et al. (2021) Brian Lester, Rami Al-Rfou, and Noah Constant. The power
    of scale for parameter-efficient prompt tuning. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing*, pp.  3045–3059, Online and
    Punta Cana, Dominican Republic, November 2021\. Association for Computational
    Linguistics. doi: 10.18653/v1/2021.emnlp-main.243. URL [https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lester et al. (2021) Brian Lester, Rami Al-Rfou, 和 Noah Constant. 参数高效的提示调优的规模效应。在*2021年自然语言处理实证方法会议论文集*，第3045–3059页，在线和多米尼加共和国蓬塔卡纳，2021年11月。计算语言学协会。doi:
    10.18653/v1/2021.emnlp-main.243。网址 [https://aclanthology.org/2021.emnlp-main.243](https://aclanthology.org/2021.emnlp-main.243)。'
- en: 'Li & Liang (2021) Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing
    continuous prompts for generation. In *Proceedings of the 59th Annual Meeting
    of the Association for Computational Linguistics and the 11th International Joint
    Conference on Natural Language Processing (Volume 1: Long Papers)*, pp.  4582–4597,
    Online, August 2021\. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.353.
    URL [https://aclanthology.org/2021.acl-long.353](https://aclanthology.org/2021.acl-long.353).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li & Liang (2021) Xiang Lisa Li 和 Percy Liang. Prefix-tuning: 优化生成的连续提示。在*第59届计算语言学协会年会和第11届国际联合自然语言处理会议（第1卷：长篇论文）*，第4582–4597页，在线，2021年8月。计算语言学协会。doi:
    10.18653/v1/2021.acl-long.353。网址 [https://aclanthology.org/2021.acl-long.353](https://aclanthology.org/2021.acl-long.353)。'
- en: 'Li et al. (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, and Shi Gu. Brecq: Pushing the limit of post-training quantization
    by block reconstruction. In *International Conference on Learning Representations*,
    2021.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2021) Yuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng Hu, Qi Zhang,
    Fengwei Yu, Wei Wang, 和 Shi Gu. Brecq: 通过块重构推动后训练量化的极限。在*国际学习表征会议*，2021年。'
- en: Liu et al. (2022a) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao
    Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning
    is better and cheaper than in-context learning. *Advances in Neural Information
    Processing Systems*, 35:1950–1965, 2022a.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022a) Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao
    Huang, Mohit Bansal, 和 Colin A Raffel. 少样本参数高效的微调优于上下文学习且更便宜。*神经信息处理系统进展*，35:1950–1965，2022a年。
- en: 'Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey
    of prompting methods in natural language processing. *ACM Computing Surveys*,
    55(9):1–35, 2023a.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023a) Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki
    Hayashi, 和 Graham Neubig. 预训练、提示和预测：自然语言处理中的提示方法系统综述。*ACM计算调查*，55(9):1–35，2023a年。
- en: 'Liu et al. (2022b) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
    Zhilin Yang, and Jie Tang. P-tuning: Prompt tuning can be comparable to fine-tuning
    across scales and tasks. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 2: Short Papers)*, pp.  61–68, 2022b.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. (2022b) Xiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengxiao Du,
    Zhilin Yang, 和 Jie Tang. P-tuning: 提示调优可以在规模和任务上与微调媲美。在*第60届计算语言学协会年会论文集（第2卷：短篇论文）*，第61–68页，2022b年。'
- en: Liu et al. (2023b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, and Jie Tang. Gpt understands, too. *AI Open*, 2023b.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2023b) Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian,
    Zhilin Yang, 和 Jie Tang. GPT 也理解。*AI Open*，2023b年。
- en: 'Mangrulkar et al. (2022) Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut,
    Younes Belkada, and Sayak Paul. Peft: State-of-the-art parameter-efficient fine-tuning
    methods. [https://github.com/huggingface/peft](https://github.com/huggingface/peft),
    2022.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mangrulkar 等人（2022）**Sourab Mangrulkar**, **Sylvain Gugger**, **Lysandre Debut**,
    **Younes Belkada** 和 **Sayak Paul**。Peft：先进的参数高效微调方法。 [https://github.com/huggingface/peft](https://github.com/huggingface/peft)，2022。
- en: Nagel et al. (2020) Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos
    Louizos, and Tijmen Blankevoort. Up or down? adaptive rounding for post-training
    quantization. In *International Conference on Machine Learning*, pp.  7197–7206\.
    PMLR, 2020.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等人（2020）**Markus Nagel**、**Rana Ali Amjad**、**Mart Van Baalen**、**Christos
    Louizos** 和 **Tijmen Blankevoort**。向上还是向下？用于后训练量化的自适应舍入。在 *国际机器学习会议* 中，pp. 7197–7206。PMLR，2020。
- en: 'Park et al. (2023) Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.
    Lut-gemm: Quantized matrix multiplication based on luts for efficient inference
    in large-scale generative language models. *arXiv preprint arXiv:2206.09557*,
    2023.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park 等人（2023）**Gunho Park**、**Baeseong Park**、**Minsub Kim**、**Sungjae Lee**、**Jeonghoon
    Kim**、**Beomseok Kwon**、**Se Jung Kwon**、**Byeongwook Kim**、**Youngjoo Lee** 和
    **Dongsoo Lee**。Lut-gemm：基于 lut 的量化矩阵乘法，用于大规模生成语言模型的高效推理。*arXiv 预印本 arXiv:2206.09557*，2023。
- en: 'Qin & Eisner (2021) Guanghui Qin and Jason Eisner. Learning how to ask: Querying
    lms with mixtures of soft prompts. *arXiv preprint arXiv:2104.06599*, 2021.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qin & Eisner（2021）**Guanghui Qin** 和 **Jason Eisner**。学习如何提问：用软提示混合查询语言模型。*arXiv
    预印本 arXiv:2104.06599*，2021。
- en: Raffel et al. (2020) Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
    Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring
    the limits of transfer learning with a unified text-to-text transformer, 2020.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raffel 等人（2020）**Colin Raffel**、**Noam Shazeer**、**Adam Roberts**、**Katherine
    Lee**、**Sharan Narang**、**Michael Matena**、**Yanqi Zhou**、**Wei Li** 和 **Peter
    J. Liu**。探索统一文本到文本转换器的迁移学习极限，2020。
- en: 'Rebuffi et al. (2017) Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg
    Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation
    learning. In *Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition*, pp.  2001–2010, 2017.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rebuffi 等人（2017）**Sylvestre-Alvise Rebuffi**、**Alexander Kolesnikov**、**Georg
    Sperl** 和 **Christoph H Lampert**。icarl：增量分类器和表示学习。在 *IEEE 计算机视觉与模式识别会议论文集* 中，pp.
    2001–2010，2017。
- en: 'Scao et al. (2023) Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,
    Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François
    Yvon, Matthias Gallé, et al. Bloom: A 176b-parameter open-access multilingual
    language model. *arXiv preprint arXiv:2211.05100*, 2023.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scao 等人（2023）**Teven Le Scao**、**Angela Fan**、**Christopher Akiki**、**Ellie
    Pavlick**、**Suzana Ilić**、**Daniel Hesslow**、**Roman Castagné**、**Alexandra Sasha
    Luccioni**、**François Yvon**、**Matthias Gallé** 等。Bloom：一个 176b 参数的开放访问多语言模型。*arXiv
    预印本 arXiv:2211.05100*，2023。
- en: Sung et al. (2021) Yi-Lin Sung, Varun Nair, and Colin A Raffel. Training neural
    networks with fixed sparse masks. *Advances in Neural Information Processing Systems*,
    34:24193–24205, 2021.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sung 等人（2021）**Yi-Lin Sung**、**Varun Nair** 和 **Colin A Raffel**。使用固定稀疏掩码训练神经网络。*神经信息处理系统进展*，34:24193–24205，2021。
- en: Suzgun et al. (2022) Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian
    Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny
    Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve
    them. *arXiv preprint arXiv:2210.09261*, 2022.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzgun 等人（2022）**Mirac Suzgun**、**Nathan Scales**、**Nathanael Schärli**、**Sebastian
    Gehrmann**、**Yi Tay**、**Hyung Won Chung**、**Aakanksha Chowdhery**、**Quoc V Le**、**Ed
    H Chi**、**Denny Zhou** 等。挑战 big-bench 任务以及链式思维是否能解决这些任务。*arXiv 预印本 arXiv:2210.09261*，2022。
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Stanford
    alpaca: An instruction-following llama model. [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca),
    2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Taori 等人（2023）**Rohan Taori**、**Ishaan Gulrajani**、**Tianyi Zhang**、**Yann Dubois**、**Xuechen
    Li**、**Carlos Guestrin**、**Percy Liang** 和 **Tatsunori B. Hashimoto**。斯坦福 alpaca：一个指令跟随的
    llama 模型。 [https://github.com/tatsu-lab/stanford_alpaca](https://github.com/tatsu-lab/stanford_alpaca)，2023。
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language
    models. *arXiv preprint arXiv:2302.13971*, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Touvron 等人（2023）**Hugo Touvron**、**Thibaut Lavril**、**Gautier Izacard**、**Xavier
    Martinet**、**Marie-Anne Lachaux**、**Timothée Lacroix**、**Baptiste Rozière**、**Naman
    Goyal**、**Eric Hambro**、**Faisal Azhar** 等。Llama：开放而高效的基础语言模型。*arXiv 预印本 arXiv:2302.13971*，2023。
- en: 'Williams et al. (2018) Adina Williams, Nikita Nangia, and Samuel Bowman. A
    broad-coverage challenge corpus for sentence understanding through inference.
    In *Proceedings of the 2018 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*,
    pp.  1112–1122\. Association for Computational Linguistics, 2018. URL [http://aclweb.org/anthology/N18-1101](http://aclweb.org/anthology/N18-1101).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 威廉姆斯等（2018）阿迪娜·威廉姆斯、尼基塔·南吉亚、塞缪尔·鲍曼。用于通过推理进行句子理解的广覆盖挑战语料库。见 *2018年北美计算语言学协会会议：人类语言技术会议论文集，第1卷（长篇论文）*，第1112–1122页。计算语言学协会，2018年。网址
    [http://aclweb.org/anthology/N18-1101](http://aclweb.org/anthology/N18-1101)。
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. Smoothquant: Accurate and efficient post-training quantization for
    large language models. *arXiv preprint arXiv:2211.10438*, 2023.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '萧等（2023）肖光轩、林吉、米歇尔·塞兹内克、吴浩、朱利安·德穆斯、韩松。Smoothquant: 准确且高效的大型语言模型后训练量化。*arXiv
    预印本 arXiv:2211.10438*，2023年。'
- en: 'Yao et al. (2021) Zhewei Yao, Zhen Dong, Zhangcheng Zheng, Amir Gholami, Jiali
    Yu, Eric Tan, Leyuan Wang, Qijing Huang, Yida Wang, Michael W. Mahoney, and Kurt
    Keutzer. Hawq-v3: Dyadic neural network quantization. In *International Conference
    on Machine Learning*. PMLR, 2021.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '姚等（2021）姚哲伟、董震、郑张成、阿米尔·戈拉米、余佳丽、艾瑞克·谭、王乐远、黄琦靖、王义达、迈克尔·W·马赫尼、库尔特·凯泽。Hawq-v3:
    对偶神经网络量化。见 *国际机器学习会议*。PMLR，2021年。'
- en: 'Yao et al. (2022) Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia
    Wu, Conglong Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training
    quantization for large-scale transformers. In *Conference on Neural Information
    Processing Systems*, 2022.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '姚等（2022）姚哲伟、雷扎·雅兹达尼·阿米纳巴迪、张敏佳、吴晓霞、李聪龙、和贺玉雄。Zeroquant: 高效且实惠的大规模变换器后训练量化。见 *神经信息处理系统会议*，2022年。'
- en: 'Yuan et al. (2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Luzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models. *arXiv preprint arXiv:2304.01089*,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '袁等（2023）袁志航、牛琳、刘家伟、刘文瑜、王兴刚、尚璐璋、孙光宇、吴强、吴家祥、吴炳哲。Rptq: 基于重排序的后训练量化方法用于大型语言模型。*arXiv
    预印本 arXiv:2304.01089*，2023年。'
- en: 'Zaken et al. (2021) Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:
    Simple parameter-efficient fine-tuning for transformer-based masked language-models.
    *arXiv preprint arXiv:2106.10199*, 2021.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '扎肯等（2021）埃拉德·本·扎肯、沙乌利·拉夫戈尔、约阿夫·戈尔德堡。Bitfit: 简单的参数高效的变换器基础掩蔽语言模型微调。*arXiv 预印本
    arXiv:2106.10199*，2021年。'
- en: 'Zhang et al. (2022) Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,
    Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin,
    Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh
    Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained
    transformer language models, 2022.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '张等（2022）苏珊·张、斯蒂芬·罗勒、纳曼·戈亚尔、米克尔·阿尔特克斯、莫雅·陈、舒辉·陈、克里斯托弗·德旺、莫娜·迪亚布、李贤、维多利亚·林、托多尔·米哈伊洛夫、迈尔·奥特、萨姆·施莱弗、库尔特·舒斯特、丹尼尔·西米格、普尼特·辛格·库拉、安贾利·斯里达尔、王天璐、卢克·泽特尔莫耶。Opt:
    开放的预训练变换器语言模型，2022年。'
- en: Appendix A Additional Method Details
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 额外的方法细节
- en: A.1 Low-Rank Adaptation in Low Precision
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.1 低精度中的低秩适应
- en: Given a pre-trained network, the first step of ModuLoRA is to apply a black-box
    quantization $\mathcal{Q}$, with the reparameterized LP-LoRA layer given by
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个预训练的网络，ModuLoRA 的第一步是应用一个黑箱量化 $\mathcal{Q}$，其重新参数化的 LP-LoRA 层由下式给出
- en: '|  | $x\mapsto\hat{\mathbf{W}}^{(i)}x+\mathbf{A}^{(i)}((\mathbf{B}^{(i)})^{\top}x)+\mathbf{b}^{(i)}.$
    |  | (7) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $x\mapsto\hat{\mathbf{W}}^{(i)}x+\mathbf{A}^{(i)}((\mathbf{B}^{(i)})^{\top}x)+\mathbf{b}^{(i)}.$
    |  | (7) |'
- en: Here $\mathbf{A}^{(i)},\mathbf{B}^{(i)}\in\mathbb{R}^{d\times r}$. This enables
    finetuning LLMs on consumer-grade GPUs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 $\mathbf{A}^{(i)},\mathbf{B}^{(i)}\in\mathbb{R}^{d\times r}$。这使得在消费级 GPU
    上微调大型语言模型成为可能。
- en: '\parclass  LPLinear(Function):@staticmethoddef  forward(ctx,  input,  hatWq,  z,  s):ctx.save_for_backward(hatWq,  z,  s)hatW  =  dequantize(hatWq,  z,  s)  #  allocates  for  hatWoutput  =  input  @  hatW.t()#  here  hatW  goes  out  of  scope  and  can  be  deallocatedreturn  output\par@staticmethoddef  backward(ctx,  grad_output):hatWq,  z,  s  =  ctx.saved_tensorshatW  =  dequantize(hatWq,  z,  s)  #  recompute  hatWgrad_input  =  grad_output  @  hatW#  here  hatW  goes  out  of  scope  and  can  be  deallocatedreturn  grad_input,  None,  None,  None\par\par\parclass  LPLoRA(Module):def  __init__(self,  pretrained_linear_layer,  rank):super().__init__()(hatWq,  z,  s)  =  quantize(pretrained_linear_layer.weight)self.bias  =  pretrained_linear_layer.bias…(A,  B)  =  lora_init(…,  rank)self.A,  self.B  =  (Parameter(A),  Parameter(B))\pardef  forward(self,  x):return  LPLinear.apply(x,  self.hatWq,  self.z,  self.s)  \  +  (x  @  self.B)  @  self.A.t()  +  self.bias'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '\parclass  LPLinear(Function):@staticmethoddef  forward(ctx,  input,  hatWq,  z,  s):ctx.save_for_backward(hatWq,  z,  s)hatW  =  dequantize(hatWq,  z,  s)  #  为  hatW
    分配空间output  =  input  @  hatW.t()#  在这里  hatW  超出 作用域  并且  可以  被  释放return  output\par@staticmethoddef  backward(ctx,  grad_output):hatWq,  z,  s  =  ctx.saved_tensorshatW  =  dequantize(hatWq,  z,  s)  #  重新计算  hatWgrad_input  =  grad_output  @  hatW#  在这里  hatW  超出
    作用域  并且  可以  被  释放return  grad_input,  None,  None,  None\par\par\parclass  LPLoRA(Module):def  __init__(self,  pretrained_linear_layer,  rank):super().__init__()(hatWq,  z,  s)  =  quantize(pretrained_linear_layer.weight)self.bias  =  pretrained_linear_layer.bias…(A,  B)  =  lora_init(…,  rank)self.A,  self.B  =  (Parameter(A),  Parameter(B))\pardef  forward(self,  x):return  LPLinear.apply(x,  self.hatWq,  self.z,  self.s)  \  +  (x  @  self.B)  @  self.A.t()  +  self.bias'
- en: 'Figure 2: PyTorch pseudocode for ModuLoRA.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：ModuLoRA 的 PyTorch 伪代码。
- en: A.2 Efficient Mixed-Precision Computation of Forward and Backward Passes
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: A.2 高效的混合精度前向和反向计算
- en: We first introduce our approach in the context of a network with $n$ to denote
    the loss.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在一个具有$n$的网络上下文中介绍我们的方法来表示损失。
- en: The Structure of a Quantized Backward Pass
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 量化反向传递的结构
- en: The backward pass seeks to compute derivatives $\mathrm{d}L/\mathrm{d}\mathbf{A}^{(i)}$,
    where we overload standard Leibniz notation to denote gradients. By the chain
    rule,
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传递旨在计算导数 $\mathrm{d}L/\mathrm{d}\mathbf{A}^{(i)}$，其中我们重载标准的莱布尼茨符号来表示梯度。根据链式法则，
- en: '|  | $\frac{\mathrm{d}L}{\mathrm{d}\mathbf{A}^{(i)}}=\frac{\mathrm{d}L}{\mathrm{d}\bar{\mathbf{y}}_{i}}\cdot\frac{\mathrm{d}\bar{\mathbf{y}}_{i}}{\mathrm{d}\mathbf{A}^{(i)}}.$
    |  | (8) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\mathrm{d}L}{\mathrm{d}\mathbf{A}^{(i)}}=\frac{\mathrm{d}L}{\mathrm{d}\bar{\mathbf{y}}_{i}}\cdot\frac{\mathrm{d}\bar{\mathbf{y}}_{i}}{\mathrm{d}\mathbf{A}^{(i)}}.$
    |  | (8) |'
- en: 'Because of the additive form of ([3](#S3.E3 "In 3.1 Low-Rank Adaptation of
    Low-Precision Models ‣ 3 Low-Precision Low-Rank Adaptation with a Modular Quantizer
    ‣ ModuLoRA: Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular
    Quantizers")), $\mathrm{d}\mathbf{y}_{i}/\mathrm{d}\mathbf{A}^{(i)}$. The second
    term can be computed via'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由于加法形式 ([3](#S3.E3 "在 3.1 低秩适应低精度模型 ‣ 3 低精度低秩适应与模块化量化器 ‣ ModuLoRA：通过与模块化量化器集成在消费级
    GPU 上微调 3 位 LLM"))，$\mathrm{d}\mathbf{y}_{i}/\mathrm{d}\mathbf{A}^{(i)}$。第二项可以通过
- en: '|  | $1$2 |  | (9) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (9) |'
- en: where ${\mathrm{d}\mathbf{y}_{i}}/{\mathrm{d}\bar{\mathbf{y}}_{i}}$ with high-precision
    vectors in the forward and backward passes.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\mathrm{d}\mathbf{y}_{i}}/{\mathrm{d}\bar{\mathbf{y}}_{i}}$ 使用前向和反向传递中的高精度向量。
- en: A.2.1 Mixed-Precision Matrix-Vector Multiplication
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: A.2.1 混合精度矩阵-向量乘法
- en: Importantly, we do not need to maintain $\hat{\mathbf{W}}^{(i)}$ may not even
    be necessary.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们不需要维护 $\hat{\mathbf{W}}^{(i)}$ 可能甚至是不必要的。
- en: Appendix B Additional Implementation Details
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 额外实现细节
- en: B.1 Configurations for BBH Evaluation
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1 BBH 评估的配置
- en: 'We evaluate the BBH dataset using LoRA adapter weights from huggingface hub
    with different configurations. For the Bits&Bytes 8-bit (LLM.int8()) LoRA adapter
    weights, we utilized two sources: the Alpaca-7B one is obtained from the ’tloen/alpaca-lora-7b’
    repository, while the weights for Alpaca-13b and 30b were sourced from ’chansung/alpaca-lora-xxb’.
    In the case of Bits&Bytes 4-bit (QLoRA) adapter weights, all configurations (Alpaca-7B,
    13B, and 30B)—were uniformly accessed from ’timdettmers/qlora-alpaca-xxb’. Note
    that for the Bits&Bytes 4-bit (QLoRA) and Bits&Bytes 8-bit (LLM.int8()) adapter
    wights of the 65B model, we obtain them by finetuning the base 65B LLaMa model
    on Alpaca dataset using the same set of hyperparameters as ours.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用来自 huggingface hub 的 LoRA 适配器权重对 BBH 数据集进行了评估，采用不同的配置。对于 Bits&Bytes 8 位（LLM.int8()）LoRA
    适配器权重，我们使用了两个来源：Alpaca-7B 的权重来自 ’tloen/alpaca-lora-7b’ 仓库，而 Alpaca-13b 和 30b 的权重来自
    ’chansung/alpaca-lora-xxb’。对于 Bits&Bytes 4 位（QLoRA）适配器权重，所有配置（Alpaca-7B、13B 和
    30B）均统一从 ’timdettmers/qlora-alpaca-xxb’ 访问。需要注意的是，对于 Bits&Bytes 4 位（QLoRA）和 Bits&Bytes
    8 位（LLM.int8()）适配器权重的 65B 模型，我们通过在 Alpaca 数据集上微调基础 65B LLaMa 模型获得，使用的超参数集与我们相同。
- en: Appendix C Additional Empirical Experiments
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 额外的实证实验
- en: 'We conducted additional experiment on Code-Alpaca ((Chaudhary, [2023](#bib.bib3))).
    The result is shown in [8](#A3.T8 "In C.1 Additional Experiments on Code-Alpaca
    with LLaMA ‣ Appendix C Additional Empirical Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers"). Consistent
    with our hypothesis, ModuLoRA performs better than or at least on par with the
    higher precision 8-bit models given the same number of trainable parameters and
    set up.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Code-Alpaca 上进行了额外的实验（Chaudhary，[2023](#bib.bib3)）。结果见 [8](#A3.T8 "在 C.1
    LLaMA 上的 Code-Alpaca 额外实验 ‣ 附录 C 额外的实证实验 ‣ ModuLoRA：通过与模块化量化器集成在消费级 GPU 上微调 3
    位 LLM")。与我们的假设一致，ModuLoRA 的表现优于或至少与更高精度的 8 位模型持平，前提是训练参数和设置相同。
- en: C.1 Additional Experiments on Code-Alpaca with LLaMA
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.1 LLaMA 上的 Code-Alpaca 额外实验
- en: '| Code Alpaca Performance | 7B | 13B | 30B | 65B |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Code Alpaca 性能 | 7B | 13B | 30B | 65B |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| LLMTools (3-bit) | 53.6 / 36.3 / 50.7 | 57.0 / 40.0 / 53.3 | 58.1 / 40.7 / 54.3
    | 60.0 / 44.1 / 58.8 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| LLMTools (3-bit) | 53.6 / 36.3 / 50.7 | 57.0 / 40.0 / 53.3 | 58.1 / 40.7
    / 54.3 | 60.0 / 44.1 / 58.8 |'
- en: '| LLMTools (4-bit) | 54.6 / 37.2 / 51.4 | 57.4 / 40.6 / 54.3 | 59.0 / 41.4 / 57.5
    | 60.2 / 43.5 / 56.8 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| LLMTools (4-bit) | 54.6 / 37.2 / 51.4 | 57.4 / 40.6 / 54.3 | 59.0 / 41.4
    / 57.5 | 60.2 / 43.5 / 56.8 |'
- en: '| Bits&Bytes 8-bit (LLM.int8()) | 54.0 / 36.3 / 50.9 | 57.7 / 41.3 / 54.9 |
    60.6 / 43.5 / 57.5 | 61.1 / 44.1 / 58.0 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| Bits&Bytes 8-bit (LLM.int8()) | 54.0 / 36.3 / 50.9 | 57.7 / 41.3 / 54.9 |
    60.6 / 43.5 / 57.5 | 61.1 / 44.1 / 58.0 |'
- en: 'Table 8: Instruction-tuned models evaluated using ROUGE 1/2/LSum on Code Alpaca
    in 3, 4, and 8 bits.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8：在 3 位、4 位和 8 位的 Code Alpaca 上评估的指令调优模型的 ROUGE 1/2/LSum。
- en: Appendix D Hyperparamters Used in Experiments
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 实验中使用的超参数
- en: D.1 LLaMA / OPT on SAMSum
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 LLaMA / OPT 在 SAMSum 上
- en: 'We set up the training procedure following Hu et al. ([2022](#bib.bib15)),
    with slight variation to accomodate our particular language models. For a fair
    comparison with the concurrent work by Dettmers et al. ([2023](#bib.bib8)), we
    use the exact same hyperparameter set up as shown in [9](#A4.T9 "In D.1 LLaMA
    / OPT on SAMSum ‣ Appendix D Hyperparamters Used in Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers") . We train
    using AdamW for 350 steps with a batch size of 128 samples. We report the results
    over 3 random seeds; the result for each run is taken from the training steps
    with the lowest validation loss.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据 Hu 等人（[2022](#bib.bib15)）的方法设置了训练程序，并对特定语言模型进行了略微调整。为了与 Dettmers 等人（[2023](#bib.bib8)）的并行工作进行公平比较，我们使用了与
    [9](#A4.T9 "在 D.1 LLaMA / OPT 上的 SAMSum ‣ 附录 D 实验中使用的超参数 ‣ ModuLoRA：通过与模块化量化器集成在消费级
    GPU 上微调 3 位 LLM") 中所示完全相同的超参数设置。我们使用 AdamW 训练 350 步，批量大小为 128 个样本。我们报告了 3 个随机种子的结果；每次运行的结果取自验证损失最低的训练步骤。
- en: 'Dataset Model LLaMA 7B / 13B / 30B / 65B OPT 7B/ 13B / 30B SAMSum Optimizer
    AdamW Warmup Ratio 0.06 Batch size 128 Evaluation Batch size 16 Evaluation Steps
    50 Total # Training Steps 350 Learning Rate Schedule Cosine Learning Rate 1e-3
    WeightDecay 0.0 LoRAConfig $r_{q}=r_{v}=8$ 32 Max Seq. Len 250'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 模型 LLaMA 7B / 13B / 30B / 65B OPT 7B / 13B / 30B SAMSum 优化器 AdamW 预热比例 0.06
    批量大小 128 评估批量大小 16 评估步骤 50 总训练步骤数 350 学习率调度 余弦学习率 1e-3 权重衰减 0.0 LoRA 配置 $r_{q}=r_{v}=8$
    32 最大序列长度 250
- en: 'Table 9: Hyperparamters configuration for ModuLoRA, Q-LoRA on SAMSum'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：ModuLoRA、Q-LoRA 在 SAMSum 上的超参数配置
- en: D.2 LLaMA on Code-Alpaca & Text-Classification
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 Code-Alpaca 和文本分类上的 LLaMA
- en: We again train using AdamW optimizer with a warmup ratio of 0.06\. We tune learning
    rate, batch size, training steps for each task. We report the results over 3 random
    seeds. The result for each run is taken from the training steps that yield the
    lowest validation loss.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次使用 AdamW 优化器，预热比率为 0.06。我们为每个任务调整学习率、批次大小和训练步骤。结果基于 3 个随机种子进行报告。每次运行的结果取自具有最低验证损失的训练步骤。
- en: '| Dataset | LLaMA Model | 13/30/65 B |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | LLaMA 模型 | 13/30/65 B |'
- en: '| Text- Classification | Optimizer | AdamW |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 文本分类 | 优化器 | AdamW |'
- en: '| Warmup Ratio | 0.06 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 预热比率 | 0.06 |'
- en: '| Batch size | 256 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 批次大小 | 256 |'
- en: '| Evaluation Batch size | 32 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 评估批次大小 | 32 |'
- en: '| Evaluation Steps | 100 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 评估步骤 | 100 |'
- en: '| Total # Training Steps | 1000 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 总训练步骤数 | 1000 |'
- en: '| Learning Rate Schedule | Cosine |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度 | 余弦 |'
- en: '| Learning Rate | 1e-3 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1e-3 |'
- en: '| WeightDecay | 0.0 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | 0.0 |'
- en: '|  | LoRAConfig | $r_{q}=r_{v}=8$ |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA 配置 | $r_{q}=r_{v}=8$ |'
- en: '|  | LoRA $\alpha$ | 32 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA $\alpha$ | 32 |'
- en: '|  | Max Seq. Len | 128 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '|  | 最大序列长度 | 128 |'
- en: 'Table 10: Hyperparamters configuration for ModuLoRA, Q-LoRA on Text-Classification'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10: 文本分类上 ModuLoRA 和 Q-LoRA 的超参数配置'
- en: '| Dataset | LLaMA Model | 7/13/30/65 B |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | LLaMA 模型 | 7/13/30/65 B |'
- en: '| Code- Alpaca | Optimizer | AdamW |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 代码-Alpaca | 优化器 | AdamW |'
- en: '| Warmup Ratio | 0.06 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 预热比率 | 0.06 |'
- en: '| Batch size | 128 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 批次大小 | 128 |'
- en: '| Evaluation Batch size | 4 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 评估批次大小 | 4 |'
- en: '| Evaluation Steps | 40 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 评估步骤 | 40 |'
- en: '| Total # Training Steps | 120 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 总训练步骤数 | 120 |'
- en: '| Learning Rate Schedule | Linear |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 学习率调度 | 线性 |'
- en: '| Learning Rate | 1e-3 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 1e-3 |'
- en: '| WeightDecay | 0.0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 权重衰减 | 0.0 |'
- en: '|  | LoRAConfig | $r_{q}=r_{v}=8$ |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA 配置 | $r_{q}=r_{v}=8$ |'
- en: '|  | LoRA $\alpha$ | 32 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | LoRA $\alpha$ | 32 |'
- en: '|  | Max Seq. Len | 165 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | 最大序列长度 | 165 |'
- en: 'Table 11: Hyperparamters configuration for ModuLoRA, Q-LoRA on Alpaca-Code'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11: Alpaca-Code 上 ModuLoRA 和 Q-LoRA 的超参数配置'
- en: D.3 LLaMA on MNLI-M
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 MNLI-M 上的 LLaMA
- en: 'Training is conducted using the AdamW optimizer, with a warmup ratio set at
    0.06\. We tune the learning rate, batch size, and training steps. Results are
    reported over three random seeds, and for each run, the performance metric is
    derived from the training step with the lowest validation loss. See [12](#A4.T12
    "In D.3 LLaMA on MNLI-M ‣ Appendix D Hyperparamters Used in Experiments ‣ ModuLoRA:
    Finetuning 3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers")
    for more details on the hyperparameters used.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '训练使用 AdamW 优化器，预热比率设置为 0.06。我们调节学习率、批次大小和训练步骤。结果基于三个随机种子进行报告，每次运行的性能指标来自于具有最低验证损失的训练步骤。有关使用的超参数的更多细节，请参见
    [12](#A4.T12 "在 D.3 MNLI-M 上的 LLaMA ‣ 附录 D 实验中使用的超参数 ‣ ModuLoRA: 通过与模块化量化器集成在消费级
    GPU 上微调 3 位 LLM")。'
- en: 'Dataset Model LLaMA 7B / 13B / 30B / 65B MNLI-M Optimizer AdamW Warmup Ratio
    0.06 Batch size 128 Evaluation Batch size 64 Evaluation Steps 64 Total # Training
    Epoch 1.0 Learning Rate Schedule Cosine Learning Rate 1e-3 WeightDecay 0.0 LoRAConfig
    $r_{q}=r_{v}=8$ 32 Max Seq. Len 128'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 模型 LLaMA 7B / 13B / 30B / 65B MNLI-M 优化器 AdamW 预热比率 0.06 批次大小 128 评估批次大小
    64 评估步骤 64 总训练轮次 1.0 学习率调度 余弦 学习率 1e-3 权重衰减 0.0 LoRA 配置 $r_{q}=r_{v}=8$ 32 最大序列长度
    128
- en: 'Table 12: Hyperparamters configuration for ModuLoRA, Q-LoRA on MNLI-M'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '表 12: MNLI-M 上 ModuLoRA 和 Q-LoRA 的超参数配置'
- en: D.4 LLaMA on Alpaca for BBH Evaluation
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.4 用于 BBH 评估的 Alpaca 上的 LLaMA
- en: 'Training is conducted using the AdamW optimizer, with a warmup ratio set at
    0.06\. We tune the learning rate, batch size, and training steps. Results are
    reported over three random seeds. See [13](#A4.T13 "In D.4 LLaMA on Alpaca for
    BBH Evaluation ‣ Appendix D Hyperparamters Used in Experiments ‣ ModuLoRA: Finetuning
    3-Bit LLMs on Consumer GPUs by Integrating with Modular Quantizers") for more
    details on the hyperparameters used.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '训练使用 AdamW 优化器，预热比率设置为 0.06。我们调节学习率、批次大小和训练步骤。结果基于三个随机种子进行报告。有关使用的超参数的更多细节，请参见
    [13](#A4.T13 "在 D.4 用于 BBH 评估的 Alpaca 上的 LLaMA ‣ 附录 D 实验中使用的超参数 ‣ ModuLoRA: 通过与模块化量化器集成在消费级
    GPU 上微调 3 位 LLM")。'
- en: 'Dataset Model LLaMA 7B / 13B / 30B / 65B Alpaca Optimizer AdamW Warmup Ratio
    0.06 Batch size 128 Total # Training Epochs 3 Learning Rate Schedule Linear Learning
    Rate 1e-3 WeightDecay 0.0 LoRAConfig $r_{q}=r_{v}=8$ 16 Max Seq. Len 256'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集 模型 LLaMA 7B / 13B / 30B / 65B Alpaca 优化器 AdamW 预热比率 0.06 批次大小 128 总训练轮次
    3 学习率调度 线性 学习率 1e-3 权重衰减 0.0 LoRA 配置 $r_{q}=r_{v}=8$ 16 最大序列长度 256
- en: 'Table 13: Hyperparamters configuration for ModuLoRA on Alpaca'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '表 13: Alpaca 上 ModuLoRA 的超参数配置'
