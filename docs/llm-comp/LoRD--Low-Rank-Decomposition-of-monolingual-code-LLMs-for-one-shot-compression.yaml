- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 19:03:56'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-08 19:03:56'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'LoRD: Low Rank Decomposition of monolingual code LLMs for one-shot compression'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'LoRD: 单语代码LLM的低秩分解用于一次性压缩'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.14021](https://ar5iv.labs.arxiv.org/html/2309.14021)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2309.14021](https://ar5iv.labs.arxiv.org/html/2309.14021)
- en: Ayush Kaushal
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ayush Kaushal
- en: Université de Montréal, Nolano AI
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 蒙特利尔大学, Nolano AI
- en: ayush@nolano.ai &Tejas Vaidhya
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: ayush@nolano.ai &Tejas Vaidhya
- en: Mila, Université de Montréal, Nolano AI
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Mila, 蒙特利尔大学, Nolano AI
- en: tejas@nolano.ai &Irina Rish
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: tejas@nolano.ai &Irina Rish
- en: Mila, Université de Montréal, Nolano AI
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Mila, 蒙特利尔大学, Nolano AI
- en: irina@nolano.ai
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: irina@nolano.ai
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Low Rank Decomposition of matrix - splitting a large matrix into a product of
    two smaller matrix offers a means for compression that reduces the parameters
    of a model without sparsification, and hence delivering more speedup on modern
    hardware. Moreover, unlike quantization, the compressed linear layers remain fully
    differentiable and all the parameters trainable, while being able to leverage
    the existing highly efficient kernels over floating point matrices. We study the
    potential to compress Large Language Models (LLMs) for monolingual Code generation
    via Low Rank Decomposition (LoRD) and observe that ranks for the linear layers
    in these models can be reduced by upto 39.58% with less than 1% increase in perplexity.
    We then use LoRD to compress StarCoder 16B to 13.2B parameter with no drop and
    to 12.3B with minimal drop in HumanEval Pass@1 score, in less than 10 minutes
    on a single A100\. The compressed models speeds up inference by up to 22.35% with
    just a single line of change in code over huggingface’s implementation with pytorch
    backend. LoRD models remain compatible with state of the art near-lossless quantization
    method such as SpQR, which allows leveraging further compression gains of quantization.
    Lastly, QLoRA over LoRD model further reduces memory requirements by as much as
    21.2% over vanilla QLoRA while offering similar gains from parameter efficient
    fine tuning. Our work shows Low Rank Decomposition (LoRD) as a promising new paradigm
    for LLM compression. ¹¹1We will release LoRDCoder at https://huggingface.co/nolanoAI
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的低秩分解 - 将大矩阵分解为两个较小矩阵的乘积提供了一种压缩的方法，这可以在不稀疏化模型的情况下减少模型参数，从而在现代硬件上提供更多的加速。此外，与量化不同，压缩的线性层仍然完全可微，并且所有参数都可以训练，同时能够利用现有的高效浮点矩阵内核。我们研究了通过低秩分解（LoRD）压缩用于单语代码生成的大型语言模型（LLMs）的潜力，并观察到这些模型的线性层的秩可以减少多达39.58%，而困惑度增加不到1%。我们然后使用LoRD将StarCoder
    16B压缩到13.2B参数，未见下降，并压缩到12.3B，HumanEval Pass@1分数的下降最小，在单个A100上少于10分钟。压缩后的模型在huggingface的pytorch后端实现中，通过仅一行代码的变化，推理速度提升最多22.35%。LoRD模型仍然兼容先进的近无损量化方法，如SpQR，这允许进一步利用量化的压缩增益。最后，QLoRA在LoRD模型上的应用将内存需求减少了多达21.2%，同时提供了类似的参数高效微调的增益。我们的工作显示低秩分解（LoRD）作为LLM压缩的一个有前景的新范式。¹¹1我们将发布LoRDCoder，地址为
    https://huggingface.co/nolanoAI
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Code LLMs have become an integral component of Copilots that boost developer
    productivity (Peng et al., [2023](#bib.bib47)) and in LLM based agents (Wang et al.,
    [2023a](#bib.bib62)). These Code LLMs are as large as 34 Billion parameters for
    the publicly available models Rozière et al. ([2023](#bib.bib51)) and more than
    175 Billion parameter for closed source ones Chen et al. ([2021a](#bib.bib10)).
    There is not only a pressing need for reducing model size and running models at
    a lower cost, but also for increasing the inference speed. The latter is especially
    significant for Copilot based applications.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 代码LLM已经成为提升开发者生产力的Copilots的重要组成部分（Peng et al., [2023](#bib.bib47)）以及基于LLM的代理（Wang
    et al., [2023a](#bib.bib62)）。这些代码LLM的公开模型参数多达340亿个（Rozière et al., [2023](#bib.bib51)），而闭源模型则超过1750亿个参数（Chen
    et al., [2021a](#bib.bib10)）。不仅需要减少模型大小和降低运行成本，还需要提高推理速度。后者对于基于Copilot的应用尤其重要。
- en: Recently, several methods have been proposed to compress and speed up inference
    of LLMs. Quantization (Frantar et al., [2023](#bib.bib24); Dettmers et al., [2023b](#bib.bib18))
    reduces the number of bits required per weight parameter of LLM by lowering the
    precision, and has shown significant model compression as well as speedups in
    low-batch decoding phases of LLMs Kim et al. ([2023a](#bib.bib32)). Quantization
    has also been shown to generalize well to quantized models Shen et al. ([2023](#bib.bib56)).
    Pruning (Sun et al., [2023a](#bib.bib57); Frantar & Alistarh, [2023](#bib.bib23))
    has offered another means of compression by removing connections from the neural
    network and hence sparsifying the weight matrices of the neural networks. Distillation
    Gu et al. ([2023](#bib.bib25)); Agarwal et al. ([2023](#bib.bib1)); Jung et al.
    ([2023](#bib.bib31)) method enables one to train a smaller model using a larger
    teacher model for supervision. While quantization and pruning methods that do
    not require re-training are viable means of compressing the model, distillation
    involves a significant amount of compute for retraining a smaller LLM, often from
    scratch. Here, we consider another compression paradigm of Low Rank Decomposition
    (LoRD) , that does not require expensive retraining as in the case of distillation
    and covers up several deficiencies of the quantization and pruning compression
    method.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，提出了几种方法来压缩和加速LLM的推理。量化（Frantar et al., [2023](#bib.bib24); Dettmers et al.,
    [2023b](#bib.bib18)）通过降低精度减少了LLM每个权重参数所需的位数，已经在LLM的低批次解码阶段显示出显著的模型压缩和加速效果 Kim
    et al. ([2023a](#bib.bib32))。量化还被证明能够很好地推广到量化模型 Shen et al. ([2023](#bib.bib56))。修剪（Sun
    et al., [2023a](#bib.bib57); Frantar & Alistarh, [2023](#bib.bib23)）则通过从神经网络中移除连接，从而使神经网络的权重矩阵变得稀疏，提供了另一种压缩手段。蒸馏
    Gu et al. ([2023](#bib.bib25)); Agarwal et al. ([2023](#bib.bib1)); Jung et al.
    ([2023](#bib.bib31)) 方法使得可以利用较大的教师模型来训练较小的模型。虽然不需要重新训练的量化和修剪方法是可行的压缩手段，但蒸馏涉及大量的计算资源用于重新训练较小的LLM，通常是从头开始。在这里，我们考虑另一种压缩范式——低秩分解（LoRD），这种方法不需要像蒸馏那样昂贵的重新训练，并且弥补了量化和修剪压缩方法的一些不足。
- en: Low Rank Decomposition factorizes a dense matrix of a neural network as a product
    of two smaller dense matrices. The LoRD model can leverage the highly optimized
    floating-point dense matrix multiplication kernels (NVIDIA, [2007](#bib.bib43);
    Blackford et al., [2002](#bib.bib6)) that have been written over modern hardware.
    In contrast, quantized models require specialized kernels to be written, often
    different for each hardware backend in order to enable fast inference. Moreover,
    the neural network remaining fully-differentiable and all the parameters remaining
    trainable even after compression, unlike quantization. The LoRA Hu et al. ([2022](#bib.bib29))
    layers of tuned models are also easier to merge back into floating point matrices
    compared to the quantized ones.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩分解将神经网络的稠密矩阵分解为两个较小的稠密矩阵的乘积。LoRD模型可以利用现代硬件上编写的高度优化的浮点稠密矩阵乘法内核（NVIDIA, [2007](#bib.bib43);
    Blackford et al., [2002](#bib.bib6)）。相比之下，量化模型需要编写专门的内核，通常针对每个硬件后端有所不同，以实现快速推理。此外，神经网络在压缩后仍然完全可微，所有参数仍然可训练，这与量化不同。LoRA
    Hu et al. ([2022](#bib.bib29)) 调整后的模型层也比量化模型更容易重新合并回浮点矩阵。
- en: Pruned models produce sparse matrix weights in the neural network. Matrix multiplication
    over sparse matrices is much slower than the resulting dense matrices in LoRD
    on most GPUs. Dense matrices, in addition avoid representation format overhead
    that sparse matrices incur from parameter reduction ²²2This overhead in sparse
    matrix occurs from having to store indices/bitmasks to indicate which values are
    present and not. This can be very significant at low levels of sparsity. PyTorch’s
    sparse formats (CSR, CSC, COO) all store indices at int64 format, and for moderate
    levels of sparsity ($<$50%), the sparse matrix takes up more space than a dense
    matrix with zero-ed out values. and often requires specialized kernels for reducing
    this overhead Dettmers et al. ([2023b](#bib.bib18)). Dense matrix multiplication
    is also easier to implement than sparse matrix multiplication, especially over
    quantized models.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝模型在神经网络中产生稀疏矩阵权重。对稀疏矩阵的矩阵乘法比在大多数 GPU 上得到的密集矩阵的乘法要慢得多。密集矩阵还避免了稀疏矩阵因参数减少而产生的表示格式开销²²2这种稀疏矩阵开销来源于存储索引/位掩码以指示哪些值存在和不存在。此开销在低稀疏度时可能非常显著。PyTorch
    的稀疏格式（CSR、CSC、COO）都以 int64 格式存储索引，对于中等水平的稀疏性（$<$50%），稀疏矩阵占用的空间比用零值填充的密集矩阵还要多，且通常需要专门的内核来减少这一开销
    Dettmers 等 ([2023b](#bib.bib18))。密集矩阵乘法也比稀疏矩阵乘法更容易实现，特别是在量化模型上。
- en: Several previous works have attempted to apply matrix decomposition methods
    like SVD, Tucker or Kronecker decomposition for compression (Ben Noach & Goldberg,
    [2020](#bib.bib4); Tahaei et al., [2022](#bib.bib59); Edalati et al., [2022](#bib.bib21)).
    However, these have been limited to small language models like Bert (Devlin et al.,
    [2019](#bib.bib20)) and GPT2 (Radford et al., [2019](#bib.bib49)), and have shown
    success only on narrow task-specific use cases or after retraining, often only
    with teacher-guided distillation supervision. These works have observed that weight
    matrices are not low rank and adapt methods like Singular Value Decomposition
    for data-aware decomposition of weights (Chen et al., [2021b](#bib.bib11); Hsu
    et al., [2022](#bib.bib28); Yu & Wu, [2023](#bib.bib69)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 之前有几项工作尝试应用矩阵分解方法，如 SVD、Tucker 或 Kronecker 分解来进行压缩（Ben Noach & Goldberg，[2020](#bib.bib4)；Tahaei
    等，[2022](#bib.bib59)；Edalati 等，[2022](#bib.bib21)）。然而，这些工作仅限于像 Bert（Devlin 等，[2019](#bib.bib20)）和
    GPT2（Radford 等，[2019](#bib.bib49)）这样的小型语言模型，且仅在特定任务的用例或重新训练后成功，通常需要教师指导的蒸馏监督。这些工作发现权重矩阵不是低秩的，并采用像奇异值分解这样的适应方法来进行数据感知的权重分解（Chen
    等，[2021b](#bib.bib11)；Hsu 等，[2022](#bib.bib28)；Yu & Wu，[2023](#bib.bib69)）。
- en: 'We, adapt these approaches for Large Language Models (Billion+ Parameters)
    over python code, and show that these models can be low-rank decomposed to compress
    and speed up inference without the need for retraining with little to no performance
    degradation. We study low-rank decomposition across two families of code LLMs
    - StarCoder and CodeGen (§[2](#S2 "2 Code LLMs are Low Rank Decomposable ‣ LoRD:
    Low Rank Decomposition of monolingual code LLMs for one-shot compression")) for
    varying parameter sizes and establish the potential for reducing rank of models
    through decomposition. We then study these trends across different kinds of linear
    layers in a transformer block and observe the potential for upto 39.58% rank reduction
    with less than 1% change in perplexity.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '我们将这些方法适配于大型语言模型（超过十亿个参数）的 Python 代码中，并展示了这些模型可以低秩分解来压缩和加速推理，而无需重新训练，且性能几乎没有下降。我们研究了两类代码
    LLM 的低秩分解——StarCoder 和 CodeGen (§[2](#S2 "2 Code LLMs are Low Rank Decomposable
    ‣ LoRD: Low Rank Decomposition of monolingual code LLMs for one-shot compression"))，涵盖了不同参数规模，并确定了通过分解减少模型秩的潜力。然后，我们研究了变压器块中不同种类的线性层的这些趋势，并观察到在困惑度变化小于
    1% 的情况下，最多可减少 39.58% 的秩。'
- en: 'We propose various considerations for compressing the models and to achieve
    inference speedup on GPUs (§[3.1](#S3.SS1 "3.1 Achieving compression and inference
    speedup ‣ 3 Compression and speedup through Decomposition ‣ LoRD: Low Rank Decomposition
    of monolingual code LLMs for one-shot compression")). Using these, we achieve
    compression of the StarCoder 16B model offering 31.67 HumanEval Chen et al. ([2021a](#bib.bib10))
    Pass@1 score down to 13.2B parameter with similar performance of 31.57 HumanEval
    and down to 12.3B parameter with 29.22 HumanEval score (§[3.2](#S3.SS2 "3.2 Performance
    of compressed models ‣ 3 Compression and speedup through Decomposition ‣ LoRD:
    Low Rank Decomposition of monolingual code LLMs for one-shot compression")). LoRD
    models, offer an inference speedup of as high as 22.35% with just one line of
    change in huggingface’s (§[3.3](#S3.SS3 "3.3 Speedup from LoRD ‣ 3 Compression
    and speedup through Decomposition ‣ LoRD: Low Rank Decomposition of monolingual
    code LLMs for one-shot compression")).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了多种压缩模型的考虑方案，并在 GPUs 上实现了推理加速（§[3.1](#S3.SS1 "3.1 实现压缩和推理加速 ‣ 3 通过分解实现的压缩和加速
    ‣ LoRD：用于一次性压缩的单语代码 LLM 的低秩分解")）。利用这些方法，我们实现了对 StarCoder 16B 模型的压缩，将 HumanEval
    Chen 等人（[2021a](#bib.bib10)）的 Pass@1 分数从 31.67 降至 13.2B 参数，同时保持类似的 31.57 HumanEval
    性能，降至 12.3B 参数时获得 29.22 HumanEval 分数（§[3.2](#S3.SS2 "3.2 压缩模型的性能 ‣ 3 通过分解实现的压缩和加速
    ‣ LoRD：用于一次性压缩的单语代码 LLM 的低秩分解")）。LoRD 模型通过仅在 huggingface 的代码中进行一行修改，实现了高达 22.35%
    的推理加速（§[3.3](#S3.SS3 "3.3 LoRD 带来的加速 ‣ 3 通过分解实现的压缩和加速 ‣ LoRD：用于一次性压缩的单语代码 LLM
    的低秩分解")）。
- en: 'These LoRD models can be further compressed via the near-lossless quantization
    method of SpQR Dettmers et al. ([2023b](#bib.bib18)) to reduce it’s precision
    to 8 and 4 bits without any further reduction in HumanEval performance (§[4.1](#S4.SS1
    "4.1 Quantization ‣ 4 Combining LoRD with Quantization and LoRA ‣ LoRD: Low Rank
    Decomposition of monolingual code LLMs for one-shot compression")). Finally, these
    decomposed models also reduce the memory requirements of adapter finetuning by
    21.2% over QLoRA (§[4.2](#S4.SS2 "4.2 Parameter Efficient tuning of LoRD models
    ‣ 4 Combining LoRD with Quantization and LoRA ‣ LoRD: Low Rank Decomposition of
    monolingual code LLMs for one-shot compression")).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 LoRD 模型还可以通过 SpQR Dettmers 等人（[2023b](#bib.bib18)）的近无损量化方法进一步压缩，将其精度降低到 8
    位和 4 位，而 HumanEval 性能没有进一步下降（§[4.1](#S4.SS1 "4.1 量化 ‣ 4 结合 LoRD、量化和 LoRA ‣ LoRD：用于一次性压缩的单语代码
    LLM 的低秩分解")）。最后，这些分解后的模型还将适配器微调的内存需求减少了 21.2%，相较于 QLoRA（§[4.2](#S4.SS2 "4.2 LoRD
    模型的参数高效调优 ‣ 4 结合 LoRD、量化和 LoRA ‣ LoRD：用于一次性压缩的单语代码 LLM 的低秩分解")）。
- en: 2 Code LLMs are Low Rank Decomposable
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 代码 LLM 是低秩可分解的
- en: 2.1 Background
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 背景
- en: Let an linear layer $L$
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 设线性层 $L$
- en: A Low Rank Decomposition or Low Rank Factorization of a layer $L$ is,
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 层 $L$ 的低秩分解或低秩因式分解是，
- en: '|  | $Y=\tilde{L}(X)=BAX+\tilde{b}\approx L(X)=WX+b$ |  | (1) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y=\tilde{L}(X)=BAX+\tilde{b}\approx L(X)=WX+b$ |  | (1) |'
- en: Singular Value Decomposition (SVD) offers the best $r$ as follows
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值分解（SVD）提供了最佳的 $r$，如下所示
- en: '|  | $1$2 |  | (2) |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: where [:a,:b] denotes a slice operation over a matrix that gives its first $a$
    columns.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 `[:a,:b]` 表示对矩阵进行切片操作，得到其前 $a$ 列。
- en: 'Eigendecomposition is another decomposition method applicable to symmetric
    matrices. We can represent the eigendecomposition of a symmetric matrix $W\in\mathbb{R}^{d_{1}\times
    d_{1}}$ eigenvalues (and corresponding eigenvectors) as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值分解是另一种适用于对称矩阵的分解方法。我们可以将对称矩阵 $W\in\mathbb{R}^{d_{1}\times d_{1}}$ 的特征值分解（及相应的特征向量）表示如下：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: Since $Q$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 $Q$。
- en: While SVD gives the optimal low-rank decomposition of matrix, in terms of Frobenius
    norm, but does not take input and output data distribution into account. Approaches
    like weighted SVD (Hsu et al., [2022](#bib.bib28)) and SVD over both weight and
    data (Chen et al., [2021b](#bib.bib11)) have been proposed but are prohibitively
    expensive to scale to larger models for their requirement of backpropagation over
    calibration dataset. SVD over very large weight matrices is also very computationally
    expensive. So, we instead leverage the observation that activations in transformers
    are low-ranked (Feng et al., [2022](#bib.bib22)) and adapt the more heuristically
    driven approach of Atomic Feature Mimicking (AFM) (Yu & Wu, [2023](#bib.bib69))
    that creates low rank matrices conditioned on a small amount of calibration data.
    Specifically, consider the eigen-decomposition of Covariance over $Y$ as
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SVD在Frobenius范数下提供了矩阵的最佳低秩分解，但没有考虑输入和输出数据分布。已提出像加权SVD（Hsu等，[2022](#bib.bib28)）和对权重和数据的SVD（Chen等，[2021b](#bib.bib11)）等方法，但由于需要在校准数据集上进行反向传播，这些方法在扩展到更大模型时成本过高。对非常大的权重矩阵进行SVD也是计算上非常昂贵的。因此，我们利用观察到变换器中的激活是低秩的（Feng等，[2022](#bib.bib22)），并采用更具启发性的Atomic
    Feature Mimicking（AFM）（Yu & Wu，[2023](#bib.bib69)）方法，该方法在少量校准数据的条件下创建低秩矩阵。具体地，考虑对$Y$的协方差进行特征分解，如下：
- en: '|  | $\mathbb{E}[yy^{T}]-\mathbb{E}[y]\mathbb{E}[y]^{T}=\hat{Q}\hat{\Lambda}\hat{Q}^{T}$
    |  | (4) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbb{E}[yy^{T}]-\mathbb{E}[y]\mathbb{E}[y]^{T}=\hat{Q}\hat{\Lambda}\hat{Q}^{T}$
    |  | (4) |'
- en: 'Here $\hat{Q}$ from Equation [1](#S2.E1 "In 2.1 Background ‣ 2 Code LLMs are
    Low Rank Decomposable ‣ LoRD: Low Rank Decomposition of monolingual code LLMs
    for one-shot compression"), we have:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '在方程[1](#S2.E1 "在2.1背景 ‣ 2代码LLMs是低秩可分解的 ‣ LoRD: 单语代码LLMs的低秩分解用于一次压缩")中，我们有：'
- en: '|  | $Y\approx\hat{Q}_{:,:r}\hat{Q}_{:,:r}^{T}WX+\hat{Q}_{:,:r}\hat{Q}_{:,:r}^{T}b$
    |  | (5) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $Y\approx\hat{Q}_{:,:r}\hat{Q}_{:,:r}^{T}WX+\hat{Q}_{:,:r}\hat{Q}_{:,:r}^{T}b$
    |  | (5) |'
- en: 'Comparing to Equation [1](#S2.E1 "In 2.1 Background ‣ 2 Code LLMs are Low Rank
    Decomposable ‣ LoRD: Low Rank Decomposition of monolingual code LLMs for one-shot
    compression"), this gives us $B=\hat{Q}_{:,:r}\in\mathbb{R}^{d_{1}\times r}$ to
    zero vector.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '与方程[1](#S2.E1 "在2.1背景 ‣ 2代码LLMs是低秩可分解的 ‣ LoRD: 单语代码LLMs的低秩分解用于一次压缩")相比，这给我们提供了$B=\hat{Q}_{:,:r}\in\mathbb{R}^{d_{1}\times
    r}$到零向量。'
- en: 2.2 Experimental Settings
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 实验设置
- en: We take our python calibration dataset from the stack (Kocetkov et al., [2022](#bib.bib34))
    and consider the corresponding subset of the stack smol (Bigcode, [2022](#bib.bib5))
    as validation data. We filter out those sequences which are less than 1024 tokens
    or 10240 characters in length. We consider CodeGen and StarCoder model family
    of models. CodeGen mono models are present across 350M, 2B, 6B and 16B parameters
    and are CodeGen models that were further trained on only python code. StarCoder
    16B is the StarCoderBase 16B model further trained on only python code from the
    stack dataset’s train split. We also consider StarCoderBase at 3B and 7B parameter
    sizes in StarCoder family due to the lack of their monolingual counterparts. All
    our experiments were performed on a single A100 GPU in under an hour for each
    run.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从堆栈中提取了我们的python校准数据集（Kocetkov等，[2022](#bib.bib34)），并将堆栈smol（Bigcode，[2022](#bib.bib5)）的相应子集视为验证数据。我们过滤掉那些少于1024个标记或10240个字符的序列。我们考虑CodeGen和StarCoder模型系列。CodeGen单语模型包括350M、2B、6B和16B参数，并且是CodeGen模型，这些模型进一步仅在python代码上进行了训练。StarCoder
    16B是StarCoderBase 16B模型，进一步仅在python代码上进行了训练，数据来自堆栈数据集的训练分割。由于缺乏单语对照模型，我们还考虑了StarCoderBase的3B和7B参数大小。我们所有的实验都在一台A100
    GPU上进行，每次运行时间不到一个小时。
- en: 'For studying the trends of increase in perplexity for a reduction in rank across
    difference model sizes, we set a fixed low-rank $r$ for all the layers. Later
    we discuss how to achieve compression and inference speedup via low-rank decomposition
    in §[3](#S3 "3 Compression and speedup through Decomposition ‣ LoRD: Low Rank
    Decomposition of monolingual code LLMs for one-shot compression")'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '为了研究不同模型大小下秩减少对困惑度增加的趋势，我们为所有层设置了固定的低秩$r$。稍后我们在§[3](#S3 "3通过分解实现压缩和加速 ‣ LoRD:
    单语代码LLMs的低秩分解用于一次压缩")中讨论如何通过低秩分解实现压缩和推理加速。'
- en: 2.3 Change in Perplexity across Reduction in Rank
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 困惑度在秩减少下的变化
- en: 'Figure [1(a)](#S2.F1.sf1 "In Figure 1 ‣ 2.3 Change in Perplexity across Reduction
    in Rank ‣ 2 Code LLMs are Low Rank Decomposable ‣ LoRD: Low Rank Decomposition
    of monolingual code LLMs for one-shot compression") and [1(b)](#S2.F1.sf2 "In
    Figure 1 ‣ 2.3 Change in Perplexity across Reduction in Rank ‣ 2 Code LLMs are
    Low Rank Decomposable ‣ LoRD: Low Rank Decomposition of monolingual code LLMs
    for one-shot compression") show the trends of increase in perplexity across reduction
    in rank of the weight matrix of CodeGen and StarCoder models. For the largest
    models in both families, we observe only about a 1% increase in perplexity for
    10% reduction in rank, and upto 35% reduction in rank for less than 10% increase
    in perplexity. The smallest model, CodeGen Mono 350M, however, can only be decomposed
    to 35% rank reduction for a similar drop in perplexity. We observe that the perplexity
    changes much slower for larger models as the % rank reduces, and hence can be
    compressed mode, similar to observations in quantization and pruning (Li et al.,
    [2020](#bib.bib39)). It should be noted that for most models, more than 50% leads
    to significant output quality degradation.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1(a)](#S2.F1.sf1 "在图1 ‣ 2.3 下降秩的困惑度变化 ‣ 2 代码LLM是低秩可分解的 ‣ LoRD：单语代码LLM的低秩分解用于一次性压缩")
    和 [1(b)](#S2.F1.sf2 "在图1 ‣ 2.3 下降秩的困惑度变化 ‣ 2 代码LLM是低秩可分解的 ‣ LoRD：单语代码LLM的低秩分解用于一次性压缩")
    显示了CodeGen和StarCoder模型的权重矩阵在减少秩后的困惑度变化趋势。在这两个系列中最大的模型中，我们观察到在秩减少10%的情况下，困惑度仅增加约1%，而在秩减少最多35%的情况下，困惑度增加不到10%。然而，最小的模型CodeGen
    Mono 350M只能在类似的困惑度下降下减少35%的秩。我们观察到，较大的模型在秩减少时困惑度变化较慢，因此可以压缩，类似于量化和剪枝中的观察（Li等，[2020](#bib.bib39)）。需要注意的是，对于大多数模型，超过50%的秩减少会导致输出质量显著下降。
- en: '![Refer to caption](img/bd9c2529ae30ed569d285bb044674680.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/bd9c2529ae30ed569d285bb044674680.png)'
- en: (a) Perplexity vs % Rank Reduction for CodeGen Models.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CodeGen模型的困惑度与秩减少百分比。
- en: '![Refer to caption](img/4ae5b32fb73829eb978e938cab5f61d2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/4ae5b32fb73829eb978e938cab5f61d2.png)'
- en: (b) Perplexity vs % Rank Reduction for StarCoder Models.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: (b) StarCoder模型的困惑度与秩减少百分比。
- en: 'Figure 1: Perplexity vs %Reduction in Rank for Different Models.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：不同模型的困惑度与秩减少百分比。
- en: 3 Compression and speedup through Decomposition
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 通过分解实现压缩和加速
- en: In this section, we discuss how we adapt the Low Rank Decomposition (LoRD) for
    reducing the size of model and achieving inference speedup without a significant
    reduction in the output quality of the model. Following (Kim et al., [2023a](#bib.bib32)),
    we assume memory bandwidth is the bottleneck for inference, and thus speedups
    for decoding are directly proportional to the size of the transformer model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了如何调整低秩分解（LoRD）以减少模型的大小并在不显著降低模型输出质量的情况下实现推理加速。参考（Kim et al., [2023a](#bib.bib32)），我们假设内存带宽是推理的瓶颈，因此解码的加速与变换器模型的大小成正比。
- en: 3.1 Achieving compression and inference speedup
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 实现压缩和推理加速
- en: 'Threshold for size reduction across rank reduction: Consider a weight matrix
    $W\in\mathbb{R}^{d_{1}\times d_{2}}$.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 跨秩减少的大小缩减阈值：考虑一个权重矩阵 $W\in\mathbb{R}^{d_{1}\times d_{2}}$。
- en: 'Matrix Aspect Ratio and Compression: Let the ratio of the smaller dimension
    to the larger dimension of the matrix (i.e. the aspect ratio) be $\alpha=\frac{d_{min}}{d_{max}}$
    and aspect ratio as:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵纵横比和压缩：设矩阵的较小维度与较大维度的比值（即纵横比）为 $\alpha=\frac{d_{min}}{d_{max}}$，纵横比为：
- en: '|  | $1$2 |  | (6) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: It should be noted that change in parameters from decomposition can either be
    positive (the number of parameters increased after decomposition), or negative
    (the number of parameters decreased after decomposition). In order to achieve
    model compression and consequently inference speedups, one would want a very high
    negative percentage change in parameters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，分解后参数的变化可以是正值（分解后参数数量增加），也可以是负值（分解后参数数量减少）。为了实现模型压缩和推理加速，期望参数的负百分比变化非常高。
- en: '![Refer to caption](img/2c9af56480f21a451c1f2fb1ec5c576a.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/2c9af56480f21a451c1f2fb1ec5c576a.png)'
- en: 'Figure 2: Parity Point across various aspect ratios ($\alpha$) of the different
    linear layers in transformers.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：不同变换器中不同线性层的各种纵横比（$\alpha$）的等效点。
- en: 'Parity Point for Compression across Rank Reduction: Using Eq. [6](#S3.E6 "In
    3.1 Achieving compression and inference speedup ‣ 3 Compression and speedup through
    Decomposition ‣ LoRD: Low Rank Decomposition of monolingual code LLMs for one-shot
    compression"), one can observe that little reduction in rank, may lead to increase
    in model parameters instead of decreasing. For instance, square matrices ($\alpha=1$),
    can be achieved with a very small percent reduction in rank and can start giving
    a reduction in model size. For compression to be achieved, we would want to reduce
    the rank by an amount to cross this parity point threshold. However, reducing
    the rank by a lot can degrade performance significantly. So we must take the aspect
    ratio into account, in order to achieve compression without much reduction in
    rank (and hence no significant degradation in output quality)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩的奇偶点与秩减少的关系：使用公式 [6](#S3.E6 "在 3.1 实现压缩和推理加速 ‣ 3 通过分解实现压缩和加速 ‣ LoRD：单语代码 LLM
    的低秩分解以实现一次性压缩")，可以观察到，秩减少很小可能会导致模型参数增加而不是减少。例如，方阵（$\alpha=1$）可以通过非常小的秩减少达到，并可以开始减少模型大小。为了实现压缩，我们希望减少的秩量能跨越这一奇偶点阈值。然而，过多减少秩可能会显著降低性能。因此，我们必须考虑纵横比，以便在不大幅减少秩（从而不显著降低输出质量）的情况下实现压缩。
- en: A transformer model had different aspect ratios across its various linear layers,
    $\alpha=1.00$.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型在其各个线性层中具有不同的纵横比，$\alpha=1.00$。
- en: '![Refer to caption](img/181d46139420d85ad17a1fa4b7b6c683.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/181d46139420d85ad17a1fa4b7b6c683.png)'
- en: (a) CodeGen 16B.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: (a) CodeGen 16B。
- en: '![Refer to caption](img/fb11531efbd751fa1de8aba86746af7f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/fb11531efbd751fa1de8aba86746af7f.png)'
- en: (b) StarCoder 16B.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: (b) StarCoder 16B。
- en: 'Figure 3: Parameter Reduction vs perplexity for decomposition across various
    layers.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同层中参数减少与困惑度的关系。
- en: 'Trends across different layers in a transformer block:'
  id: totrans-66
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 Transformer 块中的不同层的趋势：
- en: 'In addition to considering the parity point into account for deciding which
    layers to decompose, we also additionally study the sensitivity of each of these
    layers to low rank decomposition across the large model in the two model families.
    Figure [3](#S3.F3 "Figure 3 ‣ 3.1 Achieving compression and inference speedup
    ‣ 3 Compression and speedup through Decomposition ‣ LoRD: Low Rank Decomposition
    of monolingual code LLMs for one-shot compression") shows the increase in perplexity
    vs reduction in model parameters for the two models. For both models, decomposing
    all the linear layers achieves the parity point much later than any one of these
    linear layers with low aspect ratio. For CodeGen, the attention weight matrix
    (query, key and values projection) offers least increase in perplexity for the
    biggest drop in parameter count, make this layer the most suitable candidate to
    be decomposed. It shows less than 1% increase in perplexity even after 39.58%
    rank reduction. We observe the mlp 2 (downscaling mlp) to be a better candidate
    for decomposition than mlp 1 (upscaling mlp) across both models. This makes mlp
    2 to be a good candidate for low-rank decomposition over the StarCoder model.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了考虑奇偶点以决定哪些层需要分解之外，我们还进一步研究了这两种模型家族中大模型中每层对低秩分解的敏感性。图 [3](#S3.F3 "图 3 ‣ 3.1
    实现压缩和推理加速 ‣ 3 通过分解实现压缩和加速 ‣ LoRD：单语代码 LLM 的低秩分解以实现一次性压缩") 显示了两个模型中困惑度的增加与模型参数减少之间的关系。对于这两个模型，分解所有线性层的实现比任何一个低纵横比的线性层更晚地实现奇偶点。对于
    CodeGen，注意力权重矩阵（查询、键和值的投影）在参数数量大幅减少时困惑度的增加最小，使得这个层成为最适合分解的候选者。即使在 39.58% 的秩减少后，它的困惑度增加也不到
    1%。我们观察到，在这两个模型中，mlp 2（下采样 mlp）比 mlp 1（上采样 mlp）更适合分解。因此，mlp 2 成为 StarCoder 模型上进行低秩分解的一个良好候选者。
- en: 'Hardware Considerations: On modern hardware accelerators like GPU and their
    corresponding software stack, matrix multiplication kernels are faster if their
    dimensions are divisible by a high factor of 2\. So, we consider ranks at a reduction
    of approximately every 10%, rounded off to the nearest multiple of 128 in our
    experiments.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件考虑：在现代硬件加速器如 GPU 及其相应的软件栈上，如果矩阵乘法内核的维度能被较高的 2 的因子整除，则其速度会更快。因此，在我们的实验中，我们考虑的秩大约减少了每
    10%，并四舍五入到最接近的 128 的倍数。
- en: 3.2 Performance of compressed models
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 压缩模型的性能
- en: 'We consider the largest models of StarCoder and CodeGen family (16B) and perform
    low-rank decomposition on both with varying ranks. We consider decomposing layers
    that offers most parameter reduction (§[3.1](#S3.SS1 "3.1 Achieving compression
    and inference speedup ‣ 3 Compression and speedup through Decomposition ‣ LoRD:
    Low Rank Decomposition of monolingual code LLMs for one-shot compression")) with
    least increase in perplexity - mlp 2 for StarCoder and attention for CodeGen.
    We report the Pass@1 and Pass@10 scores over the Human Eval dataset (Chen et al.,
    [2021a](#bib.bib10)) using the code-eval GitHub repo (Bacaj, [2023](#bib.bib2))
    in Table [1](#S3.T1 "Table 1 ‣ 3.3 Speedup from LoRD ‣ 3 Compression and speedup
    through Decomposition ‣ LoRD: Low Rank Decomposition of monolingual code LLMs
    for one-shot compression"). We observe that StarCoder models can be low rank decomposed
    to 13.2B parameters (50% rank reduction) with no drop in Pass@1 performance and
    upto 12.3B parameters (62.5% rank reduction) with very little drop. CodeGen models
    shows similar trend in drop in Human Eval performance when measured in terms of
    rank reduction. However, in terms of parameter reduction count, while showing
    very little perplexity change with large reduction in rank (Fig. [3(a)](#S3.F3.sf1
    "In Figure 3 ‣ 3.1 Achieving compression and inference speedup ‣ 3 Compression
    and speedup through Decomposition ‣ LoRD: Low Rank Decomposition of monolingual
    code LLMs for one-shot compression")), shows much more drop in its HumanEval score
    when measured in terms of parameter count reduction due to a higher aspect ratio
    of the matrix being decomposed. It should be noted that for certain compressed
    models, the Pass@1 even slightly improves over the base model. Similar trend of
    slight improvements from compression across various metrics and benchmarks has
    been observed in the case of other compression attempts (Frantar & Alistarh, [2023](#bib.bib23);
    Cerebras, [2022](#bib.bib7)).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '我们考虑StarCoder和CodeGen家族中最大的模型（16B），并对这两个模型进行不同秩的低秩分解。我们考虑对那些在减少参数方面效果最佳的层进行分解
    (§[3.1](#S3.SS1 "3.1 实现压缩和推理加速 ‣ 3 通过分解实现压缩和加速 ‣ LoRD: 单语代码LLMs的低秩分解实现一次性压缩"))，以最小的困惑度增加
    - StarCoder选择mlp 2，CodeGen选择attention。我们在Human Eval数据集上报告了Pass@1和Pass@10分数（Chen
    et al., [2021a](#bib.bib10)），使用了code-eval GitHub仓库（Bacaj, [2023](#bib.bib2)），见表[1](#S3.T1
    "表1 ‣ 3.3 LoRD的加速 ‣ 3 通过分解实现压缩和加速 ‣ LoRD: 单语代码LLMs的低秩分解实现一次性压缩")。我们观察到，StarCoder模型可以低秩分解到13.2B参数（50%秩减少），Pass@1性能没有下降，最多可以减少到12.3B参数（62.5%秩减少），性能下降非常小。CodeGen模型在秩减少时表现出类似的Human
    Eval性能下降趋势。然而，在参数减少的计数方面，尽管在秩减少方面变化不大（图[3(a)](#S3.F3.sf1 "在图3 ‣ 3.1 实现压缩和推理加速
    ‣ 3 通过分解实现压缩和加速 ‣ LoRD: 单语代码LLMs的低秩分解实现一次性压缩")），由于矩阵被分解后的高长宽比，Human Eval分数却有更多下降。值得注意的是，对于某些压缩模型，Pass@1甚至略有提高。其他压缩尝试中也观察到各种指标和基准下的轻微改进趋势（Frantar
    & Alistarh, [2023](#bib.bib23); Cerebras, [2022](#bib.bib7)）。'
- en: 3.3 Speedup from LoRD
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 LoRD的加速
- en: We next consider accessing the inference speedup (forward pass) of the models
    over the standard cuBLAS floating point kernels. We consider the standard Huggingface
    implementation (Wolf et al., [2020](#bib.bib65)) of Starcoder with pytorch backend
    (Paszke et al., [2019](#bib.bib45)) utilizing standard cuBLAS kernels on A100
    GPUs. LoRD decomposed models were implemented by modifying just one line of code
    to replace an MLP with an extra linear layer ³³3nn.Linear(in, out) -> nn.Sequential(nn.Linear(in,
    rank), nn.Linear(rank, out)). We benchmark over 1024 tokens and 512 tokens sequence,
    averaged across 10 runs with warm up of 3 runs. We plot relative time taken and
    model size across reduction in rank in Figure
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们考虑在标准的cuBLAS浮点内核上访问模型的推理加速（前向传播）。我们考虑使用标准的Huggingface实现（Wolf et al., [2020](#bib.bib65)），以pytorch后端（Paszke
    et al., [2019](#bib.bib45)）利用标准的cuBLAS内核在A100 GPU上进行Starcoder的实现。LoRD分解模型通过修改一行代码实现，将MLP替换为额外的线性层³³3nn.Linear(in,
    out) -> nn.Sequential(nn.Linear(in, rank), nn.Linear(rank, out))。我们在1024个tokens和512个tokens序列上进行基准测试，平均经过10次运行，包含3次预热。我们绘制了时间相对变化和模型大小与秩减少的关系，如图所示。
- en: '![Refer to caption](img/8610068d53e1dfd108bec771b1f30fa5.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8610068d53e1dfd108bec771b1f30fa5.png)'
- en: 'Figure 4: Time and Model size of StarCoder 16B across ranks.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：StarCoder 16B在不同秩下的时间和模型大小。
- en: '[4](#S3.F4 "Figure 4 ‣ 3.3 Speedup from LoRD ‣ 3 Compression and speedup through
    Decomposition ‣ LoRD: Low Rank Decomposition of monolingual code LLMs for one-shot
    compression").'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '[4](#S3.F4 "图4 ‣ 3.3 从LoRD的加速 ‣ 3 通过分解的压缩和加速 ‣ LoRD：对单语代码LLMs进行一次性压缩的低秩分解").'
- en: Inference speedups as high as 22.35% are observed for decomposed models. The
    lines in the graph are generally downward sloping, Therefore reduction in rank
    beyond 25% generally implies less inference time and reduction in model size.
    However, the underlying hardware (and pertaining software kernels) also significantly
    affect the speedup gains. We notice huge gains, whenever the rank is rounded off
    to a multiple of a very high power of 2 (like 4096 and 2560 at 33% and 58% rank
    reduction), despite very little reduction in model size. In contrast, for certain
    ranks which are multiples of a lesser power of 2 (like 3584 and 2304 at 41% and
    62% rank reduction) are slower than those at slightly higher ranks. It is worth
    noting that affect of hardware inefficient matrix shape is less significant for
    longer tokens sequence of 1024 because the $O(n^{2})$ attention overhead starts
    becoming more significant, especially in the absence of SoTA attention implementation
    techniques (Rabe & Staats, [2021](#bib.bib48); Dao et al., [2022](#bib.bib14);
    Dao, [2023](#bib.bib13)) as in the case of Huggingface’s implementations.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分解模型，观察到的推理加速高达22.35%。图中的线条通常呈下降趋势，因此，超过25%的排名降低通常意味着推理时间减少和模型大小减少。然而，底层硬件（以及相关的软件内核）也显著影响加速效果。我们注意到，每当排名被四舍五入到非常高的2的幂次的倍数（例如，33%和58%排名降低时的4096和2560），尽管模型大小减少很少，依然会获得巨大的提升。相比之下，对于某些是较低2的幂次倍数（例如，41%和62%排名降低时的3584和2304）的排名，其速度较高排名的模型要慢。值得注意的是，对于长度为1024的长序列，硬件低效的矩阵形状的影响较小，因为$O(n^{2})$注意力开销开始变得更加显著，特别是在缺少SoTA注意力实现技术（Rabe
    & Staats, [2021](#bib.bib48); Dao et al., [2022](#bib.bib14); Dao, [2023](#bib.bib13)）的情况下，这在Huggingface的实现中尤为明显。
- en: '| Starcoder 16B | CodeGen 16B Mono |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| Starcoder 16B | CodeGen 16B Mono |'
- en: '| --- | --- |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Model Type | Rank | HumanEval Score | Model Type | Rank | HumanEval Score
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 模型类型 | 排名 | 人类评估分数 | 模型类型 | 排名 | 人类评估分数 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '|  | Pass @ 1 | Pass @ 10 |  | Pass @ 1 | Pass @ 10 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | 通过 @ 1 | 通过 @ 10 |  | 通过 @ 1 | 通过 @ 10 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Base Model | 6144 | 31.67 | 48.28 | Base Model | 6144 | 29.02 | 46.34 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 基础模型 | 6144 | 31.67 | 48.28 | 基础模型 | 6144 | 29.02 | 46.34 |'
- en: '| LoRDCoder 14.9B | 4480 | 33.18 | 48.41 | LoRDCoder 15.9B | 4480 | 29.08 |
    46.95 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 14.9B | 4480 | 33.18 | 48.41 | LoRDCoder 15.9B | 4480 | 29.08 |
    46.95 |'
- en: '| LoRDCoder 14.5B | 4096 | 31.69 | 45.12 | LoRDCoder 15.6B | 4096 | 28.90 |
    46.24 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 14.5B | 4096 | 31.69 | 45.12 | LoRDCoder 15.6B | 4096 | 28.90 |
    46.24 |'
- en: '| LoRDCoder 13.8B | 3584 | 30.90 | 47.56 | LoRDCoder 15.1B | 3584 | 28.54 |
    45.73 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 13.8B | 3584 | 30.90 | 47.56 | LoRDCoder 15.1B | 3584 | 28.54 |
    45.73 |'
- en: '| LoRDCoder 13.2B | 3072 | 31.57 | 45.36 | LoRDCoder 14.7B | 3072 | 27.99 |
    43.29 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 13.2B | 3072 | 31.57 | 45.36 | LoRDCoder 14.7B | 3072 | 27.99 |
    43.29 |'
- en: '| LoRDCoder 12.6B | 2560 | 29.84 | 42.31 | LoRDCoder 14.3B | 2560 | 27.32 |
    45.12 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 12.6B | 2560 | 29.84 | 42.31 | LoRDCoder 14.3B | 2560 | 27.32 |
    45.12 |'
- en: '| LoRDCoder 12.3B | 2304 | 29.22 | 40.12 | LoRDCoder 14.1B | 2304 | 27.07 |
    41.46 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 12.3B | 2304 | 29.22 | 40.12 | LoRDCoder 14.1B | 2304 | 27.07 |
    41.46 |'
- en: 'Table 1: Human Eval Score of LoRD across StarCoder and CodeGen.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：LoRD在StarCoder和CodeGen中的人类评估分数。
- en: 4 Combining LoRD with Quantization and LoRA
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 将LoRD与量化和LoRA结合
- en: 4.1 Quantization
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 量化
- en: 'While LoRD enables compression at same precision level, we study whether the
    decomposed models can be further compressing through quantization. Table [2](#S4.T2
    "Table 2 ‣ 4.1 Quantization ‣ 4 Combining LoRD with Quantization and LoRA ‣ LoRD:
    Low Rank Decomposition of monolingual code LLMs for one-shot compression") shows
    the HumanEval pass@1 results for the different LoRDCoder across 8 and 4 bit quantization
    levels, using the near lossless quantization technique of SpQR (Dettmers et al.,
    [2023b](#bib.bib18)). We observe that the LoRD models can be combined with quantization
    for further compression, showing no performance drop for 8-bit and very little
    performance drop on 4-bit quantization for most models. Slight increase in HumanEval
    after quantization is also observed, similar to Pangu-Coder2 (Shen et al., [2023](#bib.bib56)).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 LoRD 使得在相同精度水平下的压缩成为可能，我们研究了分解后的模型是否可以通过量化进一步压缩。表 [2](#S4.T2 "表 2 ‣ 4.1 量化
    ‣ 4 结合 LoRD 与量化和 LoRA ‣ LoRD：单语代码 LLMs 的低秩分解用于一次性压缩") 展示了不同 LoRDCoder 在 8-bit
    和 4-bit 量化水平下的 HumanEval pass@1 结果，使用的是接近无损的 SpQR 量化技术 (Dettmers 等， [2023b](#bib.bib18))。我们观察到，LoRD
    模型可以与量化结合以进一步压缩，对大多数模型而言，8-bit 量化下没有性能下降，4-bit 量化下性能下降也很小。量化后的 HumanEval 结果略有提高，类似于
    Pangu-Coder2 (Shen 等， [2023](#bib.bib56))。
- en: '| Model | Pass@1@FP16 | Pass@1@8-bit | Pass@1@4-bit |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | Pass@1@FP16 | Pass@1@8-bit | Pass@1@4-bit |'
- en: '| --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| LoRDCoder 14.9B | 33.18 | 33.17 | 32.01 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 14.9B | 33.18 | 33.17 | 32.01 |'
- en: '| LoRDCoder 14.5B | 31.69 | 31.58 | 32.74 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 14.5B | 31.69 | 31.58 | 32.74 |'
- en: '| LoRDCoder 13.8B | 30.90 | 31.10 | 30.73 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 13.8B | 30.90 | 31.10 | 30.73 |'
- en: '| LoRDCoder 13.2B | 31.57 | 31.52 | 32.01 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 13.2B | 31.57 | 31.52 | 32.01 |'
- en: '| LoRDCoder 12.6B | 29.84 | 29.87 | 30.22 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 12.6B | 29.84 | 29.87 | 30.22 |'
- en: '| LoRDCoder 12.3B | 29.22 | 29.14 | 29.45 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| LoRDCoder 12.3B | 29.22 | 29.14 | 29.45 |'
- en: 'Table 2: Human Eval score of quantized LoRDCoder models.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 量化 LoRDCoder 模型的人工评估得分。'
- en: 4.2 Parameter Efficient tuning of LoRD models
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 LoRD 模型的参数高效调整
- en: '![Refer to caption](img/203ddbd25d81fdb7bde040bd7f5740ed.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/203ddbd25d81fdb7bde040bd7f5740ed.png)'
- en: 'Figure 5: LoRA vs LoRD + LoRA.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: LoRA 与 LoRD + LoRA 对比。'
- en: We next test the potential for using LoRD to further reduce the memory usage
    over existing parameter-efficient techniques. We consider the code instruction
    dataset (Chaudhary, [2023](#bib.bib8)) and filter those examples that pertains
    to python programming language. We use QLoRA (Dettmers et al., [2023a](#bib.bib17)),
    which is an even more memory efficient version of LoRA (Hu et al., [2022](#bib.bib29))
    storing the weights in quantized format, for fine-tuning for 1 epoch. We compare
    results from fine-tuning two of the decomposed models LoRDCoder 13.2B and LoRDCoder
    12.3B model to the StarCoder model. We observe a HumanEval pass@1 of 37.80 and
    37.62 across LoRDCoder 13.2B and LoRDCoder 12.3B fine-tuning, competitive to the
    performance of 37.74 offered by StarCoder model.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们测试使用 LoRD 进一步减少现有参数高效技术的内存使用的潜力。我们考虑了代码指令数据集 (Chaudhary, [2023](#bib.bib8))
    并筛选出与 Python 编程语言相关的示例。我们使用 QLoRA (Dettmers 等， [2023a](#bib.bib17))，这是 LoRA (Hu
    等， [2022](#bib.bib29)) 的一个更高效的内存版本，将权重存储在量化格式中，进行 1 个 epoch 的微调。我们将两个分解后的模型 LoRDCoder
    13.2B 和 LoRDCoder 12.3B 的微调结果与 StarCoder 模型进行比较。我们观察到 LoRDCoder 13.2B 和 LoRDCoder
    12.3B 微调后的 HumanEval pass@1 分别为 37.80 和 37.62，与 StarCoder 模型提供的 37.74 的性能相当。
- en: 5 Related Work
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: There is a growing interest in compressing pretrained Large Language Models.
    Several recent attempts have been dedicated to the quantization of weights of
    LLMs (Frantar et al., [2023](#bib.bib24); Lin et al., [2023](#bib.bib41); Yuan
    et al., [2023](#bib.bib70); Park et al., [2022](#bib.bib44); Kim et al., [2023b](#bib.bib33);
    Chee et al., [2023](#bib.bib9); Li et al., [2023a](#bib.bib37)) with tricks such
    as outlier separation (Dettmers et al., [2022](#bib.bib16); Dettmers & Zettlemoyer,
    [2022](#bib.bib15); Dettmers et al., [2023c](#bib.bib19); Wei et al., [2022](#bib.bib64);
    Kim et al., [2023a](#bib.bib32); Lee et al., [2023](#bib.bib36)). Some attempts
    also quantize the activations (intermediate representations) in addition to weights
    to speed up computation time (Shao et al., [2023](#bib.bib53); Xiao et al., [2023](#bib.bib67)).
    The works in quantization that are closest to us is the Low-Rank Compensation
    (LoRC) Strategy (Yao et al., [2023](#bib.bib68); Wu et al., [2023](#bib.bib66)),
    where the difference of the quantized matrix to the original matrix is approximated
    by a product of low-rank matrices. Our work decomposes the entire matrix for compression.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于压缩预训练的大型语言模型的兴趣日益增长。最近的一些尝试致力于 LLM 的权重量化（Frantar et al., [2023](#bib.bib24)；Lin
    et al., [2023](#bib.bib41)；Yuan et al., [2023](#bib.bib70)；Park et al., [2022](#bib.bib44)；Kim
    et al., [2023b](#bib.bib33)；Chee et al., [2023](#bib.bib9)；Li et al., [2023a](#bib.bib37)），包括一些技巧如异常值分离（Dettmers
    et al., [2022](#bib.bib16)；Dettmers & Zettlemoyer, [2022](#bib.bib15)；Dettmers
    et al., [2023c](#bib.bib19)；Wei et al., [2022](#bib.bib64)；Kim et al., [2023a](#bib.bib32)；Lee
    et al., [2023](#bib.bib36)）。一些尝试还量化了激活（中间表示），以加快计算时间（Shao et al., [2023](#bib.bib53)；Xiao
    et al., [2023](#bib.bib67)）。与我们最接近的量化工作是低秩补偿（LoRC）策略（Yao et al., [2023](#bib.bib68)；Wu
    et al., [2023](#bib.bib66)），其中量化矩阵与原始矩阵的差异通过低秩矩阵的乘积来近似。我们的工作则是对整个矩阵进行分解以实现压缩。
- en: Pruning neural networks Liang et al. ([2021](#bib.bib40)), unlike quantization,
    reduces the number of parameters in a model by removing unimportant weights or
    connections. Several techniques have been proposed to scale pruning methods for
    LLMs (Sun et al., [2023a](#bib.bib57); Frantar & Alistarh, [2023](#bib.bib23);
    Ma et al., [2023](#bib.bib42)). However, pruning as a means of compression is
    yet to become viable due to no speedups over sparse matrices without significant
    performance drop at extreme levels of sparsity or structured sparsity (Zhu et al.,
    [2023](#bib.bib72)). With low-rank decomposition, we propose an alternate method
    for reducing model parameters that offer speedup even at a little reduction in
    parameter count. Certain works have also attempted to (Ren & Zhu, [2023](#bib.bib50);
    Li et al., [2023b](#bib.bib38)) to split a dense matrix as a sum of low-rank matrices
    and a sparse matrix. However, these methods require retraining and have been shown
    to work only for Language Models of less than a billion parameters.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝神经网络（Liang et al., [2021](#bib.bib40)），不同于量化，通过去除不重要的权重或连接来减少模型中的参数数量。已提出多种技术来扩展
    LLM 的剪枝方法（Sun et al., [2023a](#bib.bib57)；Frantar & Alistarh, [2023](#bib.bib23)；Ma
    et al., [2023](#bib.bib42)）。然而，剪枝作为一种压缩手段尚未成为可行的方法，因为在极端稀疏性或结构稀疏性下，没有相较于稀疏矩阵的加速，且会导致性能显著下降（Zhu
    et al., [2023](#bib.bib72)）。通过低秩分解，我们提出了一种替代方法来减少模型参数，即使在参数数量略有减少的情况下也能提供加速。某些工作也尝试（Ren
    & Zhu, [2023](#bib.bib50)；Li et al., [2023b](#bib.bib38)）将密集矩阵分割为低秩矩阵和稀疏矩阵的和。然而，这些方法需要重新训练，并且已被证明仅适用于少于十亿参数的语言模型。
- en: Low rank decomposition has been proposed for smaller language models like Bert
    or GPT2 before using SVD decomposition (Ben Noach & Goldberg, [2020](#bib.bib4))
    and Kronecker decompositions (Tahaei et al., [2022](#bib.bib59); Edalati et al.,
    [2022](#bib.bib21)). Hsu et al. ([2022](#bib.bib28)) modified SVD to be data aware
    based on approximate second-order gradient information. A better weighted SVD
    was proposed by (Hua et al., [2022](#bib.bib30)). Chen et al. ([2021b](#bib.bib11))
    proposed a data aware decomposition method with a provably optimal closed-form
    solution, utilizing a large amount of data points over specific tasks to decompose.
    Several recent works (Yu & Wu, [2023](#bib.bib69); Feng et al., [2022](#bib.bib22))
    have shown that while the weight matrix of neural networks is not inherently low-rank,
    the intermediate representations are, thus propose to decompose based on representations.
    All these works have focused on small language models and require re-training.
    We proposed low-rank decomposition for compressing neural networks without the
    need for retraining. The factorization has also been used just for the embedding
    layers (Baevski & Auli, [2019](#bib.bib3); Lan et al., [2020](#bib.bib35)), as
    they are good candidates due to their very low aspect ratio of 0.015, where a
    reduction of rank by even 5% would lead to reduction in number of parameters after
    decomposition.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩分解之前已经被提出用于像 Bert 或 GPT2 这样的小型语言模型，使用 SVD 分解 (Ben Noach & Goldberg, [2020](#bib.bib4))
    和 Kronecker 分解 (Tahaei et al., [2022](#bib.bib59); Edalati et al., [2022](#bib.bib21))。Hsu
    et al. ([2022](#bib.bib28)) 修改了 SVD，使其基于近似的二阶梯度信息对数据进行感知。Hua et al. ([2022](#bib.bib30))
    提出了更好的加权 SVD。Chen et al. ([2021b](#bib.bib11)) 提出了一个数据感知分解方法，具有可证明的最优封闭形式解，利用大量的特定任务数据点进行分解。最近的一些工作
    (Yu & Wu, [2023](#bib.bib69); Feng et al., [2022](#bib.bib22)) 显示，虽然神经网络的权重矩阵本身不是低秩的，但中间表示是低秩的，因此建议基于表示进行分解。所有这些工作都集中在小型语言模型上，并且需要重新训练。我们提出了用于压缩神经网络的低秩分解方法，无需重新训练。该分解方法也仅用于嵌入层
    (Baevski & Auli, [2019](#bib.bib3); Lan et al., [2020](#bib.bib35))，因为它们由于非常低的长宽比
    0.015 而成为良好的候选者，其中即使减少 5% 的秩，也会在分解后导致参数数量的减少。
- en: There is also a growing interest in fine-tuning large language models Taori
    et al. ([2023](#bib.bib60)); Chiang et al. ([2023](#bib.bib12)); Wang et al. ([2023b](#bib.bib63));
    Sun et al. ([2023b](#bib.bib58)). With the large memory requirements for fine-tuning
    full parameters of the LLM, the more parameter-efficient fine-tuning methods like
    LoRA (Hu et al., [2022](#bib.bib29)) are getting widely adopted. These methods
    freeze the original LLM weights, and attach two low-rank matrices or adapters,
    in a skip-connection (He et al., [2016](#bib.bib26)) to the linear layers of the
    model. These parameter-efficient fine-tuning approaches have seen improvements
    in lower activation memory (Zhang et al., [2023](#bib.bib71)) or by keeping non-trainable
    model weights at 4-bit precision (Dettmers et al., [2023a](#bib.bib17)). Our work,
    while focused on compression through low-rank decomposition, can also enable more
    efficient fine-tuning, especially in conjunction with existing methods.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大规模语言模型的微调，越来越多的研究者表现出兴趣 Taori et al. ([2023](#bib.bib60)); Chiang et al.
    ([2023](#bib.bib12)); Wang et al. ([2023b](#bib.bib63)); Sun et al. ([2023b](#bib.bib58))。由于微调整个参数的
    LLM 需要大量的内存，像 LoRA (Hu et al., [2022](#bib.bib29)) 这样更具参数效率的微调方法正被广泛采用。这些方法冻结了原始
    LLM 的权重，并在模型的线性层上附加两个低秩矩阵或适配器，形成跳跃连接 (He et al., [2016](#bib.bib26))。这些参数高效的微调方法在降低激活内存
    (Zhang et al., [2023](#bib.bib71)) 或将不可训练的模型权重保持在 4 位精度 (Dettmers et al., [2023a](#bib.bib17))
    方面取得了改进。我们的工作虽然集中于通过低秩分解进行压缩，但也可以实现更高效的微调，特别是与现有方法结合使用时。
- en: 6 Conclusion
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: We studied the compression of monolingual code generation models through a novel
    one-shot compression paradigm of low-rank decomposition. We analyse the change
    in perplexity with change in rank across the model families of StarCoder and CodeGen
    as well as their individual layers and observe that the rank of these models can
    be reduced by upto 39.58% with less than 1% change in perplexity. We then proposed
    considerations for one-shot compressing these models through Low Rank Decomposition
    (LoRD) in under 10 minutes. Consequently, we compress StarCoder 16B to 13.2B with
    no drop in HumanEval pass@1 and very little drop in HumanEval pass@1 to 12.3B
    parameters. With a minimal change in code over huggingface’s default inference
    code of just one line, we gain speedups of up to 22.35%. The LoRD models are also
    compatible with near lossless quantization techniques of SpQR, which offers gains
    of quantization based compression in addition to ones from decomposition. The
    LoRD models also reduce memory requirements by as much as 21.2% over vanilla QLoRA
    fine-tuning.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过一种新颖的低秩分解一次性压缩范式研究了单语代码生成模型的压缩。我们分析了在StarCoder和CodeGen模型系列以及它们的各个层中，随着秩的变化，困惑度的变化，并观察到这些模型的秩可以减少多达39.58%，而困惑度变化不到1%。然后，我们提出了在10分钟内通过低秩分解（LoRD）对这些模型进行一次性压缩的考虑。因此，我们将StarCoder
    16B压缩到13.2B，HumanEval pass@1没有下降，而HumanEval pass@1在12.3B参数下的下降也非常小。通过对huggingface默认推理代码仅进行一行的最小改动，我们实现了高达22.35%的加速。LoRD模型还与SpQR的几乎无损量化技术兼容，SpQR提供了除分解外的量化压缩增益。LoRD模型还比普通QLoRA微调减少了多达21.2%的内存需求。
- en: 7 Broader Impact and Future Work
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 更广泛的影响与未来工作
- en: Our work on LoRD, compresses code LLMs which enables them to run on smaller
    GPUs including as consumer grade GPUs. This is especially of pressing importance
    for the next few years when the shortage of GPU supply is relative to the increasing
    demand in today’s market. Moreover, faster inference helps reduce the GPU cycles,
    enabling lower running costs and lower power consumption for LLM inference. Our
    work helps reduce the carbon emissions incurred and moves towards a greener NLP.
    Through compression, our work also promotes inference at the edge, and therefore
    opening room for applications involving strict privacy requirements. Lower latency
    will also help improve the User Experience in applications like CoPilots where
    lag between suggestions can impact developer’s productivity. Several of these
    benefits of LoRD such as lower cost and energy consumption are also applicable
    for fine-tuning use cases of LLMs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关于LoRD的工作压缩了代码LLMs，使它们能够在包括消费级GPU在内的较小GPU上运行。这在未来几年尤为重要，因为GPU供应短缺相对于市场上日益增长的需求。此外，更快的推理有助于减少GPU周期，从而降低运行成本和功耗。我们的工作有助于减少碳排放，并向更绿色的NLP迈进。通过压缩，我们的工作还促进了边缘推理，从而为涉及严格隐私要求的应用开辟了空间。更低的延迟也有助于改善像CoPilots这样的应用中的用户体验，因为建议之间的延迟会影响开发者的生产力。LoRD的多个好处，如降低成本和能源消耗，也适用于LLMs的微调用例。
- en: Our work opens up a new paradigm for compression via Low Rank Decomposition
    over Large Language Models in a single shot without the need for retraining. Since,
    LoRD models can leverage existing floating point kernels across BLAS and cuBLAS,
    in contrast to quantization, these are much easier to implement and reap inference
    benefits. Our study on hardware considerations for speedup also opens up the potential
    for tuning the rank of decomposed models to fit best on the target hardware and
    the accompanying GEMM kernels. While our study is limited to monolingual code
    LLMs, the low rank decomposition technique is general and not specific to code
    domain. Thus exploring its applicability to more general purpose models like LLaMa
    is a promising direction for the compression of transformer LLMs beyond quantization.
    Another interesting unexplored question is whether the LoRA or QLoRA modules fine-tuned
    on original models, can be plugged in as-is for the LoRD models without any performance
    drop.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工作开启了一个新的压缩范式，通过对大型语言模型进行低秩分解，一次性完成而无需重新训练。由于LoRD模型可以利用BLAS和cuBLAS中的现有浮点内核，与量化相比，这些模型更容易实现并获得推理收益。我们关于加速硬件的研究也开启了调整分解模型秩以适应目标硬件和伴随GEMM内核的潜力。虽然我们的研究局限于单语言代码LLMs，但低秩分解技术是通用的，并不限于代码领域。因此，探索其在更通用模型如LLaMa上的适用性，是压缩变换器LLMs超越量化的一个有前景的方向。另一个有趣的未探索问题是，LoRA或QLoRA模块在原始模型上微调后，是否可以直接插入LoRD模型而不影响性能。
- en: References
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Agarwal et al. (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela
    Ramos, Matthieu Geist, and Olivier Bachem. Gkd: Generalized knowledge distillation
    for auto-regressive sequence models, 2023.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Agarwal 等 (2023) Rishabh Agarwal, Nino Vieillard, Piotr Stanczyk, Sabela Ramos,
    Matthieu Geist 和 Olivier Bachem。Gkd: 自回归序列模型的广义知识蒸馏，2023年。'
- en: Bacaj (2023) Anton Bacaj. code-eval. [https://github.com/abacaj/code-eval](https://github.com/abacaj/code-eval),
    July 2023.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bacaj (2023) Anton Bacaj. code-eval。 [https://github.com/abacaj/code-eval](https://github.com/abacaj/code-eval)，2023年7月。
- en: Baevski & Auli (2019) Alexei Baevski and Michael Auli. Adaptive input representations
    for neural language modeling. In *International Conference on Learning Representations*,
    2019. URL [https://openreview.net/forum?id=ByxZX20qFQ](https://openreview.net/forum?id=ByxZX20qFQ).
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baevski & Auli (2019) Alexei Baevski 和 Michael Auli. 神经语言建模的自适应输入表示。在*国际学习表征会议*，2019年。网址
    [https://openreview.net/forum?id=ByxZX20qFQ](https://openreview.net/forum?id=ByxZX20qFQ)。
- en: Ben Noach & Goldberg (2020) Matan Ben Noach and Yoav Goldberg. Compressing pre-trained
    language models by matrix decomposition. In *Proceedings of the 1st Conference
    of the Asia-Pacific Chapter of the Association for Computational Linguistics and
    the 10th International Joint Conference on Natural Language Processing*, pp. 884–889,
    Suzhou, China, December 2020\. Association for Computational Linguistics. URL
    [https://aclanthology.org/2020.aacl-main.88](https://aclanthology.org/2020.aacl-main.88).
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ben Noach & Goldberg (2020) Matan Ben Noach 和 Yoav Goldberg. 通过矩阵分解压缩预训练语言模型。在*亚太计算语言学协会首届会议暨第十届国际自然语言处理联合会议论文集*，第884–889页，中国苏州，2020年12月。计算语言学协会。网址
    [https://aclanthology.org/2020.aacl-main.88](https://aclanthology.org/2020.aacl-main.88)。
- en: Bigcode (2022) Project Bigcode. The stack smol, 2022. URL [https://huggingface.co/datasets/bigcode/the-stack-smol](https://huggingface.co/datasets/bigcode/the-stack-smol).
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bigcode (2022) Bigcode项目。The stack smol，2022年。网址 [https://huggingface.co/datasets/bigcode/the-stack-smol](https://huggingface.co/datasets/bigcode/the-stack-smol)。
- en: Blackford et al. (2002) L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin
    Remington, R Clint Whaley, James Demmel, Jack Dongarra, Iain Duff, Sven Hammarling,
    Greg Henry, et al. An updated set of basic linear algebra subprograms (blas).
    *ACM Transactions on Mathematical Software*, 28(2):135–151, 2002.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blackford 等 (2002) L Susan Blackford, Antoine Petitet, Roldan Pozo, Karin Remington,
    R Clint Whaley, James Demmel, Jack Dongarra, Iain Duff, Sven Hammarling, Greg
    Henry 等。更新版基本线性代数子程序 (blas)。*ACM数学软件学报*，28(2):135–151, 2002年。
- en: Cerebras (2022) Team Cerebras. Creating sparse gpt-3 models with iterative pruning,
    11 2022. URL [https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning](https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning).
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cerebras (2022) Cerebras团队。通过迭代修剪创建稀疏的gpt-3模型，2022年11月。网址 [https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning](https://www.cerebras.net/blog/creating-sparse-gpt-3-models-with-iterative-pruning)。
- en: Chaudhary (2023) Sahil Chaudhary. Code instructions dataset. [https://huggingface.co/datasets/sahil2801/code_instructions_120k](https://huggingface.co/datasets/sahil2801/code_instructions_120k),
    Jun 2023.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 曹达里（2023）萨希尔·曹达里。代码指令数据集。 [https://huggingface.co/datasets/sahil2801/code_instructions_120k](https://huggingface.co/datasets/sahil2801/code_instructions_120k)，2023年6月。
- en: 'Chee et al. (2023) Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De
    Sa. Quip: 2-bit quantization of large language models with guarantees, 2023.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奚等（2023）杰瑞·奚，姚辉·蔡，弗拉基米尔·库列绍夫，以及克里斯托弗·德·萨。Quip：具有保障的大型语言模型的2位量化，2023年。
- en: Chen et al. (2021a) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde
    de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
    Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. Evaluating large language models trained on code, 2021a.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021a）马克·陈，杰瑞·特沃雷克，赫乌·俊，齐明·袁，亨里克·庞德·德·奥利维拉·平托，贾雷德·卡普兰，哈里·爱德华兹，尤里·布尔达，尼古拉斯·约瑟夫，格雷格·布罗克曼，亚历克斯·雷，劳尔·普里，格雷琴·克鲁格，迈克尔·彼得罗夫，海迪·克拉夫，吉里什·萨斯特里，帕梅拉·米什金，布鲁克·陈，斯科特·格雷，尼克·赖德，米哈伊尔·帕夫洛夫，阿莱西娅·鲍尔，卢卡斯·凯泽，穆罕默德·巴瓦里安，克莱门斯·温特，菲利普·蒂莱，费利佩·彼得罗斯基·苏奇，戴夫·卡明斯，马蒂亚斯·普拉佩特，福提奥斯·查恩齐斯，伊丽莎白·巴恩斯，阿里尔·赫伯特-沃斯，威廉·赫布根·古斯，亚历克斯·尼科尔，亚历克斯·帕诺，尼科拉斯·特扎克，杰伊·唐，伊戈尔·巴布什金，苏奇尔·巴拉吉，尚坦努·贾因，威廉·索恩德斯，克里斯托弗·赫塞，安德鲁·N·卡尔，扬·莱克，乔什·阿基亚姆，维丹特·米斯拉，埃文·莫里卡瓦，亚历克·拉德福德，马修·奈特，迈尔斯·布伦达奇，米拉·穆拉蒂，凯蒂·迈耶，彼得·维林德，鲍勃·麦克格鲁，达里奥·阿莫代伊，萨姆·麦克坎利什，伊利亚·苏茨克维尔，以及沃伊切赫·扎伦巴。评估基于代码训练的大型语言模型，2021a。
- en: 'Chen et al. (2021b) Patrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-Jui
    Hsieh. Drone: Data-aware low-rank compression for large nlp models. In M. Ranzato,
    A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), *Advances
    in Neural Information Processing Systems*, volume 34, pp.  29321–29334\. Curran
    Associates, Inc., 2021b. URL [https://proceedings.neurips.cc/paper_files/paper/2021/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 陈等（2021b）帕特里克·陈，许翔富，英德吉特·迪伦，以及肖瑞·谢。Drone：数据感知的低秩压缩用于大型 NLP 模型。在 M.·兰扎托，A.·贝格尔齐默，Y.·多芬，P.S.·梁，和
    J.·沃特曼·沃恩（编辑），*神经信息处理系统进展*，第34卷，第29321–29334页。Curran Associates, Inc.，2021b。网址
    [https://proceedings.neurips.cc/paper_files/paper/2021/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2021/file/f56de5ef149cf0aedcc8f4797031e229-Paper.pdf)。
- en: 'Chiang et al. (2023) Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao
    Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez,
    Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4
    with 90%* chatgpt quality, March 2023. URL [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒋等（2023）魏霖·蒋，卓涵·李，子琳，英生，张浩·吴，浩张，连敏·郑，思远·庄，永浩·庄，约瑟夫·E·冈萨雷斯，伊昂·斯托伊卡，以及埃里克·P·邢。Vicuna：一个开源聊天机器人以90%*
    ChatGPT 质量打动 GPT-4，2023年3月。网址 [https://lmsys.org/blog/2023-03-30-vicuna/](https://lmsys.org/blog/2023-03-30-vicuna/)。
- en: 'Dao (2023) Tri Dao. Flashattention-2: Faster attention with better parallelism
    and work partitioning, 2023.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道（2023）Tri Dao。Flashattention-2：更快的注意力机制，具有更好的并行性和工作划分，2023年。
- en: 'Dao et al. (2022) Tri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher
    Re. Flashattention: Fast and memory-efficient exact attention with IO-awareness.
    In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), *Advances
    in Neural Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=H4DqfPSibmx](https://openreview.net/forum?id=H4DqfPSibmx).'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 道等（2022）Tri Dao，丹尼尔·Y·傅，斯特凡诺·厄蒙，阿特里·鲁德拉，以及克里斯托弗·雷。Flashattention：快速且内存高效的精确注意力机制，具有
    IO 感知。在艾丽斯·H·欧，阿列克斯·阿加瓦尔，丹妮尔·贝尔格雷夫，以及崔庆贤（编辑），*神经信息处理系统进展*，2022年。网址 [https://openreview.net/forum?id=H4DqfPSibmx](https://openreview.net/forum?id=H4DqfPSibmx)。
- en: 'Dettmers & Zettlemoyer (2022) Tim Dettmers and Luke Zettlemoyer. The case for
    4-bit precision: k-bit inference scaling laws, 2022.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 德特默斯和泽特勒摩耶（2022）蒂姆·德特默斯和卢克·泽特勒摩耶。4位精度的理由：k位推理扩展规律，2022年。
- en: 'Dettmers et al. (2022) Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
    GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Alice H.
    Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), *Advances in Neural
    Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=dXiGWqBoxaD](https://openreview.net/forum?id=dXiGWqBoxaD).'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2022）Tim Dettmers、Mike Lewis、Younes Belkada 和 Luke Zettlemoyer。GPT3.int8():
    大规模变换器的 8 位矩阵乘法。见 Alice H. Oh、Alekh Agarwal、Danielle Belgrave 和 Kyunghyun Cho（编），*神经信息处理系统的进展*，2022。URL
    [https://openreview.net/forum?id=dXiGWqBoxaD](https://openreview.net/forum?id=dXiGWqBoxaD)。'
- en: 'Dettmers et al. (2023a) Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke
    Zettlemoyer. Qlora: Efficient finetuning of quantized llms, 2023a.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2023a）Tim Dettmers、Artidoro Pagnoni、Ari Holtzman 和 Luke Zettlemoyer。Qlora:
    高效微调量化 LLM，2023a。'
- en: 'Dettmers et al. (2023b) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression, 2023b.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2023b）Tim Dettmers、Ruslan Svirschevski、Vage Egiazarian、Denis Kuznedelev、Elias
    Frantar、Saleh Ashkboos、Alexander Borzunov、Torsten Hoefler 和 Dan Alistarh。Spqr:
    用于接近无损 LLM 权重压缩的稀疏量化表示，2023b。'
- en: 'Dettmers et al. (2023c) Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian,
    Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler,
    and Dan Alistarh. Spqr: A sparse-quantized representation for near-lossless llm
    weight compression, 2023c.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等（2023c）Tim Dettmers、Ruslan Svirschevski、Vage Egiazarian、Denis Kuznedelev、Elias
    Frantar、Saleh Ashkboos、Alexander Borzunov、Torsten Hoefler 和 Dan Alistarh。Spqr:
    用于接近无损 LLM 权重压缩的稀疏量化表示，2023c。'
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. BERT: Pre-training of deep bidirectional transformers for language
    understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    Volume 1 (Long and Short Papers)*, pp.  4171–4186, Minneapolis, Minnesota, June
    2019\. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL
    [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423).'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Devlin 等（2019）Jacob Devlin、Ming-Wei Chang、Kenton Lee 和 Kristina Toutanova。BERT:
    深度双向变换器的预训练用于语言理解。见 *2019年北美计算语言学协会年会：人类语言技术会议论文集，第1卷（长短篇论文）*，第 4171–4186 页，明尼阿波利斯，明尼苏达州，2019年6月。计算语言学协会。doi:
    10.18653/v1/N19-1423。URL [https://aclanthology.org/N19-1423](https://aclanthology.org/N19-1423)。'
- en: 'Edalati et al. (2022) Ali Edalati, Marzieh Tahaei, Ahmad Rashid, Vahid Nia,
    James Clark, and Mehdi Rezagholizadeh. Kronecker decomposition for GPT compression.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 2: Short Papers)*, pp.  219–226, Dublin, Ireland, May 2022\.
    Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-short.24.
    URL [https://aclanthology.org/2022.acl-short.24](https://aclanthology.org/2022.acl-short.24).'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Edalati 等（2022）Ali Edalati、Marzieh Tahaei、Ahmad Rashid、Vahid Nia、James Clark
    和 Mehdi Rezagholizadeh。GPT 压缩的 Kronecker 分解。见 *第60届计算语言学协会年会会议论文集（第2卷：短论文）*，第
    219–226 页，都柏林，爱尔兰，2022年5月。计算语言学协会。doi: 10.18653/v1/2022.acl-short.24。URL [https://aclanthology.org/2022.acl-short.24](https://aclanthology.org/2022.acl-short.24)。'
- en: Feng et al. (2022) Ruili Feng, Kecheng Zheng, Yukun Huang, Deli Zhao, Michael
    Jordan, and Zheng-Jun Zha. Rank diminishing in deep neural networks. In Alice H.
    Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), *Advances in Neural
    Information Processing Systems*, 2022. URL [https://openreview.net/forum?id=tIqzLFf3kk](https://openreview.net/forum?id=tIqzLFf3kk).
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2022）Ruili Feng、Kecheng Zheng、Yukun Huang、Deli Zhao、Michael Jordan 和
    Zheng-Jun Zha。深度神经网络中的秩降低。见 Alice H. Oh、Alekh Agarwal、Danielle Belgrave 和 Kyunghyun
    Cho（编），*神经信息处理系统的进展*，2022。URL [https://openreview.net/forum?id=tIqzLFf3kk](https://openreview.net/forum?id=tIqzLFf3kk)。
- en: 'Frantar & Alistarh (2023) Elias Frantar and Dan Alistarh. Sparsegpt: Massive
    language models can be accurately pruned in one-shot, 2023.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar & Alistarh（2023）Elias Frantar 和 Dan Alistarh。Sparsegpt: 大型语言模型可以一次性准确剪枝，2023。'
- en: 'Frantar et al. (2023) Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan
    Alistarh. OPTQ: Accurate quantization for generative pre-trained transformers.
    In *The Eleventh International Conference on Learning Representations*, 2023.
    URL [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar 等（2023）Elias Frantar、Saleh Ashkboos、Torsten Hoefler 和 Dan Alistarh。OPTQ:
    生成预训练变换器的准确量化。见 *第十一届国际学习表征会议*，2023。URL [https://openreview.net/forum?id=tcbBPnfwxS](https://openreview.net/forum?id=tcbBPnfwxS)。'
- en: Gu et al. (2023) Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Knowledge distillation
    of large language models, 2023.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gu等（2023）Yuxian Gu, Li Dong, Furu Wei, 和 Minlie Huang. 大型语言模型的知识蒸馏, 2023。
- en: 'He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In *2016 IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR)*, pp.  770–778, 2016. doi: 10.1109/CVPR.2016.90.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'He等（2016）Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun. 用于图像识别的深度残差学习。发表于*2016年IEEE计算机视觉与模式识别会议（CVPR）*，第770–778页，2016年。doi:
    10.1109/CVPR.2016.90。'
- en: Hinton et al. (2015) Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling
    the knowledge in a neural network, 2015.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton等（2015）Geoffrey Hinton, Oriol Vinyals, 和 Jeff Dean. 神经网络中的知识蒸馏, 2015。
- en: Hsu et al. (2022) Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen,
    and Hongxia Jin. Language model compression with weighted low-rank factorization.
    In *International Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=uPv9Y3gmAI5](https://openreview.net/forum?id=uPv9Y3gmAI5).
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hsu等（2022）Yen-Chang Hsu, Ting Hua, Sungen Chang, Qian Lou, Yilin Shen, 和 Hongxia
    Jin. 带权重的低秩分解进行语言模型压缩。发表于*国际学习表征会议*，2022年。网址 [https://openreview.net/forum?id=uPv9Y3gmAI5](https://openreview.net/forum?id=uPv9Y3gmAI5)。
- en: 'Hu et al. (2022) Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu,
    Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of
    large language models. In *International Conference on Learning Representations*,
    2022. URL [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9).'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu等（2022）Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi
    Li, Shean Wang, Lu Wang, 和 Weizhu Chen. LoRA: 大型语言模型的低秩适配。发表于*国际学习表征会议*，2022年。网址
    [https://openreview.net/forum?id=nZeVKeeFYf9](https://openreview.net/forum?id=nZeVKeeFYf9)。'
- en: 'Hua et al. (2022) Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen,
    and Hongxia Jin. Numerical optimizations for weighted low-rank estimation on language
    models. In *Proceedings of the 2022 Conference on Empirical Methods in Natural
    Language Processing*, pp.  1404–1416, Abu Dhabi, United Arab Emirates, December
    2022\. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.91.
    URL [https://aclanthology.org/2022.emnlp-main.91](https://aclanthology.org/2022.emnlp-main.91).'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hua等（2022）Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen, 和 Hongxia
    Jin. 针对语言模型的加权低秩估计的数值优化。发表于*2022年自然语言处理经验方法会议论文集*，第1404–1416页，阿布扎比，阿联酋，2022年12月。计算语言学协会。doi:
    10.18653/v1/2022.emnlp-main.91。网址 [https://aclanthology.org/2022.emnlp-main.91](https://aclanthology.org/2022.emnlp-main.91)。'
- en: 'Jung et al. (2023) Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing
    Lu, Jillian Fisher, Taylor Sorensen, and Yejin Choi. Impossible distillation:
    from low-quality model to high-quality dataset & model for summarization and paraphrasing,
    2023.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jung等（2023）Jaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian
    Fisher, Taylor Sorensen, 和 Yejin Choi. 不可能的蒸馏：从低质量模型到高质量数据集和模型的摘要与改写, 2023。
- en: 'Kim et al. (2023a) Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu
    Li, Sheng Shen, Michael W. Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse
    quantization, 2023a.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim等（2023a）Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng
    Shen, Michael W. Mahoney, 和 Kurt Keutzer. Squeezellm: 密集与稀疏量化, 2023a。'
- en: 'Kim et al. (2023b) Young Jin Kim, Rawn Henry, Raffy Fahim, and Hany Hassan
    Awadalla. Finequant: Unlocking efficiency with fine-grained weight-only quantization
    for llms, 2023b.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim等（2023b）Young Jin Kim, Rawn Henry, Raffy Fahim, 和 Hany Hassan Awadalla.
    Finequant: 通过细粒度权重量化解锁大型语言模型的效率, 2023b。'
- en: 'Kocetkov et al. (2022) Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li,
    Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean
    Hughes, Thomas Wolf, Dzmitry Bahdanau, Leandro von Werra, and Harm de Vries. The
    stack: 3 tb of permissively licensed source code, 2022.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kocetkov等（2022）Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao
    Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas
    Wolf, Dzmitry Bahdanau, Leandro von Werra, 和 Harm de Vries. The stack: 3 TB的宽松许可源码,
    2022。'
- en: 'Lan et al. (2020) Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel,
    Piyush Sharma, and Radu Soricut. Albert: A lite bert for self-supervised learning
    of language representations. In *International Conference on Learning Representations*,
    2020. URL [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS).'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lan等（2020）Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
    Sharma, 和 Radu Soricut. Albert: 一种轻量级的BERT用于自监督学习语言表征。发表于*国际学习表征会议*，2020年。网址 [https://openreview.net/forum?id=H1eA7AEtvS](https://openreview.net/forum?id=H1eA7AEtvS)。'
- en: 'Lee et al. (2023) Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, and Eunhyeok
    Park. Owq: Lessons learned from activation outliers for weight quantization in
    large language models, 2023.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lee et al. (2023) Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, 和 Eunhyeok
    Park. Owq: 从激活异常值中学习的大语言模型权重量化经验教训, 2023。'
- en: 'Li et al. (2023a) Liang Li, Qingyuan Li, Bo Zhang, and Xiangxiang Chu. Norm
    tweaking: High-performance low-bit quantization of large language models, 2023a.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023a) Liang Li, Qingyuan Li, Bo Zhang, 和 Xiangxiang Chu. Norm tweaking:
    大型语言模型的高性能低位量化, 2023a。'
- en: 'Li et al. (2023b) Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng
    He, Weizhu Chen, and Tuo Zhao. Losparse: Structured compression of large language
    models based on low-rank and sparse approximation, 2023b.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li et al. (2023b) Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng
    He, Weizhu Chen, 和 Tuo Zhao. Losparse: 基于低秩和稀疏近似的大型语言模型的结构化压缩, 2023b。'
- en: 'Li et al. (2020) Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer,
    Dan Klein, and Joseph E. Gonzalez. Train large, then compress: Rethinking model
    size for efficient training and inference of transformers. In *Proceedings of
    the 37th International Conference on Machine Learning*, ICML’20\. JMLR.org, 2020.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2020) Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer,
    Dan Klein, 和 Joseph E. Gonzalez. 训练大模型，然后压缩：重新思考模型规模以提高变换器的训练和推理效率. 在 *第37届国际机器学习会议论文集*,
    ICML’20. JMLR.org, 2020。
- en: 'Liang et al. (2021) Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, and
    Xiaotong Zhang. Pruning and quantization for deep neural network acceleration:
    A survey. *Neurocomputing*, 461:370–403, 2021. ISSN 0925-2312. doi: https://doi.org/10.1016/j.neucom.2021.07.045.
    URL [https://www.sciencedirect.com/science/article/pii/S0925231221010894](https://www.sciencedirect.com/science/article/pii/S0925231221010894).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liang et al. (2021) Tailin Liang, John Glossner, Lei Wang, Shaobo Shi, 和 Xiaotong
    Zhang. 深度神经网络加速的剪枝与量化：综述. *Neurocomputing*, 461:370–403, 2021. ISSN 0925-2312.
    doi: https://doi.org/10.1016/j.neucom.2021.07.045. URL [https://www.sciencedirect.com/science/article/pii/S0925231221010894](https://www.sciencedirect.com/science/article/pii/S0925231221010894)。'
- en: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    and Song Han. Awq: Activation-aware weight quantization for llm compression and
    acceleration, 2023.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin et al. (2023) Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang,
    和 Song Han. Awq: 关注激活的权重量化用于大语言模型的压缩与加速, 2023。'
- en: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, and Xinchao Wang. Llm-pruner: On
    the structural pruning of large language models, 2023.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ma et al. (2023) Xinyin Ma, Gongfan Fang, 和 Xinchao Wang. Llm-pruner: 关于大型语言模型的结构性剪枝,
    2023。'
- en: 'NVIDIA (2007) Corporation NVIDIA. Compute unified device architecture (cuda).
    Website, 2007. URL [https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit).
    Accessed: 2023-09-17.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'NVIDIA (2007) NVIDIA公司. 计算统一设备架构 (cuda). 网站, 2007. URL [https://developer.nvidia.com/cuda-toolkit](https://developer.nvidia.com/cuda-toolkit).
    访问时间: 2023-09-17。'
- en: 'Park et al. (2022) Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, and Dongsoo Lee.
    Lut-gemm: Quantized matrix multiplication based on luts for efficient inference
    in large-scale generative language models, 2022.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Park et al. (2022) Gunho Park, Baeseong Park, Minsub Kim, Sungjae Lee, Jeonghoon
    Kim, Beomseok Kwon, Se Jung Kwon, Byeongwook Kim, Youngjoo Lee, 和 Dongsoo Lee.
    Lut-gemm: 基于 LUT 的量化矩阵乘法用于大规模生成语言模型的高效推理, 2022。'
- en: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and
    Soumith Chintala. *PyTorch: An Imperative Style, High-Performance Deep Learning
    Library*, chapter ., pp.  . Curran Associates Inc., Red Hook, NY, USA, 2019.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke et al. (2019) Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James
    Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca
    Antiga, Alban Desmaison, Andreas Köpf, Edward Yang, Zach DeVito, Martin Raison,
    Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, 和 Soumith
    Chintala. *PyTorch: 一种命令式风格的高性能深度学习库*, 第 ., 页. Curran Associates Inc., Red Hook,
    NY, USA, 2019。'
- en: 'Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    and Julien Launay. The refinedweb dataset for falcon llm: Outperforming curated
    corpora with web data, and web data only, 2023.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Penedo et al. (2023) Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra
    Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei,
    和 Julien Launay. refinedweb 数据集用于 falcon 大语言模型：超越经过整理的语料库与仅使用网络数据, 2023。
- en: 'Peng et al. (2023) Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer.
    The impact of ai on developer productivity: Evidence from github copilot, 2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人（2023）Sida Peng、Eirini Kalliamvakou、Peter Cihon 和 Mert Demirer。人工智能对开发者生产力的影响：来自GitHub
    Copilot的证据，2023。
- en: Rabe & Staats (2021) Markus N. Rabe and Charles Staats. Self-attention does
    not need $o(n^{2})$ memory, 2021.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rabe & Staats（2021）Markus N. Rabe 和 Charles Staats。自注意力不需要 $o(n^{2})$ 内存，2021。
- en: Radford et al. (2019) Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario
    Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners.
    *OpenAI Blog*, 1(8), 2019.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人（2019）Alec Radford、Jeff Wu、Rewon Child、David Luan、Dario Amodei 和 Ilya
    Sutskever。语言模型是无监督的多任务学习者。*OpenAI 博客*，1(8)，2019。
- en: Ren & Zhu (2023) Siyu Ren and Kenny Q. Zhu. Low-rank prune-and-factorize for
    language model compression, 2023.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ren & Zhu（2023）Siyu Ren 和 Kenny Q. Zhu。低秩剪枝与分解用于语言模型压缩，2023。
- en: 'Rozière et al. (2023) Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten
    Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy
    Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton
    Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal
    Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel
    Synnaeve. Code llama: Open foundation models for code, 2023.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rozière 等人（2023）Baptiste Rozière、Jonas Gehring、Fabian Gloeckle、Sten Sootla、Itai
    Gat、Xiaoqing Ellen Tan、Yossi Adi、Jingyu Liu、Tal Remez、Jérémy Rapin、Artyom Kozhevnikov、Ivan
    Evtimov、Joanna Bitton、Manish Bhatt、Cristian Canton Ferrer、Aaron Grattafiori、Wenhan
    Xiong、Alexandre Défossez、Jade Copet、Faisal Azhar、Hugo Touvron、Louis Martin、Nicolas
    Usunier、Thomas Scialom 和 Gabriel Synnaeve。Code llama：开放的代码基础模型，2023。
- en: 'Sanh et al. (2019) Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas
    Wolf. Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter,
    2019.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanh 等人（2019）Victor Sanh、Lysandre Debut、Julien Chaumond 和 Thomas Wolf。Distilbert，一种BERT的蒸馏版本：更小、更快、更便宜、更轻，2019。
- en: 'Shao et al. (2023) Wenqi Shao, Mengzhao Chen, Zhaoyang Zhang, Peng Xu, Lirui
    Zhao, Zhiqian Li, Kaipeng Zhang, Peng Gao, Yu Qiao, and Ping Luo. Omniquant: Omnidirectionally
    calibrated quantization for large language models, 2023.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shao 等人（2023）Wenqi Shao、Mengzhao Chen、Zhaoyang Zhang、Peng Xu、Lirui Zhao、Zhiqian
    Li、Kaipeng Zhang、Peng Gao、Yu Qiao 和 Ping Luo。Omniquant：大型语言模型的全方位校准量化，2023。
- en: 'Shazeer (2019) Noam Shazeer. Fast transformer decoding: One write-head is all
    you need, 2019.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer（2019）Noam Shazeer。快速变换器解码：一个写头就足够了，2019。
- en: Shazeer (2020) Noam Shazeer. Glu variants improve transformer, 2020.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shazeer（2020）Noam Shazeer。Glu变体改进变换器，2020。
- en: 'Shen et al. (2023) Bo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing
    Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang Zhao, Yuenan Guo, and
    Qianxiang Wang. Pangu-coder2: Boosting large language models for code with ranking
    feedback, 2023.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人（2023）Bo Shen、Jiaxin Zhang、Taihong Chen、Daoguang Zan、Bing Geng、An Fu、Muhan
    Zeng、Ailun Yu、Jichuan Ji、Jingyang Zhao、Yuenan Guo 和 Qianxiang Wang。Pangu-coder2：通过排名反馈提升大型语言模型，2023。
- en: Sun et al. (2023a) Mingjie Sun, Zhuang Liu, Anna Bair, and J. Zico Kolter. A
    simple and effective pruning approach for large language models, 2023a.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2023a）Mingjie Sun、Zhuang Liu、Anna Bair 和 J. Zico Kolter。大型语言模型的简单有效的剪枝方法，2023a。
- en: Sun et al. (2023b) Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang
    Chen, David Cox, Yiming Yang, and Chuang Gan. Principle-driven self-alignment
    of language models from scratch with minimal human supervision, 2023b.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun 等人（2023b）Zhiqing Sun、Yikang Shen、Qinhong Zhou、Hongxin Zhang、Zhenfang Chen、David
    Cox、Yiming Yang 和 Chuang Gan。从零开始的原则驱动语言模型自对齐，最少的人类监督，2023b。
- en: 'Tahaei et al. (2022) Marzieh Tahaei, Ella Charlaix, Vahid Nia, Ali Ghodsi,
    and Mehdi Rezagholizadeh. KroneckerBERT: Significant compression of pre-trained
    language models through kronecker decomposition and knowledge distillation. In
    *Proceedings of the 2022 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies*, pp.  2116–2127, Seattle,
    United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.154.
    URL [https://aclanthology.org/2022.naacl-main.154](https://aclanthology.org/2022.naacl-main.154).'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tahaei 等人（2022）Marzieh Tahaei、Ella Charlaix、Vahid Nia、Ali Ghodsi 和 Mehdi Rezagholizadeh。KroneckerBERT：通过Kronecker分解和知识蒸馏显著压缩预训练语言模型。见于*2022年北美计算语言学协会：人类语言技术会议论文集*，第2116–2127页，美国华盛顿州西雅图，2022年7月。计算语言学协会。doi:
    10.18653/v1/2022.naacl-main.154。网址 [https://aclanthology.org/2022.naacl-main.154](https://aclanthology.org/2022.naacl-main.154)。'
- en: 'Taori et al. (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
    Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpaca:
    A strong, replicable instruction-following model. *CRFM Stanford*, March 2023.
    URL [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html).'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Taori 等 (2023) Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen
    Li, Carlos Guestrin, Percy Liang, 和 Tatsunori B. Hashimoto. Alpaca: 一个强大且可复制的指令跟随模型。*CRFM
    Stanford*，2023年3月。网址 [https://crfm.stanford.edu/2023/03/13/alpaca.html](https://crfm.stanford.edu/2023/03/13/alpaca.html)。'
- en: 'Touvron et al. (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
    Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal,
    Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and
    Guillaume Lample. Llama: Open and efficient foundation language models, 2023.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023) Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,
    Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro,
    Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, 和 Guillaume Lample.
    Llama: 开放而高效的基础语言模型，2023。'
- en: Wang et al. (2023a) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, and Ji-Rong Wen. A survey on large language model based autonomous agents,
    2023a.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2023a) Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen
    Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei
    Wei, 和 Ji-Rong Wen. 基于大型语言模型的自主代理调查，2023a。
- en: Wang et al. (2023b) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel,
    Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith,
    Iz Beltagy, and Hannaneh Hajishirzi. How far can camels go? exploring the state
    of instruction tuning on open resources, 2023b.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等 (2023b) Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar
    Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz
    Beltagy, 和 Hannaneh Hajishirzi. 骆驼能走多远？探索开放资源上的指令调优现状，2023b。
- en: 'Wei et al. (2022) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong,
    Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression:
    Pushing the limit of low-bit transformer language models. In Alice H. Oh, Alekh
    Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), *Advances in Neural Information
    Processing Systems*, 2022. URL [https://openreview.net/forum?id=yW5zeRSFdZ](https://openreview.net/forum?id=yW5zeRSFdZ).'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wei 等 (2022) Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang
    Zhang, Qi Zhang, Fengwei Yu, 和 Xianglong Liu. 异常点抑制: 推进低位变换器语言模型的极限。载于 Alice H.
    Oh, Alekh Agarwal, Danielle Belgrave, 和 Kyunghyun Cho (编辑)，*Advances in Neural
    Information Processing Systems*，2022。网址 [https://openreview.net/forum?id=yW5zeRSFdZ](https://openreview.net/forum?id=yW5zeRSFdZ)。'
- en: 'Wolf et al. (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond,
    Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    and Alexander Rush. Transformers: State-of-the-art natural language processing.
    In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language
    Processing: System Demonstrations*, pp.  38–45, Online, October 2020\. Association
    for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-demos.6. URL [https://aclanthology.org/2020.emnlp-demos.6](https://aclanthology.org/2020.emnlp-demos.6).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wolf 等 (2020) Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement
    Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,
    Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
    Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
    和 Alexander Rush. Transformers: 先进的自然语言处理技术。载于 *Proceedings of the 2020 Conference
    on Empirical Methods in Natural Language Processing: System Demonstrations*，第
    38–45 页，在线，2020年10月。计算语言学协会。doi: 10.18653/v1/2020.emnlp-demos.6。网址 [https://aclanthology.org/2020.emnlp-demos.6](https://aclanthology.org/2020.emnlp-demos.6)。'
- en: 'Wu et al. (2023) Xiaoxia Wu, Zhewei Yao, and Yuxiong He. Zeroquant-fp: A leap
    forward in llms post-training w4a8 quantization using floating-point formats,
    2023.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wu 等 (2023) Xiaoxia Wu, Zhewei Yao, 和 Yuxiong He. Zeroquant-fp: 在使用浮点格式的llms后训练w4a8量化中的突破，2023。'
- en: 'Xiao et al. (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth,
    and Song Han. SmoothQuant: Accurate and efficient post-training quantization for
    large language models. In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara
    Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), *Proceedings of the 40th
    International Conference on Machine Learning*, volume 202 of *Proceedings of Machine
    Learning Research*, pp.  38087–38099\. PMLR, 23–29 Jul 2023. URL [https://proceedings.mlr.press/v202/xiao23c.html](https://proceedings.mlr.press/v202/xiao23c.html).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xiao 等 (2023) Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth
    和 Song Han。SmoothQuant: 大型语言模型的准确而高效的后训练量化。在 Andreas Krause, Emma Brunskill, Kyunghyun
    Cho, Barbara Engelhardt, Sivan Sabato 和 Jonathan Scarlett (编)，*第40届国际机器学习大会论文集*，*机器学习研究论文集*第202卷，第38087–38099页。PMLR，2023年7月23–29日。网址
    [https://proceedings.mlr.press/v202/xiao23c.html](https://proceedings.mlr.press/v202/xiao23c.html)。'
- en: 'Yao et al. (2023) Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, and Yuxiong
    He. Zeroquant-v2: Exploring post-training quantization in llms from comprehensive
    study to low rank compensation, 2023.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yao 等 (2023) Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn 和 Yuxiong He。Zeroquant-v2:
    从综合研究到低秩补偿，探索大型语言模型的后训练量化，2023。'
- en: 'Yu & Wu (2023) Hao Yu and Jianxin Wu. Compressing transformers: Features are
    low-rank, but weights are not! *Proceedings of the AAAI Conference on Artificial
    Intelligence*, 37(9):11007–11015, Jun. 2023. doi: 10.1609/aaai.v37i9.26304. URL
    [https://ojs.aaai.org/index.php/AAAI/article/view/26304](https://ojs.aaai.org/index.php/AAAI/article/view/26304).'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu & Wu (2023) Hao Yu 和 Jianxin Wu。压缩变压器: 特征是低秩的，但权重不是！*AAAI人工智能大会论文集*，37(9):11007–11015，2023年6月。doi:
    10.1609/aaai.v37i9.26304。网址 [https://ojs.aaai.org/index.php/AAAI/article/view/26304](https://ojs.aaai.org/index.php/AAAI/article/view/26304)。'
- en: 'Yuan et al. (2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu, and Bingzhe Wu. Rptq: Reorder-based
    post-training quantization for large language models, 2023.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yuan 等 (2023) Zhihang Yuan, Lin Niu, Jiawei Liu, Wenyu Liu, Xinggang Wang,
    Yuzhang Shang, Guangyu Sun, Qiang Wu, Jiaxiang Wu 和 Bingzhe Wu。Rptq: 基于重排序的大型语言模型后训练量化，2023。'
- en: 'Zhang et al. (2023) Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and
    Bo Li. Lora-fa: Memory-efficient low-rank adaptation for large language models
    fine-tuning, 2023.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhang 等 (2023) Longteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu 和 Bo Li。Lora-fa:
    大型语言模型微调的内存高效低秩适应，2023。'
- en: Zhu et al. (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. A
    survey on model compression for large language models, 2023.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等 (2023) Xunyu Zhu, Jian Li, Yong Liu, Can Ma 和 Weiping Wang。大型语言模型的模型压缩调查，2023。
