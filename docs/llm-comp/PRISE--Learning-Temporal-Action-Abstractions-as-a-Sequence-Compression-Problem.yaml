- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-08 18:53:33'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:53:33'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'PRISE: 将时间动作抽象视为序列压缩问题'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10450](https://ar5iv.labs.arxiv.org/html/2402.10450)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.10450](https://ar5iv.labs.arxiv.org/html/2402.10450)
- en: Ruijie Zheng
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Ruijie Zheng
- en: University of Maryland College Park
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 马里兰大学帕克分校
- en: rzheng12@umd.edu
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: rzheng12@umd.edu
- en: '&Ching-An Cheng'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 和 Ching-An Cheng
- en: Microsoft Research
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 微软研究院
- en: chinganc@microsoft.com &Hal Daumé III
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: chinganc@microsoft.com 和 Hal Daumé III
- en: University of Maryland College Park, Microsoft Resesarch
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 马里兰大学帕克分校，微软研究院
- en: me@hal3.name &Furong Huang
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: me@hal3.name 和 Furong Huang
- en: University of Maryland College Park
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 马里兰大学帕克分校
- en: furongh@cs.umd.edu &Andrey Kolobov
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: furongh@cs.umd.edu 和 Andrey Kolobov
- en: Microsoft Research
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 微软研究院
- en: akolobov@microsoft.com
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: akolobov@microsoft.com
- en: Abstract
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Temporal action abstractions, along with belief state representations, are a
    powerful knowledge sharing mechanism for sequential decision making. In this work,
    we propose a novel view that treats inducing temporal action abstractions as a
    sequence compression problem. To do so, we bring a subtle but critical component
    of LLM training pipelines – input tokenization via byte pair encoding (BPE) –
    to the seemingly distant task of learning skills of variable time span in continuous
    control domains. We introduce an approach called Primitive Sequence Encoding (PRISE)
    that combines continuous action quantization with BPE to learn powerful action
    abstractions. We empirically show that high-level skills discovered by PRISE from
    a multitask set of robotic manipulation demonstrations significantly boost the
    performance of both multitask imitation learning as well as few-shot imitation
    learning on unseen tasks. ¹¹1Our code will be released at [https://github.com/FrankZheng2022/PRISE](https://github.com/FrankZheng2022/PRISE).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 时间动作抽象，与信念状态表示一样，是顺序决策的强大知识共享机制。在这项工作中，我们提出了一种新颖的视角，将引入时间动作抽象视为序列压缩问题。为此，我们将LLM训练管道中的一个微妙但关键的组件——通过字节对编码（BPE）进行的输入标记化——引入到看似遥远的任务中，即在连续控制领域中学习具有可变时间跨度的技能。我们提出了一种称为原始序列编码（PRISE）的方法，它将连续动作量化与BPE相结合，以学习强大的动作抽象。我们通过实验证明，PRISE从一组多任务机器人操作演示中发现的高级技能显著提高了多任务模仿学习以及在未见任务上的少样本模仿学习的性能。¹¹1我们的代码将发布在
    [https://github.com/FrankZheng2022/PRISE](https://github.com/FrankZheng2022/PRISE)。
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: High-dimensional observations and decision-making over complex, continuous action
    spaces are hallmarks of typical scenarios in sequential decision-making, including
    robotics. A common way of dealing with these complexities is constructing *abstractions*
    – compact representations of belief states and actions that generalize across
    tasks and make learning to act in new scenarios robust and data-efficient. Inspired
    by techniques that have enabled ground-breaking progresses in computer vision
    (CV) and natural language processing (NLP) over the past decade, much of continuous
    control research has focused on *learning* abstractions from data rather than
    hand-crafting them. The lion’s share of these efforts study learning multi-task
    representations, which has analogies to representation learning in CV and NLP
    and has been tackled by adapting these fields’ models (see, e.g., DT (Chen et al.,
    [2021](#bib.bib6)) vs. GPT-2 (Radford et al., [2019](#bib.bib35))) and methods
    (see, e.g., R3M (Nair et al., [2022](#bib.bib29)) vs. InfoNCE (den Oord et al.,
    [2019](#bib.bib8))).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 高维观察和在复杂、连续动作空间中的决策制定是顺序决策中的典型场景的标志，包括机器人技术。处理这些复杂性的一个常见方法是构建*抽象*— 信念状态和动作的紧凑表示，这些表示在任务之间进行概括，使得在新场景中学习行动变得稳健且数据高效。受到过去十年在计算机视觉（CV）和自然语言处理（NLP）领域取得突破性进展的技术的启发，大部分连续控制研究集中在从数据中*学习*抽象，而不是手工制作这些抽象。这些努力中的大部分研究了多任务表示的学习，这与CV和NLP中的表示学习有类似之处，并且通过调整这些领域的模型（例如，DT（Chen
    et al., [2021](#bib.bib6)）与GPT-2（Radford et al., [2019](#bib.bib35)））和方法（例如，R3M（Nair
    et al., [2022](#bib.bib29)）与InfoNCE（den Oord et al., [2019](#bib.bib8)））来解决这一问题。
- en: On the other hand, learning *temporal action abstractions* – representations
    of multi-step primitive behaviors – has not benefited nearly as much from such
    a methodology transfer. This omission is especially glaring in continuous control,
    where complex policies can clearly be decomposed into versatile lower-level routines
    such as picking up objects, walking, etc, and whose popular learning method, Behavior
    Cloning (BC) (Arora et al., [2022](#bib.bib2)), has many commonalities with LLM
    training.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，学习*时间动作抽象*——即多步原始行为的表示——几乎没有从这种方法论转移中受益。这一遗漏在连续控制中尤为显著，在那里复杂的策略可以明显地被分解为多种通用的低级例程，如捡起物体、行走等，其流行的学习方法，行为克隆（BC）（Arora等，[2022](#bib.bib2)），与LLM训练有许多相似之处。
- en: In this paper, we posit that adapting discrete coding and sequence compression
    techniques from NLP offers untapped potential for action representation learning
    for continuous control. Specifically, given a pretraining dataset of demonstrations
    from multiple decision tasks over a continuous action space and a high-dimensional
    pixel observation space, we consider the problem of learning temporally extended
    action primitives, i.e., *skills*, to improve downstream learning efficiency of
    reward-free BC. We show that embedding continuous actions into *discrete* codes
    and then applying a popular NLP sequence compression method called *Byte Pair
    Encoding (BPE)* (Gage, [1994](#bib.bib11)) to the resulting discrete-code sequences
    identifies variable-timespan action primitives with the property we desire. Namely,
    in downstream tasks, policies learned by BC using these action primitives *consistently
    perform better*, often substantially, than policies learned by BC directly over
    the original action space.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们认为将离散编码和序列压缩技术从NLP应用于连续控制的动作表示学习具有未开发的潜力。具体来说，给定一个由多个决策任务的演示组成的预训练数据集，这些任务在连续动作空间和高维像素观察空间上进行，我们考虑学习时间扩展的动作原语，即*技能*，以提高奖励自由BC的下游学习效率。我们展示了将连续动作嵌入到*离散*编码中，然后应用一种流行的NLP序列压缩方法称为*字节对编码（BPE）*（Gage，[1994](#bib.bib11)）到结果离散编码序列，可以识别具有我们期望属性的可变时间跨度动作原语。即，在下游任务中，使用这些动作原语的BC学习策略*一致表现更好*，通常显著优于直接在原始动作空间上进行BC学习的策略。
- en: Our work’s main contribution is *Primitive Sequence Encoding (PRISE)*, a novel
    method for learning multi-task temporal action abstractions that capitalizes on
    a novel connection to NLP methodology. PRISE quantizes the agent’s original continuous
    action space into discrete codes, converts the pretraining training trajectories
    into action code sequences, and uses BPE to induce variable-timestep skills. During
    BC for downstream tasks, learning policies over these skills and then decoding
    skills into primitive action sequences gives PRISE a significant boost in learning
    efficiency over strong baselines such as ACT (Zhang et al., [2021](#bib.bib50)).
    We conduct extensive ablation studies to show the effect of various parameters
    on PRISE’s performance, which demonstrate BPE to be critical to PRISE’s success.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们工作的主要贡献是*原始序列编码（PRISE）*，这是一种学习多任务时间动作抽象的新方法，利用了与NLP方法论的创新连接。PRISE将代理的原始连续动作空间量化为离散编码，将预训练轨迹转换为动作编码序列，并使用BPE引入可变时间步长的技能。在下游任务的BC过程中，通过对这些技能进行学习策略，并将技能解码为原始动作序列，PRISE在学习效率上相较于强基线如ACT（Zhang等，[2021](#bib.bib50)）有显著提升。我们进行广泛的消融研究，以展示各种参数对PRISE性能的影响，这表明BPE对PRISE的成功至关重要。
- en: 2 Preliminaries
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 预备知识
- en: Problem Setting.
  id: totrans-26
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 问题设置。
- en: We consider a set of tasks $T$ is the image space.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将任务集$T$视为图像空间。
- en: Let $\mathcal{D}=\bigcup_{\mathcal{T}\in\mathscr{T}}\mathcal{D}_{\mathcal{T}}$.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 设$\mathcal{D}=\bigcup_{\mathcal{T}\in\mathscr{T}}\mathcal{D}_{\mathcal{T}}$。
- en: Vector Quantization.
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量量化。
- en: In order to construct the temporal action abstrskill tokens, our algorithm PRISE 
    first converts continuous actions into discrete codes. To do so, it uses the vector
    quantization module proposed in Van den Oord et al. ([2017](#bib.bib44)). This
    vector quantization module $\mathcal{F}$, and the second term prevents the query
    vector from switching between different codes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建时间动作抽象技能标记，我们的算法PRISE首先将连续动作转换为离散编码。为此，它使用了Van den Oord等人（[2017](#bib.bib44)）提出的向量量化模块。这个向量量化模块$\mathcal{F}$，以及第二项防止查询向量在不同编码之间切换。
- en: Byte Pair Encoding (BPE).
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 字节对编码（BPE）。
- en: After quantizing the action space into codes, PRISE identifies common code (action)
    sequences via a technique from the realm of NLP, the method of Byte Pair Encoding
    (BPE) (Gage, [1994](#bib.bib11); Sennrich et al., [2016](#bib.bib40)). In NLP,
    BPE has emerged as a pivotal technique for managing the vast and varied vocabulary
    encountered in textual data. Language models are expected to predict a text completion
    based on a text prefix. Doing so naively, by generating the text character-by-character
    is problematic, because the notion of a character differs significantly across
    languages. Instead, NLP models operate at the language-agnostic level of *bytes*.
    However, the high granularity of byte prediction has its own computational challenges.
    Therefore, language models models first find high-frequency byte *sequences* in
    the training data, assign *tokens* to these sequences, learn to predict the resulting
    tokens, which then get decoded into text. BPE is the algorithm that handles the
    common byte sequence construction problem.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在将动作空间量化为代码之后，PRISE 通过来自 NLP 领域的技术——字节对编码（BPE）方法（Gage，[1994](#bib.bib11)；Sennrich
    等，[2016](#bib.bib40)）来识别常见的代码（动作）序列。在 NLP 中，BPE 已成为管理文本数据中大量且多样化词汇的关键技术。语言模型预计根据文本前缀预测文本的补全。单纯地按字符逐个生成文本是有问题的，因为字符的概念在不同语言中差异很大。相反，NLP
    模型在语言无关的*字节*层面上操作。然而，字节预测的高粒度具有自身的计算挑战。因此，语言模型首先在训练数据中找到高频字节*序列*，为这些序列分配*标记*，学习预测结果标记，然后将其解码为文本。BPE
    是处理常见字节序列构建问题的算法。
- en: 'BPE operates by merging the most frequent pairs of bytes, assigning tokens
    to these pairs, and iteratively merging new pairs of tokens or bytes. Thereby,
    it builds up a vocabulary of tokens that plays a crucial role in modern large
    language models OpenAI ([2023](#bib.bib30)); Brown et al. ([2020](#bib.bib4));
    Radford et al. ([2019](#bib.bib35)). See [Figure 2](#S3.F2 "In 3.1 Pretraining
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem") for a demonstration. We apply BPE in the same way, but to action codes
    instead of bytes.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: BPE 通过合并最频繁的字节对、为这些对分配标记，并迭代地合并新的标记对或字节来操作。这样，它构建了一个在现代大型语言模型中发挥重要作用的标记词汇（OpenAI
    [2023](#bib.bib30)；Brown 等 [2020](#bib.bib4)；Radford 等 [2019](#bib.bib35)）。有关演示，请参见
    [图 2](#S3.F2 "在 3.1 预训练 ‣ 3 算法 ‣ PRISE：将时间动作抽象作为序列压缩问题进行学习")。我们以相同的方式应用 BPE，但应用于动作代码而非字节。
- en: '![Refer to caption](img/2ccedd27d5fb6755159b5ad169e16e03.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2ccedd27d5fb6755159b5ad169e16e03.png)'
- en: 'Figure 1: (a) Pretraining Stage I of PRISE : The goal is to learn a action
    quantization module such that conditioned on the current state and action $(o_{t},a_{t})$,
    it could assign a discrete action code. (b) Pretraining Stage II of PRISE : First
    it converts a trajectory of continuous state and actions into discrete codes.
    Then based on the corpus of quantized trajectories from the multitask offline
    dataset, PRISE applies BPE (illustrated in [Figure 2](#S3.F2 "In 3.1 Pretraining
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem")) to learn vocabulary of skill tokens, where each token represents a
    sequence of discrete action code.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：（a）PRISE 的预训练阶段 I：目标是学习一个动作量化模块，使其在给定当前状态和动作 $(o_{t},a_{t})$ 时，能够分配一个离散的动作代码。（b）PRISE
    的预训练阶段 II：首先将连续状态和动作的轨迹转换为离散代码。然后基于来自多任务离线数据集的量化轨迹语料库，PRISE 应用 BPE（见 [图 2](#S3.F2
    "在 3.1 预训练 ‣ 3 算法 ‣ PRISE：将时间动作抽象作为序列压缩问题进行学习")）来学习技能标记的词汇，每个标记代表一组离散动作代码序列。
- en: 3 Algorithm
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 算法
- en: '[Figure 1](#S2.F1 "In Byte Pair Encoding (BPE). ‣ 2 Preliminaries ‣ PRISE:
    Learning Temporal Action Abstractions as a Sequence Compression Problem") illustrates
    the key steps of PRISE. At a high level, PRISE first learns a state-dependent
    action quantization module (Pretraining I stage). This module processes the pretraining
    multitask dataset $\mathcal{D}$, assigns a token to each of them, and thereby
    constructs primitive skills, shown in [Figure 1](#S2.F1 "In Byte Pair Encoding
    (BPE). ‣ 2 Preliminaries ‣ PRISE: Learning Temporal Action Abstractions as a Sequence
    Compression Problem")(b). Once these primitive skill tokens are discovered, they
    can be used to relabel a demonstration dataset and learn via behavior cloning
    (BC) a policy that chooses these skill tokens instead of raw actions, as we later
    show in [Section 3.2](#S3.SS2 "3.2 Multitask generalist policy learning. ‣ 3 Algorithm
    ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem")
    and [Section 3.3](#S3.SS3 "3.3 Downstream few-shot adaptation to unseen tasks
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem"). In the rest of the section, we describe PRISE in more detail.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1](#S2.F1 "在字节对编码 (BPE) 中。 ‣ 2 预备知识 ‣ PRISE：将时间动作抽象学习作为序列压缩问题") 说明了 PRISE
    的关键步骤。总体而言，PRISE 首先学习一个状态依赖的动作量化模块（预训练 I 阶段）。该模块处理预训练的多任务数据集 $\mathcal{D}$，为每个数据分配一个标记，从而构建原始技能，如[图 1](#S2.F1
    "在字节对编码 (BPE) 中。 ‣ 2 预备知识 ‣ PRISE：将时间动作抽象学习作为序列压缩问题")（b）所示。一旦发现了这些原始技能标记，它们可以用来重新标记示例数据集，并通过行为克隆（BC）学习一个选择这些技能标记而非原始动作的策略，如我们在[第
    3.2 节](#S3.SS2 "3.2 多任务通用政策学习。 ‣ 3 算法 ‣ PRISE：将时间动作抽象学习作为序列压缩问题")和[第 3.3 节](#S3.SS3
    "3.3 向未见任务的下游少样本适应 ‣ 3 算法 ‣ PRISE：将时间动作抽象学习作为序列压缩问题")中展示的那样。在本节的其余部分，我们将更详细地描述
    PRISE。'
- en: 3.1 Pretraining
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 预训练
- en: 'Pretraining I: action quantization. This stage is illustrated in [Figure 1](#S2.F1
    "In Byte Pair Encoding (BPE). ‣ 2 Preliminaries ‣ PRISE: Learning Temporal Action
    Abstractions as a Sequence Compression Problem")(a), and its pseudocode is available
    in [Figure 11](#A3.F11 "In Appendix C Implementation Details ‣ PRISE: Learning
    Temporal Action Abstractions as a Sequence Compression Problem") in [Appendix C](#A3
    "Appendix C Implementation Details ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem"). Let $\mathcal{G}:\mathcal{O}^{T}\rightarrow\mathcal{Z}$
    into that action code. A similar intuition underlies action representation learning
    in, e.g., (Chandak et al., [2019](#bib.bib5)), although that work doesn’t attempt
    to simultaneously learn a latent state space.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练 I：动作量化。这个阶段在[图 1](#S2.F1 "在字节对编码 (BPE) 中。 ‣ 2 预备知识 ‣ PRISE：将时间动作抽象学习作为序列压缩问题")（a）中进行了说明，其伪代码在[图 11](#A3.F11
    "在附录 C 实现细节 ‣ PRISE：将时间动作抽象学习作为序列压缩问题")和[附录 C](#A3 "附录 C 实现细节 ‣ PRISE：将时间动作抽象学习作为序列压缩问题")中提供。让
    $\mathcal{G}:\mathcal{O}^{T}\rightarrow\mathcal{Z}$ 进入该动作编码。类似的直觉在动作表示学习中也存在，例如在
    (Chandak 等， [2019](#bib.bib5)) 中，尽管该工作没有尝试同时学习潜在状态空间。
- en: First, to ensure the action code can predict future states, we train a latent
    forward transition model $\mathcal{T}:\mathcal{Z}\times\mathcal{A}\rightarrow\mathcal{Z}$.
    To optimize the forward latent dynamics model while preventing state and action
    representation collapse, we adopt a BYOL-like objective inspired from (Schwarzer
    et al., [2021](#bib.bib39)), where we minimize
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了确保动作编码可以预测未来状态，我们训练一个潜在的前向转移模型 $\mathcal{T}:\mathcal{Z}\times\mathcal{A}\rightarrow\mathcal{Z}$。为了优化前向潜在动态模型，同时防止状态和动作表示崩溃，我们采用了一种类似
    BYOL 的目标，这一目标灵感来源于 (Schwarzer 等， [2021](#bib.bib39))，其中我们最小化
- en: '|  | $\displaystyle\mathcal{L}_{\text{dyn}}[$ |  | (1) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\text{dyn}}[$ |  | (1) |'
- en: Here, $o_{t,T}=o_{t},o_{t-1},...,o_{t-T+1}$ has 1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，$o_{t,T}=o_{t},o_{t-1},...,o_{t-T+1}$ 为 1。
- en: Next, to guarantee the raw action can be decoded from the action code and latent
    state effectively, we train a latent state-dependent decoder, $\psi$ following Mandlekar
    et al. ([2021](#bib.bib27)). This choice has been found effective in dealing with
    the inherent multimodality and noise in such human teleoperation demonstrations.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了保证可以有效地从动作编码和潜在状态中解码原始动作，我们训练了一个潜在状态依赖解码器 $\psi$，该解码器遵循 Mandlekar 等人 ([2021](#bib.bib27))
    的方法。这一选择在处理人类远程操作演示中的固有多模态性和噪声方面已被证明是有效的。
- en: 'Then the learning objective becomes to minimize the negative log likelihood
    of the GMM distribution:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 然后学习目标变成最小化 GMM 分布的负对数似然：
- en: '|  | $1$2 |  | (2) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (2) |'
- en: 'When we know the the pretraining data is collected by a fixed deterministic
    policy, we let $\psi:\mathcal{Z}\times\mathcal{E}\rightarrow\mathcal{A}$-loss:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们知道预训练数据是由固定的确定性策略收集时，我们将$\psi:\mathcal{Z}\times\mathcal{E}\rightarrow\mathcal{A}$-损失：
- en: '|  | $1$2 |  | (3) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (3) |'
- en: Combining the two objectives, we pretrain the state encoder and action encoder/quantization
    module by
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这两个目标，我们通过
- en: '|  | $\displaystyle\mathcal{L}[\theta_{\mathcal{F},\mathcal{G},Q,P,\psi}]=\mathcal{L}_{\text{dyn}}[\theta_{\mathcal{F},\mathcal{G},Q,P}]+\beta\mathcal{L}_{\text{act\_decode}}[\theta_{\mathcal{F},\mathcal{G},\psi}]$
    |  | (4) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}[\theta_{\mathcal{F},\mathcal{G},Q,P,\psi}]=\mathcal{L}_{\text{dyn}}[\theta_{\mathcal{F},\mathcal{G},Q,P}]+\beta\mathcal{L}_{\text{act\_decode}}[\theta_{\mathcal{F},\mathcal{G},\psi}]$
    |  | (4) |'
- en: Throughout the experiment, we set $\beta=1$ is parametrized by GMM to deal with
    numerical scale of the likelihood loss.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个实验中，我们设置$\beta=1$由GMM参数化，以处理似然损失的数值规模。
- en: 'Pretraining II: temporal action abstractions via BPE. [Figure 1](#S2.F1 "In
    Byte Pair Encoding (BPE). ‣ 2 Preliminaries ‣ PRISE: Learning Temporal Action
    Abstractions as a Sequence Compression Problem")(b) illustrates the mechanics
    of this stage. After training the action quantizer, we first use the pretrained
    observation embedding $\mathcal{G}$.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '预训练II：通过BPE进行时间动作抽象。[图1](#S2.F1 "在字节对编码（BPE）中 ‣ 2 基本 ‣ PRISE: 将时间动作抽象学习为序列压缩问题")(b)展示了这一阶段的机制。在训练动作量化器之后，我们首先使用预训练的观察嵌入$\mathcal{G}$。'
- en: '![Refer to caption](img/34ed44c3a64c2084cbb0ccfbce463b5b.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/34ed44c3a64c2084cbb0ccfbce463b5b.png)'
- en: 'Figure 2: Byte Pair Encoding.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：字节对编码。
- en: Summary. Overall, the pretraining of PRISE produces observation embedding $\mathcal{G}$
    of skill tokens. In the following, we discuss how to use them to learn generalist
    multitask policies and achieve efficient downstream adaptation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 总结。总体而言，PRISE的预训练生成了技能令牌的观察嵌入$\mathcal{G}$。接下来，我们讨论如何利用它们来学习通用的多任务策略，并实现高效的下游适应。
- en: 3.2 Multitask generalist policy learning.
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 多任务通用策略学习。
- en: Once we have learned the skill tokens that capture the common motion patterns
    shared across various tasks, we can leverage these skills to learn a multitask
    generalist policy. We train a high-level skill-token policy $\pi:\mathcal{Z}\rightarrow\Delta(\mathcal{V})$
    again to choose the next sequence of actions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们学习了捕捉跨任务共享的常见运动模式的技能令牌，我们就可以利用这些技能来学习多任务通用策略。我们再次训练一个高层次的技能令牌策略$\pi:\mathcal{Z}\rightarrow\Delta(\mathcal{V})$来选择下一序列的动作。
- en: '![Refer to caption](img/90fd936dd7840e060cca472fdc9519f7.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/90fd936dd7840e060cca472fdc9519f7.png)'
- en: 'Figure 3: During evaluation time, PRISE rollout its policy by querying the
    skill-token policy $\pi$ to decode actions.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：在评估时，PRISE通过查询技能令牌策略$\pi$来解码动作。
- en: To learn this skill-token policy $\pi$.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习这个技能令牌策略$\pi$。
- en: '![Refer to caption](img/7f21acec4a339b4a619ac43341375e06.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考图注](img/7f21acec4a339b4a619ac43341375e06.png)'
- en: 'Figure 4: PRISE tokenizes downstream demonstration trajectories by greedily
    searching for the longest token for each time step.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：PRISE通过贪婪地搜索每个时间步的最长令牌来对下游演示轨迹进行标记。
- en: 'We then train $\pi$ by minimizing the cross-entropy loss:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过最小化交叉熵损失来训练$\pi$：
- en: '|  | $\displaystyle\mathcal{L}_{\textbf{CE}}(\pi(\text{stopgrad}(z_{t})),\xi_{t})[\theta_{\pi}]$
    |  | (5) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\textbf{CE}}(\pi(\text{stopgrad}(z_{t})),\xi_{t})[\theta_{\pi}]$
    |  | (5) |'
- en: Note, that as stopgrad implies, we freeze the encoder $\mathcal{G}$.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，正如stopgrad所示，我们冻结了编码器$\mathcal{G}$。
- en: 3.3 Downstream few-shot adaptation to unseen tasks
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 下游少样本适应于未见过的任务
- en: In addition to learning a generalist multitask policy $\pi$ itself.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了学习一个通用的多任务策略$\pi$本身。
- en: 'We optimize $\psi$ in each trajectory has been omitted for clarity:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在每条轨迹中优化$\psi$的过程为了清晰起见已被省略：
- en: '|  | $\displaystyle\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]=\mathbb{E}_{\xi_{t}\sim\pi(\text{stopgrad}(z_{t}))}\Big{[}\mathcal{L}[\theta_{\psi}](\xi)\Big{]}$
    |  | (6) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]=\mathbb{E}_{\xi_{t}\sim\pi(\text{stopgrad}(z_{t}))}\Big{[}\mathcal{L}[\theta_{\psi}](\xi)\Big{]}$
    |  | (6) |'
- en: where
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $1$2 |  | (7) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (7) |'
- en: 'In this equation, $\hat{K}=\min(K,L_{\xi})$ is a hyperparameter, the motivation
    behind which is explained at the end of [Section 3.4](#S3.SS4 "3.4 Discussion
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem").'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '在这个方程中，$\hat{K}=\min(K,L_{\xi})$ 是一个超参数，其背后的动机在[第3.4节](#S3.SS4 "3.4 讨论 ‣ 3
    算法 ‣ PRISE: 将时间动作抽象学习为序列压缩问题")的末尾进行了说明。'
- en: 'We optimize skill-token policy $\pi$ in ([5](#S3.E5 "Equation 5 ‣ 3.2 Multitask
    generalist policy learning. ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem")). Thus, the overall objective is'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在([5](#S3.E5 "公式 5 ‣ 3.2 多任务通用策略学习 ‣ 3 算法 ‣ PRISE：将时间动作抽象作为序列压缩问题"))中优化技能令牌策略$\pi$。因此，整体目标是
- en: '|  | $\displaystyle\mathcal{L}[\theta_{\pi,\psi}]=\mathcal{L}_{\textbf{CE}}(\pi(\text{stopgrad}(z_{t})),\xi_{t})+\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]\vspace{-0.8em}$
    |  | (8) |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}[\theta_{\pi,\psi}]=\mathcal{L}_{\textbf{CE}}(\pi(\text{stopgrad}(z_{t})),\xi_{t})+\mathcal{L}_{\textbf{FT\_DECODER}}[\theta_{\pi,\psi}]\vspace{-0.8em}$
    |  | (8) |'
- en: Note that the gradients of the decoder finetuning loss $\mathcal{L}_{\textbf{FT\_DECODER}}$
    should be aware of the decoder error.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，解码器微调损失$\mathcal{L}_{\textbf{FT\_DECODER}}$的梯度应注意解码器错误。
- en: 'The underlying rationale for making the objective pay attention to matching
    both the skill tokens and the actions that these tokens generate is as follows.
    The number of skill tokens in the vocabulary is often large in order to encapsulate
    all motion patterns observed in the pretraining dataset, and simply minimizing
    the cross-entropy loss over the tokens with sparse data from few expert trajectories
    does not suffice to learn an accurate skill-token policy. Instead, the objective
    in ([8](#S3.E8 "Equation 8 ‣ 3.3 Downstream few-shot adaptation to unseen tasks
    ‣ 3 Algorithm ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem")) says that the skill-token policy $\pi$ should not only match the target
    token but also predict the tokens that have small decoding errors.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使目标关注于匹配技能令牌和这些令牌生成的动作的根本原因如下。词汇表中的技能令牌数量通常很大，以便封装在预训练数据集中观察到的所有运动模式，单纯通过最小化跨熵损失来处理稀疏数据的技能令牌不足以学习准确的技能令牌策略。相反，([8](#S3.E8
    "公式 8 ‣ 3.3 下游少样本适应未见任务 ‣ 3 算法 ‣ PRISE：将时间动作抽象作为序列压缩问题"))中的目标表示技能令牌策略$\pi$不仅应匹配目标令牌，还应预测具有小解码误差的令牌。
- en: 3.4 Discussion
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 讨论
- en: 'PRISE combines state-action abstraction (by optimizing the quantization loss
    in [Equation 4](#S3.E4 "In 3.1 Pretraining ‣ 3 Algorithm ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem")) and a temporal flavor
    of action abstraction (by applying BPE tokenization) to reduce the complexity
    of downstream IL problems. It is known that the complexity of IL depends on mainly
    two factors: the size of the state-action space and the problem horizon (Rajaraman
    et al., [2020](#bib.bib36)). The former determines the minimum complexity of the
    learner’s policy. The latter determines how much the behavior cloning error will
    compound. PRISE addresses these two factors by its state-action abstraction and
    temporal abstraction, respectively.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: PRISE结合了状态-动作抽象（通过优化[公式 4](#S3.E4 "在 3.1 预训练 ‣ 3 算法 ‣ PRISE：将时间动作抽象作为序列压缩问题")中的量化损失）和时间动作抽象（通过应用BPE分词）来降低下游IL问题的复杂性。已知IL的复杂性主要取决于两个因素：状态-动作空间的大小和问题范围（Rajaraman
    et al., [2020](#bib.bib36)）。前者决定了学习者策略的最小复杂性。后者决定了行为克隆误差的累积程度。PRISE通过其状态-动作抽象和时间抽象分别解决这两个因素。
- en: The effectiveness of PRISE’s abstraction scheme is determined by several hyperparameters.
    First, the codebook size, which determines the granularity of quantization, trades
    off the approximation error of state-action abstraction and the complexity of
    the state-action representation that the learner’s policy sees. We want the codebook
    to be large enough to represent the demonstrator’s policy well, but not too large
    in order to avoid several codes collapsing to the same set of actions in the original
    space. Many duplicate codes would reduce the amount of temporal abstraction PRISE
    can achieve.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: PRISE的抽象方案的有效性由几个超参数决定。首先，码本大小决定了量化的粒度，它在状态-动作抽象的近似误差和学习者策略看到的状态-动作表示复杂性之间进行权衡。我们希望码本足够大，以良好地表示演示者的策略，但不要过大，以避免多个代码在原始空间中收敛到同一组动作。许多重复的代码会减少PRISE能够实现的时间抽象量。
- en: 'Second, the size of the token vocabulary, which is at least as large as the
    codebook size by the design of BPE, not only affects the complexity of the state-action
    representation, but also controls how generalizable the learned tokens are. When
    the token vocabulary is large, BPE picks up less frequent patterns, which typically
    are longer. Although these tokens may help compress the length of the training
    data, they might overfit to the training trajectories and end up never getting
    used in downstream tasks. Moreover, the overfit tokens might be too long and lead
    to approximation errors when decoded back to the original action space. PRISE’s
    hierarchical learning scheme implicitly assumes the expert policy applies the
    action codes in an *open-loop* manner up the horizon $L_{\xi}$ (see [Section 3.3](#S3.SS3
    "3.3 Downstream few-shot adaptation to unseen tasks ‣ 3 Algorithm ‣ PRISE: Learning
    Temporal Action Abstractions as a Sequence Compression Problem")) that caps skills’
    possible horizon in downstream adaptation.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '其次，词汇表的大小（按 BPE 的设计至少与代码本大小相同）不仅影响状态-动作表示的复杂性，还控制学习到的词汇的泛化能力。当词汇表较大时，BPE 会选择不太频繁的模式，这些模式通常较长。尽管这些词汇可能有助于压缩训练数据的长度，但它们可能会过拟合训练轨迹，最终在下游任务中从未使用。此外，过拟合的词汇可能过长，在解码回原始动作空间时导致近似误差。PRISE
    的分层学习方案隐含地假设专家策略以*开环*方式在地平线 $L_{\xi}$ 上应用动作代码（参见[第3.3节](#S3.SS3 "3.3 下游少量适应未见任务
    ‣ 3 算法 ‣ PRISE: 将时间动作抽象作为序列压缩问题学习")），限制了技能在下游适应中的可能地平线。'
- en: On the contrary, when the token vocabulary size is set to be too small, the
    effects of temporal abstraction are minimal and would not reduce learning complexity.
    In the extreme, when the vocabulary size is the same as the codebook size, PRISE
    would only perform state-action abstraction by learning a discrete action space
    with state-action dependent encoder and decoder, without reducing the compounding
    error.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，当词汇表的大小设置得过小时，时间抽象的效果最小，无法减少学习复杂度。在极端情况下，当词汇表大小与代码本大小相同，PRISE 仅通过学习具有状态-动作依赖的编码器和解码器的离散动作空间来进行状态-动作抽象，而不会减少复合误差。
- en: 4 Related Work
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 相关工作
- en: 'Please refer to [Appendix A](#A1 "Appendix A Detailed Related Work ‣ PRISE:
    Learning Temporal Action Abstractions as a Sequence Compression Problem") for
    an extensive discussion of related work. Here we provide its summary'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '请参考[附录A](#A1 "附录 A 详细相关工作 ‣ PRISE: 将时间动作抽象作为序列压缩问题学习")了解相关工作的广泛讨论。这里我们提供其总结。'
- en: 'Temporal Action Abstractions in Sequential Decision-Making: The concept is
    rooted in the literature of fully and partially observable MDPs (Puterman, [1994](#bib.bib34);
    Sutton & Barto, [2018](#bib.bib41)). The complexity of policy learning is proportional
    to the problem horizon (Ross et al., [2011](#bib.bib38); Jiang et al., [2015](#bib.bib15);
    Rajaraman et al., [2020](#bib.bib36); Xie et al., [2021](#bib.bib47)), leading
    to hierarchical approaches (Parr & Russell, [1998](#bib.bib31); Barto & Mahadevan,
    [2003](#bib.bib3); Le et al., [2018](#bib.bib22); Nachum et al., [2018](#bib.bib28);
    Kipf et al., [2019](#bib.bib18); Kujanpää et al., [2023](#bib.bib21)) and the
    use of options or primitives (Sutton et al., [1999](#bib.bib42); Ajay et al.,
    [2021](#bib.bib1)).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 序列决策中的时间动作抽象：该概念根植于完全和部分可观测 MDP 的文献中（Puterman，[1994](#bib.bib34)；Sutton & Barto，[2018](#bib.bib41)）。策略学习的复杂性与问题的地平线成正比（Ross
    等，[2011](#bib.bib38)；Jiang 等，[2015](#bib.bib15)；Rajaraman 等，[2020](#bib.bib36)；Xie
    等，[2021](#bib.bib47)），导致了分层方法（Parr & Russell，[1998](#bib.bib31)；Barto & Mahadevan，[2003](#bib.bib3)；Le
    等，[2018](#bib.bib22)；Nachum 等，[2018](#bib.bib28)；Kipf 等，[2019](#bib.bib18)；Kujanpää
    等，[2023](#bib.bib21)）和使用选项或原语（Sutton 等，[1999](#bib.bib42)；Ajay 等，[2021](#bib.bib1)）。
- en: 'Temporal Abstraction Discovery for Decision-Making: Various methods like CompILE
    (Kipf et al., [2019](#bib.bib18)), RPL (Gupta et al., [2019](#bib.bib12)), OPAL
    (Ajay et al., [2021](#bib.bib1)), TAP (Jiang et al., [2023](#bib.bib16)), and
    ACT (Zhao et al., [2023a](#bib.bib51)) learn temporally extended action primitives
    in two stages. DDCO (Krishnan et al., [2017](#bib.bib19)), in contrast, learns
    both primitives and high-level policy simultaneously.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 决策中的时间抽象发现：各种方法如 CompILE（Kipf 等，[2019](#bib.bib18)），RPL（Gupta 等，[2019](#bib.bib12)），OPAL（Ajay
    等，[2021](#bib.bib1)），TAP（Jiang 等，[2023](#bib.bib16)）和 ACT（Zhao 等，[2023a](#bib.bib51)）以两阶段学习时间扩展的动作原语。相反，DDCO（Krishnan
    等，[2017](#bib.bib19)）同时学习原语和高级策略。
- en: PRISE differs in its use of BPE for inducing temporal action abstractions, avoiding
    challenges faced by CompILE (Kipf et al., [2019](#bib.bib18)), RPL, and OPAL.
    ACT (Zhao et al., [2023b](#bib.bib52)) is the closest to PRISE, especially in
    using BC, not RL, for downstream learning and handling pixel observations rather
    than low-level states.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: PRISE 在使用 BPE 进行时间性动作抽象时有所不同，避免了 CompILE（Kipf 等，[2019](#bib.bib18)）、RPL 和 OPAL
    面临的挑战。ACT（Zhao 等，[2023b](#bib.bib52)）与 PRISE 最为接近，特别是在使用 BC，而非 RL，进行下游学习以及处理像素观察而非低级状态。
- en: 'Policy Quantization Methods: PRISE relates to SAQ (Luo et al., [2023](#bib.bib24))
    and employs VQ-VAE (van den Oord et al., [2017](#bib.bib43)), with Conditional
    VAE (Kingma & Welling, [2014](#bib.bib17)) being another common choice.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 策略量化方法：PRISE 与 SAQ（Luo 等，[2023](#bib.bib24)）相关，并采用 VQ-VAE（van den Oord 等，[2017](#bib.bib43)），Conditional
    VAE（Kingma & Welling，[2014](#bib.bib17)）是另一个常见选择。
- en: 'Temporal Abstractions vs. Skills in Robot Learning: Distinct from skill acquisition
    approaches like DMP (Pastor et al., [2009](#bib.bib32)), Play-LMP (Lynch et al.,
    [2019](#bib.bib26)), and MCIL (Lynch & Sermanet, [2021](#bib.bib25)), PRISE and
    methods like TAP (Jiang et al., [2023](#bib.bib16)) focus on learning temporally
    extended action primitives.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人学习中的时间性抽象与技能：与技能获取方法如 DMP（Pastor 等，[2009](#bib.bib32)）、Play-LMP（Lynch 等，[2019](#bib.bib26)）和
    MCIL（Lynch & Sermanet，[2021](#bib.bib25)）不同，PRISE 和类似 TAP（Jiang 等，[2023](#bib.bib16)）的方法专注于学习时间上扩展的动作原语。
- en: 'Pretraining Data Assumptions: PRISE aligns more with Play-LMP (Lynch et al.,
    [2019](#bib.bib26)), RPL (Gupta et al., [2019](#bib.bib12)), MCIL (Lynch & Sermanet,
    [2021](#bib.bib25)), and TACO-RL (Rosete-Beas et al., [2022](#bib.bib37)), requiring
    meaningful behavioral patterns in pretraining data.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练数据假设：PRISE 更符合 Play-LMP（Lynch 等，[2019](#bib.bib26)）、RPL（Gupta 等，[2019](#bib.bib12)）、MCIL（Lynch
    & Sermanet，[2021](#bib.bib25)）和 TACO-RL（Rosete-Beas 等，[2022](#bib.bib37)），要求预训练数据中有意义的行为模式。
- en: 'Tokenization in Language Models: BPE (Gage, [1994](#bib.bib11)), Unigram (Kudo,
    [2018](#bib.bib20)), and WordPiece (Devlin et al., [2018](#bib.bib9)) are crucial
    in training language models. PRISE extends the next-token-prediction paradigm
    to continuous control, dealing with challenges like high-dimensional image observations
    and continuous action spaces.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型中的标记化：BPE（Gage，[1994](#bib.bib11)）、Unigram（Kudo，[2018](#bib.bib20)）和 WordPiece（Devlin
    等，[2018](#bib.bib9)）在训练语言模型中至关重要。PRISE 将下一个标记预测范式扩展到连续控制，处理高维图像观察和连续动作空间等挑战。
- en: 5 Experiments
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 实验
- en: 'In this section, we empirically evaluate the effectiveness of the skill tokens
    of PRISE. We run PRISE to pretrain these tokens using large-scale, multitask offline
    datasets. Then we evaluate them on two offline IL scenarios: learning a multitask
    generalist policy and few-shot adaptation to unseen tasks. We show that PRISE
    is able to improve the performance of the learner policy, both compared to an
    PRISE version approaches that doesn’t use the skill tokens and compared to a very
    strong existing approach, ACT Zhao et al. ([2023b](#bib.bib52)).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们实证评估了 PRISE 的技能标记的有效性。我们运行 PRISE 对这些标记进行预训练，使用大规模的多任务离线数据集。然后我们在两个离线
    IL 场景上进行评估：学习多任务通用策略和对未见任务的少样本适应。我们展示了 PRISE 能够提高学习者策略的表现，与不使用技能标记的 PRISE 版本和一个非常强的现有方法
    ACT Zhao 等（[2023b](#bib.bib52)）相比。
- en: 5.1 Pretraining datasets and architecture
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 预训练数据集和架构
- en: 'We evaluate PRISE on two multitask robotic manipulation benchmarks: Metaworld (Yu
    et al., [2019](#bib.bib49)) and LIBERO (Liu et al., [2023](#bib.bib23)). Below
    we describe the setups of these datasets and the architectures used in the experiments.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个多任务机器人操作基准上评估 PRISE：Metaworld（Yu 等，[2019](#bib.bib49)）和 LIBERO（Liu 等，[2023](#bib.bib23)）。下面我们描述这些数据集的设置以及实验中使用的架构。
- en: '![Refer to caption](img/64afc6f1162779ff2350dc0ebf2a68d6.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/64afc6f1162779ff2350dc0ebf2a68d6.png)'
- en: 'Figure 5: Few-shot IL on unseen tasks: pretrain and test tasks split for MetaWorld
    and LIBERO.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：未见任务的少样本 IL：MetaWorld 和 LIBERO 的预训练和测试任务拆分。
- en: For LIBERO, we pretrain on the 90 short-horizon tasks (LIBERO-90) with offline
    dataset provided by the original paper. We test the learned skill tokens both
    in terms of the multitask learning performance on LIBERO-90 as well as 5-shot
    imitation learning (IL) performance on the first 8 unseen tasks from LIBERO-LONG.
    The pretraining dataset contains 50 demonstration trajectories for each task,
    collected by human teleoperation. We use the exact architecture of ResNet-T in Liu
    et al. ([2023](#bib.bib23)) for the observation embedding.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LIBERO，我们在原始论文提供的离线数据集上对90个短视距任务（LIBERO-90）进行预训练。我们测试了学习到的技能标记，包括在LIBERO-90上的多任务学习性能和在LIBERO-LONG中前8个未见任务上的5-shot模仿学习（IL）性能。预训练数据集包含每个任务的50条演示轨迹，由人工遥控收集。我们使用Liu等人（[2023](#bib.bib23)）论文中的ResNet-T的精确架构进行观察嵌入。
- en: 'For MetaWorld, we focus on 5-shot IL since multitask IL on pretraining tasks
    is straightforward, with baseline algorithms, including PRISE, all achieving an
    average success rate of around 80%. We refer the readers to [Figure 13](#A4.F13
    "In Appendix D Additional Results on Multitask Learning ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem") in [Appendix D](#A4 "Appendix
    D Additional Results on Multitask Learning ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem") for the results. For the pretrain-test split,
    we hold out five hard tasks (hand-insert, box-close, disassemble, stick-pull,
    pick-place-wall) for few-shot evaluation and perform pretraining on the rest 45
    tasks. We generate 100 expert trajectories for each pretraining task using the
    scripted policy provided in MetaWorld. Same as in as in Yarats et al. ([2022](#bib.bib48)),
    we use a shallow CNN encoder to encode the observation, and a transformer decoder
    module as the temporal backbone to encode temporal information into the latent
    embedding $z_{t}$.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '对于MetaWorld，我们专注于5-shot IL，因为在预训练任务上的多任务IL比较直接，基准算法包括PRISE在内的所有算法的平均成功率约为80%。有关结果，请参见附录D中的[图13](#A4.F13
    "附录 D 多任务学习的附加结果 ‣ PRISE: 将时间动作抽象作为序列压缩问题进行学习")。对于预训练测试拆分，我们保留了五个困难任务（hand-insert,
    box-close, disassemble, stick-pull, pick-place-wall）用于少样本评估，并在其余45个任务上进行预训练。我们使用MetaWorld提供的脚本策略为每个预训练任务生成100个专家轨迹。与Yarats等人（[2022](#bib.bib48)）的方法相同，我们使用浅层CNN编码器对观察进行编码，并使用变压器解码模块作为时间骨干，将时间信息编码到潜在嵌入$z_{t}$中。'
- en: 'For BPE on both domains, we set the codebook size $C$ to be 10 and vocabulary
    size to be 150, and we provide detailed ablation study to analyze their impacts
    later in this section. For more implementation details, we refer the readers to
    Appendix [C](#A3 "Appendix C Implementation Details ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem").'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '对于两个领域的BPE，我们将码本大小$C$设为10，词汇表大小设为150，并在本节后面提供详细的消融研究以分析它们的影响。有关更多实现细节，请参见附录[C](#A3
    "附录 C 实现细节 ‣ PRISE: 将时间动作抽象作为序列压缩问题进行学习")。'
- en: '![Refer to caption](img/18cf1c7007af13d51492f225a737b5ee.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18cf1c7007af13d51492f225a737b5ee.png)'
- en: 'Figure 6: Multitask policy learning performance on LIBERO-90. Error bar represents
    the standard deviation across 3 random seeds. Results of the first three methods
    are taken from (Liu et al., [2023](#bib.bib23))'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：LIBERO-90上的多任务策略学习性能。误差条表示3个随机种子间的标准差。前三种方法的结果来自(Liu et al., [2023](#bib.bib23))。
- en: 5.2 Multitask generalist policy learning
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 多任务通用策略学习
- en: First we evaluate whether the pretrained skill tokens of PRISE could enable
    efficient knowledge sharing across tasks and improve multitask IL performance.
    Here we focus on LIBERO-90, a challenging multi-task benchmark where existing
    algorithms and architectures have not demonstrated satisfactory performance. For
    each algorithm in the following comparison, we first perform pretraining on the
    LIBERO-90, if applicable, and then train a multi-task generalist policy on the
    same dataset based on the pretrained outcomes (e.g., encoders, skill tokens).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们评估PRISE的预训练技能标记是否能够实现任务之间的高效知识共享，并改善多任务IL性能。这里我们专注于LIBERO-90，这是一个具有挑战性的多任务基准，现有算法和架构尚未表现出令人满意的性能。对于以下比较中的每种算法，我们首先在LIBERO-90上进行预训练（如果适用），然后根据预训练结果（例如编码器、技能标记）在同一数据集上训练一个多任务通用策略。
- en: 'Baselines. For multitask learning, our comparison includes PRISE and three
    network architectures for multitask behavior cloning (BC): ResNet-RNN, ResNet-T,
    and ViT-T, as implemented in (Liu et al., [2023](#bib.bib23)). These architectures
    utilize either ResNet-18 (He et al., [2016](#bib.bib14)) or ViT Dosovitskiy et al.
    ([2021](#bib.bib10)) to encode the pixel observations and then apply either a
    transformer or a LSTM module as temporal backbone to process a sequence of visual
    tokens. Recall that the main architecture of PRISE is the same as ResNet-T, except
    we add the vector quantization as well as other modules unique to PRISE. Additionally,
    we compare with ACT (Zhao et al., [2023a](#bib.bib51)), an IL algorithm that learns
    a generative model for sequences of actions, termed “action chunks,” rather than
    single-step actions. These action chunks can be seen as a form of temporal action
    abstraction. During policy rollout, ACT combines overlapping action chunks predicted
    by its policy using a weighted average. Thus, the critical hyperparameter of ACT
    is the length of the action chunk, which we set at 8 after evaluating the best
    option from five choices: $\{3,5,8,10,15\}$.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基线。对于多任务学习，我们的比较包括PRISE以及三种用于多任务行为克隆（BC）的网络架构：ResNet-RNN、ResNet-T和ViT-T，如(Liu
    et al., [2023](#bib.bib23))中所实现。这些架构利用ResNet-18（He et al., [2016](#bib.bib14)）或ViT（Dosovitskiy
    et al., [2021](#bib.bib10)）来编码像素观测，然后应用变换器或LSTM模块作为时间主干来处理一系列视觉标记。请记住，PRISE的主要架构与ResNet-T相同，只是我们增加了向量量化以及其他PRISE特有的模块。此外，我们还比较了ACT（Zhao
    et al., [2023a](#bib.bib51)），这是一种学习动作序列生成模型的IL算法，称为“动作块”，而不是单步动作。这些动作块可以看作是一种时间动作抽象。在策略回放期间，ACT通过加权平均来结合由其策略预测的重叠动作块。因此，ACT的关键超参数是动作块的长度，我们在从五个选择中评估最佳选项后，将其设置为8：$\{3,5,8,10,15\}$。
- en: 'Experimental Results. Figure [6](#S5.F6 "Figure 6 ‣ 5.1 Pretraining datasets
    and architecture ‣ 5 Experiments ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem") presents the average success rate across 90
    tasks in the LIBERO-90 dataset. It clearly demonstrates the significance of temporal
    action abstraction for knowledge sharing across diverse tasks, as Multitask ACT
    significantly outperforms the baseline BC approaches. More importantly, the use
    of pretrained skill tokens in PRISE  further leads to a substantial performance
    improvement over all other existing algorithms, underscoring the efficacy of PRISE’s
    pretrained skill tokens. For the details (e.g., per-task success rateas well as
    the evaluation protocol), we refer readers to Appendix [D](#A4 "Appendix D Additional
    Results on Multitask Learning ‣ PRISE: Learning Temporal Action Abstractions as
    a Sequence Compression Problem").'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果。图[6](#S5.F6 "Figure 6 ‣ 5.1 Pretraining datasets and architecture ‣ 5
    Experiments ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem")展示了LIBERO-90数据集中90个任务的平均成功率。这清楚地表明了时间动作抽象在不同任务间知识共享中的重要性，因为多任务ACT显著优于基线BC方法。更重要的是，PRISE中使用的预训练技能标记进一步在所有其他现有算法中带来了显著的性能提升，突显了PRISE预训练技能标记的有效性。有关详细信息（例如每任务成功率以及评估协议），我们请读者参考附录[D](#A4
    "Appendix D Additional Results on Multitask Learning ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem")。'
- en: 5.3 Few-shot adaptation to unseen tasks
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 少量样本适应新任务
- en: In addition to learning a generalist multitask policy, we show that the learned
    skill token of PRISE can also make learning a new task more efficient. Here, we
    evaluate the 5-shot IL performance for unseen tasks in both MetaWorld and LIBERO.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 除了学习通用多任务策略外，我们还展示了PRISE学习的技能标记如何使学习新任务更加高效。在这里，我们评估了MetaWorld和LIBERO中对未见任务的5-shot
    IL性能。
- en: 'Baselines. We compare PRISE with the baselines below. (1) BC from scratch.
    (2) BC with pretrained CNN encoder from PRISE: This is a more computationally
    efficient version of the first. We find that whether freezing the CNN encoder
    or not does not have statistically significant effects on the downstream performance.
    (3) PRISE without BPE: This is the same as PRISE with a skill-token vocabulary
    $V=\{1,...,C\}$ of just the quantized action codes. That is, there is no temporal
    abstraction. This baseline can be viewed not only as an ablated version of PRISE but
    also as an analog to existing hierarchical skill learning approaches such as OPAL (Ajay
    et al., [2021](#bib.bib1)) and TACO-RL (Rosete-Beas et al., [2022](#bib.bib37)).
    We refer readers to Section [4](#S4 "4 Related Work ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem") and Section [A](#A1 "Appendix
    A Detailed Related Work ‣ PRISE: Learning Temporal Action Abstractions as a Sequence
    Compression Problem") for a detailed discussion. (4) ACT (Zhao et al., [2023a](#bib.bib51))
    from scratch. (5) ACT (Zhao et al., [2023a](#bib.bib51)) with pretrained CNN encoder
    from PRISE. We freeze the CNN encoder and finetune other parts of the model.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '基准线。我们将 PRISE 与以下基准进行比较。 (1) 从头开始的 BC。 (2) 使用从 PRISE 预训练的 CNN 编码器的 BC：这是第一种方法的一个计算效率更高的版本。我们发现，无论是否冻结
    CNN 编码器对下游性能没有统计学上显著的影响。 (3) 没有 BPE 的 PRISE：这与具有技能标记词汇表 $V=\{1,...,C\}$ 的 PRISE
    相同，只包含量化的动作代码。也就是说，没有时间抽象。这一基准不仅可以被视为 PRISE 的一个消融版本，还可以视为现有层次技能学习方法的一个类比，例如 OPAL
    (Ajay et al., [2021](#bib.bib1)) 和 TACO-RL (Rosete-Beas et al., [2022](#bib.bib37))。我们建议读者参阅第[4](#S4
    "4 Related Work ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem)节和[A](#A1 "Appendix A Detailed Related Work ‣ PRISE: Learning Temporal
    Action Abstractions as a Sequence Compression Problem)节以获取详细讨论。 (4) 从头开始的 ACT
    (Zhao et al., [2023a](#bib.bib51))。 (5) 使用从 PRISE 预训练的 CNN 编码器的 ACT (Zhao et al.,
    [2023a](#bib.bib51))。我们冻结 CNN 编码器并微调模型的其他部分。'
- en: In few-shot learning, we train each baseline algorithm for 30 epochs and evaluate
    each algorithm every 3 epochs. We then report the best success rate for the 10
    evaluated checkpoints.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在少样本学习中，我们训练每个基准算法 30 个周期，并每隔 3 个周期评估一次每个算法。然后，我们报告 10 个评估检查点中最佳的成功率。
- en: '![Refer to caption](img/aec6252e6c3bbbde671da7ba4cd3b0ef.png)![Refer to caption](img/d5be07666d3a63fd9f46d83f1407e7de.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/aec6252e6c3bbbde671da7ba4cd3b0ef.png)![参见标题](img/d5be07666d3a63fd9f46d83f1407e7de.png)'
- en: 'Figure 7: (Left): 5-shot IL performance averaged across 5 unseen MetaWorld
    tasks. (Right): 5-shot IL performance averaged across 8 unseen LIBERO tasks. Error
    bar stands for the standard deviation across 3 random seeds.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: (左)：在 5 个未见 MetaWorld 任务中，5-shot IL 性能的平均值。 (右)：在 8 个未见 LIBERO 任务中，5-shot
    IL 性能的平均值。误差条表示 3 个随机种子的标准差。'
- en: 'Experimental Results (MetaWorld). In  [Figure 7](#S5.F7 "In 5.3 Few-shot adaptation
    to unseen tasks ‣ 5 Experiments ‣ PRISE: Learning Temporal Action Abstractions
    as a Sequence Compression Problem"), we plot the averaged task success rate of
    5-shot IL across 5 unseen tasks in MetaWorld. As shown in the figure, PRISE surpasses
    all other baselines (including PRISE w/o BPE) with a large margin, highlighting
    the effectiveness of the learned temporally extended skill tokens in adapting
    to unseen downstream tasks. Furthermore, we observe that with the pretrained visual
    encoder from PRISE, BC and ACT consistently yields improved results, indicating
    that the PRISE learning objective is also beneficial for pretraining visual representations
    from multitask offline data.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '实验结果 (MetaWorld)。在[图 7](#S5.F7 "In 5.3 Few-shot adaptation to unseen tasks
    ‣ 5 Experiments ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem")中，我们绘制了 MetaWorld 中 5 次尝试的 5-shot IL 的平均任务成功率。如图所示，PRISE 大幅超越了所有其他基准（包括没有
    BPE 的 PRISE），突出了学习的时间扩展技能标记在适应未见下游任务中的有效性。此外，我们观察到，使用 PRISE 中的预训练视觉编码器时，BC 和 ACT
    一致地取得了更好的结果，这表明 PRISE 学习目标对从多任务离线数据中预训练视觉表征也是有益的。'
- en: Experimental Results (LIBERO). We present the average 5-shot IL performance
    across 8 tasks of LIBERO in LABEL:fig:libero_unseen. Different from MetaWorld,
    as shown in the figure, PRISE pretrained encoder does not improve the performance
    of the base IL algorithms for BC and ACT. This is consistent with what is reported
    in (Liu et al., [2023](#bib.bib23)), where they observe that pretraining on LIBERO-90
    with multitask BC even leads to negative transfer to downstream tasks. However,
    with pretrained skill tokens from PRISE, we could improve the average success
    rate by 9.2% compared with the best baseline algorithms. This further demonstrates
    the effectiveness of the proposed skill tokenization mechanisms by PRISE.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 实验结果（LIBERO）。我们展示了 LABEL:fig:libero_unseen 中 LIBERO 8 个任务的平均 5-shot IL 性能。与
    MetaWorld 不同，如图所示，PRISE 预训练编码器并未提升 BC 和 ACT 基础 IL 算法的性能。这与 (Liu et al., [2023](#bib.bib23))
    的报告一致，他们观察到在 LIBERO-90 上进行的多任务 BC 预训练甚至导致了下游任务的负迁移。然而，利用 PRISE 的预训练技能标记，我们与最佳基线算法相比，平均成功率提高了
    9.2%。这进一步证明了 PRISE 提出的技能标记机制的有效性。
- en: 5.4 Ablation Analyses
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 消融分析
- en: Vocabulary Size. The vocabulary size of the BPE tokenizer, $|\mathcal{V}|$,
    indicating that as long as the vocabulary size is not too large or small, the
    performance should not vary significantly.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇大小。BPE 分词器的词汇大小 $|\mathcal{V}|$ 表明，只要词汇大小不太大或太小，性能应该不会有显著变化。
- en: '![Refer to caption](img/afa11c71fad0e71e69a7294727eb1060.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/afa11c71fad0e71e69a7294727eb1060.png)'
- en: 'Figure 8: (Left) Mean success rate of PRISE  across 8 unseen LIBERO tasks with
    different vocabulary size. (Right) The mean success rate of PRISE across 8 unseen
    LIBERO tasks with a different numbers of discrete action codes pre-trained.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8： (左) 不同词汇大小下 PRISE 在 8 个未见 LIBERO 任务上的平均成功率。 (右) 不同离散动作代码数量下 PRISE 在 8 个未见
    LIBERO 任务上的平均成功率。
- en: 'Number of Action Codes. Additionally, we verify the effects of varying the
    number of action codes, $C$ in [Figure 9](#S5.F9 "In 5.4 Ablation Analyses ‣ 5
    Experiments ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem") . The comparison reveals a clear reduction in token length with an increase
    in the number of codes.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '动作代码数量。此外，我们验证了动作代码数量 $C$ 的变化效果，如 [图 9](#S5.F9 "在 5.4 消融分析 ‣ 5 实验 ‣ PRISE:
    将时间动作抽象作为序列压缩问题") 所示。比较表明，随着代码数量的增加，标记长度明显减少。'
- en: '![Refer to caption](img/adbb3d57279314b9d47b096915111e04.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/adbb3d57279314b9d47b096915111e04.png)'
- en: 'Figure 9: (Left) Histogram of token length when $C=10$.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9： (左) 当 $C=10$ 时的标记长度直方图。
- en: 'Latent-forward-dynamics objective is crucial for learning good action codes.
    The forward-dynamics objective introduced in PRISE plays a crucial role in learning
    action code. Since we aim to learn a decoder $\psi(z_{t},e_{t})$ and leading to
    the collapse of action codes (i.e., different codes being decoded into identical
    actions). In Figure [10](#S5.F10 "Figure 10 ‣ 5.4 Ablation Analyses ‣ 5 Experiments
    ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression Problem"),
    we empirically compare the few-shot IL performance of PRISE in LIBERO with and
    without the forward dynamics objective. Indeed, we see a performance degradation
    in both domains when the forward dynamics objective is removed.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '潜在前向动态目标对学习良好的动作代码至关重要。PRISE 中引入的前向动态目标在学习动作代码时起着关键作用。由于我们旨在学习解码器 $\psi(z_{t},e_{t})$，这会导致动作代码崩溃（即不同代码被解码成相同动作）。在图
    [10](#S5.F10 "图 10 ‣ 5.4 消融分析 ‣ 5 实验 ‣ PRISE: 将时间动作抽象作为序列压缩问题") 中，我们实证比较了 LIBERO
    中 PRISE 在有无前向动态目标的少量样本 IL 性能。实际上，当移除前向动态目标时，我们看到两个领域的性能都出现了下降。'
- en: '![Refer to caption](img/d015f7d484bf75089e7c52268a87ddce.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d015f7d484bf75089e7c52268a87ddce.png)'
- en: 'Figure 10: Performance of PRISE with and without the forward dynamics objective
    on Metaworld and LIBERO.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：PRISE 在 Metaworld 和 LIBERO 上的前向动态目标有无的性能表现。
- en: Additionally, we conducted an extra experiment on LIBERO to illustrate the action
    collapsing problem further. Here, we sample a batch of 5000 data points $\{(z_{i},a_{i})\}_{i=1}^{5000}$
    for PRISE with and without the forward dynamics objective.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在 LIBERO 上进行了额外的实验，以进一步说明动作崩溃问题。在这里，我们为 PRISE 采样了一批 5000 个数据点 $\{(z_{i},a_{i})\}_{i=1}^{5000}$，比较了有无前向动态目标的效果。
- en: '|  | PRISE | PRISE w/o forward dynamics |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | PRISE | 无前向动态的 PRISE |'
- en: '| $\zeta$ | 72.42 | 14.53 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| $\zeta$ | 72.42 | 14.53 |'
- en: As illustrated in the table, PRISE without the forward dynamics objective exhibits
    a significantly smaller code-wise distance, demonstrating the necessity of the
    latent-forward-dynamics objective.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如表所示，PRISE 在没有前向动力学目标的情况下表现出显著较小的代码级距离，显示了潜在前向动力学目标的必要性。
- en: 6 Conclusion and Future Work
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来工作
- en: In this work, we propose a new temporal action method for continuous control
    scenarios,  PRISE, that leverages powerful NLP methodology during pretraining
    for sequential decision making tasks. By first pretraining a vector quantization
    module to discretize action into codes and then apply Byte Pair Encoding tokenization
    algorithm to learn temporally extended skill tokens, PRISE’s pretrained skill
    tokens can capture diverse motion patterns shared across pretraining tasks, enabling
    efficient multitask policy learning and few-shot adaptation to unseen tasks. One
    exciting future direction is to further scale up this approach to large real robot
    dataset with diverse robot embodiments such as Open Embodiment X (Collaboration
    et al., [2023](#bib.bib7)). Additionally, instead of finetuning the model to different
    downstream tasks tabula rasa, we could leverage the pretrained tokens to instruct
    finetune an existing large language model, so that we could leverage the power
    of foundational models for generalizing across different tasks and scenarios.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们提出了一种用于连续控制场景的新型时间动作方法，PRISE，它在预训练期间利用强大的 NLP 方法进行顺序决策任务。通过首先预训练一个向量量化模块将动作离散化为代码，然后应用字节对编码标记化算法学习时间扩展的技能标记，PRISE
    的预训练技能标记可以捕捉跨预训练任务共享的多样运动模式，从而实现高效的多任务策略学习和对未见任务的少样本适应。一个令人兴奋的未来方向是将这种方法进一步扩展到具有多样机器人体现的大型真实机器人数据集，例如
    Open Embodiment X（Collaboration 等，[2023](#bib.bib7)）。此外，我们可以利用预训练的标记来指导微调现有的大型语言模型，而不是从头开始微调模型，以便利用基础模型的力量在不同任务和场景中进行泛化。
- en: 7 Acknowledgements
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 致谢
- en: Zheng and Huang are supported by National Science Foundation NSF-IIS-2147276
    FAI, DOD-ONR-Office of Naval Research under award number N00014-22-1-2335, DOD-AFOSR-Air
    Force Office of Scientific Research under award number FA9550-23-1-0048, DOD-DARPA-Defense
    Advanced Research Projects Agency Guaranteeing AI Robustness against Deception
    (GARD) HR00112020007, Adobe, Capital One and JP Morgan faculty fellowships.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Zheng 和 Huang 得到了国家科学基金 NSF-IIS-2147276 FAI，DOD-ONR-海军研究办公室奖项号 N00014-22-1-2335，DOD-AFOSR-空军科学研究办公室奖项号
    FA9550-23-1-0048，DOD-DARPA-国防高级研究计划局保证 AI 对欺骗的鲁棒性（GARD）HR00112020007，Adobe，Capital
    One 和 JP Morgan 教授奖学金的支持。
- en: References
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Ajay et al. (2021) Ajay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum,
    O. OPAL: Offline primitive discovery for accelerating offline reinforcement learning.
    In *ICLR*, 2021. URL [https://openreview.net/forum?id=V69LGwJ0lIN](https://openreview.net/forum?id=V69LGwJ0lIN).'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ajay 等（2021）Ajay, A., Kumar, A., Agrawal, P., Levine, S., 和 Nachum, O. OPAL：离线原始发现以加速离线强化学习。在
    *ICLR* 中，2021。网址 [https://openreview.net/forum?id=V69LGwJ0lIN](https://openreview.net/forum?id=V69LGwJ0lIN)。
- en: 'Arora et al. (2022) Arora, K., Asri, L. E., Bahuleyan, H., and Cheung, J. C. K.
    Why exposure bias matters: An imitation learning perspective of error accumulation
    in language generation. In *Findings of ACL*, 2022.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arora 等（2022）Arora, K., Asri, L. E., Bahuleyan, H., 和 Cheung, J. C. K. 为什么暴露偏差很重要：一种语言生成错误累积的模仿学习视角。在
    *ACL 发现* 中，2022。
- en: 'Barto & Mahadevan (2003) Barto, A. and Mahadevan, S. Recent advances in hierarchical
    reinforcement learning. *Discrete Event Dynamic Systems: Theory and Applications*,
    13:41–77, 2003.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barto & Mahadevan（2003）Barto, A. 和 Mahadevan, S. 层次强化学习的最新进展。*离散事件动态系统：理论与应用*，13:41–77，2003。
- en: Brown et al. (2020) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
    Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.,
    Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess,
    B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. Language models are few-shot learners. In Larochelle, H., Ranzato, M., Hadsell,
    R., Balcan, M., and Lin, H. (eds.), *Advances in Neural Information Processing
    Systems*, volume 33, pp.  1877–1901\. Curran Associates, Inc., 2020. URL [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf).
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人（2020）Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter,
    C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J.,
    Berner, C., McCandlish, S., Radford, A., Sutskever, I., 和 Amodei, D. 语言模型是少样本学习者。见于
    Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., 和 Lin, H.（编），*神经信息处理系统进展*，第33卷，第1877–1901页。Curran
    Associates, Inc.，2020年。网址 [https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)。
- en: Chandak et al. (2019) Chandak, Y., Theocharous, G., Kostas, J., Jordan, S.,
    and Thomas, P. Learning action representations for reinforcement learning. In
    *International conference on machine learning*, pp.  941–950\. PMLR, 2019.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chandak 等人（2019）Chandak, Y., Theocharous, G., Kostas, J., Jordan, S., 和 Thomas,
    P. 强化学习中的行动表示学习。见于 *国际机器学习会议*，第941–950页。PMLR，2019年。
- en: 'Chen et al. (2021) Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin,
    M., Abbeel, P., Srinivas, A., and Mordatch, I. Decision transformer: Reinforcement
    learning via sequence modeling. In *NeurIPS*, 2021.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人（2021）Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin,
    M., Abbeel, P., Srinivas, A., 和 Mordatch, I. 决策变换器：通过序列建模进行强化学习。见于 *NeurIPS*，2021年。
- en: 'Collaboration et al. (2023) Collaboration, O. X.-E., Padalkar, A., Pooley,
    A., Jain, A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Singh,
    A., Brohan, A., Raffin, A., Wahid, A., Burgess-Limerick, B., Kim, B., Schölkopf,
    B., Ichter, B., Lu, C., Xu, C., Finn, C., Xu, C., Chi, C., Huang, C., Chan, C.,
    Pan, C., Fu, C., Devin, C., Driess, D., Pathak, D., Shah, D., Büchler, D., Kalashnikov,
    D., Sadigh, D., Johns, E., Ceola, F., Xia, F., Stulp, F., Zhou, G., Sukhatme,
    G. S., Salhotra, G., Yan, G., Schiavi, G., Su, H., Fang, H.-S., Shi, H., Amor,
    H. B., Christensen, H. I., Furuta, H., Walke, H., Fang, H., Mordatch, I., Radosavovic,
    I., Leal, I., Liang, J., Kim, J., Schneider, J., Hsu, J., Bohg, J., Bingham, J.,
    Wu, J., Wu, J., Luo, J., Gu, J., Tan, J., Oh, J., Malik, J., Tompson, J., Yang,
    J., Lim, J. J., Silvério, J., Han, J., Rao, K., Pertsch, K., Hausman, K., Go,
    K., Gopalakrishnan, K., Goldberg, K., Byrne, K., Oslund, K., Kawaharazuka, K.,
    Zhang, K., Majd, K., Rana, K., Srinivasan, K., Chen, L. Y., Pinto, L., Tan, L.,
    Ott, L., Lee, L., Tomizuka, M., Du, M., Ahn, M., Zhang, M., Ding, M., Srirama,
    M. K., Sharma, M., Kim, M. J., Kanazawa, N., Hansen, N., Heess, N., Joshi, N. J.,
    Suenderhauf, N., Palo, N. D., Shafiullah, N. M. M., Mees, O., Kroemer, O., Sanketi,
    P. R., Wohlhart, P., Xu, P., Sermanet, P., Sundaresan, P., Vuong, Q., Rafailov,
    R., Tian, R., Doshi, R., Martín-Martín, R., Mendonca, R., Shah, R., Hoque, R.,
    Julian, R., Bustamante, S., Kirmani, S., Levine, S., Moore, S., Bahl, S., Dass,
    S., Song, S., Xu, S., Haldar, S., Adebola, S., Guist, S., Nasiriany, S., Schaal,
    S., Welker, S., Tian, S., Dasari, S., Belkhale, S., Osa, T., Harada, T., Matsushima,
    T., Xiao, T., Yu, T., Ding, T., Davchev, T., Zhao, T. Z., Armstrong, T., Darrell,
    T., Jain, V., Vanhoucke, V., Zhan, W., Zhou, W., Burgard, W., Chen, X., Wang,
    X., Zhu, X., Li, X., Lu, Y., Chebotar, Y., Zhou, Y., Zhu, Y., Xu, Y., Wang, Y.,
    Bisk, Y., Cho, Y., Lee, Y., Cui, Y., hua Wu, Y., Tang, Y., Zhu, Y., Li, Y., Iwasawa,
    Y., Matsuo, Y., Xu, Z., and Cui, Z. J. Open X-Embodiment: Robotic learning datasets
    and RT-X models. [https://arxiv.org/abs/2310.08864](https://arxiv.org/abs/2310.08864),
    2023.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Collaboration 等（2023）Collaborations, O. X.-E., Padalkar, A., Pooley, A., Jain,
    A., Bewley, A., Herzog, A., Irpan, A., Khazatsky, A., Rai, A., Singh, A., Brohan,
    A., Raffin, A., Wahid, A., Burgess-Limerick, B., Kim, B., Schölkopf, B., Ichter,
    B., Lu, C., Xu, C., Finn, C., Xu, C., Chi, C., Huang, C., Chan, C., Pan, C., Fu,
    C., Devin, C., Driess, D., Pathak, D., Shah, D., Büchler, D., Kalashnikov, D.,
    Sadigh, D., Johns, E., Ceola, F., Xia, F., Stulp, F., Zhou, G., Sukhatme, G. S.,
    Salhotra, G., Yan, G., Schiavi, G., Su, H., Fang, H.-S., Shi, H., Amor, H. B.,
    Christensen, H. I., Furuta, H., Walke, H., Fang, H., Mordatch, I., Radosavovic,
    I., Leal, I., Liang, J., Kim, J., Schneider, J., Hsu, J., Bohg, J., Bingham, J.,
    Wu, J., Wu, J., Luo, J., Gu, J., Tan, J., Oh, J., Malik, J., Tompson, J., Yang,
    J., Lim, J. J., Silvério, J., Han, J., Rao, K., Pertsch, K., Hausman, K., Go,
    K., Gopalakrishnan, K., Goldberg, K., Byrne, K., Oslund, K., Kawaharazuka, K.,
    Zhang, K., Majd, K., Rana, K., Srinivasan, K., Chen, L. Y., Pinto, L., Tan, L.,
    Ott, L., Lee, L., Tomizuka, M., Du, M., Ahn, M., Zhang, M., Ding, M., Srirama,
    M. K., Sharma, M., Kim, M. J., Kanazawa, N., Hansen, N., Heess, N., Joshi, N.
    J., Suenderhauf, N., Palo, N. D., Shafiullah, N. M. M., Mees, O., Kroemer, O.,
    Sanketi, P. R., Wohlhart, P., Xu, P., Sermanet, P., Sundaresan, P., Vuong, Q.,
    Rafailov, R., Tian, R., Doshi, R., Martín-Martín, R., Mendonca, R., Shah, R.,
    Hoque, R., Julian, R., Bustamante, S., Kirmani, S., Levine, S., Moore, S., Bahl,
    S., Dass, S., Song, S., Xu, S., Haldar, S., Adebola, S., Guist, S., Nasiriany,
    S., Schaal, S., Welker, S., Tian, S., Dasari, S., Belkhale, S., Osa, T., Harada,
    T., Matsushima, T., Xiao, T., Yu, T., Ding, T., Davchev, T., Zhao, T. Z., Armstrong,
    T., Darrell, T., Jain, V., Vanhoucke, V., Zhan, W., Zhou, W., Burgard, W., Chen,
    X., Wang, X., Zhu, X., Li, X., Lu, Y., Chebotar, Y., Zhou, Y., Zhu, Y., Xu, Y.,
    Wang, Y., Bisk, Y., Cho, Y., Lee, Y., Cui, Y., hua Wu, Y., Tang, Y., Zhu, Y.,
    Li, Y., Iwasawa, Y., Matsuo, Y., Xu, Z., 和 Cui, Z. J. 开放 X-Embodiment：机器人学习数据集和
    RT-X 模型。 [https://arxiv.org/abs/2310.08864](https://arxiv.org/abs/2310.08864)，2023。
- en: den Oord et al. (2019) den Oord, A. V., Li, Y., and Vinyals, O. Representation
    learning with contrastive predictive coding, 2019.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: den Oord 等（2019）den Oord, A. V., Li, Y., 和 Vinyals, O. 对比预测编码的表征学习，2019。
- en: 'Devlin et al. (2018) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
    Pre-training of deep bidirectional transformers for language understanding. *arXiv
    preprint arXiv:1810.04805*, 2018.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Devlin 等（2018）Devlin, J., Chang, M.-W., Lee, K., 和 Toutanova, K. Bert：用于语言理解的深度双向变换器的预训练。
    *arXiv 预印本 arXiv:1810.04805*，2018。
- en: 'Dosovitskiy et al. (2021) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., and Houlsby, N. An image is worth 16x16 words: Transformers
    for image recognition at scale. In *International Conference on Learning Representations*,
    2021. URL [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy).'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 等（2021）Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
    D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly,
    S., Uszkoreit, J., 和 Houlsby, N. 一张图像胜过 16x16 个词：用于大规模图像识别的变换器。在 *国际学习表征会议*，2021。网址
    [https://openreview.net/forum?id=YicbFdNTTy](https://openreview.net/forum?id=YicbFdNTTy)。
- en: Gage (1994) Gage, P. A new algorithm for data compression. *C Users Journal*,
    12(2):23––38, 1994.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盖奇（1994）盖奇，P. 一种新的数据压缩算法。*C用户杂志*，12(2):23––38，1994。
- en: 'Gupta et al. (2019) Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman,
    K. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement
    learning. In *CoRL*, 2019. URL [https://proceedings.mlr.press/v100/gupta20a.html](https://proceedings.mlr.press/v100/gupta20a.html).'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 古普塔等（2019）古普塔，A.，库马尔，V.，林奇，C.，莱文，S.，和豪斯曼，K. 中继策略学习：通过模仿和强化学习解决长期任务。在*CoRL*，2019。网址
    [https://proceedings.mlr.press/v100/gupta20a.html](https://proceedings.mlr.press/v100/gupta20a.html)。
- en: Hausman et al. (2018) Hausman, K., Springenberg, J. T., Wang, Z., Heess, N.,
    and Riedmiller, M. Learning an embedding space for transferable robot skills.
    In *ICLR*, 2018.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 豪斯曼等（2018）豪斯曼，K.，斯普林伯根，J. T.，王，Z.，赫斯，N.，和里德米勒，M. 学习用于可迁移机器人技能的嵌入空间。在*ICLR*，2018。
- en: 'He et al. (2016) He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning
    for image recognition. In *2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR)*, pp.  770–778, 2016. doi: 10.1109/CVPR.2016.90.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '何等（2016）何，K.，张，X.，任，S.，和孙，J. 图像识别中的深度残差学习。在*2016 IEEE计算机视觉与模式识别会议（CVPR）*，第770–778页，2016。doi:
    10.1109/CVPR.2016.90。'
- en: Jiang et al. (2015) Jiang, N., Kulesza, A., Singh, S., and Lewis, R. The dependence
    of effective planning horizon on model accuracy. In *AAMAS*, 2015.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒋等（2015）蒋，N.，库勒扎，A.，辛格，S.，和刘易斯，R. 有效规划范围对模型准确性的依赖。在*AAMAS*，2015。
- en: Jiang et al. (2023) Jiang, Z., Zhang, T., Janner, M., Li, Y., Rocktäschel, T.,
    Grefenstette, E., and Tian, Y. Efficient planning in a compact latent action space.
    In *ICLR*, 2023. URL [https://openreview.net/forum?id=cA77NrVEuqn](https://openreview.net/forum?id=cA77NrVEuqn).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蒋等（2023）蒋，Z.，张，T.，贾纳，M.，李，Y.，罗克塔谢尔，T.，格雷芬斯特德，E.，和田，Y. 在紧凑潜在动作空间中的高效规划。在*ICLR*，2023。网址
    [https://openreview.net/forum?id=cA77NrVEuqn](https://openreview.net/forum?id=cA77NrVEuqn)。
- en: Kingma & Welling (2014) Kingma, D. and Welling, M. Auto-encoding variational
    Bayes. In *NIPS*, 2014.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金马和威灵（2014）金马，D.和威灵，M. 自编码变分贝叶斯。在*NIPS*，2014。
- en: 'Kipf et al. (2019) Kipf, T., Li, Y., Dai, H., Zambaldi, V., Sanchez-Gonzalez,
    A., Grefenstette, E., Kohli, P., and Battaglia, P. Compile: Compositional imitation
    learning and execution. In *ICML*, 2019.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基普夫等（2019）基普夫，T.，李，Y.，戴，H.，赞巴尔迪，V.，桑切斯-冈萨雷斯，A.，格雷芬斯特德，E.，科利，P.，和巴塔利亚，P. 编译：组合模仿学习和执行。在*ICML*，2019。
- en: 'Krishnan et al. (2017) Krishnan, S., Fox, R., Stoica, I., and Goldberg, K.
    DDCO: Discovery of deep continuous options for robot learning from demonstrations.
    In *CoRL*, 2017.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 克里什南等（2017）克里什南，S.，福克斯，R.，斯托伊卡，I.，和戈德堡，K. DDCO：从示范中发现深度连续选项以进行机器人学习。在*CoRL*，2017。
- en: 'Kudo (2018) Kudo, T. Subword regularization: Improving neural network translation
    models with multiple subword candidates. *arXiv preprint arXiv:1804.10959*, 2018.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 久保（2018）久保，T. 子词正则化：通过多个子词候选改进神经网络翻译模型。*arXiv预印本arXiv:1804.10959*，2018。
- en: Kujanpää et al. (2023) Kujanpää, K., Pajarinen, J., and Ilin, A. Hierarchical
    imitation learning with vector quantized models. In *ICML*, 2023. URL [https://proceedings.mlr.press/v202/kujanpaa23a.html](https://proceedings.mlr.press/v202/kujanpaa23a.html).
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 库扬帕等（2023）库扬帕，K.，帕贾宁，J.，和伊林，A. 带有向量量化模型的层次化模仿学习。在*ICML*，2023。网址 [https://proceedings.mlr.press/v202/kujanpaa23a.html](https://proceedings.mlr.press/v202/kujanpaa23a.html)。
- en: Le et al. (2018) Le, H. M., Jiang, N., Agarwal, A., Dudík, M., Yue, Y., and
    au2, H. D. I. Hierarchical imitation and reinforcement learning. In *ICML*, 2018.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乐等（2018）乐，H. M.，蒋，N.，阿加瓦尔，A.，杜迪克，M.，岳，Y.，和au2，H. D. I. 层次化模仿和强化学习。在*ICML*，2018。
- en: 'Liu et al. (2023) Liu, B., Zhu, Y., Gao, C., Feng, Y., qiang liu, Zhu, Y.,
    and Stone, P. LIBERO: Benchmarking knowledge transfer for lifelong robot learning.
    In *Thirty-seventh Conference on Neural Information Processing Systems Datasets
    and Benchmarks Track*, 2023. URL [https://openreview.net/forum?id=xzEtNSuDJk](https://openreview.net/forum?id=xzEtNSuDJk).'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 刘等（2023）刘，B.，朱，Y.，高，C.，冯，Y.，强 刘，朱，Y.，和斯通，P. LIBERO：终身机器人学习的知识迁移基准。在*第三十七届神经信息处理系统会议数据集和基准追踪*，2023。网址
    [https://openreview.net/forum?id=xzEtNSuDJk](https://openreview.net/forum?id=xzEtNSuDJk)。
- en: Luo et al. (2023) Luo, J., Dong, P., Wu, J., Kumar, A., Geng, X., and Levine,
    S. Action-quantized offline reinforcement learning for robotic skill learning.
    In *CoRL*, 2023. URL [https://openreview.net/forum?id=n9lew97SAn](https://openreview.net/forum?id=n9lew97SAn).
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 罗等（2023）罗，J.，董，P.，吴，J.，库马尔，A.，耿，X.，和莱文，S. 动作量化的离线强化学习用于机器人技能学习。在*CoRL*，2023。网址
    [https://openreview.net/forum?id=n9lew97SAn](https://openreview.net/forum?id=n9lew97SAn)。
- en: Lynch & Sermanet (2021) Lynch, C. and Sermanet, P. Language conditioned imitation
    learning over unstructured data. In *RSS*, 2021.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lynch & Sermanet (2021) Lynch, C. 和 Sermanet, P. 基于语言条件的模仿学习在非结构化数据上的应用。在 *RSS*，2021年。
- en: Lynch et al. (2019) Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J.,
    Levine, S., and Sermanet, P. Learning latent plans from play. In *CoRL*, 2019.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lynch 等人 (2019) Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J., Levine,
    S., 和 Sermanet, P. 从游戏中学习潜在计划。在 *CoRL*，2019年。
- en: Mandlekar et al. (2021) Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang,
    C., Kulkarni, R., Fei-Fei, L., Savarese, S., Zhu, Y., and Martín-Martín, R. What
    matters in learning from offline human demonstrations for robot manipulation.
    In *arXiv preprint arXiv:2108.03298*, 2021.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandlekar 等人 (2021) Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C.,
    Kulkarni, R., Fei-Fei, L., Savarese, S., Zhu, Y., 和 Martín-Martín, R. 从离线人类演示中学习机器人操作的关键因素。在
    *arXiv 预印本 arXiv:2108.03298*，2021年。
- en: Nachum et al. (2018) Nachum, O., Gu, S., Lee, H., and Levine, S. Data-efficient
    hierarchical reinforcement learning. In *NeurIPS*, 2018.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nachum 等人 (2018) Nachum, O., Gu, S., Lee, H., 和 Levine, S. 数据高效的层次强化学习。在 *NeurIPS*，2018年。
- en: 'Nair et al. (2022) Nair, S., Rajeswaran, A., Kumar, V., Finn, C., and Gupta,
    A. R3m: A universal visual representation for robot manipulation. In *CoRL*, 2022.
    URL [https://openreview.net/forum?id=tGbpgz6yOrI](https://openreview.net/forum?id=tGbpgz6yOrI).'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Nair 等人 (2022) Nair, S., Rajeswaran, A., Kumar, V., Finn, C., 和 Gupta, A. R3m:
    机器人操作的通用视觉表示。在 *CoRL*，2022年。URL [https://openreview.net/forum?id=tGbpgz6yOrI](https://openreview.net/forum?id=tGbpgz6yOrI)。'
- en: OpenAI (2023) OpenAI. Gpt-4 technical report. *ArXiv*, abs/2303.08774, 2023.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI. Gpt-4 技术报告。*ArXiv*，abs/2303.08774，2023年。
- en: Parr & Russell (1998) Parr, R. and Russell, S. Reinforcement learning with hierarchies
    of machines. In *NIPS*, pp.  1043–1049, 1998.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parr & Russell (1998) Parr, R. 和 Russell, S. 使用机器层次的强化学习。在 *NIPS*，第1043–1049页，1998年。
- en: Pastor et al. (2009) Pastor, P., Hoffmann, H., Asfour, T., and Schaal, S. Learning
    and generalization of motor skills by learning from demonstration. In *ICRA*,
    2009.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pastor 等人 (2009) Pastor, P., Hoffmann, H., Asfour, T., 和 Schaal, S. 通过示范学习和泛化运动技能。在
    *ICRA*，2009年。
- en: 'Perez et al. (2018) Perez, E., Strub, F., de Vries, H., Dumoulin, V., and Courville,
    A. C. Film: Visual reasoning with a general conditioning layer. In *AAAI*, 2018.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Perez 等人 (2018) Perez, E., Strub, F., de Vries, H., Dumoulin, V., 和 Courville,
    A. C. Film: 具有通用条件层的视觉推理。在 *AAAI*，2018年。'
- en: 'Puterman (1994) Puterman, M. L. *Markov decision processes: Discrete stochastic
    dynamic programming*. John Wiley and Sons, 1994.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Puterman (1994) Puterman, M. L. *马尔可夫决策过程：离散随机动态规划*。John Wiley and Sons，1994年。
- en: Radford et al. (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D.,
    and Sutskever, I. Language models are unsupervised multitask learners, 2019.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford 等人 (2019) Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., 和 Sutskever,
    I. 语言模型是无监督的多任务学习者，2019年。
- en: Rajaraman et al. (2020) Rajaraman, N., Yang, L., Jiao, J., and Ramchandran,
    K. Toward the fundamental limits of imitation learning. 2020.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rajaraman 等人 (2020) Rajaraman, N., Yang, L., Jiao, J., 和 Ramchandran, K. 朝着模仿学习的基本极限迈进。2020年。
- en: Rosete-Beas et al. (2022) Rosete-Beas, E., Mees, O., Kalweit, G., Boedecker,
    J., and Burgard, W. Latent plans for task agnostic offline reinforcement learning.
    2022.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rosete-Beas 等人 (2022) Rosete-Beas, E., Mees, O., Kalweit, G., Boedecker, J.,
    和 Burgard, W. 任务无关离线强化学习的潜在计划。2022年。
- en: Ross et al. (2011) Ross, S., Gordon, G. J., and Bagnell, J. A. A reduction of
    imitation learning and structured prediction to no-regret online learning. In
    *AISTATS*, 2011.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross 等人 (2011) Ross, S., Gordon, G. J., 和 Bagnell, J. A. 将模仿学习和结构预测简化为无悔在线学习。在
    *AISTATS*，2011年。
- en: Schwarzer et al. (2021) Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville,
    A., and Bachman, P. Data-efficient reinforcement learning with self-predictive
    representations. In *ICLR*, 2021. URL [https://openreview.net/forum?id=XpSAvlvnMa](https://openreview.net/forum?id=XpSAvlvnMa).
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schwarzer 等人 (2021) Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville,
    A., 和 Bachman, P. 数据高效的强化学习与自我预测表示。在 *ICLR*，2021年。URL [https://openreview.net/forum?id=XpSAvlvnMa](https://openreview.net/forum?id=XpSAvlvnMa)。
- en: Sennrich et al. (2016) Sennrich, R., Haddow, B., and Birch, A. Neural machine
    translation of rare words with subword units. In *ACL*, 2016.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sennrich 等人 (2016) Sennrich, R., Haddow, B., 和 Birch, A. 使用子词单元的稀有词神经机器翻译。在
    *ACL*，2016年。
- en: 'Sutton & Barto (2018) Sutton, R. and Barto, A. *Reinforcement learning: An
    introduction*. The MIT Press, 2nd edition, 2018.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sutton & Barto (2018) Sutton, R. 和 Barto, A. *强化学习：导论*。麻省理工学院出版社，第2版，2018年。
- en: 'Sutton et al. (1999) Sutton, R., Precup, D., and Singh, S. Between mdps and
    semi-mdps: A framework for temporal abstraction in reinforcement learning. *Artificial
    Intelligence*, 112(1-2):181–211, 1999.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sutton 等（1999）Sutton, R., Precup, D., 和 Singh, S. **Between mdps and semi-mdps:
    A framework for temporal abstraction in reinforcement learning**. *Artificial
    Intelligence*，112(1-2):181–211，1999。'
- en: van den Oord et al. (2017) van den Oord, A., Vinyals, O., and Kavukcuoglu, K.
    Neural discrete representation learning. In *NeurIPS*, 2017.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: van den Oord 等（2017）van den Oord, A., Vinyals, O., 和 Kavukcuoglu, K. **Neural
    discrete representation learning**. 发表在 *NeurIPS*，2017。
- en: Van den Oord et al. (2017) Van den Oord, A., Vinyals, O., and Kavukcuoglu, K.
    Neural discrete representation learning. In Guyon, I., Luxburg, U. V., Bengio,
    S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), *Advances
    in Neural Information Processing Systems*, volume 30\. Curran Associates, Inc.,
    2017. URL [https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van den Oord 等（2017）Van den Oord, A., Vinyals, O., 和 Kavukcuoglu, K. **Neural
    discrete representation learning**. 发表在 Guyon, I., Luxburg, U. V., Bengio, S.,
    Wallach, H., Fergus, R., Vishwanathan, S., 和 Garnett, R.（编辑）*Advances in Neural
    Information Processing Systems*，第 30 卷。Curran Associates, Inc.，2017。网址 [https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf](https://proceedings.neurips.cc/paper_files/paper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf)。
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need.
    In *NIPS*, 2017.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等（2017）Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A. N., Kaiser, L. u., 和 Polosukhin, I. **Attention is all you need**. 发表在
    *NIPS*，2017。
- en: Wei et al. (2023) Wei, Y., Sun, Y., Zheng, R., Vemprala, S., Bonatti, R., Chen,
    S., Madaan, R., Ba, Z., Kapoor, A., and Ma, S. Is imitation all you need? generalized
    decision-making with dual-phase training. In *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*, pp.  16221–16231, October 2023.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei 等（2023）Wei, Y., Sun, Y., Zheng, R., Vemprala, S., Bonatti, R., Chen, S.,
    Madaan, R., Ba, Z., Kapoor, A., 和 Ma, S. **Is imitation all you need? generalized
    decision-making with dual-phase training**. 发表在 *Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV)*，第 16221–16231 页，2023年10月。
- en: Xie et al. (2021) Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., and Agarwal,
    A. Bellman-consistent pessimism for offline reinforcement learning. In *NeurIPS*,
    2021.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等（2021）Xie, T., Cheng, C.-A., Jiang, N., Mineiro, P., 和 Agarwal, A. **Bellman-consistent
    pessimism for offline reinforcement learning**. 发表在 *NeurIPS*，2021。
- en: 'Yarats et al. (2022) Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. Mastering
    visual continuous control: Improved data-augmented reinforcement learning. In
    *International Conference on Learning Representations*, 2022. URL [https://openreview.net/forum?id=_SJ-_yyes8](https://openreview.net/forum?id=_SJ-_yyes8).'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yarats 等（2022）Yarats, D., Fergus, R., Lazaric, A., 和 Pinto, L. **Mastering
    visual continuous control: Improved data-augmented reinforcement learning**. 发表在
    *International Conference on Learning Representations*，2022。网址 [https://openreview.net/forum?id=_SJ-_yyes8](https://openreview.net/forum?id=_SJ-_yyes8)。'
- en: 'Yu et al. (2019) Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn,
    C., and Levine, S. Meta-world: A benchmark and evaluation for multi-task and meta
    reinforcement learning. In *Conference on Robot Learning (CoRL)*, 2019. URL [https://arxiv.org/abs/1910.10897](https://arxiv.org/abs/1910.10897).'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu 等（2019）Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., 和
    Levine, S. **Meta-world: A benchmark and evaluation for multi-task and meta reinforcement
    learning**. 发表在 *Conference on Robot Learning (CoRL)*，2019。网址 [https://arxiv.org/abs/1910.10897](https://arxiv.org/abs/1910.10897)。'
- en: Zhang et al. (2021) Zhang, A., McAllister, R. T., Calandra, R., Gal, Y., and
    Levine, S. Learning invariant representations for reinforcement learning without
    reconstruction. In *International Conference on Learning Representations*, 2021.
    URL [https://openreview.net/forum?id=-2FCwDKRREu](https://openreview.net/forum?id=-2FCwDKRREu).
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等（2021）Zhang, A., McAllister, R. T., Calandra, R., Gal, Y., 和 Levine,
    S. **Learning invariant representations for reinforcement learning without reconstruction**.
    发表在 *International Conference on Learning Representations*，2021。网址 [https://openreview.net/forum?id=-2FCwDKRREu](https://openreview.net/forum?id=-2FCwDKRREu)。
- en: 'Zhao et al. (2023a) Zhao, T. Z., Kumar, V., Levine, S., and Finn, C. Learning
    fine-grained bimanual manipulation with low-cost hardware. In Bekris, K. E., Hauser,
    K., Herbert, S. L., and Yu, J. (eds.), *Robotics: Science and Systems XIX, Daegu,
    Republic of Korea, July 10-14, 2023*, 2023a. doi: 10.15607/RSS.2023.XIX.016. URL
    [https://doi.org/10.15607/RSS.2023.XIX.016](https://doi.org/10.15607/RSS.2023.XIX.016).'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zhao 等（2023a）Zhao, T. Z., Kumar, V., Levine, S., 和 Finn, C. **Learning fine-grained
    bimanual manipulation with low-cost hardware**. 发表在 Bekris, K. E., Hauser, K.,
    Herbert, S. L., 和 Yu, J.（编辑）*Robotics: Science and Systems XIX, Daegu, Republic
    of Korea, July 10-14, 2023*，2023a。doi: 10.15607/RSS.2023.XIX.016。网址 [https://doi.org/10.15607/RSS.2023.XIX.016](https://doi.org/10.15607/RSS.2023.XIX.016)。'
- en: Zhao et al. (2023b) Zhao, T. Z., Kumar, V., Levine, S., and Finn, C. Learning
    fine-grained bimanual manipulation with low-cost hardware. In *RSS*, 2023b.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao 等（2023b）Zhao, T. Z., Kumar, V., Levine, S., 和 Finn, C. 使用低成本硬件进行细粒度双手操作学习。在
    *RSS*，2023b。
- en: 'Zheng et al. (2023) Zheng, R., Wang, X., Sun, Y., Ma, S., Zhao, J., Xu, H.,
    III, H. D., and Huang, F. TACO: Temporal latent action-driven contrastive loss
    for visual reinforcement learning. In *Thirty-seventh Conference on Neural Information
    Processing Systems*, 2023. URL [https://openreview.net/forum?id=ezCsMOy1w9](https://openreview.net/forum?id=ezCsMOy1w9).'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2023）Zheng, R., Wang, X., Sun, Y., Ma, S., Zhao, J., Xu, H., III, H.
    D., 和 Huang, F. TACO：用于视觉强化学习的时间潜在动作驱动对比损失。在 *第37届神经信息处理系统会议*，2023。网址 [https://openreview.net/forum?id=ezCsMOy1w9](https://openreview.net/forum?id=ezCsMOy1w9)。
- en: 'Zheng et al. (2024) Zheng, R., Liang, Y., Wang, X., Ma, S., au2, H. D. I.,
    Xu, H., Langford, J., Palanisamy, P., Basu, K. S., and Huang, F. Premier-taco
    is a few-shot policy learner: Pretraining multitask representation via temporal
    action-driven contrastive loss, 2024.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zheng 等（2024）Zheng, R., Liang, Y., Wang, X., Ma, S., au2, H. D. I., Xu, H.,
    Langford, J., Palanisamy, P., Basu, K. S., 和 Huang, F. Premier-taco 是一种少量样本策略学习器：通过时间驱动的对比损失进行多任务预训练，2024。
- en: Appendix A Detailed Related Work
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 详细相关工作
- en: Why learn temporal action abstractions? Learning temporally extended action
    primitives has a long history in sequential decision-making literature. In fully
    observable and partially observable MDPs (Puterman, [1994](#bib.bib34); Sutton
    & Barto, [2018](#bib.bib41)), the standard mathematical model for decision-making,
    every action takes one time step to execute. In most RL, IL, and planning algorithms,
    the decision space consists of individual actions, so standard methods produce
    policies that make a separate decision at every time step. It has long been informally
    observed and shown formally under various assumptions that the difficulty of policy
    learning scales with the problem horizon (Ross et al., [2011](#bib.bib38); Jiang
    et al., [2015](#bib.bib15); Rajaraman et al., [2020](#bib.bib36); Xie et al.,
    [2021](#bib.bib47)). This motivates hierarchical approaches (Parr & Russell, [1998](#bib.bib31);
    Barto & Mahadevan, [2003](#bib.bib3); Le et al., [2018](#bib.bib22); Nachum et al.,
    [2018](#bib.bib28); Kipf et al., [2019](#bib.bib18); Kujanpää et al., [2023](#bib.bib21)),
    which view the decision-making process as consisting of a high-level policy that
    identifies task substeps and invokes lower-level temporally extended routines
    sometimes called *options* (Sutton et al., [1999](#bib.bib42)) or *primitives* (Ajay
    et al., [2021](#bib.bib1)) to complete them. Conceptually, our PRISE method can
    be viewed as hierarchical imitation learning (HIL).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要学习时间动作抽象？学习时间扩展动作原语在序列决策文献中有着悠久的历史。在完全可观察和部分可观察的 MDPs（Puterman, [1994](#bib.bib34);
    Sutton & Barto, [2018](#bib.bib41)），决策的标准数学模型中，每个动作执行需一个时间步骤。在大多数 RL、IL 和规划算法中，决策空间由单独动作组成，因此标准方法产生的策略在每个时间步骤做出独立决策。长期以来，人们在各种假设下非正式地观察到并正式证明了，策略学习的难度随着问题的时间跨度而增加（Ross
    等, [2011](#bib.bib38); Jiang 等, [2015](#bib.bib15); Rajaraman 等, [2020](#bib.bib36);
    Xie 等, [2021](#bib.bib47)）。这激发了层次方法（Parr & Russell, [1998](#bib.bib31); Barto
    & Mahadevan, [2003](#bib.bib3); Le 等, [2018](#bib.bib22); Nachum 等, [2018](#bib.bib28);
    Kipf 等, [2019](#bib.bib18); Kujanpää 等, [2023](#bib.bib21)），这些方法将决策过程视为由高层策略组成，该策略识别任务子步骤并调用有时被称为
    *选项*（Sutton 等, [1999](#bib.bib42)）或 *原语*（Ajay 等, [2021](#bib.bib1)）的低层时间扩展程序来完成它们。在概念上，我们的
    PRISE 方法可以视为层次模仿学习（HIL）。
- en: Temporal abstraction discovery for decision-making. Recent methods that learn
    temporally extended action primitives and subsequently use them for shortening
    the effective decision-making horizon during high-level policy induction include
    CompILE (Kipf et al., [2019](#bib.bib18)), RPL (Gupta et al., [2019](#bib.bib12)),
    OPAL (Ajay et al., [2021](#bib.bib1)), TAP (Jiang et al., [2023](#bib.bib16)),
    and ACT (Zhao et al., [2023a](#bib.bib51)). They operate in two stages, learning
    the primitives during the first and applying them to solve a downstream task during
    the second, possibly adapting the primitives in the process. It is during this
    latter stage that the learned primitives provide their temporal abstraction benefits.
    This is subtly but crucially different from methods like DDCO (Krishnan et al.,
    [2017](#bib.bib19)), which learn the primitives and a higher-level policy simultaneously
    for a given task and benefit from the primitives via non-temporal mechanisms.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 用于决策的时间抽象发现。最近的方法包括 CompILE (Kipf 等, [2019](#bib.bib18))、RPL (Gupta 等, [2019](#bib.bib12))、OPAL
    (Ajay 等, [2021](#bib.bib1))、TAP (Jiang 等, [2023](#bib.bib16)) 和 ACT (Zhao 等, [2023a](#bib.bib51))，这些方法学习时间扩展的动作原语，然后在高层策略引导过程中使用这些原语来缩短有效的决策时间范围。它们分为两个阶段，首先学习原语，然后在第二阶段将其应用于下游任务，可能会在此过程中调整原语。在后一个阶段中，学习到的原语提供了它们的时间抽象优势。这与像
    DDCO (Krishnan 等, [2017](#bib.bib19)) 这样的方法 subtly 但至关重要地不同，DDCO 同时学习原语和给定任务的高级策略，通过非时间机制利用原语的优势。
- en: PRISE’s use of BPE is dissimilar from any other method of inducing temporal
    action abstractions that we are aware of, and sidesteps some of the challenges
    of these other methods. E.g., CompILE, which, like PRISE, learns *variable*-duration
    primitives, does so by segmenting pretraining demonstrations into semantically
    meaningful subtasks, for which it needs to know the number of subtasks *in each
    pretraining trajectory* and is sensitive to errors in these values (Kipf et al.,
    [2019](#bib.bib18)). RPL and OPAL avoid this complication but learn primitives
    of a fixed duration, determined by a hyperparameter. This hyperparameter is nontrivial
    to tune, because short primitive durations yield little gains from temporal abstraction
    during learning, and long ones cause many primitives to idle after achieving a
    subtask. This is distinct from PRISE’s hyperparameter $K$, which merely puts an
    *upper bound* on skills’ duration.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: PRISE 使用 BPE 的方式与我们所知的其他时间动作抽象诱导方法不同，并规避了这些方法的一些挑战。例如，CompILE 像 PRISE 一样学习 *可变*
    持续时间的原语，但通过将预训练演示分割成语义上有意义的子任务来实现，需要知道 *每个预训练轨迹* 中子任务的数量，并对这些值中的错误非常敏感 (Kipf 等,
    [2019](#bib.bib18))。RPL 和 OPAL 避免了这个复杂性，但学习的是固定持续时间的原语，由一个超参数决定。这个超参数难以调整，因为短原语持续时间在学习过程中从时间抽象中获得的收益很少，而长时间原语在完成子任务后会使许多原语闲置。这与
    PRISE 的超参数 $K$ 不同，$K$ 只是对技能的持续时间设置了 *上限*。
- en: We view ACT (Zhao et al., [2023b](#bib.bib52)) as the most comparable method
    to PRISE and use it as a baseline in our experiments. Like PRISE, ACT handles
    high-dimensional pixel observations out of the box and, crucially, uses BC during
    downstream learning. In contrast CompILE, RPL, and OPAL assume access to ground-truth
    states and rely on online RL for finetuning. In many physical continuous control
    scenarios such as robotics, BC is arguably a more practical algorithm, and benefits
    that temporal abstractions provide for BC are expected to be different from those
    in RL, where they enable more efficient exploration.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为 ACT (Zhao 等, [2023b](#bib.bib52)) 是与 PRISE 最可比的方法，并在我们的实验中使用它作为基线。与 PRISE
    一样，ACT 直接处理高维像素观测，关键是，在下游学习中使用 BC。相比之下，CompILE、RPL 和 OPAL 假设可以访问真实状态，并依赖在线 RL
    进行微调。在许多物理连续控制场景（如机器人技术）中，BC 可以说是更实际的算法，而时间抽象对 BC 的益处预计与 RL 中的不同，因为它们使得探索更高效。
- en: Policy quantization methods. PRISE’s state-conditioned policy quantization method
    is related to SAQ (Luo et al., [2023](#bib.bib24)), but, crucially, also works
    for pixel observations characteristic of many continuous control scenarios in
    robotics. The policy encoder at the heart of PRISE and SAQ is VQ-VAE (van den
    Oord et al., [2017](#bib.bib43)). Conditional VAE (Kingma & Welling, [2014](#bib.bib17))
    is another common choice for this role (see, e.g., Lynch et al. ([2019](#bib.bib26))
    and Kipf et al. ([2019](#bib.bib18))).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 策略量化方法。PRISE的状态条件策略量化方法与SAQ（Luo等，[2023](#bib.bib24)）相关，但至关重要的是，它也适用于许多机器人连续控制场景中的像素观察。PRISE和SAQ核心的策略编码器是VQ-VAE（van
    den Oord等，[2017](#bib.bib43)）。条件VAE（Kingma & Welling，[2014](#bib.bib17)）是另一种常见的选择（例如，见Lynch等（[2019](#bib.bib26)）和Kipf等（[2019](#bib.bib18)））。
- en: Temporal abstractions versus skills in robot learning. Many works on robot learning
    focus on the acquisition of *skills*. While seemingly similar to an option or
    a primitive, this term has a somewhat different meaning, and most of these works
    don’t use skills for temporal abstraction, with TACO-RL (Rosete-Beas et al., [2022](#bib.bib37))
    being a notable exception. Namely, approaches such as DMP  (Pastor et al., [2009](#bib.bib32)),
     (Hausman et al., [2018](#bib.bib13)), Play-LMP (Lynch et al., [2019](#bib.bib26)),
    and MCIL (Lynch & Sermanet, [2021](#bib.bib25)) aim to learn a multi-task goal-conditioned
    policy, and understand a skill as a latent plan for achieving a goal or several
    goal variations starting from a variety of initial states. In this paradigm, learning
    a multitask policy constitutes embedding plans into a continuous latent space,
    which happens without using those plans to shorten a task’s horizon. Indeed, the
    learned skills usually don’t have a termination condition (although TACO-RL assumes
    having a mechanism for detecting subgoal attainment), and policies produced by
    the aforementioned methods resample a latent skill at every time step. This is
    in contrast to PRISE and, e.g., TAP (Jiang et al., [2023](#bib.bib16)) that learn
    temporally extended action primitives and apply them for several time steps during
    deployment.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人学习中的时间抽象与技能。许多关于机器人学习的工作集中在获取*技能*上。虽然看似与选项或原语类似，但这一术语含义有所不同，而且大多数这些工作并不使用技能进行时间抽象，TACO-RL（Rosete-Beas等，[2022](#bib.bib37)）是一个显著的例外。即，像DMP（Pastor等，[2009](#bib.bib32)）、（Hausman等，[2018](#bib.bib13)）、Play-LMP（Lynch等，[2019](#bib.bib26)）和MCIL（Lynch
    & Sermanet，[2021](#bib.bib25)）等方法旨在学习一个多任务目标条件策略，并将技能理解为从各种初始状态开始实现目标或多个目标变体的潜在计划。在这一范式中，学习一个多任务策略就是将计划嵌入到一个连续的潜在空间中，这发生在没有使用这些计划来缩短任务的视野的情况下。确实，学习的技能通常没有终止条件（尽管TACO-RL假设有检测子目标达成的机制），上述方法产生的策略在每个时间步重新抽样潜在技能。这与PRISE和，例如，TAP（Jiang等，[2023](#bib.bib16)）学习时间扩展的动作原语并在部署期间应用它们进行几个时间步的方式形成对比。
- en: Pretraining data assumptions. In terms of the assumptions PRISE imposes on the
    trajectories it uses for pretraining, it is more similar to Play-LMP (Lynch et al.,
    [2019](#bib.bib26)), RPL (Gupta et al., [2019](#bib.bib12)), MCIL (Lynch & Sermanet,
    [2021](#bib.bib25)), and TACO-RL (Rosete-Beas et al., [2022](#bib.bib37)) rather
    than to IL or offline RL. Namely, PRISE needs this data to contain meaningful
    behavioral patters, which it extracts using an imitation-like procedure (as does
    RPL). While PRISE could be applied to arbitrary-quality data commonly assumed
    by offline RL or some state representation pretraining algorithms such as  Schwarzer
    et al. ([2021](#bib.bib39)); Wei et al. ([2023](#bib.bib46)); Zheng et al. ([2023](#bib.bib53),
    [2024](#bib.bib54)), many of the patterns there are unlikely to be useful, and
    may not easily aggregate into common temporally extended sequences that BPE aims
    to extract. On the other hand, the pretraining data doesn’t need to be imitation-quality
    either.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练数据假设。在PRISE对其用于预训练的轨迹施加的假设方面，它更类似于Play-LMP（Lynch等，[2019](#bib.bib26)）、RPL（Gupta等，[2019](#bib.bib12)）、MCIL（Lynch
    & Sermanet，[2021](#bib.bib25)）和TACO-RL（Rosete-Beas等，[2022](#bib.bib37)），而不是IL或离线RL。即，PRISE需要这些数据包含有意义的行为模式，通过类似模仿的程序（如RPL）提取这些模式。虽然PRISE可以应用于离线RL或某些状态表示预训练算法（如Schwarzer等（[2021](#bib.bib39)）；Wei等（[2023](#bib.bib46)）；Zheng等（[2023](#bib.bib53)，[2024](#bib.bib54)））通常假设的任意质量数据，但其中许多模式不太可能有用，可能不会容易汇聚成BPE旨在提取的常见时间扩展序列。另一方面，预训练数据也不需要是模仿质量的。
- en: Tokenization in language models Subword tokenization methods such as BPE (Gage,
    [1994](#bib.bib11)), Unigram (Kudo, [2018](#bib.bib20)), and WordPiece (Devlin
    et al., [2018](#bib.bib9)) play an important role in training modern large language
    models. These algorithms learn tokens from a large corpus texts to mine reusable
    patterns. A trained tokenizer is used to compress training text data into tokens,
    and a large language model is trained to predict over this compressed tokens.
    Compared to directly predicting at the alphabet level, predicting tokens (with
    a proper vocabulary size) allows better generalization, because the effective
    sequence length that the model needs to predict is shorter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 语言模型中的分词 Subword分词方法，如BPE (Gage, [1994](#bib.bib11))、Unigram (Kudo, [2018](#bib.bib20))
    和WordPiece (Devlin et al., [2018](#bib.bib9)) 在训练现代大型语言模型中发挥了重要作用。这些算法从大量语料中学习标记，以挖掘可重用的模式。经过训练的分词器用于将训练文本数据压缩为标记，并且大型语言模型被训练以预测这些压缩的标记。与直接在字母级别进行预测相比，预测标记（具有适当的词汇大小）允许更好的泛化，因为模型需要预测的有效序列长度更短。
- en: Next token prediction in NLP is a special case of behavior cloning in the context
    of decision making, where the state is the history of tokens (context). Therefore,
    our algorithm can be viewed as extension of the next-token-prediction paradigm
    in NLP to the continuous control domain. However, our setup introduces additional
    complexities not faced in NLP. Our raw action space is continuous rather than
    discrete finite alphabets in NLP. In addition, our decision agents need to consider
    high-dimensional image observations as part of the state representation, whereas
    in NLP the history is simply past tokens. Namely, if we view alphabets in NLP
    as action vectors here, the BC problem in NLP has a stateless dynamical system,
    but in, e.g., robotics it has a non-trivial internal state. These complications
    motivate the need for observation and action encodes as well as an action quantizer.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: NLP中的下一标记预测是决策制定背景下行为克隆的特殊情况，其中状态是标记的历史（上下文）。因此，我们的算法可以视为NLP中下一标记预测范式向连续控制领域的扩展。然而，我们的设置引入了在NLP中未遇到的额外复杂性。我们的原始动作空间是连续的，而不是NLP中的离散有限字母。此外，我们的决策代理需要将高维图像观察作为状态表示的一部分，而在NLP中历史仅仅是过去的标记。即，如果我们将NLP中的字母视为这里的动作向量，那么NLP中的BC问题有一个无状态的动态系统，但例如在机器人技术中，它有一个非平凡的内部状态。这些复杂性促使我们需要观察和动作编码器以及动作量化器。
- en: Appendix B Additional Experimental Results
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B 额外实验结果
- en: 'In Figure [7](#S5.F7 "Figure 7 ‣ 5.3 Few-shot adaptation to unseen tasks ‣
    5 Experiments ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem"), we present the results of mean success rate across 5 tasks in MetaWorld.
    Here we present the detailed results for each of the downstream unseen task.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '在图 [7](#S5.F7 "图7 ‣ 5.3 对未见任务的少量样本适应 ‣ 5 实验 ‣ PRISE: 将时间动作抽象学习作为序列压缩问题")中，我们展示了在MetaWorld中对5个任务的平均成功率的结果。在这里，我们展示每个下游未见任务的详细结果。'
- en: '| MetaWorld | Algorithms |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| MetaWorld | 算法 |'
- en: '| Unseen Tasks | BC (Scratch) | BC (Pretrained) | ACT (Scratch) | ACT (Pretrained)
    | PRISE w/o BPE | PRISE |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 未见任务 | BC (从头开始) | BC (预训练) | ACT (从头开始) | ACT (预训练) | PRISE 无 BPE | PRISE
    |'
- en: '| Hand Insert | $18.8\pm 12.5$ |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 手动插入 | $18.8\pm 12.5$ |'
- en: '| Box Close | $19.4\pm 8.7$ |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 盒子关闭 | $19.4\pm 8.7$ |'
- en: '| Disassemble | $8.1\pm 17.3$ |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 拆解 | $8.1\pm 17.3$ |'
- en: '| Stick Pull | $20.6\pm 5.8$ |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 棍子拉动 | $20.6\pm 5.8$ |'
- en: '| Pick Place Wall | $19.4\pm 6.4$ |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 拾取放置墙 | $19.4\pm 6.4$ |'
- en: '| Mean | $17.3\pm 4.2$ |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | $17.3\pm 4.2$ |'
- en: 'Table 1: 5-shot imitation learning performance on five unseen tasks in MetaWorld.
    Results are averaged across 3 random seeds.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在MetaWorld中对五个未见任务进行的5-shot模仿学习性能。结果是对3个随机种子的平均值。
- en: 'For LIBERO, in Table [2](#A2.T2 "Table 2 ‣ Appendix B Additional Experimental
    Results ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem"), we first provide the task instruction for each of the downstream tasks.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '对于LIBERO，在表 [2](#A2.T2 "表2 ‣ 附录B 额外实验结果 ‣ PRISE: 将时间动作抽象学习作为序列压缩问题")中，我们首先提供每个下游任务的任务指令。'
- en: '| Task ID | Task Scene | Task Instruction |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 任务ID | 任务场景 | 任务指令 |'
- en: '| 0 | living room scene2 | put both the alphabet soup and the tomato sauce
    in the basket |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 客厅场景2 | 将字母汤和番茄酱都放入篮子中 |'
- en: '| 1 | living room scene2 | put both the cream cheese box and the butter in
    the basket |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 客厅场景2 | 将奶油奶酪盒和黄油都放入篮子中 |'
- en: '| 2 | kitchen scene3 | turn on the stove and put the moka pot on it |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 厨房场景3 | 打开炉子并将摩卡壶放在上面 |'
- en: '| 3 | kitchen scene4 | put the black bowl in the bottom drawer of the cabinet
    and close it |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 厨房场景4 | 将黑色碗放在橱柜的底层抽屉中并关闭 |'
- en: '| 4 | living room scene5 | put the white mug on the left plate and put the
    yellow and white mug on the right plate |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 客厅场景5 | 将白色杯子放在左边的盘子上，将黄色和白色杯子放在右边的盘子上 |'
- en: '| 5 | study scene1 | pick up the book and place it in the back compartment
    of the caddy |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 学习场景1 | 拿起书本并将其放在小车的后部隔间中 |'
- en: '| 6 | living room scene6 | put the white mug on the plate and put the chocolate
    pudding to the right of the plate |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 客厅场景6 | 将白色杯子放在盘子上，并将巧克力布丁放在盘子的右侧 |'
- en: '| 7 | living room scene1 | put both the alphabet soup and the cream cheese
    box in the basket |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 客厅场景1 | 将字母汤和奶油奶酪盒都放入篮子中 |'
- en: 'Table 2: Language instructions for 8 LIBERO downstream tasks.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '表 2: 8个LIBERO下游任务的语言指令。'
- en: 'The detailed results for each of the LIBERO downstream unseen task are presented
    in Table [3](#A2.T3 "Table 3 ‣ Appendix B Additional Experimental Results ‣ PRISE:
    Learning Temporal Action Abstractions as a Sequence Compression Problem").'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 'LIBERO下游未见任务的详细结果见表[3](#A2.T3 "Table 3 ‣ Appendix B Additional Experimental
    Results ‣ PRISE: Learning Temporal Action Abstractions as a Sequence Compression
    Problem")。'
- en: '| LIBERO | Algorithms |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| LIBERO | 算法 |'
- en: '| Unseen Tasks | BC (Scratch) | BC (Pretrained) | ACT (Scratch) | ACT (Pretrained)
    | PRISE w/o BPE | PRISE |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 未见任务 | BC (从头开始) | BC (预训练) | ACT (从头开始) | ACT (预训练) | PRISE 无 BPE | PRISE
    |'
- en: '| Task 0 | $16.7\pm 6.2$ |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 任务 0 | $16.7\pm 6.2$ |'
- en: '| Task 1 | $25.0\pm 4.7$ |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 任务 1 | $25.0\pm 4.7$ |'
- en: '| Task 2 | $68.3\pm 7.4$ |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 任务 2 | $68.3\pm 7.4$ |'
- en: '| Task 3 | $66.7\pm 7.9$ |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 任务 3 | $66.7\pm 7.9$ |'
- en: '| Task 4 | $25.0\pm 10.8$ |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 任务 4 | $25.0\pm 10.8$ |'
- en: '| Task 5 | $70.0\pm 0.0$ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 任务 5 | $70.0\pm 0.0$ |'
- en: '| Task 6 | $23.3\pm 4.3$ |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 任务 6 | $23.3\pm 4.3$ |'
- en: '| Task 7 | $28.3\pm 4.3$ |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 任务 7 | $28.3\pm 4.3$ |'
- en: '| Mean | $40.4\pm 1.2$ |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 平均值 | $40.4\pm 1.2$ |'
- en: 'Table 3: 5-shot imitation learning performance across 8 unseen tasks in LIBERO.
    Resullts are averaged across 3 random seeds'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: LIBERO中8个未见任务的5-shot模仿学习性能。结果在3个随机种子中取平均。'
- en: Appendix C Implementation Details
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C 实现细节
- en: '1#  next_observation:  A  sequence  of  K=3  future  pixel  observations  o_{t+1},...,  o_{t+K}2#  past_observation:  A  sequence  of  H  historical  pixel  observations  o_{t-H+1},...,  o_{t}3#  action:  A  sequence  of  $K$  action  a_t,...,  a_{t+K-1}4#  obs_encoder:  Observation  Encoder  (CNN+Transformer)5#  quantizer:  Discrete  action  code  quantizer6#  dynamics:  Latent  Dynamics  Model7#  project:  Projection  layer8#  predictor:  Latent  state  predictor9#  action_decoder:  Quantized  action  code  decoder10#  beta:  Weight  of  decoder  loss  (1.0  in  MetaWorld  and  0.01  in  LIBERO  for  #  numerical  stability)1112dynamic_losses,  quantization_losses,  decoder_losses  =  0,  0,  013z  =  state_encoder(past_observation)14z_hat  =  z15for  k  in  range(K):16  z  =  state_encoder(past_observation)17  past_observation.push(next_observation[k])18  u_quantized,  quantization_loss  =  quantizer(action[k],  z.detach())19  quantization_losses  +=  quantization_loss20  decode_action  =  decoder(z,  u_quantized)21  decoder_losses  +=  action_loss(decode_action,  action[k])22  z_hat  =  dynamics(z_hat,  u_quantized)23  z_next  =  state_encoder(observation[k+1])24  y_hat  =  predictor(project(z_hat))25  y_next  =  project(z_next).detach()26  dynamic_losses  +=  -cosine_similarity(y_hat,  y_next)27(dynamic_losses  +  quantization_losses  +  beta*decoder_losses).backward()'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '1#  next_observation:  一系列未来的像素观察 $o_{t+1},...,  o_{t+K}$ 2#  past_observation:  一系列历史的像素观察
    $o_{t-H+1},...,  o_{t}$ 3#  action:  一系列 $K$ 个动作 $a_t,...,  a_{t+K-1}$ 4#  obs_encoder:  观察编码器
    (CNN+Transformer) 5#  quantizer:  离散动作编码量化器 6#  dynamics:  潜在动态模型 7#  project:  投影层
    8#  predictor:  潜在状态预测器 9#  action_decoder:  量化动作编码解码器 10#  beta:  解码器损失的权重 (MetaWorld中为1.0，LIBERO中为0.01以保证数值稳定性)
    11 12 dynamic_losses,  quantization_losses,  decoder_losses  =  0,  0,  0 13 z  =  state_encoder(past_observation)
    14 z_hat  =  z 15 for  k  in  range(K): 16  z  =  state_encoder(past_observation)
    17  past_observation.push(next_observation[k]) 18  u_quantized,  quantization_loss  =  quantizer(action[k],  z.detach())
    19  quantization_losses  +=  quantization_loss 20  decode_action  =  decoder(z,  u_quantized)
    21  decoder_losses  +=  action_loss(decode_action,  action[k]) 22  z_hat  =  dynamics(z_hat,  u_quantized)
    23  z_next  =  state_encoder(observation[k+1]) 24  y_hat  =  predictor(project(z_hat))
    25  y_next  =  project(z_next).detach() 26  dynamic_losses  +=  -cosine_similarity(y_hat,  y_next)
    27 (dynamic_losses  +  quantization_losses  +  beta*decoder_losses).backward()'
- en: 'Figure 11: Pseudocode for the pretraining stage I of PRISE'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11: PRISE预训练阶段 I 的伪代码'
- en: 'Metaworld: We generate 100 expert trajectories for each pretraining task using
    the scripted policy provided in MetaWorld. The observation for each step includes
    an $84\times 84$ third-person view image as well as an 8-dimensional proprioceptive
    state vector of the robot’s end-effector. We use the same shallow CNN as in Yarats
    et al. ([2022](#bib.bib48)) to encode the observations into a 64-dimensional latent
    vector and apply a linear layer to embed the 8-dimensional state vector also into
    a 64-dimensional latent vector. The architecture of the shallow CNN is presented
    in [Figure 12](#A3.F12 "In Appendix C Implementation Details ‣ PRISE: Learning
    Temporal Action Abstractions as a Sequence Compression Problem").'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 'Metaworld：我们使用 MetaWorld 中提供的脚本策略为每个预训练任务生成了 100 个专家轨迹。每一步的观察包括一个 $84\times
    84$ 的第三人称视角图像以及一个 8 维的机器人末端执行器的自感知状态向量。我们使用与 Yarats et al. ([2022](#bib.bib48))
    相同的浅层 CNN 将观察编码为 64 维的潜在向量，并应用线性层将 8 维状态向量也嵌入到 64 维的潜在向量中。浅层 CNN 的架构如 [图 12](#A3.F12
    "在附录 C 实现细节 ‣ PRISE: 作为序列压缩问题的时间动作抽象学习") 所示。'
- en: 1class  Encoder(nn.Module):2  def  __init__(self):3  super().__init__()4  self.repr_dim  =  32  *  35  *  355  self.convnet  =  nn.Sequential(nn.Conv2d(84,  32,  3,  stride=2),6  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),7  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),8  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),9  nn.ReLU(),10  nn.Linear(self.repr_dim,  feature_dim),11  nn.LayerNorm(feature_dim),  nn.Tanh())12  self.trunk  =  nn.Sequential(nn.Linear(self.repr_dim,  feature_dim),13  nn.LayerNorm(feature_dim),  nn.Tanh())1415  def  forward(self,  obs):16  obs  =  obs  /  255.0  -  0.517  h  =  self.convnet(obs).view(h.shape[0],  -1)18  return  self.trunk(h)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 1class  Encoder(nn.Module):2  def  __init__(self):3  super().__init__()4  self.repr_dim  =  32  *  35  *  355  self.convnet  =  nn.Sequential(nn.Conv2d(84,  32,  3,  stride=2),6  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),7  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),8  nn.ReLU(),  nn.Conv2d(32,  32,  3,  stride=1),9  nn.ReLU(),10  nn.Linear(self.repr_dim,  feature_dim),11  nn.LayerNorm(feature_dim),  nn.Tanh())12  self.trunk  =  nn.Sequential(nn.Linear(self.repr_dim,  feature_dim),13  nn.LayerNorm(feature_dim),  nn.Tanh())1415  def  forward(self,  obs):16  obs  =  obs  /  255.0  -  0.517  h  =  self.convnet(obs).view(h.shape[0],  -1)18  return  self.trunk(h)
- en: 'Figure 12: Architecture of the Shallow CNN encoder used in MetaWorld.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：MetaWorld 中使用的浅层 CNN 编码器的架构。
- en: Next, we apply a transformer decoder module with 4 layers and number of heads
    equal to 8 to extract the observation embedding. We set the context length to
    be 10. The action decoder $\psi$ is a three-layer MLP with hidden size being 1024.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们应用一个具有 4 层和 8 个头的 transformer 解码器模块来提取观察嵌入。我们将上下文长度设置为 10。动作解码器 $\psi$
    是一个三层 MLP，隐藏层大小为 1024。
- en: 'LIBERO: For LIBERO, we pretrain PRISE on the provided LIBERO-90 dataset. The
    pretraining dataset contains 50 demonstration trajectories for each task, collected
    by human teleoperation. For each timestep, the agent observes a third-person view
    image, a first-person view image from its wrist camera (both with resolution $128\times
    128$ is also a three-layer MLP with hidden size being 1024.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: LIBERO：对于 LIBERO，我们在提供的 LIBERO-90 数据集上对 PRISE 进行了预训练。预训练数据集包含每个任务的 50 个演示轨迹，这些轨迹由人工远程操作收集。对于每个时间步，代理观察到一个第三人称视角的图像和一个来自其腕部摄像头的第一人称视角图像（两者的分辨率为
    $128\times 128$），以及一个隐藏层大小为 1024 的三层 MLP。
- en: Evaluation of Few-shot Imitation Learning A batch size of 128 and a learning
    rate of 1e-4 are used for Metaworld, and a batch size of 64 is used for LIBERO.
    In total, we take 30,000 gradient steps and conduct evaluations for every 3000
    steps. For both MetaWorld and LIBERO, we execute 40 episodes and calculate the
    success rate of the trained policy. We report the highest success rates across
    10 evaluated checkpoints.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本模仿学习评估：对于 Metaworld 使用了 128 的批量大小和 1e-4 的学习率，对于 LIBERO 使用了 64 的批量大小。总共进行了
    30,000 次梯度更新，并每 3000 次更新进行一次评估。对于 MetaWorld 和 LIBERO，我们执行了 40 次实验，并计算训练策略的成功率。我们报告了
    10 个评估检查点中最高的成功率。
- en: Computational Resources For our experiments, we use 8 NVIDIA RTX A6000 with
    PyTorch Distributed DataParallel for pretraining PRISE, and we use NVIDIA RTX2080Ti
    for downstream imitation learning on Metaworld, and RTX A5000 on LIBERO.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 计算资源：对于我们的实验，我们使用了 8 台 NVIDIA RTX A6000 和 PyTorch 分布式数据并行来预训练 PRISE，而在 Metaworld
    上进行下游模仿学习时使用了 NVIDIA RTX2080Ti，在 LIBERO 上使用了 RTX A5000。
- en: Appendix D Additional Results on Multitask Learning
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 多任务学习的附加结果
- en: Below we present the per-task success rate for LIBERO-90. The results of BC
    (ResNet-RNN), BC (ResNet-T), BC (ViT-T) are taken from (Liu et al., [2023](#bib.bib23)).
    For the evaluation of ACT and PRISE, we follow the same evaluation protocol. We
    take the last training checkpoint of them respectively and evaluate the success
    rate on each task with 20 rollouts.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下我们展示了 LIBERO-90 的每项任务成功率。BC (ResNet-RNN)、BC (ResNet-T)、BC (ViT-T) 的结果摘自 (Liu
    et al., [2023](#bib.bib23))。对于 ACT 和 PRISE 的评估，我们遵循相同的评估协议。我们分别取它们的最后一个训练检查点，并对每项任务进行
    20 次滚动评估成功率。
- en: '| LIBERO-90 | Multitask Algorithms |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| LIBERO-90 | 多任务算法 |'
- en: '| Task ID | BC (ResNet-RNN) | BC (ResNet-T) | BC (ViT-T) | ACT | PRISE |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 任务 ID | BC (ResNet-RNN) | BC (ResNet-T) | BC (ViT-T) | ACT | PRISE |'
- en: '| 0 | $0.45$ |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 0 | $0.45$ |'
- en: '| 1 | $0.1$ |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 1 | $0.1$ |'
- en: '| 2 | $0.0$ |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 2 | $0.0$ |'
- en: '| 3 | $0.15$ |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 3 | $0.15$ |'
- en: '| 4 | $0.2$ |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 4 | $0.2$ |'
- en: '| 5 | $0.25$ |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 5 | $0.25$ |'
- en: '| 6 | $0.05$ |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 6 | $0.05$ |'
- en: '| 7 | $0.3$ |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 7 | $0.3$ |'
- en: '| 8 | $0.05$ |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 8 | $0.05$ |'
- en: '| 9 | $0.35$ |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 9 | $0.35$ |'
- en: '| 10 | $0.2$ |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 10 | $0.2$ |'
- en: '| 11 | $0.45$ |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 11 | $0.45$ |'
- en: '| 12 | $0.2$ |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 12 | $0.2$ |'
- en: '| 13 | $0.1$ |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 13 | $0.1$ |'
- en: '| 14 | $0.35$ |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 14 | $0.35$ |'
- en: '| 15 | $0.5$ |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 15 | $0.5$ |'
- en: '| 16 | $0.25$ |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 16 | $0.25$ |'
- en: '| 17 | $0.05$ |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 17 | $0.05$ |'
- en: '| 18 | $0.4$ |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 18 | $0.4$ |'
- en: '| 19 | $0.15$ |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 19 | $0.15$ |'
- en: '| 20 | $0.6$ |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 20 | $0.6$ |'
- en: '| 21 | $0.15$ |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 21 | $0.15$ |'
- en: '| 22 | $0.75$ |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 22 | $0.75$ |'
- en: '| 23 | $0.2$ |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 23 | $0.2$ |'
- en: '| 24 | $0.25$ |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 24 | $0.25$ |'
- en: '| 25 | $0.8$ |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 25 | $0.8$ |'
- en: '| 26 | $0.0$ |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 26 | $0.0$ |'
- en: '| 27 | $0.05$ |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 27 | $0.05$ |'
- en: '| 28 | $0.05$ |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 28 | $0.05$ |'
- en: '| 29 | $0.2$ |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 29 | $0.2$ |'
- en: '| 30 | $0.05$ |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 30 | $0.05$ |'
- en: '| 31 | $0.5$ |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 31 | $0.5$ |'
- en: '| 32 | $0.0$ |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 32 | $0.0$ |'
- en: '| 33 | $0.1$ |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 33 | $0.1$ |'
- en: '| 34 | $0.2$ |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 34 | $0.2$ |'
- en: '| 35 | $0.7$ |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 35 | $0.7$ |'
- en: '| 36 | $0.05$ |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 36 | $0.05$ |'
- en: '| 37 | $0.05$ |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 37 | $0.05$ |'
- en: '| 38 | $0.15$ |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 38 | $0.15$ |'
- en: '| 39 | $0.2$ |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 39 | $0.2$ |'
- en: '| 40 | $0.45$ |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 40 | $0.45$ |'
- en: '| 41 | $0.25$ |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 41 | $0.25$ |'
- en: '| 42 | $0.15$ |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 42 | $0.15$ |'
- en: '| 43 | $0.1$ |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 43 | $0.1$ |'
- en: '| 44 | $0.5$ |  | LIBERO-90 | Multitask Algorithms |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 44 | $0.5$ |  | LIBERO-90 | 多任务算法 |'
- en: '| Task ID | BC (ResNet-RNN) | BC (ResNet-T) | BC (ViT-T) | ACT | PRISE |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 任务 ID | BC (ResNet-RNN) | BC (ResNet-T) | BC (ViT-T) | ACT | PRISE |'
- en: '| 45 | $0.0$ |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 45 | $0.0$ |'
- en: '| 46 | $0.0$ |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 46 | $0.0$ |'
- en: '| 47 | $0.0$ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 47 | $0.0$ |'
- en: '| 48 | $0.0$ |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 48 | $0.0$ |'
- en: '| 49 | $0.0$ |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 49 | $0.0$ |'
- en: '| 50 | $0.0$ |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 50 | $0.0$ |'
- en: '| 51 | $0.0$ |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 51 | $0.0$ |'
- en: '| 52 | $0.0$ |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 52 | $0.0$ |'
- en: '| 53 | $0.0$ |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 53 | $0.0$ |'
- en: '| 54 | $0.0$ |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 54 | $0.0$ |'
- en: '| 55 | $0.0$ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 55 | $0.0$ |'
- en: '| 56 | $0.0$ |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 56 | $0.0$ |'
- en: '| 57 | $0.0$ |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 57 | $0.0$ |'
- en: '| 58 | $0.0$ |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 58 | $0.0$ |'
- en: '| 59 | $0.0$ |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 59 | $0.0$ |'
- en: '| 60 | $0.0$ |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 60 | $0.0$ |'
- en: '| 61 | $0.0$ |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 61 | $0.0$ |'
- en: '| 62 | $0.0$ |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 62 | $0.0$ |'
- en: '| 63 | $0.0$ |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 63 | $0.0$ |'
- en: '| 64 | $0.0$ |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 64 | $0.0$ |'
- en: '| 65 | $0.0$ |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 65 | $0.0$ |'
- en: '| 66 | $0.15$ |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 66 | $0.15$ |'
- en: '| 67 | $0.1$ |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 67 | $0.1$ |'
- en: '| 68 | $0.35$ |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 68 | $0.35$ |'
- en: '| 69 | $0.1$ |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 69 | $0.1$ |'
- en: '| 70 | $0.05$ |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 70 | $0.05$ |'
- en: '| 71 | $0.0$ |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 71 | $0.0$ |'
- en: '| 72 | $0.0$ |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| 72 | $0.0$ |'
- en: '| 73 | $0.0$ |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 73 | $0.0$ |'
- en: '| 74 | $0.2$ |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| 74 | $0.2$ |'
- en: '| 75 | $0.0$ |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 75 | $0.0$ |'
- en: '| 76 | $0.0$ |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| 76 | $0.0$ |'
- en: '| 77 | $0.35$ |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 77 | $0.35$ |'
- en: '| 78 | $0.25$ |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 78 | $0.25$ |'
- en: '| 79 | $0.25$ |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| 79 | $0.25$ |'
- en: '| 80 | $0.0$ |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| 80 | $0.0$ |'
- en: '| 81 | $0.2$ |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| 81 | $0.2$ |'
- en: '| 82 | $0.4$ |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 82 | $0.4$ |'
- en: '| 83 | $0.0$ |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 83 | $0.0$ |'
- en: '| 84 | $0.0$ |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 84 | $0.0$ |'
- en: '| 85 | $0.0$ |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 85 | $0.0$ |'
- en: '| 86 | $0.05$ |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 86 | $0.05$ |'
- en: '| 87 | $0.2$ |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 87 | $0.2$ |'
- en: '| 88 | $0.0$ |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 88 | $0.0$ |'
- en: '| 89 | $0.15$ |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 89 | $0.15$ |'
- en: 'Table 4: Multitask success rate on LIBERO-90.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：LIBERO-90 上的多任务成功率。
- en: 'Additionally, we also provide the results of multitask performance on 45 MetaWorld
    pretraining tasks. As shown in [13](#A4.F13 "Figure 13 ‣ Appendix D Additional
    Results on Multitask Learning ‣ PRISE: Learning Temporal Action Abstractions as
    a Sequence Compression Problem"), all three algorithms perform well on MetaWorld,
    with an average success rate around 80% across 45 tasks. Thus in this paper, we
    focus on the more challenging LIBERO-90 multitask benchmark.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们还提供了 45 个 MetaWorld 预训练任务的多任务性能结果。如 [13](#A4.F13 "图 13 ‣ 附录 D 多任务学习附加结果
    ‣ PRISE: 作为序列压缩问题学习时间动作抽象") 所示，所有三种算法在 MetaWorld 上表现良好，45 个任务的平均成功率约为 80%。因此，在本文中，我们专注于更具挑战性的
    LIBERO-90 多任务基准。'
- en: '![Refer to caption](img/d8202003cf9e625fb5abd7921bf4d29f.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/d8202003cf9e625fb5abd7921bf4d29f.png)'
- en: 'Figure 13: Multitask Learning Results on MetaWorld.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：MetaWorld 上的多任务学习结果。
