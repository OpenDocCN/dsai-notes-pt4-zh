- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:50:01'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:50:01
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GPTVQ: The Blessing of Dimensionality for LLM Quantization'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GPTVQ：LLM量化的维度祝福
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15319](https://ar5iv.labs.arxiv.org/html/2402.15319)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2402.15319](https://ar5iv.labs.arxiv.org/html/2402.15319)
- en: Mart van Baalen    Andrey Kuzmin    Markus Nagel    Peter Couperus    Cedric
    Bastoul    Eric Mahurin    Tijmen Blankevoort    Paul Whatmough
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Mart van Baalen    Andrey Kuzmin    Markus Nagel    Peter Couperus    Cedric
    Bastoul    Eric Mahurin    Tijmen Blankevoort    Paul Whatmough
- en: Abstract
  id: totrans-7
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this work we show that the size versus accuracy trade-off of neural network
    quantization can be significantly improved by increasing the quantization dimensionality.
    We propose the GPTVQ method, a new fast method for post-training vector quantization
    (VQ) that scales well to Large Language Models (LLMs). Our method interleaves
    quantization of one or more columns with updates to the remaining unquantized
    weights, using information from the Hessian of the per-layer output reconstruction
    MSE. Quantization codebooks are initialized using an efficient data-aware version
    of the EM algorithm. The codebooks are then updated, and further compressed by
    using integer quantization and SVD-based compression. GPTVQ establishes a new
    state-of-the art in the size vs accuracy trade-offs on a wide range of LLMs such
    as Llama-v2 and Mistral. Furthermore, our method is efficient: on a single H100
    it takes between 3 and 11 hours to process a Llamav2-70B model, depending on quantization
    setting. Lastly, with on-device timings for VQ decompression on a mobile CPU we
    show that VQ leads to improved latency compared to using a 4-bit integer format.
    Our source code is available at [https://github.com/qualcomm-ai-research/gptvq](https://github.com/qualcomm-ai-research/gptvq).'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们展示了通过增加量化维度，神经网络量化的规模与准确度权衡可以显著改善。我们提出了GPTVQ方法，一种新的快速后训练向量量化（VQ）方法，能够很好地扩展到大型语言模型（LLMs）。我们的方法将一个或多个列的量化与对其余未量化权重的更新交替进行，使用来自每层输出重建均方误差的Hessian的信息。量化码本通过EM算法的数据感知版本进行初始化。然后，码本被更新，并通过使用整数量化和基于SVD的压缩进一步压缩。GPTVQ在许多LLMs（如Llama-v2和Mistral）上建立了规模与准确度权衡的新最优状态。此外，我们的方法高效：在一个H100上处理Llamav2-70B模型需要3到11小时，具体取决于量化设置。最后，通过在移动CPU上对VQ解压缩进行设备时间测量，我们显示出与使用4位整数格式相比，VQ能够改善延迟。我们的源代码可在[https://github.com/qualcomm-ai-research/gptvq](https://github.com/qualcomm-ai-research/gptvq)上获得。
- en: Machine Learning, LLMs, Quantization, Vector Quantization, Compression
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习、LLMs、量化、向量量化、压缩
- en: 1 Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Large language models (LLMs) have made significant strides in enabling human-like
    natural language interfaces for various applications, from general AI assistants
    like Open AI’s GPT (Achiam et al., [2023](#bib.bib1)) to specialized roles like
    coding companions (Roziere et al., [2023](#bib.bib30)) and medical aides (Tu et al.,
    [2024](#bib.bib37)).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）在实现类似人类的自然语言接口方面取得了重大进展，应用范围从像Open AI的GPT这样的通用AI助手（Achiam等，[2023](#bib.bib1)）到像编码伴侣（Roziere等，[2023](#bib.bib30)）和医疗助手（Tu等，[2024](#bib.bib37)）这样的专业角色。
- en: However, these advanced models come with high computational costs due to their
    extensive parameter counts, necessitating frequent data transfers during execution.
    The primary bottleneck in efficient LLM inference lies in weight movement, especially
    since LLMs’ autoregressive nature requires loading and transferring weights for
    each generated token. Consequently, the weight movement’s cost often surpasses
    the computational expenses.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些高级模型由于其庞大的参数数量，导致计算成本高，因此在执行过程中需要频繁的数据传输。高效LLM推理的主要瓶颈在于权重移动，尤其是由于LLMs的自回归特性，每生成一个令牌都需要加载和传输权重。因此，权重移动的成本往往超过了计算费用。
- en: 'To address the challenge of cost reduction for these resource-intensive models,
    a critical question arises: How can we compress LLM weights to the maximum extent
    possible? Low-bit quantization has proven successful in reducing model weights
    to 4 bits without substantial accuracy loss (Frantar et al., [2022](#bib.bib10);
    Shao et al., [2023](#bib.bib31); Lin et al., [2023](#bib.bib24)). While much of
    the prior research has focused on uniform quantization for LLMs, we investigate
    the potential to achieve even greater compression by employing non-uniform quantization
    and expanding the dimensionality of the representational grid through vector quantization.
    In vector quantization (see Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization"), top right), multiple weights
    are quantized together, offering a more versatile quantization grid across multiple
    dimensions.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决这些资源密集型模型的成本降低挑战，一个关键问题出现了：我们如何能最大限度地压缩LLM的权重？低位量化已被证明能够将模型权重减少到4位，而不会显著降低准确性 (Frantar
    et al., [2022](#bib.bib10); Shao et al., [2023](#bib.bib31); Lin et al., [2023](#bib.bib24))。虽然之前的研究大多集中在LLMs的均匀量化上，我们研究了通过采用非均匀量化和通过向量量化扩展表示网格的维度来实现更大压缩的潜力。在向量量化（见图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization"), 右上方）中，多个权重被一起量化，提供了在多个维度上的更灵活的量化网格。'
- en: '![Refer to caption](img/1353b61c3c31b788de371e3ab5c32d12.png)![Refer to caption](img/0b607aa1cbe60552b8756f964c84d45a.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/1353b61c3c31b788de371e3ab5c32d12.png)![参考标题](img/0b607aa1cbe60552b8756f964c84d45a.png)'
- en: 'Figure 1: Top: An example of how vector quantization can better represent 2D
    normally distributed data compared to uniform quantization, non-uniform quantization.
    Bottom: Comparing GPTVQ to state-of-the-art uniform quantization on Llama 70B.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：顶部：一个示例，展示了向量量化如何比均匀量化和非均匀量化更好地表示2D正态分布数据。底部：比较GPTVQ与Llama 70B上的最新均匀量化。
- en: We integrate our findings into a novel algorithm for post-training quantization
    called GPTVQ. This method allows fast non-uniform and vector quantization (VQ),
    improving the performance-size trade-off significantly compared to prior state-of-the-art.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将我们的发现整合到一种新的后训练量化算法中，称为GPTVQ。该方法允许快速的非均匀和向量量化（VQ），与之前的最新技术相比，显著提高了性能与大小之间的权衡。
- en: 'The contributions of this work are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作的贡献如下：
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Our analysis and experimental results show that increasing dimensionality of
    quantization gives improved accuracy versus model size trade-offs for many LLMs.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的分析和实验结果表明，提高量化的维度可以在许多大语言模型（LLMs）中改善准确性与模型大小之间的权衡。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose a fast and accurate algorithm for post-training VQ compression. We
    show that our algorithm achieves SOTA size vs accuracy trade-offs on a wide range
    of LLMs, while having a practical run time of only 3 to 11 hours for a 70B parameter
    model.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了一种快速而准确的后训练VQ压缩算法。我们展示了我们的算法在广泛的LLMs上实现了SOTA的大小与准确性权衡，同时对于一个70B参数的模型，实际运行时间仅为3到11小时。
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We implemented and benchmarked VQ decompression on a mobile CPU. While VQ leads
    to significant memory footprint reductions, our on-device timings also demonstrate
    that it leads to improved latency compared to a 4-bit integer baseline.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们在移动CPU上实现并基准测试了VQ解压缩。虽然VQ显著减少了内存占用，但我们的设备上测试时间也表明，与4位整数基线相比，它提高了延迟。
- en: 2 Motivation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 动机
- en: Neural network quantization is commonly used to reduce model footprint, data
    transfer and compute requirements. By quantizing a model, high bit-width floating
    point weights and activations that are commonly used for training can be represented
    by lower-precision values represented by fewer bits. Quantizing to 8 bits or lower
    significantly reduces footprint, data transfer and compute bottlenecks, at the
    cost of introducing *quantization noise* in the model, resulting in a potential
    drop in accuracy. In this section we provide a brief overview of uniform scalar
    quantization, non-uniform scalar quantization and introduce vector quantization,
    each of which offers progressively more flexibility in quantization. We will then
    illustrate how these methods improve representational accuracy of (non-uniform)
    underlying distributions, and can yield improved trade-offs between compression
    and accuracy. Finally, we touch upon the challenges of vector quantization and
    the limitations of current approaches.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络量化通常用于减少模型占用空间、数据传输和计算需求。通过量化模型，常用于训练的高位宽浮点权重和激活可以用较低精度的值表示，表示为较少的位。量化到8位或更低可以显著减少占用空间、数据传输和计算瓶颈，但代价是引入了*量化噪声*，可能导致准确性的下降。本节简要概述均匀标量量化、非均匀标量量化，并介绍向量量化，每种方法提供了逐渐增加的量化灵活性。然后，我们将说明这些方法如何提高（非均匀）底层分布的表示准确性，并在压缩和准确性之间取得更好的折中。最后，我们简要提及向量量化的挑战和当前方法的局限性。
- en: 2.1 Types of quantization grid and their flexibility
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 量化网格的类型及其灵活性
- en: Uniform quantization
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 均匀量化
- en: A symmetric uniform quantizer approximates an original floating point vector
    $\textbf{x}\in\mathbb{R}^{D}$ is a higher precision quantization scale, shared
    across the components of x.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对称均匀量化器将原始浮点向量$\textbf{x}\in\mathbb{R}^{D}$ 近似为更高精度的量化尺度，这个尺度在x的所有分量中共享。
- en: Non-uniform quantization
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 非均匀量化
- en: 'Uniform quantization as presented in the previous section, while efficient,
    is very inflexible as the representable points can be solely equidistantly spaced.
    A more flexible quantization approach is non-uniform quantization using codebook
    quantization, in which floating point numbers are discretized to arbitrary scalar
    centroids in a codebook $C:C=\{c_{1},c_{2},\dots,c_{k}\}$ is less than the original
    bitwidth of the elements in x. Note that the codebook itself incurs overhead,
    which we will discuss in more detail in Sections [2.2](#S2.SS2 "2.2 Challenges
    of vector quantization ‣ 2 Motivation ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization") and [3.2](#S3.SS2 "3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization").'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '如前节所述，均匀量化虽然高效，但由于可表示点只能均匀间隔，极其不灵活。一种更灵活的量化方法是使用代码本量化的非均匀量化，其中浮点数被离散化为代码本$C:C=\{c_{1},c_{2},\dots,c_{k}\}$中的任意标量质心，且代码本的位宽小于x中元素的原始位宽。请注意，代码本本身会产生开销，我们将在[2.2](#S2.SS2
    "2.2 向量量化的挑战 ‣ 2 动机 ‣ GPTVQ: 维度的祝福")和[3.2](#S3.SS2 "3.2 GPTVQ方法 ‣ 3 GPTVQ ‣ GPTVQ:
    维度的祝福")部分详细讨论。'
- en: Vector quantization
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量量化
- en: In non-uniform quantization, as introduced in the previous paragraph, we assume
    that each scalar value in x is quantized individually. However, a more flexible
    quantizer can be constructed by choosing a higher-dimensionality for the centroids
    in codebook $C$ is also frequently referred to as product quantization (Stock
    et al., [2019](#bib.bib33)) .
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在非均匀量化中，如前段所述，我们假设x中的每个标量值是单独量化的。然而，可以通过选择代码本$C$中质心的高维度来构建更灵活的量化器，这种方法也常被称为产品量化（Stock
    et al., [2019](#bib.bib33)）。
- en: '| ![Refer to caption](img/883893ac983b72064795839df1c8484e.png) |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![参见标题](img/883893ac983b72064795839df1c8484e.png) |'
- en: 'Figure 2: Quantization SQNR depending on the dimensionality for Llama-v2 7B
    weights. Signal-to-noise ratio increases with quantization dimensionality due
    to additional flexibility in the quantization grid.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：根据维度变化的Llama-v2 7B权重的量化SQNR。由于量化网格的附加灵活性，信噪比随量化维度的增加而增加。
- en: Accuracy improvement within higher dimensionality
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高维度下的准确性改进
- en: 'It is a well known fact that non-uniformly distributed data can be more accurately
    represented by a non-uniform quantizer. When increasing the dimensionality of
    the codebook, i.e. through VQ, the flexibility of the grid increases. A visual
    representation of this is given in figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization"). In this example,
    where we quantize each value in the original to a 3-bits representation (i.e.,
    6 bits for VQ with $d=2$, but the distribution of the centroids can more closely
    match the underlying distribution, increasing the accuracy of the representation.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '众所周知，非均匀分布的数据可以通过非均匀量化器更准确地表示。当增加代码本的维度，即通过VQ时，网格的灵活性增加。图[1](#S1.F1 "Figure
    1 ‣ 1 Introduction ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")给出了这一点的可视化表示。在这个例子中，我们将原始中的每个值量化为3比特表示（即，对于$d=2$的VQ，使用6比特），但质心的分布可以更接近底层分布，从而提高表示的准确性。'
- en: 'The accuracy of representation increases the more the dimensionality of the
    codebook increases. We can see the improvement in representational accuracy of
    higher $d$ in figure [2](#S2.F2 "Figure 2 ‣ Vector quantization ‣ 2.1 Types of
    quantization grid and their flexibility ‣ 2 Motivation ‣ GPTVQ: The Blessing of
    Dimensionality for LLM Quantization"). Here we plot the effect of compressing
    the weights of LLama-v2 7B with uniform quantization, non-uniform quantization,
    and vector quantization with 2 and 4 dimensions. On the y-axis we plot the signal-to-quantization
    noise ratio (SQNR) between the original and quantized weights, where higher is
    better. For fair comparison, we ensure the codebook overhead is always equal to
    0.25b per weight for each quantization method, i.e., improved SQNR is not caused
    trivially by using more bits for our representations. We can clearly see that
    as the dimensionality increase, the SQNR improves significantly as well.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '表示的准确性随着代码本维度的增加而提高。我们可以在图[2](#S2.F2 "Figure 2 ‣ Vector quantization ‣ 2.1
    Types of quantization grid and their flexibility ‣ 2 Motivation ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization")中看到更高$d$的表示准确性的提升。在这里，我们绘制了使用均匀量化、非均匀量化以及具有2和4维的向量量化来压缩LLama-v2
    7B权重的效果。在y轴上，我们绘制了原始权重和量化权重之间的量化噪声比(SQNR)，数值越高越好。为了公平比较，我们确保每种量化方法的代码本开销始终等于每个权重0.25b，即改进的SQNR并不是由于使用更多的比特来表示造成的。我们可以清楚地看到，随着维度的增加，SQNR也显著改善。'
- en: 2.2 Challenges of vector quantization
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 向量量化的挑战
- en: Codebook size
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码本大小
- en: The improvement in accuracy of the representation comes at a cost, as we now
    need to store and transmit the VQ codebook $C_{d}$ is the VQ-dimension. If we
    aim to use VQ for compressing weight tensors we have to consider this overhead
    in finding good trade-offs between accuracy and size of the weight tensors in
    a network.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 表示的准确性提高是有代价的，因为我们现在需要存储和传输VQ代码本$C_{d}$是VQ维度。如果我们希望使用VQ压缩权重张量，就必须考虑这种开销，以在网络中的权重张量的准确性和大小之间找到良好的折衷。
- en: In the rest of this work, we use *bits per dimension* ($b$.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本研究的其余部分，我们使用*每维比特数*($b$)。
- en: Centroids and assignment setting
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 质心和分配设置
- en: In order to apply vector quantization, one has to find a codebook of representative
    centroids, and an assignment to a centroid for each weight. While there are many
    methods to achieve this, a practical and popular approach is the k-Means algorithm
    (Han et al., [2015](#bib.bib15)). For neural network weights however, clustering
    on weights alone might not yield sufficient accuracy. To improve results, several
    authors (Stock et al., [2019](#bib.bib33); Martinez et al., [2021](#bib.bib25))
    include layer reconstruction error into their optimization, a technique that has
    been shown to improve results significantly in the model efficiency literature
    (He et al., [2017](#bib.bib17); Zhang et al., [2016](#bib.bib41); Nagel et al.,
    [2020](#bib.bib27)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应用向量量化，需要找到代表性质心的代码本，并为每个权重分配一个质心。虽然有很多方法可以实现这一点，但一种实际且流行的方法是k均值算法（Han 等人，[2015](#bib.bib15)）。然而，对于神经网络权重，仅对权重进行聚类可能无法达到足够的准确性。为了改善结果，一些作者（Stock
    等人，[2019](#bib.bib33)；Martinez 等人，[2021](#bib.bib25)）在优化中加入了层重建误差，这种技术在模型效率文献中已被证明能显著提高结果（He
    等人，[2017](#bib.bib17)；Zhang 等人，[2016](#bib.bib41)；Nagel 等人，[2020](#bib.bib27)）。
- en: 'Nevertheless, we find that neither k-Means alone, nor k-Means with layer input
    data included, is performant enough on Llamav2-7B (Touvron et al., [2023b](#bib.bib36)),
    as can be seen in Table [1](#S2.T1 "Table 1 ‣ Centroids and assignment setting
    ‣ 2.2 Challenges of vector quantization ‣ 2 Motivation ‣ GPTVQ: The Blessing of
    Dimensionality for LLM Quantization"). In this experiment we apply VQ to groups
    of weights, where each group of weights has its own codebook. We select the size
    of each weight group such that the overhead is the same for each setting. We see
    that, while results do improve when data is included, the increase in perplexity
    remains unacceptably large, especially for 2 and 3 bit VQ.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管如此，我们发现单独使用k-Means或使用包含层输入数据的k-Means在Llamav2-7B（Touvron等，[2023b](#bib.bib36)）上的表现都不够理想，如表[1](#S2.T1
    "Table 1 ‣ Centroids and assignment setting ‣ 2.2 Challenges of vector quantization
    ‣ 2 Motivation ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")所示。在这个实验中，我们将VQ应用于权重组，其中每组权重都有自己的码本。我们选择每个权重组的大小，使得每个设置的开销相同。我们看到，尽管包含数据时结果有所改善，但困惑度的增加仍然过大，尤其是在2位和3位VQ中。'
- en: 'Table 1: 2D VQ on Llamav2-7B using k-Means (without and with data included).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：在Llamav2-7B上使用k-Means进行2D VQ（包括和不包括数据）。
- en: '| Setting | With input data | Perplexity |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | 包含输入数据 | 困惑度 |'
- en: '| FP16 | n/a | 5.47 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| FP16 | 不适用 | 5.47 |'
- en: '| 2 bits per dim | No | 1.3e3 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 每维2位 | 否 | 1.3e3 |'
- en: '| Yes | 948 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 948 |'
- en: '| 3 bits per dim | No | 8.23 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 每维3位 | 否 | 8.23 |'
- en: '| Yes | 6.95 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 6.95 |'
- en: '| 4 bits per dim | No | 5.97 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 每维4位 | 否 | 5.97 |'
- en: '| Yes | 5.78 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 是 | 5.78 |'
- en: '| Uniform 3 bit | Yes | 6.03 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 均匀3位 | 是 | 6.03 |'
- en: '| Uniform 4 bit | Yes | 5.74 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 均匀4位 | 是 | 5.74 |'
- en: While including layer input data improves results, the authors of methods such
    as (Stock et al., [2019](#bib.bib33); Martinez et al., [2021](#bib.bib25)) note
    that this alone does not yield satisfactory performance, and include an end-to-end
    fine-tuning step into their algorithms. Unfortunately, the size of modern LLMs
    make end-to-end fine-tuning prohibitively expensive for many practitioners. As
    we aim to have a fast and scalable method for post-training quantization, we set
    out to find a method that is accurate and takes the activations into account when
    quantizing, and is efficient and scalable to apply to significantly-sized large
    language models.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然包括层输入数据可以改善结果，但方法的作者（如Stock等，[2019](#bib.bib33); Martinez等，[2021](#bib.bib25)）指出，仅此并不能产生令人满意的性能，因此将端到端微调步骤纳入他们的算法中。不幸的是，现代LLM的规模使得端到端微调对许多从业者而言成本过高。由于我们旨在拥有一种快速且可扩展的后训练量化方法，我们致力于寻找一种准确的并且在量化时考虑激活的，同时在应用于大型语言模型时高效且可扩展的方法。
- en: 3 GPTVQ
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 GPTVQ
- en: In this section we introduce a novel method for vector-quantizing LLMs efficiently
    and accurately. As mentioned in the previous section, existing methods targeting
    VQ do not scale to LLM-sized models. Instead, we build on a recent uniform quantization
    method named GPTQ (Frantar et al., [2022](#bib.bib10)), which interleaves column-wise
    quantization with updates to the remaining (unquantized) weights, using information
    from the Hessian of the layer output reconstruction MSE. This method has been
    shown to give excellent performance on uniformly quantizing LLMs with up to hundreds
    of billions of parameters.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们介绍了一种新颖的向量量化LLM的方法，旨在高效且准确地实现。正如前一部分提到的，现有的针对VQ的方法无法扩展到LLM规模的模型。相反，我们基于一种名为GPTQ（Frantar等，[2022](#bib.bib10)）的最新均匀量化方法，该方法将列级量化与对剩余（未量化）权重的更新交替进行，利用层输出重建MSE的Hessian信息。该方法已被证明在对具有数百亿参数的LLM进行均匀量化时表现出色。
- en: We first present a brief description of GPTQ. Then, we present our GPTVQ method,
    which extends GPTQ to VQ and integrates ideas from (Stock et al., [2019](#bib.bib33))
    for accurate initialization. Finally, we present a number of novel tricks to improve
    the size vs. accuracy trade-offs of the resulting quantized models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先简要介绍GPTQ。然后，我们介绍了我们的GPTVQ方法，该方法将GPTQ扩展到VQ，并整合了（Stock等，[2019](#bib.bib33)）的准确初始化思想。最后，我们展示了一些新颖的技巧，以改善量化模型的尺寸与准确性之间的权衡。
- en: '3.1 Background: GPTQ'
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 背景：GPTQ
- en: 'As described in Section [2.1](#S2.SS1 "2.1 Types of quantization grid and their
    flexibility ‣ 2 Motivation ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization"),
    quantization introduces quantization noise. A large body of literature exists
    with methods to alleviate the effects of quantization noise on model accuracy,
    see (Nagel et al., [2021](#bib.bib28); Gholami et al., [2022](#bib.bib13)) for
    recent surveys. Post-training quantization (PTQ) approaches aim to mitigate the
    adverse effects of quantization noise on pre-trained networks, without having
    to resort to costly quantization-aware training (QAT). A popular and effective
    approach in PTQ, introduced by AdaRound (Nagel et al., [2020](#bib.bib27)), is
    to modify weights to minimize a layer’s output error as an approximation to the
    full network’s loss:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '如第 [2.1](#S2.SS1 "2.1 Types of quantization grid and their flexibility ‣ 2
    Motivation ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") 节所述，量化引入了量化噪声。文献中存在大量缓解量化噪声对模型准确性影响的方法，最近的综述见
    (Nagel et al., [2021](#bib.bib28); Gholami et al., [2022](#bib.bib13))。后训练量化（PTQ）方法旨在减轻量化噪声对预训练网络的不利影响，而无需依赖成本高昂的量化感知训练（QAT）。在
    PTQ 中，一个流行且有效的方法是 AdaRound (Nagel et al., [2020](#bib.bib27)) 提出的，通过修改权重来最小化层的输出误差，作为对整个网络损失的近似：'
- en: '|  | $1$2 |  | (1) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (1) |'
- en: where $\mathbf{W}^{\ell}$ along its columns.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}^{\ell}$ 沿其列进行处理。
- en: 'GPTQ follows Optimal Brain Quantization (OBQ; Frantar & Alistarh ([2022](#bib.bib9))),
    which uses the Hessian of Equation [1](#S3.E1 "Equation 1 ‣ 3.1 Background: GPTQ
    ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization"). This
    Hessian can be efficiently computed as $\textbf{H}^{(\ell)}=\textbf{X}^{(\ell)}\textbf{X}^{(\ell)T}$:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'GPTQ 遵循最优大脑量化（OBQ；Frantar & Alistarh ([2022](#bib.bib9)))，该方法使用方程 [1](#S3.E1
    "Equation 1 ‣ 3.1 Background: GPTQ ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization") 的 Hessian 矩阵。这个 Hessian 矩阵可以有效地计算为 $\textbf{H}^{(\ell)}=\textbf{X}^{(\ell)}\textbf{X}^{(\ell)T}$：'
- en: '|  | $\displaystyle E=\sum_{q}&#124;E_{q}&#124;_{2}^{2};$ |  | (2) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle E=\sum_{q}&#124;E_{q}&#124;_{2}^{2};$ |  | (2) |'
- en: 'GPTQ extends OBQ in the following ways. First, GPTQ exploits the fact that
    $\textbf{H}^{(\ell)}$ on the layer’s output:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GPTQ 通过以下方式扩展了 OBQ。首先，GPTQ 利用 $\textbf{H}^{(\ell)}$ 在层输出上的事实：
- en: '|  | $\mathbf{\delta}=-\frac{\mathbf{W}_{:,q}-\text{quant(}\mathbf{W}_{:,q})}{\left[\textbf{H}^{-1}\right]_{qq}}\textbf{H}_{:,(q+1):}$
    |  | (3) |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{\delta}=-\frac{\mathbf{W}_{:,q}-\text{quant(}\mathbf{W}_{:,q})}{\left[\textbf{H}^{-1}\right]_{qq}}\textbf{H}_{:,(q+1):}$
    |  | (3) |'
- en: 'To reduce data transfer, GPTQ applies the update of Equation [3](#S3.E3 "Equation
    3 ‣ 3.1 Background: GPTQ ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for
    LLM Quantization") only to a small block of $B$, which introduces a more numerically
    stable alternative to the inverse Hessian row and column removal operations of
    OBQ.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '为了减少数据传输，GPTQ 将方程 [3](#S3.E3 "Equation 3 ‣ 3.1 Background: GPTQ ‣ 3 GPTVQ ‣
    GPTVQ: The Blessing of Dimensionality for LLM Quantization") 的更新仅应用于一个小块 $B$，这引入了一种比
    OBQ 的逆 Hessian 行和列移除操作更为数值稳定的替代方法。'
- en: 3.2 The GPTVQ method
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 GPTVQ 方法
- en: 'Algorithm 1 GPTVQ: Quantize $\mathbf{W}\in\mathbb{R}^{r\times c}$'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 GPTVQ：量化 $\mathbf{W}\in\mathbb{R}^{r\times c}$
- en: 1:  $N_{b}\leftarrow\frac{c}{B}$20:  end for
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  $N_{b}\leftarrow\frac{c}{B}$20:  end for'
- en: The GPTVQ method generalizes the GPTQ method for non-uniform and vector quantization.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: GPTVQ 方法将 GPTQ 方法推广到非均匀和向量量化。
- en: 'Following the GPTQ framework we perform quantization of the weight tensor in
    a greedy manner starting from the first column. The details of the method are
    given in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization"). Given the VQ dimensionality
    $d$:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '按照 GPTQ 框架，我们以贪婪的方式从第一列开始进行权重张量的量化。该方法的详细信息见算法 [1](#alg1 "Algorithm 1 ‣ 3.2
    The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")。给定
    VQ 维度 $d$：'
- en: '|  | $j=\arg\min_{m}\left(\textbf{x}-\textbf{c}^{(m)}\right)^{T}\textbf{H}^{(i)}\left(\textbf{x}-\textbf{c}^{(m)}\right).$
    |  | (4) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $j=\arg\min_{m}\left(\textbf{x}-\textbf{c}^{(m)}\right)^{T}\textbf{H}^{(i)}\left(\textbf{x}-\textbf{c}^{(m)}\right).$
    |  | (4) |'
- en: After performing quantization of $d$ coordinates and apply it on the remaining
    weights as a single operation.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 $d$ 个坐标进行量化后，将其作为单一操作应用于剩余的权重。
- en: 'To minimize the quantization error, we use several codebooks per layer. Each
    codebook is assigned to a group of weights (see Algorithm [1](#alg1 "Algorithm
    1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for
    LLM Quantization")).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '为了最小化量化误差，我们在每层使用多个代码本。每个代码本分配给一组权重（见算法 [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ
    method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")）。'
- en: Codebook initialization
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码本初始化
- en: 'To initialize the codebook for a group of weights, we suggest the following
    variant of the EM algorithm. Given the set of $d$. The objective is the following
    sum of Hessian-weighted distance functions:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了初始化一组权重的代码本，我们建议使用 EM 算法的以下变体。给定集合 $d$。目标是以下的 Hessian 加权距离函数的总和：
- en: '|  | $1$2 |  | (5) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (5) |'
- en: where $\mathbf{H}^{(i)}$ equal to identity, the clustering method is equivalent
    to K-means. The objective can be minimized using E- and M-steps as follows.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当 $\mathbf{H}^{(i)}$ 等于单位矩阵时，聚类方法等价于 K-均值。目标可以通过 E 步骤和 M 步骤最小化，如下所示。
- en: 'E-step: find the assignment $j$ that minimizes the objective [4](#S3.E4 "Equation
    4 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for
    LLM Quantization"). Using this distance function assigns optimal centroids based
    on the data-aware loss.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 'E 步骤：找到使目标 [4](#S3.E4 "Equation 4 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization") 最小化的分配 $j$。使用这个距离函数根据数据感知损失分配最优质心。'
- en: 'M-step: find the centroid value $\textbf{c}^{(m)}$ that minimizes'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: M步：找到使得 $\textbf{c}^{(m)}$ 最小化的质心值
- en: '|  | $1$2 |  | (6) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $1$2 |  | (6) |'
- en: This objective is a quadratic form w.r.t $\mathbf{c}^{(m)}$-dim inverse sub-Hessian.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 该目标是关于 $\mathbf{c}^{(m)}$-维逆 Hessian 的二次形式。
- en: Blockwise data normalization
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块级数据归一化
- en: In order to lower the error of vector quantization, we apply blockwise data
    normalization to the data before the codebook initialization. For each group corresponding
    to a new codebook we perform element-wise division $\mathbf{W}_{i}\oslash\mathbf{S}_{i}$,
    e.g. for a block size of 16, 32, or 64.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低向量量化的误差，我们在代码本初始化之前对数据应用块级数据归一化。对于每组对应于新代码本的块，我们执行逐元素除法 $\mathbf{W}_{i}\oslash\mathbf{S}_{i}$，例如，对于块大小为16、32或64。
- en: Given a set of blocks (sub-rows) $\mathbf{w}^{(i)}$. The scaled data is used
    for codebook initialization. The inverse scaling is applied at VQ decoding step.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组块（子行） $\mathbf{w}^{(i)}$。缩放后的数据用于代码本初始化。在 VQ 解码步骤中应用逆缩放。
- en: Total bits per value
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每值总位数
- en: As a measure of total model size, we compute *bits per value*, given by $\log_{2}(k)+kdb_{c}/l+b_{s}/N_{s}$
    is an integer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 作为模型总大小的衡量，我们计算 *每值位数*，由 $\log_{2}(k)+kdb_{c}/l+b_{s}/N_{s}$ 给出，该值为整数。
- en: 3.3 Additional steps
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 附加步骤
- en: 'After the procedure in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method
    ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") is completed,
    we perform several steps to further improve model size vs perplexity trade-offs.
    Each of these steps is described below.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '在完成算法 [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The
    Blessing of Dimensionality for LLM Quantization") 中的过程后，我们执行几个步骤以进一步改善模型大小与困惑度的权衡。每一个步骤如下所述。'
- en: Codebook update
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码本更新
- en: 'We found that output reconstruction error can be further reduced through a
    *codebook update*. Recall that, in line 15 of Algorithm [1](#alg1 "Algorithm 1
    ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM
    Quantization"), Q is incrementally constructed from the elements of C. Since this
    construction constitutes a lookup of values in C, the layerwise objective can
    still be minimized w.r.t C. The objective is a quadratic program and is convex:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '我们发现通过 *代码本更新* 可以进一步减少输出重建误差。回顾算法 [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method
    ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") 的第15行，Q
    是从 C 的元素逐步构建的。由于这种构建构成了 C 中值的查找，因此逐层目标仍然可以相对于 C 进行最小化。目标是一个二次规划问题且是凸的：'
- en: '|  | $\min_{\textbf{C}_{0},\dots,\textbf{C}_{N}}&#124;&#124;\mathbf{W}\mathbf{X}-\mathbf{Q}\mathbf{X}&#124;&#124;_{F}^{2},$
    |  | (7) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\textbf{C}_{0},\dots,\textbf{C}_{N}}&#124;&#124;\mathbf{W}\mathbf{X}-\mathbf{Q}\mathbf{X}&#124;&#124;_{F}^{2},$
    |  | (7) |'
- en: where $\mathbf{Q}(\textbf{C}_{0},\dots,\textbf{C}_{N})$ only involves a look-up
    operation. In each GD step, the values in C are updated, and Q is reconstructed
    using the new values in C, keeping the assignments fixed.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{Q}(\textbf{C}_{0},\dots,\textbf{C}_{N})$ 仅涉及查找操作。在每次 GD 步骤中，更新 C
    中的值，并使用 C 中的新值重新构建 Q，同时保持分配不变。
- en: Codebook quantization
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码本量化
- en: In practical scenarios, codebooks need to be quantized to 8 bits. As a further
    post-processing step, we quantize the codebook for each group of weights to signed
    8-bit integers, using symmetric min-max quantization.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，代码本需要量化为8位。作为进一步的后处理步骤，我们将每组权重的代码本量化为有符号的8位整数，使用对称的最小最大量化方法。
- en: Further codebook compression
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 进一步的代码本压缩
- en: We achieve improved model size vs perplexity trade-offs by reducing the rank
    of the codebook tensor C. For a single tensor, C has shape $N_{G}\times k\times
    d$.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过降低代码本张量 C 的秩来实现改进的模型大小与困惑度权衡。对于单个张量，C 的形状为 $N_{G}\times k\times d$。
- en: 4 Experiments and results
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验和结果
- en: 'Table 2: Weight-only quantization results of Llama-v2, Mistral, and Mixtral-MoE
    Models. We report WikiText2 perplexity and average zero-shot accuracy; Models
    marked ‘L2’ denote Llama-v2, M denotes Mistral, and 8x7B denotes Mixtral-MoE 8x7B.
    Numbers marked in bold are SOTA or surpass it, numbers underlined are on par with
    or outperform at least one VQ variant.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：Llama-v2、Mistral 和 Mixtral-MoE 模型的权重量化结果。我们报告了 WikiText2 困惑度和平均零-shot 准确率；标记为‘L2’的模型代表
    Llama-v2，M 代表 Mistral，8x7B 代表 Mixtral-MoE 8x7B。加粗的数字是 SOTA 或超越 SOTA，下划线的数字与至少一个
    VQ 变体相当或超越。
- en: '|  |  | WikiText2 perplexity $\downarrow$ |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |  | WikiText2 困惑度 $\downarrow$ |  |'
- en: '|  |  | L2-7B | L2-13B | L2-70B | M-7B | 8x7B | L2-7B | L2-13B | M-7B | 8x7B
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  |  | L2-7B | L2-13B | L2-70B | M-7B | 8x7B | L2-7B | L2-13B | M-7B | 8x7B
    |'
- en: '| FP16 |  | 5.47 | 4.88 | 3.31 | 5.25 | 3.84 | 70.47 | 73.22 | 75.69 | 75.93
    |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| FP16 |  | 5.47 | 4.88 | 3.31 | 5.25 | 3.84 | 70.47 | 73.22 | 75.69 | 75.93
    |'
- en: '| 2.125 bpv (W2@g128) | RTN | 4.2e3 | 122.08 | 27.27 | 1.4e3 | 4.3e3 | 36.94
    | 42.06 | 37.75 | 38.29 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 2.125 bpv (W2@g128) | RTN | 4.2e3 | 122.08 | 27.27 | 1.4e3 | 4.3e3 | 36.94
    | 42.06 | 37.75 | 38.29 |'
- en: '| GPTQ | 36.77 | 28.14 | 6.74 | 15.68 | 14.17 | 41.44 | 46.56 | 41.93 | 44.54
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 36.77 | 28.14 | 6.74 | 15.68 | 14.17 | 41.44 | 46.56 | 41.93 | 44.54
    |'
- en: '| AWQ | 2.2e5 | 1.2e5 | - | - | - | - | - | - | - |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 2.2e5 | 1.2e5 | - | - | - | - | - | - | - |'
- en: '| OmniQuant | 11.06 | 8.26 | 6.55 | - | - | - | - | - | - |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 11.06 | 8.26 | 6.55 | - | - | - | - | - | - |'
- en: '| GPTVQ 1D (ours) | 11.57 | 7.34 | 5.00 | 15.03 | 8.11 | 47.51 | 60.82 | 44.85
    | 57.54 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 1D (我们) | 11.57 | 7.34 | 5.00 | 15.03 | 8.11 | 47.51 | 60.82 | 44.85
    | 57.54 |'
- en: '| GPTVQ 2D (ours) | 8.23 | 6.50 | 4.64 | 10.28 | 6.37 | 57.24 | 64.46 | 57.25
    | 64.50 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 2D (我们) | 8.23 | 6.50 | 4.64 | 10.28 | 6.37 | 57.24 | 64.46 | 57.25
    | 64.50 |'
- en: '| 2.25 bpv (W2@g64) | RTN | 431.97 | 26.22 | 10.31 | 71.52 | 155.82 | 42.40
    | 46.41 | 44.79 | 46.86 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 2.25 bpv (W2@g64) | RTN | 431.97 | 26.22 | 10.31 | 71.52 | 155.82 | 42.40
    | 46.41 | 44.79 | 46.86 |'
- en: '| GPTQ | 20.85 | 22.44 | NAN | 14.24 | 10.07 | 47.51 | 54.16 | 51.76 | 48.78
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 20.85 | 22.44 | NAN | 14.24 | 10.07 | 47.51 | 54.16 | 51.76 | 48.78
    |'
- en: '| AWQ | 2.1e5 | 1.2e5 | - | - | - | - | - | - | - |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 2.1e5 | 1.2e5 | - | - | - | - | - | - | - |'
- en: '| OmniQuant | 9.62 | 7.56 | 6.11 | - | - | - | - | - | - |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 9.62 | 7.56 | 6.11 | - | - | - | - | - | - |'
- en: '| GPTVQ 1D (ours) | 10.08 | 7.17 | 4.82 | 9.56 | 8.06 | 51.95 | 61.48 | 55.82
    | 57.12 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 1D (我们) | 10.08 | 7.17 | 4.82 | 9.56 | 8.06 | 51.95 | 61.48 | 55.82
    | 57.12 |'
- en: '| GPTVQ 2D (ours) | 7.97 | 6.47 | 4.61 | 10.11 | 6.23 | 59.08 | 64.85 | 56.14
    | 63.92 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 2D (我们) | 7.97 | 6.47 | 4.61 | 10.11 | 6.23 | 59.08 | 64.85 | 56.14
    | 63.92 |'
- en: '|  | GPTVQ 4D (ours) | 7.22 | 6.08 | 4.39 | 7.16 | 5.55 | 61.49 | 66.17 | 64.44
    | 66.43 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTVQ 4D (我们) | 7.22 | 6.08 | 4.39 | 7.16 | 5.55 | 61.49 | 66.17 | 64.44
    | 66.43 |'
- en: '| 3.125 bpv (W3@g128) | RTN | 6.66 | 5.51 | 3.97 | 6.15 | 5.18 | 67.25 | 70.75
    | 71.79 | 72.40 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 3.125 bpv (W3@g128) | RTN | 6.66 | 5.51 | 3.97 | 6.15 | 5.18 | 67.25 | 70.75
    | 71.79 | 72.40 |'
- en: '| GPTQ | 6.29 | 5.42 | 3.85 | 5.83 | 4.71 | 66.16 | 71.44 | 72.24 | 72.73 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 6.29 | 5.42 | 3.85 | 5.83 | 4.71 | 66.16 | 71.44 | 72.24 | 72.73 |'
- en: '| AWQ | 6.24 | 5.32 | - | - | - | - | - | - | - |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.24 | 5.32 | - | - | - | - | - | - | - |'
- en: '| OmniQuant | 6.03 | 5.28 | 3.78 | - | - | - | - | - | - |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 6.03 | 5.28 | 3.78 | - | - | - | - | - | - |'
- en: '| GPTVQ 1D (ours) | 5.98 | 5.17 | 3.62 | 5.76 | 4.59 | 67.61 | 71.59 | 71.56
    | 72.75 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 1D (我们) | 5.98 | 5.17 | 3.62 | 5.76 | 4.59 | 67.61 | 71.59 | 71.56
    | 72.75 |'
- en: '| GPTVQ 2D (ours) | 5.82 | 5.10 | 3.55 | 5.51 | 4.30 | 67.88 | 71.76 | 73.56
    | 74.36 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 2D (我们) | 5.82 | 5.10 | 3.55 | 5.51 | 4.30 | 67.88 | 71.76 | 73.56
    | 74.36 |'
- en: In this section we evaluate GPTVQ and compare the performance of vector quantization
    in 1, 2 and 4 dimensions against uniform quantization baseline methods.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们评估了 GPTVQ 并比较了 1、2 和 4 维向量量化与均匀量化基线方法的性能。
- en: Models
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 模型
- en: We use the Llama-1 (Touvron et al., [2023a](#bib.bib35)), Llama-2 (Touvron et al.,
    [2023b](#bib.bib36)) as well as Mistral-7B-v0.1 (Jiang et al., [2023](#bib.bib20))
    and Mixtral-MoE-8x7B-v0.1 (Jiang et al., [2024](#bib.bib21)). Additionally, we
    run a single ablation on BLOOM-560M (Workshop et al., [2022](#bib.bib38)).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了 Llama-1 (Touvron et al., [2023a](#bib.bib35))、Llama-2 (Touvron et al.,
    [2023b](#bib.bib36)) 以及 Mistral-7B-v0.1 (Jiang et al., [2023](#bib.bib20)) 和 Mixtral-MoE-8x7B-v0.1
    (Jiang et al., [2024](#bib.bib21))。此外，我们还对 BLOOM-560M (Workshop et al., [2022](#bib.bib38))
    进行了单次消融实验。
- en: Datasets
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We follow Shao et al. ([2023](#bib.bib31)) and use the WikiText2 (Merity et al.,
    [2016](#bib.bib26)) training set as the calibration dataset for all our experiments.
    We evaluate our models on token perplexity for the WikiText2 validation set, as
    well as zero-shot language tasks: PIQA (Bisk et al., [2020](#bib.bib3)), ARC-easy
    and ARC-challenge (Clark et al., [2018](#bib.bib6)), BoolQ (Clark et al., [2019](#bib.bib5)),
    HellaSwag (Zellers et al., [2019](#bib.bib40)), and WinoGrande (Keisuke et al.,
    [2019](#bib.bib22)). For all datasets except WikiText2 we use the LLM-evaluation-harness
    (Gao et al., [2023](#bib.bib11)) to run evaluations.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遵循Shao等人（[2023](#bib.bib31)）的方法，使用WikiText2（Merity et al., [2016](#bib.bib26)）训练集作为所有实验的校准数据集。我们评估了WikiText2验证集上的模型令牌困惑度，以及零-shot语言任务：PIQA（Bisk
    et al., [2020](#bib.bib3)）、ARC-easy和ARC-challenge（Clark et al., [2018](#bib.bib6)）、BoolQ（Clark
    et al., [2019](#bib.bib5)）、HellaSwag（Zellers et al., [2019](#bib.bib40)）和WinoGrande（Keisuke
    et al., [2019](#bib.bib22)）。对于WikiText2以外的所有数据集，我们使用LLM-evaluation-harness（Gao
    et al., [2023](#bib.bib11)）来进行评估。
- en: Baselines
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基线
- en: 'We compare GPTVQ against various uniform quantization methods with different
    group sizes and ensure that all have the same bits-per-value (bpv) overhead. We
    include Round-to-Nearest (RTN) and several recent state-of-the-art PTQ approaches
    targeting LLMs: GPTQ (Frantar et al., [2022](#bib.bib10)), AWQ (Lin et al., [2023](#bib.bib24)),
    and OmniQuant (Shao et al., [2023](#bib.bib31)).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将GPTVQ与不同组大小的各种均匀量化方法进行比较，并确保所有方法的每值比特数（bpv）开销相同。我们包括了最近的最先进PTQ方法，针对LLMs的Round-to-Nearest（RTN）、GPTQ（Frantar
    et al., [2022](#bib.bib10)）、AWQ（Lin et al., [2023](#bib.bib24)）以及OmniQuant（Shao
    et al., [2023](#bib.bib31)）。
- en: Main results
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 主要结果
- en: 'Table [2](#S4.T2 "Table 2 ‣ 4 Experiments and results ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") contains the main results for GPTVQ.
    In this table, we report WikiText 2 perplexity and an average over zero-shot task
    scores for the PIQA, BoolQ, ARC-easy, ARC-challenge, HellaSwag and WinoGrande
    tasks. In this table we report results for all Llama-v2 models, Mistral-7B-v0.1
    and Mixtral-8x7B-v0.1. More detailed results are included in appendix [A](#A1
    "Appendix A Extended results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization");
    Table [5](#A1.T5 "Table 5 ‣ Appendix A Extended results ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") contains individual scores for the zero-shot
    tasks and Table [4](#A1.T4 "Table 4 ‣ Appendix A Extended results ‣ GPTVQ: The
    Blessing of Dimensionality for LLM Quantization") contains WikiText2 perplexity
    for all Llama-v1 models, as well as further experiments with 4 bit quantization.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S4.T2 "Table 2 ‣ 4 Experiments and results ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization")包含GPTVQ的主要结果。在这个表格中，我们报告了WikiText 2的困惑度以及PIQA、BoolQ、ARC-easy、ARC-challenge、HellaSwag和WinoGrande任务的零-shot任务分数的平均值。这个表格中报告了所有Llama-v2模型、Mistral-7B-v0.1和Mixtral-8x7B-v0.1的结果。更详细的结果见附录[A](#A1
    "Appendix A Extended results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")；表[5](#A1.T5
    "Table 5 ‣ Appendix A Extended results ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization")包含零-shot任务的单独分数，表[4](#A1.T4 "Table 4 ‣ Appendix A Extended
    results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")包含所有Llama-v1模型的WikiText2困惑度，以及进一步的4位量化实验。'
- en: In these tables, we can see that non-uniform quantization using GPTVQ generally
    yields improved results over uniform PTQ methods. This gap becomes especially
    large at low bitwidths and for very large models. Compare e.g., GPTVQ 2D on Llamav2-70B
    to OmniQuant W2@g128, where an improvement of nearly 2 perplexity points is achieved.
    Furthermore, in nearly all cases, 2D VQ outperforms 1D VQ, and even more significant
    improvements are achieved with 4D VQ.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些表格中，我们可以看到，使用GPTVQ的非均匀量化通常比均匀PTQ方法提供了更好的结果。这个差距在低比特宽度和非常大的模型中尤为明显。例如，将GPTVQ
    2D与OmniQuant W2@g128进行比较，可以获得近2个困惑度点的改进。此外，几乎所有情况下，2D VQ的表现优于1D VQ，而4D VQ则取得了更显著的改进。
- en: 4.1 GPTVQ hyperparameters
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 GPTVQ超参数
- en: 'In all our experiments we use the WikiText training set as calibration data
    for our method. Following (Frantar et al., [2022](#bib.bib10)) we sample 128 sequences
    of 2048 tokens each. Our method has several hyperparameters: the EM initialization
    method; the number of EM iterations; the number of weights in a block of weights
    sharing the same codebook; the number of columns in each block. Furthermore, we
    can lower codebook overhead through different routes: increasing the block size;
    quantizing the codebooks; or performing SVD on the codebooks. In our main results
    we use the following hyperparameter settings: We seed EM initialization with centroids
    found by our ‘Mahalanobis’ method (see Section [4.3](#S4.SS3.SSS0.Px1 "EM initialization
    ‣ 4.3 Ablations on hyperparameter choices ‣ 4 Experiments and results ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization")), and run EM for 100 iterations
    to initialize codebook centroids. Each weight group spans (at most) 256 columns,
    e.g., a group of 1024 weights is 4 rows $\times$ 256 columns. After the procedure
    in Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization") is run, we update the codebook
    as described in [3.3](#S3.SS3 "3.3 Additional steps ‣ 3 GPTVQ ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") for 25 iterations, and by default use
    8 bit uniform quantization to represent codebook values. In Section [4.3](#S4.SS3
    "4.3 Ablations on hyperparameter choices ‣ 4 Experiments and results ‣ GPTVQ:
    The Blessing of Dimensionality for LLM Quantization") we perform an ablation on
    the choice of each of these hyperparameters. We note that applying the blockwise
    data normalization as introduced in Section [3.2](#S3.SS2 "3.2 The GPTVQ method
    ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") mostly
    improves the final performance. However, for some cases, specifically 1D VQ with
    2 bits per index, it hurts the performance and in such cases we did not apply
    it.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '在我们所有的实验中，我们使用 WikiText 训练集作为我们方法的校准数据。根据（Frantar 等，[2022](#bib.bib10)）的方法，我们抽取了
    128 个序列，每个序列包含 2048 个标记。我们的方法有几个超参数：EM 初始化方法；EM 迭代次数；在共享同一代码本的权重块中的权重数量；每个块中的列数。此外，我们可以通过不同的途径降低代码本开销：增加块大小；对代码本进行量化；或对代码本执行
    SVD。在我们的主要结果中，我们使用以下超参数设置：我们用‘Mahalanobis’方法找到的质心作为 EM 初始化的种子（见第 [4.3](#S4.SS3.SSS0.Px1
    "EM initialization ‣ 4.3 Ablations on hyperparameter choices ‣ 4 Experiments and
    results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") 节），并运行
    EM 进行 100 次迭代以初始化代码本质心。每个权重组跨度（最多）256 列，例如，一组 1024 个权重为 4 行 $\times$ 256 列。在运行算法
    [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") 后，我们按照 [3.3](#S3.SS3 "3.3 Additional
    steps ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")
    中描述的方法更新代码本 25 次迭代，默认情况下使用 8 位均匀量化来表示代码本值。在第 [4.3](#S4.SS3 "4.3 Ablations on hyperparameter
    choices ‣ 4 Experiments and results ‣ GPTVQ: The Blessing of Dimensionality for
    LLM Quantization") 节中，我们对每个超参数的选择进行了消融研究。我们注意到，应用第 [3.2](#S3.SS2 "3.2 The GPTVQ
    method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")
    节中介绍的块级数据归一化方法大多会提升最终性能。然而，对于某些情况，特别是每个索引 2 位的 1D VQ，它会降低性能，在这种情况下我们没有应用它。'
- en: Codebook overhead
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码本开销
- en: 'As described in Section [2.2](#S2.SS2.SSS0.Px1 "Codebook size ‣ 2.2 Challenges
    of vector quantization ‣ 2 Motivation ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization"), VQ codebooks introduce non-negligible overhead. A point
    rarely addressed is that the quantization scale of uniform quantization also needs
    to be stored and transmitted, and incurs an overhead. The overhead of this scale,
    while negligible for per-channel or per-tensor quantization, becomes significant
    for quantization to smaller block sizes, as is often applied in low-bitwidth quantization
    for LLMs (Rouhani et al., [2023](#bib.bib29); Frantar et al., [2022](#bib.bib10);
    Lin et al., [2023](#bib.bib24); Shao et al., [2023](#bib.bib31)). For groups of
    128 weights for example, a 16 bit scale introduces an overhead of $16/128=0.125$
    bits, meaning that each group needs to contain 2048 weights for the codebook overhead
    to meet the 2.125 bits per value target.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[2.2节](#S2.SS2.SSS0.Px1 "码本大小 ‣ 2.2 向量量化的挑战 ‣ 2 动机 ‣ GPTVQ: 维度的福音对于 LLM 量化")所述，VQ码本引入了不可忽视的开销。一个很少提到的点是，均匀量化的量化尺度也需要存储和传输，并且产生开销。这种尺度的开销，尽管对于每通道或每张量量化来说微不足道，但在量化为较小块大小时变得显著，这在LLM的低位宽量化中经常应用(Rouhani
    et al., [2023](#bib.bib29); Frantar et al., [2022](#bib.bib10); Lin et al., [2023](#bib.bib24);
    Shao et al., [2023](#bib.bib31))。例如，对于128个权重的组，16位尺度引入了$16/128=0.125$位的开销，这意味着每组需要包含2048个权重，才能使码本开销满足每值2.125位的目标。'
- en: To compare to the baseline results presented in (Shao et al., [2023](#bib.bib31)),
    we choose a combination of group size and codebook bitwidth that corresponds to
    an overhead of 0.125 or 0.25 bits per value. These settings correspond to uniform
    quantization with group sizes of 128 or 64 weights, respectively, as used in (Shao
    et al., [2023](#bib.bib31)).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与(Shao et al., [2023](#bib.bib31))中呈现的基线结果进行比较，我们选择了一组合并大小和码本位宽，这对应于每值0.125或0.25位的开销。这些设置对应于具有128或64权重组的均匀量化，如(Shao
    et al., [2023](#bib.bib31))中所用。
- en: 4.2 Data transfer speed comparision
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 数据传输速度比较
- en: To illustrate the effect of VQ on data transfer latency, we developed an optimized
    kernel for Arm® CPUs to efficiently decode VQ-compressed weights. Our implementation
    uses variants of the Arm® TBL instruction. The TBL instruction can be used to
    look up values in a lookup table (LUT), to translate an index of (at most) 5 bits
    to an 8 bit integer value. VQ in dimensions higher than 1 can be implemented by
    using multiple LUTs and corresponding TBL instructions. For example, 2D VQ with
    2 bits per index translates to 2 LUTs, one for each VQ dim, each mapping a 4-bit
    index to an 8 bit value.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明VQ对数据传输延迟的影响，我们为Arm® CPU开发了一个优化内核，以高效解码VQ压缩的权重。我们的实现使用了Arm® TBL指令的变体。TBL指令可以用于在查找表（LUT）中查找值，将一个最多5位的索引转换为8位整数值。维度高于1的VQ可以通过使用多个LUT和相应的TBL指令来实现。例如，2D
    VQ每索引2位转换为2个LUT，一个用于每个VQ维度，每个将4位索引映射到8位值。
- en: We run an experiment on a device with Snapdragon® technology¹¹1Snapdragon is
    a product of Qualcomm Technologies, Inc. and/or its subsidiaries.. In our experiments
    we measure weights transferred and decoded per second and report relative speed
    compared to an 4-bit integer baseline. We measure data transfer latency on 2D
    vector quantized data tensors with 2 or 2.5 bits per dimension, i.e. 4 or 5 bits
    per index respectively. We don’t consider settings with a higher bitwidth per
    index, as this would require double the number of TBL instructions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在搭载Snapdragon®技术¹¹Snapdragon是Qualcomm Technologies, Inc.及/或其子公司的产品。¹的设备上进行了实验。在我们的实验中，我们测量了每秒传输和解码的权重，并报告了相对于4位整数基线的相对速度。我们测量了2D向量量化数据张量的传输延迟，这些张量每维度2或2.5位，即每索引4或5位。我们不考虑每索引具有更高位宽的设置，因为这将需要双倍数量的TBL指令。
- en: 'Table [3](#S4.T3 "Table 3 ‣ 4.2 Data transfer speed comparision ‣ 4 Experiments
    and results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") shows
    the results of this experiment. In this table we show that besides large footprint
    reductions, VQ also reduces data transfer latency compared to the 4-bit integer
    baseline. Lastly, we run one LLM-generation experiment on Llamav2-7B on the same
    device. In this experiment we integrate a 1D VQ decoding kernel with the MatMul
    operation.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '表[3](#S4.T3 "表 3 ‣ 4.2 数据传输速度比较 ‣ 4 实验和结果 ‣ GPTVQ: 维度的福音对于 LLM 量化")展示了此实验的结果。在这个表中，我们展示了除了大幅减少占用空间外，VQ还降低了与4位整数基线相比的数据传输延迟。最后，我们在相同设备上对Llamav2-7B进行了一次LLM生成实验。在这个实验中，我们将一个1D
    VQ解码内核与MatMul操作集成在一起。'
- en: 'Table 3: Model footprint and latency of vector-quantized data transfer and
    decoding.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 向量量化数据传输和解码的模型足迹和延迟。'
- en: '| Setting | BPV $\downarrow$ latency |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 设置 | BPV $\downarrow$ 延迟 |'
- en: '| INT4 | 4 | 1.00$\times$ |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| INT4 | 4 | 1.00$\times$ |'
- en: '| INT8 | 8 | 2.00$\times$ |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| INT8 | 8 | 2.00$\times$ |'
- en: '| 2D 2.5B @ 512 | 3 | 0.75$\times$ |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 2D 2.5B @ 512 | 3 | 0.75$\times$ |'
- en: '| 2D 2.5B @ 2048 | 2.25 | 0.56$\times$ |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 2D 2.5B @ 2048 | 2.25 | 0.56$\times$ |'
- en: '| 2D 2B @ 1024 | 2.25 | 0.56$\times$ |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 2D 2B @ 1024 | 2.25 | 0.56$\times$ |'
- en: '| Llamav2-7B 1D 3B @ 128 | 3.5 | 0.88$\times$ |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| Llamav2-7B 1D 3B @ 128 | 3.5 | 0.88$\times$ |'
- en: 4.3 Ablations on hyperparameter choices
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 超参数选择的消融
- en: EM initialization
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EM初始化
- en: 'Starting EM initialization from a good set of seed centroids is crucial to
    the final GPTVQ performance. To find seed centroids for EM initialization, we
    compare k-Means++ (Arthur & Vassilvitskii, [2007](#bib.bib2)) to a quick and effective
    initialization method which we dub *Mahalanobis initialization*. In the latter
    method, we initialize EM for a matrix of $N$ apart from the sorted list. While
    not theoretically justifiable, intuitively this method ensures that points are
    sampled at representative distances. Table [6](#A2.T6 "Table 6 ‣ Appendix B Tables
    for hyperparameter ablations ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")
    shows perplexity after GPTVQ for different methods of finding good seed values
    for EM initialization. Here we see that Mahalanobis initialization performs comparably
    to k-Means++, at significantly increased speed.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从一组好的种子质心开始EM初始化对最终的GPTVQ性能至关重要。为了找到EM初始化的种子质心，我们将k-Means++ (Arthur & Vassilvitskii,
    [2007](#bib.bib2))与一种快速有效的初始化方法进行比较，我们称之为*马氏初始化*。在后者的方法中，我们初始化EM用于一个$N$矩阵，而不是排序列表。虽然从理论上讲并不完全合理，但直观上这种方法确保了点在代表性距离处进行采样。表[6](#A2.T6
    "表 6 ‣ 附录 B 超参数消融 ‣ GPTVQ：LLM量化的维度祝福")展示了不同的EM初始化种子值方法下，GPTVQ的困惑度结果。在这里我们看到，马氏初始化的表现与k-Means++相当，但速度显著提高。
- en: EM iterations
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: EM迭代
- en: 'We explore the effect of the number of EM initialization iterations on the
    final of perplexity of GPTVQ. Table [7](#A2.T7 "Table 7 ‣ Appendix B Tables for
    hyperparameter ablations ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization")
    shows that even up to 100 iterations, results keep slightly improving, therefore
    we use 100 iterations as default.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨了EM初始化迭代次数对GPTVQ最终困惑度的影响。表[7](#A2.T7 "表 7 ‣ 附录 B 超参数消融 ‣ GPTVQ：LLM量化的维度祝福")显示，即使达到100次迭代，结果仍然有所改善，因此我们使用100次迭代作为默认值。
- en: Codebook overhead
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码本开销
- en: 'As mentioned in section [4.1](#S4.SS1 "4.1 GPTVQ hyperparameters ‣ 4 Experiments
    and results ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization"), we
    determine a group size to target a specific overhead. However, if codebooks are
    quantized to lower bitwidths, or if codebook compression is applied as described
    in Section [3.3](#S3.SS3 "3.3 Additional steps ‣ 3 GPTVQ ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization"), the group size can be proportionally
    decreased to achieve the same overhead.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如[4.1](#S4.SS1 "4.1 GPTVQ 超参数 ‣ 4 实验和结果 ‣ GPTVQ：LLM量化的维度祝福")节所述，我们确定了一组大小以达到特定的开销。然而，如果代码本被量化为较低的比特宽度，或应用了[3.3](#S3.SS3
    "3.3 附加步骤 ‣ 3 GPTVQ ‣ GPTVQ：LLM量化的维度祝福")节中描述的代码本压缩，则可以相应地减少组大小以实现相同的开销。
- en: 'We perform experiments targeting an overhead of 0.125 bits per value, and evaluate
    which method achieves best results: keeping the codebook in 16 bit, quantizing
    the codebook to 8 bit and halving the blocksize, or keeping the codebook in 16
    bit, but reducing its rank to 50% of the original rank and halving the blocksize.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行的实验目标是每个值0.125比特的开销，并评估哪种方法能够获得最佳结果：保持代码本为16比特，将代码本量化为8比特并减半块大小，或者保持代码本为16比特，但将其秩减少到原始秩的50%并减半块大小。
- en: 'In Table [8](#A2.T8 "Table 8 ‣ Appendix B Tables for hyperparameter ablations
    ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") the results of
    these experiments show that, overall, quantizing the codebook to 8 bit generally
    yields slightly improved results.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[8](#A2.T8 "表 8 ‣ 附录 B 超参数消融 ‣ GPTVQ：LLM量化的维度祝福")中，这些实验的结果显示，总体上，将代码本量化为8比特通常会产生略微改善的结果。
- en: Codebook update
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 代码本更新
- en: 'In Table [9](#A2.T9 "Table 9 ‣ Appendix B Tables for hyperparameter ablations
    ‣ GPTVQ: The Blessing of Dimensionality for LLM Quantization") we include an ablation
    on the effect including codebook update, as described in Section [3.3](#S3.SS3
    "3.3 Additional steps ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM
    Quantization"). We find that, in all cases, updating the codebook after running
    Algorithm [1](#alg1 "Algorithm 1 ‣ 3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The
    Blessing of Dimensionality for LLM Quantization") improves final perplexity, at
    the expense of moderately increased (though still reasonable) run time. We thus
    include codebook update in all training runs.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在表[9](#A2.T9 "表9 ‣ 附录B 超参数消融表 ‣ GPTVQ：LLM量化的维度祝福")中，我们包括了对码本更新效果的消融实验，如第[3.3](#S3.SS3
    "3.3 额外步骤 ‣ 3 GPTVQ ‣ GPTVQ：LLM量化的维度祝福")节中所述。我们发现，在所有情况下，算法[1](#alg1 "算法1 ‣ 3.2
    GPTVQ方法 ‣ 3 GPTVQ ‣ GPTVQ：LLM量化的维度祝福")运行后更新码本能改善最终的困惑度，尽管运行时间会适度增加（但仍在合理范围内）。因此，我们在所有训练运行中都包括了码本更新。
- en: Method runtime
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 方法运行时间
- en: 'Our method can process large language models efficiently. Exact runtime of
    GPTVQ depends on model, quantization setting (groupsize, bitwidth, vq-dimension),
    and several hyperparameters (EM iterations, codebook update iterations). To give
    an indication of realistic run-times: on a single H100, Llamav2-7B takes between
    30 minutes and 1 hour, while Llamav2-70B takes between between 3 and 11 hours.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法可以高效地处理大型语言模型。GPTVQ的确切运行时间取决于模型、量化设置（组大小、位宽、vq维度）以及若干超参数（EM迭代次数、码本更新迭代次数）。为了给出实际运行时间的指示：在单个H100上，Llamav2-7B的运行时间在30分钟到1小时之间，而Llamav2-70B的运行时间则在3到11小时之间。
- en: Effect of blockwise data normalization
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 块状数据标准化的效果
- en: 'We investigate how applying input data normalization as described in Section [3.2](#S3.SS2
    "3.2 The GPTVQ method ‣ 3 GPTVQ ‣ GPTVQ: The Blessing of Dimensionality for LLM
    Quantization") affects final performance. Table [10](#A2.T10 "Table 10 ‣ Appendix
    B Tables for hyperparameter ablations ‣ GPTVQ: The Blessing of Dimensionality
    for LLM Quantization") shows how perplexity of the quantized model depends on
    the scaling block size. In addition, we compared perplexity for configurations
    of equal overhead with and without scaling applied, see the Table [11](#A2.T11
    "Table 11 ‣ Appendix B Tables for hyperparameter ablations ‣ GPTVQ: The Blessing
    of Dimensionality for LLM Quantization") for the results. Overall, we see that
    scaling improves the results in many cases, however sometimes it leads to perplexity
    increase, especially in the case of 1D VQ with 2 bits per index.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了应用第[3.2](#S3.SS2 "3.2 GPTVQ方法 ‣ 3 GPTVQ ‣ GPTVQ：LLM量化的维度祝福")节中描述的输入数据标准化对最终性能的影响。表[10](#A2.T10
    "表10 ‣ 附录B 超参数消融表 ‣ GPTVQ：LLM量化的维度祝福")展示了量化模型的困惑度如何依赖于缩放块大小。此外，我们比较了应用和不应用缩放的等开销配置的困惑度，结果见表[11](#A2.T11
    "表11 ‣ 附录B 超参数消融表 ‣ GPTVQ：LLM量化的维度祝福")。总体而言，我们发现缩放在许多情况下改善了结果，但有时会导致困惑度增加，尤其是在1D
    VQ每个索引2位的情况下。
- en: 5 Related work
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: Vector quantization
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 向量量化
- en: A number of works suggested using vector quantization for CNN weights compression (Gong
    et al., [2014](#bib.bib14); Martinez et al., [2021](#bib.bib25); Fan et al., [2020](#bib.bib8);
    Stock et al., [2019](#bib.bib33); Wu et al., [2016](#bib.bib39); Martinez et al.,
    [2021](#bib.bib25); Cho et al., [2021](#bib.bib4)). The most common approach is
    to reshape the weights of convolutional or fully connected layers into a matrix,
    and then apply K-means clustering directly on the columns. Typically, the clustering
    is applied on scalar or vectors of dimensionality 4 or higher. Some of the works
    consider data-aware optimization of the quantized weights. Most often, a variant
    of EM algorithm is used in order to update centroids and assignments (Stock et al.,
    [2019](#bib.bib33); Gong et al., [2014](#bib.bib14)). An alternative approach
    is using a differentiable K-means formulation which enables fine-tuning using
    SGD with the original loss function in order to recover the network accuracy (Cho
    et al., [2021](#bib.bib4); Fan et al., [2020](#bib.bib8); Tang et al., [2023](#bib.bib34)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究建议使用向量量化来压缩CNN权重（Gong et al., [2014](#bib.bib14); Martinez et al., [2021](#bib.bib25);
    Fan et al., [2020](#bib.bib8); Stock et al., [2019](#bib.bib33); Wu et al., [2016](#bib.bib39);
    Martinez et al., [2021](#bib.bib25); Cho et al., [2021](#bib.bib4)）。最常见的方法是将卷积层或全连接层的权重重塑为矩阵，然后直接在列上应用K均值聚类。通常，聚类是在标量或维度为4或更高的向量上应用的。一些研究考虑了量化权重的数据感知优化。通常，使用EM算法的变体来更新中心和分配（Stock
    et al., [2019](#bib.bib33); Gong et al., [2014](#bib.bib14)）。另一种方法是使用可微分的K均值公式，这使得可以使用原始损失函数通过SGD进行微调，以恢复网络的准确性（Cho
    et al., [2021](#bib.bib4); Fan et al., [2020](#bib.bib8); Tang et al., [2023](#bib.bib34)）。
- en: LLM quantization
  id: totrans-169
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: LLM量化
- en: Applying DNN quantization approaches for recent LLMs often poses significant
    computational challenges. Therefore, even uniform post-training quantization methods
    required revisiting to improve their scalability (Frantar et al., [2022](#bib.bib10)).
    As vector quantization approaches have higher computational complexity, using
    them for LLM weights compression has even stricter computational requirements.
    The most similar to our work is the approach (Deng et al., [2024](#bib.bib7)).
    The method uses gradient-based layer sensitivities to update the codebooks and
    a reduced complexity LoRA-based approach (Hu et al., [2021](#bib.bib18)) to partially
    recover the accuracy.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 将DNN量化方法应用于近期的LLM通常面临显著的计算挑战。因此，即使是均匀的后训练量化方法也需要重新审视以提高其可扩展性（Frantar et al.,
    [2022](#bib.bib10)）。由于向量量化方法具有更高的计算复杂性，使用它们进行LLM权重压缩的计算要求更加严格。与我们的工作最相似的方法是（Deng
    et al., [2024](#bib.bib7)）。该方法使用基于梯度的层灵敏度来更新代码簿，并采用减少复杂性的LoRA方法（Hu et al., [2021](#bib.bib18)）来部分恢复准确性。
- en: Hessian-based compression methods
  id: totrans-171
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于Hessian的压缩方法
- en: Several classical works suggest to use second-order approximation of the neural
    network loss function for accurate unstructured pruning (LeCun et al., [1989](#bib.bib23);
    Hassibi et al., [1993](#bib.bib16)). A line of more recent papers extend this
    family of methods for PTQ (Singh & Alistarh, [2020](#bib.bib32); Frantar & Alistarh,
    [2022](#bib.bib9); Frantar et al., [2022](#bib.bib10)).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 一些经典研究建议使用神经网络损失函数的二阶近似进行准确的非结构化剪枝（LeCun et al., [1989](#bib.bib23); Hassibi
    et al., [1993](#bib.bib16)）。一些较新的论文扩展了这一方法族以用于PTQ（Singh & Alistarh, [2020](#bib.bib32);
    Frantar & Alistarh, [2022](#bib.bib9); Frantar et al., [2022](#bib.bib10)）。
- en: 6 Conclusions
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this work we have shown that vector quantization in one or more dimensions
    progressively improves quantized model accuracy. We have introduced a fast method
    for post-training quantization of large networks using VQ. This method achieves
    SOTA model size vs accuracy trade-offs on a wide range of LLMs and zero-shot tasks.
    Finally, we have shown that VQ presents a HW-feasible alternative to uniform quantization
    as a compression method, yielding increased tokens per second at the same accuracy,
    or higher accuracy for a fixed tokens per second budget.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们展示了在一个或多个维度上进行向量量化可以逐步提高量化模型的准确性。我们介绍了一种用于大规模网络的快速后训练量化方法，利用了VQ。这种方法在广泛的LLM和零样本任务中实现了最先进的模型大小与准确性之间的权衡。最后，我们展示了VQ作为一种压缩方法，提供了一种硬件可行的替代均匀量化的方法，在相同的准确性下提高了每秒的令牌数，或者在固定的每秒令牌预算下提高了准确性。
- en: Acknowledgement
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: We would like to thank Amir Said for useful discussions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要感谢Amir Said的有益讨论。
- en: Impact
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 影响
- en: Efficiency
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 效率
- en: Our method can be used to make models more efficient. Given hardware and a software
    stack that supports vector quantized networks, a user can run more inference for
    a given energy budget, or reduce the energy required for a fixed inference task.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法可以用来提高模型的效率。在支持向量量化网络的硬件和软件栈的支持下，用户可以在给定的能量预算下进行更多的推理，或减少固定推理任务所需的能量。
- en: Democratization
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 民主化
- en: Reducing the inference cost of neural networks generally allows more practitioners
    to deploy models and increases democratization of deep learning. Our method itself
    is efficient enough that it can be run on consumer-grade hardware, even for very
    large networks.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 降低神经网络的推理成本通常可以让更多从业者部署模型，并增加深度学习的民主化。我们的方法本身足够高效，可以在消费者级硬件上运行，即使是非常大的网络。
- en: Bias
  id: totrans-182
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 偏差
- en: While it has been shown that model pruning can increase bias in neural networks
    (Iofinova et al., [2023](#bib.bib19)), whether this is the case for quantization
    and to what extent, and whether how this applies to large language models is an
    underexplored topic. An investigation of this topic is outside the scope of this
    paper, but we concede that our method may introduce subtle biases into quantized
    models.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然已经有研究表明模型剪枝可能会增加神经网络的偏差（Iofinova 等，[2023](#bib.bib19)），但量化是否也会如此，以及在何种程度上，以及如何将这种情况应用于大型语言模型仍然是一个未充分探索的话题。对这一话题的调查超出了本文的范围，但我们承认我们的方法可能会在量化模型中引入微妙的偏差。
- en: References
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Achiam et al. (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya,
    I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.
    Gpt-4 technical report. *arXiv preprint arXiv:2303.08774*, 2023.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Achiam 等 (2023) Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman,
    F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., 等. GPT-4 技术报告.
    *arXiv 预印本 arXiv:2303.08774*，2023年。
- en: Arthur & Vassilvitskii (2007) Arthur, D. and Vassilvitskii, S. K-means++ the
    advantages of careful seeding. In *Proceedings of the eighteenth annual ACM-SIAM
    symposium on Discrete algorithms*, pp.  1027–1035, 2007.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arthur & Vassilvitskii (2007) Arthur, D. 和 Vassilvitskii, S. K-means++：精心初始化的优势.
    见 *第十八届 ACM-SIAM 离散算法年会论文集*，页 1027–1035，2007年。
- en: 'Bisk et al. (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., et al. Piqa: Reasoning
    about physical commonsense in natural language. In *Proceedings of the AAAI conference
    on artificial intelligence*, volume 34, pp.  7432–7439, 2020.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Bisk 等 (2020) Bisk, Y., Zellers, R., Gao, J., Choi, Y., 等. Piqa: 关于自然语言中的物理常识的推理.
    见 *AAAI 人工智能会议论文集*，第 34 卷，页 7432–7439，2020年。'
- en: 'Cho et al. (2021) Cho, M., Vahid, K. A., Adya, S., and Rastegari, M. Dkm: Differentiable
    k-means clustering layer for neural network compression. *arXiv preprint arXiv:2108.12659*,
    2021.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cho 等 (2021) Cho, M., Vahid, K. A., Adya, S., 和 Rastegari, M. DKM: 神经网络压缩的可微
    K-均值聚类层. *arXiv 预印本 arXiv:2108.12659*，2021年。'
- en: 'Clark et al. (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., and Toutanova, K. Boolq: Exploring the surprising difficulty of natural yes/no
    questions. In *NAACL*, 2019.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Clark 等 (2019) Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins,
    M., 和 Toutanova, K. BoolQ: 探索自然是/否问题的惊人难度. 见 *NAACL*，2019年。'
- en: Clark et al. (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal,
    A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try
    arc, the ai2 reasoning challenge. *arXiv:1803.05457v1*, 2018.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Clark 等 (2018) Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,
    Schoenick, C., 和 Tafjord, O. 认为你已经解决了问答问题？尝试 ARC，AI2 推理挑战。*arXiv:1803.05457v1*，2018年。
- en: Deng et al. (2024) Deng, J., Li, S., Wang, C., Gu, H., Shen, H., and Huang,
    K. LLM-codebook for extreme compression of large language models, 2024. URL [https://openreview.net/forum?id=nMbWsXPUVL](https://openreview.net/forum?id=nMbWsXPUVL).
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng 等 (2024) Deng, J., Li, S., Wang, C., Gu, H., Shen, H., 和 Huang, K. LLM-代码本：用于极限压缩大型语言模型，2024年。网址
    [https://openreview.net/forum?id=nMbWsXPUVL](https://openreview.net/forum?id=nMbWsXPUVL)。
- en: Fan et al. (2020) Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R.,
    Jegou, H., and Joulin, A. Training with quantization noise for extreme model compression.
    *arXiv preprint arXiv:2004.07320*, 2020.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fan 等 (2020) Fan, A., Stock, P., Graham, B., Grave, E., Gribonval, R., Jegou,
    H., 和 Joulin, A. 使用量化噪声进行极限模型压缩. *arXiv 预印本 arXiv:2004.07320*，2020年。
- en: 'Frantar & Alistarh (2022) Frantar, E. and Alistarh, D. Optimal brain compression:
    A framework for accurate post-training quantization and pruning. *Advances in
    Neural Information Processing Systems*, 35:4475–4488, 2022.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar & Alistarh (2022) Frantar, E. 和 Alistarh, D. 最优脑压缩：一种准确的训练后量化和剪枝框架.
    *神经信息处理系统进展*，第 35 卷，页 4475–4488，2022年。
- en: 'Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh,
    D. Gptq: Accurate post-training quantization for generative pre-trained transformers.
    *arXiv preprint arXiv:2210.17323*, 2022.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Frantar et al. (2022) Frantar, E., Ashkboos, S., Hoefler, T., 和 Alistarh, D.
    Gptq: 生成预训练变换器的准确后训练量化。*arXiv 预印本 arXiv:2210.17323*，2022年。'
- en: Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A. A framework for few-shot
    language model evaluation, 12 2023. URL [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836).
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao et al. (2023) Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi,
    A., Foster, C., Golding, L., Hsu, J., Le Noac’h, A., Li, H., McDonell, K., Muennighoff,
    N., Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
    L., Tang, E., Thite, A., Wang, B., Wang, K., 和 Zou, A. 一种少样本语言模型评估框架，2023年12月。网址
    [https://zenodo.org/records/10256836](https://zenodo.org/records/10256836)。
- en: Gersho & Gray (2012) Gersho, A. and Gray, R. M. *Vector quantization and signal
    compression*, volume 159. Springer Science & Business Media, 2012.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gersho & Gray (2012) Gersho, A. 和 Gray, R. M. *向量量化与信号压缩*，第159卷。Springer Science
    & Business Media，2012年。
- en: Gholami et al. (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,
    and Keutzer, K. A survey of quantization methods for efficient neural network
    inference. In *Low-Power Computer Vision*, pp.  291–326\. Chapman and Hall/CRC,
    2022.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholami et al. (2022) Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W.,
    和 Keutzer, K. 高效神经网络推理的量化方法综述。在 *低功耗计算机视觉*，第291–326页。Chapman and Hall/CRC，2022年。
- en: Gong et al. (2014) Gong, Y., Liu, L., Yang, M., and Bourdev, L. Compressing
    deep convolutional networks using vector quantization. *arXiv preprint arXiv:1412.6115*,
    2014.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gong et al. (2014) Gong, Y., Liu, L., Yang, M., 和 Bourdev, L. 使用向量量化压缩深度卷积网络。*arXiv
    预印本 arXiv:1412.6115*，2014年。
- en: 'Han et al. (2015) Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing
    deep neural networks with pruning, trained quantization and huffman coding. *arXiv
    preprint arXiv:1510.00149*, 2015.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. (2015) Han, S., Mao, H., 和 Dally, W. J. 深度压缩：通过剪枝、训练量化和霍夫曼编码压缩深度神经网络。*arXiv
    预印本 arXiv:1510.00149*，2015年。
- en: Hassibi et al. (1993) Hassibi, B., Stork, D. G., and Wolff, G. J. Optimal brain
    surgeon and general network pruning. In *IEEE international conference on neural
    networks*, pp.  293–299\. IEEE, 1993.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassibi et al. (1993) Hassibi, B., Stork, D. G., 和 Wolff, G. J. 最优脑外科医生与通用网络剪枝。在
    *IEEE国际神经网络会议*，第293–299页。IEEE，1993年。
- en: He et al. (2017) He, Y., Zhang, X., and Sun, J. Channel pruning for accelerating
    very deep neural networks. In *Proceedings of the IEEE International Conference
    on Computer Vision*, pp.  1389–1397, 2017.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He et al. (2017) He, Y., Zhang, X., 和 Sun, J. 通过通道剪枝加速非常深的神经网络。在 *IEEE国际计算机视觉会议论文集*，第1389–1397页，2017年。
- en: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., and Chen, W. Lora: Low-rank adaptation of large language models.
    *arXiv preprint arXiv:2106.09685*, 2021.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hu et al. (2021) Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang,
    S., Wang, L., 和 Chen, W. Lora: 大型语言模型的低秩适配。*arXiv 预印本 arXiv:2106.09685*，2021年。'
- en: 'Iofinova et al. (2023) Iofinova, E., Peste, A., and Alistarh, D. Bias in pruned
    vision models: In-depth analysis and countermeasures. *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, CVPR*, pp.  24364–24373, 2023.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iofinova et al. (2023) Iofinova, E., Peste, A., 和 Alistarh, D. 剪枝视觉模型中的偏差：深入分析与对策。*IEEE/CVF计算机视觉与模式识别会议（CVPR）*，第24364–24373页，2023年。
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., et al. Mistral 7b. *arXiv preprint arXiv:2310.06825*, 2023.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., 等. Mistral 7b。*arXiv 预印本 arXiv:2310.06825*，2023年。
- en: Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
    B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F.,
    et al. Mixtral of experts. *arXiv preprint arXiv:2401.04088*, 2024.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. (2024) Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,
    B., Bamford, C., Chaplot, D. S., Casas, D. d. l., Hanna, E. B., Bressand, F.,
    等. 专家混合模型。*arXiv 预印本 arXiv:2401.04088*，2024年。
- en: 'Keisuke et al. (2019) Keisuke, S., Ronan, L. B., Chandra, B., and Yejin, C.
    Winogrande: An adversarial winograd schema challenge at scale. 2019.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Keisuke et al. (2019) Keisuke, S., Ronan, L. B., Chandra, B., 和 Yejin, C. Winogrande:
    大规模对抗性Winograd模式挑战。2019年。'
- en: LeCun et al. (1989) LeCun, Y., Denker, J., and Solla, S. Optimal brain damage.
    *Advances in neural information processing systems*, 2, 1989.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun et al. (1989) LeCun, Y., Denker, J., 和 Solla, S. 最优脑损伤。*神经信息处理系统进展*，第2卷，1989年。
- en: 'Lin et al. (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han,
    S. Awq: Activation-aware weight quantization for llm compression and acceleration.
    *arXiv preprint arXiv:2306.00978*, 2023.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Lin 等 (2023) Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., 和 Han, S. Awq:
    激活感知权重量化用于大规模语言模型压缩和加速。*arXiv 预印本 arXiv:2306.00978*，2023。'
- en: 'Martinez et al. (2021) Martinez, J., Shewakramani, J., Liu, T. W., Bârsan,
    I. A., Zeng, W., and Urtasun, R. Permute, quantize, and fine-tune: Efficient compression
    of neural networks. In *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, pp.  15699–15708, 2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Martinez 等 (2021) Martinez, J., Shewakramani, J., Liu, T. W., Bârsan, I. A.,
    Zeng, W., 和 Urtasun, R. 置换、量化与微调: 神经网络的高效压缩。载于*IEEE/CVF计算机视觉与模式识别会议论文集*，第15699–15708页，2021。'
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer
    sentinel mixture models, 2016.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 (2016) Merity, S., Xiong, C., Bradbury, J., 和 Socher, R. Pointer sentinel
    mixture models，2016。
- en: Nagel et al. (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., and
    Blankevoort, T. Up or down? adaptive rounding for post-training quantization.
    In *International Conference on Machine Learning*, pp.  7197–7206\. PMLR, 2020.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等 (2020) Nagel, M., Amjad, R. A., Van Baalen, M., Louizos, C., 和 Blankevoort,
    T. 向上还是向下？训练后量化的自适应舍入。载于*国际机器学习大会论文集*，第7197–7206页，PMLR，2020。
- en: Nagel et al. (2021) Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y.,
    Van Baalen, M., and Blankevoort, T. A white paper on neural network quantization.
    *arXiv preprint arXiv:2106.08295*, 2021.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nagel 等 (2021) Nagel, M., Fournarakis, M., Amjad, R. A., Bondarenko, Y., Van
    Baalen, M., 和 Blankevoort, T. 神经网络量化白皮书。*arXiv 预印本 arXiv:2106.08295*，2021。
- en: 'Rouhani et al. (2023) Rouhani, B., Zhao, R., Elango, V., Shafipour, R., Hall,
    M., Mesmakhosroshahi, M., More, A., Melnick, L., Golub, M., Varatkar, G., Shao,
    L., Kolhe, G., Melts, D., Klar, J., L’Heureux, R., Perry, M., Burger, D., Chung,
    E., Deng, Z., and Naumov, M. With shared microexponents, a little shifting goes
    a long way. In *Proceedings of the 50th Annual International Symposium on Computer
    Architecture*, pp.  Article No.: 83, 2023.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rouhani 等 (2023) Rouhani, B., Zhao, R., Elango, V., Shafipour, R., Hall, M.,
    Mesmakhosroshahi, M., More, A., Melnick, L., Golub, M., Varatkar, G., Shao, L.,
    Kolhe, G., Melts, D., Klar, J., L’Heureux, R., Perry, M., Burger, D., Chung, E.,
    Deng, Z., 和 Naumov, M. 通过共享微指数，一点微调能产生显著效果。载于*第50届年度国际计算机架构研讨会论文集*，第83号文章，2023。
- en: 'Roziere et al. (2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat,
    I., Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., et al. Code llama: Open
    foundation models for code. *arXiv preprint arXiv:2308.12950*, 2023.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Roziere 等 (2023) Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I.,
    Tan, X. E., Adi, Y., Liu, J., Remez, T., Rapin, J., 等. Code llama: 用于代码的开放基础模型。*arXiv
    预印本 arXiv:2308.12950*，2023。'
- en: 'Shao et al. (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z.,
    Zhang, K., Gao, P., Qiao, Y., and Luo, P. Omniquant: Omnidirectionally calibrated
    quantization for large language models. *arXiv preprint arXiv:2308.13137*, 2023.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shao 等 (2023) Shao, W., Chen, M., Zhang, Z., Xu, P., Zhao, L., Li, Z., Zhang,
    K., Gao, P., Qiao, Y., 和 Luo, P. Omniquant: 面向大规模语言模型的全方位校准量化。*arXiv 预印本 arXiv:2308.13137*，2023。'
- en: 'Singh & Alistarh (2020) Singh, S. P. and Alistarh, D. Woodfisher: Efficient
    second-order approximation for neural network compression. *Advances in Neural
    Information Processing Systems*, 33:18098–18109, 2020.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Singh & Alistarh (2020) Singh, S. P. 和 Alistarh, D. Woodfisher: 神经网络压缩的高效二阶近似。*神经信息处理系统进展*，33:18098–18109，2020。'
- en: 'Stock et al. (2019) Stock, P., Joulin, A., Gribonval, R., Graham, B., and Jégou,
    H. And the bit goes down: Revisiting the quantization of neural networks. *arXiv
    preprint arXiv:1907.05686*, 2019.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Stock 等 (2019) Stock, P., Joulin, A., Gribonval, R., Graham, B., 和 Jégou, H.
    比特下降: 再探神经网络量化。*arXiv 预印本 arXiv:1907.05686*，2019。'
- en: 'Tang et al. (2023) Tang, X., Wang, Y., Cao, T., Zhang, L. L., Chen, Q., Cai,
    D., Liu, Y., and Yang, M. Lut-nn: Empower efficient neural network inference with
    centroid learning and table lookup. In *Proceedings of the 29th Annual International
    Conference on Mobile Computing and Networking*, pp.  1–15, 2023.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Tang 等 (2023) Tang, X., Wang, Y., Cao, T., Zhang, L. L., Chen, Q., Cai, D.,
    Liu, Y., 和 Yang, M. Lut-nn: 通过质心学习和表查找提升神经网络推理效率。载于*第29届年度国际移动计算与网络会议论文集*，第1–15页，2023。'
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.
    Llama: Open and efficient foundation language models. *arXiv preprint arXiv:2302.13971*,
    2023a.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等 (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., 等. Llama: 开放和高效的基础语言模型。*arXiv
    预印本 arXiv:2302.13971*，2023a。'
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: Open foundation and fine-tuned chat models. *arXiv preprint arXiv:2307.09288*,
    2023b.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et al. Llama
    2: 开放基础和微调的聊天模型。*arXiv预印本 arXiv:2307.09288*，2023b。'
- en: Tu et al. (2024) Tu, T., Palepu, A., Schaekermann, M., Saab, K., Freyberg, J.,
    Tanno, R., Wang, A., Li, B., Amin, M., Tomasev, N., et al. Towards conversational
    diagnostic ai. *arXiv preprint arXiv:2401.05654*, 2024.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tu et al. (2024) Tu, T., Palepu, A., Schaekermann, M., Saab, K., Freyberg, J.,
    Tanno, R., Wang, A., Li, B., Amin, M., Tomasev, N., et al. 迈向对话式诊断AI。*arXiv预印本
    arXiv:2401.05654*，2024年。
- en: 'Workshop et al. (2022) Workshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick,
    E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., et al. Bloom:
    A 176b-parameter open-access multilingual language model. *arXiv preprint arXiv:2211.05100*,
    2022.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Workshop et al. (2022) Workshop, B., Scao, T. L., Fan, A., Akiki, C., Pavlick,
    E., Ilić, S., Hesslow, D., Castagné, R., Luccioni, A. S., Yvon, F., et al. Bloom:
    一个176b参数的开放访问多语言模型。*arXiv预印本 arXiv:2211.05100*，2022年。'
- en: Wu et al. (2016) Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. Quantized
    convolutional neural networks for mobile devices. In *Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition*, pp.  4820–4828, 2016.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2016) Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. 量化卷积神经网络用于移动设备。在*IEEE计算机视觉与模式识别会议论文集*，第4820–4828页，2016年。
- en: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: Can a machine really finish your sentence? In *Proceedings
    of the 57th Annual Meeting of the Association for Computational Linguistics*,
    2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Zellers et al. (2019) Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
    Choi, Y. Hellaswag: 机器真的能完成你的句子吗？在*第57届计算语言学协会年会论文集*，2019年。'
- en: Zhang et al. (2016) Zhang, X., Zou, J., He, K., and Sun, J. Accelerating very
    deep convolutional networks for classification and detection. *IEEE transactions
    on pattern analysis and machine intelligence*, 38(10):1943–1955, 2016.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2016) Zhang, X., Zou, J., He, K., and Sun, J. 加速用于分类和检测的非常深的卷积网络。*IEEE模式分析与机器智能汇刊*，38(10)：1943–1955，2016年。
- en: Appendix A Extended results
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A 扩展结果
- en: 'Table 4: Weight-only quantization results of Llama-1, Llama-2, Mistral, and
    Mixtral-MoE Models. We report WikiText2 perplexity in this table; lower is better
    Models marked ‘L1’ or ‘L2’ denote Llama-v1 and Llama-v2, respectively. M denotes
    Mistral and 8x7B denotes Mixtral-MoE 8x7B.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：Llama-1、Llama-2、Mistral 和 Mixtral-MoE 模型的权重量化结果。我们在此表中报告了WikiText2困惑度；值越低越好。标记为‘L1’或‘L2’的模型分别表示Llama-v1和Llama-v2。M表示Mistral，8x7B表示Mixtral-MoE
    8x7B。
- en: '|  | L1-7B | L1-13B | L1-30B | L1-65B | L2-7B | L2-13B | L2-70B | M-7B | 8x7B
    |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '|  | L1-7B | L1-13B | L1-30B | L1-65B | L2-7B | L2-13B | L2-70B | M-7B | 8x7B
    |'
- en: '| FP16 |  | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 | 5.25 | 3.84 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| FP16 |  | 5.68 | 5.09 | 4.10 | 3.53 | 5.47 | 4.88 | 3.31 | 5.25 | 3.84 |'
- en: '| 2.125 bpv (W2@g128) | RTN | 1.9e3 | 781.20 | 68.04 | 15.08 | 4.2e3 | 122.08
    | 27.27 | 1.4e3 | 4.3e3 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 2.125 bpv (W2@g128) | RTN | 1.9e3 | 781.20 | 68.04 | 15.08 | 4.2e3 | 122.08
    | 27.27 | 1.4e3 | 4.3e3 |'
- en: '| GPTQ | 44.01 | 15.60 | 10.92 | 9.51 | 36.77 | 28.14 | 6.74 | 15.68 | 14.50
    |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 44.01 | 15.60 | 10.92 | 9.51 | 36.77 | 28.14 | 6.74 | 15.68 | 14.50
    |'
- en: '| AWQ | 2.6e5 | 2.8e5 | 2.4e5 | 7.4e4 | 2.2e5 | 1.2e5 | - |  |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 2.6e5 | 2.8e5 | 2.4e5 | 7.4e4 | 2.2e5 | 1.2e5 | - |  |  |'
- en: '| OmniQuant | 9.72 | 7.93 | 7.12 | 5.95 | 11.06 | 8.26 | 6.55 |  |  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 9.72 | 7.93 | 7.12 | 5.95 | 11.06 | 8.26 | 6.55 |  |  |'
- en: '| GPTVQ 1D (ours) | 16.29 | 6.93 | 6.04 | 5.19 | 11.57 | 7.34 | 5.00 | 15.03
    |  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 1D (ours) | 16.29 | 6.93 | 6.04 | 5.19 | 11.57 | 7.34 | 5.00 | 15.03
    |  |'
- en: '| GPTVQ 2D (ours) | 9.64 | 6.58 | 5.63 | 4.91 | 8.23 | 6.50 | 4.64 | 10.28
    |  |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 2D (ours) | 9.64 | 6.58 | 5.63 | 4.91 | 8.23 | 6.50 | 4.64 | 10.28
    |  |'
- en: '| 2.25 bpv (W2@g64) | RTN | 188.32 | 101.87 | 19.20 | 9.39 | 431.97 | 26.22
    | 10.31 | 71.52 | 155.82 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2.25 bpv (W2@g64) | RTN | 188.32 | 101.87 | 19.20 | 9.39 | 431.97 | 26.22
    | 10.31 | 71.52 | 155.82 |'
- en: '| GPTQ | 22.10 | 10.06 | 8.54 | 8.31 | 20.85 | 22.44 | NAN | 14.24 | 9.45 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 22.10 | 10.06 | 8.54 | 8.31 | 20.85 | 22.44 | NAN | 14.24 | 9.45 |'
- en: '| AWQ | 2.5e5 | 2.7e5 | 2.3e5 | 7.4e4 | 2.1e5 | 1.2e5 | - |  |  |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 2.5e5 | 2.7e5 | 2.3e5 | 7.4e4 | 2.1e5 | 1.2e5 | - |  |  |'
- en: '| OmniQuant | 8.90 | 7.34 | 6.59 | 5.65 | 9.62 | 7.56 | 6.11 |  |  |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 8.90 | 7.34 | 6.59 | 5.65 | 9.62 | 7.56 | 6.11 |  |  |'
- en: '| GPTVQ 1D (ours) | 16.64 | 6.78 | 5.97 | 5.05 | 10.08 | 7.17 | 4.82 | 9.56
    |  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 1D (ours) | 16.64 | 6.78 | 5.97 | 5.05 | 10.08 | 7.17 | 4.82 | 9.56
    |  |'
- en: '| GPTVQ 2D (ours) | 9.90 | 6.43 | 5.56 | 4.86 | 7.97 | 6.47 | 4.61 | 10.11
    |  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 2D (ours) | 9.90 | 6.43 | 5.56 | 4.86 | 7.97 | 6.47 | 4.61 | 10.11
    |  |'
- en: '|  | GPTVQ 4D (ours) | 8.76 | 6.33 | 5.42 | 4.74 | 7.22 | 6.08 | 4.39 | 7.16
    |  |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '|  | GPTVQ 4D (ours) | 8.76 | 6.33 | 5.42 | 4.74 | 7.22 | 6.08 | 4.39 | 7.16
    |  |'
- en: '| 3.125 bpv (W3@g128) | RTN | 7.01 | 5.88 | 4.87 | 4.24 | 6.66 | 5.51 | 3.97
    | 6.15 | 5.18 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 3.125 bpv (W3@g128) | RTN | 7.01 | 5.88 | 4.87 | 4.24 | 6.66 | 5.51 | 3.97
    | 6.15 | 5.18 |'
- en: '| GPTQ | 6.55 | 5.62 | 4.80 | 4.17 | 6.29 | 5.42 | 3.85 | 5.83 | 4.75 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 6.55 | 5.62 | 4.80 | 4.17 | 6.29 | 5.42 | 3.85 | 5.83 | 4.75 |'
- en: '| AWQ | 6.46 | 5.51 | 4.63 | 3.99 | 6.24 | 5.32 | - |  |  |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 6.46 | 5.51 | 4.63 | 3.99 | 6.24 | 5.32 | - |  |  |'
- en: '| OmniQuant | 6.15 | 5.44 | 4.56 | 3.94 | 6.03 | 5.28 | 3.78 |  |  |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 6.15 | 5.44 | 4.56 | 3.94 | 6.03 | 5.28 | 3.78 |  |  |'
- en: '| GPTVQ 1D (ours) | 6.60 | 5.34 | 4.48 | 3.85 | 5.97 | 5.17 | 3.62 | 5.76 |  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 1D (ours) | 6.60 | 5.34 | 4.48 | 3.85 | 5.97 | 5.17 | 3.62 | 5.76 |  |'
- en: '| GPTVQ 2D (ours) | 6.32 | 5.31 | 4.38 | 3.79 | 5.82 | 5.10 | 3.55 | 5.51 |  |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 2D (ours) | 6.32 | 5.31 | 4.38 | 3.79 | 5.82 | 5.10 | 3.55 | 5.51 |  |'
- en: '| 4.125 bpv (W4@g128) | RTN | 5.96 | 5.25 | 4.23 | 3.67 | 5.72 | 4.98 | 3.46
    | 5.42 | 4.14 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 4.125 bpv (W4@g128) | RTN | 5.96 | 5.25 | 4.23 | 3.67 | 5.72 | 4.98 | 3.46
    | 5.42 | 4.14 |'
- en: '| GPTQ | 5.85 | 5.20 | 4.23 | 3.65 | 5.61 | 4.98 | 3.42 | 5.35 | 4.12 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 5.85 | 5.20 | 4.23 | 3.65 | 5.61 | 4.98 | 3.42 | 5.35 | 4.12 |'
- en: '| AWQ | 5.81 | 5.20 | 4.21 | 3.62 | 5.62 | 4.97 | - |  |  |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| AWQ | 5.81 | 5.20 | 4.21 | 3.62 | 5.62 | 4.97 | - |  |  |'
- en: '| OmniQuant | 5.77 | 5.17 | 4.19 | 3.62 | 5.58 | 4.95 | 3.40 |  |  |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| OmniQuant | 5.77 | 5.17 | 4.19 | 3.62 | 5.58 | 4.95 | 3.40 |  |  |'
- en: '| GPTVQ 1D (ours) | 5.96 | 5.15 | 4.18 | 3.60 | 5.61 | 4.95 | 3.38 | 5.32 |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 1D (ours) | 5.96 | 5.15 | 4.18 | 3.60 | 5.61 | 4.95 | 3.38 | 5.32 |  |'
- en: '| GPTVQ 2D (ours) | 5.94 | 5.20 | 4.18 | 3.64 | 5.68 | 4.97 | 3.39 | 5.44 |  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| GPTVQ 2D (ours) | 5.94 | 5.20 | 4.18 | 3.64 | 5.68 | 4.97 | 3.39 | 5.44 |  |'
- en: 'Table 5: LM-eval results of quantized Llama-v2 7B and 13B, Mistral-7B and Mixtral-8x7B
    models.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 'Table 5: LM-eval 量化 Llama-v2 7B 和 13B，Mistral-7B 和 Mixtral-8x7B 模型的结果。'
- en: '| $\uparrow$ | #Bits | Method | PIQA | ARC-e | Arc-c | BoolQ | HellaSwag |
    Winogrande | Avg. |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| $\uparrow$ | #Bits | 方法 | PIQA | ARC-e | Arc-c | BoolQ | HellaSwag | Winogrande
    | 平均值 |'
- en: '| Llama-v2-7B | FP16 | 79.11 | 74.58 | 46.25 | 77.74 | 75.99 | 69.14 | 70.47
    |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| Llama-v2-7B | FP16 | 79.11 | 74.58 | 46.25 | 77.74 | 75.99 | 69.14 | 70.47
    |'
- en: '| 2.125 bpv (W2@g128) | RTN | 51.09 | 27.95 | 25.00 | 41.13 | 26.57 | 49.88
    | 36.94 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 2.125 bpv (W2@g128) | RTN | 51.09 | 27.95 | 25.00 | 41.13 | 26.57 | 49.88
    | 36.94 |'
- en: '| GPTQ | 54.84 | 30.64 | 25.09 | 53.43 | 33.09 | 51.54 | 41.44 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 54.84 | 30.64 | 25.09 | 53.43 | 33.09 | 51.54 | 41.44 |'
- en: '| VQ-1D | 62.95 | 40.28 | 22.61 | 61.90 | 44.63 | 52.72 | 47.51 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 62.95 | 40.28 | 22.61 | 61.90 | 44.63 | 52.72 | 47.51 |'
- en: '| VQ-2D | 70.73 | 58.08 | 31.48 | 63.73 | 58.49 | 60.93 | 57.24 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 70.73 | 58.08 | 31.48 | 63.73 | 58.49 | 60.93 | 57.24 |'
- en: '| 2.25 bpv (W2@g64) | RTN | 58.76 | 36.66 | 24.83 | 41.87 | 40.38 | 51.93 |
    42.40 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 2.25 bpv (W2@g64) | RTN | 58.76 | 36.66 | 24.83 | 41.87 | 40.38 | 51.93 |
    42.40 |'
- en: '| GPTQ | 60.83 | 39.02 | 25.17 | 59.33 | 45.82 | 55.49 | 47.61 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 60.83 | 39.02 | 25.17 | 59.33 | 45.82 | 55.49 | 47.61 |'
- en: '| VQ-1D | 66.54 | 45.62 | 26.88 | 64.95 | 50.89 | 56.83 | 51.95 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 66.54 | 45.62 | 26.88 | 64.95 | 50.89 | 56.83 | 51.95 |'
- en: '| VQ-2D | 70.40 | 58.92 | 32.25 | 70.09 | 59.80 | 62.98 | 59.08 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 70.40 | 58.92 | 32.25 | 70.09 | 59.80 | 62.98 | 59.08 |'
- en: '| VQ-4D | 73.29 | 63.43 | 35.92 | 66.33 | 63.89 | 66.06 | 61.49 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| VQ-4D | 73.29 | 63.43 | 35.92 | 66.33 | 63.89 | 66.06 | 61.49 |'
- en: '| 3.125 bpv (W3@g128) | RTN | 76.77 | 70.50 | 42.92 | 71.71 | 73.96 | 67.64
    | 67.25 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 3.125 bpv (W3@g128) | RTN | 76.77 | 70.50 | 42.92 | 71.71 | 73.96 | 67.64
    | 67.25 |'
- en: '| GPTQ | 77.37 | 68.14 | 40.70 | 71.04 | 72.50 | 67.25 | 66.16 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 77.37 | 68.14 | 40.70 | 71.04 | 72.50 | 67.25 | 66.16 |'
- en: '| VQ-1D | 77.64 | 70.12 | 42.15 | 75.90 | 71.42 | 68.43 | 67.61 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 77.64 | 70.12 | 42.15 | 75.90 | 71.42 | 68.43 | 67.61 |'
- en: '| VQ-2D | 77.64 | 72.73 | 43.69 | 71.65 | 72.71 | 67.64 | 67.68 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 77.64 | 72.73 | 43.69 | 71.65 | 72.71 | 67.64 | 67.68 |'
- en: '| Llama-v2-13B | FP16 | 80.52 | 77.53 | 49.23 | 80.52 | 79.38 | 72.14 | 73.22
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| Llama-v2-13B | FP16 | 80.52 | 77.53 | 49.23 | 80.52 | 79.38 | 72.14 | 73.22
    |'
- en: '| 2.125 bpv (W2@g128) | RTN | 58.43 | 32.32 | 25.51 | 47.86 | 39.40 | 48.86
    | 42.06 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 2.125 bpv (W2@g128) | RTN | 58.43 | 32.32 | 25.51 | 47.86 | 39.40 | 48.86
    | 42.06 |'
- en: '| GPTQ | 59.52 | 40.15 | 27.65 | 57.06 | 41.56 | 53.43 | 46.56 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 59.52 | 40.15 | 27.65 | 57.06 | 41.56 | 53.43 | 46.56 |'
- en: '| VQ-1D | 72.74 | 63.85 | 35.75 | 65.54 | 61.60 | 65.43 | 60.82 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 72.74 | 63.85 | 35.75 | 65.54 | 61.60 | 65.43 | 60.82 |'
- en: '| VQ-2D | 75.19 | 68.27 | 39.51 | 70.67 | 65.66 | 67.48 | 64.46 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 75.19 | 68.27 | 39.51 | 70.67 | 65.66 | 67.48 | 64.46 |'
- en: '| 2.25 bpv (W2@g64) | RTN | 61.59 | 41.58 | 25.43 | 49.79 | 48.24 | 51.85 |
    46.41 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 2.25 bpv (W2@g64) | RTN | 61.59 | 41.58 | 25.43 | 49.79 | 48.24 | 51.85 |
    46.41 |'
- en: '| GPTQ | 70.13 | 56.65 | 31.57 | 51.10 | 56.62 | 58.88 | 54.16 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 70.13 | 56.65 | 31.57 | 51.10 | 56.62 | 58.88 | 54.16 |'
- en: '| VQ-1D | 72.91 | 65.32 | 36.86 | 66.48 | 62.19 | 65.11 | 61.48 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 72.91 | 65.32 | 36.86 | 66.48 | 62.19 | 65.11 | 61.48 |'
- en: '| VQ-2D | 74.97 | 66.92 | 39.51 | 70.95 | 67.36 | 69.38 | 64.85 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 74.97 | 66.92 | 39.51 | 70.95 | 67.36 | 69.38 | 64.85 |'
- en: '| VQ-4D | 76.17 | 71.89 | 43.26 | 67.55 | 69.97 | 68.19 | 66.17 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| VQ-4D | 76.17 | 71.89 | 43.26 | 67.55 | 69.97 | 68.19 | 66.17 |'
- en: '| 3.125 bpv (W3@g128) | RTN | 78.89 | 74.28 | 46.76 | 77.25 | 76.51 | 70.80
    | 70.75 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 3.125 bpv (W3@g128) | RTN | 78.89 | 74.28 | 46.76 | 77.25 | 76.51 | 70.80
    | 70.75 |'
- en: '| GPTQ | 79.33 | 75.84 | 47.01 | 78.90 | 77.16 | 70.40 | 71.44 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 79.33 | 75.84 | 47.01 | 78.90 | 77.16 | 70.40 | 71.44 |'
- en: '| VQ-1D | 78.78 | 75.55 | 47.35 | 79.36 | 76.57 | 71.90 | 71.59 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 78.78 | 75.55 | 47.35 | 79.36 | 76.57 | 71.90 | 71.59 |'
- en: '| VQ-2D | 79.43 | 75.29 | 48.12 | 78.99 | 76.96 | 71.74 | 71.76 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 79.43 | 75.29 | 48.12 | 78.99 | 76.96 | 71.74 | 71.76 |'
- en: '| Mistral-7B | FP16 | 82.10 | 79.59 | 53.92 | 83.58 | 81.07 | 73.88 | 75.69
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| Mistral-7B | FP16 | 82.10 | 79.59 | 53.92 | 83.58 | 81.07 | 73.88 | 75.69
    |'
- en: '| 2.125 bpv (W2@g128) | RTN | 53.05 | 29.42 | 26.62 | 38.56 | 29.26 | 49.57
    | 37.75 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 2.125 bpv (W2@g128) | RTN | 53.05 | 29.42 | 26.62 | 38.56 | 29.26 | 49.57
    | 37.75 |'
- en: '| GPTQ | 57.73 | 35.65 | 26.62 | 46.06 | 36.06 | 49.49 | 41.93 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 57.73 | 35.65 | 26.62 | 46.06 | 36.06 | 49.49 | 41.93 |'
- en: '| VQ-1D | 58.71 | 38.85 | 23.89 | 59.51 | 37.40 | 50.83 | 44.86 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 58.71 | 38.85 | 23.89 | 59.51 | 37.40 | 50.83 | 44.86 |'
- en: '| VQ-2D | 69.10 | 59.64 | 34.22 | 68.99 | 55.07 | 56.51 | 57.25 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 69.10 | 59.64 | 34.22 | 68.99 | 55.07 | 56.51 | 57.25 |'
- en: '| 2.25 bpv (W2@g64) | RTN | 60.72 | 38.47 | 27.56 | 44.83 | 46.10 | 51.07 |
    44.79 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 2.25 bpv (W2@g64) | RTN | 60.72 | 38.47 | 27.56 | 44.83 | 46.10 | 51.07 |
    44.79 |'
- en: '| GPTQ | 65.83 | 46.21 | 30.20 | 62.11 | 50.64 | 55.56 | 51.76 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 65.83 | 46.21 | 30.20 | 62.11 | 50.64 | 55.56 | 51.76 |'
- en: '| VQ-1D | 66.27 | 57.58 | 33.53 | 70.58 | 51.53 | 55.41 | 55.82 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 66.27 | 57.58 | 33.53 | 70.58 | 51.53 | 55.41 | 55.82 |'
- en: '| VQ-2D | 68.01 | 59.85 | 33.53 | 66.06 | 51.40 | 58.01 | 56.14 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 68.01 | 59.85 | 33.53 | 66.06 | 51.40 | 58.01 | 56.14 |'
- en: '| VQ-4D | 72.80 | 69.28 | 40.02 | 73.03 | 65.00 | 66.54 | 64.44 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| VQ-4D | 72.80 | 69.28 | 40.02 | 73.03 | 65.00 | 66.54 | 64.44 |'
- en: '| 3.125 bpv (W3@g128) | RTN | 80.79 | 74.62 | 48.46 | 80.00 | 78.66 | 68.19
    | 71.79 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 3.125 bpv (W3@g128) | RTN | 80.79 | 74.62 | 48.46 | 80.00 | 78.66 | 68.19
    | 71.79 |'
- en: '| GPTQ | 79.82 | 75.51 | 49.40 | 81.22 | 77.34 | 70.17 | 72.24 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 79.82 | 75.51 | 49.40 | 81.22 | 77.34 | 70.17 | 72.24 |'
- en: '| VQ-1D | 79.76 | 75.04 | 47.53 | 79.69 | 75.91 | 71.43 | 71.56 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 79.76 | 75.04 | 47.53 | 79.69 | 75.91 | 71.43 | 71.56 |'
- en: '| VQ-2D | 80.41 | 77.23 | 49.57 | 82.72 | 78.52 | 72.93 | 73.56 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 80.41 | 77.23 | 49.57 | 82.72 | 78.52 | 72.93 | 73.56 |'
- en: '| Mixtral-8x7B | FP16 | 83.46 | 73.74 | 55.89 | 84.74 | 82.45 | 75.30 | 75.93
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| Mixtral-8x7B | FP16 | 83.46 | 73.74 | 55.89 | 84.74 | 82.45 | 75.30 | 75.93
    |'
- en: '| 2.125 bpv (W2@g128) | RTN | 51.90 | 27.27 | 25.85 | 47.98 | 27.07 | 49.64
    | 38.29 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 2.125 bpv (W2@g128) | RTN | 51.90 | 27.27 | 25.85 | 47.98 | 27.07 | 49.64
    | 38.29 |'
- en: '| GPTQ | 59.79 | 35.44 | 27.30 | 52.08 | 41.80 | 50.83 | 44.54 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 59.79 | 35.44 | 27.30 | 52.08 | 41.80 | 50.83 | 44.54 |'
- en: '| VQ-1D | 71.00 | 52.53 | 33.96 | 68.75 | 57.30 | 61.72 | 57.54 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 71.00 | 52.53 | 33.96 | 68.75 | 57.30 | 61.72 | 57.54 |'
- en: '| VQ-2D | 76.17 | 59.43 | 41.72 | 75.08 | 66.62 | 67.96 | 64.50 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 76.17 | 59.43 | 41.72 | 75.08 | 66.62 | 67.96 | 64.50 |'
- en: '| 2.25 bpv (W2@g64) | RTN | 62.08 | 38.68 | 28.41 | 54.46 | 44.40 | 53.12 |
    46.86 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 2.25 bpv (W2@g64) | RTN | 62.08 | 38.68 | 28.41 | 54.46 | 44.40 | 53.12 |
    46.86 |'
- en: '| GPTQ | 66.05 | 42.93 | 28.58 | 50.12 | 49.59 | 55.41 | 48.78 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 66.05 | 42.93 | 28.58 | 50.12 | 49.59 | 55.41 | 48.78 |'
- en: '| VQ-1D | 73.12 | 53.41 | 34.64 | 63.64 | 56.25 | 61.64 | 57.12 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 73.12 | 53.41 | 34.64 | 63.64 | 56.25 | 61.64 | 57.12 |'
- en: '| VQ-2D | 75.52 | 58.42 | 39.42 | 73.94 | 68.05 | 68.19 | 63.92 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 75.52 | 58.42 | 39.42 | 73.94 | 68.05 | 68.19 | 63.92 |'
- en: '| VQ-4D | 76.55 | 61.74 | 43.77 | 73.67 | 70.09 | 72.77 | 66.43 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| VQ-4D | 76.55 | 61.74 | 43.77 | 73.67 | 70.09 | 72.77 | 66.43 |'
- en: '| 3.125 bpv (W3@g128) | RTN | 81.50 | 68.77 | 50.60 | 80.92 | 79.71 | 72.93
    | 72.40 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 3.125 bpv (W3@g128) | RTN | 81.50 | 68.77 | 50.60 | 80.92 | 79.71 | 72.93
    | 72.40 |'
- en: '| GPTQ | 80.85 | 69.32 | 52.05 | 81.35 | 78.40 | 74.43 | 72.73 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| GPTQ | 80.85 | 69.32 | 52.05 | 81.35 | 78.40 | 74.43 | 72.73 |'
- en: '| VQ-1D | 81.45 | 68.60 | 50.17 | 84.71 | 76.92 | 74.66 | 72.75 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| VQ-1D | 81.45 | 68.60 | 50.17 | 84.71 | 76.92 | 74.66 | 72.75 |'
- en: '| VQ-2D | 82.48 | 71.93 | 53.16 | 84.92 | 79.80 | 73.88 | 74.36 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| VQ-2D | 82.48 | 71.93 | 53.16 | 84.92 | 79.80 | 73.88 | 74.36 |'
- en: Appendix B Tables for hyperparameter ablations
  id: totrans-313
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B 超参数消融的表格
- en: 'Table 6: Effect of EM initialization. Setting used: Llamav2-7B, 2D 3-bit VQ,
    blocksize 2048.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：EM 初始化的效果。使用设置：Llamav2-7B，2D 3-bit VQ，块大小 2048。
- en: '| Lookup method | BPV | Setting | PPL | Time (s) |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 查找方法 | BPV | 设置 | PPL | 时间（秒） |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1D 3B 1024 | 3.125 | Mahalanobis | 6.05 | 605 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 1D 3B 1024 | 3.125 | Mahalanobis | 6.05 | 605 |'
- en: '| K++ | 6.16 | 3328 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| K++ | 6.16 | 3328 |'
- en: '| 2D 3B 16384 | 3.125 | Mahalanobis | 5.65 | 756 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| 2D 3B 16384 | 3.125 | Mahalanobis | 5.65 | 756 |'
- en: '| K++ | 5.63 | 3168 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| K++ | 5.63 | 3168 |'
- en: '| 1D 4B 2048 | 4.125 | Mahalanobis | 5.86 | 1272 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| 1D 4B 2048 | 4.125 | Mahalanobis | 5.86 | 1272 |'
- en: '| K++ | 5.88 | 2116 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| K++ | 5.88 | 2116 |'
- en: '| 2D 4B 65536 | 4.125 | Mahalanobis | 5.59 | 3816 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 2D 4B 65536 | 4.125 | Mahalanobis | 5.59 | 3816 |'
- en: '| K++ | 5.57 | 6644 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| K++ | 5.57 | 6644 |'
- en: 'Table 7: Effect of number of EM interations. Setting used: BLOOM-560m 2D 3-bit
    VQ with blocksize 4096, perplexity on WikiText2 test set.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：EM 迭代次数的效果。使用设置：BLOOM-560m 2D 3-bit VQ，块大小 4096，WikiText2 测试集上的困惑度。
- en: '| EM iterations | PPL |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| EM 迭代次数 | PPL |'
- en: '| --- | --- |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 10 | 24.49 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 24.49 |'
- en: '| 30 | 24.18 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 24.18 |'
- en: '| 50 | 24.12 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 24.12 |'
- en: '| 75 | 24.11 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| 75 | 24.11 |'
- en: '| 100 | 24.09 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 24.09 |'
- en: 'Table 8: Choices in experimental setup leading to comparable bits per value.
    $d$: VQ bitwidth per dimension; gs: block size; Q: 8-bit codebook quantization
    yes/no; SVD: codebook SVD yes/no. BPV: bits per value.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8：实验设置中的选择导致每值比特数相当。$d$: 每个维度的 VQ 位宽；gs: 块大小；Q: 8 位字典量化 是/否；SVD: 字典 SVD 是/否。BPV:
    每值比特数。'
- en: '| $d$ | gs | Q | SVD | BPV | PPL |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| $d$ | gs | Q | SVD | BPV | PPL |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 2 | 512 | N | N | 2.125 | 14.01 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 512 | N | N | 2.125 | 14.01 |'
- en: '| 256 | Y | N | 2.125 | 11.57 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 256 | Y | N | 2.125 | 11.57 |'
- en: '| 256 | N | Y | 2.125 | 44.99 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| 256 | N | Y | 2.125 | 44.99 |'
- en: '| 3 | 1024 | N | N | 3.125 | 6.01 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1024 | N | N | 3.125 | 6.01 |'
- en: '| 512 | Y | N | 3.125 | 5.98 |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 512 | Y | N | 3.125 | 5.98 |'
- en: '| 512 | N | Y | 3.125 | 5.98 |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 512 | N | Y | 3.125 | 5.98 |'
- en: '| 2 | 2 | 4096 | N | N | 2.125 | 8.37 |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 4096 | N | N | 2.125 | 8.37 |'
- en: '| 2048 | Y | N | 2.125 | 8.23 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 2048 | Y | N | 2.125 | 8.23 |'
- en: '| 3 | 16384 | N | N | 3.125 | 5.93 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 16384 | N | N | 3.125 | 5.93 |'
- en: '| 8192 | Y | N | 3.125 | 5.87 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 8192 | Y | N | 3.125 | 5.87 |'
- en: 'Table 9: Effect of codebook fine-tuning on final PPL for Llamav2-7B.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：对 Llamav2-7B 的最终 PPL 的字典微调效果。
- en: '| $d$ | gs | Update | PPL | Runtime (s) |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| $d$ | gs | 更新 | PPL | 运行时间（秒） |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 2 | 512 | N | 43.14 | 625 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 512 | N | 43.14 | 625 |'
- en: '| Y | 14.02 | 1857 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| Y | 14.02 | 1857 |'
- en: '| 3 | 1024 | N | 6.05 | 712 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1024 | N | 6.05 | 712 |'
- en: '| Y | 6.01 | 1916 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Y | 6.01 | 1916 |'
- en: '| 2 | 2 | 2048 | N | 8.64 | 723 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 2048 | N | 8.64 | 723 |'
- en: '| Y | 8.21 | 1335 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| Y | 8.21 | 1335 |'
- en: '| 3 | 8192 | N | 5.93 | 1585 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 8192 | N | 5.93 | 1585 |'
- en: '| Y | 5.88 | 2195 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| Y | 5.88 | 2195 |'
- en: 'Table 10: Effect of scaling block size on perplexity for Llamav2-7B. $d$: VQ
    bitwidth per dimension; gs: block size; Codebooks are quantized to 8 bits.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '表 10：Llamav2-7B 的块大小缩放对困惑度的影响。$d$: 每个维度的 VQ 位宽；gs: 块大小；字典量化为 8 位。'
- en: '| $d$ | gs | Scaling BS |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| $d$ | gs | 缩放 BS |'
- en: '| --- | --- | --- | --- |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '|  |  |  | None | 128 | 64 | 32 | 16 | 8 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  | 无 | 128 | 64 | 32 | 16 | 8 |'
- en: '| 1 | 2 | 512 | 14.01 | 16.74 | 2744.9 | 480.8 | 15.36 | 13.79 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 512 | 14.01 | 16.74 | 2744.9 | 480.8 | 15.36 | 13.79 |'
- en: '| 3 | 1024 | 6.02 | 5.97 | 6.00 | 5.87 | 5.82 | 5.72 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1024 | 6.02 | 5.97 | 6.00 | 5.87 | 5.82 | 5.72 |'
- en: '| 2 | 2 | 2048 | 8.23 | 8.38 | 8.04 | 7.97 | 7.56 | 6.89 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 2048 | 8.23 | 8.38 | 8.04 | 7.97 | 7.56 | 6.89 |'
- en: '| 3 | 8192 | 5.91 | 5.82 | 5.78 | 5.73 | 5.74 | 5.66 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 8192 | 5.91 | 5.82 | 5.78 | 5.73 | 5.74 | 5.66 |'
- en: 'Table 11: Effect of scaling on perplexity for different models. Configurations
    with equal overhead with or without the scaling are considered. $d$: VQ bitwidth
    per dimension; gs: block size; Codebooks are assumed to be quantized to 8 bit.'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '表 11：不同模型的困惑度与缩放效果。考虑了有无缩放的相等开销配置。$d$: 每个维度的 VQ 位宽；gs: 块大小；假设字典被量化为 8 位。'
- en: '| $d$ | gs | Scale | Llamav2-7B | Llamav2-13B | Mistral-7B | Mixtral-8x7B |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| $d$ | gs | 缩放 | Llamav2-7B | Llamav2-13B | Mistral-7B | Mixtral-8x7B |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 1 | 2 | 256 | N | 14.01 | 7.34 | 15.03 | 8.56 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 256 | N | 14.01 | 7.34 | 15.03 | 8.56 |'
- en: '| 512 | Y | 171.29 | 7.44 | 87.60 | 8.11 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 512 | Y | 171.29 | 7.44 | 87.60 | 8.11 |'
- en: '| 3 | 512 | N | 5.98 | 5.21 | 5.76 | 4.60 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 512 | N | 5.98 | 5.21 | 5.76 | 4.60 |'
- en: '| 1024 | Y | 6.01 | 5.17 | 5.77 | 4.59 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| 1024 | Y | 6.01 | 5.17 | 5.77 | 4.59 |'
- en: '| 2 | 2 | 2048 | N | 8.23 | 6.69 | 10.98 | 6.73 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 2048 | N | 8.23 | 6.69 | 10.98 | 6.73 |'
- en: '| 4096 | Y | 8.49 | 6.50 | 10.28 | 6.37 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 4096 | Y | 8.49 | 6.50 | 10.28 | 6.37 |'
- en: '| 3 | 8192 | N | 5.91 | 5.19 | 8.63 | 4.52 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 8192 | N | 5.91 | 5.19 | 8.63 | 4.52 |'
- en: '| 16384 | Y | 5.56 | 5.11 | 5.53 | 4.30 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 16384 | Y | 5.56 | 5.11 | 5.53 | 4.30 |'
