- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-08 18:53:24'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-08 18:53:24
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'GEAR: 一种高效的 KV 缓存压缩方法，用于近乎无损的 LLM 生成推断'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05527](https://ar5iv.labs.arxiv.org/html/2403.05527)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.05527](https://ar5iv.labs.arxiv.org/html/2403.05527)
- en: Hao Kang^∗, Qingru Zhang^∗, Souvik Kundu, Geonhwa Jeong,
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Hao Kang^∗、Qingru Zhang^∗、Souvik Kundu、Geonhwa Jeong，
- en: Zaoxing Liu, Tushar Krishna, Tuo Zhao^†
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zaoxing Liu、Tushar Krishna、Tuo Zhao^†
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Key-value (KV) caching has become the de-facto to accelerate generation speed
    for large language models (LLMs) inference. However, the growing cache demand
    with increasing sequence length has transformed LLM inference to be a memory bound
    problem, significantly constraining the system throughput. Existing methods rely
    on dropping unimportant tokens or quantizing all entries uniformly. Such methods,
    however, often incur high approximation errors to represent the compressed matrices.
    The autoregressive decoding process further compounds the error of each step,
    resulting in critical deviation in model generation and deterioration of performance.
    To tackle this challenge, we propose GEAR, an efficient KV cache compression framework
    that achieves near-lossless high-ratio compression. GEAR first applies quantization
    to majority of entries of similar magnitudes to ultra-low precision. It then employs
    a low-rank matrix to approximate the quantization error, and a sparse matrix to
    remedy individual errors from outlier entries. By adeptly integrating three techniques,
    GEAR is able to fully exploit their synergistic potentials. Our experiments demonstrate
    that compared to alternatives, GEAR achieves near-lossless 4-bit KV cache compression
    with up to $2.38\times$. Our code is publicly available at [https://github.com/HaoKang-Timmy/GEAR](https://github.com/HaoKang-Timmy/GEAR).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 键值（KV）缓存已成为加速大型语言模型（LLMs）推断生成速度的事实标准。然而，随着序列长度的增加，缓存需求的增长已将 LLM 推断转变为一个内存受限的问题，显著限制了系统吞吐量。现有方法依赖于丢弃不重要的标记或对所有条目进行均匀量化。然而，这些方法通常会导致高逼近误差，从而难以有效表示压缩矩阵。自回归解码过程进一步加剧了每一步的误差，导致模型生成出现严重偏差，并性能下降。为了解决这一挑战，我们提出了
    GEAR，这是一种高效的 KV 缓存压缩框架，能够实现近乎无损的高压缩比。GEAR 首先将大多数相似幅度的条目量化为超低精度。然后，它利用低秩矩阵来近似量化误差，并使用稀疏矩阵来修正来自异常值条目的个别误差。通过巧妙地整合这三种技术，GEAR
    能够充分发挥它们的协同效应。我们的实验表明，与其他方法相比，GEAR 实现了近乎无损的 4 位 KV 缓存压缩，压缩比高达 $2.38\times$。我们的代码公开在
    [https://github.com/HaoKang-Timmy/GEAR](https://github.com/HaoKang-Timmy/GEAR)。
- en: '^($\dagger$)^($\dagger$)footnotetext: Hao Kang, Qingru Zhang, Geonhwa Jeong,
    Tushar Krishna, and Tuo Zhao are affiliated with Georgia Tech. Souvik Kundu is
    affiliated with Intel. Zaoxing Liu is affiliated with the University of Maryland.
    Correspondence to [hkang342@gatech.edu](hkang342@gatech.edu), [qingru.zhang@gatech.edu](qingru.zhang@gatech.edu),
    [souvikk.kundu@intel.com](souvikk.kundu@intel.com), and [tourzhao@gatech.edu](tourzhao@gatech.edu).^*^*footnotetext:
    Equal contributions'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ^($\dagger$)^($\dagger$)脚注：Hao Kang、Qingru Zhang、Geonhwa Jeong、Tushar Krishna
    和 Tuo Zhao 隶属于乔治亚理工学院。Souvik Kundu 隶属于英特尔。Zaoxing Liu 隶属于马里兰大学。通讯请联系 [hkang342@gatech.edu](hkang342@gatech.edu)、[qingru.zhang@gatech.edu](qingru.zhang@gatech.edu)、[souvikk.kundu@intel.com](souvikk.kundu@intel.com)
    和 [tourzhao@gatech.edu](tourzhao@gatech.edu)。^*^*脚注：同等贡献
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The advances in large language models (LLMs) have marked a significant milestone
    in natural language processing (NLP) and artificial intelligence (AI) (Vaswani
    et al., [2017](#bib.bib28); Brown et al., [2020a](#bib.bib2); OpenAI, [2023](#bib.bib19)).
    Among these, autoregressive language models have attracted extensive attention
    (Brown et al., [2020b](#bib.bib3); Zhang et al., [2022](#bib.bib37); Touvron et al.,
    [2023a](#bib.bib26), [b](#bib.bib27)), showcasing exceptional performances across
    a wide range of applications, such as content creation and dialogue system (Yuan
    et al., [2022](#bib.bib35); Thoppilan et al., [2022](#bib.bib25); Wei et al.,
    [2022](#bib.bib30)). When serving these LLMs for generative inference, KV cache-ing
    has become a routine practice, which stores previously computed Key/Value vectors
    from attention calculation and reuses them for generating current tokens (Pope
    et al., [2022](#bib.bib21)). As such, it avoids intensive recalculations of previous
    tokens when generating each token, significantly improving generation speed.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型（LLMs）的进展标志着自然语言处理（NLP）和人工智能（AI）的一个重要里程碑（Vaswani et al., [2017](#bib.bib28);
    Brown et al., [2020a](#bib.bib2); OpenAI, [2023](#bib.bib19)）。其中，自回归语言模型吸引了广泛关注（Brown
    et al., [2020b](#bib.bib3); Zhang et al., [2022](#bib.bib37); Touvron et al.,
    [2023a](#bib.bib26), [b](#bib.bib27)），在内容创作和对话系统等多个应用中表现出色（Yuan et al., [2022](#bib.bib35);
    Thoppilan et al., [2022](#bib.bib25); Wei et al., [2022](#bib.bib30)）。在为这些 LLM
    提供生成推理服务时，KV 缓存已经成为一种常规做法，它存储从注意力计算中获得的先前计算的键/值向量，并将其用于生成当前的令牌（Pope et al., [2022](#bib.bib21)）。因此，它避免了在生成每个令牌时对先前令牌的重复计算，显著提高了生成速度。
- en: Despite its prominence, the memory consumption of the KV cache grows rapidly
    as the model size and sequence length increase, imposing significant constraints
    on system throughput. For instance, in the case of a 30 billion-parameter model
    with an input length of 1024 and batch size of 128, the resulting KV cache can
    occupy as much as 180GB of memory (Zhang et al., [2023](#bib.bib38)). To alleviate
    this pressure on the limited GPU memory, the inference system resorts to offloading
    (Aminabadi et al., [2022](#bib.bib1); Sheng et al., [2023](#bib.bib23)) – transferring
    the KV cache to CPU memory or NVMe storage. This process, however, can still introduce
    non-trivial overhead due to the limited PCIe bandwidth between GPUs and CPUs on
    many devices. Therefore, it is crucial to reduce the intensive memory footprint
    of the emerging bottleneck of KV cache in generative inference.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其重要性显著，KV 缓存的内存消耗随着模型规模和序列长度的增加而迅速增长，给系统吞吐量带来了重大限制。例如，在一个具有 30 亿参数的模型中，输入长度为
    1024 和批处理大小为 128，结果 KV 缓存可能占用高达 180GB 的内存（Zhang et al., [2023](#bib.bib38)）。为了缓解对有限
    GPU 内存的压力，推理系统采用了卸载（Aminabadi et al., [2022](#bib.bib1); Sheng et al., [2023](#bib.bib23)）——将
    KV 缓存转移到 CPU 内存或 NVMe 存储。然而，由于许多设备上 GPU 和 CPU 之间的 PCIe 带宽有限，这个过程仍然可能引入不容忽视的开销。因此，减少生成推理中
    KV 缓存这一新兴瓶颈的内存占用至关重要。
- en: '![Refer to caption](img/65fe856eb6f813303e3e3e3ee067fafb.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/65fe856eb6f813303e3e3e3ee067fafb.png)'
- en: (a) Approximation error (GSM8k)
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 近似误差 (GSM8k)
- en: '![Refer to caption](img/96af96195c6e2a65d7161a4cea3bcd1f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/96af96195c6e2a65d7161a4cea3bcd1f.png)'
- en: (b) The difference in prediction logits
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 预测 logits 的差异
- en: '![Refer to caption](img/14215bf6d09a6afc1fedf2bfe4991243.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/14215bf6d09a6afc1fedf2bfe4991243.png)'
- en: (c) GSM8k Acc of LLaMA2-7B
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (c) LLaMA2-7B 的 GSM8k 精度
- en: 'Figure 1: ([1(a)](#S1.F1.sf1 "In Figure 1 ‣ 1 Introduction ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM")) compares
    the approximation error when dropping 50% tokens (token dropping) and compressing
    KV caches to 4-bit (other methods) for LLaMA2-7B on GSM8k. ([1(b)](#S1.F1.sf2
    "In Figure 1 ‣ 1 Introduction ‣ GEAR: An Efficient KV Cache Compression Recipe
    for Near-Lossless Generative Inference of LLM")) presents the differnce in prediction
    logits from FP16 baseline after compressing KV caches of an example from GSM8k,
    which indicates the approximation error can be severely compounded along generation
    steps and significantly divert model generations. ([1(c)](#S1.F1.sf3 "In Figure
    1 ‣ 1 Introduction ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM")) shows reducing approximation error can substantially
    improve performance.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '图1: ([1(a)](#S1.F1.sf1 "图1 ‣ 1 引言 ‣ GEAR: 一种用于近无损生成推断的高效KV缓存压缩方法")) 比较了在GSM8k上丢弃50%令牌（令牌丢弃）和将KV缓存压缩到4位（其他方法）时的近似误差。([1(b)](#S1.F1.sf2
    "图1 ‣ 1 引言 ‣ GEAR: 一种用于近无损生成推断的高效KV缓存压缩方法")) 展示了在压缩GSM8k示例的KV缓存后，从FP16基线预测对数的差异，这表明近似误差可能在生成步骤中严重累积，并显著偏离模型生成结果。([1(c)](#S1.F1.sf3
    "图1 ‣ 1 引言 ‣ GEAR: 一种用于近无损生成推断的高效KV缓存压缩方法")) 显示了减少近似误差可以显著提高性能。'
- en: To address this issue, token dropping methods have been proposed to compress
    the cache size while maintaining the generative performance (Zhang et al., [2023](#bib.bib38);
    Liu et al., [2023](#bib.bib15); Ge et al., [2023](#bib.bib9)). These approaches
    harness the sparsity observed in attention patterns to evict embeddings of less
    important tokens from the KV cache while retaining frequently attended ones. For
    example, H[2]O (Zhang et al., [2023](#bib.bib38)) utilizes accumulated attention
    scores as criteria for token importance and effectively reduces the cache size
    by dropping tokens with lower scores. In addition to token dropping, quantization
    is another widely-adopted compression scheme that maps full-precision tensor values
    into discrete levels and store them at lower precision, typically 8 or 4-bits
    (Zafrir et al., [2019](#bib.bib36); Dettmers et al., [2022](#bib.bib6); Sheng
    et al., [2023](#bib.bib23)). For example, FlexGen (Sheng et al., [2023](#bib.bib23))
    employs straightforward uniform and group-wise quantization approachs to compress
    both model weights and KV caches to 4-bit, resulting in a substantial improvement
    in system throughput. However, the full potential of quantization to compress
    the online KV caches with high efficiency and negligible performance loss remains
    largely unexplored.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，已经提出了令牌丢弃方法，以在保持生成性能的同时压缩缓存大小（Zhang et al., [2023](#bib.bib38); Liu
    et al., [2023](#bib.bib15); Ge et al., [2023](#bib.bib9)）。这些方法利用在注意力模式中观察到的稀疏性，从KV缓存中驱逐不重要的令牌嵌入，同时保留频繁被关注的令牌。例如，H[2]O
    (Zhang et al., [2023](#bib.bib38))利用累积的注意力得分作为令牌重要性的标准，并通过丢弃得分较低的令牌有效地减少了缓存大小。除了令牌丢弃，量化是另一种广泛采用的压缩方案，它将全精度张量值映射到离散级别，并以较低的精度（通常为8位或4位）存储它们（Zafrir
    et al., [2019](#bib.bib36); Dettmers et al., [2022](#bib.bib6); Sheng et al.,
    [2023](#bib.bib23)）。例如，FlexGen (Sheng et al., [2023](#bib.bib23))采用了简单的均匀和分组量化方法，将模型权重和KV缓存压缩到4位，从而显著提高了系统吞吐量。然而，量化在高效压缩在线KV缓存和几乎没有性能损失方面的全部潜力仍未被充分探索。
- en: 'The methods mentioned above can effectively compress the cache size while achieving
    lossless performance on natural language understanding tasks like multiple-choice
    QA and text classification or simple summarization task, (e.g., XSum) (Zhang et al.,
    [2023](#bib.bib38)). However, a stark contrast emerges when applying these methods
    to complex generative tasks that require models to generate longer responses or
    involve reasoning, such as mathematical problem-solving (Cobbe et al., [2021](#bib.bib4))
    and chain-of-thought (CoT) reasoning (Wei et al., [2023](#bib.bib31)). Their performance
    dramatically deteriorates under a high compression ratio^*^**We define the compression
    ratio as $\frac{\text{FP16 tensor size}}{\text{Tensor size in compressed format}}$.
    (e.g., 4-bit quantization or dropping 50% tokens, Ge et al. ([2023](#bib.bib9))),
    which is noticeable in both types of methods^*^**Please refer to Section [4](#S4
    "4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM") for our empirical evidence.. This phenomenon can
    be attributed to the non-trivial approximation error induced by them, i.e., difference
    between original KV values and compressed ones. For simple tasks, models are required
    to generate only few tokens where necessary information for correct prediction
    can often be derived from a small set of important contextual tokens. Consequently,
    a relatively large approximation error does not significantly hinder the generation
    of target tokens. In contrast, the complex tasks require models to generate longer
    sequences. The autoregressive decoding process can compound the approximation
    error at every step. Consequently, the negative effect of even a relatively small
    error can be magnified along the generation steps, adversely affecting subsequent
    generation. As an example, Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ GEAR:
    An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference
    of LLM") presents the approximation error of various methods on GSM8k and illustrates
    the deviation in token generations due to the accumulated error, which degenerate
    the accuracy a lot. Moreover, the complex tasks such as CoT reasoning often involve
    densely distributed crucial information within prompts. Models must closely attend
    to most contextual details to generate correct answers. However, a high approximation
    error can often cause models to neglect some crucial details. Similarly, dropping
    a high ratio of tokens renders these information directly invisible, significantly
    impacting the performance. Therefore, the crux of the issue lies in high approximation
    errors of these methods, especially under high compression ratios.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '上述方法可以有效压缩缓存大小，同时在自然语言理解任务（如多项选择问答、文本分类或简单摘要任务，例如XSum）上实现无损性能（Zhang et al.,
    [2023](#bib.bib38)）。然而，当将这些方法应用于需要生成较长响应或涉及推理的复杂生成任务时（如数学问题解决（Cobbe et al., [2021](#bib.bib4)）和连锁思维（CoT）推理（Wei
    et al., [2023](#bib.bib31)）），会出现明显对比。它们的性能在高压缩比下显著下降^*^**我们定义压缩比为 $\frac{\text{FP16
    tensor size}}{\text{Tensor size in compressed format}}$（例如，4-bit量化或丢弃50%标记，Ge
    et al. ([2023](#bib.bib9)))，这一点在这两类方法中均有明显体现^*^**请参见第[4](#S4 "4 Experiments ‣
    GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference
    of LLM")节以获取我们的实证证据。 这一现象可以归因于它们引起的非平凡近似误差，即原始KV值与压缩后的KV值之间的差异。对于简单任务，模型只需生成少量标记，而正确预测所需的信息通常可以从一小组重要的上下文标记中获得。因此，相对较大的近似误差不会显著阻碍目标标记的生成。相比之下，复杂任务需要模型生成较长的序列。自回归解码过程可以在每一步加剧近似误差。因此，即使是相对较小的误差也可能在生成步骤中被放大，从而对后续生成产生不利影响。例如，图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ GEAR: An Efficient KV Cache Compression Recipe for
    Near-Lossless Generative Inference of LLM")展示了各种方法在GSM8k上的近似误差，并说明了由于累积误差导致的标记生成偏差，这极大地降低了准确性。此外，复杂任务如CoT推理通常涉及到密集分布的重要信息。模型必须密切关注大多数上下文细节以生成正确答案。然而，高近似误差常常会导致模型忽略一些关键细节。同样，丢弃高比例的标记使得这些信息直接不可见，严重影响性能。因此，问题的核心在于这些方法在高压缩比下的高近似误差。'
- en: 'To address this challenge, we propose GEAR (GEnerative Inference with Approximation
    Error Reduction), an efficient KV cache compression framework that leverages three
    complementary techniques to decompose KV matrices and adeptly integrate them to
    exploit their full potentials. Generally speaking, our framework consists of three
    compression components: (i) First, we apply the uniform quantization to efficiently
    compress the majority (e.g., 98%) of entries of similar magnitudes to as low as
    4-bit precision. (ii) Then, we employ a low-rank matrix to efficiently approximate
    the quantization residuals. (iii) Finally, we introduce a sparse matrix consisting
    of a negligible ratio of entries with large magnitudes to remedy the individual
    errors caused by these outliers. Such a composite approximation decouples the
    coherent parts from incoherent parts of the approximation error: the low-rank
    matrix captures the majority of coherent basis of quantization error while the
    sparse matrix rectifies the incoherency existing in individual outliers. As such,
    GEAR can effectively reduce the approximation error in a highly efficient manner,
    and hence achieve near-lossless performance on both complex and simple tasks especially
    under high compression ratios. Importantly, we find that using all three components
    is necessary for GEAR to achieve good performance. This suggests that three components
    are complementary with each other and each of them is indispensable for GEAR to
    achieve the near-lossless performance. Our method also relates to studies on weight
    quantization, which we further discuss in Section [5](#S5 "5 Related Work ‣ GEAR:
    An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference
    of LLM").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '为解决这一挑战，我们提出了 GEAR（GEnerative Inference with Approximation Error Reduction），这是一个高效的
    KV 缓存压缩框架，利用三种互补技术来分解 KV 矩阵，并巧妙地将其整合，以发挥其全部潜力。一般而言，我们的框架包含三个压缩组件：（i）首先，我们应用均匀量化，将大多数（例如，98%）相似幅度的条目高效地压缩到最低
    4 位精度。（ii）然后，我们使用低秩矩阵来高效地近似量化残差。（iii）最后，我们引入了一个稀疏矩阵，该矩阵包含比例极小的大幅度条目，以补救这些异常值所导致的个别误差。这种复合近似方法将近似误差的连贯部分与不连贯部分解耦：低秩矩阵捕捉了大多数连贯的量化误差基础，而稀疏矩阵则修正了存在于个别异常值中的不连贯性。因此，GEAR
    可以以非常高效的方式有效地减少近似误差，从而在复杂和简单任务中实现近乎无损的性能，尤其是在高压缩比下。重要的是，我们发现使用所有三个组件对于 GEAR 实现良好性能是必要的。这表明这三种组件是互补的，每一种都不可或缺，以使
    GEAR 实现近乎无损的性能。我们的方法也与权重量化的研究相关，我们将在第[5](#S5 "5 Related Work ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM")节中进一步讨论。'
- en: Additionally, as calculations of sparse and low-rank matrices incur extra latency,
    we incorporate a streaming strategy for GEAR to improve its generative inference.
    Specifically, when generating long sequences, we store KV vectors of newly generated
    tokens to a small buffer (e.g., buffer size $n_{b}=20$ additional memory.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于稀疏和低秩矩阵的计算会带来额外的延迟，我们为 GEAR 采用了流式处理策略，以改善其生成推理。具体而言，在生成长序列时，我们将新生成的 token
    的 KV 向量存储到一个小缓冲区（例如，缓冲区大小 $n_{b}=20$）中，以节省额外的内存。
- en: We conduct experiments on diverse tasks and models to demonstrate the effectiveness
    of GEAR. Specifically, we evaluate both CoT and zero-shot performance using LLaMA2-7B,
    LLaMA2-13B (Touvron et al., [2023b](#bib.bib27)), and Mistral-7B (Jiang et al.,
    [2023](#bib.bib12)) on generative tasks including mathematical reasoning (GSM8k, Cobbe
    et al. ([2021](#bib.bib4))), multitask language understanding (MMLU, Hendrycks
    et al. ([2021](#bib.bib10))), and symbolic reasoning (BigBench Hard, Suzgun et al.
    ([2022](#bib.bib24))). We show that GEAR consistently outperforms the baseline
    methods, especially under high compression ratios. For example, when compressing
    the KV cache to 28% of its FP16 size for LLaMA2-7B on GSM8k datasets with CoT
    prompts, GEAR achieves a remarkable 4.5% improvement in accuracy over the best-performing
    baseline. Notably, we are the first to achieve 4-bit KV cache compression with
    near-lossless performance on both complex and simple generative tasks. Regarding
    the inference efficiency, GEAR improve the system throughput up to 2.38$\times$.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在不同的任务和模型上进行了实验，以展示GEAR的有效性。具体而言，我们使用LLaMA2-7B、LLaMA2-13B（Touvron等人，[2023b](#bib.bib27)）和Mistral-7B（Jiang等人，[2023](#bib.bib12)）评估CoT和零-shot性能，涵盖数学推理（GSM8k，Cobbe等人（[2021](#bib.bib4)））、多任务语言理解（MMLU，Hendrycks等人（[2021](#bib.bib10)））和符号推理（BigBench
    Hard，Suzgun等人（[2022](#bib.bib24)））。我们表明，GEAR始终优于基线方法，特别是在高压缩比下。例如，当将KV缓存压缩至LLaMA2-7B在GSM8k数据集上的FP16大小的28%时，GEAR在准确性上比表现最好的基线提高了4.5%。值得注意的是，我们首次在复杂和简单的生成任务上实现了接近无损性能的4位KV缓存压缩。关于推理效率，GEAR将系统吞吐量提高至2.38$\times$。
- en: 2 Background
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'Multi-head attention. A typical transformer model consists of $L$ heads:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 多头注意力。一个典型的变换器模型由 $L$ 个头组成：
- en: '|  |  | $1$2 |  | (1) |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $1$2 |  | (1) |'
- en: where $\bm{Q}^{(i)}=\bm{X}\bm{W}_{q_{i}},\bm{K}^{(i)}=\bm{X}\bm{W}_{k_{i}},\bm{V}^{(i)}=\bm{X}\bm{W}_{v_{i}}$.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bm{Q}^{(i)}=\bm{X}\bm{W}_{q_{i}},\bm{K}^{(i)}=\bm{X}\bm{W}_{k_{i}},\bm{V}^{(i)}=\bm{X}\bm{W}_{v_{i}}$。
- en: Autoregressive decoding and KV cache. Suppose the model is required to generate
    $n_{g}$ only, avoiding the recalculations of previous tokens and significantly
    enhancing generation speed.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归解码和KV缓存。假设模型仅需生成 $n_{g}$，以避免重新计算先前的标记，从而显著提高生成速度。
- en: 'Uniform quantization. Quantization maps a full-precision (FP16/FP32) tensor
    values into discrete levels. Specifically, uniform asymmetric quantization (INT8
    or INT4, Jacob et al. ([2018](#bib.bib11))) is an efficient quantization method
    with friendly hardware support. Given a tensor $\bm{X}\in\mathbb{R}^{n\times d}$
    with:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 均匀量化。量化将全精度（FP16/FP32）张量值映射为离散水平。具体而言，均匀不对称量化（INT8或INT4，Jacob等人 ([2018](#bib.bib11)))
    是一种具有友好硬件支持的高效量化方法。给定一个张量 $\bm{X}\in\mathbb{R}^{n\times d}$，其：
- en: '|  | $\displaystyle\begin{aligned} &amp;\textrm{Quant}_{b}(\bm{X})_{ij}=\left\lceil{(\bm{X}_{ij}-\min\bm{X})}/{\Delta}\right\rfloor,\quad\
    \Delta={(\max{\bm{X}}-\min\bm{X})}/{(2^{b}-1)}\end{aligned}$ |  | (2) |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{aligned} &\textrm{Quant}_{b}(\bm{X})_{ij}=\left\lceil{(\bm{X}_{ij}-\min\bm{X})}/{\Delta}\right\rfloor,\quad\
    \Delta={(\max{\bm{X}}-\min\bm{X})}/{(2^{b}-1)}\end{aligned}$ |  | (2) |'
- en: where $b$, which can lead to non-trivial quantization error (Dettmers et al.,
    [2022](#bib.bib6)) under high compression ratios.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$，在高压缩比下可能导致非平凡的量化误差（Dettmers等人，[2022](#bib.bib6)）。
- en: 3 Method
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法
- en: 'Our method consists of three important components to decompose and compress
    a KV cache matrix: (i) a quantized matrix $\widehat{\bm{D}}$ to capture the individual
    outliers.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法包括三个重要组件，用于分解和压缩KV缓存矩阵：（i）量化矩阵 $\widehat{\bm{D}}$ 以捕获个体异常值。
- en: '![Refer to caption](img/d0e38c4d0cc1b883aa9875769d87890a.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d0e38c4d0cc1b883aa9875769d87890a.png)'
- en: (a) Error of single component
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 单个组件的误差
- en: '![Refer to caption](img/144fac3c4dff07ccd026c4fc95787e7b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/144fac3c4dff07ccd026c4fc95787e7b.png)'
- en: (b) Distribution of entries
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 条目的分布
- en: '![Refer to caption](img/952a4b9dcaeb5d92fefafefd9c9c3b23.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/952a4b9dcaeb5d92fefafefd9c9c3b23.png)'
- en: (c) Spectrum of the residual
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 残差的谱
- en: '![Refer to caption](img/1d0f1e6e46b5bc58c0d420a57c9ba68e.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/1d0f1e6e46b5bc58c0d420a57c9ba68e.png)'
- en: (d) GEAR v.s. Outlier-R. Q.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: (d) GEAR 与 Outlier-R. Q.
- en: 'Figure 2: We randomly sample a GSM8k example and analyze its KV caches by LLaMA2-7B.
    ([2(a)](#S3.F2.sf1 "In Figure 2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression
    Recipe for Near-Lossless Generative Inference of LLM")): the minimal approximation
    error of each individual technique when approximating the Value cache of the first
    layer. ([2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache
    Compression Recipe for Near-Lossless Generative Inference of LLM")): the entry
    distribution of weights and its KV caches at different layers. ([2(c)](#S3.F2.sf3
    "In Figure 2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM")): spectrum of the residual $\bm{R}$ decays rapidly.
    ([2(d)](#S3.F2.sf4 "In Figure 2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression
    Recipe for Near-Lossless Generative Inference of LLM")): low-rank approximation
    enables GEAR to achieves lower error.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2：我们随机抽取了一个 GSM8k 示例，并通过 LLaMA2-7B 分析其 KV 缓存。 ([2(a)](#S3.F2.sf1 "In Figure
    2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM"))：每种技术在近似第一层值缓存时的最小近似误差。 ([2(b)](#S3.F2.sf2 "In Figure
    2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM"))：不同层的权重条目分布及其 KV 缓存。 ([2(c)](#S3.F2.sf3 "In Figure
    2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM"))：残差 $\bm{R}$ 的谱迅速衰减。 ([2(d)](#S3.F2.sf4 "In Figure
    2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM"))：低秩近似使 GEAR 能够实现更低的误差。'
- en: 'As discussed in Section [1](#S1 "1 Introduction ‣ GEAR: An Efficient KV Cache
    Compression Recipe for Near-Lossless Generative Inference of LLM"), the approximation
    error plays a pivotal role in determining the model performance. Therefore, given
    a tensor $\bm{X}\in\{\bm{K}_{t},\bm{V}_{t}\}$. These motivations encourage us
    to explore the integration of three techniques to leverage their individual advantages
    while exploiting their synergistic potential. To achieve this, our goal becomes
    minimizing the following approximation error:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '如第[1](#S1 "1 Introduction ‣ GEAR: An Efficient KV Cache Compression Recipe
    for Near-Lossless Generative Inference of LLM")节所讨论的，近似误差在确定模型性能中起着关键作用。因此，给定一个张量
    $\bm{X}\in\{\bm{K}_{t},\bm{V}_{t}\}$。这些动机促使我们探索三种技术的整合，以利用它们各自的优势，同时发挥它们的协同潜力。为了实现这一目标，我们的最终目标是最小化以下近似误差：'
- en: '|  | $\displaystyle\min_{\widehat{\bm{D}},\bm{L},\bm{S}}\left\lVert{\bm{X}-\widehat{\bm{D}}-\bm{L}-\bm{S}}\right\rVert_{\rm
    F}.$ |  | (3) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\widehat{\bm{D}},\bm{L},\bm{S}}\left\lVert{\bm{X}-\widehat{\bm{D}}-\bm{L}-\bm{S}}\right\rVert_{\rm
    F}.$ |  | (3) |'
- en: 'One interesting idea to minimize ([3](#S3.E3 "In 3 Method ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM")) is
    alternating among quantization, singular-value decomposition (SVD) and outlier
    extraction, and iteratively updating three matrices $\widehat{\bm{D}},\bm{L},\bm{S}$
    until achieving minimal error. This idea has been introduced by Li et al. ([2023](#bib.bib14))
    to optimize a similar objective for an accurate initialization of weight quantization.
    However, the inference system has demanding speed requirements. The significant
    latency caused by these iterative updates is unacceptable for generative inference.
    Therefore, we propose an efficient approximation solution to approach ([3](#S3.E3
    "In 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM")) and compress online KV caches as follows.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '一个有趣的想法是通过交替使用量化、奇异值分解（SVD）和异常值提取，并迭代更新三个矩阵 $\widehat{\bm{D}},\bm{L},\bm{S}$，直到实现最小误差。这一想法由
    Li 等人 ([2023](#bib.bib14)) 提出，用于优化类似的目标以实现权重量化的准确初始化。然而，推理系统对速度有严格要求。由这些迭代更新造成的显著延迟对于生成推理来说是不可接受的。因此，我们提出了一种高效的近似解决方案，以接近
    ([3](#S3.E3 "In 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for
    Near-Lossless Generative Inference of LLM")) 并在线压缩 KV 缓存，如下所示。'
- en: 'Outlier-reduced quantization. Inspired by the recent study on weight quantization
    (Kim et al., [2023](#bib.bib13)), we observe that the quantized backbone $\widehat{\bm{D}}$:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值减少的量化。受到最近关于权重量化的研究（Kim 等人，[2023](#bib.bib13)）的启发，我们观察到量化的主干 $\widehat{\bm{D}}$：
- en: '|  | $\displaystyle\begin{aligned} \textrm{Filter}_{s}(\bm{X})_{ij}=\left\{\begin{array}[]{lc}\bm{X}_{ij}&amp;\textrm{if}~{}\bm{X}_{ij}~{}\textrm{in
    top or bottom $\frac{s}{2}$\%},\\ 0&amp;\textrm{otherwise}.\end{array}\right.\end{aligned}$
    |  | (4) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\begin{aligned} \textrm{Filter}_{s}(\bm{X})_{ij}=\left\{\begin{array}[]{lc}\bm{X}_{ij}&amp;\textrm{if}~{}\bm{X}_{ij}~{}\textrm{in
    top or bottom $\frac{s}{2}$\%},\\ 0&amp;\textrm{otherwise}.\end{array}\right.\end{aligned}$
    |  | (4) |'
- en: 'Then, we perform the uniform quantization for the extracted matrix and obtain
    the quantized backbone:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对提取的矩阵进行均匀量化，得到量化的骨干：
- en: '|  | $\displaystyle\widehat{\bm{D}}=\textrm{Quant}_{b}(\bm{X}-\bm{S}).$ |  |
    (5) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\widehat{\bm{D}}=\textrm{Quant}_{b}(\bm{X}-\bm{S}).$ |  |
    (5) |'
- en: 'The outlier-reduced quantization ([5](#S3.E5 "In 3 Method ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM")) has
    been introduced for weight quantization by Kim et al. ([2023](#bib.bib13)) and
    achieve excellent performance at low precision of up to 3-bit. However, it is
    important to note that the KV cache compression can be fundamentally different
    from weight quantization. The KV caches tend to contain more outliers, making
    its accurate quantization more challenging than weights (Xiao et al., [2023](#bib.bib33)).
    Figure [2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3 Method ‣ GEAR: An Efficient KV Cache
    Compression Recipe for Near-Lossless Generative Inference of LLM") shows the distribution
    of entries in KV caches and weights, which clearly illustrates that the KV values
    still span a broader range even after filtering out 10% outliers. Consequently,
    to achieve near-lossless performance at ultra-low precision such as 4-bit, we
    often need to extract a large portion of outliers (e.g., 10% as shown by our results
    in Section [4](#S4 "4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe
    for Near-Lossless Generative Inference of LLM")) and store them as a sparse matrix.
    However, such a sparse matrix results in the remaining cache size equivalent to
    that of 8-bit quantization because of its two index vectors and one value vector
    in full precision. Therefore, outlier-reduced quantization by ([5](#S3.E5 "In
    3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM")) still cannot achieve near-lossless high-ratio compression.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kim 等人 ([2023](#bib.bib13)) 已经介绍了离群值减少的量化方法 ([5](#S3.E5 "In 3 Method ‣ GEAR:
    An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference
    of LLM"))，并在高达 3-bit 的低精度下取得了优异的性能。然而，需要注意的是，KV 缓存压缩可能与权重量化有根本不同。KV 缓存往往包含更多离群值，使得其准确量化比权重更具挑战性（Xiao
    等人，[2023](#bib.bib33)）。图 [2(b)](#S3.F2.sf2 "In Figure 2 ‣ 3 Method ‣ GEAR: An
    Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of
    LLM") 显示了 KV 缓存和权重中的条目分布，清楚地说明即使在过滤掉 10% 的离群值之后，KV 值仍然跨越更广泛的范围。因此，为了在如 4-bit 这样的超低精度下实现近乎无损的性能，我们通常需要提取大量的离群值（例如，Section [4](#S4
    "4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM") 中的结果所示的 10%），并将其存储为稀疏矩阵。然而，这种稀疏矩阵导致剩余缓存的大小相当于 8-bit
    量化，因为它具有两个索引向量和一个全精度的值向量。因此，由 ([5](#S3.E5 "In 3 Method ‣ GEAR: An Efficient KV
    Cache Compression Recipe for Near-Lossless Generative Inference of LLM")) 提出的离群值减少量化方法仍无法实现近乎无损的高比率压缩。'
- en: 'Low-rank approximation. To reduce the approximation error more efficiently,
    we resort to low-rank approximation. Specifically, suppose the residual $\bm{R}=\bm{X}-(\widehat{\bm{D}}+\bm{S})$:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 低秩近似。为了更高效地减少近似误差，我们采用低秩近似。具体而言，假设残差 $\bm{R}=\bm{X}-(\widehat{\bm{D}}+\bm{S})$：
- en: '|  | $\displaystyle\bm{L}=\bm{A}\bm{B}^{\top}=\textrm{SVDSolver}_{r}(\bm{R})$
    |  | (6) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{L}=\bm{A}\bm{B}^{\top}=\textrm{SVDSolver}_{r}(\bm{R})$
    |  | (6) |'
- en: 'where $\bm{A}\in\mathbb{R}^{n\times r},\bm{B}\in\mathbb{R}^{d\times r}$ (please
    see Appendix [A](#A1 "Appendix A Power Iteration Algorithm ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM") for
    the algorithm details).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\bm{A}\in\mathbb{R}^{n\times r},\bm{B}\in\mathbb{R}^{d\times r}$（算法细节请参见附录 [A](#A1
    "附录 A Power Iteration Algorithm ‣ GEAR: An Efficient KV Cache Compression Recipe
    for Near-Lossless Generative Inference of LLM")）。'
- en: 'In summary, GEAR integrates three compression techniques to provide an efficient
    solution for minimizing the approximation error in ([3](#S3.E3 "In 3 Method ‣
    GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference
    of LLM")). Specifically, the quantized backbone $\widehat{\bm{D}}$ compensates
    for the extraction of sparse information existing in individual outliers and compliments
    the quantization process tightly. As such, GEAR effectively reduces the approximation
    error, achieving near-lossless high-ratio compression for KV caches.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '总结来说，GEAR 集成了三种压缩技术，提供了一个高效的解决方案，以最小化 ([3](#S3.E3 "In 3 Method ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM")) 中的近似误差。具体而言，量化骨干
    $\widehat{\bm{D}}$ 补偿了从单个离群值中提取的稀疏信息，并紧密配合了量化过程。因此，GEAR 有效地减少了近似误差，实现了 KV 缓存的近乎无损高比率压缩。'
- en: 'Additionally, GEAR introduces a streaming strategy to significantly improve
    its inference throughput by up to $2.88\times$ along with previous caches. We
    summarize the detailed algorithm of GEAR in Algorithm [1](#alg1 "Algorithm 1 ‣
    3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，GEAR引入了一种流式策略，通过与先前的缓存一起显著提高其推理吞吐量，提升幅度高达 $2.88\times$。我们在算法 [1](#alg1 "Algorithm
    1 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM") 中总结了GEAR的详细算法。'
- en: Algorithm 1 GEAR
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 GEAR
- en: '1:  Input: The initial $\{\bm{K}_{0},\bm{V}_{0}\}$.13:     end if14:  end for'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '1:  输入：初始 $\{\bm{K}_{0},\bm{V}_{0}\}$。13: end if14:  end for'
- en: 4 Experiments
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: We use GEAR as a plug-and-play KV cache compression for generative inference
    with various LLM models on a wide variety of generative tasks including mathematical
    reasoning (GSM8k, Cobbe et al. ([2021](#bib.bib4))), multitask language understanding
    (MMLU, Hendrycks et al. ([2021](#bib.bib10))), and symbolic reasoning (BigBench
    Hard (BBH), Suzgun et al. ([2022](#bib.bib24))) both with and without CoT prompting
    (Wei et al., [2023](#bib.bib31)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将GEAR用作即插即用的KV缓存压缩工具，应用于各种LLM模型的生成推理任务，包括数学推理（GSM8k，Cobbe et al. ([2021](#bib.bib4)））、多任务语言理解（MMLU，Hendrycks
    et al. ([2021](#bib.bib10)））和符号推理（BigBench Hard (BBH)，Suzgun et al. ([2022](#bib.bib24)）），包括和不包括CoT提示（Wei
    et al., [2023](#bib.bib31)）。
- en: Implementation details. We use the open-source pre-trained LLM models available
    at Huggingface Transformers^*^**https://github.com/huggingface/transformers (Wolf
    et al., [2019](#bib.bib32)) and apply GEAR and other alternative compression methods
    to our LLM inference framework written in PyTorch (Paszke et al., [2019](#bib.bib20)).
    In this work we focus on the compression of the KV cache, thus to understand its
    impact on the generation performance, we kept all other tensors to FP16, unless
    otherwise mentioned.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 实现细节。我们使用Huggingface Transformers^*^**https://github.com/huggingface/transformers
    (Wolf et al., [2019](#bib.bib32)) 提供的开源预训练LLM模型，并将GEAR及其他替代压缩方法应用于我们用PyTorch（Paszke
    et al., [2019](#bib.bib20)）编写的LLM推理框架。在这项工作中，我们专注于KV缓存的压缩，因此为了了解其对生成性能的影响，我们保持所有其他张量为FP16，除非另有说明。
- en: 'We focus on the compression to ultra-low precision. The implementation of 8-bit
    is friendly supported by hardware and 4-bit can be easily extended based on it.
    Hence we primarily report the results of 4-bit KV cache compression. For GEAR,
    we apply it (Algorithm [1](#alg1 "Algorithm 1 ‣ 3 Method ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM")) to
    compress KV caches batch-wise, and fix the sparsity ratio $s$.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '我们专注于超低精度的压缩。8位实现得到了硬件的友好支持，4位则可以轻松扩展。因此，我们主要报告4位KV缓存压缩的结果。对于GEAR，我们应用它（算法
    [1](#alg1 "Algorithm 1 ‣ 3 Method ‣ GEAR: An Efficient KV Cache Compression Recipe
    for Near-Lossless Generative Inference of LLM")）以批量方式压缩KV缓存，并固定稀疏率$s$。'
- en: Baselines. As far as we know, there is limited work that exhaustively demonstrate
    the generative performance with KV cache compression via quantization. Therefore,
    we implement three popular quantization methods from the literature of weight
    quantization and compare with GEAR.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基准线。根据我们所知，目前关于通过量化实现KV缓存压缩的生成性能的工作有限。因此，我们实现了三种流行的量化方法，并与GEAR进行了比较。
- en: $\bullet$ Uniform quantization (Jacob et al., [2018](#bib.bib11)) is the most
    common approach that uniformly quantize all entries. In specific, here we use
    uniform asymmetric quantization for better accuracy.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 均匀量化（Jacob et al., [2018](#bib.bib11)）是最常见的方法，均匀地量化所有条目。具体来说，我们在这里使用均匀非对称量化以获得更好的准确性。
- en: $\bullet$ Group-wise quantization (Yao et al., [2022](#bib.bib34)) is an effective
    approach that quantizes an input tensor channel-wise so as to decrease the quantization
    error. For group-quantization, unless otherwise mentioned, we use channel-wise
    grouping over the batch dimension.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 分组量化（Yao et al., [2022](#bib.bib34)）是一种有效的方法，通过通道级量化输入张量以减少量化误差。对于分组量化，除非另有说明，我们使用按批次维度进行的通道级分组。
- en: $\bullet$ Outlier-reduced quantization has been introduced by (Kim et al., [2023](#bib.bib13))
    for weight quantization, which extracts outliers before the quantization and achieves
    excellent performance. Inspired by this work, we implement an outlier-reduced
    quantization for the KV cache tensor with uniform asymmetric quantization of the
    non-outlier components.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: $\bullet$ 异常值减少量化已由 (Kim et al., [2023](#bib.bib13)) 提出用于权重量化，它在量化前提取异常值，并取得了优秀的表现。受此工作的启发，我们为KV缓存张量实现了一种异常值减少量化，采用了对非异常值组件的均匀非对称量化。
- en: '$\bullet$ H[2]O (Zhang et al., [2023](#bib.bib38)) is a recent token dropping
    method evicting unimportant tokens with lower accumulated attention scores, which
    we compare with in Section [4.4](#S4.SS4 "4.4 Analysis and Discussions ‣ 4 Experiments
    ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM").'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '$\bullet$ H[2]O (Zhang et al., [2023](#bib.bib38)) 是一种近期的令牌丢弃方法，通过驱逐具有较低累积注意力分数的不重要令牌，我们在第
    [4.4](#S4.SS4 "4.4 Analysis and Discussions ‣ 4 Experiments ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM") 节中与之进行了比较。'
- en: 4.1 Generative Performance with CoT Prompting
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 使用 CoT 提示的生成性能
- en: 'Models and datasets. We compare different compression methods with LLaMA2-7B,
    LLaMA2-13B (Touvron et al., [2023b](#bib.bib27)) and Mistral-7B (Jiang et al.,
    [2023](#bib.bib12)) on three challenging generative tasks: GSM8k, MMLU and BBH.
    GSM8k (Cobbe et al., [2021](#bib.bib4)) is a widely used math reasoning datasets
    of 8k problmes that test models’ ability of arithmetic reasoning. MMLU (Hendrycks
    et al., [2021](#bib.bib10)) is a evaluation suite of 15k problems with 57 subjects
    assessing models’ knowledge and reasoning at high-school and college levels. BBH
    (Suzgun et al., [2022](#bib.bib24)) is a suite of language and symbolic reasoning
    problems consisting of 6.5k problems within 23 subsets. Given the complexity of
    these tasks, we use the chain-of-thought prompting to enhance the model performance.
    Specifically, we follow the evaluation approach from Fu et al. ([2023](#bib.bib8))
    and use the prompts created by them, which consist of multiple examples, each
    involving multi-step reasoning. Notably, improving the performance by CoT requires
    accurate KV cache compression as models need focus on most reasoning steps to
    generate correct answers.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。我们比较了不同的压缩方法，使用 LLaMA2-7B、LLaMA2-13B (Touvron et al., [2023b](#bib.bib27))
    和 Mistral-7B (Jiang et al., [2023](#bib.bib12)) 在三个具有挑战性的生成任务上：GSM8k、MMLU 和 BBH。GSM8k
    (Cobbe et al., [2021](#bib.bib4)) 是一个广泛使用的数学推理数据集，包含 8k 个问题，用于测试模型的算术推理能力。MMLU
    (Hendrycks et al., [2021](#bib.bib10)) 是一个评估模型知识和推理能力的 15k 个问题的评估套件，涵盖了高中和大学水平的
    57 个学科。BBH (Suzgun et al., [2022](#bib.bib24)) 是一个语言和符号推理问题的套件，包含 6.5k 个问题，分为
    23 个子集。鉴于这些任务的复杂性，我们使用链式思维提示来提高模型性能。具体来说，我们遵循 Fu et al. ([2023](#bib.bib8)) 的评估方法，并使用他们创建的提示，这些提示包括多个示例，每个示例涉及多步骤推理。值得注意的是，通过
    CoT 提高性能需要准确的 KV 缓存压缩，因为模型需要专注于大多数推理步骤以生成正确答案。
- en: 'Table 1: Main results of CoT performance. Here Ratio is the compression ratio
    (i.e., the FP16 cache size divided by the remaining cache size). The best results
    on each dataset are shown in bold.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: CoT 性能的主要结果。这里的 Ratio 是压缩比（即 FP16 缓存大小除以剩余缓存大小）。每个数据集上的最佳结果以**粗体**显示。'
- en: '| Model | LLaMA2-7B | Mistral-7B | LLaMA2-13B | All |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA2-7B | Mistral-7B | LLaMA2-13B | 全部 |'
- en: '| Method | Bit $b$ | Ratio | GSM8k | MMLU | BBH | GSM8k | MMLU | BBH | GSM8k
    | BBH | Avg. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位 $b$ | 比率 | GSM8k | MMLU | BBH | GSM8k | MMLU | BBH | GSM8k | BBH |
    平均 |'
- en: '| Acc | Acc | Acc | Acc | Acc | Acc | Acc | Acc | Acc |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 准确率 | 准确率 | 准确率 | 准确率 | 准确率 | 准确率 | 准确率 | 准确率 |'
- en: '| FP16 Baseline | 16 | 1$\times$ | 16.30 | 44.80 | 33.58 | 42.84 | 59.70 |
    47.92 | 30.34 | 40.79 | 39.53 |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| FP16 基线 | 16 | 1$\times$ | 16.30 | 44.80 | 33.58 | 42.84 | 59.70 | 47.92
    | 30.34 | 40.79 | 39.53 |'
- en: '| Uniform Quant | 4 | 4$\times$ | 0 | 0 | 0 | 1.17 | 7.78 | 3.60 | 0.03 | 0
    | 1.57 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 均匀量化 | 4 | 4$\times$ | 0 | 0 | 0 | 1.17 | 7.78 | 3.60 | 0.03 | 0 | 1.57 |'
- en: '| Group Quant | 4 | 4$\times$ | 1.36 | 11.90 | 2.43 | 3.72 | 43.88 | 37.94
    | 2.12 | 7.54 | 13.86 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 组量化 | 4 | 4$\times$ | 1.36 | 11.90 | 2.43 | 3.72 | 43.88 | 37.94 | 2.12 |
    7.54 | 13.86 |'
- en: '| Outlier-R. Quant ($s=10\%$ | 11.22 | 40.67 | 31.50 | 41.39 | 58.25 | 47.53
    | 21.25 | 36.69 | 36.06 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 异常值减少量化 ($s=10\%$ | 11.22 | 40.67 | 31.50 | 41.39 | 58.25 | 47.53 | 21.25
    | 36.69 | 36.06 |'
- en: '| Outlier-R. Quant ($s=5\%$ | 9.47 | 34.17 | 28.26 | 37.67 | 57.41 | 46.43
    | 13.64 | 32.46 | 32.44 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 异常值减少量化 ($s=5\%$ | 9.47 | 34.17 | 28.26 | 37.67 | 57.41 | 46.43 | 13.64 |
    32.46 | 32.44 |'
- en: '| GEAR ($s=2\%,\rho=2\%$ | 14.17 | 44.42 | 31.53 | 41.39 | 58.32 | 46.80 |
    25.92 | 37.51 | 37.51 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| GEAR ($s=2\%,\rho=2\%$ | 14.17 | 44.42 | 31.53 | 41.39 | 58.32 | 46.80 |
    25.92 | 37.51 | 37.51 |'
- en: '| GEAR ($s=2\%,\rho=5\%$ | 15.70 | 44.45 | 33.01 | 41.69 | 58.64 | 47.12 |
    27.97 | 37.38 | 38.25 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GEAR ($s=2\%,\rho=5\%$ | 15.70 | 44.45 | 33.01 | 41.69 | 58.64 | 47.12 |
    27.97 | 37.38 | 38.25 |'
- en: 'Table 2: Main results of zeroshot performance. Here Raio is the compression
    ratio. The best results on each dataset are shown in bold.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：零-shot 性能的主要结果。这里的 Raio 是压缩比。每个数据集上的最佳结果用**粗体**显示。
- en: '| Model | LLaMA2-7B-chat | LLaMA2-7B | Mistral-7B |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | LLaMA2-7B-chat | LLaMA2-7B | Mistral-7B |'
- en: '| Method | Bit $b$ | Ratio | GSM8k | MMLU | WikiText-2 | GSM8k | MMLU | WikiText-2
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 $b$ | 比率 | GSM8k | MMLU | WikiText-2 | GSM8k | MMLU | WikiText-2
    |'
- en: '| Acc $\uparrow$ |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 $\uparrow$ |'
- en: '| FP16 Baseline | 16 | 1$\times$ | 19.8 | 29.32 | 5.14 | 21.46 | 58.45 | 5.25
    |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| FP16 基线 | 16 | 1$\times$ | 19.8 | 29.32 | 5.14 | 21.46 | 58.45 | 5.25 |'
- en: '| Uniform Quant | 4 | 4$\times$ | 0.10 | 1.62 | 2536.84 | 0.42 | 0.48 | 100.32
    |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 均匀量化 | 4 | 4$\times$ | 0.10 | 1.62 | 2536.84 | 0.42 | 0.48 | 100.32 |'
- en: '| Group Quant | 4 | 4$\times$ | 2.71 | 3.33 | 53.21 | 4.70 | 38.76 | 6.56 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 分组量化 | 4 | 4$\times$ | 2.71 | 3.33 | 53.21 | 4.70 | 38.76 | 6.56 |'
- en: '| Outlier-R. Quant ($s=10\%$ | 17.61 | 23.66 | 5.33 | 7.96 | 57.95 | 5.32 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 异常值-R. 量化 ($s=10\%$ | 17.61 | 23.66 | 5.33 | 7.96 | 57.95 | 5.32 |'
- en: '| Outlier-R. Quant ($s=5\%$ | 14.52 | 18.88 | 5.43 | 5.00 | 58.67 | 5.39 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 异常值-R. 量化 ($s=5\%$ | 14.52 | 18.88 | 5.43 | 5.00 | 58.67 | 5.39 |'
- en: '| GEAR ($s=2\%,\rho=2\%$ | 19.13 | 28.54 | 5.69 | 19.03 | 58.29 | 5.33 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| GEAR ($s=2\%,\rho=2\%$ | 19.13 | 28.54 | 5.69 | 19.03 | 58.29 | 5.33 |'
- en: '| GEAR ($s=2\%,\rho=5\%$ | 19.40 | 28.54 | 5.61 | 19.11 | 58.34 | 5.32 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| GEAR ($s=2\%,\rho=5\%$ | 19.40 | 28.54 | 5.61 | 19.11 | 58.34 | 5.32 |'
- en: Implementation details. For the outlier-reduced quantization, we find that a
    high sparsity ratio is required to achieve good performance, and hence select
    $s$.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 实施细节。对于异常值减少的量化，我们发现需要较高的稀疏比率才能实现良好的性能，因此选择了$s$。
- en: 'Main results. Table [1](#S4.T1 "Table 1 ‣ 4.1 Generative Performance with CoT
    Prompting ‣ 4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for
    Near-Lossless Generative Inference of LLM") shows experimental results about CoT
    performance of three models. We see that GEAR achieves better or on par performance
    than baseline approaches on all datasets for all models. For example, under the
    compression ratio of 2.63$\times$.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 主要结果。表 [1](#S4.T1 "表 1 ‣ 4.1 带有 CoT 提示的生成性能 ‣ 4 实验 ‣ GEAR：用于近乎无损生成推断的高效 KV 缓存压缩方案")
    显示了三种模型的 CoT 性能实验结果。我们看到 GEAR 在所有数据集和所有模型上实现了优于或等同于基线方法的性能。例如，在2.63$\times$的压缩比下。
- en: 4.2 Zero-shot Generative Performance
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 零-shot 生成性能
- en: Models and datasets. We perform the zeroshot evaluation on GSM8k and MMLU using
    LLaMA2-7B, LLaMA2-7B-chat, and Mistral-7B. Particularly, LLaMA2-7B-chat, a instruction-tuned
    model, exhibits significantly higher accuracy than LLaMA2-7B. Hence, we choose
    to assess its performance on GSM8k. Additionally, we include a comparison on Wikitext-2
    Merity et al. ([2016](#bib.bib17)) to evaluate the language modeling ability after
    applying KV compression.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型和数据集。我们使用 LLaMA2-7B、LLaMA2-7B-chat 和 Mistral-7B 对 GSM8k 和 MMLU 进行零-shot 评估。特别是，LLaMA2-7B-chat
    作为一个指令调优模型，展现出比 LLaMA2-7B 更高的准确性。因此，我们选择在 GSM8k 上评估其性能。此外，我们还包括了对 Wikitext-2 Merity
    等人 ([2016](#bib.bib17)) 的比较，以评估应用 KV 压缩后的语言建模能力。
- en: 'Main results. We summarize the experimental results in Table [2](#S4.T2 "Table
    2 ‣ 4.1 Generative Performance with CoT Prompting ‣ 4 Experiments ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM"), where
    we compare the 4-bit compression performance. The hyperparameter configuration
    are the same as Section [4.1](#S4.SS1 "4.1 Generative Performance with CoT Prompting
    ‣ 4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM"). We see that GEAR achieves the best or on-par performance
    compared with other compression methods on all datasets and all models. For example,
    under the compression ratio of 2.6$\times$, GEAR achieves near-lossless performance
    compared to FP16 baseline on GSM8k and MMLU for the LLaMA models, and exhibits
    the improvement of 4.88% and 9.66% respectively over the baseline.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 主要结果。我们在表 [2](#S4.T2 "表 2 ‣ 4.1 带有 CoT 提示的生成性能 ‣ 4 实验 ‣ GEAR：用于近乎无损生成推断的高效 KV
    缓存压缩方案")中总结了实验结果，其中比较了4-bit压缩性能。超参数配置与第 [4.1](#S4.SS1 "4.1 带有 CoT 提示的生成性能 ‣ 4
    实验 ‣ GEAR：用于近乎无损生成推断的高效 KV 缓存压缩方案")节相同。我们看到 GEAR 在所有数据集和所有模型上实现了最佳或相当的性能。例如，在2.6$\times$的压缩比下，GEAR
    在 LLaMA 模型的 GSM8k 和 MMLU 上相较于 FP16 基线实现了近乎无损的性能，分别提高了 4.88% 和 9.66%。
- en: Remark. In both CoT and zeroshot evaluations, GEAR performs close to FP16 baseline
    and achieves high compression ratios up to 3$\times$. Other alternative compression
    methods suffer from a noticeable drop in performance as the compression ratio
    increases. This decline in performance is particularly evident in tasks that involve
    reasoning (such as CoT) and long-sequence generations (e.g., GSM8k). Both types
    of tasks demand minimal approximation error. The reasoning-related tasks often
    require models to closely attend to most contextual information to generate correct
    answers. The long sequence generation can accumulate the error throughout the
    autoregressive decoding process, making it sensitive to approximation error. Remarkably,
    as effectively reducing the error, GEAR yields near-lossless and high-ratio compression
    on these complex tasks.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 备注。在CoT和zeroshot评估中，GEAR的表现接近FP16基线，并实现了高达3$\times$的压缩比。其他替代压缩方法在压缩比增加时表现出明显的性能下降。这种性能下降在涉及推理（如CoT）和长序列生成（例如GSM8k）的任务中特别明显。这两种任务都要求最小化近似误差。推理相关任务通常要求模型密切关注大部分上下文信息以生成正确的答案。长序列生成可能在自回归解码过程中积累误差，使其对近似误差敏感。值得注意的是，通过有效减少误差，GEAR在这些复杂任务中实现了近乎无损和高压缩比。
- en: 4.3 System Performance Analysis
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 系统性能分析
- en: '![Refer to caption](img/dca1042ab992acc80bf03ec3acb5a496.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dca1042ab992acc80bf03ec3acb5a496.png)'
- en: (a) Weights and KV cache size
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 权重和KV缓存大小
- en: '![Refer to caption](img/765998c73f676b35f4846974f3692816.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/765998c73f676b35f4846974f3692816.png)'
- en: (b) Peak memory comparison
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 峰值内存比较
- en: '![Refer to caption](img/7615702db60e23da94e16423433ab276.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/7615702db60e23da94e16423433ab276.png)'
- en: (c) Zig-zag system throughput
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 锯齿状系统吞吐量
- en: 'Figure 3: ([3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4.3 System Performance Analysis
    ‣ 4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM")): Comparison of the memory size of model weights
    and the KV cache. ([3(b)](#S4.F3.sf2 "In Figure 3 ‣ 4.3 System Performance Analysis
    ‣ 4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM")): Peak memory (PM) usage in practical inference
    system. GEAR can reduce peak memory up to 2.29$\times$. ([3(c)](#S4.F3.sf3 "In
    Figure 3 ‣ 4.3 System Performance Analysis ‣ 4 Experiments ‣ GEAR: An Efficient
    KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM")):
    GEAR significantly increases the throughputs of the zig-zag system.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '图3: ([3(a)](#S4.F3.sf1 "在图3 ‣ 4.3系统性能分析 ‣ 4 实验 ‣ GEAR：一种高效的KV缓存压缩方案用于近乎无损的LLM生成推理")):
    模型权重和KV缓存的内存大小比较。 ([3(b)](#S4.F3.sf2 "在图3 ‣ 4.3系统性能分析 ‣ 4 实验 ‣ GEAR：一种高效的KV缓存压缩方案用于近乎无损的LLM生成推理")):
    实际推理系统中的峰值内存（PM）使用情况。GEAR可以将峰值内存减少至2.29$\times$。 ([3(c)](#S4.F3.sf3 "在图3 ‣ 4.3系统性能分析
    ‣ 4 实验 ‣ GEAR：一种高效的KV缓存压缩方案用于近乎无损的LLM生成推理")): GEAR显著提高了锯齿状系统的吞吐量。'
- en: 'In this section, we analyze the memory footprint and system throughput (i.e., the
    number of processed batches per second) to demonstrate the practical benefits
    of GEAR on the inference system from two perspectives: (i) for the system with
    adequate memory resource, compressing KV cache can reduce the peak memory usage,
    allowing for larger batch size and longer generation length; (ii) for the system
    with limited GPU memory that needs to offload large KV cache, GEAR can enhance
    the system throughput by up to 2.29$\times$ due to its high compression ratio
    and inference speedup.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们从两个角度分析了内存占用和系统吞吐量（即每秒处理的批次数量），以展示GEAR在推理系统中的实际优势：（i）对于内存资源充足的系统，压缩KV缓存可以减少峰值内存使用，从而允许更大的批量大小和更长的生成长度；（ii）对于需要卸载大规模KV缓存的有限GPU内存系统，GEAR由于其高压缩比和推理加速，可以将系统吞吐量提高至2.29$\times$。
- en: 'Figure [3(a)](#S4.F3.sf1 "In Figure 3 ‣ 4.3 System Performance Analysis ‣ 4
    Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM") presents the memory footprint of model weights and
    KV cache for LLaMA2-7B/13B with a batch size of 30 and an input sequence length
    $n$ increase of the maximum generation length, offering the capabilities for extended
    context generation.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3(a)](#S4.F3.sf1 "在图3 ‣ 4.3系统性能分析 ‣ 4 实验 ‣ GEAR：一种高效的KV缓存压缩方案用于近乎无损的LLM生成推理")展示了LLaMA2-7B/13B在批量大小为30和输入序列长度$n$增加的情况下模型权重和KV缓存的内存占用，提供了扩展上下文生成的能力。
- en: 'In the context of the limited GPU memory resource, the inference system has
    to resorts to offloading. The start-of-the-art option is the zig-zag scheduling
    Sheng et al. ([2023](#bib.bib23)), which crafts the frequent loading and offloading
    of model weights and KV cache to optimize the throughput. Remarkably, the adoption
    of GEAR to this system significantly diminishes memory bandwidth usage. This leads
    to an improvement in system throughput, demonstrating the efficacy of the GEAR
    compression method in enhancing the operational performance of LLM deployment.
    Figure [3(c)](#S4.F3.sf3 "In Figure 3 ‣ 4.3 System Performance Analysis ‣ 4 Experiments
    ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM") shows throughput of four KV cache compression schemes on the
    zig-zag scheduler system, that achieve the similar accuracy. In specific, we use
    one single RTX Titan with 24GB GPU, 125 GB CPU memory respectively. As shown in
    the figure, GEAR can improve the system throughput by up to 2.38$\times$, respectively.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '在有限的 GPU 内存资源的背景下，推理系统不得不 resort to offloading。最先进的选择是 Zig-Zag 调度 Sheng 等 ([2023](#bib.bib23))，它通过频繁加载和卸载模型权重及
    KV 缓存来优化吞吐量。值得注意的是，将 GEAR 应用于该系统显著减少了内存带宽的使用。这导致了系统吞吐量的提升，展示了 GEAR 压缩方法在增强 LLM
    部署操作性能方面的有效性。图 [3(c)](#S4.F3.sf3 "In Figure 3 ‣ 4.3 System Performance Analysis
    ‣ 4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM") 展示了在 Zig-Zag 调度系统上四种 KV 缓存压缩方案的吞吐量，这些方案在准确率上相似。具体来说，我们使用了一台带有
    24GB GPU 和 125 GB CPU 内存的 RTX Titan。图中所示，GEAR 可以将系统吞吐量提升高达 2.38$\times$。'
- en: 4.4 Analysis and Discussions
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 分析与讨论
- en: '![Refer to caption](img/46b6363bed7faaddb9075065b50db0a0.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/46b6363bed7faaddb9075065b50db0a0.png)'
- en: (a) GSM8k with CoT prompts
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GSM8k 与 CoT 提示
- en: '![Refer to caption](img/22f03114b4b7573e1bb84fced70c2215.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/22f03114b4b7573e1bb84fced70c2215.png)'
- en: (b) MMLU without CoT prompts
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: (b) MMLU 无 CoT 提示
- en: 'Figure 4: Results of compressing KV caches of LLaMA2-13B to different compression
    ratios.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：将 LLaMA2-13B 的 KV 缓存压缩到不同压缩比的结果。
- en: 'Different compression ratios. Figuer [4](#S4.F4 "Figure 4 ‣ 4.4 Analysis and
    Discussions ‣ 4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for
    Near-Lossless Generative Inference of LLM") illustrates the experimental results
    of compressing KV caches of LLaMA2-13B to various compression ratios. We see that
    on both GSM8k and MMLU, GEAR achieves consistent performance improvement over
    all baseline methods across different compression ratios. Notably, under high
    compression ratio, GEAR still can yields near-lossless performance while other
    methods suffer from a critical drop in performance. For example, GEAR yield 43.46%
    accuracy on MMLU while compressing to a ratio of 3.43$\times$, indicating the
    effectiveness of GEAR for near-lossless high-ratio compression.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '不同压缩比。图[4](#S4.F4 "Figure 4 ‣ 4.4 Analysis and Discussions ‣ 4 Experiments
    ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM")展示了将 LLaMA2-13B 的 KV 缓存压缩到各种压缩比的实验结果。我们看到在 GSM8k 和 MMLU 上，GEAR
    在不同压缩比下相较于所有基线方法始终取得了性能的一致提升。值得注意的是，在高压缩比下，GEAR 仍能提供接近无损的性能，而其他方法则出现了性能的严重下降。例如，GEAR
    在压缩到 3.43$\times$ 的比率时在 MMLU 上达到了 43.46% 的准确率，这表明 GEAR 对于高比率接近无损压缩的有效性。'
- en: 'Comparison with token dropping. We evaluate the performance of H[2]O (Zhang
    et al., [2023](#bib.bib38)) for reducing KV cache size on GSM8k with LLaMA2-7B.
    Table [3](#S4.T3 "Table 3 ‣ 4.4 Analysis and Discussions ‣ 4 Experiments ‣ GEAR:
    An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference
    of LLM") presents its accuracy when dropping 50% tokens, which suggests H[2]0
    cannot effectively preserve the performance nor achieve high compression ratio.
    For complex tasks involving reasoning or long-sequence generation (such as GSM8k),
    models need to closely attend to most contextual information to generate correct
    answers. Token dropping methods, however, can make some information directly invisible,
    resulting in deviation in generation and degradation of performance.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '与 token 丢弃的比较。我们评估了 H[2]O（Zhang 等，[2023](#bib.bib38)）在 GSM8k 上减小 KV 缓存大小的性能。表
    [3](#S4.T3 "Table 3 ‣ 4.4 Analysis and Discussions ‣ 4 Experiments ‣ GEAR: An
    Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of
    LLM") 展示了丢弃 50% token 时的准确率，这表明 H[2]0 不能有效保持性能，也无法达到高压缩比。对于涉及推理或长序列生成的复杂任务（例如
    GSM8k），模型需要密切关注大多数上下文信息以生成正确答案。然而，token 丢弃方法会使某些信息直接不可见，从而导致生成偏差和性能下降。'
- en: 'Table 3: Accuracy of H[2]O on GSM8k with LLaMA2-7B.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：GSM8k 上 H[2]O 与 LLaMA2-7B 的准确率。
- en: '| Method | Ratio | CoT Acc. | Zero-shot Acc. |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 比例 | CoT 准确率 | 零-shot 准确率 |'
- en: '| H[2]O | $2\times$ | 6.82 | 5.96 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| H[2]O | $2\times$ | 6.82 | 5.96 |'
- en: '| GEAR | $3\times$ | 14.17 | 19.13 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| GEAR | $3\times$ | 14.17 | 19.13 |'
- en: 'Comparison on fine-tuned models. To further evaluate the effectiveness of GEAR
    on fine-tuned models, we fine-tune a LLaMA2-7B models on GSM8k for 6 epoches with
    batch size as 16\. Table [4](#S4.T4 "Table 4 ‣ 4.4 Analysis and Discussions ‣
    4 Experiments ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM") presents the accuracy of this fine-tuned model after
    applying KV cache compression methods. The hyperparameters are set as the same
    as Section [4.1](#S4.SS1 "4.1 Generative Performance with CoT Prompting ‣ 4 Experiments
    ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM"). We can see that GEAR still outperforms all of baseline methods,
    yielding an manifest accuracy improvement.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对微调模型的比较。为了进一步评估GEAR在微调模型上的有效性，我们将LLaMA2-7B模型在GSM8k上进行6轮微调，批次大小为16。表 [4](#S4.T4
    "表 4 ‣ 4.4 分析与讨论 ‣ 4 实验 ‣ GEAR：一种高效的KV缓存压缩方案，用于近乎无损的LLM生成推断")展示了在应用KV缓存压缩方法后的微调模型的准确率。超参数设置与第[4.1节](#S4.SS1
    "4.1 使用CoT提示的生成性能 ‣ 4 实验 ‣ GEAR：一种高效的KV缓存压缩方案，用于近乎无损的LLM生成推断")相同。我们可以看到，GEAR仍然优于所有基线方法，表现出明显的准确率提升。
- en: 'Table 4: Evaluation with a fine-tuned LLaMA2-7B on GSM8k when compressing the
    KV caches to 4-bit.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：在将KV缓存压缩到4位时，使用微调后的LLaMA2-7B在GSM8k上的评估。
- en: '| Method | Bit $b$ | Ratio | Acc. |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 位数 $b$ | 比例 | 准确率 |'
- en: '| FP16 Bseline | 16 | 1$\times$ | 38.10 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| FP16 基线 | 16 | 1$\times$ | 38.10 |'
- en: '| Uniform Quant | 4 | 4$\times$ | 4.20 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 均匀量化 | 4 | 4$\times$ | 4.20 |'
- en: '| Group Quant | 4 | 4$\times$ | 11.30 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| Group 量化 | 4 | 4$\times$ | 11.30 |'
- en: '| Ourlier-R. Quant ($s=5\%$ | 24.50 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Ourlier-R. 量化 ($s=5\%$ | 24.50 |'
- en: '| GEAR ($s=2\%,\rho=2\%$ | 27.10 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| GEAR ($s=2\%,\rho=2\%$ | 27.10 |'
- en: '| GEAR ($s=2\%,\rho=10\%$ | 37.00 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| GEAR ($s=2\%,\rho=10\%$ | 37.00 |'
- en: 'Ablation study on $s$. We see that GEAR does not require abundant sparse either
    low-rank components - a small ratio (2% or 5%) of sparse/low-rank matrix is adequate
    for GEAR to yield near-lossless accuracy. Further increasing the ratio may improve
    the performance but not significantly, which however results in additional memory
    consumption. More importantly, discarding either sparse or low-rank component
    can significantly degenerate the performance of GEAR (last line of Table [5](#S4.T5
    "Table 5 ‣ 4.4 Analysis and Discussions ‣ 4 Experiments ‣ GEAR: An Efficient KV
    Cache Compression Recipe for Near-Lossless Generative Inference of LLM")), which
    validates our claim in Section [1](#S1 "1 Introduction ‣ GEAR: An Efficient KV
    Cache Compression Recipe for Near-Lossless Generative Inference of LLM"). That
    is, the three components in GEAR are complementary with each other and each is
    indispensable for GEAR to achieve the near-lossless performance.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对$s$的消融研究。我们发现GEAR不需要大量稀疏或低秩组件——少量（2%或5%）的稀疏/低秩矩阵足以使GEAR实现近乎无损的准确率。进一步增加比例可能会提高性能，但提升幅度不大，且会增加额外的内存消耗。更重要的是，丢弃稀疏或低秩组件中的任何一个都会显著降低GEAR的性能（表 [5](#S4.T5
    "表 5 ‣ 4.4 分析与讨论 ‣ 4 实验 ‣ GEAR：一种高效的KV缓存压缩方案，用于近乎无损的LLM生成推断")最后一行），这验证了我们在第[1节](#S1
    "1 引言 ‣ GEAR：一种高效的KV缓存压缩方案，用于近乎无损的LLM生成推断")中的主张。也就是说，GEAR中的三个组件是互补的，每一个都对GEAR实现近乎无损的性能至关重要。
- en: 'Table 5: GEAR performance with different $s$ using LLaMA2-7B on GSM8k with
    CoT. Here we change one hyperparameter while fixing another one.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：使用LLaMA2-7B在GSM8k上应用CoT时，不同$s$的GEAR性能。在这里我们改变一个超参数，同时固定另一个超参数。
- en: '| Hyperparameter | Acc. | Hyperparameter | Acc. |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 超参数 | 准确率 | 超参数 | 准确率 |'
- en: '| $s=2\%,\rho=10\%$ | 15.09 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| $s=2\%,\rho=10\%$ | 15.09 |'
- en: '| $s=2\%,\rho=5\%$ | 15.77 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| $s=2\%,\rho=5\%$ | 15.77 |'
- en: '| $s=2\%,\rho=2\%$ | 15.09 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| $s=2\%,\rho=2\%$ | 15.09 |'
- en: '| $s=2\%,\rho=1\%$ | 13.65 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| $s=2\%,\rho=1\%$ | 13.65 |'
- en: '| $s=2\%,\rho=0\%$ | 2.12 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| $s=2\%,\rho=0\%$ | 2.12 |'
- en: 5 Related Work
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 相关工作
- en: LLM weights compression. LLM weight compression can significantly reduce the
    memory footprint and data transfer cost. GPTQ (Frantar et al., [2023](#bib.bib7))
    accelerated the optimal brain quantization for LLM weights by orders of magnitude.
    SqueezeLLM (Kim et al., [2023](#bib.bib13)) successfully compressed the model
    weights to 3 bits by extracting the outlier values and quantize the remaining
    values according to hessian matrix within 10% perplexity increases. These algorithms
    are effective and could compress weights to 2 or 3 bits with acceptable loss of
    accuracy. However, these methods often require significant latency overhead and
    gradient information to work. Thus their are not fit for KV cache compression
    since KV cache does not have any trainable parameter and changes every generation
    stage, requiring efficient light-weight method for online compression.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: LLM 权重压缩。LLM 权重压缩可以显著减少内存占用和数据传输成本。GPTQ（Frantar 等，[2023](#bib.bib7)）通过数量级的提升加速了
    LLM 权重的最优脑量化。SqueezeLLM（Kim 等，[2023](#bib.bib13)）成功地将模型权重压缩到 3 位，通过提取异常值并根据 Hessian
    矩阵对剩余值进行量化，实现了 10% 的困惑度增加。这些算法有效且可以将权重压缩到 2 位或 3 位，同时损失精度在可接受范围内。然而，这些方法通常需要显著的延迟开销和梯度信息才能工作。因此，它们不适合
    KV 缓存压缩，因为 KV 缓存没有任何可训练的参数且在每一生成阶段都会变化，需要一种高效轻量的在线压缩方法。
- en: LLM KV cache compression. Activation and KV cache compression are harder than
    weight compression since they are more sensitive and related to model inputs.
    SmoothQuant (Xiao et al., [2023](#bib.bib33)) achieved 8-bit compression both
    for activation (KV caches included) and weights by adjusting the scaling factors
    to reduce outlier error and demonstrates near lossless performance on simple generative
    tasks. Atom (Zhao et al., [2023](#bib.bib39)) successfully compressed KV Cache
    to 4 bits on simple generative tasks within 5% performance degradation by combining
    4-bit and 8-bit channel-wise quantization. Another line of work explored KV pruning
    via token dropping based on attention score analysis. In specific, H[2]O (Zhang
    et al., [2023](#bib.bib38)) and FastGen (Ge et al., [2023](#bib.bib9)) proposed
    to prune KV via dropping tokens based on attention score to decrease the KV cache
    size. SparQ (Ribar et al., [2023](#bib.bib22)) not only dropped tokens according
    to attention score sparsity but also incorporated the error of the pruned value
    cache. These pruning and quantization algorithms often work well on summarizing
    tasks and zero-shot inference. However, for fine-tuned models, CoT inference,
    and generative reasoning datasets, attention scores are denser and each token
    contains important information that can not be ignored. Moreover, token dropping
    needs to weigh each token based on attention score, which makes these methods
    hard to deploy with FlashAttention (Dao et al., [2022](#bib.bib5)). Additionally,
    recent works shows the attention sparsity to be a function of the non-linearity
    choice of the model (Mirzadeh et al., [2023](#bib.bib18)), suggesting its vulnerability
    as a metric for KV compression.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: LLM KV 缓存压缩。激活和 KV 缓存压缩比权重压缩更难，因为它们对模型输入更为敏感。SmoothQuant（Xiao 等，[2023](#bib.bib33)）通过调整缩放因子以减少异常值误差，实现了激活（包括
    KV 缓存）和权重的 8 位压缩，并在简单生成任务上展示了近乎无损的性能。Atom（Zhao 等，[2023](#bib.bib39)）通过结合 4 位和
    8 位通道量化，成功地在简单生成任务中将 KV 缓存压缩到 4 位，性能下降不超过 5%。另一项工作通过基于注意力分数分析的令牌丢弃探索了 KV 剪枝。具体而言，H[2]O（Zhang
    等，[2023](#bib.bib38)）和 FastGen（Ge 等，[2023](#bib.bib9)）提出通过基于注意力分数丢弃令牌来减少 KV 缓存的大小。SparQ（Ribar
    等，[2023](#bib.bib22)）不仅根据注意力分数稀疏性丢弃令牌，还结合了修剪值缓存的误差。这些剪枝和量化算法通常在摘要任务和零-shot 推断中表现良好。然而，对于微调模型、CoT
    推断和生成推理数据集，注意力分数更密集，每个令牌都包含重要的信息，不能被忽视。此外，令牌丢弃需要根据注意力分数对每个令牌进行加权，这使得这些方法难以与 FlashAttention（Dao
    等，[2022](#bib.bib5)）一起部署。此外，最近的研究表明，注意力稀疏性是模型非线性选择的一个函数（Mirzadeh 等，[2023](#bib.bib18)），这表明它作为
    KV 压缩度量的脆弱性。
- en: 6 Conclusions
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this paper we present GEAR, a near loss-less KV cache compression framework
    for LLM inference that achieves KV compression in ultra-low precision with minimal
    accuracy drop. Compared to the existing alternatives, GEAR demonstrates SOTA performance
    on complex generative tasks involving reasoning, while resulting much lower memory
    footprint. The substantially low peak memory demand of GEAR, enables catering
    to more inference requests compared to FP16 baseline. Additionally, GEAR can facilitate
    a throughput boost of up to $2.38\times$. We hope our method can open a new avenue
    of memory-efficient LLM inference for near-lossless complex generation serving.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了 GEAR，一种近乎无损的 KV 缓存压缩框架，用于 LLM 推理，能够以超低精度实现 KV 压缩，并且准确度下降最小。与现有替代方案相比，GEAR
    在涉及推理的复杂生成任务中展示了 SOTA 性能，同时大幅降低了内存占用。GEAR 的峰值内存需求显著较低，使得能够处理比 FP16 基线更多的推理请求。此外，GEAR
    可以提升高达 $2.38\times$ 的吞吐量。我们希望我们的方法能够为近乎无损的复杂生成服务开辟新的内存高效 LLM 推理途径。
- en: References
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Aminabadi et al. (2022) Aminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan,
    A. A., Li, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase, O. and He, Y.
    (2022). Deepspeed inference: Enabling efficient inference of transformer models
    at unprecedented scale.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aminabadi 等人 (2022) Aminabadi, R. Y., Rajbhandari, S., Zhang, M., Awan, A. A.,
    Li, C., Li, D., Zheng, E., Rasley, J., Smith, S., Ruwase, O. 和 He, Y. (2022).
    Deepspeed 推理：实现前所未有规模的变换器模型高效推理。
- en: Brown et al. (2020a) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. et al. (2020a).
    Language models are few-shot learners. Advances in neural information processing
    systems, 33 1877–1901.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020a) Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A. 等人 (2020a). 语言模型是少量样本学习者。神经信息处理系统进展,
    33 1877–1901。
- en: Brown et al. (2020b) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan,
    J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal,
    S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
    D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
    S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
    I. and Amodei, D. (2020b). Language models are few-shot learners. CoRR, abs/2005.14165.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown 等人 (2020b) Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.,
    Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
    Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.
    M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S.,
    Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I. 和
    Amodei, D. (2020b). 语言模型是少量样本学习者。CoRR, abs/2005.14165。
- en: '[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)'
- en: Cobbe et al. (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H.,
    Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. and Schulman,
    J. (2021). Training verifiers to solve math word problems.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cobbe 等人 (2021) Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser,
    L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C. 和 Schulman, J.
    (2021). 训练验证器以解决数学文字问题。
- en: 'Dao et al. (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A. and Ré, C. (2022).
    Flashattention: Fast and memory-efficient exact attention with io-awareness.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dao 等人 (2022) Dao, T., Fu, D. Y., Ermon, S., Rudra, A. 和 Ré, C. (2022). Flashattention：快速且内存高效的精确注意力与
    IO 关注。
- en: 'Dettmers et al. (2022) Dettmers, T., Lewis, M., Belkada, Y. and Zettlemoyer,
    L. (2022). Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
    arXiv preprint arXiv:2208.07339.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Dettmers 等人 (2022) Dettmers, T., Lewis, M., Belkada, Y. 和 Zettlemoyer, L. (2022).
    Llm. int8 (): 用于变换器的大规模 8 位矩阵乘法。arXiv 预印本 arXiv:2208.07339。'
- en: 'Frantar et al. (2023) Frantar, E., Ashkboos, S., Hoefler, T. and Alistarh,
    D. (2023). Gptq: Accurate post-training quantization for generative pre-trained
    transformers.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Frantar 等人 (2023) Frantar, E., Ashkboos, S., Hoefler, T. 和 Alistarh, D. (2023).
    Gptq：生成预训练变换器的准确后训练量化。
- en: 'Fu et al. (2023) Fu, Y., Ou, L., Chen, M., Wan, Y., Peng, H. and Khot, T. (2023).
    Chain-of-thought hub: A continuous effort to measure large language models’ reasoning
    performance.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fu 等人 (2023) Fu, Y., Ou, L., Chen, M., Wan, Y., Peng, H. 和 Khot, T. (2023).
    思维链中心：持续努力衡量大语言模型的推理表现。
- en: 'Ge et al. (2023) Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J. and Gao, J.
    (2023). Model tells you what to discard: Adaptive kv cache compression for llms.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge 等人 (2023) Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J. 和 Gao, J. (2023).
    模型告诉你丢弃什么：LLM 的自适应 KV 缓存压缩。
- en: Hendrycks et al. (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D. and Steinhardt, J. (2021). Measuring massive multitask language understanding.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等人 (2021) Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
    M., Song, D. 和 Steinhardt, J. (2021). 测量大规模多任务语言理解。
- en: Jacob et al. (2018) Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard,
    A., Adam, H. and Kalenichenko, D. (2018). Quantization and training of neural
    networks for efficient integer-arithmetic-only inference. In Proceedings of the
    IEEE conference on computer vision and pattern recognition.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jacob 等 (2018) Jacob, B., Kligys, S., Chen, B., Zhu, M., Tang, M., Howard, A.,
    Adam, H. 和 Kalenichenko, D. (2018). Quantization and training of neural networks
    for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference
    on computer vision and pattern recognition.
- en: Jiang et al. (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,
    Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier,
    L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T.,
    Lacroix, T. and Sayed, W. E. (2023). Mistral 7b.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang 等 (2023) Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot,
    D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L.,
    Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,
    T. 和 Sayed, W. E. (2023). Mistral 7b.
- en: 'Kim et al. (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen,
    S., Mahoney, M. W. and Keutzer, K. (2023). Squeezellm: Dense-and-sparse quantization.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kim 等 (2023) Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S.,
    Mahoney, M. W. 和 Keutzer, K. (2023). Squeezellm: Dense-and-sparse quantization.'
- en: 'Li et al. (2023) Li, Y., Yu, Y., Liang, C., He, P., Karampatziakis, N., Chen,
    W. and Zhao, T. (2023). Loftq: Lora-fine-tuning-aware quantization for large language
    models.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Li 等 (2023) Li, Y., Yu, Y., Liang, C., He, P., Karampatziakis, N., Chen, W.
    和 Zhao, T. (2023). Loftq: Lora-fine-tuning-aware quantization for large language
    models.'
- en: 'Liu et al. (2023) Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z.,
    Kyrillidis, A. and Shrivastava, A. (2023). Scissorhands: Exploiting the persistence
    of importance hypothesis for LLM KV cache compression at test time. In Thirty-seventh
    Conference on Neural Information Processing Systems.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2023) Liu, Z., Desai, A., Liao, F., Wang, W., Xie, V., Xu, Z., Kyrillidis,
    A. 和 Shrivastava, A. (2023). Scissorhands: Exploiting the persistence of importance
    hypothesis for LLM KV cache compression at test time. In Thirty-seventh Conference
    on Neural Information Processing Systems.'
- en: '[https://openreview.net/forum?id=JZfg6wGi6g](https://openreview.net/forum?id=JZfg6wGi6g)'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://openreview.net/forum?id=JZfg6wGi6g](https://openreview.net/forum?id=JZfg6wGi6g)'
- en: 'Liu et al. (2024) Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman,
    V., Chen, B. and Hu, X. (2024). Kivi: A tuning-free asymmetric 2bit quantization
    for kv cache. arXiv preprint arXiv:2402.02750.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu 等 (2024) Liu, Z., Yuan, J., Jin, H., Zhong, S., Xu, Z., Braverman, V.,
    Chen, B. 和 Hu, X. (2024). Kivi: A tuning-free asymmetric 2bit quantization for
    kv cache. arXiv 预印本 arXiv:2402.02750.'
- en: Merity et al. (2016) Merity, S., Xiong, C., Bradbury, J. and Socher, R. (2016).
    Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Merity 等 (2016) Merity, S., Xiong, C., Bradbury, J. 和 Socher, R. (2016). Pointer
    sentinel mixture models. arXiv 预印本 arXiv:1609.07843.
- en: 'Mirzadeh et al. (2023) Mirzadeh, I., Alizadeh, K., Mehta, S., Del Mundo, C. C.,
    Tuzel, O., Samei, G., Rastegari, M. and Farajtabar, M. (2023). Relu strikes back:
    Exploiting activation sparsity in large language models. arXiv preprint arXiv:2310.04564.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mirzadeh 等 (2023) Mirzadeh, I., Alizadeh, K., Mehta, S., Del Mundo, C. C.,
    Tuzel, O., Samei, G., Rastegari, M. 和 Farajtabar, M. (2023). Relu strikes back:
    Exploiting activation sparsity in large language models. arXiv 预印本 arXiv:2310.04564.'
- en: OpenAI (2023) OpenAI (2023). Gpt-4 technical report.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenAI (2023) OpenAI (2023). Gpt-4 technical report.
- en: 'Paszke et al. (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury,
    J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A.,
    Köpf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner,
    B., Fang, L., Bai, J. and Chintala, S. (2019). Pytorch: An imperative style, high-performance
    deep learning library. In Advances in Neural Information Processing Systems 32:
    Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, 2019, Vancouver, BC, Canada (H. M. Wallach, H. Larochelle, A. Beygelzimer,
    F. d’Alché-Buc, E. B. Fox and R. Garnett, eds.).'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Paszke 等 (2019) Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,
    Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Köpf,
    A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B.,
    Fang, L., Bai, J. 和 Chintala, S. (2019). Pytorch: An imperative style, high-performance
    deep learning library. In Advances in Neural Information Processing Systems 32:
    Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
    December 8-14, 2019, Vancouver, BC, Canada (H. M. Wallach, H. Larochelle, A. Beygelzimer,
    F. d’Alché-Buc, E. B. Fox 和 R. Garnett, eds.).'
- en: Pope et al. (2022) Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury,
    J., Levskaya, A., Heek, J., Xiao, K., Agrawal, S. and Dean, J. (2022). Efficiently
    scaling transformer inference.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pope 等 (2022) Pope, R., Douglas, S., Chowdhery, A., Devlin, J., Bradbury, J.,
    Levskaya, A., Heek, J., Xiao, K., Agrawal, S. 和 Dean, J. (2022). Efficiently scaling
    transformer inference.
- en: 'Ribar et al. (2023) Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C.,
    Luschi, C. and Orr, D. (2023). Sparq attention: Bandwidth-efficient llm inference.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ribar 等 (2023) Ribar, L., Chelombiev, I., Hudlass-Galley, L., Blake, C., Luschi,
    C. 和 Orr, D. (2023). Sparq attention: Bandwidth-efficient llm inference.'
- en: 'Sheng et al. (2023) Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu,
    D. Y., Xie, Z., Chen, B., Barrett, C., Gonzalez, J. E., Liang, P., Ré, C., Stoica,
    I. and Zhang, C. (2023). Flexgen: High-throughput generative inference of large
    language models with a single gpu.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Sheng 等人 (2023) Sheng, Y., Zheng, L., Yuan, B., Li, Z., Ryabinin, M., Fu, D. Y.,
    Xie, Z., Chen, B., Barrett, C., Gonzalez, J. E., Liang, P., Ré, C., Stoica, I.
    和 Zhang, C. (2023). Flexgen: 单 GPU 高吞吐量生成推理的大型语言模型。'
- en: Suzgun et al. (2022) Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay,
    Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D. and Wei, J. (2022).
    Challenging big-bench tasks and whether chain-of-thought can solve them.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Suzgun 等人 (2022) Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y.,
    Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D. 和 Wei, J. (2022).
    挑战大基准任务及链式思维是否能够解决它们。
- en: 'Thoppilan et al. (2022) Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N.,
    Kulshreshtha, A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee,
    H., Zheng, H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin,
    D., Qin, J., Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhao, V., Zhou,
    Y., Chang, C.-C., Krivokon, I., Rusch, W., Pickett, M., Srinivasan, P., Man, L.,
    Meier-Hellstern, K., Morris, M. R., Doshi, T., Santos, R. D., Duke, T., Soraker,
    J., Zevenbergen, B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina,
    A., Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M.,
    Kuzmina, V., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas,
    B., Cui, C., Croak, M., Chi, E. and Le, Q. (2022). Lamda: Language models for
    dialog applications.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thoppilan 等人 (2022) Thoppilan, R., Freitas, D. D., Hall, J., Shazeer, N., Kulshreshtha,
    A., Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng,
    H. S., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J.,
    Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhao, V., Zhou, Y., Chang,
    C.-C., Krivokon, I., Rusch, W., Pickett, M., Srinivasan, P., Man, L., Meier-Hellstern,
    K., Morris, M. R., Doshi, T., Santos, R. D., Duke, T., Soraker, J., Zevenbergen,
    B., Prabhakaran, V., Diaz, M., Hutchinson, B., Olson, K., Molina, A., Hoffman-John,
    E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M., Kuzmina, V., Fenton,
    J., Cohen, A., Bernstein, R., Kurzweil, R., Aguera-Arcas, B., Cui, C., Croak,
    M., Chi, E. 和 Le, Q. (2022). Lamda: 对话应用的语言模型。'
- en: 'Touvron et al. (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
    Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E. and Lample, G. (2023a). Llama: Open and efficient foundation
    language models.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023a) Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,
    M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez,
    A., Joulin, A., Grave, E. 和 Lample, G. (2023a). Llama: 开放且高效的基础语言模型。'
- en: 'Touvron et al. (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S. and Scialom, T. (2023b). Llama 2: Open foundation
    and fine-tuned chat models.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Touvron 等人 (2023b) Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
    A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D.,
    Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J.,
    Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini,
    S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev,
    A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,
    Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton,
    A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M.,
    Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,
    Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
    A., Stojnic, R., Edunov, S. 和 Scialom, T. (2023b). Llama 2: 开放的基础和微调聊天模型。'
- en: Vaswani et al. (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł. and Polosukhin, I. (2017). Attention is all you need.
    Advances in neural information processing systems, 30.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani 等人 (2017) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
    L., Gomez, A. N., Kaiser, Ł. 和 Polosukhin, I. (2017). Attention is all you need.
    神经信息处理系统进展, 30。
- en: 'Vogels et al. (2019) Vogels, T., Karimireddy, S. P. and Jaggi, M. (2019). Powersgd:
    Practical low-rank gradient compression for distributed optimization. CoRR, abs/1905.13727.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Vogels 等人 (2019) Vogels, T., Karimireddy, S. P. 和 Jaggi, M. (2019). Powersgd:
    分布式优化的实用低秩梯度压缩。CoRR, abs/1905.13727。'
- en: '[http://arxiv.org/abs/1905.13727](http://arxiv.org/abs/1905.13727)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[http://arxiv.org/abs/1905.13727](http://arxiv.org/abs/1905.13727)'
- en: Wei et al. (2022) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T.,
    Vinyals, O., Liang, P., Dean, J. and Fedus, W. (2022). Emergent abilities of large
    language models. Transactions on Machine Learning Research. Survey Certification.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2022) Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud,
    S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., Chi, E. H., Hashimoto, T.,
    Vinyals, O., Liang, P., Dean, J. 和 Fedus, W. (2022). 大型语言模型的突现能力。《机器学习研究交易》。调查认证。
- en: '[https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://openreview.net/forum?id=yzkSU5zdwD](https://openreview.net/forum?id=yzkSU5zdwD)'
- en: Wei et al. (2023) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
    Xia, F., Chi, E., Le, Q. and Zhou, D. (2023). Chain-of-thought prompting elicits
    reasoning in large language models.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei et al. (2023) Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B.,
    Xia, F., Chi, E., Le, Q. 和 Zhou, D. (2023). Chain-of-thought prompting 引发大型语言模型的推理。
- en: 'Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M. et al. (2019). Huggingface’s
    transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wolf et al. (2019) Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C.,
    Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M. et al. (2019). Huggingface的transformers：最先进的自然语言处理。arXiv预印本
    arXiv:1910.03771。
- en: 'Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J. and Han,
    S. (2023). Smoothquant: Accurate and efficient post-training quantization for
    large language models.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xiao et al. (2023) Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J. 和 Han,
    S. (2023). Smoothquant：准确且高效的训练后量化用于大型语言模型。
- en: 'Yao et al. (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C.
    and He, Y. (2022). Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. Advances in Neural Information Processing Systems,
    35 27168–27183.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yao et al. (2022) Yao, Z., Yazdani Aminabadi, R., Zhang, M., Wu, X., Li, C.
    和 He, Y. (2022). Zeroquant：大规模transformers的高效且经济的训练后量化。《神经信息处理系统进展》，35 27168–27183。
- en: 'Yuan et al. (2022) Yuan, A., Coenen, A., Reif, E. and Ippolito, D. (2022).
    Wordcraft: Story writing with large language models. In 27th International Conference
    on Intelligent User Interfaces. IUI ’22, Association for Computing Machinery,
    New York, NY, USA.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yuan et al. (2022) Yuan, A., Coenen, A., Reif, E. 和 Ippolito, D. (2022). Wordcraft：使用大型语言模型的故事创作。在第27届国际智能用户界面会议。IUI
    ’22，计算机协会，纽约，美国。
- en: '[https://doi.org/10.1145/3490099.3511105](https://doi.org/10.1145/3490099.3511105)'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://doi.org/10.1145/3490099.3511105](https://doi.org/10.1145/3490099.3511105)'
- en: 'Zafrir et al. (2019) Zafrir, O., Boudoukh, G., Izsak, P. and Wasserblat, M.
    (2019). Q8bert: Quantized 8bit bert. In 2019 Fifth Workshop on Energy Efficient
    Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS). IEEE.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zafrir et al. (2019) Zafrir, O., Boudoukh, G., Izsak, P. 和 Wasserblat, M. (2019).
    Q8bert：量化的8位BERT。在2019年第五届节能机器学习与认知计算研讨会 - NeurIPS版（EMC2-NIPS）。IEEE。
- en: '[http://dx.doi.org/10.1109/EMC2-NIPS53020.2019.00016](http://dx.doi.org/10.1109/EMC2-NIPS53020.2019.00016)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[http://dx.doi.org/10.1109/EMC2-NIPS53020.2019.00016](http://dx.doi.org/10.1109/EMC2-NIPS53020.2019.00016)'
- en: 'Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T. and Zettlemoyer,
    L. (2022). Opt: Open pre-trained transformer language models.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,
    Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer,
    S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T. 和 Zettlemoyer,
    L. (2022). Opt：开放的预训练变换器语言模型。
- en: 'Zhang et al. (2023) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C., Wang, Z. and Chen, B. (2023). H[2]o:
    Heavy-hitter oracle for efficient generative inference of large language models.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2023) Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai,
    R., Song, Z., Tian, Y., Ré, C., Barrett, C., Wang, Z. 和 Chen, B. (2023). H[2]o：高效生成推理的大型语言模型重型神谕。
- en: 'Zhao et al. (2023) Zhao, Y., Lin, C.-Y., Zhu, K., Ye, Z., Chen, L., Zheng,
    S., Ceze, L., Krishnamurthy, A., Chen, T. and Kasikci, B. (2023). Atom: Low-bit
    quantization for efficient and accurate llm serving.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2023) Zhao, Y., Lin, C.-Y., Zhu, K., Ye, Z., Chen, L., Zheng, S.,
    Ceze, L., Krishnamurthy, A., Chen, T. 和 Kasikci, B. (2023). Atom：低比特量化用于高效且准确的LLM服务。
- en: Appendix A Power Iteration Algorithm
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录A：幂迭代算法
- en: Algorithm 2 Low rank approximation of the error tensor
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2：误差张量的低秩近似
- en: 0:  Input matrix $\bm{X}\in\mathbb{R}^{n\times d}$  end while
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 0：输入矩阵 $\bm{X}\in\mathbb{R}^{n\times d}$ 结束 while
- en: Appendix B Discussion on the Prompts
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录B：对提示的讨论
- en: '![Refer to caption](img/b925c81f13484b7fc94226181f9fe1be.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b925c81f13484b7fc94226181f9fe1be.png)'
- en: 'Figure 5: Example of GSM8k-CoT prompt. The Red, Green, and Blue colored portions
    correspond to the example question, a common preceding prompt, and the example
    answer prompt, respectively. Here, we use the common prompt to improve the reasoning
    of the LLM.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5: GSM8k-CoT 提示的示例。红色、绿色和蓝色部分分别对应示例问题、常见前置提示和示例回答提示。在这里，我们使用常见提示来提高 LLM 的推理能力。'
- en: 'For the GSM8k dataset, there is a fixed prompt for all evaluations. The prompt
    contains 8 examples with clear guidance step by step. For the MMLU and BBH dataset,
    there are individual prompts for each sub dataset. [Figure 5](#A2.F5 "Figure 5
    ‣ Appendix B Discussion on the Prompts ‣ GEAR: An Efficient KV Cache Compression
    Recipe for Near-Lossless Generative Inference of LLM") shows one of the example
    in GSM8K dataset.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '对于 GSM8k 数据集，所有评估都有固定的提示。提示包含 8 个逐步指导的示例。对于 MMLU 和 BBH 数据集，每个子数据集都有单独的提示。[图
    5](#A2.F5 "图 5 ‣ 附录 B 提示讨论 ‣ GEAR: 一种高效的 KV 缓存压缩方案用于近乎无损的 LLM 生成推理") 显示了 GSM8K
    数据集中的一个示例。'
- en: Appendix C Discussion on Throughput Benefit of GEAR
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 C GEAR 吞吐量收益讨论
- en: In this section, we present the inference throughput benefit of GEAR on a general
    system. Specifically, we evaluate with a general inference system with one single
    100GB GPU and report the practical maximum batch-size and throughput. We set prefill
    length to 1000 and a generation length to 100 (maximum) in this experiment. We
    use LLaMA2-7B model with FP16 weights. The buffer ($n_{b}$ compared to the outlier-reduced
    quantization method yielding similar accuracy.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了 GEAR 在通用系统上的推理吞吐量收益。具体来说，我们使用一个单独的 100GB GPU 的通用推理系统进行评估，并报告实际的最大批量大小和吞吐量。在本实验中，我们将预填充长度设置为
    1000，生成长度设置为 100（最大）。我们使用 LLaMA2-7B 模型，权重为 FP16。缓冲区 ($n_{b}$) 与减少离群值的量化方法相比，精度相似。
- en: 'Table 6: General inference system throughput on one 100GB GPU with LLaMA2-7B.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '表 6: 使用 LLaMA2-7B 在单个 100GB GPU 上的一般推理系统吞吐量。'
- en: '| Method | Max Batch-size | Throughputs (tokens/s) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 最大批量大小 | 吞吐量（tokens/s） |'
- en: '| Outlier-R. Quant (b=4, s=10%) | 62 | 32 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Outlier-R. Quant (b=4, s=10%) | 62 | 32 |'
- en: '| GEAR (b=4, s=2%, r=2%, $n_{b}$=20) | 104 | 92 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| GEAR (b=4, s=2%, r=2%, $n_{b}$=20) | 104 | 92 |'
- en: Appendix D Extended Results
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 D 扩展结果
- en: D.1 Understanding the Importance of K and V Error
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.1 理解 K 和 V 错误的重要性
- en: “Do the quantization error for K and V cache play equally critical role or their
    importance are different?”.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: “K 和 V 缓存的量化误差是否扮演同等重要的角色，还是它们的重要性不同？”。
- en: $\begin{array}[]{cc}\includegraphics[width=199.16928pt]{Figures/Source/llama2_7b_kv_error_ablation.png}&amp;\end{array}$
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: $\begin{array}[]{cc}\includegraphics[width=199.16928pt]{Figures/Source/llama2_7b_kv_error_ablation.png}&amp;\end{array}$
- en: 'Figure 6: Ablation with K and V error tensor rank.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: K 和 V 错误张量秩的消融实验。'
- en: 'To shed light on this intriguing question, we conduct experiments by keeping
    the rank of either K or V frozen to a fixed value, while we keep on increasing
    the rank of other. For both cases, we keep the sparsity to 1% in GEAR. Fig. [6](#A4.F6
    "Figure 6 ‣ D.1 Understanding the Importance of K and V Error ‣ Appendix D Extended
    Results ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM") (a) shows results with LLaMA2-7B on GSM8k-CoT. The [val1, val2],
    identifies the fixed rank and variable $\rho$, respectively. From these results,
    we can confirm that the K error may play a more critical role as opposed to V
    error for complex reasoning tasks like GSM8k where multiple tokens are generated
    in a sequential way, as we see consistently improved performance while increasing
    rank to represent the former.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '为了揭示这个有趣的问题，我们通过将 K 或 V 的秩保持在固定值的同时不断增加另一个的秩来进行实验。在这两种情况下，我们将 GEAR 的稀疏度保持在
    1%。图 [6](#A4.F6 "图 6 ‣ D.1 理解 K 和 V 错误的重要性 ‣ 附录 D 扩展结果 ‣ GEAR: 一种高效的 KV 缓存压缩方案用于近乎无损的
    LLM 生成推理") (a) 显示了 LLaMA2-7B 在 GSM8k-CoT 上的结果。[val1, val2] 分别表示固定秩和变量 $\rho$。从这些结果中，我们可以确认，对于像
    GSM8k 这样的复杂推理任务，K 错误可能比 V 错误更关键，因为当增加秩以表示 K 错误时，性能一致得到改善。'
- en: D.2 GEAR Applied to Different KV Quantization Schemes
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.2 GEAR 应用于不同 KV 量化方案
- en: 'Here we present results of GEAR applied on uniform quantized KV. As demonstrated
    in Fig. [7(a)](#A4.F7.sf1 "In Figure 7 ‣ D.2 GEAR Applied to Different KV Quantization
    Schemes ‣ Appendix D Extended Results ‣ GEAR: An Efficient KV Cache Compression
    Recipe for Near-Lossless Generative Inference of LLM") application of GEAR even
    with uniform quantization as its quantization scheme, the generative inference
    performance significantly improves. We chose uniform quantization for this study,
    as it is one of the simplest quantization that does not require any custom support
    to compensate for operation delay. Note, here we used 6-bit (as our evaluations
    show that 4-bit uniform quantization of KV provides near zero accuracy for complex
    reasoning tasks) quantization scheme with a streaming gap of 20, meaning at max
    20 recent K,V tokens would be in FP16 format. In specific, Fig. [7(a)](#A4.F7.sf1
    "In Figure 7 ‣ D.2 GEAR Applied to Different KV Quantization Schemes ‣ Appendix
    D Extended Results ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless
    Generative Inference of LLM") shows that for LLaMA2-7B GEAR on uniform asymmetric
    quantization improves the accuracy by up to $\textbf{86.58}\%$, clearly demonstrating
    the importance of the GEAR as well as its generalizability across different quantization
    schemes.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '这里展示了GEAR在均匀量化KV上的应用结果。如图 [7(a)](#A4.F7.sf1 "在图7中 ‣ D.2 GEAR应用于不同KV量化方案 ‣ 附录D扩展结果
    ‣ GEAR: 一种高效的KV缓存压缩方案用于近无损生成推理") 所示，即使在均匀量化作为其量化方案的情况下，GEAR的生成推理性能也显著提高。我们选择均匀量化进行这项研究，因为它是最简单的量化方法之一，不需要任何定制支持以弥补操作延迟。注意，这里我们使用了6位量化（因为我们的评估显示4位均匀量化在复杂推理任务中的准确率接近零），量化方案的流媒体间隔为20，意味着最多20个最近的K,V令牌将处于FP16格式。具体而言，图
    [7(a)](#A4.F7.sf1 "在图7中 ‣ D.2 GEAR应用于不同KV量化方案 ‣ 附录D扩展结果 ‣ GEAR: 一种高效的KV缓存压缩方案用于近无损生成推理")
    显示，对于LLaMA2-7B，GEAR在均匀不对称量化下将准确性提高了高达$\textbf{86.58}\%$，清晰地展示了GEAR的重要性以及其在不同量化方案下的普适性。'
- en: 'We further performed experiments to show efficacy of GEAR with group quantization.
    We take inspiration from a contemporary research Liu et al. ([2024](#bib.bib16)),
    that has shown significant improvement with group quantization in yielding improved
    performance on complex reasoning tasks like GSM8k. In specific, we use the group-wise
    quantization of Liu et al. ([2024](#bib.bib16)), namely group channel-wise^*^**Note,
    unlike the channel grouping done over the batch in the main text, here we compute
    the scaling and shifting parameters separately for each batch unit following Liu
    et al. ([2024](#bib.bib16)). and token-wise as the inherent quantization of GEAR.
    As demonstrated in Fig. [7(b)](#A4.F7.sf2 "In Figure 7 ‣ D.2 GEAR Applied to Different
    KV Quantization Schemes ‣ Appendix D Extended Results ‣ GEAR: An Efficient KV
    Cache Compression Recipe for Near-Lossless Generative Inference of LLM"), for
    LLaMA2-7B, with 4-bit quantized KV cache, we show that the GEAR improves accuracy
    of group quantization significantly by up to $\textbf{33.02}\%$.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '我们进一步进行了实验，以展示GEAR在分组量化下的有效性。我们借鉴了当代研究Liu et al. ([2024](#bib.bib16))，该研究显示分组量化在复杂推理任务如GSM8k上显著提高了性能。具体而言，我们使用了Liu
    et al. ([2024](#bib.bib16))的分组通道量化^*^**注意，与主文本中对批次进行的通道分组不同，这里我们根据Liu et al. ([2024](#bib.bib16))的研究为每个批次单元单独计算缩放和位移参数。以及作为GEAR固有量化的令牌级量化。如图
    [7(b)](#A4.F7.sf2 "在图7中 ‣ D.2 GEAR应用于不同KV量化方案 ‣ 附录D扩展结果 ‣ GEAR: 一种高效的KV缓存压缩方案用于近无损生成推理")
    所示，对于LLaMA2-7B，使用4位量化KV缓存，我们显示GEAR显著提高了分组量化的准确性，提升幅度高达$\textbf{33.02}\%$。'
- en: '![Refer to caption](img/c9a918efd4a45a1258e3f6d767170c4d.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c9a918efd4a45a1258e3f6d767170c4d.png)'
- en: (a) With uniform quantization.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 使用均匀量化。
- en: '![Refer to caption](img/1bbba9aad9b635487af3aec68403ed6b.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1bbba9aad9b635487af3aec68403ed6b.png)'
- en: (b) With group quantization.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 使用分组量化。
- en: 'Figure 7: GEAR results with different quantization schemes, namely uniform
    (asymmetric) and group quantization on GSM8k-CoT.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：GEAR在不同量化方案下的结果，即均匀（不对称）和分组量化在GSM8k-CoT上的表现。
- en: D.3 GEAR with Weight Quantized Model
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: D.3 带权重量化模型的GEAR
- en: $\begin{array}[]{cc}\includegraphics[width=227.62204pt]{Figures/Source/gear_with_w8_uniform.png}\end{array}$
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: $\begin{array}[]{cc}\includegraphics[width=227.62204pt]{Figures/Source/gear_with_w8_uniform.png}\end{array}$
- en: 'Figure 8: GEAR results with weight quantized model on GSM8k-CoT. We use LLaMA2-7B
    for this evaluation.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：GEAR在GSM8k-CoT上使用权重量化模型的结果。我们使用LLaMA2-7B进行此评估。
- en: 'In this section, we present results of GEAR with outlier quantization applied
    on models with weights quantized to 8-bits. In specific, we apply 8-bit uniform
    quantization to the model weights and applied GEAR to the KV cache to simulate
    the compression performance. Here we apply GEAR on top of outlier-reduced quantization
    scheme. As shown in Fig. [8](#A4.F8 "Figure 8 ‣ D.3 GEAR with Weight Quantized
    Model ‣ Appendix D Extended Results ‣ GEAR: An Efficient KV Cache Compression
    Recipe for Near-Lossless Generative Inference of LLM"), the results with GEAR
    significantly outperforms the outlier-reduced quantized KV baselines with significantly
    higher outlier $\%$. Additionally, GEAR with weight quantized model can yield
    similar performanace as that with the FP16 weights.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们展示了应用于权重量化为8位的模型的GEAR与离群点量化的结果。具体来说，我们将8位均匀量化应用于模型权重，并将GEAR应用于KV缓存以模拟压缩性能。在此，我们在离群点减少量化方案的基础上应用GEAR。如图[8](#A4.F8
    "Figure 8 ‣ D.3 GEAR with Weight Quantized Model ‣ Appendix D Extended Results
    ‣ GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative
    Inference of LLM")所示，使用GEAR的结果显著优于离群点减少量化的KV基线，并且离群点的$\%$显著更高。此外，使用权重量化模型的GEAR能够达到与FP16权重模型相似的性能。'
