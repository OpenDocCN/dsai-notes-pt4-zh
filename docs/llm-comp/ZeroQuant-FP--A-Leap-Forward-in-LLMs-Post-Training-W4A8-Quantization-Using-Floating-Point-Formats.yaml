- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-08 18:51:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-08 18:51:23'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: 'ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using
    Floating-Point Formats'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 'ZeroQuant-FP: 一项在LLMs后训练W4A8量化中使用浮点格式的飞跃'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.09782](https://ar5iv.labs.arxiv.org/html/2307.09782)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2307.09782](https://ar5iv.labs.arxiv.org/html/2307.09782)
- en: \useunderXiaoxia Wu , Zhewei Yao^∗, Yuxiong He
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \useunderXiaoxia Wu , Zhewei Yao^∗, Yuxiong He
- en: Microsoft
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 微软
- en: '{zheweiyao, xiaoxiawu, yuxhe}@microsoft.com Equal Contribution. Code will be
    released as a part of [https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '{zheweiyao, xiaoxiawu, yuxhe}@microsoft.com 平等贡献。代码将作为[https://github.com/microsoft/DeepSpeed](https://github.com/microsoft/DeepSpeed)的一部分发布。'
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: In the complex domain of large language models (LLMs), striking a balance between
    computational efficiency and maintaining model quality is a formidable challenge.
    Navigating the inherent limitations of uniform quantization, particularly when
    dealing with outliers, and motivated by the launch of NVIDIA’s H100 hardware,
    this study delves into the viability of floating-point (FP) quantization, particularly
    focusing on FP8 and FP4, as a potential solution. Our comprehensive investigation
    reveals that for LLMs, FP8 activation consistently outshines its integer (INT8)
    equivalent, with the performance edge becoming more noticeable in models possessing
    parameters beyond one billion. For weight quantization, our findings indicate
    that FP4 exhibits comparable, if not superior, performance to INT4, simplifying
    deployment on FP-supported hardware like H100\. To mitigate the overhead from
    precision alignment caused by the disparity between weights and activations, we
    propose two scaling constraints for weight quantization that negligibly impact
    the performance compared to the standard W4A8 model. We additionally enhance our
    quantization methods by integrating the Low Rank Compensation (LoRC) strategy,
    yielding improvements especially in smaller models. The results of our investigation
    emphasize the immense potential of FP quantization for LLMs, paving the way for
    high-efficiency deployment in resource-limited settings.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型语言模型（LLMs）的复杂领域中，平衡计算效率和维持模型质量是一个艰巨的挑战。考虑到统一量化的固有限制，特别是在处理异常值时，加上NVIDIA H100硬件的推出，本研究深入探讨了浮点（FP）量化的可行性，特别是FP8和FP4，作为潜在解决方案。我们的全面调查显示，对于LLMs，FP8激活始终优于其整数（INT8）对应物，在参数超过十亿的模型中，性能优势更为明显。对于权重量化，我们的发现表明，FP4表现出与INT4相当甚至更优的性能，简化了在FP支持的硬件（如H100）上的部署。为减少由于权重和激活之间差异造成的精度对齐开销，我们提出了两种权重量化的缩放约束，这些约束对比标准的W4A8模型几乎没有影响性能。我们还通过整合低秩补偿（LoRC）策略来增强我们的量化方法，特别是在较小的模型中取得了改进。我们的研究结果强调了FP量化在LLMs中的巨大潜力，为资源有限的环境中的高效部署铺平了道路。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: As Natural Language Processing (NLP) evolves, Large Language Models (LLMs) like
    Codex [[9](#bib.bib9)] and ChatGPT [[22](#bib.bib22)] have become essential, transforming
    our interaction with technology and daily communication. However, their complexity
    and computational intensity present deployment challenges  [[23](#bib.bib23),
    [8](#bib.bib8), [26](#bib.bib26)], particularly in resource-limited settings.
    One solution is quantization, which represents data in lower-precision formats
    such as 8-bit integers or floating-point numbers, reducing memory needs and potentially
    enhancing inference latency through better GEMM computation throughput on compatible
    GPUs. Post-Training Quantization (PTQ), which directly reduces the precision of
    a fully trained model’s parameters, is often preferred for LLMs due to its simplicity
    and lower computational overhead.¹¹1Note that we do not discuss Quantize-Aware
    Training (QAT) for LLMs in this paper as QAT require the computation graph for
    back-propogation [[11](#bib.bib11), [1](#bib.bib1), [18](#bib.bib18), [31](#bib.bib31),
    [32](#bib.bib32), [4](#bib.bib4), [16](#bib.bib16)]. Recent studies indicate that
    PTQ on 8-bit integer (INT8) weight-only quantization does not compromise the quality
    of LLMs [[34](#bib.bib34), [3](#bib.bib3), [33](#bib.bib33), [29](#bib.bib29)],
    and only a minor accuracy drop is observed with INT4 weight quantization when
    advanced algorithm such as GPTQ applied [[7](#bib.bib7), [35](#bib.bib35), [12](#bib.bib12),
    [15](#bib.bib15)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自然语言处理 (NLP) 的发展，大型语言模型 (LLMs) 如 Codex [[9](#bib.bib9)] 和 ChatGPT [[22](#bib.bib22)]
    已成为不可或缺的，它们改变了我们与技术和日常交流的方式。然而，它们的复杂性和计算强度带来了部署挑战 [[23](#bib.bib23), [8](#bib.bib8),
    [26](#bib.bib26)]，特别是在资源有限的环境中。一个解决方案是量化，它以较低精度格式（如 8 位整数或浮点数）表示数据，从而减少内存需求，并通过提高兼容
    GPU 上 GEMM 计算吞吐量来可能提升推理延迟。由于其简单性和较低的计算开销，后训练量化 (PTQ) 常常被优选用于 LLMs。¹¹ 注意，我们在本文中没有讨论针对
    LLMs 的量化感知训练 (QAT)，因为 QAT 需要计算图进行反向传播 [[11](#bib.bib11), [1](#bib.bib1), [18](#bib.bib18),
    [31](#bib.bib31), [32](#bib.bib32), [4](#bib.bib4), [16](#bib.bib16)]。最近的研究表明，8
    位整数 (INT8) 权重量化的 PTQ 不会影响 LLMs 的质量 [[34](#bib.bib34), [3](#bib.bib3), [33](#bib.bib33),
    [29](#bib.bib29)]，而当应用先进算法如 GPTQ 时，INT4 权重量化只会观察到轻微的准确性下降 [[7](#bib.bib7), [35](#bib.bib35),
    [12](#bib.bib12), [15](#bib.bib15)]。
- en: The exploration of activation quantization, in addition to weight-only quantization,
    has also gained interest. This approach expedites inference times by taking advantage
    of unified precision leading to more efficient execution on hardware. The primary
    challenge in implementing activation quantization lies in the trade-off between
    efficiency and performance. As evidenced in studies such as ZeroQuants [[34](#bib.bib34),
    [35](#bib.bib35)], SmoothQuant [[33](#bib.bib33)] and others, reducing the precision
    of activation from FP16 to INT8 inevitably results in a decrease in model quality.
    This degradation is partially due to the presence of extreme values or outliers
    in the activation of LLMs [[5](#bib.bib5), [33](#bib.bib33), [15](#bib.bib15),
    [12](#bib.bib12)], which is partly attributed to the pretraining effect [[31](#bib.bib31)].
    In the presence of outliers, uniform quantization like INT8 or INT4, fail to accurately
    represent the main body of the data as they become skewed towards the outlier.
    This issue stems from the inherent assumption in these techniques of a uniform
    data distribution [[30](#bib.bib30)], an assumption that might not correspond
    to the actual data points distribution.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 除了仅限于权重量化的激活量化探索也引起了关注。这种方法通过利用统一的精度来加快推理时间，从而在硬件上实现更高效的执行。实施激活量化的主要挑战在于效率与性能之间的权衡。正如研究如
    ZeroQuants [[34](#bib.bib34), [35](#bib.bib35)]、SmoothQuant [[33](#bib.bib33)]
    等所证明的那样，将激活的精度从 FP16 降低到 INT8 无可避免地会导致模型质量下降。这种降级部分是由于 LLMs [[5](#bib.bib5), [33](#bib.bib33),
    [15](#bib.bib15), [12](#bib.bib12)] 的激活中存在极端值或异常值，这部分归因于预训练效果 [[31](#bib.bib31)]。在存在异常值的情况下，像
    INT8 或 INT4 这样的均匀量化无法准确表示数据的主要部分，因为它们倾向于偏向异常值。这个问题源于这些技术对数据分布均匀的固有假设 [[30](#bib.bib30)]，这种假设可能与实际数据点的分布不符。
- en: Considering the drawbacks of integer quantization delineated previously, floating-point
    (FP) methods like FP8 or FP4, employing ExMy notation, arise as more potent alternatives [[20](#bib.bib20),
    [2](#bib.bib2), [13](#bib.bib13), [28](#bib.bib28), [37](#bib.bib37)]. Unlike
    the fixed range of integer types, floating-point methods allow for adjusting the
    decimal point position, enabling dynamic scaling across activation maps and preserving
    important features. While there is debate about the quality of models between
    integer and floating-point quantization [[28](#bib.bib28)], recent research on
    PTQ LLMs using FP8/FP4 in [[37](#bib.bib37)] reveals FP8 to be substantially better
    than INT8 activation quantization. In terms of hardware support and performance,
    while INT8 computations are broadly supported by most modern CPUs and GPUs [[21](#bib.bib21),
    [31](#bib.bib31)], lower-bit floating-point operations are also increasingly recognized
    in the industry. An example of this is the newly release of NVIDIA’s H100 GPU,
    specifically engineered for FP8 computations [[20](#bib.bib20)]. Hence, despite
    the potentially higher computation cost of FP8 compared to INT8 and in light of
    hardware support, the improved model quality could make this trade-off worthwhile
    and merits further exploration.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到之前 delineated 的整数量化缺陷，采用 ExMy 表示法的浮点（FP）方法，如 FP8 或 FP4，成为了更强有力的替代方案 [[20](#bib.bib20),
    [2](#bib.bib2), [13](#bib.bib13), [28](#bib.bib28), [37](#bib.bib37)]。与整数类型的固定范围不同，浮点方法允许调整小数点位置，实现激活图的动态缩放，并保留重要特征。虽然关于整数和浮点量化模型质量的争论仍在继续
    [[28](#bib.bib28)]，但 [[37](#bib.bib37)] 对 PTQ LLMs 使用 FP8/FP4 的最新研究显示，FP8 在激活量化方面明显优于
    INT8。在硬件支持和性能方面，尽管 INT8 计算得到大多数现代 CPU 和 GPU 的广泛支持 [[21](#bib.bib21), [31](#bib.bib31)]，但低比特浮点操作也越来越受到行业认可。比如
    NVIDIA 新发布的 H100 GPU，专门为 FP8 计算设计 [[20](#bib.bib20)]。因此，尽管 FP8 的计算成本可能高于 INT8，并且考虑到硬件支持，改进的模型质量可能使这种权衡值得进一步探索。
- en: 'While a few studies such as the one by [[37](#bib.bib37)] have ventured into
    the realm of post-training FP quantization in LLMs, they have unveiled considerable
    drawbacks in terms of model quality. Specifically, when implementing GPTQ [[7](#bib.bib7)]
    for FP8 quantization on both weights and activation for models such as LLaMA-7B
    or LLaMA-30b [[27](#bib.bib27)], there is an observed perplexity degradation surpassing
    1.0 on Wiki-text2 dataset [[19](#bib.bib19)]. This level of model degradation
    presents significant practicality issues, hindering the optimal utilization of
    these models. In response to these findings, our paper undertakes an in-depth
    exploration into FP quantization. We primarily focus on the variance in activation
    values—an integral element that could potentially be the key to enhancing the
    performance of these quantization techniques. Our main contributions include:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管像 [[37](#bib.bib37)] 这样的研究已经涉足了 LLM 中训练后 FP 量化的领域，但它们揭示了在模型质量方面的显著缺陷。具体而言，当在如
    LLaMA-7B 或 LLaMA-30b [[27](#bib.bib27)] 等模型上实施 GPTQ [[7](#bib.bib7)] 进行 FP8 量化时，在
    Wiki-text2 数据集 [[19](#bib.bib19)] 上观察到的困惑度降级超过了 1.0。这种程度的模型降级带来了显著的实用性问题，阻碍了这些模型的最佳利用。针对这些发现，我们的论文对
    FP 量化进行了深入探索。我们主要关注激活值的变化——这可能是提高这些量化技术性能的关键。我们的主要贡献包括：
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Demonstrating minimal degradation with FP8 activation and weight quantization:
    Particularly in larger models, FP8 activation and weight quantization result in
    negligible degradation, performing comparably to the original FP16 models.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 演示 FP8 激活和权重量化的最小降级：特别是在较大的模型中，FP8 激活和权重量化导致的降级几乎可以忽略，表现与原始 FP16 模型相当。
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Identifying potential in FP8 activation and FP4 weights, and the impact of
    Low Rank Compensation (LoRC): We highlight the potential in FP8 activation and
    FP4 weights. The LoRC method, proposed in [[35](#bib.bib35)], significantly reduces
    quantization errors in the W4A8 scheme for FP quantization, especially in smaller
    models, thereby enhancing performance.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 识别 FP8 激活和 FP4 权重的潜力，以及低秩补偿（LoRC）的影响：我们强调了 FP8 激活和 FP4 权重的潜力。[[35](#bib.bib35)]
    提出的 LoRC 方法显著减少了 FP 量化中的 W4A8 方案的量化误差，尤其是在较小的模型中，从而提高了性能。
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Illustrating the maintenance of quality in the W4A8 floating-point model even
    when constraints are imposed on the scaling factors: For true efficiency in the
    W4A8 model, a conversion from FP4 to FP8 for weight is crucial. To alleviate this
    converting overhead, we here suggest two possible scaling constraints for weight
    quantization: (1) restricting all scaling factors to be a power of 2 and (2) requiring
    the scaling factors in one compute group (e.g., several rows of the weight matrix [[34](#bib.bib34)]
    to be transferable by simple bit-shifting). Our analysis indicates that these
    two restrictions negligibly affect the model’s performance in comparison to the
    conventional W4A8 configuration.'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 说明在施加缩放因子限制时W4A8浮点模型的质量保持情况：为了实现W4A8模型的真正效率，将权重从FP4转换为FP8至关重要。为减轻这种转换开销，我们在这里建议两种可能的权重量化缩放约束：（1）将所有缩放因子限制为2的幂，和（2）要求一个计算组中的缩放因子（例如，权重矩阵的几行[[34](#bib.bib34)]能够通过简单的位移转换）。我们的分析表明，这两种限制对模型性能的影响与传统W4A8配置相比微乎其微。
- en: 2 Background
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: 'The impact of 8-bit activation quantization, especially potential accuracy
    loss, is comprehensively outlined in ZeroQuant-V2 [[35](#bib.bib35)]. They present
    a direct comparison between the W16A16 and W16A8 (INT8) quantization schemes across
    a variety of models. To provide an easier understanding, we quoted their results
    in Table [1](#S2.T1 "Table 1 ‣ 2 Background ‣ ZeroQuant-FP: A Leap Forward in
    LLMs Post-Training W4A8 Quantization Using Floating-Point Formats") for both OPT[[36](#bib.bib36)]
    and BLOOM [[25](#bib.bib25)] models, which indicates that the quality of models,
    especially the OPT family, is significantly influenced by the activation quantization.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '8位激活量化的影响，尤其是潜在的准确性损失，在ZeroQuant-V2 [[35](#bib.bib35)]中有全面的说明。他们展示了W16A16与W16A8
    (INT8)量化方案在各种模型中的直接比较。为了更容易理解，我们在表[1](#S2.T1 "Table 1 ‣ 2 Background ‣ ZeroQuant-FP:
    A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")中引用了他们的结果，适用于OPT[[36](#bib.bib36)]和BLOOM
    [[25](#bib.bib25)]模型，这表明模型的质量，尤其是OPT系列，受到激活量化的显著影响。'
- en: 'Table 1: Comparison of FP16 and INT8 activation quantization. We report the
    average PPL (the lower the better) over Wikitext-2 (WIKI) [[19](#bib.bib19)],
    PTB [[17](#bib.bib17)], and C4 [[24](#bib.bib24)], for both OPT and BLOOM (BLM)
    models.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：FP16和INT8激活量化的比较。我们报告了Wikitext-2 (WIKI) [[19](#bib.bib19)]、PTB [[17](#bib.bib17)]和C4
    [[24](#bib.bib24)]上OPT和BLOOM (BLM)模型的平均PPL（越低越好）。
- en: Precision OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b
    W16-A16 11.90 11.22 10.70 10.33 20.43 17.58 14.96 10.90 W16-A8 12.62 15.36 23.57
    561.35 20.52 17.65 15.14 11.62 ![Refer to caption](img/c64400f03f850bfb687cf633b64ffd0b.png)![Refer
    to caption](img/920aca96e3de280ef872579652bd12c9.png)![Refer to caption](img/14025d8ef0cd17ad63f4272f332142ff.png)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 精度 OPT-6.7b OPT-13b OPT-30b OPT-66b BLM-1.7b BLM-3b BLM-7.1b BLM-176b W16-A16
    11.90 11.22 10.70 10.33 20.43 17.58 14.96 10.90 W16-A8 12.62 15.36 23.57 561.35
    20.52 17.65 15.14 11.62 ![参见说明](img/c64400f03f850bfb687cf633b64ffd0b.png)![参见说明](img/920aca96e3de280ef872579652bd12c9.png)![参见说明](img/14025d8ef0cd17ad63f4272f332142ff.png)
- en: 'Figure 1: Distribution of Activation values. The top, middle and bottom rows
    represents the distributions at the 2nd, 12th and final layer of the pretrained
    OPT-1.3b model. From the left to right columns, they are respectively for the
    linear modules attn.q_proj (same as attn.k_proj and attn.v_proj), attn.out_proj,
    fc1, and fc2\. The histogram’s x-axis ranges from the smallest to largest activation
    values, while the y-axis denotes their frequency in the dataset. See legend for
    their minimum and maximum values. Density functions illustrate the probability
    of different activation values. For more details, please see Section [2](#S2 "2
    Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization
    Using Floating-Point Formats").'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1：激活值的分布。顶部、中部和底部行分别表示预训练OPT-1.3b模型第2层、第12层和最终层的分布。从左到右的列分别对应线性模块attn.q_proj（与attn.k_proj和attn.v_proj相同）、attn.out_proj、fc1和fc2。直方图的x轴范围从最小激活值到最大激活值，而y轴表示它们在数据集中的频率。有关最小值和最大值，请参见图例。密度函数展示了不同激活值的概率。有关更多详细信息，请参见第[2](#S2
    "2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization
    Using Floating-Point Formats")节。'
- en: 'Distribution of Activations. We sought to understand the cause of the aforementioned
    degradation from FP16 activation and INT8, prompting us to scrutinize the distribution
    of activation values illustrated in Figure [1](#S2.F1 "Figure 1 ‣ 2 Background
    ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point
    Formats"). We selected a random sentence from the C4 dataset and processed it
    through a pre-trained OPT-1.3B model. The statistical activation inputs for the
    2nd, middle, and final layer were subsequently chosen for a detailed examination.
    The four histograms correspondingly represent the activations for the Multi-head
    Attention (MHA) and Multi-Layer Perceptron (MLP) components:²²2The hidden dimension
    for the model is 2048 for ‘attn.q_proj’, ‘attn.out_proj’ and ‘fc1’, and 8196 for
    ‘fc2’. We pick 20 tokens (position 8 to 28) and vectorize this $20\times 2048$
    matrices to plot their distributions. The plots used bin=100.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '激活分布。我们试图了解上述 FP16 激活和 INT8 退化的原因，这促使我们仔细审查图示中激活值的分布，如图 [1](#S2.F1 "Figure
    1 ‣ 2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization
    Using Floating-Point Formats") 所示。我们从 C4 数据集中选择了一个随机句子，并通过预训练的 OPT-1.3B 模型进行了处理。随后选择了第
    2 层、中间层和最终层的统计激活输入进行详细检查。这四个直方图分别代表了 Multi-head Attention (MHA) 和 Multi-Layer
    Perceptron (MLP) 组件的激活：²²2 模型的隐藏维度为 2048（‘attn.q_proj’、‘attn.out_proj’ 和 ‘fc1’），以及
    8196（‘fc2’）。我们选择了 20 个标记（位置 8 到 28），将这 $20\times 2048$ 矩阵向量化以绘制其分布图。所用图表的 bin=100。'
- en: •
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: attn.q_proj, the input for the query, key, or value in the MHA mechanism,
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: attn.q_proj，即 MHA 机制中用于查询、键或值的输入，
- en: •
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: attn.out_proj, the input for the MHA’s projection matrices,
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: attn.out_proj，即 MHA 的投影矩阵输入，
- en: •
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: fc1, the initial input for the fully-connected (fc1) projection in MLP,
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: fc1，即 MLP 中全连接 (fc1) 投影的初始输入，
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: fc2, the subsequent input for the fully-connected (fc2) projection in the MLP.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: fc2，即 MLP 中全连接 (fc2) 投影的后续输入。
- en: 'The activation distribution outlined in Figure [1](#S2.F1 "Figure 1 ‣ 2 Background
    ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point
    Formats") reveals some compelling patterns. The input to the attn.q_proj module
    in the 2nd layer (depicted in the 1st column of Figure [1](#S2.F1 "Figure 1 ‣
    2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization
    Using Floating-Point Formats") in the top row) appears to conform closely to a
    normal distribution, a result of the layer-normalization process. Yet, moving
    forward to the subsequent modules within the 2nd layer, namely attn.out_proj,
    fc1, and fc2, we notice a skewness in the distributions, with noticeable outlier
    values. Two distinct observations arise: (1) Regarding the activation distribution
    for attn.q_proj and fc1, even though they have undergone layer-normalization,
    the skewness still presents itself and becomes more conspicuous as we delve deeper
    into the layers (see the first and third column in the middle and bottom plots
    in Figure [1](#S2.F1 "Figure 1 ‣ 2 Background ‣ ZeroQuant-FP: A Leap Forward in
    LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")). (2) The
    skewness reaches its peak in the fc2 module. In this particular module, a large
    portion of the values cluster around zero, with only a handful surpassing this
    range. This phenomenon is due to the inputs being processed by the "ReLU" (Rectified
    Linear Unit) operator. This operator, purposefully, voids any negative input values,
    resulting in a skewed distribution focused around zero. Only positive activation
    values persist unmodified, giving rise to the outliers observed. This extreme
    skewness is most noticeable at the final layer (the bottom row in Figure [1](#S2.F1
    "Figure 1 ‣ 2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training
    W4A8 Quantization Using Floating-Point Formats")).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [1](#S2.F1 "图 1 ‣ 2 背景 ‣ ZeroQuant-FP：LLMs 后训练 W4A8 量化使用浮点格式的进步") 中描述的激活分布揭示了一些引人注目的模式。第二层的
    attn.q_proj 模块的输入（如图 [1](#S2.F1 "图 1 ‣ 2 背景 ‣ ZeroQuant-FP：LLMs 后训练 W4A8 量化使用浮点格式的进步")
    顶行中的第一列所示）似乎与正态分布密切一致，这是层归一化过程的结果。然而，当我们向前移动到第二层内的后续模块，即 attn.out_proj、fc1 和 fc2
    时，我们注意到分布的偏斜，具有明显的离群值。出现了两个不同的观察结果：（1）关于 attn.q_proj 和 fc1 的激活分布，尽管它们已经经过层归一化，但偏斜仍然存在，并且随着我们深入层次，偏斜变得更加显著（参见图 [1](#S2.F1
    "图 1 ‣ 2 背景 ‣ ZeroQuant-FP：LLMs 后训练 W4A8 量化使用浮点格式的进步") 中间和底部图中的第一列和第三列）。 （2）偏斜在
    fc2 模块中达到了峰值。在这个特定模块中，大部分值集中在零附近，只有少数值超过这个范围。这一现象是由于输入经过了“ReLU”（整流线性单元）运算符。该运算符故意抛弃了所有负输入值，导致围绕零的偏斜分布。只有正激活值保持不变，导致了观察到的离群值。这种极端偏斜在最终层（图 [1](#S2.F1
    "图 1 ‣ 2 背景 ‣ ZeroQuant-FP：LLMs 后训练 W4A8 量化使用浮点格式的进步") 中的底行）最为显著。
- en: These observations offer a deeper understanding of how activation quantization
    impacts various modules, even within the same layer. Consequently, this signifies
    that we must exercise caution when selecting quantization methods. Quantization
    techniques that employ integers, such as INT8 or INT4, and rely on uniform quantization,
    may not be ideally suited to manage distributions that are skewed. This is due
    to the inherent assumption of uniform distribution within these methods, which
    may not align with the actual distribution of data points.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这些观察提供了对激活量化如何影响各种模块的更深入理解，即使在同一层内。因此，这意味着我们在选择量化方法时必须谨慎。使用整数的量化技术，如 INT8 或
    INT4，并依赖于统一量化，可能不适合处理偏斜的分布。这是因为这些方法内在地假设数据分布是均匀的，这可能与实际的数据点分布不一致。
- en: The Uniform Quantization of INT8. The integer quantization such as in INT8 or
    INT4 states
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: INT8 的统一量化。诸如 INT8 或 INT4 的整数量化状态
- en: '|  | $\small Q(x)=\text{INT}\big{(}{(x-Z)}/{S}\big{)}-Z,$ |  | (1) |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  | $\small Q(x)=\text{INT}\big{(}{(x-Z)}/{S}\big{)}-Z,$ |  | (1) |'
- en: where $Q$) quantization. In scenarios where outliers exist, uniform quantization
    techniques like INT8 and INT4, regardless of their symmetric or asymmetric variants,
    frequently fail to accurately approximate the values of clustered data. Consequently,
    this makes the quantization error larger for those clustered values, as these
    methods attempt to adjust their fit to accommodate the outlier. Essentially, these
    techniques become skewed towards the outlier, leading to a reduced accuracy in
    representing the main body of the data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在异常值的情况下，均匀量化技术如INT8和INT4，无论是对称还是非对称变体，通常无法准确近似聚集数据的值。因此，这使得这些聚集值的量化误差较大，因为这些方法试图调整其拟合以适应异常值。实际上，这些技术会偏向异常值，从而导致表示数据主体的准确性降低。
- en: '![Refer to caption](img/8d22486f19b9c043f4bb1f5b2a0b36cc.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8d22486f19b9c043f4bb1f5b2a0b36cc.png)'
- en: 'Figure 2: A Contrast between INT8 and FP8 Quantization Methods. The top row
    displays the original vector in its full-precision form. The subsequent row showcases
    the vector after quantization through the INT8 Asymmetric approach. The final
    two rows present values quantized by the FP8 method, utilizing E5M2 and E4M3 formats
    respectively.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：INT8和FP8量化方法的对比。顶部行显示了原始向量的全精度形式。接下来的行展示了通过INT8非对称方法量化后的向量。最后两行分别展示了通过FP8方法量化的值，使用了E5M2和E4M3格式。
- en: Given the limitations of integer quantization, floating-point methods such as
    FP8 or FP4, utilizing ExMy notation, emerge as superior alternatives. In these
    methods, the ‘x’ and ‘y’ values represent the bits allocated for the exponent
    and mantissa, respectively, totaling to 7 in FP8 or 3 in FP4\. The flexibility
    of FP8 lies in its ability to adjust the decimal point position, unlike integer
    types with a fixed range.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于整数量化的局限性，浮点方法如FP8或FP4，利用ExMy符号，作为更优的替代方案。在这些方法中，‘x’和‘y’值分别表示分配给指数和尾数的位，总计为FP8中的7位或FP4中的3位。FP8的灵活性在于其能够调整小数点位置，与固定范围的整数类型不同。
- en: 'To demonstrate the disparity between INT8 and FP8, we present Figure [2](#S2.F2
    "Figure 2 ‣ 2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training
    W4A8 Quantization Using Floating-Point Formats") where a hypothetical 15-element
    vector with an outlier value of 100 undergoes quantization using INT8 Asymmetric
    and FP8 (with both E5M2 and E4M3 configurations). Figure [2](#S2.F2 "Figure 2
    ‣ 2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization
    Using Floating-Point Formats") illustrates that while INT8 approximates the outlier
    effectively, it struggles to accurately represent smaller numbers. Conversely,
    FP8 (whether with E5M2 or E4M3) provides greater precision in approximating the
    clustered data.³³3Please note that the FP8 format used in this paper is based
    on the Qtorch Python package, which can be installed via ‘pip install qtorch’.
    It differs slightly from Nvidia’s FP8 in H100, which requires one mantissa bit-pattern
    for NaN values.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '为了展示INT8和FP8之间的差异，我们展示了图 [2](#S2.F2 "Figure 2 ‣ 2 Background ‣ ZeroQuant-FP:
    A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")，其中一个假设的15元素向量具有异常值100，经过INT8非对称量化和FP8（包括E5M2和E4M3配置）的处理。图 [2](#S2.F2
    "Figure 2 ‣ 2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training
    W4A8 Quantization Using Floating-Point Formats") 表明，虽然INT8有效地近似了异常值，但在准确表示较小的数字时存在困难。相反，无论是E5M2还是E4M3配置的FP8，都能在近似聚集数据方面提供更高的精度。³³3
    请注意，本论文使用的FP8格式基于Qtorch Python包，可以通过‘pip install qtorch’进行安装。它与Nvidia在H100中的FP8略有不同，后者需要一个用于NaN值的尾数位模式。'
- en: Considering the advantages of ExMy, which allows for dynamic scaling across
    activation maps, quantization error is reduced and essential features are preserved.
    In this paper, we investigate the performance of FP8 or FP4 techniques for handling
    the variability in activation or weight values. This could potentially lead to
    an enhancement in the model’s performance on post-training quantization.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到ExMy的优点，它允许在激活图之间进行动态缩放，从而减少了量化误差并保留了关键特征。在本文中，我们研究了FP8或FP4技术在处理激活或权重值变异性方面的表现。这可能会导致模型在训练后量化中的性能提升。
- en: 3 Methodology
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 方法论
- en: Several lightweight optimization-based methods, where the weight of the model
    is updated during quantization, have been proposed in the literature [[34](#bib.bib34),
    [35](#bib.bib35), [5](#bib.bib5), [33](#bib.bib33), [15](#bib.bib15), [12](#bib.bib12)].
    Among these, we chose to align our approach with the principles outlined in the
    GPTQ [[7](#bib.bib7), [6](#bib.bib6)], which can be dated back to [[14](#bib.bib14),
    [10](#bib.bib10)]. While this strategy offers a robust starting point, it is imperative
    to keep in mind the dynamic and ever-evolving nature of the field of artificial
    intelligence. There may be more efficient methodologies on the horizon, waiting
    to be discovered and implemented.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了几种轻量化优化方法，其中模型的权重在量化过程中被更新 [[34](#bib.bib34), [35](#bib.bib35), [5](#bib.bib5),
    [33](#bib.bib33), [15](#bib.bib15), [12](#bib.bib12)]。在这些方法中，我们选择将我们的方法与 GPTQ
    [[7](#bib.bib7), [6](#bib.bib6)] 中概述的原则对齐，这可以追溯到 [[14](#bib.bib14), [10](#bib.bib10)]。虽然这一策略提供了一个坚实的起点，但必须牢记人工智能领域的动态和不断发展的特性。可能会有更高效的方法在前方等待被发现和实施。
- en: 'In light of ZeroQuant-V2 [[35](#bib.bib35)], we applied fine-grained quantization
    (FGQ) for weight and token-wise quantization for activation. In addition, we will
    also investigate the add-on feature LoRC (Low Rank Compensation) proposed in [[35](#bib.bib35)],
    which aims to reduce quantization errors in weights by employing low-rank matrix
    factorization. LoRC involves two main steps: first, it performs Singular Value
    Decomposition (SVD) on the error matrix, which is the difference between the original
    weight and the quantized weight. The error matrix is thus decomposed into two
    unitary matrices and a diagonal matrix. Second, the method formulates a new error
    approximation using two low-rank matrices that are derived from the matrices in
    the first step. This approximation is then added to the quantized weight to yield
    a more accurate estimate of the original weight, thereby reducing quantization
    errors.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 结合 ZeroQuant-V2 [[35](#bib.bib35)]，我们对权重应用了细粒度量化（FGQ）和对激活进行逐标记量化。此外，我们还将调查 [[35](#bib.bib35)]
    中提出的附加特性 LoRC（低秩补偿），旨在通过使用低秩矩阵分解来减少权重量化误差。LoRC 涉及两个主要步骤：首先，对误差矩阵（即原始权重与量化权重之间的差异）进行奇异值分解（SVD）。误差矩阵被分解为两个单位矩阵和一个对角矩阵。其次，该方法使用从第一步中的矩阵推导出的两个低秩矩阵来制定新的误差近似。这种近似被添加到量化权重中，以获得更准确的原始权重估计，从而减少量化误差。
- en: Based on GPTQ (without or with LoRC), we perform comprehensive comparisons between
    the use of FP8 or INT8 activation quantization, coupled with adjusting the weight
    quantization to FP8 and FP4\. Particularly we explore the potential of FP4 weight
    and FP8 activation quantization.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 GPTQ（无 LoRC 或有 LoRC），我们对 FP8 或 INT8 激活量化的使用进行了全面比较，并调整了权重量化为 FP8 和 FP4。特别地，我们探索了
    FP4 权重和 FP8 激活量化的潜力。
- en: 'Casting the FP4 to FP8. Lastly, a unique challenge arises due to the use of
    different precision levels for weights (W) and activations (A). The actual software
    implementation of W4A8 in H100 NVIDIA hardware is that one needs to cast W’s FP4
    to match the FP8 precision used in A. The direct method of dequantization followed
    by quantization again could potentially have a detrimental effect on inference
    efficiency, hence it is not a viable solution. To address this, we propose the
    bit-shifting method. This means that instead of allowing $S$ could still represent
    fractions when n is negative and whole numbers when n is not negative). There
    are two methods we will implement:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 将 FP4 转换为 FP8。最后，由于权重（W）和激活（A）使用不同的精度级别，出现了独特的挑战。在 H100 NVIDIA 硬件上实际的软件实现中，W4A8
    需要将 W 的 FP4 转换以匹配 A 中使用的 FP8 精度。直接的去量化再量化方法可能对推理效率产生不利影响，因此不是一个可行的解决方案。为了解决这个问题，我们提出了位移方法。这意味着而不是让
    $S$ 在 n 为负数时仍然表示小数，而在 n 为非负数时表示整数）。我们将实施两种方法：
- en: (M1)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （M1）
- en: Map to the nearest values represented by the power of 2, i.e., letting the new
    scale $\hat{S}=2^{\lceil\log_{2}(S)\rceil}$;
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 映射到由 2 的幂表示的最接近值，即，让新的尺度 $\hat{S}=2^{\lceil\log_{2}(S)\rceil}$；
- en: (M2)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （M2）
- en: Collect scales to form a vector $\mathbf{S}=[S_{1},S_{2},\ldots,S_{n}]$. This
    provides a far superior approximation compared to (M1).⁴⁴4To ensure the casting
    of F4-E2M1 for each weight matrix to FP8, we apply format E5M2 once a matrix is
    quantized.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 收集尺度以形成一个向量 $\mathbf{S}=[S_{1},S_{2},\ldots,S_{n}]$。这提供了比（M1）更优越的近似。为了确保每个权重矩阵的
    F4-E2M1 转换为 FP8，我们在矩阵量化后应用格式 E5M2。
- en: We reiterate that this restriction using the power of 2, either using (M1) or
    (M2), simplifies computations, especially in digital systems operating based on
    binary logic. This is a crucial element of our approach to optimizing computational
    efficiency and maintaining the performance of our model.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重申，使用2的幂（无论是使用（M1）还是（M2））来进行限制，简化了计算，特别是在基于二进制逻辑的数字系统中。这是我们优化计算效率和保持模型性能的方法中的一个关键要素。
- en: 4 Main Results
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 主要结果
- en: 'Table 2: The evaluation outcomes for LLaMA (top) and OPT (bottom ) using different
    Integer (INT) and Floating-point (FP) quantization methods applied to weight and
    activation. The performance is measured in terms of perplexity (lower scores are
    better) and spans across three datasets: WikiText-2 (WIKI), PTB, and C4\. For
    each model, the results initially highlight the average performance across the
    datasets, followed by a detailed breakdown of outcomes per dataset.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：LLaMA（上）和OPT（下）在不同的整数（INT）和浮点（FP）量化方法应用于权重和激活后的评估结果。性能通过困惑度（得分越低越好）来衡量，并涵盖了三个数据集：WikiText-2（WIKI）、PTB
    和 C4。对于每个模型，结果首先突出显示了跨数据集的平均性能，然后是每个数据集的详细结果分解。
- en: Q-type Weight- LLaMA-3b LLaMA-7b LLaMA-13b LLaMA-30b -Activation Mean WIKI/PTB/C4
    Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 W16A16 N/A 11.93 7.35/19.1/9.34
    13.37 5.68/27.35/7.78 10.31 5.09/19.22/6.61 5.79 4.10/7.30/5.98 W8A8 INT – INT
    12.00 7.41/19.16/9.41 13.58 5.72/27.89/7.13 10.63 5.16/20.07/6.67 5.90 4.21/7.42/6.06
    INT – FP 11.96 7.37/19.16/9.35 13.45 5.69/27.57/7.09 10.38 5.11/19.42/6.62 5.80
    4.11/7.31/5.99 FP – FP 11.99 7.37/19.23/9.37 13.46 5.70/27.58/7.10 10.38 5.11/19.41/6.62
    5.81 4.12/7.31/5.99 W4A8 INT – INT 12.55 7.67/20.23/9.74 16.23 6.44/34.45/7.79
    11.48 5.32/22.35/6.78 6.02 4.36/7.54/6.16 INT – FP 12.39 7.62/19.87/9.68 16.09
    6.75/33.80/7.72 11.31 5.28/21.91/6.73 5.94 4.27/7.45/6.11 FP – FP 12.45 7.62/20.05/9.67
    15.14 6.32/31.61/7.51 11.08 5.26/21.27/6.73 5.92 4.26/7.42/6.09 W4A8 INT – INT
    12.52 7.65/20.18/9.72 14.14 5.88/29.26/7.27 10.81 5.28/20.38/6.76 6.00 4.34/7.51/6.14
    +LoRC INT – FP 12.38 7.58/19.89/9.65 14.01 5.84/28.95/7.24 10.56 5.22/19.75/6.71
    5.90 4.24/7.39/6.07 FP – FP 12.42 7.61/19.98/9.66 13.95 5.87/28.75/7.24 10.80
    5.24/20.46/6.72 5.91 4.26/7.40/6.07  Q-type Weight – OPT-3b OPT-7b OPT-13b OPT-30b
    – Activation Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4
    W16A16 N/A 15.44 14.62/16.97/14.72 11.90 10.86/13.09/11.74 11.22 10.13/12.34/11.20
    10.70 9.56/11.84/10.69 W8A8 INT – INT 15.94 14.98/17.49/15.36 12.66 11.20/14.29/12.48
    15.94 12.13/19.82/15.86 25.76 14.63/32.90/29.74 INT – FP 15.85 14.93/17.56/15.05
    11.99 10.92/13.24/11.80 11.27 10.16/12.42/11.23 10.69 9.51/11.87/10.71 FP – FP
    15.86 14.97/17.55/15.05 11.99 10.91/13.24/11.81 11.27 10.16/12.42/11.23 10.69
    9.51/11.87/10.71 W4A8 INT – INT 16.41 15.39/18.22/15.62 13.18 11.61/15.00/12.92
    16.70 12.32/21.21/16.56 24.42 14.80/30.38/28.09 INT – FP 16.40 15.46/18.23/15.51
    12.20 11.13/13.49/11.99 11.34 10.20/12.53/11.30 10.73 9.54/11.91/10.75 FP – FP
    16.29 15.32/18.19/15.35 12.09 10.89/13.44/11.95 11.34 10.16/12.55/11.30 10.72
    9.52/11.90/10.75 W4A8 INT – INT 16.38 15.50/18.05/15.59 12.75 11.37/14.33/12.53
    15.89 12.06/19.76/15.85 27.20 15.94/34.50/31.16 +LoRC INT – FP 16.23 15.40/17.97/15.32
    12.13 11.07/13.43/11.90 11.34 10.23/12.49/11.29 10.71 9.48/11.91/10.74 FP – FP
    16.23 15.50/17.92/15.28 12.09 10.96/13.40/11.90 11.33 10.15/12.55/11.29 10.71
    9.48/11.90/10.75
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Q-type 权重 - LLaMA-3b LLaMA-7b LLaMA-13b LLaMA-30b - 激活 平均 WIKI/PTB/C4 平均 WIKI/PTB/C4
    平均 WIKI/PTB/C4 平均 WIKI/PTB/C4 W16A16 不适用 11.93 7.35/19.1/9.34 13.37 5.68/27.35/7.78
    10.31 5.09/19.22/6.61 5.79 4.10/7.30/5.98 W8A8 INT – INT 12.00 7.41/19.16/9.41
    13.58 5.72/27.89/7.13 10.63 5.16/20.07/6.67 5.90 4.21/7.42/6.06 INT – FP 11.96
    7.37/19.16/9.35 13.45 5.69/27.57/7.09 10.38 5.11/19.42/6.62 5.80 4.11/7.31/5.99
    FP – FP 11.99 7.37/19.23/9.37 13.46 5.70/27.58/7.10 10.38 5.11/19.41/6.62 5.81
    4.12/7.31/5.99 W4A8 INT – INT 12.55 7.67/20.23/9.74 16.23 6.44/34.45/7.79 11.48
    5.32/22.35/6.78 6.02 4.36/7.54/6.16 INT – FP 12.39 7.62/19.87/9.68 16.09 6.75/33.80/7.72
    11.31 5.28/21.91/6.73 5.94 4.27/7.45/6.11 FP – FP 12.45 7.62/20.05/9.67 15.14
    6.32/31.61/7.51 11.08 5.26/21.27/6.73 5.92 4.26/7.42/6.09 W4A8 INT – INT 12.52
    7.65/20.18/9.72 14.14 5.88/29.26/7.27 10.81 5.28/20.38/6.76 6.00 4.34/7.51/6.14
    +LoRC INT – FP 12.38 7.58/19.89/9.65 14.01 5.84/28.95/7.24 10.56 5.22/19.75/6.71
    5.90 4.24/7.39/6.07 FP – FP 12.42 7.61/19.98/9.66 13.95 5.87/28.75/7.24 10.80
    5.24/20.46/6.72 5.91 4.26/7.40/6.07 Q-type 权重 – OPT-3b OPT-7b OPT-13b OPT-30b
    – 激活 平均 WIKI/PTB/C4 平均 WIKI/PTB/C4 平均 WIKI/PTB/C4 平均 WIKI/PTB/C4 W16A16 不适用 15.44
    14.62/16.97/14.72 11.90 10.86/13.09/11.74 11.22 10.13/12.34/11.20 10.70 9.56/11.84/10.69
    W8A8 INT – INT 15.94 14.98/17.49/15.36 12.66 11.20/14.29/12.48 15.94 12.13/19.82/15.86
    25.76 14.63/32.90/29.74 INT – FP 15.85 14.93/17.56/15.05 11.99 10.92/13.24/11.80
    11.27 10.16/12.42/11.23 10.69 9.51/11.87/10.71 FP – FP 15.86 14.97/17.55/15.05
    11.99 10.91/13.24/11.81 11.27 10.16/12.42/11.23 10.69 9.51/11.87/10.71 W4A8 INT
    – INT 16.41 15.39/18.22/15.62 13.18 11.61/15.00/12.92 16.70 12.32/21.21/16.56
    24.42 14.80/30.38/28.09 INT – FP 16.40 15.46/18.23/15.51 12.20 11.13/13.49/11.99
    11.34 10.20/12.53/11.30 10.73 9.54/11.91/10.75 FP – FP 16.29 15.32/18.19/15.35
    12.09 10.89/13.44/11.95 11.34 10.16/12.55/11.30 10.72 9.52/11.90/10.75 W4A8 INT
    – INT 16.38 15.50/18.05/15.59 12.75 11.37/14.33/12.53 15.89 12.06/19.76/15.85
    27.20 15.94/34.50/31.16 +LoRC INT – FP 16.23 15.40/17.97/15.32 12.13 11.07/13.43/11.90
    11.34 10.23/12.49/11.29 10.71 9.48/11.91/10.74 FP – FP 16.23 15.50/17.92/15.28
    12.09 10.96/13.40/11.90 11.33 10.15/12.55/11.29 10.71 9.48/11.90/10.75
- en: 'In this section, we perform experiments to understand the differences of Integer
    (INT) and Floating-point (FP) quantization using the GPTQ methods [[7](#bib.bib7)]
    with or without the add-on feature LoRC [[35](#bib.bib35)]. As described in Section [2](#S2
    "2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization
    Using Floating-Point Formats"), floating-point quantization could potentially
    maintain more precise information, which might improve the model’s performance.
    To see if this is true, we include two model-type families: LLaMA [[27](#bib.bib27)]
    and OPT [[36](#bib.bib36)], with sizes ranging from 1 billion to 30 billion parameters.
    The evaluation spans across three datasets: Wikitext-2 (WIKI) [[19](#bib.bib19)],
    PTB [[17](#bib.bib17)], and C4 [[24](#bib.bib24)]. For more experiment details,
    please see Appendix [A](#A1 "Appendix A Experiment Details ‣ ZeroQuant-FP: A Leap
    Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们进行实验以了解使用 GPTQ 方法[[7](#bib.bib7)] 进行整数 (INT) 和浮点 (FP) 量化的差异，无论是否附加了
    LoRC 功能[[35](#bib.bib35)]。正如第[2](#S2 "2 Background ‣ ZeroQuant-FP: A Leap Forward
    in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")节所述，浮点量化可能保持更精确的信息，这可能提高模型性能。为了验证这一点，我们包括了两种模型类型系列：LLaMA[[27](#bib.bib27)]
    和 OPT[[36](#bib.bib36)]，规模范围从 10 亿到 300 亿参数。评估跨越三个数据集：Wikitext-2 (WIKI)[[19](#bib.bib19)]，PTB[[17](#bib.bib17)]，和
    C4[[24](#bib.bib24)]。有关更多实验细节，请参见附录[A](#A1 "Appendix A Experiment Details ‣ ZeroQuant-FP:
    A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")。'
- en: 'The primary results in Table [2](#S4.T2 "Table 2 ‣ 4 Main Results ‣ ZeroQuant-FP:
    A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")
    reveal the impact of various quantization types which are applied to weight and
    activation specified in the 2nd column; for instance, W4A8 precision, INT – FP
    means INT4 is used for weight and FP8 for activation. Our results provide an average
    performance over three datasets, offering a broad understanding of the quantization
    method’s efficiency. However, we delve further, understanding that these methods’
    performance can differ with the characteristics of datasets, thus presenting a
    detailed performance breakdown for each dataset. We find that FP8 and FP4, the
    configurations E4M3 and E2M1 respectively outperform E5M2 and E3M0, hence, they
    were used in our experiments. Further insights and explanations regarding these
    configurations’ impact on performance are to be addressed in Appendix [A](#A1
    "Appendix A Experiment Details ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training
    W4A8 Quantization Using Floating-Point Formats").'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '表[2](#S4.T2 "Table 2 ‣ 4 Main Results ‣ ZeroQuant-FP: A Leap Forward in LLMs
    Post-Training W4A8 Quantization Using Floating-Point Formats")中的主要结果揭示了应用于第二列所指定的权重和激活的各种量化类型的影响；例如，W4A8
    精度，INT – FP 意味着使用 INT4 进行权重量化，而 FP8 进行激活量化。我们的结果提供了三种数据集的平均性能，广泛了解量化方法的效率。然而，我们*深入探讨*，认识到这些方法的性能可能会因数据集的特性而有所不同，因此对每个数据集呈现了详细的性能分解。我们发现
    FP8 和 FP4，即配置 E4M3 和 E2M1，分别优于 E5M2 和 E3M0，因此它们在我们的实验中得到了应用。关于这些配置对性能影响的进一步见解和解释将在附录[A](#A1
    "Appendix A Experiment Details ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training
    W4A8 Quantization Using Floating-Point Formats")中讨论。'
- en: 'FP8 Activation is much better than INT8. The high-level summary of the results
    in Table [2](#S4.T2 "Table 2 ‣ 4 Main Results ‣ ZeroQuant-FP: A Leap Forward in
    LLMs Post-Training W4A8 Quantization Using Floating-Point Formats") indicates
    that for both LLaMA and OPT model families, FP8 activation generally outperforms
    INT8 activation. This observation corroborates the motivation discussed in Section
    [2](#S2 "2 Background ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8
    Quantization Using Floating-Point Formats"), emphasizing FP8’s superior capacity
    to capture more nuanced information, a vital aspect for generative tasks in large-scale
    LLMs.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 'FP8 激活明显优于 INT8。表[2](#S4.T2 "Table 2 ‣ 4 Main Results ‣ ZeroQuant-FP: A Leap
    Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")中的高级总结表明，对于
    LLaMA 和 OPT 模型系列，FP8 激活通常优于 INT8 激活。这一观察验证了在第[2](#S2 "2 Background ‣ ZeroQuant-FP:
    A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")节讨论的动机，强调了
    FP8 捕捉更细微信息的优越能力，这是大规模 LLMs 中生成任务的关键方面。'
- en: Interestingly, the advantage of FP8 over INT8 becomes more pronounced for larger
    models with parameters greater than 6.7 billion, such as LLaMA-7b/13b and OPT-6.7b/13b.
    For instance, when considering LLaMA-7b, shifting from INT to FP quantization
    in the W8A8 configuration leads to an additional 0.25 PPL reduction (from 10.63
    to 10.38), and in the W4A8 setup, there is an extra 0.4 PPL drop (from 11.48 to
    11.08). These performance gains are significant, considering all other optimization
    parameters remain constant, and they align with the Class-3 quantization sensitivity
    category as defined in [[35](#bib.bib35)]. Thus, the results underline the importance
    of FP8 activation, particularly in larger LLMs, to enhance the overall performance
    and precision of the model’s outputs.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，对于参数大于 67 亿的较大模型（如 LLaMA-7b/13b 和 OPT-6.7b/13b），FP8 相较于 INT8 的优势变得更加明显。例如，在考虑
    LLaMA-7b 时，从 INT 量化转向 FP 量化的 W8A8 配置导致额外减少 0.25 PPL（从 10.63 降到 10.38），在 W4A8 设置下，额外减少
    0.4 PPL（从 11.48 降到 11.08）。这些性能提升是显著的，考虑到其他优化参数保持不变，并且符合 [[35](#bib.bib35)] 定义的
    Class-3 量化敏感性类别。因此，这些结果强调了 FP8 激活的重要性，特别是在较大的 LLM 中，以提高模型输出的整体性能和精度。
- en: 'FP8 weights rival INT8, while FP4 weights potentially outperform INT4. From
    Table [2](#S4.T2 "Table 2 ‣ 4 Main Results ‣ ZeroQuant-FP: A Leap Forward in LLMs
    Post-Training W4A8 Quantization Using Floating-Point Formats"), we observe comparable
    performances between INT8 and FP8 weight quantization across various models and
    datasets, when keeping activation at FP8\. This probably due to we used FGQ on
    weight quantization. Interestingly, when weight quantization is lowered, FP4 exhibits
    certain advantages over INT4, particularly evident in LLaMA-7b (15.14 to 16.09)
    and LLaMA-13b models (11.08 to 11.31). Specifically, under the W4A8 configuration
    for LLaMA-7b, we see 0.95 improvement of FP4 over INT4, a significant gain. The
    preferable performance of FP4 over INT4 is particularly advantageous for hardware
    designs like H100, where FP8 is already supported. Thus, a simple modification
    to accommodate FP4 would be easier than implementing a system supporting INT4
    weight and FP8 activation.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 'FP8 权重与 INT8 相当，而 FP4 权重可能优于 INT4。从表 [2](#S4.T2 "Table 2 ‣ 4 Main Results ‣
    ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point
    Formats") 中，我们观察到在不同模型和数据集上，INT8 和 FP8 权重量化的性能相当，当保持激活为 FP8 时。这可能是由于我们在权重量化中使用了
    FGQ。有趣的是，当权重量化降低时，FP4 相较于 INT4 展现出一定的优势，特别是在 LLaMA-7b（15.14 到 16.09）和 LLaMA-13b
    模型（11.08 到 11.31）中尤为明显。具体而言，在 LLaMA-7b 的 W4A8 配置下，我们看到 FP4 相比于 INT4 提升了 0.95，这是一项显著的增益。FP4
    相较于 INT4 的优越性能对支持 FP8 的硬件设计如 H100 特别有利。因此，简单调整以适应 FP4 会比实现一个支持 INT4 权重和 FP8 激活的系统更为容易。'
- en: 'LoRC improves W4A8. Table [2](#S4.T2 "Table 2 ‣ 4 Main Results ‣ ZeroQuant-FP:
    A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")
    shows that the Low Rank Compensation (LoRC) method enhanced the W4A8 quantization
    scheme, reducing quantization errors. This improvement is particularly pronounced
    in smaller models, underlining the effectiveness of LoRC in optimizing the performance
    of these computing processes while impacting little on the model-size.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 'LoRC 改进了 W4A8。表 [2](#S4.T2 "Table 2 ‣ 4 Main Results ‣ ZeroQuant-FP: A Leap
    Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")
    显示，低秩补偿（LoRC）方法提高了 W4A8 量化方案，减少了量化误差。这种改进在较小模型中尤为明显，突显了 LoRC 在优化这些计算过程性能方面的有效性，同时对模型大小的影响很小。'
- en: 'Table 3: Scale values ($S$) are evaluated both without and with restrictions
    of being a power of 2, as shown in the second column. The quantization type employed
    is FP4 for weight and FP8 for activation.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：尺度值（$S$）在不限制为 2 的幂和限制为 2 的幂的情况下进行评估，如第二列所示。所使用的量化类型为权重的 FP4 和激活的 FP8。
- en: Q-type Scale LLaMA-3b LLaMA-7b LLaMA-13b LLaMA-30b $S=2^{n}$ Mean WIKI/PTB/C4
    Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 W4A8 ✗ 16.29 15.32/18.19/15.35
    12.09 10.89/13.44/11.95 11.34 10.16/12.55/11.30 10.72 9.52/11.90/10.75 ✓(M1) 16.66
    15.65/18.66/15.65 12.29 11.12/13.69/12.05 11.36 10.22/12.54/11.32 10.77 9.58/11.96/10.76
    ✓(M2) 16.47 15.23/18.55/15.62 12.25 11.11/13.61/12.03 11.40 10.22/12.61/11.36
    10.74 9.47/11.96/10.78 W4A8 ✗ 16.23 15.50/17.92/15.28 12.09 10.96/13.40/11.90
    11.33 10.15/12.55/11.29 10.71 9.48/11.90/10.75 LoRC ✓(M1) 16.47 15.59/18.37/15.45
    12.17 11.10/13.47/11.95 11.36 10.21/12.54/11.32 10.74 9.49/11.96/10.76 ✓(M2) 16.30
    15.39/18.10/15.42 12.19 11.11/13.49/11.97 11.41 10.34/12.54/11.34 10.75 9.49/11.96/10.78
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Q-type Scale LLaMA-3b LLaMA-7b LLaMA-13b LLaMA-30b $S=2^{n}$ Mean WIKI/PTB/C4
    Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 Mean WIKI/PTB/C4 W4A8 ✗ 16.29 15.32/18.19/15.35
    12.09 10.89/13.44/11.95 11.34 10.16/12.55/11.30 10.72 9.52/11.90/10.75 ✓(M1) 16.66
    15.65/18.66/15.65 12.29 11.12/13.69/12.05 11.36 10.22/12.54/11.32 10.77 9.58/11.96/10.76
    ✓(M2) 16.47 15.23/18.55/15.62 12.25 11.11/13.61/12.03 11.40 10.22/12.61/11.36
    10.74 9.47/11.96/10.78 W4A8 ✗ 16.23 15.50/17.92/15.28 12.09 10.96/13.40/11.90
    11.33 10.15/12.55/11.29 10.71 9.48/11.90/10.75 LoRC ✓(M1) 16.47 15.59/18.37/15.45
    12.17 11.10/13.47/11.95 11.36 10.21/12.54/11.32 10.74 9.49/11.96/10.76 ✓(M2) 16.30
    15.39/18.10/15.42 12.19 11.11/13.49/11.97 11.41 10.34/12.54/11.34 10.75 9.49/11.96/10.78
- en: 'Casting the FP4 to FP8. As detailed in Section [3](#S3 "3 Methodology ‣ ZeroQuant-FP:
    A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats"),
    to maximize real latency speedup on NVIDIA H100 hardware, we suggest the scale
    factor $S$. In pursuit of this, we executed a series of experiments using FP4
    for weight and FP8 for activation quantization. The results of these experiments,
    conducted both with and without LoRC, are presented in Table [3](#S4.T3 "Table
    3 ‣ 4 Main Results ‣ ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization
    Using Floating-Point Formats"). Our data shows that while constraining the scaling
    factors occasionally results in unexpected improvements in models like LLaMA-7b
    and LLaMA-13b, we generally observe a minor degradation of quality in the W4A8
    floating-point model, regardless of whether we used method M1 or M2\. M2 generally
    outperforms M1\. When we implement LoRC, this decline in quality can be mitigated,
    particularly in the OPT-1.3b, LLaMA-7b, and LLaMA-13b models. Hence, our results
    advocate for the use of LoRC, especially when considering scale restrictions for
    weight quantization in deep learning models.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '将 FP4 转换为 FP8。如第[3](#S3 "3 Methodology ‣ ZeroQuant-FP: A Leap Forward in LLMs
    Post-Training W4A8 Quantization Using Floating-Point Formats")节中详细描述，为了最大化在 NVIDIA
    H100 硬件上的实际延迟加速，我们建议使用缩放因子 $S$。为此，我们进行了系列实验，使用 FP4 进行权重量化和 FP8 进行激活量化。这些实验的结果，包括使用和不使用
    LoRC 的情况，见于第[3](#S4.T3 "Table 3 ‣ 4 Main Results ‣ ZeroQuant-FP: A Leap Forward
    in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats")表。我们的数据表明，虽然约束缩放因子偶尔会在像
    LLaMA-7b 和 LLaMA-13b 这样的模型中产生意外的改进，但我们通常观察到 W4A8 浮点模型的质量略有下降，无论是使用方法 M1 还是 M2。M2
    通常优于 M1。当我们实施 LoRC 时，这种质量下降可以得到缓解，特别是在 OPT-1.3b、LLaMA-7b 和 LLaMA-13b 模型中。因此，我们的结果支持使用
    LoRC，特别是在考虑深度学习模型中的权重量化缩放限制时。'
- en: 5 Conclusions
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结论
- en: In this study, we demonstrate that floating-point (FP) quantization significantly
    surpasses integer (INT) quantization in the context of large language models (LLMs)
    during post-training quantization. Notably, FP8 activation exceeds INT8, especially
    in larger models. Moreover, FP8 and FP4 weight quantization are either competitive
    with or surpass their INT equivalents. The Low Rank Compensation (LoRC) approach
    greatly enhances the W4A8 quantization scheme, particularly in smaller models.
    In conclusion, our work underscores the potential of FP quantization in enhancing
    model performance, and strategies such as LoRC further mitigate degradation induced
    by scale factor restrictions on weight quantization.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项研究中，我们展示了在大规模语言模型（LLMs）后训练量化的背景下，浮点（FP）量化显著优于整数（INT）量化。特别是，FP8 激活超越了 INT8，尤其是在更大的模型中。此外，FP8
    和 FP4 权重量化与其 INT 等效方法相比具有竞争力或更优。低秩补偿（LoRC）方法大大增强了 W4A8 量化方案，特别是在较小的模型中。总之，我们的工作强调了
    FP 量化在提升模型性能方面的潜力，而像 LoRC 这样的策略进一步减轻了由于缩放因子限制对权重量化造成的性能下降。
- en: Acknowledgement
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This research was conducted within the supportive environment of the DeepSpeed
    team at Microsoft, whose invaluable assistance was instrumental to this project.
    We thank Cheng Li and Connor Homes for the insightful discussions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究在微软 DeepSpeed 团队的支持环境中进行，他们的宝贵帮助对本项目至关重要。我们感谢 Cheng Li 和 Connor Homes 的深刻讨论。
- en: References
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry. Scalable methods
    for 8-bit training of neural networks. Advances in neural information processing
    systems, 31, 2018.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Ron Banner, Itay Hubara, Elad Hoffer, 和 Daniel Soudry。8位神经网络训练的可扩展方法。神经信息处理系统进展,
    31, 2018。'
- en: '[2] Léopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, Oguz H
    Elibol, and Hanlin Tang. Shifted and squeezed 8-bit floating point format for
    low-precision training of deep neural networks. arXiv preprint arXiv:2001.05674,
    2020.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Léopold Cambier, Anahita Bhiwandiwalla, Ting Gong, Mehran Nekuii, Oguz
    H Elibol, 和 Hanlin Tang。用于低精度深度神经网络训练的移位和压缩的8位浮点格式。arXiv 预印本 arXiv:2001.05674,
    2020。'
- en: '[3] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8
    (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339,
    2022.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tim Dettmers, Mike Lewis, Younes Belkada, 和 Luke Zettlemoyer。LLM. int8
    (): 大规模变换器的8位矩阵乘法。arXiv 预印本 arXiv:2208.07339, 2022。'
- en: '[4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora:
    Efficient finetuning of quantized llms. arXiv preprint arXiv:2305.14314, 2023.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, 和 Luke Zettlemoyer。Qlora：量化
    LLM 的高效微调。arXiv 预印本 arXiv:2305.14314, 2023。'
- en: '[5] Tim Dettmers and Luke Zettlemoyer. The case for 4-bit precision: k-bit
    inference scaling laws. arXiv preprint arXiv:2212.09720, 2022.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Tim Dettmers 和 Luke Zettlemoyer。支持4位精度的理由：k位推理缩放定律。arXiv 预印本 arXiv:2212.09720,
    2022。'
- en: '[6] Elias Frantar and Dan Alistarh. Optimal brain compression: A framework
    for accurate post-training quantization and pruning. arXiv preprint arXiv:2208.11580,
    2022.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Elias Frantar 和 Dan Alistarh。最佳脑压缩：用于准确后训练量化和剪枝的框架。arXiv 预印本 arXiv:2208.11580,
    2022。'
- en: '[7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq:
    Accurate post-training quantization for generative pre-trained transformers. arXiv
    preprint arXiv:2210.17323, 2022.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, 和 Dan Alistarh。GPTQ：生成预训练变换器的准确后训练量化。arXiv
    预印本 arXiv:2210.17323, 2022。'
- en: '[8] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, and Kurt Keutzer.
    Ai and memory wall. RiseLab Medium Post, 2021.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Amir Gholami, Zhewei Yao, Sehoon Kim, Michael W Mahoney, 和 Kurt Keutzer。人工智能与记忆墙。RiseLab
    Medium Post, 2021。'
- en: '[9] GitHub. Github copilot. [https://github.com/features/copilot/](https://github.com/features/copilot/),
    2021.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] GitHub。GitHub Copilot。 [https://github.com/features/copilot/](https://github.com/features/copilot/)，2021。'
- en: '[10] Babak Hassibi and David G Stork. Second order derivatives for network
    pruning: Optimal brain surgeon. In Advances in neural information processing systems,
    pages 164–171, 1993.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Babak Hassibi 和 David G Stork。用于网络剪枝的二阶导数：最佳脑外科医生。在神经信息处理系统进展中，第164–171页,
    1993。'
- en: '[11] Minje Kim and Paris Smaragdis. Bitwise neural networks. arXiv preprint
    arXiv:1601.06071, 2016.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Minje Kim 和 Paris Smaragdis。位运算神经网络。arXiv 预印本 arXiv:1601.06071, 2016。'
- en: '[12] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, and Kurt Keutzer. Squeezellm: Dense-and-sparse quantization.
    arXiv preprint arXiv:2306.07629, 2023.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
    Michael W Mahoney, 和 Kurt Keutzer。Squeezellm：密集与稀疏量化。arXiv 预印本 arXiv:2306.07629,
    2023。'
- en: '[13] Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,
    and Tijmen Blankevoort. Fp8 quantization: The power of the exponent. arXiv preprint
    arXiv:2208.09225, 2022.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Andrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus Nagel, Jorn Peters,
    和 Tijmen Blankevoort。FP8 量化：指数的力量。arXiv 预印本 arXiv:2208.09225, 2022。'
- en: '[14] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In
    Advances in neural information processing systems, pages 598–605, 1990.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Yann LeCun, John S Denker, 和 Sara A Solla。最佳脑损伤。在神经信息处理系统进展中，第598–605页,
    1990。'
- en: '[15] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song
    Han. Awq: Activation-aware weight quantization for llm compression and acceleration.
    arXiv preprint arXiv:2306.00978, 2023.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, 和 Song Han。AWQ：激活感知权重量化用于
    LLM 压缩和加速。arXiv 预印本 arXiv:2306.00978, 2023。'
- en: '[16] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. Llm-qat: Data-free
    quantization aware training for large language models. arXiv preprint arXiv:2305.17888,
    2023.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar
    Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, 和 Vikas Chandra。LLM-QAT：大语言模型的数据无关量化感知训练。arXiv
    预印本 arXiv:2305.17888, 2023。'
- en: '[17] Mary Ann Marcinkiewicz. Building a large annotated corpus of english:
    The penn treebank. Using Large Corpora, page 273, 1994.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Mary Ann Marcinkiewicz。建立大型英文注释语料库：Penn Treebank。使用大型语料库, 第273页, 1994。'
- en: '[18] Naveen Mellempudi, Abhisek Kundu, Dheevatsa Mudigere, Dipankar Das, Bharat
    Kaul, and Pradeep Dubey. Ternary neural networks with fine-grained quantization.
    arXiv preprint arXiv:1705.01462, 2017.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Naveen Mellempudi, Abhisek Kundu, Dheevatsa Mudigere, Dipankar Das, Bharat
    Kaul 和 Pradeep Dubey. 具有精细量化的三元神经网络。arXiv 预印本 arXiv:1705.01462，2017年。'
- en: '[19] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer
    sentinel mixture models. In International Conference on Learning Representations,
    2017.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Stephen Merity, Caiming Xiong, James Bradbury 和 Richard Socher. 指针哨兵混合模型。发表于国际学习表示会议，2017年。'
- en: '[20] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep
    Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John
    Kamalu, et al. Fp8 formats for deep learning. arXiv preprint arXiv:2209.05433,
    2022.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep
    Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick Judd, John
    Kamalu 等. Fp8 格式用于深度学习。arXiv 预印本 arXiv:2209.05433，2022年。'
- en: '[21] NVIDIA. FasterTransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer),
    January 2023.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] NVIDIA. FasterTransformer. [https://github.com/NVIDIA/FasterTransformer](https://github.com/NVIDIA/FasterTransformer)，2023年1月。'
- en: '[22] OpenAI. Openai chatgpt. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/),
    2022.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] OpenAI. OpenAI ChatGPT. [https://openai.com/blog/chatgpt/](https://openai.com/blog/chatgpt/)，2022年。'
- en: '[23] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
    Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff
    Dean. Efficiently scaling transformer inference. arXiv preprint arXiv:2211.05102,
    2022.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James
    Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal 和 Jeff Dean.
    高效扩展 transformer 推理。arXiv 预印本 arXiv:2211.05102，2022年。'
- en: '[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of
    transfer learning with a unified text-to-text transformer, 2019.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
    Michael Matena, Yanqi Zhou, Wei Li 和 Peter J. Liu. 通过统一的文本到文本 transformer 探索迁移学习的极限，2019年。'
- en: '[25] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé, et al. Bloom: A 176b-parameter open-access multilingual language model.
    arXiv preprint arXiv:2211.05100, 2022.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić,
    Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias
    Gallé 等. Bloom: 一个 176b 参数的开放获取多语言模型。arXiv 预印本 arXiv:2211.05100，2022年。'
- en: '[26] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
    Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay
    Korthikanti, et al. Using deepspeed and megatron to train megatron-turing nlg
    530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990,
    2022.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam
    Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay
    Korthikanti 等. 使用 deepspeed 和 megatron 训练 megatron-turing nlg 530b，一个大规模生成语言模型。arXiv
    预印本 arXiv:2201.11990，2022年。'
- en: '[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint
    arXiv:2302.13971, 2023.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
    Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal
    Azhar 等. Llama: 开放且高效的基础语言模型。arXiv 预印本 arXiv:2302.13971，2023年。'
- en: '[28] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin,
    Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga,
    et al. Fp8 versus int8 for efficient deep learning inference. arXiv preprint arXiv:2303.17951,
    2023.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Mart van Baalen, Andrey Kuzmin, Suparna S Nair, Yuwei Ren, Eric Mahurin,
    Chirag Patel, Sundar Subramanian, Sanghyuk Lee, Markus Nagel, Joseph Soriaga 等.
    Fp8 与 int8 在高效深度学习推理中的应用。arXiv 预印本 arXiv:2303.17951，2023年。'
- en: '[29] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo, and Xianglong Liu. Outlier suppression+: Accurate quantization of large language
    models by equivalent and optimal shifting and scaling. arXiv preprint arXiv:2304.09145,
    2023.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Xiuying Wei, Yunchen Zhang, Yuhang Li, Xiangguo Zhang, Ruihao Gong, Jinyang
    Guo 和 Xianglong Liu. Outlier suppression+: 通过等效和最优的平移与缩放精确量化大型语言模型。arXiv 预印本 arXiv:2304.09145，2023年。'
- en: '[30] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev, and Paulius Micikevicius.
    Integer quantization for deep learning inference: Principles and empirical evaluation.
    arXiv preprint arXiv:2004.09602, 2020.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Hao Wu, Patrick Judd, Xiaojie Zhang, Mikhail Isaev 和 Paulius Micikevicius.
    深度学习推理的整数量化：原理与实证评估。arXiv 预印本 arXiv:2004.09602，2020年。'
- en: '[31] Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, and Yuxiong
    He. Understanding int4 quantization for transformer models: Latency speedup, composability,
    and failure cases. arXiv preprint arXiv:2301.12017, 2023.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 吴晓霞, 李成, 雷扎·雅兹达尼·阿米纳巴迪, 赵哲伟, 和 贺宇雄。《理解 Transformer 模型的 int4 量化：延迟加速、可组合性和失败案例》。arXiv
    预印本 arXiv:2301.12017, 2023年。'
- en: '[32] Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, and Yuxiong He. Extreme
    compression for pre-trained transformers made simple and efficient. arXiv preprint
    arXiv:2206.01859, 2022.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 吴晓霞, 赵哲伟, 张敏佳, 李聪龙, 和 贺宇雄。《对预训练变换器进行极限压缩的简单而高效的方法》。arXiv 预印本 arXiv:2206.01859,
    2022年。'
- en: '[33] Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han.
    Smoothquant: Accurate and efficient post-training quantization for large language
    models. arXiv preprint arXiv:2211.10438, 2022.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 萧光轩, 林骥, 米卡埃尔·塞兹内克, 朱利安·德莫斯, 和 韩松。《Smoothquant：大型语言模型的准确且高效的后训练量化》。arXiv
    预印本 arXiv:2211.10438, 2022年。'
- en: '[34] Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong
    Li, and Yuxiong He. Zeroquant: Efficient and affordable post-training quantization
    for large-scale transformers. arXiv preprint arXiv:2206.01861, 2022.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 赵哲伟, 雷扎·雅兹达尼·阿米纳巴迪, 张敏佳, 吴晓霞, 李聪龙, 和 贺宇雄。《Zeroquant：大规模变换器的高效且经济的后训练量化》。arXiv
    预印本 arXiv:2206.01861, 2022年。'
- en: '[35] Zhewei Yao, Cheng Li, Xiaoxia Wu, Stephen Youn, and Yuxiong He. A comprehensive
    study on post-training quantization for large language models. arXiv preprint
    arXiv:2303.08302, 2023.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 赵哲伟, 李成, 吴晓霞, 斯蒂芬·尤恩, 和 贺宇雄。《大型语言模型后训练量化的全面研究》。arXiv 预印本 arXiv:2303.08302,
    2023年。'
- en: '[36] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
    Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open
    pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 苏珊·张, 斯蒂芬·罗勒, 纳曼·戈亚尔, 米凯尔·阿特克斯, 陆莫雅, 陈硕辉, 克里斯托弗·德万, 莫娜·迪亚布, 李贤, 维多利亚·林,
    等。《OPT：开放预训练变换器语言模型》。arXiv 预印本 arXiv:2205.01068, 2022年。'
- en: '[37] Yijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang, Ting Cao, Fan Yang,
    Mao Yang, Shanghang Zhang, and Ningyi Xu. Integer or floating point? new outlooks
    for low-bit quantization on large language models. arXiv preprint arXiv:2305.12356,
    2023.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 张奕佳, 赵凌然, 曹世杰, 王文强, 曹婷, 杨凡, 杨茂, 张尚航, 和 徐宁毅。《整数还是浮点？大型语言模型低位量化的新展望》。arXiv
    预印本 arXiv:2305.12356, 2023年。'
- en: Appendix A Experiment Details
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 A 实验细节
- en: As we used GPTQ method [[7](#bib.bib7)], we use C4 dataset to randomly select
    128 sentences for the light-weight PTQ and each of them has 2048 tokens. We run
    them on a single GPU (i.e, V100-32GB) thanks for the two open-source github repositories.⁵⁵5[https://github.com/IST-DASLab/gptq](https://github.com/IST-DASLab/gptq)
    and [https://github.com/qwopqwop200/GPTQ-for-LLaMa.git](https://github.com/qwopqwop200/GPTQ-for-LLaMa.git)
    To accommodate the real computation efficiency, the group-size for weight quantization
    is 256 for both model family (OPT and LLaMA) except the LLaMA-3b with 320 as its
    hidden-dimension is 3200\. All the checkpoints we used are from huggingface.⁶⁶6LLaMA-3b
    is [openlm-research/open_llama_3b](openlm-research/open_llama_3b) and all other
    LLaMA are from [decapoda-research/llama-#b-hf](decapoda-research/llama-#b-hf)
    where [#](#) can be 7b, 13b and 30b. As for OPT, they are from [facebook/opt-#b](facebook/opt-#b)
    where [#](#) can be 1.3b, 7b, 13b and 30b. As for activation, we perform token-wise
    quantization in order to accommodate the latency requirements. For LoRC method,
    the dimension for the two low-rank matrix we used for LLaMA is 8\. While for OPT,
    the dimension is 16, 32, 40 and 56 respectively for 1.3b, 6.7b, 13b and 30b. We
    did not try others dimension as indicated by [[35](#bib.bib35)] that dimension
    of the low-rank matrix does not play too much impact on the quantization error
    as long as it larger than 8.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用了 GPTQ 方法 [[7](#bib.bib7)]，我们使用 C4 数据集随机选择了 128 个句子进行轻量级 PTQ，每个句子有 2048
    个 token。我们在单个 GPU（即 V100-32GB）上运行它们，感谢两个开源的 GitHub 仓库 [https://github.com/IST-DASLab/gptq](https://github.com/IST-DASLab/gptq)
    和 [https://github.com/qwopqwop200/GPTQ-for-LLaMa.git](https://github.com/qwopqwop200/GPTQ-for-LLaMa.git)。为了适应实际的计算效率，权重量化的组大小对于两种模型系列（OPT
    和 LLaMA）都是 256，除了 LLaMA-3b，其隐藏维度是 3200，为 320。我们使用的所有检查点都来自 huggingface。LLaMA-3b
    的检查点来自 [openlm-research/open_llama_3b](openlm-research/open_llama_3b)，所有其他 LLaMA
    的检查点来自 [decapoda-research/llama-#b-hf](decapoda-research/llama-#b-hf)，其中 [#](#)
    可以是 7b、13b 和 30b。至于 OPT，它们来自 [facebook/opt-#b](facebook/opt-#b)，其中 [#](#) 可以是
    1.3b、7b、13b 和 30b。对于激活，我们执行逐 token 量化以适应延迟要求。对于 LoRC 方法，我们为 LLaMA 使用的两个低秩矩阵的维度为
    8。对于 OPT，维度分别为 1.3b、6.7b、13b 和 30b，为 16、32、40 和 56。我们没有尝试其他维度，因为 [[35](#bib.bib35)]
    表明低秩矩阵的维度对量化误差的影响不大，只要大于 8。
- en: 'Table A.1: Comparisons between E2M1 and E3M0\. The quantization is FP4 for
    weight and FP8 for activation.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 表 A.1：E2M1 和 E3M0 的比较。权重量化为 FP4，激活量化为 FP8。
- en: Activation (FP8) OPT-1.3b OPT-6.7b OPT-13b OPT-30b Weight-FP4 (E3M0) 16.96 12.41
    11.53 10.86 Weight-FP4 (E2M1) 16.23 12.09 11.33 10.71
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 激活（FP8）OPT-1.3b OPT-6.7b OPT-13b OPT-30b 权重-FP4（E3M0） 16.96 12.41 11.53 10.86
    权重-FP4（E2M1） 16.23 12.09 11.33 10.71
